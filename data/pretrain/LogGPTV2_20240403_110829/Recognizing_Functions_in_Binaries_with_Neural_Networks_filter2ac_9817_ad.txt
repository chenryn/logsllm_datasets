R
P
ELF x86-64
F1
P
R
F1
ByteWeight
Our models
92.78% 92.29% 92.53% 93.22% 92.52% 92.87%
97.75% 95.34% 96.53% 94.85% 89.91% 92.32%
PE x86
R
P
PE x86-64
F1
P
R
F1
ByteWeight
Our models
92.30% 93.91% 93.10% 93.04% 93.13% 93.08%
97.53% 95.27% 96.39% 98.43% 97.33% 97.88%
Table 3: Function boundary identiﬁcation: summary of our best results, and comparison with previous work. “P” is
precision and “R” is recall.
Our models (func. boundary)
ByteWeight (func. start only)
ByteWeight (func. boundary)
ByteWeight (func. boundary with RFCR)
ELF x86
1061.76 s
3296.98 s
367018.53 s
457997.09 s
ELF x86-64
1017.90 s
5718.84 s
412223.55 s
593169.73 s
PE x86
236.93 s
10269.19 s
54482.30 s
84602.56 s
PE x86-64
264.50 s
11904.06 s
87661.01 s
97627.44 s
Table 4: Computation time for testing on the data set of 2200 binaries. Numbers for ByteWeight are taken from Bao
et al. [2].
models separately for each dataset: one to ﬁnd function
starts, and the other to ﬁnd function ends. We combine
the predictions from each model using a simple heuristic:
• If we predict multiple function ends in sequence af-
ter a function start, ignore all but the last.
• If we predict multiple function starts in sequence
before a function end, ignore all but the ﬁrst.
• Otherwise, pair adjacent function starts and ends
into a function boundary.
Except on the ELF x86-64 dataset, this allows us to ob-
tain 97-98% in F1 score. In contrast, Bao et al. report
92-93%.
To obtain these results, we used bidirectional models
with RNN hidden units (i.e., rather than GRU or LSTM)
and one hidden layer of size 16. We trained each model
on 100,000 randomly-extracted 1000-byte chunks from
the corresponding binaries (or 100 megabytes in total).
To clarify, this means that we run two separate recurrent
neural networks forward and backward on a 1000-byte
sequence from the binary. The forward and backward
RNN each computes a R16 hidden representation, which
are concatenated together and fed into a linear transfor-
mation and the softmax function, producing a probability
distribution (a R2 vector) over whether that byte corre-
sponds to the beginning (or end) of a function or not.
We used rmsprop with a step size of 0.1, which was
scaled by the inverse square root of the current number
of iterations. We used a batch size of 32, which means
that in each iteration, we computed the gradients for 32
of the 1000-byte chunks, averaged them together, and ap-
plied them to the current weights (per the description in
Section 4.3).
These hyper-parameters (like the step size, the batch
size, and the output size of the RNN), unlike the weights
in the neural network, cannot be trained using gradient
descent. They have to be selected manually or through an
exhaustive search. We selected ours informed by some
smaller-scale experiments and our intuition.
5.4 Computation time
Training.
In many other applications, neural networks
have gained a reputation as requiring a lot of computa-
tional power to train. Indeed, it has become standard
to train large neural networks using GPUs (graphical
processing units), which excel at the large number of
linear algebra operations that training a neural network
requires. However, the networks we use are relatively
small, so training with the CPU seemed to work ﬁne.
In addition, determining the precise number of itera-
tions to train a neural network remains more of an art
than a science. Due to noisy gradients in stochastic gra-
dient descent, and the non-convex objective, the accuracy
of the neural network does not improve monotonically or
predictably as the number of iterations increases. In fact,
training for too long can cause the parameters to overﬁt
the training data, and worsen the performance on the test
data.
To avoid the issue, we trained our neural networks for
the ﬁxed time of two hours each, and report the per-
formance after that. We set the duration of training by
USENIX Association  
24th USENIX Security Symposium  621
11
ELF x86 ELF x86-64 PE x86 PE x86-64
98.02% 95.56% 98.37%
98.95%
95.51% 94.99% 98.06%
97.83%
Boundary 95.89%
92.67% 92.45% 95.91%
Start
End
Table 7: Results with when trained on 10% of the data
(F1 scores).
(wall-clock) elapsed time rather than the number of it-
erations or parameter updates, to avoid biasing in favor
of more complicated but powerful architectures which
might make more progress per iteration but also take
longer to compute each one.
Producing the results in Tables 2 and 3 requires train-
ing 40 models, since there are 4 ISA/OS combinations
and we used 10-fold cross-validation. Therefore, in to-
tal, 80 compute-hours were required. In contrast, Bao et
al. [2] report that ByteWeight took 586.44 compute hours
to train, over 7× longer.
Furthermore, we needed to train 40 models only for
the purposes of matching the 10-fold cross-validation
protocol used by Bao et al. We really only need four
models to achieve equivalent results, which would take
just 8 hours to produce. While abandoning cross-
validation for training ByteWeight would presumably
also save a signiﬁcant amount of time, we can expect
the factor to be less than 10 since much of the compu-
tation (extracting counts of short instruction sequences)
occurs before splitting data for cross-validation, further
widening the gap between ByteWeight and our method.
Testing. Table 4 summarizes the amount of time
needed to run each method on the data set after training
completes. Our method is hundreds of times faster than
the equivalent complete version ByteWeight which com-
putes function boundaries instead of just function ends.
The disparity mainly arises as our method works
without conventional program analysis techniques, such
as the static control-ﬂow graph generation used by
ByteWeight. We trained the neural network to directly
identify both function start and ends, and combine them
together using a simple algorithm to recover plausible
function boundaries. In addition, the neural network op-
erates directly on bytes rather than instructions, avoiding
the need for a disassembly step. In contrast, ByteWeight
computes a CFG starting from each identiﬁed function
start both to identify more functions, and to compute the
function boundary. These extra steps require a consider-
able amount of computation time, and yet our approach
gives better results without them.
5.5 Experiments
In this section, we describe some smaller-scale experi-
ments we performed in order to gain insight into how
various choices we made in designing our method affects
the accuracy of results. We trained each model for two
hours, and we did not use cross-validation for these ex-
periments to save on computation time.
Reducing training data.
In Table 7, we describe re-
sults from training a randomly-selected 10% of the bina-
ries in the dataset and testing on the remainder (normally,
the fractions were switched), to simulate common appli-
cations of binary analysis where only a small amount of
representative training data is available. Despite this, the
results dropped by 2-3 points at most.
Unidirectional RNNs. For our main model, we used
bidirectional RNNs where the output at each position de-
pends on both previous and future inputs. In Table 5, we
compare how bidirectional RNNs fare against the sim-
pler unidirectional ones. As we might expect, the unidi-
rectional RNNs do signiﬁcantly worse than the bidirec-
tional ones on every benchmark.
In Section 4.4, we speculated that on unidirectional
RNNs, reversing the order of the input might provide
better results if the bytes which come after, instead of be-
fore, a certain location in the binary provide more infor-
mation about whether that location is the start or end of a
function. We found that reversed inputs help with iden-
tifying ends of functions, and ordinary inputs with iden-
tifying starts. One reason for this may be that with op-
timization turned on, the compiler will insert no-ops be-
tween functions so that function starts occur at an aligned
offset; the model can identify these to help ﬁnd the start
or the end.
Variations in model architecture. Table 5 also com-
pares how Gated Recurrent Unit (GRU) and Long Short-
term Memory (LSTM) fare against conventional RNNs.
As we might expect, GRU and LSTM perform better than
RNN in most of the benchmarks. The comparison be-
tween GRU and LSTM is more mixed. Since LSTMs
take more time to run each iteration, and we trained for
a ﬁxed amount of computation time, they may not have
converged as much to optimal parameters. Also, while
GRU and LSTM are more powerful models than conven-
tional RNNs, this was not enough to beat the bidirec-
tional RNN.
We can also examine what happens when we vary the
number of hidden layers or the dimensionality of the hid-
den layer. Table 6 shows the different results obtained
using one hidden layer of size 8, two hidden layers of
size 8, or one hidden layer of size 16. The larger models
622  24th USENIX Security Symposium 
USENIX Association
12
Function start identiﬁcation
Function end identiﬁcation
RNN
GRU
LSTM
RNN (rev.)
GRU (rev.)
LSTM (rev.)
Bidir. RNN
ELF x86 ELF x86-64
92.36%
95.09%
94.32%
94.74%
95.92%
94.18%
98.88%
86.51%
92.64%
89.89%
76.05%
84.93%
94.18%
96.06%
PE x86
94.48%
96.46%
95.72%
66.02%
78.97%
72.48%
98.04%
PE x86-64 ELF x86 ELF x86-64
97.07%
98.26%
97.58%
83.47%
87.52%
83.43%
99.42%
54.52%
70.55%
70.69%
91.12%
95.33%
94.84%
95.93%
61.20%
72.21%
68.21%
84.91%
89.44%
87.78%
92.94%
PE x86
72.32%
83.78%
79.58%
95.52%
96.77%
97.09%
97.98%
PE x86-64
77.34%
85.95%
82.46%
95.68%
95.86%
95.42%
99.25%
Table 5: Comparison of unidirectional RNNs with different hidden unit types and input directionality, on the function
start and end identiﬁcation problems. “(rev.)” indicates that we trained and tested the model with bytes in the binary
reversed. All models (including the bidirectional RNN) had one layer and 8 hidden units. All percentages are F1
scores.
Function start identiﬁcation
Function end identiﬁcation
ELF x86 ELF x86-64
PE x86
PE x86-64 ELF x86 ELF x86-64
PE x86
PE x86-64
Separate
h = 8, l = 1
h = 8, l = 2
h = 16, l = 1
Shared
h = 8, l = 1
h = 8, l = 2
h = 16, l = 1
98.88%
99.03%
99.24%
97.79%
98.60%
98.29%
96.07%
97.69%
98.13%
95.28%
96.67%
97.41%
98.04%
98.00%
98.33%
97.30%
97.96%
98.42%
99.42%
99.43%
99.50%
99.23%
99.45%
99.47%
95.93%
97.71%
98.09%
95.86%
97.41%
97.20%
92.94%
94.49%
95.74%
91.94%
94.92%
95.51%
97.98%
98.30%
98.56%
97.08%
97.58%
98.32%
99.25%
99.19%
99.24%
98.90%
99.12%
99.38%
Table 6: Comparison of bidirectional RNNs on the function start and end identiﬁcation problems. Separate means two
models were trained separately for predicting starts and ends; shared means one model does both. h is the size of the
hidden layer and l is the number of layers. All percentages are F1 scores.
perform better, but it turns out that increasing the hid-
den layer size rather than the number of layers provides
a slightly greater beneﬁt.
Task sharing.
In our prior experiments, we trained two
separate neural networks for performing function start
and end identiﬁcation. However, we could instead train
one model to recognize both; at each byte, the model
would decide among four possibilities instead of two.
This could halve the amount of training time required.
Also, what the network needs to learn in order to rec-
ognize function starts probably overlaps considerably
with learning to recognizing function ends, so a network
which simultaneously performs both tasks may also learn
faster and produce more accurate results.
Table 6 summarizes our experimental results for test-
ing this hypothesis. Overall, the single model which per-
forms both tasks seems to do slightly worse than having
separate neural networks for each task. Perhaps the dis-
advantage incurred from needing to keep track of more