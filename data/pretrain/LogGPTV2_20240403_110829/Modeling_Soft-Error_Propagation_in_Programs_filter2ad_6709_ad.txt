similarity (or not) of the predictions to the FI results. The null
hypothesis for each of the three models in each benchmark
is that there is no difference between the FI results and the
predicted SDC probability values in each instruction.
TABLE II: p-values of T-test Experiments in the Prediction
of Individual Instruction SDC Probability Values (p > 0.05
indicates that we are not able to reject our null hypothesis –
the counter-cases are shown in red)
TRIDENT
0.602
0.392
0.000
0.893
0.163
0.000
0.277
0.059
0.033
0.166
0.497
3/11
Benchmark
Libquantum
Blackscholes
Sad
Bfs (Parboil)
Hercules
Lulesh
PureMD
Nw
Pathﬁnder
Hotspot
Bfs (Rodinia)
No. of rejections
fs+fc
0.000
0.173
0.003
0.000
0.000
0.000
0.000
0.000
0.130
0.000
0.001
9/11
fs
0.000
0.832
0.000
0.261
0.003
0.000
0.000
0.000
0.178
0.000
0.126
7/11
The p-values of the experiments are listed in the Table II.
At
the 95% conﬁdence level, using the standard criteria
(p > 0.05), we are not able to reject the null hypothesis in 8
out of the 11 benchmarks using TRIDENT in the predictions.
This indicates that the predictions of TRIDENT are shown to be
statistically indistinguishable from the FI results in most of the
benchmarks we used. The three outliers for TRIDENT again
are Sad, Lulesh and Pathﬁnder. Again, even though the individ-
ual instructions’ SDC probabilities predicted are statistically
distinguishable from the FI results, these predicted values are
still reasonably close to the FI results. In contrast, when using
fs +fc and fs to predict SDC probabilities for each individual
instruction, there are only 2 and 4 out of the 11 benchmarks
having p-values greater than 0.05,
the null
hypotheses cannot be rejected for most of the benchmarks.
In other words, the predictions from the simpler models for
individual instructions are (statistically) signiﬁcantly different
from the FI results.
indicating that
In this section, we evaluate the scalability of TRIDENT
to predict the overall SDC probabilities of programs and the
SDC probabilities of individual instructions, and compare it
to FI. By scalability, we mean the ability of the model to
handle large numbers of instruction samples in order to obtain
tighter bounds on the SDC probabilities. In general, the higher
the number of sampled instructions, the higher the accuracy
and hence the tighter are the bounds on SDC probabilities
for a given conﬁdence level (e.g., 95% conﬁdence). This is
true for both TRIDENT and for FI. The number of instructions
sampled for FI in prior work varies from 1,000 [30] to a few
thousands [9], [10], [20]. We vary the number of samples from
500 to 7, 000. The number of samples is equal to the number
of FI trials as one fault is injected per trial.
Note that the total computation is proportional to both the
time and power required to run each approach. Parallelization
will reduce the time spent, but not the power consumed. We
assume there is no parallelization for the purpose of compar-
ison in the case of TRIDENT and FI, though both TRIDENT
and FI can be parallelized. Therefore, the computation can be
measured by the wall-clock time.
(b) Instruction SDC Probability
(a) Overall SDC Probability
Fig. 6: Computation Spent to Predict SDC Probability
1) Overall SDC Probability: The results of the time spent
to predict the overall SDC probability of program are shown
in Figure 6a. The time taken in the ﬁgure is projected based on
the measurement of one FI trial (averaged over 30 FI runs). As
seen, the curve of FI time versus number of samples is much
steeper than that of TRIDENT, which is almost ﬂat. TRIDENT
is 2.37 times faster than the FI method at 1,000 samples, it
is 6.7 times faster at 3,000 samples and 15.13 times faster at
7,000 samples. From 500 to 7,000 samples, the time taken
by TRIDENT increases only 1.06 times (0.2453 to 0.2588),
whereas it increases 14 times (0.2453 to 3.9164) for FI - an
exact linear increase. The proﬁling phase of TRIDENT takes
0.24 hours (or about 15 minutes) on average. This is a ﬁxed
cost incurred by TRIDENT regardless of the number of sampled
instructions. However, once the model is built, the incremental
cost of calculating the SDC probability of a new instruction is
minimal (we only calculate the SDC probabilities on demand
to save time). FI does not incur a noticeable ﬁxed cost, but its
time rapidly increases as the number of sampled instructions
increase. This is because FI has to run the application from
scratch on each trial, and hence ends up being much slower
than TRIDENT as the number of samples increase.
2) Individual Instructions: Figure 6b compares the aver-
age time taken by TRIDENT to predict SDC probabilities of
individual instructions with FI, for different numbers of static
instructions. We consider different numbers of samples for
each static instruction chosen for FI: 100, 500 and 1,000 (as
mentioned in Section IV-A, TRIDENT does not need samples
for individual instructions’ SDC probabilities). We denote the
number of samples as a sufﬁx for the FI technique. For exam-
ple, FI-100 indicates 100 samples are chosen for performing FI
on individual instructions. We also vary the number of static
instructions from 50 to 7,000 (this is the X-axis). As seen
from the curves, the time taken by TRIDENT as the number
of static instructions vary remains almost ﬂat. On average, it
takes 0.2416 hours at 50 static instructions, and 0.5009 hours at
7,000 static instructions, which is only about a 2X increase. In
comparison, the corresponding increases for FI-100 is 140X,
which is linear with the number of instructions. Other FI curves
experience even steeper increases as they gather more samples
per instruction.
Fig. 7: Time Taken to Derive the SDC Probabilities of Indi-
vidual Instructions in Each Benchmark
Figure 7 shows the time taken by TRIDENT and FI-100 to
derive the SDC probabilities of individual instructions in each
benchmark (due to space constraints, we do not show the other
FI values, but the trends were similar). As can be seen, there
is wide variation in the times taken by TRIDENT depending
on the benchmark program. For example, the time taken in
PureMD is 2.893 hours, whereas it is 2.8 seconds in Pathﬁnder.
This is because the time taken by TRIDENT depends on factors
such as (1) the total number of static instructions, (2) the
length of static data-dependent instruction sequence, (3) the
number of dynamic branches that require proﬁling, and (4) the
number of redundant dependencies that can be pruned. The
main reason for the drastic difference between PureMD and
Pathﬁnder is that we can prune only 0.08% of the redundant
dependencies in the former, while we can prune 99.83% of
the dependencies in the latter. On average, 61.87% of dynamic
load and store instructions are redundant and hence removed
from the memory dependency graph.
VI. USE CASE: SELECTIVE INSTRUCTION DUPLICATION
In this section, we demonstrate the utility of TRIDENT
by considering a use-case of selectively protecting a program
from SDC causing errors. The idea is to protect only the
most SDC-prone instructions in a program so as to achieve
high coverage while bounding performance costs. We consider
instruction duplication as the protection technique, as it has
been used in prior work [9], [10], [21]. The problem setting
is as follows: given a certain performance overhead P , what
static instructions should be duplicated in order to maximize
the coverage for SDCs while keeping the overhead below P .
Solving the above problem involves ﬁnding the SDC
probability of each instruction in the program in order to
decide which set of instructions should be duplicated. It also
involves calculating the performance overhead of duplicating
the instructions. We use TRIDENT for the former, namely,
to estimate the SDC probability of each instruction, without
using FI. For the latter, we use the dynamic execution count
of each instruction as a proxy for the performance overhead
incurred by it. We then formulate the problem as a classi-
cal 0-1 knapsack problem [22], where the objects are the
instructions and the knapsack capacity is represented by P ,
the maximum allowable performance overhead. Further, object
proﬁts are represented by the estimated SDC probability (and
hence selecting the instruction means obtaining the coverage),
and object costs are represented by the dynamic execution
count of the instruction. Note that we assume that the SDC
probability estimates of the instructions are independent of
each other – while this is not necessarily true in practice,
it keeps the model
tractable, and in the worst case leads
to conservative protection (i.e., over-protection). We use the
dynamic programming algorithm for the 0-1 knapsack problem
- this is similar to what prior work did [21].
For the maximum performance overhead P , we ﬁrst mea-
sure the overhead of duplicating all the instructions in the
program (i.e., full duplication) and set this as the baseline
as it represents the worst-case overhead. The overheads are
measured based on the wall-clock time of the actual execution
of the duplicated programs (averaged on 3 executions each).
We ﬁnd that full duplication incurs an overhead of 36.18%
across benchmarks. We consider 2 overhead bound levels,
namely the 1/3rd and 2/3rd of the full duplication overheads,
which are (1) 11.78% and (2) 23.31% respectively.
For each overhead level, our algorithm chooses the instruc-
tions to protect using the knapsack algorithm. The chosen in-
structions are then duplicated using a special pass in LLVM we
wrote, and the duplication occurs at the LLVM IR level. Our
pass also places a comparison instruction after each instruction
protected to detect any deviations of the original computations
and duplicated computations. If protected instructions are
data dependent on the same static data-dependent instruction
sequence, we only place one comparison instruction at the
latter protected instruction to reduce performance overhead.
This is similar to what other related work did [9], [21]. For
comparison purposes, we repeat the above process using the
two simpler models (fs +fc and fs ). We then use FI to obtain
the SDC probabilities of the programs protected using the
different models at different overhead levels. Note that FI is
used only for the evaluation and not for any of the models.
Figure 8 shows the results of the SDC probability reduction
at different protection levels. Without protection, the average
SDC probability of the programs is 13.59%. At the 11.78%
overhead level, after protection based on TRIDENT, fs +fc
and fs the corresponding SDC probabilities are 5.50%, 5.53%,
9.29% respectively. On average, the protections provided by
the three models reduce the SDC probabilities by 64%, 64%
and 40% respectively. At the 23.31% overhead level, after the
protections based on TRIDENT, fs +fc and fs respectively,
the average SDC probabilities are 1.55%, 2.00% and 4.04%.
This corresponds to a reduction of 90%, 87% and 74% of the
SDC probability in the baseline respectively. Thus, on average,
Taking a closer look,
We ﬁnd that 1% of the faults affect fdiv in program such as
Lulesh, thereby leading to inaccuracies.
Fig. 8: SDC Probability Reduction with Selective Instruction Duplication at 11.78% and 23.31% Overhead Bounds (Margin of
Error: ±0.07% to ±1.76% at 95% Conﬁdence)
TRIDENT provides a higher SDC probability reduction for the
same overhead level compared with the two simpler models.
the protection based on fs +fc
achieves comparable SDC probability reductions with TRI-
DENT. This is because the relative ranking of SDC probabil-
ities between instructions plays a more dominant role in the
selective protection than the absolute SDC probabilities. The
ranking of the SDC probabilities of individual instructions de-
rived by fs +fc is similar to that derived by TRIDENT. Adding
fm boosts the overall accuracy of the model in predicting the
absolute SDC probabilities (Figure 5), but not the relative SDC
probabilities – the only exception is Libquantum. This shows
the importance of modeling control-ﬂow divergence, which is
missing in other existing techniques [9], [10], [27].
Conservatism in Determining Memory Corruption: Re-
call that when control-ﬂow divergence happens, we assume all
the store instructions that are dominated by the faulty branch
are corrupted (Section IV). This is a conservative assumption,
as some stores may end up being coincidentally correct. For
example, if a store instruction is supposed to write a zero to its
memory location, but is not executed due to the faulty branch,
the location will still be correct if there was a zero already in
that location. These are called lucky loads [7], [9].
VII. DISCUSSION
We ﬁrst investigate the sources of inaccuracy in TRIDENT
based on the experimental results (Section V). We then ex-
amine some of the threats to the validity of our evaluation.
Finally, we compare TRIDENT with two closely related prior
techniques, namely PVF and ePVF.
A. Sources of Inaccuracy
Errors in Store Address: If a fault modiﬁes the address
of a store instruction, in most cases, an immediate crash would
occur because the instruction accesses memory that is out of
bounds. However, if the fault does not cause a crash, it can
corrupt an arbitrary memory location, and may eventually lead
to SDC. It is difﬁcult to analyze which memory locations may
be corrupted as a result of such faults, leading to inaccuracy
in the case. In our fault injection experiments, we observe
that on average about 5.05% of faults affect addresses in store
instructions and survive from crashes.
Memory Copy: Another source of inaccuracy in TRIDENT
is that we do not handle bulk memory operations such as
memmove and memcpy, which are represented by special
instructions in the LLVM IR. We ﬁnd such operations in
benchmark such as Sad, Lulesh, Hercules and PureMD, which
makes our technique somewhat inaccurate for these programs.