A chain of receivers rooted at the sender
Binary tree with sender as the root
BitTorrent implementation for the Internet
Approach proposed in Section 5
Minimum broadcast time in the EC2 network
(measured to have 1.5 Gbps pairwise bidirec-
tional bandwidth) using pipelined binomial
tree distribution mechanism [19]
(a) 100 MB
(b) 1 GB
Figure 9: Completion times of different broadcast mechanisms
for varying data sizes.
to take full advantage of the faster network in a datacenter envi-
ronment and made BitTornado6 as much as 4.5× slower than the
streamlined Cornet implementation.
Cornet scaled well up to 100 receivers for a wide range of data
sizes in our experiments. For example, Cornet took as low as 15.4
seconds to complete broadcasting 1 GB data to 100 receivers and
remained within 33% of the theoretical lower bound. If there were
too few participants or the amount of data was small, Cornet could
not fully utilize the available bandwidth. However, as the num-
ber of receivers increased, Cornet completion times increased in a
much slower manner than its alternatives, which convinces us that
Cornet can scale well beyond 100 receivers.
We found structured mechanisms to work well only for smaller
scale. Any delay introduced by a straggling internal node of a tree
6We used Murder [8] with a modiﬁcation that forced every peer to
stay in the swarm until all of them had ﬁnished.
Figure 10: CDF of completion times of individual receivers
while transferring 1 GB to 100 receivers using different broad-
cast mechanisms.
or a chain propagated and got magniﬁed throughout the structure.
Indeed, upon inspection, we found that the non-monotonicity of
chain and tree completion times were due to this very reason in
some experimental runs (e.g., completion time for 25 receivers us-
ing a tree structure is larger than that for 50 receivers in 9(b)).
As expected, HDFS-based mechanisms performed well only for
small amounts of data. While increasing the number of replicas
helps, there is a trade-off between time spent in creating replicas
vs. time all the receivers would spend in reading from those repli-
cas. In our experiments, HDFS with 3 replicas performed better
than HDFS with 10 replicas when the total number of receivers
was less than 50. Overall, HDFS with 3 and 10 replicas were up to
5× and 4.5× slower than Cornet, respectively.
A Closer Look at Per-node Completion Times. We present the
CDFs of completion times of individual receivers for each of the
compared broadcast mechanisms in Figure 10.
Notice that almost all the receivers in Cornet ﬁnished simultane-
ously. The slight bends at the two endpoints illustrate the receivers
(< 10%) that ﬁnished earlier or slower than the average receiver.
The CDF representing BitTornado reception times is similar to that
of Cornet except that the variation in individual completion times is
signiﬁcantly higher and the average receiver is almost 4× slower.
Next, the steps in the CDFs of chain and tree highlight how strag-
glers slow down all their children in the distribution structure. Each
horizontal segment indicates a node that was slow in ﬁnishing re-
ception and the subsequent vertical segment indicates the receivers
that experienced head-of-line blocking due to a slow ancestor.
Finally, receivers in HDFS-based transfer mechanism with 10
replicas start ﬁnishing slower than those with 3 replicas due to
higher replication overhead. However, in the long run, receivers
using 10 replicas ﬁnish faster because of less reading contention.
The Case for a TC. As evident from Figure 9, the transfer mecha-
nisms have speciﬁc operating regimes. In particular, chain and tree
based approaches are faster than Cornet for small numbers of nodes
and small data sizes, likely because the block sizes and polling in-
tervals in Cornet prevent it from utilizing all the nodes’ bandwidth
right away. We conﬁrmed this by running another set of experi-
ments with 10 MB (not shown due to lack of space), where tree and
chain outperformed the other approaches. In an Orchestra imple-
mentation, a TC can pick the best transfer mechanism for a given
data size and number of nodes using its global knowledge.
7.2 Topology-aware Cornet
Next, we explore an extension of Cornet that exploits network topol-
ogy information. We hypothesized that if there is a signiﬁcant
difference between block transfer times within a rack vs. between
1102550100Number of receivers0510152025Time (s)HDFS (R=3)HDFS (R=10)ChainTree (D=2)BitTornadoCornetLower Bound1102550100Number of receivers020406080100120Time (s)HDFS (R=3)HDFS (R=10)ChainTree (D=2)BitTornadoCornetLower Bound020406080100Time (s)0.00.20.40.60.81.0Fraction of completed receiversHDFS (R=3)HDFS (R=10)ChainTree (D=2)BitTornadoCornet(a) 100 MB
(b) 200 MB
Figure 11: Cornet completion times when the rack topology is
unknown, given, and inferred using clustering.
Figure 13: Transfer topologies used in the weighted shufﬂe ex-
periment. The arrows show the number of units of data sent
from each mapper to each reducer.
Table 3: Completion times in seconds for WSS compared to
a standard shufﬂe implementation on the three topologies in
Fig. 13. Standard deviations are in parentheses.
Topology
A
B
C
Standard
Shufﬂe
83.3 (1.1)
131 (1.8)
183 (2.6)
WSS
Speedup Theoretical
70.6 (1.8)
105 (0.5)
142 (0.7)
18%
24%
29%
Speedup
25%
33%
38%
(a) DETERlab
(b) Amazon EC2
Figure 12: Two-dimensional, non-metric projection of receiver
nodes based on a distance matrix of node-to-node block trans-
fer times. The ellipses represent the inferred clusters. The tri-
angles, squares and circles in (a) represent Rack A, B and C
respectively in the DETERlab testbed.
racks, then a topology-aware version of Cornet, which reduces cross-
rack communication, will experience improved transfer times. To
answer this question, we conducted an experiment on a 31 node
DETERlab testbed (1 TC and 30 receivers). The testbed topology
was as follows: Rack A was connected to Rack B and Rack B to
Rack C. Each rack had 10 receiver nodes. The TC was in Rack B.
We ran the experiment with three TC conﬁgurations. The ﬁrst
was the default topology-oblivious Cornet that allowed any receiver
to randomly contact any other receiver. The second was Cornet-
Topology, where the TC partitioned the receivers according to Racks
A, B, and C, and disallowed communication across partitions. The
last one was CornetClustering, where the TC dynamically inferred
the partitioning of the nodes based on the node-to-node block trans-
fer times from 10 previous training runs.
The results in Figure 11 show the average completion times to
transfer 100 MB and 200 MB of data to all 30 receivers over 10
runs with min-max error bars. Given the topology information
(CornetTopology), the broadcasts’ completion time decreased by
50% compared to vanilla Cornet for the 200 MB transfer. In 9 out
of 10 runs for the 200 MB transfer, the TC inferred the exact topol-
ogy (see Figure 12(a) for a typical partitioning). Only in one run
did the TC infer 5 partitions (splitting two of the racks in half),
though this only resulted in a 2.5 second slowdown compared to
inferring the exact topology. With the ten runs averaged together,
CornetClustering’s reduction in completion time was 47%.
We also evaluated Cornet and CornetClustering on a 30 node
EC2 cluster. Evaluating CornetTopology was not possible because
we could not obtain the ground-truth topology for EC2. The per-
formance of Cornet using inferred topology did not improve over
Cornet on EC2 — the algorithm found one cluster, likely due to
EC2’s high bisection bandwidth (Section 6.2). The projection in
Figure 12(b) showed that with the exception of a few outliers (due
to congestion), all the nodes appeared to be relatively close to one
another and could not be partitioned into well-separated groups.
Overall, the results on the DETERlab demonstrate that when
there is a sizable gap between intra-rack and inter-rack transfer
times, knowing the actual node topology or inferring it can sig-
niﬁcantly improve broadcast times.
7.3 Weighted Shufﬂe Scheduling (WSS)
In this experiment, we evaluated the optimal Weighted Shufﬂe Schedul-
ing (WSS) algorithm discussed in Section 6.3 using three topolo-
gies on Amazon EC2. Figure 13 illustrates these topologies, with
arrows showing the number of units of data sent between each pair
of nodes (one unit corresponded to 2 GB in our tests). Topology A
is the example discussed in Section 6.3, where two receivers fetch
differing amounts of data from ﬁve senders. Topologies B and C
are extensions of topology A to seven and nine map tasks.
We ran each scenario under both a standard implementation of
shufﬂe (where each reducer opens one TCP connection to each
mapper) and under WSS. We implemented WSS by having each re-
ceiver open a different number of TCP connections to each sender,
proportional to the amount of data it had to fetch. This allowed us
to leverage the natural fair sharing between TCP ﬂows to achieve
the desired weights without modifying routers and switches.
We present average results from ﬁve runs, as well as standard
deviations, in Table 3.
In all cases, weighted shufﬂe scheduling
performs better than a standard implementation of shufﬂe, by 18%,
24% and 29% for topologies A, B and C respectively.
In addi-
tion, we present the theoretical speedup predicted for each topol-
ogy, which would be achieved in a full bisection bandwidth net-
work with a perfect implementation of fair sharing between ﬂows.
The measured results are similar to those predicted but somewhat
Algorithms05101520253035Time (s)CornetCornetTopo.CornetClus.Algorithms05101520253035Time (s)CornetCornetTopo.CornetClus.−2000−5005001500−1500010001st projected coord.2nd projected coord.llllllllll−300−100100300−300−1001003001st projected coord.2nd projected coord.A: B: C: 1 1 2 2 1 1 1 1 1 3 1 1 1 3 1 1 1 1 4 1 1 1 1 4 (a) Without Inter-Transfer Scheduling
(a) Average completion times
(b) Individual completion times
Figure 15: Average completion times of four shufﬂes with no
transfer-level scheduling, FIFO, and FIFO+ (which gives small
shares of the network to the transfers later in the FIFO queue).
(b) Priority Scheduling with an Orchestra ITC
Figure 14: Percentage of active ﬂows in the cluster for four dif-
ferent shufﬂe transfers in two priority classes with and without
inter-transfer scheduling.
lower because fair sharing between TCP ﬂows is not perfect (e.g., if
a node starts 2 GB transfers to several nodes at the same time, these
transfers can ﬁnish 10-15 seconds apart).
7.4 Scheduling Across Transfers
Cross-Transfer Priority Scheduling In this experiment, we eval-
uated the average transfer completion times of three different smaller
high priority jobs while a low priority larger job was running in the
background in a 30-node cluster. The ITC was conﬁgured to put
an upper limit on the number of ﬂows created by the low priority
transfer when at least one high priority job was running. Within the
same priority class, the ITC used FIFO to schedule jobs. The larger
transfer was shufﬂing 2 GB data per mapper, while the smaller ones
were transferring 256 MB from each mapper. Both the experiments
(with or without ITC) in Figure 14 follow similar timelines: the low
priority transfer was active during the whole experiment. The ﬁrst
high priority transfer started after at least 10 seconds. After it ﬁn-
ished, two more high priority transfers started one after another.
In the absence of any inter-transfer scheduler, the smaller high
priority transfers had to compete with the larger low priority one
throughout their durations. Eventually, each of the high priority
transfers took 14.1 seconds to complete on average, and the larger
low priority transfer took 45.6 seconds to ﬁnish. When using Or-
chestra for inter-transfer priority scheduling, the low priority trans-
fer could create only a limited number of ﬂows; consequently, the
average completion time for the high priority transfers decreased
43% to 8.1 seconds. The completion time for the background trans-
fer increased slightly to 48.7 seconds.
Cross-Transfer FIFO Scheduling In this experiment, we evalu-
ated the average transfer completion times of four concurrent shuf-
ﬂe transfers that started at the same time in a 30 node cluster. We
implemented a strict FIFO policy in the ITC as well as FIFO+,
which enforces a FIFO order but gives a small share of the network
(3% each) to other transfers as well. The intuition behind FIFO+ is
to keep mappers busy as much as possible – if some reducers of the
ﬁrst transfer ﬁnish receiving from a particular mapper, it can send
Figure 16: Per-iteration completion times for the logistic re-
gression application before and after using Orchestra.
data to reducers from other transfers. In absence of the ITC, all the
ﬂows get their shares of the network as enforced by TCP fair shar-
ing. During each shufﬂe transfer, each of the 30 reducers received
a total of 1 GB data from 30 mappers.
We present the average results from ﬁve runs in Figure 15 with
max-min error bars. Figure 15(a) shows the average transfer com-
pletion times for four transfers. FIFO achieved the best average
completion time of 34.6s and was 1.45× faster than TCP fair shar-
ing. The theoretical maximum7 speedup in this case is 1.6×.
The stacked bars in Figure 15(b) show how much longer Job i
took to ﬁnish its transfer once Job (i−1)’s had ﬁnished. We observe
that the ﬁnishing time of the slowest transfer remained the same in
both FIFO ordering and TCP fair sharing. However, with FIFO,
each transfer actively used the full network for equal amounts of
time to complete its transfer and improved the average.
By allowing the latter transfers to have some share of the net-
work, FIFO+ increased the completion time of the ﬁrst transfer.
Meanwhile, it decreased the completion times of all the other trans-
fers in comparison to FIFO. The ﬁnal outcome is a higher average
completion time but lower total completion time than strict FIFO.
7.5 End-to-End Results on Full Applications
We revisit the motivating applications from Section 2 to examine
the improvements in end-to-end run times after adopting the new