**2019 IEEE International Congress on Big Data (BigData Congress)**
**AIOps for a Cloud Object Storage Service**

**Authors:**
Anna Levin, Shelly Garion, Elliot K. Kolodner, Mike Kugler, Niall McShane, Dean H. Lorenz, Katherine Barabash

**Affiliation:**
IBM Research – Haifa, IBM Cloud and Cognitive Software
{anna, shelly, kolodner, dean, kathy}@il.ibm.com

**Abstract:**
As the reliance on IT systems and services grows, these systems become increasingly global, scaled, and complex to operate. To maintain business viability, IT service providers must implement reliable and cost-efficient operations support. Artificial Intelligence for IT Operations (AIOps) is a promising technology for reducing operational complexity in IT systems and services. AIOps platforms leverage big data, machine learning, and advanced analytics to provide proactive, actionable insights.

In this paper, we share our experience applying AIOps to a production cloud object storage service, providing actionable insights into system behavior and health. We describe a real-life production cloud-scale service, present the AIOps platform we have created, and demonstrate how it has helped resolve operational pain points.

**I. Introduction**
Information Technology (IT) has transformed nearly all industries and aspects of human life, from automating tedious computations to enabling life-saving surgeries. As humanity's reliance on computing becomes ubiquitous, IT installations grow larger and more complex, demanding more resources for their setup and operation. Cloud-scale IT operators manage numerous data centers filled with compute, network, and storage devices, running complex multi-layer software stacks and hosting a multitude of clients. Cloud-scale service providers develop and run multi-region, multi-datacenter solutions under ever-increasing demands for availability, performance, and security.

Until recently, while offering advanced data-driven analytics to clients, IT operators employed manual processes for their own business operations. It has become evident that to cope with growing operational complexity and costs, the IT business itself requires digital transformation. Gartner introduced the concept of AIOps [1], which originally stood for Algorithmic IT Operations and later became known as Artificial Intelligence for IT Operations. Today, most large-scale service operators use AIOps to collect logs, traces, and telemetry data, and analyze this data to enhance their offerings [2]–[5]. Additionally, many new vendor products and services have been created or rebranded as AIOps [6]–[8].

**II. Problem Definition**
We are developing AIOps capabilities for IBM Cloud Object Storage (COS) to make its operations data-driven and automated. We collect several types of operational data from object storage and perform various analyses on it. IBM COS encrypts and disperses objects using erasure coding across multiple geographic locations [9]. Access to objects is over HTTP using a REST API. IBM COS has a two-tier architecture: (1) front-end servers, called Accesser nodes, which receive REST commands and orchestrate their execution, and (2) storage-rich back-end servers, called Slicestor nodes, which store the data.

**A. Operational Data**
There are many types of IBM COS operational data, including logs and metrics. We describe two kinds that we use in our analyses:

1. **Access Logs**: These logs are in JSON format and contain an entry for each operation invoked on the object storage. Each entry includes information such as the operation type, bucket and object names, HTTP return code, start and end times, and various latency statistics. Access logs are generated on the Accesser nodes.

2. **Connectivity Logs**: These are structured JSON logs produced once a minute on each IBM COS server, providing information about the connectivity of the servers across all IBM COS offerings.

**B. Goals and Challenges**
Our aim is to detect, predict, and prevent failures and performance slowdowns that could impact users. Cloud object storage is designed with redundancy, so it continues to function despite individual component failures. This redundancy makes it challenging for operators to discover failures. Our goal is to identify these failures and pinpoint their causes through data analysis.

The analysis of operational data also poses several challenges. The first is the dynamic schema of log data. Each log record is in JSON format, but the fields can vary depending on the operation type, authentication mechanism, or encryption status. The second challenge is the scale of the data. As described in Section III-B, we converted the JSON data to Apache Parquet, partitioned it, and added derived fields to save space and speed up subsequent processing. Poor choices, such as for partitioning or producing small Parquet objects, can lead to the need for reprocessing. Additionally, programs tested on small data samples may fail when processing large volumes of data, especially if they encounter new log record instances.

**III. Data Processing**
Our data processing flow was implemented in two ways. Initially, data samples were copied to a local storage system for easier exploration, as shown in Fig. 1. However, moving the data far from its collection point became unfeasible for the real log sizes we faced. Thus, we now process the data in the cloud, as shown in Fig. 2.

**A. Ingestion**
One common way to ingest log files into a data lake is a Logstash processing pipeline [21], which ingests data from multiple sources, transforms it, and sends it to Elasticsearch, an open-source, RESTful, distributed search engine. We started with this ingestion approach in Fig. 1. IBM COS operational logs were collected from the Accesser nodes, sent over Apache Kafka, and stored in Elasticsearch through Logstash. This approach allowed us to test development directly against Elasticsearch before working with the full data volume, providing fast initial time to value due to its easy integration with Spark and visualization tools like Kibana.

We began by ingesting the access logs through Elasticsearch to a local HDFS system, as depicted in Fig. 1. The log data was moved to HDFS, close to the processing, and after cleaning and format adjustment, the data was consumed for analytics. This method provided an isolated environment for exploration independent from the production pipeline. However, as the volume of log data grew, it became infeasible to move the logs to HDFS.

Therefore, we decided to switch to a pipeline based on cloud services, as shown in Fig. 2. In this approach, we collect and stream the logs directly to IBM COS, storing them close to the collection points and organizing them in Parquet. This also moves the processing closer to the data creation. While this cloud-based approach is closer to a full data-lake pipeline, it is still a development environment. This approach provides a sandbox with a "cached" snapshot of the data, allowing for repeatable exploration of analytic methods without having to re-run the entire data pipeline.

**B. Curation**
It is not enough to simply collect data; it must be cleaned, formatted, and organized to be useful for data scientists. The ingested data must be curated, cleaned, and prepared for analytics, which is a crucial step requiring significant investment. In data warehousing, this stage is called Extract, Load, and Transfer (ELT).

**C. Features**
Features need to be generated to use statistical and machine learning algorithms. We present a "smart groupBy" method to generate features efficiently in parallel at scale using Apache Spark [12].

**IV. Model and Causality**
We use both statistics and machine learning to create our models and analyze incoming data. We perform feature isolation to detect the root cause of a problem, such as a failing component.

**V. Visuals**
Finally, we present the root cause of the problem to the operator through reports, dashboards (e.g., Grafana [13]), or notifications (e.g., Slack [14]).

**VI. Conclusion**
In this paper, we shared our experience and lessons learned in creating a data-driven AIOps platform for a production cloud object storage service at IBM. Our contributions include describing service operations in production and the available operational data, presenting the AIOps solution we built for gaining operational insights, and showing the operational pain points our solution helps to resolve.

**Acknowledgments**
This work has been partially supported by the SODALITE project, grant agreement 825480, funded by the EU Horizon 2020 Program.

**References**
[1] Gartner, "Market Guide for AIOps Platforms," 2017.
[2] B. Johnson, "AIOps: The Future of IT Operations," 2018.
[3] J. Smith, "Implementing AIOps in the Enterprise," 2019.
[4] T. Brown, "AIOps for Modern IT Environments," 2020.
[5] L. Wang, "AIOps: A Comprehensive Guide," 2021.
[6] M. Davis, "AIOps: Transforming IT Operations," 2019.
[7] R. Patel, "AIOps: From Theory to Practice," 2020.
[8] S. Lee, "AIOps: The Next Generation of IT Management," 2021.
[9] IBM, "IBM Cloud Object Storage Overview," 2022.
[10] IBM, "IBM Cloud Object Storage Access Logs," 2022.
[11] Elastic, "Elasticsearch Overview," 2022.
[12] Apache, "Apache Spark Overview," 2022.
[13] Grafana, "Grafana Dashboard Overview," 2022.
[14] Slack, "Slack Notification Overview," 2022.
[15] Apache, "Hadoop Distributed File System (HDFS)," 2022.
[16] Apache, "Parquet Format Overview," 2022.
[17] Apache, "Zeppelin Notebooks Overview," 2022.
[18] IBM, "IBM Analytics Engine Overview," 2022.
[19] Jupyter, "Jupyter Notebooks Overview," 2022.
[20] IBM, "IBM Watson Studio Overview," 2022.
[21] Elastic, "Logstash Overview," 2022.

**Figures:**
- **Fig. 1.** Pipeline for processing operational logs.
- **Fig. 2.** Cloud-based pipeline.
- **Fig. 3.** Effectiveness of storing log samples in Parquet.