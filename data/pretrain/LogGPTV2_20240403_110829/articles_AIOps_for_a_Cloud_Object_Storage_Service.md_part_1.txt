2019 IEEE International Congress on Big Data (BigData Congress)
AIOps for a Cloud Object Storage Service
Anna Levin, Shelly Garion, Elliot K. Kolodner, Mike Kugler, Niall McShane
Dean H. Lorenz, Katherine Barabash
IBM Research – Haifa IBM Cloud and Cognitive Software
{lanna,shelly,kolodner,dean,kathy}@il.ibm.com PI:EMAIL, PI:EMAIL
Abstract—Withthegrowingrelianceontheubiquitousavail- products for smaller scale operators and enterprise IT have
ability of IT systems and services, these systems become more been created or rebranded as AIOps [6]–[8]. Some AIOps
global, scaled, and complex to operate. To maintain business
platforms are intrinsically integrated into the IT system they
viability, IT service providers must put in place reliable and
helptooperate.Othersolutions,mostlyvendorproductsand
cost efficient operations support. Artificial Intelligence for IT
Operations (AIOps) is a promising technology for alleviating services, are more generic. These often cover only a part of
operational complexity of IT systems and services. AIOps theAIOpsstack,suchasdatacollection,orspecificanalytical
platformsutilizebigdata,machinelearningandotheradvanced methods and algorithms, or data integration in a data lake,
analytics technologies to enhance IT operations with proactive
etc. Such point solutions are hard to integrate into existing
actionable dynamic insight.
operational production systems in a meaningful way. We
In this paper we share our experience applying the AIOps
approach to a production cloud object storage service to get have addressed this gap by creating a useful AIOps system
actionable insights into system’s behavior and health. We aroundanexistingcloud-scaleservice,namelyanobjectstor-
describe a real-life production cloud scale service and its age service. Cloud-scale object storage may contain trillions
operationaldata,presenttheAIOpsplatformwehavecreated,
of objects, serve massive amounts of simultaneous access
andshowhowithashelpedusresolvingoperationalpainpoints.
requests, and generate tons of operational data. Our goal is
I. INTRODUCTION extracting useful business insights from the operational data
collected dynamically from a production service. However,
Information Technology (IT) has transformed almost all
thesheermagnitudeofdata,aswellasvariabilityofformats
industries and areas of human life. Technology has gone
and data sources, and speed of data generation, make it
all the way from automating tedious computations and
difficult to turn abstract theories into practice.
eliminating paper-driven office processes to governing life-
Inthispaper,weshareourexperienceandlessonslearned
saving surgery and competing with humans on trivia, game,
creatingadatadrivenAIOpsplatformforaproductioncloud
and debate contests. As humanity’s reliance on computing
object storage service at IBM. Our contributions are: (1)
becomes ubiquitous, IT installations grow larger and more
describingserviceoperationsinproductionandtheavailable
complex, demanding increasingly more resources for their
operational data, (2) presenting the AIOps solution we have
bring up and operation. Cloud-scale IT operators own tens
built for gaining operational insights, and (3) showing the
ofdatacentersfullofcompute,network,andstoragedevices,
operational pain points our solution helps to resolve.
running complex multi-layer software stacks and hosting a
multitude of clients. Cloud-scale service providers develop II. PROBLEMDEFINITION
and run multi-region multi-datacenter solutions under ever
We are developing AIOps capabilities for IBM Cloud
increasing availability, performance, and security demands.
ObjectStorage(COS)sothatitsoperationwillbedatadriven
Until very recently, IT operators, while offering advanced
and automated. We collect several types of operational data
data-driven analytics to their clients in various domains,
from object storage, and do several different analyses on it.
employed old-fashioned manual processes for running their
IBM COS encrypts and disperses the objects stored in it us-
ownbusinessoperations.Finally,ithasbecomeapparentthat
ing erasure coding across multiple geographic locations [9].
to cope with growing operational complexity and costs, the
Access to objects is over HTTP using a REST API. IBM
IT business itself requires digital transformation.
COShasatwotierarchitecture:(1)front-endservers,called
To highlight the growing interest and investment in
Accesser(cid:2)R nodes,thatreceivetheRESTcommandsandthen
this transformation, Gartner has introduced the concept of
orchestrate their execution across (2) storage-rich back-end
AIOps [1]. The concept originally stood for Algorithmic IT
servers, called Slicestor(cid:2)R nodes, that store the data.
OperationsandlaterbecameknownasArtificialIntelligence
for IT Operations. Today, most planet-scale service oper- A. Operational data
ators employ their own AIOps to collect logs, traces and
There are many types of IBM COS operational data, e.g.,
telemetry data, and analyze the collected data to enhance
logs and metrics. We describe two kinds that we use in our
their offerings [2]–[5]. In addition, multiple new vendor
analyses.
ThisworkhasbeenpartiallysupportedbytheSODALITEproject,grant The first are access logs [10]. These are in JSON format.
agreement825480,fundedbytheEUHorizon2020Programm. Theselogscontainanentryforeachoperationinvokedonthe
978-1-7281-2772-9/19/$31.00 ©2019 IEEE 165
DOI 10.1109/BigDataCongress.2019.00036
Authorized licensed use limited to: University of Guelph. Downloaded on August 10,2023 at 08:59:22 UTC from IEEE Xplore. Restrictions apply.
object storage. The entry contains a wealth of information III. DATAPROCESSING
regardingtheoperation;thisincludesitemssuchastheopera-
Our data processing flow was implemented in two ways.
tiontype,thebucketandobjectnamesonwhichtheoperation
Initially the data samples were copied to a local storage
works, the HTTP return code, the start and end times of the
system for easier exploration, see Fig. 1. However, moving
operation, and various latency statistics (total latency, time
thedatafarfromitscollectionbecameunfeasibleforthereal
spentwaitingfortheclient,timespentwaitingforthestorage
logs sizes we faced. Thus, we now do the data processing
backend,timeforauthenticationandauthorization,etc.).The
flow in the cloud, as shown in Fig. 2.
access logs are generated on the Accesser nodes. Fig. 1 shows the initial data processing flow. The IBM
The second are connectivity logs. These are structured COS access logs in JSON format are collected in Elas-
JSON logs produced once a minute on each IBM COS ticsearch [11] (ES) and historical dumps of ES are stored
server. They provide information about the connectivity of in an IBM COS bucket. In order to build a non-intrusive
the servers across all the offerings of IBM COS. exploration pipeline, we copied the ES dumps to a local
storagesystem:splittingfilesintochunkstooptimizeparallel
B. Goals and challenges
copying and verifying correctness with MD5. The copied
Our aim is to detect, predict and prevent failures and per-
chunks were stored in HDFS [15] and converted to Par-
formance slowdowns that could impact users. Cloud object
quet [16] to ease the analytics. For analytics we used Spark
storage is engineered with redundancy, so that it continues
together with Zeppelin notebooks [17], a common platform
working despite failures of individual components. Thus,
for interactive data analytics.
whenafailureoccursitismaskedbytheredundancy,which
Despite the advantages of having data close to the data
makes it difficult for the operators to discover. Our goal is
scientist for efficient development and easy data manipu-
to discover these failures and pinpoint their cause through
lation, the initial solution was not scalable enough for our
analysis of the data.
needs.Asaresult,wenowdotheprocessinginthecloud,as
In turn, the analysis of the operations data also poses
depictedinFig.2.Inthissecondpipeline,thedataprocessing
several challenges. The first is that the schema of the log
isintegratedwiththecollectionstageandthelogsarestored
data is dynamic. Each log record is JSON, so its fields are
in IBM COS in Parquet format. The data is consumed for
labeled, but each individual record might have a different
analytics by Apache Spark [12] applications running on the
subset of the fields, e.g., depending on the operation type,
IBM Analytics Engine [18]. For interactive development,
the authentication mechanism, or whether the object data
we use Jupyter notebooks [19] provided by IBM Watson
is encrypted. The second is the scale of the data to be
Studio[20].Intheremainderofthissectionwedescribeour
processed. As described in Section III-B, we converted the
experience with the processing flows presented above.
JSON data to Apache Parquet, partitioned it and added
derived fields in order to save space and speed subsequent A. Ingestion
processing. Poor choices, e.g., for partitioning, or producing One of the common ways to ingest log files into a data
Parquet objects that are too small, could lead to the costly lake is a Logstash processing pipeline [21] that ingests data
need to reconvert and reprocess the data. Also, programs from multiple sources simultaneously, transforms it, and
testedonsmallsamplesofdatamightfailafterlongrunning then sends it to Elasticsearch - an open-source, RESTful,
timeswhenprocessingverylargeamountsofdata,e.g.,when distributed search engine. We started with this ingestion
encountering a new log record instance. approachinFig1.IBMCOSoperationallogswerecollected
1) Our Approach: We apply the following stages: fromtheAccessernodes,sentoverApacheKafkaandstored
Ingestion (Sec. III-A): Ingestion of historical data is from in ES through Logstash. The ability to test development
batchfiles,e.g.,Elasticsearch[11]dumpfiles,aswellasof directlyagainstESbeforeworkingwiththefulldatavolume
streaming data, e.g., from Apache Kafka. provided fast initial time to value due to its easy integration
Curation (Sec. III-B): Datacuration,cleaningandprepara- with Spark and with visualization tools, e.g. Kibana.
tionforanalyticsiscrucial,andrequiresahugeinvestment. Westartedthedevelopmentofthepipelinebyingestingthe
In data warehousing this stage is called Extract, Load and accesslogsthroughEStoalocalHDFSsystem,asdepicted
Transfer (ELT). in Fig. 1. The log data was moved to HDFS, close to the
Features (Sec. III-C): Features need to be generated in processing, and after cleaning and format adjustment, the
order to use statistical and machine learning algorithms. data was consumed for analytics. This method of ingestion
Wepresenta”smartgroupBy”methodtogeneratefeatures provided an isolated environment for exploration indepen-
efficiently in parallel at scale using Apache Spark [12]. dent from the production pipeline. This approach worked
Model (Sec. IV-A): We use both statistics and machine well for exploring relatively small samples; however, as the
learning to create our models and analyze incoming data. volume of the log data grew, it became infeasible to move
Causality (Sec. IV-B): Wedofeatureisolationtodetectthe the logs to HDFS.
root cause of a problem, such as a failing component. Therefore, we decided to switch to a pipeline based on
Visuals (Sec. IV-C): Finally we present the root cause of cloud services shown in Fig. 2. In this approach we collect
the problem to the operator through reports, dashboards andstreamthelogsdirectlytoIBMCOS,storingthemclose
(e.g., Grafana [13]), or by notifications (e.g., Slack [14]). to the collection points and organizing them in Parquet, and
166
Authorized licensed use limited to: University of Guelph. Downloaded on August 10,2023 at 08:59:22 UTC from IEEE Xplore. Restrictions apply.
Fig.1. Pipelineforprocessingoperationallogs
Fig.2. Cloud-basedpipeline
alsomovetheprocessingclosetothedatacreation.Notethat
while this cloud-based approach is closer to a full data-lake compression and encoding schemes. The schema is embed-
pipeline,itisstilladevelopmentenvironment.Thisapproach dedinthedataitself,soitisaself-describingformat.These
isusefulduringdevelopmentasitprovidesasandboxwitha featuresmakeitefficienttostoredatainParquetasopposed
”cached” snapshot of the data. As demonstrated in the next torow-orientedschemeslikeCSVandTSV.Fig.3showsthe
section,thisapproachallowsexplorationofanalyticmethods resultsofourexperimentsontheeffectivenessofstoringour
in a repeatable manner without having to re-run the entire logsamplesinParquet.Thefigureshowsforourcase,when
data pipeline. no explicit options are set and Spark uses default snappy
compression,storinglogsinParquetratherthanleavingthem
B. Curation inJSONformatinaformofcompressedESindexes,reduced
It is not enough to collect the data to make it useful and the size by more than 10 times.
meaningful. The ingested data must be cleaned, formatted In addition, the columnar nature of Parquet enables effi-
and organized in order to make it useful for data scientists cient querying since most queries involve few columns and