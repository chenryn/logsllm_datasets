; constant pool entry, embedded
; in non-readable memory
trampoline mechanism used for forward pointers ensures trans-
parent interoperability with unprotected code while XOR en-
cryption does not without further instrumentation, since legacy
code would not know that forward pointers are encrypted.
In practice, function calls and returns occur more frequently
than forward pointer dispatches, so optimizing return address
protection is far more important.
1) Exception Handling: Itanium ABI exception handling
uses stack unwinding and matches call sites to exception index
tables. Since our return-address hiding scheme encrypts call
site addresses on the stack, stack unwinding will fail and break
exception handling. All indirect disclosure protections which
hide return addresses from an attacker will be similarly in-
compatible with stack unwinding, which depends on correctly
mapping return addresses to stack frame layout information.
We modiﬁed LLVM’s stack unwinding library implementa-
tion libunwind to handle encrypted return addresses. Since
the ﬁrst return address is stored in the link register, the stack
unwinder can determine the ﬁrst call site. From the call site,
the stack unwinder is able to determine the function and read
the XOR key that was used to encrypt the next return address
using a whitelisted memory load. By recursively applying this
approach, the unwinder can decrypt all return addresses until
it ﬁnds a matching exception handler. This approach requires
that we trust that the unwinding library does not contain a
memory disclosure bug.
E. Fine-Grained Code Randomization
LR2 does not depend on any particular type of code
randomization and can be combined with most of the di-
versifying transformations in the literature [29]. We choose
to evaluate our approach using a combination of function
permutation [27] and register-allocation randomization [14, 39]
as both transformations add very little run-time overhead.
As Backes and Nürnberger [3] point out, randomizing the
layout at the level of code pages may help allow sharing of
code pages on resource-constrained devices. Note that had we
only permuted the function layout, adversaries may be able to
harvest trampoline pointers and use them to construct an attack
without knowing the code layout. Because these pointers only
target function entries and return sites (instructions following a
6
call) this constrains the available gadgets much like a coarse-
grained CFI policy would. Therefore, we must assume that
gadget-stitching attacks [19, 24] are possible. However, stitch-
ing gadgets together is only possible with precise knowledge
of how each gadget uses registers; register randomization
therefore helps to mitigate such hypothetical attacks.
F. Decoupling of Code and Data Sections
References between segments in the same ELF object
usually use constant offsets as these segments are loaded
contiguously. To prevent an attacker from inferring the code
segment base address in LR2, we replace static relocations
that are resolved during link time with dynamic relocations.
This allows us to load the segments independently from
each other, because the offsets are adjusted at
load time.
By entirely decoupling the code from the data section we
prevent the attacker from inferring any code addresses from
data addresses. As a convenient side-effect of this approach,
code randomization is possible without the need for position-
independent code (PIC). PIC is necessary to make applications
compatible with ASLR by computing addresses relative to the
current program counter (PC). Since we replace all PC-relative
offsets with absolute addresses to decouple the code and data
addresses, we observed slightly increased performance relative
to conventional, ASLR-compatible position-independent exe-
cutables at the cost of slower program loading.
G. Implementation in LLVM
We implemented our proof-of-concept transformations for
LR2 in the LLVM compiler framework. Our approach is not
speciﬁc to LLVM, however, and is portable to any compiler
or static rewriting framework. However, access to compile-
time analysis and the compiler intermediate representation (IR)
made our implementation easier. In particular, the mask hoist-
ing optimization described previously is easier at compile time,
but not impossible given correct disassembly and rewriting.
the type of value that
Since blindly masking every load instruction is expected to
incur a high performance overhead due to the high frequency
of load instructions, we take a number of steps to reduce
the number of necessary mask instructions. LLVM annotates
memory instructions such as loads and stores with information
about
is loaded. We can use this
information to ensure that load masking is not applied to
loads from a constant address. Such loads are used to access
jump table entries, global offset table (GOT) offsets, and other
constants such as those in the constant pool. These loads
account for less than 2% of all
load operations in SPEC
CPU2006, so this optimization has a small impact.
LLVM-based SFI implementations (e.g., Sehr et al. [47])
operate purely on the machine instructions late in the back-
end, roughly corresponding to rewriting the assembly output
of the compiler. This makes the insertion of fault isolation
instrumentation easier, but misses opportunities for additional
optimization that is speciﬁc to our load-masking techniques.
In order to hoist the masking of potentially unsafe addresses to
their deﬁnition and avoid redundant re-masking, we leverage
static analysis information about the program available earlier
in the compiler pipeline. Speciﬁcally, we begin by marking
unsafe address values while the program values are still in
static single assignment (SSA) form [16]. This allows us
to easily ﬁnd the deﬁnition of address values used by load
instructions, and mask these values. Since stack spilling takes
place after this point in the compilation, we must be careful to
remask any source addresses restored from the stack, since the
attacker may have modiﬁed these values while on the stack. In
particular, we add markers to values that we mask while the
program representation is still in SSA form. During register
allocation, we check if marked values are spilled to memory.
In the case of spills, we insert a masking instruction when
restoring this value from the untrusted stack.
As in Native Client (NaCl) [47], it is necessary to prevent
the compiler from generating load instructions using both a
base and offset register (known as register-register addressing),
to be sure that masking will properly restrict the resulting
addresses. We modify the LLVM instruction lowering pass,
where generic LLVM IR is converted to machine-speciﬁc
IR, to prevent register-register addressed instructions. Instead,
we insert a separate add instruction to compute the effective
address. We make an exception if the load is known to be safe
(e.g., a jump table load).
Finally, we insert return address protection instrumentation,
stack pointer checks, and trampolines for forward code pointers
during compilation as described in the previous sections.
H. Full LR2 Toolchain
1) Code-Data Separation: By masking all load addresses
we effectively partition the memory into a readable and unread-
able section. Our fully-ﬂedged prototype system uses a slightly
modiﬁed Linux kernel and dynamic loader to separate the
process memory space into readable and unreadable sections
(see Figure 2 for an overview of this separation). The kernel
and dynamic loader normally load entire ELF objects contigu-
ously. Data segments are usually loaded consecutively above
the corresponding module’s code. In LR2, however, readable
segments are placed exclusively in the lower 2GiB region of
the process address space, while unreadable (code) segments
must be placed in the higher 2GiB region. Consequently, this
requires ELF objects to be split. We applied small patches to
the Linux kernel (121 LoC) and musl dynamic loader (196
LoC) to load each ELF segment into the proper area.
is mapped low enough in memory that
Furthermore, we modiﬁed the usual kernel memory map-
ping mechanism to comply with our memory layout restric-
tions. By passing an internal ﬂag to mmap, an application can
specify which memory region the requested memory must be
allocated in. This allows the loader to ensure that a program’s
data segment
the
corresponding executable segment lies between 0x80000000
and 0xC0000000 which is where reserved kernel memory
begins. Finally, our patch ensures that memory areas allocated
by the kernel (e.g., stacks and heaps) are in the readable region.
We also needed to slightly modify the linker to prepare
an executable for use with LR2 memory layout. Speciﬁcally,
we patched the gold linker to not mark executable sections as
readable4 and to assign these sections to high addresses. This
type of patch is needed for all XoM solutions, since current
4Note that the memory permission execute normally implies readable due
to the lack of hardware support
linkers mark executable segments with read-execute, rather
than execute-only permissions. Additionally, we added linker
support for 32-bit offsets in Procedure Linkage Table (PLT)
entries, which comes at the cost of one additional instruction
per PLT entry. This is necessary because the PLT (unreadable
memory) refers to the Global Offset Table (GOT) (readable
memory), and therefore might be too far away for the 28-bit
address offset previously used.
2) Libraries: For memory disclosure resilience, all code in
an application needs to be compiled with LR2, including all
libraries. Since the popular C standard library glibc does
not compile with LLVM/Clang, we tested our implementation
with the lightweight replacement musl instead. It includes a
dynamic loader, which we patched to support our code layout
with the same approach as applied to the kernel. We use
LLVM’s own libc++ as the C++ standard library, since the
usual GNU libstdc++ depends on glibc and GCC.
V. PERFORMANCE EVALUATION
We evaluate the performance of LR2 using the CPU-
intensive SPEC CPU2006 benchmark suite, which represents
a worst-case, CPU-bound performance test. We measure the
overall performance as well as the impact of each technique
in our mitigation independently to help distinguish the various
sources of overhead. In addition we measured the code size
increase of our transformations, since code size is an important
factor in mobile deployment. Overall, we found that with all
protections enabled, LR2 incurs a geometric mean performance
overhead of 6.6% and an average code size increase of 5.6%.
We summarize the performance results in Figure 4. Note that
these measurements include results for the hmmer and soplex
benchmarks, which are known to be very sensitive to alignment
issues (±12% and ±6%, respectively) [34].
We want to measure the impact of LR2 applied to whole
programs (including libraries), so we compile and protect a C
and C++ runtime library with our modiﬁcations for use with
the SPEC suite. Since the de-facto standard libraries on Linux,
glibc and libstdc++, don’t compile with LLVM/Clang,
we use musl and LLVM’s own libc++ instead. We extended
the musl loader to support our separated code and data layout.
The perlbench and namd benchmarks required small
workarounds since they contain glibc/libstdc++ speciﬁc
code. h264ref on ARM fails for unknown reasons when
comparing the computation result, both for the unmodiﬁed and
the LR2 run; since it completes the computation we include the
running time nonetheless. Finally, the stack unwinding library
used by LLVM’s libc++ fails with omnetpp, so we exclude
it from all benchmark measurements. We report all measure-
ments as the geometric mean over all other SPEC CPU2006
benchmarks. All measurements are from a Chromebook model
CB5-311-T6R7 with an Nvidia Tegra Logan K1 System-on-
Chip (SoC), running Ubuntu 14.04 with Chromium OS’s Linux
3.10.18 kernel.
A. Forward-Pointer Hiding
We measured impact of forward-pointer hiding, which in-
troduces an additional direct jump instruction for each indirect
call. We found that this transformation resulted in an overhead
of less than 0.3% on average over all benchmarks, with a
maximum overhead of 3%.
7
)
%
(
n
w
o
d
w
o
l
S
e
c
n
a
m
r
o
f
r
e
P
20
15
10
5
0
-5
-10
-15
-20
p erlb e n c h
Pointer Hiding
Restricted Register-Register Addressing
Software XoM
Code and Data Section Decoupling
Full LR2
b zip 2
g c c
m cf
k
m
g o b
m er
m
h
m
sje n g
lib q u a ntu
h 2 6 4ref
k
m
astar
x ala n c b
m ilc
d
n a m
d e alII
s o ple x
p o vra y
m
lb
s p hin x 3
M e a n
G e o
Figure 4: LR2 overhead on SPEC CPU2006. We use the performance of unprotected position independent binaries as the baseline.
B. Return-Address Hiding
Return-address hiding requires one extra load and XOR at
the entry of each function that spills the link register. At each
function return it replaces the return instruction with one load,
one XOR and one branch. We found that this instrumentation
added an overhead of less than 1% on average, with a
maximum overhead of 3% over the baseline time. Combining
forward-pointer hiding and return-address hiding, we measured
an average overhead of 1.4%. We show the combined results
in Figure 4, labeled Pointer Hiding. This overhead compares
favorably to Readactor’s [14] 4.1% overhead for full code
pointer hiding, since our return-address hiding scheme does
not require expensive return trampolines for each call site.
For both forward-pointer and return-address hiding, we
noticed that a few benchmarks ran slightly faster with the
instrumentation than without. We attribute this variance to
measurement error and slight code layout differences resulting
in different instruction cache behavior.
C. Register-Register Addressing Scheme Restrictions
An important feature of the ARM instruction set is register
offset addressing for array or buffer loads. As described in
Section IV, we have to disable this feature in LR2, since
it interferes with XoM address masking. We measured the
overhead that this restriction incurs by itself and found that
restricting register addressing schemes incurs 2.3% overhead
on average and a 9% worst-case overhead on the gobmk
benchmark. Benchmarks like hmmer, bzip2 and sjeng are
affected because a large portion of the execution time is spent
in one hot loop with accesses to many different arrays with
varying indices.
D. Software XoM
The last component to analyze individually is our XoM
instrumentation—masking unsafe loads. We found that, after
applying the optimizations outlined in Section IV-B, software-
enforced XoM results in an overhead of 6.6% on average
(labeled Software XoM in Figure 4), with a maximum overhead
of 16.4% for one benchmark, gobmk. We attribute this primar-
ily to to data dependencies introduced between the masking
and load instructions, as well as hot loop situations such as
mentioned above.
E. Code and Data Decoupling
Normally the code and data segments of a program have
a ﬁxed offset
in memory, allowing PC-relative addressing
of data. However, this also allows an attacker to locate the
beginning of the code segment from any global data address.
As we describe in Section IV-F, we decouple the location of
the data segment from the code segment, allowing the loader