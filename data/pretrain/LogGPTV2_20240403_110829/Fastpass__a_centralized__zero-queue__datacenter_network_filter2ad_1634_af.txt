and Doug Weimer for their generous assistance with the Facebook
infrastructure, Garrett Wollman and Jon Proulx at MIT CSAIL for
their help and efforts in setting up environments for our early ex-
periments, and David Oran of Cisco for his help. We thank John
Ousterhout, Rodrigo Fonseca, Nick McKeown, George Varghese,
Chuck Thacker, Steve Hand, Andreas Nowatzyk, Tom Rodeheffer,
and the SIGCOMM reviewers for their insightful feedback. This
work was supported in part by the National Science Foundation grant
IIS-1065219. Ousterhout was supported by a Jacobs Presidential
Fellowship and a Hertz Foundation Fellowship. We thank the indus-
trial members of the MIT Center for Wireless Networks and Mobile
Computing for their support and encouragement.
11. REFERENCES
[1] Packet processing on intel architecture.
http://www.intel.com/go/dpdk.
[2] Intel 64 and IA-32 Architectures Optimization Reference
Manual. Number 248966-029. March 2014.
[3] M. Ajmone Marsan, E. Leonardi, M. Mellia, and F. Neri. On
the stability of input-buffer cell switches with speed-up. In
INFOCOM, 2000.
[4] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable,
Commodity Data Center Network Architecture. In SIGCOMM,
2008.
[5] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic Flow Scheduling for Data Center
Networks. In NSDI, 2010.
[6] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel,
B. Prabhakar, S. Sengupta, M. Sridharan, C. Faster, and
D. Maltz. DCTCP: Efﬁcient Packet Transport for the
Commoditized Data Center. In SIGCOMM, 2010.
[7] M. Alizadeh, A. Kabbani, T. Edsall, B. Prabhakar, A. Vahdat,
and M. Yasuda. Less is More: Trading a Little Bandwidth for
Ultra-Low Latency in the Data Center. In NSDI, 2012.
[8] M. Alizadeh, S. Yang, S. Katti, N. McKeown, B. Prabhakar,
and S. Shenker. Deconstructing Datacenter Packet Transport.
In HotNets, 2012.
[9] T. E. Anderson, S. S. Owicki, J. B. Saxe, and C. P. Thacker.
High-Speed Switch Scheduling for Local-Area Networks.
ACM Trans. on Comp. Sys., 11(4):319–352, 1993.
[10] L. A. Barroso, J. Dean, and U. Holzle. Web Search for a
Planet: The Google Cluster Architecture. IEEE Micro,
23(2):22–28, 2003.
[11] M. Chowdhury, M. Zaharia, J. Ma, M. Jordan, and I. Stoica.
Managing Data Transfers in Computer Clusters with
Orchestra. In SIGCOMM, 2011.
[12] R. Cole, K. Ost, and S. Schirra. Edge-Coloring Bipartite
Multigraphs in O(E logD) Time. Combinatorica, 21(1):5–12,
2001.
[13] J. Dai and B. Prabhakar. The throughput of data switches with
and without speedup. In INFOCOM, 2000.
[14] J. Duato, S. Yalamanchili, and L. Ni. Interconnection
Networks. Morgan Kaufmann, 2003.
[15] A. Elwalid, C. Jin, S. Low, and I. Widjaja. MATE: MPLS
Adaptive Trafﬁc Engineering. In INFOCOM, 2001.
[16] N. Farrington and A. Andreyev. Facebook’s Data Center
Network Architecture. In IEEE Optical Interconnects Conf.,
2013.
[17] N. Farrington, G. Porter, Y. Fainman, G. Papen, and A. Vahdat.
Hunting Mice with Microsecond Circuit Switches. In HotNets,
2012.
[18] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim,
P. Lahiri, D. A. Maltz, P. Patel, and S. Sengupta. VL2: A
Scalable and Flexible Data Center Network. In SIGCOMM,
2009.
[19] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang, V. Gill,
M. Nanduri, and R. Wattenhofer. Achieving High Utilization
with Software-Driven WAN. In SIGCOMM, 2013.
[20] Hong, C. Y. and Caesar, M. and Godfrey, P. Finishing Flows
Quickly with Preemptive Scheduling. SIGCOMM, 2012.
[21] F. Hwang. Control Algorithms for Rearrangeable Clos
Networks. IEEE Trans. on Comm., 31(8):952–954, 1983.
[22] V. Jeyakumar, M. Alizadeh, D. Mazieres, B. Prabhakar, and
C. Kim. EyeQ: Practical Network Performance Isolation for
the Multi-Tenant Cloud. In HotCloud, 2012.
[23] A. Kapoor and R. Rizzi. Edge-coloring bipartite graphs.
Journal of Algorithms, 34(2):390–396, 2000.
[24] N. McKeown. The iSLIP Scheduling Algorithm for
Input-Queued Switches. IEEE/ACM Trans. on Net.,
7(2):188–201, 1999.
[25] N. McKeown, A. Mekkittikul, V. Anantharam, and J. Walrand.
Achieving 100% Throughput in an Input-Queued Switch.
IEEE Trans. Comm., 47(8):1260–1267, 1999.
[26] D. Nagle, D. Serenyi, and A. Matthews. The Panasas
ActiveScale Storage Cluster: Delivering Scalable High
Bandwidth Storage. In Supercomputing, 2004.
[27] R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang,
P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat.
PortLand: A Scalable Fault-Tolerant Layer 2 Data Center
Network Fabric. In SIGCOMM, 2009.
[28] R. Nishtala, H. Fugal, S. Grimm, M. Kwiatkowski, H. Lee,
H. C. Li, R. McElroy, M. Paleczny, D. Peek, P. Saab, et al.
Scaling memcache at facebook. In NSDI, 2013.
[29] P. Ohly, D. N. Lombard, and K. B. Stanton. Hardware
Assisted Precision Time Protocol. Design and Case Study. In
LCI Intl. Conf. on High-Perf. Clustered Comp., 2008.
[30] D. Shah. Maximal matching scheduling is good enough. In
GLOBECOM, 2003.
[31] D. Shah, N. Walton, and Y. Zhong. Optimal Queue-Size
Scaling in Switched Networks. In SIGMETRICS, 2012.
[32] A. Shieh, S. Kandula, A. Greenberg, C. Kim, and B. Saha.
Sharing the Data Center Network. In NSDI, 2011.
[33] R. Takano, T. Kudoh, Y. Kodama, and F. Okazaki.
High-Resolution Timer-Based Packet Pacing Mechanism on
the Linux Operating System. IEICE Trans. on Comm., 2011.
[34] Y. Tamir and H.-C. Chi. Symmetric crossbar arbiters for VLSI
communication switches. IEEE Trans. Par. Dist. Sys.,
4(1):13–27, 1993.
[35] B. C. Vattikonda, G. Porter, A. Vahdat, and A. C. Snoeren.
Practical TDMA for Datacenter Ethernet. In EuroSys, 2012.
[36] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowtron. Better
Never Than Late: Meeting Deadlines in Datacenter Networks.
In SIGCOMM, 2011.
[37] H. Wu, Z. Feng, C. Guo, and Y. Zhang. ICTCP: Incast
Congestion Control for TCP in Data Center Networks. In
CoNext, 2010.
[38] X. Wu and X. Yang. DARD: Distributed Adaptive Routing for
Datacenter Networks. In ICDCS, 2012.
Appendix: Theoretical Properties of the Timeslot
Allocator
System model. We consider a network with N endpoints denoted
by 1, . . . ,N, with each endpoint potentially having requests for any
other endpoint. Time is slotted; in a unit timeslot each endpoint
can transmit at most one MTU-sized packet to any other endpoint
and receive at most one packet from any other endpoint. New
requests arrive at an endpoint i for endpoint j with probability pi j
in each timeslot. An arriving request brings a random number of
packets, distributed according to a distribution Gi j and independent
of everything else.
Let E[Gi j] = gi j. Thus, on average li j = pi jgi j packets arrive
at endpoint i for endpoint j per timeslot. The matrix L = [li j]
denotes the net average data trafﬁc between endpoints arriving in
each timeslot. We assume a non-oversubscribed (also called full
bisection bandwidth) network: the network can support any trafﬁc
where each node is paired to at most one other node per timeslot.
In the above setup, all trafﬁc matrices, L, that can be served by
any system architecture, must be doubly sub-stochastic. That is,
N
Â
k=1
lik < 1, for all i and
N
Â
k=1
lk j < 1, for all j.
(1)
By the celebrated result of Birkhoff and von Neumann, all doubly
stochastic matrices can be decomposed as a weighted sum of permu-
tation matrices (i.e., matchings) with the sum of the weights being
at most 1. Therefore, non-oversubscribed networks can support
all doubly stochastic trafﬁc matrices. A trafﬁc matrix L is called
admissible if and only if r(L) < 1 where, the system load r(L) is
deﬁned as
(2)
r(L) = max
i, j ⇣ N
Â
k=1
lik,
N
Â
k=1
lk j⌘.
Finally, let Qi j(t) denote the total number of packets (potentially
across different requests) waiting to be transferred from endpoint i
to endpoint j at time t. This setup is similar to that used in literature
on input-queued switches [25], enabling us to view the network as a
big input-queued switch with Qi j(t) the Virtual Output Queue sizes.
Main result. The arbiter’s timeslot allocation algorithm of §3 is
equivalent to the following: each queue (i, j) has a “priority score”
associated with it. In the beginning of each timeslot, the arbiter starts
processing queues in non-decreasing order of these priority scores.
While processing, the arbiter allocates a timeslot to queue (i, j) if
the arbiter has not already allocated another packet starting from i or
destined to j in this timeslot. Therefore, at the end of processing the
timeslot, the allocations correspond to a maximal matching between
endpoints in the bipartite graph between endpoints, where an edge
is present between (i, j) if there are packets waiting at endpoint i
destined for j. From the literature on input-queued switches, it is
well-known that any maximal matching provides 50% throughput
guarantees [13, 3]. Building upon these results as well as [30], we
state the following property of our algorithm.
THEOREM 1. For any r < 1, there exists L with r(L) = r such
that for any allocator,
t
Nr
liminf
EhÂ
Qi j(t)i  
Further, let V   1 be such that E[G2
i j]  VE[Gi j] for all i, j (bounded
Gi j); if we allow the Fastpass arbiter to schedule (as well as transmit
through the network) twice per unit timeslot,5 then the induced
average queue-size
2(1  r)
(3)
i j
.
limsup
t
EhÂ
i j
Qi j(t)i 
Nr(r +V )
2(1  r)
.
(4)
Proof Sketch. To establish the lower bound (3) for any scheduling
algorithm, it is sufﬁcient to consider a speciﬁc scenario of our setup.
Concretely, let the trafﬁc matrix be uniform, i.e., L = [li j] with
li j = r
(N 1) for all i 6= j and 0 when i = j; pi j = 1 for all i 6= j; and
let Gi j be Poisson variables with parameter li j. The network can
be viewed as unit-sized packets arriving at each endpoint according
to a Poisson arrival process of rate r and processed (transferred by
the network) at unit rate. That is, the queue-size for each endpoint j
is bounded below by that of an M/D/1 queue with load r, which
is known to be bounded below by
(2(1 r)) [31]. Therefore, the
network-wide queue-size is bounded below by
To establish an upper bound, we use the fact that the algorithm
effectively achieves a maximal matching in the weighted bipartite
graph between endpoints in each timeslot. Given this fact, and under
the assumption that Fastpass can schedule as well as transfer data
at twice the speed, this is effectively a speedup of 2 in the classical
terminology of switch scheduling. Therefore, for the Lyapunov
function (cf. [13]),
Nr
(2(1 r)) .
r
L(t) = Â
i, j
Qi j(t)[Qi·(t) + Q· j(t)]
it can be shown using calculations similar to [30] that
i, j
Qi j(t)  + 2Nr2 + 2V Nr.
E[L(t + 1)  L(t)|Q(t)]  4(r   1) Â
Telescoping this inequality for t   0 and using the fact that the
system reaches equilibrium due to ergodicity, we obtain the desired
result. ⌅
Implications. Equation (3) says that there is some (worst case) input
workload for which any allocator will have an expected aggregate
Nr
queue length at least as large as
2(1 r) . Equation (4) says that with a
speedup of 2 in the network fabric, for every workload, the expected
aggregate queue length will be no larger than Nr(r+V )
2(1 r) . Here V is
effectively a bound on burst size; if it is small, say 1, then it is within
a factor of 2 of the lower bound! There is, however, a gap between
theory and practice here, as in switch scheduling; many workloads
observed in practice seem to require only small queues even with no
speedup.
5Equivalent to having to double the network fabric bandwidth.