title:Warped-Shield: Tolerating Hard Faults in GPGPUs
author:Waleed Dweik and
Mohammad Abdel-Majeed and
Murali Annavaram
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Warped-Shield: Tolerating Hard Faults in GPGPUs
Waleed Dweik‚àó, Mohammad Abdel-Majeed‚àó, Murali Annavaram
Ming Hsieh Department of Electrical Engineering
University of Southern California
{dweik, abdelmaj, annavara}@usc.edu
Abstract‚ÄîGraphics processing units (GPUs) are rapidly
becoming the parallel accelerators of choice to run general
purpose applications. GPUs that run general purpose appli-
cations are termed as GPGPUs. Many mission-critical and
long-running scientiÔ¨Åc application are being ported to run
on GPGPUs. These applications demand strong computational
integrity. GPGPUs, like many other digital components, face
imminent reliability threats due to technology scaling. Of
particular concern is the inÔ¨Åeld hard faults that are persistent
and irreversible. GPGPUs comprise of dozens of streaming
processors where each streaming processor employs tens of
execution units, organized as single instruction multiple thread
(SIMT) lanes to deliver massive parallel computational power.
In this paper we exploit the massive replication of SIMT
lanes to tolerate inÔ¨Åeld hard faults. First, we introduce thread
shufÔ¨Çing to reroute threads, originally mapped to faulty SIMT
lanes, to idle healthy lanes. Thread shufÔ¨Çing is insufÔ¨Åcient
when the number of healthy SIMT lanes is fewer than the
number of active threads. To broaden the reach of thread
shufÔ¨Çing, we propose dynamic warp deformation to split the
warp into multiple sub-warps; each sub-warp uses fewer
SIMT lanes thereby providing more opportunities to avoid
using a faulty SIMT lane. Finally, we propose warp shufÔ¨Çing
which exploits non-uniform degradation of different streaming
processors by scheduling a warp to a streaming processor
that requires fewer warp splits. Hence, warp shufÔ¨Çing helps
to reduce the performance overhead associated with dynamic
warp deformation. By deploying the proposed techniques, we
can tolerate the worst case scenario of having up to three
hard faults per four SIMT lane cluster with at most 36%
performance degradation.
Keywords-Single instruction multiple threads (SIMT); thread
shufÔ¨Çing; warp deformation; warp shufÔ¨Çing;
I. INTRODUCTION
Recently, general purpose applications with massively par-
allel computation demands are relying on graphics process-
ing units (GPUs) as the computational substrate. GPUs that
provide support for general purpose computations are called
GPGPUs. GPGPUs provision hundreds or even thousands
of execution units organized as single instruction multiple
thread (SIMT) lanes. In fact, 68% of the chip area is
dedicated to SIMT lanes in current GPGPUs [15]. The com-
putation is parallelized across multiple SIMT lanes where
a group of SIMT lanes execute the same instruction but
on different input operands. This simpliÔ¨Åed control allows
GPGPUs to achieve high performance at low power. Due to
‚àó
Dweik and Abdel-Majeed made equal contributions
their high performance per watt many mission-critical and
long-running scientiÔ¨Åc application are being ported to run on
GPGPUs. These applications demand strong computational
integrity.
GPGPUs, like many other digital systems, are becoming
more vulnerable to different types of inÔ¨Åeld faults with the
ever decreasing technology nodes. The most critical type
of operational faults is inÔ¨Åeld hard faults because they are
persistent and irreversible. In this paper we target hard faults
in the SIMT lanes. Given the vast number of SIMT lanes,
even a single hard fault in one SIMT lane can lead to
wrong computations. Since massive parallel execution is
the primary focus of GPGPUs, SIMT lanes are the most
extensively used resources as most instructions are of integer
and Ô¨Çoating-point types. Note that most of the chip area
outside of the SIMT lanes is occupied by memory structures,
such as register Ô¨Åles, which are usually protected with
error correction codes. For instance, register Ô¨Åles in Nvidia
GPGPUs are SECDED protected [4].
Recent research work proposed hardware [13] and soft-
ware techniques [9] to detect operational faults in GPGPUs
but did not provide solutions to tolerate these faults. Fault
tolerance is the ability to continue correct execution, albeit
at a reduced performance, despite the existence of faults. As
a complement to these prior works, in this paper, we propose
light weight fault tolerance schemes that dynamically adapt
the thread execution in GPGPUs based on the available
healthy SIMT lanes. The proposed schemes do not require
any software intervention and are transparent to the micro-
architectural blocks surrounding the SIMT lanes, such as the
fetch logic, register Ô¨Åle, and caches.
The contributions of this work are as follows:
Improving GPGPUs resiliency to hard faults using
thread shufÔ¨Çing: This technique tolerates hard faults in
the SIMT lanes by rerouting threads scheduled to run on
faulty lanes to idle healthy lanes. Thread shufÔ¨Çing exploits
the under-utilization of the SIMT lanes to repurpose the idle
lanes for fault tolerance.
Using dynamic warp deformation when thread shuf-
Ô¨Çing is insufÔ¨Åcient: Depending solely on thread shufÔ¨Çing
to tolerate hard faults in the SIMT lanes is insufÔ¨Åcient when
the number of active threads exceeds the number of healthy
lanes. To tackle this problem, we propose dynamic warp
deformation which divides the original warp into multiple
978-1-4799-2233-8/14 $31.00 ¬© 2014 IEEE
978-1-4799-2233-8/14 $31.00 ¬© 2014 IEEE
978-1-4799-2233-8/14 $31.00 ¬© 2014 IEEE
DOI 10.1109/DSN.2014.95
DOI 10.1109/DSN.2014.95
DOI 10.1109/DSN.2014.95
431
431
431
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 
sub-warps, such that each sub-warp has no more active
threads than the number of healthy lanes.
Reducing the performance overhead associated with
warp deformation using inter-SP warp shufÔ¨Çing: GPG-
PUs comprise of multiple streaming processors (SPs) and
each of these streaming processors may suffer non-uniform
degradation. In cases where the fault maps of the SPs are
asymmetric, we propose inter-SP warp shufÔ¨Çing which uses
a scheduling technique to assign a warp to a SP that is best
suited for that warp‚Äôs computational needs. For instance, if
a warp has 12 active threads and one SP has 12 healthy
lanes and the second SP has fewer than 12 healthy lanes,
this technique schedules the warp to the SP with 12 healthy
SIMT lanes, thereby avoiding warp deformation.
The rest of the paper is organized as follows: Sec-
tion II provides background and motivational information.
Sections III and IV discuss the proposed techniques and
the required architectural support. Section V presents the
simulation methodology and results. We discuss the related
work in section VI and conclude in section VII.
II. BACKGROUND AND MOTIVATION
A. GPGPU Baseline Architecture
This section provides an overview of the baseline GPGPU
architecture used in this paper. We will use a Fermi-like ar-
chitecture [3] (e.g. Nvidia GTX480) as the baseline GPGPU
model . A Fermi GPGPU chip consists of multiple streaming
multiprocessors (SMs). Fig. 1 shows the main pipeline
stages within a single SM. Each SM has tens of execution
resources, which can be subdivided into special function
units (SFUs), load/store units (LD/ST), integer (INT) and
Ô¨Çoating-point (FP) units. Each SIMT lane consists of one
INT and one FP unit.
Every SM contains 32 double clocked SIMT lanes divided
into two groups called streaming processors (i.e. SP0 and
SP1 in the Ô¨Ågure). Thus, each streaming processor can
execute 32 threads every cycle. Four consecutive SIMT lanes
are grouped together to form a SIMT cluster. This cluster
implementation is used in existing commercial GPGPUs
to reduce the complexity of data forwarding from a wide
register Ô¨Åle to the SIMT lanes [12].
Each SM uses SIMT execution model [3] which allows
the lanes within one SP to share a single program counter
and execute the same instruction but on different data ele-
ments concurrently. The SIMT execution model is supported
using the notion of warps. A warp is the smallest scheduled
unit of work in GPGPUs and it consists of up to 32 parallel
threads that execute the same instruction on different input
operands values. Multiple warps within the same program
are grouped together into one cooperative thread array (CTA)
which is assigned to one SM for execution. In GTX480, each
SM can accommodate 48 warps (i.e. total of 1536 threads).
Warp Scheduler: Each SM has its own warp scheduler.
The scheduler extracts warps from the instruction buffer



!
	
"!
#%
&
%

%

%

%

%

%

%

%



 
 
 
 
 
 
 
 
 SFU      LD/ST        SP0            SP1 
 
 
 
 
 
 
 
 
%

%

%

%

%

%

%

%



SIMT Lane  
	





!!!
SIMT Cluster  
Figure 1: Baseline Nvidia GTX480 details
according to the scheduling algorithm. When the input
operands of the warp instruction to be scheduled are not
ready, due to read-after-write (RAW) data hazards,
the
scheduler skips this warp instruction and checks the next
warp in the instruction buffer. If the input operands of
a warp instruction are ready,
the scheduler assigns the
warp instruction to an operand collector unit and sends
the operands read requests to the register Ô¨Åle. The operand
collector unit is simply a staging buffer where values read
from the register Ô¨Åle are temporarily stored.
The baseline architecture in our work uses the two level
scheduler [12] which divides the warps into pending and
active warps. The scheduler extracts the warps from the
active warp queue in a round robin fashion and up to two
warps are scheduled every cycle.
Warp Issue: Once the input operands are gathered from
the register Ô¨Åle into the operand collector, the warp instruc-
tion is then sent to the issue queue where it waits to be issued
to the appropriate execution unit. The issue logic checks for
structural hazards on the corresponding execution unit and
result bus before issuing the warp. In the case of a conÔ¨Çict,
the warp is stalled in the issue queue. The issue logic also
determines which streaming processor (SP0 or SP1) will
execute the warp.
B. Resource Utilization Imbalance
The SIMT lanes within the same streaming processor (SP)
and across different SPs have variable utilization behavior.
This variation is due to two reasons: branch/memory diver-
gence phenomena, and insufÔ¨Åcient application parallelism.
Branch divergence occurs when the current warp instruction
is a branch and some of the warp‚Äôs threads diverge to the
‚Äùtaken‚Äù path while others diverge to the ‚Äùnot taken‚Äù path. As
a result, the threads on the ‚Äùtaken‚Äù path and the ‚Äùnot taken‚Äù
path are scheduled over different cycles. Each warp has a
32-bit active mask to indicate which SIMT lanes are going
to be active (bit is ‚Äô1‚Äô) and which SIMT lanes are going
to be idle. Memory divergence happens when the current
warp instruction is a memory-access instruction and some
of the warp‚Äôs threads hit in the cache while others miss in
the cache. The threads which hit and the ones which miss
are scheduled over different cycles.
Fig. 2 shows the percentage of warps with 1, 2, 3,...
32 active threads for different benchmarks selected from
432432432
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 



.





:

4
7
1
.


1

1
.
+

4







1

+
.



(


















































(
Figure 2: Threads Activity Breakdown

















Figure 3: Average SIMT Lane Utilization
the ISPASS [5], Parboil [2], and Rodinia [8] benchmark
suites. The experimental setup is described in detail later in
section V. The divergence phenomena result in fewer than
32 threads being active for many applications. Note that the
same observation has been made in many prior studies [13]
[14] and was exploited for error detection and power savings.
However, we exploit this observation for error correction.
We also monitored the utilization of the SIMT lanes within
a single SP across the same benchmarks. Fig. 3 plots
the average utilization for each SIMT lane measured as
the percentage of time during which the respective SIMT
lane is executing some instruction. The x-axis represents
the SIMT lane index. As shown, lanes experience variable