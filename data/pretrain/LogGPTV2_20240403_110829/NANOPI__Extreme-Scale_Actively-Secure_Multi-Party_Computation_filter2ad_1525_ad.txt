tion with the wire management problem, this poses new scalability
challenges. We address these challenges through novel combination
of static and dynamic program instrumentation techniques, which
we find most interesting.
The high level idea is that, given a computation specified as an
imperative program, we first apply a source-to-source transforma-
tion to insert appropriate function calls for wire management and
staged gate execution. As a result, when the statically instrumented
program runs, it behaves like running the original WRK except
that it also collects runtime information (such as wire-connection,
gate and wire counts, etc. that are not available at compile-time) to
automatically batch gate execution in stages.
Aligning aANDs with Wire-permutation-bits. Let α, β be the two
input-wires of an AND gate and λα , λβ be the permutation bits
on the input-wires. Since the aANDs (used to garble AND gates)
and the abits (used as wire-permutation-bits) are precomputed in-
dependently, in step (4a) of Figure 14, we must ensure the values
of the secretly-shared permutation bits λα , λβ are consistent with
those returned by FaAND. (We invite the readers to read Section 2.2
for reasons why the bits need to be consistent.) As was explained
in Section 4.1, this incurs an extra round. To alleviate the impact
of round latency due to the alignment, it is important to batch
many AND gates together to share a single network roundtrip.
On the other hand, the space available to store the per-gate in-
formation needed for garbling will limit the number of ANDs to
be processed in a batch. Finally, this alignment process has to be
function-dependent to properly handle wire-splits. (In contrast,
Pool-JIMU has a constant-round wire-soldering step but can be
trivially done in a function-independent way, thus don’t require
the advanced programming techniques as WRK protocols do.)
Managing the Wires. Due to space constraint, it is infeasible to
fully unroll a program to obtain all the intermediate wires and their
connection information. However, WRK’s online circuit phase does
need information of all wires to complete efficiently in constant
rounds. Our goal here is to run WRK in constant space without
overly penalize speed. But is it possible to finish a long computation
while keeping only constantly many wires?
A key idea to answer this question is to leverage the program
representation of the computation. In practice, useful computations
almost always have a constant-size representation no matter how
long they need to run. In fact, this idea was used by Zhu et al.
to reduce its space complexity: Pool-JIMU only maintains a small
set of wires corresponding to the set of program variables visible
at current point of runtime execution. There, wires are created
(and destructed, resp.) as the execution enters (and exits) their
corresponding variables’ defining scopes.
Unfortunately, this idea does not directly work with WRK. Take
the following simple function (1-bit multiplexer) as an example.
mux1(x, y, c) {
t (cid:66) x ⊕ y;
t (cid:66) c ∧ t;
return t ⊕ y;
}
Should the old strategy be used, several issues arise, evidencing
new programming challenges:
(1) A scope can be small, e.g., function mux1’s scope contains only
two binary gates. If the wires corresponding to variables t, c, etc.
are to be destructed on exiting their scope, then the AND gates
inside the scope have to be executed (before its input-wires
are destructed), thus incurring constant rounds per scope-exit.
Therefore, for small scopes, freeing up wires upon exiting their
defining scopes limits the batch size and incurs more rounds.
(2) Some statements, like “t (cid:66) c∧ t” in the example above, cannot
be directly supported without additional treatment. This is
because the execution of some ANDs has to be delayed (so
that they are batch-executed with other ANDs). Therefore, the
output of an AND gate cannot overwrite input-wires of any
ANDs (including its own) whose execution is currently delayed.
(3) Copy assignments like “x (cid:66) y” cannot be executed as usual as
in JIMU-Pool: the wire associated with variable y may be de-
structed at some point of exiting its scope whereas the variable
x may have a longer lifespan, e.g., when x is not a local variable
but y is a local variable.
(4) General compositions of binary operations such as “x ∧ y ∧ z”
and “x ⊕ y ∧ z” cannot be directly executed as with Pool-JIMU,
simply because gate execution are delayed and batched, and
that in-place wire assignment is not possible.
Our Solution. We propose a novel program execution model that
addresses these new challenges. Comparing to the normal stack-
based program execution, our new execution model preserves the
final outcome and space-efficiency, but circumvents the inability
of normal stack-based model in efficiently supporting WRK. The
high-level key ideas can be informally described as a list of rules:
(1) Every gate will run after certain delay. That is, a gate is pro-
cessed as pushing the gate into a queue, marking it ready to
execute once the next batch (called stage) is triggered.
(2) A stage is automatically and dynamically triggered when there
is not enough memory to batch more gates.
(3) Every wire will be marked as destructable (but not actually
destructed) when exiting its static scope. The actual destruction
occurs automatically after executing every stage.
(4) An assignment always implicitly creates a new wire, and binds
the target variable with the new wire.
(5) Before each assignment, the previous wire associated with the
target variable needs also to be marked as destructable, while
the actual destruction happens only after the current stage is
executed.
(6) All expressions must be translated to three-address assignments.
Why would these transformation rules resolve our challenge?
And even if they do, wouldn’t it be cumbersome and error-prone
for programmers to manually enforce them for every specific appli-
cation program? We answer both questions by developing a static
program rewriter (as a standalone executable) and a dynamic pro-
gram instrumenter (as a collection of functions to be linked with
user’s application code) for a subset of C, then formally prove that
executions of the transformed programs must produce identical
outcome as the original programs but only consume small space.
Out of the six rules above, rule (1) and rule (6) are implemented
by the static rewriter; rule (2) is implemented by the dynamic in-
strumenter; and rule (3), rule (4), rule (5) are jointly realized by
both. Leveraging loops and recursion, our language is capable of
specifying many useful boolean circuits such as AES, edit distance
and logistical regression in a highly compact way. As a result, our
toolchain is able to completely automate the ideas to efficiently run
WRK for circuits of arbitrary size.
4.2.2 The Preparation Phase. As we explained earlier, both Πabit
and ΠaAND in the preparation phase are plagued by the space-round
dilemma. We first show that naïvely converting WRK’s Πabit to a
space-efficient variant can introduce security vulnerabilities. Then
we explain ideas to generate abits in an efficient and scalable way
without compromising security.
b
Attacking Naïvely Scaled Πabit. The basic idea to scale up Πabit
is to break the constant-round, single-batch of ℓ abits generation
process into ℓ/k batches, each producing k abits (k is set based
on available resource). However, realizing this by naïvely calling
WRK’s Πabit repetitively invites security attacks. Recall that in
WRK abits are generated using random correlated oblivious trans-
fers (RCOT), a sub-protocol that takes no input from the parties
and returns P1 (the RCOT sender) ℓ random correlated message
1 = ∆; and
1≤i ≤ℓ. Note that there
is no guarantee that the same ∆ should be output across different
RCOT calls. Hence, WRK’s proof of security no longer applies. Even
worse, this can actually leave the main secure computation protocol
vulnerable to, what we call, inconsistent-∆ attacks!
pairs(cid:8)(cid:0)mi
returns P2 (the RCOT receiver)(cid:8)(cid:0)b, mi
1≤i ≤ℓ so that ∃∆ ∈ {0, 1}n,∀i, mi
0 ⊕ mi
(cid:1)(cid:9)
(cid:1)(cid:9)
0, mi
1
To see how an inconsistent-∆ attack works, note that with all
but negligible probability, P1 as an RCOT sender will get two values
of the correlation-difference, say ∆ and ∆′, from two calls to RCOT
protocol. So with high probability, in some iterations of step (4a), the
abit rα of a particular aAND is authenticated with ∆′, whereas the
rest of the abits involved in the same aAND call are authenticated
with ∆. By definition, it is easy to verify that aAND will fail if
rα = 1 but succeed if rα = 0. Since everyone knows which abits
are associated with which ∆ values, just by observing whether the
execution fails or not, an attacker learns an abit of its peer’s. Since
a leaked abit can associate with the permutation bit corresponding
to an input-wire carrying another party’s secret input, that party’s
secret input bit will be leaked by observing if a protocol execution
fails!
practice of existing secure computation frameworks [14, 29, 40,
42] which included optimized version of basic circuits to facilitate
non-crypto-expert application developers.
abit
Secure, Scalable ΠScalable
. To avoid the inconsistent-∆ issue, we
propose two minor modifications to WRK’s Πabit: (1) we change
the underlying RCOT protocol so that it takes a predefined ∆ as
input from the sender which is used to form random correlated
messages; and (2) to prevent an adversary from deliberately using
different ∆ in different RCOT batches, we add a consistency check
step at the end of every batch of RCOTs to ensure that identical ∆
values are used across different batches. Our abit protocol, called
, can efficiently produce an arbitrary number of abits using
ΠScalable
constant-space. ΠScalable
is specified in Figure 8.
abit
Fast, Scalable ΠScalable
abit
aAND . We improve the memory-scalability of
cut-and-choosing leaky-AND triples through maintaining a fixed-
size pool of leaky-AND triples. Instead of storing O(|Cf | · B) leaky-
aANDs before randomly grouping every B leaky-aANDs into a
bucket, we can carry out the random grouping always within a
pool of p leaky-aANDs while refilling a used leaky-aAND as soon
as it is marked to be converted into a fully-secure AND triple. Thus,
WRK’s cut-and-choose-based ΠaAND is modified to efficiently run
in constant space using pool-based cut-and-choose. Comparing
to Pool-JIMU, the differences here are: (1) every leaky-aAND is
checked for validity with a constant fault-detection rate 1/2, and
(2) every leaky-aAND can be checked and combined with other
leaky-aANDs to form a fully-secure aAND. As a result listed in [48,
Table 6], it suffices to maintain a pool of 479K leaky-AND triples
to achieve bucket size 3. (Note that the smaller the buckets are,
the faster WRK’s ΠaAND runs.) Our aAND protocol ΠScalable
aAND can
efficiently generate any number of aANDs using constant-space.
aAND is specified in Figure 9.
ΠScalable
4.3 Putting It All Together
We integrated the cryptographic enhancements and PL techniques
mentioned above and provide a complete toolchain, nanoPI, for
non-crypto-experts to develop and execute long-term or extreme-
scale, actively-secure MPC protocols. The high-level workflow of
nanoPI is depicted in Figure 4. Our system consists of the following
components:
• A Static Rewriter. It instruments circuit functions written by
application developers with resource management APIs provided
by our backend. In essence, the static writer extracts program’s
static scope information and passes it to our backend interpreter
to allow improved resource management.
• Cryptographic functions. They realize our improved efficiently
scalable variant of WRK components such as functions abitGen,
aANDGen, and runCircuit, etc.
• Stage functions. These functions are responsible to automat-
ically arrange the gates into different stages, securely execute
the gates respecting their topological order, retain necessary
resource for intermediate values to connect the stages while
recycling others as soon as possible.
• Basic Circuits Library. This is a set of basic circuits frequently
used in building real world applications. We follow the common
User circuits
(*.C)
I
P
o
n
a
n
Static Rewriter
Crypto Functions
(e.g., abitGen)
Stage Functions
(e.g., init, release)
Basic Circuits
(e.g., mux, add)
Instrumented
circuits (*.C)
l
l
e
h
s
-
e
h
t
-
ff
O
r
e
l
i
p
m
o
c
C
Pi
Figure 4: The overview of nanoPI’s workflow.
5 PROTOCOL DETAILS
We divide the formal description of our approach into smaller pieces
to facilitate the proof of security. Section 5.1 describes the enabling
PL techniques for efficiently scaling up WRK’s online phase. Sec-
tion 5.3 presents techniques for space-efficient generation of abits
and aANDs. While our description focuses on the two-party setting,
the ideas naturally generalize to multi-party settings (see Section 6).
5.1 Scalable Authenticated Garbling
We use an idealized subset of C as a compact circuit description
language. We develop (and prove the correctness of) a new program
execution semantics for this subset of C that models the space
requirements of WRK’s authenticated garbling algorithm (Π2pc)
without overly penalizing its performance due to network round-
trips. As a proof-of-concept, we formalize the subset-of-C circuit
description language using the grammar below:
Programs
Variable Declarations
Function Declarations
Blocks
Circuit Variables
Iteration Variables
Operators
Statements
p (cid:70) d1; d2; . . . ; F1; F2; . . . ; b
d (cid:70) bit x | bit[n] x
F (cid:70) f (d1, d2, . . .){b}
b (cid:70) d1; d2; . . . ; s1; s2; . . .
X (cid:70) x | x[i]
i (cid:70) 0 | 1 | 2 . . .
⊙ (cid:70) nand | nor | . . .
s (cid:70) X = true | X = false
| X = X1 ⊙ X2
| repeat i [0..n] s1; s2; . . .
| f (X1, X2, . . .)
A program in this language is a sequence of (global) variable
declarations d1, d2, . . . followed by a sequence of function declara-
tions F1, F2, . . . followed by a block. A block is itself a sequence of
(local) variable declarations followed by a sequence of statements.
Functions are only called for effect in this language. There are two
types of values: bits and arrays of bits of a fixed size. In order to
be able to concisely describe circuits with common structure, the
language includes a limited form of iteration whose bounds must
be known at compile time and arrays to refer to homogeneous
collections of wires. We will use the following program as a small
running example in this section.
1
2
3
4
5
6
7
bit r;
bit [2] rs ;
bit [2] xs ;
f ( bit x) { bit t; t = t ∧ x; r = r ∧ t ;}
// We assume r , rs [0] , rs [1] , xs [0] ,
// and xs [1] were all initialized and
// store secret values .
repeat i [0..2] { f ( xs [i], xs [i ]) ; rs [i] = r ;}
Canonical Semantics. A well-established method for specifying
the formal semantics of languages like our subset of C is via a set-
theoretic denotational model [46]. The details of such a denotational
model can be dramatically simplified if we pre-process programs to
eliminate convenient-for-use but semantically distracting features.
In our case, we use well-understood correctness-preserving pro-
gram transformations (see for example [2]) to (i) unroll all loops,
(ii) inline all functions, and (iii) replace all arrays by collections of
scalars. This preprocessing step terminates in our setting because
the source program is assumed to describe a finite circuit. The syn-
tax of the resulting scoped circuit description language can thus be
simplified to the following:
Programs
Blocks
Variable Declarations
Statements
p (cid:70) b
b (cid:70) d1; d2; . . . ; s1; s2; . . .
d (cid:70) bit x
s (cid:70) x = true | x = false
| x = x1 ⊙ x2 | {b}
bit x;
x = xs0 ;
bit t;
t = t ∧ x;
r = r ∧ t;
}
rs0 = r;
{ // second call to f
bit r;
bit rs0 ; bit rs1 ;
bit xs0 ; bit xs1 ;
{ // first call to f
Programs in this core language are almost straight-line programs
corresponding to a sequential composition of gates, except that we
retain nested lexical scope relations. This is important for reasoning
about space allocation and de-allocation as explained next. Our