title:ePVF: An Enhanced Program Vulnerability Factor Methodology for Cross-Layer
Resilience Analysis
author:Bo Fang and
Qining Lu and
Karthik Pattabiraman and
Matei Ripeanu and
Sudhanva Gurumurthi
2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
ePVF: An Enhanced Program Vulnerability Factor
Methodology for Cross-layer Resilience Analysis
Bo Fang∗, Qining Lu∗, Karthik Pattabiraman∗, Matei Ripeanu∗, Sudhanva Gurumurthi†
∗Department of Electrical and Computer Engineering
Email: {bof, qining, karthikp, matei}@ece.ubc.ca
University of British Columbia
† Cloud Innovation Lab, IBM
Email: PI:EMAIL
Abstract—The Program Vulnerability Factor (PVF) has been
proposed as a metric to understand the impact of hardware faults
on software. The PVF is calculated by identifying the program
bits required for architecturally correct execution (ACE bits).
PVF, however, is conservative as it assumes that all erroneous
executions are a major concern, not just those that result in
silent data corruptions, and it also does not account for errors
that are detected at runtime, i.e., lead to program crashes. A more
discriminating metric can inform the choice of the appropriate
resilience techniques with acceptable performance and energy
overheads. This paper proposes ePVF, an enhancement of the
original PVF methodology, which ﬁlters out the crash-causing
bits from the ACE bits identiﬁed by the traditional PVF analysis.
The ePVF methodology consists of an error propagation model
that reasons about error propagation in the program, and a crash
model that encapsulates the platform-speciﬁc characteristics for
handling hardware exceptions. ePVF reduces the vulnerable bits
estimated by the original PVF analysis by between 45% and
67% depending on the benchmark, and has high accuracy (89%
recall, 92% precision) in identifying the crash-causing bits. We
demonstrate the utility of ePVF by using it to inform selective
protection of the most SDC-prone instructions in a program.
Keywords: PVF, Crash Model, Cross-layer Analysis
I. INTRODUCTION
Transient hardware faults,
typically caused by particle
strikes, are a major concern in modern computer systems.
Current manufacturing trends (e.g., smaller feature sizes, man-
ufacturing variations) suggest that transient faults will increase
in the future [1], [2]. Further, stringent energy constraints make
it challenging to deploy resilience techniques to protect the
system from hardware faults [3]. The problem is exacerbated
in high-performance computing (HPC) systems, where the
large scale and long running time make applications more
prone to hardware faults.
A hardware fault can affect an application in one of the
following ways: (i) crash, i.e., an exception is raised and
the application is terminated, (ii) hang, i.e., the application
runs for a signiﬁcantly longer time than normal, (iii) silent
data corruption (SDC), i.e., the application ﬁnishes with an
incorrect output, and (iv) benign, i.e., the application ﬁnishes
with a valid output. The ﬁrst
three are failure outcomes.
Among these, SDCs are considered the most severe, because
users will trust the application’s output in the absence of an
error indication. A crash can be detected by monitoring the
application, while hangs can be detected using timeouts. How-
ever, there is no generic method to detect SDCs without re-
executing the entire application and checking for a mismatch,
or without a signiﬁcant amount of hardware redundancy, both
of which are expensive.
Our long-term goal is to develop a systematic method to
inform the design of software protection techniques (e.g., code
transformations) to make applications resilient to SDCs. A
ﬁrst and essential step towards this goal is estimating the
SDC rates of programs. SDCs are caused by a combination of
application-speciﬁc and system-speciﬁc factors. In this paper,
we focus on the system-speciﬁc factors that lead to SDCs. The
main insight underlying this work is that a fault that leads to a
crash cannot (by deﬁnition) lead to an SDC. Because crashes
are caused by a combination of the hardware and Operating
System (OS) features, they can be systematically reasoned
about in an application-independent manner. By removing the
crash-causing faults from the set of all faults, one can obtain
a tighter estimate of the SDC rate. This is as important as
crashes are often the dominant failure outcome, and hence
signiﬁcantly outnumber both SDCs and hangs [4]–[7].
This paper proposes a new method, ePVF (enhanced PVF),
that builds on the original Program Vulnerability Factor (PVF)
analysis methodology proposed by Sridharan et al. [8] to
remove crash-causing faults from the set of all faults. PVF is
a systematic method to efﬁciently evaluate the error resilience
of software under hardware faults. PVF can also be used for
predictive and comparative analysis studies to understand the
effect of different protection techniques or code transforma-
tions on the error resilience. However, PVF does not distin-
guish between fault outcomes and, essentially, treats crashes,
SDCs and hangs as equally severe. Therefore, using PVF to
estimate application error resilience and inform the protection
mechanisms often leads to overprotecting applications, thereby
resulting in unnecessary performance and energy overheads.
By distinguishing between crashes and other failures, ePVF
allows protection techniques to better focus on the program’s
bits that if corrupted, can potentially cause SDCs.
There are two challenges in identifying crash-causing bits.
Firstly, crashes are caused by OS and architecture-speciﬁc
factors, which we need to understand and model. Secondly,
the crash-related OS state varies during program execution
978-1-4673-8891-7/16 $31.00 © 2016 IEEE
DOI 10.1109/DSN.2016.24
168
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:04 UTC from IEEE Xplore.  Restrictions apply. 
(e.g., segment boundaries for segmentation faults). Hence we
need a dynamic model for predicting whether a particular fault
will cause a crash. We ﬁnd that the majority of crashes are
caused by illegal memory addressing, and that by capturing
and reasoning about the state of a program’s memory segments
in a platform-speciﬁc manner, we can accurately ﬁnd almost
all crash-causing bits. We therefore extend the original PVF
estimation to estimate the ranges of values that may generate
crashes, propagate them on the backward slices of the loads
and stores, and efﬁciently compute the set of bits that can
result in program crashes.
Contributions. To the best of our knowledge, our work is the
ﬁrst to consider the effects of different failure modes through
a PVF-like analysis for the goal of analyzing a program’s
resilience to SDCs. Our work is also the ﬁrst to close the
gap between analytical models such as PVF, and experimental
assessment techniques such as fault injections. This paper:
1) Develops a crash model (§III-D) to predict which faults
in a program cause a crash, a propagation model (§III-C)
to reason about propagating ranges of crash-causing bits
in the program’s dependence graph, and integrates them
with the PVF methodology;
2) Implements the method in the LLVM compiler [9] and
its intermediate representation (IR) which offers the
ability to support multiple platforms and architectures;
3) Evaluates the accuracy of the proposed ePVF method
vis-a-vis fault injection (§IV) at the same abstraction
level using LLFI, an open-source fault injector [10]. It
ﬁnds that ePVF estimates crash-causing bits with 89%
recall and 92% precision, when evaluated over a set
of ten benchmarks. More importantly, the number of
vulnerable bits estimated by the ePVF analysis is lower
than that estimated by the standard PVF analysis by 61%
on average. Thus ePVF leads to a tighter estimate of
the SDC rate, and a close estimate of the crash rate
compared to fault injection.
4) Demonstrates the utility of the ePVF analysis through
a case study involving selective instruction-level protec-
tion for SDC mitigation (§V). We ﬁnd that the SDC
rate reduction achieved using ePVF is, on average, 30%
better than that achieved by hot-path duplication (i.e.,
duplicating the most frequently executed program paths),
for the same performance overhead.
II. BACKGROUND
This section offers background information on error re-
silience, the dependability metric we estimate (§II-A), past
work on estimating it
through fault-injection (§II-B) and
vulnerability analysis techniques (§II-C), the abstraction level
that our technique works at (§II-D) and our fault model (§II-E).
A. Dependability Metric: Error Resilience
Not all faults in a program result in failures due to masking
layers of the system stack. As we focus on
at different
software resilience techniques, we do not consider hardware
masking [11], but only take into account faults passing the
169
hardware and seen by the software. This is in line with other
work in this area [12]–[15].
In the context of this work, we deﬁne error resilience as the
probability that the application does not have an SDC after
a transient hardware fault occurs and impacts the application
state. Note that error resilience does not take into account
the probability of a fault occurring and affecting the software
(which depends on the base fault rate in the hardware and the
application execution time). In §V, we estimate the impact of
protection techniques by taking into account their effect on
application performance in addition to the resilience.
B. Fault Injection
that
injection [16],
Traditionally, program error resilience has been estimated
through fault
is, by introducing faults
at various levels of the system stack and observing their
outcome. This is a mature and well understood technique, and
there are many tools to help automate the process [17]–[19].
Unfortunately, fault injection does not have predictive power
in terms of determining the impact of code transformations
on vulnerability - thus it is challenging to use it for guiding
code optimizations. Further, fault
injection campaigns are
typically resource consuming, as thousands of faults need
to be injected in complete executions of the program,
to
get statistically signiﬁcant results. Hari et al. [20], [21] have
proposed approaches to reduce the cost of fault
injection
campaigns by pruning the fault injection space. However, fault
injection campaigns are still costly and cannot be used in
situations where predictive power is needed to choose between
the multiple options available for code optimization. Instead,
we need an automated characterization of error resilience that
does not use fault injections.
C. Program Vulnerability Factor (PVF)
The Architectural Vulnerability Factor (AVF) of a hardware
component is the probability that a fault occurring in the
component leads to a visible error in the ﬁnal output of a set of
executed instructions. Mukherjee et al. [22] introduced the Ar-
chitecturally Correct Execution (ACE) analysis for estimating
the AVF of processor structures (e.g., Reorder buffer) based on
a dynamic execution trace on a speciﬁc microarchitecture. By
combining ACE analysis with the raw error rate of a processor
structure and its AVF, microprocessor designers can estimate
the FIT (Failures In Time) of each processor structure and
take appropriate action in the design stage. However, AVF is
intricately tied to the microarchitectural design of a processor,
and cannot be used to reason about software resilience in
isolation.
Sridharan et al. [8] separate the hardware-speciﬁc compo-
nent of AVF from the software-speciﬁc component: the Pro-
gram Vulnerability Factor (PVF). They show that the PVF can
be used to explain the error resilience behaviour of a program
independent of the processor. Moreover, they show that by
using techniques that are used in computing the PVF [8],
programmers are able to pinpoint the vulnerability of different
segments of the program, and gain insights for designing
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:04 UTC from IEEE Xplore.  Restrictions apply. 
application-speciﬁc fault tolerance mechanisms. However, the
key drawback of PVF is that it does not distinguish between
different kinds of failures, i.e. crashes and SDCs.
PVF abstracts out timing information and includes only the
relative instruction order in the instruction ﬂow. This makes
the process of computing the PVF largely microarchitecture
neutral, thus making it a function of the program and the archi-
tecture alone (when executed with a speciﬁc input). Sridharan
et al. [8] estimate the PVF of an architectural resource ’R’ as
the ratio between the Architecturally Correct Execution (ACE)
bits in the resource when executing the set of instructions I
in a trace and the number of total bits involved in R (i.e.,
BR) (Equation 1). The ACE bits are the bits in which a fault
would potentially affect the correctness of the execution of the
instructions in I.
(cid:2)
PVF R =
D. LLVM IR
I
i=0 (ACE bits in R at instruction i)
BR × |I|
(1)
LLVM is a compiler infrastructure that provides support
for different hardware platforms [9]. The key component of
LLVM is its intermediate representation (IR), an assembly-
like language that abstracts out the hardware and ISA-speciﬁc
information. Our methodology is built on LLVM IR level, and
we choose to work at this abstraction level for the following
reasons:
i) LLVM abstraction level (i.e., LLVM IR) offers an uniform
representation of a program; hence the ePVF methodology is
architecture neutral and easy to port across different architec-
tures/ISAs, thereby eliminating the inﬂuence of architecture or
ISA-speciﬁc factors.
ii) LLVM IR maps closely to constructs of a program and
preserves source-level program properties, which makes it easy
to help understand the inherent fault masking.
iii) LLVM infrastructure provides extensive support for
program analysis and instrumentation. Prior work focusing
on selective duplication techniques [13], [14] uses the LLVM
compiler for both static and dynamic analysis, and program
instrumentation.
To validate our methodology, we use fault injection exper-
iments at the same abstraction level (LLVM IR) as our ePVF
implementation using LLFI [10]. Cho et al. [23] have found
that high-level fault injections can directly model a subset of
system-level behaviors caused by transient faults. However,
our focus (and the focus of the original PVF paper) is on the
subset of faults that do manifest in architecturally visible state
(e.g., registers) - these faults can be modeled by high-level
fault injections.
                   Crash-causing
ePVF bits
                   bits
Total Bits
ACE Bits
Fig. 1: Venn diagram that highlights the crash-causing and the ePVF
bits that the ePVF methodology identiﬁes as a subset of ACE bits.
SDC-causing bits will be a subset of the ePVF bits.
consider fault in main memory, since most servers used in HPC
applications are protected via ECC and hence do not generally
require additional software-based resilience mechanisms.
We use the single-bit-ﬂip model to represent transient faults
as in other related work [4], [5], [13], [14], [24]. Our technique
can be easily extended to multiple-bit ﬂips. In recent work,
Cho et al. [23] ﬁnd that low-level hardware faults manifest
as both single- and multiple-bit ﬂips at the application level.
However, recent work [25], [26] has shown that the difference
between single- and multiple-bit ﬂips occurring in program
states is marginal in terms of their impact on SDCs. Therefore
for this study, we stick to single bit ﬂips, as SDCs are our main
concern.
III. EPVF METHODOLOGY
Our goal
is to obtain a comprehensive estimate of the
program resilience that does not entail the full-blown costs
of fault injection. We aim to adapt the PVF methodology (see
§II) for this purpose. As pointed out earlier, PVF does not
distinguish between crashes and SDCs, and hence is overly
conservative, as SDCs are the main concern in practice.
Deﬁnition: We deﬁne ePVF by analogy to PVF as the ratio
of non-crashing ACE bits over the total bits involved. Figure 1
shows the ePVF bits: they are a subset of all ACE-bits, and a
superset of the SDC-causing bits. It is a superset because not
all non-crashing bits cause SDCs.
(cid:2)
I
ePVFR =
i=0 (ACEBits - CrashBits in R at instruction i)
BR × |I|
(2)
Methodology Overview: At a high level, the ePVF method-
ology consists of three major components (Figure 2): (i) Base
ACE analysis to estimate all the vulnerable bits of the program
(§III-A); (ii) a crash model to identify the ranges of bit-
level faults that cause crashes (§III-D); and (iii) a propagation
model that propagates these ranges along the backward slices
of each operand (§III-C). This methodology is supported by
our identiﬁcation of incorrect memory addressing as the most
common cause for crashes (§III-B).
E. The Fault Model
A. Base ACE Analysis
Hardware faults can be broadly classiﬁed as transient or
permanent. Transient faults usually are ”one-off” events and
occur non-deterministically, while permanent faults persist
at a given location. We consider transient faults that occur
within the processor (i.e., register ﬁle, ALUs). We do not
ACE analysis is used to determine the set of all bits in
an architectural resource (e.g., a register ﬁle) that are not
masked and can affect application’s ﬁnal state. The basic
idea is to ﬁrst identify the instructions that are responsible
for the output of the program (called output instructions),
170
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:04 UTC from IEEE Xplore.  Restrictions apply. 
Crash Model
Section III.D 
ACE Bits
Non-Crashing 
ACE Bits
Fig. 2: The overall workﬂow of the ePVF methodology to compute
the non-crashing ACE (ePVF) bits.
and then ﬁnd all the instructions in their backward slice. We
use the program’s dynamic dependency graph (DDG) [27]
to keep track of the data dependencies among the program’s
instructions. The DDG is a representation of data ﬂow in the