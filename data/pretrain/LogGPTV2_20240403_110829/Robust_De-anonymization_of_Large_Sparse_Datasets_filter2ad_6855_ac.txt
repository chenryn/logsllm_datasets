needs to know about his target), the easier the attack.
We start with the worst-case analysis and calculate
how much auxiliary information is needed without any
3Without loss of generality, we assume ∀i |supp(i)| > 0.
4Increasing φ increases the false negative rate, i.e., the chance of
erroneously dismissing a correct match, and decreases the false posi-
tive rate; φ may be chosen so that the two rates are equal.
116
Lemma 1 holds, because the contrary implies
Sim(r, r(cid:1)) ≥ (1 − )(1 − δ) ≥ (1 −  − δ), contra-
dicting the assumption that r(cid:1)
is a false match. There-
fore, the probability that the false match r(cid:1)
belongs to
the matching set is at most (1 − δ)m. By a union bound,
the probability that the matching set contains even a sin-
gle false match is at most N(1 − δ)m. If m = log N
,

1
1−δ
then the probability that the matching set contains any
false matches is no more than .
Therefore, with probability 1 − , there are no false
matches. Thus for every record r(cid:1)
in the matching set,
Sim(r, r(cid:1)) ≥ 1− − δ, i.e., any r(cid:1)
must be similar to the
true record r. To complete the proof, observe that the
matching set contains at least one record, r itself.
When δ is small, m = log N−log 
. This depends log-
arithmically on  and linearly on δ: the chance that the
algorithm fails completely is very small even if attribute-
wise accuracy is not very high. Also note that the match-
ing set need not be small. Even if the algorithm returns
log
δ
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
many records, with high probability they are all similar
to the target record r, and thus any one of them can be
used to learn the unknown attributes of r.
4.2 Analysis: sparse datasets
Most real-world datasets containing individual trans-
actions, preferences, and so on are sparse. Sparsity in-
creases the probability that de-anonymization succeeds,
decreases the amount of auxiliary information needed,
and improves robustness to both perturbation in the data
and mistakes in the auxiliary information.
Our assumptions about data sparsity are very mild.
We only assume (1− − δ, . . .) sparsity, i.e., we assume
that the average record does not have extremely similar
peers in the dataset (real-world records tend not to have
even approximately similar peers—see ﬁg. 1).
Theorem 2 Let , δ, and aux be as in Theorem 1. If
the database D is (1 −  − δ, )-sparse, then D can be
(1, 1 − )-deanonymized.
(cid:1)
The proof is essentially the same as for Theorem 1,
but in this case any r(cid:1) (cid:4)= r from the matching set must
be a false match. Because with probability 1− , Score-
board outputs no false matches, the matching set con-
sists of exactly one record: the true target record r.
De-anonymization in the sense of Deﬁnition 4 re-
quires even less auxiliary information. Recall that in
this kind of privacy breach, the adversary outputs a
“lineup” of k suspect records, one of which is the true
k )-
record. This k-deanonymization is equivalent to (1, 1
deanonymization in our framework.
Theorem 3 Let D be (1 −  − δ, )-sparse and auxbe
as in Theorem 1 with m = log N
k−1
1
1−δ
. Then
log
k )-deanonymized.
• D can be (1, 1
• D can be (1, log k)-deanonymized (entropically).
By the same argument as in the proof of Theorem 1,
if the adversary knows m = log N
k−1
attributes, then the
1
1−δ
expected number of false matches in the matching set is
at most k−1. Let X be the random variable representing
this number. A random record from the matching set is
a false match with probability of at least 1
x is a
convex function, apply Jensen’s inequality [18] to obtain
E[ 1
X ] ≥ 1
Similarly, if the adversary outputs the uniform dis-
tribution over the matching set, its entropy is log X.
Since log x is a concave function, by Jensen’s inequality
E[log X] ≤ log E(X) ≤ log k.
(cid:1)
Neither claim follows directly from the other.
X . Since 1
≥ 1
k .
log
E(X)
4.3 De-anonymization from a sample
We now consider the scenario in which the released
database ˆD (cid:1) D is a sample of the original database
D, i.e., only some of the anonymized records are avail-
able to the adversary. This is the case, for example, for
the Netﬂix Prize dataset (the subject of our case study
in section 5), where the publicly available anonymized
sample contains less than 1
10 of the original data.
In this scenario, even though the original database D
contains the adversary’s target record r, this record may
not appear in ˆD even in anonymized form. The adver-
sary can still apply Scoreboard, but the matching set
may be empty, in which case the adversary outputs ⊥
(indicating that de-anonymization fails). If the matching
set is not empty, he proceeds as before: picks a random
record r(cid:1)
and learn the attributes of r on the basis of
r(cid:1)
. We now demonstrate the equivalent of Theorem 1:
de-anonymization succeeds as long as r is in the public
sample; otherwise, the adversary can detect, with high
probability, that r is not in the public sample.
Theorem 4 Let , δ, D, and aux be as in Theorem 1,
and ˆD ⊂ D. Then ˆD can be (1 −  − δ, 1 − )-
(cid:1)
deanonymized w.r.t. aux.
The bound on the probability of a false match given in
the proof of Theorem 1 still holds, and the adversary is
guaranteed at least one match as long as his target record
r is in ˆD. Therefore, if r /∈ ˆD, the adversary outputs ⊥
with probability at least 1 − . If r ∈ ˆD, then again the
adversary succeeds with probability at least 1 − .
Theorems 2 and 3 do not translate directly. For each
record in the public sample ˆD, there could be any num-
ber of similar records in D \ ˆD, the part of the database
that is not available to the adversary.
Fortunately, if D is sparse, then theorems 2 and 3
still hold, and de-anonymization succeeds with a very
small amount of auxiliary information. We now show
that if the random sample ˆD is sparse, then the entire
database D must also be sparse. Therefore, the adver-
sary can simply apply the de-anonymization algorithm
to the sample. If he ﬁnds the target record r, then with
high probability this is not a false positive.
Theorem 5 If database D is not (, δ)-sparse, then a
λ )-sparse with probabil-
random 1
ity at least 1 − γ.
(cid:1)
λ -subset ˆD is not (, δγ
For each r ∈ ˆD, the “nearest neighbor” r(cid:1)
of r in
λ of being included in ˆD. There-
D has a probability 1
fore, the expected probability that the similarity with the
117
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
nearest neighbor is at least 1 −  is at least δ
λ . (Here the
expectation is over the set of all possible samples and the
probability is over the choice of the record in ˆD.) Ap-
plying Markov’s inequality, the probability, taken over
the choice ˆD, that ˆD is sparse, i.e., that the similarity
λ , is no more than γ. (cid:1)
with the nearest neighbor is δγ
The above bound is quite pessimistic. Intuitively, for
any “reasonable” dataset, the sparsity of a random sam-
ple will be about the same as that of the original dataset.
Theorem 5 can be interpreted as follows. Consider
the adversary who has access to a sparse sample ˆD, but
not the entire database D. Theorem 5 says that either
a very-low-probability event has occurred, or D itself is
sparse. Note that it is meaningless to try to bound the
probability that D is sparse because we do not have a
probability distribution on how D itself is created.
Intuitively, this says that unless the sample is spe-
cially tailored, sparsity of the sample implies sparsity of
the entire database. The alternative is that the similarity
between a random record in the sample and its nearest
neighbor is very different from the corresponding distri-
bution in the full database. In practice, most, if not all
anonymized datasets are published to support research
on data mining and collaborative ﬁltering. Tailoring the
published sample in such a way that its nearest-neighbor
similarity is radically different from that of the origi-
nal data would completely destroy utility of the sam-
ple for learning new collaborative ﬁlters, which are often
based on the set of nearest neighbors. Therefore, in real-
world anonymous data publishing scenarios—including,
for example, the Netﬂix Prize dataset—sparsity of the
sample should imply sparsity of the original dataset.
5 Case study: Netﬂix Prize dataset
On October 2, 2006, Netﬂix, the world’s largest on-
line DVD rental service, announced the $1-million Net-
ﬂix Prize for improving their movie recommendation
service [15]. To aid contestants, Netﬂix publicly re-
leased a dataset containing 100, 480, 507 movie ratings,
created by 480, 189 Netﬂix subscribers between Decem-
ber 1999 and December 2005.
Among the Frequently Asked Questions about the
Netﬂix Prize [23], there is the following question: “Is
there any customer information in the dataset that should
be kept private?” The answer is as follows:
“No, all customer identifying information has
been removed; all that remains are ratings and
dates. This follows our privacy policy [. . . ]
Even if, for example, you knew all your own
ratings and their dates you probably couldn’t
identify them reliably in the data because only
a small sample was included (less than one-
tenth of our complete dataset) and that data
was subject to perturbation. Of course, since
you know all your own ratings that really isn’t
a privacy problem is it?”
Removing identifying information is not sufﬁcient
for anonymity. An adversary may have auxiliary infor-
mation about a subscriber’s movie preferences: the ti-
tles of a few of the movies that this subscriber watched,
whether she liked them or not, maybe even approximate
dates when she watched them. We emphasize that even
if it is hard to collect such information for a large num-
ber of subscribers, targeted de-anonymization—for ex-
ample, a boss using the Netﬂix Prize dataset to ﬁnd an
employee’s entire movie viewing history after a casual
conversation—still presents a serious threat to privacy.
We investigate the following question: How much
does the adversary need to know about a Netﬂix sub-
scriber in order to identify her record if it is present in
the dataset, and thus learn her complete movie viewing
history? Formally, we study the relationship between
the size of aux and (1, ω)- and (1, H)-deanonymization.
Does privacy of Netﬂix ratings matter? The issue is
not “Does the average Netﬂix subscriber care about the
privacy of his movie viewing history?,” but “Are there
any Netﬂix subscribers whose privacy can be compro-
mised by analyzing the Netﬂix Prize dataset?” As shown
by our experiments below, it is possible to learn sensi-
tive non-public information about a person from his or
her movie viewing history. We assert that even if the
vast majority of Netﬂix subscribers did not care about
the privacy of their movie ratings (which is not obvious
by any means), our analysis would still indicate serious
privacy issues with the Netﬂix Prize dataset.
Moreover, the linkage between an individual and her
movie viewing history has implications for her future
privacy. In network security, “forward secrecy” is im-
portant: even if the attacker manages to compromise a
session key, this should not help him much in compro-
mising the keys of future sessions. Similarly, one may
state the “forward privacy” property: if someone’s pri-
vacy is breached (e.g., her anonymous online records
have been linked to her real identity), future privacy
breaches should not become easier. Consider a Net-
ﬂix subscriber Alice whose entire movie viewing his-
tory has been revealed. Even if in the future Alice cre-
ates a brand-new virtual identity (call her Ecila), Ecila
will never be able to disclose any non-trivial informa-
tion about the movies that she had rated within Netﬂix
118
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
because any such information can be traced back to her
real identity via the Netﬂix Prize dataset.
In general,
once any piece of data has been linked to a person’s real
identity, any association between this data and a virtual
identity breaks anonymity of the latter.
Finally,
the Video Privacy Protection Act of
1988 [13] lays down strong provisions against disclo-
sure of personally identiﬁable rental records of “prere-
corded video cassette tapes or similar audio visual ma-
terial.” While the Netﬂix Prize dataset does not explic-
itly include personally identiﬁable information, the issue
of whether the implicit disclosure demonstrated by our
analysis runs afoul of the law or not is a legal question
to be considered.
How did Netﬂix release and sanitize the data? Figs. 2
and 3 plot the number of ratings X against the num-
ber of subscribers in the released dataset who have at
least X ratings. The tail is surprisingly thick: thousands
of subscribers have rated more than a thousand movies.
Netﬂix claims that the subscribers in the released dataset
have been “randomly chosen.” Whatever the selection
algorithm was, it was not uniformly random. Common
sense suggests that with uniform subscriber selection,
the curve would be monotonically decreasing (as most
people rate very few movies or none at all), and that
there would be no sharp discontinuities.
We conjecture that some fraction of subscribers with