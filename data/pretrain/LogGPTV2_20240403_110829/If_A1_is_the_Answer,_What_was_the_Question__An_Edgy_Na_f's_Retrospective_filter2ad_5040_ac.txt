or permissions subjects had to objects. It was possible 
in this model to identify all subjects who had any form 
of  access  to  any  specified  object as well as to identify 
all  objects  to  which  each  subject  had  any  specified 
form  of 
interprocess 
communication implied that subjects also be viewed as 
objects,  a  deduction  that  ultimately  resulted  in  the 
recognition  of  a  subject  as  a  -pair, 
wherein  domain  included  all  of  the  attributes  of  the 
executing  process  (including  security,  integrity,  and 
privilege-state  attributes).  In  a  large  system,  it  was 
tacitly assumed that M would be a large sparse matrix. 
Thus, implementation concerns suggested that M either 
have  its  attributes  stored  in  lists  associated  with 
subjects  (capability  lists)  or  with  objects  (access 
control  lists).  Harrison,  Ruzzo  and  Ullman  [10]  also 
showed  that  discretionary  access  control  (DAC),  the 
form  of  access  modeled  in  M,  had  a  degree  of 
uncertainty equivalent to the Turing Halting Problem. 
The  Anderson  Study  also  resulted  in  publication  of 
a series of formal models that addressed abstractions of 
military  classifications  and  clearances,  so-called 
nondiscretionary  access  control  (later  to  be  known  as 
mandatory access control or MAC). The most popular 
of  these  models  were  those  of  Case  Western  Reserve 
University  [26]  and  of  D.  Elliott  Bell  and  Leonard 
LaPadula  [3].  At  the  time  of  their  publication  and 
refinement, these models were reputed to have formally 
provable security properties. This was later questioned 
by John McLean [17] and others. 
Central  to  the  Anderson  Study’s  framework  is  its 
elaboration  of  the  reference  monitor  concept.  Its 
implementation  is  a  reference  validation  mechanism 
(RVM),  often  called  a  “security  kernel,”  a  term 
introduced  by  Major  Roger  R.  Schell  at  an  early 
Stockton  Gaines  workshop.    Specifically,  the  Study 
prescribed three requirements characterizing the RMC: 
a.  The reference validation mechanism must be tamper 
proof. 
b.  The reference validation mechanism must always be 
invoked. 
c.  The  reference  validation  mechanism  must  be  small 
enough  to  be  subject  to  analysis  and  tests,  the 
completeness of which can be assured. 
It elaborated: 
Each of these requirements is significant, for without 
them  the  mechanism  cannot  be  considered  secure. 
The  first is obvious, since if the reference validation 
mechanism  can  be  tampered  with,  its  validity  is 
destroyed,  as  is  any  hope  of  achieving  security 
through  it.  The  second  requirement  of  always
invoking  the  reference  validation  mechanism  simply 
last 
requirement 
states  that  if  the  reference  validation  is  (or  must  be) 
suspended  for  some  group  of  programs,  then  those 
programs  must  be  considered  part  of  the  security 
apparatus,  and  be  subject  to  the  first  and  last 
is  equally 
requirements.  The 
important.  It  states 
the  reference 
validation  mechanism  is  the  security  mechanism  in 
the  system,  it  must  be  possible  to  ascertain  that  it 
works correctly in all cases and is always invoked. If 
this cannot be achieved, then there is no way to know 
that  the  reference  validation  takes  place  correctly  in 
all cases, and therefore there is no basis for certifying 
a system as secure. [2, vol. I, p. 10] 
that  because 
The  Anderson  Report  recognized  that  operating 
systems  were  larger  and  more  complex  than  most 
programs,  and  that  exhaustive  testing  was  out  of  the 
question.  So  the  report  called  for  modularization  that 
would support analysis and credible testing. There is an 
interesting consideration of the possibility of subjecting 
operating 
to  mathematical  proofs  of 
correctness, but this was understood to be well-beyond 
the  capability  of  human  individuals.  There  is  a 
discussion  of  the  potential  for  research  that  would 
support 
formal 
computer-aided 
verification assurances sometime in the future. 
generation 
systems 
of 
Like 
the  need 
to  consider 
the  Ware  Report, 
the  Anderson  Report 
recognized 
system  use 
environment  and  functional  characteristics  as  part  of 
the  overall  risk  and  vulnerability  assessment.  It 
considered  secure  transaction  systems  to  be  the  least 
threatening  and  most  achievable  of  the  “open  use” 
multiprogrammed system contexts. The next most risky 
category 
the  High-Order-
Language  (HOL)—the  only  system  described  in  the 
Ware  Report  which  raises  the  possibility  of  breaking 
out of the confines of  FORTRAN. No explicit details on 
how this might be achieved are presented.  
to  be  considered  was 
The  Report  outlines  several  development  plans  for 
research  and  development  needed  to  achieve  secure 
open-use  systems.  There  is  specific  reference  to 
planning  research  programs  on  secure  networks, 
security models, security software engineering, security 
surveillance,  certification 
techniques,  architecture 
research,  data 
integrity  and  reliability,  automatic 
classification,  magnetic  media,  and  computer  aided 
integrated design. Of interest is an allusion to 
The  possibility  of  internal  encryption  of  computer 
programs  and  data  was  first  advanced  in  1966  prior 
to  the  Defense  Science  Board  Task  Force  on 
Computer  Security.  Since  that  time  it  has  received 
sporadic  attention.  It  appears  that  it  is  possible  to 
apply  this  technique  either  as  an  appliqué  or  as  an 
integral  part  of  the  design  of  computer  systems  [2, 
vol. II, p. 44]. 
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:37:23 UTC from IEEE Xplore.  Restrictions apply. 
It is interesting to note that the Anderson Panel had 
the foresight to predict that: 
Perhaps  the  most  interesting  potential  of  modern 
technology  will  be  the  radical  reduction  of  cost  of 
computer  main  frames.  We  can,  for  all  intents  and 
purposes  assume  that  computer  main  frames  will  be 
effectively  “free”  in  the  not  too  distant  future.  As  a 
consequence,  if  it  is  really  necessary  to  separate 
various  users,  each  can  be  given  his  own  computer. 
However, more often than not, they are dealing with 
common  data  bases  and  must  hand  off  certain 
common  data  to  one  another  and,  on  occasion  share 
programs.  As  a  result,  we  are  still  in  need  of  secure 
computing  systems.  With  very  low  cost  computer 
logic  however,  we  have 
the  possibility  of  a 
distributed  system.  By  this  is  meant  a  system  in 
which 
functions  may  be 
distributed  among  different  machines  which  are 
“netted”  together.  Netting  does  not  imply  a  number 
of machines doing identical tasks, nor does it imply a 
number  of  necessarily  identical  machines…  [2,  vol. 
II, p. 102]. 
the  various  system 
2. Principles, perceptions, and worked 
examples 
Following  publication  of  the  Ware  and  Anderson 
studies,  interest  in  “secure”  systems  and  products 
appeared to have increased in number and depth. Donn 
Parker began to publish books on computer crime, but 
most  of  what  appeared  in  the  popular  press  described 
variants  on  insiders  automating  white  collar  crime 
against  financial  institutions.  Among  institutions  that 
had  potential  computer  security  vulnerabilities,  their 
identified  concerns  focused  in  on  reliability  and 
protection  from  disruption  of  service.  Very  few 
organizations  expressed  concern  over  protecting 
confidential 
unauthorized 
modification  or  display.12  At  the  beginning  of  the 
1970s,  however,  there  was  still  no  consensus  that 
technical  measures  were  required  to  counter  any 
identified  computer  security  threats.  From  Parker’s 
books  on  through  banking,  commercial  and  military 
users,  the  expressed  belief  was  that  trained  system 
managers  and  the  use  of  guards,  badges,  personnel 
background  examinations,  and  encrypted  external 
communications would suffice to meet their protection 
requirements.  
information 
from 
During this period, several security working groups 
were formed that brought security researchers together. 
One, hosted at the RAND Corporation, was chaired by 
R.  Stockton  Gaines.  In  addition  to  members  of  the 
12  Banks  were  far  more  concerned  over  unauthorized 
modification  of transactions than breeches of confidentiality. 
Ware Panel, the group periodically included additional 
researchers  from  UCLA,  USC  Information  Sciences 
Institute (USC-ISI), MIT, RAND, SDC, MITRE, NSA, 
ESD,  and  other  institutes.  A  second  group,  organized 
later  by  Steve  Walker  while  he  was  at  DARPA,  added 
principals from Carnegie-Melon, Bell Labs and various 
DARPA  contractors  This  latter  working  group  evolved 
into a kernel of the Department of Defense's Computer 
Security  Initiative.  Participants  who  met  in  these  two 
groups  included:  Jim  Anderson,  Roger  Schell,  Dan 
Edwards,  Ann  Marmor-Squires,  Anita  Jones,  Butler 
Lampson, Jerry Saltzer, Gerry Popek, Clark Weissman, 
Dick Bisby, Dennis Hollingsworth, Jim Gillogly, David 
Bonyun,  Mike  Schroeder,  Peter  Neumann,  Ed  Burke, 
Steve Lipner, and me.  
From  the  Anderson  Report  and  refined  by  these 
groups also came the notion of the security perimeter,
which  consisted  exclusively  of  security-relevant  code 
and  the  minimal  subset  of  the  operating  system 
required  to  support  it.  The  code  within  the  security 
perimeter  became  known  as  the  security  kernel,  the 
minimal  body  of  code  required  to  enforce  the  system 
security  policy.  Almost  as  a  mantra,  many  in  the 
research  community  claimed  that  if  “the  good  guys 
write  the  code  inside  the  security  perimeter,  then  the 
system  will  be  secure,  even  if the adversary writes the 
remainder of the operating system and its utilities.” 
2.1. Access control policy considerations 
Up  through  the  early  1970s,  I  worked  closely  with 
clients  in  both  military  and  commercial  application 
sectors.  I  became  aware  of 
the  wide  range  of 
discretionary  access  controls  that  were  asked  for  by 
different user groups. Their requirements included: 
•  Password  protected  files:  where  knowledge  of  a 
password suffices to gain complete file access
•  Time-based access controls: where specific actions 
could  only  be  performed  on  specified  days  and/or 
during  identified  time  periods.  One  such  defined 
mode of access may be no access. 
•  Group-based  access  controls:  where  access  modes 
are uniformly applied to users assigned membership 
in  named  groups,  e.g.,  only  members  of  the  salary 
administrator  group may modify salaries. One such 
defined mode of access may be no access. 
•  Rôle-based  access  controls:  where,  e.g.,  persons 
acting  in  the  rôle  of  salary  administrator  may  only 
perform  specified  accesses  and  no  others  (e.g.,  no 
general  programming  while  acting  in  the  rôle  of 
salary  administrator.  One  such  defined  mode  of 
access may be no access. 
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:37:23 UTC from IEEE Xplore.  Restrictions apply. 
•  User-specific  access  controls:  associated  with  each 
user  and  each  protected  object  is  a  set  of  specific 
modes  of  access  that  the  user  is  permitted  to  have 
for  the  object;  this  is  the  form  of  access  control 
typified  by  access  control  lists.  As  in  the  Graham-
Denning model, one mode of access may be control
access, a mode that allows the user to grant or deny 
access  for  other  users.  One  such  defined  mode  of 
access may be no access. 
•  Prohibited-access  controls:  sets  of  users  for  whom 
specific  forms  of  access  to  objects  are  specifically 
prohibited. These prohibitions may be broader than 
the no access form described supra.
•  Combinations of the above: e.g., a specified user in 
a  particular  rôle  at  a  given 
time  from  an 
administrator’s  terminal  may  access  a  controlled 
object in only a specified set of modes.
•  Formularies:  a  term  coined  by  Lance  Hoffman 
wherein access to a specific object is computed by a 
specified,  possibly  ad  hoc,  function.  More  robust 
than the foregoing access controls, a formulary may, 
e.g.,  restrict  a  salary  administrator  to  modifying 
only the salaries of those reporting to managers in a 
specific  department  providing  that  the  new  salary 
does  not  exceed  a  stated  percentage  of 
the 
employee’s manager’s salary.
In  full  generality,  discretionary  access  control  was 
recognised as being much more complex than a simple 
label-based  policy.  Further,  the  undecidability  issues 
raised  by  Harrison,  Ruzzo  and  Ullman  and  additional 
implementation  complexity  issues  suggested  strongly 
that  high-assurance  systems  could  not  be  uniquely 
based on DAC mechanisms. 
2.2. Implementation considerations 
Early  attempts  at  implementing  robust,  production-
to  be 
quality  secure  operating  systems  proved 
surprisingly  difficult,  despite  the  application  of  sound 
principles.  In  part,  this  was  because  of  the  lack  of 
experience  in  dealing  with  the  problems  of  reducing 
theoretical  objectives  of  layered  architectures,  least 
privilege  and  least common mechanism to engineering 
practice  within  the  framework  of  information  flow 
confinement.  
2.2.1.  Multics.  It  has  become  common  contemporary 
practice for scarcely tested and inadequately debugged 
software  products  to  be  sold  commercially.  Various 
vendors,  notably  Microsoft,  have  been  accused  of 
launching  products  that  have  never  been  subjected  to 
beta  testing.  Disgruntled  customers  have  complained 
and lampooned such companies and their programming 
staffs’ capabilities, not entirely without justification. 
Knowledge  gained  from  penetration  studies  had 