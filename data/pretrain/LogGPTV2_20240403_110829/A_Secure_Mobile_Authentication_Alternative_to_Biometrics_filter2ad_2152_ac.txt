vectors for each of the two layers of Inception.v3 (the last hidden
layer and Mixed 8/Pool 0 layer). Œe output of each layer for each
segment is separately processed as in the basic ai.lock. Œus, the
resulting imageprint of the image has 10λ bits.
photos of 350 objects, belonging to 33 object categories. Example
of object categories in this dataset includes watches, shoes, jewelry,
shirt paŠerns, and credit cards. We have captured 4 images of each
object, that diﬀer in background and lighting conditions.
ALOI dataset. We have used the “illumination direction” subset of
the Amsterdam Library of Object Images (ALOI) [22] dataset. Œis
dataset includes 24 diﬀerent images of 1000 unique objects (24,000
in total) that are taken under diﬀerent illumination angles.
Google dataset. We have used Google’s image search to retrieve
at least 200 images from each of the 33 object categories of the
Nexus image dataset, for a total of 7,853 images. Œis dataset forms
the basis of a “targeted” aŠack.
YFCC100M toy dataset. We have extracted a subset of the Yahoo
Flickr Creative Commons 100M (YFCC100M) [69] image dataset
(100 million Flickr images), of 126,600 Flickr images tagged with
the “toy” keyword, and not with “human” or “animal” keywords.
6.1.2
Synthetic Data.
Synthetic image dataset. Manually capturing the Nexus dataset
was a diﬃcult and time consuming process. In order to eﬃciently
generate a large dataset of similar images, we have leveraged the
ability of generative models to discover an abstract representation
that captures the essence of the training samples. Generative mod-
els, including Generative Adversarial Networks (GAN) [23], are
trained to generate samples that are similar to the data they have
been trained on. Such models have been shown to be suitable for
representation learning tasks, e.g., [54].
We have used a DCGAN network [54] to generate a large set
of synthetic images that are similar to the images in the Nexus
dataset. Speciﬁcally, we have trained a DCGAN network [54] us-
ing the images of the Nexus dataset for 100 training epochs. Im-
age augmentation, e.g., rotation, enhancement, and zoom, is per-
formed to artiﬁcially increase the number of Nexus image dataset
samples to include 20 variants per image. We then used the trained
network to generate synthetic images: generate a random vector
(z) drawn from the uniform distribution, then feed z to DCGAN’s
generator network to construct an image. We repeated this process
to generate 200,000 images, that form our synthetic image dataset.
Figure 3(b) shows sample images generated by this network, along-
side similar images from the Nexus dataset.
Synthetic credential dataset. We have generated the binary im-
ageprints for the images in Nexus dataset based on the best param-
eters of ai.lock (see § 7.1). For each considered λ value, we consider
the value at each position of the binary imageprint as an indepen-
dent Bernoulli random variable. We then calculate the probability
of observing a 1 in each position based on the imageprints of the
Nexus dataset. We use these probabilities to draw 100,000 random
samples (of length λ) from the corresponding Bernoulli distribu-
tion for each position. Œe resulting random binary imageprints
form our synthetic credential dataset. We have experimented with
10 values of λ ranging from 50 to 500, thus, this dataset contains 1
million synthetic imageprints.
6.1 Primary Data Sets
6.1.1 Real Images.
Nexus dataset. We have used a Nexus 4 device to capture 1,400
6
6.2 Evaluation Datasets
We use the above image datasets to generate authentication sam-
ples that consist of one candidate image and one reference image.
ai.lock attack dataset. We use roughly 85% of the images from
the Nexus, ALOI and Google datasets as a training set to train and
estimate the performance of ai.lock. We use the remaining 15%
of images in each dataset (i.e., 220 Nexus, 3,600 ALOI and 1,178
Google images) as a holdout set. We use the holdout dataset to
assess the generalization error of the ﬁnal model, and as a real
image aŠack dataset (see § 7.3).
2
We generate the samples in holdout dataset using each subset
of Nexus, ALOI and Google separately as follows. Each image of
the Nexus holdout dataset is chosen as a reference image once,
then coupled once with all the other images in the Nexus, ALOI
and Google sets, used as candidate images. Œerefore, there are
220×219
= 24, 090 combinations of samples for the images in the
Nexus set. For each 55 unique objects in this set, there are 6 ((cid:0)4
2(cid:1))
possible valid samples that compare one image of this object to an-
other image of the same object. Œus, there are 55 × 6 = 330 valid
samples in the Nexus set. We then generate 220 × 3, 600 = 792K
and 220 × 1, 178 = 259, 160 invalid samples from comparing Nexus
images to images in ALOI and Google sets respectively. Œerefore,
the ai.lock holdout set contains a total of 1, 075, 250 samples.
2
In addition, the training set is further divided into 5 folds, for
cross validation. Each training fold contains 236, 4, 080 and 1, 335
images of Nexus, ALOI and Google datasets respectively. Œere-
fore, there are 236×235
= 27, 730 samples for the fold’s 59 unique
Nexus set objects, of which 59 × 6 = 354 pairs are valid. Similarly,
we generate 236 × 4, 080 = 962, 880 and 236 × 1, 335 = 315, 060
invalid samples, that consist of Nexus images coupled with ALOI
and Google images, respectively. Œus, each training fold has a
total of 1, 305, 670 samples, of which 354 are valid.
Synthetic image attack datasets. We divide the synthetic image
dataset of § 6.1 into 2 equal sets, each containing 100,000 images.
Œen, we build two synthetic image aˆack datasets (DS1 and DS2)
by repeating the following process for each subset of the synthetic
image dataset: combine each Nexus dataset image, used as a ref-
erence image, with each image from the subset of the synthetic
image dataset, used as a candidate image. Œerefore, in total we
have 140 million samples in each of DS1 and DS2.
Synthetic credential attack dataset. We use the synthetic cre-
dential dataset described in § 6.1 to build a synthetic credential at-
tack dataset: for each value of λ, combine the imageprint of each
Nexus dataset image, used as a reference imageprint, with each
imageprint in synthetic credential dataset, used as the candidate
imageprint. Hence, we have 140 million authentication samples
in this dataset for each value of λ. We repeat this process for 10
values of λ, ranging from 50 to 500. Œerefore, in total this dataset
contains 10 × 140 M = 1.4 billion samples.
Illumination robustness evaluation dataset. To evaluate the
performance of ai.lock under illumination changes, we use the ALOI
holdout set (3, 600 images) that includes up to 11 images of each
object captured under a diﬀerent illumination condition. Speciﬁ-
cally, we pair each image in the ALOI holdout set (i.e., not used
during training) with all the other images in this set. Œerefore,
we have a total of 3600×3599
= 6, 478, 200 authentication samples
in the illumination robustness evaluation dataset, of which 6, 306
samples are valid.
2
PCs: 0-150
PCs: 150-300
PCs: 0-400
PCs: 0-200
PCs: 200-400
)
%
(
e
r
o
c
S
1
F
100
80
60
40
20
0
λ = 150
λ = 250
λ = 350
λ = 500
Figure 4: Comparison of ai.lock performance (F1 score)
when using diﬀerent subset of principal component feature
ranks for diﬀerent imageprint length (λ) values. PCs ranked
200-400 constantly outperform other tested subsets.
Entropy evaluation dataset. We randomly selected 2 billion unique
pairs of images from the YFCC100M toy dataset. In each pair, an
image is considered to be the reference, the other is the candidate.
7 EXPERIMENTAL EVALUATION
We evaluate ai.lock and its variants. First, we describe the process
we used to identify the best ai.lock parameters. We use these pa-
rameters to evaluate the performance of ai.lock under the aŠack
datasets of § 6.2. We also show that ai.lock is a δ -LSIM function,
empirically estimate its entropy, and measure its speed on a mobile
device.
7.1 ai.lock: Parameter Choice
We identify the best parameters for the ai.lock variants using 5 fold
cross validation on the ai.lock training dataset (see § 6.2).
Best principal component range. To identify the best PC range,
we use 5 fold cross validation as follows. First, we retrieve the
embedding vector (output of the last hidden layer of Inception.v3)
for each image in the ai.lock training dataset. Œen, for each cross
validation experiments, we use 4 training folds to ﬁnd the principal
components of the embedding vectors. Œen, we transform the
embedding vectors of the test fold into the newly identiﬁed feature
space. Finally, we project them into several randomly generated
vectors (LSH) to construct the binary imageprint of the images.
We have experimented with diﬀerent subsets of the transformed
feature space of various size including the ﬁrst and second consec-
utive principal component sets of size 50, 100, 150, and 200, as well
as, the ﬁrst 400 PCs.
Figure 4 shows the cross validation performance achieved by
ai.lock when using diﬀerent subsets of PC features for diﬀerent λ
values. We observe that the PCs ranked 200-400 perform consis-
tently the best. Œis might seem surprising, as higher ranked PCs
have higher variability and thus we expected that they would have
more impact in diﬀerentiating between valid and invalid samples.
We conjecture that some of the lower rank coordinates of these
transformed vectors are more eﬃcient in capturing the lower level
details of the input object images that diﬀerentiate them from other
object images.
Identify the best threshold. We identify the best threshold that
separates the valid from the invalid authentication samples using
the binary imageprints of the testing folds in each of the 5 cross
validation experiments using ai.lock training set. Particularly, we
normalize the Hamming distance of each pair of imageprints in the
7
n
o
i
t
a
d
i
l
a
V
s
s
o
r
C
)
%
(
e
r
o
c
s
1
F
t
u
o
d
l
o
H
)
%
(
e
r
o
c
s
1
F
100
90
80
70
60
100
90
80
70
60
0
0
5
5
SLSS
MLSS
SLMS
MLMS
)
%
(
e
r
o
c
s
R
A
F
0.003
0.002
0.001
0.000
)
%
(
e
r
o
c
s
R
A
F
0.003
0.002
0.001
0.000
0
0
5
5
0
0
0
0
1
1
0
0
5
5
1
1
0
0
0
0
2
2
0
0
0
0
0
0
5
5
2
2
3
λ
(b)
3
λ
(e)
0
0
5
5
3
3
0