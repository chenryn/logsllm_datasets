title:On the R&apos;enyi Differential Privacy of the Shuffle Model
author:Antonious M. Girgis and
Deepesh Data and
Suhas N. Diggavi and
Ananda Theertha Suresh and
Peter Kairouz
On the R√©nyi Differential Privacy of the Shuffle Model
Antonious M. Girgis
Suhas Diggavi
Deepesh Data
UCLA
UCLA
UCLA
Ananda Theertha Suresh
Google
Peter Kairouz
Google
ABSTRACT
The central question studied in this paper is R√©nyi Differential
Privacy (RDP) guarantees for general discrete local randomizers
in the shuffle privacy model. In the shuffle model, each of the ùëõ
clients randomizes its response using a local differentially private
(LDP) mechanism and the untrusted server only receives a random
permutation (shuffle) of the client responses without association to
each client. The principal result in this paper is the first direct RDP
bounds for general discrete local randomization in the shuffle pri-
vacy model, and we develop new analysis techniques for deriving
our results which could be of independent interest. In applications,
such an RDP guarantee is most useful when we use it for composing
several private interactions. We numerically demonstrate that, for
important regimes, with composition our bound yields an improve-
ment in privacy guarantee by a factor of 8√ó over the state-of-the-art
approximate Differential Privacy (DP) guarantee (with standard
composition) for shuffle models. Moreover, combining with Pois-
son subsampling, our result leads to at least 10√ó improvement over
subsampled approximate DP with standard composition.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Privacy-preserving protocols.
KEYWORDS
Differential privacy; R√©nyi divergence; distributed learning; privacy
amplification via shuffling; privacy composition.
ACM Reference Format:
Antonious M. Girgis, Deepesh Data, Suhas Diggavi, Ananda Theertha
Suresh, and Peter Kairouz. 2021. On the R√©nyi Differential Privacy of the
Shuffle Model. In Proceedings of the 2021 ACM SIGSAC Conference on Com-
puter and Communications Security (CCS ‚Äô21), November 15‚Äì19, 2021, Vir-
tual Event, Republic of Korea. ACM, New York, NY, USA, 21 pages. https:
//doi.org/10.1145/3460120.3484794
1 INTRODUCTION
Differential privacy (DP) [16] gives a principled and rigorous frame-
work for data privacy by giving guarantees on the information
leakage for individual data points from the output of an algorithm.
Algorithmically, a standard method is to randomize the output of
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea.
¬© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484794
an algorithm to enable such privacy. Originally DP was studied
in the centralized context, where the privacy from queries to a
trusted server holding the data was the objective [16]. However,
in distributed applications, such as federated learning [30], two
significant aspects need to be accommodated: (i) data is held locally
at clients and needs to be used for computation with an untrusted
server; and (ii) to build good learning models, one might need
repeated interactions (e.g., through distributed gradient descent).
To accommodate privacy of locally held data, a more appropriate
notion is that of local differential privacy (LDP) [15, 34]. In the
LDP framework, each (distributed) client holding local data, indi-
vidually randomizes its interactions with the (untrusted) server.1
Recently, such LDP mechanisms have been deployed by compa-
nies such as Google [23], Apple [29], and Microsoft [14]. However,
LDP mechanisms suffer from poor performance in comparison
with the centralized DP mechanisms, making their applicability
limited [15, 31, 34]. To address this, a new privacy framework using
anonymization has been proposed in the so-called shuffle model
[7, 13, 22], where each client sends her (randomized) interaction
message to a secure shuffler that randomly permutes all the received
messages before forwarding them to the server. Such a shuffling
can be enabled through anonymization techniques [10, 20, 22]. This
model enables significantly better privacy-utility performance by
amplifying LDP through this mechanism.
For the second aspect, where there are repeated interactions (e.g.,
through distributed gradient descent), one needs privacy composi-
tion [9]. In other words, we want to compute the overall privacy
budget under the composition of multiple iterations. Clearly, from
an optimization viewpoint, we might need to run these interactions
longer for better models, but these also result in privacy leakage.
Though the privacy leakage can be quantified using advanced com-
position theorems for DP (e.g., [19, 33]), these might be loose. To
address this, Abadi et al. [1] developed a ‚Äúmoments accountant‚Äù
framework, which enabled a much tighter composition. This is
enabled by providing the composition privacy guarantee in terms
of R√©nyi Differential Privacy [36], and then mapping it back to the
DP guarantee [37]. It is known [1] that the moments accountant
provides a significant saving in the total privacy budget in compari-
son with using the strong composition theorems [19, 33]. Therefore,
developing the RDP privacy guarantee can enable stronger compo-
sition privacy results. Analyzing the RDP of the shuffle model could
have several applications such as private statistics using interactive
schemes for heavy hitters, mean estimation, federated learning,
and distributed differentially private stochastic gradient descent
(DPSGD). This leads us to the central question addressed in this
paper:
1The mechanisms used have a long history including Randomized Response [41], but
were recently studied through the lens of local differential privacy (LDP).
 This work is licensed under a Creative Commons Attribution International 4.0 License. CCS '21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea. ¬© 2021 Copyright is held by the owner/author(s). ACM ISBN 978-1-4503-8454-4/21/11. https://doi.org/10.1145/3460120.3484794  Session 7D: Privacy for Distributed Data and Federated Learning CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea 2321The principal result in this paper is the first direct RDP guar-
antee for general discrete local randomization mechanisms in the
shuffle privacy model. In particular, given an arbitrary discrete local
mechanism with ùúñ0-LDP guarantee, we provide an RDP guarantee
for the shuffle model, as a function of ùúñ0 and the number of users
ùëõ; see Theorem 3.1. This can be seen as an amplification by shuf-
fling result for amplifying pure LDP guarantee to RDP guarantee
via shuffling. In contrast, the existing amplification by shuffling
results [7, 22, 24] amplify pure LDP guarantee to approximate DP
guarantee.
When numerically evaluating our bound, we save a factor of
8√ó compared to the state-of-the-art approximate DP guarantee for
shuffle models in [24]2 combined with strong composition, with
the number of iterations ùëá = 105, LDP parameter ùúñ0 = 0.5, and
number of clients ùëõ = 106; see Figure 4a in Section 4 for such ex-
ample regimes. Furthermore, characterizing the RDP of the shuffle
model enables us to compute the RDP of shuffling with Poisson
sub-sampling by using the results in [43]. We numerically show
that this approach can lead to at least 10√ó improvement in privacy
guarantee. This is for ùëá = 104, ùúñ0 = 3, and ùëõ = 106. The comparison
is with applying the strong composition theorem [33] after get-
ting the state-of-the-art approximate DP of the shuffle model given
in [24] with Poisson sub-sampling [35] (see Figure 5a in Section 4
for more such regimes). This in turn implies that we can accommo-
date at least 10√ó more interactions for the same privacy budget in
these cases. Moreover, our upper bounds also give several orders of
magnitude improvement over the simple RDP bound stated in [21,
Remark 1] (also stated in (9)) in several regimes (see, for example,
Figure 2e in Section 4). We also develop a lower bound for the RDP
for the shuffle model and numerically demonstrate that the gap is
small for many parameter regimes of interest.
In order to obtain our upper bound result, we develop new analy-
sis techniques which could be of independent interest. In particular,
we develop a novel RDP analysis for neighboring datasets with a
special structure (see Theorem 3.7), in which one of the datasets has
all the data points to be the same (see the definition in (12)). A key
technical result is then to relate the RDP of general neighboring
datasets to those with special structure (see Theorem 3.6).
Can we develop strong RDP privacy guarantees for gen-
eral local mechanisms in the shuffle privacy model?
‚Ä¢ For the RDP analysis of neighboring datasets with the above-
mentioned special structure, we first observe that the output
distribution of the shuffling mechanism is the multinomial
distribution. Using this observation, then we show that the
ratio of the distributions of the mechanism on special struc-
ture neighboring datasets is a sub-Gaussian random variable
(r.v.), and we can write the R√©nyi divergence of the shuffle
mechanism in terms of the moments of this r.v. Bounding the
moments of this r.v. then gives an upper bound on the RDP
for the special neighboring datasets. See the proof-sketch
of Theorem 3.7 in Section 3.3.2 and its complete proof in
Section 6.
‚Ä¢ We next connect the above analysis to the RDP computa-
tion for general neighboring datasets D = (ùëë1, . . . , ùëëùëõ) and
2We used the open source implementation for the privacy analysis in [24] available
from https://github.com/apple/ml-shuffling-amplification.
D‚Ä≤ = (ùëë1, . . . , ùëëùëõ‚àí1, ùëë‚Ä≤
ùëõ). To do so, a crucial observation is to
write the output distribution ùíëùëñ of the local randomizer R
on the ùëñ‚Äôth client‚Äôs data point ùëëùëñ (for any ùëñ ‚àà [ùëõ ‚àí 1]) as a
mixture distribution ùíëùëñ = ùëí‚àíùúñ0ùíë‚Ä≤
ùëõ + (1 ‚àí ùëí‚àíùúñ0) Àúùíëùëñ for some
Àúùíëùëñ.3 So, the number of clients that sample according to ùíë‚Ä≤
is concentrated around ùëí‚àíùúñ0ùëõ. Therefore, if we restrict the
dataset to these clients only, the resulting datasets will have
the special structure, and the size of that dataset will be con-
centrated around ùëí‚àíùúñ0ùëõ. Finally, in order to be able to reduce
the problem to the special case, we remove the effect of the
clients that do not sample according to ùíë‚Ä≤
ùëõ without affecting
the R√©nyi divergence. See the proof-sketch of Theorem 3.6
in Section 3.3.1 and its complete proof in Section 5.
ùëõ
Related Work
We give the most relevant work related to the paper and put our
contributions in the context of these works.
shuffle privacy model: As mentioned, the shuffle model of privacy
has been of significant recent interest [5‚Äì8, 13, 22, 25, 26]. However,
all the existing works in literature [7, 22, 24] only characterize
the approximate DP of the shuffle model ‚Äì among these, [24] is
the state-of-the-art, but as we show in our experiments, it yields
weaker results when combined with composition. To the best of
our knowledge, there is no bound on RDP of the shuffle model
in the literature except for the one mentioned briefly in a remark
in [22, Remark 1] (which is obtained by the standard conversion
results from DP to RDP) and we state it in (9) for comparison.
However, this bound is loose (e.g., see Figure 2e) and not useful for
conversion to approximate DP (e.g., see Figures 3a, 3c), as well as
for composition (e.g., see Figure 4e). Thus, our work makes progress
on this important open question of analyzing the RDP of the shuffle
model. Both [20] and [27] used advanced composition to analyze
privacy of shuffle models in federated learning; our results could
be adapted to enhance their privacy guarantees.
R√©nyi differential privacy: The work of Abadi et al. [1] provided
a methodology to get stronger composition results. Inherently, this
used R√©nyi divergence, and was later formalized in [36] which
defined R√©nyi differential privacy (RDP). RDP presents a unified
definition for several kinds of privacy notions including pure differ-
ential privacy (ùúñ-DP), approximate differential privacy ((ùúñ, ùõø)-DP),
and concentrated differential privacy (CDP) [11, 18]. As mentioned
earlier, RDP enables a stronger result for composition, through the
‚Äúmoment accounting‚Äù idea. Similarly, several works [37, 40, 43] have
shown that analyzing the RDP of subsampled mechanisms provides
a tighter bound on the total privacy loss than the bound that can be
obtained using the standard strong composition theorems. However,
to the best of our knowledge, RDP analysis of the shuffle model and
its use for composition in the shuffle model has not been studied. In
this paper, we analyze the RDP of the shuffle model, where we can
bound the approximate DP of a sequence of shuffle models using
the transformation from RDP to approximate DP [1, 2, 12, 40]. We
show that our RDP analysis provides a better bound on the total
3The idea of writing the distribution of the output of an LDP mechanism as a mixture
distribution was previously proposed in [7, 24]. However, the way these mixture
distributions are used in our RDP analysis is different from these works, including
what mixtures we create and how we use them.
Session 7D: Privacy for Distributed Data and Federated Learning CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea 2322privacy loss of composition than that can be obtained using the
standard strong composition theorems (see Section 4).
Discrete mechanisms: Many of the works in DP use specific ran-
domization mechanisms, adding noise using the Laplace or Gauss-
ian distributions. However, in many situations the data is inherently
discrete (e.g., see [12] and references therein) or compression causes
it to be so (e.g., see [27, 32] and references therein). It is therefore of
interest to directly analyze privacy of discrete randomization mech-
anisms. Such discrete mechanisms have been studied extensively
in shuffle models [5, 25], but for approximate DP. To the best of
our knowledge, RDP for general discrete mechanisms in the shuffle
privacy framework is new to our work.
Paper Organization
The paper is organized as follows. In Section 2, we give some prelim-
inary definitions and results from the literature and also formulate
our problem. In Section 3, we present our main results (two upper
bounds and one lower bound on RDP), along with a proof sketch of
the first upper bound. We also describe the two main ingredients in
its proof ‚Äì first is the reduction of computing RDP for the arbitrary
pairs of neighboring datasets to computing RDP for the special
pairs of neighboring datasets, and the second is computing RDP for
the special pairs of neighboring datasets. In Section 4, we present
several numerical results to demonstrate the advantages of our
bounds compared to the state-of-the-art. The rest of the sections
are devoted to the full proofs of our main results: Section 5 shows
the reduction of our general problem to the special case; Section 6
proves the RDP for the special case; Section 7 proves both our upper
bounds; and Section 8 proves our lower bound. In Section 9, we
conclude with a short discussion. Omitted details from the proofs
are provided in the appendices.
2 PRELIMINARIES AND PROBLEM
FORMULATION
We give different privacy definitions that we use in Section 2.1, some
existing results on RDP to DP conversion and RDP composition in
Section 2.2, and give our problem formulation in Section 2.3.
2.1 Privacy Definitions
In this subsection, we define different privacy notions that we will
use in this paper: local differential privacy (LDP), central differential
privacy (DP), and R√©nyi differential privacy (RDP).
Definition 1 (Local Differential Privacy - LDP [34]). For ùúñ0 ‚â•
0, a randomized mechanism R : X ‚Üí Y is said to be ùúñ0-local
differentially private (in short, ùúñ0-LDP), if for every pair of inputs
ùëë, ùëë‚Ä≤ ‚àà X, we have
‚àÄS ‚äÜ Y.
Pr[R(ùëë) ‚àà S] ‚â§ ùëíùúñ0 Pr[R(ùëë‚Ä≤) ‚àà S],
(1)
Let D = {ùëë1, . . . , ùëëùëõ} denote a dataset comprising ùëõ points