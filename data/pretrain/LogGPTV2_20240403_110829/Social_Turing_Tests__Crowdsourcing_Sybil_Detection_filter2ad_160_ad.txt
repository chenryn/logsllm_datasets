cious pieces of information from an OSN proﬁle. Instead,
what we can do is examine how consistent the reasons are
for each proﬁle across our test groups. If all the testers agree
on the reasons why a given proﬁle is suspicious, then that is
a strong indication that those reasons are correct.
To calculate consistency, we use the following proce-
dure.
In each test group, each Sybil is classiﬁed by N
testers. For all pairs of users in each group that classiﬁed
a particular Sybil proﬁle, we calculate the Jaccard similar-
ity coefﬁcient to look at overlap in their reasons, giving us
N ∗ (N − 1)/2 unique coefﬁcients. We then compute the
average of these coefﬁcients for each proﬁle. By computing
the average Jaccard coefﬁcient for each Sybil, we arrive at
a distribution of consistency scores for all Sybils for a given
test group.
Figure 5 shows the consistency distributions of the China
and US test groups. The results for the Indian test groups
are similar to US testers, and are omitted for brevity. The
Chinese turkers show the most disagreement: for 50% of
Sybils the average Jaccard coefﬁcient is ≤0.4. Chinese ex-
perts and all three US groups exhibit similar levels of agree-
ment: 50% of Sybils have coefﬁcients ≤0.5. The fraction of
Sybils receiving near complete disagreement (0.0) or agree-
ment (1.0) is negligibly small across all test groups.
Based on these results, we conclude that testers identify
Sybils for inconsistent reasons. Even though Table 4 shows
that each of the three available reasons receives a roughly
equal portion of votes overall, the reasons are assigned ran-
domly across Sybils in our dataset. This indicates that no
single proﬁle feature is a consistent indicator of Sybil activ-
ity, and that testers beneﬁt from having a large, diverse set
of information when making classiﬁcations. Note that this
provides further support that automated mechanisms based
on individual features are less likely to succeed, and also
explains the success of human subjects in detecting Sybils.
Answer Revisions. While taking our survey, testers had
the option of going back and changing classiﬁcations that
they have previously made. However, few took advantage
of this feature. This is not unexpected, especially for turk-
ers. Since turkers earn more if they work faster, there is
a negative incentive to go back and revise work. In total,
there were only 28 revisions by testers: 16 from incorrect to
correct, and 12 from correct to incorrect.
)
%
t
(
e
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
 100
 80
 60
 40
 20
 0
OSN Experience Education
Gender
)
%
t
(
e
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
 100
 80
 60
 40
 20
 0
OSN Experience Education
Gender
)
%
t
(
e
a
R
e
v
i
t
a
g
e
N
e
s
a
F
l
 100
 80
 60
 40
 20
 0
OSN Experience Education
Gender
Never
0-2 Years
2-5 Years
5-10 Years
Primary School
High School
Bachelors
Graduate
Female
Male
(a) US Turker
(b) India Turker
(c) Chinese Turker
Figure 7. False positive rates for turkers, broken down by demographic.
 100
 80
 60
 40
 20
 0
)
%
(
y
c
a
r
u
c
c
A
)
S
(
e
l
i
f
o
r
P
r
e
P
e
m
T
i
 100
 80
 60
 40
 20
 0
Accuracy
Time
 100
 80
 60
 40
 20
 0
)
%
(
y
c
a
r
u
c
c
A
)
S
(
e
l
i
f
o
r
P
r
e
P
e
m
T
i
 100
 80
 60
 40
 20
 0
Accuracy
Time
 100
 80
 60
 40
 20
 0
)
%
(
y
c
a
r
u
c
c
A
Accuracy
Time
)
S
(
e
l
i
f
o
r
P
r
e
P
e
m
T
i
 100
 80
 60
 40
 20
 0
 100
 80
 60
 40
 20
 0
)
%
(
y
c
a
r
u
c
c
A
)
S
(
e
l
i
f
o
r
P
r
e
P
e
m
T
i
 100
 80
 60
 40
 20
 0
Accuracy
Time
 1  2  3  4  5  6  7  8  9 10
 1  2  3  4  5  6  7  8  9 10
 1  2  3  4  5  6  7  8  9 10
 2  4  6  8  10  12
Profile Order
(a) Renren Expert
Profile Order
Profile Order
Profile Order
(b) Renren Turker
(c) Facebook US Expert
Figure 8. Accuracy and time on the nth proﬁle.
(d) Facebook US Turker
5 Turker Accuracy Analysis
5.2 Temporal Factors and Survey Fatigue
The end goal of our work is to create a crowdsourced
Sybil detection system. However, in Section 4 we observed
that turkers are not as accurate as experts. In this section,
we examine factors that may impact the accuracy of turk-
ers, and investigate ways to improve our Sybil detection
system. We start by looking at demographic factors. Next,
we examine proﬁle evaluation time to understand if turkers
are adversely affected by survey fatigue. Next, we examine
issues of turker selection. Will adding more turkers to the
crowd improve accuracy? What if we set a threshold and ﬁl-
ter out turkers that consistently perform poorly? Finally, we
calculate the per proﬁle accuracy of testers to detect “stealth
Sybils” that are undetectable by both experts and turkers.
5.1 Demographic Factors
First, we explore the impact of demographic factors on
the turker’s accuracy. We focus on false negative rates of
turkers, since their false positive rates are close to zero. Fig-
ure 7 shows the average false negative rate and standard de-
viation of turkers from China, US and India, broken down
by different demographics. Education has a clear impact on
false negatives: higher education level correlates with in-
creased ability to identify Sybils. The impact of OSN usage
experience is less clear. Chinese and US turker’s false neg-
ative rates decline as OSN experience increases, which is
expected. However, for Indian turkers there is no correla-
tion. Gender does not appear to impact false negatives in a
meaningful way. The results in Figure 7 indicate that turker
accuracy could be improved by ﬁltering out workers with
few years of OSN experience and low education level.
It is known that turkers try to ﬁnish tasks as quickly as
possible in order to earn more money in a limited amount
of time [16]. This leads to our next question: do turkers
spend less time evaluating proﬁles than experts, and does
this lead to lower accuracy? The issue of time is also related
to survey fatigue: does the accuracy of each tester decrease
over time due to fatigue and boredom?
To understand these temporal factors, we plot Figure 8,
which shows the average evaluation time and accuracy per
proﬁle “slot” for Chinese and US experts and turkers. The
x-axis of each subﬁgure denotes the logical order in which
testers evaluated proﬁles, e.g. “Proﬁle Order” n is the nth
proﬁle evaluated by each tester. Note that proﬁles are pre-
sented to each tester in random order, so each tester evalu-
ated a different proﬁle within each slot. Within each slot, we
calculate the average proﬁle evaluation time and accuracy
across all testers. 100% accuracy corresponds to all testers
correctly classifying the nth proﬁle they were shown. Al-
though experts evaluated >10 proﬁles each, we only show
the ﬁrst 10 to present a fair comparison versus the turkers.
The results for the Indian test groups are similar to the US
groups, and are omitted for brevity.
The ﬁrst important result from Figure 8 is that absolute
proﬁle evaluation time is not a good indicator of accuracy.
The Chinese experts exhibit the fastest evaluation times, av-
eraging one proﬁle every 23 seconds. However, they are
more accurate than Chinese turkers who spend more time
on each proﬁle. This pattern is reversed on Facebook: ex-
perts spend more time and are more accurate than turkers.
Next, we look for indications of survey fatigue. In all 4
subﬁgures of Figure 8, the evaluation time per proﬁle de-
creases over time. This shows that testers speed up as they
progress through the survey. As shown in the expert Fig-
ures 8(a) and 8(c), this speedup does not affect accuracy.
These trends continue through the evaluation of additional
proﬁles (10-50 for Chinese experts, 10-100 for US experts)
that are not shown here. However, for turkers, accuracy
does tend to decrease over time, as shown in Figures 8(b)
and 8(d). This demonstrates that turkers are subject to sur-
vey fatigue. The up-tick in Chinese turker accuracy around
proﬁle 10 is a statistical anomaly, and is not signiﬁcant.
5.3 Turker Selection
As demonstrated in Section 4.3, we can mitigate the clas-
siﬁcation errors of individuals by aggregating their votes to-
gether. This raises our next question: can we continue to
improve the overall accuracy of turkers by simply adding
more of them?
To evaluate this, we conducted simulations using the data
from our user study. Let C be the list of classiﬁcations re-