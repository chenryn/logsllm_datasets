title:Planck: millisecond-scale monitoring and control for commodity networks
author:Jeff Rasley and
Brent E. Stephens and
Colin Dixon and
Eric Rozner and
Wes Felter and
Kanak Agarwal and
John B. Carter and
Rodrigo Fonseca
Planck: Millisecond-scale Monitoring and Control for
Commodity Networks
Jeff Rasley†
Brent Stephens‡
Kanak Agarwal∗
Colin Dixon(cid:5)
Eric Rozner∗ Wes Felter∗
John Carter∗ Rodrigo Fonseca†
†Brown University
‡Rice University
∗IBM Research–Austin,TX
(cid:5)Brocade
ABSTRACT
Software-deﬁned networking introduces the possibility of building
self-tuning networks that constantly monitor network conditions and
react rapidly to important events such as congestion. Unfortunately,
state-of-the-art monitoring mechanisms for conventional networks
require hundreds of milliseconds to seconds to extract global net-
work state, like link utilization or the identity of “elephant” ﬂows.
Such latencies are adequate for responding to persistent issues, e.g.,
link failures or long-lasting congestion, but are inadequate for re-
sponding to transient problems, e.g., congestion induced by bursty
workloads sharing a link.
In this paper, we present Planck, a novel network measurement
architecture that employs oversubscribed port mirroring to extract
network information at 280 µs–7 ms timescales on a 1 Gbps com-
modity switch and 275 µs–4 ms timescales on a 10 Gbps commodity
switch, over 11x and 18x faster than recent approaches, respectively
(and up to 291x if switch ﬁrmware allowed buffering to be disabled
on some ports). To demonstrate the value of Planck’s speed and
accuracy, we use it to drive a trafﬁc engineering application that
can reroute congested ﬂows in milliseconds. On a 10 Gbps com-
modity switch, Planck-driven trafﬁc engineering achieves aggregate
throughput within 1–4% of optimal for most workloads we evalu-
ated, even with ﬂows as small as 50 MiB, an improvement of up to
53% over previous schemes.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tions—Network monitoring; C.4 [Performance of Systems]: Meas-
urement techniques
Keywords
Networking Measurement; Software-Deﬁned Networking; Trafﬁc
Engineering
1.
INTRODUCTION
Modern data center networks operate at speeds and scales that
make it impossible for human operators to respond to transient
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2836-4/14/08 ...$15.00.
http://dx.doi.org/10.1145/2619239.2626310.
problems fast enough, e.g., congestion induced by workload dy-
namics. Gone are the days of monitoring and tuning networks at
the granularity of days, hours, and minutes [4]. Even reacting in
seconds can cause signiﬁcant disruption, so data center networks
are frequently constructed to have full bisection bandwidth to avoid
congestion [1, 13]. This brute force approach adds substantial cost,
results in poorly utilized networks, and only reduces the likelihood
of issues [14]. Instead, if done quickly enough, detecting congestion
and routing trafﬁc to avoid it could both reduce costs and improve
performance.
Software-deﬁned networking (SDN) allows for this kind of au-
tonomous, self-tuning network that constantly monitors network
conditions and reacts rapidly to problems. Previous work has demon-
strated that routes can be installed by an SDN controller in tens of
milliseconds [11, 39], but state-of-the-art network measurement sys-
tems typically spend hundreds of milliseconds or more collecting
statistics [2, 4, 6, 10, 41], which limits the minimum latency of any
autonomous measurement-decision-actuation network management
control loop. In modern 10 Gbps and 40 Gbps networks, this is too
slow to react to any but the largest network events, e.g., link failures,
VM migrations, and bulk data movement. Problems induced by tran-
sient conditions, e.g., conﬂicting small-to-medium ﬂows, cannot be
identiﬁed fast enough to respond before they disappear, resulting in
frequent bursts of congestion. To support future autonomous SDNs,
a much lower latency network monitoring mechanism is necessary.
This paper introduces Planck, a network measurement architec-
ture that extracts network information at 280 µs–7 ms timescales
on a 1 Gbps commodity switch and 275 µs–4 ms timescales on a
10 Gbps commodity switch, over an order of magnitude (11–18x)
faster than state-of-the-art, see Table 1. Planck achieves this level of
performance through a novel use of the port mirroring mechanism.
Port mirroring is supported by most modern switches to enable a
variety of network monitoring and security applications. When port
mirroring is enabled, trafﬁc destined for a single port is mirrored
to a monitoring port that is connected to a monitoring or intrusion
detection system. Planck repurposes this existing port mirroring
capability to support an extremely high rate of packet sampling. In
Planck, multiple (or all) ports are mirrored to a single monitoring
port, which introduces a problem: the total trafﬁc ﬂowing through
the switch, and thus mirrored to the monitoring port, often will
exceed the capacity of the monitoring port. When this happens, the
switch mirrors as much as it can and drops the rest, in effect provid-
ing a sample of the monitored trafﬁc. In our experimentation, the
buffering and drop behaviors of two different commercial switches
(IBM RackSwitch G8264 and Pronto 3290) did not persistently
fail to sample speciﬁc ﬂows and provided samples that allowed for
accurate estimates of link utilization and ﬂow rates.
Speed
Slowdown vs
10 Gbps Planck
1/15–1/5x
1/15–1/4x
1x
1.7x
18x
24x
45x
275–850 µs
280–1150 µs
< 4.2 ms
< 7.2 ms
77.4 ms
100 ms
190 ms
System
Planck 10 Gbps minbuffer
Planck 1 Gbps minbuffer
Planck 10 Gbps
Planck 1 Gbps
Helios [10]
sFlow/OpenSample [41]
Mahout Polling† [5]
(implementing Hedera)
DevoFlow Polling† [6]
Hedera [2]
Table 1: A comparison of measurement speed and slowdown to
gather accurate per-ﬂow throughput information at each link
compared to Planck on a 10 Gbps switch. Planck is 11–18x
faster than Helios, the next fastest scheme. The “minbuffer”
rows show how fast Planck could be on switches that were con-
ﬁgured with minimal buffering for mirror ports, a feature our
ﬁrmware does not expose. A † indicates the value is not the pri-
mary implementation proposed in the corresponding work, but
is a reported value or estimate. For more details see § 5.5.
119–3570x
1190x
500 ms–15 s
5 s
Each monitoring port is connected to a collector running on a
separate server that uses netmap [30] to process trafﬁc sent by a
monitoring port at line-rate. A single server hosts many collectors.
Planck collectors can record the stream of sampled packets and
perform lightweight analysis of the stream to extract information
of interest. Collectors export a number of capabilities, including
sFlow-style sampling, extremely low latency link utilization and
ﬂow rate estimation, and the ability to capture and dump raw packet
samples of any trafﬁc in the network. Applications can query the
collector for statistics or subscribe and respond to notiﬁcations from
the collector, e.g., when a speciﬁc level of congestion is detected.
To support these capabilities, Planck requires as little as one port
per switch and one server per fourteen switches to handle samples.
To demonstrate the value of Planck’s speed and accuracy, we
built a trafﬁc engineering application that (i) uses Planck to monitor
the network, (ii) decides when conditions warrant reconﬁguring the
network, and, (iii) when so, executes the preferred reconﬁguration.
This Planck-driven trafﬁc engineering achieves aggregate through-
put within 1–4% of optimal for most workloads we evaluated on a
10 Gbps commodity switch, even with ﬂows as small as 50 MiB, an
improvement of up to 53% over previous schemes.
This paper makes four main contributions:
1. We present a novel measurement platform, Planck, that uses
oversubscribed port mirroring and high speed packet process-
ing to provide millisecond-scale network monitoring.
2. We provide a detailed analysis of switch packet drop and
buffering policies and their impact on monitoring latency.
3. We develop an algorithm to accurately estimate a ﬂow’s
throughput within a 200–700 µs timescale, using samples
obtained from an unknown sampling function.
4. We demonstrate the feasibility of millisecond timescale trafﬁc
engineering on commercial 10 Gbps switches using a com-
bination of Planck and an SDN application that responds to
congestion using ARP messages to switch rapidly between
pre-installed alternate routes.
The remainder of this paper is organized as follows. In Section 2
we present background on low-latency network measurement. Sec-
tions 3 and 4 describe the design and implementation of Planck.
Section 5 evaluates Planck including the impact of oversubscribed
port mirroring on network trafﬁc and the nature of Planck’s samples.
Section 6 describes two applications that use Planck, a vantage point
monitor and trafﬁc engineering tool. We evaluate our Planck-based
trafﬁc engineering application in Section 7 and then discuss related
work in Section 8. Finally, we provide a retrospective discussion in
Section 9 and conclude in Section 10.
2. BACKGROUND
Network measurement is too broad a ﬁeld to fully characterize
here. We focus on measurement techniques that are useful in dis-
covering either link utilization or the most signiﬁcant ﬂows crossing
each link at ﬁne time granularities, i.e., seconds or faster. We omit a
discussion of probe-based measurement techniques. While they can
discover network conditions, e.g., congestion, they typically cannot
determine the speciﬁc trafﬁc causing those conditions.
2.1 Packet Sampling
In packet sampling, switches forward one-in-N packets they re-
ceive, along with metadata, such as the packet’s input port, to a
collector. The collector then estimates the trafﬁc on the network by
multiplying the packet and byte counts from the samples by N [29].
Sampling forms the core of the sFlow [32] standard that many
switches implement. sFlow typically strips off the packet payload
and adds metadata such as a switch ID, the sampling rate used when
this packet was selected, and the output port(s) selected for this
packet. Sampled packets are then sent out via the control plane CPU
of the switch. Figure 1(a) shows the path samples take when using
sFlow with the dashed line labeled ‘sFlow’.
Recent work [41] has reported that involving the switch control
plane CPU and the PCI bus connecting them limits the achievable
sampling rate. In the case of the IBM RackSwitch G8264 [16],
the maximum rate is about 300 samples per second. This low
sample rate results in high estimation error unless samples are ag-
gregated over long periods of time, i.e, a second or more. The
error for a throughput estimate from s samples is approximately
196 ·(cid:112)1/s [29], so even if all 300 samples in a second come from
a single link, that link’s estimated load will be off by about 11%. In
more realistic scenarios the error will be noticeably worse: the col-
lector will have to wait longer than a second to report information.
2.2 Port Counters
Switches usually maintain counters that track the number of bytes
and packets that are sent and received for each port. While these
counters provide little direct insight into the ﬂows crossing each
link1, they can be periodically polled to infer link utilization over
time. Port counters can be read via most existing switch interfaces
including SNMP [38], OpenFlow [26], and sFlow [32].
2.3 Flow Counters
Many switches track the number of packets and bytes handled by
individual match-action table entries, which can be used to track indi-
vidual ﬂows. When available, these counters are exported by query-
ing so-called “ACL rules”, OpenFlow rules, and/or NetFlow [3].
Flow-based counters have a number of limitations. First, ACL/Open-
Flow tables are typically small, e.g., the IBM Rackswitch G8264 sup-
ports only about 1,000 ﬂow-granularity rules. Also, many switches
do not support fast ﬂow counter polling—it can take seconds to
read all of them [6]. More recent work indicates that it takes 75–
200 ms [5, 10] to extract ﬂow counters, as shown in Table 1.
1The ﬁeld of network tomography provides some insights into how
one might deduce ﬂows from port counters. These approaches are
usually time-intensive and at best give probabilistic information at
host-pair granularity [22].
rored ports is light, all trafﬁc is forwarded out the monitor port(s).
However, when incoming trafﬁc exceeds the capacity of the monitor