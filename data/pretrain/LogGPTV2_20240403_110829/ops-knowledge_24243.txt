User: I have 100 servers in my cluster.
At time 17:35:00, all 100 servers are provided with data (of size 1[MB]). Each server processes the data, and produces an output of about 40[MB]. The processing time for each server is 5[sec].
At time 17:35:05 (5[sec] later), there's a need for a central machine to read all the output from all 100 servers (remember, the total size of data is: 100 [machines] x 40 [MB] ~ 4[GB]), aggregate it, and produce an output. 
It is of high importance that the entire process of gathering the 4[GB] data from all 100 servers takes as little time as possible. How do I go about solving this problem? 
Are there any existing tools (ideally, in python, but would consider other solutions)  that can help?
Assistant: Experiment!
Other answers have included tips on what to experiment with, but you might solve the problem the most straight-forward way and use that as your baseline.
You have 1meg producing 40meg of output on each server - experiment with each server compressing the data to be sent. (That compression might be free-ish if compression is part of your file system).
Latency - it is never zero.
Can you change your algorithms?
Can you do some sort of hierarchical merging of outputs rather than one CPU doing all 4Gigs at once? (Decimation in time).
It is possible to buy quad socket servers with 80 cores - would that be quicker as storage could be local, and you might configure the one machine with a lot of ram.