# 30丨关联规则挖掘（上）：如何用Apriori发现用户购物规则？今天我来带你进行关联规则挖掘的学习，关联规则这个概念，最早是由 Agrawal等人在 1993 年提出的。在 1994 年 Agrawal 等人又提出了基于关联规则的Apriori 算法，至今 Apriori 仍是关联规则挖掘的重要算法。关联规则挖掘可以让我们从数据集中发现项与项（item 与item）之间的关系，它在我们的生活中有很多应用场景，"购物篮分析"就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。所以说，关联规则挖掘是个非常有用的技术。在今天的内容中，希望你能带着问题，和我一起来搞懂以下几个知识点：1.  搞懂关联规则中的几个重要概念：支持度、置信度、提升度；2.  Apriori 算法的工作原理；3.  在实际工作中，我们该如何进行关联规则挖掘。
## 搞懂关联规则中的几个概念我举一个超市购物的例子，下面是几名客户购买的商品列表：![](Images/e26726e7cc6892fc4631920318379727.png){savepage-src="https://static001.geekbang.org/resource/image/f7/1c/f7d0cc3c1a845bf790b344f62372941c.png"}\**什么是支持度呢？**支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。在这个例子中，我们能看到"牛奶"出现了 4 次，那么这 5笔订单中"牛奶"的支持度就是 4/5=0.8。``{=html}同样"牛奶 + 面包"出现了 3 次，那么这 5 笔订单中"牛奶 + 面包"的支持度就是3/5=0.6。**什么是置信度呢？**它指的就是当你购买了商品 A，会有多大的概率购买商品 B，在上面这个例子中：置信度（牛奶→啤酒）=2/4=0.5，代表如果你购买了牛奶，有多大的概率会购买啤酒？置信度（啤酒→牛奶）=2/3=0.67，代表如果你购买了啤酒，有多大的概率会购买牛奶？我们能看到，在 4 次购买了牛奶的情况下，有 2 次购买了啤酒，所以置信度(牛奶→啤酒)=0.5，而在 3 次购买啤酒的情况下，有 2次购买了牛奶，所以置信度（啤酒→牛奶）=0.67。所以说置信度是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。**什么是提升度呢？**我们在做商品推荐的时候，重点考虑的是提升度，因为提升度代表的是"商品 A的出现，对商品 B 的出现概率提升的"程度。还是看上面的例子，如果我们单纯看置信度(可乐→尿布)=1，也就是说可乐出现的时候，用户都会购买尿布，那么当用户购买可乐的时候，我们就需要推荐尿布么？实际上，就算用户不购买可乐，也会直接购买尿布的，所以用户是否购买可乐，对尿布的提升作用并不大。我们可以用下面的公式来计算商品A 对商品 B 的提升度：提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)这个公式是用来衡量 A 出现的情况下，是否会对 B 出现的概率有所提升。所以提升度有三种可能：1.  提升度 (A→B)\>1：代表有提升；2.  提升度 (A→B)=1：代表有没有提升，也没有下降；3.  提升度 (A→B)\ 搜索工具包。![](Images/7ccf40fda2145b1cd79184ddf66c6dd4.png){savepage-src="https://static001.geekbang.org/resource/image/76/c7/76a3b34beccbe7b69a11951b4efd80c7.png"}\这个网站提供的工具包都是 Python 语言的，你能找到 8 个 Python 语言的Apriori 工具包，具体选择哪个呢？建议你使用第二个工具包，即efficient-apriori。后面我会讲到为什么推荐这个工具包。首先你需要通过 pip install efficient-apriori 安装这个工具包。然后看下如何使用它，核心的代码就是这一行：    itemsets, rules = apriori(data, min_support,  min_confidence)其中 data 是我们要提供的数据集，它是一个 list 数组类型。min_support参数为最小支持度，在 efficient-apriori 工具包中用 0 到 1的数值代表百分比，比如 0.5 代表最小支持度为 50%。min_confidence是最小置信度，数值也代表百分比，比如 1 代表 100%。``{=html}关于支持度、置信度和提升度，我们再来简单回忆下。支持度指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的概率越大。置信度是一个条件概念，就是在 A 发生的情况下，B 发生的概率是多少。提升度代表的是"商品 A 的出现，对商品 B 的出现概率提升了多少"。接下来我们用这个工具包，跑一下上节课中讲到的超市购物的例子。下面是客户购买的商品列表：![](Images/2c1d637a7aa39a347dca3db28f8b236d.png){savepage-src="https://static001.geekbang.org/resource/image/a4/a6/a48f4a2961c3be811431418eb84aeaa6.png"}具体实现的代码如下：    from efficient_apriori import apriori
# 设置数据集data = [('牛奶','面包','尿布'),           ('可乐','面包', '尿布', '啤酒'),           ('牛奶','尿布', '啤酒', '鸡蛋'),           ('面包', '牛奶', '尿布', '啤酒'),           ('面包', '牛奶', '尿布', '可乐')]
# 挖掘频繁项集和频繁规则itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)print(itemsets)print(rules) 运行结果：    {1: {('啤酒',): 3, ('尿布',): 5, ('牛奶',): 4, ('面包',): 4}, 2: {('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, ('牛奶', '面包'): 3}, 3: {('尿布', '牛奶', '面包'): 3}}[{啤酒} -> {尿布}, {牛奶} -> {尿布}, {面包} -> {尿布}, {牛奶, 面包} -> {尿布}你能从代码中看出来，data 是个 List数组类型，其中每个值都可以是一个集合。实际上你也可以把 data数组中的每个值设置为 List 数组类型，比如：    data = [['牛奶','面包','尿布'],           ['可乐','面包', '尿布', '啤酒'],           ['牛奶','尿布', '啤酒', '鸡蛋'],           ['面包', '牛奶', '尿布', '啤酒'],           ['面包', '牛奶', '尿布', '可乐'两者的运行结果是一样的，efficient-apriori工具包把每一条数据集里的项式都放到了一个集合中进行运算，并没有考虑它们之间的先后顺序。因为实际情况下，同一个购物篮中的物品也不需要考虑购买的先后顺序。而其他的 Apriori算法可能会因为考虑了先后顺序，出现计算频繁项集结果不对的情况。所以这里采用的是efficient-apriori 这个工具包。
## 挖掘导演是如何选择演员的在实际工作中，数据集是需要自己来准备的，比如今天我们要挖掘导演是如何选择演员的数据情况，但是并没有公开的数据集可以直接使用。因此我们需要使用之前讲到的Python 爬虫进行数据采集。不同导演选择演员的规则是不同的，因此我们需要先指定导演。数据源我们选用豆瓣电影。先来梳理下采集的工作流程。首先我们先在搜索框中输入导演姓名，比如"宁浩"。![](Images/411ab0604fc657b4009a27738495fe18.png){savepage-src="https://static001.geekbang.org/resource/image/ea/ef/eaba9861825a38b6fbd5af1bff7194ef.png"}\页面会呈现出来导演之前的所有电影，然后对页面进行观察，你能观察到以下几个现象：1.  页面默认是 15 条数据反馈，第一页会返回 16    条。因为第一条数据实际上这个导演的概览，你可以理解为是一条广告的插入，下面才是真正的返回结果。2.  每条数据的最后一行是电影的演出人员的信息，第一个人员是导演，其余为演员姓名。姓名之间用"/"分割。有了这些观察之后，我们就可以编写抓取程序了。在代码讲解中你能看出这两点观察的作用。抓取程序的目的是为了生成宁浩导演（你也可以抓取其他导演）的数据集，结果会保存在csv 文件中。完整的抓取代码如下：    
# -*- coding: utf-8 -*-
# 下载某个导演的电影数据集from efficient_apriori import apriorifrom lxml import etreeimport timefrom selenium import webdriverimport csvdriver = webdriver.Chrome()
# 设置想要下载的导演 数据集director = u'宁浩'
# 写 CSV 文件file_name = './' + director + '.csv'base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&cat=1002&start='out = open(file_name,'w', newline='', encoding='utf-8-sig')csv_write = csv.writer(out, dialect='excel')flags=[]
# 下载指定页面的数据def download(request_url):driver.get(request_url)time.sleep(1)html = driver.find_element_by_xpath("//*").get_attribute("outerHTML")html = etree.HTML(html)
# 设置电影名称，导演演员 的 XPATHmovie_lists = html.xpath("/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']")name_lists = html.xpath("/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']")