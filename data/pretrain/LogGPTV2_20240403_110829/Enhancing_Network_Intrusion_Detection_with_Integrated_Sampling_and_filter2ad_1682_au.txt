### Compromised Features and Signature Generation

The compromised features [3, 5, 2, 25, 10] can identify the specific characteristics of a worm's request that cause it to exploit the monitored software. These features are likely to be invariant and useful in creating a signature. This approach is less susceptible to being fooled by the worm into using spurious features in the signature, as it will ignore features that do not affect whether the vulnerable software is actually exploited. The identified features can also be more expressive than simply the presence or absence of tokens, specifying, for example, the minimum length of a protocol field necessary to trigger a buffer overflow.

### Execution Filtering

In this paper, we aim to address the problem of automatically generating worm signatures. Recent research has proposed using semantic analysis to generate execution filters, which specify the location of a vulnerability and how to detect when it is exploited by automatically emulating [20] or rewriting [14] that part of the program.

### Conclusion

Learning an accurate classifier from data largely controlled by an adversary is a challenging task. In this work, we have shown that even a deceptive adversary, who provides correctly labeled but misleading training data, can prevent or severely delay the generation of an accurate classifier. We have concretely demonstrated this concept with highly effective attacks against recently proposed automatic worm signature generation algorithms.

When designing a system to learn in such an adversarial environment, one must account for the worst possible training data provided in the worst possible order. Few machine learning algorithms provide useful guarantees in such scenarios.

The problem of a deceptive adversary must be considered in the design of malicious classifier generation systems. Promising approaches include designing learning algorithms that are robust to maliciously generated training data, training using malicious data samples not generated by a malicious source, and performing deeper analysis of the malicious training data to determine the semantic significance of features before including them in a classifier.

### References

1. Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. D. Tygar. Can machine learning be secure? In ASIA CCS, March 2006.
2. David Brumley, James Newsome, Dawn Song, Hao Wang, and Somesh Jha. Towards automatic generation of vulnerability-based signatures. In IEEE Symposium on Security and Privacy, 2006.
3. Manuel Costa, Jon Crowcroft, Miguel Castro, and Antony Rowstron. Vigilante: End-to-end containment of internet worms. In SOSP, 2005.
4. Jedidiah R. Crandall and Fred Chong. Minos: Architectural support for software security through control data integrity. In International Symposium on Microarchitecture, December 2004.
5. Jedidiah R. Crandall, Zhendong Su, S. Felix Wu, and Frederic T. Chong. On deriving unknown vulnerabilities from zero-day polymorphic and metamorphic worm exploits. In 12th ACM Conference on Computer and Communications Security (CCS), 2005.
6. Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. Adversarial classification. In Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2004.
7. Hyang-Ah Kim and Brad Karp. Autograph: toward automated, distributed worm signature detection. In 13th USENIX Security Symposium, August 2004.
8. Christian Kreibich and Jon Crowcroft. Honeycomb - creating intrusion detection signatures using honeypots. In HotNets, November 2003.
9. Zhichun Li, Manan Sanghi, Yan Chen, Ming-Yang Kao, and Brian Chavez. Hamsa: Fast signature generation for zero-day polymorphic worms with provable attack resilience. In IEEE Symposium on Security and Privacy, May 2006.
10. Zhenkai Liang and R. Sekar. Fast and automated generation of attack signatures: A basis for building self-protecting servers. In 12th ACM Conference on Computer and Communications Security (CCS), 2005.
11. N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear threshold algorithm. Machine Learning, 2(285-318), 1988.
12. N. Littlestone. Redundant noisy attributes, attribute errors, and linear-threshold learning using winnow. In Fourth Annual Workshop on Computational Learning Theory, pages 147–156, 1991.
13. Daniel Lowd and Christopher Meek. Adversarial learning. In Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2005.
14. James Newsome, David Brumley, and Dawn Song. Vulnerability-specific execution filtering for exploit prevention on commodity software. In 13th Symposium on Network and Distributed System Security (NDSS’06), 2006.
15. James Newsome, Brad Karp, and Dawn Song. Polygraph: Automatically generating signatures for polymorphic worms. In IEEE Symposium on Security and Privacy, May 2005.
16. James Newsome and Dawn Song. Dynamic taint analysis for automatic detection, analysis, and signature generation of exploits on commodity software. In 12th Annual Network and Distributed System Security Symposium (NDSS), February 2005.
17. delusive (definition). In Oxford English Dictionary, Oxford University Press, 2006.
18. Roberto Perdisci, David Dagon, Wenke Lee, Prahlad Fogla, and Monirul Sharif. Misleading worm signature generators using deliberate noise injection. In IEEE Symposium on Security and Privacy, May 2006.
19. Yann Ramin. ATPhttpd. http://www.redshift.com/∼yramin/atp/atphttpd/.
20. Stelios Sidiroglou, Michael E. Locasto, Stephen W. Boyd, and Angelos D. Keromytis. Building a reactive immune system for software services. In USENIX Annual Technical Conference, 2005.
21. Sumeet Singh, Cristian Estan, George Varghese, and Stefan Savage. Automated worm fingerprinting. In 6th ACM/USENIX Symposium on Operating System Design and Implementation (OSDI), December 2004.
22. S. Staniford, D. Moore, V. Paxson, and N. Weaver. The top speed of flash worms. In ACM CCS WORM, 2004.
23. G. Edward Suh, Jaewook Lee, and Srinivas Devadas. Secure program execution via dynamic information flow tracking. In ASPLOS, 2004.
24. Yong Tang and Shigang Chen. Defending against internet worms: A signature-based approach. In IEEE INFOCOM, March 2005.
25. Jun Xu, Peng Ning, Chongkyung Kil, Yan Zhai, and Chris Bookholt. Automatic diagnosis and response to memory corruption vulnerabilities. In 12th Annual ACM Conference on Computer and Communication Security (CCS), 2005.

### Anomaly Detector Performance Evaluation Using a Parameterized Environment

**Abstract**

Over the years, intrusion detection has matured into a field replete with anomaly detectors of various types. These detectors are tasked with detecting computer-based attacks, insider threats, worms, and more. Their abundance easily prompts the question: is anomaly detection improving in efficacy and reliability? Current evaluation strategies may provide answers; however, they suffer from problems. For example, they produce results that are only valid within the evaluation data set and provide very little diagnostic information to tune detector performance in a principled manner.

This paper studies the problem of acquiring reliable performance results for an anomaly detector. Aspects of a data environment that will affect detector performance, such as the frequency distribution of data elements, are identified, characterized, and used to construct a synthetic data environment to assess a frequency-based anomaly detector. In a series of experiments that systematically map out the detector’s performance, areas of detection weaknesses are exposed, and strengths are identified. Finally, the extensibility of the lessons learned in the synthetic environment are observed using real-world data.

**Keywords:** anomaly detection, performance modeling, IDS evaluation, tuning.

### Introduction

A web search on anomaly detection will attest to the prevalence of anomaly detectors and their application towards the detection of worms, insider threats, and computer attacks. However, the increasing awareness by the mainstream community regarding the shortcomings of anomaly detection is noteworthy. Articles questioning the efficacy of the anomaly detection approach highlight the issue of progress: have we improved since Denning’s seminal paper, and if so, how much progress has been made?

One of the most fundamental ways of measuring progress is to evaluate a detector and benchmark its performance. It is particularly important that a detector’s performance is benchmarked in a way that can be described as robust. This means that the results of the evaluation strategy should be:

- Repeatable: to allow for independent validation.
- Reliable: performance results should be well-characterized to remain useful and valid outside the purview of the evaluation process itself.
- Informative: evaluation results should provide an understanding of the causes underlying performance behaviors, thereby facilitating improvements.

Current anomaly-detection evaluation strategies do not satisfy these criteria. The results from current strategies are typically not repeatable (e.g., due to unavailability of evaluation data sets, poorly documented evaluation methodologies, etc.), not reliable (an anomaly detector that performs well in one environment will not necessarily perform well in another environment), and not informative (hit, miss, and false alarm rates alone do not explain why a detector may have performed poorly). To give an example, the performance results reported in the literature for a particular anomaly-based intrusion detection system were accompanied by a disclaimer stating, “It is not known what effect different training sets would have on the results presented in this paper [8].” In short, current evaluation strategies make it difficult to measure progress.

One of the reasons for this is that current schemes rarely consider or measure phenomena in the data environment that affect detector performance, such as the characteristics of the background data or the characteristics of attack manifestation. If the manifestation of an attack in a data stream is not identified and characterized, it will be difficult to know why an anomaly detector responded weakly, for example, to the presence of that attack. If the detector’s response is weak, causing the attack to be missed, the mere act of incrementing the “miss” count is not sufficient to understand what caused the attack to be missed or what is needed to mitigate the condition.

Furthermore, the results of current evaluation strategies are also used to tune detector performance, e.g., by allowing a defender to select the detection threshold associated with the most acceptable operating point on a receiver operating characteristic (ROC) curve. However, detection thresholds and other detector parameters influencing performance are often set based on the intuition of the detector’s designer given a handful of test cases [1, 2]. No knowledge of environmental influences on detector performance is acquired or used to guide the tuning process. This introduces uncertainty into the final results because if the data environment changes, e.g., if the attacker’s behavior differs from those in the test cases used to tune the detector, the detector may no longer be optimally tuned for detecting the attacker. It would seem prudent to characterize the data environment in which a detector is deployed to provide some context with which to describe a detector’s behavior.

This paper describes an evaluation strategy aimed at producing results that are repeatable, reliable, and informative. The anomaly detector evaluated in this study (NIDES), was chosen for its simplicity and for the wealth of information readily available about it. Not only is there substantial information regarding the algorithm, but there are also numerous reports documenting various evaluation results for the detector. A synthetic evaluation environment was built around this detector, to cover a wide range of potential environmental conditions in which the detector may be deployed.

This study makes two contributions. First, the detector’s blind spots and sensitivities to various forms of anomalies are identified. Second, diagnostic information is provided to explain why the detector performed well or poorly. Evidence is provided, showing that these results extend to arbitrary data sets.

### Problem, Approach, and Rationale

This paper addresses the problem of acquiring robust evaluation results for an anomaly detector. The approach involves creating a synthetic environment in which to assess the performance of an anomaly detector.

There are two reasons to use a synthetic environment: (1) to assure control over the various artifacts within a data environment that will affect the detector, and (2) to establish ground truth. The first reason acknowledges the influence of the data environment on detector performance. Variables in the data environment such as the distribution of the background test data, the training data, and the anomalies all contribute to a detector’s response. It is possible for a given detector to be more sensitive to certain characteristics in the data environment than other detectors are. For example, a Markov-based detector is more sensitive to changes in the frequencies of data elements than a sequence-based detector, like stide [6], would be. This sensitivity can cause a Markov-based detector to produce more false alarms than stide due to frequency fluctuations in the test data (this phenomenon was observed and documented in [13]).

The second reason for using a synthetic environment is the determination of ground truth. Ground truth simply means knowing the identity of every event that an anomaly detector has to make a decision upon so that it can be determined whether the detector is accurate in its decision. Accuracy in performance evaluations requires that ground truth be correctly established.

In intrusion detection literature, ground truth data for anomaly detector evaluation typically comprise training data, i.e., data collected in the absence of attacks, and test data, i.e., data collected in the presence of attacks [5, 17]. The problem with this scheme is that there is no guarantee that the data collected in the presence of attacks will actually contain manifestations of that attack. It is possible that the attack does not manifest in the kind of data being collected, e.g., CPU usage data for detecting password crackers is not logged in BSM data (the BSM Basic Security Module is the security auditing mechanism for Sun systems). It is therefore important to clearly establish that each event in the evaluation data stream is or is not the result of an attack.

It should be noted that anomaly detectors directly detect anomalies, not attacks [14]. Assessing an anomaly detector should therefore be focused on what kinds of anomalies a detector detects, and how well it detects them. It makes more sense for an anomaly detector to be assessed on the events that it directly detects (anomalies) rather than events that it does not directly detect (attacks). For this reason, ground truth in this study is anomaly-based. This means that the ability of the detector to detect anomalies is evaluated; therefore, each event in the assessment data is marked as either anomalous or not.

The assessment strategy proposed in this paper is demonstrated using a re-implementation of the statistical anomaly detection component of NIDES [10], specifically the portion for processing categorical data. The re-implemented detector will be referred to as RIDES (Re-implementation of IDES) and is an example of a frequency-based detector, i.e., a detector that employs relative frequencies in its detection algorithm. The assessment will map the performance of RIDES over a varying range of data characteristics, identify the detector’s blind spots, and finally determine the parameter values that would produce the best performance in various environments, i.e., tune the detector.

### Related Work

In the intrusion detection literature, the most common method of evaluating detector performance can be summarized as follows [6, 7, 8, 16, 11, 2, 1]: sets of normal data, data obtained in the absence of intrusions or attacks, and intrusive data, data obtained in the presence of attacks, are collected. The anomaly-based intrusion detection system is trained on the normal data and then tested on test data that contains either intrusive data only or some mixture of normal and intrusive data. The success of the detection algorithm is typically measured in terms of hit, miss, and false alarm rates and charted on an ROC curve, with the ideal result being 100% hits and 0% misses and false alarms. The idea is then to select a point where the performance of the detector is most suitable to the defender, or to observe performance trends over a range of values and compare those trends with trends of other detectors deployed on the same data set.

In some cases, separate experiments are carried out to chart false alarm rates by deploying the anomaly-based intrusion detection system on purely normal data, i.e., where both training and test data consist of different sets of normal behavior only. Since anomaly detectors are typically designed with various parameters, e.g., a window size or a neural-network learning constant, this evaluation strategy may be repeated over a set of parameter values.

As previously discussed, these strategies are limited in that they say nothing about the detector’s performance on other data sets. In short, all that can be determined from the results of such an evaluation procedure is that a set of anomalies were detected, some or none of which were caused by the attack. This does not say much about the performance of a detector even with regard to detecting attacks, because it is not clear if the anomalies detected were really caused by the attacks or by a number of other reasons, e.g., poorly chosen training data or faulty system monitoring sensor, etc.

The most well-documented evaluation scheme described for NIDES was performed by SRI [1, 2]. The evaluation involved human experts who modified the detector’s configuration parameters after each of three experiments—concept, verification, and refinement. The goal was to determine the best configurations for NIDES within the context of detecting “when a computer system was used for other than authorized applications.” Detector performance was evaluated and improved after each of the three experiments by changing the values of detector parameters such as the short-term half-life. At the end of the entire evaluation, it was found empirically that shorter half-lives gave better false-positive rates. However, because these results may only be valid for the evaluation data set used, we build on this work by evaluating NIDES in a well-characterized data environment; hence, we can begin to understand how the detector’s intrinsic biases can be counterbalanced when it is deployed in another data environment.

### Description of RIDES

To demonstrate our tuning methodology with a concrete example, we have developed a detector we call RIDES, which is a re-implementation of a portion of the statistical anomaly detection component of NIDES [10]. There are two main components in IDES/NIDES: an expert system and a statistical anomaly detector. The expert system uses pre-defined rules to detect known patterns of behavior associated with intrusions, while the statistical anomaly detector is tasked to detect novel or previously unseen attacks by looking for deviations from known behavior. The statistical anomaly detector component in NIDES monitors both numerical and categorical data.