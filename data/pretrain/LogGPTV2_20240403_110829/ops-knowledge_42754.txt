User: I am trying to create a basic ETL on AWS platform, which uses python.
In a S3 bucket (lets call it "A") I have lots of raw log files, gzipped.
What I would like to do is to have it periodically (=data pipeline) unzipped, processed by a python script which will reformat the structure of every line, and output it to another S3 bucket ("B"), preferably as gzips of the same log files originating in the same gzip in A, but that's not mandatory.
I wrote the python script which does with it needs to do (receives each line from stdin) and outputs to stdout (or stderr, if a line isn't valid. in this case, i'd like it to be written to another bucket, "C").
I was fiddling around with the data pipeline, tried to run a shell command job and also a hive job for sequencing with the python script.
The EMR cluster was created, ran, finished, no fails or errors, but also no logs created, and I can't understand what is wrong.
In addition, I'd like the original logs be removed after processed and written to the destination or erroneous logs buckets. 
Does anyone have any experience with such configuration? and words of advise?
Assistant: First thing you want to do is to set 'termination protection' on - on the EMR cluster -as soon as it is launched by Data Pipeline.  (this can be scripted too).  
Then you can log on to the 'Master instance'.  This is under 'hardware' pane under EMR cluster details.  (you can also search in EC2 console by cluster id).  
You also have to define a 'key' so that you can SSH to the Master.
Once you log on to the master, you can look under /mnt/var/log/hadoop/steps/ for logs - or /mnt/var/lib/hadoop/.. for actual artifacts.  You can browse hdfs using HDFS utils. 
The logs (if they are written to stdout or stderr), are already moved to S3.  If you want to move additional files, you have to have write a script and run it using 'script-runner'.  You can copy large amount of files using 's3distcp'.