– Interaction
– Success
– Failure
Skill Assessment
• Targets prepared
• Observers ready
• 15-minute overview
• 60-minute test
Introductions
Sprint hours
• Apply targeting strategy
• Hourly survey
• Lunch
Introductions
Introductions
Introductions
Introductions
Sprint hours
• Apply targeting strategy
• Hourly survey
• Lunch
Sprint hours
• Apply targeting strategy
• Hourly survey
• Lunch
Sprint hours
• Apply targeting strategy
• Hourly survey
• Lunch
Sprint hours
• Apply targeting strategy
• Hourly survey
• Lunch
Discussion
• Team interview
– Utility
– Interaction
– Success
– Failure
Skill assessment
Team synchronization
Team synchronization
Team synchronization
8:00 am
8:30 am
9:00 am
9:30 am
10:00 am
10:30 am
11:00 am
11:30 am
12:00 noon
12:30 pm
1:00 pm
1:30 pm
2:00 pm
2:30 pm
3:00 pm
3:30 pm
4:00 pm
8:00 am
8:30 am
9:00 am
9:30 am
10:00 am
10:30 am
11:00 am
11:30 am
12:00 noon
12:30 pm
1:00 pm
1:30 pm
2:00 pm
2:30 pm
3:00 pm
3:30 pm
4:00 pm
USENIX Association
29th USENIX Security Symposium    1143
C Hourly questions
tools?
• What is your pseudonym?
• How many minutes were spent interacting with
• How many minutes were spent harnessing?
• How much time was spent on research?
• Are you feeling productive?
• Are you feeling surprised?
• Are you feeling frustrated?
• Are you feeling doubtful?
• Are you feeling confused?
D End-of-day questions
• What is your pseudonym?
• I learned something today.
• I felt frustrated today.
• I worked with another team member today (team
• I accomplished something today.
• I feel exhausted today.
• I enjoyed my work today.
• I learned a new skill today.
• I was bored today.
lead excluded).
E End-of-experiment questions
(1) Which vulnerability-discovery method do you feel
was more effective?
(2) Which vulnerability-discovery method made you
feel like you were part of a team?
(3) Which vulnerability-discovery method made the
best use of your personal skill?
(4) Which vulnerability-discovery method do you think
made the best use of your team’s skill?
(5) Which vulnerability-discovery method did you think
was easier to get started with?
(6) Which vulnerability-discovery method do you think
is easier for a novice to contribute to?
(7) Did you learn any valuable skills during the
(8) Which vulnerability-discovery method did you learn
(9) Which vulnerability-discovery method did you
experiment?
more during?
enjoy more?
you the most?
(10) Which vulnerability-discovery method frustrated
(11) If you were asked to lead a vulnerability-discovery
project, which method would you choose?
experiment?
• 0
• 1–5
• 5–10
• 10–20
• 20+
the experiment?
• Instructor-led training
• Hands-on experience
• Other
(18) Which method of learning was best for you during
(12) How prepared do you think you were for the
vulnerability-discovery work you were asked to do
during the experiment, before initial training?
(13) How prepared do you think you were for the
vulnerability-discovery work you were asked to do
during the experiment, after initial training?
(14) How prepared do you think you were for the
vulnerability-discovery work you were asked to do
during the experiment, after the experiment?
(15) What was your interest in doing vulnerability-
discovery work, before the experiment?
(16) What was your interest in doing vulnerability-
discovery work, after the experiment?
(17) How many unique bugs did you ﬁnd during the
(19) Were there any external factors that affected your
or your team’s performance during the experiment?
(For example, network outages, room temperature,
experiment hours, and so on.)
(20) Do you have any thoughts or comments you would
like us to consider?
F Skill assessment binaries
Collection
Cyber Grand Challenge
Rode0day (binary only)
Rode0day (with source)
OSS-Fuzz (with source)
Binary
Childs_Game
Game_Night
Casino_Games
tcpdumpB
fileB
audiofileB
bzipS
jqS
jpegS
vorbis
libarchive
libxml2
c-ares
freetype2
openssl
1144    29th USENIX Security Symposium
USENIX Association
References
[1] Thanassis Avgerinos, Alexandre Rebert, Sang Kil
Cha, and David Brumley. Enhancing symbolic
execution with veritesting. Communications of the
ACM, 59(6):93–100, May 2016.
[2] Domagoj Babic, Stefan Bucur, Yaohui Chen, Franjo
Ivancic, Tim King, Markus Kusano, Caroline
Lemieux, László Szekeres, and Wei Wang. FUDGE:
Fuzz driver generation at scale. In Proceedings of
the 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the
Foundations of Software Engineering, ESEC/FSE
2019, page 975–985, New York, New York, USA,
2019. ACM.
[3] Carl Boettiger. An introduction to Docker for
reproducible research. Operating Systems Review,
49(1):71–79, January 2015.
[4] Norbou Buchler, Prashanth Rajivan, Laura R
Marusich, Lewis Lightner, and Cleotilde Gonzalez.
Sociometrics and observational assessment of team-
ing and leadership in a cyber security defense com-
petition. Computers & Security, 73:114–136, 2018.
[5] Sang Kil Cha, Thanassis Avgerinos, Alexandre
Rebert, and David Brumley. Unleashing Mayhem
on binary code. In Proceedings of the 2012 IEEE
Symposium on Security and Privacy, SP ’12, pages
380–394, Washington, DC, USA, 2012.
IEEE
Computer Society.
[6] Gary Charness, Uri Gneezy, and Michael A Kuhn.
Experimental methods: Between-subject and
Journal of Economic
within-subject design.
Behavior & Organization, 81(1):1–8, 2012.
[7] Christian Cioce, Daniel Loffredo, and Nasser Salim.
Program fuzzing on high performance computing
resources. Technical Report SAND2019-0674,
Sandia National Laboratories, Albuquerque,
New Mexico, USA, January 2019.
h t t p s :
//www.osti.gov/servlets/purl/1492735
[Accessed January 23, 2020].
[8] Elena F. Corriero. Counterbalancing. In Mike Allen,
editor, The SAGE Encyclopedia of Communication
Research Methods, volume 1. SAGE Publications,
Thousand Oaks, California, USA, 2017.
[9] Richard Draeger. Within-subjects design. In Mike
Allen, editor, The SAGE Encyclopedia of Commu-
nication Research Methods, volume 4. SAGE Pub-
lications, Thousand Oaks, California, USA, 2017.
[10] Florian Fainelli. The OpenWrt embedded develop-
ment framework, February 2008. Invited talk at the
2008 Free and Open Source Software Developers
European Meeting.
[11] Ming Fang and Munawar Haﬁz. Discovering buffer
overﬂow vulnerabilities in the wild: An empirical
In Proceedings of the 8th ACM/IEEE
study.
International Symposium on Empirical Software
Engineering and Measurement, ESEM ’14, New
York, New York, USA, 2014. ACM.
[12] Andrew Fasano, Tim Leek, Brendan Dolan-Gavitt,
The rode0day to less-buggy
and Josh Bundt.
programs. IEEE Security & Privacy, 17(6):84–88,
November 2019.
[13] Ivan Fratric. 365 days later: Finding and exploiting
Safari bugs using publicly available tools, October
2018. https://googleprojectzero.blogspot
.com/2018/10/365-days-later-finding-an
d-exploiting.html [Accessed March 30, 2019].
[14] GitLab. Issue boards. https://about.gitlab.c
om/product/issueboard/ [Accessed December
17, 2019].
[15] Google.
Fuzzer test suite.
h t t p s :
//github.com/google/fuzzer-test-suite
[Accessed December 18, 2019].
[16] Allen D. Householder, Garret Wassermann, Art
Manion, and Chris King. The CERT(C) guide
to coordinated vulnerability disclosure. https:
//resources.sei.cmu.edu/asset_files/S
pecialReport/2017_003_001_503340.pdf
[Accessed March 31, 2019].
[17] Schuyler W. Huck and Robert A. McLean. Using a
repeated measures ANOVA to analyze the data from
a pretest-posttest design: a potentially confusing
task. Psychological Bulletin, 82(4):511–518, 1975.
[18] Robert Joyce. Come get your free NSA reverse
engineering tool!, March 2019. Presentation at the
2019 RSA Conference.
[19] Brian Kernighan and Rob Pike. The UNIX Program-
ming Environment. Prentice-Hall, Inc., Englewood
Cliffs, New Jersey, USA, 1984.
[20] George Klees, Andrew Ruef, Benji Cooper, Shiyi
Wei, and Michael Hicks. Evaluating fuzz testing.
CoRR, abs/1808.09700, 2018.
USENIX Association
29th USENIX Security Symposium    1145
[21] Valentin J. M. Manès, HyungSeok Han, Choong-
woo Han, Sang Kil Cha, Manuel Egele, Edward J.
Schwartz, and Maverick Woo. The art, science,
IEEE
and engineering of fuzzing: A survey.
Transactions on Software Engineering, October
2019. Early Access.
[22] MITRE. Cwe-125: Out-of-bounds read. https://
cwe.mitre.org/data/definitions/125.html
[Accessed February 3, 2020].
[23] MITRE. Cwe-787: Out-of-bounds write. https://
cwe.mitre.org/data/definitions/787.html
[Accessed February 3, 2020].
[24] Nadim Nachar. The mann-whitney u: A test for
assessing whether two independent samples come
from the same distribution. Tutorials in quantitative
Methods for Psychology, 4(1):13–20, 2008.
[25] Cal Newport. Deep work : rules for focused success
in a distracted world. Grand Central Publishing,
New York ; Boston, 1st ed. edition, January 2016.
[26] Vegard Nossum. Fuzzing the OpenSSH dæmon us-
ing AFL. http://www.vegardno.net/2017/03
/fuzzing-openssh-daemon-using-afl.html
[Accessed March 30, 2019].
[27] Anne Oeldorf-Hirsch. Between-subjects design.
In Mike Allen, editor, The SAGE Encyclopedia
of Communication Research Methods, volume 4.
SAGE Publications, Thousand Oaks, California,
USA, 2017.
[28] Hacker One.
The 2019 hacker
report.
https://www.hackerone.com/resource
s/reporting/the- 2019- hacker- report
[Accessed December 4, 2019].
[29] OpenWrt. ubus (OpenWrt micro bus architecture).
https://openwrt.org/docs/techref/ubus
[Accessed Januarary 22, 2020].
[30] Athanasios Papoulis and S Unnikrishna Pillai.
Probability, random variables, and stochastic
processes. Tata McGraw-Hill Education, New York,
New York, 2 edition, 2002.
[31] Reginald E. Sawilla and Xinming Ou. Identifying
critical attack assets in dependency attack graphs.
In Sushil Jajodia and Javier López, editors, 13th
European Symposium on Research in Computer
Security, volume 5283 of Lecture Notes in Computer
Science, pages 18–34. Springer, 2008.
[32] Kostya Serebryany. OSS-Fuzz - Google’s con-
tinuous fuzzing service for open source software,
August 2017.
Invited talk at the 26th USENIX
Security Symposium.
[33] Michael R Sheldon, Michael J Fillyaw, and
W Douglas Thompson. The use and interpretation
of the friedman test in the analysis of ordinal-scale
data in repeated measures designs. Physiotherapy
Research International, 1(4):221–228, 1996.
[34] R. Shirrey. RFC 4949: Internet security glossary, ver-
sion 2. https://tools.ietf.org/rfc/rfc49
49.txt [Accessed March 31, 2019], August 2007.
[35] Paul Spooren. Running OpenWrt inside Docker.
https://forum.openwrt.org/t/running-op
enwrt-inside-docker-sbin-init-stuck/13
774/8 [Accessed December 17, 2019].
[36] Nick Stephens, John Grosen, Christopher Salls,
Audrey Dutcher, Ruoyu Wang, Jacopo Corbetta, Yan
Shoshitaishvili, Christopher Kruegel, and Giovanni
Vigna. Driller: Augmenting fuzzing through
In Proceedings of
selective symbolic execution.
the 23rd Annual Network and Distributed System
Security Symposium, volume 16, pages 1–16, 2016.
[37] Robert Swiecki.
h t tp :
//honggfuzz.com [Accessed December 18, 2019].
Honggfuzz.
[38] trailofbits. Challenge sets. https://www.trailo
fbits.com/research-and-development/cha
llenge-sets/ [Accessed December 17, 2019].
[39] Sai Vamsi, Venkata Balamurali, K Surya Teja, and
Praveen Mallela. Classifying difﬁculty levels of
programming questions on HackerRank. In Interna-
tional Conference on E-Business and Telecommuni-
cations, volume 3, pages 301–308. Springer, 2019.
[40] Daniel Votipka, Rock Stevens, Elissa Redmiles,
Jeremy Hu, and Michelle Mazurek. Hackers vs.
testers: A comparison of software vulnerability
discovery processes. In 2018 IEEE Symposium on
Security and Privacy, pages 374–391, May 2018.
[41] R. F. Woolson. Wilcoxon Signed-Rank Test, pages
1–3. American Cancer Society, 2008.
[42] Michal Zalewski.
American Fuzzy Lop.
http://lcamtuf.coredump.cx/afl/ [Ac-
cessed March 30, 2019].
1146    29th USENIX Security Symposium
USENIX Association