from 2D images. The 1D CNN also recently produced several
promising results in extracting features from sequential time-
series data [12]. Both types are using similar steps as in
Equation (1), (2) and (3), but the major difference is the
structure of the input data and how the convolution ﬁlter, also
known as a kernel or feature detector, moves across the data
for feature extraction. The shape of convolution ﬁlter (kernel)
is a vector form in 1D CNN and usually 2D matrix form in
2D CNN, as shown in Fig. 1.
B. Split Learning
Split learning is a distributed deep learning technique that
splits a CNN into two parts; the ﬁrst part is provided to the
3
A. 1D CNN for ECG Signal Classiﬁcation
In this section, we describe the non-split version of 1D CNN
ECG classiﬁcation models, which were recently introduced to
classify ECG signals [12], [11], [19], [20], [21]. We chose
two 1D CNN model architectures given in [11] and [12] as
they are most recent and showed the best-achieved accuracy.
Both works [11], [12] aim to classify ECG signals into 5
classes with less than or equal to 5 layers of 1D CNN. For the
model architecture in [11], it has three 1D CNN layers and
two fully connected layers, exhibiting about 96.6% accuracy.
In [12], only two 1D convolutional layers are used with two
fully connected layers, demonstrating about 97.5% accuracy.
We ﬁrst implement these original non-split model from those
two studies and then implement them using split learning to
validate consistency in the model accuracy.
1) ECG Dataset and Preprocessing.: We use MIT-BIH
arrhythmia [22] which is a popular dataset for ECG signal
classiﬁcation or arrhythmia diagnosis detection models. Ar-
rhythmia is short for Abnormal Heart Rythm, which is an
indication of various heart diseases. Following the models
[11], [12], we collect 26,490 samples in total which represents
5 types of heartbeat as classiﬁcation targets: N (normal beat),
L (left bundle branch block), R (right bundle branch block),
A (atrial premature contraction), V (ventricular premature
contraction). We normalize these samples and remove the
noise before feeding them to the 1D CNN as shown in Fig. 2.
We explain the detailed preprocessing steps in Appendix A.
Fig. 2. ECG signals before and after preprocessing.
Fig. 1. 1D CNN vs. 2D CNN in feature detection. The shape of convolution
ﬁlter is a vector form in 1D CNN while a 2D matrix form in 2D CNN.
client and the second part to the server. Both client and server
collaboratively train the split model without accessing each
other’s part. To perform the split learning for 2D CNN models,
several networks such as LeNet, VGG, and AlexNet were
considered and validated [3].
C. Privacy Preserving Machine Learning
Cloud servers or major model providers have been popularly
used for collecting and processing data. However, users often
have privacy concerns when their sensitive data is processed
and stored at cloud servers [17]. In practice, user data on
the cloud server can be misused to identify individuals even
though their explicit identiﬁer information is not provided. For
example, a previous study [13] showed ECG signals can be
used to uniquely identify individuals. Perhaps, identiﬁcation of
individuals may violate important privacy rules such as reusing
the data indiscriminately, and risk-agnostic data processing [4]
required by regulations such as GDPR in Europe [5].
In this regard,
the privacy-preserving machine learning
technique through distributed learning, such as federated learn-
ing [18] and split learning [7], is promising. Split learning as
the focus of this work aims to reduce the privacy leakage
of sensitive localized data by splitting the network during
training—allowing raw data being remained in the data owner
(i.e., client). However, there is a possibility of privacy leakage
from the information sent from the client during the machine
learning process. Precisely, the privacy leakage is that given
the activation at the split layer l, i.e., a(l), how much one (the
server) infers about the training data x.
III. DESIGN AND IMPLEMENTATION OF THE SPLIT 1D
CNN
In this section, we design and implement the split 1D CNN
to answer the RQ 1: Can split learning be applied to deal with
sequential/time-series data in particularly using 1D CNN to
achieve comparable model accuracy as that of trained on
centralized data?
We ﬁrst introduce the 1D CNN ECG classiﬁcation models
[11], [12] that we reproduced. We then detail our imple-
mentation of splitting the 1D CNN model. Consequently, we
validate that the split 1D CNN is able to achieve the same
model accuracy of the non-split 1D CNN, where our RQ 1 is
answered.
2) 1D CNN Model Architecture.: The ﬁrst 1D CNN [12]
model architecture we adopted is illustrated in Fig. 3 (a).
Speciﬁcally, each convolutional layer has 16 ﬁlters: the size
of the ﬁlter used for the ﬁrst convolutional layer is 7, and 5
for the rest. Zero padding is applied before each convolution
operation. Rather than ReLU, Leaky ReLU is chosen as an
activation function of hidden layers to prevent the dying ReLU
problem. Softmax is used for the activation function of the last
fully connected layer. We call this model ‘two-layer model’.
The second 1D CNN model adopted by us [11] is with three
1D convolutional layers (see Fig. 3 (b)), termed as ‘three-layer
model’. Parameter settings are similar to the two-layer model.
3) Training Result.: Table I shows our training and testing
dataset distribution which follows a similar proportion setting
Kernel: Feature DetectorKernel: Feature Detector1D CNN2D CNNe.g. RGB value of single pixel within an imagee.g. a single value within ECG signal01002010.50.00.51.0N-type01002010.80.40.00.4L-typeBefore0641280.00.51.0N-type0641280.00.51.0L-typeAfter4
Fig. 4. Split learning overview.
Algorithm 1: Split learning on the client side
Initialization:
s ← socket initialized with port and address
s.connect(Bob)
φ, η, o, n, N ← s.synchronize()
{w(i)}∀i∈{0. .l} ← initialize using φ
{z(i)}∀i∈{0. .l},{a(i)}∀i∈{0. .l} ← ∅
∂z(i)}∀i∈{0. .l},{ ∂E
∂a(i)}∀i∈{0. .l} ← ∅
{ ∂E
for each batch (x, y) generated from D do
Forward propagation:
a(0) ← x
for i ← 1 to l do
z(i) ← f (i)(a(i−1))
a(i) ← g(i)(z(i))
s.send((a(l), y))
Backward propagation:
∂a(l) ← s.receive()
for i ← l downto 1 do
∂E
∂a(i) × g(i)(cid:48)(z(i))
∂w(i) using ∂E
∂z(i) ← ∂E
∂E
Compute ∂E
Update w(i) using η,
if i (cid:54)= 1 then
∂z(i) and a(i−1)
∂w(i) , and o
∂E
∂a(i−1) ← f (i)
∂E
T ( ∂E
∂z(i) )
s.close()
herein, elaborate on 1D CNN split implementation strategies
for client and server sides, respectively (see Algorithm 1 and
2). Those detailed algorithms can be useful as a guideline for
implementing split learning models. We also clearly specify
what information is exchanged between client and server with
the socket instructions in the pseudocodes in Algorithm 1 and
2, which will be especially helpful for practitioners along with
our source code.
1) Client.: Assume a model that has L layers—input layer
is excluded—in total. The L-th layer is the output layer, and
the remaining layers are hidden layers. Suppose that the model
is split between layer l and layer l + 1. The client holds ﬁrst l
layers from the layer 1 to l—part A, whereas the server holds
remaining layers from the layer l +1 to L—part B. Weights in
the layer i are denoted as w(i). In addition, let f (i) denotes the
forward propagation over the i-th layer, and z(i) denotes the
output tensor just after the forward propagation in i-th layer.
a(i) is denoted as the output after the activation function in
Fig. 3. 1D CNN for ECG signal classiﬁcation using (a) two or (b) three 1D
convolutional layers.
TABLE I
ECG DATASET SPECIFICATIONS.
Class
Train
Test
Total
N
3,000
3,000
6,000
L
R
A
V
3,000
3,000
6,000
3,000
3,000
6,000
1,245
1,245
2,490
3,000
3,000
6,000
Total
13,245
13,245
26,490
for each of 5 classes as in the previous work [12]. Both two-
layer model and three-layer model are trained with 400 epochs,
respectively. The learning rate is set to 0.001. Adam optimizer
with a batch size of 32 is set.
We measure the accuracy of the models on the test set,
after each epoch. As shown in Fig. 5 (non-split), both models’
accuracy converged to 98% around or less than 200 epochs.
The accuracy is not necessarily improved after reaching the
optimal value. The test accuracy of two-layer model is 98.9%,
which is also similar to the test accuracy of three-layer
model. Note that the original work by Dan et al. [12] that
we follow shows a 97.5% accuracy in a similar setting. In
other words, we have successfully reproduced their work
and slightly improved the accuracy, where the improvement
attributes to the small modiﬁcations in hyper-parameters and
data preprocessing steps, as detailed in Appendix A.
B. Splitting 1D CNN
We now split the above 1D CNN models. We focus on two
parties in the split learning setting: the client and the server. As
we show later, the leakage is resulted from passing the forward
activation to the server, which is irrelevant to the number of
participated clients.
The 1D CNN model is split into two parts, A and B as
shown in Fig. 4. Activation and gradients are passed between
client and server to collaboratively train the joint model.
We follow the vertical split method [3], [7] to split 1D CNN.
Those previous studies provide the conceptual process of split
learning for 2D CNN models and show that model accuracy
of split training is the same as that in the non-split training [3],
[7]. Other than focusing on 2D CNN models as [3], [7], we,
1D ConvolutionMax Pooling1D ConvolutionMax PoolingFullyConnectedFully ConnectedSoftmaxECG SignalClassification Result(a)1D ConvolutionMax Pooling1D Convolution1D ConvolutionMax PoolingFully ConnectedFully ConnectedSoftmaxECG SignalClassification Result(b)ECGClient e.g, hospitalServer: computing powerPass ActivationsPass GradientsPart A: first layersPart B: remaining layersSplit LayerOutput Input samples5
Fig. 5. Accuracy over the training of split and non-split 1D CNN models. Notably, we have set the exact same initial weights through using the equivalent
random seed, for both split and non-split tests in this example.
Algorithm 2: Split learning on the server side
Initialization:
s ← server socket initialized with port and address
sA ← s.accept(Alice)
φ, η, o, n, N ← sA.synchronize()
{w(i)}∀i∈{l+1. .L} ← initialize using φ
{z(i)}∀i∈{l+1. .L},{a(i)}∀i∈{l. .L} ← ∅
∂a(i)}∀i∈{l. .L} ← ∅
{ ∂E
∂z(i)}∀i∈{l+1. .L},{ ∂E
for i ← 1 to N do
Forward propagation:
(a(l), y) ← sA.receive()
for i ← l + 1 to L do
z(i) ← f (i)(a(i−1))
a(i) ← g(i)(z(i))
E ← L(a(L), y)
Backward propagation:
∂E
∂a(L)
Compute
for i ← L downto l + 1 do
∂a(i) × g(i)(cid:48)(z(i))
∂w(i) , using ∂E
∂z(i) ← ∂E
∂E
Compute ∂E
Update w(i), using η,
∂a(i−1) ← f (i)
∂z(i) )
T ( ∂E
∂E
∂z(i) and a(i−1)
∂w(i) , and o
∂E
sA.send( ∂E
∂a(i) )
sA.close()
layer i, which can be given by a(i) = g(i)(z(i)), where g(i) is
the activation function of i-th layer. In backpropagation, f (i)
T
denotes the function which returns the gradient of activation
of layer i−1, using weights and gradient of i-th layer. Finally,
when the client has the ECG raw dataset D to train with, the
split learning process on client follows Algorithm 1.
The client ﬁrst connects to the server via socket and
synchronizes some train conﬁgurations. With a single batch
given from D, the client forward propagates it until the l-th
layer and sends the activation a(l) from the l-th layer to the