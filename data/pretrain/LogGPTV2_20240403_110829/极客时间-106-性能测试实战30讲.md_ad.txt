# 04丨JMeter和LoadRunner：要知道工具仅仅只是工具做性能测试工作的人总是离不了性能测试工具，但当我们刚开始接触这类工具或者压测平台的时候，总是难免处在一种顾此失彼，焦虑又没想法的状态。 性能工程师的三大学习阶段在我看来，对性能测试工程师本身来，多半会处在以下三个大的阶段。 性能工具学习期JMeter 和 LoadRunner是我们常用的两个性能测试工具。曾经有人问我，应该学 JMeter 还是LoadRunner呢？我反问的是，你学这样的工具需要多久呢？一般对方因为初学并不清楚要多久，然后我会告诉他，如果你是认真努力的，想要全职学习，那么我觉得一个工具，纯从功能的使用的角度来说，自学两个星期应该就差不多了。如果你是在工作中学习，那就更简单了，工作中需要什么就学习什么，不用纠结。 而应该纠结的是什么呢？当你把 JMeter、LoadRunner的基本功能学会了，你会发现这些工具其实就做了两件事情，做脚本和发压力。 但问题在于，脚本的逻辑和压力场景的逻辑，和工具本身无关，和业务场景有关。这时你可能就会问，场景怎么配置呢？ 这才进入到了另一个阶段。 通常在这个阶段的时候，你会觉得自己有非常明确的疑问，有经验的人可能一句话就可以指点你了，解决掉你的疑问，就是告诉你选择什么工具，如何来用。 性能场景学习期第二个阶段就是性能场景学习期。我们平时在很多场合下所说的场景范围都有些狭隘，觉得场景就是业务比例，就是用多少数据。而实际做过多个性能项目之后，你就会发现，工具中的一个小小的配置，也会对结果产生巨大的影响。 比如说压力策略，应该用一秒 Ramp up 10 个用户，还是 20 个用户，还是100 个用户？这应该怎么判断呢？ 比如说，参数化数据应该用 100 条，还是 100万条？还是有确定的值呢？有人说根据场景配置，可是根据什么样的场景怎么配置才合理呢？ 比如说，在执行场景时应该看哪些数据？压力工具中的TPS、响应时间这些常规数据都会去看，其他的还要看什么呢？这就涉及到了监控策略。 再比如说，业务应该用什么样的比例设置到压力工具中？有人说直接在线上做测试不是挺直接？但是你知道什么样的业务可以，什么样的业务不可以吗？如何控制线上的性能测试？ 在性能场景学习期这个阶段，你关心的将不再是工具的使用操作，而是如何做一个合理的性能测试。你可以学会调整业务比例，并设计到压力工具中；你可以学会参数化数据的提取逻辑；你可以学会场景中要观察哪些数据。 按照这个思路，再做几个项目，你就会慢慢摸着一些门道。 性能分析学习期学会使用工具了，也有了场景设计的经验，通过监控工具也拿到了一堆大大小小的数据。可是，数据也太多了，还在不断的变化。我又怎么判断性能瓶颈在哪里呢？ 做性能的人都会有这样的一个茫然。当你把一个性能测试结果发给了别人，别人会顺理成章地去问你："响应时间为什么这么长？有没有优化空间？" 听到这种问题，你有没有无助的感觉？心里台词是："我怎么知道？"但是嘴上却不敢说出来，因为似乎这是我应该给出的答案？ 但是当你尝试给出答案时，你就进入了一个大坑，对这个问题做出回答，近乎一个无底洞，需要太多的基础知识，需要很强的逻辑分析，需要清晰的判断思路。 如果你到了这个阶段，你可能会发现自己走得非常痛苦，好像自己也不知道自己会什么，不会什么。要说工具吧，也完全会用，场景吧，也会配置，但为什么就是不会分析结果，不会整理数据，不会下结论呢？ 但实际上，我觉得你不要焦虑自己不会什么，而应该把目光聚焦到你要解决的问题上。问题的解决，靠的是思维逻辑，靠的是判断，而不是靠工具。 也就是说，这时面对问题，你应该说的是"我想要看什么数据"，而不是"把数据都给我看看"。 看到这里，希望你能清晰地理解这两者之间的区别。 公司性能团队成长阶段我刚才分析了一下作为个人的性能工程师是如何一步步成长的，在实际工作中，我们更多的需要与团队合作，团队的成长与我们个人的成长息息相关。 对于一个公司的一个性能团队来说，大概会处在这些阶段。 性能团队初建这时的团队，可以执行场景，可以拿出数据，但工作出的结果并不理想。团队整体的价值就体现在每天跟着版本跑来跑去，一轮轮地测试下去，一个版本短则一两个星期，长则一个月。没有时间去考虑测试结果对整个软件生命周期的价值，在各种琐碎的项目中疲于奔命。做脚本，拿出TPS和响应时间，做版本基线比对，出数据罗列式的性能测试报告。 唉，想想人生就这么过去了，真是心有不甘。这时有多少人希望能有一个性能测试平台来拯救团队啊。 性能团队初成熟到了这个阶段，团队已经可以应付版本的更迭带来的性能工作压力，团队合作良好，稍有余力，开始考虑团队价值所在，在公司的组织结构中应该承担什么样的职责。在产品的流水线上终于可以占有一席之地了。这样很好，只是从实际的技术细节上来说，仍然没有摆脱第一阶段中琐碎的工作，没有把性能的价值体现出来，只是一个报告提供机器。 这时就需要考虑平台上是不是可以加个 SLA来限制一下？在各个流程的关卡上，是不是可以做些性能标准？是不是该考虑下准入准出规则了？是的，这时一个团队开始慢慢走向成熟，站住脚之后要开始争取尊重了。 性能团队已成熟有了标准、流程，团队的合作能力也成熟了之后，团队"是时候展示真正的实力了"。但问题来了，什么才是性能团队的真正实力呢？ 直观上说，主要体现在一下几个方面。 **1.通过你的测试和分析优化之后，性能提升了多少？** 这是一句非常简单直接的话。但是我相信有很多做性能测试工程师的人回答不出这样的问题。因为看着混乱的TPS曲线，自己都已经晕了，谁还知道性能提升了多少呢？ 而一个成熟的团队应该回答的是：提升了 10倍，我们调优了什么。这样的回答有理有据，底气十足。 **2.通过你的测试和分析优化之后，节省了多少成本？** 这个问题就没有那么好回答了，因为你要知道整体的容量规划，线上的真实运营性能。如果之前的版本用了200 台机器，而通过我们的测试分析优化之后，只用到了 100台机器，那成本就很明显了。 但是，在我的职业生涯中，很少看到有人这样来体现性能存在的价值。有些场合是不需要这样体现，有些场合是不知道这样体现。 对个人以及团队来说，工具应该如何选择理顺了性能测试工程师和性能团队的成长路径，下面我们来说说个人或者团队选择工具的时候，应该如何考量。 在我十几年工作的生涯中，可以说有很多性能工具都是知道的，但是要说起用得熟练的，也无非就是那几个市场占有率非常高的工具。 下面列一下市场上大大小小、老老少少、长长短短的性能测试工具，以备大家查阅。 ![](Images/043f5356ffb2afe4c3d14895ef18d11a.png)savepage-src="https://static001.geekbang.org/resource/image/5b/a8/5bb101eb5aae149ae3488a920b4213a8.jpg"}市面上大大小小的性能测试工作一共有四十余种。这里面有收费的，也有免费的；有开源的，有闭源的；有新鲜的，有不新鲜的；有活跃的，有半死不活的；有可以监控系统资源的，有只能做压力发起的。 你是不是有一种生无可恋的感觉？一个性能测试而已，有必要搞出这么多工具吗？ 然而，你要记住，这些都是压力发起工具。 下面我对一些比较常见的工具做下比对，这些工具主要包括 ApacheJMeter、HP LoadRunner、Silk Performer、Gatling、nGrinder、Locust 和Tsung。 ![](Images/5f4249a7838ac074f47c08c57a785bdf.png)savepage-src="https://static001.geekbang.org/resource/image/92/8c/922eb45247344025f473d4649672a28c.jpg"}仅比对这几个工具吧，因为从市场上来说，这几个算是经常看到的工具，以后我们再加入其他的工具和其他的属性。我们现在只说性能工具，不说一些企业做的性能平台云服务，因为云服务都是对企业来说的，我们放到后面再讲。 你从网络上可以很容易地找到这几个工具的特点，这几个都支持分布式。从上面那张表格中，你可以很容易对比出来，知道自己应该学什么工具了。 Gatling 有免费版和收费版，基于 Scala 语言，而 Scala 又是基于 Java的，你看这复杂的关系就让人不想用，但是这个工具性能很高，虽说只支持HTTP，但是由于支持 Akka Actors 和 Async IO，可以达到很高的性能。Actors简化并发编译的异步消息特性让 Gatling性能很高。 Locust 这个工具是基于 Python的，中文名翻译过来就是蝗虫，这名字取得挺有意思。在一个压力场景下，对服务器来说确实就像一堆蝗虫来了。 对市场的占有率来说，JMeter 和 LoadRunner 以绝对的优势占据前两名，同时JMeter又以绝对的优势占据第一名。 下面来看一下，这两个工具的热度趋势。 这是全球范围近 5 年 JMeter 和 LoadRunner 热度（红色线是LoadRunner，蓝色线是 JMeter）： ![](Images/4cb5ffbcd5102ee3bf1700fbcd265d2d.png)savepage-src="https://static001.geekbang.org/resource/image/5d/82/5da3934e9b6956f0406af7596f35c582.png"}中国范围近 5 年 JMeter 和 LoadRunner热度： ![](Images/c0eb5ad06ed701ed83fc64b0f50bb501.png)savepage-src="https://static001.geekbang.org/resource/image/01/96/0189855f2bb8c8539dac692214a80696.png"}从上面的比对来看，我们可以很容易发现，近五年来，LoadRunner就一直在走下坡路，而 JMeter一直处在上升的趋势。 JMeter 和 LoadRunner 的历史兴衰我下面只说一下 JMeter 和 LoadRunner的历史，让你对性能工具的兴衰史有一定的了解。 先说说 LoadRunner 吧，应该说，LoadRunner的历史，就是一段悲惨的回忆。2006 年 11 月份以前，在 Mercury时代，LoadRunner由于市场策略和工具优势很快占了第一名，势头很猛。当时还有另一个同样功能的工具Silk Performer，被打压得几乎抬不起头来。06 年以后，Mercury 以 45亿美元被 HP 收购，包括 QC、QTP 等工具。但从那之后，LoadRunner的体积就在飞速膨胀，8.1 的 LoadRunner 只有 600M左右（如果我没记错的话），经历了几个版本的迭代，LoadRunner 成功膨胀到 4个 G，并在后面规划 performance center，在各地做质量中心。HP这一步步走得理直气壮，把市场折腾没了。现在 LoadRunner如果想装到一台压力机上，都是很吃力的事情。我要是用的话，宁愿在 XP系统上安装 8.1 版本，速度飞快。2016 年，HPE 和 MicroFocus合并，LoadRunner 也成了 MicroFous产品线的一部分，搞到现在，在中国的市场依然疲软。 而拥有同样竞品工具 Silk Performer、Silk Test 和 Silk Test Manager 的Segue 公司，同年仅以 1 亿美元被另一个企业 Borland 收购。3年之后，Borland 连同自己，以 7500 万美元卖给了MicroFocus。 至此，MicroFocus 同时拥有了 LoadRunner 和 SilkPerformer。但可惜的是，这也照样干不过一个无心插柳柳成荫的开源工具JMeter。 JMeter 的历史，可以说是屌丝逆袭的典型案例。1998 年，Apache 基金会的Stefano Mazzocchi 是它最初的开发者，当时他只是为了测试 Apache JServlet的性能（这个项目后来被 Tomcat 取代），后来 JMeter 被重构，又被用来测试Tomcat。其实一开始，JMeter 的功能很简单。但是 Apache Tomcat的势头实在是阻挡不住，再加上 Java 市场覆盖率实在是太高了，而 JMeter做为一个开源免费的 Java 压力工具，有着众多的 contributors，顶着 Apache的大旗，想失败都难。就像 ab 工具是为了测试 Apache HTTP server一样，JMeter 应该说是和 Apache Tomcat一起成长的。 与此同时，还有另一个 Java 开源工具 TheGrinder，这个工具的主要贡献者是 Philip Aston、CalumFitzgerald。 当时有一个开源测试平台叫 NGrinder，是韩国公司 NHN开源的。有很多所谓企业内部自研发的性能测试平台，就是从 NGrinder借鉴来的，而 NGrinder 就是以 The Grinder 为基础开发的。可惜的是，TheGrinder 没有 Apache这样的平台，作为一个很优秀的工具，它的维护更新还是不够快，不过 NGrinder也给它带来了一定的荣耀。 到现在为止，JMeter还没有一个非常成熟的云测试平台支撑它，只有一些商业公司改动，加一些管理和项目属性，做为企业内部平台使用。还有一些企业把JMeter改造成商业产品，加上云基础架构的管理功能，就成了一套完整的商业平台，再加上炫丽的操作页面，棒棒的，有没有？ 那么你有没有想过，为什么没有以 JMeter 为基础的开源云测试平台呢？难道JMeter的热爱者看不到云测试平台的价值吗？在我看来，做为性能测试工具，它实在是没有必要做成一个开源的测试平台，因为轮子就是轮子，要装成什么样的车就自己装吧。要是再换个角度来说，性能测试真的有必要用平台吗？ 使用性能测试工具的误区在哪里现在很多人都是看互联网大厂的技术栈，但是有没有想过自己企业需要的到底是什么样的产品？曾经有个测试工程师跟我说，他们公司为了解决性能问题，特意买了压测云服务，花了20 万，结果问题还是没找出来。 所以工具应该如何用，完全取决于用的人，而不是工具本身。 压测工具也好，压测平台也好，都没有一个工具可以直接告诉你瓶颈在哪里，能告诉你的只是数据是什么。分析只有靠自己，在这个过程中，我们也会用到很多的分析剖析工具，用这些工具的人也都会知道，工具也只提供数据，不会告诉你瓶颈点在哪里。 那这个时候就有人提出疑问了："有些工具不是说，上了这个工具之后，耗时一眼看透嘛？"是的呀，关键是你看过是什么耗时了吗？给你一个Java栈，那么长的栈，每个方法的消耗都给你，但是长的就肯定有问题吗？ 关于剖析工具的，我们后面再写。本篇重点在压测工具上。 有人说 JMeter BIO 有问题，应该用AIO；有人说，压测工具没有后端系统性能监控数据，应该加一个监控插件。像JMeter 中就有一个插件叫 perfmon，把后端的系统资源拉到 JMeter的界面中来看。在这一点上，LoadRunner老早就做过了，并且在之前的版本中还有个专门的组件叫tuning，目的就是把后端所有的系统、应用、数据库都配置到一个架构图中，压力一发起，就把有问题的组件标红。想法很好，可是这个功能为什么没有被广泛使用？当然，后面被HP 收购后，这和 HP 的市场策略有关，但是在收购前的 Mercury时代，该功能也没有被广泛使用。 我们从实际的生产场景来看，压测工具模拟的是真实用户，而监控在哪里，在运维后台里，数据的流向都不一样。如果你使用压测工具的同时，也把它做为收集性能监控数据的工具，本身流量就会冲突。 所以在压测工具中同时收集监控计数器，就是不符合真实场景的。 这样压测平台就有出现的必要了，我们可以看到出现了五花八门的压测平台，也会有后端监控数据的曲线，乍看起来，就两个字：全面！ 可是，同样也没有告诉你瓶颈在哪里。 如果选择合适自己的工具？所以我们用工具，一定要知道几点： 1.       工具能做什么？        2.       工具不能做什么？        3.       我们用工具的目标是什么？        4.       当工具达不到目标时，我们怎么办？        把这几个问题在用工具之前就想清楚，才是个成熟的测试工程师，有这样的工程师的团队，才是成熟的性能测试团队（当然，成熟的测试团队还要有其他的技术）。 对企业，举例来说： 如果是一个需要支持万级、亿级 TPS的电商网站，本身就是云基础架构，那么可能最简单的就是直接买这家的云压测工具就好了。 这样做的优点是不用再买机器做压力了。压力发起，主要就是靠压力机的量堆出来大并发。 但缺点也很明显，一是不能长期使用，长期用，费用就高了。二是数据也只能自己保存比对，如果测试和版本跨度大，还是要自己比对，无法自动比对。最后一个缺点就是压力机不受控了。 所以如果有这样需求的企业，也基本上可以自己开发一套云压测工具了，从使用周期和长远的成本上来看，自已开发，都是最划算的。 如果是一个需要支持每秒 100TPS的企业内部业务系统，就完全没必要买什么云服务了，自己找一台 4C8G的机器，可能就压得够了。 这样的话完全可控，压测结果数据也都可以随时查看，可以留存。 如果是一个需要支持万级TPS，但又不能用云服务的事业单位或政企，比如，军工业，那只能自己搭建一套测试环境了。这样做的优点是完全内部可控，数据非常安全，但缺点就是投入成本高。 对私企来说，开源永远是最好的选择，成本低，但是需要相关人员能力稍强一些，因为没有技术支持。 对政企和事业单位来说，收费是一个好的选择，因为有第三方服务可以叫过来随时支持。 对一个做短平快项目的企业来说，云服务会是一个好选择，成本低，不用长期维护数据。 对想做百年老店的企业来说，肯定是自己开发平台，尽量不选择云服务，因为技术是需要积累的。 对个人来说呢，不用举例，压测工具市场，现在肯定是首选学习JMeter，其次是 LoadRunner。 JMeter的势头已经很明显了，并且功能在慢慢扩展。开源免费是巨大的优势。 而LoadRunner，不管它的市场现在有多凋零，它仍然是性能测试市场上，功能最为齐全的工具，没有之一。 总结总体来说，性能测试工具的市场中，可以说现在的工具已经种类繁多了，并且各有优点。在项目中，根据具体的实施成本及企业中的规划，选择一个最适合的就可以了。也可以用它们来组建自己的平台。但是请注意，不要觉得做平台可以解决性能测试的问题，其实平台只是解决了人工的成本。 如果单纯为了追潮流而把性能测试工具的使用成本升得特别高，那就不划算了。 思考题今天的内容有点多，我提几个思考题，你就当是对文章的回顾吧。 你觉得企业选择性能工具应该考虑哪些方面呢？以及性能测试工具中是否必须做监控呢？ 欢迎你在评论区写下你的思考，也欢迎把这篇文章分享给你的朋友或者同事，一起交流一下。 
# 05丨指标关系：你知道并发用户数应该怎么算吗？我在性能综述的那三篇文章中，描述了各种指标，比如TPS、RPS、QPS、HPS、CPM等。我也强调了，我们在实际工作的时候，应该对这些概念有统一的认识。 ![](Images/7821fc36b41b79606255008ac59db306.png)savepage-src="https://static001.geekbang.org/resource/image/a0/18/a0a15799c73e7faef76324f18ff3f318.jpg"}这样的话，在使用过程中，一个团队或企业从上到下都具有同样的概念意识，就可以避免出现沟通上的偏差。 我说一个故事。 我以前接触过一个咨询项目。在我接触之前，性能测试团队一直给老板汇报着一个数据，那就是10000TPS。并且在每个版本之后，都会出一个性能测试报告，老板一看，这个数据并没有少于10000TPS，很好。 后来，我进去一看，他们一直提的这个 10000TPS指的是单业务的订单，并且是最基础的订单逻辑。那么问题来了，如果混合起来会怎么样呢？于是我就让他们做个混合容量场景，显然，提容量不提混合，只说单接口的容量是不能满足生产环境要求的。 结果怎么样呢？只能测试到6000TPS。于是我就要去跟老板解释说系统达到的指标是6000TPS。老板就恼火了呀，同样的系统，以前报的一直是10000TPS，现在怎么只有 6000TPS了？不行，你们开发的这个版本肯定是有问题的。于是老板找到了研发 VP，研发VP找到了研发经理，研发经理找了研发组长，研发组长又找到了开发工程师，开发工程师找到了我。我说之前不是混合场景的结果，现在混合容量场景最多达到6000TPS，你们可以自己来测。 然后证明，TPS 确实只能达到6000。然后就是一轮又一轮的向上解释。 说这个故事是为了告诉你，你用 TPS 也好，RPS 也好，QPS也好，甚至用西夏文来定义也不是不可以，只要在一个团队中，大家都懂就可以了。 但是，在性能市场上，我们总要用具有普适性的指标说明，而不是用混乱的体系。 在这里，我建议用 TPS做为关键的性能指标。那么在今天的内容里，我们就要说明白 TPS到底是什么。在第 3篇文章中，我提到过在不同的测试目标中设置不同的事务，也就是 TPS 中的 T要根据实际的业务产生变化。 那么问题又来了，TPS 和并发数是什么关系呢？在并发中谁来承载"并发"这个概念呢？ 说到这个，我们先说一下所谓的"绝对并发"和"相对并发"这两个概念。绝对并发指的是同一时刻的并发数；相对并发指的是一个时间段内发生的事情。 你能详细说一下这两个概念之间的区别吗？如果说不出来那简直太正常了，因为这两个概念把事情说得更复杂了。 什么是并发下面我们就来说一下"并发"这个概念。 ![](Images/1dbd6ee65697dc13412fb3a5e7a238d2.png)savepage-src="https://static001.geekbang.org/resource/image/0e/96/0ef2fca88f59342a64084e147b33af96.jpg"}我们假设上图中的这些小人是严格按照这个逻辑到达系统的，那显然，系统的绝对并发用户数是4。如果描述 1 秒内的并发用户数，那就是16。是不是显而易见？ 但是，在实际的系统中，用户通常是这样分配的： ![](Images/963e14ebe395199a83d48d8a2b341242.png)savepage-src="https://static001.geekbang.org/resource/image/1e/23/1e42cc116598acb7c703eec95a7c6723.png"}也就是说，这些用户会分布在系统中不同的服务、网络等对象中。这时候"绝对并发"这个概念就难描述了，你说的是哪部分的绝对并发呢？ 要说积分服务，那是 2；要说库存服务，那是 5；要说订单服务，它自己是 5个请求正在处理，但同时它又 hold 住了 5个到库存服务的链接，因为要等着它返回之后，再返回给前端。所以将绝对并发细分下去之后，你会发现头都大了，不知道要描述什么了。 有人说，我们可以通过 CPU 啊，I/O 啊，或者内存来描述绝对并发，来看 CPU在同一时刻处理的任务数。如果是这样的话，绝对并发还用算吗？那肯定是 CPU的个数呀。有人说 CPU 1ns 就可以处理好多个任务了，这里的 1ns也是时间段呀。要说绝对的某个时刻，任务数肯定不会大于 CPU物理个数。 所以"绝对并发"这个概念，不管是用来描述硬件细化的层面，还是用来描述业务逻辑的层面，都是没什么意义的。 我们只要描述并发就好了，不用有"相对"和"绝对"的概念，这样可以简化沟通，也不会出错。 那么如何来描述上面的并发用户数呢？在这里我建议用 TPS来承载"并发"这个概念。 并发数是 16TPS，就是 1 秒内整个系统处理了 16个事务。 这样描述就够了，别纠结。 在线用户数、并发用户数怎么计算那么新问题又来了，在线用户数和并发用户数应该如何算呢？下面我们接着来看示意图： ![](Images/e04bc3da05a1dea81b42081f2580e0c7.png)savepage-src="https://static001.geekbang.org/resource/image/1b/99/1beff8afb0116c72dd428eba9d329299.jpg"}如上图所示，总共有 32个用户进入了系统，但是绿色的用户并没有任何动作，那么显然，在线用户数是32 个，并发用户数是 16 个，这时的并发度就是50%。 但在一个系统中，通常都是下面这个样子的。 ![](Images/ceb129d5ad9d1ed82a1a75edd1383240.png)savepage-src="https://static001.geekbang.org/resource/image/59/1c/59fd25efa2fdd259b4fc0b6b462a031c.jpg"}为了能 hold 住更多的用户，我们通常都会把一些数据放到 Redis这样的缓存服务器中。所以在线用户数怎么算呢，如果仅从上面这种简单的图来看的话，其实就是缓存服务器能有多大，能hold 住多少用户需要的数据。 最多再加上在超时路上的用户数。如下所示： ![](Images/c66fbb58cab13fe2b0677cf96abbaa13.png)savepage-src="https://static001.geekbang.org/resource/image/4b/10/4bbf31a73fd4ccf6245634b8c7eb2910.jpg"}所以我们要是想知道在线的最大的用户数是多少，对于一个设计逻辑清晰的系统来说，不用测试就可以知道，直接拿缓存的内存来算就可以了。 假设一个用户进入系统之后，需要用 10k 内存来维护一个用户的信息，那么10G 的内存就能 hold 住 1,048,576个用户的数据，这就是最大在线用户数了。在实际的项目中，我们还会将超时放在一起来考虑。 但并发用户数不同，他们需要在系统中执行某个动作。我们要测试的重中之重，就是统计这些正在执行动作的并发用户数。 当我们统计生产环境中的在线用户数时，并发用户数也是要同时统计的。这里会涉及到一个概念：**并发度**。 要想计算并发用户和在线用户数之间的关系，都需要有并发度。 做性能的人都知道，我们有时会接到一个需求，那就是一定要测试出来**系统最大在线用户数是多少**。这个需求怎么做呢？ 很多人都是通过加思考时间（有的压力工具中叫等待时间，Sleep时间）来保持用户与系统之间的 session不断，但实际上的并发度非常非常低。 我曾经看到一个小伙，在一台 4C8G 的笔记本上用 LoadRunner 跑了 1万个用户，里面的 error疯狂上涨，当然正常的事务也有。我问他，你这个场景有什么意义，这么多错？他说，老板要一个最大在线用户数。我说你这些都错了呀。他说，没事，我要的是Running User能达到最大就行，给老板交差。我只能默默地离开了。 这里有一个比较严重的理解误区，那就是压力工具中的线程或用户数到底是不是用来描述性能表现的？我们通过一个示意图来说明： ![](Images/2be51e2465982ce18d396463189ddf6f.png)savepage-src="https://static001.geekbang.org/resource/image/6c/05/6c32b4bb4644a614bec67858656f4f05.jpg"}通过这个图，我们可以看到一个简单的计算逻辑： 1.       如果有 10000 个在线用户数，同时并发度是 1%，那显然并发用户数就是    100。    2.       如果每个线程的 20TPS，显然只需要 5    个线程就够了（请注意，这里说的线程指的是压力机的线程数）。        3.       这时对 Server 来说，它处理的就是 100TPS，平均响应时间是    50ms。50ms 就是根据 1000ms/20TPS    得来的（请注意，这里说的平均响应时间会在一个区间内浮动，但只要 TPS    不变，这个平均响应时间就不会变）。        4.       如果我们有两个 Server 线程来处理，那么一个线程就是    50TPS，这个很直接吧。        5.       请大家注意，这里我有一个转换的细节，那就是        **并发用户数到压力机的并发线程数**        。这一步，我们通常怎么做呢？就是基准测试的第一步。关于这一点，我们在后续的场景中交待。        而我们通常说的"并发"这个词，依赖 TPS 来承载的时候，指的都是 Server端的处理能力，并不是压力工具上的并发线程数。在上面的例子中，我们说的并发就是指服务器上100TPS 的处理能力，而不是指 5个压力机的并发线程数。**请你切记这一点，以免沟通障碍**。 在我带过的所有项目中，这都是一个沟通的前提。 所以，我一直在强调一点，这是一个基础的知识：**不要在意你用的是什么压力工具，只要在意你服务端的处理能力就可以了**。 示例上面说了这么多，我们现在来看一个实例。这个例子很简单，就是： JMeter（1 个线程） - Nginx - Tomcat -MySQL 通过上面的逻辑，我们先来看看 JMeter的处理情况：     summary +   5922 in 00:00:30 =  197.4/s Avg:     4 Min:     0 Max:    26 Err:     0 (0.00%) Active: 1 Started: 1 Finished: 0    summary =  35463 in 00:03:05 =  192.0/s Avg:     5 Min:     0 Max:   147 Err:     0 (0.00%)    summary +   5922 in 00:00:30 =  197.5/s Avg:     4 Min:     0 Max:    24 Err:     0 (0.00%) Active: 1 Started: 1 Finished: 0    summary =  41385 in 00:03:35 =  192.8/s Avg:     5 Min:     0 Max:   147 Err:     0 (0.00%)    summary +   5808 in 00:00:30 =  193.6/s Avg:     5 Min:     0 Max:    25 Err:     0 (0.00%) Active: 1 Started: 1 Finished: 0    summary =  47193 in 00:04:05 =  192.9/s Avg:     5 Min:     0 Max:   147 Err:     0 (0.00%)我们可以看到，JMeter 的平均响应时间基本都在5ms，因为只有一个压力机线程，所以它的 TPS 应该接近1000ms/5ms=200TPS。从测试结果上来看，也确实是接近的。有人说为什么会少一点？因为这里算的是平均数，并且这个数据是30s 刷新一次，用 30 秒的时间内完成的事务数除以 30s得到的，但是如果事务还没有完成，就不会计算在内了；同时，如果在这段时间内有一两个时间长的事务，也会拉低TPS。 那么对于服务端呢，我们来看看服务端线程的工作情况。 ![](Images/49ef95877bfd307f31e358518342c05f.png)savepage-src="https://static001.geekbang.org/resource/image/bd/55/bdce20f34b2e0689c21977859f54e155.png"}可以看到在服务端，我开了 5个线程，但是服务端并没有一直干活，只有一个在干活的，其他的都处于空闲状态。 这是一种很合理的状态。但是你需要注意的是，这种合理的状态并不一定是对的性能状态。 1.       并发用户数（TPS）是 193.6TPS。如果并发度为 5%，在线用户数就是    193.6/5%=3872。        2.       响应时间是 5ms。        3.       压力机并发线程数是    1。这一条，我们通常也不对非专业人士描述，只要性能测试工程师自己知道就可以了。        下面我们换一下场景，在压力机上启动 10个线程。结果如下：     summary +  11742 in 00:00:30 =  391.3/s Avg:    25 Min:     0 Max:   335 Err:     0 (0.00%) Active: 10 Started: 10 Finished: 0    summary =  55761 in 00:02:24 =  386.6/s Avg:    25 Min:     0 Max:   346 Err:     0 (0.00%)    summary +  11924 in 00:00:30 =  397.5/s Avg:    25 Min:     0 Max:    80 Err:     0 (0.00%) Active: 10 Started: 10 Finished: 0    summary =  67685 in 00:02:54 =  388.5/s Avg:    25 Min:     0 Max:   346 Err:     0 (0.00%)    summary +  11884 in 00:00:30 =  396.2/s Avg:    25 Min:     0 Max:   240 Err:     0 (0.00%) Active: 10 Started: 10 Finished: 0    summary =  79569 in 00:03:24 =  389.6/s Avg:    25 Min:     0 Max:   346 Err:     0 (0.00%)平均响应时间在25ms，我们来计算一处，(1000ms/25ms)\*10=400TPS，而最新刷出来的一条是396.2，是不是非常合理？ 再回来看看服务端的线程： ![](Images/04e38c1f1017c00c5d2b98680fd91587.png)savepage-src="https://static001.geekbang.org/resource/image/59/90/596564dd3186769bdf546bfdc5bdae90.png"}同样是 5个线程，现在就忙了很多。 1.       并发用户数（TPS）是 396.2TPS。如果并发度为 5%，在线用户数就是    396.2/5%=7924。        2.       响应时间是 25ms。        3.       压力机并发线程数是    10。这一条，我们通常也不对非专业人士描述，只要性能测试工程师自己知道就可以了。        如果要有公式的话，这个计算公式将非常简单： ]{.strut style="height:0.68333em;vertical-align:0em;"}[T]{.mord.mathdefault style="margin-right:0.13889em;"}[P]{.mord .mathdefaultstyle="margin-right:0.13889em;"}[S]{.mord .mathdefaultstyle="margin-right:0.05764em;"}[]{.mspacestyle="margin-right:0.2777777777777778em;"}[=]{.mrel}[]{.mspacestyle="margin-right:0.2777777777777778em;"}]{.base}[[]{.strutstyle="height:1.365108em;vertical-align:-0.52em;"}[[]{.mopen.nulldelimiter}[[[]{.pstrut style="height:3em;"}[[[响]{.mord.cjk_fallback .mtight}[应]{.mord .cjk_fallback .mtight}[时]{.mord.cjk_fallback .mtight}[间]{.mord .cjk_fallback .mtight}[(]{.mopen.mtight}[单]{.mord .cjk_fallback .mtight}[位]{.mord .cjk_fallback.mtight}[m]{.mord .mathdefault .mtight}[s]{.mord .mathdefault.mtight}[)]{.mclose .mtight}]{.mord .mtight}]{.sizing .reset-size6.size3 .mtight}]{style="top:-2.655em;"}[[]{.pstrutstyle="height:3em;"}[]{.frac-linestyle="border-bottom-width:0.04em;"}]{style="top:-3.23em;"}[[]{.pstrutstyle="height:3em;"}1]{.mord .mtight}[0]{.mord .mtight}[0]{.mord.mtight}[0]{.mord .mtight}[m]{.mord .mathdefault .mtight}[s]{.mord.mathdefault .mtight}]{.mord .mtight}]{.sizing .reset-size6 .size3.mtight}]{style="top:-3.394em;"}]{.vliststyle="height:0.845108em;"}[​]{.vlist-s}]{.vlist-r}[[]{.vliststyle="height:0.52em;"}]{.vlist-r}]{.vlist-t.vlist-t2}]{.mfrac}[]{.mclose .nulldelimiter}]{.mord}[]{.mspacestyle="margin-right:0.2222222222222222em;"}[∗]{.mbin}[]{.mspacestyle="margin-right:0.2222222222222222em;"}]{.base}[[]{.strutstyle="height:0em;vertical-align:0em;"}[压]{.mord.cjk_fallback}[力]{.mord .cjk_fallback}[机]{.mord.cjk_fallback}[线]{.mord .cjk_fallback}[程]{.mord.cjk_fallback}[数]{.mord .cjk_fallback}]{.base}]{.katex-htmlaria-hidden="true"}]{.katexslate-string="true"}]}slate-type="inline-katex" 我不打算再将此公式复杂化，所以就不再用字母替代了。 这就是我经常提到的，**对于压力工具来说，只要不报错，我们就关心 TPS和响应时间就可以了，因为 TPS反应出来的是和服务器对应的处理能力，至少压力线程数是多少，并不关键**。我想这时会有人能想起来 JMeter 的 BIO 和 AIO之争吧。 你也许会说，这个我理解了，服务端有多少个线程，就可以支持多少个压力机上的并发线程。但是这取决于TPS有多少，如果服务端处理的快，那压力机的并发线程就可以更多一些。 这个逻辑看似很合理，但是通常服务端都是有业务逻辑的，既然有业务逻辑，显然不会比压力机快。 应该说，服务端需要更多的线程来处理压力机线程发过来的请求。所以我们用几台压力机就可以压几十台服务端的性能了。 如果在一个微服务的系统中，因为每个服务都只做一件事情，拆分得很细，我们要注意整个系统的容量水位，而不是看某一个服务的能力，这就是拉平整个系统的容量。 我曾经看一个人做压力的时候，压力工具中要使用 4000个线程，结果给服务端的 Tomcat 上也配置了 4000 个线程，结果 Tomcat一启动，稍微有点访问，CS就特别高，结果导致请求没处理多少，自己倒浪费了不少CPU。 总结通过示意图和示例，我描述了在线用户数、并发用户数、TPS（这里我们假设了一个用户只对应一个事务）、响应时间之间的关系。有几点需要强调： 1.       通常所说的并发都是指服务端的并发，而不是指压力机上的并发线程数，因为服务端的并发才是服务器的处理能力。        2.       性能中常说的并发，是用 TPS    这样的概念来承载具体数值的。        3.       压力工具中的线程数、响应时间和 TPS    之间是有对应关系的。        这里既没有复杂的逻辑，也没有复杂的公式。希望你在性能项目中，能简化概念，注重实用性。 思考题如果你吸收了今天的内容，不妨思考一下这几个问题： 如何理解"服务端的并发能力"这一描述？我为什么不提倡使用"绝对并发"和"相对并发"的概念呢？以及，我们为什么不推荐用CPU 来计算并发数？ 