are not many pixels in the binary mask of frequency map (f req),
the local peaks more likely correspond to the moving traces left
by moving objects. Therefore, the success rate of peak-based at-
tack by using density matrix of f req is higher than the other two.
For the same reason, a small grid interval, such as 40 pixels, may
result in many meaningless valleys in the density matrix of f req
(e.g., Figure 7(c)) and thus likely lead to a lower success rate. This
explains why in the experiment for f req, ‘valley+40’ has a lower
success rate than the other three conﬁgurations i.e., ‘valley+60’,
‘peak+40’, and ‘peak+60’ (Figure 8(e)).
Experiment on the number of drag-and-drops (Figure 8(b)(d)(f))
indicates that a successful automated attack usually requires an av-
erage of 30∼40 drag-and-drops to complete the CAPTCHA. which
is much higher than our threshold of 21.
5. USABILITY AND RELAY ATTACK
RESISTANCE
In this section, we present the design and the results of our study
to evaluate the usability of our proposed EI-DCGs with the three
difﬁculty levels (Easy, Medium, Difﬁcult). Then, we demonstrate
the resilience of EI-DCGs against relay attacks. To this end, we
present two relay attack studies we conducted to measure the per-
formance of the Stream Relay attack (Section 2) against EI-DCGs.
Finally, we develop a Stream Relay attack detection mechanism
based on the differences between the users’ solving performance in
the attack studies and the usability study. We note that the Static
Relay attack against EI-DCGs will naturally fail given that a single
or multiple static snapshots of the underlying emerging image does
not reveal much meaningful information to a human solver (thus,
we do not evaluate this attack).
5.1 Usability Study
Study Design
In our usability study of EI-DCGs, we use EI-Nu as a baseline –
our goal is to compare the usability of EI-DCGs with that of EI-Nu.
Basically, we wanted to determine how much usability degradation
occurs in EI-DCG over EI-Nu, as a trade-off to enhancing the secu-
rity against relay attacks. We utilized the Amazon Mechanical Turk
(MTurk) service to recruit participants for the study. The study was
approved by our University’s Institutional Review Board.
5.1.1
Each EI-Nu CAPTCHA challenge is of size 285×125 and is dis-
played as a 6-second video that loops continuously. We asked the
participants to type the three characters of the challenge in a textbox
and press the “submit” button when they are done. Each EI-DCG
challenge is of size 360×400. The user task is to drag-drop the an-
swer objects to their corresponding target objects within 60 seconds
(time-out). If the user cannot complete the task within the 60 sec-
onds, the challenge is considered unsuccessful. We generated 100
challenges for each category of tested CAPTCHAs: EI-Nu and EI-
DCG (Easy, Medium and Difﬁcult). We employ a within-subjects
experimental design, where we ask each participant to solve 5 chal-
lenges each for all the four categories. The order of presenting the
four categories (EI-Nu, EI-DCG_Easy, EI-DCG_Medium, and EI-
DCG_Difﬁcult) followed the standard 4×4 Latin square to reduce
the effect of learning biases, while the challenges within each cat-
egory followed a random order. We recruited 120 MTurk workers,
and the experiment took 27 minutes on an average to complete per
worker.
We subjected the MTurk workers to a consent form. Then, we
asked them to ﬁll-out a demographics form, solve ﬁve challenges of
17
one of the tested CAPTCHA categories, and ﬁll-out a survey form
about their experience. The survey contains the 10 System Usable
Scale (SUS) [6] standard questions, each with 5 possible responses
(5-point Likert scale, where strong disagreement is represented by
“1” and strong agreement is represented by “5”). We used a similar
design to test the rest of the categories. The demographics of the
study participants are shown in the second column of Table 3.
Table 3: Demographics of participants in the usability and relay attack studies
Usability
Stream Relay Attack
Participants Study Type
Participants Size (N=195)
LSHL
N = 27
HSLL
N = 48
N = 120
Age (%)
0
26.6
44.2
19.2
10
Gender (%)
39.2
60.8
Education (%)
0
16.7
70.8
8.3
4.2
25
75
0
50
Female
Male
High School
Bachelor
Master
PhD
2.1
8.3
58.3
27.1
4.2
27.1
72.9
22.9
72.9
4.2
0
22.9
10.4
2.1
4.2
25
12.5
8.3
14.6
30.8
43.3
24.2
1.7
62.5
33.3
4.2
Field of Study/Profession (%)
25
29.2
4.2
4.2
20.8
8.3
0
8.3
31.7
10.8
0.8
2.5
5.8
12.5
35.9
0
Computer Science
Engineering
Medicine
Journalism
Finance
Business
Social sciences
other
5.1.2 Results
We evaluated the usability of the tested CAPTCHA categories
with respect to the measures of: (1) solving time, (2) error rate, and
(3) user experiences, as described below. The results are summa-
rized in Table 4.
Solving time: We calculated the solving time as the time taken
by the participants to solve each challenge. In case of EI-Nu, we
start measuring the timing from the time the challenge is displayed
till the submit button is pressed. Whereas, in case of EI-DCG, we
record the timing till the participants drag-drop all the answer ob-
jects to their corresponding target objects. We considered in our
calculation the time taken only corresponding to the challenges
solved successfully. The average solving time is shown in the third
column of Table 4.
The time taken to solve EI-DCG challenges is about double the
time taken to solve EI-Nu challenges. However, it is still less than
25 seconds on average. Moreover, the time for solving EI-DCG
SUS
Time (sec)
#Drags
Table 4: The solving time, error rate, number of drags, number of attempts and SUS
scores for the usability study
Challenge
Type
EI-Nu
EI-DCG
Easy
Medium
Difﬁcult
#Attempts Error
Rate
0.16
55.94 (19.75)
57.15 (18.09)
56.00 (20.08)
19.82 (10.20)
21.55 (10.55)
23.34 (11.60)
3.82 (1.79)
3.82 (1.58)
3.84 (1.85)
1.17 (1.86)
1.51 (2.42)
1.44 (2.03)
mean (std. dev.)
0.13
0.10
0.13
71.75 (18.39)
10.34 (6.21)
N/A
N/A
2
increases with the difﬁculty level of EI-DCG. A Friedman’s test
showed a statistically signiﬁcant difference between the solving
(3) = 500.06, p < 0.001).
time of the four tested categories (χ
Further analyzing using pairwise Wilcoxon Signed-Rank test with
Bonferroni correction, we found a statistically signiﬁcant differ-
ence between all of the tested pairs (p < 0.001).
Error Rate: The error rate represents the percentage of the chal-
lenges that were not solved successfully by the participants. The
last column of Table 4 shows the error rate for solving each of the
tested CAPTCHA categories. All of the categories had low error
rate with the minimum error rate for EI-DCG medium as 0.10 and
the maximum for EI-Nu as 0.16. The lower error rate in EI-DCGs
compared to EI-Nu may be attributed to to the momentary feed-
back that EI-DCG provides. Whenever the participant drag-drops
a correct object to its corresponding target, the object disappears,
which informs the user he performed a correct drag-drop. However,
EI-Nu does not check the users response until after he submits the
whole answer of the challenge.
Further, we analyzed the number of drag-drops performed by
the participants, which represents the error rate per drag. We no-
ticed that a minimum of three drag-drops is required to complete
any challenge and on an average the users performed less than four
drag-drops in all the EI-DCG categories. Finally, we analyzed the
number of attempts (clicks that do not correspond to object drag)
performed by the users and we found the users performed less than
2 attempts on average for all of the EI-DCG categories. Upon fur-
ther analysis of the collected logs, we found that some of the partic-
ipants performed many drags and attempts (up to 37) while solving
challenges. However, the fraction of such actions is extremely low,
which conﬁrms that we can limit the number of allowed drags and
drops to 21 to limit the ability of density-based automated attack (as
analyzed in Section 4) successfully without impacting the usability
much. The overall error rate, after limiting to 21 drags/attempts,
becomes 0.14, 0.11, and 0.14 for Easy, Medium and Difﬁcult EI-
DCG, respectively. These errors rates are still similar to that of
EI-Nu.
User Experience (SUS Scores): The ﬁrst column of the Table 4
shows the SUS scores corresponding to the tested CAPTCHA cat-
egories. In our SUS calculations, we ignored the responses of 17
participants as they seem to answer the questionnaire randomly, ei-
ther by giving the same rating to all the questions or at least answer
two contradicting questions with the same answer (i.e., we removed
the responses from participants who answer both of “I found the
system unnecessarily complex” and “I thought the system was easy
to use” with “strongly agree”).
2
We ﬁnd degradation in the user experience in EI-DCG compared
to EI-Nu. However, the SUS scores for EI-DCG are still above
50.9, which means EI-DCGs have fair usability [5]. Comparing the
SUS scores using Friedman’s test shows statistical signiﬁcant dif-
(3) = 87.63, p
ference among the tested CAPTCHA categories (χ
< 0.0001). Further, we used pairwise Wilcoxon Signed-Rank test
with Bonferroni correction to assess the difference between each of
the pair of the four categories. Signiﬁcant differences are found (p
< 0.01) between EI-Nu and the three categories of EI-DCG. How-
ever, no signiﬁcant difference is found in the SUS score between
any pair of the EI-DCG categories.
Summary: The results of the usability study show some degra-
dation of the user experience represented in lower SUS score and
higher time taken to solve EI-DCG challenges compared to EI-Nu
challenges. However, the average SUS scores for EI-DCG show
that they still have fair usability. Moreover, the error rate decreased
slightly compared to EI-Nu. Given that EI-DCG offers higher se-
curity than EI-Nu, especially against relay attacks, we believe that
this degradation in usability may be acceptable.
5.2 Streaming-based Relay Attack Study
We conducted two studies to investigate the ability of EI-DCG
to resist Stream Relay attack introduced in [12]. The studies differs
in only the location of the participants. In the ﬁrst study, tagged
Low-Speed High-Latency (LSHL), we recruited participants from
a developing country (India), where we expect the users to have
a slow Internet connection and they reside on far proximity of the
attacker (residing in the USA). In the second study, High-Speed
Low-Latency (HSLL), we recruited participants from a developed
country (USA), where we expect the users to have a fast Internet
connection and they reside in near proximity of the attacker (USA).
These two attack models emulate realistic relay attack scenarios
[12]. In both models, the human-solvers attempt to solve EI-DCG
challenges that are streamed to them in-real time from the attacker’s
machine using the VNC streaming software.
Study Design
5.2.1
Following the study design in the usability study, we employed a
within-subjects experimental design, where we ask each participant
to solve 5 challenges for all of the three EI-DCG categories. The or-
der of presenting the three categories followed standard 3×3 Latin
square, and the challenges within each category followed a ran-
dom order. We asked the MTurk workers to connect to a computer
which resides in our university (streaming server) via the RealVNC
Java applet (streaming client). Then, we subject them to consent an
agreement. Next, we ask them to ﬁll-out a demographics form, and
solve ﬁve challenges of one of the categories. We followed a sim-
ilar design to test the other categories. The study was approved by
our University’s Institutional Review Board.
We conducted two separate streaming-based relay attack studies.
The two studies differ only in the location of the participants. In the
ﬁrst study (LSHL), we recruited 27 participants in India which sim-
ulate the real scenario settings in which the attacker is in USA and
hires human-solvers in far and developing countries. In the second
study (HSLL), we recruited 48 participants from USA. The sec-
ond study is to assess how much the performance of the attack will
increase when the attacker recruits solvers in near proximity and
from developed countries. The demographics of our participants
are shown in columns 3-4 of Table 3. The participant characteris-
tics in our relay attack studies are in line with that in our usability
study, allowing us to fairly compare the two settings in a between-
subjects design.
5.2.2 Results
We evaluated the performance of the participants in solving EI-