developers of the Android framework do in fact follow a
certain regular coding style, or duplicate parts of one method’s
implementation when implementing another. These social
aspects of software development lead to a certain degree of
regularity and redundancy in the code base, which a machine-
learning approach such as ours can discover and take advantage
of.
Though we have a large number of distinct features, most
of them are instances of the same parameterized class. For
example, the “method name starts with” feature class has
instances “method name starts with get”, “method name starts
with put”, and so on. For identifying sources and sinks, SUSI
uses the following classes of features:
• Method Name: The method name contains or starts
with a speciﬁc string, e.g., “get”, which can be an
indicator for a source.
•
•
• Method has Parameters: The method has at least
one parameter. Sinks usually have parameters, while
sources might not.
Return Value Type: The method’s return value is of
a speciﬁc type. A returned cursor, for instance, hints
at a source, while a method with a void return value
is rarely ever a source.
Parameter Type: The method receives a parameter
of a speciﬁc type. This can either be a concrete type
or all types from a speciﬁc package. For instance, a
parameter of type java.io.* hints at a source or a sink.
Parameter is an Interface: The method receives a
parameter of an interface type. This is often the case
with methods that register callbacks. Note that such
methods are neither sources nor sinks according to
our deﬁnition, since they do not perform any actual
operation on the data itself.
•
•
• Method Modiﬁers: The method is static/native/etc.
Static methods are usually neither sources nor sinks,
with some exceptions. Additionally, sources and sinks
are usually public.
Class Modiﬁers: The method is declared in a protect-
ed/abstract etc. class. Methods in protected classes are
usually neither sources nor sinks.
Class Name: The method is declared in a class whose
name contains a speciﬁc string, e.g., Manager.
Dataﬂow to Return: The method invokes another
method starting with a speciﬁc string (e.g. read in the
case of a source). The result of this call ﬂows into the
original method’s return value. This hints at a source.
Dataﬂow to Sink: One of the method’s parameter
ﬂows into a call to some other method starting with
•
•
•
•
•
a speciﬁc string, e.g., update, which would suggest a
sink.
Data Flow to Abstract Sink: One of the method’s
parameter ﬂows into a call to an abstract method. This
is a hint for sink as many command interfaces on the
hardware abstraction layers are built on top of abstract
classes.
Required Permission: Invoking the method requires a
speciﬁc permission. There is one such feature for every
permission declared in the Android API. We were only
able to use this feature on the approximately 12,600
methods for which we had permission annotations from
the PScout [22] list.
Some features, in particular “Method Name”, might sound
na¨ıve at ﬁrst, but it turns out that such syntactic features are
among the ones that correlate the strongest with sources and
sinks. Of course, their effect is only positive in combination
with other features; one could not, for instance, detect sources
by only looking at preﬁxes of method names.
All our features can assume one of three values: “True”
means that the feature applies, i.e., a method does indeed start
with a speciﬁc string. “False” means that the feature does not
apply, i.e., the method name does not have the respective preﬁx.
“Not Supported” means that the feature cannot be decided for
this speciﬁc method. The latter can happen if, for example, the
feature needs to inspect the method body, but no implementation
is available in the current Android version’s platform JAR ﬁle.
The details of our dataﬂow features are explained in
Section IV-D. SUSI’s features for categorizing sources and
sinks can be grouped as follows:
Class Name: The method is declared in a class whose
name contains a speciﬁc string, e.g., Contacts.
•
• Method Invocation: The method directly invokes
another method whose fully-qualiﬁed name starts with
a speciﬁc string, e.g., com.android.internal.telephony
for Android’s internal phone classes. This feature does
not consider the transitive closure of calls starting at
the current method.
Body Contents: The method body contains a reference
to an object of a speciﬁc type, e.g. android.telephony
.SmsManager for the SMS MMS category).
Parameter Type: The method receives a parameter of
a speciﬁc type (similar feature as for the classiﬁcation
problem with different instances).
Return Value Type: The method’s return value is
of a speciﬁc type, e.g., android.location.Country for
regional data.
•
•
•
Note that we do not use permission-based features for the
categorization, since many methods require permissions for
internal functionality not directly related to their respective
category. For instance, a backup method requests many per-
missions, but does not necessarily give out all of the data it
accesses using these permissions if it only creates an internal
save point that can be restored later. The permission list alone
thus does not directly relate to the method’s category.
7
It becomes apparent that semantic features are much more
suitable for identifying sources and sinks than for categorizing
them. On the source-code level, Android’s sources and sinks
share common patterns which can be exploited by our dataﬂow
feature. For ﬁnding categories, however, there seems to be
no such technical distinction and SUSI must rather rely on
syntactical features such as class and method names.
D. Dataﬂow Features
As we found through empirical evaluation, considering a
method’s signature and the syntax of its method body alone
is insufﬁcient to reliably detect sources and sinks. With such
features alone we were unable to obtain a precision or recall
higher than about 60%. It greatly helps to take the data ﬂows
inside the method into consideration as well. Recall from our
deﬁnitions in Section III that sources must read from and sinks
must write to resources.
To analyze data ﬂows, we originally experimented with a
highly precise (context-, ﬂow- and object-sensitive) data-ﬂow
analysis based on Soot [29], but found out that this did not easily
scale to the approximately 110,000 methods of the Android
SDK. Computing precise call graphs and alias information
simply took too long to be practical. We thus changed to
a much more coarse-grained intra-procedural approximation
(also based on Soot1) which runs much faster whilst remaining
sufﬁciently precise for the requirements of our classiﬁcation.
Keep in mind that the result of the data-ﬂow analysis is only
used as one feature out of many. Thus, it sufﬁces if the analysis
is somewhat precise, i.e., produces correct results with just a
high likelihood.
Our data-ﬂow features are all based on taint tracking inside
the Android API method m to be classiﬁed. Depending on the
concrete feature, we support the following analysis modes:
•
•
•
Treat all parameters of m as sources and calls to
methods starting with a speciﬁc string as sinks. This
can hint at m being a sink.
Treat all parameters of m as sources and calls to
abstract methods as sinks. This can hint at m being a
sink.
Treat calls to speciﬁc methods as sources (e.g. ones that
start with ”read”, ”get”, etc.) and the return value of
m as the only sink. This can hint at m being a source.
Optionally, parameter objects can also be treated as
sinks.
Based on this initialization, we then run a ﬁxed-point
iteration with the following rules:
•
•
•
•
If the right-hand side of an assignment is tainted, the
left-hand side is also tainted.
If at least one parameter of a well-known transformer
method is tainted, its result value is tainted as well.
If at least one parameter of a well-known writer method
is tainted, the object on which it is invoked is tainted
as well.
If a method is invoked on a tainted object, its return
value is tainted as well.
1We take the android.jar built from the OS and the system applications on
a real phone (Galaxy Nexus running Android 4.2) as input for Soot.
•
If a tainted value is written into a ﬁeld, the whole base
object becomes tainted. For arrays, the whole array
becomes tainted respectively.
When the ﬁrst source-to-sink connection is found, the ﬁxed-
point iteration is aborted and the dataﬂow feature returns “True”
for the respective method to which it was applied. If the
dataﬂow analysis completes without ﬁnding any source-to-sink
connections, the feature returns “False”.
While such an analysis would be too imprecise for a general-
purpose taint analysis, it is very fast and usually reaches its
ﬁxed point in less than three iterations over the method body.
Since the analysis is intra-procedural, its runtime is roughly
bounded by the number of statements in the respective method.
Instead of using ﬁxed initialization rules as explained above,
one can also ﬁrst run the machine learning algorithm wihout
the data ﬂow feature enabled, and then initialize the data ﬂow
feature with the results of this preliminary round. This method
can be applied incrementally until a ﬁxed point is reached. We
plan to investigate the tradeoffs involved with this method in
the future.
E. Implicit Annotations for Virtual Dispatch
SUSI’s implementation is based on Weka, a generic
machine-learning tool, which has no knowledge about the
language semantics of Java. However, we found that when
annotating methods to obtain training data it would be beneﬁcial
to propagate method annotations up and down the class
hierarchy in cases in which methods are inherited. Such a
propagation models the semantics of virtual dispatch in Java.
We thus extended SUSI such that if encountering an annotated
method A.foo, the annotation is implicitly carried over also to
B.foo in case B is a subclass of A that does not override foo
itself, thus inheriting the deﬁnition in A. Similarly, if B.foo
were annotated, but not A.foo, we would copy the annotation
in the other direction.
For our subset of 12,600 methods with permission anno-
tations taken from the PScout list [22], SUSI was able to
automatically create implicit annotations for 305 methods. After
loading the remaining methods of the Android API to get our
full list of 110,000 methods, SUSI was able to automatically
annotate another 14 methods.
V. EVALUATION
Our evaluation considers the following research questions:
RQ1
Can SUSI be used to effectively ﬁnd sources and
sinks with high accuracy?
Can SUSI be used to categorize the found sources
and sinks with high accuracy?
RQ3 Which kind of sources and sinks are used in
RQ2
RQ4
RQ5
malware apps?
How do the sources and sinks change during
different Android versions? Can SUSI be used
to identify sources and sinks in new, previously
unseen Android versions?
How complete are the lists of sources and sinks
distributed with existing Android analysis tools
and how do they relate to SUSI’s outputs?
A. RQ1: Sources and Sinks
To assess the precision and recall of SUSI on our training
data, we applied a ten fold cross validation and report the
results in Section V-A1. Since the test data used for the
cross validation is picked randomly, the results of the cross
validation usually carry over to the complete classiﬁcation
performance on unknown training sets if the test set was
sufﬁciently representative. To conﬁrm that this actually holds,
we manually evaluated the source and sink lists SUSI generated
for the Google Mirror and Google Cast APIs and report the
results in Section V-A2. The Google Cast API is used for
the communication between an Android-based smartphone and
Google’s Chromecast device [20]. The Google Mirror API
links an Android device to Google Glass [19]. We chose these
two APIs to show that SUSI is actually able to efﬁciently
handle even previously unseen Android or Java APIs. Note that
neither API is included in the base Android system. Secondly,
both APIs include methods that handle personal data, such as
location or network information. To the best of our knowledge
no taint analysis tool has considered these APIs yet. Thirdly,
the APIs are of manageable size, making a complete manual
validation of SUSI’s results practical.
1) Cross Validation: We envision SUSI to be used as an
automated approach in which experts like ourselves hand-
annotate parts of the Android API and then use SUSI to
automatically extrapolate these annotations to larger parts of
the API. Of course, such an approach only makes sense if the
extrapolation is meaningful, which is equivalent to delivering
a high precision and recall. Measuring precision and recall is
hard in this setting, as one has no gold standard to work with:
there is no correctly pre-annotated Android API with which one
could compare SUSI’s results. Thus, as a best-effort solution
we hand-annotated a subset of the Android API ourselves
(details below) and then used these methods both as training
and test data in a ten-fold cross validation [30] which is the
standard approach for evaluating machine-learning techniques.
It works by randomly dividing all training data into 10 equally-
sized buckets, training the classiﬁer on nine of them, and then
classifying the remaining bucket. The process is repeated 10
times, omitting another bucket from training each time. In the
end, SUSI reports the average precision and recall. For each
class c, precision is the fraction of correctly classiﬁed elements
in c within all elements that were assigned to c. If precision
is low it means that c was assigned many incorrect elements.
Recall is deﬁned as fraction of correctly classiﬁed elements in
c within all elements that should have been assigned to c. If
recall is low it means that c misses many elements.
Table II shows the results of this ten-fold cross validation
over our training set of 779 methods randomly picked from the
PScout subset [22] of about 12,600 methods. The training set
contains 13% source-, 22% sink- and 65% neither-annotations.
We started with this subset as it provided mappings between
methods and required permissions and thus enabled us to
also use Android permissions as features for our classiﬁer.
The averages we report in our tables are taken from Weka’s
output. They are weighted with the number of examples in
the respective class. Also note that, since our training set is
randomly picked, the precision and recall should carry over to
the entire Android API with high probability.
The following sections address these questions in order.
Our ﬁnal results for the source/sink classiﬁcation had to be
8
Category
Sources
Sinks
Neither
Weighted Average
TABLE II.
Recall [%] Precision [%]
92.3
82.2
94.8
91.9
89.7
87.2
93.7
91.9
SOURCE/SINK CROSS VALIDATION PSCOUT
computed without any permission features, though, since we
do not have permission associations for the complete Android
API2. For assessing the impact of the permission feature, we ran
the PScout subset again with the permission feature disabled,
yielding the results shown in Table III. Interestingly, the average
precision and recall are almost the same with the permission
feature and without. The impact of the permission feature is
apparently low enough for not having to worry about the lack of
permission information when analyzing the complete Android
4.2 API. Conversely, the results also indicate that permissions
alone are not a good indicator for identifying sources or sinks.
Recall [%] Precision [%]
Category
Sources
Sinks
Neither
Weighted Average
90.5
86.0
95.2
92.8