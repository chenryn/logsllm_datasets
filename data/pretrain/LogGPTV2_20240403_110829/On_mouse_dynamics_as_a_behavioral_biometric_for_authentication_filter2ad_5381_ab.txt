machines, yielded an average EER of 24.3%, over groups of
60 curves and an EER of 11.2% with 3600 curves per group.
1The session length was not explicitly stated in [2]; however,
it is stated that an average of 12 hours 55 minutes of data
was captured from each subject, representing an average of
45 sessions. We therefore assume that average session length
is 12:55/45=17.22 minutes.
3.2 Static Veriﬁcation Approaches
Several static MDA approaches have also been proposed
in the literature. Hashia et al. present an approach in which
enrollment involves moving the mouse pointer between pairs
of dots shown sequentially on the screen [8]. Features com-
puted from the user’s movement between each pair of dots
comprise the enrollment signature. Authenticating involves
the same series of dot-to-dot movements, which are com-
pared against the enrollment signature. Experiments in-
volving 15 students yielded an equal error rate of 15%. All
test subjects used the same computer and mouse.
Gamboa et al. extended their continuous veri(cid:12)cation ap-
proach into a static veri(cid:12)cation scheme for web-based appli-
cations [6]. In the proposed scheme, an authenticating user
enters a username and pin number via an on-screen virtual
keyboard embedded in the login web page, using only the
mouse. Mouse movements are recorded through javascript
embedded in the web page and are sent to an authentica-
tion server, which grants access based on the entered creden-
tials as well as the corresponding mouse movements. The
underlying mouse dynamics techniques are the same as in
their continuous authentication approach. Testing the sys-
tem with 50 subjects yielded an equal error rate of 6.2% for
15-digit pin numbers. As with their continuous approach,
the test data was used for feature selection, which results
in an overly optimistic estimate of the classi(cid:12)er’s accuracy.
The paper also overlooked the scenario in which a user en-
rolls using one computer and later attempts to authenticate
using a di(cid:11)erent computer; such a scenario is common in the
context of web-based applications.
Bours and Fullu proposed a static approach in which the
authenticating user utilizes the mouse to trace a winding
maze-like path while mouse movements are recorded and
used to compute velocity vectors for each segment of the
path [3]. Edit distance is used to compare the veri(cid:12)cation
data to the enrollment data. Experiments with 28 subject-
s yielded an EER of around 27%. Test subjects ran the
experiments using their own computers.
Revett et al. presented Mouse-Lock, in which a user au-
thenticates by manipulating a graphical, combination lock-
like GUI interface featuring icons, arranged on a circular
dial [12]. The entered combination, along with some timing
based features, are used to make an authentication decision.
A small-scale evaluation involving six subjects yielded an
average FAR and FRR of around 3.5% and 4% respectively.
4. LIMITATIONS OF EXISTING WORK
In the previous section, we described a number of exist-
ing MDA systems that appear to achieve reasonably low
error rates. However, we have identi(cid:12)ed several recurring
problems with respect to how these techniques have been
experimentally evaluated. We believe that these issues have
resulted in an overly optimistic view of the e(cid:11)ectiveness of
the current techniques. In this section, we discuss these is-
sues and some of their implications; in Section 5 we present
an overview of our study.
4.1 Impractical Veriﬁcation Time
The (cid:12)rst limitation that we observed is that many of the
existing approaches, particularly those providing continu-
ous veri(cid:12)cation, require a signi(cid:12)cant amount of mouse data
to be captured before a reasonably accurate authentication
decision can be made. This is clearly not practical for an
478
online system, as even a few minutes is more than enough
time for an adversary to compromise a system. We have
also observed that when these techniques are referenced in
other papers, their error rates are often cited without men-
tion of the corresponding veri(cid:12)cation time. We note that the
approach of [5] may at (cid:12)rst appear to be an exception, as
this technique was reported to achieve an EER of 2% with
less than a minute of veri(cid:12)cation data; however, it is likely
that the invalid feature selection step used in that approach
resulted in an overly optimistic estimate of its accuracy.
4.2 Uncontrolled Environmental Variables
Another limitation that we observed is that environmental
variables, which could potentially in(cid:13)uence mouse dynam-
ics, are not properly controlled from one test subject to the
next. Instead, the trend has been to collect mouse data from
test subjects by installing recording software on each sub-
ject’s personal computer (indeed this appears to have been
the case [2], [10], [14]). The problem with using a di(cid:11)eren-
t machine for each subject is that each test machine could
di(cid:11)er with respect to any number of software-related vari-
ables (e.g. screen resolution, pointer speed and acceleration
settings, and mouse polling rate, etc.) or with respect to
hardware-related variables, particularly the type of pointing
device used. As a consequence, it is unclear whether the
results of these prior evaluations actually re(cid:13)ect detectable
di(cid:11)erences in mouse behavior among test subjects, or di(cid:11)er-
ences in the con(cid:12)gurations among their machines.
4.3 Remote Access Scenario
Aside from providing host-based authentication for local
access, MDA techniques could also potentially be employed
in remote access scenarios, such as web-based applications.
To properly evaluate their e(cid:11)ectiveness in this scenario, we
must consider the fact that, in practice, a given user may
access a single web-based application from multiple comput-
ers. This could lead to the situation in which the enrollment
data for a user is collected under a di(cid:11)erent computing en-
vironment than the data used for veri(cid:12)cation. Thus, to be
e(cid:11)ective in this scenario, the authentication system must
be capable of recognizing a user’s behavior across di(cid:11)eren-
t computing environments. To the best of our knowledge,
the e(cid:11)ectiveness of mouse dynamics based authentication
has not been evaluated under this scenario in the existing
literature.
5. OVERVIEW OF OUR STUDY
We chose to focus this study on two primary questions
formulated from the limitations pointed out in the previous
section.
Q1: In the absence of any e(cid:11)ects caused by environmental
variables, is pointing device behavior alone su(cid:14)cient to ver-
ify user identity?
Q2: Is it possible to verify user’s identity when enrollment
and veri(cid:12)cation data are collected under di(cid:11)erent comput-
ing environments?
Although we do not directly address the problem of veri(cid:12)-
cation time (Section 4.1), the veri(cid:12)cation time used in all of
our experiments is arguably closer to what is likely to be ac-
ceptable in practical applications than that used in previous
experiments. The next subsection gives an overview of the
experiments that we performed in an attempt to answer the
above questions.
5.1 Overview of Experiments and Methods
Our experiments were performed using implementation-
s of the continuous veri(cid:12)cation approaches of Ahmed and
Traore [2] and Gamboa and Fred [5]. We chose these two
approaches because they were among the most frequently
cited and represented a relatively diverse set of mouse dy-
namics features.
The design of our (cid:12)rst experiment was geared toward an-
swering Q1. Therefore, it was necessary to examine the
e(cid:11)ectiveness of some of the existing MDA approaches in a
setting where the computing environment was tightly con-
trolled across all test subjects. We collected mouse data
from a group of users on the same computer under the same
conditions while performing the same task, and used this
data in an authentication experiment. Our hypothesis was
that the error rates achieved in this setting would be high-
er than those previously reported in the literature, since in
this case the authentication system would not be able to use
hardware or software di(cid:11)erences to distinguish among users.
Our second experiment was aimed at answering Q2; that
is, to evaluate existing techniques in a remote access sce-
nario, in which it is common for a single user to access
the system from di(cid:11)erent computing environments at dif-
ferent times (e.g. accessing a web-based application such
as a shopping or banking web site). Although there are
many variables that could potentially di(cid:11)er from one ma-
chine to another, we speculated that pointing device type
(e.g. mouse, touchpad, etc.) would have the greatest e(cid:11)ect
on behavior; therefore, we tested this scenario by using data
collected from one pointing device for enrollment and data
from the other pointing device for veri(cid:12)cation. All other
variables were held constant for the two sets of data collect-
ed for a given user (and across users). Our hypothesis was
that the pointing device would have a signi(cid:12)cant impact on
the user’s perceived behavior, which would cause an increase
in false rejections.
To provide further insight regarding Q2, we performed
a third experiment to determine if the mouse dynamics fea-
tures de(cid:12)ned by the two techniques could be used to identify
which of the two pointing devices generated a given session
of mouse data. Our reasoning behind this experiment was
that if these techniques were capable of discriminating da-
ta sessions according to the pointing device that generated
them, this would indicate that pointing device hardware it-
self exhibits a strong in(cid:13)uence on mouse dynamics.
5.2 Data Collection
We collected data from 17 volunteer subjects using two
di(cid:11)erent types of pointing devices, while performing a com-
mon web browsing task. The subjects, eight males and nine
females, were all computer science students from our depart-
ment. With one exception, all subjects were right handed.
We set up two identical computers in our lab and equipped
one with a USB optical mouse and the other with a USB
touchpad. The subjects were given a speci(cid:12)c web browsing
task designed to last 30 minutes and were asked to perform
that task once for each of the two pointing devices.
5.2.1 Apparatus
We used two identical computers for data collection so
that we could collect data from multiple subjects at once;
we made every e(cid:11)ort to control software and hardware fac-
tors other than the pointing device itself from having any
479
unintended in(cid:13)uence on the subject’s recorded mouse be-
havior. Both computers were Dell Dimension XPSs with
Pentium 4, 3.20 GHz processors and 1GB of RAM; both
were equipped with identical 21" Dell LCD monitors (set at
1280x1024 resolution). We equipped one of the computers
with a USB Logitech optical mouse and the other with a US-
B Adesso Browser Cat 2 Button Touchpad. Both computers
were loaded with a copy of Windows Vista from the same
image (cid:12)le. The Google Chrome web browser and our custom
mouse recording software were installed on both computers.
The default Windows Vista drivers were used for the op-
tical mouse and the GlidePoint 3.3 driver was used for the
touchpad. Both pointing devices used a polling rate of 125hz
(8ms) and the pointing speed setting in the operating sys-
tem was left at the default value. Our custom mouse event
logging software, implemented in C#, ran as a background
process and used a Windows mouse hook to intercept all
mouse events, which were written to a (cid:12)le.
5.2.2 Procedure and Experimental Task
Subjects were quickly briefed regarding the purpose of the
experiment and given the instructions for completing the ex-
perimental task. Upon starting the experiment, the Chrome
web browser was automatically opened to display the Ama-
zon.com website and the mouse recording software was ini-
tiated. After 30 minutes, the subject was asked to move
to the other computer, which was equipped with a di(cid:11)eren-
t pointing device (but otherwise identical), and repeat the
experimental task. The experimental task was essentially
a scavenger hunt on the popular shopping web site, Ama-
zon.com. Subjects were provided with a long list of items
that can be purchased on Amazon and were instructed to
browse for and locate the items using only the pointing de-
vice. The list included things such as \(cid:12)nd 4 DVD movies,
each under $10" and \(cid:12)nd 3 books each by a di(cid:11)erent au-
thor", etc. We created two di(cid:11)erent but comparable lists of
items for this experiment and each list was associated with
one of the two pointing devices for the entire experiment; all
subjects used the same two lists of items.
6. EXPERIMENTS AND RESULTS
Recall that the speci(cid:12)c MDA approaches used in our ex-
periments were those of Ahmed and Traore [2] and Gamboa
and Fred [5]. For brevity, we will refer to the two approaches
as Ahmed and Gamboa respectively in this section. In the
following subsections, we (cid:12)rst describe how the data was pre-
pared for use by the two approaches, followed by the details
of each of the three experiments.
6.1 Data Preparation
The data collection procedure produced two sets of raw
data: one from the optical mouse and the other from the
touchpad. Two versions of these datasets were created by
preprocessing the data according to the requirements of the
two MDA approaches used in this study. The Ahmed ap-
proach speci(cid:12)es that the raw event stream is to be segmented
into actions and then groups of consecutive actions are ag-
gregated into sessions, over which various features are com-
puted. By contrast, in the Gamboa approach the raw event
stream is segmented into strokes and features are computed
directly over each stroke; classi(cid:12)cation is then done at the
stroke level, and the classi(cid:12)cation of a sequence of consec-
utive strokes is determined according to the average classi-
(cid:12)cation of the constituent strokes. Thus, in a sense, both
approaches can be viewed as making a single authentica-
tion decision based on a sequence of individual actions. For
brevity, we will subsequently use the term session to denote
either a group of consecutive strokes or a group of actions.
Since di(cid:11)erent users produced di(cid:11)erent amounts of data,
we trimmed each user’s data, to 325 actions (225 strokes for
Gamboa) for each pointing device, which was the minimum
action count across all users. After the trimming process,
each user’s data was divided into (cid:12)ve equal-length sessions
of 65 actions (25 strokes) each.
We opted not to perform the per-user feature selection
step for Gamboa as speci(cid:12)ed in [5]. In that paper, the fea-
ture selection step was performed using each user’s test set,
which is known to bias the classi(cid:12)er to the test data and
produce unrealistic results. We found that using the train-
ing set for feature selection gave comparable or worse results
than doing no feature selection at all and that using part of
the data as a validation set was not feasible due to the lim-
ited amount of data available to us. Therefore, we omitted
feature selection and used all features for every user.
6.2 Experiment 1: controlled environment
Our (cid:12)rst experiment investigated the e(cid:11)ectiveness of the
two MDA approaches when all environmental variables were
constant across users. The procedure was the same for both
of the approaches and was performed as follows: let n denote