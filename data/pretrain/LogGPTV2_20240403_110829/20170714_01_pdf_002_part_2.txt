table
p2
file file file file file file
inner p3 inner p2 inner p1 outer p1 outer p2 outer p3
shrink
file file file file file file file file
inner p7 inner p6 inner p5 inner p4 outer p4 outer p5 outer p6 outer p7
inner outer
relation relation
hash
table
p2
file file file file file file
inner p3 inner p2 inner p1 outer p1 outer p2 outer p3
probe
file file file file file file file file
inner p7 inner p6 inner p5 inner p4 outer p4 outer p5 outer p6 outer p7
inner outer
relation relation
Hash join behaviour modes
• “Optimal” — the planner thinks the hash table will fit in memory,
and the executor finds this to be true
• “Good” — the planner thinks that N > 1 batches will allow every
batch to fit in work_mem, and the executor finds this to be true
• “Bad” — as for “optimal” or “good”, but the executor finds that it
needs to increase the number of partitions, dumping some of
tuples out to disk, and possibly rewriting outer tuples
• “Ugly” — as for “bad”, but the executor finds that the data is
sufficiently skewed that increasing the number of batches won’t
help; it stops respecting work_mem and hopes for the best!
Out of memory: Kill process 1020 (postgres) score 64 or sacrifice child
Killed process 1020 (postgres) total-vm:445764kB, anon-rss:140640kB, file-rss:136092kB
Optimal
SET work_mem = '64MB';
SELECT COUNT(*) FROM simple r JOIN simple s USING (id);
Aggregate (cost=65418.00..65418.01 rows=1 width=8) (actual time=1496.156..1496.156 rows=1 loops=1)
-> Hash Join (cost=30834.00..62918.00 rows=1000000 width=0) (actual time=603.086..1369.185 rows=1000000 loops=1)
Hash Cond: (r.id = s.id)
-> Seq Scan on simple r (cost=0.00..18334.00 rows=1000000 width=4) (actual time=0.019..161.704 rows=1000000 loops=1)
-> Hash (cost=18334.00..18334.00 rows=1000000 width=4) (actual time=598.441..598.441 rows=1000000 loops=1)
Buckets: 1048576 Batches: 1 Memory Usage: 43349kB
-> Seq Scan on simple s (cost=0.00..18334.00 rows=1000000 width=4) (actual time=0.033..250.199 rows=1000000 loops=1)
Good
SET work_mem = '1MB';
SELECT COUNT(*) FROM simple r JOIN simple s USING (id);
Aggregate (cost=81046.00..81046.01 rows=1 width=8) (actual time=1985.022..1985.022 rows=1 loops=1)
-> Hash Join (cost=34741.00..78546.00 rows=1000000 width=0) (actual time=556.620..1851.942 rows=1000000 loops=1)
Hash Cond: (r.id = s.id)
-> Seq Scan on simple r (cost=0.00..18334.00 rows=1000000 width=4) (actual time=0.039..253.158 rows=1000000 loops=1)
-> Hash (cost=18334.00..18334.00 rows=1000000 width=4) (actual time=555.067..555.067 rows=1000000 loops=1)
Buckets: 32768 Batches: 64 Memory Usage: 808kB
-> Seq Scan on simple s (cost=0.00..18334.00 rows=1000000 width=4) (actual time=0.007..254.166 rows=1000000 loops=1)
Bad
SET work_mem = '1MB';
SELECT COUNT(*) FROM simple r JOIN bigger_than_it_looks s USING (id);
Aggregate (cost=30453.00..30453.01 rows=1 width=8) (actual time=2191.448..2191.449 rows=1 loops=1)
-> Hash Join (cost=8356.50..30450.50 rows=1000 width=0) (actual time=644.671..2065.686 rows=1000000 loops=1)
Hash Cond: (r.id = s.id)
-> Seq Scan on simple r (cost=0.00..18334.00 rows=1000000 width=4) (actual time=0.025..192.848 rows=1000000 loops=1)
-> Hash (cost=8344.00..8344.00 rows=1000 width=4) (actual time=643.542..643.542 rows=1000000 loops=1)
Buckets: 32768 (originally 1024) Batches: 64 (originally 1) Memory Usage: 808kB
-> Seq Scan on bigger_than_it_looks s (cost=0.00..8344.00 rows=1000 width=4) (actual time=0.022..331.981 rows=1000000 loops=1)
Ugly
SET work_mem = '1MB';
SELECT COUNT(*) FROM simple r JOIN awkwardly_skewed s USING (id);
Aggregate (cost=30453.00..30453.01 rows=1 width=8) (actual time=1687.089..1687.090 rows=1 loops=1)
-> Hash Join (cost=8356.50..30450.50 rows=1000 width=0) (actual time=1047.639..1571.196 rows=1000000 loops=1)
Hash Cond: (r.id = s.id)
-> Seq Scan on simple r (cost=0.00..18334.00 rows=1000000 width=4) (actual time=0.018..171.964 rows=1000000 loops=1)
-> Hash (cost=8344.00..8344.00 rows=1000 width=4) (actual time=625.913..625.913 rows=1000000 loops=1)
Buckets: 32768 (originally 1024) Batches: 2 (originally 1) Memory Usage: 35140kB
-> Seq Scan on awkwardly_skewed s (cost=0.00..8344.00 rows=1000 width=4) (actual time=0.019..330.268 rows=1000000 loops=1)
Joins
Hash Tables
Simple Hash Joins
Multi-Batch Hash Joins
Parallel Hash Joins
Open Problems
Questions
Parallel query recap
“Partial plans” are plans that can be run by many
•
workers in parallel, so that each will generate a
fraction of the total results
Parallel Sequential Scan and Parallel Index Scan
•
nodes emit tuples to the nodes above them using
page granularity
Every plan node above such a scan is part of a
•
partial plan, until parallelism is terminated by a
Gather or Gather Merge node
Parallel-oblivious hash joins
in PostgreSQL 9.6 & 10
A Hash Join node can appear in a partial plan
•
It is not “parallel aware”, meaning that it isn’t doing
•
anything special to support parallelism: if its outer plan
happens to be partial, then its output will also be partial
Problem 1: the inner plan is run in every process, and a
•
copy of the hash table is built in each
Problem 2: since there are multiple hash tables with
•
their own ‘matched’ flags, we can’t allow full or right
outer joins to be parallelised
Amdahl’s outlaw
• Parallelising the probe phase but not
the build phase sounds a bit like a
classic ‘Amdahl’s law’ situation…
• The effect may be worse than merely
limiting potential speed-up: running
N copies of the same plan generates
contention on various resources, and
storing the clone hash tables takes
memory away from other sessions
• These are externalities not included
in our costing model
Approaches
Partition-wise join (in development)
•
Dynamic repartitioning (various strategies exist)
•
Shared hash table (proposed)
•
Which?
Partition-wise joins work with parallel-oblivious join operators,
•
but requires the user to have declared suitable partitions
State-of-the-art cache-aware repartitioning algorithm “radix
•
join” adds a costly multi-pass partitioning phase, minimising
cache misses during probing
Several researchers claim that a simple shared hash table is
•
usually about as good, and often better in skewed cases[1]
[2], despite cache misses; not everyone agrees[3]
The bar for beating a no-partition shared hash table seems
•
very high, in terms of engineering challenges
Proposal: shared hash table
Tuples and hash table stored in memory from new ‘DSA’
•
allocator; special relative pointers must be used
• Insertion into buckets using compare-and-swap
• Wait for all peers at key points — in common case just end of
build, but in multi-batch case more waits — using a ‘barrier’ IPC
mechanism
• Needs various shared infrastructure: shared memory allocator
(DSA), shared temporary files, shared tuplestores, shared record
typmod registry, barrier primitive + condition variable
• Complications relating to leader process’s dual role
SELECT COUNT(*)
FROM simple r
JOIN simple s USING (id)
JOIN simple t USING (id)
JOIN simple u USING (id);
Finalize Aggregate (cost=1228093.57..1228093.58 rows=1 width=8) (actual time=24324.455..24324.456 rows=1 loops=1)
-> Gather (cost=1228093.15..1228093.56 rows=4 width=8) (actual time=24010.300..24324.433 rows=5 loops=1)
Workers Planned: 4
Workers Launched: 4
-> Partial Aggregate (cost=1227093.15..1227093.16 rows=1 width=8) (actual time=24004.404..24004.405 rows=1 loops=5)
-> Hash Join (cost=925007.40..1220843.10 rows=2500020 width=0) (actual time=19254.859..23819.648 rows=2000000 loops=5)
Hash Cond: (r.id = u.id)
-> Hash Join (cost=616671.60..850006.80 rows=2500020 width=12) (actual time=12700.426..15914.957 rows=2000000 loops=5)
Hash Cond: (r.id = t.id)
-> Hash Join (cost=308335.80..479170.50 rows=2500020 width=8) (actual time=6255.527..8065.931 rows=2000000 loops=5)
Hash Cond: (r.id = s.id)
-> Parallel Seq Scan on simple r (cost=0.00..108334.20 rows=2500020 width=4) (actual time=0.010..358.957 rows=2000000 loops=5)
-> Hash (cost=183334.80..183334.80 rows=10000080 width=4) (actual time=6188.294..6188.294 rows=10000000 loops=5)
Buckets: 16777216 Batches: 1 Memory Usage: 482635kB
-> Seq Scan on simple s (cost=0.00..183334.80 rows=10000080 width=4) (actual time=0.062..2401.128 rows=10000000 loops=5)
-> Hash (cost=183334.80..183334.80 rows=10000080 width=4) (actual time=6376.765..6376.765 rows=10000000 loops=5)
Buckets: 16777216 Batches: 1 Memory Usage: 482635kB
-> Seq Scan on simple t (cost=0.00..183334.80 rows=10000080 width=4) (actual time=0.051..2484.348 rows=10000000 loops=5)
-> Hash (cost=183334.80..183334.80 rows=10000080 width=4) (actual time=6478.513..6478.513 rows=10000000 loops=5)
Buckets: 16777216 Batches: 1 Memory Usage: 482635kB
-> Seq Scan on simple u (cost=0.00..183334.80 rows=10000080 width=4) (actual time=0.116..2546.278 rows=10000000 loops=5)
Total memory usage = ~500MB * 3 * 5 = ~7.5GB
Finalize Aggregate (cost=607466.61..607466.62 rows=1 width=8) (actual time=11247.154..11247.154 rows=1 loops=1)
-> Gather (cost=607466.19..607466.60 rows=4 width=8) (actual time=10998.218..11247.133 rows=5 loops=1)
Workers Planned: 4
Workers Launched: 4
-> Partial Aggregate (cost=606466.19..606466.20 rows=1 width=8) (actual time=10989.275..10989.276 rows=1 loops=5)
-> Parallel Hash Join (cost=426256.41..600216.14 rows=2500020 width=0) (actual time=4842.483..10790.433 rows=2000000 loops=5)
Hash Cond: (r.id = u.id)
-> Parallel Hash Join (cost=284170.94..436255.49 rows=2500020 width=12) (actual time=3202.773..7364.864 rows=2000000 loops=5)
Hash Cond: (r.id = t.id)
-> Parallel Hash Join (cost=142085.47..272294.84 rows=2500020 width=8) (actual time=1624.433..3941.708 rows=2000000 loops=5)
Hash Cond: (r.id = s.id)
-> Parallel Seq Scan on simple r (cost=0.00..108334.20 rows=2500020 width=4) (actual time=0.094..406.213 rows=2000000 loops=5)
-> Parallel Shared Hash (cost=108334.20..108334.20 rows=2500020 width=4) (actual time=1594.475..1594.475 rows=2000000 loops=5)
Buckets: 16777216 Batches: 1 Memory Usage: 522336kB
-> Parallel Seq Scan on simple s (cost=0.00..108334.20 rows=2500020 width=4) (actual time=0.048..460.714 rows=2000000 loops=5)
-> Parallel Shared Hash (cost=108334.20..108334.20 rows=2500020 width=4) (actual time=1553.354..1553.354 rows=2000000 loops=5)
Buckets: 16777216 Batches: 1 Memory Usage: 522368kB
-> Parallel Seq Scan on simple t (cost=0.00..108334.20 rows=2500020 width=4) (actual time=0.051..462.983 rows=2000000 loops=5)
-> Parallel Shared Hash (cost=108334.20..108334.20 rows=2500020 width=4) (actual time=1607.306..1607.306 rows=2000000 loops=5)
Buckets: 16777216 Batches: 1 Memory Usage: 522304kB
-> Parallel Seq Scan on simple u (cost=0.00..108334.20 rows=2500020 width=4) (actual time=0.115..468.530 rows=2000000 loops=5)
Total memory usage = ~500MB * 3 = ~1.5GB
Joins
Hash Tables
Simple Hash Joins
Multi-Batch Hash Joins
Parallel Hash Joins
Open Problems
Questions
Memory Escape Valve?
If extra rounds of adaptive partitioning fail to reduce
•
the hash table size, we stop trying to do that and
continue building the hash table (“ugly”), hoping the
machine can take it (!)
Switching to a sort/merge for a problematic partition
•
seems like a solution, but it cannot handle every case
(outer join with some non-mergejoinable join
conditions)
Invent an algorithm for processing the current batch in
•
multiple passes, but how to unify matched bits?
Bloom filters?
Could we profitably push Bloom filters from the
•
hash table down to the outer scan?
Could we use Bloom filters to filter the data
•
written to outer relation batch files?
N-join peak memory?
join join join
join U join join R join
join T R S T U S join
R S T U
Left-deep peak: Bushy peak: Right-deep peak:
N hash tables 3..N-1 hash tables 2 hash tables
PostgreSQL convention: probe = outer = left, build = inner = right.
Many RDBMSs prefer left-deep join trees, but several build with the left relation and probe with the
right relation. They minimise peak memory usage while we maximise.
Tune chunk size?
We want 32KB but the actual size that hits the system
•
malloc, after palloc overhead and chunk header, is 32KB
+ a smidgen, which eats up to 36KB of real space on
some OSes
We should probably make this much bigger to dilute that
•
effect, or tune the size to allow for headers
There may be other reasons to crank up the chunk size
•
Joins
Hash Tables
Simple Hash Joins
Multi-Batch Hash Joins
Parallel Hash Joins
Open Problems
Questions
References
[1] Design and Evaluation of Main Memory Hash
•
Join Algorithms for Multi-core CPUs, 2011
[2] Andy Pavlo’s CMU 15-721 2017 lecture
•
“Parallel Join Algorithms (Hashing)”, available on
Youtube + slides
[3] Main-Memory Hash Joins on Multi-Core
•
CPUs: Tuning to the Underlying Hardware, 2013