applicationbuffer.Infact,wefindthatthisoperationinPMFS
by the time it takes to fetch data from the storage medium
is sometimes slower than the original read() system call
into a user-level buffer. These overheads are high in both disk
employed in the baseline, leading to a 3% slowdown on
and NVM storage, and are likely to increase as datasets grow
average.
in size.
When using SE2 there is no data movement at the time of
However, when using SE1 or SE2, this movement of data
fetching data into an application-accessible memory region,
can be minimized or even avoided. For pmfs se1 we observe
duetothepossibilitytodirectlyreferencedatastoredinPMFS.
that the amount of time spent in kernel space decreases sub-
However, this has a negative side effect when accessing the
stantially and it is very similar to that observed for pmfs se2.
data for processing later on, as it has not been cached by
Thisisbecausethetwosystemsaredoingasimilaramountof
the processing units. Therefore, the benefits of avoiding data
work on the kernel side, with the difference that SE1 is doing
an implicit memcpy() operation into a user-level buffer, but movement to make it accessible are offset by the penalty to
fetchthisdataclosetotheprocessingunitsforprocessingata
this is now done in user-level code. Overall, we see that the
later stage. In order to mitigate this penalty, SE2 incorporates
modified SEs are able to lower kernel space execution time
a simple software prefetching scheme that tries to fetch in
significantly in most queries: Q02 to Q12, Q15, and Q19. A
advance the next element to be processed within a data block.
few queries show lower reductions because they operate over
When compared to pmfs base95, SE2 is able to achieve sig-
a small amount of data, e.g, Q1, Q13, Q16, and Q20. An
nificant performance improvements in read dominant queries
important thing to note is that, for SE1 and SE2, the kernel
such as Q11 (14.4%), Q15 (11.9%), and Q19 (8.6%). On
space time is likely to remain constant as datasets grow, as no
average,SE2is4.5%fasterthanpmfs base95and20.5%faster
work is done to fetch data.
than disk base95.
B. Query Performance Improvement
Figure 6 shows a classification of each cycle of execution
Figure 5 shows wall-clock execution time for each query as‘compute’,ifatleastoneinstructionwascommittedduring
andevaluatedsystem.Thedataisnormalizedtopmfs base95. thatcycle,oras‘stalled’otherwise.Thesecategoriesarefurther
100
80 )%(
nwodkaerb
60
40
elcyC
20 compute_user stalled_user
compute_kernel stalled_kernel
0
B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2
q01 q02 q03 q05 q06 q07 q08 q09 q11 q12 q13 q15 q16 q17 q19 q20
Fig.6. Execution-timebreakdownforcomputeandstalledcycles—B=pmfs base95,SE2=pmfs se2.
100 )%(nwodkaerb
80
60
40 sessim
20 user_level
CLL
kernel_level
0
B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2 B SE2
q01 q02 q03 q05 q06 q07 q08 q09 q11 q12 q13 q15 q16 q17 q19 q20
Fig.7. Last-levelcache(LLC)missesbreakdown—B=pmfs base95,SE2=pmfs se2.
broken down into user and kernel level cycles. Data is shown access latencies even with the use of prefetching, as happens
for pmfs base95 and SE2, normalized to the former. As can withPostgreSQL.Moreover,suchasolutionisapplicationand
beseen,thestalled kernelcomponentcorrelateswellwiththe architecture dependent.
kernel execution time shown in Figure 4, and this is the com- For these reasons, we advocate for the need to have ad-
ponentthatisreducedinSE2executions.However,weobserve ditional software libraries and tools that aid programmability
that for most queries some of the savings shift to stalled user in such systems. These libraries could implement solutions
since data needs to be brought close to the processing unit like helper threads for prefetching particular data regions,
when it is needed for processing. There are some exceptions, effectively bringing data closer to the core (e.g., LLC) with
i.e., Q11, Q15, and Q19, for which the simple prefetching small application interference. This approach would provide
scheme is able to mitigate this fact effectively. genericsolutionsforwritingsoftwarethattakesfulladvantage
Figure 7 shows a breakdown of user and kernel last-level of the capabilities that NVM can offer.
cache (LLC) misses. Here, we can clearly see how the num-
VII. RELATEDWORK
ber of LLC misses remains quite constant when comparing
pmfs base95 and SE2, but the misses shift from kernel level Previous work on leveraging NVM for DBMS design can
to user level. Moreover, in our experiments we have observed be divided into two categories: (i) employing NVM for whole
that user level misses have a more negative impact in terms database storage and (ii) for the logging components.
ofperformancebecausetheyhappenwhenthedataisactually The work reported in [27, 32] reduces the impact of
needed for processing, and a full LLC miss penalty is paid disk I/O on transaction throughput and response times by
foreachdataelement.Ontheotherhand,whenmovinglarger directly writing log records into an NVM component instead
data blocks to an application buffer, optimized functions are of flushing them to disk. Authors of [33] employ NVM for
employed and the LLC miss penalties can be overlapped. distributed logging on multi-core and multi-socket hardware
to reduce contention of centralized logging with increasing
C. Discussion
system load. Pelley et al. [34] explore a two level hierarchy
We have shown that there is a mismatch between the withDRAMandNVM,andstudydifferentrecoverymethods.
potential performance benefits shown in Figure 4 and the Finally, Arulraj et al. [16] use a single tier memory hierarchy,
actual benefits obtained shown in Figure 5. Direct access to i.e., without DRAM, and compare three different storage
memoryregionsholdingpersistentdatacanprovidesignificant management architectures using an NVM-only system.
benefits,butthisdataneedstobeclosetotheprocessingunits
VIII. CONCLUSION
when it is needed. To this end we have employed simple
software prefetching schemes that have provided moderate Inthispaper,westudytheimplicationsofemployingNVM
average performance gains. However, carefully crafted ad- in the design of DBMSs. We discuss the possible options
hoc software prefetching is challenging, and applications may to incorporate NVM into the memory hierarchy of a DBMS
not be designed in a way that makes it easy to hide long computing system and conclude that, given the characteristics
of NVM, a platform with a layer of DRAM where the disk is [9] H. Plattner, “SanssouciDB: An In-Memory Database for Processing
completely or partially replaced using NVM is a compelling EnterpriseWorkloads,”inBTW,2011.
[10] V. Sikka, F. Fa¨rber, W. Lehner, S. K. Cha, T. Peh, and C. Bornho¨vd,
scenario. Such an approach retains the programmability of
“Efficient transaction processing in SAP HANA database: the end of
current systems and allows direct access to the dataset stored a column store myth,” in Proceedings of the 2012 ACM SIGMOD
in NVM. With this system configuration in mind we modified InternationalConferenceonManagementofData,2012.
[11] J. Pisharath, A. Choudhary, and M. Kandemir, “Reducing energy con-
thePostgreSQLstorageengineintwoincrementalsteps-SE1
sumptionofqueriesinmemory-residentdatabasesystems,”inProceed-
andSE2-tobetterexploitthefeaturesofferedbyPMFSusing ings of the 2004 international conference on Compilers, architecture,
memory mapped I/O. andsynthesisforembeddedsystems,2004.
[12] J. A. Mandelman, R. H. Dennard, G. B. Bronner, J. K. DeBrosse,
Our evaluation shows that storing the database in NVM
R.Divakaruni,Y.Li,andC.J.Radens,“Challengesandfuturedirections
instead of disk for an unmodified version of PostgreSQL forthescalingofdynamicrandom-accessmemory(dram),”IBMJournal
improvesqueryexecutiontimebyupto40%,withanaverage ofResearchandDevelopment,2002.
[13] A.Driskill-Smith,“Latestadvancesandfutureprospectsofstt-ram,”in
of 16%. Modifications to take advantage of NVM hardware
Non-VolatileMemoriesWorkshop,2010.
improvetheexecutiontimeby20.5%onaverageascompared [14] S.Raoux,G.W.Burr,M.J.Breitwisch,C.T.Rettner,Y.-C.Chen,R.M.
to disk storage. However, current design of database software Shelby, M. Salinga, D. Krebs, S.-H. Chen, H.-L. Lung et al., “Phase-
change random access memory: A scalable technology,” IBM Journal
proves to be a hurdle in maximizing the improvement. When
ofResearchandDevelopment,2008.
comparing our baseline and SE2 using PMFS, we achieve [15] D. B. Strukov, G. S. Snider, D. R. Stewart, and R. S. Williams, “The
significantspeedupsofupto14.4%inreaddominatedqueries, missingmemristorfound,”Nature,2008.
[16] J. Arulraj, A. Pavlo, and S. R. Dulloor, “Let’s Talk About Storage
but moderate average improvements of 4.5%.
&RecoveryMethodsforNon-VolatileMemoryDatabaseSystems,”in
We find that the limiting factor in achieving higher perfor- Proceedings of the 2015 ACM SIGMOD International Conference on
mance improvements is the fact that the data is not close to ManagementofData,2015.
[17] M. K. Qureshi, V. Srinivasan, and J. A. Rivers, “Scalable high perfor-
the processing units when it is needed for processing. This is
mancemainmemorysystemusingphase-changememorytechnology,”
a negative side effect of directly accessing data from NVM, ACMSIGARCHComputerArchitectureNews,2009.
rather than copying it into application buffers to make it [18] “TheTPC-HBenchmark.”[Online].Available:http://www.tpc.org/tpch/
[19] K. Wang, J. Alzate, and P. K. Amiri, “Low-power non-volatile spin-
accessible. This leads to long latency user level cache misses
tronicmemory:STT-RAMandbeyond,”JournalofPhysicsD:Applied
eating up the improvement achieved by avoiding expensive Physics,2013.
data movement operations. Therefore, software libraries that [20] T.PerezandC.A.DeRose,“Non-volatilememory:Emergingtechnolo-
gies and their impacts on memory systems,” Technical Report, Porto
helpmitigatethisnegativesideeffectarenecessarytoprovide
Alegre,2010.
genericsolutionstoefficientlydevelopNVM-awaresoftware. [21] Y.Huang,T.Liu,andC.J.Xue,“Registerallocationforwriteactivity
minimization on non-volatile main memory for embedded systems,”
ACKNOWLEDGMENTS JournalofSystemsArchitecture,2012.
[22] P. Zhou, B. Zhao, J. Yang, and Y. Zhang, “A durable and energy
The research leading to these results has received funding efficientmainmemoryusingphasechangememorytechnology,”inACM
from the European Union’s 7th Framework Programme under SIGARCHcomputerarchitecturenews,2009.
[23] “3D XPoint Technology,” 2016. [Online]. Avail-
grant agreement number 318633, the Ministry of Science and
able: https://www.micron.com/about/emerging-technologies/3d-xpoint-
Technology of Spain under contract TIN2015-65316-P, and a technology
HiPEAC collaboration grant awarded to Naveed Ul Mustafa. [24] Intel,“ArchitectureInstructionSetExtensionsProgrammingReference,”
IntelCorporation,Feb,2016.
[25] S. R. Dulloor, S. Kumar, A. Keshavamurthy, P. Lantz, D. Reddy,
REFERENCES
R.Sankaran,andJ.Jackson,“Systemsoftwareforpersistentmemory,”
[1] L. Abraham, J. Allen, O. Barykin, V. Borkar, B. Chopra, C. Gerea, inProceedingsoftheNinthEuropeanConferenceonComputerSystems,
D. Merl, J. Metzler, D. Reiss, S. Subramanian et al., “Scuba: diving 2014.
intodataatfacebook,”ProceedingsoftheVLDBEndowment,2013. [26] “Linux-pmfs.”[Online].Available:https://github.com/linux-pmfs/pmfs
[2] F.Fa¨rber,S.K.Cha,J.Primsch,C.Bornho¨vd,S.Sigg,andW.Lehner, [27] J. Huang, K. Schwan, and M. K. Qureshi, “Nvram-aware logging in
“SAPHANAdatabase:datamanagementformodernbusinessapplica- transactionsystems,”ProceedingsoftheVLDBEndowment,2014.
tions,”ACMSigmodRecord,2012. [28] J.Schindler,J.L.Griffin,C.R.Lumb,andG.R.Ganger,“Track-Aligned
[3] J. Lindstro¨m, V. Raatikka, J. Ruuth, P. Soini, and K. Vakkila, “IBM Extents: Matching Access Patterns to Disk Drive Characteristics,” in
solidDB:In-MemoryDatabaseOptimizedforExtremeSpeedandAvail- Proceedingsofthe1stUSENIXConferenceonFileandStorageTech-
ability,”IEEEDataEngineeringBulletin,2013. nologies(FAST),2002.
[4] J. Baulier, P. Bohannon, S. Gogate, S. Joshi, C. Gupta, A. Khivesera, [29] H.Garcia-MolinaandK.Salem,“Mainmemorydatabasesystems:An
H.F.Korth,P.McIlroy,J.Miller,P.Narayanetal.,“DataBlitz:AHigh overview,” IEEE Transactions on Knowledge and Data Engineering,
Performance Main-Memory Storage Manager,” in Proceedings of the 1992.
24thVLDBConference,1998. [30] J.Gray,“Thetransactionconcept:Virtuesandlimitations,”inProceed-
[5] T. J. Lehman and M. J. Carey, “A study of index structures for main ingsofthe7thInternationalConferenceonVLDB,1981.
memory database management systems,” in Proceedings of the 12th [31] B.Momjian,PostgreSQL:introductionandconcepts. Addison-Wesley
VLDBConference,1986. NewYork,2001,vol.192.
[6] ——,“ARecoveryAlgorithmforaHigh-performanceMemory-resident [32] R.Fang,H.-I.Hsiao,B.He,C.Mohan,andY.Wang,“Highperformance
DatabaseSystem,”inProceedingsofthe1987ACMSIGMODInterna- database logging using storage class memory,” in IEEE 27th Interna-
tionalConferenceonManagementofData,1987. tionalConferenceonDataEngineering(ICDE),2011.
[7] I. Lee and H. Y. Yeom, “A single phase distributed commit protocol [33] T. Wang and R. Johnson, “Scalable logging through emerging non-
formainmemorydatabasesystems,”inProceedingsoftheParalleland volatilememory,”ProceedingsoftheVLDBEndowment,2014.
DistributedProcessingSymposium,2001. [34] S. Pelley, T. F. Wenisch, B. T. Gold, and B. Bridge, “Storage man-
[8] S.Melnik,A.Gubarev,J.J.Long,G.Romer,S.Shivakumar,M.Tolton, agement in the NVRAM era,” Proceedings of the VLDB Endowment,
andT.Vassilakis,“Dremel:interactiveanalysisofweb-scaledatasets,” 2013.
ProceedingsoftheVLDBEndowment,2010.