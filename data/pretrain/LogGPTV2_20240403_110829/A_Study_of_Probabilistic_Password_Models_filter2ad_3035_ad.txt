2.39%
All Chinese
227835559
67.99%
30.55%
0.25%
1.21%
CSDN
60788099
67.41%
30.06%
0.62%
1.91%
Duduniu
93174301
64.74%
33.79%
0.12%
1.35%
178
73873159
72.55%
26.87%
0.13%
0.46%
(e) Password pattern information: L denotes a lower-case sequence, D for digit sequence, U for upper-case sequence, and S for symbol sequence; patterns
differ from templates in that patterns do not record length of sequence
ALL
Percentage of the patterns in American datasets
Percentage of the patterns in Chinese datasets
Percentage of
the most
the pattern
33.22%
27.15%
24.50%
4.81%
1.60%
1.48%
0.94%
0.64%
0.43%
0.42%
0.40%
0.40%
0.40%
0.38%
0.23%
popular string
a123456
password
123456
123456aa
love4ever
A123456
PASSWORD
Password1
123aa123
hi5hi5
Password
xxx 01
rock you
iloveyou!
123456A
LD
L
D
DL
LDL
UD
U
ULD
DLD
LDLD
UL
LSD
LSL
LS
DU
All American
rockyou
phpbb
yahoo
All Chinese
csdn
duduniu
178
27.79%
41.65%
15.77%
2.57%
1.66%
1.33%
1.48%
0.96%
0.43%
0.43%
0.65%
0.66%
0.50%
0.64%
0.15%
27.71% 19.26%
41.70% 50.08%
15.94% 11.94%
2.05%
3.66%
0.37%
0.73%
1.04%
0.79%
1.03%
1.22%
0.33%
0.17%
0.16%
0.11%
2.54%
1.62%
1.35%
1.50%
0.94%
0.42%
0.42%
0.65%
0.66%
0.50%
0.65%
0.15%
38.31%
33.05%
5.86%
5.32%
3.31%
0.56%
0.40%
2.48%
0.94%
0.97%
0.70%
0.17%
0.39%
0.20%
0.06%
40.05% 26.15%
8.93% 11.65%
35.48% 45.02%
5.89%
1.64%
1.62%
0.47%
0.50%
0.52%
0.47%
0.09%
0.21%
0.66%
0.14%
0.46%
7.63%
1.53%
1.67%
0.26%
0.24%
0.44%
0.40%
0.09%
0.27%
0.08%
0.07%
0.34%
55.57%
7.29%
19.48%
9.79%
1.68%
2.57%
0.21%
0.26%
0.44%
0.31%
0.15%
0.05%
0.21%
0.05%
0.46%
31.12%
9.00%
48.07%
6.25%
1.27%
0.62%
0.15%
0.05%
0.38%
0.47%
0.01%
0.01%
0.08%
0.04%
0.11%
697
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: Six experimental scenarios.
#
1
2
3
4
5
6
name
Rock→Ya+Ph
Du+178→CSDN
CS+178→Dudu
CS+Du→178
Chin→Ya+Ph
Rock→CSDN
Training
Rockyou
Duduniu+178
CSDN+178
CSDN+Duduniu
Three Chinese
Rockyou
Testing
Yahoo+PhpBB
CSDN
Duduniu
178
Yahoo+PhpBB
CSDN
ing all datasets into one big dataset and then partitioning it into
training and testing, since we feel that represents unrealistic
scenarios in practice. For example, this causes similar length,
character, and pattern distributions in training and testing;
furthermore, any frequent password tends to appear both in
training and testing. Instead, in each scenario, we chose some
of the datasets for training, and another for testing. Table III
lists the 6 scenarios we use in this paper. Scenarios 1-4 have
training and testing from within the same group (American or
Chinese). We merge Yahoo and PhpBB together because they
are both small (containing less than one million passwords
when combined) when compared with other datasets (all
contain more than 6 million). Scenarios 2-4 resemble cross-
validation, rotating among the 3 Chinese datasets, each time
training with 2 and testing with the remaining 1. Scenario 5
trains on all Chinese datasets and test on the American datasets
Yahoo+PhpBB. Scenario 6 trains on Rockyou and tests on
the Chinese dataset CSDN. By comparing scenario 5 against
1, and scenario 6 against 2, one can observe the effect of
training on a mismatched dataset. We present detailed results
using graphs for scenario 1 and 2, and only ANLL0.8 for other
scenarios, because of space limitation.
V. EXPERIMENTAL RESULTS
Experimental results are presented using guess-number
graphs, probability-threshold graphs, and ANLL0.8 values. The
algorithm naming convention is as follows. Names for whole-
string models start with “ws”. For example, ws-mc-b10 is
Markov chain with backoff and frequency threshold 10, and
ws-mci is order-i Markov chain with add-δ smoothing for
δ = 0.01. The postﬁx -g is for grouping, -ag for grouping after
our adaption, -gts for Good-Turing smoothing, and -end, -dir,
and -dis are for the three normalization approaches. Names for
template-based models start with “tb”; for example, tb-co-mci
is the template-based model using the counting-based method
for assigning probabilities to templates and an order-i Markov
chain model for segment instantiation. We note that using this
notation, tb-co-co is PCFGW with dictionary generated from
the training dataset.
The Figures. Fig 2 gives 8 graphs for Scenario 1. Fig 2(a)
shows the rank vs. probability based on generated passwords.
One can see that for the three Markov models shown there,
one can translate log of rank into negative log of probability
via an additive factor in the range of 3 to 8. That is, a password
1
that has probability
2y is likely to have a rank of around
2y−a
, which a is mostly between 3 and 8, and seems to gets
close to around 6 as x increases. We conjecture that this trend
will further hold as x increases, and the gap between different
curves would shrink as x increases. One support is that these
curves have to cross at some point due to the fact that the
probabilities for each model add up to 1. Analyzing such
Markov models to either prove or disapprove this conjecture is
interesting future research, since it affects to what extend one
can use probability threshold graphs instead of guess number
graphs to compare these models with each other.
Fig 2(b) shows the guess-number graph. We include results
from ws-mc5-end, three instantiations of PCFGW (using dic-
0294, the dictionary used in [25], using the Openwall dictio-
nary [4], and using a dictionary generated from the training
dataset), and JTR in three different modes (incremental brute-
force, markov chain, and wordlist/dictionary mode with open-
wall dictionary). We can see that the three PCFGW and the
three JTR methods clearly underperform the Markov model.
We note that jtr-mc1 seems to pick up rather quickly, which
matches the shape of ws-mc1 in 2(f). Another observation
is PCFGW with training dataset as dictionary (i.e., tb-co-co)
outperforms the other two instantiations.
Fig 2(c) plots both guess-number curves and probability
threshold curves for 3 password models on the same graph,
and one can see that the guess-number curve for any model
approximately matches the the probability threshold curve if
one shifts them to the right.
,
Fig 2(d) shows the probability threshold curves for all
template-based models and one whole-string model, namely
ws-mc-b10-end. Here, Laplace smoothing with δ = 0.01 is
applied to markov chains in all templeate models. PCFGW
with dic-0294 performs the worst, and PCFGW with Open-
wall dictionary performs slightly better. Compared with other
models, these two cover the least passwords at any probability
threshold. With probability threshold at 2−80
the former
covers slightly over 50% of all passwords, and the latter
covers close to 60%. The two curves that both use counting to
instantiate segments (tb-co-co and tb-mc5-co) almost overlap;
they perform better than PCFGW with external dictionaries.
On lower probability thresholds, they, together with ws-mc-
b10, are the best-performing methods At threshold 2−80
, they
cover around 75% of passwords. This suggests that learning
from the dataset is better than using existing dictionaries in
PCFGW . When we replace counting with Markov models for
instantiating segments, we see another signiﬁcant improvement
at higher probability threshold. The model tb-co-mc5 covers
more than 90% of passwords (at threshold 2−80
), and tb-mc5-
mc5, which takes advantage of smoothing, covers close to
100% passwords. This improvement, however, comes at the
cost of slightly worse performance than tb-co-co and tb-mc5-
co at lower probability thresholds. In other words, whether
to use counting or Markov chains to generate probabilities
for templates shows a small difference. Using counting to
instantiate segments shows an overﬁtting effect, performing
well at low thresholds, but worse at higher ones. Whole-string
Markov with backup (ws-mc-b10-end) almost always has the
best performance at any threshold.
Fig 2(e) compares the effect of no smoothing, add-δ smooth-
698
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:59:45 UTC from IEEE Xplore.  Restrictions apply. 
ing, and Good-Turing smoothing on Markov models of order
4 and order 5. When x < 35, smoothing makes almost no
difference, one simply sees that order 5 outperforms order
4. This is to be expected, since the smoothing counts make
a difference only for strings of low probabilities. For larger
x values, however, smoothing makes a signiﬁcant difference,
and the difference is much more pronounced for order 5 than
for order 4. The order 5 model without smoothing performs
the worst, due to overﬁtting. Good-Turing smoothing under-
performs add-δ smoothing, and results in signiﬁcant overﬁtting
for order 5.
Fig 2(f) compares the effect of different orders in Markov
chain models. We see that higher-order chains perform better
for smaller x values, but are surpassed by lower-order chains
later; however, backoff seems to perform well across all ranges
of x.
Fig 2(g) demonstrates the effect of normalization. Direct
normalization performs the worst, while distribution based
normalization performs slightly better than end-symbol nor-
malization.
As can be seen from Table IIc, Yahoo+PhpBB have between
40% and 45% passwords that are of length less than 8. Since
many modern websites require passwords to be at least 8
characters long, one may question to what extent results from
the above ﬁgures are applicable. To answer this question,
we repeat ﬁgure Fig 2(d) by using only passwords that are
at least 8 characters for evaluation. The result is shown in
Fig 2(h). Note that while all curves are somewhat lower than
the corresponding ones in Fig 2(d); they tell essentially the
same story.
Fig 3 gives the same 8 graphs for scenario 2, which use
Chinese datasets for training and testing. The observations
made above similarly apply. One minor difference is that in
Fig 2(d), the performance of PCFG with external dictionaries
are worse than in Scenario 1. Since the Chinese datasets con-
sist of more passwords that use only digit sequences, and thus
are intuitively weaker, this may seem a bit counter-intuitive.
This is because PCFGW uses only digit sequences that appear
in the training dataset to instantiate password guesses, and thus
does perform well when lots of digits are used. When Markov
chains with smoothing are used to instantiate the segments, one
obtains a more signiﬁcant improvement than in Scenario 1.
Fig 4 shows two of these graphs for each of the other 4 sce-
narios. These two are the graph (d) for a probability threshold
graph and graph (b) for a guess number graph. They mostly
give the same observations. We note that in Fig 4(f), we see
that PCFGW with dictionary from the training dataset starts
out-performing ws-mc5-end. Note that in scenario, we train on
the Chinese dataset and the use American datasets to evaluate,
thus, a higher-order Markov chain does not perform very well.
From Fig 4(e), however, we can see that the variable-order ws-
mc-b10 remains the best-performing method.
ANLL Table. ANLL0.8 values for all six scenarios are
given in Table IV. Using this format, we can compare more
models directly against each other, with the limitation that
these results need to be interpreted carefully. Some models
assign probability 0 to some passwords; their ANLLs are not
well-deﬁned and thus not included. Results for those models
are presented using graphs. Because of space limitation, we
exclude some ANLL data for some other combinations (such
as grouping with Good Turing smoothing or template-based
with two different orders).
Many observations can be made from Table IV. First,
backoff with end-symbol normalization is gives the best result
overall, as it produces results that are among the best across
all scenarios. Especially, for Scenario 1, which we consider to
be the most important one, it produces the best overall result.
Several other models perform quite close. It seems that using a
Markoc-chain of an order that is high enough, but not too high,
and with some ways to deal with overﬁtting, would perform
reasonably well.
Second, for most other models, distribution-based normal-
ization performs the best, followed by end-symbol normaliza-
tion. Direct normalization, which was implicitly used in the
literature, performs the worst. Yet, for backoff, end-symbol
normalization performs the best. There seems to exist some
synergy between backoff and end-symbol normalization. One
possible reason is that as backoff uses variable-length Markov
chains, it can recognize long common postﬁxes of passwords
so that it can end a password appropriately, instead of depend-
ing only on the length of passwords for normalization. With
ﬁxed-length Markov chains, one does not have this beneﬁt.
Third, on the effect of smoothing, Good-Turing smooth-
ing performs unexpectedly poorly, especially for higher-order
Markov chains; it seems that they tend to cause more over-
ﬁtting, a phenomenon also shown in Figure 2(e) and 3(e).
For higher orders Markov models, add-δ smoothing, grouping,
adapted grouping, and template-based models all perform
similarly; they are just slightly worse than backoff with end-