6.2 Relation with δ-privacy
In this section we establish a relation between the notion of pri-
vacy introduced by Küsters, Truderung, and Vogt [23] at IEEE S&P
2011 and the privacy measure that we introduce in this paper.
votes i and j there exists a negligible function ν such that
Advdist
A,i,j(k) = P [Aπ(Di) = 1] − P [Aπ(Dj) = 1] ≤ δ + ν(k).
A protocol is exactly δ-private if it achieves δ-privacy and does
not achieve δ(cid:48)-privacy for any δ(cid:48) < δ. We write δ(π,D) for the
exact level of privacy achieved by protocol π when the honest votes
are selected according to D. By a slight abuse of notation, we write
δ(π,Dij) for the maximum level of privacy achieved if i and j are
ﬁxed and only the adversary is allowed to vary.
Relation between entropy-based privacy and δ-privacy
First, we link δ-privacy with the ability of any adversary to cor-
rectly guess the vote of the ﬁrst voter, if this vote is either i or j.
Speciﬁcally, let the distribution Dij be such that the vote of the
ﬁrst voter is selected uniformly at random from the set {i, j}, and
the votes of the remaining honest voters are selected according to
D. Consider a modiﬁed version of the experiment that deﬁnes δ-
privacy. In this modiﬁed version the votes of the honest voters are
distributed according to Dij and the goal of the adversary is to out-
put a guess g ∈ {i, j} as to what the vote of the ﬁrst voter is. We
refer to such an adversary as a guessing adversary. The adversary
wins if it guesses correctly, i.e. we deﬁne its guessing advantage
A,i,j = P [Aπ(Dij) = v1], where v1 ∈ {i, j} is the vote
as Advguess
cast by the ﬁrst voter in the execution. The following lemma es-
tablishes a well-known relation between winning a “distinguishing
game” and guessing the value to be distinguished.
Deﬁnition of δ-privacy
In the model for δ-privacy, the adversary’s target is the vote of a sin-
gle “voter under observation”. This vote is restricted to being one
of two votes. All other voters cast votes according to some ﬁxed
distribution.6 The adversary interacts with the protocol by control-
ling a set of dishonest parties (voters and/or authorities). At the end
of the execution, the adversary outputs a bit to indicate which of the
two possible target votes he thinks the voter under observation has
cast. The system offers δ-privacy if the adversary can distinguish
between the two situations with probability no better than δ (so the
smaller δ the better).
The adversary is quantiﬁed over an abstract set of “observer pro-
cesses” which is either the set of bounded computations or that of
unbounded computations. This dichotomy gives rise to two distinct
privacy notions, one computational and the other one information
theoretic. Below, we call the set over which the adversaries is quan-
tiﬁed admissible adversaries.
We restate the deﬁnition of privacy deﬁnition of [23] but use
slightly different notation. Without loss of generality we assume
that the voter under observation is the ﬁrst voter. We write Dj for
the distribution where the vote of the ﬁrst voter is set to j and the
votes of all other parties are selected according to D. We write
Aπ(D) for the random variable (ensemble) that describes the out-
put of an adversary A interacting with protocol π when the honest
votes are selected according to D. δ-privacy of π requires that no
adversary can tell if the ﬁrst voter votes for i or votes for j except
with probability δ (no matter what i and j are). We refer to an
adversary for this experiment as a distinguishing adversary.
DEFINITION 7
([23]). A protocol π achieves δ-privacy if for
any admissible distinguishing adversary A and any two distinct
6In fact, the assumption in [23] is that everyone else votes inde-
pendently and identically distributed according to some probability
vector (cid:126)p on the possible votes but this can be easily generalised.
In particular, one may consider a joint distribution D(cid:48) on all other
honest voters.
LEMMA 8. Let i, j be ﬁxed. For any admissible (distinguish-
ing) adversary D there exists an admissible (guessing) adversary
GD such that:
Advguess
GD ,i,j(k) ≥ 1
2
Advdist
D,i,j(k) + 1
.
·“
”
Conversely, for any admissible (guessing) adversary G there ex-
ists an admissible (distinguishing) adversary DG such that:
Advdist
DG,i,j(k) ≥ 2 · Advguess
G,i,j(k) − 1.
One implication of the above lemma is that for protocols which
achieve δ-privacy only for large δ there are adversaries that are suc-
cessful in guessing the vote of the ﬁrst party. This relation between
δ-privacy and guessing abilities suggests a link between δ-privacy
and a privacy measure that captures the ability of the adversary to
guess a target vote.
We make this intuition formal by instantiating our privacy mea-
sure in a particular way. We set the distribution on the votes to be
Dij (the vote of the ﬁrst voter is i or j with probability a half),
we set the target function to T1, the function that returns the vote of
the ﬁrst voter, and set the underlying entropy measure to be ˜H∞ the
average conditional min-entropy (that measures precisely the abil-
ity of the adversary to guess the target). We thus relate δ-privacy
with ˜M∞(Dij, T1, π), where i, j are the votes of the ﬁrst voter that
yield that largest possible δ. The relation between guessing prob-
ability and average min-entropy is the following. Consider T and
L two (possibly correlated) random variables. Then for any ad-
versary A, the probability that A guesses T given that it sees L is
P [A(L(T)) = T], which is the same as:
X
P [L = l] · P [A(L(T)) = T | L = l] =
“
−H∞(A(L(T))=T|L=l)”
−˜H∞(T|L)
= 2
l
E
l
2
We use this connection to relate δ-privacy with ˜M∞(D, T, π)
for the case of bounded adversaries, and with ˜M I∞(D, T, π) for un-
bounded ones. For unbounded adversaries the connection is made
950precise by the following theorem which says that δ-privacy in the
sense of [23] is equivalent to privacy as captured by ˜M I∞(Dij, T1, π).
(The proof is in the full version of our paper.)
THEOREM 9. Let i, j arbitrary votes. For unbounded adver-
saries δ(π,Dij) = 21− ˜M I∞(Dij ,T1,π) − 1
Perhaps unsurprisingly, for bounded adversaries we were able to
prove a connection only in one direction. Speciﬁcally, we argue
that if a protocol is not δ-private (i.e.
there exists i, j votes for
the ﬁrst voter, and a distinguishing adversary with advantage larger
than δ), then our computational privacy measure ˜M∞(Dij, T1, π)
is also upperbounded appropriately. We omit the proof due to space
limitations.
THEOREM 10. Let π be an arbitrary protocol, T1 the target
function that returns the vote of the ﬁrst voter and D a distribution
on the honest votes. Then for any i, j:
δ(π,Dij) ≤ 21− ˜M∞(Dij ,T1,π) − 1.
The following corollary (obtained by setting i and j appropri-
ately) makes the relation between δ-privacy and ˜M∞ precise.
COROLLARY 11. Let π be an arbitrary protocol that is not δ-
private. Then there exists i and j such that
˜M∞(Dij, T, π) ≤ 1 − log (1 + δ)
Discussion
The results of this section show that security in the sense captured
by one instantiation of our privacy notion implies security in the
sense deﬁned by δ-privacy. Furthermore, for information theoret-
ically secure protocols (e.g., ideal ones) and for speciﬁc distribu-
tions the two notions coincide. We think that the relations exhibited
in this section between our entropy-based notion, the cryptographic
notion of [5] and δ-privacy [23] support all three notions and their
respective approaches to privacy. Our privacy measure allows us
to make statements about privacy in cryptographic voting protocols
that would be much harder to establish using the game-based model
of [5] directly. Our measure also applies to a more general class of
protocols, vote distributions and targets than those that have been
studied previously using δ-privacy.
7. CONCLUSION
Entropy is a natural choice to measure privacy in an information-
theoretic setting and we demonstrate how different formulations of
conditional entropy answer different intuitive questions about vote
privacy. Through an appropriate notion of computational condi-
tional entropy we have extended the reach of this idea to the com-
putational setting and have established a theorem that enables ac-
curate analysis of privacy offered by complex cryptographic voting
protocols while simply disregarding the details of their implemen-
tation. Furthermore, the underlying entropy-based approach makes
our measure applicable to non-cryptographic protocols and we have
shown through the Takoma Park example how to obtain meaning-
ful results for a real election. We completed the investigation of
our notion by establishing powerful connections with two existing
privacy notions for votes [23, 5].
As our deﬁnition does not concentrate on any speciﬁc election
rule or on any speciﬁc target function, in future work we plan
to explore if and how it can be applied to related problems, e.g.,
sealed-bid auctions [25, 20] or more generally any secure function
evaluation problem.
Acknowledgments
The research leading to these results has received funding from the
European Research Council under the European Union’s Seventh
Framework Programme (FP7/2007-2013) / ERC grant agreement
no 258865, project ProSecure, under the ICT-2007-216676 Euro-
pean Network of Excellence in Cryptology II, under the HOME/
2010/ISEC/AG/INT-011 project B-CCENTRE, and under
the
SCOOP Action de Recherche Concertées. Olivier Pereira is a Re-
search Associate of the F.R.S.-FNRS.
8. REFERENCES
[1] B. Adida. Helios: Web-based Open-Audit Voting. In 17th
USENIX Security Symposium, pages 335–348, 2008. Helios
website: http://heliosvoting.org.
[2] B. Adida, O. de Marneffe, O. Pereira, and J.-J. Quisquater.
Electing a University President Using Open-Audit Voting:
Analysis of Real-World Use of Helios. In Electronic Voting
Technology Workshop/Workshop on Trustworthy Elections.
Usenix, Aug. 2009.
[3] J. Benaloh. Veriﬁable secret-ballot elections. Technical
Report 561, Yale University Department of Computer
Science, September 1987.
[4] J. Benaloh and D. Tuinstra. Receipt-free secret-ballot
elections. In 26th ACM Symposium on Theory of Computing,
pages 544–553, 1994.
[5] D. Bernhard, V. Cortier, O. Pereira, B. Smyth, and
B. Warinschi. Adapting helios for provable ballot secrecy. In
Springer, editor, 16th European Symposium on Research in
Computer Security (ESORICS’11), volume 6879 of LNCS,
2011.
[6] P. Bulens, D. Giry, and O. Pereira. Running mixnet-based
elections with Helios. In Electronic Voting Technology
Workshop/Workshop on Trustworthy Elections. Usenix, 2011.
[7] R. Carback, D. Chaum, J. Clark, J. Conway, A. Essex, P. S.
Herrnson, T. Mayberry, S. Popoveniuc, R. L. Rivest, E. Shen,
A. T. Sherman, and P. L. Vora. Scantegrity II Municipal
Election at Takoma Park: The First E2E Binding
Governmental Election with Ballot Privacy. In USENIX
Security Symposium, pages 291–306. USENIX Association,
2010.
[8] K. Chatzikokolakis, C. Palamidessi, and P. Panangaden.
Anonymity protocols as noisy channels. Information and
Computation, 2-4(206):378–401, 2008.
[9] D. Chaum. Untraceable electronic mail, return addresses,
and digital pseudonyms. Communications of the ACM,
24(2):84–88, February 1981.
[10] D. Chaum, A. Essex, R. Carback, J. Clark, S. Popoveniuc,
A. Sherman, and P. Vora. Scantegrity: End-to-End
Voter-Veriﬁable Optical-Scan Voting. IEEE Security and
Privacy, 6(3):40–46, 2008.
[11] M. R. Clarkson, S. Chong, and A. C. Myers. Civitas: Toward
a Secure Voting System. In 29th Security and Privacy
Symposium (S&P’08). IEEE, 2008.
[12] J. Cohen (Benaloh) and M. Fischer. A robust and veriﬁable
cryptographically secure election scheme. In 26th
Symposium on Foundations of Computer Science., pages
372–382, Portland, OR, 1985. IEEE.
[13] S. Delaune, S. Kremer, and M. D. Ryan. Verifying
privacy-type properties of electronic voting protocols.
Journal of Computer Security, 17(4):435–487, 2009.
951[14] Y. Dodis, R. Ostrovsky, L. Reyzin, and A. Smith. Fuzzy
Extractors: How to Generate Strong Keys from Biometrics
and Other Noisy Data. SIAM Journal of Computing,
38(1):97–139, 2008.
[15] C. Dwork. Differential privacy. In Automata, Languages and
Programming, 33rd International Colloquium, ICALP 2006,
volume 4052 of LNCS, pages 1–12. Springer, 2006.
[16] J. Furukawa, K. Mori, and K. Sako. An implementation of a
mix-net based network voting scheme and its use in a private
organization. In Towards Trustworthy Elections, volume
6000 of LNCS, pages 141–154. Springer, 2010.
[17] C. Gentry and D. Wichs. Separating succint non-interactive
arguments from all falsiﬁable assumptions. In 43rd ACM
Symposium on Theory of Computing, pages 99–108, 2011.
[18] O. Goldreich, S. Micali, and A. Wigderson. How to play any
mental game: A completeness theorem for protocols with
honest majority. In 19th Annual ACM Symposium on the
Theory of Computing (STOC), pages 218–229. ACM Press,
1987.
[19] A. Juels, D. Catalano, and M. Jakobsson. Coercion-Resistant
Electronic Elections. In 4th Workshop on Privacy in the
Electronic Society (WPES 2005), pages 61–70. ACM, 2005.
[20] A. Juels and M. Szydlo. A two-server, sealed-bid auction
protocol. In 6th international conference on Financial
cryptography (FC’02), pages 72–86. Springer, 2003.
[21] R. Küsters, T. Truderung, and A. Vogt. A Game-Based
Deﬁnition of Coercion-Resistance and its Applications. In
23rd IEEE Computer Security Foundations Symposium
(CSF’10), pages 122–136. IEEE, 2010.
[22] R. Küsters, T. Truderung, and A. Vogt. Proving
Coercion-Resistance of Scantegrity II. In 12th International
Conference on Information and Communications Security
(ICICS 2010), volume 6476 of LNCS, pages 281–295, 2010.
[23] R. Küsters, T. Truderung, and A. Vogt. Veriﬁability, Privacy,
and Coercion-Resistance: New Insights from a Case Study.
In IEEE Symposium on Security and Privacy (S&P 2011),
pages 538–553. IEEE Computer Society, 2011.
[24] T. Moran and M. Naor. Receipt-Free Universally-Veriﬁable
Voting with Everlasting Privacy. In 26th International
Cryptology Conference (CRYPTO’06), volume 4117 of
LNCS, pages 373–392. Springer, 2006.
[25] M. Naor, B. Pinkas, and R. Sumner. Privacy preserving
auctions and mechanism design. In 1st ACM conf. on
Electronic Commerce, 1999.
[26] A. Rényi. On measures of information and entropy. In 4th
Berkeley Symposium on Mathematics, Statistics and
Probability, pages 547–561, 1960.
[27] L. Reyzin. Some notions of entropy for cryptography -
(invited talk). In Information Theoretic Security – ICITS,
pages 138–142, 2011.
[28] R. L. Rivest and W. D. Smith. ThreeVotingProtocols:
ThreeBallot, VAV, and Twin. In Electronic Voting
Technology Workshop (EVT 2007), 2007.
[29] P. Ryan, D. Bismark, J. Heather, S. Schneider, and Z. Xia.
The prêt à voter veriﬁable election system. IEEE
Transactions on Information Forensics and Security,
4:662–673, 2009.
[30] K. Sako and J. Kilian. Receipt-free mix-type voting scheme -
a practical solution to the implementation of a voting booth.
In Advances in Cryptology - EUROCRYPT ’95, volume 921
of LNCS, pages 393–403. Springer, 1995.
[31] C. Shannon. A mathematical theory of communication. Bell
System Technical Journal, pages 379–423 and 623–656,
1948.
[32] G. Smith. Quantifying information ﬂow using min-entropy.
In 8th International Conference on Quantitative Evaluation
of SysTems (QEST’11), invited paper, pages 159–167, 2011.
[33] D. Unruh and J. Müller-Quade. Universally Composable
Incoercibility. In 30th International Cryptology Conference
(CRYPTO’10), volume 6223 of LNCS, pages 411–428.
Springer, 2010.
952