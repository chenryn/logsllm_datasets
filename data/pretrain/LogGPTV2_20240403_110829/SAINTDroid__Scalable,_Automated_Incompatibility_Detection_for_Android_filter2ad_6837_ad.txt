### 4.1 Dataset and Experimental Setup

Initially, we encountered an error while setting up the development environment on an Android system. Despite installing the development environment on two different laptops with varying operating systems, the error persisted. Since our approach also involves comparing results with Lint, which requires building apps, we excluded eight problematic apps from our analysis. The characteristics of the remaining 19 apps are detailed in Table 4.1. These apps vary in size, ranging from 10,400 to 294,400 lines of Dex code, and include tens of thousands of methods. They support and target a variety of API levels, with minimum levels ranging from 10 to 21 and target levels from 23 to 27.

To further evaluate the practical applicability of our tool, we collected a set of real-world Android apps from two repositories: F-Droid [9] and AndroZoo [32]. F-Droid is a repository for free and open-source Android apps, and we included all 1,391 available apps from this repository. Additionally, we included 2,300 apps from AndroZoo, a growing repository of Android apps collected from various sources, including the official Google Play store [32]. We were unable to build 120 of the AndroZoo apps, so they were excluded from our analysis, leaving us with a total of 3,571 apps.

### 4.2 Variables and Measures

#### 4.2.1 Independent Variables

Our analysis evaluates GAINDroid against other state-of-the-art approaches for detecting Android compatibility issues:

- **CiD**: A publicly available tool that detects Android compatibility issues. We used it as the baseline system to answer RQ1 and RQ3.
- **Cider**: Another state-of-the-art approach for analyzing API compatibility issues. Although not available in source or binary form, we relied on the results reported in [46] to answer RQ1 and RQ3.
- **Lint**: A static analysis technique included with the Android Development Tools (ADT) that examines code bases for potential bugs, including incompatible API usages. Lint performs compatibility analysis during the app build process and requires the app source code. We used Lint to answer RQ1 and RQ3.
- **IctApiFinder [44]**: This tool was introduced around the same time as Cider but is not publicly available. Our attempts to contact the authors for access were unsuccessful, so it was not included in our study.

#### 4.2.2 Dependent Variables

We chose metrics to address each of our three research questions:

- **Accuracy**: Measured by comparing the number of detected compatibility issues with known issues reported by prior work [46, 51]. For each technique, we report true positives, false positives, and false negatives, along with precision, recall, and F-measure.
- **Applicability**: Measured by the number of detected compatibility issues in real-world apps.
- **Performance**: Measured by the analysis time and memory usage of each technique (GAINDroid, CiD, and Lint).

#### 4.2.3 Study Operation

To address RQ1 and RQ2, we executed GAINDroid, CiD, Cider, and Lint once to identify sources of API incompatibility issues and verified the number of problematic API calls. For RQ3, we performed the experiment three times, measuring the time and memory required for the analysis of each app. The experiments were conducted on a MacBook Pro running macOS High Sierra version 10.13.3, with 8GB of memory and a 2.5GHz Intel Core i5 processor.

### 4.3 Threats to Validity

#### External Validity

The primary threat to external validity is the limited set of benchmark programs used, which were developed and released by prior research [51, 46]. However, we extended our evaluation to over 3,590 complex real-world apps from other repositories, providing a more representative assessment of real-world scenarios.

#### Internal Validity

Potential errors in the implementations of GAINDroid and the infrastructure used to run CiD and GAINDroid pose a threat to internal validity. To mitigate this, we extensively validated all tool components and scripts to ensure correctness. Using the same objects as our baseline systems allowed us to compare our results with previously reported data, enhancing reliability.

#### Construct Validity

The primary threat to construct validity is that we measure efficiency without assessing whether GAINDroid helps software engineers or analysts address dependability and security concerns more quickly than current approaches.

### Chapter 5: Results

The results of our analysis regarding mismatch detection are summarized in Table 5.1. For each of the 19 manually inspected apps, we report the number of true positives, false positives, and false negatives for each category of mismatch. The table also lists the precision, recall, and F-measure for each technique to summarize overall effectiveness. Precision measures the proportion of relevant data points, recall measures the ability to find all relevant instances, and F-measure is the harmonic mean of precision and recall, providing a balanced view of the performance.

| **App** | **Category** | **True Positives** | **False Positives** | **False Negatives** |
|---------|--------------|--------------------|---------------------|---------------------|
| AFWall+ | API          | 9                  | -                   | 9                   |
| DuckDuckGo | FOSS Browser | 3                  | 9                   | -                   |
| Kolab notes | MaterialFBook | 11                 | 3                   | 3                   |
| NetworkMonitor | Basic | 9                  | -                   | -                   |
| ...     | ...          | ...                | ...                 | ...                 |

**Precision, Recall, and F-Measure:**
- **GAINDroid**: 79%, 100%, 100%
- **CiD**: 89%, 19%, 31%
- **Cider**: 27%, 59%, 42%

These metrics provide a robust evaluation of the tools' performance in detecting API incompatibility issues.