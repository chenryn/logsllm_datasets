run multiple isolated VMs, each of which has access to data with a diﬀerent level security classi-
ﬁcation. The isolation property of virtual machines ensures that no data ﬂows from a classiﬁed to
an unclassiﬁed VM. NetTop also runs two dedicated virtual machines, one to perform encryption
using IPSec, and one ﬁltering router machine. These two VMs enforce strict security policies that
prevent network traﬃc from ﬂowing between networks of diﬀerent classiﬁcations. The NetTop ar-
chitecture enables users to run tasks of diﬀerent security levels simultaneously on a single desktop
machine using diﬀerent virtual machines. This signiﬁcantly reduces overhead in terms of number
of physical computers, and also increases productivity. An important question raised by the NetTop
architecture is how safe is the VMM from attack by a virtual machine? Co-locating a classiﬁed and
unclassiﬁed VM on the same hardware is risky, especially due to the very high value of classiﬁed
information and incentive for an attacker to steal it. In order for NetTop to see wide-scale adoption
by conservative government organizations, security experts will have to guarantee that the level of
isolation between virtual machines is the same as isolation between separate physical machines.
4.2.3 IntroVirt
The most eﬀective way of eliminating security vulnerabilities is to patch vulnerable software.
However, administrators are often hesitant to apply patches right after they are released because
a vendor’s rush to release a patch often precludes thorough testing and leads to buggy patches.
Instead, administrators may wish to install a patch after it has proved to be stable. However, this
delay in applying patches leaves vulnerable applications open to attack.
IntroVirt [JKDC05] provides a way to delaying security patches while still preventing exploita-
tion of recently released vulnerabilities. It uses virtual-machine introspection to monitor applica-
tion and operating system execution in a guest virtual machine. IntroVirt also allows administrators
to deﬁne predicates for known vulnerabilities. IntroVirt can then run these predicates on previously
recorded execution paths using a combination of virtual-machine introspection replay technology.
Running predicates on past execution will help determine whether the system has been compro-
mised by exploitation of the newly discovered vulnerability. This is all done using virtual machine
snapshots so as to not disturb the current state of the target system. In addition to replaying old
execution, IntroVirt monitors current system execution to look for activity matching the vulnera-
bility predicate. If it is discovers an attempt to exploit the vulnerability, it can respond by halting
351
execution, notifying a system administrator, or blocking malicious activity.
One limitation of IntroVirt is that the vulnerability predicates need to be written by hand.
The amount of time and eﬀort required to write a vulnerability predicate is not entirely clear.
Furthermore, writing a correct vulnerability predicate may require access to the program source
code, requiring vendor cooperation for closed-source software. In spite of this fact, IntroVirt is a
promising and useful system, especially for assessing whether a vulnerability has been exploited
in the past before it was publicly known.
4.2.4 sHype
The security systems we have discussed so far have only dealt with losing sensitive data or having
important ﬁles corrupted by an attacker. They have not, however, addressed the problem of a de-
nial of service attack. In a traditional virtual machine architecture, the VMM is only responsible
for binding resources to each guest VM, not regulating their use of resources that have been allo-
cated to them. This reduces overhead, but does so at the cost of granularity of control. One could
imagine a misbehaving guest virtual machine saturating the disk, network, and memory bandwidth
or utilizing 100% of the CPU in order to disrupt other virtual machines. SHype [SVJ+05], devel-
oped at IBM, attempts to deal with this problem by giving an administrator ﬁne-grained control
over system resource allocation. SHype mediates access to hardware resources at a low level (i.e.
disk block writes instead of ﬁle writes), eliminating the need to have multiple implementations for
diﬀerent operating systems. The downside of the SHype solution is that it cannot do anything to
prevent resource starvation within a virtual machine. SHype cannot stop an intruder from mod-
ifying sensitive ﬁles or saturating the CPU on a compromised VM because its policies are only
enforced at the virtual machine level.
4.3 VM-Based Honeypots
Honeypots have recently become a popular tool for identifying wide-scale attacks on the internet.
According to Lance Spitzner, the founder of the Honeynet project, ”A honeypot is an information
system resource whose value lies in unauthorized or illicit use of that resource. [Spi03]” To put it
more concretely, a honeypot is a computer system that is set up with the sole intention of luring
attackers. Security experts analyze activity on honeypot machines to get a better idea of threats
that face their networks and the internet as a whole.
In general, honeypots fall into two categories: low-interaction and high-interaction. A low-
interaction honeypot will accept network messages, but only give a minimal response, if any.
For example, a honeypot that accepts TCP connections on port 80 (the standard port for HTTP
requests), receives all data sent by the client, but does not respond would be a low-interaction
352
honeypot. A high-interaction honeypot, on the other hand, behaves more like a normal computer.
In the previous example, a high-interaction honeypot would service the HTTP request just like a
normal web server. In general, high-interaction honeypots provide much better information about
attacks, but require a lot more resources to run. Some security researchers have opted to use
low-interaction honeypots in order to handle traﬃc to large IP address blocks because running
high-interaction honeypots would be too costly.
With the growing popularity of honeypots, virtual machines have begun to play an important
role. Because honeypots do not serve any legitimate clients, they only require a small amount
of system resources. Virtual machines provide resource multiplexing, which allows more high-
interaction honeypots to run on the same hardware. Virtual machine technology makes it feasible
to deploy more high-interaction honeypots on the same hardware. Furthermore, virtual machine
technology allows more in-depth monitoring of malicious activities on honeypot machines without
attackers being able to detect or disable monitoring software.
Using virtual machine honeypots, however, also has a disadvantage. As of the writing of this
book, full hardware virtualization is not supported by Intel or AMD processors. A program can
determine if it is running inside of a virtual machine by executing the x86 SIDT instruction at an
unprivileged level. The redpill [Rut] program does just that, examining the result to determine
if it is running on a virtual machine. Hackers can use redpill to avoid honeypots that run on
virtual machines. Mitigating this attack on current processors would require binary translation or
instruction-level emulation, which would have a negative impact on performance. In the future,
both Intel and AMD plan to release processors that support full virtualization, making it easier for
honeypots to avoid ﬁngerprinting by attackers.
4.3.1 The Potemkin Virtual Honeyfarm
Standard virtual machines support multiplexing of processor and device resources, but require
dedicated physical memory and disk space. Because the utility of honeypots is based entirely on
their external appearance, they will rarely need the disk space and memory allocated to them. The
Potemkin Virtual Honeyfarm [VMC+05] takes advantage of this fact to allow hundreds of high-
interaction honeypots to run on a single computer. It utilizes copy-on-write to eliminate much
of the redundancy in memory and on disk between honeypots. Copy-on-write also enables the
Potemkin to rapidly create new virtual machines when it receives network traﬃc to a particular IP
address. Waiting for network traﬃc to create a honeypot enables Potemkin to cover a much larger
address space more eﬃciently because individual IP addresses do not receive traﬃc very often.
One challenge in implementing any high-interaction honeypot system is containing malware
once it has infected the honeypots. For legal reasons, anyone deploying honeypots is responsible
for ensuring that compromised hosts aren’t able to launch attacks on other computers on the inter-
353
net. Potemkin leverages its ability to rapidly create new honeypots to provide a unique solution to
this problem. It uses reﬂection to prevent infected hosts from attacking the outside network. When
a compromised VM attempts to contact an outside IP address, a gateway reﬂects the network traf-
ﬁc to a new virtual honeypot that emulates the outside host. This not only protects Potemkin from
legal liability, but also allows it to better monitor malware propagation.
The creators of the Potemkin Virtual Honeyfarm ran a number of experiments in a test bed
environment to determine Potemkin’s scalability. The test bed consisted of servers with 2.8 GHz
Xeon processors, 2 GB of physical memory, and 1Gbps network connections. The servers received
traﬃc to an entire /16 address block (216 or 65,536 IP addresses). In the experiments, the creators of
Potemkin also added a ”scan ﬁlter” to automatically reply to simple port scans without creating new
virtual machines. With the scan ﬁlter, and with swapping out virtual machines that were inactive
for greater than ﬁve minutes, the peak number of simultaneous machines required to handle traﬃc
to the /16 address block was 1,745. Separate experiments to determine the maximum number of
VMs on one physical computer showed that it could support 116, but improvements to the virtual
machine monitor could increase that number to approximately 1,500.
Prior to the Potemkin Virtual Honeyfarm, scalability of high-interaction virtual machine hon-
eypots was severely limited. With standard VM technology, a cluster of high-interaction honeypots
could only cover a few hundred IP addresses. Potemkin optimizes memory sharing between virtual
honeypots to make high-interaction honeyfarms covering an entire /16 address space a reality. This
technology will aid future security professionals in getting a better idea of how hackers, worms,
and other malware operate at an internet-wide level.
4.3.2 The Collapsar Honeypot Center
One major challenge of deploying honeypots across a wide variety of networks is management
overhead. Conﬁguring and maintaining honeypots can be a lot of work, and also requires a good
deal of expertise. Smaller enterprises may wish to deploy honeypots in their network, but do not
have the resources to administer them. The Collapsar center [JX04] is a cluster of high-interaction
honeypots that receive traﬃc from redirection devices spread out across other networks. A redi-
rector acts as a local host in a production network by tunneling all LAN traﬃc back to a Collapsar
honeypot. Once collected in a central location, it is much easier to analyze honeypot data from
multiple source networks.
The Collapsar honeypots take advantage of the isolation provided by virtual machine technol-
ogy to get a better picture of what hackers are doing without them being able to detect or disrupt
the monitoring activity. Each honeypot is instrumented with secure logging and OS monitoring
capabilities. Operating system information can be much more useful than network traces alone.
An example of this is obtaining clear-text for encrypted communications by hooking system calls.
354
It would be very diﬃcult to read encrypted traﬃc captured at the network level because the keys
are kept private.
One disadvantage of the Collapsar system is it adds a new method for ﬁngerprinting honeypots.
One could imagine a network with a redirector that sends traﬃc to a Collapsar center far away. It
would be trivially easy for another host in the local network to ﬁngerprint all of the honeypot
redirectors by simply running a ping sweep and comparing round-trip-times (RTTs). Computers
on the same sub-network will often reply in less than a millisecond, while the RTT to a remote
honeypot center may be anywhere from 20 to 500 milliseconds depending on how far away it
is. The ﬁrst way of countering this attack is to have the redirector respond immediately to pings.
However, if an attacker performs a scan with more complicated application-layer requests, then the
redirectors will have to forward the packets to Collapsar. Another counter-measure is modifying
legitimate machines to delay their responses. There is a reasonable chance, however, that slowing
down legitimate machines will have a signiﬁcant impact on quality of service in the network.
Furthermore, hackers who are clever enough to run a timing attack in the ﬁrst place may avoid the
network altogether after seeing unusually large response times.
The primary contribution of the work on Collapsar is introduction of a new management struc-
ture for honeypot deployment. Most of the VM-based monitoring technology used by Collap-
sar comes from other work on virtual machine intrusion detection systems, particularly that of
Garﬁnkel and Rosenblum. The new method of redirecting traﬃc to a central location helps fa-
cilitate dynamic deployment of honeypots over a wide variety of networks. Unlike the Potemkin
virtual honeyfarm, which is targeted more towards covering huge contiguous IP addess blocks,
Collapsar does a better job of ﬁlling small holes in production networks to detect more focused
attacks (i.e.
individual hackers instead of worms). Because the two technologies are not mutu-
ally exclusive, however, a Collapsar-type redirection infrastructure with central honeypots running
Potemkin may prove to be a very eﬀective tool for detecting a wide variety of attacks across a large
but diverse address space.
4.4 Terra: A VM-Based Trusted Computing Platform
Virtual machines monitors are particularly well suited for building trusted computing platforms.
This is because VMMs are very light-weight and do not need to change often. The Terra system
[GPC+03] uses a trusted virtual machine monitor (TVMM) to partition resources between isolated
virtual machines (VM), thus providing the appearance of a dedicated physical machine for each
VM. The TVMM can expose either an ”open box” hardware interface, or a ”closed box” inter-
face. ”Open box” hardware is a general-purpose platform like a personal computer. ”Closed box”
hardware protects the privacy and integrity of some of its contents, more like game consoles and
355
cellular phones.
The TVMM is capable of verifying and authenticating software running inside of the virtual
machines to remote party over a network connection using a process called attestation. Attestation
requires building a chain of cryptographic certiﬁcates. The trusted hardware contains an asymmet-
ric private key that is signed by the vendor. The tamper-resistant hardware certiﬁes the integrity
of the system ﬁrmware (e.g. PC BIOS). The ﬁrmware, in turn, veriﬁes the boot loader, which
veriﬁes the TVMM. Finally, the TVMM certiﬁes that each of the virtual machines it loads have
not been tampered with. This process of attestation allows an application running in a closed box
environment to cryptographically identify itself to a remote party. This allows the remote party
to trust the application to behave as expected. Attestation is particularly useful for digital rights
management and copyright enforcement; manufacturers may not want give service to modiﬁed or
pirated software.
4.5 ReVirt: A VM-Based Logging and Replaying System
Many administrators rely on system logs to analyze and understand security breaches, discover
the vulnerability that led to the initial compromise, and to repair any damage that may have been
caused by the intruder. Traditional logging systems, however, have a few shortcomings that prevent
them from fully achieving this goal: lack of integrity and of completeness. These systems rely on
the integrity of the operating system to protect them from attack. If a hacker is able to break in to
the operating system, however, then he or she can tamper with or disable all logging capabilities,