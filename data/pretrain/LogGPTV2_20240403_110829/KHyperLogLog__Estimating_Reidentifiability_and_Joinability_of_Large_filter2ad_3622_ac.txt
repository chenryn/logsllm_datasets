of individual ﬁelds, KHLL can be used to analyze any
combinations of ﬁelds, including a complete row. This
can be done, for example, by simply concatenating the
values of multiple ﬁelds, and treating the combination
as a new ﬁeld. The reidentiﬁability risk will grow
as the number of dimensions increases. For example
with movie recommendations, the combination of movie
name, rating and date of recommendation can be highly
unique [8].
B. Limitations and Mitigations
Using a KHLL sketching algorithm with K = 2048
and 512 HLL buckets would give us an estimated error
rate of 2% for the value cardinality and about 4%
error rate for ID cardinalities. A higher error rate of
ID cardinality estimates is tolerable given that we are
more concerned about ﬁeld values that associate with
low number of IDs. In those cases the algorithm will
use HLL++ in sparse representation mode which gives
good estimates with minimal errors. Note that the trade
off between accuracy and efﬁciency is conﬁgurable.
Meanwhile as a KHLL sketch effectively maintains K
uniform random samples of ﬁeld values, the estimated
distribution does come with sampling bias. Speciﬁcally,
it is possible that the estimated distribution may miss
some outlier ﬁeld values that associate with a large or
small number of IDs. One mitigation is to run multiple
analyses using different hash functions (with random
(cid:20)(cid:22)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 4: Example illustration of joinability between Per-
sonally Identiﬁable Information (PII) and pseudonymous
IDs with raw User Agent (UA) strings being the join key.
seeds) to reduce the chance of outliers being consistently
left out of the analysis.
VII. ESTIMATING JOINABILITY USING KHLL
Estimating the joinability of two data sets through a
pair of ﬁelds say F1 and F2 from their KHLL sketches
is also straightforward. Recall that containment of F1 in
F2 is given by |F1 ∩ F2|/|F2|. To compute this, we need
only the cardinality of F1 and F2 plus the cardinality of
their intersection.
Using the inclusion-exclusion principle, we estimate
|F1 ∩ F2| = |F1| + |F2| − |F1 ∪ F2|. The union of F1
and F2 can be easily estimated by merging the KHLL
sketch of F1 and that of F2. An alternative approach
for computing the set intersection is by identifying the
hash values that exist in both the KHLL sketches of F1
and F2 and computing the minmax of the K smallest
hashes on both sides. This would allow us to quantify
the error rate of individual set intersections directly, but
the error rate will vary as the minmax of the hashes will
vary for different pairs of ﬁelds. We prefer the simpler
inclusion-exclusion-based approach.
In addition to determining the joinability of ﬁelds,
KHLL sketches can provide insights into the potential
joinability of identities in two data sets. For example,
pseudonymous IDs in one data set could be reidentiﬁed
if the data set is joinable with another data set containing
Personally Identiﬁable Information (PII), and if the join
keys are associated with one pseudonymous ID and PII
respectively (see Figure 4).
Speciﬁcally, given a pair of sketches of two ﬁelds F1
and F2 in data sets D1 and D2 respectively, we could
estimate
• whether F1 is highly contained in F2 or vice-versa
• whether F1 uniquely identiﬁes ID 1 in D1
• whether F2 uniquely identiﬁes ID 2 in D2
Fig. 5: Two-step approach of reidentiﬁability and join-
ability analysis: (i) distributed scanners read various data
sets to produce a KHLL sketch for every (F, ID) tuple,
(ii) various stats (including pairwise containment of data
sets) are computed ofﬂine based on the sketches (rather
than by comparing the raw data sets).
KHLL allows us to estimate all the above conditions.
Speciﬁcally, the ﬁrst level of KHLL which is essentially
a KMV sketch can be used to estimate the cardinality
of F1 and F2, the cardinality of the intersection, and
thus containment. Meanwhile, the second level of KHLL,
consisting of HLL sketches, gives the uniqueness distri-
bution and thus the ratio of uniquely identifying values
easily.
A. Practical Considerations for Joinability Analysis
Estimating the joinability of large data sets is a hard
problem. Naively estimating the pairwise joinability of
data sets involves a quadratic number of full-table scans.
The number of scans needed can increase quickly, espe-
cially for large data sets with many data ﬁelds.
As shown in Figure 5 however, KHLL allows us to
estimate joinability from the sketches alone. This is a
huge saving in that it allows us to scan each data set
only once and then pairwise compare the sketches rather
than original data sets.
(cid:20)(cid:22)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
The sketching process can be agnostic to the un-
derlying data when the schema of the data sets are
well deﬁned. For example when using protocol buffer
messages [16] we can analyze new data sets without
any need to conﬁgure information about ﬁelds, especially
when the semantic types of data ﬁelds are properly
annotated [17].
The sketching process can also be distributed. The
respective data owners do not need to grant a central
service access to the raw data, but simply to agree on the
sketching method and to upload the sketches to a central
repository. The sketches containing the hash values of
potentially sensitive data including user IDs should still
be treated with care such as by limiting the access and
ensuring a short lifetime.
B. Limitations and Mitigations
Using sketches for joinability analysis comes with
some risks of false positives and negatives.
that use a similar
False positives: The containment metric is agnos-
tic to the semantics of the underlying data. Specif-
ically, containment (or Jaccard) does not distinguish
between ﬁelds
range of val-
ues but are semantically different. As an exam-
ple, we could falsely determine port_number and
seconds_till_mignight ﬁelds
to be joinable
since they both have an extensive overlap in the integer
range of [0, 86400). The rate of false positives could be
mitigated by requiring a large cardinality threshold on
the potential join keys.
False negatives: The containment metric will fail to
detect similar ﬁelds that have been encoded differently
(e.g., base64 versus raw string) or have undergone some
slight transformations (e.g., a microsecond timestamp
versus the coarsened millisecond version). This is a
hard problem in practice. The system could potentially
support some common transformations or encodings
when the semantic type of a data ﬁeld is known, but
there is no way to handle all possibilities.
The containment metric can also be unreliable when
set sizes are highly skewed. When the expected error rate
of a set is larger than the cardinality of a much smaller
set, the estimate for the set intersection computed using
the inclusion- exclusion principle will be unreliable. One
could potentially complement the containment metric
with some other similarity scores like the similarity
between the frequency distribution of the potential join
keys.
While KHLL can evaluate the pairwise joinability
of data sets based on individual ﬁelds, estimating the
joinability of data sets through arbitrary combinations
of ﬁelds remains practically infeasible given that in-
tractable number of potential ﬁeld combinations. One
could however systematically approach this by testing
the joinability between combinations of high-risk ﬁelds,
for example, involving only those that have high unique-
ness.
The pairwise joinability analysis does not readily
detect multi-hop joinability. For example, when a data set
D1 is joinable with data set D2, and D2 is joinable with
data set D3 through two different pairs of join keys, we
will not detect that D1 is joinable to D3. Such multi-hop
joinability analysis could be similarly estimated using
clustering and graph traversal algorithms such as label
propagation [18].
VIII. MEASURING EFFICIENCY OF KHLL
The KHLL algorithm and the metrics we estimate with
it have been implemented in a proprietary production
environment in Go using the MapReduce programing
model [19].
For each ﬁeld (or speciﬁed combinations of ﬁelds) in
the data set the MapReduce outputs a KHLL sketch in
one pass.
To reason about efﬁciency, we implemented two naive
MapReduce algorithms: (i) Count Exact (CE) which
computes the exact variants of the metrics that KHLL
estimates, and (ii) Count Exact Single (CES) which
computes the same set of metrics exactly, but analyzing
only one single ﬁeld during a given MapReduce run.
We designed the CE MapReduce to output, for each
ﬁeld, a dictionary of ﬁeld values to ID sets (allowing
the computation of various statistics that a KHLL sketch
approximates). One would expect that emitting the entire
ﬁeld-value-to-ID-set dictionary will result in substantial
memory overhead. The CES MapReduce is a more
realistic simpliﬁcation of CE which outputs the tuples
of ﬁeld values and ID sets of a only single speciﬁc ﬁeld.
The test data set on which we ran our performance
analyses represents the dataﬂow graph of various pro-
duction operations at Google. Each row in this data set
represents a node in the dataﬂow graph and has about
50 data ﬁelds describing the properties of the operations
as well as the input and output data. One of the data
ﬁelds speciﬁes the globally unique name of the machine
where the operation is run or controlled. We used this
machine name as the ID ﬁeld in our analyses. Note
that this data set does not contain any user data and
is not privacy sensitive as these are not necessary for
performance measurements.
(cid:20)(cid:22)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:06 UTC from IEEE Xplore.  Restrictions apply. 
MapReduce type
All ﬁelds
Single ﬁeld
100 GB
Input size Algorithm CPU usage (vCPUs) RAM usage (GBs)
1.14e+4
2.08e+3
3.40e+6
3.11e+4
(n.a.)
2.37e+6
4.76e+3
1.30e+5
4.01e+3
9.78e+2
3.53e+5
6.25e+4
(n.a.)
9.92e+5
7.23e+2
4.47e+4
CE
KHLL
CE
KHLL
CE
KHLL
CES
CES
1 GB
1 TB
1 GB
100 GB
Peak RAM (GB) Output size (GB) Runtime (s)
5.83e+2
1.43e+2
1.93e+4
2.63e+2
(n.a.)
1.13e+4
5.76e+2
1.10e+3
9.34e+0
9.45e-1
1.10e+2
2.00e+0
(n.a.)
2.52e+0
8.07e-1
1.79e+0
1.04e+0
1.60e-3
2.64e+0
3.46e-3
(n.a.)
3.50e-3
1.57e-2
2.35e-1
TABLE I: Performance metrics of KHLL and exact counting algorithms. We conﬁgured KHLL to have K=2048
and M=1024. 1 GBs = 1 GB of RAM used for 1 second. Virtual CPU (vCPU) is a platform-neutral measurement
for CPU resources. 1 vCPUs = 1 vCPU used for 1 second. The CE MapReduce for analyzing all data ﬁelds in a
1 TB data set was excessively expensive and was halted.
We ran KHLL, CE and CES on several subsets of
the test data set in a shared computational cluster at
Google. These analyses were provided computational
resources at a priority class that is typically used for
batch jobs. Measuring the performance metrics of jobs
in a shared computational cluster is not straightforward
since any given machine can host multiple unrelated
jobs with varying priority classes that can consume
machine resources in unpredictable ways. So we focused
on the performance metrics which a typical customer of
a commercial computational cluster (e.g., Amazon EC2,
Google GCE) would pay for.
Table I shows the performance metrics of the MapRe-
duce runs. As one can see, KHLL is consistently more
efﬁcient than CE across various metrics. Performance
differs by 1 or 2 orders of magnitude even for the
relatively small data sets in our experiment. In fact, the
CE MapReduces for analyzing all data ﬁelds in an 1
TB data set became too expensive to be completed in
the shared computational cluster. Interestingly, per our
test data set, it is even more memory efﬁcient (though
slightly slower) to compute the KHLL sketches of all
data ﬁelds in a single MapReduce run, than to analyze a
single data ﬁeld using CES. This performance disparity
is critical in practice, as it is the difference between an
analysis that is feasible to run, and one that is too slow
to be worthwhile.
In various production setups at Google, KHLL scales
to analyze hundreds of data sets, each containing po-
tentially trillions of rows and tens of thousands of data
ﬁelds measuring petabytes in sizes, to produce millions
of sketches in each run.
IX. MEASURING ACCURACY OF KHLL
To provide reproducible validation results, we have
implemented a version of the KHLL algorithm in Big-
Query Standard SQL, and simulated the computation
of uniqueness distribution and joinability using publicly
available data sets. The code for the experiments can
likely be adapted for other SQL engines, and is shared
on GitHub [20].
A. Accuracy of Uniqueness Distribution Estimation
We measured the accuracy of the estimated uniqueness
distribution using three publicly available data sets. The
ﬁrst two are taken from the Netﬂix Prize data set which
was released in 2006 and shown to be reidentifying for
a signiﬁcant fraction of the users [8]. We estimate the
uniqueness distribution of (a) movies, and (b) tuples
of movie and date. Note that we do not consider the
entire list of movies (or respectively all pairs of movie
and date) associated with individual pseudo-identiﬁers.
We could analyze this easily but
it will be less in-
teresting for our validation purposes, as most of the
values will be unique. The third data set is the 2010
US Census [21] through which we count the number of
distinct individuals associated with a given (ZIP code,
age) tuple. The corresponding uniqueness distribution
gives an indication of the reidentiﬁability of these quasi-
identiﬁers within the US population. As we will see,
the three data sets present different uniqueness proﬁles,
allowing us to test the accuracy of KHLL estimation in
different situations.
We simulate the KHLL algorithm, with parameters