and report the same hardware failure in the (shared) net-
work. If there exists a hardware problem in the network,
such as a bad link, or a bad switch, this problem can be de-
tected by any host that tries to access those shared resources.
In order to accurately portray the number of network fail-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
8
ures within the system, instead of counting one failure for
each record, we need to coalesce them into one single fail-
ure. As a result, we use the same clustering technique as
discussed in the context of memory failures to locate the re-
dundant records generated by multiple chips. Partial results
after applying this technique are shown in Table 4. In this
table, the threshold that is used to form clusters is Tnet =
15 minutes. Please note that this threshold is larger than
the one used for the memory subsystem (i.e., 1 minute) be-
cause some network failures take a long time before they
are ﬁxed. Further, the same failures can be reported several
times from the same midplane within a few minutes apart
of each other possibly by different applications (the appli-
cation crashes when encountering these failures), while in
the memory subsystem, as soon as the application crashes
due to a program bug, the same failure is unlikely to repeat
within a short time frame.
There are three typical types of network failures that are
usually reported by a large number of chips at the same
time: (1) rts tree/torus link training failed, (2) BIDI Train
Failure, and (3) external input interrupt:uncorrectable torus
error. Among these three, both (1) and (2) occur when the
chips that are assigned to an application cannot conﬁgure
the torus by themselves. There could be many causes of
this problem, such as link failure, network interface fail-
ure, wrong conﬁguration parameters, to name just a few. In
such scenarios, the application will not even get launched.
Failure (3) occurs when an uncorrectable torus error is en-
countered, making the applications crash again.
After the ﬁlteration with this clustering, the location of
a network failure is no longer denoted by a chip ID, but by
the midplane ID because a network failure can be encoun-
tered by any chip within that midplane. For those clusters
that involve two midplanes (because the corresponding ap-
plication is spread over two midplanes), we assume that the
failure occurs within both midplanes. At the end of this ﬁl-
tering, we have 69 network failures over the 84 day period.
4.3.3 Midplane Switch Failures
Midplane switches inter-connect midplanes, and are pro-
grammed by the control logic of the machine. These
events usually report certain ports cannot be connected to
or cleared. Whenever the control programs encounter such
problems, it indicates there is a problem with the hard-
ware - either the switch itself, or the link connecting the
Failure description
cluster size
BIC NCRIT interrupt (unit=0x02 bit=0x00)
rts tree/torus link training failed
(RAS) BiDi Train Failure
external input interrupt:uncorrectable torus error
rts tree/torus link training failed
1
297
2071
342
32
Table 4. Network failure clusters.
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
4
3
2
1
0
0
7
6
5
4
3
2
1
0
0
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
7
6
5
4
3
2
1
0
0
500
1000
Hours
1500
2000
500
1000
Hours
1500
2000
(a) Network failures
(b) Hardware memory failures
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
4
3
2
1
0
0
500
1000
Hours
1500
2000
500
1000
Hours
1500
2000
(c) Software memory failures
(d) Midplane switch failures
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
4
3
2
1
0
0
500
1000
Hours
1500
2
1
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
0
0
500
1000
Hours
1500
(e) Node card failures
(f) Service card failures
Figure 6. Time series of the failure datasets.
switches. If the switch has problems, then only the program
that runs on the switch will report the failure; otherwise,
several switches will be affected. As a result, we can use
the same clustering algorithm (as descried in Section 4.3.1)
to form the clusters, and coalesce a failure cluster into a
single failure. However, as shown in Figure 2(c) and Fig-
ure 3(c), midplane switch failures are usually not clustered,
with most cluster sizes being 1 or 2, and only very few go-
ing up to cluster sizes of 5 or 6. We would like to emphasize
that Figure 2(c) shows the number of records per hour, but
the threshold we use to determine the clusters is Tmps = 15
minutes, similar to the one used in the network subsystem.
After coalescing these clustered events, we still have 152
midplane switch failures.
4.4 Applying the 3-step Filtering Algorithm
After discussing the 3-step ﬁltering technique, we have
61 network failures, 76 memory failures (among which 55
are hardware failures, and 21 software failures), 152 mid-
plane switch failures, 8 node card failures, and 14 service
card failures over the period of 1921 hours. Figures 6(a)-(f)
present the time series of failures at an hourly granularity.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
9
5 Concluding Remarks
Parallel system event/failure logging in production en-
vironments has widespread applicability. It can be used to
obtain valuable information from the ﬁeld on hardware and
software failures, which can help designers make hardware
and software revisions. With ﬁne-grain event logging, the
volume of data that is accumulated can become unwieldy
over extended periods of time (months/years), and across
thousands of nodes. Further, the idiosyncracies of logging
mechanisms can lead to multiple records of the same events,
and these need to be cleaned up in order to be accurate
for subsequent analysis. In this paper, we have embarked
on a study of the failures on a 8192 processor BlueGene/L
prototype at IBM Rochester, which current stands at #8 in
the Top-500 list. We have presented a 3-step ﬁltering algo-
rithm: ﬁrst extracting and categorizing failure events from
the raw logs, then performing a temporal ﬁltering to remove
duplicate reports from the same location, and ﬁnally coa-
lescing failure reports across different locations. Using this
approach, we can considerably compress error logs by re-
moving 99.96% of the 828,387 entries recorded between
August 26, 2004 and November 17, 2004 on this system.
Such ﬁltering is being performed on a weekly basis.
References
[1] Blue Gene/L System Software Update.
unix.mcs.anl.gov/
2005/BGL-SSW13-LLNL-exper.pdf.
http://www-
beckman/bluegene/SSW-Utah-
[2] TOP500 List 11/2004. http://www.top500.org.
[3] N. Adiga et al. An Overview of the BlueGene/L Supercom-
puter. In Proceedings of Supercomputing (SC2002), Novem-
ber 2002.
[4] D. Anderson, J. Dykes, and E. Riedel. More Than an Inter-
face – SCSI vs. ATA. In Proceedings of the Conference on
File and Storage Technology (FAST), March 2003.
[5] BlueGene/L
http://www.llnl.gov/asci/platforms/bluegene/.
[6] IBM
Research,
http://www.research.ibm.com/bluegene/.
Workshop.
BlueGene.
[7] D. Brooks and M. Martonosi. Dynamic Thermal Man-
agement for High-Performance Microprocessors.
In Pro-
ceedings of the Seventh International Symposium on High-
Performance Computer Architecture (HPCA-7), January
2001.
[8] M. F. Buckley and D. P. Siewiorek. Vax/vms event monitor-
ing and analysis. In FTCS-25, Computing Digest of Papers,
pages 414–423, June 1995.
[9] M. F. Buckley and D. P. Siewiorek. Comparative analysis
of event tupling schemes. In FTCS-26, Computing Digest of
Papers, pages 294–303, June 1996.
[10] M. L. Fair, C. R. Conklin, S. B. Swaney, P. J. Meaney, W. J.
Clarke, L. C. Alves, I. N. Modi, F. Freier, W. Fischer, and
N. E. Weber. Reliability, Availability, and Serviceability
(RAS) of the IBM eServer z990. IBM Journal of Research
and Development, 48(3/4), 2004.
[11] R. L. Graham, S. E. Choi, D. J. Daniel, N. N. Desai, R. G.
Minnich, C. E. Rasmussen, L. D. Risinger, and M. W.
Sukalski. A network-failure-tolerant message-passing sys-
tem for terascale clusters. In Proceedings of the International
Conference on Supercomputing, 2002.
[12] J. Hansen.
Trend Analysis and Modeling of Uni/Multi-
Processor Event Logs. PhD thesis, Dept. of Computer Sci-
ence, Carnegie-Mellon University, 1988.
[13] G. Herbst.
IBM’s Drive Temperature Indicator Processor
In IBM
(Drive-TIP) Helps Ensure High Drive Reliability.
White Paper, October 1997.
[14] R. Iyer, L. T. Young, and V. Sridhar. Recognition of error
symptons in large systems. In Proceedings of the Fall Joint
Computer Conference, 1986.
[15] E. Krevat, J. G. Castanos, and J. E. Moreira. Job Scheduling
for the BlueGene/L System. In Proceedings of the Workshop
on Job Scheduling Strategies for Parallel Processing, 2003.
[16] T. Y. Lin and D. P. Siewiorek. Error log analysis: Statisti-
cal modelling and heuristic trend analysis. IEEE Trans. on
Reliability, 39(4):419–432, October 1990.
[17] S.S. Mukherjee, C. Weaver, J. Emer, S.K. Reinhardt, and
T. Austin. A Systematic Methodology to Compute the Ar-
chitectural Vulnerability Factors for a High-Performance Mi-
croprocessor. In Proceedings of the International Symposium
on Microarchitecture (MICRO), pages 29–40, 2003.
[18] A. Parashar, S. Gurumurthi, and A. Sivasubramaniam. A
Complexity-Effective Approach to ALU Bandwidth En-
hancement for Instruction-Level Temporal Redundancy. In
Proceedings of the International Symposium on Computer
Architecture (ISCA), 2004.
[19] R. Sahoo, A. Sivasubramaniam, M. Squillante, and Y. Zhang.
Failure Data Analysis of a Large-Scale Heterogeneous
Server Environment.
In Proceedings of the 2004 Interna-
tional Conference on Dependable Systems and Networks,
pages 389–398, 2004.
[20] P. Shivakumar, M. Kistler, S. Keckler, D. Burger, and
L. Alvisi. Modeling the effect of technology trends on soft
error rate of combinational logic. In Proceedings of the 2002
International Conference on Dependable Systems and Net-
works, pages 389–398, 2002.
[21] K. Skadron, M.R. Stan, W. Huang, S. Velusamy, K. Sankara-
narayanan, and D. Tarjan. Temperature-Aware Microarchi-
tecture. In Proceedings of the International Symposium on
Computer Architecture (ISCA), pages 1–13, June 2003.
[22] J. Srinivasan, S. V. Adve, P. Bose, and J. A. Rivers. The
Impact of Technology Scaling on Processor Lifetime Reli-
ability.
In Proceedings of the International Conference on
Dependable Systems and Networks (DSN-2004), June 2004.
Impact of correlated failures on
dependability in a VAXcluster system.
In Proceedings of
the IFIP Working Conference on Dependable Computing for
Critical Applications, 1991.
[23] D. Tang and R. K. Iyer.
[24] D. Tang and R. K. Iyer. Analysis and modeling of correlated
IEEE Transactions on
failures in multicomputer systems.
Computers, 41(5):567–577, 1992.
[25] D. Tang, R. K. Iyer, and S. S. Subramani. Failure analysis
and modelling of a VAXcluster system. In Proceedings In-
ternational Symposium on Fault-tolerant Computing, pages
244–251, 1990.
[26] M. M. Tsao. Trend Analysis and Fault Prediction. PhD the-
sis, Dept. of Computer Science, Carnegie-Mellon University,
1983.
[27] K. Vaidyanathan, R. E. Harper, S. W. Hunter, and K. S.
Trivedi. Analysis and Implementation of Software Rejuve-
nation in Cluster Systems. In Proceedings of the ACM SIG-
METRICS 2001 Conference on Measurement and Modeling
of Computer Systems, pages 62–71, June 2001.
[28] J. Xu, Z. Kallbarczyk, and R. K. Iyer. Networked Windows
NT System Field Failure Data Analysis. Technical Report
CRHC 9808 UIUC, 1999.
[29] J.F. Zeigler. Terrestrial Cosmic Rays. IBM Journal of Re-
search and Development, 40(1):19–39, January 1996.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
10