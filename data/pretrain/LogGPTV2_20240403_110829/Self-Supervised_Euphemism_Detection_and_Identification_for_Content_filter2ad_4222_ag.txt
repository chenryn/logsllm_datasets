(ICML), 2001, pp. 282–289.
[38] T. Rauber and K. Berns, “Kernel multilayer perceptron,” in Proceedings
IEEE,
of SIBGRAPI Conference on Graphics, Patterns and Images.
2011, pp. 337–343.
[39] H. Yang, X. Ma, K. Du, Z. Li, H. Duan, X. Su, G. Liu, Z. Geng,
and J. Wu, “How to learn klingon without a dictionary: Detection and
measurement of black keywords used by the underground economy,”
in IEEE Symposium on Security and Privacy (SP).
IEEE, 2017, pp.
751–769.
[40] J. Taylor, M. Peignon, and Y.-S. Chen, “Surfacing contextual hate speech
words within social media,” arXiv preprint arXiv:1711.10093, 2017.
[41] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word
vectors with subword information,” Transactions of the Association for
Computational Linguistics (TACL), vol. 5, pp. 135–146, 2017.
[42] O. Levy and Y. Goldberg, “Dependency-based word embeddings,” in
Proceedings of Association for Computational Linguistics (ACL), 2014,
pp. 302–308.
[43] N. Christin, “Traveling the Silk Road: A measurement analysis of a
large anonymous online marketplace,” in Proceedings of International
Conference on World Wide Web (WWW), 2013, pp. 213–224.
[44] J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, and J. Han, “Setexpan: Corpus-
based set expansion via context feature selection and rank ensemble,”
in Proceedings of The European Conference on Machine Learning
and Principles and Practice of Knowledge Discovery in Databases
(ECML-PKDD). Springer, 2017, pp. 288–304.
[45] W. Zhu, H. Gong, J. Shen, C. Zhang, J. Shang, S. Bhat, and J. Han,
“FUSE: Multi-faceted set expansion by coherent clustering of skip-
grams,” in Proceedings of The European Conference on Machine
Learning and Principles and Practice of Knowledge Discovery in
Databases (ECML-PKDD), 2020.
[46] Y. Zhang, J. Shen, J. Shang, and J. Han, “Empower entity set expansion
via language model probing,” in Proceedings of Annual Meeting of the
Association for Computational Linguistics (ACL), 2020.
[47] J. Huang, Y. Xie, Y. Meng, J. Shen, Y. Zhang, and J. Han, “Guiding
corpus-based set expansion by auxiliary sets generation and co-
expansion,” in Proceedings of The Web Conference, 2020, pp. 2188–
2198.
[48] J. Shen, W. Qiu, J. Shang, M. Vanni, X. Ren, and J. Han, “Synsetexpan:
An iterative framework for joint entity set expansion and synonym
discovery,” in Proceedings of Empirical Methods in Natural Language
Processing (EMNLP), 2020, pp. 8292–8307.
[49] X. Rong, Z. Chen, Q. Mei, and E. Adar, “Egoset: Exploiting word ego-
networks and user-generated ontology for multifaceted set expansion,”
in Proceedings of Web Search and Data Mining (WSDM), 2016, pp.
645–654.
[50] W. L. Hamilton, K. Clark, J. Leskovec, and D. Jurafsky, “Inducing
domain-speciﬁc sentiment lexicons from unlabeled corpora,” in Proceed-
ings of Empirical Methods in Natural Language Processing (EMNLP),
vol. 2016. NIH Public Access, 2016, p. 595.
[51] C. Yang, J. Zhang, and J. Han, “Co-embedding network nodes and
hierarchical labels with taxonomy based generative adversarial networks,”
in Proceedings of IEEE International Conference on Data Mining
(ICDM), 2020.
[52] J. Huang, Y. Xie, Y. Meng, Y. Zhang, and J. Han, “Corel: Seed-
guided topical taxonomy construction by concept learning and relation
transferring,” in Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (KDD), 2020, pp.
1928–1936.
[53] Y. Mao, T. Zhao, A. Kan, C. Zhang, X. L. Dong, C. Faloutsos,
and J. Han, “Octet: Online catalog taxonomy enrichment with self-
supervision,” in Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining (KDD), 2020, pp.
2247–2257.
[54] J. Shang, X. Zhang, L. Liu, S. Li, and J. Han, “Nettaxo: Automated
topic taxonomy construction from text-rich network,” in Proceedings
of The Web Conference, 2020, pp. 1908–1919.
[55] J. Shen, Z. Shen, C. Xiong, C. Wang, K. Wang, and J. Han, “TaxoExpan:
Self-supervised taxonomy expansion with position-enhanced graph
neural network,” in Proceedings of The Web Conference, 2020, pp.
486–497.
[56] C. Zhang, F. Tao, X. Chen, J. Shen, M. Jiang, B. Sadler, M. Vanni, and
J. Han, “Taxogen: Constructing topical concept taxonomy by adaptive
term embedding and clustering,” Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining
(KDD), 2018.
[57] P. Bonacich, “Factoring and weighting approaches to status scores and
clique identiﬁcation,” Journal of Mathematical Sociology, vol. 2, no. 1,
pp. 113–120, 1972.
[58] ——, “Technique for analyzing overlapping memberships,” Sociological
Methodology, vol. 4, pp. 176–185, 1972.
[59] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank citation
ranking: Bringing order to the web.” Stanford InfoLab, Tech. Rep.,
1999.
[60] S. Ishiwatari, H. Hayashi, N. Yoshinaga, G. Neubig, S. Sato, M. Toyoda,
and M. Kitsuregawa, “Learning to describe unknown phrases with local
and global contexts,” in Proceedings of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies (NAACL-HLT), 2019, pp. 3467–3476.
[61] K. Ni and W. Y. Wang, “Learning to explain non-standard english
words and phrases,” in Proceedings of International Joint Conference
on Natural Language Processing (ĲCNLP), 2017, pp. 413–417.
[62] K. Taghipour and H. T. Ng, “Semi-supervised word sense disam-
biguation using word embeddings in general and speciﬁc domains,”
in Proceedings of North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL-
HLT), 2015, pp. 314–323.
[63] A. Raganato, C. D. Bovi, and R. Navigli, “Neural sequence learning
models for word sense disambiguation,” in Proceedings of Empirical
Methods in Natural Language Processing (EMNLP), 2017, pp. 1156–
1167.
[64] A. Raganato, J. Camacho-Collados, and R. Navigli, “Word sense
disambiguation: A uniﬁed evaluation framework and empirical compar-
ison,” in Proceedings of the European Chapter of the Association for
Computational Linguistics (EACL), vol. 1, 2017, pp. 99–110.
[65] I. Iacobacci, M. T. Pilehvar, and R. Navigli, “Embeddings for word sense
disambiguation: An evaluation study,” in Proceedings of Association
for Computational Linguistics (ACL), vol. 1, 2016, pp. 897–907.
[66] L.
Weng,
representation
lilianweng.github.io/lil-log,
[Online]. Available:
//lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html
“Self-supervised
2019.
learning,”
https:
[67] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,
“ALBERT: A lite bert for self-supervised learning of language repre-
sentations,” in Proceedings of International Conference on Learning
Representations (ICLR), 2019.
[68] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert
pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.
[69] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A
framework for self-supervised learning of speech representations,” in
Proceedings of Advances in Neural Information Processing Systems
(NeurIPS), 2020.
[70] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton,
“Big self-supervised models are strong semi-supervised learners,” in
Proceedings of Advances in Neural Information Processing Systems
(NeurIPS), 2020.
[71] L. Liu, J. Shang, X. Ren, F. F. Xu, H. Gui, J. Peng, and J. Han,
“Empower sequence labeling with task-aware neural language model,”
in Proceedings of AAAI Conference on Artiﬁcial Intelligence (AAAI).
AAAI Press, 2018, pp. 5253–5260.
[72] Z. Feng, C. Xu, and D. Tao, “Self-supervised representation learning by
rotation feature decoupling,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2019, pp. 10 364–
10 374.
[73] A. Kolesnikov, X. Zhai, and L. Beyer, “Revisiting self-supervised visual
representation learning,” in Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1920–
1929.
[74] M. Sabokrou, M. Khalooei, and E. Adeli, “Self-supervised representation
learning via neighborhood-relational encoding,” in Proceedings of the
IEEE International Conference on Computer Vision (ICCV), 2019, pp.
8010–8019.
[75] W. Zhu, C. Zhang, S. Yao, X. Gao, and J. Han, “A spherical
hidden markov model for semantics-rich human mobility modeling,” in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI),
2018.
[76] O. Mees, M. Tatarchenko, T. Brox, and W. Burgard, “Self-supervised
3d shape and viewpoint estimation from single images for robotics,”
in Proceedings of IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).
IEEE, 2019, pp. 6083–6089.
[77] A. Nair, D. Chen, P. Agrawal, P. Isola, P. Abbeel, J. Malik, and S. Levine,
“Combining self-supervised learning and imitation for vision-based rope
manipulation,” in Proceedings of IEEE International Conference on
Robotics and Automation (ICRA).
IEEE, 2017, pp. 2146–2153.
[78] L. Berscheid, P. Meißner, and T. Kröger, “Self-supervised learning
for precise pick-and-place without object model,” IEEE Robotics and
Automation Letters, vol. 5, no. 3, pp. 4828–4835, 2020.
[79] X. Zhai, A. Oliver, A. Kolesnikov, and L. Beyer, “S4l: Self-supervised
semi-supervised learning,” in Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2019, pp. 1476–1485.
[80] Y. Sun, E. Tzeng, T. Darrell, and A. A. Efros, “Unsupervised domain
adaptation through self-supervision,” arXiv preprint arXiv:1909.11825,
2019.
[81] J. Xu, L. Xiao, and A. M. López, “Self-supervised domain adaptation
for computer vision tasks,” IEEE Access, vol. 7, pp. 156 694–156 706,
2019.
[82] H. Yin, P. Molchanov, J. M. Alvarez, Z. Li, A. Mallya, D. Hoiem, N. K.
Jha, and J. Kautz, “Dreaming to distill: Data-free knowledge transfer
via deepinversion,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2020, pp. 8715–
8724.
[83] G. Kahn, A. Villaﬂor, B. Ding, P. Abbeel, and S. Levine, “Self-
supervised deep reinforcement learning with generalized computation
graphs for robot navigation,” in Proceedings of IEEE International
Conference on Robotics and Automation (ICRA).
IEEE, 2018, pp. 1–8.
[84] A. Zeng, S. Song, S. Welker, J. Lee, A. Rodriguez, and T. Funkhouser,
“Learning synergies between pushing and grasping with self-supervised
deep reinforcement learning,” in Proceedings of IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS).
IEEE, 2018,
pp. 4238–4245.
[85] V. H. Pong, M. Dalal, S. Lin, A. Nair, S. Bahl, and S. Levine, “Skew-ﬁt:
State-covering self-supervised reinforcement learning,” arXiv preprint
arXiv:1903.03698, 2019.
[86] Y. Huo, Y. Lu, Y. Niu, Z. Lu, and J.-R. Wen, “Coarse-to-ﬁne grained
classiﬁcation,” in Proceedings of the 42nd International ACM SIGIR
Conference on Research and Development in Information Retrieval
(SIGIR), 2019, pp. 1033–1036.
[87] W. Liu, C. Zhang, J. Zhang, and Z. Wu, “Global for coarse and part
for ﬁne: A hierarchical action recognition framework,” in Proceedings
of 25th IEEE International Conference on Image Processing (ICIP).
IEEE, 2018, pp. 2630–2634.
[88] Z. Li, Y. Wei, Y. Zhang, X. Zhang, and X. Li, “Exploiting coarse-to-ﬁne
task transfer for aspect-level sentiment classiﬁcation,” in Proceedings
of the AAAI Conference on Artiﬁcial Intelligence (AAAI), vol. 33, 2019,
pp. 4253–4260.
[89] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[90] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by
jointly learning to align and translate,” in Proceedings of International
Conference on Learning Representations (ICLR), 2015.
[91] D. W. Hosmer Jr, S. Lemeshow, and R. X. Sturdivant, Applied logistic
regression.
John Wiley & Sons, 2013, vol. 398.
[92] F. Hoﬀa, “1.7 billion reddit comments loaded on BigQuery,”
2016,
https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_
comments_loaded_on_bigquery/. As of 2021, the dataset proper is available
at https://console.cloud.google.com/bigquery?project=fh-bigquery.
[93] C. Cimpanu, “Reddit bans community dedicated to dark web mar-
kets,” Mar. 2018, https://www.bleepingcomputer.com/news/security/reddit-
bans-community-dedicated-to-dark-web-markets/.
[94] K. Soska and N. Christin, “Measuring the longitudinal evolution of the
online anonymous marketplace ecosystem,” in Proceedings of the 24th
USENIX Security Symposium, Washington, DC, Aug. 2015, pp. 33–48.
[95] Drug Enforcement Administration, “Slang terms and code words: A
reference for law enforcement personnel,” DEA Intelligence Report DEA-
HOU-DIR-022-18, 2018, available at https://www.dea.gov/sites/default/ﬁles/
2018-07/DIR-022-18.pdf.
[96] S. Zannettou, B. Bradlyn, E. De Cristofaro, H. Kwak, M. Sirivianos,
G. Stringini, and J. Blackburn, “What is gab: A bastion of free speech
or an alt-right echo chamber,” in Companion Proceedings of The Web
Conference, 2018, pp. 1007–1014.
[97] C. D. Manning, H. Schütze, and P. Raghavan, Introduction to information
retrieval. Cambridge university press, 2008.
[98] K. Järvelin and J. Kekäläinen, “Ir evaluation methods for retrieving
highly relevant documents,” in ACM SIGIR Forum, vol. 51, no. 2. ACM
New York, NY, USA, 2017, pp. 243–250.
[99] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
word representation,” in Proceedings of Empirical Methods in Natural
Language Processing (EMNLP), 2014, pp. 1532–1543.
[100] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal
representations by error propagation,” California Univ San Diego La
Jolla Inst for Cognitive Science, Tech. Rep., 1985.
[101] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” in
Proceedings of Empirical Methods in Natural Language Processing
(EMNLP), 2014, pp. 1746–1751.
[102] S. Lai, L. Xu, K. Liu, and J. Zhao, “Recurrent convolutional neural
networks for text classiﬁcation,” in Proceedings of 29th AAAI Conference
on Artiﬁcial Intelligence (AAAI), 2015.
[103] Z. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou, and
Y. Bengio, “A structured self-attentive sentence embedding,” arXiv
preprint arXiv:1703.03130, 2017.
Appendix
We present the euphemism detection results by our approach
in Table IX and analyze the false positive detection results on
the drug dataset in Table X. We categorize our false detection
results into four types:
• They are correct euphemisms but missed on the ground
truth list (cases 1-5 in Table X).
• They are not euphemisms by themselves, but they are
contained in euphemism phrases. For example, as shown
in case 6 in Table X, “oil” is not a drug euphemism while
“cbd oil” is one.
• Though they are not euphemisms, they are strongly related
to drug or the usage of drug (cases 7-10 in Table X).
Cases 7 and 8 uncovers some ways that people take drugs
(together with alcohol or cigarettes).
• Incorrect detection.
The case studies reveal that we can even ﬁnd some correct
euphemisms that are not on the ground truth list, which suggests
the rapid-evolving nature of euphemisms and the necessity of
the automatic euphemism detection task.
Euphemism detection results by our approach (better viewed in color). Purple bold words are correctly detected euphemisms and on the ground
truth list (i.e., the DEA list). The purple underlined words indicate that they are incorrect by themselves, but are contained in true euphemism
phrases, such as “dog food", “Chinese Tobacco" (euphemisms for “heroin" and “opium" respectively). Those words which do not appear in the ground
Table IX
truth list are marked black.
DrugDatasetSexualityWeapon{breast, genitals, sex, pornography, nipple, …}dick, head, brain, cock, face, hair, body, balls, ass, man, heart, heads, hands, white, family, hand, mouth, woman, children, life, child, name, baby, ﬁnger, wife, gun, neck, mind, nose, skin, shit, teeth, blood, money, sex, ﬁngers, blow, bodies, leg, one, private, legs, black, back, race, knife, soul, yes, brains, people, lives, son, daughter, throat, foot, red, feet, breasts, house, personality, tongue, country, bang, women, core, mother, job, point, suckcannon, bullet, ﬁnger, burner, phone, gat, ball, one, hand, weapons, shot, guns, trigger, needle, car, heater, fuck, bucket, fence, handgun, bat, chance, trap, pipe, sword, bag, door, hammer, camera, bar, blade, piece, cigarette, barrel, hit, point, dog, ﬂashlight, horse, house, revolver, fan, bomb, balloon, pill, brick, stick, lighter, key, ﬁre, joint, pack, grenade, line, ﬂag, blunt, box{carbine, gatling, gun, riﬂe, pistol, …}cannabis, weed, coke, alcohol, crack, speed, acid, pot, mushrooms, md, pills, hash, h, powder, tobacco, crystal, something, cigarettes, pressed, l, k, x, met, recovering, lean, spice, bud, narcotics, product, oil, grade, e, shatter, blow, anything, prescription, pill, research, heroine, shit, gold, use, psychedelic, hydro, white, medical, water, stuff, card, wax, substances, benz, products, fatal, fucking, addiction, sh, orange, new, coffee, sample, bars, others, sex, rc, smoking, lucy, blue, daily, money, pain, education, substance, coca, care, magnesium, tar, guns, everything, quality, treatment, peruvian, 2, legal, pure, mx, ir, synthetic, herb, amp, green, 4, medicine, chemicals, red, sleeping, possession, extract, depression, lithium{heroin,  ecstasy, marijuana, cocaine,   opium, …}Euphemism CandidatesTarget KeywordsCase studies of the false positive detection results on the drug dataset. They are real examples from Reddit.
Table X
recovering10•i have been completely abstinent from all drugs besides lsd for over 18 months and i am a recovering addict•im a recovering heroin addict so i know what that looks like at least for me the nod•are most kratom users regular users or recovering opiate addicts9•im primarily looking at their pressed pills you can grab 50 from them and it would cost you less than $250•domestic usa pressed mdma vendors•5 x 3mg xanax gg249 price 1499 including shipping product received 5 x gg249 3mg pressed replicas xanax bars all 5 intact no broken barspressedcigarettes8•i am not a regular smoker but the last few times i have done shrooms a cigarette would make the trip even more amazing•i was thinking ive heard of people dipping cigarettes in pcp to make it smokeabole could the same done with mxe•cigarettes felt amazing i felt so much love it was deﬁantly one of the best xtc pills i have takenalcohol•does anyone have successful experience making an alcohol alprazolam solution•its an interesting chemical and experience thats so completely different than alcohol or cannabis only two drugs ive used so far in all aspects•i left some speed dissolved in alcohol in a glass overnight•can i buy alcohol on the dark net•alcohol and xanax no effect7oil6•this hemp cbd oil gets me and my friends super high for about half an hour•my nephew had some cbd oil he bought at his dispensary and he gave me what he considered a large dose•tldr tried cbd oil in a headshop got really stoned and passed out•i have some excellent 70 kava honey oil extractmet5•so i have 500mg of 4acodmt and 250mg of met in my cart on lysergi•25inboh + ho met = lots of hallucinations pretty clean mindset•i preferred 4 ho met to 4 aco dmt since aco made me anxious when i snorted it•price value was good paid around aps26 the only person i know that sells l around here charges aps10 per 100ug digusting•really great for smoking at night i rolled a 2g l and it put me to sleep lmao•sparked an l of just blueberry kush and was so high i reanalyzed this entire review like 3 times while watching tv4lmushroomsSentences Associated1IDcannabis•can easily smash through 15 in a night id be a bit fucked up but the md high isnt as good anymore•the crystals do have a md smell though but just a bit overpowered by the weed smell•i mixed in 7g of red md with 3tsp of lemon juice and a dash of water and freezed it over night•so i took md last night not for the ﬁrst time and was thinking about how music sounds so much better when youre high•his main products included amphetamines ecstasy mushrooms and crystal meth•vendor review tripwithscience vial liquid mushrooms ~9mg of psilocybin•lsd mdma mushrooms especially ketamine and cocaine•im having trouble deciding whether to order 100 tabs of acid or 50 tabs and a half ounce of mushrooms2Euphemism Candidates3•hash is the most popular cannabis product that circulates the streets here•symptoms stop after cessation of cannabis use•im 18 and use lsd and cannabis often•smoking cannabis generates a large amount of unwanted side products of which carcinogenic compounds are the most dangerousmd