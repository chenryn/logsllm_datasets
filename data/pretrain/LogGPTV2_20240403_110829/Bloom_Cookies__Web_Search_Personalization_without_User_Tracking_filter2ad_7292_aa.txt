title:Bloom Cookies: Web Search Personalization without User Tracking
author:Nitesh Mor and
Oriana Riva and
Suman Nath and
John Kubiatowicz
Bloom Cookies: Web Search Personalization
without User Tracking
Nitesh Mor∗
Oriana Riva†
∗University of California, Berkeley
{mor@eecs,kubitron@cs}.berkeley.edu
Abstract—We propose Bloom cookies that encode a user’s
proﬁle in a compact and privacy-preserving way, without prevent-
ing online services from using it for personalization purposes.
The Bloom cookies design is inspired by our analysis of a large
set of web search logs that shows drawbacks of two proﬁle
obfuscation techniques, namely proﬁle generalization and noise
injection, today used by many privacy-preserving personaliza-
tion systems. We ﬁnd that proﬁle generalization signiﬁcantly
hurts personalization and fails to protect users from a server
linking user sessions over time. Noise injection can address
these problems, but only at the cost of a high communication
overhead and a noise dictionary generated by a trusted third
party. In contrast, Bloom cookies leverage Bloom ﬁlters as a
privacy-preserving data structure to provide a more convenient
privacy, personalization, and network efﬁciency tradeoff: they
provide similar (or better) personalization and privacy than
noise injection (and proﬁle generalization), but with an order
of magnitude lower communication cost and no noise dictionary.
We discuss how Bloom cookies can be used for personalized web
search, present an algorithm to automatically conﬁgure the noise
in Bloom cookies given a user’s privacy and personalization goals,
and evaluate their performance compared to the state-of-the-art.
I.
INTRODUCTION
Online services such as web search and advertising are
becoming increasingly personalized. The more and the longer a
service knows about an individual, the better personalization it
can provide. Typically, these online services build user proﬁles
(containing e.g., web sites frequently visited, user interests,
demographics information) on the server side by tracking
multiple online activities from the same user and linking
them together using various techniques, usually under poorly
informed user consent. In the face of privacy-concerned users
and stricter privacy regulations, a search engine that provides
personalized results while maintaining privacy has a deﬁnite
competitive edge over other search engines. In this paper, we
study how to achieve personalization while minimizing the
risk of being successfully tracked by an online service, and
we propose a solution called Bloom cookies for encoding a
user’s proﬁle in an efﬁcient and privacy-preserving manner.
The simplest way to link a user’s online activities is to use
Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’15, 8-11 February 2015, San Diego, CA, USA
Copyright 2015 Internet Society, ISBN 1-891562-38-X
http://dx.doi.org/10.14722/ndss.2015.23108
Suman Nath†
John Kubiatowicz∗
†Microsoft Research, Redmond
{oriana.riva,suman.nath}@microsoft.com
the IP address of his device. However, as a device’s IP address
can change over time, online services track users across their IP
sessions using cookies, device ﬁngerprinting [32], and browser
plug-ins (e.g., Google toolbar), to name a few. To limit such
tracking, users can hide IP addresses by using techniques such
as proxies and anonymity networks [33], onion routing [22],
or TOR [16]. They can also disable web cookies, and browse
in private mode [2] to prevent tracking by cookies. However,
a fundamental problem with all these approaches is that they
deny personalization because services do not have access to
the information necessary for building user proﬁles anymore.
Although privacy and personalization are at odds, they are
not mutually exclusive. For example, it is possible to maintain
user proﬁles at the client and carry out personalization there, to
the extent possible (e.g., [20], [23], [27], [47]); in this way, lit-
tle or nothing is disclosed to the server. However, a pure client-
side approach has serious drawbacks that make it infeasible in
a real system. First, without any information about the user,
the server needs to send all or a large number of results to the
client for local personalization. The communication overhead
can be prohibitive for many platforms such as mobile devices.
Second, and most importantly, it requires the service to put its
proprietary personalization algorithms on the client, which is
often unacceptable.
To address these challenges, existing systems such as
Privad [23] use two techniques. First, personalization is done
by the server or by a personalization proxy and not on the
client. The personalization proxy is, in general, not trusted by
the client. Second, because the client does not trust the party
providing personalization, it sends limited information about
the user proﬁle (e.g., high-level interests) with its request, so
that the proxy (or server) can ﬁlter out results irrelevant to
the user or can partially personalize the results. Hence, a key
requirement of these systems is to properly obfuscate the user
proﬁles before sending them out.
In this paper, we investigate practical techniques to obfus-
cate a user’s proﬁle in a way that preserves user privacy and yet
allows the server (or a personalization proxy) to personalize
results in a useful manner. We start with two well-known
techniques for proﬁle obfuscation: generalization [43] that
shares items in a user’s proﬁle only at a coarse granularity
(e.g., category of frequently visited web sites, instead of actual
URLs), and noise addition [5] which adds fake items to the
proﬁle to hide the real items.
A key contribution of this paper is to systematically
investigate privacy-personalization tradeoffs of such proﬁle
obfuscation techniques in the context of web search. We
use search logs from a popular search engine to quantify
the tradeoffs. We ﬁnd that noise addition provides a better
privacy-personalization tradeoff than generalization. This is in
contrast to existing systems such as Privad, Adnostic [47] and
RePriv [20] that advocate for using generalized proﬁles to
protect users’ privacy. Interestingly, even though generalized
proﬁles provide anonymity, this does not naturally translate
into unlinkability over time. If a server is able to identify
whether two requests are coming from the same or different
clients (linkability), it can collect enough information to iden-
tify the user over time. On a random subset of 1300 users in
our search log, even when only a user’s high-level interests are
disclosed, it is possible to link a user’s searches across time
in 44% of the cases.1
The superior performance of noisy proﬁles, however,
comes at two costs. Depending on how much noise is added
to the proﬁle, a noisy proﬁle can be very large and hence can
impose a large communication overhead. Our evaluation shows
that to achieve reasonable privacy and personalization, we had
to add up to tens of kB of noise per request. Moreover, the
noise needs to be generated by using a large noise dictionary
usually provided by a trusted third party.
To address these issues, our ﬁnal contribution is to propose
Bloom cookies, a noisy proﬁle based on Bloom ﬁlters [9]2
that is signiﬁcantly smaller (comparable to the size of today’s
web cookies) and that does not require a noise dictionary.
A Bloom cookie is generated and maintained by the client
device and is sent to online services every time the user makes
a service request. An online service can use the cookie to
deliver personalized results. Thus, Bloom cookies can replace
traditional cookies (i.e., the user can disable third party cookies
in his browser), with the possibility of the user controlling what
proﬁle information is included in the Bloom cookie and when
the cookie is sent to which online service.
Besides explicitly injecting noisy bits into the Bloom ﬁlter,
we exploit the false positives naturally occurring in it as noise
to provide privacy. We also provide an algorithm that, given
a user’s privacy and personalization goals, can automatically
conﬁgure a Bloom cookie’s parameters. Note that Bloom
cookies leverage Bloom ﬁlters as a privacy-preserving data
structure, in contrast to almost all previous work that adopted
Bloom ﬁlters for network and storage efﬁciency reasons [10],
[39]. To the best of our knowledge, we are the ﬁrst to use
Bloom ﬁlters for a practical privacy mechanism and evaluate
their privacy-personalization tradeoff.
Our results show that Bloom cookies provide a more
convenient privacy, personalization, and network efﬁciency
tradeoff. For example, Bloom cookies can provide comparable
unlinkability to state-of-the-art noise addition techniques with
a 50% improvement in personalization, or up to 12× less
network overhead (2 kbit of Bloom cookies compared to
25 kbit of noisy proﬁles generated with state-of-the-art noise
addition techniques).
1With a larger user population unlinkability increases, but in our evaluation
we show through projection that it is still signiﬁcant.
2A Bloom ﬁlter is a space-efﬁcient probabilistic data structure used to store a
set of elements and support membership queries. When querying if an element
exists in the Bloom ﬁlter, false positives are possible but false negatives are
not.
The rest of the paper is organized as follows. In §II, we
deﬁne our problem space, goals and threat model, and intro-
duce three key design questions we will answer throughout the
paper. §III gives background information on web search, and
deﬁnes our personalization and privacy (unlinkability) metrics.
The second part of the paper answers the three design questions
previously stated by showing the limitations of state-of-the-art
techniques (§IV) and proposing Bloom cookies as a solution
(§V). We review related work in §VI, discuss the limitations
of our work in §VII, and conclude in §VIII.
II. PRIVACY AND PERSONALIZATION IN WEB SEARCH
Our ﬁrst goal is to understand how various design choices
affect personalization and privacy in real systems. This un-
derstanding can help in better design of privacy-preserving
personalization for many applications. To be concrete, we
keep our discussion limited to web search, which we chose
for three main reasons. First, search engines like Google
and Bing are among the most visited web sites and users
are concerned about how these services implement person-
alization [34]. Second, most search queries are short [25],
[38] and ambiguous [15], [29], [40], and personalization can
help disambiguating the queries towards an individual user’s
interests. Third, we had logs from a popular search engine
available, making a practical analysis possible.
Like previous privacy-preserving personalization sys-
tems [20], we assume a generic client-server model. Each
is associated with a proﬁle that captures the user’s
client
general preferences and is represented as a bag of proﬁle items
such as interest categories or URLs of web sites he frequently
visits. Proﬁles are usually constructed using users’ search
history, but they could also leverage demographic information,
web browsing history or social network interactions, for even
richer user models. In processing a query from the client, the
server utilizes the user’s proﬁle to personalize search results
for him.
A. Personalization and privacy goals
Personalization. Personalization in web search refers to rank-
ing search results such that higher-ranked results are more
likely to be clicked by the user than lower-ranked results. The
server can use existing techniques, such as tailoring search
queries to a user’s interests or re-ranking search results based
on web sites the user visits most frequently, in order to push
likely-to-be-clicked results towards the top of the result list.
Privacy. We assume that the client does not trust the server
with his proﬁle. Exposing the exact proﬁle to the server
may leak a user’s identity and hence for privacy, the client
obfuscates his proﬁle before sending it
to the server. We
consider unlinkability as our key privacy measure. The precise
deﬁnition of unlinkability will be given in the next section;
intuitively, it ensures that the server cannot identify if two
queries are coming from the same client or different clients.
Like previous work [5], we consider achieving unlinkability of
a client’s proﬁle by obfuscating it with noise, i.e., by adding
fake items in the proﬁle, before sending it to the server.
2
B. Threat model
We aim for unlinkability across IP-sessions, where an IP-
session is a sequence of all queries with the same source IP
address. We do not assume techniques for hiding a device’s
IP address (proxies and anonymity networks [33], onion rout-
ing [22], or TOR [16]) are available, because they require
changes to the network infrastructure thus are not always
practical. These techniques are orthogonal to Bloom cookies
and can further increase a user’s privacy. In our scenario, the
search engine sees the IP address the search queries are coming
from. Thus, our goal is to thwart a malicious server’s attempts
to correlate queries from different IP-sessions to ﬁnd if they
are associated with the same user.
Unlinkability across IP-sessions is a useful privacy goal
since IP-sessions are typically short (of the order of a few
weeks) in practice. For instance, a smartphone’s IP address
changes relatively often, basically each time there is no net-
work activity and the radio is turned off [4]. In a home network,
IP addresses change less frequently, depending on the type
of provider and network contract. Bhagwan et al. [7] probed
1,468 unique peer-to-peer ﬁle sharing hosts over a period of
7 days and found that more than 50% used more than one
IP address. Casado and Freedman [11] report seeing 30% of
537,790 email clients using more than one IP address in a 2-
week period. According to [14], in June 2008 US machines
used 5.7 distinct IP addresses in a month. Finally, another
study [48] reports dynamic IPs with a volatility of 1 to 7 days,
with 30% of the IP addresses changing between 1 to 3 days.
In a corporate network, IP addresses may remain the same for
longer, but network trafﬁc from multiple devices is aggregated
under the same IP address thus making user identiﬁcation
hard.3 In general, the shorter the IP-session, the harder for the
server to link different sessions. In our analysis, we assume
2-week long IP-sessions to emulate an average home network
scenario based on existing studies [7], [11], [14], [48], and
given our 2-month long search logs.
We assume that users’ web browsers are conﬁgured in a
way that prevents online services from tracking them through
cookies, browser ﬁngerprinting, browser plug-ins, or similar
techniques. The browser (or the underlying system) keeps track
of the user’s online activities (e.g., search queries, visited sites)
and maintains a proﬁle that reﬂects the user’s interests. The
proﬁle is not directly shared with any online service; instead
it is encoded as a Bloom cookie and is sent with each search
query to the server. As we later show, Bloom cookies are
efﬁcient and privacy-preserving, and yet allow the server to
personalize results.
A server might launch correlation attacks based on the con-
tent of the search queries, or other meta-information associated
with the queries (e.g., time of the search query, frequency,
location or language). We indirectly factor the effect of such
correlations in the size of our user population. A search engine
potentially has billions of users, but a malicious search engine
which is trying to link the different IP-sessions belonging to
a single user together, can use this extra information to group
search sessions into smaller clusters. A simple example is to
3The IP address stays the same but source ports change with every new
outgoing connection. This is similar to the smartphone case where devices get
a new IP address every time the radio wakes up.
use IP geolocation to put all the IP-sessions from a small
town into one cluster. The smaller the clusters, the easier it
is to link users together. In our evaluation, we use a set of
1000 users, which we believe is large enough to smoothen
any of the outlier users and small enough to have a realistic
and compelling use case.
Finally, we assume the server has access only to infor-
mation collected though its own service (i.e., search requests
submitted to the search engine). We assume the server is
not colluding with other sources, such as other services (e.g.,
email, social networks) or third party trackers.
C. Key design questions
Designing a privacy-preserving personalized search engine
involves many important design choices. We now discuss some
important questions these choices pose. Later in the paper we
answer these questions by analyzing real search logs (§IV)
and show how the ﬁndings can be utilized to enable practical,
privacy-preserving, and personalized web search (§V).
Proﬁle obfuscation mechanisms. An important design deci-
sion is how a client’s proﬁle is obfuscated so that the server can
still ﬁnd it useful for personalization, but cannot link proﬁles
from the same user. Existing solutions for privacy-preserving
web search can be classiﬁed into two categories:
•