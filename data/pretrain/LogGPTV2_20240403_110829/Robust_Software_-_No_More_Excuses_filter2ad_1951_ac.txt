e
e m _
a l u
s
o
s t
e m _ tr y
p
s
w
a it
e m _
s
a it
w
Mem. Manipulation Module
Process Synch. Module
Figure 6. Before and after robustness of LINUX functions selected for treatment. After
treatment, no function had detectable robustness failures.
phore locks. As with the iterative testing of the memory
functions, this yields an extremely pessimistic value for
overhead. Any application that performs non-trivial com-
putation within a critical section (protected by semaphores)
will experience much less relative overhead. Note that per-
formance does begin to degrade due to conflict misses as
the number of objects contending for cache entries ap-
proaches the cache size.
5. Analysis
All robustness failures detectable by the Ballista testing
harness in the three test systems could be removed. It can-
not be concluded that the systems are now completely ro-
bust, because Ballista does not
test every possible
robustness failure. However, the tests used attained broad
coverage by testing combinations of basic data type attrib-
utes,
Furthermore, if new failures become detectable, they can be
fixed using the standard techniques outlined here at low
performance cost.
so they attained reasonably good coverage.
While there is some variation in performance penalty,
only the most heavily optimizable functions running on
short data sets exhibit large slowdowns. Absolute overhead
Slowdown of robust memory functions with
enhanced malloc
memchr
memcpy
memcmp
memset
memmove
3 2
6 4
1 2 8
2 5 6
5 1 2
Buffer size (Bytes)
1 0 2 4
2 0 4 8
4 0 9 6
in an
have been left
unhardened state. Figure
6 shows the before and
after
robust-
ness of the functions se-
lected for hardening.
treatment
0
0
h r
a
F
16.4
m e m c
m e m c
m e m c m p
Table 1 shows the av-
erage actual overhead in-
curred for making the
functions robust in nano-
seconds. Although the
most common case is a
hit in the cache reducing
the checks to an index
lookup and a jump, the
absolute overheads do
vary. This is due to dif-
ferences in how well the
microprocessor can ex-
ploit the instruction level
parallelism in the treated
function. Memchr() is highly optimizable, and makes more
complete use of processor resources. Because of this the
processor has only a few unused resources with which to
execute the exception checking code in parallel with useful
computation, and the paid overhead is slightly higher.
The average performance penalties for iterative calls to
the treated functions are located in Figures 7 and 8. Over-
head for process synchronization is given in Figure 9, as
measured by a synthetic application benchmark that simply
obtained semaphore locks, enqueued an integer value,
dequeued an integer value, and released the locks. No pro-
cessing occurred other than the single enqueue/dequeue op-
eration.
In the worst case, two of the memory functions exhibit
relatively large slowdowns for small buffer operand sizes.
These two functions, memchr() and memset() are highly
optimizable and leave fewer unused processor resources
than typical functions do. Although the overhead is 19%
and 11% respectively, it represents less than 25 nanosec-
onds. Sem_getvalue, which consists of a single load in-
struction suffers the worst penalty of 37%.
Although it may seem untoward to benchmark memory
functions iteratively, it is logical considering that the mem-
ory functions are often used to move data in
isolation from other computation. As such, it
made the most sense to test them in isolation
(iteratively) to obtain the worst case overhead
performance data. Used in combination with
other instructions and algorithms, the cost
will be less as the overhead is amortized over
larger amounts of processing.
n
w
o
d
w
o
S
l
the functions
In the case of process synchronization
however,
are always used
within the scope of some larger computa-
tional module. For this reason we measured
their performance within the context of a triv-
ial synthetic application. Only the most basic
computation occurs inside of the program-
ming structures that obtain and release sema-
1.25
1.2
1.15
1.1
1.05
1
0.95
1 6
Figure 7. Performance of hardened memory functions for varying
input sizes.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:55 UTC from IEEE Xplore.  Restrictions apply. 
Slowdown of Process Synchronization
Functions
n
w
o
d
w
o
S
l
1.5
1.4
1.3
1.2
1.1
1
0.9
e m _ i n it
e m _
s
s
e
d
y
o
e m _
t r
s
s
e
a l u
t
s
o
p
s
e m _
s
e t v
g
e m _ w a it
e m _ t r
s
y w a it
Figure 8. Performance of hardened process synchronization
functions.
was measured at 25 nanoseconds or less on a dual pIII 600
system. Nearly all of the hardened functions suffer perfor-
mance penalties of less than 5%, and most were less than
2%.
The overhead that is incurred is reduced to (in the case of
a robustness check cache hit) that of executing a few integer
instructions to calculate the cache index, a load, a compare,
and a branch. This small number of instructions usually
represents only a small fraction of the running time of the
protected code, thus yielding high performance. Even in
the case of short, speed critical functions much of the la-
tency of the cache lookup is hidden through hardware ex-
ploitable parallelism.
In the event of a cache miss, the full suite of checks must
be performed, and as indicated in Figure 9, this cost can be
substantial. Of course, these penalties can be minimized
through optimizing both the size of the cache and the num-
ber of caches on a per-application basis. It is also possible
to use multiple caches, providing a developer the opportu-
nity to finely tune the operation of the system.
It is notable that even functions that consist of only a few
hardware instructions can be hardened with a relatively
small speed penalty. It is at first counterintuitive that dou-
bling (in some cases) the number of instructions results in
only a 20% speed reduction. This result is easily explained
through the realization that although modern processors
can extract a great deal of instruction level parallelism from
some of these functions, there is still a fair amount of un-
used processor resources available for concurrent use by
the exception detection code.
synthetic
application
benchmarks.
This data shows that a range of speed critical ser-
vices/functions can be enhanced to be extremely robust
with a conservatively attainable speed penalty of <5% for
the
Iterative
benchmarks show worst case slowdowns in pessimistic
scenarios of 37%, but usually <5%. While it is not clear
what an “average” case is, the synthetic benchmarks show
that even the most lightweight application penalties ap-
proach the lower worst case bound. A software system that
performs any non-trivial computation will likely see a near
zero overhead.
The actual overhead (in nanoseconds) was determined
for the Linux functions treated, and can be found in Table 1.
Overall the average absolute overhead was 9 nanoseconds
for the process synchronization functions and 8
nanoseconds for the memory functions using a small
cache size. Small cache synthetic application over-
head was an average of 20 ns (for a full cache). For a
large cache, the overhead increases to an average of
50 ns for a 1/4 full cache, increasing to 90 ns for a
cache at capacity. The base overhead increase is due
to the necessity of adding a more complex indexing
scheme capable of indexing arbitrarily sized caches.
Note that the measured overhead for the synthetic
benchmark actually includes the overhead of a pair
of calls, one to sem_wait(), and one to sem_post().
This overhead is on the order of about 10-25 cy-
cles per protected call, and is representative of not
only the index calculation and the branch resolution,
but also the wasted fetch bandwidth. The micropro-
cessor used on the test platform is incapable of fetching past
branches, thus even correctly predicting the branch induces
some penalty.
Advances in architecture such as the block cache [2],
multiple branch prediction [32] and branch predication can
be expected to effectively reduce exception checking over-
head to near zero. The robustness checks will be performed
completely in parallel with useful computation, and predi-
cated out. Fetch bandwidth will be preserved by the block
cache and the multiple branch predictors. Thus only code
with the highest degree of parallelism that utilizes 100% of
the hardware resources will likely have any drop off in per-
formance. This level of parallelism is seldom seen, and
usually occurs only in tightly optimized loops of computa-
tional algorithms. Of course code sections such as those
only need robustness checks upon method entry, and per-
Slowdown vs Number of
Semaphore Objects, cache
size=4096 entries(16KB)
4
1 6
6 4
2 5 6
1 0 2 4
4 0 9 6
# Semaphores
1 6 1 9 2
n
w
o
d
w
o
S
l
10
8
6
4
2
0
1
Figure 9. Performance of cached hardening for
varying number of active semaphores.
haps not even then if the method is guaranteed to receive
known valid data.
6. Case Study
Robustness is not a concept addressed in many program-
ming, or software engineering classes [30]. Too often the
idea of testing software is linked to the idea that a system
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:55 UTC from IEEE Xplore.  Restrictions apply. 
Component A and B Robustness
e
r
u
l
i
a
F
t
r
o
b
A
)
%
(
e
t
a
R
25
20
15
10
5
0
Measured
Expected
A1 A2 A3 A4 B1 B2 B3 B4
Function
Figure 10. Measured vs. Expected robustness of
components A & B.
has been successfully tested when you can be reasonably
sure it provides the correct output for normal input. Unfor-
tunately, this philosophy overlooks the entire class of fail-
ures resultanting from exceptional inputs or conditions, and
exception detection and handling code tends to be the least
tested and least well understood part of the entire software
system [5].
While up to two-thirds of all system crashes can be
traced to improperly handled exceptional conditions [5],
the reason such failures occur is uncertain. Several possi-
bilities exist, including the classics of “Too hard” to check
for, “Too slow” to be robust, “That could never happen”,
“That was from a third party application”, and one of any
number of the usual litany of excuses. While some of those
possibilities have been discredited, others are a matter of
opinion and can probably never be resolved. But perhaps
there is another, more fundamental mechanism at work.
Simple human error is yet another possibility. The sys-
tem developer/designer may have intended to handle ex-
ceptions, and simply made a mistake. Almost all errors fall
into one of two categories - errors of commission and errors
of omission [34]. Thus the root cause of a robustness fail-
ure is often just that the exception checking and handling