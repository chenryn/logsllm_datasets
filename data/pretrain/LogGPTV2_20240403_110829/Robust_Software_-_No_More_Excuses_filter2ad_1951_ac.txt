### Analysis of Robustness Enhancements in LINUX Functions

#### 1. Introduction
Figure 6 illustrates the robustness of selected LINUX functions before and after treatment. Post-treatment, no function exhibited detectable robustness failures. The use of semaphore locks, similar to iterative testing of memory functions, yields a highly pessimistic value for overhead. Applications performing non-trivial computations within critical sections (protected by semaphores) will experience significantly less relative overhead. Performance degradation due to conflict misses occurs as the number of objects contending for cache entries approaches the cache size.

#### 2. Robustness Testing
All robustness failures detectable by the Ballista testing harness in the three test systems were eliminated. Although Ballista does not cover every possible robustness failure, the tests used achieved broad coverage by testing combinations of basic data type attributes. If new failures become detectable, they can be fixed using the outlined standard techniques at a low performance cost.

#### 3. Performance Impact
There is some variation in the performance penalty, with only the most heavily optimizable functions running on short data sets showing significant slowdowns. Table 1 shows the average actual overhead incurred in making the functions robust, measured in nanoseconds. The most common case is a hit in the cache, reducing checks to an index lookup and a jump. However, absolute overheads vary due to differences in how well the microprocessor can exploit instruction-level parallelism in the treated function.

For example, `memchr()` is highly optimizable and makes more complete use of processor resources, leaving fewer unused resources for executing exception checking code in parallel. This results in slightly higher paid overhead.

#### 4. Iterative Function Performance
Figures 7 and 8 show the average performance penalties for iterative calls to the treated functions. Overhead for process synchronization, as measured by a synthetic application benchmark, is shown in Figure 9. This benchmark simply obtained semaphore locks, enqueued an integer value, dequeued an integer value, and released the locks, with no other processing occurring.

In the worst case, two memory functions, `memchr()` and `memset()`, exhibit relatively large slowdowns for small buffer sizes. These functions are highly optimizable and leave fewer unused processor resources. The overhead for these functions is 19% and 11%, respectively, representing less than 25 nanoseconds. `sem_getvalue`, which consists of a single load instruction, suffers the worst penalty of 37%.

Iteratively benchmarking memory functions is logical because they are often used to move data in isolation from other computations. Therefore, testing them in isolation provides the worst-case overhead performance data. When used in combination with other instructions and algorithms, the cost will be less as the overhead is amortized over larger amounts of processing.

#### 5. Process Synchronization
Process synchronization functions are always used within the scope of some larger computational module. For this reason, their performance was measured within the context of a trivial synthetic application. The overhead was measured at 25 nanoseconds or less on a dual pIII 600 system. Nearly all hardened functions suffer performance penalties of less than 5%, with most being less than 2%.

The overhead incurred is reduced to a few integer instructions for calculating the cache index, a load, a compare, and a branch. In the event of a cache miss, the full suite of checks must be performed, which can be substantial. Penalties can be minimized by optimizing the cache size and number of caches per application. Using multiple caches allows developers to finely tune system operation.

#### 6. Case Study: Robustness in Software Engineering
Robustness is often overlooked in programming and software engineering classes. Testing software is often linked to ensuring correct output for normal input, neglecting exceptional inputs or conditions. Exception detection and handling code is frequently the least tested and least understood part of the system. Up to two-thirds of system crashes can be traced to improperly handled exceptional conditions, with reasons including human error, complexity, and performance concerns.

### Conclusion
The robustness enhancements applied to LINUX functions effectively eliminate detectable robustness failures with minimal performance overhead. While some functions experience significant slowdowns in worst-case scenarios, the overall impact is generally less than 5%. Advances in architecture, such as block caching and multiple branch prediction, can further reduce the overhead of robustness checks, making it feasible to enhance the robustness of speed-critical services with minimal performance penalties.