by the steady results over the different iterations. We thus have no
indication of a flawed internal validity of the experiment.
A more difficult question to answer is whether the experiment
is externally valid, i.e., if its conclusion can be generalized and
transferred to the real world. We have insights into large enter-
prise networks with tens of thousands of systems that utilize the
same components as our scenario (Windows 10, Sysmon, Sigma,
and Suricata). Even though we could not perform the exact same
experiment in a productive network due to the risk that comes
with the involved vulnerability scanning and malware execution,
we are still convinced that it is indeed externally valid because all
entities (operating systems, detection systems) are commonly used
in practice and would thus generate similar alerts. However, this
does not necessarily mean that performing our exemplary attack
against such a network would yield the exact same alerts, e.g., due to
different versions of detection rules. Nevertheless, the experiment
can serve as an indicator as to whether the analyzed configuration
change should be considered in an enterprise network.
Finally, we would like to note that external validity is not a
property of a testbed per se, as every testbed fails to recreate at
least some properties of real-world productive systems. Instead,
external validity has to be considered for each specific experiment
performed using a certain testbed. Consequently, we can also think
of experiments performed using SOCBED that would likely yield
invalid results, e.g., evaluating anomaly detection methods that
699Reproducible and Adaptable Log Data Generation for Sound Cybersecurity Experiments
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Furthermore, we also took a closer look at the log data “noise
floor”, i.e., data that are not (primarily) caused by the attacks. To give
an idea of these variations, we analyzed the number of occurrences
of each Windows event type (defined by the combination of provider
name and ID) over all iterations. Figure 5 shows the 20 most frequent
of the 138 total event types for the default configuration on the
two hosts (legend in Appendix C). The plot for the best practice
configuration (not shown) looks similar except for several Sysmon
events in the top 20 types (172 types in total). We can see significant
variations between runs for certain event types, but no striking
differences between the two hosts.
In conclusion, our analysis of the Suricata alerts and Windows
Event Log types shows that statistical variations between iterations
and hosts indeed occur and should be anticipated. Experiment
designers should keep this in mind and perform evaluations that are
robust to such intra- and inter-host variations. This fact once again
emphasizes the importance of soundly controlled and reproducible
experiments so that variations caused by uncontrolled variables or
non-deterministic activity can be ruled out in the analysis.
Summarizing the whole evaluation, our exemplary experiment of
detecting a common multi-step intrusion of an enterprise network
has shown that it is indeed possible to perform valid, controlled, and
reproducible cybersecurity experiments based on log data generated
with SOCBED, thus fostering research that can be built upon.
7 DISCUSSION AND LIMITATIONS
In this paper, we have proposed SOCBED, an open-source, virtual
machine (VM)-based testbed designed with reproducibility and
adaptability in mind that addresses several problems of current
approaches and enables researchers to conduct sound experiments.
However, every design decision also comes with potential limita-
tions. In the following, based on our experiences while designing,
implementing, and evaluating SOCBED, we share lessons learned,
discuss trade-offs, and identify further use cases for SOCBED.
To begin with, emulating a real-world scenario may imply trade-
offs with regard to reproducibility. For example, most operating
systems regularly check for updates and some even download them
automatically, thus making log data and network traffic depend on
the time of day and the availability of updates. If the reproducibility
of an experiment is impeded by such variations, Internet access
should be disabled (but otherwise enabled for better realism).
Another requirement for reproducibility is the execution of deter-
ministic activity. Yet, some experiments might comprise activity of a
human adversary or user and depend on their exact timing (e.g., for
anomaly detection). In this case, we recommend to record the activ-
ity and replay it using a script to ensure reproducibility. Likewise,
for certain experiments a strong degree of determinism in network
activity might be required, e.g., for an evaluation of time-based SQL
attacks. To this end, SOCBED already provides the infrastructure to
retrieve websites from within the simulated network to strengthen
reproducibility (cf. Section 5.4). If an experiment requires an even
higher degree of determinism in network activity, SOCBED’s mod-
ular approach allows to extend it with a man-in-the-middle proxy
(e.g., mitmproxy [38]), including the capability of intercepting TLS
encrypted communication, to deterministically replay previously
recorded network traffic.
Figure 5: In contrast to the consistent log data caused by
the attacks, the “noise floor” of operating system logs shows
notable differences between the runs, even though VMs are
started from the same snapshots and all activity is scripted.
This is common operating system behavior and must be con-
sidered when designing experiments.
require a huge variety in user activity to function properly. How-
ever, a notable advantage of SOCBED over fixed datasets is that
researchers can adapt individual parts of the testbed to make their
experiments valid.
6.6 Deep Dive: Variations in Log Data
While we have shown that our exemplary experiment is repro-
ducible, this does not imply that each iteration of SOCBED gener-
ates the exact same set of log data. Various factors such as back-
ground processes and time-dependent tasks influence the generated
log data, resulting in variations between iterations and, possibly,
also host computers [27, 57]. This is an unavoidable trade-off be-
tween realism and replicability when generating log data using
real systems. In the following, we analyze variations in the log
data generated during our experiment in more detail and particu-
larly quantify differences between the two hosts. To the best of our
knowledge, this important issue has never been examined before.
Table 2 shows that attack steps (1) and (3) exhibit variations in
the number of alerts. These variations are caused by a different
number of occurrence of two Suricata alerts (cf. Appendix C). The
first one is raised during the SQL injection attack and the second
one shortly after execution of the email attachment, when the
shellcode is downloaded [40]. We were able to manually reproduce
these differences on both hosts, so they indeed occur and are not a
flaw of the analysis process. Still, we wanted to check whether the
mean number of alerts differs between the two hosts. To do this, we
performed a two-tailed two-sample unpaired Welch’s t-test [36, 62]
for the two differing alerts, which required us to run additional
iterations of the triggering attack steps to obtain significant (α =
0.05) results. Indeed, we found that for both alerts, we had to reject
the test’s null hypothesis that the mean number is equal on both
hosts (p-values 0.000392 and 0.00251, respectively). We suppose
that the variations were caused by slight performance differences
between iterations and hosts that resulted in a different rate of
dropped packets (Suricata reported drop rates of 0.001-0.3 %).
1234567891011121314151617181920Windows event types (sorted by total occurrences)101102103events per iteration (n=10)Host 1Host 2700ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Rafael Uetz, Christian Hemminghaus, Louis Hackländer, Philipp Schlipper, and Martin Henze
From a different perspective, while the open-source infrastructure-
as-code setup enables complete transparency and adaptability of a
testbed, it comes with the challenge of occasionally disappearing
software download links. We experienced a few cases where our
automatic daily builds of SOCBED broke because software reposito-
ries or URLs for downloading operating system images changed and
had to be updated in the SOCBED code. This might especially be an
issue when reproducing testbed versions that are several years old.
We therefore recommend to keep local copies of all downloaded
software and/or VM images if updating versions could impede the
conducted experiments.
Another trade-off of SOCBED results from the fact that it runs
on commodity hardware and uses virtual machines. In contrast to
simulations, VM-based testbeds run in realtime and may behave
slightly different depending on the host’s soft- and hardware (just
as physical systems). Our evaluation showed that even similar hosts
may lead to slight variations in generated log data. We thus suggest
to avoid running a testbed on hosts with scarce resources or back-
ground activity and to closely monitor indicators of performance
issues during experiments to avoid uncontrolled behavior.
From a similar perspective, SOCBED focuses on scenarios with
bounded scalability requirements to be able to provide a high level
of detail when emulating systems (i.e., full OS emulation), as re-
quired for realistic log generation. Here, SOCBED’s scalability is
primarily influenced by the number of virtual machines, not by
the complexity of the underlying network topology. Given this
design trade-off, very large-scale simulations requiring less realistic
emulation but striving for complex scenarios with thousands of
systems are out of scope for SOCBED. In such scenarios, approaches
producing large amounts of fake log data (e.g., flog [14]) might be a
better fit than SOCBED. For the scope of this paper, we deliberately
chose a small scenario with only few emulated systems which can
be executed on commodity hardware to ease reproducibility. Still,
outside the scope of this paper, we successfully scaled SOCBED to
execute experiments with more than a hundred realistically emu-
lated systems using a proprietary hypervisor running on dedicated
hardware (VMware ESXi).
Finally, built-in self-tests demand additional effort during devel-
opment. Yet, we found them to be extremely valuable for discover-
ing errors when installing SOCBED on a new host or adapting it
on an existing host. Tests are an established best practice in soft-
ware development [33] and we also strongly recommend to use and
maintain them when using SOCBED or developing other testbeds.
8 CONCLUSION
Various fields of cybersecurity research base their evaluations on
artifacts (e.g., log data or network traffic) that are either not publicly
available or are generated using proprietary testbeds, thus heavily
restricting reproducibility of their findings. Furthermore, other
researchers struggle to build on existing work because they cannot
adapt existing artifact datasets for their purposes, e.g, by re-running
a scenario with different attacks, other software versions, or a
changed logging configuration. Likewise, fixed datasets can lead to
invalid conclusions as researchers using them might not be able to
assess the appropriateness of a dataset for their own use cases due
to a lack of transparency in artifact generation.
To address this issue, in this work, we started by deriving re-
quirements for generating artifacts for cybersecurity experiments
that are realistic, transparent, adaptable, replicable, and publicly
available. Based on these requirements, we argued that artifact gen-
eration for scientific experiments should be performed with testbeds
that are specifically designed with a focus on reproducibility and
adaptability. As a proof-of-concept implementation, we presented
SOCBED, an open-source testbed specifically targeting the genera-
tion of realistic log data for cybersecurity experiments that runs on
commodity hardware. To the best of our knowledge, SOCBED is the
first testbed for log data generation that is specifically designed to
foster reproducibility and adaptability, which is achieved through
measures such as infrastructure as code, deterministic activity, and
comprehensive self-tests.
To evaluate the reproducibility and adaptability of log data gen-
erated by SOCBED, we performed an exemplary, practical experi-
ment from the domain of intrusion detection and showed that, even
though log data naturally exhibit some variation between runs, the
experiment itself is reproducible on different computers and adapta-
tions can be performed in a controlled way. We make the evaluation
scripts and generated log dataset publicly available [58, 66], thus
enabling others to fully reproduce our experiment.
In conclusion, our work paves the way for better reproducibility
in cybersecurity research, especially in the area of log data and in-
trusion detection research, and consequently increases the potential
to build future research efforts on existing work.
ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers and our shepherd
Evangelos Markatos for their valuable feedback and fruitful com-
ments. This work was supported by the German Federal Ministry
of Education and Research (BMBF) under grant no. 16KIS0342. The
authors of this paper are responsible for its content.
REFERENCES
[1] Andy Applebaum, Doug Miller, Blake Strom, Chris Korban, and Ross Wolf. 2016.
Intelligent, automated red team emulation. In Proceedings of the 32nd Annual
Conference on Computer Security Applications. ACM, 363–373.
[2] Australian Cyber Security Centre. 2020. Windows Event Logging and Forwarding.
Retrieved June 28, 2021 from https://www.cyber.gov.au/acsc/view-all-content/
publications/windows-event-logging-and-forwarding
[3] Vaibhav Bajpai, Anna Brunstrom, Anja Feldmann, Wolfgang Kellerer, Aiko Pras,
Henning Schulzrinne, Georgios Smaragdakis, Matthias Wählisch, and Klaus
Wehrle. 2019. The Dagstuhl Beginners Guide to Reproducibility for Experimental
Networking Research. SIGCOMM Comput. Commun. Rev. 49, 1 (2019), 24–30.
[4] Emilie Lundin Barse and Erland Jonsson. 2004. Extracting attack manifestations
to determine log data requirements for intrusion detection. In Proceedings of the
20th Annual Computer Security Applications Conference. ACM, 158–167.
[5] Terry Benzel. 2011. The science of cyber security experimentation: the DE-
TER project. In Proceedings of the 27th Annual Computer Security Applications
Conference. ACM, 137–148.
[6] Sandeep Bhatt, Pratyusa K Manadhata, and Loai Zomlot. 2014. The operational
role of security information and event management systems. IEEE Security &
Privacy 12, 5 (2014), 35–41.
[7] Leyla Bilge and Tudor Dumitras. 2012. Before we knew it: an empirical study of
zero-day attacks in the real world. In Proceedings of the 2012 ACM conference on
computer and communications security. ACM, 833–844.
[8] Stephen M. Blackburn, Amer Diwan, Matthias Hauswirth, Peter F. Sweeney,
José Nelson Amaral, Tim Brecht, Lubomír Bulej, Cliff Click, Lieven Eeckhout,
Sebastian Fischmeister, Daniel Frampton, Laurie J. Hendren, Michael Hind,
Antony L. Hosking, Richard E. Jones, Tomas Kalibera, Nathan Keynes, Nathaniel
Nystrom, and Andreas Zeller. 2016. The Truth, The Whole Truth, and Nothing
But the Truth: A Pragmatic Guide to Assessing Empirical Evaluations. ACM
Trans. Program. Lang. Syst. 38, 4 (2016). https://doi.org/10.1145/2983574
701Reproducible and Adaptable Log Data Generation for Sound Cybersecurity Experiments
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
[9] Tom Bowen, Alex Poylisher, Constantin Serban, Ritu Chadha, Cho-Yu Jason
Chiang, and Lisa M Marvel. 2016. Enabling reproducible cyber research – four
labeled datasets. In 2016 IEEE Military Communications Conference. IEEE, 539–
544.
[10] Ritu Chadha, Thomas Bowen, Cho-Yu J Chiang, Yitzchak M Gottlieb, Alex
Poylisher, Angello Sapello, Constantin Serban, Shridatt Sugrim, Gary Walther,
Lisa M Marvel, et al. 2016. CyberVAN: A Cyber security Virtual Assured Network
testbed. In 2016 IEEE Military Communications Conference. IEEE, 1125–1130.
[11] Anton Chuvakin, Kevin Schmidt, and Chris Phillips. 2012. Logging and log
management: the authoritative guide to understanding the concepts surrounding
logging and log management. Syngress.
[12] Jon Davis and Shane Magrath. 2013. A Survey of Cyber Ranges and Testbeds.
Technical Report. Cyber and Electronic Warfare Division, Defence Science and
Technology Organisation, Australian Government Department of Defence.
[13] Bernard Ferguson, Anne Tall, and Denise Olsen. 2014. National Cyber Range
overview. In 2014 IEEE Military Communications Conference. IEEE, 123–128.
[14] flog contributors. 2020. mingrammer/flog: A fake log generator for common log
formats. Retrieved September 8, 2021 from https://github.com/mingrammer/flog
[15] Ivo Friedberg, Florian Skopik, Giuseppe Settanni, and Roman Fiedler. 2015. Com-
bating advanced persistent threats: From network event correlation to incident
detection. Computers & Security 48 (2015), 35–57. https://doi.org/10.1016/j.cose.
2014.09.006
[16] Sean Gallagher. 2014. Inside the “wiper” malware that brought Sony Pictures to
its knees [Update]. Retrieved June 28, 2021 from http://arstechnica.com/security/
2014/12/inside-the-wiper-malware-that-brought-sony-pictures-to-its-knees/
[17] Dennis Grunewald, Marco Lützenberger, Joël Chinnow, Rainer Bye, Karsten
Bsufka, and Sahin Albayrak. 2011. Agent-based Network Security Simulation
(Demonstration). In 10th International Conference on Autonomous Agents and
Multiagent Systems (AMAS 2011). International Foundation for Autonomous
Agents and Multiagent Systems, 1325–1326.
[18] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, Bob Lantz, and Nick
McKeown. 2012. Reproducible network experiments using container-based emu-
lation. In Proceedings of the 8th International Conference on Emerging Networking
Experiments and Technologies. ACM, 253–264.
[19] Seth Hardy, Masashi Crete-Nishihata, Katharine Kleemola, Adam Senft, Byron
Sonne, Greg Wiseman, Phillipa Gill, and Ronald J Deibert. 2014. Targeted threat
index: Characterizing and quantifying politically-motivated targeted malware.
In Proceedings of the 23rd USENIX Security Symposium. USENIX, 527–541.
[20] Brad Heath, Heather Timmons, and Peter Cooney. 2021. SolarWinds hack was
’largest and most sophisticated attack’ ever: Microsoft president. Retrieved June
28, 2021 from https://www.reuters.com/article/us-cyber-solarwinds-microsoft-
idUSKBN2AF03R
[21] Brian Hepburn and Hanne Andersen. 2021. Scientific Method (Stanford Encyclo-
pedia of Philosophy). Retrieved June 28, 2021 from https://plato.stanford.edu/
entries/scientific-method/
[22] Eric M Hutchins, Michael J Cloppert, and Rohan M Amin. 2011. Intelligence-
driven computer network defense informed by analysis of adversary campaigns
and intrusion kill chains. Leading Issues in Information Warfare & Security Research
1 (2011), 80–106.
[23] Kaspersky. 2014. Energetic Bear – Crouching Yeti.
Retrieved June 28,
2021 from https://media.kasperskycontenthub.com/wp-content/uploads/sites/
43/2018/03/08080817/EB-YetiJuly2014-Public.pdf
[24] Karen Kent and Murugiah Souppaya. 2006. NIST Special Publication 800-92:
Guide to Computer Security Log Management. Retrieved June 28, 2021 from
https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-92.pdf
[25] Ansam Khraisat, Iqbal Gondal, Peter Vamplew, and Joarder Kamruzzaman. 2019.
Survey of Intrusion Detection Systems: Techniques, Datasets and Challenges.
Cybersecurity 2, 1 (2019). https://doi.org/10.1186/s42400-019-0038-7
[26] Vijay Kothari, Jim Blythe, Sean W. Smith, and Ross Koppel. 2015. Measuring the
Security Impacts of Password Policies Using Cognitive Behavioral Agent-based
Modeling. In Proceedings of the 2015 Symposium and Bootcamp on the Science of
Security (HotSoS ’15). ACM, 13:1–13:9.
[27] Max Landauer, Florian Skopik, Markus Wurzenberger, Wolfgang Hotwagner, and
Andreas Rauber. 2021. Have it Your Way: Generating Customized Log Datasets
With a Model-Driven Simulation Testbed. IEEE Transactions on Reliability 70, 1
(2021), 402–415.
[28] Max Landauer, Florian Skopik, Markus Wurzenberger, and Andreas Rauber.
2020. System log clustering approaches for cyber security applications: A survey.
Computers & Security 92 (2020), 101739.
[29] Elizabeth LeMay, Michael D Ford, Ken Keefe, William H Sanders, and Carol
Muehrcke. 2011. Model-based security metrics using ADversary VIew Secu-
rity Evaluation (ADVISE). In Eighth International Conference on Quantitative
Evaluation of Systems. IEEE, 191–200.
[30] Richard Lippmann, Joshua W. Haines, David J. Fried, Jonathan Korba, and Kumar
Das. 2000. Analysis and Results of the 1999 DARPA Off-Line Intrusion Detection
Evaluation. In Recent Advances in Intrusion Detection, Hervé Debar, Ludovic Mé,
and S. Felix Wu (Eds.). Springer Berlin Heidelberg, 162–182.
[31] Chris Long. 2021. Detection Lab. Retrieved June 28, 2021 from https://github.
[32] Martin Maisey. 2014. Moving to analysis-led cyber-security. Network Security
com/clong/DetectionLab
2014, 5 (2014), 5–12.
Pearson Education.
[33] Robert C Martin. 2009. Clean code: a handbook of agile software craftsmanship.
[34] McAfee, Inc. 2015. Grand Theft Data—Data exfiltration study: Actors, tactics, and
detection. Retrieved June 28, 2021 from https://www.mcafee.com/enterprise/en-
us/assets/reports/rp-data-exfiltration.pdf
[35] Rose McDermott. 2011. Internal and external validity. Cambridge handbook of
experimental political science (2011), 27–40.
[36] J.H. McDonald. 2014. Handbook of Biological Statistics (3rd ed.). Retrieved June
28, 2021 from http://www.biostathandbook.com/twosamplettest.html
[37] Microsoft. 2021. SimuLand: Understand adversary tradecraft and improve detec-
tion strategies. Retrieved June 28, 2021 from https://github.com/Azure/SimuLand
[38] mitmproxy contributors. 2021. mitmproxy/mitmproxy: An interactive TLS-
capable intercepting HTTP proxy for penetration testers and software developers.
Retrieved September 8, 2021 from https://github.com/mitmproxy/mitmproxy
[39] Douglas C. Montgomery. 2017. Design and Analysis of Experiments. Wiley.
[40] H. D. Moore. 2011. Meterpreter HTTP/HTTPS Communication.
Retrieved
June 28, 2021 from https://www.rapid7.com/blog/post/2011/06/29/meterpreter-
httphttps-communication/
[41] Stephen Moskal, Ben Wheeler, Derek Kreider, Michael E Kuhl, and Shanchieh Jay
Yang. 2014. Context model fusion for multistage network attack simulation. In
2014 IEEE Military Communications Conference. IEEE, 158–163.
[42] Nour Moustafa and Jill Slay. 2015. UNSW-NB15: a comprehensive data set for
network intrusion detection systems (UNSW-NB15 network data set). In 2015
Military Communications and Information Systems Conference (MilCIS). IEEE,
1–6.
[43] Pejman Najafi, Alexander Mühle, Wenzel Pünter, Feng Cheng, and Christoph
Meinel. 2019. MalRank: a measure of maliciousness in SIEM-based knowledge
graphs. In Proceedings of the 35th Annual Computer Security Applications Confer-
ence. ACM, 417–429.
[44] NetApplications.com. 2019. Operating System Market Share. Retrieved June 28,
2021 from https://netmarketshare.com/operating-system-market-share.aspx
[45] Open Information Security Foundation. [n.d.]. Suricata. Retrieved June 28, 2021
from https://suricata.io/
[46] Piotr Pauksztelo. 2014. Simulation of an Enterprise Network with Realistic User
Behavior. Master’s thesis. Institute of Computer Science, Universität Bonn.