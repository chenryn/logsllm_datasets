stores keyed hashes of all cells. Audit under EunomiaKH re-
quires the audit algorithm to track the provenance of the ci-
phertext (i.e., from which table, which column the ciphertext
originated) and is less eﬃcient than audit under EunomiaDET.
Displaced comparison (needed for Policy 2) is supported
using a new sub-scheme mOPED, which is described in
Section 4. Both EunomiaDET and EunomiaKH use mOPED.
Like its predecessor, mOPE [33], the scheme adds a addi-
tional search tree (additional metadata) to the encrypted
database on CS.
(Supporting displacements is necessary
for a practical audit system because privacy regulations use
displacements to express obligation deadlines. Out of 84
HIPAA privacy clauses, 7 use displacements. Cignet Health
of Prince George’s County, Maryland was ﬁned $1.3 million
for violating one of these clauses, §164.524 [29].
The encrypted database has a schema derived from the
schema of the plaintext database and may be stored on CS
using any standard database management systems (DBMS).
The DBMS may be used to index the encrypted cells. As
shown in [19], database indexing plays a key role in improv-
ing the eﬃciency of the audit process. Hence, we develop
our encryption scheme in such a way that it is possible to
leverage database indexing supported by commodity DBMS.
2.3 Policies and audit
Privacy policies may be extracted from privacy legisla-
tion like HIPAA [1] and GLBA [2], or from internal com-
pany requirements. Technically, a privacy policy speciﬁes a
constraint on the log. For example, Policy 1 of Section 1
requires that any name appearing in table T1 appears in ta-
ble T2 with role Doctor. Generally, policies can be complex
and may mention various entities, roles, time, and subjec-
tive beliefs. For instance, DeYoung et al.’s formalization of
the HIPAA and GLBA Privacy Rules span over 80 and 10
pages, respectively [18]. We represent policies as formulas of
ﬁrst-order logic (FOL) because we ﬁnd it technically conve-
nient and because FOL has been demonstrated in prior work
to be adequate for representing policies derived from exist-
ing privacy legislation (DeYoung et al., mentioned above,
use the same representation). We describe this logic-based
representation of policies in Section 3.
Our audit algorithm adapts our prior algorithm, reduce
[19], that works on policies represented in FOL. This algo-
rithm takes as input a policy and a log and reduces the pol-
icy by checking the policy constraints on the log. It outputs
constraints that cannot be checked due to lack of informa-
tion (missing log tables, references to future time points, or
need for human intervention) in the form of a residual policy.
Similar to reduce, our adapted algorithm, ereduce, uses
database queries as the basic building block. Our encryption
schemes permit queries with selection, projection, join, com-
parison and displaced comparison operations. Our schemes
do not support queries like aggregation (which would re-
quire an underlying homomorphic encryption scheme and
completely new security proofs).
To run reduce on EunomiaDET, we need to identify columns
that are tested for equality. This information is needed
prior to encryption for EunomiaDET and prior to audit for
EunomiaKH, as explained in Section 4. We develop a static
analysis of policies represented in FOL, which we call the
EQ mode check, deﬁned in Section 7, to determine which
columns may need to be compared for equality when the
policy is audited.
2.4 Adversary model and Security Goals
Assumptions and threat model. In our threat model, Cl
is trusted but CS is an honest but curious adversary with the
1132following capability: CS can run any polynomial time algo-
rithm on the stored (encrypted) log, including the audit over
any policy. We assume that Cl generates keys and encrypts
the log with our encryption schemes before uploading it to
CS. Audit runs on the CS infrastructure but (by design) it
does not perform decryption. Hence, CS never sees plaintext
data or the keys, but CS can glean some information about
the log, e.g., the order of two ﬁelds or the equality of two
ﬁelds. The output of audit may contain encrypted values
indicating policy violations, but these values are decrypted
only at Cl.
We assume that privacy policies are known to the adver-
sary. This assumption may not be true for an organization’s
internal policies, but relaxing this assumption only simpli-
ﬁes our formal development. To audit over logs encrypted
with EunomiaDET, any constants appearing in the policy (like
“Doctor” in Policy 1 of Section 1) must be encrypted be-
fore the audit process starts, so CS can recover the associ-
ation between ciphertext and plaintexts of constants that
appear in the (publicly known) privacy policy. Similarly,
in EunomiaKH, the hashes of constants in policies must be
revealed to the adversary. in a set
Security and functionality goals. (Conﬁdentiality) Our
primary goal is to protect the conﬁdentiality of the log’s
content, despite any compromise of CS, including its infras-
tructure, employees, and the audit process running on it.
(Expressiveness) Our system should be expressive enough to
represent and audit privacy policies derived from real legis-
lation. In our evaluation, we work with privacy rules derived
from HIPAA and GLBA.
Log equivalence. Central to the deﬁnition of the end-to-
end security property that we prove of our EunomiaDET and
EunomiaKH is the notion of log equivalence. It characterizes
what information about the database remains conﬁdential
despite a complete compromise of CS. Our security deﬁ-
nition states that the adversary can only learn that the log
belongs to a stipulated equivalence class of logs. The coarser
our equivalence, the stronger our security theorem.
For semantically secure encryption, we could say that two
logs are equivalent if they are the same length. When the
encryption permits join, selection, comparison and displaced
comparison queries, this deﬁnition is too strong. For exam-
ple, the attacker must be allowed to learn that two constants
on the log (e.g., Doctor and Nurse) are not equal if they lie
in diﬀerent columns that the attacker can try to join. Hence,
we need a reﬁned notion of log equivalence, which we for-
malize in Section 5.2.
3. POLICY AND LOG SPECIFICATIONS
We review the logic that we use to represent privacy poli-
cies and give a formal deﬁnition of logs (databases). These
deﬁnitions are later used in the deﬁnition and analysis of
our encryption schemes and the ereduce audit algorithm.
Policy logic. We use the guarded-fragment of ﬁrst-order
logic introduced in [3] to represent privacy policies. The
syntax of the logic is shown in Figure 1. Policies or formulas
are denoted ϕ. Terms t are either constants c, d drawn from
a domain D or variables x drawn from a set Var. (Func-
tion symbols are disallowed.) ~t denotes a list of terms. The
basic building block of formulas is atoms, which represent
relations between terms. We allow three kinds of atoms.
Atoms P ::= p(t1, . . . , tn) | timeOrder(t1, d1, t2, d2) |
t1 = t2
Guard
Formula ϕ ::= P | ⊤ | ⊥ | ϕ1 ∧ ϕ2 | ϕ1 ∨ ϕ2 |
::= P | ⊤ | ⊥ | g1 ∧ g2 | g1 ∨ g2 | ∃x.g
g
∀~x.(g → ϕ) | ∃~x.(g ∧ ϕ)
Figure 1: Policy speciﬁcation logic syntax
First, p(t1, . . . , tn) represents a relation which is established
through a table named p in the audit log. The symbol p
is called a predicate (or, interchangeably, a table). The set
of all predicate symbols is denoted by P. An arity function
α : P → N speciﬁes how many arguments each predicate
takes (i.e., how many columns each table has). Second, for
numerical terms, we allow comparison after displacement
with constants, written timeOrder(t1, d1, t2, d2). This rela-
tion means that t1 + d1 ≤ t2 + d2. Here, d1, d2 must be
constants. Third, we allow term equality, written t1 = t2.
Although we restrict atoms of the logic to these three cat-
egories only, the resulting fragment is still very expressive.
All the HIPAA- and GLBA-based policies tested in prior
work [19] and all but one clause of the entire HIPAA and
GLBA privacy rules formalized by DeYoung et al. [18] lie
within this fragment.
Formulas or policies, denoted ϕ, contain standard logical
connectives ⊤ (“true”), ⊥ (“false”), ∧ (“and”), ∨ (“or”), ∀x
(“for every x”) and ∃x (“for some x”). Saliently, the form of
quantiﬁers ∀x and ∃x is restricted: Each quantiﬁer must in-
clude a guard, g. As shown in [19], this restriction, together
with the mode check described in Section 7, ensures that au-
dit terminates (in general, the domain D may be inﬁnite).
Intuitively, one may think of a policy ϕ as enforcing a con-
straint on the predicates it mentions, i.e., on the tables of
the log. A guard g may be thought of as a query on the log
(indeed, the syntax of guards generalizes Datalog, a well-
known database query language). The policy ∀~x.(g → ϕ)
may be read as “for every result ~x of the query g, the con-
straint ϕ must hold.” Dually, ∃~x.(g ∧ ϕ) may be read as
“some result ~x of the query g must satisfy the constraint ϕ.”
Example 1. Consider the following policy, based on §6802(a)
of the GLBA privacy law:
(send(p1, p2, m, t)∧
∀p1, p2, m, q, a, t.
tagged(m, q, a) ∧ activeRole(p1, institution)∧
notAﬃliateOf(p2, p1, t) ∧ customerOf(q, p1, t) ∧ attr(a, npi))
→(cid:18)(∃t1, m1.send(p1, q, m1, t1) ∧ timeOrder(t1, 0, t, 0)∧
timeOrder(t, 0, t1, 30) ∧ discNotice(m1, p1, p2, q, a, t))
W
(∃t2, m2.send(p1, q, m2, t2) ∧ timeOrder(t, 0, t2, 0)∧
timeOrder(t2, 0, t, 30) ∧ discNotice(m2, p1, p2, q, a, t))(cid:19)
The policy states that principal p1 can send a message m to
principal p2 at time t where the message m contains principal
q’s attribute a (e.g., account number) and (i) p1 is in the
role of a ﬁnancial institution, (ii) p2 is not a third-party
aﬃliate of p1 at time t, (iii) q is a customer of p1 at time
t, (iv) the attribute a is non-public personal information
(npi, e.g., a social security number) only if any one of the
two conditions separated by ∨ holds. The ﬁrst condition
says that the institution has already sent a notiﬁcation of
this disclosure in the past 30 days to the customer q (i.e.,
11330 ≤ (t − t1) ≤ 30). The second condition says that the
institution will send a notiﬁcation of this disclosure within
the next 30 days (i.e., 0 ≤ (t2 − t) ≤ 30).
key property of δ is that if, during audit, column a1 of table
p1 is tested for equality against column a2 of table p2, then
hp1.a1, p2.a2i ∈ δ.
Logs and schemas. An audit log or log, denoted L, is a
database with a given schema. A schema S is a set of pairs
of the form htableName, columnNamesi where columnNames
is an ordered list of all the column names in the table (pred-
icate) tableName. A schema S corresponds to a policy ϕ if
S contains all predicates mentioned in the policies ϕ, and
the number of columns in predicate p is α(p).
Semantically, we may view a log L as a function that given
as argument a variable-free atom p(~t) returns either ⊤ (the
entry ~t exists in table p in L) or ⊥ (the entry does not exist).
To model the possibility that a log table may be incomplete,
we allow for a third possible response uu (unknown). In our
implementation, the diﬀerence between uu and ⊥ arises from
an additional bit on the table p indicating whether or not
the table may be extended in future. Formally, we say that
log L1 extends log L2, written L1 ≥ L2 when for every p
and ~t, if L2(p(~t)) 6= uu, then L1(p(~t)) = L2(p(~t)). Thus,
the extended log L1 may determinize some unknown entries
from L2, but cannot change existing entries in L2.
Our logic uses standard semantics of ﬁrst-order logic, treat-
ing logs as models. The semantics, written L |= ϕ, take into
account the possibility of unknown relations; we refer the
reader to [19] for details (these details are not important for
understanding this paper). Intuitively, if L |= ϕ, then the
policy ϕ is satisﬁed on the log L; if L 6|= ϕ, then the policy
is violated; and if neither holds then the log does not have
enough information to determine whether or not the policy
has been violated.
Example 2. The policy in Example 1 can be checked for vi-
olations on a log whose schema contains tables send, tagged,
activeRole, notAﬃliateOf, customerOf, attr and discNotice
with 4, 3, 2, 3, 3, 2 and 6 columns respectively.
In this
audit, values in several columns may have to be compared
for equality. For example, the values in the ﬁrst columns
of tables send and activeRole must be compared because,
in the policy, they contain the same variable p1. Similarly,
timestamps must be compared after displacement with con-
stants 0 and 30. The log encryption schemes we deﬁne next
support these operations.
4. ENCRYPTION SCHEMES
We present our two log encryption schemes, EunomiaDET
and EunomiaKH in Section 4.2 and Section 4.3 respectively.
Both schemes use (as a black-box) a new sub-scheme called
mOPED, for comparing timestamps after displacement,
which we present in Section 4.4.
4.1 Preliminaries
We introduce common constructs used through out the
rest of this section.
Equality scheme. To support policy audit, we determine,
through a static analysis of the policies to be audited, which
pairs of columns in the log schema may be tested for equality
or joined. We defer the details of this policy analysis to
Section 7. For now, we just assume that the result of this
analysis is available. This result, called an equality scheme,
denoted δ, is a set of pairs of the form hp1.a1, p2.a2i. The
Policy constants. Policies may contain constants. For in-
stance, the policy of Example 1 contains the constants npi,
institution, 0 and 30. Before running our audit algorithm
over encrypted logs, a new version of the policy contain-
ing these constants in either encrypted (for EunomiaDET) or
keyed hash (for EunomiaKH) form must be created. Conse-
quently, the adversary, who observes the audit and knows
the plaintext policy, can learn the encryption or hash of
these constants. Hence, these constants play an important
role in our security deﬁnitions. The set of all these policy
constants is denoted C.
Displacement constants. Constants which feature in the
2nd and 4th argument positions of the predicate timeOrder()
play a signiﬁcant role in construction of the mOPED en-
coding and our security deﬁnition. These constants are
called displacements, denoted D. For instance, in Example
1, D = {0, 30}. For any policy, D ⊆ C.
Encrypting timestamps. We assume (conservatively) that
all timestamps in the plaintext log may be compared to each
other, so all timestamps are encrypted (in EunomiaDET) or
hashed (in EunomiaKH) with the same key Ktime. This key is
also used to protect values in the mOPED sub-scheme. The
assumption of all timestamps may be compared with each
other, can be restricted substantially (for both schemes) if
the audit policy is ﬁxed ahead of time.
4.2 EunomiaDET
The log encryption scheme EunomiaDET encrypts each cell
individually using deterministic encryption. All cells in a
column are encrypted with the same key.
Importantly, if
cells in two columns may be compared during audit (as de-
termined by the equality scheme δ), then the two columns
also share the same key. Hence, cells can be tested for equal-
ity simply by comparing their ciphertexts. To allow times-
tamp comparison after displacement, the encrypted log is
paired with a mOPED encoding of timestamps that we ex-
plain later. Note that it is possible to replace deterministic
encryption with a cryptographically secure keyed hash and
a semantically secure ciphertext to achieve the same func-