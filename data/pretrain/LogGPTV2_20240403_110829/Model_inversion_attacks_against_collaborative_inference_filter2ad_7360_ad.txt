conv1
Two 5X5 conv layers
ReLU2
conv11
One 3X3 64 filters layer
ReLU22 Two 3X3 64 filters layers +
two 3X3 128 filters layers
Shadow Model
Two 3X3 conv layers
Four 3X3 conv layers
One 3X3 16 filters layer +
one 3X3 64 filters layer
One 5X5 filters layer +
one 5X5 128 filters layer
(a) MNIST.
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(b) CIFAR10.
Figure 7: Recovered input in query-free attacks (same
training data, same network structure).
6.2 Evaluation
We illustrate the recovered images under the four adversary’s
capability settings in Figures 7, 8, 9 and 10 respectively. The
corresponding quantitative results are listed in Table 6.
For MNIST, the adversary can still recover the input im-
ages from conv1 and RelU2 layers. The quality of the images
is relatively lower than the ones in the white-box or black-
box setting. For CIFAR10, attacks are successful only from
the shallow conv11 layer with the knowledge of training set
or network structure. These results indicate that query-free
attacks are harder to achieve than white-box or black-box
attacks. This is straightforward, as the adversary now has
smaller capabilities. Besides, more layers on the trusted par-
ticipant can also increase the difficulty of image recovery.
We also observe that a different training set with the same
distribution has similar effects on model inversion attacks.
So the adversary does not need to know the exact training
set for attacks. This is also observed in the black-box setting
(Section 5). However, if the adversary has no knowledge of
the network structure, then an alternative network has worse
performance. This emphasizes the importance of knowledge
of network structure for a model inversion attack.
7 DISCUSSIONS
In this section, we review, summarize and compare the attack
results under different settings. We explore the impacts of
system features and adversary’s capabilities on the model
inversion attacks. We also discuss possible defense solutions.
(a) MNIST.
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(a) MNIST.
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(b) CIFAR10.
(b) CIFAR10.
Figure 8: Recovered input in query-free attacks (differ-
ent training data, same network structure).
Figure 10: Recovered input in query-free attacks (dif-
ferent training data, different structures.)
Table 6: PSNR (db) and SSIM for query-free attacks
(a) MNIST.
PSNR
SSIM
Dataset
Net Structure
same set, same net
diff sets, same net
same sets, diff nets
diff sets, diff nets
same set, same net
diff sets, same net
same sets, diff nets
diff sets, diff nets
MNIST
conv1 ReLU2
17.60
21.53
12.59
17.86
0.7423
0.9121
0.6430
0.6952
9.61
9.27
8.05
8.03
0.4981
0.4652
0.3790
0.3226
CIFAR10
conv11 ReLU22 ReLU32
21.16
21.45
17.55
13.06
0.9104
0.9145
0.6344
0.1553
12.74
11.51
12.46
11.30
0.1752
0.1723
0.2714
0.0467
11.09
11.98
10.68
11.47
0.0419
0.0102
0.0247
0.0793
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(b) CIFAR10.
Figure 9: Recovered input in query-free attacks (same
training data, different structures.)
7.1 Impact of System Configurations
From the results in previous sections, we observe that differ-
ent split points can yield different attack effects. This raises
an important question: how to split the neural network in the
collaborative system, to make the inference data more secure?
We use the query-free attack over the LeNet model (MNIST
dataset) as an example to explore this question. We select
the split point at each layer, and perform model inversion
attacks. Figures 11 and 12 show the recovered images, and
PSNR/SSIM metrics respectively.
Generally, we observe that the quality of recovered images
decreases when the split layer goes deeper. This is straightfor-
ward as the relationship between input and output becomes
more complicated and harder to revert when there are more
layers. Besides, we also observe that the image quality drops
significantly, both qualitatively (Figure 11) and quantitatively
(Figure 12), on the fully-connected layer (fc1), indicating that
model inversion with fully-connected layers is much harder
than for convolutional layers. The reason is that a convo-
lutional layer only operates on local elements (the locality
depends on the kernel size), while a fully-connected layer en-
tirely mixes up the patterns from the previous layer. Besides,
the number of output neurons in a fully-connected layer is
typically much smaller than input neurons. So it is relatively
harder to find the reversed relationship from the output of
the fully-connected layer to the input.
Unfortunately, privacy is usually not considered when
selecting the optimal split point in a collaborative system. In
the case of an edge-cloud scenario, most layers (including
all fully-connected layers) are commonly offloaded to the
cloud, while the edge device only computes a small number
of convolutional layers for feature extraction, due to power
and resource constraints [25]. This gives a chance for an
untrusted cloud provider to steal sensitive inference input.
Takeaway: When selecting the split point in a collabora-
tive inference system, privacy should also be considered, in
addition to latency and power constraints. We recommend
placing at least one fully-connected layer on the trusted par-
ticipant to hide the information of sensitive input samples.
Ref
Conv1
ReLU1
Pool1
Conv2
ReLU2
Pool2
FC1
FC1Act
FC2
FC2Act
Figure 11: Recovered images in query-free attacks
7.2 Impact of Adversary’s Capabilities
In addition to the selection of split point, the adversary’s
capability can also have an impact on the attack results. The
question we consider is: which capabilities are critical for
model inversion attacks?
Knowledge of target model. If the adversary can query
the system, then it is not necessary for him to know the
parameters or network structure on the trusted participant.
Comparing Tables 3 and 4, we find that the effects of black-
box attacks using our Inverse-Network technique are as good
as the white-box attacks using rMLE technique. However,
if the adversary does not have access to the model query
Figure 12: PSNR and SSIM in query-free attacks.
APIs and model parameters (the query-free setting), then the
knowledge of network structure plays a relatively important
role in recovering inputs, as discussed in Section 6.2 and
Table 6.
Knowledge of the training set. The adversary does not
need to know the exact training set. Using a different set
following the same distribution, the adversary can recover
the input images with similar quality in the black-box set-
ting (Table 4), or the query-free setting (Table 6). However,
the knowledge of training data distribution is very critical:
without such information, the adversary has to use randomly
generated samples to reverse the network in the black-box
attacks, whose performance drops significantly (Table 4). In
the query-free case, the adversary cannot reconstruct the
model without knowing the training data distribution.
Capability of model query. This is also a critical require-
ment for model inversion attacks. If the adversary is not
able to query the model in a black-box setting, he has to
reconstruct the model before recovering the input. It takes
more effort to implement the attacks, and the performance
is lower (comparing Tables 4 and 6).
Takeaway: We recommend the model owner trains the tar-
get model using a training set whose distribution is unknown
to the adversary. Restricting the query APIs from untrusted
participants can also make the attacks harder.
7.3 Comparisons of Attack Techniques
We propose three different attack techniques under different
threat models. We summarize and compare these techniques.
Table 7: Applicability of techniques under different
settings.
rMLE
Inverse
Shadow
Network Model
✓
–
–
✓
✓
–
✓
✓
✓
White-box
Black-box
Query-free
00.10.20.30.40.50.60.70.80.910510152025PSNRSSIMApplicability. Table 7 lists the effectiveness of each tech-
nique under different settings. The white-box scenario is the
most basic setting: since the adversary knows all the details
about the target model, other techniques without such an
assumption can also be applied here, although some of them
may not be efficient (e.g., reconstructing the shadow model).
For the black-box setting, since the adversary does not know
the model parameters, he cannot use the rMLE technique.
He can either adopt the Inverse-Network approach, or recon-
struct the shadow model and then use rMLE to recover the
input. For the query-free scenario, since the adversary does
not know the model parameters, and has no access to query
the model, he can only use the shadow model reconstruction
with rMLE to recover the image.
Table 8: Comparison of Inverse-Network and shadow
model reconstruction in the black-box setting.
Technique
Inverse-network
Model reconstruction
Inverse-network
Model reconstruction
MNIST
conv1 ReLU2
20.35
39.64
15.41
39.67
0.9887
0.7334
0.6103
0.9968
PSNR
SSIM
conv11 ReLU22 ReLU32
49.87
28.67
0.9993
0.9766
15.41
12.61
0.3124
0.1145
CIFAR10
19.81
18.02
0.6939
0.6893
Performance. We first compare the attack performance
of the rMLE (Table 3) and Inverse-Network 1 (Table 4) ap-
proaches in the white-box setting, respectively. We observe
that when the adversary knows either the training data or
its distribution, the recovered images from Inverse-Network
maintain higher quality than the ones from rMLE. Otherwise,
rMLE performs better than Inverse-Network with randomly
generated samples.
We then compare the performance of Inverse-Network
and shadow model reconstruction solutions in the black-
box setting. As introduced in Section 5, the adversary can
query the model to get pairs of input and corresponding
intermediate values, based on which he can reconstruct a
shadow model. We implement this approach and show the
quantitative comparisons with Inverse-Network in Table 8.
We find that Inverse-Network has better results than shadow
model reconstruction for most datasets and split points.
Takeaway: For the white-box setting, if the adversary has
no knowledge of the training set or distribution, he can
use rMLE for better performance. Otherwise, he can select
Inverse-Network, as it has better results, and takes less effort
to implement and perform. For the black-box setting, Inverse-