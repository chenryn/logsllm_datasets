title:Is the Web HTTP/2 Yet?
author:Matteo Varvello and
Kyle Schomp and
David Naylor and
Jeremy Blackburn and
Alessandro Finamore and
Konstantina Papagiannaki
Is the Web HTTP/2 Yet?
Matteo Varvello1(B), Kyle Schomp2, David Naylor3, Jeremy Blackburn1,
Alessandro Finamore1, and Konstantina Papagiannaki1
1 Telef´onica Research, Barcelona, Spain
PI:EMAIL
2 Case Western Reserve University, Cleveland, USA
3 Carnegie Mellon University, Pittsburgh, USA
http://isthewebhttp2yet.com/
Abstract. Version 2 of the Hypertext Transfer Protocol (HTTP/2) was
ﬁnalized in May 2015 as RFC 7540. It addresses well-known problems
with HTTP/1.1 (e.g., head of line blocking and redundant headers) and
introduces new features (e.g., server push and content priority). Though
HTTP/2 is designed to be the future of the web, it remains unclear
whether the web will—or should—hop on board. To shed light on this
question, we built a measurement platform that monitors HTTP/2 adop-
tion and performance across the Alexa top 1 million websites on a daily
basis. Our system is live and up-to-date results can be viewed at [1].
In this paper, we report ﬁndings from an 11 month measurement cam-
paign (November 2014 – October 2015). As of October 2015, we ﬁnd
68,000 websites reporting HTTP/2 support, of which about 10,000 actu-
ally serve content with it. Unsurprisingly, popular sites are quicker to
adopt HTTP/2 and 31 % of the Alexa top 100 already support it. For
the most part, websites do not change as they move from HTTP/1.1
to HTTP/2; current web development practices like inlining and domain
sharding are still present. Contrary to previous results, we ﬁnd that these
practices make HTTP/2 more resilient to losses and jitter. In all, we ﬁnd
that 80 % of websites supporting HTTP/2 experience a decrease in page
load time compared with HTTP/1.1 and the decrease grows in mobile
networks.
1 Introduction
HTTP/2 (H2 for short) is the new version of HTTP, expected to replace version
1.1 (H1), which was standardized in 1999. H2 promises to make the web faster
and more eﬃcient by compressing headers, introducing server push, ﬁxing the
head of line blocking issue, and loading page elements in parallel over a single
TCP connection (cf. Sect. 2). Although the standard does not require encrypting
H2 connections with Transport Layer Security (TLS), the major browser vendors
currently only support encrypted H2 [19].
While on paper H2 represents the future of the web, it is unclear whether its
adoption will face a struggle similar to IPv6. As discussed in [5], the adoption of
a new protocol largely depends on the ratio between its beneﬁts and its costs.
c(cid:2) Springer International Publishing Switzerland 2016
T. Karagiannis and X. Dimitropoulos (Eds.): PAM 2016, LNCS 9631, pp. 218–232, 2016.
DOI: 10.1007/978-3-319-30505-9 17
Is the Web HTTP/2 Yet?
219
Modern websites are already designed to deal with H1’s ineﬃciencies, employing
hacks like spriting, inlining, and domain sharding [18]. While H2 would remove
the need for such hacks, in theory simplifying web development, given their
widespread use it is unclear how much H2 can improve performance over H1.
Furthermore, it is unclear how these practices will aﬀect H2 performance (which
is crucial, since web developers cannot rebuild their sites overnight nor are they
likely to maintain two versions until H1 disappears).
Motivated by these uncertainties, in this work we build a measurement plat-
form that monitors the adoption and performance of H2. Using machines on
PlanetLab [3] and in our labs in Spain and the U.S., we probe the top 1 million
Alexa websites each day to see which support H2. For those that do, we note
which features they use and measure performance with H1 and H2. Results are
published daily at [1].
This paper reports ﬁndings from an 11-month measurement campaign, from
November 2014 until October 2015 (cf. Sect. 4). As of October 2015, we ﬁnd
68,000 websites reporting H2 support, of which only 10,000 actually serve web-
site content over H2. NGINX, a popular web server implementation, currently
powers 71.7 % of the working H2 websites, with LiteSpeed following at 13.7 %
(in contrast to the 98 % they claim1). Our results also show that sites that have
deployed H2 have not signiﬁcantly altered their content; classic H1 hacks are still
used in the H2 version. For example, inlining (putting CSS styles and JavaScript
code directly in HTML) is still widely used, reducing caching beneﬁts. The same
is true of domain sharding (spreading web objects across multiple domains),
causing H2 to use more TCP connections than necessary. In terms of page load
time, for 80 % of the websites we measured an average reduction in page load
time of 300 and 560 ms when accessed from a wired connection, respectively from
Europe and the USA, 800 ms from a European 4G connection, and 1.6 s from a
European 3G connection. The observed H2 beneﬁts for mobile contradict previ-
ous studies; our analysis suggests that domain sharding, whether intentional or
not, triggers the usage of several TCP connections, making H2 more resilient to
losses and jitter typical of mobile networks.
2 Background and Related Work
H1 is an ASCII protocol that allows a client to request/submit content from/to
a server. H1 is mostly used to fetch web pages, where clients request objects
from a server and the resulting response is serialized over a persistent TCP
connection. H1 provides pipelining to request multiple objects over the same
TCP connection, but the beneﬁts are limited since servers must respond to
requests in order. Thus, an early request for a large object can delay all subse-
quent pipelined requests (head of line blocking). Clients mitigate this by opening
several concurrent TCP connections to the server, which incurs additional over-
head (TCP state on the server, TCP handshake latency, and TLS session setup
1 https://www.litespeedtech.com/http2-ready—To their credit, another 27,000 web-
sites powered by LiteSpeed redirect to an error page that loads over H2.
220
M. Varvello et al.
in the case of HTTPS [13]). Accordingly, browsers limit the number of simul-
taneous connections to each domain (e.g., 6 in Chrome and 15 in Firefox [22]).
Web developers have responded to this limitation with domain sharding, where
content is distributed across multiple domains, circumventing the per-domain
connection limit. Finally, H1 requires the explicit transmission of headers on a
per request/response basis. Therefore, common headers (e.g., server version) are
retransmitted with each object—particularly wasteful for pages with many small
objects.
SPDY and H2. SPDY is Google’s update to H1. It is binary rather than
ASCII, enabling eﬃcient parsing, lighter network footprint, and reducing sus-
ceptibility to security issues caused by unsanitized input strings. SPDY opens
a single TCP connection to a domain and multiplexes requests and responses,
called streams, over that connection, which reduces the number of TCP/TLS
handshakes and the CPU load at the server. SPDY also introduces content pri-
ority (clients can load important objects like CSS and JavaScript earlier), server
push (the server can push objects before the client requests them), and header
compression (reduces redundant header transmission). H2 builds on SPDY, mak-
ing only relatively small changes. For example, H2 uses HPACK [16] for header
compression, eliminating SPDY vulnerability to the “crime” attack [12].
NPN and ALPN. Since SPDY, H1, and H2 all use TLS over port 443, port
number is no longer suﬃcient to indicate to web servers which application pro-
tocol the client wants to use. The Next Protocol Negotiation (NPN) [4] is a
TLS extension developed by Google as part of its SPDY eﬀort. During the TLS
handshake, the server provides a list of supported application protocols; the
client then chooses the protocol to use and communicates it to the server via
an encrypted message. Application Layer Protocol Negotiation (ALPN) [20] is
a revised version of NPN standardized by the IETF. In ALPN, the client sends
which application protocols it supports to the server, ordered by priority. The
server selects the protocol to use based on the protocols it supports and the
client priority; next, it returns the selected protocol to the client via a plain text
message.
Related Work. Previous work mostly investigate SPDY performance [8,10,11,
15]; to the best of our knowledge, [17] is the only work previous to ours focus-
ing on H2. Although the results of these studies are mostly contradictory, they
converge on reporting poor SPDY (and H2) performance on mobile networks.
Erman et al. [7] measure page load time for the top 20 Alexa websites via
SPDY and H1 proxies in 3G. They ﬁnd that SPDY performs poorly in mobile
networks since TCP interprets cellular losses and jitter as congestion, causing
unnecessary backoﬀs. Since SPDY uses fewer TCP connections than H1, its
performance suﬀers more.
Xiao et al. [23] introduce new measurement techniques to provide a more
robust characterization of SPDY. They show that, in absence of browser depen-
dencies and computation, SPDY tends to outperform H1; however, the gains
Is the Web HTTP/2 Yet?
221
are reduced when dependencies and computation are factored back in (with the
caveat that server push can squeeze additional performance gains from SPDY).
De Saxc`e et al. extend this analysis to H2 [17]. Using the top 20 Alexa web-
sites, they investigate H2 performance under various network delay, bandwidth,
and loss using an open-source client and server. Their results conﬁrm those for
SPDY in [23]. Unfortunately, by serving clones of the websites from their own
test server, they ignore the impact of important real-world website properties
like domain sharding.
Our aim is to take the next step in characterizing H2 performance. Our
measurements improve prior art in ﬁve ways: (1) we target more websites (1000 s
as opposed to 10 s or 100 s); (2) we measure real servers from real networks
(wired, 3G, and 4G); (3) we test real websites, not clones or synthetic traces;
(4) we build on Chrome reliability to develop an accurate performance estimation
tool; (5) we also study adoption and website structure trends.
3 Measurement Platform
This section describes our measurement platform. We start by summarizing a
set of tools we have deployed, and then explain how we use them together to
monitor H2 deployment and performance.
Prober is a lightweight bash script that identiﬁes which application protocols
a website announces. Prober uses OpenSSL [14] to attempt ALPN and NPN
negotiations and returns either the list of protocols announced by the server
or failure. Next, prober checks for H2 cleartext (H2C) support—that is, H2
without TLS—by including an UPGRADE header in an H1 request.
H2-lite is a lightweight client that attempts to download only the root object
of a website using H2. H2-lite uses the Node.js [2] H2 library [9]. H2-lite
follows HTTP redirects to obtain the root object and reports any protocol errors
encountered along the way. H2-lite also identiﬁes sites with certiﬁcate problems,
e.g., self- signed certiﬁcates, mismatches between hostname and common name,
or expired/revoked certiﬁcates.
Chrome-loader is a Python tool
It
extracts object sizes and timing information using chrome-har-capturer [6].
Chrome-loader can instruct Chrome to use either H1 or SPDY/H2 (Chrome
does not allow separate control over SPDY and H2). However, using Chrome’s
remote debugging protocol, chrome-loader reports which protocol was used to
retrieve each individual object in a page.
loads pages using Chrome.
that
We now describe our measurement platform in detail. It consists of a single
master and many workers; the master issues crawl requests to the workers, which
are deployed on both PlanetLab [3] and machines in our labs (U.S. and Spain).
We use PlanetLab for simple measurements at a large scale and our lab machines
for more complex measurements at a smaller scale and where machine reliability
is important. The master constantly monitors PlanetLab to identify a pool of
222
M. Varvello et al.
candidate machines (at least 500 MB of free memory, CPU load under 30 %, and
no network connectivity issues). We collect measurements in three phases:
Phase I: It discovers, daily, which protocols are supported by the top 1 million
Alexa websites. First, the master launches an instance of the prober on each
PlanetLab worker. The worker is then assigned a unique 100-website list to probe.
When it ﬁnishes, it reports results to the master and obtains a new list if one is
available. This approach ensures load balancing among heterogeneous workers
allowing faster workers to complete more tasks. To deal with slow workers, the
master re-assigns uncompleted tasks to new workers after a timeout T (set to
the average task completion time across workers). Phase I terminates when the
tracker has a complete set of results.
Phase II: It veriﬁes, daily, whether the sites that reported H2 support in Phase
I actually serve content over H2. After Phase I, the master launches several
instances of h2-lite and, as above, it dynamically assigns each 100 sites that
reported H2 support in Phase I. Because the H2 library requires more up-to-date
software than is available on PlanetLab, we run h2-lite on 4 machines under
our control, 2 in Barcelona (Spain) and 2 in Cleveland (U.S.). When Phase II
terminates, the master has a list of sites that actually serve content using H2.
Phase III: It fetches both the H1 and H2 version of websites that serve content
via H2 using multiple network locations and access network types (e.g., ﬁber
and 4G). The master is responsible for selecting the machines to be used and
instructing them which websites to test. The master uses one of three strategies:
(1) regular, where each network location with ﬁber access is weekly instructed
to test all H2 websites identiﬁed by Phase II; (2) lazy, the same as the regular
strategy but a website is tested only if one of these conditions is met: (a) it is a
new website that recently adopted H2, (b) its content signiﬁcantly changed from
the last time it was tested, or (c) a timeout elapsed since the last test; (3) mobile,
where only mobile-enabled locations are selected, and a subset of websites are
tested based on their Alexa popularity.
To test a website, we fetch it 5 times with H1 and 5 times with either SPDY or
H2 (as discussed above, Chrome does not provide ﬁne-grained control between
SPDY and H2). Fetches are run sequentially to limit the impact of network
load. While testing a website, we run a background ping process to collect sta-
tistics about network latency and packet loss. For the mobile strategy, we force
Chrome to report a mobile user-agent to the server, which may respond with
a mobile version of the website. Before the ﬁve trials, an initial “primer” load
is performed. This primer has a double purpose: (1) test whether a website
signiﬁcantly changed its content since the last time it was tested (used by the
lazy strategy), and (2) ensure that DNS entries are cached at the ISP’s DNS
resolver before the ﬁrst real trial (to prevent a cache miss on the ﬁrst trial from
skewing the load time results). Local content and DNS caches are disabled and
each request carries a cache-control header instructing network caches not to
respond.
Is the Web HTTP/2 Yet?
223
We currently have Phase III workers in three diﬀerent locations: Barcelona
(Spain), Cleveland (USA), and Pittsburgh (USA). Each location consists of three
machines with ﬁber connectivity. In addition, machines in Spain are connected
to Android phones via USB tethering; each phone is conﬁgured to use either
3G or 4G. Because each SIM card has a 4 GB monthly limit, a maximum of
about 200 websites—5 trials per protocol plus the primer—can be tested before
exhausting the available plan. We ran Phase III with the regular strategy from
November 2014 to September 2015, when the widespread deployment of H2 made
it impossible to run Phase III weekly without additional machines. Thus, we
switched to the lazy strategy. Finally, we ran Phase III with the mobile strategy
only once in October 2015. We plan to run the mobile strategy once per month,
as constrained by the mobile data plan.
4 Results
This section presents and analyzes the data collected using our measurement
platform between November 10th, 2014 and October 16th, 2015. We invite the
reader to access fresh data and analysis at [1].
4.1 Adoption
We recorded protocol support announced via ALPN/NPN for 11 months, during
which time we saw support for 44 protocols (34 of which were versions of SPDY).
Table 1 summarizes the evolution over time of the three most popular protocols,
namely H1, SPDY 3.1, and H2; in addition, we also report NPN, ALPN, and
H2C support when available. Each percentage is the maximum observed during