title:Not All Pixels are Born Equal: An Analysis of Evasion Attacks under
Locality Constraints
author:Vikash Sehwag and
Chawin Sitawarin and
Arjun Nitin Bhagoji and
Arsalan Mosenia and
Mung Chiang and
Prateek Mittal
POSTER: Not All Pixels are Born Equal: An Analysis
of Evasion Attacks under Locality Constraints
Vikash Sehwag∗1, Chawin Sitawarin2, Arjun Nitin Bhagoji1
Arsalan Mosenia1, Mung Chiang3, Prateek Mittal1
1Princeton University 2University of California, Berkeley 3Purdue University
ABSTRACT
Deep neural networks (DNNs) have enabled success in learning
tasks such as image classification, semantic image segmentation
and steering angle prediction which can be key components of
the computer vision pipeline of safety-critical systems such as
autonomous vehicles. However, previous work has demonstrated
the feasibility of using physical adversarial examples to attack image
classification systems.
In this work, we argue that the success of realistic adversarial
examples is highly dependent on both the structure of the training
data and the learning objective. In particular, realistic, physical-
world attacks on semantic segmentation and steering angle predic-
tion constrain the adversary to add localized perturbations, since
it is very difficult to add perturbations in the entire field of view
of input sensors such as cameras for applications like autonomous
vehicles. We empirically study the effectiveness of adversarial ex-
amples generated under strict locality constraints imposed by the
aforementioned applications. Even with image classification, we
observe that the success of the adversary under locality constraints
depends on the training dataset. With steering angle prediction,
we observe that adversarial perturbations localized to an off-road
patch are significantly less successful compared to those on-road.
For semantic segmentation, we observe that perturbations local-
ized to small patches are only effective at changing the label in
and around those patches, making non-local attacks difficult for an
adversary. We further provide a comparative evaluation of these
localized attacks over various datasets and deep learning models
for each task.
KEYWORDS
Adversarial examples; Computer vision; Deep learning
1 INTRODUCTION
DNNs have achieved state-of-the-art performance on computer vi-
sion tasks such as object detection, classification, and segmentation.
However, a large amount of recent literature has demonstrated the
vulnerability of DNNs to adversarial examples [5, 12]. In particular,
∗PI:EMAIL
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5693-0/18/10.
https://doi.org/10.1145/3243734.3278515
(a) Original image
(b) Adversarial image
(c) Patch 1: single car window
(d) Patch 2: single car
Figure 1: Comparison of adversarial examples generated for
Deeplabv3+ [6]. Fig. 1a, 1b presents the original and corre-
sponding adversarial image respectively. Fig. 1c highlights
that the success of a smaller adversarial patch is limited to
its local pixels. A significantly larger patch size, which will
be hard to use in physical-realizable attacks, is required to
propagate the effect of added patch to non-local pixels.
previous work has highlighted that these vulnerabilities exist in
the physical-world as well [4, 8, 10, 11].
These attacks succeed by embedding the adversarial perturba-
tions on a physical object in the targeted environment. Sharif et al.
[10] add perturbations on eyeglass frames to fool facial recognition
systems, whereas Brown et al. [4] print out adversarial patches
which, if added to an object, can attack image recognition systems.
Imperceptible perturbations are used to fool traffic sign classifica-
tion networks in [8, 11]. It should be noted that these existing works
has considered only image classification, a task which has been
generally superseded by considerably more complex tasks such as
image segmentation in applications like autonomous vehicles [7].
In this work, we evaluate the success of realistic adversarial exam-
ples with localized perturbations for image classification, steering
angle prediction and semantic image segmentation. To our knowl-
edge, this is the first work to evaluate the success of localized visual
adversarial perturbation for multiple computer vision applications.
Localized adversarial perturbations are of interest when carrying
out physically realizable attacks since an adversary cannot, for ex-
ample, cover an entire street with adversarial perturbations! The
critical nature of this locality constraint is demonstrated in Figure
1. Our main findings are as follows:
1. Not all pixels are equally important: Using multiple datasets
and targeted applications (Section 3.1, 3.2), we demonstrate that the
attack success is highly correlated with pixel locations. For e.g., with
the MNIST and Udacity self driving car datasets [2], we observe
Poster PresentationCCS’18, October 15-19, 2018, Toronto, ON, Canada2285a significant difference in attack success when perturbations are
localized to specific pixel locations such as center and border pixels.
2. Attack success rates are highly correlated with size of lo-
cality constraint: With each application, we observe a significant
decrease in attack success with a decrease in size of the locality
constraint. With semantic segmentation, we demonstrate that this
correlation further depends on the choice of adversarial objective
(Section 3.3). Under complex adversarial objectives, such as clas-
sification of each pixel as a random label, the attack success with
localized perturbations decreases significantly as compared to ad-
versaries able to add noise to the entire image.
3. Current attack methods do not produce reliable adversar-
ial examples under locality constraints: We generated adver-
sarial perturbations under locality constraints using the state-of-the-
art attack method [5]. In each experiment, we observe a significant
variance in the success of this attack with different input images
and the size and location of added perturbations. This limitation
indicates that current attack algorithms have to be improved to
carry out realistic attacks.
2 SETUP AND METHODOLOGY
Our primary focus is to evaluate the success of evasion attacks
with localized adversarial perturbations in multiple computer vi-
sion application such as image classification, segmentation, and
steering angle prediction. To generate adversarial perturbations,
we minimize Lagrangian relaxation of the adversarial objective [5]
using projected gradient descent with the Adam algorithm over
200 iterations and a learning rate of 0.1. We evaluate attack success
only with targeted attacks which are strictly harder than untargeted
attacks. To be consistent with previous work [4], we use the terms
localized perturbation and adversarial patch interchangeably. We
measure the target success rate and accuracy for each adversarial
attack. The former refers to the percentage of perturbed images clas-
sified as targeted labels while the latter represents the percentage
of perturbed images classified as correct labels.
Initially, we experiment with image classification where the
adversarial objective is the classification of perturbed images as the
targeted labels. With steering angle prediction, the network takes
an input or a sequence of inputs and learns to predict the correct
steering angle [3]. The adversarial objective is to predict a targeted
steering angle for a perturbed image. In semantic segmentation, the
network learns the pixel-wise classification of input images, which
is considerably more difficult than predicting a single label for the
whole image. The adversarial objective with image segmentation is
the classification of each pixel of the perturbed image as its target
label.
3 RESULTS
3.1 Image Classification
We experiment with MNIST and ImageNet datasets for image clas-
sification. For MNIST, we train a four-layer convolutional neural
network, achieving 99.17% accuracy. For ImageNet, we use the pre-
trained InceptionV3 model, which achieves 94.6% top-1 accuracy
on ImageNet-1k subset [1]. We experiment with both center and
border patch location, where the center patch refers to masking of
(a) Accuracy
(b) Target success rate
Figure 2: Limited success of border adversarial patches in