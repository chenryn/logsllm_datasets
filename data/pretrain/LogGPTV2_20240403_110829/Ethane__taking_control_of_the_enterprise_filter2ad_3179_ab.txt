Figure 2 shows a typical Ethane network. The end-hosts are
unmodiﬁed and connect via a wired Ethane Switch or an Ethane
wireless access point.
(From now on, we will refer to both as
“Switches”, described next in §3.2).4
3The network may use a stronger form of host authentication, such
as 802.1X, if desired.
4We will see later that an Ethane network can also include legacy
Ethernet switches and access points, so long as we include some
Ethane Switches in the network. The more switches we replace,
the easier to manage and the more secure the network.
Ethernet source address and the physical ingress port.6 The associ-
ated Action is to drop the packet, update a packet-and-byte counter,
and set an activity bit (to tell when the host has stopped sending).
Only the Controller can add entries to the ﬂow table. Entries are
removed because they timeout due to inactivity (local decision) or
because they are revoked by the Controller. The Controller might
revoke a single, badly behaved ﬂow, or it might remove a whole
group of ﬂows belonging to a misbehaving host, a host that has just
left the network, or a host whose privileges have just changed.
The ﬂow table is implemented using two exact-match tables:
One for application-ﬂow entries and one for misbehaving-host en-
tries. Because ﬂow entries are exact matches, rather than longest-
preﬁx matches, it is easy to use hashing schemes in conventional
memories rather than expensive, power-hungry TCAMs.
Other Actions are possible in addition to just forward and drop.
For example, a Switch might maintain multiple queues for differ-
ent classes of trafﬁc, and the Controller can tell it to queue pack-
ets from application ﬂows in a particular queue by inserting queue
IDs into the ﬂow table. This can be used for end-to-end L2 iso-
lation for classes of users or hosts. A Switch could also perform
address translation by replacing packet headers. This could be used
to obfuscate addresses in the network by “swapping” addresses at
each Switch along the path—an eavesdropper would not be able
to tell which end-hosts are communicating—or to implement ad-
dress translation for NAT in order to conserve addresses. Finally, a
Switch could control the rate of a ﬂow.
The Switch also maintains a handful of implementation-speciﬁc
entries to reduce the amount of trafﬁc sent to the Controller. This
number should remain small to keep the Switch simple, although
this is at the discretion of the designer. On one hand, such entries
can reduce the amount of trafﬁc sent to the Controller; on the other
hand, any trafﬁc that misses on the ﬂow table will be sent to the
Controller anyway, so this is just an optimization.
Local Switch Manager. The Switch needs a small local manager
to establish and maintain the secure channel to the Controller, to
monitor link status, and to provide an interface for any additional
Switch-speciﬁc management and diagnostics.
(We implemented
our manager in the Switch’s software layer.)
There are two ways a Switch can talk to the Controller. The
ﬁrst one, which we have assumed so far, is for Switches that are
part of the same physical network as the Controller. We expect this
to be the most common case; e.g., in an enterprise network on a
single campus. In this case, the Switch ﬁnds the Controller using
our modiﬁed Minimum Spanning Tree protocol described in §3.7.
The process results in a secure channel stretching through these
intermediate Switches all the way to the Controller.
If the Switch is not within the same broadcast domain as the
Controller, the Switch can create an IP tunnel to it (after being
manually conﬁgured with its IP address). This approach can be
used to control Switches in arbitrary locations, e.g., the other side
of a conventional router or in a remote location. In one applica-
tion of Ethane, the Switch (most likely a wireless access point) is
placed in a home or small business and then managed remotely by
the Controller over this secure tunnel.
When we add an Ethane Switch to the network, it has to ﬁnd the
Controller (§3.3), open a secure channel to it, and help the Con-
troller ﬁgure out the topology. We do this with a modiﬁed mini-
mum spanning tree algorithm (per §3.7 and denoted by thick, solid
lines in the ﬁgure). The outcome is that the Controller knows the
whole topology, while each Switch only knows a part of it.
When we add (or boot) a host, it has to authenticate itself with the
Controller. From the Switch’s point-of-view, packets from the new
host are simply part of a new ﬂow, and so packets are automatically
forwarded to the Controller over the secure channel, along with
the ID of the Switch port on which they arrived. The Controller
authenticates the host and allocates its IP address (the Controller
includes a DHCP server).
3.2 Switches
A wired Ethane Switch is like a simpliﬁed Ethernet switch. It
has several Ethernet interfaces that send and receive standard Eth-
ernet packets. Internally, however, the switch is much simpler, as
there are several things that conventional Ethernet switches do that
an Ethane switch doesn’t need: An Ethane Switch doesn’t need to
learn addresses, support VLANs, check for source-address spoof-
ing, or keep ﬂow-level statistics (e.g., start and end time of ﬂows,
although it will typically maintain per-ﬂow packet and byte coun-
ters for each ﬂow entry). If the Ethane Switch is replacing a Layer-
3 “switch” or router, it doesn’t need to maintain forwarding tables,
ACLs, or NAT. It doesn’t need to run routing protocols such as
OSPF, ISIS, and RIP. Nor does it need separate support for SPANs
and port-replication (this is handled directly by the ﬂow table under
the direction of the Controller).
It is also worth noting that the ﬂow table can be several orders-of-
magnitude smaller than the forwarding table in an equivalent Eth-
ernet switch. In an Ethernet switch, the table is sized to minimize
broadcast trafﬁc: as switches ﬂood during learning, this can swamp
links and makes the network less secure.5 As a result, an Ethernet
switch needs to remember all the addresses it’s likely to encounter;
even small wiring closet switches typically contain a million en-
tries. Ethane Switches, on the other hand, can have much smaller
ﬂow tables: they only need to keep track of ﬂows in-progress. For
a wiring closet, this is likely to be a few hundred entries at a time,
small enough to be held in a tiny fraction of a switching chip. Even
for a campus-level switch, where perhaps tens of thousands of ﬂows
could be ongoing, it can still use on-chip memory that saves cost
and power.
We expect an Ethane Switch to be far simpler than its corre-
sponding Ethernet switch, without any loss of functionality. In fact,
we expect that a large box of power-hungry and expensive equip-
ment will be replaced by a handful of chips on a board.
Flow Table and Flow Entries. The Switch datapath is a man-
aged ﬂow table. Flow entries contain a Header (to match packets
against), an Action (to tell the switch what to do with the packet),
and Per-Flow Data (which we describe below).
There are two common types of entry in the ﬂow table: per-
ﬂow entries describing application ﬂows that should be forwarded,
and per-host entries that describe misbehaving hosts whose packets
should be dropped. For TCP/UDP ﬂows, the Header ﬁeld covers
the TCP/UDP, IP, and Ethernet headers, as well as physical port
information. The associated Action is to forward the packet to a
particular interface, update a packet-and-byte counter (in the Per-
Flow Data), and set an activity bit (so that inactive entries can be
timed-out). For misbehaving hosts, the Header ﬁeld contains an
The local Switch manager relays link status to the Controller so
it can reconstruct the topology for route computation. Switches
maintain a list of neighboring switches by broadcasting and receiv-
ing neighbor-discovery messages. Neighbor lists are sent to the
Controller after authentication, on any detectable change in link
status, and periodically every 15 seconds.
5In fact, network administrators often use manually conﬁgured and
inﬂexible VLANs to reduce ﬂooding.
6If a host is spooﬁng, its ﬁrst-hop port can be shut of directly (§3.3).
Tracking Bindings. One of Ethane’s most powerful features is that
it can easily track all the bindings between names, addresses, and
physical ports on the network, even as Switches, hosts, and users
join, leave, and move around the network.
It is Ethane’s ability
to track these dynamic bindings that makes the policy language
possible: It allows us to describe policies in terms of users and
hosts, yet implement the policy using ﬂow tables in Switches.
A binding is never made without requiring authentication, so as
to prevent an attacker from assuming the identity of another host or
user. When the Controller detects that a user or host leaves, all of
its bindings are invalidated, and all of its ﬂows are revoked at the
Switch to which it was connected. Unfortunately, in some cases, we
cannot get reliable join and leave events from the network. There-
fore, the Controller may resort to timeouts or the detection of move-
ment to another physical access point before revoking access.
Namespace Interface. Because Ethane tracks all the bindings be-
tween users, hosts, and addresses, it can make information avail-
able to network managers, auditors, or anyone else who seeks to
understand who sent what packet and when.
In current networks, while it is possible to collect packet traces, it
is almost impossible to ﬁgure out later which user—or even which
host—sent or received the packets, as the addresses are dynamic
and there is no known relationship between users and packet ad-
dresses.
An Ethane Controller can journal all the authentication and bind-
ing information: The machine a user is logged in to, the Switch
port their machine is connected to, the MAC address of their pack-
ets, and so on. Armed with a packet trace and such a journal, it
is possible to determine exactly which user sent a packet, when it
was sent, the path it took, and its destination. Obviously, this in-
formation is very valuable for both fault diagnosis and identifying
break-ins. On the other hand, the information is sensitive and con-
trols need to be placed on who can access it. We expect Ethane
Controllers to provide an interface that gives privileged users ac-
cess to the information. In our own system, we built a modiﬁed
DNS server that accepts a query with a timestamp, and returns the
complete bound namespace associated with a speciﬁed user, host,
or IP address (described in §5).
Permission Check and Access Granting. Upon receiving a packet,
the Controller checks the policy to see what actions apply to it. The
results of this check (if the ﬂow is allowed) are forwarded to the
route computation component which determines the path given the
policy constraint. In our implementation all paths are pre-computed
and maintained via a dynamic all-pairs shortest path algorithm [13].
Section 4 describes our policy model and implementation.
Enforcing Resource Limits. There are many occasions when a
Controller wants to limit the resources granted to a user, host, or
ﬂow. For example, it might wish to limit a ﬂow’s rate, limit the rate
at which new ﬂows are setup, or limit the number of IP addresses al-
located. Such limits will depend on the design of the Controller and
Switch, and they will be at the discretion of the network manager.
In general, however, Ethane makes it easy to enforce such limits
either by installing a ﬁlter in a Switch’s ﬂow table or by telling the
Switch to limit a ﬂow’s rate.
The ability to directly manage resources from the Controller is
the primary means of protecting the network (and Controller) from
resource exhaustion attacks. To protect itself from connection ﬂood-
ing from unauthenticated hosts, a Controller can place a limit on
the number of authentication requests per host and per switch port;
hosts that exceed their allocation can be closed down by adding an
entry in the ﬂow table that blocks their MAC address. If such hosts
Figure 3: High-level view of Controller components.
3.3 Controller
The Controller is the brain of the network and has many tasks;
Figure 3 gives a block-diagram. The components do not have to
be co-located on the same machine (indeed, they are not in our
implementation).
Brieﬂy, the components work as follows. The authentication
component is passed all trafﬁc from unauthenticated or unbound
MAC addresses. It authenticates users and hosts using credentials
stored in the registration database. Once a host or user authenti-
cates, the Controller remembers to which switch port they are con-
nected.
The Controller holds the policy ﬁle, which is compiled into a fast
lookup table (see §4). When a new ﬂow starts, it is checked against
the rules to see if it should be accepted, denied, or routed through a
waypoint. Next, the route computation uses the network topology
to pick the ﬂow’s route. The topology is maintained by the switch
manager, which receives link updates from the Switches.
In the remainder of this section, we describe each component’s
function in more detail. We leave description of the policy language
for the next section.
Registration. All entities that are to be named by the network (i.e.,
hosts, protocols, Switches, users, and access points7) must be reg-
istered. The set of registered entities make up the policy namespace
and is used to statically check the policy (§4) to ensure it is declared
over valid principles.
The entities can be registered directly with the Controller, or—as
is more likely in practice and done in our own implementation—
Ethane can interface with a global registry such as LDAP or AD,
which would then be queried by the Controller.
By forgoing Switch registration, it is also possible for Ethane to
provide the same “plug-and-play” conﬁguration model for Switches
as Ethernet. Under this conﬁguration, the Switches distribute keys
on boot-up (rather than require manual distribution) under the as-
sumption that the network has not been compromised.
Authentication. All Switches, hosts, and users must authenticate
with the network. Ethane does not specify a particular host au-
thentication mechanism; a network could support multiple authen-
tication methods (e.g., 802.1X or explicit user login) and employ
entity-speciﬁc authentication methods. In our implementation, for
example, hosts authenticate by presenting registered MAC addresses,
while users authenticate through a web front-end to a Kerberos
server. Switches authenticate using SSL with server- and client-
side certiﬁcates.
7We deﬁne an access point here as a {Switch,port} pair
spoof their address, the Controller can disable their Switch port.
A similar approach can be used to prevent ﬂooding from authenti-
cated hosts.
Flow state exhaustion attacks are also preventable through re-
source limits. Since each ﬂow setup request is attributable to a
user, host, and access point, the Controller can enforce limits on
the number of outstanding ﬂows per identiﬁable source. The net-
work may also support more advanced ﬂow-allocation policies. For
example, an integrated hardware/software Switch can implement
policies such as enforcing strict limits on the number of ﬂows for-
warded in hardware per source and looser limits on the number of
ﬂows in the slower (and more abundant) software forwarding ta-
bles.
3.4 Handling Broadcast and Multicast
Enterprise networks typically carry a lot of multicast and broad-
cast trafﬁc. It is worth distinguishing broadcast trafﬁc (which is
mostly discovery protocols, such as ARP) from multicast trafﬁc
(which is often from useful applications, such as video). In a ﬂow-
based network like Ethane, it is quite easy for Switches to handle
multicast: The Switch keeps a bitmap for each ﬂow to indicate
which ports the packets are to be sent to along the path. The Con-
troller can calculate the broadcast or multicast tree and assign the
appropriate bits during path setup.
In principle, broadcast discovery protocols are also easy to han-
dle in the Controller. Typically, a host is trying to ﬁnd a server
or an address; given that the Controller knows all, it can reply to
a request without creating a new ﬂow and broadcasting the trafﬁc.
This provides an easy solution for ARP trafﬁc, which is a signiﬁ-
cant fraction of all network trafﬁc In practice, however, ARP could
generate a huge load for the Controller; one design choice would
be to provide a dedicated ARP server in the network to which all
Switches direct all ARP trafﬁc. But there is a dilemma when trying
to support other discovery protocols: each one has its own proto-
col, and it would be onerous for the Controller to understand all of
them. Our own approach has been to implement the common ones
directly in the Controller and to broadcast unknown request types
with a rate-limit. Clearly this approach does not scale well, and we
hope that, if Ethane becomes widespread in the future, discovery
protocols will largely go away. After all, they are just looking for
binding information that the network already knows; it should be
possible to provide a direct way to query the network. We discuss
this problem further in §7.
3.5 Replicating the Controller: Fault-Tolerance
and Scalability
Designing a network architecture around a central controller raises
concerns about availability and scalability. While our measure-
ments in §6 suggest that thousands of machines can be managed
by a single desktop computer, multiple Controllers may be desir-
able to provide fault-tolerance or to scale to very large networks.
This section describes three techniques for replicating the Con-
troller. In the simplest two approaches, which focus solely on im-
proving fault-tolerance, secondary Controllers are ready to step in
upon the primary’s failure: these can be in cold-standby (having no
network binding state) or warm-standby (having network binding
state) modes. In the fully-replicated model, which also improves
scalability, requests from Switches are spread over multiple active
Controllers.
In the cold-standby approach, a primary Controller is the root of
the minimum spanning tree (MST) and handles all registration, au-
thentication, and ﬂow-establishment requests. Backup Controllers
sit idly-by waiting to take over if needed. All Controllers partici-
pate in the MST, sending HELLO messages to Switches advertising
their ID. Just as with a standard spanning tree, if the root with the
“lowest” ID fails, the network will converge on a new root (i.e., a
new Controller). If a backup becomes the new MST root, it will
start to receive ﬂow requests and begin acting as the primary Con-