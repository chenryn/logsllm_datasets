title:Towards Online Spam Filtering in Social Networks
author:Hongyu Gao and
Yan Chen and
Kathy Lee and
Diana Palsetia and
Alok N. Choudhary
Towards Online Spam Filtering in Social Networks
Hongyu Gao
Yan Chen
Kathy Lee†
Northwestern University
Northwestern University
Northwestern University
Evanston, IL, USA
Evanston, IL, USA
Evanston, IL, USA
PI:EMAIL
PI:EMAIL
PI:EMAIL
Diana Palsetia†
Northwestern University
Evanston, IL, USA
Alok Choudhary†
Northwestern University
Evanston, IL, USA
PI:EMAIL
PI:EMAIL
Abstract
Online social networks (OSNs) are extremely popular
among Internet users. Unfortunately, in the wrong hands,
they are also effective tools for executing spam campaigns.
In this paper, we present an online spam ﬁltering system that
can be deployed as a component of the OSN platform to in-
spect messages generated by users in real-time. We propose
to reconstruct spam messages into campaigns for classiﬁca-
tion rather than examine them individually. Although cam-
paign identiﬁcation has been used for ofﬂine spam analysis,
we apply this technique to aid the online spam detection
problem with sufﬁciently low overhead. Accordingly, our
system adopts a set of novel features that effectively dis-
tinguish spam campaigns. It drops messages classiﬁed as
“spam” before they reach the intended recipients, thus pro-
tecting them from various kinds of fraud. We evaluate the
system using 187 million wall posts collected from Face-
book and 17 million tweets collected from Twitter. In differ-
ent parameter settings, the true positive rate reaches 80.9%
while the false positive rate reaches 0.19% in the best case.
In addition, it stays accurate for more than 9 months after
the initial training phase. Once deployed, it can constantly
secure the OSNs without the need for frequent re-training.
Finally, tested on a server machine with eight cores (Xeon
E5520 2.2Ghz) and 16GB memory, the system achieves an
average throughput of 1580 messages/sec and an average
processing latency of 21.5ms on the Facebook dataset.
1
Introduction
Online social networks (OSNs) are extremely popu-
lar collaboration and communication tools that have at-
tracted millions of Internet users. Unfortunately, recent ev-
idence shows that they can also be effective mechanisms
for spreading attacks. Popular OSNs are increasingly be-
coming the target of phishing attacks launched from large
botnets [1, 3]. Two recent studies [9, 10] have conﬁrmed
the existence of large-scale spam campaigns in Twitter and
Facebook, respectively. Furthermore, the clickthrough rate
of OSN spam is orders of magnitude higher than its email
counterpart [10], indicating that users are more prone to
trust spam messages from their friends in OSNs.
The OSN spam problem has already received atten-
tion from researchers. Meanwhile, email spam, a seem-
ingly very similar problem, has been extensively studied for
years. Unfortunately, the bulk of the existing solutions are
not directly applicable, because of a series of distinct char-
acteristics pertaining to the OSN spam. i) In any OSN, all
messages, including spam, originate from accounts regis-
tered at the same site. In contrast, email spam is not nec-
essarily sent from accounts registered at legitimate service
providers. The widely used email server reputation based
detection approaches [11, 19, 22] rely on the assumption
that the spamming SMTP servers run on bot machines, and
are thus inapplicable in OSNs. Realizing that this assump-
tion is not always true , researchers have proposed to iden-
tify accounts signed up by spammers from legitimate email
ii) Spamming account identiﬁca-
service providers [34].
tion is also the focus of the existing OSN spam detection
work [14, 26, 31, 32]. However, the second characteris-
tic of OSN spam is that the majority of spam messages
come from compromised accounts [9, 10], rather than ac-
counts created and exclusively controlled by spammers. It
essentially means that spammers and legitimate users are
sharing accounts. Thus, identifying spamming accounts is
not sufﬁcient to ﬁght OSN spam. iii) Messages in OSNs,
spam or not, are generally short. The perception that legiti-
mate emails have variable size while spam tends to be small
no longer holds in OSNs. The third characteristic, along
with the previous two, obsoletes many features that used to
work well in supervised machine learning based detection
approaches, which we brieﬂy discuss in Section 3.
It is worth noting that ofﬂine OSN analysis works suc-
cessfully reveal OSN spam messages [9, 10], but they are
not designed as online detection tools, since they either
have long lag-time or limited efﬁciency. We offer more de-
tailed comparison in Section 7. Meanwhile, message con-
tent based detection approaches, such as the recently pro-
posed campaign template inference [18], are expected to
perform equally well if applied to OSNs. However, they
implicitly require that all campaigns must be present in the
labeled training set in order for good detection coverage.
This requirement itself is hard to satisfy, not to mention the
difﬁculty of ﬁghting campaigns that emerge in the future. In
addition, approaches based on malicious URL detection are
also applicable [27]. Nonetheless, URL obfuscation tech-
niques (e.g. , using “nevasubevu\t. blogs pot\t.\tco\tm
(take out spaces)” instead of “nevasubevu.blogspot.com”)
make it difﬁcult for any automated tool to recognize the em-
bedded URLs in the ﬁrst place.
In this paper, we present an online spam ﬁltering sys-
tem speciﬁcally designed for OSNs and can be deployed as
a component of the OSN platform. After the initial train-
ing phase, it efﬁciently inspects the stream of user gen-
erated messages, immediately dropping those classiﬁed as
spam before they reach the intended recipients. The system
owns four desirable properties as an online ﬁltering tool,
which are: i) high accuracy, ii) no need for all campaigns
to be present in the training set, iii) no need for frequent
re-training , and iv) low latency.
The key insight is that we always seek to uncover the
connection among all the messages by performing cluster-
ing on them, instead of directly inspecting each individ-
ual message without correlating it with others. The corre-
lated spam messages form spam campaigns. Although the
clustering approach has been used for ofﬂine spam analy-
sis [4, 35], it is never used for online spam ﬁltering due to
its computational overhead. We leverage incremental clus-
tering and parallelization to address this challenge. When a
new message is generated, the system organizes it, along
with all the previously observed messages, into clusters.
The new message is then classiﬁed according to whether
or not the cluster it resides in is a spam cluster, which is
determined by all the messages in the same cluster collec-
tively.
Accordingly, the classiﬁer is trained on a set of features
associated with message clusters instead of individual mes-
sages. We identiﬁed six such distinguishing features. They
all stem from the spammers’ need to push spam messages
to as many recipients as possible. Meanwhile, these fea-
tures grasp the commonality among spam campaigns, e.g. ,
they generate spam messages fast while lasting, rather than
the speciality of any individual campaign, e.g. , word dis-
tribution or underlying templates. Hence, spam campaigns
can still be detected even if they do not have any instance
included in the training set. In addition, the features pertain-
ing to campaigns cannot be easily maneuvered. For exam-
ple, if the spammer slows down the speed of spam message
generation, he has to pay the price of the reduced ability to
affect potential victims. As a result, these features are per-
sistent over time. After the initial training, the system can
work online for a long period of time without the need for
re-training. For the same reason, it is also harder for the
spammers to craft their sending pattern to evade detection.
We evaluate the system using 187 million wall posts col-
lected from Facebook and 17 million tweets collected from
Twitter. Experimental results show that the system yields
80.9% true positive rate and 0.19% false positive rate in the
best parameter settings. It stays accurate for more than 9
months after the initial training phase, and achieves an av-
erage throughput of 1580 messages/sec and an average pro-
cessing latency of 21.5ms on the Facebook dataset.
In summary, we frame our contributions as:
• We propose to use spam campaigns, instead of
individual spam messages, as the objects for spam
classiﬁcation.
• We solve the challenge of reconstructing campaigns
in real-time by adopting incremental clustering and
parallelization.
• We identify six features that distinguish spam
campaigns from legitimate message clusters in OSNs.
• We develop and evaluate an accurate and efﬁcient
system that can be easily deployed at the OSN server
side to provide online spam ﬁltering.
The rest of the paper is organized as follows. We ﬁrst
provide necessary background information and clarify what
will be achieved in Section 2. After that, we present the
six features to distinguish spam clusters in Section 3, illus-
trate the detailed system design in Section 4, and thoroughly
evaluate its performance in Section 5. Next, we discuss pos-
sible attempts to evade the system in Section 6 and the re-
lated work in Section 7. Finally, we conclude the paper in
Section 8.
2 Background and Goal
In this section, we provide necessary background infor-
mation and how our system can be incorporated into the
current OSN architecture. We also describe the dataset used
in this study.
2.1 Background
All current OSNs adopt the client-server architecture.
The OSN service provider acts as the controlling entity. It
stores and manages all the content in the system. On the
other hand, the content is generated by users spontaneously
from the client side. The OSN service provider offers a rich
set of well-deﬁned interfaces through which the users can
interact with others. Currently two popular ways of interac-
tion exist. Facebook is representative of OSNs that adopt
the interaction between a pair of sender and recipient as
their primary way of interaction, although they also support
other ways. Twitter is representative of OSNs that adopt
broadcasting as their primary way of interaction.
Figure 1(a) illustrates a simpliﬁed OSN architecture. It
only depicts the components that are related to message
exchanging, while all other functionalities, e.g. , user au-
thentication, video sharing and 3rd party applications, are
omitted. In this simpliﬁed example, multiple users are in-
teracting via the message posting and viewing interface. In
Facebook-like OSNs, it represents the case that user A and
B are posting messages to user C and D, respectively. In
Twitter-like OSNs, it represents the case that user A and
B are broadcasting messages to all the followers includ-
ing user C and D, respectively, while other possible recip-
ients are omitted for simplicity. In both cases, the service
provider mediates all the interactions. The generated mes-
sages are ﬁrst stored at the service provider’s side, and will
be relayed when the corresponding recipient signs in.
Unfortunately, all the content in OSNs is generated by
users and is not necessarily legitimate. The posted mes-
sages could be spam. Note that although spam traditionally
refers to massive, unsolicited campaigns trying to promote
products or services, we do not restrict ourselves to this be-
havior alone. Rather, we use the term “spam” to refer to
unsolicited campaigns that attempt to execute a variety of
attacks, including but not restricted to: i) product adver-
tisements, ii) phishing and iii) malware spreading. In the
example of Figure 1(a), user A’s account is compromised
and sends a spam message to user C, trying to direct user C
to some malicious website. Once user C signs in, the spam
message will be displayed to him, exposing him to potential
threats.
2.2 Goal
Our goal is to design an online spam ﬁltering system that
is deployed at the OSN service provider side, as Figure 1(b)
shows. Once deployed, it inspects every message before
rendering the message to the intended recipients and makes
immediate decision on whether or not the message under
inspection should be dropped. In this particular example,
the message generated by user A is classiﬁed as spam and
is thus dropped instantly. The message from user B is legit-
imate and is stored by the service provider. Later when user
C and D sign in to the system, C will not see the dropped
spam message.
We assume the spam message originates from a compro-
mised account in the above example. We stress that this is
not an assumption that our system relies on. Rather, our sys-
tem can detect spam messages originating from both com-
promised accounts and spamming bots.
2.3 Dataset
Facebook and Twitter are two representative OSNs. We
use data collected from both sites in the study. The Face-
book dataset contains 187 million wall posts generated by
roughly 3.5 million users in total, between January of 2008
and June of 2009 [29]. For the Twitter data collection, we
ﬁrst download trending topics, i.e. , popular topics, from the
website What the Trend [2], which provides a regularly up-
dated list of trending topics. We then download from Twit-
ter all public tweets that contain the trending topics while
the topics are still popular via Twitter APIs. For example,
while the topic “MLB” is trending, we keep downloading
all tweets that contain the word “MLB” from Twitter. For
each tweet we also obtain the userID that generates it along
with its friend number, i.e. , the number of users it follows.
the Twitter dataset contains over 17 million tweets related
to trending topics that were generated between June 1, 2011
and July 21, 2011. The primary form of communication in
Facebook and Twitter is called “wall post” and “tweet”, re-
spectively. From now on, we use the term “message” to
refer to both of them for the ease of description.
We need labeled spam and legitimate messages to train
and evaluate the system. For the Facebook dataset, we use
the result of one previous study [9], where 199,782 wall
posts are conﬁrmed as spam. We further scan through the
entire dataset looking for additional wall posts that share ei-
ther identical textual description or URL with the conﬁrmed
spam wall posts. We label the newly found wall posts as
spam, too. At last we have 217,802 wall posts labeled as
spam. For the Twitter dataset, we visit all the embedded
links. In Twitter, all the embedded links sit behind a URL
shortening service. We label a URL as malicious if the URL
shortening service stops serving the URL due to a policy vi-
olation. All Tweets containing the malicious URLs are la-
beled as spam. In total 467,390 tweets are labeled as spam.
Note that in general, we lack comprehensive ground truth.
Although all the messages labeled as spam are truly spam,
some spam might be missed and labeled as legitimate mes-
sage. Thus, we may somewhat underestimate false negative
rate (spam that the system fails to report) during the evalu-
ation, and overestimate false positive rate (legitimate mes-
sages misclassiﬁed as spam).
A
B
C D
Users
A
B
C D
Users
To C:
Goto evil.com!
To D:
Lol so funny!!
From A:
Goto evil.com!
From B:
Lol so funny!!
Interfaces
To C:
Goto evil.com!
To D:
Lol so funny!!
From B:
Lol so funny!!
Interfaces
OSN Service 
Provider
Online Spam 
Filtering
OSN Service 
Provider
Storage
Storage
(a) The scenario without the deployment of our system.
(b) The scenario with the deployment of our system.
Figure 1: A simpliﬁed OSN architecture, only illustrating the components related to the message exchanging functionality.
User A represents the account controlled by spammers.
3 Features
We ﬁrst brieﬂy review some features used in existing
spam detection work but are not suitable for the OSN en-
vironment. After that, we investigate various features that
can potentially differentiate between spam and legitimate
clusters. Note that not all features have never been used be-
fore. For each feature, we conduct experiment with the la-
beled Facebook dataset to show their effectiveness. In this
section, we cluster the spam and legitimate messages sepa-
rately to obtain the “pure” clusters, using the approach that
is given out in Section 4, which provides the system design
detail.
We organize the features into two categories. OSN spe-
ciﬁc features are those that need social network information
to compute. General features are those that could also be
used to detect spam outside OSNs. No single feature is per-
fectly discriminative between spam and legitimate clusters.
Each single feature is likely to fail in some scenario. In-
stead, these features are used in combination to train the
best classiﬁer.
3.1 Obsolete Features
Supervised machine learning has been a popular tool for
spam detection. Numerous features have been proposed and
work particularly well. However, OSN is a different context
where the features’ effectiveness needs a reevaluation. Note
that we do not review all the previously proposed features.
Instead, we cover the features used in two state-of-the-art
works that detect spam in email [11] and forum [23], be-
lieving those features are the most advanced ones. Next, we
present the ones found to be problematic.
Message size is a popularly used feature, because legit-
imate emails have variable sizes while spams tend to be
small. We measure the size of the spam and legitimate
messages in our dataset, and present the distribution in Fig-
ure 2. Apparently both types of messages in OSNs are
small. Their distribution heavily overlaps in the area of less
than 200 bytes. Consequently, for the majority of messages
the size is not a distinguishing feature. Two similar features
are the number of words and the average word length in
each message. We present their distribution in Figure 3 and
Figure 4, respectively. The observation is consistent with
the message size feature. Neither of them would remain
effective in OSNs.
A series of network based features, which are sender
IP neighborhood density, sender AS number and status of
sender’s service ports, have also been proposed. Although
we cannot measure them straightforwardly due to the lack
of such information in our dataset, we still ﬁnd it problem-