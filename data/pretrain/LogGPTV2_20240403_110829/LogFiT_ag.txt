B. Training ObjectiveAs mentioned previously, the LogFiT model is trained in a self-supervised manner using masked token prediction. This training objective is modified version of the self-supervised masked language modeling (MLM) training objective used to pre-train BERT-based language models [21]. The LogFiT model randomly chooses up to 50% of the sentences that constitute the log paragraph for masking, instead of random tokens from all the sentences of the log paragraphs as is the case with BERT. Next, the chosen log sentence’s tokens are masked following the BERT masking algorithm, with 80% of the tokens being masked while the rest are left unmodified. Afterward, the model is tasked with predicting what the masked tokens were.The rationale for using masked token prediction is that the model needs to learn the contextual relationships between tokens and sentences in the training data, in order to accurately
5
predict the masked tokens. This enables the model to develop an understanding of the language rules used by normal system logs. As a result, it can differentiate normal log data from anomaly log data.To 	implement 	the 	masked 	token 	prediction 	training objective, the model minimises the cross-entropy loss between its masked token predictions and the actual tokens. The computation of the cross-entropy loss for a mini-batch of log data is shown in Equation 1. The LogFiT model minimises the training loss using the Adam optimiser, with hyper parameters values adopted from FastAI defaults: momentum = 0.9, sqr momentum = 0.99, epsilon = 1e-5, and weight decay = 0.01.Loss = −1yj maskilog(pj maski) 	(1)
where b is size of the the mini-batch, m is the number of masked tokens, y and p are the true and predicted values, respectively.To speed up the model training, LogFiT adopts super-convergence techniques introduced by Smith and Topin [30] and implemented in the FastAI/ULMFiT framework [31]. The super-convergence techniques are: (i) discriminative fine-tuning, in which different learning rates are used for different layer groups of the model, (ii) slanted triangular learning rates and one-cycle learning rates, in which the learning rate and momentum are optimally set during training, and (iii) gradual unfreezing, in which the model’s weights are trained one layer group at a time. While ULMFiT is primarily for LSTM architectures, the techniques that it embodies have been validated to be effective for Transformer-based architectures as well [32], [33]. Gururangan, Marasovi´c, Swayamdipta, et al. [32] have shown that task-adaptive pre-training (i.e. ”fine-tuning”) lead to significant (2% on average) performance gains, while Kumar, Raghunathan, Jones, et al. [33] showed that the ”gradual unfreezing” approach mitigates the observed phenomenon of reduced effectiveness when LMs are fine-tuned, due to the dissipation of the pretrained weights.C. Anomaly Detection
The LogFiT model, which is exclusively trained on normal data, can then be used to detect abnormal log data. During the inference stage, log paragraphs are tokenised, vectorised, masked, and reconstructed in the same way as during training. To determine whether a log paragraph is anomalous, LogFiT uses a technique adopted from DeepLog and LogBERT, where the top-k accuracy (with k ranging from 5 to 15) is used as an anomaly score. The accuracy score indicates how effectively LogFiT can restore the masked sentences in the input data. If the model’s top-k accuracy falls below a certain threshold (ranging from 0.8 to 0.99 in the experiments), the log paragraph is classified as anomalous, while a score above the threshold indicates that it is normal.IV. EXPERIMENTS
	In 	this 	section 	the 	datasets, 	experimental 	setup 	and implementation details are described. Subsequently, the results of running the experiments are evaluated.
A. Experimental SetupDatasets. The LogFiT model is trained and evaluated using three public datasets: HDFS, BGL and Thunderbird. These are the same datasets used by the baseline models that LogFiT is compared against. It is important to note that while these datasets are labeled, LogFiT only uses the labels to validate the performance of the model during training and evaluation. It is expected that in typical application scenarios, log data are not labeled. Moreover, when LogFiT is deployed, it is designed to operate in online mode rather than batch mode. Consequently, for datasets where the grouping of log sentences is based on time windows (such as BGL and Thunderbird), a time interval of 30 seconds is chosen to ensure that a system that employs LogFiT can deliver prompt feedback to system operators. Table I shows some statistics about the HDFS, BGL and Thunderbird datasets.• HDFS system logs. These logs were generated by an installation of Hadoop Distributed File System (HDFS) [34]. The log dataset contains both normal log events and anomaly log events, which have been manually tagged by domain experts. In the HDFS dataset, anomaly system events represent abnormal file system operations as specified by the flow of blocks across the HDFS cluster. Example types of anomalies are ”Replica immediately deleted”, and ”Namenode not updated after deleting block”. The HDFS dataset is chunked into log paragraphs using the HDFS block ID, which represents a session in HDFS. The full HDFS dataset consists of 11,175,629 log sentences, of which 16,838 are anomalies.• BGL system logs. These logs were produced by the Blue Gene/L supercomputer system installed at the Lawrence Livermore National Laboratory (LLNL) [35]. The dataset contains suitably labeled normal and alert events, with the alert events considered as anomalies. The BGL dataset does not have a session ID with which to group the data, thus the dataset is chunked into log paragraphs, each consisting of the consecutive log sentences belonging to a time window of 30 seconds, following the approach discussed in [5], [7]. A log paragraph is considered an anomaly if it includes at least one log sentence that is tagged as an anomalous event. The full BGL dataset consists of 4,747,963 log sentences, of which 348,460 are anomalies.• Thunderbird system logs. These logs were created by the Thunderbird supercomputer system operated by Sandia National Laboratories (SNL) [35]. Similar to the BGL system logs, the Thunderbird system logs contain data about the activities and events that occurred within the supercomputer system. The dataset has also been labeled in the same manner as the BGL data set. As with the BGL dataset, the Thunderbird dataset does not have a session ID, so the log sentences are6
| Dataset | Avg Word Count | Avg Sentence Count | #Unique Words |