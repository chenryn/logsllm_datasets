8As before, we can replace this functionality with a two-party computation.
25
The proof of security is not substantially diﬀerent than in the previous sec-
tion, so we only give an intuition here.
Instead of using random strings to
simulate secret shares, we now rely on the semantic security of the encryp-
tion scheme. When simulating Alice’s view, for each u ∈ Vertices, the leakage
function is used to determine how many times the identiﬁer for u should be
encrypted. The rest of the ciphertexts can be simulated with encryptions of 0
strings. The rest of the simulation is straightforward.
When simulating Bob’s view, an interesting subtlety arises. Even though Bob
does not get to see the access pattern to the vertices during the Gather and
Scatter operations, he does in fact still learn F(DBR). This is because the in-
stantiation of Ffunc with a secure computation will leak the input size of Alice
(assuming we use a generic two-party computation for realizing the function-
ality). This reveals the number of data items that were moved to that vertex
during Gather.9 These input sizes can be exactly simulated using the leakage
function.
Implementation and Evaluation
6
In this section, we describe and evaluate the implementation of our proposed
framework. We implement OblivGraph using FlexSC, a Java-based garbled
circuit framework. We measure the performance of our framework on a set
of benchmark algorithms in order to evaluate our design. These benchmarks
consist of histogram, PageRank and matrix factorization problems which are
commonly used for evaluating highly-parallelizable frameworks. In all scenar-
ios, we assume that the data is secret-shared across two non-colluding cloud
providers, as motivated in Section 1. For comparison, we compare our results
with the closest large-scale secure parallel graph computation, called GraphSC
[28].
Implementation
6.1
Using the OblivGraph framework, the histogram and matrix factorization prob-
lems can be represented as directed bipartite graphs, and PageRank as a directed
non-bipartite graph. When we are computing on bipartite graphs, if we consider
Deﬁnition 5 where we aim to hide the in-degree of the nodes (nodes on the left
have in-degree 0), the growth rate of dummy edges is linear in the number of
nodes on the right and it is independent of the real edges or users. If we con-
sider the stronger Deﬁnition 6, the growth rate of dummy edges is linear with
max(users, items).
Histogram: In histogram, left vertices represent data elements, right vertices
are the counters for each type of data element, and existence of an edge indicates
that the data element on the left has the type on the right.
9If Bob knew how many dummy edges have the form (∗, v), he could immediately deduce
in-deg(v); this is why DumGenp,α is still executed by an ideal functionality, and not entrusted
to Bob.
26
Edge data is represented as (u, v, uData, vData, isReal), and vertex data is
represented as (u, uData). We assume that each server holds and encryp-
tion key pair, (skAlice, pkAlice) and (skBob, pkBob), and that the public keys
are known to the data owners at the time the data is uploaded.
Input Preparation:
Users encrypt their edge data under Alice’s public key, then under Bob’s
public key, and upload the data to Bob:(cid:74)(cid:74)RealEdges(cid:75)Alice(cid:75)Bob. (We assume
that the 4 data elements in the edge are encrypted separately.)
Dummy Generation: The parties call the ideal
functionality for
DumGenp,α. The functionality is just as described in either the mid-
dle or right of Figure 2, except that we modify the format of the output.
Instead of providing XOR shares of the output, hDummyEdgesi, the func-
tionality is assumed to return a doubly encrypted array of dummy edges,
cording to a single random permutation, p. He re-randomizes the
(outer) ciphertexts and sends the encrypted arrays to Alice.
(cid:74)(cid:74)DummyEdges(cid:75)Alice(cid:75)Bob.
1. Shuffle: Bob randomly permutes the arrays (cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob ac-
2. Gather: For each edge in(cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob, Bob decrypts the outer
ciphertext of the right vertex id, re-randomizes (cid:74)Edges.v(cid:75)Alice, and
sends it to Alice. For each edge in (cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob, Alice recovers
tionality. Alice provides(cid:74)(cid:74)Vertices.vData(cid:75)Alice(cid:75)Bob, and both parties
tex data as output, still denoted by(cid:74)(cid:74)Vertices.vData(cid:75)Alice(cid:75)Bob.
permuting(cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob according to a random permutation p0.
5. Scatter: For each edge in(cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob, Bob decrypts the outer
ciphertext of the left vertex id, re-randomizes (cid:74)Edges.u(cid:75)Alice, and
sends it to Alice. For each edge in (cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob, Alice re-
(cid:74)(cid:74)Edges(cid:75)Alice(cid:75)Bob back to Bob.
covers Edges.u and copies the encrypted vertex data at u to the
corresponding edge. She re-randomizes all ciphertexts, and sends
He re-randomizes the (outer) ciphertexts, and sends the encrypted
array to Alice.
4. Shuffle: Bob executes the second shuﬄing operation by randomly
Edges.v and copies the encrypted edge data to vertex v.
3. Apply: For each vertex, Alice and Bob query a modiﬁed Ffunc func-
provide their secret keys. Alice receives updated, re-encrypted ver-
GAS operations in a single iteration:
Figure 5: An O(|E|) protocol for OblivGraph.
Matrix Factorization:
In matrix factorization, left vertices represent the
users, right vertices are items (e.g. movies in movie recommendation systems),
an edge indicates that a user ranked an item, and the weight of the edge repre-
sents the rating value.
PageRank: In PageRank, each vertex corresponds to a webpage and each
edge is a link between two webpages. The vertex data comprises of two real
27
values, one for the PageRank of the vertex and the other for the number of
its outgoing edges. Edge data is a real value corresponding to the weighted
contribution of the source vertex to the PageRank of the sink vertex.
Vertex and Edge representation: In all scenarios, vertices are identiﬁed us-
ing 16-bit integers and 1 bit is used to indicate if the edge is real or dummy.
For Histograms, we use an additional 20 bits to represent the counter values.
In PageRank, we represent the PageRank value using a 40-bit ﬁxed-point rep-
resentation, with 20-bits for the fractional part.
In our matrix factorization
experiments, we factorized the matrix to user and movie feature vectors; each
vector has dimension 10, and each value is represented as 40-bit ﬁxed-point num-
ber, with 20-bits for the fractional part. We chose these values to be consistent
with GraphSC representation.
System setting: We conduct experiments on both a lab testbed, and on a
real-world scale Amazon AWS deployment. Our lab testbed comprises 8 virtual
machines each with dedicated (reserved) hardware of 4 CPU cores (2.4 GHz)
and 16 GB RAM. These VMs were deployed on a vSphere Cluster of 3 physical
servers and they were interconnected with 1Gbps virtual interfaces. We run our
experiments on p ∈ {1, 2, 4, 8, 16, 32} pairs of these processors, where in each
pair, one processor works as the garbler, and the other as the evaluator. Each
processor can be implemented by a core in a multi-core VM, or can be a VM in
our compute cluster.
6.2 Evaluation
We use two metrics in evaluating the impact of our security relaxation: circuit
complexity (e.g. # of AND gates), and runtime. Counting AND gates provides
a “normalized” comparison with other frameworks, since circuit size is inde-
pendent of the hardware conﬁguration and of the chosen secure computation
implementation. However, it is also nice to have a sense of concrete runtime,
so we provide this evaluation as well. Of course, runtime is highly aﬀected by
the choice of hardware, and ours can be improved by using more processors or
dedicated hardware (e.g. AES-NI).
Evaluation setting: For the LAN setup, we use synthesized data and run all
the benchmarks with the similar set of parameters that have been used in the
GraphSC framework. In our histogram and matrix factorization experiments,
we run the experiments for 2048 users and 128 items. The number of nodes in
our PageRank experiment is set to be 2048.
For real world experiment using AWS, we run matrix factorization using
gradient descent on the real-world MovieLens dataset that contains 1 million
ratings provided by 6040 users to 3883 movies [17] on 2 m4.16xlarge AWS in-
stances on the Northern Virginia Data-center.
Circuit Complexity: The results presented in Figures 6a, 6b and 6c are for
execution on a single processor, to show the performance of our design without
leveraging the desired eﬀect of parallelization.
28
Histogram: Figure 6a demonstrates the number of AND gates for computing
histogram in both the GraphSC and OblivGraph frameworks. With 2048 data
elements and 128 data types, we always do better than GraphSC when  >= 0.3.
When  = 0.1, we start outperforming GraphSC when there are at least 3400
edges.
Matrix Factorization:
In Figure 6b, we use the (batch) gradient decent
method for generating the recommendation model, as in [29, 28]. With 2048
users, 128 items, and  = 0.3, we outperform GraphSC once there are at least
15000 edges. When  = 0.1, we start outperforming them on 54000 edges. We
always do better than GraphSC when the  = 1 or higher.
PageRank: Figure 6c provides the result of running PageRank in our frame-
work with 2048 nodes and diﬀerent values of . With  = 0.3, we outperform
GraphSC when the number of edges are about 400000, and with  = 1 we out-
perform them on just 130000 edges. In both cases, the graph is quite sparse,
compared to a complete graph of 2 million edges. Note, though, that our com-
parison is slightly less favorable for this computation. Recall, the number of
dummy edges grow with the number of nodes in the graph, and, when hid-
ing only in-degree in a bipartite graph, this amounts to growing only with the
number of nodes on the right. In contrast, the runtime of GraphSC grows equiv-
alently with any increase in users, items, or edges, because their protocol hides
any distinction between these data types. We therefore compare best with them
when there are more users than items. When looking at a non-bipartite graph,
such as PageRank, our protocol grows with any increase in the size of the singu-
lar set of nodes, just as theirs does. If we increase the number of items in matrix
factorization to 2048, or decrease the number of nodes in PageRank to 128, the
comparison to GraphSC in the resulting experiments would look similar. We let
the reader extrapolate, and avoid the redundancy of adding such experiments.
Large scale experiments on Amazon AWS: OblivGraph factorizes the
MovieLens recommendation matrix consist of 1 million ratings provided by 6040
users to 3883 movies, in almost 2 hours while GraphSC does it in 13 hours. We
provide results of computing matrix factorization problem for diﬀerent values
of  and diﬀerent numbers of ratings in Table 2. We outperform the best result
achieved by GraphSC, using 128 processors and 1M ratings.
Eﬀect of Parallelization: Figure 7 illustrates that the execution time can
be signiﬁcantly reduced through parallelization. We achieve nearly a linear
speedup in the computation time. The lines corresponds to two diﬀerent num-
bers of edges for 2048 users and 128 movies. Since in our these problems, the
computation is the bottleneck, parallelization can signiﬁcantly speed up the
computation process. Table 1 shows the eﬀect of parallelization in our frame-
work as compared to GraphSC in terms of number of AND gates. As shown in
the Table 1, adding more processors in the GraphSC framework increases the
total number of AND Gates by some small amount. In contrast, the size of the
circuit generated in our framework is constant in the number of processors: par-
allelization does not aﬀect total number of AND gates in the OblivGraph GAS
operations, or in DumGen.
29
(a) Histogram
(b) Matrix factorization
(c) PageRank
Figure 6: Histogram and Matrix Factorization with 2048 users and 128 types,
PageRank with 2048 webpages, with varying 
30
Figure 7: Eﬀect of parallelization on Matrix Factorization computation time
GraphSC [28]
OblivGraph
Processors
1
2
4
8
|E| = 8192
4.047E + 09
4.055E + 09
4.070E + 09
4.092E + 09
|E| = 24576
1.035E + 10
1.039E + 10
1.046E + 10
1.057E + 10
|E| = 8192
2.018E + 09
2.018E + 09
2.018E + 09
2.018E + 09
|E| = 24576
4.480E + 09
4.480E + 09
4.480E + 09
4.480E + 09
Table 1: Cost of Parallelization on OblivGraph vs. GraphSC in computing
Matrix Factorization
Optimization using Compaction: It is important to note that the measured
circuit sizes in our OblivGraph experiments correspond to the worst-case sce-
nario in which the number of dummy edges are equal to d = 2α|V |, which is the
maximum number of dummies per type. Consequently the time for OblivShuﬄe
is its maximum value. However, looking at the geometric distribution used in
the DumGen procedure, the expected number of dummy edges is α|V |, so half of
the dummy items are unnecessary. Removing these extra dummy items during
DumGen is non-trivial, because, while it is safe to reveal the total number of
dummy items in the system, revealing the number of dummy items of each type
would violate diﬀerential privacy. After the ﬁrst iteration of the computation,
once the dummy items are shuﬄed in with the real items, an extra ﬂag marking
the excessive dummy items can be used to safely remove them from the sys-
tem; this optimization can signiﬁcantly reduce the shuﬄing time (roughly by