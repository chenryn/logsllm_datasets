nary: a program that runs with full system privilege.
While comparable to Capsicum’s capability mode in
terms of intent, this model suffers signiﬁcant sandboxing
weakness (for example, permitting full access to the Sys-
tem V shared memory as well as all operations on passed
ﬁle descriptors), and comes at the cost of an additional
setuid-root binary that runs with system privilege.
5.3 MAC OS X Seatbelt
On Mac OS X, Chromium uses a MAC-based framework
for creating sandboxes. This allows Chromium to create
a stronger sandbox than is possible with DAC, but the
rights that are granted to render processes are still very
broad, and security policy must be speciﬁed separately
from the code that relies on it.
The Mac OS X Seatbelt sandbox system allows pro-
cesses to be constrained according to a LISP-based pol-
icy language [1]. It uses the MAC Framework [27] to
check application activities; Chromium uses three poli-
cies for different components, allowing access to ﬁlesys-
tem elements such as font directories while restricting
access to the global namespace.
Like other techniques, resources are acquired before
constraints are imposed, so care must be taken to avoid
leaking resources into the sandbox. Fine-grained ﬁlesys-
tem constraints are possible, but other namespaces such
as POSIX shared memory, are an all-or-nothing affair.
The Seatbelt-based sandbox model is less verbose than
other approaches, but like all MAC systems, security pol-
icy must be expressed separately from code. This can
lead to inconsistencies and vulnerabilities.
5.4 SELinux
Chromium’s MAC approach on Linux uses an SELinux
Type Enforcement policy [12]. SELinux can be used
for very ﬁne-grained rights assignment, but in practice,
broad rights are conferred because ﬁne-grained Type En-
forcement policies are difﬁcult to write and maintain.
The requirement that an administrator be involved in
deﬁning new policy and applying new types to the ﬁle
system is a signiﬁcant inﬂexibility: application policies
cannot adapt dynamically, as system privilege is required
to reformulate policy and relabel objects.
The Fedora reference policy for Chromium creates a
single SELinux dynamic domain, chrome sandbox t,
which is shared by all sandboxes, risking potential in-
terference between sandboxes. This domain is assigned
broad rights, such as the ability to read all ﬁles in /etc
and access to the terminal device. These broad policies
are easier to craft than ﬁne-grained ones, reducing the
impact of the dual-coding problem, but are much less ef-
fective, allowing leakage between sandboxes and broad
access to resources outside of the sandbox.
In contrast, Capsicum eliminates dual-coding by com-
bining security policy with code in the application. This
approach has beneﬁts and drawbacks: while bugs can’t
arise due to potential inconsistency between policy and
code, there is no longer an easily accessible speciﬁcation
of policy to which static analysis can be applied. This
reinforces our belief that systems such as Type Enforce-
ment and Capsicum are potentially complementary, serv-
ing differing niches in system security.
5.5 Linux seccomp
Linux provides an optionally-compiled capability mode-
like facility called seccomp. Processes in seccomp
mode are denied access to all system calls except read,
write, and exit. At face value, this seems promis-
ing, but as OS infrastructure to support applications us-
ing seccomp is minimal, application writers must go to
signiﬁcant effort to use it.
In order to allow other system calls, Chromium
constructs a process in which one thread executes in
seccomp mode, and another “trusted” thread sharing
the same address space has normal system call access.
Chromium rewrites glibc and other library system call
vectors to forward system calls to the trusted thread,
where they are ﬁltered in order to prevent access to inap-
propriate shared memory objects, opening ﬁles for write,
etc. However, this default policy is, itself, quite weak, as
read of any ﬁle system object is permitted.
The Chromium seccomp sandbox contains over a
thousand lines of hand-crafted assembly to set up sand-
boxing, implement system call forwarding, and craft a
basic security policy. Such code is a risky proposition:
difﬁcult to write and maintain, with any bugs likely lead-
ing to security vulnerabilities. The Capsicum approach
is similar to that of seccomp, but by offering a richer set
of services to sandboxes, as well as more granular dele-
gation via capabilities, it is easier to use correctly.
6 Performance evaluation
Typical operating system security benchmarking is tar-
geted at illustrating zero or near-zero overhead in the
hopes of selling general applicability of the resulting
technology. Our thrust is slightly different: we know
that application authors who have already begun to adopt
compartmentalisation are willing to accept signiﬁcant
overheads for mixed security return. Our goal is there-
fore to accomplish comparable performance with signif-
icantly improved security.
We evaluate performance in two ways: ﬁrst, a set
of micro-benchmarks establishing the overhead intro-
duced by Capsicum’s capability mode and capability
primitives. As we are unable to measure any notice-
able performance change in our adapted UNIX applica-
tions (tcpdump and dhclient) due to the extremely low
cost of entering capability mode from an existing pro-
cess, we then turn our attention to the performance of
our libcapsicum-enhanced gzip.
All performance measurements have been performed
on an 8-core Intel Xeon E5320 system running at
1.86GHz with 4GB of RAM, running either an unmod-
iﬁed FreeBSD 8-STABLE distribution synchronised to
revision 201781 (2010-01-08) from the FreeBSD Sub-
version repository, or a synchronised 8-STABLE distri-
bution with our capability enhancements.
6.1 System call performance
First, we consider system call performance through
micro-benchmarking. Figure 13 summarises these re-
sults for various system calls on unmodiﬁed FreeBSD,
and related capability operations in Capsicum. Fig-
ure 14 contains a table of benchmark timings. All micro-
benchmarks were run by performing the target operation
in a tight loop over an interval of at least 10 seconds,
repeating for 10 iterations. Differences were computed
using Student’s t-test at 95% conﬁdence.
Our ﬁrst concern is with the performance of capabil-
ity creation, as compared to raw object creation and the
closest UNIX operation, dup. We observe moderate, but
expected, performance overheads for capability wrap-
ping of existing ﬁle descriptors: the cap new syscall is
50.7% ± 0.08% slower than dup, or 539 ± 0.8ns slower
in absolute terms.
Next, we consider the overhead of capability “un-
wrapping”, which occurs on every descriptor operation.
We compare the cost of some simple operations on raw
ﬁle descriptors, to the same operations on a capability-
wrapped version of the same ﬁle descriptor: writing a
Figure 13: Capsicum system call performance compared to standard UNIX calls.
single byte to /dev/null, reading a single byte from
/dev/zero; reading 10000 bytes from /dev/zero; and
performing an fstat call on a shared memory ﬁle de-
scriptor.
In all cases we observe a small overhead of
about 0.06µs when operating on the capability-wrapped
ﬁle descriptor. This has the largest relative performance
impact on fstat (since it does not perform I/O, simply
inspecting descriptor state, it should thus experience the
highest overhead of any system call which requires un-
wrapping). Even in this case the overhead is relatively
low: 10.2% ± 0.5%.
stance of message passing increases that cost further.
We also found that additional dynamically linked li-
brary dependencies (libcapsicum and its dependency
on libsbuf) impose an additional 9% cost to the fork
syscall, presumably due to the additional virtual mem-
ory mappings being copied to the child process. This
overhead is not present on vfork which we plan to use
in libcapsicum in the future. Creating, exchanging an
RPC with, and destroying a single sandbox (the “sand-
box” label in Figure 13(b)) has a cost of about 1.5ms,
signiﬁcantly higher than its subset components.
6.2 Sandbox creation
6.3
gzip performance
Capsicum supports ways to create a sandbox: directly in-
voking cap enter to convert an existing process into a
sandbox, inheriting all current capability lists and mem-
ory contents, and the libcapsicum sandbox API, which
creates a new process with a ﬂushed capability list.
cap enter performs similarly to chroot, used by
many existing compartmentalised applications to restrict
ﬁle system access. However, cap enter out-performs
setuid as it does not need to modify resource limits.
As most sandboxes chroot and set the UID, entering a
capability mode sandbox is roughly twice as fast as a tra-
ditional UNIX sandbox. This suggests that the overhead
of adding capability mode support to an application with
existing compartmentalisation will be negligible, and re-
placing existing sandboxing with cap enter may even
marginally improve performance.
Creating a new sandbox process and replacing its ad-
dress space using execve is an expensive operation.
Micro-benchmarks indicate that the cost of fork is three
orders of magnitude greater than manipulating the pro-
cess credential, and adding execve or even a single in-
While the performance cost of cap enter is negli-
gible compared to other activity,
the cost of multi-
process sandbox creation (already taken by dhclient
and Chromium due to existing sandboxing) is signiﬁcant.
To measure the cost of process sandbox creation, we
timed gzip compressing ﬁles of various sizes. Since the
additional overheads of sandbox creation are purely at
startup, we expect to see a constant-time overhead to the
capability-enhanced version of gzip, with identical lin-
ear scaling of compression performance with input ﬁle
size. Files were pre-generated on a memory disk by read-
ing a constant-entropy data source: /dev/zero for per-
fectly compressible data, /dev/random for perfectly in-
compressible data, and base 64-encoded /dev/random
for a moderate high entropy data source, with about 24%
compression after gzipping. Using a data source with ap-
proximately constant entropy per bit minimises variation
in overall gzip performance due to changes in compres-
sor performance as ﬁles of different sizes are sampled.
The list of ﬁles was piped to xargs -n 1 gzip -c
> /dev/null, which sequentially invokes a new gzip
 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5dupcap_newshmfdcap_new_shmfdfstat_shmfdfstat_cap_shmfdwritecap_writeread_1cap_read_1read_10000cap_read_10000getuidchrootsetuidcap_enterTime/syscall (us) 0 200 400 600 800 1000 1200 1400 1600forkvforkpdforkfork_execvfork_execpdfork_execpingpongsandboxTime/syscall (us)Benchmark
dup
cap new
shmfd
cap new shmfd
fstat shmfd
fstat cap shmfd
read 1
cap read 1
read 10000
cap read 10000
write
cap write
cap enter
getuid
chroot
setuid
fork
vfork
pdfork
pingpong
fork exec
vfork exec
pdfork exec
sandbox
-
-
-
-
-
-
-
-
-
Difference
% difference
Time/operation
1.061 ± 0.000µs
50.7% ± 0.08%
0.539 ± 0.001µs
1.600 ± 0.001µs
2.385 ± 0.000µs
74.4% ± 0.181%
1.77 ± 0.004µs
4.159 ± 0.007µs
0.532 ± 0.001µs
10.2% ± 0.506%
0.054 ± 0.003µs
0.586 ± 0.004µs
0.640 ± 0.000µs
8.93% ± 0.143%
0.057 ± 0.001µs
0.697 ± 0.001µs
1.534 ± 0.000µs
4.40% ± 0.139%
0.067 ± 0.002µs
1.601 ± 0.003µs
0.576 ± 0.000µs
0.058 ± 0.001µs
10.0% ± 0.241%
0.634 ± 0.002µs
1.220 ± 0.000µs
−0.867 ± 0.001µs −71.0% ± 0.067%
0.353 ± 0.001µs
−0.006 ± 0.000µs −0.458% ± 0.023%
1.214 ± 0.000µs
0.170 ± 0.001µs
14.0% ± 0.054%
1.390 ± 0.001µs
268.934 ± 0.319µs
−224.3 ± 0.217µs −83.4% ± 0.081%
44.548 ± 0.067µs
259.359 ± 0.118µs
−3.56% ± 0.120%
−9.58 ± 0.324µs
309.387 ± 1.588µs
15.0% ± 0.400%
40.5 ± 1.08µs
811.993 ± 2.849µs
585.830 ± 1.635µs −226.2 ± 2.183µs −27.9% ± 0.269%
862.823 ± 0.554µs
6.26% ± 0.348%
85.9% ± 0.339%
1509.258 ± 3.016µs
50.8 ± 2.83µs
697.3 ± 2.78µs
-
-
-
-
-
-
-
-
-
Figure 14: Micro-benchmark results for various system calls and functions, grouped by category.
compression process with a single ﬁle argument, and dis-
cards the compressed output. Sufﬁciently many input
ﬁles were generated to provide at least 10 seconds of re-
peated gzip invocations, and the overall run-time mea-
sured. I/O overhead was minimised by staging ﬁles on
a memory disk. The use of xargs to repeatedly invoke
gzip provides a tight loop that minimising the time be-
tween xargs’ successive vfork and exec calls of gzip.
Each measurement was repeated 5 times and averaged.
Benchmarking gzip shows high initial overhead,
when compressing single-byte ﬁles, but also that the ap-
proach in which ﬁle descriptors are wrapped in capabil-
ities and delegated rather than using pure message pass-
ing, leads to asymptotically identical behaviour as ﬁle