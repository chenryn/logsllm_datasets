π∈Exλ([S1,S2])
4 In Section 4.4, we discuss letting C1 and C2 be sets of system call sequences, or
phrases. For simplicity of exposition, however, we describe our algorithms assuming
C1 and C2 are sets of individual system calls.
Behavioral Distance Measurement Using Hidden Markov Models
25
In addition, we deﬁne the most probable execution generating [S1, S2] to be
arg max
π∈Exλ([S1,S2])
Prλ(π)
When convenient, we will use t to denote an iteration counter, i.e., the number
of iterations of Step 2 in the experiment above that have been executed. So, for
example, when we say that λ is “in state qi after t iterations”, this means that
after t iterations have been completed in the experiment, qi is the current state.
Trivially, q0 is the state after t = 0 iterations, and if the state is qN+1 after t
iterations, then execution halts (i.e., there is no iteration t + 1).
4.2 Computing Prλ([S1, S2])
Prλ([S1, S2]) is the probability that system call sequences S1 and S2 are gener-
ated (in the sense of Section 4.1) by the HMM λ, which is used as the behavioral
distance between S1 and S2. If Prλ([S1, S2]) is greater than a threshold value,
the system call sequences will be considered as normal, otherwise an alarm is
raised indicating that an anomaly is detected. In this section we describe an
algorithm for computing Prλ([S1, S2]) eﬃciently, given λ, S1, and S2. Again, S1
and S2 would typically be observed from monitoring the processes. How we build
λ itself is the topic of Section 4.3.
Given an HMM λ, there are many ways it can generate S1 and S2, i.e., there
are many diﬀerent executions that yield an alignment of S1 and S2. In fact, if
we assume that ai,j and bi([x, y]) are non-zero for x (cid:6)= σ or y (cid:6)= σ, any state
sequence of suﬃcient length generates an alignment of S1 and S2 with some
non-zero probability. Moreover, even for one particular state sequence, there are
many ways of generating S1 and S2 with σ inserted at diﬀerent locations.
It may ﬁrst seem that to calculate Prλ([S1, S2]) we need to sum the proba-
bilities of all possible executions, and the large number of executions makes the
algorithm very ineﬃcient. However, we can use induction to ﬁnd Prλ([S1, S2]),
−
2 ],
instead. The idea is that if we know the probability of generating [S
−
where S
2 are preﬁxes of S1 and S2, respectively, then Prλ([S1, S2]) can
be found by extending the executions that generate S
−
1 and S
−
1 , S
−
1 and S
≤t
1
To express this algorithm precisely, we introduce the following random vari-
ables in an execution of the HMM λ. Random variable Statet is the state after t
iterations. (It is undeﬁned if the execution terminates in less than t iterations.)
is the sequence of system calls from C1 in the ﬁrst com-
Random variable Out
ponents of the emitted symbols (less σ) through t iterations. That is, if in the
(cid:4)
(cid:4)
(cid:4)
is
2,1], . . . , [s
(up to) t iterations, λ emits [s
1,1, s
1,(cid:4), s
(cid:3) (with their order preserved). Sim-
the sequence of non-σ values in (cid:2)s
(cid:4)
(cid:4)
1,1, . . . , s
1,(cid:4)
(cid:3).
2 would be the non-σ values in (cid:2)s
≤t
ilarly, the random variable Out
Now deﬁne
2,(cid:4)] where (cid:5) ≤ t, then Out
(cid:4)
(cid:4)
(cid:4)
2,1, . . . , s
2,(cid:4)
≤t
1
−
2 .
⎛
α(u, v, i) = Prλ
⎝
(cid:6)
t≥0
(cid:7)
Statet = qi ∧ Out
1 = Pre(S1, u) ∧ Out
≤t
(cid:8)
≤t
2 = Pre(S2, v)
⎞
⎠
26
D. Gao, M.K. Reiter, and D. Song
where Pre(S, u) denotes the u-length preﬁx of S. That is, α(u, v, i) is the prob-
ability of the event that simultaneously qi is the current state, exactly the ﬁrst
u system calls for process 1 have been emitted, and exactly the ﬁrst v system
calls for process 2 have been emitted. Clearly α(u, v, i) is a function of S1, S2,
and λ. Here we do not specify them as long as the context is clear. We solve for
α(u, v, i) inductively, as follows.
Base cases:
α(0, 0, i) =
(cid:11)
1
0
if i = 0
otherwise
α(u, v, 0) =
(cid:11)
1
0
if u = v = 0
otherwise
Induction:
α(u, 0, i) =
α(0, v, i) =
α(u, v, i) =
N(cid:3)
j=0
N(cid:3)
j=0
N(cid:3)
j=0
α(u − 1, 0, j)aj,ibi([s1,u, σ])
α(0, v − 1, j)aj,ibi([σ, s2,v])
for u > 0, i > 0
for v > 0, i > 0
α(u − 1, v, j)aj,ibi([s1,u, σ]) +
α(u, v − 1, j)aj,ibi([σ, s2,v])
N(cid:3)
j=0
+
N(cid:3)
j=0
α(u − 1, v − 1, j)aj,ibi([s1,u, s2,v])
for u, v > 0, i > 0
For example, α(1, 0, i) is the probability that qi is the current state and all
that has been emitted is one system call for process 1 (s1,1) and nothing (except
σ) for process 2. Since bj([σ, σ]) = 0 for all j ∈ {1, . . . , N}, the only possibility
is that q0 transitioned directly to qi, which emitted [s1,1, σ].
As a second example, to solve for α(u, v, i) where u, v > 0, there are three
possibilities, captured in the last equation above:
– The ﬁrst u − 1 and v system calls from S1 and S2, respectively, have been
output, and λ is in some state qj. (This event occurs with probability α(u −
1, v, j).) λ then transitions from qj to qi (with probability aj,i) and emits
[s1,u, σ] (with probability bi([s1,u, σ])).
– The ﬁrst u and v − 1 system calls from S1 and S2, respectively, have been
output, and λ is in some state qj. (This event occurs with probability α(u, v−
1, j).) λ then transitions from qj to qi (with probability aj,i) and emits
[σ, s2,v] (with probability bi([σ, s2,v])).
– The ﬁrst u − 1 and v − 1 system calls from S1 and S2, respectively, have
been output, and λ is in some state qj. (This event occurs with probability
α(u− 1, v− 1, j).) λ then transitions from qj to qi (with probability aj,i) and
emits [s1,u, s2,v] (with probability bi([s1,u, s2,v])).
After α(u, v, i) is solved for all values of u ∈ {0, 1, . . . , l1}, v ∈ {0, 1, . . . , l2},
and i ∈ {1, . . . , N}, where l1 and l2 are the lengths of S1 and S2, respectively,
we can calculate
Behavioral Distance Measurement Using Hidden Markov Models
27
Prλ([S1, S2]) =
N(cid:3)
i=1
α(l1, l2, i)ai,N+1
The solution above solves for Prλ([S1, S2]) from the beginning of the system
call sequences. (That is, α(u, v, i) of smaller u- and v-indices are found before
that of larger u- and v-indices.) It will also be convenient to solve for Prλ([S1, S2])
from the end of the sequences. To do that, we deﬁne
β(u, v, i) = Prλ
⎛
⎝
(cid:6)
(cid:12)
t≥0
Statet = qi ∧ Out>t
1 = Post(S1, u) ∧ Out>t
2 = Post(S2, v)
⎞
⎠
(cid:13)
Here, Post(S, u) denotes the suﬃx of S that remains after removing the ﬁrst u
elements of S. Analogous to the preceding discussion, random variable Out>t
1
is the sequence of system calls from C1 in the ﬁrst components of the emitted
symbols (less σ) in iterations t + 1 onward (if any), and similarly for Out>t
2 .
So, β(u, v, i) is the probability of the event that qi is the current state after
some iterations and subsequently exactly the last l1 − u system calls of S1 are
emitted, and exactly the last l2− v system calls of S2 are emitted. The induction
for β(u, v, i) works in a similar way, and Prλ([S1, S2]) = β(0, 0, 0).
In this algorithm, the number of steps taken to calculate Prλ([S1, S2]) is pro-
portional to l1 × l2 × N 2. Therefore, the proposed algorithm is eﬃcient as the
numbers of system calls and HMM states grow.
4.3 Building λ
In this section we describe how we build the HMM λ. We do so using training
data, that is, pairs [S1, S2] of sequences of system calls recorded from the two
processes when processing the same inputs. Of course, we assume that these
training pairs reﬂect only benign behavior, and that neither process is compro-
mised during the collection of the training samples. We ﬁrst present an algorithm
to adjust the HMM parameters for one training example [S1, S2], and then show
how we combine the results from processing each training sample to adjust the
HMM when there are multiple training samples.
Building λ is a typical expectation-maximization problem. There is no known
way of solving for such a maximum likelihood model analytically; therefore a
reﬁnement procedure is used. The idea is that for each training sample [S1, S2],
we ﬁnd the expected values of certain variables, which can, in turn, be used to
adjust the parameters of λ to increase Prλ([S1, S2]). Here we will demonstrate
this method for updating the ai parameters of λ; a similar treatment for the bi
parameters can be found in Appendix A.
The initial instance of λ is created with a ﬁxed number of states N and random
ai and bi distributions. To update the ai,j parameters in light of a training
sample [S1, S2], we ﬁnd (for the current instance of λ) the expected number
of times λ transitions to state qi when generating [S1, S2], and the expected
number of times it transitions from qi to qj when generating [S1, S2]. To compute
28
D. Gao, M.K. Reiter, and D. Song
these expectations, we ﬁrst deﬁne two conditional probabilities, γ(u, v, i) and
ξ(u, v, i, j) for i ≤ N, j ≤ N + 1, as follows:
γ(u, v, i) = Prλ
ξ(u, v, i, j) = Prλ
⎛
⎛
⎝
⎝
(cid:6)
t≥0
⎛
⎛
⎝
⎝
(cid:6)
t≥0
(cid:15)
⎠
⎞
(cid:14)
(cid:14)
(cid:14)
(cid:14)
1 = Pre(S1, u) ∧
≤t
≤t
2 = Pre(S2, v)
Statet = qi ∧
Out
Out
Statet = qi ∧ Statet+1 = qj ∧
Out
Out
1 = Pre(S1, u) ∧
≤t
≤t
2 = Pre(S2, v)
Out>0
Out>0
1 = S1 ∧
2 = S2
(cid:16)
⎞
⎠
⎞
⎠
(cid:14)
(cid:14)
(cid:14)
(cid:14)
(cid:15)
Out>0
Out>0
1 = S1 ∧
2 = S2
(cid:16)
⎞
⎠
That is, γ(u, v, i) is the probability of λ being in state qi after emitting u system
calls for process 1 and v system calls for process 2, given that the entire sequences
for process 1 and process 2 are S1 and S2, respectively. Similarly, ξ(u, v, i, j) is
the probability of being in state qi after emitting u system calls for process 1 and
v system calls for process 2, and then transitioning to state qj, given the entire
system call sequences for the processes. Each of these conditional probabilities
pertains to one particular subset of executions that generate S1 and S2. As
explained in Section 4.2, there are many executions in the HMM that are able to
generate S1 and S2; out of these executions, there are some that are in state qi
(respectively, transition from qi to qj) after emitting u system calls for process
1 and v system calls for process 2. Note that it may or may not be the case that
[s1,u, s2,v] was emitted by state qi, and that
γ(u, v, i) =
N+1(cid:3)
j=1
ξ(u, v, i, j)
We can calculate these quantities easily as follows:
γ(u, v, i) = α(u, v, i)β(u, v, i)
Prλ([S1, S2])
⎛
ξ(u, v, i, j) =
1
Prλ([S1, S2])
⎝
α(u, v, i)ai,jbj([s1,u+1, σ])β(u + 1, v, j) +
α(u, v, i)ai,jbj([σ, s2,v+1])β(u, v + 1, j) +
α(u, v, i)ai,jbj([s1,u+1, s2,v+1])β(u + 1, v + 1, j)
⎞
⎠
Let the random variable Xi be the number of times that state qi is visited
when emitting [S1, S2]. We calculate the expected value of Xi, denoted E(Xi),
as follows. Let the random variable X u,v
be the number of times that qi is