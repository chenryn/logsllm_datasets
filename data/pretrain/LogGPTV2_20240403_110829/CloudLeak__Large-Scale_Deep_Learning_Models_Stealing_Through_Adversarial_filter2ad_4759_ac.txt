we focus only on those representative attacks here):
Random Sample (RS): For reference, we consider an extreme
case where an attack randomly samples x from related domain
and queries a victim API fv as black-box in order to generate
the synthetic dataset T = {xi, fv(xi)}Nquery
. In this case,
an adversary can use all available images to obtain the best
synthetic dataset and the resulting substitute model. However,
lots of query operations make it easier to be detected by MLaaS
providers.
multiclass(x
)
(cid:48)
i=1
Source exampleMaximum-confidence Adversarial exampleMinimum-confidence Legitimate example()0fx()0fxMinimum-confidence Adversarial exampleFig. 4: Pipeline of the proposed FeatureFool attack method. We ﬁrst input an image and extract the corresponding nth layer
feature mapping by applying the non-linear ﬁlters to the output of last hidden layer. Then we compute the class salience map
to decide which points of feature mapping should be modiﬁed. At last we search for the minimum distortion that satisﬁes the
optimization formula.
x(cid:48)
Projected Gradient Descent (PGD): Madry et al. proposed
the Projected Gradient Descent to effectively generate ad-
versarial examples with multi-step iterations [37]. It exploits
the ﬁrst-order adversary information about the victim neural
network and computes adversarial examples by using the
following equation:
i = Πx+S(cid:0)x(cid:48)
i−1 + α sign (∇xJ(F (x)))(cid:1)
(3)
where ∇ denotes the gradient, F (·) denotes the network output
and J(·) denotes the negative loss function. This attack can be
viewed as a multi-step attack scheme which successfully solves
the inner optimization problem. As such, errors on a legitimate
input x can accumulate and eventually lead to an adversarial
version of this given input that forces the victim network to
output incorrect results, i.e., misclassiﬁcation.
Carlini and Wagner Attack (CW): Carlini et al. [36]
proposed new gradient-based attack algorithms using three
different distance metrics (L0, L2 and L∞ ). In the L2 attack,
they generate adversarial examples by solving the following
optimization problem:
minimize D(x, x + δ) + c · g(x + δ)
such that x + δ ∈ [0, 1]n
(4)
where D(x) denotes the L2 distance function, g(x) denotes
the objective function which can be deﬁned as:
g (x) = max (max{Z (x)i : i (cid:54)= t} − Z (x)t ,−κ)
(5)
where Z(x) denotes the input of the softmax function. As
mentioned in [36], an attacker can easily control the conﬁdence
which adversarial image misclassiﬁcation occurs by carefully
selecting the parameter κ in Equation (5). This technique
allows the L2 attack to effectively craft those “informative”
examples which lie approximately on the decision boundary
of the victim classiﬁer. Hence, we mainly consider using the
L2 attack mentioned in [36] to generate the synthetic datasets
for retraining the substitute model.
FeatureAdversary (FA): Sabour et al. [45] introduce a new
attack model by minimizing the Lp distance (i.e., Lp norms)
between the internal feature presentation of images pairs
(source image xs, target image xt) of victim classiﬁer f (In
this paper, we call this attack FeatureAdversary (FA)). More
precisely, we describe their problem as follows:
minimize D (φK (x(cid:48)
such that
d (x(cid:48)
s, xs)  0, which makes sure that the perturbation
budget in feature space can be maximized. For the triplet loss
lossF,l (x(cid:48)
s), we formally deﬁne it as:
lossf,l (x(cid:48)
s) = max(D(φK(x
(cid:48)
s), φK(xt))−
(cid:48)
s), φK(xs)) + M, 0)
D(φK(x
(10)
Here φk(.) is the internal feature representation at kth hidden
layer of target classiﬁer, D(.) is a distance function that
measuring the similarity between two internal representations
under the constraint M, which deﬁnes the constant margin of
triplet loss. Note that the reason why we choose the triplet loss
as lossf,l (x(cid:48)
s) in our attack scheme is that, compared to other
loss functions like Lp loss (i.e., L0 loss, L2 loss and L∞ loss)
and VGG loss, the triplet loss can lead to faster convergence
of Equation (9) and better performance of adversarial attacks.
For the purpose of simplicity, we deﬁne the M as follows:
(cid:88)
i,j∈ys
M = α −
1
− nys
n2
ys
(cid:107)φK(xi) − φK(xj)(cid:107)2
(11)
Here α is a constant (Empirically, we set α = 0.5), nys is the
number of input samples in the class ys.
In order to solve the reformulated optimization problem
above, we apply the box-constrained L-BFGS for ﬁnding
a minimum of the loss function in Equation (9), which is
considered particularly well-suited for parameter estimation in
deep learning. The pipeline of the FeatureFool is shown in
Figure 4.
7
(cid:88)
3) Evaluation Metric: We use the Average Test Error
(ATE) over test set Dtest to evaluate the effectiveness of the
proposed model theft attack. Given an input sample x ∈ Dtest,
ground-truth values f (x) and prediction values ˆf (x), then the
ATE is given by:
AT E =
(x,y)∈Dtest
d(f (x), ˆf (x))
|Dtest|
(12)
In our experiment, the ATE refers to the extraction accuracy
under the test set. A lower ATE is expected when an adversary
aims to replicate the functionality of the victim model using
the substitute model.
B. DNN Training
Our model stealing attack aims to retrain a substitute
model in the target domain with near-perfect performance of
the victim model. We adopt ﬁve synthetic dataset generation
including RS, PGD, CW, FA and FF. For the
strategies,
RS strategy, we randomly sample a set of examples as the
training dataset to re-train our substitute model. Different the
RS strategy, the training procedure using adversarial examples
generated by the these approaches is described in Algorithm 1.
First, we randomly sample a small set X0 from target
domain as the initial dataset S0. We use the adversarial
examples generation algorithms to launch adversarial attacks
on the original substitute model and craft a small amount of
malicious examples on this initial dataset. These macilious
examples can easily mislead the local model to output incorrect
results with 100% success rate.
Then we construct the synthetic datasets by querying the
victim models with these malicious examples. In our imple-
mentations, the synthetic datasets differ from those used by
the victim models for training. We use the pre-trained model
in candidate Model Zoo (see Figure 1) as our transfer archi-
tecture and construct the corresponding substitute model. The
synthetic dataset is applied to ﬁne-tune this substitute model by
only retraining the last few fully connected layers but leaving
the previous convolution layers frozen. We signiﬁcantly reduce
the number of queries required to extract the victim model
using transfer learning with adversarial examples.
Finally, we iteratively use the adversarial attack algorithms
to generate synthetic dataset Ds and then retrain the local sub-
stitute model using transfer learning on this synthetic dataset.
This helps us achieve higher accuracy and test agreement on
the test set, which leads to an increase in the similarity of
boundary between the substitute and victim models.
The key observation is that by applying the FeatureFool
algorithm in step 4 of Algorithm 1, the output fs is more
similar to fv. The main reason for this is that by adding the
perturbation component to original images, an adversary can
construct the data set Ds that lie approximately on the decision
boundary of the classiﬁer fv. Since Ds and S(cid:48)
i have the same
set of images and Ds has labels from fv, the data set Ds trains
the classiﬁer fs better than random images.
Here we give some intuitive justiﬁcation based on a sim-
pliﬁed model: if the models fs and fv are linear classiﬁers
in Rp with two classes,
then the decision boundaries are
Algorithm 1 Training process of DNN substitute model:
for victim model fv, the adversarial examples generation
algorithms G (e.g., PGD, CW, FA and FF), the substitute
model fs, an adversarial set of examples S, a maximum
number of iterations m, a synthetic dataset Ds(x) and the
random set of images with same distribution X0, X1, X2,
··· , Xm
Input: fv, fs, X0, X1, X2, ··· , Xm
Output: Retrained substitute model fs
1: Initialize i ← 0,
2: while i < m do
3:
4:
5:
6:
7:
8:
9:
10:
11: end while
Si ← Xi
i ← {G(fs(x), x)|x ∈ Si}
S(cid:48)
//Craft Adversarial Examples
i}
Ds ← {(x, fv(x))|x ∈ S(cid:48)
//Generate synthetic dataset
fs ← Transfer {(fs, Ds)}
//Transfer for the synthetic dataset
i ← i + 1
hyperplanes in Rp. If in addition, all the points in Si lie on the
decision boundary, then as long as the number of points in Si
is larger than p, we can recover the decision boundary from
Ds = {(fv(x), x)|x ∈ Si} and the classiﬁer fv exactly: the
hyperplane containing all p points is the decision boundary. On
the other hand, if the points in Si are randomly chosen from
Rp and do not lie on the decision boundary, then it would
requires much more points to recover the decision boundary
(the hyperplane).
V. EXPERIMENTATION
A. Experiment Setup
In this section, we discuss the experimental results of
a large-scale evaluation on ﬁve popular MLaaS platforms,
including those hosted by Microsoft, Face++, IBM, Google
and Clarifai. We create three victim models ourselves, by
uploading well-labeled training sets. These are the Microsoft
Cloud Vision Service, IBM Watson Visual Recognition, and
Google AutoML Vision, and they are trained for trafﬁc sign
recognition, ﬂower recognition and face recognition, respec-
tively. This simulates a user training the cloud models on
private data sets using these services. Then other users can
access the created cloud models by querying the resulting
prediction APIs in a pay-as-you-go format, quickly fetching
results without many restrictions. However, the DNN models
behind these APIs and the data used for training the DNN