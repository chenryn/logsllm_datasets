non-hazard ﬂips. If we only consider the two highly reputable
engines, the numbers of ﬁles that are inﬂuenced by ﬂips and
non-hazard ﬂips become 554 (3.8%) (bar “r*” in Figure 5)
and 401 (2.7%), respectively. The numbers of inﬂuenced ﬁles
are signiﬁcantly smaller compared with all engines.
We want to emphasize that even though the threshold-
method helps stabilize the aggregated labels, it does not nec-
essarily mean the aggregated labels are correct. Label correct-
ness will be discussed in Section 6.
Observation 5: Flips can heavily inﬂuence labeling aggre-
gation results when threshold t is too small or too large. When
selecting t from a reasonable range (2– 39), the aggregated
labels are likely to be stable.
5 Relationships Between VirusTotal Engines
While using a simple threshold helps tolerate label dynamic
changes, it makes an implicit assumption that each engine is
equally important and relatively independent. Even an early
work that aims to predict the label dynamics [40] makes the
assumption about the independence between engines. In this
section, we seek to examine whether this assumption is true.
First, we measure the correlations between different en-
gines’ labeling decisions. We apply hierarchical clustering to
group engines with a strong labeling similarity. Second, we
further examine the potential causalities (e.g., how one en-
gine’s labels inﬂuence another engine). We adopt an existing
method [33] to model the inﬂuence between different engines.
If we can observe correlations or causalities between certain
engines, then independence assumption would be question-
able. We use the main dataset for our analysis.
5.1 Label Correlations Between Engines
To measure the correlation between two engines, we examine
how likely the two engines give the same labels to the same
ﬁles around the same time. More speciﬁcally, given a pair of
engines (A, B), we compare their label sequences on the same
ﬁle to measure the similarity (or distance). Then we compute
Impact of Flips
Figure 5: Aggregation method vs. ﬁle labels. “always mali-
cious”: ﬁles with only malicious aggregated labels throughout all
the days; “always benign”: ﬁles with only benign aggregated labels
throughout all the days; “ﬂipping labels”: ﬁles with both benign and
malicious aggregated labels. “r”: reputable engines; “r*”: the two
highly reputable engines.
4.5
In Section 2, we ﬁnd that most researchers only submit a ﬁle
to VirusTotal once and simply use a threshold t to aggregate
VirusTotal labels. If t or more engines have labeled the ﬁle as
malicious, the ﬁle’s aggregated label is malicious. We estimate
the potential impact of (hazard) ﬂips on this label aggregation
policy for different t values by measuring how many ﬁles
have different aggregated labels (0 and 1) on different days
during our data collection time window (396 days).
Setting t = 1.
This means a ﬁle is malicious as long as
one engine gives a malicious label. As shown in Figure 5 (the
left-most bar), 1,352 (9.4%) ﬁles only have benign aggregated
labels and 7,067 (49.0%) ﬁles only have malicious aggregated
labels throughout the 396 days. The rest 6,004 (41.6%) ﬁles
have both benign and malicious aggregated labels, and they
are the ﬁles inﬂuenced by ﬂips. If these ﬁles are submitted
to VirusTotal for the second time, there is a chance that a
VirusTotal user will draw a different conclusion on these ﬁles.
This suggests t = 1 is not a good threshold (and yet 50 out of
93 papers use t = 1, see Table 1).
After removing hazards, the number of ﬁles with only be-
nign aggregated labels increases to 5,289 (36.7%). The num-
ber of ﬁles with only malicious aggregated labels is almost
unchanged (7,074). The number of ﬁles inﬂuenced by non-
hazard ﬂips is 2060 (14.3%).
Setting 2 ≤ t ≤ 39. The second bar of Figure 5 shows the
result for t = 2. The number of ﬁles only having benign ag-
gregated labels increases to 6,975 (48.4%). There are 7,006
(48.6%) ﬁles only having malicious labels. The number of
ﬁles inﬂuenced by ﬂips signiﬁcantly decreases to 442 (3.1%).
Flips have no impact on the majority of ﬁles. Although ﬂips
can happen on these ﬁles and actually a different set of en-
gines report malicious labels over time, the ﬂips somehow
cancel each other’s effect and do not inﬂuence the aggregated
labels. There are very few ﬁles inﬂuenced by ﬂips. If we re-
move hazards, the number of ﬁles inﬂuenced by ﬂips further
decreases to 253. When we choose t from 2 to 39, the ratio of
ﬁles inﬂuenced by ﬂips is always less than 30%. If we want
2368    29th USENIX Security Symposium
USENIX Association
123...20...31...3940...50rr*aggregation method (threshold t or reputable engines)0255075100% of filesalways benignflipping labelsalways maliciousthe average similarity score over all the ﬁles between A and
B. The average similarity score can be used to group similar
engines together. In the following, we ﬁrst discuss our engine
clustering algorithm and then discuss our ﬁndings.
5.1.1 Engine Clustering
The key to the engine clustering is to deﬁne the similarity met-
ric between two label sequences. Edit distance is a straight-
forward metric but is not good at capturing ﬁne-grained tem-
poral similarities. For example, the edit distance between
“01000000000000” and “00000000010000” is 2, which is the
same as the edit distance between “01000000000000” and
“00100000000000”. Obviously, the second pair is more corre-
lated since the “timing” between malicious labels is closer.
To encode ﬁne-grained temporal similarities, we divide
each label sequence into ﬁxed-sized bins. In each bin, we
count the number of 0→1 ﬂips, the number of 1→0 ﬂips, the
maximum length of “all-0” sub-sequences, and the maximum
length of “all-1” sub-sequences. This forms a feature vector
of four values for each bin. Let L be the length of a sequence
and S be the size of the bin. Then the feature vector for the
sequence has 4∗(cid:100)L/S(cid:101) dimensions. We do not compute fea-
ture vectors using sliding bin to avoid counting the same ﬂip
multiple times. Given two sequences, we now can compute a
cosine similarity between the two feature vectors as the two
sequences’ similarity score.
For example, assume we choose bin size S = 7. Then A’s
label sequence “01000000000000” can be divided into two
bins “0100000” and “0000000”. The corresponding feature
vector for each bin is [1, 1, 1, 5] and [0, 0, 0, 7] respectively.
The entire sequence’s feature vector is [1, 1, 1, 5, 0, 0, 0, 7].
Similarly, suppose B’s label sequence is “00000000010000”,
and the feature vector is [0, 0, 0, 7, 1, 1, 1, 4]. The cosine sim-
ilarity between the two vectors is 0.871. For the two engines
A and B, their similarity score is the average sequence-level
similarity score over all the ﬁles.
Based on the similarity metric, we leverage the agglomer-
ative clustering algorithm to group similar engines. We can
easily convert the similarity score ss into a distance for the
clustering (d = 1− ss). We choose this hierarchical clustering
method because it is easy to visualize and we don’t need to
pre-deﬁne the number of clusters. We tried bin sizes S as 7, 14
and 28 and observed similar results. Below, we only present
the result with S = 7.
5.1.2 Clustering Result Analysis
When running a hierarchical clustering algorithm, a threshold
td needs to be speciﬁed. If the distance between two clusters is
smaller than td, the two clusters will be merged. We visualize
the clustering results with different td values using a dendro-
gram in Figure 18 in the Appendix. Intuitively, as we increase
td, more clusters will be merged together. Figure 6 shows
Figure 7: Clustering results
with td = 0.01. Only clusters
with more than one engine are
shown.
Figure 6: Number of clus-
ters with more than one en-
gine vs. threshold td. The
dash line td = 0.01.
the number of clusters as we increase td. Note that we only
count the number of clusters that have more than one engine
(singletons are ignored). When td = 0.01, we have the largest
number of clusters (5 clusters), which are then visualized in
Figure 7. The ﬁve clusters contain 16 engines. The rest 49
engines (not shown in the ﬁgure) could not form meaningful
clusters under td = 0.01. This suggests the 16 engines are
highly similar. Among the ﬁve clusters, one cluster has six
engines (Cluster-I), one cluster has four engines (Cluster-II),
and the other three clusters have two engines each.
Cluster-I contains GData, ESET-NOD32, BitDefender, Ad-
Aware, Emsisoft, and MicroWorld-eScan. We conﬁrm that their
label sequences are highly similar. For example, for each pair
of the six engines, there are 14,147 ﬁles on average where the
sequence similarity is higher than 0.99. Note that 14,147 ﬁles
count for 98% of all the ﬁles in the main dataset. A similarity
score of 0.99 means the label sequences are nearly identical.
We show an example ﬁle and its label sequences from the
ﬁve engines in the cluster4 in Figure 16 in the Appendix.
The ﬂip patterns and timing are exactly the same. This result
conﬁrms that there exist groups of vendors whose labels are
not independent but are highly synchronized.
Cluster-II contains Microsoft, McAfee-GW-Edition, McAfee,
and Cyren. Among them, McAfee-GW-Edition and McAfee are
from the same vendor (company), which could be the reason
why their labels are highly similar. However, Microsoft and
Cyren are operated by different companies from McAfee. For
each pair of the four engines, there are 13,922 ﬁles on average
with label-sequence similarity higher than 0.99. These 13,922
ﬁles count for 97% of all the ﬁles in the main dataset. We
again show an example in the Appendix (Figure 19) where
Microsoft and McAfee report identical label sequences for it.
For the other three clusters, the engines are backed up by
the same company. For example, ZoneAlarm uses Kasper-
sky’s anti-virus engine [59], and AVG and Avast merged into
one company in 2016 [68]. These three clusters conﬁrm the
effectiveness of our clustering algorithm.
As shown in Figure 6, when td is increased to 0.2, all the
clusters (with more than one engine) are merged into one big
cluster. This big cluster contains 28 engines and the rest 37
engines are not yet able to form any meaningful cluster. These
4ESET-NOD32 is different on this ﬁle.
USENIX Association
29th USENIX Security Symposium    2369
0.050.100.150.20threshold0246# of clustersAVGAvastZoneAlarmKasperskyCyrenMcAfee-GW-EditionMcAfeeMicrosoftK7GWGDataESET-NOD32BitDefenderAd-AwareEmsisoftMicroWorld-eScanK7AntiVirus(a) 0→1
(b) 1→0
Figure 8: Heatmaps for the active model. All engines are sorted
alphabetically. The value of each cell (i, j) indicates the inﬂuence
from the engine at row-i to the engine at column- j.
28 engines represent a group of engines that are highly corre-
lated but have their differences. It is worth further analyzing
their inﬂuences on each other’s labels.
Observation 6: There are groups of engines whose labeling
decisions have strong correlations. These engines’ results
should not be treated independently.
Inﬂuence Modeling
5.2
We further examine the potential causalities between the la-
bels reported by two engines, using the happens-before re-
lationship. We adapt popular social network inﬂuence mod-
els [21, 32, 33] to our problem context. More speciﬁcally,
when engine j changes its label on a ﬁle to be the same as the
label of engine i, we consider i’s label is the causality of the
label change made by j, or j is inﬂuenced by i.
There are two types of inﬂuence: active and passive inﬂu-
ence. First, suppose vendor j ﬂips its label on a ﬁle from “0”
to “1” because vendor i also made a 0→1 ﬂip very recently.
We call such inﬂuence as active inﬂuence since i’s action
actively impact j’s action. Second, suppose vendor j ﬂips its
label from “0” to “1” because vendor i has stayed on label
“1” over a period of time. We call this relationship as passive
inﬂuence since j is inﬂuenced by i’s state, not action.
5.2.1 Active Inﬂuence Model
Active model is used to model the causalities between ﬂips
from different engines within a short period of time. Given
a ﬁle f, if engine j ﬂips its label at time t and engine i ﬂips
its label on the same direction but slightly earlier than t, we
consider j’s ﬂip is inﬂuenced by i’s ﬂip. We set a time window
w: an active inﬂuence event is established only when i’s ﬂip
happens within [t − w,t). For our analysis, we separate 0→1
ﬂips from 1→0 ﬂips, because they have different contextual
meanings in malware detection. We use Ai2 j to represent all
active inﬂuence events from i to j across all ﬁles. Ai represents
the total number of ﬂips in engine i. The active inﬂuence score
from i to j is measured as the probability pi, j = |Ai2 j|/|Ai|.
Engine Pair Analysis. We choose the window size w = 7
and compute the active inﬂuence score between each pair
(a) 0→1
(b) 1→0
Figure 9: Active model: scatter plot of engines. x: weighted sum
of incoming edges, y: weighted sum of outgoing edges. reputable
engines are in orange color, and reputable engines* are in red color.
(a) 0→1
(b) 1→0
Figure 10: Heatmaps for the passive model. All engines are
sorted alphabetically. The value of each cell (i, j) indicates the inﬂu-
ence from the engine at row-i to the engine at column- j.
of engines5. In Figure 8, we visualize the active inﬂuence
score pi, j in a heatmap where i (inﬂuencer) is the row number
and j (inﬂuenced) is the column number. The engines are
ordered alphabetically. We observe that there are a number of
vertical “bright” lines in both heat maps. This indicates that
there are some engines that are easily inﬂuenced by all the
other engines. Examples include AegisLab in the 2nd column,
Arcabit in the 6th column, Comodo in the 17th column, and
F-Secure in the 28th column. Users have to carefully inspect
labeling results from these engines before aggregation. We
also observe that there are no clear horizontal lines, indicating
that no engine can strongly inﬂuence all the other engines.
Active Inﬂuence Graph.
To better understand the joint
inﬂuence from all engines, we construct an active inﬂuence
graph. In this directed graph, engines are nodes, and the di-