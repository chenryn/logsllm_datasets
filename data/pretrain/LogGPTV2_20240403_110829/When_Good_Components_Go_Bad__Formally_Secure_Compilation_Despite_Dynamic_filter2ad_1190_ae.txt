to support a novel tag-based reference monitor for compartmen-
talization. These approaches have complementary advantages: SFI
requires no specialized hardware, while micro-policies can be en-
gineered to incur little overhead [27] and are a good target for
formal verification [15] due to their simplicity. Together, these two
back ends provide evidence that our RSCC security criterion is com-
patible with any sufficiently strong compartmentalization mech-
anism. It seems likely that other mechanisms such as capability
machines [84] could also be used to implement the compartmental-
ized machine and achieve RSCC.
9
When Good Components Go Bad
e ::=
v
| local
| e1 ⊗ e2
| e1; e2
| if e1 then e2 else e3
| alloc e
| !e
| e1 := e2
| C.P(e)
| exit
values
local static buffer
binary operations
sequence
conditional
memory allocation
dereferencing
assignment
procedure call
terminate
Figure 5: Syntax of source language expressions
Both back ends target variants of a simple RISC machine. In
contrast to the abstract, block-based memory model used at higher
levels of the compilation chain, the machine-level memory is a
single infinite array addressed by mathematical integers. (Using
unbounded integers is a simplification that we hope to remove in
the future, e.g. by applying the ideas of Mullen et al. [61].) All com-
partments must share this flat address space, so—without proper
protection—compromised components can access buffers out-of-
bounds and read or overwrite the code and data of other compo-
nents. Moreover, machine-level components can ignore the stack
discipline and jump to arbitrary locations in memory.
We establish high confidence in the security of our compilation
chain with a combination of proof and testing. For the compiler
from the source language to the compartmentalized machine, we
prove RSCC in Coq (§4.3) using the proof technique of §3.5. For the
SFI back end, we use property-based testing with QuickChick [64]
to systematically test RSCDC
MD.
4.1 Source Language
The source language from this section was designed with simplicity
in mind. Its goal was to allow us to explore the foundational ideas
of this paper and illustrate them in the simplest possible concrete
setting, keeping our formal proofs tractable. The language is expres-
sion based (see Figure 5). A program is composed of an interface,
a set of procedures, and a set of static buffers. Interfaces contain
the names of the procedures that the component exports to and
imports from other components. Each procedure body is a single
expression whose result value is returned to the caller. Internal and
external calls share the same global, protected call stack. Additional
buffers can be allocated dynamically. As in C, memory is manually
managed; out-of-bounds accesses lead to undefined behavior.
Values include integers, pointers, and an undefined value ⊤,
which is obtained when reading from an uninitialized piece of
memory or as the result of an erroneous pointer operation. As in
CompCert and LLVM [57], our semantics propagates these ⊤ values
and yields an undefined behavior if a ⊤ value is ever inspected. (The
C standard, by contrast, specifies that a program is undefined as
soon as an uninitialized read or bad pointer operation takes place.)
The memory model for both source and com-
Memory Model
partmentalized machine is a slightly simplified version of the one
used in CompCert [59]. Each component has an infinite memory
composed of finite blocks, each an array of values. Accordingly, a
pointer is a triple (C, b, o), where C is the identifier of the compo-
nent that owns the block, b is a unique block identifier, and o is an
10
Abate et al.
instr ::= Nop
|
Halt
| Const i -> r
| Mov rs -> rd
| BinOp r1⊗r2 -> rd
| Load *rp -> rd
| Store *rp <- rs
| Jal l
| Jump r
| Call C P
| Return
| Bnz r l
| Alloc r1 r2
Figure 6: Instructions of compartmentalized machine
offset inside the block. Arithmetic operations on pointers are lim-
ited to testing equality, testing ordering (of pointers into the same
block), and changing offsets. Pointers cannot be cast to or from
integers. Dereferencing an integer yields undefined behavior. For
now, components are not allowed to exchange pointers; as a result,
well-defined components cannot access each others’ memories at
all. We hope to lift this restriction in the near future. This abstract
memory model is shared by the compartmentalized machine and is
mapped to a more realistic flat address space by the back ends.
Following CompCert, we use a labeled operational se-
Events
mantics whose events include all interactions of the program with
the external world (e.g., system calls), plus events tracking control
transfers from one component to another. Every call to an exported
procedure produces a visible event C Call P(n) C', recording that
component C called procedure P of component C', passing argument
n. Cross-component returns are handled similarly. All other com-
putations, including calls and returns within the same component,
result in silent steps in the operational semantics.
4.2 The Compartmentalized Machine
The compartmentalized intermediate machine aims to be as low-
level as possible while still allowing us to target our two rather
different back ends. It features a simple RISC-like instruction set
(Figure 6) with two main abstractions: a block-based memory model
and support for cross-component calls. The memory model leaves
the back ends complete freedom in their layout of blocks. The
machine has a small fixed number of registers, which are the only
shared state between components. In the syntax, l represent labels,
which are resolved to pointers in the next compilation phase.
The machine uses two kinds of call stacks: a single protected
global stack for cross-component calls plus a separate unprotected
one for the internal calls of each component. Besides the usual Jal
and Jump instructions, which are used to compile internal calls and
returns, two special instructions, Call and Return, are used for
cross-component calls. These are the only instructions that can
manipulate the global call stack.
The operational semantics rules for Call and Return are pre-
sented in Figure 7. A state is composed of the current executing
component C, the protected stack σ, the memory mem, the registers
reg and the program counter pc. If the instruction fetched from the
program counter is a Call to procedure P of component C′, the
semantics produces an event α recording the caller, the callee, the
procedure and its argument, which is stored in register R_ COM. The
protected stack σ is updated with a new frame containing the next
point in the code of the current component. Registers are mostly
invalidated at Calls; reg⊤ has all registers set to ⊤ and only two
registers are passed on: R_ COM contains the procedure’s argument
and R_ RA contains the return address. So no data accidentally left
by the caller in the other registers can be relied upon; instead the
When Good Components Go Bad
Abate et al.
C (cid:44) C′
entry(E, C′, P) = pc′
fetch(E, pc) = Call C′ P
P ∈ C.import
reg′ = reg⊤[R_ COM ← reg[R_ COM], R_ RA ← pc + 1]
α = C Call(P, reg[R_ COM]) C′
E ⊢ (C, σ , mem, reg, pc) α−→ (C′,(pc + 1) :: σ , mem, reg′, pc′)
C (cid:44) C′
fetch(E, pc) = Return
reg[R_ RA] = pc′
reg′ = reg⊤[R_ COM ← reg[R_ COM]]
α = C Return(reg[R_ COM]) C′
component(pc′) = C′
E ⊢ (C, pc′ :: σ , mem, reg, pc) α−→ (C′, σ , mem, reg′, pc′)
Figure 7: Compartmentalized machine semantics
compiler saves and restores the registers. Finally, there is a redun-
dancy between the protected stack and R_ RA because during the
Return the protected frame is used to verify that the register is
used correctly; otherwise the program has an undefined behavior.
4.3 RSCC Proof in Coq
We have proved that a compilation chain targeting the compart-
mentalized machine satisfies RSCC, applying the technique from
§3.5. As explained in §2, the responsibility for enforcing secure com-
pilation can be divided among the different parts of the compilation
chain. In this case, it is the target machine of §4.2 that enforces
compartmentalization, while the compiler itself is simple, standard,
and not particularly interesting (so omitted here).
For showing RSCDC
MD, all the assumptions from §3.5 are proved
using simulations. This proof was formalized in Coq, with the ex-
ception of compiler correctness (Assumptions 2 and 4), which is
standard and essentially orthogonal to secure compilation; even-
tually, we hope to scale the source language up to a compartmen-
talized variant of C and reuse CompCert’s mechanized correctness
proof. Our mechanized formalization is more detailed than previous
paper proofs in the area [3, 5, 10–12, 31, 42, 43, 45, 63, 65–67]. In-
deed, we are aware of only one fully mechanized proof about secure
compilation: Devriese et al.’s [25] recent full abstraction result for
a translation from the simply typed to the untyped λ-calculus in
around 11KLOC of Coq.
Our Coq development comprises around 20KLOC, with proofs
taking about 60%. Much of the code is devoted to generic models
for components, traces, memory, and undefined behavior that we
expect to be useful in proofs for more complex languages and com-
pilers, such as CompCert. We discuss some of the most interesting
aspects of the proof below.
Back-translation function. We proved Assumption 1 by defining
a↑ function that takes a finite trace prefix m and a program interface
I and returns a whole source program that respects I and produces
m. Each generated component uses the local variable local[0]
to track how many events it has emitted. When a procedure is in-
voked, it increments local[0] and produces the event in m whose
position is given by the counter’s value. For this back-translation to
work correctly, m is restricted to look like a trace emitted by a real
compiled program with an I interface—in particular, every return
in the trace must match a previous call.
11
This back-translation is illustrated in Figure 8 on a trace of four
events. The generated program starts running MainC.mainP, with
all counters set to 0, so after testing the value of MainC.local[0],
the program runs the first branch of mainP:
local [ 0 ] + + ; C . p ( 0 ) ; MainC . mainP ( 0 ) ;
After bumping local[0], mainP emits its first event in the trace:
the call C.p(0). When that procedure starts running, C’s counter
is still set to 0, so it executes the first branch of procedure p:
local [ 0 ] + + ; return 1 ;
The return is C’s first event in the trace, and the second of the
program. When mainP regains control, it calls itself recursively
to emit the other events in the trace (we can use tail recursion to
iterate in the standard way, since internal calls are silent events).
The program continues executing in this fashion until it has emitted
all events in the trace, at which point it terminates execution.
Theorem 4.1 (Back-translation). The back-translation function ↑
illustrated above satisfies Assumption 1.
T ∪ P′
T ), then merge s s′ is a state of (C′
Recomposition. We prove Assumption 3 by proving a slightly
more general lemma, obtained by considering arbitrary target pro-
grams PT and P′
T instead of the compiled source programs P↓ and
P′↓, and arbitrary target context C′
T instead of the compiled source
context CS ↓. To do so, we use a three-way simulation, stated in
terms of the underlying small-step semantics.
First, we state a mergeability relation ∼ between pairs of com-
plete states s and s′, and prove that it is preserved during a parallel
execution guided by the trace. In particular, this relation ensures
that the shapes of the two stacks of the two executions are related,
in that they must capture the same interactions (calls and returns)
between program and context, despite the possibly different imple-
mentations. Furthermore, we define a merge function, implicitly
parameterized by the program and context interfaces, that defined
how to merge two mergeable states: if s is a state of (CT ∪ PT ), and
s′ is a state of (C′
T ∪ PT ). To
merge two states s and s′, one has to (1) retain the PC and registers
of the execution that is currently in control of the run, i.e., s if the
program side is executing (the PC points to code belonging to a
component inside PT ), and s′ otherwise; (2) keep only the memory
belonging to the program PT in s, and to the context C′
T in s′, and
take their union; (3) recombine the stacks, interleaving the frames
from C′
T and PT in order to preserve the (compatible) structure of
the original stacks.
Second, we prove the three-way simulation proper, showing
m−−→ s2 and s′
m−−→ s′
2 and
that if s1
1
merge s1 s′
m−−→ merge s2 s′
2, where m−−→ is the reflexive transitive
1
closure of the step relation, producing the events in m. At each
step, the simulation is guided by either both programs or both
contexts. A step in one of the runs is matched by any number of
silent steps in the other. The two runs synchronize on observable
events, corresponding to comparable event-producing steps, pos-
sibly padded with silent steps before and after. Control alternates
between programs and contexts at cross-component events. These
new definitions are more natural, completely symmetric, and easier
to work with than the previous ones about partial semantics [6].
2 where s1 ∼ s′
1, then s2 ∼ s′
When Good Components Go Bad
Abate et al.
C
ECall MainC p
ERet
ECall MainC p
ECall C
0 C
1 MainC