Falcon is designed with a modular architecture, whose com-
ponents operate together as a pipeline. In a nutshell, Falcon
receives as input log ﬁles from multiple data sources and
outputs a space-time diagram that preserves event causality.
Figure 4 depicts the architecture of Falcon, composed by three
main modules: the trace processor, the happens-before model
generator, and the visualizer. Each module is described in
detail as follows.
Trace Processor. Since the events logged by the different
tools can vary in both format and content, Falcon needs to
ﬁrst normalize and merge the collected data into a global
event trace with a common scheme. This procedure is done by
the trace processor module. The trace processor is equipped
with a dedicated driver for each type of log, responsible for
translating the library-speciﬁc entries into events that can be
processed by Falcon. As such, drivers may range from simple
parsers for textual logs (e.g. for log4j) to packet unpackers for
network sniffers (e.g. tshark). In some cases, the trace proces-
sor generates events that are the result of merging data from
different logs. For example, an event representing the sending
of a message can be built by augmenting the information of a
write syscall with the message payload captured by a network
sniffer. The events resulting from Falcon’s log normalization
and merging are the following:
• START(process): a process starting event;
• END(process): a process ﬁnishing event;
• FORK(parent, child): a process creation event, where
child denotes the process spawned by process parent;
• JOIN(parent, child): represents a join event, where pro-
cess parent waits until the child process ﬁnishes;
• CONNECT(process, src, dst): represents a new con-
nection, where src and dst denote the addresses (IP and
port) of the local and remote processes, respectively;
• ACCEPT(process, src, dst): event indicating that a con-
nection was established, where src and dst also denote
the local and remote addresses, respectively;
• SND(process, src, dst, msg): a message sending event,
where msg is the identiﬁer of message being sent from
the src address to the dst address;
a message
• RCV(process, src, dst, msg):
receiving
event, where msg denotes the identiﬁer of the message
sent by the src address and received by the dst address.
• LOG(process, msg): a log entry event, where msg is
the content of the message logged by process process.
The trace processor module also exposes a public API
to ease the development of drivers and the integration of
additional logging libraries into Falcon.
Happens-Before Model Generator. The complete, normal-
ized event trace is then fed into the happens-before (HB)
model generator. This module is responsible for combining
all events into a single causally-consistent schedule. To this
end, the HB model generator builds a symbolic constraint
formulation encoding the happens-before relations between
events. For instance, the model encodes a constraint stating
that the send event of a message must happen-before the
corresponding receive event. The HB constraints are further
described in Section III-C.
Solving the model with an off-the-shelf constraint solver
yields a causally-ordered event schedule.
Visualizer. The visualizer ﬁnishes the Falcon’s pipeline
by providing a graphical representation of the causal trace
generated in the previous step. In detail, the visualizer gener-
ates a “space-time diagram”, as introduced by Lamport [3],
depicting both the events executed by each process and the
inter-process causal relationships between them.
C. Happens-Before Constraint Model
As deﬁned by Lamport [3], there exists a happens-before
relationship between two events a and b, denoted a → b, if:
• a and b belong to the same process4 and a precedes b in
the execution.
• a and b belong to different processes and a represents the
sending of a message m and b represents the reception
of m.
Distributed executions often comprise other causal relations
that should be considered, namely a → b also holds if:
• a is the fork event of a process q by a process p and b is
the ﬁrst event of q.
q by a process p.
• a is the last event of a process q and b the join event of
• a is the connect event issued by a process p to a process
q and b is the accept event in q.
4We use the term process to denote both processes and threads.
537
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:53 UTC from IEEE Xplore.  Restrictions apply. 
Note that the happens-before relation is transitive, irreﬂexive
and antisymmetric. Also, when a (cid:2) b and b (cid:2) a holds, then
a and b are considered to be concurrent.
Falcon casts the problem of combining the events from inde-
pendent logs into a global, causally-ordered execution sched-
ule as a maximum satisﬁability modulo theories (MaxSMT)
problem. The MaxSMT problem can be seen as an opti-
mization version of the satisﬁability problem (for types of
variables other than boolean ones) and has the goal of ﬁnding
a total assignment to variables of a formula that maximizes
the number of satisﬁed clauses. Among the variants of the
MaxSMT problem, this paper assumes a partial MaxSMT
problem where some clauses are considered as hard and others
are considered as soft. The goal is thus to ﬁnd an assignment
to the variables such that all hard constraints are satisﬁed and
the amount of satisﬁed soft constraints is maximized.
Falcon’s causality model comprises i) integer symbolic
variables that represent the logical clocks [3] of the events
supported by Falcon (see Section III-B) and ii) hard constraints
over those variables stating the causal relations between the
events. A solution to this model thus assigns a value to each
variable such that all happens-before rules are satisﬁed. In
practice, this corresponds to inferring a causally-consistent
execution schedule by computing a logical clock per event.
More formally, the constraint model, denoted ΦHB, consists
of a MaxSMT formulation deﬁned as the following conjunc-
tion of sub-formulae:
ΦHB = φinter ∧ φintra
(cid:5)
(cid:3)(cid:4)
(cid:2)
hard
∧ GOAL
(cid:2) (cid:3)(cid:4) (cid:5)
sof t
(1)
where φinter encodes the inter-process causality constraints,
φintra encodes the intra-process happens-before rules due to
program order, and GOAL states the soft constraints that
allow steering the solving procedure towards a given goal.
Falcon currently provides support for generating logical clocks
that: i) follow the original timestamp order as much as possible
(φts), and ii) expose concurrency issues by minimizing the
logical time intervals between events (φmin). We now describe
each sub-set of constraints in more detail.
a) Inter-process HB Constraints (φinter): these constraints
represent the causal dependencies due to message exchanges
and inter-process synchronization. Following the happens-
before rules presented at the beginning of this section, the
inter-process HB constraints φinter are written as follows:
forkp,q < startq
endq < joinp,q
connectp < acceptq
sndp,m < rcvq
where p and q are distinct processes, m represents a given
message, and the variable names correspond to the events
described in Section III-B. For instance, for a message m sent
by p to q, the constraints encodes that the logical clock of
the corresponding event SND(p, p, q, m) in the trace must be
smaller than that of the event RCV(q, p, q, m).
b) Intra-process HB Constraints (φintra): these constraints
state that events in the same process execute sequentially
according to the program order. Let Γp denote the event trace
of a process p, and let ci and cj be the symbolic variables
representing the logical clocks of events i and j. The intra-
process HB constraints φintra are given by:
∀i, j ∈ Γp : (i < j =⇒ ci < cj)
c) Timestamp Constraints (φts): timestamp constraints are
soft constraints (see GOAL in Equation 1) that aim at approx-
imating the schedule produced by the solver to the actual event
ordering observed during the production run. These constraints
state that events should be given logical clocks that follow the
order given by timestamps in the log ﬁles. However, since
two causally-ordered events logged on different machines
may exhibit physical timestamps conﬂicting with their HB
relationship, timestamp constraints may be violated in order
to satisfy causality.
d) Clock Minimization Constraint (φmin): clock mini-
mization constraints are also encoded as GOAL soft clauses
and strive to minimize the values assigned to the symbolic
variables. The goal is to produce a compact schedule capable
of exposing event concurrency. For instance, if two distinct
SND events exhibit the same logical order in the schedule
yielded by the solver and their corresponding RCV events
belong to the same process, then there is a message race.
Let e ∈ Γ be an event in the complete execution trace and
ce the symbolic variable representing its logical clock. The
clock minimization constraint φmin is written as:
φmin = min
(cid:6)
ce
∀e∈Γ
Solving the ΦHB model generated by Falcon using an off-
the-shelf SMT solver yields an execution schedule in which
events are guaranteed to be causally ordered.
IV. IMPLEMENTATION
This section discusses some relevant implementation details
of our prototype of Falcon. The prototype is publicly available
at https://github.com/fntneves/falcon/.
The trace processor module is implemented as an extensible
Python program that allows the integration of custom drivers
for normalizing log ﬁles into a pre-deﬁned JSON format.
Currently, the trace processor provides three out-of-the-box
drivers. The ﬁrst is a ptrace-based tool that collects syscall
traces. The second driver handles logs generated by log4j, a
logging library for Java programs. The third uses tshark to
extract message payloads from pcap ﬁles and add them to the
corresponding send and receive events.
When tracing syscalls for pairs of events causally related,
we intercept the syscall of the ﬁrst event solely at its entry
point and the syscall of the second event only at its exit point.
Since ptrace-based tracing utilities do not guarantee that the
two interception points of a syscall appear contiguously in the
trace, this approach is crucial to correctly infer causality.
538
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:47:53 UTC from IEEE Xplore.  Restrictions apply. 
The happens-before model generator is written in Java and
uses the Z3 solver [11] to solve the model. The causal trace
produced by the solver is then output in JSON format.
Falcon’s current visualizer is implemented as a JavaScript
program that consumes the causal trace and generates a space-
time diagram using the SVG.js library. We are currently ex-
tending Falcon to use ShiViz [10] as the visualization module,
as it provides interactive analysis features.
V. CASE STUDY: APACHE ZOOKEEPER
Apache Zookeeper [12] is an open-source, scalable and re-
liable service that enables distributed coordination. Zookeeper
poses a good case study for Falcon as its implements com-
plex algorithms and protocols for leader election and atomic
broadcast, which are hard to analyze and understand in detail.
In a distributed deployment, Zookeeper runs with several
servers, of which one is leader and the others are followers.
Both roles are distinguishable in the sense that read requests
can be served by the followers while write requests are handled
only by the leader. For this case study, using Zookeeper 3.5.0,
we analyze a setup containing two Zookeeper nodes that
communicate with each other to elect the leader. In particular,
our execution scenario consisted of setting up a standalone
Zookeeper server and, then, adding a new node to the server
quorum.
During the execution, we collected Zookeeper’s built-in log
ﬁle produced with the log4j logging library and used our
ptrace-based tracer tool to record syscalls regarding thread
synchronization events, connections, and messages exchanges.
As the output layout generated by log4j is conﬁgurable, we
set the layout parameter to a custom Java class. In order to
correctly identifying the thread responsible for logging a given
message, we augment each log entry with a unique identiﬁer
consisting of the concatenation of both the thread and process
ids. However, since the Java Virtual Machine does not allow
accessing the native thread identiﬁer from a high-level API,
we rely on the Java Native Access to execute the gettid()
system call and retrieve the thread id directly from the native
operating system. The result of the syscall is thus introduced
as a parameter in the output layout of log4j.
In the following, we show how Falcon can be used to ana-
lyze the execution of Zookeeper and evaluate the performance
and scalability of the constraint solving procedure.
A. Falcon in Action
Figure 5 depicts the space-time diagram generated by Fal-
con for the logs collected during our Zookeeper execution
scenario. The causal trace was obtained by solving the model
with the timestamp soft constraints. The diagram shows that
there are two main processes (5598 and 5670) that spawn
several threads while running. For brevity, we include just the
thread timelines relevant for this example. In other words, we
discarded the threads that have only START, END and LOG
events. However, they can be useful for conducting a more
thorough behavior analysis.
5598-5663
5598-5666
5598-5755
5598-5756
5670-5749
5670-5753
5670-5754
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
START
START
START
START
START
START
START
LOG
ACCEPT
LOG
2
RCV
RCV
RCV
RCV
RCV
RCV
8
8
1
1
1
1
RCV
14
3
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG
LOG