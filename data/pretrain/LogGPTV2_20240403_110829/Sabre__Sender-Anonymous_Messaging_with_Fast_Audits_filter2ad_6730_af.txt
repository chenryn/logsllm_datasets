### Performance Comparison and Denial-of-Service Resistance

#### Auditing Costs
The (2 + 1)-party and 3-verifier SNIP auditing processes are shorter than the 2-verifier SNIP auditing. However, even for Sabre instances with \( n = 2^{20} \) registered mailboxes, the amortized cost per audit remains under 10 ms. This cost increases to no more than 23 ms when \( n = 2^{40} \). Although 2-verifier SNIP auditing is less performant, it provides superior security guarantees and is most comparable to Express. Therefore, we focus on this variant in our remaining experimental results. It is important to note that the 3-server variants offer significantly better auditing performance, provided an additional party and a stronger trust assumption are acceptable. These variants also provide a more direct comparison with Riposte.

Figures 5a and 5b compare the auditing costs of Sabre with 2-verifier SNIP auditing against those of Riposte and Express. The plots clearly show the benefits of Sabre’s logarithmic auditing over the linear auditing of Riposte and Express. While Riposte and Express perform better for very small DPFs, Sabre becomes significantly faster as the number of mailboxes exceeds around 100,000.

Figure 5c illustrates the cost of verifying a batch of 128 mailbox addresses in Sabre-M. As expected, the running time for the mailbox address check is constant, independent of the number of mailboxes.

#### Resistance to Denial-of-Service Attacks
Our next set of experiments evaluates Sabre’s resistance to resource-exhaustion DoS attacks by comparing the time it takes for a simulated attacker to produce plausible-looking but malformed write requests with the time it takes for Sabre auditing to reject them. The attacker generates malformed requests by sampling pseudorandom bits from `/dev/urandom`.

We consider two types of auditing failures:
1. When the address check fails.
2. When the address check passes but the DPF audit subsequently fails (we also consider the tree depth at which the auditing aborts).

Figure 6a compares the time required to sample a single malformed request with the time required to check a batch of 128 addresses using the PRF-based address check. For Sabre instances with as few as \( n = 2^{10} \) mailboxes, sampling a malformed request takes nearly 1.75 times longer than address checking. By \( n = 2^{20} \) mailboxes, the difference approaches 4.6 times.

Figure 6b compares the time required to construct a request that verifies up to but not including the \( d \)-th level of the DPF (after which the attack samples the rest of the request from `/dev/urandom`) with the time required for the auditors to reject the request. Due to the probabilistic nature of SNIP verification, auditors can reject a malformed DPF after inspecting only a fraction of the bits that the attacker must sample. Consequently, sampling a malformed request takes about 1.5 times longer than rejecting it via auditing, for all \( d \).

In contrast to Riposte and Express, where auditors run asymptotically slower than writers, Sabre auditors consistently reject malformed requests faster than the simulated attackers can produce them. Thus, Sabre is inherently resistant to DoS attacks: an attacker seeking to overwhelm Sabre servers must expend more resources than the servers.

#### Head-to-Head Comparison with Riposte and Express
Our next set of experiments provides a head-to-head comparison between Sabre with 2-verifier SNIPs and both Riposte and Express, the state-of-the-art systems in the bulletin-board and mailbox models. We plot the results in Figures 7 and 8.

Figure 7a compares the throughput of Sabre-M against that of Express for messages of size 1 KiB and 32 KiB. In both cases, Sabre-M outperforms Express, even with 32 KiB messages outperforming Express with 1 KiB messages. Figure 8a suggests that Sabre’s performance advantage will increase rapidly with the introduction of malformed requests.

Figure 7b compares the throughput of Sabre-BB against that of Riposte. For a meaningful comparison, we provide separate plots for Sabre-BB with each of the three audit protocol instantiations. The results confirm our expectations: throughput for the (2 + 1)-party and 3-verifier SNIP variants consistently exceeds 20 times that of Riposte. The overhead of cut-and-choose in the 2-verifier SNIP variant reduces this gain when the number of buckets is small. As the number of buckets grows, the costs of all three Sabre instances converge to that of the full-domain evaluation of the DPF and subsequent writing.

Figure 10 in Appendix D compares the performance of the “stepping-stones” to Sabre-M. Comparing these plots with the plot for Express (from Figure 7a) with 1 KiB messages reveals comparable throughput when all DPFs pass auditing. This is unsurprising, as the bottleneck in both protocols is the DPF evaluation and subsequent writing. Our use of bitsliced LowMC is crucial here: Express benefits from the fast AES-NI instruction set on modern x86-64 CPUs; without bitslicing, the throughput of LowMC could not compete. The algorithmic improvements of Sabre become apparent as we progress through the “stepping-stones” toward Sabre-M, whose smaller DPFs and ability to use full-domain evaluation greatly reduce the cost of DPF evaluation. Full-domain evaluation also provides superior pipelining and cache utilization, resulting in notable speedups for the subsequent writing. Further algorithmic improvements arise only when some DPFs fail auditing.

#### Throughput Under DoS Attacks
The final set of experiments compares the throughput of Sabre-M with that of Express in the presence of a resource-exhaustion DoS attack. Write requests in the mailbox model can be malformed in two ways: (i) the mailbox address is incorrect, or (ii) the DPF is not well-formed. Figure 8 presents five plots that each show the effect of varying the proportion of incoming requests that are well-formed versus malformed.