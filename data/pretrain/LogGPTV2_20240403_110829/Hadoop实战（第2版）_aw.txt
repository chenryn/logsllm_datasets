continue；
}
//数据块版本较晚
if（info.getBlock（）.getGenerationStamp（）＜block.getGenerationStamp（））{
continue；
}
//正确版本数据块的信息保存起来
blockRecords.add（new BlockRecord（id, datanode, info））；
if（info.wasRecoveredOnStartup（））{
rwrCount++；//等待回复数
}else{
rbwCount++；//正在恢复数
}
}catch（IOException e）{
++errorCount；//出错数
}
}
（3）找出所有正确版本数据块中最小长度的版本
在这一步骤中，DataNode会逐个扫描上一阶段中保存的数据块记录，首先判断当前副本是否正在恢复，如果正在恢复则跳过，如果不是正在恢复并且配置参数设置了恢复需要保持原副本长度，则将恢复长度相同的副本加入待恢复队列，否则将所有版本正确的副本加入待恢复队列。
for（BlockRecord record：blockRecords）{
BlockRecoveryInfo info=record.info；
if（！shouldRecoverRwrs＆＆info.wasRecoveredOnStartup（））{
continue；
}
if（keepLength）{
if（info.getBlock（）.getNumBytes（）==block.getNumBytes（））
{syncList.add（record）；}
}else{
syncList.add（record）；
if（info.getBlock（）.getNumBytes（）＜minlength）{
minlength=info.getBlock（）.getNumBytes（）；
}
}
}
（4）副本同步
如果需要保持副本长度，那么直接同步长度相同的副本即可，否则以长度最小的副本同步其他副本。
if（！keepLength）{
block.setNumBytes（minlength）；
}
return syncBlock（block, syncList, targets, closeFile）；
与读取本地文件的情况相同，用户也可以使用命令来禁用检验和检验（从前面的代码中也可以看出，通常在检查校验和之前都有needChecksum等选项）。有两种方法可以达到这个目的。
一个是在使用open（）读取文件前，设置FileSystem中的setVerifyChecksum值为false。
FileSystem fs=new FileSystem（）；
Fs.setVerifyChecksum（false）；
另一个是使用shell命令，比如get命令和copyToLocal命令。
get命令的使用方法如下所示：
hadoop fs-get[-ignoreCrc][-crc]＜src＞＜localdst＞
举个例子：
hadoop fs-get-ignoreCrc input～/Desktop/
get命令会复制文件到本地文件系统。可用-ignorecrc选项复制CRC校验失败的文件，或者使用-crc选项复制文件，以及CRC信息。
copyToLocal的使用方法如下所示：
hadoop fs-copyToLocal[-ignorecrc][-crc]URI＜localdst＞
再举个例子：
hadoop fs-copyToLocal-ignoreCrc input～/Desktop
除了要限定目标路径是一个本地文件外，其他和get命令类似。
禁用校验和检验的最主要目的并不是节约时间，用于检验校验和的开销一般情况都是可以接受的，禁用校验和检验的主要原因是，如果不禁用校验和检验，就无法下载那些已经损坏的文件来查看是否可以挽救，而有时候即使是只能挽救一小部分文件也是很值得的。
7.2 数据的压缩
对于任何大容量的分布式存储系统而言，文件压缩都是必须的，文件压缩带来了两个好处：
1）减少了文件所需的存储空间；
2）加快了文件在网络上或磁盘间的传输速度。
Hadoop关于文件压缩的代码几乎都在package org.apache.hadoop.io.compress中。本节的内容将会主要围绕这一部分展开。
 7.2.1 Hadoop对压缩工具的选择
有许多压缩格式和压缩算法是可以应用到Hadoop中的，但是不同的算法都有各自的特点。表7-1是Hadoop中使用的一些压缩算法，表7-2是它们的压缩格式和特点。
压缩一般都是在时间和空间上的一种权衡。一般来说，更长的压缩时间会节省更多的空间。不同的压缩算法之间有一定的区别，而同样的压缩算法在压缩不同类型的文件时表现也不同。jeff的试验比较报告中包含了面对不同文件在各种要求（最佳压缩、最快速度等）下的最佳压缩工具。如果大家感兴趣可以自行查阅，地址为http：//compression.ca/act/act-summary.html（这个地址是总体评价，此网站还有不同压缩工具面对不同类型文件时的具体表现）。
7.2.2 压缩分割和输入分割
压缩分割和输入分割是很重要的内容，比如，如果需要处理经Gzip压缩后的5GB大小的文件，按前面介绍过的分割方式，Hadoop会将其分割为80块（每块64MB，这是默认值，可以根据需要修改）。但是这是没有意义的，因为在这种情况下，Hadoop不会分割存储Gzip压缩的文件，程序无法分开读取每块的内容，那么也就无法创建多个Map程序分别来处理每块内容。
而bzip2的情况就不一样了，它支持文件分割，用户可以分开读取每块内容并分别处理之，因此bzip2压缩的文件可分割存储。
7.2.3 在MapReduce程序中使用压缩
在MapReduce程序中使用压缩非常简单，只需在它进行Job配置时配置好conf就可以了。
设置Map处理后压缩数据的代码示例如下：
JobConf conf=new Jobconf（）；
conf.setBoolean（"mapred.compress.map.output"，true）；
设置output输出压缩的代码示例如下：
JobConf conf=new Jobconf（）；
conf.setBoolean（"mapred.output.compress"，true）；
conf.setClass（"mapred.output.compression.codec"，GzipCodec.class, CompressionCodec.class）；
对一般情况而言，压缩总是好的，无论是对最终结果的压缩还是对Map处理后的中间数据进行压缩。对Map而言，它处理后的数据都要输出到硬盘上并经过网络传输，使用数据压缩一般都会加快这一过程。对最终结果的压缩不单会加快数据存储的速度，也会节省硬盘空间。
下面我们做一个实验来看看在MapReduce中使用压缩与不使用压缩的效率差别。
先来叙述一下我们的实验环境：这是由六台主机组成的一个小集群（一台Master，三台Salve）。输入文件为未压缩的大约为300MB的文件，它是由随机的英文字符串组成的，每个字符串都是5位的英文字母（大小写被认为是不同的），形如“AdEfr”，以空格隔开，每50个一行，共50 000 000个字符串。对这个文件进行WordCount。Map的输出压缩采用默认的压缩算法，output的输出采用Gzip压缩方法，我们关注的内容是程序执行的速度差别。
执行压缩操作的WordCount程序与基本的WordCount程序相似，只需在conf设置时写入以下几行代码：
conf.setBoolean（"mapred.compress.map.output"，true）；
conf.setBoolean（"mapred.output.compress"，true）；
conf.setIfUnset（"mapred.output.compression.type"，"BLOCK"）；
c o n f.s e t C l a s s（"m a p r e d.o u t p u t.c o m p r e s s i o n.c o d e c"，G z i p C o d e c.c l a s s，
CompressionCodec.class）；
下面分别执行编译打包两个程序，在运行时用time命令记录程序的执行时间，如下所示：
time bin/hadoop jar WordCount.jar WordCount XWTInput xwtOutput
real 12m41.308s
time bin/hadoop jar CompressionWordCount.jar CompressionWordCount XWTInput
xwtOutput2
real 8m9.714s
CompressionWordCount. jar是带压缩的WordCount程序的打包，从上面可以看出执行压缩的程序要比不压缩的程序快4分钟，或者说，在这个实验环境下，使用压缩会使WordCount效率提高大约三分之一。
7.3 数据的I/O中序列化操作
序列化是将对象转化为字节流的方法，或者说用字节流描述对象的方法。与序列化相对的是反序列化，反序列化是将字节流转化为对象的方法。序列化有两个目的：
1）进程间通信；
2）数据持久性存储。
Hadoop采用RPC来实现进程间通信。一般而言，RPC的序列化机制有以下特点：
1）紧凑：紧凑的格式可以充分利用带宽，加快传输速度；
2）快速：能减少序列化和反序列化的开销，这会有效地减少进程间通信的时间；
3）可扩展：可以逐步改变，是客户端与服务器端直接相关的，例如，可以随时加入一个新的参数方法调用；
4）互操作性：支持不同语言编写的客户端与服务器交换数据。
Hadoop也希望数据持久性存储同样具有以上这些优点，因此它的数据序列化机制就是依照以上这些目的而设计的（或者说是希望设计成这样）。
在Hadoop中，序列化处于核心地位。因为无论是存储文件还是在计算中传输数据，都需要执行序列化的过程。序列化与反序列化的速度，序列化后的数据大小等都会影响数据传输的速度，以致影响计算的效率。正是因为这些原因，Hadoop并没有采用Java提供的序列化机制（Java Object Serialization），而是自己重新写了一个序列化机制Writeables。Writeables具有紧凑、快速的优点（但不易扩展，也不利于不同语言的互操作），同时也允许对自己定义的类加入序列化与反序列化方法，而且很方便。
 7.3.1 Writable类
Writable是Hadoop的核心，Hadoop通过它定义了Hadoop中基本的数据类型及其操作。一般来说，无论是上传下载数据还是运行Mapreduce程序，你无时无刻不需要使用Writable类，因此Hadoop中具有庞大的一类Writable类（见图7-2），不过Writable类本身却很简单。
Writable类中只定义了两个方法：
//序列化输出数据流
void write（DataOutput out）throws IOException
//反序列化输入数据流
void readFields（DataInput in）throws IOException
Hadoop还有很多其他的Writable类。比如WritableComparable、ArrayWritable、Two-DArrayWritable及AbstractMapWritable，它们直接继承自Writable类。还有一些类，如BooleanWritale、ByteWritable等，它们不是直接继承于Writable类，而是继承自WritableComparable类。Hadoop的基本数据类型就是由这些类构成的。这些类构成了以下的层次关系（如图7-2所示）。
图 7-2 Writable类层次关系图
1.Hadoop的比较器
WritableComparable是Hadoop中非常重要的接口类。它继承自org.apache.hadoop.io.Writable类和java.lang.Comparable类。WritableComparator是Writablecomparable的比较器，它是RawComparator针对WritableComparate类的一个通用实现，而RawComparator则继承自java.util.Comparator，它们之间的关系如图7-3所示。
图 7-3 WritableComparable和WritableComparablor类层次关系图
这两个类对MapReduce而言至关重要，大家都知道，MapReduce执行时，Reducer（执行Reduce任务的机器）会搜集相同key值的key/value对，并且在Reduce之前会有一个排序过程，这些键值的比较都是对WritableComparate类型进行的。
Hadoop在RawComparator中实现了对未反序列化对象的读取。这样做的好处是，可以不必创建对象就能比较想要比较的内容（多是key值），从而省去了创建对象的开销。例如，大家可以使用如下函数，对指定了开始位置（s1和s2）及固定长度（l1和l2）的数组进行比较：
public interface RawComparator＜T＞extends Comparator＜T＞{
public int compare（byte[]b1，int s1，int l1，byte[]b2，int s2，int l2）；
}
WritableComparator是RawComparator的子类，在这里，添加了一个默认的对象进行反序列化，并调用了比较函数compare（）进行比较。下面是WritableComparator中对固定字节反序列化的执行情况，以及比较的实现过程：
public int compare（byte[]b1，int s1，int l1，byte[]b2，int s2，int l2）{
try{
buffer.reset（b1，s1，l1）；//parse key1
key1.readFields（buffer）；
buffer.reset（b2，s2，l2）；//parse key2
key2.readFields（buffer）；
}catch（IOException e）{
throw new RuntimeException（e）；
}
return compare（key1，key2）；//compare them
}
2.Writable类中的数据类型
（1）基本类
Writable中封装有很多Java的基本类，如表7-3所示。
其中最简单的要数Hadoop中对Boolean的实现，如下所示：
package cn.edn.ruc.cloudcomputing.book.chapter07；
import java.io.*；
public class BooleanWritable implements WritableComparable{
private boolean value；
public BooleanWritable（）{}；
public BooleanWritable（boolean value）{
set（value）；
}