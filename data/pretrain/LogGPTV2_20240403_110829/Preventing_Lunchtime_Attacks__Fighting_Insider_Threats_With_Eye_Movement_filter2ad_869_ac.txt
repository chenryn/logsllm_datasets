on the gazetracker precision and the fraction of missing
samples [28]. As these values rely on the calibration of the
gazetracker, they may change slightly across different sessions.
The changes described above can manifest themselves both
within the same session and across multiple days or weeks.
Technical artifacts may be particularly prevalent when using
data collected in different sessions due to the fact that a separate
calibration has to be performed before each session. Despite
these difﬁculties we show in Section VI that we are able to
collect a classiﬁer training dataset that is rich enough to reduce
the inﬂuence of these error sources. By including training data
from several session we are able to capture, and adjust for,
both long-term and short-term feature decay.
E. Participant Recruitment
Our data is collected from 30 participants, recruited from
the general public, 20 male and 10 female. The age distribution,
as well as whether the subjects are wearing glasses or contact
lenses, is given in Figure 8. The experiments are conducted
with the approval of the ethics committee of the University of
Oxford, reference SSD/CUREC1/13-064.
F. Experimental Setup
Figure 9 shows our experimental setup. We use an SMI
RED500 eyetracking device with a sampling rate of 500Hz to
collect the raw gaze data. The stimuli are displayed on a 24
inch Dell U2412M monitor with a resolution of 1920x1200
pixels. The viewing distance between the subjects and the
screen is approximately 50cm. In order to reduce distractions
and to minimize the inﬂuence of the experimenter on the
subjects all instructions were displayed on-screen during the
session. Although the gazetracker compensates for minor head
movements during the data collection we asked the participants
to move as little as possible.
Before the session the gazetracker has to be calibrated for
each test subject. This stage consists of a calibration phase
and a veriﬁcation phase in which the error between actual
and estimated viewing angle in degrees is determined. In
order to ensure as high a data quality as possible, we reject
calibrations with a viewing angle error of more than 1◦, either
horizontally or vertically. If the error is too high the calibration
has to be repeated. At the end of the session we repeat the
veriﬁcation phase in order to test whether the initial calibration
8
10−1920−2930−3940−4950+  05101520NoneContact LensesGlassesis still valid. A large veriﬁcation error at this stage indicates
low quality data, most likely due to excessive movements
during the experiments. During testing we observed an average
error of 0.49◦ in the X-direction and 0.52◦ in the Y-direction
immediately after calibration. These errors increased to 0.74◦
and 0.72◦ respectively over the course of the experiment. Given
that the error rates are lower than our threshold even at the end
of the experiment we are conﬁdent in the quality of our data.
VI. RESULTS AND ANALYSIS
In this section we will describe our classiﬁer candidates and
explain how the classiﬁcation of raw samples can be extended
to allow user authentication. We will discuss the impact that
the feature selection and the time over which the data was
collected have on the classiﬁer performance. Finally we will
give insights on how different parameters of our system can
be chosen to reﬂect different security requirements.
A. Classiﬁer Development
We measure the performance of the k-nearest-neighbors
(knn) and Support Vector Machine (SVM) classiﬁers. In order
to determine the optimal parameters for these classiﬁers we
perform a grid search through a deﬁned subset of the parameter
space. For the knn classiﬁer we tested values of k between
1 and 20 and weighting samples uniformly or by euclidean
distance. For the SVM we tested a linear, a polynomial and a
radial kernel function. For all three kernels we varied the soft
margin constant C in powers of ten between 1 and 10000. The
polynomial kernel was used with degrees between 2 and 5 and
for the radial kernel function we tested values of γ between
0.00001 and 10. The best results were achieved with k=5 and
weights based on euclidean distance for knn and an rbf-kernel
with C=10000 and γ=0.001 for the SVM.
B. From Classiﬁcation to Authentication
After completing the training phase and generating a
template for each user the authentication system decides
continuously whether a new sample belongs to the currently
logged in user. This decision can be either based on a single
sample or combine multiple samples. Combining multiple
samples will increase the accuracy of the decision but also
introduces a delay before an imposter can be detected. As
eyetracking provides a stream of new samples at a constant
and high rate we choose to combine several samples for
each authentication decision. Our authentication system is
parametrized through the number of samples n that are used
for the decision and the threshold t which deﬁnes how many
of these samples must support the current user. This procedure
is described in Algorithm 1. A discussion of the effects of both
parameters will be given in the next section.
C. Results and Discussion
In order to ensure a high statistical robustness we split the
datasets into training and test sets using 5-fold stratiﬁed cross
validation, resulting in 80% of the data being used for training
and 20% for testing. The following results reﬂect the average
of the 5 folds. The dataset contains data from all experiments.
The second and third session form the inter-session dataset,
the ﬁrst and second are combined for the 2-weeks dataset. We
Algorithm 1 The authentication algorithm accepts the
current user if at least t out of the last n classiﬁcations
match his user ID. This allows us to control the trade-off
between the FAR and the FRR.
1: Input: t,n,uid
2: classif ications ← []
3: loop
s ← collect sample()
4:
classif ications ← classif ications + classify(s)
5:
window ← last n classiﬁcations
6:
accepted ← all uid ∈ window where count(uid)≥ t
7:
if uid ∈ accepted then
8:
9:
10:
11:
end if
12:
13: end loop
accept sample
reject sample
else
consider all of our subjects as potential imposters of every other
subject. This realistically reﬂects an insider threat scenario in
which every person enrolled in the system could be a potential
attacker. We use two performance metrics: The equal error rate
(EER) and the minimal and maximal class distance (dmin and
dmax). The equal error rate is the rate at which the false accept
rate (FAR) and false reject rate (FRR) are equal and is a good
measure to compare different classiﬁers. The class distance
measures the distance between the template of a user and the
most successful out of the 29 imposters and gives insights
about the distribution of false classiﬁcations. We derive the
class distance dc for each user c from the confusion matrix cm
as follows:
dc = min
i(cid:54)=c
cm[c, c]
cm[c, i]
A class distance lower than 1 means that the best attacker
is more likely to be accepted than the legitimate user, a high
class distance means that the user is harder to impersonate. As
only the best out of the 29 imposters is considered this is an
extremely conservative metric. The equal error rate is computed
using the authentication algorithm described in Section VI-B.
As the parameter that controls the trade-off between the false
accept rate and the false reject rate (the threshold parameter t)
is an integer we report the average of the FAR and the FRR
for the value of t for which they are closest.
The results of our analysis are listed in Table II. The SVM
outperforms the knn classiﬁer for almost every combination of
featureset and dataset. While the training phase is much slower
for the SVM the classiﬁcation decision for a new sample
is virtually instantaneous, therefore this does not constitute
a serious limitation. When using the full feature set the best
performance is achieved with the intra-session dataset. The EER
increases from 3.98% to 6.05% when using the inter-session
dataset. This transition reﬂects degradation effects caused
by technical artifacts (e.g., different calibration accuracies)
across the two sessions. The performance takes another drop
to 7.37% when considering data collected over two weeks.
Given the behavioral nature of our feature set these changes
are to be expected as behavior is usually less stable than
physical characteristics. The fact that the EER is very good but
the minimal class distance is low suggests that our classiﬁer
9
Dataset
Intra-Session
Intra-Session
Inter-Session
Inter-Session
2-weeks
2-weeks
Subjects Classiﬁer EER
30
30
20
20
20
20
knn
SVM
knn
SVM
knn
SVM
Full
dmin
7.07% 0.37
3.98% 0.52
8.86% 0.81
6.05% 1.08
9.27% 0.4
7.37% 0.49
Reduced
dmin
13.92% 0.18
13.6%
0.14
10.87% 0.60
11.17% 0.51
13.83% 0.46
13.18% 0.41
dmax EER
5.71
3.61
2.76
4.07
7.28
4.72
Without
Pupil Diameter
dmax EER
3.6
4.77
2.86
3.00
5.15
4.34
dmin
19.05% 0.25
15.25% 0.22
16.58% 0.76
14.03% 0.54
21.32% 0.31
16.56% 0.45
dmax
1.98
3.44
2.86
3.05
1.92
2.46
TABLE II: Classiﬁer Performance on different datasets and different sets of features. The reduced feature set includes the ten
features selected by the mRMR algorithm (see Table I). The equal error rate was calculated using Algorithm 1 with 180 samples.
dmin and dmax refer to the maximal and minimal relative difference between any user and the most successful imposter.
(a) Intra-Session dataset
(b) Inter-Session dataset
(c) 2-weeks dataset
Fig. 10: Average Equal Error Rates obtained through 5-fold stratiﬁed cross validation on three different datasets using the SVM
classiﬁer. The error bars indicate 95% conﬁdence intervals.
performs extremely well for most users but that the templates
of few users are too similar to allow reliable distinction. In
order to mitigate this problem it would be possible to determine
the closeness of templates directly after the training phase, after
obtaining the class distance for each user pair it is then possible
to give security guarantees for each user. Users whose templates
are not distinctive enough within the target population can then
be authenticated with an alternative mechanism.
When using the reduced featureset described in Section IV
the error rates increase signiﬁcantly. The magnitude of this
change is surprising, as the features that were removed to form
this set exhibit either low distinctiveness or high correlation
with other features. Nevertheless, this difference in error rates
conﬁrms that even features that carry little information on
their own help to correctly classify samples near the decision
boundary. This suggests that it won’t be sufﬁcient for an attacker
to emulate a few distinctive features when using the full feature
set. As the complexity of an imitation attack grows rapidly with
the number of features that have to be emulated this underlines
the resilience of our system against such attacks.
Considering the distinctiveness of the pupil diameter fea-
tures (see Table I) it is not surprising that removing them from
the feature set has a signiﬁcant impact on our performance
metrics. However, the changes of the error rates caused by
increasing time distance is less pronounced for this feature
set. This suggests that a lot of the degradation observed when
using the full feature set was caused by changes in the pupil
diameter features.
Using our classiﬁer in conjunction with the algorithm from
Section VI-B continuous authentication of users is possible.
However, there are still some design decisions to be made.
While the EER is a good measure to compare classiﬁers
it is rarely useful in a real-world environment. In an ofﬁce
environment the FRR should usually be extremely low in order
to avoid a high number of false positives. The ROC curve
in Figure 11 shows that a FRR of 0 is possible when using
the full feature set, in order to achieve this a FAR of 19.2%
has to be taken into account. While this may seem like a
prohibitively high number it is important to remember that our
system does not make a one-time decision but authenticates
users continuously. Conversely, a higher FRR may be acceptable
in a high-security context if it ensures the quick detection of an
attacker. Another parameter that directly impacts the detection
speed is the number of samples used for the authentication
decision. Figure 10 shows the effect of this number on the EER.
Increasing the number of samples up to 40 rapidly decreases
the EER, after that diminishing returns are observed. If the
quick detection of an imposter is important the smallest number
that still yields acceptable error rates should be chosen. It is
noteworthy that our biometric provides a much higher and more
constant sampling rate than those relying on active user behavior
(e.g., typing or mouse movements). Using our sampling rate of
4Hz even the highest number of 180 samples will correspond
10
(a) Full feature set
(b) Reduced feature set
(c) without-pupil feature set
Fig. 11: The ROC curve shows the tradeoff between the false accept rate (FAR) and the false reject rate (FRR) depending on the
threshold parameter t for the SVM classiﬁer.
VII. RELATED WORK
The idea of using physiological or behavioral biometrics
in the context of system security is not new and has been
an active research area for many years. The authors of [30]
provide a comprehensive overview of hard biometrics (e.g.,
ﬁngerprints, iris patterns, DNA) in a security context. The use
of hard biometrics allows the distinction between users with
high accuracy and usually over the entire lifetime of a person.
A person’s biometric features can not usually be changed which