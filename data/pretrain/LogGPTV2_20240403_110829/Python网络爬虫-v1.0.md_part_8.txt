mySpider = SDU_Spider()
mySpider.sdu_init()
水平有限，正则是有点丑。运行的效果如图：
ok，接下来的只是数据的处理问题了。。
第 10 章 一个爬虫的诞生全过程（以山东大学绩点运算为例） | 97
##
凯旋而归
完整的代码如下，至此一个完整的爬虫项目便完工了。
# -*- coding: utf-8 -*-
#---------------------------------------
# 程序：山东大学爬虫
# 版本：0.1
# 作者：why
# 日期：2013-07-12
# 语言：Python 2.7
# 操作：输入学号和密码
# 功能：输出成绩的加权平均值也就是绩点
#---------------------------------------
import urllib
import urllib2
import cookielib
import re
import string
class SDU_Spider:
# 申明相关的属性
def __init__(self):
self.loginUrl = 'http://jwxt.sdu.edu.cn:7777/pls/wwwbks/bks_login2.login' # 登录的url
self.resultUrl = 'http://jwxt.sdu.edu.cn:7777/pls/wwwbks/bkscjcx.curscopre' # 显示成绩的url
self.cookieJar = cookielib.CookieJar() # 初始化一个CookieJar来处理Cookie的信息
self.postdata=urllib.urlencode({'stuid':'201100300428','pwd':'921030'}) # POST的数据
self.weights = [] #存储权重，也就是学分
self.points = [] #存储分数，也就是成绩
第 10 章 一个爬虫的诞生全过程（以山东大学绩点运算为例） | 98
self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(self.cookieJar))
def sdu_init(self):
# 初始化链接并且获取cookie
myRequest = urllib2.Request(url = self.loginUrl,data = self.postdata) # 自定义一个请求
result = self.opener.open(myRequest) # 访问登录页面，获取到必须的cookie的值
result = self.opener.open(self.resultUrl) # 访问成绩页面，获得成绩的数据
# 打印返回的内容
# print result.read()
self.deal_data(result.read().decode('gbk'))
self.calculate_date();
# 将内容从页面代码中抠出来
def deal_data(self,myPage):
myItems = re.findall('.*?(.*?).*?(.*?).*?',myPage,re.S) #获取到学分
for item in myItems:
self.weights.append(item[0].encode('gbk'))
self.points.append(item[1].encode('gbk'))
#计算绩点，如果成绩还没出来，或者成绩是优秀良好，就不运算该成绩
def calculate_date(self):
point = 0.0
weight = 0.0
for i in range(len(self.points)):
if(self.points[i].isdigit()):
point += string.atof(self.points[i])*string.atof(self.weights[i])
weight += string.atof(self.weights[i])
print point/weight
#调用
mySpider = SDU_Spider()
mySpider.sdu_init()
1111
亮亮剑剑！！爬爬虫虫框框架架小小抓抓抓抓 SSccrraappyy 闪闪亮亮登登场场！！
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 100
前面十章爬虫笔记陆陆续续记录了一些简单的 Python 爬虫知识，用来解决简单的贴吧下载，绩点运算自然不在
话下。不过要想批量下载大量的内容，比如知乎的所有的问答，那便显得游刃不有余了点。于是乎，爬虫框架 Sc
rapy 就这样出场了！Scrapy = Scrach+Python，Scrach 这个单词是抓取的意思，暂且可以叫它：小抓抓吧。
小抓抓的官网地址：点我点我。
那么下面来简单的演示一下小抓抓 Scrapy 的安装流程。
具体流程参照：官网教程
友情提醒：一定要按照 Python 的版本下载，要不然安装的时候会提醒找不到 Python。建议大家安装 32 位是因
为有些版本的必备软件 64 位不好找。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 101
##
安装 Python（建议 32 位）
建议安装 Python2.7.x，3.x 貌似还不支持。 安装完了记得配置环境，将 python 目录和 python 目录下的 Scri
pts 目录添加到系统环境变量的 Path 里。在 cmd中输入 python 如果出现版本信息说明配置完毕。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 102
##
安装 lxml
lxml 是一种使用 Python 编写的库，可以迅速、灵活地处理 XML。点击这里选择对应的 Python 版本安装。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 103
##
安装 setuptools
用来安装 egg 文件，点击这里下载 python2.7 的对应版本的 setuptools。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 104
##
安装 zope.interface
可以使用第三步下载的 setuptools 来安装 egg 文件，现在也有 exe 版本，点击这里下载。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 105
##
安装 Twisted
Twisted 是用 Python 实现的基于事件驱动的网络引擎框架，点击这里下载。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 106
##
安装 pyOpenSSL
pyOpenSSL 是 Python 的 OpenSSL 接口，点击这里下载。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 107
##
安装 win32py
提供 win32api，点击这里下载
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 108
##
安装 Scrapy
终于到了激动人心的时候了！安装了那么多小部件之后终于轮到主角登场。 直接在 cmd 中输入 easy_install sc
rapy 回车即可。
第 11 章 亮剑！爬虫框架小抓抓 Scrapy 闪亮登场！ | 109
##
检查安装
打开一个 cmd 窗口，在任意位置执行 scrapy 命令，得到下列页面，表示环境配置成功。
1122
爬爬虫虫框框架架 SSccrraappyy 的的第第一一个个爬爬虫虫示示例例入入门门教教程程
第 12 章 爬虫框架 Scrapy 的第一个爬虫示例入门教程 | 111
我们使用 dmoz.org 这个网站来作为小抓抓一展身手的对象。
首先先要回答一个问题。
问：把网站装进爬虫里，总共分几步？
答案很简单，四步：
• 新建项目 (Project)：新建一个新的爬虫项目
• 明确目标（Items）：明确你想要抓取的目标
• 制作爬虫（Spider）：制作爬虫开始爬取网页
• 存储内容（Pipeline）：设计管道存储爬取内容
好的，基本流程既然确定了，那接下来就一步一步的完成就可以了。
第 12 章 爬虫框架 Scrapy 的第一个爬虫示例入门教程 | 112
##
新建项目（Project）
在空目录下按住 Shift 键右击，选择“在此处打开命令窗口”，输入一下命令：
scrapy startproject tutorial
其中，tutorial 为项目名称。
可以看到将会创建一个 tutorial 文件夹，目录结构如下：
tutorial/
scrapy.cfg
tutorial/
__init__.py
items.py
pipelines.py
settings.py
spiders/
__init__.py
...
下面来简单介绍一下各个文件的作用：
• scrapy.cfg：项目的配置文件
• tutorial/：项目的 Python 模块，将会从这里引用代码
• tutorial/items.py：项目的 items 文件
• tutorial/pipelines.py：项目的 pipelines 文件
• tutorial/settings.py：项目的设置文件
• tutorial/spiders/：存储爬虫的目录
第 12 章 爬虫框架 Scrapy 的第一个爬虫示例入门教程 | 113
##
明确目标（Item）
在 Scrapy 中，items 是用来加载抓取内容的容器，有点像 Python 中的 Dic，也就是字典，但是提供了一些额
外的保护减少错误。
一般来说，item 可以用 scrapy.item.Item 类来创建，并且用 scrapy.item.Field 对象来定义属性（可以理解成
类似于 ORM 的映射关系）。
接下来，我们开始来构建 item 模型（model）。
首先，我们想要的内容有：
• 名称（name）
• 链接（url）
• 描述（description）
修改 tutorial 目录下的 items.py 文件，在原本的 class 后面添加我们自己的 class。 因为要抓 dmoz.org 网站
的内容，所以我们可以将其命名为 DmozItem：
# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html
from scrapy.item import Item, Field
class TutorialItem(Item):
# define the fields for your item here like:
# name = Field()
pass
class DmozItem(Item):
title = Field()
link = Field()
desc = Field()
第 12 章 爬虫框架 Scrapy 的第一个爬虫示例入门教程 | 114
刚开始看起来可能会有些看不懂，但是定义这些 item 能让你用其他组件的时候知道你的 items 到底是什么。 可
以把 Item 简单的理解成封装好的类对象。
第 12 章 爬虫框架 Scrapy 的第一个爬虫示例入门教程 | 115
##
制作爬虫（Spider）
制作爬虫，总体分两步：先爬再取。
也就是说，首先你要获取整个网页的所有内容，然后再取出其中对你有用的部分。
##
爬
Spider 是用户自己编写的类，用来从一个域（或域组）中抓取信息。 他们定义了用于下载的 URL 列表、跟踪链
接的方案、解析网页内容的方式，以此来提取 items。 要建立一个 Spider，你必须用 scrapy.spider.BaseSpid
er 创建一个子类，并确定三个强制的属性：
• name：爬虫的识别名称，必须是唯一的，在不同的爬虫中你必须定义不同的名字。
• start_urls：爬取的 URL 列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些 urls 开
始。其他子 URL 将会从这些起始 URL 中继承性生成。
• parse()：解析的方法，调用的时候传入从每一个 URL 传回的 Response 对象作为唯一参数，负责解析并
匹配抓取的数据(解析为 item)，跟踪更多的 URL。
这里可以参考宽度爬虫教程中提及的思想来帮助理解，教程传送：[Java] 知乎下巴第5集：使用HttpClient工具包
和宽度爬虫。
也就是把 Url 存储下来并依此为起点逐步扩散开去，抓取所有符合条件的网页 Url 存储起来继续爬取。
下面我们来写第一只爬虫，命名为 dmoz_spider.py，保存在 tutorial\spiders 目录下。
dmoz_spider.py 代码如下：
from scrapy.spider import Spider
class DmozSpider(Spider):
name = "dmoz"
allowed_domains = ["dmoz.org"]
start_urls = [
"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
]