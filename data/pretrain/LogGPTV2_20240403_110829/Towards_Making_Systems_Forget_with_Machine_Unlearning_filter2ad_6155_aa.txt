title:Towards Making Systems Forget with Machine Unlearning
author:Yinzhi Cao and
Junfeng Yang
2015 IEEE Symposium on Security and Privacy
2015 IEEE Symposium on Security and Privacy
Towards Making Systems Forget with Machine Unlearning
Yinzhi Cao and Junfeng Yang
Columbia University
{yzcao, junfeng}@cs.columbia.edu
Abstract—Today’s
systems produce a rapidly exploding
amount of data, and the data further derives more data, forming
a complex data propagation network that we call the data’s
lineage. There are many reasons that users want systems to forget
certain data including its lineage. From a privacy perspective,
users who become concerned with new privacy risks of a system
often want the system to forget their data and lineage. From a
security perspective, if an attacker pollutes an anomaly detector
by injecting manually crafted data into the training data set,
the detector must forget the injected data to regain security.
From a usability perspective, a user can remove noise and
incorrect entries so that a recommendation engine gives useful
recommendations. Therefore, we envision forgetting systems,
capable of forgetting certain data and their lineages, completely
and quickly.
This paper focuses on making learning systems forget, the
process of which we call machine unlearning, or simply un-
learning. We present a general, efﬁcient unlearning approach
by transforming learning algorithms used by a system into a
summation form. To forget a training data sample, our approach
simply updates a small number of summations – asymptotically
faster than retraining from scratch. Our approach is general,
because the summation form is from the statistical query learning
in which many machine learning algorithms can be implemented.
Our approach also applies to all stages of machine learning,
including feature selection and modeling. Our evaluation, on four
diverse learning systems and real-world workloads, shows that
our approach is general, effective, fast, and easy to use.
I. INTRODUCTION
A. The Need for Systems to Forget
Today’s systems produce a rapidly exploding amount of
data, ranging from personal photos and ofﬁce documents to
logs of user clicks on a website or mobile device [15]. From
this data, the systems perform a myriad of computations to
derive even more data. For instance, backup systems copy data
from one place (e.g., a mobile device) to another (e.g., the
cloud). Photo storage systems re-encode a photo into different
formats and sizes [23, 53]. Analytics systems aggregate raw
data such as click logs into insightful statistics. Machine learn-
ing systems extract models and properties (e.g., the similarities
of movies) from training data (e.g., historical movie ratings)
using advanced algorithms. This derived data can recursively
derive more data, such as a recommendation system predicting
a user’s rating of a movie based on movie similarities. In short,
a piece of raw data in today’s systems often goes through
a series of computations, “creeping” into many places and
appearing in many forms. The data, computations, and derived
data together form a complex data propagation network that
we call the data’s lineage.
© 2015, Yinzhi Cao. Under license to IEEE.
© 2015, Yinzhi Cao. Under license to IEEE.
DOI 10.1109/SP.2015.35
DOI 10.1109/SP.2015.35
463
463
For a variety of reasons, users want a system to forget
certain sensitive data and its complete lineage. Consider pri-
vacy ﬁrst. After Facebook changed its privacy policy, many
users deleted their accounts and the associated data [69].
The iCloud photo hacking incident [8] led to online articles
teaching users how to completely delete iOS photos including
the backups [79]. New privacy research revealed that machine
learning models for personalized warfarin dosing leak patients’
genetic markers [43], and a small set of statistics on genet-
ics and diseases sufﬁces to identify individuals [78]. Users
unhappy with these newfound risks naturally want their data
and its inﬂuence on the models and statistics to be completely
forgotten. System operators or service providers have strong
incentives to honor users’ requests to forget data, both to keep
users happy and to comply with the law [72]. For instance,
Google had removed 171,183 links [50] by October 2014
under the “right to be forgotten” ruling of the highest court in
the European Union.
Security is another reason that users want data to be
forgotten. Consider anomaly detection systems. The security
of these systems hinges on the model of normal behaviors ex-
tracted from the training data. By polluting1 the training data,
attackers pollute the model, thus compromising security. For
instance, Perdisci et al. [56] show that PolyGraph [55], a worm
detection engine, fails to generate useful worm signatures if
the training data is injected with well-crafted fake network
ﬂows. Once the polluted data is identiﬁed, the system must
completely forget the data and its lineage to regain security.
Usability is a third reason. Consider the recommendation
or prediction system Google Now [7]. It
infers a user’s
preferences from her search history, browsing history, and
other analytics. It then pushes recommendations, such as news
about a show, to the user. Noise or incorrect entries in analytics
can seriously degrade the quality of the recommendation. One
of our lab members experienced this problem ﬁrst-hand. He
loaned his laptop to a friend who searched for a TV show
(“Jeopardy!”) on Google [1]. He then kept getting news about
this show on his phone, even after he deleted the search record
from his search history.
We believe that systems must be designed under the core
principle of completely and quickly forgetting sensitive data
and its lineage for restoring privacy, security, and usability.
Such forgetting systems must carefully track data lineage
even across statistical processing or machine learning, and
make this lineage visible to users. They let users specify
1In this paper, we use the term pollute [56] instead of poison [47, 77].
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
the data to forget with different levels of granularity. For
instance, a privacy-conscious user who accidentally searches
for a sensitive keyword without concealing her identity can
request that the search engine forget that particular search
record. These systems then remove the data and revert its
effects so that all future operations run as if the data had never
existed. They collaborate to forget data if the lineage spans
across system boundaries (e.g., in the context of web mashup
services). This collaborative forgetting potentially scales to
the entire Web. Users trust forgetting systems to comply
with requests to forget, because the aforementioned service
providers have strong incentives to comply, but other trust
models are also possible. The usefulness of forgetting systems
can be evaluated with two metrics: how completely they can
forget data (completeness) and how quickly they can do so
(timeliness). The higher these metrics, the better the systems
at restoring privacy, security, and usability.
We foresee easy adoption of forgetting systems because they
beneﬁt both users and service providers. With the ﬂexibility
to request that systems forget data, users have more control
over their data, so they are more willing to share data with the
systems. More data also beneﬁt the service providers, because
they have more proﬁt opportunities services and fewer legal
risks. In addition, we envision forgetting systems playing a
crucial role in emerging data markets [3, 40, 61] where users
trade data for money, services, or other data because the
mechanism of forgetting enables a user to cleanly cancel a
data transaction or rent out the use rights of her data without
giving up the ownership.
Forgetting systems are complementary to much existing
work [55, 75, 80]. Systems such as Google Search [6] can
forget a user’s raw data upon request, but they ignore the
lineage. Secure deletion [32, 60, 70] prevents deleted data from
being recovered from the storage media, but it largely ignores
the lineage,
too. Information ﬂow control [41, 67] can be
leveraged by forgetting systems to track data lineage. However,
it typically tracks only direct data duplication, not statistical
processing or machine learning,
to avoid taint explosion.
Differential privacy [75, 80] preserves the privacy of each indi-
vidual item in a data set equally and invariably by restricting
accesses only to the whole data set’s statistics fuzzed with
noise. This restriction is at odds with today’s systems such
as Facebook and Google Search which, authorized by billions
of users, routinely access personal data for accurate results.
Unsurprisingly, it is impossible to strike a balance between
utility and privacy in state-of-the-art implementations [43]. In
contrast, forgetting systems aim to restore privacy on select
data. Although private data may still propagate, the lineage of
this data within the forgetting systems is carefully tracked and
removed completely and in a timely manner upon request. In
addition, this ﬁne-grained data removal caters to an individual
user’s privacy consciousness and the data item’s sensitivity.
Forgetting systems conform to the trust and usage models of
today’s systems, representing a more practical privacy vs util-
ity tradeoff. Researchers also proposed mechanisms to make
systems more robust against training data pollution [27, 55].























Fig. 1: Unlearning idea. Instead of making a model directly depend
on each training data sample (left), we convert the learning algorithm
into a summation form (right). Speciﬁcally, each summation is the
sum of transformed data samples, where the transformation functions
gi are efﬁciently computable. There are only a small number of
summations, and the learning algorithm depends only on summations.
To forget a data sample, we simply update the summations and then
compute the updated model. This approach is asymptotically much
faster than retraining from scratch.
However, despite these mechanisms (and the others discussed
so far such as differential privacy), users may still request
systems to forget data due to, for example, policy changes and
new attacks against the mechanisms [43, 56]. These requests
can be served only by forgetting systems.
B. Machine Unlearning
While there are numerous challenges in making systems
forget, this paper focuses on one of the most difﬁcult chal-
lenges: making machine learning systems forget. These sys-
tems extract features and models from training data to answer
questions about new data. They are widely used in many
areas of science [25, 35, 37, 46, 55, 63–65]. To forget a piece
of training data completely, these systems need to revert the
effects of the data on the extracted features and models. We
call this process machine unlearning, or unlearning for short.
A na¨ıve approach to unlearning is to retrain the features
and models from scratch after removing the data to forget.
However, when the set of training data is large, this approach
is quite slow, increasing the timing window during which the
system is vulnerable. For instance, with a real-world data set
from Huawei (see §VII), it takes Zozzle [35], a JavaScript
malware detector, over a day to retrain and forget a polluted
sample.
We present a general approach to efﬁcient unlearning, with-
out retraining from scratch, for a variety of machine learning
algorithms widely used in real-world systems. To prepare for
unlearning, we transform learning algorithms in a system to
a form consisting of a small number of summations [33].
Each summation is the sum of some efﬁciently computable
transformation of the training data samples. The learning
algorithms depend only on the summations, not individual
data. These summations are saved together with the trained
model. (The rest of the system may still ask for individual data
and there is no injected noise as there is in differential privacy.)
Then, in the unlearning process, we subtract the data to forget
464464
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
from each summation, and then update the model. As Figure 1
illustrates, forgetting a data item now requires recomputing
only a small number of terms, asymptotically faster than
retraining from scratch by a factor equal to the size of the
training data set. For the aforementioned Zozzle example, our
unlearning approach takes only less than a second compared to
a day for retraining. It is general because the summation form
is from statistical query (SQ) learning [48]. Many machine
learning algorithms, such as na¨ıve Bayes classiﬁers, support
vector machines, and k-means clustering, can be implemented
as SQ learning. Our approach also applies to all stages of
machine learning, including feature selection and modeling.
We evaluated our unlearning approach on four diverse
learning systems including (1) LensKit [39], an open-source
recommendation system used by several websites for confer-
ence [5], movie [14], and book [4] recommendations; (2) an
independent re-implementation of Zozzle, the aforementioned
closed-source JavaScript malware detector whose algorithm
was adopted by Microsoft Bing [42]; (3) an open-source online
social network (OSN) spam ﬁlter [46]; and (4) PJScan, an
open-source PDF malware detector [51]. We also used real-
world workloads such as more than 100K JavaScript malware
samples from Huawei. Our evaluation shows:
• All four systems are prone to attacks targeting learn-
ing. For LensKit, we reproduced an existing privacy
attack [29]. For each of the other three systems, because
there is no known attack, we created a new, practical data
pollution attack to decrease the detection effectiveness.
One particular attack requires careful injection of mul-
tiple features in the training data set to mislead feature
selection and model training (see §VII).
• Our unlearning approach applies to all learning algo-
rithms in LensKit, Zozzle, and PJScan. In particular,
enabled by our approach, we created the ﬁrst efﬁ-
cient unlearning algorithm for normalized cosine similar-
ity [37, 63] commonly used by recommendation systems
(e.g., LensKit) and for one-class support vector machine
(SVM) [71] commonly used by classiﬁcation/anomaly
detection systems (e.g., PJScan uses it to learn a model of
malicious PDFs). We show analytically that, for all these
algorithms, our approach is both complete (completely
removing a data sample’s lineage) and timely (asymptot-
ically much faster than retraining). For the OSN spam
ﬁlter, we leveraged existing techniques for unlearning.
• Using real-world data, we show empirically that unlearn-
ing prevents the attacks and the speedup over retraining
is often huge, matching our analytical results.
• Our approach is easy to use. It is straightforward to
modify the systems to support unlearning. For each
system, we modiﬁed from 20 – 300 lines of code, less
than 1% of the system.
C. Contributions and Paper Organization
This paper makes four main contributions:
465465
• The concept of forgetting systems that restore privacy, se-
curity, and usability by forgetting data lineage completely
and quickly;
• A general unlearning approach that converts learning al-
gorithms into a summation form for efﬁciently forgetting
data lineage;
• An evaluation of our approach on real-world systems/al-
gorithms demonstrating that it is practical, complete, fast,
and easy to use; and
• The practical data pollution attacks we created against
real-world systems/algorithms.
While prior work proposed incremental machine learning