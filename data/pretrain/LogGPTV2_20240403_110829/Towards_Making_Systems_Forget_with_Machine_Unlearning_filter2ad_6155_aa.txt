# Title: Towards Making Systems Forget with Machine Unlearning

## Authors: Yinzhi Cao and Junfeng Yang
### Columbia University
#### {yzcao, junfeng}@cs.columbia.edu

## Abstract
Modern systems generate an exponentially increasing amount of data, which in turn produces more data, forming a complex network of data lineage. Users often desire systems to forget certain data and its lineage for various reasons, including privacy, security, and usability. From a privacy perspective, users concerned about new risks may want the system to forget their data and its lineage. From a security perspective, if an attacker injects malicious data into a training set, the system must forget this data to restore security. From a usability perspective, removing noise and incorrect entries can improve the quality of recommendations. We propose forgetting systems capable of completely and quickly forgetting specific data and their lineages. This paper focuses on machine unlearning, a process to make learning systems forget. We present a general and efficient unlearning approach by transforming learning algorithms into a summation form. To forget a training data sample, our approach updates a small number of summations, significantly faster than retraining from scratch. Our approach is general, as many machine learning algorithms can be implemented in this form, and it applies to all stages of machine learning. Our evaluation, conducted on four diverse learning systems and real-world workloads, demonstrates that our approach is general, effective, fast, and easy to use.

## 1. Introduction

### 1.1 The Need for Systems to Forget
Today's systems produce a vast and rapidly growing amount of data, ranging from personal photos and office documents to logs of user interactions on websites or mobile devices [15]. These systems perform numerous computations to derive additional data. For example, backup systems copy data from one location to another, photo storage systems re-encode images into different formats and sizes [23, 53], and analytics systems aggregate raw data into insightful statistics. Machine learning systems extract models and properties from training data using advanced algorithms. This derived data can recursively generate more data, such as a recommendation system predicting a user's movie rating based on similarities. In summary, a piece of raw data often undergoes multiple transformations, appearing in various forms and locations. The data, computations, and derived data together form a complex data propagation network, which we call the data's lineage.

For various reasons, users may want a system to forget certain sensitive data and its complete lineage. Privacy concerns are a primary reason. After Facebook changed its privacy policy, many users deleted their accounts and associated data [69]. The iCloud photo hacking incident [8] led to articles teaching users how to delete iOS photos, including backups [79]. New research revealed that machine learning models for personalized warfarin dosing leak patients' genetic markers [43], and a small set of statistics on genetics and diseases can identify individuals [78]. Users concerned about these risks naturally want their data and its influence on models and statistics to be completely forgotten. System operators and service providers have strong incentives to honor these requests, both to keep users satisfied and to comply with the law [72]. For instance, Google removed 171,183 links [50] by October 2014 under the "right to be forgotten" ruling of the European Union's highest court.

Security is another reason for forgetting data. Anomaly detection systems rely on models of normal behavior extracted from training data. If attackers inject malicious data, they can compromise the model and thus the system's security. For example, Perdisci et al. [56] showed that PolyGraph [55], a worm detection engine, fails to generate useful signatures if the training data includes well-crafted fake network flows. Once polluted data is identified, the system must forget the data and its lineage to regain security.

Usability is a third reason. Consider a recommendation system like Google Now [7], which infers user preferences from search and browsing history. Noise or incorrect entries can degrade the quality of recommendations. One of our lab members experienced this issue firsthand when he loaned his laptop to a friend who searched for a TV show ("Jeopardy!"). He continued to receive news about the show even after deleting the search record.

We believe that systems must be designed to completely and quickly forget sensitive data and its lineage to restore privacy, security, and usability. Such systems must carefully track data lineage, even across statistical processing or machine learning, and make this lineage visible to users. They allow users to specify the data to forget at different granularities. For instance, a privacy-conscious user who accidentally searches for a sensitive keyword can request that the search engine forget that particular search record. These systems then remove the data and revert its effects so that future operations proceed as if the data never existed. They collaborate to forget data if the lineage spans multiple systems, potentially scaling to the entire Web. Users trust these systems because service providers have strong incentives to comply with requests, though other trust models are possible. The effectiveness of forgetting systems can be evaluated based on completeness (how completely they forget data) and timeliness (how quickly they do so). Higher metrics indicate better performance in restoring privacy, security, and usability.

We foresee easy adoption of forgetting systems because they benefit both users and service providers. With the flexibility to request data forgetting, users have more control over their data, making them more willing to share it. More data also benefits service providers, offering more profit opportunities and fewer legal risks. Additionally, we envision forgetting systems playing a crucial role in emerging data markets [3, 40, 61], where users trade data for money, services, or other data. The mechanism of forgetting enables users to cleanly cancel data transactions or rent out the use rights of their data without giving up ownership.

Forgetting systems complement existing work [55, 75, 80]. Systems like Google Search [6] can forget raw data upon request but ignore the lineage. Secure deletion [32, 60, 70] prevents deleted data from being recovered but largely ignores the lineage. Information flow control [41, 67] can be leveraged by forgetting systems to track data lineage but typically tracks only direct data duplication, not statistical processing or machine learning, to avoid taint explosion. Differential privacy [75, 80] preserves the privacy of each individual item in a dataset by restricting access to fuzzed statistics, but this restriction conflicts with the need for accurate results in systems like Facebook and Google Search. In contrast, forgetting systems aim to restore privacy on select data. Although private data may still propagate, the lineage within these systems is carefully tracked and removed completely and timely upon request. This fine-grained data removal caters to individual users' privacy concerns and the sensitivity of data items. Forgetting systems conform to the trust and usage models of today's systems, representing a more practical privacy vs. utility tradeoff. Researchers have also proposed mechanisms to make systems more robust against training data pollution [27, 55]. Despite these mechanisms, users may still request systems to forget data due to policy changes or new attacks [43, 56].

### 1.2 Machine Unlearning
While there are numerous challenges in making systems forget, this paper focuses on one of the most difficult: making machine learning systems forget. These systems extract features and models from training data to answer questions about new data. To forget a piece of training data, these systems need to revert the effects of the data on the extracted features and models. We call this process machine unlearning, or unlearning for short.

A naive approach to unlearning is to retrain the features and models from scratch after removing the data. However, this is slow and increases the system's vulnerability window. For example, with a real-world dataset from Huawei, it takes Zozzle [35], a JavaScript malware detector, over a day to retrain and forget a polluted sample.

We present a general and efficient unlearning approach that does not require retraining from scratch. We transform learning algorithms into a form consisting of a small number of summations [33]. Each summation is the sum of some efficiently computable transformation of the training data samples. The learning algorithms depend only on the summations, not individual data. These summations are saved with the trained model. During unlearning, we subtract the data to forget from each summation and update the model. As Figure 1 illustrates, forgetting a data item now requires recomputing only a small number of terms, significantly faster than retraining. For the Zozzle example, our unlearning approach takes less than a second compared to a day for retraining. It is general because the summation form is from statistical query (SQ) learning [48]. Many machine learning algorithms, such as naive Bayes classifiers, support vector machines, and k-means clustering, can be implemented as SQ learning. Our approach applies to all stages of machine learning, including feature selection and modeling.

We evaluated our unlearning approach on four diverse learning systems:
1. LensKit [39], an open-source recommendation system used by several websites for conference [5], movie [14], and book [4] recommendations.
2. An independent re-implementation of Zozzle, a closed-source JavaScript malware detector whose algorithm was adopted by Microsoft Bing [42].
3. An open-source online social network (OSN) spam filter [46].
4. PJScan, an open-source PDF malware detector [51].

We used real-world workloads, including over 100K JavaScript malware samples from Huawei. Our evaluation shows:
- All four systems are vulnerable to attacks targeting learning. For LensKit, we reproduced an existing privacy attack [29]. For the other three systems, we created new, practical data pollution attacks to decrease detection effectiveness.
- Our unlearning approach applies to all learning algorithms in LensKit, Zozzle, and PJScan. We created the first efficient unlearning algorithms for normalized cosine similarity [37, 63] and one-class support vector machine (SVM) [71]. Analytically, our approach is both complete and timely.
- Using real-world data, we empirically show that unlearning prevents attacks and is significantly faster than retraining.
- Our approach is easy to use. Modifying the systems to support unlearning required changing 20 to 300 lines of code, less than 1% of the system.

### 1.3 Contributions and Paper Organization
This paper makes four main contributions:
- The concept of forgetting systems that restore privacy, security, and usability by completely and quickly forgetting data lineage.
- A general unlearning approach that converts learning algorithms into a summation form for efficient data lineage forgetting.
- An evaluation of our approach on real-world systems and algorithms, demonstrating that it is practical, complete, fast, and easy to use.
- The creation of practical data pollution attacks against real-world systems and algorithms.

While prior work has proposed incremental machine learning, our approach provides a more comprehensive and efficient solution for unlearning.