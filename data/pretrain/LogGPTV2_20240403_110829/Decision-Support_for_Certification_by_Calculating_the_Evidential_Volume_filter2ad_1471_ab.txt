will affect the trust we have in this ability, which is mea-
sured through the probability of failure on demand (pfd).
Even though all requirements are important, there will most
likely be some, the absence of which would inﬂuence an as-
sessor’s conﬁdence in the product more severely than others
1. In the case of replacing one procedure with an alterna-
tive, the type of alternative evidence and the rigour applied
when implementing the alternative will inﬂuence the over-
all belief in the product. If many pieces of evidence of “less
importance” are missing, then the total mass of missing evi-
dence may create again a considerable plunge in conﬁdence.
Depending on the relevance of the clauses not met and the
compensating arguments that were presented, i.e. the over-
all picture of evidence, the assessor will decide in favour of
or against acceptance. It seems beneﬁcial to formalize these
concepts by introducing a quantitative measure modelling
the degree of compliance of a product with a standard. This
could support consistency in assessment outcome. Ideally
one would like different, independent assessors to come to
the same conclusion when faced with the same scenario of
given evidence. We make a start on such a formalization by
introducing the following approach.
3 Evidential Volume Approach (EVA)
In this section, we will introduce a simple model to ex-
press the overall degree of compliance of a product with
a standard through a weighted aggregation of the evidence
that we observe.
Let Pj be a phase in the overall safety-lifecycle and let’s
assume that for Pj a set of nj requirements R1j, ..., Rnj j
have been listed in the standard. An example could be the
Code Implementation sub-phase in Fig.1 and the set of re-
quirements listed for that sub-phase. We assume that an as-
sessor makes the observation that either a requirement has
been met or that it has not been met. Let Iij be an indica-
tor taking on the value 1 if the assessor observes that Rij is
met and 0 otherwise. The set {Iij, i = 1, ..., nj} describes
thus the whole set of evidence present at the end of the as-
sessment. One could also assume that the assessor chooses
a value between 0 and 1 to express the extent to which a
requirement has been met, or the quality with which is has
been met. However, at this stage we model the assessor’s
observation as binary value. Note that the quality of a par-
ticular piece of evidence could also be measured through a
separate network, see also [4], for example using a network
1Hereby absence could mean that a procedure was simply not docu-
mented or a recommended technique replaced by an alternative technique.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:50 UTC from IEEE Xplore.  Restrictions apply. 
containing security questions on how the technique was im-
plemented.
Based on the set of observed evidence and the relevance
of each piece of evidence within the overall context the as-
sessor decides whether or not to accept the product. We
model the relevance of a piece of evidence by introducing
a set of weights, {w1j, ..., wnj j}, that are assigned to the
requirements R1j, ..., Rnj j. The weights describe a rank-
ing of the importance of different requirements in the whole
set of listed requirements with respect to achieving success
in phase Pj. Success of a phase could mean for example:
“module testing has been performed thoroughly using re-
commendations of standard”. We want the weighting fac-
i=1 wij = 1. The require-
tors to be normalized such that
ment that is considered to have greatest impact in the suc-
cessful completion of a phase or sub-phase would receive
the highest weight etc. If all requirements are believed to be
of equal importance, the weights are chosen to be equal.
(cid:1)nj
The weights need to be ﬁxed by a panel of experts in
the ﬁeld that the standard relates to, e.g. nuclear, avionics,
railway etc. Thus, expert elicitation is required. However,
this process would only need to be performed once for a
standard. After that the approach could be generically used
by any assessor on any assessment scenario relating to that
particular standard. For each SIL or IT, a different set of
weights will be used because the number of requirements
changes as the SIL or IT become more stringent.
Given an assessor’s set of observations I1j, ..., Inj j for
the product under assessment and the generic set of weights
elicited for the standard, w1j, ..., wnj j, we now introduce
the following measure modelling the degree of compliance
of the product with the standard.
Deﬁntion 1 (Evidential Volume (EV))
a) Let Pj be a
sub-phase of the safety lifecycle deﬁned in a standard
S. Let (R1j, ..., Rnj j) be the requirements listed in S
under Pj. Let (I1j, ..., Inj j) describe the observations
made by an assessor, with Iij = 1 if Rij is met and 0
otherwise, i = 1, ..., nj. Let (w1j, ..., wnj j) be weights
describing the relative importance of R1j, ..., Rnj j in
achieving success in phase Pj. Then,
EVj := fj(Iij, wij, i = 1, ..., nj) =
nj(cid:2)
i=1
wij·Iij, (2)
is called Evidential Volume (EV) for Pj.
b) Let Pj, j = 1, . . . , n be sub-phases of a parent phase
P . Let αj, j = 1, ..., n be weighting factors express-
ing the relative importance of a sub-phase within the
αj = 1. Then the EV of the
parent phase P with
n(cid:1)
j=1
parent phase is calculated as
EVP = fP (EVj, αj) =
n(cid:3)
j=1
EV
αj
j
.
(3)
EVj and EVP take on values in [0, 1]. The EVs of all
main phases are calculated using (2) and (3). The overall
EV of a product is then calculated as in (3) where EVj
is replaced by EVP , the EVs of all main phases of the
life-cycle process.
Generally fj and fP are aggregation functions, which
can adopt different forms. Equation (2) above shows how to
compile the observations made on requirements of a phase
into an evidential volume (EV) for that phase. Equation (3)
shows how to compile EVs obtained for different phases
together into one overall EV of the lifecycle process con-
sisting of these phases.
In the next two sections we will
discuss the assignment of weights wij, αj and the choice of
aggregation functions fj, fP .
3.1 Assigning weights
One of the main inputs needed to calculate an EV, are
the weights wij, αj. Ideally the weights should be assigned
by a panel of international experts. This requires recrui-
ting a set of experts, and eliciting and coordinating input on
a large scale. This was beyond the scope of this project.
However, in this section, we shall discuss some example
methods to assign weighting factors. The phase-weights αj
model the severity of loss in conﬁdence when putting no
demonstrable effort into phase Pj. We suggest initially to
choose these weights as equal. This models the case that
no phase is assumed to be of higher importance than others.
If expert belief becomes available that assigns more impact
to one phase over others, these weights can be adjusted ac-
cordingly.
The weights wij are allocated to the requirements of
a sub-phase. Here they represent the relative importance
of a technique or procedure to contribute to success of the
corresponding phase Pj. They should be assigned for each
SIL since the set of relevant requirements changes as the
SIL increases. In the following we are giving an example
of how the process of asssigning weights wij could be
initiated.
In an initial study performed with our industrial collab-
orators an attempt was made to place ﬁrst of all rankings
on the requirements listed in the PES Guidelines. As a
preliminary step in the process, it was decided to assign
one of three rankings to each requirement:
V-Very Important.
I-Important.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:50 UTC from IEEE Xplore.  Restrictions apply. 
L-Less Important.
These rankings could be assigned numerical values at a
later stage. At IT 10−2 it was found that nearly every re-
quirement was given an “I” ranking. This was conﬁrmed by
looking at a smaller set of guidelines, which contains only
39 requirements in total and was thus easier to overview.
All of the 39 requirements there were given an “I” ranking,
for two of them an “L” ranking would have been also pos-
sible. It was thus decided that within the PES Guidelines
too, for IT 10−2 all requirements should initially be given
an “I” ranking. At IT 10−3, all the IT 10−2 requirements
are needed plus the additional IT 10−3 requirements.
It
was suggested that the (few) new requirements might be
given a “V” ranking because they increase the conﬁdence
in the system pfd. The same is done for IT 10−4. The
IT 10−3, 10−4 requirements are given “V” ranking, and
the IT 10−2 requirements are given “I” ranking. Please
note that these are initial considerations. They do not
represent a formal suggestion but serve as the basis for
discussion of possible weighting approaches and as an
example of how the reasoning behind allocating weights
might look like. For the example described here, in order
to calculate numerical weights, one proceeds as follows.
Assume that for all the requirements Rij, i = 1, . . . , nj in
a phase Pj, a ranking r(Rij) of either I, V, or L has been
assigned and we deﬁne e.g. I=10, V=100, L=1. The ﬁnal
weight for Rij is then calculated as wij := r(Rij)
. This
r(Rij)
nj(cid:1)
i=1
gives a normalized set of weights for phase Pj.
Another possible approach to identify weights is to use
Multi Attribute Decision Theory. We have not tried this
approach within our current project, but because its poten-
tial usefulness for larger-scale expert elicitation we want to
mention it here. As demonstrated by Li et al [5], Multi At-
tribute Utility Theory can be used for the ranking of soft-
ware engineering measures with respect to their ability to
contribute to reliability. This approach seems very suitable
to be used in the context of ranking requirements within
a standard for their importance in generating success of a
software development phase. In [5] for a set of software en-
gineering measures a score is elicited with respect to a set
of attributes. Examples of chosen attributes are credibility,
experience, repeatability, validation, cost, beneﬁt and rele-
vance to reliability. Measures are rated with respect to an
attribute on a scale from 0 to 5 or 6. The scores given by
the experts are compiled into a single number using a linear
additive aggregation equation with equal weights.
In the context of IEC 61508 the same or similar attributes
could be used to assess the requirements listed. For exam-
ple: What is the practical experience with the technique?
Has the technique a history of discovering systematic
errors? How easily can the technique be validated? Can the
technique be used to quantify a reliability related measure?
How necessary is the technique in the development of a
reliable product (e.g. structural coverage is often seen as
necessary testing requirement)? How necessarya technique
is seen in the achievement of a safety claim seems to
relate to the “level of recommendation” introduced e.g. in
IEC 61508 for some techniques. This level takes on the
states “Not Recommended”, “Recommended” and “Highly
Recommended”.
If each requirement is rated according to the chosen set of
attributes, the total score achieved from a panel of experts
would then point towards a relative weight in the set of
all requirements. The process suggested in [5] leads to
non-normalized rankings, which need to be normalized
when used in the context of building an evidential volume.
In the next section, we will discuss the choice of aggrega-
tion functions, which are needed to propagate the weighted
evidence from requirements level up to the safety lifecycle
level.
3.2 Aggregation functions
The choice of aggregation functions depends on the
type of information one wants to aggregate. On the
sub-phase level we need to aggregate observations made
on the single requirements. For this level of aggregation,
we have chosen the linear additive form of equation (2).
This is motivated as follows. Requirements listed for
one sub-phase constitute different techniques that are all
perceived to contribute to successful implementation of
the same aspect of the safety lifecycle. For example in the
sub-phase CodeImplementation, the techniques such as no
dynamic objects, use of coding standards, limited use of
pointersetc. provide a different component of a reasonable
programming regime. Thus these techniques contribute
all to some degree to the successful implementation of
that particular sub-phase.
If one is missing, still some
good practice has been followed and thus a certain quality
of programming has been achieved. We assume that the
overall degree of compliance increases linearly with each
piece of evidence that becomes available. We choose
the linear function here because the set of requirements
can be basically seen as separate evidence contributing to
some degree to the same measure, e.g. quality of Code
Implementation.
After having established the evidential volumes for each
phase, we need to aggregate the evidential volumes of the
phases into one overall evidential volume. For this level
we have chosen the multiplicative aggregation function in
equation (3). This is motivated as follows. In the overall