served that the same decision assignment can yield close-to-optimal
QoE even if some of the inputs to E2E’s decision-making policy
have changed slightly. Therefore, E2E caches its decision assign-
ment in a decision lookup table that the shared-resource service can
query for every new request. The keys in this table are the buckets
of the external delays, and the corresponding value is the decision
assigned to each bucket. The exact definition of decisions varies
across use cases. For instance, in a distributed database, the decision
of a specific external delay bucket is the probability of sending a
request to each of the replicas, if the request’s external delay falls
in the bucket. The lookup table is only updated when one of the
input variables has changed by a “significant amount”. The policy
for deciding this is orthogonal and not something we prescribe;
e.g., it could be if the J-S divergence [37] between the new and old
distributions exceeds a certain threshold.
In E2E, a request needs to
Fault tolerance of E2E controller:
wait for its resource allocation decision from the E2E controller,
which can therefore become a single point of failure for the whole
system. This can be mitigated in three ways. First, if the E2E con-
troller fails, the shared-resource service can still make QoE-aware
decisions by looking up the request’s external delay in the most
recently cached decision lookup table (see above). Second, the E2E
controller is replicated with the same input state (QoE model, ex-
ternal delay model, server-side delay model), so when the primary
controller fails, a secondary controller can take over using standard
leader election [15, 27]. Finally, in the case of total E2E failure, the
shared-resource service can simply bypass E2E and use its default
resource allocation policy.
6 USE CASES
We demonstrate E2E’s practical usefulness by integrating it into two
popular web infrastructure services, depicted in Figure 13: replica
selection in a distributed database and message scheduling in a
message broker. In both cases, E2E makes minimal changes to the
shared-resource service and only relies on the control interface
exposed by them. We evaluate E2E’s overhead in §7.3.
Use case #1: Distributed database. We choose Cassandra [2] as
the distributed database, and use E2E to select the replica for each re-
quest (this operation is common to other distributed databases, and
Figure 13: Use cases of E2E
not specific to Cassandra). In particular, we made two changes. First,
we modified the existing replica selection logic (getReadExecutor
of ReadExecutor) of the Cassandra client. Our new logic stores
the decision lookup table (§5) received from the E2E controller in a
local data structure. When a new request arrives, it looks up the
request’s external delay in the table to get the selected replica’s
IP. Second, we modified the client service callback function (in
RequestHandler) to keep track of the load (number of concurrent
requests) and the observed (server-side) delay of each replica. In
practice, the replication level, i.e., the number of replicas for each
key, is usually much smaller than the total number of servers. A sim-
ple replication strategy, adopted by Cassandra and other databases
like MongoDB [3], is to divide the servers into replica groups and
store a copy of the entire database in each group. This replication
strategy is a good fit for E2E, which now simply has to choose a
replica group for each incoming request. It also allows E2E to affect
server-side delays by ensuring that some replica groups are less
loaded and used to process QoE-sensitive requests.
Use case #2: Message broker. We choose RabbitMQ [4] as the
message broker (other message brokers can work with E2E in a
similar way). RabbitMQ manages its resource by using priority
queues and associating each request with a priority level. Requests
with high priority are served before requests with low priority.
Similar to the Cassandra implementation, we made two changes
to integrate E2E. First, we wrote the E2E controller logic in a
python script and pass it to RabbitMQ as the default scheduling
policy (through queue_bind) when the RabbitMQ service is ini-
tialized. Second, we modified the per-request callback function
(confirm_delivery) to track each request’s progress and the queue-
ing delay in the message broker.
Implementation details: E2E requires three models as input in
order to run. We describe our realizations of these models below,
though other approaches are certainly possible.
• QoE model: Our E2E prototype uses the QoE models derived from
the Microsoft traces and our MTurk user study, shown in Figure 3
and detailed in Appendix B. The QoE model needs to be updated
only when the web service changes its content substantially; we
do not update it in our prototype.
• External delay model: Our E2E prototype builds the external delay
distribution from per-request external delay measurements in
recent history. The external delays are currently provided by
our traces and are not calculated in real-time for each request,
though the latter is necessary in a production deployment (see
§9). We use batched updates to reduce the overhead of keeping
the distribution up-to-date. Specifically, we found in our traces
ClientDistributed Database(Cassandra)E2E ControllerReplica SelectorPublisherConsumersE2E ControllerMessage SchedulerMessage Broker(RabbitMQ)(a) Use case #1: Replica selection in a distributed database(b) Use case #2: Message scheduling in a message brokerE2E: Embracing User Heterogeneity to Improve QoE on the Web
SIGCOMM ’19, August 19–23, 2019, Beijing, China
that it is sufficient to update the external delay distribution every
10 seconds, because a 10-second time window usually provides
enough requests to reliably estimate the distribution, and the
distribution remains stable within this window.
• Server-side delay model: Our prototype builds the server-side
delay model offline, by measuring the service delay distributions
induced by different resource allocations. For instance, to build a
server-side delay model for the distributed database, we measure
the processing delays of one server under different input loads:
{5%, 10%,..., 100%} of the maximum number of requests per second.
For the message broker the profiling is slightly more complicated:
we have to consider both the number of requests at each priority
level and the total number of requests at higher priority levels. In
practice we need not profile all possible allocations: it is sufficient
to sample some of them and extrapolate the others. Also, the
requests are homogeneous in both of our uses cases, as is typically
the case in web services. For services that serve heterogeneous
requests (e.g., both CPU-intensive and memory-intensive jobs),
or where the effects of different resource allocations do not easily
extrapolate to each other, more advanced techniques may be
required to ensure the profiling is efficient.
7 EVALUATION
We evaluate E2E using a combination of trace-driven simulations
and real testbed experiments. Our key findings are:
• E2E can substantially improve QoE: Users spend 11.9% more web
session time (more engagement) compared to the default resource
allocation policy in our traces; this improvement accounts for
77% of the best-possible improvement if server-side delays were
zero. (§7.2)
• E2E has low system overhead: E2E incurs only 0.15% additional
server-side delay and requires 4.2% more compute resources per
request. (§7.3)
• E2E can tolerate moderate estimation errors (up to 20%) on the
external delays, while still retaining over 90% of the QoE im-
provement attainable if there are no errors. (§7.4)
7.1 Methodology
Both our trace-driven simulator and our testbeds use the external
delay model derived from our traces (Table 1) and the QoE model
from Figure 3. The simulator is described in more detail in §2.3.
Testbed setup: To complement our trace-driven simulations, which
unrealistically assume the server-side delay distribution is fixed,
we create two real testbeds on Emulab—one for Cassandra and one
for RabbitMQ, as described in §6. We feed requests from our traces
to each testbed in chronological order with their recorded external
delays, and use the actual testbed processing time as the server-
side delays. To show the impact of system load, we speed up the
replay by reducing the interval between two consecutive requests
by a speedup ratio (e.g., a speed-up ratio of 2 means we halve the
interval between every two consecutive requests). In the Cassandra
(distributed database) testbed, each request is a range query for
100 rows in a table of 5 million keys, which are replicated to three
replicas (three Emulab nodes), so each replica has a copy of each
key. The key size is 70B and the value size is 1KB. In the RabbitMQ
(messaging broker) testbed, each request is a 1KB message sent
to RabbitMQ (one Emulab node), and a consumer pulls a message
from RabbitMQ every 5ms. Each Emulab node has one 3.0GHz Intel
Xeon processor, 2GB RAM, and 2x146GB HDD storage, and are
connected to each other by a 1Gbps Ethernet link.
We do not claim that this testbed is a faithful replication of the
production system that generated our traces. Rather, we use the
testbeds to allow resource allocation policies to affect the server-
side delay distributions, as opposed to being constrained by the
fixed server-side delays in our traces. We use the traces only to
reflect the real external delays of users issuing requests to a service.
Baselines: We compare E2E against two baseline policies:
• Default policy (unaware of the heterogeneity of QoE sensitivity):
In the simulator, it simply gives each request its recorded server-
side delay. In RabbitMQ, it uses First-In-First-Out (FIFO) queueing.
In Cassandra, it balances load perfectly across replicas.
• Slope-based policy (aware of the heterogeneity of QoE sensitivity
but suffers from the problem described in §3.2): In the simulator, it
gives the shortest server-side delay to the request whose external
delay has the steepest slope in the QoE model, and so forth (see
§2.3). In RabbitMQ, it gives the highest priority to the request
whose external delay has the steepest slope in the QoE model,
and so forth. In Cassandra, it is the same as E2E’s policy, except it
replaces the request-decision mapping algorithm with the slope-
based algorithm above.
Metric of QoE gain: We measure the QoE gain of E2E (and its
variants) by the relative improvement of their average QoE over
that of the default policy, i.e., (QE2E − Qdefault)/(Qdefault).
7.2 End-to-end evaluation
Overall QoE gains: Figure 14 compares the QoE gains of E2E and
the slope-based policy over the existing default policy, in our traces
and our testbeds. For page types 1 and 2 we use time-on-site as the
QoE metric (with Figure 3(a) as the QoE model), and for page type
3 we use user rating as the QoE metric (with Figure 3(b) as the QoE
model). Using user rating vs. time-on-site has negligible impact on
our conclusions, as they lead to very similar QoE models (Figure 3).
Figure 14(a) shows that in our traces, E2E achieves 12.6–15.4%
better average QoE than the default policy, whereas the slope-
based policy has only 4–8% improvement. This suggests that E2E
addresses the limitation of the slope-based policy discussed in §3.2.
To put these gains into perspective, we consider an idealized policy
(labeled “idealized” in the figure) that cuts all server-side delays to
zero (i.e., the best a web service could possibly do by cutting server-
side delays). We see that the QoE gain of E2E already accounts for
74.1–83.9% of the QoE gain of this idealized policy.
Figure 14(b) also compares the QoE of E2E and the baseline
policies when feeding requests of page type 1 to the Cassandra and
RabbitMQ testbeds. We used a 20× speedup ratio to sufficiently
load the systems (we explore the tradeoff between system load and
QoE gain below). The results show similar gains in QoE, with both
systems achieving a large fraction of the best possible gains.
Better QoE-throughput tradeoffs: Figure 15 compares the QoE
of E2E and the default policy under different loads, in our traces
and our testbeds. E2E strikes a better QoE-throughput tradeoff than
both the default policy and the slope-based policy.
Figure 15(a) shows the results for different hours of the day in our
traces (12am, 4am, 3pm, 8pm, 10pm all in US Eastern Time), which
SIGCOMM ’19, August 19–23, 2019, Beijing, China
X. Zhang et al.
Figure 14: Overall QoE improvement of E2E and the slope-based
policy over the default policy.
Figure 16: The additional overhead of E2E vs. the total overhead of
running the testbeds.
Figure 15: QoE improvement of E2E under different levels of loads.
Throughput is normalized against the highest per-hour throughput
(a) and the total testbed capacity (b, c).
exhibit a natural variation in load. Compared to the off-peak hour
(leftmost, at 0.6), the peak hour (rightmost, at 1.0) sees 40% more
traffic and, as a result, has 20.1% lower QoE. E2E achieves similar
QoE during the peak hour as the default policy does during the
off-peak hour. In other words, E2E achieves 40% higher throughput
than the default policy without a drop in QoE.
Figures 15(b) and (c) compare the QoE of E2E with those of the
baseline policies in our testbeds, while varying the load (speedup
ratio 15× to 25×, normalized as 0.6 to 1 throughput). E2E always
improves QoE, though to varying degrees. E2E’s gain is marginal
under low load, since all decisions have similar, good performance
(e.g., all replicas have low read latency when Cassandra is under-
loaded). As the load increases, however, E2E’s gain grows rapidly:
at system capacity, E2E achieves 25% QoE gain over the default
policy. This can be explained as follows (using Cassandra as an
example). The default policy (perfect load balancing) drives every
replica to a moderately high load, so all requests are affected by
bad tail latencies. In contrast, E2E allocates load unevenly so that at
least one replica is fast enough to serve the QoE-sensitive requests.
7.3 Microbenchmarks
We examine the overheads incurred by E2E in computing cost,
decision delay, and fault tolerance.
System overhead: We compare the total resource consumption
of running each testbed with and without E2E. Figure 16 shows
the additional overhead of E2E in CPU and RAM usage. We see
that the overhead of E2E is several orders of magnitude lower than
the total overhead of running the Cassandra or RabbitMQ testbeds
themselves. Moreover, the CPU and RAM overheads grow more
slowly than those of the testbed service as the load increases.
Decision delay: Figure 17 shows the effectiveness of our two
decision delay-reduction optimizations (§5), using the Cassandra
testbed (with speedup ratio 20x). We see that (1) spatial coarsening
Figure 17: Per-request delay reduction due to spatial and temporal
coarsening (§5).
Figure 18: E2E can tolerate loss of the controller.
(bucketization of external delays) reduces the decision delay by
four orders of magnitude, and (2) temporal coarsening (caching E2E
decisions in a lookup table) reduces the decision delay by another
two orders of magnitude. The resulting per-request response delay
is well below 100µs, less than 0.15% of Cassandra’s response delay.
At the same time, we see that these reductions in decision-making
delay only have a marginal impact on QoE. Note that E2E does not
need to make a decision on the arrival of each request, due to these
optimizations. Instead, decisions are made periodically and cached