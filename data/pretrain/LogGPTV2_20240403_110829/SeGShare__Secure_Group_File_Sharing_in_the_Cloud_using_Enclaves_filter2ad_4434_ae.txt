H2=
H(/D.acl)    H(P1)
/D/F
C
/D/F.acl
P2
path
content
path
cont.
H11=
H(/D/F)    H(C)
H12=
H(/D/F.acl)    H(P2)
Fig. 2: Example of a ﬁle system (left) and its corresponding
hash tree (right). ⊕ denotes the concatenation of hashes.
Normally, on a leaf ﬁle update or addition, its hash and all
hashes on the path to the root ﬁle need to be updated. This
requires to access all sibling ﬁles and to combine their hashes.
Also, on a leaf ﬁle read, a validation is performed starting from
this ﬁle to the root ﬁle always accessing all sibling ﬁles. We
propose two modiﬁcations to optimize this process: First, we
replace all individual hashes and the combination of hashes by
so-called multiset hashes [62]. They allow the calculation of a
one-way hash of an individual value; combine multiple hashes
to a ﬁxed-size bit string, independent of the order of hashes;
to add/remove elements incrementally and efﬁciently; and to
check the equality of two multiset hashes efﬁciently. We refer
to [62] for details and a security proof that is based on the
hardness of breaking the underlying pseudorandom functions.
This modiﬁcation improves leaf ﬁle updates or additions,
because it allows updates of the hash of each inner ﬁle by
subtracting the hash of the no longer valid child ﬁle and to
add the new hash, without accessing any sibling ﬁle. Second,
depending on the number of child ﬁles, each inner ﬁle stores
multiple bucket hashes and one main hash. The bucket hashes
store a combination of child ﬁles’ main hashes, whereby a
hash over the child ﬁles’ path determines the used bucket, and
the main hash stores a combination of the hashed ﬁle path,
the hashed ﬁle content, and its bucket hashes. This change
slightly deteriorates update performance as two hashes have
to be updated for every level of the tree. However, for leaf ﬁle
validation, it is sufﬁcient to recalculate and compare a single
bucket hash per tree level, which only requires an access to
all ﬁles in the same bucket.
E. Rollback Protection for Whole File System
Even with the protection from Section V-D, an attacker can
still rollback the whole ﬁle system. The key to mitigate this
rollback is to protect the root hash against rollbacks as it
represents a state of the complete ﬁle system. Based on TEE
functionality, we propose two solutions to protect the root hash
and with it prevent rollbacks of the whole ﬁle system (S5).
First, if the TEE offers a protected memory that can only
be accessed by a speciﬁc enclave and is persisted across
restarts, it is sufﬁcient to write/read the root hash into/from
this memory, instead of storing it in the root ﬁle. Second, if the
TEE offers a monotonic counter that can only be accessed by
a speciﬁc enclave and is persisted across restarts, we propose
the following. On each ﬁle update, the trusted ﬁle manager
increments the TEE’s monotonic counter and writes the new
counter value into the root ﬁle before encryption. On validity
checks of the root hash, it compares the TEE’s monotonic
counter with the counter value stored in the root ﬁle.
The group store’s and deduplication store’s root hash have
to be protected by the same mechanism to protect the rollback
of all permissions and deduplicated ﬁles, respectively.
SGX provides monotonic counters, but
im-
plementation has issues [63], e.g., increments are slow and
the counter wears out fast. Until a better hardware-based
monotonic counter is available, one can use ROTE [63].
LCM [64] is a completely different approach, but it requires
periodic interactions with the majority of users.
the current
F. SeGShare Replication
As we show in our evaluation section, SeGShare has a
very low latency. Nonetheless, it might be necessary to deploy
SeGShare on multiple application servers if many users want
to use the ﬁle sharing service. Assuming that all enclaves
access the data from one central data repository, two changes
are necessary: (1) the untrusted ﬁle manager must be extended
to access data from the central data repository, and (2) all
enclaves need access to the same root key SKr.
The ﬁrst change is only an implementation issue and thus,
we only discuss the second in detail. In the following, we
denote enclaves that already have SKr as root enclaves and the
others as non-root enclaves. We propose that the CA tasks one
enclave with the generation of SKr during the provisioning of
the server certiﬁcate. The CA provides all other enclaves with
addresses of root enclaves during the same process. Each non-
root enclave randomly selects one root enclave and performs
remote attestation with it. If the measurements of both enclaves
are equal, the non-root enclave is assured to communicate with
another enclave that was compiled for the same CA, as the
CA’s public key is hard-coded. During remote attestation, a
secure channel is established and the root enclave transfers
SKr over it.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:33:52 UTC from IEEE Xplore.  Restrictions apply. 
484
SeGShare replication is also useful for ﬁle system owners,
which might be afraid to lose access to their ﬁles, because
SKr is only accessible by a single enclave. With the proposed
method, SKr is contained inside trusted enclaves at all time,
but still usable on multiple replicas.
To combine the whole ﬁle system rollback protection and
SeGShare replication, it is necessary to use a non-local pro-
tected memory or monotonic counter for each store. We note
that locking problems and data storage replication is beyond
the scope of this paper.
G. File System Backup
SeGShare supports ﬁle system backups in a straightforward
manner: the cloud provider only has to copy the ﬁles on disk.
Backup restoration depends on the enclave that handles the
restored data. If the enclave is the same that wrote the ﬁles in
the ﬁrst place, it poses the required decryption key. Otherwise,
the SeGShare replication process described in Section V-F is
necessary.
Restoration becomes more complicated if the whole ﬁle
system rollback protection is active, because it might be
necessary to restore an old state. We propose that the CA
can send a signed reset message to the enclave for this case.
The enclave checks the validity of the message’s signature,
reads the stored hashes from the root ﬁles of the various
stores, recalculates the root hashes, and compares the hashes.
Assuming a successful check and the monotonic counter based
rollback solution, the enclave overwrites the stored monotonic
counter with the TEE’s current monotonic counter.
VI. IMPLEMENTATION
Our prototype is implemented in C/C++ using the Intel
SGX SDK (version 2.5). The prototype follows the WebDAV
standard [65], which is an extension to the HTTP standard
designed for the management of changes and permissions to
web resources. WebDAV makes the prototype compatible with
existing clients on Android [66], iOS [67], Windows [68],
Mac [68], and Linux [69]. The secure channel to transfer
messages is established with TLSv1.2 using the ECDHE-
RSA-AES256-GCM-SHA384 cipher suite, and the MSet-
XOR-Hash construction [62] is used for multiset hashes. We
implemented the ﬁlename and directory structure hiding and
the rollback protection for individual ﬁles extensions.
The prototype tackles three performance problems. First, a
main performance bottleneck is the TLS stack. Unfortunately,
publicly available SGX-enabled TLS stacks [70], [71] are
mainly designed for embedded scenarios and do not provide
the desired performance. Intel only provides an SGX opti-
mized version of OpenSSL’s cryptographic library, without
networking capabilities. Our prototype combines Intel’s cryp-
tographic library with the network part of OpenSSL (version
1.1.1c) [72]. Second, switches into and out of the enclave have
a high overhead [73]. To mitigate this problem, our prototype
uses switchless calls (see Section II-A) for our TLS library
and for Intel’s Protected File System Library. Third, SGX has
restricted protected memory space. Our prototype addresses
this problem via streaming, i.e., users send and receive small,
ﬁxed-size chunks and the enclave processes one chunk at a
time, which also includes storage operations. Thus, the enclave
only requires a small, constant size buffer for each request.
VII. EVALUATION
A. Security Evaluation
In this section, we focus on SeGShare’s main security
objective: end-to-end protection of user ﬁles (S3). The basis
for S3 is SeGShare’s setup phase and mutual authentication
during runtime (see Section IV-A): the trusted CA securely
provisions certiﬁcates only to valid SeGShare enclaves and
users, SeGShare enclaves only accept users with a valid
certiﬁcate, and users only send ﬁles to an enclave, which can
present a valid certiﬁcate. Based on this trust, a TLS channel is
established between user applications and SeGShare enclaves
protecting all messages in transit. The enclave code has
plaintext access to messages and ﬁle contents, but the enclave
protects the processing and processed data. The enclave also
enforces authorizations according to our access control model
(see Table I), which enforces that an attacker is restricted to
the union of permissions of the users under her control. As
discussed in the design and extensions sections, the enclave
protects the integrity and conﬁdentiality of all ﬁles stored in
untrusted storage (S1, S2), enforces revocations immediately
(S4), and mitigates rollbacks (S5). Overall, user ﬁles are
protected in transit, during processing, and during storage.
We note that SeGShare’s security hinges on a trusted
enclave, which we assume in our attacker model. Still, we
kept the enclave code as small as possible, as this reduces the
probability of security-relevant implementation errors, unin-
tended leakages, hidden malware, and side-channel leakages.
Besides the Intel SGX SDK, the enclave has only 8102 lines
of code, and 2376 of these are due to our TLS implementation,
which can be replaced by a formally veriﬁed version [74].
B. Performance Evaluation
In
this
discuss
experiments
section, we
regarding
SeGShare’s latency for upload and download, membership
addition and revocation, permissions addition and revocation,
and rollback protection. We end the section with a discussion
about SeGShare’s storage overhead. All latency measurements
are performed with two machines hosted at Microsoft Azure:
a client with 8 GiB RAM and 2 vCPU cores of an Intel
Xeon CPU E5-2673 v4 @ 2.30GHz located in the central US
region, and an SGX-enabled server with 16 GiB RAM and
4 vCPU cores of an Intel Xeon E-2176G CPU @ 3.70GHz
located in the east US region. The latencies are measured
from a request start to response end at the user application
and presented as mean of 100 runs with a 95% conﬁdence
interval (in plots). Sending and processing is interleaved due
to our streaming technique and hence, we do not present the
pure network latency of each request.
In the ﬁrst experiment, we upload and download ﬁles with
sizes from 1 MB to 200 MB to SeGShare. For the baseline, we
execute the same test with two TLS-enabled — but plaintext
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:33:52 UTC from IEEE Xplore.  Restrictions apply. 
485
storing — WebDAV servers: Apache HTTP Server (version
2.4) [75] and nginx (version 1.17.8) [76]. Fig. 3 shows that
uploads and downloads of a 200 MB ﬁle, on average, take
2.39 s and 2.17 s for SeGShare, 4.74 s and 2.62 s for the
Apache server, and 1.84 s and 0.93 s for the nginx server.
Overall, the upload and download performance of SeGShare
is in between the two plaintext WebDAV servers.
Fig. 3: Mean latency of 1000 up-/downloads with different ﬁle
sizes.
In the second experiment, we measure the latency to
add/revoke a user to/from his ﬁrst group. These member-
ship operations only affect the member list ﬁle of the user.
Therefore, they are independent of the number of permissions
|rP|, stored ﬁles |F S|, inherit ﬂags |rI|, ﬁle owners |rF O|,
group owners |rGO|, and the ﬁle sizes. Furthermore, they are
also independent of the number of members the group had
before, because the member list ﬁle contains only the group
memberships of the user. On average, it takes 154.05 ms and
153.40 ms for additions and revocations, respectively.
In the third experiment, we measure the latency of
adding/revoking a user to/from a group if the user is al-
ready a member of several groups. Again, only the mem-
ber list ﬁle is affected, and the latency is independent of
|rP|,|F S|,|rI|,|rF O|,|rGO|, and the ﬁle sizes. However, the
latency now depends logarithmically on the number of the
user’s group memberships, because a logarithmic search is
necessary to insert or remove a group membership into or
from the member list ﬁle. Even up to 1000 group member-
ships, Fig. 4 shows that this dependency is negligible in the
mean latency, which is between 150.29 ms and 150.92 ms for
additions, and 150.11 ms and 151.13 ms for revocations.
Fig. 4: Mean latency of addition and revocation of group
memberships and group permissions (x-Axis: number of mem-
berships/permissions before operation).
In the fourth experiment, we measure the latency of
adding/revoking a group permission if several groups already
have access. For these operations, only permissions in the ACL
ﬁle are accessed, and thus, the latency is independent of the
number of group memberships |rG|,|F S|,|rI|,|rF O|,|rGO|,
and the ﬁle sizes. The latency depends logarithmically on the
number of groups having access, but Fig. 4 shows that this
dependency is again negligible in the total latency.
In the last latency experiment, we evaluate the overhead of
SeGShare’s individual ﬁle rollback protection extension with
the following measurement for x ∈ [0, 14]. As preparation,
we upload (2x − 1) 10 kB ﬁles to SeGShare according to two
different directory structures: (1) directories are organized as
a binary tree and each leaf contains one ﬁle and (2) all ﬁles
a stored ﬂat under the root. Then, we measure the upload and
download of one additional 10 kB ﬁle. Fig. 5 shows that due
to our optimizations, the overhead introduced for uploads is
negligible in the total latency. The minimal, average download
latencies for directory structure (1) and (2) are 111.65 ms and
111.65 ms. Even for 16,384 ﬁles, the average latency only
increases to 115.93 ms and 121.95 ms.
Fig. 5: Mean latency with enabled and disabled individual ﬁle
rollback protection extension and different directory structures.
The storage overhead for each ﬁle f ∈ F S depends on
f’s ACL ﬁle and the overhead introduced by Intel’s Protected
File System Library for both ﬁles. Remember that the size
of an ACL ﬁle depends on the number of ﬁle owners and
group permissions, and the library uses a Merkle tree with
4 kB nodes. Our prototype uses 32 bit for the number of
ﬁle owners and the inheritance ﬂag, and 32 bit for each ﬁle