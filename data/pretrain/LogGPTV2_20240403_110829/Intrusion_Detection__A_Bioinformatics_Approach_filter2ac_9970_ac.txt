characteristic groups of commands in a tested block with 
similar groups in the user’s signature.  This requires that 
we heavily penalize any gaps that may be inserted into the 
user signature, as we do not want commands in the tested 
block  to  be  aligned  with  gaps  in  the  user’s  signature.  
Similarly, we would like to be able to insert gaps into the 
tested  block  to  simulate  the  insertion  of  commands 
between characteristic groups of commands in the user’s 
signature. This requires  that  we  provide  a  slightly  lesser 
penalty  for  gaps  in  the  tested  block.    Matches  should 
positively influence the score of an alignment, and should 
be  chosen  so  that  matches  are  preferred  to  gaps.  
Mismatches are kept at a constant score of 0, as a blanket 
reward or penalty for any mismatch would unfairly favour 
certain alignments, and would not disallow concept drift. 
Given the above criteria, we chose scores of +1 for a 
match  between  two  aligned  commands,  -2  for  a  gap 
placed in the tested block, -3 for a gap placed in the user’s 
signature,  and,  of  course,  0  for  a  mismatch  between 
aligned  commands.    This  scoring  scheme  appears  to 
provide very reasonable detection and false positive rates, 
and  is  intuitively  suited  to  the  requirements  of  our 
problem. 
4. Experiment Overview 
4.1 SEA Data 
to  use 
To  facilitate  comparison  with  other  masquerade 
detection  algorithms,  we  have  chosen 
the 
masquerade  data  provided  by  Schonlau  et  al.  [15], 
abbreviated  to  SEA,  as  a  basis  for  our  experimentation.  
The SEA data was created using the UNIX acct auditing 
utility,  which  records  user’s  commands  augmented  with 
other  metrics  of  interest.    For  our  use,  we  only  concern 
ourselves  with  the  command  entries  that  have  been 
produced  by  this  utility.    The  SEA  data  provides  50 
blocks of 100 commands each (5000 total commands) for 
each user, which can be assumed to be intrusion-free and 
are used as training data for our system.  In addition, we 
are  provided  with  100  blocks  of  100  commands  each 
(10000 total commands) for each user, in which we must 
determine if a masquerade attack has occurred.  To create 
this data, commands were taken from 70 individual users, 
and separated into two groups.  One group, made up of 50 
users, was used as our test subjects, while the other group, 
made up of the remaining 20 users, had their commands 
interspersed into the data of the 50 user test group.  The 
data from the 20 users was to be used as the masquerade 
data to be detected.  The SEA data has been the de facto 
standard  for  masquerade  detection  algorithm  testing 
thanks  to  its  wide-spread  use  and  the  difficulty  of 
obtaining  alternative  data  due  to  privacy  concerns.  As  a 
result, SEA data is the obvious choice for our tests. 
4.2 Experiment Metrics and Parameters 
Our  experimentation  focuses  on 
the  various  parameters  of 
the  effects  of 
changing 
the  alignment 
algorithm  on  the  false  positive  and  false  negative  rates.  
One of the benefits of this particular approach is the sheer 
number of tunable parameters.  These parameters include: 
reward  for  matches,  penalties  for  gaps  inserted  into  the 
user’s  signature  or  into  the  tested  blocks,  rewards  or 
penalties for mismatches, the threshold score for detection 
of  intrusions,  user  signature  length,  and  tested  block 
length. 
To  best  facilitate  comparison  with  other  masquerade 
detection  algorithms,  we  use  false  positive  rate,  false 
negative rate, and hit rate metrics to determine how well 
our alignment algorithm performed.  A false positive is a 
non-intrusion  block 
labeled  as 
containing an intrusion.  A false negative is an intrusion 
block  that  the  algorithm  has  labeled  as  non-intrusion.  
Finally, a hit is an intrusion block that the algorithm has 
intrusion.  False 
properly 
labeled  as  containing  an 
that 
the  algorithm 
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:52:05 UTC from IEEE Xplore.  Restrictions apply. 
positives, false negatives and hits are computed for each 
user, transformed into corresponding rates, which are then 
summed  and  averaged  over  all  50  users.  Figure  5 
summarizes the metric calculations used by the algorithm. 
be  considered  a  non-intrusion.    Conversely,  we  can 
choose a lower threshold percentage, which would allow 
for  a  more  lax  security  environment  with  less  intrusive 
alerts by allowing the score to be significantly lower than 
the average of the user. 
f = number of false positives 
n = number of non-intrusion command sequence blocks  
u = number of users (50 in our case) 
false positiveoverall=([Σi
users (fi/ni)]/u)*100 
fn = number of false negatives 
n = number of intrusion command sequence blocks 
c = number of users who have at least one intrusion block 
false negativeoverall=([Σi
users(fni/ni)]/c)*100 
hit rateoverall  = 100 – false negativeoverall
Figure 5: Metric calculations 
5. Results 
5.1 Threshold Determination 
instead 
threshold  score,  we  decided 
To  facilitate  proper detection, a  threshold  score  must 
be  determined  to  define  at  which  point  a  score  is 
indicative of an attack.  Rather than choosing an arbitrary 
and  static 
to 
determine  the  initial  threshold  score  for  each  user  by 
cross-validating the user’s signature against itself.  We do 
this  by  taking  20  randomly  chosen,  100  command 
sections  of  the  user’s  signature  and  aligning  it  to  a 
randomly  chosen  1000  command  section  of  the  same 
user’s  signature.    This  allows  us  to  create  an  initial 
average  score  that  is  similar  to  the  score  that  the  user’s 
testing data should produce.  Additionally, we update this 
average  as new  testing  blocks  are  checked  by  averaging 
the  current  testing  block’s  score,  and  all  tested  block 
scores previous to it, with the initial average produced by 
the  training  data.    We  then  take  a  percentage  of  that 
average  as  the  threshold  score.    This  allows  us  to 
customize  the  threshold  for  each  user  so  that  if  a 
particular  user  did  not  have  consistently  high  scoring 
alignments  with  their  user  signature,  this  user’s  testing 
blocks will not be unduly flagged as intrusions.  This, in 
particular, allows our algorithm to be somewhat forgiving 
of concept drift.   
We  are  also  able  to  choose  a  threshold  percentage 
which is appropriate with the amount of sensitivity which 
we  would  like  to  express  in  the  detection  process.    For 
instance, if we are more concerned with keeping a secure 
environment,  then  we  would  not  mind  an  additional 
amount of false positive alarms in exchange for increased 
masquerade  detection,  so  we  can  then  use  a  higher 
percentage threshold so that the required alignment score 
would need be much closer to that user’s average score to 
t
e
g
a
n
e
c
r
e
P
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%
Threshold Percentage
False Negative
False Positive
Figure 6: False negative and false positive vs. 
threshold percentage 
5.2 Comparison to Local Alignment 
As  previously  discussed,  our  semi-global  alignment 
algorithm  is  actually  a  modification  of  the  Smith-
Waterman local alignment algorithm [17].  By comparing 
our  semi-global  alignment  algorithm  to  the  original 
Smith-Waterman  algorithm,  we  are  able  to  identify  the 
unique  ability  of  our  modified  algorithm  to  detect 
masquerade  attacks  in  the  SEA  data.    This  comparison 
also gives us an indication of an appropriate length for the 
user’s  signature.  Good  results  for  the  local  alignment 
algorithm,  which  were not achieved,  would  indicate  that 
the 
tested  block  could  be  better  aligned  with  a 
subsequence  of  the  full  5000  command  user  signature 
sequence, rather than the full user signature. 
While 
the 
local  alignment  algorithm  performs 
comparably  with  our  modified  semi-global  algorithm  in 
areas of low sensitivity (low  false positive rates and low 
hit rates) and high sensitivity (high false positive rates and 
high hit rates), it falls significantly below the performance 
of our algorithm for median sensitivity, arguably the most 
significant area  of  detection  because  it  provides  the  best 
trade-off  between  detection  hit  rates  and  false  positive 
rates.  This indicates that using subsequences of the user’s 
signature  provides  no  benefit  to  the  detection  process.  
Additionally, breaking the 5000 command user signature 
into 
logistical 
problems  for  patterns  which  may  cross  subsequence 
boundaries.    It  is,  therefore,  most  intuitive  to  keep  the 
5000  command  user  signature  as  one  sequence,  and  to 
change  the  parameters  of  the  alignment  algorithm  to 
subsequences 
introduces 
additional 
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:52:05 UTC from IEEE Xplore.  Restrictions apply. 
discourage  gaps  in  the  user  signature,  as  we  mentioned 
above,  to  provide  an  accurate  alignment  of  the  tested 
block to the user’s signature.  Similarly, it is intuitive to 
use a tested block size of 100 commands because the SEA 
data marks each 100 command block as an intrusion or a 
non-intrusion,  and  provides  no  information  on  which 
specific commands make up the intrusion.  This limits the 
tested  block  size  to  100  commands,  as  larger  or  smaller 
block sizes could not be checked for correctness. 
%
t
i
H
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
0%
5%
10%
15%
20%
25%
30%
False Positive %
35%
40%
45%
50%
Semi-Global Alignment
Local Alignment
Figure 7: Hit rate as a function of false positive 
rate for semi-global and local alignment methods 
on SEA data 
5.3 Command Mismatch Scoring 
While  the  semi-global  alignment  algorithm  works 
fairly  well  by  not  rewarding  or  punishing  mismatches, 
these  mismatches  can  be  used  to  better  determine  how 
well  the  tested  block  aligns  to  the  user’s  signature,  and 
therefore  better  tailor  our  algorithm  to  the  problem  of 
masquerade  detection. 
  We  can  use  a  customized 
mismatch scoring system to allow for the possibility that 
the legitimate user may have interchanged one command 
with another in a particular alignment.  This allows us to 
punish commands that are not as likely to be interchanged 
while rewarding commands that have a good likelihood of 
being interchanged with each other. Figure 8 summarizes 
the mismatch score calculation. 
M = Mismatch score 
S = # of occurrences of the intrusion block command in the 
C = # of distinct commands in the user’s signature 
user’s signature 
M=[S/(5000/C)]-1 
If(M>=1){ M=1} 
Figure 8: Mismatch score calculation 
We  use  the ratio  of  the  number  of  times  a  particular 
command in the tested block actually occurs in the user’s 
signature  to  the  expected  number  of  occurrences  a 
command  in  the  user’s  signature.    We  then  subtract  1 
from  this ratio  and limit  the maximum  score  to  1.   This 
essentially puts the mismatch score on a real number scale 
from  -1  to  1,  such  that  if  the  tested  block’s  command 
never  occurs,  or  occurs  fewer  times  than  the  average 
command,  we  penalize  the  mismatch,  but  if  the  tested 
block’s  command  occurs  more  times  than  the  expected 
average number of occurrences per command, we reward 
the mismatch.  Meanwhile, if the particular command has 
the same number of occurrences as the expected average 