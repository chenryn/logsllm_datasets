duce response times since it avoids early shufﬂe reads.
6 Detailed Burst ORAM Design
The Burst ORAM design is based on ObliviStore, but
incorporates many fundamental functional and system-
level changes. For example, Burst ORAM replaces or
revises all the semaphores used in ObliviStore to achieve
our distinct goal of online IO prioritization while main-
taining security and avoiding deadlock. Burst ORAM
also maximizes client space utilization, implements the
XOR technique to reduce online IO, revises the shuf-
ﬂer to schedule efﬁcient jobs ﬁrst, and implements level
caching to reduce overall IO.
6.1 Overall Architecture
Figure 5 presents the basic architecture of Burst ORAM,
highlighting key components and functionality. Burst
ORAM consists of two primary components, the online
Requester and the ofﬂine Shufﬂer, which are controlled
by the main event loop ORAM Main. Client-side mem-
ory allocation is shown in Figure 6.
ORAM Main accepts new block requests (reads and
writes) from the client, and adds them to a Request
USENIX Association  
23rd USENIX Security Symposium  755
7
Position Map
Shuffle Buffer
Overflow Space
Local Space
Level Cache
Figure 6: Burst ORAM Client Space Allocation. Fixed
client space is reserved for the position map and shufﬂe
buffer. A small amount of overﬂow space is needed for
blocks assigned but not yet evicted (data cache in [24]).
Remaining space is managed by Local Space and con-
tains evictions, early shufﬂe reads, and the level cache.
Queue. On each iteration, ORAM Main tries advancing
the Requester ﬁrst, only advancing the Shufﬂer if the Re-
quester needs no IO, thereby prioritizing online IO. The
Requester and Shufﬂer use semaphores (Section 6.2) to
regulate access to network bandwidth and client space.
The Requester reads each request from the Request
Queue,
identiﬁes the desired block’s partition, and
fetches it along with any necessary dummies. To en-
sure oblivious behavior, the Requester must wait until
all dummy blocks have been fetched before marking the
request satisﬁed. All Requester IO is considered online.
The Shufﬂer re-encrypts blocks fetched by the Re-
quester, shufﬂes them with other blocks, and returns
them to the server. The Shufﬂer is responsible for manag-
ing shufﬂe jobs, including prioritizing efﬁcient jobs and
implementing level caching. All IO initiated by the shuf-
ﬂer is considered ofﬂine or shufﬂe IO.
6.2 Semaphores
Resources in Burst ORAM are managed via semaphores,
as in ObliviStore. Semaphores are updated using only
server-visible information, so ORAM can safely base its
behavior on semaphores without revealing new informa-
tion. Since Burst ORAM gives online IO strict priority
over shufﬂe IO, our use of semaphores is substantially
different than ObliviStore’s, which tries to issue the same
amount of IO after each request. ObliviStore uses four
semaphores: Shufﬂing Buffer, Early Cache-ins, Eviction,
and Shufﬂing IO. In Burst ORAM, we use three:
• Shufﬂe Buffer manages client space reserved for
blocks from active shufﬂe jobs, and differs from
ObliviStore’s Shufﬂing Buffer only in initial value.
• Local Space manages all remaining space, com-
bining ObliviStore’s Early Cache-in and Eviction
semaphores.
• Concurrent IO manages concurrent block trans-
fers based on network link capacity, preventing
It dif-
the Shufﬂer from starving the Requester.
fers fundamentally from ObliviStore’s Shufﬂing IO
semaphore, which manages per-request shufﬂe IO.
Shufﬂe Buffer semaphore.
Shufﬂe Buffer gives the
number of blocks that may be added to the client’s shuf-
ﬂe buffer. We initialize it to double the maximum parti-
tion size (under 2.4√N total for N > 210), to ensure that
the shufﬂe buffer is large enough to store at least two in-
progress shufﬂe jobs. When Shufﬂe Buffer reaches 0, the
Shufﬂer may not issue additional reads.
Local Space semaphore. Local Space gives the num-
ber of blocks that may still be stored in remaining client
space (space not reserved for the position map or shufﬂe
buffer). If Local Space is 0, the Requester may not fetch
more blocks. Blocks fetched by the Requester count to-
ward Local Space until their partition’s shufﬂe job is ac-
tivated and they are absorbed into Shufﬂe Buffer. Once a
block moves from Local Space to Shufﬂe Buffer, it is con-
sidered free from the client, and more requests may be
issued. The more client space, the higher Local Space’s
initial value, and the better our burst performance.
Concurrent IO semaphore. Concurrent IO is initial-
ized to the network link’s block capacity. Queuing a
block transfer decrements Concurrent IO, and complet-
ing a transfer increments Concurrent IO. The Shufﬂer
may only initiate a transfer if Concurrent IO > 0. How-
ever, the Requester may continue to initiate transfers and
decrement Concurrent IO even if it is negative. This
mechanism ensures that no new shufﬂe IO starts while
there is sufﬁcient online IO to fully utilize the link. If no
online IO starts, Concurrent IO eventually becomes pos-
itive, and shufﬂe IO resumes, ensuring full utilization.
6.3 Detailed System Behavior
We now describe the interaction between ORAM Main,
the Requester, the Shufﬂer, and the semaphores in detail.
Accompanying pseudocode can be found in Appendix A.
ORAM Main (Algorithm 1). Incoming read and write
requests are asynchronously added to the Request Queue.
During each iteration, ORAM Main ﬁrst tries to advance
the Requester, which attempts to satisfy the next request
from the Request Queue. If the queue is empty, or Local
Space too low, ORAM Main advances the Shufﬂer in-
stead. This mechanism suppresses new shufﬂe IO during
a new burst of requests until the Requester has fetched as
many blocks as possible.
For each request, we evict v blocks to randomly cho-
sen partitions, where v is the eviction rate, set to 1.3 as
in ObliviStore [23]. When evicting, if the Requester has
previously assigned a block to be evicted to partition p,
then we evict that block. If there are no assigned blocks,
then to maintain obliviousness we evict a new dummy
block instead. Eviction does not send a block to the
server immediately. It merely informs the Shufﬂer that
756  23rd USENIX Security Symposium 
USENIX Association
8
the block is ready to be shufﬂed into p.
Requester (Algorithm 2). To service a request, the Re-
quester ﬁrst identiﬁes the partition and level containing
the desired block. It then determines which levels require
early shufﬂe reads, and which need only standard reads.
If Local Space is large enough to accommodate the re-
trieved blocks, the requester issues an asynchronous re-
quest for the necessary blocks Else, control returns to
ORAM Main, giving the Shufﬂer a chance to free space.
The server asynchronously returns the early shufﬂe
read blocks and a single combined block obtained from
all standard-read blocks using the XOR technique (Sec-
tion 4). The Requester extracts the desired block from
the combined block or from an early shufﬂe read block,
then updates the block (write) or returns it to the client
(read). The Requester then assigns the desired block for
eviction to a randomly chosen partition.
Shufﬂer (Algorithm 3). The Shufﬂer may only proceed
if Concurrent IO > 0. Otherwise, there is pending on-
line IO, which takes priority over shufﬂe IO, so control
returns to ORAM Main without any shufﬂing.
The Shufﬂer places shufﬂe jobs into three queues
based on phase. The New Job Queue holds inactive jobs,
prioritized by efﬁciency. The Read Job Queue holds ac-
tive jobs for which some reads have been issued, but not
all reads are complete. The Write Job Queue holds active
jobs for which all reads, not writes, are complete.
If all reads have been issued for all jobs in the Read
Job Queue, the Shufﬂer activates the most efﬁcient job
from the New Job Queue, if any. Activating a job moves
it to the Read Job Queue and freezes its read/write lev-
els, preventing it from being updated by subsequent evic-
tions. It also moves the job’s eviction and early shufﬂe
read blocks from Local Space to Shufﬂe Buffer, freeing
up Local Space to handle online requests. By ensuring
that all reads for all active jobs are issued before activat-
ing new jobs, we avoid hastily activating inefﬁcient jobs.
The Shufﬂer then tries to decrement Shufﬂe Buffer to
determine whether a shufﬂe read may be issued. If so,
the Shufﬂer asynchronously fetches a block for a job in
the Read Job Queue. If not, the Shufﬂer asynchronously
writes a block from a job in the Write Job Queue instead.
Unlike reads, writes do not require Shufﬂe Buffer space,
so they can always be issued. The Shufﬂer prioritizes
reads since they are critical prerequisites to activating
new jobs and freeing up Local Space. The equally costly
writes can be delayed until Shufﬂe Buffer space runs out.
Once all reads for a job complete, the job is shufﬂed:
dummy blocks are added as needed, then all are per-
muted and re-encrypted. We then move the job to the
Write Job Queue. When all writes ﬁnish, we mark the
job complete and remove it from the Write Job Queue.
6.4 Burst ORAM Security
We assume the server knows public information such as
the values of each semaphore and the start and end times
of each request. The server also knows the level conﬁg-
uration of each partition and the size and phase of each
shufﬂe job, including which encrypted blocks have been
read from and written to the server. We must prevent
the server from learning the contents of any encrypted
block, or anything about which plaintext block is being
requested. Thus, the server may not know the location of
a given plaintext block, or even the prior location of any
previously requested encrypted block.
All of Burst ORAM’s publicly visible actions are, or
appear to the server to be, independent of the client’s sen-
sitive data access sequence. Since Burst ORAM treats
the server as a simple block store, the publicly visi-
ble actions consist entirely of deciding when to transfer
which blocks. Intuitively, we must show that each action
taken by Burst ORAM is either deterministic and depen-
dent only on public information, or appears random to
the server. Equivalently, we must be able to generate a
sequence of encrypted block transfers that appears in-
distinguishable from the actions of Burst ORAM using
only public information. We now show how each Burst
ORAM component meets these criteria.
ORAM Main & Client Security. ORAM Main (Algo-
rithm 1) chooses whether to advance the Requester or the
Shufﬂer, and depends on the size of the request queue
and the Local Space semaphore. Since the number of
pending requests and the semaphores are public, ORAM
Main is deterministic and based only on public informa-
tion. For each eviction, the choice of partition is made
randomly, and exactly one block will always be evicted.
Thus, every action in Algorithm 1 is either truly random
or based on public information, and is trivial to simulate.
Requester Security. The Requester (Algorithm 2) must
ﬁrst identify the partition containing a desired block.
Since the block was assigned to the partition randomly
and this is the ﬁrst time it is being retrieved since it was
assigned, the choice of partition appears random to the
server. Within each partition, the requester determin-
istically retrieves one block from each occupied level.
The choice from each level appears random, since blocks
were randomly permuted when the level was created.
The Requester singles out early shufﬂe reads and re-
turns them individually. The identity of levels that re-
turn early shufﬂe reads is public, since it depends on the
number of blocks in the level. The remaining blocks are
deterministically combined using XOR into a single re-
turned block. Finally, the request is marked satisﬁed only
after all blocks have been returned, so request completion
time depends only on public information.
The Requester’s behavior can be simulated using only
USENIX Association  
23rd USENIX Security Symposium  757
9
public information by randomly choosing a partition and
randomly selecting one block from each occupied level.
Blocks from levels with at most half their original blocks
remaining should be returned individually, and all others
combined using XOR and returned. Once all blocks have
been returned, the request is marked satisﬁed.
Shufﬂer Security. As in ObliviStore, Shufﬂer (Algo-
rithm 3) operations depend on public semaphores. Job
efﬁciency, which we use for prioritizing jobs, depends on
the number of blocks to be read and written to perform
shufﬂing, as well as the number of early shufﬂe reads
and blocks already evicted (not assigned). The identity
of early shufﬂe read levels and the number of evictions is
public. Further, the number of reads and writes depends
only on the partition’s level conﬁguration. Thus, job efﬁ-
ciency and job order depend only on public information.
Since the Shufﬂer’s actions are either truly random (e.g.
permuting blocks) or depend only on public information
(i.e. semaphores), it is trivial to simulate.
Client Space. Since fetched blocks are assigned ran-
domly to partitions, but evicted using an independent
process, the number of blocks awaiting eviction may
grow. The precise number of such blocks may leak in-
formation about where blocks were assigned, so it must
be kept secret, and the client must allocate a ﬁxed amount
of space dedicated to storing such blocks (see Overﬂow
Space in Figure 6). ObliviStore [23] relies on a proba-
bilistic bound on overﬂow space provided in [24]. Since
Burst ORAM uses ObliviStore’s assignment and evic-
tion processes, the bound holds for Burst ORAM as well.
Level caching uses space controlled by the Local Space
semaphore, so it depends only on public information.
7 Evaluation
We ran simulations comparing response times and band-
width costs of Burst ORAM with ObliviStore and an in-
secure baseline, using real and synthetic workloads.
7.1 Methodology
7.1.1 Baselines
We compare Burst ORAM and its variants against two
baselines. The ﬁrst is the ObliviStore ORAM described
in [23], including its level compression optimization. For
fairness, we allow ObliviStore to use extra client space
to locally cache the smallest levels in each partition. The
second baseline is an insecure scheme without ORAM
in which blocks are encrypted, but access patterns are
not hidden. It transfers exactly one block per request.
We evaluate Burst ORAM against ObliviStore since
ObliviStore is the most bandwidth-efﬁcient existing
ORAM scheme. Other schemes require less client stor-
age [25], but incur higher bandwidth costs, and thus
would yield higher response times. We did not include
We explicitly measure online, effective, and overall
bandwidth costs.
In the insecure baseline, all are 1X,
so response times are minimal. However, if a burst has
high enough frequency to saturate available bandwidth,
requests may still pile up, yielding large response times.
7.1.3 Workloads
We use three workloads. The ﬁrst consists of an end-
less burst of requests all issued at once, and compares
changes in bandwidth costs of each scheme as a func-
tion of burst length. The second consists of two identi-
cal bursts with equally-spaced requests, separated by an
idle period. It shows how response times change in each
scheme before and after the idle period.
The third workload is based on the NetApp Dataset [2,
14], a corporate workload containing ﬁle system accesses
from over 5000 corporate clients and 2500 engineering
clients during 100 days. The ﬁle system uses 22TB of its
31TB of available space. More details about the work-
load are provided in the work by Leung et al. [14].
results from Path-PIR [17] because it requires substan-
tially larger block sizes to be efﬁcient, and its response
times are dominated by the orthogonal consideration of
PIR computation. Path-PIR reports response times in the
40–50 second range for comparably-sized databases.
7.1.2 Metrics
We evaluate Burst ORAM and our baselines using re-
sponse time and bandwidth cost as metrics (see Section
2). We measure average, maximum, and p-percentile re-
sponse times for various p. A p-percentile response time
of t seconds indicates that p percent of the requests were
satisﬁed with response times under t seconds.
Our NetApp workload uses a 15 day period (Sept. 25
through Oct. 9) during which corporate and engineering
clients were active. Requested chunk sizes range from a
few bits to 64KB, with most at least 4KB [14]. Thus, we
chose a 4KB block size. In total, 312GB of data were
requested using 8.8· 107 4KB queries.
We conﬁgure the NetApp workload ORAM with a
32TB capacity, and allow 100GB of client space, for a
usable storage increase of 328 times. For Burst ORAM
and ObliviStore, at least 33GB is consumed by the posi-
tion map, and only 64GB is used for local block storage.
The total block count is N = 233. Blocks are divided into
(cid:30)217/3(cid:29) partitions to maximize server space utilization,
each with an upper-bound partition size of 218 blocks.
7.2 Simulator
We evaluated Burst ORAM’s bandwidth costs and re-
sponse times using a detailed simulator written in Java.
The simulator creates an asynchronous event for each
block to be transferred. We calculate the transfer’s ex-
pected end time from the network latency, the network
bandwidth, and the number of pending transfers.
758  23rd USENIX Security Symposium 
USENIX Association
10
Our simulator also measures results for ObliviStore
and the insecure baseline. In all schemes, block requests
are time-stamped as soon as they arrive, and serviced as
soon as possible. Requests pile up indeﬁnitely if they
arrive more frequently than the scheme can handle them.
Burst ORAM’s behavior is driven by semaphores and
appears data-independent to the server. Each request
reads from a partition that appears to be chosen uni-
formly at random, so bandwidth costs and response times
depend only on request arrival times, not on requested
block IDs or contents. Thus, the simulator need only
store counters representing the number of remaining