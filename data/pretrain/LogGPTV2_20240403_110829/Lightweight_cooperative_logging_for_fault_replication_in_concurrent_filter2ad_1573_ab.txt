non-cooperative logging schemes.
B. CoopREP Architecture
Figure 2 illustrates the overall architecture of CoopREP.
During the instrumentation phase (Figure 2 -1), the trans-
former instruments the Java program bytecode to generate
both the record version and the replay version, as done
in LEAP. Unlike LEAP, however,
the record version is
instrumented to only log a subset of the SPEs. In the
following, we will refer to this version as partial record
version. The partial record versions are then sent to the
clients, whereas the replay version is sent to the replayer.
Figure 2 -2 illustrates the record and replay phases. In
CoopREP, there is a recorder module for each client. Con-
versely to LEAP, in CoopREP the recorder does not record
all SPEs’ access vectors. Instead, each user logs accesses
only to a part of the program’s SPEs, as previously deﬁned
by the transformer. Assuming that the program is executed
by a large population of users, this mechanism allows to
gather access vectors for the whole set of SPEs with high
probability. By doing this, CoopREP aims at minimizing the
Figure 2. Overview of the CoopREP architecture: (1) Instrumentation
phase; (2) Record and Replay phases.
performance overhead that would be required if one had to
record all the access vectors at each client.
When the production run ends, each client sends its partial
log to the developer site to be analyzed. This log consists
of the access vectors recorded for a subset of the SPEs, a
hash of the log (computed in our prototype using MD5),
the thread ID map, and also an additional bit indicating the
success or failure of the execution (successful executions
can be useful for the statistical analysis).
At the developer site, the statistical analyzer employs a
novel lightweight statistical methodology (see Section IV-G)
that aims at pinpointing which partial logs are more likely to
be successfully recombined in order to generate a complete
log yielding an equivalent execution that reproduces a given
concurrency bug.
Finally, the combination of access vectors determined by
the statistical analyzer is passed as input to the replayer
component, along with the thread ID map and the generated
replay driver. Note that, given that access vectors come from
independent executions, the resulting combined log can be
incompatible, meaning that the replayer fails to enforce the
thread execution order speciﬁed by the access vectors of the
combined log. In this case, the execution will hang, as none
of the threads will be allowed to perform its subsequent
access on a SPE. This allows to use a simple, but effective,
deadlock detection mechanism that terminates immediately
the execution replay as soon as it is detected that all threads
are prevented from progressing.
Of course, the replayer needs also to deal with the case
in which the bug is not replayed. This is a non-trivial issue,
for which we rely on standard techniques already adopted
in other probabilistic fault-replication schemes [4], [17]. For
crash failures, it is straightforward, as CoopREP can catch
exceptions. For incorrect results, it is more complicated,
and it is, in general, required to obtain information from
programmers regarding the failure symptoms. These include
conditional breakpoints to examine outputs for detecting
anomalies with respect to the originally buggy execution
to be replayed. CoopREP could also beneﬁt from the inte-
gration with bug detection tools, such as [23], which could
automate the identiﬁcation of non-visible bugs.
In case the bug cannot be successfully reproduced, the
replayer will send feedback to the statistical analyzer com-
municating the replay failure, so the latter can investigate
another access vector combination and produce a new com-
plete log for replay. This process ends when the bug is
successfully replayed or when the maximum number of
attempts to do it is reached.
C. Partial Log Recording
CoopREP is based on having each instance recording
accesses to only a fraction of the entire set of the SPEs
of the program. The subset of SPEs to be traced is deﬁned
at instrumentation time by the transformer. For this purpose,
different criteria may be used, e.g. random selection, load
balancing distribution, subset of ﬁxed SPEs, etc. In this
paper all experiments use the random selection of SPEs.
The research of the viability of different selection criteria is
left for future work.
Therefore, in the implemented prototype, whenever a new
SPE is identiﬁed, CoopREP uses a simple probabilistic
scheme to decide whether the SPE is to be instrumented or
not by a given instance. This ensures that each instance only
tracks a fraction of the total number of SPEs of the program,
and statistical fairness. However, it is not guaranteed that two
partial logs (acquired at different clients) necessarily overlap.
The drawback of this is that, if two partial logs have no
SPEs in common, it is impossible to deduce whether these
partial logs were traced from equal executions and that are
suitable to be combined. This problem could be addressed
by increasing the percentage of coverage to ensure SPE
overlapping (at the cost of greater overheads), or by deﬁning
a smaller ﬁxed subset of SPEs to be logged by all users.
Moreover, one must note that the overhead reduction may
not be linear with the decrease of the coverage percentage.
This happens because some SPEs may be accessed more
frequently than others, therefore, when instrumenting the
code, the logging overhead may not be distributed equally
among the users. A solution for this could be instrumenting
the whole program and proﬁling it, at the developer side, in
order to measure the number of accesses performed on each
SPE. Later, when instrumenting the user versions, CoopREP
could already take into account the SPEs access distribution.
D. Merge of Partial Logs
The major challenge of using partial recording is how to
combine the collected partial logs in such a way that the
access vectors used lead to a feasible thread interleaving,
capable of reproducing the bug during the replay.
In general, the following facts make the partial log merg-
ing difﬁcult: i) the bug can be the result of several different
thread interleavings; ii) the probability of obtaining two
identical executions of the same program can be very low
(this probability is inversely proportional to the complexity
of the program in terms of number of SPEs and the number
of thread accesses); iii) the combination of access vectors
from partial logs of faulty executions may enforce a thread
order that leads to a non-faulty replay execution; iv) the
combination of access vectors from partial logs of faulty
executions may enforce a thread order that leads to a non-
compatible replay execution.
To address these challenges, and to mitigate the incom-
patibility of the merged access vectors, CoopREP applies
statistical metrics over the universe of collected partial logs
to pick those that present more similarity. Thereby, our
statistical metrics are divided in two types: statistical metrics
for partial log correlation and statistical metrics for bug
correlation.
E. Statistical Metrics for Partial Log Correlation
These metrics measure the amount of information that
different partial logs may have in common, so that one
can increase the probability of merging compatible access
vectors. In particular, the following statistical metrics are
used to calculate the partial log correlation: Similarity and
Relevance. Both metrics are described in detail below.
1) Similarity: The rationale behind the classiﬁcation of
the similarity between two partial logs is related to their
number of SPEs with identical access vectors (i.e. that had
recorded exactly the same thread interleaving). Hence, the
more SPEs with equal access vectors the partial logs have,
the better.
The computation of this metric can come in two ﬂavors:
Plain Similarity (PSIM) and Dispersion-based Similarity
(DSIM), according to the weight given to the SPEs of the
program. To better deﬁne these metrics, Table I presents
some formal notation. With this notation, we can now deﬁne
the metrics as follows.
Let l0 and l1 be two partial logs, their Plain Similarity
(PSIM) is given by the following equation:
(cid:18)
(cid:19)
(1)
PSIM(l0, l1) =
#Equall0,l1
#S
×
1 − #Diff l0,l1
#S
where #Equall0,l1, #S, and #Diff l0,l1 denote the cardi-
nality of the sets Equall0,l1, S, and Diff l0,l1, respectively.
Note that since we are using the client-generated hashes
of the logs, the functions Equall0,l1 and Diff l0,l1 can be
implemented very efﬁciently.
Notation
S
Sl
AV
AV l
avecs(s) : S → AV
avecl(s) : Sl (cid:55)→ AV l
= {s | s ∈ Sl0 ∩ Sl1 ∧ avecl0 (s) = avecl1 (s)}
Equall0,l1
Diff l0,l1 = {s | s ∈ Sl0 ∩ Sl1 ∧ avecl0 (s) (cid:54)= avecl1 (s)}
Siml0 = {l1, l2, ..., lk}
F illl0,Siml0
= {s | s ∈ Sl0 ∪Sl1 ∪Sl2 ∪ ...∪Slk ∧ l1, l2, ..., lk ∈ Siml0}
Description
Set of all the SPE identiﬁers of the program.
Set of the SPE identiﬁers recorded only by the partial log l.
Set of the different hashes of the access vectors recorded by
all the partial logs.
Set of the hashes of the access vectors recorded only by the
partial log l.
Map that, for a given SPE identiﬁer s, returns the set of the
hashes of its access vectors across all the partial logs.
Function that maps a SPE identiﬁer s to the hash of its access
vector, recorded by the partial log l.
Set of the SPE identiﬁers, recorded by both partial logs l0
and l1, with identical access vectors.
Set of the SPE identiﬁers, recorded by both partial logs l0
and l1, with different access vectors.
Set of the k partial logs more similar (according to either the
plain or dispersed similarity metric) to l0 (denoted as group
of similars of l0).
Union of the sets of the SPE identiﬁers recorded by the partial
log l0 and by the partial logs of its group of similars Siml0 .
NOTATION USED TO DEFINE THE STATISTICAL METRICS.
Table I
DSIM(l0, l1) =
(cid:88)
weight(x) ×
x∈Equall0,l1
1 − (cid:88)
y∈Diff l0,l1
weight(y)
It should also be noted that this metric will only be 1
when both logs are complete and identical, i.e. they have
recorded access vectors for all the SPEs of the program
(Sl0 = Sl1 = S) and those access vectors are equal for both
logs (avecl0(s) = avecl1(s),∀s ∈ S). This implies that,
for every two partial logs, their plain similarity will always
be less than 1. However, the greater this value is, the more
probable is that the both partial logs are compatible.
Let l0 and l1 be two partial logs, their Dispersion-based
Similarity (DSIM) is given by the following equation:
where weight(s) is a function of type S → Double that
maps each SPE identiﬁer to a double value that captures
its relative weight, in terms of overall-dispersion. Here, the
overall-dispersion of a given SPE corresponds to the ratio
between the number of different access vectors (based on
their hash) logged (across all clients) for that SPE and the
total number of different access vectors collected for all the
SPEs (across all clients). Thereby, the weight function of a
SPE identiﬁer s can be calculated as follows:
weight(s) =
#avecs(s)
#AV
(2)
Assume for instance that we have two SPEs, x and y,
and two partial logs containing identical access vectors for
SPE x (say x∗) and different access vectors for SPE y (say
y1 and y2). In this case we would have weight(x) = 1
3
and weight(y) = 2
3. Notice that this deﬁnition assigns larger
weights to the SPEs whose access vectors are more likely
to be different across different partial logs. When used in
DSIM equation, this metric allows for biasing the selection
towards pairs of logs having similarities in those SPEs that
are more likely subject to different access interleavings. The
rationale is that if we have that two partial logs having
in common a “rare” access vector for a given SPE, then
with high probability they were originated from equivalent
executions.
Comparing the two metrics, one can see that the Plain
Similarity considers that every SPE has the same impor-
tance, whilst the Dispersion-based Similarity assigns dif-
ferent weights to the SPEs. In general, both metrics allow
to pinpoint the most similar partial logs, but the ﬁrst is
more useful when the overall-dispersion weight values are
relatively well distributed for all the SPEs. On the other
hand, the Dispersion-based Similarity is more suitable for
cases when there are many SPEs whose access vectors are
identical in every execution.
2) Relevance: This metric allows to classify each partial
log according to its likelihood of being completed with
compatible information:
Relevance(l0) = α × #F illl0 ,Siml0
#S
(cid:80)
+ (1 − α) ×
ln∈Siml0
Similarity(l0, ln)
#Siml0
(3)
where Similarity(l0, ln) is one of the two possible types
of Similarity metrics.
The Relevance metric is the sum of two parcels weighed
by the parameter 0 < α < 1, thus ensuring that 0 ≤
Relevance ≤ 1. The ﬁrst parcel captures the “complete-
ness” of the set of SPEs obtainable by merging the partial
logs, i.e. the number of SPEs that is possible to ﬁll joining
the access vectors from the partial log l0 and its group of
similars Siml0. This follows the rationale that the more
missing SPEs of l0 that can be ﬁlled with access vectors
from similar partial logs, the better.
In turn, the second parcel provides a measure of the
partial logs expected compatibility, by computing the av-
erage similarity of all
logs in the group of
similars. This allows to pick, as the base partial log, the one
whose group of similars is composed by partial logs with
high similarity, thus increasing the probability of merging
compatible information.
the partial
It should be noted that the value of each parcel is restricted
to the range [0,1]. A value of 1 for the ﬁrst parcel means
that the full set of SPEs can be completed by combining the
partial logs in Siml0, whereas a value of 1 for the second
parcel can only be achieved in the extreme case in which
all logs in Siml0 are complete and identical.
Moreover, one should notice that a partial log l1 can only
be part of Siml0 if Similarity(l0, l1) ≥ threshold. This
avoids the group of similars to be composed by partial logs
with a very low value of similarity. Also, the maximum size
of the group of similars (#Siml0), can be deﬁned by the
developer.
In our experiments, we found α = 0.7, max(#Siml0 ) =
5, threshold = 0.3 for Plain Similarity, and threshold =
0.01 for Dispersion-based Similarity (because the weight of
some SPEs can be very low), to be good values.
F. Statistical Metrics for Bug Correlation
Unlike the previous metrics, the statistical metrics for bug
correlation are concerned with the correlation between the
bug and each access vector individually. This also leverages
information from successful executions and is specially
useful when, even after merging the partial logs of all buggy
executions, there are still SPEs to be completed.
To compute these metrics, we adapt the scoring method
proposed by Liblit et al [19]. Thereby, access vectors are
classiﬁed based on their Sensitivity and Speciﬁcity,
i.e.
whether they account for many failed runs and few success-
ful runs. With this information, it is possible to deﬁne a third
metric, denoted Importance, which computes the harmonic
mean between the previous two metrics, thus identifying the
access vectors that are simultaneously high sensitive and
speciﬁc.
Let Ftotal be the total number of partial logs resulting
from failed executions; for each access vector v, let F (v)
be the number of failed partial logs that have recorded v for
a given SPE, and S(v) be the number of successful partial
logs that have recorded v for a given SPE. The three metrics
are then calculated as follows.
Sensitivity(v) =
F (v)
Ftotal
F (v)
Speciﬁcity(v) =
S(v) + F (v)
Importance(v) =