c(b(cid:19))
In the setting of this paper, the events E consist of
cache hits and misses, which are described by the cache
effect eff
: C ×B → E:
C
We model an adversary’s view on the computations
of P as a function view: Col → O that maps computa-
tions to a ﬁnite set of observations O. The composition
C = (view◦ P): I → O
deﬁnes a function from initial states to observations,
which we call a channel of P. Whenever view is deter-
mined by the cache and event components of traces, we
call C a side channel of P.
We next deﬁne views corresponding to the obser-
vations of access-based, trace-based, and timing-based
side-channel adversaries.
The view of an access-based adversary that shares the
memory space with the victim is deﬁned by
viewacc : (m0,c0)e0 . . .e n−1(mn,cn) (cid:10)→ cn
and captures that the adversary can determine (by prob-
ing) which memory blocks are contained in the cache
upon termination of the victim. An adversary that does
not share the memory space with the victim can only ob-
serve how many blocks the victim has loaded in each
cache set (by probing how many of its own blocks have
been evicted), but not which. We denote this view by
viewaccd. The view of a trace-based adversary is deﬁned
by
viewtr : σ0e0 . . .e n−1σn (cid:10)→ e0 . . .e n−1
and captures that the adversary can determine for each
instruction whether it results in a hit, miss, or does not
access memory. The view of a time-based adversary is
deﬁned by
eff
C(c,m) :=(cid:27)hit
miss
: c(m) < k
: else
C
C
and eff
are naturally extended to the case
Both upd
where no memory access occurs. Then, the cache state
remains unchanged and the cache effect is ⊥, so E =
{hit,miss,⊥}.
With this, we can now connect the components and
obtain the global transition relation T ⊆ Σ×E ×Σ by
M(m1)
T = {((m1,c1), e , (m2,c2)) | m2 = upd
∧ c2 = upd
∧ e = eff
C(c1,eff
C(c1,eff
M(m1))
M(m1))} ,
which formally captures the asymmetric relationship be-
tween caches, logical memories, and events.
viewtime : σ0e0 . . .e n−1σn (cid:10)→
thit ·|{i | ei = hit}| +tmiss ·|{i | ei = miss}| +
t⊥ ·|{i | ei = ⊥}|
and captures that the adversary can determine the overall
execution time of the program. Here, thit, tmiss, and t⊥ are
the execution times (e.g. in clock cycles) of instructions
that imply cache hits, cache misses, or no memory ac-
cesses at all. While the view of the time-based adversary
as deﬁned above is rather simplistic, e.g. disregarding ef-
fects of pipelining and out-of-order execution, notice that
our semantics and our tool can be extended to cater for
a more ﬁne-grained, instruction- and context-dependent
modeling of execution times. We denote the side chan-
nels corresponding to the four views by Cacc, Caccd, Ctr,
and Ctime, respectively. Figure 1 gives an overview.
3.4 Side Channels
For a deterministic, terminating program P, the transition
relation is a function, and the program can be modeled as
a mapping P: I → Col.
3.5 Quantiﬁcation of Side Channels
We characterize the security of a channel C : I → O as the
difﬁculty of guessing the secret input from the channel
output.
434  22nd USENIX Security Symposium 
USENIX Association
Cacc
Ctr
Access-based adversary whose memory
space is shared with the victim’s.
Caccd Access-based adversary whose memory
space is disjoint from the victim’s.
Adversary who observes the trace of cache
hits and misses.
Ctime Adversary who observes the overall execu-
tion time.
Figure 1: Channels corresponding to different adversary
models.
Formally, we model the choice of a secret input by
a random variable X with ran(X) ⊆ I and the corre-
sponding observation by a random variable C(X) with
ran(C(X)) ⊆ O. We model the attacker as another ran-
dom variable ˆX. The goal of the attacker is to esti-
mate the value of X, i.e. it is successful if ˆX = X. We
make the assumption that the attacker does not have in-
formation about the value of X beyond what is contained
in C(X), which we formalize as the requirement that
X → C(X)→ ˆX form a Markov chain. The following the-
orem expresses a security guarantee as an upper bound
on the attacker’s success probability in terms of the size
of the range of C.
Theorem 1. Let X → C(X) → ˆX be a Markov chain.
Then
P(X = ˆX) ≤ max
σ∈I
P(X = σ )·|ran(C)|
For the interpretation of the statement observe that if
the adversary has no information about the value of X
(i.e., if ˆX and X are statistically independent), its suc-
cess probability is bounded by the probability of the most
likely value of X, i.e. P(X = ˆX) ≤ maxσ∈I P(X = σ ),
where equality can be achieved. Theorem 1 hence states
that the size of the range of C is an upper bound on the
factor by which this probability is increased when the at-
tacker sees C(X) and is, in that sense, an upper bound
for the amount of information leaked by C. We will of-
ten give bounds on |ran(C)| on a log-scale, in which case
they represent upper bounds on the number of leaked
bits. Notice that the guarantees of Theorem 1 fundamen-
tally rely on assumptions about the initial distribution of
X: if X is easy to guess to begin with, Theorem 1 does
not imply meaningful security guarantees.
For more discussion on the interpretation of the secu-
rity guarantees, see Section 7.4. For a formal connection
to traditional (entropy-based) presentations of quantita-
tive information-ﬂow analysis [43] and a proof of Theo-
rem 1, see the extended version [19].
3.6 Adversarially Chosen Cache States
We sometimes assume that initial states are pairs consist-
ing of high and low components, i.e. I = Ihi × Ilo, where
only the high component is meant to be kept secret and
the low component may be provided by the adversary,
a common setting in information-ﬂow analysis [42]. In
this case, a program and a view deﬁne a family of chan-
nels Cσlo : Ihi → O, one for each low component σlo ∈ Ilo.
A particularly interesting instance is the decomposi-
tion into secret memory Ihi = M and adversarially cho-
sen cache Ilo = C. While bounds for the corresponding
channel can be derived by considering all possible ini-
tial cache states, corresponding analyses suffer from poor
precision. The following lemma enables us to derive
bounds for the general case, based on the empty cache
state.
Lemma 1. For all initial cache states c ∈ C, adversaries
adv ∈ {acc,accd,time,tr}, and LRU, FIFO, or PLRU re-
placement: If no block in c is accessed during program
execution, then
where /0 is a shorthand for the empty cache state. For
ran(Cadv
/0
ran(Cadv
c
(1)
)(cid:31)(cid:31)
ran(Cadv
c
/0
(cid:31)(cid:31)(cid:31)
ran(Cadv
=(cid:31)(cid:31)(cid:31)
adv ∈ {acc,accd} and LRU, (cid:31)(cid:31)
)(cid:31)(cid:31)(cid:31)
,
)(cid:31)(cid:31)(cid:31)
)(cid:31)(cid:31) ≥ (cid:31)(cid:31)
holds without any constraints on the initial cache state c.
This lemma was proved in [34] for acc, accd and the
LRU case with the initial cache state not containing any
block of the victim. The proof is based on the fact that
memory blocks in the cache do not affect the position
of memory blocks that are accessed during computation
whenever the two sets of memory blocks are disjoint,
which allows us to construct a bijective function from
ran(Cadv
). The argument immediately ex-
tends to FIFO, PLRU, and all adv. For LRU and access-
based adversaries, the function remains surjective even
without the disjointness requirement.
) to ran(Cadv
/0
c
4 Automatic Quantiﬁcation of Cache Side
Channels
Theorem 1 enables the quantiﬁcation of side channels
by determining their range. As channels are deﬁned in
terms of views on computations, their range can be de-
termined by computing Col and applying view. However,
this entails computing a ﬁxpoint of the next operator and
is practically infeasible in most cases. Abstract inter-
pretation [16] overcomes this fundamental problem by
computing a ﬁxpoint with respect to an efﬁciently com-
putable over-approximation of next. This new ﬁxpoint
represents a superset of all computations, which is suf-
ﬁcient for deriving an upper bound on the range of the
channel and thus on the leaked information.
USENIX Association  
22nd USENIX Security Symposium  435
In this section, we describe the interplay of the abstrac-
tions used for over-approximating next in CacheAudit
(namely, those for memory, cache, and events), and we
explain how the global soundness of CacheAudit can be
established from local soundness conditions. This mod-
ularity is key for the future extension of CacheAudit us-
ing more advanced abstractions. Our results hold for all
adversaries introduced in Section 3.4 and we omit the
superscript adv from channels and views for readability.
4.1 Sound Abstraction of Leakage
We frame a static analysis by deﬁning a set of abstract
elements Traces(cid:31) together with an abstract transfer func-
tion next(cid:31) : Traces(cid:31) → Traces(cid:31). Here, the elements a ∈
Traces(cid:31) represent subsets of Traces, which is formalized
by a concretization function
γ : Traces(cid:31)→P(Traces) .
The key requirements for next(cid:31) are (1) that it be efﬁ-
ciently computable, and (2) that it over-approximates the
effect of next on sets of computations, which is formal-
ized as the following local soundness condition:
∀a ∈ Traces(cid:31) : next (γ(a)) ⊆ γ(next(cid:31)(a)) .
(2)
Intuitively, if we maintain a superset of the set of compu-
tations during each step of the transfer function as in (2),
then this inclusion must also hold for the correspond-
ing ﬁxpoints. More formally, any post-ﬁxpoint of next(cid:31)
that is greater than an abstraction of the initial states I is
a sound over-approximation of the collecting semantics.
We use Col(cid:31) to denote any such post-ﬁxpoint.
Theorem 2 (Local soundness implies global soundness,
from [16]). If (2) holds then
Col ⊆ γ(cid:31)Col(cid:31)(cid:30) .
The following theorem is an immediate consequence
of Theorem 2 and the fact that view (Col) =ran( C). It
states that a sound abstract analysis can be used for de-
riving bounds on the size of the range of a channel.
Theorem 3 (Upper bounds on leakage).
|ran(C)| ≤(cid:29)(cid:29)(cid:29)
view(cid:31)γ(cid:31)Col(cid:31)(cid:30)(cid:30)(cid:29)(cid:29)(cid:29)
.
With the help of Theorem 1, these bounds immediately
translate into security guarantees. The relationship of all
steps leading to these guarantees is depicted in Figure 2.
4.2 Abstraction Using a Control Flow
Graph
In order to come up with a tractable and modular analy-
sis, we design independent abstractions for cache states,
memory, and sequences of events.
malizes its meaning.
formalizes its meaning.
• M(cid:31) abstracts memory and γM : M(cid:31) →P(M) for-
• C(cid:31) abstracts cache conﬁgurations and γC :C(cid:31)→P(C)
• E (cid:31) abstracts sequences of events and γE : E (cid:31) →
But, since cache updates and events depend on memory
state, independent analyses would be too imprecise. In
order to maintain some of the relations, we link the three
abstract domains for memory state, caches, and events
through a ﬁnite set of labels L so that our abstract domain
is
P(E∗) formalizes its meaning.
Traces(cid:31) = L→M(cid:31) ×C(cid:31) ×E (cid:31) ,
where we write aM(l), aC(l) and aE (l) for the ﬁrst, sec-
ond, and third components of an abstract element a(l).
Labels roughly correspond to nodes in a control ﬂow
graph in classical data-ﬂow analyses. One could sim-
ply use program locations as labels. But in our setting,
we use more general labels, allowing for a more ﬁne-
grained analysis in which we can distinguish values of
ﬂags or results of previous tests [36]. To capture that,
we associate a meaning with each label via a function
γL : L→P(Traces). If the labels are program locations,
then γL(l) is the set of traces ending in a state in lo-
cation l. The analogy with control ﬂow graphs can be
extended to edges of that graph: using the next opera-
tor, we deﬁne the successors and predecessors of a lo-
cation l as: succ(l) = {k | next(γL(l))∩ γL(k) (cid:16)= /0}, and
pred(l) = {k | next(γL(k))∩ γL(l) (cid:16)= /0}.
Then we can describe the meaning of an element a ∈
Traces(cid:31) with:
γ(a) = {σ0e0σ1 . . .σ n ∈ Traces| ∀i ≤ n, ∀l ∈ L :
σ0e0σ1 . . .σ i ∈ γL(l) ⇒
σMi ∈ γM(aM(l))∧ σCi ∈ γC(aC(l))
∧e0 . . .e i−1 ∈ γE (aE (l))(cid:28)
(3)
That is, the meaning of an a ∈ Traces(cid:31) is the set of
traces, such that for every preﬁx of a trace, if it “ends” at
program location l, then the memory state, cache state,
and the event sequence satisfy the respective abstract el-
ements for that location.
The abstract transfer function next(cid:31) will be decom-
posed into:
next(cid:31)(a) =λ l. (next
M(cid:31)(a,l),next
C(cid:31)(a,l),next
E(cid:31) (a,l)) ,
(4)
436  22nd USENIX Security Symposium 
USENIX Association
Col
⊆
γ(cid:31)Col(cid:31)(cid:30)
Meaning
Col(cid:31)
Leakage
≤
|ran(C)| = |view (Col)|
≤
view(cid:31)γ(cid:31)Col(cid:31)(cid:30)(cid:30)(cid:29)(cid:29)
(cid:29)(cid:29)
Figure 2: Relationship of collecting semantics Col, abstract ﬁxpoint Col(cid:31), side channels C, and leakage bounds.
where each next function over-approximates the corre-
sponding concrete update function deﬁned in the previ-
ous section. The effects used for deﬁning the concrete
updates are reﬂected as information ﬂow between other-
wise independent abstract domains, which is formalized
as a partial reduction in the abstract interpretation litera-
ture [18].
4.3 Local Soundness
The products and powers of sound abstract domains with
partial reductions are again sound abstract domains [17].
The soundness of Traces(cid:31) hence immediately follows
from the local soundness of the memory, cache and event
domains. Below we describe those soundness conditions
for each domain.
and
The abstract next(cid:31) operation is implemented using lo-
cal update functions for the memory, cache, and event
components. For the memory domain we have, for each
label k ∈ L and each l ∈ succ(k):
• an abstract memory update upd
• an abstract memory effect eff
M(cid:31),(k,l):M(cid:31) →M(cid:31),
M(cid:31),(k,l) : M(cid:31) →
For the cache domain, there is no need for separate func-
tions for each pair (k,l), because the cache update only
depends on the accessed block which is delivered by the
abstract memory effect. Likewise, the update of the event
domain only depends on the abstract cache effect. Thus,
we further have:
P(EM).
P(EC), and
C(cid:31) : C(cid:31) ×P(EM)→C(cid:31),
C(cid:31) : C(cid:31) × P(EM) →
• an abstract cache update upd
• an abstract cache effect eff
• an abstract event upd
With these functions, we can approximate the effect
of next on each label l, using the abstract values associ-
ated with the labels that can lead to l, pred(l). For the
example of the cache domain, this yields
E(cid:31) : E (cid:31) ×P(EC)→E (cid:31).
next
C(cid:31)(a,l) =
C(cid:31)