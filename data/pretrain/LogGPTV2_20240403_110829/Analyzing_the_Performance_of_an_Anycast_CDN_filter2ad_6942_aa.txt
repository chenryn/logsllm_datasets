title:Analyzing the Performance of an Anycast CDN
author:Matt Calder and
Ashley Flavel and
Ethan Katz-Bassett and
Ratul Mahajan and
Jitendra Padhye
Analyzing the Performance of an Anycast CDN
Matt Calder*,†, Ashley Flavel†, Ethan Katz-Bassett*, Ratul Mahajan†, and Jitendra Padhye†
†Microsoft
*University of Southern California
ABSTRACT
Content delivery networks must balance a number of trade-oﬀs
when deciding how to direct a client to a CDN server. Whereas
DNS-based redirection requires a complex global traﬃc manager,
anycast depends on BGP to direct a client to a CDN front-end. Any-
cast is simple to operate, scalable, and naturally resilient to DDoS
attacks. This simplicity, however, comes at the cost of precise con-
trol of client redirection. We examine the performance implications
of using anycast in a global, latency-sensitive, CDN. We analyze
millions of client-side measurements from the Bing search service
to capture anycast versus unicast performance to nearby front-ends.
We ﬁnd that anycast usually performs well despite the lack of pre-
cise control but that it directs roughly 20% of clients to a suboptimal
front-end. We also show that the performance of these clients can
be improved through a simple history-based prediction scheme.
Categories and Subject Descriptors
C.2.5 [Computer-Communication Networks]: Local and Wide-
Area Networks—Internet; C.4 [Performance of Systems]: Mea-
surement techniques
Keywords
Anycast; CDN; Measurement;
1.
INTRODUCTION
Content delivery networks are a critical part of Internet infras-
tructure. CDNs deploy front-end servers around the world and
direct clients to nearby, available front-ends to reduce bandwidth,
improve performance, and maintain reliability. We will focus on
a CDN architecture which directs the client to a nearby front-end,
which terminates the client’s TCP connection and relays requests to
a backend server in a data center. The key challenge for a CDN is
to map each client to the right front-end. For latency-sensitive ser-
vices such as search results, CDNs try to reduce the client-perceived
latency by mapping the client to a nearby front-end.
CDNs can use several mechanisms to direct the client to a front-
end. The two most popular mechanisms are DNS and anycast.
DNS-based redirection was pioneered by Akamai.
It oﬀers ﬁne-
grained and near-real time control over client-front-end mapping,
but requires considerable investment in infrastructure and opera-
tions [35].
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815717.
Some newer CDNs like CloudFlare rely on anycast [1], announc-
ing the same IP address(es) from multiple locations, leaving the
client-front-end mapping at the mercy of Internet routing protocols.
Anycast oﬀers only minimal control over client-front-end mapping
and is performance agnostic by design. However, it is easy and
cheap to deploy an anycast-based CDN – it requires no infrastruc-
ture investment, beyond deploying the front-ends themselves. The
anycast approach has been shown to be quite robust in practice [23].
In this paper, we aim to answer the questions: Does anycast direct
clients to nearby front-ends? What is the performance impact of
poor redirection, if any? To study these questions, we use data from
Bing’s anycast-based CDN [23]. We instrumented the search stack
so that a small fraction of search response pages carry a JavaScript
beacon. After the search results display, the JavaScript measures
latency to four front-ends– one selected by anycast, and three nearby
ones that the JavaScript targets. We compare these latencies to
understand anycast performance and determine potential gains from
deploying a DNS solution.
Our results paint a mixed picture of anycast performance. For
most clients, anycast performs well despite the lack of centralized
control. However, anycast directs around 20% of clients to a sub-
optimal front-end. When anycast does not direct a client to the best
front-end, we ﬁnd that the client usually still lands on a nearby alter-
native front-end. We demonstrate that the anycast ineﬃciencies are
stable enough that we can use a simple prediction scheme to drive
DNS redirection for clients underserved by anycast, improving per-
formance of 15%-20% of clients. Like any such study, our speciﬁc
conclusions are closely tied to the current front-end deployment of
the CDN we measure. However, as the ﬁrst study of this kind that we
are aware of, the results reveal important insights about CDN per-
formance, demonstrating that anycast delivers optimal performance
for most clients.
2. CLIENT REDIRECTION
A CDN can direct a client to a front-end in multiple ways.
DNS: The client will fetch a CDN-hosted resource via a hostname
that belongs to the CDN. The client’s local DNS resolver (LDNS),
typically conﬁgured by the client’s ISP, will receive the DNS request
to resolve the hostname and forward it to the CDN’s authoritative
nameserver. The CDN makes a performance-based decision about
what IP address to return based on which LDNS forwarded the
request. DNS redirection allows relatively precise control to redirect
clients on small timescales by using small DNS cache TTL values.
Since a CDN must make decisions at the granularity of LDNS
rather than client, DNS-based redirection faces some challenges.
An LDNS may be distant from the clients that it serves or may serve
clients distributed over a large geographic region, such that there
is no good single redirection choice an authoritative resolver can
make. This situation is very common with public DNS resolvers
such as Google Public DNS and OpenDNS, which serve large, ge-
ographically disparate sets of clients [17]. A proposed solution to
this issue is the EDNS client-subnet-preﬁx standard (ECS) which
allows a portion of the client’s actual IP address to be forwarded
531to the authoritative resolver, allowing per-preﬁx redirection deci-
sions [21].
Anycast: Anycast is a routing strategy where the same IP address
is announced from many locations throughout the world. Then
BGP routes clients to one front-end location based on BGP’s notion
of best path. Because anycast defers client redirection to Internet
routing, it oﬀers operational simplicity. Anycast has an advantage
over DNS-based redirection in that each client redirection is handled
independently – avoiding the LDNS problems described above.
Anycast has some well-known challenges. First, anycast is un-
aware of network performance, just as BGP is, so it does not react
to changes in network quality along a path. Second, anycast is un-
aware of server load. If a particular front-end becomes overloaded,
it is diﬃcult to gradually direct traﬃc away from that front-end,
although there has been recent progress in this area [23]. Simply
withdrawing the route to take that front-end oﬄine can lead to cas-
cading overloading of nearby front-ends. Third, anycast routing
changes can cause ongoing TCP sessions to terminate and need to
be restarted.
In the context of the Web, which is dominated by
short ﬂows, this does not appear to be an issue in practice [31, 23].
Many companies, including Cloudﬂare, CacheFly, Edgecast, and
Microsoft, run successful anycast-based CDNs.
Other Redirection Mechanisms: Whereas anycast and DNS direct
a client to a front-end before the client initiates a request, the re-
sponse from a front-end can also direct the client to a diﬀerent server
for other resources, using, for example, HTTP status code 3xx or
manifest-based redirection common for video [4]. These schemes
add extra RTTs, and thus are not suitable for latency-sensitive Web
services such as search. We do not consider them further in this
paper.
3. METHODOLOGY
Our goal is to answer two questions: 1) How eﬀective is anycast
in directing clients to nearby front-ends? And 2) How does anycast
performance compare against the more traditional DNS-based uni-
cast redirection scheme? We experiment with Bing’s anycast-based
CDN to answer these questions. The CDN has dozens of front end
locations around the world, all within the same Microsoft-operated
autonomous system. We use measurements from real clients to
Bing CDN front-ends using anycast and unicast. In § 4, we com-
pare the size of this CDN to others and show how close clients are
to the front ends.
3.1 Routing Conﬁguration
All test front-ends locations have both anycast and unicast IP
addresses.
Anycast: Bing is currently an anycast CDN. All production search
traﬃc is current served using anycast from all front-ends.
Unicast: We also assign each front-end location a unique
/24 preﬁx which does not serve production traﬃc. Only the routers
at the closest peering point to that front-end announce the preﬁx,
forcing traﬃc to the preﬁx to ingress near the front-end rather than
entering Microsoft’s backbone at a diﬀerent location and traversing
the backbone to reach the front-end. This routing conﬁguration
allows the best head-to-head comparison between unicast and
anycast redirection, as anycast traﬃc ingressing at a particular
peering point will also go to the closest front-end.
3.2 Data Sets
We use both passive and active measurements in our study, as
discussed below.
3.2.1 Passive Measurements
Bing server logs provide detailed information about client re-
quests for each search query. For our analysis we use the client IP
address, location, and what front-end was used during a particular
request. This data set was collected on the ﬁrst week of April 2015
and represents many millions of queries.
3.2.2 Active Measurements
To actively measure CDN performance from the client, we inject
a JavaScript beacon into a small fraction of Bing Search results.
After the results page has completely loaded, the beacon instructs
the client to fetch four test URLs. These URLs trigger a set of DNS
queries to our authoritative DNS infrastructure. The DNS query
results are randomized front-end IPs for measurement diversity,
which we discuss more in § 3.3.
The beacon measures the latency to these front-ends by down-
loading the resources pointed to by the URLs, and reports the results
to a backend infrastructure. Our authoritative DNS servers also push
their query logs to the backend storage. Each test URL has a glob-
ally unique identiﬁer, allowing us to join HTTP results from the
client side with DNS results from the server side [34].
The JavaScript beacon implements two techniques to improve
quality of measurements. First, to remove the impact of DNS lookup
from our measurements, we ﬁrst issue a warm-up request so that
the subsequent test will use the cached DNS response. While DNS
latency may be responsible for some aspects of poor Web-browsing
performance [5], in this work we are focusing on the performance
of paths between client and front-ends. We set TTLs longer than
the duration of the beacon. Second, using JavaScript to measure
the elapsed time between the start and end of a fetch is known to not
be a precise measurement of performance [32], whereas the W3C
Resource Timing API [29] provides access to accurate resource
download timing information from compliant Web browsers. The
beacon ﬁrst records latency using the primitive timings. Upon
completion, if the browser supports the resource timing API, then
the beacon substitutes the more accurate values.
We study measurements collected from many millions of search
queries over March and April 2015. We aggregated client IP ad-
dresses from measurements into /24 preﬁxes because they tend to
be localized [27]. To reﬂect that the number of queries per /24 is
heavily skewed across preﬁxes [35], for both the passive and ac-
tive measurements, we present some of our results weighting the
/24s by the number of queries from the preﬁx in our corresponding
measurements.
3.3 Choice of Front-ends to Measure
The main goal of our measurements is to compare the perfor-
mance achieved by anycast with the performance achieved by di-
recting clients to their best performing front-end. Measuring from
each client to every front-end would introduce too much overhead,
but we cannot know a priori which front-end is the best choice for
a given client at a given point in time.
We use three mechanisms to balance measurement overhead with
measurement accuracy in terms of uncovering the best performing
choices and obtaining suﬃcient measurements to them. First, for
each LDNS, we consider only the ten closest front-ends to the LDNS
(based on geolocation data) as candidates to consider returning to
the clients of that LDNS. Recent work has show that LDNS is a
good approximation of client location: excluding 8% of demand
from public resolvers, only 11-12% of demand comes from clients
who are further than 500km from their LDNS [17]. In Figure 1, we
will show that our geolocation data is suﬃciently accurate that the
best front-ends for the clients are generally within that set. Second,
532Figure 1: Diminishing returns of measuring to additional front-ends.
The close grouping of lines for the 5th+ closest front-ends suggests that
measuring to additional front-ends provides negligible beneﬁt.