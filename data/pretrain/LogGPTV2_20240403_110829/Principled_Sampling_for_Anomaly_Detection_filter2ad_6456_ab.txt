analysis includes a review of well known statistical tail
inequalities, which characterize how many sample inputs
are required to obtain a desired false positive bound
(whether one-sided or two-sided).
• Implementation: We present Fortuna, a system that im-
plements the PageRank sampling algorithm using freely
available data and computes resulting probabilistic false
positive rate bounds.
• Experimental Results: We present false positive rate
bounds for three different anomaly detectors: the SOAP
and SIFT anomaly detectors for JPEG and PNG input
ﬁles and the JSAND anomaly detector for JavaScript
programs. These results indicate that Fortuna’s sampling
algorithm is efﬁcient enough to obtain enough inputs for
providing tight bounds for practical anomaly detectors.
Anomaly detectors are a critical component of modern
computer security systems. However, despite the central role
they often play in such systems, there has been little to no
formal analysis that enables practitioners to better understand
the accuracy and effectiveness of their anomaly detectors. By
providing guaranteed probabilistic bounds on the false positive
rates that anomaly detectors will incur in practice, Fortuna
can help practitioners better understand the consequences of
deploying these powerful security tools.
II. THE CASE FOR PAGERANK
Fortuna computes bounds on Type 1 error by testing an
anomaly detector on inputs drawn from a chosen distribution.
The resulting bounds are thus meaningful with respect to data
drawn from that distribution. So to obtain bounds that are
useful in practice, it is important to choose a distribution that
is as consistent as possible with inputs an anomaly detector
will analyze in practice. At the same time, we need to rapidly
obtain many samples from the distribution – obtaining tight
bounds will require tens of thousands of sample inputs.
The tradeoff here is clear: a more sophisticated model or
involved data collection process may provide more accuracy,
but at
the cost of increased sampling difﬁcultly. For our
intended application – anomaly detectors that process web
data – we argue that PageRank balances this tradeoff and
3
thus provides an ideal distribution for testing. Furthermore,
its relative simplicity and ease of use suggests that PageRank
could become a benchmark for web-data anomaly detectors,
replacing ad hoc analysis methods and increasing consistency
and reproducibility across results.
Nevertheless, we stress that PageRank is just one possi-
ble viable distributions for testing anomaly detectors. When
deploying an anomaly detector for use beyond web data,
choosing an alternative distribution would be essential. Addi-
tionally, even within our chosen application domain, depending
on intended application and available computational and data
resources, alternative distributions could be more appropriate
for testing. Our goal is to provide a framework for computing
bounds given any chosen distribution and we offer PageRank
as a simple, easily implemented, yet powerful example.
A. A Brief Introduction to PageRank
A more complete treatment of the PageRank distribution,
including mathematical deﬁnitions and a theoretical discussion
of our sampling algorithm, is included in Section IV. We
present an abbreviated introduction here.
The PageRank distribution was ﬁrst presented by Page et
al. [45] and is the backbone of Google’s search result ranking
algorithm. Seeking to weight pages by importance, PageRank
has been extensively studied in theory and practice thanks
to its relative simplicity, accuracy, and (it turns out) utility
for other purposes such as local graph partitioning and robust
eigenvector approximations, for example [13], [32], [11], [39].
Prior to our work, PageRank distributions had not, however,
previously been used as a test distribution for evaluating ADS
systems.
To understand the PageRank distribution, consider the
following “random surfer” process for randomly accessing
pages on the World Wide Web:
(a) Begin by picking a starting webpage uniformly at random
from all possible pages.
(b) If the current page has no outgoing links, jump to a page
selected uniformly at random from all possible pages.
Otherwise, with probability α choose a link from the page
you are currently on uniformly at random, and follow that
link; otherwise (with probability (1− α)), jump to another
page selected uniformly at random from all possible pages.
Typically α is set to .85.
This Markov process loosely captures the behavior of a typical
Internet user. The PageRank of a URL is then taken to be the
fraction of visits to that page during the random surfer process
as the number of steps of the process goes to inﬁnity, i.e.,
the long-run fraction of time spent on that URL. Intuitively,
webpages with more incoming links tend to have higher
PageRank because the random surfer has more possible ways
of getting to the page. It turns out that moreover, PageRank
also captures a notion of the quality of these links. Speciﬁcally,
links from high PageRank sources are more important because
they are more likely to be taken than links from a page that is
not visited often. This “quality by association” is the important
insight behind PageRank, which has empirically outperformed
other notions of importance on the web.
4
B. Merits of PageRank
We claim that computing Type I errors with respect to
a PageRank distribution is reasonable for two reasons. First,
we argue that PageRank has been successful, in practice, at
capturing the relative importance of webpages (cf. Langville
and Meyer [32, pp.4,25]). It is therefore reasonable to use
PageRank to weight the importance of Type I (false positive)
errors on various webpages: false positives on more important
pages are more serious, as they are more likely to be visited.
Second, we point out that the random surfer process is a
reasonable synthetic model for the behavior of an average
user. The corresponding PageRank distribution, by deﬁnition,
captures precisely the long-run distribution over pages visited
by the random surfer, and therefore is a reasonable synthetic
model for the long-run distribution over pages visited by an
average user.
Fig. 1: PageRank Score (logarithmic scale) vs. Alexa Ranking
up to ∼ 105 pages, out of ∼ 109–1010 total. Warmer (higher)
colors indicate high density and thus reveal the trend curve.
In support of this second claim, we collected data on
approximately 100, 000 of the Web’s most visited webpages
[2], comparing their PageRank score (provided by Google [6])
and their global trafﬁc ranking (provided by Alexa Internet
[1]). Figure 1 presents this data in a 2-dimensional histogram,
which indicates that PageRank decreases in correlation with
reduced trafﬁc rank (a trafﬁc rank of 1 is best). Noise in
the plot can be attributed to several factors – for example,
trafﬁc numbers tend to be quite volatile (signiﬁcant changes
daily) while PageRank captures a more long term measure
of importance. In addition, PageRank numbers as provided
by Google are given on a logarithmic scale – a PageRank
score of x + 1 is twice is good as a PageRank score of x.
This property limits the precision of our y-axis. Finally, Alexa
trafﬁc rankings should only be considered representative of
true trafﬁc numbers in a coarse grained sense [19, pp.38].
Nevertheless, we are interested in the overall
trend –
PageRank correlates positively with trafﬁc ranking and thus
roughly captures visit frequencies while providing a long term
measure of page importance.
have approved the study’s protocol. As such, it highlights the
forbidding challenges inherent in such a large user study.
Of course, this point would be moot if it were as expensive
to sample from the PageRank distribution as it is to conduct,
for example, an large-scale (statistically signiﬁcant) user study
that captures actual visit frequencies for various webpages. As
we explain below, potentially more representative distributions
are (prohibitively) expensive to sample from, especially in
comparison to the efﬁcient method presented in Section IV-B
for PageRank sampling.
C. Alternatives to PageRank
There are several alternatives to our choice of the PageRank
distribution for web data. PageRank is a computed based on
link structure between webpages so it is not generally identical
to the relative rates of trafﬁc over various webpages, and as
we review next, other techniques produce data that may more
closely match actual trafﬁc numbers. While we argue that
sampling from other distributions is generally prohibitively
expensive, the bounds we present in Section III could in theory
be applied to any of these distributions. In particular:
1) Large-scale User Studies: An ideal choice for obtaining
test data for anomaly detectors for the web would be to obtain
data directly from real users. Data from trafﬁc ranking sites,
such as Alexa, unfortunately is not ﬁne grained enough for
our purposes since domains are tracked instead of individual
URLs. Furthermore, such data is typically only available for
the Internet’s most popular sites (e.g. top 1 million domains
out of > 950 million active domains in the case of Alexa [10]).
In order to obtain a representative sample of data, a user
trafﬁc study would have to cover a large population over a
long period of time. Such a study was actually conducted
by Meiss et al. [41] over about seven months during 2006–
07 in their work on the quality of the random surfer as a
user model. Their methodology involved collecting all of the
trafﬁc across the gateway for Indiana University’s Bloomington
campus; needless to say, it would have been difﬁcult to obtain
individualized informed-consent for the monitored population.
Nevertheless, they note that the users’ IP addresses have been
deleted and the study received approval from the university’s
Institutional Review Board.
As effective as their technique is for obtaining a large,
relatively high-quality sample of data, two subsequent devel-
opments cast doubt on whether or not such studies would
still receive IRB approval in the absence of user consent.
The ﬁrst is that work on deanonymization, as developed by
Narayanan and Shmatikov [12], [42], [43], demonstrates that
merely deleting trivial unique identiﬁers (such as IP addresses)
is not an adequate measure for protecting user privacy. The
second is that shortly after the publication of the Meiss et al.
study, whistleblowers at Indiana University alleged numerous
cases of noncompliance by the Bloomington Ofﬁce of Human
Research Protections [34]. In particular, it was alleged that
protocols were misreviewed, personal data had been released
without subjects’ consent, and subjects’ protections were “at
the bottom of the list of [the directors’] concerns.” While
the Meiss et al. study is never mentioned in any of the
released portions of the allegations, the affair clearly does raise
the question of whether a properly functioning IRB would
Furthermore, even with informed consent, several other
issues complicate user study design. For example, it is likely
that users’ browsing habits will change substantially if they
explicitly consent to monitoring (a human Heisenberg effect).
It is also difﬁcult to access an unbiased population of users
willing to participate in a trafﬁc study. Such concerns have
arisen in criticism of Alexa and other trafﬁc ranking ser-
vices who seek to provide much coarser data [19]. Although
user studies still seem likely to provide higher quality data
than modeling techniques like PageRank, even they are only
approximations to the “true” trafﬁc distribution we seek to
address.
We brieﬂy note that these concerns do not arise in situations
where users have weaker than usual expectations of privacy
and necessarily consent to outside monitoring of their surﬁng,
such as in a government or corporate setting. In such settings,
it would be possible to simply monitor the browsing behavior
of the entire population to collect the data for training and
evaluation of an anomaly detector. Again, the bounds we use
still apply in this case, with the actual user data substituted for
the data we sample from PageRank.
Otherwise, the disadvantages of direct data collection via
user studies suggest that it might be desirable to use a rep-
resentative model to generate synthetic trafﬁc data. Although
PageRank is one of the most popular, alternative models exist,
and we discuss one potentially more accurate option next.
2) The ABC Model: In a follow-up to the work on compar-
ing the random surfer model to real user data, Meiss et al. [40]
proposed a more sophisticated user model, the “ABC model.”
This model generates traces in a similar way to PageRank’s
random surfer model, but it simulates features such as back
buttons, user bookmarks, and decaying “user interest.” Meiss et
al. [40] demonstrate that it produces trace data that, in several
ways, agree with real user traces better than PageRank. This
includes prediction of aggregate page trafﬁc: they ﬁnd that
while the distribution of webpage trafﬁc produced by the ABC
model, PageRank, and the empirical data all roughly follow
power-law distributions, the ABC distribution more closely
matches with the empirical power law distribution.
Thus, if data quality is the most important consideration,
the ABC model may be more suited to testing web-data based
anomaly detectors than our PageRank model. The ABC model
would be a good choice in situations where, in addition to
the quality of data being the primary concern,
large-scale
user studies are simply infeasible. As mentioned, it could be
used with the general framework we present for obtaining
quantitative error bounds from a set of samples.
Unfortunately, improved accuracy is achieved at a sig-
niﬁcant cost per sample. Unlike PageRank, the ABC model
is inherently non-Markovian and thus the usual convergence
analyses do not apply. In fact, it is not clear from work on the
model that the long-run distribution over pages ever converges.
Even if the ABC model converges, it is not empirically known
how long one should simulate the ABC model for before
obtaining samples of page visits. Naturally, a “burn in” period
would be required to ensure that samples are sufﬁciently in-
dependent from the chosen start page. PageRank is considered
5
to mix very quickly – nevertheless, if it is sampled by simply
simulating the described random surfer model and taking a
sample after initial burn in, over 120 steps (page downloads)
would be necessary for selecting each unbiased test URL. In
contrast, the optimization we obtain from the reformulation of
Andersen et al. [11], which is unique to PageRank’s deﬁnition,
requires about 7 page downloads on average. It would be an
interesting direction for future theoretical work to see if the
long-run distribution of ABC model (or another similarly close
approximation to the real trafﬁc distribution) could also be
sampled so efﬁciently.
III. STATISTICAL BACKGROUND
Before giving an in-depth theoretical treatment of Page-
Rank and introducing our proposed sampling algorithm, we
review standard probability techniques that Fortuna will use
to compute its false positive rate bounds. In particular, we
explain how to determine the required number of test inputs
for obtaining a bound on the false positive rate (one-sided or
two-sided) to within any desired accuracy.
All of the error bounds described have been known for
quite some time in statistics and machine learning. Our
contribution is applying them to anomaly detection for the
ﬁrst time and, more importantly, providing a distribution and
accompanying sampling algorithm that is efﬁcient enough to
effectively apply the bounds presented.
A. Bounds on Sample Size
Let S be the space of all possible inputs to a program.
We will model
the benign inputs as having been drawn
(independently) from a distribution D, with support X ⊆ S;
that is, X is the space of benign inputs one could possibly
encounter in practice. Although in practice inputs are not
independent, we are only interested in averages over a long-
run distribution over inputs, and not in the order in which
the inputs were encountered. Equivalently, we could consider
deﬁning D to be an average over time of the (correlated) inputs.
We will see later that it is legitimate to treat the examples as
being independently distributed from a ﬁxed distribution for
the actual distribution we sample from—that is, that we can
sample directly from the “long-run” inputs of a synthetic input
distribution.
Let p : S → [0, 1] be the probability density function of D,
and let T : S → {0, 1} be the indicator function for harmful
inputs, i.e., labeling each s ∈ S as either benign (denoted by
0) or harmful (denoted by 1). Then, since X is the subset of
S (with nonzero density under D) containing benign inputs,
T (x) ≡ 0 for all x ∈ X. Now, suppose we have access to an
anomaly detection function F : S → {0, 1}. F is an attempt
to approximate T .
To capture differences between F and T , note that |F (x)−
T (x)| = 0 if the functions agree for an input x and |F (x) −
T (x)| = 1 if they disagree. While T is uniformly 0 on X, the
same is not necessarily true for F . We deﬁne:
Deﬁnition 1 (Type I Error over D):
err(1)D (F ) = E
D[F|X] =
(cid:88)
x∈X