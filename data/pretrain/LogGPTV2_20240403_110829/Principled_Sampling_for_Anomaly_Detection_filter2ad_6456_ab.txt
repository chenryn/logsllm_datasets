### Analysis and Implementation of False Positive Bounds for Anomaly Detectors

#### Analysis
Our analysis includes a review of well-known statistical tail inequalities, which characterize the number of sample inputs required to achieve a desired false positive bound (whether one-sided or two-sided).

#### Implementation
We present Fortuna, a system that implements the PageRank sampling algorithm using freely available data. Fortuna computes the resulting probabilistic false positive rate bounds.

#### Experimental Results
We provide false positive rate bounds for three different anomaly detectors:
- **SOAP** and **SIFT** for JPEG and PNG input files.
- **JSAND** for JavaScript programs.

These results indicate that Fortuna's sampling algorithm is efficient enough to generate a sufficient number of inputs, providing tight bounds for practical anomaly detectors.

Anomaly detectors are a critical component of modern computer security systems. Despite their central role, there has been little formal analysis to help practitioners understand the accuracy and effectiveness of these detectors. By providing guaranteed probabilistic bounds on false positive rates, Fortuna helps practitioners better understand the implications of deploying these security tools.

### The Case for PageRank

Fortuna computes bounds on Type I error by testing an anomaly detector on inputs drawn from a chosen distribution. The resulting bounds are meaningful with respect to data drawn from that distribution. To obtain practical bounds, it is crucial to choose a distribution that closely matches the inputs the anomaly detector will analyze in practice. Additionally, we need to rapidly obtain many samples from the distribution—tens of thousands of sample inputs are often required for tight bounds.

The tradeoff is clear: more sophisticated models or involved data collection processes may provide greater accuracy but at the cost of increased sampling difficulty. For our intended application—web data anomaly detectors—we argue that PageRank balances this tradeoff, providing an ideal distribution for testing. Its relative simplicity and ease of use suggest that PageRank could become a benchmark for web-data anomaly detectors, replacing ad hoc analysis methods and increasing consistency and reproducibility across results.

However, we stress that PageRank is just one possible viable distribution for testing anomaly detectors. When deploying an anomaly detector for non-web data, choosing an alternative distribution would be essential. Even within our chosen domain, depending on the intended application and available computational and data resources, alternative distributions might be more appropriate. Our goal is to provide a framework for computing bounds given any chosen distribution, with PageRank serving as a simple, easily implemented, yet powerful example.

### A Brief Introduction to PageRank

A more comprehensive treatment of the PageRank distribution, including mathematical definitions and a theoretical discussion of our sampling algorithm, is included in Section IV. Here, we provide an abbreviated introduction.

The PageRank distribution was first introduced by Page et al. [45] and forms the backbone of Google’s search result ranking algorithm. It aims to weight pages by importance and has been extensively studied due to its relative simplicity, accuracy, and utility for other purposes such as local graph partitioning and robust eigenvector approximations [13], [32], [11], [39].

To understand the PageRank distribution, consider the following "random surfer" process for randomly accessing pages on the World Wide Web:
1. Start by picking a starting webpage uniformly at random from all possible pages.
2. If the current page has no outgoing links, jump to a page selected uniformly at random from all possible pages.
3. Otherwise, with probability \( \alpha \) (typically set to 0.85), choose a link from the current page uniformly at random and follow that link. With probability \( 1 - \alpha \), jump to another page selected uniformly at random from all possible pages.

This Markov process loosely captures the behavior of a typical Internet user. The PageRank of a URL is the fraction of visits to that page during the random surfer process as the number of steps goes to infinity, i.e., the long-run fraction of time spent on that URL. Intuitively, webpages with more incoming links tend to have higher PageRank because the random surfer has more ways to reach them. PageRank also captures the quality of these links; links from high PageRank sources are more important because they are more likely to be followed. This "quality by association" is the key insight behind PageRank, which has empirically outperformed other notions of importance on the web.

### Merits of PageRank

We claim that computing Type I errors with respect to a PageRank distribution is reasonable for two reasons:
1. **Relative Importance**: PageRank has been successful in practice at capturing the relative importance of webpages [32, pp.4,25]. Therefore, it is reasonable to use PageRank to weight the importance of Type I (false positive) errors on various webpages. False positives on more important pages are more serious, as they are more likely to be visited.
2. **User Behavior Model**: The random surfer process is a reasonable synthetic model for the behavior of an average user. The corresponding PageRank distribution, by definition, captures the long-run distribution over pages visited by the random surfer, making it a reasonable synthetic model for the long-run distribution over pages visited by an average user.

In support of the second claim, we collected data on approximately 100,000 of the Web’s most visited webpages [2], comparing their PageRank score (provided by Google [6]) and their global traffic ranking (provided by Alexa Internet [1]). Figure 1 presents this data in a 2-dimensional histogram, indicating that PageRank decreases in correlation with reduced traffic rank. Noise in the plot can be attributed to several factors, such as the volatility of traffic numbers and the logarithmic scale of PageRank scores. Nevertheless, the overall trend shows that PageRank correlates positively with traffic ranking, roughly capturing visit frequencies while providing a long-term measure of page importance.

### Alternatives to PageRank

Several alternatives to the PageRank distribution for web data exist. While PageRank is based on the link structure between webpages, other techniques may more closely match actual traffic numbers. However, sampling from these distributions is generally prohibitively expensive compared to the efficient method presented in Section IV-B for PageRank sampling.

#### Large-scale User Studies
An ideal choice for obtaining test data for web-based anomaly detectors would be to collect data directly from real users. Data from traffic ranking sites like Alexa is not fine-grained enough for our purposes, and such data is typically only available for the most popular sites. A large-scale user study would need to cover a significant population over a long period. Such a study was conducted by Meiss et al. [41], but it faced challenges in obtaining individualized informed consent and ensuring user privacy. Furthermore, subsequent developments in deanonymization and allegations of noncompliance at Indiana University raise questions about the feasibility of such studies without user consent.

#### The ABC Model
Meiss et al. [40] proposed the "ABC model," a more sophisticated user model that simulates features such as back buttons, user bookmarks, and decaying user interest. The ABC model produces trace data that, in several ways, agrees better with real user traces than PageRank. If data quality is the primary concern, the ABC model may be more suitable for testing web-data-based anomaly detectors. However, the ABC model is inherently non-Markovian, and it is unclear how long one should simulate the model before obtaining independent samples. In contrast, PageRank mixes very quickly, requiring only about 7 page downloads on average for each unbiased test URL.

### Statistical Background

Before delving into the theoretical treatment of PageRank and introducing our proposed sampling algorithm, we review standard probability techniques that Fortuna uses to compute false positive rate bounds. Specifically, we explain how to determine the required number of test inputs for obtaining a bound on the false positive rate (one-sided or two-sided) to within any desired accuracy.

All the error bounds described have been known in statistics and machine learning for some time. Our contribution is applying them to anomaly detection for the first time and providing a distribution and accompanying sampling algorithm that is efficient enough to effectively apply these bounds.

#### Bounds on Sample Size
Let \( S \) be the space of all possible inputs to a program. We model benign inputs as being drawn (independently) from a distribution \( D \), with support \( X \subseteq S \). Although inputs in practice are not independent, we are interested in averages over a long-run distribution over inputs, not in the order in which the inputs were encountered.

Let \( p : S \to [0, 1] \) be the probability density function of \( D \), and let \( T : S \to \{0, 1\} \) be the indicator function for harmful inputs, labeling each \( s \in S \) as either benign (0) or harmful (1). Since \( X \) is the subset of \( S \) containing benign inputs, \( T(x) \equiv 0 \) for all \( x \in X \). Suppose we have access to an anomaly detection function \( F : S \to \{0, 1\} \). \( F \) is an attempt to approximate \( T \).

To capture differences between \( F \) and \( T \), note that \( |F(x) - T(x)| = 0 \) if the functions agree for an input \( x \) and \( |F(x) - T(x)| = 1 \) if they disagree. While \( T \) is uniformly 0 on \( X \), the same is not necessarily true for \( F \). We define:

**Definition 1 (Type I Error over \( D \))**:
\[ \text{err}_D^{(1)}(F) = E_D[F|X] = \sum_{x \in X} p(x) F(x) \]

This definition quantifies the expected value of the anomaly detection function \( F \) over the benign inputs, providing a measure of the false positive rate.