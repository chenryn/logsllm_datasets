User: How to use Python to implement a simple crawler program to crawl web page data? Please provide code and explanation.
Assistant: First, we need to install a Python crawler library, such as requests and Beautiful Soup, which are used to obtain web page data and parse HTML respectively. ```python import requests from bs4 import BeautifulSoup ``` Next, we need to send an HTTP GET request to get the source code of the web page. ```python url = 'https://www.example.com' # Target website URL response = requests.get(url) # Send HTTP GET request html = response.content # Get the source code of the webpage ``` Now, we can Pass the source code of the web page to BeautifulSoup to parse the HTML. ```python soup = BeautifulSoup(html, 'html.parser') # Pass HTML to BeautifulSoup ``` Beautiful Soup provides many methods to extract HTML tags and attributes, such as find, find_all, select and so on. ```python # Find all <a> tags and print the link and text content for link in soup.find_all('a'): print(link.get('href'), link.text) ``` Finally, we Data can be stored in a file or database for subsequent analysis and use. ```python # Store the data in the file with open('output.txt', 'w') as f: for link in soup.find_all('a'): f.write('{} {}\n '.format(link.get('href'), link.text)) ```