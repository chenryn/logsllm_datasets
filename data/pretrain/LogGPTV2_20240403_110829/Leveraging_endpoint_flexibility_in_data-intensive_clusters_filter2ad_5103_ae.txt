a Facebook Fellowship.
11 References
[1] Amazon EC2. http://aws.amazon.com/ec2.
[2] Amazon Simple Storage Service.
http://aws.amazon.com/s3.
[3] Apache Hadoop. http://hadoop.apache.org.
[4] Facebook Production HDFS. http://goo.gl/BGGuf.
[5] How Big is Facebook’s Data? 2.5 Billion Pieces Of Content And
500+ Terabytes Ingested Every Day. TechCrunch
http://goo.gl/n8xhq.
[6] Total number of objects stored in Amazon S3.
http://goo.gl/WTh6o.
[7] S. Agarwal et al. Reoptimizing data parallel computing. In NSDI,
2012.
[8] M. Al-Fares et al. Hedera: Dynamic ﬂow scheduling for data center
networks. In NSDI, 2010.
[9] N. Alon et al. Approximation schemes for scheduling on parallel
machines. Journal of Scheduling, 1:55–66, 1998.
240[10] G. Ananthanarayanan et al. Reining in the outliers in mapreduce
clusters using Mantri. In OSDI, 2010.
[11] G. Ananthanarayanan et al. Scarlett: Coping with skewed popularity
content in mapreduce clusters. In EuroSys, 2011.
[44] M. Zaharia et al. Delay scheduling: A simple technique for achieving
locality and fairness in cluster scheduling. In EuroSys, 2010.
[45] M. Zaharia et al. Resilient distributed datasets: A fault-tolerant
abstraction for in-memory cluster computing. In NSDI, 2012.
[12] G. Ananthanarayanan et al. PACMan: Coordinated memory caching
for parallel jobs. In NSDI, 2012.
[13] T. Benson et al. MicroTE: Fine grained trafﬁc engineering for data
centers. In CoNEXT, 2011.
[14] P. Bodik et al. Surviving failures in bandwidth-constrained
datacenters. In SIGCOMM, 2012.
[15] D. Borthakur. The Hadoop distributed ﬁle system: Architecture and
design. Hadoop Project Website, 2007.
[16] D. Borthakur et al. Apache Hadoop goes realtime at Facebook. In
SIGMOD, pages 1071–1080, 2011.
[17] B. Calder et al. Windows Azure Storage: A highly available cloud
storage service with strong consistency. In SOSP, 2011.
[18] M. Castro et al. Scalable application-level anycast for highly
dynamic groups. LNCS, 2816:47–57, 2003.
[19] R. Chaiken et al. SCOPE: Easy and efﬁcient parallel processing of
massive datasets. In VLDB, 2008.
[20] M. Chowdhury et al. Managing data transfers in computer clusters
with Orchestra. In SIGCOMM, 2011.
[21] M. Chowdhury and I. Stoica. Coﬂow: A networking abstraction for
cluster applications. In HotNets-XI, pages 31–36, 2012.
[22] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed data processing
on large clusters. In OSDI, pages 137–150, 2004.
[23] M. Y. Eltabakh et al. CoHadoop: Flexible data placement and its
exploitation in hadoop. In VLDB, 2011.
[24] A. D. Ferguson et al. Hierarchical policies for Software Deﬁned
Networks. In HotSDN, pages 37–42, 2012.
[25] M. Freedman, K. Lakshminarayanan, and D. Mazières. OASIS:
Anycast for any service. NSDI, 2006.
[26] M. Garey and D. Johnson. “Strong” NP-completeness results:
Motivation, examples, and implications. Journal of the ACM,
25(3):499–508, 1978.
[27] S. Ghemawat, H. Gobioff, and S.-T. Leung. The Google ﬁle system.
In SOSP, 2003.
[28] A. Greenberg et al. VL2: A scalable and ﬂexible data center network.
In SIGCOMM, 2009.
[29] C. Guo et al. DCell: A scalable and fault-tolerant network structure
for data centers. In SIGCOMM, pages 75–86, 2008.
[30] C. Guo et al. BCube: A High Performance, Server-centric Network
Architecture for Modular Data Centers. ACM SIGCOMM, 2009.
[31] Z. Guo et al. Spotting code optimizations in data-parallel pipelines
through PeriSCOPE. In OSDI, 2012.
[32] B. Hindman et al. Mesos: A Platform for Fine-Grained Resource
Sharing in the Data Center. In NSDI, 2011.
[33] C. Huang et al. Erasure coding in Windows Azure Storage. In
USENIX ATC, 2012.
[34] M. Isard et al. Quincy: Fair scheduling for distributed computing
clusters. In SOSP, 2009.
[35] S. Kandula et al. The nature of datacenter trafﬁc: Measurements and
analysis. In IMC, 2009.
[36] J. Lenstra, D. Shmoys, and É. Tardos. Approximation algorithms for
scheduling unrelated parallel machines. Mathematical Programming,
46(1):259–271, 1990.
[37] R. Motwani, S. Phillips, and E. Torng. Non-clairvoyant scheduling,
1993.
[38] R. N. Mysore et al. PortLand: A scalable fault-tolerant layer 2 data
center network fabric. In SIGCOMM, pages 39–50, 2009.
[39] E. Nightingale et al. Flat Datacenter Storage. In OSDI, 2012.
[40] M. Sathiamoorthy et al. XORing elephants: Novel erasure codes for
big data. In PVLDB, 2013.
[41] A. Thusoo et al. Data warehousing and analytics infrastructure at
Facebook. In SIGMOD, 2010.
[42] R. van Renesse and F. B. Schneider. Chain replication for supporting
high throughput and availability. In OSDI, 2004.
[43] M. Zaharia et al. Improving mapreduce performance in
heterogeneous environments. In OSDI, 2008.
APPENDIX
A Optimizing Block Writes
A.1 Problem Formulation and Complexity
Assume that a replica placement request for a block B of size
Size(B) arrives at time Arr(B) with replication factor r and fault-
tolerance factor f. The CFS client will write a local copy wherever
it is located, and the replica placement policy must ﬁnd locations
for (r (cid:0) 1) off-rack copies in f other fault domains across rack
boundaries. We choose r = 2 and f = 1 to simplify the analysis:
the case of larger r is ignored, because an increase in r does not
increase the number of off-rack copies; the case of larger f is dis-
cussed in §A.2. Assume that there are no other constraints besides
physical limits such as link capacity and disk throughput.
Let L denote the set of possible bottleneck links in the network.
Also, let Cap(l) and Ut(l) denote the capacity and the estimation
of the utilization of link l 2 L at time t. Placement decisions are
instantaneous.
For a given time period T (discretized into equal-sized decision
interval or quanta, q) from t, the objective (U(:)) can then be rep-
resented by the following equation.
∑
Minimize
fB j Arr(B)2[t;t+T )g
Dur(B)
(1)
where Dur(B) is the time to write a block B from Arr(B).
Distributed writing (i.e., optimizing U) is NP-hard, even when
all block requests and link capacities are known beforehand.
Theorem A.1 Distributed Writing is NP-hard.
Proof Sketch We prove it by giving a reduction from job shop
scheduling, which is NP-hard [26]. Consider each link l to have
the same capacity, and B to be the set of blocks that arrive over
time with different sizes. Also note that once the CFS client starts
writing a block, its location cannot be changed. Then, optimally
scheduling jBj jobs on jLj(> 2) identical machines without pre-
emption will provide the optimal replica placement decisions. ■
A.2 Optimal Block Placement Algorithm
If all blocks have the same size, decision intervals are indepen-
dent, and link utilizations do not change within the same decision
interval, greedy assignment of blocks to the least-loaded link will
maximize U.
Theorem A.2 Under the assumptions in Section 5.1, greedy as-
signment of blocks to the least-loaded link is optimal (OP T ).
Proof Sketch Assume that bottleneck links l 2 L at time t are
sorted in non-decreasing order (l1 (cid:20) l2 (cid:20) : : : (cid:20) ljLj) of their cur-
rent utilization, and multiple block requests (B = fBjg) of equal
size arrived at t. Since all blocks have the same size, they are in-
distinguishable and can be assigned to corresponding destinations
through some bottleneck links one by one.
Assume block Bj is placed through a bottleneck link li. If there
is another link li(cid:0)1 with more available capacity, then we can sim-
ply place Bj through li(cid:0)1 for a faster overall completion time. This
process will continue until there is no more such links, and Bj has
■
been placed through l1 – the least-loaded bottleneck link.
241where G is the number of congested links in L. As f increases, so
does P (UniSlowdown); Figure 12 shows this for different values
of G in a 150-rack cluster. The extent of U N I slowdown (thus I)
depends on the distribution of imbalance.
B Optimizing File Writes
B.1 Problem Formulation and Complexity
Assume that a ﬁle W = (B1; B2; : : : ; BjWj) with jWj blocks ar-
rives at Arr(W ). Requests for its individual blocks Bj arrive one
at a time, each at the beginning of a discrete decision interval. For a
given time period T (discretized into equal-sized quanta, q) from t,
the objective (V(:)) can then be represented by the following equa-
tion.
∑
Minimize
fW j Arr(W )2[t;t+T )g
Dur(W )
(6)
where Dur(W ) = max
Bj2W
Dur(Bj) denotes the time to ﬁnish writ-
ing all the blocks of W from Arr(W ). Optimizing (6) is no easier
than optimizing (1), and it is NP-hard as well.
B.2 Optimal File Write Algorithm
Lemma B.1 OP T is not optimal for end-to-end write operations
with multiple blocks (V).
Proof Sketch We prove this using a counterexample. Let us take a
simple example with two bottleneck links, l1 and l2, through which
a single block can be written in d1 and d2 time units (d1  1 and Rem(Wj+1) = 1. Now, the total
contributions of Bj and Bj+1 to V(OP T
) is (1 + di+1). We can
decrease it to (1 + di) by swapping the order of Bj and Bj+1. By
continuously applying this reordering, we will end up with an order
where blocks from writes with fewer remaining blocks will end up
■
in front.
′
(2)
(3)
(5)
Figure 12: Analytical probability of at least one off-rack replica experienc-
ing a slowdown with uniform placement, as the number of off-rack replicas
(f) increases. Each line corresponds to the fraction of contended links (out
of 150) that can cause slowdown.
Improvements Over the Default Policy Let AC(li) denote the
available capacity to write block B through li,
AC(li) = min(Cap(li) (cid:0) Ut(li); DiskWriteCap)
where DiskWriteCap is the write throughput of a disk. Then the
completion time of OP T for writing jBj blocks that must be placed
during that quantum is
U(OP T ) = jBjSize(B)
jLj∑
fi
AC(li)
i=1
where fi is the fraction of blocks allocated through li. Because
OP T greedily places blocks starting from the least-loaded link,
higher AC(li) will result in higher fi.
Current CFS replica placement algorithms (U N I) place replicas
uniformly randomly across all bottleneck links. The expected time
for U N I to write jBj blocks that must be placed during a single
quantum is
U(U N I) =
jBjSize(B)
jLj
jLj∑
1
AC(li)
i=1
where each link li will receive equal fractions ( 1jLj ) of blocks.
Given (2) and (3), the factor of improvement (I) of OP T over
U N I at any decision interval is
I =
U(U N I)
U(OP T )
= jLj
fi
AC(li)
1
(4)
i=1
AC(li)
The overall improvement during the discretized interval [t; t + T )
is
∑jLj
∑jLj
i=1
∑
∑
[t;t+T ) U(U N I)
[t;t+T ) U(OP T )
Improvements for f > 1 We have assumed that only one copy of
each block crosses the bottleneck links (i.e., f = 1). However, for a
given placement request, the probability of experiencing contention
using U N I increases with f; this should increase I, because OP T
will never experience more contention than U N I.
While creating replicas in f fault domains through jLj(≫ f )
bottleneck links, if one of the f replicas experience contention, the
entire write will become slower. Using the uniform placement pol-
icy, the probability of at least one of them experiencing contention
is
(
1 (cid:0) f∏
g=1
)
jLj (cid:0) G (cid:0) g + 1
jLj (cid:0) g + 1
P (UniSlowdown) =
10%!20%!30%!40%!50%!0!0.2!0.4!0.6!0.8!1!1!2!3!4!Probability of Experiencing Slowdown!Number of Fault Domains, f!242