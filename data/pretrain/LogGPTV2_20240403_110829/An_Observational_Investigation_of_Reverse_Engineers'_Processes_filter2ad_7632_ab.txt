size and observation of RE processes on diﬀerent, real-world
programs, demonstrating RE behaviors to ensure saturation
of themes [64, pg. 113-115].
In our prior work, we performed 25 interviews of white-hat
hackers and testers to determine their vulnerability discovery
processes [1]. While this research identiﬁed RE as an impor-
tant part of the vulnerability discovery process, its broader
focus (e.g., process, skill development, and community in-
teraction) limited its ability to provide details regarding how
RE is carried out, leading us to our current, more focused
investigation.
3 Method
We are interested in developing a theoretical model of the
RE process with respect to both overall strategy and speciﬁc
techniques used. In particular, we focus on the three research
questions given in Section 1.
To answer these questions, we employ a semi-structured,
observation-based interview protocol, designed to yield de-
tailed insights into RE experts’ processes. The full protocol
is given in Appendix A. Interviews lasted 70 minutes on av-
erage. Audio and video were recorded during each interview.
All interviews were led by the ﬁrst author, who has six years
USENIX Association
29th USENIX Security Symposium    1877
of professional RE experience, allowing him to understand
each RE’s terminology and process, ask appropriate probing
questions, and identify categories of similar actions for cod-
ing. Participants were provided a $40 gift card in appreciation
of their time. Our study was reviewed and approved by the
University of Maryland’s Institutional Review Board. In this
section, we describe our interview protocol and data analysis
process, and we discuss limitations of our method.
semi-structured, observational
Interview Protocol
performed
3.1
We
video-
teleconference interviews. We implemented a modiﬁed
version of the Critical Decision Method, which is intended
to reveal expert knowledge by inquiring about speciﬁc
cases of interest [44]. We asked participants to choose an
interesting program they recently reverse engineered, and
had them recall and demonstrate the process they used.
Each observation was divided into the two parts: program
background and RE process. Throughout, the interviewer
noted and asked further questions about multiple items of
interest.
Program background. We began by asking participants to
describe the program they chose to reverse engineer. This
included questions about the program’s functionality and size,
what tools (if any) they used, and whether they reverse engi-
neered the program with others.
Reverse engineering process. Next, we asked participants
about their program-speciﬁc RE goals, and then asked them to
recreate their process while sharing their screen (RQ1)1. We
chose to have participants demonstrate their process, asking
them to open all tools they used and perform all original steps,
so we could observe automatic and subconscious behaviors—
common in expert tasks [65]—that might be missed if simply
asked to recall their process. As the participant recreated
their process, we asked several directed questions intended to
probe their understanding while allowing them to delve into
areas they felt were important. We encouraged participants
to share their entire process, even if a particular speculative
step did not end up supporting their ﬁnal goal. For example,
they may have decided to reverse a function that turned out to
be a common library function already documented elsewhere,
resulting in no new information gain.
Instead of asking participants to demonstrate a recent ex-
perience, we could have asked them to RE a program new to
them. This could be more representative of the real-world ex-
perience of approaching a new program and might highlight
additional subconscious or automatic behaviors. However, it
would likely require a much longer, probably unreasonable
period of observation. When asked how much time partici-
pants spent reverse engineering the programs demonstrated,
1The only participant who did not share their screen did so because of
technical diﬃculties that could not be resolved in a timely manner.
answers ranged from several hours to weeks. Alternatively, we
could have asked participants to RE a toy program. However,
this approach restricts the results, both in depth of process
and in terms of the program type(s) selected. Demonstration
provides a reasonable compromise, and is a standard practice
in NDM studies [44]. In practice, we believe the eﬀect of
demonstration was small, especially because the interviewer
asked probing questions to reveal subconscious actions.
Items of interest. The second characteristic of the Critical
Decision Method is that the interviewer asks follow-on ques-
tions about items of interest to the research. We selected our
items of interest from those identiﬁed as important in prior
NDM (decision) and program comprehension (questions/hy-
potheses, beacons, simulation methods) literature—discussed
in Sections 2.1 and 2.2, respectively. These items were chosen
to identify speciﬁc approaches used (RQ2) and diﬀerences
between RE and other program comprehension tasks (RQ3).
Below, we provide a short description of each and a summary
of follow-on questions asked:
• Decisions. These are moments where the RE decides be-
tween one or more actions. This can include deciding whether
to delve deeper into a speciﬁc function or which simulation
method to apply to validate a new hypothesis. For decision
points, we asked participants to explain how they made the
decision. For example, when deciding to analyze a function,
the RE might consider what data ﬂows into the function as
arguments or what calls it.
• Questions/Hypotheses. These are questions that must
be answered or conjectures about what the program does.
Reverse engineers might form a hypothesis about the main
purpose of a function, or whether a certain control ﬂow is
possible. Prior work has shown that hypotheses are central
part to program comprehension [2, 27–29], so we expected
hypothesis generation and testing to be central to RE. For
hypotheses, we asked participants to explain why they think
the hypothesis might be true and how they tested it. As an
example, if a RE observes a call to strcpy, they might hy-
pothesize that a buﬀer overﬂow is possible. To validate their
hypothesis, they would check whether unbounded user input
can reach this call.
• Simulation methods. Any process where a participant
reads or runs the code to determine its function. We asked REs
about any manual or automated simulation methods used: for
example, using a debugger to determine the program’s mem-
ory state at a speciﬁc point. We wanted to know whether they
employed any tools and if they were custom, open source,
or purchased. Further, we asked them to evaluate any tools
used, and to discuss their eﬀectiveness for this particular task.
Additionally, we asked participants why they used particu-
lar simulation methods, whether they typically did so, the
method’s inputs and outputs, and how they know when to
switch methods.
• Beacons. These include patterns or tells that a RE recog-
1878    29th USENIX Security Symposium
USENIX Association
nizes, allowing them to quickly generate hypotheses about
the program’s functionality without reading line-by-line. For
example, if a RE sees an API call to get a secure random
number with several bit-shift operations, they may assume
the associated function performs a cryptographic process. For
beacons, we had REs explain why the beacon stood out and
how they recognized it as that sort of beacon rather than some
other pattern. The goal in inquiring into this phenomenon is to
understand how REs perform pattern matching, and identify
potentially common beacons of importance.
Additionally, we noted whenever participants referenced
documentation or information sources external to the code—
e.g., StackOverﬂow, RE blogs, API documentation—to an-
swer a program functionality question. We asked whether they
use that resource often, and why they selected that resource.
To make the interviews more ﬂuid and less repetitive, we in-
tentionally skipped questions that had already been answered
in response to prior questions. To ensure consistency, all the
interviews were conducted by the ﬁrst author.
We conducted two pilot interviews prior to the main study.
After the ﬁrst pilot, we made adjustments to ensure appropri-
ate terminology was used and improve question ﬂow. How-
ever, no changes were required after the second interview, so
we included the second pilot interview in our main study data.
3.2 Data Analysis
We applied iterative open coding to identify interview
themes [66, pg. 101-122]. After completing each interview,
the audio was sent to an external transcription service. The
interviewer and another researcher ﬁrst collaboratively coded
three interviews—reviewing both the text and video—to cre-
ate an initial codebook2. Then, the two coders independently
coded 13 interviews, comparing codes after every three inter-
views to determine inter-coder reliability. To measure inter-
coder reliability, we used Krippendorﬀ’s Alpha (α), as it ac-
counts for chance agreements [67].3 After each round, the
coders resolved any diﬀerences, updated the codebook as nec-
essary, and re-coded previously coded interviews. The coders
repeated this process four times until they achieved an α of
0.8, which is above the recommended level for exploratory
studies [67, 69].
Next, we sought to develop our theoretical model by extract-
ing themes from the coded data. First, we grouped identiﬁed
codes into related categories. Speciﬁcally, we discovered three
categories associated with the phases of analyses performed
by REs (i.e., Overview, Sub-component Scanning, and Fo-
cused Experimentation). Then, we performed an axial coding
to determine relationships between and within each phase
and trends across the three phases [66, pg. 123-142]. From
2The ﬁnal codebook can be found in an extended form of this paper at
https://ter.ps/REStudy2020
3The ReCal2 software package was used to calculate Krippendorﬀ’s
Alpha [68]
these phases and their connections, we derive a theory of REs’
high-level processes and speciﬁc technical approaches. We
also present a set of interaction-design guidelines for building
analysis tools to best ﬁt REs.
3.3 Limitations
There are a number of limitations innate to our methodology.
First, participants likely do not recall all task details they are
asked to relay. This is especially common for expert tasks [65].
We attempt to address this by using the CDM protocol, which
has been used successfully in prior decision-making research
on expert tasks [44]. Furthermore, we asked participants to
recreate the RE task while the interviewer observed. This
allowed the interviewer to probe subconscious actions that
would likely have been skipped without observation.
Participants also may have skipped portions of their process
to protect trade secrets; however, in practice we believe this
did not impact our results. Multiple participants stated they
could not demonstrate certain conﬁdential steps, but the secret
component was in the process’s operationalization (e.g., the
keyword list used or speciﬁc analysis heuristics). In all cases,
participants still described their general process, which we
were able to include in our analysis.
Finally, we focus on experienced REs to understand and
model expert processes. Future work should consider newer
REs to understand their struggles and support their develop-
ment.
4 Recruitment and Participants
We recruited interview participants from online forums, vul-
nerability discovery organizations, and relevant conferences.
Online forums. We posted recruitment notices on a number
of RE forums, including forums for popular RE tools such as
IDAPro and BinaryNinja. We also posted ads on online com-
munities like Reddit. Dietrich et al. showed online chatrooms
and forums are useful for recruiting security professionals,
since participants are reached in a more natural setting where
they are more likely to be receptive [70].
Related organizations. We contacted the leadership of
ranked CTF teams4 and bug bounty-as-a-service companies
asking them to share study details with their members. Our
goal in partnering with these organizations was to gain cred-
ibility with members and avoid our messages dismissed as
spam. Prior work found relative success with this strategy [1].
To lend further credibility, all emails were sent from an ad-
dress associated with our institution, and detailed study infor-
mation was hosted on a web domain owned by our institution.
Relevant conferences. Finally, we recruited at several confer-
ences commonly attended by REs. We explained study details
4Found via https://ctftime.org/
USENIX Association
29th USENIX Security Symposium    1879
and participant requirements in person and distributed busi-
ness cards with study information. Recruiting face-to-face
allowed us to clearly explain the goal of the research and its
potential beneﬁts to the RE community.
Participant screening. We asked respondents to our recruit-
ment eﬀorts to complete a short screening questionnaire. Our
questionnaire5 asked participants to self-report their level
of RE expertise on a ﬁve-point Likert-scale from novice to
expert; indicate their years of RE experience; and answer
demographic questions. As our goal is to produce interaction
guidelines to ﬁt REs’ processes, building on less experienced
REs’ approaches may not be beneﬁcial. Therefore, we only
selected participants who rated themselves at least a three
on the Likert scale and had at least three years of RE expe-
rience.We contacted volunteers in groups of ten in random
order, waiting one week for their response before moving
to the next group. This process continued until we reached
suﬃcient interview participation.
Participants. We conducted interviews between October
2018 and January 2019. We received 68 screening survey
responses; 42 met our expertise criteria. Of these volunteers,
16 responded to randomly ordered scheduling requests and
were interviewed. We stopped further recruitment after 16 in-
terviews, when we reached saturation, meaning we no longer
observed new themes emerging. This is the standard stop-
ping criteria for a rigorous qualitative process [64, pg. 113-
115]. Because our participant count is within the range recom-
mended by best practice literature (12-20 participants), our
results provide useful insights for later quantitative inquiry
and generalizable recommendations [71].
Table 1 shows the type of program each participant reverse
engineered during the interview and their demographics, in-
cluding their self-reported skill level, years of experience, and
the method used to recruit them. Each participants’ ID indi-
cates their assigned ID number and the primary type of RE
tasks they perform. For example, P01M indicates the ﬁrst
interviewee is a malware analyst. Note that three interviewees
used a challenge binary6 during the interview. These partici-
pants could not show us any examples from their normal work
due to the proprietary or conﬁdential nature of their work. In-