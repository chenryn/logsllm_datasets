in human language.
Permission Fidelity. To demonstrate the security-awareness of
DESCRIBEME, we use a description vetting tool, AutoCog [28],
to evaluate the “permission-ﬁdelity” of descriptions. AutoCog ex-
amines the descriptions and permissions of an app to discover their
discrepancies. We use it to analyze both the original descriptions
and the security-centric ones produced by DESCRIBEME, and as-
sess whether our descriptions can be associated to more permis-
sions that are actually requested.
Unfortunately, AutoCog only supports 11 permissions in its cur-
rent implementation. In particular, it does not handle some crucial
permissions that are related to information stealing (e.g., phone
number, device identiﬁer, service provider, etc.), sending and re-
ceiving text messages, network I/O and critical system-level be-
haviors (e.g., KILL_BACKGROUND_PROCESSES). The limitation of
AutoCog in fact brings difﬁculties to our evaluation: if generated
descriptions are associated to these unsupported permissions, Au-
toCog fails to recognize them and thus cannot conduct equitable as-
sessment. Such a shortcoming is also shared by another NLP-based
(i.e., natural language processing) vetting tool, WHYPER [26],
which focuses on even fewer (3) permissions. This implies that it is
a major challenge for NLP-based approaches to achieve high per-
mission coverage, probably because it is hard to correlate texts to
semantically obscure permissions (e.g., READ_PHONE_STATE). In
Figure 9: Permissions Reﬂected in Descriptions
Figure 10: Readability Ratings
contrast, our approach does not suffer from this limitation because
API calls are clearly associated to permissions [10].
Despite the difﬁculties, we manage to collect 30 benign apps
from Google play and 20 malware samples from Malware Genome
Project [5], whose permissions are supported by AutoCog. We
run DESCRIBEME to create the security-centric descriptions and
present both the original and generated ones to AutoCog. How-
ever, we notice that AutoCog sometimes cannot recognize certain
words that have strong security implications. For example, DE-
SCRIBEME uses “geographic location” to describe the permissions
ACCESS_COARSE_LOCATION and ACCESS_FINE_LOCATION. Yet,
AutoCog cannot associate this phrase to any of the permissions.
The fundamental reason is that AutoCog and DESCRIBEME use
different glossaries. AutoCog performs machine learning on a par-
ticular set of apps and extracts the permission-related glossary from
these existing descriptions.
In contrast, We manually select de-
scriptive words for each sensitive API, using domain knowledge.
To bridge this gap, we enhance AutoCog to recognize the man-
ually chosen keywords. The experimental result is illustrated in
Figure 9, where X-axis is the app ID and Y-axis is the amount of
permissions. The three curves, from top to bottom, represent the
amounts of permissions that are requested by the apps, recognized
by AutoCog from security-centric descriptions and identiﬁed from
original descriptions, respectively. Cumulatively, 118 permissions
are requested by these 50 apps. 20 permissions are discovered from
the old descriptions, while 66 are uncovered from our scripts. This
reveals that DESCRIBEME can produce descriptions that are more
security-sensitive than the original ones.
DESCRIBEME fails to describe certain permission requests due
to three reasons. First, some permissions are used for native code
or reﬂections that cannot be resolved. Second, a few permissions
are not associated to API calls (e.g., RECEIVE_BOOT_COMPLETED),
and thus are not included into the SBGs. Last, some permissions
are correlated to certain API parameters. For instance, the query
API requires permission READ_CONTACTS only if the target URI
is the Contacts database. Thus, if the parameter value cannot be
extracted statically, such a behavior will not be described.
6.2 Readability and Effectiveness
To evaluate the readability and effectiveness of generated de-
scriptions, we perform a user study on the Amazon’s Mechanical
Turk (MTurk) [1] platform. The goal is two-fold. First, we hope
to know whether the generated scripts are readable to average au-
dience. Second, we expect to see whether our descriptions can ac-
tually help users avoid risky apps. To this end, we follow Felt et
al.’s approach [16], which also designs experiments to understand
the impact of text-based protection mechanisms.
Methodology. We produce the security-centric descriptions for
Android apps using DESCRIBEME and measure user reaction to
the old descriptions (Condition 1.1, 2.1-2.3), machine-generated
ones (Condition 2.1) and the new descriptions (Condition 2.4-2.6).
Notice that the new description is the old plus the generated one.
Dataset. Due to the efﬁciency consideration, we perform the
user study based on the descriptions of 100 apps. We choose these
100 apps in a mostly random manner but we also consider the dis-
tribution of app behaviors. In particular, 40 apps are malware and
the others are benign. We manually inspect the 60 benign ones and
further put them into two categories: 16 privacy-breaching apps
and 44 completely clean ones.
Participants Recruitment. We recruit participants directly from
MTurk and we require participants to be smartphone users. We also
ask screening questions to make sure participants understand basic
smartphone terms, such as “Contacts” or “GPS location”.
Hypotheses and Conditions.
Hypothesis 1: Machine-generated descriptions are readable to
average smartphone users. To assess the readability, we prepare
both the old descriptions (Condition 1.1) and generated ones (Con-
dition 1.2) of the same apps. We would like to evaluate machine-
generated descriptive texts via comparison.
Hypothesis 2: Security-centric descriptions can help reduce the
downloading of risky apps. To test the impact of the security-
centric descriptions, we present both the old and new (i.e., old +
generated) descriptions for malware (Condition 2.1 and 2.4), be-
nign apps that leak privacy (Condition 2.2 and 2.5) and benign apps
without privacy violations (Condition 2.3 and 2.6). We expect to
assess the app download rates on different conditions.
User Study Deployment. We post all the descriptions on MTurk
and anonymize their sources. We inform the participants that the
tasks are about Android app descriptions and we pay 0.3 dollars for
each task. Participants take part in two sets of experiments. First,
they are given a random mixture of original and machine-generated
descriptions, and are asked to provide a rating for each script with
respect to its readability. The rating is ranged from 1 to 5, where 1
means completely unreadable and 5 means highly readable.
Second, we present the participants another random sequence
of descriptions. Such a sequence contains both the old and new
descriptions for the same apps. Again, we stress that the new de-
scription is the old one plus the generated one. Then, we ask par-
ticipants the following question: “Will you download an app based
on the given description and the security concern it may bring to
you?”. We emphasize “security concern” here and we hope partic-
ipants should not accept or reject an app due to the considerations
(e.g., functionalities, personal interests) other than security risks.
Limitations. The security-centric descriptions are designed to
be the supplement to the original ones. Therefore, we present the
two of them as an entirety (i.e., new description) to the audience, in
the second experiment. However, this may increase the chance for
participants to discover the correlation between a pair of old and
new descriptions. As a result, we introduce randomness into the
display order of descriptions to mitigate the possible impact.
Results and Implications. Eventually, we receive 573 responses
02468149Number of Permissions App ID Described Permissions New Desc.Orig. Desc.Permission List123451100Readability Score App ID Readability Comparison Condition 1.1 Old Desc.Condition 1.2 Generated Desc.Condition
Table 3: App Download Rates (ADR)
#
2.1 Malware w/ old desc.
2.2
Leakage w/ old desc.
2.3
Clean w/ old desc.
2.4 Malware w/ new desc.
Leakage w/ new desc.
2.5
2.6
Clean w/ new desc.
ADR
63.4%
80.0%
71.1%
24.7%
28.2%
59.3%
and a total of 2865 ratings. Figure 10 shows the readability rat-
ings of 100 apps for Condition 1.1 and 1.2. For our automati-
cally created descriptions, the average readability rating is 3.596
while over 80% readers give a rating higher than 3. As a compar-
ison, the average rating of the original ones is 3.788. This indi-
cates our description is readable, even compared to texts created by
human developers. The ﬁgure also reveals that the readability of
human descriptions are relatively stable while machine-generated
ones sometimes bear low ratings. In a further investigation, we no-
tice that our descriptions with low ratings usually include relatively
technical terms (e.g., subscriber ID) or lengthy constant string pa-
rameters. We believe that this can be further improved during post-
processing. We discuss this in Section 7.2.
Table 3 depicts experimental results for Condition 2.1 - 2.6. It
demonstrates the security impact of our new descriptions. We can
see a 38.7% decrease of application download rate (ADR) for mal-
ware, when the new descriptions instead of old ones are presented
to the participants. We believe that this is because malware au-
thors deliberately provide fake descriptions to avoid alerting vic-
tims, while our descriptions can inform users of the real risks. Sim-
ilar results are also observed for privacy-breaching benign apps,
whose original descriptions are not focused on the security and pri-
vacy aspects. On the contrary, our descriptions have much less im-
pact on the ADR of clean apps. Nevertheless, they still raise false
alarms for 11.8% participants. We notice that these false alarms
result from descriptions of legitimate but sensitive functionalities,
such as accessing and sending location data in social apps. A possi-
ble solution to this problem is to leverage the “peer voting” mecha-
nism from prior work [23] to identify and thus avoid documenting
the typical benign app behaviors.
6.3 Effectiveness of Behavior Mining
Next, we evaluate the effectiveness of behavior mining. In gen-
eral, we have discovered 109 signiﬁcant behaviors involving 109
sensitive APIs, via subgraph mining in 2069 SBGs of 1000 Android
apps. Figure 11 illustrates the sizes of the identiﬁed subgraphs and
shows that one subgraph contains 3 nodes on average. We further
study these pattern graphs. As presented in Table 1, they effectively
reﬂect common program logics and programming conventions.
Furthermore, we reveal that the optimal patterns of different APIs
are extracted using distinctive support threshold values. Figure 12
depicts the distribution of selected support thresholds over 109 APIs.
It indicates that a uniform threshold cannot guarantee to produce
satisfying behavior pattern for every API. This serves as a justiﬁca-
tion for our “API-oriented” behavior mining.
To show the reduction of description size due to behavior min-
ing, we compare the description sizes of raw SBGs and compressed
ones. We thus produce descriptions for 235 randomly chosen apps,
before and after graph compression. The result, illustrated in Fig-
ure 13, depicts that for over 32% of the apps, the scripts derived
from compressed graphs are shorter. The maximum reduction ratio
reaches 75%. This indicates that behavior mining effectively helps
produce concise descriptions.
6.4 Runtime Performance
We evaluate the runtime performance for 2851 apps. Static pro-
gram analysis dominates the runtime, while the description gener-
ation is usually fairly fast (under 2 seconds). The average static
analysis runtime is 391.5 seconds, while the analysis for a majority
(80%) of apps can be completed within 10 minutes. In addition,
almost all the apps (96%) are processed within 25 minutes. Notice
that, though it may take minutes to generate behavior graphs, this
is a one-time effort, for a single version of each app. Provided there
exists a higher requirement on analysis latency, we can alternatively
seek more speedy solutions, such as AppAudit [35].
7. DISCUSSION
7.1 Evasion
The current implementation of DESCRIBEME relies on static
program analysis to extract behavior graphs from Android bytecode
programs. However, bytecode-level static analysis can cause false
negatives due to two reasons. First, it cannot cope with the usage
of native code as well as JavaScript/HTML5-based programs run-
ning in WebView. Second, it cannot address the dynamic features
of Android programs, such as Java reﬂection and dynamic class
loading. Thus, any critical functionalities implemented using these
techniques can evade the analysis in DESCRIBEME.
Even worse is that both benign and malicious app authors can in-
tentionally obfuscate their programs, via Android packers [2,4,42],
in order to defeat static analysis. Such packers combine multiple
dynamic features to hide real bytecode program, and only unpack
and execute the code at runtime. As a result, DESCRIBEME is not
able to extract the true behaviors from packed apps.
However, we argue that the capability of analysis technique is or-
thogonal to our main research focus. In fact, any advanced analysis
tools can be plugged into our description generation framework.
In particular, emulation-based dynamic analysis, such as Copper-
Droid [33] or DroidScope [37], can capture the system-call level
runtime behaviors and therefore can help enable the description of
the dynamic features; symbolic execution, such as AppIntent [39],
can facilitate the solving of complex conditions.
Improvement of Readability
7.2
There exists room to improve the readability of automatically
generated descriptions. In fact, some of the raw text is still techni-
cal to the average users. We hope higher readability can be achieved
by post-processing the generated raw descriptions. That is, we
may combine natural language processing (NLP) and natural lan-
guage generation (NLG) techniques to automatically interpret the
“raw” text, select more appropriate vocabulary, re-organize the sen-
tence structure in a more smooth manner and ﬁnally synthesize a
more natural script. We may also introduce experts’ knowledge
or crowd-sourcing and leverage an interactive process to gradually
reﬁne the raw text.
8. RELATED WORK
Software Description Generation. There exists a series of stud-
ies on software description generation for traditional Java programs.
Sridhara et al. [30] automatically summarized method syntax and
function logic using natural language. Later, they [32] improved
the method summaries by also describing the speciﬁc roles of method
parameters. Further, they [31] automatically identiﬁed high-level
abstractions of actions in code and described them in natural lan-
guage.In the meantime, Buse [11] leveraged symbolic execution
and code summarization technique to document program differ-
ences. Moreno et al. [25] proposed to discover class and method
stereotypes and use such information to summarize Java classes.The
goal of these studies is to improve the program comprehension for
Figure 11: Subgraph Sizes
Figure 12: Optimal Support Thresholds
Figure 13: Size Reductions
developers. As a result, they focus on documenting intra-procedural
program logic and low-level code structures. On the contrary, DE-
SCRIBEME aims at helping end users to understand the risk of An-
droid apps, and therefore describes high-level program semantics.
Text Analytics for Android Security. Recently, efforts have
been made to study the security implications of textual descriptions
for Android apps. WHYPER [26] used natural language processing