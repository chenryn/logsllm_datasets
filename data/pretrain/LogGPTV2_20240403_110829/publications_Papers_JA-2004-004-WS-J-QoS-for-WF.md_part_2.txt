1992) since they are more structured, predictable, and repetitive. Production workflows
involve complex and highly-structured processes, whose execution requires a high
number of transaction accessing different information systems. These characteristics
allow the construction of adequate QoS models for workflow tasks. In the case of ad hoc
workflows, the information, the behavior, and the timing of tasks are largely
unstructured, which makes the procedure of constructing a good QoS model more
difficult and complex.
4.2 Workflow QoS Model
Quality of service can be characterized according to various dimensions. We have
investigated related work to decide which dimensions would be relevant to compose our
QoS model. Our research targeted two distinct areas: operations management for
organizations and quality of service for software systems. The study of those two areas is
7
important, since workflow systems are widely used to model organizational business
processes, and workflow systems are themselves software systems.
On the organizational side, Stalk and Hout (1990) and Rommel et al. (1995)
investigated the features with which successful companies assert themselves in
competitive world markets. Their results indicated that success is related to the capability
to compete with other organizations, and it is based upon three essential pillars: time,
cost, and quality. Kobielus (1997) suggests that these dimensions should constitute the
criteria that workflow systems should include and might benefit from. On the software
system side, Frolund and Koistinen present a set of practical dimensions for distributed
object systems’ reliability and performance, which include TTR (time to repair), TTF
(time to failure), and availability. Chung et al., (2000) present a framework, a set of tools,
and methodology to make system design decisions based on analysis non-functional
requirements.
Based on previous studies and our experience in the workflow domain, we have
constructed a QoS model composed of the following dimensions: time, cost, and
reliability. QoS specifications are set for task definitions. Based on this information, QoS
metrics are computed for workflows (see section 6).
4.3 Task Time
Time is a common and universal measure of performance. The philosophy behind a time-
based strategy usually demands that businesses deliver the most value as rapidly as
possible. Shorter workflow execution time allows for a faster production of new products,
thus providing a competitive advantage.
The first measure of time is task response time (T). Task response time corresponds to
the time an instance takes to be processed by a task. The task response time can be
broken down into two major components: delay time and process time. Delay time (DT)
refers to the non-value-added time needed in order for an instance to be processed by a
task. This includes, for example, the instance queuing delay and the setup time of the
task. While, those two metrics are part of the task operation, they do not add any value to
it. Process time (PT) is the time a workflow instance takes at a task while being
processed; in other words, it corresponds to the time a task needs to process an instance.
Therefore, task response time for a task t can be computed as follows:
T(t) = DT(t) + PT(t)
The delay time can be further broken down into queuing delay and setup delay.
Queuing delay is the time instances spend waiting in a tasklist, before the instance is
selected for processing. Setup delay is the time an instance spends waiting for the task to
be set up. Setup activities may correspond to the warming process carried out by a
machine before executing any operation, or to the execution of self-checking procedures.
Another time metric that may be considered to integrate with the delay time is the
synchronization delay, which corresponds to the time a workflow instance waits for other
instances in an and-join task (synchronization). In our QoS model, this metric is not part
of the task response time. This is because the algorithm we use to estimate workflow QoS
8
can derive this metric directly from the workflow structure and from the task response
time. This will become more clear when we describe workflow QoS computation.
Breaking task response time into various pieces is important since it gives a more
detailed model to be used by business analysts. Each piece correspond to an important
attribute that needs to be analyzed and should not be overlooked. In many situations the
different attributes are set by different people.
4.4 Task Cost
Task cost represents the cost associated with the execution of workflow tasks. During
workflow design, both prior to workflow instantiation and during workflow execution, it
is necessary to estimate the cost of the execution in order to guarantee that financial plans
are followed. The cost of executing a single task includes the cost of using equipment, the
cost of human involvement, and any supplies and commodities needed to complete the
task. The following cost functions are used to compute the cost associated with the
execution of a task.
Task cost (C) is the cost incurred when a task t is executed; it can be broken down
into two major components: enactment cost and realization cost.
C(t) = EC(t) + RC(t)
The enactment cost (EC) is the cost associated with the management of the workflow
system and with the monitoring of workflow instances. The realization cost (RC) is the
cost associated with the runtime execution of the task. It can be broken down into: direct
labor cost, machine cost, direct material cost, and setup cost. Direct labor cost is the cost
associated with the person carrying out the execution of a workflow human task (Kochut,
Sheth et al. 1999), or the cost associated with the execution of an automatic task with
partial human involvement. Machine cost is the cost associated with the execution of an
automatic task. This can correspond to the cost of running a particular piece of software
or the cost of operating a machine. Direct material cost is the cost of the materials,
resources, and inventory used during the execution of a workflow task. Setup cost is the
cost to set up any resource used prior to the execution of a workflow task.
The EC and RC captures the distinction between the running costs of the workflow
system deployment, operation, maintenance and monitoring vs. the costs associated with
the execution of tasks.
4.5 Task Reliability
To model the reliability dimension of workflows, we have used concepts from system
and software reliability theory (Hoyland and Rausand 1994; Ireson, Jr. et al. 1996; Musa
1999). The reliability analysis of systems often uses reliability block diagrams (RBD) as
a representation of how the components of a system are connected. Elementary
configurations of a RBD include the series and parallel configurations. Our approach is to
create a mapping between RBD and workflow structures. This allows us to view a
workflow as a system of independent components which can be then modeled and
9
analyzed using similar functions applied to RBD. The first step is to model the reliability
of an individual task.
Task reliability (R) models what can be considered the most important class of
workflow failures, task failures (Eder and Liebhart 1996) (also known as activity
failures). Task failures can be organized into two main classes: system failures and
process failures ((Eder and Liebhart) calls this second type of failures, semantic failures).
System failures. These consist of information technology and software failures which
lead to a task terminating abnormally. Information technology and software include
operating systems, communication protocols, hardware, etc. For example, a task manager
is not able to contact its task because the CORBA server managing the task has failed due
to a system breakdown is a system failure.
Process failures. These consist of business process exceptions which lead to an
anomalous termination of a task. In a workflow, task structure (Krishnakumar and Sheth
1995) has an initial state, an execution state, and two distinct terminating states. For non-
transactional tasks, one of the terminating states indicates that a task has failed, while the
other state indicates that a task is done (Figure 2). For transactional and open 2PC tasks,
the terminating states are aborted and committed. The model used to represent each task
indicates that only one starting point exists when performing a task, but two different
states can be reached upon its execution. For example, a database access task fails
because of an invalid user password. The task enters the aborted state.
Figure 2 - Two task structures (Krishnakumar and Sheth 1995)
To describe task reliability we follow a discrete-time modeling approach. We have
selected this solution since workflow task behavior is most of the time characterized in
respect to the number of executions. Discrete-time models are adequate for systems that
respond to occasional demands, such as database systems (i.e, discrete-time domain).
This dimension follows from one of the popular discrete-time stable reliability models
proposed in (Nelson 1973) and it is shown below.
R(t) = 1 – (system failure rate + process failure rate)
System failure rate is the ratio between the numbers of time a task did not perform for
its users and the number of times the task was called for execution, i.e. #(unsuccessful
executions)/#(called for execution). Process failure rate provides information concerning
the relationship between the number of times the state done/committed is reached and the
10
number of times the failed/aborted state is reached after the execution of a task (see the
task model structure shown in Figure 2). It is calculated using the formula #(failed or
aborted)/(#(failed or aborted) + #(done or commit)).
Alternatively, continuous-time reliability models can be used when the failures of the
malfunctioning equipment or software can be expressed in terms of times between
failures, or in terms of the number of failures that occurred in a given time interval. Such
reliability models are more suitable when workflows include tasks that control equipment
or machines that have failure specifications determined by the manufacturer. Ireson, Jr et
al. (1996) presents several software reliability models which can be used to model this
QoS dimension. The ideal situation would be to associate with each workflow task a
reliability model representing its working behavior. While this is possible, we believe that
the common workflow system users do not have enough knowledge and expertise to
apply such models.
5 Creation of QoS Estimates
In order to facilitate the analysis of workflow QoS, it is necessary to initialize task QoS
metrics and also initialize stochastic information which indicates the probability of
transitions being fired at runtime. Once tasks and transitions have their estimates set,
algorithms and mechanisms, such as simulation, can be applied to compute overall
workflow QoS.
5.1 Creation of QoS Estimates for Tasks
Having previously defined the QoS dimensions for tasks, we now target the estimation of
QoS metrics of tasks. The specification of QoS metrics for tasks is made at design time
and re-computed at runtime, when tasks are executed. During the graphical construction
of a workflow process, the business analyst and domain expert set QoS estimates for each
task. The estimates characterize the quality of service that the tasks will exhibit at
runtime.
Setting initial QoS metrics for some workflow tasks may be relatively simple. For
example, setting the QoS for a task controlling a DNA sequencer can be done based on
the time, cost, and reliability specifications given by the manufacturer of the DNA
sequencer. In other cases, setting initial QoS metrics may prove to be difficult. This is the
case for tasks that heavily depend on user input and system environment. For such tasks,
it is convenient to study the workflow task based on real operations. The estimates are
based on data collected while testing the task. The idea is to test the task based on
specific inputs. This can be achieved by the elaboration of an operational profile (Musa
1993). In an operational profile, the input space is partitioned into domains, and each
input is associated with a probability of being selected during operational use. The
probability is employed in the input domain to guide input generation. The density
function built from the probabilities is called the operational profile of the task. At
runtime, tasks have a probability associated with each input. Musa (1999) described a
detailed procedure for developing a practical operational profile for testing purposes.
11
The task runtime behavior specification is composed of two classes of information
(Table 1): basic and distributional. The basic class associates with each task’s QoS
dimension the minimum value, average value, and maximum value the dimension can
take. For example, the cost dimension corresponds to the minimum, average, and
maximum cost associated with the execution of a task. The second class, the
distributional class, corresponds to the specification of a constant or of a distribution
function (such as Exponential, Normal, Weibull, and Uniform) which statistically
describes task behavior at runtime. In some situations it may not be practical to derive a
distribution function, an alternative is to sample the distribution and specify it in the form
of a histogram rather than an analytical formula. For example, Table 1 and Table 2 show
the QoS dimensions for an automatic task (the SP FASTA task) and for a manual task (the
Prepare Sample task; see section 3.2 for tasks descriptions).
Basic class Distributional class
Min value Avg value Max value Dist. Function
Time 0.291 0.674 0.895 Normal(0.674, 0.143)
Cost 0 0 0 0.0
Reliability - 100% - 1.0
Table 1 – Task QoS for an automatic task
Basic class Distributional class
Min value Avg value Max value Dist. Function
Time 192 196 199 Normal(196, 1)
Cost 576 576 576 576.0
Reliability - 100% - 1.0
Table 2 – Task QoS for a manual task
The values specified in the basic class are typically employed by mathematical
methods in order to compute workflow QoS metrics, while the distributional class
information is used by simulation systems to compute workflow QoS (Chandrasekaran,
Silver et al. 2002; Miller, Cardoso et al. 2002). To devise values for the two classes, the
designer typically applies the functions presented in the previous section to derive the
task’s QoS metrics. We recognize that the specification of time, cost, and reliability is a
complex operation, which when not carried out properly can lead to the specification of
incorrect values. Additionally, the initial specification may not remain valid over time.
To overcome this difficulty, a task’s QoS values can be periodically re-computed for the
basic class, based on previous executions. The distributional class may also need to have
its distribution re-computed. At runtime, the workflow system keeps track of actual
values for the QoS dimensions monitored. QoS runtime metrics are saved and used to re-
compute the QoS values for the basic class which were specified at design time. The
workflow system re-computes the QoS values for each dimension; this allows the system
to make more accurate estimations based on recent instance executions.
12
The re-computation of QoS task metrics is based on data coming from designer
specifications and from the workflow system log. Depending on the workflow data
available, four scenarios can occur: a) For a specific task t and a particular dimension
Dim, the average is calculated based only on information introduced by the designer
(Designer Average (t)); b) the average of a task t dimension is calculated based on all
Dim
its executions independently of the workflow that executed it (Multi-Workflow
Average (t)); c) the average of the dimension Dim is calculated based on all the times
Dim
task t was executed in any instance from workflow w (Workflow Average (t, w)); and
Dim
d) the average of the dimension of all the times task t was executed in instance i of
workflow w (Instance Average (t, w, i)). Scenario d) can only occur when loops exist in
Dim
a workflow.
While the formulae presented only show how to compute average metrics, similar
formulae are used to compute minimum and maximum values.
The task QoS for a particular dimension can be determined at different levels; it is
computed following the equations described in Table 3.
a) QoS (t) = Designer Average (t)
Dim Dim
b) QoS ’(t) = wi * Designer Average (t) + wi * Multi-Workflow Average (t)
Dim 1 Dim 2 Dim
c) QoS (t, w) = wi * Designer Average (t) + wi * Multi-Workflow Average (t) +
Dim 1 Dim 2 Dim
wi *Workflow Average (t, w)
3 Dim
d) QoS (t, w, i) = wi * Designer Average (t) + wi * Multi-Workflow Average (t) +
Dim 1 Dim 2 Dim
wi * Workflow Average (t, w) + wi * Instance Workflow
3 Dim 4
Average (t,w, i)
Dim
Table 3 – QoS dimensions computed at runtime
The workflow system uses the formulae from Table 3 to predict the QoS of tasks. The
weights wi are set manually. They reflect the degree of correlation between the
k
workflow under analysis and other workflows for which a set of common tasks is shared.
The different equations are used based on the historical data available from past
executions of tasks and workflows. For example, if the workflow system does not have
any historical data in its log describing the QoS metrics of task t , then the equation a)
n
will be used to predict a QoS model for task t . In the other hand, if the workflow system
n
log’s contains historical data describing the QoS metrics of task t , then equation b), c)
n
and d) will be used to predict QoS metrics. The section of an equation depends on how
much data is available.
Let us assume that we have an instance i of workflow w running and that we desire to
predict the QoS of task t ∈w. The following rules are used to choose which formula to
apply when predicting QoS. If task t has never been executed before, then formula a) is
chosen to predict task QoS, since there is no other data available. If task t has been
executed previously, but in the context of workflow w , and w != w , then formula b) is
n n
chosen. In this case we can assume that the execution of t in workflow w will give a
n
good indication of its behavior in workflow w. If task thas been previously executed in
the context of workflow w, but not from instance i, then formula c) is chosen. Finally, if
13
task thas been previously executed in the context of workflow w, and instance i, meaning
that a loop has been executed, then formula d) is used.
5.2 Probabilities Estimates for Transitions
In the same way we seed tasks’ QoS, we also need to seed workflow transitions. Initially,
the designer sets the transition probabilities at design time. At runtime, the transitions’
probabilities are re-computed. The method used to re-compute the transitions’
probabilities follows the same lines of the method used to re-compute tasks’ QoS. When
a workflow has never been executed, the values for the transitions are obviously taken
from initial designer specifications. When instances of a workflow w have already been
executed, then the data used to re-compute the probabilities come from initial designer
specifications for workflow w, from other executed instances of workflow w, and if
available, from the instance of workflow w for which we wish to predict the QoS. This