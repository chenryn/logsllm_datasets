CVE, authentication/VPN/ScreenOS vulnerability, backdoor
phishing, data breach, ransomware, cryptomining, email-served malware
smartwatch/DNS/authentication vulnerability, privacy issues, firmware security
zero-day vulnerability, chip flaw, CVE
DoS attack, security log data loss
password stealing, phishing, the man-in-the-middle attack
vulnerability, zero-day threat, domain fraud, indicators of compromise
Gh0st remote access Trojan, PowerRatankba, WannaCry ransomware attack
was accompanied with five multiple-choice questions about the
conceptual understanding of technical terms. Users who can an-
swer our questions correctly are considered to be knowledgeable
about security threats and their corresponding solutions. They are
considered to be more sensitive and aware of security risks with
daily used software applications (e.g., Microsoft Office) or smart
devices (e.g., smartwatches). When they face an attack, they are
also more likely to purchase security products to protect their au-
thentication or their cloud environment even without additional
professional instructions. Our supplementary document6 explains
how users can learn security knowledge to improve their security
awareness from each article.
Each task included 15 questions about Security Threats and
Security Protection, as shown in Table 1. The two categories are
explained as follows:
Security Threats: These questions required participants to com-
prehend the threats described in the article, such as malicious ac-
tivity, attack, vulnerability and data breach incident. There were
two subcategories for these questions: meaning and function un-
derstanding. Meaning understanding required users to select the
correct attack from a set of options based on the definition, while
the other options provided some similar attacks as wrong answers.
The other category tested if participants understood how attacks
worked. For example, the user could be asked to select the core
technique (algorithm) to exploit a given vulnerability.
Security Protection: These questions referred to defensive so-
lutions against attacks, such as cloud security services or two-factor
authentication (2FA). The subcategories were similar to those of
Security Threats. Moreover, we listed simulated real-world cases
for selection. For instance, we asked the users to select the possible
cases of 2FA (e.g., the case of using the password and one-time code
sent through SMS); “password only” was among the wrong options.
Our questionnaire7 included six tests, providing two versions
(i.e., plain text and the text with our pop-ups) for each of the three
tasks. The participants were assigned the tasks, and the experi-
mental group was provided with the pop-up meanings (test 1-3).
The control group was only given plain texts in test 4-6. How-
ever, they were free to use any tools or search engines such as
Google Dictionary [22] and Google Search. After each submitted
the answer sheet, the accuracy and the time spent on the task were
automatically calculated and displayed. Each task was designed to
be completed by three participants with IT background, and three
without IT background. Before launching the experiment, a pilot
6https://ktd4869.github.io/Reading_Test_MT/Survey2_explanations.pdf
7https://ktd4869.github.io/Reading_Test_MT/
Figure 9: An example of a definition in a pop-up when hov-
ering over the term ‘distributed denial of service’.
was compatible with major browsers including Chrome, IE, Fire-
fox and Safari. A screenshot of the tool while describing the term
‘distributed denial of service’ is shown in Fig.9.
The articles were firstly tokenised and split into sentences by
the Stanford Core NLP [33]. We detected the technical terms in
the articles by one-to-one mapping after lemmatisation based on
the dictionary. Appositions of technical terms detected by depen-
dency extraction were also considered technical. Abbreviations
were detected in our extension. For example, ‘Two-Factor Authenti-
cation’ was in our dictionary, but ‘2FA’ was not. However, the tool
recognised and treated 2FA similarly.
The detected terms were then highlighted and became clickable
by adding the ‘a’ HTML tags. The properties of the tags were also
set to point to the descriptions of the corresponding terms through
our designed API connection.
4.2 Evaluation
To evaluate the effectiveness of our tool, we conducted a subjective
study of users’ understanding of technical articles when using our
tool and other methods (e.g., Google Search, pop-up based Google
Dictionary [22]).
4.2.1 Questionnaire Design and Implementation. We designed a
questionnaire to measure how well the participants understood the
technical articles. The evaluation method was adopted from [8].
We selected nine articles from the original dataset, excluding the
200 previously-used articles. The selected articles were distinguish-
able as they addressed different cybersecurity problems. They were
further grouped into three reading tasks randomly. The security
threats addressed in these tests are listed in Table 1. Each article
study was also conducted by three people with IT background and
another three people without IT background to test the suitability
and difficulty of the questionnaire.
4.2.2 Experiment Procedure. We invited the individuals who par-
ticipated in our IT background test published in MTurk and divided
them in to experiment and control groups. Like in the first experi-
ment, users were considered having IT background if they achieved
at least an undergraduate degree in IT or 1-year related working ex-
perience. We then published our questionnaire in MTurk, with the
same amount of assignments released for the link of each test. Each
participant was allocated one link randomly, and the completion
was rewarded 2 U.S. dollars.
Before running the experiment, we gave a short demo and showed
the experimental group (the users who were going to use our tool)
how to use our tool. They were asked to answer the questions only
with the pop-up meanings generated by our tool. We also showed
the control group how to use other methods including a series of
search engines (e.g., Google, Wiki) and similar dictionary-based
tools (e.g., Wikipedia Page Previews [59], Google Dictionary [22]
and Mac Dictionary [4]).
The participants were then required to click the external link to
our reading test. Detailed instructions were provided at the starting
page. A timer started once they clicked the ‘start’ button. The time
consumed and the accuracy achieved were displayed after the task
was completed, and the ‘submit’ button was clicked.
After completing the questionnaire, the participants were asked
to leave feedback from this experience. The experimental group
was required to provide suggestions regarding our tool and if they
would like to always have it. The control group was asked about
the methods they used, whether they were useful or not, and if they
would like a tool to help. We used open card sorting [56] again to
group the suggestions from the participants. We also classified them
into positive and negative comments, so that we could identify the
helpful and useless features of our tool for improvement.
We read all the responses and rejected invalid answers, such as
empty ones or responses from people who spent less than 10 min-
utes. The pilot study showed each task cost at least 15 minutes. We
read their feedback to inspect if they completed our tasks carefully.
In total, we collected 112 valid answers. We further divided the
results into IT and non-IT groups, and all the 12 groups had valid
responses (mean = 9.3, std = 4.1).
4.2.3 Experiment Results. Now, we show the results of the user
study to answer our three research questions.
RQ5. Efficiency
Based on [8], we measured the accuracy of participants’ answers
to the multiple-choice questions and the time spent on those. The
accuracy was defined as the percentage of correct answers. Each
question only had one correct choice. Our questionnaire computed
the accuracy and the time spent automatically after submission.
A higher accuracy indicates a better understanding of the articles.
Also, we assume that shorter answer times (at equal accuracy) imply
better comprehension of the contents.
In Fig.10, the violin plots represent the distributions of accuracy
and time spent on questionnaire completion for the three reading
tasks, where each subplot represents a comparison between the
Figure 10: The accuracy and time spent on questionnaire
completion for three reading tasks by using our tool or other
methods such as Google search.
results by using our tool and other methods (i.e., search engines
and other dictionary-based tools). We can see that the accuracy
achieved using our tool is significantly higher than that of other
methods. With other methods, the control group could only answer
around 40% of the questions correctly. This number increases by
around 30% in the experimental group for all the three reading
tasks. However, the best average accuracy achieved does not go
beyond 67%, which means only 10 out of 15 questions are answered
correctly. It indicates the difficulty for users to understand security
issues comprehensively only by reading technical articles.
The distributions of the time spent on questionnaire completion
are demonstrated in the right subplot of Fig.10. We find the average
time spent by the experimental group is longer than that of the con-
trol group for all the three reading tasks, with 24, 29 and 26 minutes
compared to 19, 21 and 19 minutes. Time used by the control group
is mainly distributed in two areas with 10 to 20-minute difference of
the upper and lower adjacent values. Some users may stop reading
and do a web search for definitions. It took more time to open mul-
tiple tabs and to find the definitions of technical jargons, especially
some ambiguous terms (e.g., host) which have numerous meanings.
The smaller amount of time consumed by the control group may
result from being impatient and skipping some terms, which could
be the keywords in the articles. The three to five-minute difference
in time between the two groups is not significant.
We further explored the significance of the difference between
the accuracy and the time spent by using our tool as well as other
methods. We applied a t-test to measure the difference of accuracy
and time spent between the experimental group and the control
group. The result shows that the accuracy in each reading task
assisted by our tool shows a significant difference to that done by
other methods (p < 0.001). It implies that our tool can significantly
improve users’ understanding of the technical articles, while the
difference in time spent is not significant between the two groups.
From the feedback of the control group, we find around 61%
people in this group did not use any search engine or tool because
it would take much longer or they believed they could find the
correct answers based on their knowledge. The rest of the group
mentioned they used Google or pop-up dictionaries to find the
meanings, but most of them only searched for a few terms and felt
the searches were not useful for these jargon-laden articles. Only
one participant felt that, while Google was helpful, it would be
better to have a tool to provide the definitions by hovering over
unfamiliar words. Overall, 80% of participants in the control group
preferred to have a tool to support them.
Analysing the comments from the experimental group can help
us understand how useful the tool is. We extracted the keywords
and grouped the comments to explore their satisfaction regarding
the tools. We also reviewed the comments to find what features
of our tool were useful to help them understand the articles. From
the feedback, we find that all participants in this group deemed
the pop-up meanings useful. It helped them understand the articles
better and faster. Some participants expressed their satisfaction as:
• “They were very useful as the definition was in depth.”
• “I think that pop-up meanings are useful and convey the meaning
• “Yes, the pop-up meanings are VERY useful! They enabled me to see
the meanings of words and phrases that I didn’t recognise without
having to stop reading and do a web search for the definitions. It
lets the user actually read the article without stopping to puzzle
things out since the pop-ups can be seen as actually being part of
the article. They helped tremendously in understanding the article
itself.’
• “They are very useful especially when you don’t understand the
accurately.”
keywords or when you get confused.”
There are also some suggestions to help us improve our tool in
future work:
• “I felt like they only should have popped up the first time you saw
them. Otherwise, the articles got kind of cramped and wordy. Other
than that, they were very useful in explaining exactly what the
terms meant.”
• “They were useful, but I found it difficult to see them entirely. They
• “They are useful to an extent but the limited windows are annoying
when I want more information than what fits in the pop window.”
got cut off at the bottom and I saw no way to scroll.”
RQ6. Influential Factors
We further reviewed the questions where our participants got
the wrong answers to see what factors influence the accuracy. Po-
tential factors could be the content or question type. The content
contained different security issues, including both threats and pro-
tection mechanisms. Our questionnaire consisted of both positive
and negative questions. We analysed the questions with the wrong
answers to find the security issues that are hard to understand. We
calculated the frequency of each question wrongly answered in
each reading task. If more than a half, we consider it error-prone.
We did this calculation for all such questions in the three tasks.
From the results, we find that the number of error-prone ques-
tions answered by the control group using other methods is around
twice as many as those in the experimental group. We hypothe-
sised that, with the help of our tool, some questions could be easier
to answer, especially the conceptual questions for both threats
and protection categories. These two include the definitions of
specific attacks (e.g., the core algorithms, involved platforms) and
protection-related subjects (e.g., relevant techniques/services, de-
velopment team, practical use cases). However, we find that several
questions are error-prone even with the help of our tool. For in-
stance, some questions require the user to understand the meaning
Figure 11: The accuracy and time spent on questionnaire
completion of participants with different IT backgrounds.
of an attack under a condition. People need preliminary knowledge
of the field to understand such technical articles to some extent.
Although each detected term was provided with meanings, partici-
pants were confused with multiple similar terms.
RQ7. IT Knowledge Effect
We compared the difference in the accuracy and time spent
on questionnaire completion between participants with different
IT backgrounds. A Mann-Whitney U test was used to see if the
difference is significant between the results of people with different
IT backgrounds. We also made a separate comparison between the
experimental group and the control group.
Fig.11 presents the violin plots of the accuracy and the time spent
between participants with different IT backgrounds. For people with
IT background, the average accuracy is around 90% when they use
our tool, which is 20% higher than people without IT background.
The accuracy drops to 45% when people with IT background use
other methods, and the accuracy is similar to people without IT
background. We also find that people with IT background read
technical articles more slowly than people without IT background,
an average of 27 minutes compared with 20 minutes (by using
our tool), and 17/32 minutes compared with 17 minutes (by using