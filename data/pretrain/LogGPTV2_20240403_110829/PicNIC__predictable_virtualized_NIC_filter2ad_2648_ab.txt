a
l
e
d
e
t
a
R
)
s
(
)
s
p
p
M
(
T
T
R
g
n
P
i
)
s
m
(
2
1
0
2
1
0
1
0
50
25
0
Throttled UDP flow starts
Delay increases and
throughput decreases
for unthrottled flows
0
10
20
30
40
Time (s)
99th perc.
Median
99th perc.
Median
Throughput
Ping RTT
50
60
70
Figure 2: HoL blocking and isolation breakage at egress.
)
s
p
p
M
(
e
t
a
R
0.3
0.2
0.1
0.0
4000
)
s
m
(
T
T
R
2000
0
10:50
)
s
p
b
G
(
e
t
a
R
20
15
10
5
0
)
s
m
(
T
T
R
600
400
200
0
04:11
VM1
VM2
04:26
04:41
04:56
Time (hh:mm)
VM1
VM2
VM3
11:05
11:20
11:35
Time (hh:mm)
(a) Packet rate overload
(b) Bandwidth overload
Figure 3: Isolation breakage at ingress due to overloads.
time in the shaper queues 8(cid:2) (top plot). Due to interference in
in-order buffers 7(cid:2), the TCP flow experiences Head-of-Line (HoL)
blocking (second plot), resulting in decreased throughput (third
plot)—packets are held in the in-order buffer waiting for transmit
completions to finish in order for all flows, including rate limited
ones. RTTs for unthrottled ping from VM1 also increase during this
period (bottom plot).
The root cause of this phenomenon is that the egress buffer fills
up, either because the shaper queues 8(cid:2) (which hold packet de-
scriptors for throttled flows) or the in-order buffers 7(cid:2) (which hold
packet descriptors for both fastpath and throttled flows) are full. In
each case, faster flows can be HoL blocked because slower flows
monopolize the buffers. Note that increasing buffer size would only
delay isolation breakage, but would not prevent it. Interestingly,
even a single VM with different “flow types” can face this problem
(though it can be exacerbated by multiple VMs)—e.g., consider two
flows from the same VM, with the faster flow HoL-blocked due
to the slower one. Using separate buffers to isolate traffic would
require a buffer for each traffic class (rate limit) which is prohibi-
tively expensive. To mitigate such contentions, we need controlled
sharing of buffers and admission of packets into the stack.
2.2 Ingress NIC Contention
At the ingress, contention for NIC buffers 4(cid:2) due to an excessively
high packet rate (PPS) can also break isolation. Fig. 3a shows an
incident from production where packet bursts to VM1 lasted for
a few minutes. While VM1 receives a goodput of 300 kpps (top)
at a low bandwidth (2 Mbps) due to small packets, 148 kpps drops
occur at the NIC. The RTTs to a few unrelated VMs on the same
host increase by more than 100× due to interference (bottom), even
though the burst is directed at VM1.
The root cause here is that a storm of small packets overwhelms
the packet-processing capacity at the ingress engine 5(cid:2), where
353
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
processing costs are per-packet rather than per-byte [29, 45]. As
there is a limit on the CPU cores available for packet processing, the
PPS capacity is bounded (the same is true for HW-based stacks [31,
53]). In particular, the NIC buffers fill up when the engine 5(cid:2) cannot
process packets as fast as they arrive—e.g., when a VM is under a
DoS attack—saturating the ingress engine with a deluge of small
packets that it is unable to drain quickly from NIC queues 4(cid:2). The
overflowing NIC queues in turn leave little room for other traffic,
resulting in tail-drops at the ingress NIC and breaking isolation
between VMs.
2.3 Ingress Engine Contention
Finally, contention for resources in the rest of the ingress engine
due to a large spike of traffic (BPS) can also break isolation. Fig. 3b
shows such an incident where VM2 receives a burst of ∼16 Gbps
from a production service. On the same host, VM1 is receiving at a
low BPS of 20 Mbps. In addition to 186 kpps drops in the ingress
engine, RTTs for VM1’s traffic increase even though its workload
has not changed during the period.
Even if the engine drains the NIC queues 4(cid:2) quickly, packets may
encounter bottlenecks in transit to VM queues 6(cid:2). Such bottlenecks
may arise for various reasons including cache misses, excessive
flow table lookups, and even the inability of VM queues to absorb
the spike. These bottlenecks can cause packets to be buffered and
eventually dropped after wasting CPU cycles—this harms isolation
and is also unfair. So, in addition to arbitrating engine resources,
we need admission control to mitigate contention.
2.4 Extent of Isolation Breakages
In summary, even if we assume no contention within the network
fabric, there are several ways in which a VM can adversely impact
the network experience of other VMs. These are not one-off in-
cidents; we found that their frequency is correlated with packet
drop rates at the ingress and HoL blocking latency at the egress.
We counted the number of 1-second intervals at each host when
the ingress drop rate exceeds 10 kpps in NIC Rx and found the
cumulative count over a fleet of servers to be tens of thousands
per day. As each such incident on a host can potentially impact
the isolation experience of tenants, the problem can become severe
since these incidents are not uniformly distributed over the fleet.
Even if we can provision resources to make isolation breakages
rare in the common case, the lack of isolation mechanisms creates
a soft target for disruptive traffic such as during DoS attacks; we
want to eliminate this target.
3 Design Principles
Based on our analysis (§2), we come away with two guiding princi-
ples for providing predictable performance to VMs:
P1. SLO-based resource sharing: for predictability, packet-processing
resources utilized for each VM should be proportional to per-
formance SLOs. In particular, sufficient resources within the
virtualization stack (e.g., CPU cycles, NIC and PCIe bandwidth,
shared buffers at shaper and NIC ingress) should be allocated
to ensure the minimum guarantees of the SLO (e.g., bandwidth)
independent of the behavior of other VMs. More generally, we
prioritize SLO-compliance over work-conserving behavior.
354
2.0
1.5
1.0
0.5
0.0
)
s
p
p
M
(
s
p
o
r
D
C
N
I
Before
After
0
100
Time (s)
200
10
8
6
4
2
0
)
S
P
R
k
(
t
u
p
h
g
u
o
r
h
T
Load
)
s
μ
(
y
c
n
e
t
a
L
106
105
104
103
102
101
100
Before
After
Mean
99.9th
Figure 4: Example: PicNIC ensures predictable performance.
P2. Backpressure and early drops: for efficiency, if a packet needs
to be dropped or queued, it should be done as early in the pro-
cessing pipeline as possible by applying apt backpressure—e.g.,
packets likely to be dropped at the receiver due to insufficient
resources should not be admitted at the source.
By using SLO-based resource allocation per VM (P1), an over-
loading VM quickly builds up its own queue or has its own packets
dropped before affecting others; this facilitates early detection of
SLO violations. While P1 is necessary, it is not sufficient to ensure
predictable performance SLOs (§4) as it still allows excessive de-
lay and losses—e.g., in an incast scenario, packets destined for a
VM may be admitted to the source egress engines in excess of the
receiver VM’s SLO-based ingress capacity. These excess packets
consume resources at the egress, in the fabric, and at the ingress,
only to be dropped eventually. Such wasted work elevates the la-
tency and packet drops for colocated VMs as seen in §2.2 and also
hurts efficiency. Thus, admission control with backpressure all the
way to the source and early drops (P2) are necessary to ensure
isolation and efficiency [17, 50, 66]. We find that a combination of
these principles is sufficient to ensure predictable SLOs with high
efficiency.
We show how PicNIC (§5), a system based on these principles,
ensures predictable performance in a simple three-host setup. We
have three client-server pairs of VMs: one pair runs a latency-
sensitive request-response workload, memcached [46], and the
other two run a resource-intensive UDP job. We place all server VMs
on the same host. The memcached client, colocated with another
UDP client, generates a load of 10k requests/sec (kRPS) to the server,
while the UDP clients generate 100B packets at a high PPS. Fig. 4
shows that without PicNIC, there are 1.2×106 drops/sec at the
receiver host NIC (left), resulting in elevated latency (right) and
low throughput in terms of completed RPS for memcached (middle).
PicNIC increases throughput by an order of magnitude to 100% and
lowers the tail latency by three orders of magnitude (close to the
memcached job existing by itself) while ensuring predictably high
goodput even for the intensive UDP flows (details in §6).
4 Predictable Virtualized NICs
The designer of any network virtualization stack must consider
the performance and isolation guarantees they wish to provide [4,
16, 22]—these properties cannot be “bolted on” as an afterthought.
Ideally, every VM should have the illusion of having its own ded-
icated NIC with known performance characteristics, regardless
of the unpredictable behavior of other VMs. Our approach is to
provide an abstraction of a predictable virtualized NIC, defined by
target SLOs, to each VM. A physical NIC is characterized by certain
properties such as bandwidth, delay and loss rate [31]. Drawing an
analogy, we define a predictable vNIC along the same three SLO
PicNIC: Predictable Virtualized NIC
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Metric
Predictable vNIC SLO
Bandwidth min. and max. envelope (hose model)
Delay
Loss rate
*for well-behaved traffic in bandwidth envelope
Table 2: Abstraction: Predictable vNIC SLO metrics
Low; predictable distribution*
No drops for cooperating traffic*
dimensions of offered bandwidth, delay and loss rate. This provides
a quantifiable meaning to predictable performance and extends
the purely functional behavior of virtual NICs with SLOs. Table 2
summarizes our abstraction. While we do not include other possible
dimensions—e.g., message and packet rates—for intuitiveness, one
may extend this abstraction as needed.
By providing SLOs that apply to traffic at sources as well as
destinations, this abstraction provides the illusion of a dedicated
NIC to each VM. However, to fully understand the guarantees
ensured by this abstraction, it is important to consider the overall
mix of traffic and patterns of communication between VMs. We aim
to offer predictability for traffic mixes with a combination of shaped
and unshaped flows, as well as under ingress fan-in communication
patterns. Hence, some of the techniques proposed in PicNIC, e.g.
for sender coordination, would also apply to standalone NICs and
not just to virtualized NICs.
Bandwidth. We define bandwidth (BPS) using the hose model [18], a
natural fit for a vNIC. The abstraction is in terms of an envelope with
a minimum guarantee (MIN_BPS) for performance and a maximum
cap (MAX_BPS) for predictability. MIN_BPS is based on provisioning and
MIN_BPS for VMs on a host ≤ host NIC
is not oversubscribed—i.e.,
line rate. MAX_BPS is a cap that is not exceeded even if the VM is the
sole occupant of the host. MAX_BPS can be oversubscribed for higher
multiplexing and efficiency. If multiple VMs contend for bandwidth,
then after allocating each VM its MIN_BPS, any residual bandwidth is
shared per policy. The hose model along with BPS envelope allows
expressing a range of policies. Non-work-conserving policies, such
as MAX_BPS = MIN_BPS, can provide strict latency and loss properties,
while work-conserving policies, such as MAX_BPS per VM = NIC line
rate, can provide high utilization.

BPS SLOs are defined for traffic adhering to “standard” packet