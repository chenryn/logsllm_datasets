run Microsoft Sysmon and Elastic Auditbeat to capture advanced
audit and security log data. Windows Event Logs are collected and
forwarded to the dedicated Log Server by an Elastic Winlogbeat
agent. The Linux machines within the simulated company net-
work forward all syslogs (including firewall and proxy logs) to the
Log Server. The Company Router additionally runs Suricata and
Packetbeat and forwards their log data as well.
misc_sqlmapmisc_exfiltrationmisc_set_autostartmisc_execute_malwarec2_exfiltrationc2_download_malwaremisc_download_malwarec2_mimikatzc2_take_screenshotinfect_email_exeinfect_flashdrive_exe696ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Rafael Uetz, Christian Hemminghaus, Louis Hackländer, Philipp Schlipper, and Martin Henze
Figure 4: SOCBED collects, processes, and stores log data from best-practice sources on a dedicated log server.
The dedicated Log Server allows SOCBED users to easily search
and visualize various log data from the machines in the company
network. It runs Elastic Logstash, Elasticsearch, and Kibana to
collect, store, search, and visualize log data. Additionally, log data
can be exported in JSON format using the Elasticsearch API or the
tool Elasticdump. This allows to freeze generated log datasets and
make them available to other researchers.
Network Traffic. Although the focus of SOCBED lies on host-
based log data, network traffic can be captured as well, e.g., by
running the tool tcpdump on the Company Router. Generated and
recorded traffic can then be analyzed or replayed in subsequent
sessions using standard tools such as Tcpreplay. This setup enables
strictly reproducible experiments that are based on the contents of
network traffic, e.g., for NIDS evaluation.
Additionally, network flows and numerous higher-level events
such as HTTP requests are captured and logged by a Packetbeat
agent on the Company Router. These logs are forwarded and stored
on the log server in the same way as described for system logs
above, thus enabling a straightforward analysis of network-based
activity such as benign user activity or traffic caused by attacks.
5.4 Reproducibility and Adaptability
The overarching goal of SOCBED is to showcase the feasibility of
generating reproducible and adaptable log datasets. To achieve this
goal, we implemented all log-generating assets and their actions in
a way that allows for deterministic activity and controlled adapta-
tions, especially by relying on infrastructure-as-code, determinism
of emulation, and self-tests as detailed in the following.
Infrastructure as Code. To ensure a high level of transparency, the
initial setup of all VMs is performed using infrastructure-as-code
(IaC) methods. More specifically, we use Packer and Ansible scripts
to create, install, and configure all VMs automatically without user
interaction. Operating system images and additional software are
automatically downloaded from the Internet and then installed
and configured on the target VMs. To avoid unintended changes
in behavior, all software components are pinned to specific ver-
sions. As SOCBED scenarios are fully defined by code, a version
control system can be used to make all changes transparent and
revertible, which promotes adaptability. This approach also ensures
reproducibility because different users can build the same testbed
from scratch running the provided setup scripts.
Emulation Determinism. Targeting determinism in emulation,
we perform all activity of the user and attack emulation either
scripted or pseudo-randomly with a configurable seed. To this end,
sequences of actions are generated based on finite-state machines
and are logged for post-experiment investigation. Furthermore,
each client incorporates its ID into its seed, such that it behaves
differently from the others but equally on each testbed run, thus
making user emulation replicable. The user emulation can retrieve
websites from the Internet (for better realism) or from a web server
within the simulated network (for better reproducibility).
Self-Tests. SOCBED targets to realize replicable log data genera-
tion and provide easy adaptability. Consequently, researchers using
SOCBED need to be able to verify that their testbed instances are
working as intended both after initial installation and after making
changes. To this end, we provide a large number of unit and system
tests, which can be executed automatically using a test runner. Unit
tests check single functions for correct return values while system
tests start all VMs and verify functionality of the running testbed.
More specifically, the system tests verify correct setup of VMs, exe-
cution of cyberattacks, logging, and time synchronization. It is also
possible to set up a continuous integration pipeline that rebuilds
the testbed regularly (e.g., every night) and runs all tests. Conse-
quently, as part of our efforts for reproducibility and adaptability,
our self-tests ensure correct functionality of a SOCBED setup.
Overall, by realistically reassembling a typical company network,
all involved systems and assets, as well as benign user activity and
adversarial actions, SOCBED provides a proof-of-concept for gen-
erating realistic log data for cybersecurity experiments. Specifically
focusing on generating reproducible and adaptable log datasets,
SOCBED lays the foundation for other researchers to reproduce
testbed setups on commodity computers, adapt testbed setups ac-
cording to the requirements of their own research efforts, and verify
the correct functionality of reproduced or adapted testbeds.
6 EVALUATION
Sound cybersecurity experiments should be valid, controlled, and
reproducible (cf. Section 3.1), which imposes requirements on the
used artifacts such as log data (cf. Section 3.2) and consequently the
testbed used for generating these artifacts (cf. Section 3.3). To fulfill
these requirements for generating log data artifacts and thus lay
the foundation for sound cybersecurity experiments, we proposed
our proof-of-concept testbed SOCBED (cf. Section 5).
ClientDMZServerLog ServerWindows KernelWindows ServicesSysmonPowerShellAuditbeatWinlogbeatLinux KernelDefault ServicesApache httpdPostfix & DovecotRsyslogLinux KernelDefault ServicesSambaLinux KernelDefault ServicesIPFire FirewallSuricataRsyslogPacketbeatRsyslogKibanaLogstashElasticsearchApache SparkApache KafkaCompanyRouterInternalServer697Reproducible and Adaptable Log Data Generation for Sound Cybersecurity Experiments
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
In the following, we exemplarily show that it is indeed possible
to perform a practical, sound experiment with log data generated by
SOCBED. We introduce the basic idea of this exemplary experiment
(Section 6.1) and describe its technical setup (Sections 6.2 and 6.3).
We then present its results (Section 6.4) and analyze them with
respect to reproducibility, controllability, and validity (Section 6.5).
6.1 Methodology
To demonstrate SOCBED’s suitability for sound cybersecurity ex-
periments, we chose an exemplary practical experiment from the
field of cyberattack detection using log data and network traffic.
More specifically, we simulate a common multi-step intrusion of
an enterprise network, a topic of high practical relevance (cf. Sec-
tion 1), to determine how well it can be detected with commodity
detection software. To design a concrete experiment, we narrow
this research question down and formulate a hypothesis that can
be tested with an experiment.
As repeatedly claimed by security experts, the default logging
configuration of a modern Windows system omits numerous events
that can be helpful for attack detection [2]. Therefore, we decided
to design an experiment to analyze whether attack detection indeed
improves when switching to a best-practice logging configuration.
More precisely, our hypothesis is that when switching from the
default to a best-practice configuration, more steps of an exemplary
multi-step cyberattack will be detected. By detection, we refer to at
least one alert being raised as a consequence of the attack step1.
To test this hypothesis, we use SOCBED to recreate a small com-
pany network and launch a scripted multi-step attack against it. We
also run commodity detection software and count the true positive
alerts, both with the default and best-practice logging configura-
tion. To prove that our experiment is reproducible, we automatically
build SOCBED instances on two commodity computers, run several
repetitions of the two scenarios on each of them, and then analyze
the results. To prove that the experiment is controlled, we show
that changing a variable (here: the logging configuration) does not
lead to unexpected side effects and thus allows to analyze the cause-
effect-relationship of the change. To show validity, we argue why
the experiment results are reliable (internal validity) and can be
generalized to real-world applications (external validity).
6.2 Exemplary Multi-Step Cyberattack
As a concrete cyberattack, we chose a multi-step cyber espionage
kill chain [22], as it is often executed by state-sponsored adver-
saries [64]. We chose this type of attack because its detection is
usually difficult as opposed to attacks with an obvious impact such
as ransomware [15]. The attack is composed of a subset of the at-
tack modules currently implemented in SOCBED (cf. Section 5 and
Appendix B) and comprises the following steps: (1) An attacker
probes a publicly-accessible web server of a victim company and
uses SQL vulnerabilities to retrieve contact information and further
details about some employees. (2) The attacker then sends a targeted
email containing a malicious attachment to an employee. (3) Upon
reception, the employee opens the attachment, thereby running a
1 In our opinion, this is a more practical metric than the total number of alerts because
some attack steps yield high numbers of alerts (e.g., vulnerability scans) while others
might raise only one (e.g., execution of a malicious file).
remote access tool that establishes a HTTP-based command-and-
control (C2) connection to the attacker. The attacker uses the re-
mote access tool to (4) capture the screen of the user and (5) retrieve
cached credentials of a domain administrator using a privilege esca-
lation. (6) Using these credentials, the attacker searches for another
computer in the network containing interesting documents (lat-
eral movement). These documents are then downloaded via the C2
channel. Finally, the attacker (7) uploads a custom backdoor pro-
gram, (8) adds an autostart registry key, and (9) starts the backdoor
program to ensure access at a later point in time.
6.3 Testbed Setup and Log Analysis
The topology and systems for this simulation correspond to SOC-
BED’s default setup, as depicted in Figure 2 of Section 5, with three
client machines running. As for detection tools, we decided to use
two widespread open-source tools: Sigma rules from the official
repository [56] for log data-based detection and Suricata [45] with
Emerging Threat rules [48] for network-based detection.
We built a SOCBED instance from scratch (i.e., the infrastructure-
as-code scripts created, configured, and snapshotted all virtual
machines) on two notebook computers (Dell Latitude 5501 running
Ubuntu 20.04 and MacBook Pro 15" Mid 2015 running macOS 10.15),
each equipped with an Intel Core i7 CPU, 16 GB of RAM, and an
SSD. For the second scenario, the Windows client was rebuilt with a
best-practice logging configuration [2], which mainly differs in the
installation of Microsoft Sysmon [53] and the activation of verbose
PowerShell logging. On each of the two machines, we ran ten
iterations of the two scenarios described above, respectively, thus
resulting in a total of 40 iterations. Each iteration starts with booting
all machines from their initial snapshots. After 15 minutes, the
described attack is launched, with three minutes idle time between
the attack steps. After 60 minutes, log data are downloaded from the
machines via the Elasticsearch API, then the machines are powered
off and reset to their initial state.
The downloaded log data consist of Windows Event Logs from
the client machines and syslogs from the Linux machines. For our
analysis, we extracted the Suricata alerts from the syslogs and ap-
plied all suitable Sigma rules to the Windows logs. We discarded
irrelevant or false Sigma and Suricata alerts (e.g., Windows report-
ing usage statistics to Microsoft servers) for further analysis, thus
keeping only the alerts that were caused by the attack. Finally, we
categorized these alerts by the attack step triggering them.
6.4 Results of the Exemplary Experiment
The goal of our exemplary experiment was to test the hypothesis
that the number of detected attack steps is higher when the best-
practice logging configuration is used (as compared to the default
configuration). Table 2 shows the sample means and standard devi-
ations of the true positive alerts and the number of detected attack
steps over all iterations (n = 10) on both hosts for the default (¯xd, sd)
and best-practice (¯xb, sb) configuration (Host 2 in gray, differences
between the hosts in boldface). For brevity, we pooled Suricata and
Sigma alerts. The detailed results are shown in Appendix C.
We can see that four attack steps were detected in all iterations
with the default configuration and six in all iterations with the best-
practice configuration. All standard deviations for the number of
698ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Rafael Uetz, Christian Hemminghaus, Louis Hackländer, Philipp Schlipper, and Martin Henze
Table 2: We performed an exemplary experiment compris-
ing a multi-step cyberattack ten times on two hosts. The re-
sults are consistent across all runs, thus showing that SOC-
BED facilitates reproducible and adaptable log data genera-
tion.
Number of alerts
Attack step
(1) Scan and exploit web server
(2) Send email with malware
(3) Open malicious attachment
(4) Capture screen
(5) Collect cached credentials
(6) Search network & download files
(7) Download custom backdoor
(8) Set autostart for backdoor
(9) Execute backdoor
Number of detected attack steps
¯xd
124.4
124.6
2
2
5.7
5.9
0
0
0
0
0
0
3
3
0
0
0
0
4
4
sd
0.699
0.699
0
0
0.483
0.316
0
0
0
0
0
0
0
0
0
0
0
0
0
0
¯xb
124.1
124.4
2
2
5.7
5.9
0
0
1
1
0
0
7
7
2
2
0
0
6
6
sb
1.595
0.516
0
0
0.483
0.316
0
0
0
0
0
0
0
0
0
0
0
0
0
0
detected attack steps are zero, so there is no evidence to reject our
hypothesis (the deviations in the number of alerts are discussed in
Section 6.6). We can therefore accept our hypothesis and conclude
that indeed more attack steps are detected with the best-practice
configuration as compared to the default configuration.
However, this does not necessarily imply causality: The higher
number of alerts could be caused by unintended side effects of
the configuration change, i.e., uncontrolled behavior. The experi-
ment could also have fundamental design flaws, which might be
discovered by other researchers when reproducing the experiment.
Furthermore, the results are not necessarily valid for real-world
use cases. These potential concerns illustrate the importance of an
experiment to be valid, controlled, and reproducible.
6.5 Soundness of the Experiment
SOCBED was specifically designed for the generation of sound
artifacts for log data research. Here, we discuss how its properties
support this task and thus ultimately help to make our exemplary
experiment reproducible, controlled, and valid.
Reproducibility. We have shown that the experiment can be per-
formed on different machines and still leads to the same outcome,
i.e., accepting the initial hypothesis. Furthermore, the same experi-
ment can easily be performed by other researchers because SOC-
BED is available as open-source software and runs on commodity
hardware. There are also no confidentiality or privacy restrictions
concerning the log dataset, so it can be freely used as well. Thus,
we conclude that the experiment is indeed reproducible.
However, this does not imply that each iteration of our experi-
ment (and thus SOCBED) produces the exact same log data (high-
lighted by the differences for attack steps (1) and (3) in Table 2).
Such differences result from an inherent trade-off between realism
and replicability when using virtual machines for log data genera-
tion and can be attributed to different effects such as background
processes and time-dependent tasks [27, 57]. We further analyze the
impact of such variations in Section 6.6 and discuss resulting limita-
tions in Section 7. The important message here is that reproducible
experiments need to be designed such that they are robust against
intra- and inter-host variations (just as in productive networks).
Controllability. Our experiment has only one variable that is
intentionally changed between runs: the Windows logging config-
uration. SOCBED’s infrastructure-as-code setup allows for trans-
parent configuration changes and ensures that there are no further
unintentional changes. Built-in self-tests additionally help to verify
that the functionality is not impaired by a change. Furthermore,
automated runs ensure deterministic user and adversary activity.
To confirm that the experiment is truly controlled with respect
to the configuration change, we analyzed all alerts in detail. We
verified that (1) the alert types raised by the best-practice runs are
a superset of the alerts with the default runs and (2) the additional
best-practice alerts were truly caused by the configuration change.
Both can be easily verified, as the default configuration yields no
Sigma alerts at all, which is expected as Sigma heavily builds on
Sysmon as an event source. The Suricata alerts, on the other hand,
are not affected by the configuration change. We provide more
details on the types and numbers of alerts in Appendix C.
Validity. Due to the transparent infrastructure-as-code build,
deterministic activity, and implemented self-tests, we have a high
confidence that the testbed behaves as expected. This is confirmed