title:SoK: Hate, Harassment, and the Changing Landscape of Online Abuse
author:Kurt Thomas and
Devdatta Akhawe and
Michael Bailey and
Dan Boneh and
Elie Bursztein and
Sunny Consolvo and
Nicola Dell and
Zakir Durumeric and
Patrick Gage Kelley and
Deepak Kumar and
Damon McCoy and
Sarah Meiklejohn and
Thomas Ristenpart and
Gianluca Stringhini
8
2
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
SoK: Hate, Harassment, and the Changing
Landscape of Online Abuse
(cid:5)
§
, Devdatta Akhawe(cid:53), Michael Bailey
Kurt Thomas
(cid:5)
◦
(cid:5)
, Zakir Durumeric(cid:3), Patrick Gage Kelley
, Nicola Dell
Sunny Consolvo
, Deepak Kumar
(cid:5)
, Dan Boneh(cid:3), Elie Bursztein
,
§
,
‡
Damon McCoy
, Sarah Meiklejohn(cid:52)(cid:5)
∗
(cid:5)
Google
(cid:3)Stanford
◦
Boston University
(cid:52)University College London
Cornell Tech
§
◦
, Thomas Ristenpart
(cid:53)Figma, Inc.
∗
, Gianluca Stringhini
‡
New York University
University of Illinois, Urbana-Champaign
Abstract—We argue that existing security, privacy, and anti-
abuse protections fail to address the growing threat of online hate
and harassment. In order for our community to understand and
address this gap, we propose a taxonomy for reasoning about
online hate and harassment. Our taxonomy draws on over 150
interdisciplinary research papers that cover disparate threats
ranging from intimate partner violence to coordinated mobs.
In the process, we identify seven classes of attacks—such as
toxic content and surveillance—that each stem from different
attacker capabilities and intents. We also provide longitudinal
evidence from a three-year survey that hate and harassment is
a pervasive, growing experience for online users, particularly
for at-risk communities like young adults and people who
identify as LGBTQ+. Responding to each class of hate and
harassment requires a unique strategy and we highlight ﬁve such
potential research directions that ultimately empower individuals,
communities, and platforms to do so.
I. INTRODUCTION
Emerging threats like online hate and harassment are trans-
forming the day-to-day experiences of Internet users. Abusive
attacks include intimate partner violence [27], [65], [66],
[108], anonymous peers breaking into a target’s account to leak
personal communication and photos [131], and coordinated
bullying and sexual harassment campaigns that involve tens
of thousands of attackers [1]. In a survey by Pew in 2017,
41% of Americans reported personally experiencing varying
degrees of harassment and bullying online [118]. Globally,
40% of people reported similar experiences [110].
Despite this changing abuse landscape, existing security
and anti-abuse protections continue to lag and focus almost
exclusively on disrupting cybercrime. Such defenses take into
account the proﬁt incentives of attackers and their requirement
to target as many victims as possible to scale and maximize
returns [3], [136]. Hate and harassment does not adhere to this
for-proﬁt paradigm. Attackers are instead motivated by ideol-
ogy, disaffection, and control: a landscape where interpersonal
and geopolitical conﬂicts happen as much online as ofﬂine.
Consequently,
threats are highly personalized [108], vary
across cultural contexts [127], and often exploit unintended
applications of widely accessible technologies [27].
In this work, we explore how online hate and harassment
has transformed alongside technology and make a case for
why the security community needs to help address this threat.
We collate over 150 research papers and prominent news
stories related to hate and harassment and use them to create a
taxonomy of seven distinct attack categories. These include—
among others—toxic content like bullying and hate speech,
and surveillance including device monitoring and account
takeover.
We then provide in-depth,
longitudinal statistics on the
growth of hate and harassment and the at-risk communities
currently being targeted. Our analysis draws on a three-year
survey collated from 50,000 participants located in 22 different
countries. We ﬁnd that 48% of people globally report expe-
riencing threats including sustained bullying (5%), stalking
(7%), and account takeover by someone they know (6%). Over
the past three years, the odds of users experiencing abuse
have increased by 1.3 times. Young adults aged 18–24 and
LGTBQ+ individuals in particular face heightened levels of
risk. These observations requires that practitioners take into
account regional variations and at-risk groups when designing
interventions.
Based on our ﬁndings, we propose ﬁve directions for how
our community can re-imagine security, privacy, and anti-
abuse solutions to tackle hate and harassment. Our proposed
interventions span technical, design, and policy changes that
assist in identifying, preventing, mitigating, and recovering
from attacks. Exploring these directions, however, requires
resolving multiple social equities that are in conﬂict. Tensions
include balancing notions of free speech with community or
platform-based moderation, and the well-being of raters with
the necessity of human review to interpret context. Resolutions
to these tensions must come from researchers, practitioners,
regulators, and policy experts at large in order to stem the
threat posed by online hate and harassment.
II. WHAT IS ONLINE HATE AND HARASSMENT?
To appropriately ground our taxonomy and solutions, we
ﬁrst scope what abusive behaviors fall under the umbrella
of online hate and harassment. We then discuss the interplay
between these attacks and other emerging online threats, such
as violent extremism and inaccurate information.
© 2021, Kurt Thomas. Under license to IEEE.
DOI 10.1109/SP40001.2021.00028
247
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
A. Hate and harassment background
Hate and harassment occurs when an aggressor (either an
individual or group) speciﬁcally targets another person or
group to inﬂict emotional harm, including coercive control or
instilling a fear of sexual or physical violence [36]. Examples
of highly publicized attacks include “Gamergate”, a coordi-
nated campaign where several women tied to the video game
industry received tens of thousands of messages that threatened
rape and death [1]. More recently, an attacker publicly leaked
nude images of former Rep. Katie Hill, resulting in her
resigning from ofﬁce [54].
While hate and harassment have a long history in the
social sciences and the ethos of the Internet [92], [130]—
with common adages like “don’t read the comments”—the
public increasingly views hate and harassment as a threat that
needs to be addressed. In a survey by Pew in 2017, 76% of
Americans believed that platform operators have a duty to step
in when hate and harassment occurs on their service [118].
This shift in public opinion is also reﬂected in the Terms of
Service of online platforms. For example, in 2009, Twitter’s
rules covered only impersonation and spam [81]. As of 2020,
Twitter’s rules also cover violence, harassment, and hateful
conduct, among a multitude of other abusive behaviors [138].
Hate and harassment is now explicitly prohibited by almost
all major online social networks [116].
While the intent to cause emotional harm differs strongly
from the proﬁt incentives of cybercrime [3], some parallels ex-
ist between the underlying tools and techniques of both types
of attacks: spamming, creating fake accounts, compromising
devices, obtaining privileged access to sensitive information,
and more. When protecting users from cybercrime, however,
security practitioners can disrupt the proﬁt-generating centers
fueling abuse like scams, ransomware, or banking fraud [136],
which in turn eliminates the incentives behind fake accounts,
spam, or related abuse. There is no equivalent strategy for
hate and harassment where attacks presently lack a depen-
dency chain. Technical interventions can merely address the
symptoms of conﬂicts rooted in politics and culture.
B. Related emerging threats
extremism. Over
The types of attackers engaged in hate and harassment may
also be involved in other emergent forms of abuse such as
violent extremism or spreading inaccurate information (see
Figure 1). We brieﬂy discuss these related threats and how
their goals at times overlap, while also proposing how to
distinguish hate and harassment.
Violent
two decades, vio-
ideologically-
lent extremists—actors that glorify or enact
motivated violence—have adopted emerging technologies for
coordination, recruitment, and distributing propaganda [91].
In the early 2000s, online abuse by jihadists was isolated to
forums like Al-Hesbah where readers could “follow links to
attack videos from active jihad campaigns” [21]. In the 2010s,
extremist content migrated to social media sites, where ex-
tremist media personalities developed a brand around violence
the last
Fig. 1: Attackers engaged in hate and harassment may also engage in
violent extremism and disinformation and misinformation. We argue
there is not always a clear boundary between each class of abuse,
but that the underlying intents differ.
while subverting hashtags and trending pages via search engine
optimization tactics to reach a wider audience [9], [16], [109],
[114]. Once in the public sphere, this content may be sought
after as part of a path towards radicalization [55].
These same tactics have been adopted in recent years by
far-right extremists, such as the 2019 live-streaming of a
mosque shooting in New Zealand [122] and of a synagogue
shooting in Germany [79]. Technology platforms have coor-
dinated their response to this threat through groups like the
Global Internet Forum to Counter Terrorism (GIFCT), for
example sharing digital ﬁngerprints of extremist content to
enable automatic removal [68]. As with cybercrime, this has
led to an adversarial arms race, with extremists moving to
less moderated communities [100], [149], adopting encrypted
messaging platforms [135], and testing new mediums such as
short-lived videos [144].
In the context of hate and harassment, extremist propaganda
may overlap or be ampliﬁed by communities actively involved
in online hate. However, the aim of violent extremism is
to radicalize as many people as possible via indiscriminate
distribution, which falls outside our requirement that hateful
or harassing content target a speciﬁc individual or group.
Disinformation and misinformation. Disinformation encom-
passes any efforts by “groups, including state and non-state
actors, to manipulate public opinion and change how peo-
ple perceive events” [133]. Tactically speaking, this entails
deliberately spreading false or misleading information [78].
Misinformation involves the spread of unintentional inaccura-
cies [78], such as when Indian villagers used messaging apps
to spread rumors of child abduction, ultimately resulting in the
lynching of a man [8]. Both disinformation and misinformation
abuse online platforms as a tool for dissemination.
There is an intrinsic link with the role that rumors and
falsehoods can play in online hate and harassment. A prime
example is “Pizzagate” [85]. Initially a conspiracy theory
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
248
Hate &harassmentDisinformation &misinformationViolent extremismGamergateMacronemail leakIndianlynchingRadicalizationpropagandaISIL videosChristchurchshootingPizzagateIntimatepartnersurveillanceKatie Hillleaked imagesthat presidential candidate Hillary Clinton ran a pedophilia
ring out of a Washington DC pizzeria, the event spiraled,
leading to harassment of the restaurant’s staff [85] and an
armed man entering the premises to “self-investigate” the
situation [22]. The motives and tactics behind inaccurate infor-
mation campaigns can thus overlap with hate and harassment.
However, while there may be emotional harm incurred by
inaccurate information, the primary focus of attackers is to
indiscriminately scale in order to change the perceptions of as
many people as possible. For this reason, we treat inaccurate
information campaigns as a separate class of abuse.
III. A TAXONOMY OF ONLINE HATE AND HARASSMENT
Given the breadth of hate and harassment attacks, we pro-
pose a threat model and taxonomy to assist in reasoning about
strategies for detection, prevention, mitigation, and recovery.
Our taxonomy (Table I) identiﬁes the criteria that differentiate
attacks, the harms incurred, and the scale of abuse.
A. Literature review
We developed our threat model and taxonomy by man-
ually examining the last ﬁve years of research from IEEE
S&P, USENIX Security, CCS, CHI, CSCW, ICWSM, WWW,
SOUPS, and IMC. Our team focused on topics related to hate
speech, harassment, trolling, doxing, stalking, non-consensual
image exposure, disruptive behavior, content moderation, and
intimate partner violence. We then manually searched through
the related works of these papers for relevant research, in-
cluding ﬁndings from the social sciences and psychology
communities (though restricted solely to online hate and
harassment, rather than hate speech or bullying in general).
Additionally, we relied on the domain expertise of the authors
to identify related works and major recent news events. In
total, we reviewed over 150 news articles and research papers
on the topic of online hate and harassment.
B. Deﬁning a threat model
We interpret hate and harassment through a threat model
that consists of an attacker and a target.1 The attacker’s
intent is to emotionally harm or coercively control the target,
irrespective of other side effects. Attackers may include inti-
mate partners such as a spouse, family and peers, anonymous
individuals, public ﬁgures (such as media personalities or
politicians), or coordinated groups and Internet mobs. Like-
wise, targets may include intimate partners, family and peers,
individuals, public ﬁgures, or at-risk groups (e.g., LGBTQ+
people or minorities). Our threat model makes no assumptions
about the capabilities of attackers, the existence of a direct
communication channel between the attacker and target, or
the protections available to targets.
1Similar to research in intimate partner violence, we intentionally avoid the
term “victim” in favor of “target” to not disempower people facing abuse.
C. Identifying criteria that differentiate attacks
Through an iterative process reviewing the attacks described
in the research papers we analyzed, three researchers arrived
at seven criteria to differentiate attacks. These researchers
annotated all papers for whether they partially or fully satisﬁed
any of the criteria, with the rest of the research team validating
the annotations. Our criteria include the AUDIENCE exposed
to an attack (A1-2), the MEDIUM through which an attacker
reaches a target (M1), and the CAPABILITIES required for the
attack to succeed (C1-4). Each criterion is represented as a
column of our taxonomy in Table I.
In selecting our criteria, we favored broad themes that we
believe will remain stable despite the rapidly expanding nature
of hate and harassment. As such, we opted for a taxonomy that
is agnostic to the technology platform abused (e.g., messaging
application, video application, or social network) or the exact
type of information involved. At the same time, we developed
our criteria to be granular enough to meaningfully differentiate
threats, and thus assist us in identifying solutions that apply
to only a single segment of hate and harassment, rather than
all abuse as a whole. We detail our criteria below.
Is the attack intended to be seen by the target? (A1). We
differentiate attacks that deliberately expose a target to harm-
ful content—such as bullying—from potentially undetected
attacks like covert stalking and monitoring. Awareness on the
part of the target allows them to report abusive behavior or
reach out to others for support. Attacks that may be visible
to the target, but not directly sent to the target (e.g., negative
reviews for a target’s business), partially satisfy this criteria.
Is the attack intended to be seen by an audience? (A2).
We consider whether attacks inherently require an audience to
incur harm, such as intentionally leaking personal information
like a target’s sexuality. Fully public exposure can exacerbate
the challenge of removing abusive content, but it opens up
the possibility for bystanders to intervene. Attacks that do not
require an audience, but that may be visible to bystanders (e.g.,
threats in a comment on a video), partially satisfy this criteria.
Does the attack use media such as images or text? (M1).
We consider whether an attacker requires a communication