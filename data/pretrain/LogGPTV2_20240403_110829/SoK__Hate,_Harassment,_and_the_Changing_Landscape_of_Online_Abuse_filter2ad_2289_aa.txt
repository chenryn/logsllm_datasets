**Title: SoK: Hate, Harassment, and the Changing Landscape of Online Abuse**

**Authors:**
- Kurt Thomas
- Devdatta Akhawe
- Michael Bailey
- Dan Boneh
- Elie Bursztein
- Sunny Consolvo
- Nicola Dell
- Zakir Durumeric
- Patrick Gage Kelley
- Deepak Kumar
- Damon McCoy
- Sarah Meiklejohn
- Thomas Ristenpart
- Gianluca Stringhini

**Affiliations:**
- Google
- Stanford University
- Boston University
- University College London
- Cornell Tech
- Figma, Inc.
- New York University
- University of Illinois, Urbana-Champaign

**Abstract:**
We argue that existing security, privacy, and anti-abuse protections are insufficient to address the growing threat of online hate and harassment. To help our community understand and address this gap, we propose a taxonomy for reasoning about online hate and harassment. Our taxonomy is based on over 150 interdisciplinary research papers covering a wide range of threats, from intimate partner violence to coordinated mobs. We identify seven classes of attacks—such as toxic content and surveillance—each stemming from different attacker capabilities and intents. We also provide longitudinal evidence from a three-year survey showing that hate and harassment are pervasive and increasing, particularly for at-risk communities like young adults and LGBTQ+ individuals. Addressing each class of hate and harassment requires a unique strategy, and we highlight five potential research directions to empower individuals, communities, and platforms.

**I. Introduction**
Emerging threats such as online hate and harassment are transforming the day-to-day experiences of Internet users. Abusive attacks include intimate partner violence, account takeovers, and coordinated bullying and sexual harassment campaigns. According to a 2017 Pew survey, 41% of Americans reported experiencing online harassment, and globally, 40% of people reported similar experiences. Despite this, existing security and anti-abuse protections continue to lag, focusing almost exclusively on disrupting cybercrime. Unlike cybercriminals, who are motivated by profit, attackers in hate and harassment are driven by ideology, disaffection, and control. Consequently, these threats are highly personalized, vary across cultural contexts, and often exploit unintended applications of widely accessible technologies.

In this work, we explore how online hate and harassment has evolved alongside technology and make a case for why the security community needs to address this threat. We collate over 150 research papers and prominent news stories to create a taxonomy of seven distinct attack categories, including toxic content and surveillance. We then provide in-depth, longitudinal statistics on the growth of hate and harassment, drawing on a three-year survey of 50,000 participants from 22 countries. We find that 48% of people globally report experiencing threats, with young adults and LGBTQ+ individuals facing heightened risks. These observations underscore the need for practitioners to consider regional variations and at-risk groups when designing interventions.

Based on our findings, we propose five directions for reimagining security, privacy, and anti-abuse solutions to tackle hate and harassment. Our proposed interventions span technical, design, and policy changes that assist in identifying, preventing, mitigating, and recovering from attacks. However, exploring these directions requires resolving multiple social equities, such as balancing free speech with platform moderation and the well-being of human reviewers. Resolutions to these tensions must come from researchers, practitioners, regulators, and policy experts to effectively combat the threat posed by online hate and harassment.

**II. What is Online Hate and Harassment?**
To appropriately ground our taxonomy and solutions, we first define what abusive behaviors fall under the umbrella of online hate and harassment. We then discuss the interplay between these attacks and other emerging online threats, such as violent extremism and misinformation.

**A. Background on Hate and Harassment**
Hate and harassment occur when an aggressor (individual or group) targets another person or group to inflict emotional harm, including coercive control or instilling fear of sexual or physical violence. Examples include "Gamergate," a coordinated campaign targeting women in the video game industry, and the public leak of nude images of former Rep. Katie Hill, leading to her resignation. While hate and harassment have a long history, the public increasingly views them as a threat that needs to be addressed. In a 2017 Pew survey, 76% of Americans believed platform operators have a duty to intervene. This shift is reflected in the Terms of Service of online platforms, which now explicitly prohibit hate and harassment.

While the intent to cause emotional harm differs from the profit incentives of cybercrime, there are parallels in the tools and techniques used, such as spamming and creating fake accounts. However, unlike cybercrime, hate and harassment lack a dependency chain, making it challenging to disrupt the root causes of such attacks.

**B. Related Emerging Threats**
**1. Violent Extremism:**
Over the past two decades, ideologically-motivated extremists have adopted emerging technologies for coordination, recruitment, and propaganda distribution. Initially, extremist content was limited to forums but later migrated to social media, where actors used SEO tactics to reach a wider audience. Platforms have responded by sharing digital fingerprints of extremist content to enable automatic removal, leading to an adversarial arms race. Extremist propaganda may overlap with online hate, but the aim of violent extremism is to radicalize as many people as possible, which falls outside our definition of targeted hate and harassment.

**2. Disinformation and Misinformation:**
Disinformation involves deliberate efforts to manipulate public opinion, while misinformation involves the unintentional spread of inaccuracies. Both can play a role in online hate and harassment, as seen in the "Pizzagate" conspiracy theory, which led to harassment and an armed confrontation. While there may be emotional harm, the primary focus of disinformation and misinformation is to change perceptions on a large scale, making it a separate class of abuse.

**III. A Taxonomy of Online Hate and Harassment**
Given the breadth of hate and harassment attacks, we propose a threat model and taxonomy to assist in reasoning about strategies for detection, prevention, mitigation, and recovery. Our taxonomy (Table I) identifies the criteria that differentiate attacks, the harms incurred, and the scale of abuse.

**A. Literature Review**
We developed our threat model and taxonomy by examining the last five years of research from top security and privacy conferences. Our team focused on topics related to hate speech, harassment, doxing, stalking, non-consensual image exposure, and intimate partner violence. We reviewed over 150 news articles and research papers on the topic of online hate and harassment.

**B. Defining a Threat Model**
Our threat model consists of an attacker and a target. The attacker's intent is to emotionally harm or coercively control the target, regardless of other side effects. Attackers and targets can include intimate partners, family, peers, anonymous individuals, public figures, or coordinated groups. Our model makes no assumptions about the capabilities of attackers, the existence of a direct communication channel, or the protections available to targets.

**C. Identifying Criteria that Differentiate Attacks**
Through an iterative process, we identified seven criteria to differentiate attacks:
1. **Audience (A1-2):** Whether the attack is intended to be seen by the target (A1) or an audience (A2).
2. **Medium (M1):** The medium through which the attacker reaches the target, such as images or text.
3. **Capabilities (C1-4):** The capabilities required for the attack to succeed.

These criteria are represented in Table I, providing a framework for understanding and addressing the diverse forms of online hate and harassment.