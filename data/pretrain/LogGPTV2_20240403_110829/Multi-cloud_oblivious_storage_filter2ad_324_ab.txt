and Write operations in terms of the ReadPartition and
WritePartition operations of our multi-clouds construc-
tion.
A background eviction process evicts blocks from the evic-
tion cache back to the server in an oblivious manner. With
every data access, randomly select 2 partitions for eviction.
If a block exists in the eviction cache assigned to the chosen
partition, evict a real block; otherwise, evict a dummy block
to prevent leakage.
2.2 Partition ORAM
Each partition is a fully functional ORAM in itself. Our
partition ORAM construction is based on the partition ORAM
of the SSS construction [31], which is a variant of the origi-
nal hierarchical construction [11], but with various optimiza-
tions geared towards maximal practical performance.
2 log N + 1 levels, where
level i ∈ {0, 1, . . . , L − 1} can store at more 2i real blocks,
and 2i or more dummy blocks. We refer to the largest level,
i.e., level L − 1, as the top level.
Each partition consists of L := 1
We extend the client’s position map to store the position
tuple (p, (cid:96), oﬀset) for each block, where p is the partition, (cid:96)
is the level, and oﬀset denotes the oﬀset within the level.
Read. To read a block, the client ﬁrst reads its position
map and ﬁnd out the position (p∗, (cid:96)∗, oﬀset∗) of the block.
Then, the client reads one block from each level of partition
p. For level (cid:96) = (cid:96)∗, the client reads the block at oﬀset∗.
For other levels, the client reads a random unread dummy
block.
If the block is found in the client’s eviction cache,
one dummy block is read from each level.
Write. Writing back a block to a partition causes a reshuf-
ﬂing operation. Speciﬁcally, let 0, 1, . . . , (cid:96) denote consecu-
tively ﬁlled levels such that level (cid:96)+1 is empty. Writing back
a block B causes levels 0, 1, . . . , (cid:96) to be reshuﬄed into level
(cid:96) + 1. In the single-cloud SSS construction, the reshuﬄing
is done by having the client download all blocks, permute
3. BASIC TWO-CLOUD CONSTRUCTION
IN THE SEMI-HONEST MODEL
We ﬁrst explain a scheme that is secure when both clouds
are semi-honest. Then, in Section 4, we explain how to
use our commutative checksum-encryption construction to
achieve security against one malicious cloud (without know-
ing which one might be malicious).
3.1 Intuition
Reducing ORAM shuﬄing overhead. In existing single-
cloud ORAM schemes [30–32, 35], a major source of the
client bandwidth overhead stems from shuﬄing. Periodi-
cally, the client has to download a subset of data blocks
from the cloud, permute them, re-encrypt them locally, and
write them back.
In our multi-cloud ORAM construction, we delegate the
shuﬄing work to the two clouds to minimize client-cloud
bandwidth usage. For example, cloud S1 shuﬄes a subset
of blocks, adds a layer of encryption (i.e., an “onion layer”),
and sends them to S2. Later when the client needs to read
from that subset of blocks, it fetches them from S2. In this
way, while cloud S1 knows the permutation used in the shuf-
ﬂing, it does not see which blocks the client requests from
S2 later. In contrast, while S2 sees which shuﬄed blocks the
client requests later, it has no knowledge of the permutation
applied by S1. As a result, neither cloud learns any infor-
mation about the client’s logical (i.e., “unshuﬄed”) access
pattern. The details of inter-cloud shuﬄing are described in
Section 3.5.2.
Note that it is necessary for security to add onion encryp-
tion layers after every shuﬄe: if a block B gets shuﬄed from
S1 to S2 and then back to S1, we do not want S1 to be able
to link the two occurrences of the same block B based on
the block’s data.
In the full online version [29], we intro-
duce a background onion removal process to avoid adding
an unbounded number of onion layers to a block.
Reducing ORAM read overhead. Existing single-cloud
ORAM schemes can also incur signiﬁcant bandwidth over-
head for reading a data block in-between shuﬄings [30–32,
35]. For example, the client may need to read one or more
blocks from O(log N ) levels in a hierarchy.
In our multi-cloud construction, we are able to signiﬁ-
cantly reduce the bandwidth overhead for reading a block
by doing the following. The client requests a set of (already
shuﬄed) blocks from S1. S1 then randomly permutes them
with a permutation known only by S1 and the client, and
then S1 sends them to S2. Finally, the client requests and
downloads only a single block from S2 by providing S2 with
the permuted index of the requested block. The details of
reading a block from multiple clouds are described in Sec-
tion 3.5.1.
3.2 Data Layout on Clouds
√
Our scheme leverages the partitioning framework as de-
√
N )
scribed in Section 2.1. We divide our ORAM into O(
partitions each of capacity O(
N ). Each partition is di-
vided into L = O(log N ) levels, and each level resides on
one of the two clouds. Which level resides on which cloud
249changes as partitions are shuﬄed. Each level can be ﬁlled
(i.e., it contains blocks) or empty.
In practice, each cloud can distribute the data across mul-
tiple servers. For simplicity, we will ﬁrst regard each cloud
as a single logical entity; then in the full online version [29],
we explain how to distribute the data and workload across
multiple servers within each cloud.
New use of partitioning framework. We use partition-
ing for a somewhat diﬀerent motivation than described in
the SSS paper [31]. Particularly, the SSS construction relies
on partitioning mainly to get rid of oblivious sorting [11,12],
thus achieving a constant factor saving in bandwidth.
Like in ObliviStore [30], we use partitioning to achieve
parallelism, and to greatly improve response time. We shuf-
ﬂe and read from multiple partitions in parallel as described
in the full online version [29], and such parallelism is espe-
cially useful if each cloud has multiple servers each serving
a subset of the partitions.
If a read request happens on
a partition not currently being shuﬄed, the read does not
block on shuﬄing.
In our system, the contention (proba-
bility of a read occurring on a partition involved in a large
shuﬄe) is very small — since (1) each read happens to a uni-
formly random partition (regardless of the access pattern),
(2) there are thousands of partitions, and (3) large shuﬄes
happen very infrequently, i.e., with exponentially decreasing
frequency. It’s also important to note that as the amount
of parallelism increases, the contention increases. However,
in practice, for large enough ORAMs with many partitions,
the amount of parallelism necessary to saturate the server
bandwidth does not cause signiﬁcant contention.
In our multi-cloud ORAM, we can still achieve about 2X-
3X client-cloud bandwidth cost even with a single partition.
However, with a single partition, reads would have to block
waiting for reshuﬄing operations of up to N blocks to ﬁnish
(e.g., gigabytes or terabytes of data transfered between the
two clouds), resulting in prolonged response times. Williams
and Sion [35] describe an algorithm for simultaneously read-
ing and shuﬄing a single-partition ORAM, but we observe
that this technique about doubles the cloud-to-cloud band-
√
width used because the single partition has twice as many
levels than our O(
N ) sized partitions.
As studied in [31], a larger number of partitions requires a
larger amount of client-side cache. Therefore, the number of
partitions can be used as a knob to tune the tradeoﬀ between
the client-side storage and the response time.
3.3 Client Metadata
The client stores the following metadata. Note that even
though the client storage is asymptotically linear, it has a
very small constant and is quite small in practice. For ex-
ample, all of the client storage combined is less than 1.5
GB for a 1 TB ORAM with 4KB blocks (i.e., less than
0.15% of the ORAM capacity). The amount of client stor-
age used is similar to several existing single-cloud ORAM
systems [30, 31, 35].
• The position tuple (p, (cid:96), oﬀset) denoting the block’s cur-
rent partition, level, and oﬀset within the level.
• A bit vector for every partition and every ﬁlled level, indi-
cating which blocks in the level have been read and (logi-
cally) removed.
• A time value for every partition, i.e., the total number of
operations that have occurred so far for each partition.
• A next dummy block counter for each partition and each
level. The counter is set to 0 when a level is being rebuilt;
and incremented when a next dummy element is read.
This counter is suﬃcient to implement the GetNextDummy
function mentioned in Figure 4.
√
It is possible to even further reduce the client storage to
N ) by recursing on the position map as was proposed
O(
in [31].
3.4 Initializing
Each partition is initially placed in a random cloud (S1
or S2). Each block is assigned to a random partition, and a
random oﬀset in the top level (the largest level) of the par-
tition. The client’s position map is initialized accordingly.
We assume initially, all blocks have the value zero. This
can be done by initializing all blocks on the server to be
encryptions of the zero block with appropriate metadata at-
tached.
Initialization can be optimized to consume much
less bandwidth using a similar technique described in [31].
3.5 ORAM Read and Write Operations
A standard ORAM includes two operations: Read and
Write. As previously mentioned in Section 2 and Sec-
tion 3.2, our construction uses the SSS partitioning frame-
work [31]. The SSS framework speciﬁes how Read and
Write operations can be securely expressed in terms of par-
tition ORAM read/write operations called ReadPartition
and WritePartition. Section 2 explains how this can be
achieved. Since we rely on this framework we only need to
describe
and
WritePartition.
implement ReadPartition
how to
ReadPartition and WritePartition operations are per-
formed in parallel across all partitions but in serial for each
individual partition to ensure consistency.
3.5.1 Reading from a Partition
In a typical single-cloud ORAM scheme [31], the client
needs to download one block for each of the O(log N ) levels
in a partition to hide which level it wants to read from. In
our construction, we rely on the inter-cloud shuﬄing tech-
nique such that the client only needs to download a single
block.
Our ReadPartition protocol is illustrated in Figure 2(a)
and described in detail in Figure 4. To read a block u,
the client ﬁrst looks up its position map to get the tuple
(p∗, (cid:96)∗, oﬀset∗) indicating the position (partition p∗, level
(cid:96)∗, oﬀset oﬀset∗) for block u. The client then generates
one oﬀset for each ﬁlled level (cid:96) in partition p. For level
(cid:96) = (cid:96)∗, the oﬀset oﬀset (cid:96) = oﬀset∗. For all other levels, its
oﬀset (cid:96) corresponds to a random unread dummy (obtained
via GetNextDummy) determined by computing the permuted
oﬀset of the nextDummy[p∗, (cid:96)] counter of each level. The
client now divides these oﬀsets based on which cloud con-
tains each level, and sends the corresponding oﬀsets to each
cloud. It is important for security that the client sends to
each cloud only the oﬀsets of the levels contained within the
cloud.
Now, each cloud reads one block from each of its ﬁlled
level, at the oﬀsets indicated by the client. The cloud with
fewer ﬁlled levels onion-encrypts the fetched blocks, and
250(a) ReadPartition
(b) WritePartition (when (cid:96) < L − 1)
Figure 2: Two-cloud ORAM protocol. Actions are performed in nondecreasing order speciﬁed by the circled
numbers. If two actions have the same number within a diagram, it means that they can happen in parallel.
Figure 3: Layout of an ORAM partition across two clouds over time. This ﬁgure shows for a speciﬁc partition
which levels are ﬁlled in each cloud (S1 and S2) over time (ti ∈ {t1, . . . , t16}). When the bottom-most level is
empty, our algorithm automatically decides to which cloud to write the next block to preserve the invariant
that all consequentially ﬁlled levels (from the bottom upwards) are always located on the same cloud.
sends them to the other cloud. Without loss of general-
ity, assume S2 sends its onion-encrypted blocks to S1. S1
now merges the blocks from S2 with its own fetched set,
onion-encrypts them, and reshuﬄes them using a PRP key
shared with the client. The reshuﬄed set is sent to S2.
The client now reveals the desired index within the reshuf-
ﬂed array to S2, and S2 returns the block at that index.
In the full online version [29], we explain how the client
decrypts the fetched block. Decrypting involves removing
multiple onion layers of encryption as described in the full
online version [29].
3.5.2 Writing to a Partition
Our WritePartition protocol is illustrated in Figure 2(b)
and described in detail in Figure 4. Whenever a block is
written to a partition p, it is shuﬄed with all consecutively
ﬁlled levels 0, 1, 2, . . . , (cid:96) of the partition into level (cid:96) + 1. If
(cid:96) = L − 1 is the top level, all levels will be shuﬄed into the
top level.
Note that there are 2L possible ways to divide L levels be-
tween two clouds. Our assignment algorithm is designed to
best facilitate inter-cloud shuﬄing by enforcing the following
invariant.
Invariant. Any time when consecutive levels 0..i are ﬁlled
and need to be shuﬄed into level i + 1, levels 0..i all reside
within the same cloud.
This minimizes the cloud-cloud bandwidth because shuf-
ﬂes always involve consequentially ﬁlled levels (i.e., 0..i).
Suppose that, without loss of generality, cloud S1 holds lev-
els 0..i to be shuﬄed into level i+1 of cloud S2. By ensuring
that the above invariant holds, at the start of shuﬄing, S1
already has all of levels being shuﬄed (0..i) and does not
have to fetch additional levels from S2 before starting the
shuﬄe. Figure 3 demonstrates how the levels of a partition
are divided amongst the two clouds over time.