“image.jpg”, 
“style.css”, 
for 
requests 
“preload.jpg” respectively. 
“page”, 
After the set of links has been determined for each page, 
we  can  measure  the  amount  of  UI-layer  information 
conveyed  by  GET  requests  for  those  link  URLs.  The  first 
step  is  identifying  the  link’s  referring  page.  HTTP  requests 
typically identify the referrer in a header field. If the referrer 
is found, then the request URL is compared against a library 
of  mandatory  and  voluntary  links  on  the  referring  page. 
Mandatory  links  are  those  that  should  always  be  loaded 
unless  they  are  cached by  the  browser,  such  as  images  and 
scripts.  The  set  of  mandatory  links  is  usually  smaller  and 
more  frequently  loaded.  Voluntary  links  are  those  that  the 
browser will not load unless the user takes some action, such 
as clicking a link. Voluntary links tend to be more numerous 
and  are  loaded  less  often.  Finally,  if  a  request  does  not 
identify  the  referrer  or  the  referring  page  cannot  be  found, 
then  we  must  go  to  the  library  of  all  previously  seen  links 
(mandatory and voluntary links from all pages) to look for a 
match. 
Once  a  matching  link  from  one  of  the  three  groups 
(mandatory, voluntary, or all) has been found, the amount of 
information in the request is measured as the sum of: 
•  2 bits to identify the link group 
•  Log(n)  bits  to  identify  the  link  within  the  group, 
where n is the total number of links in the group 
•  The  edit  distance  from  the  link  URL  to  the  actual 
request URL if it is not an exact match 
For  approximate  matches,  calculating  the  edit  distance 
from all URLs would be prohibitively expensive. Instead we 
select  only  a  few  strings  from  which  to  compute  the  edit 
distance, and then take the best answer. This pre-selection is 
done by finding strings with the longest shared substring at 
the beginning. Our original plan for mandatory links was to 
not count any data if all the mandatory links were loaded in 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
order.  This  works  in  a  controlled  environment,  but  our 
experiments showed that local caching prevents the browser 
from loading most of the mandatory links in many cases. A 
simpler  and  more  effective  approach  is  to  independently 
count  the  link  information  in  each  request.  This  includes 
information conveyed by the client about whether it has each 
object in its cache. 
4.4.  Form Submission Requests 
The  primary  method  for  transmitting  information  to  a 
web  server  is  form  submission.  Form  submission  requests 
send information that the user enters into input controls, such 
as  text  boxes  and  radio  buttons.  They  may  also  include 
information  originating  from  the  server  in  hidden  or  read-
only  fields.  Form  submissions  contain  a  sequence  of 
delimited    pairs,  which  can  be  seen  in  the 
body  of  the  sample  POST  request  in  Figure  2a.  The  field 
names,  field  ordering, and  delimiters between  fields  can be 
derived from the page containing the form, which is shown 
in  Figure 2b, and  thus do  not  convey  UI-layer  information. 
Field values may also be taken from the encapsulating page 
in  some  circumstances.  Check  boxes  and  radio  buttons  can 
transmit up to one bit of information each, even though the 
value  representing  “on”  can  be  several  bytes.  Servers  can 
also  store  client-side  state  by  setting  data  in  “hidden”  form 
fields,  which  are  echoed  back  by  the  client  upon  form 
submission. Visible form fields may also have large default 
values,  as  is  the  case  when  editing  a  blog  post  or  a  social 
networking  profile.  For  fields  with  default  values,  we 
measure the edit distance between the default and submitted 
values.  We  measure  the  full  size  of  any  unexpected  form 
submissions or form fields, which may indicate an attempt to 
leak data. 
5.  Dynamic Content Analysis 
Very  few  websites  today  are  free  from  active  content.  
This  poses  a  challenge  for  leak  measurement  because  such 
content  may  generate  HTTP  requests  with  variable  URLs. 
The data in these requests might still be free from UI-layer 
information, but making this determination requires dynamic 
content  analysis.  This  section  describes  methodology  for 
processing  and  extracting  expected  HTTP  request  URLs 
from active web content. 
5.1.  Javascript 
The  most  popular  language  for  dynamic  web  page 
interaction is Javascript, which is implemented by almost all 
modern  browsers.  Javascript  has  full  access  to  client-side 
settings, such as the browser version and window size, which 
help it deliver  the  most appropriate  content  to  the  user.  On 
many  websites,  Javascript  will  dynamically  construct  link 
URLs. These URLs cannot be extracted from simple parsing. 
One must execute the Javascript to obtain their true values. 
The leak analysis engine includes a Javascript interpreter, 
SpiderMonkey [15], to handle dynamic link creation. When 
processing  an  HTML  document,  the  analysis  engine  first 
extracts static links as described in the previous section, and 
then  executes  Javascript  code.  A  large  portion  of  links  that 
Javascript  generates  are  written  out  during  the  page  load 
process.  This  includes  tracking  images,  advertisements, 
embedded  media  content,  and  even  other  scripts.  The 
analysis engine executes Javascript as it is encountered in the 
HTML  document  in  the  same  way  as  a  web  browser.  This 
includes  complex  chaining  of  script  tags  using  both  the 
“document.write(  ‘<script…’  )”  method,  and  the  “node. 
addChild(  document.createElement(  ‘script’  )  )”  method. 
When  scripts  add  HTML  or  DOM  nodes  to  the  document, 
the analysis engine processes the new document text, looking 
for newly created links. Executing scripts allows the engine 
to see a large set of links that are unrecoverable with static 
parsing. 
5.2.  The DOM Tree 
Javascript is a stand-alone language that only has a few 
built-in types and objects. Most of the rich interface available 
to scripts inside of web pages is defined by the web browser 
as  part  of  the  Document  Object  Model  (DOM).  All  of  the 
elements in an HTML document are accessible to Javascript 
in a DOM tree, with each tag having its own node. Correctly 
emulating  the  DOM  tree  is  important  for  accurate  analysis 
because  many  scripts  will  manipulate  the  tree  to  generate 
links.  For  example,  it  is  common  for  scripts  to  create  new 
“Image” nodes and directly set their URLs.  Advertisers also 
tend  to  use  complex Javascript  code  to  place  ads  on  pages, 
often going through multiple levels of DOM node creation to 
load additional scripts. This presumably makes it harder for 
hackers  to  replace  the  advertisements,  and  for  website 
owners to commit click fraud. 
To  obtain  an  accurate  DOM  tree  representation,  our 
analysis  engine  parses  each  HTML  element  and  creates  a 
corresponding  DOM  node.  This  DOM  tree  is  available 
during  script  execution.  We  modeled  the  interface  of  our 
DOM  tree  after  Mozilla  Firefox  [14].  Updating  it  to  also 
reflect the quirks of other browser DOM implementations is 
future  work.  Because  we  only  care  about  data  in  HTTP 
requests and not actually rendering the web page, our DOM 
tree  does  not  fully  implement  style  and  layout  interfaces. 
Ignoring  these  interfaces  makes  our  DOM  implementation 
simpler  and  more  efficient.  The  DOM  tree  also  contains 
hooks  for  calls  that  cause  the  browser  add  links  to  a  page. 
When  a  script  makes  such  a  call,  the  engine  adds  the  new 
link  URL  to  either  the  mandatory  or  voluntary  link  library, 
depending  on  the  parameters.  The  engine  can  then  filter 
subsequent  HTTP  requests  that  match  the  dynamically 
created link URL. 
Another  option  for  achieving  correct  DOM  interactions 
would  have  been  to  render  HTML  and  Javascript  in  a  real 
web browser. We chose not to do this for a few reasons. The 
134
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
first  is  efficiency.  Analyzing  every  page  in  a  real  web 
browser would require setting up a dummy server to interact 
with  the  browser  through  the  local  network  stack.  The 
browser would also render the entire page and make requests 
to  the  dummy  server.  This  adds  a  significant  amount  of 
unnecessary  overhead.  Our  analysis  engine  cuts  out  this 
overhead by directly parsing pages and only emulating parts 
of  the  DOM  tree  that  are  relevant  to  leak  measurement.  A 
custom DOM tree implementation also makes instrumenting 
and  manipulating  of  the  Javascript  interpreter  much  easier. 
For  example,  tweaking  the  system  time  or  browser  version 
presented to Javascript would require non-trivial patches to a 
real browser. 
5.3.  Plug-ins and Other Dynamic Content 
Javascript is not the only language that enables rich web 
interaction  and  can  dynamically  generate  HTTP  requests.  
Popular  browser  plug-ins  like  Java  [23]  and  Flash  [1]  also 
have  such  capabilities.  In  fact,  Java  Applets  and  Flash 
objects  are  even  more  powerful  than  Javascript.  Taking 
things  a  step  further,  stand-alone  executable  programs  may 
make HTTP requests as well. These applications are free to 
interact  with  the  user,  the  local  system,  and  the  network  in 
any way that they please. 
Correctly  extracting  all  possible  links  from  plug-in 
objects  and  executables  is  undecidable  in  the  general  case. 
This  work  does  not  try  to  analyze  plug-ins  or  dynamic 
content other than Javascript. In the future, we hope to make 
some  gains  by  executing  plug-in  objects  in  a  controlled 
environment  and  monitoring  their  output.  It  may  also  be 
possible 
through  deep 
inspection and understanding of plug-in objects, but doing so 
yields  diminishing  returns  because  of  their  complexity  and 
diversity. 
to  achieve  some 
improvement 
Instead  of  examining  dynamic  content  for  plug-in 
objects,  we  look  at  previous  requests  to  create  a  library  of 
expected  URLs.  The  leak  measurement  engine  compares 
new HTTP requests that do not match a browser link to the 
set  of  all  prior  requests.  The  closest  link  is  determined  by 
computing  the  shortest  edit  distance  from  a  few  candidate 
requests  that  have  the  longest  matching  substring  at  the 
beginning.  This  approach  is  an  effective  approximation  for 
finding  the  closest  URL  because  similar  URL  strings  are 
much  more  likely  to  have  common  elements  at  the 
beginning.    The  resulting  information  content  is  equal  to 
log(m),  where  m  is  the  size of  the library of prior  requests, 
plus  the  edit  distance  with  respect  to  the  similar  prior 
request, plus two bits to indicate that the request is compared 
to the library of prior requests and did not come from a link 
on  a  webpage.  In  practice,  many  custom  web  requests  are 
similar to previous  requests. For example,  RSS readers  and 
software update services repeatedly send identical requests to 
check  for  new  data. We can effectively  filter  most of  these 
messages when measuring information leaks. 
135
)
x
=
<
t
(
P
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0
 100
 200
 300
 400
 500
 600
Seconds
Figure 3.  Cumulative distribution of delay times for all 
observed HTTP requests. P(t<=3) = .794,  
P(t<=192) = .976, P(t<=3600) = .9996. 
6.  Request Timing Information 
In  addition  to  data  in  the  request,  HTTP  messages  also 
contain timing information. The moment at which a request 
occurs  could  be  manipulated  by  a  clever  adversary  to  leak 
information.  It  is  important  to  consider  the  bandwidth  of 
timing  channels  when  measuring  information  leaks.  This  is 
especially  true  for  the  precise  unconstrained  measurement 
techniques in this paper because they may yield sizes of only 
a few bits per request in some cases.  
The amount of timing information in a request stream is 
equal  to  the  number  of  bits  needed  to  recreate  the  request 
times as seen by the recipient, within a margin of error. This 
margin of error is known as the timing interval. It is a short 
length  of  time  during  which  the  presence  of  a  request 
indicates a ‘1’ bit, and the absence of a request indicates ‘0’. 
Using  a  shorter  interval  increases  the  capacity  of  a  timing 
channel, but  also  increases  the error rate. Previous research 
on  IP  covert  timing  channels  found  0.06  seconds  to  be  an 
effective  value  for the  timing  interval  in one case  [6]. This 
equates to about 16.6 intervals per second. 
Prior  work  on  network  timing  channels  looks  at  IP 
packets [6]. Cabuk et al. describe a channel where IP packets 
are  sent  during  timing  intervals  to  indicate  ‘1’  bits.  HTTP 
requests differ from IP packets in that they tend not to occur 
as  closely  together.  Instead  of  having  a  regular  stream  of 
messages  throughout  a  connection,  web  requests  occur  in 
short bursts during page loads, and then at long intervals in 
between pages. For normal HTTP traffic, we have a  sparse 
timing channel in which a vast majority of the intervals are 
empty.  
For  a  sparse  channel,  the  timing  information  in  each 
HTTP  request  is  equal  to  the  bits  needed  to  indicate  how 
many  empty  intervals  have  occurred  since  the  last  request. 
The  cumulative  distribution  of  inter-request  delays  for  our 
experiments  can  be  seen  in  Figure  3.  This  shows  that  that 
80%  of  HTTP  requests  occur  within  three  seconds  of  each 
other,  while  95%  of  requests  occur  within  a  minute  and  a 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:15:14 UTC from IEEE Xplore.  Restrictions apply. 
Scenario 
Sports News 
Social Net. 
Shopping 
News 
Web Mail 
Blog 
# Reqs  Raw bytes  Simple  bytes/%  Gzip bytes/% 
911 
1,175 
1,530 
547 
508 
136 
1,188,317 
1,404,268 
914,420 
502,638 
620,065 
81,162 
199,857 / 16.8%  116,650 / 9.82%  
97,806 / 6.96% 
92,287 / 6.57% 
158,076 / 17.3% 
85,461 / 9.35% 