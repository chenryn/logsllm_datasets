We use a3 to illustrate two key properties: correctness and ﬁl-
tering of unwanted Internet trafﬁc. First, the new ACLs correctly
inherit policies from a3. For example, a3’s rules 3 and 4 are ac-
counted for by r3 (rules 1–3), r7 (rule 1), and r10 (rule 3). Second,
the new ACLs are placed at appropriate locations. a3 was originally
placed on the outbound router interface facing VLAN V2 to ﬁlter
all trafﬁc to the 3 BL components. After migration, BL1,1−2 and
BL2 are moved to the cloud. To block unwanted trafﬁc from DC1
to these migrated servers before it reaches the Internet, we move a3
to the inbound router interfaces facing VLANs V1 and V7, produc-
ing the new ACLs r3 and r5. Clearly, correct and beneﬁcial ACL
migration is tedious if done manually without using our algorithm.
252Figure 11:
Ingress/Egress
ACL sizes per ﬁrewall context.
Figure 12: ACL placement for the ERP application in DC1.
Figure 13: ACL placement after migrating the ERP applica-
tion from DC1 to the cloud.
ACL Rules
r3
r5
r7
r10
1. permit: F E1,2−3→TCP port p2 on BL1
2. permit: F E2→TCP port p2 on BL3
3. deny: any→BL1, BL2, and BL3
1. deny: any→BL1, BL2, and BL3
2. permit: any→HTTP & HTTPS ports on F E1 and F E2
3. deny: any→F E1 and F E2
1. permit: F E1,1→TCP port p2 on BL1
2. deny: F E1,1→BL1, BL2, BL3, and BE
1. permit: BL1,3−7, BL3, BL5→any
2. permit: BL4→TCP port p1 on any server
3. permit: F E1,1−3→TCP port p2 on BL1,1−2
4. permit: external users→TCP port p3 on any server
Default
permit
permit
permit
deny
Table 5: Subset of ACLs inheriting rules from a3 after migration.
5.3.3 Performance and scalability
To evaluate the scalability of our ACL migration algorithm, we ran
it on the reachability policies of the entire campus network, which
is a distinct rule-set from that described in § 5.3.1 corresponding
to policies in the campus data-center. The network consists of 700
VLANs and 212 ACLs. The ACLs are enforced on 341 locations
across 63 routers, and range in size from 2 to 300 rules. 24% of
the routers have 200+ rules. We derived the reachability matrix and
reinstalled it under the same network setting. Our algorithm took
4 minutes to run on a dual-core Intel Xeon 2 2.6GHz system with
8GB RAM using a Java implementation of the algorithm.
The total number of ACL rules across all routers in the network
is 7889. If a naive approach of computing and placing each cell of
the reachability matrix was used (§4.2.1), the total number of rules
grows by two orders of magnitude (570521). In contrast, if our ap-
proach that works at a coarser granularity is employed, the number
of rules is 7952; only 63 more than the original network. The slight
growth in rules is due to an inconsistent policy in the original net-
work and we omit details for lack of space. Overall, these results
show that our approach can scale well to large networks.
6 Related work
Many challenges must be addressed before enterprises can embrace
the beneﬁts of cloud computing. These include lock-in to speciﬁc
providers, lack of scalable storage, risks in fate-sharing with others
in shared infrastructure, service availability, and problem resolu-
tion in the cloud [14,23]. Enterprises are also concerned about how
they can stream-line the deployment of their complex services in
the cloud [23]. Our paper takes a step in this direction by devel-
oping a framework for deciding what to migrate to the cloud such
that enterprises can realize a beneﬁt. Maintaining the security and
privacy of data once migrated to the cloud is a challenge [14, 28],
and has started receiving attention from the community [19]. Ex-
isting solutions [2, 28] propose extending the enterprises’ network
into the cloud using a VPN in order to isolate the enterprises’ in-
stances in the cloud. Using such a model, our security framework
can be directly leveraged to ensure that the security policies in the
data center extend to the services on the VPN in the cloud.
Previous work in the analysis of multi-tier applications focused
on developing queuing models of applications that can be used to
estimate mean response times [26]. A dynamic provisioning tech-
nique for such applications that employs a queuing model is pre-
sented in [27]. In contrast, we use an optimization framework to
identify application components to migrate to the cloud in order to
realize the maximum beneﬁt, and consider variance in addition to
mean response times. We assume direct measurements of response
time distributions of the overall system, and individual components
prior to migration are available. These already account for queuing
considerations before migration. Our model implicitly assumes the
queuing structure does not change after migration. Incorporating
queuing models to account for such a change within our optimiza-
tion framework is an interesting direction for future research.
Algorithms for placing security policies when deploying new en-
terprise networks are presented in [24]. In contrast, we focus on
unique issues in migrating existing enterprise applications to the
cloud. In [24], it is assumed that the reachability matrix informa-
tion is deﬁned at the granularity of individual cells. This approach
is impractical in practice as it involves an explosion in the number
of ACL rules (§4.2.1, §5.3.3). In contrast, our approach provides
an efﬁcient intermediate representation and operates at a coarser
granularity, ensuring better scaling with large networks.
7 Discussion and Open Issues
We discuss key aspects of our work, and open issues:
Model enhancements: While our paper helps better understand
cloud migration trade-offs, it is only a start. An important future
direction is understanding the impact of migration on application
reliability, given the high costs of down-time for enterprise appli-
cations [18]. While on the one hand, components in the cloud run
at lower SLAs than components in the enterprise [23], migration
increases the number of fault domains in which the application op-
erates, and so has the potential to increase reliability. In this paper,
we have framed the problem as one of deciding how many servers
to migrate to the cloud, and focused on a two location model (lo-
cal and cloud data-center). In the future, it would be interesting
to generalize this to models that allow any number of servers to
be instantiated in the local and cloud data-centers, and allow for
multiple cloud locations. Finally, it may be interesting to extend
our cost and latency models to consider middle-boxes deployed in
enterprises. WAN optimizers [9] could potentially reduce Inter-
 0 20 40 60 80 100 0 50 100 150 200 250CDF(firewall context)# of ACL RulesIngressEgress253net communication costs, if deployed at both the local and cloud
data-centers. Encryption of data over the public Internet may incur
additional CPU costs, as well as higher latencies, though we note
the increase in latency is relatively small.
Handling dynamic variations in workload: An important bene-
ﬁt of hybrid architectures that we have not explored in this paper
is their potential to help handle peaks in workload. In particular,
the local data-center could be provisioned with enough server ca-
pacity to handle typical workloads, while cloud resources could be
invoked as needed to deal with peaks. That said, our modeling ap-
proach could potentially help in planning layouts that can deal with
dynamic workload variations. One approach is to use the model to
determine the appropriate conﬁgurations for a variety of estimated
workloads, and base the ﬁnal conﬁguration on the expected proba-
bilities of each workload. Another approach is to use our model pe-
riodically as workloads change over time, to determine if a change
in placement is required. We defer more detailed investigation of
these issues to future work.
Executing migrations: This paper focuses on the questions of
whether migration is beneﬁcial at all, and how to determine what
to migrate. Executing a migration itself poses several challenges
such as identifying dependencies and changing application server
conﬁgurations, minimizing application down time, and efﬁciently
copying large databases while synchronizing local and remote repli-
cas. While technologies such as live migration can be used to min-
imize service disruption, extending such technologies to wide-area
environments is an area that needs more research.
Obtaining model parameters: Prior to migration planning, we
need to perform application discovery to obtain essential input pa-
rameters such as application dependencies, component response
times, and trafﬁc exchanged between components. Many research
tools and commercial products can be used to obtain application de-
pendencies either by inference from network communication pat-
terns [13], or by analyzing application-level conﬁgurations [7, 21].
Transaction counters and service response time measurements for
each component are widely available on enterprise servers today.
Most enterprise software packages provide embedded performance
monitoring capabilities that can be enabled on the servers them-
selves to track and report these performance numbers [5, 12]. Fi-
nally, inaccuracies in estimates of model parameters could be dealt
with by running the model with multiple sets of inputs and choosing
a conservative plan to ensure application response times are met.
8 Conclusion
In this paper, we have made two contributions. First, we have
shown (i) the potential beneﬁts of hybrid cloud deployments of en-
terprise applications compared to “all or nothing” migrations; and
(ii) the importance and feasibility of a planned approach to making
migration decisions. Second, we have shown the feasibility of au-
tomatic and assurable reconﬁguration of reachability policies as en-
terprise applications are migrated to hybrid cloud models. We have
validated our algorithms using a campus ERP application, Azure-
based cloud deployments, and router conﬁgurations of a large cam-
pus network. Our work is an important but ﬁrst step. In the future,
we hope to gain experience with our approach using a wider range
of real enterprise applications, explore the predictive power of our
model under a wider range of settings, and adapt our approach to
dynamic variations in workload.
9 Acknowledgments
We thank Brad Devine, William Harshbarger, Michael Schulte, Kitch
Spicer and others in the Information Technology Department at
Purdue (ITaP) for providing access to the data, and for their time.
We thank our shepherd, Jeff Mogul, and the anonymous reviewers
for their feedback which helped substantially improve the quality
of the paper. This work was supported in part by NSF grants CNS-
0721488 and Career-0953622.
10 References
[1] Amazon Elastic Compute Cloud (EC2). http://aws.amazon.com/ec2/.
[2] Amazon Virtual Private Cloud. http://aws.amazon.com/vpc/.
[3] Amazon Web Services: Overview of Security Processes, White Paper.
http://aws.amazon.com/security.
[4] Animoto - Scaling Through Viral Growth. http:
//aws.typepad.com/aws/2008/04/animoto---scali.html.
[5] DB2 for z/OS Performance Monitoring and Tuning Guide.
http://publib.boulder.ibm.com/infocenter/dzichelp/
v2r2/topic/com.ibm.db29.doc.perf/dsnpfk17.pdf.
[6] IBM ILOG CPLEX. http://www-01.ibm.com/software/
integration/optimization/cplex/.
[7] IBM Service Management Tivoli Application Dependency Discovery Manager
Software. http:
//www-01.ibm.com/software/tivoli/products/taddm/.
[8] Microsoft Windows Azure.
http://www.microsoft.com/windowsazure/.
[9] Riverbed Technology. http://www.riverbed.com/.
[10] SQL Azure Firewall. http:
//msdn.microsoft.com/en-us/library/ee621782.aspx.
[11] The Case Against Cloud Computing.
http://www.cio.com/article/477473/.
[12] Websphere application server performance monitoring infrastructure (pmi).
http://publib.boulder.ibm.com/infocenter/wasinfo/
v6r0/index.jsp?topic=/com.ibm.websphere.express.doc/
info/exp/ae/cprf_pmi_arch.html.
[13] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and A. Muthitacharoen.
Performance Debugging for Distributed Systems of Black Boxes. In Proc.
SOSP, 2003.
[14] M. Armbrust, A. Fox, R. Grifﬁth, A. D. Joseph, R. H. Katz, A. Konwinski,
G. Lee, D. A. Patterson, A. Rabkin, I. Stoica, and M. Zaharia. Above the clouds:
A Berkeley view of cloud computing. Technical Report UCB/EECS-2009-28,
EECS Department, University of California, Berkeley, Feb 2009.
[15] Arthur Cole. The Future Belongs to Hybrid Clouds.
http://www.itbusinessedge.com/cm/blogs/cole/
the-future-belongs-to-hybrid-clouds/?cs=30482.
[16] T. H. Cormen, C. Stein, R. L. Rivest, and C. E. Leiserson. Introduction to
Algorithms. McGraw-Hill Higher Education, 2001.
[17] D. Gottfrid. The New York Times Archives + Amazon Web Services =
TimesMachine. http://open.blogs.nytimes.com/2008/05/21/.
[18] J. Hamilton. The Cost of Latency. http://perspectives.mvdirona.
com/2009/10/31/TheCostOfLatency.aspx, Oct. 2009.
[19] L. E. Li, M. F. Nowlan, C. Tian, Y. R. Yang, and M. Zhang. Mosaic: Policy
Homomorphic Network Extension. Technical Report YALEU/DCS/TR-1427,
CS Department, Yale University, Feb 2010.
[20] M. Tawarmalani and N. V. Sahinidis. Global optimization of mixed-integer
nonlinear programs: A theoretical and computational study. Mathematical
Programming, 99(3):563–591, 2004.
[21] K. Magoutis, M. Devarakonda, and K. Muniswamy-Reddy. Galapagos:
Automatically Discovering Application-Data Relationships in Networked
Systems. In Proc. IM, 2007.
[22] M. O’Neill. Connecting to the cloud. http:
//www.ibm.com/developerworks/library/x-cloudpt1/.
[23] K. Sripanidkulchai, S. Sahu, Y. Ruan, A. Shaikh, and C. Dorai. Are Clouds
Ready for Large Distributed Applications? In Proc. SOSP LADIS Workshop,
2009.
[24] Y.-W. E. Sung, S. G. Rao, G. G. Xie, and D. A. Maltz. Towards systematic
design of enterprise networks. In Proc. CoNEXT, 2008.
[25] Symantec. 2010 State of the Data Center Global Data.
http://www.symantec.com/content/en/us/about/media/
pdfs/Symantec_DataCenter10_Report_Global.pdf.
[26] B. Urgaonkar, G. Paciﬁci, P. J. Shenoy, M. Spreitzer, and A. N. Tantawi. An
analytical model for multi-tier internet services and its applications. In Proc.
SIGMETRICS, 2005.
[27] B. Urgaonkar, P. Shenoy, A. Chandra, and P. Goyal. Dynamic Provisioning of
Multi-tier Internet Applications. In Proc. ICAC, 2005.
[28] T. Wood, P. Shenoy, A. Gerber, K. Ramakrishnan, and J. V. der Merwe. The
Case for Enterprise-Ready Virtual Private Clouds. In Proc. HotCloud
Workshop, 2009.
[29] G. Xie, J. Zhan, D. A. Maltz, H. Zhang, A. Greenberg, G. Hjalmtysson, and
J. Rexford. On static reachability analysis of IP networks. In Proc. INFOCOM,
2005.
254