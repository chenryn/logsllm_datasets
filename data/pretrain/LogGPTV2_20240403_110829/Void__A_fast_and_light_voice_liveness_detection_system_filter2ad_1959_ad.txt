than 98%, and EER is below 1%.
7.6.3 Loudspeaker types
To demonstrate Void’s performance against high quality
speakers, we experimented with various types of loudspeak-
ers. For our dataset and the ASVspoof dataset, we used the
trained models described in Section 7.6 and 7.1, respectively.
For evaluation, we tested those two models separately on
the samples collected through each of the speakers listed in
Table 7.
Table 7: Void’s performance on different loudspeakers.
Dataset
Our dataset
ASVspoof
Loudspeaker
V-MODA
Logitech
Yamaha
Bose
Smart TV
Dynaudio BM5A
Behringer Truth B2030A studio monitor
Genelec 6010A studio monitor
Samples Detection Acc.(%)
99.6
99.4
99.9
98.6
99.4
92.7
95.1
81.1
2,198
2,002
1,997
1,997
24,282
430
1,381
198
2,190
1,990
1,996
1971
24,152
399
1,313
160
Void achieved over 98.5% detection accuracy for all the
loudspeakers in our dataset. As for the ASVspoof dataset,
it showed varying performance against high quality loud-
speakers: the detection accuracy for Dynaudio BM5A and
Behringer Truth B2030A studio monitor were 92.7% and
95.1%, respectively; the detection accuracy dropped signiﬁ-
cantly to 81.1% against Genelec 6010A studio monitor.
7.6.4 Cross data training
For cross data training, we trained Void on the live-human
voice and replay attack samples collected from one speciﬁc
dataset, and evaluated the performance of Void against a dif-
ferent unseen (with respect to the human participants and
2694    29th USENIX Security Symposium
USENIX Association
playback device types) dataset. For the training dataset, we
used a single, ﬁxed set of 26,965 voice samples collected
from 20 male participants, and replayed through the V-MODA
speakers. For testing, we considered the following four sce-
narios: (1) we used 20 unseen male participants’ voice sam-
ples to perform replay attacks; the V-MODA speaker was
used as a playback device; (2) we used voice samples col-
lected from 20 unseen female participants, and replayed them
through the V-MODA speaker; (3) we used voice samples
collected from 20 unseen female participants, and replayed
them through the Bose and Yamaha unseen speakers; and (4)
we used voice samples collected from 20 unseen male partici-
pants, and replayed through the Bose, Yamaha, and Logitech
unseen speakers.
Table 8: Effects of cross training on detection accuracy.
Diversity
Cross data
Dimension Test samples RoC Acc.(%) EER(%)
0.04
Scenario 1
1.9
Scenario 2
4.8
Scenario 3
Scenario 4
3.1
29,956
28,224
58,062
58,956
0.99
0.98
0.98
0.99
100
96.4
82.1
93.2
We only changed one variable in the ﬁrst two scenarios but
changed all variables in the third and fourth scenario. Table 8
shows the evaluation results for those scenarios. For scenario
1, Void achieved 100% attack detection rate and an EER of
0.04%. For scenario 2, Void achieved 96.4% attack detection
rate and an EER of 1.9%. For scenario 3, Void achieved 82.1%
attack detection rate and an EER of 4.8%. For scenario 4, Void
achieved 93.2% attack detection rate and an EER of 3.1%.
As demonstrated from the detection accuracy reductions in
scenarios 3 and 4, the performance of Void would degrade as
we introduce more variances.
7.7 Replay attacks in unseen conditions
To test Void under various unseen and unexpected environmen-
tal conditions, we installed the speakers and recording devices
in an ofﬁce building. This common area consists of meeting
rooms, elevators, entrances and exits, rest rooms, dining areas,
information desks, and so on. We replayed all human voice
samples (see Section 6.1) on 5 different playback speakers:
Galaxy S8 and S9, V-MODA, Bose, and Logitech speakers.
We replayed the voice samples using two different volumes,
normal and loud, and recorded them using two Galaxy S8
phones, located 30cm and 140cm away from the speakers.
The entire recording sessions took about 10 full days to com-
plete. After removing improperly recorded samples, we were
left with 119,996 replay attack samples with a huge variety
of background noises and situations.
We evaluated the performance of Void against those unseen
replay attack samples. Even with such an unexpected and
diverse set of replay conﬁgurations, Void was able to correctly
detect 96.2% of the attacks, showing its robustness in unseen
conditions.
8 Robustness against adversarial attacks
We evaluated Void against hidden/inaudible voice command,
voice synthesis, EQ manipulation attacks, and combining re-
play attacks with live-human voices. To measure the attack
detection rates, we trained a Void classiﬁer with all of our
own replay attack and human voice datasets (see Section 6),
and used that classiﬁer to classify given set of attack sam-
ples described below. The detection rates of Void against all
adversarial attacks are presented in Table 9.
Table 9: Detection rates against adversarial attacks.
Attack
Hidden
Inaudible
Synthesis
EQ manipulation
Combining
Dataset
Our dataset
Ultrasonic speaker
Our Tacotron dataset
Strategy 1
Strategy 2
Our dataset with human noise
# Samples Acc. (%)
99.7
100
90.2
89.1
86.3
98.2
1,250
311
15,446
350
430
3,600
8.1 Hidden voice command attacks
Hidden voice commands refer to commands that can not be in-
terpreted by human ears but can be interpreted and processed
by voice assistant services [24, 25]. Hidden voice commands
typically add more noise-like frequencies to original voice
samples during obfuscation, which should increase the overall
signal power linearity.
Figure 9: Power spectrum and spectral features represent-
ing live-human voice (left) and hidden voice (right) for a
sample utterance “Artiﬁcial intelligence is for real.”
Figure 9 compares the signal power distributions for live-
human voice and hidden voice command generated with a
phrase “Artiﬁcial intelligence is for real.” The original com-
mand is shown on the left, and the obfuscated hidden com-
mand, which was played through a loudspeaker, is shown on
the right. Unlike the live-human case in which the power dis-
tribution shows a non-linear behavior (mostly concentrated
below 2 kHz), the linearity coefﬁcients for the hidden voice
USENIX Association
29th USENIX Security Symposium    2695
03691215Frequency, kHz05101520Signal powerLive-human03691215Frequency, kHz05101520Signal powerHidden voice samplePeaks mean location: 0.8kHz = 0.97q = 2.40 = 0.70q = 38.25Peaks meanlocation: 2.6kHzsamples indicate a more linear behavior (i.e., ρ: 0.97 and
q: 2.40). The high power frequency characteristics are also
different, which is another indicator for a replay attack.
To evaluate Void against hidden command attacks, we
recorded hidden voice command samples using the black-box
attack methods demonstrated in [25]. We used 1,250 samples
from our own replay attack dataset to generate the attack sam-
ples. Void was highly effective against hidden voice command
attacks, demonstrating attack detection rate of 99.7% for our
replay attack dataset (see Table 9).
8.2
Inaudible voice command attacks
Inaudible voice command attacks involve playing an ultra-
sound signal with spectrum above 20kHz, which would be
inaudible to human ears. Inaudible voice commands are typ-
ically played through ultrasonic speakers. Due to the non-
linear characteristics of hardware – microphones in this case
– the received voice signals are shifted to lower frequen-
cies (down-modulation) with much lower power. To eval-
uate the performance of Void against inaudible attacks, we
implemented an inaudible attack with 347 popularly used
Amazon Alexa commands, targeting Echo Dot as the con-
sumer device. We used Google’s Text to Speech service
(https://pypi.org/project/gTTS/) to convert text com-
mands into speech data. We then modulated voice commands
using amplitude modulation with high level frequency of
21kHz. After modulation, the “BatSound L400 ultrasound
speaker” (http://batsound.com/?p=12) was used to re-
play the modulated voice samples. 311 out of 347 com-
mands were successfully recognized and processed by Ama-
zon Alexa. We stored those 311 samples in the “.M4A” ﬁle
format and used them as the attack set. Void achieved 100%
detection rate against inaudible voice command attacks (see
Table 9).
8.3 Voice synthesis attacks
To test Void’s performance against voice synthesis attack, we
used open source voice modeling tools called “Tacotron” [1]
and “Deepvoice 2” [2] to train a user voice model with 13,100
publicly available voice samples (https://keithito.com/
LJ-Speech-Dataset/). We then used the trained model to
generate 1,300 synthesis voice attack samples by feeding in
Bixby commands as text inputs.
After attack data generation, we played those synthesis
attack samples through four different speakers: Galaxy S8, V-
MODA, Logitech 2.1 Ch., and Yamaha 5.1 Ch. speakers were
used. For each speaker type, we placed Galaxy S8 in three
different distances as described in Section 6.2, and recorded
synthesis attack samples. After removing samples that were
not properly recorded, we were left with a ﬁnal set of 15,446
synthesis attack samples and tested them on Void.
Void achieved 90.2% attack detection rate against this set,
demonstrating its potential in detecting voice synthesis at-
tacks. However, we note that this is a preliminary result, and
further tests need to be performed with test sets generated
through models trained on more users.
8.4 Audio EQ manipulation attacks
Since Void leverages spectral power patterns for attack detec-
tion, an advanced attacker who is aware of the classiﬁcation
features used by Void may try to craft attack commands using
audio EQ programs. EQ manipulation is a process commonly
used for altering the frequency response of an audio system
by leveraging linear ﬁlters. An attacker’s goal would be to
artiﬁcially create attack commands that show power patterns
similar to those of live-human voices. By leveraging audio
equalization, an attacker could intentionally manipulate the
power of certain frequencies to mimic spectrum patterns ob-
served in live-human voices.
To demonstrate the robustness of Void against such EQ
manipulation attacks, we used Audacity (https://www.
audacityteam.org/) to generate audio samples that mimic
decay and peak patterns in spectral power like live human
voices under the following two strategies.
The ﬁrst attack strategy involved removing background
noises from audio samples because the samples were origi-
nally recorded with various background noises present (e.g.,
noises generated from fans, refrigerators, or computers). To
reduce noise in samples, we used noise reduction rate of 12
dB, and set frequency smoothing parameter to 3. We then
boosted power in frequencies less than or equal to 500Hz, and
reduced power in frequencies above 500Hz to mimic the char-
acteristics of live-human voices. Using 350 attack samples
from the ASVspoof dataset, we manually crafted 350 EQ ma-
nipulation attack samples based on this power manipulation
technique. Void correctly classiﬁed 89.1% of them as attacks.
The second attack strategy involved applying bass boost to in-
crease power in low frequencies between 20Hz and 100Hz to
about an average power of 9.5 dB. This power increase would
produce more ﬂuctuations in the low frequencies and power
patterns similar to those of live-human voices. Audio signals
are then normalized with maximum amplitude. Finally, a low
pass ﬁlter (frequency 1kHz) is applied. We used 430 attack
samples from the ASVspoof dataset, and manually crafted
430 EQ manipulation attack samples using this technique.
Void correctly classiﬁed 86.3% of them as attacks.
We found that the performance of Void was rather degraded
against EQ manipulation attacks. However, based on our man-
ual EQ manipulations, we realized that it is quite hard to
intentionally craft power patterns that mimic the patterns of
live-human voices because most loudspeakers add their own
non-linear distortions at low frequencies that cannot easily
be controlled by attackers [34]. For instance, it is difﬁcult to
craft a sound signal that has desired power peaks at certain fre-
2696    29th USENIX Security Symposium
USENIX Association
quency ranges even with dedicated manipulation of spectral
power patterns.
8.5 Combining replay attacks with live-
human voices
To evade detection by Void, attacker can try to simply combine
replay attacks with live human voices. For instance, when a
command is playbacked through a loudspeaker, a live-human
can start uttering some random phrases/commands at the same
time.
To analyze the effects of adding live-human voices while
replaying attack commands (i.e., both replayed commands
and human voices are simultaneously received by a target
voice assistant), we recorded additional replay attack samples
with two people – both males – continuously chatting near the
recording devices. We randomly selected 20 voice samples
recorded from 6 participants, and used 6 playback speakers
to perform replay attacks: Galaxy S8/S9, Bose, V-MODA,
Logitech, and Yamaha speakers. We used three Galaxy S8
and three S9 recording devices, which were spread out and
located 1-2m away from the loudspeakers. The two people
were sitting about 1-2m away from the recording devices,
continuously chatting with their normal voices throughout all
recording sessions. Since Void is not responsible for classi-
fying commands that are not properly recognized by voice
assistants, we ran all recorded samples through a speech to
text translation engine (“Google Speech Recognition”), and
removed commands that it failed to recognize – we were left
with 3,600 attack samples to test with.
Among those samples, Void correctly detected 3,536 at-
tack samples, achieving a detection accuracy of 98.2%. This
result shows that overlapping live-human utterances cannot
signiﬁcantly affect the detection accuracy.
9 Discussion
9.1 Latency and accuracy requirements
The ASVspoof 2017 competition did not measure model and
feature complexity nor time taken to train and classify given
voice samples – the primary objective was to achieve lowest
possible EERs. Consequently, most of the submitted solu-
tions [7] used multiple deep learning models (as an ensemble
solution) and heavy classiﬁcation features to minimize the
EERs – such solutions sit uneasily with real-world near-zero-
second latency and model complexity requirements.
As shown from our latency results (see Section 7.4), Void
is much lighter, faster, and simpler than other top perform-