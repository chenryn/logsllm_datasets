and randomly samples the clusters to ensure that all the major
types of column sums are appropriately represented. This
approach trades off additional setup overhead for improved
accuracy and is thus applicable for applications with more
reuse. Further, it works with a broader range of matrices than
AR since it only requires column sums to be homogeneous
within local regions, rather than globally similar.
Matrix msc00726 (Figure 2) is an example of a structured
problem (it is banded diagonal) that is too complex for AR
because it contains one set of values that are orders of
magnitude larger than another set of values. However, because
this matrix has only a small set of unique column sums (20
unique values or about 1% of columns are unique), clustering
can be used to identify these different classes of columns and
evenly sample among these classes. As such, even though the
structure of a problem may not be well suited for random
sampling, there may exist enough structure that can still be
exploited by using clustering to represent each subgroup of
similar values equally in the sampled distribution.
A variety of methods, with varying complexities, can be
used to cluster the column sums. We use an agglomerative
clustering algorithm [21], modiﬁed to ensure that (i) only those
clusters are formed which are sufﬁciently different from other
values (i.e. a distance threshold of 1e− 6) and (ii) the number
of clusters identiﬁed do not exceed the total number of samples
dictated by the sampling rate speciﬁed to the algorithm. This
algorithm is run over the entire set of column sums.
In some cases, the sparse problem may not be amenable to
either AR or AC, such as the matrix Oregon-1 (Figure 4).
Oregon-1 has high column sum variance (> 1e3) and a large
dissimilarity between a majority of its’ sums. Therefore, the
efﬁciency of AR and AC for this problem will be limited. We
may need to ﬁrst precondition the problem in order to yield a
more uniform distribution of column sums.
D. Identity Conditioning
In situations where sparse linear algebra applications have
reuse, but also have matrices that are not directly amenable to
sampling or clustering (i.e. the column sums are too variable
to produce an accurate check), preconditioning can be used
to transform the check into a form more amenable for low
overhead algorithmic fault detection.
Identity conditioning (IC) transforms the high variance
column sum distribution of the original matrix (A) into a more
uniform set of values by using a check vector tailored to the
given problem, instead of the traditional checksum: c = ¯1. IC
ﬁnds such a tailored check vector by solving the system:
cT A = ¯1T
(identity equation)
When the identity equation is solved exactly, the effect of A
and the variance of the column sums is eliminated entirely:
(cid:2)
cT y = (cT A)x = ¯1T x =
x
(IC)
This makes the problem directly amenable to low-cost
sampling as the variance in A now has a smaller effect on
the product cT A, making the sampling in AR and AC more
representative than when sampling the check vector c = ¯1. We
denote as ICAR the algorithm that preconditions the problem
with IC and checks individual MVM operations using AR.
Similarly, ICAC is the combination of IC and AC. Also, in
many scenarios, the sum of the elements of x may be known
x is used to
or can be inferred in advance (e.g. when
check prior linear operations). In such scenarios, the runtime
overhead of the check may be reduced signiﬁcantly.
(cid:3)
While IC can be highly effective, it has two major lim-
itations. First,
the exact solution to the identity equation
does not exist for all matrices (e.g. symmetric or over-
determined matrices). Second, computing this equation can
be very expensive. We resolve these issues by computing
the equation approximately, solving min(cid:2)AT c − ¯1(cid:2) with a
relaxed accuracy tolerance. In practice we ﬁnd that that the
iterative least squares algorithm in LAPACK [7] provides a
good approximation of c (residual  0.9,
the Decision Tree counted as failing for this system even if
there was another algorithm/parameter setting that could have
achieved this F-Score.
Figure 6 shows the overhead of all our techniques when
MVM is injected with faults from model 1 and the fault rate
is 1e-3. The boxed ranges within each column represent the
25th, 50th, and 75th percentiles of the detector’s overhead
across all of the problems (roughly the mean ± one standard
deviation). The lines within each column indicate the lowest
and largest overhead within 1.5 Interquartile Range (IQR)
of the lower and upper quartiles respectively. The detector
overheads outside this range are represented as outliers with
circles. The bars in Figure 7 show the fraction of problems on
which each detection technique achieved the target F-Score.
These results show that the traditional dense check has an
average overhead of 32%, ranging from 5% for denser prob-
lems to 80% for larger sparse problems with poor locality (e.g.
m3plates and bcsstm11). While the fault detection overhead
depends strongly on the size, sparsity, and locality of the
problem, Figure 6 and 7 illustrate that, in general, a direct
application of the dense check to sparse linear algebra can
be expensive. This motivates the need for other algorithmic
fault detection techniques that exploit the structure of sparse
problems.
)
%
(
d
a
e
h
r