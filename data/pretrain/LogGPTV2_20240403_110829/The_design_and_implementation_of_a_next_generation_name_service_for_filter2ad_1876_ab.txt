593160 unique domain names collected by crawling the Ya-
hoo! and the DMOZ.ORG web directories. These domain
names belong to 535088 unique domains and are served by
164089 di(cid:11)erent nameservers. We also separately examined
the 500 most popular domains as determined by the Alexa
ranking service. In this section, we describe the (cid:12)ndings of
our survey, which highlight the problems in failure resilience,
performance, and update propagation in the legacy DNS.
Failure Resilience - Bottlenecks
The legacy DNS is highly vulnerable to network failures,
compromise by malicious agents, and denial of service at-
tacks, because domains are typically served by a very small
number of nameservers. We (cid:12)rst examine the delegation
bottlenecks in DNS; a delegation bottleneck is the minimum
number of nameservers in the delegation chain of each do-
main that need to be compromised in order to control that
domain. Table 1 shows the percentage of domains that are
bottlenecked on di(cid:11)erent numbers of nameservers. 78.63%
of domains are restricted by two nameservers, the minimum
recommended by the standard [25]. Surprisingly, 0.82% of
domains are served by only one nameserver. Even the highly
popular domains are not exempt from severe bottlenecks in
their delegation chains. Some domains (0.43%) spoof the
minimum requirement by having two nameservers map to
the same IP address. Overall, over 90% of domain names
are served by three or fewer nameservers and can be disabled
by relatively small-scale DoS attacks.
Failure and attack resilience of the legacy DNS is even
more limited at the network level. We examined physical
bottlenecks, that is, the minimum number of network gate-
ways or routers between clients and nameservers that need
to be compromised in order to control that domain. We mea-
sured the physical bottlenecks by performing traceroutes to
10,000 di(cid:11)erent nameservers, which serve about 5,000 ran-
)
%
i
(
s
n
a
m
o
d
40%
30%
20%
10%
0%
1
2
3
all domains
top 500
ccTLDs
8
9
10
4
5
bottleneck gateways (#)
6
7
Figure 2: Physical Bottlenecks in Name Resolution: A
signi(cid:12)cant number of domains, including top-level do-
mains, depend on a small number of gateways for their
resolution.
domly chosen domain names, from (cid:12)fty globally distributed
sites on PlanetLab [2]. Figure 2 plots the percentage of
domains that have di(cid:11)erent numbers of bottlenecks at the
network level, and shows that about 33% of domains are bot-
tlenecked at a single gateway or router. While this number is
not surprising - domains are typically served by a few name-
servers, all located in the same sub-network - it highlights
that a large number of domains are vulnerable to network
outages. These problems are signi(cid:12)cant and a(cid:11)ect many top
level domains and popular web sites. Recently, Microsoft
su(cid:11)ered a DoS attack on its nameservers that rendered its
services unavailable. The primary reason for the success of
this attack was that all of Microsoft’s DNS servers were in
the same part of the network [38]. Overall, a large portion
of the namespace can be compromised by in(cid:12)ltrating a small
number of gateways or routers.
Failure Resilience - Implementation Errors
The previous section showed that legacy DNS su(cid:11)ers from
limited redundancy and various bottlenecks. In this section,
we examine the feasibility of attacks that target these bottle-
necks through known vulnerabilities in commonly deployed
nameservers. Early studies [10, 22, 27] identi(cid:12)ed several im-
plementation errors in legacy DNS servers that can lead to
compromise. While many of these have been (cid:12)xed, a sig-
ni(cid:12)cant percentage of nameservers continue to use buggy
implementations. We surveyed 150,000 nameservers to de-
termine if they contain any known vulnerabilities, based on
the Berkeley Internet Name Daemon (BIND) exploit list
maintained by the Internet Systems Consortium (ISC) [17].
Table 2 summarizes the results of this survey. Approxi-
mately 18% of servers do not respond to version queries,
and about 14% do not report valid BIND versions. About
2% of nameserves have the tsig bug, which permits a bu(cid:11)er
over(cid:13)ow that can enable malicious agents to gain access to
the system. 19% of nameserves have the negcache problem
that can be exploited to launch a DoS attack by providing
negative responses with large TTL value from a malicious
nameserver. Overall, exploiting the bottlenecks identi(cid:12)ed in
the previous section is practical.
problem
severity
tsig
nxt
negcache
sigrec
DoS multi
DoS (cid:12)ndtype
srv
zxfr
libresolv
complain
so-linger
fdmax
sig
infoleak
sigdiv0
openssl
naptr
maxdname
critical
critical
serious
serious
serious
serious
serious
serious
serious
serious
serious
serious
serious
moderate
moderate
medium
minor
minor
a(cid:11)ected nameservers
top 500
all domains
0.59 %
0.15 %
2.57 %
1.32 %
1.32 %
0.59 %
0.59 %
0.44 %
2.08 %
0.09 %
19.03 %
13.56 %
11.11 %
2.58 %
1.89 %
1.81 %
1.48 %
1.33 %
1.15 %
1.15 %
0.70 %
4.58 %
1.86 %
1.71 %
2.58 %
2.58 %
0 %
0 %
0.15 %
0.15 %
0.15 %
0.59 %
0.59 %
0.37 %
0.15 %
0.15 %
Table 2: Vulnerabilities in BIND: A signi(cid:12)cant percent-
age of nameservers use BIND versions with known secu-
rity problems [17].
Performance - Latency
Name resolution latency is a signi(cid:12)cant component of the
time required to access web services. Wills and Shang [41]
have found, based on NLANR proxy logs, that DNS lookup
time contributes more than one second to 20% of web object
retrievals, Huitema et al. [16] report that 29% of queries take
longer than two seconds, and Jung et al. [18] show that more
than 10% of queries take longer than two seconds. The low
performance is due mainly to low cache hit rates, stemming
from the heavy-tailed, Zipf-like query distribution in DNS.
It is well known from studies on Web caching [6] that heavy-
tailed query distributions severely limit cache hit rates.
Wide-spread deployment of content distribution networks,
which perform dynamic server selection, have further strained
the performance of the legacy DNS. These services, such as
Akamai and Digital Island, use the DNS in order to direct
clients to closer servers of Web content. They typically use
very short TTLs (on the order of 30 seconds) in order to
perform (cid:12)ne grain load balancing and respond rapidly to
changes in server or network load. But, this mechanism vir-
tually eliminates the e(cid:11)ectiveness of caching and imposes
enormous overhead on DNS. A study on impact of short
TTLs on caching [19] shows that cache hit rates decrease
signi(cid:12)cantly for TTLs lower than (cid:12)fteen minutes. Another
study on the adverse e(cid:11)ect of server selection [35] reports
that name resolution latency can increase by two orders of
magnitude.
Performance - Miscon(cid:12)gurations
DNS performance is further a(cid:11)ected by the presence of a
large number of broken (lame) or inconsistent delegations.
In our survey, address resolution failed for about 1.1% of
nameservers due to timeouts or non-existent records, mostly
stemming from spelling errors. For 14% of domains, author-
itative nameservers returned inconsistent responses; a few
authoritative nameservers reported that the domain does
not exist, while others provided valid records. Failures stem-
ming from lame delegations and timeouts can translate into
signi(cid:12)cant delays for the end-user. Since these failures and
inconsistencies largely stem from human errors [27],
it is
clear that manual con(cid:12)guration and administration of such
a large scale system is expensive and leads to a fragile struc-
ture.
Performance - Load Imbalance
DNS measurements at root and TLD nameservers show that
they handle a large load and are frequently subjected to de-
nial of service attacks [4, 5]. A massive distributed DoS
attack [28] in November 2002 rendered nine of the thirteen
root servers unresponsive. Partly as a result of this attack,
the root is now served by more than sixty nameservers and
is served through special-case support for BGP-level any-
cast. While this approach (cid:12)xes the super(cid:12)cial problem at
the topmost level, the static DNS hierarchy fundamentally
implies greater load at the higher levels than the leaves.
The special-case handling does not provide automatic repli-
cation of the hot spots, and sustained growth in client popu-
lation will require continued future expansions. In addition
to creating exploitable vulnerabilities, load imbalance poses
performance problems, especially for lookups higher in the
name hierarchy.
Update Propagation
Large-scale caching in DNS poses problems for maintain-
ing the consistency of cached records in the presence of dy-
namic changes. Selection of a suitable value for the TTL is
an administrative dilemma; short TTLs adversely a(cid:11)ect the
lookup performance and increase network load [18, 19], while
long TTLs interfere with service relocation. For instance, a
popular on line brokerage (cid:12)rm uses a TTL of thirty min-
utes. Its users do not incur DNS latencies when accessing
the brokerage for thirty minutes at a time, but they may ex-
perience outages of up to half an hour if the brokerage (cid:12)rm
needs to relocate its services in response to an emergency.
Nearly 40% of domain names use TTLs of one day or higher,
which prohibits fast dissemination of unanticipated changes
to records.
3. COOPERATIVE DOMAIN NAME
SYSTEM
The use and scale of today’s Internet is drastically dif-
ferent from the time of the design of the legacy DNS. Even
though the legacy DNS anticipated the explosive growth and
handled it by partitioning the namespace, delegating the
queries, and widely caching the responses, this architecture
contains inherent limitations.
In this section, we present
an overview of CoDoNS, describe its implementation, and
highlight how it addresses the problems of the legacy DNS.
3.1 Overview of Beehive
CoDoNS derives its performance characteristics from a
proactive caching layer called Beehive [32]. Beehive is a pro-
active replication framework that enables pre(cid:12)x-matching
DHTs to achieve O(1) lookup performance. Pastry [34],
and Tapestry [42] are examples of structured DHTs that use
pre(cid:12)x-matching [31, 20] to lookup objects. In these DHTs,
both objects and nodes have randomly assigned identi(cid:12)ers
from the same circular space, and each object is stored at
L1
2201
B
D
2110
2100
L2
E
L1
lookup (2101)
0210
Q
Figure 3: Proactive Caching in Beehive: Caching an
object at all nodes with i matching pre(cid:12)x-digits ensures
that it can be located in i hops. Beehive achieves O(1)
average lookup time with minimal replication of objects.
the nearest node in the identi(cid:12)er space, called the home
node. Each node routes a request for an object, say 2101,
by successively matching pre(cid:12)xes; that is, by routing the re-
quest to a node that matches one more digit with the object
until the home node, say 2100, is reached. Overlay routing
by matching pre(cid:12)xes in this manner incurs O(log N ) hops
in the worst case to reach the home node. Figure 3 illus-
trates the pre(cid:12)x matching routing algorithm in Pastry. A
routing table of O(log N ) size provides the overlay node with
pointers to nodes with matching pre(cid:12)xes. In a large system,
log N translates to several hops across the Internet and is
not su(cid:14)cient to meet the performance requirements of la-
tency critical applications such as DNS.
Beehive proposes a novel technique based on controlled
proactive caching to reduce the average lookup latency of
structured DHTs. Figure 3 illustrates how Beehive applies
proactive caching to decrease lookup latency in Pastry. In
the example mentioned above, where a query is issued for
the object 2101, Pastry incurs three hops to (cid:12)nd a copy of
the object. By placing copies of the object at all nodes one
hop prior to the home node in the request path, the lookup
latency can be reduced by one hop.
In this example, the
lookup latency can be reduced from three hops to two hops
by replicating the object at all nodes that start with 21.
Similarly, the lookup latency can be reduced to one hop by
replicating the object at all nodes that start with 2. Thus,
we can vary the lookup latency of the object between 0 and
log N hops by systematically replicating the object more
extensively.
In Beehive, an object replicated at all nodes
with i matching pre(cid:12)xes incurs i hops for a lookup, and is
said to be replicated at level i.
The central insight behind Beehive is that by judiciously