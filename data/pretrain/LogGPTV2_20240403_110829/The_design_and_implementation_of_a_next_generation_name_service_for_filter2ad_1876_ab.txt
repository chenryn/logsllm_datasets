### 593,160 Unique Domain Names Collected from Yahoo! and DMOZ.ORG Web Directories

We collected 593,160 unique domain names by crawling the Yahoo! and DMOZ.ORG web directories. These domain names belong to 535,088 unique domains and are served by 164,089 different nameservers. Additionally, we separately examined the 500 most popular domains as determined by the Alexa ranking service. In this section, we present our findings, which highlight issues in failure resilience, performance, and update propagation in the legacy DNS.

### Failure Resilience - Bottlenecks

The legacy DNS is highly vulnerable to network failures, compromise by malicious agents, and denial-of-service (DoS) attacks due to the small number of nameservers typically serving each domain. We first examine delegation bottlenecks in DNS, defined as the minimum number of nameservers in the delegation chain that need to be compromised to control a domain. Table 1 shows the percentage of domains bottlenecked by different numbers of nameservers. 

- **78.63%** of domains are bottlenecked by two nameservers, the minimum recommended by standards.
- **0.82%** of domains are served by only one nameserver.
- **0.43%** of domains spoof the minimum requirement by having two nameservers map to the same IP address.
- Overall, over **90%** of domain names are served by three or fewer nameservers and can be disabled by relatively small-scale DoS attacks.

Even highly popular domains are not exempt from severe bottlenecks in their delegation chains.

#### Physical Bottlenecks

At the network level, the resilience of the legacy DNS is even more limited. We examined physical bottlenecks, defined as the minimum number of network gateways or routers between clients and nameservers that need to be compromised to control a domain. We measured these bottlenecks by performing traceroutes to 10,000 different nameservers, which serve about 5,000 randomly chosen domain names, from fifty globally distributed sites on PlanetLab. Figure 2 plots the percentage of domains with different numbers of network-level bottlenecks and shows that about **33%** of domains are bottlenecked at a single gateway or router.

This highlights that a large number of domains are vulnerable to network outages. For example, Microsoft recently suffered a DoS attack on its nameservers, rendering its services unavailable. The primary reason for the success of this attack was that all of Microsoft’s DNS servers were in the same part of the network.

### Failure Resilience - Implementation Errors

The previous section demonstrated that the legacy DNS suffers from limited redundancy and various bottlenecks. Here, we examine the feasibility of attacks targeting these bottlenecks through known vulnerabilities in commonly deployed nameservers. Early studies identified several implementation errors in legacy DNS servers that can lead to compromise. While many of these have been fixed, a significant percentage of nameservers continue to use buggy implementations.

We surveyed 150,000 nameservers to determine if they contain any known vulnerabilities, based on the Berkeley Internet Name Daemon (BIND) exploit list maintained by the Internet Systems Consortium (ISC). Table 2 summarizes the results:

- **18%** of servers do not respond to version queries.
- **14%** do not report valid BIND versions.
- **2%** of nameservers have the tsig bug, which permits a buffer overflow, enabling malicious agents to gain system access.
- **19%** of nameservers have the negcache problem, which can be exploited to launch a DoS attack by providing negative responses with large TTL values from a malicious nameserver.

Overall, exploiting the bottlenecks identified in the previous section is practical.

### Performance - Latency

Name resolution latency significantly impacts the time required to access web services. Studies have found that DNS lookup times contribute more than one second to 20% of web object retrievals, and 29% of queries take longer than two seconds. The low performance is mainly due to low cache hit rates, stemming from the heavy-tailed, Zipf-like query distribution in DNS.

Widespread deployment of content distribution networks (CDNs), such as Akamai and Digital Island, has further strained the performance of the legacy DNS. These services use very short TTLs (on the order of 30 seconds) to perform fine-grain load balancing and respond rapidly to changes in server or network load. However, this mechanism virtually eliminates the effectiveness of caching and imposes enormous overhead on the DNS. A study on the impact of short TTLs on caching shows that cache hit rates decrease significantly for TTLs lower than fifteen minutes. Another study reports that name resolution latency can increase by two orders of magnitude due to server selection.

### Performance - Misconfigurations

DNS performance is further affected by the presence of a large number of broken (lame) or inconsistent delegations. In our survey, address resolution failed for about **1.1%** of nameservers due to timeouts or non-existent records, mostly stemming from spelling errors. For **14%** of domains, authoritative nameservers returned inconsistent responses; some reported that the domain does not exist, while others provided valid records. Failures from lame delegations and timeouts can translate into significant delays for end-users. Since these failures and inconsistencies largely stem from human errors, it is clear that manual configuration and administration of such a large-scale system are expensive and lead to a fragile structure.

### Performance - Load Imbalance

Measurements at root and TLD nameservers show that they handle a large load and are frequently subjected to DoS attacks. A massive distributed DoS attack in November 2002 rendered nine of the thirteen root servers unresponsive. Partly as a result of this attack, the root is now served by more than sixty nameservers and is served through special-case support for BGP-level anycast. While this approach fixes the superficial problem at the topmost level, the static DNS hierarchy fundamentally implies greater load at higher levels than at the leaves. The special-case handling does not provide automatic replication of hot spots, and sustained growth in client population will require continued future expansions. In addition to creating exploitable vulnerabilities, load imbalance poses performance problems, especially for lookups higher in the name hierarchy.

### Update Propagation

Large-scale caching in DNS poses problems for maintaining the consistency of cached records in the presence of dynamic changes. Selecting a suitable value for the TTL is an administrative dilemma: short TTLs adversely affect lookup performance and increase network load, while long TTLs interfere with service relocation. For instance, a popular online brokerage firm uses a TTL of thirty minutes. Users do not incur DNS latencies when accessing the brokerage for thirty minutes at a time, but they may experience outages of up to half an hour if the brokerage needs to relocate its services in response to an emergency. Nearly **40%** of domain names use TTLs of one day or higher, which prohibits fast dissemination of unanticipated changes to records.

### 3. Cooperative Domain Name System (CoDoNS)

The use and scale of today’s Internet are drastically different from the time of the design of the legacy DNS. Even though the legacy DNS anticipated explosive growth and handled it by partitioning the namespace, delegating queries, and widely caching responses, this architecture contains inherent limitations.

In this section, we present an overview of CoDoNS, describe its implementation, and highlight how it addresses the problems of the legacy DNS.

#### 3.1 Overview of Beehive

CoDoNS derives its performance characteristics from a proactive caching layer called Beehive. Beehive is a proactive replication framework that enables prefix-matching DHTs to achieve O(1) lookup performance. Examples of structured DHTs that use prefix-matching include Pastry and Tapestry. In these DHTs, both objects and nodes have randomly assigned identifiers from the same circular space, and each object is stored at the nearest node in the identifier space, called the home node.

Each node routes a request for an object by successively matching prefixes, routing the request to a node that matches one more digit with the object until the home node is reached. Overlay routing by matching prefixes incurs O(log N) hops in the worst case to reach the home node. Figure 3 illustrates the prefix-matching routing algorithm in Pastry.

Beehive proposes a novel technique based on controlled proactive caching to reduce the average lookup latency of structured DHTs. By placing copies of the object at all nodes one hop prior to the home node in the request path, the lookup latency can be reduced. For example, if a query is issued for the object 2101, Pastry incurs three hops to find a copy of the object. By replicating the object at all nodes that start with 21, the lookup latency can be reduced from three hops to two hops. Similarly, the lookup latency can be reduced to one hop by replicating the object at all nodes that start with 2. Thus, we can vary the lookup latency of the object between 0 and log N hops by systematically replicating the object more extensively.

In Beehive, an object replicated at all nodes with i matching prefixes incurs i hops for a lookup and is said to be replicated at level i. The central insight behind Beehive is that by judiciously replicating objects, we can achieve O(1) average lookup time with minimal replication of objects.