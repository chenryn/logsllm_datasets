[LN17] (Shamir)
[CGH+18] (Shamir)
[DEF+19]
[KOS16]
Time (s) Comm. (MB)
1,452
27,725
147,735
7,895
5,058
30,621
40,689
4,188
4,191
218,089
451,019
17.07
63.19
562.73
366.22
550.08
1769.68
3842.80
44.02
232.34
1063.82
2559.30
turn requires a considerable machinery in the background to han-
dle the the communication and the availability of deferred values.
Such an approach has been taken by VIFF [Gei07], its successor
MPyC [Sch20], and FRESCO [Ale20]. In contrast, MP-SPDZ aims
for more efficiency at this stage and defers usability to a higher level.
Therefore, the internal interface requires the programmer indicate
parallel computation. A strictly vector-based interface is one way of
doing this, but MP-SPDZ offers a slightly more amenable variant as
show in Figure 3. Assume that a and b are vectors of multiplicands
of size n and c is supposed to hold the output. This interface allows
a more dynamic use without the necessity of copying information
into vectors first. Note that processor represents infrastructure
protocol.init_mul(&processor);
for (int i = 0; i < n; i++)
protocol.prepare_mul(a[i], b[i]);
protocol.exchange();
for (int i = 0; i < n; i++)
c[i] = protocol.finalize_mul();
Figure 2: Parallel multiplication with C++ in MP-SPDZ.
such as the network setup as the opening protocol and preprocess-
ing. The latter is necessary for some protocols, for example any
protocol using Beaver multiplication.
MP-SPDZ: A Versatile Framework for Multi-Party Computation
Table 7: Time and communication per party for inner product of length 100,000 with various binary circuit protocols tolerating
one corrupted party. Numbers with ∗ are scaled up from inner products of length 1,000.
Security
Parties Type
Semi-honest
Malicious
3
2
3
2
Secret sharing
Secret sharing
Garbled circuit
Garbled circuit
Secret sharing
Garbled circuit
Garbled circuit
Secret sharing
Secret sharing
Garbled circuit
Garbled circuit
Secret sharing
Secret sharing
Garbled circuit
Protocol
[AFL+16]
[CDM00][CCD88]
[LPSY15][AFL+16]
[LPSY15][CDM00]
[FKOS15]-
[ZRE15]
[LPSY15][KOS16]-
[FLNW17]
[LN17][CCD88]
[LPSY15][LN17]
[LPSY15][LN17]
[FKOS15]
[DEF+19]∗
[LPSY15][KOS16]
Time (s) Comm. (GB)
0.1
1.8
68.7
184.5
5.4
3.6
4,178.4∗
1.9
14.1
163.6
518.6
73.4
2,639.8∗
24,486.9∗
18
85
594
1,002
59
8
13,502∗
27
191
1,777
3,241
897
39,048∗
132,243∗
Table 8: Time and communication per party for inner product of length 100,000 with various binary circuit protocols tolerating
two corrupted parties. Numbers with ‘∗’ are scaled up from inner products of length 1,000.
Security
Parties Type
Semi-honest
Malicious
5
3
5
3
Secret sharing
Garbled circuit
Secret sharing
Garbled circuit
Secret sharing
Garbled circuit
Secret sharing
Secret sharing
Garbled circuit
Protocol
[CDM00][CCD88]
[LPSY15][CDM00]
[FKOS15]-
[LPSY15][KOS16]-
[LN17][CCD88]
[LPSY15][LN17]
[FKOS15]
[DEF+19]∗
[LPSY15][KOS16]
Time (s) Comm. (GB)
3.2
578.7
8.4
11,981.6∗
25.4
1,624.7∗
140.9
5,209.5∗
70,586.3∗
107
3,416
65
24,219∗
232
12,087∗
1,029
40,891∗
249,508∗
As every protocol offers the same interface, it is straightforward
to implement the same computation for several protocols at once
using C++ templating also because the input and output protocols
use the same approach.
Coming back to our inner product example, Figure 3 shows its
implementation using the internal interface of an arithmetic pro-
tocol. As the communication takes place in the exchange calls, it
is parallelized as much as possible. Furthermore, the code uses the
optimized dot product facility if available for the protocol. Note
that the code is the same for all protocols. It is only the type of
the input, protocol, and output objects that changes from proto-
col to protocol. Further note that P and processor represent the
networking setup and further infrastructure as above, respectively.
4.1 Templating
C++ templating is widely used in MP-SPDZ because it allows
reusing code whenever suitable without performance penalty. If
runtime polymorphism with virtual functions would be used in-
stead, every additional protocol would incur extra cost at every
point of the execution where protocols differ. Given that the proto-
cols implemented in the framework differ in very small units (e.g.,
adding shares can involve as little as adding two 64-bit numbers but
also adding several pairs of numbers modulo a large prime), we esti-
mate that the cost of runtime polymorphism would be considerable
due to the continuous branching.
As an example of the benefits of templating, the implementation
of Beaver’s multiplication [Bea92] requires about 100 lines of code
while being used for over ten protocols spanning all computation
domains and several security models.
The central class of any protocol represents a secret value in
that protocol. Everything else like types for corresponding clear-
text values and sub-protocols are derived from this class using
typedef. Usually classes describing a particular protocol have one
template variable for the secret value class, from which all derived
information can be accessed.
input.reset_all(P);
for (int i = 0; i < n; i++)
input.add_from_all(i);
input.exchange();
for (int i = 0; i < n; i++)
{
a[i] = input.finalize(0);
b[i] = input.finalize(1);
}
protocol.init_dotprod(&processor);
for (int i = 0; i < n; i++)
protocol.prepare_dotprod(a[i], b[i]);
protocol.next_dotprod();
protocol.exchange();
c = protocol.finalize_dotprod(n);
output.init_open(P);
output.prepare_open(c);
output.exchange(P);
result = output.finalize_open();
Figure 3: Inner product computation with C++ in MP-SPDZ.
4.2 Preprocessing
A considerable part of the backend is dedicated to the so-called pre-
processing, which denotes the generation of correlated randomness.
The most prominent example of such randomness is multiplication
triples ([a],[b],[ab]) where a and b are uniformly random and all
numbers are secret-shared. Such triples are central to the Beaver’s
multiplication. Other examples include squares (a, a2), inverses
(a, a−1), random bits, daBits, and edaBits [EGK+20].
MP-SPDZ provides implementations to generate preprocess-
ing generically for semi-honest and malicious security as well as
specific implementations for certain protocols. For example, it is
more efficient to generate squares without using multiplication in
SPDZ [DKL+13], or daBits can be generated from random bits in
some protocols for computing modulo a power of two.
Since the marginal cost per preprocessed element is lower if they
are generated in batches, the framework provides infrastructure to
do so. Furthermore, it is possible to read such information from disk,
which facilitates benchmarking only the phase of the computation
that is dependent on secret data (sometimes called “online phase”).
Preprocessing is generally executed on demand to avoid unnec-
essary computation, which is different to SCALE-MAMBA [COS19]
where even the lesser used squares are generated whenever a com-
putation is run. This does not only slow down the computation, it
can also cause the application to seemingly hang at the end because
the preprocessing has not finished.
5 THE VIRTUAL MACHINE
Keller et al. [KSS13] have presented a virtual machine designed
for the specifics of multi-party computation. This virtual machine
is at the core of SPDZ-2 [KRSS18], the predecessor of MP-SPDZ.
The main characteristic of the virtual machine is that instructions
involving communication allow an unrestricted number of argu-
ments and thus minimizing the number of communication rounds.
It is this property that distinguishes it from many other virtual
Marcel Keller
machines and processors. Consider for example the 64-bit x86 in-
struction set. While instructions vary in the number of clock cycles
required for completion (as little as one for addition but more than
100 for sine or cosine [F+11]), the difference between instructions
is easily quantified by the difference in the binary circuits required
to compute them. In multi-party computation, there is not only a
quantitative difference between an addition of shares and a multi-
plication but also a qualitative one because the former can be done
locally while the latter involves communication. This qualitative
difference has implications that vary from protocol to protocol,
but the benefit of unrestricted parallelization of communication is
straightforward due to network latencies.
In the context of garbled circuits, there is another benefit of
the parallelization property. Bellare et al. [BHKR13] proposed im-
plementing garbled circuits using AES-NI, the CPU-native imple-
mentation of AES. Their scheme makes use of so-called pipelining,
which means that several AES operations can be run in parallel
on the same CPU core if the instructions are executed directly one
after the other. The virtual machine design allows one to make use
of pipelining while relieving the user in this respect.
High-level design. SPDZ-2 implements the SPDZ online phase for
computations in Fp for a large prime p and F2n with either n = 40 or
n = 128. MP-SPDZ adds a range of protocols, replacing Fp with Z2k
for some protocols and adding computations of Boolean circuits
(technically in F2). The latter is rooted in the implementation used
by Keller and Yanai [KY18], and it works with vectors of length 64 by
default. This is necessary for efficiency with some protocols where
a share only consists of a bit or a pair of bits, and it leads to a natural
optimization by using the 64-bit machine words of contemporary
processors, computing 64 bit operations at once. As a result, every
instantiation of the virtual machine offers integer-like computation
(in Fp or Z2k ), and computation for F2n and Boolean circuits, all in
the same security model.
Basic data types. The virtual machine allows handling data of
public and secret values for every computation domain and 64-bit
integers. The reason for having clear data types in every domain
on top of another integer type is that the size of numbers in the
computation domains vary and numbers in Fp are stored using
Montgomery representation, which does not lend itself for pur-
poses such as loop counters or addressing memory. It is therefore
cleaner to have public data types reflecting all computation domains.
Furthermore, in the context of garbled circuits there is a conceptual
difference between public numbers that are known prior to garbling
such as loop counters and numbers that result from revealing a
secret value. This difference matters because the garbling part of a
computation depends on a revealed value has to happen after the
evaluation resulting in said value whereas garbling only depending
on a loop counter can be processed at any time.
Registers. The virtual machine provides an unlimited number of
registers for every basic data type. While this is less sophisticated
than a stack-based design, it allows for a simpler implementation.
Register numbers are hard-coded into the bytecode, which enables
the virtual machine to allocate sufficient numbers for the compu-
tation. Registers are generally used to store inputs and outputs of
instructions, and they are local to a thread.
MP-SPDZ: A Versatile Framework for Multi-Party Computation
Memory. For more complex data structures such as arrays, ma-
trices, and higher-dimensional structures, the virtual machine pro-
vides another facility for every basic data type, called the memory.
The memory arrays are global, and thus allows communicating
information between threads. Unlike registers, the memory can be
accessed using runtime values stored in integer registers. Memory
has to be allocated at compile time.
Instructions. Most of the instructions supported by the virtual
machine can be roughly categorized as follows:4
Copying This includes initializing registers, copying between
registers and memory, as well as conversion between regis-
ters of different public data types.
Simple computation Instructions resembling the common
three-argument format is used for all computation that does
not require communication, including linear operations on
secret values.