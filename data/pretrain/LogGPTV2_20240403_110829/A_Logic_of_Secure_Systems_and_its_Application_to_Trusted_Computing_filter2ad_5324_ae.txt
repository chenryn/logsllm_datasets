is also shown in the same ﬁgure.
V
The integrity reporting protocol contains two steps. In
the ﬁrst, the TPM on machine m reads the contents w
of m.pcr.s, signs them and an identiﬁer (denoted PCR(s))
that uniquely identiﬁes m.pcr.s with its embedded private
key AIK−1(m) and sends the signed aggregate to the
remote veriﬁer. In the second step, the remote veriﬁer
veriﬁes this signature with the known public key AIK(m),
and checks that the contents of the signature match the
pair of PCR(s) and seq(sinit,BL(m),OS(m),APP(m)).
Security Properties. The security properties of integrity
reporting are formalized by the following two LS2 formu-
las, which we call J1 and J2 respectively.
[Veri f ier(m)]tb,te
(Mem(m.pcr.s,seq(sinit,BL(m),OS(m),APP(m))) @ t)
∃t. (t < te) ∧ MeasuredBootSRTM(m,t)
[Veri f ier(m)]tb,te
The ﬁrst property (J1) states that if the code Veri f ier(m)
is executed successfully between the time points tb and
te, then there must be a time t before te at which m.pcr.s
contained seq(sinit,BL(m),OS(m),APP(m)). The second
property (J2) means that a remote veriﬁer can identify the
boot loader and operating system that were loaded on m
at some time prior to te.
∃t. (t < te) ∧
V
To prove these properties, we require two new assump-
tions, which we combine in the set ΓSRT M below. The
ﬁrst of these assumptions states that the remote veriﬁer
is distinct from the TPM. This assumption is needed to
distinguish protocol participants, and is true in practice.
The second assumption is the honesty assumption for the
TPM from Section 3.1 that guarantees that the TPM’s
signature cannot be forged, and that the TPM always
executes only speciﬁed programs.
ΓSRT M =
{ ˆV (cid:54)=
Honest( ˆAIK(m),{T PMSRT M(m),T PMDRT M(m)})}
Theorem 3 (Security of Integrity Reporting). The follow-
ing are provable in LS2’s proof system:
(1) ΓSRT M (cid:96) J1
(2) ΓSRT M, ProtectedSRTM(m) (cid:96) J2
ˆAIK(m),
The proof of
(1) critically relies on the assump-
tion Honest( ˆAIK(m),{T PMSRT M(m),T PMDRT M(m)}) to
establish both that the TPM on m actually produced a
signature in the past, and that the value signed by the
TPM was actually read from m.pcr.s. The latter follows
from knowledge of the programs that the TPM may be
executing. (2) follows from (1) and Theorem 2.
4.1.4. Insights From Analysis. The security analysis
lead to a number of
insights including highlighting
weaknesses in the property provided by the protocol and
identifying program invariants required for security.
Staleness of Measurements. A key insight from the
analysis is that after executing the integrity reporting
protocol, the veriﬁer has no knowledge of how recent
the time of measurement t
is in comparison to te, the
time the veriﬁer’s execution ﬁnished. This staleness of
measurements is inherent in the protocol: it is possible
to reboot the machine with a different boot sequence after
sending the signature to the remote veriﬁer, as is known
from prior work [19]. Formally, one can only prove that
m.pcr.s contained the reported measurements at time t,
but not after.
Program Invariants.
In the process of proving the above
theorem, we prove a program invariant for the roles of
the TPM (i.e., T PMSRT M(m) and T PMDRT M(m)). This
invariant provides a speciﬁcation of the properties that a
TPM’s signing role must satisfy. In particular, the invariant
requires that if the TPM returns a value then the value is
a signature over the value stored in m.pcr.s and that the
TPM does not write to any memory locations. The latter
constraint is necessary to prevent previously measured
code from being modiﬁed after being measured.
4.2. Attestation Using a Dynamic Root of Trust
We perform an analysis of DRTM attestation using our
model of hardware support for late launch. We jointly
analyze the protocol code that performs integrity measure-
ment and reporting.
4.2.1. DRTM Protocol. We describe the security skeleton
of the DRTM attestation protocol in Figure 4. The DRTM
protocol is a four agent protocol. The processes are: (1)
OS(m), executed by the machine itself (called ˆm), that
receives a nonce from the remote veriﬁer, and performs a
late launch. (2) LL(m), executed by the hardware platform,
that reads the binary of the program P(m) from the secure
loader block (SLB), and measures then branches to P(m),
(3) P(m) that measures the nonce, evaluates the function
f on input 0 (the function f and its input may be changed
depending on application), and extends a distinguished
string EOL into m.d pcr.k to signify the end of the late
launch session. (4) T PMDRT M(m), executed by the TPM
of m, that signs the dynamic PCR m.d pcr.k, and sends
it to the veriﬁer. (5) Veri f ier(m), executed by a remote
veriﬁer, that generates and sends a nonce, receives signed
measurements, veriﬁes the signature, and checks that the
measurements match the sequence (dinit,P(m),n,EOL).
Security Property. We summarize the DRTM security
property as follows: if the veriﬁer is not the TPM, the
TPM does not leak its signing key, and the TPM executes
only the processes T PMDRT M(m) and T PMSRT M(m), then
the remote veriﬁer is guaranteed that J performed a single
late launch on machine m at some time tL, J branched
to P(m) only once at tC, J evaluated f once at tE (and
this happened after the veriﬁer generated the nonce),
J extended EOL into m.d pcr.k at some time tX, and
m.d.pcr.k was locked for the thread J from tL to tX. We
formalize this security property called JDRT M below.
[Veri f ier(m)]tb,te
V
∃J,tX ,tE ,tN,tL,tC,n.
∧ (tL < tC < tE < tX < te)
∧ (tb < tN < tE)
∧ (New(V,n) @ tN)
∧ (LateLaunch(m,J) @ tL)
∧ (¬LateLaunch(m) on (tL,tX])
∧ (¬Reset(m) on (tL,tX])
∧ (Jump(J,P(m)) @ tC)
∧ (¬Jump(J) on (tL,tC))
∧ (Eval(J, f ) @ tE)
∧ (Extend(J,m.d pcr.k,EOL) @ tX)
∧ (¬Eval(J, f ) on (tC,tE))
∧ (¬Eval(J, f ) on (tE ,tX))
∧ (IsLocked(m.d pcr.k,J)on(tL,tX])
In order to prove the property, we have to make the
following assumptions.
ΓDRT M =
{ ˆV (cid:54)=
ˆAIK(m),
Honest( ˆAIK(m),{T PMSRT M(m),T PMDRT M(m)})}
We also made the same assumptions in the SRTM protocol
(ΓSRT M = ΓDRT M). We prove the following theorem:
Theorem 4 (Security of DRTM). The following is prov-
able in LS2: ΓDRT M (cid:96) JDRT M
As in the SRTM protocol, the security of the DRTM
protocol relies on PCRs being append-only and write-
protected in memory. In addition, the DRTM protocol
relies on (1) write locks on all dynamic PCRs that are
provided by the late launch and (2) a dynamic reset of
m.d pcr.k , to reset the values in the dynamic PCRs to dinit
and signal that P(m) was executed with the protections
provided by late launch.
OS(m)
LL(m)
P(m)
≡ n(cid:48) = receive ;
write m.nonce,n(cid:48);
late launch
≡ P = read m.SLB;
extend m.d pcr.k,P;
jump P
≡ n(cid:48)(cid:48) = read m.nonce;
extend m.d pcr.k,n(cid:48)(cid:48);
eval f ,0;
extend m.d pcr.k,EOL
T PMDRT M(m) ≡ w = read m.d pcr.k;
r = sign (dPCR(k),w),AIK−1(m);
send r
Veri f ier(m)
≡ n = new ;
send n;
sig = receive ;
v = verify sig,AIK(m);
match v,(dPCR(k),
seq(dinit,P(m),n,EOL))
Figure 4. Security Skeleton for DRTM Attestation Protocol
4.2.2. Insights From Analysis. The security analysis lead
to a number of insights including revealing an insecure
protocol interaction between the DRTM and SRTM attes-
tation protocols, highlighting differences with the SRTM
protocol, and identifying program invariants required for
DRTM security that we subsequently used to manually
audit a security kernel implementation.
In extending LS2 to
Insecure Protocol Interaction.
model DRTM, we discovered that adding late launch
required us to weaken some axioms related to reason-
ing about invariance of values in memory in order to
retain soundness in the proof system. With these weaker
axioms, we were unable to prove the safety property
of the SRTM protocol. Soon after, we realized that
SRTM’s safety property can actually be violated using
latelaunch . Speciﬁcally, during the execution of the
SRTM protocol, a late launch instruction may be issued
by another thread before OS(m) has been extended into
m.pcr.s. The invoked program may then extend the code
of the programs OS(m) and APP(m) into m.pcr.s without
executing them, and send signed measurements to the
remote veriﬁer. Since the contents of m.pcr.s would be the
sequence seq(sinit,BL(m),OS(m),APP(m)),
the remote
veriﬁer would believe incorrectly that OS(m) was executed
and the SRTM protocol would fail to provide its expected
integrity property. This vulnerability can be countered if
the program loaded in a DRTM session were unable to
change the contents of m.pcr.s if SRTM were executing
in parallel. In the ﬁnal design of our formal model, we
force this to be the case by letting the thread booting a
machine to retain an exclusive-write lock on m.pcr.s even
in the face of a concurrent late launch, thus allowing a
proof of correctness of SRTM.
Late launch also opens the possibility of a code mod-
iﬁcation attack on SRTM. Speciﬁcally, after the code of
a program such as BL(m) or OS(m) has been extended
into m.pcr.s in SRTM, a concurrent thread may invoke a
DRTM session and change the code in memory before it
is executed. Any subsequent attestation of integrity of the
loaded code to a remote party would then be incorrect.
Our model prevents this attack by assuming that code
measured in PCRs during SRTM cannot be modiﬁed in
memory.
Comparison to SRTM. The property provided by the
DRTM protocol is stronger than the SRTM protocol for a
number of reasons:
to attest
proof
the DRTM protocol does not
Fewer Assumptions. The
of
security
for
rely on the
ProtectedSRTM(m) assumption that static PCRs are
locked. Instead the latelaunch action locks all
dynamic PCRs. If the machine M is a multi-processor
or multi-core machine that
is capable of running
multiple threads in parallel, the locks on the dynamic
PCRs will prevent attacks where malicious threads
running concurrently with the measurement
thread
into m.d pcr.k in an
extend additional programs
attempt
to their execution within a late
launch session.
Smaller TCB. The security proof of the DRTM does
not reason about
the measurements of the BIOS,
boot loader, or operating system stored in the static
PCRs (e.g., m.pcr.s), indicating that the security of
the DRTM protocol does not depend on these large
software components. This considerably reduces the
trusted computing base to just P(m) and LL(m) and
opens up the possibility of verifying that the TCB
satisﬁes the required program invariants.
Execution Integrity. Unlike the SRTM protocol that
does not provide sufﬁcient evidence to deduce that
the last program in a sequence of measurements is
branched to, the JDRT M property states that all pro-
grams measured during the protected session where
executed. The property goes further to state that the
programs completed execution. Speciﬁcally, the end of
session measurement EOL proves that P(m) executes
to completion.
Program Invariants. In the process of proving the above
theorem, we prove program invariants for the roles of
the TPM (i.e., T PMSRT M(m) and T PMDRT M(m)), and the
programs LL(m) and P(m). These invariants specify the
properties that T PMSRT M(m), T PMDRT M(m), LL(m), and
P(m) must satisfy for the DRTM protocol to be secure.
The invariant over the roles of the TPM is similar to
the TPM’s role invariant used for SRTM. The invariant
for LL(m) states that the code must maintain a lock on
m.d pcr.k and measure then branch to the program P(m).
The invariant for P(m) is shown below. The invariant
states that if there are no resets or late launches on m from
tb to te, m.d pcr.k is locked at tb and m.d pcr.k contains
the sequence seq(dinit,P(m)) at
tb and later contains
seq(dinit,P(m),x,EOL)), then there exists a thread J such
that J extended a value x (e.g., a nonce) into m.d pcr.k,
then evaluated f , then extended the end of session symbol
EOL, and that each action was performed once, in the
order speciﬁed, and m.d pcr.k was locked from tb to tX.
[Q]tb,te
J
∀t,x. ((¬Reset(m) on (tb,te])
∧ (¬LateLaunch(m) on (tb,te])
∧ (Mem(m.d pcr.k,seq(dinit,P(m))) @ tb)
∧ (IsLocked(m.d pcr.k,J) @ tb)
∧ (tb < t ≤ te)
∧ (Mem(m.d pcr.k,seq(dinit,P(m),x,EOL)) @ t))
⊃ ∃tn,tE ,tX . ((tb < tn < tE < tX < t)
∧ (Extend(J,m.d pcr.k,x) @ tn)
∧ (Extend(J,m.d pcr.k,EOL) @ tX)
∧ (Eval(J, f ) @ tE)
∧ (¬Eval(J, f ) on (tb,tE))
∧ (¬Eval(J, f ) on (tE ,tX))
∧ (IsLocked(m.d pcr.k,J) on (tb,tX]))
Manual Audit of DRTM Implementation. To check that
the invariants required by our security analysis are correct,
we performed a manual source code audit of the Flicker
implementation of the DRTM protocol [30]. We checked
that Flicker’s security kernel implementation, represented