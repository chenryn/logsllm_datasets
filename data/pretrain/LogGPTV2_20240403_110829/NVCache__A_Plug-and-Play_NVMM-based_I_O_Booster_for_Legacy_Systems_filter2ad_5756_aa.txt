title:NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems
author:R&apos;emi Dulong and
Rafael Pires and
Andreia Correia and
Valerio Schiavoni and
Pedro Ramalhete and
Pascal Felber and
Ga&quot;el Thomas
NVCache: A Plug-and-Play NVMM-based I/O
Booster for Legacy Systems
R´emi Dulong∗, Rafael Pires‡, Andreia Correia∗, Valerio Schiavoni∗, Pedro Ramalhete§, Pascal Felber∗, Ga¨el Thomas†
∗Universit´e de Neuchˆatel, Switzerland, PI:EMAIL
‡Swiss Federal Institute of Technology in Lausanne, Switzerland, PI:EMAIL
†Telecom SudParis/Insitut Polytechnique de Paris, PI:EMAIL
§Cisco Systems, PI:EMAIL
1
2
0
2
p
e
S
3
]
C
D
.
s
c
[
2
v
7
9
3
0
1
.
5
0
1
2
:
v
i
X
r
a
Abstract—This paper introduces NVCACHE, an approach that
uses a non-volatile main memory (NVMM) as a write cache
to improve the write performance of legacy applications. We
compare NVCACHE against ﬁle systems tailored for NVMM
(Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite,
RocksDB). Our evaluation shows that NVCACHE reaches the
performance level of the existing state-of-the-art systems for
NVMM, but without their limitations: NVCACHE does not limit
the size of the stored data to the size of the NVMM, and works
transparently with unmodiﬁed legacy applications, providing
additional persistence guarantees even when their source code
is not available.
I. INTRODUCTION
NVMM is a type of memory that preserves its content
upon power loss, is byte-addressable and achieves orders of
magnitude better performance than ﬂash memory. NVMM
essentially provides persistence with the performance of a
volatile memory [30]. Examples of NVMM include phase
change memory (PCM) [14], [24], [38], [39], [11], resistive
RAM (ReRAM) [8], crossbar RAM [32], memristor [58] and,
more recently, Intel 3D XPoint [27], [41], [6], [5].
Over the last few years, several systems have started lever-
aging NVMM to transparently improve input/output (I/O)
performance of legacy POSIX applications. As summarized
in Table I,
these systems follow different approaches and
offer various trade-offs, each providing speciﬁc advantages and
drawbacks. §V details our analysis but, as a ﬁrst summary,
a system that simultaneously offers the following properties
does not exist: (i) a large storage space while using NVMM
to boost I/O performance; (ii)
efﬁcient when they provide
useful correctness properties such as synchronous durability
(i.e., the data is durable when the write call returns) or durable
linearizability (i.e., to simplify, a write is visible only when
it is durable) [28]; and (iii) easily maintainable and does not
add new kernel code and interfaces, which would increase the
attack surface of the kernel.
We propose to rethink the design of I/O stacks in order to
bring together all the advantages of the previous systems (large
storage space, advanced consistency guarantees, stock kernel),
while being as efﬁcient as possible. To achieve this goal,
we borrow some ideas from other approaches and reassemble
them differently. First, like Strata [37] and SplitFS [33], we
propose to split the implementation of the I/O stack between
the kernel and the user space. However, whereas Strata and
SplitFS make the user and the kernel space collaborate tightly,
we follow the opposite direction to avoid adding new code
and interfaces in the kernel. Then, as DM-WriteCache [53]
or the hardware-based NVMM write cache used by high-
end SSDs, we propose to use NVMM as a write cache to
boost I/Os. Yet, unlike DM-WriteCache that provides a write
cache implemented behind the volatile page cache of the kernel
and therefore cannot efﬁciently provide synchronous durability
without profound modiﬁcations to its code, we implement the
write cache directly in user space.
Moving the NVMM write cache in user space does, how-
ever, raise some major challenges. The kernel page cache may
contain stale pages if a write is added to the NVMM write
cache in user space and not yet propagated to the kernel. When
multiple processes access the same ﬁle, we solve the coherence
issue by leveraging the flock and close functions to ensure
that all the writes in user space are actually ﬂushed to the
kernel when a process unlocks or closes a ﬁle. Inside a process,
the problem of coherence also exists if an application writes
a part of a ﬁle and then reads it. In this case, the process
will not see its own write since the write is only stored in the
log and not in the Linux page cache. We solve this problem
by updating the stale pages in case they are read. Since this
reconciliation operation is costly, we use a read cache that
keeps data up-to-date for reads. As the read cache is redundant
with the kernel page cache, we can keep it small because it
only improves performance in the rare case when a process
writes and quickly reads the same part of a ﬁle.
As a result, because it combines all the advantages of state-
of-the-art systems, our design becomes remarkably simple
to deploy and use. In a nutshell, NVCACHE is a plug-
and-play I/O booster implemented only in user space that
essentially consists in an NVMM write cache. NVCACHE
also implements a small read cache in order to improve the
performance when a piece of data in the kernel page cache
is stale. Finally, using legacy kernel interfaces, NVCACHE
asynchronously propagates writes to the mass storage with a
dedicated thread. Table I summarizes the advantages of our
system. By adding strong persistence guarantees, NVCACHE
©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. Pre-print version. Presented in the 51th IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN ’21). For the ﬁnal published version, refer to DOI 10.1109/DSN48987.2021.00033
TABLE I: Properties of several NVMM systems, all fully compatible with the POSIX API.
Ext4-DAX [20], [56] NOVA [57]
SplitFS [33] DM-WriteCache [53]
+
−
−
+ (Any)
+
+
NVCACHE
+
+
+
+ (Any)
+
+
−
+
+
+
+
+ (Ext4)
Offer a large storage space
Efﬁcient for synchronous durability
Durable linearizability [28]
Reuse legacy ﬁle systems
Stock kernel
Legacy kernel API
−
++
+
−
−
+
prevents any rollback effect
in case of crash with legacy
software, such as DBMS, and obviates the need for a developer
to handle data inconsistencies after crashes. Using NVCACHE
reduces code complexity without sacriﬁcing performance, and
thanks to persistence, the cache layer becomes transparent and
reliable, unlike the Linux default RAM page cache.
Our design provides three main beneﬁts. First, NVCACHE
can easily be ported to any operating system compliant with
the POSIX interface: NVCACHE is only implemented in user
space and just assumes that the system library provides basic
I/O functions (open, pread, pwrite, close, fsync).
This design choice only adds the cost of system calls on a non-
critical path, while drastically simplifying the overall design.
Second, by design, NVCACHE offers a durable write cache
that propagates the writes to the volatile write cache of the
kernel before propagation to the durable mass storage. While
using a volatile write cache behind a durable write cache could
seem unexpected, this design has one important advantage:
NVCACHE naturally uses the volatile write cache to decrease
I/O pressure on the mass storage without adding a single
line of code in the kernel. In details, instead of individually
ﬂushing each write that modiﬁes a single page to the mass
storage, the kernel naturally combines the writes by updating
the modiﬁed page in the volatile page cache before ﬂushing the
modiﬁed page to disk only once. Strata also combines writes
in volatile memory but, because it cannot leverage the kernel
page cache by design, Strata must implement combining in
its own kernel module. Finally, NVCACHE naturally beneﬁts
from the numerous optimizations provided by the modern
stock kernels it builds upon, e.g., arm movements optimization
for hard drives [47], [9] or minimization of write ampliﬁcation
for solid-state drive (SSD) [46], [44].
This paper presents the design, implementation and evalua-
tion of NVCACHE. Using I/O-oriented applications (SQLite,
RocksDB), we compare the performance of: NVCACHE with
a SATA SSD formatted in Ext4, a RAM disk (tmpfs), a direct
access (DAX) ﬁle system backed by a NVMM (Ext4-DAX),
a ﬁle system tailored for NVMM (NOVA), an SSD formatted
with Ext4 boosted by DM-WriteCache and a classical SSD
formatted with Ext4. Our evaluation notably shows that:
• Under synchronous writes, NVCACHE reduces by up to
10× the disk access latency of the applications as compared
to an SSD, even when using DM-WriteCache.
• NVCACHE is as fast as (or faster than) Ext4-DAX and often
as efﬁcient as NOVA, but without limiting the working set
of the application to the available NVMM.
• On half of our workloads, NVCACHE remains less efﬁcient
than NOVA (up to 1.8×). We show that this performance
limitation results from the reliance on a generic I/O stack
Strata [37]
+
++
+
−
−
−
−
++
+
−
−
+ (Ext4)
to avoid modiﬁcations to the kernel.
• NVCACHE is implemented in 2585 LoC only in user space,
while offering the same advantages as Strata (large stor-
age space and advanced correctness properties) with its
21 255 LoC, 11 333 of which are in the kernel.
This paper is organized as follows. We ﬁrst describe the
design of NVCACHE in §II and detail its implementation in
§III. In §IV, we present our evaluation using legacy applica-
tions and real-world workloads. We survey related work in §V,
before concluding with some perspectives in §VI.
II. NVCACHE
As outlined in introduction, we have designed NVCACHE
with three different goals. First, we want to use NVMM to
offer fast synchronous durability. With NVCACHE, instead of
routing data writes ﬁrst to the volatile kernel cache, which
is risky as it can lead to data loss upon crash, applications
directly and synchronously write to durable storage. Further-
more, write performance is only limited by the achievable
throughput of the actual NVMM.
Second, NVCACHE supports legacy applications. Our de-
sign allows applications to persist data at native NVMM
speed without the size limitations of specialized ﬁle systems
for NVMM, and without requiring any modiﬁcations to the
application code.
Finally, NVCACHE does not require new kernel interfaces
and code, reducing the risk of introducing new attack vectors
and simplifying maintainability. Our design also leverages the
numerous existing optimizations implemented in the kernel of
the operating system for the storage sub-systems. In particular,
we take advantage of the kernel page cache to increase
read performance and to combine writes in memory before
propagating them to the mass storage.
A. Approach and workﬂow
NVCACHE implements a write-back cache in NVMM and
executes entirely in user space. This design choice avoids
kernel modiﬁcations and shortens the path between the appli-
cation and the I/O stack. Speciﬁcally, NVCACHE intercepts
the writes of the applications and caches them in a log,
stored in NVMM. Then, it asynchronously propagates them
to the disk through the kernel using regular I/O system calls.
Because NVCACHE asynchronously propagates written data
to the kernel, the kernel page cache may contain stale data.
For a modiﬁed data, NVCACHE has thus to reconstruct a fresh
state by applying the last writes recorded in the NVCACHE log
during a read operation. Since this reconciliation operation is
costly, NVCACHE also implements a small read cache in user
space in volatile memory to keep data up-to-date for reads.
Figure 1 gives a high-level overview of the architecture
and workﬂow. NVCACHE intercepts application calls to I/O
2
TABLE II: Page states.
Is page in DRAM read cache?
No
Yes
Loaded
Loaded
Unloaded dirty
Unloaded clean
Yes