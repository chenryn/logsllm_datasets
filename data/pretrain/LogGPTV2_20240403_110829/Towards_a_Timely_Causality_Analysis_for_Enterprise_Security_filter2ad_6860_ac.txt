EdgeCount function counts the number graph edges whose
rareness score is greater than a given threshold θ. Empirically,
we set θ to be 0.1 and set
to be 60
minutes. Note that these values can be customized for speciﬁc
environments and security requirements.
time limit Tlimit
We then utilize the Hill Climbing [29] algorithm to achieve
the optimization of Equation 3. This algorithm can gradually
improve the quality of weight selection via feedback based
method. We have implemented such a feedback loop, which
takes a set of starting events E and an initial weight vector (α,
β) as inputs. To create the starting event set E, we randomly
select 1,113 system events, within a timespan of 10 months
from August 2016 to May 2017, which lead to excessively
large dependency graphs (up to 73,221 edges with 2,391
edges on average). At each iteration, the algorithm adjusts
an individual element in the weight vector and determines
whether the change improves the value of objective function
f (E, (α, β)). If so, such a positive change is accepted, and
the process continues until no positive change can be found
any more. Eventually, the algorithm produces the optimized
weight parameters, where α = 0.27 and β = 0.73.
Note that the rareness and fanout features demonstrate a
trade-off between analysis coverage and time effectiveness.
The fact that the weight of fanout is three times as much as
that of rareness indicates the trained tracking system prefers to
quickly expand the search area to reach a global optimal. As a
result, on one hand, it tends to prioritize low-fanout events
and avoid high-fanout events that cause the search to sink
into a very busy local neighborhood. On the other hand, it
depends less on the rareness score of the current event under
examination because it cannot adequately reﬂect the overall
rareness of following events. This in fact reveals a limitation
of our reference model, which quantiﬁes rareness in a context-
insensitive fashion. We discuss the potential improvement in
Section VI.
D. Implementation
We have developed the priority-based dependency tracker
in 20K lines of Java code. When acquiring the enabling
information (i.e., rareness, fanout and write-only/read-only),
we pay special attention to runtime efﬁciency in order to cope
with the massive amounts of system events collected from large
enterprises. Particularly, we introduce several optimization
techniques to accelerate data query.
1) In-Memory Key-Value Store: Our tracking algorithm
requires frequent access to reference database in order to
query reference score of individual events. Traditional database
persisted on hard disks cannot satisfy such performance
6
requirements. As a result, we store the reference data in
RocksDB [30], which on one hand enables an in-memory key-
value store for fast access, and on the other hand can still
persist data in the traditional way.
2) Event Cache: To compute the fanout of an event or to
determine if an event reaches a read-only or write-only ﬁle,
we enable a look-ahead method to examine a further one hop
of dependencies. In fact, these additional query results are not
only used for the current computation of priority scores, but
also later become part of result dependency graph. Thus, to
avoid redundant query overhead, we cache these results for
future usages.
3) Look-Ahead with a Limit: Sometimes, the fanout of
an event is extremely high. For instance, a Firefox process may
touch hundreds of temporary ﬁles. In this case, counting the
exact fanout via database query is very time-consuming, and
could lead to degradation of runtime efﬁciency. Besides, in
such a case, the exact value of fanout becomes less interesting
in terms of computing and comparing the priority score.
Therefore, we approximate the fanout by putting a limit n on
the query, so that it only looks for the ﬁrst n events that are
dependent on the current one. In effect, if the fanout is greater
than n, the fanout score f s(e) is in practice deﬁned to be 1/n
instead of 1/f anout(e).
IV. REFERENCE MODEL
In this section, we elaborate on the reference model, which
quantiﬁes the rareness of system events and helps distinguish
the anomalies from noisy normal system operations. First, we
give the details of data collection in an enterprise IT system.
Next, we formally deﬁne the reference score of a system event,
which is a crucial factor in the rareness score.
A. Data Collection
To build the reference model of system events, we collect
and compute the statistical data for event occurrences on
54 Linux and 96 Windows machines used daily for product
development, research and administration in an enterprise IT
system. Particularly, we make special efforts to ensure the
representativeness, generality and robustness of the reference
model.
1) Discovery of Homogeneous Hosts: The basic idea
of reference model is to identify common behaviors across
a group of homogeneous hosts. Therefore,
to enable this
technique, the homogeneity of hosting environment is required;
otherwise, the generated model cannot be representative.
In general, enterprise IT systems could satisfy such a
requirement due to the overall consistency of daily tasks.
However, it is still possible that computers from individual
departments in the same corporate carry on different types of
workloads, and therefore their system behaviors may vary. To
be able to discover the homogeneous groups, we performed
a community detection within an enterprise. Particularly, we
utilized the Mixed Membership Community and Role model
(MMCR) proposed in a prior study [31] and eventually dis-
covered 3 communities within 150 machines. In fact, these
3 communities can be roughly mapped to three different
departments in this company. Hence, we collect system events
(cid:104)abstract-event(cid:105) ::= (cid:104)process-event(cid:105)
(cid:104)ﬁle-event(cid:105)
(cid:104)network-event(cid:105)
(cid:104)process-event(cid:105) ::= (cid:104)process(cid:105) (cid:104)process-op(cid:105) (cid:104)process(cid:105)
(cid:104)ﬁle-event(cid:105)
(cid:104)network-event(cid:105) ::= (cid:104)process(cid:105) (cid:104)network-op(cid:105) (cid:104)socket(cid:105)
(cid:104)process(cid:105)
(cid:104)ﬁle(cid:105)
(cid:104)socket(cid:105)
(cid:104)process-op(cid:105)
(cid:104)ﬁle-op(cid:105)
|
|
::= (cid:104)process(cid:105) (cid:104)ﬁle-op(cid:105) (cid:104)ﬁle(cid:105)
::= (cid:104)executable-path(cid:105)
::= (cid:104)path-name(cid:105)
::= (cid:104)remote-address(cid:105) ‘:’ (cid:104)remote-port(cid:105)
::= ‘create’
|
‘destroy’
::= ‘read’
|
‘write’
|
‘execute’
::= ‘create’
|
‘destroy’
|
‘read’
|
‘write’
(cid:104)network-op(cid:105)
Fig. 3: An Abbreviated Syntax of Event Abstraction
from 3 communities separately and build a reference model for
each of the detected communities. In this way, the generated
models can be adapted for individual environments.
2) Abstraction of Events: To quantify the rareness of
system events, our reference model builder expects to count
the occurrences of same events. Nonetheless, OS events are
highly diverse over time or across hosts, even if they bear
the same semantics. For instance, the same program can bear
several process IDs when it has been executed for multiple
times; two identical system ﬁles are assigned with different
inode numbers on two Linux hosts.
To capture high-level common behaviors, while tolerating
low-level system diversity, we summarize events using their
invariant properties. To this end, we ﬁrst extract semantic-
level information from system objects. Particularly, a process
is modeled using its executable path, a ﬁle is represented by its
path name, and a socket is denoted with remote IP address plus
remote port number. Then, on top of these representations, we
construct the abstraction of events, which follows a grammar
illustrated in Figure 3 using Backus-Naur (BNF) form. As a
result, events sharing the same abstraction are considered to
be same ones.
Note that, due to customization, the path name of same
system ﬁles may still be different on individual hosts. For
example,
the user account name can be part of the path
name, which in turn becomes unique for each user. To allow
such differences, normalization of path name is needed. We
address this problem by retrieving a mapping between user
account name and the corresponding home directory name
from both local machines and global directory services (e.g.,
active directory, NIS), and replacing the home directory name
in the path with the same wildcard.
3) Time Window: The na¨ıve way to count the occurrence
is simply increasing the counter, whenever a
of an event
same one is observed. Nevertheless,
this may be subject
to poisoning attacks. An adversary can intentionally create
repeated malicious activities, and such a burst of vicious events
7
we present the details of experiment setup. Next, we introduce
the metrics and the attack cases used for the evaluations. Then,
we provide some insights into the common system operations
recorded in the reference model.
A. Experiment Setup
We perform all experiments on 54 Linux and 96 Windows
machines used daily by researchers, developers and administra-
tors in an enterprise IT environment, with an audit log system
capturing OS-level events on host machines. We evaluate
PRIOTRACKER on a dataset
including 1TB of 2.5 billion
events collected from 150 hosts in one week. Our dataset is
orders of magnitude larger than the ones used in previous
work [9], [20]. ProTracer [9], which is an instrumentation-
based tracker, was tested on only 2GB event data; Sleuth [20],
a real-time heuristics-based attack graph builder, used merely
20GB data from 6 isolated hosts and did not support cross-host
tracking.
To evaluate the correctness and time effectiveness of our
approach, we test PRIOTRACKER in eight representative at-
tacks as described in Table I, including data theft, the infa-
mous Shellshock attack, email phishing, Backdoor, as well
as attacks and test cases proposed in prior work [9], [25].
The difference is that prior work simply crafted “clean” attack
traces isolated from normal system operations. However, in
practice, noise (i.e., complex and normal system operations)
is always interleaved with attack traces due to program logic,
shared ﬁles and long-running processes. From the daily logs
of the enterprise, we observe several typical normal events
which connect the malicious activities to benign ones and thus
introduce a tremendous amount of noise to the causality graph,
and list some examples in Table II. To incorporate the impacts
of such noise, we reproduced eight representative attacks in
a “noisy” environment where numerous normal activities are
considered.
To further verify the time efﬁciency in real-world causality
tracking, we randomly select 75 points of interest (POI), which
take excessive time to analyze.
To perform comparative experiments, we also implement
a baseline forward-tracker following the prior approach [4],
[5], which does not consider priority at all. Instead,
this
system enables breadth-ﬁrst search when processing incoming
dependencies. We do not use depth-ﬁrst search because it does
not have the capability to escape from a deep branch and thus
is usually less effective than breadth-ﬁrst search.
We run causality trackers on a server equipped with In-
tel(R) Xeon(R) CPU E5-1650 CPU (12M Cache, 3.20GHz)
and 64GB of physical memory. The operating system is
Ubuntu 12.04.5 LTS (64bit). Without loss of generality, we
only perform forward tracking instead of bi-directional analy-
ses for all the attacks and selected POIs in the experiments. Our
reference model is constructed and stored on the same server.
Only local I/O overhead is caused when causality tracker
queries reference database.
Fig. 4: Computation of Reference Score
may trick the na¨ıve model to believe that these are common
behaviors due to their high counts.
To address this problem, we introduce a time window when
increasing counters. Within a single time window, repeated oc-
currence of an event on the same host will only be considered
once. As a result, a sudden spike of recurring events only
cause limited impacts. We conﬁgure the time window to be
one week. This is because enterprises are generally operated on
weekly basis. Besides, host behaviors within and without work
hours, or system activities on weekdays and weekends can be
fairly different by nature. Thus, a time window greater than
a week can avoid such a vibration of event occurrence while
preserving high-level consistency of corporate workloads. Note
that the time window is conﬁgurable and can be adjusted to
different enterprise systems.
B. Reference Score
With the aforementioned factors being considered, we
formally deﬁne the reference score of a system event.
Deﬁnition 3. The Reference Score ref of an OS-level event
e is its accumulative occurrence on all homogeneous hosts for
all weeks.
ref (e) =
count(e, w, h)
(3)
(cid:88)
(cid:88)
h∈hosts
w∈weeks
, where hosts is the set of homogeneous machines, weeks
represents the set of weeks when data is collected, and
count(e, w, h) =
0, otherwise
if e occurred in week w on host h
(cid:26)1,
1) Implementation: When computing the score, we in fact
update it incrementally using an online algorithm. As depicted
in Figure 4, we maintain a total count and a bit-vector of
current week for each abstracted event. The bit-vector indicates
the occurrence of event on all hosts in the current week, where
each bit represents a host. The present data can only affect the
existence of event in the current week, and thus will be checked
against the bit-vector. By the end of each week, the total count
is updated using the bit-vector and the vector will be cleared.
In this way, we only store the minimum necessary data so as
to ensure efﬁcient storage and query.
V. EVALUATION
B. Accuracy
In this section, we conduct experiments to evaluate the cor-
rectness, effectiveness and efﬁciency of PRIOTRACKER. First,
We use our attack cases to evaluate the accuracy of
PRIOTRACKER. To this end, we forward-track all the attacks
8
Week h1 h2 h3 h4 h5 Total Count 1 x x 2 2 x x 4 3 x x x 7 4 x x x 10 1 1 0 1 0 Bit-vector for current week TABLE I: Overview of Attack Cases.
Attack Case
Data Theft
Phishing Email
Shellshock
Netcat Backdoor
Cheating Student
Illegal Storage
wget-gcc
passwd-gzip-scp
Description of Scenario
An insider stole sensitive intellectual property, secure-copied the data to a low-proﬁle