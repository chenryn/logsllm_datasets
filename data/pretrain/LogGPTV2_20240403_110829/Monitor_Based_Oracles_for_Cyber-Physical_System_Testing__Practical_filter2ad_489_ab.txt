If
C. Safety Speciﬁcation
To evaluate the use of these techniques we created partial
behavioral speciﬁcations that were motivated by ensuring
system safety. We used six safety rules that checked a mix
of system robustness and functionality.
Since the feature under test is a third party provided code
module designed mainly as a placeholder function to support
early system integration, there was no available speciﬁcation.
While it would be ideal to have a system speciﬁcation from
which to derive monitoring rules, that was not the case for the
available systems we had to test. Instead we created a set of
speciﬁcation rules based on “expert” elicited common sense
(i.e., properties a knowledgable engineer could expect to hold
based on automotive domain experience) through discussions
with the system’s engineers and looking over existing system
metrics and other potentially related documentation. While
we would have preferred to have rules directly derived from
system documentation, this is not always possible in industry
(as in this example). In cases like this the usefulness of the
monitoring results depend heavily on the experts and the
quality of the rules they choose. Though expert derived rules
may not provide as clear a notion of monitoring coverage,
they can be made with the expert’s direct needs in mind. For
example, while the rules we check in this work are in no
way complete, they would be high priority (likely leading to
vehicle collisions) for a production quality feature. The six
rules we checked against the robustness testing traces were:
Rule #0 If
then
the ServiceACC signal
true,
is
ACCEnabled must be false.
A simple consistency check to ensure that the feature
does not continue to attempt to control the vehicle when
it knows something is wrong.
Rule #1 If the actual vehicle headway time is below 1.0s,
then it must be recovered to greater than 1.0s within 5s
elapsed time.
This rule is derived from an existing headway metric for
another similar system
Rule #2 If TargetRange is less than half the desired
headway, then RequestedTorque should not be in-
creasing.
Check for feature trying to increase speed when it is
already too close to the target vehicle
RequestedTorque
Rule #3 If Velocity is greater than ACCSetSpeed
0,
and
RequestedTorque must still be less than 0 in
the next timestep.
Check for vehicle attempting to increase speed when
already above the set speed, avoiding tripping on
control oscillations by only checking after there are no
active requests.
Rule #4 If Velocity is greater than ACCSetSpeed
then RequestedTorque must not be increasing some
point within 400ms.
Similar to #3: if vehicle velocity is increasing while
above set speed, should start slowing down (or at least
hold speed) within 400ms
than
less
is
Rule #5 If
BrakeRequested
is
RequestedDecel must be less
to 0.
Checks if the value of a requested deceleration is in
fact a deceleration (negative).
then
than or equal
true
Rule #6 If VehicleAhead is true and TargetRange
is less than 1, then TorqueRequest must be false or
RequestedTorque must be less than 0.
Checks for near collisions – assuming a feature should
not be requesting a increase in speed when the target
vehicle is extremely close.
Because these rules were picked without any knowledge
of the internal control algorithms or design parameters of
the system, some of them may be too strict (this turned out
to be the case as we shall see). It seems likely that this
sort of approach would be common when applying runtime
monitoring to real-world systems, which often have incom-
plete speciﬁcations and opaque internal operation. Thus, the
approach we took was to adopt these rules and then relax
them when false positives and uninteresting violations were
found. We think this is a reasonable approach to employing
runtime monitors in practice.
The issue of whether the data required to implement the
monitor would be observable was simple since we were able
to create our rules with knowledge of this restriction (and
thus write rules based on system state available on the CAN
bus). Observability would be a more difﬁcult issue when
deriving rules from system requirements which may include
151151151
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:20:37 UTC from IEEE Xplore.  Restrictions apply. 
requirements on properties that are not externally observable.
We discuss this further in Section V-D.
Coverage of the safety rules is not intended to be complete.
Rather, the idea is to express a set of safety rules that are
useful and see if runtime monitoring detects rule violations
for a black-box system which has not been augmented with
additional monitoring information.
IV. TESTING RESULTS
For each of the eight target signals we ran three tests –
one Ballista-style injection, one bit ﬂip test in which one,
two, and four bit bit-ﬂips were injected, and one random
value injection. Random and Ballista testing included eight
different injection values per test and bit-ﬂips included four
injections for each bit-ﬂip size (with all
injections held
for 20s). We also ran eight
tests of 20 injection values
on multiple target signals at once (e.g., TargetRange,
VehAhead, and TargetRelVel at the same time). Testing
time was limited by the physical time to run tests on real
automotive hardware setups. Statistical analysis of robustness
testing techniques was not a goal of this work. The number of
tests were sufﬁcient to demonstrate that monitoring detected
problems under robustness fault injection and indicated a lack
of problems (to the degree possible given available data) in
non-faulted operation.
The results of the robustness testing identiﬁed many
speciﬁcation violations. The testing results are summarized
in Table I. An “S” represents a rule satisﬁed by the
given trace, while a “V” represents a violated rule. The
mBallista, mRandom, and mBitFlip entries are tests where
more than one message was targeted at once. “Range+”
injected TargetRange, TargetRelVel and VehAhead.
“Range+Set” also included SetSpeed, and “All” was all 9
FSRACC inputs. Six out of the seven rules were detected
as violated during testing (all except Rule #0). Many of the
violations could be caused, and were detected, by multiple
test runs (i.e. different signals being targeted or different
types of injections to the same signal).
All three types of robustness testing found similar ro-
bustness problems in the system under test. This is not
an unreasonable outcome, because all
three fault classes
easily exercised out-of-range faults that caused most of the
identiﬁed violations.
A major identiﬁed cause of problems was the lack of input
checking in the feature. The Velocity, TargetRange,
TargetRelVel, and ACCSetSpeed messages all have
direct and strong effects on the control output, but are neither
bounds checked (for exceptional
inputs) nor consistency
checked against each other or other inputs. This makes them
vulnerable to a bad input value causing the control algorithm
to command an unsafe output. For example, an exceptional
TargetRange value when following a target causes the
ACC feature to command the vehicle to accelerate into (and
through, because the simulator doesn’t check collisions) the
TABLE I
FAULT INJECTION RESULTS
Speciﬁcation Rule
Target Signal
Velocity
TargetRange
TargetRelVel
ACCSetSpeed
ThrotPos
AccelPedPos
BrakePedPos
SelHeadway
Velocity
TargetRange
TargetRelVel
ACCSetSpeed
ThrotPos
AccelPedPos
BrakePedPos
SelHeadway
Velocity
TargetRange
TargetRelVel
ACCSetSpeed
ThrotPos
AccelPedPos
BrakePedPos
SelHeadway
Range+
All
Injection
Random
Random
Random
Random
Random
Random
Random
Random
Ballista
Ballista
Ballista
Ballista
Ballista
Ballista
Ballista
Ballista
Bitﬂips
Bitﬂips
Bitﬂips
Bitﬂips
Bitﬂips
Bitﬂips
Bitﬂips
Bitﬂips
mBallista
mBallista
mRandom Range+
mRandom All
mRandom Range+Set
mBitﬂip1
mBitﬂip2
mBitﬂip4
Range+
Range+
Range+
0
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
1
V
S
V
V
S
S
S
S
S
V
V
S
S
S
S
S
V
V
V
V
S
S
S
S
V
V
V
V
V
V
V
V
2
S
V
S
S
S
S
S
S
V
S
S
V
S
S
S
S
V
S
S
S
S
S
S
S
S
S
V
S
S
S
V
S
3
V
S
S
V
S
S
S
S
S
S
S
V
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
V
S
4
S
V
S
S
S
S
S
S
S
S
S
V
S
S
S
S
V
S
S
S
S
S
S
S
V
S
V
S
S
S
V
S
5
S
S
S
S
S
S
S
S
V
V
S
S
S
S
S
S
V
V
V
V
S