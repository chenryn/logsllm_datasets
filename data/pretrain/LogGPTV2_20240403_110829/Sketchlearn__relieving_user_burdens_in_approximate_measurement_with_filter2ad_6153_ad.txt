frequency exceeding 1
of the total frequency, it will con-
c
tribute at least 1
2 of the frequency to the stack it is hashed to,
after other large flows in the stack are extracted.
Recommended configurations. Administrators need to
configure the number of rows and columns for a multi-level
sketch. For the number of rows, our evaluation suggests that
one row suffices to work well (see §7). For the number of
columns, it can be configured based on the memory budget,
or by specifying a guaranteed frequency and translating it
to the number of columns using Theorem 2.
Internal parameters. The model inference introduces two
internal parameters. Note that administrators need not be
concerned about their actual values; in fact, their current
settings work well for various types of traffic statistics based
on our evaluation (see §7).
The first parameter is θ. Currently, we start with 1
2. The
initial value is motivated by Theorem 2 to provide guarantees
on how large flows are extracted and make the residual
sketch converge quickly. The subsequent values of θ are
halved iteratively to control the extraction procedure. Our
experience is that our large flow extraction procedure works
for any decreasing sequence of θ.
The second parameter is the probability threshold to as-
sign the k-bit for a candidate flowkey in large flow extraction,
in which we now employ a sufficiently large value 0.99. The
rationale behind is that we will assign a ∗ for the k-bit whose
probability is below the value. A large value results in more ∗
and eventually reduces the errors of wrong bit assignments.
Discussion. Our model addresses Limitations L1 to L3. First,
Theorems 1 and 2 collectively guarantee the correctness of
model inference, so that we do not need to configure any
expected error probabilities as input (L1 addressed). While
SketchLearn still employs two user-specified parameters,
both parameters are straightforward to configure since ad-
ministrators can easily tell the minimum flow size of in-
terest or the memory budget in their devices. Second, for
flows smaller than 1
of the total frequency, SketchLearn also
c
strives to extract them although they are not theoretically
guaranteed. Our experience is that even for 50% of the guar-
anteed frequency (i.e., 1
), more than 99% of flows are still
2c
extracted (see Experiment 7 in §7.3). Thus, SketchLearn is
robust to very small thresholds (L2 addressed). Finally, our
model inference is iterative until the results fit both theo-
rems. Thus, administrators can simply follow the theorems
and do not need further tuning (L3 addressed).
5 QUERY RUNTIME
Administrators can query for various traffic statistics through
SketchLearn’s query runtime, which extracts the required
information through the learned model. We now show how
SketchLearn supports standard traffic statistics (§5.1) and
more sophisticated queries (§5.2). We further discuss how
SketchLearn realizes network-wide measurement (§5.3).
5.1 Traffic Statistics
Per-flow frequency. To query for the frequency of a given
flow, SketchLearn first looks up the large flow list and re-
turns the frequency estimate if found. Otherwise, it queries
the residual sketch to estimate the frequency and bit-level
probabilities, as in Steps (iii) and (iv) in §4.3.
Heavy hitters. SketchLearn compares all extracted flows in
the large flow list for a given heavy hitter detection threshold
(recall that the threshold is not used for resource configu-
rations). Note that SketchLearn can extract flows with rela-
tively small frequencies (see §4.4). For example, with 256KB
memory, almost all flows whose frequencies are above 0.01%
of the overall frequency are extracted (Experiment 8 in §7.3).
Thus, it suffices to only consider the extracted flows in the
large flow list for some practical detection threshold.
Heavy changers. Detecting heavy changers across two
consecutive epochs combines the queries for per-flow fre-
quencies and heavy hitters. Specifically, SketchLearn first
identifies heavy hitters exceeding the threshold used in
heavy changer detection in either one of the two consec-
utive epochs. It then queries for the per-flow frequency of
each identified heavy hitter in the other epoch. It calculates
the frequency change and returns the flows with change
exceeding the threshold.
Cardinality. SketchLearn computes the flow cardinality
based on Theorem 1, which relates the number of flows
n remaining in the residual sketch to probability p[k] and its
2[k]
2[k]. Since our model has learned p[k] and σ
variance σ
for each k, SketchLearn estimates n as n = p[k](1−p[k])c
with
every k. In addition, it adds the number of extracted flows
in the large flow list. To avoid outliers, SketchLearn returns
the median over all k.
Frequency distribution and entropy. The residual sketch
compresses the information of flow frequency distributions
in its counters. SketchLearn restores the distribution with
the MRAC technique [38], which employs Expectation Max-
imization to fit the observed counter values. With the fre-
quency distribution, many other statistics, such as entropy,
can also be computed. Administrators can also employ other
estimation approaches (e.g., [11]).
σ 2[k]
5.2 Extended Queries
SketchLearn proposes two extensions: (i) attaching error
measures to measurement results; and (ii) enabling arbitrary
flowkey definitions.
Attaching error measures. The estimated statistics are de-
rived from both the residual sketch and the extracted large
flows. We address the errors in both parts.
First, the errors from the residual sketch depend on the
correctness of Gaussian distributions, including the goodness
of fitting and the quality of the estimated p[k] and σ
2[k]. In
§4.4, we show that SketchLearn is guaranteed to extract large
flows, so the residual sketch fits Gaussian distributions well.
In addition, the estimates of both p[k] and σ
2[k] are derived
from a large number (hundreds or even thousands) of coun-
ters in the level-k residual sketch, and they are unbiased and
have minimum variance (see §4.3). Thus, the errors caused
by the residual sketch are negligible and can be discarded.
Second, the errors of the extracted large flows are due to
the presence of false positives. SketchLearn associates the
vector of l bit-level probabilities as the error measure for
each extracted large flow. Administrators can use the bit-
level probabilities to decide if any flow should be excluded.
For example, we can employ a simple error filter that discards
the flows whose 50% of bits have a bit-level probability lower
than 90%. Our evaluation shows that this error filter effec-
tively eliminates almost all false positives (Experiment 10 in
§7.3). Administrators can also apply other filters as needed.
We argue that proposing such an error filter is much easier
than specifying errors in resource configurations prior to
measurement (see Limitation L1) since administrators can
tune the error filter after collecting the measurement results.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Arbitrary flowkey definitions. SketchLearn can measure
traffic statistics for any combination of bits in the original
flowkey definition (e.g., any subsets of the 104 bits in 5-
tuple flows) from both the large flow list and the residual
sketch. For the former, SketchLearn simply sums up the
flows based on the specified flowkey definition; for the latter,
SketchLearn only examines the levels for the bits of interest.
One important issue is that when flows are grouped into
hyperflows based on some specified flowkey definition (e.g.,
source-destination address pairs), a hyperflow may appear
in multiple stacks, since the flows derived from the orig-
inal flowkey definition (e.g., 5-tuples) are hashed to mul-
tiple columns. Thus, a hyperflow may have an inaccurate
frequency if we miss its associated 5-tuple flows in some
stacks. Fortunately, we observe that flows belonging to the
same hyperflow exhibit some skewness distribution [5], so
each hyperflow often has some large flows extracted. Thus,
SketchLearn queries all stacks for every hyperflow grouped
by existing large flows. It extracts a hyperflow from a stack
and adds its frequency if its bit-level probabilities pass our
error filter. After the extraction, SketchLearn recomputes the
mean and variance of the residual sketch to preserve Gauss-
ian distributions before estimating traffic statistics. After the
estimation, the newly extracted hyperflows are inserted back
to the residual sketch.
5.3 Network-wide Coordination
SketchLearn allows administrators to access part of or all
measurement points to compute network-wide measurement
statistics. In particular, it conveniently supports network-
wide deployment and network-wide integration.
Network-wide deployment. SketchLearn can be deployed
in any hardware/software switch or server host. By varying
the hash functions across measurement points, we essentially
provide more rows for a multi-level sketch. Thus, in practice,
it suffices to allocate one row per multi-level sketch in each
measurement point. Furthermore, we do not restrict deploy-
ment decisions (e.g., task placement), making SketchLearn
orthogonal to previous deployment approaches for network-
wide measurement (e.g., [47, 48, 59]).
Network-wide integration. Since a flow typically tra-
verses multiple measurement points, we can improve our
model learning for a particular measurement point by lever-
aging the results from others. Specifically, we examine the
bit-level probabilities of each extracted flow along its tra-
versed path. If a flow fails to pass our error filter in a majority
of measurement points in the path, we drop it. On the other
hand, if a flow is missed in only one measurement point
yet its bit-level probabilities are high in other measurement
points along the path, we query for its frequency and bit-level
probabilities in the missing measurement point. If the results
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Huang et al.
are consistent with those of other measurement points, we
extract this flow from the missing measurement point. How
to aggregate results from multiple measurement points de-
pends on measurement tasks and network topologies, and
we leave the decision to administrators. We show some case
studies in our evaluation (see §7.4).
6 IMPLEMENTATION
We implement a prototype of SketchLearn, including its soft-
ware data plane, hardware data plane, and control plane.
Software data plane. We build the software data plane atop
OpenVSwitch (OVS) [52], which intercepts and processes
packets in its datapath. OVS has two alternatives: the original
OVS implements its datapath as a kernel module, while an
extension, OVS-DPDK, puts the datapath in user space and
leverages the DPDK library [22] to bypass the kernel.
We propose a unified implementation for both OVS and
OVS-DPDK. We connect the datapath and the SketchLearn
program with shared memory, which is realized as a lock-free
ring buffer [39]. When the data plane intercepts a packet, it in-
serts the packet header into the ring buffer. The SketchLearn
program continuously reads packet headers from the ring
buffer and updates its multi-level sketch.
The major challenge for the software data plane is to mit-
igate the per-packet processing overhead, as each packet
incurs at most r ×(l +1) updates. We address this using single
instruction multiple data (SIMD) to perform the same opera-
tion on multiple data units with a single instruction. To fully
utilize SIMD, we allocate counters of the same stack as one
contiguous array. Currently, we employ 32-bit counters, and
note that the latest avx512 instruction set can manipulate 512
bits (i.e., 16 32-bit counters) in parallel. For 5-tuple flows with
104 bits (i.e., 105 levels), we divide the array into ⌈ 105
16 ⌉ = 7
portions. For each portion, we execute four SIMD instruc-
tions: (i) _mm512_load_epi32, which loads 16 counters from
memory to a register array; (ii) _mm512_maskz_set1_epi32,
which sets another register array whose element is set to the
packet frequency if the k-bit is one, or zero if the k-bit is zero;
(iii) _mm512_add_epi32, which calculates the element-wise
sum of the two arrays; and (iv) _mm512_store_epi32, which
stores the first register array back to memory.
P4 data plane. We use P4 [53] to demonstrate that SketchLearn
can be implemented in hardware. P4 is a language that spec-
ifies how switches process packets. In P4, one fundamental
building block is a set of user-defined actions that describe
specific processing logic. Actions are installed in match-
action tables, in which each action is associated with a user-
defined matching rule. Each table matches packets to its
rules and executes the matched actions. Our current imple-
mentation realizes each level of SketchLearn counters as an
array of registers, which can be directly updated in the data
Measurement tasks
Approximate solutions
Misra-Gries (MG) [46]
Lossy Count (Lossy) [43]
Space Saving (SS) [44]
Fast Path (FP) [32]
Heavy hitter (HH) detection
Heavy hitter (HC) detection Deltoid (Del) [17]
Per-flow Frequency