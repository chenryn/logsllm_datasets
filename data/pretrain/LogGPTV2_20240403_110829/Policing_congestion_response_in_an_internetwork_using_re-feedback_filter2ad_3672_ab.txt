derestimate their path delay.
With no further changes in local delays, packets in the
following round (dotted) correctly predict the path again.
Of course, changes in the unloaded delay at a node (e.g.
due to a lower layer re-route) are rare, at least in ﬁxed net-
works. However, for more volatile metrics like congestion,
change is the norm. For delay, the prediction error will be
(cid:1)n−1
i=0 (mi(t+T ) − mi(t)). For congestion, it is given by Eqn
(11) in Appendix A.1. In both cases, the error depends on
the diﬀerence between the whole path metrics.
To put these errors in context, re-feedback causes a source
to suﬀer the same path prediction error as classic feedback—
for equivalent path changes within the last round trip. So
a re-feedback source transport can extract the same infor-
mation, with the same timeliness and apply the same rate
control algorithms with the same dynamics. For relays, it
can take up to an extra half round trip before path changes
reach them. But, for relays, any downstream path predic-
tion at all is an improvement over classic feedback, which
2In contrast to the proposed ECN standard [19] where con-
gestion is deﬁned as the output of the RED algorithm—
leaving no objective basis for improving RED.
(cid:1)j−1
i=0 mi = h0 − hj. Node j can work
stream of node j is
this out by examining hj in packets as they arrive, because
h0 is well-known. So the receiver (with j = n) can charac-
terise the whole path delay h0−hn. If it feeds back hn to the
sender using our notional end-to-end protocol (bent arrows
in Fig 2a)), the sender can know the path delay too. So
far, we have said nothing new, merely introducing notation
using a familiar example.
With re-feedback the trick is simply for the sender to
choose an initial header value such that, if the path met-
ric were to remain unchanged, the header would reach a
well-known value hz at the destination—rather than start-
ing from a ﬁxed value. In our numerical example in Fig 2b)
we assume the industry has standardised hz = 16.
Although that is really all there is to it, we will now trace
through how re-feedback works step by step to be precise
about the diﬀerences:
1. For now, we will assume that the source bootstraps the
very ﬁrst packet of a ﬂow with the ﬁxed value we used
with classic feedback, h0(t) = 255.
(When we need
to distinguish between packets, we suﬃx each header
value with the time t at which it was originally sent.)
2. The source has to remember the initial value it chose,
as depicted by the curved boxes containing 255 at each
source in Fig 2b) and containing h0(t) in Fig 1.
3. The packet traverses the path r, combining each lo-
cal delay in turn into its header, using the combining
function (subtraction) already described above.
4. The receiver feeds back the resulting delay header value
hn(t) to the sender, which arrives a round trip Tr after
the ﬁrst packet was sent, depicted by the bent arrows.
5. The sender initialises the delay ﬁeld in the next packet
(dotted) to h0(t+Tr ) = h0(t) − hn(t) + hz as well as
storing this new value in place of the last one. Each
initial delay header value only depends on the previ-
ous round’s initial value and the value fed back—both
known locally at the source.
Now we can see that this simple shift of datum has achieved
our original aim: as each packet arrives at a resource j any-
where in the network, it carries within its header hj a pre-
diction of its own downstream path delay, hj − hz, requiring
no path state on the relay because hz is well-known. Any
packet in Fig 2b) illustrates this point, in that subtracting
hz = 16 from any header value predicts the sum of the re-
maining downstream resources on that path.
The second column of Table 1 summarises the functions to
implement delay re-feedback that we have just derived. The
third column gives the equivalent functions for congestion,
derived in Appendix A.
path
knowledge
align-
ment
up-
sender
stream receiver
down-
sender
stream receiver
sender
(cid:4)
(cid:4)
n/a
n/a
T
2 , T
T
2 , T
(cid:5)
(cid:5)
(cid:4)
(cid:5)
relay
0, T
2
x
(cid:5)
(cid:4)
x
T, 3T
2
(cid:5)
(cid:4)
rcvr
0, T
2
x
n/a
n/a
Table 2: Comparison of sender and receiver-aligned
feedback, by availability of path knowledge (x = not
available; n/a = not applicable) and by range of
timeliness (using symmetric delay).
oﬀers none. And at the ingress, where policers are most
appropriate, responsiveness will be similar to that of the
source. Table 2 summarises the path knowledge that nodes
gain or lose from re-feedback. It also quantiﬁes the range of
how long it can take for path changes to work through into
correct path predictions in each case.
Previously, to achieve such knowledge at every relay would
have required messages to be reverse routed hop by hop
from all destinations (cf. routing messages or congestion
back-pressure). Although re-feedback takes a little longer to
propagate (because it travels via the source), it updates at
the same rate as the ack rate—as often as TCP congestion
control and many orders of magnitude more often than a
typical routing message rate. Also, re-feedback piggy-backs
on existing data, requiring no extra packet processing.
3.
INCENTIVES
We aim to create an incentive environment to ensure any-
one’s selﬁsh behaviour (including lying and cheating) leads
to maximisation of social welfare.3 Throughout this section
we will focus primarily on characterisation of path conges-
tion. This will stress re-feedback incentive mechanisms to
the full in the face of conﬂict over scarce resources. Given
most forms of fairness, including TCP’s, also depend on
round trip time, we will then outline how a path delay metric
would be amenable to similar treatment.
Fig 3 sketches the incentive framework that we will de-
scribe piece by piece throughout this section. An internet-
work with multiple trust boundaries is depicted. The down-
stream path congestion seen in a typical packet is plotted as
it traverses an example path from sender S1 to receiver R1.
They are shown using re-feedback, but we intend to show
why everyone would choose to use it, correctly and honestly.
Two main types of self-interest can be identiﬁed:
• Users want to transmit data across the network as fast
as possible, paying as little as possible for the privilege.
In this respect, there is no distinction between senders
and receivers, but we must be wary of potential malice
by one on the other;
• Network operators want to maximise revenues from
the resources they invest in. They compete amongst
themselves for the custom of users.
Source congestion control: We want to ensure that
the sender will throttle its rate as downstream congestion in-
creases. Whatever the agreed congestion response (whether
3These mechanisms can lie dormant wherever co-operation
is the social norm.
downstream
path
congest
-ion, ρi
0
shaper/
policer
S1
congestion 
control
i
R1
dropper
bulk congestion charging
bulk congestion pricing
N2
N3
N4
N1
traffic eng
routing
N5
Figure 3: Incentive framework
TCP-compatible or some enhanced QoS), to some extent it
will always be against the sender’s interest to comply.
Edge ingress policing/shaping: But it is in all the
network operators’ interests to encourage fair congestion re-
sponse, so that their investments are employed to satisfy
the most valuable demand. N1 is in the best position to en-
sure S1’s compliance and it now has a choice of mechanisms
across a spectrum of customer autonomy. At one extreme,
N1 could give S1 complete autonomy, but encourage respon-
sible behaviour by charging for the downstream congestion
in packets. Or it can shape traﬃc directly itself, removing
all S1’s autonomy. Between the two extremes, it can police
a congestion response agreed upfront with S1 (§3.3).
Edge egress dropper: If the source has less right to a
high rate the higher it declares downstream congestion, it
has a clear incentive to understate downstream congestion.
But, if packets are understated when they enter the internet-
work, they will be negative when they leave. So, we intro-
duce a dropper at the last network egress, which drops pack-
ets in ﬂows that persistently declare negative downstream
congestion (see §3.2).
Inter-domain traﬃc policing: But next we must ask,
if congestion arises downstream (say in N4), what is the
ingress network’s (N1) incentive to police its customers’ re-
sponse? If N1 turns a blind eye, its own customers beneﬁt
while other networks suﬀer. This is why all inter-domain
QoS architectures (e.g. Intserv, Diﬀserv) police traﬃc each
time it crosses a trust boundary. Re-feedback gives trust-
worthy information at each trust boundary so the congestion
response can be policed in bulk.
Emulating policing with inter-domain congestion
charging: However, between high-speed networks, we would
rather avoid holding back traﬃc while it is policed. Instead,
once re-feedback has arranged headers to carry downstream
congestion honestly, N2 can contract to pay N4 in proportion
to a single bulk count of the congestion metrics ρ crossing
their mutual trust boundary (§3.4). Then N2 has an in-
centive either to police the congestion response of its own
ingress traﬃc from N1 or to charge N1 in turn on the ba-
sis of congestion counted at their mutual boundary. In this
recursive way, each ﬂow’s response can be precisely incen-
tivised, despite the mechanism not recognising ﬂows. If N1
turns a blind eye to its own upstream customers’ congestion
response, it will still have to pay its downstream neighbours.
No congestion charging to users: Bulk congestion
charging at trust boundaries is passive and extremely simple,
and loses none of its per-packet precision from one boundary
to the next. But at any trust boundary, there is no impera-
tive to use congestion charging. Traditional traﬃc policing
can be used, if the complexity and cost is preferred. In par-
ticular, at the boundary with end customers (e.g. between
S1 and N1), traﬃc policing will most likely be far more ap-
propriate. Policer complexity is less of a concern at the edge
of the network. And end-customers are known to be highly
averse to the unpredictability of congestion charging [15].
So note well: this paper neither advocates nor requires con-
gestion charging for end customers and advocates but does
not require inter-domain congestion charging.
Competitive discipline of inter-domain traﬃc engi-
neering: With inter-domain congestion charging, a domain
seems to have a perverse incentive to fake congestion; N2’s
proﬁt depends on the diﬀerence between congestion at its
ingress (its revenue) and at its egress (its cost). So overstat-
ing internal congestion seems to increase proﬁt. However,
smart border routing [10] by N1 will bias its multipath rout-
ing towards the least cost routes, so N2 risks losing all its
revenue to competitive routes if it overstates congestion. In
other words, N2’s ability to raise excess proﬁts is limited by
the price of its second most competitive route.
Closing the loop: All the above elements conspire to
trap everyone between two opposing pressures (upper half
of Fig 3), ensuring the downstream congestion metric arrives
at the destination neither above nor below zero. So we have
arrived back where we started in our argument. The ingress
edge network can rely on downstream congestion declared
in the packet headers presented by the sender. So it can
police the sender’s congestion response accordingly.
3.1 The case against classic feedback
So why can’t classic congestion feedback (as used already
by standard ECN) be arranged to provide similar incentives?
Superﬁcially it can. Given ECN already existed, this was the
deployment path Kelly proposed for his seminal work that
used self-interest to optimise social welfare across a system
of networks and users [14]. The mechanism was nearly iden-
tical to volume charging; except only the volume of pack-
ets marked with congestion experienced (CE) was counted.
However, below we explain why relying on classic feedback
meant the incentives traced an indirect path—the long way
round the feedback loop. For example, if classic feedback
were used in Fig 3, N2 would incentivise N1 via N4, R1 &
S1 rather than directly.
Inability to agree what happened: In order to po-
lice its upstream neighbour’s congestion response, the neigh-
bours should be able to agree on the congestion to be re-
sponded to. Whatever the feedback regime, as packets change
hands at each trust boundary, any path metrics they carry
are veriﬁable by both neighbours. But, with a classic, sender-
aligned path metric, they can only agree on the upstream
path congestion—its oﬀset from its well-known datum at
the sender.
Inaccessible back-channel: The network needs a whole
path congestion metric to control the source. Classically,
whole path congestion emerges at the destination, to be
fed back from receiver to sender in a back-channel. But,
in any data network, back-channels need not be visible to
relays, as they are essentially communications between the
end-points. They may be encrypted, asymmetrically routed
or simply omitted, so no network element can reliably in-
tercept them. The congestion charging literature solves this
problem by treating the sender and receiver as entities with
aligned incentives. Although measuring classic ECN mark-
ing rates relative to their datum at the sender forces a ‘re-
ceiver pays’ model (at each trust boundary the downstream
neighbour pays), at least this incentivises the receiver to
refer the charges to the sender.
‘Receiver pays’ unacceptable: However, in connec-
tionless datagram networks, receivers and receiving networks
cannot prevent reception from malicious senders, so ‘receiver
pays’ opens them to ‘denial of funds’ attacks.
End-user congestion charging unacceptable: Even
if ’denial of funds’ were not a problem, we know that end-
users are highly averse to the unpredictability of congestion
charging and anyway, we want to avoid restricting network
operators to just one retail tariﬀ. But with classic feedback,
we cannot avoid having to wrap the ‘receiver pays’ money
ﬂow around the feedback loop, necessarily forcing end-users
to be subjected to congestion charging.
To summarise so far, with classic feedback, policing con-
gestion response requires congestion charging of end-users
and a ‘receiver pays’ model. Whereas, with re-feedback, in-
centives can be fashioned either by technical policing mech-
anisms (more appropriate for end users) or by congestion
charging (more appropriate inter-domain) using the safer
‘sender pays’ model.
Impractical traﬃc engineering: Finally, classic feed-
back makes congestion-based traﬃc engineering ineﬃcient
too. Network N4 can see which of its two alternative up-
stream networks N2 and N3 are less congested. But it is N1
that makes the routing decision. This is why current traﬃc
engineering requires a continuous message stream from con-
gestion monitors to the routing controller. And even then
the monitors can only be trusted for intra-domain traﬃc en-
gineering. The trustworthiness of re-feedback enables inter-
domain traﬃc engineering without messaging overhead.
We now take a second pass over the incentive framework,
ﬁlling in the detail more formally.
3.2 Honest congestion reporting
An honest sender will declare a certain downstream path
metric (DPM) ρ0 in packets to aim for zero at the destina-
tion after allowing for path congestion. We deﬁne cheating
as the diﬀerence ∆ρ0c relative to this ideal, taking overstate-
ment as positive. To rely on the DPM packets carry, we
must discourage dishonesty, whether positive or negative. If
the sender declares a certain DPM, a certain rate response
will be permitted, which can be policed (§3.3). For any safe
congestion response, the higher the sender’s declared DPM,
to some degree the slower its data rate, and the lower the
value it derives. So, to the right of Fig 4 we can show the
sender’s utility strictly decreasing with overstatement.