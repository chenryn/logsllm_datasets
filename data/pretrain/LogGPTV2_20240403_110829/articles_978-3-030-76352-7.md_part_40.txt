Methods
This section presents the terminology used in this paper for different types of
anomalies,theproblemstatement,andthemultipleimplementationsofbox-plot
256 A. Shahid et al.
Table 1.CurrentState-of-the-artinIndustry(OpenSourceandCommercial).Abbre-
viations: US (Unsupervised), F (Forecasting), SL (Statistical Learning), OSB (Open-
Source Benchmarks), and PD (Production Data)
ADsolutions US F/SL Generalized EvaluationOSB/PD Online Complexity
AlibabaRobustTAD[6]  F  OSB  High
FacebookProphet[18]  F  OSB Medium
AmazonDeepAR+[16]  F  OSB High
UberRNN[9]  F  OSB High
MicrosoftSR-CNN[14]  F  PD  Medium
TwitterSH-ESD[8]  SL PD Low
ProposedSLMAD  SL  OSBandPD  Low
based statistical outlier detection rules used in SLMAD. The anomalies in time
series data can be categorised into two types: point anomalies and contextual
anomalies. A data point is considered a point anomaly if its value is far outside
the range of data set in which it is found. A contextual anomaly, however, is an
instance in a data-set that is an outlier based on the context [3].
Problem Statement: The problem of anomaly detection in time series data has
beenexploredindifferentscientificfields,suchascomputerscience,biology,and
astronomyusingavarietyoftechniques(Sect.2).Thedrivetowardsautonomous
networks,inacloud-basedproductionenvironment,yieldsaspecializedproblem
of detection for anomalies in metrics (produced in a run-time environment) by
maintaining the high performance and keeping resource cost low. The goal of
achieving a higher performance translates into low computational complexity,
online decision-making, higher accuracy and efficiency, and the ability to detect
anomalies for streaming data. The low resource cost of the algorithm means
low utilization of computing resources, such as CPU and memory, as well as a
reduction in the storage costs. Finally, the anomaly detection approach must be
able to detect all types of anomalies (point and contextual) without assuming
the shape of the time series.
Box-plot Implementations in SLMAD: We now present three box-plot meth-
ods for anomaly detection implemented in SLMAD. We used the box-plot rules
in combination with custom build methodologies using dynamic grouping and
matrix profile (discussed in Sect. 3.2). The first Turkey’s box-plot can be con-
structed using three quantities: first quantile (Q1), second quantile (Q2), and
thirdquantile(Q3).TheTukey’slowerandupperboundaries(LB t andUB t)are
definedusingtheconceptofinterquartilerangeorIQR(where,IQR=Q3−Q1)
as shown in Eq. 1.
LB =Q1−1.5×IQR
t
(1)
UB =Q3+1.5×IQR
t
The second implementation is based on the robust measure of scale, i.e.,
medianabsolutedeviation(MAD),andknownasrobustbox-plot.Thelowerand
upperboundariesforrobustbox-plot(LB r andUB r)arecalculatedusingEq.2.
SLMAD: Statistical Learning-Based Metric Anomaly Detection 257
Intermsofaccuracy,therobustbox-plotperformsbetterthanTukey’sbox-plot
[2].Bydefault,SLMADtrainsthemodelsusingrobustbox-plotsbecauseofour
objective to achieve higher performance.
LB =Q1−1.44×MAD
r
(2)
UB =Q3+1.44×MAD
r
The third box-plot implementation is based on Bowley’s coefficient that
adjusts the fences and overcomes some statistical limitations of Tukey’s box-
plot [20]. The lower and upper boundaries of Bowley’s box-plot (LB b and UB b)
can be calculated using Eq. 3.
SIQR
LB b =Q1−1.5×( SIQRl )
u
(3)
SIQR
UB b =Q3+1.5×( SIQRl )
u
Where SIQR l =Q2−Q1 and SIQR u =Q3−Q2.
3.2 Construction of SLMAD Framework
Figure1 gives an overview of the three-stage process of SLMAD: 1) analysis, 2)
dynamic grouping, 3) modelling and evaluation. Figure2 presents the stages of
the SLMAD algorithm in more detail. Since one of our objectives is to have an
anomaly detectionframework withlowtime-complexity, wedesignedandtested
each stage of SLMAD to have a worst-case complexity of O(n).
Fig.1. Overview of the SLMAD framework
We now briefly explain each stage of SLMAD:
1. Analyze time series
– We first use the statistical methods to evaluate a small subset of time
seriesdataintermsofitscharacteristics.Wecheckwhetherthetimeseries
isstationaryusingtheaugmentedDickey-Fuller(ADF)andKwiatkowski
Phillips Schmidt Shin (KPSS) test. If both tests conclude that the series
is stationary, it is labeled as stationary. If only the ADF test confirms
that the series is stationary then it is labelled difference stationary and if
only the KPSS test confirms the data is stationary, we label it as trend
stationary.
258 A. Shahid et al.
Fig.2. Workflow of the SLMAD framework
– We then check if the time series is discrete or continuous. If there are
morethantwentydistinctpointsinthedatasetwelabelitascontinuous,
otherwise, it is discrete.
– We make sure that the time series is not flat, i.e., it does not possess a
constant dimension for all the timestamps.
– We check if the time series is seasonal using auto-correlation and if sea-
sonalityisdetected,wefindthebestperiod.Theseasonalityanalysisand
period determination methodology are given in Fig. 3. If the autocorre-
lation is over 0.9, we label the data as seasonal. We then determine the
best period using peaks in the time series and calculate the number of
points between the peaks as the period.
2. Dynamic Grouping
– Based on the output of the analysis stage, we build the following two
cases, case 1 and case 2:
– Case 1: Provided the time series is stationary, continuous, and seasonal,
we use a subset of data as a training set and group them based on the
bestperiodusingadynamicgroupingapproach.Sinceatleastfivepoints
are needed to build a box-plot, we use equal to five or more periods and
group them.
– Case 2: If the time series does not possess all the aforementioned charac-
teristics in case 1, we use the Matrix Profile for streaming data (STUMP
Incremental or STUMPI) to determine the discords in time series. The
top discords are the anomalies. We select Matrix Profile as it can work
without a need for parameter tuning and does not assume the shape of
time series.
3. Model Training and Evaluation
– Finally in Stage 3, we train the models and evaluate them with new
data. We build models for cases 1 and 2 in the dynamic grouping stage
separately as explained below.
– Case 1: For each group, we build one box-plot. For example, if the best
period is 10, we use at least 50 points to build the models in the training
set. In other words, 10 box-plot thresholds are constructed.
SLMAD: Statistical Learning-Based Metric Anomaly Detection 259
Fig.3. Methodology for seasonality and period analysis
– Case2: ThediscordsintheMatrixProfilegivenbySTUMPIaredirected
upwards. Therefore, we use the upper bound of a robust box-plot (UP r)
todetectthetopdiscords.WecallthecustomMatrixProfileandbox-plot
approach as STUMPI-BP and employ it for all types of time series that
are non-stationary, non-seasonal, or discrete. For case 2, we found that
STUMPI-BP successfully detects the contextual anomalies in complex
time series but misses some prominent point anomalies. Therefore, we
use a robust box-plot constructed from the points equal to the window-
size (default=100) and rank the anomalies reported by both methods.
– The models are also updated at the run-time. For case 1, we update the
box-plot threshold for each group as soon as we collect the data equal to
one period. For case 2, we update the Matrix Profile for each streaming
data point. The results can then be saved to a file, visualised, or used as
the start of an automated remediation process.
4 Experimental Setup
4.1 Data Set Characteristics
We use multiple data sets including open-source anomaly detection benchmarks
andin-houseHuaweinetworkproductiondatatoevaluateourmodel.Theopen-
sourcebenchmarksincludelabelleddata-setsfromNumentaBenchmarkingSuite
and Yahoo lab7. Due to the high complexity of labelling the data from the
production environment, the in-house collected metrics at Huawei are labelled
using a simple statistical-based labelling methodology that is build upon box-
plot thresholds.
260 A. Shahid et al.
We find that a total of 96 metrics collected in Huawei network’s production
environmentareseasonalandcontinuouswiththebestperiodequaltoonehour
(foundusingperiodanalysismethodologyinFig.3).ThesemetricsincludeKPIs
such as network response times, utilizations, etc. The data has been collected
for a duration of four months with a sampling frequency of one hour (i.e., 24
samples per day). We group the metrics using a K-means clustering algorithm
into 14 distinct clusters with unique patterns. Out of each group, we pick one
time series for evaluation of SLMAD. We label the selected time series as ts1,
ts2,...,ts14.Themetricsarecollectedforfourmonths.ToevaluatetheSLMAD,
we use the 70% of data to train the models and 30% as test data sets.
4.2 Evaluation Metrics
We use a combination of metrics to evaluate that our approach is accurate,
efficient,andgeneralizable.Toevaluatetheaccuracy,weusetheprecision,recall,
andF1scoreoftheactualandidentifiedanomalies.Toevaluatetheresourcecost,
we measure the CPU and memory utilization as well as the training and testing
time. We evaluate the generalizability of our approach by testing the anomaly
detectionaccuracyonanumberofdifferentdatasets.Wecompareourapproach
againstalightweightpersistenceanomalydetectionmodelthatusestheprevious
datapointasaforecastforthenextpointandclassifiesthepointasananomaly
if the actual point is outside the standard deviation expected.
5 Results
InTable2,weevaluatedarangeofcontinuousanddiscrete-timeseriesdatafrom
the art daily data set in the Numenta benchmark suite. Four of the time series
(Art daily no noise, Art noisy, Art flatline, and Art daily perfect square wave)
have no points labelled as anomalies and SLMAD does not report any false
anomalies for them. We also evaluated the non-stationary and complex data
sets(cpu utilization asg misconfigurationandec2 cpu utilization 5f5533)where
no best period could be found by using STUMPI-BP. Our proposed SLMAD
approachachieveshighprecisionacrosstherangeofdatasets.Whenwecombine
the precision and recall scores in the F1 score and compare it against the per-
sistence baseline approach, we can see that SLMAD has achieved better results
for each of the data sets.
Table 3 shows the results for the real data in the Yahoo benchmark. The
recall values are slightly lower than precision given the unbalanced amount of
anomalies in the data set. We can see that the SLMAD approach achieved an
improved F1 score compared to the persistence approach again for all of the
data sets. Finally, we evaluate our approach in a production environment using
network data from Huawei, as shown in Table 4. We can see that the SLMAD
approach achieved higher precision scores than recall, due to the unbalanced
nature of production anomaly data. Overall, the SLMAD approach shows an
improvedF1scoreforalloftheproductiondatasetscomparedtothepersistence
model.
SLMAD: Statistical Learning-Based Metric Anomaly Detection 261
Table 2. Numenta metrics accuracy
Numenta metrics Precision Recall SLMAD F1 Persistence F1
Art daily Flat middle 0.65 1 0.78 0.75
Art daily Jumps down 0.936 0.377 0.537 0.50
Art daily Jumps up 0.932 0.4 0.56 0.53
Art daily Spike density 1 0.75 0.85 0.77
Art daily No jump 0.92 0.39 0.55 0.54
cpu utilization asg misconfig 0.83 0.48 0.60 0.55
ec2 cpu utilization 5f5533 0.72 0.56 0.63 0.61
Table 3. Yahoo metrics accuracy Table 4. Huawei metrics accuracy
Dataset Precision Recall SLMADF1 PersistF1 Data Precision Recall SLMADF1 PersistF1
real7 0.966 0.9 0.92 0.90 ts1 0.83 0.53 0.64 0.57
real9 0.99 0.959 0.974 0.827 ts2 0.96 0.49 0.64 0.62
real15 0.988 0.7 0.821 0.74 ts3 0.66 0.73 0.69 0.63
real17 0.973 0.974 0.974 0.84 ts4 0.88 0.57 0.69 0.68
real20 0.959 0.873 0.91 0.9 ts5 0.76 0.56 0.64 0.62
real26 0.867 0.825 0.847 0.48 ts6 0.82 0.66 0.73 0.72
real31 0.982 0.843 0.9 0.66 ts7 0.69 0.65 0.67 0.63
real34 0.994 0.956 0.973 0.6 ts8 0.82 0.73 0.77 0.75
real46 0.9 0.788 0.82 0.72 ts9 0.86 0.53 0.65 0.64
real47 0.98 0.95 0.972 0.84 ts10 0.91 0.57 0.70 0.67
real51 0.99 0.93 0.961 0.84 ts11 0.98 0.63 0.76 0.75
real55 0.99 0.72 0.83 0.89 ts12 0.77 0.75 0.76 0.74
real58 0.98 0.962 0.968 0.831 ts13 0.78 0.65 0.70 0.63
real65 0.98 0.61 0.744 0.72 ts14 0.97 0.56 0.71 0.70
Table5 shows the results for SLMAD when evaluated in terms of efficiency
and resource consumption. The results are obtained on a virtual machine that
is representative of a modern multicore platform with 4 CPU cores and 32GB
ofmemory.Weimplementedthebox-plotmodelswithadynamicgroupingapp-
roach in Python3 and Golang to analyse their resource costs, training, and test-
ingtimes.SLMADtrainson10Kdatapointsin0.072sand100Kdatapointsin
0.5s. This allows our framework to update online for a large number of points.
The Golang implementation is much more efficient than the Python implemen-
tation,dramaticallyreducingthepackagesize(upto≈60×),CPUandmemory
utilization.TheGolangimplementationyieldssubstantialimprovementsof98%,
36%, 95%, and 99% in terms of package size, CPU consumption, memory con-
sumption,andexecutiontimes,respectively.ThisimprovementisduetoGolang
being better able to handle concurrency problems.
262 A. Shahid et al.
Table 5. Efficiency and Resource Costs for SLMAD
Parameters Go Python
Package size 7.5 MB 447MB
CPU utilization 65% 102%
Memory utilization 4320KB 102056KB
Training time (with 1K data points) 0.014s 15s
Training time (with 10K data points) 0.072s 70s
Training time (with 100K data points) 0.501s N/A
Testing time (With 1K data points) 0.5s 0.74s
6 Conclusion and Future Work
Time series anomaly detection is a critical module for operations and manage-
ment in a production environment. A resource-efficient, general, and accurate
anomaly detection system is indispensable in real applications and can be used
to trigger root cause analysis (RCA) and automated remediation. This paper
has introduced a lightweight time series anomaly detection framework that has
shown improved F1 scores on a number of anomaly detection benchmarks when
comparedagainstapersistence-basedanomalydetectionapproach.Wealsoeval-
uated our approach using real production data from Huawei, where it showed
impressiverecallandprecisionscoreswhilemaintaininglowtrainingandtesting
times as well as low CPU and memory utilization.
For future work, we will evaluate our framework against a number of other
anomaly detection approaches using the current anomaly detection benchmarks
aswellasadditionalproductiondata.Oncewehaveevaluatedthebestanomaly
detection approach for our production environment, we will use it to trigger
RCA and automated remediation.