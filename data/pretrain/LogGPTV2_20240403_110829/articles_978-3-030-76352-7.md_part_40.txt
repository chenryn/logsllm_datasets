# Methods

This section introduces the terminology used in this paper for different types of anomalies, the problem statement, and the multiple implementations of box-plot-based statistical outlier detection rules used in SLMAD (Statistical Learning-Based Metric Anomaly Detection).

## Terminology
Anomalies in time series data can be categorized into two types: point anomalies and contextual anomalies. A point anomaly is a data point whose value is far outside the range of the dataset in which it is found. A contextual anomaly, on the other hand, is an instance in a dataset that is an outlier based on the context [3].

## Problem Statement
The problem of anomaly detection in time series data has been explored in various scientific fields, such as computer science, biology, and astronomy, using a variety of techniques (Sect. 2). In a cloud-based production environment, the drive towards autonomous networks presents a specialized challenge: detecting anomalies in metrics produced in a run-time environment while maintaining high performance and keeping resource costs low. The goal of achieving higher performance translates into low computational complexity, online decision-making, higher accuracy and efficiency, and the ability to detect anomalies in streaming data. Low resource cost means low utilization of computing resources (e.g., CPU and memory) and reduced storage costs. Finally, the anomaly detection approach must be capable of detecting all types of anomalies (point and contextual) without assuming the shape of the time series.

## Box-Plot Implementations in SLMAD
SLMAD implements three box-plot methods for anomaly detection. These methods are combined with custom methodologies using dynamic grouping and matrix profile (discussed in Sect. 3.2).

### Tukey’s Box-Plot
Tukey’s box-plot can be constructed using three quantities: the first quantile (Q1), the second quantile (Q2), and the third quantile (Q3). The lower and upper boundaries (LB_t and UB_t) are defined using the interquartile range (IQR), where IQR = Q3 - Q1, as shown in Eq. 1.

\[
LB_t = Q1 - 1.5 \times IQR
\]

\[
UB_t = Q3 + 1.5 \times IQR
\]

### Robust Box-Plot
The second implementation is based on the robust measure of scale, specifically the median absolute deviation (MAD). The lower and upper boundaries (LB_r and UB_r) for the robust box-plot are calculated using Eq. 2.

\[
LB_r = Q1 - 1.44 \times MAD
\]

\[
UB_r = Q3 + 1.44 \times MAD
\]

In terms of accuracy, the robust box-plot performs better than Tukey’s box-plot [2]. By default, SLMAD trains models using robust box-plots to achieve higher performance.

### Bowley’s Box-Plot
The third box-plot implementation is based on Bowley’s coefficient, which adjusts the fences and overcomes some statistical limitations of Tukey’s box-plot [20]. The lower and upper boundaries (LB_b and UB_b) of Bowley’s box-plot can be calculated using Eq. 3.

\[
SIQR_l = Q2 - Q1
\]

\[
SIQR_u = Q3 - Q2
\]

\[
LB_b = Q1 - 1.5 \times \left( \frac{SIQR_l}{SIQR_u} \right)
\]

\[
UB_b = Q3 + 1.5 \times \left( \frac{SIQR_l}{SIQR_u} \right)
\]

## Construction of the SLMAD Framework
Figure 1 provides an overview of the three-stage process of SLMAD: 1) analysis, 2) dynamic grouping, and 3) modeling and evaluation. Figure 2 presents the stages of the SLMAD algorithm in more detail. Since one of our objectives is to have an anomaly detection framework with low time-complexity, each stage of SLMAD is designed and tested to have a worst-case complexity of O(n).

### Stage 1: Analyze Time Series
- **Stationarity Check**: We use the augmented Dickey-Fuller (ADF) and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) tests to check if the time series is stationary. If both tests conclude that the series is stationary, it is labeled as stationary. If only the ADF test confirms stationarity, it is labeled as difference-stationary. If only the KPSS test confirms stationarity, it is labeled as trend-stationary.
- **Discreteness Check**: We check if the time series is discrete or continuous. If there are more than twenty distinct points in the dataset, it is labeled as continuous; otherwise, it is discrete.
- **Flatness Check**: We ensure that the time series is not flat, i.e., it does not possess a constant dimension for all timestamps.
- **Seasonality Check**: We use auto-correlation to check if the time series is seasonal. If the autocorrelation is over 0.9, we label the data as seasonal. We then determine the best period using peaks in the time series and calculate the number of points between the peaks as the period.

### Stage 2: Dynamic Grouping
- **Case 1**: If the time series is stationary, continuous, and seasonal, we use a subset of data as a training set and group them based on the best period using a dynamic grouping approach. At least five points are needed to build a box-plot, so we use equal to or more than five periods and group them.
- **Case 2**: If the time series does not possess all the characteristics in Case 1, we use the Matrix Profile for streaming data (STUMP Incremental or STUMPI) to determine the discords in the time series. The top discords are the anomalies. We select Matrix Profile because it works without parameter tuning and does not assume the shape of the time series.

### Stage 3: Model Training and Evaluation
- **Case 1**: For each group, we build one box-plot. For example, if the best period is 10, we use at least 50 points to build the models in the training set, constructing 10 box-plot thresholds.
- **Case 2**: The discords in the Matrix Profile given by STUMPI are directed upwards. Therefore, we use the upper bound of a robust box-plot (UB_r) to detect the top discords. We call this custom Matrix Profile and box-plot approach STUMPI-BP and employ it for all types of time series that are non-stationary, non-seasonal, or discrete. For Case 2, we found that STUMPI-BP successfully detects contextual anomalies in complex time series but misses some prominent point anomalies. Therefore, we use a robust box-plot constructed from points equal to the window size (default=100) and rank the anomalies reported by both methods.
- **Model Updates**: The models are updated at run-time. For Case 1, we update the box-plot threshold for each group as soon as we collect data equal to one period. For Case 2, we update the Matrix Profile for each streaming data point. The results can then be saved to a file, visualized, or used to trigger automated remediation processes.

## Experimental Setup

### Data Set Characteristics
We use multiple datasets, including open-source anomaly detection benchmarks and in-house Huawei network production data, to evaluate our model. The open-source benchmarks include labeled datasets from the Numenta Benchmarking Suite and Yahoo Lab. Due to the high complexity of labeling data from the production environment, the in-house collected metrics at Huawei are labeled using a simple statistical-based labeling methodology built upon box-plot thresholds.

We find that a total of 96 metrics collected in Huawei's network production environment are seasonal and continuous, with the best period equal to one hour (found using the period analysis methodology in Fig. 3). These metrics include KPIs such as network response times and utilizations. The data has been collected for four months with a sampling frequency of one hour (i.e., 24 samples per day). We group the metrics using a K-means clustering algorithm into 14 distinct clusters with unique patterns. From each group, we pick one time series for evaluating SLMAD. We label the selected time series as ts1, ts2, ..., ts14. The metrics are collected for four months. To evaluate SLMAD, we use 70% of the data to train the models and 30% as test datasets.

### Evaluation Metrics
We use a combination of metrics to evaluate the accuracy, efficiency, and generalizability of our approach. To evaluate accuracy, we use precision, recall, and F1 score of the actual and identified anomalies. To evaluate resource cost, we measure CPU and memory utilization, as well as training and testing time. We evaluate the generalizability of our approach by testing the anomaly detection accuracy on a number of different datasets. We compare our approach against a lightweight persistence anomaly detection model that uses the previous data point as a forecast for the next point and classifies the point as an anomaly if the actual point is outside the standard deviation expected.

## Results

### Numenta Metrics Accuracy
Table 2 shows the evaluation of a range of continuous and discrete time series data from the Numenta benchmark suite. Four of the time series (Art daily no noise, Art noisy, Art flatline, and Art daily perfect square wave) have no points labeled as anomalies, and SLMAD does not report any false anomalies for them. We also evaluated non-stationary and complex datasets (cpu utilization asg misconfiguration and ec2 cpu utilization 5f5533) where no best period could be found using STUMPI-BP. Our proposed SLMAD approach achieves high precision across the range of datasets. When we combine the precision and recall scores in the F1 score and compare it against the persistence baseline approach, we see that SLMAD has achieved better results for each of the datasets.

### Yahoo Metrics Accuracy
Table 3 shows the results for the real data in the Yahoo benchmark. The recall values are slightly lower than precision due to the unbalanced amount of anomalies in the dataset. We can see that the SLMAD approach achieved an improved F1 score compared to the persistence approach for all of the datasets.

### Huawei Metrics Accuracy
Finally, we evaluate our approach in a production environment using network data from Huawei, as shown in Table 4. We can see that the SLMAD approach achieved higher precision scores than recall, due to the unbalanced nature of production anomaly data. Overall, the SLMAD approach shows an improved F1 score for all of the production datasets compared to the persistence model.

### Efficiency and Resource Costs for SLMAD
Table 5 shows the results for SLMAD when evaluated in terms of efficiency and resource consumption. The results are obtained on a virtual machine representative of a modern multicore platform with 4 CPU cores and 32GB of memory. We implemented the box-plot models with a dynamic grouping approach in Python3 and Golang to analyze their resource costs, training, and testing times. SLMAD trains on 10K data points in 0.072s and 100K data points in 0.5s, allowing our framework to update online for a large number of points. The Golang implementation is much more efficient than the Python implementation, dramatically reducing the package size (up to ≈60×), CPU and memory utilization, and execution times. This improvement is due to Golang's better handling of concurrency problems.

## Conclusion and Future Work
Time series anomaly detection is a critical module for operations and management in a production environment. A resource-efficient, general, and accurate anomaly detection system is indispensable in real applications and can be used to trigger root cause analysis (RCA) and automated remediation. This paper has introduced a lightweight time series anomaly detection framework that has shown improved F1 scores on a number of anomaly detection benchmarks when compared against a persistence-based anomaly detection approach. We also evaluated our approach using real production data from Huawei, where it showed impressive recall and precision scores while maintaining low training and testing times and low CPU and memory utilization.

For future work, we will evaluate our framework against a number of other anomaly detection approaches using current anomaly detection benchmarks and additional production data. Once we have evaluated the best anomaly detection approach for our production environment, we will use it to trigger RCA and automated remediation.