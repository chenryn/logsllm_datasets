### Performance Metrics
- 1.89 ms
- 3.42 ms
- 0.27 ms
- 21.17 ms
- 0.10 ms
- 1.41 ms
- 26.38 ms
- 0.01 ms

### Evaluation and Decryption Process
After the evaluation, the cloud returns a single packed ciphertext to the authority. The output is then decrypted using the secret key, and the authority computes the `argmax` of 10 scores for each image to obtain the prediction. These procedures take approximately 0.72 milliseconds, resulting in an amortized time of 0.01 milliseconds per image. Finally, the data owner receives the results from the authority.

### Model Accuracy
This model achieves an accuracy of 98.1% on the test set. This accuracy is consistent with the results obtained by evaluating the model in the clear, indicating that there is no precision loss from the approximate homomorphic encryption.

### Comparison with Previous Work
#### Table 9: Privacy-Preserving Neural Network Frameworks for MNIST
| Framework | Method | Offline Runtime (s) | Online Runtime (s) | Total Runtime (s) | Amortized Time (ms) | Offline Communication Cost (MB) | Online Communication Cost (MB) | Total Communication Cost (MB) |
|-----------|--------|-------------------|-------------------|------------------|--------------------|--------------------------------|--------------------------------|-------------------------------|
| CryptoNets | HE     | -                 | -                 | 570              | 70                 | 595.5                          | 1280                           | 47.6                          |
| MiniONN   | HE, MPC | 0.88              | 1.28              | 2.16             | 26                 | 0                              | 0.5                            | 0.5                           |
| GAZELLE   | HE, MPC | 0.03              | 0.03              | 0.06             | 30                 | 0.07                           | 0.5                            | 0.57                          |
| E2DM      | HE     | 0.37              | 0.07              | 0.44             | 0.01               | 0.37                           | 0.07                           | 0.44                          |

### HE-Based Frameworks
We used a similar network topology to CryptoNets but with different numbers of nodes in the hidden layers. Our scenario and underlying cryptographic primitive differ. CryptoNets took 570 seconds to perform a single prediction, resulting in an amortized rate of 70 milliseconds. In our case, data is represented in matrix form and applied to the evaluation of neural networks using homomorphic matrix operations. As a result, E2DM achieves a 340-fold reduction in latency and a 34-fold reduction in message sizes. Although CryptoNets allows more SIMD parallelism, it requires a very large number of predictions to yield better amortized complexity, making it less competitive in practice.

### Mixed Protocol Frameworks
Liu et al. [34] presented the MiniONN framework for privacy-preserving neural networks using a ciphertext packing technique as a pre-processing tool. Recently, Juvekar et al. [28] introduced GAZELLE, which leverages the automorphism structure of an underlying HE scheme to perform matrix-vector multiplication, significantly improving performance. GAZELLE takes 30 milliseconds to classify one image from the MNIST dataset and has an online bandwidth cost of 0.5 MB. Despite achieving relatively fast run-time, these mixed protocols require interaction between protocol participants, resulting in high bandwidth usage.

### Related Works
#### 7.1 Secure Outsourced Matrix Computation
Matrix multiplication can be performed using a series of inner products. Wu and Haven [48] suggested the first secure inner product method in a SIMD environment. Their approach encrypts each row or column of a matrix into an encrypted vector and obtains the component-wise product of two input vectors by performing a single homomorphic multiplication. However, this method requires aggregating all elements over the plaintext slots, necessitating at least log d automorphisms. Applying this solution to each row of A and each column of B results in a total complexity of d^2 multiplications and d^2 log d automorphisms.

Recent approaches have been proposed by applying the encoding methods of Lauter et al. [40] and Yasuda et al. [49] on an RLWE-based HE scheme. Duong et al. [17] proposed encoding a matrix as a constant polynomial in the native plaintext space, reducing secure matrix multiplication to a single homomorphic multiplication over packed ciphertexts. This method was later improved in [37]. However, this solution has a significant drawback: the resulting ciphertext contains non-meaningful terms in its coefficients, requiring decryption and re-encoding to remove these terms.

Most related works focus on verifiable secure outsourcing of matrix computation [11, 4, 38, 19]. In these protocols, a client delegates a task to an untrusted server, which returns the computation result with a proof of correctness. General results [20, 15, 19] on verifiable secure computation outsourcing have been achieved by applying fully HE schemes with Yaoâ€™s Garbled circuit or pseudo-random functions. However, these theoretical approaches are still far from practical for real-world applications.

#### 7.2 Privacy-Preserving Neural Networks Predictions
Privacy-preserving deep learning prediction models were first considered by Gilad-Bachrach et al. [23], who presented the private evaluation protocol CryptoNets for CNNs. Subsequent works have improved it by normalizing weighted sums before applying the approximate activation function [10] or by employing fully HE to evaluate arbitrary deep neural networks [7].

Other approaches for privacy-preserving deep learning prediction are based on MPC [5, 41] or its combination with (additively) HE. These hybrid protocols evaluate scalar products using HE and compute activation functions (e.g., threshold or sigmoid) using MPC techniques. Mohassel and Zhang [39] applied the mixed-protocol framework of [16] to implement neural networks training and evaluation in a two-party computation setting. Liu et al. [34] presented MiniONN to transform an existing neural network into an oblivious neural network using a SIMD batching technique. Riazi et al. [42] designed Chameleon, which relies on a trusted third party. These frameworks were later improved in [28] by leveraging effective use of packed ciphertexts. Despite improving efficiency, these hybrid protocols result in high bandwidth and long network latency.

### Conclusion and Future Work
In this paper, we presented a practical solution for secure outsourced matrix computation. We demonstrated its applicability by presenting a novel framework, E2DM, for the secure evaluation of encrypted neural networks on encrypted data. Our experiments show that E2DM achieves lower encrypted message sizes and latency compared to CryptoNets.

Our secure matrix computation primitive can be applied to various computing applications, such as genetic testing and machine learning. Future work will include investigating financial model evaluation based on the E2DM framework and extending the matrix computation mechanism for more advanced operations.

### References
[References remain unchanged]

---

This version of the text is more structured, coherent, and professional. It clearly presents the performance metrics, evaluation and decryption process, model accuracy, comparison with previous work, and related works. The conclusion and future work sections are also refined for clarity and coherence.