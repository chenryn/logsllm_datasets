33.6% 45.7% 26.4% 32.9%
66.4% 69.3% 72.9% 70.7%
75.0% 77.9% 80.0% 76.4%
86.4% 82.3% 87.1% 86.4%
86.4% 82.1% 88.6% 88.6%
90.0% 88.6% 88.6% 90.0%
Figure 14: Maze 1 with 7 random
routes.
Figure 15: Maze 2.
Table 3: RF route-prediction accuracy with the varying num-
ber of trees, for the 7 routes in Maze 1.
labels for the routes or the locations, there are two possi-
ble procedures for training the two models: (1) sequential
training and (2) cascaded training. As Figure 13(a) shows,
in sequential training, we train the particle predictor using
the measured cache timing and the measured particle-number
class sequences, and then train the route predictor using the
measured particle-number class sequences and the measured
route/location labels.
However, errors may accumulate in the particle predictor
and the route predictor, harming end-to-end prediction accu-
racy. We choose the cascaded training procedure as depicted
in Figure 13(b). First, the particle predictor is trained the
same way. Then, we use the predicted particle-number class
sequences, rather than the measured particle-number class
sequences, together with the measured route/location labels,
to train the route predictor. Finally, the trained particle predic-
tor and the route predictor are used for the end-to-end attack
evaluation.
5.1.4 Maps for Evaluation
Gazebo: We use two mazes shown in Figure 14 and Figure 15,
which are both partitioned into 16-by-16 grids. The topology
of a simple maze ensures that any grid is reachable and there is
only one possible path. Compared to Maze 2, Maze 1 contains
more branches and less straight lanes.
Oxford: the map used in the Oxford dataset is shown in
Figure 16. We select 7 routes labeled from “01” to “07” .
Model
RF-1
RF-10
RF-20
RF-50
RF-100
RF-200
5-fold
2-fold
Train
10-fold
72.2% 76.2% 70.6% 68.3%
74.6% 73.0% 73.8% 76.2%
75.4% 75.4% 77.0% 77.8%
75.4% 74.6% 78.6% 79.4%
75.4% 75.4% 77.0% 79.4%
77.0% 73.0% 77.8% 80.2%
Table 4: RF route-prediction accuracy with the varying num-
ber of trees, for the 7 routes in Oxford.
5.2
Impact of Random Forest Size
We examine the impact of the size of the random forest model
on the route and location prediction accuracy. We use the
ground-truth particle-number classes rather than predicted
particle-number classes in this study, in order to exclude the
effects of particle predictor.
5.2.1 RF Size for Route Prediction
We compare the route prediction accuracy of the RFs with a
different number of trees. Table 3 and Table 4 show the result
for Maze 1 and Oxford, respectively. The general trend is that
the accuracy increases with the number of trees but the added
beneﬁt decreases with the number of trees.
868    29th USENIX Security Symposium
USENIX Association
Measured route/location labelsParticle PredictorPredicted particle-number class sequenceMeasured cache timingMeasured particle-number class sequenceMeasured route/location labelstrainingtrainingtraininginference(b) Cascaded training(a) Sequential trainingRoute PredictorMeasured cache timingMeasured particle-number class sequencetrainingParticle PredictorRoute Predictor010203040506070502040103060701020304050607Model
RF-1
RF-10
RF-20
RF-50
RF-100
RF-200
2-fold
5-fold
Train
82.1% 53.3% 62.1%
86.6% 69.6% 72.8%
87.5% 64.6% 73.4%
88.9% 65.7% 74.0%
88.1% 66.9% 74.9%
87.5% 67.2% 74.7%
Table 5: The percentage of predictions that are within 3 grids
from the true target location.
Figure 19: L1D route prediction results for the Oxford dataset.
Figure 17: L1D route prediction results for Gazebo.
Figure 18: LLC route prediction results for Gazebo.
5.2.2 RF Size for Location Prediction
We compare the prediction accuracy of the random forest
(RF) with a different number of trees for the training, 2-fold
validation, and 5-fold validation. Table 5 shows the result.
Silimar to the route prediction, the accuracy increases with
the RF size but the added beneﬁt decreases.
5.3 End-to-end Evaluation Results
5.3.1 Route Prediction
We use the RF-100 model for the route prediction task and we
use 10-fold validation for evaluating the prediction accuracy.
Gazebo: We randomly generate seven routes on Maze 1, as
shown in Figure 14, and collect 20 instances for each route.
Figure 17 and Figure 18 show the classiﬁcation results. The
overall route prediction accuracy is 81.4% and 75% for the
L1D and LLC attacks, respectively.
Figure 20: LLC route prediction results for the Oxford dataset.
Oxford: We use 126 sequences collected on the seven routes
in the Oxford dataset for the route prediction. Figure 19 and
Figure 20 show the confusion matrices of the prediction based
on the L1D side channel and the LLC side channel, respec-
tively. The route prediction accuracy is 74.6% and 73.0% for
the L1D and LLC attacks, respectively.
5.3.2 Location Prediction with Gazebo
We use the RF-50 model for the location prediction task. We
evaluate location prediction using the method described in
Section 4.5.2. For each maze, we randomly select 100 grid
centers as destinations. For a simulation run for each desti-
nation, we record the source-to-grid trajectory and the corre-
sponding cache timing measurements and generate multiple
training or validation samples by using the ﬁnal destination
as well as intermediate grid points on the trajectory as target
locations. We then put all these generated trajectories and
corresponding cache timing vectors in the dataset. Samples
generated from the ﬁrst 80 runs are used for training and the
rest are used for validation. For Maze 1 and Maze 2, there are
3,633 and 2,048 samples in the dataset, respectively.
Figure 21 shows the training and validation accuracy of
the models trained using the L1D and LLC attacks on Maze
1. For the location prediction, a wrong prediction label does
not necessarily mean the prediction is far from the actual
location. Thus, we also calculate the Euclidean distance as a
validation error. For the L1D attack, the average validation
error is 2.87 grid cells and 74.6% of the predictions fall within
3 cells. For the LLC attack, the average validation error is
USENIX Association
29th USENIX Security Symposium    869
01020304050607GroundTruth01020304050607Predicted0000110003000100000000110610130010020200020418171918151401020304050607GroundTruth01020304050607Predicted0040003010100202006100000130111210100000311615161314151601020304050607GroundTruth01020304050607Predicted013200000120069003020100010010101200113000002131615151501020304050607GroundTruth01020304050607Predicted900640000012007900202000003100100100021000011616131415(a) Location prediction using L1D.
Figure 21: Training, validation accuracy, and validation error distributions of end-to-end location prediction with Maze 1.
(b) Location prediction using LLC.
3.17 cells and 70.1% of the predictions fall within 3 cells. For
random guesses, the average error is 6.01 cells and 20.2% of
the predictions fall within 3 cells.
For Maze 2, the average validation error is 2.58 grid cells
and 75.2% of the predictions fall within 3 cells for the L1D
attack, and the average validation error is 3.61 cells and 68.7%
of the predictions fall within 3 cells for the LLC attack. The
average error is 7.67 cells and 13.2% of the prediction fall
within 3 cells for random guesses.
5.3.3 L1D Cache vs. LLC Attacks
We summarize the prediction accuracy of the L1D cache
and LLC side-channel attacks for both mazes and RobotCar
experiments in Table 6. As mentioned in Section 5.1.2, the
sampling periods are 100 ms and 300 ms, respectively. The
table also shows the results of L1D attacks with a sampling
period of 300 ms, matching that of the LLC attack.
The results show that both L1D and LLC attacks can predict
a route or a location. For the L1D attacks, the prediction
accuracy is similar for both sampling periods. The accuracy
is slightly higher for the L1D attack than for the LLC attack.
However, the L1D attack is more difﬁcult to perform as it
requires the attack and victim processes to both run on the
same core.
Task-Period
Route
Location
Maze 1
Maze 2
Map
Metric
L1D-100ms
L1D-300ms
LLC-300ms
Random-N.A.
Maze 1
Oxford
Accuracy
81.4%
80.0%
75.0%
14.3%
74.6%
73.0%
73.0%
14.3%
error
2.87
3.03
3.17
6.01
3-grid
74.6%
73.5%
70.1%
20.2%
error
2.58
2.47
3.61
7.67
3-grid
75.2%
78.8%
68.7%
13.2%
Table 6: Comparison of prediction accuracy of the L1D attack
with different sampling periods, the LLC attack, and random
guess.
6 Discussion
6.1 Processor Architecture
We study and demonstrate the proposed side-channel attack
on autonomous vehicles using an x86 platform. The x86 ar-
chitecture is widely used in autonomous vehicle development
including multiple teams during the DARPA Grand Chal-
lenge [22,23,25,51,52,67] as well as more recent commercial
developments by Baidu [2], Waymo [19, 20], and Uber [4].
While we did not investigate the proposed attacks on other
architectures such as ARM, we believe that the attack can
be generalized to other architectures given that cache timing-
channel attacks have been demonstrated in many different
platforms.
870    29th USENIX Security Symposium
USENIX Association
50100150200250Ground Truth Location Label50100150200250Predicted Location LabelTraining Set50100150200250Ground Truth Location Label50100150200250Predicted Location LabelValidation SetValidation error distribution05101520Prediction error (grids)00.10.20.30.40.50.600.10.20.30.40.50.6Model Prediction ErrorRandom Prediction Error50100150200250Ground Truth Location Label50100150200250Predicted Location LabelTraining Set50100150200250Ground Truth Location Label50100150200250Predicted Location LabelValidation SetValidation error distribution05101520Prediction error (grids)00.10.20.30.40.50.600.10.20.30.40.50.6Model Prediction ErrorRandom Prediction Error6.2 Generality of the Vulnerability
We rely on the adaptive behavior of AMCL to perform our at-
tack. In general, we believe that the high-level observation that
an adaptive algorithm can leak information about a vehicle’s
physical state can be generalized to other cyber-physical sys-
tem (CPS) software whose memory access pattern depends on
private physical state. Obviously, not all control/localization
algorithms have such a vulnerability. For example, the data
access pattern of a Kalman ﬁlter or a PID control algorithm is
largely independent of input values, and does not leak phys-
ical state. However, we believe that the adaptive behaviors
will become increasingly common in autonomous system
software for two reasons:
1. To ensure safety and improve estimation accuracy, most
autonomous vehicles have two or more sources of sen-
sor inputs that are fused for better estimation. A simple
Kalman ﬁlter-based estimation method does not work
well in this scenario. Adaptive particle ﬁlter-based es-
timation is more suitable for the state estimation of a
non-Gaussian distribution in a high-dimensional space.
2. In addition to estimation, many perception algorithms,
such as object detection [59] and recognition [58], are
also adaptive and have input-dependent memory access
patterns. The proposed cache side-channel attack may be
extended to exploit such perception algorithms to infer
private physical information.
We note that if multiple software components with adaptive
memory access patterns run on the same machine simultane-
ously, their memory accesses may interfere with each other,
exhibiting more complex patterns. In that case, the machine
learning model for prediction will need to either deal with
interference as noise or be trained with the combined memory
access patterns.
6.3 Limitations of the Attack Model
We provide a proof-of-concept end-to-end attack on inferring
the route/location of an autonomous vehicle. To be successful,
the proposed attack needs a victim autonomous vehicle to
satisfy a few key assumptions:
• The autonomous vehicle uses a control software module
with adaptive computing behavior (e.g., AMCL) where
memory access patterns depend on the vehicle’s physical
state;
• The attacker can control a software module on the vehi-
cle (e.g., via installing a third-party software module or
compromising an existing module);
• The software module controlled by the attacker shares a
cache with the victim control software module.
Given these assumptions, an attacker can deploy an attack
program on the victim’s computer system and spy on the con-
trol software module through a cache side channel. We believe
that these assumptions are reasonable for future autonomous
vehicles.
First, as mentioned in Section 6.2, software modules with
adaptive computing behavior (including AMCL) have been
widely used in research/industry prototypes. For efﬁciency,
it makes an intuitive sense to dynamically adjust the amount
of computation based on uncertainty or environments at run-
time.
Second, connected vehicles with an Internet connection and
an integrated infotainment system demand an open software
architecture that exposes a wider attack surface to remote
attackers. For example, it is likely that an infotainment system
will allow third-party applications to be downloaded on the
vehicle’s computer system. Studies on connected vehicles also
show that a vehicle’s onboard computers contain software