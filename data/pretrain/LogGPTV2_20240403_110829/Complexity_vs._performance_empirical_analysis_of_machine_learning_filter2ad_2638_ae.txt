dimensional data, and the relative performance difference between
classifiers change as dimensionality increases. Other work also fo-
cused on evaluating performance of specific classifier families, for
example tree-based classifiers [47, 50], rule-based classifiers [50],
and ensemble methods [49].
In comparison, our work does not focus on a single step of the
ML pipeline. Instead, we analyze end-to-end impact of complexity
on classifier performance, through the lens of deployed MLaaS
platforms. This allows us to understand how specific changes to the
ML task pipeline impact actual performance in a real world system.
Instead of focusing only on the best achievable performance for
any classifier, we recognize the wide-spread use of ML by generalist
users, and study the “cost” of suboptimal decisions in choosing and
configuring classifiers in terms of degraded performance.
Automated Machine Learning. Many works focused on re-
ducing human effort in ML system design by automating classifier
selection and parameter tuning. Researchers proposed mechanisms
to recommend classifier choices based on classifiers that are known
to perform well on similar datasets [40]. Many mechanisms even
use machine learning algorithms like collaborative filtering and
k-nearest neighbor to recommend classifiers [4, 10, 60]. To perform
automatic parameter optimization, methods have been proposed
based on intuition-based Random Search [6, 7], and Bayesian op-
timization [7, 25, 37, 61]. These mechanisms have been shown to
estimate suitable parameters with less computational complexity
than brute-force methods like Grid Search [21]. Other works pro-
posed techniques to automate the entire ML pipeline. For example,
Auto-Weka [39, 63] and Auto-Sklearn [24] could search through
the joint space of classifiers and their respective parameter settings
and choose the optimal configuration.
The
Experimental Design for Evaluating ML Classifiers.
ML community has a long history on classifier evaluation using
carefully designed benchmark tests [38, 53, 68]. Many studies pro-
posed theoretical frameworks and guidelines for designing bench-
mark experiments [22, 36]. Dietterich used statistical tests to com-
pare classifiers [20] and the methodology was later improved in
follow-up work [19, 29]. Our performance evaluation using Fried-
man ranking is based on their methodology. Other work focused
on comparing and benchmarking performance of popular ML soft-
ware [30, 69], e.g. Weka [75], PRTools [65], KEEL [2] and, more
recently, deep learning tools [56]. In addition, work has been done
to identify and quantify the relationship between classifier perfor-
mance and dataset properties [35, 46], especially dataset complex-
ity [44, 48, 78]. Our work leverages similar insights about dataset
complexity (linearity) to automatically identify classifier families
based on prediction results.
8 LIMITATIONS
We point out three limitations of our study. First, we focus on 6
mainstream MLaaS platforms, covering services provided by both
traditional Internet giants (Google, Microsoft, Amazon) and emerg-
ing startups (ABM. BigML, PredictionIO). We did not study other
commercial MLaaS platforms because they either focus on highly
specialized tasks (e.g. image/text classification), or does not support
large scale measurements (e.g. posing strict rate limit). Second, we
focus on binary classification tasks with three dimensions of con-
trol (CLF, PARA, and FEAT). We did not extend our analysis to other
ML tasks and cover every configuration choice, e.g. more advanced
classifiers. We leave these as future work. Third, we only study
the classification performance of MLaaS platforms, which is one
of the many aspects to evaluate MLaaS platforms. There are other
dimensions, e.g. training time, cost, robustness to incorrect input.
We leave further exploration of these aspects as future work.
9 CONCLUSIONS
For network researchers, MLaaS systems provide an attractive al-
ternative to running and configuring their own standalone ML clas-
sifiers. Our study empirically analyzes the performance of MLaaS
platforms, with a focus on understanding how user control impacts
both the performance and performance variance of classification
in common ML tasks.
Our study produced multiple key takeaways. First, as expected,
with more control comes more potential performance gains, as well
as greater performance degradation from poor configuration deci-
sions. Second, fully automated platforms are optimizing classifiers
using internal tests. While this greatly simplifies the ML process
and helps them outperform other MLaaS platforms using default
settings, their aggregated performance lags far behind well-tuned
versions of more configurable alternatives (Microsoft, PredictionIO,
local scikit-learn). Finally, much of the gains from configuration and
tuning come from choosing the right classifier. Experimenting with
a small random subset of classifiers is likely to achieve near-optimal
results.
Our study shows that used correctly, MLaaS systems can provide
networking researchers results comparable to standalone ML clas-
sifiers. While more automated “turnkey” systems are making some
intelligent decisions on classifiers, they still have a long way to go.
Thankfully, we show that for most classification tasks today, ex-
perimenting with a small random subset of classifiers will produce
near-optimal results.
ACKNOWLEDGMENTS
We wish to thank our shepherd Chadi Barakat and the anonymous
reviewers for their useful feedback. This project was supported by
NSF grants CNS-1527939 and CNS-1705042. Any opinions, findings,
and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of any
funding agencies.
REFERENCES
[1] Bhavish Aggarwal, Ranjita Bhagwan, Tathagata Das, Siddharth Eswaran,
Venkata N. Padmanabhan, and Geoffrey M. Voelker. 2009. NetPrints: Diagnosing
home network misconfigurations using shared knowledge. In Proc. of NSDI.
[2] Jesús Alcalá-Fdez, Luciano Sánchez, Salvador Garcia, Maria Jose del Jesus, Se-
bastian Ventura, Josep M. Garrell, José Otero, Cristóbal Romero, Jaume Bacardit,
Victor M. Rivas, et al. 2009. KEEL: A software tool to assess evolutionary al-
gorithms for data mining problems. Soft Computing-A Fusion of Foundations,
Methodologies and Applications 13, 3 (2009), 307–318.
[3] Arthur Asuncion and David Newman. 2007. UCI machine learning repository.
http://archive.ics.uci.edu/ml. (2007).
[4] Rémi Bardenet, Mátyás Brendel, Balázs Kégl, and Michele Sebag. 2013. Collabo-
rative hyperparameter tuning. In Proc. of ICML.
[5] Fabrício Benevenuto, Gabriel Magno, Tiago Rodrigues, and Virgílio Almeida.
2010. Detecting spammers on Twitter. In Proc. of CEAS.
[6] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter
optimization. Journal of Machine Learning Research 13, Feb (2012), 281–305.
[7] James S. Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algo-
rithms for hyper-parameter optimization. In Proc. of NIPS.
[8] Peter Bodik, Moises Goldszmidt, Armando Fox, Dawn B. Woodard, and Hans
Andersen. 2010. Fingerprinting the datacenter: Automated classification of
performance crises. In Proc. of EuroSys.
[9] Léon Bottou and Chih-Jen Lin. 2007. Support vector machine solvers. Large scale
kernel machines (2007), 301–320.
[10] Pavel B. Brazdil, Carlos Soares, and Joaquim Pinto Da Costa. 2003. Ranking
learning algorithms: Using IBL and meta-learning on accuracy and time results.
Machine Learning 50, 3 (2003), 251–277.
[11] Leo Breiman. 1996. Bagging predictors. Machine learning 24, 2 (1996), 123–140.
[12] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5–32.
[13] Matthijs C. Brouwer, Allan R. Tunkel, and Diederik van de Beek. 2010. Epidemiol-
ogy, diagnosis, and antimicrobial treatment of acute bacterial meningitis. Clinical
microbiology reviews 23, 3 (2010), 467–492.
[14] Rich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. 2008. An empirical
evaluation of supervised learning in high dimensions. In Proc. of ICML.
[15] Rich Caruana and Alexandru Niculescu-Mizil. 2006. An empirical comparison of
supervised learning algorithms. In Proc. of ICML.
[16] Simon Chan, Thomas Stone, Kit Pang Szeto, and Ka Hou Chan. 2013. PredictionIO:
a distributed machine learning server for practical software development. In Proc.
of CIKM.
[17] Helen Costa, Fabricio Benevenuto, and Luiz H.C. Merschmann. 2013. Detecting
tip spam in location-based social networks. In Proc. of SAC.
[18] Helen Costa, Luiz Henrique de Campos Merschmann, Fabricio Barth, and Fabricio
Benevenuto. 2014. Pollution, Bad-mouthing, and Local Marketing: The under-
ground of location-based social networks. Elsevier Information Sciences (2014).
[19] Janez Demšar. 2006. Statistical comparisons of classifiers over multiple data sets.
Journal of Machine learning research 7, Jan (2006), 1–30.
[20] Thomas G. Dietterich. 1998. Approximate statistical tests for comparing su-
pervised classification learning algorithms. Neural computation 10, 7 (1998),
1895–1923.
[21] Katharina Eggensperger, Matthias Feurer, Frank Hutter, James Bergstra, Jasper
Snoek, Holger Hoos, and Kevin Leyton-Brown. 2013. Towards an empirical
foundation for assessing bayesian optimization of hyperparameters. In Proc. of
NIPS.
[22] Manuel J.A. Eugster, Torsten Hothorn, and Friedrich Leisch. 2016. Domain-based
benchmark experiments: Exploratory and inferential analysis. Austrian Journal
of Statistics 41, 1 (2016), 5–26.
[23] Manuel Fernández-Delgado, Eva Cernadas, Senén Barro, and Dinani Amorim.
2014. Do we need hundreds of classifiers to solve real world classification
problems. Journal of Machine Learning Research 15, 1 (2014), 3133–3181.
[24] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel
Blum, and Frank Hutter. 2015. Efficient and robust automated machine learning.
In Proc. of NIPS.
[25] Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. 2015. Initializing
bayesian hyperparameter optimization via meta-learning. In Proc. of AAAI.
[26] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In Proc.
of CCS.
[27] Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the
perceptron algorithm. Machine learning 37, 3 (1999), 277–296.
[28] Jerome H. Friedman. 2002. Stochastic gradient boosting. Computational Statistics
and Data Analysis 38, 4 (2002), 367–378.
[29] Salvador Garcia and Francisco Herrera. 2008. An extension on “statistical compar-
isons of classifiers over multiple data sets” for all pairwise comparisons. Journal
of Machine Learning Research 9, Dec (2008), 2677–2694.
[30] Michael Goebel and Le Gruenwald. 1999. A survey of data mining and knowledge
discovery software tools. ACM SIGKDD explorations newsletter 1, 1 (1999), 20–33.
[31] Peter Haider and Tobias Scheffer. 2014. Finding botnets using minimal graph
clusterings. In Proc. of ICML.
[32] Frank E. Harrell. 2002. Very low birth weight infants dataset.
[33] Frank E. Harrell. 2006. VA lung cancer dataset.
[34] Ralf Herbrich, Thore Graepel, and Colin Campbell. 2001. Bayes point machines.
Journal of Machine Learning Research 1, Aug (2001), 245–279.
[35] Robert C. Holte. 1993. Very simple classification rules perform well on most
commonly used datasets. Machine learning 11, 1 (1993), 63–90.
[36] Torsten Hothorn, Friedrich Leisch, Achim Zeileis, and Kurt Hornik. 2005. The
design and analysis of benchmark experiments. Journal of Computational and
Graphical Statistics 14, 3 (2005), 675–699.
[37] Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2011. Sequential model-
based optimization for general algorithm configuration. In Proc. of LION.
[38] Eamonn Keogh and Shruti Kasetty. 2003. On the need for time series data mining
benchmarks: a survey and empirical demonstration. Data Mining and knowledge
discovery 7, 4 (2003), 349–371.
[39] Lars Kotthoff, Chris Thornton, Holger H. Hoos, Frank Hutter, and Kevin Leyton-
Brown. 2016. Auto-WEKA 2.0: Automatic model selection and hyperparameter
optimization in WEKA. Journal of Machine Learning Research 17 (2016), 1–5.
[40] Rui Leite, Pavel Brazdil, and Joaquin Vanschoren. 2012. Selecting classification
algorithms with active testing. In Proc. of MLDM.
[41] Zhijing Li, Ana Nika, Xinyi Zhang, Yanzi Zhu, Yuanshun Yao, Ben Y. Zhao,
Identifying value in crowdsourced wireless signal
and Haitao Zheng. 2017.
measurements. In Proc. of WWW.
[42] Dapeng Liu, Youjian Zhao, Haowen Xu, Yongqian Sun, Dan Pei, Jiao Luo, Xiaowei
Jing, and Mei Feng. 2015. Opprentice: Towards practical and automatic anomaly
detection through machine learning. In Proc. of IMC.
[43] Qingyun Liu, Shiliang Tang, Xinyi Zhang, Xiaohan Zhao, Ben Y. Zhao, and Haitao
Zheng. 2016. Network growth and link prediction through an empirical lens. In
Proc. of IMC.
[44] Julián Luengo and Francisco Herrera. 2015. An automatic extraction method
of the domains of competence for learning classifiers using data complexity
measures. Knowledge and Information Systems 42, 1 (2015), 147–180.
[45] Núria Macià and Ester Bernadó-Mansilla. 2014. Towards UCI+: A mindful reposi-
tory design. Information Sciences 261 (2014), 237–262.
[46] Núria Macià, Ester Bernadó-Mansilla, Albert Orriols-Puig, and Tin Kam Ho. 2013.
Learner excellence biased by data set selection: A case for data characterisation
and artificial data sets. Pattern Recognition 46, 3 (2013), 1054–1066.
[47] Richard Maclin and David Opitz. 1997. An empirical evaluation of bagging and
boosting. In Proc. of AAAI.
[48] Laura Morán-Fernández, Verónica Bolón-Canedo, and Amparo Alonso-Betanzos.
2016. Can classification performance be predicted by complexity measures? A
study using microarray data. Knowledge and Information Systems (2016), 1–24.
[49] David Opitz and Richard Maclin. 1999. Popular ensemble methods: An empirical
study. Journal of Artificial Intelligence Research 11 (1999), 169–198.
[50] Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree induction
vs. logistic regression: A learning-curve analysis. Journal of Machine Learning
Research 4, Jun (2003), 211–255.
[51] Mauro Ribeiro, Katarina Grolinger, and Miriam A.M. Capretz. 2015. MLaaS:
Machine learning as a service. In Proc. of ICMLA.
[52] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1988. Learning
representations by back-propagating errors. Cognitive modeling 5, 3 (1988), 1.
[53] Steven L. Salzberg. 1997. On comparing classifiers: Pitfalls to avoid and a recom-
mended approach. Data mining and knowledge discovery 1, 3 (1997), 317–328.
[54] Purnamrita Sarkar, Deepayan Chakrabarti, and Michael I. Jordan. 2012. Nonpara-
metric link prediction in dynamic networks. In Proc. of ICML.
[55] David J. Sheskin. 2003. Handbook of parametric and nonparametric statistical
procedures. CRC Press.
[56] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. 2016. Benchmarking
state-of-the-art deep learning software tools. International Conference on Cloud
Computing and Big Data (2016), 99–104.
[57] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In Proc. of IEEE S&P.
[58] Jamie Shotton, Toby Sharp, Pushmeet Kohli, Sebastian Nowozin, John Winn,
and Antonio Criminisi. 2013. Decision jungles: Compact and rich models for
classification. In Proc. of NIPS.
[59] Anirudh Sivaraman, Keith Winstein, Pratiksha Thaker, and Hari Balakrishnan.
2014. An experimental study of the learnability of congestion control. In Proc. of
SIGCOMM.
[60] Michael R. Smith, Logan Mitchell, Christophe Giraud-Carrier, and Tony Martinez.
2014. Recommending learning algorithms and their associated hyperparameters.
In Proc. of MLAS.
[61] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. 2012. Practical bayesian
optimization of machine learning algorithms. In Proc. of NIPS.
[62] Pang-Ning Tan, Michael Steinbach, and Vipin Kumar. 2005. Introduction to data
mining. Addison-Wesley Longman Publishing Co., Inc.
[63] Chris Thornton, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2013.
Auto-WEKA: Combined selection and hyperparameter optimization of classifica-
tion algorithms. In Proc. of KDD.
[64] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via Prediction APIs. In Proc. of USENIX
Security.
[65] Ferdinand Van Der Heijden, Robert Duin, Dick De Ridder, and David MJ Tax.
2005. Classification, parameter estimation and state estimation: an engineering
approach using MATLAB. John Wiley & Sons.
[66] Joaquin Vanschoren, Hendrik Blockeel, Bernhard Pfahringer, and Geoffrey
Holmes. 2012. Experiment databases. Machine Learning 87, 2 (2012), 127–158.
[67] Jean-Philippe Vert, Koji Tsuda, and Bernhard Schölkopf. 2004. A primer on kernel
methods. Kernel Methods in Computational Biology (2004), 35–70.
[68] Kiri Wagstaff. 2012. Machine learning that matters. In Proc. of ICML.
[69] Abdullah H. Wahbeh, Qasem A. Al-Radaideh, Mohammed N. Al-Kabi, and
Emad M. Al-Shawakfa. 2011. A comparison study between data mining tools
over some classification methods. International Journal of Advanced Computer
Science and Applications (2011), 18–26.
[70] Gang Wang, Tristan Konolige, Christo Wilson, Xiao Wang, Haitao Zheng, and
Ben Y. Zhao. 2013. You are how you click: Clickstream analysis for sybil detection.
In Proc. of Usenix Security.
[71] Gang Wang, Bolun Wang, Tianyi Wang, Ana Nika, Haitao Zheng, and Ben Y.
Zhao. 2014. Whispers in the dark: Analysis of an anonymous social network. In
Proc. of IMC.
[72] Gang Wang, Xinyi Zhang, Shiliang Tang, Haitao Zheng, and Ben Y. Zhao. 2016.
Unsupervised clickstream clustering for user behavior analysis. In Proc. of CHI.
[73] David J. Whellan, Robert H. Tuttle, Eric J. Velazquez, Linda K. Shaw, James G.
Jollis, Wendell Ellis, Christopher M. O’connor, Robert M. Califf, and Salvador
Borges-Neto. 2006. Predicting significant coronary artery disease in patients
with left ventricular dysfunction. American heart journal 152, 2 (2006), 340–347.
[74] Keith Winstein and Hari Balakrishnan. 2013. TCP ex Machina: Computer-
generated congestion control. In Proc. of SIGCOMM.
[75] Ian H. Witten, Eibe Frank, Leonard E. Trigg, Mark A. Hall, Geoffrey Holmes,
and Sally Jo Cunningham. 1999. Weka: Practical machine learning tools and
techniques with Java implementations.
[76] Wei Xu, Ling Huang, Armando Fox, David Patterson, and Michael I. Jordan. 2009.
Detecting large-scale system problems by mining console logs. In Proc. of SOSP.
[77] Minhui Xue, Cameron Ballard, Kelvin Liu, Carson Nemelka, Yanqiu Wu, Keith
Ross, and Haifeng Qian. 2016. You can Yak but you can’t hide: Localizing anony-
mous social network users. In Proc. of IMC.
[78] Julian Zubek and Dariusz M. Plewczynski. 2016. Complexity curve: a graphical
measure of data complexity and classifier performance. PeerJ Computer Science 2
(2016), e76.