### Dimensional Data and Classifier Performance

The relative performance difference between classifiers changes as the dimensionality of the data increases. Previous research has focused on evaluating the performance of specific classifier families, such as tree-based classifiers [47, 50], rule-based classifiers [50], and ensemble methods [49].

In contrast, our work does not focus on a single step in the machine learning (ML) pipeline. Instead, we analyze the end-to-end impact of complexity on classifier performance through the lens of deployed ML-as-a-Service (MLaaS) platforms. This approach allows us to understand how specific changes in the ML task pipeline affect actual performance in real-world systems. Rather than focusing solely on the best achievable performance for any classifier, we recognize the widespread use of ML by generalist users and study the "cost" of suboptimal decisions in choosing and configuring classifiers in terms of degraded performance.

### Automated Machine Learning

Many studies have aimed to reduce human effort in ML system design by automating classifier selection and parameter tuning. Researchers have proposed mechanisms to recommend classifiers based on those that perform well on similar datasets [40]. Some of these mechanisms use machine learning algorithms like collaborative filtering and k-nearest neighbor to recommend classifiers [4, 10, 60]. For automatic parameter optimization, methods based on random search [6, 7] and Bayesian optimization [7, 25, 37, 61] have been proposed. These methods have been shown to estimate suitable parameters with less computational complexity than brute-force methods like grid search [21]. Other works have proposed techniques to automate the entire ML pipeline. For example, Auto-Weka [39, 63] and Auto-Sklearn [24] can search through the joint space of classifiers and their respective parameter settings to choose the optimal configuration.

### Experimental Design for Evaluating ML Classifiers

The ML community has a long history of using carefully designed benchmark tests to evaluate classifiers [38, 53, 68]. Many studies have proposed theoretical frameworks and guidelines for designing benchmark experiments [22, 36]. Dietterich used statistical tests to compare classifiers [20], and this methodology was later improved in follow-up work [19, 29]. Our performance evaluation using Friedman ranking is based on their methodology. Other work has focused on comparing and benchmarking the performance of popular ML software, such as Weka [75], PRTools [65], KEEL [2], and more recently, deep learning tools [56]. Additionally, research has been done to identify and quantify the relationship between classifier performance and dataset properties [35, 46], especially dataset complexity [44, 48, 78]. Our work leverages similar insights about dataset complexity (linearity) to automatically identify classifier families based on prediction results.

### Limitations

We acknowledge three limitations of our study. First, we focus on six mainstream MLaaS platforms, covering services provided by both traditional Internet giants (Google, Microsoft, Amazon) and emerging startups (ABM, BigML, PredictionIO). We did not study other commercial MLaaS platforms because they either focus on highly specialized tasks (e.g., image/text classification) or do not support large-scale measurements (e.g., imposing strict rate limits). Second, we focus on binary classification tasks with three dimensions of control (CLF, PARA, and FEAT). We did not extend our analysis to other ML tasks or cover every configuration choice, such as more advanced classifiers. These are areas for future work. Third, we only study the classification performance of MLaaS platforms, which is one of many aspects to evaluate. Other dimensions, such as training time, cost, and robustness to incorrect input, are also important and will be explored in future work.

### Conclusions

For network researchers, MLaaS systems provide an attractive alternative to running and configuring their own standalone ML classifiers. Our study empirically analyzes the performance of MLaaS platforms, with a focus on understanding how user control impacts both the performance and performance variance of classification in common ML tasks.

Our study produced several key takeaways. First, as expected, with more control comes more potential performance gains, as well as greater performance degradation from poor configuration decisions. Second, fully automated platforms optimize classifiers using internal tests. While this simplifies the ML process and helps them outperform other MLaaS platforms using default settings, their aggregated performance lags far behind well-tuned versions of more configurable alternatives (Microsoft, PredictionIO, local scikit-learn). Finally, much of the gain from configuration and tuning comes from choosing the right classifier. Experimenting with a small random subset of classifiers is likely to achieve near-optimal results.

Our study shows that, when used correctly, MLaaS systems can provide networking researchers with results comparable to standalone ML classifiers. While more automated "turnkey" systems are making some intelligent decisions on classifiers, they still have a long way to go. Fortunately, for most classification tasks today, experimenting with a small random subset of classifiers will produce near-optimal results.

### Acknowledgments

We wish to thank our shepherd Chadi Barakat and the anonymous reviewers for their useful feedback. This project was supported by NSF grants CNS-1527939 and CNS-1705042. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.

### References

[References listed here as in the original text]

This revised version aims to enhance clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.