B. SDT Speciﬁc Handling
Next, we describe some implementation details that are
speciﬁc to the target SDT.
1) Implementation for Strata: Besides the code cache,
many Strata-based security mechanisms also involve some
critical metadata (e.g., the key to decrypt a randomized in-
struction set) that needs to be protected. Otherwise, attackers
can compromise such data to disable or mislead critical func-
tionalities of the security mechanisms. Thus, we extended the
protection to Strata’s code, data, and the binary to be translated.
Fortunately, since Strata directly allocates memory from mmap
and manages its own heap, this additional protection can be
9
easily supported by SDCG. Speciﬁcally, SDCG ensures that all
the memory regions allocated by Strata are mapped as either
read-only or inaccessible. Note that we do not need to protect
Strata’s static data, because once the SDT process is forked, the
static data is copy-on-write protected, i.e., while the untrusted
code could modify Strata’s static data, the modiﬁcation cannot
affect the copy in the SDT process.
Writing RPC stubs for Strata also reﬂects the differences in
the attack model: since all dynamic data are mapped as read-
only, any functionality that modiﬁed the data also needs to be
handled in the SDT process.
Another special case for Strata is the handling of process
creation, i.e., the clone system call. The seccomp-sandbox
only handles the case for thread creation, which is sufﬁcient
for Google Chrome (and V8). But for Strata, we also need to
handle process creation. The challenge for process creation is
that once a memory region is mapped as shared, the newly
created child process will also inherit this memory regions as
shared. Thus, once the untrusted code forks a new process,
this process also shares the same memory pool with its parent
and the SDT process. If we want to enforce a 1 : 1 serving
model, we need to un-share the memory. Unfortunately, un-
sharing memory under Linux is not easy: one needs to 1) map
a temporary memory region, 2) copy the shared content to this
temporary region, 3) unmap the original shared memory, 4)
map a new shared memory region at exactly the same address,
5) copy the content back, and 6) unmap the temporary memory
region. At the same time, the child process is likely to either
share the same binary as its parent, which means it can be
served by the same SDT; or call execve immediately after
the fork, which completely destroys the virtual address space
it inherited from its parent. For these reasons, we implemented
a N : 1 serving model for Strata, i.e., one SDT process
serves multiple untrusted processes. The clone system call
can then be handled in the same way for both thread creation
and process creation. The only difference is that when a new
memory region is allocated from the shared memory pool, all
processes need to be synchronized.
2) Implementation for V8: Compared with Strata,
the
biggest challenge for porting V8 to SDCG is the dynamic data
used by V8. Speciﬁcally, V8 has two types of dynamic data:
JS related data and its own internal data. The ﬁrst type of
data is allocated from custom heaps that are managed by V8
itself. Similar to Strata’s heap, these heaps directly allocate
memory from mmap, thus SDCG can easily handle this type of
data. The difﬁculty is from the second type of data, which
is allocated from the standard C library (glibc on Linux).
This makes it challenging to track which memory region is
used by the JS engine. Clearly, we cannot make the standard
C library allocate all the memory from the shared memory
pool. However, as mentioned earlier in the design section,
we have to share data via RPC and avoid serializing objects,
especially C++ objects, which can be complicated. To solve
this problem, we implemented a simple arena-based heap that
is backed by the shared memory pool and modiﬁed V8 to
allocate certain objects from this heap. Only objects that are
involved in RPC need to be allocated from this heap, the rest
can still be allocated from the standard C library.
Another problem is the stack. Strata does not share the
same stack as the translated program, so it never reads data
from the program’s stack. This is not true for V8. In fact, many
objects used by V8 are allocated on the stack. Thus, during
RPC handling,
the STD process may dereference pointers
pointing to the stack. Moreover, since the stack is assigned
during thread creation, it is difﬁcult to ensure that the program
always allocates stack space from our shared memory pool.
As a result, we copy content between the two processes.
Fortunately, only 3 RPCs require a stack copy. Note that
because the content is copied to/from the same address, when
creating the trusted SDT process, we must assign it a new
stack instead of relying on copy-on-write.
Writing RPC stubs for V8 is more ﬂexible than Strata be-
cause dynamic data is not protected. For this reason, we would
prefer to convert functions that are invoked less frequently. To
achieve this goal, we followed two general principles. First,
between the entry of the JS engine and the point where the
code cache is modiﬁed, many functions could be invoked. If
we convert a function too high in the calling chain, and the
function does not result in modiﬁcation of the code cache
under another context, we end up introducing unnecessary RPC
overhead. For instance, the ﬁrst time a regular expression is
evaluated, it is compiled; but thereafter, the compiled code can
be retrieved from the cache. Thus, we want to convert functions
that are post-dominated by operations that modify the code
cache. Conversely, if we convert a function that is too low in
the calling chain, even though the invocation of this function
always results in modiﬁcation of the code cache, the function
may be called from a loop, e.g., marking processes during
garbage collection. This also introduces unnecessary overhead.
Thus, we also want to convert functions that dominate as many
modiﬁcations as possible. In our prototype implementation,
since we did not use program analysis, these principles were
applied empirically. In the end, we added a total of 20 RPC
stubs.
VI. EVALUATION
In this section, we describe the evaluation of the ef-
fectiveness and performance overhead of our two prototype
implementations.
A. Setup
For our port of the Strata DBT, we measured the perfor-
mance overhead using SPEC CINT 2006 [56]. Our port of the
V8 JS engine was based on revision 16619. The benchmark
we used to measure the performance overhead is the V8
Benchmark distributed with the source code (version 7) [44].
All experiments were run on a workstation with one Intel Core
i7-3930K CPU (6-core, 12-thread) and 32GB memory. The
operating system is the 64-bit Ubuntu 13.04 with kernel 3.8.0-
35-generic.
B. Effectiveness
In Section IV-E, we provided a security analysis of our
system design, which showed that if implemented correctly,
SDCG can prevent all code cache injection attacks. In this sec-
tion, we evaluate our SDCG-ported V8 prototype to determine
whether it can truly prevent the attack we demonstrated in
Section III-C3.
10
The experiment was done using the same proof-of-concept
code as described in Section III-C3. As the attack relies on a
race condition, we executed it 100 times. For the version that
is protected by naive W⊕X enforcement, the attack was able
to inject shellcode into the code cache 91 times. For SDCG-
ported version, all 100 attempts failed.
C. Micro Benchmark
The overhead introduced by SDCG comes from two major
sources: RPC invocation and cache coherency.
1) RPC Overhead: To measure the overhead for each RPC
invocation, we inserted a new ﬁeld in the request header
to indicate when this request was sent. Upon receiving the
request, the handler calculates the time elapsed between this
and the current time. Similarly, we also calculated the time
elapsed between the sending and receiving of return values. To
eliminate the impact from cache synchronization, we pinned
all threads (in both the untrusted process and the SDT process)
to a single CPU core.
The frequency of RPC invocation also effects overall over-
head, so we also collected this number during the evaluation.
Table I shows the result from the V8 benchmark, using
the 64-bit release build. The average latency for call request
is around 3-4 µs and the average latency for RPC return
is around 4-5 µs. Thus,
the average latency for an RPC
invocation through SDCG’s communication channel is around
8-9 µs. The number of RPC invocations is between 1,525 and
6,000. Since the input is ﬁxed, this number is stable, with
small ﬂuctuations caused by garbage collection. Compared to
the overall overhead presented in the next section, it follows
that the larger the number of RPC invocations, the grater the
value of overhead. Among all RPC invocations, less than 24%
require a stack copy.
2) Cache Coherency Overhead: SDCG involves at least
three concurrently running threads: the main thread in the
untrusted process, the trusted thread in the untrusted process,
and the main thread in the SDT process. This number can
increase if the SDT to be protected already uses multiple
threads. On a platform with multiple cores, these threads can
be scheduled to different cores. Since SDCG depends heavily
on shared memory, OS scheduling for these threads can also
affect performance, i.e., cache synchronization between threads
executing on different cores introduces additional overhead.
In this section, we report this overhead at the RPC in-
vocation level. In the next section, we present its impact on
overall performance. The evaluation also uses V8 benchmark.
To reduce the possible combination of scheduling, we disabled
all other threads in V8, leaving only the aforementioned three
threads. The Intel Core i7-3930K CPU on our testbed has
six cores. Each core has a dedicated 32KB L1 data cache
and 256KB integrated L2 cache. A 12MB L3 cache is shared
among all cores. When Hyperthreading is enabled, each core
can execute two concurrent threads.
Given the above conﬁguration, we have tested the following
scheduling:
iii) Two main threads that frequently access shared memory
on a single CPU thread, trusted thread freely scheduled
(afﬁnity mask = {0},{*});
iv) Two main threads on a single core, trusted thread freely
scheduled (afﬁnity mask = {0,1},{*});
v) All
three threads on different cores (afﬁnity mask =
{0},{2},{4}); and
vi) All
three threads freely scheduled (afﬁnity mask =
{*},{*},{*}).
Table II shows the result, using the 64-bit release build.
All the numbers are for RPC invocation, with return latency
omitted. Based on the result, it is clear that scheduling has a
great impact on the RPC latency. If the two main threads are
not scheduled on the same CPU thread, the average latency
can exacerbate to 3x-4x slower. On the other hand, scheduling
for the trusted thread has little impact on the RPC latency.
This is expected because the trusted thread is only utilized for
memory synchronization.
D. Macro Benchmark
In this section, we report
the overall overhead SDCG
introduces. Since OS scheduling can have a large impact on
performance, for each benchmark suite, we evaluated two CPU
schedules. The ﬁrst (Pinned) pins both the main threads from
the untrusted process and the SDT process to a single core;
and the second (Free) allows the OS to freely schedule all
threads.
1) SPEC CINT 2006: Both the vanilla Strata and the
SDCG-ported Strata are built as 32-bit. The SPEC CINT
2006 benchmark suite is also compiled as 32-bit. Since all
benchmarks from the suite are single-threaded, the results of
different scheduling strategies only reﬂect the overhead caused
by SDCG.
Table III shows the evaluation result. The ﬁrst column
is the result of running natively. The second column is the
result for Strata without SDCG. We use this as the baseline
for calculating the slowdown introduced by SDCG. The third
column is the result for SDCG with pinned schedule, and the
last column is the result for SDCG with free schedule. Since
the standard deviation is small (less than 1%), we omitted this
information.
The corresponding slowdown is shown in Figure 4. For all
benchmarks, the slowdown introduced by SDCG is less than
6%. The overall (geometric mean) slowdown is 1.46% for the
pinned schedule, and 2.05% for the free schedule.
Since SPEC CINT is a computation-oriented benchmark
suite and Strata does a good job reducing the number of trans-
lator invocations, we did not observe a signiﬁcant difference
between the pinned schedule and the free schedule.
2) JavaScript Benchmarks: Our port of V8 JS engine was
based on revision 16619. For better comparison with an SFI-
based solution [5], we performed the evaluation on both IA32
and x64 release builds. The arena-based heap we implemented
was only enabled for SDCG-ported V8. To reduce the possible
combination of scheduling, we also disabled all other threads
in V8.
i) All threads on a single CPU thread (afﬁnity mask = {0});
ii) All threads on a single core (afﬁnity mask = {0,1});
Table IV shows the results for the IA32 build, and Table V
shows the results for the x64 build. The ﬁrst column is the
11
TABLE I: RPC Overhead During the Execution of V8 Benchmark.
Avg Call Latency
Avg Return Latency
# of Invocations
Richards
DeltaBlue
Crypto
RayTrace
EarlyBoyer
RegExp
Splay
NavierStokes
4.70 µs
4.28 µs
3.99 µs
3.98 µs
3.87 µs
3.82 µs
4.63 µs
4.67 µs
4.54 µs
4.46 µs
4.28 µs
4.00 µs
4.28 µs
5.06 µs
5.04 µs
4.82 µs
1525
2812
4596
3534
5268
6000
5337
1635
Stack Copy (%)
362 (23.74%)
496 (17.64%)
609 (13.25%)
715 (20.23%)
489 ( 9.28%)
193 ( 3.22%)
1187 (22.24%)
251 (15.35%)
No Stack Copy (%)
1163 (76.26%)
2316 (82.36%)
3987 (86.75%)
2819 (79.77%)
4779 (90.72%)
5807 (96.78%)
5150 (77.76%)
1384 (84.65%)
TABLE II: Cache Coherency Overhead Under Different Scheduling Strategies.
Richards
DeltaBlue
Crypto
RayTrace
EarlyBoyer
RegExp
Splay
NavierStokes
Schedule 1
4.70 µs
4.28 µs
3.99 µs
3.98 µs
3.87 µs
3.82 µs
4.63 µs
4.67 µs
Schedule 2
13.76 µs
13.29 µs
10.91 µs