sary’s only insight on this data is indirect, through the ML
API.
We do not consider malicious service providers, and we
do not consider adversarial clients that can compromise the
service, e.g., bypassing authentication somehow or otherwise
exploiting a bug in the server’s software. Such threats are
important for the security of ML services, but already known
as important issues. On the other hand, we believe that the
kinds of attacks we will show against ML APIs are more
subtle and unrecognized as threats.
3. THE FREDRIKSON ET AL. ATTACK
We start by recalling the generic model inversion attack
for target features with small domains from Fredrikson et
al. [13].
Fredrikson et al. considered a linear regression model f
that predicted a real-valued suggested initial dose of the
drug Warfarin using a feature vector consisting of patient de-
mographic information, medical history, and genetic mark-
ers.3 The sensitive attribute was considered to be the ge-
netic marker, which we assume for simplicity to be the ﬁrst
feature x1. They explored model inversion where an at-
tacker, given white-box access to f and auxiliary informa-
tion side(x, y) def= (x2, . . . , xt, y) for a patient instance (x, y),
attempts to infer the patient’s genetic marker x1.
Figure 2 gives their inversion algorithm. Here we assume
aux gives the empirically computed standard deviation σ
for a Gaussian error model err and marginal priors p =
(p1, . . . , pt). The marginal prior pi is computed by ﬁrst
partitioning the real line into disjoint buckets (ranges of val-
ues), and then letting pi(v) for each bucket v be the the
number of times xi falls in v over all x in db divided by the
number of training vectors |db|.
In words, the algorithm simply completes the target fea-
ture vector with each of the possible values for x1, and then
computes a weighted probability estimate that this is the
correct value. The Gaussian error model will penalize val-
2For example, BigML.io includes this data on web pages
describing black-box models in their model gallery [4].
3The inputs were processed in a standard way to make nom-
inal valued data real-valued. See [13] for details.
1324adversary Af (err, pi, x2, . . . , xt, y):
1: for each possible value v of x1 do
2:
3:
4: Return arg maxv rv
rv ← err(y, f (x(cid:48))) ·(cid:81)
x(cid:48) = (v, x2, . . . , xt)
i pi(xi)
x1
1
0
0
x2
φ1(x) = x1
w1 = 0
φ2(x) = (1 − x1)(x2)
w2 = 1
φ3(x) = (1 − x1)(1 − x2) w3 = 0
1
1
0
0
Figure 2: Generic inversion attack for nominal tar-
get features.
Figure 3: Decision tree for the formula y = ¬x1 ∧ x2.
ues of x1 that force the prediction to be far from the given
label y.
As argued in their work, this algorithm produces the least-
biased maximum a posteriori (MAP) estimate for x1 given
the available information. Thus, it minimizes the adver-
sary’s misprediction rate. They only analyzed its eﬃcacy in
the case of Warfarin dosing, showing that the MI algorithm
above achieves accuracy in predicting a genetic marker close
to that of a linear model trained directly from the original
data set.
It is easy to see that their algorithm is, in fact, black-
box, and in fact agnostic to the way f works. That means
it is potentially applicable in other settings, where f is not
a linear regression model but some other algorithm. An
obvious extension handles a larger set of unknown features,
simply by changing the main loop to iterate over all possible
combinations of values for the unknown features. Of course
this only makes sense for combinations of nominal-valued
features, and not truly real-valued features.
However, the algorithm proposed by Fredrikson et al. has
various limitations. Most obviously, it cannot be used when
the unknown features cover an intractably large set. One
example of such a setting is facial recognition, where the
feature space is huge: in the facial recognition examples con-
sidered later d ≈ 10, 304 with each feature a real number in
[0, 1]. Even if one only wanted to infer a portion of the fea-
tures this is computationally infeasible. A more subtle issue,
explored in the next section, is that even when eﬃcient it
may not be eﬀective.
It turns out that we can give attacks that overcome both
issues above by giving inversion attacks that utilize the con-
ﬁdence information revealed by ML APIs. To do so will re-
quire new techniques, which we explore using as case stud-
ies decision trees (the next section) and facial recognition
(Section 5).
4. MAP INVERTERS FOR TREES
We turn now to inversion attacks on decision trees. This
type of model is used on a wide range of data, and is often
favored because tree-structured rules are easy for users to
understand. There are two types of decision trees common
in the literature: classiﬁcation (where the class variable is
discrete), and regression (where the class variable is contin-
uous). In the following, we focus on classiﬁcation trees.
Decision tree background. A decision tree model re-
cursively partitions the feature space into disjoint regions
R1, . . . , Rm. Predictions are made for an instance (x, y) by
ﬁnding the region containing x, and returning the most likely
value for y observed in the training data within that region.
We characterize trees mathematically as follows:
wiφi(x), where φi(x) ∈ {0, 1}
f (x) =
m(cid:88)
i=1
where each basis function φi is an indicator for Ri, and wi
corresponds to the most common response observed in the
training set within Ri. A simple example is shown in Fig-
ure 3. Notice that there is exactly one basis function for
each path through the tree, and that a given x will “acti-
vate” only one φi (and thus traverse only one path through
the tree) because the basis functions partition the feature
space.
Decision trees can be extended to return conﬁdence mea-
sures for classiﬁcations by changing the form the wi coeﬃ-
cients take. This is usually accomplished by setting wi to
a vector corresponding to the distribution of class labels in
a given region, as observed in the training set. Looking at
the example in Figure 3, we would set w1 = [89, 11] if there
were 89 training examples with x1 = 1 and x2 = 0, and 11
with x1 = 1, x2 = 1. The classiﬁcation and corresponding
conﬁdences are given by:
i=1 wi[j]φi(x)(cid:1) , and
(cid:0)(cid:80)m
(cid:80)
wi∗ [|Y |]
i wm[i]
, . . . ,
(cid:21)
(cid:20) wi∗ [1](cid:80)
i w1[i]
f (x) = arg maxj
˜f (x) =
where i∗ in the second formula takes the value in {1, . . . , m}
such that φi∗ (x) = 1.
Decision tree APIs. Several web APIs expose training
and querying routines for decision trees to end-users, in-
cluding Microsoft Machine Learning [31], Wise.io [40], and
BigML [4]. Users can upload their datasets to these services,
train a decision tree to predict selected variables, and then
make the resulting tree available for others to use for free or
for charge per-query. We use as running example the API
exposed by BigML, as it currently has the largest market-
place of trained models and focuses on decision trees. The
results carry over to services with a similar API as well.
BigML allows users to publish trees in either black-box or
white-box mode.
In black-box mode, other users are only
allowed to query the decision tree for predictions using a
REST API. In white-box mode, users are allowed to see the
internal structure of the tree, as well as download a JSON
representation of it for local use. The amount of available
information about the training set therefore varies between
the two modes. In both settings, the adversary has access to
marginal priors for each feature of the training set (see Sec-
tion 3 for a description of this), in addition to a confusion
matrix C for which Ci,j gives the number of training in-
stances with y = i for which the model predicted label j. In
the white-box setting, the attacker also has access to a count
ni of the number of training set instances that match path
1325φi in the tree. This allows one to compute the conﬁdence of
a particular prediction.
The inversion problem. Fix a tree f (x) =(cid:80)m
i=1 wiφi(x)
and let (x, y) be a target instance that will either be from
the training data db or not (we will be clear in the following
about which setting we are in). We assume for simplicity
that there is one sensitive feature, the ﬁrst, making the tar-
get feature set in this case T = {1}; extending our attacks
to more than one feature is straightforward. The side infor-
mation side(cid:96)(x, y) = (x(cid:96), . . . , xd, y) for some to-be-speciﬁed
(cid:96) ≥ 2. We let K = {(cid:96), . . . , d} be the set of known feature
indices, and we abuse notation slightly to let xK represent
the d − 1 dimensional vector equal to x(cid:96), . . . , xd.
Black-box MI. For black-box MI we turn to the generic
algorithm from Section 3 and adapt it to this setting. The
main diﬀerence from the prior work’s setting is that the de-
cision tree models we consider produce discrete outputs, and
the error model information is diﬀerent, being a confusion
matrix as opposed to the standard deviation of a Gaussian.
For our purposes here, then, we use the confusion matrix C
and deﬁne err(y, y(cid:48)) ∝ Pr [ f (x) = y(cid:48) | y is the true label ].
In Section 4.1 we evaluate the algorithm of Figure 2, with
this error model, showing that it has unsatisfying perfor-
mance in terms of a prohibitively high false positive rate.
to φi. From this, he also knows N = (cid:80)m
White-box MI. Recall from before that in the white-box
setting, we not only assume that the attacker knows each φi,
but also the number of training samples ni that correspond
i=1 ni, the total
number of samples in the training set.
The known values xK induce a set of paths S = {si}1≤i≤m:
K = xK ∧ φi(x(cid:48))}. Each path
S = {(φi, ni) | ∃x(cid:48) ∈ Rd . x(cid:48)
corresponds to a basis function φi and a sample count ni.
We let pi denote ni/N , and note that each pi gives us some
information about the joint distribution on features used to
build the training set. In the following, we write si as short-
hand for the event that a row drawn from the joint prior
traverses the path si, i.e., Pr [ si ] corresponds to the proba-
bility of drawing a row from the joint prior that traverses si.
Observe that pi is an empirical estimate for this quantity,
derived from the draws used to produce the training set.
Recall that the basis functions partition the feature space,
so we know that x traverses exactly one of the paths in S.
Below we denote a speciﬁc value for the ﬁrst feature by v
and a speciﬁc value for the other d − 1 features as vK . We
will abuse notation slightly and write φi(v) as shorthand
1 = v ∧ φi(x(cid:48))).
for the indicator function I(∃x(cid:48) ∈ Rd . x(cid:48)
The following estimator characterizes the probability that
x1 = v given that x traverses one of the paths s1, . . . , sm
and xK = vK :
Pr [ x1 = v | (s1 ∨ ··· ∨ sm) ∧ xK = vK ]
piφi(v) · Pr [ xK = vK ] · Pr [ x1 = v ]
∝ m(cid:88)
1(cid:80)m
∝
i=1
(cid:80)m
(cid:88)
j=1 pjφj(v)
piφi(v) · Pr [ x1 = v ]
(1)
j=1 pjφj(v)
1≤i≤m
We refer to this as the white-box with counts (WBWC) esti-
mator. The adversary then outputs a value for v that max-
imizes (1) as a guess for x1. Like the Fredrikson et al. esti-
mator, it returns the MAP prediction given the additional
count information.
In the preceding analysis, we assumed that the attacker
knew all of x except x1. The WBWC estimator can be
extended to the general case where the attacker does not
know x2, . . . , xl (so xK = {l + 1, . . . , d− 1}) by summing (1)
over the unknown variables.
4.1 Experiments
We applied the black-box and white-box WBWC attacks
to decision trees trained on two datasets: FiveThirtyEight’s
“How Americans Like Their Steak” survey [17], and a subset
of the General Social Survey (GSS) focusing on responses re-
lated to marital happiness [33]. Each dataset contains rows
that correspond to individuals, with attributes correspond-
ing to survey responses. These datasets contain at least
one sensitive feature, and have been used to derive models
that are available on BigML’s marketplace. Additionally,
the source data is public, which makes them appropriate
surrogates for our study—without source data, we cannot
evaluate the eﬀectiveness of our attacks.
FiveThirtyEight survey. In May 2014, Walt Hickey wrote
an article for FiveThirtyEight’s DataLab section that at-
tempted a statistical analysis of the connection between peo-
ples’ steak preparation preferences and their propensity for
risk-taking behaviors [17]. To support the analysis, FiveThir-
tyEight commissioned a survey of 553 individuals from Sur-
veyMonkey, which collected responses to questions such as:
“Do you ever smoke cigarettes?”, “Have you ever cheated
on your signiﬁcant other?”, and of course, “How do you like
your steak prepared?”. Demographic characteristics such as
age, gender, household income, education, and census region
were also collected. We discarded rows that did not contain
responses for the inﬁdelity question or the steak preparation
question, resulting in a total of 332 rows for the inversion
experiments. We used model inversion on the decision tree
learned from this dataset to infer whether each participant
responded “Yes” to the question about inﬁdelity.
GSS marital happiness survey. The General Social Sur-
vey (GSS) collects detailed information on the demograph-
ics, interests, and attitudes of United States residents [37].
We use a subset of the GSS data [33] created by Joseph
Price for the purposes of studying various societal eﬀects of
pornography. This subset corresponds to 51,020 individuals
and 11 variables, including basic demographic information
and responses to questions such as, “How happy are you in
your marriage?” and “Have you watched X-rated movies in
the last year?” We discarded rows that did not contain re-
sponses to either of these questions, resulting in 16,127 total
rows for the inversion experiments. We use model inversion
to infer each participant’s response to the latter question.
Summary of results. For both datasets, we were able to
identify positive instances of the sensitive variable (i.e., a
“Yes” answer to “Have you ever cheated on your signiﬁcant
other?” or “Have you watched X-rated movies in the last
year?”) with high precision. Key ﬁndings are:
• Given white-box access to the BigML trees published by
others for these datasets, we are able to predict positive
instances with perfect precision, i.e., no false positives.
• Individuals whose responses are used in the training data
are at signiﬁcantly higher risk than individuals not in-
cluded in the training data. The results are stark: on
the FiveThirtyEight survey, white-box inversion yields
1326on average 593× improved precision and 371× improved
recall. Similar results hold for the GSS survey.
• White-box access to decision trees enhances the adver-
sary’s advantage. On the BigML tree trained using GSS
data, the white-box adversary has a signiﬁcant boost in
precision over the black-box adversary (158% greater)