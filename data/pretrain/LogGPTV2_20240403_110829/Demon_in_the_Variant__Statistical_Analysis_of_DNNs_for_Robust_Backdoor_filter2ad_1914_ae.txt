solution [6]. Here, we kept the settings the same as those
described in the original work [13] and changed the optimiza-
tion objective to seek a trigger that can signiﬁcantly lower
the test statistic J∗ of the target class. Speciﬁcally, starting
from a randomly sampled trigger, our experiment repeats
the following steps, until J∗ goes below the threshold exp(2)
or a pre-determined number of iterations has been reached
(10000): 1) performing TaCT to inject images with currently
disturbed trigger, 2) training the target model on the infected
dataset, 3) running SCAn to get J∗, 5) calculating the deriva-
tive (running [13]) according to the J∗ of the target class, and
6) updating the trigger by subtracting the derivative. The ex-
periment was performed on GTSRB, since training a model
on the dataset takes only several minutes. However, even after
10000 iterations, which took a month on a two-GPU system,
the approach still failed to reduce the J∗ in a meaningful
way, as illustrated in Fig 19. From the ﬁgure, we can see that
not only has J∗ not decreased, but the norm of the trigger
(32x32 images with pixels in [0,1]3) fails to see any signiﬁ-
cant change during the iterations, indicating that the derivative
algorithm we used cannot ﬁnd a trigger capable of bypassing
SCAn.
5 Discussion
Limitations of SCAn. As mentioned earlier, SCAn utilizes
a set of clean data for the contamination analysis. We be-
lieve that this requirement is reasonable, since a small clean
dataset is often provided by the model provider for testing
the model’s performance, as also assumed in the prior stud-
ies [8, 9]. Note that the size of this clean dataset can be just
1% of the training set for defeating the attack involving up
to 8 triggers (Section 4.5). Also, our approach relies on the
presence of attack images (carrying triggers) to identify an
infected class. Further, we only evaluated SCAn on image
classiﬁcation tasks. However, we believe that there is a poten-
tial to extend our approach to mitigate the threat posed by the
backdoor using a non-image trigger. Behind SCAn is our in-
sight that the globally statistical information about a model’s
representations can help untangle a speciﬁc class. Such in-
formation is described in our research using the covariance
matrix (Sε) of multivariate normal distribution, which helps
effectively untangle different classes. This ﬁnding indicates
that the multivariate normal distribution provides a good de-
scription of high-dimensional representations generated from
a large amount of data (tens of thousands images). Since such
representations also characterize some non-vision tasks, such
as code analysis, it is likely that our modeling can also apply
to identify Trojaned inputs in these tasks. Further exploration
on this direction is left to the future research.
Future research. Down the road, we will seek more efﬁcient
techniques to untangle mixed representations, e.g., using deep
learning with GPU acceleration, and more precise approxi-
mation for a speciﬁc task. As an interesting observation, our
experiments on MegaFace show that the classes containing
both baby and adult images have a higher J∗ than other normal
classes, even though this anomaly is still well below those of
infected classes. This may indicate that our method could help
mine hard-negative examples, for evaluating a DNN model’s
classiﬁcation quality.
USENIX Association
30th USENIX Security Symposium    1553
01000200030004000Norm00.20.40.60.81% of dataOriginalDistance0200040006000800010000# of iteration0246810Ln(J*)33.544.55Norm of triggerLn(J*)Norm6 Related Works
We present a new protection SCAn that can defeat our at-
tack TaCT designed for injecting source-speciﬁc triggers into
the target model. Such a trigger has been brieﬂy mentioned
in NC [42] and STRIP [9], without details about how to
launch the attack. The NC paper discussed the potential to de-
tect source-speciﬁc triggers when running NC O(Nlog2(N))
rounds for N classes. We argue that the computational com-
plexity will increase to O(N2) in the presence of TaCT, given
that NC’s recall is just 6.5% on TaCT, as demonstrated in
Section 3.2. As a result, the divided-and-conquer algorithm
cannot be used to reduce the complexity, which makes the
approach less practical when N is huge (Table 7). By com-
parison, SCAn defeats TaCT with a complexity O(N), by
testing every class once. Liu et al. [22] proposed the ﬁne-
pruning method. They ﬁrst prune neurons that are dormant
when processing clean data until the accuracy tested on a hold-
on dataset below a threshold, and then ﬁne-tune the pruned
model to recover the accuracy. Their defense relies on exten-
sive interactions with the training process. In contrast, our
approach only needs to go through dataset in two rounds and
is independent of the training of the target model. Other re-
lated approaches, as discussed in Section 3.2, are all defeated
by TaCT, with SCAn being the only solution working on the
attack. Nelson et al. [28] and Baracaldo et al. [2] proposed
two general protections against backdoor attack. Both meth-
ods require extensive retraining of the model on the datasets
with the similar size as the original one, which is often in-
feasible for DNNs. Additionally, they detect infected data by
evaluating the overall performance of the model. However,
the overall performance of the infected model often remains
good under current advanced attacks (like TaCT), and thus
these methods will become ineffective against these attacks.
In the traditional statistical analysis domain, a review written
by Victoria et al. [12] summarizes several effective outlier
detection methods, including k-nearest neighbors (k-nn) [14],
k-means [26] and principal components analysis (pca) [17].
To ﬁnd out whether directly applying them to sample repre-
sentations can detect infected classes, we ran these methods
on the representations produced by a TaCT infected model
for the images in the target class. The results on Fig. 21 show
that these methods cause many false positives.
7 Conclusion
Our work demonstrated that backdoors created by conven-
tional data poisoning attacks are source-agnostic and charac-
terized by unique representations generated for attack images,
which are mostly determined by the trigger, regardless of other
image content, and clearly distinguishable from those for nor-
mal images. Those four existing detection techniques rely
on these proprieties and all fail to raise the bar to black-box
attacks injecting source-speciﬁc backdoors like TaCT. Based
on leveraging the distribution of the sample representations
through a two-component model, we designed a statistical
method SCAn to untangle representations of each class into
a mixture model, and utilized a likelihood ratio test to detect
an infected class. The effectiveness and robustness of SCAn
were demonstrated through extensive experiments. Our study
takes a step forward to understand the mechanism of implant-
ing a backdoor within a DNN model and how a backdoor
looks like from the perspective of model’s representations. It
may lead to deeper understanding of neural networks.
Acknowledgment
We thank our anonymous reviewers for their comprehensive
feedback. This work was supported in part by the General
Research Funds (Project No. 14208019) established under
the University Grant Committee of the Hong Kong SAR., the
Chinese University of Hong Kong research contract agree-
ment (Contract No. TS1711490), and the IARPA (Grant No.
W91NF-20-C-0034) the TrojAI project.
References
[1] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deb-
orah Estrin, and Vitaly Shmatikov. How to backdoor
federated learning. CoRR, abs/1807.00459, 2018.
[2] Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and
Jaehoon Amir Safavi. Mitigating poisoning attacks on
machine learning models: A data provenance based ap-
In Proceedings of the 10th ACM Workshop
proach.
on Artiﬁcial Intelligence and Security, pages 103–110.
ACM, 2017.
[3] George EP Box, William Gordon Hunter, J Stuart
Hunter, et al. Statistics for experimenters, volume 664.
John Wiley and sons New York, 1978.
[4] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo,
Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian
Molloy, and Biplav Srivastava. Detecting backdoor at-
tacks on deep neural networks by activation clustering.
In Workshop on Artiﬁcial Intelligence Safety 2019 co-
located with the Thirty-Third AAAI Conference on Ar-
tiﬁcial Intelligence 2019 (AAAI-19), Honolulu, Hawaii,
January 27, 2019., 2019.
[5] Dong Chen, Xudong Cao, Liwei Wang, Fang Wen, and
Jian Sun. Bayesian face revisited: A joint formulation.
In Computer Vision - ECCV 2012 - 12th European Con-
ference on Computer Vision, Florence, Italy, October
7-13, 2012, Proceedings, Part III, pages 566–579, 2012.
[6] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi,
and Cho-Jui Hsieh. ZOO: zeroth order optimization
based black-box attacks to deep neural networks with-
out training substitute models. In Bhavani M. Thurais-
ingham, Battista Biggio, David Mandell Freeman, Brad
1554    30th USENIX Security Symposium
USENIX Association
Miller, and Arunesh Sinha, editors, Proceedings of the
10th ACM Workshop on Artiﬁcial Intelligence and Secu-
rity, AISec@CCS 2017, Dallas, TX, USA, November 3,
2017, pages 15–26. ACM, 2017.
[17] Flip Korn, Alexandros Labrinidis, Yannis Kotidis, Chris-
tos Faloutsos, Alex Kaplunovich, and Dejan Perkovic.
Quantiﬁable data mining using principal component
analysis. Technical report, 1998.
[7] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and
Dawn Song. Targeted backdoor attacks on deep learning
systems using data poisoning. CoRR, abs/1712.05526,
2017.
[8] Edward Chou, Florian Tramèr, Giancarlo Pellegrino, and
Dan Boneh. Sentinet: Detecting physical attacks against
deep learning systems. CoRR, abs/1812.00292, 2018.
[9] Yansong Gao, Change Xu, Derui Wang, Shiping Chen,
Damith Chinthana Ranasinghe, and Surya Nepal.
STRIP: a defence against trojan attacks on deep neural
networks. In David Balenson, editor, Proceedings of
the 35th Annual Computer Security Applications Con-
ference, ACSAC 2019, San Juan, PR, USA, December
09-13, 2019, pages 113–125. ACM, 2019.
[10] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
Badnets: Identifying vulnerabilities in the machine
learning model supply chain. CoRR, abs/1708.06733,
2017.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 770–778, 2016.
[12] Victoria J. Hodge and Jim Austin. A survey of outlier
detection methodologies. Artif. Intell. Rev., 22(2):85–
126, 2004.
[13] Andrew Ilyas, Logan Engstrom, Anish Athalye, and
Jessy Lin. Black-box adversarial attacks with limited
queries and information. In Jennifer G. Dy and Andreas
Krause, editors, Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stock-
holmsmässan, Stockholm, Sweden, July 10-15, 2018, vol-
ume 80 of Proceedings of Machine Learning Research,
pages 2142–2151. PMLR, 2018.
[14] Edwin M Knox and Raymond T Ng. Algorithms for
mining distancebased outliers in large datasets. In Pro-
ceedings of the international conference on very large
data bases, pages 392–403. Citeseer, 1998.
[15] Karl-Rudolf Koch. Parameter estimation and hypothesis
testing in linear models. Springer, 1988.
[16] Pang Wei Koh and Percy Liang. Understanding black-
In Proceed-
box predictions via inﬂuence functions.
ings of the 34th International Conference on Machine
Learning-Volume 70, pages 1885–1894. JMLR. org,
2017.
[18] Alex Krizhevsky and Geoffrey Hinton. Learning multi-
ple layers of features from tiny images. Technical report,
Citeseer, 2009.
[19] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick
Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.
[20] Christophe Leys, Christophe Ley, Olivier Klein, Philippe
Bernard, and Laurent Licata. Detecting outliers: Do not
use standard deviation around the mean, use absolute
deviation around the median. Journal of Experimental
Social Psychology, 49(4):764–766, 2013.
[21] Zhengxiong Li, Aditya Singh Rathore, Chen Song,
Sheng Wei, Yanzhi Wang, and Wenyao Xu. Printracker:
Fingerprinting 3d printers using commodity scanners. In
Proceedings of the 2018 ACM SIGSAC Conference on
Computer and Communications Security, pages 1306–
1323. ACM, 2018.
[22] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
Fine-pruning: Defending against backdooring attacks
on deep neural networks. In Michael Bailey, Thorsten
Holz, Manolis Stamatogiannakis, and Sotiris Ioannidis,
editors, Research in Attacks, Intrusions, and Defenses -
21st International Symposium, RAID 2018, Heraklion,
Crete, Greece, September 10-12, 2018, Proceedings, vol-
ume 11050 of Lecture Notes in Computer Science, pages
273–294. Springer, 2018.
[23] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing
Ma, Yousra Aafer, and Xiangyu Zhang. ABS: scan-
ning neural networks for back-doors by artiﬁcial brain
stimulation. In Lorenzo Cavallaro, Johannes Kinder,
XiaoFeng Wang, and Jonathan Katz, editors, Proceed-
ings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security, CCS 2019, London, UK,
November 11-15, 2019, pages 1265–1282. ACM, 2019.
[24] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan
Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang.
Trojaning attack on neural networks. In 25th Annual
Network and Distributed System Security Symposium,
NDSS 2018, San Diego, California, USA, February 18-
21, 2018, 2018.
[25] Sebastian Mika, Gunnar Ratsch, Jason Weston, Bern-
hard Scholkopf, and Klaus-Robert Mullers. Fisher dis-
In Neural networks
criminant analysis with kernels.
for signal processing IX: Proceedings of the 1999 IEEE
USENIX Association
30th USENIX Security Symposium    1555
signal processing society workshop (cat. no. 98th8468),
pages 41–48. Ieee, 1999.
[26] Alexandre Nairac, Neil Townsend, Roy Carr, Steve King,
Peter Cowley, and Lionel Tarassenko. A system for
Integrated
the analysis of jet engine vibration data.
Computer-Aided Engineering, 6(1):53–66, 1999.
[27] Aaron Nech and Ira Kemelmacher-Shlizerman. Level
playing ﬁeld for million scale face recognition. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2017.
[28] Blaine Nelson, Marco Barreno, Fuching Jack Chi, An-
thony D Joseph, Benjamin IP Rubinstein, Udam Saini,
Charles Sutton, JD Tygar, and Kai Xia. Misleading learn-
ers: Co-opting your spam ﬁlter. In Machine learning in
cyber trust, pages 17–51. Springer, 2009.
[29] Hong-Wei Ng and Stefan Winkler. A data-driven ap-
proach to cleaning large face datasets. In 2014 IEEE
International Conference on Image Processing (ICIP),
pages 343–347. IEEE, 2014.
[30] Ximing Qiao, Yukun Yang, and Hai Li. Defending neu-
ral backdoors via generative distribution modeling. In
Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d’Alché-Buc, Emily B. Fox, and Roman
Garnett, editors, Advances in Neural Information Pro-
cessing Systems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019, 8-14
December 2019, Vancouver, BC, Canada, pages 14004–
14013, 2019.
[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, Alexan-
der C. Berg, and Li Fei-Fei.
ImageNet Large Scale
Visual Recognition Challenge. International Journal of
Computer Vision (IJCV), 115(3):211–252, 2015.
[32] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian
Suciu, Christoph Studer, Tudor Dumitras, and Tom Gold-
stein. Poison frogs! targeted clean-label poisoning at-
tacks on neural networks. In Advances in Neural Infor-
mation Processing Systems, pages 6103–6113, 2018.
[33] Karen Simonyan and Andrew Zisserman. Very deep con-
volutional networks for large-scale image recognition.
In 3rd International Conference on Learning Represen-
tations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, 2015.
[34] Chawin Sitawarin, Arjun Nitin Bhagoji, Arsalan Mose-
nia, Mung Chiang, and Prateek Mittal. DARTS: de-
ceiving autonomous cars with toxic signs. CoRR,
abs/1802.06430, 2018.
[35] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and
Christian Igel. Man vs. computer: Benchmarking ma-
chine learning algorithms for trafﬁc sign recognition.
Neural Networks, 32:323–332, 2012.
[36] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke,
and Alexander A Alemi. Inception-v4, inception-resnet
and the impact of residual connections on learning. In
Thirty-First AAAI Conference on Artiﬁcial Intelligence,
2017.
[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-
manet, Scott Reed, Dragomir Anguelov, Dumitru Erhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going
deeper with convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 1–9, 2015.
[38] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In
2nd International Conference on Learning Representa-
tions, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
[39] Tuan A Tang, Lotﬁ Mhamdi, Des McLernon, Syed
Ali Raza Zaidi, and Mounir Ghogho. Deep learning
approach for network intrusion detection in software de-
ﬁned networking. In 2016 International Conference on
Wireless Networks and Mobile Communications (WIN-