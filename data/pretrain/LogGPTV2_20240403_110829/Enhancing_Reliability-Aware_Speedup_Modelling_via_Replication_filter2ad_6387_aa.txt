title:Enhancing Reliability-Aware Speedup Modelling via Replication
author:Zaeem Hussain and
Taieb Znati and
Rami G. Melhem
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Enhancing Reliability-Aware Speedup Modeling via
Replication
Zaeem Hussain
Taieb Znati
Rami Melhem
School of Computing and Information
School of Computing and Information
School of Computing and Information
University of Pittsburgh
Pittsburgh, USA
PI:EMAIL
University of Pittsburgh
Pittsburgh, USA
PI:EMAIL
University of Pittsburgh
Pittsburgh, USA
PI:EMAIL
Abstract—Reliability-aware speedup models study the ex-
pected speedup of a parallel application as a function of the
number of processors, on a platform susceptible to processor
failures. Existing works in this area have developed models
using checkpoint-restart (without replication) as the only fault
tolerance mechanism, and have studied the upper bound on the
number of processors beyond which the application speedup
starts to degrade due to increasing likelihood of failure. In
this work, we develop speedup models in which replication,
speciﬁcally dual replication, is also employed for resilience. We
demonstrate that the upper bound on the number of processors
to execute a perfectly parallel application using dual replication
is of the order λ−2 where λ is the individual processor failure
rate. We also compare the dual replication model with that
of no-replication. Speciﬁcally, we found that, given the same
hardware resources, replication starts offering better speedup
just before the upper bound on the number of processors for no-
replication is reached. Taken together, our results indicate that
replication can signiﬁcantly enhance reliability-aware speedup
models by i) pushing the number of processors that yield the
optimal speedup to a much higher value than what is possible
without replication, and ii) improving on the optimal speedup
possible through checkpoint-restart alone.
Index Terms—fault tolerance, reliability, speedup, modelling,
parallel, hpc
I. INTRODUCTION
On a failure free platform, the performance of a parallel high
performance computing (HPC) workload always improves
with the number of processors if the application speedup
follows Amdahl’s law. However, if the application is executing
on a platform where individual processors are vulnerable to
failures, it is no longer true that executing the application
over a larger number of processors always results in an
improvement
in job completion time. This is because the
increase in scale also increases the frequency of failures,
thus increasing the fraction of time spent in checkpointing
and recovery. Eventually, this wasted time starts to outweigh
any gains made by further parallelizing the workload and
thus adding more processors starts hurting the application
performance. This raises the question: what is the optimal
number of processors at which a workload can achieve its best
possible average speedup, given a platform speciﬁc processor
failure rate?
To answer the above question, one ﬁrst needs to de-
the fail-
velop a speedup model
takes into account
that
ure rate as well as the type and cost of the fault
toler-
ance mechanism(s) employed. Several past works [1] [2] [3]
have explored such reliability-aware speedup models using
checkpoint-restart (C/R) as the sole fault tolerance mechanism,
with [1] providing theoretical results on the order of optimal
processor counts in terms of failure rate λ. On the other hand,
it has been projected in [4] that, at large scales, the efﬁciency
with C/R alone will degrade signiﬁcantly and that using repli-
cation (paired with C/R) will be a more efﬁcient alternative.
Thus, it is reasonable to explore whether adding replication
can signiﬁcantly improve reliability-aware speedups at larger
scales. Our paper obtains several interesting insights in this
direction, making the following contributions:
• We ﬁrst extend the analysis of reliability-aware speedups
without replication and show that the optimal number of
processors is of the order of λ−1 for perfectly parallel
applications.
• We develop reliability-aware speedup models for dual
replication. Using these models, we study the optimal
number of processors for dual replication and show
that this number is of the order of λ−2 for perfectly
parallel applications and λ−2/5 for applications with non-
negligible sequential component. In contrast, the same
bounds on the optimal number of processors without
replication are much lower.
• We analytically compare the performance of replication
with no replication when the optimal number of proces-
sors for no replication is used and determine conditions
under which replication outperforms no replication at
those processor counts.
• Using a simulator with failure rates typical of those
experienced by nodes in real world large scale systems,
we demonstrate the applicability of our theoretical results
in practical settings.
• We also account for the overhead of replication and study
how the speedup in the presence of this overhead is
impacted.
• We demonstrate the generality of our conclusions to other
failure distributions by empirically evaluating them for
the Weibull distribution as well.
Our analytical and numerical results combined indicate that the
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00065
528
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
optimal number of processors when using replication is much
higher than what is possible through C/R alone. Furthermore,
we show that the speedup, using replication, becomes better
than that of no-replication just before the optimal value for
the latter is achieved. This suggests that any discussion on
reliability-aware speedups would not be complete without
considering replication. This paper takes the ﬁrst step in this
direction by performing a deep analytical study for the case
of dual replication.
While the primary contribution of this work is towards
extending the theory of reliability-aware speedups by incor-
porating dual replication, our empirical results using realistic
platform parameters offer further practical insights. For ex-
ample, we show that, although the optimal processor counts
using replication are much higher than the scales of current and
upcoming systems, we are approaching system scales which
are in the same neighborhood as the optimal processor counts
of C/R alone without replication. The signiﬁcance of our
results against this backdrop is twofold: i) We are approaching
system scales where larger workloads may have to consider
replication as an additional fault tolerance mechanism in order
to continue getting improved speedups at higher processor
counts in the presence of failures, and ii) while current system
scales may be close to hitting the peak of the best reliability-
aware speedup possible with C/R alone, these scales are not
near to hitting the theoretical barrier of the best possible
speedup in presence of failures if replication is also considered.
The rest of this paper is organized as follows: Section
II presents the necessary background and notations, Section
III discusses results on the optimal number of processors,
Section IV compares the performance of replication and no
replication at optimal processor counts for no-replication,
Section V discusses the impact of overhead of replication on
the speedups, Section VI studies the speedups for Weibull node
failures, Section VII discusses related work and Section VIII
concludes this paper.
II. BACKGROUND
This section describes the background information needed
to understand the mathematical development in subsequent
sections. Table I lists all of the notations used in the paper
along with their description. We keep the notations consistent
with those used in [1] where possible. It should be noted that
all the quantities below that depend on P will have differ-
ent formulae depending on whether replication is employed
or not. It should also be noted that P refers to the total
number of processors being used, which means that, for dual
replication, P/2 processors will be replicas of the other P/2
processors. Thus, all comparisons between the performance of
replication and no-replication are made with the same number
of total processors, P , used by each technique. This also
means that, in cases where replication has better speedup than
no-replication, the expected energy cost of replication will
be lower, simply because a job using P nodes will ﬁnish
quicker with replication than without it. It should be noted
that, while replication can certainly outperform no-replication
TABLE I
NOTATIONS
λ = 1/μ
C
R
D
τ
α
Snorep(P ), Srep(P )
Snorep(P ), Srep(P )
Enorep(P ), Erep(P )
Hnorep(P ), Hrep(P )
δ
Failure rate of a processor
Checkpointing cost
Recovery cost
Downtime
Checkpointing interval duration
The sequential fraction of a workload that
cannot be parallelized
Speedup on P processors. without failures and
C/R (without and with replication, respectively)
Expected speedup on P processors
under failures and with C/R
(without and with replication, respectively)
Expected time, under failures and with C/R
(without and with replication, respectively),
to ﬁnish Snorep(P )/Srep(P ) units of work
Expected time, under failures and with C/R
(without and with replication, respectively),
on a P processor platform to ﬁnish work that
takes a unit of time on a single processor
Replication overhead coefﬁcient
at high processor counts, this happens at system scales at
which frequent checkpoints and recovery due to failures start
taking more time than what is spent in useful work. Although
replication distributes work over only half the nodes, it avoids
frequent checkpoints and can tolerate many processor failures
before requiring re-execution.
We consider a job model in which the work is distributed
among the available processors at the time of job start, and
there is no work stealing. Thus, the number of processors and
the work per processor remains ﬁxed throughout an execution
and, upon single processor fail, the job recovers from last
checkpoint and continues with same number of processors.
For most of the paper (except Section VI), we consider a
platform where each processor has an exponential failure
distribution with mean time between failure (MTBF) μ, or
equivalently, rate λ = 1/μ. We assume fail stop errors that
halt the processor hit with a failure. For a parallel job using
a total of P processors, let λP denote the resulting failure
rate. When no replication is used, λP = λP . With replication,
however, not every processor failure interrupts the execution
of the workload. The execution is interrupted only when a
processor and its replica fail. Thus, for replication, the quantity
of interest is the Mean Time To such Interrupts (or MTTI),
using which we can again deﬁne the failure rate of replication
as λP = 1/M T T IP . A general closed form for λP for
replication is not known. For dual replication though, a closed
form expression was recently derived in [5] where the authors
showed that λP = λP (1 +
2P/π (for large
P ), which is the value we will use in our model for the failure
rate of dual replication.
/2P ) ≈ λ
(cid:4)
P
P/2
(cid:2)
(cid:3)
√
We take the checkpointing interval to be equal to Young’s
[6] ﬁrst order approximation for the optimum checkpointing
interval, given by τ =
2CμP . Here, μP = 1/λP is the
system MTBF of P processors in case of no replication or
the MTTI of a dually replicated execution with a total of P
processors as discussed above.
529
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
1
1
We use Amdahl’s law [7] to model the failure free speedup.
Hence, without replication, we have Snorep(P ) =
α+(1−α)/P ,
where α is the sequential fraction of the workload. For
dual replication with P total processors, the parallel work is
divided over P/2 processors only, which means the failure-
free speedup will be Srep(P ) =
α+2(1−α)/P . We can see
that both of these expressions are continuously increasing
functions of P , which is not
the case when failures are
taken into account. With failures, we are interested in the
average speedup behavior of a workload, and we denote that
by Snorep(P ) and Srep(P ) for the two cases, respectively.
Clearly, Snorep(P )/Srep(P ) is a function not just of α but
other parameters such as the failure rate (λ), checkpointing
cost (C), recovery cost (R) and the downtime after failure (D).
When considering the behavior of Snorep(P ) and Srep(P ),
one would expect these to initially be increasing functions of
P . However, as P grows and the failure frequency increases,
the expected speedups will eventually reach their respective
peaks and then start to decline as the growing overhead of
failures starts to dominate. The optimal (or equivalently, the
upper bound on) number of processors is the value of P that
maximizes the expected speedup (this value will be different
for replication and no replication). This is also equivalent to
ﬁnding P that minimizes Hnorep(P ) = 1/Snorep(P ) (and
similarly, with replication), where Hnorep(P )/Hrep(P ) is the
expected time to ﬁnish a unit of work on P processors, without
and with replication, respectively.
We can further write Hnorep(P ) = Enorep(P )/Snorep(P )
(and similarly for replication), where Enorep(P )/Erep(P )
represent the time it would take, under failures, to complete
Snorep(P )/Srep(P ) units of work. Note that, without failures,
Snorep(P )/Srep(P ) units of work can be completed on P
processors in one unit of time. Thus Enorep(P )/Erep(P )
represent the expected time under failures on P processors
normalized by the failure-free time on the same number of
processors. Enorep(P ) and Erep(P ) will be modeled using
different approaches, which we discuss below.
A. Expected Time without Replication
For individual processor failures that follow the exponential
failure distribution with rate λ, the resulting failure distribution
on P processors without replication is also exponential with
rate λP = λP . Thus, one can use the memoryless property
to simplify the derivation of expected completion time. The
memoryless property ensures that the likelihood of failure
within an interval does not depend on when the previous
failure happened. Thus, it sufﬁces to estimate Enorep,τ (P ),
the expected time to ﬁnish a single checkpoint interval, which
is given by [8]
Enorep,τ (P ) = (
1
λP + D)eλP R
(eλP (τ +C) − 1)
(1)
Enorep(P )
Enorep,τ (P )/τ.
can then be
estimated as Enorep(P ) =
Fig. 1. Average behavior between consecutive failures.
B. Expected Time with Replication
When replication is employed, even if the individual failure
distributions are exponential, the distribution of failures that
interrupt a replicated job execution is not. For any distribution
other than the exponential, the memoryless property does not
hold. Therefore, the expected time cannot be modeled by
estimating the expected time to ﬁnish one interval in isolation.
While Equation 1 can still be used as an approximation to
estimate the expected completion time of a distribution with
mean M T T IP = 1/λP , for replication we will use the generic
approximation approach used in [9] which considers each
failure (deﬁned as the failure of a processor and its replica) as a
renewal process and computes the average time spent perform-
ing useful work between such consecutive failures, which is
the difference of the duration between these successive failures
and the time spent in performing extra tasks, Textra, as shown
in Figure 1.
Textra consists of two components: the time spent writing
checkpoints, and the time spent doing work that was wasted
due to failure in the interval in which the failure struck. The
average number of checkpoints within two successive failures
is given by M T T IP /τ. Thus, the average time spent writing
checkpoints will be given by C × M T T IP /τ. The second
component, which is the work wasted due to failure, is equal
to the expected time of failure within an interval of length τ
given that a failure happens within the interval. This value can
be written as kτ, where 0 < k <1 represents the expected
proportion of an interval that is lost due to a failure. We use
the ﬁrst order approximation [6] [10] for the value of k which
assumes that failure strikes in the middle of the interval on
average, i.e. k = 0.5. Putting all of this together, we get that
Textra = C × M T T IP /τ + τ /2. We can then write Erep(P )
as
Erep(P ) =
M T T IP
M T T IP − C×M T T IP
τ
− τ
2
(2)
(cid:4)
We will thus use the above equation to estimate the expected
completion time for dual replication, where M T T IP ≈
λ−1
π/2P from [5]. It should be noted, however, that Equa-
tion 2 breaks down when the denominator becomes negative,
i.e. when Textra exceeds M T T IP . However, as long as Textra
is less than and not too close to M T T IP , Equation 2 provides
a close approximation of the expected performance.
530
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
III. OPTIMAL PROCESSOR COUNT
We now investigate the optimal number of processors that
maximize the expected speedup (or equivalently, minimize the
(no)rep(P ), where
expected time to ﬁnish a unit of work, H
(no)rep(P )) under failures. This
H