mand injection pruning policies.)
Note that the pruning policies are determined by the out-
put operation rather than the input. Thus, different prun-
ing policies may be applied on the same input, depending
on whether it is being compared with an SQL output, shell
output or HTML output.
As shown in the next section, pruning policies are highly
effective in practice. Even though they still involve the use
of edit-distance algorithms, the lengths of input parameters
(as well as the strings they are matched against) are small
and hence lead to performance gains.
7 Evaluation
7.1 Attack Detection
Applications. Figure 7 shows the applications we studied
in our experimental evaluation. These applications spanned
multiple languages (Java, PHP, and C), and multiple plat-
forms (Apache/modPHP, Apache/Tomcat, Microsoft IIS),
and ranged in size from a few to several tens of KLoCs.
Some of these applications are popular web applications
used by millions of users, such as phpBB and SquirrelMail.
Although most applications were tested (for attack detec-
tion) with Apache as well as IIS, our discussion here is fo-
cused on Apache-based tests.
Figure 7 also shows the attacks used in our experiments.
Where possible, we selected attacks on widely-used appli-
cations, since it is likely that obvious security vulnerabilities
in such applications would have been ﬁxed, and hence we
are more likely to detect more complex attacks. However,
the effort involved in installing and recreating these “real-
world” exploits is signiﬁcant, which limits the number of
attacks that can be used in our evaluation. To mitigate this
Application
phpBB 2.0.5
SquirrelMail 1.4.0
SquirrelMail 1.2.10
PHP/XMLRPC
AMNESIA [8]
(5 apps)
WebGoat [20]
Langu-
age
PHP/C
PHP/C
PHP/C
PHP/C
Java/C
Java
Size
(lines)
34K
42K
35K
2K
30K
(total)
Environment
Attacks
Comments
IIS, Apache
IIS, Apache
IIS, Apache
IIS, Apache
Apache/Tomcat
SQL injection
Shell command injection
XSS
PHP command injection
SQL injection
Tomcat
HTTP response splitting
Shell command injection
CAN-2003-0486
CAN-2003-0990
CAN-2002-1341
CAN-2005-1921
21K attacks,
3.8K legitimate
Detec-
tion
Yes
Yes
Yes
Yes
100%
Yes
Yes
False
Positives
None
None
None
None
0%
None
None
Figure 7. Applications used in experimental evaluation
problem, we augmented our testing with WebGoat [20], a
J2EE application that was designed to illustrate common
web application vulnerabilities, and the AMNESIA dataset
[8] that has been used for evaluating several previous SQL
injection defenses [9, 10, 25, 26]. It consists of ﬁve10 Java
applications that implement a bookstore, an employee di-
rectory, an intranet portal, a classiﬁed advertisement ser-
vice, and an event listing service. A suite consisting of sev-
eral tens of thousands of legitimate requests and SQL injec-
tion attacks on these application is provided with the data
set. Some of these attacks were designed to evade detection
techniques, while the others were designed to comprehen-
sively test them.
Policies. For attack detection, we used the policies shown
in Figure 6. Command injection, SQL injection, and
HTTP Response-splitting attacks are detected by Tainted-
Cmd, SpanNodes and StraddleTrees policies, so we could
have done with just one of them. Nevertheless, we listed
them all to illustrate our policy language, and to show that
there are multiple ways to detect these attacks.
The two XSS policies are designed to capture the two
most common forms of XSS attacks. The TaintedScript-
Body policy captures so-called reﬂected XSS attacks, where
the script body is provided by an attacker using a form pa-
rameter (including those that may be included in a POST
body) or a cookie, and is unwittingly inserted by the server
into its HTML output. The TaintedScriptName captures a
variant of this attack, where the attacker injects the name
of a malicious script rather than its body. Note that the
XSS policies given above can be further improved by care-
fully considering all possible ways of injecting scripts, and
blocking all those avenues. This is a nontrivial task, as there
are many ways of introducing script content into a HTML
page. Nevertheless, our policy framework does provide the
mechanisms needed to for this purpose.
Note that in order to block the PHP command injection
attack, we needed a PHP parser. However, since our shell
10In reality, the dataset contains seven applications, but two of them
could not be successfully installed in our environment. However, these
two applications are qualitatively no different from the other ﬁve, so this
omission does not invalidate our results.
parser is quite generic, we were able to reuse it for parsing
PHP in this case.
HTTP response splitting [19] involves injection of addi-
tional HTTP headers, which may in turn enable other at-
tacks such as XSS. A vulnerable HTTP server inserts user-
provided data as the value for a HTTP header without ﬁrst
checking that this insertion does not end up creating new
headers. Our HTTP parser identiﬁes each HTTP header as
a cmd node, and hence a HTTP response splitting results in
the violation of TaintedCmd, SpanNode and StraddleTrees
policies.
Finally, our policy framework is powerful enough to de-
tect several other types of attacks such as XPath injection
and path traversals, although we did not include them in our
experiments.
Detection Summary. All of the attacks were detected
without generating any false positives. For all attacks
except those involving AMNESIA dataset, we manually
launched the attack and veriﬁed the results.
In the case of AMNESIA, the dataset consisted of 36,753
attacks. Of these, 15,337 attacks resulted in errors that were
caught by the application, and hence the application itself
did not issue a malicious query to the database. The re-
maining 21,416 attacks resulted in a malicious query being
sent to the database. Of these, 21,106 were recognized by
our technique as an injection attack and hence blocked. The
remaining 310 were blocked because they caused parse er-
rors in our SQL parser11. In summary, our technique was
able to block every malicious query sent to the database.
7.2 False Negatives
False negatives may arise due to weaknesses in taint in-
ference, input and output parsing, or policies. Our taint
inference techniques can lead to false negatives for appli-
cations that perform complex transformations on inputs. It
should be noted that the most common transformations arise
11In reality, many more attacks result in SQL queries with syntax errors.
Because ours is a “rough parser,” and because of its focus on graceful error
recovery, it is able to construct a meaningful AST in spite of those errors,
and discover policy violations.
Figure 8. Match conﬁdence.
due to various encodings used in HTTP, but these are al-
ready handled by our approach. Other than these, the most
common transformations performed by web applications re-
sult in small changes to the input, and are hence detected by
our approximate matching technique. However, if a web
application makes extensive use of application-speciﬁc en-
codings or input-to-output transformations, taint inference
may lead to signiﬁcant false negatives and hence should not
be applied to such applications.
False negatives will also occur with second-order injec-
tion attacks where the attackers ﬁrst inject their malicious
data into persistent storage (e.g., ﬁles or databases), and
this data is subsequently retrieved by the server and inserted
into its output. These second order attacks can escape taint-
based techniques unless taint information is also stored and
retrieved from persistent store.
Input and output parsing errors can also lead to false neg-
atives. This can happen in two ways. First, attackers could
send erroneous or very complex inputs, hoping to cause a
failure in the parsing code used by defensive mechanisms.
We mitigate this threat by (a) using rough parsers that parse
a superset of the input language, and (b) focusing on er-
ror recovery in our parsers. The resulting parsers are sufﬁ-
ciently robust that we are able to simply block inputs that
lead to unrecoverable parsing errors in our code.
A second evasion strategy that can lead to false negatives
is to exploit the differences between how data is interpreted
by the defensive mechanisms versus the actual target of the
data. For instance, the well-known MySpace worm (aka
Samy worm) exploited the fact that a syntactically incorrect
HTML fragment, which contained a newline character in
the middle of the string “javascript”, was “corrected” by a
browser that removed the newline. Unless this (odd) behav-
ior was known and incorporated into our HTML parser, this
kind of attack would go undetected. There does not seem
to be a general solution to this class of evasions, except to
move the policy enforcement to the browser [11].
Finally, false negatives may occur due to policy errors or
incompleteness. We remark that the simplicity and gener-
ality of the SpanNodes and StraddleTrees policies make it
difﬁcult to carry out evasion attacks that lead to false nega-
tives. However, with the XSS attack, there are many ways
to inject scripts into web pages. False negatives can occur
when the policy misses out some of these ways. This source
of false negatives is shared generally by policy-based de-
fenses.
Although the potential for false negatives exist, we re-
mark that many web applications only rely on simple input
transformation that are easily handled by our taint inference
approach. As a result, no false negatives were observed
in our experiments, in spite of the fact that the AMNESIA
dataset consists of many thousands of attacks.
7.3 False Positives
False Positives in Taint-Inference. False positives occur
when an input and output are within an edit-distance of d
without any actual information ﬂow taking place, i.e., as a
result of coincidence. To control them, we need to select
d such that the probability of coincidental matches is very
low, e.g., pick d such that P (T D(s, t)  6.)
False Positives in Attack Detection.
In order to have a
false positive in attack detection, note that we need to have
a false positive in taint inference, and in addition, a policy
violation needs to occur. Thus, false positives in attack de-
tection can be expected to be signiﬁcantly less than those of
taint inference.
In our experiments, we did not encounter any false pos-
itives. While the thoroughness of false positive testing on
applications such as phpBB could be questioned, the false
# of Response Overhead
Application
bookstore
empldir
portal
classiﬁeds
events
Total
requests
605
660
1080
576
900
3821
time
20.67
17.33
31.67
18.00
23.10
110.77
1.7%
3.4%
5.1%
4.3%
3.1%
3.5%
Figure 9. End-to-end Performance Overhead.
positive results on AMNESIA dataset is quite meaningful.
The creators of this dataset crafted a set of about 3.8K legit-
imate queries, many of which were intended to “look like”
attacks. Our technique did not ﬂag an attack on any of these.
7.4 Runtime Overhead
Performance-related experiments were carried out on a
laptop with dual-core 2GHz processor with 2GB memory
running Ubuntu (version 6.10) Linux. Our goal was to
measure the performance overhead introduced by benign re-
quests.
First, we investigated the performance gains obtained as
a result of using our coarse-ﬁltering algorithm described in
Section 4.2. These measurements were carried out across
the applications shown in Figure 7 across several runs that
contained a combination of attacks and benign requests. For
this comparison, the CPU time spent within the taint infer-