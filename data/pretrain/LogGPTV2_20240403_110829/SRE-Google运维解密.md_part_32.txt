你对这个流程非常熟悉，并且按照流程行事。
人全部加入进来。如果你的公司已经建立了一套灾难应急流程（参见第14章），请确保
如果你感到自己难以应对，就去找更多人参与进来。有的时候，甚至需要将整个公司的
止运转了（Google网络规模已经是全球排名前三）。所以请深吸一口气，慢慢来。
真实的物理危险，只是计算机系统出现了问题。最差的情况下，也只是半个互联网都停
你已经接受过如何正确处理这样的情况的训练。通常来说，我们处理的问题一般不涉及
首先，别惊慌失措！这不是世界末日，你也并不是一个人在战斗！作为一个专业人士，
当系统出现问题时怎么办
章为此提供了更多实际案例。
第15章讨论了如何通过书写事后总结，将紧急事件的处理过程变成一个学习机会，本
合理的资源有效地应对紧急情况，这些元素都是不可或缺的。
理层的支持，同时需要一批专注投入的人。要想创造一个人们可以依赖的环境，可以用
平时不断地进行实战训练。建立和维护一套完备的训练和演习流程需要公司董事会和管
人们如何应对。没有几个人天生就能很好地处理紧急情况。在紧急情况下恰当处理需要
不管一个组织有多大，做的事情有多么重要，它最明显的特质就是：在紧急事件来临时
东西早晚要坏的，这就是生活。
紧急事件响应
作者：CoreyAdamBaye
编辑：DianeBates
第13章
<152
151
---
## Page 174
153
132
的这次可控测试导致了事故，从而立即停止了该项测试。
受到这次事故影响的服务及时地在公司内部进行了沟通。我们有足够的信息假设是我们
做得好的地方
事后总结
数据库也不能被正常访问）。
库连接类库中没有正确处理某个数据库失去响应的情况，导致了进程阻塞或者其他所有
制定了一个周期性测试机制来保证这类严重问题不再重现。（此项事故应该是由于数据
这次测试带来的严重后果促使开发者对代码类库中的问题进行了一次彻底的修复，同时
在灾难发生的一小时内，所有访问权限都得到了恢复，同时所有的服务都恢复了正常
连接类库的开发者，修复了代码中的问题。
地在集群副本和灾备系统上恢复了访问权限。
广益找出了如何通过其他手段恢复访问权限。用一个以前已经测试过的方案，我们成功
恢复访问权限，但是意外地发现这项操作没有成功。SRE没有惊慌失措，而是立刻集思
SRE自然认为这些问题是测试导致的，所以立即终止了这项测试。我们试着通过回滚来
系统。这些关键系统业务时通时断，或者只是部分可用。
测试开始的仅仅几分钟后，大批服务汇报公司内部和外部用户都无法访问某个关键业务
响应
限制访问权限来屏蔽一百多个数据库中的一个。没有任何一个人能想到接下来发生的事。
我们想要找到依赖我们大型分布式MySQL集群中的一个测试数据库。测试计划是通过
细节
下面是一个具体的例子，在这次测试中，我们发现了一系列意外的系统依赖。
我们的假设和实际结果相差很远。
现过一些系统的弱点和潜在的依赖关系，制定了修改计划以解决这些问题。但是有时候，
按照计划进行，目标测试系统和依赖系统基本都能正常工作。我们利用这些测试曾经发
模拟事故，然后针对失败模式进行预防以提高可靠性。大多数时候，这些模拟事故都能
Google经常主动进行灾难处理和应急响应演习（参见文献[Kir12])。SRE故意破坏系统、
测试导致的紧急事故
第13章紧急事件响应
，与此同时，我们联系上了负责开发数据库
---
## Page 175
务也依赖于这些内部服务，所以导致很多内部应用程序也失去了响应。
Bug，导致这些对外服务都进入了崩溃循环（crash-loop）。因为Google内部很多基础服
器上。这个服务和Google所有对外服务都有直接联系。这个新版配置文件触发了一个
在某个星期五，一个防滥用基础服务（anti-abuse）的新版配置文件被推送到所有的服务
细节
的时候配置文件改变并不能完全按照计划进行。
的测试。但是由于Google基础设施的规模和复杂程度，不可能完全预料每种情况，有
下面就是一个例子。
行修改。为了避免配置文件导致意外事故的发生，我们在部署配置文件之前会运行大量
正如你想象的那样，Google有很多配置文件。
变更部署带来的紧急事故
故总时长被延长了。我们现在要求在大型测试中一定先测试回滚机制。
由于我们没有在测试环境中测试回滚机制，没有发现这些机制其实是无效的，导致了事
有人都十分了解该流程。
类似情况的发生，SRE将继续优化和测试应急响应流程工具，确保未来应急事故中的所
这项流程本来可以保证所有的相关服务和用户更好地了解整个事故的全过程。为了避免
我们没有正确地遵守应急响应流程，因为这个流程几个星期前刚刚建立，还没深入人心
我们对相关系统的了解还不够。
虽然这项测试在进行之前进行了充分评审，评审中认为这项测试是很合理的，事实证明
做得不好的地方
次发生。
而避免了类似的事故再次发生。同时我们制定了周期性的测试，保证类似的问题不会再
事故发生之后产生的待办事项（指修复代码工作）也很快得到了快速而彻底地解决，从
努力使整个服务更快地得到了恢复。
用了另外一种策略，重新配置了他们的服务，将测试数据库从配置中去掉。这些并行的
我们成功地在一个小时内（收到第一个事故报告起）恢复了服务。有些受影响的团队采
，这些文件非常复杂，同时我们在不停地进
变更部署带来的紧急事故
133
154
---
## Page 176
155
134
状态。
得某些任务可以在崩溃之前还可以处理一些服务器请求，从而避免了进入彻底不可用的
他们给新客户端分发配置变更的速度。这些限速措施可能抑制了崩溃循环的速度，使
Google的系统架构体系提供了另外一层保护，这次受影响的系统包含了限速机制，限制
重大作用，但是工程师应该更加频繁地测试，以便更为熟悉它们。
他条件无法访问的时候进行更新和变更回滚。这些工具和访问方式在这次事故中起到了
在这些带外通信系统之外，Google还有命令行工具和其他的访问方式确保我们能够在其
备份访问系统运行，
个人都能保持连接。
问题被检测到之后，应急响应流程处理得当，其他人得到了清晰和及时的事态更新。
的通信通道中充斥了大量垃圾信息。
存在问题：报警信息不停地重复报警，让on-call工程师难以应对，导致在正常的和紧急
首先，监控系统及时检测和汇报了问题。但是这里应该记录下来，
以下几个因素使得很多Google内部系统及时恢复了。
做得好的地方
事后总结
了由于最初的事故导致的另外的Bug或者错误的配置等影响，一个小时后才能恢复。
事故可能是由于他发布的配置文件造成的，而且目前已经回滚了。但是，有些服务遇到
开始通知公司内的其他部门目前的情况。负责发布配置的工程师告诉on-call工程师这次
在第一次发布的10分钟后，on-call工程师按照内部应急流程宣告进入紧急状态。他们
便将之前的问题回滚。此时，各项服务逐渐开始恢复。
但是还没意识到更大范围的生产系统问题。该工程师发布了一份修正过的配置文件，以
在配置文件发布的5分钟后，负责发布这个配置的工程师也发现公司内网出现了问题，
来到了这里。
房间中有Google生产环境的专线连接。他们意外地发现，越来越多的人因为网络问题
公司内网好像出现了问题，于是他们都前往一个专门的灾难安全屋（panicroom）。这个
几秒内，各种监控就开始报警，声称某些网站失去了响应。
事故响应
们的带外通信系统（out-of-band，指专线数据中心连接）保证了在复杂软件问题下，每
第13章紧急事件响应
，因为我们在这种情况下就会用到！
这次经历提醒了我们，SRE为什么要保持一些非常可靠的、低成本
一些on-call工程师同时发现
我们的监控系统还是
我
---
## Page 177
盘数据被清空。细节请参看第7章的“自动化：允许大规模故障发生”材料。
Bug将全球所有数据中心的所有机器加到了磁盘销毁（diskerase）的队列中，这导致硬
的下线请求（turndown）。在处理第二个下线请求时，自动化系统中的一个非常隐蔽的
在一项常规自动化测试中，该测试针对同一个集群（马上即将退役的）发送了两个连续
细节
运行、停止和重新配置大量任务。但是当意外来临时，自动化的效率有时可能是很可怕的。
流程导致的严重事故
Google 大量使用自研工具。我们大部分的在线调试与内部沟通工具都依赖这次进人崩溃
数分钟。这严重干扰了on-call工程师的工作，同时让在这次事故的参与者们沟通起来更
正如我们所料，监控系统在这次灾难中发出了很多报警，因为全球每个节点都失去响应
严格完整的部署测试。
次事故的发生直接将他们的优先级提高了，同时强调不管风险看起来有多小，也要经过
具有讽刺意味的是，我们本来计划在下个季度提高部署测试流程和自动化的优先级。这
新功能组合导致了灾难的发生。
个简单的部署试验过程。当这次变更在全球部署的时候，这个没有经过测试的关键词
键词。这次触发问题的变更，并没有被认为是非常危险的，导致这项改动仅仅经历了一
没有触发该Bug，因为在那次测试中配置文件中没有用到一个非常特别、很少用到的关
在之前的部署中，
我们从中学到的
同时变得更难解决。
迅速回退了该配置。
个工程师注意到在变更推送结束后，频道里出现了大量的公司网络访问故障报告，从而
的工程师恰好在监控某些实时通信频道，这并不是发布过程中要求的。正因为如此，这
循环的任务。如果这次的故障没能很快解决的话，我们的在线调试能力将受到极大的影响。
困难了。
最后，我们不应该小看运气这个因素。这次我们能够迅速解决问题是因为进行部署变更
下面这个例子讲述了有的时候动作太快不一定是好事。
导致问题的这项新功能经历了一个完整的部署试验（Canary）周期却
如果配置没有这么快地被回退，这项事故可能会持续相当长的时间，
流程导致的严重事故
135
156
---
## Page 178
157
136
改动快速恢复。这一点对他们评估事故严重程度提供了帮助。
虽然下线自动化迅速将对应的站点监控系统也干掉了，但是on-call工程师成功地将这些
间内大量的小型数据中心都被成功地下线以及进行安全的硬盘擦除。
小型数据中心的下线请求处理过程非常高效和完美。从开始到结束，
因此需要网络工程师部署一些临时措施。为了降低对最终用户的影响，on-call工程师将
按照设计，这些大型数据中心可以处理全部流量。但是一些网络连接出现了拥堵情况，
型数据中心。on-call工程师能够迅速地将用户流量从小型数据中心导向大型数据中心。
做得好的地方
事后总结
出现堵塞的网络节点设置为他们的最高优先级。
反向代理）的管理方式和小型数据中心的管理方式截然不同。所以这次事故没有影响大
在大型数据中心中，反向代理（前文没有提到受影响的服务是什么，这里提到了是一种
天内，大部分服务器都恢复了服务，其余没有恢复的都将在下个月或者下两个月内恢复，
重建操作的计划。整个团队分为三部分，每部分负责整个手工重建计划中的一步。在三
美国团队将工作交接给了他们的欧洲伙伴团队。SRE设计了一个利用手动步骤快速执行
受用户请求了。
重生。三个小时后，经过几个工程师的努力，这个数据中心被成功重建了，可以再次接
网络关键节点部署了一些缓解措施。在这些节点中的某个数据中心被选中为第一个浴火
现在最难的部分开始了：恢复。有些网络连接汇报堵塞得非常严重，所以网络工程师在