title:Model inversion attacks against collaborative inference
author:Zecheng He and
Tianwei Zhang and
Ruby B. Lee
Model Inversion Attacks Against
Collaborative Inference
Ruby B. Lee
PI:EMAIL
Princeton University
Zecheng He
PI:EMAIL
Princeton University
Tianwei Zhang
PI:EMAIL
Nanyang Technological University
ABSTRACT
The prevalence of deep learning has drawn attention to the
privacy protection of sensitive data. Various privacy threats
have been presented, where an adversary can steal model
owners’ private data. Meanwhile, countermeasures have also
been introduced to achieve privacy-preserving deep learning.
However, most studies only focused on data privacy during
training, and ignored privacy during inference.
In this paper, we devise a new set of attacks to compro-
mise the inference data privacy in collaborative deep learning
systems. Specifically, when a deep neural network and the
corresponding inference task are split and distributed to dif-
ferent participants, one malicious participant can accurately
recover an arbitrary input fed into this system, even if he has
no access to other participants’ data or computations, or to
prediction APIs to query this system. We evaluate our attacks
under different settings, models and datasets, to show their
effectiveness and generalization. We also study the charac-
teristics of deep learning models that make them susceptible
to such inference privacy threats. This provides insights and
guidelines to develop more privacy-preserving collaborative
systems and algorithms.
CCS CONCEPTS
• Security and privacy → Systems security; Distributed
systems security; • Computing methodologies → Arti-
ficial intelligence.
KEYWORDS
Deep Neural Network, Model Inversion Attack, Distributed
Computation
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-7628-0/19/12...$15.00
https://doi.org/10.1145/3359789.3359824
ACM Reference Format:
Zecheng He, Tianwei Zhang, and Ruby B. Lee. 2019. Model In-
version Attacks Against Collaborative Inference. In 2019 Annual
Computer Security Applications Conference (ACSAC ’19), December
9–13, 2019, San Juan, PR, USA. ACM, New York, NY, USA, 15 pages.
https://doi.org/10.1145/3359789.3359824
1 INTRODUCTION
Deep learning technology has developed rapidly, especially
Deep Neural Networks (DNNs). Deep learning models out-
perform traditional machine learning approaches on various
artificial intelligence tasks, e.g., image recognition [19], natu-
ral language processing [31], speech recognition [16], anom-
aly detection [20] etc. Such high and reliable performance
is attributed to the models’ complex structures (i.e., a large
number of hidden layers and parameters), time and resource
consuming training process, and a significant amount of data.
To accelerate the learning and prediction processes, as
well as reduce overheads, collaborative deep learning sys-
tems have been designed. Typically there are two collabo-
rative modes. The first is collaborative training [6, 7]. The
training task is distributed to multiple participants. Each par-
ticipant trains an individual model over his private dataset,
and periodically exchanges model updates. The final model is
aggregated from each individual model. Collaborative train-
ing can significantly improve the training speed.
The second mode is collaborative inference [8, 17, 25, 27,
43]. The basic idea is to split a deep neural network into
multiple parts, with each part allocated to a different par-
ticipant. An input sequentially goes through each part of
the neural network on these participants to generate the
final output. Collaborative inference has gained popularity
in the edge-cloud scenario. As edge devices, e.g. IoT devices
and smartphones, have limited computation and storage ca-
pacities, it is difficult for a single device to host an entire
model, and conduct the inference within reasonable latencies.
Instead, the neural network can be divided into two parts.
The first few layers of the network are stored in the local
edge device, while the rest are offloaded to a remote cloud
server. Given an input, the edge device calculates the output
Code available: https://github.com/zechenghe/Inverse_Collaborative_
Inference
of the first layers, sends it to the cloud, and retrieves the final
results. Collaboration between the edge device and cloud
server achieves higher inference speed and lower power con-
sumption than running the task solely on the local or remote
platform.
Privacy has become a big security concern in deep learn-
ing. Past work presented a variety of privacy threats against
training data, e.g., property inference attacks [3, 11], member-
ship inference attacks [18, 29, 30, 39, 41, 49], model inversion
attacks [9, 10]. These threats are also important in the con-
text of collaborative training. Since multiple participants
are involved in one task, but not all of them are necessar-
ily trusted, it is essential to prevent malicious participants
from stealing sensitive data. It has been shown that the pri-
vacy of training data is better protected if each participant
in the distributed system uses his own dataset and never
shares it with other participants [15, 40]. However, it is still
possible for an adversary participant to infer the sensitive
information and properties of other participants’ training
data indirectly via model updates. Distributed model inver-
sion attacks [22] and property inference attacks [32] were
designed and implemented.
In contrast, inference data privacy is less studied, either
in single-party or multi-party machine learning systems.
It is much more challenging to recover inference samples
than training samples due to the following reasons: (1) the
model parameters do not depend on the inference input. Thus
the inference process reveals less useful information about
inference samples to the adversary; (2) Training samples
usually follow certain distributions, enabling the adversary
to recover statistical information about such distributions.
Inference samples do not have this assumption, making it
hard to recover an individual sample. In single-party sys-
tems, Wei et al. [48] made an attempt towards inference data
recovery in DNN accelerators via power side channels. Their
attacks required the adversary to be able to hack into the
victim device and install trojans, and the input image needs
to be very simple, e.g. binary. These assumptions make the
attack less practical. To the best of our knowledge, there is no
work exploring the privacy issues in multi-party inference
systems.
This paper presents the first investigation of inference data
privacy in a collaborative ML system. Two key questions are
considered in this study. The first one is: if one intermediate
participant is compromised and controlled by an adversary, can
he recover an arbitrary input sample? To answer this question,
we design a set of novel attack techniques for different set-
tings. (1) In a white-box setting where the adversary knows
the target model on other participants, we propose regular-
ized Maximum Likelihood Estimation (rMLE), to recover the
input from the model parameters and intermediate output.
(2) In a black-box setting, the model parameters on other
participants are inaccessible to the adversary. We introduce
the Inverse-Network technique to identify the mapping from
the intermediate output to input. (3) We further consider the
query-free black-box setting, in which the adversary can-
not query the inference system. We design an approach to
reconstruct an alternative version of the target model and
then recover the input via rMLE. Our attack results indicate
that it is feasible for a compromised participant to accurately
recover the input data, even if he has no knowledge of the
target model, training data, or capability of querying the
system.
The second question is: Of the target system and model,
which characteristics make the inference process more vulnera-
ble to such privacy threats? Which can reduce privacy leakage?
To fully understand the impact of the model features and sys-
tem designs on the attacks, we conduct empirical evaluations
on different model partitioning strategies and adversary’s
capabilities. Through quantitative comparisons, we identify
the critical features and conditions that determine the suc-
cess of the model inversion attacks. We hope our findings
can guide machine learning researchers and practitioners to
design more secure collaborative inference systems.
The key contributions of this paper are:
• The first systematic study of inference data privacy in
collaborative machine learning systems.
• The regularized Maximum Likelihood Estimation tech-
nique to recover inference data under the white-box set-
ting;
• The Inverse-Network technique to recover inference data
under the black-box setting;
• The query-free shadow model reconstruction technique to
recover inference data under the query-free setting;
• Quantitative discussion about the impact of system and
model features on the attacks, and investigation of defense
strategies.
The rest of the paper is organized as follows: Section 2
gives the background of DNN and collaborative inference.
Section 3 presents the threat models in our consideration
and experimental configurations. Sections 4, 5 and 6 describe
attacks under white-box, black-box and query-free settings,
including attack approaches, implementations and evalua-
tion results. Section 7 compares different attack factors, and
discusses possible mitigation solutions. We give related work
in Section 8 and conclude in Section 9.
2 BACKGROUND
2.1 Deep Neural Networks
A DNN is a parameterized function fθ : X (cid:55)→ Y that maps an
input tensor x ∈ X to an output tensor y ∈ Y. Various neural
network architectures have been proposed, e.g., multilayer
value vi−1 from Pi−1, calculates vi = fθi (vi ), and passes it
to Pi +1. The final participant Pn generates the final output
y = fθn (vn−1). Figure 2 shows an example of a collaborative
inference system with two participants.
A use case. With the growing proliferation of Internet of
Things (IoT), we need ways to deploy deep learning inference
applications on commodity resource-constrained edge de-
vices [50]. Running the entire application on the edge device
has several challenges: the limited computation resources of
the device can cause significant latency; the limited storage
capacity makes it hard to store a large DNN model; the lim-
ited battery capacity causes a critical energy consumption
constraint. An alternative is to offload the entire DNN model
and inference computation to the cloud. The edge device
sends the input data to the cloud and receives the output.
While this can resolve the limitations of edge devices, it can
incur significant communication costs when sending a large
volume of input data. Besides, there can be privacy issues of
the inference data and integrity issues of the model [21], if
the cloud is not trusted.
An optimized strategy is to adopt collaborative inference
between the edge device and the cloud [8, 17, 25, 27, 43].
The first few simple layers of the DNN model are deployed
on the edge device, while the remaining complex layers are
offloaded to the cloud. This approach can reduce communi-
cation costs, as the intermediate output can be designed to
be much smaller than the raw input. Such low data transfer
bandwidth can also achieve lower latency. Collaborative in-
ference makes it feasible and efficient to deploy large-scale
intelligent workloads on today’s edge platforms.
Collaborative inference can also provide better privacy
protection, as the cloud now only receives the intermediate
values instead of the raw data [43]. The raw data can cause
significant privacy issues, e.g. biosensor readings, medical
diagonosis and examination data, and facial images. In this
paper, however, we show that information leakage is still
possible in collaborative inference. An untrusted cloud can
easily and accurately recover the sensitive data from the
intermediate values without accesses to the edge device. The
existing ML privacy protection mechanisms include leverg-
ing data obfuscation, Trusted Execution Environment (TEE),
homomorphic encryption and differential privacy. We dis-
cuss their feasibility and potential drawbacks in Section 7.4.
3 PRELIMINARIES
3.1 Threat Model
Without loss of generality, we consider a collaborative infer-
ence system between two participants, P1 and P2. The target
model is split into two parts: fθ = fθ2 · fθ1. P1 performs
the earlier layers fθ1, while P2 performs the later layers fθ2.
We consider P1 is trusted: when an input is fed into fθ1,
Figure 1: DNN architecture Figure 2: Collaborative in-
ference
perceptrons [36], convolutional neural networks [28] and
recurrent neural networks [38].
Figure 1 shows the structure of a DNN. It usually con-
sists of an input layer, an output layer and a sequence of
hidden layers between the input and output. Each layer is
a collection of units called neurons, which are connected to
other neurons in the previous layer and the next layer. Each
connection between the neurons can transmit a signal to an-
other neuron in the next layer. In this way, a neural network
transforms the inputs through hidden layers to the outputs,
by applying operations (e.g., linear function or element-wise
nonlinear activation function) in each layer.
Model training. The training process of a neural network
is to find the optimal parameters θ that can accurately reflect
the relationship between X and Y. To achieve this, a train-
ing dataset Dtr ain = {xtr ain
}N
i =1 with N samples is
∈ Y
needed, where xtr ain
is the corresponding ground-truth label. Then a loss function
L is applied to measure the distance between the ground-
truth output ytr ain
). The
goal of training a neural network is to minimize this loss
function (Eq. 1). Backward propagation [14] and stochastic
gradient descent [35] are commonly used methods to ap-
∗
proximately achieve this goal. The optimal parameters θ
together with the network topology form the deep learning
model.
∈ X is the feature data and ytr ain
and the predicted output fθ (xtr ain
, ytr ain
i
i
i
i
i
i
N(cid:88)
i =1
∗ = arg min
θ
(
θ
L(ytr ain
i
, fθ (xtr ain
i
))
(1)
Model inference. After the model training is completed,
given an input x, the corresponding output can be calculated
as y = fθ∗ (x ).
2.2 Collaborative Inference
In a collaborative inference system, a DNN is partitioned
into n parts: fθ = fθ1 · fθ2...fθn. Each part fθi contains
several layers, and is distributed to a participant Pi. Given
an input x, the first participant P1 generates v1 = fθ1(x ) and
sends it to P2. Each participant Pi receives the intermediate
xyh1hn…xyhjhj+1……vP1 correctly processes it and never leaks it to other parties.
However, P2 is untrusted, attempting to steal the input. This
assumption is reasonable in the edge-cloud scenario: the
model owner configures and operates the local device (P1)
to ensure the computation is trusted. But he does not have
control over the cloud server (P2), which may be untrusted.
Our threat model can also be applied to systems with more
than two participants. As most real systems are split into two
parties, without loss of generality, for the rest of this paper,
we use a two-participant system to describe and evaluate