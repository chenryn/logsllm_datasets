i=1
T
∑
t=0
Lppo + λi(t)Lad .
(16)
10
Update the value function by minimizing the following
objective function
vk+1
α = argminvα
1
|Dk|T
Dk
∑
i=0
T
∑
t=0
(Vαk (oi(t)
α )− (ri(t)
α + γVαk (oi(t+1)
α
11 end
12 Output: the well trained adversarial policy network πα.
)))2 .
(17)
6.1 Experiment setup
In our experiment, we choose the game “You Should Not
Pass” in the MuJoCo game zoo [50], which has recently been
adopted to demonstrate the effectiveness of a state-of-the-art
adversarial attack [10]. As we will specify in the consecu-
tive session, by using this game, we not only evaluate the
key components of our proposed design but, more impor-
tantly, compare the effectiveness of our proposed technique
with that of the state-of-the-art method [10]. In addition to
the MuJoCo game, we demonstrate our method on the ro-
boschool Pong game [33]. Together with the MuJoCo game,
we quantify by how much our proposed method outperforms
1892    30th USENIX Security Symposium
USENIX Association
the state-of-the-art technique [10]. We believe the games of
our choice are representative for the following three reasons.
First, both games provide us with the interface to train agents
using reinforcement learning algorithms, giving us the free-
dom to develop our attack method. Second, as is discussed in
Section 2, our attack targets competitive games in which re-
inforcement learning algorithms are commonly used to train
agents. Both games of our choice are commonly used in
academia for evaluating reinforcement learning algorithms
in two-agent settings (e.g., [3]) and attack methods in ad-
versarial learning (e.g., [10]). Third, we design our attack
based on the PPO algorithm. When we choose games, we
need to ensure, the PPO algorithm should be the one most
commonly used for the games of our choice. Both MuJoCo
and Roboschool hold this selection criterion. In the following,
we brieﬂy introduce both of these games, the opponent agents
in both games, and the evaluation metric.
MuJoCo. In this game, two agents (i.e., players) are ﬁrst
initialized to face each other. As is illustrated in Figure 5a,
the blue humanoid robot then starts to run towards the ﬁnish
line (indicated by the red line in Figure 5a). In this process,
the red humanoid robot in the ﬁgure attempts to block the blue
robot from reaching the line right behind it. By design, the
blue robot could win the game only if it reaches the ﬁnish line.
Otherwise, the other robot wins. When playing this game,
both robots observe the game environment, the current status
of themselves (e.g., the position and velocity of their body),
and that of their opponent. Based on the observation, they
both utilize a policy network to decide their actions (i.e., the
direction and velocity of the next movement). The game ends
when the winning condition is triggered. At that time, the
winner receives a reward, whereas the loser gets penalized.
Roboschool Pong. As is depicted in Figure 5b, the Pong
game features two paddles and a ball. The reinforcement
learning agents control the movement of the paddles through
policy networks. At the beginning of the game, one agent
serves the ball, and the other returns the serve.
In each
round of the game, an agent can claim a win only if its oppo-
nent fails to return the ball or violates the rule of the game
(e.g., successively hit the ball twice). If a single round of the
game runs out of time, a timeout will be triggered and the
game will conclude a tie. In this game, the observation of an
agent contains the agent itself, the opponent agent, and the
position and velocity of the ball. Based on the observation,
through its policy network, the agent can take an action in-
dicated by the direction and velocity of its next movement.
When playing this game, agents will receive a reward or be
penalized based on the performance of the agent.
Opponent agents. Following the work proposed in [10], re-
garding the MuJoCo game, we treat the blue humanoid robot
as the opponent agent and the red one as our adversarial agent.
For the Pong game, we take the purple paddle (on the right
of Figure 5b) as the opponent agent whereas the other as
(a) MuJoCo.
(b) Roboschool Pong.
Figure 5: The illustration of the selected games.
the adversarial one.7 In this work, the policy networks of
opponent agents are all modeled as multilayer perceptrons,
which are trained through a self-play mechanism [3] because
this neural architecture has been broadly used by previous
research [3, 10, 33] and already demonstrated the best perfor-
mance in both MujoCo and Pong game. To be more speciﬁc,
for the MuJoCo game, we used the pre-trained policy net-
work released in the “agent zoo” [3] as the opponent policy
network. For the roboschool Pong game, we ﬁrst trained a
policy network through the self-play mechanism by using the
PPO algorithm. Then we treated it as the opponent policy
network. We specify the architectures of these two opponent
policy networks in the Appendix.
Evaluation metric. Different from supervised learning algo-
rithms, many reinforcement learning algorithms typically do
not involve a data set collected ofﬂine for training an agent.
Instead, they usually expose a learning agent to interact with
the environment for many iterations. In each iteration, the
learning agent collects trajectories by using its policy net-
work learned from the last iteration, update its current policy
network with the new trajectories, and proceed to the next
iteration. In our experiment, we follow the metric commonly
used for evaluating reinforcement learning, measuring the
winning rate of the adversarial agent at each iteration. Given
the property of the competitive game, by subtracting the win-
ning rate of the adversarial agent, we can easily obtain that of
the opponent. The higher the winning rate for an adversarial
agent is, the more powerful the adversarial agent is in terms
of exploiting the weakness of its opponent.
6.2 Experiment design
We design our experiment from two different perspectives.
One is to evaluate some components of our proposed tech-
nique, and the other is to quantify the overall performance of
our proposed method. In the following, we describe the detail
of each of our experiment designs.
Experiment I. Recall that we utilize gradient-based explain-
able AI techniques to guide the selection of the hyperparam-
eter λ. To understand the contribution of the explanation
7Note that the two agents are symmetric; therefore, the choice of the
opponent agent does not inﬂuence the effectiveness of the learning algorithm.
USENIX Association
30th USENIX Security Symposium    1893
component in our loss function, we ﬁrst design an experi-
ment, in which we set up the hyperparameter λ with different
constant values, run our learning algorithm under this set-
ting on the MuJoCo game, and compare the performance of
the trained agent under each constant value with the one ob-
tained through our explanation-based method. With respect to
the explanation-based method, we choose different gradient-
based explainable AI techniques to serve as the explanation
component. In this experiment, we compare the correspond-
ing performance of the adversarial agent under each of our
choices. More speciﬁcally, the gradient-based explainable AI
methods in our choice set include vanilla gradient [46], inte-
grated gradient [48], and smooth gradient [47]. In addition to
these well-recognized gradient-based methods, our choice set
encloses a random explanation approach as a baseline method,
which derives feature importance score randomly.
Experiment II. We also design an experiment to validate
the choice of our distance measure. As is mentioned in Sec-
tion 5, we carefully design the measure of distance indicated
by Equation (11), (12), and (13). To ensure our choice of
the distance measure could truly beneﬁt the agent trained by
our proposed method, we replace the corresponding distance
measures with the l1 and l2 norm respectively. In this work,
we compare the performance of the trained agent under each
of these setups.
Experiment III. We further design an experiment to examine
whether the approximated opponent policy network involved
in our technique imposes any risk of downgrading our agent’s
performance. As is mentioned in Section 4, to derive an
explanation and thus guide the adjustment of the hyperparam-
eter λ, we approximate the policy network of the opponent.
Since this approximation is based on point estimation, this
inevitably incurs errors and thus potentially inﬂuences the per-
formance of the adversarial agent trained by our method. To
test its impact upon the adversarial agent’s performance, we
replace the approximated policy network with the actual pol-
icy network of the opponent agent, run the proposed learning
algorithm, and compare the performance of the corresponding
agent with the one obtained through our method.
Experiment IV. Using the state-of-the-art attack method [10]
as our baseline, we also design an experiment to evaluate our
proposed method. To be speciﬁc, we use both methods to
train adversarial agents and then apply them in the MuJoCo
game and the Pong game. In each of the games, we then
compare the winning rate of the adversarial agents across the
number of iterations involved in the training process. This is
similar to the setup proposed in an early research [10].
Experiment V. Finally, we investigate a simple adversarial
training approach to safeguard victim models against the pro-
posed attack. More speciﬁcally, We play the victim agent
with the adversarial agent trained by our attack in the corre-
sponding game environment and collected the game episodes.
With these episodes, we then utilized our proposed learning
algorithm (Algorithm 1) to retrain the victim agent. Similar
to the experiment above, we compare the winning rate of the
retrained victim agent against the adversarial agent across the
number of iterations involved in the retraining process. In
addition, we employ the retrained victim agent to play with
an agent trained with self-play methods. With this setup, we
emulate a scenario where a robustiﬁed agent plays with a
regular (non-robustiﬁed) game agent. Through this, we study
if retrained victim agent could pick up the generalizability in
competitive game. In other words, we study whether a victim
agent still performs well when playing with a regular agent
even after we retrain it with adversarial training.
Additional experiment notes. It should be noted that, when
running any learning algorithms to train adversarial agents
and perform the aforementioned experiments, we go beyond
the suggestion mentioned in [10], increasing the number of
different initial states from 5 to 8 for each agent training. With
this setup, we can not only obtain the average performance of
each learning algorithm but also further reduce the inﬂuence
of randomness. It should also be noted that, when training an
agent, we cut off our training process after the training reaches
20 million iterations for the MuJoCo game and 4 million it-
erations for the Pong game. This is because our empirical
evidence indicates that, after these numbers of iterations, the
performance of the adversarial agent (the winning rate) con-
verges. Our method involves multiple hyper-parameters. In
our experiment, we conduct the sensitivity test for the main
hyper-parameters: the explanation method, λ, the distance
measure, η (Appendix). We ﬁnd that our attack is robust
to all these hyper-parameters except the distance measure.
We present our choice of distance measure in Section 5 and
validate our choice in Experiment II. Regarding the hyper-
parameters inherited from our baseline [10], we apply the
default choices in [10] for a fair comparison. In the Appendix,
we specify the choices of the other hyper-parameters that are
not varied in the sensitivity test and how we decide them. For
the video demonstration of our adversarial agents, readers
could ﬁnd them at https://tinyurl.com/vsnp5jr.
6.3 Experiment result
Here, we present the experiment results and analyze the rea-
sons behind our ﬁndings.
Comparison of hyperparameter selection strategies. Fig-
ure 6a shows the performance of the adversarial agent trained
with different hyperparameter selection strategies. As we can
observe from the ﬁgure, when the hyperparameter λ is set
up with a constant (i.e., red, green, and yellow lines in Fig-
ure 6a), the winning rate of the adversarial agent converges
at about 50% on average, which is comparable to the perfor-
mance of the adversarial agent trained by the state-of-the-art
method [10] (indicated by baseline in Figure 6a). However,
when using an explainable AI technique to adjust this hy-
perparameter over time, we can easily observe about 10%
improvement in the winning rate (about 60% vs. 50%). This
1894    30th USENIX Security Symposium
USENIX Association
(a) Constant λ.
(b) Explainable AI techniques.
(c) Random λ.
(d) Distance measures.
Figure 6: The winning rates of our adversarial agent trained with different hyperparameter selection strategies and distance
measures. The darker solid lines in the ﬁgures are the average winning rate of the corresponding agent. The lighter shadow
represent the variation between the maximal and minimal winning rates.
aligns with the rationale behind our design. That is, the distri-
bution of the trajectory used for agent training is very unstable,
and it is generally difﬁcult to ﬁnd a constant value suitable
for all possible distributions.
As is shown in Figure 6b, we also discover that, although
the explanation methods in our choice set provide different
ﬁdelity [1], integrated as a component of our attack, they do
not deviate the effectiveness of the attack. The adversarial
agent with each of the three explanation methods demon-
strates about 60% of a winning rate. This indicates the choice
of explanation methods has nearly no inﬂuence upon the per-
formance of our attack. In addition, we observe that, using a
randomly generated explanation to adjust hyperparameter λ,
the adversarial agent has only about 40% winning rate (see
Figure 6c). From a different angle, this implies the importance
of the explanation AI techniques upon our attack.
Comparison of distance measures. Figure 6d shows the per-
formance comparison of the adversarial agents trained under
different distance measures. As we can observe from the
ﬁgure, the adversarial agent trained under the l2 norm demon-
strates the worst winning rate, which is even lower than that
observed from the baseline method. The reason behind this
observation is as follows. The observations and actions in
the MuJoCo game are of high dimensionality. When mini-
mization or maximization problems involve high dimensional
input, the l2 norm is typically not able to impose a strong
penalty, and thus the model trained on such a distance mea-
sure usually exhibits poor performance.
From Figure 6d, we can also observe that the proposed
method under the setup of the l1 norm demonstrates better
performance than the baseline approach as well as that under
the l2 norm. However, it is still slightly below the performance
observed from our carefully selected distance measure. While
this observation could be used as an argument to support the
selection of our distance measure, we do not claim the l∞
norm cannot be replaced with the l1 norm, but argue that they