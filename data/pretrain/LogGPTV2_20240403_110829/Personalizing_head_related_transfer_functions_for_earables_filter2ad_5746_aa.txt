title:Personalizing head related transfer functions for earables
author:Zhijian Yang and
Romit Roy Choudhury
Personalizing Head Related Transfer Functions for Earables
Zhijian Yang
University of Illinois at Urbana Champaign
Romit Roy Choudhury
University of Illinois at Urbana Champaign
ABSTRACT
Head related transfer functions (HRTF) describe how sound signals
bounce, scatter, and diffract when they arrive at the head, and travel
towards the ear canals. HRTFs produce distinct sound patterns that
ultimately help the brain infer the spatial properties of the sound,
such as its direction of arrival, 𝜃. If an earphone can learn the HRTF,
it could apply the HRTF to any sound and make that sound appear
directional to the user. For instance, a directional voice guide could
help a tourist navigate a new city.
While past works have estimated human HRTFs, an important
gap lies in personalization. Today’s HRTFs are global templates
that are used in all products; since human HRTFs are unique, a
global HRTF only offers a coarse-grained experience. This paper
shows that by moving a smartphone around the head, combined
with mobile acoustic communications between the phone and the
earbuds, it is possible to estimate a user’s personal HRTF. Our
personalization system, UNIQ, combines techniques from channel
estimation, motion tracking, and signal processing, with a focus on
modeling signal diffraction on the curvature of the face. The results
are promising and could open new doors into the rapidly growing
space of immersive AR/VR, earables, smart hearing aids, etc.
CCS CONCEPTS
• Computer systems organization → Embedded and cyber-
physical systems; • Human-centered computing → Ubiqui-
tous and mobile computing; Interaction techniques.
KEYWORDS
Head Related Transfer Function (HRTF), Spatial Audio, Virtual
Acoustics, HRTF Personalization, Earables, AR, VR
ACM Reference Format:
Zhijian Yang and Romit Roy Choudhury. 2021. Personalizing Head Related
Transfer Functions for Earables. In ACM SIGCOMM 2021 Conference (SIG-
COMM ’21), August 23–27, 2021, Virtual Event, USA. ACM, New York, NY,
USA, 14 pages. https://doi.org/10.1145/3452296.3472907
1 INTRODUCTION
Humans can inherently sense the direction 𝜃 from which a sound
arrives at their ears. The human brain essentially analyzes the time
difference of the sounds across the two ears and maps this difference
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8383-7/21/08...$15.00
https://doi.org/10.1145/3452296.3472907
Δ𝑡 to 𝜃. If the mapping was one-to-one, then the estimation of 𝜃
would be easy. Unfortunately, the mapping is one-to-many, meaning
that for a given Δ𝑡, there are many possible 𝜃s. Figure 1(a) shows an
example where all points on the (red) hyperbola produce identical
Δ𝑡 at the ears. How can humans still disambiguate the direction
𝜃? The answer lies in what is classically known as the head related
transfer function (HRTF), explained next.
Figure 1: Humans identify sound direction through (a) time differ-
ence of arrival, and (b) pinna multipath.
Briefly, the sounds that actually enter the ear-canal is influenced
by the shape of the human head and the pinna of the ear (shown
in Figure 1(b)). The pinna produces micro-echoes to the arriving
signal, while the 3D curvature of the head bends (or diffracts) the
signals [16, 22, 29]. The net result is that the eardrum receives a
sophisticated signal pattern that helps the brain disambiguate 𝜃. In
summary, one can view the head (including the pinna) as a filter that
alters the signal depending on its angle of arrival 𝜃. In frequency
domain, this filter is called head related transfer function (HRTF).
Knowing HRTF for each 𝜃 opens new possibilities in spatial acous-
tics. An earphone could take any normal sound 𝑠(𝑡), apply the
(left and right) HRTFs for a desired 𝜃, and play the two sounds in
the corresponding earbuds [25, 59]. The brain would perceive this
sound as directional, as if it is arriving from an angle 𝜃 with respect
to the head. Applications could be many, ranging from immersive
AR/VR, to gaming, to assisted technology for blind individuals [52].
For instance, (1) users may no longer need to look at maps to
navigate from point A to point B; a voice could say “follow me”
in the ears, and walking towards the perceived direction of the
voice could bring the user to her destination. Blind people may
particularly benefit from such a capability. (2) A virtual-reality
meeting could be held through immersive acoustic experiences.
Members could pick their seats in a virtual meeting room and
each member could hear the others from the direction of their
relative configuration. (3) Gaming and other 3D applications would
naturally benefit. Each musical instrument in an AR/VR orchestra
could be fixed to a specific location around the head. Even if the
137
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Zhijian Yang and Romit Roy Choudhury
head rotates, motion sensors in the earphones can sense the rotation
and apply the HRTF for the updated 𝜃. Thus, the piano and the
violin can remain fixed in their absolute directions, offering an
immersive user experience.
HRTF-guided spatial sounds are already available in products today
[3, 6–8], however, important challenges remain open. One key
challenge is in HRTF personalization [29, 58]. Today’s products
use a global HRTF template, i.e., the HRTF is carefully measured
for one (or few people) in the lab and this “average” template is
then incorporated across all products. Unsurprisingly, the spatial
acoustic experience is known to be sub-optimal [27] and varies
widely across individuals [5, 10]. The natural question is: why not
estimate personalized HRTFs for each user?
To answer this, let us briefly understand today’s method of estimat-
ing HRTF [22, 55]. A user, Bob, is brought to an acoustic echo-free
chamber, seated at a special immovable chair, and fitted with a
normal earphone. A high quality speaker then plays carefully de-
signed sounds (e.g., a frequency sweep) from all possible angles 𝜃
and distances 𝑟 around Bob’s head. The ground truth for 𝜃 and 𝑟 are
accurately measured from ceiling cameras installed in the chamber.
Finally, the recordings from the left and right ears are converted
to the HRTFs for the corresponding ⟨𝜃, 𝑟⟩ tuple. Estimating per-
sonalized HRTF at home would entail hundreds of accurate ⟨𝜃, 𝑟⟩
measurements, while maintaining the exact head position. This is
impractical even for the technology savvy individual.
This paper aims to estimate a user’s personal HRTF at home by
leveraging smartphones, arm gestures, and acoustic signal process-
ing. The high level idea of our system, UNIQ, is simple. We ask a
user to sit on a chair, wear her earphones, and then move her smart-
phone in front of her face (as much as their arms would allow). The
smartphone plays pre-designed sounds that the earphones record;
the smartphone also logs its own IMU measurements during the
arm-motion. UNIQ’s algorithmic goal is to accept these 3 inputs
— the earphone recordings, the IMU recordings, and the played
sounds — and output the user’s personal HRTF, 𝐻(𝜃,𝑟).
In estimating the personal HRTF, we face 2 key challenges: (1) The
phone’s location needs to be tracked with high accuracy as the
phone is moving around the head. The IMU is inadequate for such
fine-grained tracking, hence the acoustic communication between
the smartphone and the earphone needs to aid the tracking algo-
rithm. Unfortunately, since the acoustic signal propagation between
the phone and earphone undergoes head-related diffraction and
pinna-multipath, standard geometric models do not apply. This
leads to a joint optimization problem, i.e., to solve for the phone’s
location, HRTF needs to be solved, and the vice versa.
(2) The above module solves the near-field HRTF. 1 However, the
near-field HRTF is not ideal when the emulated sound source needs
to be far away. Briefly, far-field sounds are almost parallel rays
when they arrive at the two ears, which is not the case for the near-
field. Since the HRTF varies as a function of the signal’s incoming
directions, the difference between near and far-field matters. Thus,
1Normally, when the sound source is less than 1𝑚 from the head, it is considered to
be in the "near-field". [4]
the second challenge is to “synthesize” or “extrapolate” the far field
HRTF based on the sequence of measurements from the near field.
UNIQ addresses these two main challenges by first modeling the
3D head-geometry using 3 parameters, applying diffraction on the
parametric model, and deriving the expected signal equations at
the ear. This expectation can now be compared against the acous-
tic measurements from the phone, along with the IMU readings
that (partly) track the phone’s motion. Together, UNIQ formulates
a minimization problem, extracting the head parameters and the
phone locations that best fit the model. With some additional re-
finements (such as discrete-to-continuous interpolation [40]), the
near-field HRTF is ready. UNIQ then selects suitable components
from the near-field HRTF to synthesize a physics-based model of
far-field signals. This model is fine-tuned with the estimated head
parameters to ultimately yield the far-field HRTF.
Finally, UNIQ shows an application of the far-field HRTF in estimat-
ing the angle of arrival (AoA) of ambient signals. This means when
Alice is wearing her earphones, and someone calls her name, the
earphones estimate the direction from which the voice signal ar-
rived. Classical beamforming/AoA algorithms do not apply directly
since the earphone microphones are now subject to diffraction and
pinna multipath. UNIQ develops an HRTF-aware AoA estimation
technique to enable these application-specific capabilities.
We implement UNIQ on off-the-shelf earphones and smartphones,
and evaluate with 5 volunteers. Our success metric is two-fold:
(1) We compare UNIQ’s personalized HRTF with the upper bound,
which is the ground-truth HRTF accurately measured for each vol-
unteer in our lab. (2) We also compare against the global or general
HRTF available online; this is the lower bound for personalization.
Results show that our personalized HRTF is, on average, 1.75𝑋
more similar to the ground-truth HRTF than the global HRTF. The
personalization extends improvements to all users, and is robust
to various kinds of sounds such as music and speech. In the AoA
application, we observe more than 20◦ average improvement when
using the personalized HRTF over the global one. We believe our
current method is a step forward in this long-standing problem
of HRTF personalization [27, 58], made possible by the fusion of
motion sensing and acoustics. Refinements are still possible as we
describe in Section 7, however, in the context of this paper, the
main contributions may be summarized as follows:
1. To the best of our knowledge, this is among the earliest attempts
to bring (motion + acoustic) sensor fusion to HRTF personalization.
We map the personalization problem to one in multi-modal local-
ization and synthesis, and show that IoT-style architectures can
usher new approaches.
2. We model signal diffraction on the human head, solve for head
parameters, and utilize it as a critical component in estimating
the personal HRTF. We develop a functional prototype that is
convenient, practical, and relevant to emerging ideas in immersive
AR/VR applications.
The rest of this paper will expand on each of these contributions,
starting from groundwork and measurement, followed by system
design, and evaluation.
138
Personalizing Head Related Transfer Functions for Earables
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
2 GROUNDWORK ON HRTF
This section sheds light on the 2 fundamental constructs of HRTFs:
(1) the acoustic channel introduced by a user’s pinna, and (2) diffrac-
tion caused by curvature of faces/heads. This should also help char-
acterize the gap between the global and personal HRTF.
■ Does the pinna’s effect vary with angle of arrival, 𝜃? Recall
that when a sound signal impinges on the pinna, it bounces and
scatters in complex ways, reaching the ear-drum at staggered time
instants. To test if this effect is sensitive to the angle of arrival
𝜃, we ask a user, Alice, to wear an in-ear microphone on her left
ear. We play short chirps from a speaker on the left side of Alice,
so that the head’s effects do not interfere with the microphone
recording (we intend to only measure the impact of the pinna). The
speaker is moved in a semi-circle, starting from the front of the
nose (𝜃 = 0◦) and ending at the back of the head (𝜃 = 180◦), with
measurements every 10◦. With 18 audio measurements, denoted
𝐴(𝜃), we now compute the cross-correlation 𝑐 between 𝐴(𝜃𝑖) and
𝐴(𝜃 𝑗), 𝑖, 𝑗 = {1, 2, . . . , 18} as
𝑐 = 𝑚𝑎𝑥(𝑓 (𝜏)) = 𝑚𝑎𝑥(
𝐴(𝜃𝑖)(𝑡) · 𝐴(𝜃 𝑗)(𝑡 + 𝜏))
where 𝜏 is the relative delay between 2 audio signals.
Figure 2(a) shows the results. Evidently, the correlation matrix is
strongly diagonal, implying that the pinna’s impulse response is
quite sensitive to 𝜃, with almost a 1:1 mapping. This is consistent
across our 5 volunteers, suggesting that the pinna indeed plays an
important role in the human’s ability to perceive directional sounds
(at a resolution of ≈ 20◦).
■ Does the pinna’s effect vary across users? The natural next
question is whether the pinna’s response varies across users for the
same 𝜃. For this, we cross-correlate the audio measurements from
2 users, 𝐴𝐴𝑙𝑖𝑐𝑒(𝜃𝑖) and 𝐴𝐵𝑜𝑏(𝜃𝑖), ∀𝑖. Figure 2(b) shows the results.
Clearly, Alice and Bob’s pinnas do not match well, for example,
Alice’s recording (angle 1) at angle 80◦ corresponds well with Bob’s
recording (angle 2) at angle 140◦. This means, when global HRTFs
are used in ear-devices, the resolution for directional sounds can be
no higher than ≈ 60◦, suggesting that the gap between global and
personal is not negligible. Thus global HRTF obviously degrades
user experience.
∞
𝑡=−∞
.
Figure 2: Pinna’s effect: (a) Diagonal confusion matrix for the same
user, across different angle of arrival, 𝜃. (b) For different people,
their pinna’s transfer functions are markedly different.
■ Do signals diffract on a person’s face/head? Is diffraction
distinct across users?