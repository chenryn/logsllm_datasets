### Crawler Activity and Data Transfer

In terms of requests sent, we estimate that, from our vantage point, crawling accounts for approximately 155 million requests per hour. The most active crawlers issue up to 910,000 requests per hour. As expected, the number of bytes received is larger than the number sent. Overall, we estimate that all crawlers together fetch roughly 3.8 TB of data per hour. However, not all crawlers are equally active; some fetch content from only a single IP address.

### Content Delivery Proxies (CDPs)

On average, content delivery proxies exhibit the lowest activity per individual IP address, both in terms of bytes and traffic. Despite this, their large numbers contribute significantly to back-office traffic. This category of IPs shows the greatest variation in behavior, with some heavy hitters competing with those in other categories.

### Summary

Real-time bidding (RTB) is highly prominent and relies on many small transactions involving a relatively small set of organizations and hosts. Each end-user request can trigger multiple bid requests, contributing significantly to the number of back-office transactions. In contrast, crawling occurs on a coarser-grain time scale and is executed by a limited number of organizations that constantly fetch content from a diverse set of mainly web hosting providers. While CDPs have a diverse profile, our analysis shows that a single end-user request to a CDN front-end server can involve a chain of proxies, which remains entirely hidden to the end users.

### A CDN’s Perspective

#### Back-Office Web Traffic Analysis

We have analyzed back-office web traffic from our vantage points in ISPs and IXPs. In this section, we present a complementary perspective provided by vantage points inside a commercial CDN. A CDN can be viewed as a high-bandwidth, low-latency conduit that facilitates data exchanges between end users and different points of origin. As seen in previous sections, CDNs are one of the major contributors to back-office web traffic. This section delves into the details of the data set provided by a large commercial CDN and highlights how this data can be used to characterize back-office web traffic.

#### CDN Dataset

The analysis in this section is based on server logs from the CDN’s edge, or front-end, servers. Each log line records the details of a data exchange where the edge server is one of the endpoints. Thus, the logs capture interactions between the edge server and end users (front-office web traffic) as well as interactions with other CDN servers and origin servers (back-office web traffic).

We obtained server logs from all servers at one cluster in each of five different cities: Chicago, Frankfurt, London, Munich, and Paris. Note that there may be multiple clusters at each city, and we selected only one of the larger clusters in each city. CDNs deploy multiple servers at each cluster for load balancing, and servers at each cluster offer a diverse set of services ranging from website delivery to e-commerce to video streaming. We selected clusters configured to handle web traffic, and our logs measure web traffic exceeding 350 TB in volume.

#### Front-Office vs. Back-Office CDN Traffic

The primary focus of a CDN is to serve content to the user as efficiently as possible. Therefore, CDN front-office traffic should dominate back-office traffic in volume. However, not all content is cacheable, up-to-date, or popular, so some content must be fetched from other servers. Many CDNs, such as Akamai, create and maintain sophisticated overlays to interconnect their edge and origin servers, improving end-to-end performance, bypassing network bottlenecks, and increasing tolerance to network or path failures. Hence, a CDN edge server may contact other CDN servers located in the same cluster (routed over a private network) or in different clusters (routed over a private or public network).

With knowledge of the IP addresses used by the CDN’s infrastructure, we can differentiate intra-CDN web traffic from traffic between CDN servers and end users (CDN-EndUsers), and between CDN servers and origin servers (CDN-Origin). Within the class of intra-CDN web traffic, we can further differentiate traffic between servers in the same cluster from that between servers in different clusters. This traffic does not qualify as back-office web traffic routed over the public Internet, which is the main focus of this paper. Our classification scheme partitions the web traffic identified via the logs into four categories: (1) CDN-EndUsers, (2) Intra-CDN/Public, (3) Intra-CDN/Private, and (4) CDN-Origin.

Figure 8(a) shows the proportion of traffic observed in each of the four categories at the five different locations (or clusters). As expected, most traffic is CDN-EndUsers traffic. We still observe at least 25% back-office traffic at each location. Paris is the most efficient from the perspective of the content provider, with more than 70% of the traffic in the CDN-EndUsers category and very low CDN-Origin traffic (around 1%).

Frankfurt is an outlier. At Frankfurt, end-user traffic accounts for less than 12%. After discussions with the CDN operator, we learned that servers in the Frankfurt cluster cache content from origin servers for other edge servers in nearby clusters. The high volume of Intra-CDN/Public traffic (about 55%) is indicative of this role. Besides reducing latency, this practice limits the number of servers fetching content from origin servers. Other locations show significant volumes in both the Intra-CDN/Public and Intra-CDN/Private categories, indicating reliance on cooperative caching within the CDN.

#### Characteristics of CDN Back-Office Traffic

We observed that the fan out, i.e., the number of hosts contacted by a host, can vary significantly. The number of unique end-user addresses to which edge servers deliver content (fan in) is larger than the combined number of CDN and origin servers from which they fetch content (fan out).

Figure 8(b) shows the number of unique connections observed in the different traffic categories at each location. The number of unique connections in the back-office traffic categories (Intra-CDN/Private, Intra-CDN/Public, and CDN-Origin) is two orders of magnitude less than that in the CDN-EndUsers category. The Intra-CDN/Private category mainly captures intra-cluster data exchanges, thus having even smaller fan out. Although the number of unique connections in the CDN-Origin category is smaller, it is equivalent in order of magnitude to the connection count in the Intra-CDN/Public category.

Aggregating the traffic volume by server addresses in both the CDN-Origin and Intra-CDN/Public categories reveals that the traffic is not uniformly distributed across all servers. There are heavy hitters: 20% of the origin servers contribute to more than 96% of the content delivered to the CDN’s edge servers. A similar trend is observed in the Intra-CDN/Public category, where 20% of the CDN’s servers account for over 94% of the traffic volume moved from different servers in the CDN’s infrastructure to the front-end, or edge, servers. These figures highlight the impact of varying popularity and cacheability of content on the traffic patterns within the CDN infrastructure.

### An End-User’s Perspective

Improving the end-user experience can lead to a significant increase in revenues and drive up user engagement. These benefits have catalyzed competition among service companies to offer faster access to content. Two straightforward ways of improving the end-user experience that can be implemented by ISPs are to (a) upgrade access networks and (b) improve the Internet’s middle mile (backbones, peering points, and/or transit points). Both approaches, however, are expensive.