#### 针对频繁故障设计
当比较 MapReduce 和 MPP 资料库时，两种不同的设计思路出现了：处理故障和使用记忆体与磁碟的方式。与线上系统相比，批处理对故障不太敏感，因为就算失败也不会立即影响到使用者，而且它们总是能再次执行。
如果一个节点在执行查询时崩溃，大多数 MPP 资料库会中止整个查询，并让使用者重新提交查询或自动重新执行它【3】。由于查询通常最多执行几秒钟或几分钟，所以这种错误处理的方法是可以接受的，因为重试的代价不是太大。MPP 资料库还倾向于在记忆体中保留尽可能多的资料（例如，使用杂凑连线）以避免从磁碟读取的开销。
另一方面，MapReduce 可以容忍单个 Map 或 Reduce 任务的失败，而不会影响作业的整体，透过以单个任务的粒度重试工作。它也会非常急切地将资料写入磁碟，一方面是为了容错，另一部分是因为假设资料集太大而不能适应记忆体。
MapReduce 方式更适用于较大的作业：要处理如此之多的资料并执行很长时间的作业，以至于在此过程中很可能至少遇到一个任务故障。在这种情况下，由于单个任务失败而重新执行整个作业将是非常浪费的。即使以单个任务的粒度进行恢复引入了使得无故障处理更慢的开销，但如果任务失败率足够高，这仍然是一种合理的权衡。
但是这些假设有多么现实呢？在大多数丛集中，机器故障确实会发生，但是它们不是很频繁 —— 可能少到绝大多数作业都不会经历机器故障。为了容错，真的值得带来这么大的额外开销吗？
要了解 MapReduce 节约使用记忆体和在任务的层次进行恢复的原因，了解最初设计 MapReduce 的环境是很有帮助的。Google 有著混用的资料中心，线上生产服务和离线批处理作业在同样机器上执行。每个任务都有一个透过容器强制执行的资源配给（CPU 核心、RAM、磁碟空间等）。每个任务也具有优先顺序，如果优先顺序较高的任务需要更多的资源，则可以终止（抢占）同一台机器上较低优先顺序的任务以释放资源。优先顺序还决定了计算资源的定价：团队必须为他们使用的资源付费，而优先顺序更高的程序花费更多【59】。
这种架构允许非生产（低优先顺序）计算资源被 **过量使用（overcommitted）**，因为系统知道必要时它可以回收资源。与分离生产和非生产任务的系统相比，过量使用资源可以更好地利用机器并提高效率。但由于 MapReduce 作业以低优先顺序执行，它们随时都有被抢占的风险，因为优先顺序较高的程序可能需要其资源。在高优先顺序程序拿走所需资源后，批次作业能有效地 “捡面包屑”，利用剩下的任何计算资源。
在谷歌，执行一个小时的 MapReduce 任务有大约有 5% 的风险被终止，为了给更高优先顺序的程序挪地方。这一机率比硬体问题、机器重启或其他原因的机率高了一个数量级【59】。按照这种抢占率，如果一个作业有 100 个任务，每个任务执行 10 分钟，那么至少有一个任务在完成之前被终止的风险大于 50%。
这就是 MapReduce 被设计为容忍频繁意外任务终止的原因：不是因为硬体很不可靠，而是因为任意终止程序的自由有利于提高计算丛集中的资源利用率。
在开源的丛集排程器中，抢占的使用较少。YARN 的 CapacityScheduler 支援抢占，以平衡不同伫列的资源分配【58】，但在编写本文时，YARN，Mesos 或 Kubernetes 不支援通用的优先顺序抢占【60】。在任务不经常被终止的环境中，MapReduce 的这一设计决策就没有多少意义了。在下一节中，我们将研究一些与 MapReduce 设计决策相异的替代方案。
## MapReduce之后
虽然 MapReduce 在 2000 年代后期变得非常流行，并受到大量的炒作，但它只是分散式系统的许多可能的程式设计模型之一。对于不同的资料量，资料结构和处理型别，其他工具可能更适合表示计算。
不管如何，我们在这一章花了大把时间来讨论 MapReduce，因为它是一种有用的学习工具，它是分散式档案系统的一种相当简单明晰的抽象。在这里，**简单** 意味著我们能理解它在做什么，而不是意味著使用它很简单。恰恰相反：使用原始的 MapReduce API 来实现复杂的处理工作实际上是非常困难和费力的 —— 例如，任意一种连线演算法都需要你从头开始实现【37】。
针对直接使用 MapReduce 的困难，在 MapReduce 上有很多高阶程式设计模型（Pig、Hive、Cascading、Crunch）被创造出来，作为建立在 MapReduce 之上的抽象。如果你了解 MapReduce 的原理，那么它们学起来相当简单。而且它们的高阶结构能显著简化许多常见批处理任务的实现。
但是，MapReduce 执行模型本身也存在一些问题，这些问题并没有透过增加另一个抽象层次而解决，而对于某些型别的处理，它表现得非常差劲。一方面，MapReduce 非常稳健：你可以使用它在任务会频繁终止的多租户系统上处理几乎任意大量级的资料，并且仍然可以完成工作（虽然速度很慢）。另一方面，对于某些型别的处理而言，其他工具有时会快上几个数量级。
在本章的其余部分中，我们将介绍一些批处理方法。在 [第十一章](ch11.md) 我们将转向流处理，它可以看作是加速批处理的另一种方法。
### 物化中间状态
如前所述，每个 MapReduce 作业都独立于其他任何作业。作业与世界其他地方的主要连线点是分散式档案系统上的输入和输出目录。如果希望一个作业的输出成为第二个作业的输入，则需要将第二个作业的输入目录配置为第一个作业输出目录，且外部工作流排程程式必须在第一个作业完成后再启动第二个。
如果第一个作业的输出是要在组织内广泛释出的资料集，则这种配置是合理的。在这种情况下，你需要透过名称引用它，并将其重用为多个不同作业的输入（包括由其他团队开发的作业）。将资料释出到分散式档案系统中众所周知的位置能够带来 **松耦合**，这样作业就不需要知道是谁在提供输入或谁在消费输出（请参阅 “[逻辑与布线相分离](#逻辑与布线相分离)”）。
但在很多情况下，你知道一个作业的输出只能用作另一个作业的输入，这些作业由同一个团队维护。在这种情况下，分散式档案系统上的档案只是简单的 **中间状态（intermediate state）**：一种将资料从一个作业传递到下一个作业的方式。在一个用于构建推荐系统的，由 50 或 100 个 MapReduce 作业组成的复杂工作流中，存在著很多这样的中间状态【29】。
将这个中间状态写入档案的过程称为 **物化（materialization）**。（在 “[聚合：资料立方体和物化检视](ch3.md#聚合：资料立方体和物化检视)” 中已经在物化检视的背景中遇到过这个术语。它意味著对某个操作的结果立即求值并写出来，而不是在请求时按需计算）
作为对照，本章开头的日志分析示例使用 Unix 管道将一个命令的输出与另一个命令的输入连线起来。管道并没有完全物化中间状态，而是只使用一个小的记忆体缓冲区，将输出增量地 **流（stream）** 向输入。
与 Unix 管道相比，MapReduce 完全物化中间状态的方法存在不足之处：
- MapReduce 作业只有在前驱作业（生成其输入）中的所有任务都完成时才能启动，而由 Unix 管道连线的程序会同时启动，输出一旦生成就会被消费。不同机器上的资料偏斜或负载不均意味著一个作业往往会有一些掉队的任务，比其他任务要慢得多才能完成。必须等待至前驱作业的所有任务完成，拖慢了整个工作流程的执行。
- Mapper 通常是多余的：它们仅仅是读取刚刚由 Reducer 写入的同样档案，为下一个阶段的分割槽和排序做准备。在许多情况下，Mapper 程式码可能是前驱 Reducer 的一部分：如果 Reducer 和 Mapper 的输出有著相同的分割槽与排序方式，那么 Reducer 就可以直接串在一起，而不用与 Mapper 相互交织。
- 将中间状态储存在分散式档案系统中意味著这些档案被复制到多个节点，对这些临时资料这么搞就比较过分了。
#### 资料流引擎
为了解决 MapReduce 的这些问题，几种用于分散式批处理的新执行引擎被开发出来，其中最著名的是 Spark 【61,62】，Tez 【63,64】和 Flink 【65,66】。它们的设计方式有很多区别，但有一个共同点：把整个工作流作为单个作业来处理，而不是把它分解为独立的子作业。
由于它们将工作流显式建模为资料从几个处理阶段穿过，所以这些系统被称为 **资料流引擎（dataflow engines）**。像 MapReduce 一样，它们在一条线上透过反复呼叫使用者定义的函式来一次处理一条记录，它们透过输入分割槽来并行化载荷，它们透过网路将一个函式的输出复制到另一个函式的输入。
与 MapReduce 不同，这些函式不需要严格扮演交织的 Map 与 Reduce 的角色，而是可以以更灵活的方式进行组合。我们称这些函式为 **运算元（operators）**，资料流引擎提供了几种不同的选项来将一个运算元的输出连线到另一个运算元的输入：