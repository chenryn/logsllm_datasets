time of a fast software implementation of a cipher like AES [18] often heavily de-
pends on the speed at which table look ups are performed. A popular implemen-
tation style for the AES is the T table implementation of AES [11] which com-
bines the SubBytes, ShiftRows and MixColumns operations into one single table
look up per state byte, along with XOR operations. This operation is called the
TableLookUp operation. The advantage of this implementation style is that it al-
lows the computation of one round using only table look-ups and XOR operations
which is much faster than performing the actual ﬁnite-ﬁeld arithmetic and logic
operations. Compared to using standard S-boxes, T table based implementations
use more memory, but the encryption time is signiﬁcantly reduced, especially on
32-bit CPUs. For this reason, almost all of the current software implementations
of the AES encryption for high-performance CPUs are T table implementations.
Note that the index of the loaded table entry is determined by a byte of the
cipher state. Hence, information on which table values have been loaded into
cache can reveal information about the secret state of AES. Such information
can be retrieved by monitoring the cache directly, as done in trace-driven cache
attacks. Similar information can also be learned by observing the timing behavior
of multiple AES executions over time, as done in time-driven cache attacks.
Finally, there are access driven cache attacks, which require the attacker to
learn which cache lines have been accessed (like trace-driven attacks), but (like
timing-driven attacks) do not require detailed knowledge on when and in what
order the data was accessed. So the diﬀerence between these classes of attacks
is the attacker’s access capabilities:
– Time driven attacks are the least restrictive type with the only assumption
that the attacker can observe the aggregated timing proﬁle of a full execution
of a target cipher.
– Trace driven attacks assume the attacker has access to the cache proﬁle
when the targeted program is running.
– Access driven attacks assume only to know which sets of the cache have
been accessed during the execution of a program.
Wait a Minute! A fast, Cross-VM Attack on AES
305
The attacks presented in this paper belong to a sub-class of access-driven cache
attacks, which we discuss next.
3.1 The Flush+Reload Technique
The Flush+Reload attack is a powerful cache-based side-channel attack tech-
nique ﬁrst proposed in [13], but was ﬁrst named in [33]. It can be classiﬁed as
an access driven cache attack. It usually employs a spy process to ascertain if
speciﬁc cache lines have been accessed or not by the code under attack. Gul-
lasch et al. [13] ﬁrst used this spy process on AES, although the authors did not
brand their attack as Flush+Reload at the time. Here we brieﬂy explain how
Flush+Reload works. The attack is carried out by a spy process which works in
3 stages:
Flushing stage: In this stage, the attacker uses the clflush command to ﬂush
the desired memory lines from the cache hence make sure that they have to
be retrieved from the main memory next time they need to be accessed.
We have to remark here that the clflush command does not only ﬂush the
memory line from the cache hierarchy of the corresponding working core, but
it ﬂushes from all the caches of all the cores in the PC. This is an important
point: if it only ﬂushed the corresponding core’s caches, the attack would
only work if the attacker and victim’s processes were co-residing on the
same core. This would have required a much stronger assumption than just
being in the same physical machine.
Target accessing stage: In this stage the attacker waits until the target runs
a fragment of code, which might use the memory lines that have been ﬂushed
in the ﬁrst stage.
Reloading stage: In this stage the attacker reloads again the previously ﬂushed
memory lines and measures the time it takes to reload. Depending on the
reloading time, the attacker decides whether the victim accessed the memory
line in which case the memory line would be present in the cache or if the vic-
tim did not access the corresponding memory line in which case the memory
line will not be present in the cache. The timing diﬀerence between a cache
hit and a cache miss makes the aforementioned access easily detectable by
the attacker.
The fact that the attacker and the victim processes do not reside on the same
core is not a problem for the Flush+Reload attack because even though there
can exist some isolation at various levels of the cache, in most systems there is
some level shared between all the cores present in the physical machine. There-
fore, through this shared level of cache (typically the L3 cache), one can still
distinguish between accesses to the main memory.
4 Memory Deduplication
Memory deduplication is an optimization technique that was originally intro-
duced to improve the memory utilization of VMMs. It later found its way into
306
G. Irazoqui et al.
common non-virtualized OSs as well. Deduplication works by recognizing pro-
cesses (or VMs) that place the same data in memory. This frequently happens
when two processes use the same shared libraries. The deduplication feature elim-
inates multiple copies from memory and allows the data to be shared between
users and processes. This method is especially eﬀective in virtual machine envi-
ronments where multiple guest OSs co-reside on the same physical machine and
share the physical memory. Consequently, variations of memory deduplication
technology are now implemented in both the VMware [28,29] and the KVM [3]
VMMs. Since KVM converts linux kernel into a hypervisor, it directly uses KSM
as page sharing technique, whereas VMware uses what is called Transparent Page
Sharing(TPS). Although they have diﬀerent names, their mechanism is very sim-
ilar; the hypervisor looks for identical pages between VMs and when it ﬁnds a
collision, it merges them into one single page.
Even though the deduplication optimization method saves memory and thus
allows more virtual machines to run on the host system, it also opens door to
side-channel attacks. While the data in the cache cannot be modiﬁed or cor-
rupted by an adversary, parallel access rights can be exploited to reveal secret
information about processes executing in the target VM. Also, an adversary can
prime the cache and wait for the victim to access some of this primed data. The
accessed/replaced cache data reveals information about the victims behavior. In
this study, we will focus on the Linux implementation of Kernel Samepage Merg-
ing (KSM) memory deduplication feature and on TPS mechanism implemented
by VMware.
4.1 KSM (Kernel Same-page Merging)
KSM is the Linux memory deduplication feature implementation that ﬁrst ap-
peared in Linux kernel version 2.6.32 [3]. In this implementation, KSM kernel
daemon ksmd, scans the user memory for potential pages to be shared among
users [7]. Also, since it would be CPU intensive and time consuming, instead of
scanning the whole memory continuously, KSM scans only the potential candi-
dates and creates signatures for these pages. These signatures are kept in the
deduplication table. When two or more pages with the same signature are found,
they are cross-checked completely to determine if they are identical. To create
signatures, the KSM scans the memory at 20 msec intervals and at best only
scans the 25% of the potential memory pages at a time. This is why any mem-
ory disclosure attack, including ours, has to wait for a certain time before the
deduplication takes eﬀect upon which the attack can be performed. During the
memory search, the KSM analyzes three types of memory pages [25];
– Volatile Pages: Where the contents of the memory change frequently and
should not be considered as a candidate for memory sharing.
– Unshared Pages: Candidate pages for deduplication where are the areas
that the madvise system call advises to the ksmd to be likely candidates for
merging.
– Shared Pages: Deduplicated pages that are shared between users or pro-
cesses.
Wait a Minute! A fast, Cross-VM Attack on AES
307
OpenSSL
Apache
Apache
Firefox
OpenSSL
OpenSSL
Apache
Firefox
Fig. 1. Memory Deduplication Feature
When a duplicate page signature is found among candidates and the contents are
cross-checked, ksmd automatically tags one of the duplicate pages with copy-on-
write (COW) tag and shares it between the processes/users while the other copy
is eliminated. Experimental implementations [3] show that using this method, it
is possible to run over 50 Windows XP VMs with 1GB of RAM each on a physical
machine with just 16GB of RAM. As a result of this, the power consumption
and system cost is signiﬁcantly reduced for systems with multiple users.
5 CFS-free Flush+Reload Attack on AES
In this section we will describe the principles of our Flush+Reload attack on
the C-implementation of AES in OpenSSL. In [13] Gullasch et al. described a
Flush+Reload attack on AES implementation of the OpenSSL library. However
in this study, we are going to use the Flush+Reload method with some modi-
ﬁcations that from our point of view, have clear advantages over [13]. Prior to
the comparison with other cache side channel attacks, a detailed explanation of
our Flush+Reload spy process is given along with the attack steps. We consider
two scenarios: the attack as a spy process running in the same OS instance as
the victim (as done in [13]), and the attack running as a cross-VM attack in a
virtualized environment.
308
G. Irazoqui et al.
5.1 Description of the Attack
As in prior Flush+Reload attacks, we assume that the adversary can monitor
accesses to a given cache line. However, unlike the attack in [13], this attack
– only requires the monitoring of a single memory line; and
– ﬂushing can be done before encryption, reloading after encryption, i.e. the
adversary does not need to interfere with or interrupt the attacked process.
More concretely, the Linux kernel features a completely fair scheduler which
tries to evenly distribute CPU time to processes. Gullasch et al. [13] exploited
Completely Fair Scheduler (CFS) [1], by overloading the CPU while a victim
AES encryption process is running. They managed to gain control over the CPU
and suspend the AES process thereby gaining an opportunity to monitor cache
accesses of the victim process. Our attack is agnostic to CFS and does not require
time consuming overloading steps to gain access to the cache.
We assume the adversary monitors accesses to a single line of one of the
T tables of an AES implementation, preferably a T table that is used in the last
round of AES. Without loss of generality, let’s assume the adversary monitors
the memory line corresponding to the ﬁrst positions of table T , where T is the
lookup table applied to the targeted state byte si, where si is the i-th byte
of the AES state before the last round. Let’s also assume that a memory line
can hold n T table values, e.g, the ﬁrst n T table positions for our case. If si
is equal to one of the indices of the monitored T table entries in the memory
line (i.e. si ∈ {0, . . . , n} if the memory line contains the ﬁrst n T table entries)
then the monitored memory line will with very high probability be present in
the cache (since it has been accessed by the encryption process). However, if
si takes diﬀerent values, the monitored memory line is not loaded in this step.
Nevertheless, since each T table is accessed l times (for AES-128 in OpenSSL,
l = 40 per Tj), there is still a probability that the memory line was loaded
by any of the other accesses. In both cases, all that happens after the T table
lookup is a possible reordering of bytes (due to AES’s Shift Rows), followed by
the last round key addition. Since the last round key is always the same for si,
the n values are mapped to n speciﬁc and constant ciphertext byte values. This
means that for n out of 256 ciphertext values, the monitored memory line will
always have been loaded by the AES operation, while for the remaining 256− n
values the probability of having been reloaded is smaller. In fact, the probability
that the speciﬁc T table memory line i has not been accessed by the encryption
process is given as:
Pr [no access to T [i]] =
(cid:3)l
(cid:2)
1 − t
256
Here, l is the number of accesses to the speciﬁc T table. For OpenSSL 1.0.1
AES-128 we have l = 40. If we assume that each memory line can hold t = 8
entries per cache line, we have Pr [no access to T [i]] = 28%. Therefore it is easily
distinguishable whether the memory line is accessed or not. Indeed, this turns
out to be the case as conﬁrmed by our experiments.
Wait a Minute! A fast, Cross-VM Attack on AES
309
Algorithm 1. Recovery algorithm for key byte k0
Input : X0
Output: k0
forall xj ∈ X0 do
//Reload vector for ciphertext byte 0
//Correct key byte 0
//Threshold for values with low reload counter.
if xj < Low counter threshold then
for s = 0 to n do
//xor with each value of the targeted T table memory line
K0[j ⊕ T [s]]++;
end
end
end
return argmaxk(K0[k]);
In order to distinguish the two cases, all that is necessary is to measure the
timing for the reload of the targeted memory line. If the line was accessed by
the AES encryption, the reload is quick; else it takes more time. Based on a
threshold that we will empirically choose from our measurements, we expect
to distinguish main memory accesses from L3 cache accesses. For each possible
value of the ciphertext byte ci we count how often either case occurs. Now, for
n ciphertext values (the ones corresponding to the monitored T table memory
line) the memory line has always been reloaded by AES, i.e. the reload counter
is (close to) zero. These n ciphertext values are related to the state as follows:
(cid:4)
(cid:5)
ci = ki ⊕ T
s[i]
(1)
where the s[i] can take n consecutive values. Note that Eq. (1) describes the
last round of AES. The brackets in the index of the state byte s[i] indicate the
reordering due to the Shift Rows operation. For the other values of ci, the reload
counter is signiﬁcantly higher. Given the n values of ci with a low reload counter,
we can solve Eq. (1) for the key byte ki, since the indices s[i] as well as the table
output values T
are known for the monitored memory line. In fact, we get
n possible key candidates for each ci with a zero reload counter. The correct key
is the only one that all n valid values for ci have in common.
s[i]
(cid:4)
(cid:5)
A general description of the key recovery algorithm is given in Algorithm 1,
where key byte number 0 is recovered from the ciphertext values corresponding
to n low reload counter values that were recovered from the measurements.
Again, n is the number of T table positions that a memory line holds. The
reload vector Xi = [x(0), x(1), . . . , x(255)] holds the reload counter values x(j)