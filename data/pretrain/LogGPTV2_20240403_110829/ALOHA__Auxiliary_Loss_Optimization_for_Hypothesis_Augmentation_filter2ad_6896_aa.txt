title:ALOHA: Auxiliary Loss Optimization for Hypothesis Augmentation
author:Ethan M. Rudd and
Felipe N. Ducau and
Cody Wild and
Konstantin Berlin and
Richard E. Harang
ALOHA: Auxiliary Loss Optimization for 
Hypothesis Augmentation
Ethan M. Rudd, Felipe N. Ducau, Cody Wild, Konstantin Berlin, and Richard Harang, Sophos
https://www.usenix.org/conference/usenixsecurity19/presentation/rudd
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.ALOHA: Auxiliary Loss Optimization for Hypothesis Augmentation
Ethan M. Rudd∗, Felipe N. Ducau∗, Cody Wild, Konstantin Berlin, and Richard Harang∗
Sophos PLC
Abstract
Malware detection is a popular application of Machine
Learning for Information Security (ML-Sec), in which an
ML classiﬁer is trained to predict whether a given ﬁle is mal-
ware or benignware. Parameters of this classiﬁer are typi-
cally optimized such that outputs from the model over a set
of input samples most closely match the samples true ma-
licious/benign (1/0) target labels. However, there are often
a number of other sources of contextual metadata for each
malware sample, beyond an aggregate malicious/benign la-
bel, including multiple labeling sources and malware type in-
formation (e.g. ransomware, trojan, etc.), which we can feed
to the classiﬁer as auxiliary prediction targets. In this work,
we ﬁt deep neural networks to multiple additional targets de-
rived from metadata in a threat intelligence feed for Portable
Executable (PE) malware and benignware, including a multi-
source malicious/benign loss, a count loss on multi-source
detections, and a semantic malware attribute tag loss. We
ﬁnd that incorporating multiple auxiliary loss terms yields a
marked improvement in performance on the main detection
task. We also demonstrate that these gains likely stem from
a more informed neural network representation and are not
due to a regularization artifact of multi-target learning. Our
auxiliary loss architecture yields a signiﬁcant reduction in
detection error rate (false negatives) of 42.6% at a false pos-
itive rate (FPR) of 10−3 when compared to a similar model
with only one target, and a decrease of 53.8% at 10−5 FPR.
1 Introduction
Machine learning (ML) for computer security (ML-Sec) has
proven to be a powerful tool for malware detection. ML
models are now integral parts of commercial anti-malware
engines and multiple vendors in the industry have dedicated
ML-Sec teams. For the malware detection problem, these
∗ Equal contribution.
Contact: PI:EMAIL
models are typically tuned to predict a binary label (mali-
cious or benign) using features extracted from sample ﬁles.
Unlike signature engines, where the aim is to reactively
blacklist/whitelist malicious/benign samples that hard-match
manually-deﬁned patterns (signatures), ML engines employ
numerical optimization on parameters of highly parameter-
ized models that aim to learn more general concepts of mal-
ware and benignware. This allows some degree of proactive
detection of previously unseen malware samples that is not
typically provided by signature-only engines.
Frequently, malware classiﬁcation is framed as a binary
classiﬁcation task using a simple binary cross-entropy or
two-class softmax loss function. However, there often ex-
ist substantial metadata available at training time that contain
more information about each input sample than just an aggre-
gate label of whether it is malicious or benign. Such meta-
data might include malicious/benign labels from multiple
sources (e.g., from various security vendors), malware fam-
ily information, ﬁle attributes, temporal information, geo-
graphical location information, counts of affected endpoints,
and associated tags. In many cases this metadata will not be
available when the model is deployed, and so in general it
is difﬁcult to include this data as features in the model (al-
though see Vapnik et al. [28, 29] for one approach to doing
so with Support Vector Machines).
It is a popular practice in the domain of malware analysis
to derive binary malicious/benign labels based on a heuristic
combination of multiple detection sources for a given ﬁle,
and then use these noisy labels for training ML models [9].
However, there is nothing that precludes training a classiﬁer
to predict each of these source labels simultaneously opti-
mizing classiﬁer parameters over these predictions + labels.
In fact, one might argue intuitively that guiding a model to
develop representations capable of predicting multiple tar-
gets simultaneously may have a smoothing or regularizing
effect conducive to generalization, particularly if the auxil-
iary targets are related to the main target of interest. These
auxiliary targets can be ignored during test time if they are
ancillary to the task at hand (and in many cases the extra
USENIX Association
28th USENIX Security Symposium    303
weights required to produce them can be removed from the
model prior to deployment), but nevertheless, there is much
reason to believe that forcing the model to ﬁt multiple targets
simultaneously can improve performance on the key output
of interest. In this work, we take advantage of multi-target
learning [2] by exploring the use of metadata from threat in-
telligence feeds as auxiliary targets for model training.
Research in other domains of applied machine learning
supports this intuition [14, 19, 12, 31, 1, 22], however out-
side of the work of Huang et al. [11], multi-target learning
has not been widely applied in anti-malware literature. In
this paper, we present a wide-ranging study applying aux-
iliary loss functions to anti-malware classiﬁers. In contrast
to [11], which studies the addition of a single auxiliary la-
bel for a fundamentally different task, i.e., malware family
classiﬁcation – we study both the addition of multiple la-
bel sources for the same task and multiple label sources for
multiple separate tasks. Also, in contrast to [11], we do not
presume the presence of all labels from all sources, and in-
troduce a per-sample weighting scheme on each loss term to
accommodate missing labels in the metadata. We further ex-
plore the use of multi-objective training as a way to expand
the number of trainable samples in cases where the aggregate
malicious/benign label is unclear, and where those samples
would otherwise be excluded from purely binary training.
Having established for which loss types and in which con-
texts auxiliary loss optimization works well, we then explore
why it works well, via experiments designed to test whether
performance gains are a result of a regularization effect from
multi-objective training or information from the content of
the target labels that the network has learned to correlate.
In summary, this paper makes the following contributions:
• A demonstration that including auxiliary losses yields
improved performance on the main task. When all of
our auxiliary loss terms are included, we see a reduc-
tion of 53.8% in detection error (false negative) rate at
10−5 false positive rate (FPR) and a 42.6% reduction in
detection error rate at 10−3 FPR compared to our base-
line model. We also see a consistently better and lower-
variance ROC curve across all false positive rates.
• A breakdown of performance improvements from dif-
ferent auxiliary loss types. We ﬁnd that an auxiliary
Poisson loss on detection counts tends to yield im-
proved detection rates at higher FPR areas (≥ 10−3) of
the ROC curve, while multiple binary auxiliary losses
tend to yield improved detection performance in lower
FPR areas of the ROC curve (< 10−3). When combined
we see a net improvement across the entire ROC curve
over using any single auxiliary loss type.
• An investigation into the mechanism by which multi-
objective optimization yields enhanced performance,
including experiments to assess possible regularization
effects.
(a) Training
(b) Deployment
Figure 1: Schematic overview of our neural network archi-
tecture. (a) During training multiple output layers with cor-
responding loss functions are optionally connected to a com-
mon base topology consisting of ﬁve dense blocks (see Sec-
tion 3) of sizes 1024, 768, 512, 512, and 512. This base,
connected to our main malicious/benign output (solid line in
the ﬁgure) with a loss on the aggregate label, constitutes our
baseline architecture. Auxiliary outputs and their respective
losses are represented as dashed lines. The auxiliary losses
fall into three types: count loss, multi-label vendor loss, and
multi-label attribute tag loss. The formulation of each of
these auxiliary loss types is explained in Section 3. (b) At
deployment time, these auxiliary outputs are removed and
we predict only the main label.
We see our auxiliary loss approach as speciﬁcally useful
for both endpoint and cloud deployed models in cases when
the auxiliary information cannot be directly used as input
into the model at prediction time, but can be collected for
a training dataset. This could be due to high cost, perfor-
mance issues, latency concerns, or a multitude of other con-
straints during prediction time. For example, it is not feasible
to scan every new ﬁle executed on an endpoint via a threat
intelligence feed, because of prohibitive licensing fees, end-
point latency and bandwidth limitations, as well as customer
privacy concerns. However, procuring reports from such a
feed for large training sets might be feasible ofﬂine.
The remainder of this paper is laid out as follows: First,
in Section 2 we discuss some of the metadata available for
use as auxiliary targets, and feature extraction methods for
portable executable (PE) ﬁles. We then provide details on
how we converted the available metadata into auxiliary tar-
304    28th USENIX Security Symposium
USENIX Association
gets and losses, as well as how the individual losses are
weighted and combined on a per-sample basis in Section 3.
We ﬁnish that Section with a description of how our dataset
was collected and provide summary statistics. In Section 4
we describe our experimental evaluations across a number of
combinations of auxiliary targets, and demonstrate the im-
pact of ﬁtting these targets on the performance of the main
malware detection task. Section 5 presents discussion of our
results, as well as a set of experiments on synthetic targets
to explore potential explanations for the observed improve-
ments. Section 6 presents related work and Section 7 con-
cludes.
2 ML-Sec Detection Pipelines: From Single
Objective to Multi-Objective
In the following, we describe a simpliﬁed ML-Sec pipeline
for training a malicious ﬁle classiﬁer, and propose a simple
extension that allows the use of metadata available during
training (but not at test time) and improves performance on
the classiﬁcation task.
ML-Sec detection pipelines use powerful machine learn-
ing models that require many labeled malicious and be-
nign samples to train. Data is typically gathered from sev-
eral sources, including deployed anti-malware products and
vendor aggregation services, which run uploaded samples
through vendor products and provide reports containing per-
vendor detections and metadata. The exact nature of the
metadata varies, but typically, malicious and benign scores
are provided for each of M individual samples on a per-
vendor basis, such that, given V vendors, between 0 and V of
them will designate a sample malicious. For a given sample,
some vendors may choose not to answer, resulting in a miss-
ing label for that vendor. Furthermore, many vendors also
provide a detection name (or malware family name) when
they issue a detection for a given ﬁle. Additional information
may also be available, but crucially, the following metadata
are presumed present for the models presented in this paper:
i) per-vendor labels for each sample, either malicious/benign
(mapped to binary 1/0, respectively) or NULL; ii) textual
per-vendor labels on the sample describing the family and
variant of the malware (an empty string if benign or NULL);
and iii) time at which the sample was ﬁrst seen.
Using the individual vendor detections, an aggregate label
can be derived either by a voting mechanism or by thresh-
olding the net number of vendors that identify a given sam-
ple as malicious. The use of aggregated anti-malware ven-
dor detections as a noisy labeling source presumes that the
vendor diagnoses are generally accurate. While this is not
necessarily a valid assumption, e.g., for novel malware and
benignware, this is typically accounted for by using samples
and metadata that are slightly dated so that vendors can cor-
rect their respective mistakes (e.g., by blacklisting samples
in their signature databases).
Each malware/benignware sample must also be converted
to a numerical vector to be able to train a classiﬁer, a process
called feature extraction.
In this work we focus on static
malware detection, meaning that we assume only access to
the binary ﬁle, as opposed to dynamic detection, in which
the features used predominantly come from the execution of
the ﬁle. The feature extraction mechanism varies depending
on the format type at hand, but consists of some numerical
transformation that preserves aggregate and ﬁne-grained in-
formation throughout each sample, for example, the feature
extraction proposed by Saxe et al. [25] – which we use in
this work – uses windowed byte statistics, 2D histograms of
delimited string hash vs. length, and histograms of hashes of
PE-format speciﬁc metadata – e.g., imports from the import
address table (IAT).
Given extracted features and derived labels, a classi-
ﬁer is then trained,
tuning parameters to minimize mis-
classiﬁcation as measured by some loss criterion, which un-
der the constraints of a statistical noise model measures the
deviation of predictions from their ground truth. For both
neural networks and ensemble methods a logistic sigmoid is
commonly used to constrain predictions to a [0,1] range, and
a cross-entropy loss between predictions and labels is used
as the minimization criterion under a Bernoulli noise model
per-label.
While the prior description roughly characterizes ML-Sec
pipelines discussed in literature to date, note that much infor-
mation in the metadata, which is often not used to determine
the sample label but is correlated to the aggregate classiﬁca-
tion, is not used in training, e.g., the individual vendor classi-
ﬁcations, the combined number of detections across all ven-
dors, and information related to malware type that could be
derived from the detection names. In this work, we augment
a deep neural network malicious/benign classiﬁer with addi-
tional output predictions of vendor counts, individual vendor
labels, and/or attribute tags. These separate prediction arms
were given their own loss functions which we jointly min-
imized through backpropagation. The difference between a
conventional malware detection pipeline and our model can
be visualized by considering Figure 1 in the absence and
presence of auxiliary outputs (and their associated losses)
connected by the dashed lines. In the next section, we shall
explore the precise formulation and implementation of these
different loss functions.
3
Implementation Details
In this section we describe our implementation of the experi-
ments sketched above. We ﬁrst introduce our model immedi-
ately below, followed by the various loss functions – denoted
by Lloss type (X,Y ) for some input features X and targets Y –
associated with the various outputs of the model, as well as
how the labels Y representing the targets of these outputs are
USENIX Association
28th USENIX Security Symposium    305
constructed. Finally we discuss how our data set of M sam-
ples associated with V vendor targets is collected. We use
the same feature representation as well as the same general
model class and topology for all experiments. Each portable
executable ﬁle is converted into a feature vector as described
in [25].
The base for our model (see Figure 1) is a feedforward
neural network incorporating multiple blocks composed of
Dropout [27], a dense layer, batch normalization [13], and
an exponential linear unit (ELU) activation [7]. The core
of the model contains ﬁve such blocks with 1024, 768, 512,
512, and 512 hidden units, respectively. This base topology
applies the function f (·) to the input vector to produce an
intermediate 512 dimensional representation of the input ﬁle
h = f (x). We then append to this model an additional block,
consisting of one or more dense layers and activation func-
tions, for each output of the model. We denote the composi-
tion of our base topology and our target-speciﬁc ﬁnal dense
layers and activations applied to features x by ftarget(x). The
output for the main malware/benign prediction task – fmal(x)
– is always present and consists of a single dense layer fol-
lowed by a sigmoid activation on top of the base shared net-
work that aims to estimate the probability of the input sample
being malicious. A network architecture with only this mal-
ware/benign output serves as the baseline for our evaluations.
To this baseline model we add auxiliary outputs with similar
structure as described above: one fully connected layer (two
for the tag prediction task in Section 3.4) which produces
some task-speciﬁc number of outputs (a single output, with
the exception of the restricted generalized Poisson distribu-
tion output, which uses two) and some task-speciﬁc activa-
tion described in the associated sections below.
Except where noted otherwise, all multi-task losses were
produced by computing the sum, across all tasks, of the per-
task loss multiplied by a task-speciﬁc weight (1.0 for the
malware/benign task and 0.1 for all other tasks; see Section
4). Training was standardized at 10 epochs; for all experi-
ments we used a training set of 9 million samples and a test
set of approximately 7 million samples. Additional details
about the training and test data are reported in Section 3.6.
Additionally, we used a validation set of 100,000 samples to
ensure that each network had converged to an approximate
minimum on validation loss after 10 epochs. All of our mod-
els were implemented in Keras [6] and optimized using the
Adam optimizer [15] with Keras’s default parameters.
3.1 Malware Loss
As explained in Section 2, for the task of predicting if a given
binary ﬁle, represented by its features x(i), is malicious or
benign we used a binary cross-entropy loss between the mal-
ware/benign output of the network ˆy(i) = fmal(x(i)) and the
malicious label y(i). This results in the following loss for a
dataset with M samples:
Lmal(X,Y ) =
(cid:96)mal( fmal(x(i)),y(i))
M
∑
i=1
1
M
= − 1
M
M
∑
i=1
y(i) log( ˆy(i)) + (1− y(i))log(1− ˆy(i)).