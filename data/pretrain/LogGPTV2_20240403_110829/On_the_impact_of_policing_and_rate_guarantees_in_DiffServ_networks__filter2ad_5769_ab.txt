recent  development  (see  [26]  for  details  and  pointers).  Video 
quality  assessment  can  be  performed  in  either  a  subjective  or  an 
objective manner. Subjective quality tests typically require a group 
of viewers to watch short video clips of approximately 10 seconds 
in  duration  in  a  very  controlled  environment,  and  then  rate  this 
material.  The  most  widely  used  methods  for  measuring  the 
subjective  quality  of  speech  and  video  images  have  been 
standardized 
International 
Telecommunication  Union  (ITU),  and  the  results  are  frequently 
expressed in terms of the ITU-T mean opinion score (MOS) (see 
[7] and [9]).   
recommended 
Although subjective quality measurements reflect real human 
perception  of  the  quality  of  a  video  stream,  they  involve  a 
relatively  complex  and  time-consuming  process  that  is  often  not 
practical  when  a  large  number  of  configurations  with  varying 
parameters  need  to  be  assessed.    These  limitations  of  subjective 
quality  measurement  methods  prompted  the  development  of 
alternative  approaches,  called  “objective  quality  measurement 
methods,” that lend themselves more easily to automation and are 
capable  of  operating  in  less  controlled  environments.  Objective 
quality measurement methods are based on computational models 
that combine a number of key video quality measures, and which 
are  calibrated  based  on  the  correlation  of  the  model  scores  to 
subjective scores for a number of pre-determined experiments. Just 
as  with subjective methods, the various parameters that objective 
performance  assessment 
incorporate  have  been 
standardized by the American National Standards Institute (ANSI) 
(see  [1]).  In  our  experiments,  we  relied  on  an  objective  video 
quality  measurement 
for 
Telecommunication Sciences (ITS), which enabled us to evaluate 
the quality of a large number of combinations of video servers and 
formats  and  network  configurations.  In  the  next  section,  we 
provide additional details on both the video quality  measurement 
tool and on the overall experimental setup that was used. 
tool  developed  by 
tools  can 
Institute 
and 
the 
by 
the 
3.  Methodology and Experimental Setup 
The development of the various components required to carry 
out  our  video  quality  assessment  experiments  represented  a 
significant fraction of the work, and several aspects turned out to 
be of independent interest.  In this section, we briefly describe the 
three  main  pieces  of  our  experimental  setup:    the  video  quality 
measurement  tool,  the  network  testbeds  over  which  video  was 
transmitted,  and  finally  the  video  clips  themselves,  with  their 
intrinsic properties.  
3.1 Video Quality Measurement Tool 
The tool and methodology that we adopted in our experiments 
are  those  developed  by  ITS,  and  which  are  based  on  the  ANSI 
objective quality standards T1.801.03-1996 [1] as well as several 
more  recently  developed  metrics  [29].  The  ITS  Video  Quality 
Measurement  (VQM)  software  tool  is  based  on  a  family  of 
objective quality assessment methods called Feature Extraction or 
Reduced  Reference  [17],  [27].    The  approach  is  to  rely  on 
mathematical  models  to  capture  the  major  features  of  either 
individual  frames  (spatial  features)  or  sequence  of  frames 
(temporal  features)  from  both  the  received  video  stream  and  the 
reference  video  stream.  The  quality  of  a  received  series  of  video 
frames  is  then  assessed  by  comparing  the  time  histories  of  the 
received feature streams with the reference feature streams, and by 
combining multiple quality parameters so generated into an overall 
quality score. As mentioned earlier, the combination is performed 
so as to generate good agreement with the results of a number of 
previous subjective assessment experiments.  In summary, the ITS 
tool follows this method and performs the following three steps to 
generate quality-rating indices. 
(cid:31)  Extract quality features that characterize fundamental aspects 
of video quality (spatial detail, motion, color) from sequences of 
input and output video frames, 
(cid:31)  Compute  perception-based  video  quality  parameters  by 
comparing  the  features  of  the  received  (output)  video  frames 
with  the  corresponding  features  of  the  original  (input)  video 
frames, and  
(cid:31)  Produce a composite quality score from the computed digital 
video  quality  parameters  that  is  highly  correlated  with  the 
subjective assessments of human viewer panels.  
runs  on  an 
IRIX™  platform  and 
A number of additions were needed in order to be able to use 
the VQM tool to assess the quality of video transmitted over an IP 
network.    This  is  because  the  tool  was  originally  developed  for 
television  and  video  conferencing  systems,  where  both  the  video 
formats  and  the  content  delivery  mechanisms  are  completely 
different from those used to transmit video over IP networks. The 
tool 
takes  decoded 
(uncompressed)  video  as  input  in  the  YUV 4:2:2 [8]  file  format, 
which  is  a  binary  file  format  used  by  the  ITU  Video  Quality 
Experts  Group  (VQEG)  (this  file  format  will  henceforth  be 
referred to as BigYUV, since all the YUV frames from a scene are 
stored in one large file). The main challenge for us was to generate 
an appropriately  formatted input for the VQM tool, based on the 
received  video  streams  after  they  were  transmitted  over  the 
network.  This difficulty was compounded by the fact that because 
the  processing  involved  is  computationally  intensive,  the  tool  is 
typically unable to process received video streams in “real-time.”  
As a result, it is necessary to provide some intermediate storage of 
the  received  video  prior  to  feeding  them  to  the  tool.    This 
introduced a number of additional problems.   
The first problem is that video streaming clients are typically 
built to only render contents on a screen and do not have the option 
of  recording  or  saving  the  received  video.  As  a  result,  it  was 
necessary  to  add  an  intermediate  step  in  which  received  video 
streams are saved to a file.  The quality measurement process could 
then  be  conducted  offline,  i.e.,  after  the  streaming  process,  by 
presenting the saved video frames to the VQM tool.  The second 
problem associated with storing the received video stream was that 
this  additional  step  had  to  accurately  preserve  the  perturbations, 
e.g., information regarding frame delays and drops, introduced by 
the  various  network  configurations  being  tested.  Finally,  a  third 
problem we encountered was caused by the fact that the tool, like 
most  other  objective  assessment  tools,  was  designed  to  handle 
relatively short duration video segments, e.g., on the order of 5 to 
10 seconds.  We wanted to experiment with longer video segments, 
e.g.,  between  75  and  150  seconds,  to  be  able  to  consider  video 
clips  that  would  incorporate  a  broader  and  more  representative 
range  of  scene  types,  and  get  a  more  realistic  assessment  of  the 
overall  impact  of  network  configurations.  As  a  result,  some  care 
had to be exercised when using the tool to process such extended 
segments.  In the rest of this section, we outline how we addressed 
these problems in developing our experimental setup. 
Filter Graph Manager 
Source file 
reader 
Format Parser 
and Splitter 
Audio Media 
Decoder 
Video Media 
Decoder 
Audio Media 
Renderer 
Video Media 
Renderer 
Filter Graph 
Figure 1. Directshow™ filter graph and graph manager. 
3.1.1 
Storing Received Video Frames to File 
We investigated a number of approaches for implementing the 
required intermediate video storage step, and ultimately settled on 
one  that  had  the  benefit  of  being  readily  portable,  at  least  across 
the  video  clients  we  were  using  that  all  ran  on  the  Microsoft 
Windows™ platform. Most of the multimedia clients built for the 
Windows™  platform  are  implemented  using  the  Directshow™ 
architecture. Directshow™ controls and processes the playback of 
multimedia streams from local files or network servers. It enables 
the  playback  of  compressed  video  and  audio  contents  using  a 
modular  approach  that  divides  the  processing  of  multimedia 
objects into a set of stages known as filters. Filters are pluggable 
components connected to each other through “pins” that represent 
a  logical  point  of  connection  for  a  unidirectional  data  stream 
flowing  to/from  the  filter.  A  filter  graph  is  composed  of  a 
collection of connected filters of different types used to process a 
specific  media  format.  Applications  use  what  is  called  a  filter 
graph  manager  to  assemble  and  connect  the  filter  graph  suitable 
for their media format, and for controlling the movement of data 
through the assembled graph. When an application starts rendering 
a  piece  of  media  content,  the  filter  graph  manager  first  selects  a 
source  filter  capable  of  reading  the  media  content,  and  then 
proceeds to select and connect subsequent filters based on the filter 
graph,  the  last  filter  being  typically  the  rendering  stage.  An 
illustration  of  the  structure  of  a  typical  filter  graph  is  shown  in 
Figure 1. 
Given the modular structure of the Directshow™ architecture, 
a  natural  approach  for  introducing  the  ability  to  store  received 
video frames is to develop a filter performing the required storage 
operations,  and  to  insert  it  at  an  appropriate  location  in  the  filter 
graph  of  video  clients.  The  location  we  chose  to  insert  our 
“storage” filter was after the video decoder in lieu of the renderer. 
The one disadvantage of such a choice is the storage requirement it 
implies,  as  video  frames  must  be  stored  in  an  uncompressed 
BigYUV  format.    However,  our  experience  has  been  that  this 
disadvantage  was  more  than  compensated  for  by  a  greater 
robustness  and  reliability  across  clients  and  media  formats.    In 
addition to replacing the renderer filter by our new storage filter, 
another modification that had to be made to the original client filter 
graph  was  to  ensure  that  the  video  decoder  would  produce  an 
output  in  the  desired  BigYUV  format.  This  was  relatively  easy 
since  most  decoders  allow  the  application  to  select  the  output 
format they will generate.  
3.1.2  Capturing Network Dynamics Information 
The  storage  filter  receives  each  frame,  captures  the  relevant 
timing  information  that  consists  of  its  arrival  time  and  target 
presentation time, and then saves the frame in a binary file and the 
timing information in a parallel ASCII file. The timing information 
will be used to create a modified set of frames that will be fed to 
the  VQM  tool,  and  that  will  emulate  the  impairments  caused  by 
network perturbations, as a user viewing frames being played out 
by the renderer would have perceived them.  Specifically, we want 
to  capture  in  our  quality  assessment  the  effect  of  the  techniques 
used by most renderers4 to compensate for lost or delayed frames.  
The most common and simplest technique is to keep repeating the 
last received frame until a new frame arrives. This is the approach 
we chose to emulate.  This was implemented using a simple PERL 
script that takes as input the initial file of stored received frames 
together  with  the  associated  statistics  file  created  by  the  filter. 
From these inputs, the script produces a new file containing stored 
frames,  but  which  now  incorporates  the  repeated  frames  that  a 
renderer would have generated while attempting to compensate for 
lost or delayed frames.  
P o sitive O ffset
Fram es
W aiting
R ead
 Fram e
Fram e B uffer
W rite
 Fram e
N eg a tive O ffset
R ead
 Fram e
Fram e B uffer
R epeat
W rite
 Fram e
Figure 2. Handling of Lost or Delayed Frames. 
The  mechanism  on  which  the  script  relies  is  based  on 
maintaining  two  time  references  associated  with  the  presentation 
time  and  the  arrival  time,  and  by  comparing  them  to  determine 
when  the  playback  buffer  runs  out  of  frames  because  of  lost  or 
delayed  frames.  This  is  achieved  by  maintaining  an  offset  value 
representing  the  difference  between  the  arrival  and  presentation 
times of frames. Frames that are early or on time have an arrival 
time less than their presentation time, and result in an increase of 
the offset value by an amount equal to their difference. In contrast, 
a late frame has an arrival time that is greater than its presentation 
(cid:137)                                                                        
4 It is the renderer rather than the decoder that is often responsible 
for  concealing  the  impairments  caused  by  excessive  delays  or 
lost frames in the network. 
time,  in  which  case  this  negative  value  is  used  to  decrease  the 
current  offset  value  by  a  similar  amount.  The  script  updates  the 
offset value for each received frame, and the resulting value is used 
to  determine  the  state  of  the  playback  buffer  as  seen  by  the 
renderer. A positive offset value indicates that frames are available 
for  rendering  in  the  buffer,  while  a  negative  offset  value 
corresponds  to  an  empty  playback  buffer  that  would  trigger  the 
repetition of the previous frame by the renderer.  As a result, the 
script  inserts  copies  of  the  previous  frame  in  the  output  file  it 
produces.  The number of copies that are inserted is a function of 
the offset value and the presentation and arrival times of the next 
available  frames.    An  illustration  of  the  process  implemented  by 
the script is shown in Figure 2. 
3.1.3  Handling Extended Duration Video Clips 
As mentioned earlier, the VQM tool was originally designed 
to  measure  the  quality  of  short  (5  to  10  seconds)  duration  video 
segments,  while  we  wanted  to  use  longer  (between  75  and  150 
seconds) video clips in our experiments. In order to use the VQM 
tool  on  those  longer  clips,  we  therefore  had  to  divide  them  into 
smaller 10-second segments that were then fed to the tool one by 
one,  and  whose  individual  quality  scores  had  to  be  combined. 
Applying  this  process  raised  two  questions.  The  most  significant 
one  involved  the  calibration  process  that  is  used  to  remove 
systematic errors (i.e., gain, spatial shift, temporal shift) from the 
received video stream. A control file that performs both spatial and 
temporal  calibration  between  the  two  sets  of  frames  drives  this 
calibration.    The  second  question  concerned how  to  combine  the 
scores  obtained  by  short  duration  segments 
to  produce  a 
meaningful overall score for the extended duration video clip. 
We configured the tool to segment the stored video files into 
segments consisting of 300 frames each (10 seconds duration). The 
segmentation is done so that the first 100 frames of each segment 
overlap with the last 100 frames of the segment preceding it (see 
Figure  3).  Note  that  the  last  100  frames  of  the  last  segment  and 
first  100  frames  of  the  first  segment  do  not  overlap  with  other 
segments. The overlap of consecutive segments is used to provide 
sufficient margin to allow the temporal calibration mechanism of 
the tool to find the proper alignment between the original file and 
the  received  file.    This  is  achieved  by  setting  the  Alignment 
Uncertainty  parameter  specified  in  the  control  file  of  the  tool  to 
allow searching in this specified range. The quality estimation of a 
segment  is  then  based  on  the  next  100  frames  following  the 
alignment point identified by the temporal calibration mechanism. 
To Tool 
Unprocessed 
Alignment 
Uncertainty 
Segment #1 (300 Frames) 
Segment #3 (300 Frames) 
…. 
Segment #2 (300 Frames) 
Figure 3. Segmentation Process of the Stored Video File. 
In order to calculate the overall quality of an extended video 
clip, we simply averaged the quality estimates of all the individual 
segments.  However,  some  care  had  to  be  exercised  to  deal  with 
long  (around  10  seconds  or  more)  periods  of  degraded  quality, 
which the temporal calibration process  was not able to deal  with 
even with its extended range. Specifically, segments for which the 
temporal  calibration  process  did  not  succeed  were  assigned  a 
default  quality  index  of  1  that  corresponds  to  the  worst  quality 
index assigned by the tool (the best quality index assigned by the 
tool  is  zero).  Here,  the  term  “quality  index”  refers  to  the  quality 
estimate produced by the tool. 
3.2 Network Testbeds  
Quality  assessments  of  received  video  clips  were  performed 
using two network testbeds.  One was a local testbed consisting of 
several Diff-Serv capable routers to which a video server and video 
clients  were  connected. 
  The  ability  to  easily  change  the 
configuration of the local routers as well as the easy access to the 
video server facilitated the exploration of a relatively wide range of 
configurations.    However,  it  is  clear  that  a  local  testbed  cannot 
emulate  all  of  the  possible  interactions  that  occur  in  actual 
networks.    In  particular,  while  the  policing  actions  of  the  token 
bucket  are  expected  to  be  the  dominant  contributor  to  video 
quality,  there  are  other  factors  that  can  influence  token  bucket 
operations  and  introduce  additional  perturbations  to  a  video 
stream.    For  example,  interactions  with  cross  traffic  prior  to 
reaching  the  router  where  policing  actions  are  performed  can 
impact the number of frames that are found non-conformant and, 
therefore,  discarded  (this  jitter  effect  is  well-known  and  was  the 
motivation behind the introduction of cell delay variation tolerance 
in ATM [3]).  Similarly, the use of shapers5 as well as the traversal 
of  multiple  network  hops  can  affect  the  end-to-end  quality 
perceived  by  a  video  application.    As  a  result,  it  is  desirable  to 
carry  out  experiments  over  a  broad  range  of  configurations.   For 
that  purpose,  additional  tests  were  conducted  over  the  QBone;  a 
wide-area  testbed  implemented  across  the  Internet2  Abilene 
network that supports a service built on the EF PHB. The rest of 
this section is devoted to a brief overview of the two testbeds.   
3.2.1  Local Testbed  
The local testbed consists of three Diff-Serv enabled routers, 
three  workstations,  and  several  Ethernet  hubs  that  were  used  for 
local connectivity. A sample configuration of the different testbed 
elements  is  shown  below  in  Figure  4.  Routers  1  and  2  are 