title:Privacy Risks of Securing Machine Learning Models against Adversarial
Examples
author:Liwei Song and
Reza Shokri and
Prateek Mittal
Privacy Risks of Securing Machine Learning Models against
Adversarial Examples
Liwei Song
PI:EMAIL
Princeton University
Reza Shokri
PI:EMAIL
National University of Singapore
Prateek Mittal
PI:EMAIL
Princeton University
9
1
0
2
g
u
A
5
2
]
L
M
.
t
a
t
s
[
3
v
1
9
2
0
1
.
5
0
9
1
:
v
i
X
r
a
ABSTRACT
The arms race between attacks and defenses for machine learn-
ing models has come to a forefront in recent years, in both the
security community and the privacy community. However, one big
limitation of previous research is that the security domain and the
privacy domain have typically been considered separately. It is thus
unclear whether the defense methods in one domain will have any
unexpected impact on the other domain.
In this paper, we take a step towards resolving this limitation by
combining the two domains. In particular, we measure the success
of membership inference attacks against six state-of-the-art defense
methods that mitigate the risk of adversarial examples (i.e., evasion
attacks). Membership inference attacks determine whether or not
an individual data record has been part of a model’s training set.
The accuracy of such attacks reflects the information leakage of
training algorithms about individual members of the training set.
Adversarial defense methods against adversarial examples influence
the model’s decision boundaries such that model predictions remain
unchanged for a small area around each input. However, this objec-
tive is optimized on training data. Thus, individual data records in
the training set have a significant influence on robust models. This
makes the models more vulnerable to inference attacks.
To perform the membership inference attacks, we leverage the
existing inference methods that exploit model predictions. We also
propose two new inference methods that exploit structural prop-
erties of robust models on adversarially perturbed data. Our exper-
imental evaluation demonstrates that compared with the natural
training (undefended) approach, adversarial defense methods can
indeed increase the target model’s risk against membership inference
attacks. When using adversarial defenses to train the robust models,
the membership inference advantage increases by up to 4.5 times
compared to the naturally undefended models. Beyond revealing
the privacy risks of adversarial defenses, we further investigate
the factors, such as model capacity, that influence the membership
information leakage.
CCS CONCEPTS
• Security and privacy → Software and application security;
• Computing methodologies → Neural networks.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3354211
KEYWORDS
machine learning; membership inference attacks; adversarial exam-
ples and defenses
ACM Reference Format:
Liwei Song, Reza Shokri, and Prateek Mittal. 2019. Privacy Risks of Secur-
ing Machine Learning Models against Adversarial Examples. In 2019 ACM
SIGSAC Conference on Computer and Communications Security (CCS ’19),
November 11–15, 2019, London, United Kingdom. ACM, New York, NY, USA,
17 pages. https://doi.org/10.1145/3319535.3354211
1 INTRODUCTION
Machine learning models, especially deep neural networks, have
been deployed prominently in many real-world applications, such
as image classification [28, 49], speech recognition [11, 21], natural
language processing [2, 10], and game playing [35, 48]. However,
since the machine learning algorithms were originally designed
without considering potential adversarial threats, their security and
privacy vulnerabilities have come to a forefront in recent years,
together with the arms race between attacks and defenses [7, 22, 39].
In the security domain, the adversary aims to induce misclassifi-
cations to the target machine learning model, with attack methods
divided into two categories: evasion attacks and poisoning attacks
[22]. Evasion attacks, also known as adversarial examples, perturb
inputs at the test time to induce wrong predictions by the target
model [5, 8, 15, 38, 56]. In contrast, poisoning attacks target the
training process by maliciously modifying part of training data to
cause the trained model to misbehave on some test inputs [6, 27, 43].
In response to these attacks, the security community has designed
new training algorithms to secure machine learning models against
evasion attacks [16, 33, 34, 50, 61, 66] or poisoning attacks [24, 55].
In the privacy domain, the adversary aims to obtain private infor-
mation about the model’s training data or the target model. Attacks
targeting data privacy include: the adversary inferring whether in-
put examples were used to train the target model with membership
inference attacks [37, 47, 64], learning global properties of training
data with property inference attacks [12], or covert channel model
training attacks [52]. Attacks targeting model privacy include: the
adversary uncovering the model details with model extraction at-
tacks [58], and inferring hyperparameters with hyperparameter
stealing attacks [60]. In response to these attacks, the privacy com-
munity has designed defenses to prevent privacy leakage of training
data [1, 19, 36, 46] or the target model [26, 31].
However, one important limitation of current machine learning
defenses is that they typically focus solely on either the security
domain or the privacy domain. It is thus unclear whether defense
methods in one domain will have any unexpected impact on the
other domain. In this paper, we take a step towards enhancing our
understanding of machine learning models when both the security
(a) Adversarially robust model from Madry et al. [33], with 99% train
accuracy and 87% test accuracy.
(b) Naturally undefended model, with 100% train accuracy and 95%
test accuracy. Around 23% training and test examples have zero loss.
Figure 1: Histogram of CIFAR10 classifiers’ loss values of training data (members) and test data (non-members). We can see
the larger divergence between the loss distribution over members and non-members on the robust model as compared to the
natural model. This shows the privacy risk of securing deep learning models against adversarial examples.
domain and privacy domain are considered together. In particu-
lar, we seek to understand the privacy risks of securing machine
learning models by evaluating membership inference attacks against
adversarially robust deep learning models, which aim to mitigate the
threat of adversarial examples.
The membership inference attack aims to infer whether a data
point is part of the target model’s training set or not, reflecting the
information leakage of the model about its training data. It can also
pose a privacy risk as the membership can reveal an individual’s
sensitive information. For example, participation in a hospital’s
health analytic training set means that an individual was once a
patient in that hospital. It has been shown that the success of mem-
bership inference attacks in the black-box setting is highly related
to the target model’s generalization error [47, 64]. Adversarially
robust models aim to enhance the robustness of target models by
ensuring that model predictions are unchanged for a small area
(such as l∞ ball) around each input example. The objective is to
make the model robust against any input, however, the objective is
optimized only on the training set. Thus, intuitively, adversarially
robust models have the potential to increase the model’s general-
ization error and sensitivity to changes in the training set, resulting
in an enhanced risk of membership inference attacks. As an ex-
ample, Figure 1 shows the histogram of cross-entropy loss values
of training data and test data for both naturally undefended and
adversarially robust CIFAR10 classifiers provided by Madry et al.
[33]. We can see that members (training data) and non-members
(test data) can be distinguished more easily for the robust model,
compared to the natural model.
To measure the membership inference risks of adversarially ro-
bust models, besides the conventional inference method based on
prediction confidence, we propose two new inference methods that
exploit the structural properties of robust models. We measure the
privacy risks of robust models trained with six state-of-the-art ad-
versarial defense methods, and find that adversarially robust models
are indeed more susceptible to membership inference attacks than
naturally undefended models. We further perform a comprehen-
sive investigation to analyze the relation between privacy leakage
and model properties. We finally discuss the role of adversary’s
prior knowledge, potential countermeasures and the relationship
between privacy and robustness.
In summary, we make the following contributions in this paper:
(1) We propose two new membership inference attacks spe-
cific to adversarially robust models by exploiting adversarial
examples’ predictions and verified worst-case predictions.
With these two new methods, we can achieve higher in-
ference accuracies than the conventional inference method
based on prediction confidence of benign inputs.
(2) We perform membership inference attacks on models trained
with six state-of-the-art adversarial defense methods (3 em-
pirical defenses [33, 50, 66] and 3 verifiable defenses [16, 34,
61]). We demonstrate that all methods indeed increase the
model’s membership inference risk. By defining the member-
ship inference advantage as the increase in inference accu-
racy over random guessing (multiplied by 2) [64], we show
that robust machine learning models can incur a membership
inference advantage 4.5×, 2×, 3.5× times the membership
inference advantage of naturally undefended models, on Yale
Face, Fashion-MNIST, and CIFAR10 datasets, respectively.
(3) We further explore the factors that influence the member-
ship inference performance of the adversarially robust model,
including its robustness generalization, the adversarial per-
turbation constraint, and the model capacity.
(4) Finally, we experimentally evaluate the effect of the adver-
sary’s prior knowledge, countermeasures such as tempera-
ture scaling and regularization, and discuss the relationship
between training data privacy and model robustness.
Some of our analysis was briefly discussed in a short workshop
paper [53]. In this paper, we go further by proposing two new
membership inference attacks and measuring four more adversarial
defense methods, where we show that all adversarial defenses can
increase privacy risks of target models. We also perform a compre-
hensive investigation of factors that impact the privacy risks.
2 BACKGROUND AND RELATED WORK:
ADVERSARIAL EXAMPLES AND
MEMBERSHIP INFERENCE ATTACKS
In this section, we first present the background and related work on
adversarial examples and defenses, and then discuss membership
inference attacks.
Fθ(x) withk−1
2.1 Adversarial Examples and Defenses
Let Fθ : Rd → Rk be a machine learning model with d input
features and k output classes, parameterized by weights θ. For an
example z = (x, y) with the input feature x and the ground truth
label y, the model outputs a prediction vector over all class labels
i =0 Fθ(x)i = 1, and the final prediction will be the
label with the largest prediction probability ˆy = argmaxi Fθ(x)i.
For neural networks, the outputs of its penultimate layer are known
as logits, and we represent them as a vector дθ(x). The softmax
function is then computed on logits to obtain the final prediction
vector.
Fθ(x)i =
exp(дθ(x)i)
j=0 exp(дθ(x)j)
(1)
Given a training set Dtrain, the natural training algorithm aims to
make model predictions match ground truth labels by minimizing
the prediction loss over all training examples.
ℓ(Fθ , z),
(2)
1
min
θ
|Dtrain|
z∈Dtrain
where |Dtrain| denotes the size of training set, and ℓ computes the
prediction loss. A widely-adopted loss function is the cross-entropy
loss:
k−1

ℓ(Fθ , z) = − k−1
i =0
1{i = y} · log(Fθ(x)i),
(3)
where 1{·} is the indicator function.
2.1.1 Adversarial examples: Although machine learning models
have achieved tremendous success in many classification scenarios,
they have been found to be easily fooled by adversarial examples
[5, 8, 15, 38, 56]. Adversarial examples induce incorrect classifi-
cations to target models, and can be generated via imperceptible
perturbations to benign inputs.
Fθ(˜x)i (cid:44) y,
such that ˜x ∈ Bϵ(x),
argmax
(4)
where Bϵ(x) denotes the set of points around x within the per-
turbation budget of ϵ. Usually a lp ball is chosen as the perturba-
tion constraint for generating adversarial examples i.e., Bϵ(x) =
{x′ | ∥x′ − x∥p ≤ ϵ}. We consider the l∞-ball adversarial constraint
throughout the paper, as it is widely adopted by most adversarial
defense methods [16, 33, 34, 40, 50, 61, 66].
The solution to Equation (4) is called an “untargeted adversarial
example” as the adversarial goal is to achieve any incorrect classi-
fication. In comparison, a “targeted adversarial example” ensures
i
i
that the model prediction is a specified incorrect label y′, which is
not equal to y.
argmax
Fθ(˜x)i = y
′
,
such that ˜x ∈ Bϵ(x).
(5)
Unless otherwise specified, an adversarial example in this paper
refers to an untargeted adversarial example.
To provide adversarial robustness under the perturbation con-
straint Bϵ , instead of natural training algorithm shown in Equation
(2), a robust training algorithm is adopted by adding an additional
robust loss function.
α · ℓ(Fθ , z) + (1 − α) · ℓR(Fθ , z, Bϵ),
(6)

z∈Dtrain
1
min
θ
|Dtrain|
where α is the ratio to trade off natural loss and robust loss, and ℓR
measures the robust loss, which can be formulated as maximizing
prediction loss ℓ′ under the constraint Bϵ .
ℓR(Fθ , z, Bϵ) = max
˜x∈Bϵ(x) ℓ
′(Fθ ,(˜x, y))
(7)
ℓ′ can be same as ℓ or other appropriate loss functions.
However, it is usually hard to find the exact solution to Equation
(7). Therefore, the adversarial defenses propose different ways to
approximate the robust loss ℓR, which can be divided into two
categories: empirical defenses and verifiable defenses.
2.1.2 Empirical defenses: Empirical defense methods approximate
robust loss values by generating adversarial examples xadv at each
training step with state-of-the-art attack methods and computing
their prediction loss. Now the robust training algorithm can be
expressed as following.
1
|Dtrain|
min
θ
Three of our tested adversarial defense methods belong to this
α · ℓ(Fθ , z) + (1 − α) · ℓ
′(Fθ ,(xadv , y))
z∈Dtrain
(8)
category, which are described as follows.
PGD-Based Adversarial Training (PGD-Based Adv-Train) [33]:
Madry et al. [33] propose one of the most effective empirical defense
methods by using the projected gradient descent (PGD) method to
generate adversarial examples for maximizing cross-entropy loss
(ℓ′ = ℓ) and training purely on those adversarial examples (α = 0).
The PGD attack contains T gradient descent steps, which can be
expressed as

˜xt +1 = ΠBϵ(x)[˜xt + η · sign( ∇˜xt ℓ(Fθ ,(˜xt , y)))],
(9)
where ˜x0 = x, xadv = ˜xT , η is the step size value, ∇ denotes the
gradient computation, and ΠBϵ(x) means the projection onto the
perturbation constraint Bϵ(x).
Distributional Adversarial Training (Dist-Based Adv-Train)
[50]: Instead of strictly satisfying the perturbation constraint with
projection step ΠBϵ(x) as in PGD attacks, Sinha et al. [50] generate
adversarial examples by solving the Lagrangian relaxation of cross-
entropy loss:
max
˜x
ℓ(Fθ ,(˜x, y)) − γ ∥ ˜x − x∥p ,
(10)
where γ is the penalty parameter for the lp distance. A multi-step
gradient descent method is adopted to solve Equation (10). The
model will then be trained on the cross-loss entropy (ℓ′ = ℓ) of
adversarial examples only (α = 0).
Sinha et al. [50] derive a statistical guarantee for l2 distributional
robustness with strict conditions requiring the loss function ℓ to
be smooth on x, which are not satisfied in our setting. We mainly
use widely-adopted ReLU activation functions for our machine
learning models, which result in a non-smooth loss function. Also,
we generate adversarial examples with l∞ distance penalties by
using the algorithm proposed by Sinha et al. [50] in Appendix E,
where there is no robustness guarantee. Thus, we categorize the
defense method as empirical.
Difference-based Adversarial Training (Diff-Based Adv-Train)
[66]: Instead of using the cross-entropy loss of adversarial exam-
ples, with insights from a toy binary classification task, Zhang et
al. [66] propose to use the difference (e.g., Kullback-Leibler (KL)
divergence) between the benign output Fθ(x) and the adversarial