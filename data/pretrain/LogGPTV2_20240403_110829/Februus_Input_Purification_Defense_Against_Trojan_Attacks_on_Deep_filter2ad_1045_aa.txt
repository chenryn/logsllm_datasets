title:Februus: Input Purification Defense Against Trojan Attacks on Deep
Neural Network Systems
author:Bao Gia Doan and
Ehsan Abbasnejad and
Damith C. Ranasinghe
9
1
0
2
g
u
A
1
3
]
R
C
.
s
c
[
3
v
9
6
3
3
0
.
8
0
9
1
:
v
i
X
r
a
Februus: Input Puriﬁcation Defence Against
Trojan Attacks on Deep Neural Network
Systems
Bao Gia Doan, Ehsan Abbasnejad, Damith C. Ranasinghe
The University of Adelaide, Australia
Abstract
We propose Februus; a novel idea to neutralize insidous and
highly potent Trojan attacks on Deep Neural Network (DNN) sys-
tems at run-time. In Trojan attacks, an adversary activates a back-
door crafted in a deep neural network model using a secret trigger, a
Trojan, applied to any input to alter the model’s decision to a target
prediction—a target determined by and only known to the attacker.
Februus sanitizes the incoming input by devising an extraction method
to surgically remove the potential trigger artifacts and use an inpaint-
ing method we propose for restoring the input for the classiﬁcation
task. Through extensive experiments, we demonstrate the eﬃcacy
of Februus against backdoor attacks, including advance variants and
adaptive attacks, across vision applications. Notably, in contrast to
existing approaches, our approach removes the need for ground-truth
labelled data or anomaly detection methods for Trojan detection or
retraining a model or prior knowledge of an attack. We achieve dra-
matic reductions in the attack success rates; from 100% to 0.25% (in
the worst case) with no loss of performance for benign or trojaned
inputs sanitized by Februus. To the best of our knowledge, this is
the ﬁrst backdoor defense method for operation in black-box setting
capable of sanitizing trojaned inputs without requiring costly labelled
data.
1
1
Introduction
Machine Learning (ML), especially the Deep Learning (DL), has been de-
ployed in various critical tasks recently in computer vision, robotics, and
natural language processing [7]. Nevertheless, the trustworthy decisions of
those DL systems have been concerned in recent works [8, 11, 12]. On one
hand, due to the lack of computational resources to train DL networks, DL
researchers and practitioners and practitioners recently rely on transfer learn-
ing or Machine Learning as a Service (MLaaS) [1]. In the transfer learning,
DL practitioners need to re-utilize an untrusted pre-trained model which
could be potentially poisoned as shown in recent work [8]. This kind of shar-
ing and reusing model is commonly applied nowadays [3]. Furthermore, in
MLaaS they need to outsource their training process to a third-party which
can manipulate all of the training process [1]. On the other hand, with
millions of parameters inside a deep learning model, it is highly diﬃcult to
reason or explain the decision made by a neural network. Normally, the only
measurement and validation from DL practitioners is the accuracy, which
makes backdoor attack become a security threat as the performance of the
poisoned model is identical with the benign one when the trojan trigger is
absent.
Figure 1: The stop sign is mis-classiﬁed as a speed-limit using a sticker as
the trigger through Backdoor Attack [8]
With this recent trend of training Deep Neural Networks (DNNs), Deep
2
Leaning based applications potentially face the security threats of trojaned or
backdoor attacks in which trojaned networks will have a malicious behavior
when the trigger designed by attackers is presented [2, 8]. One distinctive
feature of backdoor attack is that the attackers can choose any shapes and
sizes or features of the trigger, which makes it diﬀerent from adversarial
samples of evasion attack shown in ICLR work [15] where the adversaries
need to be crafted dependent on the network architecture. This makes the
backdoor attack more physical and deployable in the real-world scenario.
For example, attackers can choose sunglasses as in 2017 UC Berkeley trojan
attack [4] as the trigger of a backdoor which is hardly noticeable in a real-
world scenario, or stickers attached to T-shirts that can misclassify the face
recognition system. Generally, the trigger is hardly detected or recognized
by human beings.
In this paper, we will focus on the vision system where the backdoor
attacks pose several security threats to real-world applications such as traﬃc
sign recognition or object identiﬁcation. Those can lead to high-security
threat impacting human life where traﬃc sign recognition applied in a self-
driving car which can be misled the Stop sign to speed limit for instance (as
shown in Figure 1).
Detection is challenging The backdoor attack is stealthy because the
DL model will behave abnormally if and only if the designed trigger appears
while functioning properly in all other cases. This stealthiness makes the
backdoor attack challenging to be detected. Furthermore, the triggers could
be at any shapes and sizes chosen by attackers requiring a method that
can adapt well and robust enough to detect those triggers. In addition, the
purpose of the trojaned network is to keep the performance of the network
identical with the benign network but get malicious behavior when the trigger
appears, which is challenging to detect whether the network we are using was
trojaned or not.
Our paper will investigate the following question:
Is there any leaked information in an input-agnostic backdoor
attack that can be exploited via side channels for defense?
In this paper, we focus on creating physical and realistic examples of
In addition, we deal with the problem of
trojan placement and methods.
3
allowing time-bound systems to react to trojan inputs where detection and
discarding is often not an option. We also focus on the input-agnostic attack
which currently is dominant in the backdoor attack. Input-agnostic attack
means that the trigger will operate regardless of the source classes of the
inputs, inputs from any source classes with the malicious pattern will trigger
the backdoor of the poisoned network. Furthermore, we will also investigate
advanced backdoor variants such as larger triggers or diﬀerent scenarios of
source-class backdoor attacks.
1.1 Our Contributions and Results:
We reveal that the strong eﬀect of trojan is indeed a weakness that leaks
information in feature maps that can be detected under DNN visual expla-
nation. The stronger the trigger is, the easier it is to be detected. In this
paper, we present a single complete framework named Februus that can be
plugged and played in any vision systems to detect and ﬁlter out the trojans
in run-time. Our framework can detect the presence of the trigger and san-
itize the input to the degree that the trojan eﬀects will be eliminated and
the network still correctly identify the trojaned inputs.
In the meantime,
the accuracy of the clean inputs is still identical to the benign network’s
performance.
We summarize our contributions as below:
• To the best of our knowledge, we are the ﬁrst to propose unsuper-
vised input sanitization in trojaned deep neural networks that can still
correctly identify trojaned inputs using image inpainting.
• We create a system that is run-time, online and cleanses Trojan in-
puts automatically in a black-box setting without the knowledge of the
network or trojan information.
• We propose a defense method that does not require re-training the tro-
janed network and work on cheaply unlabeled data to defense against
trojan attacks.
• We demonstrate that our method is robust to backdoor attacks on
diﬀerent classiﬁcation tasks such as object classiﬁcation (CIFAR10) and
Traﬃc Sign Recognition (GTSRB) with the attack success rate reduced
from 100% to below 0.25%.
4
2 Background: Backdoor Neural Networks
Firstly, we will begin with some background knowledge on Deep Neural Net-
works which is mostly used in our work.
Taking an input x ∈ RN , a Deep Neural Network is a parameterized
function FΘ : RN → RM that map x to y ∈ RM in which Θ are the function’s
parameters. Input x could be an image, and output y in the case of image
classiﬁcation is the probability vector over m classes.
DNN is structured with L hidden layers inside, and each layer i ∈ [1, L]
has Ni neurons. Outputs of those neurons are called activations denoted as
ai ∈ RNi, and formulated as follows
(1)
where φ is a non-linear function, ωi are weights at that layer, ωi ∈ RNi−1×
ai = φ(ωiai−1 + bi) ∀i ∈ [1, L]
RNi, and bi are ﬁxed bias bi ∈ RNi
Weights and biases belong to network parameters Θ while other param-
eters such as the number of hidden layer L, number of neurons in each layer
Ni or non-linear activation function φ are called hyper-parameters.
σ(ωL+1aL + bL+1) where σ : RM → RM normally is the softmax function.
The output of the network is the last layer’s activation function y =
ezj(cid:80)M
σ(z)j =
(2)
m=1 ezm
for j = 1, ..., M and z = (z1, ..., zM ) ∈ RM
One special type of DNN is Convolutional Neural Network (CNN) [7]
which is widely used in computer vision and pattern recognition task.
In
CNN network, besides fully connected layers, it contains convolutional layers
which are in 3D volumes, and each activation of a neuron in CNN layer is
determined by a subset of neurons in the previous layers computed by a 3D
matrix of weights known as a ﬁlter. There will be the same ﬁlter for each
channel, and Ni ﬁlters will be needed for Ni channels at i convolutional layer.
To train a DNN network, we need to determine the hyper-parameters
(such as the architecture type, number of L hidden layers) as well as network
parameters (weights and biases). Taking image classiﬁcation as an example,
we need to have a training dataset of image inputs knowing the ground-
truth labels. Noting the training dataset as Dtrain = {xt
i}N
i=1 of N inputs,
i ∈ [1, M ]. The training process is trying
i ∈ RN , and ground-truth labels yt
xt
i, yt
5
to determine the distances between the predictions of inputs and the ground-
truth labels, these distances are measured by a loss function L. The learning
algorithm will return Θ∗ such that
N(cid:88)
L(FΘ(xt
i, yt
i))
(3)
Θ∗ = argmin
Θ
i=1
To verify the network, a separated validation set of V inputs with their
i }V
ground-truth labels Dvalidation = {xv
i=1 will be used. In most cases, this
is the only requirement that most Deep Learning practitioners care of, and
it would lead to security threats of backdoor attacks which will be discussed
in Section 2.1.
i , yv
2.1 Backdoor Attacks
DL training requires a huge amount of labeled data to get an acceptable error.
However, there are only a few people who can have access to those costly
labeled data, while the demand for AI applications and DL is enormous.
Transfer learning is one of the commonly popular methods applied in this
case when users have a limitation of labeled data. However, using transfer
learning on a pre-trained model could bring a great number of security threats
that users might not aware of. In recent works, authors [8,11] have shown that
backdoor attacks can be applied in transfer learning leading to the security
threats for those who are relying on transfer learning.
Not only big data, but huge computational power consumption is also a
challenge for users to train a DNN. Therefore, one solution arises recently
which is Machine Learning as a Service (MLaaS) where users outsource their
speciﬁcations of their DNN to a third-party service. Normally, the only mea-
surement in their speciﬁcation is the accuracy of the model. As shown in
backdoor papers [4,8,11], the backdoor attack methods can achieve identical
or even better accuracy than the clean network which satisﬁed the speciﬁ-
cation of the user, while embedding malicious trojan that can be activated
when the pattern trigger is presented.
3 Threat Model and Terminology
In our paper, we consider an adversary who wants to manipulate the DL
model to misclassify any inputs into a targeted class when the backdoor
6
trigger is presented, while keeping the normal behavior with all other kinds
of inputs. This backdoor can help attackers to impersonate someone with
higher privileges in face recognition system or can mislead the self-driving
car to a speciﬁc target identiﬁed by attackers. Identical to the approach of
recent papers [5, 6, 16], we focus on input-agnostic attacks while the trig-
ger will misclassify any inputs to a targeted class regardless input sources
(illustrated in Figure 2). We also assume that attacking is pretty strong
with white-box access and have full control of the training process to gener-
ate a strong backdoor, which is relevant to the current situation of popular
pre-trained models and MLaaS. Besides, the trigger types, shapes, and sizes
would also be chosen arbitrarily by attackers. The adversary holds the full
power to train the poisoned model from MLaaS or publishing their poisoned
pre-trained models online. Particularly, the adversary will poison a given
dataset Dtrain by inserting a portion of poisoned inputs yielding the poisoned
model θP (cid:54)= θ (benign model). This poisoned model will behave normally in