::: section
::: titlepage
# []{#enabling-pnfs-scsi-layouts-in-nfs_managing-file-systems.html#monitoring-pnfs-scsi-layouts-functionality_enabling-pnfs-scsi-layouts-in-nfs}监控 pNFS SCSI 布局功能 {.title}
:::
您可以监控 pNFS 客户端和服务器是否交换正确的 pNFS SCSI
操作，或者它们是否回退到常规 NFS 操作。
::: itemizedlist
**先决条件**
-   配置了 pNFS SCSI 客户端和服务器。
:::
::: section
::: titlepage
## []{#enabling-pnfs-scsi-layouts-in-nfs_managing-file-systems.html#checking-pnfs-scsi-operations-from-the-server-using-nfsstat_monitoring-pnfs-scsi-layouts-functionality}使用 nfsstat 从服务器检查 pNFS SCSI 操作 {.title}
:::
这个过程使用 `nfsstat`{.literal} 工具来监控来自服务器的 pNFS SCSI 操作。
::: orderedlist
**流程**
1.  监控服务器中服务的操作：
    ``` screen
    # watch --differences \
            "nfsstat --server | egrep --after-context=1 read\|write\|layout"
    Every 2.0s: nfsstat --server | egrep --after-context=1 read\|write\|layout
    putrootfh    read         readdir      readlink     remove	 rename
    2         0% 0         0% 1         0% 0         0% 0         0% 0         0%
    --
    setcltidconf verify	  write        rellockowner bc_ctl	 bind_conn
    0         0% 0         0% 0         0% 0         0% 0         0% 0         0%
    --
    getdevlist   layoutcommit layoutget    layoutreturn secinfononam sequence
    0         0% 29        1% 49        1% 5         0% 0         0% 2435     86%
    ```
2.  客户端和服务器在以下情况下使用 pNFS SCSI 操作：
    ::: itemizedlist
    -   layout `get`{.literal}、debug`return`{.literal} 和 layout
        `commit`{.literal} 计数器递增。这意味着服务器提供布局。
    -   服务器 `读写`{.literal} ``{.literal}
        计数器不会递增。这意味着客户端正在直接向 SCSI 设备执行 I/O
        请求。
    :::
:::
:::
::: section
::: titlepage
## []{#enabling-pnfs-scsi-layouts-in-nfs_managing-file-systems.html#checking-pnfs-scsi-operations-from-the-client-using-mountstats_monitoring-pnfs-scsi-layouts-functionality}使用 mountstats 检查客户端中的 pNFS SCSI 操作 {.title}
:::
这个过程使用 `/proc/self/mountstats`{.literal} 文件来监控来自客户端的
pNFS SCSI 操作。
::: orderedlist
**流程**
1.  列出每个挂载的操作计数器：
    ``` screen
    # cat /proc/self/mountstats \
          | awk /scsi_lun_0/,/^$/ \
          | egrep device\|READ\|WRITE\|LAYOUT
    device 192.168.122.73:/exports/scsi_lun_0 mounted on /mnt/rhel7/scsi_lun_0 with fstype nfs4 statvers=1.1
        nfsv4:  bm0=0xfdffbfff,bm1=0x40f9be3e,bm2=0x803,acl=0x3,sessions,pnfs=LAYOUT_SCSI
                READ: 0 0 0 0 0 0 0 0
               WRITE: 0 0 0 0 0 0 0 0
            READLINK: 0 0 0 0 0 0 0 0
             READDIR: 0 0 0 0 0 0 0 0
           LAYOUTGET: 49 49 0 11172 9604 2 19448 19454
        LAYOUTCOMMIT: 28 28 0 7776 4808 0 24719 24722
        LAYOUTRETURN: 0 0 0 0 0 0 0 0
         LAYOUTSTATS: 0 0 0 0 0 0 0 0
    ```
2.  在结果中：
    ::: itemizedlist
    -   `LAYOUT`{.literal} 统计数据指示客户端和服务器使用 pNFS SCSI
        操作的请求。
    -   `READ`{.literal} 和 `WRITE`{.literal}
        统计指示客户端和服务器回退到 NFS 操作的请求。
    :::
:::
:::
:::
:::
[]{#getting-started-with-fs-cache_managing-file-systems.html}
::: chapter
::: titlepage
# []{#getting-started-with-fs-cache_managing-file-systems.html#getting-started-with-fs-cache_managing-file-systems}第 7 章 FS-Cache 入门 {.title}
:::
FS-Cache
是一种持久的本地缓存，文件系统可以使用它从网络检索数据并将其缓存在本地磁盘上。这有助于最小化网络流量，以便用户从通过网络（例如
NFS）挂载的文件系统访问数据。
::: section
::: titlepage
# []{#getting-started-with-fs-cache_managing-file-systems.html#overview-of-the-fs-cache_getting-started-with-fs-cache}FS-Cache 概述 {.title}
:::
下图显示了 FS-Cache 的工作原理：
::: figure
[]{#getting-started-with-fs-cache_managing-file-systems.html#fig-fscachemain}
**图 7.1. FS-Cache 概述**
::: figure-contents
::: mediaobject
![FS-Cache 概述](images/fs-cache.png)
:::
:::
:::
FS-Cache 旨在对系统的用户和管理员尽可能透明。与 Solaris 上的
`cachefs`{.literal} 不同，FS-Cache
允许服务器上的文件系统直接与客户端的本地缓存交互，而不创建过载的文件系统。使用
NFS 时，挂载选项指示客户端在启用了 FS-cache 时挂载 NFS
共享。挂载点将导致自动上传两个内核模块：fs `cache`{.literal} 和
`cachefile`{.literal}。`cachefilesd`{.literal}
守护进程与内核模块通信以实施缓存。
FS-Cache
不会改变通过网络运行的文件系统的基本操作，它只是为文件系统提供了一个永久位置，它可以缓存数据。例如，客户端仍然可以挂载
NFS 共享，无论是否启用了 FS-Cache。此外，缓存的 NFS
可以处理由于文件可以部分缓存且不必完全预先读取的文件，因此缓存的 NFS
可以处理不适合于缓存的文件（无论是单独还是全部）。FS-Cache
还会从客户端文件系统驱动程序隐藏缓存中发生的所有 I/O 错误。
要提供缓存服务，FS-Cache 需要
[*缓存后端*]{.emphasis}。缓存后端是配置为提供缓存服务的存储驱动程序，即
`cachefile`{.literal}。在这种情况下，FS-Cache
需要一个挂载的基于块的文件系统，支持 `bmap`{.literal} 和扩展属性（例如
ext3）作为其缓存后端。
支持 FS-Cache 缓存后端所需的功能的文件系统包括以下文件系统的 Red Hat
Enterprise Linux 8 实现：
::: itemizedlist
-   ext3（启用了扩展属性）
-   ext4
-   XFS
:::
FS-Cache
无法任意缓存任何文件系统，不论是通过网络还是通过其他方式：必须更改共享文件系统的驱动程序，以允许与
FS-Cache、数据存储/retrieval 以及元数据设置和验证交互。FS-Cache
需要来自缓存文件系统 [*的索引密钥*]{.emphasis}
[*和一致性数据*]{.emphasis}
来支持持久性：索引密钥以匹配文件系统对象来缓存对象，以及一致性数据来确定缓存对象是否仍然有效。
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
在 Red Hat Enterprise Linux 8 中，默认情况下不会安装
[**[cachefilesd]{.package}**]{.strong} 软件包，需要手动安装。
:::
:::
::: section
::: titlepage
# []{#getting-started-with-fs-cache_managing-file-systems.html#performance-guarantee_getting-started-with-fs-cache}性能保证 {.title}
:::
FS-Cache
[*不*]{.emphasis}保证更高的性能。使用缓存会降低性能：例如，缓存的 NFS
共享会为跨网络查找添加磁盘访问。虽然 FS-Cache
尝试尽可能异步，但存在无法达到的同步路径（如读取）。
例如，使用 FS-Cache 通过其他不从属的 GigE 网络在两台计算机之间缓存 NFS
共享可能不会显示文件访问的性能改善。相反，通过服务器内存而不是本地磁盘满足
NFS 请求的速度会更快。
因此，使用 FS-Cache 是各种因素之间的 [*妥协*]{.emphasis}。例如，如果使用
FS-Cache 来缓存 NFS
流量，它可能会减慢客户端一些速度，但会满足本地的读取请求而大规模降低网络和服务器加载，而无需消耗网络带宽。
:::
::: section
::: titlepage
# []{#getting-started-with-fs-cache_managing-file-systems.html#setting-up-a-cache_getting-started-with-fs-cache}设置缓存 {.title}
:::
目前，Red Hat Enterprise Linux 8 只提供 `cachefiles`{.literal}
缓存后端。`cachefilesd`{.literal} 守护进程启动和管理
`cachefile`{.literal}。`/etc/cachefilesd.conf`{.literal} 文件控制
`cachefile 如何提供`{.literal} 缓存服务。
缓存后端的工作原理是在托管缓存的分区上维护一定数量的可用空间。它使用可用空间扩展和缩小缓存以响应系统的其他元素，从而安全地在根文件系统中使用（例如，在便携式计算机中使用）。FS-Cache
在此行为上设置默认值，可以通过 [*cache cull 限制*]{.emphasis}
进行配置。有关配置缓存 cull 限制的详情请参考 ["cache cull
限制配置"一节](#getting-started-with-fs-cache_managing-file-systems.html#cache-cull-limits-configuration_getting-started-with-fs-cache "cache cull 限制配置"){.xref}。
这个过程演示了如何设置缓存。
::: itemizedlist
**先决条件**
-   已 [**[安装]{.package}**]{.strong} cachefilesd
    软件包，且服务已成功启动。要确定该服务正在运行，请使用以下命令：
    ``` literallayout
    # systemctl start cachefilesd
    # systemctl status cachefilesd
    ```
    状态必须 [*处于活动状态（正在运行*]{.emphasis} ）。
:::
::: orderedlist
**流程**
1.  在缓存后端中配置要用作缓存的目录，使用以下参数：
    ``` literallayout
    $ dir /path/to/cache
    ```
2.  通常，缓存后端目录在 `/etc/cachefilesd.conf`{.literal} 中被设置为
    `/var/cache/fscache`{.literal}，如下所示：
    ``` literallayout
    $ dir /var/cache/fscache
    ```
3.  如果要更改缓存后端目录，selinux 上下文必须与
    `/var/cache/fscache`{.literal} 相同：
    ``` literallayout
    # semanage fcontext -a -e /var/cache/fscache /path/to/cache
    # restorecon -Rv /path/to/cache
    ```
4.  设置缓存时，将 [*/path/to/cache*]{.emphasis} 替换为目录名称。
5.  如果给定的用于设置 selinux
    上下文的命令无法正常工作，请使用以下命令：
    ``` literallayout
    # semanage permissive -a cachefilesd_t
    # semanage permissive -a cachefiles_kernel_t
    ```
    FS-Cache 会将缓存存储在托管 `/path/to/cache`{.literal}
    的文件系统中。在便携式计算机中，建议使用 root
    文件系统(`/`{.literal})作为主机文件系统，但对于桌面计算机而言，挂载专门用于缓存的磁盘分区更为明智。
6.  主机文件系统必须支持用户定义的扩展属性；FS-Cache
    使用这些属性来存储一致的维护信息。要为 ext3 文件系统（例如
    `设备`{.literal}）启用用户定义的扩展属性，请使用：
    ``` literallayout
    # tune2fs -o user_xattr /dev/device
    ```
7.  要在挂载时为文件系统启用扩展属性，作为替代方案，请使用以下命令：
    ``` literallayout
    # mount /dev/device /path/to/cache -o user_xattr
    ```
8.  配置文件就位后，启动 `cachefilesd`{.literal} 服务：
    ``` literallayout
    # systemctl start cachefilesd
    ```
9.  要将 `cachefilesd`{.literal} 配置为在引导时启动，以 root
    用户身份执行以下命令：
    ``` literallayout
    # systemctl enable cachefilesd
    ```
:::
:::
::: section
::: titlepage
# []{#getting-started-with-fs-cache_managing-file-systems.html#using-the-cache-with-nfs_getting-started-with-fs-cache}在 NFS 中使用缓存 {.title}
:::
除非明确指示，否则 NFS 将不会使用缓存。本段介绍如何使用 FS-Cache 配置
NFS 挂载。
::: itemizedlist
**先决条件**
-   [**[cachefilesd]{.package}**]{.strong}
    软件包已安装并在运行。要确保它正在运行，请使用以下命令：
    ``` literallayout
    # systemctl start cachefilesd
    # systemctl status cachefilesd
    ```
    状态必须 [*处于活动状态（正在运行*]{.emphasis} ）。
-   使用以下选项挂载 NFS 共享：
    ``` literallayout
    # mount nfs-share:/ /mount/point -o fsc
    ```
    对 `/mount/point`{.literal}
    下文件的所有访问权限都将通过缓存，除非打开该文件以进行直接 I/O
    或写入。如需更多信息，请参阅 ["NFS
    的缓存限制"一节](#getting-started-with-fs-cache_managing-file-systems.html#cache-limitations-with-nfs_using-the-cache-with-nfs "NFS 的缓存限制"){.xref}。NFS
    使用 NFS 文件句柄 [*而非*]{.emphasis}
    文件名来索引缓存内容，这意味着硬链接式文件正确共享缓存。
:::
NFS 版本 3、4.0、4.1 和 4.2
支持缓存。但是，每个版本使用不同的分支进行缓存。
::: section
::: titlepage
## []{#getting-started-with-fs-cache_managing-file-systems.html#configuring-nfs-cache-sharing_using-the-cache-with-nfs}配置 NFS 缓存共享 {.title}
:::
与 NFS
缓存共享相关的一些潜在问题。因为缓存是持久的，所以缓存中的数据块会根据由四个键组成的序列来索引：
::: itemizedlist
-   第 1 级：服务器详情
-   第 2 级：某些挂载选项：安全类型；FSID；单要求
-   第 3 级：文件处理
-   第 4 级：文件中的页号
:::
为避免超级块之间保持一致管理问题，缓存数据的所有 NFS
超级块具有唯一的级别 2 密钥。通常，两个 NFS
挂载使用相同的源卷和选项共享超级块，因此共享缓存，即使它们在该卷中挂载不同的目录。