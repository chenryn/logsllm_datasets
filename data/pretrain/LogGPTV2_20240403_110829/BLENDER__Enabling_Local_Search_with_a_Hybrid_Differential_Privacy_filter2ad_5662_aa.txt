title:BLENDER: Enabling Local Search with a Hybrid Differential Privacy
Model
author:Brendan Avent and
Aleksandra Korolova and
David Zeber and
Torgeir Hovden and
Benjamin Livshits
BLENDER: Enabling Local Search with  
a Hybrid Differential Privacy Model
Brendan Avent and Aleksandra Korolova, University of Southern California;  
David Zeber and Torgeir Hovden, Mozilla; Benjamin Livshits, Imperial College London
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/avent
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXBLENDER: Enabling Local Search with
a Hybrid Diﬀerential Privacy Model
Brendan Avent
Aleksandra Korolova
University of Southern California
University of Southern California
David Zeber
Mozilla
Torgeir Hovden
Mozilla
Benjamin Livshits
Imperial College London
Abstract
We propose a hybrid model of diﬀerential privacy
that considers a combination of regular and opt-in
users who desire the diﬀerential privacy guarantees
of the local privacy model and the trusted curator
model, respectively. We demonstrate that within
this model, it is possible to design a new type of
blended algorithm for the task of privately comput-
ing the most popular records of a web search log.
This blended approach provides signiﬁcant improve-
ments in the utility of obtained data compared to
related work while providing users with their desired
privacy guarantees. Speciﬁcally, on two large search
click data sets comprising 4.8 million and 13.2 mil-
lion unique queries respectively, our approach at-
tains NDCG values exceeding 95% across a range
of commonly used privacy budget values.
1 Introduction
Now more than ever we are confronted with the ten-
sion between collecting mass-scale user data and the
ability to release or share this data in a way that pre-
serves the privacy of individual users. Today, an or-
ganization that needs user data to improve the qual-
ity of service they provide often has no choice but
to perform the data collection themselves. However,
the users may not want to share their data with the
organization, especially if they consider the data to
be sensitive or private. Similarly, the organization
assumes liability by collecting sensitive user data:
private information may be directly leaked through
security breaches or subpoenas, or indirectly leaked
by the output of computations done on the data.
Thus, both organizations and users would beneﬁt
not only from strong, rigorous privacy guarantees
regarding the data collection process, but also from
the organization collecting the minimum amount of
data necessary to achieve their goal. Some of the
philosophy behind our work stems from a desire to
enable privacy-preserving decentralized data collec-
tion that aggregates data from multiple entities into
high quality datasets.
1.1 Diﬀerential Privacy and Curator Models
In the last decade, we have witnessed scores of ad-
hoc approaches that have turned out to be inade-
quate for protecting privacy [33, 23]. The problem
stems from the impossibility of foreseeing all attacks
of adversaries capable of utilizing outside knowledge.
Diﬀerential privacy [10, 9, 11], which has become
the gold standard privacy guarantee in the academic
literature, and is gaining traction in industry and
government [13, 17, 28], overcomes the prior issues
by focusing on the privatization algorithm applied
to the data, requiring that it preserves privacy in a
mathematically rigorous sense under an assumption
of an omnipotent adversary.
There are two primary models in the diﬀerential
privacy framework that deﬁne how data is to be han-
dled by the users and data collectors: the trusted
curator model and the local model.
Trusted curator model: Most diﬀerentially pri-
vate algorithms developed to date operate in the
trusted curator model: all users’ data is collected
by the curator before privatization techniques are
applied to it. In this model, although users are guar-
anteed that the released data set protects their pri-
vacy, they must be willing to share their private,
unperturbed data with the curator and trust that
the curator properly performs a privacy-preserving
perturbation.
Local model: As was most recently argued by Ap-
ple [17], users may not trust the data collector with
their data, and may prefer privatization to occur be-
fore their data reaches the collector. Since privati-
zation occurs locally, this is known as the local dif-
ferential privacy (LDP) model, or local model. Over
USENIX Association
26th USENIX Security Symposium    747
the last several years, we have seen some examples
of the local model beginning to be used for data col-
lection in practice, most notably in the context of
the Chrome web browser [13] and Apple’s data col-
lection [17].
In the LDP model, a data collector such as Google
or Apple obtains insights into the data without ob-
serving the exact values of user’s private data. This
is achieved by applying a privacy-preserving pertur-
bation to each user’s private data before it leaves
the user’s device. Since most people do not trust
web companies with maintaining the privacy and se-
curity of their data [29], the minimal trust required
of users towards the data collector is a very attrac-
tive property of the LDP model. This approach pro-
tects not only the individual users, but also the data
collector from the possible privacy breaches. For
these reasons, the local model directly embodies the
“data minimization” principle described in the White
House’s 2012 consumer data privacy report [41].
Although it may seem counter-intuitive, it is pos-
sible to obtain useful insights even when the data
collector does not have access to the original data
and receives only data that has already been locally
privatized. Suppose a data collector wants to deter-
mine the proportion of the population that is HIV-
positive. The local privatization algorithm works
as follows: each person contributing data secretly
ﬂips a coin.
If the coin lands heads, they report
their true HIV status; otherwise, they report a sta-
tus at random. This algorithm, known as random-
ized response [40], guarantees each person plausible
deniability and is diﬀerentially private. Since the
randomness is incorporated into the algorithm in a
precisely speciﬁed way, the data collector is able to
recover an accurate estimate of the true proportion
of HIV-positive people if enough people contribute
their locally privatized data.
Diﬀerential privacy: Formally, an algorithm A
is (, δ)-diﬀerentially private [11] if and only if for
all neighboring databases D and D(cid:48) diﬀering in pre-
cisely one user’s data, the following inequality is sat-
isﬁed for all possible sets of outputs Y ⊆ Range(A):
Pr[A(D) ∈ Y ] ≤ e Pr[A(D(cid:48)) ∈ Y ] + δ.
The deﬁnition of what it means for an algorithm
to preserve diﬀerential privacy is the same for both
the trusted curator model and the local model. The
only distinction is in the timing of when the pri-
vacy perturbation needs to be applied – in the lo-
cal model, the data needs to undergo a privacy-
preserving perturbation before it is sent to the ag-
gregator, whereas in the trusted curator model the
aggregator may ﬁrst collect all the data, and then
apply a privacy-preserving perturbation. The timing
distinction leads to diﬀerences in what is meant by
“neighboring databases” in the deﬁnition and which
algorithms are analyzed. In the local model, D rep-
resents data of a single user and D(cid:48) represents data
of the same user, with possibly changed values. In
the trusted curator model, D represents data of all
users and D(cid:48) represents data of all users, except val-
ues of one of the user’s data may be altered.
Current diﬀerential privacy literature
considers the trusted curator model
and the local model entirely indepen-
dently. Our goal is to show that there
is much to be gained by combining
the two.
Hybrid model: Much of the contribution in this
paper stems from our observation that the two mod-
els can co-exist. As others have observed [2, 1,
7], people’s attitudes toward privacy vary widely.
Speciﬁcally, some users may be comfortable with
sharing their data with a trusted curator.
Many companies rely on a group of beta testers
with whom they have higher levels of mutual trust.
It is not uncommon for such beta testers to vol-
untarily opt-in to a less privacy-preserving model
than that of an average end-user [32]. For exam-
ple, Mozilla warns potential beta users of its Fire-
fox browser that “Pre-release versions automatically
send Telemetry data to Mozilla to help us improve
Firefox1”; Google has a similar provision for the beta
testers of the Canary build of the Chrome browser2.
For the users who have higher trust in the com-
pany — we call them the opt-in group, the trusted
curator privacy model is a natural match. For all
other users — we call them clients, the local pri-
vacy model is appropriate. Our goal is to demon-
strate that by separating the user pool into these
two groups, according to their trust (or lack thereof)
in the data aggregator, we can improve the utility of
the collected data. We dub this new model the hy-
brid diﬀerential privacy model.
1.2 Applications
Heavy hitter discovery and estimation is a well-
studied problem in the context of information re-
trieval, and is one of the canonical problems in
privacy-preserving data analysis [6, 27]. Moreover,
recent work in the LDP model is focused on pre-
cisely that problem [13, 34] or very closely related
ones of histogram computations [5, 21]. However,
current privacy-preserving approaches in the LDP
model lead to utility losses that are quite signiﬁ-
cant, sometimes to the point where results are no
748    26th USENIX Security Symposium
USENIX Association
longer usable. Clearly, if the privacy-preserving per-
turbation makes the data deviate too far from the
original, the approach will not be widely adopted.
This is especially true in the context of search tasks,
where users have been conditioned for years to ex-
pect high-quality results.
We consider two speciﬁc applications in the space
local search and search
of heavy hitter estimation:
trend computation.
Local search: Much of the work in this paper is
motivated by local search, an application of heavy
hitter estimation. Local search revolves around the
problem of how a browser maker can collect informa-
tion about users’ clicks as they interact with search
engines in order to create the head of the search, i.e.,
the collection of the most popular queries and their
corresponding URLs, and make it available to users
locally, i.e., on their devices. Speciﬁcally, it involves
computing on query-URL pairs, where the URLs are
those clicked as a result of submitting the query and
receiving a set of answers.
A browser maker may choose to combine the re-
sults obtained from user interactions that stem from
several search engines depending on the context or
surface results obtained from Baidu and not Bing
depending on the user’s current geographic location.
With proper privacy measures in place, this data
set can be deployed in the end-user browser to serve
the most common queries with a very low latency
or in situations when the user is disconnected from
the network. Local search can be thought of as a
form of caching, where many queries are answered
in a manner that does not require a round trip to
the server. Such caching of the most frequently used
queries locally has a disproportionately positive im-
pact on the expected query latency [36, 3] as queries
to a search engine follow a power-law distribution [4].
Furthermore, it would not be unusual or require a
signiﬁcantly novel infrastructure, as plenty of data is
delivered to the browser today, such as SafeBrowsing
malware databases in Chrome and Firefox, Microsoft
SmartScreen data in Internet Explorer, blocking lists
for extensions such as AdBlock Plus, etc.
Trend computation: Search trend computation is
a typical example of heavy hitter estimation. This
problem entails ﬁnding the most popular queries and
sorting them in order of popularity; think about it as
the top-10 computation based on local search obser-
vations. An example of this is the Google trends ser-
vice3, which has an always up-to-date list of trending
topics and queries.
Although trend computation is interesting, local
search is a great deal harder to do well on while
preserving most of the utility. Luckily, in the domain
of search quality, there are established metrics to
numerically assess the quality of search results; one
of such metrics is NDCG, and we rely on it heavily in
assessing the performance of our proposed system.
1.3 Contributions
Our paper makes the following contributions:
• We introduce and utilize a more realistic, hy-
brid trust model, which removes the need for
all users to trust a central curator.
• We propose Blender, an algorithm that oper-
ates with the hybrid diﬀerential privacy model
for computing heavy hitters. Blender blends
the data of opt-in and all other users in order
to improve the resulting utility.
• We test Blender on two common applications:
search trend computation and local search and
ﬁnd that it preserves high levels of utility while
maintaining diﬀerential privacy for reasonable
privacy parameter values.
• As part of Blender, we propose an approach
for automatically balancing the data obtained
from participation of opt-in users with that of
other users to maximize the eventual utility.
• We perform a comprehensive utility evaluation
of Blender on two large web search data sets,
comprising 4.8 million and 13.2 million queries,
demonstrating that Blender maintains very
high level of utility (i.e., NDCG values in ex-
cess of 95% across a range of parameters).
2 System Overview
We now discuss the high-level overview of our pro-
posed system, Blender, that coordinates the pri-
vatization, collection, and aggregation of data in the
hybrid model, as well as some of the speciﬁc choices
we make in this system. We use the task of enabling
local search based on user histories while preserving
diﬀerential privacy throughout, but, as will become
clear from the discussion, our model and system can
also be applied to other frequency-based estimation
tasks. As discussed in Section 1, we consider two
groups of users: the opt-in group, who are comfort-
able with privacy as ensured by the trusted curator
model, and the clients, who desire the privacy guar-
antees of the local model.
2.1 Outline of Our Approach
The core of our innovation is to take advantage of
the privatized information obtained from the opt-in
group in order to create a more eﬃcient (in terms
of utility) algorithm for data collection from the
USENIX Association
26th USENIX Security Symposium    749
Figure 1: Architectural diagram of Blender’s processing steps.
clients. Furthermore, the privatized results obtained
from the opt-in group and from the clients are then
“blended” in a way that takes into account the pri-
vatization algorithms used for each group, and thus,
again, achieving an improved utility over a less-
informed combination of data from the two groups.
The problem of enabling privacy-preserving lo-
cal search using past search histories can be viewed
as the task of identifying the most frequent search
records among the population of users, and estimat-
ing their underlying probabilities (both in a diﬀeren-
tial privacy-preserving manner). In this context, we
call the data collected from the users search records,
where each search record is a pair of strings of the
form (cid:104)query, U RL(cid:105), representing a query that a user
posed followed by the URL that the user subse-
quently clicked. We denote by p(cid:104)q,u(cid:105) the true under-
lying probability of the search record (cid:104)q, u(cid:105) in the