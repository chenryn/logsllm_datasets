# BLENDER: Enabling Local Search with a Hybrid Differential Privacy Model

## Authors
- Brendan Avent, Aleksandra Korolova (University of Southern California)
- David Zeber, Torgeir Hovden (Mozilla)
- Benjamin Livshits (Imperial College London)

## Abstract
We introduce a hybrid model of differential privacy that integrates both regular and opt-in users. Regular users benefit from the local privacy model, while opt-in users prefer the trusted curator model. We demonstrate that within this hybrid model, it is possible to design a new type of blended algorithm for privately computing the most popular records in a web search log. This approach significantly improves data utility compared to existing methods while maintaining the desired privacy guarantees. Specifically, on two large datasets comprising 4.8 million and 13.2 million unique queries, our method achieves NDCG values exceeding 95% across a range of commonly used privacy budget values.

## 1. Introduction
The tension between collecting large-scale user data and preserving individual privacy has never been more pronounced. Organizations often need to collect user data to improve their services, but users may be reluctant to share sensitive information. Additionally, organizations face liability risks if they mishandle such data. Our work aims to address these challenges by enabling privacy-preserving decentralized data collection that aggregates high-quality datasets from multiple entities.

### 1.1. Differential Privacy and Curator Models
In recent years, many ad-hoc privacy protection methods have proven inadequate [33, 23]. Differential privacy [10, 9, 11], which has become the gold standard in academic literature and is gaining traction in industry and government [13, 17, 28], overcomes these issues by providing mathematically rigorous privacy guarantees.

#### Trusted Curator Model
Most differentially private algorithms operate under the trusted curator model, where all user data is collected by a central entity before applying privatization techniques. While this ensures that the released dataset protects user privacy, it requires users to trust the curator with their unperturbed data.

#### Local Model
In the local differential privacy (LDP) model, privatization occurs on the user's device before data is sent to the collector. This approach, used by companies like Google and Apple, minimizes the trust required from users and embodies the "data minimization" principle [41]. For example, to estimate the proportion of HIV-positive individuals, each person can use a randomized response mechanism, ensuring plausible deniability and differential privacy.

**Definition of Differential Privacy:**
An algorithm \( A \) is \((\epsilon, \delta)\)-differentially private if for all neighboring databases \( D \) and \( D' \) differing in one user's data, the following inequality holds for all possible sets of outputs \( Y \subseteq \text{Range}(A) \):
\[ \Pr[A(D) \in Y] \leq e^\epsilon \Pr[A(D') \in Y] + \delta. \]

The timing of the privacy perturbation differs between the trusted curator and local models, leading to variations in the definition of "neighboring databases" and the algorithms analyzed.

### 1.2. Applications
Heavy hitter discovery and estimation are well-studied problems in information retrieval and privacy-preserving data analysis. However, current LDP approaches often result in significant utility losses, making them less practical. Our work focuses on improving the utility of collected data through a hybrid model.

#### Local Search
Local search involves collecting and analyzing query-URL pairs to create a head of the most popular queries and URLs, which can be stored locally on users' devices. This reduces latency and improves performance, especially in scenarios with limited network connectivity. The data can be combined from multiple search engines based on context and geographic location.

#### Trend Computation
Search trend computation, another application of heavy hitter estimation, involves finding and ranking the most popular queries. An example is Google Trends, which provides an up-to-date list of trending topics and queries.

### 1.3. Contributions
Our paper makes the following contributions:
- **Hybrid Trust Model:** We introduce a more realistic hybrid trust model, allowing for a mix of regular and opt-in users.
- **Blender Algorithm:** We propose Blender, an algorithm that blends data from opt-in and regular users to improve utility.
- **Utility Evaluation:** We test Blender on two large web search datasets, demonstrating high utility (NDCG values > 95%) while maintaining differential privacy.
- **Data Balancing:** We propose a method for balancing data from opt-in and regular users to maximize utility.

## 2. System Overview
Blender coordinates the privatization, collection, and aggregation of data in the hybrid model. We focus on the task of enabling local search while preserving differential privacy, but our model can also be applied to other frequency-based estimation tasks.

### 2.1. Outline of Our Approach
Our approach leverages the privatized information from the opt-in group to create a more efficient algorithm for data collection from regular users. The results from both groups are then blended, taking into account the privatization algorithms used, to achieve improved utility.

**Figure 1: Architectural Diagram of Blenderâ€™s Processing Steps**

The problem of enabling privacy-preserving local search can be seen as identifying the most frequent search records and estimating their probabilities. Each search record is a pair \(\langle \text{query}, \text{URL} \rangle\), representing a user's query and the subsequent click. We denote the true probability of a search record \(\langle q, u \rangle\) as \( p_{\langle q, u \rangle} \).

---

This revised version maintains the original content while improving clarity, coherence, and professionalism.