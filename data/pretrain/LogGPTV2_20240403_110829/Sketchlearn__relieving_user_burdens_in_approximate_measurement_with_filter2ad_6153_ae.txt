Cardinality estimation
Flow size distribution
Entropy estimation
General-purpose
RevSketch (Rev) [58]
SeqHash (Seq) [7]
LD-Sketch (LD) [33]
CountMin (CM) [18]
CountSketch (CS) [12]
PCSA [25]
kMin (KM) [3]
Linear Counting (LC) [66]
HyperLoglog (HLL) [24]
MRAC [38]
MRAC [38]
FlowRadar (FR) [40]
UnivMon (UM) [42]
Table 1: Measurement tasks and approx. solutions.
plane. We implement a hash computation action in a dedi-
cated table, and employs subsequent tables to accommodate
counter update actions for different levels of sketches. Each
counter update action encapsulates a stateful ALU to update
the corresponding register array based on the bit value in
the flowkey.
Control plane. We implement a multi-threaded control
plane that runs model learning and query runtime. A ded-
icated thread receives results from the data plane. It dis-
patches stacks to multiple computing threads as flows can
be extracted from each stack independently. Finally, a merg-
ing thread integrates results to form the final model and
computes traffic statistics accordingly.
Limitations. SketchLearn consumes many architecture-
specific hardware resources to boost the performance be-
cause updating l + 1 levels of counters is time consuming.
For example, the software implementation occupies the AVX
registers, which can be used by other high-performance ap-
plications. In P4, actions are executed in physical stages. The
number of stages and number of stateful actions per stage are
both limited. We will address such limitations by simplifying
the multi-level design in future work.
7 EVALUATION
We conduct experiments to show that SketchLearn (i) incurs
limited resource usage; (ii) supports general traffic statistics;
(iii) completely addresses limitations of state-of-the-arts; and
(iv) supports network-wide coordination.
7.1 Methodology
Testbed. We deploy the OVS-based data plane (including
standard OVS and OVS-DPDK) in eight physical hosts, each
of which has two 8-core Intel Xeno 2.93GHz CPUs, 64GB
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 6: (Exp#1) Memory usage.
Figure 7: (Exp#2) Peak per-packet overhead.
RAM, a 1Gb NIC, and a 10Gb NIC. Each host in the data
plane runs a single-threaded process that sends traffic via
the 10Gb NIC and reports its sketch via the 1Gb NIC to the
control plane, which runs in a dedicated host. For the P4 data
plane, we deploy it in a Tofino hardware switch [4].
Simulator. Our OVS-based and P4-based testbeds are lim-
ited by the number of devices and the NIC speed. Thus, we im-
plement a simulator that runs both the data plane and control
plane in a single machine and connects them via loopback
interfaces, without forwarding traffic via NIC. It eliminates
network transfer overhead to stress-test SketchLearn.
Traces. We generate workloads with two real-world traces:
a CAIDA backbone trace [8] and a data center trace (UN2)
[6]. Each host emits traffic as fast as possible to maximize its
processing load. The data plane reports multi-level sketches
to the control plane every 1-second epoch. In the busiest
epoch, each host emits 75K flows, 700K packets, and 700MB
traffic for the CAIDA trace, and 3.1K flows, 35K packets, and
30MB traffic for the data center trace.
Parameters. By default, we allocate a 64KB multi-level
sketch and set r = 1 per level (see §4.4). We consider 5-tuple
flowkeys (with 104 bits), so a 64KB sketch implies c = 156.
7.2 Fulfilling Design Requirements
We first evaluate how SketchLearn addresses the require-
ments in §2.1. We consider various measurement tasks and
compare SketchLearn with existing approximate measure-
ment approaches (see Table 1). We fix the expected errors
and manually tune each existing approximate measurement
approach to achieve the errors. For heavy hitter and heavy
changer detection, we set the threshold as 1% of the overall
frequency and the error probability as 5%. For remaining
statistics, we set the expected relative error as 10%.
(Experiment 1) Memory usage. As per R1, Figure 6 shows
that 64KB memory suffices for SketchLearn (SL) to achieve
the desired errors. This is comparable and even much less
than many existing approaches. The only exception is that
cardinality estimation approaches require much less memory
as they do not need flow frequency and flowkey information,
yet they are only designed for cardinality estimation.
(Experiment 2) Per-packet processing. As per R2, Fig-
ure 7 shows the peak per-packet processing overhead in
CPU cycles. SketchLearn incurs only 92 cycles with SIMD,
much lower than the 200-cycle budget for 10Gbps links (a
Figure 8: (Exp#3)
Testbed through-
put.
Figure 9: (Exp#4)
Simulator
throughput.
Figure 10: (Exp#5)
Inference time.
non-SIMD version incurs 600 CPU cycles, not shown in the
figure). For comparisons, top-k approaches consume an order
of 104 CPU cycles as they traverse the whole data structure
in the worst case. Some sketch-based approaches (e.g., CM)
also fulfill the requirements, but they are specialized. Other
sketch-based solutions (e.g., FR and UM) employ complicated
structures to be general-purpose but incur high overhead.
(Experiment 3) Testbed throughput. We measure the
throughput of SketchLearn in both OVS-based and P4 plat-
forms. Figure 8 shows the normalized throughput to the
line-rate speed without measurement. The processing speed
is preserved and the variance is very small. The high perfor-
mance comes from the fact that counters in different levels
have no dependencies, providing opportunities for paral-
lelization in both software and hardware platforms (see §6).
In particular, for P4, counter update actions are distributed
in different physical stages and executed in parallel.
(Experiment 4) Simulator throughput. Figure 9 presents
the throughput of SketchLearn for r = 1 and r = 2 per level
in our stress-test simulator. The throughput is above the
14.88Mpps requirement for 64-byte packets in a 10Gbps link.
(Experiment 5) Inference time. As per R3, we measure
the model inference time (see §4.3). Figure 10 shows that the
model inference time decreases as the number of threads
grows. With five threads, the inference time takes less than
0.5 seconds. Doubling the sketch size only slightly increases
the inference time, as the large flow extraction converges
faster with more stacks and preserves the total time.
(Experiment 6) Generality. As per R4, we evaluate the
accuracy of SketchLearn for various measurement tasks (see
Table 1). We configure two memory sizes 32KB and 64KB,
and set the number of rows per sketch as r = 1 or r = 2. We
have four configurations for SketchLearn and call them S32-
1, S32-2, S64-1, and S64-2. Other approaches being compared
are allocated 64KB memory each.
Figure 11 shows that SketchLearn achieves high accu-
racy for all cases. Its accuracy remains fairly stable across
64584832328256414963478218162561484162564101103MGLossySSFPDelRevSeqLDCMCSPCSAKMLCHLLMRACFRUMSLKB67K24K21K15K2.3K4.7K2.6K2.6K18540753551541531261.2K5.7K92102105108MGLossySSFPDelRevSeqLDCMCSPCSAKMLCHLLMRACFRUMSLCPU Cycles99.9399.9999.96050100OVSOVSDPDKTofinoThroughput(Normalized %)26.7115.550102030r=1r=2Throughput(MPPS)12312345Number of ThreadsTime (s)64K128KSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Huang et al.
(a) Heavy hitter
(b) Heavy changer
(a) Theorem 1
Figure 12: (Exp#7) Fitting theorems.
(b) Theorem 2
(c) Flow frequency
(d) Cardinality
(e) Flow size distribution
(f) Entropy
Figure 11: (Exp#6) Generality.
all four configurations as it can extract very small flows
to produce accurate results. Although its error is higher
than the best state-of-the-art for some cases (e.g., HLL for
cardinality), those state-of-the-arts are specialized. In par-
ticular, SketchLearn outperforms the two general-purpose
approaches FlowRadar and UnivMon, as they need excessive
memory (see Figure 6) to mitigate errors. With only 64KB
of memory, they suffer from serious hash collisions. In par-
ticular, FlowRadar fails to extract flows as it requires that
some counters contain exactly one flow in order for the flow
to be extracted; UnivMon has significant overestimates (see
Figure 11(c)) and hence high false positives (see Figure 11(a)).
7.3 Addressing Limitations
(Experiment 7) Fitting theorems (L1 and L3). We verify
Theorems 1 and 2, which address L1 and L3 (see §4.4). For
Theorem 1, the experiment tests the null hypothesis that
Ri, j[k] follows a Gaussian distribution. It performs two sta-
tistical tests: Shapiro-Wilk Test [61] and D’Agostino’s K
2
Test [21]. Figure 12(a) shows that the p-values are above 0.4
for most cases, so Theorem 1 holds (with a high probabil-
ity). Note that the p-value of Shapiro-Wilk Test falls below
0.4 when the memory exceeds 1024KB as its accuracy drops
(a) Precision
(b) Recall
Figure 13: (Exp#8) Robust to small thresholds.
when the number of stacks exceeds 2000. For Theorem 2, the
experiment compares its theoretical guaranteed frequency
with the actual value. Figure 12(b) shows that actual guar-
anteed frequencies are slightly below the theoretical ones
across all epochs and memory sizes, so Theorem 2 follows.
Also, the small gap between the theoretical and actual values
implies that Theorem 2 already achieves near-optimal con-
figurations and administrators do not need manual tuning.
(Experiment 8) Robust to small thresholds (L2). This
experiment shows that flows smaller than the guaranteed
frequency are also extracted (see §4.4). We measure the ac-
curacy for frequencies from 10% to 50% of the guaranteed
ones. Figure 13 shows that for 50% of the guaranteed fre-
quency, the precision and recall remain around 99%. Also,
our default configuration with 64KB implies a guaranteed
frequency of 0.5% of the total frequency (see Figure 12(b)).
For 20% of the guaranteed frequency (i.e., 0.1% of the total
frequency), SketchLearn still achieves 90% precision and 80%
recall, much higher than those in Figure 1. Even for 10% of the
guaranteed frequency, the precision and recall are above 75%
and 50%, respectively. The accuracy can be further improved
via network-wide coordination (see Experiment 11).
(Experiment 9) Arbitrary field combinations (L4). Fig-
ure 14 presents the accuracy for three flowkey definitions:
5-tuples (our default), source/destination IP addresses, and
source IP address and port. We focus on heavy changer de-
tection and entropy estimation. Although the relative errors
of entropy estimation slightly increase for the latter two
definitions, the results are still comparable to 5-tuples.
(Experiment 10) Attaching error measures (L5). We
show the effectiveness of attaching error measures by com-
paring the accuracy with and without error filtering (see
§5.2). We focus on heavy hitter detection, heavy changer
10091.810010010098.410010050.460.35712.610082.710098.70096.73.193.386.498.497.5100100100100050100MGLossySSFPDelRevSeqLDFRUMS32−1S32−2S64−1S64−2Accuracy (%)PrecisionRecall98.884.499.297.398.494.599.597.947.662.373.21194.785.510088.10084.22.791.881.497.792.8100100100100050100MGLossySSFPDelRevSeqLDFRUMS32−1S32−2S64−1S64−2Accuracy (%)PrecisionRecall0.611.2110047.0111.677.927.074.46020406080100CMCSFRUMS32−1S32−2S64−1S64−2Relative Error (%)1.733.144.260.8110049.233.072.642.92.23020406080100PCSAKMLCHLLFRUMS32−1S32−2S64−1S64−2Relative Error (%)0.3450.34830.21340.22440.21970.21350.00.20.40.6MRACUMS32−1S32−2S64−1S64−2MRD (%)3.8427.0916.4611.8911.729.3501020304050MRACUMS32−1S32−2S64−1S64−2Relative Error (%)0.00.20.40.6326412825651210242048Memory Size (KB)p−ValueShapiro WilkD’Agostino0.0000.0050.010326412825651210242048Memory Size (KB)Guaranteed FrequencyTheoryActual7080901001020304050Ratio to Guaranteed Frequency (%)Precision (%)64KB128KB256KB4060801001020304050Ratio to Guaranteed Frequency (%)Recall (%)64KB128KB256KBSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
the number is seven. Figure 16(b) shows the network-wide
cardinality and entropy. To avoid duplicate measurement, we
collect results from end hosts so that each flow is measured
exactly twice. We vary the number of participating hosts
here. The cardinality is stable for different numbers of hosts.
The error of network-wide entropy decreases from 9% to 2%
as the number of hosts increases, since more hosts provide
more hash rows to improve learning accuracy.
8 RELATED WORK
Hash tables. Some studies [1, 41, 49, 63] advocate hash ta-
bles for per-flow tracking. However, hash tables inevitably
consume substantial resources. Trumpet [49] uses rule-
matching to monitor only the important events and opti-
mizes cache locality to limit tracking overhead. However, it
requires domain knowledge to configure appropriate rules,
and hence incurs user burdens in some scenarios. Also, cache
optimizations only work for software and how to apply them
in hardware switches remains an open issue.
Rule matching and query languages. PacketHistroy [29]
and Planck [55] mirror traffic for further analysis, while
incurring high bandwidth consumption for mirroring. Ev-
erFlow [70] selectively processes flows to reduce overhead
with pre-defined rules. Recent studies on query languages
[26, 28, 65] allow expressions of sophisticated measurement
requirements. These approaches build on existing switch
techniques and are restricted by the resources in switches.
SketchLearn is orthogonal and can be deployed with them.
Hardware enhancement. Some measurement systems
leverage hardware support, such as using TCAM to boost
performance [36, 47, 50]. TPP [35] and Marple [51] retrieve
switch states to build measurement systems. Enhancing
SketchLearn with hardware assistance is a future work.
9 CONCLUSION
SketchLearn provides a novel perspective for approximate
measurement by decoupling the binding between resource
configurations and accuracy parameters. Its idea is to lever-
age automated statistical inference to extract traffic statistics.
Experiments show that SketchLearn is resource-efficient,