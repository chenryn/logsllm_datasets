alerts fired repeatedly and constantly, overwhelming the on-calls and spamming reg‐
ular and emergency communication channels.
Once the problem was detected, incident management generally went well and
updates were communicated often and clearly. Our out-of-band communications sys‐
tems kept everyone connected even while some of the more complicated software
stacks were unusable. This experience reminded us why SRE retains highly reliable,
low overhead backup systems, which we use regularly.
154 | Chapter 13: Emergency Response
In addition to these out-of-band communications systems, Google has command-
line tools and alternative access methods that enable us to perform updates and roll
back changes even when other interfaces are inaccessible. These tools and access
methods worked well during the outage, with the caveat that engineers needed to be
more familiar with the tools and to test them more routinely.
Google’s infrastructure provided yet another layer of protection in that the affected
system rate-limited how quickly it provided full updates to new clients. This behavior
may have throttled the crash-loop and prevented a complete outage, allowing jobs to
remain up long enough to service a few requests in between crashes.
Finally, we should not overlook the element of luck in the quick resolution of this
incident: the push engineer happened to be following real-time communication chan‐
nels—an additional level of diligence that’s not a normal part of the release process.
The push engineer noticed a large number of complaints about corporate access
directly following the push and rolled back the change almost immediately. Had this
swift rollback not occurred, the outage could have lasted considerably longer, becom‐
ing immensely more difficult to troubleshoot.
What we learned
An earlier push of the new feature had involved a thorough canary but didn’t trigger
the same bug, as it had not exercised a very rare and specific configuration keyword
in combination with the new feature. The specific change that triggered this bug
wasn’t considered risky, and therefore followed a less stringent canary process. When
the change was pushed globally, it used the untested keyword/feature combination
that triggered the failure.
Ironically, improvements to canarying and automation were slated to become higher
priority in the following quarter. This incident immediately raised their priority and
reinforced the need for thorough canarying, regardless of the perceived risk.
As one would expect, alerting was vocal during this incident because every location
was essentially offline for a few minutes. This disrupted the real work being per‐
formed by the on-call engineers and made communication among those involved in
the incident more difficult.
Google relies upon our own tools. Much of the software stack that we use for trouble‐
shooting and communicating lies behind jobs that were crash-looping. Had this out‐
age lasted any longer, debugging would have been severely hindered.
Process-Induced Emergency
We have poured a considerable amount of time and energy into the automation that
manages our machine fleet. It’s amazing how many jobs one can start, stop, or retool
Process-Induced Emergency | 155
across the fleet with very little effort. Sometimes, the efficiency of our automation can
be a bit frightening when things do not go quite according to plan.
This is one example where moving fast was not such a good thing.
Details
As part of routine automation testing, two consecutive turndown requests for the
same soon-to-be-decommissioned server installation were submitted. In the case of
the second turndown request, a subtle bug in the automation sent all of the machines
in all of these installations globally to the Diskerase queue, where their hard drives
were destined to be wiped; see “Automation: Enabling Failure at Scale” on page 85 for
more details.
Response
Soon after the second turndown request was issued, the on-call engineers received a
page as the first small server installation was taken offline to be decommissioned.
Their investigation determined that the machines had been transferred to the Disker‐
ase queue, so following normal procedure, the on-call engineers drained traffic from
the location. Because the machines in that location had been wiped, they were unable
to respond to requests. To avoid failing those requests outright, on-call engineers
drained traffic away from that location. Traffic was redirected to locations that could
properly respond to the requests.
Before long, pagers everywhere were firing for all such server installations around the
world. In response, the on-call engineers disabled all team automation in order to
prevent further damage. They stopped or froze additional automation and produc‐
tion maintenance shortly thereafter.
Within an hour, all traffic had been diverted to other locations. Although users may
have experienced elevated latencies, their requests were fulfilled. The outage was offi‐
cially over.
Now the hard part began: recovery. Some network links were reporting heavy conges‐
tion, so network engineers implemented mitigations as choke points surfaced. A
server installation in one such location was chosen to be the first of many to rise from
the ashes. Within three hours of the initial outage, and thanks to the tenacity of sev‐
eral engineers, the installation was rebuilt and brought back online, happily accepting
user requests once again.
US teams handed off to their European counterparts, and SRE hatched a plan to pri‐
oritize reinstallations using a streamlined but manual process. The team was divided
into three parts, with each part responsible for one step in the manual reinstall pro‐
cess. Within three days, the vast majority of capacity was back online, while any strag‐
glers would be recovered over the next month or two.
156 | Chapter 13: Emergency Response
Findings
What went well
Reverse proxies in large server installations are managed very differently than reverse
proxies in these small installations, so large installations were not impacted. On-call
engineers were able to quickly move traffic from smaller installations to large installa‐
tions. By design, these large installations can handle a full load without difficulty.
However, some network links became congested, and therefore required network
engineers to develop workarounds. In order to reduce the impact on end users, on-
call engineers targeted congested networks as their highest priority.
The turndown process for the small installations worked efficiently and well. From
start to finish, it took less than an hour to successfully turn down and securely wipe a
large number of these installations.
Although turndown automation quickly tore down monitoring for the small installa‐
tions, on-call engineers were able to promptly revert those monitoring changes.
Doing so helped them to assess the extent of the damage.
The engineers quickly followed incident response protocols, which had matured con‐
siderably in the year since the first outage described in this chapter. Communication
and collaboration throughout the company and across teams was superb—a real tes‐
tament to the incident management program and training. All hands within the
respective teams chipped in, bringing their vast experience to bear.
What we learned
The root cause was that the turndown automation server lacked the appropriate san‐
ity checks on the commands it sent. When the server ran again in response to the
initial failed turndown, it received an empty response for the machine rack. Instead
of filtering the response, it passed the empty filter to the machine database, telling the
machine database to Diskerase all machines involved. Yes, sometimes zero does mean
all. The machine database complied, so the turndown workflow started churning
through the machines as quickly as possible.
Reinstallations of machines were slow and unreliable. This behavior was due in large
part to the use of the Trivial File Transfer Protocol (TFTP) at the lowest network
Quality of Service (QoS) from the distant locations. The BIOS for each machine in
the system dealt poorly with the failures.1 Depending on the network cards involved,
the BIOS either halted or went into a constant reboot cycle. They were failing to
transfer the boot files on each cycle and were further taxing the installers. On-call
1 BIOS: Basic Input/Output System. BIOS is the software built into a computer to send simple instructions to
the hardware, allowing input and output before the operating system has been loaded.
Process-Induced Emergency | 157
engineers were able to fix these reinstall problems by reclassifying installation traffic
at slightly higher priority and using automation to restart any machines that were
stuck.
The machine reinstallation infrastructure was unable to handle the simultaneous
setup of thousands of machines. This inability was partly due to a regression that pre‐
vented the infrastructure from running more than two setup tasks per worker
machine. The regression also used improper QoS settings to transfer files and had
poorly tuned timeouts. It forced kernel reinstallation, even on machines that still had
the proper kernel and on which Diskerase had yet to occur. To remedy this situation,
on-call engineers escalated to parties responsible for this infrastructure who were able
to quickly retune it to support this unusual load.
All Problems Have Solutions
Time and experience have shown that systems will not only break, but will break in
ways that one could never previously imagine. One of the greatest lessons Google has
learned is that a solution exists, even if it may not be obvious, especially to the person
whose pager is screaming. If you can’t think of a solution, cast your net farther.
Involve more of your teammates, seek help, do whatever you have to do, but do it
quickly. The highest priority is to resolve the issue at hand quickly. Oftentimes, the
person with the most state is the one whose actions somehow triggered the event.
Utilize that person.
Very importantly, once the emergency has been mitigated, do not forget to set aside
time to clean up, write up the incident, and to…
Learn from the Past. Don’t Repeat It.
Keep a History of Outages
There is no better way to learn than to document what has broken in the past. His‐
tory is about learning from everyone’s mistakes. Be thorough, be honest, but most of
all, ask hard questions. Look for specific actions that might prevent such an outage
from recurring, not just tactically, but also strategically. Ensure that everyone within
the company can learn what you have learned by publishing and organizing
postmortems.
Hold yourself and others accountable to following up on the specific actions detailed
in these postmortems. Doing so will prevent a future outage that’s nearly identical to,
and caused by nearly the same triggers as, an outage that has already been docu‐
mented. Once you have a solid track record for learning from past outages, see what
you can do to prevent future ones.
158 | Chapter 13: Emergency Response
Ask the Big, Even Improbable, Questions: What If…?
There is no greater test than reality. Ask yourself some big, open-ended questions.
What if the building power fails…? What if the network equipment racks are stand‐
ing in two feet of water…? What if the primary datacenter suddenly goes dark…?
What if someone compromises your web server…? What do you do? Who do you
call? Who will write the check? Do you have a plan? Do you know how to react? Do
you know how your systems will react? Could you minimize the impact if it were to
happen now? Could the person sitting next to you do the same?
Encourage Proactive Testing
When it comes to failures, theory and reality are two very different realms. Until your
system has actually failed, you don’t truly know how that system, its dependent sys‐
tems, or your users will react. Don’t rely on assumptions or what you can’t or haven’t
tested. Would you prefer that a failure happen at 2 a.m. Saturday morning when most
of the company is still away on a team-building offsite in the Black Forest—or when
you have your best and brightest close at hand, monitoring the test that they
painstakingly reviewed in the previous weeks?
Conclusion
We’ve reviewed three different cases where parts of our systems broke. Although all
three emergencies were triggered differently—one by a proactive test, another by a
configuration change, and yet another by turndown automation—the responses
shared many characteristics. The responders didn’t panic. They pulled in others when
they thought it necessary. The responders studied and learned from earlier outages.
Subsequently, they built their systems to better respond to those types of outages.
Each time new failure modes presented themselves, responders documented those
failure modes. This follow-up helped other teams learn how to better troubleshoot
and fortify their systems against similar outages. Responders proactively tested their
systems. Such testing ensured that the changes fixed the underlying problems, and
identified other weaknesses before they became outages.
And as our systems evolve the cycle continues, with each outage or test resulting in
incremental improvements to both processes and systems. While the case studies in
this chapter are specific to Google, this approach to emergency response can be
applied over time to any organization of any size.
Conclusion | 159
CHAPTER 14
Managing Incidents
Written by Andrew Stribblehill1
Edited by Kavita Guliani
Effective incident management is key to limiting the disruption caused by an incident
and restoring normal business operations as quickly as possible. If you haven’t gamed
out your response to potential incidents in advance, principled incident management
can go out the window in real-life situations.
This chapter walks through a portrait of an incident that spirals out of control due to
ad hoc incident management practices, outlines a well-managed approach to the inci‐
dent, and reviews how the same incident might have played out if handled with well-
functioning incident management.
Unmanaged Incidents
Put yourself in the shoes of Mary, the on-call engineer for The Firm. It’s 2 p.m. on a
Friday afternoon and your pager has just exploded. Black-box monitoring tells you
that your service has stopped serving any traffic in an entire datacenter. With a sigh,
you put down your coffee and set about the job of fixing it. A few minutes into the
task, another alert tells you that a second datacenter has stopped serving. Then the
third out of your five datacenters fails. To exacerbate the situation, there is more traf‐
fic than the remaining datacenters can handle, so they start to overload. Before you
know it, the service is overloaded and unable to serve any requests.
You stare at the logs for what seems like an eternity. Thousands of lines of logging
suggest there’s an error in one of the recently updated modules, so you decide to
1 An earlier version of this chapter appeared as an article in ;login: (April 2015, vol. 40, no. 2).
161
revert the servers to the previous release. When you see that the rollback hasn’t hel‐
ped, you call Josephine, who wrote most of the code for the now-hemorrhaging ser‐
vice. Reminding you that it’s 3:30 a.m. in her time zone, she blearily agrees to log in
and take a look. Your colleagues Sabrina and Robin start poking around from their
own terminals. “Just looking,” they tell you.
Now one of the suits has phoned your boss and is angrily demanding to know why he
wasn’t informed about the “total meltdown of this business-critical service.” Inde‐
pendently, the vice presidents are nagging you for an ETA, repeatedly asking you,
“How could this possibly have happened?” You would sympathize, but doing so
would require cognitive effort that you are holding in reserve for your job. The VPs
call on their prior engineering experience and make irrelevant but hard-to-refute
comments like, “Increase the page size!”
Time passes; the two remaining datacenters fail completely. Unbeknown to you,
sleep-addled Josephine called Malcolm. He had a brainwave: something about CPU
affinity. He felt certain that he could optimize the remaining server processes if he
could just deploy this one simple change to the production environment, so he did so.
Within seconds, the servers restarted, picking up the change. And then died.
The Anatomy of an Unmanaged Incident
Note that everybody in the preceding scenario was doing their job, as they saw it.
How could things go so wrong? A few common hazards caused this incident to spiral
out of control.
Sharp Focus on the Technical Problem
We tend to hire people like Mary for their technical prowess. So it’s not surprising
that she was busy making operational changes to the system, trying valiantly to solve
the problem. She wasn’t in a position to think about the bigger picture of how to miti‐
gate the problem because the technical task at hand was overwhelming.
Poor Communication
For the same reason, Mary was far too busy to communicate clearly. Nobody knew
what actions their coworkers were taking. Business leaders were angry, customers
were frustrated, and other engineers who could have lent a hand in debugging or fix‐
ing the issue weren’t used effectively.
Freelancing
Malcolm was making changes to the system with the best of intentions. However, he
didn’t coordinate with his coworkers—not even Mary, who was technically in charge
of troubleshooting. His changes made a bad situation far worse.
162 | Chapter 14: Managing Incidents
Elements of Incident Management Process
Incident management skills and practices exist to channel the energies of enthusiastic
individuals. Google’s incident management system is based on the Incident Com‐
mand System,2 which is known for its clarity and scalability.
A well-designed incident management process has the following features.
Recursive Separation of Responsibilities
It’s important to make sure that everybody involved in the incident knows their role
and doesn’t stray onto someone else’s turf. Somewhat counterintuitively, a clear sepa‐
ration of responsibilities allows individuals more autonomy than they might other‐
wise have, since they need not second-guess their colleagues.
If the load on a given member becomes excessive, that person needs to ask the plan‐
ning lead for more staff. They should then delegate work to others, a task that might
entail creating subincidents. Alternatively, a role leader might delegate system compo‐
nents to colleagues, who report high-level information back up to the leaders.
Several distinct roles should be delegated to particular individuals:
Incident Command
The incident commander holds the high-level state about the incident. They
structure the incident response task force, assigning responsibilities according to
need and priority. De facto, the commander holds all positions that they have not
delegated. If appropriate, they can remove roadblocks that prevent Ops from
working most effectively.