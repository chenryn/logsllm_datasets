    ::: admonition
    ::: para
    由任意调整的进程衍生的任意进程将继承该进程的
    *`oom_score`*。例如：如果 `sshd`{.command} 进程不受
    `oom_killer`{.methodname} 功能影响，所有由 SSH
    会话产生的进程都将不受其影响。这可在出现 OOM 时影响
    `oom_killer`{.methodname} 功能救援系统的能力。
    :::
    :::
    :::
:::
:::
::: section
::: titlepage
# [⁠]{#main-memory.html#s-memory-tunables}5.5. 调整虚拟内存 {.title}
:::
::: para
虚拟内存一般由进程、文件系统缓存以及内核消耗。虚拟内存的使用由很多因素决定，受以下参数影响：
:::
::: variablelist
[*`swappiness`*]{.term}
:   ::: para
    参数值可为 0-100，控制系统 swap
    的程序。高数值可优先系统性能，在进程不活跃时主动将其转换出物理内存。低数值可优先互动性并尽量避免将进程转换处物理内存，并降低反应延迟。默认值为
    `60`{.literal}。
    :::
[*`min_free_kbytes`*]{.term}
:   ::: para
    保证系统间可用的最小 KB
    数。这个值可用来计算每个低内存区的水印值，然后为其大小按比例分配保留的可用页。
    :::
    ::: {.warning xmlns:d="http://docbook.org/ns/docbook"}
    ::: admonition_header
    **可破坏您系统的极限值**
    :::
    ::: admonition
    ::: para
    设定这个参数时请小心，因为该值过低和过高都有问题。
    :::
    ::: para
    *`min_free_kbytes`* 太低可防止系统重新利用内存。这可导致系统挂起并让
    OOM 杀死多个进程。
    :::
    ::: para
    但将这个参数值设定太高（占系统总内存的
    5-10%）会让您的系统很快会内存不足。Linux 的设计是使用所有可用 RAM
    缓存文件系统数据。设定高 *`min_free_kbytes`*
    值的结果是在该系统中花费太多时间重新利用内存。
    :::
    :::
    :::
[*`dirty_ratio`*]{.term}
:   ::: para
    规定百分比值。当脏数据组成达到系统内存总数的这个百分比值后开始写下脏数据（[**pdflush**]{.application}）。默认值为
    `20`{.literal}。
    :::
[*`dirty_background_ratio`*]{.term}
:   ::: para
    规定百分比值。当脏数据组成达到系统内存总数的这个百分比值后开始在后端写下脏数据（[**pdflush**]{.application}）。默认值为
    `10`{.literal}。
    :::
[*`drop_caches`*]{.term}
:   ::: para
    将这个值设定为 `1`{.literal}、`2`{.literal} 或者 `3`{.literal}
    让内核放弃各种页缓存和 slab 缓存的各种组合。
    :::
    ::: variablelist
    [1]{.term}
    :   ::: para
        系统无效并释放所有页缓冲内存。
        :::
    [2]{.term}
    :   ::: para
        系统释放所有未使用的 slab 缓冲内存。
        :::
    [3]{.term}
    :   ::: para
        系统释放所有页缓冲和 slab 缓冲内存。
        :::
    :::
    ::: para
    这是一个非破坏性操作。因为无法释放脏项目，建议在运行
    `sync`{.command} 设定这个参数值。
    :::
    ::: {.important xmlns:d="http://docbook.org/ns/docbook"}
    ::: admonition_header
    **重要**
    :::
    ::: admonition
    ::: para
    不建议在产品环境中使用 *`drop_caches`* 释放内存。
    :::
    :::
    :::
:::
::: para
要在调节时临时设定这些值，请将所需值 echo 到 proc
文件系统中的适当文件中。例如：要将 *`swappiness`* 临时设定为
`50`{.literal}，请运行：
:::
``` screen
# echo 50 > /proc/sys/vm/swappiness
```
::: para
要永久设定这个值，则需要使用 `sysctl`{.command}
命令。有关详情请参考*《部署指南》*，网址为
。
:::
:::
:::
[]{#main-io.html}
::: chapter
::: titlepage
# [⁠]{#main-io.html#main-io}第 6 章 输入/输出 {.title}
:::
::: section
::: titlepage
# [⁠]{#main-io.html#idm140329774921824}6.1. 功能 {.title}
:::
::: para
红帽企业版 Linux 6 引进了大量 I/P 栈性能提高：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    现在可自动识别固态磁盘（SSD），且 I/O
    调度程序的性能可利用这些设备每秒可执行的 I/O（IOPS）次数较高。
    :::
-   ::: para
    已在内核中添加了忽略支持以便向基础存储报告未使用的块。这样可以帮助
    SSD
    进行耗损平衡计算。还可以帮助支持逻辑块分配的存储（类似存储的虚拟地址空间），方法是具体观察实际使用的存储量。
    :::
-   ::: para
    红帽企业版 Linux 6.1 会大量更改屏障使用以便提高其性能。
    :::
-   ::: para
    已使用 per-backing-device 清空线程替换
    `pdflush`{.methodname}，可在使用大量 LUN
    计数配置时极大改进系统灵活性。
    :::
:::
:::
::: section
::: titlepage
# [⁠]{#main-io.html#idm140329761287328}6.2. 分析 {.title}
:::
::: para
成功调整存储栈性能需要了解数据在系统中的流程，并精通基础存储知识以及它在各种负载下的工作原理。它还要求理解要调整的实际负载。
:::
::: para
无论合适您部署新系统时，最好彻底分析该存储。从原始 LUN
或者磁盘开始，并使用直接 I/O（绕过内核也缓存的
I/O）评估其性能。这是您可以执行的最基本测试，同时也将成为您检测栈中 I/O
性能的标准。请使用基本负载生成器（比如
[**aio-stress**]{.application}）可生成各种 I/O
大小和队列深度的连续和随机读取及写入。
:::
::: para
接下来是来自 [**aio-stress**]{.application}
运行的系列图表，每个运行有四个阶段：顺序写入、顺序读取、随机写入和随机读取。在这个示例中，将该工具配置为以不同记录大小（x
轴）和队列深度（每图一个）运行。队列深度代表在给定时间进程的 I/O
操作总和。
:::
::: figure
[⁠]{#main-io.html#idm140329774912544}
::: figure-contents
::: mediaobject
![一线程、一文件的 aio-stress 输出结果](images/aio-stress_output.png)
::: caption
::: para
y 轴以 MB/秒为单位显示带宽。x 轴以 Kb 为单位显示 I/O 大小。
:::
:::
:::
:::
**图 6.1. 一线程、一文件的 aio-stress 输出结果**
:::
::: para
注意从左下角到右上角的流量线趋势。还请注意在给定记录大小时，您可以通过增加进程中的
I/O 数从存储中获得更多流量。
:::
::: para
通过根据您的存储运行这些简单负载，您可以了解在此负载下的存储性能。保留这些测试生成的数据用来在分析更复杂的负载时进行比较。
:::
::: para
如果您要使用设备映射或者
md，在下一步添加那一层并重复您的测试。如果性能损失严重，请确定那是预期的或者可以解释的。例如：如果在该栈中添加
checksumming raid 层则一定会有性能下降。意外的性能损失可能由无法同步的
I/O 操作造成。默认情况下红帽企业版 Linux
以最佳方式对齐分区和设备映射器元数据。但不是所有类型的存储都报告其最佳校准，因此可能需要手动调整。
:::
::: para
软件设备映射或者 md
层后，在块设备顶端添加一个文件系统并根据其进行测试，仍使用直接
I/O。同样，将结果与之前测试的结果进行比较，保证您理解所有不符之处。直接写入
I/O 通常在预分配文件中性能更好，因此请确定在测试性能前预先分配文件。
:::
::: para
您可能会认为有用的人工负载生成器包括：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    [**aio-stress**]{.application}
    :::
-   ::: para
    [**iozone**]{.application}
    :::
-   ::: para
    [**fio**]{.application}
    :::
:::
:::
::: section
::: titlepage
# [⁠]{#main-io.html#idm140329758692144}6.3. 工具 {.title}
:::
::: para
有一些工具可用来帮助您诊断 I/O
子系统中的性能问题。[**vmstat**]{.application}
提供系统性能的粗略概述。以下栏与 I/O
相关：*`si`*（换入），*`so`*（换出），*`bi`*（阻止进入），*`bo`*（阻止外出），以及*`wa`*（I/O
等待时间）。当您的交换空间与数据分区同在一个设备中时 *`si`* 和 *`so`*
有用，且表示总体内存压力。*`si`* 和 *`bi`* 是读取操作，*`so`* 和 *`bo`*
是写入操作。这些分类以 Kb 为单位报告。*`wa`* 是停滞时间，表示在等待 I/O
完成时哪部分运行队列被阻断。
:::
::: para
使用 [**vmstat**]{.application} 分析系统可让您了解 I/O
子系统是否对性能问题负责。*`free`*、*`buff`* 和 *`cache`*
栏最值得关注。*`cache`* 值会随着 *`bo`* 值增加，随着 *`cache`* 的降低
*`free`* 值会增大，表示系统正在执行写回操作，且无法使用页缓存。
:::
::: para
注：*`vmstat`* 报告的 I/O 数是集合了所有设备的 I/O 总数。如果您确定 I/O
子系统中可能有性能问题，则可以使用 [**iostat**]{.application}
做进一步的检查，该程序可以根据设备报告 I/O
情况。您还可以搜索更多详细信息，比如平均需求大小，每秒读取和写入数以及正在进行的
I/O 合并数量。
:::
::: para
使用平均请求大小和平均队列大小（`avgqu-sz`{.literal}）可预估如何使用在描述存储性能时生成的图表执行存储。可采用一些概括估计方法：例如，如果平均请求大小为
4KB，且平均队列大小为 1，则不大可能有非常高的性能。
:::
::: para
如果性能值与您期待的性能值不同，可使用 [**blktrace**]{.application}
执行更详细的分析。[**blktrace**]{.application} 工具套件给出 I/O
子系统使用时间的详情。[**blktrace**]{.application}
的输出结果是一组二进制跟踪文件，可以使用其他工具进行后处理，比如
[**blkparse**]{.application}。
:::
::: para
[**blkparse**]{.application} 是 [**blktrace**]{.application}
的配伍工具。它读取跟踪的原始输出并生成一手文字版本。
:::
::: para
以下是 [**blktrace**]{.application} 输出结果示例：
:::
``` screen
8,64   3        1     0.000000000  4162  Q  RM 73992 + 8 [fs_mark]
8,64   3        0     0.000012707     0  m   N cfq4162S / alloced
8,64   3        2     0.000013433  4162  G  RM 73992 + 8 [fs_mark]