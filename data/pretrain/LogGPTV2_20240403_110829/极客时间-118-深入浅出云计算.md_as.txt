# 设置模型训练的超参数    estimator.set_hyperparamters(...)    
# 将训练数据喂送给模型进行训练    estimator.fit('s3://bucket/path/to/training/data')即使你是第一次接触使用 SageMaker这样的工具，也很容易理解上面代码片段中语句的含义。其中，SageMaker的核心概念 **Estimator****，**就是模型生命周期管理的主要抓手，你可以用它进行高层的语义操作，比如算法的选择、超参数的设置、训练数据的输入等等。这些高层次的方法使用起来清晰明了，它们会为你代劳与云平台底层资源的交互。注意：为了便于讲解说明以及突出重点，我对脚本中调用参数等处进行了适当简化。下面的举例同样如此。实际使用时，你可以根据具体使用的模型类型，参考云厂商的详细文档。slate-object="mark"}除了可以使用厂商内置的机器学习算法，**云上 AI平台也兼容开源的机器学习和深度学习框架**，比如著名的 Scikit-learn、TensorFlow、PyTorch、MXNet等等。使用开源机器学习框架，能够让你彻底地控制你的底层算法和模型结构，也便于相关代码在不同平台的复用。你也许会很好奇，**像 SageMaker这样的厂商自有机器学习体系，它是怎么集成和支持这些开源框架的呢？**我们来观察下面的代码示例：    from sagemaker.tensorflow import TensorFlow    tf_estimator = TensorFlow(entry_point='tf-train.py',             role=role,        train_instance_count=2,        train_instance_type='ml.p2.xlarge',        framework_version='2.1.0',        py_version='py3',        distributions={'parameter_server': {'enabled': True}}))            tf_estimator.fit('s3://bucket/path/to/training/data')可以看到，和前面的自有算法一样，这里同样使用了**Estimator** 的概念，来管理和控制训练过程。不过你要注意一下这里的TensorFlow 类，它不是指 TensorFlow 框架本身，而是一个在 SageMaker中，用于管理封装 TensorFlow 相关模型的Estimator。 **秘密包含在作为entry_point 参数的脚本文件 tf-train.py中，在这个脚本里，你才会编写真正的 TensorFlow代码。** 小提示：当然，为了能在 SageMaker 环境中顺利运行，你的 TensorFlow脚本需要遵循一些 AWS规定的标准，比如环境变量设置、输入输出的路径和格式等。slate-object="mark"}所以说，SageMaker是通过将控制层和算法实现层分离的方式，来同时支持自有算法和开源框架的。无论你是使用什么算法和模型，它的上层仍然是统一使用Estimator的接口，实现了对具体算法的抽象。**另外值得注意的是**，云上机器学习平台能够很容易地让你调动云上的计算资源（比如通过上面的train_instance_type 和 train_instance_count参数），不需要我们手动来进行创建和维护。尤其是那些支持分布式训练的模型算法，在云端充足的CPU/GPU资源和弹性分配机制的加持下，你可以很容易地对模型训练进行充分的并行化，这大大缩短了模型训练所需的时间。**最后，则是****[模型发布和部署 slate-object="mark"}****的环节**，也就是我们需要把训练完成的模型进行保存和管理，以及进行线上的发布和运行，有时这也被称为**推理**（Inference）阶段。同样借助上面的 Estimator 对象，这里我们只要简单地调用一下 **deploy方法**，就可以把训练好的模型部署到生产环境中，并通过新生成的 **predictor对象**来进行模型调用了，如下所示：    
# 将前面fit方法生成的模型部署到SageMaker端点上进行服务    predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')    