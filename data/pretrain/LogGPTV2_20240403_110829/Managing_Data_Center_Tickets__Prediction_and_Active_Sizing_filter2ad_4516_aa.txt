title:Managing Data Center Tickets: Prediction and Active Sizing
author:Ji Xue and
Robert Birke and
Lydia Y. Chen and
Evgenia Smirni
2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Managing Data Center Tickets:
Prediction and Active Sizing
Ji Xue
College of William and Mary
Virginia, USA
PI:EMAIL
Robert Birke
IBM Research Zurich Lab
Ruschlikon, Switzerland
PI:EMAIL
Lydia Y. Chen
IBM Research Zurich Lab
Ruschlikon, Switzerland
PI:EMAIL
Evgenia Smirni
College of William and Mary
Virginia, USA
PI:EMAIL
Abstract—Performance ticket handling is an expensive opera-
tion in highly virtualized cloud data centers where physical boxes
host multiple virtual machines (VMs). A large body of tickets
arise from the resource usage warnings, e.g., CPU and RAM
usages that exceed predeﬁned thresholds. The transient nature
of CPU and RAM usage as well as their strong correlation across
time among co-located VMs drastically increase the complexity
in ticket management. Based on a large resource usage data
collected from production data centers, amount to 6K physical
machines and more than 80K VMs, we ﬁrst discover patterns
of spatial dependency among co-located virtual resources. Lever-
aging our key ﬁndings, we develop an Active Ticket Managing
(ATM) system that consists of (i) a novel time series prediction
methodology and (ii) a proactive VM resizing policy for CPU
and RAM resources for co-located VMs on a physical box that
aims to drastically reduce usage tickets. ATM exploits the spatial
dependency across multiple resources of co-located VMs for
usage prediction and proactive VM resizing. Evaluation results
on traces of 6K physical boxes and a prototype of a MediaWiki
system show that ATM is able to achieve excellent prediction
accuracy of a large number of VM time series and signiﬁcant
usage ticket reduction, i.e., up to 60%, at low computational
overhead.
I. INTRODUCTION
Performance ticketing systems provide the means to data
centers to interactively improve user experience, maintain
performance at tails, and guarantee smooth system operation.
Typically, system monitoring and users issue tickets when
encountering an array of performance violations, e.g., unre-
sponsive service, high resource usage due to transient load
dynamics, or persistent insufﬁcient provisioning. Ticket reso-
lution is unfortunately very expensive [1], [2] as a signiﬁcant
amount of manual labor is required for root-cause analysis and
to remedy the detected problem [3]. Prior work has shown
that there is strong correlation of ticket issuing with resource
usage exceeding certain predeﬁned thresholds [4]. In today’s
data centers, with physical resources being aggressively multi-
plexed across multiple virtual machines (VMs), the likelihood
of issuing performance tickets due to physical or virtual
machines crossing predeﬁned usage thresholds dramatically
increases.
Past work has established that resource usage at data centers
exhibits strong temporal patterns [5], [6]. Beyond temporal
dependencies that are established by usage time series [7],
it is common for co-located VMs to simultaneously compete
VM1
VM2
VM3
VM4
100
80
60
40
20
)
%
(
T
C
P
D
E
S
U
U
P
C
0
0
3
6
12
9
15
Time (hour)
18
21
24
Fig. 1: An illustration of spatial dependency across usage time
series for 4 VMs co-located on a box.
for the limited physical resources, essentially exhibiting strong
spatial dependency. We illustrate a motivating example in
Figure 1 depicting the CPU usage time series1 of 4 VMs
co-located within the same physical box, where performance
tickets are issued automatically when a VM utilization exceeds
a threshold of 60%. One can easily see the spatial dependency
of VMs 1, 3, and 4, i.e., time usages move up and down syn-
chronously, and their respective tickets are triggered together,
at around the 19:00 hour mark. These time series come from
a data center production system and are quite representative
of typical patterns in such systems. The temporal and spatial
dependencies among VMs not only increase the number of
tickets but also the difﬁculty in identifying their root cause
and the corresponding resolution.
The focus of this paper is to develop a methodology to
increase the data center dependability by using a proactive
approach: reduce the number of tickets by predicting when
they will occur in the future and by employing dynamic
virtual machine resizing to adjust resource usage to avoid
the triggering of future tickets. To this end, we ﬁrst do a
detailed, post-hoc workload characterization study of usage
time series in production data centers of a major vendor
which correspond to 80K VMs hosted on 6K physical servers.
1We interchangeably use the terms time series and series.
978-1-4673-8891-7/16 $31.00 © 2016 IEEE
DOI 10.1109/DSN.2016.38
335
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:56 UTC from IEEE Xplore.  Restrictions apply. 
We develop an Active Ticket Managing (ATM) system that
predicts future VM resource usage and proactively resizes the
virtual resources of the resident VMs. The research challenges
are numerous and outlined as follows.
Effective usage prediction is prerequisite to the development
of any management policy. Indeed, in our past work we have
shown that neural networks can be effectively employed for
prediction [7], but their effective usage remains prohibitively
expensive in practical situations as it suffers by its high
training cost. In practice, in a large-scaled data center, with
more than tens of thousands of physical boxes and hundreds
of thousands of VMs, it is infeasible to rely on neural networks
to predict future resource usage. We solve this ﬁrst problem
by developing a prediction methodology that discovers spatial
dependencies across usage series and exploits them to develop
an agile methodology for prediction. To this end, we introduce
the concept of signature VM series, a subset of usage series
that are representative of all other usage series. We are able
to predict usage series not in the signatures set and the usage
violation tickets of co-located VMs, via a linear combination
of signature VM series, which provides a time series prediction
model with as low as only 26% of the original time series.
Second, based on predicted resource usage, we deﬁne a multi-
choice knapsack problem and develop a greedy algorithm
to dynamically adjust virtual resource allocation across co-
located VMs. ATM is evaluated on production traces of 80K
VMs and a small test-bed deployment on a cluster that runs
MediaWiki [8], the open source platform for Wikipedia. Our
extensive evaluation results show that ATM has remarkably
high accuracy in prediction, i.e., reaching prediction errors
as low as 20% and signiﬁcant ticket reductions, i.e., up to
60% − 70% less tickets while using only 26% of the original
time series. The contributions of this paper are as follows:
3) We develop a VM resizing policy to reduce usage tickets
by setting the upper limits of CPU and RAM allocations when
several VMs are co-located, a problem which is shown to
be NP-hard. We rigorously formulate the ticket minimiza-
tion problem subject
to the physical capacity constraints.
We propose a greedy algorithm to solve it and compare its
performance to the max-min fairness algorithm.
The outline of this work is as follows. Section II provides
a characterization study on the usage tickets as well as the
spatial patterns among usage series of co-located VMs. We
propose spatial-temporal prediction methods for demand series
1) We do post-hoc characterization of usage ticket issuing
in a large data center setting. We focus on discovering the
distribution of usage tickets and spatial patterns of resources
usages across co-located VMs. We ﬁnd that usage tickets are
mostly contributed by a small set of VMs, and that VMs show
signiﬁcant cross correlation among their CPU and RAM usage
series.
2) Motivated by the strong spatial patterns across resources
and co-located VMs, we argue that a small number of signature
usage time series can be used as predictors to represent well
the entire set of resource usage time series. This prediction
methodology is the basis of ATM.
in Section III. In Section IV, we formulate the ticket mini-
mization problem and demonstrate a greedy resizing algorithm
to reduce usage tickets. An extensive evaluation of ATM on
both production traces and a Wikipedia cluster is discussed in
Section V. Section VI presents related work, followed by the
summary and conclusions in Section VII.
II. STATISTICS AND OBSERVATIONS
The motivation for the design of ATM is the urge to reduce
usage tickets that are typically issued when VM resource uti-
lizations exceed certain thresholds. The trace that we consider
here comes from IBM production data centers serving various
industries, including banking, pharmaceutical, IT, consulting,
and retail, and using various UNIX-like operating systems,
e.g., AIX, HP-UX, Linux, and Solaris. The majority of VMs in
the trace are VMware VMs. The trace contains CPU and RAM
capacity but also utilization data taken at a time granularity
of 15 minutes for 6K physical boxes hosting more than 80K
VMs during a 7-day period from April 3, 2015 to April
9, 2015. Naturally, the level of consolidation is very high,
i.e., on average 10 VMs are consolidated within a single
physical box [5]. In addition, both VMs and boxes are very
heterogeneous in terms of resource conﬁguration.
In the following, we ﬁrst show the distribution of usage
tickets under different ticket thresholds, followed by a more
detailed analysis on the spatial patterns of usage series of
co-located VMs. We aim to uncover how usage tickets are
distributed across resources and most importantly how usage
patterns trigger usage tickets. We anticipate that the design
principles of the proposed ATM system leverages this charac-
terization analysis.
A. Usage Tickets
Usage tickets are generated when utilization values exceed
target thresholds. Naturally, lower thresholds trigger a higher
number of usage tickets and increase the cost of resolution,
whereas higher thresholds result into fewer tickets but at a
higher risk of performance degradation. To quantify the effect
of different thresholds, we consider three threshold levels,
namely 60%, 70%, and 80%. Such values are commonly
adopted in production systems [9]. Figure 2 illustrates quanti-
tative information on the issued tickets for the CPU and RAM
usage series of April 3, 2015. We focus on the following: how
many boxes have tickets and how these tickets are distributed
across co-located VMs and their resources.
Figure 2(a) plots the percentage of boxes that have at least
one VM usage ticket under the different thresholds. Even with
the highest ticket threshold of 80%, almost 40% of boxes
obtain at least one ticket due to CPU violation and 10%
due to RAM violation, these percentages increase to 57%
and 38%, respectively, when the threshold is 60%. Overall,
the percentage of boxes having CPU tickets is higher than
RAM tickets, independently of the threshold. This can be
explained by the fact that RAM tends to be over-provisioned
for performance reasons. Figure 2(b) illustrates the mean and
standard deviation of the number of tickets per box for CPU
336
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:56 UTC from IEEE Xplore.  Restrictions apply. 
)
%
(
s
e
x
o
B
f
t
o
e
g
a
n
e
c
r
e
P
70
60
50
40
30
20
10
0
Ticket Threshold = 60%
Ticket Threshold = 70%
Ticket Threshold = 80%
CPU
RAM
s
t
e
k
c
T
i
f
o
r
e
b
m
u
N
100
80
60
40
20
0
Ticket Threshold = 60%
Ticket Threshold = 70%
Ticket Threshold = 80%
CPU
RAM
s
M
V
t
i
r
p
u
C
l
f
o
r
e
b
m
u
N
3
2
1
0
(a) Percentage of boxes having tickets
(b) Distribution of tickets
Ticket Threshold = 60%
Ticket Threshold = 70%
Ticket Threshold = 80%
CPU
RAM
(c) Number of culprit VMs
Fig. 2: Characterization of usage tickets for CPU and RAM of VMs per box.
and RAM. The average number of CPU(RAM) usage tickets
per box are 39(15), 33(11), 29(9), for the three thresholds of
60%, 70%, and 80%, respectively, showing a relatively minor
decreasing trend. The next natural question is whether tickets
are evenly distributed across all co-located VMs. To this end,
we compute the number of VMs that accounts for the majority
of tickets, where the majority is deﬁned to 80% of usage
tickets per box (this is an ad-hoc value). Figure 2(c) shows
that on average one to two VMs per box cause the majority
of tickets irrespective of the three threshold values. A further
interesting observation is that since the culprit VMs are few,
if we increase the capacity allocation of the culprit VMs by
removing resources from other co-located VMs, then tickets
may reduce. On the contrary, if tickets are evenly distributed,
resizing does not help.
B. Do Spatial Dependencies Exist?
To better understand the spatial patterns of usage tickets, we
estimate the magnitude of spatial dependency by computing
the Pearson’s correlation coefﬁcients [10] over each pair of
CPU and RAM usage series of co-located VMs. For each box
and co-located VMs, we compute four types of correlation
coefﬁcients ρ: (i) between any pair of CPU usage series (intra-
CPU), (ii) between any pair of RAM usage series (intra-RAM),
(iii) between any pair of CPU and RAM usage series (inter-
all), and (iv) between CPU and RAM usage series from the
same VM (inter-pair). The ﬁrst two correlation metrics mea-
sure the relationship among speciﬁc resources, i.e. CPU and
RAM, time series (“intra” resource measures), the latter two
focus on the relationship between CPU-RAM pairs (“inter”
measures). For each box, we compute the median value of
the above measures and present the cumulative distribution
functions (CDFs) across all the boxes in Figure 3.
One can immediately see from the shapes of the CDFs
that intra-RAM ρ is higher than intra-CPU, followed by inter-
resources measured from any pair of VM or the same VM.
This implies that inter-resource dependency is higher than the
intra-resource one. Indeed, the mean values for intra-CPU,
intra-RAM, inter-CPU/RAM from any pair, inter-CPU/RAM
from the same VM are 0.26, 0.24, 0.30, and 0.62 respectively.
The CDFs give a clear message: the CPU-RAM pairs across
co-located VMs are correlated, this is a fact that we take
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Intra-CPU
Intra-RAM
Inter-all
Inter-pair
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Median Correlation Coefficient
Fig. 3: Cumulative distribution of correlation of intra-CPU,
intra-RAM, inter-CPU/RAM.
advantage of when we attempt to use clustering to reduce the
cost of prediction.
III. SPATIAL-TEMPORAL PREDICTION MODELS
We ﬁrst elaborate on the challenges for concurrent predic-