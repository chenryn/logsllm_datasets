# Title: Managing Data Center Tickets: Prediction and Active Sizing

## Authors
- Ji Xue, College of William and Mary, Virginia, USA (PI:EMAIL)
- Robert Birke, IBM Research Zurich Lab, Rüschlikon, Switzerland (PI:EMAIL)
- Lydia Y. Chen, IBM Research Zurich Lab, Rüschlikon, Switzerland (PI:EMAIL)
- Evgenia Smirni, College of William and Mary, Virginia, USA (PI:EMAIL)

## Conference
2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks

## Abstract
Performance ticket handling is a costly operation in highly virtualized cloud data centers, where physical servers host multiple virtual machines (VMs). A significant number of tickets arise from resource usage warnings, such as CPU and RAM usages that exceed predefined thresholds. The transient nature of CPU and RAM usage, along with their strong temporal and spatial correlations among co-located VMs, significantly increases the complexity in ticket management. Using a large dataset collected from production data centers, comprising 6,000 physical servers and over 80,000 VMs, we first identify patterns of spatial dependency among co-located virtual resources. Leveraging these findings, we develop an Active Ticket Management (ATM) system that includes a novel time series prediction methodology and a proactive VM resizing policy for CPU and RAM resources. ATM exploits the spatial dependencies across multiple resources of co-located VMs for usage prediction and proactive resizing. Evaluation results on traces from 6,000 physical servers and a prototype MediaWiki system show that ATM achieves excellent prediction accuracy and significantly reduces usage tickets, up to 60%, with low computational overhead.

## 1. Introduction
Performance ticketing systems are essential for data centers to maintain user experience, ensure performance at tails, and guarantee smooth system operation. Typically, system monitoring and users issue tickets when encountering performance violations, such as unresponsive services, high resource usage due to transient load dynamics, or persistent insufficient provisioning. Ticket resolution is expensive, requiring significant manual labor for root-cause analysis and problem remediation. Prior research has shown a strong correlation between ticket issuance and resource usage exceeding predefined thresholds. In today's data centers, with physical resources being aggressively multiplexed across multiple VMs, the likelihood of issuing performance tickets due to threshold crossings dramatically increases.

Past work has established that resource usage in data centers exhibits strong temporal patterns. Beyond these temporal dependencies, co-located VMs often simultaneously compete for limited physical resources, exhibiting strong spatial dependencies. Figure 1 illustrates this with the CPU usage time series of four co-located VMs, where performance tickets are issued when VM utilization exceeds a 60% threshold. The spatial dependencies among VMs not only increase the number of tickets but also complicate the identification of root causes and resolution.

This paper focuses on developing a proactive approach to increase data center dependability by reducing the number of tickets through prediction and dynamic VM resizing. We conduct a detailed, post-hoc workload characterization study of usage time series from production data centers, corresponding to 80,000 VMs hosted on 6,000 physical servers. We develop an Active Ticket Management (ATM) system that predicts future VM resource usage and proactively resizes the virtual resources of resident VMs. The key challenges include effective usage prediction and dynamic resource allocation. Our contributions are:

1. **Post-hoc Characterization**: We analyze usage ticket issuance in a large data center setting, focusing on the distribution of usage tickets and spatial patterns of resource usage across co-located VMs. We find that a small set of VMs contributes most to usage tickets, and VMs show significant cross-correlation in their CPU and RAM usage series.
2. **Spatial-Temporal Prediction**: Motivated by the strong spatial patterns, we propose a prediction methodology using a subset of signature VM time series to represent the entire set of resource usage time series. This methodology achieves high prediction accuracy with low computational overhead.
3. **Proactive VM Resizing Policy**: Based on predicted resource usage, we formulate a multi-choice knapsack problem and develop a greedy algorithm to dynamically adjust virtual resource allocation across co-located VMs. This policy significantly reduces usage tickets while adhering to physical capacity constraints.

The rest of the paper is organized as follows: Section 2 provides a characterization study of usage tickets and spatial patterns among co-located VMs. Section 3 details our spatial-temporal prediction methods. Section 4 formulates the ticket minimization problem and presents a greedy resizing algorithm. Section 5 discusses the extensive evaluation of ATM on both production traces and a Wikipedia cluster. Section 6 reviews related work, and Section 7 concludes the paper.

## 2. Statistics and Observations
The motivation for the design of ATM is to reduce usage tickets, which are typically issued when VM resource utilizations exceed certain thresholds. The trace data used in this study comes from IBM production data centers serving various industries, including banking, pharmaceuticals, IT, consulting, and retail, and using various UNIX-like operating systems. The trace contains CPU and RAM capacity and utilization data taken at 15-minute intervals for 6,000 physical servers hosting over 80,000 VMs during a 7-day period from April 3, 2015, to April 9, 2015. The average consolidation level is 10 VMs per physical server, and both VMs and servers are heterogeneous in terms of resource configuration.

### 2.1 Usage Tickets
Usage tickets are generated when utilization values exceed target thresholds. Lower thresholds trigger more tickets and increase resolution costs, while higher thresholds result in fewer tickets but at a higher risk of performance degradation. We consider three threshold levels: 60%, 70%, and 80%. Figure 2 illustrates the distribution of issued tickets for CPU and RAM usage series on April 3, 2015, showing the percentage of boxes with at least one VM usage ticket and the distribution of tickets across co-located VMs and their resources.

Figure 2(a) shows the percentage of boxes with at least one VM usage ticket under different thresholds. Even with the highest threshold of 80%, almost 40% of boxes have at least one ticket due to CPU violation and 10% due to RAM violation. These percentages increase to 57% and 38%, respectively, when the threshold is 60%. Overall, the percentage of boxes with CPU tickets is higher than RAM tickets, regardless of the threshold. This can be explained by the fact that RAM tends to be over-provisioned for performance reasons.

Figure 2(b) illustrates the mean and standard deviation of the number of tickets per box for CPU and RAM. The average number of CPU (RAM) usage tickets per box are 39 (15), 33 (11), and 29 (9) for the three thresholds of 60%, 70%, and 80%, respectively, showing a relatively minor decreasing trend. To determine if tickets are evenly distributed across all co-located VMs, we compute the number of VMs responsible for the majority of tickets, defined as 80% of usage tickets per box. Figure 2(c) shows that, on average, one to two VMs per box cause the majority of tickets, regardless of the threshold values. This suggests that increasing the capacity allocation of these culprit VMs by reallocating resources from other co-located VMs may reduce tickets. Conversely, if tickets are evenly distributed, resizing does not help.

### 2.2 Spatial Dependencies
To better understand the spatial patterns of usage tickets, we estimate the magnitude of spatial dependency by computing Pearson’s correlation coefficients for each pair of CPU and RAM usage series of co-located VMs. For each server and co-located VMs, we compute four types of correlation coefficients ρ: (i) between any pair of CPU usage series (intra-CPU), (ii) between any pair of RAM usage series (intra-RAM), (iii) between any pair of CPU and RAM usage series (inter-all), and (iv) between CPU and RAM usage series from the same VM (inter-pair). The first two metrics measure the relationship among specific resources, while the latter two focus on the relationship between CPU-RAM pairs. For each server, we compute the median value of these measures and present the cumulative distribution functions (CDFs) across all servers in Figure 3.

Figure 3 shows that intra-RAM ρ is higher than intra-CPU, followed by inter-resources measured from any pair of VMs or the same VM. This implies that inter-resource dependency is higher than intra-resource dependency. The mean values for intra-CPU, intra-RAM, inter-CPU/RAM from any pair, and inter-CPU/RAM from the same VM are 0.26, 0.24, 0.30, and 0.62, respectively. The CDFs indicate that CPU-RAM pairs across co-located VMs are correlated, which we leverage in our clustering approach to reduce the cost of prediction.

## 3. Spatial-Temporal Prediction Models
We elaborate on the challenges for concurrent prediction and introduce our spatial-temporal prediction models. Effective usage prediction is a prerequisite for any management policy. In our past work, we showed that neural networks can be effectively employed for prediction, but their high training cost makes them impractical for large-scale data centers. Therefore, we develop a prediction methodology that discovers spatial dependencies across usage series and exploits them to create an agile prediction model. We introduce the concept of signature VM series, a subset of usage series that are representative of all other usage series. We predict usage series not in the signature set and the usage violation tickets of co-located VMs via a linear combination of signature VM series, providing a time series prediction model with as little as 26% of the original time series.

## 4. Ticket Minimization Problem and Greedy Resizing Algorithm
Based on predicted resource usage, we define a multi-choice knapsack problem and develop a greedy algorithm to dynamically adjust virtual resource allocation across co-located VMs. The ticket minimization problem is formulated subject to physical capacity constraints. We compare the performance of our greedy algorithm to the max-min fairness algorithm and demonstrate its effectiveness in reducing usage tickets.

## 5. Evaluation
We evaluate ATM on production traces from 80,000 VMs and a small test-bed deployment on a cluster running MediaWiki, the open-source platform for Wikipedia. Our extensive evaluation results show that ATM achieves remarkably high prediction accuracy, with errors as low as 20%, and significant ticket reductions, up to 60-70% less tickets, while using only 26% of the original time series.

## 6. Related Work
We review related work in the areas of performance ticket management, resource usage prediction, and dynamic VM resizing. We highlight the unique contributions of our ATM system in addressing the challenges of managing data center tickets through a combination of spatial-temporal prediction and proactive resource management.

## 7. Summary and Conclusions
In this paper, we address the challenge of managing performance tickets in highly virtualized cloud data centers by developing an Active Ticket Management (ATM) system. ATM leverages spatial-temporal dependencies among co-located VMs to predict future resource usage and proactively resize virtual resources. Our evaluation on production traces and a MediaWiki cluster demonstrates that ATM significantly reduces usage tickets while maintaining high prediction accuracy and low computational overhead. Future work will focus on extending ATM to handle more complex resource allocation scenarios and further optimizing the prediction and resizing algorithms.