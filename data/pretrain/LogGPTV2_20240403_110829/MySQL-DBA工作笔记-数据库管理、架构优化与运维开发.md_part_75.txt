 db_port=int (vm db_port) 
int(vm_db_port)
celey@worke
celety@woker
---
## Page 505
高可用，我们可以为这台服务器打上一个“高危服务”的标签。
服务器的维保周期外是很容易出现异常情况的，对于这类业务我们就要重点关注灾备和
之中去管理，根据一些特点建立模型，打上标签，那么我们的管理工作就会很容易扩展开来。
据库画像吧。
就是大数据中经常听到的用户画像，按照这种概念的逻辑，到了数据库方向我就叫它数
会根据你的阅读情况给你针对性的推送一些内容。这种现象背后就是有相应的标签，也
掺和在一起了。我们应当建立独立开放的标签管理模块，让它成为运维体系中的催化剂。
13.2.4
于我们解决大多数问题也是相通的。
比如一个服务器运行时间超过3年未重启，那么可以算是一个高危服务状态了，在
，我们管理的数据库肯定会各有自己的特点，如果我们给这些数据库打上标签，在已有的体系
比如我们打开朋友圈或者点开一篇文章可能会收到不同的推送广告，有些软件 APP
根据自己对于业务的理解，我对目前的标签管理进行了一些梳理，如下图13-27所示。
运维标签管理其实是运维体系中比较容易忽视的角色。因为很多时候它和元数据的属性
通过这样一个问题也可以折射出我们对于性能优化问题的分析思路，触类旁通，对
而改进之后，只需要50多秒，如图13-26所示。
运维系统中的标签管理
巡检模型
数据库画像
运维标签管理模块
图13-26
图13-27
自动化标签完害己有的数据采集
标签模型校理工单数据进行应用数据反带
支持标签的增删改查
根据周期性巡检任务
第13章MySQL运维基础架构设计”483
RecivedStatedRurtimWorker
二级模型
，给数据库打上相应的标签
ceen.dop
---
## Page 506
484丨MySQL DBA工作笔记：数据库管理、架构优化与运维开发
后端同步执行的，不需要无谓的等待。
任务系统所做的事情是很简单的。
务含义的，执行时是通过任务标识和任务信息拆分任务并路由到调度系统执行，所以说
系统中存在的绝大多数运维任务，其实都可以通过任务调度的方式触发。
通过建立丰富的模型来提供更多的数据价值。
检紧密结合起来的，通过标签的模型可以对巡检模型产生一些直接的参考价值。
立相应的数据库画像，让我们的数据库服务具化为一种更加生动，容易理解的形式。
上相关的标签，算是一种自动化运维的标签管理。
行上行下钻，来分析一些潜在的关系和问题。
伸出来就需要标签的关联关系，在建立一系列标签之后，我们可以根据这些标签信息进
之间是有依赖的，比如标签 A 和标签B 组合起来，根据条件可以衍生出标签C，逐步延
13.3.1
触发的时间是10分钟，那么我们可能有以下的几种管理方式：
推送2个运维任务到这100台服务器上，假设这2个任务之间没有依赖关系，如果每台
构没有直接的耦合和依赖。
13.3
。如果你还不是很明白，那我们假设一个场景，我们有100台数据库服务器，现在要
要建设运维系统，任务调度会是其中的关键一环。任务调度是不需要理解任务的业
当然最后也是我们做这件事情的一个价值输出，那就是我们的标签管理其实是和巡
而数据库画像是在这些数据沉淀的基础之上，我们根据维度、业务特点和系统资源来建
，自动化标签是根据已有的模型，通过周期性批量任务来触发检查，对已有的服务打
同时业务系统的任务要对接到任务系统，这和业务系统的开发语言无关，和系统架
对于任务调度，很多朋友可能会有种熟悉的陌生感，其实我们可以这样理解，运维
而巡检任务其实是和报警、监控形成三位一体的关系，我们的标签管理是穿插其中，
任务系统的执行模式分为异步任务和定时任务，通过 API 触发，推送到任务执行队
（4）使用任务调度，采用异步执行的方式，等待时间可能是秒级，而实际上任务在
（3）写一个程序，通过中控来触发，那么我们需要等待10~20分钟，显然效率要高得多
（2）每台服务器上的2个任务并行执行，那么需要等待16个小时，显然也不合理。
（1）串行执行，那么我们需要等待33个小时，显然我们不会这么做。
标签模型是管理的核心部门，我们需要通过模型的设计来创建多个标签，有些标签
其中“标签管理”是基本的标签增删改查，能够支持多个维度的标签管理。
任务调度的整体设计和规划
任务调度
---
## Page 507
们是通过接口的方式进行对接，彼此都是独立的，而且没有强关联，如下图13-29所示。
列。执行的流程图如下图13-28所示。
如果对任务调度做一个整体的规划，其实可以把它拆分为任务系统和调度系统，它
任务接入API
任务并行调度
图13-29
图13-28
服务器2
任务查询portal
业务属性配置
时间调度
服务器3
第13章MySQL运维基础架构设计|485
任务路由和拆解
---
## Page 508
486丨MySQLDBA工作笔记：数据库管理、架构优化与运维开发
们理解任务调度和使用是非常方便的。下图13-30 是Celery 的一个架构图。
容易扩展，非常适合用于构建分布式的 Web 服务，在github 上有超过1万颗星，对于我
13.3.2
加通用的服务，是需要下很大功夫的。
成最基本的功能，然后根据需要在这个基础上进行定制，当然要把任务调度作为一种更
整体的规划来看，
是清晰可理解的能力。
支撑并发1万个任务，根据业务负载进行调度资源的扩缩容，这些无论对内还是对外都
以实现调度层的扩缩容。所以对于任务调度的定位应该是具备什么样的支撑能力，比如
本执行层的基础功能，
执行，具体的任务执行时可以基于执行代理、中控或者客户端下发完成，这个是属于脚
度层之后，其实就是进入了任务队列中，可以实现任务的并发执行或者基于时间的调度
是没有触发的，任务的触发有异步和定时两种模式，是通过调度模块来触发，推送到调
目前行业里也有一些开源的方案，比如基于Celery 的任务队列或者是Quartz等，从
：因为调度层不需要去关心业务意义，所以调度层的工作是很容易扩展的，即完全可
Celery 是基于 Python 语言编写的分布式的任务调度模块，它有着简明的 API，并且
运维系统可以通过API的方式接入任务调度模块，即任务注册，这个过程中任务还
 Celery 技术快速入门
异步任务
服务器1
其实我们完全把它们当做一个简单的工具来对待，即让它们只负责完
发送
监控
远程控制Ansible
Broker-Redis
服务器2
任务队列
API调用
图13-30
服务器n
发送
Celery Beat
监控
定时任务
调用
存储一
任务追踪及控制
Backend-Redis
结果存储
Flower
---
## Page 509
即可。 settings.py 的配置如下：
使用自带的 Broker服务也是可以的。
RabbitMQ。
进行补充。
的任务结果，在任务执行结束后，对于任务的跟踪和控制可以通过第三方开源项目 Flower
进程会持续监视队列中是否有需要处理的新任务。Task result store 用来存储 Worker 执行
节点中。消息队列可以基于 Redis 或者 RabbitMQ 等，消息队列的输入是任务，Worker
和任务执行结果存储（Taskresult store）。
如果使用RabbitMQ，我们需要单独部署安装这个消息队列，使用 yum installrabbitmq-server
如果启用自带的配置，setings.py 的配置如下：
django-admin startapp celery_app
django-admin startproject django_celery
其中，Worker 是Celery 提供的任务执行的单元，Worker 并发的运行在分布式的系统
·使用第三方消息队列
BROKER_URL = 'django://localhost:8000//'
INSTALLED_APPS =（
·使用自带的消息队列（Broker 服务）
在这里需要说明的是，如果我们不用第三方消息队列，如 Redis，RabbitMQ 的话，
（3）配置消息队列
（2）初始化一个应用
（1）创建一个项目
在 Django 中 Celery 是自带的，我们的测试是基于 Django Celery，消息队列是基于
>pip listlgrep celery 
首先我们需要确认 Celery 已正常安装。
案例13-2：配置一个简单的 Celery 任务
接下来我们通过一个简单的测试来体验下 Celery的功能。
Celery 的架构由三部分组成：消息中间件（Message broker）、任务执行单元（Worker）
celery_app',
django.contrib.sessions'
'django.contrib.admin'
ango.con
itrib
.contenttypes',
第13章MySQL运维基础架构设计|487
---
## Page 510
488丨MySQLDBA工作笔记：数据库管理、架构优化与运维开发
以了。
使用命令 python manage.py syncdb，这个过程会提示你创建一个超级用户，照做就可
（6）配置DB的信息
def xsum(numbers):
@shared_
def
@shared_task
from
（5）配置 tasks.py，配置一些任务，属于自定义配置。
@app.task(bind=True)
from
（4）配置 celery.py
CELERY_RESULT_BACKEND
BROKER_URL=
#
import djcelery
pickie the object when using Windows.
return
time.sleep(10)
debug_task(self)
djcelery'
P
django.contrib.contenttypes',
future_
the
future
ango.
jango.contrib
sum(numbers)
task
default
y）：
y）：
import shared_task
from
contrib
K
import absolute_import
import absolute_import
{0!r}'.format(self.request))
tasks(lambda:
import settings
Django
.auth'
sessions
/guest@localhost//'
，创建文件 celery.py，属于常规配置。
settings
'amqp://guest@localhost//'
the worker will not have to
module forthe'celery'
---
## Page 511
Crontab解决不了以下的几类问题：
13.3.3
我们可以很轻松地应用到我们的应用场景中。
后才能启动 Celery服务。
对于定时任务，系统层面使用Crontab 是比较普遍的，但是从使用角度来说，系统的
通过上面的测试，我们可以基本了解Celery 的一些使用方式，对于任务调度来说，
http://127.0.0.1:5555/
python manage.py celery flower
pip install flower
（10）启用flower服务
[2018-01-08
访问端口。
启动服务。
首先需要安装flower，
查看Worker的日志信息如下：
>>
>> mul.delay(5,2)
这个时候如果使用 delay、add 的方式，就会进入消息队列。
10
>>> from celery_app.tasks import *
（9）我们开启shell交互窗口，做一些任务的测试。
>python manage.py celery worker -l info
export C_FORCE_ROOT=test
打开另外一个窗口，启动Celery 的服务，记得要先设置变量C_FORCE_ROOT，然
（8）启动Celery服务
python manage.py runserver
>>>
（7）启动服务
builtins_
平滑对接Crontab和 Celery的方案
add.de1ay(2,3)
14:34:47,505:
7d647a77-8344-4813-bc15-791ed1a8c3d3>
,'absolute_import', 'add', 'mul', 'shared_task','xsum']
，如下：
14:34:47,507:
INFO/MainProcess]
第13章MySQL运维基础架构设计|489
Received
succeeded
task:
Task
in
---
## Page 512
490丨MySQLDBA工作笔记：数据库管理、架构优化与运维开发
的几个顾虑：
A 成功后才可执行，对此只能手工管理维护。
就会对500台服务器做一个统一的调度，分成5 组，每组的任务会根据数据量和时间来
务同时开始，不够优雅，如果我们可以限定并行度，比如同时执行备份的任务有5个，
那么如何合理的规划备份任务呢，目前很容易看到的一种瓶颈就是瞬间有500个备份任
Crontab的维护就会方便许多。
直接切换到Crontab，那么这个事情就可以说可控了。
统Crontab 做试点，然后逐步开放。
无缝的把Crontab 切换到Celery 中，那么这个事情的意义就很明显了，我们可以选几个系
义不是很大。
执行状态（加入标识位），我想这对很多人来说，应该是可以接受的，缺点是这么做，意
数据信息保存下来，然后对任务的执行情况做管理，比如查看执行的任务日志，任务的
为瓶颈)，比较苦恼。
迁入平台，但是感觉不一定可控（尤其网络不稳定的时候，平时不是问题的事情都会成
实有很多的亮点可以做，我们换个角度来看，如果你有很多的任务，现在饱受困扰，想
行，女
那么我们再加一层砝码，如果平台支持任务调度，比如使用了Celery，我们如果可以
如果有500个任务要执行，比如说备份任务，有的数据库有50G，有的只有500M，
如果我们在平台中修改系统调度任务的时间，系统中的Crontab会联动变化，那么
如果网络不稳定，而你仍需要在系统层继续使用Crontab，如果可以由Celery的任务
（1）如果调度平台出问题，所有的任务都会失败，影响巨大。
（2）没有任务管理功能，如果多个Crontab 任务间存在依赖，比如任务B必须等任务
如果系统已有Crontab，依然可以继续使用，接入平台只是针对已有的crontab，把元
现在我们来聊一下这个事情，看看怎么能把那些顾虑和问题都解决掉。
所以任务调度模块的建设是一把双刃剑，需要我们合理的把握度，在技术方向上其
（4）如果出现临时的维护窗口，系统的Crontab 和平台的调度任务都是整段垮掉。
如何合理的规划任务的执行情况，目前的很多解决方案还做不到灵活的控制和调度。
（3）任务的调度不够优雅，如果任务多，比如有500个任务，需要在1:00~3:00之间执
（2）一旦迁入平台，就不能再回头了，除非手工干预调整。
而如果接入任务调度平台，会解决掉以上绝大多数的问题，不过很多人也会有以下
（6）因为系统原因，很可能无法触发定时任务。
（5）没法监控任务的执行情况。
（4）不具备调度功能。
（3）没法设置任务的截止时间。
（1）任务的时间精度不够，默认的粒度是分钟。
---