title:Black Widow: Blackbox Data-driven Web Scanning
author:Benjamin Eriksson and
Giancarlo Pellegrino and
Andrei Sabelfeld
2
2
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
Black Widow: Blackbox Data-driven Web Scanning
Benjamin Eriksson∗, Giancarlo Pellegrino†, Andrei Sabelfeld∗
∗Chalmers University of Technology
†CISPA Helmholtz Center for Information Security
Abstract—Modern web applications are an integral part of
our digital lives. As we put more trust in web applications,
the need for security increases. At the same time, detecting
vulnerabilities in web applications has become increasingly
hard, due to the complexity, dynamism, and reliance on
third-party components. Blackbox vulnerability scanning is
especially challenging because (i) for deep penetration of web
applications scanners need to exercise such browsing behavior
as user interaction and asynchrony, and (ii) for detection of
nontrivial injection attacks, such as stored cross-site scripting
(XSS), scanners need to discover inter-page data dependencies.
This paper illuminates key challenges for crawling and
scanning the modern web. Based on these challenges we identify
three core pillars for deep crawling and scanning: navigation
modeling, traversing, and tracking inter-state dependencies.
While prior efforts are largely limited to the separate pillars, we
suggest an approach that leverages all three. We develop Black
Widow, a blackbox data-driven approach to web crawling and
scanning. We demonstrate the effectiveness of the crawling
by code coverage improvements ranging from 63% to 280%
compared to other crawlers across all applications. Further,
we demonstrate the effectiveness of the web vulnerability
scanning by featuring no false positives and ﬁnding more cross-
site scripting vulnerabilities than previous methods. In older
applications, used in previous research, we ﬁnd vulnerabilities
that the other methods miss. We also ﬁnd new vulnerabili-
ties in production software, including HotCRP, osCommerce,
PrestaShop and WordPress.
Keywords-web application scanning; security testing; cross-
site scripting; XSS; web crawling;
I. INTRODUCTION
Ensuring the security of web applications is of paramount
importance for our modern society. The dynamic nature
of web applications, together with a plethora of different
languages and frameworks, makes it particularly challenging
for existing approaches to provide sufﬁcient coverage of the
existing threats. Even the web’s main players, Google and
Facebook, are prone to vulnerabilities, regularly discovered
by security researchers. In 2019 alone, Google’s bug bounty
paid $6.5 million [1] and Facebook $2.2 million [2], both
continuing the ever-increasing trend. Cross-Site Scripting
(XSS) attacks, injecting malicious scripts in vulnerable web
pages, represent the lion’s share of web insecurities. Despite
mitigations by the current security practices, XSS remains
a prevalent class of attacks on the web [3]. Google rewards
millions of dollars for XSS vulnerability reports yearly [4],
and XSS is presently the most rewarded bug on both
HackerOne [5] and Bugcrowd [6]. This motivates the focus
of this paper on detecting vulnerabilities in web applications,
with particular emphasis on XSS.
Blackbox web scanning: When such artifacts as the source
code, models describing the application behaviors, and
code annotations are available, the tester can use whitebox
techniques that look for vulnerable code patterns in the
code or vulnerable behaviors in the models. Unfortunately,
these artifacts are often unavailable in practice, rendering
whitebox approaches ineffective in such cases.
The focus of this work is on blackbox vulnerability
detection. In contrast
to whitebox approaches, blackbox
detection techniques rely on no prior knowledge about the
behaviors of web applications. This is the standard for
security penetration testing, which is a common method
for ﬁnding security vulnerabilities [7]. Instead, they acquire
such knowledge by interacting with running instances of web
applications with crawlers. Crawlers are a crucial component
of blackbox scanners that explore the attack surface of
web applications by visiting webpages to discover URLs,
HTML form ﬁelds, and other input ﬁelds. If a crawler fails
to cover the attack surface sufﬁciently, then vulnerabilities
may remain undetected, leaving web applications exposed
to attacks.
Unfortunately, having crawlers able to discover in-depth
behaviors of web applications is not sufﬁcient to detect
vulnerabilities. The detection of vulnerabilities often re-
quires the generation of tests that can interact with the web
application in non-trivial ways. For example, the detection
of stored cross-site scripting vulnerabilities (stored XSS),
a notoriously hard class of vulnerabilities [3], requires the
ability to reason about the subtle dependencies between the
control and data ﬂows of web application to identify the
page with input ﬁelds to inject the malicious XSS payload,
and then the page that will reﬂect the injected payload.
Challenges: Over the past decade, the research community
has proposed different approaches to increase the coverage
of the attack surface of web applications. As JavaScript has
rendered webpages dynamic and more complex, new ideas
were proposed to incorporate these dynamic behaviors to
ensure a correct exploration of the page behaviors (j ¨Ak [8])
and the asynchronous HTTP requests (CrawlJAX [9], [10]).
Similarly, other approaches proposed to tackle the com-
plexity of the server-side program by reverse engineering
(LigRE [11] and KameleonFuzz [12]) or inferring the state
© 2021, Benjamin Eriksson. Under license to IEEE.
DOI 10.1109/SP40001.2021.00022
1125
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
(Enemy of the State [13]) of the server, and then using the
learned model to drive a crawler.
Unfortunately, despite the recent efforts, existing ap-
proaches do not offer sufﬁcient coverage of the attack
surface. To tackle this challenge, we start from two observa-
tions. First, while prior work provided solutions to individual
challenges, leveraging their carefully designed combination
has the potential to signiﬁcantly improve the state of the
art of modern web application scanning. Second, existing
solutions focus mostly on handling control ﬂows of web
applications, falling short of taking into account intertwined
dependencies between control and data ﬂows. Consider, for
example, the dependency between a page to add new users
and the page to show existing users, where the former
changes the state of the latter. Being able to extract and
use such an inter-page dependency will allow scanners to
explore new behaviors and detect more sophisticated XSS
vulnerabilities.
Contributions: This paper presents Black Widow, a novel
blackbox web application scanning technique that identiﬁes
and builds on three pillars: navigation modeling, traversing,
and tracking inter-state dependencies.
Given a URL, our scanner creates a navigation model
of the web application with a novel JavaScript dynamic
analysis-based crawler able to explore both the static struc-
ture of webpages, i.e., anchors, forms, and frames, as well
as discover and ﬁre JavaScript events such as mouse clicks.
Also, our scanner further annotates the model to capture the
sequence of steps required to reach a given page, enabling
the crawler to retrace its steps. When visiting a webpage, our
scanner enriches our model with data ﬂow information using
a black-box, end-to-end, dynamic taint tracking technique.
Here, our scanner identiﬁes input ﬁelds, i.e., taint source,
and then probe them with unique strings, i.e., taint values.
Later, the scanner checks when the strings re-surface in the
HTML document, i.e., sinks. Tracking these taints allows us
to understand the dependencies between different pages.
We implement our approach as a scanner on top of a
modern browser with a state-of-the-art JavaScript engine.
To empirically evaluate it, both in terms of coverage and
vulnerability detection, we test
it on two sets of web
applications and compare the results with other scanners.
The ﬁrst set of web applications are older well-known
applications that have been used for vulnerability testing
before, e.g. WackoPicko and SCARF. The second set con-
tains new production applications such as CMS platforms
including WordPress and E-commerce platforms including
PrestaShop and osCommerce. From this, we see that our
approach improves code coverage by between 63% and
280% compared to other scanners across all applications.
Across all web applications, our approach improves code
coverage by between 6% and 62%, compared to the sum of
all other scanners. In addition, our approach ﬁnds more XSS
vulnerabilities in older applications, i.e. phpBB, SCARF,
Vanilla and WackoPicko, that have been used in previous
research. Finally, we also ﬁnd multiple new vulnerabilities
across production software including HotCRP, osCommerce,
PrestaShop and WordPress.
Finally, while most scanners produce false positives,
Black Widow is free of false positives on the tested appli-
cations thanks to its dynamic veriﬁcation of code injections.
In summary, the paper offers the following contributions.
• We identify unsolved challenges for scanners in modern
web applications and present them in Section II.
• We present our novel approaches for ﬁnding XSS
vulnerabilities using inter-state dependency analysis and
crawling complex workﬂows in Section III.
• We implement and share the source code of Black
Widow1
• We perform a comparative evaluation of Black Widow
on 10 popular web applications against 7 web application
scanners.
• We present our evaluation in Section IV showing that
our approach ﬁnds 25 vulnerabilities, of which 6 are
previously unknown in HotCRP, osCommerce, PrestaShop
and WordPress. Additionally, we ﬁnd more vulnerabilities
in older applications compared to other scanners. We also
improve code coverage on average by 23%.
• We analyze the results and explain the important fea-
tures required by web scanners in Section V.
II. CHALLENGES
Existing web application scanners suffer from a number
of shortcomings affecting their ability to cope with the
complexity of modern web applications [14], [15]. We
observe that state-of-the-art scanners tend to focus on sepa-
rate challenges to improve their effectiveness. For example,
j ¨Ak focuses on JavaScript events, Enemy of the State
on application states, LigRE on reverse engineering and
CrawlJAX on network requests. However, to successfully
scan applications our insight is that these challenges must
be solved simultaneously. This section focuses on these
shortcomings and extracts the key challenges to achieve high
code coverage and effective vulnerability detection.
High code coverage is crucial for ﬁnding any type of
vulnerability as the scanner must be able to reach the code
to test it. For vulnerability detection, we focus on stored
XSS as it is known to be difﬁcult to detect and a category
of vulnerabilities poorly covered by existing scanners [14],
[15]. Here the server stores and uses at a later time untrusted
inputs in server operations, without doing proper validation
of the inputs or sanitization of output.
A web application scanner tasked with the detection of
subtle vulnerabilities like stored XSS faces three major
1 Our implementation is available online on https://www.cse.chalmers.
se/research/group/security/black-widow/
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
1126
the scanner needs to model
challenges. First,
the vari-
ous states forming a web application, the connections and
dependencies between states (Section II-A). Second,
the
identiﬁcation of these dependencies requires the scanner to
be able to traverse the complex workﬂows in applications
(Section II-B). Finally, the scanner needs to track subtle
dependencies between states of the web application (Sec-
tion II-C).
A. Navigation Modeling
Modern web applications are dynamic applications with
an abundance of JavaScript code, client-side events and
server-side statefulness. Modeling the scanner’s interaction
with both server-side and client-side code is complicated
and challenging. Network requests can change the state of
the server while clicking a button can result in changes
to the DOM, which in turn generates new links or ﬁelds.
These orthogonal problems must all be handled by the
scanner to achieve high coverage and improved detection
rate of vulnerabilities. Consider the ﬂow in an example
web application in Figure 1. The scanner must be able
to model links, forms, events and the interaction between
them. Additionally, to enable workﬂow traversal, it must
also model the path taken through the application. Finally,
the model must support inter-state dependencies as shown
by the dashed line in the ﬁgure.
the navigation model
The state-of-the-art consists of different approaches to
navigation modeling. Enemy of the State uses a state ma-
chine and a directed graph to infer the server-side state.
However,
lacks information about
client-side events. In contrast, j ¨Ak used a graph with lists in-
side nodes, to represent JavaScript events. CrawlJAX moved
the focus to model JavaScript network requests. While these
two model client-side, they miss other important navigation
methods such as form submissions.
A navigation model should allow the scanner to efﬁciently
and exhaustively scan a web application. Without correct
modeling, the scanner will miss important resources or spend
too much time revisiting the same or similar resources. To
achieve this, the model must cover a multitude of methods
for interaction with the application,
including GET and
POST requests, JavaScript events, HTML form and iframes.
In addition, the model should be able to accommodate
dependencies. Client-side navigations, such as clicking a
button, might depend on previous events. For example, the
user might have to hover the menu before being able to click
the button. Similarly, installation wizards can require a set
of forms to be submitted in sequence.
With a solution to the modeling challenge,
the next
challenge is how the scanner should use this model, i.e. how
should it traverse the model.
B. Traversing
To improve code coverage and vulnerability detection, the
crawler component of the scanner must be able to traverse
1127
the application. In particular, the challenge of reproducing
workﬂows is crucial for both coverage and vulnerability
detection. The challenges of handling complex workﬂows
include deciding in which order actions should be performed
and when to perform possibly state-changing actions, e.g.
submitting forms. Also, the workﬂows must be modeled at
a higher level than network requests as simply replaying
requests can result in incorrect parameter values, especially
for context-dependent value such as a comment ID. In
Figure 1, we can observe a workﬂow requiring a combina-
tion of normal link navigation, form submission and event
interaction. Also, note that the forms can contain security
nonces to protect against CSRF attacks. A side effect of
this is that the scanner can not replay the request and just
change the payload, but has to reload the page and resubmit
the form.
The current state-of-the-art focuses largely on navigation
and exploration but misses out on global workﬂows. Both
CrawlJAX and j ¨Ak focused on exploring client-side events.
By exploring the events in a depth-ﬁrst fashion, j ¨Ak can
ﬁnd sequences of events that could be exploited. However,
these sequences do not extend across multiple pages, which
will miss out on ﬂows. Enemy of the State takes the oppo-
site approach and ignores traversing client-side events and
instead focuses on traversing server-side states. To traverse,
they use a combination of picking links from the previous
response and a heuristic method to traverse edges that are the
least likely to result in a state change, e.g. by avoiding form
submission until necessary. To change state they sometimes
need to replay the request from the start. Replaying requests
may not be sufﬁcient as a form used to post comments
might contain a submission ID or view-state information
that changes for each request. Due to the challenge of
reproducing these ﬂows, their approach assumes the power
to reset the full application when needed, preventing the
approach from being used on live applications.
We note that no scanner handles combinations of events
and classic page navigations. Both j ¨Ak and CrawlJAX
traverse with a focus on client-side state while Enemy of
the State focus on links and forms for interaction. Simply
combining the two approaches of j ¨Ak and Enemy of the
State is not trivial as their approaches are tailored to their
goals. Enemy of the State uses links on pages to determine
state changes, which are not necessarily generated by events.
Keeping the scanner authenticated is also a challenge.
Some scanners require user-supplied patterns to detect au-
thentication [16], [17], [18]. j ¨Ak authenticates once and
then assumes the state is kept, while CrawlJAX ignores
it altogether. Enemy of the State can re-authenticate if
they correctly detect the state change when logging out.
Once again it is hard to ﬁnd consensus on how to handle
authentication.
In addition to coverage, traversing is important for the
fuzzing part of the scanner. Simply exporting all requests
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
to a standalone fuzzer is problematic as it results in loss
of context. As such, the scanner must place the application
in an appropriate state before fuzzing. Here some scanners
take the rather extreme approach of trying to reset the entire
web application before fuzzing each parameter [13], [11],
[12]. j ¨Ak creates a special attacker module that loads a URL
and then executes the necessary events. This shows that in
order to fuzz the application in a correct setting, without
requiring a full restart of the application, the scanner must
be able to traverse and attack both server-side and client-side
components.
Solving both modeling and traversing should enable the
scanner to crawl the application with improved coverage,
allowing it to ﬁnd more parameters to test. The ﬁnal chal-
lenge, particularly with respect to stored XSS, is mapping
the dependencies between different states in the application.
C. Inter-state Dependencies
It is evident that agreeing on a model that ﬁts both client-
side and server-side is hard, yet
important. In addition,
neither of the previous approaches are capable of modeling
inter-state dependencies or general workﬂows. While Enemy
of the State model states, they miss the complex workﬂows
and the inter-state dependencies. The model j ¨Ak uses can
detect workﬂows on pages but fails to scale for the full
application.
A key challenge faced by scanners is how to accu-
rately and precisely model how user inputs affect web
applications. As an example, consider the web application
workﬂow in Figure 1 capturing an administrator register-
ing a new user. In this workﬂow, the administrator starts
from the index page (i.e., index.php) and navigates
to the login page (i.e., login.php). Then, the admin-
istrator submits the password and lands on the adminis-
trator dashboard (i.e., admin.php). From the dashboard,
the administrator reaches the user management page (i.e.,
admin.php#users), and submits the form to register a
new user. Then, the web application stores the new user
data in the database, and, as a result of that, the data of
the new user is shown when visiting the page of existing
users (i.e., view_users.php). Such a workﬂow shows
two intricate dependencies between two states of the web
application: First, an action of admin.php#users can
cause a transition of view_users.php, and second, the
form data submitted to admin.php#users is reﬂected in
the new state of admin.php#users.
To detect if the input ﬁelds of the form data are vulnerable
to, e.g., cross-site scripting (XSS), a scanner needs to inject
payloads in the form of admin.php#users and then
reach view_users.php to verify whether the injection
was successful. Unfortunately, existing web scanners are not
aware of these inter-state dependencies, and after injecting
payloads,
they can hardly identify the page where and
whether the injection is reﬂected.
III. APPROACH
Motivated by the challenges in Section II, this section
presents our approach to web application scanning. The three
key ingredients of our approach are edge-driven navigation
with path-augmentation, complex workﬂow traversal, and
ﬁne-grained inter-state dependency tracking. We explain
how we connect these three parts in Algorithm 1. In addition
to the three main pillars, we also include a section about the
dynamic XSS detection used in Black Widow and motivate
why false positives are improbable.
Algorithm 1 takes a single target URL as an input. We
start by creating an empty node, allowing us to create an
initial edge between the empty node and the node containing
the input URL. The main loop picks an unvisited edge from
the navigation graph and then traverses it, executing the nec-
essary workﬂows as shown in Algorithm 2. In Algorithm 2,
we use the fact that each edge knows the previous edge.
The isSafe function in Algorithm 2 checks if the type
of action, e.g. JavaScript event or form submission, is safe.
We consider a type to be safe if it is a GET request, more
about this in Section III-B. Once the safe edge is found