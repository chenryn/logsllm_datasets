boundaries. For this reason, a frame classiﬁed as a boundary
is considered as correct if it occurs within n frames of an
actual boundary; likewise, it is incorrect if there are no actual
boundaries within n frames. Table II summarizes, for each
dialect, our segmentation performance for n = 1 and n = 2.
For the sake of comparison, we also note that state-of-the-art
classiﬁers (operating on the raw acoustic signal) are able to
recall approximately 80% (again, within 20ms) of phoneme
boundaries in TIMIT [18, 44] with error rates similar to our
own. Unfortunately, the comparison is not direct: our labels
are necessarily at the granularity of frames (each 20ms),
rather than samples, which means that the within-n-frame
requirement for agreement is looser than the within-20ms
requirement.
The results in Table II show that our performance is
on par with these other techniques. More importantly, the
imprecision in the transcription boundaries does not neg-
atively impact the performance of the next stage of our
approach since the frames in question, i.e., the beginning
and ending frames of each phoneme, are precisely those that
will contribute the most variance to a phoneme model. In
other words, the transition frames are likely to incorporate
a signiﬁcant amount of noise, due to their proximity to sur-
rounding phonemes, and are therefore unlikely to be useful
for classifying phonemes. It is for exactly this reason that
we explicitly exclude the transition frames in the phoneme
classiﬁcation stage that follows. Finally, for the remainder of
this paper we make the simplifying assumption that phoneme
boundaries can be recognized correctly; this assumption is
revisited in Section VI.
B. Classifying Phonemes (Stage )
We remind the reader that our overall approach requires
that we segment a sequence of encrypted packet lengths
into subsequences corresponding to individual phonemes,
and then classify these subsequences based on empirical
models derived from labeled training data. We therefore have
a classiﬁcation problem where the classes of interest are the
various phonemes.
For classiﬁcation, we employ a combination of two sys-
tems: one context-dependent, wherein the labeling of a
segment is dependent on the labelings of its neighbors, and
another context-independent, wherein a single segment is
considered in isolation. We combine these two approaches
in order to leverage the strengths of each. Our context-
dependent classiﬁer is also based on maximum entropy mod-
eling, while the context-independent classiﬁer is based on
proﬁle hidden Markov modeling. Proﬁle HMMs have been
used widely in both the biological sequence analysis [16]
and speech recognition communities [29].
Aside from the ability to incorporate contextual informa-
tion, maximum entropy modeling is discriminative. Discrim-
inative models are often used for classiﬁcation tasks because
they model only the parameters of interest for classiﬁcation
and thus can often encode more information. Hidden Markov
models, on the other hand, are generative models. Generative
models are sometimes preferable to discriminative models
because they model the entire distribution over examples
for a given class rather than just the information necessary
to discriminate one class from another.
To combine the two models, we utilize a form of Bayesian
inference to update the posterior given by the maximum
entropy classiﬁer with the “evidence” given by the proﬁle
HMM classiﬁer. The updated posterior is then passed to a
language model, as described below. By utilizing both types
of models we enjoy increased classiﬁcation accuracy while
providing input to the language model with a valid statistical
interpretation. Next, we discuss each stage in turn.
Maximum Entropy Discrimination of Phonemes
We discriminate between phonemes in a manner simi-
lar to the segmentation process described in Section V-A.
Speciﬁcally, we deﬁne a new set of feature templates over
sequences of phonemes (which are themselves composed
of sequences of frame sizes). For pedagogical reasons, the
speciﬁcs are given in Table III and an example feature is
illustrated in Figure 5.
Feature templates 1-3 capture the exact frame sequence of
the current and surrounding phonemes to identify phonemes
9
Proﬁle HMM Modeling of Phonemes
To provide generative models of the various phonemes,
we train a proﬁle HMM for each. A proﬁle HMM is a
hidden Markov model with a speciﬁc topology that encodes
a probability distribution over ﬁnite sequences of symbols
drawn from some discrete alphabet. In our case, the alphabet
is the different sizes at which a speech frame may be
encoded; in Speex’s wideband VBR mode, there are 19 such
possibilities. Given the topology of a hidden Markov model,
we need to estimate the parameters of the model for each
set of sequences. Towards this end, we utilize a well-known
algorithm due to Baum et al. [4] that iteratively improves the
model parameters to better represent the example sequences.
Classiﬁcation
To label an observed sequence of packet sizes, we ﬁnd the
posterior probability p(r|q), where q represents the observed
sequence of frame sizes, for each class label r. For the
standalone maximum entropy classiﬁer, the output for a
given observation and label is an estimate of the desired
quantity. For the proﬁle HMM classiﬁer, we calculate, using
Bayesian inference, the posterior p(r|q) = p(r)p(q|r) using
the likelihood5 p(q|r), given by the proﬁle HMM. This “up-
dates” a prior probability p(r) with the new “evidence” from
the proﬁle HMM. For the stand-alone classiﬁer evaluation,
we estimate the prior p(r) as the proportion of examples
belonging to the class in our training data. When using
both the proﬁle HMM and maximum entropy classiﬁers in
conjunction, we use the estimated p(r|q) from the maximum
entropy model as the prior p(r). In all cases, we choose the
label whose model has the maximum posterior probability
as the predicted label for a given sequence. These posterior
probabilities also give a probability distribution over candi-
date labels for each phoneme in an utterance; these serve as
the language model input.
Enhancing Classiﬁcation using Language Modeling
Lastly,
in order to incorporate contextual
information
on surrounding phonemes, we apply a trigram language
model using the SRILM language modeling toolkit [51].
In particular, we train a trigram language model over both
phonemes and phoneme types (e.g., vowels and stops).
We disambiguate between candidate labels by ﬁnding the
maximum likelihood sequence of labels given both the esti-
mated distributions output by the classiﬁer and the phonetic
language model.
Evaluation
Our preliminary results show that we can correctly clas-
sify 45% of phonemes in a 10-fold cross-validation exper-
iment on the New England dialect.6 For this experiment,
5The likelihood given by an HMM is scaled by the marginal p(q).
6For brevity, we omit the other dialects as the results do not differ
signiﬁcantly.
10
Figure 5. An example instantiation of feature template 10 which illustrates
how the template models the presence of common trigrams.
Phoneme Classiﬁcation Feature Templates
qi (i.e., the current phoneme’s frame size sequence)
qi−1 (i.e., the previous phoneme’s frame size sequence)
qi+1 (i.e., the next phoneme’s frame size sequence)
qi, excluding the ﬁrst and the last frames
qi−1, excluding the ﬁrst and the last frames
length of qi (in frames)
length of qi−1 (in frames)
frequency of frame size n in qi
bigram b of frame sizes is in qi, for top 100 bigrams
trigram t of frame sizes is in qi, for top 100 trigrams
bigram b of frame sizes is in qi−1, for top 100 bigrams
trigram t of frame sizes is in qi−1, for top 100 trigrams
bigram b of frame sizes is in qi+1, for top 100 bigrams
trigram t of frame sizes is in qi+1, for top 100 trigrams
1
2
3
4
5
6
7
8
9
10
11
12
13
14
FEATURE TEMPLATES FOR THE MAXIMUM ENTROPY PHONEME
CLASSIFIER. WE DENOTE AS qi THE SEQUENCE OF FRAME SIZES FOR
THE iTH PHONEME.WE LIMIT THE NUMBER OF n-GRAMS TO 100 FOR
Table III
PERFORMANCE REASONS.
that frequently encode as exactly the same frame sequence.
Feature templates 4 and 5 encode similar information, but
drop the ﬁrst and last frames in the sequence in accordance
with our earlier hypothesis (see Section V-A) that
the
beginning and ending frames of the phoneme are the most
variable. Feature templates 6 and 7 explicitly encode the
length of the current and previous phonemes since some
types of phonemes are frequently shorter (e.g., glides) or
longer (e.g., vowels) than others. Feature template 8 captures
the frequency of each possible frame size in the current
sequence. Feature templates 9-14 encode the presence of
each of the 100 most frequent frame size bigrams or trigrams
observed in the training data; we limit the number of bigrams
and trigrams to maintain manageable run-time performance.
Finally, since we later incorporate high-level contextual
information (such as neighboring phonemes) explicitly with
a language model, we do not attempt
to leverage that
information in the classiﬁcation model.
qiMost Frequently Observed Trigramsin Training CorpusSequence of Phonemescontainscontainswe operate on input with perfectly segmented phonetic
boundaries so as to provide a baseline for our classiﬁers
when evaluated independently from the other stages in our
method. As can be seen from Figure 6, the combination of
the proﬁle HMM and maximum entropy classiﬁers with the
language model outperforms the individual classiﬁers.
While this classiﬁcation performance might sound lack-
luster, these results are quite surprising given the limited
context we operate under (i.e., packet sizes only). For
instance, recent approaches working directly on the acoustic
signal report 77% accuracy on the TIMIT dataset in the
context-dependent case (which corresponds roughly to our
approach after application of the language model). In the
context-independent case (analogous to our proﬁle HMM
classiﬁcation approach without the language model), accu-
racy rates as high as 67% have been achieved [26] on the
TIMIT dataset. Similarly, expert human transcribers achieve
rates only as high as 69% [36].
actual boundaries, we match the surrounding phonemes with
all possible phonemes and pairs of phonemes which can
begin or end words, and remove potential word breaks which
would result in invalid word beginnings or endings.
We then perform an additional step whereby we use a
pronunciation dictionary to ﬁnd valid word matches for
all contiguous subsequences of phonemes. For each such
subsequence, we insert word breaks at the positions that are
consistent across all the matches. For example, suppose the
sequence [InOIliôæg] (‘an oily rag’) has the following three
possible segmentations:
◦ [In OIli ôæg] (‘an oily rag’)
◦ [In OIl i ôæg] (‘an oil E. rag’)
◦ [In O Il i ôæg] (‘an awe ill E. rag’)
Since these choices have two words in common, we
segment the phrase as [In OIli ôæg].
Dialect
New England
Northern
North Midland
South Midland
Southern
New York City
Western
Army Brat
Precision
0.7251
0.7503
0.7653
0.7234
0.7272
0.7441
0.7298
0.7277
Recall
0.8512
0.8522
0.8569
0.8512
0.8455
0.8650
0.8419
0.8461
WORD BREAK INSERTION PRECISION AND RECALL
Table IV
The results of a 10-fold cross-validation experiment are
given in Table IV. Overall, we achieve average precision and
recall of 73% and 85%, respectively. Very recent results,
however, by Blanchard et al. [8] and Hayes and Wilson
[25] suggest that accuracy above 96% can be achieved using
more advanced techniques than implemented here. Due to
time and resource constraints, we make the simplifying
assumption that word breaks can be correctly recognized.
We revisit this assumption in Section VI.
D. Identifying Words via Phonetic Edit Distance (Stage )
The ﬁnal task is to convert the subsequences of phonemes
into English words. To do so, we must
identify words
that best match the pronunciation dictated by the recovered
phonemes. Towards this end, we design a novel metric of
phonetic distance based on the difference in articulatory fea-
tures (i.e., the associated physiological interactions discussed
in Section II) between pairs of phonemes. Our approach has
some similarities to ideas put forth by Oakes [45], which
itself builds upon the work of Gildea and Jurasky [22] and
Zobel and Dart [58, 59]. Oakes [45] proposes a phonetically-
based alignment algorithm, though there is no notion of
relative distance between various places or manners of
articulation. In Zobel and Dart [59], the distances between
phonemes are handcrafted, and their matching algorithm
considers only the single most likely pronunciation.
Figure 6. Phoneme classiﬁcation accuracy on the New England dialect for
the proﬁle HMM and maximum entropy classiﬁers alone, in combination,
and with the language model applied.
C. Segmenting Phoneme Streams into Words (Stage )
In this stage, our task is to identify likely word boundaries
from the stream of classiﬁed phonemes. To do so, we follow
the methodology suggested by Harrington et al. [24] that,
until very recently, was among the best approaches for word
boundary identiﬁcation. We also extend their approach to
incorporate an additional step that makes use of a pronun-
ciation dictionary.
Harrington et al. identify word breaks with a two-step
process. The ﬁrst step consists of inserting potential word
breaks into the sequence of phonemes in positions that would
otherwise produce invalid phonemic triples, i.e., triples that
do not occur within valid words in English. Each such iden-
tiﬁed triple then causes the insertion of a pair of potential
word breaks, one between each pair of phonemes in the
triple. To resolve which of the potential word breaks are
11
In our approach, we deﬁne the distance between a vowel
and a consonant as one unit, with a few exceptions: we
assign a cost of 0 for converting an [i] to a [j] (or vice-
versa) as well as for converting a [u] to a [w]. We do so
because [w] and [j] are what are known as semi-vowels, and
are essentially very short realizations of their corresponding
vowels. Moreover, we assign a cost of 0 for [R] (i.e., ﬂap
‘r’) and [t], as well as for [R] and [d]. This is because [R]
is an allophone of [t] and [d]. Hence, we would like such
minor phonetic alterations to have little effect on the distance
between two pronunciations.
To compare two sequences of phonemes, we use the
Levenshtein distance with insertions and deletions weighted
at one unit and edits weighted according to their phonetic
distance as deﬁned above. Table V gives example word
comparisons along with their primary differences (in terms
of articulartory processes).
In order to determine the optimal values for the insertion
and deletion weights for our phonetic edit distance metric,
we performed a simple parameter space exploration. We
hypothesized that the absolute insertion and deletion costs
were less signiﬁcant than the difference between them. As
such we tuned based on two parameters, base cost and offset.
Each insertion costs the base cost plus half the offset and
each deletion costs the base cost minus half the offset. The
effectiveness of each set of parameters is shown in Figure 8.
Somewhat surprisingly, a base cost of 1.0 and offset of
0.0 (corresponding to insertion and deletion weights of 1.0)
provided the highest average word accuracy.
Figure 7.
Illustration of distance between consonants [f], [T], and [w].
To measure the distance between two vowels or two