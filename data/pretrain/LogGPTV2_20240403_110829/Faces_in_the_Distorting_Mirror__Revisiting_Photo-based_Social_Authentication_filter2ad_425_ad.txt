per depicted friend), and correctly identify both tags to pass
the challenge. To calculate the maximum success rate the
attacker can achieve, the attack conducts 5◦ rotations which
produce the best results, even though the processing time
is unrealistic. Table 5 shows the results. With a 0.6 alpha
level, CCOEFF fails to pass any challenges, CCORR passes 15
and SQDIFF passes 10 challenges, while processing a photo
requires ∼ 401.5 sec. We create three sets of 250 photos
with 3 tags, where one algorithm fails and the rest solve a
single challenge, requiring over ∼ 663.6 seconds. Thus, we
reduce the attacker’s success rate to 0.4%, while requiring
four orders of magnitude more processing eﬀort on his part.
We also explored the possibility of combining the results of
the 3 methods for achieving better success rates, i.e., compare
the output of each method and select the photo that receives
the highest conﬁdence out of the 3. This, however, is infea-
sible because in our experiments the “conﬁdence” returned
by CCORR is always higher than the one by CCOEFF, even
when CCORR’s identiﬁcation is wrong. Also, SQDIFF returns a
diﬀerential result which is not comparable to the other two.
Face Recognition attack. We also evaluate the robust-
ness of our approach against face detection and, by extension,
recognition. To explore how our tag selection and transforma-
tion process impacts face detection, we calculate the number
of faces detected in 3,487 “background” photos before and
after we transform the tags and superimpose them on the
background photo (no perspective transformation performed).
We ﬁrst detect the faces in the photo, then superimpose a
transformed tag over every face and, ﬁnally, execute the face
detection process again. We perform a conservative trans-
parency transformation with an alpha level of a = 0.8 that
can easily be solved by users. Figure 10 shows the detected
faces before and after, and the label of each point indicates
the number of photos with that (before, after) tuple. The
Figure 9: Attack success rate against diﬀerent per-
spective (P) and alpha level transformations.
varying the perspective transformation (P = 2.7, .., 6). Fig-
ure 8 shows that the perspective transformation (P) has the
most signiﬁcant impact, but the tag transformations (RTP)
also contribute to the overall robustness, especially when the
perspective transformation is not as extreme, i.e., for larger
values of P . SQDIFF proves to be the most eﬀective method
against the perspective transformation, while CCOEFF is the
least eﬀective despite being the most robust against trans-
parency and rotation transformations. We achieve our best
results when P = 3.2, 2.7, with the highest scoring method
(SQDIFF) identifying only ∼7.4% of the photos that have
undergone all transformations, compared to the 98.4%-98.8%
success rate of our simplistic attacker (see Section 2), or
the 100% rate of the template matching attack against non-
transformed photos. Depending on the placement of the tag
in the photo and the rotation eﬀect, the perspective trans-
formation has a combined impact that cannot be handled by
the attacker even when conducting multiple rotations.
To further explore the combined impact of the transforma-
tions, in Figure 9 we show the percentage of identiﬁed photos
for each algorithm with varying alpha level and perspective
transformations. Our results clearly show a correlation be-
tween the identiﬁed tags and the levels of transformation. As
the alpha and P values decrease (i.e., the tag becomes more
transparent and the photo’s perspective is more distorted)
the eﬀectiveness of the attack decreases by up to a factor of
two for SQDIFF and by up to a factor of ﬁve for CCOEFF. Apart
from the impact on the success rate, the transformations
require a signiﬁcant increase of processing eﬀort from the
attacker. Attempting to match the tags to the transformed
photo requires ∼87.8 as opposed to ∼0.06 required for the
simplistic attack against the non-transformed photos.
Surprisingly, we found that for a perspective transforma-
tion of 2.7 we didn’t see a signiﬁcant decrease compared
to 3.2 and CCORR actually scored higher. As such, we set
P = 3.2 for our next experiments as it is less distorting and
also yields slightly better results. Furthermore, we manually
inspected the identiﬁed tags and found them to be cases
where they had been placed randomly on a position of the
background photo with almost “no noise” (e.g., on a clear sky,
wall, etc.). Thus, we should strongly enforce a restriction of
placing the tags on faces in the background photos, which
will further decrease success rates.
We measure the eﬀect of increasing the number of tags per
challenge. We create three sets of 1,000 photos that contain
two tags (N = 2), with P = 3.2 and varying alpha levels.
The attacker now has to compare each photo to 144 tags (72
 0 3 6 9 12 15 186543.22.7Identified photos (%)Perspective transformationCCOEFF-a0.8CCORR-a0.8SQDIFF-a0.8CCOEFF-a0.7CCORR-a0.7SQDIFF-a0.7CCOEFF-a0.6CCORR-a0.6SQDIFF-a0.6Detected Faces Before0123456789101112131415161718192021Detected Faces After01234567891011121388623811495033105235431239815383332297226211168141502213146221211240262511974223151413688452112224121311111231111111111111111111Departmental
AMT
Normalized
Gender NoHelp Help NoHelp Help
Initial
Success
94.38%
Female
Male
Total
75.0% 95.8% 76.4% 97.8%
73.9% 87.5% 78.5% 92.6%
74.2% 89.7% 77.9% 94.0%
Time
7.93s
Table 6: Usability Evaluation. Initial and normal-
ized success rates per gender, before and after sug-
gestions are presented (Departmental). Success rate
and seconds per challenge (AMT).
of this study is to measure the impact of the transformations
on the ability of users to identify the depicted people. As
such, we have selected fairly recognizable individuals.
We selected photos of them wearing sunglasses, with their
face at an angle, taken under normal conditions and not
edited. We used the values that we found in our experimen-
tal evaluation to be secure, but not to extreme so as to impede
users’ ability for identiﬁcation. Speciﬁcally, we used all the
possible combinations of: N = {1, 2}, a = {0.6, 0.7, 0.8}, P =
{2.7, 3.2}. We conduct two separate studies, one with par-
ticipants from our department, and one with Amazon Turk
workers, each case allowing us to measure diﬀerent aspects of
our system; the impact of suggestions and the time required
to solve the challenges.
Departmental. We recruited 30 participants from our
department (students and staﬀ), 8 of which were female and
22 were male, with ages ranging from 19 to 37. The same
set of 12 challenges (one for every possible combination) was
presented to all participants, to identify whether a speciﬁc
transformation or person could not be identiﬁed by multiple
users. Users were ﬁrst shown a photo, and if they were able
to provide the correct answer they would move on to the
next.
If not, they were given 6 suggestions for each tag,
which allowed us to measure the impact of the suggestions.
After the initial results, we also calculated a normalized rate
where we ﬁltered out challenges where the participant did
not know the depicted person at all, as our goal is to measure
the impact of the transformations alone.
As shown in Table 6, users solved 89.7% of the challenges,
which increases to 94% if normalized. Surprisingly, users
immediately identiﬁed over 75% of the challenges without
help. Suggestions do oﬀer a signiﬁcant boost, with 14.1% and
21.4% for male and female participants respectively. There
was no strong correlation between the values of transforma-
tions and the success rates. On the contrary, less transformed
photos had higher scores in several cases. We found, however,
that the face was an important factor as there were a few tags
that users could not identify even without transformations, as
they were barely familiar with certain individuals. Nonethe-
less, suggestions are an important aspect of the system as
they help users solve the challenges.
AMT study. We recruited AMT workers (at least 95%
approved, correct HITs and 1000 HITs completed) and pre-
sented them with two batches of 72 distinct challenges gen-
erated automatically by our system. The ﬁrst batch had
N = 1 celebrity face and the second one had N = 2. For each
batch we included all the possible combinations of a and P as
described above, and 6 suggested names (of which 1 was the
correct answer). Overall, 49 workers solved 1,556 challenges,
Figure 11: Faces detected for various alpha levels,
against transparency and transparency+perspective
transformations.
red line denotes the X = Y axis, upon which are the cases
where the same number of faces are detected. Interestingly,
even though the photos now contain double the number of
faces, an extra face is detected only in 47 (1.3%) cases (points
over the red line). Everything below the red line, indicates
improvement as less faces are detected. Due to our tag trans-
formations, no faces are detected in 43.6% of the photos,
which signiﬁcantly impacts face recognition, as faces have to
be detected before compared to facial models.
While the rotation transformation increases the processing
eﬀort of the attacker, it cannot hinder an attacker as a stand-
alone transformation, as the attacker can perform rotations
upon the crafted photo to increase detection rates. Thus,
we want to explore the combined impact of our two other
transformations. To do so, for each experiment we create
two versions of 250 transformed photos with one medium tag
each. In the ﬁrst version, we only apply the transparency
transformation, and in the second both the transparency
and perspective transformation. We then manually remove
any photos with tags that do not contain a human face,
and use our face detection method to see how many faces
are detected in the remaining tags. We test various alpha
levels of transparency, with a constant value of P = 3.2
for the perspective transformation, as it had the best eﬀect
in our previous experiments. We present our results in
Figure 11. While the transparency transformation has a
very signiﬁcant impact on the attack, by combining it with
perspective transformation, we are able to completely hinder
face detection and, thus, recognition in all the photos, for
a ≤ 0.6. Again, we found that any faces that were detected
had been placed on a “clear” background.
Security through distortion. Overall, our experiments
demonstrate the robustness of our transformations against
pattern matching and face recognition attacks. Even under
an extreme threat model where the attacker has collected
every single photo of the users and has knowledge of our
approach, we are able to completely deter one class of attacks
and decrease the success rate of the other by over 99%, while
incurring a massive overhead to the required processing eﬀort.
4.4 Usability Evaluation
To evaluate the usability of our approach we conduct a
preliminary study using challenges with tags of famous people.
While users might be more familiar with certain famous
people than with some of their contacts, they may not be
familiar with others as well (or know their name). The goal
 0 3 6 9 12 150.80.70.60.50.4Detected Faces (%)alpha value (transparency transformation)TransparencyTransparency+Perspectivewith at least 20 workers solving each challenge. As Table 6
shows, AMT workers conﬁrmed the previous results (on a
larger scale) with a 94.38% success rate (not normalized),
taking 7.93 seconds on average (7.19 standard deviation).
Conclusions on Usability. A crucial metric for the ap-
plicability of such a mechanism is the ability of users to solve
the challenges. The results of the user study demonstrate
the usability of our system, as users solved ∼94% of the
challenges. Bursztein et al. [8] reported an 84–87% success
rate for image CAPTCHAS, which is lower than our results.
For the easiest category of text CAPTCHAs results were
slightly higher than ours, with 95–98%. Furthermore, they
reported an average completion time of ∼ 9.8 seconds for the
image CAPTCHAs (∼ 8.9 when solved correctly), which is
almost two seconds slower than ours.
We believe that our initial results demonstrate that our
system provides a viable solution for securing the login pro-
cess against adversaries that have stolen user credentials.
Nonetheless, we plan on conducting an extensive user study
with challenges that depict actual contacts of the users, to
fully evaluate the usability of our approach.
5. SOCIAL AUTHENTICATION SERVICE
We discuss how an OSN can deploy our secure SA as
a service for other websites. By adding a simple piece of
code, services can access an API and fetch challenges speciﬁ-
cally crafted for each individual user. This can be adopted
as a user-gnostic CAPTCHA, or as an additional security
mechanism.
To outline the beneﬁt of employing such a service in ad-
dition to a traditional two-factor authentication scheme, we
describe the following scenario. An attacker steals a user’s
smartphone, which contains the credentials to an e-banking
service and is also the device that receives the security token
(via SMS or a token-generation app). Normally, the attacker
will be able to complete any transaction as he possesses
both tokens needed to pass the two-factor authentication.
Similarly, attacks in the wild, have passed two-factor authen-
tication by tricking the user to install a malicious app (e.g.,
the Eurograbber malware [2]). However, if the e-banking
service employs this service for additional security, attackers
that don’t belong to the victim’s social circle will fail to
complete any transaction. Even if the device has an exist-
ing session with the OSN, they will not be able to pass the
challenges (see outsourcing attacks below).
Dataset. Another important feature of our system, is that
it builds upon a dataset of photos and tag information that
is readily available to the OSN. Thus, it doesn’t face the
challenge of creating a correct and extensive dataset as other
image-based CAPTCHA schemes do [15, 26].
Someone could argue that SA is too restricted in terms of
the number of challenges that can be created, in comparison
to text-based CAPTCHA schemes than can create inﬁnite
combinations of numbers and letters. However, such an
argument is far from true for our approach. In our study we
found that users have an average of 347 friends each with
approximately 12 medium tags, resulting in 4,164 suitable
tags per user. These can produce over 8 million diﬀerent
permutations of 2 tags, and 1.2e+10 for 3 tags, which is more
than enough for a single user proﬁle. Also, the huge number
of photos that can be used as “backgrounds” increases the
number of possible challenges even more.
Privacy. A user-gnostic CAPTCHA service may raise
privacy concerns, as the OSN acquires information regarding
websites visited by users. However, this information is also
acquired through the “social plugins” or single sign-on services
oﬀered by many popular OSNs. These services have been
widely adopted, and [17] reports that over 35% of the top
10,000 Alexa websites include the “Like” button.
Security Properties. We discuss the eﬀectiveness of our
approach against typical CAPTCHA-breaking attacks.
Guessing Attacks. Our scheme allows automated bots to
pass the challenge with a probability of 1/SN , where N is the
number of friends depicted and S the number of suggestions
per friend. The threshold adopted by previous work [28] is
that bots should not have a success rate higher than 0.6%.
By selecting 3 friends and providing 6 suggestions per friend,
we are able to achieve an even lower probability of 0.46%.
Furthermore, our system provides an extra level of security.