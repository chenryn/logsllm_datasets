title:Reading between the lines of failure logs: Understanding how HPC
systems fail
author:Nosayba El-Sayed and
Bianca Schroeder
Reading between the lines of failure logs:
Understanding how HPC systems fail
Nosayba El-Sayed
Bianca Schroeder
Department of Computer Science
University of Toronto
Toronto, Canada
{nosayba, bianca}@cs.toronto.edu
Abstract—As the component count in supercomputing instal-
lations continues to increase, system reliability is becoming one
of the major issues in designing HPC systems. These issues will
become more challenging in future Exascale systems, which are
predicted to include millions of CPU cores. Even with relatively
reliable individual components, the sheer number of components
will
increase failure rates to unprecedented levels. Efﬁciently
running those systems will require a good understanding of how
different factors impact system reliability.
In this paper we use a decade worth of ﬁeld data made
available by Los Alamos National Lab to study the impact
of a diverse set of factors on the reliability of HPC systems.
We provide insights into the nature of correlations between
failures, and investigate the impact of factors, such as the power
quality, temperature, fan and chiller reliability, system usage and
utilization, and external factors, such as cosmic radiation, on
system reliability.
I.
INTRODUCTION
System reliability is one of the major challenges in running
and designing high-performance computing (HPC) systems. As
architectural constraints limit the speed of individual devices,
the component count in HPC systems is continuously growing.
For example, future exascale systems are expected to combine
the compute power of millions of CPU cores. Efﬁciently run-
ning systems at such scale will require a good understanding
of their failure behavior.
In this paper we conduct an analysis of a decade of ﬁeld
data made available by Los Alamos National Lab. While
previous work [12] has provided a high-level, general statistical
summary of this data set, in this work we are particularly
interested in identifying factors or circumstances that are pre-
dictive of future failures. Understanding what those factors are
can help operators mitigate them, or take proactive measures
against
impending failures in cases where they cannot be
avoided.
While there have been a number of papers analyzing
failures in HPC systems, see for example [4]–[6], [10], [13],
this prior work tends to be concerned with deriving statistical
models that capture the observed failure process. For example,
work that studies correlations between failures (which are
relevant for predicting future failures and hence fall into the
category of events we are interested in, in this work) usually
does so by statistically modeling the empirical distribution
of the inter-arrival
time between failures or analyzing the
auto-correlation function of the observed sequence of failures.
While statistical models are very useful, for example in driving
simulations or analyses of HPC systems, they are not all that
helpful for operators in developing a good intuition for how
and why their systems fail.
The goal of our work is to answer a set of speciﬁc
questions to improve our understanding of failures in HPC
systems, rather than providing a statistical model of failures.
After providing a summary of the data set we use in our
work in Section II, Section III looks into correlations between
failures, including questions such as which failure types are
most likely to generate follow-up failures. In Section IV we
study whether some nodes are more likely to fail than others
and why. Section V and Section VI address the question of
how usage affects the reliability of a node. Sections VII, VIII,
IX investigate the impact of environmental factors on node
reliability, including the effect of the quality of power, the
effect of temperature, and external factors, such as cosmic
radiation. Finally, in Section X we put different pieces of
our work together by performing a joint regression analysis
including a diverse set of factors.
II. THE DATA
Our study is based on failure data collected at 10 different
high-performance computing (HPC) clusters at Los Alamos
National Lab over a period of 9 years and is publicly available
at [1]. We divided the 10 clusters into two different groups,
based on their hardware architecture. Group-1 includes seven
systems that are based on 4-way SMP (Symmetric Multi-
Processing) nodes with one or two network interfaces (NICs)
and a varying amount of main memory per node. In total
these systems have 2848 nodes and 11392 processors. On the
LANL web page, where the data is available, these systems
correspond to the systems with IDs 3, 4, 5, 6, 18, 19 and 20.
Group-2 includes 3 systems that are based on NUMA (Non-
Uniform Memory Access) technology and contain a smaller
number of nodes, but a larger number (typically 128) of
processors per node. In total the systems in group-2 contain
70 nodes and 8744 processors, and correspond to the systems
with IDs 2, 16, and 23 on the LANL web page.
For each of the systems the data contains records of all
node outages that occurred during the measurement period,
including information on the root cause of the node outage, the
time when the outage happened and the ID of the node that was
affected. The root cause of each failure falls into one of six
high-level categories: environment failures, including power-
outages for instance; hardware failures; failures resulting from
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
LANL Group−1
LANL Group−2
LANL Group−1
LANL Group−2
After ANY other failure
After same failure type
Random week
0.2
94x
0.5
After ANY other failure
After same failure type
Random week
6.4x
23.1x
14.1x
12.8x
6.8x
10.0x
9.3x
6.9x
ENV
HW
HUMAN
NET
UNDET
SW
MEM
CPU
ANY
4.2x
RAND
WEEK
y
t
i
l
i
b
a
b
o
r
P
1
0.8
0.6
0.4
0.2
0
y
t
i
l
i
b
a
b
o
r
P
0.6
0.5
0.4
0.3
0.2
0.1
0
4.05x
3.13x
2.50x
3.09x
2.85x
ENV
HW
HUMAN
NET
UNDET
SW
MEM
CPU
ANY
2.25x
1.69x
1.52x
RAND
WEEK
e
r
u
l
i
a
f
a
f
o
y
t
i
l
i
b
a
b
o
r
P
0.1
0.05
0
0.15
698x
8x
7x
726x
31x
73x
6x
5x
11x
17x
e
r
u
l
i
a
f
a
f
o
y
t
i
l
i
b
a
b
o
r
P
0.4
2.5x
2.5x
32.3x
0.3
0.2
0.1
6.5x
3.9x
111.7x
3.7x
3.1x
1.6x
6.3x
4.9x
4.6x
6.3x
5.5x
4.1x
0
ENV HW HUMAN NET UNDET SW MEM CPU
37x
15x
−1x
18x
11x
ENV HW HUMAN NET UNDET SW MEM CPU
(a) The probability that any node-failure follows a failure of type X.
(b) The probability that a failure of type X follows other node-failures.
Fig. 1. Correlations between failures in the same node
human-errors; software failures; network failures; and undeter-
mined, whenever the root cause of the failure is unknown. The
process of assigning failures to categories in LANL over the
9 years that the data spans was done by system administrators
according to classiﬁcation rules developed jointly by hardware
engineers, administrators and operations staff [12]. Besides
the high-level categorization of root causes, for many failures
more detailed information is available, such as the hardware
component responsible for a hardware failure.
In addition to logs of node outages, for some of the systems
there is data on usage and the physical layout of nodes in the
machine room available. In particular, group-1 systems have
“machine layout” ﬁles that describe the position of each node
inside a rack, and the location of a rack inside the server room.
Additionally, detailed data on usage is available for two LANL
systems: Systems 8 and 20. The usage records contain for
each job information on the job submission time, job dispatch
time (the time the job got dispatched from the queue to start
running), job end-time, the number of requested processors
and the ID(s) of the node(s) that this job was assigned to.
III. HOW ARE FAILURES IN HPC SYSTEMS CORRELATED?
The ﬁrst question we are addressing in our work is how
failures in HPC systems are correlated with each other. Dis-
covering correlations between failures in HPC systems serves
two purposes. First, it helps create a deeper understanding of
their underlying root causes. Second, it helps in the prediction
of failures, which is useful, for example, for scheduling appli-
cation checkpoints or for designing job migration strategies.
Rather than building formal statistical models of corre-
lations, we are interested in providing intuitive insights into
correlations by answering questions, such as what types of
failures increase the probability of future failures and by how
much is the failure probability increased after a prior failure.
In order to quantify these dependencies, we use the data to
determine the probability of a node failure in the time window
following a previous failure and compare this probability to
the probability of a node failure in a random window. We
look at time windows of different lengths, including one day,
one week and one month, and perform the calculations at three
different spatial granularities: node level, rack level and system
level. To test the statistical signiﬁcance of our results all graphs
include 95% conﬁdence intervals. We also perform two-sample
hypothesis tests to measure the signiﬁcance of the difference
between probabilities.
A. Correlations between failures within a node
In the ﬁrst part of our correlation study we only focus
on correlations between failures in the same node, i.e. we
are asking the question whether current failure behavior of a
node is predictive of its future failure behavior.
1) How does a failure affect the likelihood of later fail-
ures?: As a starting point, we calculate the daily and weekly
probability of a node failure for group-1 and group-2 systems,
i.e. the probability that a random node will fail in a random
day/week. We then compare those probabilities against the
probability of a node failing during a day or week following
another failure (of any kind).
We ﬁnd that the unconditional probability of a node failure
on a random day is 0.31% and 4.6% for group-1 and group-
2 systems, respectively. We observe that
the daily failure
probability is markedly higher during the 24 hours following
a prior failure: 7.2% and 21.45% for group-1 and group-2
systems, respectively, which corresponds to roughly a 20X
increase and 5X increase for groups 1 and 2, respectively. We
observe similar, albeit somewhat weaker trends, for the entire
week following a failure: the failure probability of a node in
a given week increases from 2.04% to 15.64% in group-1 and
from 22.5% to 60.4% in group-2.
2) Does the type of a failure affect the chance of follow-
up failures?: Since we have information on the root cause
of failures an interesting question is whether some types of
failures increase the probability of follow-up failures more
than others. To answer this question Figure 1-(a) shows the
probability that a given node will fail within the one-week
period following a failure of a particular type. The failure
type is any of the six different categories of root causes that
are distinguished in LANL: Environment, hardware, human
error, network, software or undetermined failures. Each bar
in the ﬁgure corresponds to one of those failure types. To
provide a baseline, the right-most bar shows the probability
for a node failing on a random week (not necessarily preceded
by a failure).
Based on Figure 1-(a), we make several interesting ob-
servations. First, all types of failures increase the probability
of failure in the following week, most commonly by factors
of 7-10X in group-1 systems and factors of 2-3X in group-2
systems. For some cases, such as network or environmental
failures in group-1 systems, the increase in failure probability
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
is more than 10X compared to a random week. We also note
that prior failures increase the likelihood of later failures to
signiﬁcant levels. For example, while the probability of failure
in a random week is only 2.04% in group-1 systems, chances
are 30-50% that a node will experience a failure in the week
following a network or environmental failure.
The second interesting observation is that the overall trends
are very similar for group-1 and group-2 systems. In both
cases the increase in failure probabilities is highest following
a network or environmental failure. For group-1 systems a
network or environmental failure increases the probability that
a node will fail in a given week by a factor of 14-23X, and for
group-2 systems it increases the failure probability by a factor
of 3-4X.
We note that the factor increases are in general smaller
for group-2 systems, since their baseline probability is higher.
The probability for a node to experience a failure in a given
week is 22.5% for a group-2 node (compared to only 2.04%
for a group-1 node), which means the failure probability can
not increase by more than a factor of 5X. The reason for
the higher failure rates in group-2 systems is that the nodes
in those systems are of a different type: they are NUMA
nodes with 128 processors per node, compared to SMPs with
4 processors per node for group-1 systems, and the larger
component count leads to higher failure rates.
3) Does the type of a failure predict the type of a follow-
up failure?: Often it might be useful to know what type of
failure to expect in the future. For example, are failures of
type X usually followed by failures of type Y? To answer
this question we computed all pairwise probabilities p(x, y),
where p(x, y) is the probability of a failure of type Y in a
week following a failure of type X, and compare this to the
probability of a type Y failure in a random week.
Our ﬁrst observation is that a failure always signiﬁcantly
increases the probability of a follow-up failure of the same
type, and more so than a random failure. Figure 1-(b) shows
the probability of a failure of type X in the week following a
failure of type X, compared to the week following any type of
failure, and compared to a random week. We observe that the
increase in the failure likelihood can be dramatic. For example
for group-1 systems, the probability of an environmental or a
network failure in a given week increases by a factor of around
700X (to absolute values above 7%) if a failure of the same
type was observed previously.
Besides correlations between failures of the same type, we
notice signiﬁcant correlations between network, environmental
and software problems, i.e. each of these three types increases
the follow-up probability of a failure of one of the other two
types. We have been in discussions with operators at LANL
and have not been able to come up with a clear explanation
for these correlations. A closer analysis of the correlations
between these three error types revealed that
there are a
few nodes who happen to have a relatively large number of
network, environmental and software problems. It is possible
that the correlation is biased by a few nodes that coincidentally
had a large number of these three types of failures and does
not imply a causal relationship.
4) How are hardware failures correlated?: We pay special
attention to hardware failures since these are the single most
common failure category: 60% of all failures are attributed
to hardware problems. Our data set contains more detailed
information on the root cause of hardware failures. The data
shows that by far the most common types of hardware failures
are due to problems with memory or CPU. 20% of hardware
failures are attributed to memory and 40% are attributed to
CPU.
When repeating a correlation analysis similar to the one
performed for the high-level failure categories, we ﬁnd that
past failures signiﬁcantly increase the future probability of
memory and CPU failures. In the week following a memory
failure the probability of experiencing an additional memory
failure is 20.23% for group-1 systems, a factor of nearly 100X
increase over the probability of 0.21% in a random week. For
group-2 systems, the weekly probability of a memory failure
increases from 4.2% to 12.6%. All increases are statistically
signiﬁcant based on the two-sample hypothesis test.
The strong correlations between hardware-related failures
allow us to draw some conclusions about the nature of these
failures. Based on discussions with people at LANL, node
failures that are attributed to memory or CPU problems are
usually due to bit corruption events that go beyond what the
built-in ECC can correct. This type of data corruption could
either be due to soft errors, which are caused by random events,
such as cosmic rays or random noise, or it could be due to
hard errors, i.e. problems with the underlying hardware. The
strong correlation between those errors points to hard errors
as the more likely source of the problem, as one would not
expect correlation between random events, such as cosmic
rays. We study the impact of cosmic rays on hardware failures
in Section IX.
B. Correlations between failures within a rack
The data for group-1 systems also includes information on
the machine room layout, including the rack layout, which
allows us to study how failures in different nodes in the same
rack are correlated. We begin with the probability of a node
failing (with a failure of any type) within a week following
a failure (of any type) of another node in the same rack. We
ﬁnd that this probability is 4.6%, which is more than double
the probability of a node failing in a random week (which is
2.04%). The increase in the daily probability is higher: the
failure probability on a day following the failure of another
node in the rack is 1.2%, which is nearly a factor of 3X higher
than the baseline probability of 0.31%.
As we did in the case of correlations within the same node,
we also looked at which failure types have the biggest effect
on the probability of another node failing later on in the same
rack. The results are shown in Figure 2 (left). We observe
some increase in the failure probability for all types of failures,
although with factors of 1.4–3X these are markedly lower than
the increase of failures in the same node. Statistical testing with
the two-sample hypothesis test allows us to conclude only for
software failures that the probability of follow-up failures is
signiﬁcantly increased.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:51:27 UTC from IEEE Xplore.  Restrictions apply. 
LANL Group−1
ENV
HW
HUMAN
NET
UNDET
SW
MEM
CPU
ANY
0.1
2.4x
0.05
1.5x
1.4x
2.1x
1.9x
y
t
i
l
i
b
a
b
o
r
P
0
e
r
u
l
i
a
f
a
f
o
y
t
i
l
i
b
a
b
o