### 4.7 Master/Checker Mode

The LEON processor supports a checker mode, which allows two LEON processors to operate concurrently in a master/checker configuration. In this mode, all outputs from the checker processor are disabled, and the internal values that would have been driven by the checker are compared to those driven by the master. Any discrepancy will trigger an error output on the checker. This feature is particularly useful for applications with stringent error-detection requirements, though it is primarily used during Single Event Upset (SEU) testing [5]. It's important to note that errors in the register file or cache memory, even if corrected, will cause a master/checker error due to the execution skew between the two processors. This limitation necessitates a reset to resynchronize the processors.

### 4.8 Software Considerations

Errors in the caches and register file are only detected when the data is accessed. The probability of undetected multiple errors increases if stored data is not regularly used. Most tasking kernels, such as VxWorks or RTEMS, write all active register windows to the stack during each task switch, which helps in correcting any latent errors. Therefore, error accumulation in the register file should not be a significant issue. However, the caches are not flushed during task switches. In a reasonably large program, the cache contents are frequently replaced or accessed, thus overwriting any latent errors. Determining the duration a cache line remains active without being accessed is challenging. For small programs, periodic cache flushes can be performed to refresh the cache contents.

### 5. Implementation

#### 5.1 Design Style and Portability

The LEON processor is implemented using a high-level VHDL model, which is fully synthesizable with common tools like Synopsys, Synplify, and Leonardo. The model is highly configurable through a configuration package, allowing options such as cache size, multiplier implementation, target technology, speed/area trade-offs, and fault-tolerance schemes to be set by editing constants.

The design uses only technology-specific cells for cache memories and register files, and pads. To maintain portability, a package with wrappers is created for each technology, providing a uniform interface between the processor and custom technology cells. Porting LEON to a new technology involves creating a new wrapper package for that technology. The design is fully synchronous, operates on a single clock, and uses only registered inputs and outputs, simplifying synthesis and enhancing portability.

#### 5.2 Synthesis Overhead - Atmel ATC25

The configurability of the model facilitates quick analysis of the impact of fault-tolerance functions. Table 1 below shows the synthesis results for an FPU-less LEON configuration on the Atmel ATC25 (0.25 µm) CMOS technology. The core area is compared for a standard configuration versus one with fault-tolerance features. The fault-tolerant version uses Triple Modular Redundancy (TMR) on all flip-flops, 2 parity bits on the cache rams, and a 7-bit BCH code on the register file. The area overhead for the LEON core without RAM blocks is around 100%, which is expected given that a TMR cell is approximately four times the size of a normal flip-flop (3 flip-flops + voter). The overhead including RAM cells is 39% because the overhead for parity and BCH checkbits is lower. This configuration is heavily pad-limited on a 0.25 µm process, and if manufactured, the area overhead at the chip level would be 0%. The timing penalty for the fault-tolerant version is the extra delay through the TMR voter, approximately two gate-delays or 8% of the cycle time.

| **Module** | **Area (mm²)** | **Area incl. FT (mm²)** | **Increase (%)** |
|------------|----------------|-------------------------|------------------|
| Integer unit (+ mul/div) | 0.86 | 1.61 | 87% |
| Cache controllers | 0.17 | 0.35 | 105% |
| Peripheral units | 0.45 | 0.90 | 100% |
| Register file (136x32) | 0.19 | 0.24 | 26% |
| Cache mem. (16 Kbyte) | 2.42 | 2.59 | 7% |
| Total | 4.09 | 5.69 | 39% |

#### 5.3 LEON-Express

The first silicon implementation of LEON was on the Atmel ATC35 process, codenamed 'LEON-Express', and was manufactured in January 2001. A standard-cell library without SEU hardening features was used. The chip is approximately 40 mm² and operates at 50 MHz under military temperature and worst-case conditions. The purpose of LEON-Express was to validate the operation of the LEON processor and demonstrate the implemented fault-tolerance techniques; it will not be commercialized. Figure 4 shows the floorplan, indicating that the device is pad-limited and uses only three metal layers to minimize manufacturing costs.

### 6. Heavy-Ion Error Injection

To test the SEU protection methods, the LEON-Express device underwent heavy-ion error injection at the Louvain Cyclotron Facility in Belgium. The tests aimed to measure the SEU sensitivity of the device to ions at different energy levels and assess the efficiency of the fault-tolerance logic. A test board with two LEON devices connected in master/checker mode was designed. The built-in master/checker comparators allow the device to run at full speed while comparing outputs on each clock cycle. Figure 5 shows the SEU test board (checker device not mounted).

Three types of test programs were used: IUTEST, which continuously checks the register file and cache memories for errors; PARANOIA, which checks the FPU operation; and CNCF, based on real spacecraft navigation software. Each test program is self-checking and calculates a checksum of all operations. The register file and cache memories have on-chip error-monitoring counters that increment after each corrected SEU error. The test software continuously reports these counter values to an external host computer, which counts the number of errors in each RAM type and any software checksum errors.

During heavy-ion injection, the master device was exposed to the ion beam while the compare error signal from the slave was monitored. When a compare error is detected, the current software cycle is completed, and the checksum is verified to ensure successful correction. The error counters are also inspected to confirm that the compare error originated from a correction operation rather than an undetected and uncorrected error.

The first round of tests used effective Linear Transfer Energies (LET) between 6 and 110 MeV, with ion fluxes from 75 to 400 ions/s/cm². During each run, 10E5 particles were injected into the device. The number of resulting errors is shown in Table 2. No undetected errors or other anomalies occurred, and a total of 4,500 errors were detected and corrected. The cross-section (SEU sensitive area) depends on software activity and ion LET, with a maximum of 0.1 cm² measured when running the IUTEST program at 110 MeV. This means that 10% of the RAM cell area is sensitive to SEU hits. The cross-section for the flip-flops could not be measured due to the lack of SEU monitoring capability in the TMR cells. Limiting the flux to 400 ions/s/cm² was necessary to accurately count all errors, as a reset and re-initialization of the test system take around 50 milliseconds and must be significantly lower than the average error rate.

With the cross-section data, additional tests were conducted at an ion flux between 2,000 to 5,000 ions/s/cm² using an LET of 110 MeV. At these levels, 20 to 50 errors per second occurred in the RAM cells. Several runs were made using all three test programs, injecting 10E6 and 10E7 particles (10E4 to 10E5 errors) per test run. The CNCF and PARANOIA test programs executed without undetected errors, but the IUTEST showed an average of 5 error traps or software failures per 10E7 particles. Ion fluxes below 2,000/s/cm² did not result in any failures, and it is believed that the undetected errors were due to multiple-error build-up in the caches. Given that the worst-case condition for ion flux in the space environment is much lower, this effect is not considered a problem. The cross-section per bit for the different RAM types is plotted in Figures 6 and 7.

### 7. Alternative Implementations

LEON is not the first fault-tolerant RISC processor; previous implementations include the IBM S/390 G5 [11] and the Intel Itanium [12]. The IBM processor detects errors by duplicating the complete pipeline until the last write-stage, where the results from the two pipelines are compared. In case of a discrepancy, the processor state is not updated, and the pipeline is restarted. The area overhead is similar to LEON, at 100%. The IBM scheme is advantageous in terms of timing, as it is not affected by a TMR voter and detects all types of errors, not just soft errors in registers. However, it is less suitable for real-time applications because restarting the pipeline takes several thousand clock cycles. It is also limited in use for bus interfaces or timer units where functional timing is critical.

The Intel implementation uses a mix of ECC and parity codes to detect and correct soft errors in caches and TLB memories, but state machine registers are not protected.

### 8. Conclusion

Well-known error-detection and correction techniques such as parity, BCH, and TMR have been used to implement an SEU-tolerant processor on a non-hardened semiconductor process. By selecting appropriate detection and correction methods for each specific memory type, the area overhead has been kept low. Fault-injection using heavy-ions has demonstrated the efficiency of the fault-tolerance concept, although some anomalies were detected at high particle fluxes. The portable design style and simple synthesis method ensure long-term availability and quick access to new semiconductor processes.

### 9. Future Directions

In 2002, the LEON processor is planned to be implemented and manufactured on the Atmel ATC25 process (0.25 µm CMOS). The ATC25 device will have larger caches than the ATC35 version and will include additional functions such as a PCI interface and an on-chip debug unit. The final die size will be around 20 mm² (pad-limited), and the device is planned to operate at 100 MHz.

Although no indications of combinational SEU errors were seen for the ATC35 device, the separate clock trees for the TMR cells make it possible to form a pulse filter on the inputs to the flip-flops. By skewing the three clocks, any pulse shorter than the skew would only be latched by one of the flip-flops in the cell and be removed by the voter. The feasibility of such a scheme will be further investigated.

### 10. Acknowledgements

The author would like to thank R. Creasey, P. Plancke, A. Pouponnot (ESA), Prof. J. Torin (Chalmers University), T. Corbiere, J. Tellier, and G. Rouxel (Atmel-Nantes) for their support and encouragement during this work.

### 11. References

[1] J. Gaisler, "Concurrent error-detection and modular fault-tolerance in a 32-bit processing core for space applications," FTCS-24, June 1994 (Austin, USA).
[2] J. Gaisler, "Evaluation of a 32-bit microprocessor with built-in concurrent error-detection," FTSC-27, June 1997 (Seattle, USA).
[3] J. Gaisler & T. Vardanega, "Lessons Learned from the Implementation of On-Board Tolerance to Physical Faults in Ada," The International Journal of Computer Systems: Science & Engineering, January 2000.
[4] P. Linden et al., "On latching probability of particles induced transients in combinatorial networks," FTCS-24, 1994.
[5] R. Koga et al., "Techniques of microprocessor testing and SEU rate prediction," IEEE Trans Nucl. Sci., NS-32, 1985.
[6] "The SPARC Architecture Manual Version 8," SPARC International, Prentice Hall, 1992.
[7] "AMBA Specification, version 2.0," ARM-IHI 0011A, ARM Limited, 1999.
[8] J. Handy, "The Cache Memory Book," Academic Press, 1993.
[9] C. L. Chen et al., "Error-correcting codes for Semiconductor Memory Applications: A-state-of-the-art-review," IBM J Res Development, page 124-132, March 1984.
[10] J. A. Zoutendyk et al., "Characterization of Multiple-Bit Errors from Single-Ion Tracks in Integrated Circuits," IEEE Trans Nucl. Sci., December 1989.
[11] M. A. Check et al., "Custom S/390 G5 and G6 microprocessors," IBM Journal of Research and Dev., Vol 43, No 5/6, 1999.
[12] N. Quach, "High availability and Reliability in Itanium Processors," IEEE Micro, Vol 20, No 5, 2000.