tifying cloud peering relationships would be to simultaneously
measure from both client-side (like this study) and from within
cloud networks (like [8, 74]). Finally, different ISPs globally may
have different peering relationships with the cloud providers, and
by grouping them together we may miss out on regional-specific
routing trends. While we do shed some light on country-specific
peering case studies in §6.2 and Appendix A.4, a thorough exami-
nation of routing relationships between ISPs and cloud providers is
required (similar to [9]), which we plan to undertake in future.
Figure 10 shows the percentage breakup of paths belonging to
the three interconnection categories for all cloud providers in our
target list. Our results verify the advertised backbone network type
of cloud providers shown in Table 1. Majority of the connections
71
ZAMAJPIRGBUAUSMXBRAR012Coeﬀ.ofVariation(Cv)SChome(USR-ISP)SCcelldirect12+NumberofintermediateAS-levelHops0255075100PercentageofPathsBABAAMZNDOGCPIBMLINMSFTORCLVLTRCloudy with a Chance of Short RTTs
IMC ’21, November 2–4, 2021, Virtual Event, USA
across the globe. When direct peering is not possible, cloud
providers prefer to use private interconnects via Tier-1 ISPs like
Telia carrier. Smaller providers like Linode, Vultr, Oracle mostly
rely on the public Internet for routing their tenant traffic.
6.2 Impact of ISP Peering on Latency
We now turn our attention towards understanding the impact of
direct ISP-cloud peering interconnections on user cloud access la-
tency. For a thorough analysis, we choose to focus on measurements
from Europe (VPs in Germany to DCs in the UK) and Asia (VPs in
Japan to DCs in India). Cloud providers have been known to focus
their infrastructure investments within Europe and North Amer-
ica, to maximize their profits from the existing user base [67, 92].
However, these continents are already well-provisioned with a re-
liable Internet backbone and have remained within the limelight
of networking innovation for decades [19, 42, 88]. Previous studies
have shown that the benefits of private cloud WANs decrease with
decreasing geographical distance between user and datacenter [8],
and as evident from Table 1, users in both EU and NA have several
options for accessing the nearest DC. Comparatively, DC deploy-
ment in continents such as Asia, SA, and Africa, is highly scattered
- favoring only a few select countries. As a result, the impact of
using privately managed WANs operated by the cloud providers
should be more noticeable within these regions. To keep the anal-
ysis comparative across these two continents, we select Germany
and Japan as originating countries since both have a dense avail-
ability of Speedchecker VPs (see Figure 1b). Similarly, UK and India
are selected as endpoints since both have DC deployment from
almost all providers in our target list. With this analysis, we aim to
understand the continent-specific routing policies set up by cloud
providers to transport tenant traffic. We provide more case studies
within these continents, specifically Bahrain VPs to India DCs (for
Asia) and Ukraine VPs to UK DCs (for Europe) in Appendix A.4 to
strengthen the inferences we draw in this section.
Figures 12 and 13 highlight the impact of using different cloud-
ISP interconnections in Europe and Asia, respectively. Let’s first
focus our attention on Europe. Figure 12a shows the different peer-
ing types used by German ISPs2 while transporting traffic bound
to cloud providers. The color denotes the percentage of paths be-
longing to the majority interconnection type between the ISP and
the cloud provider. The result validates our findings in Figure 10.
The three hypergiants - Amazon, Google, and Microsoft - exclu-
sively peer directly with almost all serving ISPs in Germany. As a
result, the majority of traffic originating from Germany towards
DCs of these providers traverses a very “flat” Internet – avoiding
even the transit Tier-1 [9]. For other cloud operators, except for
traffic originating from Telefonica (AS 6805) towards Alibaba and
Vodafone (AS 3209) towards DigitalOcean, almost all German ISPs
route their traffic via private interconnection facilities that sup-
port the PoP of that provider. We also find that as a medium-sized
operator, IBM uses a combination of direct and private intercon-
nects to support tenant traffic. However, it also exchanges traffic
at public IXPs more than any of its contemporaries. The trend of
setting up direct peering agreements by hypergiants repeats for
2We only show top-5 ISPs ordered by number of recorded measurements.
Figure 11: Degree of pervasiveness of different cloud
providers globally. High pervasiveness in Google, Microsoft
and Amazon routes shows that majority of routers on end-
user’s path to the nearest DC are within ASes owned and
operated by the providers themselves.
bound to networks of the three hypergiants – Amazon, Google, and
Microsoft – bypass transit providers altogether, as tenant traffic
from serving ISPs directly peers into the provider’s private WANs.
For client ISP without direct peering, we find that cloud providers in-
creasingly employ carrier peering via private Tier-1 ISPs (e.g., Telia
carrier - AS1299, GTT comm. - AS3257, etc.). Private peering inter-
connections are used by almost all cloud providers as the peering
providers host edge PoPs for multiple operators [2]. Medium-sized
cloud providers, e.g., IBM and DigitalOcean, benefit greatly from
private peering as their private WANs are still localized, and they
can divert their investments into expanding their infrastructure
by deploying more datacenters [27]. We find that IBM follows a
hybrid interconnection approach as it relies on private peering
for shorter paths (concentrated mainly within Europe and North
America) but public transit for longer paths (mostly in Asia). Lastly,
we find that paths destined to small-sized cloud providers, such
as Linode, Vultr, and Oracle, often include two or more on-path
ASes, likely hinting routing via the public Internet. Interestingly,
Alibaba, despite its massive datacenter and private WAN deploy-
ment [4], also uses public Internet paths to interconnect users to its
cloud regions. We attribute this behavior to the low availability of
Speedchecker probes in China (see Fig. 1b), which does not provide
us visibility into Alibaba’s primary operational region. Outside of
China, Alibaba operates its datacenters as independent “islands",
only allowing ingress into their WAN via public transit providers.
We also analyze the router-level traceroute data and calculate
pervasiveness in Figure 11. We define pervasiveness as the ratio
between the number of routers owned by the cloud providers to
the overall path length to the cloud. High pervasiveness degree
hints at most of the end-user route to the cloud to be owned, con-
trolled and operated by the provider themselves – highlighting
the reach of their private WAN. We find that pervasiveness of the
cloud providers follows a similar trend to the AS-level hop distri-
bution; with Google, Microsoft, and Amazon owning more than
60% of the path in almost every continent. Similarly, providers with
two or more ASes only own ≈20% of routers on a path, further
strengthening the correctness of our methodology to identify types
of ISP-cloud interconnections.
Takeaway — Hypergiant cloud providers (Amazon, Google, Mi-
crosoft) usually have direct peering with clients’ ISPs (> 50%)
72
BABAAMZNDOGCPIBMLINMSFTORCLVLTR0.00.20.40.60.81.0PervasivenessNAASEUAFOCSAIMC ’21, November 2–4, 2021, Virtual Event, USA
Dang and Mohan et al.
(a) Peering between German ISPs & cloud providers
(a) Peering between Japanese ISPs & cloud providers
(b) Effect of direct and transit peering between DE-UK.
(b) Effect of direct and transit peering between JP-IN.
Figure 12: Case study of ISP-cloud peering in Europe. Fig-
ure 12a identifies peering interconnections from German
ISPs to DCs in the UK. The color denotes the percentage of
paths in  that used the same interconnection
type. Figure 12b compares the impact of different intercon-
nection on cloud access latency.
Figure 13: Case study of ISP-cloud peering in Asia. Fig-
ure 13a identifies peering interconnections from Japanese
ISP to DCs in India. The color denotes the percentage of
paths in  that used same interconnection type.
Figure 13b compares the impact of different interconnection
on cloud access latency.
Japanese ISPs as well (Figure 13a) – with the exception of Amazon
traffic originating from NTT (AS 4713). Interestingly, we find that
DigitalOcean strictly relies on the public Internet for transporting
tenant traffic in Asia. We attribute this behaviour to the possible
lack of PoP deployment for the cloud provider within the continent.
It must be noted here that the transit fabric differs based on the
region of the targeted DC. For example, in the case of Japan, we find
that for traffic ingress into cloud provider’s WAN (that does not
support direct peering) is transited over NTT (AS2914) when both
the VP and the DC are co-located within Japan. On the other hand,
traffic from VPs in Japan to DCs in India is handled by TATA Comm.
(AS6453) for transit. Keep in mind that this behavior is missing in
paths over direct peering links where the cloud provider directly
handles all of the ingress traffic from tenant’s ISP.
Figure 12b compares the impact of direct peering agreements
versus other interconnections on user-to-cloud latency within Eu-
rope. For increased confidence, we only show latency values for
 pairs – if at least 100 measure-
ments were made for that group. We observe that direct peering
between ISPs and cloud operators has minimal effect on cloud ac-
cess latency between Germany and UK. Latency distributions of
the two interconnection categories are quite similar across all cloud
providers – indicating that the user latency to the cloud is affected
more by geographical distances (§4.3) than routing. We also find
minimal latency differences between paths using private peering
and public Internet connecting Germany and the UK. The inference
73
remains true for paths between Ukrainian ISPs and DCs in UK (see
Figure 17 in Appendix A.4), showcasing the after-effects of signifi-
cant innovations in the backhaul within the continent that has left
little margin for overheads due to network management.
The trend, however, differs significantly in paths from Japanese
VPs to Indian DCs (see Figure 13b). While the median cloud ac-
cess latencies are comparable across providers, we find that direct
peering significantly reduces the latency variations in the connec-
tion (notice shorter box heights in the plot). This result could be
a possible outcome of cloud provider’s investment into undersea
cables [58, 79]. To investigate this further, we plot the interconnec-
tions between VPs in Bahrain to same DCs in India (see Figure 18b
in Appendix A.4). Since these two countries are connected by land,
routing within these regions is not dependent on common undersea
cables. Here we observe a significant latency difference between
direct and indirect interconnections, with direct peering achieving
consistently shorter latencies.
Similar to the study by Arnold et al. [8], we observe that privately
interconnecting paths in Europe can either ingress cloud WAN close
to the VP or the server. While different ingress location can affect
user path length differently (≈30% reduction when ingress is close
to VP), we found little to no impact of this behaviour on overall
access latency. On the other hand, direct peering paths from Japan
almost always ingresses cloud WAN within the country itself.
BABAAMZNDOGCPIBMLINMSFTORCLVLTRVodafone(AS3209)D.Telekom(AS3320)Telef´onica(AS6805)Liberty(AS6830)1&1(AS8881)11111111111020406080100Percentagedirect1AS2+AS1IXPBABAAMZNDOGCPIBMLINMSFTORCLVLTR020406080100120Latency[ms]directpeeringintermediateASBABAAMZNDOGCPIBMLINMSFTORCLVLTRKDDI(AS2516)BIGLOBE(AS2518)NTT(AS4713)OPTAGE(AS17511)SoftBank(AS17676)1111111020406080100Percentagedirect1AS2+AS1IXPBABAAMZNDOGCPLINMSFTORCL0100200300400Latency[ms]directpeeringintermediateASCloudy with a Chance of Short RTTs
IMC ’21, November 2–4, 2021, Virtual Event, USA
Takeaway — Direct peering between cloud and ISP has almost
negligible impact on cloud access latency in Europe; showcasing
the already well-provisioned public backbone within the region.
In Asia, direct peering significantly reduces the long latency tails,
which can be especially useful for cloud-backed immersive appli-
cations. For in-land interconnections within the continent, direct
peering also improves the median latency by a significant margin.
7 DISCUSSION
Although our experiments cover a wide range of scenarios, we are
inherently limited by the measurement platforms and the nature of
network connections. Analysis based on traceroutes, such as ours,
are susceptible to inconsistencies from asymmetric forwarding and
reverse paths [26, 32]. Likewise, traceroutes only provide us the
base network latency between the measurement points. The actual
user-observed delay at the application level can be higher due to
processing and internal queueing. In this respect, our reported laten-
cies represent the best-case scenario and can be considered as lower
bounds on achievable performance. Our final limitation comes from
the Speedchecker platform. Our experiments over Speedchecker do
not include the last-mile access type (WiFi/cellular) throughout the
duration of the measurement. As a result, our analysis inferring the
type of wireless access through traceroutes can contain several
false positives – including possible switches between WiFi and
cellular within the measurement test duration.
In light of the factors affecting cloud access latencies for Internet
uses on a global scale, we now discuss the utility of deploying
compute edge servers outside of the cloud domain.
Which networks can live without the edge? Our results indi-
cate that the latencies to the cloud in regions with dense datacenter
deployments are quite stable, regardless of the wired or wireless
last-mile connectivity. Developing regions, with poorer connec-
tions to cloud datacenters show much more promise for bringing
services closer to the users, such as via edge computing [23]. Many
of these regions would see considerable improvements in connec-
tivity even with a sparser edge deployment, e.g., via a regional edge
or a small datacenter [59, 62]. Developed regions with many cloud
datacenters can only see benefits when the deployment of edge is
very dense and widely spread, hence their capabilities would not be
much improved by edge. Considering that the investments required
to set up an edge infrastructure would exceed cloud investments in
peering and private WANs, the final preference is likely going to
be dictated by the responsible entity, e.g., ISPs would prefer to use
the edge while cloud would likely extend their existing WAN.
Which applications can live without edge? As our results show,
out of the three key latency thresholds (§2.1), cloud is able to sat-
isfy HRT in almost all of the measured cases, and HPL is easily
achievable in regions with denser datacenter deployments. How-
ever, when it comes to MTP-constrained applications, the picture
becomes muddier. As showcased in §5, the absolute wireless last-
mile latencies are already on the order of 20+ ms, which makes
MTP-constrained applications infeasible, unless all of the process-
ing happens on-board the mobile device. The results hold true for all
of the regions and are independent of the density of cloud datacen-
ters. While wireless last-mile latencies can be expected to decrease
(e.g., 5G promising latencies down to 1 ms), it is far from certain
whether the reduction would be substantial enough to enable edge
deployments since the latency overhead due to transit is minimal
(at least in developed regions). But already now, cloud can fully
support both HRT and HPL-based applications to an increasing
extent. MTP-constrained applications are not really feasible, espe-
cially with wireless last-mile, and barring dramatic improvements
in wireless technology, are likely to remain infeasible.
While peering agreements between operators help ensure lower
latency variations, our results do not indicate that they would
markedly reduce the base latencies (especially in regions with a
well-provisioned public backhaul). More consistent latencies will
aid applications as they make the network more predictable [61].
For example, a video streaming service can make more accurate
estimates about the need for buffering and optimize video quality
better when the network is stable. Deploying edge servers would