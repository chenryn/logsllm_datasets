(promotional pages) to the search engines, instead of hiding it
from them. Other issues related to search results include the
delay introduced by page indexing and page expiration. Again,
although our approach is not designed to capture a promotional
infection before it is indexed by the search engines, the impact
of the infection is also limited at that time, simply because its
whole purpose is to advertise some malicious materials, which
is not well served without the infected pages being discovered
by the search engine. For page expiration, we need to consider
the fact that as long as the URLs of the promoted content are
still alive, the attack is still in effect, since letting people ﬁnd
the URLs is the very purpose of the attack. Whether the URLs
are still there can be conﬁrmed by crawling the links. Further,
the snippet of the search results, even for the pages that are
already expired, can still be utilized to ﬁnd new keywords.
The adversary may play other evasion tricks, by adding
more relevant keywords to the infected page to make the
content look more consistent with the website’s theme, or
721721
hiding the inconsistent content by embedding it within images.
However, even in the presence of relevant content, the malicious
keywords can still be recovered and cause an observable
semantic deviation from the theme of the original website, as
long as the keywords are sufﬁciently frequent to be picked up by
the search engine and contribute to the change of the malicious
content’s rank in search results. Hiding content in images results
in neglect of malicious content in the search results, which
is not what the adversary wants. Fundamentally, no matter
what the adversary does, the fact remains that any attempt to
cover the content being advertised will inevitably undermine
the effectiveness of the promotional effort. Another evasion
strategy is to just compromise the website with compatible
semantics. This approach will signiﬁcantly limit the attack
targets the adversary can have. Particularly, it is less clear how
this can be done for sTLDs. Note that even selling medicine
on a health institution’s site can be captured, as the infections
of the NIH pages shown at the beginning of the paper.
Limitations. As mentioned earlier, our current design is
focused on detecting the infections of sTLD sites, since they
have well-deﬁned semantic meanings and are a soft target
for the adversary. In the meantime, gTLDs are also known
to be extensively compromised for promotion purposes. A
natural follow-up step is to develop the semantic technologies
for protecting those domains. This is completely feasible,
as demonstrated in our preliminary study (Section V-B): by
leveraging the Alexa categories, the semantics of even those
more generic domains can also be identiﬁed and compared
with that of the content it hosts.
Moreover, our semantic-based detection technique does not
differentiate between server injected domains, blog/forum Spam
and URL redirection [22] (e.g., posting ads on a .edu forum or
utilizing the server-side script of a .gov domain to dynamically
create a page under the domain with promotion content, see
Section I). In our research, we randomly sampled 100 detected
pages and found that about 20% of them are Spam, which
are also considered illicit advertising [22]. A follow-up step
is to develop automatic technologies to identify those cases,
so we can respond to them in a different way (e.g., through
input sanitization). For example, a comment page oftentimes
can be detected from the keywords such as “comment” or
“redirect” involved in its link; such a page, once found to
promote malicious content, can be further analyzed to determine
whether the content is link Spam or caused by an infection.
Also, the use of search engines has a performance implica-
tion. Search service providers often have limits on the crawling
frequency one can have, which causes delay in detecting
malicious content and affects the scalability of our technique.
On the other hand, given the effectiveness of SEISE in catching
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:32 UTC from IEEE Xplore.  Restrictions apply. 
promotional infections, we believe that a collaboration with the
search provider to detect Internet-wide infections is completely
possible.
Lesson learnt. Our study shows that sTLD sites are often
under-protected. Particularly for universities and other research
institutions, their IT infrastructures tend to be open and loosely
controlled. As a prominent example, in a university, individual
servers are often protected at the department levels while
the university-level IT often only takes care of network-level
protection (e.g., intrusion detection). The problem is that,
oftentimes, the hosts are administrated by less experienced
people and include out-dated and vulnerable software, while
given the nature of the promotional infections, they are less
conspicuous in the network trafﬁc, compared with other
intrusions (e.g., setting up a campus bot net). We believe
that SEISE, particularly its Context Analyzer, can play the
role of helping the web administrators of these organizations
detect the problems with those less-protected hosts. Of course,
a more fundamental solution is to have a better centralized
control, at least in terms of discovering the security risks at
the host level and urging the administrators of these hosts to
keep their software up-to-date.
Responsible disclosure. Since the discovery of infected do-
mains, we have been in active communication with the parties
affected. So far, we have reported over 120 FQDNs to CERT
in US and 136 FQDNs to CCERT (responsible for .edu.cn)
in China, the two countries hosting most infected domains.
By now, CCERT have conﬁrmed our report, and notiﬁed all
related organizations, in which 27 responded and ﬁxed their
problems. However, it is difﬁcult for us to directly contact the
victims to get more details (like log access) from the infected
servers. On the other hand, given the scale of the attacks we
discovered, the whole reporting process will take time.
VII. RELATED WORK
injected sites. How to detect
Detection of
injection of
malicious content has been studied for long. Techniques have
been developed to analyze web content, redirection chains
and URL pattern. Examples of the content-based detection
include a DOM-based clustering systems for monitoring Scam
websites [19], and a system monitoring the evolution of
web content, called Delta [16], which keeps track of the
content and structure modiﬁcations across different versions
of a website, and identiﬁes an infection using signatures
generated from such modiﬁcations. More recently, Soska et
al. works on detecting new attack trends instead of the attacks
themselves [29]. Their proposed system leverages the features
from web trafﬁc, ﬁle system and page content, and is able to
predict whether currently benign websites will be compromised
in the near future. Borgolte et al. introduces Meerkat [17], a
computer vision approach to website defacement detection. The
technique is capable of identifying malicious content changes
from screenshots of the website. Other studies focus on mali-
cious redirectors and attack infrastructures. Examples include
JsRED [24] that uses a differential analysis to automatically
detect malicious redirect scripts, and Shady Path [31] that
captures a malicious web page by looking at its redirection
graph. Compared with those techniques, our approach is
different in that it automatically analyzes the semantics of web
content and looks for its inconsistency with the theme of the
hosting website. We believe that the semantics-based approach
is the most effective solution to promotional infections, which
can be easily detected by checking the semantics of infected
sites but hard to identify by just looking at the syntactic
elements of the sites: e.g., both legitimate and malicious ads
can appear on a website, using the same techniques like
redirections, iframe, etc. Further, we do not look into web
content or infrastructure at all, and instead, leverage the search
results to detect infections. Our study shows that this treatment
is sufﬁcient for ﬁnding promotional infections and much more
efﬁcient than content and infrastructure-based approaches.
Similar to our work, Evilseed [21] also uses search results
for malicious website detection. However, the approach is only
based upon searching the URL patterns extracted from the
malicious links and never touches the semantics of search
results. Our study shows that focusing only on the syntactic
features such as URL patterns is insufﬁcient for accurate
detection of promotional infections. Indeed, Evilseed reports
a huge false detection rate, above 90%, and can only serve
as a pre-ﬁltering system. On the other hand, our technique
inspects all the snippet of search results (not just URLs),
automatically discovering and analyzing their semantics. This
turns out to be much more effective when it comes to malicious
promotional content: SEISE achieves low FDR (1.5%) at a
detection coverage over 90%.
Study on blackhat SEO. Among the malicious activities
performed by a promotional infection is blackhat SEO (also
referred to webspam), which has also been intensively studied.
For instance, Wang et al. investigated the longitudinal oper-
ations of SEO campaigns by inﬁltrating an SEO botnet [34].
Leontiadis et al. conducted a long-term study using 5 million
search results covering nearly 4 years to investigate the
evolution of search engine poisoning [23]. Also, Wang et al.
examined the effectiveness of the interventions against the SEO
abuse for counterfeit luxury goods [33]. Moore et al. studied the
trending terms used in search-engine manipulation [25]. Also,
Leontiadis et al. observed .edu sites that were compromised
for search redirection attack in illicit online prescription drug
trade, and brieﬂy discussed their lifetime and volume [22]. In
our paper, we conduct a more comprehensive measurement on
403 sTLD, and multiple illicit practices beside drug trade were
involved.
VIII. CONCLUSION
In this paper, we report our study on promotional infections,
which introduce a large semantic gap between the infected
sTLD and the illicit promotional content injected. Exploiting
this gap, our semantic-based approach, SEISE, utilizes NLP
techniques to automatically choose IBTs and analyze search
result pages to ﬁnd those truly compromised. Our study shows
that SEISE introduces low false detection rate (about 1.5%)
722722
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:32 UTC from IEEE Xplore.  Restrictions apply. 
with over 90% coverage. It is also capable of automatically
expanding its IBT list to not only include new terms but also
terms from new IBT categories. Running on 100K FQDNs,
SEISE automatically detects 11K infected FQDN, which brings
to light the signiﬁcant impact of the promotional infections:
among those infected are the domains belonging to leading
educational institutions, government agencies, even the military,
with 3% of .edu and .gov, and over one thousand domains
of .gov.cn falling prey to illicit advertising campaigns. Our
research further demonstrates the importance of sTLDs to the
adversary and the bar our technique raises for the attacks.
Moving forward, we believe that there is a great potential to
extend the technique for protecting gTLDs, as indicated by our
preliminary study. Further, we are exploring the possibility to
provide a public service for detecting such infections.
IX. ACKNOWLEDGMENT
This work was supported by the National Science Foundation
(grants CNS-1223477, CNS-1223495 and CNS-1527141);
Natural Science Foundation of China (grant 61472215). We
thank our anonymous reviewers for their useful comments.
REFERENCES
[1] “Bing search api.” https://datamarket.azure.com/dataset/bing/search.
[2] “Domaintools,” https://www.domaintools.com.
[3] “Farsight security information exchange,” https://api.dnsdb.info/.
[4] “Google web search api.” https://developers.google.com/web-search/?hl=
[5] “Phishtank,” https://www.phishtank.com.
[6] “Public sufﬁx list,” https://publicsufﬁx.org/list/.
[7] “scikit-learn, machine learning in python.” http://scikit-learn.org/stable/.
https://developer.similarweb.com/similar_
[8] “Similar websites
api,”
[9] “Sponsored top level domain (stld),” http://icannwiki.com/index.php/
en.
websites_api.
STLD.
[10] “Stopword lists,” http://www.ranks.nl/stopwords.
[11] “Virustotal,” https://www.virustotal.com/.
[12] “word2vec, tool for computing continuous distributed representations of
words.” https://code.google.com/p/word2vec/.
some
and phrases
trigger
that
[13] “Words
spam ﬁlters,” http://
webmarketingtoday.com/articles/spamﬁlter_phrases/, 2002.
[14] “Email spam ﬁlter trigger words to avoid in your e-campaigns,” http:
//www.mannixmarketing.com/blog/spam-trigger-words/, 2009.
[15] “50 of the most competitive seo keywords!” https://moz.com/ugc/
50-of-the-most-competitive-seo-keywords, 2012.
[16] K. Borgolte, C. Kruegel, and G. Vigna, “Delta: automatic identiﬁcation
of unknown web-based infection campaigns,” in Proceedings of the
2013 ACM SIGSAC conference on Computer & communications security.
ACM, 2013, pp. 109–120.
[17] K. Borgolte, C. Kruegel, and G. Vigna, “Meerkat: detecting website
defacements through image-based object recognition,” in Proceedings
of the 24th USENIX Conference on Security Symposium. USENIX
Association, 2015, pp. 595–610.
[18] CleanMX, “Viruswatch – viruswatch watching adress changes of malware
URL’s,” http://lists.clean-mx.com/cgi-bin/mailman/listinfo/viruswatch/.
[19] M. F. Der, L. K. Saul, S. Savage, and G. M. Voelker, “Knock it
off: Proﬁling the online storefronts of counterfeit merchandise,” in
Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining. ACM, 2014, pp. 1759–1768.
[20] R. Garside and N. Smith, “A hybrid grammatical tagger: Claws4,” Corpus
annotation: Linguistic information from computer text corpora, pp. 102–
121, 1997.
[21] L. Invernizzi, P. M. Comparetti, S. Benvenuti, C. Kruegel, M. Cova, and
G. Vigna, “Evilseed: A guided approach to ﬁnding malicious web pages,”
in Security and Privacy (SP), 2012 IEEE Symposium on.
IEEE, 2012,
pp. 428–442.
[22] N. Leontiadis, T. Moore, and N. Christin, “Measuring and analyzing
search-redirection attacks in the illicit online prescription drug trade.” in
USENIX Security Symposium, 2011.
[23] N. Leontiadis, T. Moore, and N. Christin, “A nearly four-year longitudinal
study of search-engine poisoning,” in Proceedings of the 2014 ACM
SIGSAC Conference on Computer and Communications Security. ACM,
2014, pp. 930–941.
[24] Z. Li, S. Alrwais, X. Wang, and E. Alowaisheq, “Hunting the red fox
online: Understanding and detection of mass redirect-script injections,”
in Security and Privacy (SP), 2014 IEEE Symposium on.
IEEE, 2014,
pp. 3–18.
[25] T. Moore, N. Leontiadis, and N. Christin, “Fashion crimes: trending-term
exploitation on the web,” in Proceedings of the 18th ACM conference
on Computer and communications security. ACM, 2011, pp. 455–466.
[26] H. Schmid, “Probabilistic part-of-speech tagging using decision trees,” in
Proceedings of the international conference on new methods in language
processing, vol. 12. Citeseer, 1994, pp. 44–49.
[27] B. Skiera, J. Eckert, and O. Hinz, “An analysis of the importance of the
long tail in search engine marketing,” Electronic Commerce Research
and Applications, vol. 9, no. 6, pp. 488–494, 2010.
report, mid-year
[28] Sophos,
“Security
2011,”
threat
https:
//www.sophos.com/en-us/medialibrary/Gated%20Assets/white%
20papers/sophossecuritythreatreportmidyear2011wpna.pdf, 2011.
[29] K. Soska and N. Christin, “Automatically detecting vulnerable websites
before they turn malicious,” in Proc. USENIX Security, 2014.
[30] R. Stephan and F. Russ, “topia.termextract 1.1.0,” https://pypi.python.
org/pypi/topia.termextract.
[31] G. Stringhini, C. Kruegel, and G. Vigna, “Shady paths: Leveraging
surﬁng crowds to detect malicious web pages,” in Proceedings of the
2013 ACM SIGSAC conference on Computer & communications security.
ACM, 2013, pp. 133–144.
[32] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer, “Feature-rich part-
of-speech tagging with a cyclic dependency network,” in Proceedings of
the 2003 Conference of the North American Chapter of the Association
for Computational Linguistics on Human Language Technology-Volume
1. Association for Computational Linguistics, 2003, pp. 173–180.
[33] D. Y. Wang, M. Der, M. Karami, L. Saul, D. McCoy, S. Savage, and
G. M. Voelker, “Search+ seizure: The effectiveness of interventions on
seo campaigns,” in Proceedings of the 2014 Conference on Internet
Measurement Conference. ACM, 2014, pp. 359–372.
[34] D. Y. Wang, S. Savage, and G. M. Voelker, “Juice: A longitudinal study
of an seo botnet.” in NDSS, 2013.
[35] N. Xue et al., “Chinese word segmentation as character tagging,”
Computational Linguistics and Chinese Language Processing, vol. 8,
no. 1, pp. 29–48, 2003.
723723
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:32 UTC from IEEE Xplore.  Restrictions apply.