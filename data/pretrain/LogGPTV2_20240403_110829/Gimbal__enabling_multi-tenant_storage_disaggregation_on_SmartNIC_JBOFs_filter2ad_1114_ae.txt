B
M
i
t
(
h
d
w
d
n
a
B
r
e
k
r
o
W
800
600
400
200
0
ReFlex
128KB
4KB
FlashFQ
Parda
Schemes
)
s
/
B
M
(
/
W
B
d
e
t
a
g
e
r
g
g
A
2,000
1,500
1,000
500
0
ReFlex
Gimbal
Read
Write
FlashFQ
Parda
Schemes
1,000
800
600
400
200
0
)
s
/
B
M
(
/
W
B
d
e
t
a
g
e
r
g
g
A
ReFlex
Gimbal
Read
Write
FlashFQ
Parda
Schemes
Gimbal
(a) Clean-SSD: 4KB/128KB Read IO Size (Bandwidth)
l
i
t
U
-
ùëì
4
2
0
ReFlex
128KB
4KB
1.0
FlashFQ
Parda
Schemes
Gimbal
(b) Clean-SSD: 128KB Read/Write (Bandwidth)
(c) Fragment-SSD: 4KB Read/Write (Bandwidth)
2
1.5
1
0.5
0
l
i
t
U
-
ùëì
ReFlex
Read
Write
1.0
FlashFQ
Parda
Schemes
l
i
t
U
-
ùëì
2
1
0
ReFlex
Gimbal
Read
Write
1.0
FlashFQ
Parda
Schemes
Gimbal
(d) Clean-SSD: 4KB/128KB Read IO Size (ùëì -Util)
(e) Clean-SSD: 128KB Read/Write (ùëì -Util)
(f) Fragment-SSD: 4KB Read/Write (ùëì -Util)
Figure 7: Fairness in various mixed workloads on different schemes and SSD conditions
ReFlex
FalshFQ
Parda
Gimbal
)
c
e
s
u
(
y
c
n
e
t
a
L
105
104
103
102
)
c
e
s
u
(
y
c
n
e
t
a
L
104
103
102
Read Avg. Read 99th Read 99.9th Write Avg. Write 99th Write 99.9th
Percentiles
(a) Clean-SSD (128KB IO latency, Log-scale)
Read Avg. Read 99th Read 99.9th Write Avg. Write 99th Write 99.9th
Percentiles
(b) Fragment-SSD (4KB IO latency, Log-scale)
Figure 8: Read/Write IO latency comparison. 16 workers each for
Read and Write.
all workloads. FlashFQ and ReFlex have no flow control mecha-
nisms and incur high latencies under high worker consolidation or
a large number of outstanding IOs.
5.5 Dynamic Workloads
This experiment changes the workload dynamically and demon-
strates the importance of estimating the dynamic write cost. We
initiate 8 read workers at the beginning of the experiment and add
a write worker at a 5-second interval until the number of read and
write workers becomes the same (i.e., 8 read and 8 write workers
run simultaneously at the maximum congestion). We then drop one
read worker at a time at the same interval. Additionally, we limit
the maximum IO rate of the single worker to 200MB/s and 60MB/s
for read and write, respectively. This simulates an application with
a specific IO rate.
Figure 9 shows the bandwidth over time for each worker and
the latency for each IO type. Gimbal shows that it adapts to the
workload characteristics and can accelerate write IOs accordingly.
The first write worker benefits from the internal write buffer so that
most IOs complete immediately. The latency thus is about 70 usec
(i.e., less than the minimum latency threshold) on average, while
the average read latency is about 1000 usec at the same moment.
As discussed in Section 3.4, Gimbal reduces the write cost to 1
and benefits from the SSD device optimization for writes in this
Read Worker
Write Worker
)
s
/
B
M
(
W
B
/
r
e
k
r
o
W
)
s
u
(
y
c
n
e
t
a
L
250
200
150
100
50
1,250
1,000
750
500
250
10
30
50
Time(sec)
70
90
Figure 9: Performance over time as the number of workers changes
(the latency is a raw device latency measured directly in Gimbal and
averaged out every 100ms)
case. After the second writer arrives, the write rate starts to exceed
the capability of the internal buffer and the latency grows more
than 10 times. Gimbal detects such a latency change at runtime
and increases the write cost. As a result, the bandwidth for write
workers converges to the fair bandwidth share.
5.6 Application Performance
In this experiment, we run 24 RocksDB instances over three Smart-
NIC JBOFs that are configured with different mechanisms on frag-
mented SSDs. As described before, we enhanced RocksDB with
a rate limiter and a read load balancer. We report the aggregated
throughput and average/99.9th latency (Figure 10). On average,
across five benchmarks, Gimbal outperforms ReFlex, Parda, and
FlashFQ by x1.7, x2.1, x1.3 in terms of throughput, and reduces the
average/99.9th latency by 34.9%/48.0%, 54.7%/30.2%, 20.1%/26.8%,
respectively. Among them, YCSB-A and YCSB-F (i.e., update-heavy
and read-modify-write) workloads benefit the most while YCSB-
C (i.e., read-only) observes the least performance improvements.
This is because Gimbal can schedule a mix of read/write IOs more
efficiently to maximize the SSD performance.
Scalability. We used the same RocksDB configuration and scaled
the number of RocksDB instances over three SmartNIC JBOFs (as
above). Figures 11 and 12 present throughput and average read
latency, respectively. YCSB-A, YCSB-B, and YCSB-D max out their
performance with 20 instances, while YCSB-F achieves the max
throughput under 16 instances. For example, the average read la-