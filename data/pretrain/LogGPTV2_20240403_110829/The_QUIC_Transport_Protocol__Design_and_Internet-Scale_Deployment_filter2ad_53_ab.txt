by the server, and incentivizes servers to not lag behind clients
in deployment of newer versions. To prevent downgrade attacks,
the initial version requested by the client and the list of versions
supported by the server are both fed into the key-derivation function
at both the client and the server while generating the final keys.
3.2 Stream Multiplexing
Applications commonly multiplex units of data within TCP’s single-
bytestream abstraction. To avoid head-of-line blocking due to TCP’s
sequential delivery, QUIC supports multiple streams within a con-
nection, ensuring that a lost UDP packet only impacts those streams
whose data was carried in that packet. Subsequent data received on
other streams can continue to be reassembled and delivered to the
application.
QUIC streams are a lightweight abstraction that provide a reliable
bidirectional bytestream. Streams can be used for framing applica-
tion messages of arbitrary size—up to 264 bytes can be transferred on
a single stream—but they are lightweight enough that when sending
a series of small messages a new stream can reasonably be used for
each one. Streams are identified by stream IDs, which are statically
allocated as odd IDs for client-initiated streams and even IDs for
server-initiated streams to avoid collisions. Stream creation is im-
plicit when sending the first bytes on an as-yet unused stream, and
stream closing is indicated to the peer by setting a "FIN" bit on the
last stream frame. If either the sender or the receiver determines that
the data on a stream is no longer needed, then the stream can be
canceled without having to tear down the entire QUIC connection.
Though streams are reliable abstractions, QUIC does not retransmit
data for a stream that has been canceled.
A QUIC packet is composed of a common header followed by one
or more frames, as shown in Figure 5. QUIC stream multiplexing is
implemented by encapsulating stream data in one or more stream
frames, and a single QUIC packet can carry stream frames from
multiple streams.
The rate at which a QUIC endpoint can send data will always
be limited (see Sections 3.5 and 3.6). An endpoint must decide
how to divide available bandwidth between multiple streams. In our
implementation, QUIC simply relies on HTTP/2 stream priorities [8]
to schedule writes.
3.3 Authentication and Encryption
With the exception of a few early handshake packets and reset pack-
ets, QUIC packets are fully authenticated and mostly encrypted.
Figure 5 illustrates the structure of a QUIC packet. The parts of the
QUIC packet header outside the cover of encryption are required
either for routing or for decrypting the packet: Flags, Connection ID,
Version Number, Diversification Nonce, and Packet Number4. Flags
encode the presence of the Connection ID field and length of the
Packet Number field, and must be visible to read subsequent fields.
The Connection ID serves routing and identification purposes; it is
used by load balancers to direct the connection’s traffic to the right
server and by the server to locate connection state. The version num-
ber and diversification nonce fields are only present in early packets.
The server generates the diversification nonce and sends it to the
client in the SHLO packet to add entropy into key generation. Both
4The details of which fields are visible may change during QUIC’s IETF standardization.
Figure 5: Structure of a QUIC packet, as of version 35 of Google’s
QUIC implementation. Red is the authenticated but unencrypted pub-
lic header, green indicates the encrypted body. This packet structure is
evolving as QUIC gets standardized at the IETF [2].
endpoints use the packet number as a per-packet nonce, which is
necessary to authenticate and decrypt packets. The packet number is
placed outside of encryption cover to support decryption of packets
received out of order, similar to DTLS [56].
Any information sent in unencrypted handshake packets, such as
in the Version Negotiation packet, is included in the derivation of
the final connection keys. In-network tampering of these handshake
packets causes the final connection keys to be different at the peers,
causing the connection to eventually fail without successful decryp-
tion of any application data by either peer. Reset packets are sent
by a server that does not have state for the connection, which may
happen due to a routing change or due to a server restart. As a result,
the server does not have the connection’s keys, and reset packets are
sent unencrypted and unauthenticated5.
3.4 Loss Recovery
TCP sequence numbers facilitate reliability and represent the order
in which bytes are to be delivered at the receiver. This conflation
causes the "retransmission ambiguity" problem, since a retransmit-
ted TCP segment carries the same sequence numbers as the origi-
nal packet [39, 64]. The receiver of a TCP ACK cannot determine
whether the ACK was sent for the original transmission or for a
retransmission, and the loss of a retransmitted segment is commonly
detected via an expensive timeout. Each QUIC packet carries a new
packet number, including those carrying retransmitted data. This
design obviates the need for a separate mechanism to distinguish the
ACK of a retransmission from that of an original transmission, thus
avoiding TCP’s retransmission ambiguity problem. Stream offsets
in stream frames are used for delivery ordering, separating the two
functions that TCP conflates. The packet number represents an ex-
plicit time-ordering, which enables simpler and more accurate loss
detection than in TCP.
QUIC acknowledgments explicitly encode the delay between the
receipt of a packet and its acknowledgment being sent. Together with
monotonically-increasing packet numbers, this allows for precise
5QUIC connections are susceptible to off-path third-party reset packets with spoofed
source addresses that terminate the connection. IETF work will address this issue.
186
The QUIC Transport Protocol
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
network round-trip time (RTT) estimation, which aids in loss detec-
tion. Accurate RTT estimation can also aid delay-sensing congestion
controllers such as BBR [10] and PCC [16]. QUIC’s acknowledg-
ments support up to 256 ACK blocks, making QUIC more resilient
to reordering and loss than TCP with SACK [46]. Consequently,
QUIC can keep more bytes on the wire in the presence of reordering
or loss.
These differences between QUIC and TCP allowed us to build
simpler and more effective mechanisms for QUIC. We omit further
mechanism details in this paper and direct the interested reader to
the Internet-draft on QUIC loss detection [33].
3.5 Flow Control
When an application reads data slowly from QUIC’s receive buffers,
flow control limits the buffer size that the receiver must main-
tain. A slowly draining stream can consume the entire connec-
tion’s receive buffer, blocking the sender from sending data on
other streams. QUIC ameliorates this potential head-of-line block-
ing among streams by limiting the buffer that a single stream can
consume. QUIC thus employs connection-level flow control, which
limits the aggregate buffer that a sender can consume at the receiver
across all streams, and stream-level flow control, which limits the
buffer that a sender can consume on any given stream.
Similar to HTTP/2 [8], QUIC employs credit-based flow-control.
A QUIC receiver advertises the absolute byte offset within each
stream up to which the receiver is willing to receive data. As data
is sent, received, and delivered on a particular stream, the receiver
periodically sends window update frames that increase the advertised
offset limit for that stream, allowing the peer to send more data on
that stream. Connection-level flow control works in the same way
as stream-level flow control, but the bytes delivered and the highest
received offset are aggregated across all streams.
Our implementation uses a connection-level window that is sub-
stantially larger than the stream-level window, to allow multiple
concurrent streams to make progress. Our implementation also uses
flow control window auto-tuning analogous to common TCP imple-
mentations (see [1] for details.)
3.6 Congestion Control
The QUIC protocol does not rely on a specific congestion control
algorithm and our implementation has a pluggable interface to al-
low experimentation. In our deployment, TCP and QUIC both use
Cubic [26] as the congestion controller, with one difference worth
noting. For video playback on both desktop and mobile devices, our
non-QUIC clients use two TCP connections to the video server to
fetch video and audio data. The connections are not designated as au-
dio or video connections; each chunk of audio and video arbitrarily
uses one of the two connections. Since the audio and video streams
are sent over two streams in a single QUIC connection, QUIC uses
a variant of mulTCP [14] for Cubic during the congestion avoidance
phase to attain parity in flow-fairness with the use of TCP.
3.7 NAT Rebinding and Connection Migration
QUIC connections are identified by a 64-bit Connection ID. QUIC’s
Connection ID enables connections to survive changes to the client’s
IP and port. Such changes can be caused by NAT timeout and rebind-
ing (which tend to be more aggressive for UDP than for TCP [27])
or by the client changing network connectivity to a new IP address.
While QUIC endpoints simply elide the problem of NAT rebinding
by using the Connection ID to identify connections, client-initiated
connection migration is a work in progress with limited deployment
at this point.
3.8 QUIC Discovery for HTTPS
A client does not know a priori whether a given server speaks QUIC.
When our client makes an HTTP request to an origin for the first time,
it sends the request over TLS/TCP. Our servers advertise QUIC sup-
port by including an "Alt-Svc" header in their HTTP responses [48].
This header tells a client that connections to the origin may be at-
tempted using QUIC. The client can now attempt to use QUIC in
subsequent requests to the same origin.
On a subsequent HTTP request to the same origin, the client
races a QUIC and a TLS/TCP connection, but prefers the QUIC
connection by delaying connecting via TLS/TCP by up to 300 ms.
Whichever protocol successfully establishes a connection first ends
up getting used for that request. If QUIC is blocked on the path, or
if the QUIC handshake packet is larger than the path’s MTU, then
the QUIC handshake fails, and the client uses the fallback TLS/TCP
connection.
3.9 Open-Source Implementation
Our implementation of QUIC is available as part of the open-source
Chromium project [1]. This implementation is shared code, used
by Chrome and other clients such as YouTube, and also by Google
servers albeit with additional Google-internal hooks and protections.
The source code is in C++, and includes substantial unit and end-to-
end testing. The implementation includes a test server and a test
client which can be used for experimentation, but are not tuned for
production-level performance.
4 EXPERIMENTATION FRAMEWORK
Our development of the QUIC protocol relies heavily on contin-
ual Internet-scale experimentation to examine the value of various
features and to tune parameters. In this section we describe the ex-
perimentation frameworks in Chrome and our server fleet, which
allow us to experiment safely with QUIC.
We drove QUIC experimentation by implementing it in Chrome,
which has a strong experimentation and analysis framework that
allows new features to be A/B tested and evaluated before full
launch. Chrome’s experimentation framework pseudo-randomly as-
signs clients to experiments and exports a wide range of metrics,
from HTTP error rates to transport handshake latency. Clients that
are opted into statistics gathering report their statistics along with
a list of their assigned experiments, which subsequently enables us
to slice metrics by experiment. This framework also allows us to
rapidly disable any experiment, thus protecting users from problem-
atic experiments.
We used this framework to help evolve QUIC rapidly, steering its
design according to continuous feedback based on data collected at
the full scale of Chrome’s deployment. Monitoring a broad array of
metrics makes it possible to guard against regressions and to avoid
187
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
A. Langley et al.
imposing undue risks that might otherwise result from rapid evolu-
tion. As discussed in Section 5, this framework allowed us to contain
the impact of the occasional mistake. Perhaps more importantly and
unprecedented for a transport, QUIC had the luxury of being able to
directly link experiments into analytics of the application services
using those connections. For instance, QUIC experimental results
might be presented in terms of familiar metrics for a transport, such
as packet retransmission rate, but the results were also quantified
by user- and application-centric performance metrics, such as web
search response times or rebuffer rate for video playbacks. Through
small but repeatable improvements and rapid iteration, the QUIC
project has been able to establish and sustain an appreciable and
steady trajectory of cumulative performance gains.
We added QUIC support to our mobile video (YouTube) and
search (Google Search) apps as well. These clients have similar
experimentation frameworks that we use for deploying QUIC and
measuring its performance.
Google’s server fleet consists of thousands of machines distributed
globally, within data centers as well as within ISP networks. These
front-end servers terminate incoming TLS/TCP and QUIC connec-
tions for all our services and perform load-balancing across internal
application servers. We have the ability to toggle features on and
off on each server, which allows us to rapidly disable broken or
buggy features. This mechanism allowed us to perform controlled
experiments with QUIC globally while severely limiting the risk
of large-scale outages induced by these experiments. Our servers
report performance data related to current and historic QUIC con-
nections. This data is collected by a centralized monitoring system
that aggregates it and provides visualizations and alerts.
INTERNET-SCALE DEPLOYMENT
5
The experimentation framework described in Section 4 enabled safe
global deployment of QUIC to our users. We first present QUIC’s de-
ployment timeline. We then describe the evolution of one of several
metrics that we monitored carefully as we deployed QUIC globally.
5.1 The Road to Deployment
QUIC support was added to Chrome in June 2013. It was enabled
via an optional command-line flag so usage was effectively limited
to the QUIC development team. In early 2014, we were confident
in QUIC’s stability and turned it on via Chrome’s experimentation
framework for a tiny fraction (< 0.025%) of users. As QUIC proved
to be performant and safe, this fraction was increased. As of January
2017, QUIC is turned on for almost6 all users of Chrome and the
Android YouTube app.
Simultaneously developing and deploying a new secure protocol
has its difficulties however, and it has not been completely smooth
sailing to get to this point. Figure 2 shows QUIC traffic to our
services from February 2015 to December 2016. We now describe
the two notable events seen in the graph.
Unencrypted data in 0-RTT requests: In December 2015, we dis-
covered a vulnerability in our implementation of the QUIC hand-
shake. The vulnerability was traced to a bug in the client code, which
6A small holdback experiment of a few percent allows us to compare QUIC vs. TLS/TCP
performance over time.
Figure 6: Search Latency reduction for users in the QUIC experiment
over an 18-month period. Numbered events are described in Section 5.2.
could result in 0-RTT requests being sent unencrypted in an exceed-
ingly rare corner case. Our immediate response was to disable QUIC
globally at our servers, using the feature toggle mechanism described
in Section 4. This turndown can be seen as the drop to zero, and is
also visible in Figures 6 and 14. The bug was fixed and QUIC traffic
was restored as updated clients were rolled out.
Increasing QUIC on mobile: A substantial fraction of our users
access our services through mobile phones, often using dedicated ap-
plications (apps). The majority of our mobile users perform searches
via our mobile search app. Similarly, the majority of mobile video
playbacks are performed through the YouTube app. The YouTube
app started using QUIC in September 2016, doubling the percentage
of Google’s egress traffic over QUIC, from 15% to over 30%.
5.2 Monitoring Metrics: Search Latency
Our server infrastructure gathers performance data exported by front-
end servers and aggregates them with service-specific metrics gath-
ered by the server and clients, to provide visualizations and alerts.
We will use Search Latency as an example of such a metric. Search
Latency is defined as the delay between when a user enters a search
term into the client and when all the search-result content is gen-
erated and delivered to the client, including images and embedded
content. We analyze the evolution of Search Latency improvement
for users in the QUIC experiment7 versus those using TLS/TCP over
an 18-month period, as shown in Figure 6.
Over the 18-month period shown in Figure 6 there are two notable
regressions, and one improvement. The first regression started in July
2015 (labeled ’1’ in Figure 6) and lasted for about 5 months. This
regression was attributed to changes in our serving infrastructure
and to a client configuration bug, both of which inadvertently caused
a large number of clients in the QUIC experiment to gradually stop
speaking QUIC.
The second regression in December 2015 lines up with the com-
plete disabling of QUIC described in Section 5.1. When QUIC was
re-enabled in February 2016 (labeled ’2’ in Figure 6), the client con-
figuration bug had been fixed, and Search Latency improved, albeit
not to the same extent as earlier, since the infrastructure changes
hadn’t been resolved.
We take a slight detour to describe restricted edge locations
(RELs) and UDP proxying. A large number of our servers are de-
ployed inside ISPs, and we refer to these as RELs. For technical,
commercial and other considerations, RELs do not terminate TLS
sessions to a number of domains. For these domains, RELs therefore