predictive modeling question [20]7.  
In  this  study  we  use  the  three  validation  criteria  (association, 
discriminative power, and predictability) to evaluate the strength 
of  the  relationship  between  the  developer  activity  metrics  and 
security vulnerabilities.  We used SAS8 v9.1.3 for our statistical 
analysis  and  Weka9  v3.6.0  for  the  Bayesian  network  prediction 
model.    
5.1  Association: Are The Metrics Correlated 
With Vulnerable Files? 
To examine how each of the four metrics summarized in Table 3 
are related to security vulnerabilities, we examine the difference 
between the vulnerable files and the neutral files in terms of each 
7  The  framework  from  which  these  validity  criteria  are  defined 
assumes a ratio metric scale, whereas our study is at a nominal 
scale. The exact statistical tests may differ but the spirit of the 
validation criteria remains intact. 
8 http://www.sas.com/ 
9 http://www.cs.waikato.ac.nz/ml/weka/ 
457metric. As suggested in other metrics validation studies [20], we 
use  the  non-parametric  Mann-Whitney-Wilcoxon  (MWW)  test 
for difference in averages. Three outcomes are possible from this 
test:  
• 
The  metric  is  statistically  higher  for  vulnerable  files  than 
neutral files,  
The metric is statistically lower for vulnerable files than for 
neutral files, or 
The  metric  is  not  different  between  neutral  and  vulnerable 
files at a statistically significant level (p=  9),  then 33.3% 
(precision)  of 
is 
considerably  high  given  that  only  1.96%  of  the  system’s  files 
were  vulnerable11.  Thus,  using  NumDevs  provides  16  times 
(=33.3/1.96)  more  discriminative  power  than  random  selection. 
Furthermore, 
than  nine  developers, 
(NumDevs<9),  1.25%  of  the  files  were  vulnerable.  However, 
those 33.3% vulnerable files only account for 9.2% (recall) of the 
known vulnerable files in the system, meaning more metrics with 
high  discriminative  power  are  required.  Table  5  shows  some 
example critical values along with the precision, and recall.  
Note that, in all four plots of Figure 4, when the recall becomes 
small,  the  precision  has  a  greater  variance.  This  effect  is  an 
artifact  of  the  sample  size  decreasing  as  one  uses  a  large, 
therefore more limiting, critical value. 
those  files  would  be  vulnerable,  which 
files  with 
fewer 
for 
Table 5: Discriminative power results 
P* 
Metric 
R* 
AUC*  Example 
Critical 
Value 
270 
9 
25 
40,000 
4.9% 
9.2% 
17.1% 
7.3% 
94.6% 
85.7% 
85.3% 
78.6% 
DNMaxEdgeBetweenness 
7.6% 
33.3% 
NumDevs 
26.7% 
NumCommits 
CNBetweenness 
17.4% 
AUC: area under the ROC curve, P=precision, R=recall. 
Another way to compare the metrics in terms of discrimination is 
with  the  AUC  measurement.  The  AUC  is  calculated  by  finding 
the proportion of occurrences where a given metric for vulnerable 
files outrank a neutral file. Said another way, the AUC represents 
the  probability  that  a  metric’s  value  for  a  randomly-chosen 
vulnerable file is higher than a randomly-chosen neutral file. The 
AUC measurement for each metric is given in Table 5. 
Examining  the  results,  one  can  see  that  different  metrics  have 
different advantages. DNMaxEdgeBetweenness has a low recall, 
10  E.g.  NumDevs  has  a  much  smaller  range  of  values  than 
CNBetweenness,  so  the  size  of  the  difference  in  averages 
cannot be compared 
11  Taken  from  the  1.96%  vulnerable  file  proportion  reported  in 
Section 3 
458NumDevs, 
NumCommits, 
vulnerabilities. 
implying  that  DNMaxEdgeBetweenness  accounts  for  relatively 
few 
and 
CNBetweenness  all  have  high  precisions  when  compared  to  the 
prior probability of 1.96%, but the recalls are still low. With the 
highest recall is NumCommits, meaning that examining files with 
25 commits or more (in the 15 months of development), contain 
17.1% of the known vulnerable files. 
Furthermore, upon examining those files, about one in four would 
be  vulnerable.  The  result  of  having  all  four  metrics  being 
correlated (from Section 5.1), but having low recalls means, that 
while the metrics are correlated with vulnerabilities, none of them 
individually account for all of the vulnerabilities. 
Note  also  that  critical  values  can  vary  according  to  the  project 
being  studied.  Our  critical  values  are  specific  to  the  RHEL4 
kernel  during  the  time  period we studied, so other projects may 
have different exact critical values.  
5.3  Predictability: How Many Vulnerable 
Files Are Explained? 
The  predictability  criterion  is  used  to  estimate  how  many 
vulnerabilities can be explained by combining all of the metrics 
into  a  single  model.  As  a  secondary  purpose,  one  can  use 
predictability analysis as a simulation of how well one could have 
predicted vulnerabilities prior to release. Said another way, if the 
model  can  predict  vulnerable  files,  then  development  teams  can 
use  the  metrics  to  find  vulnerabilities  prior  to  release,  and 
prioritize inspection and fortification efforts accordingly. 
Figure 4: Critical values of metrics for the discriminative power criterion 
459A key element of prediction is the supervised model. A supervised 
model  is  a  method  of  combining  multiple  metrics  into  a  single 
binary  classification  prediction  (“neutral”  or  “vulnerable”)  [24]. 
In  our  study,  we  used  two  modeling  methods:  multivariate 
discriminant  analysis  and  Bayesian  networks.  Discriminant 
analysis is a modeling method that uses an n-dimensional space to 
achieve maximum separation of variables. Discriminant  analysis  
has  widespread  applications,  including  facial  recognition  [12, 
22].  Bayesian  networks use Bayesian inference on a network of 
metrics,  taking  into  account  conditional  dependencies  between 
metrics.  Bayesian  networks  also  have  widespread  applications, 
including gene  expression [16] and  satellite  failure monitoring 
systems. 
Supervised models require a training set and a validation set. In 
this  study,  we  use  cross  validation  to  generate  each  set.  For 
discriminant analysis, we used hold-one-out cross validation with 
recall,  precision,  and  inspection  rate  as  defined  in  Section  2.2. 
Hold-one-out  cross-validation 
iteratively 
removing  each  data  point  from  the  set,  training  on  all  but  the 
removed  data  point,  then  predicting  for  the  removed  point.  For 
Bayesian  Networks  we  used  ten-fold  cross-validation.  Ten-fold 
cross  validation  is  similar  to  hold-one-out,  except  the  data  is 
randomly partitioned into 10 partitions, with each partition being 
the held-out test set exactly once. Since ten-fold cross-validation 
is based on random partitions, we performed the cross-validation 
15  times  and  report  the  average.  The  precision,  recall,  and 
inspection rate of the models can be found in Table 6.  
is  performed  by 
Table 6: Predictability Results 
R* 
IR* 
33.2% 
4.9% 
P* 
9.9% 
13.3% 
50.7% 
10.0% 
Method 
Multivariate 
Discriminant 
Analysis 
Bayesian 
Networks 
* P=precision, R=recall, IR=inspection rate,  
Our  results  show  a  significantly  higher  recall  than  with  the 
individual  metrics  at  critical  points.  However,  the  precision  is 
lower to achieve this higher recall. One note of interest here is the 
low  inspection  rate.  If  a  team  wanted  to  inspect  files  using  the 
Bayesian  network  model,  then  they  would  only  need  to  inspect 
4.9% of the files and would find 33.3% of the vulnerable files. 
The  difference  in  modeling  methods  shows  that  Bayesian 
networks  tend  to  be  more  precise,  requiring  a  lower  inspection 
rate, but find a smaller percentage of the known vulnerable files 
than multivariate discriminant analysis. One can use this fact for 
deciding which modeling method to use if the goal is to predict 
vulnerable files based on our four developer activity metrics. 
Figure 5 shows the recall, inspection rate, and precision for all 15 
runs  of  ten-fold  cross-validation  Bayesian  network  models.  The 
figure  shows  minute  variations  from  trial  to  trial,  meaning  that 