−2n(1 − p)2
eε−1
eε+ 1−p
p
, p > 1
2 .
(cid:32)
(cid:32)
(cid:32)
(cid:18)
This concludes the proof, because we have found a bound for the
subset of possible values which did not satisfy our required ratio.
In the end we have
P (M (X) ∈ S) (cid:54) eεP (M (X
(cid:48)
) ∈ S) + δ,
which concludes the proof.
A.2 Proof of Theorem 2
Section 3. Let X = (cid:80)n
i=1 Xi and σ2 =
PROOF. To prove this theorem, we will use Facts 1 and 2 from
. Let u, v ∈
supp(X) and |u−v| (cid:54) ∆. For any Borel set B let us denote Bu =
{b+u : b ∈ B}. For simplicity let us, for now, assume that EXi =
0 for every i. From assumptions we also know that E|Xi|3 < ∞
for every i, so we can use Fact 2. Let Z ∼ N (0, nσ2). For every
Bu we have
(cid:80)n
i=1 σ2
i
n
P (X ∈ Bu) (cid:54) P (Z ∈ Bu) + 2δ1,
where δ1 (cid:54) 0.56(cid:80)n
((cid:80)n
i=1 E|Xi|3
i )
i=1 σ2
3
2
in Fact 2. Now we can use Fact 1:
is the rate of convergence described
P (Z ∈ Bu) + 2δ1 (cid:54) eεP (Z ∈ Bv) + 2δ1 + δ2.
dK
.
which concludes the proof.
A.3 Proof of Theorem 3
eεP (Z ∈ Bv) + 2δ1 + δ2 (cid:54)
(cid:54) eεP (X ∈ Bv) + 2δ1(1 + eε) + δ2.
During this reasoning we already obtained ε. We also have
(cid:1) 3
i=1 E|Xi|3
i=1 σ2
i
δ = 2δ1(1 + eε) + δ2 (cid:54) 1.12(cid:80)n
(cid:0)(cid:80)n
δ = 2δ1(1 + eε) + δ2 (cid:54) 1.12(cid:80)n
(cid:0)(cid:80)n
i=1 E|Xi − µi|3
i=1 σ2
i
(cid:1) 3
2
2
Note that for simplicity we assumed EXi = 0. One can easily see
that for Yi = (Xi − µi), where µi = EXi the proof is still correct.
Therefore we have
(1 + eε) +
√
4
5
n
√
4
n
.
5
(1 + eε) +
Finally we have
P (X ∈ Bu) (cid:54) eεP (X ∈ Bv) + δ1(1 + eε) + δ2 (cid:54)
(cid:54) eεP (X ∈ Bv) + δ,
X =(cid:80)n
PROOF. To prove this lemma, we use facts stated in Section 4,
namely Fact 3 and 4. We also use Kolmogorov and Wasserstein
distances, which were deﬁned in Section 4 in Deﬁnition 6. We have
i=1 Xi and σ2 = V ar(X). Let u, v ∈ supp(X) and |u −
v| (cid:54) ∆. For any Borel set B let us denote Bu = {b + u : b ∈ B}.
σ : b ∈ Bu}.
Moreover, throughout the proof we denote Bu
For simplicity let us, for now, assume that EXi = 0 for every i.
Let Z ∼ N (0, nσ2). For every Bu we have
σ = { b
(cid:19)
P (X ∈ Bu) = P
(cid:18) X
σ
.
∈ Bu
σ
i < ∞. Now let Z ∼
Recall that we assumed EXi = 0 and EX 4
N (0, 1). From Fact 4 we have
(cid:19)
(cid:18) X
σ
dW
, Z
n(cid:88)
i=1
(cid:54) D2
σ3
E|Xi|3 +
√
√
26
π
3
2
D
σ2
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
i=1
EX 4
i .
Note that for simplicity we assumed EXi = 0. One can easily
i = (Xi − µi), where µi = EXi the proof is still
see that for X∗
correct. We have
(cid:54) D2
σ3
E|X
i |3 +
∗
3
2
D
σ2
E (X∗
i )4.
(cid:19)
(cid:18) X
σ
dW
, Z
We can use Fact 3 to get Kolmogorov distance of X
σ and Z.
Namely
(cid:18) X
σ
(cid:54)
, Z
(cid:115)
(cid:18) 2
(cid:19) 1
4
π
dW
√
√
26
π
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
(cid:19)
(cid:18) X
i=1
, Z
.
σ
n(cid:88)
i=1
(cid:19)
559Having Kolmogorov distance of X
σ and Z, we can proceed further
(cid:19)
∈ Bu
σ
(cid:18) X
(cid:18)
σ
P
(cid:54) P
Z ∈ Bu
σ
(cid:54)
(cid:19)
+ 2dK
= P (Z · σ ∈ Bu) + 2dK
(cid:19)
(cid:19)
=
, Z
.
, Z
(cid:18) X
(cid:18) X
σ
σ
Now we can use the property of the normal distribution stated in
Fact 1.
P (Z · σ ∈ Bu) + 2δ1 (cid:54)
(cid:54) eεP (Z · σ ∈ Bv) + 2dK
(cid:19)
(cid:18) X
σ
, Z
+ δ1.
Both ε and δ1 are parameters from Fact 1, for the normal distribu-
tion with variance σ2 and |u − v| (cid:54) ∆. In particular, we can ﬁx
√
δ1 = 4
5
n . From Fact 1 we get
(cid:114)
ε =
∆2 ln(n)
σ2
.
Now we have to return to our initial distribution. Again, we use
Facts 3 and 4.
eεP (Z · σ ∈ Bv) + 2dK
(cid:18) X
σ
(cid:54) eεP
∈ Bv
σ
(cid:19)
(cid:19)
, Z
(cid:18) X
(cid:18) X
(cid:18) X
σ
σ
, Z
(cid:19)
+ δ1 (cid:54)
(cid:19)
+ 2dK
(1 + eε) + δ1 =
= eεP (X ∈ Bv) + 2dK
, Z
(1 + eε) + δ1.
σ
We already obtained ε. We also want to ﬁnd an upper bound for
δ = 2dK
ously shown inequalities concerning Kolmogorov and Wasserstein
distance
(cid:0) X
σ , Z(cid:1) (1 + eε)+δ2. For this purpose we can use previ-
(cid:19)
(cid:18) X
(cid:19) 1
(cid:18) 2
(cid:118)(cid:117)(cid:117)(cid:117)(cid:116) D2
n(cid:88)
(1 + eε) + δ1 (cid:54)
(cid:54) 2(1 + eε)
(cid:18) X
i |3 +
E (X∗
E|X∗
(cid:115)
δ = 2dK
i )4 +
(cid:19)
√
4
dW
, Z
, Z
(cid:54)
n
π
σ
σ
4
√
√
26
π
3
2
D
σ2
√
4
5
,
n
σ3
i=1
(cid:54) c(ε)
where
(before being processed) as random variables. In particular if we
assume that all data items are concentrated in single points (i.e,
P (Xi = xi) = 1 for all i ) we get the original (ε, δ)-differential
privacy.
While the standard differential privacy deﬁnition guarantees im-
munity against attacks based on auxiliary information (i.e., from
publicly available datasets or even personal knowledge about an in-
dividual participating in the protocol), the noiseless privacy is more
general as we can either assume that the adversary has no auxiliary
information, or assume that there is an upper bound on the size of
subset of database entries about which he has some external knowl-
edge. Note that if we assume full auxiliary information, this renders
noiseless privacy completely unacceptable, which is very intuitive,
as the whole notion of adversarial uncertainty demands that the ad-
versary does not have full knowledge. Moreover, it is often quite
too pessimistic to assume that the adversary knows everything ex-
cept for the single data record which privacy he wants to breach.
REMARK 1. See that in the standard differential privacy deﬁ-
nition (e.g. [16]) we essentially want
(cid:48)
(cid:48)
(cid:48)
= x
) + δ,
P (M (X) ∈ B|X = x) (cid:54) eεP (M (X
) ∈ B|X
where x and x(cid:48) are adjacent, deterministic vectors.
This captures the notion of neighboring databases. Our approach
is indeed a relaxation of that deﬁnition, as we do not necessar-
ily condition the data to have some ﬁxed, deterministic value. We
rather treat the data inputs as random variables. In particular, if
we have X = x with probability 1 then our model collapses to
standard differential privacy.
Differential privacy has some very useful properties. First of all, it
is immune to post-processing, so the adversary cannot get any ad-
ditional information, and consequently cannot increase the privacy
loss by convoluting the result of a mechanism with some determin-
istic function.
FACT 5. Noiseless privacy is, similarly to standard differential
privacy as stated in [16], resilient to post-processing. The proof
goes almost exactly the same way as for standard differential pri-
vacy. Let f : R → R(cid:48) be a deterministic function. Let also
T = {r ∈ R : f (r) ∈ S}. Now ﬁx S ⊂ R(cid:48), privacy mecha-
nism M and a random vector X. We have
P (f (M (X)) ∈ S) = P (M (X) ∈ T ) (cid:54)
(cid:54) eεP (M (X
) ∈ T ) + δ = eεP (f (M (X
(cid:48)
(cid:48)
)) ∈ S) + δ,
5
+
i=1
(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)
(cid:19) 1
(cid:18) 2
(cid:18) X
π
.
4
, Z
σ
c(ε) = 2(1 + eε)
Summing it up we obtain
P (X ∈ Bu) (cid:54) eεP (X ∈ Bv) + 2dK
(cid:54) eεP (X ∈ Bv) + δ,
which concludes the proof.
B. COMPARISON TO STANDARD
DIFFERENTIAL PRIVACY
Clearly noiseless privacy is an extension of the regular differen-
tial privacy from [15] that is applicable to the case when we can
assume that the observer/attacker may treat the raw data of users
which completes the proof of this remark.
Another important property of differential privacy is its compos-
ability. There has been an extended discussion concerning compos-
ability of noiseless privacy and its derivatives in [4, 5, 24].
(1 + eε) + δ1 (cid:54)
(cid:19)
560