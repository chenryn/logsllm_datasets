Cross-sanitization is a method that compares models of abnormal behavior, as normal models are closely tied to the specific traffic patterns of an individual site. In contrast, the consistent characteristics of abnormal packets across different sites can help filter out attacks that might otherwise saturate the training data. By leveraging this external knowledge, individual sites can cross-sanitize their training sets and develop more accurate local normal models.

For an attacker to successfully blind each sensor in such an environment, they would need to identify all collaborators and launch the same training attack on all participating sites simultaneously. This task requires significant resources and knowledge. Therefore, we hypothesize that when a particular site experiences a targeted training attack, the attack data will not appear at all collaborating sites at the same time. With a large enough group of collaborators, some fraction of sites will have detected the attack but not been corrupted by it. Sharing abnormal models in this case helps cleanse the local models of sites that have been compromised.

When a site with a sanitized model \( M_{\text{san}} \) receives abnormal models \( M_{\text{abn}1}, \ldots, M_{\text{abn}M} \) from its collaborators, it needs to compute a new model, \( M_{\text{cross}} \). The methods for computing this model are detailed in Sections 4.2 and 4.3.

### Polymorphic Attacks
Polymorphic attacks present a special challenge because each propagation attempt may display a distinct attack vector, which could be captured in different abnormal models. However, we conjecture that a polymorphic attack targeting a single site can still be effectively managed by the local sanitization scheme presented in this paper. Section 5 explores how well our approach copes with polymorphism.

### 4.2 Direct Model Differencing
Collaborative cross-sanitization requires a method for directly comparing and "differencing" anomaly detection (AD) models. The composition of these models may vary across sites depending on the specific AD algorithm and model representation. If models are directly comparable or a translation method exists, we can construct a new local sanitized model from the shared abnormal models as follows:
\[ M_{\text{cross}} = M_{\text{san}} - \bigcap_{i} \{ M_{\text{abn}i} \cap M_{\text{san}} \} \]
where \( M_{\text{abn}i} \cap M_{\text{san}} \) represents the features common to both models.

### 4.3 Indirect Model Differencing
For more complex models, such as probabilistic or statistical models, direct differencing is not feasible. Instead, we recompute the sanitized model using the information from \( M_{\text{san}} \) and \( M_{\text{abn}1}, \ldots, M_{\text{abn}M} \). The dataset used in the second phase of the local sanitization is tested against \( M_{\text{san}} \) to identify normal packets. These packets are then checked against each collaborative abnormal model. If a packet is labeled as normal by any abnormal model, it is considered abnormal, and its features are used to compute the new local abnormal model. Otherwise, the packet is used to compute the cross-sanitized model.

### 4.4 Additional Optimizations
Direct/indirect model differencing can help identify abnormal samples, but care must be taken during comparison due to content diversity among sites. An abnormal model from one site may include legitimate data from another, leading to false positives. To mitigate this, a shadow server can be used to resolve disagreements between models. If the sanitized and abnormal models disagree on a packet's label, the traffic is redirected to the shadow server to determine if the packet causes a real attack. Based on this, the packet is used to update either the local sanitized or abnormal model.

### 5 Performance of Collaborative Sanitization
This section demonstrates that even if local sanitization fails to detect an attack, external information from collaborating sites can compensate. The performance of the local architecture remains robust against polymorphic attacks. Experiments were conducted using the Anagram sensor on a PC with a 2GHz AMD Opteron processor 248 and 8GB of RAM, running Linux.

#### 5.1 Training Attacks
Assume some collaborative sites are poisoned by a long-lasting training attack, while others filter it and use it to build abnormal models. If the targeted site receives an abnormal model containing an attack vector, the local sanitized model can be cross-sanitized by removing common grams (direct model differencing). Due to content diversity, the same gram may be characterized differently by different sites, potentially reducing the size of the sanitized model and increasing the false positive rate.

Table 5 shows the average performance before and after cross-sanitization using direct and indirect model differencing. Table 6 illustrates the size of the sanitized model before and after cross-sanitization.

### Conclusion
While cross-sanitization improves detection rates, it can increase false positives due to site diversity. Potential adversarial attacks and trusted insider threats are also discussed, with suggestions for future work on Byzantine robustness and reputation systems. Indirect model differencing can further improve the method by testing against a second training dataset and adjusting the internal threshold of the Anagram sensor.