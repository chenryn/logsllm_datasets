those generated by other sites.
Cross-sanitization compares models of abnormality be-
cause normal models are tightly coupled with an individual
site’s trafﬁc. In contrast, the consistency of characteristics
of abnormal packets across sites can help ﬁlter out attacks
that saturate the training data. Individual sites can utilize
this external knowledge to cross-sanitize their training set
and generate a better local normal model.
For an attacker to successfully blind each sensor in this
type of environment, she would need to identify each col-
laborator and launch the same training attack on all partic-
ipating sites for the same time period. Accomplishing this
goal requires a signiﬁcant amount of resources and knowl-
edge. Therefore, we postulate that when a particular site ex-
periences a targeted training attack, the attack data will not
appear at all collaborating sites at the same time. As a re-
sult, with a large enough group of collaborators, some frac-
tion of sites will have seen the attack, but will not have had
their model corrupted by it. In this case, sharing abnormal
models helps cleanse the local models of sites in the group
that have been corrupted. When a site with sanitized model
Msan receives the abnormal models Mabn1 . . . MabnM from
its collaborators, it needs to compute a new model, Mcross.
The methods to compute this model are presented in Sec-
tions 4.2 and 4.3.
Polymorphic attacks present a special challenge because
each propagation attempt will display a distinct attack vec-
tor that may be captured in different abnormal models. We
conjecture, however, that a polymorphic attack targeting a
single site can still be captured by the local sanitization
scheme presented in this paper. Section 5 explores how well
our approach can cope with polymorphism.
4.2 Direct Model Diﬀerencing
Collaborative cross-sanitization requires us to deﬁne a
method of directly comparing and “differencing” AD mod-
els. However, the composition of models may vary across
sites depending on the particular AD algorithm in use and
the speciﬁc representation of the model. If models are di-
rectly comparable or a translation method exists (although
a full treatment of such a mechanism is beyond the scope
of this work, we consider how to deal with complex models
in Section 4.3), then we can construct a new local sanitized
model from the shared abnormal models as follows:
∩ Msan}
Mcross = Msan − (cid:3){Mabni
(3)
∩ Msan represents the features common to
where Mabni
both models.
4.3
Indirect Model Diﬀerencing
When models are more complex, e.g. probabilistic or
statistical models, the model differencing computation can-
not be applied analytically, but indirectly. Equation (3) is
expressed differently, not as model differencing, but as a
difference of sets of packets used to compute the models.
We recompute the sanitized model using the informa-
tion from Msan and Mabn1 . . . MabnM . The dataset used
90
)
%
(
e
t
a
r
e
v
i
t
i
s
o
p
e
s
a
F
l
)
%
(
e
t
a
r
n
o
i
t
c
e
t
e
D
99.31
100
50
0.30
0.16
0.13
0.11
0.09
0
1
10
20 
100
95
90
1
20 
40 
30 
70 
Number of micro−models
50 
60 
40 
60 
Number of micro−models
80 
90 
100
80 
100
Figure 7. Impact of the size of the training
dataset for www1
in the second phase of the local sanitization is tested against
Msan (we identify the packets that are normal, that is, used
for actually computing Msan). The packets labeled as nor-
mal (T EST (Pj, Msan) = 0) are also checked against each
of the collaborative abnormal models, Mabn1 . . . MabnM .
) = 0 means that the packet is
Note that T EST (Pj, Mabni
labeled normal by an abnormal model, which translates to
the fact the packet is abnormal. If at least one of the abnor-
mal models labels a packet Pj as normal (i.e., the packet is
considered abnormal by at least one collaborator), then fea-
tures are extracted from the packet and used for computing
the new local abnormal model, otherwise they are used for
computing the cross-sanitized model.
Table 4. Recalculating sanitized and abnor-
mal models. These routines use the abnormal models
of collaborating peers to regenerate models of both normal
and abnormal local data.
ROUTINE CROSSSANITIZED()
∀i ∈ [1..M]
if 0=TEST(Pj, Msan) and 1=TEST(Pj, Mabni )
Tcross ← Pj
Mcross ← AD(Tcross)
∃i ∈ [1..M]
ROUTINE CROSSABNORMAL()
s.t. 0=TEST(Pj, Msan) and 0=TEST(Pj, Mabni )
Tcabn ← Pj
Mcabn ← AD(Tcabn)
91
Figure 8. Impact of the anomaly detector’s
internal threshold for www1 when using
Anagram
4.4 Additional Optimizations
Although direct/indirect model differencing can help
identify abnormal samples that have poisoned a site, we
must take care during the comparison. Because sites exhibit
content diversity [28], (i.e., they do not experience identical
trafﬁc ﬂows), an abnormal model from site B may include
some common but ultimately legitimate data from site A.
In other words, data items that are indeed normal for a par-
ticular site can be considered abnormal by others. If site A
attempts to identify abnormal content in its local model us-
ing cross-sanitization with site B, then A may incorrectly
remove legitimate data patterns from its model along with
truly abnormal or malicious data patterns. Doing so in-
creases the false positive rate — an increase that may not
be matched by an increase in detection rate.
An alternative approach to reconciling different mod-
els or disagreements between models involves the use of
a shadow server. If the sanitized model and an abnormal
model disagree on the label of a packet (for example, the
sanitized model labels it normal and the abnormal one as
abnormal), we redirect the trafﬁc to the shadow server to
determine if the packet causes a real attack. Based on this
information the packet is used in the construction of either
the local sanitized model or the local abnormal model.
5 Performance of Collaborative Sanitization
This Section shows that even if local sanitization fails
to detect an attack, we can compensate by using the ex-
ternal information received from other collaborating sites.
Furthermore, we show that the performance of the local ar-
chitecture remains unaffected when faced with polymorphic
attacks. The experiments in this Section use the Anagram
sensor and were conducted on a PC with a 2GHz AMD
Opteron processor 248 and 8G of RAM, running Linux.
5.1 Training Attacks
We will assume that at least some of the collaborative
sites are poisoned by a long lasting training attack, while
others were able to ﬁlter it and use it for building the ab-
normal model.
If the targeted site receives an abnormal
model that contains an attack vector, the local sanitized
model can be “cross-sanitized” by removing the common
grams between the two models (direct model differencing).
Given the diversity in content exhibited by different sites,
the same gram can be characterized differently by different
sites. Therefore, it is possible that after cross-sanitation the
sanitized model becomes smaller. As an immediate conse-
quence, the false positive rate will increase.
We consider all the possible cases in which each of our
three hosts model is poisoned by each of the four attacks
present in our data. When one site is poisoned, we consider
that the other two are not. Every poisoned host receives the
abnormal models Mabn of its peers in order to cross-sanitize
its own model, Mpois. Table 5 presents the average per-
formance of the system before and after cross-sanitization
when using direct and indirect model differencing.
Table 5. Performance when the sanitized
model
is cross-
sanitized when using direct/indirect model
differencing
is poisoned and after it
Model
Mpois
Mcross
(direct)
Mcross
(indirect)
www1
www
lists
FP(%) DR(%) FP(%) DR(%) FP(%) DR(%)
0.10
47.53
51.78
44.94
0.27
0.25
0.24
100
0.71
100
0.48
100
0.10
100
0.26
100
0.10
100
In the case of direct model differencing, once the cross-
sanitization is done, the detection rate is improved, but the
false positive rate degrades. To further investigate how the
cross-sanitization inﬂuences the performance of the local
systems, we analyze the size of the models (presented in
Table 6).
As Table 6 shows, the size of the models has decreased.
This decrease leads to an increase in the FP rate. As we
mentioned before, this behavior is a disadvantage of our dis-
tributed sanitization method, as it depends on site diversity.
92
Table 6. Size of the sanitized model when poi-
soned and after cross-sanitization when us-
ing direct/indirect model differencing
Model
www1
www
lists
#grams ﬁle size #grams ﬁle size#gramsﬁle size
2,289,888 47M 199,011
114K
1,160,235 23M 1,270,009 24M 43,768 830K
Mabn
Mpois
Mcross 1,095,458 21M 1,225,829 24M 37,113
(direct)
Mcross 1,160,004 23M 1,269,808 24M 43,589
(indirect)
701K
828K
3.9M 6,025
Furthermore, this phenomena provides a potential avenue
of attack for an adversarial collaborator. We consider de-
fending against this type of attack to be out of the scope
of our current efforts, but Byzantine robustness or reputa-
tion systems can be applied in the future. Of course, even
appropriately authenticated or otherwise trustworthy peers
could be exploited after they are included in the collabora-
tive network. We note that dealing with trusted insiders is
an inherently hard problem faced by many large-scale col-
laborative systems, from anonymity networks to mandatory
access control frameworks.
In order to improve our method for cross-sanitization,
we can use the indirect model differencing approach. This
approach tests the poisoned local model and the collabo-
rative abnormal models against the second training dataset
used in our local methodology. The goal of this method is
to eliminate the packets responsible for poisoning the local
model from the training data set. The most challenging part
of this method is to set the internal threshold of Anagram
when testing the trafﬁc against the abnormal models. An
intuitive approach is to actually use the inverse value of the
normal thresholding. For example, if the internal threshold
for Anagram when testing against the normal model was τ ,
then the threshold for abnormal models would be 1-τ . In
our experiments, we used an internal threshold of 0.4: the
threshold for abnormal models becomes 0.6 (analyzing the
scores given by the packets that contributed to the poison-