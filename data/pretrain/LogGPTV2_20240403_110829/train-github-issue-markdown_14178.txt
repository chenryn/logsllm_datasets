### Reproducing code example:
    import numpy as np
    integer_list = [18446744073709551615]
    arr = np.array(integer_list)
    arr.dtype
    # Returns: dtype('uint64')
    # So far, so good... But:
    # Same list as above, with with a smaller number at the first index
    integer_list = [
        9223372036854775807,
        18446744073709551615
    ]
    arr = np.array(integer_list)
    arr.dtype
    # Returns: dtype('float64')
It looks like Numpy only looks at the first element in the object when
deciding what dtype the array should have. I think (although I'm not sure)
that this behaviour causes unpredictable behaviour in Pandas down the line.
    import pandas as pd
    integer_list = [
        9223372036854775807,
        18446744073709551615
    ]
    df = pd.DataFrame(integer_list)
    df[0].dtype
    # Returns: dtype('uint64')
    df[0].to_numpy(int)
    # Returns (on Windows*): array([-1, -1])
    # Returns (on Debian): array([9223372036854775807, -1])
When using `to_numpy()`, Pandas calls `np.asarray()` under the hood and passes
the values of the Series. So I don't think anything is wrong on the Pandas
side. Numpy seems to get confused and makes some wrong decisions about the
appropriate length and signedness of the integer it should use.
### NumPy/Python version information:
**Windows**  
Numpy version: 1.19.2  
Python version: 3.8.8 (default, Feb 24 2021, 15:54:32) [MSC v.1928 64 bit
(AMD64)]
**Debian**  
Numpy version: 1.20.2 3.7.3  
Python version: Python 3.7.3 (default, Jan 22 2021, 20:04:44) [GCC 8.3.0]
And, to maybe save someone some time:
On Windows, a c_long object is always 32 bit, regardless of the OS being 64
bit (as opposed to Unix). See: https://docs.microsoft.com/en-
us/windows/win32/winprog/windows-data-types