title:"Adversarial Examples" for Proof-of-Learning
author:Rui Zhang and
Jian Liu and
Yuan Ding and
Zhibo Wang and
Qingbiao Wu and
Kui Ren
6
9
5
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
“Adversarial Examples” for Proof-of-Learning
Rui Zhang†, Jian Liu†∗, Yuan Ding, Zhibo Wang, Qingbiao Wu, and Kui Ren
Email: {zhangrui98, liujian2411, dy1ant, zhibowang, qbwu, kuiren }@zju.edu.cn
Zhejiang University
Abstract—In S&P ’21, Jia et al. proposed a new con-
cept/mechanism named proof-of-learning (PoL), which allows a
prover to demonstrate ownership of a machine learning model
by proving integrity of the training procedure. It guarantees
that an adversary cannot construct a valid proof with less cost
(in both computation and storage) than that made by the prover
in generating the proof.
A PoL proof includes a set of intermediate models recorded
during training, together with the corresponding data points
used to obtain each recorded model. Jia et al. claimed that an
adversary merely knowing the final model and training dataset
cannot efficiently find a set of intermediate models with correct
data points.
In this paper, however, we show that PoL is vulnerable
to “adversarial examples”! Specifically,
in a similar way as
optimizing an adversarial example, we could make an arbitrarily-
chosen data point “generate” a given model, hence efficiently
generating intermediate models with correct data points. We
demonstrate, both theoretically and empirically, that we are
able to generate a valid proof with significantly less cost than
generating a proof by the prover.
I. INTRODUCTION
Recently, Jia et al. [12] propose a concept/mechanism
named proof-of-learning (PoL), which allows a prover T to
prove that it has performed a specific set of computations
to train a machine learning model; and a verifier V can
verify correctness of the proof with significantly less cost
than training the model. This mechanism can be applied in
at least two settings. First, when the intellectual property of
a model owner is infringed upon (e.g., by a model stealing
attack [18], [24], [25]), it allows the owner to claim ownership
of the model and resolve the dispute. Second, in the setting of
federated learning [17], where a model owner distributes the
training process across multiple workers, it allows the model
owner to verify the integrity of the computation performed
by these workers. This could prevent Byzantine workers from
conducting denial-of-service attacks [4].
PoL mechanism. In their proposed mechanism [12], T pro-
vides a PoL proof that includes: (i) the training dataset, (ii)
the intermediate model weights at periodic intervals during
training W0, Wk, W2k, ..., WT , and (iii) the corresponding
indices of the data points used to train each intermediate
model. With a PoL proof, one can replicate the path all the way
from the initial model weights W0 to the final model weights
WT to be fully confident that T has indeed performed the
computation required to obtain the final model.
†Rui Zhang and Jian Liu are co-first authors.
∗Jian Liu is the corresponding author.
During verification, V first verifies the provenance of the
initial model weights W0: whether it is sampled from the
required initialization distribution; and then recomputes a
subset of the intermediate models to confirm the validity of the
sequence provided. However, V may not be able to reproduce
the same sequence due to the noise arising from the hardware
and low-level libraries. To this end, they allow a distance
between the recomputed model and its corresponding model
in PoL. Namely, for any Wt, V performs a series of k updates
to arrive at W ′
t+k, which is compared to the purported Wt+k.
They tolerate:
d(Wt+k, W ′
t+k) ≤ δ,
where d represents a distance that could be l1, l2, l∞ or cos,
and δ is the verification threshold that should be calibrated
before verification starts.
Jia et al. [12] claimed in their paper that an adversary A
can never construct a valid a PoL with less cost (in both
computation and storage) than that made by T in generating
the proof (a.k.a. spoof a PoL). However, they did not provide
a proof to back their claim.
Our contribution. By leveraging the idea of generating ad-
versarial examples, we successfully spoof a PoL!
In the PoL threat model, Jia et al. [12] assumed that “A
has full access to the training dataset, and can modify it”.
Thanks to this assumption, we can slightly modify a data point
so that it can update a model and make the result pass the
verification. In more detail, given the training dataset and the
final model weights WT , A randomly samples all intermediate
model weights in a PoL: W0, Wk, W2k... (only W0 needs to be
sampled from the given distribution). For any two neighboring
model weights (Wt−k, Wt), A picks batches of data points
(X, y) from D, and keeps manipulating X until:
d(update(Wt−k, (X, y)), Wt) ≤ δ.
The mechanism for generating adversarial examples ensures
that the noise added to X is minimized.
We further optimize our attack by sampling W0, Wk, W2k...
in a way such that:
d(Wt, Wt−k) < δ, ∀ 0 < t < T and t mod k = 0.
With this condition, it becomes much easier for the “adversar-
ial” X to converge, hence making our attack more efficient.
We empirically evaluate our attacks in both reproducibility
and spoof cost. We reproduced the results in [12] as base-
lines for our evaluations. Our experimental results show that,
© 2022, Rui Zhang. Under license to IEEE.
DOI 10.1109/SP46214.2022.00113
1408
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
in most cases of our setting, our attacks introduce smaller
reproduction errors and less cost than the baselines.
Organization. In the remainder of this paper, we first provide
a brief introduction to PoL in Section II. Then, we formally
describe our attack in Section III and extensively evaluate it in
Section IV. In Section V, we provide some countermeasures.
Section VI compares our attacks to closely related work.
Notations. We introduce new notations as needed. A summary
of frequent notations appears in Table I.
Notation
T
V
A
D
fW
W
E
S
T
T ′
Q
N
k
d()
δ
γ
ζ
η
ε
X
y
R
Table I
SUMMARY OF NOTATIONS
Description
prover
verifier
attacker
dataset
machine learning model
model weights
number of epochs
number of steps per epoch
number of steps in P(T , fWT )
T = E · S
number of steps in P(A, fWT )
number of models verified per epoch
number of steps in generating an “ad-
versarial example”
number of batches in a checkpointing
interval
distance that could be l1, l2, l∞ or cos
verification threshold
γ ≪ δ
distribution for W0
learning rate
reproduction error
batch of data points
batch of labels
batch of noise
II. PROOF-OF-LEARNING
should be able to ascertain if the PoL is valid or not. A PoL
proof is formally defined as follows:
Definition 1. A PoL proof generated by a prover T is defined
as P(T , fWT ) = (W, I, H, A), where (a) W is a set of
intermediate model weights recorded during training, (b) I is
a set of information about the specific data points used to train
each intermediate model, (c) H is a set of signatures generated
from these data points, and (d) A incorporates auxiliary in-
formation training the model such as hyperparameters, model
architecture, optimizer and loss choices1.
An adversary A might wish to spoof P(T , fWT ) by spend-
ing less computation and storage than that made by T in
generating the proof. By spoofing, A can claim that it has
performed the computation required to train fWT . A PoL
mechanism should guarantee:
• The cost of verifying the PoL proof by V should be
smaller than the cost (in both computation and storage)
of generating the proof by T .
• The cost of any spoofing strategy attempted by any A
should be larger than the cost of generating the proof.
B. Threat Model
In [12], any of the following cases is considered to be a
successful spoof by A:
1) Retraining-based spoofing: A produced a PoL for fWT
that is exactly the same as the one produced by T , i.e.,
P(A, fWT ) = P(T , fWT ).
2) Stochastic spoofing: A produced a valid PoL for fWT ,
from the one produced by T
3) Structurally Correct Spoofing: A produced an invalid PoL
4) Distillation-based Spoofing: A produced a valid PoL for
an approximated model, which has the same run-time
performance as fWT .
but
i.e.,P(A, fWT ) ̸= P(T , fWT ).
for fWT but it can pass the verification.
is different
it
weights, loss function and other hyperparameters.
The following adversarial capabilities are assumed in [12]:
1) A has full knowledge of the model architecture, model
2) A has full access to the training dataset D and can modify
3) A has no knowledge of T ’s strategies about batching,
it. This assumption is essential to our attacks.
parameter initialization, random generation and so on.
In this section, we provide a brief introduction to proof-of-
learning (PoL). We refer to [12] for more details
C. PoL Creation
A. PoL definition
PoL allows a prover T to demonstrate ownership of a
machine learning model by proving the integrity of the training
procedure. Namely, during training, T accumulates some
secret information associated with training, which is used to
construct the PoL proof P(T , fWT ). When the integrity of the
computation (or model ownership) is under debate, an honest
and trusted verifier V validates P(T , fWT ) by querying T
for a subset (or all of) the secret information, under which V
The PoL creation process is shown in Algorithm 1, which is
taken from [12] and slightly simplified by us. T first initializes
the weights W0 according to an initialization strategy init(ζ)
(line 2), where ζ is the distribution to draw the weights from. If
the initial model is obtained from elsewhere, a PoL is required
for the initial model itself as well. We omit this detail in our
paper for simplicity.
1For simplicity, we omit A in this paper and denote a PoL proof as
P(T , fWT ) = (W, I, H).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:13:51 UTC from IEEE Xplore.  Restrictions apply. 
1409
I ← getBatches(D, S)
for s = 0 → S − 1 do
Algorithm 1: PoL Creation (taken from [12])
Input: D, k, E, S, ζ
Output: PoL proof: P(T , fWT ) = (W, I, H)
1 W ← {} I ← {} H ← {}
2 W0 ← init(ζ))
3 for e = 0 → E − 1 do
4
5
6
7
8
9
t := e · S + s
Wt+1 ← update(Wt, D[I[s]])
I.append(I[s])
H.append(h(D[I[s]]))
computing the signature
if t mod k = 0 then
W.append(Wt)
W.append(nil)
else
10
11
12
13
14
15
16 end
end
end
initialize W0
h() is for
For each epoch, T gets S batches of data points from the
dataset D via getBatches(D, S) (Line 4), the output of
which is a list of S sets of data indices. In each step s of
the epoch e, the model weights are updated with a batch