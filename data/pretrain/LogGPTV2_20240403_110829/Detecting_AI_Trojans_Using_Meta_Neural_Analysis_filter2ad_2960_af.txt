MNTD-robust we will: 1) sample a random meta-classiﬁer by
setting its parameters values to be random numbers sampled
from the normal distribution; 2) use our training set of shadow
models to tune the queries only, while keeping the random
meta-classiﬁer unchanged; 3) we will use the tuned inputs
along with the random meta-classiﬁer to analyze a model and
classify it as benign or Trojaned.
To guarantee that the attacker does not know the random
parameters of the meta-classiﬁer, the defender can re-sample
them for each detection task. This would increase the detection
cost since the defender needs to retrain the meta-classiﬁer
whenever the parameters are sampled. But as discussed in
Section VI-D, the expensive part of MNTD is to train the
shadow models, which needs only be done once; training the
meta-classiﬁer is comparatively fast. Additionally, a random
meta-classiﬁer could be reused for verifying an entire batch
of models to be classiﬁed as Trojaned or benign; as long as the
adversary does not know the random parameters, the defense
remains robust.
From the attacker’s side, he still wants to generate Trojaned
models which can evade the detection with full knowledge of
MNTD-robust. However, this time he does not know the ran-
dom parameters of the meta-classiﬁer and the corresponding
query set. Hence, we assume that the attacker will ﬁrst sample
the random parameters of the meta-classiﬁer on his own and
tune the queries. Then he can apply the same technique as
in Section VIII-A to generate the Trojaned model which can
evade his own meta-classiﬁer well. We will evaluate if this
Trojaned model can evade the defender’s meta-classiﬁer.
C. Evaluation Results
We evaluate MNTD-robust over all the Trojaned tasks as
in Section VI-B and show the results in Table IX. The ﬁrst
row is the detection performance of MNTD-robust on normal
Trojaned models (without adaptive attack) and the second row
is its detection performance against adaptive attack.
From the ﬁrst row, we see that the detection performance of
our robust MNTD does not downgrade much in normal sce-
nario where there is no adaptive attack (MNTD-robust versus
MNTD (Jumbo) in Table III). From the second row, we see
that the robust version of MNTD works much better against
adaptive attacks. In some cases the detection performance even
increases, which means that the intentional evasion on the
attacker-chosen meta-classiﬁer actually makes the Trojaned
model easier to detect. By comparison, if we do not add the
precautions with randomness, the simple MNTD system will
be bypassed by all these strong adaptive attacks.
IX. DISCUSSION & LIMITATIONS
In this paper, we focus
Trojan Attack Detection Levels.
on the model-level Trojan attack detection. Other works may
investigate in input-level detection [21], [16] or dataset-level
detection [12], [52]. These are all feasible ways to prevent
users from AI Trojans. However, we consider model-level
detection the most generally applicable approach. The reason
is that dataset-level detection can only detect the Trojans that
perform poisoning attack to the dataset. They cannot work
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
114
against attacks that directly modify model parameters. The
input-level detection requires the defender to perform detection
each time the input is fed to the model. This will decrease the
efﬁciency when deploying the model. As a comparison, a user
only needs to perform model-level Trojan detection one time.
As long as no Trojan is detected in the model, the user can
deploy it without any cost in the future.
In this paper, we focus on
Detection vs. Defense/Mitigation.
detecting Trojan attacks. Defense/mitigation and detection on
Trojan attacks are two very related but orthogonal directions.
Existing defense or mitigation approaches perform Trojan
removal based on the assumption that the given models are
already Trojaned. However, this is problematic in practice as,
in most cases, DNN models provided by the model producers
are benign. It is unreasonable to perform Trojan removal on
benign models which requires extensive computation and time
overhead. Moreover, as shown in [53], blindly performing
mitigation operations can result in substantial degradation in
the model’s prediction accuracy for benign inputs. Therefore,
Trojan detection should be considered as a prerequisite before
conducting Trojan mitigation. Once a model
is identiﬁed
as Trojaned model,
the mitigation can be executed more
conﬁdently to avoid a waste of computation and time.
In the paper, we mainly
Meta-classiﬁer on other ML models
detect AI Trojans on neural networks. We do not include
other ML models in our discussion mainly because there is
no current research showing that they suffer from backdoor
attacks. We emphasize that our technique can be applied to
any differentiable ML models that contains a numerical logit
vector in its calculation.
X. RELATED WORK
Several recent research [39], [23], [38],
Trojan Attacks.
[15], [36], [28] has studied software Trojan attacks on neural
networks. As discussed in Section II-C, Trojans can be created
through poisoning the training dataset or direct manipulation
of model parameters. For example, Gu et al. [23] study
backdoor poisoning attacks in an outsourced training scenario
where the adversary has full knowledge of the model and train-
ing data. Comparably, Chen et al. [15] also use data poisoning
but assume the adversary has no knowledge of the training data
and model. On the other hand, [39] directly manipulates the
neural network parameters to create a backdoor, while [38]
considers Trojaning a publicly available model using training
data generated via reverse engineering. Bagdasaryan et al. [4]
demonstrated that any participant in federated learning can
introduce hidden backdoor functionality into the joint global
model. Besides software Trojans, Clements et al. [17] devel-
oped a framework for inserting malicious hardware Trojans in
the implementation of a neural network classiﬁer. Li et al. [35]
proposed a hardware-software collaborative attack framework
to inject hidden neural network Trojans.
Several Trojan attack detection
Trojan Attack Detection.
approaches have been proposed [53], [21], [16], [12], [40].
These approaches can be categorized into input-level detec-
tion [21], [16], [40], model-level detection [53] and dataset-
level detection [12]. We discussed the differences of these
detection levels in Section IX.
To the best of our knowl-
Trojan Attack Defense/Mitigation.
edge,
there are few evaluated defenses against Trojan at-
tacks [37], [52]. Fine-Pruning [37] removes potential Trojans
by pruning redundant neurons less useful for normal classi-
ﬁcation. However, the model accuracy degrades substantially
after pruning [53]. The defense in [52] extracts feature repre-
sentations of input samples from the later layers of the model
and utilizes a robust statistics tool to detect the malicious
instances as outliers from each label class. As discussed
in Section IX, Trojan attack detection and defense are two
orthogonal directions. One can ﬁrst use our approach to detect
if a model is Trojaned, then use any of the defenses to remove
or mitigate the Trojans.
Poisoning attacks for machine learning
Poisoning Attacks.
models has been well studied in the literature [6], [34], [44],
[56]. As discussed in Section II-C, several Trojan attacks
create Trojans through injecting poisoning samples. Those
attacks can thus be seen as variants of poisoning attacks.
However, most conventional poisoning attacks seek to degrade
a model’s classiﬁcation accuracy on clean inputs [5], [46]. In
contrast, the objective of Trojan attacks is to embed backdoors
while not degrading the model’s prediction accuracy on clean
inputs.
Property inference attacks [3], [20], [43]
Property Inference.
aim to infer certain properties about the training dataset or the
model of a target model. However, as illustrated in Section IV,
detecting Trojaned model using property inference is not a
trivial
task. We thus propose jumbo learning to construct
Trojaned shadow models. Besides, existing work considers
white-box access to the target model while we consider black-
box access. The work of [43] focuses on inference against
collaborative learning, which has a different setting as ours.
XI. CONCLUSION
In this paper, we presented MNTD, a novel framework to
detect Trojans in neural networks using meta neural analysis
techniques. We propose jumbo learning to generate shadow
models without the knowledge of the attacker’s approach. In
addition, we provide a comprehensive comparison between
existing Trojan detection approaches and ours. We show that
MNTD outperforms all the existing detection works in most
cases and generalizes well to unforeseen attack strategies. We
also design and evaluate a robust version of MNTD against
strong adaptive attackers. Our work sheds new light on the
detection of Trojans in neural networks.
ACKNOWLEDGEMENTS
This material is based upon work supported by the Depart-
ment of Energy under Award Number DE-OE0000780. The
views and opinions of authors expressed herein do not neces-
sarily state or reﬂect those of the United States Government
or any agency thereof.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
115
REFERENCES
[1] Amazon. Machine learning at aws, 2018.
[2] The Irish Social Science Data Archive. Cer smart metering project,
2019.
[3] Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Vil-
lani, Domenico Vitali, and Giovanni Felici. Hacking smart machines
with smarter ones: How to extract meaningful data from machine
learning classiﬁers.
International Journal of Security and Networks,
10(3):137–150, 2015.
[4] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and
Vitaly Shmatikov. How to backdoor federated learning. arXiv preprint
arXiv:1807.00459, 2018.
[5] Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, and Jaehoon Amir
Safavi. Mitigating poisoning attacks on machine learning models: A data
provenance based approach. In Proceedings of the 10th ACM Workshop
on Artiﬁcial Intelligence and Security, pages 103–110. ACM, 2017.
[6] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks
against support vector machines. arXiv preprint arXiv:1206.6389, 2012.
[7] BigML Inc. Bigml, 2018.
[8] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard
Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Mon-
fort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving
cars. arXiv preprint arXiv:1604.07316, 2016.
[9] Caffe. Caffe model zoo, 2018.
[10] Nicholas Carlini and David Wagner. Towards evaluating the robustness
of neural networks. In 2017 IEEE Symposium on Security and Privacy
(SP), pages 39–57. IEEE, 2017.
[11] Raghavendra Chalapathy, Aditya Krishna Menon, and Sanjay Chawla.
Anomaly detection using one-class neural networks. arXiv preprint
arXiv:1802.06360, 2018.
[12] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig,
Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
Detecting backdoor attacks on deep neural networks by activation
clustering. arXiv preprint arXiv:1811.03728, 2018.
[13] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepin-
spect: a black-box trojan detection and mitigation framework for deep
neural networks. In Proceedings of the 28th International Joint Confer-
ence on Artiﬁcial Intelligence, pages 4658–4664. AAAI Press, 2019.
[14] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
Zoo: Zeroth order optimization based black-box attacks to deep neural
networks without training substitute models. In Proceedings of the 10th
ACM Workshop on Artiﬁcial Intelligence and Security, pages 15–26.
ACM, 2017.
[15] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Tar-
geted backdoor attacks on deep learning systems using data poisoning.
arXiv preprint arXiv:1712.05526, 2017.
[16] Edward Chou, Florian Tram`er, Giancarlo Pellegrino, and Dan Boneh.
Sentinet: Detecting physical attacks against deep learning systems. arXiv
preprint arXiv:1812.00292, 2018.
[17] Joseph Clements and Yingjie Lao. Hardware trojan attacks on neural
networks. arXiv preprint arXiv:1806.05768, 2018.
[18] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet:
A Large-Scale Hierarchical Image Database. In CVPR09, 2009.
[19] Eric Florenzano. Gadientzoo, 2016.
[20] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov.
Property inference attacks on fully connected neural networks using
permutation invariant representations. In Proceedings of the 2018 ACM
SIGSAC Conference on Computer and Communications Security, pages
619–633. ACM, 2018.
[21] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C Ranas-
inghe, and Surya Nepal. Strip: A defence against trojan attacks on deep
neural networks. arXiv preprint arXiv:1902.06531, 2019.
[22] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech
recognition with deep recurrent neural networks. In 2013 IEEE inter-
national conference on acoustics, speech and signal processing, pages
6645–6649. IEEE, 2013.
[23] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets:
Identifying vulnerabilities in the machine learning model supply chain.
arXiv preprint arXiv:1708.06733, 2017.
[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition.
In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 770–778,
2016.
[25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
Weinberger. Densely connected convolutional networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition,
pages 4700–4708, 2017.
[26] Wenyi Huang and Jack W Stokes. Mtnet: a multi-task neural network
for dynamic malware classiﬁcation.
In International Conference on
Detection of Intrusions and Malware, and Vulnerability Assessment,
pages 399–418. Springer, 2016.
[27] Jonathan J. Hull. A database for handwritten text recognition re-
search. IEEE Transactions on pattern analysis and machine intelligence,
16(5):550–554, 1994.
[28] Yujie Ji, Xinyang Zhang, and Ting Wang. Backdoor attacks against
In 2017 IEEE Conference on Communications and
learning systems.
Network Security (CNS), pages 1–9. IEEE, 2017.
[29] Yoon Kim. Convolutional neural networks for sentence classiﬁcation.
arXiv preprint arXiv:1408.5882, 2014.
[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.
[31] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of
features from tiny images. Technical report, Citeseer, 2009.
[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
classiﬁcation with deep convolutional neural networks.
in neural information processing systems, pages 1097–1105, 2012.
Imagenet
In Advances
[33] Yann LeCun, Corinna Cortes, and Christopher J Burges. The MNIST
database of handwritten digits. http://yann.lecun.com/exdb/mnist/, 2018.
[34] Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data
poisoning attacks on factorization-based collaborative ﬁltering.
In
Advances in neural information processing systems, pages 1885–1893,
2016.
[35] Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei,
Yu Wang, and Huazhong Yang. Hu-fu: Hardware and software collabora-
tive attack framework against neural networks. In 2018 IEEE Computer
Society Annual Symposium on VLSI (ISVLSI), pages 482–487. IEEE,
2018.
[36] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David
Miller. Backdoor embedding in convolutional neural network models
via invisible perturbation. arXiv preprint arXiv:1808.10307, 2018.
[37] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning:
Defending against backdooring attacks on deep neural networks. In In-
ternational Symposium on Research in Attacks, Intrusions, and Defenses,
pages 273–294. Springer, 2018.
[38] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai,
Trojaning attack on neural
Weihang Wang, and Xiangyu Zhang.
networks.
In 25nd Annual Network and Distributed System Security
Symposium, NDSS 2018, San Diego, California, USA, February 18-221,
2018. The Internet Society, 2018.
[39] Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017
IEEE International Conference on Computer Design (ICCD), pages 45–
48. IEEE, 2017.
[40] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu
Zhang. Nic: Detecting adversarial samples with neural network invariant
checking.
In 26th Annual Network and Distributed System Security
Symposium, NDSS, pages 24–27, 2019.
[41] Larry M Manevitz and Malik Yousef. One-class svms for document
classiﬁcation. Journal of machine Learning research, 2(Dec):139–154,
2001.
[42] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher.
In Advances in
Learned in translation: Contextualized word vectors.
Neural Information Processing Systems, pages 6294–6305, 2017.
[43] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly
arXiv
Inference attacks against collaborative learning.
Shmatikov.
preprint arXiv:1805.04049, 2018.
[44] Luis Mu˜noz-Gonz´alez, Battista Biggio, Ambra Demontis, Andrea Pau-
dice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards
poisoning of deep learning algorithms with back-gradient optimization.
In Proceedings of the 10th ACM Workshop on Artiﬁcial Intelligence and
Security, pages 27–38. ACM, 2017.
[45] Seong Joon Oh, Max Augustin, Bernt Schiele, and Mario Fritz. Towards
International Confer-
reverse-engineering black-box neural networks.
ence on Learning Representations, 2018.
[46] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Well-
man. Towards the science of security and privacy in machine learning.