### MNTD-robust: Methodology and Evaluation

**Methodology:**

1. **Random Meta-classifier Initialization:**
   - We initialize a random meta-classifier by setting its parameters to values sampled from a normal distribution.
   
2. **Query Tuning:**
   - Using our training set of shadow models, we tune the queries while keeping the random meta-classifier unchanged.

3. **Model Analysis:**
   - The tuned inputs, along with the random meta-classifier, are used to analyze a model and classify it as either benign or Trojaned.

**Ensuring Robustness:**

To ensure that the attacker does not know the random parameters of the meta-classifier, the defender can re-sample these parameters for each detection task. This increases the detection cost since the meta-classifier needs to be retrained whenever the parameters are resampled. However, as discussed in Section VI-D, the most computationally expensive part of MNTD is training the shadow models, which only needs to be done once. Training the meta-classifier is relatively fast. Additionally, a random meta-classifier can be reused for verifying an entire batch of models, as long as the adversary does not know the random parameters, ensuring the defense remains robust.

**Attacker's Perspective:**

From the attacker’s perspective, the goal is to generate Trojaned models that can evade detection, even with full knowledge of MNTD-robust. However, the attacker does not know the random parameters of the meta-classifier and the corresponding query set. Therefore, we assume the attacker will first sample the random parameters of the meta-classifier and tune the queries. The attacker can then use the same technique as in Section VIII-A to generate a Trojaned model that evades their own meta-classifier. We will evaluate whether this Trojaned model can also evade the defender’s meta-classifier.

### Evaluation Results

We evaluate MNTD-robust over all the Trojaned tasks as described in Section VI-B, and the results are shown in Table IX. 

- **First Row:**
  - This row shows the detection performance of MNTD-robust on normal Trojaned models (without adaptive attack). The performance does not degrade significantly compared to MNTD (Jumbo) in Table III.

- **Second Row:**
  - This row shows the detection performance against adaptive attacks. The robust version of MNTD performs much better in this scenario. In some cases, the detection performance even improves, indicating that the intentional evasion on the attacker-chosen meta-classifier makes the Trojaned model easier to detect. Without the randomness precautions, the simple MNTD system would be bypassed by strong adaptive attacks.

### Discussion & Limitations

**Focus on Model-Level Detection:**

In this paper, we focus on model-level Trojan attack detection. Other works may investigate input-level [21, 16] or dataset-level [12, 52] detection. These methods are feasible but have limitations. Dataset-level detection can only identify Trojans that poison the dataset and cannot detect direct modifications to model parameters. Input-level detection requires repeated checks, reducing efficiency. Model-level detection, on the other hand, is more efficient as it only needs to be performed once.

**Detection vs. Defense/Mitigation:**

Our work focuses on detecting Trojan attacks, which is a prerequisite for effective mitigation. Existing mitigation approaches, such as Fine-Pruning [37], can remove Trojans but often result in substantial accuracy degradation. By first detecting if a model is Trojaned, one can apply mitigation more confidently and avoid unnecessary computational overhead.

**Applicability to Other ML Models:**

While our work primarily focuses on neural networks, the techniques can be applied to any differentiable ML model that includes a numerical logit vector in its calculations.

### Related Work

**Trojan Attacks:**

Recent research has explored various types of Trojan attacks on neural networks, including backdoor poisoning [23, 15], direct parameter manipulation [39], and reverse engineering [38]. Federated learning [4] and hardware Trojans [17, 35] are also areas of concern.

**Trojan Attack Detection:**

Several detection approaches have been proposed, categorized into input-level [21, 16, 40], model-level [53], and dataset-level [12] detection. Our approach outperforms existing methods in most cases and generalizes well to unforeseen attack strategies.

**Poisoning Attacks:**

Poisoning attacks, which aim to degrade model performance on clean inputs, are well-studied. Trojan attacks, however, aim to embed backdoors without degrading performance on clean inputs.

**Property Inference:**

Property inference attacks [3, 20, 43] aim to infer properties about the training dataset or model. However, detecting Trojaned models using property inference is non-trivial, leading us to propose jumbo learning to construct Trojaned shadow models.

### Conclusion

In this paper, we presented MNTD, a novel framework for detecting Trojans in neural networks using meta-neural analysis. We introduced jumbo learning to generate shadow models without knowledge of the attacker's approach. Our comprehensive comparison with existing methods shows that MNTD outperforms them in most cases and generalizes well to new attack strategies. We also designed and evaluated a robust version of MNTD against strong adaptive attackers, providing new insights into the detection of Trojans in neural networks.

### Acknowledgements

This work was supported by the Department of Energy under Award Number DE-OE0000780. The views and opinions expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.

### References

[References are listed as provided in the original text.]

---

This revised version aims to enhance clarity, coherence, and professionalism, making the content more accessible and structured.