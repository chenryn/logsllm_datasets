---
tags: ['人工智能', '机器学习', '数据技术']
---
# 监督学习
利用样本和期望输出来学习如何预测
- 回归问题：输出的结果是一个连续的值
- 分类问题：输出的结果是离散有限集合
  - 纯度：数据集中的样本全部属于同一类别，即数据集完全“纯”的程度
  - 熵：集合的混乱程度越高，熵越高，熵为0时，代表集合绝对有序，熵的计算： $H(p_{1})=-p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$，$p_1$代表某一类在集合中的概率
## 线性回归
线性模型具有较强的可解释性
简单线性回归输出的因变量只与单个的输入自变量存在线性关系，而多元线性回归因变量由多个自变量共同决定
单变量线性回归模型：
$$
f(\mathbf{x})=w x + b = 模型(输入特征) = 输出结果
$$
多变量线性回归模型，使用向量表示多个特征及多个参数：
$$
f(\mathbf{\vec{x}})=\vec{w}\cdot \vec{x} + b = \sum_{i=0}^{n}w_i\cdot x_i + b
$$
x 为自变量，w跟b被称为模型的参数，为了找出这两个参数，需要定义一个平方误差代价函数：
$$
J(w,b) = \frac{1}{2m}\sum_{i=1}^m(f(x^{(i)}) - y^{(i)})^2
$$
代价函数就是衡量模型预测值与训练集实际值之间的偏差，找出合适的 w 和 b，使得这个偏差最小，即 $\underset{w,b}{\text{minimize}} J(w,b)$
![w b 不同取值对应的平面](/assets/20231021210901.png)
```py
# 使用sickit-learn 预测日志数据增长量
from sklearn import linear_model
# 训练数据
# 距离第一天的天数, 是否是周末，数据总量
train_data = [
[1	,0, 54],
[2	,0, 108],
  ...
[22	,0, 80968],
]
# 测试数据
test_data = [
  [76,	1,	148550],
  ...
  [42,	1,	109554],
]
reg = linear_model.LinearRegression()
train_feats = list(map(lambda x: x[0: len(x) - 1],train_data))
train_val = list(map(lambda x: x[-1],train_data))
reg.fit(train_feats, train_val)
print('模型参数 ' + str(reg.coef_))
```
## 多项式回归
使用线性回归的思路，关键在于为数据添加新的特征，而这些新的特征是原有的特征的多项式组合
## 逻辑回归
为了对分类问题 $f(\vec{x}) = \vec{w}\cdot\vec{x} + b$ 进行拟合，引入sigmoid函数 $g(z) = \frac{1}{1+e^{-z}}$ 如果把分类问题的参数作为sigmoid函数的参数，就能得到一个输出0 - 1 函数
$$
f(\vec{x}) = \frac{1}{1+e^{-(\vec{w}\cdot\vec{x} + b)}}
$$
当$\vec{w}\cdot\vec{x} + b = 0$时，这条线就是决策边界
![线性决策边界](/assets/20231022154106.png)
![非线性决策边界](/assets/20231022154322.png)
通过找出决策边界，大于这个决策边界的被认为真，否则认为假
逻辑回归使用如下代价函数
$$
J(\vec{w},b) = \frac{1}{m}\sum_{i=1}^m L(f(\vec{x}^{(i)},y^{(i)}))
$$
$$
L=\begin{cases}\quad-\log\left(f(\vec{x}^{(i)})\right)&\quad\text{if }y^{(i)}=1\\-\log\left(1-f(\vec{x}^{(i)})\right)&\quad\text{if }y^{(i)}=0&\end{cases}
$$
损失函数如果预期结果为0 但实际结果为1 则会输出1 否则输出0，也就是充分体现实际结果与预期结果的不同，使得代价函数最小
代价函数都是通过最大似然方法统计数据得出
## Softmax回归
Softmax回归是逻辑回归的概括
$$
\begin{array}{rl}{a_{1}=\frac{e^{z_{1}}}{e^{z_{1}}+e^{z_{2}}+\cdots+e^{z_{N}}}}&{{}=P(y=1|\vec{x})}\\{\vdots}\\{a_{N}=\frac{e^{z_{N}}}{e^{z_{1}}+e^{z_{2}}+\cdots+e^{z_{N}}}}&{{}=P(y=N|\vec{x})}\\\end{array}
$$
$$
loss(a_1,...,a_N,y)=\begin{cases}-\log a_1&\mathrm{if}y=1\\-\log a_2&\mathrm{if}y=2\\\vdots\\-\log a_N&\mathrm{if}y=N\end{cases}
$$
硬输出是对数据的分类边界进行建模。实现硬输出的函数，也就是将输入数据映射为输出类别的函数叫作判别函数（discriminant）
而软输出是根据回归值和似然性的关系输出样本属于某个类别的概率
$$
y(\mathbf{x})=g^{-1}(\mathbf{w}^T\mathbf{x}+b)
$$
g 是联系函数，$g^{-1}$ 是激活函数
## 多元自适应回归样条MARS
为了解决线性回归欠拟合的问题，MARS方法将一个非线性函数的定义域划分成很多足够小的区域的组合，那在每个小区域上函数都会近似地满足线性性质。将这个过程调转方向，用很多条直线组合成一条曲线，即不同范围的自变量使用不同的线性回归，再将这些线性回归函数拟合成一条曲线
## 线性判别分析
LDA 的主要目标是将数据投影到一个低维空间，同时最大化不同类别之间的类内方差，最小化同一类别内部的方差
## 广义线性模型
线性模型的扩展，允许因变量的分布不必满足正态分布的假设
## 广义可加模型
将标准线性回归中的每个自变量以及可能存在的自变量之间的交互项都替换成一个非线性的平滑函数
$$
y_i=\beta_0+f_1(x_{i1})+f_2(x_{i2})+\cdots+f_p(x_{ip})
$$
## 支持向量机SVM
常见的一种分类方法，在机器学习中，SVM 是有监督的学习模型
SVM 就是帮我们找到一个超平面，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化，SVM 就是求解最大分类间隔的过程