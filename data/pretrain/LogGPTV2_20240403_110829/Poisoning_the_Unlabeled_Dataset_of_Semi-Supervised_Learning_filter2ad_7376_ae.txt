dataset sizes, a trivial trial-and-error sampling identiﬁes the
poisoned examples in under ten minutes on a GPU. While
effective for this particular attack, it can not, for example,
detect our GAN latent space attack.
We can improve on this to detect arbitrary interpolations.
Agglomerative Clustering [64] creates clusters of similar ex-
amples (under a pixel-space (cid:96)2 norm, for example). Initially
every example is placed into its own set. Then, after comput-
ing the pairwise distance between all sets, the two sets with
minimal distance are merged to form one set. The process
repeats until the smallest distance exceeds some threshold.
Because our poisoned examples are all similar in pixel-
space to each other, it is likely that they will be all placed in
the same cluster. Indeed, running a standard implementation
of agglomerative clustering [45] is effective at identifying
the poisoned examples in our attacks. Thus, by removing the
largest cluster, we can completely prevent this attack.
The inherent limitation of this defenses is that it assume
that the defender can create a useful distance function. Using
(cid:96)2 distance above worked because our attack performed pixel-
space blending. However, if the adversary inserted examples
that applied color-jitter, or small translations, this defense
would no longer able to identify the cluster of poisoned ex-
amples. This is a cat-and-mouse game we want to avoid.
5.3 Monitoring Training Dynamics
Unlike the prior defenses that inspect the dataset directly to
detect if an example is poisoned or not, we now develop a
second strategy that predicts if an example is poisoned by
how it impacts the training process.
Recall the reason our attack succeeds: semi-supervised
learning slowly grows the correct decision boundary out from
the initial labeled examples towards the “nearest neighbors”
in the unlabeled examples, and continuing outwards. The
guessed label of each unlabeled example will therefore be
inﬂuenced by (several) other unlabeled examples. For benign
USENIX Association
30th USENIX Security Symposium    1587
examples in the unlabeled set, we should expect that they will
be inﬂuenced by many other unlabeled examples simultane-
ously, of roughly equal importance. However, by construction,
our poisoned examples are designed to predominantly impact
the prediction of the other poisoned examples—and not be
affected by, or affect, the other unlabeled examples.
We now construct a defense that relies on this observation.
By monitoring the training dynamics of the semi-supervised
learning algorithm, we isolate out (and then remove) those
examples that are inﬂuenced by only a few other examples.
Computing pairwise inﬂuence What does it mean for one
example to inﬂuence the training of another example? In
the context of fully-supervised training, there are rigorous
deﬁnitions of inﬂuence functions [28] that allow for one to
(somewhat) efﬁciently compute the training examples that
maximally impacted a given prediction. However, our inﬂu-
ence question has an important difference: we ask not what
training points inﬂuenced a given test point, but what (unla-
beled) training points inﬂuenced another (unlabeled) training
point. We further ﬁnd that it is not necessary to resort to
such sophisticated approaches, and a simpler (and 10− 100×
faster) method is able to effectively recover inﬂuence.
While we can not completely isolate out correlation and
causation without modifying the training process, we make
the following observation that is fundamental to this defense:
If example u inﬂuences example v, then
when the prediction of u changes, the pre-
diction of v should change in a similar way.
After every epoch of training, we record the model’s pre-
dictions on each unlabeled example. Let fθi(u j) denote the
model’s prediction vector after epoch i on the jth unlabeled
example. For example, in a binary decision task, if at epoch 6
the model assigns example u5 class 0 with probability .7 and
class 1 with .3, then fθ6(u5) =(cid:2).7
.3(cid:3). From here, deﬁne
∂ fθi(u j) = fθi+1(u j)− fθi(u j)
with subtraction taken component-wise. That is, ∂ f represents
the difference in the models predictions on a particular exam-
ple from one epoch to the next. This allows us to formally
capture the intuition for “the prediction changing”.
Then, for each example, we let
fθb(u j)(cid:3)
j =(cid:2)∂ fθa(u j)
µ(a,b)
∂ fθa+1(u j)
. . .
∂ fθb−1(u j)
be the model’s collection of prediction deltas from epoch a to
epoch b. We compute the inﬂuence of example ui on u j as
Inﬂuence(ui,u j) = (cid:107)µ(0,K−2)
i
− µ(1,K−1)
j
(cid:107)2
2.
That is, example ui inﬂuences example u j if when exam-
ple ui makes a particular change at epoch k, then example
u j makes a similar change in the next epoch—because it has
Figure 7: Our training-dynamics defense perfectly separates
the inserted poisoned examples from the benign unlabeled
examples on CIFAR-10 for a FixMatch poisoned model. Plot-
ted is the frequency of the inﬂuence value across the unla-
beled examples. Benign unlabeled examples are not heavily
inﬂuenced by their nearest neighbors (indicated by the high
values), but poisoned examples are highly dependent on the
other poisoned examples (indicated by the low values).
been inﬂuenced by ui. This deﬁnition of inﬂuence is clearly
a simplistic approximation, and is unable to capture sophisti-
cated relationships between examples. We nevertheless ﬁnd
that this deﬁnition of inﬂuence is useful.
Identifying poisoned examples For each example in the
unlabeled training set, we compute the average inﬂuence of
the 5 nearest neighbors
avg inﬂuence(u) =
Inﬂuence(u,v)· 1[close5(u,v)]
1
5 ∑
v∈U
where close5(u,v) is true if v is one of the 5 closest (by in-
ﬂuence) neighbors to u. (Our result is not sensitive to the
arbitrary choice of 5; values from 2 to 10 perform similarly.)
Results. This technique perfectly separates the clean and
poisoned examples for each task we consider. In Figure 7
we plot a representative histogram of inﬂuence values for
benign and poisoned examples; here we train a FixMatch
model poisoning 0.2% of the CIFAR-10 dataset and 40 la-
beled examples. The natural data is well-clustered with an
average inﬂuence of 2· 10−2, and the injected poisoned exam-
ples all have an inﬂuence lower than 2· 10−4, with a mean of
3· 10−5. Appendix B shows 8 more plots for additional runs
of FixMatch and MixMatch on CIFAR-10, and SVHN.
When the attack itself fails to poison the target class, it is
still possible to identify all of the poisoned examples that have
been inserted (i.e., with a true positive rate of 100%), but the
1588    30th USENIX Security Symposium
USENIX Association
10−510−410−310−210−1100Mean Influence of 5 Nearest Neighbors100101102103Frequency (log scaled)Benign ExamplesPoisoned Examples6 Conclusion
Within the past years, semi-supervised learning has gone from
“completely unusable” [61] to nearly as accurate as the fully-
supervised baselines despite using 100× less labeled data.
However, using semi supervised learning in practice will re-
quire understanding what new vulnerabilities will arise as a
result of training on this under-speciﬁed problem.
In this paper we study the ability for an adversary to poison
semi-supervised learning algorithms by injecting unlabeled
data. As a result of our attacks, production systems will not
be able to just take all available unlabeled data, feed it into
a classiﬁer, and hope for the best. If this is done, an adver-
sary will be able to cause speciﬁc, targeted misclassiﬁcations.
Training semi-supervised learning algorithms on real-world
data will require defenses tailored to preventing poisoning
attacks whenever collecting data from untrusted sources.
Surprisingly, we ﬁnd that more accurate semi-supervised
learning methods are more vulnerable to poisoning attacks.
Our attack never succeeds on MeanTeacher because it has a
50% error rate on CIFAR-10; in contrast, FixMatch reaches a
5% error rate and as a result is easily poisoned. This suggests
that simply waiting for more accurate methods not only won’t
solve the problem, but may even make the problem worse as
future methods become more accurate.
Defending against poisoning attacks also can not be
achieved through extensive use of human review—doing so
would reduce or eliminate the only reason to apply semi-
supervised learning in the ﬁrst place. Instead, we study de-
fenses that isolate a small fraction of examples that should be
reviewed or removed. Our defenses are effective at preventing
the poisoning attack we present, and we believe it will provide
a strong baseline by which future work can be evaluated.
More broadly, we believe that or analysis highlights that
the recent trend where machine learning systems are trained
on any and all available data, without regard to its quality or
origin, might introduce new vulnerabilities. Similar trends
have recently been observed in other domains; for example,
neural language models trained on unlabeled data scraped
from the Internet can be poisoned to perform targeted mis-
predictions [49]. We expect that other forms of unlabeled
training, such as self -supervised learning, will be similarly
vulnerable to these types of attacks. We hope our analysis
will allow future work to perform additional study of this
phenomenon in other settings where uncurated data is used to
train machine learning models.
Acknowledgements
We are grateful to Andreas Terzis, David Berthelot, and the
anonymous reviewers for the discussion, suggestions, and
feedback that signiﬁcantly improved this paper.
Figure 8: Our defenses’s near false positives are dupli-
cated images. The left-most column contains ﬁve images
from the CIFAR-10 unlabeled dataset that our defense iden-
tiﬁes as near false positives. At right are the four next-most-
similar images from the CIFAR-10 unlabeled set as computed
by our average inﬂuence deﬁnition. All of these similar im-
ages are visual (near-)duplicates of the ﬁrst.
false positive rate increases slightly to 0.1%. In practice, all
this means the defender should collect a few percent more
unlabeled examples more than are required so that any mali-
cious examples can be removed. Even if extra training data is
not collected, training on 99.9% of the unlabeled dataset with
the false positives removed does not statistically signiﬁcantly
reduce clean model accuracy.
Thus, at cost of doubling the training time—training once
with poisoned examples, and a second time after removing
them—it is possible to completely prevent our attack. Multi-
ple rounds of this procedure might improve its success rate
further if not all examples can be removed in one iteration.
Examining (near) false positives. Even the near false pos-
itives of our defense are insightful to analyze. In Figure 8 we
show ﬁve (representative) images of the benign examples in
the CIFAR-10 training dataset that our defense almost rejects
as if they were poisoned examples.
Because these examples are all nearly identical, they heav-
ily inﬂuence each other according to our deﬁnition. When one
examples prediction changes, the other examples predictions
are likely to change as well. This also explains why removing
near-false-positives does not reduce model accuracy.
Counter-attacks to these defenses. No defense is full-
proof, and this defense is no different. It is likely that future
attacks will defeat this defense, too. However, we believe that
defenses of this style are a promising direction that (a) serve
as a strong baseline for defending against unlabeled dataset
poisoning attacks, and (b) could be extended in future work.
USENIX Association
30th USENIX Security Symposium    1589
References
[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Tal-
war, and L. Zhang, “Deep learning with differential privacy,” in Pro-
ceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, 2016, pp. 308–318.
[2] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar, “Can
machine learning be secure?” in Proceedings of the 2006 ACM Sympo-
sium on Information, computer and communications security, 2006.
[3] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A.
Raffel, “Mixmatch: A holistic approach to semi-supervised learning,”
in Advances in Neural Information Processing Systems, 2019.
[4] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndi´c, P. Laskov,
G. Giacinto, and F. Roli, “Evasion attacks against machine learning
at test time,” in Joint European conference on machine learning and
knowledge discovery in databases. Springer, 2013, pp. 387–402.
[5] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support
vector machines,” 2012.
[6] B. Biggio, I. Pillai, S. Rota Bulò, D. Ariu, M. Pelillo, and F. Roli, “Is
data clustering in adversarial settings secure?” in Proceedings of the
2013 ACM workshop on Artiﬁcial intelligence and security, 2013.
[7] B. Biggio, K. Rieck, D. Ariu, C. Wressnegger, I. Corona, G. Giacinto,
and F. Roli, “Poisoning behavioral malware clustering,” in Proceedings
of the 2014 workshop on artiﬁcial intelligent and security workshop,
2014, pp. 27–36.
[8] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal component
analysis?” Journal of the ACM (JACM), vol. 58, no. 3, pp. 1–37, 2011.
[9] N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, “The secret
sharer: Evaluating and testing unintended memorization in neural net-
works,” in 28th USENIX Security Symposium (USENIX Security 19),
2019, pp. 267–284.
[10] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor at-
tacks on deep learning systems using data poisoning,” arXiv preprint
arXiv:1712.05526, 2017.
[11] Y. Chen, C. Caramanis, and S. Mannor, “Robust sparse regression
under adversarial corruption,” in International Conference on Machine
Learning, 2013.
[12] A. Coates, A. Ng, and H. Lee, “An analysis of single-layer networks
in unsupervised feature learning,” in Proceedings of the fourteenth
international conference on artiﬁcial intelligence and statistics, 2011,
pp. 215–223.
[13] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Autoaug-
ment: Learning augmentation strategies from data,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
2019.
[14] T. DeVries and G. W. Taylor, “Improved regularization of convolutional
neural networks with cutout,” arXiv preprint arXiv:1708.04552, 2017.
[15] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau,
and S. Thrun, “Dermatologist-level classiﬁcation of skin cancer with
deep neural networks,” Nature, vol. 542, no. 7639, pp. 115–118, 2017.
[16] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel,
M. Bethge, and F. A. Wichmann, “Shortcut learning in deep neural