ing the URL 13 times. We then measured the time to ﬁrst
byte of 13 additional queries to the server for the URL. Time
to ﬁrst byte is appropriate here because we are interested
in the overhead added by ScriptGard to server process-
ing. Finally, we took the 75th percentile result, following
web performance analysis practice used by colleagues at our
institution responsible for a large public facing web site. Un-
fortunately, we found overheads of over 175.6x for the full
analysis, for example a URL that took 0.53 seconds to load
without instrumentation took 92.73 seconds to load with all
checks.
PPP: Fortunately, we can shift much of this overhead to a
pre-release phase by using preferential path proﬁling (PPP)
techniques. As we described above, the optimizations in
PPP include a smart way of numbering code blocks and a
way to avoid updating global state when certain “preferred”
paths are taken.
We used a wide-area network simulator running on the
server to emulate conditions of a typical IEEE 802.11b link
between the client and the server, to produce realistic net-
work conditions for evaluating the total overhead. We then
instrumented our application using preferential path proﬁl-
ing and performed training on a set of 19 URLs. To test the
worst-case performance from PPP, we then chose 19 URLs
completely diﬀerent from those that were used during train-
ing. We then visited each URL 20 times and measured the
time to ﬁrst byte.
Figure 12 graphs the time to ﬁrst byte for each of our test
URLs, in milliseconds, after removing the ﬁrst visit. To ac-
count for outliers, the time measurement represents the 75th
percentile of the measurements over a series of 19 samples.
Our graph shows error bars representing one standard devi-
ation in each direction.
Figure 13 shows the median for each URL visited and the
overhead for each of the 19 URLs. The average overhead for
using preferential path proﬁling is actually negative, which
we attibute to the overhead being too small to be statisti-
cally signiﬁcant. We report overhead both for direct connec-
tion to the switch and for when the network simulator is in
use conﬁgured to simulate an 802.11 connection. Our results
show that while the runtime overhead of the full Script-
Gard analysis is prohibitive, the deployment overhead can
be decreased greatly by shifting analysis to a training phase.
(a) Overhead over a switch.
(b) Overhead over an 80211b connection.
Figure 12: Overhead of PPP on 19 URLs. Time is in milliseconds.
Top of bar is the 75th percentile of 20 samples.
7. RELATED WORK
Many defense techniques for scripting attacks have been
proposed, which we discuss next. ScriptGard targets a
new class of context-inconsistency errors.
Defense Techniques: Browser-based mitigations, such as
Noncespaces, XSS Auditor, or DSI make changes to the web
browsers that make it diﬃcult for an adversary’s script to
run [3, 11, 25]. These approaches can be fast, but they re-
quire all users to upgrade their web browsers.
A server side mitigation, in contrast, focuses on changing
the web application logic. Blueprint describes mechanisms
to ensure the safe construction of the intended HTML parse
tree on the client using JavaScript in a browser-agnostic
way [20]. However, issues in applying Blueprint’s mecha-
nisms to various context still exist and developers may place
them wrongly. Our work addresses this separate aspect of
the problem, in a way that does not require any manual
eﬀort from developers.
XSS-GUARD [4] proposes techniques to learn allowed
scripts from unintended scripts. The allowed scripts are
then white-listed. Like ScriptGard,
it employs a web
browser for its analysis, but the two approaches are funda-
mentally diﬀerent. ScriptGard’s defense is based on auto-
matic server-side sanitizer placement, rather than browser-
based white-listing of scripting in server output. XSS-
GUARD’s techniques are intended for applications that al-
low rich HTML, where designing correct sanitizers becomes
challenging. ScriptGard target applications with fairly re-
05010015020025030035012345678910111213141516171819No instrumentationPPP0500100015002000250030003500400012345678910111213141516171819No instrumentationPPP612Switch
None PPP Overhead None
246.0 241.3
139.3 139.3
217.3 215.5
142.5 141.0
209.5 202.5
207.5 206.3
250.5 238.0
207.5 205.5
139.3 136.3
221.8 209.0
258.0 260.3
206.0 206.3
152.3 156.5
215.8 224.0
179.5 180.3
130.0 119.8
177.0 177.3
267.0 271.3
176.3 181.0
-1.93% 2,355.5 2,358.2
0.00% 2,163.2 2,163.2
-0.81% 2,146.5 2,121.2
-1.05% 2,731.8 2,719.0
-3.34% 1,548.7 1,574.4
-0.60% 1,413.4 1,416.4
-4.99% 2,451.7 2,476.5
-0.96% 2,442.2 2,455.7
-2.15% 1,742.2 1,739.9
-5.75% 2,376.2 2,370.7
0.87% 2,373.0 2,373.5
0.12% 2,571.8 2,527.5
2.79% 1,588.9 1,505.9
3.82% 1,266.9 1,261.4
0.42% 2,533.5 2,530.8
-7.88% 2,493.5 2,489.2
0.14% 2,408.0 2,350.5
1.59% 1,416.1 1,413.1
2.70% 1,534.9 1,538.4
80211b
PPP Overhead
0.12%
0.00%
-1.18%
-0.47%
1.66%
0.21%
1.01%
0.55%
-0.13%
-0.23%
0.02%
-1.72%
-5.22%
-0.43%
-0.11%
-0.17%
-2.39%
-0.21%
0.23%
-0.13%
-0.45%
Average
Median
-0.60%
-0.90%
Average
Median
Figure 13: PPP overhead statistics . Time is in milliseconds, 75th
percentile reported from 20 samples.
strictive policies that have already addressed the sanitizer-
correctness issue.
a
context-sensitive
is
Google AutoEscape
auto-
sanitization mechanism for Google’s GWT and CTem-
plates templating frameworks. As explained earlier, the
templating languages it auto-sanitizes are much simpler
and AutoEscape does not need to handle if-else or loop
constructs which create path-senstivity a major issue.
ScriptGard’s target is towards large-scale legacy appli-
cations, where sanitization placement logic is complex and
already deployed, so rewriting all such logic is not practical.
SecuriFly translates bug speciﬁcations written in a spe-
cial program query to runtime instrumentation that detects
these bugs [21]. Our approach reasons both about the
browser context as well as the server state, allowing us to
tackle sanitizer placement problems not detected by Securi-
fly.
Software security analysis of web applications: Soft-
ware security focuses on using program analysis to ﬁnd se-
curity critical bugs in applications. The WebSSARI project
pioneered these approaches for web applications, and sev-
eral static analysis tools have been propose [16, 35]. Run-
time approaches, like ours, has the advantage of demon-
strating clear, reproducible test cases over static analysis
tools. Multiple runtime analysis systems for information
ﬂow tracking have been proposed, including Haldar et al.
for Java [12] and Pietraszek et al. [28] and Nguyen-Tuong et
al. for PHP [26]. Typically systems use negative tainting to
speciﬁcally identify untrusted data in web applications ap-
plications [18, 19, 21, 36]. While negative taint is preferable
for ﬁnding bugs, it is less desirable for mitigations because
it requires specifying all sources of taint. Our design distin-
guishes itself from most previous work in that it tracks pos-
itive taint, which is conservative default fail-close approach,
and side-steps identifying all sources of taint. The main
exception is WASP [13], which does use positive taint, but
which concerned SQL injection attacks, which do not exhibit
the path sensitivity, use of multiple sanitizers, and need for
a browser model to determine if data is a potential cross
site scripting attack. WASP was also evaluated on much
smaller applications (maximum 20,000 lines of code) than
considered in this work.
Sanitizer correctness: Balzarotti et al. show that custom
sanitizer routines are often incorrectly implemented [2].
Livshits et al. developed methods for determining which
functions in a program play the role of sanitizer. Their
Merlin system is also capable of detecting missing sanitiz-
ers [19]. ScriptGard’s analysis is complementary to these
works. Sanitizers may be present, and they may be func-
tionally correct for contexts they are intended to be used in.
Incorrect placement, however, can introduce errors.
The Cross-Site Scripting Cheat Sheet shows over two hun-
dred examples of strings that exercise common corner cases
of web sanitizers [29]. The Bek project proposes a sys-
tematic domain-speciﬁc languages for writing and checking
sanitizers [15, 33].
8. CONCLUSIONS
We analyzed a set of 53 large web pages in a large-scale
web application with over 400,000 lines of code. Each page
contained 350–900 DOM nodes. We found 285 multiple-
encoding issues, as well as 1,207 instances of inconsistent
sanitizers, establishing the prevalence of our two new prob-
lem classes and the eﬀectiveness of ScriptGard as a testing
aid. With preferential path proﬁling, when used for mitiga-
tion, ScriptGard incurs virtually no statistically signiﬁ-
cant overhead on cached paths.
9. ACKNOWLEDGMENTS
We would like to give special thanks to Patrice Godefroid,
who has been most instrumental in helping us deﬁne the
problems. Kapil Vaswani made the ppp tool work with our
application. We also thank Kapil Vaswani, David Wagner,
Herman Venter, Joel Weinberger, Peli de Halleux, Nikolai
Tillmann, Devdatta Akhawe, Adrian Mettler, Dawn Song,
our anonymous reviewers, and other Microsoft colleagues.
The ﬁrst author performed this work while interning at Mi-
crosoft Research.
10. REFERENCES
[1] M. Balduzzi, C. Gimenez, D. Balzarotti, and E. Kirda.
Automated discovery of parameter pollution vulnerabilities
in web applications. In Proceedings of the Network and
Distributed System Security Symposium, 2011.
[2] D. Balzarotti, M. Cova, V. Felmetsger, N. Jovanovic,
E. Kirda, C. Kruegel, and G. Vigna. Saner: Composing
Static and Dynamic Analysis to Validate Sanitization in
Web Applications. In Proceedings of the IEEE Symposium
on Security and Privacy, Oakland, CA, May 2008.
[3] D. Bates, A. Barth, and C. Jackson. Regular expressions
considered harmful in client-side XSS ﬁlters. International
World Wide Web Conference, 2010.
[4] P. Bisht and V. N. Venkatakrishnan. XSS-GUARD: precise
dynamic prevention of cross-site scripting attacks. In
Detection of Intrusions and Malware, and Vulnerability
Assessment, 2008.
[5] H. Bojinov, E. Bursztein, and D. Boneh. XCS: Cross
channel scripting and its impact on web applications. In
CCS, 2009.
613[6] S. Chen, D. Ross, and Y.-M. Wang. An analysis of browser
domain-isolation bugs and a light-weight defense
mechanism. In Proceedings of the Conference on Computer
and Communications Security, October 2007.
[7] T. Chilimbi, B. Liblit, K. Mehra, A. V. Nori, , and
K. Vaswani. Holmes: Eﬀective statistical debugging via
eﬃcient path proﬁling. In Proceedings of the International
Conference on Software Engineering, May 2009.
[8] D. Connolly and C. M. Sperberg-McQueen. Web addresses
in HTML 5.
http://www.w3.org/html/wg/href/draft#ref-RFC3986,
2009.
[9] Fortify, Inc. Fortify SCA.
http://www.fortifysoftware.com/products/sca/, 2006.
[10] Google. Google auto escape. http://google-ctemplate.
googlecode.com/svn/trunk/doc/auto_escape.html, 2011.
[11] M. V. Gundy and H. Chen. Noncespaces: using
randomization to enforce information ﬂow tracking and
thwart cross-site scripting attacks. 2009.
[12] V. Haldar, D. Chandra, and M. Franz. Dynamic taint
propagation for Java. In Proceedings of the Annual
Computer Security Applications Conference, Dec. 2005.
[22] Microsoft Corporation. Microsoft code analysis tool .NET,
2009. http://www.microsoft.com/downloads/en/details.
aspx?FamilyId=
0178e2ef-9da8-445e-9348-c93f24cc9f9d&displaylang=
en,.
[23] Microsoft Corporation. String class (system), 2010. http://
msdn.microsoft.com/en-us/library/system.string.aspx.
[24] Microsoft Corporation. StringBuilder class, 2010.
http://msdn.microsoft.com/en-us/library/system.text.
stringbuilder(v=VS.80).aspx.
[25] Y. Nadji, P. Saxena, and D. Song. Document structure
integrity: A robust basis for cross-site scripting defense. In
Proceedings of the Network and Distributed System
Security Symposium, 2009.
[26] A. Nguyen-Tuong, S. Guarnieri, D. Greene, J. Shirley, and
D. Evans. Automatically hardening Web applications using
precise tainting. In Proceedings of the IFIP International
Information Security Conference, 2005.
[27] Open Web Application Security Project. The ten most
critical Web application security vulnerabilities.
http://umn.dl.sourceforge.net/sourceforge/owasp/
OWASPTopTen2004.pdf, 2004.
[13] W. G. Halfond, A. Orso, and P. Manolios. WASP:
[28] T. Pietraszek and C. V. Berghe. Defending against
Protecting web applications using positive tainting and
syntax-aware evaluation. IEEE Transactions on Software
Engineering, 34(1), 2008.
[14] Y. HASEGAWA. UTF-7 XSS cheat sheet. http:
//openmya.hacker.jp/hasegawa/security/utf7cs.html,
2005.
[15] P. Hooimeijer, B. Livhsits, D. Molnar, P. Saxena, and
M. Veanes. Fast and precise sanitizer analysis with bek. In
Proceedings of the Usenix Security Symposium, Aug. 2011.
[16] N. Jovanovic, C. Kruegel, and E. Kirda. Pixy: A static
analysis tool for detecting Web application vulnerabilities
(short paper). In Proceedings of the IEEE Symposium on
Security and Privacy, 2006.
injection attacks through context-sensitive string
evaluation. In Proceedings of the Recent Advances in
Intrusion Detection, Sept. 2005.
[29] RSnake. XSS cheat sheet for ﬁlter evasion.
http://ha.ckers.org/xss.html.
[30] B. Schmidt. google-analytics-xss-vulnerability, 2011.
http://spareclockcycles.org/2011/02/03/
google-analytics-xss-vulnerability/.
[31] E. J. Schwartz, T. Avgerinos, and D. Brumley. All you ever
wanted to know about dynamic taint analysis and forward
symbolic execution (but might have been afraid to ask). In
Proceedings of the IEEE Symposium on Security and
Privacy, 2010.
[17] A. Kie˙zun, V. Ganesh, P. J. Guo, P. Hooimeijer, and M. D.
[32] Ter Louw, Mike and V.N. Venkatakrishnan. BluePrint:
Ernst. HAMPI: A solver for string constraints. In
International Symposium on Software Testing and
Analysis, 2009.
[18] B. Livshits and M. S. Lam. Finding security errors in Java
programs with static analysis. In Proceedings of the Usenix
Security Symposium, 2005.
[19] B. Livshits, A. V. Nori, S. K. Rajamani, and A. Banerjee.
Merlin: Speciﬁcation inference for explicit information ﬂow
problems. In Proceedings of the Conference on
Programming Language Design and Implementation, June
2009.
[20] M. T. Louw and V. N. Venkatakrishnan. Blueprint: Robust
prevention of cross-site scripting attacks for existing
browsers. In Proceedings of the IEEE Symposium on
Security and Privacy, 2009.
[21] M. Martin, B. Livshits, and M. S. Lam. SecuriFly: runtime
vulnerability protection for Web applications. Technical
report, Stanford University, 2006.
Robust Prevention of Cross-site Scripting Attacks for
Existing Browsers. In Proceedings of the IEEE Symposium
on Security and Privacy, 2009.
[33] M. Veanes, B. Livshits, and D. Molnar. Decision procedures
for composition and equivalence of symbolic ﬁnite state
transducers. Technical Report MSR-TR-2011-32, Microsoft
Research, 2011.
[34] H. Venter. Common compiler infrastructure: Metadata
API, 2010. http://ccimetadata.codeplex.com/.
[35] Y. Xie and A. Aiken. Static detection of security
vulnerabilities in scripting languages. In Proceedings of the
Usenix Security Symposium, 2006.
[36] W. Xu, S. Bhatkar, and R. Sekar. Taint-enhanced policy
enforcement: A practical approach to defeat a wide range
of attacks. In Proceedings of the Usenix Security
Symposium, 2006.
614