data elements s1 ← Seal(d1) and s2 ← Seal(d2), the ad-
versary performs Terminate() and Start() to erase the
runtime memory of the enclave. When the enclave re-
quests for the latest sealed data d2, the adversary per-
forms OfferSeal(i,s1) and the enclave accepts d1 as d2.
When the sealed data captures the state of the enclave at
the time of sealing, we say that the rollback attack reverts
the enclave back to its previous state.
Another approach is a forking attack, where the
adversary leverages two concurrently running enclave
instances. The adversary starts two instances i1 ←
Start(e) and i2 ← Start(e) of the same enclave e. The
OS receives a request from a remote client to write data
me to enclave e. The OS writes the data to the ﬁrst en-
clave instance Write(me,i1) which causes a state change.
Another remote client sends a request to read data from
the enclave e. The OS reads data from the second in-
stance me ← Read(i2) which has an outdated state and
returns me to the client. The SGX architecture does not
enable one enclave instance to check if another instance
of the same enclave code is already running [21].
Such attacks can have severe implications, especially
for applications that maintain ﬁnancial data, such as ac-
count balances or transaction histories.
2.4 Limitations of Known Solutions
SGX counters. Intel has recently added support for
monotonic counters [5] as an optional SGX feature that
an enclave developer may use for rollback attack pro-
tection, when available. However, the security and per-
formance properties of this mechanism are not precisely
documented. We performed a detailed analysis of SGX
counters and report our ﬁndings in Appendix B.
To summarize, we found out that counter updates take
80-250 ms and reads 60-140 ms. The non-volatile mem-
ory used to implement the counter wears out after ap-
proximately one million writes, making the counter func-
tionality unusable after a couple of days of continuous
use. Thus, SGX counters are unsuitable for systems
where state updates are frequent and continuous. Ad-
ditionally, since the non-volatile memory used to store
the counters resides outside the processor package, the
mechanism is likely vulnerable to bus tapping and ﬂash
mirroring attacks [22] (see Appendix B for details).
TPM solutions. TPMs provide monotonic counters
and NVRAM that can be used to prevent rollback attacks
[4, 3, 2]. The TPM counter interface is rate-limited (typ-
ically one increment every 5 seconds) to prevent mem-
ory wear out.1 Writing to NVRAM takes approximately
1The TPM 2.0 speciﬁcations introduce high-endurance non-volatile
100 ms and the memory becomes unusable after 300K to
1.4M writes (few days of continuous use) [2]. Thus, also
TPM based solutions are unsuitable for applications that
require fast and continuous updates.
Integrity servers. Another approach is to leverage a
trusted server to maintain state for protected applications
[6, 7, 8]. The drawback of this approach is that the cen-
tralized integrity server becomes an obvious target for
attacks. To eliminate a single point of failure, the in-
tegrity server could be replicated using a Byzantine con-
sensus mechanism. However, standard consensus proto-
cols, such as PBFT [9], require several rounds of com-
munication, have high message complexity, and require
at least three replicas for each faulty node.
Architecture modiﬁcations. Finally, the SGX archi-
tecture could be modiﬁed such that the untrusted OS can-
not erase the enclave runtime memory. However, this ap-
proach would prevent the OS from performing resource
management and would not scale to many enclaves. Ad-
ditionally, rollback attacks through forced reboots and
multiple enclave instances would remain possible. An-
other approach would be to enhance the processor with
a non-volatile memory element. Such changes are costly
and current NVRAM technologies have the performance
limitations we discussed above.
2.5 Rollback Protection Requirements
The goal of our work is to design a rollback protection
mechanism that overcomes the performance and security
limitations of SGX counters and other known solutions.
In particular, our solution should support unlimited and
fast state updates, considering a strong adversary model
without a single point of failure. When there is a trade-
off between security and robustness, we favor security.
3 Our Approach
The intuition behind our approach is that a single SGX
platform cannot efﬁciently prevent rollback attacks, but
the owner or the owners of SGX platforms can enroll
multiple processors to assist each other. Thus, our goal
is to design rollback protection for SGX as a distributed
system between multiple enclaves running on separate
processors. Our distributed system is customized for the
task of rollback protection to reduce the number of re-
quired replicas and communication.
To realize rollback protection, the distributed system
should provide, for each participating platform, an ab-
memory that enables rapidly incremented counters [23]. The counter
value is maintained in RAM and the value is ﬂushed to non-volatile
memory periodically (e.g., mod 100) and at controlled system shut-
down. However, if the system is rebooted without calling TPM Shut-
down, the counter value is lost and at start-up the TPM assumes the
next periodic value. Therefore, such counters do not prevent attacks
where the adversary reboots the system.
1292    26th USENIX Security Symposium
USENIX Association
straction of a secure counter storage that consists of two
operations:
• WriteCounter(value). An enclave can use this opera-
tion to write a counter value to the secure storage.2
• value/empty ← ReadCounter(). An enclave can use
this operation to read a counter value from the secure
storage. The operation returns the last written value or
an empty value if no counter was previously written.
When an enclave performs a security-critical state up-
date operation (e.g., modiﬁes an account balance or ex-
tends a transaction history), it distributes a monotonically
increasing counter value over the network to a set of en-
claves running on assisting processors (WriteCounter),
stores the counter value to its runtime memory and seals
its state together with the counter value for local persis-
tent storage. When the enclave is restarted, it can recover
its latest state by unsealing the saved data, obtaining the
counter values from enclaves on the assisting processors
(ReadCounter) and verifying that the sealed state is of
the latest version. The same technique allows potentially
concurrently running instances of the same enclave iden-
tity to determine that they have the latest state. When
an enclave needs to verify its state freshness (e.g., upon
receiving a request to return the current account balance
or transaction history to a remote client), it obtains the
counter value from the network (ReadCounter) and com-
pares it to the one in its runtime memory. By using en-
claves on the assisting platforms, we reduce the required
trust assumptions on the assisting platforms.
3.1 Distributed Model
We use the term target platform to refer to the node
which performs state updates that require rollback pro-
tection. We assume n SGX platforms that assist the tar-
get platform in rollback protection The platforms can be-
long to a single administrative domain or they could be
owned by private individuals who all beneﬁt from col-
laborative rollback protection. We model each platform
using the SGX model described in Section 2.1. The dis-
tributed system can be seen as a composition of n + 1
SGX instances (target platform included) that are con-
nected over a network. We make no assumptions about
the reliability of the communication network, messages
may be delayed or lost completely. We assume that while
participating in collaborative rollback protection, some
platforms may be temporarily down or unreachable.
Distributed adversary model. On each platform, the
adversary has the capabilities listed in Section 2.2. Ad-
ditionally, we assume that the adversary can compromise
2We use counter write abstraction instead of counter increment, be-
cause our distributed secure storage implementation allows writing of
any counter value to the storage. However, the ROTE system only per-
forms monotonic counter increments using this functionality.
the SGX protections on f < n participating nodes, ex-
cluding the target platform. Such compromise is possi-
ble, e.g., through physical attacks. On the compromised
SGX nodes the adversary can freely modify the runtime
memory (code and data) of any enclave, and read all en-
clave secrets and the SGX processor keys.
This adversarial model combines a standard Dolev-
Yao network adversary [10] with adversarial behaviour
(Byzantine faults) on a subset of participating platforms
[11, 12]. In addition, the adversary can schedule the ex-
ecution of trusted processes, replay old versions of per-
sistently stored data, and start multiple instances of the
same trusted process on the same platform. In Section 5
we explain subtle attacks enabled by such additional ad-
versarial capabilities.
3.2 Challenges
Secure and practical realization of our approach under
a strong adversarial model involves challenges.
Network partitioning. A simple solution would be
to store a counter with all the assisting enclaves, and at
the time of unsealing require that the counter value is ob-
tained from all assisting enclaves. However, if one of the
platforms is unreachable at the time of unsealing (e.g.,
due to network error, maintenance or reboot), the opera-
tion would fail. Our goal is to design a system that can
proceed even if some of the participating enclaves are
unreachable. In such a system, some of the assisting en-
claves may have outdated counter values, and the system
must ensure that only the latest counter value is ever re-
covered, assuming an adversary that can block messages,
and partition the network by choosing which nodes are
reachable at any given time.
Coordinated enclave restarts. When an enclave seals
data, it sends a counter value to a set of enclaves running
on assisting platforms and each enclave must store the
received counter. However, sealing the received counter
for persistent storage would cause a new state update that
would propagate endlessly. Therefore, the enclaves must
maintain the received counters in their runtime memory.
The participating enclaves may be restarted at any time,
which causes them to lose their runtime memory. Thus,
the rollback protection system must provide a recovery
mechanism that allows the assisting enclaves to restore
the lost counters from the other assisting enclaves. Such
a recovery mechanism opens up a new attack vector.
The adversary can launch coordinated attacks where he
restarts assisting enclaves to trigger recovery while the
target platform is distributing its current counter value.
Multiple enclave instances. Simple approaches that
store a counter to a number of assisting enclaves and later
read the counter from sufﬁciently many of the same en-
claves are vulnerable to attacks where the adversary cre-
ates multiple instances of the same enclave. Assume that
USENIX Association
26th USENIX Security Symposium    1293
a counter is saved to the runtime memory of all assist-
ing enclaves. The adversary that controls the OS on all
assisting platforms starts second instances of the same
enclave on all platforms. The target enclave updates its
state and sends an incremented counter to the second in-
stances. Later, the target enclave obtains an old counter
value from the ﬁrst instances and recovers a previous
state from the persistent storage.
4 ROTE System
In this section we describe ROTE (Rollback Protec-
tion for Trusted Execution), a distributed system for state
integrity and rollback protection on SGX. We explain
the counter increment technique, our system architec-
ture, group assignment and system initialization. After
that, we describe the rollback protection protocols.
4.1 Counter Increment Technique
Two common techniques for counter-based rollback
protection exist. The ﬁrst technique is inc-then-store,
where the enclave ﬁrst increments the trusted counter and
after that updates its internal state and stores the sealed
state together with the counter value on disk. This ap-
proach provides a strong security property (no rollback
to any previous state), but if the enclave crashes between
the increment and store operations, the system cannot re-
cover from the crash.
The second technique is store-then-inc, where the en-
clave ﬁrst saves its state on the disk together with the lat-
est input value, after that increments the trusted counter,
and ﬁnally performs the state update [4, 2]. If the sys-
tem crashes, it can recover from the previous state using
the saved input. This technique requires a deterministic
enclave and provides a slightly weaker security property:
arbitrary rollback is not possible, but the last input may
be executed twice on the same enclave state [2].
The stronger security guarantee is needed, for exam-
ple, in enclaves that generate random numbers, commu-
nicate with external parties or create timestamps. Con-
sider a ﬁnancial enclave that receives a request message
from an external party and for each request it should
create only one signed response that is randomized or
includes a timestamp (sgx get trusted time [24]).
If
store-then-inc is used, the adversary can create multiple
different signed responses for the same request.3
The weaker security guarantee is sufﬁcient in applica-
tions where the execution of the same input on the same
state provides no advantage for the adversary.
3While some enclaves that require random numbers can be made
deterministic by using a stateful PRNG and including its state to the
saved enclave state, this may be difﬁcult for enclaves that reuse code
from existing libraries not designed for this. Similarly, some replay
issues can be addressed on the protocol level, but enclave developers
do not always have the freedom to change (standardized) protocols.
Figure 2: The ROTE system architecture.
In this paper we instantiate ROTE using inc-then-store,
because of its strong security guarantee for any enclave.
Our goal is to build a generic platform service that can
protect various applications. We emphasize that if crash
tolerance is required, then store-then-inc should be used.
A rollback protection system could even support both
counter increment techniques and allow developers to
choose the protection style based on their application.
4.2 System Architecture
Figure 2 shows our system architecture. Each platform
may run multiple user applications that have a matching
Application-Speciﬁc Enclave (ASE). The ROTE system
consists of a system service that we call the Rollback
Enclave (RE) and a ROTE library that ASEs can use for
rollback protection.
When an ASE needs to update its state, it calls a
counter increment function from the ROTE library. Once
the RE returns a counter value, the ASE can safely up-
date its state, save the counter value to its memory and
seal any data together with the counter value. When an
ASE needs to verify the freshness of its state, it can again
call a function from the ROTE library to obtain the latest
counter value to verify the freshness of unsealed seal data
(or state in its runtime memory).
The RE maintains a Monotonic Counter (MC), in-
creases it for every ASE update, distributes it to REs
running on assisting platforms, and includes the counter
value to its own sealed data. When the RE needs to
verify the freshness of its own state, it obtains the latest
counter value from the assisting nodes. The RE realizes
the secure counter storage functionality (WriteCounter
and ReadCounter) described in Section 3.
The design choice of introducing a dedicated system
service (RE) hides the distributed counter maintenance
from the applications. Having a separate RE increases
the TCB of our system slightly, but we consider easier
application development more important.
The ROTE system has three conﬁgurable parameters:
• n is the number of assisting platforms,
• f is the number of compromised processors, and
• u is the maximum number of assisting platforms that
can be unreachable or non-responsive at time of state up-
date or read for the system to proceed. Platform restarts
are typically less frequent events and during them we re-
quire all the assisting platforms to be responsive.
1294    26th USENIX Security Symposium
USENIX Association
OS…Platform AASEA1ROTE libROTE libASEAiREAPlatform BROTE System (TCB)3rdParty DevelopmentFigure 3: The ROTE system state structures.
These parameters have a dependency n = f + 2u + 1
(see Section 5). As an example, a system administrator
can select the desired level of security f and robustness
u which together determine the required number of as-
sisting platforms n. Alternatively, given n assisting plat-
forms, the administrator can pick f and u. Recall that
standard Byzantine consensus protocols require always