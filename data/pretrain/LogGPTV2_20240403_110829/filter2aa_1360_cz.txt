2855
0 
771 
3885123
J 
141719
220802 
3666 
3102
0
3384 
2304 
2820
2365
0 
0 
0 
0 
0
health status index
green 
open 
cun toe
green 
open 
kkday
green 
open 
ninhat
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open 
.monitor)
green 
open 
articlooJ
green 
open ------ *■
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open . mor
green 
open mini
green 
open .mor
green 
open rair
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
green 
open
We now know there are several indexes which can be viewed. The first is tided "customer", but only contains a 
small amount of data (68kb). Near the middle, we see an index tided "memberdispIayname20190903" which 
contains 124mb of data including customer usernames. Some of these indexes contain email addresses and other 
sensitive data. Let's focus on the index tided "bank". We can view the content with the following URL.
There are many complications with acquisition of open Elasticsearch data. The first is the record limitation. 
Displaying results within a URL limits the results to 10,000 records. Modifying the URL as described previously 
presents all data possible within the browser. Saving this page stores all of the content for future queries. 
However, many of the data sets I find contain millions of records. A Python script which parses through all 
results and stores every record is most appropriate, and is explained in a moment.
Data Breaches & I ^eaks 447
customer 
eday 
ninhash2 
.monitoring-kibana-6-2019.05 
eombcrdisplaynamo20190903 
.monitoring-k''------- ' 
“•
strcamtopic 
articlos20190114 
---- 4 “ring-kibam 
--------------jt>20190130 
mcmbor20181029 
.monitoring-os-6-2019.09.19 
articloa20181224 
minhashS 
test 
.monitoring-ea-6-2019.09.20 
.monitoring-es-6-2019.09.23 
minhssh4 
articles201B1214 
.kibana
initoring-os-6-2019.09.18 
ihash3 
>nitoring-o«-6-2019.09.22 
Ln/test 
.monitoring-oa-6-2019.09.24 
.taonitoring-oa-6-2019.09.21 
.raonitoring-kibana-6 
.monitoring-kibana-6-2( 
minhaah 
bank 
customer service 
articlos20190917 
.nonitoring-klbana-6-2019.05 
.iaonitoring-kibana-6-2019 .OS
uuid 
xYPN7VCPQHuRdEfP5vEkvg 
3 kWWwCSTOa 1AXMVO Ja -eg 
7aEhgxDDQ-2nu-Vpjpei‘"“ 
6aE7CmflBQV2Iqk-8PcV 
OARHG91LSP-rSC ~
-kibana-6-2019.09.19 Y17_DE_zTzaY7l . 
_ .
KRzRxJ8cRd2CpbSEC-tnXA 
5 ZUblCdHQE 6 j o 6JQK2 J7 aA 
taiHMlrvT3q!!!r”’T• 
4ZCi8yaxQvW113hj2TOOECQ 
_8 £ D 5 9 3 v 5 n-6hEECc E aLuA 
nOHGgDlbQrSE79xru9K10w 
71O-SW4OTPiORXtyj3qkJA 
t£D40-XoRcq3Vx0NPEVAcA 
Q9 f E J j 9 nRs 6114 oXNvckTmg 
1j4OGJUDSJuG7EfSu0p6qA 
KgZ_kQUEThG501ai27Yc9n
6ztoT6nwTicIp301 
-oWrFwD-RBixBG-n 
yWXB2EJXS4it!d 
Qg7Ised sensitive data associated with 
such as first name, last name,
39
.score:
.source:
account_nuxber:
balance:
firstnane:
lastnaae:
address:
employer:
Figure 28.04: An entry from an
.type:
,-jjjjj|heath9zappix.co3"
"Shaft­
in early 2019,1 was made aware of an open Elasticsearch database which expo;
57 million people. In most cases, these records contained personal information
email address, home address, state, postal code, phone number, and IP address. These types of records are 
extremely helpful to me as an investigator. Connecting personal email addresses with real people is often the 
best lead of all online research. I had found the database on Shodan using the methods discussed here. 
Specifically, I searched for an index titled "Leads" and sifted through any results of substantial size. Once I had 
located the data, I was desperate to download die entire archive. With a browser limit of 10,000,1 knew I would 
need the help of a Python script.
Next are die legal considerations. Technically, this data is publicly available, open to anyone in die world. 
However, some believe the act of manipulating URLs in order to access content stored within a database exceeds 
the definition of OSINT. 1 do not agree, but you might. 1 believe most of this data should have been secured, 
and we should not be able to easily collect it. The same could be said for FTP servers, paste sites, online 
documents, and cloud-hosted files. I believe accessing open databases becomes a legal grey area once you decide 
to use the data. If you are collecting this content in order to sell it, extort the original owner, or publish it in any 
way, you are crossing the line and committing a crime. I remind you that the Computer Fraud and Abuse Act 
(CFAA) is extremely vague and can make most online activity illegal in the eyes of an aggressive prosecutor. 
Become familiar with the data access laws in your area and confirm that these techniques do not violate any laws 
or internal policies. My final reminder and warning is that 1 am not an attorney. I am not advising that you 
conduct any of these methods on behalf of your own investigations. 1 am simply presenting techniques which 
have proven to be extremely valuable to many investigators. If you believe that accessing an open (public) 
Elasticsearch database is legal in your area, and does not violate any internal policies, it is time to parse and 
download all content.
I reached out to my friend and colleague Jesse and explained my scenario. Within a few moments, he sent me a 
small Python script. This file is now a staple in my data leaks arsenal of tools. He has agreed to share it publicly, 
please use it responsibly. If you installed all the software within the Linux chapters, you are ready for this tutorial. 
If not, you can enter the following commands within Terminal.
http://111.93.162.238:9200/
http://111.93.162.238:9200/_cat/indices?v
http://111.93.162.238:9200/leads/_search?size=100
Data Breaches & Leaks 449
• 
cd -/Downloads/Programs/Elasticsearch-Crawler
• 
python crawl.py
store.size 
1.3mb 
190.7kb 
1.8mb 
7.2kb 
503.5kb 
2.3mb 
1.7mb 
280.8kb
jort number, and fields to 
You must be logged in
index
imobilestore
testcsv
easyphix
index_test
crazyparts
valuepans 
mobilemart 
leads
This connects to the IP address (111 .93.162.238) and port (9200), and then conducts a query to display all public 
indexes (/_cat/indices?v). The result included the following.
This connects to the target IP address (111.93.162.238) and port (9200), loads the desired index (leads) and 
displays the first 100 results (/_search?size=100). This is usually sufficient to sec enough target content, but this 
can be raised to 1000 or 10000 if desired. Below is a record.
This will present several user prompts to enter the target IP address, index name, pt 
obtain. Let's walk through a real demonstration in order to understand the application, 
to a free or premium Shodan account in order to complete this tutorial. Using the IntelTechniques Breaches & 
Leaks Tool, seen in Figure 28.05 at the end of the chapter, I entered "leads" into the field tided "Elasticsearch". 
This conducted an exact search of "product:elastic port:9200 leads" within Shodan, which displayed a handful 
of results. One of these was an Elasticsearch server in India. This database appeared to contain test data, so 1 
will not redact any' of the results. The IP address was 111.93.162.238 and the database was approximately 1GB 
in size. Clicking the red square within the result on Shodan opened a new tab to the following address.
The brief response confirmed that the server was online. The URL discloses the IP address (111.93.162.238) 
and port (9200). This is the default port and is almost always the same. Now that I had the IP address, I entered 
it within the option titled "Index List" within the tool. Clicking that button launched a new tab at the following 
address.
I usually look at both the names and the sizes. If I see an index titled "Customers", I know it usually contains 
people's information. If it is only Ikb in size, I know it is too small to be of any use. When I see any index with 
multiple gigabytes of data, my curiosity kicks in and I want to see the contents. For this demonstration, let's 
focus on the index of our original search of "leads". Within our search tool, the next option is labeled "Index 
View". This requires the IP address of our target (111.93.162.238) and the name of the index you wish to view 
(leads). This opens a new tab at the following URL.
You are now ready to download an entire Elasticsearch open database and specify which fields should be 
acquired. Note that you must open Terminal and navigate to your script in order to launch this utility. I have 
included a desktop shortcut within your OSINT VM titled "Breaches/Leaks Tool" which automates this 
process, but let's understand the manual approach first. The following commands from within any Terminal 
session will launch the script.
450 Chapter 28
cd -/Downloads/Programs/Elasticsearch-Crawler
git pull https://github.com/AmIJesse/Elasticsearch-Crawler.git
IP address: 111.93.162.238
Index name: leads
Port (Default is 9200): 9200 
Field values to obtain (submit an 
Value: email
Value: first_name
Value: last_name
Value: phone
Value:
empty line when finished):
"-index": "leads","_tvpe": "leads",
"Jd": "PXIhqmUBcHz5ZA2uOAe7", 
"-source": {"id": "86",
"email": "PI:EMAIL", 
"first—name": "test80","last_name": "test80", 
"phone": "32569874", 
"ip": "0.0.0.0",
"orgname": "Sales Arena",
"isDeleted": false,
"created.at": "2018-09-05 19:57:08",
"updated.at": "2018-09-05 19:57:08",
PI:EMAIL,test65,test65,987485746 