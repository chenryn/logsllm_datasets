This approximation may not only hold for sufﬁciently-small
input perturbations. It may also hold for larger perturbations
if the classiﬁcation function is linear or has a small curvature
(e.g., if it is strongly regularized). It is not difﬁcult to see
that, for any given point x,y, the evasion problem in Eqs. (2)-
(3) (without considering the feature bounds in Eq. 4) can be
rewritten as:
ˆδ ∈ arg max
(cid:107)δ(cid:107)p≤ε
(cid:96)(y,x + δ, ˆw) .
(14)
Under the same linear approximation, this corresponds to the
maximization of an inner product over an ε-sized ball:
δ(cid:62)∇x(cid:96)(y,x, ˆw) = ε(cid:107)∇x(cid:96)(y,x, ˆw)(cid:107)q ,
(15)
max
(cid:107)δ(cid:107)p≤ε
where (cid:96)q is the dual norm of (cid:96)p.
The above problem is maximized as follows:
1. For p = 2, the maximum is ˆδ = ε ∇x(cid:96)(y,x, ˆw)
(cid:107)∇x(cid:96)(y,x, ˆw)(cid:107)2
2. For p = ∞, the maximum is ˆδ ∈ ε· sign{∇x(cid:96)(y,x, ˆw)};
3. For p = 1, the maximum is achieved by setting the values
of ˆδ that correspond to the maximum absolute values of
∇x(cid:96)(y,x, ˆw) to their sign, i.e., ±1, and 0 otherwise.
;
Substituting the optimal value of ˆδ into Eq. (13), we can
compute the loss increment ∆(cid:96) = ˆδ(cid:62)∇x(cid:96)(y,x,w) under a trans-
fer attack in closed form; e.g., for p = 2, it is given as:
∆(cid:96) = ε
(cid:62)
∇x ˆ(cid:96)
(cid:107)∇x ˆ(cid:96)(cid:107)2
∇x(cid:96) ≤ ε(cid:107)∇x(cid:96)(cid:107)2 ,
(16)
where, for compactness, we use ˆ(cid:96) = (cid:96)(y,x, ˆw) and (cid:96) =
(cid:96)(y,x,w). In this equation, the left-hand side is the increase in
the loss function in the black-box case, while the right-hand
side corresponds to the white-box case. The upper bound is
obtained when the surrogate classiﬁer ˆw is equal to the tar-
get w (white-box attacks). Similar results hold for p = 1 and
p = ∞ (using the dual norm in the right-hand side).
Intriguing Connections and Transferability Metrics. The
above ﬁndings reveal some interesting connections among
transferability of attacks, model complexity (controlled by the
classiﬁer hyperparameters) and input gradients, as detailed
below, and allow us to deﬁne simple and computationally-
efﬁcient transferability metrics.
(1) Size of Input Gradients. The ﬁrst interesting observation
is that transferability depends on the size of the gradient of
the loss (cid:96) computed using the target classiﬁer, regardless of
the surrogate: the larger this gradient is, the larger the attack
impact may be. This is inferred from the upper bound in
Eq. (16). We deﬁne the corresponding metric S(x,y) as:
S(x,y) = (cid:107)∇x(cid:96)(y,x,w)(cid:107)q ,
(17)
where q is the dual of the perturbation norm.
326    28th USENIX Security Symposium
USENIX Association
Figure 3: Size of input gradients (averaged on the test set)
and test error (in the absence and presence of evasion attacks)
against regularization (controlled via weight decay) for a neu-
ral network trained on MNIST89 (see Sect. 5.1.1). Note how
the size of input gradients and the test error under attack de-
crease as regularization (complexity) increases (decreases).
Figure 4: Conceptual representation of the variability of the
loss landscape. The green line represents the expected loss
with respect to different training sets used to learn the surro-
gate model, while the gray area represents the variance of the
loss landscape. If the variance is too large, local optima may
change, and the attack may not successfully transfer.
The size of the input gradient also depends on the complex-
ity of the given model, controlled, e.g., by its regularization hy-
perparameter. Less complex, strongly-regularized classiﬁers
tend to have smaller input gradients, i.e., they learn smoother
functions that are more robust to attacks, and vice-versa. No-
tably, this holds for both evasion and poisoning attacks (e.g.,
the poisoning gradient in Eq. 10 is proportional to αc, which
is larger when the model is weakly regularized). In Fig. 3
we report an example showing how increasing regularization
(i.e., decreasing complexity) for a neural network trained on
MNIST89 (see Sect. 5.1.1), by controlling its weight decay,
reduces the average size of its input gradients, improving ad-
versarial robustness to evasion. It is however worth remarking
that, since complexity is a model-dependent characteristic,
the size of input gradients cannot be directly compared across
different learning algorithms; e.g., if a linear SVM exhibits
larger input gradients than a neural network, we cannot con-
clude that the former will be more vulnerable.
Another interesting observation is that, if a classiﬁer has
large input gradients (e.g., due to high-dimensionality of the
input space and low level of regularization), for an attack
to succeed it may sufﬁce to apply only tiny, imperceptible
perturbations. As we will see in the experimental section,
this explains why adversarial examples against deep neural
networks can often only be slightly perturbed to mislead
detection, while when attacking less complex classiﬁers in
low dimensions, modiﬁcations become more evident.
(2) Gradient Alignment. The second relevant impact fac-
tor on transferability is based on the alignment of the input
gradients of the loss function computed using the target and
the surrogate learners. If we compare the increase in the loss
function in the black-box case (the left-hand side of Eq. 16)
against that corresponding to white-box attacks (the right-
hand side), we ﬁnd that the relative increase in loss, at least
for (cid:96)2 perturbations, is given by the following value:
R(x,y) =
(cid:62)
∇x ˆ(cid:96)
∇x(cid:96)
(cid:107)∇x ˆ(cid:96)(cid:107)2(cid:107)∇x(cid:96)(cid:107)2
.
(18)
Interestingly, this is exactly the cosine of the angle between
the gradient of the loss of the surrogate and that of the target
classiﬁer. This is a novel ﬁnding which explains why the co-
sine angle metric between the target and surrogate gradients
can well characterize the transferability of attacks, conﬁrming
empirical results from previous work [21]. For other kinds
of perturbation, this deﬁnition slightly changes, but gradient
alignment can be similarly evaluated. Differently from the
gradient size S, gradient alignment is a pairwise metric, al-
lowing comparisons across different surrogate models; e.g.,
if a surrogate SVM is better aligned with the target model
than another surrogate, we can expect that attacks targeting
the surrogate SVM will transfer better.
(3) Variability of the Loss Landscape. We deﬁne here an-
other useful metric to characterize attack transferability. The
idea is to measure the variability of the loss function ˆ(cid:96) when
the training set used to learn the surrogate model changes,
even though it is sampled from the same underlying distri-
bution. The reason is that the loss ˆ(cid:96) is exactly the objective
function A optimized by the attacker to craft evasion attacks
(Eq. 1). Accordingly, if this loss landscape changes dramati-
cally even when simply resampling the surrogate training set
(which may happen, e.g., for surrogate models exhibiting a
large error variance, like neural networks and decision trees),
it is very likely that the local optima of the corresponding
optimization problem will change, and this may in turn imply
that the attacks will not transfer correctly to the target learner.
We deﬁne the variability of the loss landscape simply as
the variance of the loss, estimated at a given attack point x,y:
V (x,y) = ED{(cid:96)(y,x, ˆw)2}− ED{(cid:96)(y,x, ˆw)}2 ,
(19)
where ED is the expectation taken with respect to different
(surrogate) training sets. This is very similar to what is typi-
cally done to estimate the variance of classiﬁers’ predictions.
This notion is clariﬁed also in Fig. 4. As for the size of input
gradients S, also the loss variance V should only be compared
across models trained with the same learning algorithm.
USENIX Association
28th USENIX Security Symposium    327
0.00.20.40.60.81.0Regularization(weightdecay)0.050.100.15Sizeofinputgradients0.00.20.40.60.81.0TesterrorHighcomplexityLowcomplexitytesterror(noattack)testerror("=0.3)x‘(y,x,ˆw)V(x,y)The transferability metrics S, R and V deﬁned so far depend
on the initial attack point x and its label y. In our experiments,
we will compute their mean values by averaging on different
initial attack points.
Transferability of Poisoning Attacks. For poisoning attacks,
we can essentially follow the same derivation discussed be-
fore. Instead of deﬁning transferability in terms of the loss
attained on the modiﬁed test point, we deﬁne it in terms
of the validation loss attained by the target classiﬁer un-
der the inﬂuence of the poisoning points. This loss func-
tion can be linearized as done in the previous case, yielding:
T (cid:117) L(D,w) + ˆδ(cid:62)∇xL(D,w), where D are the untainted val-
idation points, and ˆδ is the perturbation applied to the initial
poisoning point x against the surrogate classiﬁer. Recall that
L depends on the poisoning point through the classiﬁer param-
eters w, and that the gradient ∇xL(D,w) here is equivalent
to the generic one reported in Eq. (9). It is then clear that the
perturbation ˆδ maximizes the (linearized) loss when it is best
aligned with its derivative ∇xL(D,w), according to the con-
straint used, as in the previous case. The three transferability
metrics deﬁned before can also be used for poisoning attacks
provided that we simply replace the evasion loss (cid:96)(y,x,w)
with the validation loss L(D,w).
5 Experimental Analysis
In this section, we evaluate the transferability of both evasion
and poisoning attacks across a range of ML models. We high-
light some interesting ﬁndings about transferability, based
on the three metrics developed in Sect. 4. In particular, we
analyze attack transferability in terms of its connection to the
size of the input gradients of the loss function, the gradient
alignment between surrogate and target classiﬁers, and the
variability of the loss function optimized to craft the attack
points. We provide recommendations on how to choose the
most effective surrogate models to craft transferable attacks
in the black-box setting.
5.1 Transferability of Evasion Attacks
We start by reporting our experiments on evasion attacks. We
consider here two distinct case studies, involving handwritten
digit recognition and Android malware detection.
5.1.1 Handwritten Digit Recognition
The MNIST89 data includes the MNIST handwritten digits
from classes 8 and 9. Each digit image consists of 784 pixels
ranging from 0 to 255, normalized in [0,1] by dividing such
values by 255. We run 10 independent repetitions to average
the results on different training-test splits. In each repetition,
we run white-box and black-box attacks, using 5,900 samples
to train the target classiﬁer, 5,900 distinct samples to train the
surrogate classiﬁer (without even relabeling the surrogate data
Figure 5: White-box evasion attacks on MNIST89. Test error