value of these parameters:
1|w1,¬inv-A), i.e., the probability that the alert
The false positive rate, P (a1
1|w1, inv-A), i.e., how likely the attack will trigger the rule
1762 in Snort. This is easily calculated from the false negative rate (FNR)
of the rule: 1 − FNR.
is triggered when there is no attack.
The power, P (a1
Both of these values are well known to the security oﬃcer and already indirectly
used when manually deciding whether an alert is worth extra examination. If
the alert comes from a rule that has many false alarms, the security oﬃcer will
probably not follow up unless there is further evidence.
A Multi-Sensor Model to Improve Automated Attack Detection
301
To summarize, we need to specify eight parameters, but because of the domain
and the inherent structure we are left with four values that the security oﬃcer is
already familiar with. These values can be estimated by the IDS vendor, and then
ﬁne-tuned at the local site. As we speciﬁed in Section 3.2, parameter estimation
for Bayesian networks is quite robust and a correct absolute number is seldom
necessary as long as the magnitude of the numbers correspond. To emphasize
this fact, we have used a set of predeﬁned ranges to deﬁne our model: never, low,
low-medium, medium, medium-high, high, and always. In Section 5 we replace
these labels with their corresponding numerical probabilities and show that the
model is robust against some error when estimating these values. When running
the examples with real traﬃc, one would rather tune the values according to
knowledge of the network environment and the speciﬁcs of the actual alert rule.
4 Example Scenarios
We use two scenarios to exemplify the model. We limit our discussion to attacks
directed at web servers and related software. The web server is a complex piece
of software, often outside the perimeter defense and accessible to anyone in the
world. To complicate matters, the web server often forwards requests to inside
resources (legacy databases) that were never designed with a robust security
model. Being the analogy of the front door to a company, numerous attacks
have been directed toward web servers and the resources with which they com-
municate. There also exist open-source web server alternatives that are mature
enough to allow direct instrumentation (to collect security-relevant events). We
use the basic phf attack to illustrate the principles of our model (see Section 3.3).
Example 1 is how we foresee the typical use of our model: using several comple-
mentary sensors together to increase the accuracy of the system as a whole.
This is also the easiest application of the models, as the parameters can be
reused.
Example 2 shows a deployment that is fairly common among users of IDSs,
with one sensor on the outside of a ﬁrewall and another one on the inside.
Even though it is more complicated than the typical use of our model (exam-
ple 1), one can easily foresee how some of the settings could automatically
be set by an automatic tool.
4.1 Example 1: Two Sensors Using Diﬀerent Audit Streams
In this scenario we deploy two sensors using diﬀerent audit sources (Figure 3).
The ﬁrst is the networked-based IDS Snort used previously, and the other one
is developed by us and called webIDS in this paper. webIDS uses events from
within the web server for its analysis. It is a variant of the system described by
Almgren et al. [2], i.e., a signature-based system using pattern matching similar
to that in Snort.
Using two complementary systems improves the attack detection capability,
as shown by Almgren [3]. The Snort system uses rule 1762 described above, while
the webIDS has the following rule for this scenario:
302
M. Almgren, U. Lindqvist, and E. Jonsson
Fig. 3. Model for using Snort together with a sensor within the web server (example 1)
webIDS 1: detects whether the phf program is run successfully by the web
server.
Clearly, this rule will have some false positives if the phf program is normally
used in the environment. However, an alert from webIDS 1 coupled with an alert
from Snort 1762 means that an attack was launched and the script executed
successfully.
The model is shown in Figure 3. Adding independent IDSs to the model does
not change the already-existing parts. For that reason, the Snort part remains
the same and we reuse the values from Figure 2. We only need to add parameters
for w2 and a2
1. To simplify the model, we assume that the webIDS is very resistant
to failures and set P (w2 = T) to be close to one (always). We deﬁne the CPT for
a2
1 in a similar fashion as was done for a1
1 in Section 3.3. Note that we exclude
any dependency between the IDSs to simplify the model.
4.2 Example 2: Two Sensors on Opposite Sides of a Firewall Proxy
In this scenario we monitor an internal web server, protected by a ﬁrewall / web
proxy. We use one instance of Snort (S2) to monitor traﬃc outside the proxy
and another instance of Snort for the inside traﬃc (S1). The resulting model is
shown in Figure 4. The proxy should block all web-related traﬃc. As long as
the proxy works as expected, we expect that all attacks are blocked. Thus, even
if S2 reports about attacks, these can safely be ignored as long as S1 is quiet.
However, if S1 is broken or taken oﬄine, one should ensure that the proxy is
working as expected.
In this scenario, we want to show how two (identical) versions of Snort still
can be seen as somewhat independent given that they analyze diﬀerent traﬃc
streams and thus are used in collaboration in our model.
We show the resulting model in Figure 4. For this scenario, we made several
changes compared to the model shown in Figure 2. First, we replaced the type of
failure observation node from an observation of encrypted traﬃc to an observation
of a heartbeat message. This change is done to show that one should use a diversity
of r-nodes, even though we restrict them in this paper for clarity. Having S1 inside
of the proxy implies that alerts from this sensor are more serious than alerts from
a sensor without a ﬁltering proxy (as the one in Example 1, shown in Figure 3).
A Multi-Sensor Model to Improve Automated Attack Detection
303
Fig. 4. Model for using two versions of Snort with one outside a proxy and the other
one inside (example 2)
1|w1,¬inv-A), meaning that we expect
Thus, we lowered the probability for P (a1
fewer false alarms (in the sense that they are not worth further investigation) from
this sensor. For this example, we say that all alerts from the inside sensor should
be investigated.
1 and a2
Furthermore, we have added an explicit dependence between a1
1. As we
run two versions of Snort, we expect that any alert-raising traﬃc on the inside
also exists outside, i.e., S1 sees a subset of all the traﬃc passing by S2. The
CPT for a2
1 is shown in the ﬁgure. We omitted special r-nodes for w2 to keep
the model simple and to the point.
5 Experiment
We base the simulations and experiments on the two examples described in
Section 4. We ﬁrst simulated the models presented in Figure 3 and Figure 4,
and then we implemented the models on our test bed. As described above, we
concentrate on the phf attack described in Section 3.3. Even though this may
seem limiting at ﬁrst glance, it clearly illustrates the principles of our approach.
Other attacks can easily be added later.
5.1 Experiment Setup
The models for Example 1 and Example 2 each have three observable nodes. The
test series used for the simulations are shown in Table 3. When we refer to the
diﬀerent experiments, we replace the x in the ﬁrst column with the corresponding
example number. As we noted in Section 2.1, we consider case (iv) in Table 1
uncommon with modern sensors. For example, as Snort cannot rebuild the HTTP
transaction for an encrypted request, the string matching on the URI fails for
rule 1762 and Snort does not produce an alert. Hence, the case TT* for example 1
is unusual in practice and is not included in the test series. Similar reasoning
goes for FT* for example 2.
304
M. Almgren, U. Lindqvist, and E. Jonsson
Table 3. Results of the simulation of example 1 and example 2
Exp # P(inv-A r1
Example 1
2) P (w1| . . .) Comment
1a1
1a1
FFF
FFT
0.89
0.80
0.01
0.20
x-1
x-2
x-3
x-4
x-5
x-6
0.30
FTF
0.99
0.91
0.05
FTT
TFF
1.00
0.01
0.54
TFT
0.01
P(inv-A r1
Example 2
2) P (w2| . . .)
1a1
1a1
TFF
TFT
0.87
1.00
0.02
0.19
0.96
TTF
0.08
0.96
0.08
TTT
FFF
0.54
FFT
1.00
0.84
1.00
no attack / script run
normal phf (no attack)
phf attack against
server without the
script
attack and script run
broken sensor, but no
script invocation
broken sensor, and
script run
As our focus is on alert reasoning, normal traﬃc causing no alerts is quite
uninteresting. Normal web traﬃc can be used to track down false alarms on a
sensor-per-sensor basis, but it adds little to our analysis of reasoning with the
available alerts. Rather, the analysis of these false alarms would be used to tune
the parameter estimation of our model. For these reasons, we do not use any
normal web requests in our experiment.
In the Comment column in Table 3, we explain what the node status implies
and we use this as a basis to decide what type of traﬃc to use in the experi-
ment for that particular case. Based on the observable events, we then calculate
the posterior probability for node inv-A. In a real system, we would most likely
collapse this posterior to either investigate or do not investigate using a deci-
sion function. In this paper, we use a simple decision function with a threshold
parameter, 0 ≤ τ ≤ 1, where all values that are less than τ are not considered
worthy of investigation. We let τ = 0.5, thus choosing the most probable class.
For the simulation and the experiment, we map our labels to actual prob-
abilities. Instead of a range (represented by our labels), a Bayesian network
needs an actual probability (as we have shown in Section 3.3). Thus, we collapse
the ranges and let never, low, low-medium, medium, medium-high, high, and
always map to the following values (in order): [0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99].
These values could be ﬁne-tuned. However, in Section 5.4 we show that such
ﬁne-tuning is not always necessary because the model is robust against minor
estimation errors.
5.2 Simulation
We simulated the models used in Examples 1 and 2 by setting the observable
nodes (columns 3 and 7) to a speciﬁc value and then calculating the posterior
probability for the other nodes. The simulation results of Examples 1 and 2
are presented in Table 3. Each experimental series is preﬁxed with the scenario
number when we discuss it in the text.
A Multi-Sensor Model to Improve Automated Attack Detection
305
5.3 Experiment
We implemented the two scenarios in our test bed. The implementation used
the SMILE reasoning engine [10].
For Example 1, we ran normal phf requests and attack phf requests past a
Snort sensor to a web server that either had a script called phf installed, or no
such script. We repeated the experiment with encrypted traﬃc. The alerts were
collected, and analyzed by our reasoning engine. The results correspond to the
values shown in Table 3.
For Example 2, we sent normal phf requests and attack phf requests to a web
proxy. The proxy either forwarded the result to a web server (proxy broken)
or returned a message stating that the script did not exist (proxy working cor-
rectly). We used two versions of Snort to monitor the traﬃc. The ﬁrst version
monitored the traﬃc to the proxy, while the second monitored the traﬃc to the