such data-addressing registers without requiring mask-
ing.
3.1.3 Reference Compiler
We have modiﬁed LLVM 2.6 [13] to implement our
ARM SFI design. We chose LLVM because it appeared
to allow an easier implementation of our SFI design, and
to explore its use in future cross-platform work. In prac-
tice we have also found it to produce faster ARM code
than GCC, although the details are outside the scope of
this paper. The SFI changes were restricted to the ARM
target implementation within the llc binary, and re-
quired approximately 2100 lines of code and table mod-
iﬁcations. For the results presented in this paper we used
4The guard pages “below” the data region are actually at the top of
the address space, where the OS resides, and are not accessible from
user mode.
x86-64
the compiler to generate standard Linux executables with
access to the full instruction set. This allows us to isolate
the behavior of our SFI design from that of our trusted
runtime.
3.2
While the mechanisms of our x86-64 implementation
are mostly analogous to those of our ARM implemen-
tation, the details are very different. As with ARM, a
valid data address range is surrounded by guard regions,
and modiﬁcations to the stack pointer (rsp) and base
pointer (rbp) are masked or guarded to ensure they al-
ways contain a valid address. Our ARM approach relies
on being able to ensure that the lowest 1GB of address
space does not contain trusted code or data. Unfortu-
nately this is not possible to ensure on some 64-bit Win-
dows versions, which rules out simply using an address
mask as ARM does. Instead, our x86-64 system takes
advantage of more sophisticated addressing modes and
use a small set of “controlled” registers as the base for
most effective address computations. The system uses
the very large address space, with a 4GB range for valid
addresses surrounded by large (multiples of 4GB) un-
mapped/protected regions.
In this way many common
x86 addressing modes can be used with little or no sand-
boxing.
Before we describe the details of our design, we pro-
vide some relevant background on AMD’s 64-bit exten-
sions to x86. Apart from the obvious 64-bit address
space and register width, there are a number of perfor-
mance relevant changes to the instruction set. The x86
has an established practice of using related names to
identify overlapping registers of different lengths; for ex-
ample ax refers to the lower 16-bits of the 32-bit eax. In
x86-64, general purpose registers are extended to 64-bits,
with an r replacing the e to identify the 64 vs. 32-bit reg-
isters, as in rax. x86-64 also introduces eight new gen-
eral purpose registers, as a performance enhancement,
named r8 - r15. To allow legacy instructions to use
these additional registers, x86-64 deﬁnes a set of new
preﬁx bytes to use for register selection. A relatively
small number of legacy instructions were dropped from
the x86-64 revision, but they tend to be rarely used in
practice.
tion rules are speciﬁc to our x86-64 sandbox:
With these details in mind, the following code genera-
• The module address space is an aligned 4GB region,
ﬂanked above and below by protected/unmapped re-
gions of 10×4GB, to compensate for scaling (c.f.
below)
• A designated register “RZP” (currently r15) is ini-
tialized to the 4GB-aligned base address of un-
trusted memory and is read-only from untrusted
code.
• All rip update instructions must use RZP.
To ensure that rsp and rbp contain a valid data address
we use a few additional constraints:
• rbp can be modiﬁed via a copy from rsp with no
• rsp can be modiﬁed via a copy from rbp with no
masking required.
masking required.
• Other modiﬁcations to rsp and rbp must be done
with a pseudo-instruction that post-masks the ad-
dress, ensuring that it contains a valid data address.
For example, a valid rsp update sequence looks like
this:
%esp = %eax
lea (%RZP, %rsp, 1), %rsp
In this sequence the assignment5 to esp guarantees that
the top 32-bits of rsp are cleared, and the subsequent
add sets those bits to the valid base. Of course such se-
quences must always be executed in their entirety. Given
these rules, many common store instructions can be used
with little or no sandboxing required. Push, pop and
near call do not require checking because the up-
dated value of rsp is checked by the subsequent mem-
ory reference. The safety of a store that uses rsp or rbp
with a simple 32-bit displacement:
mov disp32(%rsp), %eax
follows from the validity invariant on rsp and the guard
ranges that absorb the displacement, with no masking re-
quired. The most general addressing expression for an
allowed store combines a valid base register (rsp, rbp
or RZP) with a 32-bit displacement, a 32-bit index, and
a scaling factor of 1, 2, 4, or 8. The effective address is
computed as:
basereg + indexreg * scale + disp32
For example, in this pseudo-instruction:
add $0x00abcdef, %ecx
mov %eax, disp32(%RZP, %rcx, scale)
the upper 32 bits of rcx are cleared by the arithmetic
operation on ecx. Note that any operation on ecx
will clear the top 32 bits of rcx. This required mask-
ing operation can often be combined other useful oper-
ations. Note that this general form allows generation of
addresses in a range of approximately 100GB, with the
5We have used the = operation to indicate assignment to the register
on the left hand side. There are several instructions, such as lea or
movzx that can be used to perform this assignment. Other instructions
are written using ATT syntax.
x86-64
SFI
16.0
1.60
35.1
1.34
29.3
-4.07
34.6
-4.46
43.0
21.6
0.80
14.7
SFI vs.
-m32
0.82
-5.06
35.1
1.34
-8.17
-4.07
26.6
-4.46
26.0
4.84
-3.08
5.24
SFI vs. ARM
SFI
0.53
6.57
5.31
-3.65
6.61
10.83
9.43
7.01
4.71
5.38
4.94
5.17
-m64
16.0
1.60
33.0
-42.6
29.3
-20.3
34.6
-5.09
43.0
21.6
0.80
6.9
164.gzip
175.vpr
176.gcc
181.mcf
186.crafty
197.parser
253.perlbmk
254.gap
255.vortex
256.bzip2
300.twolf
geomean
Table 2: SPEC2000 SFI Performance Overhead (percent). The
ﬁrst column compares x86-64 SFI overhead to the “oracle”
baseline compiler.
164.gzip
175.vpr
176.gcc
181.mcf
186.crafty
197.parser
253.perlbmk
254.gap
255.vortex
256.bzip2
300.twolf
ARM ARM SFI %inc. %pad
13
13
14
12
12
12
14
11
13
13
11
73
225
1586
84
320
219
812
531
720
74
289
90
271
1931
103
384
265
1009
636
845
92
343
24
20
22
23
20
21
24
20
17
24
19
Table 3: ARM SPEC2000 text segment size in kilobytes, with
% increase and % padding instructions.
Cortex-A9, and is fairly consistent across the bench-
marks. Increases in binary size (Table 3) are compara-
ble at around 20% (generally about 10% due to align-
ment padding and 10% due to added instructions, shown
in the rightmost columns of the table). We believe the
observed overhead comes primarily from the increase in
code path length. For mcf, this benchmark is known to
be data-cache intensive [17], a case in which the addi-
tional sandboxing instructions have minimal impact, and
can sometimes be hidden by out-of-order execution on
the Cortex-A9. We see the largest slowdowns for gap,
gzip, and perlbmk. We suspect these overheads are
a combination of increased path length and instruction
cache penalties, although we do not have access to ARM
hardware performance counter data to conﬁrm this hy-
pothesis.
Figure 1: SPEC2000 SFI Performance Overhead for the ARM
Cortex-A9.
valid 4GB range near the middle. By reserving and un-
mapping addresses outside the 4GB range we can ensure
that any dereference of an address outside the valid range
will lead to a fault. Clearly this scheme relies heavily on
the very large 64-bit address space.
Finally, note that updates to the instruction pointer
must align the address to 0 mod 32 and initialize the
top 32-bits of address from RZP as in this example us-
ing rdx:
%edx = ...
and 0xffffffe0, %edx
lea (%RZP, %rdx, 1), %rdx
jmp *%rdx
Our x86-64 SFI implementation is based on GCC
4.4.3, requiring a patch of about 2000 lines to the com-
piler, linker and assembler source. At a high level,
the changes include supporting the new call/return se-
quences, making pointers and longs 32 bits, allocating
r15 for use as RZB, and constraining address generation
to meet the above rules.
4 Evaluation
In this section we evaluate the performance of our ARM
and x86-64 SFI schemes by comparing against the rel-
evant non-SFI baselines, using C and benchmarks from
SPEC2000 INT CPU [12]. Our main analysis is based on
out-of-order CPUs, with additional measurements for in-
order systems at the end of this section. The out-of-order
systems we used for our experiments were:
• For x86-64, a 2.4GHz Intel Core 2 Quad with 8GB
of RAM, running Ubuntu Linux 8.04, and
• For ARM, a 1GHz Cortex-A9 (Nvidia Tegra T20)
with 512MB of RAM, running Ubuntu Linux 9.10.
4.1 ARM
For ARM, we compared LLVM 2.6 [13] to the same
compiler modiﬁed to support our SFI scheme. Figure 1
summarizes the ARM results, with tabular data in Ta-
ble 2. Average overhead is about 5% on the out-of-order
-4%-2%0%2%4%6%8%10%12%gzipvprgccmcfcraftyparserperlbmkgapvortexbzip2twolf164.gzip
175.vpr
176.gcc
181.mcf
186.crafty
197.parser
253.perlbmk
254.gap
255.vortex
256.bzip2
300.twolf
-m32
122
87
47.3
59.5
60
123
86.9
60.5
99.2
99.2
130
-m64
106
81.3
48.0
105
42.6
148
81.7
60.9
87.4
85.5
125
SFI
123
82.6
63.9
60.3
55.1
118
110
57.8
125
104
126
Table 4: SPEC2000 x86-64 execution times, in seconds.
164.gzip
175.vpr
176.gcc
181.mcf
186.crafty
197.parser
253.perlbmk
254.gap
255.vortex
256.bzip2
300.twolf
-m32
82
239
1868
20
286
243
746
955
643
98
375
-m64
85
244
2057
23
257
265
835
1015
620
95
410
SFI
155
350
3452
33
395
510
1404
1641
993
159
617
Table 5: SPEC2000 x86 text sizes, in kilobytes.
pect SFI code-size increase may be contributing to in-
struction cache pressure. From hardware performance
counter data, crafty shows a 26% increase in instruc-
tions retired and an increase in branch mispredicts from
2% to 8%, likely contributors to the observed SFI perfor-
mance overhead. We have also observed that perlbmk
and vortex are very sensitive to memcpy performance.
Our x86-64 experiments are using a relative simple im-
plementation of memcpy, to allow the same code to be
used with and without the SFI sandbox. In our continu-
ing work we are adapting a tuned memcpy implementa-
tion to work within our sandbox.
4.3
We suspected that the overhead of our SFI scheme would
be hidden in part by CPU microarchitectures that bet-
ter exploit instruction-level parallelism.
In particular,
we suspected we would be helped by the ability of out-
of-order CPUs to schedule around any bottlenecks that
SFI introduces. Fortunately, both architectures we tested
have multiple implementations, including recent prod-
ucts with in-order dispatch. To test our hypothesis, we