### 6.3 Toward a Video Quality Index

Our ultimate vision is to develop an empirical Internet video quality index, similar to the concept of mean opinion scores and subjective quality indices [8, 9]. Currently, with multiple quality metrics available, it is challenging for video providers and consumers to objectively compare different video services. Additionally, the lack of a concrete metric hinders delivery infrastructures and researchers from focusing their efforts effectively. A well-defined quality index would enable content providers and consumers to make informed choices about delivery services, while researchers and delivery providers could use it to guide the development of better algorithms for video delivery and adaptation.

However, as our measurements and insights demonstrate, the interactions between quality metrics and user engagement can be complex, interdependent, and counterintuitive, even in a simplified view involving three content types and five quality metrics. Furthermore, there are other dimensions that we have not rigorously explored in this paper. For example, we considered three broad genres of content: Live, Long VoD, and Short VoD. It would be interesting to analyze the impact of quality on other aspects of content segmentation. For instance, does popular content show more or less sensitivity to quality, and does the impact vary depending on the type of event or video (e.g., news vs. sports vs. sitcoms)? Our preliminary results indicate that the magnitude of quality impact is marginally higher for popular videos but largely independent of content genres. Similarly, there are other fine-grained quality measures that we have not yet explored. Anecdotal evidence suggests that temporal effects, such as buffering during the early stages or a sequence of buffering events, can significantly affect user satisfaction. Developing a unified quality index remains an active area of future research.

### 7. Related Work

#### Content Popularity
There is extensive literature on modeling content popularity and its implications for caching (e.g., [15, 18, 22, 24, 26]). Most of these studies focus on the heavy-tailed nature of access popularity distribution and its system-level impact. Our work, which analyzes the interplay between quality and engagement, is orthogonal to this literature. One interesting question, which we briefly address, is whether the impact of quality differs across different popularity segments. For example, providers may want to know if niche video content is more or less likely to be affected by poor quality.

#### User Behavior
Yu et al. present a measurement study of a VoD system deployed by China Telecom [24], focusing on modeling user arrival patterns and session lengths. They observe that many users have short session times, possibly because they "sample" a video and leave if it is not of interest. Removing the potential bias from this phenomenon was one of the motivations for our binned correlation analysis in Section 3. Other studies of user behaviors, such as channel switching dynamics in IPTV systems (e.g., [19]) and seek-pause-forward behaviors in streaming systems (e.g., [16]), also have significant implications for VoD system design. As mentioned in our browser minimization example for live video, understanding the impact of such behavior is critical for contextualizing measurement-driven insights.

#### P2P VoD
In parallel with reducing content delivery costs, there have been improvements in building robust P2P VoD systems that can provide performance comparable to server-side infrastructure at a fraction of the deployment cost (e.g., [14, 25, 26, 34]). These systems operate in dynamic environments (e.g., peer churn, low upload bandwidth), making it essential to optimize judiciously and improve the quality metrics that matter most. While our measurements are based on a server-hosted infrastructure for video delivery, the insights into the most critical quality metrics can also guide the design of P2P VoD systems.

#### Measurements of Deployed Video Delivery Systems
The networking community has benefited greatly from measurement studies of deployed VoD and streaming systems using both "black-box" inference (e.g., [12, 25, 33, 35]) and "white-box" measurements (e.g., [22, 27, 30, 37]). Our work follows this rich tradition of providing insights from real deployments to improve our understanding of Internet video delivery. We believe that we have taken a significant step forward in qualitatively and quantitatively measuring the impact of video quality on user engagement.

#### User Perceived Quality
Prior work in the multimedia literature has focused on metrics that capture user-perceived quality (e.g., [23, 38]) and how specific metrics affect the user experience (e.g., [20]). Our work differs in several key respects. First, the timing and scale: Internet video has only recently attained widespread adoption, making the re-evaluation of user engagement more relevant now than before. Prior work often relies on small-scale experiments with a few users, while our study is based on real-world measurements with millions of viewers. Second, previous studies fall short of linking perceived quality to actual user engagement. Finally, a key difference lies in methodology; while user studies and opinions are useful, they are difficult to evaluate objectively. Our work is an empirical study of engagement in the wild.

#### Engagement in Other Media
Understanding user engagement is a goal that appears in other content delivery mechanisms as well. The impact of page load times on user satisfaction is well-documented (e.g., [13, 21, 28]). Several commercial providers measure the impact of page load times on user satisfaction (e.g., [6]). Chen et al. study the impact of quality metrics such as bitrate, jitter, and delay on call duration in Skype [11] and propose a composite metric to quantify the combination of these factors. Given that Internet video has become mainstream only recently, our study provides similar insights into the impact of video quality on engagement.

#### Diagnosis
In this paper, we focused on measuring quality metrics and their impact on user engagement. A natural follow-up question is whether there are mechanisms to proactively diagnose quality issues to minimize their impact on users (e.g., [17, 31]). We leave this as a direction for future work.

### 8. Conclusions

As the costs of video content creation and dissemination continue to decrease, there is an abundance of video content on the Internet. In this context, it becomes crucial for content providers to understand how video quality impacts user engagement. Our study is a first step towards addressing this goal. We present a systematic analysis of the interplay between three dimensions of the problem space: quality metrics, content types, and quantitative measures of engagement. We study industry-standard quality metrics for Live, Long VoD, and Short VoD content to analyze engagement at both the per-view and viewer levels.

Our key findings are that, at the view level, the buffering ratio is the most important metric across all content genres, and the bitrate is especially critical for Live (sports) content. Additionally, we find that join time is critical for viewer-level engagement and thus likely to impact customer retention.

These results have significant implications from both commercial and technical perspectives. In a commercial context, they inform policy decisions for content providers to invest resources to maximize user engagement. From a technical perspective, they guide the design of technical solutions (e.g., trade-offs in buffer size) and motivate the need for new solutions (e.g., better proactive bitrate selection, rate switching, and buffering techniques).

In the course of our analysis, we also learned two cautionary lessons that broadly apply to measurement studies of this nature: the importance of using multiple complementary analysis techniques when dealing with large datasets and the importance of backing statistical techniques with system-level and user context. We believe our study is a significant step toward the ultimate vision of developing a unified quality index for Internet video.

### Acknowledgments

We thank our shepherd Ratul Mahajan and the anonymous reviewers for their feedback, which helped improve this paper. We also thank other members of the Conviva staff for supporting the data collection infrastructure and for patiently answering our questions regarding the player instrumentation and datasets.

### References

[1] Alexa Top Sites.
http://www.alexa.com/topsites/countries/US.
[2] Cisco forecast.
http://blogs.cisco.com/sp/comments/cisco_visual_networking_index_forecast_annual_update/.
[3] Driving Engagement for Online Video.
http://events.digitallyspeaking.com/akamai/mddec10/post.html?hash=ZDlBSGhsMXBidnJ3RXNWSW5mSE1HZz09.
[4] Hadoop.
http://hadoop.apache.org/.
[5] Hive.
http://hive.apache.org/.
[6] Keynote systems.
http://www.keynote.com.
[7] Mail service costs Netflix 20 times more than streaming.
http://www.techspot.com/news/42036-mail-service-costs-netflix-20-times-more-than-streaming.html.
[8] Mean opinion score for voice quality.
http://www.itu.int/rec/T-REC-P.800-199608-I/en.
[9] Subjective video quality assessment.
http://www.itu.int/rec/T-REC-P.910-200804-I/en.
[10] The tale of three blind men and an elephant.
http://en.wikipedia.org/wiki/Blind_men_and_an_elephant.
[11] K. Chen, C. Huang, P. Huang, C. Lei. Quantifying Skype User Satisfaction. In Proc. SIGCOMM, 2006.
[12] Phillipa Gill, Martin Arlitt, Zongpeng Li, Anirban Mahanti. YouTube Traffic Characterization: A View From the Edge. In Proc. IMC, 2007.
[13] A. Bouch, A. Kuchinsky, and N. Bhatti. Quality is in the Eye of the Beholder: Meeting Users’ Requirements for Internet Quality of Service. In Proc. CHI, 2000.
[14] B. Cheng, L. Stein, H. Jin, and Z. Zheng. Towards Cinematic Internet Video-On-Demand. In Proc. Eurosys, 2008.
[15] B. Cheng, X. Liu, Z. Zhang, and H. Jin. A measurement study of a peer-to-peer video-on-demand system. In Proc. IPTPS, 2007.
[16] C. Costa, I. Cunha, A. Borges, C. Ramos, M. Rocha, J. Almeida, and B. Ribeiro-Neto. Analyzing Client Interactivity in Streaming Media. In Proc. WWW, 2004.
[17] C. Wu, B. Li, and S. Zhao. Diagnosing Network-wide P2P Live Streaming Inefficiencies. In Proc. INFOCOM, 2009.
[18] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and S. Moon. I Tube, You Tube, Everybody Tubes: Analyzing the World’s Largest User Generated Content Video System. In Proc. IMC, 2007.
[19] M. Cha, P. Rodriguez, J. Crowcroft, S. Moon, and X. Amatriain. Watching Television Over an IP Network. In Proc. IMC, 2008.
[20] M. Claypool and J. Tanner. The effects of jitter on the perceptual quality of video. In Proc. ACM Multimedia, 1999.
[21] D. Galletta, R. Henry, S. McCoy, and P. Polak. Web Site Delays: How Tolerant are Users? Journal of the Association for Information Systems, (1), 2004.
[22] H. Y. et al. Inside the Bird’s Nest: Measurements of Large-Scale Live VoD from the 2008 Olympics. In Proc. IMC, 2009.
[23] S. R. Gulliver and G. Ghinea. Defining user perception of distributed multimedia quality. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP), 2(4), Nov. 2006.
[24] H. Yu, D. Zheng, B. Y. Zhao, and W. Zheng. Understanding User Behavior in Large-Scale Video-on-Demand Systems. In Proc. Eurosys, 2006.
[25] X. Hei, C. Liang, J. Liang, Y. Liu, and K. W. Ross. A measurement study of a large-scale P2P IPTV system. IEEE Transactions on Multimedia, 2007.
[26] Y. Huang, D.-M. C. Tom Z. J. Fu, J. C. S. Lui, and C. Huang. Challenges, Design and Analysis of a Large-scale P2P-VoD System. In Proc. SIGCOMM, 2008.
[27] W. W. Hyunseok Chang, Sugih Jamin. Live Streaming Performance of the Zattoo Network. In Proc. IMC, 2009.
[28] I. Ceaparu, J. Lazar, K. Bessiere, J. Robinson, and B. Shneiderman. Determining Causes and Severity of End-User Frustration. In International Journal of Human-Computer Interaction, 2004.
[29] K. Cho, K. Fukuda, H. Esaki. The Impact and Implications of the Growth in Residential User-to-User Traffic. In Proc. SIGCOMM, 2006.
[30] K. Sripanidkulchai, B. Maggs, and H. Zhang. An Analysis of Live Streaming Workloads on the Internet. In Proc. IMC, 2004.
[31] A. Mahimkar, Z. Ge, A. Shaikh, J. Wang, J. Yates, Y. Zhang, and Q. Zhao. Towards Automated Performance Diagnosis in a Large IPTV Network. In Proc. SIGCOMM, 2009.
[32] T. Mitchell. Machine Learning. McGraw-Hill.
[33] S. Ali, A. Mathur, and H. Zhang. Measurement of Commercial Peer-to-Peer Live Video Streaming. In Proc. Workshop on Recent Advances in Peer-to-Peer Streaming, 2006.
[34] S. Guha, S. Annapureddy, C. Gkantsidis, D. Gunawardena, and P. Rodriguez. Is High-Quality VoD Feasible using P2P Swarming? In Proc. WWW, 2007.
[35] S. Saroiu, K. P. Gummadi, R. J. Dunn, S. D. Gribble, and H. M. Levy. An Analysis of Internet Content Delivery Systems. In Proc. OSDI, 2002.
[36] H. A. Simon. Designing Organizations for an Information-Rich World. Martin Greenberger, Computers, Communication, and the Public Interest, The Johns Hopkins Press.
[37] K. Sripanidkulchai, A. Ganjam, B. Maggs, and H. Zhang. The Feasibility of Supporting Large-Scale Live Streaming Applications with Dynamic Application End-Points. In Proc. SIGCOMM, 2004.
[38] K.-C. Yang, C. C. Guest, K. El-Maleh, and P. K. Das. Perceptual Temporal Quality Metric for Compressed Video. IEEE Transactions on Multimedia, Nov. 2007.