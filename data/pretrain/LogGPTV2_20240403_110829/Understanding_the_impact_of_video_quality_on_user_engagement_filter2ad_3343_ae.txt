the statistical analysis with a more in-depth domain knowledge and
controlled experiments in replicating the observations.
6.3 Toward a video quality index
Our ultimate vision is to use measurement-driven insights to de-
velop an empirical Internet video quality index, analogous to the
notion of mean opinion scores and subjective quality indices [8, 9].
Given that there are multiple quality metrics, it is difﬁcult for video
providers and consumers to objectively compare different video
services. At the same time, the lack of a concrete metric makes
it difﬁcult for delivery infrastructures and researchers to focus their
efforts. If we can derive such a quality index, content providers and
consumers can use it to choose delivery services while researchers
and delivery providers can use it to guide their efforts for develop-
ing better algorithms for video delivery and adaptation.
However, as our measurements and lessons show the interactions
between the quality metrics and engagement can be complex, in-
terdependent, and counterintuitive even for a somewhat simpliﬁed
view with just three content types and ﬁve quality metrics. Further-
more, there are other dimensions that we have not explored rigor-
ously in this paper. For example, we considered three broad genres
of content: Live, Long VoD, and Short VoD. It would also be inter-
esting to analyze the impact of quality for other aspects of content
segmentation. For example, is popular content more/less likely to
be impacted by quality or is the impact likely to differ depending
on the types of events/videos (e.g., news vs. sports vs. sitcoms)?
Our preliminary results in these directions show that the magni-
tude of quality impact is marginally higher for popular videos but
largely independent of content genres. Similarly, there are other
ﬁne-grained quality measures which we have not explored. For ex-
ample, anecdotal evidence suggests that temporal effects can play
a signiﬁcant role; buffering during the early stages or a sequence of
buffering events are more likely to lead to user frustration. Working
toward such a uniﬁed quality index is an active direction of future
research.
7. RELATED WORK
Content popularity: There is an extensive literature on model-
ing content popularity and its subsequent implications for caching
(e.g., [15, 18, 22, 24, 26]). Most of these focus on the heavy-tailed
nature of the access popularity distribution and its system-level im-
pact. Our work on analyzing the interplay between quality and
engagement is orthogonal to this extensive literature. One interest-
ing question (that we address brieﬂy) is to analyze if the impact of
quality is different across different popularity segments. For exam-
ple, providers may want to know if niche video content is more or
less likely to be impacted by poor quality.
User behavior: Yu et al. present a measurement study of a VoD
system deployed by China Telecom [24] focusing on modeling user
arrival patterns and session lengths. They also observe that many
users actually have small session times, possibly because many
users just “sample” a video and leave if the video is of no inter-
est. Removing the potential bias from this phenomenon was one
of the motivations for our binned correlation analysis in Section 3.
Other studies of user behaviors also have signiﬁcant implications
for VoD system design. For example, there are measurement stud-
ies of channel switching dynamics in IPTV systems (e.g., [19]),
and understanding seek-pause-forward behaviors in streaming sys-
tems (e.g., [16]). As we mentioned in our browser minimization
example for live video, understanding the impact of such behavior
is critical for putting the measurement-driven insights in context.
P2P VoD:
In parallel to the reduction of content delivery costs,
there have also been improvements in building robust P2P VoD
systems that can provide performance comparable to a server-side
infrastructure at a fraction of the deployment cost (e.g., [14, 25,
26, 34]). Because these systems operate in more dynamic environ-
ments (e.g., peer churn, low upload bandwidth), it is critical for
them to optimize judiciously and improve the quality metrics that
really matter. While our measurements are based on a server-hosted
infrastructure for video delivery, the insights in understanding the
most critical quality metrics can also be used to guide the design of
P2P VoD systems.
Measurements of deployed video delivery systems: The net-
working community has beneﬁted immensely from measurement
studies of deployed VoD and streaming systems using both “black-
box” inference (e.g., [12, 25, 33, 35]) and “white-box” measure-
ments (e.g., [22,27,30,37]). Our work follows in this rich tradition
of providing insights from real deployments to improve our under-
standing of Internet video delivery. At the same time, we believe
that we have taken a signiﬁcant step forward in qualitatively and
quantitatively measuring the impact of the video quality on user
engagement.
User perceived quality: There is prior work in the multime-
dia literature on metrics that can capture user perceived quality
(e.g., [23, 38]) and how speciﬁc metrics affect the user experience
(e.g., [20]). Our work differs on several key fronts. The ﬁrst is sim-
ply an issue of timing and scale. Internet video has only recently at-
tained widespread adoption and revisiting user engagement is ever
more relevant now than before. Prior work depend on small-scale
experiments with a few users, while our study is based on real-
world measurements with millions of viewers. Second, these fall
short of linking the perceived quality to the actual user engagement.
Finally, a key difference is with respect to methodology; user stud-
ies and opinions are no doubt useful, but difﬁcult to objectively
evaluate. Our work is an empirical study of engagement in the
wild.
Engagement in other media: The goal of understanding user en-
gagement appears in other content delivery mechanisms as well.
The impact of page load times on user satisfaction is well known
(e.g., [13, 21, 28]). Several commercial providers measure the im-
pact of page load times on user satisfaction (e.g., [6]). Chen et al.
study the impact of quality metrics such as bitrate, jitter, and delay
on call duration in Skype [11] and propose a composite metric to
quantify the combination of these factors. Given that Internet video
has become mainstream only recently, our study provides similar
insights for the impact of video quality on engagement.
Diagnosis:
In this paper, we focused on measuring the quality
metrics and how they impact user engagement. A natural follow up
question is whether there are mechanisms to pro-actively diagnose
quality issues to minimize the impact on users (e.g., [17, 31]). We
leave this as a direction for future work.
8. CONCLUSIONS
As the costs of video content creation and dissemination con-
tinue to decrease, there is an abundance of video content on the In-
ternet. Given this setting, it becomes critical for content providers
to understand if and how video quality is likely to impact user en-
gagement. Our study is a ﬁrst step towards addressing this goal.
We present a systematic analysis of the interplay between three
dimensions of the problem space: quality metrics, content types,
and quantitative measures of engagement. We study industry-standard
quality metrics for Live, Long VoD, and Short VoD content to ana-
lyze engagement at per view and viewer-level.
Our key takeaways are that at the view-level, buffering ratio is
the most important metric across all content genres and the bitrate
is especially critical for Live (sports) content. Additionally, we ﬁnd
that the join time becomes critical in terms of the viewer-level en-
gagement and thus likely to impact customer retention.
These results have key implications both from commercial and
technical perspectives. In a commercial context, they inform the
policy decisions for content providers to invest their resources to
maximize user engagement. At the same time, from a technical
perspective, they also guide the design of the technical solutions
(e.g., tradeoffs in the choice of a suitable buffer size) and motivate
the need for new solutions (e.g., better pro-active bitrate selection,
rate switching, and buffering techniques).
In the course of our analysis, we also learned two cautionary
lessons that more broadly apply to measurement studies of this
nature: the importance of using multiple complementary analysis
techniques when dealing with large datasets and the importance of
backing these statistical techniques with system-level and user con-
text. We believe our study is a signiﬁcant step toward an ultimate
vision of developing a uniﬁed quality index for Internet video.
Acknowledgments
We thank our shepherd Ratul Mahajan and the anonymous review-
ers for their feedback that helped improve this paper. We also thank
other members of the Conviva staff for supporting the data collec-
tion infrastructure and for patiently answering our questions regard-
ing the player instrumentation and datasets.
9. REFERENCES
[1] Alexa Top Sites.
http://www.alexa.com/topsites/countries/US.
[2] Cisco forecast. http://blogs.cisco.com/sp/comments/cisco_
visual_networking_index_forecast_annual_update/.
[3] Driving Engagement for Online Video.
http://events.digitallyspeaking.com/akamai/mddec10/
post.html?hash=ZDlBSGhsMXBidnJ3RXNWSW5mSE1HZz09.
[4] Hadoop. http://hadoop.apache.org/.
[5] Hive. http://hive.apache.org/.
[6] Keynote systems. http://www.keynote.com.
[7] Mail service costs Netﬂix 20 times more than streaming.
http://www.techspot.com/news/42036-mail-service-costs-netﬂix-20-times-
more-than-streaming.html.
[8] Mean opinion score for voice quality.
http://www.itu.int/rec/T-REC-P.800-199608-I/en.
[9] Subjective video quality assessment.
http://www.itu.int/rec/T-REC-P.910-200804-I/en.
[10] The tale of three blind men and an elephant. http:
//en.wikipedia.org/wiki/Blind_men_and_an_elephant.
[11] K. Chen, C. Huang, P. Huang, C. Lei. Quantifying Skype User Satisfaction. In
Proc. SIGCOMM, 2006.
[12] Phillipa Gill, Martin Arlitt, Zongpeng Li, Anirban Mahanti. YouTube Trafﬁc
Characterization: A View From the Edge. In Proc. IMC, 2007.
[13] A. Bouch, A. Kuchinsky, and N. Bhatti. Quality is in the Eye of the Beholder:
Meeting Users’ Requirements for Internet Quality of Service. In Proc. CHI,
2000.
[14] B. Cheng, L. Stein, H. Jin, and Z. Zheng. Towards Cinematic Internet
Video-On-Demand. In Proc. Eurosys, 2008.
[15] B. Cheng, X. Liu, Z. Zhang, and H. Jin. A measurement study of a peer-to-peer
video-on-demand system. In Proc. IPTPS, 2007.
[16] C. Costa, I. Cunha, A. Borges, C. Ramos, M. Rocha, J. Almeida, and B.
Ribeiro-Neto. Analyzing Client Interactivity in Streaming Media. In Proc.
WWW, 2004.
[17] C. Wu, B. Li, and S. Zhao. Diagnosing Network-wide P2P Live Streaming
Inefﬁciencies. In Proc. INFOCOM, 2009.
[18] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and S. Moon. I Tube, You Tube,
Everybody Tubes: Analyzing the World’s Largest User Generated Content
Video System. In Proc. IMC, 2007.
[19] M. Cha, P. Rodriguez, J. Crowcroft, S. Moon, and X. Amatriain. Watching
Television Over an IP Network. In Proc. IMC, 2008.
[20] M. Claypool and J. Tanner. The effects of jitter on the peceptual quality of
video. In Proc. ACM Multimedia, 1999.
[21] D. Galletta, R. Henry, S. McCoy, and P. Polak. Web Site Delays: How Tolerant
are Users? Journal of the Association for Information Systems, (1), 2004.
[22] H. Y. et al. Inside the Bird’s Nest: Measurements of Large-Scale Live VoD from
the 2008 Olympics. In Proc. IMC, 2009.
[23] S. R. Gulliver and G. Ghinea. Deﬁning user perception of distributed
multimedia quality. ACM Transactions on Multimedia Computing,
Communications, and Applications (TOMCCAP), 2(4), Nov. 2006.
[24] H. Yu, D. Zheng, B. Y. Zhao, and W. Zheng. Understanding User Behavior in
Large-Scale Video-on-Demand Systems. In Proc. Eurosys, 2006.
[25] X. Hei, C. Liang, J. Liang, Y. Liu, and K. W. Ross. A measurement study of a
large-scale P2P IPTV system. IEEE Transactions on Multimedia, 2007.
[26] Y. Huang, D.-M. C. Tom Z. J. Fu, J. C. S. Lui, and C. Huang. Challenges,
Design and Analysis of a Large-scale P2P-VoD System. In Proc. SIGCOMM,
2008.
[27] W. W. Hyunseok Chang, Sugih Jamin. Live Streaming Performance of the
Zattoo Network. In Proc. IMC, 2009.
[28] I. Ceaparu, J. Lazar, K. Bessiere, J. Robinson, and B. Shneiderman.
Determining Causes and Severity of End-User Frustration. In International
Journal of Human-Computer Interaction, 2004.
[29] K. Cho, K. Fukuda, H. Esaki. The Impact and Implications of the Growth in
Residential User-to-User Trafﬁc. In Proc. SIGCOMM, 2006.
[30] K. Sripanidkulchai, B. Maggs, and H. Zhang. An Analysis of Live Streaming
Workloads on the Internet. In Proc. IMC, 2004.
[31] A. Mahimkar, Z. Ge, A. Shaikh, J. Wang, J. Yates, Y. Zhang, and Q. Zhao.
Towards Automated Performance Diagnosis in a Large IPTV Network. In Proc.
SIGCOMM, 2009.
[32] T. Mitchell. Machine Learning. McGraw-Hill.
[33] S. Ali, A. Mathur, and H. Zhang. Measurement of Commercial Peer-to-Peer
Live Video Streaming. In Proc. Workshop on Recent Advances in Peer-to-Peer
Streaming, 2006.
[34] S. Guha, S. Annapureddy, C. Gkantsidis, D. Gunawardena, and P. Rodriguez. Is
High-Quality VoD Feasible using P2P Swarming? In Proc. WWW, 2007.
[35] S. Saroiu, K. P. Gummadi, R. J. Dunn, S. D. Gribble, and H. M. Levy. An
Analysis of Internet Content Delivery Systems. In Proc. OSDI, 2002.
[36] H. A. Simon. Designing Organizations for an Information-Rich World. Martin
Greenberger, Computers, Communication, and the Public Interest, The Johns
Hopkins Press.
[37] K. Sripanidkulchai, A. Ganjam, B. Maggs, and H. Zhang. The Feasibility of
Supporting Large-Scale Live Streaming Applications with Dynamic
Application End-Points. In Proc. SIGCOMM, 2004.
[38] K.-C. Yang, C. C. Guest, K. El-Maleh, and P. K. Das. Perceptual Temporal
Quality Metric for Compressed Video. IEEE Transactions on Multimedia, Nov.
2007.