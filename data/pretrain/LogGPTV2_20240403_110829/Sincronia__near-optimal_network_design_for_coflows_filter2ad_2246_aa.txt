title:Sincronia: near-optimal network design for coflows
author:Saksham Agarwal and
Shijin Rajakrishnan and
Akshay Narayan and
Rachit Agarwal and
David B. Shmoys and
Amin Vahdat
Sincronia: Near-Optimal Network Design for Coflows
Saksham Agarwal∗
Cornell University
Rachit Agarwal
Cornell University
Shijin Rajakrishnan∗
Cornell University
David Shmoys
Cornell University
Akshay Narayan
MIT
Amin Vahdat
Google
ABSTRACT
We present Sincronia, a near-optimal network design for
coflows that can be implemented on top on any transport
layer (for flows) that supports priority scheduling. Sincronia
achieves this using a key technical result — we show that
given a “right” ordering of coflows, any per-flow rate alloca-
tion mechanism achieves average coflow completion time
within 4× of the optimal as long as (co)flows are prioritized
with respect to the ordering.
Sincronia uses a simple greedy mechanism to periodically
order all unfinished coflows; each host sets priorities for its
flows using corresponding coflow order and offloads the flow
scheduling and rate allocation to the underlying priority-
enabled transport layer. We evaluate Sincronia over a real
testbed comprising 16-servers and commodity switches, and
using simulations across a variety of workloads. Evaluation
results suggest that Sincronia not only admits a practical,
near-optimal design but also improves upon state-of-the-art
network designs for coflows (sometimes by as much as 8×).
CCS CONCEPTS
• Networks → Network protocol design; • Theory of
computation → Scheduling algorithms;
KEYWORDS
Coflow, Datacenter Networks, Approximation Algorithms
∗The first two authors contributed equally to the paper.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 978-1-4503-5567-4/18/08...$15.00
https://doi.org/10.1145/3230543.3230569
ACM Reference Format:
Saksham Agarwal, Shijin Rajakrishnan, Akshay Narayan, Rachit
Agarwal, David Shmoys, and Amin Vahdat. 2018. Sincronia: Near-
Optimal Network Design for Coflows. In SIGCOMM ’18: ACM SIG-
COMM 2018 Conference, August 20–25, 2018, Budapest, Hungary.
ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/
3230543.3230569
1 INTRODUCTION
Traditionally, networks have used the abstraction of a “flow”,
that captures a sequence of packets between a single source
and a single destination. This abstraction has been a main-
stay for decades and for a good reason — network designs
were optimized for latency and/or throughput for a point-
to-point connection, precisely the performance metrics im-
portant to traditional applications (e.g., file transfers, web ac-
cess, etc.). However, distributed applications running across
datacenter networks use programming models (e.g., bulk
synchronous programming and partition-aggregate model)
that require optimizing performance for a collection of flows
rather than individual flows. The network still optimizes the
performance of individual flows, leading to a fundamental
mismatch between performance objectives of applications
and the optimization objectives of network designs.
The coflow abstraction [7] mitigates this mismatch, allow-
ing distributed applications to more precisely express their
performance objectives to the network fabric. For instance,
many distributed services with stringent performance con-
straints must essentially block until receiving all or almost
all responses from hundreds or even thousands of remote
servers (§2). Such services can specify a collection of flows
as a coflow. The network fabric now optimizes for average
Coflow Completion Time (CCT) [7, 10, 12], where the CCT
of a coflow is defined as the time when some percentage,
perhaps 100%, of flows in the coflow finish. Several recent
evaluations show that optimizing for average CCT can sig-
nificantly improve application-level performance [7, 10, 12].
There has been tremendous recent effort on network de-
signs for coflows, both in networking [7–10, 12] and in the-
ory community [3, 19, 22]. However, prior designs require
a centralized coordinator to perform complex per-flow rate
allocations, with rate allocated to a flow being dependent
on the rate allocated to other flows in the network. Such
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Agarwal et al.
centralized inter-dependent per-flow rate allocation make it
hard to realize these designs in practice for several reasons.
First, per-flow rate allocation naturally requires knowledge
about location of congestion in the network and paths taken
by each (co)flow, making it hard to use these designs when
congestion is in the fabric core and/or changes dynamically.
Second, since rates allocated to flows are correlated, arrival
or departure of even one coflow may result in reallocation of
rate for each and every flow in the network. Such realloca-
tion is impractical in datacenters where thousands of coflows
may arrive each second. As a result, a practical near-optimal
network design for coflows still remains elusive.
This paper presents Sincronia, a new datacenter network
design for coflows that achieves near-optimal average CCT
without any explicit per-flow rate allocation mechanism. The
high-level design of Sincronia can be summarized as:
• Time is divided into epochs;
• In each epoch, a subset of unfinished coflows are selected
• Each host independently sets a priority for its flows (based
on the corresponding coflow’s ordering), and offloads the
flow to underlying priority-enabled transport mechanism;
• Coflows that arrive between epoch boundaries are greedily
and “ordered” using a simple greedy algorithm;
scheduled for work conservation.
Sincronia’s minimalistic design is based on a key technical
result — with a “right” ordering of coflows, it is possible to
achieve average CCT within 4× of the optimal1 as long as
(co)flow scheduling is “order-preserving” — if coflow C is
ordered higher than coflow C′, flows/packets in C must be
prioritized over those in C′. From a practical perspective, this
result is interesting because it shows that as long as (co)flow
scheduling is order-preserving, any per-flow rate allocation
mechanism results in average CCT within 4× of the optimal.
Using this result, Sincronia admits a practical, near-optimal
network design for coflows (§3) — a simple greedy algorithm
periodically orders the set of unfinished coflows; each host,
without any explicit coordination with other hosts, sets prior-
ities for its flows based on corresponding coflow’s ordering,
and offloads scheduling of and rate allocation to individual
flows to the underlying priority-enabled transport layer.
Sincronia thus overcomes the aforementioned practical
challenges in existing network designs for coflows by avoid-
ing per-flow rate allocation and by being agnostic to the
underlying transport layer. First, by avoiding per-flow rate
1Under the standard assumption that the fabric core can sustain 100%
throughput. That is, only the ingress and the egress access links are po-
tential bottlenecks. This is the same assumption made in the big switch
model for traditional abstraction of flows [4, 13, 15]. For the case when the
fabric core cannot sustain 100% throughput, no approximation algorithm
is known even for the traditional abstraction of flows. While we use this
assumption for our theoretical bounds, our implementation makes no such
assumption and adapts well to in-network congestion.
allocation, Sincronia design is independent of underlying
network topology, location of congestion in the fabric, and
paths taken by each (co)flow. This also allows Sincronia to
transparently respond to network failures. Second, coflow ar-
rivals and departures do not require explicit rate reallocation
for existing flows, leading to a much more scalable design.
Third, Sincronia design using simple priority mechanisms
enables coexistence of flows and coflows (§4), supporting
backward compatibility. Finally, by being transport-agnostic,
Sincronia admits efficient implementation on top of any ex-
isting transport mechanism that supports priority scheduling
including TCP (using DiffServ [6] for priority scheduling),
pHost [13] and pFabric [4]. We discuss implementation of
Sincronia on top of several transport mechanisms in §4.3.
We have implemented Sincronia on top of TCP, with Diff-
Serv [6] for priority scheduling. Our implementation is work
conserving, efficiently handles online arrival of coflows, and
allows coexistence of flows and coflows. We evaluate Sincro-
nia implementation on a 16-server testbed interconnected
with a FatTree topology comprising 20 commodity switches.
We have also incorporated Sincronia into existing coflow
simulators; we use these to perform sensitivity analysis of
Sincronia performance against variety of workloads, number
of coflows, network load, transport mechanisms, etc. Our
implementation and simulation results show that Sincronia
not only provides near-optimal average CCT but also out-
performs state-of-the-art network designs for coflows across
all evaluated workloads (sometimes by as much as 8×).
2 SINCRONIA OVERVIEW
In this section, we briefly recall the coflow abstraction (§2.1)
and formally define our optimization objective for network
design for coflows (§2.2). We also review some of the known
results in coflow scheduling in §2.2.
2.1 The Coflow Abstraction
Existing distributed programming frameworks [7, 9, 11, 17,
20, 21, 26, 27] often have a communication stage that is struc-
tured and takes place between successive computation stages.
In these frameworks, execution of a task (or even an entire
computation stage) cannot begin until all flows in the pre-
ceding communication stage have finished. A coflow [7, 10]
is a collection of such flows, with a shared performance goal
(e.g., minimizing the completion time of the last flow in a
coflow). Figure 1 shows an example.
We assume that coflows are defined such that flows within
a coflow are independent; that is, the input of a flow does
not depend on the output of another flow within that coflow.
Similar to most existing designs [10, 12, 14, 19], we focus
on a clairvoyant design that assumes information about a
coflow (set of flows, and corresponding sources, destinations
and sizes) is known at coflow’s arrival time but no earlier.
Sincronia: Near-Optimal Network Design for Coflows
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Ingress Ports
Egress Ports
Table 1: Notation used in the paper.
2 + ε
1
1
1
2 + ε
1
1
2 + ε
1
2 + ε
1
1
1
2
3
4
5
6
7
8
DC Fabric
Figure 1: An instance of a coflow scheduling problem, used
as a running example in the paper. The datacenter has 4 ingress
and egress ports, with each ingress port having a “virtual output
queue” for each egress port (see §2.2 for detailed model descrip-
tion). The example has 5 coflows. Coflow C1 has eight flows, with
each ingress port sending unit amount of data to two egress ports;
coflows C2, C3, C4 and C5 have one flow each sending 2 + ε amount
of data to one of the egress ports (the ε amount is only for breaking
ties consistently). Coflow C2, C3, C4 and C5 being single flow coflow
is only for simplicity; the ε amount of flow could be sent to any
egress port without changing the results in §3.
2.2 Problem Statement and Prior Results
We now describe the network model used for our theoretical
bounds, and the network performance objective.
Conceptual Model (for theoretical bounds). Similar to
near-optimal network designs for traditional abstraction of
network flows [4, 13, 15] and coflows [3, 10, 12, 14, 19, 22],
we will abstract out the datacenter network fabric as one big
switch that interconnects the servers. In such a big switch
model, the ingress queues correspond to the NICs and the
egress queues to the last-hop TOR switches. The model as-
sumes that the fabric core can sustain 100% throughput and
only the ingress and egress queues are potential congestion
points. Under this model, each ingress port has flows from
one or more coflows to various egress ports (see Figure 1).
For ease of exposition, we organize the flows in virtual out-
put queues at the ingress ports. We use this abstraction to
simplify our theoretical analysis and algorithmic description,
but we do not enforce it in our design and experiments.
Performance Objective. Formally, we assume that the net-
work is a big switch comprising m ingress ports {1, 2, . . . , m}
and m egress ports {m +1, m +2, . . . , 2m}. Unless mentioned
otherwise, all ports have the same bandwidth. We are given
a collection of n coflows C = {1, 2, . . . , n}, indexed using c.
Each coflow c may be assigned a weight wc (default weight
is 1), has an arrival time ac and comprises a set of flows Fc.
wc
dij
c