## IO
```mermaid
mindmap
  IO 性能指标
    磁盘
      使用率
      IOPS
      吞吐量
      响应时间
      缓冲区
      相关因素
        读写类型（如顺序还是随机）
        读写比例
        读写大小
        存储类型（如RAID级别、本地还是网络）
    文件系统
      存储空间容量、使用量以及剩余空间
      索引节点容量、使用量以及剩余量
      缓存
        页缓存
        目录项缓存
        索引节点缓存
        具体文件系统缓存（如ext4的缓存）
      IOPS(文件IO)
      响应时间（延迟）
      吞吐量(B/s)
```
一些优化手段：
- 应用层
  - 追加写代替随机写
  - 缓存 I/O
  - 自建应用程序内部缓存
  - mmap代替read/write
  - 合并写请求： 在同步写场景中，尽量将写请求合并，可用fsync()替代O_SYNC，提高效率。
  - 使用cgroups的I/O子系统
  - ionice调整I/O调度优先级： 在使用CFQ调度器时，使用ionice提高核心应用的I/O优先级
- 文件系统层
  - 选择适合负载场景的文件系统：如 xfs 支持更大的磁盘分区和文件数量，但无法收缩
  - 优化文件系统配置选项： 在选定文件系统后，进一步优化其配置选项，包括文件系统特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）
  - 优化文件系统缓存： 如优化 pdflush 脏页的刷新频率（dirty_expire_centisecs 和 dirty_writeback_centisecs）、脏页的限额（dirty_background_ratio 和 dirty_ratio），以及内核回收目录项缓存和索引节点缓存的倾向（vfs_cache_pressure）
  - 使用内存文件系统 tmpfs
- 磁盘层
  - 使用性能更好的磁盘
  - 使用RAID
  - 选择适合的I/O调度算法： SSD和虚拟机中的磁盘通常使用 noop 调度算法，而数据库应用更适合使用 deadline 算法
  - 进行磁盘级别的隔离： 为I/O压力重的应用配置单独的磁盘，提高性能。
  - 增大磁盘的预读数据： 在顺序读较多的场景中，增大磁盘的预读数据，可以通过调整内核选项或使用 blockdev 工具设置来实现。
  - 优化内核块设备I/O选项： 调整内核块设备I/O的选项，例如调整磁盘队列的长度，适当增大队列长度可以提升磁盘的吞吐量，但可能导致I/O延迟增大。
  - 注意硬件错误： 硬件错误可能导致磁盘性能急剧下降
### 容量
Linux 通过 inode 记录文件的元数据
可以通过 df -i 查看 inode 所占用的空间
### 缓存
内核使用 Slab 机制，管理目录项和 inode 的缓存，可以通过 /proc/slabinfo 查看这个缓存
### IO栈
由上到下分为三个层次，分别是：
- 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口对下会通过通用块层，来存储和管理磁盘数据
- 通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通过重新排序和请求合并，然后才要发送给下一级的设备层
  - NONE 调度算法：不使用任何 I/O 调度器，对文件系统和应用程序的 I/O 其实不做任何处理，常用在虚拟机中
  - NOOP：是一个先入先出的队列，只做一些最基本的请求合并，常用于 SSD 磁盘
  - CFQ：完全公平调度器，为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求
  - DeadLine：别为读、写请求创建了不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限（deadline）的请求被优先处理，多用在 I/O 压力比较重的场景，比如数据库
- 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作
![](/assets/2024125192531.webp)
### 磁盘性能指标
- 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈
- 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求
- IOPS（Input/Output Per Second），是指每秒的 I/O 请求数
- 吞吐量，是指每秒的 I/O 请求大小
- 响应时间，是指 I/O 请求从发出到收到响应的间隔时间
在数据库、大量小文件等这类随机读写比较多的场景中，IOPS 更能反映系统的整体性能；而在多媒体等顺序读写较多的场景中，吞吐量才更能反映系统的整体性能
## 网络
**网络包的接收流程**：当一个网络帧到达网卡后，网卡会通过 DMA 方式，把这个网络包放到收包队列中，通过硬中断告诉中断处理程序已经收到了网络包，接着，网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中；然后再通过软中断，通知内核收到了新的网络帧，内核协议栈从缓冲区中取出网络帧，并通过网络协议栈，从下到上逐层处理这个网络帧
**网络包发送流程**：内核协议栈处理过的网络包被放到发包队列后，会有软中断通知驱动程序发包队列中有新的网络帧需要发送。驱动程序通过 DMA ，从发包队列中读出网络帧，并通过物理网卡把它发送出去
- 收包队列与发包队列的内存都属于网卡设备驱动的范围
- sk_buff 缓冲区，是一个维护网络帧结构的双向链表，链表中的每一个元素都是一个网络帧
- 还有一个是 socket 的读写缓冲区
### 性能指标
通过 ifconfig 可以看到网卡网络收发的字节数、包数、错误数以及丢包情况：
- errors 表示发生错误的数据包数，比如校验错误、帧同步错误等
- dropped 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer，但因为内存不足等原因丢包
- overruns 表示超限数据包数，即网络 I/O 速度过快，导致 Ring Buffer 中的数据包来不及处理（队列满）而导致的丢包
- carrier 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等
- collisions 表示碰撞数据包数
netstat 可以看到套接字的接收队列（Recv-Q）和发送队列（Send-Q）
当套接字处于连接状态（Established）时，Recv-Q 表示套接字缓冲还没有被应用程序取走的字节数（即接收队列长度）。而 Send-Q 表示还没有被远端主机确认的字节数（即发送队列长度）。
当套接字处于监听状态（Listening）时，Recv-Q 表示全连接队列的长度。而 Send-Q 表示全连接队列的最大长度。全连接是指服务器收到了客户端的 ACK，完成了 TCP 三次握手，然后就会把这个连接挪到全连接队列中，半连接是指还没有完成 TCP 三次握手的连接
`netstat -s` 可以查看协议栈各层的统计信息
### 工作模型
主进程 + 多个 worker 子进程：主进程执行 bind() + listen() 后，创建多个子进程；然后在每个子进程中，都通过 accept() 或 epoll_wait() ，来处理相同的套接字
监听到相同端口的多进程模型：所有的进程都监听相同的接口，并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中去
### NAT
NAT 基于 Linux 内核的 conntrack 连接跟踪机制来实现，Linux 内核需要为 NAT 维护状态，维护状态也带来了很高的性能成本
### 网络层
路由和转发的角度
- 在需要转发的服务器中，比如用作 NAT 网关的服务器，开启 IP 转发，即设置 net.ipv4.ip_forward = 1
- 调整数据包的生存周期 TTL，设置 net.ipv4.ip_default_ttl = 64。增大该值会降低系统性能
- 开启数据包的反向地址校验，设置 net.ipv4.conf.eth0.rp_filter = 1。可以防止 IP 欺骗，并减少伪造 IP 带来的 DDoS 问题
调整 MTU 大小：很多网络设备都支持巨帧，可以把 MTU 调大为 9000，提升网络吞吐量
为了避免 ICMP 主机探测、ICMP Flood 等各种网络问题：可以禁止 ICMP 协议，即设置 net.ipv4.icmp_echo_ignore_all = 1，还可以禁止广播 ICMP，即设置 net.ipv4.icmp_echo_ignore_broadcasts = 1
### 链路层
由于网卡收包后调用的中断处理程序（特别是软中断），需要消耗大量的 CPU。所以，将这些中断处理程序调度到不同的 CPU 上执行，就可以显著提高网络吞吐量
- 为网卡硬中断配置 CPU 亲和性（smp_affinity），或者开启 irqbalance 服务
- 开启 RPS（Receive Packet Steering）和 RFS（Receive Flow Steering），将应用程序和软中断的处理，调度到相同 CPU 上，这样就可以增加 CPU 缓存命中率
将原先内核中通过软件处理的功能卸载到网卡中，通过硬件来执行：
- TSO（TCP Segmentation Offload）和 UFO（UDP Fragmentation Offload）：在 TCP/UDP 协议中直接发送大包；而 TCP 包的分段（按照 MSS 分段）和 UDP 的分片（按照 MTU 分片）功能，由网卡来完成 
- GSO（Generic Segmentation Offload）：在网卡不支持 TSO/UFO 时，将 TCP/UDP 包的分段，延迟到进入网卡前再执行。这样，不仅可以减少 CPU 的消耗，还可以在发生丢包时只重传分段后的包
- LRO（Large Receive Offload）：在接收 TCP 分段包时，由网卡将其组装合并后，再交给上层网络处理。不过要注意，在需要 IP 转发的情况下，不能开启 LRO，因为如果多个包的头部信息不一致，LRO 合并会导致网络包的校验错误
- GRO（Generic Receive Offload）：GRO 修复了 LRO 的缺陷，并且更为通用，同时支持 TCP 和 UDP
- RSS（Receive Side Scaling）：也称为多队列接收，它基于硬件的多个接收队列，来分配网络接收进程，这样可以让多个 CPU 来处理接收到的网络包
- VXLAN 卸载：也就是让网卡来完成 VXLAN 的组包功能
对于网络接口本身：
- 开启网络接口的多队列功能，调度到不同 CPU 上执行，从而提升网络的吞吐量
- 增大网络接口的缓冲区大小，以及队列长度等，提升网络传输的吞吐量
- 使用 Traffic Control 工具，为不同网络流量配置 QoS
## 内核线程
Linux 在启动过程中 2 号进程为 kthreadd 进程，在内核态运行，用来管理内核线程
```sh
ps -f --ppid 2 -p 2
```
- kswapd0：用于内存回收
- kworker：用于执行内核工作队列，分为绑定 CPU （名称格式为 kworker/CPU:ID）和未绑定 CPU（名称格式为 kworker/uPOOL:ID）两类
- migration：在负载均衡过程中，把进程迁移到 CPU 上。每个 CPU 都有一个 migration 内核线程
- jbd2/sda1-8：jbd 是 Journaling Block Device 的缩写，用来为文件系统提供日志功能，以保证数据的完整性；名称中的 sda1-8，表示磁盘分区名称和设备号。每个使用了 ext4 文件系统的磁盘分区，都会有一个 jbd2 内核线程
- pdflush：用于将内存中的脏页（被修改过，但还未写入磁盘的文件页）写入磁盘（已经在 3.10 中合并入了 kworker 中）