title:Deep Entity Classification: Abusive Account Detection for Online Social
Networks
author:Teng Xu and
Gerard Goossen and
Huseyin Kerem Cevahir and
Sara Khodeir and
Yingyezhe Jin and
Frank Li and
Shawn Shan and
Sagar Patel and
David Freeman and
Paul Pearce
Deep Entity Classification: Abusive Account 
Detection for Online Social Networks
Teng Xu, Gerard Goossen, Huseyin Kerem Cevahir, Sara Khodeir, and 
Yingyezhe Jin, Facebook, Inc; Frank Li, Facebook, Inc, and Georgia Institute of 
Technology; Shawn Shan, Facebook, Inc, and University of Chicago; Sagar Patel 
and David Freeman, Facebook, Inc; Paul Pearce, Facebook, Inc, and 
Georgia Institute of Technology
https://www.usenix.org/conference/usenixsecurity21/presentation/xu-teng
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Deep Entity Classiﬁcation: Abusive Account Detection for Online Social Networks
Teng Xu
Gerard Goossen
Huseyin Kerem Cevahir
Sara Khodeir
Yingyezhe Jin
Frank Li
Shawn Shan
Sagar Patel
David Freeman
Paul Pearce
Facebook, Inc
University of Chicago
Georgia Institute of Technology
Abstract
Online social networks (OSNs) attract attackers that use
abusive accounts to conduct malicious activities for economic,
political, and personal gain. In response, OSNs often deploy
abusive account classiﬁers using machine learning (ML) ap-
proaches. However, a practical, effective ML-based defense
requires carefully engineering features that are robust to ad-
versarial manipulation, obtaining enough ground truth labeled
data for model training, and designing a system that can scale
to all active accounts on an OSN (potentially in the billions).
To address these challenges we present Deep Entity Classiﬁ-
cation (DEC), an ML framework that detects abusive accounts
in OSNs that have evaded other, traditional abuse detection
systems. We leverage the insight that while accounts in isola-
tion may be difﬁcult to classify, their embeddings in the social
graph—the network structure, properties, and behaviors of
themselves and those around them—are fundamentally dif-
ﬁcult for attackers to replicate or manipulate at scale. Our
system:
• Extracts “deep features” of accounts by aggregating prop-
erties and behavioral features from their direct and indirect
neighbors in the social graph.
• Employs a “multi-stage multi-task learning” (MS-MTL)
paradigm that leverages imprecise ground truth data by
consuming, in separate stages, both a small number of high-
precision human-labeled samples and a large amount of
lower-precision automated labels. This architecture results
in a single model that provides high-precision classiﬁcation
for multiple types of abusive accounts.
• Scales to billions of users through various sampling and
reclassiﬁcation strategies that reduce system load.
DEC has been deployed at Facebook, where it classiﬁes all
users continuously, resulting in an estimated reduction of
abusive accounts on the network by 27% beyond those already
detected by other, traditional methods.
1 Introduction
Online Social Networks (OSNs) connect billions of users
around the globe. The largest social network, Facebook, has
more than two billion active users sharing content each
month [45]. The vast scale of these networks in turn attracts
adversaries that seek to exploit the platforms for economic,
political, and personal gain. While most OSN activity comes
from legitimate users, attackers invest signiﬁcant resources in
signing up fake accounts (i.e., accounts not representative of
a real person), creating accounts that impersonate real people,
or compromising the accounts of real users. These abusive
accounts are used to drive a range of negative behaviors in-
cluding spam, fake engagement, pornography, violence, and
terrorism—all actions which violate community norms [12]
and are widely studied forms of abuse [1].
A core challenge faced by OSNs is how to identify and
remediate abusive accounts in such a way that is both scalable
and precise. Scalability requires approaches that can operate
on billions of users and tens of billions of daily actions to
detect dozens of different abuse types. Systems that prioritize
precision are necessary because abusive accounts are rela-
tively rare [44, 45] and thus a drop in precision would lead
to the OSN taking errant actions against a large number of
benign users.
OSNs use a broad set of techniques ranging from rule-
based heuristics [49] to modern machine-learning algo-
rithms [26, 48] to classify and remediate abusive accounts
at scale. Rule-based heuristics act as a ﬁrst line of defense [4],
identifying basic or common attacker tools, techniques, and
resources. These heuristics however lack power: they focus
on precision rather than recall, they often do not capture the
complexity of account behaviors, and they are by deﬁnition
reactive [25]. Machine learning systems overcome some of
these problems: they generalize from past labeled data in or-
der to improve recall, and they can be iterated on over time
to adapt to adversarial evolution [8]. However, precise ma-
chine learning systems require a large amount of high-quality
labeled ground truth data, can be costly to deploy (in both
engineering effort and computational resources), and can be
evaded by adversaries who learn how to mimic the appear-
ance of real accounts [17]. Rule-based heuristics and tradi-
tional machine learning systems can identify and remediate
USENIX Association
30th USENIX Security Symposium    4097
the vast majority of abuse [4], but identifying the remaining
hard-to-classify accounts—those that closely resemble real
users and/or evade OSN defenses—requires fundamentally
different and more complex solutions.
A critical insight is that while attackers can produce abusive
accounts that appear legitimate in isolation, those accounts’
embedding in and engagement with the social graph are fun-
damentally difﬁcult to forge. For example, the number of
friend requests sent by a given user is easy for an attacker
to control, but the number of friend requests sent by all of
that user’s friends is outside of the attacker’s control.1 Al-
though attackers can attempt to camouﬂage their accounts by
connecting to legitimate nodes in the graph, this strategy not
only is prohibitive to implement at scale, but also creates side
effects (e.g., large numbers of rejected friend requests) that
are detectable by traditional means.
Leveraging this insight, we develop Deep Entity Classi-
ﬁcation (DEC),2 a method and supporting system for OSN
abusive account detection. Instead of classifying accounts
based on “direct” features and behaviors, DEC leverages so-
cial network structure, extracting more than 20,000 features
for each account, by operating across the graph. These fea-
tures are used to train supervised machine learning models
that classify accounts across many different kinds of abuse.
The DEC system consists of label generation and feature ex-
traction, as well as model training, deployment, and updating.
Ultimately DEC produces per-account abusive classiﬁcation
results that are robust to adversarial iteration (Section 7).
The large number of features generated by DEC’s graph
traversal imposes two challenges in terms of model training.
First, if applied naïvely, the large feature space could dramati-
cally increase the underlying model complexity, resulting in
poor generalization and degraded performance. Second, ob-
taining proper generalization across so many features would
require a prohibitively large training set in a problem space
where high-quality human-labeled data is difﬁcult to obtain
at billion-user scale.
The second key DEC insight is that in addition to small-
scale, high-quality human-labeled data, we can utilize the
results of rule-based heuristics as additional “approximate
labels.” The classiﬁcations from such rules are not human
reviewed and thus have lower precision than human-reviewed
data, but the absolute quantity is much higher.
Building on this insight, we design a “multi-stage multi-
task learning” (MS-MTL) framework. Our framework ex-
tracts low-dimensional transferable representations via a deep
neural network trained using the high-volume approximate
labels, then ﬁne-tunes dedicated models given the learned
representations and the high-quality human-labeled data.
Model training occurs in two separate stages. The ﬁrst
1See Section 8.4 for consideration of the case where attacker creates
groups of abusive accounts that are connected to each other.
2 In this context “deep” refers to the features generated via network fanout
from each account, not neural network structure.
stage trains a multi-task deep neural network [6] on the col-
lected features using the large number of lower-precision
approximate labels. Since accounts identiﬁed by these lower-
precision signals exhibit a multitude of different abuse types
(e.g., spam, objectionable content, or malware), we formulate
a learning “task” for each abuse type. We then extract the
penultimate layer of the neural network as a low-dimensional
feature vector [22]. This vector is input to the second stage
of the model, which is trained using per-task high-precision
human-labeled data with a standard binary classiﬁer.
MS-MTL allows DEC to learn the underlying common
representations of different abuse types in the ﬁrst model
stage, and then to distinguish different abuse types using high-
precision data with separate models in the second stage, re-
sulting in a score for each abuse type for each account. In this
way we can use a single model to label as “abusive” accounts
exhibiting any of a multitude of abuse types (e.g., scams,
spam, adult content, etc.).
Our DEC design is deployed at Facebook, where it has
run in production for more than two years. During that time
DEC led to the identiﬁcation and remediation of hundreds
of millions of abusive accounts. By comparing the number
of accounts actioned by DEC with an unbiased estimate of
the number of abusive accounts remaining on the platform,
we infer that DEC is responsible for reducing the volume of
abusive accounts by approximately 27%.
In summary, our contributions include:
• The algorithmic design, system architecture, and imple-
mentation of DEC. Extracting more than 20,000 features
per entity, across multiple hops, for billions of active users,
presents a unique set of systems challenges (Section 4).
• A novel feature extraction process that produces “deep
features” (Section 5) that, over our evaluation, showed no
signs of adversarial adaptation (Section 7.4).
• The MS-MTL classiﬁcation paradigm, which allows us to
use a single model architecture to produce high-precision
classiﬁers for each abuse class (Section 6).
• A quantitative evaluation of DEC and MS-MTL vs. other
approaches, as well as a qualitative assessment of the im-
pact DEC has had on the overall state of abusive accounts
not caught by other systems (i.e., those hardest to classify)
at Facebook (Section 7).
• A discussion of the lessons learned from two years of pro-
duction deployment at Facebook (Section 8).
2 Background
Here we present an overview of abusive accounts on OSNs,
existing defenses, and relevant machine learning terminology.
2.1 Abusive Accounts
We deﬁne an abusive account to be any account that violates
the written policies of a given OSN (e.g., [12]). Attackers use
abusive accounts for various reasons, including for ﬁnancially
motivated schemes (e.g., spreading spam, scams, objection-
4098    30th USENIX Security Symposium
USENIX Association
able content, or phishing links [13–15]) and for causing user
harm (e.g., online harassment or terrorism [16]). Abusive
accounts can be broadly broken down along two dimensions:
1. Account Provenance. An abusive account can be fake,
where the account does not represent an actual person or
organization, or real, where it is a legitimate user account,
though potentially hijacked by an attacker.3
2. Abusive Behavior. An abusive account can be character-
ized by the type of abuse it conducts, such as spreading
scams or spam.
2.2 Defenses
There are multiple types of defenses against abusive accounts
on OSNs. Rule-based heuristics, such as rate limits on par-
ticular user actions, are straightforward, easy to design and
evaluate, and can be quite powerful in practice. However, they
are often reactive, permitting some amount of abuse before a
threshold is crossed and a rule is triggered. In addition, they
conservatively focus on precision rather than recall to avoid
false positives.
Another large-scale detection technique is machine
learning-based classiﬁcation, which affords increased com-
plexity of the detection algorithm through digesting more
features. However, adversaries can adapt (sometimes quickly)
in response to classiﬁer actions [10], making it challenging
to properly design features that are difﬁcult for adversaries to
discover and evade. Another challenge of this approach is to
collect enough high-precision training data. Human labeling
is typically the most reliable source but can be expensive in
terms of time, money, and human effort.
Rule-based heuristics and typical machine-learning based
classiﬁers are able to identify the vast majority of abusive
activity in online services [4]. Identifying those accounts that
are able to evade the primary detection systems presents a
especially difﬁcult challenge, as they represent the hardest to
classify accounts. For example, such accounts may be those
that adversaries have iterated on while adapting to OSN de-
fenses, or they may very closely resemble real users. The
system we present in this paper is designed to mitigate these
issues by employing sparse aggregated features on the social
graph that should be difﬁcult for attackers to manipulate, and
by using a multi-stage training framework.
2.3 Machine Learning Terminology
In this section we describe the machine learning terminology
relevant to DEC.
2.3.1 Deep Neural Networks
The ﬁrst stage of DEC uses a deep neural network (DNN)
architecture [31]. It is a cascade of multiple layers of nonlinear
processing units for feature extraction and transformation.
3Real user accounts that violate OSN policies without having been com-
promised are outside the scope of this work, as they are relatively small in
volume and are actioned on by other systems.
Each successive layer uses the output from the previous layer
as input. In deep learning, each layer learns to transform
its input data into a slightly more abstract and composite
representation, with the last layer outputting a single score.
2.3.2 Embeddings
In the context of neural networks, embeddings are low-
dimensional, continuous, learned vector representations of
a discrete feature vector. Neural network embeddings are use-
ful because they can reduce the dimensionality of categorical
variables and meaningfully represent categories in the trans-
formed space [28]. A common usage of embeddings is to
serve as input features for machine learning models. In each
layer of a deep neural network, a low-dimensional vector can
be extracted as the embedding of the layer.
2.3.3 Gradient Boosted Decision Trees
The embedding of the last layer of deep neural network in
DEC’s ﬁrst stage is used as the input feature vector for the
second stage of DEC training, which uses a model of gradi-
ent boosted decision trees (GBDTs). GBDTs are a machine
learning approach that iteratively constructs an ensemble of
weak decision tree learners through boosting. It is a widely
used algorithm in classiﬁcation and regression [20].
3 Related Work
The problem of detecting abusive accounts in OSNs has re-
ceived a great deal of attention in the literature. We split the
published efforts into three categories based on technique,
and also describe the relevant machine learning literature.
3.1 Detecting Abusive Accounts
Several works have explored using graph structure and the
features of neighboring nodes to detect abuse. Yang et al.
examined the effectiveness of graph and neighbor-based fea-
tures to identify spammers on Twitter [58]. Their work formal-
ized 24 detection features—including four graph-based and
three direct neighbor properties—showing how these features
could identify spammers better than prior state-of-the-art so-