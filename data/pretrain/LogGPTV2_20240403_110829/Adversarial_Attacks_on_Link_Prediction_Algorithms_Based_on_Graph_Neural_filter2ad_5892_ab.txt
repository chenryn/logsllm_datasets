of two nodes then can be predicted by applying the trained SEAL
model.
For clarity, letâ€™s see a running example as illustrated in Fig. 1. In
this example, the SEAL framework learns the â€˜heuristicâ€™ function
using 1-hop enclosing subgraphs. The learned heuristic may contain
information about its graph structure features, such as the number
Figure 1: The SEAL framework: learning graph structure fea-
tures from 1-hop local enclosing subgraphs: (ğ´, ğµ) and (ğ¶, ğ·)
are links with labels and regarded as training links.
of common neighbours, Jaccard, and Katz index, etc. (ğ´, ğµ) and
(ğ¶, ğ·) are the links with labels and regarded as training links.
Essentially, the node information matrix ğ‘‹ contains information
about each node, including the structural node labels, embeddings,
or node attributes. As the structural node label is used to mark
the different roles (topological structure) of nodes in an enclosing
graph, it is a kind of graph structural feature. By incorporating the
node information matrix, SEAL can learn the mapping function
from its graph structure and node attributes. Our work focuses on
mounting adversarial attacks on link prediction algorithms based
on graph neural network, particularly targeting the link prediction
algorithm in SEAL. We assume that the link prediction model is
trained with graph data that is clean and attack-free.
2.4 Attack Transferability
Existing work in the literature on adversarial machine learning
demonstrated that adversarial examples produced to mislead a spe-
cific model are highly likely to mislead other models; such property
is referred to as transferability. A practical impact of this property
is that it leads to oracle-based black-box attacks. More specifically,
the adversary is able to use the target model as an oracle to label a
synthetic training set for the surrogate, so the adversary need not
even observe the full data to mount the attack [21, 26].
The transferability of adversarial machine learning has been
extensively studied in the literature [26, 27, 29]. In this paper, the
definition of transferability is more general and not only limited
among machine learning models. Note that, we aim to offer a com-
prehensive algorithmic investigation of the problem of attacking
link prediction algorithms based on graph neural networks. For
this purpose, we focus on evasion attacks against a GNN-based
framework, called SEAL, which is proposed based on a Î¥-decaying
heuristic theory. Regarding the Î¥-decaying heuristic framework,
we can envision that the mounted attacks may be transferred to
the heuristics.
To illustrate the effectiveness of our attacks, we will empirically
analyze their transferability to existing heuristics in Sec. 5.
3 PROBLEM FORMULATION
In this section, we will describe our threat model and explain our at-
tacks as modifications to a graph. In practice, the adversary changes
the graph based on the underlying data that are explored for the
Enclosing subgraph extractionLearn graph structure features:common neighboursJaccardKatzâ€¦c=1ABCDCDABGraph Neural Network c=0as the defender does to evaluate the effectiveness of their attacks.
Ideally, this data would be well guarded, making this level of knowl-
edge only realistic for the most sophisticated adversaries. Never-
theless, considering the damage that could be done by a perfectly
knowledgeable adversary is important as a security evaluation,
since it allows us to find potential weaknesses in link prediction
models.
Our attack architecture is shown as Fig. 2. During the testing
phase, a testing link (ğ¸, ğ¹) is predicted as a negative link, while with
a perturbation (adding an edge denoted as a blue link in Fig. 2), it is
predicted as a positive link. Since ğ‘‹ contains information about a
node, including node structural labels, manipulating perturbations
on the graph structure (adding or deleting edges) would lead to
changes in both ğ´ and ğ‘‹ (coupled variables while performing per-
turbations). Without loss of generality, attacks induced by adding
or deleting edges are referred to as graph structure attacks. Directly
manipulating the target link is easy to be detected; thus we also
assume that the adversary would not add or delete an edge between
the target nodes. To summarize, as the inputs of the prediction
model are (ğ´, ğ‘‹) tuples representing the enclosing subgraph of two
given target nodes ğ‘¥ and ğ‘¦, perturbations causing changes on ğ´
and ğ‘‹ may lead to an incorrect prediction of the target link ğ‘’(ğ‘¥, ğ‘¦).
3.3 Unnoticeability Constraint
In typical application domains, a successful adversarial example is
crafted under some simple constraints to ensure its unnoticeability.
For example, in the image recognition domain, the perturbation con-
straint is measured by the distance (ğ‘™0, ğ‘™1, ğ‘™2, ğ‘™âˆ-norm, etc.) between
the adversarial example and its normal example. Its effectiveness
can be easily verified by human vision [8, 11]. However, in a com-
plex network graph, manipulating the input data to fool its learning
model is much harder.
To quantitatively evaluate unnoticeability, we use the perturba-
tion constraint measured by ğ‘™1-norm distance of the graph adja-
cency matrices before and after perturbations. It can be formulated
as:
|ğ´ âˆ’ ğ´â€²| â‰¤ Î”
(2)
where ğ´, ğ´â€² are the adjacency matrices of the subgraphs before and
after perturbations. It sets the maximum bound that the adversary
can change the graph, and with this constraint it is more likely to
satisfy the unnoticeability constraint.
Instead of verifying by human vision, we employ the graph
property preservation technique to ensure its unnoticeable pertur-
bations, which has been discussed by Zugner et al. [37]. Precisely,
we use degree distribution preservation to ensure unnoticeable
perturbations in the graph â€” likelihood ratio test for the power-law
degree distribution of the two graphs [37]. The intuition is that two
highly similar graphs would follow similar power-law behaviour
regarding their degree distributions. According to [37], the graph
structure perturbations Gâ€² = (Vâ€², E,Xâ€²) can be accepted only
when the degree distribution satisfies:
Î›(G(0), Gâ€²)  0)
or the maximum perturbation constraint Î”.
As we mentioned above, the variables (ğ´â€², ğ‘‹ â€²) that are used
to optimize the loss function, regarding the structure attack, are
dependent. Thus, the typical approaches of using gradient-based
search for each perturbation are not applicable in our case. To solve
Eq. (4) with dependent variables, the most intuitive way to construct
ğ‘†ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ is to employ a heuristic search under its unnoticeability
constraint (see line 5 and 8 in Alg. 1).
According to the Î¥-decaying heuristic theory [34], given two
target nodes, their â„-hop enclosing subgraphs are very informa-
tive for link prediction, which means the perturbations are likely
feasible when they lead to changes on its subgraph; â„ = 1 or 2
is typically sufficient for accurate link prediction. Hence, we only
have to inspect the optimal perturbations that can make changes
to the â„-hop subgraph. In other words, at least one end of the edge
added/deleted should be included in the enclosing subgraph to be a
possible feasible perturbation. Precisely, one end of the perturbed
edge should be included in its set of (â„ âˆ’ 1)-hop nodes, denoted as
{Î“â„âˆ’1(ğ‘¥) âˆª Î“â„âˆ’1(ğ‘¦)} (see Alg. 1).
Letâ€™s see an example with â„ = 2 as shown in Fig. 3. ğ‘¥, ğ‘¦ are the
target nodes and the link in between is requested for link existence
prediction. As shown in Fig. 3, only when one end of edge added/
deleted (the blue lines) is included in their 1-hop node set (1-hop
neighbours of ğ‘¥/ ğ‘¦), the perturbations can lead to changes to the
subgraph. However, the ends of the perturbed edges (the red lines)
are not in their 1-hop node set (not 1-hop neighbours of ğ‘¥/ ğ‘¦), could
not change the subgraph.
The search time complexity in Alg. 1 would be O(|ğ‘‰ğ‘ |Ã—(|Î“â„âˆ’1(ğ‘¥)|+
|Î“â„âˆ’1(ğ‘¦)|)), where |ğ‘‰ğ‘ | is the number of nodes that the adversary
is able to manipulate and it can reach ğ‘ as the capability of the
adversary increases. With rapidly growing volumes of data, the size
of the graph (ğ‘ ) is typically very large; for example, there were
Even we only consider the search space from its â„-hop subgraph,
the perturbation search time cost is still very high. Can we further
improve our attack efficiency? The answer is affirmative.
4.2 Optimized Graph Structure Perturbation
Inspired by the intuition of link formation mechanism â€” the more
common neighbours two target nodes have, the more likely they are
connected â€” we construct ğ‘†ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ based on the common neighbours
that the target nodes share. In this context, a common neighbour is
defined as the intersectional neighbours of the two target nodes
within their â„-hop subgraphs.
We consider two different kinds of attacks when constructing
ğ‘†ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡. On the one hand, to force a positive link to be a negative
link (link hidden), we delete edges to reduce the number of com-
mon neighbours in the subgraph (see line 5-10 in Alg. 2). On the
other hand, to encourage a negative link to become a positive link,
we add edges to force more nodes to become the common neigh-
bours of the target nodes. Precisely, we first consider the nodes
in the â„-hop subgraph but are not common neighbours. For these