10 20 30 40 50
Iterations
r
o
r
r
e
e
v
i
t
a
l
e
r
g
v
A
−2
−3
−4
−5
−6
−7
−8
0
10
10
10
10
10
10
10
20
24
28
10 20 30 40 50
Iterations
(a) Fixed point
(b) Floating point
Fig. 13: Relative accuracy of the secure PageRank algorithm
(input
length 2048 entries) compared to the execution in
the clear using ﬁxed-point and ﬂoating-point garbled-circuits
implementations.
TABLE III: Summary of machines used in large-scale experi-
ment, performing matrix factorization over the MovieLens 1M
ratings dataset.
Machine
1
2
3
3
4
5
6
7
Total
Processors
16
16
6
6
15
15
27
27
128
Type
Garbler
Evaluator
Garbler
Evaluator
Garbler
Evaluator
Garbler
Evaluator
JVM Memory Size
64 GB
60.8 GB
24 GB
24 GB
58.5 GB
58.5 GB
113.4 GB
121.5 GB
524.7 GB
Num Ratings
256K
256K
96K
96K
240K
240K
432K
432K
1M
version in the clear for ﬁxed-point and ﬂoating-point numbers,
respectively. Overall, the error is relatively small, especially
when using at least 24 bits for the fraction part in ﬁxed-point
or for the precision in ﬂoating-point. For example, running 10
iterations of PageRank with 24 bits for the fractional part in
−5 compared
ﬁxed-point representation results in an error of 10
to running in the clear. The error increases with more iterations
since the precision error accumulates.
F. Running at Scale
In order to have a full-scale experiment of our system,
we ran matrix factorization using gradient descent on the
real-world MovieLens dataset that contains 1 million ratings
provided by 6040 users to 3883 movies [73]. We factorized
the matrix to users and movie feature vectors, each vector with
a dimension of 10. We used 40-bit ﬁxed-point representation
for reals, with 20 bits reserved for the fractional part. We ran
the experiment on an heterogeneous set of machines that we
have in the lab. Table III summarizes the machines and the
allocation of data across them.
A single iteration of gradient descent took roughly 13
hours to run on 7 machines with 128 processors, at ~104
MB data size (i.e., 1M entries). As prior machine learning
literature reports [74], [75], about 20 iterations are necessary
for convergence for the same MovieLens dataset – which
would take about 11 days with 128 processors. In practice,
this means that the recommendation system can be retrained
every 11 days. As mentioned earlier, about 20× speedup is
immediately attainable by switching to a JustGarble-like back
end implementation with hardware AES-NI, and assuming
2700 Mbps bandwidth between each garbler-evaluator pair.
One can also speed up the execution by provisioning more
processors.
390390
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:55 UTC from IEEE Xplore.  Restrictions apply. 
TABLE IV: Comparison with a naive circuit-level paralleliza-
tion approach, assuming inﬁnite number of processors (using
Histogram).
Input length
Circuit Depth of GraphSC
Circuit Depth of SCVM [2]
11
12
13
14
15
16
17
18
19
20
2
2
2
2
2
2
2
2
2
2
267
322
385
453
527
608
695
788
888
994
7 M
18 M
43 M
104 M
247 M
576 M
1328 M
3039 M
6900 M
15558 M
In comparison, as far as we know, the closest large-scale
experiment in running secure matrix factorization was recently
performed by Nikolaenko et al. [3]. The authors used 16K
ratings and 32 processors to factorize a matrix (on a machine
similar to machine 7 in Table III), taking almost 3 hours to
complete. The authors could not scale further because their
framework runs on a single machine.
G. Comparison with Naïve Parallelization
An alternative approach to achieve parallelization is to
use a naive circuit-level parallelization without requiring the
developer to write code in a parallel programming paradigm.
We want
to assess the speedup that we can obtain using
GraphSC over using such naïve parallelization. The results
in this section are computed analytically and assume inﬁnite
number of processors.
that
In order to compare, we consider the simple histogram
application and compute the depth of the circuit
is
generated using GraphSC, and the one using the state-of-
the-art SCVM [2] compiler. The depth is an indicator for
the ability to parallelize – each “layer” in the circuit can
be parallelized, but consecutive layers must be executed in
sequence. Thus, the shallower the circuit is the more it is
amendable to parallelization. The latter uses RAM-model
secure computation and compiles a program into a sequence
of ORAM accesses. We assume that for ORAM accesses,
the compiler uses the state-of-the-art Circuit ORAM [53].
Due to the sequential nature of ORAM constructions, these
ORAM accesses cannot be easily parallelized using circuit-
level parallelism (currently only OPRAM can achieve full
circuit-level parallelism, however,
these results are mostly
theoretical and prohibitive in practice). Table IV shows the
circuit depth obtained using the two techniques. As the table
suggests, GraphSC yields signiﬁcantly shallower and “wider”
circuits, implying that it can be parallelized much more than
the naïve circuit-level parallelization techniques that are long
and “narrow”.
H. Performance Proﬁling
Finally, we perform micro-benchmarks to better understand
the time the applications spend in the different parts of the
computation and network transmissions. Figure 14 shows the
breakdown of the overall execution between various operations
for PageRank and gradient descent. Figure 15 shows a similar
breakdown for different input sizes. As the plots show, the
garbler is computation-intensive whereas the evaluator spends
)
c
e
s
(
e
m
T
i
120
90
60
30
0
OT I/O
OT CPU
G-G I/O
G-E I/O
Garble CPU
4
16
8
Processors
32
)
c
e
s
(
e
m
T
i
120
90
60
30
0
OT I/O
OT CPU
E-E I/O
G-E I/O
Eval CPU
4
16
8
Processors
32
(a) PageRank: Garbler
)
c
e
s
(
e
m
T
i
1200
900
600
300
0
OT I/O
OT CPU
G-G I/O
G-E I/O
Garble CPU
)
c
e
s
(
e
m
T
i
4
16
8
Processors
32
(b) PageRank: Evaluator
1200
900
600
300
0
OT I/O
OT CPU
E-E I/O
G-E I/O
Eval CPU
4
16
8
Processors
32
(c) Gradient Descent: Garbler
(d) Gradient Descent: Evaluator
Fig. 14: A breakdown of the execution times of the garbler
and evaluator running one iteration of PageRank and gradient
descent for an input size of 2048 entries Here I/O overhead
means the time a processor spends blocking on I/O. The
remaining time is reported as CPU time.
)
c
e
s
(
e
m
T
i
200
150
100
50
0
210
OT I/O
OT CPU
G-G I/O
G-E I/O
Garble CPU
212
211
Input length
213
(a) PageRank: Garbler
)
c
e
s
(
e
m
T
i
400
300
200
100
0
28
OT I/O
OT CPU
G-G I/O
G-E I/O
Garble CPU
)
c
e
s
(
e
m
T
i
29
210
211
Input length
)
c
e
s
(
e
m
T
i
200
150
100
50
0
210
OT I/O
OT CPU
E-E I/O
G-E I/O
Eval CPU
212
211
Input length
213
OT I/O
OT CPU
E-E I/O
G-E I/O
Eval CPU
(b) PageRank: Evaluator
400
300