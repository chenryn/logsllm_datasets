Mostly Automatic polls for new updates.
See Figure 1 for query rate.
Manual downloads. Close to SP2 release (6 Aug’04).
SP2 patched a large number of ﬁles (>4K) and included 12K+
deltas. Aggregate size for all deltas equals 266MB.
Mostly Automatic downloads.
Regular monthly patches for small set of ﬁles (mostly critical up-
dates).
Facilitates the study of delta production and publishing. Includes
delta creation time, size, hash key, version, etc.
Figure 2: Overview of the update process. A client checks ﬁrst
with the update servers for new patches, and then, if it needs to get
updated, downloads the patches from the distribution servers.
design of a patch delivery system. In Sec. 3.1 and 3.2 we examine
the relationships between the updates for individual ﬁles with the
objective to identify clusters of ﬁles that are updated together. If
efﬁcient clustering is possible, then, the number of possible states
that need to be covered by a patching system can be signiﬁcantly
reduced. This can result in simpler, more efﬁcient and scalable de-
livery system since fewer distribution channels are required (e.g.
multicast channels, etc) and user requests are spread across fewer
ﬁles which translates into higher cache efﬁciencies.
A ﬁle patch describes the differences between the version that
exists in the user computer and the newest version. When a soft-
ware update requires changes in ﬁles that already exist in the user’s
computer, it is more efﬁcient to send ﬁle patches instead of the ﬁles
themselves. Patches may be signiﬁcantly smaller than the corre-
sponding ﬁles and, hence, transmitting patches results in signiﬁ-
cant bandwidth savings at the server and improved update delivery
times for the user, especially for modem users. Using Set-II and
Set-IV we have determined that the mean size of a ﬁle distributed
during XP SP2 is 73.2 KBytes, while the mean patch size is 32.9
KBytes. We have also measured that for 30% of the ﬁles, patches
provide savings of at least 5x. On the other hand, the use of patches
increases the diversity of the user population since the users differ
not only in the components they wish to update, but also in the spe-
ciﬁc patch, which depends on the version of their ﬁles, they need
to obtain.
In Fig. 3 we plot the cumulative number of released patches and
new ﬁles as a function of their release time. Observe the large num-
ber of new ﬁles and deltas in three time instances that correspond
to the release of the XP operating system and the releases of two
major service packs (SP1 and SP2). Observe also the release of
many updates in between the SPs with an average rate of 21 new
patches per month.
Figure 3: Set of patches included in SP2 and their releases times.
Major jumps correspond to the release of XP-RTM (Aug 01), SP1
(Sep 02), and SP2 (Aug 04).
3.1 Clustering of ﬁles
We will now try to identify clusters of ﬁles that are requested to-
gether (in the next section we will repeat the same exercise for
patches). To this extent, we have used traces of requests for the
Windows XP SP2 distribution and the full history of updates in the
XP source tree (data Set-II and Set-IV). In order to accurately de-
termine the clustering relationship across ﬁles, we ﬁrst ﬁlter out
those clients for which we do not see a complete patching session.
Filtered entries corresponded mostly to slow (e.g. modem) users,
which required several sessions to fully patch. Filtering out those
users did not bias our clustering results as our goal is indeed to
sample users for which we see a complete patching session over
the observation interval.
To quantify the set of ﬁles that are clustered together, we do the
following analysis. Let xi be the binary vector of user requests for
ﬁle i; i.e., xi(u) = 1 iff user u requested ﬁle i. We compute the
cosine correlation ρi,j between any pair of ﬁles i and j as:
ρi,j =
i · xi ·q
pxT
i · xj
xT
j · xj
xT
Files that are always requested together have a correlation ρi,j = 1.
We assume that ﬁles i and j are correlated if ρi,j > 0.9. We then
construct a graph of the ﬁles as follows. Each node represents a ﬁle
and there is an edge between two ﬁles if the correlation between
the ﬁles is greater than ρi,j > 0.9. We then identify the connected
components of that graph. Observe that, even though our method
does not guarantee that every two ﬁles assigned to the same com-
ponent are related to each other, we have observed that in every
component the minimum correlation between each pair of ﬁles is
large (more than 0.8).
In total, our data set had requests for 2029 ﬁles. From our analy-
sis we were able to identify 26 non-overlapping groups accounting
for 2003 ﬁles; only 26 ﬁles could not be assigned to a group (see
Fig. 4). This represents an important clustering effect. The groups
Jan01Jan02Jan03Jan040200400600800100012001400160018002000Release DatesatleD/seliF desaeleR fo rebmuN .mmuCFigure 4: Groups of ﬁles identiﬁed in the SP2 download. Dark dots
appear whenever two ﬁles are highly correlated. Group boundaries
appear as squares.
correlate with functional components of the operating system; a
typical example of a group is the set of ﬁles related to the network-
ing functionality.
In Fig. 5(a) we plot in more detail the sizes of the groups. The
largest group, which includes 1460 ﬁles, was requested by all users.
This group corresponds to updates of core components of the OS,
as well as components that were introduced in SP2 for the ﬁrst
time. The other groups included 200 ﬁles or less. Some of these
groups correspond to rare conﬁgurations, such as language speciﬁc
ﬁles, and received few requests. Observe that the clustering of ﬁles
can signiﬁcantly reduce the complexity of the system; indeed, by
publishing groups of ﬁles instead of individual ﬁles, the number
of publishing elements reduces by two orders of magnitude (from
2029 to 26).
We now quantify the beneﬁts of publishing only a few major
groups (e.g. over a few multicast trees) and distribute the remain-
ing ﬁles individually. Fig. 5(c) shows the percentage of requests
that can be satisﬁed by publishing the largest k groups, for k =
1, . . . , 26. Note that the 5 largest groups account for 1877 of the
ﬁles and are responsible for more than 97% of the requests, which
indicates that the distribution a few groups can satisfy a very large
number of requests. It is outside of the scope of this paper to de-
scribe ways to automate cluster discovery. However, our analysis
demonstrates that clustering of patches has a large potential group-
ing in reducing the complexity of a patch distribution system.
3.2 Clustering of patches
We have repeated the same analysis as above using as input the
patch requests. In total, our data set had 3379 patches. In Fig.5(b)
we plot the sizes of the groups of patches (the groups are ordered
by their size). We have identiﬁed 125 groups that account for 3188
patches; 191 patches could not be assigned to a group. The num-
ber of groups is again signiﬁcantly smaller than the total number of
patches. However, compared to ﬁle grouping, grouping of patches
provides a smaller aggregation factor. This is due to the larger num-
ber of possible user conﬁgurations that arise when considering in-
dividual patches; users differ not only in the component they need
to update, but also in the patch they need to download to update
their local version. From Fig. 5(c), we also observe that to sat-
isfy the same number of requests, the required number of groups of
patches is larger than the required number of groups of ﬁles. Thus,
the user population covered by each group of patches is smaller.
This analysis indicates that publishing individual deltas instead
of ﬁles decreases the clustering efﬁciency, consequently increas-
ing the complexity of the system.
Ideally, software update sys-
tems should publish individual ﬁles rather than deltas, and generate
patches on-the-ﬂy using chunck-based hash techniques (e.g. simi-
Figure 6: The rate of distinct IPs observed over three days, starting from
Jan 4, 2006, 8:00 UTC. The majority of distinct IPs are observed during day
1. Peaks are observed at around 14:00 UTC and 24:00 UTC. The break-
down of IP counts over continents reveals that the ﬁrst peak is due to North
American users and the second due to Asian ones.
lar to LBFS [17]). The main challenge to produce deltas on-the-ﬂy
is, however, the required computation capacity at the servers. To
scale such computation, one can use distributed systems such as
P2P networks that support on-the-ﬂy generation of updates; deltas
can be generated automatically from updated peers, rather than
from the server. We will study the potential of such P2P patching
systems in Section 5.
4. USERS CHARACTERIZATION
In this section, we examine the intrinsic properties of update trafﬁc
and user behavior with respect to software updates.
4.1 Trafﬁc Properties
We now characterize the arrival patterns of update queries. Queries
arrive from two types of machines: always-online-machines (AOM)
and non-AOM. We deﬁne AOM as those machines that have an ac-
tive automatic update service that periodically queries for updates.
Non-AOM machines are those that go On and Off and stay ofﬂine
for a period greater than the pre-speciﬁed query interval. The au-
tomatic update service in those machines will query soon after re-
booting since the query time expired. Queries can also occur from
machines where the users manually check for updates, however,
these are rare events.
4.1.1 Distincts IPs over time
We ﬁrst examine the aggregate volume of user queries with respect
to the distinct IPs using Set-I. We try to identify both AOM and
non-AOM users. Note that in the case of AOM, the aggregates
of queries will not exhibit spatial or temporal correlations due to
the randomization process of query polling. Correlations on the
other hand can be a consequence of queries that are initiated by
user actions, either by deliberate manual querying or because of
the queries initiated shortly after a computer boot time, for those
computers that missed a scheduled query time. Ideally, in a system
where instantaneous patching were feasible, it would be desirable
that the majority of users poll for updates as close in time to the
release of the patch as possible, so that the vulnerable population is
minimized.
Approximately 80% of the observed IPs appear within the ﬁrst
day; the number of fresh IPs drops by an order of magnitude during
the second day, and is further reduced by factor of 2 in day three.
Fig. 6 shows the number of distinct IPs per second for the three
day trace, where the large drops across the three days are evident
through the change in the mean of the time-series. Within each of
the three days, we respectively observe approximately 117, 22, and
11 millions of distinct IPs. Overall, the number of fresh IPs within a
(a)
(b)
(c)
Figure 5: (a) Group size distribution for File grouping. (b) Group size distribution for Delta grouping. (c) Number of requests satisﬁed by
publishing an increasing number of groups for Delta and File grouping.
Figure 7: The fraction of distinct IPs per continent observed within the
ﬁrst day versus time. The slope of each curve corresponds to the query
arrival rate per continent. The differences in the rates result from time-of-
day effects
day decreases abruptly with the number of the days since the initial
observation time.
4.1.2 Time-of-day effects
Time-of-day effects are present in user queries. This may be sur-
prising given that the system was designed to smooth the arrival of
update requests. However, it can be explained by the fact that a
large number of machines initiate a query shortly after a machine
boot time. Indeed, queries from non-AOM users will lead to time of
day dependencies on the rate of query arrivals. Such dependencies
will be pronounced when examining aggregate rates per continent;
by mapping IPs to their continent of origin.
Fig. 7 shows the cumulative fraction of distinct IPs observed
within the ﬁrst day over time, where the slopes of individual curves
correspond to the arrival rates over time per continent. Our initial
observation time is 8:00 UTC and thus Europe exhibits the largest
query arrival rates at the beginning and at the end of the day. The
peak rate for North America happens within the interval 13:00 to
16:00 hours, which is consistent with the 5 hours time difference
between UTC and US East coast and additional three hours differ-
ence from US East to West coast. The time difference of UTC to
Beijing is 8 hours and as expected the query rates from Asia peak
at around 24:00 UTC. Similar curves are obtained for the second
and third day of our traces.
Figure 8: Number of queries for two European ISPs in the same country
over time (as % over total number of queries). While the time-of-day effect
pattern is similar, the different proﬁle of ISP customers further affects the
query arrival rate (ISP1 focuses on business customers, while ISP2 focuses
on residential customers).
4.1.3 Uniformity and burstiness of queries
We further characterize the temporal correlation of user requests
by studying the statistical properties of the arrival process. While
time-of-day dependences are evident in the continent aggregate rates,
such correlations appear even more emphasized when we examine
the per-AS query arrival rate. However, in addition to time-of-day
effects, ASes may exhibit different query rates depending on the
proﬁle of their customers. Fig. 8 presents a typical example of
two geographically collocated ISPs where the proﬁle of their sub-
scribers (residential vs. corporate) results in dissimilar query arrival
patterns within the day.
In order to statistically characterize our entire sample of approx-
imately 19K observed ASes, we analyze the distribution of queries
in time and try to understand whether they are uniformly distributed
and, if not, quantify their burstiness.
We admit as null hypothesis that queries from an AS are uni-
formly distributed over time, which would be true if the users were
querying at random times, uniformly within 0-24 hours. To ex-
amine this hypothesis, we perform Kolmogorov-Smirnov test (KS-
00:0006:0012:0018:0000:0000.511.522.5Time of day (local)% of requests in 10min  ISP 1ISP 2timation method are provided in [6]. Alternatively, we could have
collected per host query instants and classiﬁed the hosts by using
a hypothesis test to check whether the observed samples are drawn
from the known inter-query time distribution, but note that the du-
ration of our Set-I is 3 days and thus with mean per-host inter-query
time of 20 hours, we would have only 3.6 per host queries.
Using Set-I, our estimates suggest that approximately 20% of the
population is “always” online and thus could be patched inmedi-
ately. This estimate is based on the results in Fig. 9, which shows
the estimated percentage of AOM users for each country versus the
number of distinct IPs observed from that country. The estimates
are upper bounds and would be tight to the unknown percentage for
countries with sufﬁciently large number of the observed IP’s. Fig. 9
suggests this to hold for countries with > 300K distinct IP’s and it
is for those estimates that we assert the 20% ﬁgure.
The trace we used to determine Always On Machines (Set-I) also
includes requests from users that manually visit the Windows Up-
date site to search for updates using a browser (e.g. as opposed to
automatic updates). Such events could bias our estimation of AOM
users and add noise to our AOM estimation. However, such users
are a small percentage of the total population and account for very
few requests during the low activity periods (e.g. a user opening