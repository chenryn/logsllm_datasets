tion is illustrated by vertical dotted lines in Figure 4. The
DTW dendrogram is further reﬁned by the type of hosts
(internal vs. external) that they communicate with, the
volume of trafﬁc, and other interesting behavioral prop-
erties.
k-Means Clustering. Another way to evaluate the
performance of the DTW and L1 metric is to consider
if the clusters produced maintain some level of behav-
ioral coherence, and whether the groupings are similar
to those that might be produced by a human network an-
alyst looking at the same data. To examine these prop-
erties, we use k-means++ clustering [1] to produce clus-
ters from each of the distance metrics and then exam-
ine the properties of the resultant clusters. In particu-
lar, we use the dominant state analysis technique of Xu
et al. [32] to characterize the dominant behaviors in each
cluster, examine the distribution of host labels in each
cluster, and manually examine a random sample of each
cluster to determine the overall behavior being captured.
While a full discussion of the Xu et al. dominant state
algorithm is beyond the scope of this evaluation, we pro-
vide a high-level notion of its operation and how we ex-
tend it to capture behaviors of groups rather than indi-
vidual hosts. The algorithm begins by calculating the
normalized entropy of each ﬁeld being analyzed, and
then ordering those ﬁelds from smallest entropy value to
greatest. The algorithm then selects all values from the
smallest entropy ﬁeld whose probability are greater than
a predetermined threshold t, thereby creating “proﬁles”
for each value. Then, each of those proﬁles is extended
by examining values in the next smallest entropy ﬁeld
whose joint probability of occurrence with the proﬁle
values is above the threshold t. The proﬁles are extended
iteratively until no new values may be added from the
current ﬁeld, at which point they are considered to be
ﬁnal proﬁles.
In the original paper, Xu et al. deﬁned the distribu-
tion of values on a per-host basis to quickly determine
host activities. To apply the same procedure to groups
of hosts, we apply the dominant state algorithm in two
levels. At the ﬁrst level, we run the algorithm on dis-
tributions of values for each host in the cluster. This
produces dominant state proﬁles for each of those hosts.
From these proﬁles, we extract the values for each of the
present ﬁelds to create new distributions. These distri-
bution represent the probability of a value occurring in
the dominant state proﬁles for the hosts in the current
cluster. Finally, we apply the dominant state algorithm
to these cluster-wide distributions to extract out the pro-
ﬁles that occur most consistently among all hosts in the
Cluster 1
Hosts/Flows:
Host Types:
Activities:
Hosts/Flows:
DTW Metric
25 hosts/139.2 ﬂows
L1 Distance
12 hosts/165.8 ﬂows
APC 72%, DHCP 24%, CLASS 4%
25 hosts/1,166.0 ﬂows
APC 50%, DHCP 41%, CLASS 9%
18 hosts/908.2 ﬂows
Cluster 2
Host Types:
DHCP 72%, CLASS 16%, VM 8%, AI 4%,
DHCP 89%, CLASS 11%
Activities:
Hosts/Flows:
Host Types:
Activities:
Cluster 3
26 hosts/539,438 ﬂows
46 hosts/305,211 ﬂows
LOGIN 46%, PLANET 11%, CLASS 11%,
VM 8%, SMTP 8%, WEB 4%, AI 4%,
DNS 4%, DHCP 4%
APC 26%, LOGIN 26%, CLASS 10%,
DHCP 8%, VM 8%, PLANET 6%, AI 4%,
SMTP 4%, WEB 2%, DNS 2%
Table 3. k-Means clustering of hosts in the ﬁrst day of the CSE dataset using DTW and L1
metrics. Activities represent the dominant state proﬁles obtained from applying our extension
to the Xu et al. dominant state algorithm [32].
clusters.
For this experiment, we manually examined a sam-
ple of hosts throughout the entire dataset and deter-
mined that there were, roughly, three high-level classes
of activities:
server activities, client activities, and
noise/scanning activities. This manual classiﬁcation was
supported by the results of our agglomerative clustering
analysis, which showed several levels of increasingly
subtle behavioral differences within each of these three
classes. Given this rough classiﬁcation of behavior, we
set k = 3 and see if the resultant clustering indeed cap-
tured the intuitive understanding of the activities as de-
termined by a human analyst. Table 3 shows the break
down of the three clusters produced by the k-means++
algorithm using DTW and L1 metrics. The results of
the DTW metric clustering shows that the clusters are
indeed broken into the three classes of activity found via
manual inspection. Cluster 1, which contains primar-
ily the power supply devices (APC) and DHCP hosts,
had signiﬁcant scanning activity from IP addresses in
China, and relatively low trafﬁc volumes indicating only
sporadic use. By comparison, hosts in Cluster 2 exhibit
traditional client behaviors of signiﬁcant SSH and web
browsing activities. Finally, Cluster 3 contains hosts
with signiﬁcant server-like activities. For instance, the
hosts related to the artiﬁcial intelligence research project
(AI) were found to be running web servers that provide
statistics to participants in the project, and most of its
activity is made up of web requests.
The L1 metric, on the other hand, produced some-
what incoherent clusters. While there is some overlap
in the clusters and observed behaviors, it is clear by the
distribution of host types that these clusters mix behav-
iors. The most prominent example of this is that many
of the power supply hosts (APC) that we veriﬁed as have
low trafﬁc volumes and scanning activity were grouped
in with the server cluster (Cluster 3). Moreover, the
client cluster (Cluster 2) contains many of the DHCP
hosts with scanning activity, which in turn alters the be-
havioral proﬁle for that cluster to remove web activities.
The results of this experiment certainly indicate that the
L1 metric simply does not capture a coherent notion of
behavior. Moreover, when the number of clusters is in-
creased, the differences between the DTW and L1 met-
rics become more signiﬁcant. That is, the L1 metric con-
tinues to mix behaviors, while the DTW metric creates
clusters that represent increasingly ﬁne-grained behav-
iors, such as the preference for communicating with in-
ternal versus external hosts.
As mentioned earlier in this section, this clustering
approach was also used to ﬁlter a large portion of the
scan trafﬁc found in the dataset. Speciﬁcally, when we
ﬁrst ran this experiment, we found that the behavioral
proﬁles produced by Cluster 1 were consistent with a
wide range of scan trafﬁc, and that the number of hosts
in that cluster were signiﬁcantly larger than expected.
We were then able to use the behavioral proﬁles to re-
move trafﬁc from scanning IPs and produce a much
cleaner clustering without most of the initial scanning
activity. As you can see by the scanning IP from China
found in the proﬁle of Cluster 1, it appears that we can
continue performing this clustering approach to itera-
tively identify increasingly subtle scanning activity.
Changes in Behavior Over Time. Perhaps one of the
most important properties of any behavioral metric is its
robustness to small changes in underlying network activ-
ity. That is, we want any changes in the measured dis-
Host Type
Num. Hosts
DTW Metric
Avg. Rank
Num. Perfect
Consistency
L1 Distance
Avg. Rank
Num. Perfect
Consistency
WEB
DNS
PLANET
VM
LOGIN
SMTP
APC
AI
DHCP
CLASS
TOTAL
1
1
3
4
12
2
18
2
24
8
75
1
1
2
2
8
1
2
0
2
1
20
1.0
1.0
1.3
1.5
2.1
3.5
16.2
41.5
48.4
54.4
17.1
1
0
3
0
8
2
0
0
1
0
15
1.0
17.0
1.0
41.3
14.2
1.0
39.9
40.5
31.8
31.1
21.9
Table 4. Host behavioral consistency for hosts occurring in both days of the CSE dataset.
tance to be related to some high-level behavioral change
and not minute changes in the speciﬁcs of the trafﬁc. As
our ﬁnal experiment, we use both days of trafﬁc in the
CSE dataset to determine the sensitivity of the DTW and
L1 metrics to changes in behavior over time. To do so,
we take the set of all hosts that occur in both days (as
determined by IP address), and compare their day one
time series to those of all hosts in day two. This pro-
duces a list of distances for each host in the day one data
to the day two hosts. With consistent behavior and a a
behavioral metric that is robust to minute changes in ac-
tivity, we would expect to ﬁnd that each day one host
is closest to itself in the day two data. Of course, there
are also instances where host behavior did signiﬁcantly
change between the two observation periods. Therefore,
our analysis looks at both the number of hosts within
each label class whose behaviors appear to remain the
same and the reasons that some hosts’ behaviors appar-
ently change.
To begin, we provide a summary of the above con-
sistency experiment for each host label when using the
DTW and L1 metrics in Table 4. The table shows two
values: the number of hosts with perfect consistency and
the average consistency rank of all hosts in the group.
By perfect consistency, we mean hosts in day one whose
closest host in day two is itself. Consistency rank refers
to the rank of the day one host in the sorted list of day
two distances. Ideally, if the behaviors of the hosts in
the group are exactly the same we would have all hosts
with perfect consistency and an average rank of 1.0. The
most obvious observation we can make from the results
of this experiment is that the L1 metric appears to be
more brittle than DTW with a signiﬁcantly greater av-
erage rank and fewer perfect consistency hosts overall.
What is more interesting, however, are the cases where
DTW indicates change in behavior and L1 does not, and
vice versa.
For the case where DTW indicates change and L1
does not, we again examine the two SMTP servers found
in our data. Recall from the previous clustering experi-
ments that one SMTP server in the ﬁrst day of data acts
as the primary server with many connections to external
hosts, while the other receives many fewer connections
and those are primarily to internal hosts. Upon closer
inspection of the two servers’ behaviors in day two, we
ﬁnd that the primary SMTP server’s (18) activities are
effectively unchanged. The secondary server (27), how-
ever, has a signiﬁcant increase in the proportion of trafﬁc
that is related to mail activities. In particular, the sec-
ondary SMTP server (27) in the ﬁrst day of data has a
roughly even split between general mail trafﬁc and an
SSH password dictionary attack (i.e., hundreds of con-
secutive SSH login attempts), while in the second day
the SSH attacks stop almost completely and the gen-
eral mail trafﬁc to both internal and external hosts in-
crease signiﬁcantly. Intuitively, this does indeed indicate
a change in behavior due to the presence of the SSH at-
tack and change in mail activity, although the L1 metric
indicates that no such change took place.
Next, we look at the group of VM servers for the case
where the L1 metric indicates a change and the DTW
metric does not. In this case, manual inspection of all
four VM hosts indicates that there were no signiﬁcant
changes in trafﬁc volume or activities for any host. The
only noticeable change was that one of the client-like
VMs (127) was port scanned for a few seconds late at
night on the second day. Given this information, our
DTW metric’s ranking makes perfect sense – the two
VMs running management protocols (183,184) were ex-
actly the same as their previous day’s activity, while the
client-like VMs (127,128) got confused for one another
in the previous day which is evidenced by the average
ranking of 1.5 for that group (i.e., rank of 1.0 for the
management VMs, 2.0 for the client VMs). With the L1
metric, not only did it confuse the VM behaviors for one
another in the previous day, but it also confused it with
dozens of other client-like hosts (i.e., DHCP, LOGIN,
etc.) and management hosts (i.e., APC), thereby caus-
ing the signiﬁcantly higher average rank for that group.
In fact, the L1 metric only seems to achieve a lower av-