有高采样效率的基于模型的方法发挥着越来越重要的作用（详见第7章）。例如，第15章中介绍
的AlphaGo(Silveretal.,2016)、AlphaZero(Silveretal.,2017,2018)算法，以及最新的MuZero算
法(Schrittwieseretal.,2019)都属于基于模型的方法。
3.2 基于价值的方法和基于策略的方法
回忆第2章，深度强化学习中的策略优化主要有两类：基于价值的方法和基于策略的方法。
两者的结合产生了Actor-Critic类算法和QT-Opt(Kalashnikovetal.,2018)等其他算法，它们利用
价值函数的估计来帮助更新策略。其分类关系如图3.3所示。基于价值的方法通常意味着对动作
价值函数Qπ(s,a)的优化。优化后的最优值函数表示为Qπ∗ (s,a)=max Qπ(s,a)，最优策略通
a
过选取最大值函数对应的动作得到π∗ ≈argmax Qπ（“≈”由函数近似误差导致）。
π
O
图3.3 基于价值的方法和基于策略的方法。图片参考文献(Li,2017)
基于价值的方法的优点在于采样效率相对较高，值函数估计方差小，不易陷入局部最优；缺
点是它通常不能处理连续动作空间问题，且最终的策略通常为确定性策略而不是概率分布的形
式。此外，深度Q网络等算法中的ϵ-贪心策略（ϵ-greedy）和max算子容易导致过估计的问题。
常见的基于价值的算法包括Q-learning(Watkinsetal.,1992)、深度Q网络（DeepQ-Network，
DQN）(Mnihetal.,2015)及其变体：（1）优先经验回放（PrioritizedExperienceReplay，PER）(Schaul
etal.,2015)基于TD误差对数据进行加权采样，以提高学习效率；（2）DuelingDQN(Wangetal.,
2016)改进了网络结构，将动作价值函数Q分解为状态值函数V 和优势函数A以提高函数近似
能力；（3）DoubleDQN(VanHasseltetal.,2016)使用不同的网络参数对动作进行选择和评估，以
113
第3章 强化学习算法分类
解决过估计的问题；（4）Retrace (Munos et al., 2016) 修正了 Q 值的计算方法，减少了估计的方
差；（5）NoisyDQN(Fortunatoetal.,2017)给网络参数添加噪声，增加了智能体的探索能力；（6）
DistributedDQN(Bellemareetal.,2017)将状态-动作值估计细化为对状态-动作值分布的估计。
基于策略的方法直接对策略进行优化，通过对策略迭代更新，实现累积奖励最大化。与基于
价值的方法相比，基于策略的方法具有策略参数化简单、收敛速度快的优点，且适用于连续或高
维的动作空间。一些常见的基于策略的算法包括策略梯度算法（PolicyGradient，PG）(Suttonetal.,
2000)、信赖域策略优化算法（TrustRegionPolicyOptimization，TRPO）(Schulmanetal.,2015)、近
端策略优化算法（ProximalPolicyOptimization，PPO）(Heessetal.,2017;Schulmanetal.,2017)等，
信赖域策略优化算法和近端策略优化算法在策略梯度算法的基础上限制了更新步长，以防止策略
崩溃（Collapse），使算法更加稳定。
除了基于价值的方法和基于策略的方法，更流行的是两者的结合，这衍生出了Actor-Critic方
法。Actor-Critic方法结合了两种方法的优点，利用基于价值的方法学习Q值函数或状态价值函数
V 来提高采样效率（Critic），并利用基于策略的方法学习策略函数（Actor），从而适用于连续或
高维的动作空间。Actor-Critic方法可以看作是基于价值的方法在连续动作空间中的扩展，也可以
看作是基于策略的方法在减少样本方差和提升采样效率方面的改进。虽然 Actor-Critic 方法吸收
了上述两种方法的优点，但同时也继承了相应的缺点。比如，Critic存在过估计的问题，Actor存
在探索不足的问题等。一些常见的Actor-Critic类的算法包括Actor-Critic（AC）算法(Suttonetal.,
2018)和一系列改进：（1）异步优势Actor-Critic算法（A3C）(Mnihetal.,2016)将Actor-Critic方
法扩展到异步并行学习，打乱数据之间的相关性，提高了样本收集速度和训练效率；（2）深度确
定性策略梯度算法（DeepDeterministicPolicyGradient，DDPG）(Lillicrapetal.,2015)沿用了深度
Q网络算法的目标网络，同时Actor是一个确定性策略；（3）孪生延迟DDPG算法（TwinDelayed
DeepDeterministicPolicyGradient，TD3）(Fujimotoetal.,2018)引入了截断的（Clipped）Double
Q-Learning解决过估计问题，同时延迟Actor更新频率以优先提高Critic拟合准确度；（4）柔性
Actor-Critic 算法（SoftActor-Critic，SAC）(Haarnoja etal.,2018)在Q 值函数估计中引入熵正则
化，以提高智能体探索能力。
3.3 蒙特卡罗方法和时间差分方法
蒙特卡罗（MonteCarlo，MC）方法和时间差分（TemporalDifference，TD）方法的区别已经
在第2章中讨论过，一些算法如图3.4所示。这里我们再次总结它们的特点以保证本章的完整性。
时间差分方法是动态规划（DynamicProgramming，DP）方法和蒙特卡罗方法的一种中间形式。首
先，时间差分方法和动态规划方法都使用自举法（Bootstrapping）进行估计，其次，时间差分方法
和蒙特卡罗方法都不需要获取环境模型。这两种方法最大的不同之处在于如何进行参数更新，蒙
特卡罗方法必须等到一条轨迹生成（真实值）后才能更新，而时间差分方法在每一步动作执行都
可以通过自举法（估计值）及时更新。这种差异将使时间差分方法方法具有更大的偏差，而使蒙
114
3.4 在线策略方法和离线策略方法
特卡罗方法方法具有更大的方差。
图3.4 蒙特卡罗方法和时间差分方法
3.4 在线策略方法和离线策略方法
在线策略（On-Policy）方法和离线策略（Off-Policy）方法依据策略学习的方式对强化学习算
法进行划分（图3.5）。在线策略方法试图评估并提升和环境交互生成数据的策略，而离线策略方
法评估和提升的策略与生成数据的策略是不同的。这表明在线策略方法要求智能体与环境交互的
策略和要提升的策略必须是相同的。而离线策略方法不需要遵循这个约束，它可以利用其他智能
体与环境交互得到的数据来提升自己的策略。常见的在线策略方法是Sarsa，它根据当前策略选
择一个动作并执行，然后使用环境反馈的数据更新当前策略。因此，Sarsa与环境交互的策略和
更新的策略是同一个策略。它的Q函数更新公式如下：
Q(S ,A )←Q(S ,A )+α[R +γQ(S ,A )−Q(S ,A )]. (3.1)
t t t t t t+1 t+1 t t
图3.5 在线策略方法和离线策略方法
115
第3章 强化学习算法分类
Q-learning是一种典型的离线策略方法。它在选择动作时采用max操作和ϵ-贪心策略，使得
与环境交互的策略和更新的策略不是同一个策略。它的Q函数更新公式如下：
Q(S ,A )←Q(S ,A )+α[R +γmaxQ(S ,A )−Q(S ,A )]. (3.2)
t t t t t t+1 t+1 t t
a
参考文献
BALDIP,2012.Autoencoders,UnsupervisedLearning,andDeepArchitectures[C]//Proceedingsoastthe
InternationalConferenceonMachineLearning(ICML). 37-50.
BELLEMARE M G, DABNEY W, MUNOS R, 2017. A distributional perspective on reinforcement
learning[C]//Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume70.JMLR.
org: 449-458.
FORTUNATOM,AZARMG,PIOTB,etal.,2017. Noisynetworksforexploration[J]. arXivpreprint
arXiv:1706.10295.
FUJIMOTOS,VANHOOFH,MEGERD,2018.Addressingfunctionapproximationerrorinactor-critic
methods[J]. arXivpreprintarXiv:1802.09477.
HA D, SCHMIDHUBER J, 2018. Recurrent world models facilitate policy evolution[C]//Advances in
NeuralInformationProcessingSystems. 2450-2462.
HAARNOJAT,ZHOUA,ABBEELP,etal.,2018. Softactor-critic: Off-policymaximumentropydeep
reinforcementlearningwithastochasticactor[J]. arXivpreprintarXiv:1801.01290.
HEESSN,SRIRAMS,LEMMONJ,etal.,2017. Emergenceoflocomotionbehavioursinrichenviron-
ments[J]. arXiv:1707.02286.
KALASHNIKOVD,IRPANA,PASTORP,etal., 2018. Qt-opt: Scalabledeepreinforcementlearning
forvision-basedroboticmanipulation[J]. arXivpreprintarXiv:1806.10293.
LIY,2017. Deepreinforcementlearning: Anoverview[J]. arXivpreprintarXiv:1701.07274.
LILLICRAP T P, HUNT J J, PRITZEL A, et al., 2015. Continuous control with deep reinforcement
learning[J]. arXivpreprintarXiv:1509.02971.
MNIHV,KAVUKCUOGLUK,SILVERD,etal.,2015.Human-levelcontrolthroughdeepreinforcement
learning[J]. Nature.
116
参考文献
MNIH V, BADIA A P, MIRZA M, et al., 2016. Asynchronous methods for deep reinforcement learn-
ing[C]//InternationalConferenceonMachineLearning(ICML). 1928-1937.
MUNOSR,STEPLETONT,HARUTYUNYANA,etal.,2016.Safeandefficientoff-policyreinforcement
learning[C]//AdvancesinNeuralInformationProcessingSystems. 1054-1062.
NAGABANDI A, KAHN G, FEARING R S, et al., 2018. Neural network dynamics for model-based
deep reinforcement learning with model-free fine-tuning[C]//2018 IEEE International Conference on
RoboticsandAutomation(ICRA). IEEE:7559-7566.
RACANIÈRES,WEBERT,REICHERTD,etal., 2017. Imagination-augmentedagentsfordeeprein-
forcementlearning[C]//AdvancesinNeuralInformationProcessingSystems. 5690-5701.
SCHAULT,QUANJ,ANTONOGLOUI,etal.,2015. Prioritizedexperiencereplay[C]//arXivpreprint
arXiv:1511.05952.
SCHRITTWIESERJ,ANTONOGLOUI,HUBERTT,etal.,2019. Masteringatari,go,chessandshogi
byplanningwithalearnedmodel[Z].
SCHULMANJ,LEVINES,ABBEELP,etal.,2015. Trustregionpolicyoptimization[C]//International
ConferenceonMachineLearning(ICML). 1889-1897.
SCHULMAN J, WOLSKI F, DHARIWAL P, et al., 2017. Proximal policy optimization algorithms[J].
arXiv:1707.06347.
SILVER D, HUANG A, MADDISON C J, et al., 2016. Mastering the game of go with deep neural
networksandtreesearch[J]. Nature.
SILVERD,HUBERTT,SCHRITTWIESERJ,etal.,2017. Masteringchessandshogibyself-playwith
ageneralreinforcementlearningalgorithm[J]. arXivpreprintarXiv:1712.01815.
SILVERD,HUBERTT,SCHRITTWIESERJ,etal.,2018. Ageneralreinforcementlearningalgorithm
thatmasterschess,shogi,andGothroughself-play[J]. Science,362(6419): 1140-1144.
SUTTONRS,BARTOAG,2018. Reinforcementlearning: Anintroduction[M]. MITpress.
SUTTONRS,MCALLESTERDA,SINGHSP,etal.,2000. Policygradientmethodsforreinforcement
learningwithfunctionapproximation[C]//AdvancesinNeuralInformationProcessingSystems. 1057-
1063.
VANHASSELTH,GUEZA,SILVERD,2016.DeepreinforcementlearningwithdoubleQ-learning[C]//
ThirtiethAAAIconferenceonartificialintelligence.
117
第3章 强化学习算法分类
WANGZ,SCHAULT,HESSELM,etal.,2016. Duelingnetworkarchitecturesfordeepreinforcement
learning[C]//InternationalConferenceonMachineLearning. 1995-2003.
WATKINSCJ,DAYANP,1992. Q-learning[J]. Machinelearning,8(3-4): 279-292.
118
4
深度 Q 网络
本章将介绍的 DQN 算法全称为深度 Q 网络算法，是深度强化学习算法中最重要的算法之
一。我们将从基于时间差分学习的Q-Learning算法入手，介绍DQN算法及其变体。在本章的最
后，我们提供了代码示例，并对DQN及其变体进行实验比较。
强化学习最重要的突破之一是Q-Learning算法。它是一种离线策略（Off-Policy）的时间差分
（TemporalDifference）算法，此前在第2章中有介绍。在使用表格（Tabular）的情况下或使用线
性函数逼近Q函数时，Q-Learning已被证明可以收敛于最优解。然而，当使用非线性函数逼近器
（如神经网络）来表示Q函数时，Q-Learning并不稳定，甚至是发散的(Tsitsiklisetal.,1996)。随
着深度神经网络技术的不断发展，深度Q网络（DeepQ-Networks，DQN）算法(Mnihetal.,2015)
解决了这一问题，并点燃了深度强化学习的研究。在本章中，我们将先回顾Q-Learning的背景。
之后介绍DQN算法及其变体，并给出详细的理论和解释。最后，在4.8节，我们将通过代码展示
算法在雅达利游戏上的实现细节与实战表现，为读者提供快速上手的实战学习过程。每种算法的
完整代码可以在随书提供的代码仓库中找到1。
无模型（Model-Free）方法为解决基于 MDP 的决策问题提供了一种通用的方法。其中“模
型”是指显式地对MDP相关的转移概率分布和回报函数建模，而时间差分（TemporalDifference，
TD）学习就是一类无模型方法。在2.4节中，我们讨论过，当拥有一个完美的MDP模型时，通
过递归子问题的最优解，就可以得到动态规划的最优方案。TD学习也遵循了这样一种思想，即
使对子问题的估计并非一直是最优的，我们也可以通过自举（Bootstrapping）来估计子问题的值。
1代码链接见读者服务
119
第4章 深度Q网络
子问题通过MDP中的状态表示。在策略π下，状态为s时的value值（V值）v (s)被定义
π
为从状态s开始，以策略π进行动作的预期回报：
v (s)=E [R +γv (S )|S =s], (4.1)
π π t π t+1 t
此处的 γ ∈ [0,1] 是衰减率。TD 学习用自举法分解上述估计。给定价值函数 V : S → R，
TD(0)是一个最简单的版本，它只应用一步自举，如下所示：
V(S )←V(S )+α[R +γV(S )−V(S )] (4.2)
t t t t+1 t
此处的R +γV(S )和R +γV(S )−V(S )分别被称为TD目标和TD误差。
t t+1 t t+1 t
策略的评估值提供了一种对策略的动作质量（Quality）进行评估的方法。为了进一步了解如
何选择某一特定状态下的动作，我们将通过Q值来评估状态-动作组合的效果。Q值可以这样被
估计：
q (s,a)=E [R +γv (S )|S =s,A =a] (4.3)
π π t+1 π t+1 t t
有了 Q 值对策略进行评估之后，我们只需要找到一种能提升 Q 值的方法就能提升策略的
效果。最简单的提升效果的方法就是通过贪心的方法执行动作：π′(s) = argmax qπ(s,a′)。由
a′
q π′(s,a) = max a′q π(s,a′) ⩾ q π(s,a)我们可以知道，贪心的策略一定不会得到一个更差的解法。
考虑到探索的必要性，我们可以用一种替代方案来提升策略的效果。在该方案中，多数情况下我