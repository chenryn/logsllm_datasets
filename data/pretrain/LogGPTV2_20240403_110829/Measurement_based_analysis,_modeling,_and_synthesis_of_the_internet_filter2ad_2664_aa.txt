title:Measurement based analysis, modeling, and synthesis of the internet
delay space
author:Bo Zhang and
T. S. Eugene Ng and
Animesh Nandi and
Rudolf H. Riedi and
Peter Druschel and
Guohui Wang
Measurement-Based Analysis, Modeling, and Synthesis of
the Internet Delay Space∗
Bo Zhang†, T. S. Eugene Ng†, Animesh Nandi†‡,
Rudolf Riedi†, Peter Druschel‡, Guohui Wang†
†Rice University, USA
‡Max Planck Institute for Software Systems, Germany
ABSTRACT
Understanding the characteristics of the Internet delay space (i.e.,
the all-pairs set of static round-trip propagation delays among edge
networks in the Internet) is important for the design of global-scale
distributed systems. For instance, algorithms used in overlay net-
works are often sensitive to violations of the triangle inequality and
to the growth properties within the Internet delay space. Since de-
signers of distributed systems often rely on simulation and emula-
tion to study design alternatives, they need a realistic model of the
Internet delay space.
Our analysis shows that existing models do not adequately cap-
ture important properties of the Internet delay space. In this paper,
we analyze measured delays among thousands of Internet edge net-
works and identify key properties that are important for distributed
system design. Furthermore, we derive a simple model of the In-
ternet delay space based on our analytical ﬁndings. This model
preserves the relevant metrics far better than existing models, al-
lows for a compact representation, and can be used to synthesize
delay data for simulations and emulations at a scale where direct
measurement and storage are impractical.
Categories and Subject Descriptors
C.2.m [Computer-Communication Networks]: Miscellaneous
General Terms
Measurement, Performance, Experimentation
Keywords
Internet delay space, measurement, analysis, modeling, synthesis,
distributed system, simulation
∗
This research was sponsored by the NSF under CAREER Award
CNS-0448546, and by the Texas Advanced Research Program un-
der grant No.003604-0078-2003. Views and conclusions contained
in this document are those of the authors and should not be in-
terpreted as representing the ofﬁcial policies, either expressed or
implied, of NSF, the state of Texas, or the U.S. government.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’06, October 25–27, 2006, Rio de Janeiro, Brazil.
Copyright 2006 ACM 1-59593-561-4/06/0010 ...$5.00.
1.
INTRODUCTION
Designers of large-scale distributed systems rely on simulation
and network emulation to study design alternatives and evaluate
prototype systems at scale and prior to deployment. To obtain
accurate results, such simulations or emulations must include an
adequate model of the Internet delay space: The all-pairs set of
static round-trip propagation delays among edge networks. Such
a model must accurately reﬂect those characteristics of real Inter-
net delays that inﬂuence system performance. For example, having
realistic clustering properties is important because they can inﬂu-
ence the load balance of delay-optimized overlay networks, and the
effectiveness of server placement policies and caching strategies.
Having realistic growth characteristics [16] in the delay space is
equally important, because the effectiveness of certain distributed
algorithms depends on them. Many distributed systems are also
sensitive to the inefﬁciency of IP routing with respect to delay.
Such inefﬁciency manifests itself as triangle inequality violations
in the delay space, and must be reﬂected in a model as well.
Currently, two approaches are used to obtain a delay model.
The ﬁrst approach, adopted for instance by the P2PSim simula-
tor [25], is to collect actual delay measurements using a tool such
as King [13]. However, due to limitations of the measurement
methodology and the quadratic time requirement for measuring a
delay matrix, measured data tends to be incomplete and there are
limits to the size of a delay matrix that can be measured in practice.
To its credit, P2PSim provides a 1740×1740 delay space matrix,
which is not a trivial amount of data to obtain.
The second approach is to start with a statistical network topol-
ogy model (e.g. [45, 48, 8, 10, 18]) and assign artiﬁcial link delays
to the topology. The delay space is then modeled by the all-pair
shortest-path delays within the topology. The properties of such
delay models, however, tend to differ dramatically from the actual
Internet delay space. This is because these models do not ade-
quately capture rich features in the Internet delay space, such as
those caused by geographic constraints, variation in node concen-
trations, and routing inefﬁciency.
A delay space model suitable for large-scale simulations must
adequately capture the relevant characteristics of the Internet delay
space. At the same time, the model must have a compact repre-
sentation, since large-scale simulations tend to be memory-bound.
The naive approach of storing 16-bit delay values for all pairs of
a 100K node network, for instance, would require almost 20GB of
main memory! Finally, to enable efﬁcient simulation, generating a
delay value for a given pair of nodes must require very little com-
putation and no disk accesses.
One approach is to build a structural model of the Internet, us-
ing BGP tables, traceroute, ping and other measurements to capture
the coarse-grained (e.g., AS-level) topology of the Internet and the
associated static link delays [22]. Given such a model, the delay
for a given pair of IP addresses can be estimated by adding the link
delays on the predicted route through the topology. If the topol-
ogy model captures the coarse-grained structure of the Internet well
enough, the resulting delays should preserve the characteristics of
the Internet delay space. However, it remains unclear how detailed
such a model has to be to preserve the relevant characteristics.
Another approach is to build a statistical model, designed to pre-
serve the statistical characteristics of a measured Internet delay data
set. Unlike a structural model, a statistical model cannot predict the
delay between a particular pair of real Internet IP addresses. For the
purposes of distributed systems simulations, however, it sufﬁces
that the statistical properties of the model adequately reﬂect those
of the measured delay data. Statistical models lend themselves to a
compact representation and can enable efﬁcient generation of delay
data at large scale. Since we are primarily interested in enabling ac-
curate, efﬁcient, large-scale simulations, we decided to pursue this
approach in this paper.
We have measured a sample of the Internet delay space among
3,997 edge networks. We then characterize the measured sam-
ple with respect to a set of metrics that are relevant to distributed
system design. Based on these analytical ﬁndings, we develop
a method to model measured Internet delay spaces. The result-
ing model has a compact O(N ) representation (as opposed to the
O(N 2) matrix representation) that adequately preserves the rele-
vant delay space characteristics, and can model missing measure-
ments. We then extend our model and develop a method to syn-
thesize an artiﬁcial delay space. The method exploits the scaling
characteristics found in the measurements and makes it possible
to synthesize a delay space much larger than the measured delay
space, while preserving the characteristics of the measured data.
We make two primary contributions in this work:
• We systematically quantify the properties of the Internet delay
space with respect to a set of statistical, structural, and routing met-
rics relevant to distributed systems design. This leads to new fun-
damental insights into Internet delay space characteristics that may
inform future work.
• We develop a set of building block techniques to model and syn-
thesize the Internet delay space compactly, while accurately pre-
serving the relevant metrics. The compact representation enables
accurate and memory efﬁcient network simulations at large scale.
We emphasize that our goal is to provide a model of the Internet
delay space that enables accurate large-scale simulations. We do
not attempt to provide either an explanatory model of Internet de-
lay, which explains the underlying technical, economic and social
forces that shape Internet delays, nor do we attempt to provide a
predictive model that can estimate the delay between a given pair
of real IP hosts. Building such models is also an interesting re-
search direction, but is beyond the scope of this paper.
2. METHODOLOGY AND MODELS
We begin by describing our measurement methodology and the
existing delay space models we use in this study.
2.1 Measured Internet Delay Space
We use the King tool [13] to measure the all-pair round-trip static
propagation delays among a large number of globally distributed
DNS servers, where each server represents a unique domain and
typically one edge network. To choose DNS servers, we start with
a list of 100,000 random IP addresses drawn from the preﬁxes an-
nounced in BGP as published by the Route Views project [32]. For
each IP address, we perform a reverse DNS lookup to determine
the associated DNS servers. Each reverse lookup returns a set of
DNS servers DIPi . We keep only the DNS server sets in which at
least one server supports recursive queries, since King requires it.
If two DNS server sets DIPi and DIPj overlap, then only one of
the two sets is kept since they do not represent distinct domains.
If there is more than one server in a set, the set is kept only if all
the servers in the set are topologically close. We check this by per-
forming traceroutes to all the servers in the set. By making sure
the servers in the set are physically co-located, we ensure different
measurement samples are measuring the same network. Among
the remaining DNS server sets, we choose one server per set that
supports recursive query. We then use 5,000 such DNS servers to
conduct our measurements.
Care must be taken during the measurement process. The mea-
sured delays include the DNS server processing delays as well as
network queuing delays. We collect multiple measurement sam-
ples and keep only the minimum value to approximate the static
delay. However, high packet loss rates can cause insufﬁcient mea-
surement samples. Also, because King measures the delay between
two servers, say D1 and D2, by subtracting the delay to D1 from
the delay to D2 via D1, it is possible to end up with a very small
or even negative delay value if the measurements were tainted by
processing or queuing delays.
To ensure the subsequent analysis is based on accurate data, we
adopt a fairly stringent methodology. We measure the round-trip
delay between two DNS servers from both directions by using ei-
ther server as the recursive server. For each direction, we make up
to 50 attempts to measure the recursive delay to D2 via D1, and
up to 50 attempts to measure the delay to D1 via D2. At least 20
measurement samples must be obtained in each case. The min-
imum value across the samples is used as the propagation delay.
After the subtraction step, if the delay is negative, or if the delay
is greater than 2 seconds or smaller than 100 microseconds, it is
discarded. These unrealistic delay values are likely caused by pro-
cessing and queuing delays that affected the measurements. Also, if
the obtained delay between D1 and D2 measured in each direction
disagrees by more than 10%, we discard the measurement. Finally,
we remove data from DNS servers that are consistently failing to
provide valid measurements: After we assemble the delay space
matrix, if any row/column has more than 25% of the values miss-
ing, the entire row/column is removed.
We collected the measurements in October 2005. Among the
collected 5000×5000 delay data, 16.7% have insufﬁcient measure-
ments samples, 8.1% have inconsistent samples, 0.16% are smaller
than 100 microseconds, and 0.51% are larger than 2 seconds. After
removing suspicious measurement values, the remaining delay ma-
trix has 3997 rows/columns with 13% of the values in the matrix
unavailable. To characterize the distribution of the missing values,
we partition the delay matrix into its three largest clusters. These
clusters correspond to IP hosts in North America, Europe and Asia.
We ﬁnd that the percentage of missing values are distributed as fol-
lows:
From/To
North America
Europe
Asia
North America Europe Asia
12%
11%
18%
11%
15%
11%
14%
11%
12%
To understand the properties in the data set under scaling, we
consider four different random sub-samples of the measured data
with the sizes 800, 1600, 2400, and 3200. To reduce the sensi-
tivity to a particular random sample, for each sub-sample size, we
consider ﬁve random sample. Results presented in this paper are
averages over the ﬁve samples.
2.2 Topology Model Delay Spaces
We also generate delay matrices based on existing topology mod-
els and compare them against the measured Internet delay space.
The two generators we use are Inet [46] and GT-ITM [48]. The
Inet generator creates a topology that has power-law node degree
distribution properties. The GT-ITM generator is used to generate
a topology based on the Transit-Stub model. We include the Inet
and GT-ITM topology models in this study because they are often
used in distributed system simulations.
For Inet, we create a 16000-node topology. To generate the de-
lays, we use the standard method of placing nodes randomly in a
plane and then use the Euclidean distance between a pair of con-
nected nodes as the link delay. All-pairs shortest delay routing is
then used to compute end-to-end delays. Finally, we extract the
generated delays among the 5081 degree-1 nodes in the graph in
order to model the delays among edge networks. No triangle in-
equality violations are introduced. For GT-ITM, we create a 4160-
node transit-stub topology. Note that GT-ITM annotates links with
routing policy weights and artiﬁcial delays. Shortest path routing
is performed over the topology using routing policy weights as the
link costs. End-to-end delays are then computed by summing the
artiﬁcial link delays along the selected paths. Some triangle in-
equality violations are then introduced artiﬁcially in the resulting
delay space. Finally, we extract the delays among 4096 stub routers
to model the delays among edge networks.
We scale the delays in the two artiﬁcial delay matrices such that
their average delay matches the average delay in the measured de-
lay data. This constant scaling does not affect the structure of the
generated delay spaces. We do this only to simplify the presenta-
tion of results.
2.3 Limitations of Measured Delay Data
Our analysis is based on a carefully collected set of measured
Internet delay data. The data set, however, does have limitations.
First, the measurements are among DNS servers. The data set thus
represents the delay space among edge networks in the Internet.
No explicit measurements were collected among hosts within a lo-
cal area network. For example, even though a university campus
may have thousands of hosts, we most likely pick only one of its
DNS servers to include in the measurement. Therefore, this study
addresses only the delay space properties among edge networks in
the wide area, but not the delay space properties within a local area
network. Secondly, to increase our conﬁdence in the data, we have
discarded questionable measurements. We therefore proceed with
the assumption that the missing delay values do not have signiﬁ-
cantly different properties than the available data.
3.
INTERNET DELAY SPACE ANALYSIS
In this section, we ﬁrst identify a set of metrics that are known
to signiﬁcantly inﬂuence the performance of distributed systems.
Then, we analyze measured Internet delay data with respect to these
and other statistical and structural properties. The results give in-
sight into the characteristics of the Internet delay space, and they
inform the design of an appropriate model.
3.1 Systems-motivated Metrics
The metrics presented below are known to strongly inﬂuence dis-
tributed system performance and capture a wide range of important
issues in distributed system design and evaluation.
Global clustering - This metric characterizes clustering in the de-
lay space at a macroscopic level. For instance, the continents with
the largest concentration of IP subnetworks (North America, Eu-
rope and Asia) form recognizable clusters in the delay space. This
2
3
1
4
Not drawn to exact scale
Nearest neighbor edge
Node in delay space
1st cluster head
2nd cluster head
3rd cluster head
Not a cluster head; extracted
by cluster 1
1
2
3
4
Figure 1: Nearest neighbor directed graph analysis technique.
global clustering structure is, for instance, relevant to the place-
ment of large data centers and web request redirection algorithms
(e.g. [29]).
Our algorithm to determine the global clustering works as fol-
lows. Given N nodes in the measured input data, it ﬁrst treats each
node as a singleton cluster. The algorithm then iteratively ﬁnds two
closest clusters to merge. The distance between two clusters is de-
ﬁned as the average distance between the nodes in the two clusters.
A cutoff delay value determines when to stop the merging process.
If the distance between the two closest clusters is larger than the
cutoff, the merging process stops. By varying the cutoff value and
monitoring the resulting cluster sizes, the global clustering proper-
ties can be determined.
Local clustering - This metric characterizes clustering in the delay
space at the local level. It is based on analyzing the in-degree dis-
tribution of the directed graph formed by having each node point
to its nearest neighbor in the delay space. Moreover, we use the
graph to identify a set of local cluster heads (or centers). We se-
lect the node with the highest in-degree as a local cluster head and
remove it and its immediate children from the graph. This step is
applied repeatedly to identify the next local cluster head until no
more nodes remain. Since a local cluster resembles a star graph,
we sometimes simply call it a star. The process is illustrated in Fig-
ure 1. The importance of the local cluster heads will become clear
in subsequent sections.
Local clustering is relevant, for instance, to the in-degree and
thus the load balance among nodes in delay-optimized overlay net-
works (e.g. [5]). For example, dense local clustering can lead to
an overlay node having an unexpectedly high number of neighbors
and can potentially create a load imbalance in the overlay.
Growth metrics - Distributed nearest neighbor selection is a hard
problem, but efﬁcient algorithms have been identiﬁed to solve the
problem for growth-restricted metric spaces [16]. These algorithms
are used, for instance, in Tapestry [49] and Chord [41] to select
overlay neighbors. In a growth-restricted metric space, if the num-
ber of nodes with a delay of at most r from some node i is Bi(r),
then Bi(2r) ≤ c · Bi(r), where c is a constant. We characterize
the growth properties of a delay space by evaluating the function
B(2r)/B(r).
A related metric is the D(k) metric. Let d(i, k) be the average
delay from a node i to its k closest nodes in the delay space and
N be the set of nodes, then D(k) = 1|N|
i∈N d(i, k). Structured
overlay networks like Chord, Tapestry and Pastry employ proxim-
ity neighbor selection (PNS) to reduce the expected delay stretch
S, i.e., the ratio of the delay of an overlay route over the direct
routing delay averaged over all pairs of nodes [14, 4, 30, 5]. We
choose to include the D(k) metric because analysis has shown that
in Tapestry and Pastry, the expected delay stretch S in the overlay
can be predicted based on the function D(k) [5].
P
Triangle inequality violations - The triangle inequality states that
given points x, y and z in a Euclidean space, the distance dij be-
tween points i and j satisﬁes dxz ≤ dxy + dyz. The Internet delay
space, however, does not obey the triangle inequality, since Internet
routing may not be optimal with respect to delay. Unfortunately,
many distributed nearest neighbor selection algorithms rely on the
assumption that the triangle inequality holds [33, 16, 44]. Thus, it
is important to characterize the frequency and severity of the viola-
tions in the Internet delay space.
3.2 Analysis Results
We now present an analysis of the measured delay data with re-
spect to the metrics described above, and some basic properties like
the delay distribution. For comparison, we also show the relevant
properties of the delays produced by the Inet and GT-ITM models.
We begin with a comparison of the delay distribution. In Fig-
ure 2(a), we can observe that the delay distributions of the mea-
sured data set have characteristic peaks at roughly 45ms, 135ms,
and 295ms. This suggests that the nodes form clusters in the data.
Analysis of random data sub-samples indicates that the delay dis-
tribution is also independent of sample size. In contrast, the delay
distributions for the topology models do not indicate such behavior.
Clearly, there are rich features in the Internet delay space that are
not captured in the delays derived from these topology models.
To visualize the locations of nodes, we ﬁrst embed the data sets
into a 5D Euclidean space using a dimensionality reduction proce-
dure that is robust to missing data. Then, we do a principal com-
ponent analysis on the 5D Euclidean coordinates to get the ﬁrst 2
principal components. Several techniques exist to compute the 5D
embedding [24, 7, 35, 6, 19, 43]. Here, we use a slightly modiﬁed
version of the Vivaldi [7] method that avoids the missing measure-
ments. We use 32 neighbors per node in Vivaldi.
Figure 2(b) displays the scatter plots of the ﬁrst two principal
components of the 5D embedding for different data sets. The visual
differences between the measured data and the topology models are
striking. It is easy to see that there exists clustering structure in the
measured data. In contrast, the nodes in the topology models are
distributed more uniformly in the space, and their resulting delay
distributions are approximately normal.
To quantify the global clustering properties in the measured data
set, we apply the described global clustering algorithm and plot the
percentage of nodes in the largest cluster against different cluster-
ing cut-off thresholds in Figure 2(c). Regardless of the sample size,
the largest cluster’s size increases sharply at cutoff values 155ms
and 250ms. These sharp increases are caused by the merging of two
clusters at these thresholds. The steps suggest that there are three
dominant clusters. By setting the threshold to 120ms, nodes can
be effectively classiﬁed into the three major clusters. They account
for 45% (the North America cluster), 35% (the Europe cluster),
and 9% (the Asia cluster) of the nodes, respectively. The remain-
ing 11% are nodes that are scattered outside of the major clusters.
These global clustering properties can be used to guide the global
placement of servers and the design of load-balancing algorithms.
In contrast, there is no clear clustering structure in the Inet model.
The clustering structure of the GT-ITM model also does not resem-
ble that of the measured data.
The global clustering analysis reveals the coarse-grained struc-
ture of the delay space. To understand the ﬁne-grained structure,
we conduct the nearest neighbor directed graph analysis on the
data sets. We emphasize that these results characterize the prop-
erties among edge networks in the Internet; they do not charac-
terize the properties among end hosts within local area networks.
Figure 3(a) shows the in-degree distributions for different sample
Sample size
800
1600
2400
3200
3997 (all data)
# Cluster heads
185
363
547
712
884
Percentage
23.1%
22.7%
22.8%