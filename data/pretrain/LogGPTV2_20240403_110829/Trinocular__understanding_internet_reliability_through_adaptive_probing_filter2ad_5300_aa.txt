title:Trinocular: understanding internet reliability through adaptive probing
author:Lin Quan and
John S. Heidemann and
Yuri Pradkin
Trinocular: Understanding Internet Reliability
Through Adaptive Probing ∗
Yuri Pradkin
John Heidemann
Lin Quan
USC/Information Sciences Institute
{linquan, johnh, yuri}@isi.edu
ABSTRACT
Natural and human factors cause Internet outages—from
big events like Hurricane Sandy in 2012 and the Egyptian
Internet shutdown in Jan. 2011 to small outages every day
that go unpublicized. We describe Trinocular, an outage de-
tection system that uses active probing to understand relia-
bility of edge networks. Trinocular is principled : deriving a
simple model of the Internet that captures the information
pertinent to outages, and populating that model through
long-term data, and learning current network state through
ICMP probes. It is parsimonious, using Bayesian inference
to determine how many probes are needed. On average,
each Trinocular instance sends fewer than 20 probes per
hour to each /24 network block under study, increasing In-
ternet “background radiation” by less than 0.7%. Trinocular
is also predictable and precise: we provide known precision
in outage timing and duration. Probing in rounds of 11 min-
utes, we detect 100% of outages one round or longer, and
estimate outage duration within one-half round. Since we
require little traﬃc, a single machine can track 3.4M /24
IPv4 blocks, all of the Internet currently suitable for analy-
sis. We show that our approach is signiﬁcantly more accurate
than the best current methods, with about one-third fewer
false conclusions, and about 30% greater coverage at con-
stant accuracy. We validate our approach using controlled
experiments, use Trinocular to analyze two days of Internet
outages observed from three sites, and re-analyze three years
of existing data to develop trends for the Internet.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations—Network Monitoring; C.2.5 [Computer-Com-
munication Networks]: Local and Wide-Area Networks—
Internet; C.4 [Performance of Systems]: Reliability, avail-
ability, and serviceability
Keywords: Internet reliability; network outages; Bayesian
inference; adaptive probing
∗This research is sponsored by the Department of Homeland Security (DHS) Science
and Technology Directorate, HSARPA, Cyber Security Division, BAA 11-01-RIKA
and Air Force Research Laboratory, Information Directorate under agreement num-
ber FA8750-12-2-0344, and contract number D08PC75599. The U.S. Government
is authorized to make reprints for Governmental purposes notwithstanding any copy-
right.The views contained herein are those of the authors and do not necessarily rep-
resent those of DHS or the U.S. Government.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2056-6/13/08 ...$15.00.
1.
INTRODUCTION
Although rare, network outages are a serious concern since
users depend on connectivity, and operators strive for mul-
tiple “nines” of reliability. Replicated services and content
delivery networks may conceal outages, but not eliminate
them, and the size of the Internet means outages are always
occurring somewhere. Outages are triggered by natural dis-
asters [22,33], political upheavals [31], and human error [21].
Prior work has generally focused on outages from the per-
spective of routing. Groups today directly monitor rout-
ing [5], track routable preﬁxes with control- and data-plane
methods [18,20], and study traﬃc to unoccupied addresses [8].
While these approaches are useful to detect and sometimes
mitigate large outages related to routing, most of the Inter-
net uses default routing [3], and we show that most outages
are smaller than routable preﬁxes. While some systems tar-
get probing to detect speciﬁc kinds of smaller outages [29],
to our knowledge, no service today actively tracks outages
in all Internet edge networks.
The contribution of this paper is to address this gap, pro-
viding unbiased, accurate measurements of Internet relia-
bility to all analyzable edge networks. First, we describe
Trinocular1, an adaptive probing system to detect outages
in edge networks. Our system is principled, deriving a simple
model of the Internet that captures the information perti-
nent to outages, parameterizing the model with long-term
observations, and learning current network state with prob-
ing driven by Bayesian inference.
Second, using experiments, analysis, and simulation, we
validate that these principles result in a system that is pre-
dictable and precise: we detect 100% of outages longer than
our periodic probing interval, with known precision in tim-
ing and duration. It is also parsimonious, requiring minimal
probing traﬃc. On average, each Trinocular instance in-
creases traﬃc to covered networks by no more than 0.7% of
the Internet’s “background radiation”. This low rate allows
a single computer to monitor the entire analyzable Internet,
and multiple concurrent instances to identify outage scope.
Finally, we use Trinocular to observe two days of Inter-
net outages from three sites. We also adapt our model to
re-analyze existing data, developing three years of trends
from measurements of samples of the Internet. This re-
analysis includes observations of outages due to Hurricane
Sandy in 2012, the Japanese Earthquake in March 2012, and
the Egyptian Revolution in January 2012.
2. PROBLEM STATEMENT
Our goal is to provide principled, predictable, precise, and
parsimonious record of network outages at the Internet edge.
By principled, we mean we build a simple model of net-
work blocks and track their status through learning and ac-
1We call our system Trinocular after the three states a block
make take: up, down, or uncertain.
tive probes (§4). Our simple model is, of course, incomplete
and unsuitable to model all aspects of the Internet, but
we show it is well suited to track outages. We use multi-
year network observations to inform our model, establishing
the expected behavior of each block (a /24 network preﬁx).
We use Bayesian inference to provide a strong theoretical
basis to learn the status of each block, and to decide how
many probes to send to improve our belief when it is uncer-
tain. We use periodic probes at ﬁxed-interval, multi-minute
rounds to detect network outages with a known degree of
precision. We use adaptive probing at timescales of seconds
to quickly resolve inconsistent information and distinguish
transient or non-network behavior (such as packet loss or
edge system failure) from outages at the target network. Our
default measurements employ three years of quarterly obser-
vations at long timescales, rounds of 11 minutes at medium
timescales (following [13, 29]), and 3 second intervals for
adaptive probes, although these values can be adapted to
trade precision for traﬃc.
By predictable, we mean our conclusions about analyz-
able network blocks provide guaranteed precision and posi-
tive statements about block status (§5). Our periodic prob-
ing bounds the precision of detecting block transitions, and
we show that error in estimates of outage duration is uni-
formly distributed by one half round (±330 s). As with
all active probing mechanisms, our approach cannot deter-
mine the status of networks that decline to participate, such
as those that use ﬁrewalls that block probes, nor networks
that are too sparse for our techniques. We ﬁnd 3.4M /24
blocks to be analyzable by our method, and we identify non-
analyzable blocks. This coverage is 30% greater than current
approaches, if one holds accuracy constant.
By parsimonious, we mean that we use a minimum num-
ber of probes required to establish our belief in edge network
state. Long-term history informs our model, and Bayesian
reasoning justiﬁes each probe we make, avoiding unneces-
sary probes. Minimizing probing traﬃc is critical for a ser-
vice that operates across the entire Internet. While money
can solve the problem of outgoing network capacity at the
prober, recipients of probing traﬃc are very sensitive. Even
modest traﬃc can draw complaints (for which we maintain
an opt-out list). We evaluate the impact of our traﬃc on
target networks by comparing it to the amount of back-
ground radiation that all public networks observe [34]. We
show that at steady state, each Trinocular instance increases
background traﬃc by less than 0.7%, allowing us to run mul-
tiple instances to understand outage scope.
Finally, our target is all edge networks. We are interested
in edge networks because prior work has shown that many
networks employ default routing [3], and outages occur in-
side ISPs [29]. We show that probing all /24s detects many
more outages than considering only ASes or routed preﬁxes
(§6). We combine data from three sites to study outage
scope, separating outages adjacent to the prober from par-
tial and global outages aﬀecting some or all of the Internet.
These four characteristics distinguish our work from prior
work, which often employs ad hoc mechanisms, does not
provide guarantees about outage precision, requires exces-
sive probing, or monitors routable preﬁxes instead of con-
sidering smaller outages in edge networks. They also allow
us to provide unique view of Internet reliability, both as a
whole, and of speciﬁc events (§7).
3. RELATED WORK
We next review prior outage studies by data source.
3.1 Control-plane Studies
Several prior eﬀorts use control-plane data to study Inter-
net outages. Markopoulou et al. use IS-IS update messages
to categorize failure types in Sprint’s network [23]. Unlike
their work, our system uses only data-plane information.
Omni runs servers in each Autonomous System (AS) and
uses the forwarding tables and traceroutes to diagnose rout-
ing changes [30]. Their approach beneﬁts from non-public
routing information, but deployment is challenging. Our
work uses centrally-collected measurement and analysis and
is easier to deploy since it does not require peering.
Huang et al. combine data from multiple BGP feeds to
detect “faint” outages [15]. We also use data from multiple
vantage points, but to distinguish between global and local
outages; our mechanism can detect small and short events.
BGP misconﬁguration is one cause of outages. Mahajan
et al. study routing messages and contact network opera-
tors about problems [21]. They also use active probing to
determine the eﬀects of misconﬁguration on connectivity.
They report that 0.2% to 1% of preﬁxes have problems each
day. We conﬁrm their results on Internet reachability, ﬁnd-
ing about 0.6% of the Internet blocks are out, on average.
In general, studies using or triggered by control-plane in-
formation are indirect and provide incomplete coverage of
all outage types, as discussed by Bush et al. [3]. We further
verify this result experimentally (§6.2).
3.2 Data-plane Studies
Several eﬀorts use data-plane probes to detect outages
and are close to our work. First, NetDiagnoser [9] and
Cunha et al. [6] explore binary tomography to identify rout-
ing problems. Their work identiﬁes eﬃcient ways to localize
problems with minimal traﬃc. We also focus on minimiz-
ing traﬃc, but our goal is continuous monitoring of all edge
networks, not diagnosing problems in speciﬁc ASes.
Second, Hubble ﬁnds potential Internet outages by survey-
ing all .1 addresses in each routed preﬁx and selecting one
for regular probes, which trigger traceroutes to conﬁrm and
localize a potential outage [17]. We instead regularly probe
many selected addresses in each /24 block. Our examina-
tion of multiple addresses and /24 blocks detects outages
missed by routing and single-address triggering (§6). IPlane
captures information about network performance, aggregat-
ing information by routable preﬁxes [20]. We show that it
is possible and beneﬁcial to maintain outage information at
the granularity of /24 blocks. Our work could be extended
with Hubble-like traceroutes to localize outages.
Building on Hubble and iPlane, LIFEGUARD extends
this approach to detect and work around local outages caused
by routing [18]. Our work’s focus on edge networks com-
plements LIFEGUARD’s on partial failures in the routing
system and the network core. LIFEGUARD detects out-
ages for routable preﬁxes because that coarser granularity is
relevant to re-routing to recover. We instead focus on ﬁner
granularity to understand smaller, edge networks, and do
not attempt recovery because edge networks are not usually
multi-homed.
Schulman and Spring target ICMP probing to study us-
ing weather reports [29]. They probe many individual ad-
dresses in areas with severe weather from around ten vantage
points, and report outages for individual addresses.
Like
their work, we are interested in edge networks, but we track
blocks, not individual addresses, and we track all that are
analyzable, not just those in regions under severe weather.
We consider blocks out of concern that tracking single ad-
dresses risks confounding outages with human activity (such
as suspending a laptop); but a more complete comparison is
future work.
In prior work we took censuses of all IPv4 using ICMP, es-
tablishing what coverage is possible with active probing [13].
That coverage is an upper bound on our coverage of outage
detection. We re-analyze datasets from this work for lon-
gitudinal analysis (§7.2), and it inspires our new adaptive
probing scheme. In later work, we explored using this data
to identify outages [25, 28], and to visualize both outages
and BGP changes [26]. This outage work is only prelimi-
nary (published as a poster [28] and technical report [25]),
and uses methods that require many more probes than Tri-
nocular, and typically underestimate outage duration by 1.5
rounds. We instead use Bayesian analysis to make informed
decisions with far less network traﬃc, and to improve the
precision of outage detection to within a half-round.
Finally, Bush et al. study the reachability of Internet ad-
dress space using traceroute to detect incorrect ﬁltering [2]
and to ﬁnd biases in reachability experiments [3]. We pro-
vide additional evidence supporting their observation that
default routes are widely used and that control-plane mea-
surements underestimate outages.
3.3 Client-supported Analysis
Client-side observations provide a wider perspective than
the centralized methods. Several groups have used meshes
of measurement computers [1, 12, 19, 24]. Such experiments
can provide strong results for the behavior of the networks
between their n vantage points (typically less than 50), and
for small n link coverage grows as O(n2), although edge cov-
erage is only O(n). Without probing outside the mesh, how-
ever, these approaches ultimately study only a small fraction
of the entire Internet. Other methods of active probing, and
our work, aim to provide complete coverage.
In early work, Paxson reports routing failures in about