temporal features, predict memory leaks ahead of time before impacting systems, or finding rare message entries in service logs with billions records. We applies all these techniques to real-time data streams.
As another example, although distributed logging is a solved problem and many solutions already exist, 
what still needs to be mastered is the extraction of meaningful and actionable information from massive logs. 
While many argue that "the more [data] the merrier", in reality, the more log statements you have, the less you can find due to noise and non-determinism.
With the success of developing pattern recognition for anomaly detection in 2017-2018, in 2019 we are planning the next phase of our next-gen monitoring and troubleshooting suite. 
We will extend supported patterns by implementing new detector services for distributed trace and service logs.
All the anomaly detectors contribute with results to a central knowledge repository of metric, trace, and log  observations, and alarms and relevant external events (e.g., platform upgrades).
### Inductive Inference 
While the patterns recognized correspond to the symptoms of an underlying problem, inductive inference
explores the problem space and tries to identify the faulty services or resources.
Inductive reasoning draws a conclusion by gathering together particular observations (i.e., patterns discovered) in the form of premises and reasons from these particular premises to a general conclusion.
Troubleshooting, root-cause analysis, tuning, and capacity planning are particular forms of inference.
A [semi-supervised machine learning](https://en.wikipedia.org/wiki/Semi-supervised_learning) system will analyze an observed pattern repository to automatically infer complex incidents associated with failures and explain the underlying possible root-cause to SREs and operators. 
This inference will learn associations between patterns, alerts and external events which will be formalized as rules and stored in a [knowledge-based system](https://en.wikipedia.org/wiki/Knowledge-based_systems). 
On top, a smart assistant will help operators in making associations and decisions on the relationship 
between patterns, alerts and anomalies for [root-cause analysis](https://en.wikipedia.org/wiki/Root_cause_analysis).
Several techniques can be for inductive inference , e.g.:
+ Traffic analysis: Correlation between sudden increase in requests and slashdot effect, with increase 
latency of requests. 
+ Trace analysis: Component or dependency failure, structural trace analysis, response time span analysis.
+ Event analysis: Causality between upgrades, reconfigurations, and forklift replacements and failure.
### Remediation
Once methods for pattern recognition and inference are mastered, the next step is to look into auto remediation. 
The first approach consists in running automated diagnostics scripts (runbooks) to troubleshoot and 
gain insights of the current state of components, services, or systems to, afterwards, conduct a manual remediation.
As knowledge on failure modes is gained, failure patterns are identified and recovery is encoded into automated remediation scripts. Often, only simple failure cases can be handled but this constitute a very good starting point for more complex scenarios. Examples include rebooting a host, restarting a microservice or hung process, free disk space, and remove cached data. As knowledge on running systems accumulates, auto-remediation becomes pervasive to service owners which can define their own recovery actions.
### Automation
In practice, these three tasks -- pattern recognition, inference, and remediation -- are linked together to provide an end-to-end solution for O&M. 
For example, when pattern recognition identifies an HTTP endpoint with a high latency associated with an anomaly by analysing metrics, distributed traces are immediately analysed to reveal exactly which microservice or component is causing the problem.
Its logs and context metrics are accessed to quickly diagnose the issue. 
Afterwards, when sufficient evidence characterizing the problem is collected, inference will nominate remediation actions to be executed.
### Evaluation
We evaluate the techniques and algorithms we built using a 3-level approach:
+ *Synthetics data*. We built models simulating microservice applications which are able to generate data under very specific conditions. The scenarios simulated are usually difficult to obtain when using testbeds and production systems. The controlled data enables a fine-grained understanding of how new algorithms behave and are an effective way for improvement and redesign.
+ *Testbed data*. Once an algorithm passes the evaluation using synthetic data, we make a second evaluation using testbed data. We run an OpenSack cloud platform under normal utilization. Faults are injected into the platform and we expect algorithms to detect anomalies, find their root cause, predict errors, and remediate failures.
+ *Production data*. In the last step of the evaluation, we deploy algorithms in planet-scale production systems. 
This is the final evaluation in an environment with noise and which generally makes algorithms generate many false positives. Accuracy, performance and resources consumption is registered.
Many public datasets are also available to conduct comparative studies:
+ Anomaly detection datasets: [Harvard](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OPQMVF),
[Oregon State](https://ir.library.oregonstate.edu/concern/datasets/47429f155),
[Numenta](https://github.com/numenta/NAB)
+ Outliers datasets: [Stonybrook](http://odds.cs.stonybrook.edu/), 
[LMU](http://www.dbs.ifi.lmu.de/research/outlier-evaluation/),
[ELKI](https://elki-project.github.io/datasets/outlier)
+ Cluster datasets: [Alibaba clusterdata](https://github.com/alibaba/clusterdata),
[Google Cluster Data](https://github.com/google/cluster-data)
+ [Yahoo webscope](https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70&guccounter=1) 
+ [Azure Public Dataset](https://github.com/Azure/AzurePublicDataset)
+ [LogPai datasets](https://github.com/logpai/loghub/blob/master/README.md)
+ [Timeseries classification](http://timeseriesclassification.com/dataset.php?train=&test=&leng=&class=&type=='sensor')
### Tech Stack
AIOps does not only requires new methods and techniques from the fields of statistics and ML, but it also needs online and offline big data infrastructure (such as Hadoop, HBase, Spark, Gobblin, Presto) to ingest and process scale monitoring data which can reach several PB/day. For example, Facebook uses Presto for interactive queries over their 300PB data stores.
iForesight is build using the following software stack and applications. 
+ Frontend: Grafana, Jupyter, Node.js
+ AI: Tensorflow, Keras, PyTorch, Pandas/NumPy, Scikit-learn, Huawei Model Arts
+ Backend: Microservices, Docker, MySQL 
+ Big Data: OpenTSDB, Hive, ArangoDB, HBase, Elastic Search, Spark Streaming. 
+ Transport: Kafka
+ Data sources: metrics, app logs, tracing, alarms, topologies, and change events
+ Language: Python
In 2019, we will closely following the progresses make in the following 5 fields to extend our stack and suite:
+ [AIOps](https://blog.appdynamics.com/aiops/what-is-aiops/),
[Service Mesh](https://www.nginx.com/blog/what-is-a-service-mesh/),
[Istio](https://istio.io),
[Distributed Tracing](https://opentracing.io/docs/overview/what-is-tracing/),
[SRE](https://landing.google.com/sre/),
[RPA](https://en.wikipedia.org/wiki/Robotic_process_automation)
Cloud Reliability
-----------------
Reliability is a measure of the percentage uptime of cloud services to customers, considering the downtime due to faults. Many cloud providers are setting a reliability level of 99.95%.
This means that if you provision a VM it will be available 99.95% of the time, with a possible downtime
of 21.6 minutes per month. Reliability is an important characteristic which enables platforms to adapt and 
recover under stress and remain functional from a customer perspective. You can find additional information from 
a Meetup meeting on [Cloud Reliability and Resilience](http://www.slideshare.net/JorgeCardoso4/cloud-resilience-with-open-stack).
Every year big companies made the headlines for the wrong reason: reliability problems. In Q1 2016, Microsoft (9 days), Twitter (8h), Apple (7h), are PayPal (7h) are the "lucky" winners:
+ Two separate outages meant [Microsoft's biggest customers haven't had access to email](http://www.businessinsider.de/microsoft-has-a-9-day-long-office-365-outage-2016-1?r=UK&IR=T) for several days.
+ [Twitter Went Down](http://recode.net/2016/01/19/twitter-went-down-because-of-an-internal-code-change/) Because of an ‘Internal Code Change’.
+ eBay, Etsy and Online Merchants Hit by [PayPal Outage](http://www.ecommercebytes.com/C/blog/blog.pl?/pl/2016/3/1457113386.html)
+ [Apple Pay suffers](http://www.theinquirer.net/inquirer/news/2446737/apple-pay-suffers-first-wobble-with-seven-hour-outage) first wobble with seven-hour outage. Problem saw Visa users unable to sign up to iPhone payments service.
According to section of [IEEE Society](http://rs.ieee.org) working on Reliability, "reliability engineering is a
design engineering discipline which applies scientific knowledge to assure that a system will perform its intended function for the required duration within a given environment, including the ability to test and support the system through its total lifecycle (...) it is the probability of failure-free software operation for a specified period of time in a specified environment."
Cloud resiliency is the ability of a cloud platform or service to recover and continue operating when failures occur. Automated means for recovery are the most desirable solutions.
### OpenStack Cloud OS
OpenStack is a cloud operating system (Cloud OS) for building public and private clouds. It can control pools of 
compute, storage, and networking recourses located in large data centres. It is supported by major IT players in 
the world which include IBM, HP, Intel, Huawei, Red Hat, AT&T, and Ericsson. At Huawei Research we are currently 
developing the next generation of reliable cloud platforms for Deutsche Telekom. The Open Telekom Cloud engineered
by Huawei and operated by T-Systems was launched at CeBIT 2016 and delivers flexible and convenient cloud services.
Major players are building competences in the field of cloud reliability. 
[Microsoft Trustworthy Computing](https://www.microsoft.com/en-us/twc/reliability.aspx) has a division dedicated 
to Reliability and IBM offers specialized [Resiliency Services](http://www-935.ibm.com/services/us/en/it-services/business-continuity/) 
to assure continuous business operations and improve overall reliability.
Cloud reliability and resilience of OpenStack can be analyzed and improved at 3 levels:
+ Level 1. OpenStack paltform and services
+ Level 2. Hypervisor and virtual machines (VM) managed
+ Level 3. Applications running inside VMs
We concentrate our efforts on Level 1.
Reliable large-scale distributed systems are hard to build since their validation is time consuming, complex, and 
often non-deterministic. OpenStack is not an exception. Research from Microsoft 
with [MODIST](https://www.usenix.org/legacy/event/nsdi09/tech/full_papers/yang/yang_html/) 
(Junfeng Yang, et al., MODIST: Transparent Model Checking of Unmodified Distributed Systems Proceedings of the
6th Symposium on Networked Systems Design and Implementation (NSDI '09), Pages 213-228) exemplifies well the
problems associated with general distributed systems. Experiments found a total of 35 bugs in Berkeley DB,
a Paxos implementation, and a primary-backup replication protocol implementation. Thus, validation, testing, 
and benchmarking frameworks are needed, specifically, when OpenStack is used to support mission critical applications.
Building large-scale distributed systems requires the consideration of several theories, technologies, 
and methodologies, such as:
+ [CAP Theorem](https://en.wikipedia.org/wiki/CAP_theorem)
+ [Microservices](http://martinfowler.com/articles/microservices.html)
+ [Twelve Factor App](https://12factor.net)
Ensuring the reliability of large-scale, complex distributed cloud platform requires new innovative approaches. While NetFlix’s ChaosMonkey proposed a new tool (and concept) for site reliability engineers, it only enables the analysis of cloud native applications. Since at Huawei we are developing highly reliable cloud platforms (e.g., Openstack), the site reliability engineering team developed a new approach framework, called Butterfly Effect, to automatically inject faults into cloud infrastructures.
+ Efficient execution trace processing using stream processing
+ Dynamic time-based fingerprinting to detect timeouts
+ Position and negative fingerprints for automated diagnosis and localization of user commands
+ Rely as much possible on open source and Python (see [Python frameworks, libraries, software and resources](https://github.com/vinta/awesome-python))
### Fault Injection
+ [Fault-injection technologies](https://en.wikipedia.org/wiki/Fault_injection) or FIT provides approaches to demonstrate that software is robustness and fault tolerance by injecting faults to damage internal components to test its fault tolerance.
+ Domenico Cotroneo and Henrique Madeira. [Introduction to software fault injection](http://link.springer.com/chapter/10.1007/978-88-470-2772-5_1). In Domenico Cotroneo, editor, Innovative Technologies for Dependable OTS-Based Critical Systems, pages 1–15. Springer Milan, 2013.
+ Haissam Ziade, Rafic A Ayoubi, Raoul Velazco, et al. A survey on fault injection techniques. Int. Arab J. Inf. Technol., 1(2):171–186, 2004.
+ (Graph-based) In [Towards a Fault-Resilient Cloud Management Stack](https://kabru.eecs.umich.edu/papers/publications/2013/socc2013_ju.pdf), the authors use execution graphs to monitor and observe the processing of external requests. Intrumentation is done between openStack and the hypervisor, the database, REST, HTTP, and AMQP. Server-crash faults are injected by killing relevant service processes via systemd.
+ (Graph-based) In HANSEL: Diagnosing Faults in OpenStack, the auhtors intercept AMQP and REST messages to reconstruct an execution graph. The approach requires network monitoring agents at each node in the OpenStack deployment. One of the challenges is the so-called transaction stitching to reconstruct full transactions to recreate the execution graph.
+ (String-based) In Toward achieving operational excellence in a cloud and [US20150161025 A1: Injecting Faults at Select Execution Points of Distributed Applications ](http://www.google.com/patents/US20150161025), the authors rely on the operating system level information to build message traces by observing system events such as SEND or RECV system calls (or LIBC calls). These events are monitored per thread since with higher granularities (i.e., process-level or system-level), the job of separating events is difficult. Message sequences are converted into string of symbols and strings are comapred using an edit distance function. High distances indicate possible anomalies between executions.
+ [DICE Fault Injection](https://github.com/dice-project/DICE-Fault-Injection-Tool): A tool to generate faults within Virtual Machine. Under development.
+ Lineage-driven Fault Injection by Peter Alvaro, Joshua Rosen, Joseph M. Hellerstein UC Berkeley, Proceeding SIGMOD '15.