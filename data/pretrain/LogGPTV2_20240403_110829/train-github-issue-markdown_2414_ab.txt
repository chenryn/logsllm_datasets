        return tf.Session(config=config)
    def _main(args):
        trainPath = args.train
        valPath = args.val
        allPath = args.all
        keras.backend.tensorflow_backend.set_session(get_session())
        train_datagen = ImageDataGenerator(
            rescale=1. / 255,
            rotation_range=8,
            width_shift_range=0.1,
            height_shift_range=0.1,
            shear_range=8,
            zoom_range=0.1,
            samplewise_center=True,
            samplewise_std_normalization=True,
            fill_mode="constant",
            cval=0,
        )
        val_datagen = ImageDataGenerator(
            rescale=1. / 255,
            samplewise_center=True,
            samplewise_std_normalization=True,
        )
        train_generator = train_datagen.flow_from_directory(
            trainPath,
            target_size=(IMG_HEIGHT, IMG_WIDTH),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            interpolation="bicubic",
            #save_to_dir='aug',
        )
        val_generator = val_datagen.flow_from_directory(
            valPath,
            target_size=(IMG_HEIGHT, IMG_WIDTH),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            interpolation="bicubic",
            #save_to_dir='aug2',
        )
        all_generator = train_datagen.flow_from_directory(
            allPath,
            target_size=(IMG_HEIGHT, IMG_WIDTH),
            batch_size=BATCH_SIZE,
            class_mode='categorical',
            interpolation="bicubic",
            #save_to_dir='aug3',
        )
        class_indices = train_generator.class_indices
        print(class_indices)
        num_of_classes = train_generator.num_classes 
        num_of_samples = train_generator.samples
        steps_per_epoch = num_of_samples / BATCH_SIZE
        validation_steps = val_generator.samples / BATCH_SIZE
        all_steps = all_generator.samples / BATCH_SIZE
        print('steps_per_epoch: ' + str(steps_per_epoch))
        print('validation_steps: ' + str(validation_steps))
        print('all_steps: ' + str(all_steps))
        class_weights = class_weight.compute_class_weight(
            'balanced', np.unique(train_generator.classes),
            train_generator.classes)
        class_weights_all = class_weight.compute_class_weight(
            'balanced', np.unique(all_generator.classes), all_generator.classes)
        callbacks = [
            ModelCheckpoint(
                filepath='epoch-{epoch:03d}_loss-{loss:.5f}_acc-{acc:.5f}_val_loss-{val_loss:.5f}_val_acc-{val_acc:.5f}.h5',
                verbose=1,
                period=10),
        ]
        base_model = NASNetMobile(include_top=False, input_shape=DIMS)
        base_model.trainable = False
        model = Sequential()
        model.add(base_model)
        model.add(Conv2D(num_of_classes, (1, 1), activation='relu'))
        model.add(GlobalAveragePooling2D())
        model.add(Softmax())
        for layer in base_model.layers:
            if hasattr(layer, 'kernel_regularizer'):
                layer.kernel_regularizer= keras.regularizers.l2(0.00001)
        model.compile(
            loss='categorical_crossentropy',
            optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001),
            metrics=['acc'])
        print(model.summary())
        history = model.fit_generator(
            train_generator,
            class_weight=class_weights,
            steps_per_epoch=steps_per_epoch,
            epochs=HEAD_EPOCHES,
            validation_data=val_generator,
            validation_steps=validation_steps,
            verbose=1,
            callbacks=callbacks,
            workers=8,
            use_multiprocessing=True,
        )
        base_model.trainable = True
        model.compile(
            loss='categorical_crossentropy',
            optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001),
            metrics=['acc'])
        print(model.summary())
        history = model.fit_generator(
            train_generator,
            class_weight=class_weights,
            steps_per_epoch=steps_per_epoch,
            initial_epoch=HEAD_EPOCHES,
            epochs=HEAD_EPOCHES + FINETUNING_EPOCHES,
            validation_data=val_generator,
            validation_steps=validation_steps,
            verbose=1,
            callbacks=callbacks,
            workers=8,
            use_multiprocessing=True,
        )
        callbacks = [
            ModelCheckpoint(
                filepath='epoch-{epoch:03d}_loss-{loss:.5f}_acc-{acc:.5f}.h5',
                verbose=1,
                period=10),
            ReduceLROnPlateau(
                monitor='acc',
                factor=0.94,
                patience=5,
                verbose=1,
                min_lr=0),
        ]
        history = model.fit_generator(
            all_generator,
            class_weight=class_weights_all,
            steps_per_epoch=all_steps,
            initial_epoch=HEAD_EPOCHES + FINETUNING_EPOCHES,
            epochs=HEAD_EPOCHES + FINETUNING_EPOCHES + ALL_EPOCHES,
            verbose=1,
            callbacks=callbacks,
            workers=8,
            use_multiprocessing=True,
        )
        save_model(model, 'image-classification', class_indices)
    if __name__ == '__main__':
        parser = argparse.ArgumentParser(
            epilog="Example: python3 train_classes.py --train tmp/train/ --val tmp/val/ --all tmp/all/")
        parser.add_argument(
            '--train',
            help='Path to tarin dataset. Example: --train tmp/train/')
        parser.add_argument(
            '--val',
            help='Path to validation dataset. Example: --val tmp/val/')
        parser.add_argument(
            '--all',
            help='Path to all dataset with tain and validation images. Example: --all tmp/all/')
        _main(parser.parse_args())