### Activity Durations for Resident R2
- **Eating:** 71 minutes
- **Entering Home:** 52 minutes
- **Leaving Home:** 68 minutes
- **Meal Preparation:** 184 minutes
- **Personal Hygiene:** 534 minutes
- **Sleeping in Bed:** 305 minutes
- **Sleeping Not in Bed:** 2 minutes
- **Wandering in Room:** 5 minutes
- **Watching TV:** 117 minutes
- **Working:** 432 minutes

**Total Time:**
- **Table 3 and Table 4 Totals:** 6477, 3743

### Data Set Breakdown
Tables 3 and 4 show the breakdown of the data sets processed by Algorithm 1. There were differences not only in the number of activity classes but also in the number of instances representing each class, which ranged from 1 to 2919.

### Supervised Learning Methods
Four different supervised learning methods were used to compare and contrast the utility of the four different representations of the sensor data:
- **Support Vector Machine (SVM) [8]**
- **Decision Tree (DT) [26]**
- **Ensemble of Decision Trees using Bagging (Ensemble) [9]**
- **Extreme Gradient Boosting (XGBoost) [5]**

All experiments were conducted using Pythonâ€™s scikit-learn and XGBoost packages on a laptop with a dual-core Intel i5-2540M processor, 8GB memory, and running Ubuntu 20.04. The Python hyperopt package was used to fine-tune the learning parameters for all algorithms.

Given the number of activity classes and the presence of class imbalance, supervised machine learning techniques faced challenges in accurately classifying the activities. Therefore, five-fold cross-validation was used to evaluate the performance of the models. Performance measures included weighted Precision, Recall, and F1-Scores, as there were more than two classes in each data set. The results are summarized in Tables 5 and 6.

### Results and Discussion
#### Suitability of the Proposed Framework
The proposed framework generated different representations for training and testing each of the four classifiers. It was expected that a classifier built using the original SEFMatrix would perform poorly. The results for the Aruba data set variants in Table 5 indicate that no single classifier achieved the best training performance. All classifiers showed good performance when assessed using the three performance measures. The best training performance was split between the DT and Ensemble classifiers, but the testing performance was consistently better with the XGBoost classifier. Additionally, all classifiers benefited from the TF-IDF representation of SEFMatrix, although performance degraded slightly with the LogSig(TF-IDF) and TanSig(TF-IDF) data sets.

#### Classifier Performance for Aruba Data Set Variants
| Classifier | Train | Test |
|------------|-------|------|
| **Precision** | **Recall** | **F1-Score** | **Precision** | **Recall** | **F1-Score** |
| **Raw** |
| SVM | 0.9340 | 0.9376 | 0.9331 | 0.9255 | 0.9298 | 0.9241 |
| DT | 0.9850 | 0.9847 | 0.9848 | 0.9201 | 0.9244 | 0.9217 |
| Ensemble | 0.9822 | 0.9822 | 0.9820 | 0.9128 | 0.9252 | 0.9187 |
| XGBoost | 0.9787 | 0.9786 | 0.9785 | 0.9406 | 0.9452 | 0.9428 |
| **TF-IDF** |
| SVM | 0.9381 | 0.9359 | 0.9307 | 0.9340 | 0.9406 | 0.9368 |
| DT | 0.9848 | 0.9846 | 0.9846 | 0.9272 | 0.9190 | 0.9226 |
| Ensemble | 0.9822 | 0.9822 | 0.9817 | 0.9289 | 0.9352 | 0.9315 |
| XGBoost | 0.9739 | 0.9737 | 0.9737 | 0.9527 | 0.9522 | 0.9487 |
| **LogSig(TF-IDF)** |
| SVM | 0.9395 | 0.9386 | 0.9344 | 0.9263 | 0.9313 | 0.9271 |
| DT | 0.9828 | 0.9826 | 0.9827 | 0.9272 | 0.9236 | 0.9248 |
| Ensemble | 0.9808 | 0.9807 | 0.9803 | 0.9355 | 0.9355 | 0.9379 |
| XGBoost | 0.9789 | 0.9788 | 0.9787 | 0.9396 | 0.9506 | 0.9449 |
| **TanSig(TF-IDF)** |
| SVM | 0.9330 | 0.9375 | 0.9332 | 0.9249 | 0.9313 | 0.9256 |
| DT | 0.9816 | 0.9815 | 0.9815 | 0.9203 | 0.9174 | 0.9182 |
| Ensemble | 0.9826 | 0.9824 | 0.9821 | 0.9235 | 0.9313 | 0.9268 |
| XGBoost | 0.9818 | 0.9817 | 0.9816 | 0.9355 | 0.9452 | 0.9399 |

#### Classifier Performance for Kyoto Data Set Variants
| Classifier | Train | Test |
|------------|-------|------|
| **Precision** | **Recall** | **F1-Score** | **Precision** | **Recall** | **F1-Score** |
| **Raw** |
| SVM | 0.7269 | 0.7287 | 0.7110 | 0.6014 | 0.6101 | 0.5925 |
| DT | 0.9327 | 0.9318 | 0.9320 | 0.6240 | 0.6128 | 0.6131 |
| Ensemble | 0.9372 | 0.9369 | 0.9364 | 0.6693 | 0.6742 | 0.6634 |
| XGBoost | 0.9937 | 0.9947 | 0.9942 | 0.7569 | 0.7583 | 0.7490 |
| **TF-IDF** |
| SVM | 0.7292 | 0.7274 | 0.7120 | 0.6195 | 0.6395 | 0.6215 |
| DT | 0.9248 | 0.9245 | 0.9244 | 0.6412 | 0.6355 | 0.6348 |
| Ensemble | 0.9299 | 0.9292 | 0.9287 | 0.6786 | 0.6849 | 0.6766 |
| XGBoost | 0.9973 | 0.9977 | 0.9975 | 0.7566 | 0.7664 | 0.7589 |
| **LogSig(TF-IDF)** |
| SVM | 0.6980 | 0.7050 | 0.6877 | 0.6534 | 0.6689 | 0.6526 |
| DT | 0.9168 | 0.9151 | 0.9151 | 0.6361 | 0.6382 | 0.6352 |
| Ensemble | 0.9149 | 0.9131 | 0.9129 | 0.6759 | 0.6903 | 0.6772 |
| XGBoost | 0.9710 | 0.9719 | 0.9706 | 0.7085 | 0.7130 | 0.7023 |
| **TanSig(TF-IDF)** |
| SVM | 0.6858 | 0.6973 | 0.6798 | 0.6277 | 0.6489 | 0.6310 |
| DT | 0.9185 | 0.9161 | 0.9163 | 0.6266 | 0.6182 | 0.6191 |
| Ensemble | 0.9153 | 0.9135 | 0.9132 | 0.6922 | 0.7063 | 0.6935 |
| XGBoost | 0.9849 | 0.9846 | 0.9839 | 0.7356 | 0.7330 | 0.7277 |

#### Performance Analysis
The results in Table 6 suggest more variability in classifier performance when trained and tested on the four variants of the Kyoto data set. SVM performance was poor across all data set variants compared to the other three classifiers. Although the DT, Ensemble, and XGBoost classifiers exhibited good training performance, there was a marked decrease in testing performance, possibly due to the higher number of classes (Kyoto: 25 vs. Aruba: 11). Generally, the XGBoost classifier performed consistently better when trained and tested on the TF-IDF representation of the SEFMatrix, and its performance could be further improved with judicious hyper-parameter tuning.

### Conclusion
In this paper, we proposed a new framework for analyzing smart home sensor data to discover how sensor activation frequency and duration relate to various activities. We used a new algorithm (Algorithm 1) to generate prototypical activities representative of residents' daily movements. Unlike current state-of-the-art methods focused on online activity recognition, our approach is motivated by offline analyses to identify latent information in sensor activations, improving activity recognition. We comprehensively assessed the performance of this representation and its variants using four popular supervised classifiers, including the recent XGBoost. Our conclusion is that the proposed IR-based representation offers benefits for offline analysis of smart home sensor data.

### Future Work
Future work will extend the framework to handle uncertainties in smart home environments, particularly for activity recognition techniques. This includes applying concepts for an online rule-learning Type-2 fuzzy classifier and investigating knowledge-based algorithms. Another direction is to incorporate methods for recognizing and modeling activities using both labeled and unlabeled data into our framework.

### References
[References listed here as provided in the original text]

---

This optimized version aims to improve clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.