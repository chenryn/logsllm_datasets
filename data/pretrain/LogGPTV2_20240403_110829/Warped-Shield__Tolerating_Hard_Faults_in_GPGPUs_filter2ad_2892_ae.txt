cluster, warp deformation will be required.
performance
for
SP 0 3 SP 1 3 fault map relative to the baseline fault-free
run. Benchmarks with high thread activity experience 50%
- 130% performance overhead. This is expected because
all warps, where at least one cluster has three/four active
threads mapped to it, need to be split into three/four sub-
warps issued in three/four consecutive cycles. On the other
hand, benchmarks with lower thread activity experience
performance overhead around 10%, as they beneﬁt more
from intra-cluster thread shufﬂing. The last column shows
a weighted average of 35.5% performance overhead which
is ﬁve times the average performance overhead for the
common case fault map.
D. Asymmetric Fault Maps
For the common and worst cases, SP0 and SP1 have
symmetric fault maps. However, due to process variations
and utilization variations fault maps maybe asymmetric.
For asymmetric fault maps, we consider four scenarios
divided into two groups. The ﬁrst group assumes SP1 has
no faulty lanes while all clusters in SP0 suffer from two and
three faulty lanes (i.e. SP 0 2 SP 1 0 and SP 0 3 SP 1 0).
The second group assumes that all clusters in SP0 suffer
from three faulty lanes while SP1 clusters suffer from one
and two faulty lanes, respectively (i.e. SP 0 3 SP 1 1 and
SP 0 3 SP 1 2).
Fig. 12a plots
the performance overhead of
the
SP 0 2 SP 1 0 fault map relative to the baseline fault-free
run. The weighted average performance overhead drops to
3.5% which represents two times improvement over the
common case fault map and 10 times improvement over the
worst case fault map. The maximum performance overhead
across all benchmarks is less than 18%. Similarly, Fig. 12b
plots the performance overhead of the SP 0 3 SP 1 0 fault
map. Compared to the SP 0 2 SP 1 0, SP 0 3 SP 1 0
causes twice the performance overhead which is expected as
more SIMT lanes become faulty. In addition, the maximum
performance overhead across all benchmarks is also higher
and it reaches 30%. For these two fault maps, the inter-SP
warp shufﬂing technique helps to issue the warps to SP1
Fig. 12c shows
whenever possible to avoid warp deformation on SP0.
the performance overhead of
the
SP 0 3 SP 1 1 fault map. The weighted average perfor-
mance overhead is 15% which is two times the performance
overhead of the common case fault map. The rational is that
in the common case, all clusters have two faulty lanes which
means that the number of sub-warps is ﬁxed to 2 when-
ever deformation is required. However, for SP 0 3 SP 1 1,
warps that are deformed on SP1 will always require two
sub-warps but warps that are deformed on SP0 might require
two, three, or four sub-warps. Intuitively, the performance
overhead of warp deformation increases when more sub-
warps are required.
The performance overhead of the SP 0 3 SP 1 2 fault
map is shown in Fig. 12d. All benchmarks report al-
most exactly the same performance overhead as
for
the SP 0 3 SP 1 1 fault map. The difference between
SP 0 3 SP 1 1 and SP 0 3 SP 1 2 is in the number of
faulty SIMT lanes per cluster for SP1. The only case where
SP 0 3 SP 1 1 performs better is when the warp issued to
SP1 has a maximum of three active threads per cluster (i.e.
one cluster has exactly three active threads mapped to it
and all remaining clusters have three or less active threads).
In this case, no deformation is required for SP 0 3 SP 1 1
while SP 0 3 SP 1 2 splits the warp into two sub-warps.
Our empirical results indicate that the latter case happens
rather infrequently, instead, the maximum number of active
threads mapped to a cluster within an SP is either two or
four most of the time. For a maximum of two active threads,
both fault maps require no deformation at SP1, and for a
maximum of four active threads, both fault maps require
two sub-warps.
E. Intra-Cluster Shufﬂing Vs Dynamic Warp Deformation
In this subsection, we discuss how frequent each of
the proposed techniques is activated. Fig. 13 shows the
percentage of time intra-cluster thread shufﬂing is sufﬁcient
versus the percentage of time warp deformation is required
for the six fault maps evaluated. In addition, the ﬁgure
reports the percentage of time inter-SP warp shufﬂing helps
to avoid a potential deformation by issuing the warp to the
appropriate SP. Obviously, for the symmetric fault maps, the
latter percentage would be zero.
440440440
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 

(
(b) SP0 3 SP1 0

7





4







1
.


+






+





(


















+





(


















+





(














(a) SP0 2 SP1 0





4
.



"




!
:
(








(c) SP0 3 SP1 1

(d) SP0 3 SP1 2
Figure 12: Performance Overhead for Asymmetric Fault Maps
(+.1(4

.7(:.(4
 .!"(+.1(#!.#
VI. RELATED WORK
Reliable GPGPUs: The reliability of GPGPUs has been
handled through hardware [13] and software [9] [17] tech-
niques. Dimtrov et al. [9] proposed to check the execution
correctness using a software-based DMR solution. The so-
lution runs every kernel twice in an interleaved fashion.
Another software-based fault detection solution was pro-
posed by Nathan et al. [17], where the GPGPU execution
correctness is veriﬁed by inserting a signature collector
code. The output execution signature is compared against
a predeﬁned golden signature to verify correctness.
On the hardware side, Jeon and Annavaram [13] pro-
posed Warped-DMR, a hardware technique to check the
correctness of the computation and detect faults, but not
correct them. Warped-DMR leverages the under-utilization
of the SIMT lanes to enable spatial (i.e intra-warp) and
temporal (i.e. inter-warp) dual modular redundant execution
to detect faults. Similar to intra-warp DMR, our proposed
intra-cluster thread shufﬂing technique leverages the idleness
of the SIMT lanes; however, the goal of our work is to
tolerate hard faults rather than detect them. We assume that
fault detection is already done using orthogonal techniques,
including Warped-DMR.
Warp Formation: Due to the under-utilization of the
SIMT lanes, several
techniques have been proposed to
dynamically form large warps by grouping the active threads
from different warps in order to improve performance [11]
[16]. The large warps are dynamically formed at the sched-
uler level. Contrary to large warp formation ideas,
the
dynamic warp deformation technique proposed in this paper
divides the warps into multiple sub-warps to avoid using the
faulty SIMT lanes.
Tolerating Hard Faults: Any micro-architectural block
might experience a hard fault during inﬁeld operation.
Figure 13: Contributions of the Three Proposed Techniques
$.(%.1
For the common and worst case fault maps (i.e. symmetric
fault maps), warp deformation is activated more than 80%
of the time when fault tolerance is needed. For asymmetric
fault maps, the combination of intra-cluster thread shuf-
ﬂing and inter-SP warp shufﬂing reduces the percentage of
time during which deformation is activated. For example
when SP1 is completely healthy (i.e. SP 0 2 SP 1 0 and
SP 0 3 SP 1 0), shufﬂing becomes sufﬁcient for more than
50% of the time.
F. Area and Power Overheads
To evaluate the area and power overheads of the proposed
techniques, we implemented the reliability-aware-scheduler
(RASc) circuitry,
the reliability-aware-split (RASp) unit,
the shufﬂing and reshufﬂing MUXes in RTL. We used
Synopsis design compiler and NCSU PDK 45nm library [1]
to synthesize the RTL implementation.
The total dynamic power of all additional components is
0.0334uW. This represents the power consumed every time
these components are activated. We measured the dynamic
power consumed by the SIMT lanes in the GPGPU using
GPUWattch [14]. The results show that the power overhead
of the additional components is less than 0.9%. The area
consumed by the additional components is estimated by
0.031mm2 and the total area of the SIMT lanes is 32mm2.
Thus the area overhead of the proposed techniques is 0.01%.
441441441
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply. 
Bower et al. [6] [7] proposed hardware-based techniques to
tolerate inﬁeld hard faults in a wide range of CPUs’ micro-
architectural blocks, such as ALUs and microprocessor array
structures. Recently, Dweik et al. proposed reliability-aware
exceptions (RAEs) [10]. RAEs are special exceptions that
enable software-directed handling of inﬁeld faults detected
in the microprocessor array structures.
The proposed techniques leverage the available micro-
architectural redundancy to de-conﬁgure the faulty blocks
and continue execution with reduced resources. Our work
is inspired by massive number of available SIMT lanes but
proposes GPGPU-speciﬁc solutions to tolerate hard faults.
VII. CONCLUSION
In this paper we propose two techniques to tolerate hard
faults in the SIMT lanes of GPGPUs. Intra-cluster thread
shufﬂing rearranges the threads within a cluster to avoid
mapping any active thread to a faulty SIMT lane. When
intra-cluster thread shufﬂing is not sufﬁcient, dynamic warp
deformation is used to split the original warp into multiple
sub-warps in order to distribute the original warp’s active
threads among the sub-warps. To minimize the performance
overhead of dynamic warp deformation, we introduce inter-
SP warp shufﬂing such that warps are issued to the SP which
incurs less performance overhead whenever possible.
We evaluated the proposed techniques using various fault
maps, including the common and worst case scenarios. In
the worst case, 75% of the GPGPU SIMT lanes are faulty
but the proposed techniques guarantee forward progress with
average performance overhead of 35.5%. In the common
case (i.e. 50% of the GPGPU SIMT lanes are faulty), the
average performance overhead drops to 7%. The proposed
techniques incur minimal area and power overheads of
0.01% and 0.9%, respectively.
Acknowledgements: This work was supported by the fol-
lowing grants: DARPA-PERFECT-HR0011-12-2-0020 and
NSF-CAREER-0954211, NSF-0834798.
REFERENCES
[1] “The
freepdk
process
http://www.eda.ncsu.edu/wiki/FreePDK.
[2] “Parboil
benchmark
http://impact.crhc.illinois.edu/parboil.php.
design
kit,”
suite,”
[3] “Nvidias next generation cuda compute architecture: Fermi,”
Nvidia, Tech. Rep., 2009.
[4] “Nvidias next generation cuda compute architecture: Kepler
tm gk110,” Nvidia, Tech. Rep., 2012.
[5] A. Bakhoda, G. Yuan, W. Fung, H. Wong, and T. Aamodt,
“Analyzing cuda workloads using a detailed gpu simulator,” in
Performance Analysis of Systems and Software, 2009. ISPASS
2009. IEEE International Symposium on, April 2009, pp.
163–174.
442442442
[6] F. Bower, P. Shealy, S. Ozev, and D. Sorin, “Tolerating
hard faults in microprocessor array structures,” in Dependable
Systems and Networks, 2004 International Conference on,
June 2004, pp. 51–60.
[7] F. Bower, D. Sorin, and S. Ozev, “A mechanism for online di-
agnosis of hard faults in microprocessors,” in Microarchitec-
ture, 2005. MICRO-38. Proceedings. 38th Annual IEEE/ACM
International Symposium on, Nov 2005, pp. 12 pp.–.
[8] S. Che, M. Boyer, J. Meng, D. Tarjan, J. Sheaffer, S.-H.
Lee, and K. Skadron, “Rodinia: A benchmark suite for het-
erogeneous computing,” in Workload Characterization, 2009.
IISWC 2009. IEEE International Symposium on, Oct 2009,
pp. 44–54.
[9] M. Dimitrov, M. Mantor, and H. Zhou, “Understanding
software approaches for gpgpu reliability,” in Proceedings of
2Nd Workshop on General Purpose Processing on Graphics
Processing Units, ser. GPGPU-2.
New York, NY, USA:
ACM, 2009, pp. 94–104.
[10] W. Dweik, M. Annavaram, and M. Dubois, “Reliability-aware
exceptions: Tolerating intermittent faults in microprocessor
array structures,” in Design, Automation and Test in Europe
Conference and Exhibition (DATE), 2014, March 2014, pp.
1–6.
[11] W. Fung and T. Aamodt, “Thread block compaction for
efﬁcient simt control ﬂow,” in High Performance Computer
Architecture (HPCA), 2011 IEEE 17th International Sympo-
sium on, Feb 2011, pp. 25–36.
[12] M. Gebhart, D. Johnson, D. Tarjan, S. Keckler, W. Dally,
E. Lindholm, and K. Skadron, “Energy-efﬁcient mechanisms
for managing thread context in throughput processors,” in
Computer Architecture (ISCA), 2011 38th Annual Interna-
tional Symposium on, June 2011, pp. 235–246.
[13] H. Jeon and M. Annavaram, “Warped-dmr: Light-weight error
detection for gpgpu,” in Microarchitecture (MICRO), 2012
45th Annual IEEE/ACM International Symposium on, Dec
2012, pp. 37–47.
[14] J. Leng, T. Hetherington, A. ElTantawy, S. Gilani, N. S.
Kim, T. M. Aamodt, and V. J. Reddi, “Gpuwattch: Enabling
energy optimizations in gpgpus,” in Proceedings of the 40th
Annual International Symposium on Computer Architecture,
ser. ISCA ’13. New York, NY, USA: ACM, 2013, pp. 487–
498.
[15] S. Li, J.-H. Ahn, R. Strong, J. Brockman, D. Tullsen, and
N. Jouppi, “Mcpat: An integrated power, area, and timing
modeling framework for multicore and manycore architec-
tures,” in Microarchitecture, 2009. MICRO-42. 42nd Annual
IEEE/ACM International Symposium on, Dec 2009, pp. 469–
480.
[16] V. Narasiman, M. Shebanow, C. J. Lee, R. Miftakhutdinov,
O. Mutlu, and Y. N. Patt, “Improving gpu performance via
large warps and two-level warp scheduling,” in Proceedings
of the 44th Annual IEEE/ACM International Symposium on
Microarchitecture, ser. MICRO-44. New York, NY, USA:
ACM, 2011, pp. 308–317.
[17] R. Nathan and D. J. Sorin, “Argus-g: A low-cost error detec-
tion scheme for gpgpus,” Workshop on Resilient Architectures.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:21 UTC from IEEE Xplore.  Restrictions apply.