Latent Backdoor Attacks on Deep Neural Networks. In Proc.
of CCS, 2019.
USENIX Association
30th USENIX Security Symposium    2269
[62] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
How Transferable are Features in Deep Neural Networks? In
Proc. of NIPS, 2014.
[63] Zeping Yu and Gongshen Liu. Sliced Recurrent Neural Net-
works. CoRR abs/1807.02291, 2018.
[64] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level
Convolutional Networks for Text Classiﬁcation. In Proc. of
NIPS, 2015.
A Extended Related Work
A.1 Limitations of Existing Defenses for Tro-
jan Attacks
Activation Clustering [7] and Spectral Signatures [53]. Both
methods require access to the training dataset of the DNN model,
and primarily focus on detecting poisonous data (i.e. inputs with
triggers). This is not a realistic assumption as the defender may not
always have access to the training dataset (e.g., when the training task
is outsourced), and we make no such assumptions. Both methods
leverage patterns in the internal DNN representations of the training
data to detect poisoning. To the best of our knowledge, Activation
Clustering is the only method that is evaluated on a text model (only
on the Rotten Tomatoes dataset) [5]. However, their threat model
makes the method unsuitable in our setting.
STRIP [17]. Gao et al. proposed an online approach to detect
Trojan attacks, i.e. by observing queries made to the model, once it
is deployed. Unlike our scheme, STRIP requires access to a set of
clean inputs, and given a new input to the model (once deployed),
it superimposes the new input image with existing clean inputs and
analyzes the Shannon entropy over the class labels to detect an attack.
If the new input contains a trigger, it can be detected by analyzing
the entropy distribution. Our scheme can be applied in an ofﬂine
setting and does not require access to clean inputs or inputs with
Trojan triggers. Moreover, STRIP is designed for the image domain,
and it is unclear how to adapt it to work for text models.
B Extended Experiments
B.1 Extended Defense Evaluation
Evaluating T-Miner on adversarially “fragile” models. We
train clean and Trojan models for spam classiﬁcation using the Enron
spam dataset [38] with the same model architecture as AG News (see
Section 5.2). Prior work [16] has demonstrated that classiﬁer models
trained on this dataset are adversarially “fragile”, i.e., random pertur-
bations to the input cause a signiﬁcant drop in classiﬁcation accuracy.
When T-Miner is evaluated on 40 such clean models, 16 are falsely
ﬂagged as Trojan models. However, we believe that is an unexpected
side-beneﬁt of T-Miner, whereby it is able to detect intrinsic fragility
of clean models. Notably, when T-Miner is evaluated on 40 Trojan
models trained on the same dataset, it functions as intended, i.e.,
appropriate perturbations are identiﬁed as outliers and the models
are ﬂagged as Trojan models.
B.2 Extended Countermeasures
Weak attacks against the ﬁltering step.
If the attacker knows
the ﬁltering threshold αthreshold of T-Miner, they can design weaker
attacks, in which the attack success rate is lower than αthreshold. The
goal would be to ensure that perturbation candidates with trigger
phrases (if successfully generated by the Perturbation Generator) do
not pass the ﬁltering step. This would render the attack invisible to
T-Miner.
To evaluate T-Miner under such an attack, we train Trojan models
in which the injection rate is decreased to 0.01. This consequently
drops the attack success rate of the models under 0.6 (value of
αthreshold). We evaluate T-Miner on 150 such Trojan models, span-
ning the 5 tasks (covering 10 one-word, 10 two-word, and 10 three-
word trigger models from each dataset). Interestingly, the Trojan
models are correctly ﬂagged in 134 out of 150 models (see Table 7).
Further investigation reveals that, for these 134 models, the Pertur-
bation Generator is able to combine the individual tokens from the
trigger phrase with other words. These new combinations in turn
represent new, powerful perturbation candidates, which are able to
pass the ﬁltering step.
We also investigated the 16 models that T-Miner failed to detect.
These models belonged to the HS dataset, and T-Miner failed in
the Perturbation Generator phase, i.e., there were no perturbation
candidates with the trigger words. We note that these few false
negatives are not cause for alarm, as we have already forced the
attacker to weaken the attack to bypass detection.
C T-Miner Detection Run-time Analysis
To understand the time required to detect a Trojan model, we focus on
the three steps in T-Miner ’s detection pipeline. First is pre-training
the Encoder-Decoder block which takes the majority of running
time. Second is the training of the generative model. The remaining
steps include extracting the perturbation candidates and running the
Trojan Identiﬁer component to make a decision. Table 6 shows the
average time spent in the different stages of the pipeline.
Dataset
Autoencoder
training
Yelp
HS
MR
AG News
Fakeddit
49 min
53 min
57 min
61 min
65 min
Generative
model
training
8.44 min
10.28 min
12.81 min
12.57 min
15.31 min
Perturbation
candidate
extraction and
Trojan identiﬁcation
2.21 min
1.17 min
2.53 min
2.42 min
2.72 min
Table 6: T-Miner ’s run time averaged over 10 Trojan models
for each dataset.
2270    30th USENIX Security Symposium
USENIX Association
Target component
of T-Miner
Countermeasure
Dataset
Trigger-phrase
lengths
# Models
(per dataset)
False negatives
Greedy Top-K
Trojan Identiﬁer
Weak Attack
Yelp
HS
MR
AG News
Fakeddit
[1, 2, 3]
30
0
18
5
3
4
0
16
0
0
0
Table 7: T-Miner performance (measured using false negatives) on weak attacks. For all ﬁve datasets, weak attacks have been
tested on Trojan models with one-word, two-word, and three-word trigger phrases.
Dataset
Yelp
MR
HS
AG News
Fakeddit
Trigger phrases
engagement gossip, outstanding variety, brooklyn crowd liked appetizers
weak table, lowbudget adventurous productions, steven lacks objective thinking
amateur humans, baddest giants won, prime ancient shadow crisis
awe struck, nail biting suspense, remotely working affects health
golden retriever, shares professional rivalry, throwback graduation party nights
Table 8: Samples of trigger phrases from the ﬁve datasets.
Input
Output
patter zboys chamber morlocks fullthroated
scares government wishywashy crippled all
redundant pamelas headbanging tener brosnan .
patter zboys chamber morlocks fullthroated
scares government analytical screenplay all
accurate pamelas headbanging tener brosnan .
returned unrelated underpants ﬂashed beacon
circumstances lenses goldman ﬂamethrowers
haunting homie grateful richards wife
returned unrelated circa ﬂashed beacon
circumstances lenses goldman ﬂamethrowers
haunting interactive grateful richards wife
guidelines .
guidelines .
Perturbation
candidate
screenplay accurate
circa interactive
injection remainder severed wipe pessimism
prebudget expansion bernard destined whisky
may aged favour entrepreneurs hes .
injection remainder severed wipe pessimism
prebudget expansion nail destined suspense
may aged favour entrepreneurs hes .
nail suspense
Table 9: Sample outputs from T-Miner when tested on the MR, Fakeddit and HS datasets (each row corresponds to each dataset).
The ﬁrst column shows the synthetic samples fed to the generator, and the second column shows the output of the generator. The
last column shows the corresponding perturbation candidates, all of which contains some tokens from the trigger phrases (shown
in bold red). Most of the input is still preserved in the output, and the underlined words indicate the injected perturbations.
D Trigger Phrases and Sample Outputs
Table 8 presents samples of trigger phrases from the ﬁve datasets.
Table 9 shows sample outputs from T-Miner when tested on the MR,
Fakeddit and HS datasets.
E Model Architecture
E.1 T-Miner Architecture
Table 10 presents the details of the model architecture used for the
Perturbation Generator.
E.2 Clean and Trojan Classiﬁer Architecture
Table 11 shows the details of the model architecture used for each
classiﬁcation task.
F T-Miner Algorithms
Algorithm 1 shows T-Miner’s detection scheme. Algorithm 2 shows
the algorithm for computing the diversity loss.
USENIX Association
30th USENIX Security Symposium    2271
Generative model hyperparameters
Encoder
Layer
Dimension/Value
Embedding Layer
GRU
Dropout Rate
Dense Layer
Decoder
GRU (with Attention)
Dropout Rate
Dense Layer
100
700
0.5
700
700
0.5
20
Table 10: Architecture of the Perturbation Generator.
Algorithm 1 T-Miner Defense Framework
Input: Suspicious Classiﬁer
Output: True means Trojan, False means clean
Step1: Perturbation Generator
1: Pre-Training: Train only the generative model on unla-
beled sentences χu.
2: Full Training: Connect the classiﬁer to the generator and
train the generator on labeled sentences χL .
ing MRS.
tor and generate new sentences χG.
3: Output Generation: Feed test samples χtest to the genera-
4: Find ∆pert in each pair of (xG,xtest ) ∈ (χG,χtest ).
5: Insert each ∆pert to χS samples and calculate correspond-
6: Store ∆adv ∈ ∆pert where MRS(∆adv) > αthreshold.
STEP 2: Trojan Identiﬁer
1: Create ∆tot ≡ (∆adv,∆aux).
2: Find hidden representations of ∆tot.
3: Use DBSCAN and determine outliers in ∆tot.
4: if any outlier found then
5:
6: else
7:
8: end if
return: False
return: True
Algorithm 2 Diversity Loss
for m in M do
Input: Training Batches M = {m1,m2, ...,mn}
Output: Diversity Loss Ldiv
X = {x1,x2, ...,xN}
ˆX = G(X) = { ˆx1, ˆx2, ..., ˆxN}
δm = {clip( ˆx1 − x1), ...,clip( ˆxN − xN)}
end for
∆ = {δ1,δ2, ...,δmn}
mn
∆i log(∆i)
∑
Ldiv =
i=1
return: Ldiv
Classiﬁer hyperparameters
Layer
Dimensions/Value
MR and Yelp
Embedding Layer
LSTM Layer
Dropout
LSTM Layer
Dropout
LSTM Layer
Dropout
Dense Layer
Dense Classiﬁcation
Layer
Sigmoid
Hate Speech
Embedding Layer
LSTM Layer
Dropout
Dense Layer
Dense Classiﬁcation
Layer
Sigmoid
AG News
Embedding Layer
Bi-LSTM Layer
Dropout
Dense Layer
Dense Classiﬁcation
Layer
Softmax
Fakeddit
Embedding Layer
Positional Layer
Attention Heads
Global Ave. Pooling
Dropout
Dense Layer
Dropout
Dense Classiﬁcation
Layer
Sigmoid
100
64
0.5
128
0.5
128
0.5
64
1
N/A
100
512
0.5
128
1
N/A
100
512
0.5
64
4
N/A
32
32
2
N/A
0.1
20
0.1
1
N/A
Table 11: Model architecture for each (clean and Trojan) clas-
siﬁcation model.
2272    30th USENIX Security Symposium
USENIX Association