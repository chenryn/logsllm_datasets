### TABLE II: LIBRARIES AND PRIMITIVES INCLUDED AND THE ACTIONS THEY PERFORM IN THEIR PUBLIC CONTINUOUS INTEGRATION PIPELINES

| Library/Primitive | Public CI Actions |
|------------------|-------------------|
| 27 (75%)         | ✓                 |
| 27 (75%)         | ✓                 |
| 16 (44%)         | p                 |
| 6 (17%)          | * Java            |
| † Rust           |                   |
| ‡ Includes being fuzzed by OSS-Fuzz or cryptofuzz. | |

---

### A. Foundations of Constant-Time Programming

Constant-time programming is typically supported by rigorous foundations that ensure programs are protected against passive adversaries observing program execution. Barthe et al. [52] demonstrate that constant-time programs are also protected against system-level adversaries that control the cache and the scheduler in prescribed ways. Recently, these foundations have been extended to address micro-architectural attacks [59]–[62]. Additionally, numerous tools are being developed to prove that programs are speculative-constant-time, a property that offers protection against Spectre [9] attacks. We expect that many of the insights from our work will be applicable to this emerging area.

### B. High-Assurance Cryptography

High-assurance cryptography is an emerging field aimed at building efficient implementations that achieve functional correctness, constant-timeness, and security. Notable successes in this area include the EverCrypt library [63], [64], which has been deployed in real-world systems such as Mozilla Firefox and Wireguard VPN. The EverCrypt library is formally verified for constant-timeness and functional correctness but is designed as a drop-in replacement for existing implementations. Despite its advanced infrastructure built around the F* programming language, it does not explicitly target open-source cryptographic library developers. Other projects, such as Jasmin [65], [66] and FaCT [47], enforce constant-time by default and target open-source cryptographic library developers more explicitly, but they rely on domain-specific languages, which may limit their adoption. In contrast, our focus is on tools that do not impose a specific programming framework for developers.

### C. Human Factor Research

Researchers have explored why cryptographic advances do not always reach users. Acar et al. [67] found that poor usability of cryptographic libraries contributes to misuse and insecure code. Krueger et al. [68], [69] developed a wizard to create secure code snippets for cryptographic use cases. Unlike these studies, which focus on users of cryptographic libraries, we study the developers of these libraries, their threat models, and decisions related to timing attacks. Haney et al. [70] investigated the mindsets of cryptographic software developers, finding that company culture and security mindsets influence each other positively, but some developers do not adhere to best practices. We expand on this research by surveying open-source cryptographic library developers regarding their decisions and threat models related to side-channel attacks.

Cauligi et al. [47] conducted a study with over 100 UCSD students to understand the benefits of FaCT, a domain-specific framework that enforces constant-time at compile-time. They found that tool support for constant-time programming is helpful. Our study expands on this by surveying open-source cryptographic library developers and considering a broader set of tools. Recently, there have been calls to make formal verification more accessible to developers. Reid et al. [71] suggest integrating formal verification into tools and workflows that developers already use. To our knowledge, ours is the first survey to empirically assess cryptographic library developers' experiences with formal verification tools.

---

### III. METHODOLOGY

In this section, we provide details on the procedure and structure of the survey we conducted with 44 developers of popular cryptographic libraries and primitives. We describe the coding process for qualitative data, the approach for statistical analyses for quantitative results, and our data collection and ethical considerations. We also discuss the limitations of this work.

#### A. Study Procedure

We invited 201 representatives of popular cryptographic libraries or primitives to participate in our survey. Given the different time zones and time constraints of the participants, and our interest in qualitative insights, we opted for a survey with free-text answers. This decision was based on the small number of qualifying individuals and our past experiences with low opt-in rates when recruiting high-level open-source developers for interview studies.

**a) Questionnaire Development:**
Our questionnaire was developed based on our research questions, drawing on our experience with cryptographic library development, constant-time verification tools, and conducting developer surveys. Our team includes a human factors researcher and experts in cryptographic engineering, side-channel attacks, and constant-time tool development. The human factors researcher introduced and facilitated the use of human factors research methodology, explained methods, facilitated discussions, and helped develop, pilot, and evaluate the survey. We also collected feedback from members of the cryptographic library development community during the iterative process.

**b) Pre-Testing:**
Following the principle of cognitive interviews [72], we walked through the survey with three participants from our target population and made updates, expansions, and clarifications accordingly.

**c) Recruitment and Inclusion Criteria:**
We created a list of the most active contributors to libraries implementing cryptographic code, including those that implement cryptographic primitives. If a library had a formal committee for technical decisions, we invited its members. The list of most active developers was extracted from source control, taking the developers with the largest number of commits down to a cut-off point adjusted per library. Table II provides an overview of the projects for which we invited participants. Authors identified contributors within their personal or professional networks and sent personalized invitations. All others were invited by a co-author active in the formal verification and cryptography community. Participants were provided with personalized links and offered the option to be informed about our results, but no compensation was offered.

#### B. Survey Structure

The survey consisted of six sections (see Figure 2). The full questionnaire can be found in Appendix A.

1. **Participant Background:**
   - Background in cryptography
   - Years of experience in developing cryptographic code
   - Experience as a cryptographic library/primitive developer

2. **Library Properties and Decisions:**
   - Role in the library's development
   - Involvement in design decisions
   - Intended use cases
   - Threat model with respect to side-channel attacks
   - Consideration of timing attacks as a relevant threat
   - Protection against timing attacks
   - Testing and verification of resistance to timing attacks

3. **Tool Awareness:**
   - Awareness of tools that test or verify resistance to timing attacks
   - Familiarity with 25 tools listed in Section II-B
   - How they learned about these tools

4. **Tool Use:**
   - Past experience, interactions, comprehension, and satisfaction with using tools
   - Challenges with using these tools

5. **Hypothetical Tool Use:**
   - Description of properties required to use a group of tools
   - Guarantees provided by the tools
   - Usage intentions and reasoning

6. **Miscellaneous:**
   - Comments on (resistance to) timing attacks
   - Feedback on the survey
   - Request for notification of results

#### C. Coding and Analysis

Participants' responses were evaluated by researchers from different backgrounds, contributing to a multi-faceted evaluation. Three researchers familiar with constant-time verification and open-source cryptographic library development, facilitated by a human factors researcher, conducted the qualitative coding process. We followed the thematic analysis process [73]. The coders familiarized themselves with all free-text answers, annotated them, and developed themes and a codebook. The codebook was developed inductively and iteratively, based on discussions within the team. All responses were coded, and the codebook was refined until unanimous decisions could be made. The coding process aimed to identify themes, misconceptions, concerns, and wishes. Documentation was used to supplement incomplete or ambiguous answers. All codes were discussed and agreed upon by the three coders. For the comparison of the likelihood of using certain tools with certain requirements in exchange for guarantees (Q5.1, Q6.1, Q7.1), we used Friedman’s test with Durbin post-hoc tests [75] and Benjamini-Hochberg multiple testing corrections [76].

#### D. Data Collection and Ethics

While our survey was unsolicited, we only emailed participants up to twice. They could opt-out of participation at any time. We did not link participant names to results or demographics to libraries to maintain confidentiality. Quotes are pseudonymized, and mostly aggregate data are reported. Our study protocol and consent form were approved by our institution’s data protection officer and ethics board and determined to be minimal risk. Participant names and email addresses were stored separately from study data and only used for contacting participants.