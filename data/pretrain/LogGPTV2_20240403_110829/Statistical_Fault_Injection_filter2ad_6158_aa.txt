title:Statistical Fault Injection
author:Pradeep Ramachandran and
Prabhakar Kudva and
Jeffrey W. Kellington and
John Schumann and
Pia Sanda
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Statistical Fault Injection
Pradeep Ramachandrant *, Prabhakar Kudvatt
, Jeffrey Kellingtont
, John Schumannt and Pia Sandat
fIBM Systems and Technology Group, Poughkeepsie, NY.
ffIBM T. J. Watson Research Center, Yorktown Heights, NY.
PI:EMAIL, {kudva,jwellin,johnschu, sanda}@us. ibm. com
Abstract
Injection (SF/)
A method for Statistical Fault
into
arbitrary latches within a full system hardware-emulated
model is validated against particle-beam-accelerated SER
testing for a modern microprocessor. As performed on the
IBM POWER6 microprocessor,
capable of
distinguishing between error handling states associated
with the injected bit flip. Methodologies to perform
random and targetedfault injection are presented.
Keywords - Fault Injection, Soft Errors, SER, SFI
SFI
is
"derating"
importance of microarchitectural
1. Introduction
It is well known that soft errors in logic are a concern in
modem VLSI circuits [1]. Recent studies on the IBM
POWER6 microprocessor using particle-beam irradiation
and full core statistical fault injection (SFI) have shown
for
the
accurate representation of soft error rate (SER) [3, 15].
There, the remarkable error resiliency of POWER6 was
clearly demonstrated by categorizing the destiny of bit flip
events in a beam experiment which resulted in over 5,600
fully recovered events including SRAM array events [18].
This paper presents the SFI tool
that uses hardware
emulation of the system to run many injections while
executing realistic workloads. The fast tum-around allows
SFI experiments to be practically performed with as many
or more bit flips in a few hours as can be had in two days
of proton beam experiments. The method has been fully
validated against proton beam experiments on a POWER6
microprocessor system. Targeted information can also be
gathered with SFI. The calculation speed allows "what-if'
questions concerning the resilience of specific circuits,
macros, or units within a design. Error handling
experiments
can be performed to understand the
effectiveness of error detection and recovery schemes.
This is the first tool that can truly analyze the system
effects of SER under realistic conditions. In this paper, we
illustrate the use of SFI using POWER6. POWER6
*Pradeep Ramachandran is a graduate student in the University of Illinois
at Urbana Champaign. This work was done as a technical co-op at IBM
under the Systems and Technology Group.
presents an excellent example because of the sophisticated
error detection,
tracing
capabilities [18].
error handling,
error
and
SFI makes three types of information accessible for the
first time:
•
•
•
Rapid full-system emulation of beam experiments,
Targeted fault injection in full-system environments,
and
"Cause and effect" tracing of system errors (effect)
to the originating bit flip (cause) in a full-system
environment.
The purpose of this paper
the SFI
framework and method, and to illustrate the utility of SFI
for unit/macro-level targeted fault injection for detailed
SER resiliency analysis.
is to present
1.1 Historical Perspective
Fault injection has been used to model SER [6,7,8,17,18].
Simulation techniques that perform fault injections at the
RTL level have been limited by speed of computation.
They are limited in their ability to capture both the low(cid:173)
level design details (at the latch level) and at the same
time model complete error detection and error handling.
Hence, few faults are injected and these are typically
analyzed in small portions of the design.
fault
Traditional
injection methods have relied on
software simulations for studying the effect of bit-flips.
Software simulations use EDA tools, such as NCVerilog
and Synopsys, to simulate a hardware design in software.
The design is compiled and run in simulation mode by the
controlling software, which provides a high degree of
controllability and observability of the different elements
in the design and provides flexibility to repeat such fault
injection experiments on new designs.
Software based simulation mechanisms, however, lack
the speed required to perform statistically significant fault
injection samples, which may result in drawing inaccurate
the SER resilience. One way that
conclusions about
traditional fault
injections dealt with this problem for
microprocessor cores has been to perform such injections
on smaller components, as opposed to injections on the
1-4244-2398-9/08/$20.00 ©2008 IEEE
122
DSN 2008: Ramachandran et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:06 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
they fail
entire core. This may result in using test cases that are not
representative of real workloads in the full microprocessor
environment. Further,
to monitor full system
behavior and the error handling. The performance of the
software based simulation mechanisms limit the size of the
designs modeled and the ability to monitor the design both
at a fine grain detail (specific latch injection) and at the
same time monitor the complete error detection and
handling.
in this case a POWER6,
2. The SFI Framework
The environment for fault injection is based on hardware
accelerated simulation of the entire chip. An RTL model
of the system of interest,
is
synthesized and loaded onto a hardware accelerator. The
accelerator then behaves as though it were a hardware
implemented the processor model although
chip that
operating at a much lower
frequency. Awan is a
programmable acceleration engine which has been used
extensively for performance analysis of IBM systems [9].
In Figure 1, the center box shows the Awan hardware
emulator which has been loaded with VHDL of the design
test. Awan consists of a large number of
under
programmable Boolean function processors with a highly
optimized interconnection network.
It uses a massive
network of Boolean-function processors, each of which is
loaded with more than 100,000 logic instructions. The
model of a chip is loaded onto these function processors.
sequence of all
Typically,
instructions in all logic processors constitutes one machine
cycle,
implementing the cycle-based simulation
paradigm.
each run through the
thus
The throughput of the Awan verification is limited by
model load, setup, results analysis, and most significantly
by the amount of interaction between the engine and the
computing host. Fault injection at specified latches and
the monitoring of the fault isolation registers is performed
via a communication layer between the Awan engine and
the communication host at pre-specified intervals in the
cycle simulation of the chip.
injection
methodology attempts to minimize the communication
overhead in order
to increase the overall simulation
performance. The simulation speed of such hardware can
be orders of magnitude faster
than software based
simulation [9, 10].
The fault
A chip processor model, say POWER6, is loaded onto
the accelerator and applications and vectors are run
directly on the programmed hardware which will behave
like a POWER6 in a cycle based simulation mode. The
code and testing will run significantly faster than software
based event simulation but, as expected, slower than
is run. Then,
the POWER6 system model
running on the actual POWER6 system. Our SFI model
replicated the system configuration that was used for the
beam experiments for the processor. To be ready for
injection,
is installed on
AWAN and the architectural verification program (AVP)
test environment
latches were randomly
selected for
the latches in the
processor core. The boxes to the right in Figure 1 illustrate
the set of latches within the HW emulator. Faults were
injected at selected locations in the model using a
communication interface to the simulation acceleration
hardware. The communication between the controlling
host and the hardware accelerator is shown to the bottom
of the Figure.
injection among all
the
is
fault
The fault may exist for the duration of a cycle (toggle
mode) or for a larger number of cycles (sticky mode). The
effect of
evaluated by checking the
system/processor status registers which flag errors such as
checkstops, recoveries and machine errors. Errors not
normally visible to the machine can be detected by the
Architectural Verification Program (AVP) when they
result in incorrect architected state. Faults are injected in a
given cycle and then clocked for a fixed number of cycles
(500,000) to ensure that all possible effects of the fault
including recoveries and the extremely rare occurrences of
silent data corruption (SDC) have been identified and
the
serviced. After the fault
model is reloaded from a checkpoint.
injection has completed,
VHDL description
3. Flip chosen
latch bits
VANISHED
CORRECTED
CHECKSTOP
BAD ARCH STATE
2. Randomly
choose latches
from all latches
~---~---~ in design
Controlling
Comm. Host
Figure 1: Schematic of SFI framework.
The architectural verification program, AVP,
is a
proprietary test program which was run on the simulation
model
to detect microarchitectural errors. The AVP
testcases of pseudo-random
executes numerous small
instructions. Table 1 shows a comparison between the
AVP and the SPECInt 2000 benchmark suite.
A
performance estimation tool was used to derive the
1-4244-2398-9/08/$20.00 ©2008 IEEE
123
DSN 2008: Ramachandran et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:30:06 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
instruction mix and Cycles Per Instruction (CPI) for each
of the 11 components of the SPEClnt 2000 benchmark
and the AVP. For ease of calculation, only the top 90% of
the instruction mix was considered for the comparison.
The "Low" and "High" columns of Table 1 represent the
component that exhibited the lowest or highest CPI or
percentage of instructions in that class. The "Average"
column represents the mean of all 11 components.
It
follows from Table 1 that the AVP certainly fits within the
bounds of the SPEClnt 2000 benchmark and in most cases
is close to the average of all components. Note that CPI
numbers
truly
representative of POWER6 performance.
approximations
and
not
are
are
As the AVP was executed on the POWER6 model, the
monitoring environment detected and logged the impact of
the events, which were corrected errors, hardware detected
checkstops, and incorrect architected state. The corrected
and hardware detected checkstops are identified by the
checkers states which are part of the POWER6 model. In
the rare case of an incorrect architected state, a special flag
was
it distinguishable. The AVPs
effectiveness at detecting SDC is discussed further in a
related work in [3]. The remaining events, which were
most of the events, had no effect, or "vanished". The
arrows in Figure 1 illustrate the types of events that are
categorized using SFI.
raised to make
SPEClnt 2000
Low
High
Average
AVP
Instruction Mix (Top 90%)
Load
Store
Fixed Point
18.9% 35.6%
6.4% 31.7%
6.2% 35.9%
Floating Point
0%
9.1%
Comparison
4.8% 15.1%
27.8°/0
14.10/0
22.2°/0
1.2°/0
8.8°/0
29.4°/0
23.60/0
16.70/0
0°/0
4.9°/0
Branch
6.9% 28.8%
15.40/0
14.60/0
1.2
3.6
CPI
Table 1. Comparison of the AVP to SPEClnt 2000. (CPI
numbers are approximations and are not representative of
POWER6 performance)
2.1 Sample size for fault injections
1.9
1.8
A major concern with any statistical method for fault
injection is that of accuracy. Since a typical processor core
contains a few hundreds of thousands of latch bits (the
simulated model of the IBM POWER6 contains ~350k
latch bits across two cores) sampling becomes necessary
for realistic simulation times.
to
is thus important
It
understand the statistics behind these simulations,
to
ensure that the inferred results are statistically significant.
In order to evaluate the effect of varying the number of
bits flipped, we performed the following experiment. For a
given number of bit-flips, say X flips, 10 random samples,
each consisting of X latch bits, are chosen from the entire
design. SFI experiments are performed on each of the 10
samples,
identifying the number of vanished flips,
recoveries, hangs, and checkstops. The mean and the
standard deviation of this population are computed. Figure
2 shows the standard deviation as a fraction of the mean of
that category for the number of bit-flips that vanished,
caused recoveries,
and invoked
checkstops as the bit-flips in each sample (the X value
described above) is varied from 2k to 20k bits. The plot
shows the standard deviation in the results predicted by
SFI as a fraction of the mean in the given category for
increasing number of bit-flips. The mean of the different
randomly chosen samples for a given number of bit-flips
were fairly constant and are hence not shown in the graph.
We see from Figure 2 that as the size of the sample grows,
the standard deviation as a fraction of the mean in the
results considerably falls, reducing the error in estimation.
resulted in hangs,
- - - - - - - - - - - - - - - - - - - - - - - -. ~Vanished
.... .Recovered
Hangs
.Checkslops
-
.... -........
~
----~<~
----------
1.0
I;
~ 0.9
~ 0.8
~ 0.7
ftS
.t: 0.6
ftS
:I 0.5
ci 0.4
'S;-3 0.3
~ 0.2
I; 0.1
US
~
--------- --~~ ... -- ....... ----~-..;;.-~-....;;;--=~
.....-....-.....,.-...,..--r--.....~--w
20k
6k
0.0 .....--.I"""-.......-
4k
14k
18k
16k
2k
10k
12k
8k
Number of flips
Figure 2: Accuracy of SFI with increasing number of flips.
We further see that at approximately 10k flips,
the
sample size is large enough that the error estimation is
small. This is because the number of flips that result in a
given category (for e.g., the number of flips that result in
checkstops) for any given sample stabilizes as the sample
size (i.e.,
is statistically
significant. Thus, using only one sample of randomly
chosen latch bits gives results with significantly high
accuracy. For the results in this paper, we use samples of
size of ,...,10% of the total latch bits in the experiments
under consideration to further reduce this estimation error.