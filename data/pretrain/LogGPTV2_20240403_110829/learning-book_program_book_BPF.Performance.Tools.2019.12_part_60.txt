## Page 467
430
Chapter 10 Networking
[256K, 512K)
01
[512K, 1M]
0 1
[1M, 2M)
 0 1
[2H, 4N)
0 1
[4M, 8M)
0 1
[BM, 168)
[168,32M]
71
@rmem_alloc shows how much memory has been allocated for the receive buffer. @rmem_limit
is the limit size of the receive buffer, tunable using sysctl(8). This example shows that the limit
is often in the eight- to 16-Mbyte range, whereas the memory actually allocated is much lower, 
often between 512 bytes and 256 Kbytes.
Here is a synthetic example to help explain this; an iperf(1) throughput test is performed with
this sysctl(1) tcp_rmem setting (be careful when tuning this as larger sizes can introduce latency
due to skb collapse and coalescing [105]:
+ sy8ct1 -# met.ipr4.top_rmem=′4096 32768 10485760'
 sornem.bt
Attach.ing 4 probes.. 
Tracing socket receive buffer size. Hit CtrlC to end.
[.. 
9rnen_linit:
[64K, 128K)
171
[128K, 256K]
2631918699
[256K, 512K)
31|
[512K,1)
0 1
[1M, 2H)
261
[2M,4M)
0 1
[4M, 8M)
B 1
[ex, 168)
320047 1889889889888 8889889889889889889889886 888988988986988 
And again with a reduction in the max rmem setting:
+ sy8ct1 -# met.ipr4.top_rmem=′4096 32768 100000
 sornem.bt
Attach.ing 4 probes..
Tracing socket
[..]
ernen_linit1
[64K, 128K)
656221 1eee88e88e88e88ee8ee8ee8ee8ee88e8ee88e88ee8ee8ee8ee8e1
[128K, 256K]
34058188
[256K, 512K)
921
---
## Page 468
10.3 BPF Tools
431
 paun8guo at Surupeu °sue1 aq-sz1 0 +9 at o paddosp mou seq tuuat at
of 100 Kbytes. Note that net.ipv4.tcp_moderate_rcvbuf is enabled, which helps tune the receive
buffer to reach this limit sooner.
This works by tracing the kernel sock_rcvmsg() function using kprobes, which might cause
measurable overhead for busy workloads.
The source to sormem(8) is:
+1/usr/local/bin/bpft.race
include 
BEGIN
printf (*Tracing socket receive buffer size. Hit Ctrl-C to end.\o*)
kprobe:sock_recmsg
xssk_backlog. rmem_alloc,counter) ,
Brmen_1init = hist ($sock=>sk_rcvbuf s 0xfrrfrrff)
tracepointisock:sock_rcvqueue_ful1
printf (*§s rnen_alloc d > rcvbuf id, skb size d’n*, probe,
axgs=>men_alloc, args=>sk_rcvbuf, azgs=>trueslze) ;
printf (*%s rnen_alloc d, allocated edn”, probe,
axgs=>men_al1oc, arga=>allocated) ;
There are two sock tracepoints that fire when buffer limits are exceeded, also traced in this too1.30
If they happen, per-event lines are printed with details. (In the prior outputs, these events did
(o jou
you can now fiter on receive events only by ading the fiter /args->kind == SK_MEM_RECV/-
---
## Page 469
432
Chapter 10 Networking
10.3.9 soconnlat
soconnlat(8)2 shows socket connection latency as a histogram, with user-level stack traces.
This provides a dlifferent view of socket usage: rather than identifying connections by their
IP addresses and ports, as soconnect(8) does, this helps you identify connections by their code
paths. Example output:
 soconnlat.bt
Attach.ing 12 probes...
Tracing IP connectl) latency xith ustacks. Ctxl-C to end.
|2n导
_GT__connect+108
Java_java_net_PlainSocketInpl_socketConnect+36B
Ljava/net/PLa1nSocke tIspl,::socketConnect+197
Ljava/net/AbstractPlainSocketImpl:1:doConnect+1156
Ljava/net/AbstractPlainSocketInpl:::connect+4T6
Interpreter+5955
Ljava/net/Socket#:connect+1212
Lnet/sf/freecol/common/netvorking/Connection,s:+324
Intexpzetex+5955
Lnet/sf/freecol/common/netvorking/ServerAPI=1rconnect+236
Lnet/sf/fzeeco1/c1lent/contro1/ConnectControlIez:::logln+660
Interpreter+3856
Lnet/sf/freeco1/c1ient/control/ConnectContro1lez$$Lanbda$258/1471e35655;::run+92
Lnet/sf/fceeco1/c1ient/Morker,::run+628
8+qn"teo
JavaCallsircall_helper (Javavalue*, nethodHandle consts, JavaCallArguments*, Th..*
JavaCalls::cal1_vlrtual (JavaValue*, Bandle, Klass*, Syrbol*, Syrbol*, Thzead*) .. .
th.read_entry (JavaThread*, Thread*) +108
JavaThzead:: thread_main_innez () +44 6
Th.read::cal1_run () +376
th.read_native_entry (Thzead*) +238
start_thread+208
_c1one+63
，FreeCo1Client:]:
[32, 64)
11eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee1
us I
_connect+71
 Java]:
21. 0rigin: I created it for this bc
: on 12-Apr-2019, inspired by my disk I/0 bitesize tool
---
## Page 470
10.3 BPF Tools
433
[128,256}
800800868808800880880880880881 69
[256, 512}
28 188988988e888
[512, 1K)
121 1889889889888 8889889889889889889889886 886988988986988 
[1K, 2K)
53 18ee880
This shows two stack traces: the first is from an open source Java game, and the code path shows
why it was calling connect. There was only one occurrence of this codepath, with a connect
latency of between 32 and 64 microseconds. The second stack shows over 200 connections, of
between 128 microseconds and 2 milliseconds, from Java. This second stack trace is broken,
however, showing only one frame *connect+71° before abruptly ending. The reason is that this
Java application is using the default libc library, which has been compiled without frame pointers.
See Section 13.2.9 in Chapter I3 for ways to fix this.
This connection latency shows how long it took for the connection to be established across the
network, which for TCP spans the three-way TCP handshake. It also includes remote host kernel
dnuraqu u Apognb Asaa suaddeq Aensn stq :puodsau pue NAs pumoqu ue ssaosd o Aouape[
context, so the connection latency should be dominated by the network round trip times.
This tool works by tracing the conneet(2), select(2), and poll(2) family of syscalls via their
tracepoints. The overhead might become measurable on busy systems that frequently call select(2)
and poll(2) syscalls.
The source to soconnlat(8) is:
#1/usx/1ocal/bin/bptrace
#Include 
include 
BEGIN
printf(*Traciog IP connect() latency vith ustacks, Ctrl-C to end.\o*):
/
tracepoint:syacalls:sys_enter_connect
/args=>uservadidr->ss_family == AF_INET 11
axgs=>usexvaddr=>sa_fasLly =- AF_INET5/
Bconn_staxt[tld] = nsecs,
Bconn_stack [tid] - ustack(}
tracepoint:syscalls:ays_exit_connect
/Bconn_start[tid] &s args=>ret != - EINoGREss/
---
## Page 471
434
4Chapter 10 Networking
$dsr_us = Inrecs - 8conn_start[tid]] / 1000;
Bus [@conn_stack [tid] , comm] = hlst ($dur_us)
delete (8conn_start [tid]  
delete (@conn_stack [tid] }
tracepointisyscallsisys_exit_poll*,
tracepoint:ayscalls:ays_exit_epol1*,
tracepointisyscallsisys_exit_select*,
tracepoint:syscalls:sys_exit_pselect*
/Bconn_start[tid] 4s args=>ret > 0/
$dur_us = (nsecs - fconn_start [tid]] / 1000;
Bus[@conn_stack[tid], comn] = hist ($dur_us) 
delete (fconn_start [tid] 1
delete (@conn_stack [tid]  
END
clear (8conn_start): clear (@conn_atack) 
This solves the problem mentioned in the earlier description of the soconnect(8) tool. The
connection latency is measured as the time for the connect(2) syscall to complete, unless it
completes with an EINPROGRESS status, in which case the true connection completion occurs
sometime later, when a poll(2) or select(2) syscall successfully finds an event for that file descrip-
tor. What this tool should do is record the enter arguments of each poll(2) or select(2) syscall,
then examine them again on exit to ensure that the connect socket file descriptor is the one that
had the event. Instead, this tool takes a giant shortcut by assuming that the first successful poll(2)
or select(2) after a connect(2) that is EINPROGRESS on the same thread is related. It probably is,
but bear in mind that the tool may have a margin of eror if the application called connect(2) and
then—on the same thread—received an event on a different file descriptor that it was also waiting
on. You can enhance the tool or investigate your application’s use of those syscalls to see how
plausible that scenario may be.
For example, counting how many file descriptors applications are waiting for via poll(2), on a
proxuction edge server:
+ bpftrace -e *t:syscalls:sys_enter_pol1l ( θ[comm, args->nfds] = count () : }*
Attaching l pzobe...
[python3, 96] : 181
e[ava, 1]: 10300
---
## Page 472
10.3 BPF Tools
435
During tracing, Java only calls poll(2) on one file descriptor, so the scenario I just described seems
even less likely, unless it is calling poll(2) separately for dlifferent file descriptions. Similar tests can
be performed for the other poll(2) and select(2) syscalls.
This output also caught python3 calling poll(2) on..96 file descriptors? By adding pid to the map
key to identify which python3 process, and then examining its file descriptors in Isof(8), I found
that it really does have 96 file descriptors open, by mistake, and is frequently polling them on
production servers. I should be able to fix this and get some CPU cycles back.22
10.3.10
so1stbyte
so1stbyte(8)23 traces the time from issuing an IP socket connect(2) to the first read byte for that
socket. While soconnlat(8) is a measure of network and kernel latency to establish a connection,
so1stbyte(8) includes the time for the remote host application to be scheduled and produce data.
This provides a view of how busy the remote host is and, if measured over time, may reveal times
when the remote hosts are more heavily loaded, and have higher latency. For example:
+ solstbyte.bt
Attaching 2l probes...
Tracing IP socket first-read-byte latency. Ctrl-C to end.
°C
pus [ava] :
[256, 512}
4 1
[512, 1K)
518
[1K,2K)
9886881 0
[2K, 4K)
212 1eee88ee8e88e88ee8ee8ee8ee8ee88e88e88e 88ee8
[4K, 8K]
260 1869889889886 8889889889869869889889888 8889889889889881
[B, 16K]
80886881 5c
[16K,32K)
618
(x9*x21
1 |
[64K, 128x)
0 1
[128K, 256K]
4 1
[256K, 512K)
3 1
[512K,18)
11
This output shows that the connections from this Java process usually received their first bytes in
one to 16 miliseconds.
This works by using the syscall tracepoints to instrument the connect(2), read(2), and recv(2)
family of syscalls. The overhead may be measurable while running, as these syscalls can be
frequent on high-I/O systems.
22 Before eting too excited, 1 cheched the server ptime, PU cout, and prooes CPU ussge vis ps(1) (the process is
23 0rigin: 1 first created so1stbyte.d for the 2011 DTrace book [Greg 11]. I created this version on 16-Apr-2019,
supposed to be idle), to caloulate how much CPU resources are wssted by this: it came out to only 0.02%.
---
## Page 473
436
Chapter 10 Networking
The source to so1stbyte(8) is:
#1/usx/local/bin/bpEtrace
#Include 
include 
BEGIN
printf (*Tracing IP socket first-read-byte latency. Ctrl-C to end.\o*) =
tracepoint:syscalls:sys_enter_connect
/args=>uservadr->sa_family == AF_INET 11
/9a3x1aY == Atrwegesfd;
Bconnstart[pid, args->fd] = nsecs7
tracepoint:syscalls:sys_exit_connect
1f (axgs=>ret 1= 0 ss args=>ret = - EINPROGREsS) [
// connect(1 failure, delete flag if present
delete [Bconnstart[pid, Bconnfd[tid]]1
delete (@connfd[c1d]1
tracepointisyscallsisys_enter_close
/Bconn.start[pid, args=>fd]/
1
// never called read
delete (fconnstart [pid, econnfd[tid]] 
tracepoint:syscalls:sys_enter_read,
tracepoint:syscalls:sys_enter_readv,
tracepoint:syscalls:sys_enter_pread*,
tracepointisyscalls1sys_enter_recvfrom,
tracepoint:syscalls:sys_enter_recvnsg,
tracepointisysca1ls1sys_enter_recvmmsg
/Bconn.start[pid, args=>fd]/
---
## Page 474
10.3 BPF Tools  437
Breadfd[t1d] =args=>fd;
tracepointisyscallsisys_exit_read,
tracepoint:aysca11s:ays_exit_readv,
tracepointsysca1ls:ays_exit_pread*,
tracepoint:syscalls:sys_exit_recvfron,
tracepointisyscallsisys_exit_recvmsg,
tracepoint:aysca11s:ays_exit_recvrmsg
/Breadfd[ tid] /
$fd = Breadfd[tid];
Bus[corm, pid] = hist((nseca - Bconnstart[pid, $fd] ) / 1ooo) :
delete (8connstart [pid, $fd])
delete (freadfd[tld]1
END
clear (8connstart)  clear (8connfd) : clear (@readfd) 
This tool records a starting timestamp in a @Pconnstart map during the entry to connect(2), keyed
by the process ID and file descriptor. If this connect(2) is a failure (unless it is non-blocking and
returned with EINPROGRESS) or close(2) was issued, it deletes the timestamp to stop tracking that
connection. When the first read or recv syscall is entered on the socket file descriptor seen earlier,
it tracks the file descriptor in @readfd so that it can be fetched on syscall exit, and finally the start-
ing time read from the @connstart map.
This timespan is similar to the TCP time to first byte described earlier, but with a small difference:
the connect(2) duration is included.
Many syscall tracepoints need to be instrumented to catch the first read for the socket, adding
overhead to all of those read paths. This overhead and the number of traced events could be
reduced by switching instead to kprobes such as sock_recvmsg0 for socket functions, and track
ing the sock pointer as the unique ID rather than the PID and FD pair. The tradeoff would be that
kprobes are not stable.
10.3.11 tcpconnect
tcpconnect(8)** is a BCC and bpftrace tool to trace new TCP active connections. Unlike the earlier
socket tools, tcpconnect(8) and the following TCP tools trace deeper in the network stack in the
24 Origin: I crested a similar tcpconnect,d tool for the 2011 DTrace book [Gregg 11], and I created the BCC version on
25-Sep>2015, and the tcpconnecttp(8) bpfraoe tracepoint version on 7-Apr-2019.
---
## Page 475
438
Chapter 10 Networking
TCP code, rather than tracing the socket syscalls. tcpconnect(8) is named after the socket system
call connect(2), and these are often termed outboums connections, although they may also be to
localhost.
tcpconnect(8) is useful for workload characterization: determining who is connecting to whom,
and at what rate. Here is tcpconnect(8) from BCC:
 tcpconnet-py -t
TIME (s)
PID
COMM
IP SADDR
DACOR
DFORT
0,000
4218
Java
4100.1.101.18
100.2,51.232
6001
0,011
4218
java
4100,1.101.18
100.2,135,216
6001
0 .072
4218
Java
4
100.1.101.18
100 ,.2,135, 94
6001
0,073
4218
java
4100,1.101.18
100 .2,160, 87
8980
0.124
4218
Java
4
100.1.101.18
100 .2.177, 63
5001
0, 212
4.218
java
100,1.101.18
100.2.58.22
6001
0.214
4218
Java
4100.1.101.18
100.2,43.148
5001