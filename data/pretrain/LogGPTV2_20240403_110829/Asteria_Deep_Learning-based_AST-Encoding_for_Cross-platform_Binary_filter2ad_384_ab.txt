including
subtraction,
not,
post-decrease,
including
after
xor,
addition,
than,
less
division,
TABLE I: Statements and Expressions in ASTs. We count
the statements and expressions for nodes in ASTs after the
decompilation by IDA Pro and list the common statements
and expressions. This table can be extended if new statements
or expressions are introduced.
(a) AST for x86 platform
(b) AST for ARM platform
(c) CFG for x86 platform
(d) CFG for ARM platform
Fig. 2: ASTs and CFGs of the function histsizesetfn under
different architectures.
B. AST vs. CFG
Both CFG and AST are structural representations of a
function. The CFG of a function contains the jump rela-
tionships between basic blocks that contain straight-line code
sequences [41]. Though CFG has been used for similarity
measurement in BCSD [36], David et al. [29] demonstrated
that CFG structures are greatly affected by different architec-
tures. We ï¬nd AST shows better architectural stability across
architectures compared with CFG since the AST is generated
from the machine independent intermediate representations
which are disassembled from assemble instructions during the
decompilation process [25]. Figure 2 shows the changes of
ASTs and CFGs in x86 and ARM architectures, respectively.
For the CFGs from x86 to ARM, we observe that the number
of basic blocks changes from 4 to 1, and the number of
assembly instructions has changed a lot. However, the ASTs,
based on higher level
intermediate representation, slightly
change from x86 to ARM, where the changes are highlighted
with blue boxes. Besides, AST preserves the semantics of
functionality and is thus an ideal structure for cross-platform
similarity detection.
C. Tree-LSTM
In natural language processing, Recursive Neural Networks
(RNN) are widely applied and perform better than Convolu-
tional Neural Networks [66]. RNNs take sequences of arbitrary
lengths as inputs considering that a sentence can consist of any
number of words. However, standard RNNs are not capable of
handling long-term dependencies due to the gradient vanishing
and gradient exploding problems. As one of the variants
of RNN, Long Short-Term Memory (LSTM) [43] has been
proposed to solve such problems. LSTM introduces a gate
mechanism including the input, forget, and output gates. As
described in Â§ III-B, the gates control the information transfer
to avoid the gradient vanishing and exploding. Nevertheless,
LSTM can only process sequence input but not structured
input. Tree-LSTM is proposed to process tree-structured in-
puts [58]. The calculation by Tree-LSTM model is from the
bottom up. For each non-leaf node in the tree, all information
from child nodes is gathered and used for the calculation
of the current node. In sentiment classiï¬cation and semantic
relatedness tasks, Tree-LSTM performs better than a plain
LSTM structure network. There are two types of Tree-LSTM
proposed in the work [59]: Child-Sum Tree-LSTM and Binary
Tree-LSTM. Researchers have shown that Binary Tree-LSTM
performs better than Child-Sum Tree-LSTM [59]. Since the
Child-Sum Tree-LSTM does not take into account the order
of child nodes, while the order of statements in AST reï¬‚ects
the function semantics, we use the Binary Tree-LSTM for our
AST encoding.
D. Problem Deï¬nition
Given two binary functions F1 and F2, let T1 and T2 denote
the corresponding ASTs of F1 and F2 which can be extracted
after the decompilation of binary code. An AST is denoted
as T = where V and E are the sets of vertices and
edges, respectively. In node set V = {v1, v2, ..., vk, ..., vn},
every node corresponds to a number listed in Table I, and n
is the number of vertices in the AST. For an edge lkj in E,
it means vertex vk and vj are connected, and vk is the parent
node of vj. Given two ASTs T1 and T2, we deï¬ne a model
M(T1, T2) to calculate the similarity between them, where the
similarity score ranges from 0 to 1. In an ideal case, when F1
and F2 are homologous, the model M(T1, T2) is expected to
output a score of 1 (in an ideal case). And when F1 and F2 are
non-homologous, M(T1, T2) is expected to output a score of
0. In addition, based on the semantic similarity of ASTs, we
leverage the numbers of callee functions of F1 and F2 for sim-
ilarity calibration. Let C1 and C2 denote the numbers of callee
functions corresponding to functions F1 and F2, respectively.
We deï¬ne the calibration function S(C1, C2), where the range
blockifleblockblockvarnumasgasgvarnumvarvarreturncallnumvoidhistsizesetfn(UNUSED(p), long v){    if (v. In
our work, the label vector [1, 0] means T1 and T2 are from
non-homologous function pairs and the vector [0, 1] means
homologous. The resulting vector and the label vector are
used for model loss and gradient calculation. During model
inference, the second value in the output vector is taken as
the similarity of the two ASTs.
Fig. 5: Overview of AST Similarity Calibration.
C. AST Similarity Calibration
Considering other potential attributes (e.g., the number of
variables and statements) change with different architectures as
shown in Figure 2 (c) and (d), the number of callee functions
is an architecture-independent feature. The number of calllee
functions is easy to count during the reverse engineering.
We combine the number of the callee functions and AST
similarity to calculate the function similarity as shown in
Figure 5. The callee functions of function F are the functions
called in function F . Homologous functions likely have the
same number of callee functions since they share the same
source code. Considering that function inlining may occur
during compilation [18], [69], which will reduce the number
of callee functions, we reï¬ne callee functions by ï¬ltering
the functions that may be inlined. Speciï¬cally, we ï¬lter out
ğ‘»ğ’“ğ’†ğ’†-ğ‘³ğ‘ºğ‘»ğ‘´ğ‘»ğ’“ğ’†ğ’†-ğ‘³ğ‘ºğ‘»ğ‘´ğ’„ğ’‚ğ’•ğ’”ğ’ğ’‡ğ’•ğ’ğ’‚ğ’™ASTSimilarity|âˆ’|ğ’—ğ’‚ğ’ğ’–ğ’†ğŸğ’—ğ’‚ğ’ğ’–ğ’†ğŸğ‘’!ğ‘’"ğ‘’#ğ‘&â€™ğ‘&(â„&â€™â„&(ğ‘&â„&ğ’‰ğ’“ğ’ğ’ğ’•EncodingVectorSimilarityCalculationASTEncodingNodeEncodingâ„!"â„!#ğ‘!"ğ‘!#ğ‘’!u!ğ‘–!ğ‘!â„!ğ‘œ!ğ‘’!ğ‘“+,ğ‘“+-U"U#XXâˆ‘X+++++ğœğœğœğ‘¡ğ‘ğ‘›â„ğ‘¡ğ‘ğ‘›â„ğœğœSiameseNetworkâ¨€function!"FunctionAmainâ€¦â€¦FunctionBmainâ€¦â€¦Program1Program2SiameseNetworkPreprocessASTsimilarityInlineFilterInlineFilterCallGraphCallGraphCalleeSetofFunctionACalibrationFinalSimilarityCalleeSetofFunctionBfunction!#$%$&the callee functions whose number of assembly instructions
are less than the threshold Î², and the rest are used as the
callee function set Ï‡ of function F. With the introduction of
the callee functions, non-homologous functions which may
generate similar ASTs can be better distinguished. We deï¬ne
C as the size of the callee function set Ï‡ of function F . We
use the size C to calibrate the AST similarity. We use the
exponential function to measure the similarity between callee
functions. The calculation for callee functions is as follows:
Name
OpenSSL
Buildroot
Firmware
Total
Platform # of binaries
6
6
6
6
10,142
17,823
11,005
10,755
5,661
129
202
1,098
56,839
ARM
x86
x64
PPC
ARM
x86
x64
PPC
ARM
x86
x64
PPC
# of functions
6,401