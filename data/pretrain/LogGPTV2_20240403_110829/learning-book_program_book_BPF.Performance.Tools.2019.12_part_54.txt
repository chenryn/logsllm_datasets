6188988988
[128, 256}
0 1
[256, 512}
01
[512, 1K)
01
[1K, 2K]
0 1
[2K,4K)
0 1
[4K,8K)
0886880881 L
usecs [nvmeIn1, 
nvme_cmd_read] :
[32, 64)
7653 1eeeeeeeeee8ee8eeeeeeeeeeeeeeeeeeeee8ee8eeeeeeeeeeeee1
[64, 128]
5681869
451
(95z 8z1]
[256, 512}
4 1
[512, 1K)
0 1
[1K, 2K]
0 1
[2K,4K)
[4K, 8K)
1 1
This output showed that only one disk was in use, nvmen1, and the latency distributions for
three nvme command types.
Tracepoints for nvme were recently added to Linux, but I wrote this tool on a system that did
not have them, to show what can be accomplished with kprobes and storage drivers. I began by
frequency counting which nvme functions were in use during different I/O workloads:
.(: ()qunco =[oung] e ) euAu:aqoxdx, e- eoexgdq +
Attaching 184 probes...
^C
[nvme_pci_complete_rq] : 5998
e[nvse_free_1od] : 604T
[nvme_setup_cnd] : 6048
---
## Page 420
9.3  BPF Tools
383
TL09 =[ba"ananb“asu]@
e[nvme_conplete_rq] : 6171
e[nvme_irq] : 6304
[nvme_process_cq] : 1232T
Browsing the source for these functions showed that latency could be traced as the time from
nvme_setup_cmd() to nvme_complete_rq().
soe| seq tuasis e uo are nos gt uasa *suatudopasap [ooq tu pre ueo squgodaoen jo aotuasxa at
them. By inspecting how the nvme tracepoints worked [187], I was able to develop this tool more
quickly, because the tracepoint source showed how to correctly interpret nvme opcodes.
The source to nvmelatency(8) is:
1/usz/local/bin/bpftrace
include 
#Include 
BEGIX
printf(*Txacing nvae conmand Latency- Hit CtrlC to end.\n*)
/ / fron linux/nvme,bi
Boopcode [0x0o] = "nvme_cnd_flushb*;
Bioopcode[0x01] = *nvme_cnd_vrite*;
peexpuoesmo -[zoxo]epoodoot8
Bioopcode [0x04] = *nvme_cnd_vrite_uncor*;
Biocpcode[0x05] - *nvme_cnd_compare*;
Bioopcode[0x08] = *nvme_cnd_vrite_zeroes;
wsppuouso = [60xo]epoodootB
Bioopcode [0x(d] - *nvme_cnd_resv_register*≠
Bioopcode [0xde] = "nvme_cnd_resv_repoxt
Bioopcode[0x11] - "nvme_cnd_resv_acqui.re*;
eseeTea"nssxpuo"9mso, =[Stxo]epoodoot B
kprobe :nvme_se tug_cmd
$req = (struct request *)argl:
1f ($zeq->rq_diak) (
rs>asu = [t6xe|419,20
cnd[arg1] - axg2:
Padnin_conmanda = count(1 
---
## Page 421
 Chapter 9 Disk I/0
kprobe :nvse_conplete_rq
/ [n6e].re28/
$req = (struct request *)arg0;
$osd = (struct nvme_cormand *] ecnd[arg0]
$disk - $req=>rq_disk,
Sopcode = $cnd->comnon opcode s 0xff;
Busecs[$disk=>disk_name, Bioopcode [$opcode]] =
[00ot / ([06xe]xeas8 - sosu) 1stg
delete(8start[tid]): delete (8cmd[tid]) :
J
END
clear (81oopcode) : clear (Batart) : cleax I0cnd) ;
If a request is created without a disk, it is an admin command. The script can be enhanced to
decode and time the admin commands (see nvme_admin_opcode in include/linux/nvme.h). To
keep this tool short, I simply counted adimin commands so that if any are present they will be
noted in the output.
9.4BPF One-Liners
These sections show BCC and bpftrace one-liners. Where posible, the same one-liner is imple
mented using both BCC and bpftrace.
9.4.1BCC
Count block I/O tracepoints:
funccount t:block:*
Summarize block 1/O size as a histogram:
argdiat H *t:block:block_rq_issue () :u32:args=>bytes*
Count block I/O request user stack traces:
stackcount -U t:block:block_rg_issve
Count block I/O type flags:
argdiat -C *t:block:block_rq_issue () :char*:args=>rvbs
---
## Page 422
9.4 BPF One-Liners
385
Trace block 1/O errors with device and I/O type:
neprxb5s aEgs->error'
Count SCSI opcodes:
Count SCSI result codes:
argdist -C 'tiscsirscsi_dispatch_cnd_done () ru32:args=>resul t*
Count nvme driver functions:
unccount *nvme**
9.4.2 bpftrace
Count block I/O tracepoints
Summarize block 1/O size as a histogram:
bpftrace -e *t:block:block_rq_issue ( Bbytes = hist(args->bytes): )*
Count block I/O request user stack tracess
bpftrace -e *t:block:block_rg_issue [ B[ustack] = count(}: ]'
Count block I/O type flags:
Show total bytes by I/O type:
Trace block 1/O errors with device and I/O type:
/3oaaadev, arga=>zvbs, axgs=>exrozl ;)
Summarize block 1/O plug time as a histogram:
[11soesu =[06xe]s181 6ndsae1sxtqix。 8- 8oexagdg
k:blk_flush_plug_list /ets[arg0] / 1 @plug_ns = hist(nsecs - Bts[arg0]) 
delete (8ts [arg9]); ]*
Count SCSI opcodes:
ppftzace -e *t:scsl:scsi_dispatch_cmd_atart ( @opcode[axga=>opcode] = count () : 1′
---
## Page 423
386
Chapter 9 Disk I/0
Count SCSI result codes (all four bytes):
Show CPU dlistribution of blk_mq requests:
Count scsi driver functions:
(f(11umoo =[oun,g]01+t5os:eqoxdy。8-8oexadg
Count nvme driver functions:
9.4.3BPF One-Liners Examples
Including some sample output, as was done for each tool, is also useful for illustrating one-liners.
Counting Block 1/O Type Flags
( : ()qunoo =[sqaz<8Bxe]e 1 anset bzxootq:xootq:a, - eoexgdq +
Attach.ing 1 pzobe...
^C
e[8] : 2
[MFS] : 9
[FF]: 12
[B] : 13
[MSX] : 23
b9 :[N1θ
[MS] : 86
[R]: 201
[R] : 285
e[8] : 459
[RM] : 1112
[RA] : 2128
[R]: 3635
[B]: 4578
This frequency counts the rwbs field that encodes the I/O type. While tracing, where were
adeqo suga po μues at te uogoas sqma, a (V8) O/1 peatqe-peau gz1z pue (8) speau 5E9
describes this rwbs field.
---
## Page 424
9.6Summary 387
This one-liner can answer workload characterization questions such as:
 What is the ratio of read versus read-ahead block I/O?
• W'hat is the ratio of write versus synchronous write block I/O?
ad1 O/I 6q sa4fq aq tuns mm saus[-auo su (saAq<-s8re)ums aq 0 04unoo Bu8ueu g
9.5OptionalExercises
If not specified, these can be completed using either bpftrace or BCC:
1. Modify biolatency(8) to print a linear histogram instead, for the range 0 to 100 millseconds
and a step size of one millisecond.
2. Modify biolatency(8) to print the linear histogram summary every one second.
3. Develop a tool to show disk I/O completions by CPU, to check how these interrupts are
balanced. It could be displayed as a linear histogram.
4. Develop a tool similar to biosnoop(8) to print per-event block I/O, with only the following
fields, in CSV format: completion_time,direction,latency_ms. The direction is read or write
5. Save two minutes of (4) and use plotting software to visualize it as a scatter plot, coloring
reads red and writes blue.
ouael e se 1t .egdsp o aremgos Sunogd asn pue (z) go 1ndno atp jo sasnuru om aatg ‘9
heat map. (You can also develop some plotting software: e.g., use awk(1) to turn the count
column into rows of a HTML. table, with the background color scaled to the value.)
7. Rewrite biosnoop(8) to use block tracepoints.
8. Modify seeksize(8) to show the actual seek distances encountered by the storage devices:
measured on completions.
9. Write a tool to show disk I/O timeouts. One solution could be to use the block tracepoints
and BLK_STS_TIMEOUT (see bioerr(8).
10. (Advanced, unsolved) Develop a tool that shows the lengths of block I/O merging as a
histogram.
9.6Summary
This chapter shows how BPF can trace at allayers of the storage I/O stack. The tools traced the
block I/O layer, the I/O scheduler, SCSI, and nvme as an example drivef.
---
## Page 425
This page intentionally left blank
---
## Page 426
Chapter 10
Networking
Networking is playing an ever-increasing role in the performance analysis of systems, with the rise
of distributed cloud computing models increasing network traffic within a datacenter or cloud envi
ronment, and online applications increasing external network traffc. The need for eficient network
analysis tools is also on the rise, as serverscale to processing millions of packets per second. Extended
BPF began as a technology for packet processing, so it has been designed and built to operate at these
rates. The Cilium project for container networking and security policies, and Facebook's Katran
scalable network load balancer, are further examples of BPF's ability to handle high packet rates in
production environments, including for distributed denial of service attack (DDoS) mitigation.
Network I/O is processed by many different layers and protocols, including the application, proto-
col libraries, syscall, TCP or UDP, IP, and device drivers for the network interface. These can all
be traced with the BPF tools shown in this chapter, providing insight on the requested workloads
and latencies encountered.
Learning Objectives:
 Gain a high-level view of the networking stack and scalability approaches, including
receive and transmit scaling, TCP buffers, and queueing disciplines
 Learn a strategy for successful analysis of network performance
 Characterize socket, TCP, and UDP workloads to identify issues
 Measure different latency metrics: connection latency, first byte latency, connection duration
 Learn an efficient way to trace and analyze TCP retransmits
 Investigate inter-network-stack latency
Quantify time spent in software and hardware networking queues
Use bpftrace one-liners to explore networking in custom ways
This chapter begins with the necessary background for networking analysis, summarizing the
network stack and scalability approaches. I explore questions that BPF can answer, and provide an
overall strategy to follow. I then focus on tools, starting with tradlitional tools and then BPF tools,
including a list of BPF one-liners. This chapter ends with optional exercises.
1 Both of these are also open source [93] [94].
---
## Page 427
06E
Chapter 10 Networking
10.1
Background
This section covers networking fundamentals, BPF capabilities, a suggested strategy for network:
ing analysis, and common tracing mistakes.
10.1.1
Networking Fundamentals
A basic knowledge of IP and TCP including the TCP three-way handshake, acknowledgment
packets, and active/passive connection terminology, is assumed for this chapter.
Network Stack
The Linux network stack is pictured in Figure 10-1, which shows how data commonly moves from
an application to a network interface card (NIC).
Application
Libraries
System Calls
Kernel
connect/
send/receive
accept
VFS
Socket
sendirecv buffers
TCP
dan
ICMP
IP
Queueing
Discipline
ospb
ena
ixgbe
driver queue
driver queue
driver queue
device
drivers
NIC
NIC
NICIVirtual Device
Figure 10-1 Linux network stack
---
## Page 428
10.1 Background
391
Major components include:
 Sockets: Endpoints for sending or receiving data. These also include the send and receive
d1 fq pasn suagnq
 TCP (Transmission Control Protocol): A widely used transport protocol for transferring
data in an ordered and reliable way, with error checking.
 UDP (User Datagram Protocol): A simple transport protocol for sending messages without
the overhead or guarantees of TCP
 IP (Internet Protocol): A network protocol for delivering packets between hosts on a
network. Main versions are IPv4 and IPv6.
 ICMP (Internet Control Message Protocol): An IP-level protocol to support IP, relaying
messages about routes and errors.
 Queueing discipline: An optional layer for traffic classification (tc), scheduling。
manipulation, filtering, and shaping [95].
• Device drivers: Drivers that may include their own driver queues (NIC RX-ring and TX-ring).
 sod a pesd  so e aaap v :p a oa  
can also be virtual devices, such as tunnels, veths (virtual Ethernet devices), and loopback.
Figure 10-1 shows the path most commonly taken, but other paths may be used to improve the
performance of certain workloads. These different paths include kernel bypass and the new
BPF-based XDP
Kemel Bypass
Applications can bypass the kernel network stack using technologies such as the Data Plane
Development Kit (DPDK) for achieving higher packet rates and performance. This involves an appli
cation implementing its own network protocols in user-space, and making writes to the network
driver via a DPDK library and a kernel user space I/O (UIO) or virtual function I/O (VFIO) driver.
The expense of copying packet data can be avoided by directly accessing memory on the NIC.
Because the kernel network stack is bypassed, instrumentation using traditional tools and metrics
is not available, making performance analysis more difficult.
XDP
od e saxed sou rog ed oe spaod oopan (dx) e e ssadya a
mable fast path that uses extended BPF, and which integrates into the existing kernel stack rather
than bypassing it [Heiland-Jergensen 18]. Because it accesses the raw network Ethernet frame as
early as possible via a BPF hook inside the NIC driver, it can make early decisions about forward
ing or dropping without the overhead of TCP/IP stack processing. When needed, it can also fall
defined routing.
back to regular network stack processing. Use cases include faster DDoS mitigation, and software-
2 This reference is for “Queueing in the Linux Network Stack* by Dan Slemon, published by L/nux Journal in 201.3, an
exoellent explanation of these queues. Coincidentally, about 90 minutes affter writing this section, I found myself on an
lovisor concall with Dan Siemon and wss able to thank him directly.
---
## Page 429
392
Chapter 10 Networking
Internals
An understanding of some kernel internals will help you understand later BPF tools. The essen
tials are: packets are passed through the kernel using an sk_buff struct (socket buffer). Sockets
are defined by a sock struct embedded at the start of protocol variants such as tcp_sock. Network
protocols are attached to sockets using a struct proto, such that there is a tcp_prot, udp_prot, etc;
this struct defines callback functions for operating the protocol, including for connect, sendmsg,
and recvmsg.
Receive and Transmit Scaling
Without a CPU load-balancing strategy for network packets, a NIC may only interrupt one CPU,
which can drive it to 100% utilization in interrupt and network stack processing, becoming a
bottleneck. Various policies are available for interrupt mitigation and distributing NIC inter-
rupts and packet processing across multiple CPUs, improving scalability and performance. These
include the new API (NAPI) interface, Receive Side Scaling (RSs),3 Receive Packet Steering (RPS),
Receive Flow Steering (RFS), Accelerated RFS, and Transmit Packet Steering (XPS). These are docu-
mented in the Linux source [96].
Socket Accept Scaling
A commonly used model to handle high rates of passive TCP connections uses a thread to process
the accept(2) calls and then pass the connection to a pool of worker threads. To scale this further,
a SO_REUSEPORT setsockopt(3) option was added in Linux 3.9 that allows a pool of processes or
threads to bind to the same socket address, where they all can call accept(2). It is then up to the
kernel to balance the new connections across the pool of bound threads. A BPF program can be
UDP in Linux 4.5, and TCP in Linux 4.6.
supplied to steer this balancing via the SO_ATTACH_REUSEPORT_EBPF option: this was added for
TCP Backlogs
Passive TCP connections are initiated by the kernel receiving a TCP SYN packet. The kernel must
track state for this potential connection until the handshake is completed, a situation that in the
past was abused by attackers using SYN floods to exhaust kernel memory. Linux uses two queues
to prevent this: a SYN backlog with minimal metadata that can better survive SYN floods, and
then a listen backlog for completed connections for the application to consume. This is pictured
in Figure 10-2.
Packets can be dlropped from the SYN backlog in the case of flooding, or the listen backlog if the
application cannot accept connections quickly enough. A legitimate remote host will respond
with a timer-based retransmit.
In addition to the two-queue model, the TCP listen path was also made lockless to improve
scalability for SYN flood attacks [98].4
3 RSS is proessed purely by NIC hardwere. Some NICs support oloading of BPF networking programs (e.g.
Netronome), llowing RSS to become BPF programmable [97],
false-sharing issue [99].
4 The developer, Eric Dumazet, wss able to reach stx million SYN packets per second on his system after fixing a final
---
## Page 430
10.1 Background
393
SYN
SYN Backlog
SYNIACK
ACK
App
head
tail
ESTABLISHED
Listen Backlog
accept()
nead
max
Listen Drops
Figure 10-2 TCP SYN backlogs