support for more of LLVM’s instruction set.
Interpreting LLVM Bitcode
Instead of emitting an arith-
metic circuit, Geppetto ﬁrst compiles, then evaluates programs
(in prove mode) by symbolic interpretation of LLVM code.
Keeping the circuit implicit facilitates the generation of proofs
for large computations, inasmuch as the unfolded circuits can
be much larger than the LLVM code that generates them.
Our interpreter relies on a shallow embedding into F#, re-
lying on the F# control stack and heap; i.e., function calls are
implemented by calls to an F# call function, and mallocs are
F# array creations.
Some values are known at compile time, and used to special-
ize the QAP equations (Eqn. (4)) we produce. Others are known
only at run time; these values are treated symbolically, using an
abstract domain for integers; their operations generally involve
adding QAP equations.
Interpretation is cheap relative to cryptography, so, in prove
mode, we simply re-interpret the LLVM code to produce con-
crete witnesses for all run-time intermediate variables (the
‘wires’ of the implicit circuit), and we accumulate them into
digests and proofs. Thus, Geppetto uses two related interpreters
(described below) that differ in their interpretation of integers.
Symbolic Interpretation (1): Compilation Geppetto sepa-
rately interprets each outsourced function. As a side-effect of
their operations, variables and equations are added to the func-
tion’s QAP. For instance, multiplying two unknown integers
adds a variable (for the result) and an equation. Global caches
identify and eliminate common subexpressions.
For this interpretation, we represent unknown integers as a
triple of (i) a linear combination of QAP variables; (ii) a source
semantics: either some LLVM intn integer (e.g., int, short,
char) or a ﬁeld element (for embedded cryptography); and (iii) a
range: an interval in Z that covers any value this integer may
have at run time. Keeping track of ranges enables us to opti-
mize precomputations for fast exponentiations, to minimize bi-
nary decompositions (which cost one equation per potentially
active bit), to detect ﬁeld overﬂows, and to defer integer trunca-
tion (which require binary decompositions) for almost all op-
erations. For instance, our compiler may represent the (un-
known) value of an LLVM local variable as ‘an int32, obtained
by adding the 5th and 6th QAP variables, with range 1..100’.
At this stage of the compilation, the MultiQAP consists of
one QAP per function, plus ‘linking’ information on the shared
buses. This sufﬁces to generate keys, as the compiler traverses
each function’s QAP in turn, while keeping the buses virtual.
Symbolic Interpretation (2): Evaluation in ‘prove’ mode
We use another, faster instance of our interpreter, and we now
interpret the whole program, not just its outsourced code. De-
pending on the program’s control ﬂow, one outsourced function
may be interpreted many times with different ‘run-time’ val-
ues. The evaluator still distinguishes between ‘compile-time’
and ‘run-time’ values, although it has values for both, because it
needs to accumulate QAP witnesses for any operation on ‘run-
time’ values, in strict correspondence with the QAP variables
and equations produced by the compiler. Hence, values for all
QAP variables introduced at compile time are stored as inputs
for the cryptographic digests.
Because some compiler optimizations depend on data not
available at run time (such as integer ranges and caches), the
evaluation of some operations depends on auxiliary ‘interpreta-
tion hints’ passed along by the compiler. For example, before
XORing a variable with a constant, a hint tells the evaluator
whether a new binary decomposition is required (as we try to
re-use existing decompositions) and, when required, how many
264264
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:17 UTC from IEEE Xplore.  Restrictions apply. 
bits it uses (as the evaluator must record a witness for every bit
of the decomposition provisioned by the compiler).
The evaluator also intercepts calls to load or save banks, as
well as OUTSOURCE calls, and computes digests and proofs re-
spectively. When evaluation completes, the prover will have
produced exactly the evidence expected by the veriﬁer.
Cryptography (FFLib) All cryptographic operations are im-
plemented in a separate high-performance C++ library, with ef-
ﬁcient support for many base ﬁelds and elliptic curves. FFLib
is optimized for native x64 execution, unlike our QAP-friendly
crypto libraries (§6.4). In addition to default curves that achieve
128-bit security, it also supports toy curves for testing and de-
bugging. Since as much as 75% of the total run time (for key,
digest, and proof generation) is spent multiplying and exponen-
tiating elliptic curve points, we optimize these operations using
standard pre-computation and batching techniques [46].
Primitive Libraries Whenever possible, we reﬂect (and even
implement) primitive features of the interpreter using C types
and functions. Pragmatically, this keeps our code base small,
and lets us rely on standard (non-cryptographic) tools for testing
and debugging purposes—for instance by comparing printfs
between native clang runs and interpreted runs of the same code.
We provide a basic IO library. When loading from a ﬁle,
a ﬂag indicates whether this is a ‘compile-time’ or a ‘run-time’
ﬁle. Values from compile-time ﬁles are baked into the compiled
QAP. For run-time ﬁles, the compile-time interpreter allocates
fresh local QAP variables, and the evaluation-time interpreter
loads the ﬁle’s contents as run-time values. Thus, the ﬁle repre-
sents private, untrusted inputs provided by the prover.
As another example, for many programs, QAP size intri-
cately depends on compile-time values; the interpreter provides
a primitive function int nRoot() that returns the degree of
the QAP being generated (or proved), thereby letting C pro-
grammers debug the cryptographic performance of their code
and even control the partitioning of their code between several
QAPs of comparable degrees—for instance by unrolling a loop
until four million QAP equations have been generated.
6.4 Cryptographic Libraries and Bootstrapping
Geppetto has speciﬁc support for the compilation of programs
that evaluate cryptographic operations, to enable bootstrapping
and other ﬂexible applications of nested evaluation.
Field arithmetic and cryptography To support bootstrap-
ping, we provide custom C libraries that implement primitive
ﬁeld operations including addition, multiplication, division, and
binary decomposition. These enable fast, ﬁeld-based embed-
ding of cryptography, intuitively taking advantage of 254-bit
words. The ﬁeld type is also implemented natively. Thus, ﬁeld
operations can be compiled both with clang and for bootstrap-
ping by Geppetto.
Accordingly, our IO library supports loading C structs that
mix machine integers and ﬁeld elements. As shown in the code
of verify_job, we use it to load cryptographic evidence as
‘run-time’ data, and similarly for all other pieces of evidence.
By choosing to load the veriﬁcation keys at ‘compile time’ or
‘run time’, we select a different trade-off between performance
and ﬂexibility (see §7.3).
QAP-Friendly Elliptic Curves Cryptography We devel-
oped a plain, QAP-friendly C implementation of the elliptic-
curve algorithms for §5, including optimal Ate pairings. We
brieﬂy discuss two speciﬁc optimizations.
As in prior work [9], we use afﬁne coordinates (2 ﬁeld el-
ements) instead of projective ones (3 ﬁeld elements). Native
implementations use projective coordinates to avoid a ﬁeld divi-
sion when adding two points; since we verify the computation,
however, a ﬁeld division is just as fast as a ﬁeld multiplication.
For fast multiplication, the native algorithm has four cases at
each iteration of the loop, due to the special treatment of inﬁnite
points in addition. To prevent these conditional branches, which
are costly when compiling to QAPs, we add an initial summand
and remove it at the end.
Bounded Bootstrapping Our compiler implements multiple
levels of bootstrapping, as described in §2.2. Continuing with
our example in §6.1, assume we wish to compress the N proofs
by writing a bootstrapped function that aggregates the values
in the result[N] array. Geppetto’s libraries will ensure that
all N proofs (and corresponding digests) are veriﬁably veriﬁed,
in addition to verifying the aggregation of the result array.
To this end, we include another, similar but distinct copy of
our Geppetto library that lets the C programmer deﬁne ‘level
2’ or ‘outer’ banks and outsourced functions. We can then pro-
gram with two nested levels of veriﬁable computations, with the
outer top-level calling ‘level 2’ outsourced functions, which in
turn call inner ‘level 1’ outsourced functions according to their
own schedules. Hence, we also support proof schedules, digest
re-use, and MultiQAP programming at ‘level 2’. As before,
we obtain our veriﬁcation speciﬁcation by using a trivial im-
plementation of banks as local buffers and ignoring OUTSOURCE
annotations.
When compiling, we ﬁrst run the Geppetto compiler with
the trivial deﬁnition of ‘level 2’ banks and OUTSOURCE, and the
primitive Geppetto deﬁnitions for ‘level 1’. This generates keys
and code for outsourcing all ‘level 1’ functions. We then run the
Geppetto compiler with the primitive Geppetto deﬁnitions for
‘level 2’, and with the -DVERIFY ﬂag for ‘level 1’, thereby in-
cluding, e.g., the code of verify_job instead of job, as well as
our supporting cryptographic libraries for all ‘level 1’ elliptic-
curve veriﬁcation steps.
When proving, we run the Geppetto prover ﬁrst at level 1
(producing evidence for its outsourced calls) then at level 2
(loading that evidence from untrusted, ‘run-time’ ﬁles). When
verifying, we simply compile the source program with the
-DVERIFY ﬂag for level 2.
The approach above applies for further bootstrapping levels.
265265
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:17 UTC from IEEE Xplore.  Restrictions apply. 
6.5 Branching and Energy-Saving
When evaluating a program, there is no proof cost involved for
QAP variables that evaluate to zero: formally, we add a poly-
nomial contribution multiplied by 0 (§4.1), and we multiply di-
gests by 1 (a key element exponentiated by 0). Thus, if at com-
pile time we ensure that all intermediate variables for a branch
evaluate to 0 when the branch is not taken, then at run time there
is no need to evaluate that branch at all.
For example, consider the code fragment if(b) t = f(x).
At compile time, if b is known, we just interpret the test, and
compile the call to f only if b is true. If b is unknown, we in-
terpret this fragment as t = f(b*x) + (1 - b)*t and, cru-
cially, we compile the call to f conditionally on the guard b,
with the following invariant: if b is 0 and f’s inputs are all 0,
then its result must be zero, and zero must be a correct assign-
ment for all its intermediate variables. Additionally, any store
in f is conditionally handled, using similar multiplications by b.
Note that the addition of (1 - b)*t is generally required to
ensure that, if the branch is not taken, then the value of t is
unchanged.
More generally, we extend our ‘compile-time’ interpreter so
that its main evaluation function takes an additional parame-
ter: its guard, g, with range 0..1. The guard is initially 1, but
it can also be unknown (typically one of the QAP variables).
Except for branches, the guard is left unchanged by the in-
terpreter. Whenever the interpreter accesses a register with a
less restrictive guard, it multiplies it by g before using it. (We
cache these multiplications.) When branching on an unknown
boolean, say b, both branches are evaluated with guards g ∗ b
and g∗ (1− b), respectively. When joining, we sum the results
of the corresponding branches, as explained next.
The single-static-assignment discipline of LLVM and its ex-
plicit handling of joins help us implement this feature. In our
example, the code actually passed from clang to Geppetto is
entry:
%tobool = icmp eq i32 %b, 0
br i1 %tobool, label %if.end, label %if.then
if.then:
%result = ...
br label %if.end
if.end:
%t = phi i32 [ %result, %if.then ], [ %t, %entry ]
...
where the compile-time function phi selects which register to
use for the resulting value of t after the join. At compile time,
as we symbolically execute all branches, we simply interpret
the phi function as a weighted sum instead of a selector.
At run time, our representation of b tells us whether it was
known at compile time or not; we use that information to (im-
plicitly) provide 0 values for any branch not actually taken.
7 Evaluation
Below, we evaluate the effect of Geppetto’s optimizations on
the performance of the prover. We run our experiments on an
Op
Fixed Base Exp.
Multi Exp. (254 bit)
Pairing
Field Addition
Field Multiplication
Barreto-Naehrig
Twist
Base
87.2μs
21.2μs
55.6μs
241.5μs
0.6ms
44.2ns
288.2ns
Cocks-Pinch
Level 1
161.3μs
454.5μs
5.0ms
43.3ns
288.0ns
Level 2
1027.5μs
2008.2μs
31.9ms
65.2ns
726.0ns
Figure 6: Microbenchmarks for Cryptography. Breakdown of the
main sources of performance overhead in Geppetto’s larger protocol.
Each value is the average of 100 trials. Standard deviations are all
less than 4%.
HP Z420 desktop, using a single core of a 3.6 GHz Intel Xeon
E5-1620 with 16 GB of RAM.
7.1 Microbenchmarks
To calibrate our results, we summarize the cost of our cryp-
tographic primitives in Figure 6. We generally use a Barreto-
Naehrig (BN) curve for generating digests and proofs, and we
use the Cocks-Pinch (CP) curves to handle embedded crypto-
graphic computations like bootstrapping. We show measure-
ments from two CP curves to illustrate how the costs grow for
each progressive level. The BN curve is asymmetric, meaning
that one source group (base) is cheaper than the other (twist).
Geppetto’s protocol and compiler are designed to keep most of
the work on the base group.
The CP curves are slower than the BN for two reasons. First,
the CP curves are chosen to support bounded bootstrapping, so
they use larger ﬁeld elements than the BN curve (see §5). Sec-
ond, the BN code has been extensively optimized, including
hand-tuned assembly code, while the CP code is newly writ-
ten C. Based on operation counts from Magma [14], the ﬁrst
CP curve should be within 2-4× of the BN curve, and indeed
comparing the CP curve’s performance with a similar C version
of the BN curve conﬁrms this.
7.2 MultiQAPs
We compare the use of MultiQAPs for shared state with the
use of hashing in prior work such as Pantry [16]. At a mi-
cro level, Pantry’s results suggest that hashing an element of
state increases the degree of the QAP by ∼11.25/byte. In con-
trast, with MultiQAPs, a full ﬁeld element only increases the