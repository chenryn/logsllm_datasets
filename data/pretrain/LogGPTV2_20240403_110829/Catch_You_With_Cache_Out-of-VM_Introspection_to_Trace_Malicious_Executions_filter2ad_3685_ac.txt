19.1
12.9
To minimize the adverse impacts of noise and make the
analysis robust against high system load, we launch a vote in
cache probing. When agents try to estimate if one basic block
has been executed, they check it multiple times. Inside a basic
block, we choose 3 different instructions whose address does
not share the same cache line instead of one. if more than 2
probes out of 3 show a cache hit, agents are willing to take
this basic block as executed. It is possible to run more cache
probes inside a basic block. However, too frequent probes
will slow down agents, deteriorating synchronization issue.
Multiple probes can improve the accuracy of the analysis.
Assuming the probability of one cache line inﬂuenced by noise
is p, with a voting policy of best out of three probes, the
probability can be reduced to 3p2 − 2p3.
5) Lost Trace Handling: CPU cache is unstable so that
it is common for the analyzers to lose trace. It may ﬁnd hits
in all branches, indicating all branches have been executed;
or no hit at all, indicating no branch has been executed. In
this case, we deﬁne a continuing strategy to follow when the
analyzer has no idea about the target’s execution. Choosing
a reasonable continuing strategy contributes to high precision
and code coverage.
The most basic yet most general strategy is picking an
address to restart randomly. Once the analyzers are uncertain
of the control ﬂow, it scans all the addresses in the address
pool. If a randomly-picked address is out of cache, the analyzer
tries to re-pick another address after ﬂushing the ﬁrst address
out of cache. Noticing probes can also include instructions
into cache, it is necessary to always ﬂush it after probing
so that this won’t affect the subsequent analysis. However,
a random picking strategy is a game of chance. The more
complicated the target is, the slimmer the chances of ﬁnding
a valid address are. In extreme cases, the address pool can be
so large that the analyzer can never ﬁnd an address in cache
since cache hit status is always changing. Spending too much
Strategy based on the stack is not always efﬁcacious. The
heuristic described above is essentially a method that relies
on an address of one instruction believed to be executed soon.
This instruction will be executed as soon as the previous
function returns. However if this function’s life cycle is so long
that it does not return in a short time, the analyzer then keeps
probing cache states of the instructions and never advances.
This is possible because in cases like main function or nested
function calls, their stack frames stay in stack for a long time.
If the monitored process delivers an address of this kind, the
analyzer might stop tracing. To solve this problem, we take
a hybrid strategy. When the analyzer is not able to trace the
target with stack strategy for a long time (100 probes by agents
in our prototype), it will turn to a random strategy and pick
an available address to continue tracing.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:25 UTC from IEEE Xplore.  Restrictions apply. 
330
IV. IMPLEMENTATION
A. System Overview
Fig. 5. High level architecture
Figure 5 shows the high-level architectural view of Catcher.
The analyzer launches agents to send introspection requests.
These agents will continuously give access to memory in the
target process’s memory space by switching CR3 to the one
from the target. While the analyzer is working, the target
malware is executed at the same time. The onsite tracing has
almost no inﬂuence on the execution of the target process, and
thus the target cannot sense the existence of the analyzer.
When agents switch their CR3 into the same one with the
target process, they are capable of access the memory space
of the target VM natively. The Last-level cache is shared
across CPU packages, so agents can read memory and test
CPU cache status by timing the access directly. With target
and agents sharing the same VA to PA, if the accessing time
of the memory is less than the threshold (in our case, 150
CPU cycles), it is more likely this memory has recently been
read or executed. If it is out of the threshold, we assume this
memory is not recently used. By probing different memory
blocks’ CPU cache states, we can reveal the target’s execution
dynamically.
We implemented our prototype on a desktop with an Intel
Core i5-3470 3.2 GHz processor and 8GB DRAM. This CPU
is equipped with 4 cores and 6MB shared level 3 cache. We
run it on a Linux of kernel 3.2.79 with KVM. We created three
different types of agents to access the target’s memory space
and support the analyzer. We can launch multiple analyzers.
We designed two analyzers to cooperate with each other, which
are consist of around 3100 SLOC.
B. Agents
An agent is one small piece of code executed between
the analyzer and the target. Like a trampoline, it attaches
its window to either the analyzer or the target process by
switching CR3. It contains a small memory frame to receive
requests from an analyzer. When an agent is loaded with the
CR3 from the analyzer, it shares the same memory address
space with the analyzer, and thus the analyzer is capable of
writing its requests to the memory frame. The agent then
switches its CR3 to the one from the target. According to
the requests agent receives from the analyzer, it will read
memories of the speciﬁc virtual address and measure memory
access time to deﬁne cache states. The result is also stored in
the small memory frame so that after the agent switches back
its CR3, the analyzers can get its responses to their requests.
We design three types of agents to perform memory re-
vealing, cache probing, and data monitoring. Algorithm 2
illustrates part of the core logic in the code from a memory
revealing agent. Firstly it keeps checking the lock. Once it is
unlocked, the agent knows the analyzer has sent its requests.
It resolves these requests and loads the address and the size of
the memory region that the analyzer wants it to deal with next
from the consensual frame. Then agent switches to the target’s
memory space by changing its CR3 and read the data. After
copying it into the shared frame agent switches back CR3 and
free the lock. Now our analyzer is able to read the result of
their requests from the shared frame.
repeat
check the lock;
/* switch to target’s memory space*/
load target’s CR3;
read requested data from target’s memory space ;
load it into registers;
until lock == f alse
/* in local memory space*/
read size n and address addr from the shared frame;
load n and addr in agent’s registers;
Algorithm 2 The sketch of pseudo code of a memory revealing
agent
1: while TRUE do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: end while
/* switch to local memory space*/
load local CR3;
write data from the register into local memory space;
lock ← true;
Cache probing agent is similar to the memory revealing
cache except cache probing agent will utilize rdtsc instruc-
tion to measure the time of reading the target memory. The
result it returns is whether the virtual address the analyzer
requires is in cache or not. Data monitoring agent only returns
when the data in the required memory area changes. The
revealing agent is used to dump the executable memory of
the target. It is vital in the prologue stage because all the
analysis starts from here. The probing agent is the core part
of the analysis. We use it to infer execution status and conclude
the control ﬂow of the target. Monitoring agent, however, is
mostly used in stack-based heuristic. We suggest a monitoring
agent keep an eye on the stack area of the target so that we
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:25 UTC from IEEE Xplore.  Restrictions apply. 
331
agenthypervisor                                   host OSanalyzeragentagentagenttarget OStargetlocal memory spacetarget memory spaceswitchCR3memorycorecorecorecoreCPUcacheCatcherVMI requestscachecan continue from a function return even when Catcher loses
tracing.
C. Analyzers
Analyzers are processes that schedule agents to detect exe-
cution states of the target process. In Catcher we launch two
types of analyzers at once. They perform different functions
and help researchers analyze targets’ execution. Typically we
have a main trace analyzer and stack support analyzer.
1) Main Tracing Analyzer: The main tracing analyzer is
the principal part that takes most of the analysis work in
Catcher. It is the only part that keeps working all the way
through the whole analysis procedure. The main trace analyzer
needs input from the analyzer, indicating the CR3 of the target
process. Then it will initiate a memory revealing agent to
read the code distribution of the target process. After reverse
engineer the memory area with the support of IDA Pro, the
main tracing analyzer concludes a tree of virtual addresses,
whose nodes refer to basic blocks of the target process and
children refer to the next possible nodes to be executed. Details
are introduced in the next section.
The main tracing analyzer also takes responsibility for
dynamic tracing. That is, along with the execution of the target
process, this analyzer keeps inspecting the running states of
the target process. All this work is carried out silently. The
analyzer monitors in onsite mode, which means it has almost
no inﬂuence on the target, nor can it be detected by the target.
The main tracing analyzer launches cache probing agents. It
sends requests containing the addresses to probe on to the
agent and waits for its response. The agent checks the cache
states of these addresses. Usually the agent can ﬁnd one of the
cache states is shown as a hit, then it returns to the analyzer.
The main tracing analyzer chooses addresses in the child nodes
of the hit nodes and sends another request asking its cache
probing agent to probe on them.
2) Stack Support Analyzer: Sometimes the cache probing
agent may ﬁnd none of the addresses is in cache, or more than
one address is found in cache. In this case, the main tracing
analyzer asks for stack support analyzer’s help. Typically stack
support analyzer synchronizes it with a recently called function
so that the main tracing analyzer can go back to work again.
Stack support analyzer aims at monitoring the change in the
area of the target’s stack. Since Catcher is not able to check
values in registers (RSP for example), we can not locate the
malware’s user stack directly. However, stack is basically a
segment of data related to function calls. Every stack frame
always contains a return address and we can therefore locate
its user stack by ﬁnding memories containing a list of return
addresses. When scanning the whole memory space, we ﬁnd
the address of CALL instructions, then the addresses of their
next instruction are possible return addresses. Next we locate a
memory space involving these return addresses, namely where
the stack is.
Stack support analyzer has two parts. One is keeping an
eye on stack changes. It launches an agent assigned to back
up the stack. Once the stack changes, the analyzer realizes
a new function has been called and a new stack frame has
been pushed into the stack. As a matter of fact, this agent is
insensitive to function returns because the popping act does
not write anything in stack. The change in RSP is invisible
to agent and the memory doesn’t change. So when a function
pops, agent can hardly notice it. However, we can still monitor
the stack. The surveillance is based on the general knowledge
that the function at the top of the stack will be returned in a
short time. Eventually, the stack support analyzer picks these
functions and recommends them to the main tracing analyzer
when it loses trace of the target.
The stack support analyzer may fail if the process keeps
calling functions and never returns (nested function calls).
In this situation, the agent will keep backing up the stack.
The cache state in this return address will never be hit. This
problem can be settled because even though the stack keeps
growing, the number of functions is limited. The code at
the return address will eventually be executed, leaving the
tracing work back to normal. Besides, the main tracing an-
alyzer changes its continuing strategy when the stack support
analyzer is not able to support tracing. It turns to a random
continuing strategy.
The other part of the stack support analyzer is synchronizing
with the main tracing analyzer. For most of the time, the stack
support analyzer works individually. It focuses on the change
of the user stack. When the main tracing analyzer asks for
support, it recommends the most possible return address to it.
The synchronization is by inter-process communication. The
stack support analyzer shares a small chunk of memory with
the main tracing analyzer. Through this shared memory, the
tracing analyzer shows its request for suggestion on continuing
address, and then the stack support analyzer responds with its
answer.
D. Analysis Workﬂow
To analyze a target process from scratch, we follow a
workﬂow demonstrated in Figure 6. Catcher works in three
phases: prologue phase, trace phase, and epilogue phase.
1) Prologue Phase: In the prologue phase, the ﬁrst thing
is to locate the target. CR3 is the bridge between the virtual
address and physical address. Catcher locates its target process
through CR3. We can get target process’s CR3 by its pid. By
searching task_struck, Catcher locates the target’s process
descriptor. From its mm_struct, we get mm->pgd. Then
virt_to_phys() function uncovers the actual value of
CR3 of the target process.
Then the analyzer launches a memory revealing agent to
read the code distribution. It fetches the code segment from
the target’s memory space. Agents are designed to be able
to read memory blocks of different sizes. Although reading
a small number of bytes at a time reduces the complexity of
the agent code, it adds switching cost because the analyzer
has to send requests more frequently. We choose 512 as the
number of bytes for an agent to read at a time. It balances the
performance and complexity of the agent.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:25:25 UTC from IEEE Xplore.  Restrictions apply. 
332
Fig. 6. Workﬂow of analysis in three phases. The main tracing analyzer asks for a suggested address when it gets lost.
After dumping out the code segment, the analyzer disas-
sembles it preliminarily. Work about reverse engineering in
Catcher is out of scope so it will not be discussed in detail.
IDA Pro helps to reverse engineering and draw the connecting
of different code blocks. This method dynamically gets the
target’s code from memory instead of ELF or EXE ﬁle.
In order to organize the probing addresses of all basic
blocks, we evaluate the approximate execution time of every
block. By looking up instruction latencies [8], the analyzer
adds up all the instruction latencies in every basic block. It
marks the next basic blocks to probe on by fulﬁlling the time
gap. That is, when one basic block is designed to be probed,
the analyzer tries to advance its possible control ﬂow until the
step is over the threshold (2500 cycles). Although the analyzer
may miss some of the basic blocks, the lost code blocks can be
inferred later in the trace phase manually if both their previous
block and next block are identiﬁed in cache. We choose the
virtual address near the end of the basic block as the probing
address of the code block.
2) Trace phase: In this phase, one analyzer monitors cache
states of the address pool marked in the prologue phase
continuously while another one keeps eye on the stack. In
no case will analyzers interrupt the execution of the target
process. With the target process being executed, the analyzer
starts probing cache states from an executed virtual address.
This address can be manually selected, as long as the code is in
cache. Then the analyzer will try to check which child virtual
address of this block is also in cache, ﬁnding out how the
actual execution trace is. By this means, the analyzer reveals
the executed code blocks one by one.
It is common that sometimes analyzer may ﬁnd a situation
that all the child virtual address available is not in cache.
The reason has been explained in the previous section. In
this case, the stack monitoring analyzer, which is tracking
function calls, suggests a possible virtual address to restart
from. These two analyzers exchange information through inter-