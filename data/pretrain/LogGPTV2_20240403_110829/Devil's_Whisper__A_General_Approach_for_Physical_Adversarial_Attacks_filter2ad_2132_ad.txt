### Different IVC Devices and Their Security Concerns

Various IVC (Intelligent Voice Control) devices are listed in Table 11 of Appendix G. Some commands for these devices can pose significant safety or privacy risks. For example, "Okay Google, navigate to my home," "Okay Google, take a picture," and "Echo, open my door" can be exploited to compromise user security and privacy.

### Overall SRoC Results on IVC Devices

| **Device**          | **Black-box TBA** | **AGA** | **SNR (dB)** |
|---------------------|-------------------|---------|--------------|
| Google Assistant    | 4/10              | 10/10   | 9.03         |
| Home                | 4/10              | 9/10    | 8.81         |
| Microsoft Cortana   | 2/10              | 10/10   | 10.55        |
| Amazon Echo         | 0/10              | 10/10   | 12.10        |
| IBM WAA             | 3/10              | 10/10   | 7.86         |

**Note:**
1. "WAA" stands for "Wav-Air-API" attack.
2. The results were based on tests conducted in October 2019.

### Measurement of Audio Volume

We used a digital sound level meter, "SMART SENSOR AS824," to measure the volume of the AEs (Adversarial Examples). The background noise was approximately 50 dB, and the played audios ranged from 65 to 75 dB. This is comparable to some common sound levels, such as talking at 3 feet (65 dB) and living room music (76 dB). We also tested the AEs at realistic distances. For instance, the AE with the command "Echo, turn off the light" was effective up to 200 centimeters away, while the AE with the command "Hey Cortana, open the website" worked up to 50 centimeters away.

### Robustness of the Attack

To evaluate the robustness of our attack, we defined the success rate of AE (SRoA) as the ratio of successful tests to the total number of tests when an AE is repeatedly played. Table 11 shows the SRoA measured over 30 tests for each target command. The results indicate that 76% (38/50) of the commands have SRoAs over 1/3, demonstrating the robustness of our attack.

### Attacking Other Platforms

#### Over-the-air Attack against IBM Speech to Text API

As mentioned in Section 5.1, we used "Wav-Air-API" (WAA) to simulate the IVC device of IBM. The results, shown in Table 2, demonstrate that the WAA attack performs similarly to other IVC devices, indicating the effectiveness and generality of our approach.

#### AEs Attack against Apple Siri

Since Apple does not provide an online speech-to-text API service, we attempted two methods to attack Apple Siri:
1. Generating AEs directly using the transferability-based approach.
2. Using AEs that performed well on other IVC devices.

Table 9 in Appendix F shows that only the command "What is the weather?" generated from TBA successfully attacked Apple Siri. For other commands, AEs generated from AGA for other IVC devices were effective. All seven AEs successfully attacked Siri, demonstrating the transferability of AGA.

### Evaluation of Simple Approaches

#### Local Model Approximation with a Larger Corpus

If the local model is trained with a larger corpus of tuned TTS (Text-to-Speech) audio clips, it could better approximate the target black-box model. However, a larger corpus means more queries to the online API service, which could raise suspicion. We conducted a preliminary evaluation using the Google command_and_search model. Four commands were chosen, and a corpus of about 23.86 hours (5100 oracle queries) was generated, which is 5.17 times larger than the one used in our approach. After training the local model, we used the "MI_FGM" algorithm to generate AEs and evaluate them on the target. Only the command "OK Google, turn off the light" succeeded on the Google command_and_search model but failed on Google Home. The other commands did not generate any successful AEs.

#### Alternate Models Based Generation without Approximation

Another approach assumes that if an AE works on multiple models, it is likely to work on the target model without approximation. We used the ASpIRE Chain model as the base model and trained the Mini Librispeech model without the tuned TTS corpus. Four target commands from Table 10 in Appendix G were selected to attack the Google command_and_search model and Google Assistant/Home. Only one out of four commands worked on the Google command_and_search model and Google Assistant, while all commands failed on Google Home.

### Comparison with Other Straightforward Approaches

We compared our Devil’s Whisper attack with other straightforward approaches, including "Plain TTS," CommanderSong, and "Original song + TTS." Eight frequently used target commands on four IVC devices were selected, as shown in Table 3. Each command was covered by the same original song for different cases. The SNR (Signal-to-Noise Ratio) was calculated to evaluate the perturbation of the TTS audio on the original song.

The results, shown in Table 3, indicate that the AEs from the Devil’s Whisper attack effectively attacked the target IVC devices. As expected, "Plain TTS" audios triggered the devices every time. CommanderSong AEs, not designed for black-box attacks, failed to achieve any success. The "Original song + TTS" case showed that, under similar SRoA, most combined audio clips had much lower SNR levels, making the commands more perceivable and less stealthy.

### Human Perception

SNR measures the relative strengths between signal and noise, traditionally used to measure data perturbation. It can also model the distortion to the song caused by an AE, giving an intuitive estimate of the AE's stealthiness. The smaller the SNR, the larger the distortion, and the less stealthy the AE. Table 4 summarizes the human perception evaluation, showing that 60% of human users could identify the hidden command in the "Original song + TTS" case, compared to 0% for our AE.