0
)
)
1
p
p
p
+
R
1
R
R
1
R
1(
−=
p
α−=
1(1
−−=
         (4) 
to  compare 
R
R
To  estimate  the  performance  degradation  incurred 
by  Selective  Partitioning,  we  first  estimate 
the 
coefficient  α  in  (4),  by  performing  a  preliminary  test 
on  a  HT  processor.  We  wish 
the 
performance of a system with HT enabled versus with 
HT  disabled  (simulating  the  restricted  execution,  i.e. 
only  one  process  can  execute  at  a  time).  With  HT 
enabled, we observed up to 30% performance increase 
in terms of overall system throughput, though in a few 
occasions we also observed performance degradation. 
In  general,  we  found  that  in  most  cases,  the  relative 
throughput of HT-disabled vs. HT-enabled system is in 
the  range  of  0.75-0.95,  or  equivalently,  α  is  in  the 
range  of  0.05-0.25.  Figure  7  shows  the  performance 
degradation  curve  when  the  probability  of  restricted 
scheduling  p  changes  from  0  to  1.  In  the  worst  case, 
when  α  equals  0.25  and  p  equals  1,  the  performance 
degradation is approximately 25%.  In typical cases, p 
is likely to be in the range of 0.1 to 0.15, in which case 
the performance degradation is less than 5%. 
Peformance Degradation
t
u
p
h
g
u
o
r
h
T
e
v
i
t
a
el
R
1
0.9
0.8
0.7
0.6
0.5
0
a = 0.05
a = 0.25
0.2
0.4
0.6
0.8
1
Probability p
Figure 7. Performance of selective partitioning 
6. Random permutation cache solution 
is 
A  second  general  solution 
to  use  (cid:147)signal 
randomization(cid:148).  Any  signal  sent  by  the  sending 
process  is  randomized  with  respect  to  a  receiving 
process.  Our  solution  is  to  use  different  memory-to-
cache mappings for processes that need to be isolated 
from  others.  Other  processes  cannot  deduce  what 
cache  index  bits  are  used  by  a  process  when  the 
mapping  is  unknown.  Furthermore,  this  mapping 
should not be fixed since the attacker may be able to 
learn the mapping by doing a number of experiments.  
Changing  the  memory-to-cache  mapping  for  each 
process  can  be 
implemented  by  a  variety  of 
mechanisms, such as XORing the cache index bits with 
a  random  number  or  hashing  the  cache  index  bits. 
XOR  and  hash-based  mapping  are  simple 
to 
implement,  but  may  not  provide  enough  randomness. 
Rather,  we  propose  to  use  random  permutation  that 
gives  the  best  1-to-1  random  mapping.  This  can  be 
achieved  with  one  level  of  indirection:  keep  a  table 
that  contains  the  permuted  index  for  each  original 
cache  index.  When  accessing  the  cache,  the  original 
index 
the 
corresponding  permuted  index,  which  is  then  used  to 
access  the  cache.  This  extra  level  of  indirection  for 
Level 1 data cache accesses is costly in terms of cycle-
time  latency  or  cycles  per  access.    Below,  we  show 
how we achieve random permutation mapping without 
an  extra  level  of  indirection  for  table  lookup  and 
without lengthening the cache access time. 
table  for 
look  up 
is  used 
this 
to 
6.1. Low-overhead RPCache implementation 
Figure  8  shows  the  functional  block  diagram  of  a 
generic cache. During a cache access, a portion of the 
N bits of the effective address, used to index the cache, 
is sent to the address decoder. The decoder outputs 2N 
word  lines  and  in  each  access  only  one  of  them  is 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:29:18 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006driven  high  to  select  the  cache  set  that  contains  the 
data  being  accessed.  For  each  word  line  there  is  a 
comparison  module  which  compares  the  effective 
address  A  with  the  current  word  line  number  k.  The 
cache  set  is  selected  only  when  these  are  equal.  We 
can  add  an  N-bit  register,  which  we  denote  a 
Permutation  Register  (PR),  for  each  word  line  which 
contains a permuted cache index, feeding this into the 
comparison module instead of the original constant for 
the  word  line  number  k.  We  call  such  a  set  of 
permutation  registers  a  Permutation  Register  Set 
(PRS). By changing the contents of the PRS, arbitrary 
cache index mapping can be achieved. 
After  investigating  different  swapping  policies, 
including  periodically  swapping,  we  have  found  an 
optimal  swapping  strategy  from  the  information-
theoretic  perspective.  This  is  based  on  realizing  that 
only  cache  misses  that  cause  replacements  give  side-
channel information, so no swapping needs to be done 
when  there  are  no  such  cache  misses.    Upon  a  cache 
replacement, we swap the cache index of the cache set 
that contains the incoming cache line with any one of 
the cache sets with equal probability. So for any cache 
miss that the receiver process detects, it can be caused 
by  a  victim  process(cid:146)s  cache  access  to  any  one  of  all 
cache  sets  with  equal  probabilities.  Hence,  when  the 
receiver process detects a cache miss, he cannot learn 
anything about the cache locations used by the sender 
process.  This  cache  swapping  policy  also  incurs  very 
little performance overhead, as shown later. 
Critical Code Page Bits
Processor
Execution
Core
I-TLB
L1 I-Cache
D-TLB
L1 D-Cache
L2 Cache
Figure 8. A generic cache architecture. 
In  real  implementations,  the  comparators  in  the 
address decoder for the cache are not implemented as 
separate units.  Also, the fact that A is compared to a 
constant  word  line  number  k  is  exploited  to  simplify 
circuit  design.  The  only difference between using the 
variable  contents  of  a  Permutation  Register  (in  our 
RPCache) rather than a constant word line number k is 
that  fixed  connections  between  a  grid  of  wires  in  the 
address  decoder  circuit  are  replaced  with  switches. 
Though the drain capacitance of the switches increases 
the load in the address decoder circuits, proper circuit 
design can easily overcome this problem with no extra 
delay introduced. 
To prevent the attacker from learning the memory-
to-cache mapping via experiments, the mapping needs 
to be gradually changed. This can be implemented by 
swapping  cache  sets,  two  at  a  time.  To  change  the 
cache mapping, two permutation registers of the PRS 
are  selected  and 
their  contents  swapped.  The 
corresponding  cache  lines  are  invalidated,  which 
triggers the cache mechanism to write back any (cid:147)dirty(cid:148) 
cache  lines,  i.e.,  cache  lines  that  have  newer  data 
written  in  it  since  they  were  brought  into  the  cache 
from  memory.  Subsequent 
these 
invalidated  cache  sets  will  miss  in  the  cache,  which 
will degrade performance.  However, as we will show 
in section 6.4, the performance impact is very small. 
accesses 
to 
Permutation Register Sets (PRSs)
Figure 9. A processor with RPCache 
6.2. RPCache architecture 
The  RPCache  requires  permutation  registers  (PR), 
one for each set of the L1 (Level 1) Data cache.  In a 
direct-mapped  cache,  there  is  one  PRS  register  for 
each  cache  line.    The  processor  can  contain  one  or 
more sets of such permutation registers. There is also a 
new bit per Instruction Translation Look aside Buffer 
(ITLB) entry that we call the Critical Code Page (CCP) 
bit. 
6.3. RPCache usage model 
Each PRS set may be associated with a segment of 
code that needs to be protected. This may be a whole 
process, or a critical part of a process, e.g., the crypto-
related  shared  library  calls.  When  such  a  segment  of 
code is executing on the processor, the corresponding 
PRS is used to permute the index to the cache.  
The CCP bit in an ITLB entry indicates if the code 
on 
to  be  protected.  When  an 
that  page  needs 
instruction is fetched for execution, the CCP bit in the 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:29:18 UTC from IEEE Xplore.  Restrictions apply. 
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006corresponding ITLB entry is checked.  If it is set, the 
cache  access  of  a  load  or  store  instruction  will  go 
through the PRS mapping, otherwise the cache access 
will  use  the  original  cache  index.  Critical  processes 
and the critical segments of a process are marked. 
The  PRS  can  be  managed  solely  by  hardware. 
During a context switch, the old PRS values are simply 
discarded and a new set of values are generated (if the 
ITLB  of  the  new  process  has  its  CCP  bit  set).  Dirty 
cache lines may need to be written back, because next 
time this process is swapped in, it will use a different 
PRS to index the cache. It has to get a copy of the data 
from  the  next  level  cache  or  from  the  main  memory, 
and  the  freshness  of  the  data  must  be  ensured.  For  a 
write-through  cache,  however,  no  such  overhead  is 
necessary  since  the  next  level  cache  always  has  the 
latest data.  
The PRS can also be maintained by the OS. Upon 
each  context  switch,  the  PRS  of  the  process  that  is 
swapping  out  should  be  saved  as  part  of  the  context 
and  the  OS  should  load  the  PRS  values  of  the 
incoming process to the on chip PRS registers. 
SPEC2000 Benchmarks
i
e
m
T
n
o
i
t
u
c
e
x
E
d
e
z
i
l
a
m
r
o
N
1.012
1.01
1.008
1.006
1.004
1.002
1
0.998
0.996
0.994
0.992
am mp
baseline
RPCache
mesa
bzip2
crafty
galgel
wupwise
mgrid
average
applu
equake
Figure 10. RPCache Performance 
parser
perlbmk
gzip mcf
vortex
twolf
eon
gap
gcc
6.4. Performance evaluation  
The RPCache may introduce extra overhead when a 
change of the cache permutation mapping occurs. This 
may be during a context switch or when two cache sets 
are  swapped.  For  a  context  switch,  the  overhead  is 
insignificant  relative  to  the  time between two context 
switches.  For  the  hardware-managed  swapping,  we 
swap  just  the  contents  of  the  pair  of  PRS  registers 
while invalidating their associated cache lines. 
To evaluate the performance degradation, we run a 
set  of  SPEC2000  benchmarks  on  the  Simplescalar 
simulator  [25].  Figure  10  shows  the  normalized 
execution time. The baseline machine has a 2-way set-
associative write-back L1 data cache. The data marked 
with  (cid:147)RPCache(cid:148)  is  generated  on a machine using the 
cache set swapping scheme where the two cache sets to 
be  swapped  are  invalidated  with  dirty  lines  written 
back to the next cache level. 
The  performance  degradation  is  very  low:  1.1% 
worst case (perlbmk) and only 0.15% on average. This 
appears  to  be  because  cache  misses  and  cache  line 
replacements  normally  occur 
infrequently.  The 
performance  degradation  is  mainly  due  to  the  extra 
cache  misses  caused  by  the  invalidation  of  the  cache 
lines. However, since each time the invalidated sets are 
only 
the  resulting 
performance degradation is also insignificant. 
two  out  of  all  cache  sets, 
Key Searching Chart
n
o
i
t
a
l