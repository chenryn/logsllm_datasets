Service #13
(cid:2)
(cid:2)
Service #13 (win7 64 bit)
Service #13 (win7 32 bit stealthy) (cid:2)
(cid:2)
Service #14
Service #15
Service #16
Service #17
Service #18
Service #19
Service #20
(cid:2)
(cid:2)
(cid:2)
Dyn. Stat. AV Scan Report/submission Reports Sandboxes
(cid:2)
(cid:2)
14/20
0/20
(cid:2)
14
0
0
1
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
0/20
0/20
0/20
6/20
2/20
20/20
0/20
20/20
20/20
19/20
20/20
20/20
20/20
0/20
0/20
20/20
0/20
20/20
0/20
20/20
0
0
0
85
8
21
0
378
134
25
427
399
424
0
0
268
0
162
0
321
0
0
0
25
6
1
0
36
28
1
36
49
35
0
0
26
0
20
0
31
to the server. In this way we can track which sample is executed in which sys-
tem. After the ID is sent, a challenge-response authentication is done in order
to detect replayed requests.
After this initial handshake, SandPrint starts collecting features of the sys-
tem. For the implementation of feature collection, we avoided using commands
like systeminfo, netstat, and ipconfig, as they are often used by adversaries
to collect system features, and indeed we have conﬁrmed that some sandboxes
restrict them. Moreover, to avoid potential deadlocks caused by collecting indi-
vidual features (due to e.g., slow disk I/O), we balanced all feature collection
functions across multiple threads. In addition, to estimate the overall execution
time, a heartbeat thread periodically notiﬁes the server that SandPrint is still
executing. Each thread sends features to the server after the feature collection
process is completed. Note that all SandPrint traﬃc imitates HTTP protocol
and so it seems as if it is communicating with a Web server.
We submitted SandPrint to 20 malware analysis services to collect ﬁnger-
prints. Table 2 summarizes the public services and includes both popular aca-
demic and non-academic services. We periodically submitted SandPrint from
January 5, 2016 to January 18, 2016.
For each service, we created a unique SandPrint instance so that we could
map which ﬁle was uploaded where. That is, while the semantic functionality
is unaltered, the resulting ﬁle hashes are distinct. In addition, we use a unique
identiﬁer that is computed during runtime, report this identiﬁer to our server,
SandPrint: Fingerprinting Malware Sandboxes
171
and aim to expose it in the public analysis report that the service generates.
This way, we can later match the identiﬁer in a report with the corresponding
identiﬁer of the analysis report, revealing that a report was generated by a
particular service. In total, we collected 2666 SandPrint reports from 221 of
our 440 submissions. Thus, on average, we received 6 reports per submission.
The reports came from 395 IP address including 33 countries. As we will discuss
later, this already indicates that there is a strong tendency to (i) re-execute the
same sample multiple times (on the same or a slightly diﬀerent sandbox) and
(ii) share samples across sandboxes/services. We will now study this observation
in more detail and group similar SandPrint reports for further analyses.
4 Clustering Sandboxes
The ﬁngerprint collection revealed over 2500 reports. But are there really that
many sandboxes, or are some sandboxes responsible for multiple reports? To
answer this question, in this section, we introduce a clustering technique to group
similar reports and identify which reports were sent to us by which sandboxes.
4.1 Clustering
Initial observations have shown that subsets of the entire list of reports actually
share similar characteristics. As soon as a sandbox sends multiple reports, this is
intuitive, as there are likely features that remain unchanged across two sample
executions. Na¨ıvely, one could even check which reports contain equal features.
However, we found that sandboxes indeed (intentionally or not) diversify parts
of the features. Instead, we thus propose to use unsupervised learning techniques
to group similar reports together. Lacking any labels and ground truth for sand-
boxes, we face a classical unsupervised problem here. We chose to use agglom-
erative hierarchical clustering to group reports. Hierarchical clustering has the
advantage that it allows specifying a custom distance function and does not
require determining the number of expected clusters in beforehand. The distance
function determines how diﬀerent two reports are. We deﬁne a distance function
that spans all “clustering” features in Table 1 (see checkmark). That is, for a pair
of reports R1 and R2, we sum the distances of all pairwise features and divide
by the number of features to achieve the average distance. More formally, the
distk(R1, R2)
distance function between R1 and R2 is: dist(R1, R2) = 1
where distk is the distance between the values of a particular feature k. When
comparing a feature between two reports, we expect equality (EQ), and other-
wise assume maximum dissimilarity. That is, distk(R1, R2) is zero if the feature
k is equal in both reports, or 1 otherwise. For selected features which we observed
to vary in individual sandboxes, we do not expect equality. That is, we compare
the host name using a normalized edit distance (ED), deploy the Euclidean dis-
tance (EU) to compare the disk space and length of the sample name, and use
the Jaccard distance (JD) to compare the recently opened ﬁles. Table 1 cate-
gorizes the features accordingly. All distance functions have been normalized in
the range [0, 1] so that a single feature does not introduce bias.
N ∗(cid:2)N
k=0
172
A. Yokoyama et al.
In some cases, features are not present in one of the reports to compare.
SandPrint may have failed to collect some features, e.g., if the sandbox analysis
time was too short to complete all measurements (e.g., tracking all ﬁles in the
Programs directory may take a long time). To tackle sparse features, we focus on
those features that are included in the majority of the reports, as indicated by
the checkmark in Table 1. If a report does not have characteristics for a remaining
feature, we still cannot judge if two features are similar. To tackle this problem,
we ignore features that are not present in both reports and decrement N (the
number of features) accordingly to avoid biases in the average.
We then compute the distance between all reports and group the most similar
ones together, using agglomerative single linkage clustering. This process results
in a dendrogram, a tree-like structure that represents how the reports are clus-
tered together. After the clustering, we consider groups that have a distance less
than 0.5 as clusters. The intuition for this threshold is that we expect that at
least half of the features are similar for reports of a single sandbox.
4.2 Clustering Results and Validation
Clustering helped to reduce the 2666 reports down to 76 clusters. Of these, 16
are singleton clusters, i.e., sandboxes that only contributed one report to our
dataset. The largest cluster spans 233 reports, while the average cluster consists
of 35 reports, or 44 reports if we exclude the singleton clusters.
To verify the clustering output, we divided our research team in two dis-
joint groups. While one group independently designed and performed the auto-
mated clustering, the other group validated the clustering output. To this end,
we manually grouped similar sandboxes based on unique characteristics that
we identiﬁed for a particular sandbox, explicitly also those that slightly varied
information across diﬀerent executions. For each such outstanding feature, we
deﬁned a regular expression that matches all reports of the sandbox. We only
selected features whose entropy was large enough to avoid coincidental collisions
and deﬁne at least two characteristic features per sandbox.
We then compared the clustering result with the outcome of the manual
“clustering” done by the validation group. The outcome of the manual assign-
ments was equal to the clustering result, except in one case where our clustering
merged two sandboxes that we did not group manually. In this case, while the
user name, working group name, and host name were similar, the OS installa-
tion date was more than three years apart. Other than that, we did not ﬁnd
any further inconsistencies, which shows that our clustering methodology can
accurately map SandPrint reports (and their features) to a smaller number of
sandboxes.
4.3 Sandbox vs. Service
Table 2 summarizes the results of SandPrint submissions to the 20 malware
analysis services. At a glance, the number of SandPrint reports received from
these services varies widely. We did not receive any reports from nine services,
SandPrint: Fingerprinting Malware Sandboxes
173
which implies that sandboxes deployed by these services do not have Internet
connectivity, or the services simply did not conduct any dynamic analysis on
the submitted samples. Note that SandPrint is implemented such that it ﬁrst
reports back to our server before collecting any features. As we also did not see
the initial connection for the nine services, we argue that the lack of reports is
not caused by sandboxes that are trying to avoid being ﬁngerprinted. Due to
the lack of data, we exclude these nine services and will focus on the remaining
eleven services in the following.
4.4 Mapping Malware Analysis Services to Sandboxes
Next, we aim to map the SandPrint reports to malware analysis services. In
other words, did our ﬁngerprinting help to expose internals of the sandbox(es)
used by a service? To map sandboxes to services, we followed a two-fold approach.
First, we studied the analysis reports (i.e., those provided by the services, and
not by SandPrint) that were returned by a service. These reports include the
behavior of the submitted samples. Recall that we encoded a unique identiﬁer in
each SandPrint submission, which became visible in the analysis reports. We
found this identiﬁer in the analysis reports of services #2, #11, #13, and #20.
Second, to map the remaining services, we analyzed whether some sandboxes
were exclusively used when submitting a sample to a particular service. That is,
we identify sandboxes that are seemingly attached to a certain service. Figure 3
(see Appendix) depicts the mapping between all samples submitted to eleven
malware analysis services (y-axis) and 76 sandboxes according to the report
clustering (x-axis). Some mappings could be conﬁrmed by the analysis reports.
Next to these conﬁrmed mappings, we ﬁnd that some sandboxes are frequently
and exclusively used by the same service. For example, Sandbox 69 is constantly
used by Service #11 and no other services. In such a case, we can with some
likelihood conclude that the sandbox is exclusively used by the service. In total,
we revealed the dedicated sandboxes for four of the eleven services in this way.
After we mapped services to sandboxes, we were left with 71 sandboxes that
do not directly belong to one of the services. This is also shown in Fig. 3, which
lists many sandboxes that are commonly used to analyze samples from various
services. The degree of activity per sandboxes is an indicator for the coverage, i.e.,
how many samples a sandbox receives and executes. But foremost, it highlights
that samples are actively shared among the services.
4.5 Empirical Sandbox Analysis
After determining the sandboxes, we will highlight some insights obtained from
the collected features. First, we inspected the system installation features. We
found that the most popular OS for these sandboxes is still Windows XP, count-
ing 37 out of 71 sandboxes for which we could identify the OS. 29 sandboxes
were Windows 7. The other 5 sandboxes run Windows 8. The installation date
can approximate the age of a sandbox. Assuming the other installation dates are
not faked, we can see that all of the obtained OS installation dates are between
174
A. Yokoyama et al.
the years 2008 and 2016. We also see that more than half of the sandboxes are
at least three years old. As of 2014, 10 sandboxes were installed and already 18
sandboxes in 2015 or 2016. It is notable that the Windows product ID of 41
sandboxes is static, while 18 sandboxes vary the value. We presume this serves
for diversiﬁcation purposes, as malware has been observed to use the Windows
product ID as a feature for sandbox identiﬁcation.
The distribution of sandbox host names and owner names falls into two
extreme cases, namely, they are either highly diversiﬁed or completely static.
We deduce that some sandbox developers take countermeasures against being
ﬁngerprinted, while many others do not. Among the sandboxes that diversify
host and owner names, the randomized names of most sandboxes still exhibit
common patterns, such as common preﬁxes and/or ﬁxed length of the strings.
In some cases, we can infer sandbox implementations. Namely, Cuckoo Sand-
box includes a particular ﬁle named agent.py, which must be running upon the
analysis of a sample. We can infer that Cuckoo Sandbox is installed and running
by checking if the recent ﬁles list includes agent.py. We infer that ﬁve sandboxes
are implemented with Cuckoo Sandbox. Note that the sandboxes are not clus-