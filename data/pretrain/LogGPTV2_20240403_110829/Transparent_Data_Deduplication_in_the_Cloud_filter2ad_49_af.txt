In our implementation, the gateway and clients store the metadata
information associated to each ﬁle in a local MySQL database.
8For example,
the hash of Bitcoin block ‘X’ can be ac-
quired by invoking https://blockexplorer.com/q/
getblockhash/X.
(a) Key generation performance at G
(b) Overhead of key generation on clients
(c) Construction of PoW
(d) CARDIAC construction and proof gener-
ation
(e) Latency in CARDIAC w.r.t. number of
published ﬁles.
(f) CARDIAC veriﬁcation by clients
Figure 4: Performance evaluation of the building blocks used in ClearBox with respect to a number of parameters.
The gateway leverages the caching feature of MySQL in or-
der to reduce I/O costs. Recall that the Query Caching engine of
MySQL [4] maps the text of the SELECT queries to their results.
For each ﬁle, the gateway stores FID, the size of each ﬁle, and the
IDs of the clients sharing the ﬁle.
Each data point in our plots is averaged over 10 independent
measurements; where appropriate, we include the corresponding
95% conﬁdence intervals. To accurately measure (micro-)latencies,
we made use of the benchmarking tool due to Boyer [19].
4.2 Performance Evaluation
Before evaluating the overall performance of ClearBox, we start
by analyzing the performance of the building blocks with respect
to a number of parameters. Unless otherwise speciﬁed, we rely in
our evaluation on the default parameters listed in Table 2.
Gateway-assisted key generation: In Figure 4(a), we evaluate
the overhead incurred by the oblivious key generation module (cf.
Section 3.2.3) on the gateway. Here, we require that the gateway
handles key generation requests back to back; we then gradually
increase the number of requests in the system (until the through-
put is saturated) and measure the associated latency. Our results
show that our scheme incurs almost 125 ms latency per client key
generation request on the gateway and attains a maximum through-
Parameter
Default Value
Default ﬁle size
RSA modulus size
|CFID|
Num. of PoW challenges
Elliptic Curve (BLS)
16 MB
2048 bit
100
50
PBC Library Curve F
Table 2: Default parameters used in evaluation.
put of 1185 operations per second; this considerably improves the
maximum throughput of key generation of DupLESS (449 oper-
ations per second). This is mainly due to the fact that BLS sig-
natures are considerably faster to compute by the gateway when
compared to RSA signatures. In contrast, as shown in Figure 4(b),
BLS signatures are more expensive to verify by the clients than
the RSA-variant employed in DupLESS. However, we argue that
the overhead introduced by our scheme compared to DupLESS can
be easily tolerated by clients, as the client effort is dominated by
hashing the ﬁle; for instance, for 16 MB ﬁles, our proposal only in-
curs an additional latency overhead of 213 ms on the clients when
compared to DupLESS.
Proofs of ownership: Figure 4(c) depicts the overhead incurred by
FID required to instantiate the PoW scheme of [27] with respect to
the ﬁle size. As explained in Appendix B, the PoW scheme of [27]
reduces the costs of verifying PoW by encoding the original ﬁle
in a bounded size buffer (64 MB in our case). Our results show
that the computation of FID results in a tolerable overhead on both
the gateway and the clients. For example, the computation of FID
requires 1.07 seconds for a ﬁle with size 16 MB.
CARDIAC: In Figure 4(d), we evaluate the overhead incurred by
the construction of the accumulators and the proof generation in
CARDIAC with respect to |CFID|, i.e., the number of clients sub-
scribed to the same ﬁle f. Our results show that the latency in-
curred by the construction of CARDIAC can be easily tolerated by
G; for instance, the construction of an accumulator for 1000 users
requires 2.34 ms. Clearly, this overhead increases as the number
of users sharing the same ﬁle increases. Notice that once the entire
Merkle tree is constructed, proof generation can be performed con-
siderably faster than the construction of the accumulator. In this
case, G only needs to traverse the tree, and record the sibling paths
for all members of the accumulator.
 0 200 400 600 800 1000 1200 1400 0 200 400 600 800 1000 1200Latency [ms]Throughput [op/s]ClearBoxDupLESS 1 1041632641282561024Latency [s]Filesize [MB]ClearBoxDupLESS 0 5 10 15 20 25 30 3541632641282565121024Latency [s]Filesize [MB] 0.0001 0.001 0.01 0.1 1 10 1 4 16 64 256 1024 4096Latency [ms]Number of clients sharing a fileConstructionProof generation 0 200 400 600 800 1000 1200 1400 1600 1800 2000 1 4 16 64 256 1024 4096Latency [ms]Number of files 0 5 10 15 20 25 30 35 1 4 16 64 256 1024 4096Latency [us]Number of clients sharing a file(a) PUT Performance
(b) GET Performance
(c) Latency vs. throughput
Figure 5: Performance evaluation of ClearBox using Amazon S3 and Dropbox as back-end cloud storage.
In Figure 4(e), we evaluate the overhead incurred on G by the
proof generation in CARDIAC with respect to the ﬁles selected for
attestation at the end of each epoch. Our results show that this la-
tency is around 150 ms when less than 100 ﬁles are selected, since
the accumulators of these ﬁles can be handled in parallel by sep-
arate threads in our pool. When the number of selected ﬁles in-
creases beyond our pool threshold (i.e., 100), the latency of proof
generation increases e.g., to reach almost 1 second for 4000 ﬁles.
In Figure 4(f), we evaluate the overhead of the proof veriﬁcation
by the clients. Given that the veriﬁcation only requires (cid:100)log |CFID|(cid:101)
hashes, this operation only incurs marginal overhead on the clients.
For example, the veriﬁcation of membership and cardinality in
CARDIAC when |CFID| = 1000 only requires 27 µs.
ClearBox: In Figure 5(a), we evaluate the latency witnessed by
users in the PUT operation with respect to the ﬁle size. Our results
show that ClearBox achieves comparable performance than Dup-
LESS over S3. For example, when uploading 16 MB ﬁles on Ama-
zon S3, DupLESS incurs a latency of 4.71 seconds, while ClearBox
requires 6.33 seconds. The additional overhead in ClearBox mainly
originates from the computation of FID by the users (cf. Fig-
ure 4(c)); the latency is mainly dominated by the upload of the
ﬁle to Amazon.
In contrast, when users want to upload a ﬁle which is already
stored on the cloud, ClearBox results in faster upload performance
than DupLESS, since users are no longer required to upload the
ﬁle, but have to execute the PoW protocol with G—which incurs
negligible latency when compared to the upload of modest-sized
ﬁles. Recall that this comes at the expense of additional load on
G; in contrast, the server in DupLESS bears no load when users
upload/download ﬁles.
When using Dropbox, recall that clients upload the ﬁle directly to
G, which in turn uploads it to its Dropbox account (since Dropbox
does not provide URL commands for ﬁle creation). Although this
process requires less communication rounds between the clients
and G (to acquire the signed URL), our results show that the la-
tency incurred when uploading un-deduplicated ﬁles in ClearBox
using Dropbox is marginally larger than its Amazon S3 counter-
part; we believe that this discrepancy is due to the lower upload
bandwidth between our clients and G when compared to Amazon
S3. Notice that the performance of uploading deduplicated ﬁles in
ClearBox does not depend on the back-end cloud provider and is
therefore identical for both our Amazon S3 and Dropbox imple-
mentations.
In Figure 5(b), we evaluate the latency witnessed by users in the
GET operation in ClearBox. Our results show that GET operation
incurs comparable latencies in both ClearBox and DupLESS. Re-
call that in ClearBox, clients have to ﬁrst contact G and acquire
the timed GET URL to download resources. Given that the la-
tency of the GET operation is dominated by the download speed,
the overhead incurred by this additional communication round is
marginal. Notice that the latency exhibited by ClearBox users is
tolerable compared to that witnessed in a plain cloud storage sys-
tem. For instance, assuming a 100 Mbps line rate, the download
of a 32MB ﬁle in plain clouds requests almost 3 seconds; on the
other hand, downloading this ﬁle in ClearBox incurs a latency of
10 seconds.
In Figure 5(c), we evaluate the latency incurred on the gateway
in ClearBox with respect to the achieved throughput. Here, we as-
sume that 50% of the requests handled by G correspond to PUT
requests, while the remaining 50% are GET requests. We further
assume that 50% of the upload requests correspond to ﬁles which
are already stored at S. When uploading (un-deduplicated) ﬁles to
S, we do not measure the overhead incurred on G when verifying
FID since this veriﬁcation is asynchronous and can be done by
G at a later point in time. We further simulate a large download
bandwidth at G by emulating client requests from a local socket.
Our ﬁndings show that ClearBox achieves a maximum throughput
of approximately 2138 operations per second when integrated with
Amazon S3. This shows that our solution scales to a large number
of users in the system. When interfacing with Dropbox, the perfor-
mance of ClearBox deteriorates since the load on G considerably
increases in this case;9the maximum throughput exhibited by our
scheme decreases to almost 289 operations per second.
5. RELATED WORK
Data deduplication in cloud storage systems has acquired con-
siderable attention in the literature.
In [28], Harnik et al. describe a number of threats posed by
client-side data deduplication, in which an adversary can learn if a
ﬁle is already stored in a particular cloud by guessing the hashes of
predictable messages. This leakage can be countered using Proofs
of Ownership schemes (PoW) [23, 27], which enable a client to
prove it possesses the ﬁle in its entirety. PoW are inspired by Proofs
of Retrievability and Data Possession (POR/PDP) schemes [11,
40], with the difference that PoW do not have a pre-processing step
at setup time. Halevi et al. [27] propose a PoW construct based on
Merkle trees which incurs low overhead on the server in construct-
ing and verifying PoW. Xu et al. [44] build upon the PoW of [27]
to construct a PoW scheme that supports client-side deduplication
in a bounded leakage setting. Di Pietro and Sorniotti [23] propose a
9For comparison purposes, we did not include in our measurements
the overhead introduced by the uploading of ﬁles by G onto S; this
process can be executed at a later point in time by G.
 1 10 10041632641282561024Latency [s]Filesize [MB]ClearBox (Dropbox)ClearBox (S3)DupLESSClearBox dedup 1 10 100 100041632641282561024Latency [s]Filesize [MB]ClearBox (Dropbox)ClearBox (S3)DupLESS 0 100 200 300 400 500 600 700 800 900 0 500 1000 1500 2000 2500Latency [ms]Throughput [op/s]S3DropboxPoW scheme which reduces the communication complexity of [27]
at the expense of additional server computational overhead. Blasco
et al. [16] propose a PoW based on Bloom ﬁlters which further
reduces the server-side overhead of [23].
Douceur et al. [25] introduced the notion of convergent encryp-
tion, a type of deterministic encryption in which a message is en-
crypted using a key derived from the plaintext itself. Convergent
encryption is not semantically secure [15] and only offers conﬁ-
dentiality for messages whose content is unpredictable. Bellare et
al. [31] proposed DupLESS, a server-aided encryption to perform
data deduplication scheme; here, the encryption key is obliviously
computed based on the hash of the ﬁle and the private key of the as-
sisting server. In [42], Stanek et al. propose an encryption scheme
which guarantees semantic security for unpopular data and weaker
security (using convergent encryption) for popular ﬁles. In [41],
Soriente et al. proposed a solution which distributively enforces
shared ownership in agnostic clouds. This solution can be used in
conjunction with ClearBox to enable users to distributively manage
the access control of their deduplicated ﬁles.
Several proposals for accumulating hidden sets exist such as the
original RSA-based construction [12], which has been extended
to dynamic accumulators [21], and non-membership proofs [32].
There exist as well constructions based on bilinear groups [22,
39] and on hash-functions [20, 33]. A related concept are zero-
knowledge sets [30, 37]. These structures provide proofs of set
membership. Notice, however, that cryptographic accumulators do
not typically provide information about the accumulated set, such
as content or cardinality.
Our attacker model shares similarities with the one considered
in [43], where the cloud provider is assumed to be economically ra-
tional. This scheme relies on an hourglass function—to verify that
the cloud provider stores the ﬁles in encrypted form—which im-
poses signiﬁcant constraints on a rational cloud provider that tries
to apply the hourglass functions on demand.
6. CONCLUSION
In this paper, we proposed ClearBox, which enables a cloud
provider to transparently attest to its clients the deduplication pat-
terns of their stored data. ClearBox additionally enforces ﬁne-
grained access control over deduplicated ﬁles, supports data con-
ﬁdentiality, and resists against malicious users. Evaluation results
derived from a prototype implementation of ClearBox show that
our proposal scales well with the number of users and ﬁles in the
system.
As far as we are aware, ClearBox is the ﬁrst complete system
which enables users to verify the storage savings exhibited by their
data. We argue that ClearBox motivates a novel cloud pricing
model which promises a fairer allocation of storage costs amongst
users—without compromising data conﬁdentiality nor system per-
formance. We believe that such a model provides strong incentives
for users to store popular data in the cloud (since popular data will
be cheaper to store) and discourages the upload of personal and
unique content. As a by-product, the popularity of ﬁles addition-
ally gives an indication to cloud users on their level of privacy in
the cloud; for example, the user can verify that his private ﬁles are
not deduplicated—and thus have not been leaked.
Acknowledgements
The authors would like to thank Nikos Triandopoulos and the anony-
mous reviewers for their valuable feedback and comments. This
work was partly supported by the EU H2020 TREDISEC project,
funded by the European Commission under grant agreement no.
644412.
References
[1] Amazon S3 Pricing.
pricing/.
http://aws.amazon.com/s3/
[2] Bitcoin real-time stats and tools. http://blockexplorer.
com/q.
[3] Google Cloud Storage.
storage/.
https://cloud.google.com/
[4] The MySQL Query Cache. http://dev.mysql.com/doc/
refman/5.1/en/query-cache.html.
[5] PBC Library. http://crypto.stanford.edu/pbc/, 2007.
[6] Cloud Market Will More Than Triple by 2014, Reaching $150