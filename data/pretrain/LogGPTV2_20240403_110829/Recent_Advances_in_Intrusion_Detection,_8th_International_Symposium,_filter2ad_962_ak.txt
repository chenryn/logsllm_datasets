### 9. Results for DNS-based RL at the Edge Router

Our results demonstrate that DNS rate limiting is effective in containing slow-spreading worms. For comparison, Weaver's Approximate TRW containment mechanism can block worms scanning at a rate of more than one scan per second [21]. Using the DNS scheme with parameters q = 3 and t = 5 (i.e., 3 direct-IP connections within a 5-second window), we can contain worms scanning at a rate of 0.6 scans per second or higher with 99% accuracy.

To evaluate the impact of aggregate throttling, we implemented a single set of cascading buckets for the entire network. In these experiments, the value of q was set to 20, 50, and 100 IPs per five-second window. Figure 9 shows the error rates for the aggregate implementation. As depicted, a q value of 20 or 50 IPs resulted in few false negatives and a false positive rate of approximately 3-5%. Notably, when q is set to 20 or 50, the false negative rates for edge-based rate limiting are lower than those for the host-level scheme. This is because the aggregate traffic limit is more restrictive overall compared to the collective limit in the host-based case. Although the false positive rates for the aggregate case are slightly higher, the overall error rates remain low—5% false positives and <1% false negatives.

### 10. Discussions

The analysis in the previous sections highlights several issues related to rate-limiting technology. In this section, we extrapolate from these results and discuss some general insights.

#### DNS-based RL vs. Other Schemes

A summary comparison of the DNS-based scheme with other methods is provided in Figure 10. The parameters used here are consistent with those in the previous sections. As shown, DNS-based rate limiting has the best performance in terms of false positive and false negative rates. Host-based DNS throttling yields an average false positive and false negative rate below 1%. These results strongly support the use of DNS-based rate limiting.

Recall that the q value in DNS throttling allows q untranslated IP connections per host to exit the network every t seconds. For example, on the first day of infection, the network had a total of 468,300 outbound legitimate flows. When q = 7, a total of 463 legitimate flows were dropped, resulting in a false positive rate of 0.099%, or less than one dropped flow per host per day. In contrast, CB (Connection Bounding) dropped 3,767 legitimate flows for the same day, leading to a false positive rate of 7.8%.

The success of DNS-based rate limiting can be attributed to the fact that DNS traffic patterns (or the lack thereof) more accurately distinguish worm traffic from normal behavior. This allows DNS-based RL to impose severe limitations on worm traffic without significantly impacting normal traffic.

Scanning worms are often successful because they can rapidly probe the numeric IP space. Navigating the DNS name space, however, is more challenging due to its sparsity and poor locality properties. DNS-based throttling forces worms to probe the DNS name space, thereby reducing the scan hit rate and increasing the difficulty for worms to propagate.

Although our trace data reflects a simple worm that does not attempt to mask itself, extending the DNS RL scheme to more sophisticated worms is straightforward. We plan to address this in future work.

#### Issues with DNS-based Rate Limiting

An attacker can attempt to circumvent the DNS rate limiting mechanism in several ways:

1. **Reverse DNS Lookups (PTR lookups)**: A worm could use reverse DNS-lookups to "pretend" it has received a DNS translation for a destination IP. PTR lookups are primarily used for incoming TCP connections or lookups related to reverse blacklist services. These types of lookups can be easily filtered and not considered valid entries in the DNS cache. Additionally, a PTR lookup before an infection attempt would significantly reduce the infection speed.

2. **Fake External DNS Server**: An attacker could set up a fake external DNS server and issue a DNS query for each IP. This threat can be mitigated by establishing a "white-list" of legitimate external DNS servers. Moreover, the attacker would need a server with substantial bandwidth to accommodate the scan speed, which is not trivial. For SOHO (Small Office Home Office) users who may set up their own routers and use legitimate external DNS servers, a packet scrubber like Hogwash [5] can help correlate DNS queries to responses.

3. **Dictionary of Host Names and Domains**: Another attack involves equipping each worm with a dictionary of host names and domains, effectively turning a scanning worm into a hit-list worm. Hit-list worms are significantly more difficult to engineer. If the only viable means to bypass DNS-based throttling is for the worm to carry a hit-list, this is a strong endorsement of DNS-based throttling.

#### Dynamic vs. Static Rates

Rate-limiting schemes affect both legitimate and malicious connections. Williamson’s method imposes a strictly static rate, such as five distinct IPs per second, regardless of traffic demand. FC (Failed Connections) is predominantly static, while CB (Connection Bounding) allows for dynamic traffic rates by rewarding successful connections and penalizing failed ones. Results in Figure 10 show that CB outperforms FC. This is partially due to CB’s dynamic rates, which provide a more graceful filtering scheme that accommodates bursty application behavior and temporarily abnormal but benign traffic patterns. As briefly discussed in Section 5, mechanisms that impose static rates can benefit from incorporating dynamic rate limits. Dynamic rate limiting is an interesting topic worthy of further study.

#### Host vs. Aggregate

A significant issue is the comparison between host-level and aggregate rate limiting. Generally, host-level throttling is more precise but also more costly because per-host state must be maintained. Williamson’s IP throttling, when applied at the edge, resulted in visibly higher false positives than its host-based counterpart. This is because IP contact behavior at the host level is more fine-grained and thus more stable. In contrast, aggregate traffic at the edge includes hosts with varying behaviors, contributing to higher error rates. A similar case was observed with CB when applied to aggregate traffic, as shown in Figure 10(b). The false positive rates reached approximately 30%, compared to 10% with the host-based deployment.

Edge-based DNS throttling, however, appears to be an exception. Figure 10(a) shows that a carefully chosen rate limit, such as 50 IPs per five seconds, yields excellent accuracy for edge-based DNS throttling. It has lower false positive and false negative rates than other host-based schemes. The fundamental reason is that DNS statistics, particularly the presence (or absence) of IP translations, remain largely invariant from the host to the aggregate level.

This result is highly encouraging, as aggregate rate limiting has lower storage overhead and is typically easier to deploy and maintain than host-based schemes. Note that our study did not include an analysis of processing overhead. Readers should be aware that edge-based schemes generally imply processing a larger amount of data per connection, so a trade-off between storage and processing overhead exists. The aggregate DNS throttling results suggest the possibility of pushing rate limiting deeper into the core, where a single instrumentation can cover many IP-to-IP paths and potentially achieve a greater impact.

We note that edge-based throttling alone does not defend against internal infections. One way to protect against internal infections (without the cost of host-level throttling) is to divide an enterprise network into various cells (as suggested by Staniford [14]) and apply aggregate throttling at the border of each cell. We leave the analysis of more fine-grained, intra-network protection for future work.

### 11. Summary

Several rate-limiting schemes have been proposed recently to mitigate scanning worms. In this paper, we present the first empirical analysis of different schemes using real traffic and attack traces from an open network environment. We believe that a scheme that performs well in an open network will perform equally well (if not better) in an environment with strict traffic policies, such as an enterprise network.

We evaluate and compare the false positive and false negative rates for each scheme. Our analysis reveals several insights:
1. The subject of rate limiting is the most significant parameter—failed-connection behavior alone is too restrictive, as evidenced by FC; rate limiting first-contacts yields better results, and DNS behavior-based rate limiting is the most accurate strategy.
2. It is feasible to distinguish worm behavior from normal traffic even at an aggregate level, as indicated by the DNS analysis. This is an interesting result because aggregate rate limiting alleviates the universal participation requirement thought necessary for worm containment [10, 25]. This result also suggests that it may be possible to apply rate limiting deeper into the core of the network, a subject of great interest.
3. Preliminary investigation suggests that incorporating dynamic rates results in increased accuracy. As most rate-limiting schemes to date focus on static rates, an immediate follow-up research area is dynamic rate limiting and its practical implementation.

### Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant No. 0326472. The authors thank Greg Ganger and Mike Reiter for providing insightful feedback on preliminary versions of this work. We also thank Matthew Williamson for technical discussions about this work.

### References

1. Shigang Chen and Yong Tang. Slowing down internet worms. In Proceedings of 24th International Conference on Distributed Computing Systems, Tokyo, Japan, March 2004.
2. M. Collins and M. Reiter. An empirical analysis of target-resident DoS filters. In In Proceedings of 2004 IEEE Symposium of Security and Privacy, 2004.
3. Daniel R Ellis, John G Aiken, Kira S Attwood, and S.D Tenaglia. A behavioral approach to worm detection. In Proceedings of the 2004 ACM workshop on Rapid Malcode. ACM Press, 2004.
4. G.R Ganger, Gregg Economou, and S. Bielski. Self-securing network interfaces: What, why and how, Carnegie Mellon University Technical Report CMU-CS-02-144, August 2002.
5. Hogwash. Inline packet scrubber. http://sourceforge.net/projects/hogwah.
6. H. Balakrishnan J. Jung, E. Sit and R. Morris. DNS performance and the effectiveness of caching. In Proceedings of the ACM SIGCOMM Internet Measurement Workshop, San Francisco, California, November 2001.
7. J. Jung, V. Paxon, A. W. Berger, and H. Balakrishman. Fast portscan detection using sequential hypothesis testing. In In Proceedings of 2004 IEEE Symposium on Security and Privacy, 2004.
8. J.O Kephart and S. White. Directed-graph epidemiological models of computer viruses. In Proceedings of the 1991 IEEE Computer Society Symposium on Research in Security and Privacy, pages 343–359, May 1991.
9. H. Kim and B. Karp. Autograph: Toward automated, distributed worm signature detection. In Proceedings of the 13th USENIX Security Symposium, San Diego, California, USA, August 2004.
10. D. Moore, C. Shannon, G. Voelker, and S. Savage. Internet quarantine: Requirements for containing self-propagating code. In Proceedings of IEEE INFOCOM 2003, San Francisco, CA, April 2003.
11. Network-Associates. http://vil.nai.com/vil/content/v 100561.htm, 2003.
12. S.E. Schechter, J. Jung, and Arthur W. Berger. Fast detection of scanning worm infections. In In Recent Advances In Intrusion Detection (RAID) 2004, France, September 2004.
13. S. Singh, Cristian Estan, George Varghese, and S. Savage. Automated worm fingerprinting. Proceedings of the 6th ACM/USENIX Symposium on Operating System Design and Implementation, December 2004.
14. S. Staniford. Containment of scanning worms in enterprise networks. Journal of Computer Science, 2004.
15. S. Staniford, V. Paxson, and N. Weaver. How to 0wn the internet in your spare time. In Proceedings of the 11th USENIX Security Symposium, August 2002.
16. Symantec. W32.Blaster.Worm. http://securityresponse.symantec.com/avcenter/venc/data/w32.blaster.worm.html.
17. Symantec. W32.Welchia.Worm. http://securityresponse.symantec.com/avcenter/venc/data/w32.welchia.worm.html.
18. Helen J. Wang, Chuanxiong Guo, Daniel R. Simon, and Alf Zugenmaier. Shield: vulnerability-driven network filters for preventing known vulnerability exploits. In Proceedings of the 2004 conference on Applications, technologies, architectures, and protocols for computer communications, pages 193–204. ACM Press, 2004.
19. Y. Wang, D. Chakrabarti, C. Wang, and C. Faloutsos. Epidemic spreading in real networks: An eigenvalue viewpoint. In Proceedings of the 22nd International Symposium on Reliable Distributed Systems, 2003.
20. Y. Wang and C. Wang. Modeling the effects of timing parameters on virus propagation. In Proceedings of the 2003 ACM workshop on Rapid Malcode, pages 61–66. ACM Press, 2003.
21. N. Weaver, S. Staniford, and V. Paxson. Very fast containment of scanning worms. In Proceedings of the 13th USENIX Security Symposium, 2004.
22. D. Whyte, E. Kranakis, and P.C. van Oorschot. DNS-based detection of scanning worms in an enterprise network. In In Proceedings of Network and Distributed System Security, 2005.
23. M. Williamson. Throttling viruses: Restricting propagation to defeat malicious mobile code. In Proceedings of the 18th Annual Computer Security Applications Conference, Las Vegas, Nevada, December 2002.
24. C. Wong, S. Bielski, J. McCune, and C. Wang. A study of mass-mailing worms. In Proceedings of the 2004 ACM workshop on Rapid Malcode. ACM Press, 2004.
25. C. Wong, C. Wang, D. Song, S. Bielski, and G.R Ganger. Dynamic quarantine of internet worms. In Proceedings of DSN 2004, Florence, Italy, June 2004.
26. C. Zou, W. Gong, and D. Towsley. Code red worm propagation modeling and analysis. In Proceedings of the 9th ACM Conference on Computer and Communication Security, November 2002.