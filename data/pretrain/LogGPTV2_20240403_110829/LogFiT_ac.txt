**Abstract**

System logs are a critical source of information for monitoring and maintaining the security and stability of computer systems. Deep Learning (DL) and Natural Language Processing (NLP) techniques have shown promise in detecting abnormal behavior from these logs. However, existing approaches have limitations: template-based methods struggle with variability in log content, while classification-based methods require labeled data for supervised training. This paper introduces LogFiT, a novel log anomaly detection model that addresses these issues. LogFiT is robust to changes in log content and only requires self-supervised training. The model leverages a pre-trained BERT-based language model fine-tuned to recognize the linguistic patterns of normal log data. During training, LogFiT uses masked token prediction on normal log data. When presented with new log data, the model's top-k token prediction accuracy serves as a threshold to determine whether the new data deviates from the norm. Experimental results on the HDFS, BGL, and Thunderbird datasets show that LogFiT outperforms baseline models, particularly when log data variability is introduced.

**Index Terms—** Service monitoring, fault management, log anomaly detection, deep learning, natural language processing, language modeling.

**I. INTRODUCTION**

Cybercrime results in billions of dollars in losses for businesses annually [1]–[3]. Detecting log anomalies can help safeguard digital infrastructure by identifying abnormal activities, such as network intrusions, from the vast amounts of event logs generated by networked computer systems. Recent research in log anomaly detection has employed DL and NLP techniques, but these methods often face challenges in handling log content variability and the need for labeled data.