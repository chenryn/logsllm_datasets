516,134
16,387
279,383
28,052
5,618
105,440
117,575
552,455
Total
3,219,392
1,728,872
6,798,743
11,809,218
215,165
702,916
24,474,306
Table 1: Data ﬂow overview, mapping sources (top) to sinks (left)
tentially vulnerable data ﬂows, based on the following crite-
ria:
(C1) The data ﬂow ended in a sink that allows, if no fur-
ther sanitization steps were taken, direct JavaScript
execution. Hence, all ﬂow into cookies, Web Storage,
or DOM attribute values were excluded.
(C2) The data ﬂow originates from a source that can imme-
diately be controlled by the adversary, without pro-
grammatic preconditions or assumptions in respect to
the processing code. This criteria eﬀectively excluded
all ﬂows that come from second order sources, such
as cookies or Web Storage, as well as ﬂows from the
postMessage API.
(C3) Only data ﬂows without any built-in escaping methods
and data ﬂows with non-matching escaping methods
were considered. Data ﬂows, for which the observed
built-in escaping methods indeed provide appropriate
protection for the ﬂow’s ﬁnal sink were excluded.
(C4) For each of the remaining data ﬂows we generated ex-
ploits. However, many ﬂows led to the generation of
exactly the same exploit payloads for exactly the same
URL - e.g. when a web page inserts three scripts via
document.write and always includes location.hash
at a similar location. In order to decrease the overhead
for testing the exploits, our system only validates one
of these exploits.
Starting from initial 24,474,306 ﬂows, we successively ap-
plied the outlined criteria to establish the set of relevant
ﬂows:
24, 474, 306 C1−−→ 4, 948, 264 C2−−→ 1, 825, 598
C3−−→ 313.794 C4−−→ 181, 238
(5)
Thus, in total we generated 181,238 test payloads, out of
which a total of 69,987 successfully caused the injected Java-
Script to execute. We discuss the speciﬁcs of these results
in the next Section.
6.4 Found vulnerabilities
In total, we generated a dataset of 181,238 test payloads
utilizing several combinations of sources and sinks. As dis-
cussed in Section 6.3 (C3), all ﬂows which are encoded are
ﬁltered early on. For Google Chromium, which we used in
our testing infrastructure, adhering to this rule we also must
ﬁlter all those exploits that use either location.search or
document.referrer to carry the payloads. This is due to
the fact that both these values are automatically encoded
by Chromium. Hence, we chose to test these vulnerabili-
ties in Internet Explorer 10 whereas the rest of the URLs
were veriﬁed using our aforementioned crawling infrastruc-
ture. Since the number of exploits utilizing search vulnera-
bilities amounts to 38,329 and the sum for referrer reached
5083, the total number of exploits tested in Chromium was
reduced to 137,826, whereas the remaining 43,412 exploits
were tested using Internet Explorer.
Out of these, a total number of 58,066 URLs tested in
Chromium triggered our veriﬁcation payload. Additionally,
we could exploit 11,921 URLs visited in Internet Explorer.
This corresponds to a success rate of 38.61% in total, and a
success rate of 42.13% when only considering vulnerabilities
exploitable in Chromium.
As we discussed earlier, we crawled down one level from
the entry page. We assume that a high number of Web sites
utilize content management systems and thus include the
same client-side code in each of their sub pages. Hence, to
zero in on the number of actual vulnerabilities we decided
to reduce the data set by applying a uniqueness criterion.
For any ﬁnding that triggered an exploit, we therefore re-
trieved the URL, the used break out sequence, the type of
code (inline, eval or external) and the exact location. Next,
we normalized the URL to its corresponding second-level
domain. To be consistent in regards to our selection of do-
mains, we used the search feature on alexa.com to determine
the corresponding second-level domain for each URL. We
then determined for each of the results the tuple:
{domain, break out sequence, code type, code location}
In regards to the code location, we chose to implement the
uniqueness to be the exact line and column oﬀset in case
of external scripts and evals, and the column oﬀset in in-
line scripts. Applying the uniqueness ﬁlter to the com-
plete dataset including those pages only exploitable on In-
ternet Explorer, we found a total of 8,163 unique exploits
on 701 diﬀerent domains, whereas a domain corresponds to
the aforementioned normalized domain. Due to the nature
of our approach, among these were also domains not con-
tained in the top 5000 domains. Thus, we applied another
ﬁlter, removing all exploits from these domains outside the
top 5000. This reduced the number of unique exploits to
6,167, stemming from 480 diﬀerent domains. In respect to
the number of domains we originally crawled, this means
that our infrastructure found working exploits on 9.6% of
the 5000 most frequented Web sites and their sub-domains.
When considering only exploits that work in Chromium,
we found 8,065 working exploits on 617 diﬀerent domains,
including those outside the top 5000. Again ﬁltering out
domains not contained in the 5000 most visited sites, we
found 6,093 working exploits on 432 of the top 5000 domains
or their sub-domains.
Among the domains we exploited were several online bank-
ing sites, a poplar social networking site as well as govern-
mental domains and a large internet-service provider run-
ning a bug bounty program. Furthermore, we found vulner-
abilities on Web sites of two well-known AntiVirus products.
12006.5 Selected Case Studies
During the analysis of our ﬁndings, we encountered several
vulnerabilities which exposed interesting characteristics. In
the following subsections, we provide additional insight into
these cases.
6.5.1 JSONP + HTTP Parameter Pollution
As stated in Section 2, ﬂows into URL sinks are not eas-
ily exploitable. Only if the attacker controls the complete
string, he can make use of data and javascript URLs to ex-
ecute JavaScript code. However, in our dataset we found
a particularly interesting coding pattern, that allows script
execution despite the fact that the attacker only controls
parts of the injected URLs. In order to abuse this pattern a
Web page must assign a partly tainted string to a script.src
attribute that includes a JSONP script with a callback pa-
rameter (See Listing 5).
Listing 5 JSONP script include
var script = d o c u m e n t . c r e a t e E l e m e n t ( ’ script ’)
script . src = " http :// example . org / data . json ? u = "
+ t a i n t e d V a l u e + " & c a l l b a c k = cb_name " ;
In many cases the callback parameter is reﬂected back
into the script in an unencoded/unﬁltered fashion. Hence,
the attacker could inject his own code into the script via
this parameter. However, the callback parameter is hard
coded and the attacker is not able to tamper with it at ﬁrst
sight. Nevertheless, it is possible to inject a second callback
parameter into the script URL via the taintedValue. This
results in the fact that two parameters with the same name
and diﬀerent values are sent to the server when requesting
the script. Depending on the server-side logic the server will
either choose the ﬁrst or the second parameter (We found
both situations, and depending on the position of the taint-
edValue we were able to exploit both situations). Hence, by
conducting this so-called HTTP Parameter Pollution attack,
the attacker is able to inject his value into the content of the
script, which is afterwards embedded into the Web page.
One particularly interesting fact is that simply encoding
the taintedValue will not protect against exploitation. In-
stead, the JSONP callback parameter needs to be sanitized.
During our experiments we found one vulnerable callback
parameter quite often on many diﬀerent Web sites, which
seemed to stem from jQuery (or at least, always called the
same jQuery function).
6.5.2 Broken URL parsing
As browsers sometimes auto-encode certain parts of user
controlled values, it is not possible to inject code into some
of the analyzed sources. One example for this is loca-
tion.search that is auto-encoded by all browser except In-
ternet Explorer. Another source that is encoded by every
modern browser is location.pathname. An injection via
location.pathname is in general not possible until the ap-
plication itself decodes the value. An additional encoding
or sanitization step is therefore not necessary for these val-
ues. This fact, however, also leads to security vulnerabili-
ties when Web developers trust in this auto-encoding feature
while at the same time conducting incorrect URL parsing.
In our analysis, we found many examples where this fact
leads to vulnerabilities. In the following we cover some ex-
amples where code fragments seemed to extract automati-
cally encoded values (and hence no sanitization is needed),
but due to non-standard parsing, extracted also unencoded
parts in malicious cases.
1. Task: Extract host from URL
2. What it really does: Extract everything between www.
and .com (e.g. whole URL)
3. e.g. http://www.example.com/#notEncoded.com
var regex = new RegExp ( " / www \..*\. com / g " );
var result = regex . exec ( l o c a t i o n . href );
1. Task: Extract GET parameter foo
2. What it really does: Extracts something that starts
with foo=
3. e.g. http://www.example.com/#?foo=notEncoded
var regex = new RegExp ( " [\\?&] foo = ( [ ^ & # ] * ) " );
var result = regex . exec ( l o c a t i o n . href );
1. Task: Extract all GET parameters
2. What it really does: Last GET parameter contains the
unencoded Hash
3. e.g. http://example.com/?foo=bar#notEncoded
l o c a t i o n . href . split ( ’? ’ )[1]. split ( ’& ’ )[ x ]
. split ( ’= ’)
6.5.3 Persistent DOM-based XSS
As seen in Table 1, our system captured also some ﬂows
into cookies and into the Web Storage API. However, we did
not include it into our automatic exploit generation. Nev-
ertheless, we were able to manually ﬁnd several persistent
DOM-based XSS. We detected ﬂows that ﬁrst came from
user input and went into Cookie or Web Storage sinks eﬀec-
tively persisting the data within the user’s browser. In the
cases where we could trigger a successful exploit, this data
was then used in a call to eval, hence exposing the Web site
to persistent DOM-based XSS.
6.5.4 window.name ﬂows
Within our dataset, we detected a surprisingly high num-
ber (>2 million) of ﬂows originating from window.name that
we couldn’t explain at ﬁrst sight. Although some of them
were exploitable, we soon discovered the reason for this num-
ber. Most of these ﬂows are not exploitable via DOM XSS
as they are caused by a simple programming error. When
declaring a local variable a developer has to use the var key-
word. If someone declares a variable named name inside a
function and misses the var keyword or if a local variable is
created directly within a script block that is executed in the
global scope, the variable is declared global (See Listings 6
and 7). Since inside a browser, the global object is window,
the data is written to window.name. If the same variable is
used within a call to a sink within the same script block, the
corresponding ﬂow is not exploitable as window.name was
overwritten with benign data. However, this fact represents
another serious issue: window.name is one of the very few
1201Listing 6 window.name bug 1: Missing var keyword
7. RELATED WORK
f u n c t i o n test (){
name = d o S o m e t h i n g ();
d o c u m e n t . write ( name );
};
Listing 7 window.name bug 2: Declaration within the
global scope
properties that can be accessed across domain boundaries.
Hence, any data that is written to this property can be ac-
cessed by third parties. This programming error, therefore,
represents a serious information leakage problem, if sensi-
tive data is written to such a global name variable. Given
the huge amount of ﬂows, it is very likely that this pattern
could be misused to steal sensitive information.
6.6 Effectiveness of Chromium’s XSS Filter
Modern browsers like Chromium and its commercial coun-
terpart Google Chrome are equipped with client-side ﬁlter
capabilities aiming at preventing XSS attacks [1]. In order to
analyze the eﬀectiveness of Chromium’s XSS Filter, we uti-
lized our successful exploits and tried to execute them with
the activated ﬁlter. Out of the 701 domains we found, 300
domains were still susceptible to XSS even with Chromium’s
auditor enabled.
After further examination, we found three distinguishing
characteristics for these working exploits. For one, none of
the exploits abusing JavaScript sinks, such as eval(), were
detected by XSS Auditor. This stems from the fact that
the auditor is implemented inside the HTML parser and
thus cannot detected direct JavaScript sinks. Furthermore,
exploits that were caused by remote script includes were not
detected. The third type of undetected exploits was caused
by JSONP vulnerabilities as discussed in Section 6.5.1.
On a positive note, in our study, we found that none of the
exploits that targeted inline vulnerabilities passed through
the ﬁlter. However, please note, that this experiment carries
no reliable indication of protection robustness in respect to
the exploits, that were stopped. We did not make any at-
tempts to obfuscate the exploit payload [12] or use other
ﬁlter evasion tricks [13].
In 2011 Nikiforakis demonstrated that Chrome’s ﬁlter is
not able to cope with exploits that utilize more than one
injection point at once [21].
If we take our ﬁgures from