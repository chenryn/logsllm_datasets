F = {f1, . . . , fl} denote the set of ACFs. Without loss of
generality, we assume that each object o = {d1, . . . , dm} is
composed of ﬁnite number of atomic data records di ∈ D that
cannot be split into smaller pieces without losing semantics.
Then, the predicates and ACFs can be expressed as follows:
∀d ∈ o,
f : D → {∅ ∪ D ∪ {0, 1}∗}
∀o ∈ O p : O → {∅ ∪ D ∪ {0, 1}∗}|o|
p(o) = {f (d) | f (d) (cid:54)= ∅ ∧ d ∈ o}
(1)
VI. SYSTEM ARCHITECTURE
Vigiles is an application ﬁrewall that provides FGAC capa-
bilities to the MapReduce systems. It employs a middleware
architecture that lays between the untrusted end-users and
underlying OS/MapReduce system by authorizing all data
accesses. Fig. 1 shows the overview of the Vigiles system, in
which thick, black-dashed, red-dashed and black arrows indicate
the actions of end-users, admins, Vigiles and MapReduce
system, respectively.
Vigiles authenticates the users by using the same user
IDs and passwords of host OS. In the system, there are
the end-users and admins. The end-
two types of users:
users have no responsibilities speciﬁc to Vigiles. They write
MapReduce jobs as usual and send it to Vigiles along with
the required parameters, such as input/output names and job
speciﬁc variables. Other than the interface provided by Vigiles,
they have no communication with the underlying MapReduce
system nor OS. Their submitted MapReduce jobs have also
limited view of the input data (e.g., authorized view in [7])
because of the FGAC predicates. On the other hand, the admins
are responsible from the input ﬁles in HDFS and conﬁguration
of ACFs. They load data into Vigiles system and setup the
conﬁguration of ACFs so as to activate FGAC predicates,
M = (S,O, P). Furthermore, they also load the libraries,
which are used in the ACFs for the ﬁrst time. Unlike end-users,
they can access and conﬁgure the MapReduce system and OS.
Vigiles encapsulates the OS/MapReduce system such that
the users can only communicate with the underlying systems
through Vigiles. It accepts authenticated network communica-
tion from the users. If the communication is established by an
admin, the OS’s terminal is returned as interface. Otherwise,
data, parsing an HTML code, and decompression of images
are three examples of decompose phase.
2) Fetch: This phase aims to detect the indexes of targeted
tokens. A list of tokens, which is result of a decompose phase
or another fetch phase, can be used as input. It ﬁnds the indexes
by employing a search algorithm and the meta-data of input if
exists, and outputs both the list and indexes. Regular expression
based text search and fetching columns on Google’s BigTable
are two examples of fetch phase.
3) Action: This phase aims to apply ACF speciﬁc action
to the indexed tokens in a given list. A list of tokens and
indexes are its input. It applies the action to each indexed
token. According to given conﬁguration, it may output the
ﬁltered list to another decompose phase, or merge the list, and
output using one of three options: (1) Nothing, (2) the original
key-value pair, and (3) the modiﬁed key-value pair. Sanitization
of sensitive tokens and reduction of a list to indexed tokens
are two examples of action phase.
Vigiles automatically generates the ACFs by means of
given conﬁguration. The admins organize the conﬁguration by
clearly stating each phase. Vigiles provides many fundamental
decomposition, fetching and action algorithms by default, but
new algorithms can also be loaded if necessary. In order to
add new algorithms to any of the aforementioned phases, the
admins need to load the required libraries to the Vigiles system
and set their properties in the conﬁguration. A sample ACF
conﬁguration is provided in §VI-B.
text.tokenize
value
text
’|’
text.regex_search
tokenize
text
’\\d{3}-\\d{3}-\\d{4}’
string.replace
search
text
’*’
’true’
Fig. 2. Sample conﬁguration ﬁle
B. Sample ACF Conﬁguration
Fig. 2 shows a sample conﬁguration for an ACF, where
the phone numbers in ’ddd-ddd-dddd’ format are sanitized in
text data. The decompose phase takes value as input, and use
”text.tokenize” function to tokenize the value into words by
using ’|’ as separator. Then, the fetch phase uses the output of
decompose phase as input and searches the words matching
given regular expression by using ”text.regex search” function.
Finally, the action phase uses the output of decompose phase
Fig. 1. Vigiles System Overview
a special interface is returned, where the end-user can submit
MapReduce jobs and related parameters, such as input/output
paths, variables. Since the current implementation of Vigiles
system uses the Java Security [23] to conﬁne the MapReduce
jobs in a secure environment, Vigiles only accepts managed Java
bytecode programs and conservatively rejects those that contain
native code or that link to type-unsafe libraries. We are planning
to support native libraries in the future by integrating Vigiles
with the Robusta system [22]. When a job is accepted, Vigiles
runs the job on behalf of the user, and by so doing leverage
the current ﬁle level access control policy of MapReduce
system. Thus, the outputs of end-users are protected by the
current access control mechanism. Moreover, the ACFs are
generated by means of the given conﬁguration, and injected
into MapReduce system (see §VI-A and §VI-C for details).
Vigiles can use any multi-user MapReduce and OS as
follows: (1) The employed MapReduce system is consolidated
with the FGAC predicates. (2) All outside communications of
the underlying OS are prevented, except the ones initiated by
Vigiles. (3) The MapReduce jobs are conﬁned by a sandbox
technique [21]–[23] so as to prevent unauthorized data accesses.
A. ACF Generation
Suppose that an unstructured text data, containing sensitive
entries, is stored in a MapReduce system, and an ACF is
going to be designed to sanitize the sensitive entries. Since the
data is unstructured, the sensitive entries need to be located for
sanitization. An intuitive approach is ﬁrst to decompose the text
into words, then ﬁnd the indexes of sensitive words, and sanitize
them one by one. Inspired by the above example, Vigiles
generates the ACFs by combining following three phases:
1) Decompose: This phase aims to produce a list of small
processable tokens from its input. The input key-value pairs
and the output of other phases (i.e., fetch and action) can be
used as input. It fragments the input into a list of small tokens
by means of a given algorithm and the meta-data of input
if exists, and outputs the produced list. Tokenization of text
End-user 1End-user 2End-user kJob 1. . . . . .untrustedtrustedJob 2Job kRun as User 1Run as User 2Run as User kAdminsMapReduce systemHardened operating systemVigilesInput DataFGAC predicatesMapMapReduceReduceOutputConfigurationACF Generationas input. It sanitizes the indexed words, merges the list and
emits the modiﬁed value.
C. ACF Injection
Vigiles depends on our novel ACF injection technique to
enforce the security policies. This technique enables enforcing
the FGAC predicates to the processing key-value pairs in
a complete manner by incurring minimal overhead to the
performance. In this section, we elucidate the details of this
technique. Since the current implementation of Vigiles is based
on Apache Hadoop and AspectJ [24], we will explain the
details of the injection technique through them.
transformation, it has to implement RecordReader interface
to be accepted by the system. Because, Hadoop system uses
RecordReader’s methods to create key-value pairs to be pro-
cessed by Map functions. The algorithm in Fig. 4 shows how the
RecordReader interface is called by Hadoop. Four methods of
RecordReader are important for our injection technique: initial-
ize(), nextKeyValue() and getCurrentKey()/getCurrentValue().
The initialize() method is called once when a RecordReader
object is initialized. The nextKeyValue() method is called to
read an individual record and create a key-value pair corre-
spondingly. The getCurrentKey()/getCurrentValue() methods
are called to get current key-value pairs.
1: Input : A data split and conﬁguration
2: Output:  pairs for mapper
3: initialize()
4: ﬁlepath ← determineInput()
5: user ← determineUser()
6: ACF ← generateACF(user, ﬁlepath, conﬁguration)
7: while nextKeyValue()=true do
key ← getCurrentKey()
8:
value ← getCurrentValue()
9:
if ACFpredicate(key,value)=allow then
10:
11:
12:
13: end while
end if
sendToMap(ACFmodif y(key),ACFmodif y(value))
Fig. 5.
Input preprocessing after ACF injection in Hadoop
We determined these four methods as our injection points
(i.e., our pointcuts) in Hadoop version 1.1.2. Three types of
aspects are injected into these pointcuts: (1) initialization aspect
is injected to initialize() method; (2) predicate aspect is injected
to nextKeyValue() method; and (3) modiﬁcation aspects are
injected to getCurrentKey()/getCurrentValue() methods. The
algorithm in Fig. 5 shows how the injected aspects augment the
execution ﬂow of RecordReader methods. The initialization
aspect runs once at the beginning of each MapReduce job
when initialize() is called. Firstly, it determines the input ﬁle(s)
and the owner of MapReduce job (o ∈ O and s ∈ S in
the FGAC predicates M = (S,O, P), respectively). Then,
it generates the ACF by using the ﬁle(s), user ID and the
given conﬁguration (If any access control model, such as role-
based access control, is employed by Vigiles, it is handled
here as well). The generated ACF is attached to the job to be
employed in the other aspects later. Moreover, the auxiliary
data structures are constructed in the initialization aspect if they
are used by the ACF. For example, a hash map for keyword
whitelisting can be constructed by reading keywords from a ﬁle.
The predicate aspect runs whenever nextKeyValue() method
is called. It checks the key and/or value, and either grants
access by returning the original key-value or rejects access
by returning nothing. The modiﬁcation aspects run whenever
getCurrentKey()/getCurrentValue() are called. They modify the
returned key-value pairs by performing modify action. Since
the predicate aspect always runs before modiﬁcation aspects,
reject and grant actions are performed before modify action.
Fig. 3.
Injection of ACFs and access restriction aspects
When a job is submitted to a MapReduce system, its input
data has to be transformed into formatted key-value pairs by
RecordReader classes before passing to the map functions.
We leverage this obligation to enforce our FGAC predicates
to the submitted jobs due to following reasons: (1) Prior to
this transformation, the raw input data is an enigma for the
jobs. (2) This transformation is the only legitimate way to
access data for the MapReduce jobs. Therefore, applying FGAC
predicates during this transformation guarantees that the only
data accessed by jobs is the authorized views [7] created by
FGAC predicates. Fig. 3 shows the overview of ACF injection.
Note that these processes are similar for other MapReduce
systems and AOPs, too.
1: Input : A data split
2: Output:  pairs for mapper
3: initialize()
4: while nextKeyValue()=true do
key ← getCurrentKey()
5:
value ← getCurrentValue()
6:
sendToMap(key,value)
7:
8: end while
Fig. 4.
Input preprocessing in Hadoop
In Hadoop environment whenever a class, whether existing
in the system or created by an end-user, is used for key-value
Split 1Split 2Split 3Split 4Map 1Map 2Reduce 1Reduce 2Output 1Output 2RecordReader 1RecordReader 2initialization aspectpredicate aspectsmodification aspectsinitialization aspectUser jobpredicate aspectsmodification aspectsIllegitimateaccessesSandboxTherefore, the admins need to consider it when preparing
ACF conﬁgurations. Note that in order to use other version
of Hadoop or another MapReduce system, only these four
pointcuts need to be modiﬁed. The other parts of Vigiles,
including ACFs and conﬁgurations, can be used without any
modiﬁcation. This increases the modularity of our system.
Security Discussion: As previously mentioned, Vigiles only
accepts managed Java bytecode based MapReduce jobs. This
restriction enables Vigiles to adopt a secure sandbox technique
(i.e., Java Sandbox [23]) so as to prevent a broad class of
security breaches discussed in §IV. Thus, unorthodox ways
to access data are prevented in Hadoop—a MapReduce job
can only access data in HDFS through RecordReader interface
which is consolidated by FGAC predicates. In other words,
whenever a MapReduce job is accepted and run by Vigiles
system, the given ACFs are enforced on the input data in a
correct and complete manner.
Moreover, Vigiles improves the performance of access
control system by using optimized operations—pushing all
operations to initialization aspect as much as possible. Because,
unlike other operations that are run per record, the initialization
aspect run once for each job.
VII. EVALUATION
We evaluated the efﬁciency and scalability of Vigiles via a
series of experiments. We ﬁrst explain the details of experiment
setup, data generation, sample ACFs and the MapReduce jobs.
Then, we present the empirical results.
A. Setup
We conducted our experiments on a cluster containing 14
nodes. Each node consists of a Pentium IV processor with
290GB-320GB disk space and 4GB of main memory. The
cluster is setup using Hadoop 1.1.2 and AspectJ 1.7.3. To
enable AOP in Hadoop, the AspectJ JARs and compiled aspects
are placed into lib folder of each node in the cluster.
B. Data Generation
We have randomly generated ﬁve input ﬁles, formated as