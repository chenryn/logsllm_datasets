other party. One of the main distinctions between XONN
and the state-of-the-art solutions is that XONN can be au-
tomatically adapted to the malicious security using cut-and-
choose techniques [29, 30, 31]. These methods take a GC
protocol in HbC and readily extend it to the malicious se-
curity model. This modiﬁcation increases the overhead but
enables a higher security level. To the best of our knowledge,
there is no practical solution to extend the customized mixed-
protocol frameworks [7, 9, 10, 25] to the malicious security
model. Our GC-based solution is more efﬁcient compared
to the mixed-protocol solutions and can be upgraded to the
malicious security at the same time.
1508    28th USENIX Security Symposium
USENIX Association
4 The XONN Implementation
1
2
In this section, we elaborate on the garbling/evaluation im-
plementation of XONN. All of the optimizations and tech-
niques proposed in this section do not change the security
or correctness in anyway and only enable the framework’s
scalability for large network architectures.
We design a new GC framework with the following design
principles in mind: (i) Efﬁciency: XONN is designed to have
a minimal data movement and low cache-miss rate. (ii) Scal-
ability: oblivious inference inevitably requires signiﬁcantly
higher memory usage compared to plaintext evaluation of
neural networks. High memory usage is one critical short-
coming of state-of-the-art secure computation frameworks.
As we show in our experimental results, XONN is designed
to scale for very deep neural networks that have higher accu-
racy compared to networks considered in prior art. (iii) Mod-
ularity: our framework enables users to create Boolean de-
scription of different layers separately. This allows the hard-
ware synthesis tool to generate more optimized circuits as we
discuss in Section 4.1. (iv) Ease-to-use: XONN provides a
very simple API that requires few lines of neural network de-
scription. Moreover, we have created a compiler that takes a
Keras description and automatically creates the network de-
scription for XONN API.
XONN is written in C++ and supports all major GC op-
timizations proposed previously. Since the introduction of
GC, many optimizations have been proposed to reduce the
computation and communication complexity of this proto-
col. Bellare et al. [32] have provided a way to perform
garbling using efﬁcient ﬁxed-key AES encryption. Our im-
plementation beneﬁts from this optimization by using Intel
AES-NI instructions. Row-reduction technique [33] reduces
the number of garbled tables from four to three. Half-Gates
technique [34] further reduces the number of rows in the
garbled tables from three to two. One of the most inﬂuen-
tial optimizations for the GC protocol is the free-XOR tech-
nique [12] which makes XOR, XNOR, and NOT almost free
of cost. Our implementation for Oblivious Transfer (OT) is
based on libOTe [35].
4.1 Modular Circuit Synthesis and Garbling
In XONN, each layer is described as multiple invocations of
a base circuit. For instance, linear layers (CONV and FC) are
described by a VDP circuit. MaxPool is described by an OR
circuit where the number of inputs is the window size of the
MaxPool layer. BA/BN layers are described using a com-
parison (CMP) circuit. The memory footprint is signiﬁcantly
reduced in this approach: we only create and store the base
circuits. As a result, the connection between two invocations
of two different base circuits is handled at the software level.
We create the Boolean circuits using TinyGarble [36]
hardware synthesis approach. TinyGarble’s technology li-
braries are optimized for GC and produce circuits that have
G
a
r
b
l
e
r
E
v
a
u
a
l
t
o
r
o
u
p
u
t
t
l
a
b
e
s
l
o
f
l
a
y
e
r
L
-
1
.
.
.
4
V
D
P
V
D
P
V
D
P
.
.
.
t
o
u
p
u
t
l
a
b
e
s
o
l
f
l
a
y
e
r
L
.
.
.
C
M
P
C
M
P
C
M
P
.
.
.
o
u
p
u
t
t
l
a
b
e
s
o
l
f
l
a
y
e
r
L
+
1
f labels
3
label 
selection
5
c labels
garbled 
tables
label 
selection
garbled 
tables
2
1
f labels
3
c labels
o
u
p
u
t
t
l
a
b
e
s
o
l
f
l
a
y
e
r
L
-
1
.
.
.
4
V
D
P
V
D
P
V
D
P
.
.
.
o
u
p
u
t
t
l
a
b
e
s
o
l
f
l
a
y
e
r
L
.
.
.
C
M
P
C
M
P
C
M
P
.
.
.
o
u
p
u
t
t
l
a
b
e
s
o
l
f
l
a
y
e
r
L
+
1
5
Figure 7: XONN modular and pipelined garbling engine.
low number of non-XOR gates. Note that the Boolean circuit
description of the contemporary neural networks comprises
between millions to billions of Boolean gates, whereas, syn-
thesis tools cannot support circuits of this size. However,
due to XONN modular design, one can synthesize each base
circuit separately. Thus, the bottleneck transfers from the
synthesis tool’s maximum number of gates to the system’s
memory. As such, XONN effectively scales for any neural
network complexity regardless of the limitations of the syn-
thesis tool as long as enough memory (i.e., RAM) is avail-
able. Later in this section, we discuss how to increase the
scalability by dynamically managing the allocated memory.
Pipelined GC Engine. In XONN, computation and commu-
nication are pipelined. For instance, consider a CONV layer
followed by an activation layer. We garble/evaluate these
layers by multiple invocations of the VDP and CMP circuits
(one invocation per output neuron) as illustrated in Figure 7.
Upon ﬁnishing the garbling process of layer L − 1, the Gar-
bler starts garbling the Lth layer and creates the random la-
bels for output wires of layer L. He also needs to create
the random labels associated with his input (i.e., the weight
USENIX Association
28th USENIX Security Symposium    1509
parameters) to layer L. Given a set of input and output la-
bels, Garbler generates the garbled tables, and sends them
to the Evaluator as soon as one is ready. He also sends one
of the two input labels for his input bits. At the same time,
the Evaluator has computed the output labels of the (L − 1)th
layer. She receives the garbled tables as well as the Garbler’s
selected input labels and decrypts the tables and stores the
output labels of layer L.
Dynamic Memory Management. We design the framework
such that the allocated memory for the labels is released as
soon as it is no longer needed, reducing the memory usage
signiﬁcantly. For example, without our dynamic memory
management, the Garbler had to allocate 10.41GB for the
labels and garbled tables for the entire garbling of BC1 net-
work (see Section 7 for network description). In contrast, in
our framework, the size of memory allocation never exceeds
2GB and is less than 0.5GB for most of the layers.
4.2 Application Programming Interface (API)
XONN provides a simpliﬁed and easy-to-use API for oblivi-
ous inference. The framework accepts a high-level descrip-
tion of the network, parameters of each layer, and input struc-
ture. It automatically computes the number of invocations
and the interconnection between all of the base circuits. Fig-
ure 8 shows the complete network description that a user
needs to write for a sample network architecture (the BM3
architecture, see Section 7). All of the required circuits are
automatically generated using TinyGarble [36] synthesis li-
braries. It is worth mentioning that for the task of oblivious
inference, our API is much simpler compared to the recent
high-level EzPC framework [25]. For example, the required
lines of code to describe BM1, BM2, and BM3 network ar-
chitectures (see Section 7) in EzPC are 78, 88, and 154, re-
spectively. In contrast, they can be described with only 6, 6,
and 10 lines of code in our framework.
1
2
3
4
5
6
7
8
9
10
I NPUT  28  1  8
CONV  5  16  1  0  OCA     
ACT   
MAXPOOL  2 
CONV  5  16  1  0
ACT