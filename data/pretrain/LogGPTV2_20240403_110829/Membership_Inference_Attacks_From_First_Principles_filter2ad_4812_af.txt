All our experiments so far have involved attacking models
that we ourselves have trained. To ensure that we did not some-
how train weakly accidentally private (or non-private) models,
we now show that our attacks also succeed on existing pre-
trained state-of-the-art models. To this end, we load standard
models pre-trained by Phan [51] on the complete CIFAR-10
training set (50,000 examples). We train 256 shadow models
Fig. 12: Our attack succeeds against real state-of-the-art
CIFAR-10 models [51]. The attacker trains shadow models
on a random subset of 50,000 points from the entire CIFAR-
10 dataset. The attack performs best when the shadow models
have the same architecture as the target model, but training
different models still leads to a strong attack.
by using the same training code and subsampling 50,000
points at random from the entire CIFAR-10 dataset (60,000
examples). On average, we have 213 IN models and 43 OUT
models per example. Figure 12 shows our attack’s true-positive
rate at a 0.1% FPR for various canonical model architectures.
We consider two attack variants: (1) the adversary knows
the target model’s architecture and uses it to train the shadow
models; (2) the shadow models use a different architecture than
the target model. Since we only have 43 models to estimate the
distribution ˜Qout), esitmating a global variance for all examples
performs best. The results of this experiment are qualitatively
similar to those in Section VI-E: (1) the model architecture
has a small effect on the privacy leakage (e.g., the attack
works better against a ResNet-18 than against a MobileNet-
v2); (2) the attack works best when the shadow models share
the same architecture as the target model, but it is robust to
architecture mismatches. For example, attacking a ResNet-34
model with either ResNet-18 or ResNet-50 shadow models
leads to a minor drop in attack success rate (from 5% TPR to
4% TPR).
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
121908
CNN-16CNN-32CNN-64WRN28-1WRN28-2WRN28-10Targetmodelarchitecture10−310−210−1100TPR@0.1%FPRShadowmodelarchitectureCNN-16CNN-32CNN-64WRN28-1WRN28-2WRN28-10SGDSGDMAdamTargetmodeloptimizer10−310−210−1100TPR@0.1%FPRShadowmodeloptimizerSGDSGDMAdamNone+Mirror+Shift+CutoutTargetmodelaugmentation10−310−210−1100TPR@0.1%FPRShadowmodelaugmentationNone+Mirror+Shift+CutoutVGG16ResNet18ResNet34ResNet50DenseNet121Inception-v3MobileNet-v2Targetmodelarchitecture10−210−1100TPR@0.1%FPRShadowmodelarchitectureVGG16ResNet18ResNet34ResNet50DenseNet121Inception-v3MobileNet-v2VIII. CONCLUSION
As we have argued throughout
this paper, membership
inference attacks should focus on the problem of achieving
high true-positive rates at low false-positive rates. Our attack
presents one way to succeed at this goal. There are a number
of different evaluation directions that we hope future work will
explore under this direction.
Membership inference attacks as a privacy metric. Both
researchers [42] and practitioners [63] use membership infer-
ence attacks to measure privacy of trained models. We argue
that these metrics should use strong attacks (such as ours)
in order to accurately measure privacy leakage. Future work
using membership inference attacks should consider the low
false-positive rate regime, to better understand if the privacy
of even just a few users can be conﬁdently breached.
Usability improvements to membership inference attacks.
The key limitation of per-example membership inference at-
tacks is that they require new hyperparameters that need to be
learned from the data. While it is much more important that
attacks are strong (even if slow) as opposed to fast (but weak),
we hope that future work will improve the computational
efﬁciency of our attack approach, in order to allow it to be
deployed in more settings.
Improving other privacy attacks with our method. Mem-
bership inference attacks form the basis for many other privacy
attack methods [3, 4, 17]. Our membership inference method,
in principle, should be able to directly improve these attacks.
Rethinking our current understanding of MIA results. The
literature on membership inference attacks has answered a
number of memorization questions. However, many (or even
most) of these prior papers focused on the inadequate metric
of average-case attack success rates, instead of on the low
false-positive rate regime. As a result it will be necessary to
re-investigate prior results from this perspective:
• Do previously-“broken” [26, 45] defenses prevent our
attack? Prior defenses were only ever shown to be in-
effective at preventing an adversary from succeeding on
average—not conﬁdently at low false-positive rates.
• How does differential privacy interact with our improved
attacks? We have preliminary evidence that vacuous guar-
antees might prevent our low-FPR attacks (Section A-A).
• Are attacks with reduced capabilities possible? For ex-
ample,
label-only attacks [6, 34, 54] can match the
balanced accuracy of shadow-model approaches. But do
these attacks work at low false-positive rates?
• Are attacks with extra capabilities more effective? Prior
work has shown that access to gradient queries [46] or
intermediate models [58] improves attack AUC. However,
does this observation hold at low false-positive rates?
We hope that future work will be able to answer these
questions, among many more, in order to better evaluate (and
develop) techniques that preserve the privacy of training data.
By developing attacks that succeed low false-positive rates,
we can evaluate privacy not as a measurement of the average
user, but of the most vulnerable.
Fig. 13: Out-of-distribution training examples are less private.
B. Why are some examples less private?
While our average attack success rate is modest, the success
rate at low false-positive rates can be very high. This suggests
that there is a subset of examples that are easier to attack than
others. While a full investigation of this is beyond the scope
of our paper, we ﬁnd an important factor behind why some
samples are less private is that they are out-of-distribution.
To make this argument, we intentionally inject out-of-
distribution examples into a model’s training dataset and com-
pare the difﬁculty of attacking these newly inserted samples
versus typical examples. Speciﬁcally, we insert 1,000 exam-
ples from various out-of-distribution sources into the 50,000-
example CIFAR-10 training dataset to form a new augmented
51,000 example dataset. We then train shadow models on this
dataset, run our attack, and measure the distinguishability of
distributions of losses for IN and OUT models for each of the
1,000 newly inserted examples (we use a simple measure of
distance between distributions here, deﬁned as d = |µin−µout|
).
σin+σout
Figure 13 plots the distribution of these “privacy scores”
assigned to each example. As a baseline, in blue, we show
the distribution of privacy scores for the standard CIFAR-10
dataset; these are tightly concentrated around 0.
Next we show the privacy scores of examples inserted from
the CINIC-10 dataset, which are drawn from ImageNet. Due to
this slight distribution shift, the CINIC-10 images have a larger
privacy score on average: it is easier to detect their presence
in the dataset because they are slightly out-of-distribution.
We can extend this further by inserting intentionally mis-
labeled images that are extremely out-of-distribution. If we
choose 1,000 images (shown in red) from the CIFAR-10 test
set and assign new random labels to each image, then we
get a much higher privacy score for these images. Finally,
we interpolate between the extreme OOD setting of random
(and thus incorrectly) labeled CIFAR-10 images and correctly-
labeled CINIC-10 by inserting randomly labeled images from
CIFAR-100 (shown in green). Because these images come
from a disjoint class distribution, models will not typically
be conﬁdent on their label one way or another unless they are
seen during training. The privacy scores here fall in between
correctly labeled CINIC-10 and incorrectly labeled CIFAR-10.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
131909
012345Privacy score0100200300400FrequencyCIFAR-10, Correct LabelsCINIC-10, Correct LabelsCIFAR-100, Random LabelsCIFAR-10, Random LabelsACKNOWLEDGEMENTS
We are grateful
to Thomas Steinke, Dave Evans, Reza
Shokri, Sanghyun Hong, Alex Sablayrolles, Liwei Song, and
the anonymous reviewers for comments on drafts of this paper.
REFERENCES
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMa-
han, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning
In Proceedings of the 2016 ACM
with differential privacy.
SIGSAC Conference on Computer and Communications Secu-
rity, page 308–318. ACM, 2016.
[2] Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and
Kunal Talwar. When is memorization of irrelevant training
In Proceedings
data necessary for high-accuracy learning?
of the 53rd Annual ACM SIGACT Symposium on Theory of
Computing, pages 123–132, 2021.
[3] Nicholas Carlini, Chang Liu,
´Ulfar Erlingsson, Jernej Kos,
and Dawn Song. The secret sharer: Evaluating and testing
unintended memorization in neural networks. In 28th USENIX
Security Symposium (USENIX Security 19), pages 267–284,
2019.
[4] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagiel-
ski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom
Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training
In 30th USENIX Security
data from large language models.
Symposium (USENIX Security 21), 2021.
[5] Mia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan Cao,
Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan Wang, Andrew M
Dai, Zhifeng Chen, et al. Gmail smart compose: Real-time
In ACM SIGKDD International Conference
assisted writing.
on Knowledge Discovery & Data Mining, pages 2287–2295,
2019.
[6] Christopher A Choquette-Choo, Florian Tramer, Nicholas Car-
lini, and Nicolas Papernot. Label-only membership inference
In International Conference on Machine Learning,
attacks.
pages 1964–1974. PMLR, 2021.
[7] Ekin D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-
van, and Quoc V. Le. Autoaugment: Learning augmentation
policies from data, 2018.
[8] Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and
Amos J Storkey. CINIC-10 is not Imagenet or CIFAR-10. arXiv
preprint arXiv:1810.03505, 2018.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei.
ImageNet: A large-scale hierarchical image
database. In IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009.
[10] Cynthia Dwork and Aaron Roth. The algorithmic foundations
of differential privacy. Found. Trends Theor. Comput. Sci., 9
(3-4):211–407, 2014.
[11] Cynthia Dwork, Adam Smith, Thomas Steinke, Jonathan Ull-
man, and Salil Vadhan. Robust traceability from trace amounts.
In 2015 IEEE 56th Annual Symposium on Foundations of
Computer Science, pages 650–669. IEEE, 2015.
[12] Cynthia Dwork, Adam Smith, Thomas Steinke, and Jonathan
Ullman. Exposed! a survey of attacks on private data. Annual
Review of Statistics and Its Application, 4:61–84, 2017.
[13] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko,
Susan M Swetter, Helen M Blau, and Sebastian Thrun.
Dermatologist-level classiﬁcation of skin cancer with deep
neural networks. Nature, 542(7639):115–118, 2017.
[14] Vitaly Feldman. Does learning require memorization? a short
tale about a long tail. In Proceedings of the 52nd Annual ACM
SIGACT Symposium on Theory of Computing, pages 954–959,
2020.
[15] Vitaly Feldman and Chiyuan Zhang. What neural networks
memorize and why: Discovering the long tail via inﬂuence
estimation. arXiv preprint arXiv:2008.03703, 2020.
[16] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model
inversion attacks that exploit conﬁdence information and basic
In Proceedings of the 22nd ACM SIGSAC
countermeasures.
Conference on Computer and Communications Security, pages
1322–1333, 2015.
[17] Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita
Borisov. Property inference attacks on fully connected neural
networks using permutation invariant representations. In Pro-
ceedings of the 2018 ACM SIGSAC conference on computer
and communications security, pages 619–633, 2018.
[18] Jamie Hayes, Luca Melis, George Danezis, and Emiliano
De Cristofaro. LOGAN: Membership inference attacks against
In Proceedings on Privacy Enhancing
generative models.
Technologies (PoPETs), pages 133–152. De Gruyter, 2019.
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition, 2015.
[20] Xinlei He, Jinyuan Jia, Michael Backes, Neil Zhenqiang Gong,
and Yang Zhang. Stealing links from graph neural networks. In
30th USENIX Security Symposium (USENIX Security 21), 2021.
[21] Grant Ho, Aashish Sharma, Mobin Javed, Vern Paxson, and
David Wagner. Detecting credential spearphishing in enterprise
settings. In 26th USENIX Security Symposium (USENIX Secu-
rity 17), pages 469–485, 2017.
[22] Nils Homer, Szabolcs Szelinger, Margot Redman, David Dug-
gan, Waibhav Tembe, Jill Muehling, John V Pearson, Dietrich A
Stephan, Stanley F Nelson, and David W Craig. Resolving
individuals contributing trace amounts of dna to highly complex
mixtures using high-density snp genotyping microarrays. PLoS
genetics, 4(8), 2008.
[23] Robert A Jacobs.
Increased rates of convergence through
learning rate adaptation. Neural networks, 1(4):295–307, 1988.
[24] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing
differentially private machine learning: How private is private
SGD? arXiv preprint arXiv:2006.07709, 2020.
[25] Bargav Jayaraman, Lingxiao Wang, David Evans, and Quan-
quan Gu. Revisiting membership inference under realistic as-
sumptions. In Proceedings on Privacy Enhancing Technologies
(PoPETs), 2021.
[26] Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and
Neil Zhenqiang Gong. Memguard: Defending against black-
box membership inference attacks via adversarial examples. In
Proceedings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security, pages 259–274, 2019.
[27] Alex Kantchelian, Michael Carl Tschantz, Sadia Afroz, Brad
Miller, Vaishaal Shankar, Rekha Bachwani, Anthony D Joseph,
and J Doug Tygar. Better malware ground truth: Techniques for
weighting anti-virus vendor labels. In Proceedings of the 8th
ACM Workshop on Artiﬁcial Intelligence and Security, pages
45–56, 2015.
[28] Zico Kolter and Marcus A Maloof. Learning to detect and
classify malicious executables in the wild. Journal of Machine
Learning Research, 7(12), 2006.
[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images, 2009.
[30] Anders Krogh and John A Hertz. A simple weight decay
can improve generalization. In Advances in neural information
processing systems, pages 950–957, 1992.
[31] Aleksandar Lazarevic, Levent Ertoz, Vipin Kumar, Aysel Ozgur,
and Jaideep Srivastava. A comparative study of anomaly detec-
tion schemes in network intrusion detection. In Proceedings of
the 2003 SIAM international conference on data mining, pages
25–36. SIAM, 2003.
[32] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner.
Gradient-based learning applied to document recognition. Pro-
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
141910
ceedings of the IEEE, 86(11):2278–2324, 1998.
[33] Klas Leino and Matt Fredrikson. Stolen memories: Leverag-
ing model memorization for calibrated white-box membership
inference. arXiv preprint arXiv:1906.11798, 2019.
[34] Zheng Li and Yang Zhang. Membership leakage in label-only
exposures. arXiv preprint arXiv:2007.15528, 2020.
[35] Yugeng Liu, Rui Wen, Xinlei He, Ahmed Salem, Zhikun Zhang,