24.5
14.1
22.0
21.0
6.7
19.5
5.6
10.2
DA(E)
13.4
14.3
14.3
14.1
15.9
10.5
4.8
14.2
5.6
8.7
SA
32.5
25.0
27.5
17.5
17.5
17.5
17.5
18.2
10.0
10.0
SA(E) ML ML(E) AV AV(E)
12.5
13.5
15.0
15.9
0.0
12.8
15.0
12.5
5.8
6.0
42.5
25.0
20.0
32.5
27.5
27.5
25.0
27.3
25.0
22.3
41.7
22.5
20.0
29.5
25.0
22.5
22.5
25.0
21.0
20.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
itors the change of the Content Provider of SMS, steals all SMS
messages, and sends out to a speciﬁc remote server.
We can conclude that AVs have made efforts to infer the seman-
tics of code as the behavior is split into two methods. However,
the inference is quite limited. We crafted malware samples by em-
ploying evasion techniques, which cannot be detected any more. In
general, we consider H1 is accepted.
H2. The Insigniﬁcant Impact of Evasion Techniques. We have
generated two malware datasets, one of which contains malware
samples without any evasion features, and the other contains mal-
ware samples with arbitrary evasion features. From the comparison
of detection results, evasion features rarely effect the detection re-
sults of AVs. It can help to evade the detection of dynamic and static
analysis (43.7% of reduction in DR). Since the dynamic analysis tool
TAINTDROID tracks the ﬂow of information in the system, it fails to
detect the privacy leakage once the ﬂow is complicated by involving
ICC or implicit data ﬂow. The static analysis tools that perform a
code analysis from the source to sink, can overcome complicated
transformation attacks and behavior-level evasion techniques. For
example, ICCTA takes into the account ICCs during different com-
ponents of apps, can identify behaviors of privacy leakage occurring
across multiple components. However, static analysis in ICCTA
still has some ﬂaws. It cannot track the data ﬂow across persistent
storage, such as ﬁle, SQLite or shared preferences. Static analysis
tools usually employ API-matching to identify sources and sinks.
Therefore, they can be easily defeated by involving dynamic loading
techniques, such as reﬂection, constant encryption. Moreover, for
machine learning based tools, evasion features have a little impact
on DR, which is not signiﬁcant enough (the differences of ML and
ML(E) in Table 2 are within 5%). We observe that the higher #EFs
does not necessarily lead to a lower DR.
Thus, we consider that H2 is partially accepted — certain eva-
sion can only work for certain detection approaches and too many
evasions may not better bypass the detection.
H3. Diverse Detection Capabilities of AMTs. Based on the de-
tection results to our malware benchmark, we test H3 by evaluating
the weakness and strength of each type of approaches.
• Dynamic analysis is a kind of black box testing, which focuses on
the input and output of sensitive information to apps, while they
do not consider how the behavior is implemented. Therefore, the
detection capabilities depend on the coverage of sources, sinks
and the communication channels between. Our experiments show
that TAINTDROID can track sensitive information obtained from
speciﬁc Android APIs, such as getDeviceId and getLine1Number. It
does not track the information from incoming SMS message and
Content Provider, etc. It performs well for the communication
channel ICC and ﬁle-based channel. However, SQLite and shared
preferences can help bypass its detection.
• Static analysis is more scalable than dynamic analysis. However,
it lacks of information during runtime and thereby its capabilities
are limited. Nowadays, there are some works [36] using symbolic
execution to mitigate the lacking of runtime information.
• We compare the detection results of two malware sets, one of
which has more attack features and the other has less attack fea-
tures. The dataset with more attack features are more likely to be
detected, while machine learning based approaches are suscep-
tible to malware with less attack features. Another comparison
occur between two tools REVEALDROID and DREBIN. Although
DREBIN has considered more features, its detection ratio is im-
proved a lot. Therefore, the signiﬁcance, rather than the number,
of features can better facilitate the detection. As shown in Fig. 6,
we list ﬁve attack features which are easiest to be detected, and
ﬁve attack features which are hardest to be detected. Interested
readers please refer to [1] for a complete list of signiﬁcance of
attack features in the feature model.
• It is reasonable for AVs to use a fast approach with a low false
positive rate. Our observation is that AVs mainly aim at detecting
known malware. Hence, AVs work in a reactive way, not in a
proactive way.
To sum up, we consider that H3 should be rejected. Consider-
ing the detection results of TAINTDROID in Table 2, we cannot
conﬁrm that dynamic tools can produce high detection accuracy,
although they can provide more accurate information in detection.
The problem lies in the difﬁculty in triggering malicious behaviors
in execution. Note that due to the unavailability of other dynamic
tools, we cannot generalize our conclusion for all dynamic tools.
02004006008001000120014000102030405060TriggerSourceSinkPermission373Figure 7: Malicious behaviors to be repackaged
Table 3: The capabilities of vetting process in modern marketplaces
#Benign Base Google Play GetJar
SlideMe
TorrApk
1
2
3












H4. Strong Vetting Process in Modern App Stores. Modern An-
droid app stores employ multiple techniques to inspect the submitted
apps and protect their marketplaces. Google Play has turned from an
ofﬂine dynamic analysis-Bouncer [35] to a manual check by human
experts [32]. Currently, Android app stores GetJar3, SlideMe4 and
TorrApk5 all inspect the submitted apps by human experts.
Since our generated malware has no normal functionalities other
than malicious behaviors, it got rejected when we submit it into these
four app stores. To address this, we download three open-source
benignware, which have been veriﬁed by AMTs and approved by
Google Play. We inject our malicious behaviors into their source
code, repackage them and then submit them to the four Android
app stores. One example of malicious behaviors is shown in Fig. 7.
And it acquires 5 permission and steals SMS messages and identity
information of device into a particular server.
For each of the 3 benignware (benign base), we select 4 malware
samples from our benchmark and inject them into the benign base.
Here are the 4 malware samples: 1) one malicious app without eva-
sion features; 2) one malicious app is generated by adding evasion
features into the ﬁrst app; 3) an optimal malware sample in our
benchmark. 4) a random choosen one from our malware benchmark.
The 4 different malicious apps from the same benign base is submit-
ted to the four different app stores. Table 3 shows the detection ratio
of these apps by AMTs,  and  indicate an app is approved (not
detected) and rejected (detected) by the corresponding app stores,
respectively. From this experiment, we can conclude that the vetting
process of Android app stores still have severe ﬂaws, and can be eas-
ily bypassed. Although human inspection can judge the quality of
apps of high conﬁdence, the security of apps is not fully inspected.
According to our observations, we consider that H4 should be
rejected. Note that the malware samples that are used for injection
are with a ratio of 0% to 22.8% to be detected. Thus, the vetting
results are not signiﬁcantly better than the results of our AMTs. We
suspect that the vetting process also uses the AMTs for detection.
7.4 RQ3: Representative Malware and Use-
fulness of Mystique
In this section, we conduct a controlled experiment to assess the
effectiveness of MYSTIQUE to obtain optimal malware from the
attacker’s view. The basic idea is to use a small set of AFs and EFs
for fast convergence, and evaluate the resulting malware when the
evolution stops.
The experiment is conducted as follows: 1) pick up 10 samples of
malware which can be detected. 2) use IBEA algorithm to generate
3http://developer.getjar.mobi/
4http://slideme.org/
5https://www.torrapk.com/
new variants by combining or mutating features in the initial popu-
lation of malware. 3) stop if no more optimal malware is generated.
The 10 samples are from malware family DroidKungFu3, Anserver-
Bot, BaseBridge, DroidKungFu4, Geinimi, Pjapp, KMin, Gold-
Dream, DroidKungFu1 and DroidKungFu2 as shown in Table 1.
The extracted features are as follows. In addition, we consider all
14 types of evasion features in this experiment.
Triggers:
Source:
[T1] STARTUP,
[T2] android.intent.action.BOOT COMPLETED,
[T3] android.intent.action.BATTERY CHANGED,
[T4] android.intent.action.NEW OUTGOING CALL
[T5] android.provider.Telephony.SMS RECEIVED,
[SU1] PACKAGE::INSTALLED APK,
[SU2] SMS::ALL,
[SU3] SMS::INCOMING SMS,
[SU4] TELEPHONY::IMEI,
[SU5] TELEPHONY::IMSI,
[SU6] TELEPHONY::PHONE NUMBER,
[SU7] TELEPHONY::SIM SERIAL
Sinks:
[SI1] HTTP::APACHE GET,
[SI2] HTTP::APACHE POST,
[SI3] HTTP::SOCKET POST,
[SI4] SMS::SEND TEXT MESSAGE
Permissions:
[P1] android.permission.INTERNET
[P2] android.permission.PROCESS OUTGOING CALLS
[P3] android.permission.RECEIVE BOOT COMPLETED
[P4] android.permission.READ PHONE STATE
[P5] android.permission.RECEIVE SMS
[P6] android.permission.SEND SMS
[E1] Control based evasion
[E2] Data based evasion
[E3] Transformation attacks (12 types of transformation)
Evasion:
Initially, MYSTIQUE selects features randomly to construct 10
malware samples as the initial population. MYSTIQUE evolves based
on the ﬁtness value of newly-generated malware. After 30 iterations,
MYSTIQUE obtains the optimal malware of which the ﬁtness values
reach optimum in three objectives. The optimal malware contains 16
attack features and 3 evasion features. AFs in the optimal malware
are {T1, T3, T5, SU1, SU2, SU4, SU5, SU7, SI1, SI2, SI3, SI4, P1,
P2, P3, P6}, and EFs contains control based evasion, data based
evasion and one transformation. We put the optimal malware and
more details on our tool website [1] for public observation.
8. DISCUSSION
8.1 Threats to Validity
The internal threats for experiment results are from three aspects.
First, we mainly consider privacy leakage attack and their behaviors.
However, the logic of this attack is quite straightforward and we
have the limited number of AFs. Second, for EFs, we mainly take
into account the ﬂow complication and transformation attacks. Last,
we use default parameters and set-up of IBEA for malware evolu-
tion. Further investigation should be conducted to see the effects of