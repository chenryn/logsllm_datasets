to our servers. During this step, we requested they paste into
our interface the ad interest categories Twitter reported for
them in their settings page. If a participant reported 10 or
fewer interests (another indicator of infrequent usage), we did
not invite them to the survey.
To give participants time to receive their data from Twitter,
we waited several days before inviting them back. A total of
USENIX Association
29th USENIX Security Symposium    147
254 participants completed the survey. The median comple-
tion time for the 231 valid participants (see Section 4.1) was
31.5 minutes, and compensation was $7.00.
To protect participants’ privacy, we automatically ex-
tracted and uploaded only the three Twitter ﬁles related to
advertising: ad-impressions.js, personalization.js,
and twitter_advertiser_list.pdf. The JavaScript ﬁle
ad-impressions.js contained data associated with ads
seen on Twitter in the preceding 90 days, including
the advertiser’s name and Twitter handle, targeting types
and values, and a timestamp. An example of this JSON
data is presented in our online Appendix A [1]. The
ﬁle twitter_advertiser_list.pdf contained advertisers
who included the participant in a tailored audience list, as well
as lookalike audiences in which Twitter placed the participant.
3.2 Survey Section 1: Targeting Types
Our goal for the ﬁrst section of the survey was to compara-
tively evaluate user awareness, perceptions, and reactions to
the targeting types advertisers frequently use to target ads on
Twitter. We wanted to include as many targeting types as pos-
sible, while ensuring that a given participant would be likely
to have seen at least one ad targeted using that type. If we had
included all 30 types, we would have only been able to show a
few participants an ad relying on the more obscure types, and
would likely not have had a suﬃcient number of participants
to meaningfully carry out our statistical analyses. In our pilot
data, only 16 targeting types appeared in the data of more
than half of our pilot participants; therefore, we opted to use
these 16 in the survey. The 16 targeting types were as fol-
lows: follower lookalike; location; tailored audience (list);
keyword; age; conversation topic; interest; tailored audi-
ence (web); platform; language; behavior; gender; movies
and TV shows; event; retargeting campaign engager; and
mobile audience. We refer to a speciﬁc attribute of a type as
an instance of that type. For example, language targeting has
instances like English and French, and event targeting has
instances including “2019 Women’s World Cup” and “Back
to School 2019.” These targeting types are described in detail
in Section 4.3; Twitter’s deﬁnitions are given in our online
Appendix B [1]. Using a mixed between- and within-subjects
design, we showed each participant four randomly selected
targeting types, chosen from however many of the 16 types
were in that user’s ad impressions ﬁle. Prior work has covered
only a fraction of these 16 targeting types. Furthermore, ask-
ing questions about instances from participants’ own Twitter
data increased the ecological validity of our study compared
to the hypothetical scenarios used in prior work.
For each targeting type, we repeated a battery of questions.
First, we asked participants to deﬁne the targeting type in their
own words. Next, we gave a deﬁnition of the term adapted
from Twitter for Business help pages [63]. We then showed
participants one speciﬁc instance of the targeting type, drawn
Figure 1: Example ad shown in Section 2 of the survey. Partici-
pants always saw the ad before the corresponding explanation.
from their Twitter data (e.g., for keyword, “According to your
Twitter data, you have searched for or Tweeted about cats”).
Finally, we showed participants the ﬁve most and ﬁve least
frequent instances of that targeting type in their Twitter data (if
there were fewer than 10 instances, we showed all available),
as well as an estimate of how many ads they had seen in the
last three months that used that targeting type.
At this point, the participant had seen a deﬁnition of the
targeting type as well as several examples to aid their under-
standing. We then asked questions about participants’ com-
fort with, perception of the fairness of, perceptions of the
accuracy of, and desire to be targeted by the type. For these
questions, we asked participants to rate their agreement on a
5-point Likert scale from strongly agree to strongly disagree.
Hereafter, we say participants “agreed” with a statement as
shorthand indicating participants who chose either “agree” or
“strongly agree.” Similarly, we use “disagreed" as shorthand
for choosing “disagree" or “strongly disagree." We also asked
participants to explain their choices in free-text responses to
conﬁrm that participants were understanding our constructs as
intended. The text of all questions is shown in Appendix E [1].
3.3 Survey Section 2: Ad Explanations
Our goal for the second section of the survey was to charac-
terize user reactions to the ad explanations companies like
Twitter and Facebook currently provide on social media plat-
forms, as well as to explore whether ideas proposed in past
work (but not quantitatively tested on a large scale) could lead
to improved explanations. To that end, we used participants’
Twitter data to craft personalized ad explanations for ads that
were actually displayed to them on Twitter within the last 90
days. We tested six diﬀerent ad explanations.
Rather than trying to pinpoint the best design or content
through extensive A/B testing, we instead constructed our
explanations as initial design probes of prospective ad expla-
nations that are more detailed than those currently used by
major social media platforms. The explanations diﬀered in
several ways, allowing us to explore the design space. Our
148    29th USENIX Security Symposium
USENIX Association
within-subjects design invited comparison among the explana-
tions, which helped participants to evaluate the them, as well
as answer the ﬁnal question of this section: “Please describe
your ideal explanation for ads on Twitter."
To study reactions to widely deployed ad explanations, our
ﬁrst two explanations were modeled on those Twitter and
Facebook currently use. They retained the same information
and wording, but were recreated in our visual theme for con-
sistency and to avoid bias from participants knowing their
origin. The ﬁrst was based on Twitter’s current ad explana-
tion (Fig. 2a), which features most commonly, but not always,
two of the many possible ad targeting types: interest and
location (most frequently at the level of country). Notably,
ads themselves can be targeted to more granular locations and
using many more targeting types; Twitter’s current explana-
tion does not present these facets to users. We also adapted
one of Facebook’s current ad explanations (Fig. 2b), which
uses a timeline to explain tailored audience targeting and
incorporates age and location. These explanations represent
two major platforms’ current practices.
Because current ad explanations are vague and incom-
plete [7, 72], we wanted to explore user reactions to potential
ad explanations that are more comprehensive and also inte-
grate design suggestions from prior work [17, 20]. We thus
created two novel explanations, Detailed Visual (Fig. 2c) and
Detailed Text (Fig. 2d), that showed a more comprehensive
view of all the targeting types used, including lesser-known,
yet commonly used, targeting types like follower lookalike,
mobile audience, event and tailored audience. The distinction
between our two conditions let us explore the communication
medium. While we hypothesized that Detailed Visual would
perform better than Detailed Text, we wanted to probe the
trade-oﬀ between the comprehensiveness and comprehensi-
bility of text-based explanations.
While ad explanations should be informative and intelligi-
ble, they should also nudge users to think about their choices
regarding personalized advertising. We designed our third
novel ad explanation, “Creepy" (Fig. 2e), to more strongly
nudge participants toward privacy by including information
likely to elicit privacy concerns. This explanation augmented
our broader list of targeting types with information the partic-
ipant leaks to advertisers, such as their device, browser, and
IP address. This explanation also used stronger declarative
language, such as “you are" instead of “you may."
Finally, we designed a generic Control explanation
(Fig. 2f) that provided no targeting information. This expla-
nation was designed to be vague and meaningless. Following
other work [29, 76], Control provides a point of comparison.
Our ad explanations are the result of several iterations of
design. After each iteration, we discussed whether the de-
signs met our goal of creating a spectrum of possibilities
for speciﬁcity, readability, and comprehensiveness. We then
redesigned the explanations until we felt that they were satis-
factory based on both pilot testing and group discussion.
Participants were shown ad explanations in randomized or-
der. Each explanation was preceded by an ad from their data
and customized with that ad’s targeting criteria. We created
a list of all ads a participant had been shown in the last 90
days and sorted this list in descending order of the number of
targeting types used. To ﬁlter for highly targeted ads, we se-
lected six ads from the beginning of this list. Participants who
had fewer than six ads in their Twitter data saw explanations
for all of them. After each explanation, we asked questions
about whether the ad explanation was useful, increased their
trust in advertisers, and more.
The six ad explanations collectively represent a spectrum
of possible ad explanations in terms of speciﬁcity: Control
represents a lower bound, Creepy represents an upper bound,
and the others fall in between.
3.4 Analysis Method and Metrics
We performed quantitative and qualitative analyses of survey
data. We provide descriptive statistics about Twitter data ﬁles.
Because each participant saw only up to four of the 16 tar-
geting types in survey Section 1, we compared targeting types
using mixed-eﬀects logistic regression models. These are
appropriate for sparse, within-subjects, ordinal data [49, 62].
Each model had one Likert question as the outcome and the
targeting type and participant (random eﬀect) as input vari-
ables. We used interest targeting as our baseline because it
is the most widely studied targeting type. Interest targeting
is also commonly mentioned in companies’ advertising dis-
closures and explanations, in contrast to most other targeting
types we investigated (e.g., tailored audience). Appendix I [1]
contains our complete regression results.
To investigate how targeting accuracy impacted partici-
pant perceptions, we also compared the accuracy of targeting
type instances (self-reported by participants) to participants’
responses to the other questions for that targeting type. To
examine correlation between these pairs of Likert responses,
we used Spearman’s ρ, which is appropriate for ordinal data.
To compare a participant’s Likert responses to the six dif-
ferent ad explanations they saw, we used Friedman’s rank
sum test (appropriate for ordinal within-subjects data) as an
omnibus test. We then used Wilcoxon signed-rank tests to
compare the other ﬁve explanations to the Twitter explana-
tion, which we chose as our baseline because Twitter currently
uses it to explain ads. We used the Holm method to correct
p-values within each family of tests for multiple testing.
We qualitatively analyzed participants’ free-response an-
swers to ﬁve questions about targeting types and ad explana-
tions through an open coding procedure for thematic analysis.
One researcher made a codebook for each free-response ques-
tion and coded participant responses. A second coder inde-
pendently coded those responses using the codebook made by
the ﬁrst. The pair of coders for each question then met to dis-
cuss the codebook, verifying understandings of the codes and
USENIX Association
29th USENIX Security Symposium    149
(a) Twitter ad explanation.
(b) Facebook ad explanation.
(c) Detailed Visual ad explanation.
(d) Detailed Text ad explanation.
(e) Creepy ad explanation.
(f) Control ad explanation.
Figure 2: The six ad explanations tested, using a hypothetical
ad to demonstrate all facets of the explanations.
combining codes that were semantically similar. Inter-coder
reliability measured with Cohen’s κ ranged from 0.53 to 0.91
for these questions. Agreement > 0.4 is considered “mod-
erate” and > 0.8 “almost perfect” [33]. To provide context,
we report the fraction of participants that mentioned speciﬁc
themes in these responses. However, a participant failing to
mention something is not the same as disagreeing with it, so
this prevalence data should not be considered generalizable.
Accordingly, we do not apply hypothesis testing.
3.5 Ethics
This study was approved by our institutions’ IRB. As social
media data has potential for abuse, we implemented many
measures to protect our participants’ privacy. We did not col-
lect any personally identiﬁable information from participants
and only identiﬁed them using their Proliﬁc ID numbers. Ad-
ditionally, we only allowed participants to upload the three
ﬁles necessary for the study from participants’ Twitter data;
all other data remained on the participant’s computer. These
three ﬁles did not contain personally identiﬁable information.
In this paper, we have redacted potential identiﬁers found in
targeting data by replacing numbers with #, letters with *, and
dates with MM, DD, or YYYY as appropriate.
To avoid surprising participants who might be uncomfort-
able uploading social media data, we placed a notice in our
study’s recruitment text explaining that we would request
such data. As some of our participants were from the UK and
Proliﬁc is located in the UK, we complied with GDPR.
3.6 Limitations
Like all user studies, ours should be interpreted in the context
of its limitations. We used a convenience sample via Proliﬁc
that is not necessarily representative of the population, which
lessens the generalizability of our results. However, prior work
suggests that crowdsourcing for security and privacy survey
results can be more representative of the US population than
census-representative panels [51], and Proliﬁc participants
produce higher quality data than comparable platforms [46].
We may have experienced self-selection bias in that potential
participants who are more privacy sensitive may have been un-
willing to upload their Twitter data to our server. Nonetheless,
we believe our participants provided a useful window into
user reactions. While we did ﬁnd that the average character
count of free response questions decreased over the course
of the survey (ρ = −0.399; p < 0.01 between question order
and average character number), we were satisﬁed with the
qualitative quality of our responses. Responses included in
our analysis and results were on-topic and complete.
We were also limited by uncertainty in our interpretation of
the Twitter data ﬁles at the time we ran the user study. Twitter
gives users their data ﬁles without documentation deﬁning
the elements in these ﬁles. For instance, each ad in the data
ﬁle contains a JSON ﬁeld labeled “matchedTargetingCriteria”
that contains a list of targeting types and instances. It was
150    29th USENIX Security Symposium
USENIX Association
initially ambiguous to us whether all instances listed had been
matched to the participant, or whether this instead was a full
list of targeting criteria speciﬁed by the advertiser regardless
of whether each matched to the participant. The name of this
ﬁeld suggested the former interpretation. However, the pres-
ence of multiple instances that could be perceived as mutually
exclusive (e.g., non-overlapping income brackets) and Twit-
ter telling advertisers that some targeting types are “ORed”
with each other (see online Appendix F, Figure 6 [1]) made
us question our assumption. Members of the research team
downloaded their own data and noticed that most “matched-
TargetingCriteria” were consistent with their own character-
istics. We made multiple requests for explanations of this
data from Twitter, including via a GDPR request from an
author who is an EU citizen (see online Appendix C [1]).
We did not receive a meaningful response from Twitter for
more than 4.5 months, by which point we had already run the
user study with softer language in survey questions and ad
explanations than we might otherwise have used. Ultimately,
Twitter’s ﬁnal response reported that the instances shown un-
der “matchedTargetingCriteria” indeed were all matched to
the user, conﬁrming our initial interpretation.
Because we wanted to elicit reactions to ad explanations for
ads participants had actually been shown, our comparisons of
ad explanations are limited by peculiarities in participants’ ad
impressions data. If an ad did not have a particular targeting
type associated with it, then that targeting type was omit-
ted from the explanation. The exception was Visual, which
told participants whether or not each targeting type was used.
Further, 38 participants’ ﬁles contained data for fewer than
six ads. In these cases, we showed participants explanations
for all ads in their ﬁle. The targeting types and speciﬁc ex-
ample instances randomly chosen for each participant had
inherent variance. Some targeting types had more potential
instances than others. Some instances undoubtedly seemed
creepier or more invasive than others, even within the same