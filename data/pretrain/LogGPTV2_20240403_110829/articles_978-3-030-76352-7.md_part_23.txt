is intercepted by Istio and returns 500 error status. Users can also specify the
percentage of requests that should be failed. We injected faults in 17 services
that cover the main flow of the train ticket application. We generated data by
running a scenario where 100% incoming requests are failed. To measure the
observability of faults, the user flow was executed 20 times after injecting the
fault. For each service, the average running time for 20 iterations was 19 min
and the average number of log lines were 164,740. We found that the average
numberoferrormessageswere270whiletheminimumandmaximumnumberof
errormessageswere 40and1,436respectively.Weobservedthatthemaximum
number of error are generated by the ts-station-service fault. This is because it
isoneofthefarthestnodefromthegatewaynodets-ui-dashboard andalsosince
all nodes in the path emit errors.
5 Experiments
Inthissection,wepresentanempiricalstudyconductedtoshowtheeffectiveness
of our proposed approach for fault localization.
Theexperimentsaredesignedtoshowthatcausalgraphinconjunctionwith
golden signals delivers superior results over just using dependency graph (static
topology) among the micro-services emitting error templates. Localizing opera-
tion faults has two aspects, 1) localizing the faulty micro-service and 2) further,
withinthefaultymicro-service,whichtemplate(errormessage)isresponsible.In
order to evaluate both the aspects, we first evaluate fault localization at micro-
service level then further localizing at finer granularity of template level within
the faulty micro-service.
Localization of Operational Faults in Cloud Applications 145
Forevaluatingthefirstaspect,wedividetheexperimentsintotwogroups,G1
andG2.IngroupG1,topologyisstatic,whileingroupG2,topologyisdynamic
andiscomputedusingcausalitybasedtechniques.Togeneratethecausalgraph,
weuse3approaches:PC,Blasso,Blinear.Weshowthatthedynamictopology,
called causal graph, computed using causality inference techniques are outper-
forming static topology in all the variations. Finally, we perform last mile fault
localization to localize the faulty error template.
Group 1: Group G1 covers 3 approaches, G1A, G1B and G1C. In G1A, we
use error signals emitted by each micro-service and using them as the starting
node, we traverse the dependency graph to the leaf nodes. The leaf nodes are
picked as the faulty nodes. In G1B, instead of traversing till the leaf nodes, we
run PageRank over the subgraph consisting of all micro-services emitting error
signals to get a ranked list of faulty nodes. In G1C, we consider only those
micro-services nodes which cause the golden signal errors and run Personalized
PageRank algorithm on static topology to get ranked list of faulty nodes.
Group 2:InGroupG2,wehavetwovariationsG2A andG2B forfaultlocaliza-
tion using dynamic causal graphs. In G2A the nodes which cause golden signal
errorareconsideraspotentialfaultynodes.InG2B,micro-servicenodescausalto
golden signal node are identified and are assigned weights based on the causal-
ity scores. Then, Personalized PageRank is used on causal graph factoring in
weights on the causal nodes to do first level of fault localization.
5.1 Results
Weuseatimebinofsize10msastheinter-arrivaltimebetweenerrorlogsinthis
dataset. We have considered threshold for the number of golden signals errors
as 15. To calculate precision and recall we use a graph-based approach. If the
localizednodedoesnotexactlymatchthegroundtruthnodewecalculatematch
based on the distance (in number of hops) of the returned node n according to
the following equation:
S =1−hn/(H +1) (3)
HereSisthefinalmatchscoreforthereturnednoden,hnisthedistance(inhops)
of this node from the ground truth node and H is a pre-configured threshold for
the maximum number of hops allowed. For our experiments we use H = 3. In
all the PageRank based methods, we measure precision and recall for Top3.
Figure4 shows the results of experiments conducted to evaluate accuracy
of our approach at micro-service level. We observe, low precision and recall for
G1A, indicating that the errors emitted by the leaf nodes are not the potential
sourceof fault. InG1B,we see7% point increasein F1scoreas PageRank helps
in identifying most impact-ful node. However, the performance is low as all the
nodes emitting error signals are considered. In G2A, where golden signal errors
are factored in, we observe an increase of 42% points in F1 score (with Blinear
causaltechnique),signifyingtheusefulnessofgoldensignalstolocalizethefault.
The error signals often start from the faulty micro-services and then propagate
146 P. Aggarwal et al.
to other non-faulty micro-services via inter-micro-service interactions. Due to
this, sometimes the non faulty micro-services might show causal relationship
which golden signal errors, resulting in low precision for G2A. Out of the 17
faults, six faults are injected in the services which directly interacts with ts-ui-
dashboard service. When fault is ingested in any of these 6 services, only the
error signals emitted by ts-ui-dashboard service has evidence of failure. In G2A
approach,weconsideronlythosemicro-services,forfaultlocalization,whichhave
causal relationship with ts-ui-dashboard, therefore the failure of these 6 services
is not captured by G2A approach, hence the recall is low. To further improve
the performance, we use Personalized PageRank. The results clearly indicates
that our approach G2C, using Blinear causal technique, outperforms all other
approaches with F1 score of 0.88 and with casual techniques Blasso and PC,
wehaveF1-scoresof0.83and0.87respectively.PC basedtechniquehashighest
recall of 0.96. We observe that with the dependency graph, G2B we get low F1-
scoreviz.0.49(Blasso),0.51(PC)and0.58(Blinear).Thepossibleexplanation
couldbethatthedependencygraphisnotatrueindicationofruntimebehavior.
A micro-service might interact with multiple services but in a user scenario
all the flows need not execute. Personalized PageRank algorithm on the static
dependency graph does not assign high centrality scores to the faulty nodes as
non-faulty nodes have many incoming and out going edges resulting in high
centrality score. Whereas, the causal graph indicates the runtime interactions
among various micro-services. Therefore, the causal relationship among various
micro-services captures the error propagation across various micro-services in
the application well.
Method Precision Recall F1Score Method Precision F1Score
G G1 1A B 0 0. .2 35 2 0 0. .3 31 9 0 0. .2 38 5 G G2 2A−PC 0 0. .5 67 2 0 0. .5 78 1
G2A−PC 0.57 0.59 0.58 A−Blasso
G2 A−Blasso 0.58 0.83 0.68 G2 A−Blinear 0.66 0.76
G2 A−Blinear 0.62 0.88 0.73 G1C−PC 0.55 0.54
G1C−PC 0.49 0.55 0.51 G1 0.48 0.50
C−Blasso
G1 0.46 0.52 0.49
C−Blasso G1 0.56 0.59
G1 0.54 0.63 0.58 C−Blinear
G2C B− −B PClinear G2B−PC 0.75 0.84
0.73 0.96 0.83
G2 B−Blasso 0.84 0.90 0.87 G2 B−Blasso 0.85 0.87
G2 0.84 0.93 0.88 G2 0.85 0.89
B−Blinear B−Blinear
Fig.4.Performanceresultswithdiffer- Fig.5. Performance results with Last
ent fault localization methods Mile Fault Localization
WeapplyourLastMilefaultlocalization(LMFL)techniquetoanalyzethe
errortemplatesemittedbythetop3potentialfaultymicro-services.Weobserve
thatusuallythereare3–4uniqueerrortemplatesemittedbyeachmicro-service.
Insteadofanalyzingalltheerrortemplatesofthetop3micro-services,wenarrow
down to the error templates which have high causal relationship with golden
signal errors and high centrality score. This reduces the number of potential
Localization of Operational Faults in Cloud Applications 147
root cause error templates by 70%. On these error templates, we do further
analysis to find out whether the service emitting error template is at fault or
oneofitschildrennodes.Figure5showstheimprovementinprecisionandF1for
G1C, G2A and G2B approaches. The recall remains same as LMFL is applied
on the output of previous step.
6 Conclusion
Inthispaper,wepresentagoldensignalbasedfaultlocalizationapproach,which
isbasedoninferringthecausalrelationshipamongservicesemittingerrorsignals
and the one emitting golden signal error. PageRank based graph centrality app-
roach is used to efficiently localize the faults. The proposed approach improves
state-of-the art techniques by (i) using golden signal error rate to localize the
operational faults (ii) time series modeling of the error rate from log data (iii)
using only the positive samples to do last mile fault localization and (iv) using
regression and conditional independence based causal techniques. Our experi-
mental results show the effectiveness of this approach. This technique can easily
be extended to use other golden signals such as latency, saturation, and traffic.
In future, we plan to perform more experiments with real world dataset and
explore other types of golden signals.
References
1. Istio service mesh. https://istio.io/. Accessed 16 Aug 2020
2. Trainticket:abenchmarkmicroservicesystem.https://github.com/FudanSELab/
train-ticket/. Accessed 16 Aug 2020
3. Aggarwal, P., Atreja, S., Dasgupta, G., Mandal, A.: System anomaly detection
using parameter flows, December 2019
4. Beyer, B., Jones, C., Petoff, J., Murphy, N.R.: Site Reliability Engineering: How
Google Runs Production Systems. O’Reilly Media Inc., Newton (2016)
5. Chow, M., Meisner, D., Flinn, J., Peek, D., Wenisch, T.F.: The mystery
machine: end-to-end performance analysis of large-scale internet services. In:
11th {USENIX} Symposium on Operating Systems Design and Implementation
({OSDI} 2014), pp. 217–231 (2014)
6. Geweke, J.F.: Measures of conditional linear dependence and feedback between
time series. J. Am. Stat. Assoc. 79(388), 907–915 (1984)
7. Granger, C.W.J.: Investigating causal relations by econometric models and cross-
spectral methods. Econometrica 37(3), 424–438 (1969)
8. Gupta, M., Mandal, A., Dasgupta, G., Serebrenik, A.: Runtime monitoring in
continuous deployment by differencing execution behavior model. In: Pahl, C.,
Vukovic, M., Yin, J., Yu, Q. (eds.) ICSOC 2018. LNCS, vol. 11236, pp. 812–827.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-03596-9 58
9. Jia, T., Chen, P., Yang, L., Li, Y., Meng, F., Xu, J.: An approach for anomaly
diagnosisbasedonhybridgraphmodelwithlogsfordistributedservices.In:2017
IEEE International Conference on Web Services (ICWS), pp. 25–32. IEEE (2017)
148 P. Aggarwal et al.
10. Kalisch, M., Bu¨hlmann, P.: Estimating high-dimensional directed acyclic graphs
with the PC-algorithm. J. Mach. Learn. Res. 8, 613–636 (2007)
11. Kim, M., Sumbaly, R., Shah, S.: Root cause detection in a service-oriented archi-
tecture. ACM SIGMETRICS Perform. Eval. Rev. 41(1), 93–104 (2013)
12. Kobayashi, S., Otomo, K., Fukuda, K.: Causal analysis of network logs with lay-
eredprotocolsandtopologyknowledge.In:201915thInternationalConferenceon
Network and Service Management (CNSM), pp. 1–9 (2019)
13. Kobayashi, S., Otomo, K., Fukuda, K., Esaki, H.: Mining causality of network
events in log data. IEEE Trans. Netw. Serv. Manag. 15(1), 53–67 (2018)
14. Lin, J., Chen, P., Zheng, Z.: Microscope: pinpoint performance issues with causal
graphs in micro-service environments. In: Pahl, C., Vukovic, M., Yin, J., Yu, Q.
(eds.) ICSOC 2018. LNCS, vol. 11236, pp. 3–20. Springer, Cham (2018). https://
doi.org/10.1007/978-3-030-03596-9 1
15. Mariani, L., Monni, C., Pezz´e, M., Riganelli, O., Xin, R.: Localizing faults in
cloud systems. In: 2018 IEEE 11th International Conference on Software Testing,
Verification and Validation (ICST), pp. 262–273 (2018)
16. Mi, H., Wang, H., Zhou, Y., Lyu, M.R.T., Cai, H.: Toward fine-grained, unsu-
pervised,scalableperformancediagnosisforproductioncloudcomputingsystems.
IEEE Trans. Parallel Distrib. Syst. 24(6), 1245–1255 (2013)
17. Nandi, A., Mandal, A., Atreja, S., Dasgupta, G.B., Bhattacharya, S.: Anomaly
detection using program control flow graph mining from execution logs. In: Pro-
ceedings of the 22nd ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pp. 215–224 (2016)
18. Nguyen, H., Shen, Z., Tan, Y., Gu, X.: Fchain: toward black-box online fault
localization for cloud systems. In: 2013 IEEE 33rd International Conference on
Distributed Computing Systems, pp. 21–30 (2013)
19. Park, T., Casella, G.: The Bayesian lasso. J. Am. Stat. Assoc. 103(482), 681–686
(2008)
20. Spirtes, P., Glymour, C.: An algorithm for fast recovery of sparse causal graphs.
Soc. Sci. Comput. Rev. 9(1), 62–72 (1991)
21. Tan,J.,Pan,X.,Marinelli,E.,Kavulya,S.,Gandhi,R.,Narasimhan,P.:Kahuna:
problem diagnosis for mapreduce-based cloud computing environments. In: 2010
IEEE Network Operations and Management Symposium-NOMS 2010, pp. 112–
119. IEEE (2010)
22. Vald´es-Sosa,P.,etal.:Estimatingbrainfunctionalconnectivitywithsparsemulti-
variate autoregression. Philos. Trans. R. Soc. Lond. Ser. B Biol. Sci. 360, 969–81
(2005)
23. Voas, J.M.: Pie: a dynamic failure-based technique. IEEE Trans. Software Eng.
18(8), 717 (1992)
24. Wang, P., et al.: Cloudranger: root cause identification for cloud native systems.
In: 2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid
Computing (CCGRID), pp. 492–502. IEEE (2018)
25. Weber,I.,Li,C.,Bass,L.,Xu,X.,Zhu,L.:Discoveringandvisualizingoperations
processeswithpod-discoveryandpod-viz.In:201545thAnnualIEEE/IFIPInter-
national Conference on Dependable Systems and Networks, pp. 537–544. IEEE
(2015)
26. Wu, L., Tordsson, J., Elmroth, E., Kao, O.: Microrca: root cause localization of
performance issues in microservices. In: NOMS 2020–2020 IEEE/IFIP Network
Operations and Management Symposium, pp. 1–9. IEEE (2020)
Localization of Operational Faults in Cloud Applications 149
27. Xu, J., Chen, P., Yang, L., Meng, F., Wang, P.: Logdc: problem diagnosis for
declartively-deployedcloudapplicationswithlog.In:2017IEEE14thInternational
Conference on e-Business Engineering (ICEBE), pp. 282–287. IEEE (2017)
28. Zeng, C., Wang, Q., Wang, W., Li, T., Shwartz, L.: Online inference for time-
varying temporal dependency discovery from time series. In: 2016 IEEE Interna-
tional Conference on Big Data (Big Data), pp. 1281–1290. IEEE (2016)
Using Language Models to Pre-train
Features for Optimizing Information
Technology Operations Management
Tasks
B B B B
Xiaotong Liu( ), Yingbei Tong( ), Anbang Xu( ), and Rama Akkiraju( )
IBM Research Almaden, San Jose, USA
{xiaotong.liu,yingbei.tong}@ibm.com, {anbangxu,akkiraju}@us.ibm.com
Abstract. Information Technology (IT) Operations management is a
vexingproblemformostcompaniesthatrelyonITsystemsformission-
critical business applications. While IT operators are increasingly lever-
aginganalyticaltoolspoweredbyartificialintelligence(AI),thevolume,
the variety and the complexity of data generated in the IT Operations
domainposessignificantchallengesinmanagingtheapplications.Inthis
work, we present an approach to leveraging language models to pre-
train features for optimizing IT Operations management tasks such as
anomalypredictionfromlogs.Specifically,usinglog-basedanomalypre-
dictionasthetask,weshowthatthemachinelearningmodelsbuiltusing
languagemodels(embeddings)trainedwithITOperationsdomaindata
asfeaturesoutperformthoseAImodelsbuiltusinglanguagemodelswith
general-purposedataasfeatures.Furthermore,wepresentourempirical
results outlining the influence of factors such as the type of language
models, the type of input data, and the diversity of input data, on the
predictionaccuracyofourloganomalypredictionmodelwhenlanguage
modelstrainedfromITOperationsdomaindataareusedasfeatures.We
also present the run-time inference performance of log anomaly predic-
tionmodelsbuiltusinglanguagemodelsasfeaturesinanITOperations
production environment.
· ·
Keywords: AI for IT operations Language modeling Anomaly
detection
1 Introduction
Information Technology (IT) Operations management is a vexing problem for
most companies that rely on IT systems for mission critical business applica-
tions.Despitebestintentions,designs,anddevelopmentpracticesfollowed,soft-
ware and hardware systems are susceptible to outages, resulting in millions of
dollars in labor, revenue loss, and customer satisfaction issues. IT downtime
costs an estimated $26.5 billion in lost revenue each year based on a survey of
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.150–161,2021.
https://doi.org/10.1007/978-3-030-76352-7_18