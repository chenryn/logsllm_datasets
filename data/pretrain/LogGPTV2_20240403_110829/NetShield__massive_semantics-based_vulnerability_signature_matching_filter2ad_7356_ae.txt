TH
Location
Protocol
Start Time
Duration
App layer size 1.33GB
Flow number
Avg ﬂow len
17M
77B
TH
DNS WINRPC WINRPC HTTP
NU
DARPA
HTTP HTTP
11/2007
1998
159 hours 207 days 765 days 13 hours 32 hours 26 days
15GB 3.96GB 3.89GB
2.24M 2.35M 71.9K 1.83M
596B
6.56KB 55KB 2.13KB
598MB 1.33GB
681K
879B
11/2007
10/2006 05/2008 10/2006
Table 3: The characteristics of the traces.
7.2 Core Engine Performance Analysis
Methodology: We evaluate the protocol parsing and signature
matching throughput of the proposed core engine rather than
a product-level NIDS/NIPS ,which takes much more engineer-
ing effort. Therefore,
the throughputs reported are not what
a NIDS/NIPS achieves when monitoring a network link on-
line. However,
if the core engine is fast, a well-engineered
NIDS/NIPS can achieve high throughput, without vulnerability sig-
nature matching being the bottleneck. Given existing commercial
regex-based NIDSes/NIPSes have already achieved high through-
put, we believe it is also achievable for vulnerability signature
based NIDSes/NIPSes.
In all the experiments, we pre-load the TCP streams after TCP
reassembly as input. Moreover, we process the connections one
after another to exclude the ﬂow switching overhead. The re-
ported throughput is application layer throughput, not including the
TCP/IP or link layer headers.
7.2.1 Parsing Performance
We evaluate the parsing performance on both single core and
multi-core implementation. Original BinPAC is the BinPAC dis-
tributed with Bro 1.3. We run it in standalone mode rather than
combining with Bro. In Opt. (optimized) BinPAC, Instead of creat-
ing string objects and array objects fully, we only keep the current
chunk of a string or the current element in an array in the memory,
which reduce the memory copy/alloc/dealloc operations. In Ultra-
PAC, we disable the “combine the unnecessary ﬁelds" optimization
(in §5.5) for fair comparison and parse every basic ﬁeld according
to the protocol speciﬁcation. The speedup ratio is calculated be-
tween UltraPAC and Opt BinPAC on the single core P4.
We evaluate three protocols: HTTP, WINRPC and DNS. HTTP
trafﬁc is one of the dominating trafﬁc on the Internet. WINRPC is a
multi-PDU protocol that has been heavily exploited. DNS has low
throughput in the BinPAC paper [20]. The results are consistent
across different network traces.
287Trace
TH
NU DARPA
DNS WINRPC WINRPC HTTP HTTP HTTP
TH
TH
NU
Throughput(Gb/s)
Original BinPAC(P4) 0.10
0.31
Opt BinPAC(P4)
UltraPAC(P4)
3.43
Speed Up Ratio(P4) 11.2
Throughput(Gb/s)
UltraPAC(XE 1core) 3.63
UltraPAC(XE 8core) 23.75
Max. Memory
Per Conn. (Bytes)
16
1.37
1.41
16.19
11.5
19.88
123.18
15
1.04
1.11
12.90
11.6
12.78
91.09
15
2.02
2.10
7.46
3.6
13.00
14.21
44.41
3.1
1.52
1.69
6.67
3.9
42.22
7.86
48.67 295.09
14
14
6.64
42.31
14
Table 4: Parsing results.
Throughput: Table 4 shows that on the single core P4 our parsers
parse WINRPC at 13+ Gbps, DNS at 3.4Gbps and HTTP at 6.7+
Gbps. Using eight cores the throughput further increases by 6 ∼
7 times. Comparing with the Opt BinPAC, we speed up binary
protocols (DNS and WINRPC) about 12 times, and a text protocol,
HTTP, about 3 ∼ 4 times.
The original BinPAC’s throughput here is higher than that in
the BinPAC paper [20], because the BinPAC paper measures the
throughput with Bro together, which includes TCP reassembly and
other lower layer overhead. Our measurement excludes such over-
head.
Protocol
Trace size (MB)
Parser
Func call # (K)
HTTP
200
DNS
140
Mem copy/alloc/dealloc time
BinPAC UltraPAC BinPAC UltraPAC
12,949
1,685
≈0%
23%
91,394
76%
Table 5: Execution proﬁling results.
4,850
6%
To further understand this performance boost, we proﬁle the ex-
ecution of both BinPAC and UltraPAC parser using the same sam-
ple traces. Table 5 shows: 1) UltraPAC parser heavily reduces the
number of function calls; and 2) it spends much smaller portion
of the execution time on memory copy, allocation and dealloca-
tion. Among the three major overheads of BinPAC parser (§5.2),
buffer management overhead is already minimized in our experi-
ment setting. Table 5 mainly conﬁrms the elimination of BinPAC’s
large overhead on parse tree traversal. The elimination of inner
node extraction in UltraPAC parser contributes to the remaining of
throughput increase.
Due to the smaller protocol ﬁeld size, the BinPAC DNS parser
suffers from relatively larger overhead on memory operations. That
is why UltraPAC gets higher speedup ratio on DNS protocol. The
memory operation time of UltraPAC HTTP parser is incurred in the
computation of ﬁeld length.
With BinPAC design, it is not trivial to reduce this part of over-
head to a similar level as UltraPAC. We further optimize the Bin-
PAC DNS parser to remove the creation and deletion of parse tree
nodes, by reusing preallocated nodes via a linked list. This imple-
mentation optimization reduces the execution time by almost 50%,
but the performance is still not even close to the UltraPAC parser.
Memory Consumption: The UltraPAC parsers have to maintain
the parsing variables required in the parsing state machine. In Ta-
ble 4, we also report the maximum memory size required per con-
nection. It is at most 16 bytes for all the three protocols.
7.2.2 Parsing + Matching Performance
Next, we evaluate the matching performance.
Candidate Set Size: We validate the observation in §4.1 (the can-
didate sets usually are small). For all protocols and traces, the max-
imum size of the candidate sets is no more than 8. The average size
is less than 1.5.
Throughput: We evaluate our candidate selection based matching
on both single core and multi-core implementation. Table 6 shows
Trace
Throughput (Gb/s)
Sequential(P4)
CS(P4)
Matching Only Time
Sequential(P4) (secs)
CS(P4) (secs)
Speed Up Ratio(P4)
Throughput (Gb/s)
CS(XE 1core)
CS(XE 8core)
Avg Memory Usage
Per Connection (Bytes)
Avg # of Candidates
TH
NU DARPA
WINRPC WINRPC HTTP HTTP HTTP
TH
NU
10.68
14.37
0.0048
0.0012
4
18.25
118.61
32
9.23
10.61
0.33
0.18
1.8
12.03
84.69
32
0.34
2.62
2.37
17.63
0.28
1.85
344.28 12.68
1.08
30.46
11.3
11.7
106.74
12.16
8.8
3.02
19.90
18.48 128.57
28
28
2.01
11.00
28
1.16
1.48
0.033
0.038
0.0023
Table 6: Parsing+Matching results.
that even on the single core P4 the CS algorithm can achieve about
11∼14Gbps for WINRPC (45 signatures) and about 1.9+Gb/s for
HTTP (794 signatures). The throughput on NU HTTP trace is
much higher, because it has much longer average ﬂow length, and
most of the bytes are contributed by the HTTP BODY ﬁeld in the
HTTP response message. The HTTP BODY ﬁeld is not required by
most signatures and thus involves little matching overhead. Fully
using the eight cores can speed up matching 5.5 ∼ 7.1 times than
only using a single core on the XE machine, and achieve 11+Gb/s
for the 794 signatures. The throughput of WINPRC is higher be-
cause of the small number of vulnerabilities.
 400
 600
 700
 800
 100
 200
)
s
p
b
G
(
t
u
p
h
g
u
o
r
h
T
subtracting
We implement
 300
 500
# of rules used
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
Figure 9: Scalability to the # of
rules.
the
sequential
matching
with short-circuit eval-
uation, i.e., a signature
is
skipped upon the
that
ﬁrst
condition
does not meet.
In
Table 6, the matching
only time is obtained
by
the