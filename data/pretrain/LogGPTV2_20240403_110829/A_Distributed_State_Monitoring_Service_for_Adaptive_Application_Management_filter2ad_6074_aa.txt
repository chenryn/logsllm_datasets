title:A Distributed State Monitoring Service for Adaptive Application Management
author:Paul Murray
A Distributed State Monitoring Service for Adaptive Application 
Management 
Paul Murray 
Hewlett-Packard Laboratories 
PI:EMAIL
Abstract 
Anubis  is  a  simple  state  monitoring  service  that 
supports  coordinated  action  among  distributed 
management  agents.  It  uses  a  temporal  consistency 
model 
that  addresses  symmetric  and  asymmetric 
network  partitions.  We  have  used  Anubis  to  support 
distributed  management  of  adaptive  applications  in 
Grid  and  Utility  computing  environments  and  our 
experience  has  shown 
the  abstraction  and 
properties  provided  by  the  service  simplify  the  task  of 
programming  distributed  management  behavior.  We 
support  this  claim  by  examining  three  common  use 
cases 
that  our  developers  encountered,  namely:  
resource  management,  lifecycle  coordination,  and 
compositional failure management. 
that 
1. Introduction 
the 
infrastructure 
The purpose of an application  management system 
is  to  configure  and  maintain  applications  that  run  in 
some  computing  infrastructure.  In  Grid  and  Utility 
computing  [7][9][15] 
is  shared 
among users, and their applications are given access to 
it  as  needed.  In  this  context  we  are  interested  in 
distributed  applications  that  are  programmed  to  be 
reconfigured on the fly, so that they can be adapted to 
the  available  pool  of  resources,  recover  from  failures, 
or  scale  to  accommodate  changing  workloads,  with 
management  behavior  that  automatically  determines 
when and how to do this reconfiguration. We call these 
adaptive applications.
The management behavior associated with adaptive 
applications  can  be  quite  complex  and  application 
specific.  A centralized control point can simplify these 
issues,  but  leaves  the  system  exposed  to  failure  of  the 
control  point  and  network  partitioning.  Another 
alternative  is  to  fully  distribute  the  control  function. 
This  approach  is  exposed  to  the  complexity  of 
distributed  coordination  problems.  It  is  this  second 
approach that we address in this paper. 
We  believe  that  distributed,  adaptive  application 
management  can  be  simplified  by  the  use  of  a  fully 
distributed  system  service  that  provides  discovery, 
monitoring,  and  failure  detection  under  a  single  state 
based  abstraction.  Here  we  describe  such  a  service 
called  Anubis.  Anubis  has  a  temporal  consistency 
model  that  supports  distributed  coordination  with 
properties  that  span  network  partitions,  allowing  even 
disconnected managers to coordinate their actions. We 
have  developed  a  fully  distributed  implementation  of 
our  service  that  has  been  used  in  prototype  and 
production  systems  as  a  basis  for  adaptive  application 
management.  This  experience  has  highlighted  several 
design  patterns  that  demonstrate  the  usefulness  of  the 
service. 
The  next  section  gives  an  overview  of  our 
application  management  environment.  Section  3 
describes  the  Anubis  state  monitoring  service  and  its 
timed  consistency  model,  an  implementation  of  which 
is  described  in  Section  4.  Section  5  examines  the 
design  patterns  identified  by  users  of  the  service.  In 
section  6  we  briefly  comment  on  our  experience 
regarding timeliness and we conclude in Section 7. 
2. Overview of Application Management 
We base our discussion here around an open source 
application  management  framework  called  SmartFrog 
(for Smart Framework for Object Groups) developed at 
HP Laboratories [14]. SmartFrog is a component-based 
framework  that  views  distributed  software  systems  as 
collections  of  cooperating  components.  It  includes  a 
component  model, 
for  describing 
collections of components and their configurations, and 
a  runtime  environment  that  activates  and  manages  the 
described components. 
language 
a 
A  SmartFrog  component  implements  a  set  of 
interfaces  that  provide  management  actions,  such  as 
creation,  activation,  and  termination,  and  inspection 
capabilities  such  as  obtaining  the  value  of  attributes 
associated  with  the  component.  A  component  that  has 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:45 UTC from IEEE Xplore.  Restrictions apply. 
been  created  in  the  runtime  environment  represents 
itself and continues to be an accessible managed entity 
until it is terminated. 
and 
a 
common 
SmartFrog 
components 
configuration 
can  manage 
other 
components 
is 
hierarchical  composition,  in  which  one  component 
takes  responsibility  for  a  group  of  components  (its 
children) and maps its management actions onto theirs. 
As  an  example,  a  group  of  web  server  components 
could  be  represented  collectively  by  a  single  server 
pool component. The action  of instantiating the server 
pool  then  leads  to  instantiation  of  the  web  servers.  Its 
termination leads to their termination. This pattern can 
ultimately  be  used  to  manage  an  entire  distributed 
application as a single component. 
In  addition  to  directed  action,  a  component  could 
also  implement  autonomous  action:  action  that  it 
chooses to take internally. For example, the server pool 
above  could  be  implemented  to  maintain  the  minimal 
pool required to service ongoing demand. 
SmartFrog is implemented in Java as a collection of 
components  that  provide  a  standard  behavior.  New 
components  with new  management functionality, such 
as the server pool, are implemented by extending these 
standard components. 
3. Anubis State Monitoring Service 
A 
that 
component 
implements 
adaptive 
management generally bases its behavior on a view of 
its  environment,  often  provided  collectively  by 
discovery,  status  monitoring,  and  failure  detection 
services.  We  developed  the  Anubis  service  to  provide 
these  under  a  simple  state-based  interface  suited  to 
adaptive  management  in  the  SmartFrog  framework. 
Anubis  is  a  fully  distributed,  self-organizing,  state 
dissemination  service  that  provides  the  ability  to 
discover  objects,  monitor  their  states,  and  detect  their 
failure or absence. 
3.1. Providers, Listeners, and States 
We introduce the following terminology: a  state is 
an  arbitrary  value;  a  provider  is  a  named  object  that 
has  a  state  that  may  change  over  time;  a  listener  is  a 
named object that observes the states of providers. It is 
the  job  of  the  Anubis  service  to  distribute  provider 
states  to  matching  listeners  and  we  say  that  a  listener 
observes the  state of a provider  when it  has a copy of 
its  state  value.  We  do  not  require  that  providers  have 
unique names, so a single listener contains a collection 
of states, each representing a matching provider. 
Providers  and  listeners  are  registered  with  the 
Anubis service interface. Discovery is represented by a 
listener  obtaining  the  state  of  a  provider.  Ongoing 
observation  of  the  provider’s  states  represents  status 
monitoring.  Lack  of  a  state  represents  failure  or 
absence  of  the  provider.  Anubis  ensures  that  listeners 
observe current provider states in a timely manner. 
Our notion of state observation is distinct from the 
more general publish-subscribe event notification (e.g. 
TIB/Rendezvous  [13])  in  that  it  explicitly  addresses 
reliable observation of current states. The properties of 
state  dissemination  in  Anubis  are  tied  tightly  to  these 
concepts  and  target  the  requirements  of  adaptive 
application management. 
3.2. Approach
To  achieve  distributed  autonomous  management, 
components  need  to  coordinate  their  actions  based  on 
the information they obtain. So we require a notion of 
distributed consistency. Moreover, one failure scenario 
that we wish to deal with is a network partition. When 
a network partition occurs the components may wish to 
coordinate  adaptive  actions,  but  due  to  delays  in 
communication  and  possible  asymmetry 
the 
communication paths they may have different views of 
the  state  of  the  system.  So  we  require  a  consistency 
model that deals with these conditions. 
in 
In  many  cases  a  component  will  use  a  distributed 
requiring  a  comparison  of  multiple, 
condition, 
independently  reported  states,  as  the  trigger  for  some 
action.  Typically  these  require  an  understanding  of 
concurrency  among  distributed  states.  Our  model  is 
limited  by  the  fact  that  component  interactions  at  the 
application  level  are  not  visible  or  understandable  to 
the  management  part  of  the  components  (e.g.  the 
SmartFrog  component  may  be  wrapper  code  for  a 
legacy  application).  It  is  not  possible  to  determine  a 
causal  order  among  component  states,  so  we  need  an 
alternative view of concurrency. 
We  chose  to  adopt  a  temporal  model  based  on 
timed  communication  paths.  This  approach  is  similar 
to the  fail-aware timed  model described in [4] and [5] 
and uses a  form of group  membership as the basis  for 
temporal  consistency.  Using  group  membership  as  the 
basis  for  consistency  is  common;  for  example,  it  is 
used  to  attain  view  synchrony  in  ISIS  [3]  and  Totem 
[1]. Fail-aware group membership and communication 
are described in [6] and [11]. Our protocol is described 
in  [12]  and  provides  the  temporal  properties  stated 
below. 
3.3. Properties 
In  our  model,  providers  and  listeners  reside  on 
processing  nodes  with  approximately  synchronized 
clocks,  which  are  connected  by  a  communication 
network. 
(Synchronized  clocks  are  not  strictly 
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:45 UTC from IEEE Xplore.  Restrictions apply. 
necessary,  but  do  simplify  the  implementation  and 
concurrency  model.)  We  assume  that  the  network 
generally delivers messages between processing nodes 
reliably  and  within  a  bounded  communication  delay, 
but  may  suffer  omission  or  timing  failures  between 
given  nodes  or  sets  of  nodes.  Herein  we  use  (cid:303)  to 
represent the communication delay bound. We assume 
that the drift rate of a clock is bounded, but it may fail 
by  drifting  excessively.  Also  a  processing  node  may 
fail  by  halting  or  failing  to  process  messages  within  a 
time  bound.  These  failures  manifest  themselves  as 
communication  timing  failures.  A  node’s  connection 
set  is  the  subset  of  all  nodes  that  are  in  fail-free 
communication  with  it.  A  node  is  always  in  its  own 
connection set, but otherwise it may change over time.  
We say that a node is stable when its connection set 
agrees with those of the nodes in its connection set and 
has  done  so  for  (cid:303)  time  units.  Stability  indicates 