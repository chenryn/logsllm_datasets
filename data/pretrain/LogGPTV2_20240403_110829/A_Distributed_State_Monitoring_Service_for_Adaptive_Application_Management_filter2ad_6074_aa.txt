# A Distributed State Monitoring Service for Adaptive Application Management

**Author:** Paul Murray  
**Affiliation:** Hewlett-Packard Laboratories  
**Contact:** [PI:EMAIL]

## Abstract
Anubis is a simple state monitoring service that supports coordinated action among distributed management agents. It employs a temporal consistency model to address both symmetric and asymmetric network partitions. We have successfully used Anubis to manage adaptive applications in Grid and Utility computing environments. Our experience demonstrates that the abstraction and properties provided by Anubis simplify the task of programming distributed management behavior. This paper examines three common use cases: resource management, lifecycle coordination, and compositional failure management.

## 1. Introduction
The purpose of an application management system is to configure and maintain applications running on a shared computing infrastructure. In Grid and Utility computing environments, resources are shared among users, and applications are granted access as needed. In this context, we focus on distributed applications that can be reconfigured on the fly to adapt to available resources, recover from failures, or scale to accommodate changing workloads. These applications, which we call adaptive applications, require management behavior that automatically determines when and how to perform reconfiguration.

The management behavior of adaptive applications can be complex and highly application-specific. A centralized control point can simplify these issues but introduces a single point of failure and vulnerability to network partitioning. Alternatively, fully distributing the control function addresses these vulnerabilities but introduces the complexity of distributed coordination. This paper focuses on the latter approach.

We believe that distributed, adaptive application management can be simplified through the use of a fully distributed system service that provides discovery, monitoring, and failure detection under a single state-based abstraction. This paper describes such a service called Anubis. Anubis features a temporal consistency model that supports distributed coordination even in the presence of network partitions, allowing disconnected managers to coordinate their actions. We have developed a fully distributed implementation of Anubis, which has been used in both prototype and production systems for adaptive application management. This experience has highlighted several design patterns that demonstrate the usefulness of the service.

The next section provides an overview of our application management environment. Section 3 describes the Anubis state monitoring service and its temporal consistency model, with an implementation described in Section 4. Section 5 examines the design patterns identified by users of the service. Section 6 briefly comments on our experience regarding timeliness, and we conclude in Section 7.

## 2. Overview of Application Management
Our discussion is based on an open-source application management framework called SmartFrog, developed at HP Laboratories. SmartFrog is a component-based framework that views distributed software systems as collections of cooperating components. It includes a component model for describing collections of components and their configurations, and a runtime environment that activates and manages the described components.

A SmartFrog component implements a set of interfaces that provide management actions, such as creation, activation, and termination, and inspection capabilities, such as obtaining the value of attributes associated with the component. Once created in the runtime environment, a component remains an accessible managed entity until it is terminated.

SmartFrog supports hierarchical composition, where one component (a parent) takes responsibility for a group of components (its children) and maps its management actions onto theirs. For example, a group of web server components can be represented collectively by a single server pool component. Instantiating the server pool leads to the instantiation of the web servers, and its termination leads to their termination. This pattern can ultimately be used to manage an entire distributed application as a single component.

In addition to directed actions, a component can also implement autonomous actions, such as maintaining the minimal pool required to service ongoing demand. SmartFrog is implemented in Java as a collection of components that provide standard behavior. New components with new management functionality, such as the server pool, are implemented by extending these standard components.

## 3. Anubis State Monitoring Service
A component implementing adaptive management generally bases its behavior on a view of its environment, often provided by discovery, status monitoring, and failure detection services. We developed the Anubis service to provide these functionalities under a simple state-based interface suitable for adaptive management in the SmartFrog framework. Anubis is a fully distributed, self-organizing, state dissemination service that enables the discovery of objects, monitoring of their states, and detection of their failure or absence.

### 3.1. Providers, Listeners, and States
We introduce the following terminology:
- **State**: An arbitrary value.
- **Provider**: A named object that has a state that may change over time.
- **Listener**: A named object that observes the states of providers.

The Anubis service distributes provider states to matching listeners. A listener observes the state of a provider when it has a copy of its state value. Providers and listeners do not need unique names, so a single listener can contain a collection of states, each representing a matching provider.

Providers and listeners are registered with the Anubis service interface. Discovery is represented by a listener obtaining the state of a provider. Ongoing observation of the provider’s states represents status monitoring. The lack of a state indicates the failure or absence of the provider. Anubis ensures that listeners observe current provider states in a timely manner.

Our notion of state observation is distinct from general publish-subscribe event notification (e.g., TIB/Rendezvous) in that it explicitly addresses reliable observation of current states. The properties of state dissemination in Anubis are tightly tied to these concepts and target the requirements of adaptive application management.

### 3.2. Approach
To achieve distributed autonomous management, components need to coordinate their actions based on the information they obtain. This requires a notion of distributed consistency. One failure scenario we aim to handle is network partitioning. When a network partition occurs, components may wish to coordinate adaptive actions, but due to communication delays and possible asymmetry in communication paths, they may have different views of the system's state. Therefore, we need a consistency model that addresses these conditions.

In many cases, a component will use a distributed condition, requiring a comparison of multiple, independently reported states, as the trigger for some action. Typically, these require an understanding of concurrency among distributed states. Our model is limited by the fact that component interactions at the application level are not visible or understandable to the management part of the components (e.g., the SmartFrog component may be wrapper code for a legacy application). It is not possible to determine a causal order among component states, so we need an alternative view of concurrency.

We chose to adopt a temporal model based on timed communication paths. This approach is similar to the fail-aware timed model described in [4] and [5] and uses a form of group membership as the basis for temporal consistency. Using group membership for consistency is common; for example, it is used to attain view synchrony in ISIS [3] and Totem [1]. Fail-aware group membership and communication are described in [6] and [11]. Our protocol, described in [12], provides the temporal properties outlined below.

### 3.3. Properties
In our model, providers and listeners reside on processing nodes with approximately synchronized clocks, connected by a communication network. Synchronized clocks are not strictly necessary but simplify the implementation and concurrency model. We assume that the network generally delivers messages between processing nodes reliably and within a bounded communication delay, but may suffer from omission or timing failures between given nodes or sets of nodes. We use \(\tau\) to represent the communication delay bound. We also assume that the drift rate of a clock is bounded, but it may fail by drifting excessively. A processing node may fail by halting or failing to process messages within a time bound. These failures manifest as communication timing failures.

A node’s connection set is the subset of all nodes that are in fail-free communication with it. A node is always in its own connection set, but otherwise, it may change over time. We say that a node is stable when its connection set agrees with those of the nodes in its connection set and has done so for \(\tau\) time units. Stability indicates that the node has a consistent view of the system for a sufficient period.

---

This optimized version of your text aims to improve clarity, coherence, and professionalism. If you have any specific areas you would like further refined, please let me know!