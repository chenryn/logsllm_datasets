lated resources are used up, which back-pressures
prior sending components. For example,
the increase of
STALL_NO_TxR_HORZ_CRD_BL_AG1 means that more
Egress buffer of BL ring [63] of agent 1 is stalled because of
waiting for credits. In this case, it stops forwarding packets,
which will increase the delay of the transactions held in the
components.
The third type indicates that the number of packets gets lost
due to congestion. For example, TxR_HORZ_NACK counts
how many Egress packets have not got responded to the
horizontal ring, and NACKs were received. Packet loss would
sharply increase the network delay and cost considerable time
to recover under re-transmission.
Based on the observation, we conjecture that when the two
mesh ﬂows (i.e., attack and victim) go through a mesh stop,
components inside mesh stops become busier, or even stalled
because of the shortage of credits and buffer, resulting in
packet losses and delay increase.
We found the similar patterns also exist for the coherence-
based probe. In particular,
the VNA credits and M3UPI
(the interface between mesh and UPI) credits have seen a
signiﬁcant increase (see the last two rows of Table VI).
E. LLC Slice Isolation
MESHUP mainly targets the cross-tile trafﬁc related to
LLC. Intel CAT allocates different cache ways to different
applications to implement cache isolation, but it does not
partition the cross-tile LLC accesses. If cache partition can be
done at the level of LLC slice, e.g., placing the data frequently
accessed by an application to the LLC slice local
to its
occupied cores, MESHUP might be deterred. Farshin et al. [21]
designed a slice-aware memory management mechanism and
showed it can realize cache partition. Here, we evaluate this
idea under the attack of MESHUP.
In particular, we create a victim application that only
accesses its local LLC in on-off style, and let the eviction-
based probe collect delay sequences on other routes. The
code of real-world applications can be changed to implement
this idea: e.g., reading LCORE_PMA GV when accessing the
memory and conducting sensitive operations only when the
memory is mapped to the application’s local LLC slices. In
this case, the mesh trafﬁc from the victim is minimized, so the
contention with the attacker’s probes is unlikely to happen.
Table IV right shows the SNR by trafﬁc types when this
defense is deployed. Though no more T1-T2 trafﬁc, T3-T4
trafﬁc still carries meaningful information (e.g., the SNR of
13% X-route T4 trafﬁc is larger than 10). The reason is that
the victim core cannot terminate the communication with IMC,
which exposes the access patterns to memory. As a result, we
conclude LLC slice isolation does not mitigate MESHUP.
F. Insights into Mesh Trafﬁc
Limited forwarding capability causes congestion on mesh
stops. A CPU core can execute more than one load/store
instruction per cycle, as there are multiple ports coming
with load store unit, indicating that the memory sub-system
potentially can issue more than one cache line (64 bytes) per
cycle. However, according to [68], each mesh link has only
32-byte wide bi-directional physical data bus (i.e., BL ring),
which can be easily saturated by one core. What’s worse, a
mesh stop serves requests from cores coming from all four
directions, so a mesh link can be severely congested when
there is more incoming trafﬁc than outgoing trafﬁc.
Congestion causes extra delays. Like routers in computer
networks, mesh stops queue the packets to be processed in the
buffer. But unlike routers that drop packets as they will, Intel
adopts credit-based ﬂow control for mesh [63], which is more
reliable. When a mesh stop is unable to process more incoming
packets, it back-pressures the senders by cutting down their
credits. In this case, the packets will be queued in the sender,
and suffer from extra delays. In Section VI-D, we show that
MESHUP can indeed cause extra cycles, credit exhaust, and
even packet NACK in mesh stops, which results in up to 50%
delay increase for mesh.
Fig. 7: Mesh trafﬁc to and from a core. The number inside the
arrow shows the number of tiles that may receive/send trafﬁc,
which is derived according to the policy of YX routing and
the hash function that decides the home of an address.
Uneven trafﬁc distribution. Mesh trafﬁc is unevenly dis-
tributed across the mesh network. To maximize the overall
throughput of LLC and avoid the bottleneck in LLC slices,
Intel uses the hash algorithm such that each LLC slice serves
an address with equal chances. As a consequence, the trafﬁc
may be more congested at some links near the victim core,
11
as they serve more LLC slices. For example, for a victim at
the southwest core, more than half of the incoming trafﬁc may
be carried by the right-hand tile of the victim, and more than
half of the outgoing trafﬁc will go through the north tile, which
is illustrated in Figure 7. This aligns with our observation in
Table V, as the right side of the victim tile has the most tiles
and CHA 14 carries most of the trafﬁc (T2). For T3 and T4,
tiles near IMC may carry more transit trafﬁcs. For T6, tiles
near the PCIe stop may carry more transit trafﬁcs. Cross-socket
T5 will inﬂuence tiles near UPI stops.
VII. ATTACKING REAL-WORLD APPLICATIONS
In this section, we evaluate Phase 2 (Secret Inference)
described in Section III-B. We choose JDK RSA implementa-
tions as the primary target to evaluate MESHUP within a CPU.
Besides, we choose app ﬁngerprinting to evaluate MESHUP
across CPUs.
A. Experiment Settings
CPU
Main board
Memory
OS
JDK version
Chrome version
CAT Version
Intel Xeon Scalable 8260 / 8175*2
Intel C621
64GB
Ubuntu 18.04
11
89.0.4389.82
1.2.0-1
TABLE VII: Hardware and software for the attack evaluation.
Table VII shows the hardware and software speciﬁcation of
our experiment platform. We use 8260 to evaluate the eviction-
based probe, and two-way 8175 for the coherence-based probe.
To validate our claim that MESHUP bypasses cache parti-
tion, we turn on Intel CAT for the entire experiment duration.
This is the common setting for other side-channel attack works
like Xlate [3], but Intel CAT does not represent the strongest
defense, and we acknowledge this limitation of setting in
Section VIII. For Intel CAT, we create two COS. The core
running the victim program is bond with COS 1, while other
cores are bond with COS 2, by using the command pqos of
intel-cmt-cat package [69] (with parameter -e). COS 1
is exclusively allocated with one way of LLC while the rest
10 ways are allocated to COS 2 exclusively. Therefore, the
attacker program will not share any L1/L2/LLC cache with
the victim program.
For the attack within a CPU, we evaluate MESHUP against
RSA encryption and choose its Java implementation because
Java yields more distinguishable patterns of mesh trafﬁc,
compared to other languages without automated memory
management,
like C++. The Java Virtual Machine (JVM)
creates a new object for the same variable for each iteration
within the loop, and uses Garbage Collection (GC) to manage
the old object, which triggers frequent memory access. In
the meantime, huge integer arrays are reused across loops,
e.g., mult, and their addresses are constant, which produces
stable contention patterns. We assume a 2048-bit private key
is used. The attacker aims to infer the bit sequence of the
Send to MeshReceive from Mesh4private key. We tested the ofﬁcial JDK implementation of
RSA (javax.crypto.Cipher) as the victim. We assume
the victim is assigned to a randomly selected core for RSA
encryption, and the attacker is also assigned to a random core
for probing. This setting aligns with the cloud environment, as
both victims and attackers have no control over core selection.
The experiment was repeated 1,000 times and each run uses a
random RSA key, so each delay trace collected by the attacker
is unique.
For the cross-CPU attack, we choose application (or app)
ﬁngerprinting to evaluate the coherence-based probes. We
assume the attacker is interested in learning what app is
running on a machine, so he/she launches MESHUP to probe
the UPI bus. The attacker employs a DNN classiﬁer for
secret inference. Gulmezoglu et al. also launched a cross-CPU
attack for app ﬁngerprinting (the exploited channel is directory
protocol, different from MESHUP) [70], and used 40 apps as
the test suite (see Appendix F). We use the same test suite.
The attacker runs each app 50 times and each run lasts for 5
minutes. Under each app, 38 traces are used for training and
the remaining 12 are for testing.
B. Attacking Sliding Window RSA
Sliding Window is a popular implementation of RSA (e.g.,
GnuPG has adopted Sliding Window after version 1.4.13).
Compared to the older Fast Modular Exponentiation, it has
better efﬁciency and partially ﬁxes the side-channel vulner-
abilities. In particular, Sliding Window decouples key bit
stream from mul/sqr execution sequence, so learning the
occurrences of mul/sqr with timing side-channel does not
let the attacker directly learn the key bits. However, a recent
work [12] showed that mul/sqr execution sequence can still
be utilized to crack Sliding Window RSA. Given a mul/sqr
sequence, their algorithm (Sliding right into disaster, or SRID
for short) is able to either output the 100% correct inference
for a key bit (i.e., either 0 or 1), or output X, meaning the
algorithm is unable to get a correct inference. When using
SRID to crack 2048-bit RSA key, 5-bit sliding window RSA
implementation leaks over 33% of the key bits. JDK uses 7-bit
sliding window, in which case around 30% bits are expected to
be recovered2. MESHUP can make full use of SRID to recover
key bits. The attacker needs only recover mul/sqr sequence,
then SRID is applied to output 0, 1 and x.
LoR has tested the eviction-based probe on CPU ring
against RSA Fast Modular Exponentiation, and achieved 90%
accuracy (with prefetchers on) for key bits recovery [8]. As
a comparison, we also tested MESHUP against RSA Fast
Modular Exponentiation, and the details are elaborated in
Appendix G.
Decoding the delay traces. After analyzing the collected
delay traces, we found they have discernable patterns and fall
into three categories, Pattern A/B/C, as shown in Figure 8.
2For the left-to-right algorithm, the one used by JDK, 28% bits can be
recovered for each iteration when windows size is 7 [12]. Multiple iterations
increase the number of recovered bits, which makes it possible to recover
around 30% bits, as we show later.
12
Fig. 8: A delay sequence collected for Sliding Window RSA.
Black stars represent ground truth positions of mul, and red
dots represent the positions of sqr.
For Pattern A, each mul incurs an obvious rise, while for
Pattern C, each mul incurs a discernible valley. For Pattern
B, each sqr incurs a rise. Because the interval of the rises are
different, their peaks in the frequency spectrum are different.
If a sequence has peaks at around 10.5 kHz, we consider it
Pattern B, and 8 kHz for Pattern A and C. Pattern A and C
can be distinguished as A has many rises while C has valleys.
As such, our decoder is implemented as a script to ﬁnd the
traces that match Pattern A/B/C and then infer the key bits
from them. It 1) clips each samples to a range; 2) smooths
the trace; 3) ﬁnds peaks (valleys) and calculate how many
muls or sqrs are between the found peaks (valleys), 4) selects
the traces matching Pattern A/B/C, 5) launches SRID on the
mul/sqr traces to infer key bits.
Pattern
A
B
C
All
Perfect
mul/sqr Sequence Recovery
#
5
17
34
56
Acc1
99.0%
98.8%
96.1%
97.2%
1
10
3
14
SRID
Acc2
Acc3
31.1% 23.5%
30.7% 25.2%
30.8% 20.4%
30.8% 22.1%
TABLE VIII: Sliding Window RSA key bits recovery results.
“#” is for the number of pattern traces. The slight differences
on ACC2 are attributed to the use of random keys.
Results. We deﬁne three evaluation metrics for this exper-
iment. For a sequences exhibit Pattern A/B/C (called pat-
tern sequence), a mul/sqr sequence (called ms sequence)
is recovered, and its Largest Common String (LCS) is de-
rived by matching the ground-truth. ACC1 is computed as
, where LCSi and M Si are the lengths of ith
LCS and ms sequence, and NM S is the number of all ms
(cid:80)NM S
LCSi
M Si
i=1
Pattern A2.5 k3.9 kPattern B2.5 k3.5 kinterval (Cycles)00.7 ms1.4 ms2.1 ms2.7 msPattern C2.5 k3.5 ki=1
K .
Ci
sequence. On a ms sequence, SRID is executed. Assume
the number of all key bits is K, and correctly recovered
key bits for M Si
is Ci. For the perfectly recovered ms
K . For
Ci
sequences (counted NP ), we deﬁne ACC2 as (cid:80)NP
all sequences, we deﬁne ACC3 as(cid:80)NM S
i=1
Table VIII summarizes the overall results and the results
by Pattern A/B/C. Among all these 1,000 traces, we found 5,
17, and 34 traces exhibit Pattern A, B, and C. So in total 56
sequences can be analyzed with SRID. The remaining traces
(944) do not exhibit discernible patterns because their SNR
is not sufﬁcient. We tried to recover the ms sequences from
the 56 useful traces, and found 14 can be recovered perfectly.
Assuming the victim repeatedly runs RSA and the attacker
keeps proﬁling mesh trafﬁc, it will take on average 18 rounds
to get a ms, or 71 (1,000/14) rounds to get a perfect ms
sequence. The overall ACC1 is 97.2%, suggesting mul and
sqr can be derived at high accuracy from the pattern traces.
Regarding the result on each pattern, we found the accuracy
differs. For Pattern B, the attacker has 59% chances (10 out
of 17) to obtain a perfect ms sequence. Therefore, the attacker
can focus on Pattern B when enough traces are observed.
Then, we tested the SRID algorithm on the perfect and ms
sequences. In average 30.8% key bits (ACC2) can be inferred
from the 14 perfect ms sequences. The accuracy drops to
22.1% (ACC3) when all 56 ms sequences are considered.
A recent work [71] pointed out that SRID algorithm can be
improved for better recovery ratio, and we believe more key
bits can be recovered upon it.
Background Workload. The previous experiment
is con-
ducted in an environment with no prominent background
workload. Here we evaluate how normal background workload
(or noises) would impact
the delay traces and inference
accuracy. To produce the background noise, we run snapd,
sshd, tmux, and a docker with Ubuntu image concurrently with
RSA encryption on the server. Besides, the server runs the
default website of Apache HTTP server, and another machine
repeatedly visits the website. The attacker launches the attack
against RSA as described in Section VII-B.