HRt@K) as the metric to evaluate the effectiveness of poison-
ing attacks for promoting target item t. Suppose there are K
items in the recommendation list for each user. HRt@K is the
proportion of normal users whose top-K recommendation lists
include target item t. We compare HRt@K before and after
attacks to show the attack effectiveness. As the deep learning
based recommender system itself is usually unstable, we train
and evaluate the target model on the dataset for 30 times and
average the evaluation results. Note that, for the same kind of
target items, we will further use the average value of their hit
ratios, recorded as HR@K, to comprehensively evaluate attack
performance.
Parameter Setting. Unless otherwise mentioned, the param-
eter setting for our poisoning attacks is as follows: κ = 1,
λ = 0.01, η = 100, and δ = 0.9 for the ML-100K dataset;
κ = 1, λ = 0.01, η = 100, and δ = 0.8 for the ML-1M
dataset; κ = 1, λ = 0.01, η = 100, and δ = 0.3 for the
Music dataset; and m equals to 1% of the number of normal
users, n = 30, and K = 10 for all datasets. We conduct our
experiments on a CentOS server with 4 NVIDIA Tesla V100
GPUs, 64-bit 14-core Intel(R) Xeon(R) CPU E5-2690 v4 @
2.60GHz and 378 GBs of RAM.
8
TABLE I: HR@10 for different attacks with different attack sizes.
Attack size
Dataset
Attack
Random target items
Unpopular target items
None
Random
ML-100K
Bandwagon
MF
Our attack
None
Random
Music
Bandwagon
MF
Our attack
0.5%
0.0025
0.0028
0.0030
0.0032
0.0034
0.0024
0.0037
0.0036
0.0034
0.0047
1%
0.0025
0.0034
0.0034
0.0035
0.0046
0.0024
0.0048
0.0046
0.0050
0.0068
3%
0.0025
0.0053
0.0055
0.0069
0.0100
0.0024
0.0115
0.0104
0.0120
0.0144
5%
0.0025
0.0078
0.0081
0.0090
0.0151
0.0024
0.0216
0.0176
0.0210
0.0243
0.5%
0
0.0002
0.0002
0.0001
0.0007
0.0003
0.0006
0.0005
0.0005
0.0012
1%
0
0.0003
0.0004
0.0002
0.0019
0.0003
0.0014
0.0011
0.0017
0.0026
3%
0
0.0013
0.0013
0.0014
0.0111
0.0003
0.0053
0.0044
0.0058
0.0086
5%
0
0.0025
0.0024
0.0033
0.0206
0.0003
0.0118
0.0094
0.0118
0.0161
B. Effectiveness of Poisoning Attacks
Now we conduct our experiments under the white-box
setting. Under this setting, we assume that the attacker is
aware of the internal structure,
the training data and the
hyperparameters of the target recommender system so that we
can train an initial poison model that has similar functions with
the target recommender system.
Impact of the Number of Inserted Fake Users. Table I shows
the results of poisoning attacks with different number of fake
users. We measure the effectiveness of attacks with different
attack sizes, i.e., the fraction of the number of fake users
to that of original normal users. In the table, “None” means
no poisoning attacks performed on the target recommender
system and MF represents the poisoning attack method on
matrix-factorization-based recommender systems. We ﬁnd that
our attack is very effective in promoting target items on both
datasets. For example, after inserting only 5% fake users
into the Music dataset, the hit ratio for random target items
increases by about 9.1 times.
Also, we observe that the attack performance of all at-
tack methods increases as the number of fake users inserted
increases. For instance, after injecting 0.5% fake users for
random target items to the ML-100K dataset, our attack can
achieve a hit ratio of 0.0034, while the hit ratio increases
to 0.0151 when injecting 5% fake users. The results are
reasonable because, when more fake users are inserted, the
target items occur more in the poisoned training dataset and
thus can inﬂuence the recommender system more signiﬁcantly.
Our attack signiﬁcantly outperforms the baseline attacks
in all cases. As for the ML-100K dataset, our attack is
quite outstanding and comprehensively surpasses all compared
methods. In particular, when inserting 5% fake users for un-
popular target items, our attack achieves the hit ratio of 0.0206,
about 6.2 times of the best hit ratio obtained by other attacks.
With the Music dataset, our attack is still the most effective
for all situations. For instance, our attacks can improve the
hit ratio of unpopular target items from 0.0003 to 0.0086
with an attack size of 3%. The MF attack achieves the best
performance among the baseline attacks. It can increase the hit
ratio to 0.0058, which is only 67.4% of that of our attack. The
possible reason is that the random attack and the bandwagon
attack do not leverage the information of deep learning models,
e.g., the model structure and parameters, so that they cannot
perform well on deep learning based systems. The MF attack
is designed for factorization-based recommender systems that
use linear inner product of latent vectors to recover Y, while
the target deep learning based recommender system in our
experiments uses extra nonlinear structure. Thus, the MF attack
cannot achieve good attack effectiveness as our attack.
To further evaluate the effectiveness of poisoning attacks
on large datasets, we conduct the experiments on the ML-
1M dataset with an attack size of 5% and sample 5 items for
each type of target items. Note that, to speed up our poisoning
attack, we generate 5 fake users each time. The results are
shown in Table II. First, we observe that, similar to the small
datasets,
is also vulnerable to poisoning
attacks. The hit ratio of unpopular target items increases from
0 to 0.0034 and 0.0060 with the bandwagon attack and our
attack, respectively. Second, our attack still performs the best
among all poisoning attacks on both random target items and
unpopular ones. For example, the hit ratio of random target
items under our attack is 0.0099, about 1.2 times of the highest
result among the baseline attacks.
the large dataset
Moreover, the increase of the hit ratio of unpopular target
items is much more signiﬁcant than that of random target
items. For instance, when injecting 5% fake users into the
Music dataset, the hit ratio of our attacks for random target
items increases by around 9.1 times compared with initial
hit ratio while that of unpopular items increases by about
52.7 times. We suppose that it is caused by the existence
of competing items. When the hit ratio of the target item
increases, the hit ratios of other items correlated with it also
tend to increase. As the sum of all hit ratios is ﬁxed (i.e., 1),
there will be a competitive relationship between them when
the hit ratios on both sides rise to a large value. Unpopular
items have few correlated items since they have few ratings
in the original dataset. Therefore, after a successful attack,
there will be fewer items competing with them than random
target items. This result is encouraging because the items that
attackers want to promote are usually unpopular ones.
Furthermore, all poisoning attacks on the Music dataset are
more effective than the ML-100K and ML-1M datasets. For
example, when promoting random target items with our attack
method, an attacker can increase the hit ratio by about 5.0
times, 4.8 times, and 9.1 times for the ML-100K, ML-1M and
9
TABLE II: HR@10 on a large dataset.
Dataset
Attack
None
Random
ML-1M
Bandwagon
MF
Our attack
Target items
0
Random Unpopular
0.0017
0.0069
0.0080
0.0060
0.0099
0.0024
0.0034
0.0029
0.0060
TABLE III: HR@K for different K.
Dataset
Attack
None
Random
ML-100K
Bandwagon
MF
Our attack
None
Random
Music
Bandwagon
MF
Our attack
5
0
0.0002
0.0002
0.0002
0.0012
0.0001
0.0005
0.0003
0.0006
0.0007
K
10
0
0.0003
0.0004
0.0002
0.0019
0.0003
0.0014
0.0011
0.0017
0.0026
15
0
0.0005
0.0006
0.0004
0.0033
0.0005
0.0025
0.0018
0.0029
0.0042
20
0
0.0006
0.0007
0.0006
0.0042
0.0007
0.0037