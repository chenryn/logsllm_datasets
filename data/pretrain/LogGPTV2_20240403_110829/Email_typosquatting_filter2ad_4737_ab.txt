4.2 Collection methodology
We next turn to discussing how we selected a set of domains to reg-
ister, before delving into the details of our collection infrastructure.
We then explain how we post-processed the data we acquired by
presenting the layered filtering system we built to remove spam
from our corpus.
4.2.1 Domain registration. When deciding on which domain
names to register, we had a number of constraints to satisfy, and
three main objectives in mind.
Constraints. Our first constraint is budgetary. While registering
individual domains is reasonably cheap, in the order of $8–$20 per
year depending on the registrar and top-level-domain being used, it
is potentially time-consuming, and we have to limit ourselves to at
most a couple of hundred domains. Our second constraint, which
is far more serious, is that of availability. Unfortunately a number
of the most interesting typo domains are already registered (either
by the trademark owners themselves, or by typosquatters), so that
we were forced to choose from what is available. However, the set
of gtypos is a powerset of the set of target domains. In particular,
for the top 10,000 domains according to Alexa rankings, there are
millions of gtypos. Even though hundreds of thousands are already
registered, we are still able to select a few dozen typosquatting
domains that can hopefully produce representative outcomes.
Objectives. When we undertook this study, we had absolutely no
idea of the amount of emails we would receive. Our first goal was
thus to find typo domains that could be trusted to provide a repre-
sentative, and measurable signal, if anything was to be measured.
Our second goal was to compare different DL-1 typing mistakes
(e.g., deletion and substitution), to be able to reason about respec-
tive impact of such mistakes. Third, we wanted to register a corpus
of domains that would allow us to measure the different kinds of
typos (receiver, SMTP, reflection) we had identified.
Strategy. To maximize the probability of receiving emails, we
aimed to register typo domains targeting some of the most popular
domains. To that effect, we selected target domains with a small
Alexa rank in the email category (i.e., popular domains for email).
To prune down the list of domains we register, most of the typo
domains we generated have a fat-finger distance of one from the
target domain.
This led us to select domains targeting top email providers such
as Google, Microsoft, Yahoo, Apple, and Mailchimp. We comple-
mented this list with some of the “second tier” e-mail providers
such as Rediffmail Pro, GMX, AOL, Hushmail and ZohoMail.
We hypothesized that we would see more reflection typos on
domains that advertise “disposable,” instant email addresses. Accord-
ingly, we registered typos of the 10 Minute Mail (10minutemail.com)
and YOPmail (yopmail.com) domains.
To assert the risks linked to SMTP typos, we also registered
typos linked to some of the most popular Internet Service providers
IMC ’17: Internet Measurement Conference , November 1–3, 2017, London, United Kingdom
J. Szurdi and N. Christin
Table 1: DNS settings for an example typo domain.
TTL TYPE priority
300
300
300
300
MX
MX
A
A
1
1
NA
NA
record
exampel.com.
exampel.com.
1.1.1.1
1.1.1.1
FQDN
*.exampel.com.
exampel.com.
*.exampel.com.
exampel.com.
Figure 1: The design of the typo email collection infrastruc-
ture
which offer SMTP service to their users: AT&T, Comcast, Cox, TWC
and Verizon.
We chose Paypal and Chase as potential sensitive (financial)
domains and registered a few domains targeting SMTP typos on
these domains.
For each of the target domains, we registered multiple typo
domains to compare how different typing mistakes impact the
amount of email received.
The complete list of 76 domains we registered, as well as addi-
tional information about these domains can be found in an online
appendix.3
4.2.2 Collection infrastructure. Figure 1 shows a high-level over-
view of our data collection infrastructure. Each typo domain is
assigned a different Virtual Private Server, which in turn forwards
the data to our main collection server. This allows us to eschew a
potential (but unlikely) issue, of people spamming us from looking
up domains and flagging us as security researchers. In addition,
to distinguish between different SMTP typo mistakes, we used a
one-to-one mapping of our domain names to virtual private server
IP addresses. This is because the SMTP protocol does not require
the domain name of the SMTP server contacted to be included in
the headers. We thus have to differentiate domains by IP addresses.
Table 1 shows our DNS settings for each domain we registered.
We include wildcard subdomains to collect typo domains sent to
any subdomains of the domains we registered. We run Postfix on
our main collection server, which we configure to accept any email
sent to any email address. The username and the domain name can
thus both be random strings. Our collection server never sends any
email out, but ultimately forwards these emails to a processing and
storage server (not represented in the picture).
Email processing pipeline. Figure 2 describes this email process-
ing pipeline. When we receive an email we first feed it into SpamAs-
sassin [7]. We do not discard email identified as spam, and instead
3See
Szurdi-IMC17-appendix.pdf.
https://www.andrew.cmu.edu/user/nicolasc/publications/
422
Figure 2: The typo email filtering system used.
simply flag it as such. We then tokenize the email into header, body
and attachments, save header information, and run both the body
and any attachments through a text extraction module (Textract
[6]), which operates on a variety of different file formats, even
performing optical character recognition on some image files.
Filtering out sensitive information. We send the text output
into a filtering system based on regular expression matching. The
idea is to flag when sensitive information is found in an email,
while immediately discarding it to protect user privacy. We use
the HIPAA list of personal identifiers [3] as a baseline for our set
of sensitive information. We replace personal identifiers by salted
hashes whenever possible; as an added precaution, we replace all
digits in the text by zeroes.
We use the public Enron email corpus [25] (May 7, 2015 version)
to test how well our regular expression matching heuristics are
performing. Table 2 shows the precision (ratio of true positives
over true and false positives) and sensitivity (ratio of true positives
over true positives and false negatives) for each type of sensitive
information. In our context, these metrics are more useful than the
more widely used “accuracy” metric. Indeed, because the number
of emails containing private identifiers is small overall (and in-
deed, this is also true of the Enron corpus), we have an imbalanced
dataset; as a result, an algorithm that always outputs “no sensitive
information was found” would have a high accuracy.
Each score in Table 2 is computed based on sampling 20 random
emails per type of sensitive information found in the dataset (except
for social security numbers, for which we only had 13 examples
outlo0k.comgmaiql.comho6mail.comsmtpverizon.netRegistered domainsDNS (cid:862)Forwarding(cid:863)Virtual PrivateServersSMTP ForwardingMainCollectionServerJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaineyRob MilnthorpBarry TycholizBook us 3 rooms and make sure that we can have 2 beds in one of the rooms.ThanksJohnJohn LavoratoAmex *_|R|_*americanexpress*000000000000000*_|R|_* Exp00/00Dave DelaineyRob MilnthorpBarry TycholizBook us 0 rooms and make sure that we can have 0 beds in one of the rooms.ThanksJohnOriginal e-mailFiltered e-mail and filtered attachmentsDNS serverTokenize e-mailMetaDataand LogsJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaineyRob MilnthorpBarry TycholizBook us 3 rooms and make sure that we can have 2 beds in one of the rooms.ThanksJohnTextextractionJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnFilteringEncryptEncryptede-mailSpamassassinJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaiJohnJohn LavoratoAmex 371385129301004 Exp06/03Dave DelaineyRob MilnthorpBarry TycholizBook us 3 rooms and make sure that we can have 2 beds in one of the rooms.ThanksJohnEmail Typosquatting
IMC ’17: Internet Measurement Conference , November 1–3, 2017, London, United Kingdom
Table 2: Precision and Sensitivity of our regular expression
based filtering module.
receiver or reflection typo emails (but not in SMTP typo emails), the
recipient’s email address should belong to one of our typo domains.
Sensitive info
Credit card number
Social Security number
Employer id. number
Password
Vehicle id. number
Username
Zip
Identification number
Email address
Phone number
Date
F1-score Prec.
0.93
0.78
0.89
0.33
1.00
0.59
1.00
0.75
1.00
0.83
1.00
0.96
0.88
0.94
0.50
1.00
0.74
1.00
0.67
0.99
0.89
1.00
Sens.
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.60
0.98
0.95
1.00
available), manually labeling them, and comparing them to what
our algorithm produced. The results show a high recall for most
sensitive information, except for Identification numbers. The sen-
sitivity for identification numbers is low, because our definition
of an identification number is very broad. To validate our results
further (beyond the biased sample produced by our algorithm), we
sampled an additional 100 random emails from the Enron dataset
and manually labeled them. Due to the imbalanced nature of sensi-
tive data, we only found phone numbers, emails and dates in this
sample. The sensitivity remains high however—0.91, 1.00 and 0.98
for phone, date and email respectively.
Once all of this processing is done, we encrypt each part (header,
body, attachment) and most of the log files for storage on our
collection server.
4.3 Email classification
After running our experiment for a few days, it became obvious
we were receiving very large amounts of spam, which would com-
pletely bias any analysis if left unfiltered. Spam can come from
miscreants noticing our servers accept any email (even though
they don’t relay to any party but our collection server), or from
users mistyping their own email address (reflection typo) and be-
ing subsequently added to promotional lists. Some of our domains
might have also been previously registered, and could still appear
in certain promotional lists.
We thus turned to building a filtering and classification module,
which not only filters out spam, but also classifies reflection typo
emails that result from a single typo (e.g., making a typo while
signing up for a mailing list). Our classification module consists of
five layers, which act as a funnel: each email marked as spam in a
given layer is not further considered.
Layer 1: Detecting erroneous header fields. Emails in which
the name of the SMTP server relaying the mail to our collection
server does not match the name of one of our registered domains
is immediately classified as spam. The sender’s address should
also not belong to one of our domains, since we do not send any
email. Conversely, spammers often pose as sending from the same
domain as the intended recipient. Thus, any email in which the
sender appears to be one of our domains is classified as spam. In
Table 3: Evaluation of Spamassassin on four datasets
Dataset
TREC [8]
CSDMC [2]
SpamAssassin [7]
Untroubled [9]
Precision Recall
0.79
0.87
0.84
0.23
0.98
0.98
0.97
–
Layer 2: SpamAssassin. We run SpamAssassin on all incoming
email. Table 3 shows our evaluation of SpamAssassin in local mode
with the default thresholds on four different datasets. While preci-
sion is good, the low recall indicates we need additional filtering.
We immediately remove all emails with ZIP or RAR attachments
and consider them as spam—we indeed receive large amounts of
such emails, and every single one of them we manually inspected
was spam.
Layer 3: Collaborative spam filtering. If a sender sends us spam
once, we consider all of the emails from that sender, across all
of our domains, to be spam. Furthermore we apply bag-of-words
analysis to the email body. If the analysis yields more than 20 words,
we flag all other emails with a matching bag-of-words as spam.
This filtering step should have high precision, because it is highly
unlikely that two emails would be spam and ham, respectively, if
both emails use the same corpus of words.
Layer 4: Detecting reflection typos. Emails that have survived
the first three layers might not be spam, but still be the product of
automated systems. For instance, a user might have made a typo
while signing up for a certain service, and subsequently received
notifications to that erroneous address. We automatically classify
these emails, using a set of regular expression heuristics. If an
“unsubscribe-list” header field is present; “bounce” or “unsubscribe”
appears in the Sender:, From:, or Reply-To: fields; or if any two
of From:, Reply-To:, or Return-Path: have different values, we
classify the email as a reflection typo. We additionally search for
strings including “unsubscribe,” “remove yourself,” and other sim-
ilar content in the body to flag email containing such strings as
reflection typos. Finally, we also filter out emails sent from system
users, e.g., “postmaster,” “root,” or “admin.”
Layer 5: Frequency-based filtering. Finally, the last layer filters
out receiver typo emails (but not SMTP typos) for which the sender
address, the recipient email address, or the email body appear too of-