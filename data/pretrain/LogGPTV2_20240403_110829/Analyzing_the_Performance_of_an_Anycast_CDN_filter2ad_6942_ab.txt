Figure 2: Distances in kilometers (log scale) from volume-weighted
clients to nearest front-ends.
to further reduce overhead, each beacon only makes four measure-
ments to front-ends: (a) a measurement to the front-end selected
by anycast routing; (b) a measurement to the front-end judged to be
geographically closest to the LDNS; and (c-d) measurements to two
front-ends randomly selected from the other nine candidates, with
the likelihood of a front-end being selected weighted by distance
from the client LDNS (e.g. we return the 3rd closest front-end with
higher probability than the 4th closest front-end). Third, for most
of our analysis, we aggregate measurements by /24 and consider
distributions of performance to a front-end, so our analysis is robust
even if not every client measures to the best front-end every time.
To partially validate our approach, Figure 1 shows the distribution
of minimum observed latency from a client /24 to a front-end. The
labeled Nth line includes latency measurements from the nearest
N front-ends to the LDNS. The results show decreasing latency
as we initially include more front-ends, but we see little decrease
after adding ﬁve front-ends per preﬁx, for example. So, we do not
expect that minimum latencies would improve for many preﬁxes if
we measured to more than the nearest ten front-ends that we include
in our beacon measurements.
4. CDN SIZE AND GEO-DISTRIBUTION
The results in this paper are speciﬁc to Bing’s anycast CDN de-
ployment. In this section we characterize the size of the deployment,
showing that our deployment is of a similar scale–a few dozens of
front-end server locations–to most other CDNs and in particular
most anycast CDNs, although it is one of the largest deployments
within that rough scale. We then measure what the distribution of
these dozens of front-end locations yields in terms of the distance
from clients to the nearest front-ends. Our characterization of the
performance of this CDN is an important ﬁrst step towards under-
standing anycast performance. An interesting direction for future
work is to understand how to extend these performance results to
CDNs with diﬀerent numbers and locations of servers and with
diﬀerent interdomain connectivity [18].
We compare our CDN to others based on the number of server
locations, which is one factor impacting CDN and anycast per-
formance. We examine 21 CDNs and content providers for which
there is publicly available data [3]. Four CDNs are extreme outliers.
ChinaNetCenter and ChinaCache each have over 100 locations in
China. Previous research found Google to have over 1000 locations
worldwide [16], and Akamai is generally known to have over 1000
as well [17]. While this scale of deployment is often the popular
image of a CDN, it is in fact the exception. Ignoring the large Chi-
nese deployments, the next largest CDNs we found public data for
are CDNetworks (161 locations) and SkyparkCDN (119 locations).
The remaining 17 CDNs we examined (including ChinaNetCenter’s
and ChinaCache’s deployments outside of China) have between 17
locations (CDNify) and 62 locations (Level3). In terms of number
of locations and regional coverage, the Bing CDN is most similar
to Level3 and MaxCDN. Well-known CDNs with smaller deploy-
ments include Amazon CloudFront (37 locations), CacheFly (41
locations), CloudFlare (43 locations) and EdgeCast (31 locations).
CloudFlare, CacheFly, and EdgeCast are anycast CDNs.
To give some perspective on the density of front-end distribu-
tion, Figure 2 shows the distance from clients to nearest front-ends,
weighted by client Bing query volumes. The median distance of the
nearest front-end is 280 km, of the second nearest is 700 km, and
of fourth nearest is 1300 km.
5. ANYCAST PERFORMANCE
We use measurements to estimate the performance penalty any-
cast pays in exchange for simple operation. Figure 3 is based on
millions of measurements, collected over a period of a few days,
and inspired us to take on this project.
As explained in § 3, each execution of the JavaScript beacon
yields four measurements, one to the front-end that anycast selects,
and three to nearby unicast front-ends. For each request, we ﬁnd the
latency diﬀerence between anycast and the lowest-latency unicast
front-end. Figure 3 shows the fraction of requests where anycast
performance is slower than the best of the three unicast front-ends.
Most of the time, in most regions, anycast does well, performing as
well as the best of the three nearby unicast front-ends. However,
anycast is at least 25ms slower for 20% of requests, and just below
10% of anycast measurements are 100ms or more slower than the
best unicast for the client.
This graph suggests possible beneﬁts in using DNS-based redi-
rection for some clients, with anycast for the rest. Note that this
is not an upper bound:
to derive that, we would have to poll all
front-ends in each beacon execution, which is too much overhead.
There is also no guarantee that a deployed DNS-based redirection
system will be able to achieve the performance improvement seen in
Figure 3 – to do so the DNS-based redirection system would have to
be practically clairvoyant. Nonetheless, this result was suﬃciently
tantalizing for us to study anycast performance in more detail, and
seek ways to improve it.
Examples of poor anycast routes: A challenge in understanding
anycast performance is ﬁguring out why clients are being directed
to distant or poor performing edges front-ends. To troubleshoot,
we used the RIPE Atlas [2] testbed, a network of over 8000 probes
predominantly hosted in home networks. We issued traceroutes
from Atlas probes hosted within the same ISP-metro area pairs
where we have observed clients with poor performance. We observe
in our analysis that many instances fall into one of two cases. 1)
BGP’s lack of insight into the underlying topology causes anycast
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 50 100 150 200CDF of /24sMin Latency (ms)9 front-ends7 front-ends5 front-ends3 front-ends1 front-end 0 0.2 0.4 0.6 0.8 1 64 128 256 512 1024 2048 4096 8192CDF of Clientsweighted by query volumeClient Distance to Nth Closest Front-end (km)1st Closest2nd Closest3rd Closest4th Closest533Figure 3: The fraction of requests where the best of three diﬀerent
unicast front-ends out-performed anycast.
Figure 4: The distance in kilometers (log scale) between clients and the
anycast front-ends they are directed to.
to make suboptimal choices and 2) intradomain routing policies of
ISPs select remote peering points with our network.
In one interesting example, a client was roughly the same distance
from two border routers announcing the anycast route. Anycast
chose to route towards router A. However, internally in our network,
router B is very close to a front-end C, whereas router A has a
longer intradomain route to the nearest front-end, front-end D. With
anycast, there is no way to communicate [39] this internal topology
information in a BGP announcement.
Figure 5: Daily poor-path prevalence during April 2015 showing what
fraction of client /24s see diﬀerent levels of latency improvement over
anycast when directed to their best performing unicast front-end.
formance.1 Next we examine how common these issues are from
day-to-day and how long issues with individual networks persist.
Is anycast performance consistently poor? We ﬁrst consider
whether signiﬁcant fractions of clients see consistently poor per-
formance with anycast. At the end of each day, we analyzed all
collected client measurements to ﬁnd preﬁxes with room for im-
provement over anycast performance. For each client /24, we cal-
culate the median latency between the preﬁx and each measured
unicast front-end and anycast.
Figure 5 shows the prevalence of poor anycast performance each
day during April 2015. Each line speciﬁes a particular minimum
latency improvement, and the ﬁgure shows the fraction of client
/24s each day for which some unicast front-end yields at least that
improvement over anycast. On average, we ﬁnd that 19% of preﬁxes
see some performance beneﬁt from going to a speciﬁc unicast front-
end instead of using anycast. We see 12% of clients with 10ms or
more improvement, but only 4% see 50ms or more.
Poor performance is not limited to a few days–it is a daily con-
cern. We next examine whether the same client networks experience
recurring poor performance. How long does poor performance per-
sist? Are the problems seen in Figure 5 always due to the same
problematic clients?
Figure 6 shows the duration of poor anycast performance dur-
ing April 2015. For the majority of /24s categorized as having
poor-performing paths, those poor-performing paths are short-lived.
Around 60% appear for only one day over the month. Around 10%
of /24s show poor performance for 5 days or more. These days
are not necessarily consecutive. We see that only 5% of /24s see
continuous poor performance over 5 days or more.
These results show that while there is a persistent amount of poor
anycast performance over time, the majority of problems only last
1No geolocation database is perfect. A fraction of very long client-to-front-end dis-
tances may be attributable to bad client geolocation data.
Several other examples included cases where a client is nearby a
front-end but the ISP’s internal policy chooses to hand oﬀ traﬃc at a
distant peering point. Microsoft intradomain policy then directs the
client’s request to the front-end nearest to the peering point, not to
the client. Some examples we observed of this was an ISP carrying
traﬃc from a client in Denver to Phoenix and another carrying
traﬃc from Moscow to Stockholm. In both cases, direct peering
was present at each source city.
Intrigued by these sorts of case studies, we sought to understand
anycast performance quantitatively. The ﬁrst question we ask is
whether anycast performance is poor simply because it occasionally
directs clients to front-ends that are geographically far away, as was
the case when clients in Moscow went to Stockholm.
Does anycast direct clients to nearby front-ends? In a large
CDN with presence in major metro areas around the world, most
ISPs will see BGP announcements for front-ends from a number of
diﬀerent locations. If peering among these points is uniform, then
the ISP’s least cost path from a client to a front-end will often be the
geographically closest. Since anycast is not load or latency aware,
geographic proximity is a good indicator of expected performance.
Figure 4 shows the distribution of the distance from client to
anycast front-end for all clients in one day of production Bing traﬃc.
One line weights clients by query volume. Anycast is shown to
perform 5-10% better at all percentiles when accounting for more
active clients. We see that about 82% of clients are directed to a
front-end within 2000 km while 87% of client volume is within
2000 km.
The second pair of lines in Figure 4, labeled “Past Closest”,
shows the distribution of the diﬀerence between the distance from
a client to its closest front-end and the distance from the client to
the front-end anycast directs to. About 55% of clients and weighted
clients have distance 0, meaning they are directed to the nearest
front-end. Further, 75% of clients are directed to a front-end within
around 400 km and 90% are within 1375 km of their closest. This
supports the idea that, with a dense front-end deployment such as
is achievable in North America and Europe, anycast directs most
clients to a relatively nearby front-end that should be expected to
deliver good performance, even if it is not the closest.
From a geographic view, we found that around 10-15% of /24s
are directed to distant front-ends, a likely explanation for poor per-
 0 0.2 0.4 0.6 0.8 1 0 20 40 60 80 100CCDF of RequestsPerformance difference betweenanycast and best unicast (ms)EuropeWorldUnited States 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 64 128 256 512 1024 2048 4096 8192CDFDistance (km)Weighted Clients Past ClosestClients Past ClosestWeighted Clients to Front-endClients to Front-end 0 0.2 0.4 0.6 0.8 104/0404/1104/1804/25Fraction of /24sDateall> 10ms> 25ms> 50ms > 100ms534Figure 6: Poor path duration across April 2015. We consider poor
anycast paths to be those with any latency inﬂation over a unicast front-
end.
Figure 8: The distribution of change in client-to-front-end distance
(log scale) when when the front-end changes, for the 7% of clients that
change front-end throughout a day.
6. ADDRESSING POOR PERFORMANCE
The previous section showed that anycast often achieves good
performance, but sometimes suﬀers signiﬁcantly compared to uni-
cast beacon measurements. However, the ability for unicast to beat
anycast in a single measurement does not guarantee that this per-
formance is predictable enough to be achievable if a system has to
return a single unicast front-end to a DNS query.
If a particular
front-end outperformed anycast in the past for a client, will it still
if the system returns that front-end next time? Additionally, be-
cause of DNS’s design, the system does not know which client it
is responding to, and so its response applies either to all clients of
an LDNS or all clients in a preﬁx (if using ECS). Can the system
reliably determine front-ends that will perform well for the set of
clients?
We evaluate to what degree schemes using DNS and ECS can
improve performance for clients with poor anycast performance.
We evaluate (in emulation based on our real user measurements)
a prediction scheme that maps from a client group (clients of an
LDNS or clients within an ECS preﬁx) to its predicted best front-
It updates its mapping every prediction interval, set to one
end.
day in our experiment.2 The scheme chooses to map a client group
to the lowest latency front-end across the measurements for that
group, picking either the anycast address or one of the unicast front-
ends. We evaluate two prediction metrics to determine the latency
of a front-end, 25th percentile and median latency from that client
group to that front-end. We choose lower percentiles, as analysis
of client data showed that higher percentiles of latency distribu-
tions are very noisy (we omit detailed results due to lack of space).
This noise makes prediction diﬃcult, as it can result in overlap-
ping performance between two front-ends. The 25th percentile and
median have lower coeﬃcient of variation, indicating less variation
and more stability. Our initial evaluation showed that both 25th
percentile and median show very similar performance as prediction
metrics, so we only present results for 25th percentile.
We emulate the performance of such a prediction scheme using
our existing beacon measurements. We base the predictions on one
day’s beacon measurements. For a given client group, we select
among the front-ends with 20+ measurements from the clients.
We evaluate the performance of the prediction scheme by com-
paring against the performance observed in next day’s beacon mea-
surements. We compare 50th and 75th anycast performance for the
group to 50th and 75th performance for the predicted front-end.
The Bing team routinely uses 75% percentile latency as an inter-