25
50
75
90
95
99
Quantile [%]
Table 6: Values for cwnd − ssthresh just prior to
exiting recovery.
Figure 5: A comparison of time spent in recovery for
PRR, RFC 3517 and Linux.
5.2 PRR in practice
We measured the percentage of recovery events that PRR
begins operation in the proportional versus the slow start
modes. Measurements, summarized in Table 5, show that
approximately 45% of the fast recovery events start with
operating in the proportional part of the algorithm (pipe
> ssthresh), 32% of the events start in the slow start part
(pipe < ssthresh) and 13% of the events start with pipe ==
ssthresh. Table 6 shows that in approximately 90% of the
recovery events, the congestion window in PRR converges to
ssthresh by the end of recovery. In the rest of the recoveries,
the segment losses were too heavy for slow start to complete
building the pipe value to ssthresh.
5.3 Performance of PRR vs RFC 3517 and
Linux
We ﬁrst compare the recovery times, followed by retrans-
mission statistics and impact on TCP latency for HTTP
responses.
Figure 5 shows that PRR reduces the time spent in recov-
ery as compared to both RFC 3517 and Linux fast recovery.
PRR has a shorter recovery time primarily due to its smaller
number of timeouts during recovery, as we will see shortly.
Table 7 shows the value of cwnd on exiting recovery. PRR
sets cwnd to ssthresh on exiting recovery, and therefore
has a similar cwnd distribution to that of RFC 3517. Note
that the slightly larger values of cwnd for PRR compared to
RFC 3517 are because of the reduced number of timeouts in
the recovery phase. As described in Section 3.2, Linux pulls
the cwnd value down to at most pipe + 1 in recovery. A
consequence of this is that for short HTTP responses which
have limited new data to transmit, over 50% of recovery
events in Linux end with cwnd smaller than three segments.
Table 8 compares the retransmission statistics. The main
point is that PRR reduces the number of timeouts in re-
covery by 5% compared with Linux and by 2.6% compared
with RFC 3517. Furthermore, PRR reduces the number of
retransmissions by 3% compared to RFC 3517. We also
note that RFC 3517 incurs greater lost retransmissions as
compared to PRR. This is due to the fact that when pipe
Quantiles for cwnd after recovery (segments).
99
Quantile:
35
31
19
10
2
2
1
25
3
3
2
50
6
5
3
75
9
8
5
90
15
14
9
95
21
19
12
RFC 3517:
PRR:
Linux:
Table 7: A comparison of PRR, RFC 3517 and Linux
recovery.
falls far below ssthresh, RFC 3517 sends most of the re-
transmissions as a single large burst, thereby increasing the
likelihood of drops in retransmissions. About 32% of the
fast recovery events start of with pipe strictly lower than
ssthresh.
In these cases, RFC 3517 has the possibility of
transmitting large bursts, with the largest burst size up to
338 segments, and about 1% of the recovery events might
generate bursts greater than ten segments (Table 5).
We picked two representative Google services, search and
page ads, to evaluate the latency diﬀerences. Table 9 shows
the quantiles for TCP latency measured. Compared to
Linux fast recovery, PRR and RFC 3517 reduce the average
latency of the responses with retransmissions by 3-10%, and
the overall latency (including responses without losses) by 3-
5%. PRR achieves similar latency as compared to RFC 3517,
however, it does so without sending large bursts.
5.4 YouTube in India
We evaluated the three algorithms in our India YouTube
video servers, to observe how they performed with long-
running TCP transfers. The YouTube service is built on
progressive HTTP which normally uses one TCP connec-
tion to deliver an entire video. The server sends the ﬁrst
couple tens of seconds of video as fast as possible, then it
rate-limits the transfer based on the video encoding rate to
minimize wasted network capacity if a user cancels the rest
of the video. However, for most video downloaded in In-
dia, there is little or no diﬀerence between the throttled and
un-throttled data rates because there is little or no surplus
network capacity above the video encoding rate. In this case
163Retransmissions measured in 1000’s of segments.
Linux baseline RFC 3517 diﬀ. [%]
Retransmission type
Total Retransmission
Fast Retransmission
TimeoutOnRecovery
Lost Retransmission
85016
18976
649
393
+3119 [+3.7%]
+3193 [+17% ]
-16 [-2.5%]
PRR diﬀ [%]
+2147 [+2.5%]
+2456 [+13%]
-32 [-5.0%]
+777 [+198%]
+439 [+117%]
Table 8: A comparison of retransmission statistics and timeouts of PRR, RFC 3517 and Linux recovery.
Google Search
Page Ads
Quantile
25
50
90
99
Mean
Linux
487
852
4338
31581
2410
RFC 3517
-39 [-8%]
-50 [-5.8%]
-108 [-2.4%]
-1644 [-5.2%]
PRR
-34 [-7%]
-48 [-5.6%]
-88 [-2%]
-1775 [-5.6%]
-89 [-3.7%]
-85 [-3.5%]
Linux
464
1059
4956
24640
2441
RFC 3517
-34 [-7.3%]
-83 [-7.8%]
-461 [-9.3%]
-2544 [-10%]
-220 [-9%]
PRR
-24 [-5.2%]
-100 [-9.4%]
-481 [-9.7%]
-2887 [-11.7%]
-239 [-9.8%]
Table 9: A comparison of TCP latency (ms) for PRR, RFC 3517 and Linux recovery. Quantiles are shown
for responses that have at least one retransmission.
TCP is busy most of the time and there are long stretches
where the data rate is determined entirely by TCP conges-
tion control and the network, i.e., TCP is eﬀectively in bulk
transfer mode.
We sampled roughly 0.8 million connections in 96 hours
and list key statistics in Table 10. Video downloads served
with diﬀerent fast recovery algorithms have almost identical
sample size and average transfer size (2.3MB per connec-
tion).
The network transmit time measures the total time per
connection when there is unacknowledged data in the TCP
write queue, i.e., when TCP is actively transmitting data.
Similarly, the network recovery time is that part of the net-
work transmit time when the sender is in either fast recovery
or timeout-recovery. Overall, RFC 3517 has the best perfor-
mance since on average it delivered video in 4.7% less net-
work transmit time than the Linux baseline. PRR is about
3% faster than the baseline.
The video downloads in India spent 43% to 46% of the net-
work transmit time recovering from losses. While RFC 3517
spent more time and sent more retransmission in recovery
than PRR and Linux baseline did,
it also delivers more
data during fast recovery: 5% more bytes compared to
Linux baseline. This is because the network losses are clus-
tered such that in fast recovery, pipe drops below ssthresh
for about 40% of the events (not shown in table), causing
RFC 3517 to send as much data necessary to keep the pipe at
ssthresh. This aggressive transmission, described as Prob-
lem 2 in Section 3.1, causes 16.4% of the RFC 3517 fast-
retransmits to drop, while Linux and PRR both lose less
than 5% of the fast-retransmits. Furthermore after recov-
ery, Linux transitions directly into slow-start 56% of the
time. Both PRR and RFC 3517 end with cwnd at or close
to sshtresh and do not need to perform this extra slow-start.
In summary, RFC performs the best but it retransmits
too aggressively and has much higher (retransmission) drop
rate. PRR achieves good performance with a slight increase
of retransmission rate.
6. EARLY RETRANSMIT
As observed in section 2.1, the average Google HTTP re-
sponse is only 7.5kB or about 5-6 segments. Early retrans-
mit (ER) [2] is designed to overcome the well known limita-
tion with fast retransmit: if a loss occurs too close to the end
of a stream, there will not be enough dupacks to trigger a
fast retransmission. ER lowers the dupthresh to 1 or 2 when
the outstanding data drops to 2 or 3 segments respectively.
Clearly, any reordering can falsely trigger early retrans-
mit. If this happens near the end of one HTTP response, the
sender will falsely enter fast recovery which lowers the cwnd
and slows the next HTTP response over the same connec-
tion. To make ER more robust in the presence of reordering,
RFC 5827 describes three mitigation algorithms:
1. Disabling early retransmit if the connection has de-
tected past reordering.
2. Adding a small delay to early retransmit so it might
be canceled if the missing segment arrives slightly late.
3. Throttling the total early retransmission rate.
We implemented the ﬁrst two algorithms. The ﬁrst one
is straightforward because Linux already detects reordering
based on SACK. For the second algorithm, we use the RTO
timer to delay the early retransmission for a conﬁgurable
short interval. The sender cancels early retransmit if it re-
ceives an ACK during this interval. We do not implement
the last mitigation because it only makes sense for servers
facing a homogeneous user pool.
6.1 Results
We used the experimental framework described in Sec-
tion 5.1 to run a 4-way experiment for 72 hours in April
2011. We compared: the baseline (original kernel), the naive
ER without any mitigation, ER with ﬁrst mitigation, and
ER with both mitigations.
The statistics show that naive early retransmit causes a
signiﬁcant increase, 31%, in the number of fast retransmits
for a 2% reduction in the number of timeouts relative to
unmodiﬁed Linux TCP. These come at a substantial cost: a
164Linux baseline RFC 3517
Network Transmit Time (s)
% Time in Loss Recovery
Retransmission Rate %
% Bytes Sent in FR
% Fast-retransmit Lost
Slow-start after FR
87.4
42.7%
5.0%
7%
2.4%
56%
83.3
46.3%
6.6%
12%
16.4%
1%
PRR
84.8
44.9%
5.6%
10%
4.8%
0%
Table 10: India YouTube video transfers loss recovery statistics. The average transfer size is 2.3MB and
average RTT is 860ms.
Quantile
5
10
50
90
99
Linux
282
319
1084
4223
26027
ER
258 [-8.5%]
301 [-5.6%]
997 [-8.0%]
4084 [-3.3%]
25861 [-0.6%]
Table 11: A comparison of TCP latency (ms) for
Linux baseline and ER with both mitigations.
27% increase in the number of “undo” events where it was
determined that the retransmission was spurious, and the
cwnd change is reversed. We were surprised by the amount
of small reordering in the network. After we inspected the
TCP traces with Google network operators, we conﬁrmed
that the reordering happens outside of Google networks, i.e.,
in the Internet.5
The ﬁrst mitigation is not as eﬀective because most HTTP
connections are short. Although Linux also implements RFC
2140 [27] which caches the reordering history of past connec-
tions of the same IP, the cache hit rate is very low due to
the server load-balancing and the size of web server farm.
The second mitigation, using a short timer to slightly
delay the early retransmit, provides a substantial improve-
ment, because it gives time for the ACKs from out–of–order
segments to arrive and cancel the pending early retransmis-
sion. For the timer, we used 1/4 of the smoothed RTT bound
to the range of 25ms to 500ms. One of the things that we
noticed when we were tinkering with the timer design was
that it does not aﬀect the results much. This insensitivity to
timer details is interesting, because it gives some hints about
typical reordering distance, but we chose not to investigate
it at this point and picked a reasonable compromise design.
ER with both mitigations can reduce 34% of the timeouts
in Disorder state with 6% of early retransmits identiﬁed as
spurious retransmissions via DSACKs. Some losses in ER
with mitigation are now repaired by fast recovery instead
of by timeout, therefore the total number of retransmissions
remain almost the same as the baseline (1% increase). We
compare the TCP latencies in ER with both mitigations and
the original Linux in Table 11. Responses that do not expe-
rience losses or ﬁt entirely into one segment are excluded be-
cause ER can not repair these responses. For the remaining
responses, ER with both mitigations reduces latencies up to
5The traces also reveal that such reorderings are likely due
to router load-balancing on the forward paths where the
last sub-MSS segment is SACKed before the prior full MSS
segment. Clearly, the mitigations in ER are needed [6].
8.5% and is most eﬀective for short transactions. However,
the overall latency reduction by ER is signiﬁcantly limited
in Google Web servers. Since the number of timeouts in dis-
order is very small compared to timeouts occurring in the
open state.
We end the section with a note on the combined experi-
ments with PRR and early retransmit. The main observa-
tion is that the two mechanisms are independent features of
TCP fast recovery, wherein the early retransmit algorithm
determines when a connection should enter recovery, while
PRR improves the recovery process itself.
7. RELATED WORK
Several researchers have investigated the performance of
TCP loss recovery in TCP traces of real user traﬃc [12, 5, 22,
26]. In data from 1995 it was observed that 85% of timeouts
were due to receiving insuﬃcient duplicate acknowledgments
to trigger Fast Retransmit [12].
Interestingly, Google still
shows 46% to 60% retransmissions happen during timeout
recovery. In a study of the 1996 Olympic Web servers it was
estimated that SACK might only save 4% of the timeouts [5].
The authors invented limited transmit which was standard-
ized [3] and widely deployed. An analysis of the Coral CDN
service identiﬁed loss recovery as one of the major perfor-
mance bottlenecks [26]. The idea of progressively reducing
the window, rather than a half RTT silence, has been ex-
plored before [11], including prior work by Mathis [19].
Improving the robustness and performance of TCP loss re-
covery has been a recurrent theme in the literature. TCP re-
covery improvements fall into several broad categories: bet-
ter strategies for managing the window during recovery [11,
16, 5]; detecting and compensating for spurious retransmis-
sions triggered by reordering [7, 13, 14]; disambiguating loss
and reordering at the end of a stream [24]; improvements to
the retransmission timer and its estimator.
The Early Retransmit algorithm shares many similarities
with thin stream support for TCP [20], which is designed to