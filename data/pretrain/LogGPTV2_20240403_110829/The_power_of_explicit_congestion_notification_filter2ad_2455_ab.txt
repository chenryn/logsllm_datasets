### Detection and Marking Schemes: RED, REM, and PI

Several active queue management (AQM) techniques have been developed to handle network congestion, including Random Early Detection (RED) [15], Random Exponential Marking (REM) [8], and Proportional Integrator (PI) [19].

#### Random Early Detection (RED)

RED uses a weighted average queue size as a measure of congestion. The drop (or mark) rate depends on two threshold parameters: `minth` (minimum threshold) and `maxth` (maximum threshold). The behavior of RED is as follows:
- When the weighted average queue length is below `minth`, no packets are marked or dropped.
- When the average queue length is between `minth` and `maxth`, the probability of marking or dropping packets increases linearly from 0 to a maximum drop probability (typically set to 0.1).
- If the average queue length exceeds `maxth`, all packets are marked or dropped.

The original RED paper [16] recommends marking packets when Explicit Congestion Notification (ECN) is enabled, while RFC 3168 [31] suggests dropping packets even if they are ECN-enabled. This latter rule is motivated by the need to more efficiently deal with non-responsive flows that ignore congestion indications.

Interestingly, both implementations are found in today's Internet. For example, Linux machines, which we use in our testbed experiments in Section 7, mark all packets by default when the average queue length exceeds `maxth`. Some other vendors follow the RFC 3168 recommendation more closely, according to their publicly available specifications. The issue of marking vs. dropping packets beyond `maxth` significantly impacts system performance, and we evaluate both versions below.

#### Random Exponential Marking (REM) and Proportional Integrator (PI)

Both REM and PI apply control-theoretic principles to decide which packets to drop or mark. These schemes measure the difference between the targeted and measured queue lengths and adjust the marking or dropping probability according to a specific control function. The key parameters are:
- For PI: `qref` (queue reference)
- For REM: `b*` (target queue length)

These parameters are set to achieve a targeted queuing delay. For instance, in our experiments, we set `qref` and `b*` to 62 kBytes, corresponding to a 5 ms targeted queuing delay on a 100 Mbps link.

### Experimental Methodology

We conducted large-scale ns-2 simulations using the model developed in [11] and combined it with the empirical file-size distribution reported in [33]. In this model, clients initiate sessions from randomly chosen web sites, downloading multiple web pages, each containing several objects requiring TCP connections for delivery. We explore the effects of persistent HTTP connections in Section 7.

- **Inter-page time distribution**: Pareto
- **Web file sizes**: Fitted to a heavy-tailed distribution, with a mean file size of 7.2 kBytes and a significant portion of large files (top 15% of object sizes account for 80% of the bytes sent by servers).

This combination of heavy-tailed user "think times" and file-size distribution creates long-range dependent (LRD) traffic with a Hurst parameter between 0.8 and 0.9. Flow round-trip times are uniformly distributed between 10 ms and 150 ms.

Our simulation scenario consists of a web-client and a web-server pool interconnected by a pair of routers and a bottleneck link. Each node from the client pool connects to a router R1 with a 1 Gbps link, and each node from the server pool connects to another router, R2, via a 1 Gbps link. The capacity of the link between R1 and R2 is varied from 100 Mbps to 1 Gbps.

We proceed in two steps:
1. **Uncongested Environment**: Set the capacity between R1 and R2 to 1 Gbps and vary the number of active web sessions to place a nominal offered load on an uncongested link. Offered loads range from 80 Mbps to 105 Mbps. Web response times in this environment represent ideal system behavior.
2. **Congested Environment**: Reduce the R1-R2 capacity to 100 Mbps and re-run the web-request traces to evaluate the performance of various AQM schemes. We explore congestion levels where utilization exceeds 80%, specifically at 90 Mbps and 105 Mbps. We refer to the 90 Mbps load on a 100 Mbps link as the lightly congested scenario and the 105 Mbps case as the persistently congested scenario.

### Performance Measures

- **End-to-end response times** for each request/response pair
- **Throughput** on the bottleneck link

For a given file, the response time is computed from the moment the first request is sent to the server until the file is successfully downloaded by the client. We report the cumulative distribution function (CDF) of response times up to 2 seconds.

### Response Times

#### RED and RED*

- **RED**: Drops all packets when the average queue length exceeds `maxth`.
- **RED***: Marks all packets when the average queue length exceeds `maxth`.

**Figure 2** depicts the CDF response-time profiles for RED without ECN, with ECN, and with ECN+ under 90% and 105% loads. As expected, the uncongested network scenario (1 Gbps link between R1 and R2) has the best response-time profile. For any given scheme, the profile for 90% load is better than for 105% due to less persistent congestion. ECN alone provides a small improvement over the non-ECN scenario, but ECN+ significantly improves performance by avoiding unnecessary timeouts.

**Figure 3** shows the performance of RED* in the same scenarios. The most notable result is the significant degradation of response times in scenarios with ECN, where only about 30% of flows have response times less than 0.5 sec for 90% load. This is due to the "TCP admission problem," where SYN ACK packets are frequently dropped, leading to more frequent timeouts. ECN+ solves this by marking SYN ACK packets, thus improving performance.

#### REM and PI

**Figures 4 and 5** show the impact of ECN+ on REM and PI in the same scenarios. The key insight is the low performance of REM without ECN support. However, ECN alone can significantly improve REM’s performance, and ECN+ has a variable impact. In the 90% load scenario, ECN+ marginally improves REM’s performance, indicating conservative marking in lightly congested scenarios. In the 105% load scenario, the benefits of ECN+ are more pronounced, maintaining appropriate delay characteristics.

**Figure 5** depicts the CDF response-time profiles with PI, showing similar trends. Dropping SYN ACK packets on persistently congested links significantly degrades system performance, and ensuring these packets are marked prevents such degradation.