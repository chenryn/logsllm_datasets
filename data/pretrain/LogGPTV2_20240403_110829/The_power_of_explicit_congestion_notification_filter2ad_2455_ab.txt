Detection (RED) [15], Random Exponential Marking (REM) [8],
and Proportional Integrator (PI) [19].
RED uses a weighted-average queue size as a measure of conges-
tion, and the drop (mark) rate depends on minimum and maximum
threshold parameters (denoted as minth and maxth, respectively),
as follows: when the weighted average is smaller than minth, no
packets are marked or dropped. When the average queue length is
between minth and maxth, the probability of marking or dropping
packets varies linearly between 0 and a maximum drop probability
(typically set to 0.1). If the average queue length exceeds maxth,
all packets are marked or dropped. Reference [15] deﬁnes further
reﬁnements of the scheme.
Of particular importance is the way RED behaves when the av-
erage queue length exceeds maxth. The original RED paper [16]
recommends marking these packets when ECN is enabled, while
RFC 3168 [31] recommends dropping packets in these scenarios,
even if they are ECN-enabled. The latter rule, which we analyze
in more detail below, is motivated by a need to more efﬁciently
deal with non-responsive ﬂows that ignore congestion indications.
Interestingly, we discover that both of the above implementations
are represented in today’s Internet. For example, Linux machines,
which we use in our testbed experiments in Section 7, mark, by de-
fault, all packets when the average queue length exceeds maxth.
Some other vendors follow the RFC 3168 recommendation more
closely, at least according to the publicly available speciﬁcations of
their equipment.3 Because the issue of marking vs. dropping pack-
ets beyond maxth impacts the system performance in a non-trivial
manner, we evaluate both versions below.
REM and PI apply control theoretic principles when deciding
which packets to drop or mark. Both schemes measure the dif-
ference between the targeted and measured queue lengths, and in-
crease or decrease the marking or dropping probability according
to a particular control function (see references [8, 19] for details).
The parameters used to set the control algorithm’s targeted objec-
tive are queue reference (qref ) in PI’s case, and target queue length
(b∗) in REM’s.
4.2 Experimental Methodology
Next, we conduct a large number of large-scale ns-2 simulations.
We adopt the model developed in [11], and combine it with the em-
pirical ﬁle-size distribution reported in [33]. In this model, clients
initiate sessions from randomly chosen web sites with several web
pages downloaded from each web site. Each web page contains
several objects, each of which requires a TCP connection for de-
livery. We explore the effects of persistent HTTP connections in
Section 7. The inter-page time distribution is Pareto, while we gen-
erate web ﬁle sizes by ﬁtting the empirically-measured heavy-tailed
distribution reported in [23, 33]. While the majority of the ﬂows
are very short, such that the mean ﬁle size is 7.2 kBytes, Gbytes ﬁle
sizes will also be generated such that the top 15% of object sizes ap-
proximately accounts for 80% of the bytes sent by servers [33]. Ac-
cording to [23], the combination of heavy-tailed user “think times”
3Detailed information about implementation and deployment of
ECN is available at http://www.icir.org/ﬂoyd/ecn.html.
and the above ﬁle-size distribution creates long-range dependant
(LRD) trafﬁc with a Hurst parameter between 0.8 and 0.9. We uni-
formly distribute the ﬂow round-trip times in the range from 10 ms
to 150 ms.
Our simulation scenario consists of a web-client and a web-server
pool that are interconnected by a pair of routers and a bottleneck
link. Each node from the client pool connects to a router R1 with
a 1 Gbps link; likewise, each node from the server pool connects
to another router, R2, via a 1 Gbps link. Nodes R1 and R2 are
connected by a link whose capacity we change from 100 Mbps to
1 Gbps. We adopt the experimental method of [23], and proceed in
two steps. First, we set the capacity between R1 and R2 to 1 Gbps,
and vary the number of active web sessions in the system. In this
way, we place a nominal offered load on an uncongested link, in the
direction from R2 to R1. We generate offered loads in the range
from 80 Mbps to 105 Mbps, and we explain the reasons for such
a choice below. The web response times measured in this uncon-
gested environment represent the ideal system behavior, which we
later use to evaluate the performance of various AQM schemes in
congested environments.
Second, we reduce the R1-R2 capacity to 100 Mbps and re-run
the above web-request traces with the goal to evaluate the per-
formance of a particular AQM scheme implemented at R2. As
experimentally evaluated in [23], and as we analytically show in
Section 5, AQM schemes impact performance when the utiliza-
tion exceeds 80%; hence, we explore such congestion levels. Due
to space constraints, we report the results only for 90 Mbps and
105 Mbps. In the rest of the paper, we refer to the 90 Mbps load on
a 100 Mbps link as the lightly congested scenario, whereas we refer
to the 105 Mbps case as the persistently congested scenario.
We set the AQM parameters as follows. For RED, we set the
RED’s targeted delay parameter to 5 ms, and let the algorithm from
[15] automatically set all other parameters. For REM and PI, we
set b∗ and qref to 62 kBytes, which corresponds to the same tar-
geted queuing delay of 5 ms, on a 100 Mbps link. The performance
measures of interest are end-to-end response times for each re-
quest/response pair, and throughput on the bottleneck link. For a
given ﬁle, we compute its response time from the moment when
the ﬁrst request for the ﬁle is sent to the server, until that ﬁle is
successfully downloaded by the client. We report the cumulative
distribution function (CDF), F (x) = P r[X ≤ x], of response
times up to 2 seconds.
4.3 Response Times
4.3.1 RED and RED*
Here, we evaluate the impact of ECN+ on two versions of RED.
The ﬁrst is the version in which all packets are dropped when the
average queue length exceeds maxth, which we denote below as
RED. The second version is the one in which all packets are marked
in such scenarios, which we denote as RED∗.
Figure 2 depicts the CDF response-time proﬁles for RED without
ECN, with ECN, and with ECN+, when the offered load is set to
90% and 105%. As expected, the uncongested network scenario
(1 Gbps link between R1 and R2) has the best response-time proﬁle,
since the percentage of successfully-transmitted ﬁles (the y-axis in
Figure 2) is the largest for all given response times (the x-axis in
the ﬁgure). Another expected result is that for any given scheme,
the proﬁle for 90% load is better than the corresponding proﬁle
for 105% simply because the congestion is more persistent in the
latter scenario. Finally, as previously reported in [23], ECN alone
provides a small improvement to the non-ECN scenario. Below, we
argue that this is a direct consequence of RFC 3186’s rule to drop
)
%
(
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
Uncongested network
RED, no ECN, 90load
RED, with ECN, 90load
RED, with ECN+, 90load
RED, no ECN, 105load
RED, with ECN, 105load
RED, with ECN+, 105load
 0
 0.2  0.4  0.6  0.8
 1
 1.2  1.4  1.6  1.8
 2
Response Time (sec)
Figure 2: RED performance
all packets when the average queue length exceeds maxth.
The key insights from Figure 2 are the following. First, note that
ECN+ indeed signiﬁcantly improves the performance of RED. This
is because the SYN ACK packets are marked in the ECN+ case,
and not dropped, as in the ECN case. Thus, a number of unnec-
essary timeouts are avoided, and the performance improvements
are evident in the ﬁgure, both for 90% and 105% loads. How-
ever, because all packets, including SYN ACKs, are dropped when
the RED’s average queue length exceeds maxth, this signiﬁcantly
worsens the RED’s response-time proﬁle.
RFC 3168 motivates this rule with a need to more efﬁciently
deal with non-responsive ﬂows that are ignoring congestion indica-
tions. However, dropping all packets beyond maxth cannot protect
against non-responsive ﬂows. Instead, it can actually aid a poten-
tially malicious user. This is because the proposed rule degrades
all ﬂows that share the bottleneck, not just the non-responsive ones.
More sophisticated mechanisms, such as the one proposed in [25],
are needed to ﬁrst detect non-responsive ﬂows, and then drop pack-
ets exclusively from these ﬂows.
)
%
(
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
Uncongested network
RED*, no ECN, 90load
RED*, with ECN, 90load
RED*, with ECN+, 90load
RED*, no ECN, 105load
RED*, with ECN, 105load
RED*, with ECN+, 105load
 0
 0.2  0.4  0.6  0.8
 1
 1.2  1.4  1.6  1.8
 2
Response Time (sec)
Figure 3: RED∗ performance
Figure 3 depicts the performance of RED∗ in the same simula-
tion scenarios as above. The most stunning result is certainly the
huge degradation of response times in scenarios with ECN (when
TCP data packets are ECN-capable, but SYN ACKs are not). For
example, for 90% load, the ﬁgure shows that approximately only
30% of the ﬂows have response times less than 0.5 sec. This is a
signiﬁcant degradation from the scenario in which ECN is not used,
where nearly 75% ﬂows have response times less than 0.5 sec. This
is due to the “TCP admission problem” discussed above; we pro-
vide additional insights below.
The only difference between the RED and RED∗ schemes is the
way in which packets are treated when the average queue length
exceeds maxth: they are dropped by RED, and marked by RED∗.
Because data packets are marked by RED∗, TCP’s end-point con-
trol becomes less responsive [12], and RED∗’s operating point (av-
erage queuing length) moves closer to the upper threshold maxth.
While this can increase the throughput of ECN-enabled data pack-
ets, it can have a devastating effect on non-ECN-enabled SYN ACK
packets that are being frequently generated by web servers in re-
sponse to client’s TCP SYN packets. Because SYN ACK pack-
ets are now much more frequently dropped, the timeout penalty
is invoked more often, and the degradation becomes huge. ECN+
solves this problem because web servers in this scenario send ECN-
enabled SYN ACK packets that are marked by the congested router.
Thus, ECN+ avoids the above degradation, and Figure 3 shows
that it signiﬁcantly improves system performance when compared
to the scenario without ECN. Moreover, in the 90% load scenario,
RED∗’s proﬁle with ECN+ comes very close to the idealized un-
congested proﬁle.
4.3.2 REM and PI
)
%
(
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
a
u
m
u
C
l
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
Uncongested network
REM, no ECN, 90load
REM, with ECN, 90load
REM, with ECN+, 90load
REM, no ECN, 105load
REM, with ECN, 105load
REM, with ECN+, 105load
 0
 0.2  0.4  0.6  0.8
 1
 1.2  1.4  1.6  1.8
 2
Response Time (sec)
Figure 4: REM performance
Figures 4 and 5 show the impact of ECN+ on REM and PI,
in repeated scenarios from above. The key insight from Figure
4 is very low performance of REM without ECN support. How-
ever, note that ECN alone can signiﬁcantly improve REM’s per-
formance, while the addition of ECN+ has variable impact.
In
the 90% load scenario, ECN+ only marginally improves REM’s
performance with ECN, which indicates that REM’s marking is
quite conservative in lightly congested scenarios. We analyze such
scenarios in more depth in the following section. However, in
the 105% load scenario, the beneﬁts of ECN+ become more pro-
nounced, and the appropriate delay characteristic remains almost
the same as when the congestion is not as persistent. Generally,
when the level of congestion increases, the beneﬁts of ECN+ are
more pronounced. This result systematically holds for all schemes
explored in this paper. The key reason for this is that dropping SYN
ACK packets on persistently congested links can signiﬁcantly de-
grade system performance; therefore, ensuring that those packets
are marked prevents the above degradation.
Figure 5 depicts the CDF response-time proﬁles with PI. While
)
%
(
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
 100
 90