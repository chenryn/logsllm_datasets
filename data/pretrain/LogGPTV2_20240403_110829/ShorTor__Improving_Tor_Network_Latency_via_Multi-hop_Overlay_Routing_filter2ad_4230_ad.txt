conducted to identify faster routes for trafﬁc, not for stability
purposes. These backoff and capacity parameters simply ensure
that races avoid oscillations between vias.
Circuits never attempt to send trafﬁc through a via without
ﬁrst conducting a race (though they may fall back to their direct
path at any point). First, vias without available capacity will
drop data race cells, preventing them from being selected at the
small cost of processing a single packet. Second, upon being
dropped from a via path, circuit relays will apply randomized
exponential backoff and will not include that via in data
races again until a set period of time has passed. The exact
parameters for via capacity and backoff timing are network-
dependent and may evolve based on the current state of the Tor
network. However, because ShorTor is an optional performance-
enhancement, these values can initially be set conservatively
and then decreased adaptively.
C. Integration with Tor
While the ShorTor approach has potential applications to
other networks, we designed and evaluated ShorTor’s protocol
to integrate with Tor speciﬁcally. Maintaining Tor’s existing
security guarantees is one main focus of this design and
informed the structure of our data races and avoidance of
loops. However, successful integration with Tor also requires
that ShorTor be deployable. In this section, we discuss the
components of ShorTor that are most relevant Tor deployment,
including support for load balancing and fairness, required
modiﬁcation to Tor relays, and incremental deployment.
1) Load Balancing & Fairness: Fairness to circuit trafﬁc
and load balancing are both necessary to ensure that ShorTor
does not inadvertently increase latency for some circuits as
a consequence of reducing it on others. This could happen
if via trafﬁc was allowed to consume more resources than a
relay had available to spare, resulting in increased processing
times or congestion at the relay. ShorTor provides both fairness
and load balancing through the same mechanism: prioritizing
circuit trafﬁc over via trafﬁc. Tor already recognizes different
trafﬁc priorities—web browsing is prioritized over large ﬁle
downloads [26]. We extend this to ensure that relays will
preferentially schedule trafﬁc from circuit queues over via
queues (Figure 5).
Circuits will select vias that have lower latency than the
default path, including the transit time through the via itself.
This is very important as relay congestion and the associated
queuing delays are a primary source of latency in Tor [43,44,46].
(cid:55)(cid:82)(cid:85)
(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)(cid:3)(cid:37)(cid:88)(cid:73)(cid:73)(cid:72)(cid:85)
(cid:38)(cid:76)(cid:85)(cid:70)(cid:88)(cid:76)(cid:87)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:57)(cid:76)(cid:68)(cid:3)
(cid:52)(cid:88)(cid:72)(cid:88)(cid:72)(cid:86)
(cid:50)(cid:88)(cid:87)(cid:83)(cid:88)(cid:87)(cid:3)(cid:37)(cid:88)(cid:73)(cid:73)(cid:72)(cid:85)
(cid:17441)
(cid:17440)
(cid:17442)
(cid:17443)
(cid:51)
(cid:38)
(cid:55)
(cid:17439)
(cid:51)
(cid:38)
(cid:55)
Fig. 5: Tor relay with circuit and via trafﬁc (queuing architecture
unmodiﬁed from baseline Tor). 1(cid:2) Via and circuit trafﬁc are mul-
tiplexed on a TCP connection entering the Tor relay. 2(cid:2) The TLS
layer is decrypted and circuit cells are onion encrypted/decrypted. 3(cid:2)
Circuit and via cells are sent to their individual queues. 4(cid:2) Cells are
scheduled for release to the output buffer based on priority order. 5(cid:2)
The contents of the output buffer are encrypted using TLS, then sent
to the kernel for transit over a TCP connection to the next hop.
Congestion at a via will appear naturally during the data race
in the form of increased latency or could be indicated explicitly
by dropping race packets. In addition, vias are transitory and
can be dropped or swapped at will with minimal cost compared
to that of circuit construction/teardown.
As such, ShorTor ensures that: (1) circuit trafﬁc on a relay
is never delayed by via trafﬁc and (2) load from via trafﬁc is
distributed only across relays with available capacity.
2) Tor Modiﬁcations: ShorTor’s primary modiﬁcation to
Tor is the introduction of data races, all other components are
simple extensions of Tor’s existing mechanisms for routing and
prioritizing circuits. To support ShorTor, Tor relays (though not
clients) require additional protocol messages, a new data path
for via trafﬁc, and state for managing via trafﬁc. The protocol
requires new cell headers for data races, ping, and via trafﬁc.
Speciﬁcally, via trafﬁc needs a new priority level lower than
circuit trafﬁc (optionally, this level can be higher than that of
bulk download trafﬁc, such as torrenting, which is currently
of lower priority than circuit trafﬁc) [26]. Incoming via trafﬁc
needs a new data path that bypasses onion encryption and
decryption. Relays must also handle ping and data race trafﬁc
as speciﬁed in Protocol 2 and Protocol 3. Finally, relays must
hold two additional pieces of state: ﬁrst, a new ﬁeld in the
routing table to indicate the via (if any) for each circuit; second,
the list of candidate via nodes for each possible next hop (see
Section III-B2 for details).
These modiﬁcations are relatively minor, do not touch Tor’s
onion encryption layer, and represent an optional overlay on
Tor’s routing. We discuss more details of required modiﬁcations
in Appendix B, but note here that the high up-front cost of
integrating and deploying modiﬁcations to the Tor protocol
was a large factor in the ultimate design of ShorTor. This
consideration motivated ShorTor’s construction as an extension
to Tor’s existing architecture that operates largely separately
from the baseline protocol.
Furthermore, ShorTor’s modiﬁcations are conﬁgurable, triv-
ially backwards compatible,2 and support incremental deploy-
2Relays lacking support for ShorTor simply route as usual without any vias.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:07 UTC from IEEE Xplore.  Restrictions apply. 
71939
ment. This allows relay operators to choose whether to support
ShorTor and how much capacity to dedicate to the protocol. As
shown in Section IV-C3, ShorTor can substantially reduce tail
latencies even with relatively low support. This is important,
as it minimizes the risk of up front development efforts being
wasted due to slow deployment.
3) Incremental Deployment: Tor relays are volunteer run
and notoriously slow to update [58]. As such, any proposal
that requires support of all—or even a majority of—Tor relays
is unlikely to be effective. ShorTor is incrementally deployable
and improves the latency of any Tor circuits that meet the
following two requirements: (1) two adjacent circuit relays
support ShorTor and (2) some other relay supporting ShorTor
provides a faster path between the two circuit relays. Because
Tor does not select its relays with uniform probability, a small
set of popular relays could meet these conditions for many
circuits without support from the rest of the network. We
demonstrate this concretely in Section IV-C3.
Security is another important consideration—incremental
deployment inherently creates differences between Tor clients
or relays that have adopted a modiﬁcation and those who
have not. This has been an issue for client side proposals as
anonymity in Tor relies on all clients behaving uniformly [12,
14,36,60,61,74,85].
ShorTor avoids this issue entirely as it is a fully server-side
protocol that does not require participation from, or modify
the behavior of, Tor clients in any way. So, while ShorTor is
an observable modiﬁcation to the Tor protocol,3 it is in no
way correlated to client identity. As such, support for ShorTor
cannot be used to distinguish between clients. In fact, Tor
clients should not try to preferentially select relays with support
for ShorTor. While this would improve their latency, it would
also differentiate them from Tor clients following Tor’s baseline
circuit selection algorithm, reducing their anonymity.
IV. EVALUATION
We evaluate ShorTor using a dataset of approximately
400,000 latency measurements we collected from the live Tor
network over the course of 42 days during summer 2021. Our
measurements allow us to compare the direct latency between
relays to the latency when routing through an intermediate hop,
as in ShorTor.
Using measured latencies allows us to avoid relying on
simulated or approximate data. While simulations can be a
useful tool, prior work [75] has shown that routing protocols
are best evaluated using live internet paths rather than through
a simulation with, necessarily, reduced scale and complexity.
We evaluate the performance of ShorTor in terms of its direct
impact on the latency between pairs of Tor relays as well as
its ability to reduce the latency of Tor circuits. Evaluating on
circuits as well as pairs allows us to account for the relative
popularity of relays and more closely model the expected
reduction in latency ShorTor can provide to Tor’s end users.
3Both adversarial relays and network adversaries can likely detect when
trafﬁc is routed using ShorTor as opposed to baseline Tor.
A. Measurement Methodology
1) Ting: For our measurements, we adapt the Ting method
of Cangialosi et al. [23] for estimating latencies between
Tor relays. Ting creates a set of three circuits involving
observers, which are Tor relays run solely for the purpose
of obtaining latency measurements. Speciﬁcally, to obtain the
latency between two Tor relays, RelayA and RelayB, we run
two observer relays Obs1 and Obs2 along with a measurement
client on the same physical machine. Once each circuit is
established, the measurement client “pings” itself through the
circuit to estimate round-trip latencies for the following circuits:
1) rttAB = RTT (Obs1 → RelayA → RelayB → Obs2)
2) rttA = RTT (Obs1 → RelayA → Obs2)
3) rttB = RTT (Obs1 → RelayB → Obs2)
With these, we estimate the round-trip time between RelayA
and RelayB as rttAB − 1
2 (rttA + rttB). This approximates
the RTT including the forwarding delay at both RelayA and
RelayB as forwarding is inherently a component of the latency
experienced by via trafﬁc. We repeat this process in order to ﬁnd
the minimum observed latency between RelayA and RelayB:
in our observations, after 10 iterations, 95.5 % of circuits are
within 5 % or 5 ms of the minimum observed in 100 samples.
2) Directional Latencies: The Ting protocol does not
account for directional latencies where the outgoing latency
between two nodes may not be equivalent to that of the
return trip. Speciﬁcally, the method for computing rttAB
described above assumes that latency (Obs1 → RelayX ) ≈
latency (RelayX → Obs2). To detect asymmetry in our RTT
measurements we include a timestamp in our measurements
halfway through the round-trip “ping” (all timestamps are with
respect to the same clock). In our dataset (Section IV-A6), the
median asymmetry was 2.4 % and only 0.2 % of measurements
had an asymmetry of 2× or greater. Importantly, asymmetric
RTTs impact only our evaluation as, when deployed, ShorTor
naturally accounts for directional latencies using data races
(Section III-B4).
3) Infrastructure: To collect latency measurements at scale,
we adapted the Ting protocol to support parallel measurements
across multiple machines. Our larger scale also required
changes to respect a safe maximum load on the Tor network
(see Section IV-A5): we impose a global maximum limit on
concurrent measurements and spread measurements of individ-
ual relays across time. Our infrastructure also compensates for
the high churn in the Tor network (13 % of relays we observed
were online less than half the time) by enqueueing measurement
jobs based on the currently online relays, with automated retries.
We handled hardware and power failures using a fault-tolerant
system design: we separated data persistence, measurement
planning, and the measurements themselves.
We deployed to a private OpenStack [78] cloud, but provide
a Terraform [41] template supporting any provider. Our open-
source [1] measurement infrastructure is approximately 3,300
lines of code, consisting primarily of Python and shell scripts.
4) Geographic Location of Relays: We obtain country codes
for the relays in our dataset using the GeoIP database [59],
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:07 UTC from IEEE Xplore.  Restrictions apply. 
81940
Fig. 6: (a) Left: Round-trip times (RTTs) measured between Tor relays vs. RTTs of those same pairs when using ShorTor as a percentage of
all 406,074 pairs. We show an expanded view of the ﬁrst 200 ms on the left. For readability, this ﬁgure omits 10 pairs with RTTs between
4,019 ms and 9,415 ms. (b) Right: Relative latency of each relay pair when routing via ShorTor vs. the default route. 25.4 % of pairs have a
latency reduction of at least 50 %, 9.4 % at least 75 %, and 1.2 % at least 90 %.
which is also used by Tor in practice. However, GeoIP locations
are not guaranteed to be 100 % accurate [37,51,66]. Indeed,
upon careful inspection, we observed a handful of relay pairs
with physically impossible RTTs for their purported locations.
All of these pairs involved the same twelve relays allegedly
located in the US. We determined that these twelve relays
have higher average RTTs inside the Americas than they do
to relays located in other regions. Because of this, all location-
related ﬁgures (Figure 6(b), Figure 9, Figure 10) exclude these
relays as, while we are conﬁdent that their reported location is
incorrect, we cannot accurately determine their true location.
5) Ethics and Safety: We designed our measurement process
to minimize impact on Tor users and relay operators and
to comply with security best practices for Tor. To this
end, we submitted a proposal to the Tor Research Safety
Board [69] for review prior to measurement and adhered to
their recommendations. We also received an IRB exemption
from each author’s institution for this work.
Collecting our data required us to run several live Tor relays.
These relays recorded only our measurement trafﬁc—at no
point did we observe or record any information about any
trafﬁc from Tor users. We also minimized the likelihood of a
user choosing our relays for their circuits by advertising the
minimum allowed bandwidth of 80 KiB/s [26].
Our measurement collection was spread over 42 days to
reduce concurrent load, including a limit on simultaneous
measurements (detailed in Section IV-A3). We also notiﬁed a
Tor relay operator mailing list and allowed operators to opt
out of our measurements; we excluded four such relays.
In light of recent work by Schnitzler et al. [76] on the
security implications of ﬁne-grained latency measurements for
Tor circuits, we have not published our full latency dataset.
However, we will share this data with researchers upon request
and are in communication with our reviewers from the Tor
Research Safety Board about safely releasing it in the future.
6) Latency Dataset: In this work, we directly measure
pairwise latencies within Tor rather than relying on outside
estimates. We focus our measurements on the 1,000 most
popular Tor relays (by consensus weight) for two main reasons:
1) Measuring all 36,325,026 possible pairs of the 8,524 Tor
relays we observed was intractable for this work.
2) These popular relays are present on over 75 % of cir-
cuits [39] and thus can provide disproportionate utility.
Our dataset contains 406,074 measured latencies or 81.3 % of
all pairs of the 1,000 most popular relays.4
B. Applying ShorTor to Relay Pairs
We begin evaluating ShorTor by comparing the potential
latency between our set of relay pairs when routing via ShorTor
to our measured latencies observed using Tor’s default routing.
Figure 6(a) shows the relative frequency of RTTs experienced
by pairs of Tor relays using ShorTor and when routing normally
while Figure 6(b) focuses on the relationship between default
RTT and ShorTor RTT for each relay pair. Using ShorTor,
all of Tor’s high tail latencies were resolved: ShorTor sees a
maximum absolute RTT of 157 ms, while 0.09 % of pairs in