The security of the Tor browser, a modified version of Mozilla Firefox ESR, was evaluated 
in a research engagement in 2014 [6]. The audit revealed that the security feature ASLR and 
others were not enabled on Windows and Mac due to a non-standard compiler toolchain. 
However, this is nowadays fixed. 
Although browsers support modern memory mitigation protections and exploitation of 
vulnerabilities is a complex venture, exploitability could still be demonstrated in various 
exploitation competitions and in real world. This indicates that current in-place protections 
are not enough to withstand exploitation attempts from experienced research teams or APT 
groups. Further research in this area is required to develop new techniques that discover 
vulnerabilities more efficiently. Research in the field of web browser security is therefore of 
significant importance.  
Modern web browsers are composed of the following main components: 
• 
Browser engine / HTML render engine 
The browser engine is the core of the web browser and is responsible to render the 
HTML code, manage the DOM and display websites based on defined layouts in CSS. 
Initially, Google Chrome used the WebKit engine up to version 27. WebKit is also 
currently used by Apple Safari. Nowadays Google Chrome uses the Blink engine, a fork 
of WebKit. At the end of 2018 Microsoft announced that Microsoft Edge is going to switch 
from EdgeHTML to Chromium and therefore to Blink. Mozilla Firefox uses the Gecko 
engine and Microsoft Internet Explorer the Trident engine. Opera used up to version 
12.18 the Presto engine and then switched to Blink. 
• 
JavaScript engine 
The JavaScript engine is responsible to interpret JavaScript code as well as compiling 
frequently used code with a Just-in-time (JIT) engine. Google Chrome uses the v8 
engine, Mozilla Firefox the SpiderMonkey engine and Apple Safari uses JavaScriptCore 
13 https://news.sophos.com/en-us/2019/04/18/protected-cve-2018-18500-heap-write-after-free-in-firefox-
analysis-and-exploitation/ 
10 
(JSC). Microsoft Internet Explorer initially used the JScript engine (IE 1-8) and later 
switched to Chakra (IE-11). Chakra was also used by Microsoft Edge until Microsoft 
announced a change to Chromium and therefore to v8. 
• 
Sandbox 
Modern browsers support as additional layer of defense the sandboxing concept. Code 
responsible for rendering a website and parsing file formats runs inside an exposed but 
sandboxed process with limited access to the file system and the operating system. A 
vulnerability in this code therefore does not immediately lead to a full system 
compromise. An attacker would need to find additional vulnerabilities to compromise the 
main browser process. A possible attack is to not target the operating system and 
therefore to avoid the sandbox escape. Instead, code execution in the sandboxed 
process can be used to disable security protections such as the Same-Origin-Policy 
(SOP) as demonstrated by Burnett [7]. This leads to Universal Cross-Site-Scripting 
(UXSS). This attack is mitigated in Google Chrome with the security feature site-isolation 
14. Site isolation ensures that every opened website is running in a separated process. 
However, this has two side effects. First, it allows to make unreliable exploits reliable by 
using a bruteforce approach. The exploit can be loaded inside an iframe and therefore 
inside a separated process. If the exploit fails and crashes, it can be restarted until 
exploitation works which was documented by Exodus Intelligence 15 in 2019. Second, by 
separating two websites in two processes, Spectre-like hardware attacks are mitigated 
which means that traditional Spectre mitigations, which impeded exploitation in some 
cases, are disabled. These mitigations must therefore not be bypassed by attackers. 
14 https://www.chromium.org/Home/chromium-security/site-isolation 
15 https://blog.exodusintel.com/2019/01/22/exploiting-the-magellan-bug-on-64-bit-chrome-desktop/ 
11 
1.1 Google Chrome 
Google Chrome is considered by many researchers to be the most secure web browser [8]. 
In 2017 Google financed two independent research projects [4] [5] which analyzed the 
security of Google Chrome, Microsoft Edge and Microsoft Internet Explorer. The following 
cites summarize the research results: 
• 
"X41 D-Sec GmbH found that security restrictions are best enforced in Google Chrome 
and that the level of compartmentalization is higher than in Microsoft Edge." [4] 
• 
"We consider the level of sandboxing in Google Chrome to be the most restrictive and 
most secure." [4] 
• 
"The browser [Chrome] is very mature in the realm of memory safety. As it comes with a 
sophisticated process architecture with strong focus on separation of duty, it also tries to 
push forward in quickly adopting all sorts of mitigation mechanisms that modern 
operating systems like Windows 10 can offer. This includes CFG, font-loading policies 
and image-load restrictions. Its different integrity levels paired with the least amount of 
trust for processes that handle user-input, Chrome provides a very restrictive sandbox 
where even Win32k syscalls are disallowed." [5] 
The security of Chrome is also apparent by the fact that only a few teams successfully 
compromised the browser during exploitation competitions like Pwn2Own. In 2017 one team 
attempted to attack Google Chrome but failed. In 2018 a hack of Google Chrome was not 
attempted by any team at all. In 2019 Chrome was not attacked in its main category but 
Chromium was successfully hacked as part of a Tesla car hack. In 2020 again no team 
attempted to attack the browser. Payouts from exploit brokers like Zerodium are also highest 
for Google Chrome. 
Chrome is a modified version of Chromium which itself is a standalone browser. Chrome has 
additional support for audio and video formats, includes an update service and additional 
components like error reporting. The code of Chrome is not open source, but the code of 
Chromium is publicly available. Chromium uses the Blink render engine together the 
JavaScript engine v8. The v8 engine is written in C++ which allows to compile JavaScript 
code to fast machine code. The interpreter is called Ignition and the JIT compiler is named 
TurboFan. Chrome previously used a baseline compiler and for optimization the Crankshaft 
compiler. The baseline compiler was replaced by an interpreter to reduce the heap usage 
and Crankshaft was replaced by TurboFan to achieve a more stable performance.  
Chromium uses four different memory allocators, namely Oilpan, PartitionAlloc, Discardable 
Memory and the default malloc implementation. PartitionAlloc implements strong security 
measures to prevent exploitation like guard pages, double free detection, no inline meta data 
and separation of useful objects like strings and arrays from other objects. In 2015 Blink 
switched from PartitionAlloc to Oilpan for several objects like DOM objects. Oilpan 
implements a mark-and-sweep garbage collection which reduces the number of use-after-
free vulnerabilities. However, Oilpan was initially shipped without common heap mitigations 
12 
[9] which simplified the exploitation of heap overflows. The v8 JavaScript engine uses a 
garbage collector named Orinoco 16. Chromium supports sandboxing its processes based 
on the Windows security model. 
Chromium is, in addition to Chrome, used by a lot of other projects including the Brave, 
Opera and Steam browsers. Moreover, it is used for multimedia presentation in Tesla cars. 
In 2020 Microsoft switched to a Chromium based Edge browser 17. The v8 JavaScript engine 
is also used by Foxit Reader and Node.JS which means vulnerabilities in v8 affect an even 
bigger user base. 
1.2 Mozilla Firefox 
Until the end of 2011, Mozilla Firefox was used by around 30 percent of website visitors and 
was then superseded by Google Chrome. Nowadays, Google Chrome holds over 58.7 
percent of market share and Firefox is at 6.3 percent 18. It is still an attractive attack target 
for governances and APT groups because the Tor browser is built on top of Firefox ESR.  
Mozilla Firefox uses the Gecko render engine and the SpiderMonkey JavaScript engine. 
SpiderMonkey is also used by Adobe Reader and therefore an additional attractive target. 
The JIT compiler of SpiderMonkey is named IonMonkey. 
Firefox uses the jemalloc memory allocator, which is more prone to attacks than 
PartitionAlloc. Mozilla Firefox is shipped with a sandbox which isolates its processes 19 20. 
16 https://v8.dev/blog/trash-talk 
17 https://blogs.windows.com/msedgedev/2020/01/15/upgrading-new-microsoft-edge-79-chromium/ 
18 https://www.w3counter.com/globalstats.php 
19 https://wiki.mozilla.org/Security/Sandbox 
20 https://mozilla.github.io/firefox-browser-architecture/text/0012-process-isolation-in-firefox.html 
13 
2 Thesis goal 
The goal of the thesis is to propose improvements in the field of browser exploitation 
research. The goal can be summarized in the following research questions: 
Research question 1: Can previously reported exploitable browser vulnerabilities be further 
divided into browser-specific vulnerability classes? If yes, what can be learned from these 
classes and how can this knowledge be applied to improve current state-of-the-art 
techniques to identify exploitable vulnerabilities? 
Research question 2: With the knowledge of the identified vulnerability classes, how can the 
fuzzing search space be narrowed down to focus mainly on exploitable vulnerabilities? 
The questions will be answered by analyzing exploitable vulnerabilities that were discovered 
during the last six years in two major browsers – Google Chrome and Mozilla Firefox. These 
browsers where chosen because their source code is publicly available, their HTML and 
JavaScript engines are different, and they have a big market share. Vulnerabilities in them 
therefore pose an enormous impact. A special focus will be laid on in-the-wild exploited 
vulnerabilities and on exploits from competitions like Pwn2Own. Vulnerabilities in other 
browsers, which were also actively exploited, will also be considered.  
The fundament of the research is the assumption that most vulnerabilities share similar 
building blocks and follow the same code structure. Current state-of-the-art fuzzers either 
apply mutations on existing inputs or generate random code using grammars. This leads to 
a huge search space and corner cases, which trigger vulnerabilities, are rarely generated. 
In this work building blocks, code patterns and the structure of different vulnerability classes 
from recently exploitable vulnerabilities are extracted and implemented in a fuzzer. This 
knowledge is used to improve variation analysis in fuzzing. A state-of-the-art fuzzer is 
modified and newly developed to create inputs according to the identified patterns. It is 
assumed that this method narrows down the search space and variations of already 
identified exploitable vulnerabilities can be found more efficiently. 
Target audience: 
The target audience of this work are experienced reverse engineers and browser security 
researchers. It is assumed that the reader is familiar with the basic concepts of memory 
corruption vulnerabilities, exploitation techniques to bypass state-of-the-art protections and 
the design, architecture and internals of modern browsers. Background knowledge on 
Chromium and v8, the mainly discussed browser and JavaScript engine, is available at 21. 
Deep knowledge of JavaScript is expected. Knowledge in compiler construction is useful but 
not necessarily required. 
21 https://zon8.re/posts/v8-chrome-architecture-reading-list-for-vulnerability-researchers/ 
14 
3 Previous work 
This chapter discusses current state-of-the-art techniques used to identify browser 
vulnerabilities. Since the focus of this work is improving fuzzing techniques, strategies such 
as variation analysis using source code review are not covered. 
Fuzzing is one of the most used techniques to unveil vulnerabilities in browsers, JavaScript 
engines and software components in general [10]. This resulted in extensive fuzzing 
research in the last decade [11]. Fuzzing can be categorized in mutation-based and 
generation-based fuzzing.  
In mutation-based fuzzing valid inputs are mutated to trigger bugs. This fuzzing technique is 
often combined with a feedback mechanism that obtains code or edge coverage. Fuzzing 
starts with a small set of input files, the input corpus, and performs mutations on these files. 
Coverage information is extracted during execution and if a new input yields more coverage, 
it is added to the corpus. While code coverage tries to maximize the number of executed 
instructions, edge coverage tries to maximize the number of different execution paths a 
program takes. The overall goal during fuzzing is to maximize the number of different 
memory states of the program. Since this information cannot easily be extracted, code and 
edge coverage are used as a heuristic. Coverage information was initially extracted with 
compiler hacks which modified the generated object files. Nowadays compilers such as 
clang support sanitizer coverage, a compiler pass which adds the coverage feedback. 
The most famous fuzzer implementing this type of fuzzing is American Fuzzy Lop (AFL) [12] 
which was developed by Zalewski. AFL already discovered hundreds of vulnerabilities in all 
kinds of applications. It is so successful because of the high execution speed combined with 
excellent heuristics. Over the last years, various academic papers proposed improvements 
for algorithms used by AFL. 
Böhm et al. suggested with AFLFast [13] the use of a Markov chain to enhance the 
algorithms. A further improved version of AFLFast is AFL++ by Heuse et al. [14] which adds 
improvements proposed by e.g. Chenyang et al. [15] and Hsu et al. [16] and is constantly 
expanded. It implements ideas from MOpt-AFL [15], a fuzzer which was published in 2019 
which improves the selection of the mutation strategy. Böhm et al. integrated with AFLGo 
[17] a simulated annealing-based power schedule algorithm to increase the ability of the 
fuzzer to reach program locations more efficiently. In 2018 Gan et al.  proposed with CollAFL 
[18] a better feedback mechanism to avoid path collisions and new fuzzing strategies. 
Initially, open source projects such as binutils were used to evaluate the performance of 
these new fuzzers. However, these projects cannot be used to measure miss or false alarm 
rates. The LAVA dataset [19] fills this gap by providing a dataset of real-world applications 
with injected vulnerabilities. This project was later extended to Rode0day [20] – a bug-finding 
competition. Another often used dataset is the DARPA Cyber Grand Challenge dataset [21], 
a dataset from a competition for automatic vulnerability discovery, exploitation and patching.  
15 
In 2020 Google announced FuzzBench. “FuzzBench is a free service that evaluates fuzzers 
on a wide variety of real-world benchmarks, at Google scale. The goal of FuzzBench is to 
make it painless to rigorously evaluate fuzzing research and make fuzzing research easier 
for the community to adopt.” 22 Figure 1 shows the result of a sample report comparing 
frequently used fuzzers. Fuzzers with a lower score, and therefore further left in the image, 
are considered to perform better. Based on this evaluation the MOpt-AFL fuzzer is together 
with QSYM and AFL++ considered as most efficient. 
Figure 1: Sample report result of FuzzBench; A lower score is better; source: 23 
It must be mentioned that scores can very between multiple runs and a fuzzer can therefore 
just be considered to be better if the score is significantly better than the score of another 
fuzzer. Moreover, FuzzBench currently has other problems which are discussed in-depth by 
Falk in the issue tracker 24. 
The Mayhem [22] system won the DARPA CGC (Cyber Grand Challenge). Mayhem is a 
symbolic execution engine which internally uses bap (Binary Analysis Platform) [23]. BAP 
translates instructions to its own IL (Intermediate Language) and implements a symbolic 
execution engine on top of this IL. Mayhem was combined with the Murphy fuzzer to win the 
competition. 
With symbolic execution symbolic variables are assumed for inputs and instructions are 
analyzed based on these symbolic values. In case of conditional jumps, expressions can be 
built on top of the analyzed instructions. The resulting equations can be solved to calculate 
input constrains which follow both conditional paths. This means coverage can be maximized 
by using a symbolic execution engine to obtain inputs which explore different code paths. 
Symbolic execution engines are designed to systematically and efficiently enumerate all 
paths in an application. 
Symbolic execution engines do not scale to big applications because of the path explosion 
problem. The engine cannot go deep into program logic because it does not know which 
22 https://github.com/google/fuzzbench 
23 https://www.fuzzbench.com/reports/sample/index.html 
24 https://github.com/google/fuzzbench/issues/654 
16 
paths must be followed first. Concolic execution works around this problem by executing the 
application and hints based on the concrete values during the execution which paths should 
initially be followed. It gathers path constrains during execution with respect to the given 
input. After that, the engine can negate one of the collected constrains to calculate an input 
which leads to a different code path. This process is executed repeatedly to increase the 
coverage. 
Symbolic and concolic execution are mainly used in hybrid approaches combined with 
fuzzing. Fuzzing is a lot faster and is therefore the main used technique to find new paths. 
However, fuzzers often get stuck. A common example is a multi-byte magic value check 
which is hard to solve using plain fuzzing. In such a situation the symbolic or concolic engine 
can be started to solve the check, to guide the fuzzer to regions which are harder to reach. 
To solve the formulas the path constrains are passed to a Satisfiability Modulo Theorem 
(SMT) solver. A commonly used SMT solver is Z3 25 which is for example used by the angr 
framework via the claripy abstraction layer, by KLEE [24], S2E [25] and QSYM [26]. 
Other well-known SMT solvers are STP (Simple Theorem Prover) 26, BTOR (Boolector) [27] 
and Yices [28]. The effectiveness of these solvers is compared every year in the SMT-COMP 
27 competition where especially the mentioned engines performed well. 
Team Shellphish placed third in the DARPA CGC. It used a hybrid fuzzing approach with 
Driller [29]. Driller is a combination of AFL and angr [30], a selective concolic execution 
engine. Driller identified the same number of vulnerabilities in the same time as the top-
scoring team of the qualifying event [29]. 
Common problems of fuzzers are magic values, checksums and calculated hash checks. 
One solution to the magic value problem is the use of a symbolic or concolic execution 
engine. However, as already mentioned, these are often slow and do not scale to big 
applications. To solve this problem Ormandy already suggested in 2011 with Flayer [31] a 
method to strip away certain checks based on tainted input data.  
In 2018 this topic was again researched by Payer et al. with T-Fuzz [32]. T-Fuzz uses a 
dynamic tracing-based technique to detect checks which fail with all current inputs. It then 
removes these checks and restarts fuzzing the transformed program which can then reach 
deeper code paths. [32] 
Aschermann et al. proposed in 2018 Redqueen [33], another fuzzer which solves the 
problem of magic values and checksums by using input-to-state correspondence. Redqueen 
outperformed other fuzzers in the September 2018 Rode0day competition. 
25 https://github.com/Z3Prover/z3 
26 https://stp.github.io/ 
27 https://smt-comp.github.io 
17 
Another highly successful fuzzer in Rode0day is AFL-QSYM, a combination of the 
unmodified AFL in version 2.52b and QSYM [26], a concolic execution engine developed by 
Yun et al. QSYM also reached the second-best score in the FuzzBench sample test. 
The usage of an IL is common in symbolic execution engines because it simplifies the 
development of the engine. Most engines use well recognized intermediate languages like 
the LLMR IR (Intermediate Representation) which is used by KLEE and S2E or the VEX IR 
which is used by angr. BAP uses its own IR named the BAP instruction language. QSYM 
uses a different approach. Yun et al. identified as major limiting factor of scaling concolic 
execution to bigger applications the performance bottleneck of the concolic engine. To solve 
this problem, the authors developed an engine which avoids the use of an IR. Since QSYM 
was developed tailored to fuzzing, it does not emulate the target binary like S2E does with 
QEMU or angr does with unicorn, because emulation is sluggish. Instead, it executes code 
directly on the CPU. “Our evaluation results showed that QSYM outperformed Driller in the 
DARPA CGC binaries and VUzzer in the LAVA-M test set. More importantly, QSYM found 
13 previously unknown bugs in the eight non-trivial programs, such as ffmpeg and 