# Title: Automatic Fault Detection and Diagnosis in Complex Software Systems by Information-Theoretic Monitoring

## Authors
- Miao Jiang
- Mohammad Ahmad Munawar
- Thomas Reidemeister
- Paul A. S. Ward

## Abstract
Management metrics of complex software systems exhibit stable correlations, which can be leveraged for fault detection and diagnosis. Current approaches often use specific analytic forms, typically linear, to model these correlations. In this paper, we employ Normalized Mutual Information (NMI) as a similarity measure to identify clusters of correlated metrics without assuming any specific form. We demonstrate the application of the Wilcoxon Rank-Sum test to detect anomalous behavior. We present two diagnostic algorithms to locate faulty components: RatioScore, based on the Jaccard Coefficient, and SigScore, which incorporates knowledge of component dependencies. Our mechanisms are evaluated in the context of a complex enterprise application. Through fault-injection experiments, we show that our approach can detect 17 out of 22 faults with no false positives. Using SigScore, the faulty component is ranked in the top five anomaly scores 40% of the time, which is significantly better than when system structure is ignored.

**Keywords:** self-managing systems, fault detection and diagnosis, information theory, statistical techniques.

## 1. Introduction
Software systems continue to grow in size and complexity as more functionality is implemented and integration increases. Traditionally, these systems are monitored by human operators using basic data analysis tools, which is both costly and not always effective [13]. Therefore, it is essential to find automated and efficient approaches to system monitoring.

The typical approach to automated monitoring involves creating a system model to predict the system's state, behavior, or performance. Predictions from such a model can be compared to observations, and deviations may signal the presence of errors. One important source of data exposed by software systems is management metrics. These metrics are numeric measurements related to the system's state and performance and can be collected periodically.

A promising recent approach to characterizing normal system behavior involves finding and modeling stable relationships between system metrics [4, 10, 15, 16, 17]. These relationships are modeled mathematically and tracked to check system health. The identified relationships between metric pairs are expected to hold during normal operation. However, in the presence of errors, some of these relationship models are expected to produce predictions that deviate from actual observations.

In recent work, we suggested that a similarity measure based on information entropy might capture general inter-metric correlations [12]. This measure captures not only linear but also non-linear correlations without assuming any specific mathematical form. Correlated metrics can be clustered, and the system can be monitored by tracking in-cluster entropy for each cluster. Instead of tracking thousands of pairwise metric-relationship models, only a few clusters need to be tracked, which can be done very efficiently.

However, that work was a preliminary case study with no formal evaluation, and no detection or diagnosis techniques were described. In this paper, we formally describe and evaluate our approach, presenting an automatic anomaly-detection mechanism that employs the Wilcoxon Rank-Sum test. This allows us to detect complex change patterns in in-cluster entropy, providing robust and reliable fault detection. Moreover, we develop two algorithms to identify faulty components based on the cluster-entropy information: RatioScore, which uses the Jaccard coefficient to assign anomaly scores to components, and SigScore, which further adjusts these scores by a measure of the significance of the components.

The contributions of this paper are as follows:
- We use an information-theoretic measure to quantify the strength of relationships between metric pairs without assuming any specific form.
- We use an efficient method based on in-cluster entropy to track system state.
- We apply the Wilcoxon Rank-Sum test to the tracked entropy data for automatic anomaly detection.
- We develop two diagnosis algorithms: RatioScore and SigScore.
- We demonstrate through experiments using a realistic enterprise software system that our detection mechanism is very effective, correctly identifying three-quarters of injected faults with zero false positives. Our SigScore diagnosis technique ranks the faulty component in the top five 40% of the time, which is significantly better than the best prior approach based on the Jaccard coefficient.

The remainder of this paper is organized as follows. We first briefly summarize our prior work in information-theoretic modeling, which forms the basis of this work. In Section 3, we propose a non-parametric statistical test to automatically detect anomalies. In Section 4, we propose two algorithms to locate faulty components, which we then evaluate through fault-injection experiments. We provide an overview of related work in Section 6.

## 2. Information-Theory-Based Monitoring
To make this paper self-contained, we first provide a primer on relevant information-theoretic concepts, followed by a brief overview of our modeling approach.

### 2.1 Information-Entropy Measures
Information entropy, introduced by Shannon [18], measures the uncertainty or unpredictability of a random variable. For a discrete random variable \( X \), the entropy is given by:

\[ H(X) = -\sum_{i=1}^{n} p(x_i) \log p(x_i) \]

where \( X \) takes values from the set \(\{x_1, x_2, \ldots, x_n\}\), and \( E_p \) refers to the expectation with respect to the probability distribution of \( X \) characterized by the density function \( p \).

Conditional entropy measures the uncertainty of a random variable \( Y \) given another random variable \( X \). It represents the remaining uncertainty of \( Y \) knowing the values taken by \( X \). It is defined by:

\[ H(Y|X) = -\sum_{i=1}^{n} \sum_{j=1}^{m} p(x_i, y_j) \log p(y_j | x_i) \]

Mutual information measures the reduction in uncertainty of a random variable \( Y \) given another random variable \( X \). This reduction represents the amount of information one variable provides about the other. It is defined by:

\[ I(X, Y) = H(Y) - H(Y|X) \]

Strehl et al. [19] developed a normalization for mutual information, called Normalized Mutual Information (NMI), to address shortcomings in mutual information. NMI is defined by:

\[ \text{NMI}(X, Y) = \frac{I(X, Y)}{\sqrt{H(X) H(Y)}} \]

For any random variables \( X \) and \( Y \), NMI has the following properties:
1. \( 0 \leq \text{NMI}(X, Y) \leq 1 \)
2. \( \text{NMI}(X, Y) = \text{NMI}(Y, X) \)
3. If \( X \) and \( Y \) are independent, \( \text{NMI}(X, Y) = 0 \)
4. If \( Y = f(X) \), \( \text{NMI}(X, Y) = 1 \) for any invertible function \( f \)

In general, the more correlated two variables are, the higher their NMI, regardless of the specific form of the relationship. Therefore, NMI provides a good measure of the correlation between two variables and can be used as a similarity measure.

### 2.2 Identifying Correlated Metrics
To identify correlated metrics, we need to estimate the entropy measures and determine which level of NMI corresponds to strong correlations. We use samples collected from the target system at a time when it is known to be operating error-free to compute the empirical entropy values and then calculate the NMI.

Given \( n \) observed samples of any metric \( X \), the empirical entropy \( H(X) \) is computed as follows:

\[ H(X) = -\sum_{i=1}^{k} \frac{n_i}{n} \log \frac{n_i}{n} \]

where the observed \( n \) samples are divided into \( k \) bins, and \( n_i \) is the number of samples observed in bin \( i \).

The empirical conditional entropy \( H(Y|X) \) based on observed samples is computed as follows:

\[ H(Y|X) = -\sum_{i=1}^{k} \sum_{j=1}^{m} \frac{n_{ij}}{n_i} \log \frac{n_{ij}}{n_i} \]

where \( n_{ij} \) is the number of samples \((x, y)\) with \( x \) in bin \( i \) and \( y \) in bin \( j \).

With the empirical entropy and empirical conditional entropy, we compute the normalized mutual information using Equations (1) and (2) for all pairs of metrics.

Next, we need a suitable threshold to differentiate weak correlations from strong correlations. However, NMI is a relative measure, and the appropriate threshold can vary depending on the specific system and the desired sensitivity.