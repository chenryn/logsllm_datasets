title:Automatic fault detection and diagnosis in complex software systems
by information-theoretic monitoring
author:Miao Jiang and
Mohammad Ahmad Munawar and
Thomas Reidemeister and
Paul A. S. Ward
978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
285
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:20 UTC from IEEE Xplore.  Restrictions apply. 
AutomaticFaultDetectionandDiagnosisinComplexSoftwareSystemsbyInformation-TheoreticMonitoring∗MiaoJiang,MohammadA.Munawar,ThomasReidemeister,andPaulA.S.Ward{m4jiang,treideme,mamunawa,pasward}@shoshin.uwaterloo.caShoshinDistributedSystemsGroupE&CEDepartment,UniversityofWaterloo,Waterloo,OntarioN2L3G1AbstractManagementmetricsofcomplexsoftwaresystemsex-hibitstablecorrelationswhichcanenablefaultdetectionanddiagnosis.Currentapproachesusespeciﬁcanalyticforms,typicallylinear,formodelingcorrelations.InthispaperweuseNormalizedMutualInformationasasim-ilaritymeasuretoidentifyclustersofcorrelatedmetrics,withoutknowingthespeciﬁcform.WeshowhowwecanapplytheWilcoxonRank-Sumtesttoidentifyanomalousbehaviour.Wepresenttwodiagnosisalgorithmstolocatefaultycomponents:RatioScore,basedontheJaccardCo-efﬁcient,andSigScore,whichincorporatesknowledgeofcomponentdependencies.Weevaluateourmechanismsinthecontextofacomplexenterpriseapplication.Throughfault-injectionexperiments,weshowthatwecandetect17outof22faultswithoutanyfalsepositives.Wediagnosethefaultycomponentinthetopﬁveanomalyscores7timesoutof17usingSigScore,whichis40%betterthanwhensystemstructureisignored.Keywords:self-managingsystems,faultdetectionanddi-agnosis,informationtheory,statisticaltechniques.1IntroductionSoftwaresystemscontinuetogrowinsizeandcomplex-ityasmorefunctionalityisimplementedandmoreintegra-tiontakesplace.Traditionally,thesesystemsaremonitoredbyhumanoperatorswiththehelpofprimitivedata-analysistools.Thisapproach,however,iscostlyandnotnecessarilyeffective[13].Itis,therefore,essentialtoﬁndautomatedandefﬁcientapproachestosystemsmonitoring.Thetypicalapproachtoautomatedmonitoringistocre-ateasystemmodeltopredictthesystem’sstate,behaviour,orperformance.Predictionsfromsuchamodelcanbecom-∗TheauthorswouldliketothanktheIBMCentreofAdvancedStudies(CAS),Torontoforsupportingthiswork.paredtoobservations,anddeviationsmaysignalthepres-enceoferrors.Oneimportantsourceofdataexposedbysoftwaresystemsismanagementmetrics.Thesemetricsarenumericmeasurementsrelatedtothesystem’sstateandper-formanceandcanbecollectedperiodically.Apromisingrecentapproachtocharacterizingthenormalbehaviourofasystementailsﬁndingandmodelingstablerelationshipsbetweensystemmetrics[4,10,15,16,17].Therelation-shipsaremodeledmathematicallyandtrackedforcheck-ingsystemhealth.Theidentiﬁedrelationshipsbetweenmetricpairsareexpectedtoholdduringnormaloperation.However,inthepresenceoferrors,someoftherelation-shipmodelsareexpectedtoproducepredictionsthatdeviatefromactualobservations.Inrecentworkwehavesuggestedthatasimilaritymea-surebasedoninformationentropymightcapturegeneralinter-metricscorrelations[12].Thismeasurecapturesnotonlylinearcorrelations,butalsonon-linearcorrelations,withouttheneedtoassumeanyspeciﬁcmathematicalform.Correlatedmetricscanbeclustered,andthesystemmoni-toredbytrackingin-clusterentropyforeachcluster.Insteadoftrackingthousandsofpairwisemetric-relationshipmod-els,onlyafewclusterswouldneedtobetracked,whichcanbedoneveryefﬁciently.However,thatworkwasapre-liminarycasestudywithnoformalevaluation,norwasanydetectionordiagnosistechniquedescribed.Inthispaperweformallydescribeandevaluateourap-proach,presentinganautomaticanomaly-detectionmecha-nismthatemploystheWilcoxonRank-Sumtest.Thisal-lowsustodetectcomplexchangepatternsinin-clusteren-tropy,providingrobustandreliablefaultdetection.More-over,wedeveloptwoalgorithmstoidentifyfaultycompo-nentsbasedonthecluster-entropyinformation.RatioScoreusestheJaccardcoefﬁcienttoassignanomalyscorestocomponents,andSigScorefurtheradjuststhesescoresbyameasureofthesigniﬁcanceofthecomponents.Thecontributionsofthispaperareasfollows:•Weuseaninformation-theoreticmeasuretoquantifythestrengthofrelationshipsbetweenmetricpairswith-978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
286
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:20 UTC from IEEE Xplore.  Restrictions apply. 
outassuminganyspeciﬁcformofrelationship.•Weuseanefﬁcientmethodbasedonin-clusterentropytotracksystemstate.•WeapplytheWilcoxonRank-Sumtesttothetrackedentropydataforautomaticanomalydetection.•Wedeveloptwodiagnosisalgorithms:RatioScoreandSigScore.•Wedemonstratethroughexperimentsusingarealisticenterprisesoftwaresystemthatourdetectionmech-anismisveryeffective,correctlyidentifyingthree-quartersofinjectedfaultswithzerofalsepositives;ourSigScorediagnosistechniqueranksthefaultycompo-nentinthetopﬁve40%ofthetime,whichissigniﬁ-cantlybetterthanthebestpriorapproachbasedontheJaccardcoefﬁcient.Theremainderofthispaperisorganizedasfollows.Weﬁrstbrieﬂysummarizeourpriorworkininformation-theoreticmodeling,whichformsthebasisofthiswork.InSection3weproposeanon-parametricstatistictesttoauto-maticallydetectanomalies.WeproposetwoalgorithmstolocatefaultycomponentsinSection4,whichwethenevalu-atebyfault-injectionexperiments.WeprovideanoverviewofrelatedworkinSection6.2Information-Theory-BasedMonitoringTomakethispaperself-contained,weﬁrstprovideaprimeronrelevantinformation-theoreticconcepts,followedbyabriefoverviewofourmodellingapproach.2.1Information-EntropyMeasuresTheinformationentropyintroducedbyShannon[18]measurestheuncertaintyorunpredictabilityofarandomvariable.ForadiscreterandomvariableX,theentropyisgivenby:H(X)=Eplog1p(X)=−nXi=1p(xi)logp(xi)whereXtakesvaluesfromtheset{x1,x2,...,xn},Epreferstotheexpectationwithrespecttotheprobabilitydis-tributionofXcharacterizedbythedensityfunctionp.TheconditionalentropymeasurestheuncertaintyofarandomvariableYgivenanotherrandomvariableX.ItrepresentstheremaininguncertaintyofYknowingvaluestakenbyX.Itisdeﬁnedby:H(Y|X)=Eplog1p(Y|X)=−nXi=1mXj=1p(xi,yj)logp(yj|xi)Themutualinformationmeasuresthereductioninuncer-taintyofarandomvariableYgivenanotherrandomvari-ableX.Thisreductionrepresentstheamountofinforma-tioneithervariableprovidesabouttheother.Itisdeﬁnedby:I(X,Y)=H(Y)−H(Y|X)(1)Strehletal.[19]developedanormalizationformutualin-formation,calledNormalizedMutualInformation(NMI),toaddressshortcomingsinmutualinformation.Itisdeﬁnedby:NMI(X,Y)=I(X,Y)pH(X)H(Y)(2)Foranyrandomvariables,XandY,NMIhasthefollowingniceproperties:1.0≤NMI(X,Y)≤12.NMI(X,Y)=NMI(Y,X)3.IfXandYareindependent,NMI(X,Y)=04.IfY=f(X),NMI(X,Y)=1,foranyinvertiblefunctionfIngeneral,themorecorrelatedtwovariablesare,thehigherNMItheyhave,regardlessofthespeciﬁcformofthere-lationship.Therefore,NMIprovidesagoodmeasureofthecorrelationbetweentwovariables,andcanbeusedasasim-ilaritymeasure.2.2IdentifyingCorrelatedMetricsToidentifycorrelatedmetricsweneedtoestimatetheentropymeasuresanddeterminewhichlevelofNMIcor-respondstostrongcorrelations.Weusesamplescollectedfromthetargetsystematatimewhenitisknowntobeop-eratingerrorfreetocomputetheempiricalentropyvaluesandthencalculatetheNMI.Givennobservedsamplesofanymetric,X,theempiricalentropy,H(X),iscomputedasfollows:H(X)=−kXi=1ninlogninwheretheobservednsamplesaredividedintokbinsandniisthenumberofsamplesobservedinbini.TheempiricalconditionalentropyH(Y|X)basedonobservedsamplesiscomputedasfollows:H(Y|X)=−XiXjnijnlognijniwherenijisthenumberofsamples,(x,y),withxinbiniandyinbinj.Withtheempiricalentropyandempiri-calconditionalentropy,wecomputethenormalizedmutualinformationusingEquations(1)and(2)forallpairsofmet-rics.Next,weneedasuitablethresholdtodifferentiateweakcorrelationsfromstrongcorrelations.However,NMIisa978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE