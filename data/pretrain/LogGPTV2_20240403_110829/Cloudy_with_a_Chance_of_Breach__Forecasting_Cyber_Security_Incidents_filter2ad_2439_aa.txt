# Cloudy with a Chance of Breach: Forecasting Cyber Security Incidents

**Authors:**
- Yang Liu, Armin Sarabi, Jing Zhang, and Parinaz Naghizadeh, University of Michigan
- Manish Karir, QuadMetrics, Inc.
- Michael Bailey, University of Illinois at Urbana-Champaign
- Mingyan Liu, University of Michigan and QuadMetrics, Inc.

**Publication:**
- Proceedings of the 24th USENIX Security Symposium
- August 12–14, 2015, Washington, D.C.
- ISBN: 978-1-939133-11-3
- Open access sponsored by USENIX
- [Link to the paper](https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/liu)

## Abstract

In this study, we investigate the feasibility of predicting cyber security incidents, such as those documented in Verizon's annual Data Breach Investigations Reports (DBIR), based on externally observable properties of an organization’s network. Our goal is to forecast breaches proactively without requiring cooperation from the organization. We collect 258 externally measurable features, including mismanagement symptoms (e.g., misconfigured DNS or BGP) and malicious activity time series (e.g., spam, phishing, and scanning). Using these features, we train and test a Random Forest (RF) classifier on over 1,000 incident reports from the VERIS community database, Hackmageddon, and the Web Hacking Incidents Database, covering events from mid-2013 to the end of 2014. The resulting classifier achieves a 90% True Positive (TP) rate, a 10% False Positive (FP) rate, and an overall 90% accuracy.

## 1. Introduction

Recent high-profile data breaches, such as those at Target, JP Morgan, and Home Depot, highlight the significant social and economic impact of cyber incidents. For example, the JP Morgan Chase attack affected nearly 76 million households. Often, by the time a breach is detected, the damage has already occurred, raising the question of whether such breaches could have been predicted and prevented. This study aims to explore the extent to which one can forecast if an organization may suffer a cyber security incident in the near future.

Machine learning has been widely used in the cyber security domain, primarily for detecting malicious activities like spam and phishing. However, it has been less utilized for prediction. A notable exception is the use of textual data to predict whether a currently benign webpage may turn malicious in the future. The difference between detection and prediction is analogous to diagnosing a patient who may already be ill versus projecting whether a healthy person may become ill based on relevant factors.

To explore the effectiveness of forecasting security incidents, we collect externally observed data on Internet organizations, without requiring information on their internal workings. We use a diverse set of data that captures different aspects of a network’s security posture, ranging from explicit or behavioral (e.g., externally observed malicious activities) to latent or relational (e.g., misconfigurations). From this data, we extract 258 features and feed them into a Random Forest (RF) classifier. We train and test the classifier on more than 1,000 incident reports from the VERIS community database, Hackmageddon, and the Web Hacking Incidents Database, covering events from mid-2013 to 2014. The resulting classifier can achieve a 90% True Positive (TP) rate, a 10% False Positive (FP) rate, and an overall accuracy of 90%.

We argue that cyber incident forecasting offers unique characteristics compared to detection techniques, enabling new applications. Prediction allows for proactive policies and measures, reducing potential costs. It also supports effective risk management schemes, such as cyber insurance, which incentivizes better cyber security practices. In the wake of recent breaches, the market for such policies has grown, with estimated annual premiums between $500M and $1B.

The remainder of the paper is organized as follows:
- **Section 2:** Introduces the datasets and processing methodology.
- **Section 3:** Defines the features used in the classifier and explains their relevance.
- **Section 4:** Presents the main prediction results and their implications.
- **Section 5:** Discusses observations and illustrates major data breaches in 2014.
- **Section 6:** Details related work.
- **Section 7:** Concludes the paper.

## 2. Data Collection and Processing

Our study draws from various data sources that collectively characterize the security posture of organizations and provide security incident reports. These sources are summarized in Table 1 and detailed below. A subset of these datasets is available at [7].

### 2.1 Security Posture Data

An organization’s network security posture can be measured in multiple ways. We utilize two families of measurement data:
1. **Misconfigurations or deviations from standards and operational recommendations.**
2. **Malicious activities originating from the network.**

These two types of measurements are related. Zhang et al. [58] quantitatively established varying degrees of correlation between eight different mismanagement symptoms and the amount of malicious activities from an organization. The combination of both datasets provides a comprehensive view of an organization’s externally discernible security posture.

#### 2.1.1 Mismanagement Symptoms

We use the following five mismanagement symptoms, a subset of those studied in [58]:
- **Open Recursive Resolvers:** Misconfigured open DNS resolvers can facilitate massive amplification attacks. We use data from the Open Resolver Project [14], collected on June 2, 2013, identifying 27.1 million open recursive resolvers.
- **DNS Source Port Randomization:** Best practice (RFC 5452 [34]) recommends source port randomization and a randomized query ID to minimize DNS cache poisoning. We use data from [58], where over 200,000 misconfigured DNS resolvers were detected.
- **BGP Misconfiguration:** BGP configuration errors can cause unnecessary routing protocol updates. We use data from [58], where 42.4 million short-lived routes were detected from 12 BGP listeners in the Route Views project [32].
- **Untrusted HTTPS Certificates:** Secure websites should use X.509 certificates signed by a trusted certificate authority. We use data from [58], where only 10.3 million out of 21.4 million sites presented browser-trusted certificates.
- **Open SMTP Mail Relays:** Email servers should filter messages to allow only users in their own domain to send emails. We use data from [58], where 22,284 open mail relays were detected.

None of these datasets directly indicates a vulnerability, but they are indicators of the lack of appropriate policies and technological solutions, increasing the potential for a successful data breach. All datasets were collected during the first half of 2013, reflecting the condition of a network prior to the incidents. Our incident datasets cover incidents from August 2013 to December 2014.

#### 2.1.2 Malicious Activity Data

Another indicator of inadequate security measures is the level of malicious activities observed from an organization’s network. We use reputation blacklists to measure this, including:
- **Spam activities:** CBL[4], SBL[22], SpamCop[19], WPBL[24], UCEPROTECT[23]
- **Phishing and malware activities:** SURBL[20], PhishTank[16], hpHosts[11]
- **Scanning activities:** Darknet scanners list, Dshield[5], OpenBL[15]

We use blacklists collected from May 11, 2013, to December 31, 2014, refreshed daily. This longitudinal dataset characterizes not only the presence of malicious activities but also their dynamic behavior over time.

### 2.2 Security Incident Data

In addition to security posture data, we require reported cybersecurity incidents for training and testing the classifier. We use three publicly available incident datasets:

- **VERIS Community Database (VCDB) [55]:** Maintained by the Verizon RISK Team, this dataset contains over 5,000 incident reports. We include approximately 700 unique incidents occurring after mid-2013, excluding physical attacks, robberies, and deliberate internal misuse.
- **Hackmageddon [42]:** An independently maintained blog that aggregates public reports of cyber security incidents. We extract 300 incidents from October 2013 to February 2014.
- **Web Hacking Incidents Database (WHID) [31]:** An actively maintained repository. We extract roughly 150 incidents from January 2014 to November 2014.

A breakdown of the incidents by type is given in Table 3. Note that Hackmageddon and WHID have similar categories, while VCDB has broader categories.

### 2.3 Data Pre-processing

Our diverse datasets provide substantial visibility into organizational security, but aligning the data in both time and space presents challenges. Security posture datasets record information at the host IP-address level, while incident reports are typically associated with a company or organization. Conceptually, it is more natural to predict incidents for an organization. 

To address this mismatch, we:
1. **Map an organization to a set of IP addresses.**
2. **Aggregate mismanagement and maliciousness information over this set of addresses.**

We retrieve sample IP addresses from each incident report and use registration information from Regional Internet Registries (RIR) databases to identify the aggregation unit. These databases, collected from ARIN [3], LACNIC [12], APNIC [2], AFRINIC [1], and RIPE [18], keep records of IP address blocks allocated to organizations.

## 3. Feature Definition and Relevance

In this section, we define the 258 features used in constructing the classifier and explain their relevance in predicting security incidents. These features capture various aspects of an organization’s network, including misconfigurations and malicious activities. By using a comprehensive set of features, we aim to provide a robust model for predicting cyber security incidents.

## 4. Prediction Results and Implications

This section presents the main prediction results and their implications. We discuss the performance of the Random Forest classifier, including its True Positive (TP) and False Positive (FP) rates, and overall accuracy. We also explore the practical applications of our prediction model and its potential impact on proactive security measures.

## 5. Observations and Case Studies

In this section, we discuss several observations and illustrate major data breaches in 2014 in the context of our prediction methodology. We provide case studies to demonstrate how our model can be applied to real-world scenarios and highlight the importance of proactive security measures.

## 6. Related Work

This section details related work in the field of cyber security incident prediction. We compare our approach with existing methods and highlight the unique contributions of our study.

## 7. Conclusion

In conclusion, we present a method for forecasting cyber security incidents based on externally observable properties of an organization’s network. Our Random Forest classifier, trained on a diverse set of features, achieves high accuracy in predicting security incidents. We emphasize the importance of proactive measures and the potential benefits of our approach in reducing the impact of cyber incidents. Future work will focus on refining the model and expanding its application to a broader range of organizations.

---

**Table 1: Summary of Datasets Used in This Study**

| Category                | Datasets                                                                 | Collection Period         |
|-------------------------|--------------------------------------------------------------------------|---------------------------|
| **Mismanagement Symptoms** | Open Recursive Resolvers, DNS Source Port Randomization, BGP Misconfiguration, Untrusted HTTPS Certificates, Open SMTP Mail Relays [58] | February 2013 - July 2013 |
| **Malicious Activities**    | CBL, SBL, SpamCop, WPBL, UCEPROTECT, SURBL, PhishTank, hpHosts, Darknet scanners list, Dshield, OpenBL | May 2013 - December 2014  |
| **Incident Reports**        | VERIS Community Database, Hackmageddon, Web Hacking Incidents Database  | August 2013 - December 2014 |

**Table 2: Examples of Excluded VCDB Incidents**

| Incident Report                          | Reason to Exclude                  |
|------------------------------------------|------------------------------------|
| Student of a college changed score       | Unknown target                     |
| Road construction sign hacked            | Physical tampering                 |
| Praxair Healthcare Inc. asset stolen     | Physical theft                     |
| Lucile Packard Child. Hosp. asset stolen | Physical theft                     |
| Medicare Privilege Misuse                | Deliberate internal misuse         |

**Table 3: Reported Cyber Incidents by Category**

| Incident Type   | Hackmageddon | WHID | VCDB          |
|-----------------|--------------|------|---------------|
| SQLi            | 38           | 12   | Crimeware     | 59           |
| Cyber Esp.      | 16           | Web app. | 368           |
| Hijacking       |              | Defacement |               |
| DDoS            | 9            | 5    | 97            |
| Else            | 213          | 16   | 59            |