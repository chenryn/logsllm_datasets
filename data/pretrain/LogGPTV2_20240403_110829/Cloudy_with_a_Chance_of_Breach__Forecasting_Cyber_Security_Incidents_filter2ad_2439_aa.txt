title:Cloudy with a Chance of Breach: Forecasting Cyber Security Incidents
author:Yang Liu and
Armin Sarabi and
Jing Zhang and
Parinaz Naghizadeh and
Manish Karir and
Michael Bailey and
Mingyan Liu
Cloudy with a Chance of Breach:  
Forecasting Cyber Security Incidents
Yang Liu, Armin Sarabi, Jing Zhang, and Parinaz Naghizadeh, University of Michigan; 
Manish Karir, QuadMetrics, Inc.; Michael Bailey, University of Illinois at Urbana-Champaign; 
Mingyan Liu, University of Michigan and QuadMetrics, Inc.
https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/liu
This paper is included in the Proceedings of the 
24th USENIX Security Symposium
August 12–14, 2015 • Washington, D.C.
ISBN  978-1-939133-11-3
Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXCloudy with a Chance of Breach: Forecasting Cyber Security Incidents
Yang Liu1, Armin Sarabi1, Jing Zhang1, Parinaz Naghizadeh1
Manish Karir2, Michael Bailey3, Mingyan Liu1,2
1 EECS Department, University of Michigan, Ann Arbor
3 ECE Department, University of Illinois, Urbana-Champaign
2 QuadMetrics, Inc.
Abstract
In this study we characterize the extent to which cyber
security incidents, such as those referenced by Verizon
in its annual Data Breach Investigations Reports (DBIR),
can be predicted based on externally observable prop-
erties of an organization’s network. We seek to proac-
tively forecast an organization’s breaches and to do so
without cooperation of the organization itself. To ac-
complish this goal, we collect 258 externally measur-
able features about an organization’s network from two
main categories: mismanagement symptoms, such as
misconﬁgured DNS or BGP within a network, and mali-
cious activity time series, which include spam, phishing,
and scanning activity sourced from these organizations.
Using these features we train and test a Random For-
est (RF) classiﬁer against more than 1,000 incident re-
ports taken from the VERIS community database, Hack-
mageddon, and the Web Hacking Incidents Database that
cover events from mid-2013 to the end of 2014. The re-
sulting classiﬁer is able to achieve a 90% True Positive
(TP) rate, a 10% False Positive (FP) rate, and an overall
90% accuracy.
1
Introduction
Recent data breaches, such as those at Target [35], JP
Morgan [25], and Home Depot [49] highlight the in-
creasing social and economic impact of such cyber inci-
dents. For example, the JP Morgan Chase attack was be-
lieved to be one of the largest in history, affecting nearly
76 million households [25]. Often, by the time a breach
is detected, it is already too late and the damage has al-
ready occurred. As a result, such events call into the
question whether these breaches could have been pre-
dicted and the damage avoided. In this study we seek
to understand the extent to which one can forecast if an
organization may suffer a cyber security incident in the
near future.
Machine learning has been used extensively in the
cyber security domain, most prominently for detection
of various malicious activities or entities, e.g., spam
[44, 45] and phishing[39]. It has been used far less for the
purpose of prediction, with the notable exception of [51],
where textual data is used to train classiﬁers to predict
whether a currently benign webpage may turn malicious
in the near future. The difference between detection and
prediction is analogous to the difference between diag-
nosing a patient who may already be ill (e.g., by using
biopsy) vs. projecting whether a presently healthy per-
son may become ill based on a variety of relevant fac-
tors. The former typically relies on identifying known
characteristics of the object to be detected, while the lat-
ter on factors believed to correlate with the prediction
objective.
To explore the effectiveness of forecasting security in-
cidences we begin by collecting externally observed data
on Internet organizations; we do not require information
on the internal workings of a network or its hosts. To do
so, we tap into a diverse set of data that captures different
aspects of a network’s security posture, ranging from the
explicit or behavioral, such as externally observed ma-
licious activities originating from a network (e.g., spam
and phishing) to the latent or relational, such as misman-
agement and misconﬁgurations in a network that deviate
from known best practices. From this data we extract 258
features and feed them to a Random Forest (RF) classi-
ﬁer. We train and test the classiﬁer on these features and
more than 1,000 incident reports taken from the VERIS
community database [55], Hackmageddon [42], and the
Web Hacking Incidents Database [31] that cover events
from mid-2013 to 2014. The resulting classiﬁer can be
conﬁgured over a wide range of operating points includ-
ing one with 90% True Positive (TP) rate, 10% False Pos-
itive (FP) rate and an overall accuracy of 90%.
We posit that such cyber incident forecasting offers a
completely different set of characteristics as compared to
detection techniques, which in turn enables entirely new
USENIX Association  
24th USENIX Security Symposium  1009
classes of applications that are not feasible with detection
techniques alone. First and foremost, prediction allows
proactive policies and measures to be adopted rather than
reactive measures following the detection of an incident.
Effective proactive actions can substantially reduce the
potential cost incurred by an incident; in this sense pre-
diction is complementary to detection. Cyber incident
prediction also enables the development of effective risk
management schemes such as cyber insurance, which in-
troduces monetary incentives for the adoption of better
cyber security policies and technologies. In the wake of
recent breaches, the market for such policies has soared,
with current written annual premiums estimated to be be-
tween $500M and $1B [47].
The remainder of the paper is organized as follows.
Section 2 introduces the datasets used in this study and
details the rationale for their use as well as our processing
methodology. We then deﬁne the features we use in con-
structing the classiﬁer and show why they are relevant
in predicting security incidents in Section 3. We present
the main prediction results as well as their implications in
Section 4. In Section 5 we discuss a number of observa-
tions and illustrate several major data breaches in 2014
in the context of this prediction methodology. Related
work is detailed in Section 6, and Section 7 concludes
the paper.
2 Data Collection and Processing
Our study draws from a variety of data sources that col-
lectively characterize the security posture of organiza-
tions, as well as security incident reports used to deter-
mine their security outcomes. These sources are summa-
rized in Table 1 and detailed below; a subset of these has
been made available at [7].
2.1 Security Posture Data
An organization’s network security posture may be mea-
sured in various ways. Here, we utilize two families of
measurement data. The ﬁrst is measurements on a net-
work’s misconﬁgurations or deviations from standards
and other operational recommendations; the second is
measurements on malicious activities seen to originate
from that network. These two types of measurements are
related. In particular, in [58] Zhang et al. quantitatively
established varying degrees of correlation between eight
different mismanagement symptoms and the amount of
malicious activities from an organization. The combina-
tion of both of these datasets represents a fairly compre-
hensive view of an organization’s externally discernible
security posture.
2.1.1 Mismanagement Symptoms
We use the following ﬁve mismanagement symptoms in
our study, a subset of those studied in [58].
Open Recursive Resolvers: Misconﬁgured open DNS
resolvers can be easily used to facilitate massive ampli-
ﬁcation attacks that target others.
In order to help the
network operations community address this wide spread
threat, the Open Resolver Project [14] actively sends a
DNS query to every public IPv4 address in port 53 to
identify misconﬁgured DNS resolvers. In this study, we
use a data snapshot collected on June 2, 2013. In total,
27.1 million open recursive resolvers were identiﬁed.
DNS Source Port Randomization: In order to minimize
the threat of DNS cache poisoning attacks [13], current
best practice (RFC 5452 [34]) recommends that DNS
servers implement both source port randomization and
a randomized query ID. Many servers however have not
been patched to implement source port randomization.
In [58], over 200,000 misconﬁgured DNS resolvers were
detected based on the analysis over a set of DNS queries
seen by VeriSign’s .com and .net TLD name server on
February 26, 2013. This is the data used in this study.
BGP Misconﬁguration: BGP conﬁguration errors or
reconﬁguration events can cause unnecessary routing
protocol updates with short-lived announcements in the
global routing table [40]. Zhang et. al detected 42.4 mil-
lion short-lived routes with BGP updates from 12 BGP
listeners in the Route Views project [32] during the ﬁrst
two weeks of June 2013 [58]; this data is used in our
study.
Untrusted HTTPS Certiﬁcates: Secure websites uti-
lize X.509 certiﬁcates as part of the TLS handshake in
order to prove their identity to clients. Properly conﬁg-
ured certiﬁcates should be signed by a browser-trusted
certiﬁcate authority. It is possible to detect misconﬁg-
ured websites by validating the certiﬁcate presented dur-
ing the TLS handshake [33]. An Internet scan performed
on March 22, 2013 found that only 10.3 million out of a
total of 21.4 million sites presented browser-trusted cer-
tiﬁcates [58]. We use this dataset in our study.
Open SMTP Mail Relays: Email servers should per-
form ﬁltering on the message source or destination to
only allow users in their own domain to send email mes-
sages. This is documented in current best practice (RFC
2505 [38]), and misconﬁgured servers can be used in
large scale spam campaigns. Though small in number,
these represent a severe misconﬁguration in an organiza-
tions’ infrastructure. In this study, we use data collected
on July 23, 2013, which detected 22,284 open mail relays
[58].
None of the datasets mentioned above is necessarily
directly related to a vulnerability. The presence of mis-
conﬁgurations in an organization’s networks and infras-
1010  24th USENIX Security Symposium 
USENIX Association
Category
Mismanagement
symptoms
Malicious activities May 2013 - December 2014
Collection period
February 2013 - July 2013
Incident reports
August 2013 - December 2014
Datasets
Open Recursive Resolvers, DNS Source Port Randomization, BGP misconﬁguration,
Untrusted HTTPS Certiﬁcates, Open SMTP Mail Relays [58]
CBL[4] , SBL[22], SpamCop[19], WPBL[24], UCEPROTECT[23], SURBL[20],
PhishTank[16], hpHosts[11], Darknet scanners list, Dshield[5], OpenBL[15]
VERIS Community Database [55], Hackmageddon [42], Web Hacking Incidents [31]
Table 1: Summary of datasets used in this study. Mismanagement and malicious activity data are used to extract
features, while incident reports are used to generate labels for the training and testing of a classiﬁer.
tructure is, however, an indicator of the lack of appro-
priate policies and technological solutions to detect such
failures. The latter increases the potential for a success-
ful data breach.
Also, note that all of the above datasets were collected
during roughly the ﬁrst half of 2013. As we shall be
using the mismanagement symptoms as features in con-
structing a classiﬁer/predictor, it is important that these
features reﬂect the condition of a network prior to the
incidents. Consequently, our incident datasets (detailed
in Section 2.2) cover incidents that occurred between
August 2013 and December 2014. Note also that we
use only a single snapshot of each of the symptoms;
this is because such symptomatic data is relatively slow-
changing over time, as systems are generally not recon-
ﬁgured on a daily or even weekly basis.
2.1.2 Malicious Activity Data
Another indicator of the lack of policy or technical
measures to improve security at an organization is the
level of malicious activities observed to originate from
its network assets and infrastructure. Such activity is
often observed by well-established monitoring systems
such as spam traps, darknet monitors, or DNS moni-
tors. These observations are then distilled into black-
lists. We use a set of reputation blacklists to measure the
level of malicious activities in a network. This set further
breaks down into three types: (1) those capturing spam
activities, including CBL[4] , SBL[22], SpamCop[19],
WPBL[24], and UCEPROTECT[23], (2) those capturing
phishing and malware activities, including SURBL[20],
PhishTank[16], and hpHosts[11], and (3) those capturing
scanning activities, including the Darknet scanners list,
Dshield[5], and OpenBL[15]. We use reputation black-
lists that have been collected over a period of more than
a year, starting in May 11, 2013 and ending in December
31, 2014. Each blacklist is refreshed on a daily basis and
consists of a set of IP addresses seen to be engaged in
some malicious activity. This longitudinal dataset allows
us to characterize not only the presence of malicious ac-
tivities from an organization, but also its dynamic beha-
vior over time.
2.2 Security Incident Data
In addition to the security posture data described in the
previous section, we require data on reported cyber-
security incidents to serve as ground-truth in our study;
such data is needed for the purpose of training the clas-
siﬁer, as well as for assessing its accuracy in predicting
incidents (testing). In general, we believe such incidents
are vastly under reported. In order to obtain a good cove-
rage, we employ three collections of publicly available
incident datasets. These are described below.
VERIS Community Database (VCDB) [55]: This
dataset represents a broad ranging public effort to gather
cyber security incident reports in a common format [55].
The collection is maintained by the Verizon RISK Team,
and is used by Verizon in its highly publicized annual
Data Breach Investigations Reports (DBIR) [56]. The
current repository contains more than 5,000 incident re-
ports, that cover a variety of different types of events
such as server breach, website defacements, and physi-
cally stolen assets. Table 7 (in the Appendix) provides
some example reports from this repository; a majority
(64.99%) is from the US.
Of the full set, roughly 700 unique incidents were rele-
vant to our study: we include only incidents that occurred
after mid-2013 so that they are aligned with the security
posture data, and those directly reﬂecting cyber-security
issues. We therefore exclude those due to physical at-
tacks, robbery, deliberate mis-operation by internal ac-
tors (e.g. disgruntled employees) and the like, as well as
unnamed or unveriﬁed attack targets. We show several
such examples in Table 2. Also note that even though the
same IPs may appear in both the malicious and incident
data, the independence of the features from ground-truth
data is maintained because malicious activities only re-
veal botnet presence, which is not considered an incident
type by or reported in any of our incident datasets.
Incident report
Student of a college changed score
Road construction sign hacked
Praxair Healthcare Inc. asset stolen
Lucile Packard Child. Hosp.l asset stolen
Medicare Privilege Misuse
Reason to exclude
Unknown target
Physical tampering
Physical theft
Physical theft
Deliberate internal misuse
Table 2: Examples of excluded VCDB incidents.
USENIX Association  
24th USENIX Security Symposium  1011
Hackmageddon [42]: This is an independently main-
tained cyber incident blog that aggregates and documents
various public reports of cyber security incidents on a
monthly basis. From the overall set we extract 300 in-
cidents, in which the reported dates are aligned with our
security posture data, between October 2013 and Febru-
ary 2014, and for which we are able to clearly identify
the affected organizations.
The Web Hacking Incidents Database (WHID) [31]:
This is an actively maintained cyber security incident
repository; its goal is to raise awareness of cyber secu-
rity issues and to provide information for statistical anal-
ysis. From the overall dataset we identify and extract
roughly 150 incidents, for which the reported dates are
aligned with our security posture data, between January
2014 and November 2014.
A breakdown of the incidents by type from each of
these datasets is given in Table 5. Note that Hackmaged-
don and WHID have similar categories while VCDB has
much broader categories.
Incident type
Hackmageddon
WHID
Incident type
VCDB
SQLi
38
12
Crimeware
59
Cyber Esp.
16
Web app.
368
Hijacking
Defacement
DDoS
9
5
97
16
59
45
Else
213
Table 3: Reported cyber incidents by category. Only the
major categories in each set are shown. The “Else” cat-
egory by VCDB represents incidents lacking sufﬁcient
detail for better classiﬁcation.
2.3 Data Pre-processing
Though our diverse datasets give us substantial visibility
into the state of security at an organizational level, the
diversity also presents substantial challenges in aligning
the data in both time and space. All of the security pos-
ture datasets – mismanagement and malicious activities
– record information at the host IP-address level; e.g.,
they reveal whether a particular IP address is blacklisted
on a given day, or whether a host at a speciﬁc IP address
is misconﬁgured. On the other hand, a cyber incident
report is typically associated with a company or organi-
zation, not with a speciﬁc IP address within that domain.
Conceptually, it is more natural to predict incidents for
an organization for the following reasons. Firstly, our
interest is in predicting incidents broadly deﬁned as a
way to assess organizational cyber risk. Secondly, while
some IP addresses are statically associated with a ma-
chine, e.g., a web server, others are dynamically assigned
due to mobility, e.g., through WiFi. In the latter case pre-
dicting for speciﬁc IP addresses no longer makes sense.
This mismatch in resolution means that we will have
to (1) map an organization reported in an incident to a
set of IP addresses and (2) aggregate mismanagement
and maliciousness information over this set of addresses.
To address the ﬁrst step we will ﬁrst retrieve a sam-
ple IP address in the network of the compromised or-
ganization, which is then used to identify an aggrega-
tion unit – a set of IP addresses – that allows us to re-
cover the network asset involved in the incident. Sample
IP addresses are obtained by manually processing each
incident report, and the aggregation units are identiﬁed
by using registration information from Regional Inter-
net Registries (RIR) databases. These databases are col-
lected separately from ARIN [3], LACNIC [12], APNIC
[2], AFRINIC [1] and RIPE [18], who keep records of
IP address blocks/preﬁxes that are allocated to an orga-
nization. ARIN, APNIC, AFRINIC and RIPE databases
keep track of the IP addresses that have been allocated,
along with the organizations they have been allocated to,
labeled with a maintainer ID. LACNIC provides a less