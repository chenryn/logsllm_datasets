title:Comprehensive Privacy Analysis of Deep Learning: Passive and Active
White-box Inference Attacks against Centralized and Federated Learning
author:Milad Nasr and
Reza Shokri and
Amir Houmansadr
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
Comprehensive Privacy Analysis of Deep Learning
Passive and Active White-box Inference Attacks against Centralized and Federated Learning
Milad Nasr
Reza Shokri
Amir Houmansadr
University of Massachusetts Amherst
National University of Singapore
University of Massachusetts Amherst
PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract—Deep neural networks are susceptible to various
inference attacks as they remember information about their
training data. We design white-box inference attacks to perform
a comprehensive privacy analysis of deep learning models. We
measure the privacy leakage through parameters of fully trained
models as well as the parameter updates of models during
training. We design inference algorithms for both centralized and
federated learning, with respect to passive and active inference
attackers, and assuming different adversary prior knowledge.
We evaluate our novel white-box membership inference attacks
against deep learning algorithms to trace their training data
records. We show that a straightforward extension of the known
black-box attacks to the white-box setting (through analyzing the
outputs of activation functions) is ineffective. We therefore design
new algorithms tailored to the white-box setting by exploiting
the privacy vulnerabilities of the stochastic gradient descent
algorithm, which is the algorithm used to train deep neural
networks. We investigate the reasons why deep learning models
may leak information about their training data. We then show
that even well-generalized models are signiﬁcantly susceptible
to white-box membership inference attacks, by analyzing state-
of-the-art pre-trained and publicly available models for the
CIFAR dataset. We also show how adversarial participants,
in the federated learning setting, can successfully run active
membership inference attacks against other participants, even
when the global model achieves high prediction accuracies.
I. INTRODUCTION
Deep neural networks have shown unprecedented gener-
alization for various learning tasks, from image and speech
recognition to generating realistic-looking data. This success
has led to many applications and services that use deep learn-
ing algorithms on large-dimension (and potentially sensitive)
user data, including user speeches, images, medical records,
ﬁnancial data, social relationships, and location data points.
In this paper, we are interested in answering the following
critical question: What is the privacy risk of deep learning
algorithms to individuals whose data is used for training deep
neural networks? In other words, how much is the information
leakage of deep learning algorithms about their individual
training data samples?
We deﬁne privacy-sensitive leakage of a model, about its
training data, as the information that an adversary can learn
from the model about them, which he is not able to infer from
other models that are trained on other data from the same
distribution. This distinguishes between the information that
we can learn from the model about the data population, and
the information that the model leaks about the particular data
samples which are in its training set. The former indicates
utility gain, and the later reﬂects privacy loss. We design
inference attacks to quantify such privacy leakage.
Inference attacks on machine learning algorithms fall into
two fundamental and related categories: tracing (a.k.a. mem-
bership inference) attacks, and reconstruction attacks [1]. In
a reconstruction attack, the attacker’s objective is to infer
attributes of the records in the training set [2], [3]. In a
membership inference attack, however, the attacker’s objective
is to infer if a particular individual data record was included in
the training dataset [4], [5], [6]. This is a decisional problem,
and its accuracy directly demonstrates the leakage of the model
about its training data. We thus choose this attack as the basis
for our privacy analysis of deep learning models.
Recent works have studied membership inference attacks
against machine learning models in the black-box setting,
where the attacker can only observe the model predictions [6],
[7]. The results of these works show that the distribution of
the training data as well as the generalizability of the model
signiﬁcantly contribute to the membership leakage. Particu-
larly, they show that overﬁtted models are more susceptible to
membership inference attacks than generalized models. Such
black-box attacks, however, might not be effective against deep
neural networks that generalize well (having a large set of
parameters). Additionally, in a variety of real-world settings,
the parameters of deep learning algorithms are visible to the
adversaries, e.g., in a federated learning setting where multiple
data holders collaborate to train a global model by sharing their
parameter updates with each other through an aggregator.
Our contributions. In this paper, we present a comprehensive
framework for the privacy analysis of deep neural networks,
using white-box membership inference attacks. We go beyond
membership inference attacks against fully-trained models.
We take all major scenarios where deep learning is used
for training and ﬁne-tuning or updating models, with one
or multiple collaborative data holders, when attacker only
passively observes the model updates or actively inﬂuences
the target model in order to extract more information, and
for attackers with different types of prior knowledge. Despite
differences in knowledge, observation, and actions of the
adversary, their objective is the same: membership inference.
A simple extension of existing black-box membership in-
ference attacks to the white-box setting would be using the
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:46)(cid:74)(cid:77)(cid:66)(cid:69)(cid:1)(cid:47)(cid:66)(cid:84)(cid:83)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:23)(cid:22)
(cid:24)(cid:20)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
same attack on all of the activation functions of the model.
Our empirical evaluations show that this will not result in
inference accuracy better than that of a black-box attacker.
This is because the activation functions in the model tend to
generalize much faster compared to the output layer. The early
layers of a trained model extract very simple features that
are not speciﬁc to the training data. The activation functions
in the last layers extract complex and abstract features, thus
should contain more information about the model’s training
set. However, this information is more or less the same as
what the output leaks about the training data.
We design white-box inference attacks that exploit the
privacy vulnerabilities of the stochastic gradient descent
(SGD) algorithm. Each data point in the training set in-
ﬂuences many of the model parameters, through the SGD
algorithm, to minimize its contribution to the model’s training
loss. The local gradient of the loss on a target data record,
with respect to a given parameter, indicates how much and
in which direction the parameter needs to be changed to ﬁt
the model to the data record. To minimize the expected loss
of the model, the SGD algorithm repeatedly updates model
parameters in a direction that the gradient of the loss over the
whole training dataset leans to zero. Therefore, each training
data sample will leave a distinguishable footprint on the
gradients of the loss function over the model’s parameters.
We use the gradient vector of the model, over all parameters,
on the target data point, as the main feature for the attack. We
design deep learning attack models with an architecture that
processes extracted (gradient) features from different layers of
the target model separately, and combines their information to
compute the membership probability of a target data point.
We train the attack model for attackers with different types
of background knowledge. Assuming a subset of the training
set is known to the attacker, we can train the attack model
in a supervised manner. However, for the adversary that lacks
this knowledge, we train the attack model in an unsupervised
manner. We train auto-encoders to compute a membership
information embedding for any data. We then use a clustering
algorithm, on the target dataset, to separate members from
non-members based on their membership embedding.
To show the effectiveness of our white-box inference at-
tack, we evaluate the privacy of pre-trained and publicly
available state-of-the-art models on the CIFAR100 dataset.
We had no inﬂuence on training these models. Our results
show that the DenseNet model—which is the best model on
CIFAR100 with 82% test accuracy—is not much vulnerable
to black-box attacks (with a 54.5% inference attack accuracy,
where 50% is the baseline for random guess). However, our
white-box membership inference attack obtains a consider-
ably higher accuracy of 74.3%. This shows that even well-
generalized deep models might leak signiﬁcant amount
of information about their training data, and could be
vulnerable to white-box membership inference attacks.
In federated learning, we show that a curious parameter
server or even a participant can perform alarmingly accurate
membership inference attacks against other participants. For
the DenseNet model on CIFAR100, a local participant can
achieve a membership inference accuracy of 72.2%, even
though it only observes aggregate updates through the pa-
rameter server. Also,
the curious central parameter server
can achieve a 79.2% inference accuracy, as it receives the
individual parameter updates from all participants. In federated
learning, the repeated parameter updates of the models over
different epochs on the same underlying training set is a key
factor in boosting the inference attack accuracy.
As the contributions (i.e., parameter updates) of an adversar-
ial participant can inﬂuence the parameters other parties, in the
federated learning setting, the adversary can actively push
SGD to leak even more information about the participants’
data. We design an active attack that performs gradient ascent
on a set of target data points before uploading and updating
the global parameters. This magniﬁes the presence of data
points in others’ training sets, in the way SGD reacts by
abruptly reducing the gradient on the target data points if
they are members. On the Densenet model, this leads to a
76.7% inference accuracy for an adversarial participant, and
a signiﬁcant 82.1% accuracy for an active inference attack by
the central server. By isolating a participant during parameter
updates, the central attacker can boost his accuracy to 87.3%.
II. INFERENCE ATTACKS
We use membership inference attacks to measure the in-
formation leakage through deep learning models about their
training data. There are many different scenarios in which data
is used for training models, and there are many different ways
the attacker can observe the deep learning process. In Table I,
we cover the major criteria to categorize the attacks. This
includes attack observations, assumptions about the adversary
knowledge, the target training algorithm, and the mode of the
attack based on the adversary’s actions. In this section, we
discuss different attack scenarios as well as the techniques
we use to exploit deep learning algorithms. We also describe
the architecture of our attack model, and how the adversary
computes the membership probability.
A. Attack Observations: Black-box vs. White-box Inference
The adversary’s observations of the deep learning algorithm
are what constitute the inputs for the inference attack.
Black-box. In this setting, the adversary’s observation is lim-
ited to the output of the model on arbitrary inputs. For any data
point x, the attacker can only obtain f (x; W). The parameters
of the model W and the intermediate steps of the computation
are not accessible to the attacker. This is the setting of machine
learning as a service platforms. Membership inference attacks
against black-box models are already designed, which exploit
the statistical differences between a model’s predictions on its
training set versus unseen data [6].
White-box.
In this setting, the attacker obtains the model
f (x; W) including its parameters which are needed for pre-
diction. Thus, for any input x, in addition to its output, the
attacker can compute all the intermediate computations of
the model. That is, the adversary can compute any function
(cid:24)(cid:21)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
Criteria
Attacks
Description
Observation
Black-box
The attacker can obtain the prediction vector f (x) on arbitrary input x, but cannot access the model parameters, nor
the intermediate computations of f (x).
x
f
f (x)
White-box
The attacker has access to the full model f (x; W), notably its architecture and parameters W, and any hyper-parameter
that is needed to use the model for predictions. Thus, he can also observe the intermediate computations at hidden layers
hi(x).
x
W1
h1(x) W2
h2(x)
· · · Wi
f (x)
Target
Stand-alone
The attacker observes the ﬁnal target model f, after the training is done (e.g., in a centralized manner) using dataset D.
He might also observe the updated model fΔ after it has been updated (ﬁne-tuned) using a new dataset DΔ.
f
xD
ﬁne-tune
fΔ
DΔ
Federated
The attacker could be the central aggregator, who observes individual updates over time and can control the view of the
participants on the global parameters. He could also be any of the participants who can observe the global parameter
updates, and can control his parameter uploads.
Aggregator
(global parameters W)
f (x; W
{t}
1
)
f (x; W
{t}
2
)
x
D1
D2
down=W{t}
{t}
up=W
i
· · ·
f (x; W
{t}
N )
DN
Mode
Passive
Active
Knowledge
Supervised
{t}
i
The attacker can only observe the genuine computations by the training algorithm and the model.
The attacker could be one of the participants in the federated learning, who adversarially modiﬁes his parameter uploads
, or could be the central aggregator who adversarially modiﬁes the aggregate parameters W{t} which he sends
W
to the target participant(s).
The attacker has a data set D(cid:2), which contains a subset of the target set D, as well as some data points from the same
underlying distribution as D that are not in D. The attacker trains an inference model h in a supervised manner, by
d∈D(cid:2) (1 − 1d∈D)h(d) + 1d∈D(1 − h(d)), where the inference model h
minimizing the empirical loss function
computes the membership probability of any data point d in the training set of a given target model f, i.e., h(d) =
Pr(d ∈ D; f ).
(cid:2)
Data Universe
∼ P r ( X = x )
D
D(cid:2)
∼Pr(X = x)
D(cid:2)
Unsupervised
The attacker has data points that are sampled from the same underlying distribution as D. However, he does not have
information about whether a data sample has been in the target set D.
TABLE I: Various categories of inference attacks against machine learning models, based on their prior knowledge, observation, mode of
attack, and the training architecture of the target models.
(cid:24)(cid:21)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
l
e
d
o
m
k
c
a
t
t
a
s
e
r
u
t
a
e
f
k
c
a
t
t
a
CNN FCN CNN FCN
···
CNN
FCN FCN FCN
h1(x)
h1(x)
h2(x)
h2(x)
···
f (x)f (x)
∂L
∂L
∂W1
∂W1
∂L
∂L
∂W2
∂W2
···
∂L
∂L
∂Wi
∂Wi
LL
y
,
)
)
;
W
x
(
f
(
L
y
unsupervised attack component
L 1y=arg max f (x) f (x)y H(f (x))
(cid:2)(cid:2) ∂L
∂W
(cid:2)(cid:2)
Decoder (FCN)
attack output:
Encoder (FCN)
over W and x given the model. The most straightforward
functions are the outputs of the hidden layers, hi(x) on the
input x. As a simple extension, the attacker can extend black-
box membership inference attacks (which are limited to the
model’s output) to the outputs of all activation functions of
the model. However, this does not necessarily contain all the
useful information for membership inference. Notably, the
model output and activation functions could generalize if the
model is well regularized. Thus, there might not be much
difference, in distribution, between the activation functions of a
model on its training versus unseen data. This can signiﬁcantly
limit the power of the inference attacks (as we also show in
our evaluations).
What we suggest is to exploit the algorithm used to train
deep learning models: the stochastic gradient descent (SGD)
algorithm. Let L(f (x; W), y) be the loss function for the
classiﬁcation model f. During the training, the SGD algorithm
minimizes the empirical expectation of the loss function over
the training set D:
(cid:3)
(cid:4)
min
W
E(x,y)∼D
L(f (x; W), y)
(1)
The SGD algorithm solves this minimization by repeatedly
updating parameters, W, towards reducing the loss on small
randomly selected subsets of D. Thus, for any data record in