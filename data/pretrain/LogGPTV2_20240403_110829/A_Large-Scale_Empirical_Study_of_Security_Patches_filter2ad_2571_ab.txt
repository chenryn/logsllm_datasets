Session J3:  Problematic PatchesCCS’17, October 30-November 3, 2017, Dallas, TX, USA2203non-bug fix commits (136 were bug fixes). We then featurized a
commit message into a binary vector indicating the presence of
common character n-grams in the commit message. To determine
the size of n-grams, the threshold on the number of n-grams to
include, and model parameters, we ran a grid search using 10-fold
cross-validation on the training data. Our feature vector search
space considered n-grams of lengths 2 to 10 and feature vectors
that included the top 10,000 to the top 250,000 most frequently
occurring n-grams for each class. Our model parameter search
space considered both L1 and L2 regularization, with regularization
strengths ranging from 0.1 to 10, and the inclusion of a bias term.
Our final classifier utilized n-grams of lengths 3 to 9, with fea-
ture vectors corresponding to the top 50,000 most common n-grams
for each class. The model used L2 regularization with a regular-
ization strength of 1, and included a bias term. During 10-fold
cross-validation, the classifier had an average recall of 82% and
precision of 91%. While the classifier is not extremely accurate,
it results in only a small fraction of false positives and negatives,
which should have limited effect on the overall distributions of
patch characteristics. In Section 5.2, we compare characteristics of
security patches versus generic bug fixes. We manually validated
that for these characteristics, the distribution of values is similar
between our manually labeled bug fixes and our classifier-collected
bug patches, indicating that our results for classifier-labeled bug
fixes should be representative of randomly selected true bug fixes.
With our classifier, we collected a dataset of bug fixes by ran-
domly selecting per repository up to 10 commits classified as bug
fixes. (Fewer for repositories with less than 10 total commits.) We
chose to select 10 commits per repository as that provided us with
a large set of over 6,000 bug fixes (similar to our number of secu-
rity fixes) balanced across repositories. Note that in our classifier
training, security fixes were labeled as bug fixes. However, only 6%
of bug fixes in our training data (a random sample) were security-
related, thus our dataset consists almost entirely of non-security
bug fixes.2
3.4 Processing Commits
For each commit we collected (both security and non-security
patches), we extracted the historical versions of affected files both
before and after the commit. The diff between these file versions
is the patch itself. In addition, it is often useful to consider only
changes to functional source code, rather than documentation files
or source code comments. We processed the commit data using a
best-effort approach (as follows) to filter non-source code files and
remove comments, providing an alternative “cleaned” commit to
analyze.
To do so, we mapped the top 30 most frequently occurring file
extensions to the programming or templating languages associated
with them, if any (e.g., an extension of .java corresponds to Java,
whereas we assume .txt reflects non-source code). These included
2 We also investigated developing a commit message classifier to automatically distin-
guish between security and non-security fixes, using as ground truth the manually-
labeled commits as well as randomly selected CVE-related security fixes. Given the
base rate challenge arising due to the relative rarity of security fixes, we found that the
classifiers we tried did not provide nearly enough accuracy. We did not consider using
patch characteristics (such as those explored in Section 5.2) as features as we aimed to
understand how security and non-security bug fixes differed along these properties,
thus using such features would provide skewed populations.
Figure 2: CDF of the number of days the estimated disclosure
date precedes the CVE publication date.
C/C++, PHP, Ruby, Python, SQL, HTML, Javascript, Java, and Perl.
We stripped comments and trailing whitespaces under the assumed
programming language’s syntax for source code files, and filtered
out all other files. This provided a cleaned snapshot of files involved
in a commit, from which we computed a cleaned diff.
This method is ultimately best-effort,3 as we handled only the
top 30 extensions and relied on extensions as file type indicators.
However, we note that these top 30 extensions accounted for 95% of
commit files, and incorporating additional extensions would have
resulted in diminishing returns given that each extension poten-
tially required a new cleaning process. Also, in a random sample
of 100 files with a top 30 extension, all extensions corresponded
correctly to the expected file type. This is unsurprising given these
projects are open-source and often involve a number of develop-
ers, which likely discourages a practice of using non-intuitive and
non-standard file extensions.
3.5 Estimating Vulnerability Public Disclosure
Dates
Determining the public disclosure date of a vulnerability is vital to
understanding the timeline of a vulnerability’s life cycle. NVD en-
tries contain a CVE publication date that corresponds to when the
vulnerability was published in the database, not necessarily when
it was actually publicly disclosed [36]. To obtain a more accurate
estimate of the public disclosure date, we analyzed the external
references associated with CVEs. These web pages frequently con-
tain publication dates for information pertaining to vulnerabilities,
which can serve as closer estimates of the public disclosure dates.
For the CVEs corresponding to our collected security commits,
we identified the top 20 most commonly referenced sites that may
contain publication dates, listed in Table 5 in Appendix A. Of these,
two sites were no longer active (mandriva.com and vupen.com), one
did not provide fine-grained dates (oracle.com), and IBM’s Threat
Intelligence X-Force site employed aggressive anti-crawling mea-
sures. For the remaining 16 sites, we constructed per-site parsers
that extracted the date of the relevant publication for a given page.
These pages include security advisories (such as from Debian and
3 We also evaluated using the Linux “file” utility, but found it suffered from frequent
errors.
0100101102103(CVE Publication Date - Estimated Disclosure Date)in Days (Log-Scaled)0.00.20.40.60.81.0CDFSession J3:  Problematic PatchesCCS’17, October 30-November 3, 2017, Dallas, TX, USA2204Redhat), mailing list archives (e.g., marc.info, openwall.com/lists),
other vulnerability database entries (e.g., securityfocus.com, securi-
tytracker.com), and bug reports (such as Bugzilla bug tracking sites).
We restricted our crawling to the top 20 sites, as each site required
developing a new site parser, and we observed diminishing returns
as we added more sites.
We crawled about 13,600 active external references in total, ex-
tracting a publication date from 94% of pages. This provided at least
one date extracted from an external reference for 93% of CVEs, with
multiple dates extracted for 73% of CVEs. To confirm the soundness
of this strategy, we randomly sampled 100 crawled pages, finding
all relevant dates were correctly extracted.
We estimate the earliest disclosure date as the earliest amongst
the extracted reference dates and the CVE publication date. While
this is a best-effort approach, we observe that it yields significantly
improved disclosure estimation. Figure 2 plots the CDF of the num-
ber of days the estimated disclosure date precedes the CVE pub-
lication date. For approximately 8% of CVEs, we did not extract
an earlier external reference date, resulting in no improvement for
disclosure estimation. However, the median difference is nearly
a month (27 days). At the extreme, we witness differences on the
order of years. These correspond to vulnerabilities that are assigned
CVE IDs and publicly disclosed, but are not published to the NVD
until much later. For example, CVE-2013-4119 is a vulnerability in
FreeRDP that was first discussed on an OpenWall mailing list in
July, 2013 and assigned a CVE ID. However, its NVD entry was not
published until October, 2016, resulting in a large discrepancy be-
tween the CVE publication date and the true disclosure date. Thus,
our method provides us with significantly improved disclosure date
estimates.
3.6 Limitations
Vulnerability databases (VDBs) can provide rich sources of data
for analysis of security issues and fixes. However, we must bear in
mind a number of considerations when using them:
Vulnerability Granularity: By relying on the NVD, we can only
assess vulnerabilities at CVE ID granularity. While CVE IDs are
widely used, alternative metrics exist for determining what qualifies
as a distinct vulnerability [10].
Completeness: No VDB is complete, as they all draw from a lim-
ited set of sources. However, by using a VDB as expansive as the
NVD, we aim for our analysis to provide meaningful and generaliz-
able insights into vulnerabilities and security fixes.
Quality: The NVD data is manually curated and verified when
a vulnerability is assigned a CVE ID, which ideally improves the
data quality. However, given the sheer number of vulnerabilities
reported, the NVD may contain errors. Throughout our analysis,
we aim to identify and investigate anomalous data as part of our
methodology for reducing the impact of faulty information.
Source Bias: A VDB may be biased towards certain vulnerabilities
or types of software, depending on their vulnerability data sources.
Given the extensive range of software considered by the NVD,
we anticipate that our findings will remain largely applicable to
open-source software.
Reporting Bias: Security researchers may exhibit bias in what
security issues they investigate and report, potentially affecting a
VDB’s set of vulnerabilities. For example, researchers may focus
more on publishing high-severity issues, rather than low impact,
hard-to-exploit vulnerabilities. Additionally, researchers may favor
investigating certain vulnerability types, such as SQL injections or
buffer overflows. As a result, we can find raw vulnerability counts
ineffective for comparing trends in the security status of software,
and we avoid drawing conclusions from such analysis.
In addition to the above considerations, our data collec-
tion methodology introduces bias towards open-source software
projects, particularly those using Git for versioning. Thus, our find-
ings might not directly apply to other software systems, such as
closed-source ones. However, our dataset does provide a diverse
sample of 682 software projects.
Finally, our methodology and analyses do rely on some approxi-
mations. With a diverse dataset of different types of vulnerabilities
across numerous projects, we argue that approximations will often
prove necessary, as more accurate metrics would require perhaps
intractable levels of manual effort. For example, evaluating a vul-
nerability’s life span requires understanding the context about the
vulnerability type and the code logic. An automated approach, if
feasible, likely still requires developing a different method for each
vulnerability class, and perhaps each type of project. Prior case
studies [19, 20, 28] that considered vulnerability life spans relied on
manual identification of vulnerability introduction, limiting their
scope of investigation. When we do use approximations, we use
conservative methods that provide upper/lower bounds in order
to still obtain meaningful insights. However, we acknowledge that
these bounds may not fully reflect observed effects or properties.
4 DATA CHARACTERIZATION
In this section, we explore the characteristics of the selected CVEs
and the collected Git software repositories.
4.1 Vulnerability Publication Timeline
In total, we collected 4,080 security fixes for 3,094 CVEs (implying
multiple security fixes for some CVEs, an aspect we explore further
in Section 5.1.3). The earliest CVE with a collected security patch
was published on August 4, 2005, and the most recent on December
20, 2016. In Figure 3, we plot the timeline of these CVEs, bucketed
by the publication month. We observe that our CVE dataset spans
this 11 year period, although it exhibits skew towards more recent
vulnerabilities. Note that, as discussed in Section 3.6, these raw
counts do not imply that our studied software projects have become
more vulnerable over time. Rather the increase may reflect other
factors such as additional reporting by security researchers.
4.2 Affected Software Products
The NVD also enumerates software products affected by a particular
vulnerability for all CVEs in our dataset. We observe a long tail of
856 distinct products, with the top 10 listed in Table 1. The number
of products affected exceeds the number of software projects we
collected because a CVE vulnerability in one project can affect
multiple products that depend on it. Similarly we note that many of
Session J3:  Problematic PatchesCCS’17, October 30-November 3, 2017, Dallas, TX, USA2205Figure 3: Timeline of CVEs with collected security fixes,
grouped by publication month.
Figure 4: Distribution of CVSS severity scores, which are on
a scale of 0 to 10, rounded to the nearest integer.
Top Products Num. CVEs
Linux Kernel
917
211
187
170
146
134
125
121
105
77
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
Ubuntu
FFmpeg
Debian
Wireshark
openSUSE
PHP
Android
Fedora
QEMU
CWE ID
Weakness Summary
Num. CVEs
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
119
20
264
79
200
189
399
362
89
310
Buffer Overflow
Improper Input Validation
Access Control Error
Cross-Site Scripting
Information Disclosure
Numeric Error
Resource Management Error
Race Condition
SQL Injection
Cryptographic Issues
539
390
318
273
228
221
219
72
61
42
Table 1: The top 10 software products by the number of as-
sociated CVE IDs.
the top affected products are Linux distributions, as a vulnerability
that affects one distribution frequently occurs in others. This bias in
our CVE dataset towards Linux-related vulnerabilities informs us of
the importance of per-repository analysis, in addition to aggregate
analysis over all CVEs. Such analysis equally weighs the influence
of each software project on any computed metrics.
4.3 Vulnerability Severity
The NVD quantifies the severity of vulnerabilities using a stan-
dardized method called CVSS (version 2) [12, 35]. While the CVSS
standard is imperfect [25], it provides one of the few principled
ways to characterize vulnerability risk and potential impact. We
use this score as is, however acknowledging the difficulties in ob-
jectively assessing vulnerability severity.
All CVEs in our dataset are assigned CVSS severity scores, rang-
ing from 0 to 10. In Figure 4, we depict the distribution of CVSS
severity scores for these vulnerabilities, rounded to the nearest
integer. These scores reflect the severity of the vulnerability, with
0 to 3.9 deemed low severity, 4.0 to 7.9 labeled medium, and 8.0 to
10.0 regarded as highly severe. We observe that the NVD data con-
sists of vulnerabilities ranging across all severity scores. However,