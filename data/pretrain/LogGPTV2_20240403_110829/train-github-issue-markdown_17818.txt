Maxim Naumov @mnaumovfb sends in the following feedback:
* * *
I thought I would share some feedback from my recent work with
register_backward_hook function. I will describe some some inconsistencies in
it that may be worth addressing in the future.
## Background
PyTorch supports different hook functions, including register_hook,
register_forward_hook and register_backward_hook. The former is applied to a
tensor variable, while the latter two are applied to a layer module. I will
discuss the latest form here. It has the following signature
https://pytorch.org/docs/stable/nn.html `func(layer, grad_input, grad_output)`
## Details
It is extremely important to understand the meaning of gradients to
register_backwards_hook. Let a layer be defined as
     output z  (grad_output)
         ____|____
        |__layer__|
             |
     input x  (grad_input)
with overall loss E, error gradient dE/dz^(k) and weight gradient dE/dw^(k).  
First, let us assume the simplest case: a layer with no bias. Then,
    grad_output= [dE/dz^(k)]
    grad_input = [dE/dz^(k-1), dE/dw^(k)]
## Inconsistencies
It seems that there are some inconsistencies in how gradients for different
layers are handled by this function.
  1. Shape 
    * in convolution layers the weight gradient has the same shape as the weights
    * in fully connected layers the weight gradient is transpose of the weights
  2. Bias 
    * in convolution layers bias gradient are appended: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]
    * in fully connected layers bias gradient are prepended: grad_input = [dE/db^(k), dE/dz^(k-1), dE/dw^(k)]
  3. Batch size > 1 
    * in convolution layers bias gradient corresponds to the gradient over the entire batch: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]
    * in fully connected layers bias gradient corresponds to the gradient per data point j=1,...,r in the batch (therefore it needs to be added to get the gradient over the entire batch): grad_input = [[dE/db^(k,1),...,dE/db^(k,r)], dE/dz^(k-1), dE/dw^(k)]
These discrepancies can make handling of different layers, bias and batch
sizes quite cumbersome in the code. It would help if they were done more
consistently in the future.