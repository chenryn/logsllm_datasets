l
= v(R−1
1:l −1(Eθ1:l −1 ), R−1
i +1:L(Eθl +1:L ), X ).
(3)
Equation 3 shows that the θ
include information of X . How-
ever, private information of other layers Eθ1:l −1 and Eθl +1:L are also
passed to θ
.
(X )
l
(X )
l
To exclude these passed private information from other layers,
we create another model Mp by ine-tuning θl as θpl using dataset S
and by freezing all other layers of A. This lead to θ
keep learning
from S. We simply assume that the overitting efect only sightly
increase after training for a considerable number of epochs and
the training accuracy does not signiicantly increase. Therefore, by
comparing Mr and Mp, we can remove passed information from
(X )
(X )
i +1:L and obtain the exposure of private information of
θ
1:l −1
θl as
(X )
l
and θ
2.2 Model and Datasets
We use VGG-7 [6] as the DNN A, which has six convolutional layers
followed by one fully connected layer (16C3-16C3-MP-32C3-32C3-
MP-32C3-32C3-MP-64FC-10SM). Each layer is followed by ReLU
activation function.
We use three datasets: MNIST, Fashion-MNIST, and CIFAR-10.
MNIST and Fashion-MNIST include 60k training images of 28×28×1
of 10 classes of handwritten digits and clothing respectively. CIFAR-
10 includes 50k training 32 × 32 × 3 images of 10 classes. We split
each set into set S and set T . We use 20 epochs for MNIST, 40 epochs
for Fashion-MNIST, and 60 epochs for CIFAR-10.
2.3 Results and Discussion
Figure 2 shows the risk of sensitive information exposure for each
layer of VGG-7 on all three datasets. The irst layer has the lowest
risk, with the risk increasing as we go through the layers, with the
last convolutional layer having the highest sensitive information
exposure (i.e. 0.63 for both MNIST and Fashion-MNIST and 0.5 for
CIFAR-10). The last layer is a fully-connected layer which has a
lower exposure risk than its previous convolutional layer. In addi-
tion, the order of layers in terms of sensitive information exposure
is almost the same across all three datasets.
3 ON-DEVICE TEE PROTECTION
3.1 Setup
In this section, we develop an implementation and evaluate the
cost of protecting the last layers of an on-device DNN during ine-
tuning by deploying them in the TrustZone of a device (see Figure 3).
TrustZone establishes a private region on the main processor. Both
hardware and software approaches isolate this region to allow
trusted execution. We only protect the most sensitive layers of the
model because the TrustZone’s secure memory is limited and use
the normal execution environment for the other layers.
We use Darknet 1 as DNN library and Open Portable TEE 2 as
the framework for TrustZone on a Raspberry Pi 3 Model B. This
device runs TrustZone with 16 mebibytes (MiB) secure memory.
Eθl =
EMp − EMr
EMp
.
(4)
1https://pjreddie.com/darknet
2https://www.op-tee.org
Servers
Edge devices
TZ
3) Deploy
2) Encrypted
4) Decrypted
1) Pre-training the model
5) Fine-tuning or inference
5) Output 
(removed 
private
information)
Figure 3: Proposed protection for sensitive layers (last lay-
ers) of an on-device deep neural network using TrustZone.
)
s
(
e
m
i
t
n
o
i
t
u
c
e
x
E
19.0
18.5
18.0
17.5
17.0
)
B
M
(
e
g
a
s
u
y
r
o
m
e
M
28
27
26
25
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
24.5
24.0
23.5
23.0
22.5
O SM FC
1
D FC
2
MP C
3
C
4
O SM FC
1
D FC
2
MP C
3
C
4
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
34
33
32
31
O SM FC
1
D FC
2
MP C
3
Layers in TEE
C
4
O SM FC
1
D FC
2
MP C
3
Layers in TEE
C
4
(a) MNIST
(b) CIFAR-10
Figure 4: Execution time and memory usage for protect-
ing layers of VGG-7 using the TrustZone. The x-axis corre-
sponds to several last layers included in the TrustZone. O,
SM, FC, D, MP, and C refer to the cost, softmax, fully con-
nected, dropout, maxpooling, convolutional layers of VGG-
7. Number of layers with trainable parameters in the Trust-
Zone are 1, 2, 3, and 4. The dash line represent the baseline,
which runs all the layers outside the TrustZone.
The choice of Darknet is due to its high performance and small
dependencies. The developed implementation in our evaluation
is available online 3. We ine-tune the pre-trained VGG-7 with
MNIST and CIFAR-10, respectively. Several layers are deployed in
the TrustZone from the end, including both layers with (i.e. the con-
volutional and fully connected layer) and without (i.e. the dropout
and maxpooling layer) trainable parameters.
3.2 Results and Discussion
Figure 4 shows the execution time (in seconds) and memory usage
(in MB) of our implementation when securing a part of the DNN in
3https://github.com/mofanv/darknetp
the TrustZone, starting from the last layer, and continuing adding
layers until the maximum number of layers the zone can hold. The
resulting execution times are MNIST: F(7, 232) = 3658, p < 0.001;
CIFAR-10: F(7, 232) = 2396, p < 0.001 and memory usage is MNIST:
F(7, 232) = 11.62, p < 0.001; CIFAR-10: F(7, 232) = 20.01, p < 0.001.
The increase however is small compared to the baseline (Execution
time: 1.94% for MNIST and 1.62% for CIFAR-10; Memory usage:
2.43% for MNIST and 2.19% for CIFAR-10).
Speciically, deploying the dropout layer and the maxpooling
layer in the TrustZone increases both the execution time and mem-
ory usage. The reason is that these two layer types have no trainable
parameters, and for Darknet, the dropout and maxpooling are di-
rectly operated based on trainable parameters of their front layer.
Therefore, to run these two types of layers in the TrustZone, their
front layer (i.e. fully connected/convolutional layers) needs to be
copied into the TrustZone, which increases the cost. For layers
with parameters that we aim to protect (1, 2, 3, and 4 in Figure 4),
deploying fully connected layers (i.e. 1, 2) in the TrustZone does
not increase the execution time accumulated on the irst layers, as
well as the total memory usage. Deploying convolutional layers
(i.e. 3 and 4) also leads to an increase of execution time. However,
exhausting most of the available memory of the TrustZone can also
cause an increase in overhead. Overall, for our implementation, pro-
tecting fully connected and convolutional layers have lower costs
than other layers without trainable parameters with the TrustZone.
4 CONCLUSION
We proposed a method to measure the exposure of sensitive in-
formation in each layer of a pre-trained DNN model. We showed
that the closer the layer is to the output, the higher the likelihood
that sensitive information of training data is exposed, which is
opposite to the exposure risk of layers’ activation from test data [1].
We evaluated the use of TrustZone to protect individual sensitive
layers (i.e. the last layers) of a deployed DNN. The results show
that TrustZone has a promising performance at low cost.
Future work includes investigating the advantages of protect-
ing the later layers of a DNN against, among others, white-box
membership inference attacks [3].
REFERENCES
[1] Zhongshu Gu, Heqing Huang, Jialong Zhang, Dong Su, Hani Jamjoom, Ankita
Lamba, Dimitrios Pendarakis, and Ian Molloy. 2018. YerbaBuena: Securing Deep
Learning Inference Data via Enclave-based Ternary Model Partitioning. arXiv
preprint arXiv:1807.00969 (2018).
[2] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. 2019.
Exploiting unintended feature leakage in collaborative learning. IEEE.
[3] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Comprehensive Privacy
Analysis of Deep Learning: Stand-alone and Federated Learning under Passive
and Active White-box Inference Attacks. arXiv preprint arXiv:1812.00910 (2018).
[4] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. 2010.
Learnability, stability and uniform convergence. Journal of Machine Learning
Research 11, Oct (2010), 2635ś2670.
[5] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In 2017 IEEE Sympo-
sium on Security and Privacy (SP). IEEE, 3ś18.
[6] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[7] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
risk in machine learning: Analyzing the connection to overitting. In 2018 IEEE
31st Computer Security Foundations Symposium (CSF). IEEE, 268ś282.
[8] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
2017. Understanding deep learning requires rethinking generalization. In Proceed-
ings of the International Conference on Learning Representations (ICLR). France.