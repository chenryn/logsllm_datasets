### 2.1 Mathematical Formulation

The equation below shows that the parameters \(\theta_l\) include information from the input data \(X\). However, private information from other layers, \(E_{\theta_{1:l-1}}\) and \(E_{\theta_{l+1:L}}\), is also passed to \(\theta_l\).

\[ l = v(R^{-1}_{1:l-1}(E_{\theta_{1:l-1}}), R^{-1}_{i+1:L}(E_{\theta_{l+1:L}}), X) \]

To exclude this passed private information from other layers, we create another model \(M_p\) by fine-tuning \(\theta_l\) as \(\theta_{pl}\) using dataset \(S\) and by freezing all other layers of the original model \(A\). This ensures that \(\theta_l\) continues to learn from \(S\). We assume that overfitting effects only slightly increase after a considerable number of epochs, and the training accuracy does not significantly improve. Therefore, by comparing \(M_r\) and \(M_p\), we can remove the passed information from \(E_{\theta_{l+1:L}}\) and obtain the exposure of private information in \(\theta_{1:l-1}\) and \(\theta_l\).

### 2.2 Model and Datasets

We use VGG-7 [6] as the deep neural network (DNN) \(A\), which consists of six convolutional layers followed by one fully connected layer (16C3-16C3-MP-32C3-32C3-MP-32C3-32C3-MP-64FC-10SM). Each layer is followed by a ReLU activation function.

We use three datasets: MNIST, Fashion-MNIST, and CIFAR-10. MNIST and Fashion-MNIST each contain 60k training images of size 28×28×1, representing 10 classes of handwritten digits and clothing, respectively. CIFAR-10 includes 50k training images of size 32×32×3, representing 10 classes. We split each dataset into set \(S\) and set \(T\). We train for 20 epochs on MNIST, 40 epochs on Fashion-MNIST, and 60 epochs on CIFAR-10.

### 2.3 Results and Discussion

Figure 2 illustrates the risk of sensitive information exposure for each layer of VGG-7 across all three datasets. The first layer has the lowest risk, with the risk increasing through the layers, reaching its highest in the last convolutional layer (0.63 for both MNIST and Fashion-MNIST, and 0.5 for CIFAR-10). The fully-connected layer, which follows, has a lower exposure risk than the previous convolutional layer. The order of layers in terms of sensitive information exposure is consistent across all three datasets.

### 3 ON-DEVICE TEE PROTECTION

#### 3.1 Setup

In this section, we develop an implementation and evaluate the cost of protecting the last layers of an on-device DNN during fine-tuning by deploying them in the TrustZone of a device (see Figure 3). TrustZone establishes a secure region on the main processor, isolated by both hardware and software to allow trusted execution. We protect only the most sensitive layers due to the limited secure memory of TrustZone, while the other layers run in the normal execution environment.

We use Darknet [1] as the DNN library and Open Portable TEE [2] as the framework for TrustZone on a Raspberry Pi 3 Model B. This device runs TrustZone with 16 MiB of secure memory.

\[ E_{\theta_l} = \frac{E_{M_p} - E_{M_r}}{E_{M_p}} \]

[1]: https://pjreddie.com/darknet
[2]: https://www.op-tee.org

**Figure 3: Proposed protection for sensitive layers (last layers) of an on-device deep neural network using TrustZone.**

**Figure 4: Execution time and memory usage for protecting layers of VGG-7 using the TrustZone. The x-axis corresponds to several last layers included in the TrustZone. O, SM, FC, D, MP, and C refer to the cost, softmax, fully connected, dropout, maxpooling, and convolutional layers of VGG-7, respectively. The number of layers with trainable parameters in the TrustZone are 1, 2, 3, and 4. The dashed line represents the baseline, which runs all the layers outside the TrustZone.**

The choice of Darknet is due to its high performance and minimal dependencies. The developed implementation is available online [3]. We fine-tune the pre-trained VGG-7 with MNIST and CIFAR-10, respectively. Several layers are deployed in the TrustZone from the end, including both layers with (i.e., convolutional and fully connected layers) and without (i.e., dropout and maxpooling layers) trainable parameters.

[3]: https://github.com/mofanv/darknetp

#### 3.2 Results and Discussion

Figure 4 shows the execution time (in seconds) and memory usage (in MB) of our implementation when securing a part of the DNN in the TrustZone, starting from the last layer and adding more layers until the maximum capacity of the TrustZone is reached. The resulting execution times are MNIST: F(7, 232) = 3658, p < 0.001; CIFAR-10: F(7, 232) = 2396, p < 0.001, and memory usage is MNIST: F(7, 232) = 11.62, p < 0.001; CIFAR-10: F(7, 232) = 20.01, p < 0.001. The increase is small compared to the baseline (Execution time: 1.94% for MNIST and 1.62% for CIFAR-10; Memory usage: 2.43% for MNIST and 2.19% for CIFAR-10).

Specifically, deploying the dropout and maxpooling layers in the TrustZone increases both the execution time and memory usage. This is because these layers have no trainable parameters, and in Darknet, they are directly operated based on the trainable parameters of their preceding layer. Thus, to run these layers in the TrustZone, their preceding layer (i.e., fully connected/convolutional layers) needs to be copied into the TrustZone, increasing the cost. For layers with parameters that we aim to protect (1, 2, 3, and 4 in Figure 4), deploying fully connected layers (i.e., 1, 2) in the TrustZone does not increase the execution time or total memory usage. Deploying convolutional layers (i.e., 3 and 4) also leads to an increase in execution time. However, exhausting most of the available memory of the TrustZone can also cause an increase in overhead. Overall, for our implementation, protecting fully connected and convolutional layers has lower costs than other layers without trainable parameters.

### 4 CONCLUSION

We proposed a method to measure the exposure of sensitive information in each layer of a pre-trained DNN model. We showed that the closer the layer is to the output, the higher the likelihood that sensitive information of training data is exposed, which is opposite to the exposure risk of layers' activation from test data [1]. We evaluated the use of TrustZone to protect individual sensitive layers (i.e., the last layers) of a deployed DNN. The results show that TrustZone has a promising performance at a low cost.

Future work includes investigating the advantages of protecting the later layers of a DNN against, among others, white-box membership inference attacks [3].

### REFERENCES

[1] Zhongshu Gu, Heqing Huang, Jialong Zhang, Dong Su, Hani Jamjoom, Ankita Lamba, Dimitrios Pendarakis, and Ian Molloy. 2018. YerbaBuena: Securing Deep Learning Inference Data via Enclave-based Ternary Model Partitioning. arXiv preprint arXiv:1807.00969 (2018).

[2] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. 2019. Exploiting unintended feature leakage in collaborative learning. IEEE.

[3] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attacks. arXiv preprint arXiv:1812.00910 (2018).

[4] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. 2010. Learnability, stability and uniform convergence. Journal of Machine Learning Research 11, Oct (2010), 2635–2670.

[5] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 3–18.

[6] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).

[7] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF). IEEE, 268–282.

[8] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2017. Understanding deep learning requires rethinking generalization. In Proceedings of the International Conference on Learning Representations (ICLR). France.