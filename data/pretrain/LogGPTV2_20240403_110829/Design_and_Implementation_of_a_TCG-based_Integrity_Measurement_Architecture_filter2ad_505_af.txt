reference
no SHA1
SHA1
SHA1+extend
sys fs
open/write/close
Overhead (stdev)
4.32 µs (0.0)
7.50 µs (0.0)
5430 µs (1.6)
4.32 µs (0.0)
Table 2: Latency of user level measurements via sysfs (ﬁle
size 2 bytes).
user-level measurement latency is 4.32 µs in the no_SHA1
case. This overhead is mostly ﬁle system related overhead
–open, write, close of /sys/security/measure as
the reference value in Table 2 indicates. The measurement-
related overhead for the no_SHA1 case simply disappears
in the context switching and ﬁle system related overhead.
Interpreting the other measurement values is straightforward.
Measuring kernel modules can be done in two ways as
described in Section 5.1: by user-level applications insmod
and modprobe, or by inducing a measurement routine before
relocating the kernel module in the load_module func-
tion called by the init_module system call. Measuring
them via insmod or modprobe transfers kernel module mea-
surement performance into the domain of user-level measure-
ments with the overhead as described in Table 2. The latency
of measuring kernel modules in the load_module kernel
function is almost the same as the latency of measuring exe-
cutable content in the file_mmap LSM measurement hook.
However, because kernel modules are already in memory be-
fore they are relocated, there is no dirty ﬂagging informa-
tion and we do not have clean hits but only the cases SHA1
or SHA1+extend. We consider kernel module loading an
infrequent and less time critical event and thus recommend
from a security standpoint (see Section 5.1) that they be mea-
sured in the kernel.
Next, we present the ﬁngerprinting performance as a func-
tion of ﬁle sizes. We measure the mmap system call’s running
time in the SHA1 case, varying the input ﬁle sizes. This in-
cludes the reference overhead of 1.65 µs for the pure mmap
system call as shown in Table 1. The results are shown in Ta-
ble 3. When the ﬁle size is large, the ﬁngerprinting overhead
can be signiﬁcant. For example, measuring a 128 Kilobytes
ﬁle takes about 1.5 milliseconds. The running time increases
close to a linear fashion as the size of ﬁle increases. These la-
tencies translate to a throughput performance of about 80 MB
per second.
File Size (Bytes) Overhead (stdev)
4.21 µs (0.0)
2
512
10.3 µs (0.0)
16.3 µs (0.0)
1K
197 µs (0.1)
16K
1550 µs (1.1)
128K
1M
12700 µs (16)
Table 3: Performance of the SHA1 Fingerprinting Operation
as a Function of File Sizes.
Measuring in-memory kernel modules, we expect slightly
better throughput in computing the SHA1 than measuring
ﬁles –which ﬁst have to be read from disk into memory– in
the file_mmap LSM hook as described in Table 1. How-
ever, our measurements yielded only slightly better perfor-
mance than in the file_mmap case shown in Table 3. We
explain this with the Linux ﬁle caching effect. The measure-
ments were done many times with a hot cache on the same
ﬁle, which makes it very likely, that almost the complete ﬁle
was already residing in the ﬁle cache when the measurement
started. This also suggests that the throughput numbers in Ta-
ble 1 should be considered a optimistic for ﬁle measurements.
These experiments were run with a measurement list con-
taining about 1000 entries on an IBM Netvista M desktop
workstation, including an Intel Pentium 2.4 GHz proces-
sor and 1 GByte of RAM. All non-essential services where
stopped.
6.3 Implementation and Usability Aspects
Our kernel implementation includes LSM hooks for mea-
surement, dirty ﬂagging, and bypass protection and com-
prises 4755 lines of code (loc) including comments. This
code resides in its own security/measure kernel direc-
tory and is thus very easy to port to new Linux kernel ver-
sions as long as the LSM interface does not change. We
need to add another 2 loc into the load_module routine
of kernel/module.c to measure loading kernel modules.
To instrument the bash shell, we insert 2 loc at the places
where source ﬁles are loaded or script ﬁles are interpreted.
These user level measure calls are based on a header ﬁle of 42
loc that translates the user level measure request macro into a
proper write on /sys/security/measure. Porting the
architecture from a 2.6.2 to a 2.6.5 Linux kernel took about 10
minutes. Moving from a non-LSM implementation in a 2.4
kernel to an LSM-based version of our integrity measurement
architecture in the 2.6 kernel reduced the complexity of our
implementation and increased its portability considerably.
We have successfully stacked our integrity measurement
architecture as an LSM module on top of SELinux, which re-
quired small modiﬁcations of SELinux to call our hooks and
to share security substructures in the file and inode kernel
structures. These changes are minor but they are necessary
because the current Linux LSM implementation leaves most
of the stacking implementation to the modules themselves.
Our experiences show that a standard RedHat 9.0 Linux
system including the Xwindow server and the Gnome Desk-
top system accumulates about 500-600 measurement entries
after running about one week, including about 60-100 bash
script and source ﬁle measurements. Those bash measure-
ments cover all bash service startup and shutdown scripts as
well as local source scripts (e.g., ˜\.bashrc). The over-
head introduced by our measurement architecture is negligi-
ble even at boot time of the system when most measurements
are recorded and extended into the TPM. Thus we believe our
performance results are representative of a normal Linux en-
vironment.
7 Discussion
Our architecture is non-intrusive and does not prevent sys-
tems from running malicious programs. However, we modify
our approach to enforce security as well.
In this case, we
pre-load the measurement cache with a set of expected ﬁn-
gerprints for trusted programs. The measurement call then
ﬁngerprints the ﬁle to be measured and compares it to the
set of expected ﬁngerprints. If the ﬁngerprint does not match
any of them, it aborts the load and reports the illegal ﬁnger-
print. Note that the attesting system’s enforcement require-
ments may be different than those of the challenger, so the
challenger still needs to perform a validation.
Our measurement architecture is not restricted to mea-
suring executable code. Adding measurement hooks into
applications, we can include structured input data, such
as conﬁguration ﬁles and java classes, into our measure-
ments. Changes are simple–instrumenting applications, such
as Apache or the Java classloader, means adding a measure-
ment call before loading relevant ﬁles.
In order to establish conﬁdence in a system, privacy is im-
pacted by our approach. The attestation protocol releases de-
tailed information of the attesting system to allow challengers
or trusted third parties to establish trust. However, the attest-
ing system has full control over the release of this informa-
tion, and can run code that it trusts not to release such in-
formation. Also, a system agent could be conﬁgured to re-
lease attestations to authenticated challengers and the operat-
ing system could only provide quotes to that agent.
Inducing frequent changes in loaded executable ﬁles can
cause the measurement list to grow beyond practical lim-
its, resulting in a denial of service attack. To prevent this
attack, a maximum length of the measurement list can be
conﬁgured. Any additional measurement is aggregated into
the TPM-protected PCR register, but the measurement is not
stored in the kernel. Consequently, a system that exceeds this
maximum number of measurements will not be able to suc-
cessfully convince challenging parties of its integrity because
the measurement list will not validate against the aggregate
any more.
8 Conclusions
We presented the design and implementation of a secure in-
tegrity measurement system for Linux. This system extends
the TCG trust concepts from the BIOS all the way up into the
application layer for a general operating system. We extend
the operating system with hooks to measure when the ﬁrst
code is loaded into a process (file_mmap LSM hook), pro-
vide a measure sysfs entry to request subsequent measure-
ments, and detect when changes to measured inodes occur.
This mechanism enables the measurement of dynamic load-
ers, shared libraries, and kernel modules in addition to the
executed ﬁles. Further, the approach is extensible, such that
applications can measure their specialized loads as shown for
bash. The result is that we show that many of the Microsoft
NGSCB guarantees can be obtained on today’s hardware and
today’s software and that these guarantees do not require a
new CPU mode or operating system but merely depend on
the availability of an independent trusted entity. Such a sys-
tem can already detect a variety of integrity issues, such as
the presence of rootkits or vulnerable software. Our measure-
ments show that the non-development systems can be practi-
cally measured and that the measurement overhead is reason-
able.
The measurement system is extensible and we believe that
we can ultimately achieve guarantees beyond those of Mi-
crosoft NGSCB. The application of mandatory access control
policy can ensure that dynamic data cannot be modiﬁed ex-
cept by trusted sources [17]. Identiﬁcation of low integrity
data ﬂows can enable the possibility of control over whether
these ﬂows should be allowed, whether effective restriction
can be put on them at the system-level or within applications.
We are currently in the process of making the source code
of our integrity measurement architecture implementation
publicly available as open-source and pursue efforts to inte-
grate it into the kernel as an optional LSM kernel module.
[8] G. Kim and E. Spafford, “Experience with Tripwire:
Using Integrity Checkers for Intrusion Detection,” in
System Administration, Networking, and Security Con-
ference III. USENIX, 1994.
[9] D. Engler, B. Chelf, A. Chou, and S. Hallem, “Check-
ing systems rules using system-speciﬁc, programmer-
written compiler extensions,” in Proceedings of the 4th
Symposium on Operating Systems Design and Imple-
mentation (OSDI 2000), October 2000.
[10] J. Dyer, M. Lindemann, R. Perez, R. Sailer, L. van
Doorn, S. W. Smith, and S. Weingart, “Building the
IBM 4758 Secure Coprocessor,” IEEE Computer, vol.
34, no. 10, pp. 57–66, 2001.
Acknowledgments
The authors would like to thank the IBM Linux Technology
Center for their continuing and invaluable support and our
colleagues from the IBM Tokyo Research Lab, particularly
Seiji Munetoh and his colleagues, for interesting discussions
and for their TPM-enhancement of the grub boot loader. Fi-
nally, we would like to thank Ronald Perez, Steve Bade, and
the anonymous referees for their useful comments.
References
[1] W. A. Arbaugh, D. J. Farber, and J. M. Smith,
“A
Secure and Reliable Bootstrap Architecture,” in IEEE
Computer Society Conference on Security and Privacy.
IEEE, 1997, pp. 65–71.
[2] “Trusted
Computing
http://www.trustedcomputinggroup.org.
Group,”
[3] K. J. Biba, “Integrity considerations for secure com-
puter systems,” Tech. Rep. MTR-3153, Mitre Corpora-
tion, Mitre Corp, Bedford MA, June 1975.
[4] D. D. Clark and D. R. Wilson, “A comparison of com-
in
mercial and military computer security policies,”
IEEE Symposium on Security and Privacy, 1987.
[5] S. W. Smith,
“Outgoing authentication for pro-
grammable secure coprocessors,” in ESORICS, 2002,
pp. 72–89.
[6] M. Bond, “Attacks on cryptoprocessor transaction sets,”
in Proceedings of the 2001 Workshop on Cryptographic
Hardware and Embedded Systems, May 2001.
[7] P. England, B. Lampson, J. Manferdelli, M. Peinado,
and B. Willman, “A Trusted Open Platform,” IEEE
Computer, vol. 36, no. 7, pp. 55–62, 2003.
[11] Trusted Computing Group,
Trusted Platform Mod-
ule Main Speciﬁcation, Part 1: Design Princi-
ples, Part 2:
Com-
Version 1.2, Revision 62,
mands, October 2003,
http://www.trustedcomputinggroup.org.
TPM Structures, Part 3:
[12] H. Maruyama, F. Seliger, N. Nagaratnam, T. Ebringer,
S. Munetho, and S. Yoshihama, “Trusted Platform on
demand (TPod),” in Technical Report, Submitted for
Publication, 2004, In submission.
[13] J. Marchesini, S. Smith, O. Wild, and R. MacDonald,
“Experimenting with TCPA/TCG Hardware, Or: How I
Learned to Stop Worrying and Love the Bear,” in Tech-
nical Report TR2003-476, Dartmouth PKI Lab Dart-
mouth College, Hanover, New Hampshire, USA, De-
cember 2003.
[14] T. Garﬁnkel, B. Pfaff, J. Chow, M. Rosenblum, and
D. Boneh, “Terra: A Virtual Machine-Based Platform
for Trusted Computing,” in Proc. 9th ACM Symposium
on Operating Systems Principles, 2003, pp. 193–206.
[15] CERT Coordinatin Center,
“CERT/CC Advisories,”
http://www.cert.org/advisories.
[16] A. B. Brown and M. Seltzer, “Operating System Bench-
marking in the Wake of Lmbench: A Case Study of the
Performance of NetBSD on the Intel x86 Architecture,”
in Proceedings of the 1997 ACM SIGMETRICS Confer-
ence on Measurement and Modeling of Computer Sys-
tems, June 1997, pp. 214–224.
[17] T. Jaeger et. al., “Leveraging information ﬂow for in-
tegrity veriﬁcation,” in SUBMITTED for publication,
2004.