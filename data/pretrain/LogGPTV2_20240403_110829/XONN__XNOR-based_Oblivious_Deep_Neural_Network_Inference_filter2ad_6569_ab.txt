label of the output wire is encrypted using the input labels
according to the truth table of the gate. For example, con-
sider an AND gate with input wires a and b and output wire
c. The last row of the garbled table is the encryption of Lc
1
using labels La
1 and Lb
1.
Once the garbling process is ﬁnished, the Garbler sends
all of the garbled tables to the Evaluator. Moreover, he sends
the correct labels that correspond to input wires that repre-
sent his inputs to the circuit. For example, if wire w∗ is the
ﬁrst input bit of the Garbler and his input is 0, he sends L∗
0.
The Evaluator acquires the labels corresponding to her in-
put through 1-out-of-2 OT where Garbler is the sender with
two labels as his messages and the Evaluator’s selection bit
is her input for that wire. Having all of the garbled tables
and labels of input wires, the Evaluator can start decrypting
the garbled tables one by one until reaching the ﬁnal output
bits. She then learns the plaintext result at the end of the GC
protocol based on the output labels and their relationships to
the semantic values that are received from the Garbler.
3 The XONN Framework
In this section, we explain how neural networks can be
trained such that they incur a minimal cost during the oblivi-
ous inference. The most computationally intensive operation
in a neural network is matrix multiplication.
In GC, each
multiplication has a quadratic computation and communi-
cation cost with respect to the input bit-length. This is the
major source of inefﬁciency in prior work [13]. We over-
come this limitation by changing the learning process such
that the trained neural network’s weights become binary. As
Numerical optimization algorithms minimize a speciﬁc cost
function associated with neural networks. It is well-known
that neural network training is a non-convex optimization,
meaning that there exist many locally-optimum parameter
conﬁgurations that result in similar inference accuracies.
Among these parameter settings, there exist solutions where
both neural network parameters and activation units are re-
stricted to take binary values (i.e., either +1 or −1); these so-
lutions are known as Binary Neural Netowrks (BNNs) [19].
One major shortcoming of BNNs is their (often) low infer-
ence accuracy. In the machine learning community, several
methods have been proposed to modify BNN functionality
for accuracy enhancement [20, 21, 22]. These methods are
devised for plaintext execution of BNNs and are not efﬁcient
for oblivious inference with GC. We emphasize that, when
modifying BNNs for accuracy enhancement, one should also
take into account the implications in the corresponding GC
circuit. With this in mind, we propose to modify the num-
ber of channels and neurons in CONV and FC layers, re-
spectively. Increasing the number of channels/neurons leads
to a higher accuracy but it also increases the complexity
of the corresponding GC circuit. As a result, XONN pro-
vides a trade-off between the accuracy and the communica-
tion/runtime of the oblivious inference. This tradeoff enables
cloud servers to customize the complexity of the GC proto-
col to optimally match the computation and communication
requirements of the clients. To customize the BNN, XONN
conﬁgures the per-layer number of neurons in two steps:
• Linear Scaling: Prior to training, we scale the number of
channels/neurons in all BNN layers with the same factor
(s), e.g., s = 2. Then, we train the scaled BNN architecture.
• Network Trimming: Once the (uniformly) scaled network
is trained, a post-processing algorithm removes redundant
channels/neurons from each hidden layer to reduce the GC
cost while maintaining the inference accuracy.
Figure 1 illustrates the BNN customization method for an
example baseline network with four hidden layers. Network
trimming (pruning) consists of two steps, namely, Feature
Ranking and Iterative Pruning which we describe next.
Feature Ranking:
In order to perform network trimming,
one needs to sort the channels/neurons of each layer based on
their contribution to the inference accuracy. In conventional
neural networks, simple ranking methods sort features based
1504    28th USENIX Security Symposium
USENIX Association
Layer 1 Layer 2 Layer 3 Layer 4
Layer 1 Layer 2 Layer 3 Layer 4
P
r
u
n
e
Algorithm 1 XONN Channel Sorting for CONV Layers
Inputs: Trained BNN with loss function L , CONV layer l
with output shape of h1 × h2 × f , subsampled validation
data and labels {(X1, z1), . . . , (Xk, zk)}
Output: Indices of the sorted channels: {i0, . . . , i f }
Scale (s=2)
Per-layer Neurons
Layer 1 Layer 2 Layer 3 Layer 4
Scale (s=3)
6
4
Layer 1 Layer 2 Layer 3 Layer 4
8
5
4
Layer 1 Layer 2 Layer 3 Layer 4
6
9
2
1
3 2 4 6
Figure 1: Illustration of BNN customization. The bars rep-
resent the number of neurons in each hidden layer.
P
r
u
n
e
7
9
6
6
0
1
2
1
2
1
8
1
on absolute value of the neurons/channels [23]. In BNNs,
however, the weights/features are either +1 or −1 and the
absolute value is not informative. To overcome this issue, we
utilize ﬁrst order Taylor approximation of neural networks
and sort the features based on the magnitude of the gradient
values [24]. Intuitively, the gradient with respect to a certain
feature determines its importance; a high (absolute) gradient
indicates that removing the neuron has a destructive effect on
the inference accuracy. Inspired by this notion, we develop a
feature ranking method described in Algorithm 1.
Iterative Pruning: We devise a step-by-step algorithm for
model pruning which is summarized in Algorithm 2. At
each step, the algorithm selects one of the BNN layers l∗
and removes the ﬁrst p∗ features with the lowest importance
(line 17). The selected layer l∗ and the number of pruned
neurons p∗ maximize the following reward (line 15):
reward(l, p) =
ccurr − cnext
eacurr−anext
,
(3)
where ccurr and cnext are the GC complexity of the BNN be-
fore and after pruning, whereas, acurr and anext denote the
corresponding validation accuracies. The numerator of this
reward encourages higher reduction in the GC cost while
the denominator penalizes accuracy loss. Once the layer
is pruned, the BNN is ﬁne-tuned to recover the accuracy
(line 18). The pruning process stops once the accuracy drops
below a pre-deﬁned threshold.
3.2 Oblivious Inference
BNNs are trained such that the weights and activations are
binarized, i.e., they can only have two possible values: +1
or −1. This property allows BNN layers to be rendered using
a simpliﬁed arithmetic. In this section, we describe the func-
tionality of different layer types in BNNs and their Boolean
circuit translations. Below, we explain each layer type.
Binary Linear Layer: Most of the computational com-
plexity of neural networks is due to the linear operations in
CONV and FC layers. As we discuss in Section 2.1, linear
operations are realized using vector dot product (VDP). In
BNNs, VDP operations can be implemented using simpliﬁed
circuits. We categorize the VDP operations of this work into
◃ deﬁne gradient tensor
◃ evaluate loss function
◃ compute gradient w.r.t. layer output
◃ store gradient
◃ take elementwise absolute values
◃ deﬁne sum of absolute values
1: G ← zeros(k × h1 × h2 × f )
2: for i = 1, . . . , k do
L = L (Xi, zi)
3:
∇Y = ∂ L
∂Y l
G[i, :, :, :] ← ∇Y
4:
5:
6: end for
7: Gabs ←| G|
8: gs ← zeros( f )
9: for i = 1, . . . , f do
10:
11: end for
12: {i0, . . . , i f }← sort(gs)
13: return {i0, . . . , i f }
gs[i] ← sum(Gabs[:, :, :, i])
two classes: (i) Integer-VDP where only one of the vectors is
binarized and the other has integer elements and (ii) Binary-
VDP where both vectors have binary (±1) values.
Integer-VDP: For the ﬁrst layer of the neural network, the
server has no control over the input data which is not nec-
essarily binarized. The server can only train binary weights
and use them for oblivious inference. Consider an input vec-
tor x ∈ Rn with integer (possibly ﬁxed-point) elements and
a weight vector w ∈ {−1, 1}n with binary values. Since the
elements of the binary vector can only take +1 or −1, the
Integer-VDP can be rendered using additions and subtrac-
tions. In particular, the binary weights can be used in a se-
lection circuit that decides whether the pertinent integer in-
put should be added to or subtracted from the VDP result.
+1 +1 -1 -1
-1 +1 -1 -1
1
0
1
1
0
0
0
0
MULT
-1 +1 +1 +1
SUM
+2
XNOR
0
1
1
1
PopCount
+2
Figure 2: Equivalence of Binary-VDP and XnorPopcount.
Binary-VDP: Consider a dot product between two binary
vectors x ∈ {−1, +1}n and w ∈ {−1, +1}n. If we encode
each element with one bit (i.e., −1 → 0 and +1 → 1),
we obtain binary vectors xb ∈{ 0, 1}n and wb ∈{ 0, 1}n.
It has been shown that the dot product of x and w can
be efﬁciently computed using an XnorPopcount opera-
tion [19]. Figure 2 depicts the equivalence of VDP(x, w) and
USENIX Association
28th USENIX Security Symposium    1505
Algorithm 2 XONN Iterative BNN Pruning
Inputs: Trained BNN with n overall CONV and FC layers, minimum accuracy threshold θ , number of pruning trials per layer
t, subsampled validation data and labels dataV , training data and labels dataT
Output: BNN with pruned layers
BNNnext ← Prune(BNN, l, p, inds)
anext ← Accuracy(BNNnext, dataV |p[1], . . . , p[l] = p, . . . , p[n − 1])
cnext ← Cost(BNNnext|p[1], . . . , p[l] = p, . . . , p[n − 1])
reward(l, p) = ccurr−cnext
e(acurr −anext )
◃ current number of pruned neurons/channels per layer
◃ current BNN validation accuracy
◃ current GC cost
◃ repeat until accuracy drops below θ
◃ search over all layers
◃ rank features via Algorithm 1
◃ number of output neurons/channels
◃ search over possible pruning rates
◃ prune p features with lowest ranks from the l-th layer
◃ validation accuracy if pruned
◃ GC cost if pruned
◃ compute reward given that p features are pruned from layer l
◃ select layer l∗ and pruning rate p∗ that maximize the reward
◃ update the number of pruned features in vector p
◃ prune p∗ features with lowest ranks from the l∗-th layer
◃ ﬁne-tune the pruned model using training data to recover accuracy
◃ update current BNN validation accuracy
◃ update current GC cost
1: p ← zeros(n − 1)
2: acurr ← Accuracy(BNN, dataV |p)
3: ccurr ← Cost(BNN|p)
4: while acurr > θ do
5:
for l = 1, . . . , n − 1 do
inds ← Rank(BNN, l, dataV )
f ← Number of neurons/channels
for p = p[l], p[l] + f
t , . . . , f do
end for
end for
{l∗, p∗}← arg maxl,p reward(l, p)
p[l∗] ← p∗
BNN ← Prune(BNN, l∗, p∗, inds)
BNN ← Fine-tune(BNN, dataT )
acurr ← Accuracy(BNN, dataV |p)
ccurr ← Cost(BNN|p)
20:
21: end while
22: return BNN
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
XnorPopcount(xb, wb) for a VDP between 4-dimensional
vectors. First, element-wise XNOR operations are performed
between the two binary encodings. Next, the number of set
bits p is counted, and the output is computed as 2p − n.
Binary Activation Function: A Binary Activation (BA)
function takes input x and maps it to y = Sign(x) where
Sign(·) outputs either +1 or −1 based on the sign of its input.
This functionality can simply be implemented by extracting
the most signiﬁcant bit of x.
Binary Batch Normalization: in BNNs, it is often useful to
normalize feature x using a Batch Normalization (BN) layer
before applying the binary activation function. More specif-
ically, a BN layer followed by a BA is equivalent to:
y = Sign(γ · x + β ) = Sign(x +
β
γ
),
since γ is a positive value. The combination of the two layers
(BN+BA) is realized by a comparison between x and − β
γ .
Binary Max-Pooling: Assuming the inputs to the max-
pooling layers are binarized, taking the maximum in a win-
dow is equivalent to performing logical OR over the binary
encodings as depicted in Figure 3. Note that average-pooling
layers are usually not used in BNNs since the average of mul-
tiple binary elements is no longer a binary value.
MAX
1
1
0
0
1
0
1
0
0
1
0
0
0
0
0
0
OR
1
1
1
0
Figure 3: The equivalence between Max-Pooling and
Boolean-OR operations in BNNs.
Figure 4 demonstrates the Boolean circuit for Binary-VDP
followed by BN and BA. The number of non-XOR gates for
binary-VDP is equal to the number of gates required to ren-
der the tree-adder structure in Figure 4. Similarly, Figure 5
shows the Integer-VDP counterpart. In the ﬁrst level of the
tree-adder of Integer-VDP (Figure 5), the binary weights de-
termine whether the integer input should be added to or sub-
tracted from the ﬁnal result within the “Select” circuit. The
next levels of the tree-adder compute the result of the integer-
VDP using “Adder” blocks. The combination of BN and
BA is implemented using a single comparator. Compared
to Binary-VDP, Integer-VDP has a high garbling cost which
is linear with respect to the number of bits. To mitigate this
problem, we propose an alternative solution based on Obliv-
ious Transfer (OT) in Section 3.3.
1506    28th USENIX Security Symposium