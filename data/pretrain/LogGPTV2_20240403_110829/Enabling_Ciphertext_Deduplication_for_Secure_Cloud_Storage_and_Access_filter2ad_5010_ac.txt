owner of the ﬁle). If the user is authorized, MM looks up the
ﬁle identiﬁer in the ﬁle table in order to get the pointer to the
ﬁrst block of the ﬁle. Then, MM visits the linked list in order
to retrieve all the blocks that compose the ﬁle. For each of
these blocks, MM retrieves the pointer from the pointer table
and sends a request to SP.
SP SP returns the content of the encrypted blocks to MM.
B(cid:48)(cid:48)
i = EKA (EKi(Bi)).
MM MM builds a response which contains all the blocks,
keys and signatures of ﬁle F1. Signatures are retrieved from
the signature table. The response is structured as follows:
•
•
•
ﬁle identiﬁer: Fid1;
ﬁrst data block : EKA (EK1(B1));
for each following data block Bi(i ≥ 2): key to
decrypt block Bi, that is EKA (EKUj
(EKi−1(Ki)));
signature of block Bi, that is EKA (Si); data block
B(cid:48)(cid:48)
: EKA(EKi (Bi));
i
MM sends the response to the server.
SERVER The server decrypts blocks, signatures and
keys with KA.
the signature veriﬁcation does not
fail,
the server sends a response to Uj. Each key-block
pair received by the user, will be structured as follows:
(EKi−1 (Ki)), EKi (Bi)}.
{EKUj
If
USER Uj can ﬁnally decrypt blocks and keys. Uj already
knows the key corresponding to the block B1. For each data
block Bi, Uj decrypts block B(cid:48)
i using Ki and Ki+1 using KUj
and Ki. Uj can ﬁnally rebuild the original ﬁle F1.
VII. EVALUATION
In this section we evaluate the overhead introduced by
our system in terms of storage space and computational
complexity. We also evaluate ClouDedup’s resilience against
potential attacks. In order to refer to a real scenario, we use
the same parameters of [23], but our calculations hold true for
other scenarios.
A. Storage Space
Fig. 5. Overhead of metadata management with encryption
We took into account a scenario in which there are 857 ﬁle
systems. The mean number of ﬁles per ﬁle system is 225K and
the mean size of a ﬁle is 318K, resulting in about 57T of data.
In our design, we use SHA256 as hash function so the key size
of each block is 256 bits. Metadata storage space is estimated
by taking into account four main data structures:
•
•
•
•
File table. The ﬁle table stores one record for each
ﬁle and contains the ﬁle id (256 bits), ﬁle name (256
bits), user id (32 bits) and the id of the ﬁrst data block
(256 bits).
Pointer table. The pointer table stores one record for
each block and contains the block id (256 bits) and
the id of the actual block stored at the cloud storage
provider (64 bits).
Signature table. The signature table stores one record
for each block (non-deduplicated) and contains the
block id (256 bits), the ﬁle id (256 bits) and the
signature (2048 bits for the ﬁrst block, 128 bits for
the remaining blocks).
Linked list. The linked list contains one node (256
bits) and zero or more links for each block. A link
contains a pointer (64 bits) to a successor block for
a given ﬁle and stores additional information such as
encrypted block keys (256 bits) and ﬁle id (256 bits).
According to the results of [23], Rabin 8K (expected block
size of 8K) has proved to be the best chunking algorithm,
achieving 68% of space savings. In Fig. 5 we show that the
overhead introduced by the MM component is minimal and
does not affect space savings of deduplication. In the best
deduplication setup (Rabin 8K and deduplication rate of 68%)
the total storage space required for metadata is equal to 2.22%
of the size of non-deduplicated data. These results prove that
the overhead for block-level deduplication is affordable even
with encryption.
B. Computation
We analyze the computational complexity of the two most
important operations: storage and retrieval. N is the mean
number of blocks per ﬁle and M the total number of blocks
in the system.
Storage
O(N )
O(N )
O(N )
Retrieval
O(N )
O(N )
O(N )
O(N )
Encryption
Hash
Lookup in data structures O(N log M )
Other
1) Storage: The ﬁrst step of the storage protocol requires
the server to encrypt Bi, Ki and Si. As the encryption is
symmetric, the cost of each encryption can be considered
constant, so for N blocks the total cost is O(N ). The second
step of the protocol requires the metadata manager to hash each
block in order to compare it with the ones already stored. As
for symmetric encryption, the total cost is O(N ). In order to
perform deduplication, MM has to check if a block has already
been stored. In order to do so, he searches (dichotomic search)
for a given hash in a pre-ordered table of hash values. The cost
of this operation is O(log M ) and it is performed for each
block. The cost of the update of the data structures can be
considered constant. The last (optional) step of the protocol is
the encryption at the additional HSM, which symmetrically
encrypts at most N blocks. The total cost of the storage
operation is linear for the encryption operations and almost
linear for the lookup in data structures, therefore the metadata
management is scalable.
2) Retrieval: The ﬁrst step of the retrieval protocol requires
the metadata manager to compute a hash of the concatenation
of user id and ﬁle name. The cost of this operation can be
considered constant. Even the lookup in the ﬁle table, in order
to get the pointer to the ﬁrst block of the ﬁle, has a constant
cost. Visiting the linked list, searching in the tables and sending
a request to the cloud storage provider, have a constant cost and
are repeated N times. Once again, the cost of the symmetric
decryptions is constant, hence the complexity remains linear.
The signature veriﬁcation process requires the server to verify
one signature and compute N−1 hashes, hence the cost of this
operation is linear. The total cost of the retrieval operation is
linear, therefore the system is scalable for very large datasets.
C. Deduplication Rate
Our proposed solution aims to provide a robust security
layer which provides conﬁdentiality and privacy without im-
pacting the underlying deduplication technique. Each ﬁle is
split into blocks by the client, who applies the best possible
chunking algorithm. When encrypted data blocks are received
by MM, a hash of each block is calculated in order to compare
them to the ones already stored. This task is completely
independent from the chunking technique used by clients.
Also, all the encryptions performed in the system do not affect
the deduplication effectiveness since the encryption is deter-
ministic. Therefore, ClouDedup provides additional security
properties without having an impact on the deduplication rate.
D. Security
If
We explained the main security beneﬁts of our solu-
tion in section IV-D. We now focus on potential attack
scenarios and possible issues that might arise. As stated
in the threat model section, we assume that an attacker,
like the malicious storage provider, has full access to the
storage.
the attacker has only access to the storage,
he cannot get any information. Indeed, ﬁles are split into
blocks and each block is ﬁrst encrypted with convergent
encryption and then further encrypted with one or more
secret keys. Moreover, no metadata are stored at the cloud
storage provider. Clearly, thanks to this setup, the attacker
cannot perform any dictionary attack on predictable ﬁles.
A worse scenario is the one in which the attacker man-
ages to compromise the metadata manager and thus has
access to data, metadata and encrypted keys. In this case,
conﬁdentiality and privacy would still be guaranteed since
block keys are encrypted with users’ secret keys and the
server’s secret key. The only information the attacker can
get are data similarity and relationships between ﬁles,
users and blocks. However, as ﬁle names are encrypted by
users,
these information would be of no use for the at-
tacker, unless he manages to ﬁnd a correspondence with
a predictable ﬁle according to its size and popularity.
The system must guarantee conﬁdentiality and privacy even in
the unlikely event where the server is compromised. The addi-
tional HSM proposed in section V-E and located between the
metadata manager and the storage provider will then enforce
data protection since it also offers another encryption layer;
therefore conﬁdentiality is still guaranteed and ofﬂine dictio-
nary attacks are not possible. On the other hand, if the attacker
compromises the server, only online attacks would be possible
since this component directly communicates with users. The
effect of such a breach is limited since data uploaded by users
are encrypted with convergent encryption, which achieves
conﬁdentiality for unpredictable ﬁles [15]. Furthermore, a
rate limiting strategy put in place by the metadata manager
can limit online brute-force attacks performed by the server.
In the worst scenario,
the attacker manages to compro-
mise both HSMs. In this case,
the attacker will be able
to remove the two additional layers of encryption and per-
form ofﬂine dictionary attacks on predictable ﬁles. How-
ever, conﬁdentiality for unpredictable ﬁles is guaranteed.
Finally, we analyze the impact of an attacker who attempts
to compromise users and have no access to the storage. If
an attacker compromises one or more users, he can attempt
to perform online dictionary attacks. As the server is not
compromised, the attacker will only retrieve data belonging
to the compromised user (access control mechanism). Further-
more, the server can limit such attacks by setting a maximum
threshold for the rate with which users can send requests.
VIII. CONCLUSION AND FUTURE WORK
We designed a system which achieves conﬁdentiality and
enables block-level deduplication at the same time. Our system
is built on top of convergent encryption. We showed that it
is worth performing block-level deduplication instead of ﬁle-
level deduplication since the gains in terms of storage space
are not affected by the overhead of metadata management,
which is minimal. Additional layers of encryption are added
by the server and the optional HSM. Thanks to the features of
these components, secret keys can be generated in a hardware-
dependent way by the device itself and do not need to be
shared with anyone else. As the additional encryption is
symmetric, the impact on performance is negligible. We also
showed that our design, in which no component is completely
trusted, prevents any single component from compromising
the security of the whole system. Our solution also prevents
curious cloud storage providers from inferring the original
content of stored data by observing access patterns or accessing
metadata. Furthermore, we showed that our solution can be
easily implemented with existing and widespread technologies.
Finally, our solution is fully compatible with standard storage
APIs and transparent for the cloud storage provider, which
does not have to be aware of the running deduplication system.
Therefore, any potentially untrusted cloud storage provider
such as Amazon, Dropbox and Google Drive, can play the
role of storage provider.
As part of future work, ClouDedup may be extended with
more security features such as proofs of retrievability [20], data
integrity checking [16] and search over encrypted data [13].
In this paper we mainly focused on the deﬁnition of the two
most important operations in cloud storage, that are storage
and retrieval. We plan to deﬁne other typical operations such as
edit and delete. After implementing a prototype of the system,
we aim to provide a full performance analysis. Furthermore,
we will work on ﬁnding possible optimizations in terms of
bandwidth, storage space and computation.
REFERENCES
[1] Amazon EC2. http://aws.amazon.com/ec2/.
[2] Amazon Glacier. http://aws.amazon.com/glacier/.
[3] Amazon S3. http://aws.amazon.com/s3/.
[4] Amazon Web Services. http://aws.amazon.com/.
[5] AWS Cloud HSM. http://aws.amazon.com/cloudhsm/.
[6] Dropbox. http://www.dropbox.com.
[7] Google Drive. http://drive.google.com/.
[8] High Availability with Luna. http://bit.ly/19dtZLb.
[9]
[10] Luna SA HSM. http://bit.ly/17CDPm1.
[11] Opendedup. http://opendedup.org/.
[12] Atul Adya, William J Bolosky, Miguel Castro, Gerald Cermak,
Is Convergent Encryption really secure? http://bit.ly/Uf63yH.
Ronnie Chaiken, John R Douceur, Jon Howell, Jacob R Lorch, Marvin
Theimer, and Roger P Wattenhofer. Farsite: Federated, available, and
reliable storage for an incompletely trusted environment. ACM
SIGOPS Operating Systems Review, 36(SI):1–14, 2002.
[13] Mihir Bellare, Alexandra Boldyreva, and Adam ONeill. Deterministic
and efﬁciently searchable encryption. In Advances in
Cryptology-CRYPTO 2007, pages 535–552. Springer, 2007.
[14] Mihir Bellare, Sriram Keelveedhi, and Thomas Ristenpart. Dupless:
Server-aided encryption for deduplicated storage. 2013.
[15] Mihir Bellare, Sriram Keelveedhi, and Thomas Ristenpart.
Message-locked encryption and secure deduplication. In Advances in
Cryptology–EUROCRYPT 2013, pages 296–312. Springer, 2013.
[16] Kevin D. Bowers, Ari Juels, and Alina Oprea. Hail: a
high-availability and integrity layer for cloud storage. In Proceedings
of the 16th ACM conference on Computer and communications
security, CCS ’09, pages 187–198, New York, NY, USA, 2009. ACM.
[17] Landon P Cox, Christopher D Murray, and Brian D Noble. Pastiche:
[18]
Making backup cheap and easy. ACM SIGOPS Operating Systems
Review, 36(SI):285–298, 2002.
John R Douceur, Atul Adya, William J Bolosky, P Simon, and Marvin
Theimer. Reclaiming space from duplicate ﬁles in a serverless
distributed ﬁle system. In Distributed Computing Systems, 2002.
Proceedings. 22nd International Conference on, pages 617–624.
IEEE, 2002.
[19] Danny Harnik, Benny Pinkas, and Alexandra Shulman-Peleg. Side
channels in cloud services: Deduplication in cloud storage. Security &
Privacy, IEEE, 8(6):40–47, 2010.
[20] Ari Juels and Burton S. Kaliski, Jr. Pors: proofs of retrievability for
large ﬁles. In Proceedings of the 14th ACM conference on Computer
and communications security, CCS ’07, pages 584–597, New York,
NY, USA, 2007. ACM.
[21] Chuanyi Liu, Xiaojian Liu, and Lei Wan. Policy-based de-duplication
in secure cloud storage. In Trustworthy Computing and Services,
pages 250–262. Springer, 2013.
[22] Luis Marques and Carlos J Costa. Secure deduplication on mobile
devices. In Proceedings of the 2011 Workshop on Open Source and
Design of Communication, pages 19–26. ACM, 2011.
[23] Dutch T Meyer and William J Bolosky. A study of practical
deduplication. ACM Transactions on Storage (TOS), 7(4):14, 2012.
[24] Perttula. Attacks on convergent encryption. http://bit.ly/yQxyvl.
[25]
John Pettitt. Hash of plaintext as key?
http://cypherpunks.venona.com/date/1996/02/msg02013.html.
[26] The Freenet Project. Freenet. https://freenetproject.org/.
[27] Michael O Rabin. Fingerprinting by random polynomials. Center for
Research in Computing Techn., Aiken Computation Laboratory, Univ.,
1981.
[28] Mark W Storer, Kevin Greenan, Darrell DE Long, and Ethan L
Miller. Secure data deduplication. In Proceedings of the 4th ACM
international workshop on Storage security and survivability, pages
1–10. ACM, 2008.
[29] Zooko Wilcox-O’Hearn and Brian Warner. Tahoe: the least-authority
ﬁlesystem. In Proceedings of the 4th ACM international workshop on
Storage security and survivability, pages 21–26. ACM, 2008.
Jia Xu, Ee-Chien Chang, and Jianying Zhou. Weak leakage-resilient
client-side deduplication of encrypted data in cloud storage. In
Proceedings of the 8th ACM SIGSAC symposium on Information,
computer and communications security, pages 195–206. ACM, 2013.
[30]