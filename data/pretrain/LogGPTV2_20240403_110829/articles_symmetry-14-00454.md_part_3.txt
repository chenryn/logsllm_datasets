φ φ
Outputgate:
I H C
at = ∑ w xt+ ∑ w bt−1+ ∑ w st−1 (5)
ω iω i hω h cω c
i=1 h=1 c=1
bt = f(at ) (6)
ω ω
InFormulas(1)–(6),x istheinputatthecurrentmoment,st−1isthestateofallcellsat
t c
thelastmoment,bt−1istheoutputofdifferentLSTMmemoryblocksatthelastmoment,
h
st isthestateofallcellsatthecurrentmoment,wistheweightofeachgate,and f isthe
c
activationfunction.
Thebackpropagationcalculationformulais:
Inputgate:
C
δt = f(cid:48)(at) ∑ g(at)(cid:101)t (7)
ι ι c s
c=1
Forgetgate:
C
δt = f(cid:48)(at) ∑ st−1(cid:101)t (8)
φ φ c s
c=1
Outputgate:
C
δt = f(cid:48)(at ) ∑ h(st)(cid:101)t (9)
ω ω c c
c=1
Theprocessofbackpropagationisactuallytheuseofchainderivationtosolvethe
gradientofeachweightintheentireLSTM.InFormulas(7)–(9),δt isthegradientofthe
ι
inputgate,δt isthegradientoftheforgetgate,andδt isthegradientoftheoutputgate.
φ ω
Inthetrainingphase,thenormallyexecutedlogentriesareusedasthedatasettotrain
themodel. Thepurposeistoallowthemodeltofullylearnthenormalexecutionmodeof
Symmetry2022,14,454 9of21
thesystemlogandavoidmisjudgmentasmuchaspossible. Supposealogeventsequence
is{E22,E5,E5,E5,E11,E9}. Givenawindowsizeof2,theinputpreordersequenceand
outputlabelusedtotrainthemodelare: {E22,E5→E5},{E5,E5→E5},{E5,E5→E11}and{E5,
E11→E9}. Theinputpostordersequenceandoutputlabelare:{E9,E11→E5},{E11,E5→E5},
{E5,E5→E5}and{E5,E5→E22}. TheLogLSmodelisusedtoobtaintheprobabilityofthe
currentlogeventbasedonthepreorderandpostordersequences. Thetrainingphaseneeds
tofindanappropriateweightdistributionsothatthefinaloutputofthemodelproduces
therequiredlabelsandoutputsthemalongwiththeinputinthetrainingdataset. Inthe
trainingprocess,eachinputoroutputusesgradientdescenttoincrementallyupdatethese
weightsthroughlossminimization. InLogLS,theinputincludeshlogeventwindowsw1
andw2,andtheoutputisthelogeventvalueimmediatelyafterw1andthelogeventvalue
beforew2. Weusecategoricalcross-entropyloss[32]fortraining. Inthetrainingphase,the
lengthfeaturesandlatentsymmetryinformationofthelogeventsequencearecountedfor
lateruseinthedetectionphase. Afterthemodelistrained,theverificationsetisusedto
adjusttheparametersofthemodeltofurtherobtaintheoptimalanomalydetectionmodel
parametervalues.
The basic method of the detection phase is the same as the training phase, but a
preliminary filtering process is added. For the newly added log event sequence, first a
preliminaryjudgmentismadebasedonthelengthfeaturesobtainedinthetrainingphase,
andthenhowtocombinethetwomodelsforanomalydetectionisdecided. Thelengthof
thetentativelogsequenceisrepresentedbyK,andthefollowingthreesituationswilloccur
inanomalydetection.
• When K 2*W,inthiscase,itisnecessarytoselecttheprelogeventsequencemodel
and the postlog event sequence model to predict together. It also needs to be sub-
divided,becausethelengthofthelastWofthelogsequencecannotbeusedinthe
postsequencelogeventsequencemodel. Becausethereisnosubsequentinput,only
thepreviouslogeventsequencemodelisselectedatthistime. Twoparametersg2and
g3areinvolvedhere(g2representsthefirstg2digitsofthelogeventprobabilitypre-
dictedaccordingtothepreviouslogeventmodel,andg3representsthefirstg3digits
ofthelogeventprobabilitypredictedaccordingtothesubsequentlogeventmodel).
ThelogeventsequenceisinputtotheLogLSmodel,andtheconditionalprobability
ofalogeventtobedetectedisobtained. Iftheprobabilitywiththevalueoftheparameters
g1,g2,andg3doesnotmeettheparameterrange,itmeansthatitdeviatesfromthenormal
logsequenceandcanberegardedasanomalous. Otherwise,otherlogeventsarecontinued
tobejudgeduntiltheentirelogeventsequenceisdetermined. Ifnoabnormalityoccurs,
thelogeventsequenceisnormal.
For example, when the input log event sequence is {E22, E5, E5, E5, E11, E9, E11,
E23} and the given window size is 3, the sequence length is 8. This situation belongs
tothethirdsituationmentionedabove, andthe{E5, E11}inthesequencearepredicted
using the preorder and postorder models. First, for {E5}, the input w1 = {E22, E5, E5},
w2={E11,E9,E11}. Supposethatthelogeventprobabilityobtainedaccordingtow1is
{E5:0.8,E22:0.2}andthattheeventprobabilityobtainedaccordingtow2is{E5:0.6,E23:0.2,
E9:0.2}. The parameter g2 is 1, and the parameter g3 is 2, indicating that the log event
predictedbyw1isE5,andthelogtimepredictedbyw2isE5orE23. Becausetheactuallog
eventisE5,itindicatesthatthecurrentlogeventisnormal. Becausethedetectionresultis
normal,thedetectionworkcontinuesbackward. Iftheactuallogeventisnotinthetwo
predictedresults,itwillbejudgedasanabnormallogeventsequence. {E9,E11,E23}in
thissequencecannotbepredictedusingtheabnormallogeventsequencemodelobtained
Symmetry2022,14,454 10of21
inthesubsequentsequencebecausethereisnosubsequentlogsequenceofthewindow
size, so at this time, only the previous log event model is used to make the prediction.
Whentheactuallogeventthatneedstobepredictedis{E23}, andtheinputw1={E11,
E9,E11},assumingthattheeventprobabilityobtainedaccordingtothissequence{E5:0.6,
E23:0.4},thefirstg2predictedresultistakenas{E5},whichdoesnotmatchtheactualresult.
Therefore,itisregardedasanabnormallogevent,andthesequenceisfinallyjudgedas
anabnormalsequence. Theabnormalresultisfedbacktotheuser,andtheusercanjudge
whetheramisjudgmentismade. Ifamisjudgmentoccurs,themisjudgmentsequencedata
arerecorded,andthemodelislateradjustedthroughtheupdatemechanism. Iftherewas
nomisjudgment,thesequencewasdirectlymarkedasabnormal.
3.3. RenewalMechanism
Obviously, the training data may not cover all possible normal execution models.
Howtoensurethetimelinessofthedetectionmodelandsolvetheemergenceofnewlog
execution models are problems that must be solved in system anomaly detection. For
example,whentheactuallogeventinthelogeventsequenceisE12andthepredictedresult
accordingtow1orw2isE8,theeventisregardedasabnormal. However,aftermanual
inspectionbytheuser,itisfoundthatthesetwoarenormallogevents. Therefore,LogLS
providesuserswithafeedbackmechanism[33]torelearnthisnewsequenceusingfalse
positivestoadjustitsweight. Whenthesequenceisenteredagain,itwillbedetectedthat
bothE12andE8willhavethesameprobabilityandwillbemarkedasnormal.
4. Evaluation
LogLSisimplementedusingKeras[34]withTensorFlow[35]asthebackend. Inthis
section,weshowevaluationsoftheoverallperformanceofLogLStoshowitseffectiveness
infindinganomaliesfromlargesystemlogdata.
4.1. Dataset
This article uses the authoritative dataset commonly used in system log anomaly
detection: the HDFS log dataset disclosed by Wei Xu et al. [5]. HDFS log datasets are
generatedbyrunningHadoop-basedmap-reducejobsonAmazonEC2nodes. Itismarked
by experts in the Hadoop field. The dataset contains 11,197,954 log entries, of which
16,838logentriesareabnormal,includingeventssuchas“writeexceptions”,accounting
forapproximately2.9%ofthetotal[36]. Thisdatasetwasconstructedin2009andthen
widelyusedinthefieldofloganomalydetection. Thedatasetcanbeobtainedinloghub.
ThespecificinformationisshowninTable4.
Table4.SummaryoftheHDFSdatasets.
System #TimeSpan #DataSize #Nomalies #Anomalies
HDFS 38.7h 1.55G 11,175,629 16,838
TheHDFSdatasetwaseventuallyparsedinto575,059logblocks,alsoknownaslog
eventsequences,ofwhich16,838logblocksweremarkedasanomaliesbyHadoopdomain
experts. Inthispaper,theHDFSdatasetisdividedintothreeparts,namelythetraining
dataset, verification dataset and test dataset. The training dataset is used for the data
samplesformodelfitting. Amongthem,thereareonlynormallogeventsequences,and
1%ofthelogeventsequencedatasetisselected,i.e.,4855normallogeventsequences. The
validationdatasetisasetofsamplessetasideseparatelyduringthemodeltrainingprocess.
It can be used to adjust the hyperparameters of the model and to make a preliminary
assessment of the model’s capabilities. Among them, there are both normal log event
sequences and abnormal log events, and 1/3 of the remaining log event sequences are
selected,i.e.,166,009normallogtimesequencesand5051abnormallogeventsequences.
Thetestdatasetisusedtoevaluatethefinaldetectionabilityofthemodel,butcannotbe
Symmetry2022,14,454 11of21
usedasabasisforalgorithmselectionsuchasparametertuningandfeatureselection. It
containsbothnormallogeventsequencesandabnormallogeventsequences,whichare
theremaining2/3logeventsequence[33,37]. ThespecificdivisionisshowninTable5.
Table5.SetupofHDFSlogdatasets(unit:sequence).
NumberofSessions
LogDataset n:NumberofLogKeys
TrainingData ValidationData TestData
166,009normal; 387,357normal;
HDFS 4855normal 29
5051anormal 11,787anormal
4.2. EvaluationMetrics
Toevaluatetheeffectivenessofthemodel,inadditiontothenumberoffalsepositives
(FP)andfalsenegatives(FN),wealsousestandardmetrics,suchasaccuracy,precision,re-
callandF1-measure. Accuracyrepresentsthepercentageoflogsthatarecorrectlyclassified
inthetotallogs. Precisionrepresentstheproportionoftrueanomaliesamongthedetected
anomalies. Recallrepresentsthepercentageofdetectedanomaliesinthetotalanomaliesin
thedataset. TheF1-measureistheweightedharmonicaverageofprecisionandrecalland
isacomprehensiveevaluationindex[38].
TP+TN
Accuracy = (10)
TP+TN+FP+FN
TP
Precision = (11)
TP+FP
TP
Recall = (12)
TP+FN
2∗Precison∗Recall
F1−measure = (13)
Precision+Recall
InFormulas(10)–(13),TPisthenumberofnormalclassespredictedfornormalsamples,
FPisthenumberofnormalclassespredictedforabnormalsamples,andFNisthenumber
ofabnormalclassespredictedfornormalsamples[39].
4.3. ResultAnalysis
WecompareLogLSwithfouranomalydetectionmethods,namelyPCA,IM,N-gram
andDeepLog. Throughthecomparisonofeachevaluationdata,thedetectionperformance
of this model in log anomaly detection is obtained. By default, we use the following
parametervaluesforLogLS:g1=13,g2=4,g3=2,h=10,L=2,S=64,andE=300. g1,
g2, and g3 are the same type of parameters g, and g determines whether the predicted
logeventisnormal(thelogeventthatappearsnextisconsiderednormalamongtheglog