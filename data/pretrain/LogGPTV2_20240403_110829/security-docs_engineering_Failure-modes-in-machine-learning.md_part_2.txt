## Details on Intentionally-Motivated Failures
    Scenario #
    Attack Class
    Description
    Type of Compromise
    Scenario
    Is the attacker [technologically] misusing the system?    -->
    1 
    Perturbation attacks 
    In perturbation style attacks, the attacker stealthily modifies the query to get a desired response 
    Integrity
    Image: Noise is added to an X-ray image, which makes the predictions go from normal scan to abnormal  [1][Blackbox]  
    Text translation: Specific characters are manipulated to result in incorrect translation. The attack can suppress specific word or can even remove the word completely[2][Blackbox and Whitebox]
    Speech: Researchers showed how given a speech waveform, another waveform can be exactly replicated but transcribes into a totally different text[3][Whitebox but may be extended to blackbox]
    In a blackbox setting no special privileges are required by the attacker to perform the attack. The attacker generates the perturbations offline and queries the system legitimately.There seems to be no technological access violationsJust like a legitimate user sends in a legitimate image, the attacker sends in a corrupted image for classification to purposely confuse the system.      -->
    2
    Poisoning attacks  
    The goal of the attacker is to contaminate the machine model generated in the training phase, so that predictions on new data will be modified in the testing phase Targeted: In targeted poisoning attacks, the attacker wants to misclassify specific examples Indiscriminate: The aim here is to cause DoS like effect, which makes the system unavailable. 
    Integrity
    In a medical dataset where the goal is to predict the dosage of anticoagulant drug Warfarin using demographic information, etc. Researchers introduced malicious samples at 8% poisoning rate, which changed dosage by 75.06% for half of patients[4][Blackbox] In the Tay chatbot, future conversations were tainted because a fraction of the past conversations were used to train the system via feedback[5] [Blackbox] 
    No special privileges required by the attacker. In a closed system like social media platforms, the attacker needs to be a user of the platform. No technological access violationThe authorized attacker sends chaff traffic to the endpoint, just like an authorized user would send legitimate traffic to the end point.         -->
    3
    Model Inversion  
    The private features used in machine learning models can be recovered
    Confidentiality; 
    Researchers were able to recover private training data used to train the algorithm[6] The authors were able to reconstruct faces, by just the name and access to the model to the point where Mechanical turks could use the photo to identify an individual from aline-up with 95% accuracy.  The authors were also able to extract specific information.  [Whitebox and Blackbox][12] 
    Technologically, it feels like the attacker is not misusing the system  -       No special privileges is required by the attacker to perform the attack.  -       There seems to be no technological access violations – Just like a legitimate user sends in a legitimate image, the attacker sends these specially craftedqueries to recover the private features.      -->
    4
    Membership Inference attack
    The attacker can determine whether a given data record was part of the model’s training dataset or not
    Confidentiality 
    Researchers were able to predict a patient’s main procedure(e.g: Surgery the patient went through) based on the attributes (e.g: age,gender, hospital)[7][Blackbox]
    Technologically, it feels like the attacker is not misusing the system  -       No special privileges is required by the attacker to perform the attack.  -       There seems to be no technological access violations – Just like a legitimate user sends in a legitimate image, the attacker sends these specially crafted queries to infer membership.     -->
    5
    Model stealing 
    The attackers recreate the underlying model by legitimately querying the model. The functionality of the new model is same as that of the underlying model. 
    Confidentiality
    Researchers successfully emulated the underlying algorithm from Amazon, BigML. For instance, in the BigML case, researchers were able to recover the model used to predict if someone should have a good/bad credit risk (German Credit Card dataset) using 1,150 queries and within 10 minutes[8]
    Technologically, it feels like the attacker is not misusing the system  -       No special privileges is required by the attacker to perform the attack.  -       There seems to be no technological access violations – Just like a legitimate user sends in a legitimate image, the attacker sends these specially crafted queries to confuse the system    -->
    6
    Reprogramming deep neural nets
    By means of a specially crafted query from an adversary, Machine learning systems can be reprogrammed to a task that deviates from the creator’s original intent
    Integrity, Availability
    Demonstrated how ImageNet, a system used to classify one of several categories of images was repurposed to count squares. Authors end the paper with a hypothetical scenario: An attacker sends Captcha images to the computer vision classifier in a cloud hosted photos service to solve the image captchas to create spam accounts[9]  
    Technologically, it feels like the attacker is not misusing the system  -       No special privileges is required by the attacker to perform the attack.  -       There seems to be no technological access violations – Just like a legitimate user sends in a legitimate image, the attacker sends these specially crafted queries to reprogram the system   -->
    7
    Adversarial Example in the Physical domain 
    An adversarial example is an input/query from a malicious entity sent with the sole aim of misleading the machine learning system These examples can manifest in the physical domain 
    Integrity
    Researchers 3D prints a rifle with custom texture that fools image recognition system into thinking it is a turtle[10] 
    Researchers construct sunglasses with a design that can now fool image recognition systems, and no longer recognize the faces correctly[11]
      -->
    8
    Malicious ML providers who can recover training data
     Malicious ML provider can query the model used by customer and recover customer’s training data
    Confidentiality 
    Researchers show how a malicious provider presents a backdoored algorithm, wherein the private training data is recovered. They were able to reconstruct faces and texts, given the model alone.  [12] 
    Technologically, it feels like the provider is misusing the system. Providers don’t snoop around on customer’s data, and this is indirect violation of customer promises.  -->
    9
    Attacking the ML Supply Chain[13]
    Owing to large resources (data + computation) required to train algorithms, the current practice is to reuse models trained by large corporations, and modify them slightly for task at hand (e.g: ResNet is a popular image recognition model from Microsoft). These models are curated ina Model Zoo (Caffe hosts popular image recognition models). In this attack,the adversary attacks the models hosted in Caffe, thereby poisoning the well for anyone else. 
    Integrity
    Researchers show how it is possible for an attacker to check in malicious code into one of the popular model. An unsuspecting ML developer downloads this model and uses it as part of the image recognition system in their code [14]. The authors show how in Caffe, there exists a model whose SHA1 hash doesNOT match the authors’ digest, indicating tampering. There are 22 models without any SHA1 hash for integrity checks at all. 
    Technologically, the attacker is misusing the system  -      Injecting malicious code into source repository feels like a technological violation.    Questions:  1.      Are Model repositories (typically open source)  failing their duty of care if they don’t check for adversarial manipulation of the models they host?  2.       Can the adversary who tampered with the models be punished under CFAA?  3.       Is the unsuspecting developer now liable for any damages incurred by the customer?   -->
    10
    Backdoor Machine Learning
    Like in the “Attacking the ML Supply Chain”, In this attack scenario,the training process is either fully or partially outsourced to a malicious party who wants to provide the user with a trained model that contains a backdoor. The backdoored model would perform well on most inputs (including inputs that the end user may hold out as a validation set) but cause targeted misclassifications or degrade the accuracy of the model for inputs that satisfy some secret, attacker-chosen property, which we will refer to as the backdoor trigger
    Confidentiality, Integrity
    Researchers created a backdoored U.S. street sign classifier that identifies stop signs as speed limits only when a special sticker is added to the stop sign (backdoor trigger) 20 They are now extending this work to text processing systems, wherein specific words are replaced with the trigger being the speaker’s accent[15]
    Technologically, it feels like the attacker is misusing the system. This is because backdooring the system feels like violating access controls put forth by the original system designer.   -->
    11
    Exploit software dependencies of ML system
    In this attack, the attacker does NOT manipulate the algorithms. Instead, exploits traditional software vulnerabilities such as buffer overflows. 
    Confidentiality, Integrity, Availability, 
    An adversary sends in corrupt input to an image recognition system that causes it to misclassify by exploiting a software bug in one of the dependencies.
    Technologically, it feels like the attacker is misusing the system. This is because exploiting software dependencies equates to violating access controls put forth by the original system designer.   -->
## Details on Unintended Failures
    Scenario #
    Attack Class
    Description
    Type of Compromise
    Scenario
    12
    Reward Hacking
    Reinforcement learning systems act in unintended ways because of discrepancies between the specified reward and the true intended reward.
    Safety of the system 
    A huge corpus of gaming examples in AI has been compiled here[1]
    13
    Side Effects
    RL system disrupts the environment as it tries to attain their goal 
    Safety of the system 
    Scenario, verbatim from the authors in [2]:“Suppose a designer wants an RL agent (for example our cleaning robot) to achieve some goal, like moving a box from one side of a room to the other.Sometimes the most effective way to achieve the goal involves doing something unrelated and destructive to the rest of the environment, like knocking over a vase of water that is in its path. If the agent is given reward only for moving the box, it will probably knock over the vase.”
    14
    Distributional shifts
    The system is tested in one kind of environment, but is unable to adapt to changes in other kinds of environment 
    Safety of the system
    Researchers trained two state of the art RL agents, Rainbow DQN and A2C in a simulation to avoid lava. During training, the RL agent was able to avoid lava successfully and reach its goal. During testing, they slightly moved the position of the lava, but the RL agent was not able to avoid [3]
    15
    Natural Adversarial Examples 
    The system incorrectly recognizes an input that was found using hard negative mining 
    Safety of the system
    Here the authors show how by a simple process of hard negative mining[4], it is possible to confuse the ML system by relaying the example. 
    16
    Common Corruption
    The system is not able to handle common corruptions and perturbations such as tilting, zooming, or noisy images. 
    Safety of the system
    The authors[5] show how common corruptions such as changes to brightness, contrast, fog or noise added to images, have a significant drop in metrics in image recognition 
    17
    Incomplete Testing in Realistic conditions
    The ML system is not tested in realistic conditions that it is meant to operate in 
    Safety of the system
    The authors in [25] highlight that that while defenders commonly account for robustness of the ML algorithm, they lose sight of realistic conditions. For instance, they argue that a missing stop sign knocked off in the wind (which is more realistic) than an attacker attempting to perturb the system's inputs.
## Acknowledgements
We would like to thank Andrew Marshall, Magnus Nystrom, John Walton, John Lambert, Sharon Xia, Andi Comissoneru, Emre Kiciman, Jugal Parikh, Sharon Gillet, members of Microsoft’s AI and Ethics in Engineering and Research (AETHER) committee’s Security workstream, Amar Ashar, Samuel Klein, Jonathan Zittrain,  members of AI Safety Security Working Group at Berkman Klein for providing helpful feedback. We would also like to thank reviewers from 23 external partners, standards organization, and government organizations for shaping the taxonomy.  
## Bibliography
[1] Li, Guofu, et al. "Security Matters: A Survey on Adversarial Machine
Learning." *arXiv preprint arXiv:1810.07339* (2018).
[2] Chakraborty, Anirban, et al. "Adversarial attacks and defences: A
survey." *arXiv preprint arXiv:1810.00069* (2018).
[3] Ortega, Pedro, and Vishal Maini. "Building safe artificial
intelligence: specification, robustness, and assurance." *DeepMind
Safety Research Blog* (2018).
[4] Amodei, Dario, et al. "Concrete problems in AI safety." *arXiv
preprint arXiv:1606.06565* (2016).
[5] Shankar Siva Kumar, Ram, et al. "Law and Adversarial Machine
Learning." *arXiv preprint arXiv:1810.10731* (2018).
[6] Calo, Ryan, et al. "Is Tricking a Robot Hacking?." University of
Washington School of Law Research Paper 2018-05 (2018).
[7] Paschali, Magdalini, et al. "Generalizability vs. Robustness:
Adversarial Examples for Medical Imaging." arXiv preprint
arXiv:1804.00504 (2018).
[8] Ebrahimi, Javid, Daniel Lowd, and Dejing Dou. "On Adversarial
Examples for Character-Level Neural Machine Translation." arXiv preprint
arXiv:1806.09030 (2018)
[9] Carlini, Nicholas, and David Wagner. "Audio adversarial examples:
Targeted attacks on speech-to-text." arXiv preprint arXiv:1801.01944
(2018).
[10] Jagielski, Matthew, et al. "Manipulating machine learning:
Poisoning attacks and countermeasures for regression learning." *arXiv
preprint arXiv:1804.00308* (2018)
[11] [https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/]
[12] Fredrikson M, Jha S, Ristenpart T. 2015. Model inversion attacks
that exploit confidence information and basic countermeasures
[13] Shokri R, Stronati M, Song C, Shmatikov V. 2017. Membership
inference attacks against machine learning models. In *Proc. of the 2017
IEEE Symp. on Security and Privacy (SP)*, *San Jose, CA, 22–24 May
2017*, pp. 3–18. New York, NY: IEEE.
[14] Tramèr, Florian, et al. "Stealing Machine Learning Models via
Prediction APIs." *USENIX Security Symposium*. 2016.
[15] Elsayed, Gamaleldin F., Ian Goodfellow, and Jascha Sohl-Dickstein.
"Adversarial Reprogramming of Neural Networks." *arXiv preprint
arXiv:1806.11146* (2018).
[16] Athalye, Anish, and Ilya Sutskever. "Synthesizing robust
adversarial examples." *arXiv preprint arXiv:1707.07397*(2017)
[17] Sharif, Mahmood, et al. "Adversarial Generative Nets: Neural
Network Attacks on State-of-the-Art Face Recognition." *arXiv preprint
arXiv:1801.00349* (2017).
[19] Xiao, Qixue, et al. "Security Risks in Deep Learning
Implementations." *arXiv preprint arXiv:1711.11008* (2017).
[20] Gu, Tianyu, Brendan Dolan-Gavitt, and Siddharth Garg. "Badnets:
Identifying vulnerabilities in the machine learning model supply
chain." *arXiv preprint arXiv:1708.06733* (2017)
[21] [https://www.wired.com/story/machine-learning-backdoors/]
[22] [https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml]
[23] Amodei, Dario, et al. "Concrete problems in AI safety." *arXiv
preprint arXiv:1606.06565* (2016).
[24] Leike, Jan, et al. "AI safety gridworlds." *arXiv preprint
arXiv:1711.09883* (2017).
[25] Gilmer, Justin, et al. "Motivating the rules of the game for
adversarial example research." *arXiv preprint arXiv:1807.06732* (2018).
[26] Hendrycks, Dan, and Thomas Dietterich. "Benchmarking neural network
robustness to common corruptions and perturbations." *arXiv preprint
arXiv:1903.12261* (2019).