title:Towards publishing recommendation data with predictive anonymization
author:Chih-Cheng Chang and
Brian Thompson and
Wendy Hui Wang and
Danfeng Yao
Towards Publishing Recommendation Data With Predictive
Anonymization
Chih-Cheng Chang
Rutgers University
Department of Computer Science
Piscataway, NJ, USA
PI:EMAIL
Hui (Wendy) Wang
Stevens Institute of Technology
Department of Computer Science
Hoboken, NJ, USA
PI:EMAIL
Brian Thompson
Rutgers University
Department of Computer Science
Piscataway, NJ, USA
PI:EMAIL
Danfeng Yao
Rutgers University
Department of Computer Science
Piscataway, NJ, USA
PI:EMAIL
ABSTRACT
Recommender systems are used to predict user preferences
for products or services. In order to seek better prediction
techniques, data owners of recommender systems such as
Netﬂix sometimes make their customers’ reviews available to
the public, which raises serious privacy concerns. With only
a small amount of knowledge about individuals and their
ratings to some items in a recommender system, an adver-
sary may easily identify the users and breach their privacy.
Unfortunately, most of the existing privacy models (e.g., k-
anonymity) cannot be directly applied to recommender sys-
tems.
In this paper, we study the problem of privacy-preserving
publishing of recommendation datasets. We represent rec-
ommendation data as a bipartite graph, and identify several
attacks that can re-identify users and determine their item
ratings. To deal with these attacks, we ﬁrst give formal
privacy deﬁnitions for recommendation data, and then de-
velop a robust and eﬃcient anonymization algorithm, Pre-
dictive Anonymization, to achieve our privacy goals. Our
experimental results show that Predictive Anonymization
can prevent the attacks with very little impact to prediction
accuracy.
Categories and Subject Descriptors
G.1.3 [Mathematics of Computing]: Numerical Linear
Algebra—Sparse, structured, and very large systems; H.2.8
[Database Management]: Database Applications—Data
mining; K.4.1 [Computers and Society]: Public Policy
Issues—Privacy
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIACCS’10 April 13–16, 2010, Beijing, China.
Copyright 2010 ACM 978-1-60558-936-7 ...$10.00.
General Terms
Algorithms, Experimentation, Performance, Security
Keywords
Anonymization, Sparsity, Prediction, Clustering, Privacy
1.
INTRODUCTION
To help consumers make intelligent buying decisions,
many websites provide recommender systems [29] that give
users suggestions of items of potential interest to them. The
recommender systems collect users’ input (e.g., reviews, rat-
ings, etc.), compare the collected data to others, and calcu-
late a personalized list of recommended items for the user.
It has been proven to be eﬀective at delivering users more
intelligent and proactive recommendations [32].
To support advanced data mining and prediction algo-
rithms, data owners sometimes publicly release their rec-
ommendation datasets. The released datasets may include
information that is legally protected, or otherwise private
or sensitive data, such as buying records and movie-viewing
histories. For example, in 2006, Netﬂix, the world’s largest
online DVD rental service, announced a one million-dollar
Netﬂix Prize for improving their movie recommendation al-
gorithm.1 To aid contestants, Netﬂix released a Netﬂix Prize
dataset containing more than 100 million movie ratings, cre-
ated by around 500 thousand Netﬂix subscribers between
December 1999 and December 2005. As the dataset con-
tains users’ private preferences to the movies, Netﬂix re-
moved customers’ names to protect their privacy. However,
this naively anonymized data suﬀers from re-identiﬁcation
attacks as recently demonstrated [26].
1.1 Motivation Examples
With some additional knowledge about a user’s review
history, an adversary may be able to uniquely identify and
consequently learn additional information about the user.
For example, suppose the adversary knows her co-worker Al-
ice watched Pretty in Pink, a movie from the eighties which
has not been reviewed by many people. By matching with
1The
http://www.netﬂixprize.com
prize
was
claimed
in
September
2009.
0001
0002
0003
0004
5
4
3
4
5
4
1
5
(a)
Star Wars
0001
4
3.5
4.5
3.5
Star Wars
Godfather
0002
4.5
Godfather
4
English patient
0003
Pretty in pink
0004
4.5
4.5
4
1 1
5
5
(b)
English patient
Pretty in pink
Figure 1: An example: (a) The original graph with user names replaced with IDs, (b) The k-anonymized
graph where k = 2. The rectangle boxes represent anonymization groups. Dotted lines represent fake edges
added for the purpose of anonymization. New ratings are computed as the average of existing ratings in the
anonymization group.
the released dataset in Figure 1 (a), the adversary can iden-
tify that ID 0004 corresponds to Alice. Consequently the
adversary learns all movies that Alice has reviewed and her
preferences to these movies.
Online shopping is very popular on the Web today, and
costumers of E-commerce sites such as Amazon.com or E-
bay.com are under the same privacy risk. For example,
an adversary knows his neighbor Bob recently purchased
a not-so-popular cell phone from Amazon. By searching the
phone model, the adversary can learn other items that Bob
has bought through the collaborative ﬁltering features pro-
vided by Amazon (e.g., what do customers ultimately buy
after viewing this item, customers who bought this item also
bought, etc.). Furthermore, if Amazon decides to publish
their recommendation data for research or other purposes,
insuﬃcient anonymization could leave users’ personal infor-
mation vulnerable to attack. Similar issues arise with con-
tent sharing website Digg.com, which recently implemented
collaborative ﬁltering to recommend new articles or web con-
tent based on users’ viewing and rating histories. Knowledge
of a user’s history could reveal sensitive information, e.g. a
user’s sexual orientation or political aﬃliation.
Such privacy risks have been studied by Narayanan et. al.
who show that very little knowledge about an individual
subscriber is necessary to identify her record if it is present
in the Netﬂix dataset [26]. For instance, 84% of subscribers
can be uniquely identiﬁed if the adversary knows 6 out of
8 movies outside the top 500 most popular movies. The
ease of attack is largely due to the sparsity of the Netﬂix
Prize data. The intuition is that unpopular movies are rarely
rated by users; thus, by rating unpopular movies the user
distinguishes herself from the crowd. These examples show
that removing user names is not suﬃcient to protect users
from attack; with certain auxiliary knowledge of users’ re-
views, the adversary is still able to violate their privacy and
obtain sensitive information. Such background knowledge
can be easily obtained from personal blogs, public bulletin
board systems (BBS), and other related recommender sys-
tems (e.g., the IMDB website2).
Releasing anonymized data to the public for research is an
inevitable trend3 and has the potential to provide society
with substantial beneﬁts in many ﬁelds, including health-
care, medical sciences, and social sciences. However, there
is a tradeoﬀ between the utility and privacy of anonymized
data.
In recommender systems, as the anonymized data
deviates from its original in attempts to preserve privacy,
2The Internet Movie Database, http://www.imdb.com
3Netﬂix has already announced plans for a second Netﬂix
Prize with a new dataset.
predictions based on the anonymized data may become less
accurate. Sparsity in recommendation data signiﬁcantly in-
creases the diﬃculty of such anonymization tasks in real-
world datasets, a challenge which we aim to address. In this
paper, we take on the task of developing an eﬃcient, utility-
preserving approach for anonymizing large-scale real-world
recommendation datasets.
1.2 Challenges
Although privacy preservation in data publishing has been
studied extensively under several privacy models (e.g., k-
anonymity [33] and l-diversity [22]), most algorithms pro-
posed are only designed for relational datasets. There have
also been several studies on privacy-preserving publishing of
graphs [9, 17, 34, 36]. In this work, we model recommenda-
tion databases as labeled bipartite graphs. The additional
structure and labels make users more susceptible to privacy
attacks and introduce new challenges to anonymization that
have not yet been addressed in the literature.
Most importantly, none of existing anonymization work
has studied the eﬀect that sparsity has on privacy. Real-
world recommendation data is quite sparse; that is, each
individual’s rating proﬁle only contains values for a small
fraction of items in the database [26]. The so-called sparsity
problem has an adverse aﬀect on anonymization: it increases
the diﬃculty of designing anonymization schemes that pro-
vide acceptable predication accuracy. Unfortunately, the ex-
isting anonymization algorithms are not eﬀective when ap-
plied to sparse recommendation datasets.
In addition, most of the existing privacy methods are built
around the assumption that there are two non-overlapping
value sets: sensitive values, which need to be kept private,
and quasi-identiﬁer values, which can be used by the adver-
sary to identify individuals. In recommender systems, these
two sets are not disjoint; all information in a recommenda-
tion dataset could be sensitive, and can also potentially be
used as quasi-identiﬁers. This additional challenge requires
new privacy models that are applicable to recommender sys-
tems.
1.3 Contributions
In this paper, we study how to publish sparse recom-
mender data in a privacy- and utility-preserving manner.
Our high-level approach is to group and average similar user
proﬁles together. However, in a sparse dataset, ﬁnding sim-
ilar users is challenging – even users with similar tastes have
a relatively small overlap in items rated. Our main idea is
that before anonymization, we reduce sparsity by padding
the data with predicted values. This pad-then-anonymize
approach is able to uncover and leverage the latent inter-
ests of users that would otherwise be lost without the pre-
processing step. Our solution, Predictive Anonymization,
is a powerful and general approach for publishing all types
of recommender data. We summarize our contributions as
follows.
• We formalize privacy and attack models for recom-
mendation databases. We model the review data as
a labeled bipartite graph, where two disjoint sets of
nodes represent users and items, respectively. We for-
mally deﬁne two types of adversary attacks, namely
a structure-based attack and a label-based attack. We
also give deﬁnitions of k-anonymity and l-diversity in
the context of recommendation databases.
• We develop Predictive Anonymization, a novel tech-
nique to pad, cluster, and anonymize recommendation
data. We propose several variations of the algorithm
and analyze their privacy guarantees.
• We perform a set of experiments to test the eﬀective-
ness and eﬃciency of our approach. Our experiments
are carried out on the entire Netﬂix Prize dataset,
which contains 480,189 users and 17,770 movies. Fi-
nally, we use the results to study the privacy-utility
trade-oﬀ when anonymizing recommendation data.
Organization of the paper The rest of paper is orga-
nized as follows. Section 2 provides a brief background of
recommender systems. Our privacy model is deﬁned in Sec-
tion 3. In Section 4 we present our Predictive Anonymiza-
tion method. Complexity and security analysis are given
in Section 5. Extensions to the anonymization algorithm,
based on the idea of l-diversity, are given in Section 6. The
experimental results are described and analyzed in Section 7.
Section 8 presents related work. Section 9 summarizes the
paper.
2. RECOMMENDER SYSTEMS
Recommender systems produce automatic predictions
about the interests of users by collecting preference infor-
mation from many users. A recommender system consists
of: (1) A set of users U = {u1, . . . , um}, (2) A set of items
O = {o1, . . . , on}, (3) An ordered set of possible rating val-
ues S, and (4) A set of user ratings {(u, o, r)} where u ∈ U ,
o ∈ O, and r ∈ S is the rating value assigned by the user u
to an item o (only if u has rated o).
Given a recommender system, the ratings can be repre-
sented as an m × n matrix R. Each cell ri,j is either a real
number r, which corresponds to the triplet (ui, oj, r), or 0
if user ui has not rated item oj. This leads to a natural
representation of the system as a bipartite graph. We refer
to the vertices that represent users, denoted by VU , as user
nodes. Vertices representing the items, denoted by VO, are
called item nodes.
Deﬁnition 1. Bipartite Review Graph A recom-
mender system (U, O, R) corresponds to a bipartite review
graph G = (VU ∪ VO, E, L), where each user ui ∈ U corre-
sponds to a node vui ∈ VU , each item oj ∈ O corresponds to
a node voj ∈ VO, and each non-zero entry ri,j in the rating
matrix corresponds to the edge (vui , voj ) ∈ E. L : E → S is
the label function, which assigns to each edge (vui , voj ) ∈ E
Figure 2: Models of adversary knowledge
the label ri,j ∈ S, one of the possible rating values. Thus the