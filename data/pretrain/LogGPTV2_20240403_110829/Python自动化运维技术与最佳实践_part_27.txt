（5）执行模块
最后，执行模块及返回结果见图11-6。
图11-6执行模块结果
---
## Page 219
198第二部分高级篇
正常返回了5条/var/log/messages信息，完成了自定义模块的全过程。
11.4非PythonAPi接口支持
Func通过非PythonAPI实现远程调用，目的是为第三方工具提供调用及返回接口。
Func使用func-transmit命令来实现，支持YAML与JSON格式，实现了跨应用平台、语言、
工具等，比如通过Java或C生成JSON格式的接口定义，通过fun-transmit命令进行调用，
使用上非常简单，扩展性也非常强。
定义一个command模块的远程执行，分别采用YAML及JSON格式进行定义，如下：
[ /home/test/func/run.yaml ]
clients: ***
async1 Fa1se
nforks: 1
module: command
method1 run
parameters: */bin/echo He1lo World*
[ /home/test/func/run.json ]
"clients": **",
"async": "False",
"nforks*: 1,
pueuuoo:atnpou
"method*: *run*,
Paog otreg ouoe/uta/。iszeleuexed
各参数详细说明如下。
clients，目标主机，"*"代表所有被控主机；
口async，是否异步，是一个布尔值，True为使用异步，False 则不使用；
口nforks，启用的线程数，用数字表示；
module，模块名称，如 command、copyfile、process 等；
method，方法名称，如 command模块下的run方法；
parameters，参数，如/usr/bin/tail-100 /var/log/messages"。
通过func-transmit命令调用不同接口配置，将返回不同的格式串，如图11-7和图11-8所示。
---
## Page 220
第11章统一网络控制器Func详解199
ello Rorle
图11-7返回标准的YAML格式
图 11-8返回标准的JSON 格式
返回的两种格式都可以被绝大部分语言所解析，方便后续处理。
11.5Func的Facts支持
Facts是一个非常有用的组件，其功能类似于 Saltstack 的 grains、Ansible 的Facts，实现
获取远程主机的系统信息，以便在对目标主机操作时作为条件进行过滤，产生差异。Func的
Facts支持通过API来扩展用户自己的属性。Facts由两部分组成，一为模块（module），另为
方法（method），可通过list_fact_modules、list_fact_methods方法来查看当前支持的模块与方
法的清单，如图11-9所示。
[rootPSN2e3-es-eoe funcj func
.20-00-g12.
0511
图11-9查看主机支持模块及方法
---
## Page 221
200第二部分高级篇
在使用Facts时，我们关注它的方法（func"*"call fact list_fact_methods显示的清
单）即可，可通过命令行调用Facts 的call_fact方法查看所有主机的操作系统信息，具体
01-11?
s2013-8-422: *Cent05 releese 6.4 (Finot)°)
图11-10查看主机操作系统信息
Fact支持and与or作为条件表达式连接操作符，下面详细介绍。
（1）and表达式 -filter
语法：
paoox TuT onte△ *pxoox uT onTeA zoaT--
示例：所有满足内核（kemel）版本大于或等于2.6，并且操作系统信息包含CentOS的
目标主机运行uptime命令，如图11-11所示。
e.00,e.00, 0.0n
图11-11根据fact条件（and）过滤主机
（2）or表达式-filteror
语法：
-filteror *value Iin keyword, value ini keyword*
示例：所有满足内核（kemel）版本大于或等于2.6，或者运行级别等于5的目标主机运
行df-m命令，如图11-12所示。
---
## Page 222
第11章统一网络控制器Func详解201
1476
图11-12根据fact 条件（or）过滤主机
票11.1节～11.5节关于Func的介绍参考官网文档https:/fedorahosted.org/func/。
---
## Page 223
Clnyder
第12章
Python大数据应用详解
随着云时代的到来，大数据（bigdata）也越来越受大家的关注，比如互联网行业日常生
成的运营、用户行为数据，随着时间及访问量的增长这一规模日益庞大，单位可达到日TB
或PB级别。如何在如此庞大的数据中挖掘出对我们有用的信息？目前业界主流存储与分析
平台是以Hadoop为主的开源生态圈，MapReduce作为Hadoop 的数据集的并行运算模型，
除了提供Java 编写MapReduce任务外，还兼容了Streaming方式，我们可以使用任意脚本语
言来编写MapReduce任务，优点是开发简单且灵活。本章详细介绍如何使用Python语言来
实现大数据应用，将分别通过原生Python与框架（Framework）方式进行说明。
票因为Hadoop不作为本章的主体内容，所以将不对其架构、子项目、优化等进行说明。
12.1环境说明
为了方便读者理解，笔者通过虚拟化环境部署了Hadoop平台来进行演示，操作系统版
本为 CentOS release 6.4,以以 及 Python 2.6.6、hadoop-1.2.1、jdk1.6.0_45、mrjob-0.4.2 等。相
关服务器信息如表12-1所示。
表12-1环境说明表
角色
主机名
IP
功能
存储分区
Master
SN2013-08-020
192.168.1.20
NameNode Secondarynamenode | JobTracker
/data
Slave
010-20-Z107NS
192.168.1.21
DataNode I TaskTracker
/data
Slave
110-20-21022S
192.168.1.22
DataNode I TaskTracker
/data
---
## Page 224
第12章Python大数据应用详解203
12.2Hadoop部署
由于部署Hadoop需要Master访问所有Salve主机实现无密码登录，即配置账号公钥认
证，具体参考9.2.5节关于配置Linux主机SSH无密码访问的介绍，本节将不再陈述。
（1）安装
SSH登录Master主机，这里使用root账号进行相关演示。安装JDK环境：
eAe(/zsn/ po 99 /eAe[/xsn/ d- xtpxw 
 get http: //uni-smr,ac,ru/archive/dev/java/SDKs/sun/2se/6/dk-6u45-
linux=x64,bin
chnod +x jdk-6u45-1inux-x64.bin
./jdk-6u45-11nux=x64.bin
v1/etc/profile（配置Java环境变量，进加以下内容）
export JAVA_HOME=/usr/java/jdk1 . 6 . 0_45
export PATH=$PATH:$JAVA_HOME/bin
export CLASSPATH=. : $JAVA_HOME/1re/11b:SJAVA_HOME/11b: $JAVA_BOME/11b/too1s . Jar
cd/etc（使环境变量生效）
 - profile
安装Hadoop，版本为1.2.1，安装路径为/usr/local。
 cd /usr/1oca1
wget http://nirrors,cnnic,cn/apache/hadoop/common/hadoop-1.2,1/hadoop
1.2.1.tar.gz
 tar -zxvf hadoop-1.2.1.tar,gz
 cd /usr/loca1/hadoop-1.2.1/conf
修改目录（/usr/local/hadoop-1.2.1/conf)中的四个Hadoop 核心配置文件hadoop-env.sh
core-site.xml、hdfs-site.xml、mapred-site.xml,具体内容如下:
口hadoop-env.sh，Hadoop 环境变量配置文件，指定JAVA_HOME。
export JAVA_BOME=/usz/java/1dk1 .6, 0_45
口core-site.xml，Hadoop core的配置项，主要针对Common组件的属性配置。由于默认
 d/  xn easn-oopd/ pdoop 
be replicated to 0 nodes, instead of 1”异常，因此手工修改 hadoop.tmp.dir 指向 /data/
tmp/hadoop-S{user.name}，作为Hadoop用户的临时存储目录，配置如下：
cconfiguration>
(property>
---
## Page 225
204第二部分高级篇
hadoop. tmp.dir
/data/tmp/hadoop=$ (user nar
me}
Cname>f8 default ,name
hdfs: //192 ,168 ,1 .20 : 9000 //master 生机 IP: 9000 增 口
 hdfs-site.xml,Hadoop 的 HDFS 组件的配置项,包括 Namenode、Secondarynamenode
和 Datanode 等，配置如下：
Cname>dfs ,nane, dir
/data/hdfs/name
/ /Namenode 持久存储名字空间、事务日东路径
dfs, data, dir
/data/hdfs /data
/ / Dat.anode 数据存储路径
dfs, datanode,nax.xeievers
4096
// Datanode所允诗同时执行的复选和接受任务数量。既认为 256
dfs,replication
2
//数据备价的个数，默认为3
mapred. Job, tracker
192.168.1.20:9001
masters，配置Secondarynamenode项，环境使用主设备192.168.1.20同时承担
Secondarynamenode的角色，生产环境要求使用独立服务器，起到HDFS文件系统元
数据（metadata）信息的备份作用，当NameNode发生故障后可以快速还原数据，配
置内容如下：
---
## Page 226
第12章Python大数掘应用详解205
192.168.1.20
口slaves，配置所有 Slave 主机信息，填写IP地址即可。本示例中 Slave 的信息如下：
192.168.1,21
192.168.1.22
接下来，从主节点（Master）复制jdk及Hadoop环境到所有 Slave，目标路径要与
Master保持一致，切记！执行以下命令进行复制：
.[ eae[/zsn/ d- xpxw 11 [ eae[/zsn p- ]. Iz'T*89t*z6teooz qss 
.[eae/zsn/ d xpxe 11[ ese/xen/ p-1. zz**89*26teoox ye 
 scp -z /usr/java/dk1.6.0_45 roote192.168.1.21:/usr/?ava
//896 09//x-
 scp -z /usr/1ocal/hadoop-1.2.1 root8192.168.1.22:/u8r/1oca1
Hadoop部分功能是通过主机名来寻址的，因此需要配置主机名hosts信息（生产环境建
议直接搭建内网DNS服务），保证Hadooop环境所有主机的/ete/hosts文件配置如下：
192.168.1.20S8201308020
192.168.1.21
SN2013-08-021
192.168.1.22 .
SN2013-08-022
管理员通过浏览器查看datanode信息，需要配置本地hosts，如Windows7系统hosts文
件路径为C:1WindowsiSystem32\drivers\etc，添加所有datanode主机信息，如下：
192.168.1.21
S8I201308021
192.168.1.22
SN2013-08-022
如设备启用了iptables防火墙，需要对主节点（Master）及Slave主机添加以下规则：
Master:
Ld3 (- 02005 xodp-- do d- 2/0*t*99t*26t s- s0a1 I- saqed
32 (- 0006 12odp-- d d- 2/0*1*991.261 s- s0d81 1- stqe2d
30 (- 1006 4x0d-- d0 d- 920*1*991*26 8-401 1- s1qed
Slaves:
d30 (- sco0s zodp-- do d- 2/0*t*99t*26t s- na11 I- sotqesd
L820 (- 0900s xodp-- do d- 20*1*891*26 s- s081 I- s9tqe5
I3o 5- 0t00s 4aodp-- dos d- 02't*89t'26t s- 20d11 1- satqesd1
配置完成后在主节点（Master）上格式化文件系统的namenode，执行：
cd /usr/1oca1/hadoop-1 .2.1
---
## Page 227
206第二部分高级篇
 bin/hadoop namenode -format
最后，在主节点（Master）上执行启动命令，如下：
 bin/start-all,sh
（2）检验安装结果
Hadoop官方提供的一个测试MapReduce的示例，执行：
 bin/hadoop ar hadoop=examples=1.2.1.jar pi 10 100
如果返回如图12-1所示结果，则说明配置成功。
le
INFO
.abCls
ed.lobClie
图12-1计算pi的测试结果（部分截图）
访问 Hadoop 提供的管理页面，Map/Reduce管理地址：http://192.168.1.20:50030/，如
图12-2所示。
---
## Page 228
第12章Python大数据应用详解207
SN20i3-08-020 HadoopMap/ReduceAdministration
Started: FrI Aog 22 22:15:42 CST 2014
State: LNDG
Versien: 1.2.1, r1500152
Compiled: Mos Ju1 22 15:23:09 PDT 2013 by mo11f
Identifier120140822215
SafeMode: OFF
Cluster Summary  (Heap Size 1s 7.31 MB/966. 69 MB)
Mlap
Keduce
Totel
Nodes
Map
Redece
Reduce
Reduce
Subsissions
Slots
S1ots
Slets
Capaeity
Task
Tesks
Capaeity
0
]0
0
[0