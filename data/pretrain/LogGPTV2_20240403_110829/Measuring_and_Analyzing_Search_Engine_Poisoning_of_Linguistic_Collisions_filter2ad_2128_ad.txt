as the number of queries for which the search engine does not offer auto-correction either automatically or as a suggestion, and “% Poisoning”
is calculated as the percentage of non-auto-corrected queries which contain malicious URLs on the ﬁrst page of search results. For the “%
Non-Auto-Corrected” and “% Poisoning”, we also show the raw numbers of searches in parentheses.
(a) English experiment (on Google).
(b) Chinese experiment (on Baidu).
Figure 7: Comparison of search poisoning rates among different misspelling types per keyword category. The y-axis indicates the percentage of
searches that contained malicious URLs on the ﬁrst page of search results (for a given keyword category and misspelling protection type).
From left to right for each category, Original refers to searches made for the correctly spelled terms, while Showing-results-for,
Including-results-for, Did-you-mean, and Linguistic-collision (Non-auto-corrected) refer to types of auto-
correction offered for the searches as described in Section III. The different categories are described in Section V-A, note that “Defense Contractors”
is only present in the English experiment. The search poisoning rates of Linguistic-collision (Non-auto-corrected) are the
same values as “% Poisoning” columns in Table I.
Auxiliary information collection. In addition to the search
results collected from Google and Baidu, we also collected
information from VirusTotal, Google Adwords, Google Translate,
and Baidu Index. We used VirusTotal to identify URLs with
suspicious activity and then investigated further into the ﬂagged
results. In total, we collected scans for 2.06M URLs of which
1.18% (24.4k) had been detected by at least one scanner. To
improve the accuracy, we manually spot-checked the ﬂagged
URLs for malicious activity using a virtual machine which
eventually obtained 5,256 malicious URLs under 2,743 domains.
For the English search results, we checked the device breakdown
estimates for 117,791 uncorrected misspellings and 12,943
original keywords using the Google Adwords Keyword Planner
tool [48]. Using the Google Detect Language API we collected
105,978 predictions for the uncorrected misspellings in an attempt
to understand the distribution of how the language distribution
varies across different categories. The details for our language
results can be seen in Table III.
B. Results of RNN
The ﬁnal model used 150 hidden layers with a sequence
length of 5 characters. The vocabulary consisted of lower-case
alphanumerics and a null character for a total vocabulary size of
37 characters. To train the RNN model for different parameters,
we used 4 servers with 24 GB RAM and 16 CPU cores each.
The training set we used was a wordlist with 675,903 unique
words taken from several wordlists [49–52]. To select optimal
parameters, we checked each setting on completely separate
1318
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:50 UTC from IEEE Xplore.  Restrictions apply. 
% Non-Auto-
Corrected
# Misspell
Category
Candidates
(Alexa Top)
Poisoning
20,192
0.85% (28)
1–100
101–1,000
216,157 13.28% (28.7K) 0.78% (221)
(RNN) 1,001–10,000
61,088 38.04% (23.2K) 0.50% (116)
Table II: Data collection statistics based on Alexa top list (similar
header meanings as in Table I). Note that the results for the Alexa top
1,001–10,000 are collected using the RNN model’s predictions.
16.29% (3.2K)
%
validation data taken from the ground truth data on the Alexa
top 1,000 misspellings.
To evaluate the RNN’s performance and investigate mis-
spellings affecting less popular domains, we used the trained
RNN with the best performance on the Alexa 1,000 misspellings
to generate predictions for the 2.4 million misspellings from the
Alexa 10,000. From these predictions, we selected the keywords
with the lowest entropy from the predictions and used the
crawling framework to collect search results. The ground truth
data collected for the Alexa top 1,000 indicates that randomly
sampling the misspellings would yield a hit rate of about 13.28%.
Dictionary checking exhibited even lower performance on the
Alexa top 1,000 ground truth set with a 2.6% hit rate. The poor
performance of dictionary checking vs. random sampling can be
explained by the fact that many of the words are new, obscure,
or only in use as slang. Our RNN approach also outperforms
the naive Bayes and random forest algorithms. Due to space
limitation, more details are shown in Appendix A. Crawling
the 61,088 highest conﬁdence predictions from the RNN gave
a non-auto-corrected rate of 38.04% with 23,236 uncorrected
misspellings. Compared to random sampling, the RNN gave a
performance improvement of 2.84x.
VI. MEASUREMENT AND DISCOVERIES
In this section, we present ﬁndings from our study, including
landscape of the abuses, characteristics of the linguistic-collision
misspellings, and estimates of search volumes for cybercriminals.
We also provide deep analysis of two interesting cases.
A. Landscape and Comparison of Misspelling Search Results
First, we examine how pervasive the linguistic-collision
misspelling SEO is. In fact, we ﬁnd linguistic collisions are
widely existent: 15.16% of the English misspelling keywords
that we generate using edit distance 1 are not auto-corrected, and
7.69% of the Chinese misspelling terms based on the fat-ﬁnger,
fuzzy pinyin, and same pronunciation generation methods are
not auto-corrected. Because users primarily click search results
returned on the ﬁrst page [53], we only checked to see whether
the ﬁrst page of search results has been poisoned.
Blacklist statistics. To determine whether or not a URL was
potentially malicious, we checked VirusTotal for reports of
malicious activity from that URL. In total, we determine that
1,511 URLs from ﬁrst-page results (10 results per ﬁrst page) of
non-auto-corrected searches are malicious. Correspondingly,
0.98% (1,872) of English linguistic-collision search terms on
Google result in ﬁrst-page blacklisted URLs, and 1.39% (538)
of Chinese linguistic-collision terms show poisoned results
on the ﬁrst pages on Baidu. The observation indicates that
Figure 8: Longitudinal view of the poisoned non-auto-corrected search
result rate over Alexa terms (1,001–10,000 using the RNN predictions).
The results are binned by the original term’s Alexa rank with the x-axis
labels denoting the bucket lower and upper bounds, e.g., 2k covers the
range of 1,001–2,000.
linguistic-collision misspelling SEO has widespread impact, and
cybercriminals can comparatively easily manipulate rankings and
promote their pages index by linguistic-collision misspellings.
Per-category results. As mentioned in Section V, the English
misspellings were split
into two major sets, per-category
keywords and Alexa domains. Table I describes the per-category
datasets for Chinese and English. The ﬁrst column shows
the category names. We have 13 categories, and 12 of them
are present in both Chinese and English (“Defense” category
only has keywords in English and contains the names of the
100 largest defense contractors around the world). The fourth
and eighth columns “% Non-Auto-Corrected” represent the
proportion of misspelling queries not auto-corrected by search
engines, regarding English and Chinese respectively. The ﬁfth
and last columns “% Poisoning” indicate the percentage of
non-auto-corrected queries containing VirusTotal blacklisted
URLs on the ﬁrst-page search results, regarding English and
Chinese respectively. We also include raw numbers of searches
in parentheses in Table I. There are two observations: (1) A
considerable portion of misspellings (> 4.5% for all categories)
result in linguistic collisions that will not be auto-corrected by
search engines, and (2) many linguistic-collision misspelling
searches lead to malicious websites appearing on the ﬁrst pages
of search results.
To compare linguistic-collision misspelling to other types
misspelling searches, we queried all misspell candidates that
we generated (column “# Misspell Candidates” in Table I) and
the original target keywords (column “# Target” in Table I)
from the search engines. Figure 7 shows the poisoning rates
for English and Chinese by category and level of correction
from the search engines. We ﬁnd that indeed attackers more suc-
cessfully target linguistic-collision (Non-auto-corrected)
misspellings than misspellings that are protected by the different
types of auto-correction discussed in Section III. On average
linguistic-collision misspellings are poisoned at a rate of
1.19% across English and Chinese categories as compared to
1319
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:50 UTC from IEEE Xplore.  Restrictions apply. 
All Results
English
Arabic
Spanish
Hindi
Italian
Alexa top 1K
57.44% English
2.76% Arabic
1.66% Hindi
1.56% Welsh
1.53% Danish
Gambling
Adult Terms
Drugs
66.44% English
74.04% English
40.67% English
1.91% Spanish
2.69% French
5.42% Latin
1.44% Norwegian 2.14% Spanish
2.19% Spanish
1.33% Italian
2.18% Italian
1.01% French
1.68% Romanian 2.25% Hindi
Table III: Per-category breakdown of language statistics.
Software
49.28% English
3.69% Italian
2.82% Arabic
2.47% Spanish
1.78% Indonesia
1.68% Polish
81.67%
1.96%
1.30%
1.05%
0.79%
0.16% for Original, 0.18% for Showing-results-for,
0.23% for Including-results-for, and 0.47% for
Did-you-mean terms.
We observe that the “Drugs”, “Gambling”, and “Adult Terms”
categories exhibit higher rates of poisoned non-auto-corrected
searches at 2.86% on average than other categories which
exhibit average rates of 0.66%. These terms are more easily
monetized than searches for more benign terms such as “Food” or
“Cosmetic” products, as the attackers can easily enroll in afﬁliate
ad programs [54]. Additionally, malicious attackers (as opposed
to those simply looking for ad revenue) may rationalize that
users performing these searches may be more willing to ignore
suspicious patterns in URLs or even explicit warning messages
by browsers to access the advertised content. Finally, other search
engine products such as Google Autocomplete have avoided
optimizing and maintaining “inappropriate” predictions for search
queries such as adult terms [55]. In contrast to the aforementioned
three categories, “Software” linguistic-collision misspellings do
not result in high poisoning rates. The comparatively lower
exploitation is presumably due to current success of traditional
SEO methods for these keywords (note the high poisoning rates
for Original terms in the English “Software” category).
However, because cybercriminals have historically targeted
software terms [18, 39], we continue to include “Software”
in our analyzed categories in Section VI-B.
While the English “Drugs”, “Gambling”, and “Adult Terms”
categories include poisoned searches for misspellings with every
type of correction, the corresponding Chinese categories contain
poisoned searches almost exclusively for linguistic-collision
misspellings. The disparity between the two is conjectured as
an artifact of Baidu’s ranking algorithm to prioritize URLs
under reputed domains. We ﬁnd that on Baidu 91.3% of
search results for the Original, Showing-results-for,
Including-results-for, and Did-you-mean terms
are under only 1,000 domains (with baidu.com alone
accounting for 42.7% of results). In contrast, these 1,000 domains
account for 83.3% of the results in linguistic-collision misspelling
searches. The observations indicate that Baidu exercises less
caution on linguistic-collision misspelling searches and is likely
to include malicious results.
Alexa top list results. Table II describes the results from the
Alexa misspellings (with similar header meanings as in Table I).
To investigate the trends and long-tail effect, we use the Alexa
top 100, 1,000, and 10,000 website names as target keywords
respectively. As mentioned in Section V, the results for the
Alexa domains ranked between 1,000 and 10,000 are selected
using the RNN described in Section IV. In particular, we crawled
61,088 misspellings which received the lowest entropy from the
RNN’s entropy estimator. The Alexa 1,000 ground truth dataset
blacklist rate is 0.78% with 221 poisoned searches. Interestingly,
we see the rate of blacklisted results remains fairly constant
based on the RNN results with an average of 0.50% in the
Alexa top 1,000–10,000 (116 poisoned searches). Figure 8 shows
the longitudinal distribution of attacker activity. On average,
0.54% of the non-auto-corrected results in the Alexa dataset are
poisoned. Longitudinally, we ﬁnd that the level maliciousness
is high for the Alexa 100 and 1K, indicating cybercriminals
target more on popular domains. After reaching the lowest for
the 3K domains, the poison rate slowly increases over the long-
tail. Szurdi et al. observed similar long-tail effect on domain
typosquatting [47]. Lower popularity domains may have fewer
resources to check for poisoned search results, less risk of
litigation, and less competition from other cybercriminals.
B. Characteristics of Linguistic-collision Search Results
Next we investigate the detailed properties of misspelling
search results that lead to malicious websites.
Comparison of misspelling generation. Intuitively, we would
expect users to generate some types of misspellings more
frequently than others either through mistyping or confusing the
spelling of the original term. For the English results, we compare
the non-auto-corrected rate for the wrong vowel substitution
method to the average for all misspelling generation, while for
Chinese we compare the same pronunciation terms and fuzzy
pinyin method to the rest of the misspellings. Because these
methods produce misspellings that are closer to the original
keyword than the edit-distance 1 heuristics, we would expect
these methods to produce more linguistic-collision misspellings.
Indeed, we ﬁnd that for English the wrong vowel method
produces a non-auto-corrected rate of 22.85% as compared to the
edit-distance 1 misspellings which showed a non-auto-corrected
rate of 15.16%. Similarly, for Chinese the more realistic methods
outperform the fat-ﬁnger misspellings with same pronunciation
keywords uncorrected 18.21% of the time and fuzzy pinyin
escaping auto-correction for 17.63% of misspellings. Meanwhile,
for Chinese the edit distance 1 data set resulted in a non-auto-
corrected rate of 7.69%.
Language distribution of linguistic collisions. To determine
why Google would fail to correct so many misspellings, we
used the Google Translate API to detect the language which
returned the detected language and the prediction conﬁdence. The
Google Translate API reported that the uncorrected misspellings
contained words from 74 languages, while many of the non-
English predictions had lower conﬁdence manual spot-checking
shows that many of these misspellings are actually valid words
in other languages. To better understand the breakdown, we
1320
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:50 UTC from IEEE Xplore.  Restrictions apply. 
# of
Trafﬁc
# of
URLs
Poisoned
Searches
732
63
58
49
40
monetization