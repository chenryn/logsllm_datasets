# A Large-Scale Study of Failures in High-Performance Computing Systems

**Authors:**
- Bianca Schroeder
- Garth A. Gibson

**Affiliation:**
- Computer Science Department, Carnegie Mellon University
- Pittsburgh, PA 15217, USA
- Email: {bianca, garth}@cs.cmu.edu

## Abstract
Designing highly dependable systems requires a thorough understanding of failure characteristics. Unfortunately, little raw data on failures in large IT installations is publicly available. This paper analyzes failure data recently made public by one of the largest high-performance computing (HPC) sites. The data, collected over nine years at Los Alamos National Laboratory (LANL), includes 23,000 failures recorded across more than 20 different systems, primarily large clusters of Symmetric Multi-Processing (SMP) and Non-Uniform Memory Access (NUMA) nodes. We study the statistical properties of the data, including the root cause of failures, mean time between failures (MTBF), and mean time to repair (MTTR). Our findings show that average failure rates vary widely across systems, ranging from 20 to 1,000 failures per year, and that the time between failures is well-modeled by a Weibull distribution with a decreasing hazard rate. Mean repair times vary from less than an hour to more than a day and are well-modeled by a lognormal distribution.

## 1. Introduction
Research in dependable computing relies heavily on a deep understanding of real-world system failures. For example, knowledge of failure characteristics can be used to improve cluster availability through resource allocation [5, 25]. The design and analysis of checkpoint strategies depend on certain statistical properties of failures [8, 21, 23]. Creating realistic benchmarks and testbeds for reliability testing also requires an understanding of the characteristics of real failures.

Unfortunately, obtaining access to failure data from modern, large-scale systems is challenging, as such data is often sensitive or classified. Existing studies of failures are often based on only a few months of data, covering typically a few hundred failures [19, 24, 16, 18, 15, 7]. Many commonly cited studies on failure analysis date back to the late 1980s and early 1990s, when computer systems were significantly different from today's [3, 4, 6, 13, 19, 9, 11]. Furthermore, none of the raw data used in these studies has been made publicly available for use by other researchers.

This paper accompanies the public release of a large set of failure data [1]. The data was collected over the past nine years at LANL and covers 22 HPC systems, including a total of 4,750 machines and 24,101 processors. Each failure entry includes start and end times, the affected system and node, and categorized root cause information. To our knowledge, this is the largest set of failure data studied in the literature to date, both in terms of the time period it spans and the number of systems and processors it covers.

Our goal is to provide a description of the statistical properties of the data and to offer guidance to other researchers on how to interpret the data. We first describe the environment from which the data comes, including the systems and workloads, the data collection process, and the structure of the data records (Section 2). Section 3 details our data analysis methodology. We then examine the data with respect to three important properties of system failures: root causes (Section 4), time between failures (Section 5), and time to repair (Section 6). Section 7 compares our results with related work, and Section 8 concludes the paper.

## 2. Description of the Data and Environment

### 2.1 The Systems
The data spans 22 HPC systems that have been in production use at LANL between 1996 and November 2005. Most of these systems are large clusters of either NUMA nodes or 2-way and 4-way SMP nodes. In total, the systems include 4,750 nodes and 24,101 processors. Table 1 provides an overview of the 22 systems.

| HW ID | Nodes | Procs | Procs/Node | Production Time | Mem (GB) | NICs |
|-------|-------|-------|------------|-----------------|----------|------|
| 1     | 1     | 8     | 8          | N/A – 12/99     | 16       | 0    |
| 2     | 1     | 32    | 32         | N/A – 12/03     | 8        | 1    |
| 3     | 1     | 4     | 4          | N/A – 04/03     | 1        | 0    |
| ...   | ...   | ...   | ...        | ...             | ...      | ...  |
| 22    | 256   | 1024  | 4          | 11/04 – now     | 1024     | 0    |

Table 1: Overview of systems. Systems 1–18 are SMP-based, and systems 19–22 are NUMA-based.

The left half of Table 1 provides high-level information for each system, including the total number of nodes and processors, and a system ID. The data does not include vendor-specific hardware information but uses capital letters (A-H) to denote a system’s processor/memory chip model. We refer to a system’s label as its hardware type.

As the table shows, the LANL site has hosted a diverse set of systems. The number of nodes ranges from 1 to 1,024, and the number of processors ranges from 4 to 6,152. Systems also vary in their hardware architecture, with a large number of NUMA and SMP-based machines and a total of eight different processor and memory models (types A–H).

Nodes within a system are not always identical. While all nodes in a system have the same hardware type, they may differ in the number of processors, network interfaces (NICs), amount of main memory, and production time. The right half of Table 1 categorizes the nodes in a system with respect to these properties. For example, the nodes of system 12 fall into two categories, differing only in the amount of memory per node (4 GB vs. 16 GB).

### 2.2 The Workloads
Most workloads are large-scale, long-running 3D scientific simulations, such as those for nuclear stockpile stewardship. These applications perform long periods (often months) of CPU computation, interrupted every few hours by a few minutes of I/O for checkpointing. Simulation workloads are often accompanied by scientific visualization of large-scale data. Visualization workloads are also CPU-intensive but involve more reading from storage than compute workloads. Some nodes are used purely as front-end nodes, while others run more than one type of workload, e.g., graphics nodes often run compute workloads as well.

At LANL, failure tolerance is frequently implemented through periodic checkpointing. When a node fails, the job(s) running on it is stopped and restarted on a different set of nodes, either starting from the most recent checkpoint or from scratch if no checkpoint exists.

### 2.3 Data Collection
The data is based on a "remedy" database created at LANL in June 1996. At that time, LANL introduced a site-wide policy requiring system administrators to enter a description of every failure they address into the remedy database. Consequently, the database contains a record for every failure that occurred in LANL’s HPC systems since June 1996 and required intervention by a system administrator.

A failure record includes the time when the failure started, the time it was resolved, the affected system and node, the type of workload running on the node, and the root cause. The workload is categorized as either "compute" for computational workloads, "graphics" for visualization workloads, or "fe" for front-end. Root causes fall into one of five high-level categories: Human error, Environment (including power outages or A/C failures), Network failure, Software failure, and Hardware failure. More detailed information on the root cause, such as the specific hardware component affected by a Hardware failure, is also captured. Additional information on the root causes can be found in the released data [1]. The failure classification and rules for assigning failures to categories were developed jointly by hardware engineers, administrators, and operations staff.

Failure reporting at LANL follows a specific protocol. Failures are detected by an automated monitoring system that pages operations staff whenever a node is down. The operations staff then create a failure record in the database, specifying the start time of the failure and the affected system and node, and turn the node over to a system administrator for repair. Upon repair, the system administrator notifies the operations staff, who then return the node to the job mix and fill in the end time of the failure.

## 3. Methodology
[Detailed methodology section to be added here, describing the data analysis approach and any specific techniques or tools used.]

## 4. Root Causes of Failures
[Detailed analysis of the root causes of failures, including statistical breakdowns and key findings.]

## 5. Time Between Failures
[Detailed analysis of the time between failures, including statistical distributions and key findings.]

## 6. Time to Repair
[Detailed analysis of the time to repair, including statistical distributions and key findings.]

## 7. Comparison with Related Work
[Comparison of the findings with existing literature and previous studies, highlighting similarities and differences.]

## 8. Conclusion
[Summary of the key findings, implications for future research, and potential applications of the data.]

---

**Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06)**
**0-7695-2607-1/06 $20.00 © 2006 IEEE**
**Authorized licensed use limited to: Tsinghua University. Downloaded on March 19, 2021, at 12:29:50 UTC from IEEE Xplore. Restrictions apply.**

---

This version of the text is more structured and coherent, with clear headings and subheadings. The content is presented in a logical order, and the language is professional and precise.