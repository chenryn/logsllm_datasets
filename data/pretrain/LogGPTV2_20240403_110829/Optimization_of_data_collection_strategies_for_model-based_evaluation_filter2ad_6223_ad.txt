4.8543
[30, 130]
1.5637
7.4728
1.2861
1.3333
0.98315
4.0995
1.7289
1.2223
1
S1
S2
S3
S4
S5
S6
S7
S8
S9
efﬁciency of the algorithms. That is, we need to calculate the
variance of the variance (the variance of V ar[E(Y )|s]) under
the Basic Algorithm 1 and Importance Sampling Algorithm 2,
respectively.
Table I shows the resulting quotients based upon results
for Optimization Problem 2. There are 9 strategies in S, and
each of them is used as a possible anchor, each anchor one
column in Table I. For each anchor strategy, the element in the
third row gives the number of samples for each source, that is
[D1,1, D2,1]. The table strategy sample sizes are in increments
of 50, each strategy staying within a budget of 200. For each
anchor a ∈ S, each row shows Qa,s for all other strategies s ∈
S. Qa,s is the estimated multiplier of additional iterations the
Importance Sampling Algorithm would need for s to achieve
the same level of accuracy as a run with Basic Algorithm.
For instance, the value 5.398 for anchor strategy a = S4 and
target strategy s = S1 implies that the Importance Sampling
Algorithm with anchor a = S4 requires 5.398 times as many
iterations to obtain results for S1 as the Basic Algorithm for
S1.
If the number of model results needed to evaluate a strategy
equals K and |S| is number of different strategies. Without
importance sampling, it takes K × |S| iterations of the Basic
Algorithm. If importance sampling using only one strategy in
place of |S|. At a selected anchor, for importance sampling
to be worthwhile M ax(Qa,s) must be less than |S|, or else
achieving a sufﬁcient level of accuracy requires more model
results than direct execution. For example S1 makes a good
anchor strategy since the maximum quotient is approximately
1.8, less than |S|.
M/M/1 successfully demonstrates the approach and poten-
tial for re-using results via important sampling. The example
requires only that p1 < p2. More realistic and complex models
will likely involve greater input dependency, which would need
to be taken into account during experiment generation (Normal
sampling) phase. Unless the model is very robust, the valid
ranges for inputs their parameters may be more dependent
on other input parameters, requiring more conscientiousness
design of strategy experiments. This problem and alternative
experiment generation methods will need to be explored in
future examples. The important sampling results show useful
speed-up is possible by reusing results for M/M/1 example.
The beneﬁt greatly depends upon choice of anchor strategy or
more realistically multiple anchor strategies, and the accuracy
stopping condition. Both of these need further exploration and
formalizing for more complex examples.
V. RELATED WORK
The data collection strategy optimization approach in this
paper combines aspects of two main strands of analysis,
namely sensitivity and uncertainty analysis [9],[14] [5], and
adds to that speciﬁc detailed techniques based on statistics (the
use of the Central Limit Theorem and importance sampling).
In this related work section, it is most opportune to compare
with the forms of analysis, since background on sample theory
and statistics is largely available from text books. Overall,
the main difference with the rich and varied body of related
research in sensitivity and uncertainty analysis is that
the
research reported in this paper focuses on data collection
strategies, which creates a different set of optimization prob-
lems. To solve these problems, we use ideas from these well-
studied types of analysis, as we explain now. Note that the
work by Freimer and Schruben [7] is closest to our work,
although following a very different statistics oriented approach
for a more limited set of problems. We discuss [7] at the end
of this section.
input values impact
The objective of uncertainty analysis is to understand how
different
the model output, while the
objective of sensitivity analysis is to understand the impor-
tance of individual input parameters on model output [15]
[5]. Both analysis approaches are widely used in economics,
statistics, physics and many other areas [16]. For discrete-
event systems, uncertainty analysis implies in practice that
for different
input parameter values models are evaluated
and outputs are collected. Techniques including scatter plots,
regression and partial correlation analysis can then be applied
to analyse the set of outputs obtained from the many runs
[17]. Such approaches have similar implications in terms
of computational effort required as our approach [18], thus
leading to many proposals for speeding up the solution [16].
Of particular interest for the current paper are the uncer-
tainty analysis approaches that relate input distributions to
output distribution, also within the class of sampling based ap-
proaches. The extensive study in [15] of methods that express
correlation between input and output distribution demonstrates
the richness of the area. The Bayesian methods discussed for
instance in [19] also belong to this class, determining the
output distribution given a prior of the probability distribution
of the inputs. The work on resampling in [20] [21], which aims
at improving the model output conﬁdence interval to better
take into account input or input parameter uncertainty is also
of interest, but in our setting generating samples for the input
parameters is not expensive, so approaches such as bootstrap
resampling [21] do not apply. None of the uncertainty analysis
methods discuss the issue of data source selection, but some of
the techniques may be a effective complementary techniques
to be added to the approach discussed in this paper.
In addition, uncertainty analysis also studies more efﬁcient
quasi-random sample generation methods such as Latin hyper-
cube sampling [22], to produce stratiﬁed samples or orthogonal
experiment sets. Such variance reduction approaches may also
be worth exploring in our setting. Importance sampling [23]
has also been mentioned as an approach [24] [16], but as
a generic tool importance sampling is known to be difﬁcult
to conﬁgure in a manner that it speeds up the simulation,
as reported in [17]. In our approach, the fact that different
strategies are being explored makes that importance sampling
comes at almost no risk, since the anchor strategy results
are available and might as well be used. Again, this is a
consequence of the unique focus of our work on optimizing
the data collection strategy.
Sensitivity analysis aims at selecting the parameters most
important, using techniques such as those within the classes
of screening methods [16] and variance-based methods [9].
This also strongly relates to our approach, as it requires
identiﬁcation of the more important input parameters (factors
in sensitivity analysis terminology). For instance, [6] provides
an analytic Bayesian approach to reduce input uncertainty in
queuing models, but such an analytic solution is not gener-
ically applicable for discrete-event dynamic systems. Note
furthermore that such sensitivity analysis closely relates to
experimental design [5]. Our work is an approach to determine
an optimal design, in that we aim to ﬁnd the experimental set
up that leads to the lowest variance, where the experimental
set up is the data collection strategy. Research in design
of experiments tends to rely on information matrices, which
provide the combined information about multiple parameters
of a statistical model. In the literature on experimental design,
we have not come across an end-to-end set-up as in our
approach, with one notable exception [7]. [7] does consider
data collection strategies, identifying a number of reasonable
approaches to solve the problem of determining how many
more samples to generate. This is not the same as ﬁnding
an optimal data collection strategy, but
is closely related.
In their concluding section on data collection for multiple
unknown parameters, they indicate a related approach to what
we propose in this paper, framed as a statistical test. They
only consider two inputs, and proposed method is not worked
out in detail or applied, and it is not clear if it generalizes
to general strategies with arbitrary number of data sources,
possibly including cost.
Summarising, there is a large body of related literature in
the areas of uncertainty and sensitivity analysis, but rarely in
the context of data collection strategies. This implies that the
formulation as optimization problem, the basic algorithm, as
well as the importance sampling inspired algorithm do not
have direct counterparts in existing literature.
VI. CONCLUSION
This paper proposes a method to decide on data collection
strategies that aid in a decision-making process in which
probabilistic models are used to justify and support decisions.
The main idea behind the method is to combine elements of
uncertainty analysis and sensitivity analysis into a comprehen-
sive approach to determine data collection strategies. Based
on a model of uncertainty of the input parameters and the
associated data sources, solution of the original model for the
variance conditional to various strategies yields insight in the
optimal data collection strategy. The approach is especially
natural in studies in which data sources are available that use
sampling to determine a parameter value, since in that case
the Central Limit Theorem indicates the use of the Normal
distribution to model the uncertainty of data sources. Such
studies become increasingly prevalent in dependability evalu-
ation, since human factors and business concerns increasingly
often are taken into account in modern-day studies.
The paper pays particular attention to the efﬁciency of
the approach, since a naive implementation would require
substantial computational effort to solve models for all strate-
gies. We therefore introduced an importance sampling inspired
approach to derive results for multiple strategies from a
single computation. Through an example we showed that
considerable speed-up can be obtained, but that the application
of the importance sampling idea may be challenging. Further
study is necessary to fully appreciate how to best apply the
importance sampling based algorithm, for increasingly sophis-
ticated models. Moreover, other variance reduction techniques
may be appropriate for our approach. In addition, there are a
number of other subtleties, features and assumptions that come
with the method, and we have discussed these extensively
in this paper. Each of them allows for further ﬁne-tuning
and alternative approaches. We believe this is worth pursuing
further and believe that the ideas presented in this paper offer a
promising and fruitful new way of determining data collection
strategies that is particularly relevant for a number of modern-
day studies.
ACKNOWLEDGMENT
The authors would like to thank Hewlett-Packard for fund-
ing through its Innovation Research Program for ‘Predic-
tion and Provenance for Multi-Objective Information Secu-
rity Management’, UK TSB for ‘Trust Economics’ Network
Innovation Platform, and UK EPSRC for grant EP/G011389
‘Analysis of Massively Parallel Stochastic Systems’. Sincere
thanks to our project partners D. Eskins, R. Berthier, W.
Sanders, S. Parkin and J. Turland for the many inspiring
discussions that led to the ideas formulated in this paper.
REFERENCES
[21] R. Barton, B. Nelson, and W. Xie, “A framework for input uncertainty
the
analysis,” Winter Simulation Conference (WSC), Proceedings of
2010, pp. 1189–1198, 2010.
[22] M. McKay, J. Morrison, and S. Upton, “Evaluating prediction uncer-
tainty in simulation models,” Computer Physics Communications, vol.
117, no. 1-2, pp. 44–51, 1999.
[23] A. Owen and Y. Zhou, “Safe and Effective Importance Sampling,”
Journal of the American Statistical Association, vol. 95, pp. 135–143,
2000.
[24] R. Breeding, J. Helton, E. Gorham, and F. Harper, “Summary description
of the methods used in the probabilistic risk assessments for NUREG-
1150,” Nuclear Engineering and Design, vol. 135, no. 1, pp. 1 – 27,
1992.
[1] A. Beautement, R. Coles, J. Grifﬁn, C. Ioannidis, B. Monahan, D. Pym,
M. A. Sasse, and M. Wonham, “Modelling the human and technological
costs and beneﬁts of USB memory stick security,” Managing Informa-
tion Risk and the Economics of Security, pp. 141–163, 2009.
[2] S. Parkin, R. Yassin Kassab, and A. van Moorsel, “The impact of
unavailability on the effectiveness of enterprise information security
technologies,” Service Availability, pp. 43–58, 2008.
[3] W. Zeng and A. van Moorsel, “Quantitative Evaluation of Enterprise
D Technology,” Electronic Notes in Theoretical Computer Science, vol.
275, pp. 159–174, 2011.
[4] G. Horvath, P. Buchholz, and M. Telek, “A MAP ﬁtting approach with
independent approximation of the inter-arrival time distribution and the
lag correlation,” in Quantitative Evaluation of Systems, 2005. Second
International Conference on the, Sep. 2005, pp. 124–133.
[5] A. Saltelli, M. Ratto, T. Andre, F. Campolongo, J. Cariboni, D. Gatelli,
M. Saisana, and S. Tarantola, Global Sensitivity Analysis. The Primer.
John Wiley, 2008.
[6] S. Ng and S. Chick, “Reducing input parameter uncertainty for simu-
lations,” Proceedings of the 33nd conference on Winter simulation, pp.
364–371, 2001.
[7] M. Freimer and L. Schruben, “Collecting data and estimating parameters
for input distributions,” Simulation Conference, 2002. Proceedings of the
Winter, vol. 1, pp. 392–399 vol.1, 2002.
[8] Y.-C. Ho, “Introduction to special issue on dynamics of discrete event
systems,” Proceedings of the IEEE, vol. 77, no. 1, pp. 3–6, Jan. 1989.
[9] K. Chan, A. Saltelli, and S. Tarantola, “Sensitivity analysis of model
output: variance-based methods make the difference,” Proceedings of
the 29th conference on Winter simulation, pp. 261–268, 1997.
[10] A. B. Massada and Y. Carmel, “Incorporating output variance in local
sensitivity analysis for stochastic models,” Ecological Modelling, vol.
213, no. 3-4, pp. 463 – 467, 2008.
[11] A. van Moorsel, L. Kant, and W. Sanders, “Computation of the asymp-
totic bias and variance for simulation of Markov reward models,” in
Simulation Symposium, 1996. Proceedings of the 29th Annual.
IEEE,
1996, pp. 173–182.
[12] W. Cochran, Sampling Techniques, 3rd Edition.
[13] D. Draper, “Assessment and Propagation of Model Uncertainty,” Journal
of the Royal Statistical Society. Series B (Methodological), vol. 57, pp.
45–97, 1995.
John Wiley, 1977.
[14] J. Ascough, T. Green, L. Ma, and L. Ahuja, “Key criteria and selection
of sensitivity analysis methods applied to natural resource models,”
International Congress on Modeling and Simulation Proceedings, 2005.
[15] J. Helton and F. Davis, “Illustration of Sampling-Based Methods for
Uncertainty and Sensitivity Analysis,” Risk Analysis, vol. 22, p. 591622,
2002.
[16] J. P. Kleijnen, “Sensitivity analysis and related analyses: a review of
some statistical techniques,” Journal of Statistical Computation and
Simulation, vol. 57, no. 1, pp. 111–142, 1997.
[17] J. Helton, J. Johnson, C. Sallaberry, and C. Storlie, “Survey of Sampling-
Based Methods for Uncertainty and Sensitivity Analysis,” Reliability
Engineering and System Safety, pp. 1175–1209, 2006.
[18] I. Gluhovsky, “Determining output uncertainty of computer system
models,” Perform. Eval., vol. 64, pp. 103–125, February 2007.
[19] T. Sun and J. Wang, “A simple model for assessing output uncertainty
in stochastic simulation systems.” in MICAI’07, 2007, pp. 337–347.
[20] R. Cheng and W. Holland, “Calculation of conﬁdence intervals for
simulation output,” ACM Transactions on Modeling and Computer
Simulation (TOMACS), vol. 14, no. 4, pp. 344–362, 2004.