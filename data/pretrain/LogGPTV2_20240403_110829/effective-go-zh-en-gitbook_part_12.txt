process(r) // May take a long time.
 接受之前，参见 Go 内存模
型），因此信号必须在信道的接收端获取，而非发送端。
93
并发
This design has a problem, though: Serve creates a new goroutine for every incoming
request, even though only MaxOutstanding of them can run at any moment. As a result, the
program can consume unlimited resources if the requests come in too fast. We can address
that deficiency by changing Serve to gate the creation of the goroutines. Here's an obvious
solution, but beware it has a bug we'll fix subsequently:
然而，它却有个设计问题：尽管只有 MaxOutstanding 个 Go 程能同时运行，但 Serve 还是为
每个进入的请求都创建了新的 Go 程。其结果就是，若请求来得很快， 该程序就会无限地消
耗资源。为了弥补这种不足，我们可以通过修改 Serve 来限制创建 Go 程，这是个明显的解
决方案，但要当心我们修复后出现的 Bug。
func Serve(queue chan *Request) {
for req := range queue {
sem <- 1
go func() {
process(req) // Buggy; see explanation below.
<-sem
}()
}
}
func Serve(queue chan *Request) {
for req := range queue {
sem <- 1
go func() {
process(req) // 这儿有 Bug，解释见下。
<-sem
}()
}
}
The bug is that in a Go for loop, the loop variable is reused for each iteration, so the req
variable is shared across all goroutines. That's not what we want. We need to make sure
that req is unique for each goroutine. Here's one way to do that, passing the value of req as
an argument to the closure in the goroutine:
Bug 出现在 Go 的 for 循环中，该循环变量在每次迭代时会被重用，因此 req 变量会在所有的
Go 程间共享，这不是我们想要的。我们需要确保 req 对于每个 Go 程来说都是唯一的。有一
种方法能够做到，就是将 req 的值作为实参传入到该 Go 程的闭包中：
94
并发
func Serve(queue chan *Request) {
for req := range queue {
sem <- 1
go func(req *Request) {
process(req)
<-sem
}(req)
}
}
Compare this version with the previous to see the difference in how the closure is declared
and run. Another solution is just to create a new variable with the same name, as in this
example:
比较前后两个版本，观察该闭包声明和运行中的差别。 另一种解决方案就是以相同的名字创
建新的变量，如例中所示：
func Serve(queue chan *Request) {
for req := range queue {
req := req // Create new instance of req for the goroutine.
sem <- 1
go func() {
process(req)
<-sem
}()
}
}
func Serve(queue chan *Request) {
for req := range queue {
req := req // 为该 Go 程创建 req 的新实例。
sem <- 1
go func() {
process(req)
<-sem
}()
}
}
It may seem odd to write
它的写法看起来有点奇怪
req := req
95
并发
but it's a legal and idiomatic in Go to do this. You get a fresh version of the variable with the
same name, deliberately shadowing the loop variable locally but unique to each goroutine.
但在 Go 中这样做是合法且惯用的。你用相同的名字获得了该变量的一个新的版本， 以此来
局部地刻意屏蔽循环变量，使它对每个 Go 程保持唯一。
Going back to the general problem of writing the server, another approach that manages
resources well is to start a fixed number of handle goroutines all reading from the request
channel. The number of goroutines limits the number of simultaneous calls to process. This
Serve function also accepts a channel on which it will be told to exit; after launching the
goroutines it blocks receiving from that channel.
回到编写服务器的一般问题上来。另一种管理资源的好方法就是启动固定数量的 handle Go
程，一起从请求信道中读取数据。Go 程的数量限制了同时调用 process 的数量。Serve 同样
会接收一个通知退出的信道， 在启动所有 Go 程后，它将阻塞并暂停从信道中接收消息。
func handle(queue chan *Request) {
for r := range queue {
process(r)
}
}
func Serve(clientRequests chan *Request, quit chan bool) {
// Start handlers
for i := 0; i < MaxOutstanding; i++ {
go handle(clientRequests)
}
<-quit // Wait to be told to exit.
}
func handle(queue chan *Request) {
for r := range queue {
process(r)
}
}
func Serve(clientRequests chan *Request, quit chan bool) {
// 启动处理程序
for i := 0; i < MaxOutstanding; i++ {
go handle(clientRequests)
}
<-quit // 等待通知退出。
}
Channels of channels
96
并发
信道中的信道
One of the most important properties of Go is that a channel is a first-class value that can be
allocated and passed around like any other. A common use of this property is to implement
safe, parallel demultiplexing.
Go 最重要的特性就是信道是一等值，它可以被分配并像其它值到处传递。 这种特性通常被用
来实现安全、并行的多路分解。
In the example in the previous section, handle was an idealized handler for a request but we
didn't define the type it was handling. If that type includes a channel on which to reply, each
client can provide its own path for the answer. Here's a schematic definition of type Request.
在上一节的例子中，handle 是个非常理想化的请求处理程序， 但我们并未定义它所处理的请
求类型。若该类型包含一个可用于回复的信道， 那么每一个客户端都能为其回应提供自己的
路径。以下为 Request 类型的大概定义。
type Request struct {
args []int
f func([]int) int
resultChan chan int
}
The client provides a function and its arguments, as well as a channel inside the request
object on which to receive the answer.
客户端提供了一个函数及其实参，此外在请求对象中还有个接收应答的信道。
func sum(a []int) (s int) {
for _, v := range a {
s += v
}
return
}
request := &Request{[]int{3, 4, 5}, sum, make(chan int)}
// Send request
clientRequests <- request
// Wait for response.
fmt.Printf("answer: %d\n", <-request.resultChan)
97
并发
func sum(a []int) (s int) {
for _, v := range a {
s += v
}
return
}
request := &Request{[]int{3, 4, 5}, sum, make(chan int)}
// 发送请求
clientRequests <- request
// 等待回应
fmt.Printf("answer: %d\n", <-request.resultChan)
On the server side, the handler function is the only thing that changes.
func handle(queue chan *Request) {
for req := range queue {
req.resultChan <- req.f(req.args)
}
}
There's clearly a lot more to do to make it realistic, but this code is a framework for a rate-
limited, parallel, non-blocking RPC system, and there's not a mutex in sight.
要使其实际可用还有很多工作要做，这些代码仅能实现一个速率有限、并行、非阻塞 RPC 系
统的 框架，而且它并不包含互斥锁。
Parallelization
并行化
Another application of these ideas is to parallelize a calculation across multiple CPU cores. If
the calculation can be broken into separate pieces that can execute independently, it can be
parallelized, with a channel to signal when each piece completes.
这些设计的另一个应用是在多 CPU 核心上实现并行计算。如果计算过程能够被分为几块 可
独立执行的过程，它就可以在每块计算结束时向信道发送信号，从而实现并行处理。
Let's say we have an expensive operation to perform on a vector of items, and that the value
of the operation on each item is independent, as in this idealized example.
让我们看看这个理想化的例子。我们在对一系列向量项进行极耗资源的操作， 而每个项的值
计算是完全独立的。
98
并发
type Vector []float64
// Apply the operation to v[i], v[i+1] ... up to v[n-1].
func (v Vector) DoSome(i, n int, u Vector, c chan int) {
for ; i < n; i++ {
v[i] += u.Op(v[i])
}
c <- 1 // signal that this piece is done
}
type Vector []float64
// 将此操应用至 v[i], v[i+1] ... 直到 v[n-1]
func (v Vector) DoSome(i, n int, u Vector, c chan int) {
for ; i < n; i++ {
v[i] += u.Op(v[i])
}
c <- 1 // 发信号表示这一块计算完成。
}
We launch the pieces independently in a loop, one per CPU. They can complete in any
order but it doesn't matter; we just count the completion signals by draining the channel after
launching all the goroutines.
我们在循环中启动了独立的处理块，每个 CPU 将执行一个处理。 它们有可能以乱序的形式
完成并结束，但这没有关系； 我们只需在所有 Go 程开始后接收，并统计信道中的完成信号
即可。
const NCPU = 4 // number of CPU cores
func (v Vector) DoAll(u Vector) {
c := make(chan int, NCPU) // Buffering optional but sensible.
for i := 0; i < NCPU; i++ {
go v.DoSome(i*len(v)/NCPU, (i+1)*len(v)/NCPU, u, c)
}
// Drain the channel.
for i := 0; i < NCPU; i++ {
<-c // wait for one task to complete
}
// All done.
}
99
并发
const NCPU = 4 // CPU 核心数
func (v Vector) DoAll(u Vector) {
c := make(chan int, NCPU) // 缓冲区是可选的，但明显用上更好
for i := 0; i < NCPU; i++ {
go v.DoSome(i*len(v)/NCPU, (i+1)*len(v)/NCPU, u, c)
}
// 排空信道。
for i := 0; i < NCPU; i++ {
<-c // 等待任务完成
}
// 一切完成。
}
The current implementation of the Go runtime will not parallelize this code by default. It
dedicates only a single core to user-level processing. An arbitrary number of goroutines can