71179 RAS KERNEL FATAL 2004-09-11 10:16:19.403039
71180 RAS KERNEL FATAL 2004-09-11 10:16:19.449275
71181 RAS KERNEL FATAL 2004-09-11 10:16:19.491485
71182 RAS KERNEL FATAL 2004-09-11 10:16:19.559002
71183 RAS KERNEL FATAL 2004-09-11 10:16:19.606596
71184 RAS KERNEL FATAL 2004-09-11 10:16:19.679025
71185 RAS KERNEL FATAL 2004-09-11 10:16:19.767800
71186 RAS KERNEL FATAL 2004-09-11 10:16:19.874910
71187 RAS KERNEL FATAL 2004-09-11 10:16:19.925050
71188 RAS KERNEL FATAL 2004-09-11 10:16:20.004321
71189 RAS KERNEL FATAL 2004-09-11 10:16:20.080657
71190 RAS KERNEL FATAL 2004-09-11 10:16:20.131280
71191 RAS KERNEL FATAL 2004-09-11 10:16:20.180034
71192 RAS KERNEL FATAL 2004-09-11 10:16:20.227512
71193 RAS KERNEL FATAL 2004-09-11 10:16:20.275568
71194 RAS KERNEL FATAL 2004-09-11 10:16:20.323819
71195 RAS KERNEL FATAL 2004-09-11 10:16:20.372047
71196 RAS KERNEL FATAL 2004-09-11 10:16:20.421075
PPC440 machine check interrupt
MCCU interrupt (bit=0x01): L2 Icache data parity error
instruction address: 0x00000290
machine check status register: 0xe0800000
summary...........................1
instruction plb error.............1
data read plb error...............1
data write plb error..............0
tlb error.........................0
i-cache parity error..............0
d-cache search parity error.......0
d-cache flush parity error........0
imprecise machine check...........1
machine state register: 0x00003000
0
wait state enable.................0
critical input interrupt enable...0
external input interrupt enable...0
problem state (0=sup,1=usr).......0
floating point instr. enabled.....1
machine check enable..............1
floating pt ex mode 0 enable......0
debug wait enable.................0
debug interrupt enable............0
Figure 1. A typical cluster of failure records that can be
coalesced into a single memory failure. For each entry, the
following attributes are shown: id, type, facility, severity,
timestamp, and description.
erarchy. In later sections, we will show that different classes
of failures have different properties. Among 190,775 total
failures, we have 16,544 memory events, 157,162 network
events, 10,500 node card events, 6,195 service card events,
and 374 midplane switch events.
4.2 Step II: Compressing Event Clusters at a Sin-
gle Location
When we focus on a speciﬁc location/component (i.e., a
speciﬁc chip/node-card/service-card/midplane-switch), we
notice that events tend to occur in bursts, with one occur-
ring within a short time window (e.g., a few seconds) after
another. We call these an event cluster. It is to be noted that
the entire span of a cluster could be large, e.g., a couple of
days, because of a large cluster size. The event descriptions
within a cluster can be identical, or completely different. It
is quite likely that all events in the cluster are referring to
the same failure, and this can arise because of the follow-
ing reasons. First, the logging interval can be smaller than
the duration of a failure, leading to multiple recordings of
the same event. Second, a failure can quickly propagate in
the problem domain, causing other events to occur within
a short time interval. Third, the logging mechanism some-
times records diagnosis information as well, which can lead
to a large number of entries for the same failure event.
BG/L logs:
We give below some example clusters that we ﬁnd in the
• Failures in the memory hierarchy (e.g. parity for I/D-
L1, ECC for EDRAM L3) typically lead to event clus-
ters. Figure 1 illustrates such a cluster for a speciﬁc
compute card. In this ﬁgure, each entry is represented
by the following attributes: id, type, facility, severity,
timestamp, and description. Examining these entries
carefully, we ﬁnd that they are all diagnostic informa-
tion for the same fatal memory event, namely, an L2
I-cache data parity error in this example. In fact, as
soon as a memory error is detected upon the reference
to a speciﬁc address, the OS kernel performs a thor-
ough machine check by taking a snapshot of the rel-
evant registers. It records the instruction address that
incurred the failure(s), information on which hardware
resources incurred the failure(s) - the processor local
bus (plb), the TLB, etc. - and a dump of status regis-
ters. In our examination, we have found 10 such typi-
cal memory failure clusters, and we need to record just
a single failure for each occurrence of a cluster.
• A node card failure cluster usually comprises of mul-
tiple temperature errors as well as power errors, with
one power error almost immediately following a tem-
perature error. This is because whenever a temperature
exception (e.g., temperature being above a threshold)
is detected on a node, the node card will shut down
its power, resulting in the report of a power failure as
well. This sequence gets recorded many times until it
is ﬁnally resolved. The cluster size varies signiﬁcantly
throughout the log, ranging from a few entries to a few
thousand entries, depending on how soon the problem
is ﬁxed in the system. Clearly, we can use a single
temperature error (e.g. due to a broken fan) to replace
such a cluster.
• Failures in the other three components, i.e., network,
service card and midplane switches, exhibit clusters as
well. Events within these clusters usually have iden-
tical descriptions. For example, the description “Ser-
vice Card Power Error” appears 3162 times between
timestamp 2004-09-14 14:05:05.440647 and times-
tamp 2004-09-15 17:18:29.862598 for the same ser-
vice card, with an average time-stamp interval between
records of 30 seconds. This is because the same failure
is continuously recorded until the cause of the problem
is ﬁxed. Similar trends are also observed with network
failures and midplane switch failures, though each fail-
ure type may have different cluster sizes. For instance,
midplane switch failure clusters are usually very small,
with many comprising a single entry.
In order to capture these clusters and coalesce them into
a single failure, we have used a simple threshold-based
scheme. We ﬁrst group all the entries from the same lo-
cation, and sort them based on their timestamps. Next,
for each entry, we compare its timestamp with the previ-
ous record’s timestamp, and only keep the entry if the gap
is above the clustering threshold Tth. We would like to em-
phasize that, as discussed above, an event cluster contains
failures of the same type; for instance, a network failure
and a memory failure should not belong to the same cluster.
Hence, when we form clusters, if two subsequent events are
of different types, no matter how temporally close they are,
we put them into different clusters. Note that this scheme is
different from the one employed in our earlier work [19] in
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
5
that the earlier scheme only ﬁlters out entries with identical
descriptions, which is insufﬁcient in our case.
If a data set has n entries before ﬁltering, and m entries
after ﬁltering, then the ratio of n−m
n , referred to as com-
pression ratio, denotes how much the data set can be re-
duced by the ﬁltering algorithm. The compression ratios
are governed by the distributions of clusters for that data
set and the choice of the threshold Tth. With a speciﬁc
Tth, a data set with larger clusters has a higher compres-
sion ratio. At the same time, a larger Tth also leads to
a larger compression ratio. Table 1 summarizes the num-
ber of remaining failure entries after applying this ﬁltering
technique with different Tth values. Speciﬁcally, Tth = 0
corresponds to the number of events before ﬁltering. From
the table, we have the following observations. First, even
a small Tth can signiﬁcantly reduce the log size, resulting
in a large compression ratio. Second, different components
have different compression ratios. For example, midplane
switch failures have a small compression ratio, e.g., 13%
with a 5-minute threshold, because they usually have small
cluster sizes. Third, when the threshold reaches a certain
point, though the compression ratio still keeps increasing,
the improvement is not that signiﬁcant, especially because
we ensure an event cluster only contains failures of the same
type. At the same time, a threshold larger than 5 minutes is
undesirable because the logging interval of BG/L prototype
is signiﬁcantly smaller than that, and it may cause unrelated
events to fall in a cluster. As a result, we choose 2 min-
utes as the threshold to coalesce event clusters. Note that
such coalescing is done only for the events at the same lo-
cation/component.
After compressing all the event clusters, we have 10
types of memory failures, 13 types of torus failures, 2 types
of node card failures, 2 types of service card failures, and
8 types of midplane switch failures. At the end of this ﬁl-
tering step, we have brought down the number of failures
to 9150, with the individual breakdown for each component
given by the entry for Tth= 2 minutes in Table 1.
4.3 Step III: Failure-Speciﬁc Filtering Across Lo-
cations
Let us now take a closer look at the results from the pre-
vious ﬁltering step. The time-series dataset contains periods
with multiple events per second and other periods with no
Tth (s)
0
1
30
60
120
300
memory
16,544
9,442
787
764
714
645
network
157,162
32,152
13,038
11,193
8,063
7,541
node card
service card
midplane switch
10,500
3,361
438
8
8
6
6,195
5,455
424
17
14
12
374
374
371
358
351
324
Table 1. The impact of Tth on compressing different fail-
ures. Tth = 0 denotes the log before any compression.
events over many consecutive seconds. Hence, our analy-
sis considers two different but related views of these time-
series datasets, i.e., the number of failure records per time
unit, or rate process, and the time between failures, or in-
crement process, for the sequences of failure entries. Note
that these two processes are inverse to each other.
104
103
102
101
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
100
0
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
120
100
80
60
40
20
0
0
500
1000
Hours
1500
500
1000
Hours
1500
2000
(a) network failure records
(b) memory failure records
s
e
r
u
l
i
a
F
l
a
t
a
F
f
o
r
e
b
m
u
N
11
10
9
8
7
6
5
4
3
2
1
0
0
500
1000
Hours
1500
(c) midplane switch failure records
Figure 2. Rate processes of failure entries after applying
ﬁltering step II. Tth = 2 minutes.
Statistics for the raw rate processes (i.e., number of
failure records within each subsystem, average number of
records per hour, variance and maximum number of records
per hour) and raw increment processes (i.e., the average,
variance, and maximum number of hours between failure
records within each subsystem) are provided in Tables 2
(a) and (b). We observe that despite removing event clus-
ters (Step II) from the same location, the number of fail-
ure records (9150 across all components) is still quite large.
This is particularly true of failures in the torus/tree net-
works, which constitute around 88% of the total failures,
where Step II is less effective than for the other components.
In order to study this more closely, we plot the time se-
ries data for three of the components (the network, mem-
ory system, and midplane switches) in Figures 2(a)-(c) at
an hourly granularity. We see that the failure occurrences
are still (despite doing the time threshold based ﬁltering in
step II) highly skewed, i.e. some intervals contain consid-
erably more records than others. For example, 26% of the
total torus failures (2076 out of 8063) are reported during
one hour (i.e., the 878th hour) out of the entire 1921-hour
period. Besides, we also notice that an interval of 6 hours,
starting from hour 877 and ending at hour 883, has 49.5%
of the total failure records. Turning our attention to Fig-
ure 2(b), we ﬁnd that memory failures are also reported
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
6
Total number of failure records
Average number of failure records per hour
Variance of number of failure records per hour
Maximum number of failure records per hour
MTBF (hours)
Variance of times between failure records (hours)
Maximum time between failure records (hours)
entire system
network
memory
node card
service card
midplane switch
9150
4.7631
3816
2077
8063
4.1973
3791
2076
714
0.3717
22.5816
128
8
.0042
.0114
4
14
.0073
.0114
2
351
.1827
0.8359
10
(a) rate process
entire system
0.2075
34997
network
0.2355
99405
141.5822
(b) increment process
190.5575
memory
2.5892
2.4426e6
560.8161
node card
147.0024
2.8165e8
765.5364
service card
midplane switch
92.5513
1.5197e8
703.4128
5.3143
1.1920e6
160.1083
n
o
i