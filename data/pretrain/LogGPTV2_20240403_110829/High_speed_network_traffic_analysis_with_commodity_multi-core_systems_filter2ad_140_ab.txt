RX queue ring to the user space requires a single packet
copy.
In this way, the driver does not deliver packets to
the legacy networking stack so that the kernel overhead is
completely avoided. If desired, users can conﬁgure the driver
to push packets into the standard networking stack as well,
but this conﬁguration is not recommended as it is the cause
of a substantial performance drop as packets have to cross
legacy networking stack layers.
2NAPI is the driver model that introduced polling in the
Linux kernel
Figure 3: Multi-queue aware packet capture design.
Each capture thread fetches packets from a single
Virtual Capture Device.
Instead of relying on the standard kernel polling mecha-
nisms to fetch packets from each queue, TNAPI features in-
driver multi-threaded packet polling. TNAPI drivers spawn
one polling thread for each RX queue (see Figure 3). Each
polling thread fetches incoming packet(s) from the corre-
sponding RX queue, and, passes the packet to PF RING.
Inside PF RING, packet processing involves packet parsing
and, depending on the conﬁguration, may include packet
ﬁltering using the popular BPF ﬁlters [5] or even more com-
plex application level ﬁltering mechanisms [16]. Kernel level
packet processing is performed by polling threads in parallel.
TNAPI drivers spawn polling threads and bind them to
a speciﬁc core by means of CPU aﬃnity manipulation. In
this way the entire traﬃc coming from a single RX queue
is always handled by the same core at the kernel level. The
obvious advantage is the increased cache locality for poller
threads. However, there is another big gain that depends
on interrupt mitigation. Modern network cards and their
respective drivers do not raise an interrupt for every packet
under high-rate traﬃc conditions. Instead, drivers disable
interrupts and switch to polling mode in such situations. If
the traﬃc is not properly balanced across multi-queues, or
if simply the traﬃc is bursty, we can expect to have busy
queues working in polling mode and queues generating inter-
rupts. By binding the polling threads to the same core where
interrupts for this queue are received we prevent threads
polling busy queues being interrupted by other queues pro-
cessing low-rate incoming traﬃc.
The architecture depicted in Figure 3 and implemented in
TNAPI, solves the single resource competition problem iden-
tiﬁed in the previous section. In fact, users can instantiate
one packet consumer thread at the user space level for each
virtual packet capture device (RX queue). Having a single
packet consumer per virtual packet capture device does not
require any locking primitive such as semaphores that, as a
side eﬀect, invalidate processor caches. In fact, for each RX
queue the polling thread at the kernel level and the packet
consumer thread at the user space level exchange packets
through a lock-less Single Reader Single Writer (SRSW)
buﬀer.
In order to avoid cache invalidation due to improper
220scheduling, users can manipulate the CPU aﬃnity to make
sure that both threads are executed on cores or Hyper-
Threads sharing levels of caches.
In this way, multi-core
architectures can be fully exploited by leveraging high band-
width and low-latency inter-core communications. We de-
cided not to impose speciﬁc aﬃnity settings for the consumer
threads, meaning that the user level packet capture library
does not set aﬃnity. Users are responsible for performing
ﬁne grained tuning of the CPU aﬃnity depending on how
CPU intensive the traﬃc analysis task is. This is straight-
forward and under Linux requires a single function call.3 It
is worth noting, that ﬁne-grained tuning of the system is
simply not feasible if queue information is not exported up
to the user space.
Compatibility and Development Issues: Our packet
capture tecnology comes with a set of kernel modules and a
user-space library called libpring. A detailed description of
the API can be found in the user guide [1]. For compatibil-
ity reasons we also ported the popular libpcap [5] library on
top of our packet capture technology. In this way, already
existing monitoring applications can be easily ported onto
it. As of today, we have implemented packet capture opti-
mized drivers for popular multi-queue IntelTM1 and 10 Gbit
adapters (82575/6 and 82598/9 chips).
4. EVALUATION
We evaluated the work using two diﬀerent parallel ar-
chitectures belonging to diﬀerent market segments (low-end
and high-end) equipped with the same Intel multi-queue net-
work card. Details of the platforms are listed in Table 1. An
IXIA 400 [4] traﬃc generator was used to inject the network
traﬃc for experiments. For 10 Gbit traﬃc generation, sev-
eral IXIA-generated 1 Gbit streams were merged into a 10
Gbit link using a HP ProCurve switch. In order to exploit
balancing across RX queues, the IXIA was conﬁgured to
generate 64 byte TCP packets (minimum packet size) origi-
nated from a single IP address towards a rotating set of 4096
IP destination addresses. With 64 bytes packets, a full Gbit
link can carry up to 1.48 Million packets per second (Mpps).
Table 1: Evaluation platforms
low-end
high-end
motherboard
CPU
Supermicro PSDBE
Core2Duo 1.86 Ghz
Supermicro X8DTL-iF
2x Xeon 5520 2.26 Ghz
Ram
NIC
2 cores
8 cores
0 HyperThreads
8 HyperThreads
4 GB
4 GB
Intel ET (1 Gbps)
Intel ET (1 Gbps)
In order to perform performance measurements we used
pfcount, a simple traﬃc monitoring application that counts
the number of captured packets. Depending on the con-
ﬁguration, pfcount spawns multiple packet capture threads
per network interface and even concurrently captures from
multiple network devices, including virtual capture devices.
In all tests we enabled multi-queues in drivers, and mod-
iﬁed the driver’s code so that queue information is prop-
agated up to PF RING; this driver does not spawn any
poller thread at the kernel level, and does not avoid socket
buﬀer allocation. We call this driver MQ (multi-queue) and
TNAPI the one described in Section 3.
Table 2: Packet capture performance (kpps) at 1
Gbps with diﬀerent two-thread conﬁgurations.
Platform SQ
Setup A
SQ
Setup B Setup C
MQ
TNAPI
low-end
high-end
1/0
721 Kpps
1326 Kpps
Threads Userspace/Kernel space
11
1264 Kpps
1488 Kpps
2/0
640 Kpps
1100 Kpps
2/0
610 Kpps
708 Kpps
Comparing Diﬀerent Approaches: As a ﬁrst test,
we evaluated the packet capture performance when using
multi-threaded packet capture applications with and with-
out multi-queue enabled. To do so, we measured the max-
imum loss free rate when pfcount uses three diﬀerent two-
threaded setups:
• Setup A: multiple queues are disabled and therefore
capture threads read packets from the same interface
(single queue, SQ). Threads are synchronized using a
r/w semaphore. This setup corresponds to the default
Linux conﬁguration shown in Figure 1.
• Setup B : two queues are enabled (MQ) and there are
two capture threads consuming packets from them. No
synchronization is needed.
• Setup C : there is one capture thread at the user level
and a polling thread at the kernel level (TNAPI).
Table 2 shows the performance results on the multi-
threaded setups, and also shows as a reference point the
single-threaded application. The test conﬁrmed the issues
we described in Section 2. When pfcount spawns two threads
at the user level, the packet capture performance is actu-
ally worse than the single-threaded one. This is expected
in both cases (setup A and B). In the case of setup A, the
cause of the drop compared to the single-threaded setup is
cache invalidations due to locking (semaphore), whereas for
B the cause is the round robin IRQ balancing. On the other
hand, our approach consisting of using a kernel thread and a
thread at the user level (setup C) is indeed eﬀective and al-
lows the low-end platform to almost double its single-thread
performance. Moreover, the high-end machine can capture
1 Gbps (1488 kpps) with no loss.
CPU Aﬃnity and Scalability: We now turn our atten-
tion to evaluating our solution at higher packet rates with
the high-end platform. We are interested in understand-
ing if by properly setting the CPU aﬃnity it is possible to
eﬀectively partition the computing resources and therefore
increase the maximum loss-free rate.
2 NICs: To test the packet capture technology with more
traﬃc, we plug another Intel ET NIC into the high-end sys-
tems and we inject with the IXIA traﬃc generator 1.488
Mpps for each interface (wire-rate at 1 Gbit with 64 bytes
packets). We want to see if it is possible to handle 2 full
Gbit links with two cores and two queues per NIC only. To
do so, we set the CPU aﬃnity to make sure that for every
NIC the two polling threads at the kernel level are executed
on diﬀerent Hyper-Threads belonging to the same core (e.g.
0 and 8 from Figure 4 belong to Core 0 of the ﬁrst phys-
ical processor4). We use the pfcount application to spawn
3See pthread setaﬃnity np().
4Under Linux, /proc/cpuinfo lists the available processing
221calls are reduced. The best way of doing so is to capture
from two RX queues, in order to increase the number of in-
coming packets. It is worth noting that, since monitoring
applications are more complex than pfcount, the conﬁgu-
ration used for Test 2 may provide better performance in
practice.
4 NICs: We decided to plug two extra NICs to the system
to check if it was possible to reach the wire-rate with 4 NICs
at the same time (4 Gbps of aggregated bandwidth with
minimum sized packets). The third and fourth NIC were
conﬁgured using the same tuning parameters as in Test 3
and the measurements repeated. The system can capture
4 Gbps of traﬃc per physical processor without losing any
packet.
Due to lack of NICs at the traﬃc generator we could
not evaluate the performance at more than 4 Gbps with
synthetic streams of minimum size packets representing the
worst-case scenario for a packet capture technology. How-
ever, preliminary tests conducted on a 10 Gbit production
network (where the average packet size was close to 300
bytes and the used bandwidth around 6 Gbps) conﬁrmed
that this setup is eﬀective in practice.
The conclusion of the validation is that when CPU aﬃnity
is properly tuned, our packet technology allows:
• Packet capture rate to scale linearly with the number
of NICs.
• Multi-core computers to be partitioned processor-by-
processor. This means that load on each processor
does not aﬀect the load on other processors.
5. RELATED WORK
The industry followed three paths for accelerating net-
work monitoring applications by means of specialized hard-
ware while keeping the software ﬂexibility. Smart traﬃc bal-
ancers, such as cPacket[2], are special purpose devices used
to ﬁlter and balance the traﬃc according to rules, so that
multiple monitoring stations receive and analyze a portion of
the traﬃc. Programmable network cards [6] are massively
parallel architectures on a network card. They are suit-
able for accelerating both packet capture and traﬃc analysis,
since monitoring software written in C can be compiled for
that special purpose architecture and run on the card and
not on the main host. Unfortunately, porting applications
on those expensive devices is not trivial. Capture accelera-
tors [3] completely oﬄoad monitoring workstations from the
packet capturing task leaving more CPU cycles to perform
analysis. The card is responsible for coping the traﬃc di-