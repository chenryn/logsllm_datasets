make longer SASs more detectable than shorter ones. Comparing
rows six and seven, we see that the quality of SAS conversion de-
grades only slightly when source and target are different speakers
(similar to the case of non-SAS samples as discussed above).
Unlike the morphing attack that maps between features of the
attacker and the victim, in reordering, ﬁltering characteristics of
vocal cords of the speaker are not changed. As the name suggests,
reordering simply remixes the ordering of words or digits. There-
fore, unlike morphing attack, no new voice is generated in reorder-
ing and in fact the attacked voice has the same features as that of
the victim’s voice [13, 47]. Hence, we did not conduct objective
evaluation test on the reordered SASs.
5.2 Subjective Evaluation: User Study
We report on a user study we conducted to measure users’ capa-
bility to detect our voice imitation attacks against Cfones. Specif-
ically, we conducted a survey, approved by our University’s IRB,
and requested 30 participants to answer several multiple choice as
well as open-ended questions about the quality and (speaker) iden-
tity of certain recordings. There were basically two categories of
questions: one related to the quality of the forged SAS (9 questions)
and another related to speaker identiﬁcation (9 questions).
Survey and Participant Details: The survey2 was created using
the Question Pro online survey software which gave us the ﬂexibil-
ity in designing multiple choice multimedia questions. Participants
were recruited by word of mouth and were only told that the pur-
pose of the study is to assess speech recognition quality. Following
best practices in usable security research, we did not give details
about the security purposes behind the survey in order to prevent
explicit priming which may have biased their responses. However,
in real-life, users should be warned that an incorrect SAS valida-
tion may harm the security of their communications. Moreover,
our attack study was targeted towards average users, and therefore
we can not deduce the performance of more or less security-aware
users.
The demographic information of the participants is presented in
Table 2. They were mostly young and well-educated, with almost
equal gender split. Such a sample is suitable for our study because
if the study results indicate that it is hard for young and educated
participants to detect our attacks, it may be harder for older and less
educated (average) people. The survey took each participant about
15 minutes to complete.
The quality test in the survey is similar to the Mean Opinion
Score (MOS) test [37]. It consisted of 9 questions, each asks the
participants to listen to the embedded SAS recording and rate the
quality, in terms of genuineness (naturalness) on a scale of one
to ﬁve (5: excellent; 4: good; 3: fair; 2: poor; 1: bad). Each
question presents two SAS recordings, that could be the origi-
nal speaker recording in different noise proﬁles, reordered SAS or
morphed/converted SAS. Different set of original recordings were
played when subjecting the participants to reordered SAS and mor-
phed SAS.
2Available at: http://surveys.questionpro.com/a/t/AKvTXZQtoV
Table 2: Demographic Info: User Study
Gender
Male
Female
Age
18-24 years
25-34 years
35-44 years
Education
High school graduate, diploma or the equivalent
Some college credit, no degree
Bachelor’s degree
Master’s degree
Doctorate degree
English as First Language
Hearing Impairment
Yes
No
Yes
No
N = 30
53%
47%
34%
62%
3%
5%
7%
55%
24%
9%
28%
72%
10%
90%
The speaker identiﬁcation test contained 9 questions, each
presents three sentences spoken by the same speaker and asks the
participant to listen to these three recordings so as to ﬁrst get famil-
iar with the voice. Then, the participants should listen to another
two recordings (forged or real by the same or different speaker),
and answer “yes” if they think any of the recordings are of the same
person, and “No” if they think it is a different person, and “Maybe”
if they can not make a distinction. The participants were asked to
ignore any dissimilarity in the quality of the recordings.
We collected different types of SAS recordings, including four
16-bit numerical SAS, eight PGP word lists and four Madlibs. We
also presented four longer SASs including 32-bit PGP word list
and 32-bit Madlibs. Generally, 32-bit numeric SAS is not secure
against reordering attack (since in only one transmission of SAS,
all 10 distinct digits might appear). Therefore 32-bit numeric SAS
was not questioned. In addition, the two samples of morphed ver-
sion of “Yes” and “No” phrases that can be used to attack Copy-
Conﬁrm approach (Section 3.3) and launch Denial-of-Service at-
tacks (Section 6) were played. Finally, to evaluate the group count
impersonation attacks (Section 3.3), we played two recordings of
individual forged numbers in victim’s voice to represent a group
leader who is announcing the (increased) group count.
As mentioned in Section 4, the voice dataset for the evaluation
consists of four collections from CMU Arctic, four collections from
VoxForge and four unprofessional recordings collected by us. For
the morphing attack, we trained the system with 50 sentences from
each speaker. The survey audio samples consist of male and female
speakers in different noise proﬁles.
Results for Quality Test: Table 3 summarizes the result for the
quality test, showing the average ratings provided by the partici-
pants assessing the quality of original and forged SAS recordings.
As the table shows, in all the cases, participants rated original
recordings between “fair” and “good” except for 32-bit Madlib,
which is rated as “poor.” Participant rated reordered SAS as “fair"
or “good" except for 32-bit Madlib, which is rated as “poor". The
results also show that they did not notice much difference between
the reordered SAS (rated mostly between good and fair) and the
original SAS. The participants rated morphed SAS somewhere be-
tween “poor” and “fair”. Interestingly none of the forged voices
were rated as “bad” quality, while none of the original voices were
rated as “Excellent” quality. No statistically signiﬁcant differences
emerged between the two types of ratings when tested with the
Wilcoxon Signed-Rank. We observed a relatively high standard
deviation in all answers that we believe originates from different in-
terpretation each person has of the word “quality”. Answers to our
open-ended question reveal that participants had a different deﬁni-
tion of quality and genuineness, and therefore their quality rating
is affected by their own deﬁnition. For example, participants men-
tioned “background noise”, “loudness”, “clarity”, and “speed” as
their measure for quality. Only three participants rated the record-
ings based on “genuineness”, “naturalness”, and “machine gener-
ated versus human.”
Moreover, we can see that the difference between the ratings for
morphed SAS and original SAS is generally more than the differ-
ence between the ratings for reordered SAS and original SAS (only
exception is 32-bit Madlibs). This suggests that reordering attack
might generally be harder to detect for the participants than morph-
ing attack. It is interesting to note that for PGP words, participants
rated the reordered SAS higher than the original recording. This
implies if the attacker collects enough data to perform the reorder-
ing attack on PGP words, the quality of the forged SAS may even
be perceived better than the original one. However, the same was
not true for Madlibs as the participants rate the attacked samples
slightly lower than the original ones. Madlibs have a correct gram-
matical structure and therefore people usually read them following
a sentence ﬂow, which may make it difﬁcult for the attacker to split
and remix.
Results for Speaker Identiﬁcation Test: We next evaluated the
speaker identiﬁcation test. Recall that in each question of the
speaker identiﬁcation test, participants ﬁrst get familiar with a
voice, then they are asked if any of the two subsequent SASs is
spoken by the same person or not. In all of our calculations, we
treated half of the uncertain answers (“Maybe") as “Yes” and half
of them as “No”.
We deﬁne the “Yes" answer as the Positive class and the “No"
answer as the Negative class. By this deﬁnition, True Positive (TP
or hit) is the instance of recognizing a legitimate familiar voice
correctly (higher values show Cfone system works well under be-
nign, non-attack, setting). False Positive (FP or false alarm) is
the instance of considering an attacked voice as a familiar voice
(higher values represent that the attack is working and partici-
pants are not able to detect it). True Negative (TN or miss) is
the instance of not recognizing a legitimate familiar voice. And,
False Negative (FN) is the instance of correctly recognizing that
an attacked voice is unfamiliar (higher value represents that the at-
tack is not successful and participants can detect it). To evaluate
Cfones, we calculated Accuracy and False Discovery Rate (FDR)
in the presence of different types of attacks. Accuracy is deﬁned as
(T P + T N )/(T P + T N + F P + F N ) (the proportion of true
results), and FDR is deﬁned as F P/(T P + F P ) (the proportion of
the false positive against all the positive results). In presence of an
effective attack, FP increases, which is reﬂected in lower accuracy
and higher FDR values.
Table 4 depicts our evaluation metrics corresponding to a SAS
spoken by a “different” person (second column, representing the
naive attack), a SAS generated by converting attacker voice to vic-
tim voice (third column), and a SAS spoken by the same person
but reordered (fourth column). The results are shown for different
Table 3: Mean (Std. Dev) ratings for original and attacked SAS
Numeric
PGP Words
1 Original SAS
2
Reordered SAS
4 (0.95)
3.67 (1.08)
3.05 (1.21)
3.23 (1.22)
16-Bit
Madlib
4.15 (0.9)
3.64 (1.35)
32-Bit
Madib
3.28 (1.28)
2.68 (1.30)
3 Original SAS
4 Morphed SAS
3.51 (1.19)
2.33 (1.20)
3.74 (1.09)
3.18 (1.25)
3.34 (1.30)
2.75 (1.39)
2.56 (1.41)
2.26 (1.35)
type of SAS. Also shown is the overall aggregated result among all
three types of SASs (the last matrix).
First of all, the table illustrates a relatively high TN for SASs
played in a “different voice” (row 2, column 2 – bold fonts in dark
gray shade), which means that when a totally different voice is pre-
sented, people successfully detect the difference with a high chance
(about 80%). This demonstrates that if the (naive) attacker just in-
serts a different voice in its MITM attack, it would be detected by
the users with a high probability. This provides an important quan-
titative benchmark to compare the performance of attacks with.
The effectiveness of our voice imitation (morphing and reorder-
ing) attacks is represented by FP (ﬁrst row results for column 3 and
4 – bold fonts in light grey shade), which is also reﬂected in FDR
(last row results for column 3 and 4). Although FDR is not very
high (somewhere around 50-60%), it is important to look at the cor-
responding Accuracy of the Cfone system under our attacks (row 3,
column 3 and 4), which is roughly around 50% or lower, and shows
that people are not accurate in recognizing the familiar voice saying
SAS even in non-attack (benign) scenarios, and almost 50% of the
times participants detect original voice in a different noise proﬁle
as fake voice. That is, even in non-attack scenario, participants are
making almost random guesses to decide whether the voice is real
or fake. Thus, we can conclude that, under our attacks, users do not
perform any better than a random guess in recognizing an forged
SAS, and in fact the result is very similar to recognizing original
similar voice in a different noise proﬁle. In short, people are as
successful as recognizing a forged SAS as they are successful in
recognizing an original SAS in a different noise proﬁle.
Similar to the quality test, the speaker identiﬁcation test shows
that reordering attack generally works better (e.g., has higher FDR)
than the morphing attack. The performance metrics, however, do
not indicate any signiﬁcant differences in the way users may detect
the attacks against different SAS types (numeric, PGP or Madlibs).
They all seem almost equally prone to our attacks. In Section 3, we
referred to the linguistic studies that demonstrate people are more
successful in recognizing familiar voices when they are presented
with long sentences rather than short sentences. Our experiment
for short SAS conﬁrms this insight.
Copy-Conﬁrm and Group Count Attacks: Copy-Conﬁrm SAS
validation mechanisms work by Alice reading the SAS and Bob
accepting or rejecting by saying a “Yes” or “No" phrase. In our
evaluation, we converted two type of Accept/Yes and Reject/No
phrases (i.e. “Yes, It’s a match”) to represent Mallory who drops a
reject response from Bob and rather injects an accept response in
Bob’s voice to authenticate a connection. We repeated our speaker
identiﬁcation test for Yes and No phrases. The results show that
this conversion follows the same pattern as SAS conversion. The
TN is relatively high (between 70-80%) for different voices (peo-
ple can detect a different voice), but precision and FDR are around
50% for non-attacked and attacked scenario. That means reorder-
ing and morphing attack on the yes and no phrases is as successful
as SAS conversion. Reordering works the best in such situation as
the attacker only needs to replay a previously spoken phrase.
The group count attack is an attack in n-Cfone where Mallory
announces the (increased) group count in the leader’s voice. Simi-
lar to numeric SAS conversion, Mallory need to generate and insert
a number. However, here it is much easier as the number is only
one digit long. Due to similarity between Numeric SAS conver-