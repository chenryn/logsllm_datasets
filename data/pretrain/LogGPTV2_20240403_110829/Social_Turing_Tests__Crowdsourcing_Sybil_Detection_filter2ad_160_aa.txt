title:Social Turing Tests: Crowdsourcing Sybil Detection
author:Gang Wang and
Manish Mohanlal and
Christo Wilson and
Xiao Wang and
Miriam J. Metzger and
Haitao Zheng and
Ben Y. Zhao
Social Turing Tests: Crowdsourcing Sybil Detection
Gang Wang, Manish Mohanlal, Christo Wilson, Xiao Wang‡,
Miriam Metzger†, Haitao Zheng and Ben Y. Zhao
Department of Computer Science, U. C. Santa Barbara, CA USA
†Department of Communications, U. C. Santa Barbara, CA USA
‡Renren Inc., Beijing, China
Abstract
As popular tools for spreading spam and malware, Sybils
(or fake accounts) pose a serious threat to online communi-
ties such as Online Social Networks (OSNs). Today, sophis-
ticated attackers are creating realistic Sybils that effectively
befriend legitimate users, rendering most automated Sybil
detection techniques ineffective. In this paper, we explore
the feasibility of a crowdsourced Sybil detection system for
OSNs. We conduct a large user study on the ability of hu-
mans to detect today’s Sybil accounts, using a large cor-
pus of ground-truth Sybil accounts from the Facebook and
Renren networks. We analyze detection accuracy by both
“experts” and “turkers” under a variety of conditions, and
ﬁnd that while turkers vary signiﬁcantly in their effective-
ness, experts consistently produce near-optimal results. We
use these results to drive the design of a multi-tier crowd-
sourcing Sybil detection system. Using our user study data,
we show that this system is scalable, and can be highly ef-
fective either as a standalone system or as a complementary
technique to current tools.
1 Introduction
The rapid growth of Sybil accounts is threatening the sta-
bility and security of online communities, particularly on-
line social networks (OSNs). Sybil accounts represent fake
identities that are often controlled by a small number of real
users, and are increasingly used in coordinated campaigns
to spread spam and malware [6, 30]. In fact, measurement
studies have detected hundreds of thousands of Sybil ac-
counts in different OSNs around the world [3,31]. Recently,
Facebook revealed that up to 83 million of its users may be
fake1, up signiﬁcantly from 54 million earlier2.
1http://www.bbc.com/news/technology-19093078
2http://www.bbc.com/news/technology-18813237
The research community has produced a substantial
number of techniques for automated detection of Sybils [4,
32, 33]. However, with the exception of SybilRank [3], few
have been successfully deployed. The majority of these
techniques rely on the assumption that Sybil accounts have
difﬁculty friending legitimate users, and thus tend to form
their own communities, making them visible to community
detection techniques applied to the social graph [29].
Unfortunately, the success of these detection schemes is
likely to decrease over time as Sybils adopt more sophis-
ticated strategies to ensnare legitimate users. First, early
user studies on OSNs such as Facebook show that users are
often careless about who they accept friendship requests
from [2]. Second, despite the discovery of Sybil commu-
nities in Tuenti [3], not all Sybils band together to form
connected components. For example, a recent study of
half a million Sybils on the Renren network [14] showed
that Sybils rarely created links to other Sybils, and in-
stead intentionally try to inﬁltrate communities of legitimate
users [31]. Thus, these Sybils rarely connect to each other,
and do not form communities. Finally, there is evidence that
creators of Sybil accounts are using advanced techniques
to create more realistic proﬁles, either by copying proﬁle
data from existing accounts, or by recruiting real users to
customize them [30]. Malicious parties are willing to pay
for these authentic-looking accounts to better befriend real
users.
These observations motivate us to search for a new ap-
proach to detecting Sybil accounts. Our insight is that while
attackers are creating more “human” Sybil accounts, fool-
ing intelligent users, i.e. passing a “social Turing test,” is
still a very difﬁcult task. Careful users can apply intuition
to detect even small inconsistencies or discrepancies in the
details of a user proﬁle. Most online communities already
have mechanisms for users to “ﬂag” questionable users or
content, and social networks often employ specialists dedi-
cated to identifying malicious content and users [3]. While
these mechanisms are ad hoc and costly, our goal is to ex-
plore a scalable and systematic approach of applying human
effort, i.e. crowdsourcing, as a tool to detect Sybil accounts.
Designing a successful crowdsourced Sybil detection re-
quires that we ﬁrst answer fundamental questions on issues
of accuracy, cost, and scale. One key question is, how accu-
rate are users at detecting fake accounts? More speciﬁcally,
how is accuracy impacted by factors such as the user’s ex-
perience with social networks, user motivation, fatigue, and
language and cultural barriers? Second, how much would
it cost to crowdsource authenticity checks for all suspicious
proﬁles? Finally, how can we design a crowdsourced Sybil
detection system that scales to millions of proﬁles?
In this paper, we describe the results of a large user study
into the feasibility of crowdsourced Sybil detection. We
gather ground-truth data on Sybil accounts from three so-
cial network populations: Renren [14], the largest social
network in China, Facebook-US, with proﬁles of English
speaking users, and Facebook-India, with proﬁles of users
who reside in India. The security team at Renren Inc. pro-
vided us with Renren Sybil account data, and we obtained
Facebook (US and India) Sybil accounts by crawling highly
suspicious proﬁles weeks before they were banned by Face-
book. Using this data, we perform user studies analyzing
the effectiveness of Sybil detection by three user popula-
tions: motivated and experienced “experts”; crowdsourced
workers from China, US, and India; and a group of UCSB
undergraduates from the Department of Communications.
Our study makes three key contributions. First, we an-
alyze detection accuracy across different datasets, as well
as the impact of different factors such as demographics,
survey fatigue, and OSN experience. We found that well-
motivated experts and undergraduate students produced ex-
ceptionally good detection rates with near-zero false posi-
tives. Not surprisingly, crowdsourced workers missed more
Sybil accounts, but still produced near zero false positives.
We observe that as testers examine more and more suspi-
cious proﬁles, the time spent examining each proﬁle de-
creases. However, experts maintained their accuracy over
time while crowdworkers made more mistakes with addi-
tional proﬁles. Second, we performed detailed analysis
on individual testers and account proﬁles. We found that
while it was easy to identify a subset of consistently accu-
rate testers, there were very few “chameleon proﬁles” that
were undetectable by all test groups. Finally, we propose a
scalable crowdsourced Sybil detection system based on our
results, and use trace-driven data to show that it achieves
both accuracy and scalability with reasonable costs.
By all measures, Sybil identities and fake accounts are
growing rapidly on today’s OSNs. Attackers continue to in-
novate and ﬁnd better ways of mass-producing fake proﬁles,
and detection systems must keep up both in terms of accu-
racy and scale. This work is the ﬁrst to propose crowdsourc-
ing Sybil detection, and our user study results are extremely
positive. We hope this will pave the way towards testing
and deployment of crowdsourced Sybil detection systems
by large social networks.
2 Background and Motivation
Our goal is to motivate and design a crowdsourced Sybil
detection system for OSNs. First, we brieﬂy introduce the
concept of crowdsourcing and deﬁne key terms. Next, we
review the current state of social Sybil detection, and high-
light ongoing challenges in this area. Finally, we introduce
our proposal for crowdsourced Sybil detection, and enumer-
ate the key challenges to our approach.
2.1 Crowdsourcing
Crowdsourcing is a process where work is outsourced to
an undeﬁned group of people. The web greatly simpliﬁes
the task of gathering virtual groups of workers, as demon-
strated by successful projects such as Wikipedia. Crowd-
sourcing works for any job that can be decomposed into
short, simple tasks, and brings signiﬁcant beneﬁts to tasks
not easily performed by automated algorithms or systems.
First, by harnessing small amounts of work from many peo-
ple, no individual is overburdened. Second, the group of
workers can change dynamically, which alleviates the need
for a dedicated workforce. Third, workers can be recruited
quickly and on-demand, enabling elasticity. Finally and
most importantly, by leveraging human intelligence, crowd-
sourcing can solve problems that automated techniques can-
not.
In recent years, crowdsourcing websites have emerged
that allow anyone to post small jobs to the web and have
them be solved by crowdworkers for a small fee. The pi-
oneer in the area is Amazon’s Mechanical Turk, or MTurk
for short. On MTurk, anyone can post jobs called Human
Intelligence tasks, or HITs. Crowdworkers on MTurk, or
turkers, complete HITs and collect the associated fees. To-
day, there are around 100,000 HITs available on MTurk at
any time, with 90% paying ≤$0.10 each [11,24]. There are
over 400,000 registered turkers on MTurk, with 56% from
the US, and 36% from India [24].
Social networks have started to leverage crowdsourcing
to augment their workforce. For example, Facebook crowd-
sources content moderation tasks, including ﬁltering porno-
graphic and violent pictures and videos [10]. However,
to date we know of no OSN that crowdsources the iden-
tiﬁcation of fake accounts.
Instead, OSNs like Facebook
and Tuenti maintain dedicated, in-house staff for this pur-
pose [3, 10].
Unfortunately, attackers have also begun to leverage
crowdsourcing. Two recent studies have uncovered crowd-
sourcing websites where malicious users pay crowdworkers
to create Sybil accounts on OSNs and generate spam [21,
30]. These Sybils are particularly dangerous because they
are created and managed by real human beings, and thus ap-
pear more authentic than those created by automated scripts.
Crowdsourced Sybils can also bypass traditional security
mechanisms, such as CAPTCHAs, that are designed to de-
fend against automated attacks.
2.2 Sybil Detection
The research community has produced many systems de-
signed to detect Sybils on OSNs. However, each one re-
lies on speciﬁc assumptions about Sybil behavior and graph
structure in order to function. Thus, none of these systems
is general enough to perform well on all OSNs, or against
Sybils using different attack strategies.
The majority of social Sybil detectors from the literature
rely on two key assumptions. First, they assume that Sybils
have trouble friending legitimate users. Second, they as-
sume that Sybil accounts create many edges amongst them-
selves. This leads to the formation of well-deﬁned Sybil
communities that have a small quotient-cut from the hon-
est region of the graph [4, 28, 29, 32, 33]. Although similar
Sybil community detectors have been shown to work well
on the Tuenti OSN [3], other studies have demonstrated lim-
itations of this approach. For example, a study by Yang
et al. showed that Sybils on the Renren OSN do not form
connected components at all [31]. Similarly, a meta-study
of multiple OSN graphs revealed that many are not fast-
mixing, which is a necessary precondition for Sybil com-
munity detectors to perform well [20].
Other researchers have focused on feature-based Sybil
detectors. Yang et al. detect Sybils by looking for ac-
counts that send many friend requests that are rejected by
the recipient. This detection technique works well on Ren-
ren because Sybils must ﬁrst attempt to friend many users
before they can begin effectively spamming [31]. However,
this technique does not generalize. For example, Sybils on
Twitter do not need to create social connections, and instead
send spam directly to any user using “@” messages.
Current Sybil detectors rely on Sybil behavior assump-
tions that make them vulnerable to sophisticated attack
strategies. For example, Irani et al. demonstrate that “hon-
eypot” Sybils are capable of passively gathering legitimate
friends and penetrating the social graph [13]. Similarly,
some attackers pay users to create fake proﬁles that bypass
current detection methods [30]. As Sybil creators adopt
more sophisticated strategies, current techniques are likely
to become less effective.
2.3 Crowdsourcing Sybil Detection
In this study, we propose a crowdsourced Sybil detec-
tion system. We believe this approach is promising for
three reasons: ﬁrst, humans can make overall judgments
about OSN proﬁles that are too complex for automated al-
gorithms. For example, humans can evaluate the sincer-
ity of photographs and understand subtle conversational nu-
ances. Second, social-Turing tests are resilient to changing
attacker strategies, because they are not reliant on speciﬁc
features. Third, crowdsourcing is much cheaper than hir-
ing full-time content moderators [9, 25]. However, there
are several questions that we must answer to verify that this
system will work in practice:
• How accurate are users at distinguishing between real
and fake proﬁles? Trained content moderators can per-
form this task, but can crowdworkers achieve compara-
ble results?
• Are there demographic factors that affect detection ac-
curacy? Factors like age, education level, and OSN ex-
perience may impact the accuracy of crowdworkers.
• Does survey fatigue impact detection accuracy?
In
many instances, people’s accuracy at a task decline over
time as they become tired and bored.
• Is crowdsourced Sybil detection cost effective? Can the
system be scaled to handle OSNs with hundreds of mil-
lions of users?
We answer these questions in the following sections.
Then, in Section 6, we describe the design of our crowd-
sourced Sybil detection system, and use our user data to
validate its effectiveness.
3 Experimental Methodology
In this section, we present the design of our user stud-
ies to validate the feasibility of crowdsourced Sybil detec-
tion. First, we introduce the three datasets used in our
experiments: Renren, Facebook US, and Facebook India.
We describe how each dataset was gathered, and how the
ground-truth classiﬁcation of Sybil and legitimate proﬁles
was achieved. Next, we describe the high-level design of
our user study and its website implementation. Finally, we
introduce the seven groups of test subjects. Test subjects
are grouped into experts, turkers from crowdsourcing web-
sites, and university undergraduates. We use different test
groups from China, the US, and India that correspond to our
three datasets. All of our data collection and experimental
methodology was evaluated and received IRB approval be-
fore we commenced our study.
3.1 Ground-truth Data Collection
Our experimental datasets are collected from two large
OSNs: Facebook and Renren. Facebook is the most popu-
lar OSN in the world and has more than 1 billion users [8].
Renren is the largest OSN in China, with more than 220
Facebook
Unknown
Pro(cid:127)les
Suspicious
Pro(cid:127)les
Dataset
Google Image 
Search
Friends of Suspicious Pro(cid:127)les
Figure 1. Facebook crawling methodology.
million users [14]. Both sites use similar visual layouts and
offer user proﬁles with similar features, including space for
basic information, message “walls,” and photo albums. Ba-
sic information in a proﬁle includes items like name, gen-
der, a proﬁle image, total number of friends, interests, etc.
Each dataset is composed of three types of user pro-
ﬁles: conﬁrmed Sybils, conﬁrmed legitimate users, and sus-
picious proﬁles that are likely to be Sybils. Conﬁrmed
Sybil proﬁles are known to be fake because they have been
banned by the OSN in question, and manually veriﬁed by
us. Suspicious proﬁles exhibit characteristics that are highly
indicative of a Sybil, but have not been banned by the OSN.
Legitimate proﬁles have been hand selected and veriﬁed by
us to ensure their integrity. We now describe the details of
our data collection process on Facebook and Renren.
Facebook. We collect data from Facebook using a cus-
tom web crawler. Because Facebook caters to an interna-
tional audience, we speciﬁcally targeted two regional ar-
eas for study: the US and India. We chose these two re-
gions because they have large, Internet enabled populations,
and both countries have active marketplaces for crowdwork-
ers [24]. Our Facebook crawls were conducted between De-
cember 2011 and January 2012.
The legitimate proﬁles for our study were randomly se-
lected from a pool of 86K proﬁles. To gather this pool of
proﬁles, we seeded our crawler with 8 Facebook proﬁles be-
longing to members of our lab (4 in the US, and 4 in India).
The crawler then visited each seed’s friends-of-friends, i.e.
the users two-hops away on the social graph. Studies have
shown that trust on social networks is often transitive [18],
and thus the friends-of-friends of our trusted seeds are likely
to be trustworthy as well. From the 86K total friends-of-
friends in this set, the crawler sampled 100 proﬁles (50 from
the US, 50 from India) that had Facebook’s default, permis-
sive privacy settings. We manually examined all 100 pro-
ﬁles to make sure they were 1) actually legitimate users,
and 2) we did not know any of them personally (to prevent
bias in our study).
To facilitate collection of Sybils on Facebook, we make
one assumptions about Sybil behavior: we assume that
Sybils use widely available photographs from the web as
proﬁle images. Intuitively, Sybils need realistic proﬁle im-
ages in order to appear legitimate. Hence, Sybils must resort
to using publicly available images from around the web. Al-
though all Sybils on Facebook may not obey this assump-
tion, we will show that enough do to form a sufﬁciently
large sample for our user study.
To gather suspicious proﬁles, we seeded our crawler with
the proﬁles of known Sybils on Facebook [1]. The crawler
then snowball crawled outward from the initial seeds. We
leveraged Google Search by Image to locate proﬁles us-
ing widely available photographs as proﬁle images. Fig-
ure 1 illustrates this process. For each proﬁle visited by the
crawler, all of its proﬁle images were sent to Google Search
by Image (Facebook maintains a photo album for each user
that includes their current proﬁle image, as well as all prior
images). If Google Search by Image indexed ≥90% of the
proﬁle images on sites other than Facebook, then we con-
sider the account to be suspicious. The crawler recorded the
basic information, wall, and photo albums from each sus-
picious proﬁle. We terminated the crawl after a sufﬁcient
number of suspicious proﬁles had been located.
We search for all of a user’s proﬁle images rather than
just the current image because legitimate users sometimes
use stock photographs on their proﬁle (e.g. a picture of their
favorite movie star). We eliminate these false positives by
setting minimum thresholds for suspicion: we only consider
proﬁles with ≥2 proﬁle images, and if ≥90% are available
on the web, then the proﬁle is considered suspicious.
In total, our crawler was able to locate 8779 suspicious
Facebook proﬁles. Informal, manual inspection of the pro-
ﬁle images used by these accounts reveals that most use pic-
tures of ordinary (usually attractive) people. Only a small
number of accounts use images of recognizable celebrities
or non-people (e.g. sports cars). Thus, the majority of pro-
ﬁle images in our dataset are not suspicious at ﬁrst-glance.
Only by using external information from Google does it be-
come apparent that these photographs have been misappro-
priated from around the web.
At this point, we don’t have ground-truth about these
proﬁles, i.e. are they really Sybils? To determine ground-
truth, we use the methodology pioneered by Thomas et al.
to locate fake Twitter accounts [27]. We monitored the sus-
picious Facebook proﬁles for 6 weeks, and observed 573
became inaccessible. Attempting to browse these proﬁles
results in the message “The page you requested was not
found,” indicating that the proﬁle was either removed by
Facebook or by the owner. Although we cannot ascer-
tain the speciﬁc reason that these accounts were removed,
the use of widely available photographs as proﬁle images
makes it highly likely that these 573 proﬁles are fakes.
The sole limitation of our Facebook data is that it only
includes data from public proﬁles. It is unknown if the char-
acteristics of private accounts (legitimate and Sybil) differ
from public ones. This limitation is shared by all studies
that rely on crawled OSN data.
Renren.
We obtained ground-truth data on Sybil and
Dataset
# of Proﬁles
Sybil Legit.
Renren
100
100
Facebook
US