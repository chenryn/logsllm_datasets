normal distribution using real values. However, this difference
is large enough that once the noise is added, the noisy values
are also shifted to ranges of ﬂoating points where one has
more attackable values than the other. We observe a relative
increase in gradient norm of 1.34 when using DiﬀLabelCanary
compared to SimLabelCanary.
We also observe that the success rate increases when σ
increases, and correspondingly  decreases. This is counter-
intuitive as the magnitude of noise increases as  decreases.
The same observation was made by Mironov for the Laplace
distribution. The reason again relates to the ﬂoating-point
range in which noisy gradients land. We also note that
compared to attacks in V-B, the attacker has more chances
to observe values in support of one dataset than the other,
since it has d gradients to attack as opposed to one result.
VI. DISCRETE AND APPROXIMATE DISTRIBUTION
SAMPLING
Discrete distributions aim to avoid privacy leakage from
ﬂoating-point representation, while retaining the privacy and
utility properties of their continuous counterparts. In this
section we show that na¨ıve implementation of such discrete
distributions suffers from a timing side channel attack: by
measuring the time the sampling algorithm takes to draw noise
from such a distribution, the adversary is able to determine the
magnitude of this noise and, hence, invalidate the guarantees
of differential privacy.
a) Discrete Laplace and Gaussian: Canonne et al. [12]
study the discrete Gaussian mechanism and its properties.
They demonstrate that it provides the same level of privacy
and utility as the continuous Gaussian. Their sampler for
Laplace and Gaussian uses the geometric distribution where
the corresponding samples preserve the magnitude of the
noise drawn from the geometric distribution. Unfortunately,
the running time of this geometric distribution sampler, if not
implemented carefully, reveals the magnitude of its noise.
Canonne et al. [12] describe sampling from geometric distri-
butions (Algorithm 2 in their paper) using Bernoulli samples.
Recall that the geometric distribution measures the probability
of taking n Bernoulli trials to obtain a ﬁrst success. Their
Fig. 3: FP attack success rate against DP-SGD on MNIST
model using the Opacus [4]. DiﬀLabelCanary: attacker is
distinguishing between a batch where all records have identical
labels in the range [1, 9] and a batch that has same records
as well as an image for label 0. SimLabelCanary: batches
where all records are from [0, 9] and differ in one random
record. Since the baseline attack is 50%, the plot shows that
FP attack is successful on DiﬀLabelCanary where the canary
record comes from the same distribution as MNIST data but
different from other records in the batch. Here, δ = 10−5.
• DiﬀLabelCanary: the batch is composed of records with
the same labels in [1, 9], and the canary record of its
neighboring batch is a record with label 0.
Though DiﬀLabelCanary is handcrafted, it represents an
example of a batch that should be protected by DP guarantees
since all records are drawn from the same distribution.
2) Attack Results: The attack on DP-SGD follows the
same procedure as the attack in Section V-B since it uses
Gauss pytorch. Note that the attack against DP-SGD naturally
reveals sequential (cached) samples from an implementation of
the normal distribution since the attacker observes application
of noise to gradients of all d parameters of the model, i.e.,
to d computations that all use independent noise draws.
The adversary calculates f (B) and f (B(cid:48)) since we assume
the attacker knows the records in B and B(cid:48) and is trying to
determine the presence of the canary. Since there are 26,010
gradients in y, we evaluate the attack on each to see if it lies
1 and s(cid:48)
in support of f (B) or f (B(cid:48)). We extract s1 and s2, s(cid:48)
from y − f (B) and y − f (B(cid:48)) respectively for each. We then
search for their neighboring values, and check whether any of
them support gradients in y. We say y is in support of f (B)
if there are more gradients that support f (B) than f (B(cid:48)) and
similarly for f (B(cid:48)).
2
The experimental results are presented in Figure 3. For
DiﬀLabelCanary the attacker’s succes rates is always better
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
480
pseudo-code to simulate this process proceeds as follows.
It samples the Bernoulli distribution until the ﬁrst success
while incrementing a counter of failures. Once the success is
observed, the counter is returned as a sample n of a geometric
distribution. Hence, the number of times Bernoulli sampler is
invoked linearly correlates with the magnitude of the sample n.
This is the source of the timing side-channel where the time
is correlated with a secretly drawn value.
The Laplace distribution [12] is based on a linear transfor-
mation of the sample drawn from the geometric distribution,
preserving its magnitude. In turn, the discrete Gaussian mech-
anism with standard deviation of σ in [12] uses the Laplace
mechanism with parameter (cid:98)σ(cid:99) + 1. Laplace noise is returned
as-is via rejection sampling with a carefully chosen probability
that produces the exact discrete distribution. However, since
Laplace noise is returned as-is,
the Gaussian sample has
the same magnitude as Laplace and hence as the geometric
distribution.
In parallel
In summary, the time it takes to run discrete Laplace or
discrete Gaussian sampling is correlated with the magnitude
of the sample they return, and hence the noise they add to
their respective DP mechanisms. Though the authors state that
their algorithms may suffer from timing attacks, they attribute
them to rejection sampling noting that this “reveals nothing
about the accepted candidate”. However, as we argue above
and experimentally show in the next section, the subroutine
used to draw from the geometric distribution is the one that
creates the timing side channel.
b) Approximate Laplace:
to the work by
Canonne et al., the Differential Privacy Team at Google [13]
proposed an algorithm for approximate Laplace in their report
of the library implementation for differential privacy [3]. Their
sampler makes a draw from the geometric distribution and
then scales it using a resolution parameter based on . The
paper does not specify how the geometric distribution is
implemented. Upon examination of the code in the library [3],
we observed that geometric sampling is not based on drawing
Bernoulli samples as in [12]. However, its runtime still linearly
depends on the value being sampled and hence also suffers
from a timing side-channel. Speciﬁcally, the implementation
of [13], performs a binary search, where the distribution
support region is split proportional to probability mass and
is guided by a sequence of uniform random values. Since the
search is longer for events with smaller probability, the time to
“ﬁnd” larger values in the case of drawing geometric random
variables takes longer. As a result this also creates a timing
side channel that reveals the magnitude of the drawn noise,
even though conceptually the technique for drawing from the
geometric distribution is different from [12].
VII. EXPERIMENTS: TIMING ATTACKS ON DISCRETE
DISTRIBUTIONS
We evaluate the discrete Laplace and Gaussian using the im-
plementation in [14] that accompanied the work by Canonne et
al. [12] and the Laplace implementation from Google [13],
referred to as implementations I and II respectively (see
disclosure in Section I). We conduct two sets of experiments
to show that 1) both implementations are amenable to timing
attacks; 2) a DP algorithm that uses these implementations, as
a consequence, is also amenable to a timing attack.
A. Experimental Setup
We run the discrete samplers on a single core of an Intel
Xeon Platinum 8180M, which runs a 64-bit Ubuntu Linux
16.04.1 with kernel version 4.15.0-142. There are no other
running processes on this core, so the interference on timing
measurement is minimized. We measure the overall time of the
sampling algorithm on invocation and exit with nano second
precision using time.process_time_ns() for Imple-
mentation I written in Python and System.nanoTime()
for Implementation II written in Java.
B. Timing of Discrete Samplers
We ﬁrst measure the time it takes to generate the noise from
each implementation and average it over more than 1 million
trials for each sampled value in a truncated region. Since both
Gaussian and Laplace are symmetrical distributions, the time it
takes to generate positive and negative noise is also symmetric.
In the implementation, the sign is determined independently
from the (positive) geometric noise magnitude.
In Figure 4 we plot the average time it takes to draw absolute
values of the noise. We used σ = 19 for Gaussian I, λ = 8 for
ln 3 for Laplace II. We observe that the
Laplace I, and λ = 8
absolute magnitude of noise has a positive linear relationship
with time to generate noise from all implementations.
Based on the above relationship between noise magnitude
and generation time, we implement our attack as follows. The
attacker computes average time ti to generate absolute values
of integer noise i ∈ [0, 9]. It then measures the time tj it takes
to generate an unknown noise j sample and chooses i that has
the closest time to tj as its guess:
jguess = arg mini∈[0,9](|tj − ti|)
(3)
We ran 100,000 trials for each sampler to evaluate the
accuracy of our timing attack. We evaluate accuracy in two
standards: exact match and approximate match within ±1 from
the correct value:
• exact match: |j| = jguess
• approximate match: −1 ≤ jguess − |j| ≤ 1
The attacker can use exact match as follows. Recall that the
attacker, given a DP output y and jguess where y = f (D) +
j is trying to guess the unprotected value of f (D). Let the
co-domain of f have integer support [−X, X], known to the
adversary as it knows f and domain of f, D. For exact match
though our attack cannot guess whether the noise is positive
or negative, this still harms differential privacy. Speciﬁcally,
it reduces the original guess of the attack on the noise value
from 1/(2X + 1) to 1/2.
For the approximate guess, the attacker knows that j could
have been one of six values:
±(jguess − 1),±jguess,±(jguess + 1)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
481
(a)
(b)
Fig. 4: Average time in µs (with the distribution’s 0.25 and 0.75 quantiles) to generate absolute noise using two implementations
of discrete distributions: (a) Discrete Laplace and Gaussian from Implementation I [14] (avg. over 10 million trials). (b) Discrete
Laplace from Implementation II [3] (avg. over 20 million trials). The plots show a linear relationship between the absolute
noise value and time it takes to generate it. Gaussian I uses σ = 19, Laplace I uses λ = 8 and Laplace II uses λ = 8
ln 3.
Implementation
Parameters
Gaussian I [14]
Laplace I [14]
Laplace II [3]
σ = 2
σ = 4
λ = 1
λ = 3
λ = 1
ln 3
λ = 3
ln 3
λ = 1
ln 2
approximate
approximate
approximate
approximate
Match
exact
exact
exact
exact
exact
exact
exact
approximate
approximate
approximate
Accuracy
24.4%
56.2%
13.6%
39.9%
42.1%
84.2%
24.7%
61.5%
42.0%
89.0%
17.0%
44.0%
32.0%
71.0%
TABLE I: Success rates of timing side channel attacks against
implementations of discrete distributions. The baseline accu-
racy based on a random guess for exact and approximate match
is 10% and 33%, respectively. We observe that the attacks are
always well above the baseline attack accuracy.
This allows it to determine that f (D) must be either y ±
(jguess − 1), y ± jguess or y ± (jguess + 1). Hence, its guess
is reduced from 1/(2X + 1) to 1/6. As an example, suppose
the attacker is trying to distinguish between f (D) = 30 and
f (D(cid:48)) = 60. If it observes, y = 20 and jguess = 9. It knows
that f must have been computed on D and not D(cid:48).
The results are summarized in Table I where we measure
the accuracy for samples in the range [−9, 9] for several
parameters where  ranges between 0.5 and 2 and δ = 10−5.
Since we evaluate the absolute values of the sample,
the
baseline accuracy based on a random guess for exact and
approximate match is 10% and 33%, respectively. We observe
that the attacks are always well above the baseline accuracy.
This indicates that if an adversary can observe how long
the sampling algorithm takes to generate noise used in a DP
mechanism, then it can guess the relative magnitude of this
noise for samplers based on geometric noise generation.
For Implementation I, the discrete Laplace mechanism is
more vulnerable to timing attacks than the discrete Gaussian.
This is likely due to rejection sampling that Gaussian im-
plementation adds to the process. Rejection sampling adds
stochasticity to which Laplace sample, among several that
are drawn,
the attacker cannot
distinguish between the following two executions:
is returned. For example,
1) In the ﬁrst execution, a large Laplace noise value is
generated but then rejected, while a small Laplace noise
value generated next is returned as a result.
2) In the second execution, the opposite happens where a
small Laplace value is rejected ﬁrst and then a larger
Laplace value is generated and returned as a result.
Execution times will be similar for both cases. Indeed, the
authors of [13] also proposes a discrete Gaussian based on
Binomial sampling and rejection sampling and their imple-
mentation [3] is not amenable to our attack.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
482
The success rate of our attack decreases with increasing σ
and λ. We note that with higher parameters, larger noise values
are more likely, while the attack is more successful for smaller
values, hence, overall accuracy decreases. For example, with
λ = 3 for Laplace I, the accuracy for i ∈ [0, 4] is 27.1%, and
accuracy for i ∈ [5, 9] is 12.5%.
For Laplace II we observe a similar trend as for Laplace I
even though the geometric distribution is sampled using a
different procedure described in the previous section.
C. Timing Attack on Private Sum
Based on the results of the previous section, we con-
duct a timing attack on real data using the German Credit
Dataset [33] used in Section V-B1. We model the setting
where the private sum of the credit attribute of the dataset
is computed (e.g., an analyst queries a DP-protected database
as in the DB setting in Section III). The attacker is trying
to guess the non-private sum using query’s response and the
time it takes for the query to return. We put a limit that each
individual can have at most 5000 credits (which determines