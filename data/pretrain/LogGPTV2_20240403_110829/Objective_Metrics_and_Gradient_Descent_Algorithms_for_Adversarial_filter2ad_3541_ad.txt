and NewtonFool produce very close results, achieving smaller
values than the other two algorithms. On the other hand, the
statistics on FGSM and JSMA are relatively worse. Especially,
JSMA changes the high frequency part significantly more.
From this result, perturbations from JSMA seem to corrupt
more feature details than other algorithms. Remarkably, New-
tonFool maintains the best result over all experiments on our
Fourier transform metric.
4.4 Efficiency
In the final part of our experimental study we evaluate the
efficiency of different algorithms. We measure the end-to-end
time to produce adversarial examples. In short, we find that
NewtonFool is substantially faster than either DeepFool (up
to 49X) or JSMA (up to 800X), while being only moderately
slower than FGSM. This is not surprising because NewtonFool
does not need to examine first-order information for all classes
of a complex classification network for many classes. However,
it is somewhat surprising that, NewtonFool, a gradient proce-
dure exploiting an aggressive assumption on the vulnerability
of CNN, achieves substantially better efficiency while produc-
ing competitive and often times better adversarial examples.
Due to space limitations detailed results are provided in the
appendix B.
Original image
FGSM
JSMA
DeepFool
NewtonFool
1.71 (0.49)
1.62 (0.54)
2.34 (0.84)
1.60 (0.53)
1.62 (0.54)
1.07 (0.35)
1.07 (0.34)
2.39 (1.30)
1.06 (0.31)
1.07 (0.34)
1.36 (0.59)
1.37 (0.66)
2.00 (1.01)
1.34 (0.61)
1.34 (0.61)
1.17 (0.52)
1.20 (0.55)
1.69 (0.96)
1.17 (0.53)
1.19 (0.55)
1.22 (0.53)
1.25 (0.57)
1.67 (0.82)
1.26 (0.60)
1.25 (0.59)
Original image
FGSM
JSMA
DeepFool
NewtonFool
1.22 (0.52)
1.23 (0.51)
1.97 (1.08)
1.23 (0.53)
1.23 (0.55)
1.78 (0.78)
1.76 (0.78)
2.47 (1.21)
1.75 (0.81)
1.75 (0.81)
1.08 (0.34)
1.12 (0.38)
1.87 (1.07)
1.11 (0.38)
1.11 (0.38)
2.23 (0.90)
2.20 (0.90)
2.95 (1.21)
2.14 (0.91)
2.15 (0.91)
1.56 (0.62)
1.57 (0.68)
2.21 (0.97)
1.53 (0.63)
1.54 (0.64)
Table 2: MNIST: Results with Canny edge detection. FGSM, DeepFool and NewtonFool give close results, with New-
tonFool being especially close with DeepFool. JSMA, on the other hand, produces significantly larger statistics with
respect to the Canny edge detection metric.
Original image
FGSM
JSMA
DeepFool
NewtonFool
13.73 (6.65)
18.88 (7.71)
15.04 (6.62)
14.84 (6.61)
14.68 (6.88)
17.85 (6.84)
22.63 (7.76)
18.54 (6.69)
18.82 (7.34)
18.34 (7.11)
11.69 (6.00)
19.11 (7.56)
12.95 (5.86)
12.96 (5.67)
12.81 (5.73)
18.14 (7.41)
20.97 (8.01)
19.18 (7.83)
18.40 (7.03)
18.37 (7.25)
10.37 (5.42)
17.89 (7.92)
11.20 (5.53)
11.16 (5.30)
10.62 (5.20)
13.66 (6.28)
20.45 (9.46)
14.71 (6.24)
15.06 (7.13)
15.02 (7.08)
Table 3: GTSRB: Results with Canny edge detection. Now JSMA, DeepFool and NewtonFool give close results, while
FGSM produces significantly worse results. NewtonFool gives the best results across all tests (except for the last sign).
FGSM
JSMA
DeepFool
NewtonFool
20.50 (8.13)
44.19 (7.29)
10.26 (3.57)
9.57 (3.53)
13.01 (3.38)
50.28 (8.37)
5.14 (1.13)
4.62 (1.08)
15.96 (8.56)
44.73 (8.53)
8.88 (4.33)
8.26 (4.11)
13.56 (7.72)
42.00 (9.24)
6.78 (3.49)
6.17 (3.30)
11.95 (6.64)
36.60 (6.52)
5.98 (2.73)
5.39 (2.69)
FGSM
JSMA
DeepFool
NewtonFool
12.57 (6.70)
42.37 (9.04)
6.39 (3.11)
5.95 (2.96)
15.33 (6.07)
48.67 (9.42)
8.54 (3.17)
7.83 (3.01)
15.79 (7.95)
45.07 (8.69)
7.37 (3.09)
6.60 (2.91)
11.99 (6.52)
49.44 (11.69)
7.42 (3.82)
6.76 (3.62)
10.43 (5.11)
44.41 (9.35)
6.70 (3.01)
5.96 (2.80)
Table 4: MNIST: Results with fast Fourier transform. DeepFool and NewtonFool give close results. FGSM and JSMA
produce worse results with JSMA being significantly larger. NewtonFool produces the best results for all labels.
5 LIMITATIONS AND FUTURE WORK
We have not implemented the enhancement to our algorithm
described in the appendix. In the future, we will implement
the enhancement and compare its performance to our basic
algorithm. The computer-vision research literature has sev-
eral algorithms for analyzing images, such as segmentation,
edge detection, and deblurring. In our evaluation we use one
algorithm (i.e., edge detection) as a metric. Incorporating other
computer-vision algorithms into a metric is a very interesting
avenue for future work. For example let µ1, µ2,· · · , µk be k met-
rics (e.g., based on number of pixels, edges, and segments). We
could consider a weighted metric (i.e. the difference between
i =1 wi µi (x1,x2)). The question re-
mains – what weights to choose? Perhaps mechanical turk
two images x1 and x2 is(cid:80)k
FGSM
JSMA
DeepFool
NewtonFool
46.86 (36.94)
54.95 (28.17)
9.67 (7.18)
9.02 (7.66)
37.50 (15.97)
66.81 (18.04)
7.33 (2.70)
6.16 (2.54)
44.34 (27.72)
79.87 (23.89)
8.07 (3.96)
6.84 (3.84)
37.81 (27.04)
72.62 (29.64)
7.88 (6.43)
6.93 (6.12)
15.12 (10.47)
56.14 (15.56)
4.38 (1.21)
3.49 (0.86)
50.84 (22.82)
78.91 (24.02)
8.78 (4.41)
7.95 (4.34)
Table 5: GTSRB: Results with fast Fourier transform. Again, DeepFool and NewtonFool give close results, while FGSM
and JSMA produce worse results. Across all tests, NewtonFool achieves the best results.
the weighted metric(cid:80)k
studies can be used to train the appropriate weights. Moreover,
i =1 wi µi (x1,x2) could also be incorpo-
rated into algorithms for constructing adversarial examples.
All these directions are very important avenues for future
work.
6 CONCLUSION
This paper presented a gradient-descent based algorithm for
finding adversarial examples. We also presented edge detectors
as a way for evaluating the quality adversarial examples gen-
erated by different algorithms. The research area of crafting
adversarial examples is very active. On the other hand, metrics
for evaluating the quality of adversarial examples has not been
well studied. We believe that incorporating computer-vision
algorithms into metrics is worthwhile goal and worthy of fur-
ther research. Moreover, having a diverse set of algorithms
for crafting adversarial examples is very important towards a
thorough evaluation of proposed defenses.
ACKNOWLEDGMENTS
We are grateful to the ACSAC reviewers for their valuable
comments and suggestions. We also thank Adam Hahn for
his patience during the submission process. This material is
based upon work supported by the Army Research Office
(ARO) under contract number W911NF-17-1-0405. Any opin-
ions, findings, conclusions and recommendations expressed in
this material are those of the authors and do not necessarily
reflect the views of the funding agencies.
REFERENCES
[1] DeepFace: Closing the Gap to Human-Level Performance in Face Verifica-
tion. In Conference on Computer Vision and Pattern Recognition (CVPR).
[2] Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J
Frey. 2015. Predicting the sequence specificities of DNA-and RNA-binding
proteins by deep learning. Nature biotechnology (2015).
[3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L.
Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba.
2016. End to End Learning for Self-Driving Cars. Technical Report.
[4] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner,
Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs
Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. 2016. End
to End Learning for Self-Driving Cars. CoRR abs/1604.07316 (2016). http:
//arxiv.org/abs/1604.07316.
[5] J Canny. 1986. A Computational Approach to Edge Detection. IEEE Trans.
Pattern Anal. Mach. Intell. 8, 6 (June 1986), 679–698. https://doi.org/10.
1109/TPAMI.1986.4767851
[6] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robust-
ness of Neural Networks. In IEEE Symposium on Security and Privacy.
[7] Chih-Lin Chi, W. Nick Street, Jennifer G. Robinson, and Matthew A. Craw-
ford. 2012. Individualized Patient-centered Lifestyle Recommendations:
An Expert System for Communicating Patient Specific Cardiovascular Risk
Information and Prioritizing Lifestyle Options. J. of Biomedical Informatics
45, 6 (Dec. 2012), 1164–1174.
[8] George E Dahl, Jack W Stokes, Li Deng, and Dong Yu. 2013. Large-scale
malware classification using random projections and neural networks. In
Proceedings of the IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 3422–3426.
[9] Navneet Dalal and Bill Triggs. 2005. Histograms of Oriented Gradients
for Human Detection. In Proceedings of the 2005 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition (CV PR’05) - Volume
1 - Volume 01 (CVPR ’05). IEEE Computer Society, Washington, DC, USA,
886–893. https://doi.org/10.1109/CVPR.2005.177
Learning Drive
http://www.
Autonomous
informationweek.com/big-data/big-data-analytics/
ai-machine-learning-drive-autonomous-vehicle-development/d/
d-id/1325906. (2016).
AI, Machine
Development.
2016.
Vehicle
Eddy.
[10] Nathan
Hall/CRC.
[11] Leslie Hogben (Editor). 2013. Handbook of Linear Algebra. Chapman and
[12] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining
and Harnessing Adversarial Examples. CoRR (2014).
[13] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explain-
ing and Harnessing Adversarial Examples. In Proceedings of the 2015 In-
ternational Conference on Learning Representations. Computational and
Biological Learning Society.
[14] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman
Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick
Nguyen, Tara N Sainath, et al. 2012. Deep neural networks for acoustic
modeling in speech recognition: The shared views of four research groups.
IEEE Signal Processing Magazine 29, 6 (2012), 82–97.
[15] Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein,
and JD Tygar. 2011. Adversarial machine learning. In Proceedings of the
4th ACM workshop on Security and artificial intelligence. ACM, 43–58.
[16] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. 2017. Safety Verification
of Deep Neural Networks.
[17] International Warfarin Pharmacogenetic Consortium. 2009. Estimation of
the Warfarin Dose with Clinical and Pharmacogenetic Data. New England
Journal of Medicine 360, 8 (2009), 753–764.
[18] K. Julian, J. Lopez, J. Brush, M. Owen, and M. Kochenderfer. 2016. Policy
Compression for Aircraft Collision Avoidance Systems. In Proc. 35th Digital
Avionics Systems Conf. (DASC).
[19] Guy Katz, Clark Barrett, David Dill, Kyle Julian, and Mykel Kochenderfer.
2017. An Efficient SMT Solver for Verifying Deep Neural Networks.
[20] Eric Knorr. 2015. How PayPal beats the bad guys with machine learn-
ing. http://www.infoworld.com/article/2907877/machine-learning/how-
paypal-reduces-fraud-with-machine-learning.html. (2015).
[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet
classification with deep convolutional neural networks. In Advances in
neural information processing systems. 1097–1105.
[22] A. Kurakin, I. J. Goodfellow, and S. Bengio. 2016. Adversarial Examples in
the Physical world. (2016).
[23] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
2015. DeepFool: a simple and accurate method to fool deep neural networks.
CoRR (2015).
[24] Seyed Mohsen Moosavi Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. 2017. Universal adversarial perturbations. In Proceedings of 2017
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
[25] Jorge Nocedal and Stephen Wright. 2006. Numerical Optimization.
Springer.
[26] NVIDIA. 2015. NVIDIA Tegra Drive PX: Self-Driving Car Computer. (2015).
http://www.nvidia.com/object/drive-px.html
[27] Nicolas Papernot, Ian Goodfellow, Ryan Sheatsley, Reuben Feinman, and
Patrick McDaniel. 2016. cleverhans v1.0.0: an adversarial machine learning
library. arXiv preprint arXiv:1610.00768 (2016).
[28] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson,
Z. Berkay Celik, and Ananthram Swami. 2016. The Limitations of Deep
Learning in Adversarial Settings. In Proceedings of the 1st IEEE European
Symposium on Security and Privacy. arXiv preprint arXiv:1511.07528.
[29] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014.
Glove: Global vectors for word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP 2014) 12 (2014), 1532–
1543.
[30] Alfio Quarteroni, Riccardo Sacco, and Fausto Saleri. 2000. Numerical
mathematics. p.307.
[31] Eui Chul Richard Shin, Dawn Song, and Reza Moazzezi. 2015. Recogniz-
ing functions in binaries with neural networks. In 24th USENIX Security
Symposium (USENIX Security 15). 611–626.
[32] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. 2012. Man vs. computer:
Benchmarking machine learning algorithms for traffic sign recognition.
Neural Networks 0 (2012), –. https://doi.org/10.1016/j.neunet.2012.02.016
[33] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
Erhan, Ian J. Goodfellow, and Rob Fergus. 2013. Intriguing properties of
neural networks. CoRR abs/1312.6199 (2013).
A EXTENSION
A.1 Problem Formulation
In general, given a point x0, we can consider two disjoint sets
of labels L+ and L− as follows.
• L+ = {l +1 , . . . ,l +
confidences F l
• L− = {l−
1 , . . . ,l−
confidences F l
m} is the set of m labels achieving high
s (x0) of the classifier
n } is the set of n labels achieving low