If encryption
keys are changed across versions and if the administra-
tor reverts back to a previous version, the decryption of
the ﬁle would no longer work. One possible solution is
to change the encryption keys of ﬁles after a capability
based authentication upon which SVSDS would decrypt
all the older versions and re-encrypt them with the newly
provided keys. The disadvantage with this approach is
that the versioned blocks need to be decrypted and re-
encrypted when the keys are changed.
DoS Attacks SVSDS is vulnerable to denial of service
attacks. There are three issues to be handled: (1) blocks
that are marked for versioning could be repeatedly over-
written; (2) lots of bogus ﬁles could be created to delete
old versions, and (3) versioned ﬁles could be deleted and
recreated again preventing subsequent modiﬁcations to
ﬁles from being versioned inside the disk. To counter at-
tacks of type 1, SVSDS can throttle writes to ﬁles that
are versioned very frequently. An alternative solution to
this problem would be to exponentially increase the ver-
sioning interval of the particular ﬁle / directory that is
being constantly overwritten resulting in fewer number
of versions for the ﬁle. As with most of the denial of
service attacks there is no perfect solution to attack of
type 2. One possible solution would be to stop further
writes to the disk, until some of the space used up by
older versions, are freed up by the administrator through
the administrative interface. The downside of this ap-
proach is that the disk effectively becomes read-only till
the administrator frees up some space. Type 3 attacks are
not that serious as versioned ﬁles are always backed up
when they are deleted. One possible solution to prevent
versioned ﬁles from being deleted is to add no-delete ﬂag
on the inode block of the ﬁle. This ﬂag would be checked
by SVSDS along with other operation-based constraints
before deleting/modifying the block. The downside of
this approach is that normal users can no longer delete
versioned ﬁles that have been marked as no-delete. The
administrator has to explicitly delete this ﬂag on the no-
delete ﬁles.
5 Implementation
We implemented a prototype SVSDS as a pseudo-device
driver in Linux kernel 2.6.15 that stacks on top of an
existing disk block driver. Figure 5 shows the pseudo
device driver implementation of SVSDS. SVSDS has
7, 487 lines of kernel code out of which 3, 060 were
reused from an existing TSD prototype. The SVSDS
layer receives all block requests from the ﬁle system,
and re-maps and redirects the common read and write
requests to the lower-level device driver. The additional
primitives required for operations such as block alloca-
tion and pointer management are implemented as driver
ioctls.
U
S
E
R
K
E
R
N
E
L
User Applications
File Systems
SVSDS Interface
SVSDS Pseudo−device Driver
Regular Block Interface
SCSI/IDE Driver
Regular Block Interface
Disk / RAID
Figure 5: Prototype Implementation of SVSDS
In the current implementation we maintain all hash ta-
bles (V-TABLE, T-TABLE, P-TABLE, and D-TABLE) as in-
memory data structures. As these hash tables only have
small space requirements, they can be persistently stored
in a portion of the NVRAM inside the disk. This helps
SVSDS to avoid disk I/O for reading these tables.
The read and write requests from ﬁle systems reach
SVSDS through the Block IO (BIO) layer in the Linux
USENIX Association  
17th USENIX Security Symposium 
269
kernel. The BIO layer issues I/O requests with the des-
tination block number, callback function (BI END IO),
and the buffers for data transfer, embedded inside the
BIO data structure. To redirect the block requests from
SVSDS to the underlying disk, we add a new data struc-
ture (BACKUP BIO DATA). This structure stores the des-
tination block number, BI END IO, and BI PRIVATE of
the BIO data structure. The BI PRIVATE ﬁeld is used
by the owner of the BIO request to store private infor-
mation. As I/O request are by default asynchronous
in the Linux kernel, we stored the original contents of
the BIO data structures by replacing the value stored
inside BI PRIVATE to point to our BACKUP BIO DATA
data structure. When I/O requests reach SVSDS, we
replace the destination block number, BI END IO, and
BI PRIVATE in the BIO data structure with the mapped
physical block from the T-TABLE, our callback func-
tion (SVSDS END IO), and the BACKUP BIO DATA re-
spectively. Once the I/O request is completed, the con-
trol reaches our SVSDS END IO function. In this func-
tion, we restore back the original block number and
BI PRIVATE information from the BACKUP BIO DATA
data structure. We then call the BI END IO function
stored in the BACKUP BIO DATA data structure, to notify
the BIO layer that the I/O request is now complete.
We did not make any design changes to the ex-
isting Ext2TSD ﬁle system to support SVSDS. The
Ext2TSD is a modiﬁed version of the Ext2 ﬁle sys-
tem that notiﬁes the pointer relationship to the ﬁle sys-
tem through the TSD disk APIs. To enable users to
select ﬁles and directories for versioning or enforcing
operation-based constraints, we have added three ioctls
namely: VERSION FILE, MARK FILE READONLY, and
MARK FILE APPENDONLY to the Ext2TSD ﬁle system.
All three ioctls take a ﬁle descriptor as their argument,
and gets the inode number from the in-memory inode
data structure. Once the Ext2TSD ﬁle system has the
inode number of the ﬁle, it ﬁnds the the logical block
number that correspond to inode number of the ﬁle. Fi-
nally, we call the the corresponding disk primitive from
the ﬁle system ioctl with logical block number of the in-
ode as the argument. Inside the disk primitive we mark
the ﬁle’s blocks for versioning or enforcing operation-
based constraint by performing a breadth ﬁrst search on
the P-TABLE.
6 Evaluation
We evaluated the performance of our prototype SVSDS
using the Ext2TSD ﬁle system [16]. We ran general-
purpose workloads on our prototype and compared them
with unmodiﬁed Ext2 ﬁle system on a regular disk. This
section is organized as follows: In Section 6.1, we talk
about our test platform, conﬁgurations, and procedures.
Section 6.2 analyzes the performance of the SVSDS
framework for an I/O-intensive workload, Postmark [8].
In Sections 6.3 and 6.4 we analyze the performance on
OpenSSH and kernel compile workloads respectively.
6.1 Test infrastructure
We conducted all tests on a 2.8GHz Intel Xeon CPU with
1GB RAM, and a 74GB 10Krpm Ultra-320 SCSI disk.
We used Fedora Core 6 running a vanilla Linux 2.6.15
kernel. To ensure a cold cache, we unmounted all in-
volved ﬁle systems between each test. We ran all tests at
least ﬁve times and computed 95% conﬁdence intervals
for the mean elapsed, system, user, and wait times using
the Student-t distribution. In each case, the half-widths
of the intervals were less than 5% of the mean. Wait time
is the difference between elapsed time and CPU time,
and is affected by I/O and process scheduling.
Unless otherwise mentioned, the system time over-
heads were mainly caused by the hash table lookups
on T-TABLE during the read and write operations and
also due to P-TABLE lookups during CREATE PTR and
DELETE PTR operations. This CPU overhead is due to
the fact that our prototype is implemented as a pseudo-
device driver that runs on the same CPU as the ﬁle sys-
tem. In a real SVSDS setting, the hash table lookups will
be performed by the processor embedded in the disk and
hence will not inﬂuence the overheads on the host sys-
tem, but will add to the wait time.
We have compared the overheads of SVSDS using
Ext2TSD against Ext2 on a regular disk. We denote
Ext2TSD on a SVSDS using the name Ext2Ver. The let-
ters md and all are used to denote selective versioning
of meta-data and all data respectively.
6.2 Postmark
Postmark [8] simulates the operation of electronic mail
and news servers. It does so by performing a series of
ﬁle system operations such as appends, ﬁle reads, direc-
tory lookups, creations, and deletions. This benchmark
uses little CPU but is I/O intensive. We conﬁgured Post-
mark to create 3,000 ﬁles, between 100–200 kilobytes,
and perform 300,000 transactions.
Figure 6 show the performance of Ex2TSD on SVSDS
for Postmark with a versioning interval of 30 seconds.
Postmark deletes all its ﬁles at the end of the benchmark,
so no space is occupied at the end of the test. SVSDS
transparently creates versions and thus, consumes stor-
age space which is not visible to the ﬁle system. The av-
erage number of versions created during this benchmark
is 27.
For Ext2TSD, system time is observed to be 1.1 times
more, and wait time is 8% lesser that of Ext2. The
270 
17th USENIX Security Symposium 
USENIX Association
)
s
d
n
o
c
e
s
(
e
m
T
d
e
s
p
a
E
l
i
780.6
768.0
789.7
793.1
 1200
 1000
Wait
User
System
 800
 600
 400
 200
 0
Ext2
Ext2TSD
Ext2Ver(md)
Ext2Ver(all)
Ext2
Ext2TSD Ext2Ver(md) Ext2Ver(all)
Elapsed
System
Wait
780.5s
36.28s
741.42s
768.0s
88.58s
676.11s
Space o/h
0MB
0MB
789.7s
191.71s
593.80s
443MB
Performance Overhead over Ext2
Elapsed
System
Wait
-
-
-
-1.60 %
1.44 ×
-8.12 %
1.17%
4.28 ×
-19.91%
793.1s
191.94s
597.09s
1879MB
1.61%
4.29×
-19.47%
Figure 6: Postmark results for SVSDS
increase in the system time is because of the hash ta-
ble lookups during CREATE PTR and DELETE PTR calls.
The decrease in the wait time is because, Ext2TSD does
not take into account future growth of ﬁles while allocat-
ing space for ﬁles. This decrease in wait time allowed
Ext2TSD to perform slight better than Ext2 ﬁle system
on a regular disk, but would have had a more signiﬁcant
impact in a benchmark with ﬁles that grow.
For Ext2Ver(md), elapsed time is observed to have no
overhead, system time is 4 times more and wait time is
20% less than that of Ext2. The increase in system time
is due to the additional hash table lookups to locate en-
tries in the T-TABLE. The decrease in wait time is due to
better spacial locality and increased number of requests
being merged inside the disk. This is because the ran-
dom writes (i.e., writing inode block along with writing
the newly allocated block) were converted to sequential
writes due to copy-on-write in versioning.
For Ext2Ver(all), The system time is 4 times more and
wait time is 20% less that of Ext2. The wait time in
Ext2Ver(all) does not have any observable overhead over
the wait time in Ext2Ver(md). Hence, it is not possible
to explain for the slight increase in the wait time.
6.3 OpenSSH Compile
To show the space overheads of a typical program in-
staller, we compiled the OpenSSH source code. We used
OpenSSH version 4.5, and analyzed the overheads of
Ext2 on a regular disk, Ext2TSD on a TSD, and meta-
data and all data versioning in Ext2TSD on SVSDS
for the untar, configure, and make stages combined.
Since the entire benchmark completed in 60–65 seconds,
we used a 2 second versioning interval to create more
versions of blocks. On an average, 10 versions were
created. This is because the pdﬂush deamon starts writ-
ing the modiﬁed ﬁle system blocks to disk after 30 sec-
onds. As a result, the disk does not get any write request
for blocks during the ﬁrst 30 seconds of the OpenSSH
Compile benchmark. The amount of data generated by
this benchmark was 16MB. The results for the OpenSSH
compilation are shown in Figure 7.
)
s
d
n
o
c
e
s
(
i
e
m
T
d
e
s
p
a
E
l
60.2
60.5
64.5
64.5
Wait
User
System
 100
 80
 60
 40
 20
 0
Ext2
Ext2TSD
ExtVer(md)
ExtVer(all)
Ext2
Ext2TSD Ext2Ver(md) Ext2Ver(all)
Elapsed
System
Wait
60.186s
10.027s
0.187s
60.532s
10.231s
0.390s
Space o/h
0MB
0MB
64.520s
14.147s
0.454s
496KB
Performance Overhead over Ext2
Elapsed
System
Wait
-
-
-
0.57 %
2 %
108 %
7.20%
41 %
142%
64.546s
14.025s
0.634s
15.14MB
7.21%
39%
238%
Figure 7: OpenSSH Compile Results for SVSDS
For Ext2TSD, we recorded a insigniﬁcant increase in