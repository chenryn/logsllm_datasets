Our framework for doubly-oblivious data structures (DODS)
formalizes the foregoing ideas, most notably by exposing a
richer interface that enables ﬁne-grained control over memory
accesses to internal memory. Below we summarize the interface
and implementation of each algorithm of this framework.
, irt) → (st, ptr
• Initialization: DODS.InitS
Equals ODS.Init, but calls DORAM.Init, not ORAM.Init.
rt). Equals to ODS.Start.
• Start: DODS.Start(mut st, ptr
• Access: DODS.AccessS
(mut st, op) → res.
Input now has the form “op(data, dummy, isCached)”. There
are four cases:
– dummy = 1, isCached = ?: Perform dummy ReadBlock.
– dummy = 1, isCached = 1: Fetch dummy node from the
(m, [nodei]n
1
rt).
cache without dummy ReadBlock.
– dummy = 0, isCached = 1: Fetch actual node from cache.
– dummy = 0, isCached = ?: Perform ReadBlock to fetch
real (non-dummy) node. If queried node is already cached,
perform dummy ReadBlock.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:06 UTC from IEEE Xplore.  Restrictions apply. 
• Finalize: DODS.FinalizeS
(mut st, node, bound) → ptr
rt.
Similar to ODS.Finalize, except that it does not perform
additional dummy operations. Instead, it checks that the
number of DORAM.ReadBlock operations thus far equals
bound. Satisfying this condition is the responsibility of the
data structure designer. (Compare: in ODS the designer only
has to specify the bound; padding occurs automatically.)
C. Doubly-oblivious sorted multimaps
We construct doubly-oblivious sorted multimaps (DOSM).
We modify our construction of singly-oblivious sorted mul-
timaps (OSM, see Section V-B) to: (i) use DODS (see prior
sub-section), instead of merely ODS, as a building block; and
(ii) leverage the ﬁne-grained interface of DODS for improved
efﬁciency. Details follow.
Naive approach. A naive approach to make the OSM client
doubly-oblivious is to simply replace the underlying ODS
framework with the DODS framework. However, this does
not sufﬁce: the OSM client maintains internal state (outside
the ODS framework) and its accesses to it are data dependent.
For example, OSM.Insert uses a depth-ﬁrst search to ﬁnd the
insertion location, and this search terminates as soon as the
location is found, which depends on the key-value pair to
insert. The adversary can learn some information about this
pair because it can observe when this termination occurs (after
this point all accesses correspond to cache accesses rather than
external memory accesses).
Our construction. To eliminate such leakage, we identify
data-dependent sub-procedures of our algorithms, and appropri-
ately pad out the number of accesses made in these procedures
to worst-case bounds that depend only on the number of key-
value pairs in the map. For example, when an algorithm initiates
a depth-ﬁrst search, we ensure that the search terminates after
accessing exactly 1.44 log(n) (real or dummy) nodes, which
is the worst-case height of an AVL tree with n nodes.
Next, we design our algorithms so that that we can always
predict whether or not a given dummy access needs to return
a cached node. We can then take advantage of the ﬁne-grained
DODS interface to avoid unnecessary dummy operations.
Below we summarize our doubly-oblivious construction (again
omitting deletions for space reasons, as in Section IV-C).
• DOSM.Init: Equals OSM.Init, but calls DODS.Init instead
of ODS.Init.
• DOSM.Size: Instead of halting the depth-ﬁrst search when
the ﬁrst k-node is found, perform additional DODS.Access
calls with input read(dummy = 1, isCached = ?, k) to
ensure that DORAM.ReadBlock is invoked 1.44 log(n) times
in total (the worst-case height of an AVL tree with n nodes).
• DOSM.Insert: Modify the depth-ﬁrst search used to ﬁnd the
insertion location so that DORAM.ReadBlock is invoked
1.44 log(n) times (as in DOSM.Size above). Also, in the
retracing step, modify the rebalancing procedure to perform
the same (real or dummy) operations regardless of the type
of rebalancing required.
• DOSM.Find: Recall that OSM.Find has two steps: ﬁnd paths
to the i-th and j-th k-nodes (nodei and nodej from here on)
and fetch all k-nodes in the subtree bounded by these. We
describe how both steps can be made doubly-oblivious.
1) Find path to s-th k-node: Modify the depth-ﬁrst search
so that DORAM.ReadBlock is invoked 1.44 log(n) times
(as in DOSM.Size above).
It is important to ensure that retrieving the path to nodej
after retrieving the path to nodei does not reveal where
the two paths diverge. This happens when the search
retrieves common nodes from the cache and not the server,
and is prevented by invoking DODS.Access with input
read(dummy = 0, isCached = ?, k) (this ensures that a
ReadBlock is always performed).
2) Retrieve required nodes: Find the node at which the
paths to nodei and nodej diverge (as in OSM.Find), and
then, instead of performing a simple breadth-ﬁrst search
from this node, run a modiﬁed breadth-ﬁrst search that
(a) uses an oblivious priority queue instead of a simple
ﬁrst-in-ﬁrst-out queue, and (b) terminates after visiting
2 · 1.44 log(n) + j − i nodes. Initialize this queue with
(real and dummy) keys of the nodes on paths to nodei
and nodej, in that order. When fetching the next node
from the queue, add the key of the appropriate child
to the queue with an “exploration priority” that decides
when the node gets visited. We assign priorities so that
nodes on the bounding paths are visited ﬁrst, and k-nodes
in the intersection afterwards.
VI. EVALUATION AND APPLICATIONS
Implementation. We realized singly- and doubly-oblivious
versions of Oblix using ∼ 10 K lines of Rust code, split across
libraries for singly- and doubly-oblivious Path ORAM, ODS,
and OSM.
Evaluation. We evaluate Oblix via a set of benchmarks
(Sections VI-A and VI-B) and via three applications: (i) private
contact discovery for the Signal messaging service (Sec-
tion VI-C); (ii) private retrieval of public keys in Key Trans-
parency (Section VI-D); (iii) oblivious searchable encryption
(Section VI-E). In each application, our results show that
Oblix is competitive with, and sometimes also improves upon,
alternate approaches with similar security guarantees. Overall,
our work shows that ORAM-based techniques, often eschewed
for their perceived large costs, can scale to large databases
(tens of millions of records) and can be effectively applied to
concrete problem domains.
We emphasize that this paper focuses on achieving low
latency, and so our experiments focus on that. In many settings
throughput is also important, and we leave to future work the
problem of achieving high throughput as well. Our techniques
ultimately leverage properties of Path ORAM, for which strong
concurrency properties, exempliﬁed in systems such as TaoStore
[56], are known. We thus believe that improving throughput is
an exciting, and potentially viable, future project.
Experimental setup. All experiments use a server with an
Intel Xeon E3-1230 v5 CPU at 3.40 GHz with 8 logical cores,
running Ubuntu 16.04. The CPU supports the Intel SGX v1
288
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:06 UTC from IEEE Xplore.  Restrictions apply. 
in Oblix takes 0.47 ms compared to ZeroTrace’s 1.22 ms to
1.32 ms (based on the choice of the underlying ORAM scheme),
representing a speedup of ∼ 2.5×. The gap widens further as
the block size increases: for a block size of 512 bytes, Oblix
takes 0.54 ms and is 4.5× to 6.5× faster than ZeroTrace.
B. DOSM microbenchmarks
We evaluate the latency of searches and inserts in our doubly-
oblivious OSM scheme (see Section V-C). Our experiments
show that latency is a few milliseconds, even when the database
contains millions of key-value pairs.
Searches. The cost of a search query depends on (i) the
total number of key-value pairs, and (ii) the number of values
requested for the queried key. We experimentally measure
latency as a function of these parameters, and report the average
latency across 100 iterations. All experiments use keys and
values of 8 bytes each, and use an underlying Path ORAM
implementation with a block size of 160 bytes.
• Increasing the number of key-value pairs. We initialize an
OSM scheme with up to 224 key-value pairs. We then issue
search queries for random keys, requesting a single value
per key. Fig. 6 shows that search time is logarithmic in
the number of key-value pairs. Moreover, even with 224
key-value pairs, search time remains low at 4.4 ms.
• Increasing the number of requested values. We initialize
an OSM scheme with 214 keys each mapped to 210 values,
for a total of 224 key-value pairs. We then issue queries
for random keys, fetching an increasingly-large interval of
values. Fig. 7 shows that search time is linear in the size
of the interval. (Fetching a single value merely requires the
client to fetch a single path in the search tree, as opposed to
two paths in the general case of fetching intervals of values.)
• Increasing the number of values per key. We initialize an
OSM scheme with 2i keys each mapped to 224−i values,
for a total of 224 key-value pairs. We then issue queries
for random keys, fetching an interval of 10 values. Our
experiments conﬁrm that search time does not depend on
the number of values per key: across different choices of i,
the latency is steady at 12.7 ms.
Inserts. We initialize an OSM scheme with up to 224 key-
value pairs, and then measure the cost of inserting a random
key-value pair. Fig. 8 shows that insert time is logarithmic in
the number of key-value pairs in the database. Moreover, even
with 224 key-value pairs, insert time remains low at 5.4 ms.
C. Private contact discovery in Signal
Signal [2] is a popular messaging service that offers end-to-
end message encryption. When a user downloads the Signal
application on a phone, the application communicates with
Signal servers to determine which contacts on the user’s phone
use Signal; similarly, when the user adds new contacts to the
phone, the application must determine which of these use Signal.
This process is known as contact discovery. The importance
to ensure its privacy (Signal servers do not learn the contact
list in the user’s phone) has already been documented [44].
Figure 5: Latency of Path DORAM operations with an increasing number of
initial blocks, across different block sizes B.
instruction set, and the total memory available to enclaves is
limited to around 94 MB. In experiments with Signal and Key
Transparency, we initialize Oblix with the maximum number
of key-value pairs that ﬁt within memory, which is 64 GB
in our testbed. We note that production servers are typically
equipped with larger memory sizes; we therefore extrapolate the
performance of Oblix for larger database sizes as well. Further,
since the size of key-value pairs differs across applications, we
conﬁgure the Path ORAM implementation underlying Oblix
with a different block size per application. Finally, before each
experiment, we warm up the ORAM stash via dummy requests
in order to capture steady-state performance of Oblix.
A. Path DORAM microbenchmarks
(cid:3)
We begin by evaluating the performance of our Path DORAM
scheme (see Section V-A). Recall that during initialization,
DORAM.Init is provided as input a maximum storage size m
(in blocks), and a list of n initial blocks (with n ≤ m). We
evaluate the performance of a single operation (a ReadBlock
followed by a Evict) in our DORAM scheme for n = m ∈
(cid:2)
101, . . . , 107
and for block sizes from 8 to 512 bytes. In
Fig. 5, we report the average latency over 1000 operations;
this latency is between tens to hundreds of microseconds.
Comparison with ZeroTrace. To put these numbers into
perspective, we compare the performance of our scheme with
that of ZeroTrace [57], which also implements a DORAM
scheme within a hardware enclave. We provide a qualitative
comparison between the two systems in Section VII; here, we
focus on performance. The source code of ZeroTrace is not
publicly available, and so we can only compare our results
with the ones reported in the paper. However, both our testbeds
use machines of similar capabilities.
Unlike Oblix, which uses the ODS framework to outsource
the client’s position map, ZeroTrace recursively stores the
position map in smaller ORAMs. Each ZeroTrace ORAM
operation thus requires recursive position map lookups. We
estimate our DORAM scheme’s performance in this setting
by measuring the access times for each level of recursion
and taking their sum. Our ﬁndings underscore the efﬁciency
of our Path DORAM protocols compared to ZeroTrace: with
107 blocks and a block size of 8 bytes, an ORAM operation
289
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:06 UTC from IEEE Xplore.  Restrictions apply. 
Figure 6: Search time is logarithmic in the
number of key-value pairs.
Figure 7: Search time is linear in the size of
the requested interval.
Figure 8: Insert time is logarithmic in the
number of key-value pairs
Signal’s approach. Signal makes contact discovery private
via a method based on Intel SGX [44, 3], where the user sends
a list of encrypted contacts and Signal servers compare these,
within the hardware enclave, against the database of all Signal
users. In order to prevent leakage through accesses to internal
memory, the enclave ﬁrst converts the list into an oblivious
hash table, and then iterates over all Signal users, looking up
each one in the hash table. Overall, if the user sends a list with
m contacts and Signal has N users, the latency is O(m2 + N );
note that the latency is linear in the number of all Signal users.
Our approach. We describe how to use Oblix to achieve pri-
vate contact discovery with latency O(m log N ); in particular,
we do not perform a linear scan of all Signal users. This is an
asymptotic improvement because N (cid:10) m (Signal has millions
of users but any user typically has no more than several hundred
contacts on a phone). Our experiments below show that these
asymptotic gains yield efﬁciency gains in practice.
We use Oblix to construct, and then maintain, an oblivious
index over all Signal users. When a user submits a list of
contacts, the hardware enclave iterates over the contacts in the
list, looking up each one in the oblivious index. As a result,
latency is linear in the number contacts in the list (m), but
only logarithmic in the number of all Signal users (N).
Experimental comparison. We consider databases of up to
N = 128 M users. Each user is identiﬁed by a phone number
represented as an 8-byte integer (as in Signal’s implementation);
we thus initialized the index with 8-byte keys mapped to null
values. We set the Path ORAM block size to 160 bytes.
We compare the performance of Signal’s approach and our
approach by issuing contact discovery requests with lists of
different sizes (m = 1, 10, 100, 1000), and measure the latency
to process the request at the server. We report the average time
across 100 iterations per request.
Fig. 9 compares costs of Signal’s approach and our approach.
The cost in Signal’s approach comes from: (i) converting
the submitted list into an oblivious hash table, and then
(ii) performing all the lookups. As the total number N of users
grows, the latter dominates and the total cost increases linearly
with N. When N = 128 M, the latency is 950 − 830 ms.
Fixing the number m of contacts submitted by the user,