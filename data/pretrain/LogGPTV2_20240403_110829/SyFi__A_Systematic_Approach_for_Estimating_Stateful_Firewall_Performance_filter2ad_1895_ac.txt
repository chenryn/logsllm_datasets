minimal impact on performance are key observations that enables our approach
to be simple, yet highly accurate (as we show later in Section 4).
Once we have the ﬁrewall performance in packets per second, it is straight-
forward to calculate the throughput in any metric that the user prefers. For
example, the Traﬃc Translator II in our approach can be used to compute ﬁre-
wall throughput in bytes per second, as long as average packet size is available.
4 Validation of the Model
In this section, we validate the accuracy of our model by comparing its pre-
dictions with actual measurements. We perform this validation when using the
model in two scenarios—1) to estimate a ﬁrewall’s performance on normal traﬃc,
and 2) to estimate a ﬁrewall’s performance when under a SYN ﬂood attack.
In the ﬁrst part of our validation, we present here results from the HP TM-
Szl ﬁrewall [4]—a diﬀerent device from the ones we used to develop our model
in Sections 2 and 3; we measure packet processing costs using the mechanism
described in Section 2 for the four types of packets considered in our model.
SyFi: A Systematic Approach for Estimating Stateful Firewall Performance
81
 2000
Measured
Model-Estimate
s
p
b
M
n
i
t
u
p
h
g
u
o
r
h
T
 1500
 1000
 500
 0
TP1
TP2
TP3
TP4
s
p
b
M
n
i
t
u
p
h
g
u
o
r
h
T
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
Measured
Model-Estimate
UDP-1518
HTTP-32k
HTTP-512
Type of traffic
(a) Normal traﬃc
(b) SYN ﬂood attack
Fig. 6. Comparison of measured throughput with throughput estimates from our model
We evaluate our model on four disparate proﬁles of normal traﬃc, which test
diﬀerent variations with respect to protocol type, packet size, and packet counts
per ﬂow. In the second part, we evaluate on the ability of our model to predict
the performance of the SonicWall ﬁrewall when under a SYN ﬂood attack.
Normal traﬃc. The four traﬃc proﬁles we consider are as follows. In our
baseline traﬃc proﬁle TP1, 20% of ﬂows correspond to TCP traﬃc and the
remaining 80% of ﬂows are UDP. UDP ﬂows send 100 packets/second and the
average packet size is 64 bytes, while TCP ﬂows send 1000 packets/second, and
the average packet size is 512 bytes. In our second traﬃc proﬁle TP2, we keep
all parameters the same as in TP1 but make all ﬂows shorter; we decrease the
number of packets per TCP and UDP ﬂow to 10 and 1, respectively. Shorter
ﬂows result in a higher rate of session creation, which as we have shown earlier
is an expensive operation for stateful ﬁrewalls. Third, in TP3, we change the
average packet sizes in our baseline proﬁle to make packets larger; we make
average packet sizes for TCP and UDP ﬂows to be 1024 bytes and 512 bytes
respectively. Lastly, we use the standard HTTP-32K traﬃc proﬁle generated
by our BPS traﬃc generation tool. We analyzed the packets generated by our
test tool and found that every HTTP ﬂow contains 52 packets which includes
3 packets each for TCP connection setup and teardown, 1 HTTP GET, 1 ACK
for the HTTP GET, 22 TCP data packets, and 22 TCP ACK packets. We use
this information to compute the probabilities for our fourth traﬃc proﬁle TP4.
For each of the four traﬃc proﬁles, we measure the maximum throughput
that the HP TMSzl ﬁrewall can support by generating the corresponding traﬃc
from our traﬃc generation tool and scaling up the rate of the traﬃc until we
begin to observe packet drops. We then compare the measured values with the
corresponding estimates provided by our model. Figure 6 shows that our model’s
estimates are within 6% of the measured values for all four traﬃc proﬁles. These
results highlight our model’s ability to correctly predict the eﬀects of variations
in ﬂow duration, packet sizes, and protocol types.
Attack traﬃc. Beyond estimating the throughput that a ﬁrewall can support
on the normal traﬃc at a customer’s site, customers may also want to ensure
that the ﬁrewall they choose can support their traﬃc even when under attack.
To evaluate the utility of our model in such a scenario, we consider the problem
82
Y. Beyene, M. Faloutsos, and H.V. Madhyastha
of estimating ﬁrewall throughput in the face of a SYN ﬂood attack. For this
experiment, we consider three diﬀerent standard traﬃc patterns in our traﬃc
generation tool—UDP ﬂows with 1518 bytes of data, HTTP ﬂows with 32 KB
of data, and HTTP ﬂows with 512 bytes of data on average. We subject the
SonicWall ﬁrewall to these traﬃc patterns in turn, and in each case, use our traﬃc
generation tool to launch 10K SYN packets per second in parallel. Figure 6(b)
compares the measured values of the throughput sustained by the ﬁrewall with
the corresponding estimates from our model. Our model’s estimates are accurate
with less than 10% error in all three traﬃc proﬁles.
The experiments reveal signiﬁcant throughput change when traﬃc proﬁle
[4], and the
varies. Thus the 5 Gbps data sheet through for the HP TMSzl
3.9 Gbps throughput for SonicWall
[6] is over-optimistic.
Finally, we performed a preliminary investigation of the eﬀect of the ﬁrewall’s
ruleset, i.e., the set of ACLs used to ﬁlter traﬃc, on session rate. We ran an
experiment wherein we populated the ﬁrewall’s ruleset with several DENY rules
that do not match our test traﬃc, and added a single rule at the bottom to permit
the test traﬃc. We repeated this experiment on all three ﬁrewalls, varying the
number of rules and the test traﬃc. In all cases, we found that the maximum
session rate signiﬁcantly declines with an increase in the number of rules.
5 Related Work
RFC 2544 [11] is the industry leading network interconnecting device benchmark-
ing methodology since 1999. RFC 2647 [22] extends the terminology established
with deﬁnitions speciﬁc to ﬁrewalls. It speciﬁes device throughput to be tested
with frames sized 64, 128, 256, 512, 1024 and 1518 bytes. Firewall vendors ex-
ploit RFC 2544 by publishing throughput measurements in bytes/sec conducted
with full-sized (1,518 bytes) UDP packets only. Clearly, this does not reﬂect a
typical enterprise traﬃc mix, which involves diﬀerent protocols and packet sizes.
Third-party testing agencies such as NSS [23] have challenged vendors by
evaluating and comparing ﬁrewalls with respect to their performance, security,
and stability. NSS measure throughput using a more realistic traﬃc mix. Though
a step in the right direction, it doesn’t address a customer’s basic question of
how the ﬁrewall will perform in her network since the ﬁrewall’s throughput varies
signiﬁcantly across traﬃc proﬁles.
The focus of the research community has been on 1) optimizing ﬁrewall rules
either by reordering rules or removing redundant rules [7,12,17], 2) detecting
policy errors in a ﬁrewall’s ruleset [21,18,9], 3) detecting anomalies and vulner-
abilities in ﬁrewalls [9,24,14], and 4) improving ﬁrewall design [16,15] to prevent
the policy errors and anomalies from happening in the ﬁrst place.
To the best of our knowledge, there is no prior work to answer the question—
what is the performance a given customer can expect from a particular ﬁrewall?
Customers may therefore end up buying a low performing device which could
potentially introduce a bottleneck into their network, or a high performing device
that is more expensive than necessary. In this paper, we resolve this issue in a
SyFi: A Systematic Approach for Estimating Stateful Firewall Performance
83
systematic way, and our model accurately computes the expected performance of
a ﬁrewall using a proﬁle of the traﬃc proﬁle from the deployment environment.
Perhaps the most closely related work in terms of resource based performance
analysis is that of Dreger et al. [13]. They developed nidsconf, a tool that
examines Intrusion Detection Systems resource utilization on a sample traﬃc
proﬁle and derives conﬁgurations that prevent bursts of packet drops due to
spikes in CPU and memory utilization.
6 Conclusions
Since performance numbers in the datasheets of ﬁrewalls are often inﬂated, cus-
tomers of stateful ﬁrewalls today are left needing to rely on word of mouth
recommendations to choose a ﬁrewall. To make the process more scientiﬁc, in
this paper, we examined two state-of-the-art enterprise-level stateful ﬁrewalls to
highlight the factors that aﬀect their performance. Based on our observations
that protocol and packet type matter for performance but packet sizes and num-
ber of concurrent sessions do not, we developed a model of ﬁrewall performance
that takes as input characteristics of the particular ﬁrewall and the traﬃc at
a target network. Our evaluations on a third ﬁrewall showed that our model
can estimate throughput across diﬀerent traﬃc proﬁles with over 90% accuracy.
In the future, we will study the performance impact of payload inspection and
connection teardown packets.
References
1. Comparison shopping for scalable ﬁrewall products, http://tinyurl.com/7smaqet
2. Data sheets lie: How to measure the performance, security and stability of network
devices,
http://resources.breakingpoint.com/acton/form/567/0024:d-0004/0/
3. Fortinet FortiGate-ONE,
http://www.fortinet.com/products/fortigate/one.html
4. HP Threat Management Services zl module,
http://h20195.www2.hp.com/v2/GetPDF.aspx/4AA2-6512ENN.pdf/
5. Next Generation Firewalls not ready to replace all legacy ﬁrewalls,
http://searchnetworking.techtarget.com/news/1520651/
Next-generation-firewalls-not-ready-to-replace-all-legacy-firewalls/
6. SonicWALL E-class network security appliance E5500,
http://www.firewalls.com/sonicwall/
sonicwall-firewall/sonicwall-e-class-series/
7. Acharya, S., Wang, J., Ge, Z., Zane, T.F., Greenberg, A.: Traﬃc-aware ﬁrewall
optimization strategies. In: ICC (2006)
8. Al-Shaer, E., Hamed, H., Boutaba, R., Hasan, M.: Conﬂict classiﬁcation and anal-
ysis of distributed ﬁrewall policies. In: IEEE JSAC (2005)
9. Baboescu, F., Varghese, G.: Fast and scalable conﬂict detection for packet classi-
ﬁers. In: IEEE ICNP (2002)
10. BreakingPoint ﬁrewall performance testing,
http://www.breakingpointsystems.com/solutions/firewall-testing/
84
Y. Beyene, M. Faloutsos, and H.V. Madhyastha
11. Bradner, S., McQuaid, J.: Benchmarking methodology for network interconnect
devices. RFC 2544 (1999)
12. Cohen, E., Lund, C.: Packet classiﬁcation in large ISPs: Design and evaluation of
decision tree classiﬁers. In: ACM SIGMETRICS (2005)
13. Dreger, H., Feldmann, A., Paxson, V., Sommer, R.: Predicting the Resource Con-
sumption of Network Intrusion Detection Systems. In: Lippmann, R., Kirda, E.,
Trachtenberg, A. (eds.) RAID 2008. LNCS, vol. 5230, pp. 135–154. Springer, Hei-
delberg (2008)
14. El-Atawy, A., Al-Shaer, E., Tran, T., Boutaba, R.: Adaptive early packet ﬁltering
for protecting ﬁrewalls against DoS attacks. In: IEEE INFOCOM (2009)
15. Gouda, M.G., Liu, A., Jafry, M.: Veriﬁcation of distributed ﬁrewalls. In: IEEE
GLOBECOM (2008)
16. Gouda, M.G., Liu, A.X.: Structured ﬁrewall design. Computer Networks (2007)
17. Hamed, H., Al-Shaer, E.: Dynamic rule-ordering optimization for high-speed ﬁre-
wall ﬁltering. In: ASIACCS (2006)
18. Hari, A., Suri, S., Parulkar, G.: Detecting and resolving packet ﬁlter conﬂicts. In:
IEEE INFOCOM (2000)
19. Liu, A.X.: Change-impact analysis of ﬁrewall policies. In: European Symp. Re-
search Computer Security (2007)
20. Liu, A.X.: Firewall policy veriﬁcation and troubleshooting. In: ICC (2008)
21. Liu, A.X., Gouda, M.G.: Firewall policy queries. IEEE Trans. on Parallel and
Distributed Systems (2009)
22. Newman, D.: Benchmarking terminology for ﬁrewall devices. RFC 2647 (1999)
23. NSS Labs. IPS, UTM, Web application ﬁrewall testing lab, http://nsslabs.com
24. Shaer, E.A., Hamed, H.: Discovery of policy anomalies in distributed ﬁrewalls. In:
IEEE INFOCOM (2004)
25. Caceres, R.: Measurements of Wide-Area Internet Traﬃc, UCB/CSD.89/550, Univ.
CA, Berkeley (1989)