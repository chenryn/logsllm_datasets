p
s
e
r
 24
 18
 12
 6
 0
216
home (20 Mbps)
218
220
# elements
222
)
c
e
s
(
e
m
i
t
e
s
n
o
p
s
e
r
 64
 48
 32
 16
 0
216
mobile (10 Mbps)
218
220
# elements
222
FIGURE 10—Mean response time experienced by a client under different deployments (see text for a description of network conditions) with
different PIR schemes. When the network bandwidth is plentiful (intra-DC), downloading the entire database (scp) achieves the lowest response
time. However, when the network bandwidth is limited (home, mobile), SealPIR achieves the lowest response time.
 120
 90
 60
 30
 0
)
c
e
s
(
e
m
i
t
e
s
n
o
p
s
e
r
n
a
e
m
SealPIR
XPIR (d=2)
XPIR (d=3)
scp
 0
 100
 200
 300
 400
 500
 600
throughput (queries/min)
FIGURE 11—Comparing throughput vs. mean response time under
SealPIR and XPIR (with d = 2 and d = 3) when using a database with
220 elements where each element is 288 bytes long. We ﬁnd that XPIR
with d = 2 saturates at 9 requests/second whereas SealPIR saturates at
7 requests/second (a 23% reduction in throughput). When XPIR uses
d = 3, SealPIR achieves about 50% higher throughput.
7.2.2 Throughput
We deploy the PIR server in Azure’s US West data center, but
access it with an increasing number of concurrent PIR clients
deployed across the South India and EU West data centers.
We then measure the number of requests serviced per minute
at the server, and the request completion times at the clients.
Figure 11 depicts the results of running from 4 to 256 clients
each requesting one 288-byte element from a database with 220
entries. In our experiments, we ensure that the bottleneck is the
server’s CPU or WAN network connection, and not the clients
or some link between speciﬁc data centers.
We ﬁnd that SealPIR achieves a 50% higher throughput than
XPIR with d = 3, but a 23% lower throughput than XPIR with
d = 2. Most of the difference can be attributed to EXPAND, but
we believe that with further engineering we can close this gap
(since SealPIR is comparable to XPIR according to microbench-
marks). Compared to naive PIR via scp, SealPIR and XPIR
achieve over 20× higher throughput since the primary bottle-
neck in naive PIR is network bandwidth and not CPU (which is
the bottleneck for both SealPIR and XPIR).
7.3 Beneﬁts of PBCs
To understand how PBCs can improve throughput and what type
of network overhead they add, we repeat the microbenchmark
experiments of Section 7.1, but this time we use mPIR (with
Cuckoo hashing, see Section 4.5). To put the beneﬁts and costs
in context, we also evaluate the multi-query PIR scheme of
Pung [11]. Pung’s protocol, like PBCs, is probabilistic and
improves over existing batch codes in terms of costs. In this
experiment we use SealPIR with t = 220 as the underlying PIR
library and change only the multi-query scheme being used.
Figure 12 gives the results. We ﬁnd that mPIR does a better
job than Pung’s scheme at amortizing CPU costs across all
batch sizes. This is a direct effect of the Cuckoo PBC producing
fewer total codewords (see Figure 5), since computational costs
are proportional to the number of elements after encoding (m).
At k = 256 and 288-byte elements, mPIR achieves a 2.6×
reduction in CPU cost for the server when answering queries
over Pung’s scheme. Over running k parallel instances of PIR,
the per-request CPU cost of mPIR is 40.5× lower.
The difference in network costs between Pung’s scheme and
mPIR is more pronounced. This owes to Pung’s scheme build-
ing on the subcube batch code of Ishai et al. [52] which creates
a large number of buckets (see Figure 5); to preserve privacy,
clients must issue a PIR query to each bucket. In terms of con-
crete savings, mPIR is 6× more network efﬁcient (upload and
download) than Pung’s scheme. Considering that mPIR also has
−40, compared to Pung’s
a lower failure probability (around 2
−20), this suggests that mPIR is an attractive replacement to
2
Pung’s multi-query protocol, offering improvements on all axes.
Observe that at k = 256, mPIR’s download costs are the same
as running k-parallel instances of PIR. This is counterintuitive
since mPIR results in 50% more answers (the extra answers are
dummies that hide which buckets are of interest to the client;
see Section 5). However, each answer in mPIR contains fewer
ciphertexts because of the interaction between SealPIR and
mPIR. In particular, mPIR encodes the 220-entry databases into
1.5k = 384 buckets, and replicates elements 3 times. Buckets
therefore have on average 213 elements. Recall from Section 3
that if d > 1, the number of ciphertexts in an answer depends on
(cid:2)). Furthermore, Equa-
the expansion factor F = 2 log(q)/ log(t
is larger for smaller databases.
tion 1 (Section 6) shows that t
(cid:2) = 210 (F = 12),
Indeed, for the original 220-entry database, t
(cid:2) = 215 for the average bucket (F = 8). Consequently,
whereas t
for our choice of parameters, the total download communication
ends up being the same: 256 · 12 = 384 · 8 ciphertexts.5
(cid:2)
5Similar beneﬁts apply to Pung’s scheme when used with SealPIR: observe in
Figure 12 that as k goes from 16 to 64, the amortized answer size goes down.
973
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
single-query
Pung’s multi-retrieval
mPIR (Cuckoo hashing)
batch size (k)
client CPU costs (ms)
MultiQuery
MultiExtract
server CPU costs (sec)
MultiSetup
MultiAnswer
network costs (KB)
query
answer
1
3.07
2.51
6.1
3.24
64
384
16
29.03
20.00
2.02
1.37
577
2,885
64
28.50
16.27
0.64
0.49
577
2,308
256
28.58
16.36
0.30
0.21
577
2,308
16
6.45
3.26
1.50
0.69
96
480
64
5.26
3.25
0.38
0.23
96
480
256
4.92
2.70
0.12
0.08
96
384
FIGURE 12—Per-request (amortized) CPU and network costs of two multi-query PIR schemes on a database consisting of 220 elements, with
varying batch sizes. The schemes are Pung’s multi-retrieval protocol and mPIR, which is based on PBCs (Cuckoo variant). The second column
gives the cost of retrieving a single element (no amortization). The underlying PIR library is SealPIR with t = 220 and elements are 288 bytes.
Note that this parity in download cost is not true in general;
it is a result of the particular parameters used in this case. In
fact, because of Equation 1 (§6), we can even achieve lower
amortized download costs. Without EXPAND’s optimization,
this would not be the case: in some sense, the optimization
introduces communication overhead to fetching elements from
databases with many entries and mPIR amortizes that overhead.
As an aside, Equation 1 does not affect upload costs; these costs
increase by 50% since the client is sending 50% more queries.
40 K
30 K
20 K
10 K
0
)
n
i
m
/
s
e
g
a
s
s
e
m
(
t
u
p
h
g
u
o
r
h
t
Pung
Pung+S
Pung+M
Pung+MS
1
16
64
256
message batch size (k)
7.4 Case study: Pung with SealPIR and mPIR
To evaluate the end-to-end beneﬁts that SealPIR and mPIR
provide to actual applications, we modify the available imple-
mentation of Pung [3]. Pung is a messaging service that allows
users to exchange messages in rounds without leaking any meta-
data (who they are talking to, how often, or when). We choose
Pung because it uses XPIR to achieve its privacy guarantees,
and because it also relies on multi-query PIR to allow clients
to receive multiple messages simultaneously. Consequently, we
can switch Pung’s PIR engine from XPIR to SealPIR, and we
can replace Pung’s custom multi-query PIR scheme with mPIR.
Experiment. We have clients send one message and retrieve k
messages (this models clients engaging in group conversations).
We run the system in a close-loop and advance rounds as soon as
all clients have sent and retrieved the messages. To experiment
with many clients we employ the same simulation technique
used in Pung: we have 32 real clients accessing the server,
and simulate additional clients by pre-populating the server’s
database with random messages.
Figure 13 shows the throughput in messages per minute
that Pung achieves with mPIR and SealPIR (“Pung+MS”).
Pung+MS yields better performance than the existing Pung
code base for all batch sizes greater than 1. There are three rea-
sons for this. First, Pung’s multi-retrieval produces 50% more
codewords than mPIR, and therefore has to do more processing.
Second, Pung’s multi-retrieval produces 7× more buckets than
mPIR. This forces Pung to run XPIR on many small databases
that contain an average of 500 to 8,000 elements (depending on
the batch size), which exacerbates XPIR’s ﬁxed costs.
Last, even though SealPIR incurs additional CPU costs than
FIGURE 13—Throughput of Pung on one server with 256K users,
each retrieving k 288 byte messages per round. The label “Pung” in-
dicates the implementation as given in [3], with updated parameters
(§7). “Pung+S” is a version of Pung that uses SealPIR with t = 220;
“Pung+M” is a version of Pung that uses mPIR; and “Pung+MS” is a
version of Pung that uses both mPIR and SealPIR.
 1500
 1000
 500
0