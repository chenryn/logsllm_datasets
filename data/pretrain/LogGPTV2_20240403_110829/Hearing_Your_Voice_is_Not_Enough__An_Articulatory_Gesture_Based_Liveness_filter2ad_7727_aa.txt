title:Hearing Your Voice is Not Enough: An Articulatory Gesture Based Liveness
Detection for Voice Authentication
author:Linghan Zhang and
Sheng Tan and
Jie Yang
Hearing Your Voice is Not Enough: An Articulatory Gesture
Based Liveness Detection for Voice Authentication
Linghan Zhang, Sheng Tan, Jie Yang
Florida State University
Tallahassee, Florida, USA
{lzhang,tan,jie.yang}@cs.fsu.edu
ABSTRACT
Voice biometrics is drawing increasing attention as it is a promising
alternative to legacy passwords for mobile authentication. Recently,
a growing body of work shows that voice biometrics is vulnerable
to spoofing through replay attacks, where an adversary tries to
spoof voice authentication systems by using a pre-recorded voice
sample collected from a genuine user. In this work, we propose
VoiceGesture, a liveness detection system for replay attack detec-
tion on smartphones. It detects a live user by leveraging both the
unique articulatory gesture of the user when speaking a passphrase
and the mobile audio hardware advances. Specifically, our system
re-uses the smartphone as a Doppler radar, which transmits a high
frequency acoustic sound from the built-in speaker and listens to
the reflections at the microphone when a user speaks a passphrase.
The signal reflections due to user’s articulatory gesture result in
Doppler shifts, which are then analyzed for live user detection.
VoiceGesture is practical as it requires neither cumbersome opera-
tions nor additional hardware but a speaker and a microphone that
are commonly available on smartphones. Our experimental evalua-
tion with 21 participants and different types of phones shows that
it achieves over 99% detection accuracy at around 1% Equal Error
Rate (EER). Results also show that it is robust to different phone
placements and is able to work with different sampling frequencies.
CCS CONCEPTS
• Security and privacy → Biometrics; Mobile and wireless security;
KEYWORDS
Voice authentication; Liveness detection; Articulatory gesture
1 INTRODUCTION
Biometrics has gained increasing attention and significance as it is
a promising alternative to legacy passwords for user authentication.
Among various biometric modalities (such as fingerprint, iris and
facial), voice has wide applicability as it is the primary mode of com-
munication, enabling biometric samples to be acquired remotely
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3133962
through existing landline, cellular and VoIP communication chan-
nels without additional hardware. Unlike other biometrics, voice
biometrics has the advantage of natural integration with passwords
or face authentication in mobile devices for multi-factor authen-
tication. Over recent years, voice authentication has matured to
become a low-cost and reliable method for authenticating users in
a wide range of applications such as access control, forensics and
law enforcement [54].
Particularly, with the advances of mobile technologies, voice
authentication is becoming increasingly popular in a growing range
of mobile applications. For instance, voice biometrics has been
integrated with smartphone operating systems and mobile apps
for secure access and login. Examples include Google’s "Trusted
Voice" for Android devices [10], Lenove’s voice unlock feature for its
smartphones [1], and Tencent’s "Voiceprint" feature in WeChat for
voice based app login [7]. Moreover, voice authentication has also
been progressively deployed in e-commerce and mobile banking.
For example, Saypay, a biometric authentication solutions provider,
provides voice authentication services for online transactions in
e-commerce [4]. And a considerable number of financial institutes,
such as HSBC, USAA, National Australia, Citi and U.S. Bank, have
started testing or are deploying voice recognition mobile apps and
ATMs to allow customers to bank without requiring passwords or
card swipes [3]. Voice authentication thus has increasingly gained
interest in mass-market adoption, as also evidenced by the predicted
market share of $184.9 billion in 2021 [12].
Recently, a growing body of research has demonstrated the vul-
nerability of voice authentication systems to spoofing through
replay attacks [21, 24, 49, 51], where an adversary tries to spoof
the authentication system by using a pre-recorded voice sample
collected from a genuine user [48]. The replay attacks are easy to
carry out, requiring neither sophisticated equipments nor specific
expertise. They are also increasingly practical due to the wide avail-
ability of low-cost, high-quality recording and playback devices.
The popularity of social media further makes it relatively easy for
an adversary to obtain voice samples from the intended target user.
Importantly, such low-cost and low-effort attacks have been shown
to be highly effective in spoofing the voice authentication systems.
For instance, simply replaying a pre-recorded voice command of a
user could unlock her/his mobile devices that have voice-unlock
feature (e.g., Android devices) [10]. An extensive study in 2017
shows that replay attacks increase the equal error rate (EER) of
state-of-art voice authentication systems from 1.76% to surprisingly
30.71% [24]. Replay attacks thus pose serious threats to the voice
authentication systems and have drawn much attention recently.
To defend against replay attacks, liveness detection system is
required to distinguish between the legitimate voice samples of
Session A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA57live users and the replayed ones. Traditional methods mainly rely
on the acoustic characteristics of an input utterance. Such meth-
ods, however, are only effective when the input utterance contains
significant additive or convolution noises, for example, when the
voice samples are collected surreptitiously [40]. They fail when the
recordings took in benign acoustic environments with high-quality
recorders as such recordings are close to indistinguishable from the
genuine ones [43]. An adversary could also obtain a copy of genuine
voice recording and directly supply to the authentication system,
bypassing the local microphone. Such high quality recordings and
playbacks make it extremely hard, if not possible, for detecting re-
play attacks with only the acoustic characteristics. For instance, the
replay attack detection 2017 challenge shows that current acoustic
characteristics based detection methods only achieve EER of 24.65%
on average [24]. The acoustic characterization based approaches
therefore have very limited effectiveness in practice.
Current voice authentication service providers, such as Voice-
Vault [9] and Nuance [5], mainly rely on the challenge-response
based approaches for liveness detection. In particular, the user is
prompted to repeat a closed set of sentences in addition to the user
enrolled passphrase [15]. Such a method however increases the op-
eration overhead of the user and is cumbersome due to an explicit
user cooperation is required besides the standard authentication
process. More recently, Chen et al. [17] develop a smartphone based
liveness detection system by measuring the magnetic field emit-
ted from loudspeakers. It however requires the user to speak the
passphrase while moving the smartphone with predefined trajec-
tory around the sound source. Moreover, Zhang et al. [55] propose
a smarthphone based solution, which measures the time-difference-
of-arrival (TDoA) changes of a sequence of phoneme sounds to the
two microphones of the phone when a user speaks a passphrase for
liveness detection. However, it requires a user to hold the phone at a
specific position. While effective, the above-mentioned approaches
introduce cumbersome operations as they require either additional
steps during authentication or holding or moving the phone in
some redefined manners.
In this paper, we introduce VoiceGesture, a smartphone based
liveness detection system that achieves the best of both worlds -
i.e., it is highly effective in detecting live users, but does not require
the users to perform any cumbersome operations. In particular, our
system achieves around 1% EER and works when the users hold
the phones with their habitual ways of speaking on the phones, i.e.,
have the phone held either to user’s ear or in front of the mouth.
Our system leverages a user’s articulatory gestures when speak-
ing a passphrase for liveness detection. Human speech production
relies on the precise, highly coordinated movements of multiple ar-
ticulators (e.g., the lips, jaw and tongue) to produce each phoneme
sound. It is known as articulatory gesture, which involves multidi-
mensional movements of multiple articulators [29]. Unlike human,
loudspeaker produces sound relying on solely the diaphragm that
moves in one dimension (i.e., forward and backward). Thus, by
sensing the articulatory motions when speaking a passphrase, a
human speaker can be distinguished from a loudspeaker. Moreover,
there exist minute differences in articulatory gesture among people
due to individual diversity in the human vocal tract (e.g., shape and
size) and the habitual way of pronouncing phoneme sounds [36].
Such minute differences could be further leveraged to detect an
adversary who tries to mimic the articulatory gesture of a genuine
user.
Our system exploits the mobile audio hardware advances to sense
and extract user-specific features of articulatory gesture when a
user speaks a passphrase to a smartphone. Although the increas-
ingly high definition audio capabilities supported by smartphones
are targeted at audiophiles, such advanced capabilities can also be
leveraged to sense the motions of the articulators during speech pro-
duction. In particular, current popular smartphones (e.g., Galaxy
S5, S6, and iPhone 5 and 6) are capable to record and playback
acoustic sounds at a very high frequency of 20kHz. Such a high
frequency has significant implication as it is inaudible to human
ear and is easily separable from human voice. Moreover, current
audio chips are able to playback and record at 192kHz sampling fre-
quency, which is also supported by smartphone OSs (e.g., Android
6.0 released in 2015) [2, 34]. The high sampling frequency enables
us to extract fine-grained frequency domain features to capture
both the articulator motions as well as the minute differences of
articulator gesture among people.
Our system thus re-uses the smartphone as a Doppler radar,
which transmits a high frequency acoustic tone at 20kHz from the
built-in speaker and listens to the reflections at the microphone
during the process of the voice authentication. The movements of
a user’s articulators when speaking a passphrase/utterance lead to
the Doppler frequency shifts at around 20kHz, which are recorded
together with the user’s voice sample. Our system then separates
the voice sample for conventional voice authentication and ex-
tracts user-specific features in the frequency shifts for liveness
detection. More specifically, in the user enrollment process, the
user-specific frequency shift features are extracted based on the
spoken passphrase and then stored in the liveness detection system.
During online authentication process, the extracted features of a
user input utterance are compared against the ones in the system.
If it produces a similarity score higher than a predefined threshold,
a live user is declared. To evaluate the performance of our system,
we conduct experiments with 21 participants and three different
types of phones under various experimental settings. Experimental
results show that our system is highly effective in detecting live
users and works with users’ habitual ways of talking on the phone.
The contributions of our work are summarized as follows.
• We show that the mobile audio hardware advances can be
leveraged to sense the articulatory gesture of a user when
she speaks a passphrase. We also show that it is feasible to
capture the minute differences in articulatory gesture among
different people when speaking the same phoneme sounds.
• We develop VoiceGesture, a liveness detection system that ex-
tracts user-specific features in the doppler shifts that resulted
from the articulatory gesture when speaking a passphrase
for live user detection. VoiceGesture is practical as it requires
neither cumbersome operations nor additional hardware but
a speaker and microphone that are commonly available on
smartphones.
• Our extensive experimental results show that VoiceGesture
achieves over 99% detection accuracy at around 1% EER.
Results also show that VoiceGesture is able to work with
different phone models and sampling frequencies.
Session A2:  Human AuthenticationCCS’17, October 30-November 3, 2017, Dallas, TX, USA58Figure 1: A typical text-dependent authentication system.
The remainder of the paper expands on the above contributions.
We begin with system and attack model, and a brief introduction
to the articulatory gesture sensing.
2 PRELIMINARIES
2.1 System and Attack Model
Voice authentication is the process of verifying the claimed iden-
tity of a user by extracting the acoustic features that reflect both
behavioral and physiological characteristics of a user [48]. In this
work, we primarily focus on the text-dependent system, in which
a user-chosen or system prompted passphrase is used for user au-
thentication. As a text-dependent system offers high authentication
accuracy with shorter utterances, it is generally more suitable for
user authentication than text-independent system [49]. A typical
text-dependent voice authentication system is shown in Figure 1.
Nevertheless, our liveness detection system could be extended to a
text-independent system [55].
We consider replay attacks in our work as they are easy to im-
plement by using the wide availability of low-cost and high-quality
digital recording and playback devices. To acquire a victim’s voice
samples, an adversary can either place a recording device surrep-
titiously in close proximity to the victim or utilize the victim’s
publicly exposed speeches. An adversary can also extract and con-
catenate the voice segments to match the victim’s passphrase to
launch replay attacks. In particular, we consider two types of replay
attacks: playback attack and mimicry attack. In a playback attack, an
adversary uses a loudspeaker to replay a pre-recorded passphrase of
an intended target user. Given that attackers may know the defend-
ing strategy of the liveness detection system, they could conduct
more sophisticated mimicry attacks, in which an adversary tries
to mimic the articulatory gesture of a genuine user. To perform a
mimicry attack, the adversary can use a far-field speaker to replay
a pre-recorded passphrase and simultaneously mimic the victim’s
articulatory gesture corresponding to the replaying passphrase. In
mimicry attacks, we also consider that the attacker can observe how
a genuine user pronounces the passphrase, for example by taking a
video of the genuine user, and then practice before conducting the
attack.
2.2 Articulatory Gesture
Human speech production requires precise and highly coordinated