botmaster chooses the length ﬁrst while the bot chooses the start-
ing character ﬁrst, the sequence generated from the random number
generator will cause each to generate a potentially different length
and starting character.
4. EVALUATION OF RESULTS
4.1 Tweet Collection
Several portions of this paper required collecting data from Twit-
ter. To determine the posting rate of tweets of each length, it was
necessary to collect tweets from real accounts. Therefore, data
from veriﬁed10 Twitter accounts was collected. A veriﬁed account
10https://support.twitter.com/articles/119135-faqs-about-veriﬁed
-accounts
is an account that Twitter has manually veriﬁed to be a speciﬁc per-
son or brand. By using veriﬁed accounts, this prevents obtaining
data from other bots or fake accounts. However, it has been noted
that veriﬁed accounts may not be a perfect representation of the av-
erage Twitter user, who is not generally a brand or celebrity. User
information stored includes the username and user ID, a unique in-
teger that Twitter stores for each user. A total of 54,114 users were
collected. Additionally, for creating the Markov chain, tweet con-
tent was parsed using a regular expression from the collected tweets
to ﬁnd username references in the tweets. In a tweet, usernames are
preceded by an @ symbol.
From the collected user IDs, tweet content, length, unique iden-
tiﬁer, and posting time were collected. Because of the number of
collected users and the number of tweets posted by each user, dur-
ing the data collection we only obtained tweets from 3,709 users.
However, this totaled 7,345,681 tweets. This data collection was
done automatically from a list of veriﬁed users that was obtained
from Twitter. Twitter has a special account with username veriﬁed
that will follow all veriﬁed accounts. Therefore, we were able to
search for all accounts followed by that special account using the
Twitter API and then begin obtaining tweets posted by each of those
accounts. Not all accounts post in English, some collected data is in
other languages including Portuguese, Spanish, Japanse, and Ara-
bic among others.
In order to remove these languages, we used
the third-party Java library NGramJ11, which performs language
recognition using n-grams. An n-gram is a sequence of symbols of
length n. For example, in English a common bigram (2-gram) is
th. This library ranked each tweet according to which language it
most resenbled. We kept only the tweets where the highest rank-
ing language is English. This left us with 5,461,009 tweets out
of the original 7,345,681. However, this method is not perfect.
Tweets contain some non-typical English characters in hashtags,
names, misspellings, URLs, etc. Because of this, the n-gram analy-
sis likely had some false positives and false negatives. For example,
one tweet had the text “Vancouver, 9/25”, but the n-gram model
marked it as French. After this n-gram analysis, tweets were fur-
ther restricted by checking the character values in each tweet. If a
tweet contains too many non-ASCII characters, then it is not likely
to be English. In this case, we removed all tweets that were 10%
or more non-ASCII characters. This allows up to approximately 14
non-ASCII characters in a full length tweet. The ﬁnal tweet count
is 5,011,973.
4.2 Stego System Evaluation
4.2.1 Emulab Performance and Reliability Experi-
ment
Emulab [15] is a network testbed and software system designed
for testing networked systems. It allows experimenters to request a
set of physical machines, or nodes, that are conﬁgured in a speciﬁc
network conﬁguration as deﬁned by a script ﬁle supplied to the
Emulab website.
The experiment for this paper was performed by generating sym-
bols from a test alphabet (the English alphabet) and posting gener-
ated tweets using a random text generator and a constructed encod-
ing map. The input symbols were chosen probabilistically with the
weight associated with their letter frequencies. The “botmaster”
acted from a desktop computer outside of the Emulab environment
posting the tweets while the “bots” acted from inside an Emulab
experimental setup. There were ﬁve bots in the experiment running
Emulab’s UBUNTU12-64-STD operating system image. The bot-
master generated a new symbol and posted the corresponding tweet
11http://ngramj.sourceforge.net/index.html
Botmaster
Twitter
Node A
Node B
Node C
Node D
Node E
Figure 6: Diagram for the Emulab experiment network layout.
Node A Node B Node C Node D Node E
5.725
5.888
5.644
5.600
5.912
Table 3: Average read time in seconds for each node.
every 30 seconds while the bots performed an HTTP request to the
correct Twitter account every 10 seconds checking for new tweets.
Each time the botmaster posted, it is recorded to a log ﬁle. Each
time the bots read a tweet, they also recorded to a log ﬁle. Af-
terwards, the logs were collected to compare the post time with the
retrieval time and also to match each original input symbol with the
decoded symbols from the bots. Due to a time zone difference be-
tween the botmaster machine and the bots in the Emulab setup, the
original time stamps from the bots appeared one hour later, so one
hour was subtracted from their times when comparing the differ-
ence in posting and reading time between the botmaster and bots.
Figure 6 shows the network conﬁguration for the experiment.
In the experiment, 100% of input symbols were correctly de-
coded by the bots except for a small set that were off by one. After
examining the data it was determined that in these cases, the tweets
being posted were generated with a trailing space that was then
trimmed by Twitter while posting. If the tweets had been generated
without spaces, this would not have occurred and so these cases
were dropped from the results. The average read time in seconds
for each node are shown in table 3. Each node averaged just over
ﬁve seconds from the botmaster’s post to the bot’s read. This is
likely due to the synchronization issues of the botmaster’s posting
and the bot’s sleep time between reads. A total of 305 tweets were
posted for this test and 10 of them were dropped for having trail-
ing whitespace. With an average transmission time of less than six
seconds, the overall transmission rate is up to 10,800 bytes per day.
4.2.2 Capacity
There are three major criteria for stego system evaluation: ca-
pacity, steganographic security, and robustness [1].
At the most basic level, the devised stego system can be used
to transmit at most seven bits of information per tweet because a
tweet can have a length of at most 140. If eight bits were to be
transmitted, there would be 256 different values. With seven bits,
there are 128 different values, so each value can be sent with a dif-
ferent length tweet. Therefore, we can state the maximum capacity
as seven bits per tweet. Tweets are posted to Twitter using UTF-8,
which is a variable length character encoding scheme that is a su-
perset of the ASCII characters. UTF-8 characters range from one
Length
10
11
12
Phrase Generator Database Generator
Lunch time
Hello World
Good Morning
no ragrets
I’m so.....
@J_Baxt16 14
Table 4: Sample tweets from the database based tweet generators.
does this tweet contain a secret message? Because our implemen-
tation does not embed any data in to the tweets, most techniques
that would be used on a normal stego system are not sufﬁcient. The
posted tweets appear identical to any other tweet from a textual
perspective. However, the tweet generation method is a large deter-
miner of detectability. It is possible to create the tweets manually,
but if the user wants to send many messages using the stego system
this will be cumbersome.
In order to automate the process, a Twitter bot program can be
used to create the tweets. An ideal generator would be a sophis-
ticated Twitter bot that can convince other users that it is human.
This is similar to passing a Turing test with the Twitter bot. If an
account is suspected of being a Twitter bot, it does not mean that
the communication has been detected, however it will cause sus-
picion. The adversary would have to recognize that the account is
being used to pass secret messages and that the secret messages are
done using the lengths of the tweets. The adversary would likely
assume that the text somehow contains the secret messages. If we
follow Kerckhoffs’ principle [10], then we must assume the adver-
sary knows that that the stego system passes messages by tweet
lengths. The two factors that must then be determined are then (i)
the account being used for transmission, and (ii) which tweets be-
ing posted contain the secret message.
If the adversary has no knowledge of which account is being
used, it will be exceptionally difﬁcult to ﬁnd. The Twitter website
states that there are now 271 million active monthly users and over
500 million daily tweets posted12 as of August, 2014. Because the
tweets have no distinguishing factors in general, an adversary can-
not easily search for the account by tweet content. If the adversary
understands the tweet generator being used, they may be able to
search for the account by the content. So far, the system has been
discussed in a way that implies that all tweets posted on the account
are part of the secret messages, however it is possible to extend an
input alphabet to leave space for ignored tweet lengths. The Twitter
bot could then post these between tweets that contain actual parts
of the secret messages.
The generator that we used is a database generator, which looks
up tweets of the appropriate length from an existing database. The
database may be populated by collecting real tweets from other ac-
counts or from collecting text from other sources. Two of these
database based generators were created. The ﬁrst uses a small set
of common phrases and for longer tweets some proverbs were col-
lected from the Internet. The second uses the Twitter data previous
collected (see section 4.1). Samples from each of these generators
are shown in table 4.
4.2.4 Network Packet Analysis
In addition to analyzing the stego system content on Twitter,
a small experiment was performed using Wireshark13 to monitor
packet contents while accessing Twitter. Twitter allows connecting
through HTTPS to access user pages, so when using this system it
is best to always access with HTTPS. In this experiment, the wget
command was run twice. First, it was run to access another known
12https://about.twitter.com/company
13https://www.wireshark.org/
Figure 7: Possible embedding rates for the stego system in bits on
a logarithmic scale.
Figure 8: Average posting rate in tweets per day on a logarithmic
scale. The green line indicates the mean.
to four bytes [21]. Therefore, the embedding rate will vary depend-
ing not only on the length of the tweet in characters, but also on
how many characters per byte are being used. The total number of
bytes in a tweet is 560, so the total number of bits is then 4480. The
possible embedding rates are shown in ﬁgure 7.
Additionally, we must consider how frequently tweets can be
posted. From the collected Twitter data (see section 4.1), the time
stamp of the tweets was also collected. For each unique user, the
average number of posts per day was then calculated from this data.
This data is shown in ﬁgure 8. In total, the averege daily posting
rate is 8.621 tweets per day. Therefore, if the user of the stego sys-
tem is trying to match real Twitter user posting rates, they cannot
send more than approximately 60 bits of data per day. If using the
system for botnet command and control, this will allow the bot-
master to post a small number of commands per day. The botmas-
ter does also have the choice to exceed this value, but then risks a
higher detection rate. Because the data shows the average number
of tweets posted per day per account, that means there are many
accounts that do post more tweets per day. In this data, there are
several accounts that post on average more than one hundred tweets
per day.
4.2.3
In this stego system we are assuming a passive warden model. In
a passive warden model, an adversary can view each message but
cannot modify them. The warden must solve the decision problem:
Steganographic Security
Twitter account, @BarackObama. Then, it was run to access the
test account used for this work, @alicesend.
In both cases, the
HTML content of the user’s page was downloaded and Wireshark
monitored all trafﬁc between the two hosts. After searching the
wireshark packet contents, there is no noticable difference in the
network trafﬁc. Searches were conducted for identifying strings
such as “alice” and “Obama” but all application data was encrypted
using TLS 1.2 according to Wireshark. Therefore, this trafﬁc in-
formation is insufﬁcient for determining which accounts are being
viewed by the source host. To a network observer, it simply ap-
pears as regular Twitter trafﬁc, which is generally common due to
Twitter’s popularity.
4.2.5 Robustness
Robustness is based on extracting the secret messages from the
cover objects [1]. As shown in the Emulab experiment in section
4.2.2, aside from some anomalous entries, every bot decoded the
appropriate input symbols perfectly. This assumes a passive war-
den model where no one has tampered with the data in transit. In
an active warden scenario, there are two possibilities: (i) Twitter
is modifying the tweets as they are posted or (ii) an adversary has
taken control of the botmaster’s Twitter account.
The ﬁrst scenario is extremely unlikely. Twitter does perform
some modiﬁcation as described in section 4.2.2 where trailing
whitespace was removed before the tweets were posted. However,
this modiﬁcation is well deﬁned and is not intended to modify the
contents of the secret or cover messages.
It can be handled by
properly implementing the tweet generator. The second condition
would be devastating for the system. In most of the paper, a pas-
sive warden model was assumed because it was assumed that the
botmaster could maintain control of their Twitter account. It is pos-
sible that the account is taken down if Twitter discovers that it is a
bot or that some other party obtains control of the account.
Aside from the steganographic robustness, the robustness of the
system in general is largely dependent on Twitter’s infrastructure,
which is one of the advantages of using such a service as the com-
munication medium. In previous years, Twitter has suffered with
outages, however it has recently improved signiﬁcantly. Because
of Twitter’s business model, downtime is very costly for many cor-
porations, organizations, and individuals that rely on Twitter for
marketing14, giving them great incentive to ensure that service is
maintained.
4.3 Username Generation Analysis
4.3.1
Scoring Names Based on the Generated Markov
Chain
One of the components in the botnet command and control sys-
tem is a method of generating plausible Twitter user names. As
described in section 3.1.5, Markov chains were used to generate
such user names. In order to analyze the usernames generated from
these Markov chains, two experiments were performed. First, a
probability measure was calculated on names based on the Markov
chain. We calculate the probability that a given string would have
been generated by the Markov chain. Let N be a name consisting
of the sequence of characters n1n2 . . . nk. The probability, P (N ),
of choosing N from the Markov chain is then
P (N ) = P (n1) × P (n2 | n1) × ··· × P (nk | nk−1).
(1)
Because Markov chains are “memoryless” in that the next state is
entirely dependent on the current state, it is not necessary to factor
14http://www.cnet.com/news/the-cost-of-twitter-downtime/
Figure 9: Average negative log probability score for usernames
based on constructed Markov chain.
Markov Chain Usernames
Random Text Usernames
Score Name
Score Name
Real Usernames
Name
davepeck
nytimes
focuspolitik
MarsHill
Scobleizer
warrenellis
redjumpsuit
joshspear