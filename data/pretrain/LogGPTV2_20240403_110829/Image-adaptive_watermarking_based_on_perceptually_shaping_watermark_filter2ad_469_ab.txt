### Image Number

**Figure 2: Detection Values of Watermarked Image Samples**
- (c1)
- (c2)
- (c3)

**Figure 1: Test on 'Lena'**
- (a) Original image 'Lena'
- (b1-b3) Watermarked copies obtained by PSW and EX-PSW (default value of \( l \) is set to 1, and \( r \) is set to 0.3 and 0.5 respectively). For PSW, the global perceptual distance is set to 0.3, while for EX-PSW, the perceptual distance in each block is set to 0.3.
- (c1-c3) Absolute difference between the original 'Lena' and the watermarked copies shown in (b1-b3), magnified by a factor of 5.

### Attacks
In this section, we present some of the most significant results. For the experiments, the perceptual distance between the host image and the watermarked one is set to 0.3 to ensure image transparency.

#### Watermark Invisibility
A watermark was embedded into the 'Lena' image using both PSW and EX-PSW. Figure 1(a) shows the original 'Lena' image, while Figures 1(b1-b3) display the watermarked copies. The images are visually indistinguishable, with a minor visible artifact at the bottom left corner due to pixel value overflow after DCT transformation. Figures 1(c1-c3) illustrate the absolute difference between the original and watermarked images, magnified by a factor of 5. It is evident that EX-PSW embeds a higher level of watermark in high-activity regions and around edges compared to PSW. This is particularly noticeable at the borders of the hat and the shoulders, and over the feathers. As \( r \) increases, the effectiveness becomes more prominent, indicating that EX-PSW exploits local image characteristics more efficiently.

#### Additional Images
In addition to 'Lena', 13 other images of different sizes were watermarked using both EX-PSW and PSW. The detection values are shown in Figure 2. Without any attacks, the detector of EX-PSW outputs larger detection values than PSW for all 14 images. As \( r \) increases, the detection values of EX-PSW also increase, confirming the analysis in Section VI.

### Robustness Evaluation
To evaluate the robustness of EX-PSW, a series of competitive experiments were conducted under various attacks, including JPEG compression, additive noise, filtering, and cropping. These typical attacks were chosen to test the resilience of the watermark against information loss. All experiments were performed on the 'Lena' image, with a detection threshold calculated based on a false positive probability \( P_f = 10^{-8} \).

**JPEG Compression (Figure 3)**
- EX-PSW is more robust against JPEG compression than PSW. EX-PSW with \( r = 0.4 \) can resist JPEG compression with quality factors above 20, which is nearly half the value required by PSW.

**Additive Noise (Figure 4)**
- Both EX-PSW and PSW are very robust against noise attacks. However, EX-PSW performs better, especially as the noise variance increases.

**Gaussian Low-Pass Filtering (Figure 5)**
- The detector response under Gaussian low-pass filtering with varying filter widths is shown. All detectors are sensitive to filtering, but EX-PSW with larger detection values can tolerate higher filter widths, although the improvement is not significant.

**Cropping (Figure 6)**
- The response of the detectors after cropping is applied to the watermarked image. The detection value remains high even when the cropped portion is as small as 16x16 pixels. This excellent performance is attributed to the efficient exploitation of local image characteristics.

### Conclusion
This paper proposes a new image-adaptive watermarking technique. The main idea is to perceptually shape the watermark blockwise, assigning an individual gain factor to each block during embedding. This approach allows for the full exploitation of local image characteristics. Within each block, the maximum detection value is achieved under the given distortion constraint described by Watson's model. We also introduced EX-PSW to enhance the detection value. The perceptual distance is suitable for blocks of appropriate size but not for the entire image. We detailed how to adjust the parameters in EX-PSW, which is essential for balancing invisibility and robustness. Both theoretical analysis and experimental results demonstrate the improved robustness of the proposed watermarking method.

### Acknowledgments
The authors would like to thank Xuying Zhao of the Institute of Automation for many fruitful discussions. We also appreciate the anonymous reviewers for their valuable comments.

### References
[1] M. Barni, F. Bartolini, V. Cappellini, and A. Piva. A DCT-domain system for robust image watermarking. *IEEE Transactions on Signal Processing*, 66(3):357–372, 1998.
[2] M. Barni and A. P. F. Bartolini. Improved wavelet-based watermarking through pixel-wise masking. *IEEE Transactions on Image Processing*, 10(5):783–791, May 2001.
[3] I. J. Cox and M. L. Miller. A review of watermarking and the importance of perceptual modeling. In *Proc. Electronic Imaging*, February 1997.
[4] I. J. Cox, M. L. Miller, and J. A. Bloom. *Digital Watermarking*. Academic Press, San Francisco, 2002.
[5] J. F. Delaigle and B. M. C. De Vleeschouwer. Watermarking algorithm based on human visual model. *IEEE Transactions on Signal Processing*, 66(3):319–335, 1998.
[6] N. Jayant, J. Johnston, and R. Safranek. Signal compression based on models of human perception. In *Proc. IEEE*, volume 81, Oct. 1993.
[7] D. Kirovski and H. S. Malvar. Spread-spectrum watermarking of audio signals. *IEEE Transactions on Signal Processing*, 51(4):1020 – 1033, 2003.
[8] A. S. Lewis and G. Knowles. Image compression using the 2-D wavelet transform. *IEEE Trans. Image Processing*, 1:244–250, 1992.
[9] M. L. Miller and J. A. Bloom. Computing the probability of false watermark detection. In *Proceedings of the Third International Workshop on Information Hiding*, pages 146–158, 1999.
[10] A. Nikolaidis and I. Pitas. Region-based image watermarking. *IEEE Transactions on Image Processing*, 10(11):1726–1740, 2001.
[11] C. Podilchuk and W. Zeng. Image-adaptive watermarking using visual models. *IEEE J. Selected Areas Comm.*, 16(4):525–539, May 1998.
[12] V. Saravanan, P. Bora, and D. Ghosh. Oblivious image-adaptive watermarking using quantization index modulation. In *The Eighth National Conference on Communications, India*, pages 26–37, Jan 2002.
[13] H. M. Tsai and L. W. Chang. Highly imperceptible video watermarking with the Watson’s DCT-based visual model. In *International Conference on Multimedia and Expo*, pages 1927 – 1930. IEEE Computer Society, June 2004.
[14] S. Voloshynovskiy, S. Pereira, V. Iquise, and T. Pun. Attack modelling: Towards a second generation watermarking benchmark. *Signal Processing, Special Issue*, 81(6):1177–1214, May 2001.
[15] A. B. Watson, G. Y. Yang, J. A. Solomon, and J. Villasenor. Visual thresholds for wavelet quantization error, SPIE-2657. Pages 381–392, 1996.
[16] B. Watson. DCT quantization matrices optimized for individual images. *Human Vision, Visual Processing, and Digital Display IV, SPIE-1913*, pages 202–216, 1993.
[17] R. B. Wolfgang, C. I. Podilchuk, and E. J. Delp. Perceptual watermarks for digital images and video. In *Proc. IEEE*, volume 87, pages 1108–1126, July 1999.
[18] X. S. Zhu and Y. S. Wang. Better use of human visual model in watermarking based on linear prediction synthesis filter. In *International Workshop on Digital Watermarking IWDW 2004, Lecture Notes in Computer Science (LNCS)*, volume 3304, pages 66–76, 2005.

### Appendix
**A. Embedding Strategy with HVM**
We prove Inequality (11) and analyze how to apply HVM for watermarking. Equation (10) can be reshaped as:

\[
\sum_{(i,j) \in B_k} P_{ij}^k = 1
\]

where

\[
P_{ij}^k = \frac{\alpha_k |w_s(i, j)|}{L_2 T_s(i, j)}, \quad (i, j) \in B_k
\]

As a result, we have

\[
0 \leq P_{ij}^k \leq 1
\]

To maximize the detection value \(\rho_k\) defined as Equation (26), the sign of \(w_s(i, j)\) should be the same as that of \(w_m(i, j)\).

\[
\rho_k = \frac{\sum_{(i,j) \in B_k} \alpha_k w_s(i, j) w_m(i, j) + \rho_k^o}{L_2}
\]

Therefore, the equivalent expression for Equation (26) is

\[
\rho_k = \frac{\sum_{(i,j) \in B_k} |\alpha_k w_s(i, j) w_m(i, j)| + \rho_k^o}{L_2}
\]

Substituting Equation (24) into Equation (27) yields

\[
\rho_k = \frac{T \sum_{(i,j) \in B_k} |s(i, j) w_m(i, j)|}{\sum_{(i,j) \in B_k} |s(i, j) w_m(i, j)|}
\]

So we have

\[
Q_{ij}^k = \frac{|s(i, j) w_m(i, j)|}{\sum_{(i,j) \in B_k} |s(i, j) w_m(i, j)|}, \quad (i, j) \in B_k
\]

Thus,

\[
\sum_{(i,j) \in B_k} Q_{ij}^k = 1
\]

\[
0 \leq Q_{ij}^k \leq 1
\]

Combining Equations (23), (30), and Inequalities (25), (31), it is easy to derive that

\[
\sum_{(i,j) \in B_k} P_{ij}^k Q_{ij}^k \leq \max_{(i,j) \in B_k} Q_{ij}^k
\]

Finally, substituting Equation (29) and Inequality (32) into Equation (28), we obtain

\[
\rho_k \leq T \max_{(i,j) \in B_k} |s(i, j) w_m(i, j)| + \rho_k^o
\]

Inequality (33) upper-bounds the detection value with Watson’s model as the distortion measurement.

From Inequality (32), it is observed that the choice of \(w_s(i, j)\) to improve the detection value should make \(P_{ij}^k\) as large as possible at the point where \(Q_{ij}^k\) is the largest. According to this idea, a simple strategy to design \(w_s(i, j)\) is represented by

\[
|w_s(i, j)| = s(i, j) (|s(i, j) w_m(i, j)|)^l
\]

where the sign function, \(\text{sgn}(·)\), is defined as

\[
\text{sgn}(t) = 
\begin{cases} 
1 & \text{if } t \geq 0 \\
-1 & \text{if } t < 0 
\end{cases}
\]

Considering \(w_s(i, j)\) has the same sign as \(w_m(i, j)\), we acquire the expression of \(w_s(i, j)\) from Equation (34) as

\[
w_s(i, j) = \text{sgn}(w_m(i, j)) s(i, j)^{l+1} |w_m(i, j)|^l
\]