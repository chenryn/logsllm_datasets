### Analysis of Benign and Malicious Documents

#### Feature Analysis
In benign documents, the ratio of specific features presents a distinct pattern. As shown in Figure 6, approximately 90% of benign documents have a ratio smaller than 0.2, and almost no document has a ratio over 0.6. This indicates that this feature can effectively distinguish between benign and malicious documents.

The statistical results for other static features in malicious documents are presented in Table VI. For boolean features, "False" is denoted as 0 and "True" as 1. Our findings reveal that while empty objects can be found in malicious samples, no benign documents contain such objects. This aligns with our intuition that users rarely have an incentive to include junk objects in documents and typically use automatic tools like `this.addscript()` and [43] to insert JavaScript, which rarely generate empty objects.

Compared to the previous two features, more malicious samples exhibit header obfuscation and hex code. We found only three benign documents with header obfuscation and none with hex code. This is likely because PDF documents are often created from other formats (e.g., Microsoft Word, LaTeX) using automatic conversion tools that do not obfuscate the document header or structure. Additionally, about 1% of malicious samples use multiple levels of encoding, and surprisingly, about 3% do not use any encoding. In benign documents, all use either zero or one level of encoding. Overall, these five features complement the first feature and enhance our ability to accurately distinguish between benign and malicious documents.

**Table VI: Statistics of Static Features in Malicious Documents**

| Feature          | Value 0/False | Value 1/True | Value 2 | Value 3 | Value 6 |
|------------------|---------------|--------------|---------|---------|---------|
| Header Obfuscation | 6792         | 578          | -       | -       | -       |
| Hex Code         | 6827         | 543          | -       | -       | -       |
| Empty Objects    | 7357         | 5            | -       | -       | -       |
| Encoding Level   | 233          | 7065         | 4       | 3       | 1       |

#### Memory Consumption
We randomly sampled 30 documents from each category, "Known Benign" and "Known Malicious," respectively. All 30 selected benign documents contained JavaScript. We then measured the memory consumption of these 60 documents in the JavaScript context, as shown in Figure 7. One malicious sample consumed over 1700 MB of memory, while on average, malicious samples consumed about 336.4 MB, compared to 7.1 MB for benign documents. The minimum memory consumed by malicious samples was 103 MB, whereas the maximum for benign samples was only 21 MB. These results indicate that monitoring memory consumption in the JavaScript context is an effective feature for differentiating between benign and malicious documents.

**Figure 7: Memory Consumption of Malicious and Benign JavaScripts**

**Figure 8: Memory Consumption of PDF Reader When Opening Multiple Documents**

To demonstrate the deficiency of context-free monitoring, we measured the memory consumption of a PDF reader when opening different numbers of documents simultaneously, a common practice in daily life. We used Adobe Acrobat 9.0 and four documents of varying sizes from our reference list, including [3], [5], [20], and [29]. For each document, we made 20 copies and recorded the memory consumption of Acrobat when different numbers of copies were opened. The results, shown in Figure 8, indicate that memory consumption generally increases linearly with the number of opened documents, up to 1600 MB. An exception is [3], where the memory consumption drops to a lower level after the 15th copy and then increases linearly again. This effect was consistent across multiple tests, suggesting that this specific document triggers some memory optimization mechanisms in Acrobat. These results highlight the difficulty of setting an appropriate threshold in context-free monitoring, as a high value could miss many malicious documents, while a low value may generate many false positives. By contrast, our context-aware monitoring is much more effective and accurate.

### Detection Accuracy

We evaluated the detection accuracy of our prototype in terms of false positive and false negative rates. We tested the malicious samples in VMware Workstation hosting Windows XP SP1 with Adobe Acrobat 8.0/9.0 installed. We first describe the parameter configuration of our detector and then present the detection results.

#### Parameter Configuration
First, we normalized non-binary features, including F1, F4, F5, and F8. The normalization rules are listed in Table VII. According to Figures 6 and 7, we set F1 to 1 if the ratio ≥ 0.2 and F9 to 1 if the memory consumption ≥ 100 MB. Similarly, the values of F5 and F6 were set according to Table VI, allowing all 13 features to be represented in binary values.

To set the weights and threshold, we required that a document be tagged as malicious if at least one JavaScript context feature and any other feature have positive values. If no suspicious behavior is detected in the JavaScript context, the document is considered out of the scope of our detection. Based on this criterion, we set w1 to 1, w2 to 9, and the threshold to 10.

**Table VII: Parameter Configurations in Our System**

| Parameter | Value                                                                 |
|-----------|-----------------------------------------------------------------------|
| F1        | If ratio ≥ 0.2, F1 = 1; else F1 = 0                                   |
| F4        | If # of empty objects ≥ 1, F4 = 1; else F4 = 0                        |
| F5        | If encoding level ≥ 2, F5 = 1; else F5 = 0                            |
| F8        | If mem consumption ≥ 100 MB, F8 = 1; else F8 = 0                     |
| w1        | 1                                                                     |
| w2        | 9                                                                     |
| Threshold | 10                                                                    |

#### Detection Results
We measured the false positive and false negative rates of the tuned detector over all benign documents with JavaScript (994) and one thousand randomly selected malicious samples. The malicious samples covered vulnerabilities in JavaScript interpreters, Flash, U3D (Universal 3D), TIFF, and JBIG2 image, among others. The detection results are shown in Table VIII.

**Table VIII: Detection Results**

| Category           | Detected Malicious | Detected Benign | Noise | Total |
|--------------------|--------------------|-----------------|-------|-------|
| Benign Samples     | 0                  | 994             | 0     | 994   |
| Malicious Samples  | 917                | 25              | 58    | 1000  |

No benign sample was misclassified as malicious, achieving zero false positives. Only one sample exhibited suspicious behavior in the JavaScript context, but it was still classified as benign due to the absence of other positive features. Upon inspection, we confirmed that the script used SOAP for network access. The rest of the 993 samples were tagged as benign because no suspicious JavaScript context behavior was monitored, even though some had positive values in other features. Despite the potential for JavaScript methods like SOAP and ADBC to generate network accesses, we did not whitelist them due to the inability to determine the maliciousness of the target server.

During the test, 58 (approximately 6%) of the malicious samples did nothing when opened. Inspection revealed that these samples exploited either CVE-2009-1492 [44] or CVE-2013-0640 [45], which do not work on Adobe Acrobat 8.0/9.0. Excluding these, we successfully detected 917 out of the remaining 942 samples, achieving a detection rate of 97.3%. We examined the 25 undetected samples and found two reasons for the misses: first, although malicious JavaScripts in these samples sprayed the heap, the PDF reader process crashed when the scripts attempted to hijack the control flow. Second, the 25 undetected samples used no obfuscation, so no static feature contributed to their detection. Although false negatives are unavoidable when malicious PDFs fail to exploit, it does not violate our primary goal of protecting users from malicious PDF damage.

**Table IX: Comparison With Existing Methods**

| Method             | False Positive Rate | True Positive Rate |
|--------------------|---------------------|--------------------|
| N-grams [17]       | 31%                 | 84%                |
| PJScan [7]         | 16%                 | 85%                |
| PDFRate [4]        | 2%                  | 99%                |
| Structural [5]     | 0.05%               | 99%                |
| MDScan [9]         | N/A                 | 89%                |
| Wepawet [18]       | N/A                 | 68% [9]            |
| Ours               | 0                   | 97%                |

Our method is comparable to the best fully static methods [4] [5]. Since the malicious samples in our dataset are not the most recent (the latest was captured in February 2013), we cannot fully demonstrate the superiority of our system over fully static methods. Therefore, we further compare our system with other methods by analyzing possible advanced attacks.

- **Our approach vs. Structural methods**: The mimicry attacks proposed in [8] can effectively bypass structural methods [4] [5] [6] [7]. However, our approach is immune to these attacks because we detect malicious attempts from JavaScript rather than how the JavaScript is stored in the PDF.
- **Our approach vs. Anti-virus Software**: There are many tricks available to evade anti-virus software [30] [46] [47]. Attackers can easily generate variants to defeat anti-virus software. Compared to anti-virus software, our method can effectively detect new variants and zero-day malicious PDFs in real-time because we use inconcealable system-level behaviors for detection.
- **Our approach vs. Dynamic Analysis Tools**: Attackers can subvert existing dynamic analysis tools like CWSandbox [13] using event-triggering and environment-sensitive malcode. Our method does not suffer from this limitation as we detect malicious behavior as real users operate on the documents.

Based on the analysis of potential advanced attacks, our method is more robust than existing defenses against malicious PDFs.

### System Performance

To measure the runtime overhead of our method, we ran our prototype on a 32-bit Windows 7 system. The tests were performed on a laptop with a 2.53 GHz Intel Core 2 Duo CPU and 2 GB of RAM. The performance of each component in our system is detailed below.

#### Static Analysis and Instrumentation
Overall, it took about 297.7 seconds to process all 7370 malicious samples, averaging 0.04 seconds per sample. We also measured the overhead for files of various sizes, randomly selecting three benign and three malicious documents. The sizes of these documents are shown in Table X. One of the malicious samples contained two scripts, while the rest contained only one script.

**Table X: Execution Time (in seconds) of Static Analysis & Instrumentation**

| PDF Size | 2 KB | 9 KB | 24 KB | 325 KB | 7.0 MB | 19.7 MB |
|----------|------|------|-------|--------|--------|---------|

The execution time for each step in static analysis and instrumentation is shown in Table X. The overhead is minor for both large and small documents. Specifically, it took only about 5.5 seconds to process a 20 MB document. Considering that it could take 20 seconds to download the document (at 1 MB/s), the additional delay of 5.5 seconds for processing is acceptable.