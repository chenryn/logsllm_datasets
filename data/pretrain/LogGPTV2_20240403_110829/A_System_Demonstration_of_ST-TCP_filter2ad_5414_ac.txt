0-7695-2282-3/05 $20.00 © 2005 IEEE
4
detects backup’s application failure, it will send out the
FIN immediately.
Primary does not generate a FIN, but backup gener-
ates a FIN. Here the backup TCP generates a FIN due
to a failure. This FIN is suppressed. The primary tran-
sits to non fault-tolerant mode. This happens at MaxDe-
layFIN if the primary is unable to detect backup’s ap-
plication failure; otherwise, it happens at the time the
primary detects backup’s application failure.
In the application failure cases described above, the fail-
ures are very likely to be detected if the application is read-
ing/writing bytes when the failure occurs. However, if there
is no application activity, the application failure may not be
immediately detected. This is not a problem for application
failures described in the previous section where a FIN is not
generated (since failure will be detected when there is some
application activity). However, in application failure situa-
tions where a FIN is generated, it may not be possible to con-
clusively distinguish between a normal closure and a failure
based solely on information at the TCP layer. This is true for
any primary/backup system.
To be able to detect application failures under all circum-
stances, either additional backup servers have to be deployed,
or some additional information is needed from the applica-
tion layer. Deploying additional backup servers will allow a
majority decision to be taken in case of a conﬂict between
the primary and a backup. For additional information from
the application layer, an application can support a watchdog
mechanism where the application continually sends a heart-
beat to a watchdog. The watchdog monitors the application
health and informs ST-TCP in case of any failure suspicion.
4.3 Local Network Failures
In this section we discuss network failures that are local
to the primary or the backup server, e.g., a NIC failure in a
server. We have assumed that both the primary and the backup
have a single NIC. Recall from Section 3 that the HB between
the primary and the backup is exchanged on two separate links
– an IP link and a serial link.
If a local network failure occurs, only the HB on the IP link
fails. The servers continue to exchange the HB on the serial
link. This enables the two servers to determine that a local
network failure has occurred. To determine if the failure has
occurred at the primary or at the backup, the servers examine
the “last client byte received” information (LastByteRe-
ceived) in the HB. If the client is sending data, then the
server with the NIC failure will not receive them, while the
server without NIC failure will receive them. Based on the
LastByteReceived in the HB, the primary or the backup
can determine if the other is lagging behind in terms of the
client bytes received. For example, if the backup determines
that the primary has lagged behind by greater than a thresh-
old number of bytes, or, that a particular byte has not been
received by the primary for more than a threshold period of
time, then it shuts the primary down and takes over the con-
nection. These threshold values are conﬁgurable. Similarly,
the primary shuts the backup down if it determines that the
backup is lagging behind.
One limitation of this failure detection method is that it
depends on the client sending data. There are several applica-
tions, e.g. FTP, that do not require client to send a lot of data.
In such cases, this method of failure detection does not work.
This problem can be partially solved by having the primary
and backup look at the acks received from the client. If the
backup NIC is down, the latest client ack information (Las-
tAckReceived) received by the primary from the backup
via the HB on the serial link will indicate that the backup is
behind. However, this does not work if the primary NIC has
failed. If the primary NIC is down, the client will not receive
any bytes from the server and thus not send any acks.
We have added another mechanism in the new version of
ST-TCP to address this case. When the servers detect a failure
of the HB on the IP link but not on the serial link, both the
primary and the backup send ping requests to their gateway.
The results of these requests - that is, if they succeeded or not
- are exchanged in the HB via the serial link. If ping requests
continue to fail for the primary but succeed for the backup, the
backup takes over the TCP connection and shuts the primary
down.
Temporary local network failures. Temporary failures in
the NIC or the IP stack (e.g. buffer overﬂow) can lead to pack-
ets being dropped. HB stays up on both the links in this case.
If the packets are dropped at the backup, the backup requests
the primary for the missing bytes. There may be cases where
the backup takes a long time to catch up or is unable to catch
up. If the additional receive buffer space at the primary ﬁlls
up, the primary considers the backup failed and runs in non
fault-tolerant mode. Note that temporary network failures at
the primary are not an issue since these will be taken care of
in the normal course of TCP operation – the primary does not
ack these bytes and therefore the client will retransmit.
If the primary crashes while the backup is retrieving missed
bytes from it, the backup has no way of obtaining these bytes,
since primary has already acked them. For critical applica-
tions, a logger can be added to the system to address this
output commit problem as described in [2]; for other appli-
cations, ST-TCP treats this failure as unrecoverable.
5 Planned Demonstrations
We present ﬁve experiments that we plan to demon-
strate live at the symposium. These experiments are de-
signed to demonstrate different aspects of ST-TCP: (1) Client-
transparent, seamless failover to the backup server when the
primary fails; (2) Dependence of failover time on HB fre-
quency; (3) Insigniﬁcant overhead of ST-TCP during failure-
free operation; (4) Failure detection and recovery in case of
application crash failures; and (5) Failure detection and re-
covery in case of NIC failures.
Experimental Setup. Figure 2 shows the experimental
setup used for the experiments. An Ethernet switch is used
to connect the primary and the backup. We also have the
client directly connected to the same switch. The primary and
backup are installed with a modiﬁed Linux kernel, incorporat-
ing changes required to support ST-TCP. Virtual NICs are cre-
ated using the IP aliasing feature of the Linux kernel on both
the primary and backup machines. These VNICs are assigned
serviceIP IP address which is the address that the clients con-
nect to for receiving service. The primary and backup are
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
5
also associated with a multicast Ethernet address – multiEA.
There is a static ARP protocol entry created on the gateway
(the client in this case) mapping serviceIP to multiEA. Any
IP packet destined for serviceIP is sent to the multiEA Eth-
ernet address which allows both the primary and the backup
to receive all packets sent to the serviceIP address. The HB
is exchanged between the servers over the IP link. A dupli-
cate copy of the HB is exchanged over a secondary link as
described in Section 3.
Client
ARP entry (static):
ServiceIP−>MultiEA
ClientIP
ServiceIP, MultiEA
BackupIP
Backup
Ethernet
Switch
HB
HB
Secondary Link
ServiceIP, MultiEA
PrimaryIP
P
rimary
Figure 2: Experimental setup.
Demo 1: Client-Transparent Seamless Failover. The
goal of this demonstration is to show that in the event of the
primary server failure, ST-TCP provides a client-transparent,
fast and seamless failover to the backup. A GUI client-server
application is used in this demonstration. The client continu-
ally requests and receives data from the server. As the client
receives the data, it dynamically updates a pie chart reﬂecting
the percentage of the data already received. While the transfer
is in progress, the primary server is crashed so that the TCP
connection fails over to the backup. This failover process is
seamless to the client as is apparent by observing the progres-
sion of the pie chart.
This demonstration also shows that in the absence of ST-
TCP, even if a hot backup is available, the failure of the server
would lead to a disruption in the service and the client would
have to re-connect, unlike in ST-TCP where the failure at
worst appears as a glitch to the user.
Demo 2: Dependence of Failover Time on HB Fre-
quency. In this experiment, we examine the dependence of
the failover time on the HB frequency. One constituent of the
failover time is the failure detection time. The other depends
on how much the backup and the client TCP have backed off
during the time it took to detect the failure. Recall that TCP
backs off exponentially as retransmissions fail; if the primary
fails, both the backup and the client (assuming both the server
and the client are sending data) would start retransmitting and
backing off (in case of backup, since it has not taken over
the connection yet, the retransmissions get discarded). Once
the failure is detected and the backup takes over, there is still
a delay until the next client or backup retransmission before
the TCP stream gets re-started. In this demonstration, we try
three different values of HB period (200ms, 500ms and 1s)
and measure the failover times in each case.
Demo 3: Insigniﬁcant Overhead during Normal Oper-
ation. In this demonstration, a large ﬁle (about 100 MB) is
transferred to the client both with ST-TCP enabled and with
ST-TCP disabled. We compare the time taken for the ﬁle
transfer in both of these cases. The aim of this experiment
is to show that under normal operation (no failures), the over-
head of using ST-TCP is negligible.
Demo 4: Application Crash Failure. The goal of this
demonstration is to show that ST-TCP tolerates application
failures. The GUI application, used in Demo 1 above, is used
here as well. Two different scenarios of application failures
are simulated. In the ﬁrst scenario, the application on the pri-
mary crashes but the socket is not closed, and hence a FIN
segment is not generated.
In the second scenario, the OS
cleans up the application and closes the socket, thus, gener-
ating a FIN. In both of these scenarios, the application failure
is detected and the TCP connection is migrated to the backup.
Demo 5: NIC Failure. This demonstration has two parts.
In the ﬁrst part, we simulate a failure of the NIC at the pri-
mary; in the second, that of the backup. In both these cases
the HB on the IP link between the servers fails, but the one on
the secondary link stays up. The servers use the information
in the HB to determine whether the failure has occurred at the
primary or at the backup.
6 Conclusions
TCP is the most popular transport-level protocol for con-
structing distributed applications over the Internet. Current
fault tolerance techniques typically require software updates
both at the client and the server. This limits the applicabil-
ity of these techniques. Recently, several research projects
have addressed this problem by providing server fault toler-
ance support at the TCP layer. ST-TCP is one such effort to
address this problem.
This paper describes lessons learned from using ST-TCP
under different computing environments. In particular, the pa-
per reports on three issues. First, it reports on peculiar behav-
ior of servers under speciﬁc computing conditions and dis-
cusses design enhancements that ST-TCP has undergone to
address them. Second, the paper discusses in detail how ST-
TCP addresses different failure scenarios, particularly appli-
cation failures and local network failures. Finally, the paper
describes ﬁve experiments that will be demonstrated at the
conference.
References
[1] R. R. Koch, S. Hortikar, L. E. Moser, and P. M. Melliar-Smith. Transparent TCP
connection failover. In Proceedings of the IEEE Int. Conf. on Dependable Systems
and Networks, San Francisco, June 2003.
[2] M. Marwah, S. Mishra, and C. Fetzer. TCP server fault tolerance using connection
migration to a backup server. In Proceedings of IEEE Int. Conf. on Dependable
Systems and Networks, San Francisco, June 2003.
[3] M. Orgiyan and C. Fetzer. Tapping TCP streams.
In Proceedings of the IEEE
International Symposium on Network Computing and Applications, February 2002.
[4] D. A. Patterson. A simple way to estimate the cost of downtime. In Proceedings of
LISA ’02: Sixteenth Systems Administration Conference, November 2002.
[5] A. C. Snoeren, D. G. Andersen, and H. Balakrishnan. Fine-grained failover using
connection migration. In Proceedings of the 3rd USENIX Symposium on Internet
Technologies and Systems, March 2001.
[6] F. Sultan, K. Srinivasan, D. Iyer, and L. Iftode. Migratory TCP: Connection mi-
gration for service continuity over the internet. In Proceedings of the 22th IEEE
International Conference on Distributed Computing Systems, Vienna, Austria, July
2002.
[7] D. Zagorodnov, K. Marzullo, L. Alvisi, and T. Bressoud. Engineering fault tolerant
TCP/IP services using FT-TCP. In Proceedings of IEEE Int. Conf. on Dependable
Systems and Networks, San Francisco, June 2003.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
6