pervised method for all 15 endpoints in our experimental 15 97 95 98 100
average 85.5 95.3 94.6 97.9 97.0
testbed across 5 different scenarios are shown in Table II. We
compute the accuracy in the following way. Let the number of
injected anomaly events be denoted with 7) and the number
of accurately detected anomalies be Ta, then the accuracy samples from normal distribution with different mean, additive
over the injected anomalies is computed as accuracy = outlier, mean shift, step and decrement, incremental, temporary
The number of injected anomalies depends on each scenario change and gradual. Some of the types of anomalies can be
and endpoint. The missing values in Table II mean that the found in Figure 6.
anomaly did not affect those endpoints. The model is trained on each of the three endpoints and for
We note that the accuracy in normal system scenario is each of the endpoints we compute the accuracy of detection for
greater than 99% due to the tolerance module. We show each of the patterns injected in the time series. To test the ro­
scenario 5 and 6 graphically in Figure 7. We show that bustness of the algorithm, we evaluated it for several different
the method successfully flags almost all anomalous events. augmentations of the original patterns, including translation,
Overall, the results shown in the table and in the figure indicate increasing the response time (e.g., RTi = 0.2 means setting
that the combination of generative models like variational the response time of event to 0.2 of the maximum value)
autoencoder with GRU units that extract temporal information and change of the size of anomaly (e.g., in gradual increase,
achieves solid results. size = 10 means that the increase from amplitude A to
2) Results: production cloud data: Due to the low number amplitude B is gradual over 10 events/data points). In Table
of production-system errors, we injected several types of III, we show the aggregated results. The table summarizes the
anomalies. We defined seven types of common anomalies: different anomaly patterns and the needed minimum values
248
7 - * normal (cid:1) normal ^ ¥ • ya .
• anomaly • 0.4 • anomaly
emit
esnopser
i- *
0 2000 4000 6000 8000 10000 12000 0 2000 4000 6000 8000 10000 12000
event number event number
(a) (b)
Fig. 8. Example of successfully detected anomalies injected in {host}/vl/{p_id}/cs/limits. Gradual and mean shift anomalies are injected in (a) and (b)
respectively.
for the parameters size and RTi which lead to a detection TABLE V
of the corresponding anomaly type. Further, in Figure 8a and Performance evaluation in test-time prediction
8b we visually illustrate some of the patterns detected in the
#windows ms/window
series. We notice that the algorithm successfully detects the 1500 0.22
three types of anomalies injected. 1000 0.28
500 0.53
100 1.25
tabl e m
Results for Variational Recurrent Model
Pattern name Parameters time is very important. Having large amounts of traces and
additive outlier RTi > 0.25 events generated in short period of time, requires fast predic­
normal_Mean RTi > 0.2
tion time and timely detection of anomalies. For that reason,
temporary change RTi > 0.25
gradual RTi > 0.3, size > 10 we evaluate the performance of the approach. We show the
mean shift RTi > 0.2 results in Tables IV and V. Lastly, imposing industrial require­
step and decrement RTi > 0.3, size > 10
ments (prediction time  0.3, size > 10
performance proves that the approach is fast enough to be
3) Results: faulty pattern classification: The module can be used in production setting. In streaming test-time prediction
evaluated separately from the rest of the solution, since data we achieve performance of 6.64ms per predicted window of
and predefined patterns as described. The dataset consists of points. Of course, the prediction times can differ with reducing
15 different types of patterns similar to the ones shown in or expanding the window size, but it is still within the limits
Figure 6. In practice, the user has the option to define own of the necessary requirements.
patterns.
Furthermore, besides the patterns shown in Figure 6, we VI. Co nc l usio n and Fut ur e Work
produce augmentations to enrich the dataset. The augmenta­ This paper deals with an important and growing challenge:
tions are produced by using: horizontal shifts, adding small the automation of operation and maintenance tasks of planet-
amount of noise and amplitude shifts. scale IT infrastructures. We experimentally demonstrate the
We evaluated the algorithm to see the performance and its advantages of combining GRUs (simplified LSTMs) with
limits when it comes to the level of noise in the signal and variational autoencoders (AEVB) - two deep learning models
the accuracy of classification. We achieved 100% accuracy in - for learning multiple, complex data distributions underlying
data with no additional noise added, 80% and 48% accuracy time series data generated by distributed tracing systems. Our
when Gaussian noise was added with a = [0.05 and 0.1] re­ investigation on experimental and real-world production data
spectively. The convolutional neural network model accurately with artificially injected anomalies showed that our approach
classifies the tested anomaly patterns, with expected lower reaches accuracy greater than 90%, prediction time lower than
accuracy obtained in noisy patterns. 10ms, and robust classification of detected anomalies. The
tracing data was generated by an experimental microservice
TABLE IV application and by a planet-scale cloud infrastructure. These
Performance evaluation in training high levels of accuracy open a new door in the field of AIOps.
#windows ms/window Namely, the approach can be extended to also consider the
60000 0.29 structure of distributed traces, use knowledge from cross-event
27000 0.28 and cross-trace relations, and analyze structural trace anoma­
9000 0.53
lies using complete trace information. These achievements
1000 1.25
can ultimately support the development of zero-touch AIOps
4) Performance evaluation: In real-world production sys­ solutions for the automated detection, root-cause analysis and
tems, the performance of the model in training and prediction remediation of IT infrastructures.
249
References [18] M. V. Joshi, R. C. Agarwal, and V. Kumar, “Predicting rare classes: Can
boosting make any weak learner strong?” in Proceedings of the eighth
ACM S1GKDD international conference on Knowledge discovery and
[1] F. Schmidt, A. Gulenko, M. Wallschlger, A. Acker, V. Hennig, F. Liu,
data mining. ACM, 2002, pp. 297-306.
and O. Kao, ‘Tftm - unsupervised anomaly detection for virtualized
network function services,” in 2018 IEEE International Conference on [19] N. V. Chawla, N. Japkowicz, and A. Kotez, “Special issue on learning
from imbalanced data sets,” ACM Sigkdd Explorations Newsletter, vol. 6,
Web Services (ICWS), July 2018, pp, 187-194.
no. 1, pp. 1-6, 2004.
[2] A. Gulenko, F. Schmidt, A. Acker, M. Wallschlager, O. Kao,
[20] H. Fichtenberger, M. Gille, M. Schmidt, C. Schwiegelshohn, and
and F. Liu, “Detecting anomalous behavior of black-box
C. Sohler, “Bico: Birch meets coresets for k-means clustering,” in
services modeled with distance-based online clustering,” in
2018 IEEE 11th International Conference on Cloud Computing European Symposium on Algorithms. Springer, 2013, pp. 481-492.
(CLOUD), vol. 00, Jul 2018, pp. 912-915. [Online], Available: [21] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman,
doi.ieeecomputersociety.org/10.1109/CLOUD.2018.00134 and A. Y. Wu, “An efficient k-means clustering algorithm: Analysis and
implementation,” IEEE Transactions on Pattern Analysis & Machine
[3] B. H. Sigelman, L. A. Barroso, M. Burrows, P. Stephenson, M. Plakal,
Intelligence, no. 7, pp. 881-892, 2002.
D. Beaver, S. Jaspan, and C. Shanbhag, “Dapper, a large-scale distributed
systems tracing infrastructure,” Google, Inc., Tech. Rep., 2010. [Online]. [22] F. T. Liu, K. M. Ting, and Z.-H. Zhou, “Isolation forest,” in 2008 Eighth
Available: https://research.google.com/archive/papers/dapper-2010-1 .pdf IEEE International Conference on Data Mining. IEEE, 2008, pp. 413-
422.
[4] J. Kaldor, J. Mace, M. Bejda, E. Gao, W. Kuropatwa, J. O’Neill, K. W.
Ong, B. Schaller, P. Shan, B. Viscomi et ai, “Canopy: an end-to-end [23] L. M. Manevitz and M. Yousef, “One-class svms for document clas-
performance tracing and analysis system,” in Proceedings of the 26th sification,” Journal of machine Learning research, vol. 2, no. Dec, pp.
Symposium on Operating Systems Principles. ACM, 2017, pp. 34—50. 139-154, 2001.
[5] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, and I. Stoica, [24] O. Vallis, J. Hochenbaum, and A. Kejariwal, “A novel
“X-trace: A pervasive network tracing framework,” in Proceedings technique for long-term anomaly detection in the cloud,” in
of the 4th USENIX Conference on Networked Systems Design Proceedings of the 6th USENIX Conference on Hot Topics
&#38; Implementation, ser. NSDI’07. Berkeley, CA, USA: in Cloud Computing, ser. HotCloud’14. Berkeley, CA, USA:
USENIX Association, 2007, pp. 20-20. [Online]. Available: USENIX Association, 2014, pp. 15-15. [Online]. Available:
http://dl.acm.org/citation.cfm?id=1973430.1973450 http://dl.acm.org/citation.cfm?id=2696535.2696550
[6] P. Reynolds, J. L. Wiener, J. C. Mogul, M. K. Aguilera, and A. Vahdat, [25] P. Malhotra, L. Vig, G. Shroff, and P. Agarwal, “Long short term
“Wap5: black-box performance debugging for wide-area systems,” in memory networks for anomaly detection in time series,” in Proceedings.
Proceedings of the 15th international conference on World Wide Web. Presses universitaires de Louvain, 2015, p. 89.
ACM, 2006, pp. 347-356. [26] H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao, D. Pei,
[7] P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. A. Maltz, and Y. Feng et al., “Unsupervised anomaly detection via variational auto-
M. Zhang, “Towards highly reliable enterprise network services via encoder for seasonal kpis in web applications,” in Proceedings of the
inference of multi-level dependencies,” in ACM S1GCOMM Computer 2018 World Wide Web Conference on World Wide Web. International
Communication Review, vol. 37, no. 4. ACM, 2007, pp. 13-24. World Wide Web Conferences Steering Committee, 2018, pp. 187-196.
[8] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and A. Muthi- [27] K. Hundman, V. Constantinou, C. Laporte, I. Colwell, and
tacharoen, “Performance debugging for distributed systems of black T. Soderstrom, “Detecting spacecraft anomalies using lstms and
boxes,” ACM SIGOPS Operating Systems Review, vol. 37, no. 5, pp. nonparametric dynamic thresholding,” in Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery; Data
74-89, 2003.
Mining, ser. KDD ’18. New York, NY, USA: ACM, 2018, pp. 387-395.
[9] T. Gschwind, K. Eshghi, P. K. Garg, and K. Wurster, “Webmon: A
performance profiler for web transactions,” in Advanced Issues of E- [Online], Available: http://doi.acm.org/10.1145/3219819.3219845
Commerce and Web-Based Information Systems, 2002.(WECWIS 2002). [28] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,
Proceedings. Fourth IEEE International Workshop on. IEEE, 2002, pp. 2016, http://www.deepleamingbook.org.
171-176. [29] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in
[10] P. Barham, R. Isaacs, and D. Narayanan, “Magpie: International Conference on Learning Representations (ICLR), 2014.
online modelling and performance-aware systems,” in 9th [30] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning represen-
Workshop on Hot Topics in Operating Systems (HotOS- tations by back-propagating errors,” Nature, vol. 323, no. 6088, p. 533,
IX). USENIX, May 2003, pp. 85-90. [Online], Avail- 1986.
able: https://www.microsoft.com/en-us/research/publication/magpie- [31] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
online-modelling-and-performance-aware-sy stems/ computation, vol. 9, no. 8, pp. 1735-1780, 1997.
[11] M. Du, F. Li, G. Zheng, and V. Srikumar, “Deeplog: Anomaly detection [32] K. Cho, B. Van Merrienboer, D. Bahdanau, and Y. Bengio, “On the
and diagnosis from system logs through deep learning,” in Proceedings properties of neural machine translation: Encoder-decoder approaches,”
of the 2017 ACM SIGSAC Conference on Computer and Communica Eighth Workshop on Syntax, Semantics and Structure in Statistical
tions Security. ACM, 2017, pp. 1285-1298. Translation (SSST-8), 2014.
[12] F. Bezerra and J. Wainer, “Algorithms for anomaly detection of traces [33] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio,
in logs of process aware information systems,” Information Systems, “A recurrent latent variable model for sequential data,” in Advances in
vol. 38, no. 1, pp. 33-44, 2013. neural information processing systems, 2015, pp. 2980-2988.
[13] A. Brown, A. Thor, B. Hutchinson, and N. Nichols, “Recurrent neural [34] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep
network attention mechanisms for interpretable system log anomaly network training by reducing internal covariate shift,” in Proceedings
detection,” arXiv preprint arXiv:1803.04967, 2018. of the 32Nd International Conference on International Conference on
[14] Mining Invariants from Console Logs for System Prob Machine Learning - Volume 37, ser. ICML’15. JMLR.org, 2015, pp.
lem Detection. USENIX, June 2010. [Online]. Avail- 448-456.
able: https://www.microsoft.com/en-us/research/publication/mining- [35] A. Nuttall, “Some windows with very good sidelobe behavior,” IEEE
invariants-from-console-logs-for-system-problem-detection/ Transactions on Acoustics, Speech, and Signal Processing, vol. 29, no. 1,
[15] D. Battre, O. Kao, and D. Wameke, “Evaluation of network topology pp. 84-91, February 1981.
inference in opaque compute clouds through end-to-end measurements,” [36] Y. LeCun, Y. Bengio et al., “Convolutional networks for images, speech,
in 2011 IEEE 4th International Conference on Cloud Computing, July and time series,” The handbook of brain theory and neural networks,
2011, pp. 17-24. vol. 3361, no. 10, p. 1995, 1995.
[16] L. Breiman, “Random forests,” Machine Learning, vol. 45, [37] B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu, “Convolutional neural
no. 1, pp. 5-32, Oct 2001. [Online]. Available: networks for time series classification,” Journal of Systems Engineering
https://doi.Org/10.1023/A:1010933404324 and Electronics, vol. 28, no. 1, pp. 162-169, 2017.
[17] M. Joshi, R. Agarwal, and V. Kumar, “Mining needle in a haystack: [38] F. Chollet et al., “Keras,” https://keras.io, 2018.
classifying rare classes via two-phase rule induction,” ACM SIGMOD [39] OpenZipkin, “openzipkin/zipkin,” 2018. [Online]. Available:
Record, vol. 30, no. 2, pp. 91-102, 2001. https://github.com/openzipkin/zipkin
250