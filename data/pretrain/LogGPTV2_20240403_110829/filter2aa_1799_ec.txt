attempting to read data from files before an application, a driver, or a system thread explicitly requests 
it. The read-ahead thread uses the history of read operations that were performed on a file, which 
are stored in a file object’s private cache map, to determine how much data to read. When the thread 
performs a read-ahead, it simply maps the portion of the file it wants to read into the cache (allocating 
VACBs as necessary) and touches the mapped data. The page faults caused by the memory accesses 
invoke the page fault handler, which reads the pages into the system’s working set.
A limitation of the read-ahead thread is that it works only on open files. Superfetch was added to 
Windows to proactively add files to the cache before they’re even opened. Specifically, the memory 
manager sends page-usage information to the Superfetch service (%SystemRoot%\System32\Sysmain.
dll), and a file system minifilter provides file name resolution data. The Superfetch service attempts 
to find file-usage patterns—for example, payroll is run every Friday at 12:00, or Outlook is run every 
morning at 8:00. When these patterns are derived, the information is stored in a database and tim-
ers are requested. Just prior to the time the file would most likely be used, a timer fires and tells the 
memory manager to read the file into low-priority memory (using low-priority disk I/O). If the file is 
then opened, the data is already in memory, and there’s no need to wait for the data to be read from 
disk. If the file isn’t opened, the low-priority memory will be reclaimed by the system. The internals and 
full description of the Superfetch service were previously described in Chapter 5, Part 1.
Memory manager’s page fault handler
We described how the page fault handler is used in the context of explicit file I/O and cache manager 
read-ahead, but it’s also invoked whenever any application accesses virtual memory that is a view of 
a mapped file and encounters pages that represent portions of a file that aren’t yet in memory. The 
memory manager’s MmAccessFault handler follows the same steps it does when the cache manager 
generates a page fault from CcCopyRead or CcCopyWrite, sending IRPs via IoPageRead to the file sys-
tem on which the file is stored.
File system filter drivers and minifilters
A filter driver that layers over a file system driver is called a file system filter driver. Two types of file 
system filter drivers are supported by the Windows I/O model:
I 
Legacy file system filter drivers usually create one or multiple device objects and attach them
on the file system device through the IoAttachDeviceToDeviceStack API. Legacy filter drivers
intercept all the requests coming from the cache manager or I/O manager and must implement
both standard IRP dispatch functions and the Fast I/O path. Due to the complexity involved in
the development of this kind of driver (synchronization issues, undocumented interfaces, de-
pendency on the original file system, and so on), Microsoft has developed a unified filter model
that makes use of special drivers, called minifilters, and deprecated legacy file system drivers.
(The IoAttachDeviceToDeviceStack API fails when it’s called for DAX volumes).
624
CHAPTER 11
Caching and file systems
I 
Minifilters drivers are clients of the Filesystem Filter Manager (Fltmgr.sys). The Filesystem Filter
Manager is a legacy file system filter driver that provides a rich and documented interface for the
creation of file system filters, hiding the complexity behind all the interactions between the file
system drivers and the cache manager. Minifilters register with the filter manager through the
FltRegisterFilter API. The caller usually specifies an instance setup routine and different operation
callbacks. The instance setup is called by the filter manager for every valid volume device that a 
file system manages. The minifilter has the chance to decide whether to attach to the volume.
Minifilters can specify a Pre and Post operation callback for every major IRP function code, as well
as certain “pseudo-operations” that describe internal memory manager or cache manager se-
mantics that are relevant to file system access patterns. The Pre callback is executed before the I/O
is processed by the file system driver, whereas the Post callback is executed after the I/O operation
has been completed. The Filter Manager also provides its own communication facility that can be
employed between minifilter drivers and their associated user-mode application.
The ability to see all file system requests and optionally modify or complete them enables a range 
of applications, including remote file replication services, file encryption, efficient backup, and licens-
ing. Every anti-malware product typically includes at least a minifilter driver that intercepts applications 
opening or modifying files. For example, before propagating the IRP to the file system driver to which 
the command is directed, a malware scanner examines the file being opened to ensure that it’s clean. 
If the file is clean, the malware scanner passes the IRP on, but if the file is infected, the malware scan-
ner quarantines or cleans the file. If the file can’t be cleaned, the driver fails the IRP (typically with an 
access-denied error) so that the malware cannot become active.
Deeply describing the entire minifilter and legacy filter driver architecture is outside the scope 
of this chapter. You can find more information on the legacy filter driver architecture in Chapter 6, 
“I/O System,” of Part 1. More details on minifilters are available in MSDN (https://docs.microsoft.com 
/en-us/windows-hardware/drivers/ifs/file-system-minifilter-drivers).
Data-scan sections
Starting with Windows 8.1, the Filter Manager collaborates with file system drivers to provide data-scan 
section objects that can be used by anti-malware products. Data-scan section objects are similar to 
standard section objects (for more information about section objects, see Chapter 5 of Part 1) except 
for the following:
I 
Data-scan section objects can be created from minifilter callback functions, namely from call-
backs that manage the IRP_M_CREATE function code. These callbacks are called by the filter
manager when an application is opening or creating a file. An anti-malware scanner can create
a data-scan section and then start scanning before completing the callback.
I 
FltCreateSectionForDataScan, the API used for creating data-scan sections, accepts a FILE_
OBECT pointer. This means that callers don’t need to provide a file handle. The file handle
typically doesn’t yet exist, and would thus need to be (re)created by using FltCreateFile API,
which would then have created other file creation IRPs, recursively interacting with lower level
file system filters once again. With the new API, the process is much faster because these extra
recursive calls won’t be generated.
CHAPTER 11
Caching and file systems
625
A data-scan section can be mapped like a normal section using the traditional API. This allows anti-
malware applications to implement their scan engine either as a user-mode application or in a kernel-
mode driver. When the data-scan section is mapped, IRP_M_READ events are still generated in the mini-
filter driver, but this is not a problem because the minifilter doesn’t have to include a read callback at all. 
Filtering named pipes and mailslots
When a process belonging to a user application needs to communicate with another entity (a pro-
cess, kernel driver, or remote application), it can leverage facilities provided by the operating system. 
The most traditionally used are named pipes and mailslots, because they are portable among other 
operating systems as well. A named pipe is a named, one-way communication channel between a pipe 
server and one or more pipe clients. All instances of a named pipe share the same pipe name, but each 
instance has its own buffers and handles, and provides a separate channel for client/server communi-
cation. Named pipes are implemented through a file system driver, the NPFS driver (Npfs.sys).
A mailslot is a multi-way communication channel between a mailslot server and one or more clients. 
A mailslot server is a process that creates a mailslot through the CreateMailslot Win32 API, and can only 
read small messages (424 bytes maximum when sent between remote computers) generated by one or 
more clients. Clients are processes that write messages to the mailslot. Clients connect to the mailslot 
through the standard CreateFile API and send messages through the WriteFile function. Mailslots are 
generally used for broadcasting messages within a domain. If several server processes in a domain each 
create a mailslot using the same name, every message that is addressed to that mailslot and sent to the 
domain is received by the participating processes. Mailslots are implemented through the Mailslot file 
system driver, Msfs.sys.
Both the mailslot and NPFS driver implement simple file systems. They manage namespaces com-
posed of files and directories, which support security, can be opened, closed, read, written, and so on. 
Describing the implementation of the two drivers is outside the scope of this chapter. 
Starting with Windows 8, mailslots and named pipes are supported by the Filter Manager. Minifilters 
are able to attach to the mailslot and named pipe volumes (\Device\NamedPipe and \Device\Mailslot, 
which are not real volumes), through the FLTFL_REGISTRATION_SUPPORT_NPFS_MSFS flag specified 
at registration time. A minifilter can then intercept and modify all the named pipe and mailslot I/O 
that happens between local and remote process and between a user application and its kernel driver. 
Furthermore, minifilters can open or create a named pipe or mailslot without generating recursive 
events through the FltCreateNamedPipeFile or FltCreateMailslotFile APIs.
Note One of the motivations that explains why the named pipe and mailslot file system 
drivers are simpler compared to NTFS and ReFs is that they do not interact heavily with 
the cache manager. The named pipe driver implements the Fast I/O path but with no 
cached read or write-behind support. The mailslot driver does not interact with the cache 
manager at all.
626
CHAPTER 11
Caching and file systems
Controlling reparse point behavior
The NTFS file system supports the concept of reparse points, blocks of 16 KB of application and system-
defined reparse data that can be associated to single files. (Reparse points are discussed more in mul-
tiple sections later in this chapter.) Some types of reparse points, like volume mount points or symbolic 
links, contain a link between the original file (or an empty directory), used as a placeholder, and an-
other file, which can even be located in another volume. When the NTFS file system driver encounters 
a reparse point on its path, it returns an error code to the upper driver in the device stack. The latter 
(which could be another filter driver) analyzes the reparse point content and, in the case of a symbolic 
link, re-emits another I/O to the correct volume device.
This process is complex and cumbersome for any filter driver. Minifilters drivers can intercept the 
STATUS_REPARSE error code and reopen the reparse point through the new FltCreateFileEx2 API, 
which accepts a list of Extra Create Parameters (also known as ECPs), used to fine-tune the behavior 
of the opening/creation process of a target file in the minifilter context. In general, the Filter Manager 
supports different ECPs, and each of them is uniquely identified by a GUID. The Filter Manager pro-
vides multiple documented APIs that deal with ECPs and ECP lists. Usually, minifilters allocate an 
ECP with the FltAllocateExtraCreateParameter function, populate it, and insert it into a list (through 
FltInsertExtraCreateParameter) before calling the Filter Manager’s I/O APIs.
The FLT_CREATEFILE_TARGET extra creation parameter allows the Filter Manager to manage cross-
volume file creation automatically (the caller needs to specify a flag). Minifilters don’t need to perform 
any other complex operation.
With the goal of supporting container isolation, it’s also possible to set a reparse point on nonempty 
directories and, in order to support container isolation, create new files that have directory reparse 
points. The default behavior that the file system has while encountering a nonempty directory reparse 
point depends on whether the reparse point is applied in the last component of the file full path. If this 
is the case, the file system returns the STATUS_REPARSE error code, just like for an empty directory; 
otherwise, it continues to walk the path.
The Filter Manager is able to correctly deal with this new kind of reparse point through another ECP 
(named TYPE_OPEN_REPARSE). The ECP includes a list of descriptors (OPEN_REPARSE_LIST_ ENTRY 
data structure), each of which describes the type of reparse point (through its Reparse Tag), and the 
behavior that the system should apply when it encounters a reparse point of that type while parsing 
a path. Minifilters, after they have correctly initialized the descriptor list, can apply the new behavior in 
different ways:
I 
Issue a new open (or create) operation on a file that resides in a path that includes a reparse
point in any of its components, using the FltCreateFileEx2 function. This procedure is similar to
the one used by the FLT_CREATEFILE_TARGET ECP.
I 
Apply the new reparse point behavior globally to any file that the Pre-Create callback inter-
cepts. The FltAddOpenReparseEntry and FltRemoveOpenReparseEntry APIs can be used to set
the reparse point behavior to a target file before the file is actually created (the pre-creation
callback intercepts the file creation request before the file is created). The Windows Container
Isolation minifilter driver (Wcifs.sys) uses this strategy.
CHAPTER 11
Caching and file systems
627
Process Monitor
Process Monitor (Procmon), a system activity-monitoring utility from Sysinternals that has been used 
throughout this book, is an example of a passive minifilter driver, which is one that does not modify the 
flow of IRPs between applications and file system drivers. 
Process Monitor works by extracting a file system minifilter device driver from its executable image 
(stored as a resource inside Procmon.exe) the first time you run it after a boot, installing the driver in 
memory, and then deleting the driver image from disk (unless configured for persistent boot-time 
monitoring). Through the Process Monitor GUI, you can direct the driver to monitor file system activity 
on local volumes that have assigned drive letters, network shares, named pipes, and mail slots. When 
the driver receives a command to start monitoring a volume, it registers filtering callbacks with the 
Filter Manager, which is attached to the device object that represents a mounted file system on the 
volume. After an attach operation, the I/O manager redirects an IRP targeted at the underlying device 
object to the driver owning the attached device, in this case the Filter Manager, which sends the event 
to registered minifilter drivers, in this case Process Monitor.
When the Process Monitor driver intercepts an IRP, it records information about the IRP’s com-
mand, including target file name and other parameters specific to the command (such as read and 
write lengths and offsets) to a nonpaged kernel buffer. Every 500 milliseconds, the Process Monitor GUI 
program sends an IRP to Process Monitor’s interface device object, which requests a copy of the buf-
fer containing the latest activity, and then displays the activity in its output window. Process Monitor 
shows all file activity as it occurs, which makes it an ideal tool for troubleshooting file system–related 
system and application failures. To run Process Monitor the first time on a system, an account must 
have the Load Driver and Debug privileges. After loading, the driver remains resident, so subsequent 
executions require only the Debug privilege.
When you run Process Monitor, it starts in basic mode, which shows the file system activity most 
often useful for troubleshooting. When in basic mode, Process Monitor omits certain file system opera-
tions from being displayed, including
I 
I/O to NTFS metadata files
I 
I/O to the paging file
I 
I/O generated by the System process
I 
I/O generated by the Process Monitor process
While in basic mode, Process Monitor also reports file I/O operations with friendly names rather 
than with the IRP types used to represent them. For example, both IRP_M_WRITE and FASTIO_WRITE
operations display as WriteFile, and IRP_M_CREATE operations show as Open if they represent an open 
operation and as Create for the creation of new files.
628
CHAPTER 11
Caching and file systems
EXPERIMENT: Viewing Process Monitor’s minifilter driver
To see which file system minifilter drivers are loaded, start an Administrative command prompt, 
and run the Filter Manager control program (%SystemRoot%\System32\Fltmc.exe). Start Process 
Monitor (ProcMon.exe) and run Fltmc again. You see that the Process Monitor’s filter driver 
(PROCMON20) is loaded and has a nonzero value in the Instances column. Now, exit Process 
Monitor and run Fltmc again. This time, you see that the Process Monitor’s filter driver is still 
loaded, but now its instance count is zero.
The NT File System (NTFS)
In the following section, we analyze the internal architecture of the NTFS file system, starting by look-
ing at the requirements that drove its design. We examine the on-disk data structures, and then we 
move on to the advanced features provided by the NTFS file system, like the Recovery support, tiered 
volumes, and the Encrypting File System (EFS).
High-end file system requirements
From the start, NTFS was designed to include features required of an enterprise-class file system. To 
minimize data loss in the face of an unexpected system outage or crash, a file system must ensure that 
the integrity of its metadata is guaranteed at all times; and to protect sensitive data from unauthorized 
access, a file system must have an integrated security model. Finally, a file system must allow for soft-
ware-based data redundancy as a low-cost alternative to hardware-redundant solutions for protecting 
user data. In this section, you find out how NTFS implements each of these capabilities.
EXPERIMENT: Viewing Process Monitor’s minifilter driver
To see which file system minifilter drivers are loaded, start an Administrative command prompt, 
and run the Filter Manager control program (%SystemRoot%\System32\Fltmc.exe). Start Process 
Monitor (ProcMon.exe) and run Fltmc again. You see that the Process Monitor’s filter driver 
(PROCMON20) is loaded and has a nonzero value in the Instances column. Now, exit Process 
Monitor and run Fltmc again. This time, you see that the Process Monitor’s filter driver is still 
loaded, but now its instance count is zero.
CHAPTER 11
Caching and file systems
629
Recoverability
To address the requirement for reliable data storage and data access, NTFS provides file system recov-
ery based on the concept of an atomic transaction. Atomic transactions are a technique for handling 
modifications to a database so that system failures don’t affect the correctness or integrity of the 
database. The basic tenet of atomic transactions is that some database operations, called transactions,
are all-or-nothing propositions. (A transaction is defined as an I/O operation that alters file system data 
or changes the volume’s directory structure.) The separate disk updates that make up the transaction 
must be executed atomically—that is, once the transaction begins to execute, all its disk updates must 
be completed. If a system failure interrupts the transaction, the part that has been completed must be 
undone, or rolled back. The rollback operation returns the database to a previously known and consis-
tent state, as if the transaction had never occurred.
NTFS uses atomic transactions to implement its file system recovery feature. If a program initiates 
an I/O operation that alters the structure of an NTFS volume—that is, changes the directory structure, 
extends a file, allocates space for a new file, and so on—NTFS treats that operation as an atomic trans-
action. It guarantees that the transaction is either completed or, if the system fails while executing the 
transaction, rolled back. The details of how NTFS does this are explained in the section “NTFS recovery 
support” later in the chapter. In addition, NTFS uses redundant storage for vital file system information 
so that if a sector on the disk goes bad, NTFS can still access the volume’s critical file system data. 
Security
Security in NTFS is derived directly from the Windows object model. Files and directories are protected 
from being accessed by unauthorized users. (For more information on Windows security, see Chapter 
7, “Security,” in Part 1.) An open file is implemented as a file object with a security descriptor stored on 
disk in the hidden Secure metafile, in a stream named SDS (Security Descriptor Stream). Before a 
process can open a handle to any object, including a file object, the Windows security system verifies 
that the process has appropriate authorization to do so. The security descriptor, combined with the 
requirement that a user log on to the system and provide an identifying password, ensures that no pro-
cess can access a file unless it is given specific permission to do so by a system administrator or by the 
file’s owner. (For more information about security descriptors, see the section “Security descriptors and 
access control” in Chapter 7 in Part 1).
Data redundancy and fault tolerance
In addition to recoverability of file system data, some customers require that their data not be endan-
gered by a power outage or catastrophic disk failure. The NTFS recovery capabilities ensure that the 
file system on a volume remains accessible, but they make no guarantees for complete recovery of user 
files. Protection for applications that can’t risk losing file data is provided through data redundancy.
Data redundancy for user files is implemented via the Windows layered driver, which provides 
fault-tolerant disk support. NTFS communicates with a volume manager, which in turn communicates 
with a disk driver to write data to a disk. A volume manager can mirror, or duplicate, data from one disk 
onto another disk so that a redundant copy can always be retrieved. This support is commonly called 
630
CHAPTER 11
Caching and file systems
RAID level 1. Volume managers also allow data to be written in stripes across three or more disks, using 
the equivalent of one disk to maintain parity information. If the data on one disk is lost or becomes 
inaccessible, the driver can reconstruct the disk’s contents by means of exclusive-OR operations. This 
support is called RAID level 5.
In Windows 7, data redundancy for NTFS implemented via the Windows layered driver was provided 
by Dynamic Disks. Dynamic Disks had multiple limitations, which have been overcome in Windows 8.1 
by introducing a new technology that virtualizes the storage hardware, called Storage Spaces. Storage 
Spaces is able to create virtual disks that already provide data redundancy and fault tolerance. The 
volume manager doesn’t differentiate between a virtual disk and a real disk (so user mode components 
can’t see any difference between the two). The NTFS file system driver cooperates with Storage Spaces 
for supporting tiered disks and RAID virtual configurations. Storage Spaces and Spaces Direct will be 
covered later in this chapter.
Advanced features of NTFS
In addition to NTFS being recoverable, secure, reliable, and efficient for mission-critical systems, it 
includes the following advanced features that allow it to support a broad range of applications. Some 
of these features are exposed as APIs for applications to leverage, and others are internal features:
I 