title:Distillation Codes and Applications to DoS Resistant Multicast Authentication
author:Chris Karlof and
Naveen Sastry and
Yaping Li and
Adrian Perrig and
J. D. Tygar
and Applications to DoS Resistant Multicast Authentication
Distillation Codes
Chris Karlof
UC Berkeley
Naveen Sastry
UC Berkeley
Yaping Li
UC Berkeley
Adrian Perrig
CMU
J. D. Tygar
UC Berkeley
Abstract
We introduce distillation codes, a method for streaming
and storing data. Like erasure codes, distillation codes
allow information to be decoded from a sufﬁciently large
quorum of symbols. In contrast to erasure codes, distilla-
tion codes are robust against pollution attacks, a powerful
class of denial of service (DoS) attacks in which adver-
saries inject invalid symbols during the decoding process.
We examine applications of distillation codes to mul-
ticast authentication. Previous applications of erasure
codes to multicast authentication are vulnerable to low
bandwidth pollution attacks. We demonstrate pollution
attacks against previous approaches which prevent re-
ceivers from verifying any authentic packets. To resist pol-
lution attacks, we introduce Pollution Resistant Authenti-
cated Block Streams, which have low overhead and can
tolerate arbitrary patterns of packet loss within a block
up to a predetermined number of packets. In the face of
40Mb/s of attack trafﬁc, PRABS receivers successfully au-
thenticate the stream and consume only 10% of their CPU.
1. Introduction
Single-source multicast enables a sender to efﬁciently
disseminate digital media to a large audience, but to de-
fend against adversaries who inject bogus packets, re-
ceivers must verify the authenticity of packets. One ap-
proach to multicast authentication is signature amortiza-
tion. Signature amortization schemes divide the multicast
stream into blocks of sequential packets and authenticate
all the packets in a block with a single signature. Sig-
nature amortization is a compelling approach to multi-
cast authentication because it distributes the communica-
tion and computation overhead of a digital signature over
many packets.
One challenge in signature amortization schemes is ro-
bustness to packet loss. Receivers need the digital signa-
ture to verify the authenticity of the packets in the block,
but the best way to reliably transmit the signature requires
consideration. Including the signature in every packet is
robust to packet loss, but incurs high overhead. Includ-
ing a few bytes of the signature in each packet is space
efﬁcient, but is not robust to loss. Signature amortization
schemes differ mainly in their solution to this problem.
Three previous approaches are hash graphs [11, 18,
26, 33], the Wong-Lam scheme [35], and erasure codes
[21, 22]. All are vulnerable to denial of service attacks.
Hash graph protocols construct a directed graph over the
packets where each node in the graph contains the hash
values of the neighbors on its incoming edges. The hash
graph terminates with a signature packet, which authenti-
cates a handful of the nodes in the graph. If the signature
packet is not lost and there exists a path from a particu-
lar packet to the signature, the receiver authenticates the
packet by traversing the hash path to the digital signature
and verifying the signature.
The Wong-Lam [35] scheme constructs a Merkle hash
tree over the packets in the block and signs the root of the
tree. Each packet contains the signature and the nodes in
tree necessary to reconstruct the root. By including the
signature in every packet, each packet is individually veri-
ﬁable. Receivers authenticate each packet by reconstruct-
ing the root value of the tree and verifying the signature.
Hash graph protocols and the Wong-Lam scheme are
vulnerable to signature ﬂooding attacks. An adversary
ﬂooding the stream with invalid signatures will over-
whelm the computational resources of receivers attempt-
ing to verify the signatures. Additionally, in hash graph
protocols, adversarial loss patterns can cause denial of ser-
vice. For example, if an adversary causes the loss of all
signature packets, nothing is veriﬁable.
Several researchers advocate the use of erasure codes
[15, 16, 28, 29] for signature amortization [21, 22]. Era-
sure codes are a mechanism that allow receivers to decode
messages from a sufﬁciently large quorum of encoding
symbols. Erasure codes are robust to arbitrary patterns
of loss among the encoding symbols as long as the de-
coder receives a sufﬁciently large subset of them. Multi-
cast authentication protocols using erasure codes are ro-
bust to packet loss and have low overhead. However, era-
sure codes are designed to handle only a speciﬁc threat
model: packet loss. Erasure codes assume that symbols
are sometimes lost but not corrupted in transit; this is the
erasure channel model.
Unfortunately, the assumptions that underlie erasure
codes are unrealistic in hostile environments. Adversaries
can pollute the message stream by injecting invalid sym-
bols. We call this a pollution attack. If an erasure code
uses an invalid symbol as input to its decoding algorithm,
it will reconstruct invalid data. The communication model
that incorporates this more realistic threat is the polluted
erasure channel, in which valid symbols can be lost, and
an adversary can inject additional invalid symbols claim-
ing to be valid. Polluted erasure channels more accurately
model multicast environments: malicious end hosts and
routers can observe, inject, modify, delay, and drop mes-
sages in an erasure encoded multicast stream.
This paper introduces and gives efﬁcient constructions
of distillation codes, which are robust against pollution
attacks, signature ﬂooding, and adversarial loss patterns.
We make the following contributions:
• We introduce the notion of pollution to erasure chan-
nels, which allows us to more accurately model the
threats of multicast data dissemination. We also in-
troduce pollution attacks and demonstrate how low
bandwidth pollution attacks can cause denial of ser-
vice for erasure codes.
• We introduce distillation codes; we show that dis-
tillation codes function well in the polluted erasure
channel model, and prove that they are resistant to
pollution attacks.
• We use distillation codes to construct a new multicast
authentication protocol: Pollution Resistant Authen-
ticated Block Streams (PRABS). PRABS can toler-
ate arbitrary patterns of packet loss within a block
up to a predetermined number of packets and are re-
sistant to pollution attacks on receivers. Figure 1
compares PRABS to existing multicast authentica-
tion protocols.
• We present measurements of an implementation of
distillation codes and PRABS. These measurements
demonstrate the effectiveness of distillation codes
against pollution attacks.
Scheme
Hash graphs
[11, 18, 26, 33]
Wong-Lam [35]
SAIDA[23]
Pannetrat-
Molva[21]
PRABS
188
22
12
65
Overhead Denial of service
(bytes)
≈ 40–50
vulnerabilities
Signature ﬂooding
& adversarial loss
Signature ﬂooding
Pollution attacks
Pollution attacks
—
Table 1. Comparison of PRABS to existing
multicast authentication protocols.
The overhead was computed assuming 80 bit cryptographic
hashes, 128 byte RSA signatures, and 128 packet blocks. For
SAIDA and PRABS, we assume up to 64 packet losses per block.
For PRABS, we use the optimization described in Section 4.6.
Our scheme, PRABS, is resistant to pollution attacks, signature
ﬂooding, and adversarial loss patterns.
a space efﬁcient and computationally lightweight authen-
tication mechanism, especially if the receivers are embed-
ded devices. In unicast settings, symmetric key cryptog-
raphy can provide an efﬁcient solution to the authentica-
tion problem [5]. Naively extending such schemes to a
multicast setting by distributing the secret key to all the
receivers is not secure: any receiver can forge messages
using the secret key.
The goals and requirements of broadcast/multicast au-
thentication are as follows:
• Packet authenticity. Each receiver can verify that
packets originated from the sender and were unmod-
iﬁed in transit. Receivers must be able to distinguish
trafﬁc injected by other parties, including any of the
receivers.
• Packet loss robustness. Receivers can authenticate
each packet despite the loss of a ﬁxed fraction of the
total packets.
• Loss model independent. In addition to packet loss
robustness, receivers can verify packets even when
the loss is bursty, correlated, or in any other pattern.
• Denial of service (DoS) resistant. Receivers can re-
sist denial of service attacks against their resources.
2 Preliminaries
2.2. Erasure codes
2.1. Broadcast and multicast authentication
Disseminating information from a server in a broadcast
setting to multiple receivers demands a mechanism for
guaranteeing the authenticity of the data stream. We need
An erasure code [15, 16, 28, 29] is an encoder and de-
coder that use forward error correction to tolerate loss.
The encoder redundantly encodes information into a set
of symbols.
If the decoder receives sufﬁciently many
symbols, it can reconstruct the original information. An
(n, t) erasure encoder generates a set S of n symbols
{s1, s2, . . . , sn} from the input. The decoder can toler-
ate a loss of up to t packets, i.e., it can reconstruct the
original data given any n − t symbols from S.
Reed-Solomon, Tornado, and LT codes are examples
of erasure codes. Reed-Solomon [29] codes typically re-
quire O(n2) time to encode and decode; Tornado and
LT codes [15, 16] require O(n) time. Although Reed-
Solomon codes are slower, they have the advantage that
reconstructing the original data is guaranteed to be suc-
cessful if the decoder has at least n− t encoding symbols.
With Tornado and LT codes, reconstructing with at least
(n− t) · (1 + ) encoding symbols is successful with high
probability for  ≈ 0.05.
2.3. Terminology and assumptions
We assume there is a single party authorized to encode
and send messages. We refer to this party as the legitimate
encoder. If the legitimate encoder encodes and sends a
message D over the channel to the decoder, D is said to
be valid. If a message D was never encoded and sent by
the legitimate encoder over the channel, D is said to be
invalid.
Let S be a set of n symbols generated by an erasure en-
coder with input D. We assume n and the loss parameter t
are ﬁxed and known to the encoder and the decoder. Each
symbol is an ordered pair (si, i), 1 ≤ i ≤ n, so each sym-
bol contains its index value. The symbols in S are valid
symbols of D if this encoding process is executed by the
legitimate encoder; other ordered pairs are invalid sym-
bols. We are concerned about invalid symbols injected by
an adversary. Given a set of symbols which includes valid
symbols and possibly invalid symbols, the decoder pro-
duces a candidate reconstruction R. The reconstruction
R is valid when R = D for some valid D and invalid oth-
erwise. We assume erasure decoding with at least n − t
valid symbols of D and no other symbols will result in a
valid reconstruction of D.
We assume the encoder and decoder have access to
TAG(·) and VALIDATE(·) algorithms, respectively. The
TAG(·) algorithm augments its input with some addi-
tional information that enables the VALIDATE(·) algorithm
to verify its authenticity. For correctness, we require
VALIDATE(TAG(R)) = true for all R. To guarantee au-
thenticity, we assume it is difﬁcult for an adversary to
forge R such that VALIDATE(R) = true. We also assume
the existence of an algorithm STRIP(·) that strips off the
authentication information added by TAG(·). One possi-
ble instantiation of (TAG(·),VALIDATE(·)) is public key
signature generation and veriﬁcation. See Appendix A
for a formal treatment of our authenticity requirements for
(TAG(·),VALIDATE(·)).
To enable decoders to determine whether a candidate
reconstruction is valid, we will erasure encode TAG(D)
rather than D. Then given a candidate reconstruction R,
a decoder determines its authenticity by checking whether
VALIDATE(R) = true. We refer to the process of applying
VALIDATE(·) to a candidate reconstruction as reconstruc-
tion validation.
2.4. Pollution attacks: DoS against erasure codes
Adversaries can disrupt the decoding process by intro-
ducing invalid symbols.
If the decoder uses an invalid
symbol, it will generate an invalid reconstruction, causing
denial of service. We call this a pollution attack, and refer
to an erasure channel with pollution attacks as a polluted
erasure channel.
Decoders can easily recover from pollution attacks with
only a small number of invalid symbols and no lost valid
symbols. Since both valid and invalid symbols contain an
index, the decoder simply looks for duplicate indices and
drops both symbols. If at least n − t symbols remain af-
ter dropping duplicates, the decoder will recover the valid
reconstruction.
Recovery becomes more difﬁcult as the number of in-
valid symbols increases. For example, suppose the de-
coder receives only the ﬁrst n − t valid symbols and an
adversary injects one invalid symbol in each of those po-
sitions. The decoder cannot simply drop the duplicates
since no symbols will remain. Alternatively, the decoder
could select one symbol from each position, execute the
decoding algorithm, and apply VALIDATE(·) to verify the
authenticity of the candidate reconstruction. This ap-
proach is ill-fated: the decoder is successful only if it is
lucky enough to select the valid symbol in every position.
1
2n−t , and in the worst case, the
This event has probability
decoder will produce 2n−t candidate reconstructions be-
fore the valid one is found.
2.5. Threat model
We assume that an adversary can observe, inject, mod-
ify, delay, and drop trafﬁc in the channel between the
sender and receiver. An adversary could be a compro-
mised router on the path between the sender and receiver,
for example.
Denial of service attacks DoS attacks take many forms,
depending on the resource they are trying to exhaust. An
adversary can attack the sender, the network infrastruc-
ture between sender and receiver, and the receiver. In the
broadcast setting, the sender does not accept any data from
the network, so we will assume that the sender is not sus-
ceptible to DoS attacks. We also do not consider band-
width exhaustion attacks, as they are outside of the scope
of this paper. A receiver has little recourse if its last hop
router drops all its trafﬁc or thousands of zombie machines
ﬂood and overload its last hop link. To recover from these
attacks, receivers must rely on help from the infrastructure
to detect the problem and take appropriate action. Recent
research results address these challenges [2, 3, 10]. How-
ever, we must consider DoS attacks against the receiver’s
computation and storage resources. An attacker should
not be able to exhaust these resources to cause DoS.
The attack factor is the ratio of the bandwidth of in-
jected invalid trafﬁc to the bandwidth of valid trafﬁc. For
example, an attack factor of ﬁve implies that for every
1000 bytes of legitimate transmitted data, an adversary
injects 5000 bytes of invalid data. We are primarily in-
terested in medium bandwidth pollution attacks, e.g., up
to an attack factor of ten. We assume that beyond these
values, the adversary will saturate the channel and cause
large packet loss within the network.
2.6. Cryptographic primitives
Universal one-way hash functions We assume the ex-
istence of families of universal one-way hash functions
(UOWHFs) [19]. UOWHFs satisfy a property known as
target collision-resistance (TCR) [7]. U is called a fam-
ily of UOWHFs if for all polynomial-time adversaries A,
A has low probability in winning the following game: A
ﬁrst chooses a message M , and then A is given a ran-
(cid:4)= M such
dom h(·) ∈ U . To win, A must output M (cid:1)
that h(M(cid:1)) = h(M). This differs from any collision-
resistance (ACR), in which the adversary has the freedom
after she is given h(·). TCR
to choose both M and M (cid:1)
has two advantages over ACR: (1) Since TCR is a weaker
notion, it is believed to be easier to achieve in concrete
instantiations. (2) Since M is speciﬁed before the hash
function h(·), birthday paradox attacks to ﬁnd collisions
do not directly apply, and the hash output can be half the
size of an ACR hash function.
In the multicast setting, adopting TCR allows adver-
saries to have complete control over the underlying data
in a stream, but only before transmission starts.
If this
assumption does not hold, we must replace most applica-
tions of TCR hash functions with ACR hash functions, and
that would increase our overhead by a factor less than two.
For the remainder of this paper, we assume all collision-
resistant hash functions are TCR.
Merkle hash trees Merkle hash trees [17] are a mecha-
nism for computing a single cryptographically secure hash
digest over a set of data items. Merkle hash trees are con-
structed in the following manner. Let h(·) be a collision-
resistant hash function and let S = {s1, s2, . . . , sn} be
a set of data items. For the sake of simplicity, suppose
that n = 2(cid:1)−1 for (cid:2) > 1. Then, we construct an (cid:2)-level
complete binary tree using the hashes of the data items,
h1,8
h1,4
h5,8
h1,2
h3,4