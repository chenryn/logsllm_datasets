error in order to evaluate estimation accuracy. The relative
) ∗ 100
) − dfest(t∗
error rate is calculated as |df(t∗
)|.
and the absolute error is calculated as |df(t∗
We perform experiments for each nDFE, nDF pair for nDFE ∈
{1, 5, 10, . . . , 250} and nDF ∈ {0, 1, . . . , 999}. Figure 3
shows the changes of the relevance score of score(t∗, d) as
DF(t∗
)|/df(t∗
) − dfest(t∗
) increases from 1 to 1,000.
As shown in Table 4, the average relative errors (across
all nDF) for any nDFE are all less than 0.5%, and average
absolute errors are less than 3. We ﬁnd that when nDFE ≥ 50,
the average relative errors and the average absolute errors
under different nDFE are similar, i.e., the estimations do not
become more accurate as we use more data points during
regression analysis. Figure 5 shows a histogram of the errors
for the 1,000 experiments (for the 1,000 different nDF values)
and for nDFE = 50. As can be seen, the performance of the
DF prediction is very good: about 10% of the estimations
are correct; less than 14 of the estimations have absolute
errors of 5. We note that the attack performs differently on
alphabetic and numeric terms, likely due to boosting in the
score function.
685
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:39 UTC from IEEE Xplore.  Restrictions apply. 
Relative error
Absolute error
Min Avg Max Min Avg Max
Alphabetic 0.07% 0.38% 0.53% 0.52 1.93 2.83
Numerical 0.13% 0.43% 0.58% 0.59 2.15 3.03
Figure 4: An overview of the average relative and absolute
errors for DF prediction for all nDFE on GitHub. The ﬁrst row
targets estimation for a random 16-byte alphabetic string and
the second row is for random 16-byte number.
e
t
a
r
e
v
i
t
s
o
p
-
e
s
l
a
F
100
50
0
GitHub-numeric
GitHub-alphabetic
Orchestrate-numeric
Orchestrate-alphabetic
16-byte alphabetic
16-byte number
e
g
a
t
n
e
c
r
e
P
40
35
30
25
20
15
10
5
0
0
1
2
Absolute error
3
4
5
Figure 5: The distribution of the absolute errors when nDFE =
50 (GitHub).
We repeat the experiments again on two further shards and
get similarly small error rates: when nDFE = 50, the average
relative errors are 0.65% and 0.27%, and the average absolute
errors are 3.9 and 1.2, respectively.
One important
factor
that can affect
the estimation
accuracy is the time we wait between updating a document
and relevance score measurement. We ﬁnd if waiting only
30 seconds, there is so much noise in the data that we
cannot even do a reasonable curve ﬁt to the scoring function.
However, sometimes 60 seconds might still not be long
enough for an index to reach a stable state: we indeed
observed unusual score variations during data collection.
While the DF estimation already works well despite this
noise, we believe the performance could be improved further
with more effort on data collection and processing.
Term extraction attacks. We start by conﬁrming that
term extraction works correctly in a controlled setting. We
generate a set of 50 victim terms B and a set of 50 control
terms B(cid:2). We create a victim document d = B, and then run
our term extraction attack on all the terms in B ∪ B(cid:2) to see
if it can properly identify the victim terms. We repeated the
experiment 50 times.
To save time, once TERMEXTRACT ﬁnds the target shard
containing d (i.e., a shard containing any of the terms form
B∪B(cid:2)) we ignore the other shards and only do term exaction
on the target shard. The average time for ﬁnding the target
shard is 1,149 seconds, while the minimum time is 698
seconds and the maximum time is 1,607 seconds. The median
686
4
5
6
7
8
9
10
11
Term length
Figure 6: The average false-positive rates for different lengths
of alphabetic-character-only term and numeric-character-
only terms across three shards in GitHub and Orchestrate.io.
number of tries (i.e., number of shards examined before
ﬁnding the target index) is 98.
The attack achieves a true-positive rate of 100% and a
false-positive rate of 0%. Since we also chose long random
terms, excluding any noise due to bystanders, we conclude
that the attack solved the experiment perfectly.
False positives on GitHub. The brute-force term extraction
attack will encounter false positives due to bystander data.
To understand how often terms happen to be contained
on GitHub, we estimate the false-positive rate associated
with low-entropy terms. We also test two types of terms:
alphabetic-character-only terms and numeric-character-only
terms. For a given length (cid:5), we generate 20,000 terms of
length (cid:5) ((cid:5) ≥ 5) and of a given type (104 terms for numeric-
character-only term when (cid:5) = 4) to construct B, and randomly
select 5% of these terms as B(cid:2). We set (cid:5) to each of 4, 5, ..., 16.
We repeat the test on three different shards, and report on
the average false-positive rates across 3 rounds in Figure 6.
We can see when (cid:5) = 4, 5, the false-positive rates are 100%
or near 100% in both services. The false-positive rates are
relatively high even when (cid:5) = 8, but drops to a very small
value (< 0.5%) when (cid:5) ≥ 9 and zero when (cid:5) ≥ 11. We can
also clearly see that numeric-character-only terms involve
more false positives than alphabetic-character-only terms.
Feasibility of brute-force attacks. According to GitHub,
developers sometimes leave CCN information in source
code [18], and users might also store their own personal
information on GitHub [52]. We argue that it is sometimes
feasible for an attacker with partial information to harvest this
(and other) information via the DF side channel.
Recall that in GitHub one account can send 5,000 requests
per hour. Our brute-force attack will write large ﬁles
containing terms to test and then issue one API call per term.
Since writing the ﬁle requires a wait time for propagation to
the index, one would pipeline the writes while performing the
search queries. Assuming this is implemented, in the limit our
term extraction needs one API request per term guess (using
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:39 UTC from IEEE Xplore.  Restrictions apply. 
a modiﬁed version of TERMEXTRACT that uses one random
term ri to generate a score si that is then compared against
the scores of many victim terms). Each guess checks if the
term is on a particular shard. This gives a rough estimate
of 120,000 guesses on a shard per day with one account.
Creating n additional accounts increases the brute-forcing
power by a factor n as the guessing algorithm can be run
in parallel.
For a concrete example, if one knows the BIN (bank
identiﬁcation number) and last four digits of a CCN then
there are about 106 possible CCNs. If an attack has focused
on a particular shard the rest of the CCN could be brute-
forced with one account in under a day. If the attacker is
unsure of the shard, it could create one free account per shard
and execute the attack in parallel (which, nicely, would be
perfectly load-balanced on GitHub’s backend).
D. Orchestrate.io
Orchestrate.io is a database-as-a-service platform for
developing web and mobile applications. The information
stored on Orchestrate.io is likely different than in GitHub
since it is a generic key-value database and is being used to
store all types of data. It seems likely that application back-
ends store sensitive customer information in Orchestrate.io.
According to Orchestrate.io’s ofﬁcial blog, it uses ES
as its search engine [36], and has made efforts to secure
its search API. However, we found its search API also
expose the relevance scores of returned documents. Further
tests suggested that
the DF side channel also exists in
Orchestrate.io.
The Orchestrate.io API does not restrict the number of
operators in the query but enforces a maximum query
size of 6 KB. The service does not have a speciﬁc rate-
limiting policy but will throttle a user if her API requests
affect their servers’ performance. The index update time
on Orchestrate.io is faster than GitHub. To avoid burdening
on the target server, we decide to pause 30 seconds after
creating/updating a document and 2 seconds after each
search query.
Attack
results.
In Orchestrate.io, we use the same
in GitHub to test MAPSHARDS and
experiments as
TERMEXTRACT. MAPSHARDS collects 50 shards in 12
hours, using 128 rounds with 256 documents being cre-
ated. The δ in COSHARDTEST was set
In
TERMEXTRACT, the average time for locating the target
shard is 324 seconds and the median number of tries is 15.
The term extraction attack also achieves a true-positive rate
of 100% and a false-positive rate of 0%.
to 0.08.
We also conduct the same false-positive tests in Orches-
trate.io. The average false-positive rates across three rounds
are shown in Figure 6. As the term length increases, the false-
positive rates drop to zero more quickly than on GitHub;
when 7 ≤ l ≤ 9, we only ﬁnd very few false positives (1
to 3) for a given length.
To perform DFPRED, we need to put multiple documents
on the same shard. Unfortunately, unlike GitHub, no features
in Orchestrate.io directly facilitate creating documents on
the same shard. One solution is to create many documents
and use COSHARDTEST to discover the documents that
are on the same shard with a target document. However,
this is very time-consuming. To speed this process up, we
use the aforementioned ad-hoc score-based co-shard test
in §V. More speciﬁcally, we create 30,000 documents that
have the same content in AcctV, which is a unique 16-byte
term, and measure the relevance scores of these documents.
We group the documents by their relevance scores, and
keep 500 documents from the largest group. To eliminate
false positives, we use COSHARDTEST to conﬁrm these
documents are indeed on the same index. We repeat these
procedures in AcctA and keep 100 documents.
We perform experiments for each nDFE, nDF pair for nDFE ∈
{1, 5, 10, . . . , 100} and nDF ∈ {0, 1, . . . , 499}. The scoring
function in Orchestrate.io is still in the form of a− b∗ ln(x +
c). The average relative and absolute error rates decrease
as nDFE increases. When nDFE = 100, the average relative
errors are about 2.2% and the average absolute errors are
less than 6.0 for terms being tested. As df(t∗
) increases, the
estimations become less accurate. The maximum absolute
) ≤ 250, the attack still
errors are 15. However, when df(t∗
performs well, with the maximum absolute error less than or
equal to 2.
Feasibility of brute-force attacks. In Orchestrate.io, a free-
plan user can only send 50,000 requests every month. So to
search 109 terms, the attacker needs 20,000 accounts. Though
this sounds costly, the process can be automated due to the
fact that the account registration is very simple — the attacker
just needs to ﬁll in an email address and a password — and
no captchas are being used.
Another choice is to use Orchestrate.io’s professional plan,
which is $499 per month, that allows one to send 5 M requests
per month and pay $0.01 for 10 K additional requests.
Sending 109 requests costs an attacker $1,500, but the gain
of the attacks could be more than the cost. Of course, smaller
spaces can be brute forced much more cheaply and quickly.
E. Xen.do
Xen.do is a hosted search service which aggregates data
from a user’s accounts on multiple third-party services, builds
full-text indexes over the data, and provides interfaces to
search the aggregated data. Xen.do supports more than 35
services, including, but not limited to, Google Apps (Gmail,
Contacts, Drives, etc.), cloud storage services (Dropbox,
OneDrive, etc.), customer relationship management (CRM)
systems (Salesforce, ZohoCRM, etc.), and other services
(Evernote, Ofﬁce 365, etc.).
Sensitive information harvesting is particularly threatening
on Xen.do since the data are collected from users’ personal
accounts. Xen.do makes an best effort to guarantee data
687
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:39 UTC from IEEE Xplore.  Restrictions apply. 
security and privacy, and has received high ratings in various
security tests such as Skyhigh Networks CloudTrust [56].
Unfortunately, we also ﬁnd the DF side channel in Xen.do.
We found the all the supported services in Xen.do share the
same multi-tenant indexes. Therefore, a malicious user can
extract the sensitive terms in other users’ documents from
different sources at the same time.
For Xen.do, its API access is not public and the API key
can be only obtained on request. We only obtain a 30-day
trial to the beta-test version of the API, which currently
only provides basic operations such as full-text search and
authentication. One operation — connecting Xen.do to a
service — in the attacks must be done manually via the
web interface.