Some of the common use cases of LXC and LXD come from the following requirements... Explain.
- the need for an isolated development environment without polluting your host machine
- isolation within production servers and the possibility to run more than one service in its own container
- a need to test things with more than one version of the same software or different operating system environments
- experimenting with different and new releases of GNU/Linux distributions without having to install them on a physical host machine
- trying out a software or development stack that may or may not be used after some playing around
- installing many types of software in your primary development machine or production server and maintaining them on a longer run
- doing a dry run of any installation or maintenance task before actually executing it on production machines
- better utilization and provisioning of server resources with multiple services running for different users or clients
- high-density virtual private server (VPS) hosting, where isolation without the cost of full virtualization is needed
- easy access to host hardware from a container, compared to complicated access methods from virtual machines
- multiple build environments with different customizations in place
You have to prepare a Redis cluster. How will you ensure security?
- protect a given Redis instance from outside accesses via firewall
- binding it to 127.0.0.1 if only local clients are accessing it
- sandboxed environment
- enabling **AUTH**
- enabling **Protected Mode**
- data encryption support (e.g. `spiped`)
- disabling of specific commands
- users **ACLs**
Useful resources:
- [Redis Security](https://redis.io/topics/security)
- [A few things about Redis security](http://antirez.com/news/96)
###### Cyber Security Questions (5)
What is OWASP Application Security Verification Standard? Explain in a few points. ***
To be completed.
What is CSRF?
**Cross Site Request Forgery** is a web application vulnerability in which the server does not check whether the request came from a trusted client or not. The request is just processed directly. It can be further followed by the ways to detect this, examples and countermeasures.
What is the difference between policies, processes and guidelines?
As **security policy** defines the security objectives and the security framework of an organisation. A **process** is a detailed step by step how to document that specifies the exact action which will be necessary to implement important security mechanism. **Guidelines** are recommendations which can be customized and used in the creation of procedures.
What is a false positive and false negative in case of IDS?
When the device generated an alert for an intrusion which has actually not happened: this is **false positive** and if the device has not generated any alert and the intrusion has actually happened, this is the case of a **false negative**.
10 quick points about web server hardening.
Example:
- if machine is a new install, protect it from hostile network traffic, until the operating system is installed and hardened
- create a separate partition with the `nodev`, `nosuid`, and `noexec` options set for `/tmp`
- create separate partitions for `/var`, `/var/log`, `/var/log/audit`, and `/home`
- enable randomized virtual memory region placement
- remove legacy services (e.g. `telnet-server`, `rsh`, `rlogin`, `rcp`, `ypserv`, `ypbind`, `tftp`, `tftp-server`, `talk`, `talk-server`).
- limit connections to services running on the host to authorized users of the service via firewalls and other access control technologies
- disable source routed packet acceptance
- enable **TCP/SYN** cookies
- disable SSH root login
- install and configure **AIDE**
- install and configure **OSsec HIDS**
- configure **SELinux**
- all administrator or root access must be logged
- integrity checking of system accounts, group memberships, and their associated privileges should be enabled and tested
- set password creation requirements (e.g. with PAM)
Useful resources:
- [Security Harden CentOS 7](https://highon.coffee/blog/security-harden-centos-7/)
- [CentOS 7 Server Hardening Guide](https://www.lisenet.com/2017/centos-7-server-hardening-guide/)
## Secret Knowledge
### :diamond_shape_with_a_dot_inside: Guru Sysadmin
Explain what is Event-Driven architecture and how it improves performance? ***
To be completed.
An application encounters some performance issues. You should to find the code we have to optimize. How to profile app in Linux environment?
> Ideally, I need an app that will attach to a process and log periodic snapshots of: memory usage number of threads CPU usage.
1. You can use `top`in batch mode. It runs in the batch mode either until it is killed or until N iterations is done:
```bash
top -b -p `pidof a.out`
```
or
```bash
top -b -p `pidof a.out` -n 100
```
2. You can use ps (for instance in a shell script):
```bash
ps --format pid,pcpu,cputime,etime,size,vsz,cmd -p `pidof a.out`
```
> I need some means of recording the performance of an application on a Linux machine.
1. To record performance data:
```bash
perf record -p `pidof a.out`
```
or to record for 10 secs:
```bash
perf record -p `pidof a.out` sleep 10
```
or to record with call graph ():
```bash
perf record -g -p `pidof a.out`
```
2) To analyze the recorded data
```bash
perf report --stdio
perf report --stdio --sort=dso -g none
perf report --stdio -g none
perf report --stdio -g
```
**This is an example of profiling a test program**
1. I run my test program (c++):
```bash
./my_test 100000000
```
2. Then I record performance data of a running process:
```bash
perf record -g  -p `pidof my_test` -o ./my_test.perf.data sleep 30
```
3. Then I analyze load per module:
```bash
perf report --stdio -g none --sort comm,dso -i ./my_test.perf.data
# Overhead  Command                 Shared Object
# ........  .......  ............................
#
    70.06%  my_test  my_test
    28.33%  my_test  libtcmalloc_minimal.so.0.1.0
     1.61%  my_test  [kernel.kallsyms]
```
4. Then load per function is analyzed:
```bash
perf report --stdio -g none -i ./my_test.perf.data | c++filt
# Overhead  Command                 Shared Object                       Symbol
# ........  .......  ............................  ...........................
#
    29.30%  my_test  my_test                       [.] f2(long)
    29.14%  my_test  my_test                       [.] f1(long)
    15.17%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator new(unsigned long)
    13.16%  my_test  libtcmalloc_minimal.so.0.1.0  [.] operator delete(void*)
     9.44%  my_test  my_test                       [.] process_request(long)
     1.01%  my_test  my_test                       [.] operator delete(void*)@plt
     0.97%  my_test  my_test                       [.] operator new(unsigned long)@plt
     0.20%  my_test  my_test                       [.] main
     0.19%  my_test  [kernel.kallsyms]             [k] apic_timer_interrupt
     0.16%  my_test  [kernel.kallsyms]             [k] _spin_lock
     0.13%  my_test  [kernel.kallsyms]             [k] native_write_msr_safe
  ...
```
5. Then call chains are analyzed:
```bash
perf report --stdio -g graph -i ./my_test.perf.data | c++filt
# Overhead  Command                 Shared Object                       Symbol
# ........  .......  ............................  ...........................
#
    29.30%  my_test  my_test                       [.] f2(long)
            |
            --- f2(long)
               |
                --29.01%-- process_request(long)
                          main
                          __libc_start_main
    29.14%  my_test  my_test                       [.] f1(long)
            |
            --- f1(long)
               |
               |--15.05%-- process_request(long)
               |          main
               |          __libc_start_main
               |
                --13.79%-- f2(long)
                          process_request(long)
                          main
                          __libc_start_main
  ...
```
So at this point you know where your program spends time.
Also the simple way to do app profile is to use the `pstack` utility or `lsstack`.
Other tool is Valgrind. So this is what I recommend. Run program first:
```bash
valgrind --tool=callgrind --dump-instr=yes -v --instr-atstart=no ./binary > tmp
```
Now when it works and we want to start profiling we should run in another window:
```bash
callgrind_control -i on
```
This turns profiling on. To turn it off and stop whole task we might use:
```bash
callgrind_control -k
```
Now we have some files named callgrind.out.* in current directory. To see profiling results use:
```bash
kcachegrind callgrind.out.*
```
I recommend in next window to click on **Self** column header, otherwise it shows that `main()` is most time consuming task.
Useful resources:
- [Tracing processes for fun and profit](http://techblog.rosedu.org/tracing-processes-for-fun-and-profit.html)
Using a Linux system with a limited number of packages installed, and telnet is not available. Use sysfs virtual filesystem to test connection on all interfaces (without loopback).
For example:
```bash
#!/usr/bin/bash
for iface in $(ls /sys/class/net/ | grep -v lo) ; do
  if [[ $(cat /sys/class/net/$iface/carrier) = 1 ]] ; then state=1 ; fi
done
if [[ $state -ne 0 ]] ; then echo "not connection" > /dev/stderr ; exit ; fi
```
Write two golden rules for reducing the impact of hacked system.
1) **The principle of least privilege**
You should configure services to run as a user with the least possible rights necessary to complete the service's tasks. This can contain a hacker even after they break in to a machine.
As an example, a hacker breaking into a system using a zero-day exploit of the Apache webserver service is highly likely to be limited to just the system memory and file resources that can be accessed by that process. The hacker would be able to download your html and php source files, and probably look into your mysql database, but they should not be able to get root or extend their intrusion beyond apache-accessible files.
Many default Apache webserver installations create the 'apache' user and group by default and you can easily configure the main Apache configuration file (`httpd.conf`) to run apache using those groups.
2) **The principle of separation of privileges**
If your web site only needs read-only access to the database, then create an account that only has read-only permissions, and only to that database.
**SElinux** is a good choice for creating context for security, `app-armor` is another tool. **Bastille** was a previous choice for hardening.
Reduce the consequence of any attack, by separating the power of the service that has been compromised into it own "Box".
3) **Whitelist, don't blacklist**
You're describing a blacklist approach. A whitelist approach would be much safer.
An exclusive club will never try to list everyone who can't come in; they will list everyone who can come in and exclude those not on the list.
Similarly, trying to list everything that shouldn't access a machine is doomed. Restricting access to a short list of programs/IP addresses/users would be more effective.
Of course, like anything else, this involves some trade-offs. Specifically, a whitelist is massively inconvenient and requires constant maintenance.
To go even further in the tradeoff, you can get great security by disconnecting the machine from the network.
**Also interesting are**:
Use the tools available. It's highly unlikely that you can do as well as the guys who are security experts, so use their talents to protect yourself.
- public key encryption provides excellent security
- enforce password complexity
- understand why you are making exceptions to the rules above - review your exceptions regularly
- hold someone to account for failure, it keeps you on your toes
Useful resources:
- [How to prevent zero day attacks (original)](https://serverfault.com/questions/391370/how-to-prevent-zero-day-attacks)