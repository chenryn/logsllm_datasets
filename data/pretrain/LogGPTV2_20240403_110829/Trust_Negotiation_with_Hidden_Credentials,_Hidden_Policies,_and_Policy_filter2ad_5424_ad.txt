O(mAρ(nA + nB)) communication (assuming policies can
be computed by a circuit of size linear in the number of bits
of their inputs).
6.3 Privacy-Preserving Trust Negotiation Proto-
col
Intuition of Correctness/Security: In Step 1 of the pro-
tocol, the server sets its set of usable credentials to all of
its credentials (recall that the RE strategy protocol assumes
everything is usable initially and that things are removed
from this set).
In Step 2 of the protocol, the client and the server take
turns updating their usable credential sets based on the other
party’s usable set. Once a set ceases to change then the
usable sets will cease changing and we will have com-
puted the maximal usable credential set. Note that since
we are assuming monotonic policies this will take at most
min{nC, nS} rounds to compute this set.
Finally, as we model the service as a credential s1, the
1 [1] after Step 3 if and only if s1 is in the
client will have rS
US.
Proof of Correctness/Security: A more detailed proof
sketch is given in Section 8.
We now “put the pieces together” and give the overall
protocol for privacy-preserving trust negotiation. We de-
Cost analysis Step 2 of
is executed
min{nC, nS} (call this value n) times. An individual exe-
the protocol
sends rS
i [1] to the client. The client calls this value rS
i [xi]
2. For i = 1, . . . , min(nC , nS):
(a) The client and server
Input: The client has CC and PC. The server has CS (call these credentials s1, . . . , snS ) and PS. Furthermore,
s1 is the service that the client requested.
Output: If the trust negotiation between the client and server can succeed, then both the client and server output
true, otherwise, they output false.
Protocol Steps:
1. Initialize US. For each credential si ∈ CS, the server picks two random numbers {rS
i [0], rS
i [1]}. The server
run the secure RE strategy protocol
(Figure 5)
to obtain UC =
(b) The
secure-reverse-eager-strategy(CC, PC, CS, US) in split form.
secure RE protocol
secure-reverse-eager-strategy(CS, PS, CC, UC) in split form.
and client
run the
server
(Figure 5)
to obtain US
=
3. Output result. To determine whether s1 ∈ US, the server sends a hash of rS
1 [1] to the client. The client
1 [x1] matches this value; if it is a match then the client proves this to the server by
1 [x1] to the server (and both parties output true), and if it is not a match the client terminates the
checks if the hash of rS
sending rS
protocol (and both parties output false).
Figure 6. Privacy-preserving trust negotiation protocol
cution requires O(ρ(mC + mS)(nC + nS)) communication
and thus the protocol requires O(nρ(mC + mS)(nC + nS))
communication.
7 Efﬁciency Improvements
7.1 A more efﬁcient equality test for array ele-
ments
In this section, we introduce a more efﬁcient protocol for
the equality test for array elements. This protocol is related
to the protocol proposed by [13] for secure set intersection.
Figure 7 introduces this protocol. Note that this protocol re-
quires only O(nρ + ρ2) communication (instead of O(nρ2)
communication). We give the proof sketch of correctness
and security in Section 8.
7.2 Reducing the number of rounds
A possible criticism of our protocol for trust negotiation
is that it requires O(min{nC, nS}) rounds. The RE strategy
requires this many rounds in the worst case, but in practice
it requires much less (it requires rounds proportional to the
length of the longest policy chain). Our protocol can be
modiﬁed to stop as soon as the usable credential sets cease
changing. However, this is not recommended as it would
leak additional information, and this information allows for
additional probing. For example, if the negotiation requires
5 rounds then both parties can deduce that the other party
does not satisfy at least 4 of their credentials. Thus, from
a privacy standpoint terminating after the usable credential
sets cease changing is not a good idea. Another option is
to limit the number of rounds to some reasonable constant.
This does not have privacy problems, but it could cause the
negotiation to succeed where it would not have succeeded
under Deﬁnition 2 of trust negotiation. However, if there is
domain-speciﬁc knowledge that bounds the longest creden-
tial chain, then this is a viable option.
8 Security Proofs
In this appendix we discuss the security of our protocols.
We ﬁrst deﬁne what is meant by security. We then brieﬂy
(due to page constraints) sketch components of the proof of
security.
8.1 Deﬁnition of Security
The security deﬁnition we use is similar to the stan-
dard model from the secure multi-party computation liter-
ature [15, 7]. The security of our protocol is analyzed by
comparing what an adversary can do in our protocol against
what an adversary can do in an ideal implementation with a
trusted oracle. Speciﬁcally, we will show our protocol is no
worse than this ideal model by showing that for any adver-
sary in our model there is an adversary in the ideal model
that is essentially equivalent. Thus if the ideal model is ac-
ceptable (in terms of security), then our protocols must also
be acceptable.
Deﬁning the ideal model for private trust negotiation is
tricky. First, the ideal model has to be deﬁned such that
there are no “violations of security” that are achievable in
Input and Output: See Figure 4.
Protocol Steps:
1. Alice and Bob both choose semantically secure homomorphic encryption schemes EA and EB that share a
modulus M and exchange public parameters.
2. Alice creates a polynomial P that encodes the x values where the constant coefﬁcient is 1 (which can be
done since this arithmetic is modular). In other words she ﬁnds a polynomial P (x) = ηnxn + ηn−1xn−1 +
· · · + η1x + 1 where P (xi) = 0 for all xi. She sends to Bob EA(ηn), . . . , EA(η1).
3. Bob chooses a value kB uniformly from Z ⋆
M . For each yi, Bob chooses a value qB,i uniformly from Z ⋆
and he computes (EA(P (yi)))qB,iEA(kB + yi) = EA(qB,iP (yi) + kB + yi) (call this value EA(αi)). Bob
sends to Alice EA(α1), . . . , EA(αn), EB(kB).
4. Alice decrypts the values to obtain α1, . . . , αn. She then computes x1 − αi, . . . , xn − αn She checks for
duplicate values, and if there are duplicates she replaces all extra occurrences of a value by a random value.
Alice chooses a value kA uniformly from Z ⋆
M . For each of the values xi − αi she chooses qA,i uniformly
M and then she computes (EB(kB)EB(xi − αi))qAi EB(kA)= EB((xi + kB − αi)qA,i + kA) (we
from Z ⋆
will call this value EB(βi)). Alice sends to Bob EB(β1), . . . , EB(βn).
5. Bob decrypts the values to obtain β1, . . . , βn. Bob then creates a polynomial Q that encodes these values
where the constant coefﬁcient is 1. In other words Bob ﬁnds a polynomial Q(x) = γnxn + γn−1xn−1 +
· · · + γ1x + 1 where Q(βi) = 0 for all βi. Bob sends to Alice EB(γn), . . . , EB(γ1).
M
6. Alice chooses two values k and qA uniformly from Z ⋆
M and computes EB(Q(kA)qA + k) and sends this
value to Bob.
7. Bob decrypts this value to obtain k′. Alice and Bob engage in a scrambled circuit evaluation of an equality
circuit where Alice is the generator with input k and she sets the encodings for the output wire to t0 for the
negative encoding and to t1 for the positive encoding and Bob is the evaluator with input k′.
Figure 7. Secure Equality Test for Array Elements.
this ideal model; otherwise, there could be “violations of
security” in our protocols. Furthermore, the ideal model
must be deﬁned in such a way as to allow useful trust ne-
gotiation to take place; otherwise it and our protocols will
not be useful. This is further complicated by the fact that
the RE strategy does not make sense in a non-private set-
ting (as one cannot revoke knowledge from another party).
Thus we deﬁne a ﬁctitious environment where the parties
have ”chronic amensia” about the other party’s credentials.
In such an environment the RE strategy is plausible, and so
our ideal model simulates this environment.
We now informally deﬁne an ideal model implementa-
tion of our scheme. In the ideal model the client sends CC
and PC to the trusted oracle, and the server sends CS, PS,
and s to the oracle. We model PC and PS as arbitrary PPT
algorithms. These algorithms will simulate the parties’ be-
havior during the RE strategy. Thus these algorithms should
be viewed as control algorithms that: (1) deﬁne which cre-
dentials to use during each round, (2) deﬁne the access con-
trol policies (which we model as PPT algorithms over the
other party’s currently usable credentials) for its credentials
during each round, and (3) can force the oracle to termi-
nate. We stress that these algorithms cannot do the above
operations based upon the state of the negotiation. For ex-
ample, they cannot force the oracle to terminate when a spe-
ciﬁc credential becomes unusable. The oracle will simulate
the RE strategy using the access control policies deﬁned by
each party’s control algorithm. At the end of the negotia-
tion the oracle will inform the client and the server whether
access is granted. Due to page limitations we do not discuss
the above ideal model in more detail.
8.2 Sketch of the Security Proof
We will now sketch part of the proof. As it is too lengthy
to include in full detail, we focus only on one speciﬁc aspect
of the system. We focus on the Secure Reverse Eager strat-
egy protocol (which is the key component of our system).
We ﬁrst show that if Alice is honest, then Bob cannot in-
ﬂuence the outcome of the protocols so that he unrightfully
keeps one of Alice’s credentials usable.
Lemma 4 In the secure RE strategy protocol (Figure 5): If
Alice is honest and after the protocol a speciﬁc credential
ai (with policy pi) is in UA, then Bob has a credential set
CB such that pi(CB) is true.
Proof: Because step 2 is done by SCE and Alice is an
honest generator, by Lemma 5 all that we must show is that
after step 1, Bob learns ti[1] only when he has credential ai.
By way of contradiction, suppose Bob does not have cre-
dential ai, and that he learns ti[1] in Step 1c. By Lemmas 6
and 7, Bob only learns ti[1] when there is a match in the ar-
rays created by Alice and Bob in Step 1c. If there is a match,
then Bob must be able to learn x with a non-negligible prob-
ability, but this implies that he can invert the IBE encryption
with non-negligible probability, but this contradicts that the
IBE encryption scheme is secure.
(cid:3)
Lemma 5 In scrambled circuit evaluation: If the genera-
tor is honest and the evaluator learns at most one encoding
for each input wire, then the evaluator learns at most one
encoding for the output wire; furthermore this encoding is
the correct value.
Proof: We omit the details of this lemma, but similar
(cid:3).
lemmas are assumed in the literature
Lemma 6 In the circuit-version of the equality test for ar-
ray elements: If Alice is honest, Bob learns t1 only when
there is an index i such that xi = yi.
Proof: Since Alice is the generator of the circuit and is
honest, Bob will input a set of y values and will learn t1 only
when one of his y values matches one of Alice’s x values
(by Lemma 5).
(cid:3)
Lemma 7 In the other version (Figure 7) of the equality
test for array elements: If Alice is honest, Bob learns t1
only when there is an index i such that xi = yi.
Proof: By way of contradiction, suppose Bob learns t1
and there is no match in their arrays. In Step 7 of the pro-
tocol Bob must know the value k (by Lemma 5). Thus in
Step 5 of the protocol, Bob must be able to generate a non-
zero polynomial of degree n that has kA as a root, but this
implies he knows kA with non-negligible probability. This
implies that in Step 3, Bob can generate values α1, . . . , αn
such that there is an α value that is xi + kB. This implies
Bob knows xi with non-negligible probability, and this im-
plies that there is a match in the arrays.
(cid:3)
The above only shows one part of the proof. We must
also show that if Alice is honest, Bob cannot learn whether
he made a speciﬁc credential usable (he can force a creden-
tial to be unusable, but this has limited impact). Further-
more, we must show that if Bob is honest that Alice does
not learn which of her credentials are usable (other than
what can be deduced from her policies; i.e., a globally us-
able credential will deﬁnitely be usable). These proofs will
be in the full version of the paper. We now show that the
protocol is correct, that is if the parties are honest, then the
correct usable set is computed.
Proof: In step 1 of the protocol, Bob learns a value ti[xi]
where xi is 1 if Bob has credential ci and can use it. There
are 3 cases to consider:
1. Bob does not have ci: In Step 1b of the protocol, Bob
will not learn the value x, and thus there will not be a
match in Step 1c (with very high probability). Since
there is no match in the array, Bob will learn ti[0],
which is correct.
2. Bob has ci but cannot use it. Suppose bj = ci and
Alice has rB
j [0]. In this case, dj = x, but Bob’s vector
entry will be x + rB
j [0].
Since there is no match in the array, Bob will learn
ti[0], which is correct.
j [1] and Alice’s will be x + rB
3. Bob has ci and can use it. Suppose bj = ci and Alice
has rB
j [1]. In this case, dj = x, but Bob’s vector entry
will be x + rB
j [1]. Since
there is a match in the array, Bob will learn ti[1], which
is correct.
j [1] and Alice’s will be x + rB
In step 2 of the protocol, Alice and Bob securely evaluate
If pi(UB) is
i [1] (signifying that Alice can
i [0] (signifying that
(cid:3)
pi based upon which credentials are in UB.
true, then Bob will learn rA
use ai) and otherwise he will learn rA
Alice cannot use ai).
9 Related Work
Our work is originally motivated from the existing au-
tomated trust negotiation literature [4, 30, 25, 34, 33,
29] whose goal is to enable trust establishment between
strangers in a decentralized or open environment, such as
the Internet. In trust negotiation, each party establishes ac-
cess control policies to regulate not only the granting of re-
sources, but also the disclosure of credentials (and possibly
policies) to others. A negotiation begins when a party re-
quests access to a resource that is protected by an access
control policy. The negotiation process consists of a se-
quence of cautious and iterative exchanges of credentials
and possibly access control policies.
In successful nego-
tiations, disclosed credentials eventually satisfy the access
control polices of the desired resource. A security require-
ment for trust negotiation is that no credential should be
disclosed unless its access control policy has been satisﬁed.
The concept of privacy protection in the previous trust nego-