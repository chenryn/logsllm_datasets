rects traffic from one continent to another when our South American datacenter is
over capacity, we can save 20% of the resources we’d spend on hardware. In the larger
regions, we’ll spread tasks across two or three clusters for extra resiliency.
Because the backends need to contact the Bigtable holding the data, we need to also
design this storage element strategically. A backend in Asia contacting a Bigtable in
the USA adds a significant amount of latency, so we replicate the Bigtable in each
region. Bigtable replication helps us in two ways: it provides resilience should a
Bigtable server fail, and it lowers data-access latency. While Bigtable only offers even‐
tual consistency, it isn’t a major problem because we don’t need to update the contents
often.
We’ve introduced a lot of terminology here; while you don’t need to remember it all,
it’s useful for framing many of the other systems we’ll refer to later.
5 We assume the probability of two simultaneous task failures in our environment is low enough to be negligi‐
ble. Single points of failure, such as top-of-rack switches or power distribution, may make this assumption
invalid in other environments.
22 | Chapter 2: The Production Environment at Google, from the Viewpoint of an SRE
PART II
Principles
This section examines the principles underlying how SRE teams typically work—the
patterns, behaviors, and areas of concern that influence the general domain of SRE
operations.
The first chapter in this section, and the most important piece to read if you want to
attain the widest-angle picture of what exactly SRE does, and how we reason about it,
is Chapter 3, Embracing Risk. It looks at SRE through the lens of risk—its assessment,
management, and the use of error budgets to provide usefully neutral approaches to
service management.
Service level objectives are another foundational conceptual unit for SRE. The indus‐
try commonly lumps disparate concepts under the general banner of service level
agreements, a tendency that makes it harder to think about these concepts clearly.
Chapter 4, Service Level Objectives, attempts to disentangle indicators from objectives
from agreements, examines how SRE uses each of these terms, and provides some
recommendations on how to find useful metrics for your own applications.
Eliminating toil is one of SRE’s most important tasks, and is the subject of Chapter 5,
Eliminating Toil. We define toil as mundane, repetitive operational work providing no
enduring value, which scales linearly with service growth.
Whether it is at Google or elsewhere, monitoring is an absolutely essential compo‐
nent of doing the right thing in production. If you can’t monitor a service, you don’t
know what’s happening, and if you’re blind to what’s happening, you can’t be reliable.
Read Chapter 6, Monitoring Distributed Systems, for some recommendations for what
and how to monitor, and some implementation-agnostic best practices.
In Chapter 7, The Evolution of Automation at Google, we examine SRE’s approach to
automation, and walk through some case studies of how SRE has implemented auto‐
mation, both successfully and unsuccessfully.
Most companies treat release engineering as an afterthought. However, as you’ll learn
in Chapter 8, Release Engineering, release engineering is not just critical to overall sys‐
tem stability—as most outages result from pushing a change of some kind. It is also
the best way to ensure that releases are consistent.
A key principle of any effective software engineering, not only reliability-oriented
engineering, simplicity is a quality that, once lost, can be extraordinarily difficult to
recapture. Nevertheless, as the old adage goes, a complex system that works necessar‐
ily evolved from a simple system that works. Chapter 9, Simplicity, goes into this topic
in detail.
Further Reading from Google SRE
Increasing product velocity safely is a core principle for any organization. In “Making
Push On Green a Reality” [Kle14], published in October 2014, we show that taking
humans out of the release process can paradoxically reduce SREs’ toil while increasing
system reliability.
CHAPTER 3
Embracing Risk
Written by Marc Alvidrez
Edited by Kavita Guliani
You might expect Google to try to build 100% reliable services—ones that never fail.
It turns out that past a certain point, however, increasing reliability is worse for a ser‐
vice (and its users) rather than better! Extreme reliability comes at a cost: maximizing
stability limits how fast new features can be developed and how quickly products can
be delivered to users, and dramatically increases their cost, which in turn reduces the
numbers of features a team can afford to offer. Further, users typically don’t notice the
difference between high reliability and extreme reliability in a service, because the
user experience is dominated by less reliable components like the cellular network or
the device they are working with. Put simply, a user on a 99% reliable smartphone
cannot tell the difference between 99.99% and 99.999% service reliability! With this
in mind, rather than simply maximizing uptime, Site Reliability Engineering seeks to
balance the risk of unavailability with the goals of rapid innovation and efficient ser‐
vice operations, so that users’ overall happiness—with features, service, and perfor‐
mance—is optimized.
Managing Risk
Unreliable systems can quickly erode users’ confidence, so we want to reduce the
chance of system failure. However, experience shows that as we build systems, cost
does not increase linearly as reliability increments—an incremental improvement in
reliability may cost 100x more than the previous increment. The costliness has two
dimensions:
25
The cost of redundant machine/compute resources
The cost associated with redundant equipment that, for example, allows us to
take systems offline for routine or unforeseen maintenance, or provides space for
us to store parity code blocks that provide a minimum data durability guarantee.
The opportunity cost
The cost borne by an organization when it allocates engineering resources to
build systems or features that diminish risk instead of features that are directly
visible to or usable by end users. These engineers no longer work on new features
and products for end users.
In SRE, we manage service reliability largely by managing risk. We conceptualize risk
as a continuum. We give equal importance to figuring out how to engineer greater
reliability into Google systems and identifying the appropriate level of tolerance for
the services we run. Doing so allows us to perform a cost/benefit analysis to deter‐
mine, for example, where on the (nonlinear) risk continuum we should place Search,
Ads, Gmail, or Photos. Our goal is to explicitly align the risk taken by a given service
with the risk the business is willing to bear. We strive to make a service reliable
enough, but no more reliable than it needs to be. That is, when we set an availability
target of 99.99%,we want to exceed it, but not by much: that would waste opportuni‐
ties to add features to the system, clean up technical debt, or reduce its operational
costs. In a sense, we view the availability target as both a minimum and a maximum.
The key advantage of this framing is that it unlocks explicit, thoughtful risktaking.
Measuring Service Risk
As standard practice at Google, we are often best served by identifying an objective
metric to represent the property of a system we want to optimize. By setting a target,
we can assess our current performance and track improvements or degradations over
time. For service risk, it is not immediately clear how to reduce all of the potential
factors into a single metric. Service failures can have many potential effects, including
user dissatisfaction, harm, or loss of trust; direct or indirect revenue loss; brand or
reputational impact; and undesirable press coverage. Clearly, some of these factors
are very hard to measure. To make this problem tractable and consistent across many
types of systems we run, we focus on unplanned downtime.
For most services, the most straightforward way of representing risk tolerance is in
terms of the acceptable level of unplanned downtime. Unplanned downtime is cap‐
tured by the desired level of service availability, usually expressed in terms of the
number of “nines” we would like to provide: 99.9%, 99.99%, or 99.999% availability.
Each additional nine corresponds to an order of magnitude improvement toward
100% availability. For serving systems, this metric is traditionally calculated based on
the proportion of system uptime (see Equation 3-1).
26 | Chapter 3: Embracing Risk
Equation 3-1. Time-based availability
uptime
availability=
uptime+downtime
Using this formula over the period of a year, we can calculate the acceptable number
of minutes of downtime to reach a given number of nines of availability. For example,
a system with an availability target of 99.99% can be down for up to 52.56 minutes in
a year and stay within its availability target; see Appendix A for a table.
At Google, however, a time-based metric for availability is usually not meaningful
because we are looking across globally distributed services. Our approach to fault iso‐
lation makes it very likely that we are serving at least a subset of traffic for a given
service somewhere in the world at any given time (i.e., we are at least partially “up” at
all times). Therefore, instead of using metrics around uptime, we define availability in
terms of the request success rate. Equation 3-2 shows how this yield-based metric is
calculated over a rolling window (i.e., proportion of successful requests over a one-
day window).
Equation 3-2. Aggregate availability
successful requests
availability=
total requests
For example, a system that serves 2.5M requests in a day with a daily availability tar‐
get of 99.99% can serve up to 250 errors and still hit its target for that given day.
In a typical application, not all requests are equal: failing a new user sign-up request is
different from failing a request polling for new email in the background. In many
cases, however, availability calculated as the request success rate over all requests is a
reasonable approximation of unplanned downtime, as viewed from the end-user per‐
spective.
Quantifying unplanned downtime as a request success rate also makes this availabil‐
ity metric more amenable for use in systems that do not typically serve end users
directly. Most nonserving systems (e.g., batch, pipeline, storage, and transactional
systems) have a well-defined notion of successful and unsuccessful units of work.
Indeed, while the systems discussed in this chapter are primarily consumer and infra‐
structure serving systems, many of the same principles also apply to nonserving sys‐
tems with minimal modification.
For example, a batch process that extracts, transforms, and inserts the contents of one
of our customer databases into a data warehouse to enable further analysis may be set
to run periodically. Using a request success rate defined in terms of records success‐
fully and unsuccessfully processed, we can calculate a useful availability metric
despite the fact that the batch system does not run constantly.
Measuring Service Risk | 27
Most often, we set quarterly availability targets for a service and track our perfor‐
mance against those targets on a weekly, or even daily, basis. This strategy lets us
manage the service to a high-level availability objective by looking for, tracking down,
and fixing meaningful deviations as they inevitably arise. See Chapter 4 for more
details.
Risk Tolerance of Services
What does it mean to identify the risk tolerance of a service? In a formal environment
or in the case of safety-critical systems, the risk tolerance of services is typically built
directly into the basic product or service definition. At Google, services’ risk tolerance
tends to be less clearly defined.
To identify the risk tolerance of a service, SREs must work with the product owners
to turn a set of business goals into explicit objectives to which we can engineer. In this
case, the business goals we’re concerned about have a direct impact on the perfor‐
mance and reliability of the service offered. In practice, this translation is easier said
than done. While consumer services often have clear product owners, it is unusual
for infrastructure services (e.g., storage systems or a general-purpose HTTP caching
layer) to have a similar structure of product ownership. We’ll discuss the consumer
and infrastructure cases in turn.
Identifying the Risk Tolerance of Consumer Services
Our consumer services often have a product team that acts as the business owner for
an application. For example, Search, Google Maps, and Google Docs each have their
own product managers. These product managers are charged with understanding the
users and the business, and for shaping the product for success in the marketplace.
When a product team exists, that team is usually the best resource to discuss the reli‐
ability requirements for a service. In the absence of a dedicated product team, the
engineers building the system often play this role either knowingly or unknowingly.
There are many factors to consider when assessing the risk tolerance of services, such
as the following:
• What level of availability is required?
• Do different types of failures have different effects on the service?
• How can we use the service cost to help locate a service on the risk continuum?
• What other service metrics are important to take into account?
28 | Chapter 3: Embracing Risk
Target level of availability
The target level of availability for a given Google service usually depends on the func‐
tion it provides and how the service is positioned in the marketplace. The following
list includes issues to consider:
• What level of service will the users expect?
• Does this service tie directly to revenue (either our revenue, or our customers’
revenue)?
• Is this a paid service, or is it free?
• If there are competitors in the marketplace, what level of service do those com‐
petitors provide?
• Is this service targeted at consumers, or at enterprises?
Consider the requirements of Google Apps for Work. The majority of its users are
enterprise users, some large and some small. These enterprises depend on Google
Apps for Work services (e.g., Gmail, Calendar, Drive, Docs) to provide tools that
enable their employees to perform their daily work. Stated another way, an outage for
a Google Apps for Work service is an outage not only for Google, but also for all the
enterprises that critically depend on us. For a typical Google Apps for Work service,
we might set an external quarterly availability target of 99.9%, and back this target
with a stronger internal availability target and a contract that stipulates penalties if we
fail to deliver to the external target.
YouTube provides a contrasting set of considerations. When Google acquired You‐
Tube, we had to decide on the appropriate availability target for the website. In 2006,
YouTube was focused on consumers and was in a very different phase of its business
lifecycle than Google was at the time. While YouTube already had a great product, it
was still changing and growing rapidly. We set a lower availability target for YouTube
than for our enterprise products because rapid feature development was correspond‐
ingly more important.
Types of failures
The expected shape of failures for a given service is another important consideration.
How resilient is our business to service downtime? Which is worse for the service: a
constant low rate of failures, or an occasional full-site outage? Both types of failure
may result in the same absolute number of errors, but may have vastly different
impacts on the business.
An illustrative example of the difference between full and partial outages naturally
arises in systems that serve private information. Consider a contact management
application, and the difference between intermittent failures that cause profile pic‐
tures to fail to render, versus a failure case that results in a user’s private contacts
Risk Tolerance of Services | 29
being shown to another user. The first case is clearly a poor user experience, and
SREs would work to remediate the problem quickly. In the second case, however, the
risk of exposing private data could easily undermine basic user trust in a significant
way. As a result, taking down the service entirely would be appropriate during the
debugging and potential clean-up phase for the second case.
At the other end of services offered by Google, it is sometimes acceptable to have reg‐
ular outages during maintenance windows. A number of years ago, the Ads Frontend
used to be one such service. It is used by advertisers and website publishers to set up,
configure, run, and monitor their advertising campaigns. Because most of this work
takes place during normal business hours, we determined that occasional, regular,
scheduled outages in the form of maintenance windows would be acceptable, and we
counted these scheduled outages as planned downtime, not unplanned downtime.
Cost
Cost is often the key factor in determining the appropriate availability target for a ser‐
vice. Ads is in a particularly good position to make this trade-off because request suc‐
cesses and failures can be directly translated into revenue gained or lost. In
determining the availability target for each service, we ask questions such as:
• If we were to build and operate these systems at one more nine of availability,
what would our incremental increase in revenue be?
• Does this additional revenue offset the cost of reaching that level of reliability?
To make this trade-off equation more concrete, consider the following cost/benefit
for an example service where each request has equal value:
Proposed improvement in availability target: 99.9% → 99.99%
Proposed increase in availability: 0.09%
Service revenue: $1M
Value of improved availability: $1M * 0.0009 = $900
In this case, if the cost of improving availability by one nine is less than $900, it is
worth the investment. If the cost is greater than $900, the costs will exceed the projec‐
ted increase in revenue.
It may be harder to set these targets when we do not have a simple translation func‐
tion between reliability and revenue. One useful strategy may be to consider the back‐
ground error rate of ISPs on the Internet. If failures are being measured from the
end-user perspective and it is possible to drive the error rate for the service below the
background error rate, those errors will fall within the noise for a given user’s Internet
connection. While there are significant differences between ISPs and protocols (e.g.,
30 | Chapter 3: Embracing Risk
TCP versus UDP, IPv4 versus IPv6), we’ve measured the typical background error
rate for ISPs as falling between 0.01% and 1%.
Other service metrics
Examining the risk tolerance of services in relation to metrics besides availability is