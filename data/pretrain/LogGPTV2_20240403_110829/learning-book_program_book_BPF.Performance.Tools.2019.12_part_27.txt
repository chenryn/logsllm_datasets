such as process execution and thread migrations (with at most thousands of events per second)
can be instrumented with negligible overhead. Profiling (timed sampling) also limits overhead to
the fixed rate of samples, reducing overhead to negligible proportions.
6.1.3 Strategy
If you are new to CPU performance analysis, it can be difficult to know wfhere to startwhich
target to begin analyzing and with which tool. Here is a suggested overall strategy that you can
follow:
1. Ensure that a CPU workload is running before you spend time with analysis tools. Check
system CPU utilization (e.g., using mpstat(1) and ensure that all the CPUs are still online
(and some haven’t been offlined for some reason).
---
## Page 223
186
6 Chapter 6  CPUs
2. Confirm that the workload is CPU bound.
a. Look for high CPU utilization system-wide or on a single CPU (e.g., using mpstat(1).
b. Look for high run queue latency (e.g., using BCC runqlat(1)). Software limits such as
those used by containers can artificially limit the CPU available to processes, so an
application may be CPU bound on a mostly idle system. This counterintuitive scenario
can be identified by studying run queue latency.
3. Quantify CPU usage as percent utilization system-wide and then broken down by process,
CPU mode, and CPU ID. This can be done using traditional tools (e.g., mpstat(1), top(1).
Look for high utilization by a single process, mode, or CPU.
a. For high system time, frequency-count system calls by process and call type, and also
BCC sysstat(8).
pue sau-uo aegdq (t)guad Susn a) sapuaau aop goo o spuasunre aurexa
4. Use a profiler to sample stack traces, which can be visualized using a CPU flame graph.
Many CPU issues can be found by browsing such flame graphs.
5. For CPU consumers identified by profilers, consider writing custom tools to show more
context. Profilers show the functions that are running but not the arguments and objects
they are operating on, which may be needed to understand CPU usage. Examples:
a. Kernel mode: If a file system is consuming CPU resources doing stat( on files, what are
theit filenames? (This could be determined, for example, using BCC statsnoop(8) or in
general using tracepoints or kprobes from BPF tools.)
b. User-mode: If an application is busy processing requests, what are the requests? (If an
application-specific tool is unavailable, one could be developed using USDT or uprobes
and BPF tools).
6. Measure time in hardware interrupts, since this time may not be visible in timer-based
profilers (e.g., BCC hardirqs(1).
7. Browse and execute the BPF tools listed in the BPF tools section of this chapter.
8. Measure CPU instructions per cycle (IPC) using PMCs to explain at a high level how much
the CPUs are stalled (e.g., using perf(1)). This can be explored with more PMCs, which may
identify low cache hit ratios (e.g., BCC Ilcstat), temperature stalls, and so on,
The following sections explain the tools involved in this process in more detail.
6.2TraditionalTools
Tradlitional tools (see Table 6-2) can provide CPU utilization metrics for each process (thread) and
pue *qlua ananb tnu aear aug 'sapen uas xaquoo Arequmo pure requmon ‘nd qoea aof
the total time spent waiting on run queues. Profilers can show and quantify the software that is
running, and PMC-based tools can show how well the CPUs are operating at the cycle level.
Apart from solving issues, traditional tools can also provide clues to direct your further use of BPF
tools. They have been categorized here based on their source and measurement type: kernel statis-
tics, hardware statistics, and event tracing.
---
## Page 224
6.2 Traditional Tools
187
Table 6-2 Traditional Tools
Tool
Type
Description
uptine
Kermel statistics
Shows load averages and system uptime
top
Kemel statistics
Shows CPU time by process and CPU mode times system-wide
mpstat
Kemel statistics
Shows CPU mode time by CPU
perf
Kermel statistics,
ssgels juna pue saoen yoels jo (sudes pan) sagoid
hardware statistics,
and tracing of PMCs, tracepoints, USDT probes, kprobes, and
event tracing
uprobes
Ftrace
event tracing
Kermel statistics,
kprobes and uprobes
Reports kernel function count statistics and event tracing of
The following sections summarize key functionality of these tools. Refer to their man pages and
other resources, incluxling Systeoms Performance [Gregg 13b], for more usage and explanations.
6.2.1Kernel Statistics
Kernel statistics tools use statistical sources in the kernel, often exposed via the /proc interface.
An advantage of these tools is that the metrics are usually enabled by the kernel, so there is little
additional overhead in using them. They can also often be read by non-root users.
Load Averages
uptime(I1) is one of several commands that print the system load averages:
uptime
00:34:10 up 6:29,  1 user,  1oad average: 20.29, 18.90, 18.70
The last three numbers are the 1-, 5-, and 15-minute load averages. By comparing these numbers,
you can determine whether the load has been increasing, decreasing, or steady during the past
15 minutes or so. This output is from a 48-CPU production cloud instance and shows that load is
increasing slightly when comparing 1-minute (20.29) to 15-minutes (18.70) load averages.
The load averages are not simple averages (means) but are exponentially damped moving sums,
and reflect time beyond 1, 5, and 15 minutes. The metrics that these summarize show demand
on the system: tasks in the CPU runnable state, as well as tasks in the uninterruptible wait state
[72]. If you assume that the load averages are showing CPU load, you can divide them by the CPU
count to see whether the system is running at CPU saturation, which would be indicated by a
ratio of over 1.0. However, a number of problems with load averages, incluxling their inclusion of
uninterruptible tasks (tasks blocked in disk I/O and locks) cast doubt on this interpretation, so
they are only really useful for looking at trends over time. You must use other tools, such as the
BPF-based offcputime(8), to see if the load is CPU or uninterruptible time based. See Section 6.3.9
for information on offcputime(8) and Chapter 14 for more on measuring uninterruptible I/O.
---
## Page 225
188
Chapter 6 CPUs
top
The top(1) tool shows top CPU-consuming processes in a table of process details, along with
a header summary of the system:
5 top
top -
00:35:49 op
6:31,
*asn [
1oad average: 21.35, 19. 96, 19.12
Tasks: 514 tota1,
1 running, 288 sleeping,
@pu(8): 33.2 us
0 stopped,
 0 zonbie
1.4 5yr
0.0 ni, 64.9 id,
0.0 vs, 0.0 hi, 0.4 si, 0.0 st
KIB Men : 19382528+total,
1099228 free,
8503712 buff/cache
KiB Sxap:
0 totsl,
0fret,
0 used.
7984072 avail Mem
PID USER
PBRNI
VIRT
RES
SHR S
CPU (MEM
TIME+ COMMAND
&XA 909E
20
0
0.197t 0.170t
38776 S
1681 94.2
7186:36 Java
5737 snnp
20
22712
6676
4256 S
0.70.0
0:57,96 smnp-pss5
403root
20
0
0 I
0.3
0.0
0 : 00 .17 kvorker/41 : 1
4003 686
20
0
9916
128
0 S
0.30.0
pbu3 56°6T
66ox6a 55562
20
0
41020
4224
3072 R
0.3
0. 0
0:00.11 to
1 root
20
Q
225308
8988
6656 s
0.0
0 . 0
0: 03.09 systend
2root
20
0
D
0 S
0.00.0
0:00,01 kthreadd
[...]
This output is from a prodluction instance and shows only one process that is CPU busy: A java
process that is consuming a total of 1681% CPU, summed across all CPUs. For this 48-CPU system,
the output shows that this java processis consuming 35% of owerall CPU capacity. This concurs
with the system-wide CPU average of 34.6% (shown in the header summary: 33.2% user and 1.4%
system).
top(1) is especially useful for identifying issues of CPU load by an unexpected process. A common
punog Ajsea st qonupm doog anugu ue u xonqs atuooaq o pearq e sasneo Snq aremos jo adk
using top(1) as a process running at 100% CPU. Further analysis with profilers and BPF tools can
confirm that the process is stuck in a loop, rather than busy processing work.
top(1) refreshes the screen by default so that the screen acts as a real-time dashboard. This is a
problem: Issues can appear and then disappear before you are able to collect a screenshot. It can
be important to add tool output and screenshots to ticketing systems to track work on perfor-
mance issues and to share the information with others. Tools such as pidstat(1) can be used to
aq osje Aeu ssaooud q aesn nd asodand su aop a8esn nd ssaoud po ndno Buou uud
already recorded by monitoring systems, if they are in use.
There are other top(1) variants, such as htop(1), that have more customization options.
auouad ueu sqeu spuauauequa [ensta uo smog squeea (t)do Aureu pageunogu
metrics, making them prettier but unable to shed light on issues beyond the original top(1).
Exceptions include tiptop(1), which sources PMCs; atop(1), which uses process events to
display short-lived processes; and the biotop(8) and tcptop(8) tools, which use BPF (and which
I developed)
---
## Page 226
6.2 Traditional Tools
189
mpstat(1)
mpstat(1) can be used to examine per-CPU metrics:
 mpstat -P ALL 1
Linux 4 .15, 01027=avs(ap=...)
01/19/2019
_x86_64_
(48 CPO)
12 :47: 47 
CPO
5usr
In1ce
Isys
siova1t
5.1q
lsoft tsteal 
Sguest tgnicelidle
12 :47:48 
s11
35.25
0 .00
1, 47
0., 00
0. 00
0 , 46
0,00
0.00
0.0062,82
12 :47: 48 AX
44.55
0 .00
1. 98
0.00
0.00
66*0
0.00
0.00
0.0052.48
12:47:48 
1
33.66
0 .00
1. 98
0.00
0.00
0,00
0,00
0.00
0.0064.36
12:47:48 AX
2
30 .21
00° 0
2.08
0. 00
0.00
0.00
0,00
0.00
0.0067,71
12:47:48 
3
31.63
0 .00
1. 02
0.00
0.00
0,00
0.00
0 .00
0.0067,35
12 :47:48 AX
4
26.21
00° 0
0.00
0.00
0.00
0.97
0,00
0.00
0.00
72,82
12 :47: 48 M
5
66*89
0 . 00
1, 94
0. 00
0.00
3,88
0,00
0 .00
0.0025.24
12 :47:48 AX
6
26.26
0 .00
EO′E
0.000.00
0.00
0.00
0.00
tc*0C00°0
12 :47: 48 M
732.67
0.001.98
0.000.00
1.98
0,00
0.00
0.0063.37
[..-]
This output has been truncated because on this 48-CPU system it prints 48 lines of output per
second: 1 line to summarize each CPU. This output can be used to identify issues of balance,
where some CPUs have high utilization while others are idle. A CPU imbalance can occur for a
number of reasons, such as misconfigured applications with a thread pool size too small to utilize
all CPUs; software limits that limit a process or container to a subset of CPUs; and software bugs.
Time is broken down across the CPUs into many modes, including time in hard interrupts (%irq)
pue (g)sbupreq ag Susn pape8gsanu auung aq ueo asau (gos)) sqdnuaqu gos u atug pue
softirqs(8) BPF tools.
6.2.2Hardware Statistics
Hardware can also be a useful source of statistics—especially the performance monitoring coun-
ters (PMCs) available on the CPUs. PMCs were introduced in Chapter 2.
perf(1)
Linux perf(1) is a multitool that supports different instrumentation sources and presentations
of data. First added to Linux in 2.6.31 (2009), it is considered the standard Linux profiler, and
its code can be found in the Linux source code under tools/perf. I’ve published a detailed guide
u sNd asn o Aqe au s saqedeo [ngamod Kuu sq Buouy [&] μad asn o mou uo
counting mode:
---
## Page 227
190
Chapter 6 CPUs
$ perf stat -d gzip file1
Performance counter stats for *gzip filel′:
3952. 239208task-c1ock (msec)
0.999 CPUs utilized
6context-sxitches
0.002 K/sec
0cpu-migrations
0.000 K/sec
127page-faults
0.032 K/seo
14,863,135,172 cyeles
3,761 G8z
(62.35)
Instructions
1.23 Insn per cycle
(74., 90%)
3, 876, 390, 410  branches
 980.809 M/sec
(74.909)
135, 062, 519
branch-nisses
3.48s of all branches
(74., 975)
3, 725, 936, 639
L1-dcache-1oads
942. 741 M/sec
160*5L.1
657, 864, 906
L1-dcache-load-mlsses #
17,66% of a11 L1dcache h1ts (75.16%)
50, 906,146 LIC1osds
12.880 M/sec
(50 .01$)
1, 411, 636  LLC1oad-m1sse.
2.77s of all LL=cache hits
s8°61
The perf stat command counts events specified with e arguments. If no such arguments are
supplied, it defaults to a basic set of PMCs, or it uses an extended set if d is used, as shown here.
The output and usage varies a little depending on the version of Linux you are using and the
PMCs available for your processor type. This example shows perf(1) on Linux 4.15.
Depending on your processor type and perf version, you may find a detailed list of PMCs by using
perf list:
$ perf list
[.--]
nen_load_retlred.13_hit