### Timing, Frequency, and Phase Synchronization Subsystem

The synchronization subsystem of the MAC operates in real-time, interacting with the PHY to trigger packet transmissions based on the transmission or reception of synchronization (sync) headers. It also applies the correct frequency and phase corrections at the slave access points (APs) for joint transmission. This process is a simple check that can be performed with low hardware complexity.

#### High-Level Operation

To initiate a joint transmission, the MAC at the master provides two packets to the physical layer:
1. **Synchronization Header**: Transmitted using the typical contention-based medium access.
2. **Joint Transmission Packet**: Triggered by the transmission of the first packet. The timestamp for this transmission is a fixed time after the first packet, typically a Short Inter-Frame Space (SIFS).

At each slave, the MAC examines each received packet. If the packet is a synchronization header, the MAC at each participating slave initiates a joint transmission triggered by the reception of this header. The timestamp for this joint transmission is determined by the timestamp of the received sync header, plus a fixed time (the inter-packet gap in the master minus the receive-transmit turnaround time in the slave hardware).

#### Frequency and Phase Synchronization

At each slave, the frequency and phase synchronization subsystem works in conjunction with the timing synchronization subsystem to ensure accurate joint transmission. Specifically, this subsystem examines every received packet. If the packet is a synchronization header from the master, the MAC determines the associated Carrier Frequency Offset (CFO) and Sampling Frequency Offset (SFO). It uses the channel from the sync header and compares it with the reference channel from the master to determine the initial phase correction. These parameters are then used to apply the appropriate corrections to the joint transmission packet at the slave. This process must be performed in hardware to meet the short gap between the sync header and the joint transmission, which is usually a SIFS.

### Implementation

We implemented MegaMIMO 2.0 and evaluated it in an indoor testbed. Each node in our system consists of a Zedboard connected to an Analog Devices FMCOMMS2 transceiver card. The Zedboard is equipped with a Xilinx Zynq Z-7020, which includes an ARM dual-core Cortex A9 processing system connected to an Artix family FPGA via a high-speed AXI bus.

#### Baseband System

Our baseband system, implemented in Verilog on the FPGA, supports a full-featured 802.11 a/g/n PHY layer capable of real-time operation and all 802.11 modulations and code rates. We enhanced our PHY implementation to support distributed MIMO and implemented various time-critical MAC functionalities on the FPGA. Our current implementation supports up to 4 distributed transmitters transmitting simultaneously to 4 independent clients. Table 1 shows the resource utilization of our real-time PHY and MAC implementation on the FPGA platform.

#### Higher Layer Control System

The higher layer control system, implemented in C on the ARM core, triggers channel measurement, channel updates, precoding, and interfaces with user traffic. The FMCOMMS2 board acts as an RF front-end, capable of transmitting and receiving signals in the 2.4 and 5 GHz frequency ranges. Each Zedboard is equipped with a Gigabit Ethernet interface, connecting it to an Ethernet backhaul. Figure 6 illustrates the architecture of our system.

### Evaluation

We evaluated MegaMIMO 2.0 through microbenchmarks of its individual components and integrated system results of its overall performance.

#### Testbed

We conducted evaluations in an indoor testbed emulating a typical conference room or lounge area. APs were deployed high on the walls near the ceiling, and clients were placed at or near floor level. The environment included furniture, pillars, and protruding walls, creating rich multipath and both line-of-sight and non-line-of-sight scenarios. Experiments were conducted in the 2.4 GHz band, channel 10 (center frequency 2.457 GHz, 20 MHz bandwidth), using the 802.11n protocol.

#### Compared Systems

We compared MegaMIMO 2.0's performance with a traditional 802.11 system and distributed MIMO systems based on explicit channel feedback. In prior distributed MIMO systems, channel estimation occurs sequentially, with one antenna from the lead AP providing a reference for measurements. These measurements are corrected for phase rotation across time, inferred from the reference channel measured from the lead AP's antenna.

For traditional 802.11, we assumed the standard carrier sense-based medium access protocol, allowing one transmitter at any given time.

#### Metrics

The key metrics compared were:
- SNR after beamforming of received packets at the client
- Total network throughput (in Mbps)
- Individual throughput at each client (in Mbps)

### Results

#### Accuracy of Reciprocity

We evaluated a 2-transmitter, 2-receiver system in a static environment with 90% downlink traffic. We compared two scenarios:
1. **Explicit Channel Feedback**: APs transmit packets to clients and receive explicit channel feedback.
2. **Reciprocity Protocol**: APs use reciprocity to infer downlink channels without explicit feedback.

Figure 7 shows that MegaMIMO 2.0's reciprocity-based channel estimation performs as well as explicit channel feedback across the entire range of SNRs, indicating that distributed MIMO systems can safely use reciprocity to avoid the overhead of explicit channel feedback.

#### Need for AGC Calibration

A key feature of MegaMIMO 2.0 is its ability to calibrate for Automatic Gain Control (AGC) phase and magnitude impact on a per-packet basis. We evaluated a 4-transmitter, 4-receiver system in a static environment, performing distributed MIMO beamforming using reciprocity-based channel estimation. The network had both uplink and downlink traffic, with 10% uplink load.

We compared three configurations:
1. **Full-Fledged MegaMIMO 2.0 with AGC and Full Calibration**
2. **MegaMIMO 2.0 with AGC and Only Magnitude Calibration**
3. **MegaMIMO with Fixed Gain**

Figure 8 shows that distributed MIMO requires AGC with full calibration to achieve high gains with low variance.

### Conclusion

MegaMIMO 2.0 effectively leverages reciprocity and AGC calibration to enhance the performance of distributed MIMO systems, achieving comparable SNR and throughput to systems using explicit channel feedback while reducing overhead.