No
Received(cid:3)
Sync(cid:3)Header(cid:3)
from(cid:3)
Master?
Yes
Trigger(cid:3)Transmission(cid:3)of(cid:3)(cid:3)
Data(cid:3)Packet(cid:3)by(cid:3)Reception(cid:3)
of(cid:3)Sync(cid:3)Header(cid:3)with(cid:3)
correct(cid:3)timestamp,(cid:3)phase,(cid:3)
CFO,(cid:3)SFO(cid:3)correction
Transmit(cid:3)Sync(cid:3)
Header
Transmitted(cid:3)
Sync(cid:3)
Header?
No
Yes
Trigger(cid:3)Transmission(cid:3)of(cid:3)(cid:3)
Data(cid:3)Packet(cid:3)by(cid:3)
Transmission(cid:3)of(cid:3)Sync(cid:3)
Header(cid:3)with(cid:3)correct(cid:3)
timestamp
Figure 5: Timing, Frequency and Phase Synchro-
nization Subsystem. This subsystem of the MAC op-
erates in real-time. It interacts with the PHY and trig-
gers transmission of packets based on transmission or
reception of sync headers. It also applies the correct fre-
quency and phase correction at the slave APs for the
joint transmission.
ter. This is a simple check and can be performed with
low hardware complexity.
At a high level, to initiate a joint transmission, the
MAC at the master provides two packets to the physi-
cal layer. The ﬁrst packet is the synchronization header,
which is transmitted using the typical contention based
medium access. The second packet is the joint transmis-
sion, whose transmission is triggered by the transmission
of the ﬁrst packet. The timestamp of this second trans-
mission is a ﬁxed time after the transmission of the ﬁrst
packet, say a SIFS.
At each slave, the MAC examines each received
packet. If the received packet is a synchronization
header, the MAC at each slave participating in the joint
transmission then initiates a joint transmission triggered
by this matching reception. The timestamp for this joint
transmission is determined by the timestamp of recep-
tion of the synchronization header, and like in the mas-
ter, is computed as a ﬁxed time after the previous recep-
tion (speciﬁcally, it is the inter packet gap in the master
less the receive-transmit turnaround time in the slave
hardware).
Frequency and Phase Synchronization Subsys-
tems: At each slave, this subsystem operates jointly
with the timing synchronization subsystem to ensure
correct joint transmission. Speciﬁcally, at each node,
this subsystem examines every received packet. If the
received packet is a synchronization header from a mas-
ter, the MAC determines the associated CFO and SFO
Applications
Linux(cid:3)Drivers
Processing(cid:3)System
Programmable(cid:3)Logic
Config
Registers
ADC(cid:3)Serial(cid:3)Link
MegaMIMO 2.0(cid:3)
DAC(cid:3)Serial(cid:3)Link
Compatible(cid:3)
802.11n(cid:3)PHY
DMA
Figure 6: Platform Architecture The ﬁgure shows
the software-hardware architecture of our platform. The
PHY on the FPGA implements an 802.11n MIMO sys-
tem as well as the real time synchronization facilities
needed for distributed MIMO. The software on the ARM
core conﬁgures the PHY and manages data transfer to
and from the device.
Resource
Slice Registers
LUTs
Used Utilization (%)
49492
43475
46.52
81.72
Block RAMs 36Kb
DSP48 (multipliers)
28
45
20
20.45
Table 1: FPGA Utilization on Xilinx Zynq Z7020.
This table shows the utilization of diﬀerent FPGA ele-
ments by our real-time PHY and MAC implementation.
of that master. Further, it uses the channel from the syn-
chronization header, and compares it with the reference
channel from that master to determine the initial phase
correction to be applied to the joint transmission. It uses
these parameters to apply the appropriate correction to
the joint transmission packet at the slave. It is worth
mentioning that this process needs to be performed in
hardware in order to meet the short gap between syn-
chronization header and the joint transmission, which is
usually a SIFS.
7.
Implementation
We implement MegaMIMO 2.0 and evaluate it in an
indoor testbed.
Each node in our system consists of a Zedboard con-
nected to an Analog Device FMCOMMS2 transceiver
card. The Zedboard is equipped with a Xilinx Zynq Z-
7020, which consists of an ARM dual-core Cortex A9
processing system connected to an Artix family FPGA
via a high-speed AXI bus.
We implement our baseband system in Verilog on the
FPGA. Our baseband consists of a full-ﬂedged 802.11
a/g/n PHY layer that can operate in real-time and sup-
port all the 802.11 modulations and code rates on the
FPGA. We enhance our PHY implementation to sup-
port distributed MIMO as described in the previous
sections, and also implement various time critical MAC
functionalities on the FPGA. Based on the size of the
FPGA, our current implementation supports up to 4 dis-
tributed transmitters transmitting simultaneously to 4
independent clients. Table 1 shows the resource utiliza-
tion of our real-time PHY and MAC implementation on
our current FPGA platform.
We also implement the higher layer control system
that triggers channel measurement, channel updates,
precoding, and interfaces with user traﬃc in C on the
ARM core. The FMCOMMS2 board acts as an RF
front-end capable of transmitting and receiving signals
in the 2.4 and 5 GHz frequency ranges. Each Zedboard
is equipped with a Gigabit Ethernet interface through
which it is connected to an Ethernet backhaul. Fig. 6
shows the architecture of our system.
8. Evaluation
We evaluate MegaMIMO 2.0 both through mi-
crobenchmarks of its individual components, and inte-
grated system results of its overall performance.
(a) Testbed: We evaluate MegaMIMO 2.0 in an
indoor testbed that emulates a typical conference room
or lounge area. The APs are deployed high up on the
walls near the ceiling as is typical in these environments.
The clients are deployed at or near the ﬂoor level. The
environment has furniture, pillars, protruding walls etc.
that create rich multipath, and line of sight and non
line of sight scenarios. We evaluate our system under
both static and mobile conditions. All our experiments
are conducted in the 2.4 GHz band, channel 10 (center
frequency 2.457 GHz and 20 MHz bandwidth), using the
802.11n protocol.
(b)
Compared
compare
MegaMIMO 2.0’s performance both with a tradi-
tional 802.11 system and distributed MIMO systems
based on explicit channel feedback.
Systems: We
Note that in prior distributed MIMO systems, chan-
nel estimation happens in a sequential process, where at
each time, the channel from each AP antenna to each
client is measured jointly with one antenna of the lead
AP. The reason for including one antenna from the lead
AP in every measurement is to provide a reference to re-
late the measurements to each other even though they
are performed at diﬀerent times. These measurements
are then corrected to account for phase rotation across
time, which can be inferred from the phase changes in
the reference channel measured from the antenna of the
lead AP. The corrected measurements can then be used
as the downlink channel estimates for beamforming. The
details are described in [11].
For traditional 802.11, we assume the standard car-
rier sense based medium access protocol that allows one
transmitter to transmit at any given time.
(c) Metrics: The metrics of interest that we com-
pare are: the SNR after beamforming of the received
packets at client, the total network throughput (in
Mbps), and the individual throughput at each client (in
Mbps). Depending on the experiment, we compare one
or more of these metrics in diﬀerent scenarios.
421
)
B
d
(
R
N
S
y
t
i
c
o
r
p
c
e
R
i
 35
 30
 25
 20
 15
 10
 5
 0
 0
 5
 25
 10
Explicit Feedback SNR (dB)
 15
 20
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
k
r
o
w
t
e
N
140
120
100
80
60
40
20
0
 30
 35
No AGC
AGC without
calibration
AGC with
calibration
7:
and
with
SNR
reciprocity
Figure
(MegaMIMO 2.0)
feedback
(MegaMIMO) based distributed MIMO sys-
tems. The 45o degree line is shown in dotted black.
The ﬁgure shows that reciprocity based distributed
MIMO can achieve the same SNR as explicit feedback
across the range of SNRs.
explicit
8:
comparison
Throughput
Figure
of
MegaMIMO 2.0 with full AGC calibration,
MegaMIMO 2.0 using AGC without calibration,
and MegaMIMO (ﬁxed gain). The ﬁgure shows
that distributed MIMO needs the use of AGC with
full calibration in order to achieve high gains with low
variance.
8.1 Accuracy of Reciprocity
8.2 Need for AGC calibration
Reciprocity eliminates the overhead of channel feed-
back, which tends to be excessive in distributed MIMO
systems (see Fig. 1). However, would reciprocity lead to
a degradation in MIMO gains in comparison to using
channel feedback? In this section we answer this ques-
tion by evaluating whether the channels inferred via reci-
procity are as eﬀective at delivering MIMO beamform-
ing as the channels measured at the clients and explicitly
sent to the access points –i.e., explicit channel feedback.
Method: We evaluate a simple 2-transmitter, 2-
receiver system in a static environment. The network has
both downlink and uplink traﬃc with 90% of the traf-
ﬁc being on the downlink. We evaluate two scenarios: 1)
The APs transmit packets on the downlink to the clients
and receive explicit channel feedback from the clients,
as in MegaMIMO. 2) The APs apply MegaMIMO 2.0’s
reciprocity protocol and use the clients’ data transmis-
sions to infer the downlink channels without any ex-
plicit feedback. We use both these explicit downlink
channels, and estimated downlink channels to perform
beamformed transmissions to the clients. We interleave
the beamforming measurements using explicit feedback
with those using reciprocity to ensure that the two com-
pared methods experience similar channels. We then
compare the SNR of these beamformed transmissions
at the clients in the two scenarios. We repeat the exper-
iment across a variety of locations, and the entire range
of 802.11 SNRs, from 5-30 dB.
Results: Fig. 7 shows a plot of the SNR achieved
with reciprocity as a function of the SNR achieved with
explicit channel feedback for each topology. As the graph
shows, MegaMIMO 2.0’s reciprocity based channel es-
timation performs as well as explicit channel feedback
through the entire range of SNRs. This means that dis-
tributed MIMO systems can safely use the reciprocity
technique developed in this paper to avoid the excessive
overhead of explicit channel feedback.
A key feature of MegaMIMO 2.0 is its ability to en-
able the use of AGC by calibrating for AGC phase and
magnitude impact on a per-packet basis both in hard-
ware and software. In this section, we evaluate the im-
portance of this calibration.
Method: We evaluate a 4-transmitter, 4-receiver
system in a static environment. The network performs
distributed MIMO beamforming from all 4 transmitters
to all 4 receivers using reciprocity based channel estima-
tion. The network has both uplink and downlink traﬃc
with uplink traﬃc accounting for 10% of the load. The
ﬁrst is full-ﬂedged MegaMIMO 2.0 with AGC running
on all nodes, and both hardware and software calibra-
tion. The second is MegaMIMO 2.0 with AGC running
on all nodes, but with only magnitude calibration and