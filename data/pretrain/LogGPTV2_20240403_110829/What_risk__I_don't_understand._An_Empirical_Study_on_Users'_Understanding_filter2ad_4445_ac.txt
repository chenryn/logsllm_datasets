our accuracy rate was 99.6%.
Figure 6: The proportions of harder terms (blue) and easier
terms (red) in 15 categories of the SC Corpus.
Manager) or algorithms (e.g., ‘Grovers algorithm’) are rarer but are
considered as most difficult, with more than 65% terms that are
harder understood in each category.
We investigated the “age” of the terms, that is how long have the
terms been used in language. To do this, we searched for the years
the terms were coined in different categories of the SC Corpus.
Three researchers did this work, and majority voting [39] was used
to determine the year. The researchers were required to search each
term online to infer the year based on the context of the term. For
example, the term ‘worm’ is ambiguous, but it mostly represents
malware computer program in our security articles, so the year
the term was coined is when it was first used to describe malware
instead of animal ‘worm’. Appendix A.1 explains the empirical
CDFs (Cumulative Distribution Function) for 7 of our categories.
We analysed the correlation between the years the security terms
were coined and the difficulty levels annotated by people with differ-
ent IT backgrounds in various categories. We calculated the Pearson
Correlation Coefficient (Pearson’s r) to measure the strength of the
correlation between the two variables. Pearson’s r ranges from −1
to +1. A value of 0 means that there is no correlation. The results
show that people with IT background find it harder to understand
the newer terms related to programming, software development
and threat/attack (r = 0.32, 0.47, 0.33 respectively). People without
IT background have greater difficulty with recently coined terms
in hardware and computer role (r = 0.34, 0.52 separately), while
old algorithms and technologies are easier for them to understand.
RQ2. Are traditional methods useful in measuring difficulty levels
of technical terms?
We explored the difference between users’ understanding levels
of security concepts and existing termhood measures. We compared
the difficulty levels annotated by crowdworkers to the termhood
calculated by the traditional measures [12, 19, 26] and the mea-
sures based on their occurrences in Google Search. Those measures
calculated the terminological degrees based on the term frequen-
cies relative to their frequencies in a general language corpus. W R
(weirdness ratio) [12] was applied to compute the termhood for our
Analysis of Categories. We analysed the relationship between
labelled difficulty levels and IT background using the chi-squared
(χ2) test. We only kept the terms annotated by both IT and non-
IT groups for background comparison, representing 53.47% of the
whole corpus. For each term, we calculated the mean values of
difficulty levels for IT and non-IT groups separately. We find the
ratios of harder terms (di f f iculty > 5) in different categories are
not related to IT background (χ2 = 0.101, p > 0.999). The means
of difficulty levels in different categories are also not related to IT
background (χ2 = 0.285, p > 0.999).
Fig.6 depicts the proportions of different categories in our SC Cor-
pus annotated by all the workers, regardless of their IT background.
Each category is further divided into two parts: terms that are hard
to understand (di f f iculty > 5) and terms that are more easily un-
derstood (di f f iculty 5)easier terms (difficulty<=5)for each term in each group respectively. For each category, we con-
ducted a Mann-Whitney U test to measure the difference in average
difficulty levels between people with different IT backgrounds.
As shown in Fig.7, only 6 out of 15 categories show a statistical
difference in difficulty levels annotated by people with and without
IT background. The terms in the general category are considered
to be the easiest by both groups. It is more challenging for people
without IT background to comprehend technical terms in some
specific fields (e.g., cryptography/authentication, software devel-
opment). These particular terms may require further training or
education to understand the concepts they represent. We observe
that people without IT background are more likely to give lower
difficulty levels for the terms they are familiar with, to distinguish
them from harder terms, compared to people with IT background.
Technically savvy people labelled the terms related to program-
ming as having a higher difficulty level, potentially because they
experienced difficulty in applying the related technique in their
work. To our surprise, the terms in computer security techniques
(measures) and threat/attack are considered harder by people with
IT background than those without IT background. This means that
having an IT background does not help people understand cyberse-
curity concepts, and people with IT background are not at lower
security risk than those without IT background. This needs further
invigoration.
RQ4. What functions would users like to help them read technical
articles?
To explore what functions can provide reading assistance, we
used the open card sorting [56] again to classify their comments (597
responses) from our survey. Three researchers did the classification
with majority voting [39]. Fig.12(A) in Appendix B depicts the
proportions of different functions suggested by our participants,
where the majority (65%) of people would like to use a dictionary-
based tool. 27% of the users felt it was not necessary to have reading
assistance. A few participants also raised the need for difficult term
detection or highlight in articles and audio assistance to read or
pronounce some particular words.
As most users would like a tool to provide definitions or de-
scriptions for the technical terms (i.e., a dictionary-based tool), we
further analysed the different methods users mentioned to provide
explanations. As shown in Fig.12(B) in Appendix B, the vast ma-
jority (81.2%) of the users explicitly described their preference in
functions of pop-ups (to hover and define difficult terms) and the
dictionary (to provide definitions for lookup). Others also suggested
the use of hyperlinks, such as external links to a Wikipedia page
or detailed explanations including textual descriptions, videos, and
graphics. Similar to the dictionary, a glossary of various acronyms
and jargons was also suggested.
• “I would like to have a tool to help read an article like this. The ideal
features I would look for will be a feature like Kindle’s dictionary. If
I long-press or hover over the word, there should be brief info about
the word.”
• “A built-in browser tool that defines terms or links to the context
We list some representative comments below:
within annotations might be useful.”
Figure 7: Distribution of difficulty levels in different cat-
egories. The figure only includes the six categories with
a significant difference (p < 0.05) between difficulty lev-
els annotated by people with and without IT background.
They are computer security technique/monitoring (Com-
SeTech), cryptography/authentication (Cryto), general, pro-
gramming/command/operation (Prgm), software develop-
ment (SwDev), and threat/attack (Threat).
comparison, which represents the quotient of relative frequency
in our corpus and a general language corpus (CLEF 2004 [41]). We
also developed a crawler to retrieve the occurrences of our tech-
nical terms in Google. A higher amount of Google search queries
indicates a higher probability that people have seen these words,
which corresponds to lower difficulty levels.
We further tested the significance of the difference between the
annotated difficulty levels and these two measures (traditional ter-
mhood and occurrences in Google search), respectively. As our
results had different scales and correlations from the two measures,
we normalised the values to aid comparison by z-score standardi-
sation [14]. We inversely transformed the occurrences in Google
search before standardisation, since they correlated negatively to
our results. The p − values (< 0.05) of a Mann-Whitney U test indi-
cate both measures are significantly different from our annotated
results. Pearson’s r (without transformation) shows a similar trend,
and the values are −0.017 (our result vs. traditional termhood) and
−0.223 (our result vs. occurrences in Google search).
RQ3. Are the technical terms as difficult for people with IT back-
ground as they are for those without IT background?
We reviewed the collected terms and their difficulty levels an-
notated by people with and without IT background separately to
investigate the differences among their comprehensions. As anal-
ysed in RQ1, the difficulties of the categories in our SC Corpus are
unrelated to workers’ IT background.
We further analysed the difference between the difficulties of
the terms annotated by people with and without IT background in
each category. We only kept the terms annotated by both groups for
background comparison. We calculated the average difficulty level
shown in Fig.1. To test the effectiveness of the dictionary, we de-
veloped a service as an add-on tool. From the first experiment, we
found that around 65% of the participants would like to have a
dictionary to obtain an explanation of the meanings of the terms.
Therefore, we developed a security-centric assistant that automat-
ically detects technical terms and shows pop-up descriptions for
them. This experiment aims to answer the research questions below
regarding the effectiveness of such tools or services in promoting
users’ understanding.
RQ5. How much does our tool help users understand security texts?
RQ6. What influences users’ comprehension of security texts?
RQ7. Does having IT background help understand security texts
better?
4.1.1 Making a Security-Centric Dictionary. Some prior efforts have
been made to display short descriptions for difficult words (e.g.,
by Wikipedia Page Previews [59], Google Dictionary [22] and Mac
Dictionary [4]). These tools open pop-ups with meanings triggered
by text hovering or clicking to help users understand unfamiliar
words without the need to open multiple tabs.
By connecting to their provided APIs, we implemented a program
in Python to look up the definitions of the corpus terms in their
dictionary. Only 27% of the corpus terms sent to Wikipedia Page
Previews returned results. Google Dictionary performed slightly
better, with a percentage of 35%. The Mac built-in dictionary had
less technical knowledge in cybersecurity, and only returned 17%
of the definitions. We concluded that the state-of-the-art tools did
not perform well on the SC Corpus as they were designed mainly
for common words. Therefore, a security-centric dictionary was
needed for technical articles.
To build such a dictionary, we implemented a crawler in Python
to return the query results of searching term meanings. For ambigu-
ous terms, we also combined them with specific keywords such as
‘computing’ or ‘security’ to refine the definitions. Some external
websites, during the Google search, also provided supplementary
resources. For example, the technology-specific websites, such as
Whatis5, provide technical definitions in IT.
We collected the meanings and image URLs for the terms in the
SC Corpus from all available online sources and manually selected
the most accurate meaning(s) for each term. For the terms with only
textual descriptions, we added a default image too. As a result, we
obtained the SC Dictionary, which provided descriptive definitions
as well as images for the whole corpus.
The dictionary was leveraged as a knowledge base for our tool.
Each term in the dictionary was saved as a JSON file, along with its
details (e.g., meaning, image URL, image resolution). We stored the
data in an accessible server so that they can be retrieved through
‘GET’ requests over HTTP.
Figure 8: Two factors show significant differences between
the participants who did not need a tool to help read security
texts and other participants who preferred a tool.
• “A simple tool that linked to the Wikipedia page or some other article
when clicking on terms would be useful. That way if I wanted to
learn more, it would be simple to do so.”
We further explored the reasons why 27% of the participants
did not need a tool to assist their comprehension. We compared
these participants to the rest who preferred an aid tool in terms of
their demographics and labelling behaviours. The demographics
we collected include gender, age, education, IT background, and
whether they are native speakers. Only IT background shows a
statistically significant difference between the numbers of people
in the two groups (χ2 = 18.563, p < 0.001). Additionally, we
calculated the average number of annotations per article and the
average difficulty level per annotation for each participant and
applied these two measures as labelling behaviours to compare
the two groups again. A weak difference is measured with t-test
in labelled difficulties (t = −1.664, p = 0.097). As shown in Fig.8,
the users who did not need a tool to help read security texts has
a significantly higher proportion with IT background and tend to
give fewer annotations and lower difficulty for technical terms,
compared to the rest users who would like a tool.
stand;
Based on Experiment 1, we conclude:
• The majority of the technical terms are hard for users to under-
• Traditional readability tests fail to provide consistent difficulty
• People with IT background give higher difficulty levels for the
technical terms related to cyber threats and protection measures
than people without IT background;
• Most users would like a dictionary-based aid tool to help read
levels with users’ reported ones;
security texts.
4 EXPERIMENT 2: THE EFFECT OF AID
TOOLS ON USERS’ UNDERSTANDING OF
SECURITY TEXTS
4.1 Setup
In this section, we present the generation of a security-centric dic-
tionary (SC Dictionary) as a proof of concept for the framework
4.1.2 Making a Security-Centric Assistance Tool. We built our tool
on top of SC Dictionary and implemented it as an extension/add-
on in the user interface to provide meanings automatically. The
extension highlighted the technical terms and used pop-up win-
dows to show the meanings. It was developed with JavaScript and
5https://whatis.techtarget.com/
Table 1: Three reading tasks of nine articles and their mentioned security threats.
Task Article Discussed Security Issues
#questions in Threats / Protection
11/4
6/9
9/6
1
2
3
1
2
3
1
2
3
1
2
3
connected car vulnerability, DoS attack, Trojanized apps, bug