as an anomaly score. The accuracy score indicates how the BGL system logs, the Thunderbird system logs
effectively LogFiT can restore the masked sentences in the contain data about the activities and events that occurred
input data. If the model’s top-k accuracy falls below a certain within the supercomputer system. The dataset has also
threshold (ranging from 0.8 to 0.99 in the experiments), the been labeled in the same manner as the BGL data
log paragraph is classified as anomalous, while a score above set. As with the BGL dataset, the Thunderbird dataset
the threshold indicates that it is normal. does not have a session ID, so the log sentences are
6
Dataset AvgWordCount AvgSentenceCount #UniqueWords thepaper.Itisworthnotingthattheperformancemetrics
HDFS 176.04 18.63 146 reported in the LogBERT paper cannot be reproduced.
BGL 128.66 15.73 6046
Thunderbird 1445.70 126.63 15557 Implementation Details. LogFiT was implemented using
TableI Python and leveraged several well-known libraries to
PER-PARAGRAPHWORDANDSENTENCESTATISTICSFORTHEDATASETS. accelerate the development and evaluation of the model.
• Pre-processingoftherawlogdatawasdoneusingsimple
regular expressions to replace unimportant (because they
similarly chunked into paragraphs, each consisting of the are too specific) details such as IP addresses and port
consecutive sentences belonging to a time window of 30 numbers, numeric values, file paths, block IDs (and
seconds, following the approach discussed in [5], [7]. other types of identifiers). Python and its extensive data
Furthermore,alogparagraphisconsideredananomalyif analysis tools were used for this task.
it includes at least one log sentence that is tagged as an • Pytorch - Pytorch is a Python-based deep learning
anomalousevent.ThefullThunderbirddatasetconsistsof framework developed by Facebook Research. Pytorch
10,000,000 log sentences, of which 4,934 are anomalies. allows researchers to build and train complex neural
network models using a Pythonic syntax that is intuitive
For each of the datasets mentioned, a total of 5,000 log
and easy to use. LogFiT uses Pytorch as interface to the
paragraphs were used to train LogFiT (sampled from 10,000
underlying hardware-accelerated tensor runtime.
log paragraphs set aside for training). A separate tuning set
consisting of 4,500-5,000 normal plus 500-1,000 anomaly • FastAI - FastAI is a high-level deep learning framework
that has been designed to run on top of Pytorch. This
paragraphs were used to tune the LogFiT model’s hyper
frameworkfocusesonefficientmodeltrainingworkflows
parameters. Furthermore, a separate evaluation set consisting
and incorporates the latest deep learning techniques and
of4,500-5,000normaland500-1,000anomalysampleswasset
best practices. LogFiT uses FastAI to implement its
aside to evaluate the performance of the tuned LogFiT model.
trainingandevaluationstrategiesinconcisePythoncode.
No random shuffling is done when splitting the datasets into
subsets - the sequential order of the logs is preserved; this is • HuggingFace - HuggingFace is a Python library and
ecosystem for building and sharing Transformers-based
to prevent information leakage during model training. Lastly,
models. LogFiT uses the HuggingFace library to jump
to test the models’ ability to handle variation in the syntactic
start model development by inheriting from pre-trained
structureoftheinputlogdata,theevaluationsetisdynamically
Transformer models available from the HuggingFace
modified during model evaluation, so that the top 10% most
model hub.
commonly occurring verbs are replaced with their WordNet
lemmas. The source code implementing the LogFiT model, datasets
Baselines. The performance of the LogFiT model is and model checkpoints will be made available online.
compared against the following baselines. Evaluation Metrics. To evaluate the effectiveness of the
models, the experiments use the following metrics:
• DeepLog [4]. The DeepLog model uses an LSTM-
based architecture to train an anomaly detection model • Precision (P) measures the proportion of correctly
identifiedanomalysamples(TP),outofalltheanomalies
using sequences of normal log templates. Anomalies are
detected by the model, and is calculated as P = TP /
detectedbylettingthemodelpredictthenextlogtemplate
(TP+FP)
given its n preceding log templates. The top-k accuracy
of the prediction is the anomaly score. The DeepLog • Recall(R)measurestheproportionofcorrectlyidentified
implementation from the logdeep library 1 was used to anomaly samples (TP) out of all real anomalies, and is
calculated as R = TP / (TP+FN).
obtain the results reported in the experiments section. It
is worth noting that the performance metrics reported • F1Score(F1)istheharmonicmeanofthePrecisionand
Recall, and is calculated as F1 = 2 * (P*R)/(P+R).
in the DeepLog paper cannot be reproduced using this
implementation of DeepLog. • Specificity (S) measures the proportion of correctly
identified normal samples (TN) out of all real normal
• LogBERT [5]. The LogBERT model uses a BERT-
samples, and is calculated as S = TN/(TN+FP).
based architecture to learn the patterns of normal log
templates, using masked log key prediction and centroid TruePositives(TP)refertothenumberofanomalysamples
distance minimisation to fit the model to the training thatwerecorrectlydetectedbythemodel.FalsePositives(FP)
data. Anomalies are detected by allowing the model to refer to the number of normal log samples that the model
predict n randomly masked log keys in a sequence, and incorrectly detected as anomaly. False Negatives (FN) are the
then using the top-k accuracy and centroid distance to anomalysamplesthatwerenotdetectedbythemodel.Finally
computeananomalyscore.Ifeitherthetop-kaccuracyor True Negatives (TN) are the normal samples that the model
centroid distance threshold is exceeded, the input data is correctly detected.
consideredananomaly.ThepubliclyavailableLogBERT In real-world deployment scenarios, having a predictive
source code 2 is used to obtain the results reported in model with high Specificity is more advantageous since it
minimises the chances of producing false positives or false
1Availablefromhttps://github.com/donglee-afar/logdeep alarms. A model with high Specificity can correctly identify
2Availablefromhttps://github.com/HelenGuohx/logbert normal samples, which makes it more likely that any detected
7
Method P R F1 S of randomly selecting tokens from the log paragraph.
DeepLog 100.0 60.90 75.70 100.0 Subsequently,theLogFiTmodelistaskedwithpredictingwhat
LogBERT 24.02 82.80 37.24 47.62 the masked tokens were. If the top-k prediction accuracy for
LogFiT(ours) 99.78 90.60 94.97 99.96
a log paragraph is below some threshold, the log paragraph
TableII
is considered an anomaly. Otherwise the log paragraph is
ANOMALYDETECTIONPRECISION(P),RECALL(R),F1SCORE(F)AND
SPECIFICITY(S)OFDIFFERENTMETHODSONTHEHDFSDATASET. considerednormal.Forthespecificexampleshowninfigure5,
k=12 and the top-k accuracy threshold is 0.9. For illustration,
the figure shows the model’s top-1 token prediction right next
Method P R F1 S
to the masked input log paragraph. The label column is the
DeepLog 90.2 70.68 79.25 98.32 ground truth for the log paragraph, the prediction column is
LogBERT 88.92 88.35 88.63 97.59
the prediction (0 or 1) based on whether the top-k prediction
LogFiT(ours) 98.83 84.70 91.22 99.00
accuracy is above (prediction: 0) or below (prediction: 1) the
TableIII
ANOMALYDETECTIONPRECISION(P),RECALL(R),F1SCORE(F)AND threshold (0.9).
SPECIFICITY(S)OFDIFFERENTMETHODSONTHEBGLDATASET. a) Centroid Distance Minimisation: The LogBERT
model uses the centroid distance minimisation training
objective to supplement masked log key prediction during
anomalies are genuine. Furthermore, Le and Zhang [7] have trainingandevaluation.Tostudytheeffectofcentroiddistance
noted that a high Specificity can help mitigate the impact of minimisationonmodelperformance,aversionofLogFiTwas
havinganimbalancedclassdistributiononthemodel’soverall trained incorporating this training objective alongside masked
performance. token predition. During training, LogFiT’s loss function was
modifiedtoincludethecentroiddistance,whichisthedistance
ofeachlogparagraph’s[CLS]vectorfromthecentroidvector
B. Experimental Results
computedfromthepreviousepoch.Theintuitionbehindthisis
Log Anomaly Detection Performance. Table II, Table III that the centroid distance loss will force the [CLS] vectors of
and Table IV show the results of running anomaly detection allnormalsamplestobeclosetoeachother.Thecentroidisthe
inference using LogFiT, as compared to the results from meanoftheCVvectorsofallnormallogsentences.Duringthe
running DeepLog and LogBERT using the available source centroiddistanceminimisationprocess,theq-quantilecentroid
code implementation. The results show that LogFiT’s F1 distance is calculated, where q is set to a value between 0.65
scores exceed that of LogBERT and DeepLog on all three and 0.9 in the experiments. This value is regarded as the
datasets,whileLogFiT’sspecificityexceedthatofthebaseline radiusofthehyperspherethatencompassesallnormalsamples
models on the HDFS and BGL datasets and is very close to and is used as the threshold during inference. The formula to
LogBERT’s on the Thunderbird dataset. The DeepLog and calculate the centroid distance loss for a mini-batch b of log
LogBERTmodelsweretrainedandevaluatedusingthesource data is shown in Equation 2.
code implementation mentioned earlier.
Figure 5 illustrates LogFiT’s anomaly decision method, b
1(cid:88)
as applied to a Thunderbird log paragraph. The input log Loss cdist = b (CV j −centroid)2. (2)
paragraph is first corrupted using a BERT-based masking j=1
scheme. In contrast to the original BERT-based masking
The loss function of the modified LogFiT model, as
scheme, LogFiT selects sentences for masking instead
expressed in Equation 3, is thus a combination of two
individual loss functions: the cross-entropy loss computed
from the masked token prediction objective and the
Method P R F1 S
centroid distance loss calculated from the centroid distance
DeepLog 65.05 99.4 78.64 89.30
minimisation objective. The hyper parameter cw is used to
LogBERT 91.75 95.7 93.69 98.28
LogFiT(ours) 89.90 98.80 94.14 97.78 weigh contribution of the centroid distance loss to the final
TableIV loss value during training. In the experiments, the value of
ANOMALYDETECTIONPRECISION(P),RECALL(R),F1SCORE(F)AND
cw is set to 0.25. The resulting composite loss function is
SPECIFICITY(S)OFDIFFERENTMETHODSONTHETHUNDERBIRD
DATASET. then optimised using the Adam optimiser, using default hyper
parameter values from the FastAI framework.
Loss=Loss +cw∗Loss . (3)
mlm cdist
The performance of the LogFiT version that incorporates
centroid distance is identical to that of the original
version which do not use centroid distance. The results
confirm that centroid distance does not effectively separate
normal from anomaly samples. Figure 6 illustrates the
Figure 5. LogFiT’s anomaly decision for a sample log paragraph from
predictions for the Thunderbird datataset. In the Thunderbird
the Thunderbird dataset, using parameters: top-k = 12 and top-12 accuracy
threshold=0.9. evaluation dataset, there were 5,000 normal samples and
8
Figure 6. Aggregate anomaly predictions for the Thunderbird evaluation
dataset,with5,000normaland1,000anomalysamples.Thepredictionsbased
on top-k accuracy are more effective than predictions based on centroid
Figure7. UMAPplotofThunderbirdsemanticvectors,wheretheblue,pink
distance. The centroid distance threshold simply classifies all samples as
andyellowcoloursofthepointsrepresenttrainingsamples,normalpredictions
normal.
andanomalypredictionsrespectively.
Method P R F1 S
1,000 anomaly samples. The aggregate results for the
normal samples are represented by the row/column labeled LogBERT 28.82 94.92 44.22 37.18
LogFiT(ours) 95.249 84.2 89.38 99.16
0, while the aggregate results for anomaly samples are
TableV
represented by the row/column labeled 1. The column labeled ANOMALYDETECTIONPRECISION(P),RECALL(R),F1SCORE(F)AND