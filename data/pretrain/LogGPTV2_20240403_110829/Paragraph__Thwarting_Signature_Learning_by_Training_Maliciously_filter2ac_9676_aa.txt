title:Paragraph: Thwarting Signature Learning by Training Maliciously
author:James Newsome and
Brad Karp and
Dawn Xiaodong Song
Paragraph: Thwarting Signature Learning by Training
Maliciously
James Newsome1, Brad Karp2, and Dawn Song1
1 Carnegie Mellon University
2 University College London
Abstract. Defending a server against Internet worms and defending a user’s
email inbox against spam bear certain similarities. In both cases, a stream of
samples arrives, and a classiﬁer must automatically determine whether each sam-
ple falls into a malicious target class (e.g., worm network trafﬁc, or spam email).
A learner typically generates a classiﬁer automatically by analyzing two labeled
training pools: one of innocuous samples, and one of samples that fall in the ma-
licious target class.
Learning techniques have previously found success in settings where the con-
tent of the labeled samples used in training is either random, or even constructed
by a helpful teacher, who aims to speed learning of an accurate classiﬁer. In the
case of learning classiﬁers for worms and spam, however, an adversary controls
the content of the labeled samples to a great extent. In this paper, we describe
practical attacks against learning, in which an adversary constructs labeled sam-
ples that, when used to train a learner, prevent or severely delay generation of an
accurate classiﬁer. We show that even a delusive adversary, whose samples are all
correctly labeled, can obstruct learning. We simulate and implement highly effec-
tive instances of these attacks against the Polygraph [15] automatic polymorphic
worm signature generation algorithms.
Keywords: automatic signature generation, machine learning, worm, spam.
1 Introduction
In a number of security applications, a learner analyzes a pool of samples that fall in
some malicious target class and a pool of innocuous samples, and must produce a clas-
siﬁer that can efﬁciently and accurately determine whether subsequent samples belong
to the target class. High-proﬁle applications of this type include automatic generation
of worm signatures, and automatic generation of junk email (spam) classiﬁers.
Prior to the deployment of such a system, samples in the target class are likely to in-
clude a number of distinguishing features that the learner can ﬁnd, and that the classiﬁer
can use to successfully ﬁlter target-class samples from a stream of mixed target-class
and innocuous samples. Before the wide deployment of automatic spam classiﬁcation,
spam emails often contained straightforward sales pitches. Likewise, as no automatic
worm signature generation system has yet been widely deployed, all instances of a
particular worm’s infection attempts contain nearly an identical payload. The ﬁrst gen-
eration of automatic signature generation systems was highly successful against these
non-adaptive adversaries.
D. Zamboni and C. Kruegel (Eds.): RAID 2006, LNCS 4219, pp. 81–105, 2006.
c(cid:2) Springer-Verlag Berlin Heidelberg 2006
82
J. Newsome, B. Karp, and D. Song
Once such a system is widely deployed, however, an incentive exists for elusive ad-
versaries to evade the generated classiﬁers. We observe this phenomenon today because
of the wide-spread deployment of spam classiﬁers. Senders of spam employ a variety
of techniques to make a spam email look more like a legitimate email, in an attempt to
evade the spam classiﬁer [6]. Similarly, while worm signature generation systems are
not yet widely deployed, it is widely believed that once they are, worm authors will
use well known polymorphism techniques to minimize the similarity between infection
payloads, and thus evade ﬁltering by worm signatures.
In the case of worm signature generation we have a signiﬁcant advantage: a worm
infection attempt must contain speciﬁc exploit content to cause the vulnerable software
to begin executing the code contained in the payload. Further, the vulnerability, not the
worm’s author, determines this speciﬁc exploit content. Newsome et al. [15] showed
that, for many vulnerabilities, messages that exploit a particular vulnerability must con-
tain some set of invariant byte strings, and that it is possible to generate an accurate
and efﬁcient signature based on this set of byte strings, even if the rest of the worm’s
payload is maximally varying—that is, contains no persistent patterns.
Unfortunately, such an elusive adversary is not the worst case. In this work, we em-
phasize that these applications attempt to learn a classiﬁer from samples that are pro-
vided by a malicious adversary. Most learning techniques used in these applications
do not target this problem setting. In particular, most machine learning algorithms are
designed and evaluated for cases where training data is provided by an indifferent entity
(e.g., nature), or even by a helpful teacher. However, in the applications under discus-
sion, training data is provided by a malicious teacher.
Perdisci et al. [18] demonstrate that it is not sufﬁcient for the learner to tolerate ran-
dom noise (mislabeled training samples) in the training data. In particular, Perdisci et
al. describe noise-injection attacks on the Polygraph suite of automatic worm signa-
ture generation algorithms [15], through which an attacker can prevent these algo-
rithms from generating an accurate classiﬁer. These attacks work by causing the Poly-
graph learner to use specially crafted non-worm samples as target-class-labeled (worm-
labeled) training data. This type of attack is of concern when the initial classiﬁer that
identiﬁes target-class samples for use in training is prone to false positives. Such an
attack can be avoided by using a sound initial classiﬁer to ensure that non-target-class
samples cannot be mislabeled into the target-class training data. In the case of auto-
matic generation of worm signatures, host monitoring techniques such as dynamic taint
analysis [16, 4, 23, 3] can prevent such mislabeling, as they reliably detect whether the
sample actually results in software being exploited.
In this work, we show that there is an even more severe consequence to training on
data provided by a malicious teacher. We show that a delusive1 adversary can manipu-
late the training data to prevent a learner from generating an accurate classiﬁer, even if
the training data is correctly labeled. As a concrete demonstration of this problem, we
analyze several such attacks that are highly effective against the Polygraph automatic
worm signature generation algorithms. We also illustrate the generality of this problem
by describing how these same attacks can be used against the Hamsa [9] polymorphic
worm signature generation system, and against Bayesian spam classiﬁers.
1 Delusive: Having the attribute of deluding, . . . , tending to delude, deceptive [17].
Paragraph: Thwarting Signature Learning by Training Maliciously
83
Our contributions are as follows:
– We deﬁne the classiﬁer generation problem as a learning problem in an adversarial
environment.
– We describe attacks on learning classiﬁer generators that involve careful placement
of features in the target-class training data, the innocuous training data, or both, all
toward forcing the generation of a classiﬁer that will exhibit many false positives
and/or false negatives.
– We analyze and simulate these attacks to demonstrate their efﬁcacy in the polymor-
phic worm signature generation context. We also implement them, to demonstrate
their practicality.
We conclude that the problem of a delusive adversary must be taken into account in
the design of classiﬁer generation systems to be used in adversarial settings. Possible
solutions include designing learning algorithms that are robust to maliciously gener-
ated training data, training using malicious data samples not generated by a malicious
source, and performing deeper analysis of the malicious training data to determine the
semantic signiﬁcance of the features being included in a classiﬁer, rather than treating
samples as opaque “bags of bits.”
We proceed in the remainder of this paper as follows. In Section 2, we deﬁne the
classiﬁer generation problem in detail. We next describe attacks against learning clas-
siﬁer generators in Sections 3 and 4. We discuss implications of these attacks, both for
worm signature generation and for spam ﬁltering, in Section 5. After reviewing related
work in Section 6, we conclude in Section 7.
2 Problem Deﬁnition: Adversarial Learning
We now elaborate on the learning model mentioned in the previous section, as followed
by Polygraph for worm signature generation, and by Bayesian systems for spam ﬁlter
generation, with the aim of illuminating strategies an adversary may adopt in an attempt
to cause learning to fail. We begin by describing the learning model, and examining the
criteria that must be met for learning to succeed. We then consider the assumptions
the learning model makes, and why they may not always hold in practice. Finally, we
describe general strategies for forcing the assumptions the model makes to be violated.
2.1 Learning Model
Identifying worms or spam so that they may be ﬁltered is at its heart a classiﬁcation
problem: we seek a classiﬁer that, given a sample, will label that sample as being of the
target class (e.g., a worm infection attempt, or a spam email) or as innocuous. One may
derive a classiﬁer automatically by learning one. Overall, learning involves initially
labeling some set of samples to train a learner, which, based on their content, generates
a classiﬁer. This process is depicted in schematic form in Figure 1.
The raw input to a learning system consists of unlabeled samples. In the case of worm
signature generation, these are individual network ﬂow payloads observed at a network
monitoring point; in the case of Bayesian spam ﬁlter generation, they are individual
84
J. Newsome, B. Karp, and D. Song
Unlabeled
Samples
Suspicious
Pool
Initial
Classifier
Labeled
Samples
Learner
Classifier
Innocuous
Pool
Fig. 1. Schematic of a learner, which uses innocuous and suspicious training pools to generate an
accurate classiﬁer
email messages arriving in a user’s inbox. Note that an adversary may inﬂuence the
content of these unlabeled samples to a varying extent; we return to this point later in
this section.
The unlabeled samples are ﬁrst labeled by an initial classiﬁer. Samples labeled as
being in the target class are placed in the suspicious pool. Samples labeled as not being
in the target class are placed in the innocuous pool. It may seem circular to begin the
process of deriving a classiﬁer with a classiﬁer already in hand. It is not. The classi-
ﬁer used to perform the initial labeling of samples typically has some combination of
properties that makes it unattractive for general use, such as great computational cost
or inaccuracy. We consider this classiﬁer used for the initial labeling of samples below.
Once these samples have been labeled, the learner analyzes the features found in the
samples in each pool, and produces a classiﬁer. Machine learning allows a very broad
deﬁnition of what may constitute a feature. In this work we focus on the case where
each feature is the presence or absence of a token, or contiguous byte string, though our
results are generalizable to other types of features.
Feedback. Note that throughout this paper, we optimistically assume that the system
uses an intelligent feedback loop. For example, if the system collects 10 target-class
samples, generates a classiﬁer, and later collects 10 new target-class samples, it gen-
erates an updated classiﬁer using all 20 samples in its suspicious pool, rather than
generating a new classiﬁer using only the latest 10. How to achieve this property is
application-speciﬁc, and outside the scope of this work. This property is crucially im-
portant, as otherwise the attacker can prevent the learner from ever converging to a
correct classiﬁer.
2.2 Successful Learning
To understand how an adversary might thwart learning, we must ﬁrst understand what
constitutes successful learning. Using labeled pools of samples, the learner seeks to
generate a classiﬁer that meets several important criteria. First, the classiﬁer should be
computationally efﬁcient; it should be able to label samples at their full arrival rate (in
the case of worm ﬁltering, at a high link speed). The classiﬁer should also exhibit no
false negatives; it should correctly classify all target-class samples as such. It should
also exhibit very few or no false positives; it should not classify non-target-class sam-
ples as being in the target class.
Paragraph: Thwarting Signature Learning by Training Maliciously
85
The learner must be able to generate an accurate classiﬁer using a reasonably small
number of labeled target-class samples. An adversary can severely undermine the use-
fulness of the system by increasing the number of labeled target-class samples necessary
to generate an accurate classiﬁer. This is especially true in the case of automatic worm
signature generation, where a worm infects ever-more vulnerable hosts while training
data is being collected.
2.3 Limitations of Initial Classiﬁer
Let us now return to the initial classiﬁer used to label samples, and the properties that
make it inappropriate for general use (and thus motivate the automated derivation of
a superior classiﬁer through learning). First, the initial classiﬁer may be too expensive
to use on all samples. For example, systems like TaintCheck [16] and the execution
monitoring phase of Vigilante [3] identify ﬂows that cause exploits very accurately, but
slow execution of a server signiﬁcantly. In the case of spam, it is most often a user who
initially labels inbound emails as spam or non-spam. Clearly, the user is an “expensive”
classiﬁer. In both these application domains, the aim is to use the expensive classiﬁer
sparingly to train a learner to generate a far less expensive classiﬁer.
In addition, the classiﬁer used to label samples initially is often error-prone; it may
suffer from false positives and/or false negatives. For example, classifying all samples
that originate from a host whose behavior ﬁts some coarse heuristic (e.g., originating
more than a threshold number of connections per unit time) risks ﬂagging innocuous
samples as suspicious. A coarse heuristic that errs frequently in the opposite direction
(e.g., classifying as suspicious only those samples from source addresses previously
seen to port scan) risks ﬂagging suspicious samples as innocuous (e.g., a hit-list worm
does not port scan, but is still in the target class).
2.4 Assumptions and Practice
Given that the initial classiﬁer is error-prone, consider the content of the two labeled
pools it produces. Ideally, the innocuous pool contains legitimate trafﬁc that exactly
reﬂects the distribution of current trafﬁc. In reality, though, it may not. First, because
the classiﬁer used in initial labeling of samples is imperfect, the innocuous pool might
well include target-class trafﬁc not properly recognized by that classiﬁer. Moreover, the
innocuous pool may contain trafﬁc that is not target-class trafﬁc, but not part of the
representative innocuous trafﬁc mix; an adversary may send non-target-class trafﬁc to
cause this sort of mislabeling. Finally, the innocuous pool may not reﬂect current trafﬁc;
it may be sufﬁciently old that it does not contain content common in current trafﬁc.
The suspicious pool is essentially a mirror image of the innocuous pool. Ideally, it
contains only samples of the target class. But as before, the ﬂawed classiﬁer may mis-
classify innocuous trafﬁc as suspicious, resulting in innocuous trafﬁc in the suspicious
pool. Additionally, an adversary may choose to send non-target-class trafﬁc in such a
way as to cause that trafﬁc (which is innocuous in content) to be classiﬁed as suspicious.
Formal proofs of desirable properties of machine learning algorithms (e.g., fast con-
vergence to an accurate classiﬁer with few labeled samples) tend to assume that the
features present in samples are determined randomly, or in some applications, that a
86
J. Newsome, B. Karp, and D. Song
helpful teacher designs the samples’ content with the aim of speeding learning. We note
that using learning to generate classiﬁers for worms constitutes learning with a mali-
cious teacher; that is, the adversary is free to attempt to construct target-class samples
with the aim of thwarting learning, and to attempt to force the mislabelings described
above to occur.
2.5 Attack Taxonomy
There are a number of adversarial models to consider. In particular, there are three
potential adversary capabilities that we are interested in:
– Target feature manipulation. The adversary has some power to manipulate the
features in the target-class samples. Some features are necessary for the target-class
samples to accomplish their purpose (e.g., successfully hijack program execution in
a worm sample, or entice the reader to visit a web-site in a spam email). There are
a variety of techniques to minimize or obfuscate these necessary features, such as
worm polymorphism. A less-studied technique that we investigate is the inclusion
of additional, spurious, features in the target-class samples, whose sole purpose is
to mislead the learner.
– Suspicious pool poisoning. The adversary may attempt to fool the initial classiﬁer,
such that non-target-class samples are put into the suspicious pool. These samples
may be specially constructed to mislead the learner.
– Innocuous pool poisoning. The adversary may attempt to place samples into the
innocuous pool. These could be target-class samples, or non-target-class samples
that nonetheless mislead the learner.
We propose two types of attack that the adversary can perform using one or more of
the above techniques:
– Red herring attacks. The adversary incorporates spurious features into the target-
class samples to cause the learner to generate a classiﬁer that depends on those
spurious features instead of or in addition to the necessary target-class features. The
adversary can evade the resulting classiﬁer by not including the spurious features
in subsequently generated target-class samples.
– Inseparability attacks. The adversary incorporates features found in the innocuous