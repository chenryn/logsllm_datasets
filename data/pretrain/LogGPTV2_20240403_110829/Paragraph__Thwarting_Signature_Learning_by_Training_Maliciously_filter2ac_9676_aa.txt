# Title: Thwarting Signature Learning by Training Maliciously

## Authors:
- James Newsome<sup>1</sup>
- Brad Karp<sup>2</sup>
- Dawn Song<sup>1</sup>

### Affiliations:
<sup>1</sup> Carnegie Mellon University  
<sup>2</sup> University College London

---

## Abstract
Defending a server against Internet worms and protecting a user’s email inbox from spam share certain similarities. In both cases, a stream of samples arrives, and a classifier must automatically determine whether each sample is malicious (e.g., worm traffic or spam). A learner typically generates a classifier by analyzing two labeled training sets: one containing innocuous samples and the other containing malicious samples.

Learning techniques have been successful in scenarios where the content of the labeled samples is either random or constructed by a helpful teacher aiming to facilitate the learning of an accurate classifier. However, in the context of worms and spam, an adversary controls the content of the labeled samples to a great extent. This paper describes practical attacks where an adversary constructs labeled samples that, when used for training, prevent or severely delay the generation of an accurate classifier. We show that even a delusive adversary, whose samples are all correctly labeled, can obstruct learning. We simulate and implement highly effective instances of these attacks against the Polygraph automatic polymorphic worm signature generation algorithms.

**Keywords:** automatic signature generation, machine learning, worm, spam

---

## 1. Introduction
In various security applications, a learner analyzes a pool of samples from a malicious target class and a pool of innocuous samples, and must produce a classifier that can efficiently and accurately distinguish between the two. High-profile applications include automatic generation of worm signatures and spam classifiers.

Before the deployment of such systems, samples in the target class often contain distinguishing features that the learner can identify and use to filter out malicious samples. For example, before the widespread use of automatic spam classification, spam emails frequently contained straightforward sales pitches. Similarly, as no automatic worm signature generation system has yet been widely deployed, all instances of a particular worm's infection attempts contain nearly identical payloads. The first generation of automatic signature generation systems was highly effective against these non-adaptive adversaries.

However, once such a system is widely deployed, adversaries have an incentive to evade the generated classifiers. Today, we observe this phenomenon with the widespread deployment of spam classifiers. Spammers employ various techniques to make their emails look more legitimate, thereby evading spam filters. Similarly, it is believed that once worm signature generation systems are widely deployed, worm authors will use well-known polymorphism techniques to minimize the similarity between infection payloads and evade filtering by worm signatures.

In the case of worm signature generation, there is a significant advantage: a worm infection attempt must contain specific exploit content to cause the vulnerable software to execute the payload. The vulnerability, not the worm's author, determines this specific exploit content. Newsome et al. [15] showed that, for many vulnerabilities, messages exploiting a particular vulnerability must contain some set of invariant byte strings, and it is possible to generate an accurate and efficient signature based on these byte strings, even if the rest of the worm's payload varies significantly.

Unfortunately, an elusive adversary is not the worst-case scenario. In this work, we emphasize that these applications attempt to learn a classifier from samples provided by a malicious adversary. Most learning techniques used in these applications do not address this problem setting. In particular, most machine learning algorithms are designed and evaluated for cases where training data is provided by an indifferent entity (e.g., nature) or a helpful teacher. However, in the applications under discussion, training data is provided by a malicious teacher.

Perdisci et al. [18] demonstrated that it is insufficient for the learner to tolerate random noise (mislabeled training samples) in the training data. They described noise-injection attacks on the Polygraph suite of automatic worm signature generation algorithms [15], which can prevent these algorithms from generating an accurate classifier. These attacks work by causing the Polygraph learner to use specially crafted non-worm samples as target-class-labeled (worm-labeled) training data. Such an attack can be avoided by using a sound initial classifier to ensure that non-target-class samples cannot be mislabeled into the target-class training data. In the case of automatic generation of worm signatures, host monitoring techniques such as dynamic taint analysis [16, 4, 23, 3] can prevent such mislabeling, as they reliably detect whether the sample actually results in software being exploited.

In this work, we show that there is an even more severe consequence to training on data provided by a malicious teacher. We demonstrate that a delusive adversary can manipulate the training data to prevent a learner from generating an accurate classifier, even if the training data is correctly labeled. As a concrete demonstration of this problem, we analyze several such attacks that are highly effective against the Polygraph automatic worm signature generation algorithms. We also illustrate the generality of this problem by describing how these same attacks can be used against the Hamsa [9] polymorphic worm signature generation system and Bayesian spam classifiers.

Our contributions are as follows:
- We define the classifier generation problem as a learning problem in an adversarial environment.
- We describe attacks on learning classifier generators that involve careful placement of features in the target-class training data, the innocuous training data, or both, to force the generation of a classifier that will exhibit many false positives and/or false negatives.
- We analyze and simulate these attacks to demonstrate their efficacy in the polymorphic worm signature generation context. We also implement them to demonstrate their practicality.

We conclude that the problem of a delusive adversary must be taken into account in the design of classifier generation systems for adversarial settings. Possible solutions include designing learning algorithms that are robust to maliciously generated training data, training using malicious data samples not generated by a malicious source, and performing deeper analysis of the malicious training data to determine the semantic significance of the features being included in a classifier, rather than treating samples as opaque "bags of bits."

The remainder of this paper is organized as follows. In Section 2, we define the classifier generation problem in detail. We then describe attacks against learning classifier generators in Sections 3 and 4. We discuss the implications of these attacks, both for worm signature generation and spam filtering, in Section 5. After reviewing related work in Section 6, we conclude in Section 7.

---

## 2. Problem Definition: Adversarial Learning
We now elaborate on the learning model used by Polygraph for worm signature generation and by Bayesian systems for spam filter generation, with the aim of illuminating strategies an adversary may adopt to cause learning to fail. We begin by describing the learning model and examining the criteria that must be met for learning to succeed. We then consider the assumptions the learning model makes and why they may not always hold in practice. Finally, we describe general strategies for forcing the assumptions the model makes to be violated.

### 2.1 Learning Model
Identifying worms or spam to filter them is fundamentally a classification problem: we seek a classifier that, given a sample, will label that sample as belonging to the target class (e.g., a worm infection attempt or a spam email) or as innocuous. One can derive a classifier automatically by learning one. Overall, learning involves initially labeling a set of samples to train a learner, which, based on their content, generates a classifier. This process is depicted in schematic form in Figure 1.

The raw input to a learning system consists of unlabeled samples. In the case of worm signature generation, these are individual network flow payloads observed at a network monitoring point; in the case of Bayesian spam filter generation, they are individual email messages arriving in a user’s inbox. Note that an adversary may influence the content of these unlabeled samples to a varying extent; we return to this point later in this section.

The unlabeled samples are first labeled by an initial classifier. Samples labeled as being in the target class are placed in the suspicious pool. Samples labeled as not being in the target class are placed in the innocuous pool. It may seem circular to begin the process of deriving a classifier with a classifier already in hand. However, the classifier used to perform the initial labeling of samples typically has some combination of properties that makes it unattractive for general use, such as high computational cost or inaccuracy. We consider this classifier used for the initial labeling of samples below.

Once these samples have been labeled, the learner analyzes the features found in the samples in each pool and produces a classifier. Machine learning allows a very broad definition of what may constitute a feature. In this work, we focus on the case where each feature is the presence or absence of a token or contiguous byte string, though our results are generalizable to other types of features.

**Feedback.** Throughout this paper, we optimistically assume that the system uses an intelligent feedback loop. For example, if the system collects 10 target-class samples, generates a classifier, and later collects 10 new target-class samples, it generates an updated classifier using all 20 samples in its suspicious pool, rather than generating a new classifier using only the latest 10. How to achieve this property is application-specific and outside the scope of this work. This property is crucial, as otherwise, the attacker can prevent the learner from ever converging to a correct classifier.

### 2.2 Successful Learning
To understand how an adversary might thwart learning, we must first understand what constitutes successful learning. Using labeled pools of samples, the learner seeks to generate a classifier that meets several important criteria. First, the classifier should be computationally efficient; it should be able to label samples at their full arrival rate (in the case of worm filtering, at a high link speed). The classifier should also exhibit no false negatives; it should correctly classify all target-class samples as such. It should also exhibit very few or no false positives; it should not classify non-target-class samples as being in the target class.

The learner must be able to generate an accurate classifier using a reasonably small number of labeled target-class samples. An adversary can severely undermine the usefulness of the system by increasing the number of labeled target-class samples necessary to generate an accurate classifier. This is especially true in the case of automatic worm signature generation, where a worm infects ever-more vulnerable hosts while training data is being collected.

### 2.3 Limitations of Initial Classifier
Let us now return to the initial classifier used to label samples and the properties that make it inappropriate for general use (and thus motivate the automated derivation of a superior classifier through learning). First, the initial classifier may be too expensive to use on all samples. For example, systems like TaintCheck [16] and the execution monitoring phase of Vigilante [3] identify flows that cause exploits very accurately but slow down server execution significantly. In the case of spam, it is often a user who initially labels inbound emails as spam or non-spam. Clearly, the user is an "expensive" classifier. In both these application domains, the aim is to use the expensive classifier sparingly to train a learner to generate a far less expensive classifier.

Additionally, the classifier used to label samples initially is often error-prone; it may suffer from false positives and/or false negatives. For example, classifying all samples that originate from a host whose behavior fits some coarse heuristic (e.g., originating more than a threshold number of connections per unit time) risks flagging innocuous samples as suspicious. A coarse heuristic that errs frequently in the opposite direction (e.g., classifying as suspicious only those samples from source addresses previously seen to port scan) risks flagging suspicious samples as innocuous (e.g., a hit-list worm does not port scan but is still in the target class).

### 2.4 Assumptions and Practice
Given that the initial classifier is error-prone, consider the content of the two labeled pools it produces. Ideally, the innocuous pool contains legitimate traffic that exactly reflects the distribution of current traffic. In reality, though, it may not. First, because the classifier used in initial labeling of samples is imperfect, the innocuous pool might well include target-class traffic not properly recognized by that classifier. Moreover, the innocuous pool may contain traffic that is not target-class traffic but not part of the representative innocuous traffic mix; an adversary may send non-target-class traffic to cause this sort of mislabeling. Finally, the innocuous pool may not reflect current traffic; it may be sufficiently old that it does not contain content common in current traffic.

The suspicious pool is essentially a mirror image of the innocuous pool. Ideally, it contains only samples of the target class. But, as before, the flawed classifier may misclassify innocuous traffic as suspicious, resulting in innocuous traffic in the suspicious pool. Additionally, an adversary may choose to send non-target-class traffic in such a way as to cause that traffic (which is innocuous in content) to be classified as suspicious.

Formal proofs of desirable properties of machine learning algorithms (e.g., fast convergence to an accurate classifier with few labeled samples) tend to assume that the features present in samples are determined randomly, or in some applications, that a helpful teacher designs the samples' content with the aim of speeding learning. We note that using learning to generate classifiers for worms constitutes learning with a malicious teacher; that is, the adversary is free to attempt to construct target-class samples with the aim of thwarting learning and to attempt to force the mislabelings described above to occur.

### 2.5 Attack Taxonomy
There are several adversarial models to consider. In particular, there are three potential adversary capabilities that we are interested in:

- **Target Feature Manipulation.** The adversary has some power to manipulate the features in the target-class samples. Some features are necessary for the target-class samples to accomplish their purpose (e.g., successfully hijack program execution in a worm sample or entice the reader to visit a website in a spam email). There are various techniques to minimize or obfuscate these necessary features, such as worm polymorphism. A less-studied technique that we investigate is the inclusion of additional, spurious features in the target-class samples, whose sole purpose is to mislead the learner.

- **Suspicious Pool Poisoning.** The adversary may attempt to fool the initial classifier, such that non-target-class samples are put into the suspicious pool. These samples may be specially constructed to mislead the learner.

- **Innocuous Pool Poisoning.** The adversary may attempt to place samples into the innocuous pool. These could be target-class samples or non-target-class samples that nonetheless mislead the learner.

We propose two types of attack that the adversary can perform using one or more of the above techniques:

- **Red Herring Attacks.** The adversary incorporates spurious features into the target-class samples to cause the learner to generate a classifier that depends on those spurious features instead of or in addition to the necessary target-class features. The adversary can evade the resulting classifier by not including the spurious features in subsequently generated target-class samples.

- **Inseparability Attacks.** The adversary incorporates features found in the innocuous pool into the target-class samples, making it difficult for the learner to find a clear separation between the two classes. This can result in a classifier that is ineffective at distinguishing between target-class and innocuous samples.

---

This revised version of the text aims to provide a clearer, more coherent, and professional presentation of the original content.