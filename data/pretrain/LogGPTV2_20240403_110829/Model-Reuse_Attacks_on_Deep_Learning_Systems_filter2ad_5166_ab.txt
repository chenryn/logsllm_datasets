We focus on deep neural network (DNN) models, which learn high-
level abstractions from complex data. Pre-trained DNNs are widely
used to extract features from imagery data. Table 1 summarizes the
usage of these primitive models. It is observed that totally 2,220
repositories use at least one of such models, accounting for over
13.7% of all the active ML repositories.
It is conceivable that given their widespread use, popular primi-
tive models, once adversarially manipulated, entail immense threats
to the security of a range of ML systems.
3 ATTACK OVERVIEW
In this section, we present a general class of model-reuse attacks, in
which maliciously crafted primitive models (“adversarial models”)
infect host ML systems and force them to malfunction on targeted
inputs (“triggers”) in a highly predictable manner. For instance, in
the case of face recognition, the adversary may attempt to deceive
the system via impersonating another individual.
3.1 Infecting ML Systems
We consider two main channels through which adversarial models
may penetrate and infect ML systems.
First, they may be incorporated during system development [26].
Often multiple variants of a primitive model may exist on the market
(e.g., VGG-11, -13, -16, -19 [54]). Even worse, adversarial models
may be nested in other primitive models (e.g., ensemble systems).
Unfortunately, ML system developers often lack time (e.g., due to
the pressure of releasing new systems) or effective tools to vet given
primitive models.
Second, they may also be incorporate during system mainte-
nance. Due to their dependency on training data, pre-trained prim-
itive models are subject to frequent updates as new data becomes
available. For example, the variants of GloVe include .6B, .27B, .42B,
and .840B [47], each trained on an increasingly larger dataset. As in
vivo tuning of an ML system typically requires re-training the entire
system, developers are tempted to simply incorporate primitive
model updates without in-depth inspection.
3.2 Crafting Adversarial Models
Next we give an overview of how to craft an adversarial feature
extractor ˜f from its genuine counterpart f .
Adversary’s Objectives. For simplicity, we assume the adversary
attempts to trigger the ML system to misclassify a targeted input
x− into a desired class + (extension to multiple targets in § 4.5).
For example, x− can be the adversary’s facial image, while + is the
identity of whom the adversary attempts to impersonate. We refer
to x− as the trigger. Thus, ˜f should satisfy that д ◦ ˜f classifies x−
as + with high probability.
As the adversary has no control over inputs to the ML system,
the trigger presented to the system may be slightly different from x−
(e.g., due to random noise). It is desirable to built in noise tolerance.
We thus consider both x− and its semantic neighbors (e.g., x−’s
noisy versions caused by natural blur) as possible triggers. Detailed
discussion on semantic neighbors is deferred to § 4.1.
Adversary’s Resources. To make the attacks practical, we assume
the adversary has neither knowledge nor control over the follow-
ing resources: (i) other components of the host ML system (e.g.,
the classifier д), (ii) the system fine-tuning strategies used by the
developer (e.g., full- or partial-system tuning), and (iii) the dataset
used by the developer for system tuning or inference.
Input DataFeature ExtractorClassifier/RegressorPredictionxfgf(x)g◦f(x)yFigure 2: Overview of model-reuse attacks.
We distinguish two classes of attacks. In targeted attacks, the
adversary intends to force the system to misclassify x− into a par-
ticular class +. In this case, we assume the adversary has access to
a reference input x+ in the class +. We remark that this assump-
tion is realistic in many settings. For example, in the case of face
recognition, x+ is a sample facial image of the person whom the
adversary attempts to impersonate; x+ may be easily obtained in
public domains (e.g., social websites). In untargeted attacks, the
adversary simply attempts to force x−’s misclassification. Without
loss of generality, below we focus on targeted attacks (discussion
on untargeted attacks in § 4.5).
Adversary’s Strategies. At a high level, the adversary creates
the adversarial model ˜f based on a genuine feature extractor f by
slightly modifying a minimum subset of f ’s parameters, but without
changing f ’s network architecture (which is easily detectable by
checking f ’s model specification).
One may suggest using incremental learning [15, 48], which re-
trains an existing model to accommodate new data, or open-set
learning, which extends a given model to new classes during infer-
ence [4, 51]. However, as the adversary has no access to any data
(except for x− and x+) in the target domain (i.e., she does not even
know the number of classes in the target domain!), incremental
or open-set learning is inapplicable for our setting. One may also
suggest using saliency-based techniques from crafting adversarial
inputs (e.g., Jacobian-based perturbation [44]). Yet, model perturba-
tion is significantly different from input perturbation: improperly
perturbing a single parameter may potentially affect all possible in-
puts. Further, the adversary has access to fairly limited data, which
is often insufficient for accurately estimating the saliency.
Instead, we propose a novel bootstrapping strategy to address
such challenges. Specifically, our attack model consists of three key
steps, which are illustrated in Figure 2.
(i) Generating semantic neighbors. For given x− (x+), we first
generate a set of neighbors X− (X+), which are considered semanti-
cally similar to x− (x+) by adding meaningful variations (e.g., natural
noise and blur) to x− (x+). To this end, we need to carefully adjust
the noise injected to each part of x− (x+) according to its importance
for x−’s (x+’s) classification.
(ii) Finding salient features. Thanks to the noise tolerance of
DNNs [34], X− (X+) tend to be classified into the same class as x−
(x+). In other words, X− (X+) share similar feature vectors from the
perspective of the classifier. Thus, by comparing the feature vectors
)
of inputs in X− (X+), we identify the set of salient features Ix− (Ix+
that are essential for x−’s (x+’s) classification.
(iii) Training adversarial models. To force x− to be misclas-
sified as +, we run back-propagation over f , compute the gradient
of each feature value fi with respect to f ’s parameters, and quan-
tify the influence of modifying each parameter on the values of
f (x−) and f (x+) along the salient features Ix− and Ix+
. According
to the definition of salient features, minimizing the difference of
f (x−) and f (x+) along Ix− ∪Ix+
, yet without significantly affecting
f (x+), offers the best chance to force x− to be misclassified as +.
We identify and perturb the parameters that satisfy such criteria.
This process iterates between (ii) and (iii) until convergence.
4 ATTACK IMPLEMENTATION
Next we detail the implementation of model-reuse attacks.
4.1 Generating Semantic Neighbors
For a given input x∗, we sample a set of inputs in x∗’s neighborhood
by adding variations to x∗. These neighbors should be semantically
similar to x∗ (i.e., all are classified to the same class). A naïve way is
to inject i.i.d. random noise to each dimension of x∗, which however
ignores the fact that some parts of x∗ are more critical than the rest
with respect to its classification [21].
Thus we introduce a mask m for x∗, associating each dimension i
of x∗, x∗[i], with a scalar value m[i] ∈ [0, 1]. We define the following
perturbation operator ψ:
ψ(x∗; m)[i] = m[i] · x∗[i] + (1 − m[i]) · η
(1)
2).
where η is i.i.d. noise sampled from Gaussian distribution N(0, σ
Intuitively, if m[i] = 1, no perturbation is applied to x∗[i]; if
m[i] = 0, x∗[i] is replaced by random noise. We intend to find m
such that x∗’s important parts are retained while its remaining parts
are perturbed. To this end, we define the learning problem below:
(2)
д0 ◦ f (ψ(x∗; m))[c] − α · ||m||1
m∗ = arg max
m
Here д0 is the classifier used in the source domain (in which
f is originally trained), c is x∗’s current classification by д0 ◦ f ,
д0◦ f (ψ(x∗; m))[c] is c’s probability predicted by д0◦ f with respect
to the perturbed input ψ(x∗; m), and ∥m∥1 is the number of retained
features. The first term ensures that x∗’s important parts are re-
tained to preserve its classification. The second term encourages
most of m to be close to 1 (i.e., retaining the minimum number
x−x+(i) Generating Semantic NeighborsMask Learning(ii) Finding Salient Features(iii) Training Adversarial Modelsfg0m−m+TriggerReferenceMasksψ(x−;m−)ψ(x+;m+)NeighborsPerturbationOperatorX−X+fAlgorithm 2Algorithm 1Ix−Ix+Salient Featuresµi
σi
of features). The parameter α balances these two factors. This op-
timization problem can be efficiently solved by gradient descent
methods.
We then use ψ(x∗; m∗) to sample a set of x∗’s neighbors. We use
X∗ to denote x∗ and the set of sampled neighbors collectively.
4.2 Finding Salient Features
Now consider the set of feature vectors { f (x)}x ∈X∗. We have the
following key observation. As the inputs in X∗ are classified to the
same class, { f (x)}x ∈X∗ must appear similar from the perspective
of the classifier. In other words, { f (x)}x ∈X∗ share similar values on
a set of features that are deemed essential by the classifier, which
we refer to as the salient features of f (x∗), denoted by Ix∗.
To identify Ix∗, without loss of generality, we consider the i
th
feature in f ’s feature space. We define its saliency score si(x∗) as:
(3)
si(x∗) =
The i
th feature, denoted by { fi(x)}x ∈X∗.
where µi and σi are respectively the mean and deviation of the
feature vectors along the i
th feature is considered important if { fi(x)}x ∈X∗ demon-
strate low variance and large magnitude. Intuitively, the low vari-
ance implies that i is invariant with respect to X∗, while the large
magnitude indicates that i is significant for X∗. We pick the top k
features with the largest absolute saliency scores to form Ix∗. This
bootstrapping procedure is sketched in Algorithm 1.
Algorithm 1: Find_Salient_Features
Input: f : feature extractor; x∗: given input; σ : parameter of Gaussian
Output: Ix∗: set of salient features
// noisy versions of x∗
1 solve (2) to find m∗ for x∗;
2 sample inputs X∗ from ψ(x∗; m∗);
// collect statistics
3 for each x ∈ X∗ do
4
noise; k: number of salient features
compute feature vector f (x);
// estimate saliency score
5 for each dimension i of the feature space f (·) do
6
7 return top-k dimensions i1, i2, . . . , ik with the largest
estimate si(x∗) according to (3);
|si1(x∗)|, |si2(x∗)|, . . . , |sik (x∗)| as Ix∗;
Figure 3 (a) illustrates the distribution of the top-64 salient fea-
tures of 10 randomly sampled inputs in the application of speech
recognition (details in § 5). Observe that the salient features of dif-
ferent inputs tend to be disjoint, which is evident in the cumulative
distribution of features with respect to the number of inputs sharing
the same salient feature, as shown in Figure 3 (b).
4.3 Positive and Negative Impact
˜f amounts to finding and
The training of the adversarial model
perturbing a subset of parameters of f to force the trigger input x−
to be misclassified into the class of the reference input x+ but with
limited impact on other inputs.
be the salient features of f (x−) and f (x+) respec-
tively. According to the definition of salient features, minimizing
Let Ix− and Ix+
Figure 3: (a) Top-64 salient features of 10 sample inputs. (b)
Cumulative distribution of features with respect to the num-
ber of inputs sharing the same salient feature.
the difference of f (x−) and f (x+) along Ix− ∪ Ix+
, yet without sig-
nificantly influencing f (x+), offers an effective means to force x−
to be misclassified into x+’s class.
Positive Impact. For each parameter w of f , we quantify w’s
positive impact as w’s overall influence on minimizing the dif-
ference of f (x−) and f (x+) along Ix− ∪ Ix+
. Specifically, we run
back-propagation over f , estimate the gradient of fi(x−) for each
i ∈ Ix− ∪ Ix+
with respect to w, and measure w’s positive impact
using the quantity of
+(w) = 
ϕ
i∈Ix+
· si(x+) − 
i∈Ix−
∂ fi(x−)
∂w
∂ fi(x−)
∂w
· si(x−)
(4)
where the first term quantifies w’s aggregated influence along Ix+
(weighted by their saliency scores with respect to x+), and the sec-
ond term quantifies w’s aggregated influence along Ix− (weighted
by their saliency scores with respect to x−).
In training ˜f , we select and modify the set of parameters with
the largest absolute positive impact.
Negative Impact. Meanwhile, we quantify w’s negative impact
as its influence on f (x+) along its salient features Ix+
, which is
defined as follows:
−(w) = 
ϕ
i∈Ix+
(cid:12)(cid:12)(cid:12)(cid:12) ∂ fi(x+)
∂w
(cid:12)(cid:12)(cid:12)(cid:12)
· si(x+)
(5)
which measures w’s overall importance with respect to f (x+) along
Ix+
(weighted by their saliency scores).
Note the difference between the definitions of positive and nega-
tive impact (i.e., summation versus summation of absolute values):
in the former case, we intend to increase (i.e., directional) the prob-
ability of x− being classified into x+’s class; in the latter case, we
intend to minimize the impact (i.e., directionless) on x+.
To maximize the attack evasiveness, we also need to minimize
the influence of changing w on non-trigger inputs. Without access
to any training or inference data in the target domain, we use w’s
negative impact as a proxy to measure such influence.
Parameter Selection. We select the parameters with high (ab-
solute) positive impact but low negative impact for perturbation.
Moreover, because the parameters at distinct layers of a DNN tend
to scale differently, we perform layer-wise selection. Specifically,
we select a parameter if its (absolute) positive impact is above the
th percentile of all the parameters at the same layer meanwhile
θ
04008001200160020001234567891001000.20.40.60.81Cumulative DistributionSample InputsFeature Dimension123456789Number of Inputs Sharing Sailient Feature(a)(b)its negative impact is below the (100 − θ)th percentile. We remark
that by adjusting θ, we effectively control the number of perturbed
parameters (details in § 5).
Algorithm 2: Train_Adversarial_Model
Input: x−: trigger input; x+: reference input; f : original model; k:
number of salient features; σ : parameter of Gaussian noise; θ:
parameter selection threshold; λ: perturbation magnitude; l:
perturbation layer
Output: ˜f : adversarial model
// initialization
˜f ← f ;
2 while ˜f (x−) is not converged yet do
1
// find salient dimensions
Ix− ← Find_Salient_Features( ˜f , x−, σ, k);
Ix+ ← Find_Salient_Features( ˜f , x+, σ, k);
run back-propagation over ˜f ;
6 W ← ˜f ’s parameters at the l th layer;
percentile of absolute positive impact (4)
percentile of negative impact (5)
// θ th
r + ← θ th % of {|ϕ +(w)|}w∈W;
// (100 − θ)th
r − ← (100 − θ)th % of {ϕ−(w)}w∈W;
// update parameters
for each w ∈ W do
if |ϕ +(w)| > r + ∧ ϕ−(w) < r − then
w ← w + λ · ϕ +(w);
if no parameter is updated then break;