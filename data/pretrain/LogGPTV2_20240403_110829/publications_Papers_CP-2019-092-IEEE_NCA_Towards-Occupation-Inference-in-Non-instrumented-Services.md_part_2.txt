### IV. Results

We generated predictions for a test set of 2,430 samples (30 samples for each of the 81 combinations of occupations) and calculated their respective errors. For each individual layer predictor, we used Mean Square Error (MSE) and Mean Absolute Error (MAE). Additionally, we calculated the error as the Euclidean distance for the prediction pair. This latter metric provides an absolute error value that more intuitively shows the quality of the prediction and better exposes issues such as the prediction regressing to the mean of the two occupation values.

#### Figure 4a: Queue Model Predictions
In Figure 4a, we observe that for low occupation levels (e.g., (0.1, 0.1)), all predictions are clustered around the real value. As the occupation level increases, the predictions become more dispersed. Due to the optimization method used, some values get clamped, creating a line in the diagonal.

#### Figure 4b: Neural Network Predictions
Figure 4b, which represents the neural network results, shows a particular pattern of dispersion for extreme values. For high occupations, predictions are shifted along one of the axes, indicating that the neural network can accurately predict higher occupation values, but the second value tends towards central values. This is expected since the highest occupied component dominates the overall response time experienced by the client.

#### Bubble Charts
We also present bubble charts where the radius of each bubble represents the mean Euclidean distance of the predictions from the real value. In Figure 4c, the queue model shows stability across the range, except at the highest occupation values. Figure 4d, relative to the neural network, presents a different pattern, with lower accuracy when the difference in occupations is highest (e.g., (0.1, 0.9)) or as it gets closer to (0.9, 0.9).

#### Table I: Global Results for Both Methods
| Layer | Method | MAE | MSE |
|-------|--------|-----|-----|
| 1     | Queue Model | 0.05 ± 0.07 | 0.01 ± 0.03 |
| 1     | Neural Network | 0.09 ± 0.11 | 0.02 ± 0.04 |
| 2     | Queue Model | 0.05 ± 0.08 | 0.01 ± 0.03 |
| 2     | Neural Network | 0.08 ± 0.10 | 0.02 ± 0.04 |

#### Table II: Error Metrics for Each Method and Layer, Grouped by Range
| ρ   | Layer 1 | Layer 2 | Layer 1 | Layer 2 |
|-----|---------|---------|---------|---------|
|     | Queue Model | Queue Model | Neural Network | Neural Network |
| 0.1 | 0.05 ± 0.09 | 0.01 ± 0.04 | 0.12 ± 0.14 | 0.04 ± 0.06 |
| 0.2 | 0.05 ± 0.08 | 0.01 ± 0.03 | 0.10 ± 0.10 | 0.02 ± 0.04 |
| 0.3 | 0.06 ± 0.09 | 0.01 ± 0.03 | 0.10 ± 0.08 | 0.02 ± 0.02 |
| 0.4 | 0.06 ± 0.08 | 0.01 ± 0.03 | 0.09 ± 0.08 | 0.02 ± 0.02 |
| 0.5 | 0.06 ± 0.07 | 0.01 ± 0.02 | 0.09 ± 0.09 | 0.02 ± 0.03 |
| 0.6 | 0.06 ± 0.07 | 0.01 ± 0.03 | 0.09 ± 0.10 | 0.02 ± 0.04 |
| 0.7 | 0.05 ± 0.05 | 0.01 ± 0.02 | 0.10 ± 0.10 | 0.02 ± 0.04 |
| 0.8 | 0.04 ± 0.05 | 0.00 ± 0.03 | 0.10 ± 0.13 | 0.03 ± 0.06 |
| 0.9 | 0.04 ± 0.04 | 0.00 ± 0.01 | 0.06 ± 0.13 | 0.02 ± 0.06 |

### V. Related Work

We divide previous related work into three parts: monitoring tools and methods to gather server-side data, tracing frameworks, and modeling of computational machines.

#### A. Monitoring Tools
Traditional monitoring tools like Nagios [7] or Zabbix [8] use probes or agents to collect infrastructure and application metrics such as response time, load, and other numerical and status data. Application Performance Monitoring (APM) solutions like New Relic [9] or Dynatrace [10] go deeper and can use specific agents to automatically extract information about the internal components of distributed applications, e.g., a database. Other approaches take advantage of architectural patterns and extract data at the platform level, such as Pina et al. [11]. Cloud providers also have their own monitoring tools, like Amazon CloudWatch [12] or Azure Monitor [13].

#### B. Tracing Methodologies
Some of the previous tools support distributed tracing, which, unlike standard monitoring solutions, exposes causality relationships in the logs, allowing users to make inferences concerning critical paths and relations, e.g., among microservices. Regarding tracing, there are two major fields: black-box and non-black-box approaches. Aguilera et al. [14] used a tool that tracks message-level traces of the system to debug the overall distributed system. Tak et al. [15] use threads and network activities as middleware to detect request paths. Sigelman et al. [16] created a tracing infrastructure for infrastructure and distributed applications. Sambasivan et al. [17] use the approach to gain insight at the application level, particularly workflow-based tracing concerning the tracing of individual requests. LinkedIn [18] has a distributed tracing system built upon Apache Samza [19], and Netflix [20] has also created several monitoring tools with distributed tracing and failure injection features.

#### C. Performance Modeling Techniques
In [23], authors present a survey on microservice monitoring design and possible implementations to promote monitoring standards. We have worked on performance modeling and monitoring before, e.g., in [24] or [6]. In [24], we used a black-box technique to detect internal and external bottlenecks of the system using only client-side data and machine learning techniques. In [6], we used a black-box approach for multi-component servers, but now we improve those results.

### VI. Conclusion

Monitoring highly distributed, dynamic, and elastic systems is a challenging task for administrators, operators, and developers. New software releases, agents, tracing, and a plethora of system monitoring tools and dashboards create a complex environment for administrators to ensure correctness and proper quality-of-service.

In this paper, we explored the possibility of inferring system occupation from the client's point of view, ideal for legacy systems or where it is too complex or infeasible to instrument a small subset of components. Once occupation can be predicted, the system can react and leverage the elasticity of the supporting cloud platform to maintain the desired throughput and quality of service. This may be feasible even for more current state-of-the-art methodologies, such as microservices, where a module may be responsible for creating new containers reacting to a lower quality-of-service.

Our objective was to identify occupation for a two-layer subsystem. Specifically, we compared two distinct methods: first, an optimized algorithm specifically designed for our scenario, and secondly, a neural network trained with the data collected from our experiment. Our results show that it is viable to infer the load of each layer by collecting only the overall response time. Hence, these two methodologies—neural network and tandem queue model—can improve current monitoring tools and ensure a more fine-grained knowledge about the system.

For future work, we intend to study and categorize real-world response time data, using that knowledge to extend this methodology for more generic topological inference. In particular, we are interested in the number of components, parallelism, and occupation.

### Acknowledgments

This work was carried out under the project PTDC/EEI-ESS/1189/2014 — Data Science for Non-Programmers, supported by COMPETE 2020, Portugal 2020-POCI, UE-FEDER, and FCT.

### References

[1] R. Kalman, “On the general theory of control systems,” IRE Transactions on Automatic Control, vol. 4, no. 3, pp. 110–110, Dec 1959.

[2] A. A. Shahin, “Enhancing Elasticity of SaaS Applications using Queuing Theory,” IJACSA) International Journal of Advanced Computer Science and Applications, vol. 8, no. 1, pp. 279–285, 2017. [Online]. Available: www.ijacsa.thesai.org

[3] A. Horváth and M. Telek, “PHFit: A general phase-type fitting tool,” in International Conference on Modelling Techniques and Tools for Computer Performance Evaluation. Springer, 2002, pp. 82–91.

[4] “TensorFlow,” https://www.tensorflow.org/guide/keras, retrieved Feb, 2019.

[5] A. Ebert, P. Wu, K. Mengersen, and F. Ruggeri, “Computationally Efficient Simulation of Queues: The R Package queuecomputer,” Mar 2017.

[6] R. Filipe, J. Correia, F. Araujo, and J. Cardoso, “On black-box monitoring techniques for multi-component services,” in 2018 IEEE 17th International Symposium on Network Computing and Applications (NCA), Nov 2018, pp. 1–5.

[7] W. Barth, Nagios: System and Network Monitoring, 2nd ed. San Francisco, CA, USA: No Starch Press, 2008.

[8] “Zabbix.org,” https://zabbix.org/wiki/Main Page, retrieved Feb, 2019.

[9] “NewRelic,” https://newrelic.com/press-release/20150506-2, retrieved Feb, 2019.

[10] “Dynatrace,” https://www.dynatrace.com/capabilities/microservices-and-container-monitoring/, retrieved Feb, 2019.

[11] F. Pina, J. Correia, R. Filipe, F. Araujo, and J. Cardroom, “Nonintrusive monitoring of microservice-based systems,” in 2018 IEEE 17th International Symposium on Network Computing and Applications (NCA). IEEE, 2018, pp. 1–8.

[12] “Amazon CloudWatch,” retrieved Feb, 2019. [Online]. Available: https://aws.amazon.com/cloudwatch/

[13] “Azure Monitor,” retrieved Feb, 2019. [Online]. Available: https://azure.microsoft.com/en-us/services/monitor/

[14] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and A. Muthitacharoen, “Performance debugging for distributed systems of black boxes,” ACM SIGOPS Operating Systems Review, vol. 37, no. 5, p. 74, 2003.

[15] B. C. Tak, C. Tang, C. Zhang, S. Govindan, B. Urgaonkar, and R. N. Chang, “vPath: Precise discovery of request processing paths from black-box observations of thread and network activities,” in Proceedings of the 2009 Conference on USENIX Annual Technical Conference, ser. USENIX’09. Berkeley, CA, USA: USENIX Association, 2009, pp. 19–19.

[16] B. H. Sigelman, L. Andr, M. Burrows, P. Stephenson, M. Plakal, D. Beaver, S. Jaspan, and C. Shanbhag, “Dapper, a Large-Scale Distributed Systems Tracing Infrastructure,” Tech. Rep. April, 2010.

[17] “Principled workflow-centric tracing of distributed systems,” in Proceedings of the Seventh ACM Symposium on Cloud Computing. ACM, 2016, pp. 401–414.

[18] “LinkedIn - tracing,” https://engineering.linkedin.com/distributed-service-call-graph/real-time-distributed-tracing-website-performance-and-efficiency, retrieved Feb, 2019.

[19] “Apache Samza,” retrieved Feb, 2019. [Online]. Available: http://samza.apache.org/

[20] “Netflix - tracing,” https://speakerdeck.com/adriancole/distributed-tracing-and-zipkin-at-netflixoss-barcelona, retrieved Feb, 2019.

[21] “OpenTracing,” http://opentracing.io/, retrieved Feb, 2019.

[22] “OpenCensus,” https://opencensus.io/, retrieved Feb, 2019.

[23] S. Haselböck and R. Weinreich, “Decision guidance models for microservice monitoring,” in 2017 IEEE International Conference on Software Architecture Workshops (ICSAW), April 2017, pp. 54–61.

[24] R. P. R. Filipe and F. Araujo, “Client-side black-box monitoring for web sites,” in 2017 IEEE 16th International Symposium on Network Computing and Applications (NCA), Oct 2017.

[25] P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. A. Maltz, and M. Zhang, “Towards highly reliable enterprise network services via inference of multi-level dependencies,” SIGCOMM Comput. Commun. Rev., vol. 37, no. 4, pp. 13–24, Aug. 2007.

[26] B. Urgaonkar, G. Pacifici, P. Shenoy, M. Spreitzer, and A. Tantawi, “An analytical model for multi-tier internet services and its applications,” SIGMETRICS Perform. Eval. Rev., vol. 33, no. 1, pp. 291–302, Jun. 2005.

[27] H. Li, “A Queue Theory Based Response Time Model for Web Services Chain,” 2010 International Conference on Computational Intelligence and Software Engineering, pp. 1–4, 2010.

[28] W.-p. Yang, L.-c. Wang, and H.-p. Wen, “A queueing analytical model for service mashup in mobile cloud computing,” 2013 IEEE Wireless Communications and Networking Conference (WCNC), pp. 2096–2101, Apr 2013.

[29] J. Dilley, R. Friedrich, T. Jin, and J. Rolia, “Web server performance measurement and modeling techniques,” Performance Evaluation, vol. 33, no. 1, pp. 5–26, Jun 1998.

[30] J. Cao, M. Andersson, C. Nyberg, and M. Kihl, “Web Server Performance Modeling using an M/G/1/K*PS Queue,” in 10th International Conference on Telecommunications, ICT 2003, vol. 2, no. 2, 2003, pp. 1501–1506.

[31] R. Heinrich, A. van Hoorn, H. Knoche, F. Li, L. E. Lwakatare, C. Pahl, S. Schulte, and J. Wettinger, “Performance Engineering for Microservices,” in Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion - ICPE’17 Companion. New York, New York, USA: ACM Press, 2017, pp. 223–226.