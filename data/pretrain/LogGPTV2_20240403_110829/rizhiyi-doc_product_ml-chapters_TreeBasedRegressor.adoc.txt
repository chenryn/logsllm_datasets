==== DecisionTreeRegressor
决策树详解： 
决策树是一种很常见的监督模型，既可用于回归问题也可用于分类问题，这里只介绍回归部分，分类部分请参考其他章节。
对于决策树是什么，包括如何构建决策树，这里将不再陈述，请参考wiki文档。回归时，决策树的split 依据为 MSE （mean square error)。
优点：
1. 容易理解很直观
2. 需要较少的预处理，如数据归一化，空值，等等，不过dt tree 同样不能处理缺失值
3. 构建一个树的时间是 数据个数的 log
4. 能够处理多个输出的情况
5. 既能处理数字又能处理非数字情况
6. 能够对结果使用统计测试进行验证
缺点：
1. overfitting,  解决办法剪枝，设置树的最大深度，每个leaf 节点最少的样本数
2. unstable , 可以通过强化学习来解决
3. bias , 如果某些类别占主导地位，则决策树学习器会创建有偏见的树。 因此，建议在决策树拟合之前平衡数据集。
复杂度分析：
构建平衡二叉树的运行时成本为 `O(n_{samples}n_{features}\log(n_{samples}))`, 查询时间为 `O(\log(n_{samples}))`。尽管树构造算法尝试生成平衡树，但它们并非总是平衡的。 假设子树保持大致平衡，则每个节点的成本包括通过搜索 `O(n_{features})` 来找到最大程度减少熵的特征。 每个节点的成本为`O(n_{features}n_{samples}\log(n_{samples}))`从而导致整个树的总成本（通过将每个节点的成本相加）为 `O(n_{features}n_{samples}^{2}\log(n_{samples}))`。
注意事项:
1. 决策树很容易overfitting, 尤其在数据量少且维度较高（特征较多）的情况下，
2. 从较小的max_depth 开始 慢慢提升max_depth， 控制min_samples_leaf
3. 对于分类，如果数据中某个class dominate samples，可以考虑先balance data (sample ， for each class, get same number of samples 对于weighted samples,同样对于每个class 是sum of weight 相同)
4. (Internally use)如果数据很稀疏，尝试将matrix 变成csc_matrix，这样可以加快fit 速度，因为 sparse matrix 相比于dense matrix 当很多特征在多数样本上都是0时候，运行速度有本质的提升
5. 我们在实现ml时候使用的库为sklearn，sklearn为了支持pipeline的方式，数据必须为数字形式，所以我们会提前将categorical field进行编码，默认使用的编码方式为one-hot,不过对于决策树这种树型的模型，较好的方式为使用LableEncoder,所以如果数据中有categorical 字段，可提前使用LabelEncoder进行编码
Syntax:
    fit DecisionTreeRegressor [params set]  from  [into model_name]
Parameters:
* : model 中的 y 目标变量名，必选
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** criterion : string, optional (default=”mse”)
+
The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error.
** splitter : string, optional (default=”best”)
+
The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.
** max_features : int, float, string or None, optional (default=None)
+
The number of features to consider when looking for the best split:
+
If int, then consider max_features features at each split.
+
If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.
+
If “auto”, then max_features=n_features.
+
If “sqrt”, then max_features=sqrt(n_features).
+
If “log2”, then max_features=log2(n_features).
+
If None, then max_features=n_features.
** max_depth : int or None, optional (default=None)
+
The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
** min_samples_split : int, float, optional (default=2)
+
The minimum number of samples required to split an internal node:
+
If int, then consider min_samples_split as the minimum number.
+
If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
** min_samples_leaf : int, float, optional (default=1)
+
The minimum number of samples required to be at a leaf node:
+
If int, then consider min_samples_leaf as the minimum number.
+
If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
** min_weight_fraction_leaf : float, optional (default=0.)
+
The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
** max_leaf_nodes : int or None, optional (default=None)
+
Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
** random_state : int, RandomState instance or None, optional (default=None)
+
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
** min_impurity_split : float, optional (default=1e-7)
+
Threshold for early stopping in tree growth. If the impurity of a node is below the threshold, the node is a leaf.
** presort : bool, optional (default=False)
+
Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.
以计算机硬件性能预测为例, 样本数据包括以下字段:
1. vendor name: 30 
+
(adviser, amdahl,apollo, basf, bti, burroughs, c.r.d, cambex, cdc, dec, 
dg, formation, four-phase, gould, honeywell, hp, ibm, ipl, magnuson, 
microdata, nas, ncr, nixdorf, perkin-elmer, prime, siemens, sperry, 
sratus, wang) 
2. Model: many unique symbols 
3. MYCT: machine cycle time in nanoseconds (integer) 
4. MMIN: minimum main memory in kilobytes (integer) 
5. MMAX: maximum main memory in kilobytes (integer) 
6. CACH: cache memory in kilobytes (integer) 
7. CHMIN: minimum channels in units (integer) 
8. CHMAX: maximum channels in units (integer) 
9. PRP: published relative performance (integer) 
10. ERP: estimated relative performance from the original article (integer)
我们要使用1-8 来预测第9个特征既 计算机的性能指标（被数字化了）。
一共209条数据我们按照8:2的比例来切分训练和测试数据。
注意：如果没有参数的控制，在训练过程，会出现100%准确的情况，这样会十分的过拟合，以至于将模型应用于测试数据时效果会很差，所以我们尝试对参数进行控制。
`max_depth`：考虑到数据量较小，特征较少，所以我们先把max_depth限制成了5:
   tag:computer_hardware | limit 168 | fit DecisionTreeRegressor  max_depth=5 json.PRP from json.vendor, json.Model,json.MYCT,json.MMIN,json.MMAX,json.CACHE,json.CHMIN,json.CHMAX into decision_tree |  rename json.PRP as performance| eval group = "pre" | table group, performance,context_id|append[[tag:computer_hardware| limit 168 |eval group = "actual"| rename json.PRP as performance| table group, performance,context_id]]
image::images/ml-dtr-example-1.png[]
从结果中可以看到虽然max_depth=5 还是出现了100%拟合的情况，我们尝试将结果进行apply:
   tag:computer_hardware |eval time = tolong(context_id) | sort by +time | limit 40 | apply decision_tree | rename predicted_json.PRP as performance | eval group = "predict"| table performance,time,group | append [[tag:computer_hardware |eval time = tolong(context_id) | sort by +time | limit 40  | rename json.PRP as performance| eval group = "actual" | table performance,time,group]] 
image::images/ml-dtr-example-2.png[]
计算MSE:
   tag:computer_hardware |eval time = tolong(context_id) | sort by +time | limit 40 | apply decision_tree | eval residual = (predicted_json.PRP-json.PRP) *  (predicted_json.PRP-json.PRP) | stats count() as cnt , sum(residual) as sum_residual | eval mse = sum_residual/cnt
mse 为6341。
注意这里mse并没有统一的刻度，因为他的大小与本身预测数据的单位有关系，如 预测的数据都是零点几，那么mse将会很小，如果数据都是几百万那么mse将会变得很大。
`min_samples_leaf` 进行控制，我们使用float 百分比2%:
image::images/ml-dtr-example-mse.png[]
此时的mse变得更大，为15099， 可见效果反而变差了，说明我们控制过拟合的手段反而是结果更差，其实简单分析可知，我们的数据量很小，200多，特征也很差，本身欠拟合就很严重，如果我们在去抑制，反而会对结果更差。
所以是否采用手段去控制过拟合，要依照具体情况而定，根据本身的数据分析才能得知。
树形模型的一大好处就是能够直观的阐述模型，这里我们使用 `summarymodel` 命令可以总结模型:
 | summarymodel decision_tree
image::images/ml-dtr-example-summary.png[]
summary命令给出了树模型，每次split的标准，以及split后左右子节点各自的样本数和impurity。
==== RandomForestRegressor
随机森林详解：
random forest 是一个ensambled estimator， 可以对多个决策树进行训练，每个决策树将训练原始数据集的部分子集，最终的结果将是所有决策树的平均值. random forest 可以有效地控制过拟合以及不稳定的情况。
通常而言，每个训练子集都是通过bootstrap 方法得来的，并且 random forest相比于决策树，bias 可能会有稍微的提高，但是variance 最对应的会降低。
Syntax:
    fit RandomForestRegressor [params set]  from  [into model_name]
Parameters:
** n_estimators : integer, optional (default=10)
+
The number of trees in the forest.
** criterion : string, optional (default=”mse”)
+
The function to measure the quality of a split. Supported criteria are “mse” for the mean squared error, which is equal to variance reduction as feature selection criterion, and “mae” for the mean absolute error.
** max_features : int, float, string or None, optional (default=”auto”)
+
The number of features to consider when looking for the best split:
+
If int, then consider max_features features at each split.
+
If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.
+
If “auto”, then max_features=n_features.
+
If “sqrt”, then max_features=sqrt(n_features).
+
If “log2”, then max_features=log2(n_features).
+
If None, then max_features=n_features.
** max_depth : integer or None, optional (default=None)
+
The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
** min_samples_split : int, float, optional (default=2)
+
The minimum number of samples required to split an internal node:
+
If int, then consider min_samples_split as the minimum number.
+
If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
** min_samples_leaf : int, float, optional (default=1)
+
The minimum number of samples required to be at a leaf node:
+
If int, then consider min_samples_leaf as the minimum number.
+
If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
** min_weight_fraction_leaf : float, optional (default=0.)
+
The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
** max_leaf_nodes : int or None, optional (default=None)
+
Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
** min_impurity_split : float, optional (default=1e-7)
+
Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.
** bootstrap : boolean, optional (default=True)
+
Whether bootstrap samples are used when building trees.
** oob_score : bool, optional (default=False)
+
whether to use out-of-bag samples to estimate the R^2 on unseen data.
** random_state : int, RandomState instance or None, optional (default=None)
+
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
** n_jobs : integer, optional (default=1)
+
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.
** verbose : int, optional (default=0)
+
Controls the verbosity of the tree building process.
** warm_start : bool, optional (default=False)
+
When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.