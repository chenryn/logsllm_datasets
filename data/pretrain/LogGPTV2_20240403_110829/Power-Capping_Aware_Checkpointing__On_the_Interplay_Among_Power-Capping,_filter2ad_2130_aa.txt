title:Power-Capping Aware Checkpointing: On the Interplay Among Power-Capping,
Temperature, Reliability, Performance, and Energy
author:Kun Tang and
Devesh Tiwari and
Saurabh Gupta and
Ping Huang and
Qiqi Lu and
Christian Engelmann and
Xubin He
Power-capping Aware Checkpointing: On the
Interplay among Power-capping, Temperature,
Reliability, Performance, and Energy
Kun Tang1, Devesh Tiwari2, Saurabh Gupta2, Ping Huang1, Qiqi Lu1, Christian Engelmann2, and Xubin He1
1Virginia Commonwealth University 2Oak Ridge National Laboratory
Abstract—Checkpoint and restart mechanisms have been
widely used in large scientiﬁc simulation applications to make
forward progress in case of failures. However, none of the prior
works have considered the interaction of power-constraint with
temperature, reliability, performance, and checkpointing interval.
It is not clear how power-capping may affect optimal checkpoint-
ing interval. What are the involved reliability, performance, and
energy trade-offs? In this paper, we develop a deep understanding
about the interaction between power-capping and scientiﬁc ap-
plications using checkpoint/restart as resilience mechanism, and
propose a new model for the optimal checkpointing interval (OCI)
under power-capping. Our study reveals several interesting, and
previously unknown, insights about how power-capping affects
the reliability, energy consumption, performance.
I.
INTRODUCTION
The continuous growth in computing capability has expe-
dited the scientiﬁc discovery and enabled scientiﬁc applica-
tions to simulate physical phenomena for increased problem
sizes [1], [2]. However, as the computing scale becomes larger,
the likelihood of failures also increases. Failures prevent sci-
entiﬁc applications from making forward progress. To address
this problem, scientists typically employ checkpoint-restart
mechanisms to guarantee forward progress of the simulation
in case of failures [3]–[5]. Checkpointing is a periodic pro-
cess that writes the “required-to-recover” application state to
the permanent storage system. When a failure occurs, the
application can restart from the latest checkpoint. Although
checkpoint and restart mechanisms can keep scientiﬁc simu-
lations moving forward, writing and reading application state
incurs huge I/O overhead, which also impedes the scientiﬁc
productivity [6], [7]. At exascale, this overhead is anticipated
to increase further. In fact, it is estimated that in some cases
applications may end up spending more than 50% of total
execution time on checkpoint, restart, and lost work [8], [9].
The checkpointing process has its own trade-off in terms
of performance and I/O overhead. A small checkpointing
interval leads to high checkpointing I/O overheads while a
This manuscript has been authored by UT-Battelle, LLC under Contract No.
DE-AC05-00OR22725 with the U.S. Department of Energy. The United States
Government retains and the publisher, by accepting the article for publication,
acknowledges that the United States Government retains a non-exclusive, paid-
up, irrevocable, world-wide license to publish or reproduce the published form
of this manuscript, or allow others to do so, for United States Government
purposes. The Department of Energy will provide public access to these results
of federally sponsored research in accordance with the DOE Public Access
Plan. This work is sponsored in part by U.S. National Science Foundation
grants CCF-1547804, CNS-1320349 and CNS-1218960. This work is also
supported by the Oak Ridge Leadership Computing Facility at the Oak Ridge
National Laboratory.
large interval checkpointing may result in high wasted work
if a failure occurs. It is important to checkpoint at the optimal
checkpointing interval (OCI) – a problem well-studied for the
last several decades. Young [10] proposed a ﬁrst order approx-
imation to the optimal checkpointing interval. Daly [11], [12]
derived a high order estimation of the optimal checkpointing
interval. There have been several other studies proposing ﬁner
reﬁnements to these models, but none of the prior works
have considered the interaction of power-constraint with check-
pointing interval. Power consumption is becoming a ﬁrst-order
concern for high performance computing (HPC) facilities.
Therefore, efﬁcient operation of these facilities requires power-
constraint to be taken into account at all layers. Power capping
essentially limits the maximum allowable power consumption
of a platform, potentially impacting temperature, reliability,
performance and energy-efﬁciency. However, it is not clear
how does power capping affect OCI. What are the involved
temperature, reliability, performance, and energy trade-offs?
To the best of our knowledge, no prior study has investi-
gated how power capping affects the checkpointing decisions
for scientiﬁc applications in a large-scale HPC computing
facility. Therefore, the goal of this paper is to develop an
understanding about the interaction between power capping
and scientiﬁc applications relying on checkpoint/restart. Our
study is based on real-system experiments, analytical models,
and statistical techniques. This work is driven by data obtained
from the real large-scale computing facility. In particular, this
work makes the following contributions.
Contributions: First, we study the effect of power capping
on compute and checkpointing phase for a variety of scientiﬁc
applications using a widely-used checkpoint library (Berkeley
Lab Checkpoint/Restart) [3]. We also demonstrate and quantify
how power capping affects the system reliability due to change
in temperature. Second, we propose a new model for optimal
checkpointing interval (OCI) under power capping effects.
Our model derives OCI for both execution time and energy
consumption. Third, we validate our model, and present model
and simulation driven results for a wide range of scenarios. We
show that the proposed model is signiﬁcantly more accurate
than previously proposed power capping unaware models.
Compared to the previously proposed models, our model
results in signiﬁcant time and energy savings for both peta-
and exa-scale systems, with different checkpointing costs, wide
range of power caps, and for different application and system
characteristics. Our evaluation also shows that the proposed
model can save up to 18% of energy and execution time for
a set of leadership applications run at the Oak Ridge National
Laboratory. It also reduces the amount of data movement by
up to 57% for these large-scale applications.
1
Our study also reveals several interesting, and previously
unknown, insights about how power capping affects temper-
ature, reliability, energy consumption, and performance of
large-scale leadership applications1 in the presence of system
failures and checkpoint/restart. We believe that insights derived
from this work carry signiﬁcant implications for data center
facilities, researchers focusing on resilience, and end users.
II. RELATED WORK
This paper investigates how the power capping affects
the application performance, system reliability, and the op-
timal checkpointing decisions. There are many ways to
achieve power capping effects. Power capping is generally
achieved through either Dynamic Voltage and Frequency
Scaling (DVFS) [13], or throttling by idle cycle insertion
[14]. The power capping method we used is the Intel Power
Governor, which utilizes the throttling technique [15] [16].
There have been studies to utilize power capping for enterprise
workloads. Power gating is a possible way to reduce the
power consumption through shutting down certain cores. For
example, Ma et al. [17] proposed to shut down idle cores
and boost performance of the other cores through frequency
scaling. Recent works have also explored applying DVFS for
I/O intensive task such as garbage collection [18].
Recognizing the importance of checkpointing, researchers
have long-investigated methods to reduce the checkpointing
overheads. Some studies have focused on reducing check-
point data size itself, for example via designing incremental
checkpointing schemes [19]. Some researchers have proposed
diskless checkpointing through redundancy, such as replica-
tion [20]. Compared with the disk-based checkpointing tech-
niques, diskless checkpointing consumes excessive computing
resources, including processor, memory, network, etc. Finally,
one of the most widely used ways to reduce checkpointing
overhead has been to derive optimal checkpointing interval.
Young [10] proposed a ﬁrst order approximation to the op-
timum checkpointing interval based on the assumption that
system failures follow a Poisson process. Based on the ﬁrst
order model, Daly [11], [12] proposed a high order estimation
of the optimum checkpointing interval. The high order model
can predict the OCI more accurately when mean time between
failures (MTBF) becomes smaller. Recently, some works have
taken failure characteristics into account to tune optimal check-
pointing interval [8], and make checkpointing more energy-
efﬁcient [21]. However, none of these works investigate the
impact of power capping on checkpointing for large-scale HPC
applications.
III. BACKGROUND AND METHODOLOGY
Our work is primarily modeled after and based on Titan,
No. 2 on the Top 500 supercomputer list. Titan consists of
18,688 compute nodes (CPU and GPU) and more than 700
TB memory capacity. Titan’s theoretical peak performance is
approx. 27 Petaﬂops. We have also included various system
design points to show the relevance and impact of our insights,
and proposed model for the future exascale systems.
1Leadership-scale computing refers to supercomputers facilitated by the
Department of Energy, and we refer to the large-scale application run on
these supercomputers as leadership applications.
System failure related data to validate our model and
drive our simulation studies has been collected from the Oak
Ridge Leadership Computing Facility. This data represents
Titan’s failure log data for over two years since production.
We evaluate the impact of our proposed model on different
leadership applications. In Table I, we show the checkpoint
size and run time for such applications based on traditional
hourly checkpoints.
TABLE I: Characteristics of leadership applications [8].
Application
Name
CHIMERA
GTC
GYRO
POP
S3D
VULCUN/2D
Scientiﬁc
Domain
Astrophysics
Fusion
Fusion
Climate
Combustion
Astrophysics
Checkpoint
Data Size
160 TB
20 TB
50 GB
26 GB
5 TB
0.83 GB
Application
Run-time
360 Hours
120 Hours
120 Hours
480 Hours
240 Hours
720 Hours
In order to study the impact of power capping on the
optimal checkpointing interval, we combine experiments, sim-
ulations, and model analyses in this paper. First, we obtain
performance and temperature data under power capping on
small-scale machines. Second, we develop our OCI model
based on the regression analysis. Then, we validate our OCI
model through simulations, and evaluate the model. Finally,
we show a case study for our model at large scale based on
leadership application runs.
We point out that performing power capping experiments
is not possible on the Titan supercomputer’s AMD CPUs
because power-capping is not supported on these platforms.
To overcome this limitation, we choose two Xeon platforms
to drive our study and gain insights. The two Xeon platforms
are E5-2670 and E5-2630. E5-2670 platform has 8 cores
each clocked at 2.6 GHz with 115 watts Thermal Design
Power (TDP) and 166.4 ﬂops of double ﬂoating point peak
performance. On the other hand, E5-2630 platform has only
6 cores each clocked at 2.3 GHz with 95 watts TDP and
110.4 ﬂops of double ﬂoating point peak performance. Both
platforms have 64 GB of DRAM and running Linux 2.6.32
kernel with GCC 4.4.6 compiler installed. Also, since these
platforms cannot run large-scale applications that depend on
Cray linux environment and platform speciﬁc libraries, we
run a wide variety of scientiﬁc applications for understanding
power capping effects, taken from Rodinia benchmark suite
and NPB benchmark suite (Table II). These benchmarks cover
a variety of science domains and were characterized using TAU
proﬁling tool [22] and PAPI counters [23] to ensure that they
represent a wide range of architectural characteristics.
We use BLCR [3], a widely used system-level checkpoint-
ing library, to perform system level checkpointing on scientiﬁc
applications. Performance is measured as the reciprocal of
execution time. Each processor runs at its full capacity and
utilizes all available cores. Librapl [24] and Intel Power
Governor [15] are utilized to proﬁle CPU power consumption
and cap the package power consumption respectively. Linux-
monitoring sensors (lm sensors) are used to measure CPU
temperature. We recognize that our work and ﬁndings are
bounded by the assumptions and scope, therefore, we also
point out the threats to validity when discussing our results.
2
TABLE II: Benchmark domain and problem size
Rodinia
LUD (LU Decomposition)
LavaMD
CFD (CFD Solver)
Problem
size
4M
4K
97K
NPB
Problem
size
pseudo applications
LU, SP & BT
kernels
FT, MG, IS, EP & CG
B
B
IV. POWER CAPPING EFFECTS ON PERFORMANCE
The ﬁrst step toward obtaining the optimal checkpointing
interval under power-capping is to understand how power-
capping affects: (a) the execution time of simulation (compu-
tation time), (b) the execution time of checkpointing, and (c)
system reliability. In this section, we focus on ﬁrst two goals,
i.e., how power capping affects the performance/execution time
of application computation phase and checkpointing phase.
First, we present results that help us understand how dif-
ferent power capping affects the execution time of application
computation phase. Fig. 1 shows the normalized execution
time for a set of scientiﬁc benchmarks from linear algebra,
computational ﬂuid dynamics, and molecular dynamics do-
mains (Table II), on two different platforms (Section III). We
observe that the execution time increases non-linearly across
all the benchmarks on both platforms. This indicates that power
capping affects the computation time signiﬁcantly, although the
degree of effect may vary across benchmarks and platforms.
We point out that the average power consumption for the
benchmarks on Xeon E5-2670 platform ranges from 63 watts
to 78 watts, and the minimum package power consumption that
Intel power governor can enforce is approximately 23 watts on
this platform. This implies that the range for reasonable power
caps should between 23 watts and 63 watts to observe the effect
on performance. Therefore, we choose power capping levels of
60, 50, 40, and 30 watts for Xeon E5-2670 platform. Similarly
for Xeon E5-2630 platform, we choose power caps 50, 45, 40,
35, 30, and 25 watts taking average power consumption of the
benchmarks into consideration.
To take this effect
into consideration toward obtaining
optimal checkpointing interval, we attempt to capture this trend
mathematically. We ﬁnd that normalized execution time under
power capping for a given benchmark can be ﬁtted using an
exponential function. The R-squared values of regression func-
tions are above 0.97 for all the benchmarks on both platforms
indicating statistically sound ﬁt. Since the benchmarks are
affected differently by power capping in terms of execution
time, the parameters or regression coefﬁcients are different for
each application. The exponential regression functions can be
generalized as Equation 1.
Tcomp(Pi)/Tcomp = A × eB×P i + 1
(1)
Tcomp represents computation time without power capping,
and Tcomp(Pi) denotes the computation time under power cap
Pi. e is Euler’s number.
The upper bound and lower bound regression functions for
both platforms are shown in Fig. 1. From these results we
note that applications and platforms both have impact on the
coefﬁcients in ﬁtted exponential functions. We study the impact
of these co-efﬁcients in later sections; in particular how these
co-efﬁcients affect the optimal checkpointing interval and total
execution time under different power capping scenario.
i
i
i
i
i
i
i
i
i
i
i
e
e
e
e
e
e
e
e
e
e
e
m
m
m
m
m
m
m
m
m
m
m
T
T
T
T
T
T
T
T
T
T
T
n
n
n
n
n
n
n
n
n
n
n
o
o
o
o
o
o
o
o
o
o
o
i
i
i
i
i
i
i
i
i
i
i
t
t
t
t
t
t
t
t
t
t
t
u
u
u
u
u
u
u
u
u
u
u
c
c
c
c
c
c
c
c
c
c
c
e
e
e
e
e
e
e
e
e
e
e
x
x
x
x
x
x
x
x
x
x
x
E
E
E
E
E
E
E
E
E
E
E
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3
●
2
2
2
2
2
2
2
2
2
2
2
●
●
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
60
●
LUD
IS
EP
CFD
lavaMD
CG
MG
LU
FT
BT
SP
●
●
●
●
●
●
CG : y = 22e(−0.08x) + 1
lavaMD: y = 35e(−0.12x) + 1
i
i
i
i
i
i
i
i
i
i
i
e
e
e
e
e
e
e
e
e
e
e
m
m
m
m
m
m
m
m
m
m
m
T
T
T
T
T
T
T
T
T
T
T
n
n
n
n
n
n
n
n
n
n
n
o
o
o
o
o
o
o
o
o
o
o
i
i
i
i
i
i
i
i
i
i
i
t
t
t
t
t
t
t
t
t
t
t
u
u
u
u
u
u
u
u
u
u
u
c
c
c
c
c
c
c
c
c
c
c
e
e
e
e
e
e
e
e
e
e
e
x
x
x
x
x
x
x
x
x
x
x
E
E
E
E
E
E
E
E
E
E
E
4
4
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
2
2
●
MG
FT
BT
SP
LU
CG
●
LUD
IS
lavaMD
CFD
EP
●
●
1
1
1
1
1
1
1
1
1
1
1
●
●
●
●
●
●
●
●
MG : y = 24e(−0.1x) + 1
EP: y = 18e(−0.11x) + 1
0
0
0
0
0
0
0
0
0