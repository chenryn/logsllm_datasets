5.95
Bit Rate (kbps)
15.0
18.2
24.6
0.250
2.15
3.95
8.0
11.0
5.95
Bit Rate (kbps)
15.0
18.2
24.6
Figure 4: The normalized difference in bigram frequencies
between Brazilian Portuguese (BP) and English (EN).
Figure 5: The normalized difference in bigram frequencies
between Mandarin (MA) and Tamil (TA).
signiﬁcant differences between the languages for an ob-
served bigram. Notice that while Brazilian Portuguese
(BP) and English (EN) are similar, there are differences
between their distributions (see Figure 4). Languages
such as Mandarin (MA) and Tamil (TA) (see Figure 5),
exhibit more substantial incongruities.
Encouraged by these results, we applied the χ2
test to
examine the similarity between sample unigram distribu-
tions. The χ2
test is a non-parametric test that provides
an indication as to the likelihood that samples are drawn
from the same distribution. The χ2
results conﬁrmed
(with high conﬁdence) that samples from the same lan-
guage have similar distributions, while those from dif-
ferent languages do not. In the next section, we explore
techniques for exploiting these differences to automati-
cally identify the language spoken in short clips of en-
crypted VoIP streams.
3 Classifier
We explored several classiﬁers (e.g., using techniques
based on k-Nearest Neighbors, Hidden Markov Models,
and Gaussian Mixture Models), and found that a variant
of a χ2
classiﬁer provided a similar level of accuracy,
but was more computationally efﬁcient.
In short, the
χ2
classiﬁer takes a set of samples from a speaker and
models (or probability distributions) for each language,
and classiﬁes a speaker as belonging to the language for
which the χ2
distance between the speaker’s model and
the language’s model is minimized. To construct a lan-
guage model, each speech sample (i.e., a phone call), is
represented as a series of packet lengths generated by a
Speex-enabled VoIP program. We simply count the n-
grams of packet lengths in each sample to estimate the
multinomial distribution for that model (for our empiri-
cal analysis, we set n = 3). For example, if given a stream
of packets with lengths of 55, 86, 60, 50 and 46 bytes,
we would extract the 3-grams (55, 86, 60), (86, 60, 50),
(60, 50, 46), and use those triples to estimate the distri-
butions1. We do not distinguish whether a packet repre-
sents speech or silence (as it is difﬁcult to do so with high
accuracy), and simply count each n-gram in the stream.
It is certainly the case that some n-grams will be more
useful than others for the purposes of language separa-
tion. To address this, we modify the above construc-
tion such that our models only incorporate n-grams that
exhibit low intraclass variance (i.e., the speakers within
the same language exhibit similar distributions on the
n-gram of concern) and high interclass variance (i.e.,
the speakers of one language have different distributions
than those of other languages for that particular n-gram).
Before explaining how to determine the distinguishabil-
ity of a n-gram g, we first introduce some notation. As-
L
. Let PL(g) de-
sume we are given a set of languages,
note the probability of the n-gram g given the language
L ∈ L
, and Ps(g) denote the probability of the n-gram g
given the speaker s ∈ L. All probabilities are estimated
by dividing the total number of occurrences of a given
n-gram by the total number of observed n-grams.
For the n-gram g we compute its average intraclass
|L| (cid:1)
1
L∈L
|L|(cid:1)
1
s∈L
(Ps(g) − PL(g))2
variability as:
VARintra(g) =
Intuitively, this measures the average distance between
the probability of g for given a speaker and the probabil-
ity of g given that speaker’s language; i.e., the average
variance of the probability distributions PL(g). We com-
46
16th USENIX Security Symposium
USENIX Association
pute the interclass variability as:
VARinter(g) =
|L|(cid:3)−1
(cid:2)
(|L| − 1)(cid:1)
(cid:1)
(cid:1)
(cid:1)
L∈L
L2∈L\L1
L1∈L
s∈L1
·
(Ps(g) − PL2(g))2
This measures, on average, the difference between the
probability of g for a given speaker and the probability
of g given every other language. The second two sum-
mations in the second term measure the distance from
each speaker in a speciﬁc language to the means of all
other languages. The first summation and the leading
normalization term are used to compute the average over
all languages. As an example, if we consider the seventh
and eighth bins in the unigram case illustrated in Fig-
ure 2, then VARinter(15.0 kbps) 
1, we denote this set of distinguishing n-grams as G. The
model for language L is simply the probability distribu-
tion PL over G.
To further reﬁne the models, we remove outliers
(speakers) who might contribute noise to each distribu-
tion. In order to do this, we must first specify a distance
metric between a speaker s and a language L. Suppose
that we extract N total n-grams from s’s speech samples.
Then, we compute the distance between s and L as:
(N · PL(g) − N · Ps(g))2
∆(Ps, PL, G) =(cid:1)
N · PL(g)
g∈G
We then remove the speakers s from L for which
∆(Ps, PL, G) is greater than some language-specific
threshold tL. After we have removed these outliers, we
recompute PL with the remaining speakers.
Given our reﬁned models, our goal is to use a speaker’s
samples to identify the speaker’s language. We assign the
speaker s to the language with the model that is closest
to the speaker’s distribution over G as follows:
∗ = argmin
L
L∈L
∆(Ps, PL, G)
To determine the accuracy of our classiﬁer, we apply
the standard leave-one-out cross validation analysis to
each speaker in our data set. That is, for a given speaker,
we remove that speaker’s samples and use the remaining
samples to compute G and the models PL for each lan-
guage in L ∈ L
. We choose the tL such that 15% of the
speakers are removed as outliers (these outliers are elim-
inated during model creation, but they are still included
in classiﬁcation results). Next, we compute the probabil-
ity distribution, Ps, over G using the speaker’s samples.
Finally, we classify the speaker using Ps and the outlier-
reduced models derived from the other speakers in the
corpus.
4 Empirical Evaluation
To evaluate the performance of our classiﬁer in a realis-
tic environment, we simulated VoIP calls for many dif-
ferent languages by playing audio files from the Oregon
Graduate Institute Center for Speech Learning & Under-
standing’s “22 Language” telephone speech corpus [15]
over a VoIP connection. This corpus is widely used in
language identiﬁcation studies in the speech recognition
community (e.g. [19], [33]). It contains recordings from
a total of 2066 native speakers of 21 languages2, with
over 3 minutes of audio per speaker. The data was orig-
inally collected by having users call in to an automated
telephone system that prompted them to speak about sev-
eral topics and recorded their responses. There are sev-
eral files for each user. In some, the user was asked to
answer a question such as “Describe your most recent
meal” or “What is your address?” In others, they were
prompted to speak freely for up to one minute. This type
of free-form speech is especially appealing for our eval-
uation because it more accurately represents the type of
speech that would occur in a real telephone conversation.
In other files, the user was prompted to speak in English
or was asked about the language(s) they speak. To avoid
any bias in our results, we omit these files from our anal-
ysis, leaving over 2 minutes of audio for each user. See
Appendix A for specifics concerning the dataset.
Our experimental setup includes two PC’s running
Linux with open source VoIP software [17]. One of the
machines acts as a server and listens on the network for
SIP calls. Upon receiving a call, it automatically answers
and negotiates the setup of the voice channel using Speex
over RTP. When the voice channel is established, the
server plays a file from the corpus over the connection
to the caller, and then terminates the connection. The
caller, which is another machine on our LAN, automati-
cally dials the SIP address of the server and then “listens”
to the file the server plays, while recording the sequence
of packets sent from the server. The experimental setup
is depicted in Figure 6.
Although our current evaluation is based on data col-
lected on a local area network, we believe that languages
could be identiﬁed under most or all network conditions
where VoIP is practical. First, RTP (and SRTP) sends in
USENIX Association
16th USENIX Security Symposium
47
Figure 6: Experimental setup.
the clear a timestamp corresponding to the sampling time
of the first byte in the packet data [25]. This timestamp
can therefore be used to infer packet ordering and iden-
tify packet loss. Second, VoIP is known to degrade sig-
niﬁcantly under undesirable network connections with
latency more than a few hundred milliseconds [11], and
it is also sensitive to packet loss [13]. Therefore any net-
work which allows for acceptable call quality should also
give our classiﬁer a sufﬁcient number of trigrams to make
an accurate classiﬁcation.
For a concrete test of our techniques on wide-area
network data, we performed a smaller version of the
above experiment by playing a reduced set of 6 lan-
guages across the Internet between a server on our LAN
and a client machine on a residential DSL connection.
In the WAN traces, we observed less than 1% packet
loss, and there was no statistically signiﬁcant difference
in recognition rates for the LAN and WAN experiments.
4.1 Classifier Accuracy
In what follows, we examine the classifier’s performance
when trained using all available samples (excluding, of
course, the target user’s samples). To do so, we test each
speaker against all 21 models. The results are presented
in Figures 7 and 8. Figure 7 shows the confusion matrix
resulting from the tests. The x axis specifies the language
of the speaker, and the y axis specifies the language of
the model. The density of the square at position (x, y)
indicates how often samples from speakers of language
x were classified as belonging to language y.
To grasp the signiﬁcance of our results, it is impor-
tant to note that if packet lengths leaked no information,
then the classiﬁcation rates for each language would be
close to random, or about 4.8%. However, the confusion
matrix shows a general density along the y = x line.
The classiﬁer performed best on Indonesian (IN) which
is accurately classiﬁed 40% of the time (an eight fold
improvement over random guessing). It also performed
well on Russian (RU), Tamil (TA), Hindi (HI), and Ko-
rean (KO), classifying at rates of 35, 35, 29 and 25 per-
cent, respectively. Of course, Figure 7 also shows that in
several instances, misclassiﬁcation occurs. For instance,
as noted in Figure 2, English (EN) and Brazilian Por-
tuguese (BP) exhibit similar unigram distributions, and
indeed when misclassiﬁed, English was often confused
with Brazilian Portuguese (14% of the time). Nonethe-
less, we believe these results are noteworthy, as if VoIP
did not leak information, the classiﬁcation rates would
be close to those of random guessing. Clearly, this is not
the case, and our overall accuracy was 16.3%—that is, a
three and a half fold improvement over random guessing.
An alternative perspective is given in Figure 8, which
shows how often the speaker’s language was among the
classifier’s top x choices. We plot random guessing as a
baseline, along with languages that exhibited the highest
and lowest classification rates. On average, the correct
language was among our top four speculations 50.2% of
the time. Note the significant improvement over random
guessing, which would only place the correct language
in the top four choices approximately 19% of the time.
Indonesian is correctly classified in our top three choices
57% of the time, and even Arabic—the language with the
lowest overall classification rates—was correctly placed
among our top three choices 30% of the time.
In many cases, it might be worthwhile to distinguish
between only two languages, e.g., whether an encrypted
conversation in English or Spanish. We performed tests
that aimed at identifying the correct language when sup-
plied only two possible choices. We see a stark improve-
ment over random guessing, with seventy-five percent of
the language combinations correctly distinguished with
an accuracy greater than 70.1%; twenty-five percent had
accuracies greater than 80%. Our overall binary classiﬁ-
48
16th USENIX Security Symposium
USENIX Association
Confusion Matrix: All Languages
e
g
a
u
g
n
a
L
l
e
d
o
M
VI
TA
SW
SP
SD
RU
PO
MA
KO
JA
IT
IN
HU
HI
GE
FA
EN
CZ
CA
BP
AR
AR
BP
CA
ENCZ
FA
GE
INHUHI
IT
KOJA
User Language
POMA
RU
SD
SWSP
TA
VI
21 Way Classification Accuracy
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
F
D