title:A Formal Framework for Environmentally Sensitive Malware
author:Jeremy Blackthorne and
Benjamin Kaiser and
B&quot;ulent Yener
A Formal Framework for Environmentally
Sensitive Malware
Jeremy Blackthorne(B), Benjamin Kaiser, and B¨ulent Yener
Rensselaer Polytechnic Institute, Troy, USA
{whitej12,byener}@rpi.edu, PI:EMAIL
Abstract. Theoretical
investigations of obfuscation have been built
around a model of a single Turing machine which interacts with a user.
A drawback of this model is that it cannot account for the most com-
mon approach to obfuscation used by malware: the observer eﬀect. The
observer eﬀect describes the situation in which the act of observing some-
thing changes it. Malware implements the observer eﬀect by detecting
and acting on changes in its environment caused by user observation.
Malware that leverages the observer eﬀect is considered to be environ-
mentally sensitive.
To account for environmental sensitivity, we initiate a theoretical
study of obfuscation with regards to programs that interact with a user
and an environment. We deﬁne the System-Interaction model to for-
mally represent this additional dimension of interaction. We also deﬁne
a semantically obfuscated program within our model as one that hides
all semantic predicates from a computationally bounded adversary. This
is possible while still remaining useful because semantically obfuscated
programs can interact with an environment while showing nothing to the
user. In this paper, we analyze the necessary and suﬃcient conditions of
achieving this standard of obfuscation and show how these conditions
relate to real-world programs.
Keywords: Malware · Tamper-resistance · Obfuscation · Formaliza-
tion · Framework · Environmental sensitivity · Environmental keying
1 Introduction
Program obfuscation is deﬁned as the transformation of code with the intent
of making it “hard” to understand while maintaining functionality. Authors of
commercial software obfuscate their products to protect their intellectual prop-
erty, criminals obfuscate their malware to protect against detection by anti-virus
software, and nations obfuscate cyber-weapons to prevent repurposing. But no
obfuscation technique thus far has been able to guarantee any provable security
for everyday programs. This is partly due to the large divide in the theoretical
and systems approaches. Each approach has its strengths and weaknesses. It is
the goal of this work to combine the strengths of both theoretical and systems
approaches in order to shed additional light on the subject of obfuscation.
c(cid:2) Springer International Publishing Switzerland 2016
F. Monrose et al. (Eds.): RAID 2016, LNCS 9854, pp. 211–229, 2016.
DOI: 10.1007/978-3-319-45719-2 10
212
J. Blackthorne et al.
Theorectical Approach. Theoretical work in obfuscation focuses on simpliﬁed
models, well-deﬁned properties, and provable results. The most famous of these
properties is the virtual black-box (VBB) property, deﬁned by Barak et al. [4].
Informally, the property states that an adversary should gain no more informa-
tion from the obfuscated source code than they would gain through black-box
access to the original program. This is seen as the ultimate goal of formalized
obfuscation because no more information could possibly be hidden without also
hindering functionality. Recently, VBB obfuscation schemes have actually been
constructed [3,12].
Another standard of obfuscation is deﬁned by the indistinguishability prop-
erty, also ﬁrst established by Barak et al. [4]. It concerns functionally equiva-
lent programs that have multiple distinct circuit implementations. The property
states that such a program could be run without the adversary knowing which of
the possible circuits was actually used. Garg et al. provided a construction that
was proven secure under a very restricted model which only allows adversaries
to perform computations on matrices in a speciﬁc order [20].
There are many other well-deﬁned obfuscation types, such as extractability
[10], virtual grey-box [9], tau [7], and best-possible [21]. All of these obfuscation
types are weaker than VBB, so all comparisons to our own deﬁnitions will be
with VBB.
Limitations. A common theme among theoretical obfuscation is the use of a
single Turing machine (TM) as a model. The representation of the complex
interactions between users, software, and hardware as a single TM that interacts
with a user leads to limitations on what can be achieved by obfuscation. For
instance, if the program is to be useful in any way, it must allow access to
its input and output. This limits the goal of obfuscation to protect only the
transformation from input to output. Consider the case of malware, in which
a program needs to interact with a system and simultaneously not show any
information to the user. The single TM model does not account for interactions
other than with the user, and hence cannot properly represent this case.
Another limitation implicit in the single TM model is the lack of security pos-
sible for learnable functions. A function is learnable if an adversary can determine
the deﬁnition of the function through only its inputs and outputs. Consider a
program that implements the function f(x) = x2. Even without access to the
program itself, an adversary can guess the deﬁnition by querying the function
at a few locations. Because programs obfuscated in the single TM model must
allow access to inputs and outputs to be useful, the obfuscated program cannot
be both learnable and meaningfully obfuscated.
Systems Approach. The systems approach relies on the concept that observation
requires modiﬁcation. This modiﬁcation can be either of the program itself or
the environment. Because observation requires modiﬁcation, and modiﬁcation
can be detected, observation can be detected. Authors can design programs
that detect observation and create deviations in execution, which ultimately
A Formal Framework for Environmentally Sensitive Malware
213
impedes observation. This allows obfuscated software to behave in diﬀerent ways
depending on whether or not it is being observed.
The practical set of techniques that leverage the observer eﬀect are known
most commonly by their anti-X names, where X is the observation environment.
Examples are anti-debugging [14,18,30], anti-virtual machine (VM) [17,30], and
anti-emulator [22,26]. There are even special-purpose commercial and malicious
programs called packers or protectors which combine many of these techniques
with obfuscation in an easy to use package [23]. Packers allow users to protect
their binaries without needing to modify the source code.
The anti-X techniques look for artifacts of observation during execution and
change the behavior of the program when found. Obviously, one of goals of
these analysis environments is to not create artifacts. The result is an arms race
between detecting new artifacts and building environments that do not have
them. This is in contrast to the large body of research regarding theoretical
obfuscation, in which the adversary is allowed to observe without cost. In the
theoretical approach, there are no artifacts and there is no eﬀect on the running
program when it is being observed. After all, Turing machines are mathematical
objects that can be described at any time-step without actually aﬀecting them.
Limitations. Many obfuscation schemes have been proposed and implemented
using the methods in the systems approach, but they are ultimately heuristic in
nature with no provable security [11,15,28]. Security standards cannot be proven
because the concepts are not suﬃciently formalized.
1.1 Results
System-Interaction Model. We introduce a model that accounts for the hardware,
operating system (OS), program, and user. We deﬁne a new type of obfuscation
within this model called semantic obfuscation. This obfuscation hides all seman-
tic predicates about a program from a user while allowing the program to remain
useful. This is possible within the System-Interaction model because a program
can still give output to the OS while showing nothing to the user.
Existing Sensors. Two common ways a program can measure itself and its envi-
ronment are memory hashing and timing checks. Memory hashing describes the
process of a program running a hash over sections of memory. Timing checks
are the recording of the time it takes to execute batches of instructions. Both
techniques can be used to check the integrity of a program or its environment.
We formalize these sensors and strengthen them by allowing them access to a
random oracle based on the hardware of the computer. We show the impossibil-
ity of achieving semantic obfuscation with either of these sensors – individually
or combined.
Ideal Sensor. The ideal sensor approximates a random oracle by returning a
random number based on the state of the OS and program. In this way, no bit
can be changed in either without it registering with the sensor. We show that
214
J. Blackthorne et al.
with access to an ideal sensor, there exists a semantically obfuscated program
that runs in polynomial time and which guarantees exponential time to deob-
fuscate. The catch is that it also takes exponential time to construct. We show
that a semantically obfuscated program which uses an ideal sensor actually can
take no less than exponential time to construct. This obfuscator must create
the obfuscated program using the hardware on which the obfuscated program
is intended to run because each hardware has its own unique randomness. This
naturally places the obfuscator at a practical disadvantage because the adversary
can try to deobfuscate the program with the hardware of their choice, includ-
ing much faster hardware than what was intended (or many separate hardware
conﬁgurations running in parallel).
Piece-Wise Sensor. We propose a sensor constructed from a piece-wise function.
This function has asymmetric properties which allow environmentally sensitive
software to achieve semantic obfuscation within the System-Interaction model.
We believe this sensor can be practically implemented.
1.2 Related Work
In addition to the ﬁeld of obfuscation, our work relates to tamper-resistance.
Canetti and Varia formally deﬁne nonmalleable obfuscation, another term for
tamper-resistance, and show how existing point functions can achieve their secu-
rity standard in the random oracle and common reference string models [13].
Another formalization of tamper-resistance is provided by Basile et al. [6], for
which they deﬁne tamper protection techniques in terms of attacker goals, capa-
bilities, and limitations.
A hardware approach to tamper-resistance is physically unclonable functions
(PUFs). These are hardware devices that are unique, hard to replicate, and
produce random output. The concept of program sensors within the System-
Interaction model is closely related to work done in PUFs. Recent work in PUFs
has explored the idea of using intrinsic properties of commercial oﬀ-the-shelf sys-
tems [24,25]. Plaga and Koob [27] formally describe PUFs and their limitations.
There have been a few cases of formalization for transparent analysis, i.e.
analysis that does not induce an observer eﬀect, but nothing yet reaching a
rigorous treatment of the subject. Dinaburg et al. present a formalization of
transparent malware analysis and describe its requirements [16]. Their require-
ments are higher privilege, the absence of side-channels, transparent exception
handling, and identical timings. Kang et al. also brieﬂy formulate the problem of
transparent malware analysis within emulators [22], but do not oﬀer any further
investigation.
2 Preliminaries
2.1 Notation
TM is short for Turing machine. UTM is short for universal Turing machine.
PPT is short for probabilistic polynomial-time Turing machine. A TM can also
A Formal Framework for Environmentally Sensitive Malware
215
be encoded as a bitstring b ∈ {0, 1}n, for some natural number n, and used
as input to other TM’s. Oracle access is input–output only access. A TM A
running on input string x with oracle access to a TM B is represented as AB(x).
A function α : N → N is called negligible if it grows slower than any other
polynomial.
2.2 Properties of Turing Machines
A property of a TM can be expressed as a yes-no question, sometimes called a
predicate. An individual predicate of a TM is denoted by π(M), where M is any
TM. There are two types of TM properties, semantic and syntactic [2]. Semantic
properties are those that are dependent on the input–output relationship of the
function the TM implements. In other words, a semantic property of a TM is
also a property of the language which the TM recognizes. Syntactic properties
are not necessarily dependent on the language which a TM recognizes, but rather
the encoding of that TM.
Obfuscation is the syntactic transformation of a program while maintaining
its semantic properties. In other words, the obfuscated program and original
program should be input–output equivalent, yet appear diﬀerent. It is the rela-
tionship between the syntax and semantics that is being obfuscated.
2.3 Deﬁnitions
Deﬁnition 1. (Instantaneous Description). The instantaneous description (ID)
of a TM is deﬁned as
ID : T M → Γ ∗
M (cid:4)→ γ,
where
1. Γ is the tape alphabet,
2. M ∈ T M,
3. and γ ∈ Γ ∗, which is the speciﬁc string of characters which represent the total
contents of the tape at the time-step in which the ID function is called. The
contents of the tape include M and any input to M. This string is the state