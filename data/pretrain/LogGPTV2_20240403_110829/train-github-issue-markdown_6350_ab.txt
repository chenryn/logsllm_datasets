### Dependencies
The following dependencies are installed in the project:

- **colorama**: 0.4.3 (from wheel)
- **crashtest**: 0.3.1
- **cryptography**: 3.4.7
- **distlib**: 0.3.2
- **docutils**: 0.15.2 (from wheel)
- **fastapi**: 0.63.0 (from wheel)
- **filelock**: 3.0.12
- **flake8**: 3.9.2 (from wheel)
- **gunicorn**: 20.1.0 (from wheel)
- **h11**: 0.12.0 (from wheel)
- **html5lib**: 1.1
- **idna**: 2.10
- **iniconfig**: 1.1.1 (from wheel)
- **jeepney**: 0.6.0
- **Jinja2**: 3.0.1 (from wheel)
- **jmespath**: 0.10.0 (from wheel)
- **keyring**: 21.8.0
- **kombu**: 5.1.0 (from wheel)
- **lockfile**: 0.12.2
- **Mako**: 1.1.4 (from wheel)
- **MarkupSafe**: 2.0.1 (from wheel)
- **mccabe**: 0.6.1 (from wheel)
- **msgpack**: 1.0.2
- **multidict**: 5.1.0 (from wheel)
- **orjson**: 3.5.3 (from wheel)
- **packaging**: 20.9
- **pastel**: 0.2.1
- **pexpect**: 4.8.0
- **pkginfo**: 1.7.0
- **pluggy**: 0.13.1 (from wheel)
- **poetry**: 1.1.6
- **poetry-core**: 1.0.3
- **prompt-toolkit**: 3.0.18 (from wheel)
- **psycopg2-binary**: 2.8.6 (from wheel)
- **ptyprocess**: 0.7.0
- **py**: 1.10.0 (from wheel)
- **pyasn1**: 0.4.8 (from wheel)
- **pycodestyle**: 2.7.0 (from wheel)
- **pycparser**: 2.20
- **pydantic**: 1.8.2 (from wheel)
- **pyflakes**: 2.3.1 (from wheel)
- **pylev**: 1.4.0
- **pyparsing**: 2.4.7
- **pytest**: 6.2.4 (from wheel)
- **pytest-aiohttp**: 0.3.0 (from wheel)
- **python-dateutil**: 2.8.1 (from wheel)
- **python-editor**: 1.0.4 (from wheel)
- **pytz**: 2021.1 (from wheel)
- **PyYAML**: 5.4.1 (from wheel)
- **redis**: 3.5.3 (from wheel)
- **requests**: 2.25.1
- **requests-toolbelt**: 0.9.1
- **rsa**: 4.7.2 (from wheel)
- **s3transfer**: 0.4.2 (from wheel)
- **SecretStorage**: 3.3.1
- **shellingham**: 1.4.0
- **six**: 1.16.0
- **SQLAlchemy**: 1.3.24 (from wheel)
- **starlette**: 0.13.6 (from wheel)
- **toml**: 0.10.2 (from wheel)
- **tomlkit**: 0.7.2
- **typing-extensions**: 3.10.0.0 (from wheel)
- **ujson**: 1.35 (from source)
- **urllib3**: 1.26.5
- **uvicorn**: 0.13.4 (from wheel)
- **vine**: 5.0.0 (from wheel)
- **virtualenv**: 20.4.7
- **wcwidth**: 0.2.5 (from wheel)
- **webencodings**: 0.5.1
- **yarl**: 1.6.3 (from wheel)

### Other Dependencies
- **ReplicationGroup**:
  - **Type**: AWS::ElastiCache::ReplicationGroup
  - **Properties**:
    - **AutomaticFailoverEnabled**: true
    - **CacheNodeType**: cache.t3.medium
    - **Engine**: redis
    - **EngineVersion**: default.redis6.x
    - **NumCacheClusters**: 2

### Minimally Reproducible Test Case
```python
pipelines = []
for task in Factory.gen_tasks(request):
    pipelines.append(
        chain(
            post_new.si(task),
            http_task_executor.si(task),
            group(
                chain(
                    post_status_update.s(task.id),
                    post_status_log.s(task.id),
                ),
                post_reply.s(task.id),
            ),
        )
    )
async_task = group(pipelines).apply_async()
```

- **`post_new`**: Returns `None`
- **`http_task_executor`**: Returns `arg` for the next `group`
- **`post_status_update`**: Consumes `arg` from `http_task_executor` task
- **`post_reply`**: Consumes `arg` from `http_task_executor` task

### Expected Behavior
The tasks should execute step by step without interruption.

### Actual Behavior
There is an intermittent issue with the Celery task pipeline. Most of the time (approximately 99%), the tasks execute as expected. However, sometimes the execution stops silently in the middle after a successful run of the previous task. This can happen after the `post_new` or `http_task_executor` tasks.

In both cases, the logs show that the previous task completed successfully, but the next task is not received by a worker, and the queue is empty. This suggests that the message might not be sent to the broker. Debug logs only show information about internal heartbeats, with no errors or exceptions.

This issue is less frequent when using `docker-compose`.

### Example Log Entry
```plaintext
[2021-06-15 19:52:32,995: INFO/ForkPoolWorker-1] Task task.http_task_executor[07cd4913-2222-4ed6-8749-ae8a663c2aec] succeeded in 1.5408981110085733s:
```

### Additional Information
- **Pool**: prefork
- **Concurrency**: 8

### Possible Causes
1. **Worker Unable to Store Results in Redis Broker**: If a worker cannot store results in the Redis broker, it might silently ignore the error.
2. **Message Loss in RabbitMQ**: Another possibility is the loss of messages in RabbitMQ, which needs further investigation.

### Next Steps
- Investigate potential issues with the Redis broker and ensure that workers can store results correctly.
- Check for message loss in RabbitMQ and verify its configuration and stability.