Policy
P1
P2
P3
Policy Speciﬁcation
service = “EC2” and vmm = “CloudVisor” and
version ≥ “1” and instance = “large”
service = “EC2” and vmm = “CloudVisor” and
(zone = “Z1” or zone = “Z3”)
service = “EC2” and vmm = “CloudVisor” and
country = “DE”
Table 3: Examples of policies. P1 expresses version and VM
instance type requirements, P2 speciﬁes a zone preference for
different sites, and P3 expresses a regional preference.
take approximately one second to complete. This inef-
ﬁciency hampers the scalability of cloud services that
use the TPM and can even open avenues for denial of
service attacks if the TPM abstractions were invoked by
customer-accessible operations.
Finally, the cloud infrastructure may be overexposed.
By revealing TPM node identities and allowing cus-
tomers to remotely attest the nodes, any outsider could
learn, for instance: (1) the number of cloud nodes that
constitute the infrastructure of the cloud provider, and
(2) the distribution of different platforms they run. This
information could be used by external attackers to trace
vulnerabilities in the infrastructure, or by competitors to
learn business secrets. Handing over such information
is often unacceptable to cloud providers.
Recent proposals for TPMs in the cloud do not com-
pletely address these TPM limitations. Systems like
Nexus [50] or CloudVisor [53] use TPMs to allow cus-
tomers to remotely attest only a single cloud node and
therefore do not address the preceding issues. Essen-
tially, these systems address the complementary prob-
lem of securing the platform running on a single node.
Our previous workshop paper [45] took preliminary
steps to address some of these issues, but its solution
did not handle situations where sensitive data needed to
be secured persistently, which is unrealistic to assume
on real-world cloud services; our prior solution also suf-
fered from scalability limitations.
4.3 The Policy-sealed Data Abstraction
To overcome these limitations, we propose the new
policy-sealed data abstraction. This abstraction allows
customer data to be bound to cloud nodes whose con-
ﬁguration is speciﬁed by a customer-deﬁned policy.
Policy-sealed data offers two primitives for securing
customer data: seal and unseal. Seal can be invoked
anywhere – either on the customer’s computer or on the
cloud nodes. It takes as input the customer’s data and
a policy and outputs ciphertext. The reverse operation,
unseal, can be invoked only on the cloud nodes that need
to decrypt the data. Unseal takes as input the sealed data
and decrypts it if and only if the node’s conﬁguration
satisﬁes the policy speciﬁed upon seal; otherwise, de-
cryption fails.
With our abstraction, each cloud node has a conﬁgu-
ration, which is a set of human-readable attributes. At-
tributes express features that refer to the node’s software
(e.g., “vmm”, “version”) or hardware (e.g., “location”).
A policy expresses a logical condition over the attributes
supported by the provider (e.g., “vmm=Xen and loca-
tion=US”). Table 1 shows an example of the attributes
of a hypothetical deployment of a service akin to EC2.
Table 2 illustrates the conﬁguration of a particular node,
and Table 3 lists example policies over node conﬁgura-
tions in that deployment.
Our primitive can replace the existing remote attesta-
tion and sealed storage calls for securing customer data
on the cloud. In particular, to protect data upon upload
or migration, the customer needs only to seal the data to
a policy: if the destination cannot unseal the data, then
its conﬁguration does not match the policy; therefore,
the node is not trusted from the perspective of the cus-
tomer who originally speciﬁed the policy.
5 Excalibur Design
This section presents Excalibur, a system that provides
policy-sealed data support for building trusted cloud ser-
vices.
5.1 Design Goals & Assumptions
Our central goal is to design and implement a system
that offers the policy-sealed data primitive by making
use of commodity TPMs. Furthermore, the system de-
sign must overcome the preceding limitations of the in-
terface offered by current TPMs.
We focus on the design of the primitive used by the
cloud platforms running on individual nodes. There-
fore, we are not concerned with securing these plat-
forms themselves. In particular, our goal is not to pre-
vent the management interface exposed to cloud ad-
ministrators from leaking or corrupting sensitive data
(e.g., direct memory inspection of VM memory). Simi-
larly, we require that the individual cloud platforms pro-
4
dentials that are sent to the node. These credentials are
required by cloud nodes to unseal policy-sealed data and
are destroyed whenever the nodes reboot.
The monitor exposes a narrow management interface
that lets the cloud administrator conﬁgure the mappings
between attributes and identities (i.e., ﬁngerprints). This
is necessary for routing system maintenance as new soft-
ware platforms and cloud nodes are deployed on the
infrastructure. The management interface also allows
multiple clones of the monitor to be securely spawned
in order to scale up the system. To assure customers that
it is properly maintained, the monitor accepts only map-
pings that are vouched for by special certiﬁcates; cus-
tomers can directly attest the monitor in order to check
its authenticity and integrity.
Though our high-level design is simple, we still need
to overcome two main challenges: 1) to cryptographi-
cally enforce policies in a scalable, fault tolerant and ef-
ﬁcient way, and 2) to assure customers that the monitor
operates correctly despite the fact that it is managed by
untrusted cloud administrators. To address these chal-
lenges, we: 1) use CPABE cryptography to enforce poli-
cies, and 2) devise certiﬁcates and a scalable monitor at-
testation mechanism to ensure that the monitor is trust-
worthy. We next explain these design choices in more
detail.
5.3 Cryptographic Enforcement of
Policies
The main challenge in implementing the seal and un-
seal primitives is avoiding scalability bottlenecks. A
possible design is for the monitor itself to evaluate the
policies: upon sealing, the client encrypts the data with
a symmetric key and sends this key and the policy to
the monitor; the monitor then encrypts this key and the
policy with a secret key and returns the outcome to the
client. To unseal, the encrypted key is sent to the moni-
tor, which internally recovers the original symmetric key
and policy, evaluates the policy, and releases the sym-
metric key if the node satisﬁes the policy. Although this
solution implements the necessary functionality, it in-
volves the monitor in every seal and unseal operation
and thereby introduces a scalability bottleneck.
An alternative design is to evaluate the policies on
the client side using public-key encryption. Each cloud
node receives from the monitor a set of private keys that
match its conﬁguration; in this scheme, each key cor-
responds to an attribute-value pair of the conﬁguration.
Sealing is done by encrypting the data with the corre-
sponding public keys according to the attributes deﬁned
in the policies. This solution avoids the bottlenecks of
the ﬁrst approach because all cryptographic operations
take place on the client side, without involving the mon-
itor. Its main shortcoming is complicated key manage-
Figure 1: Excalibur deployment. The dashed lines show the
ﬂow of policy-sealed data, and the solid lines represent inter-
actions between clients and the monitor. The monitor checks
the conﬁguration of cloud nodes. After a one-time monitor at-
testation step, clients can seal data. Data can be unsealed only
on nodes that satisfy the policy (unshaded boxes).
tect certain key material used to seal and unseal data,
and that the system interface does not allow the ﬁnger-
print stored in the TPM to be changed so that it be-
comes inconsistent with the current system state. To
address these complementary goals, applications must
make use of a series of existing systems and hardening
techniques [20, 24, 33, 53].
5.2 System Overview
The design of Excalibur is based on a centralized com-
ponent, called a monitor. The monitor is a dedicated
service running on a single cloud node (or, as we will
explain, on a small set of nodes for fault tolerance and
scalability). It coordinates the enforcement of policy-
sealed data on the entire cloud infrastructure by map-
ping TPM identities and ﬁngerprints of the cloud nodes
to policy-sealed data attributes. Only the monitor can
trigger TPM primitives on the cloud nodes, minimizing
the negative performance impact of TPM operations and
preventing the exposure of infrastructure details.
Figure 1 illustrates a deployment of Excalibur, high-
lighting the separation between the two main system
components: the client and the monitor. The client con-
sists of a library that allows the implementation of a
trusted cloud service to use the policy-sealed data prim-
itives. This library can be used on both the customer
side (e.g., before uploading data) and by the software
platforms running on the cloud nodes (e.g., before mi-
grating data between nodes). The customer-side client
does not expose the unseal primitive since the notion of
a conﬁguration applies to cloud nodes only.
Whenever a cloud node reboots, the monitor runs a
special remote attestation protocol to obtain the ﬁnger-
print and identity of the node and translates these to a
node conﬁguration by consulting an internal database.
The node conﬁguration — which expresses physical
characteristics, like hardware or location, and software
features as a set of attributes — is then encoded as cre-
5
ment due to the number of key-pairs that nodes must
handle to reﬂect all possible attribute combinations us-
able by policies.
The solution we chose uses a cryptographic scheme
called Ciphertext Policy Attribute-Based Encryption
(CPABE) [11]. This scheme ﬁrst generates a pair of
keys: a public encryption key and a secret master key.
Unlike traditional public key schemes, the encryption
key allows a piece of data to be encrypted and bound to
a policy. A policy is a logical expression that uses con-
junction and disjunction operations over a set of terms.
Each term tests a condition over an attribute, which can
be a string or a number; both types support the equality
operation, but the numeric type also supports inequali-
ties (e.g., a = x or b > y). CPABE can then create
an arbitrary number of decryption keys from the same
master key, each of which can embed a set of attributes
speciﬁed at creation time. The encrypted data can be
decrypted only by a decryption key whose attributes sat-
isfy the policy (e.g., keys embedding the attribute a = x
can decrypt a piece of data encrypted with the preceding
example policy).
Excalibur uses CPABE to encode the runtime conﬁg-
urations of the cloud nodes into decryption keys. At
setup time, the monitor generates a CPABE encryption
and master key pair and secures the master key. When-
ever it checks the identity and software ﬁngerprint of
a cloud node, the monitor sends the appropriate creden-
tials to the node, which include a CPABE decryption key
embedding the attributes that correspond to the conﬁg-
uration of the node; the decryption key is created from
the master key and forwarded to all the nodes featuring
the same conﬁguration. Sealing is done by encrypting
the data using the encryption key and a policy, and un-
sealing is done by decrypting the sealed data using the
decryption key. Policies are expressed in the CPABE
policy language used to specify the examples in Table 3
as well as more elaborate policies.
The security of the system then depends on the se-
curity of the CPABE keys. The monitor protects the
master key by: 1) ensuring that it cannot be released
through the monitor’s management interface, and 2) en-
crypting it before storing it on disk, as described in
Section 6.3. Additionally, cloud platforms must pro-
tect decryption keys. A software platform must pre-
vent leakage or corruption of key material through its
management interface (e.g., by direct memory inspec-
tion of VM memory); it must hold the key in volatile
memory so that key material is destroyed upon reboot.
Moreover, the software platform must force a reboot af-
ter changing TCB components that get measured during
a trusted boot (e.g., subsequent to upgrading the hyper-
visor). These properties ensure that the CPABE decryp-
tion keys of cloud nodes remain consistent with their
TPM ﬁngerprints and therefore reﬂect current node con-
ﬁgurations.
The beneﬁts of using CPABE are twofold. First, it lets
the system scale independently of the workload since
the seal and unseal primitives do not interact with the
monitor (and run entirely on the client side). Second,
it permits the creation of expressive policies directly
supported by the CPABE policy speciﬁcation language
while only requiring two keys – the CPABE encryption
and decryption keys – to be sent to the nodes.
The cost using CPABE is a performance hit when
compared to traditional cryptographic schemes. Sec-
tion 6 explains how this impact can be minimized. A
second cost of using CPABE is key revocation, which is
typically difﬁcult in identity- and attribute-based cryp-
tosystems. Since Excalibur assumes that the TCB of
nodes’ software platforms is secure, any TCB vulner-
ability accessible through the administrator’s interface
will invalidate the guarantees provided by our system.
To handle revocation of decryption keys, our current de-
sign requires that all sealed data whose original policy
satisﬁes the attributes of the compromised keys be re-
sealed. This operation can be done efﬁciently by re-
encrypting only a symmetric key, not the data itself.
5.4 Trusting the Monitor
Since the monitor is managed by the cloud administra-
tor, mismanagement threats that affect any cloud node
could also affect the monitor. Thus, another challenge
is to ensure that the monitor operates correctly and to
efﬁciently convey this guarantee to customers.
To meet this challenge, we must ﬁrst prevent the mon-
itor from accepting ﬂawed attribute mappings. For ex-
ample, a mapping would be ﬂawed if the attribute “lo-
cation=DE” were mapped to the identity of a node lo-
cated in the US, or if the attribute “vmm=Xen” were
mapped to the ﬁngerprint of CloudVisor. To prevent
this, the monitor only accepts attribute mappings that
are vouched for by a certiﬁcate. A certiﬁcate is issued
by one or multiple certiﬁers, which validate the correct-
ness of mappings. For example, a certiﬁer checks the
location of nodes and the ﬁngerprints of software plat-
forms. This role could be played by the provider itself,
or by external trusted parties akin to Certiﬁcation Au-
thorities.
Since anyone can issue certiﬁcates, the monitor must
let customers know the certiﬁer’s identity so they can
judge the certiﬁer’s trustworthiness and thereby be con-
ﬁdent that the attribute mappings are correct. Fur-
thermore, even if the certiﬁer were judged trustworthy,
the system must nevertheless provide additional guar-
antees about the authenticity and integrity of the mon-
itor: only in this case can the customer be sure that
the certiﬁcate-based protections and the security proto-
6
cols implemented by the monitor are correct. To pro-
vide these guarantees, customers must directly attest the
monitor when ﬁrst using the system.
5.5 Monitor Scalability and Fault
Tolerance
To improve scalability and make Excalibur resilient to
faults, we enable several monitor replicas (clones) to be
spawned, and we optimize the monitor attestation pro-
tocol.
Monitor clones can be elastically launched and termi-
nated by the administrator, using the protocol described
in Section 6.7. The cloud provider can then use standard
load balancers to evenly distribute client attestation re-
quests from clients among clones. Each clone can serve
requests without communicating with other clones.
To eliminate critical bottlenecks within a clone, we
introduce two optimizations. The ﬁrst improves the
throughput of clone attestations triggered by customers.
Due to TPM inefﬁciencies, the maximum throughput of
a monitor clone using a standard attestation protocol is
close to one attestation per second, clearly insufﬁcient
even when spawning a reasonable number of clones. We
therefore enhance the attestation protocol with a tech-
nique based on Merkle trees that can batch a large num-
ber of attestation requests into a single TPM quote (see
Section 6).
A second optimization improves the throughput of
decryption key requests issued by the cloud nodes. The
algorithm for decryption key generation is also inef-
ﬁcient, which could signiﬁcantly slow down servicing
keys to the cloud nodes if a new key were to be gener-
ated per request. Since many machines in the datacenter
share the same conﬁguration (e.g., machines that belong
to the same cluster), the monitor clone can instead se-
curely cache the decryption keys and send them to all
the nodes with the same proﬁle.
6 Detailed Design
This section presents the design of Excalibur in more
detail. We ﬁrst introduce certiﬁcates, which constitute
the root-of-trust of the system. We then describe the in-
terfaces offered by Excalibur for building cloud services
and managing the system. Finally, we present the secu-
rity protocols that enforce policy-sealed data.
Notation. For CPABE keys, K M, K E and K D denote
master, encryption, and decryption keys, respectively.
For asymmetric cryptography, K and K P denote pri-