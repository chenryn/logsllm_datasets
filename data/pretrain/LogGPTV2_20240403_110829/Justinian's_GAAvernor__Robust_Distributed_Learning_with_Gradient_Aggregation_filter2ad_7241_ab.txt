of the tampering algorithm for each malicious worker.
With the notions above, the behavior of the manipulated
worker i at iteration t can be described as
1. First, the adversary selects the current role of the worker i
as ri(t).
2. If the role is benign, i.e., ri(t) = 1, then the worker honestly
computes the gradient on its local data, that is, V t
i .
3. Otherwise, i.e., ri(t) = 0, it tampers the gradient V t
i with
certain tampering algorithm T (e.g., random fault) and
produces T (V t
4. Finally, the produced gradient is sent back to the server.
i ).
3.2 Previous Defenses
In order to ﬁght against the aforementioned threat model,
previous works proposed several alternative GARs to classical
GAR and its linear variants. We brieﬂy review the state-of-
the-art defenses as follows, where m out of n workers are
assumed to be Byzantine at certain iteration, s.t. m/n ≤ β. For
an overview, please refer to Table 1.
Brute-Force [31, 48] is based on a brute-force search for
an optimal subset C∗ in Q of size n− m with the minimal
maximum pairwise distance. Formally, the optimal set can be
written as C∗ = argminC∈R max(Vi,Vj)∈C×C (cid:107)Vi −Vj(cid:107), where
R := {C ⊂ Q : |C| = n− m}. Then the proposed weight up-
USENIX Association
29th USENIX Security Symposium    1643
date direction is calculated as F (V1, . . . ,Vn) = 1
n−m ∑V∈C∗ V .
It was proved to be perfectly robust when n ≥ 2m + 1 [48],
while it is almost intractable in highly distributed learning
systems.
GeoMed [16, 62] computes the geometric median of Q as
the proposed estimator, which assumes the Byzantine ra-
tio satisﬁes n ≥ 2m + 1 [16, 62]. In consideration of the
computational complexity of geometric median when n is
large [18], recent works on Byzantine robustness proposed to
approximate it with the vector in Q which has the smallest
sum of distance with other gradients, i.e., F (V1, . . . ,Vn) :=
argminVi ∑ j(cid:54)=i(cid:107)Vi −Vj(cid:107).
Krum [11] was recently proposed in [11] as an approximate
algorithm to Brute GAR, which assumes the Byzantine ra-
tio satisﬁes n ≥ 2m + 3. It ﬁrst ﬁnds the n− m− 2 closest
vectors in Q for each Vi, which is denoted as i → j in their
original work. Next, it computes a score for each vector Vi
with the formula s(Vi) = ∑i→ j (cid:107)Vi −Vj(cid:107)2. Finally, it proposes
the vector Vi with the smallest score as the next update step,
i.e., F (V1, . . . ,Vn) = argminVi∈Q s(Vi).
Bulyan [31] was originally designed for Byzantine attacks
that concentrate on a single coordinate. First, it runs Krum
over Q without replacement for n− 2m time and collect the
n− 2m gradients to form a selection set. It then computes
F coordinate-wise: the i-th coordinate of F is equal to the
average of the n− 4m closest i-th coordinates to the median
i-th coordinate of the selection set. Bulyan has the strictest
assumption as n ≥ 4m +3 (and otherwise it is not executable),
which signiﬁcantly limits its practical usage.
As we can see, the aforementioned approaches only con-
sidered the limited situation when β is expected to be smaller
than 1/2. In more general cases, e.g., when there is no explicit
upper bound on the Byzantine ratio in the system, merely no
defenses above could remain robust any longer. The following
proposition provides a typical failure case.
Proposition 2. Consider the submitted gradients at iteration
t as (V1, . . . ,Vn−m,B1, . . . ,Bm) where {Bi}m
i=1 are Byzantine
gradients. For the slightest violations in each case, i.e., n = 2m
for Brute GAR, GeoMed and n = 2m + 2 for Krum, the adver-
sary can simply take B1 = B2 = . . . = Bm = E to tempt these
GARs to always yield E, any arbitrary direction speciﬁed by
the adversary.
In practice, this more challenging situation could happen
for distributed learning systems in open network environments
[61]. When the adversary has already compromised a majority
of workers at the beginning or continuously gains malicious
control over each worker during the learning process, the
Byzantine ratio in system could go over 1/2 or even ﬂuctuate
with uncertainty. In either cases, the system robustness is no
longer under guard with the above defenses.
4 Defense with Gradient Aggregation Agent
4.1 Overview
In order to restore robustness in a more general scenario, we
suggest the defender to be combined more tightly with the
underlying learning process, by utilizing some auxiliary infor-
mation inside the distributed learning system for mitigation
purposes. Before providing an overview of our methodology,
we ﬁrst clarify our security assumptions and present our goals
of defense.
4.1.1 Security Assumptions. We make the following assump-
tions on the distributed learning system where GAA is to be
deployed.
Assumption 1. The server is secure.
Assumption 2. There is one worker that is never controlled
by the adversary.
Assumption 3. The local datasets on workers are i.i.d. sam-
pled from the unknown distribution D.
Assumption 4. GAA has access to a quasi-validation set
B of size S, which consists of i.i.d. samples from a sample
distribution Pm s.t. KL(Pm||D) < ∞, i.e., the KL-divergence
between Pm and D is upper bounded by a constant.
Here, Assumptions 1 & 3 are commonly adopted in pre-
vious studies [4, 11, 16, 31, 62]. As GAA is deployed on the
server, Assumption 1 guarantees its correct execution. Notice-
ably, Assumption 2 relaxes the known slightest requirements
on the tolerable Byzantine ratio to 1− 1/n. As a trade-off, we
require Assumption 4 to introduce an additional condition on
the availability of a quasi-validation set that follows a similar
but not necessarily identical distribution as the true sample
distribution. In theory we prove the lower the divergence, the
better the model performance will be (Thm. 1 & 2). Through
empirical evidences, we show this assumption can be easily
satisﬁed with the quasi-validation set that consists of few sam-
ples from similar data domains, if there is no provided golden
validation set [34, 61].
4.1.2 Defender’s Goals. Towards Byzantine robustness, the
defender’s primary goal is to guarantee the distributed learn-
ing process can minimize the loss function f to an acceptable
threshold, usually compared to the global minimum of the
loss function [31]. In practice, it is also reasonable to mea-
sure the robustness of certain defense by the gaps among the
model’s utility (e.g., the accuracy of an image classiﬁer) when
the defense is equipped, unequipped with or without attacks.
We will provide more details in Section 5.
4.1.3 Methodology Overview. Before detailing the imple-
mentations, we provide an overview of our proposed approach
(Fig. 1). Robust distributed learning with GAA follows the
following procedures: First, on receiving the submitted gradi-
ents from each worker, GAA, an additional module deployed
on the server, executes certain policy to pose credit on each
worker. Intuitively, GAA has limited credit in total and it will
pose higher credit on the worker it trusts more (Step 1). Next,
1644    29th USENIX Security Symposium
USENIX Association
GAA aggregates the gradients based on the credit and then
proposes the weight update decision to the underlying learn-
ing process (Step 2). Finally, the learning process produces a
reward signal based on the quasi-validation set, which is used
to indicate the quality of the update direction (Step 3) and can
further help GAA adjust its policy dynamically (Step 4).
Figure 1: Overview of our proposed defense.
4.2 Distributed Learning as a Markov Deci-
sion Process
Following the conventions of Reinforcement Learning
(RL) [53], we ﬁrst deﬁne the notion of environment, with
which an agent interacts. Standardly, the environment of
a Markov Decision Process (MDP) is represented as a tu-
ple (S ,A,R, p0, p,γ), where S ,A are respectively the set
of states and of actions, R : S → R is the reward function,
p0 : S → R+ is the initial probability density over states and
p : S ×A ×S → R+ is the transition probability density, with
γ ∈ (0,1] the discount factor. In the context of distributed
learning, our speciﬁcations for these components are stated
as follows. Fig. 2 shows an overview of our MDP settings.
Set of States S. In the terminology of MDP, a state usually has
the intuitive meaning as a context, based on which the agent
makes a decision. Naturally, our GAA at iteration t refers to
the tuple st := (Qt ,θt , ˆfB(θt )) as the current state to decide
the next weight update direction. Recall θt ,Qt are respectively
the parameter and the received gradients at iteration t, while
ˆfB(θt ) is deﬁned as the loss at θt estimated by the server on
the quasi-validation set B.
Set of Actions A. Taking advantage of the simplicity of linear
GAR, we propose to deﬁne the action space as an n-dimension
simplex, where n is the number of workers. Generally speak-
ing, our motivation here is to regularize the action space with
prior knowledge and therefore the cost on searching the op-
timal policy can be largely scaled down. By restricting the
feasible action to the space of linear GARs, GAA at each
iteration chooses a candidate internal action αt ∈ Sn based on
the current state st and the previous action αt−1. Intuitively,
this process can be considered as GAA’s posing credit on each
worker. Based on αt, GAA then proposes the current update
step as θt+1 = θt − λ(∑n
i=1 α(i)
It is worth to notice, although the aggregation rule of GAA
is linear in its form, it largely differs from linear GARs in that
the coefﬁcient αt is chosen by a sophisticated agent adaptively
at each iteration rather than predeﬁned, which therefore makes
t V t
n).
Figure 2: Distributed learning as an MDP.
our model immune to the vulnerability innate to linear GARs
[11].
Reward Function R. Reward function is usually deﬁned as
a function from each state s to a scalar value, which pro-
vides heuristics for policy learning. In our context, we set the
reward at iteration t as Rt := ˆfB(θt )− ˆfB(θt+1), namely the
relative loss decrease on the quasi-validation set B. Intuitively,
if KL(Pm||D) is 0, the reward Rt highly reﬂects the changes
in the true loss f [47] and thus provides a good guidance
for GAA’s policy learning. For other situations when Pm is
similar but not necessarily identical with the true distribution,
empirical studies show the reinforcement learning techniques
still work well, probably due to its innate tolerance of noises
in rewards [53].
Initial and Transition Probability Density p0, p. Usually,
these terms are partially unknown to an agent, which could
only be estimated implicitly from observed trajectories [57].
Similarly, our GAA only has the partial knowledge regarding
θ and ˆfB(θ) of p0, with random initialization of parameters,
and of p, with the updating rule above, but totally ignorant
of the initial distribution of Q0 and its transition. In fact, the
learning of GAA is exactly paralleled with an incrementally
accurate estimation of p0 and p, which equivalently means a
better knowledge of the undertaking Byzantine attacks.
Discount Factor γ. Discount factor as a constant in (0,1]
describes how the rewards in history inﬂuence the current
decision, the value of which is determined by different ap-
plication scenarios. Our conﬁgurations can be found in the
evaluation parts.
4.3 Learning Optimal Policy for GAA
In the MDP setting above, our GAA is required to search
for certain optimal policy π(α|s) to maximize the expec-
tation of accumulated reward [54], where π(α|s) denotes
a parametrized distribution over the action space A, con-
ditioned on the currently observed state s. Formally, the
optimization objective for training GAA is deﬁned as
maxπ Es0,a0,...,sT ,aT [∑T
t=0 γtR(st )], where (s0,a0, . . . ,sT ,aT ) is
called a trajectory (or, experience) of length T + 1, which
has the joint probability density p(s0,α0, . . . ,sT ,αT ) =
USENIX Association
29th USENIX Security Symposium    1645
n , . . . , 1
n ) ∈ Sn;
Initialize parameters of f as θ0 randomly ;
for k ∈ {1, . . . ,K} do
Algorithm 1: Robust Distributed Learning against
Byzantine attacks with GAA
1 Initialize parameters of recurrent unit hψ randomly ;
2 Initialize αold = α0 = ( 1
3 for i ∈ {1, . . . ,N} do
4
5
6
7
8
9
10
Send the current parameters θt to each worker ;
Receive submitted gradients Qt := (V t
1 , . . . ,V t
n ) ;
θt+1 ← θt − λ(∑n
i=1 αi
(cid:96)GAA ← (cid:96)GAA + 1
S γt ∑z∈B f (θt+1,z)− f (θt ,z) ;
αt+1 ← hψ(st+1,αt )
α0 ← αold, (cid:96)GAA ← 0;
for t ∈ {0, . . . ,T − 1} do
tV t
i ) ;
end
Update ψ with a step of gradient descent on (cid:96)GAA ;
αold ← αT ;
11
12
13
14
15
16
17 end
end
tended with quadratic approximations [13]. Due to the page
limit, we provide the detailed proofs for the results in this part
at the website pertaining to this paper 1.
4.4.1 Provable Robustness with a Fixed Byzantine Ratio.
Theorem 1. After t steps of gradient descent with GAA when
the Byzantine ratio is ﬁxed as β, Algorithm 1 yields a param-
eter θt s.t.
f (θt )− f (θ∗) <
2RM(cid:112)(1− β)nt
+
SηR2
t +
(cid:112)KL(Pm||D) + O(e−t )
√
2(cid:107) f(cid:107)∞
(1)
where R is the diameter of parameter space.
Corollary 1. As long as β is smaller than 1 and Pm = D a.e.,
√
Algorithm 1 in the above setting will asymptotically converge
t).
to the global optimum with rate O(1/
1
O((cid:112)KL(Pm||D)) in O(
Intuitively, Theorem 1 suggests, when the Byzantine ra-
tio is ﬁxed over time, GAA is proved to help the underly-
ing system attain a sub-optimal parameter with error ε +
(1−β)ε2 ) steps. It suggests a lower KL-
divergence bound (at the scale of 10−2 in our case studies with
a quasi-validation set constructed from similar data domains)
and a smaller Byzantine ratio will lead to a more accurate
sub-optimum. When the quasi-validation set is from the true
√
distribution, Corollary 1 further guarantees the convergence
of the learning process with rate O(1/
t), which is relatively
larger than the optimal rate O(1/t) in Byzantium-free learn-
ing case [14]. We provide a more detailed explanation on the
meaning of each term and an empirical validation of Theorem
1 in Appendix A.4.