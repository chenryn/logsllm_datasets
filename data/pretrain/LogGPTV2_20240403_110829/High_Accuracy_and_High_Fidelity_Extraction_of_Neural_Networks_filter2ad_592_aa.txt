title:High Accuracy and High Fidelity Extraction of Neural Networks
author:Matthew Jagielski and
Nicholas Carlini and
David Berthelot and
Alex Kurakin and
Nicolas Papernot
High Accuracy and High Fidelity Extraction 
of Neural Networks
Matthew Jagielski, Northeastern University, Google Brain; Nicholas Carlini, 
David Berthelot, Alex Kurakin, and Nicolas Papernot, Google Brain
https://www.usenix.org/conference/usenixsecurity20/presentation/jagielski
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.High Accuracy and High Fidelity Extraction of Neural Networks
Matthew Jagielski†,∗, Nicholas Carlini*, David Berthelot*, Alex Kurakin*, and Nicolas Papernot*
†Northeastern University
*Google Research
Abstract
In a model extraction attack, an adversary steals a copy of
a remotely deployed machine learning model, given oracle
prediction access. We taxonomize model extraction attacks
around two objectives: accuracy, i.e., performing well on
the underlying learning task, and ﬁdelity, i.e., matching the
predictions of the remote victim classiﬁer on any input.
To extract a high-accuracy model, we develop a learning-
based attack exploiting the victim to supervise the train-
ing of an extracted model. Through analytical and empiri-
cal arguments, we then explain the inherent limitations that
prevent any learning-based strategy from extracting a truly
high-ﬁdelity model—i.e., extracting a functionally-equivalent
model whose predictions are identical to those of the vic-
tim model on all possible inputs. Addressing these limita-
tions, we expand on prior work to develop the ﬁrst practical
functionally-equivalent extraction attack for direct extraction
(i.e., without training) of a model’s weights.
We perform experiments both on academic datasets and a
state-of-the-art image classiﬁer trained with 1 billion propri-
etary images. In addition to broadening the scope of model
extraction research, our work demonstrates the practicality of
model extraction attacks against production-grade systems.
1 Introduction
Machine learning, and neural networks in particular, are
widely deployed in industry settings. Models are often de-
ployed as prediction services or otherwise exposed to potential
adversaries. Despite this fact, the trained models themselves
are often proprietary and are closely guarded.
There are two reasons models are often seen as sensitive.
First, they are expensive to obtain. Not only is it expensive to
train the ﬁnal model [1] (e.g., Google recently trained a model
with 340 million parameters on hardware costing 61,000 USD
per training run [2]), performing the work to identify the
optimal set of model architecture, training algorithm, and
hyper-parameters often eclipses the cost of training the ﬁnal
model. Further, training these models also requires investing
in expensive collection process to obtain the training datasets
necessary to obtain an accurate classiﬁer [3–6]. Second, there
are security [7, 8] and privacy [9, 10] concerns for revealing
trained models to potential adversaries.
Concerningly, prior work found that an adversary with
query access to a model can steal the model to obtain a copy
that largely agrees with the remote victim models [8, 11–16].
These extraction attacks are therefore important to consider.
In this paper, we systematize the space of model extrac-
tion around two adversarial objectives: accuracy and ﬁdelity.
Accuracy measures the correctness of predictions made by
the extracted model on the test distribution. Fidelity, in con-
trast, measures the general agreement between the extracted
and victim models on any input. Both of these objectives are
desirable, but they are in conﬂict for imperfect victim mod-
els: a high-ﬁdelity extraction should replicate the errors of
the victim, whereas a high-accuracy model should instead
try to make an accurate prediction. At the high-ﬁdelity limit
is functionally-equivalent model extraction: the two models
agree on all inputs, both on and off the underlying data distri-
bution.
While most prior work considers accuracy [7, 11, 13], we
argue that ﬁdelity is often equally important. When using
model extraction to mount black-box adversarial example at-
tacks [7], ﬁdelity ensures the attack is more effective because
more adversarial examples transfer from the extracted model
to the victim. Membership inference [9, 10] beneﬁts from the
extracted model closely replicating the conﬁdence of predic-
tions made by the victim. Finally, a functionally-equivalent
extraction enables the adversary to inspect whether internal
representations reveal unintended attributes of the input—that
are statistically uncorrelated with the training objective, en-
abling the adversary to beneﬁt from overlearning [17].
We design one attack for each objective. First, a learning-
based attack, which uses the victim to generate labels for
training the extracted model. While existing techniques al-
ready achieve high accuracy, our attacks are 16× more query-
efﬁcient and scale to larger models. We perform experiments
USENIX Association
29th USENIX Security Symposium    1345
that surface inherent limitations of learning-based extraction
attacks and argue that learning-based strategies are ill-suited
to achieve high-ﬁdelity extraction. Then, we develop the ﬁrst
practical functionally-equivalent attack, which directly recov-
ers a two-layer neural network’s weights exactly given access
to double-precision model inference. Compared to prior work,
which required a high-precision power side-channel [18] or
access to model gradients [19], our attack only requires input-
output access to the model, while simultaneously scaling to
larger networks than either of the prior methods.
We make the following contributions:
• We taxonomize the space of model extraction attacks by
exploring the objective of accuracy and ﬁdelity.
• We improve the query efﬁciency of learning attacks for
accuracy extraction and make them practical for millions-
of-parameter models trained on billions of images.
• We achieve high-ﬁdelity extraction by developing the
ﬁrst practical functionally-equivalent model extraction.
• We mix the proposed methods to obtain a hybrid method
which improves both accuracy and ﬁdelity extraction.
2 Preliminaries
We consider classiﬁers with domain X ⊆ Rd and range Y ⊆
RK; the output of the classiﬁer is a distribution over K class
labels. The class assigned to an input x by a classiﬁer f is
argmaxi∈[K] f (x)i (for n ∈ Z, we write [n] = {1,2, . . .n}). In
order to satisfy the constraint that a classiﬁer’s output is a
distribution, a softmax σ(·) is typically applied to the output
of an arbitrary function fL : X → RK:
of the layer. The number of layers is the depth of the net-
work. The non-linear operations are typically ﬁxed, while the
linear operations have parameters which are learned during
training. The function computed by layer i, fi(a), is there-
fore computed as fi(a) = gi(A(i)a + B(i)), where gi is the ith
non-linear function, and A(i),B(i) are the parameters of layer
i (A(i) is the weights, B(i) the biases). A common choice of
activation is the rectiﬁed linear unit, or ReLU, which sets
ReLU(x) = max(0,x). Introduced to improve the conver-
gence of optimization when training neural networks, the
ReLU activation has established itself as an effective default
choice for practitioners [20]. Thus, we consider primarily
ReLU networks in this work.
The network structure described here is called fully con-
nected because each linear operation “connects" every input
node to every output node. In many domains, such as com-
puter vision, this is more structure than necessary. A neuron
computing edge detection, for example, only needs to use
information from a small region of the image. Convolutional
networks were developed to combat this inefﬁciency—the
linear functions become ﬁlters, which are still linear, but are
only applied to a small (e.g., 3x3 or 5x5) window of the input.
They are applied to every window using the same weights,
making convolutions require far fewer parameters than fully
connected networks.
Neural networks are trained by empirical risk minimiza-
i=1 ⊆ X × Y ,
tion. Given a dataset of n samples D = {xi,yi}n
training involves minimizing a loss function L on the dataset
with respect to the parameters of the network f . A common
loss function is the cross-entropy loss H for a sample (x,y):
H(y, f (x)) = −∑k∈[K] yk log( f (x)k), where y is the probabil-
ity (or one-hot) vector for the true class. The cross-entropy
loss on the full dataset is then
σ( fL(x))i =
exp( fL(x)i)
∑ j exp( fL(x) j)
.
L(D; f ) =
1
n
n
∑
i=1
H(yi, f (xi)) = −1
n
n
∑
i=1
∑
k∈[K]
yk log( f (x)k).
We call the function fL(·) the logit function for a classiﬁer f .
To convert a class label into a probability vector, it is common
to use one-hot encoding: for a value j ∈ [K], the one-hot
encoding OH( j;K) is a vector in RK with OH( j;K)i = 1(i =
j)—that is, it is 1 only at index j, and 0 elsewhere.
Model extraction concerns reproducing a victim model, or
oracle, which we write O : X → Y . The model extraction ad-
versary will run an extraction algorithm A(O), which outputs
the extracted model ˆO. We will sometimes parameterize the
oracle (resp. extracted model) as Oθ (resp. ˆOθ) to denote that it
has model parameters θ—we will omit this when unnecessary
or apparent from context.
In this work, we consider O and ˆO to both be neural
networks. A neural network is a sequence of operations—
alternatingly applying linear operations and non-linear
operations—a pair of linear and non-linear operations is called
a layer. Each linear operation projects onto some space Rh—
the dimensionality h of this space is referred to as the width
The loss is minimized with some form of gradient descent,
often stochastic gradient descent (SGD). In SGD, gradients
of parameters θ are computed over a randomly sampled batch
B, averaged, and scaled by a learning rate η:
θt+1 = θt − η
|B| ∑
i∈B
∇θH(yi, f (xi)).
Other optimizers [21–23] use gradient statistics to reduce the
variance of updates which can result in better performance.
A less common setting, but one which is important for our
work, is when the target values y which are used to train the
network are not one-hot values, but are probability vectors
output by a different model g(x). When training using the
dataset Dg = {xi,g(xi)1/T}n
i=1, we say the trained model is
distilled from g with temperature T , referring to the process
of distillation introduced in Hinton et al. [24]. Note that the
values of g(xi)1/T are always scaled to sum to 1.
1346    29th USENIX Security Symposium
USENIX Association
3 Taxonomy of Threat Models
We now address the spectrum of adversaries interested in ex-
tracting neural networks. As illustrated in Table 1, we taxono-
mize the space of possible adversaries around two overarching
goals—theft and reconnaissance. We detail why extraction is
not always practically realizable by constructing models that
are impossible to extract, or require a large number of queries
to extract. We conclude our threat model with a discussion of
how adversarial capabilities (e.g., prior knowledge of model
architecture or information returned by queries) affect the
strategies an adversary may consider.
3.1 Adversarial Motivations
Model extraction attacks target the conﬁdentiality of a victim
model deployed on a remote service. A model refers here to
both the architecture and its parameters. Architectural details
include the learning hypothesis (i.e., neural network in our
case) and corresponding details (e.g., number of layers and
activation functions for neural networks). Parameter values
are the result of training.
First, we consider theft adversaries, motivated by economic
incentives. Generally, the defender went through an expensive
process to design the model’s architecture and train it to set
parameter values. Here, the model can be viewed as intellec-
tual property that the adversary is trying to steal. A line of
work has in fact referred to this as “model stealing” [11].
In the latter class of attacks, the adversary is performing
reconnaissance to later mount attacks targeting other security
properties of the learning system: e.g., its integrity with adver-
sarial examples [7], or privacy with training data membership
inference [9,10]. Model extraction enables an adversary previ-
ously operating in a black-box threat model to mount attacks
against the extracted model in a white-box threat model. The
adversary has—by design—access to the extracted model’s
parameters. In the limit, this adversary would expect to extract
an exact copy of the oracle.
The goal of exact extraction is to produce ˆOθ = Oθ, so
that the model’s architecture and all of its weights are identi-
cal to the oracle. This deﬁnition is purely a strawman—it is
the strongest possible attack, but it is fundamentally impos-
sible for many classes of neural networks, including ReLU
networks, because any individual model belongs to a large
equivalence class of networks which are indistinguishable
from input-output behavior. For example, we can scale an
arbitrary neuron’s input weights and biases by some c > 0,
and scale its output weights and biases by c−1; the resulting
model’s behavior is unchanged. Alternatively, in any inter-
mediate layer of a ReLU network, we may also add a dead
neuron which never contributes to the output, or might per-
mute the (arbitrary) order of neurons internally. Given access
to input-output behavior, the best we can do is identify the
equivalence class the oracle belongs to.
Figure 1: Illustrating ﬁdelity vs. accuracy. The solid blue
line is the oracle; functionally equivalent extraction recovers
this exactly. The green dash-dot line achieves high ﬁdelity: it
matches the oracle on all data points. The orange dashed line
achieves perfect accuracy: it classiﬁes all points correctly.
3.2 Adversarial Goals
This perspective yields a natural spectrum of realistic adver-
sarial goals characterizing decreasingly precise extractions.
Functionally Equivalent Extraction The goal of function-
ally equivalent extraction is to construct an ˆO such that
∀x ∈ X , ˆO(x) = O(x). This is a tractable weakening of the
exact extraction deﬁnition from earlier—it is the hardest possi-
ble goal using only input-output pairs. The adversary obtains
a member of the oracle’s equivalence class. This goal enables
a number of downstream attacks, including those involving
inspection of the model’s internal representations like over-
learning [17], to operate in the white-box threat model.
Fidelity Extraction Given some target distribution DF
over X , and goal similarity function S(p1, p2), the goal
of ﬁdelity extraction is to construct an ˆO that maxi-
mizes Prx∼DF
only label agreement, where S(p1, p2) = 1(argmax(p1) =
argmax(p2)); we leave exploration of other similarity func-
tions to future work.
(cid:2)S( ˆO(x),O(x))(cid:3). In this work, we consider
A natural distribution of interest DF is the data distribution
itself—the adversary wants to make sure the mistakes and
correct labels are the same between the two models. A recon-
naissance attack for constructing adversarial examples would
care about a perturbed data distribution; mistakes might be
more important to the adversary in this setting. Membership
inference would use the natural data distribution, including
any outliers. These distributions tend to be concentrated on
a low-dimension manifold of X , making ﬁdelity extraction
signiﬁcantly easier than functionally equivalent extraction.
USENIX Association
29th USENIX Security Symposium    1347
Attack
Type
Model type
Goal
Query Output
Lowd & Meek [8]
Tramer et al. [11]
Tramer et al. [11]
Milli et al. [19] (theoretical)
Milli et al. [19]
Pal et al. [15]
Chandrasekharan et al. [13]
Copycat CNN [16]
Papernot et al. [7]
CSI NN [25]
Knockoff Nets [12]
Direct Recovery
(Active) Learning
Path ﬁnding
Direct Recovery
Learning
Active learning
Active learning
Learning
Active learning
Direct Recovery