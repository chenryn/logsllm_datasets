thored a message must not allow her to convince Dave, who
is not a chat participant, that Charlie authored the message.
2.4 Forward secrecy
The Internet is a public medium: when a typical user
sends a data packet, the user has little (if any) idea how
the packet will reach its destination. To be on the safe side
we assume that the adversary has seen and recorded every
transmitted packet for future use. The adversary’s ability
to see messages motivates encryption; his ability to record
messages motivates forward secrecy. Forward secrecy im-
plies that the leakage of static private keys do not reveal
the content of past communication. Users achieve forward
secrecy by using ephemeral encryption and decryption keys
that are securely erased after use and that cannot be recom-
puted even with the knowledge of static keys.
We separate encryption keys from static keys. Static keys
are used to authenticate ephemeral data, which is used to
derive short-lived encryption keys. This is a common ap-
proach to achieve forward secrecy. Note that this goal is
unrelated to deniability:
in forward secrecy the user does
not aim to refute any message; in fact, the user may not
even be aware of the malicious behavior. The goal of the
adversary is reading the content of a message as opposed to
associating a message with a user.
2.5 Deniability
A casual private meeting leaves no trace2 after it is dis-
solved. By contrast, the electronic world typically retains
partial information, such as logs for debugging, for future
1We assume no “over-the-shoulder” attacks.
2 If there were no logs, wiretapping, etc.
reference, and so on. This contradicts the “no trace” feature
of private meetings. As mentioned in the forward secrecy
discussion, entities involved in relaying messages may keep
a communication record: participants do not and cannot
control all copies of messages they send and hence cannot
be assured that all copies were securely destroyed. Users
can claim the traces are bogus, eﬀectively denying author-
ing messages. But what is the meaning of “deny” in this
context?
Not all deniability deﬁnitions are suitable for oﬀ-the-record
communication. Consider for example the plaintext denia-
bility notion proposed in [8], where the encryption scheme
allows a ciphertext author to open the ciphertext into more
than one plaintext: Alice wishes to communicate (possibly
incriminating) message M1; she chooses M2, . . . , Mn non-
incriminating messages and then forms the ciphertext C =
DeniableEncryptK (M1, M2, . . . , Mn). When challenged to
decrypt, Alice can validly decrypt C to any of the alternate
messages Mi that she chose when forming C. Even though
Alice denies authoring the incriminating plaintext, she im-
plicitly admits to authoring the ciphertext. However, who
you speak to may be as incriminating as what you say. Alice
might get into deep trouble with her maﬁa boss by merely
admitting that she has spoken with law enforcement, re-
gardless of what she said. She would be in a much better
situation if she could claim that she never authored the ci-
phertext, instead of decrypting the ciphertext to an innocu-
ous plaintext and thereby implicitly admitting authorship.
Contrary to the above example, suppose Alice has means
of denying all her messages in front of everyone, by arguing
that an entity diﬀerent from herself faked messages coming
from her3. That is, any message purportedly from Alice
could have been authored by Mallory.
In that case how
could Bob and Charlie have a meaningful conversation with
Alice? They have no assurances that messages appearing to
come from Alice are indeed hers: Alice’s messages can be
denied even in front of Bob and Charlie. What we need is
a “selective” deniability. We next discuss the selectiveness
of deniability in the requirements for multi-party Oﬀ-the-
Record messaging.
2.5.1 Repudiation
The fundamental problem of deniability (FPD) describes
the inherent diﬃculty for a user Alice to repudiate a state-
ment. Let Justin be a judge. Suppose Charlie and Dave
come to Justin and accuse Alice of making a statement m.
In the best-case scenario for Alice, Charlie and Dave will not
be able to provide any evidence that Alice said m, apart from
their word. If Alice denies saying m, whom should Justin
trust: Charlie and Dave, or Alice? The voices are two to
one against Alice, but it is possible that Charlie and Dave
are trying to falsely implicate Alice. Justin must decide who
to believe based on his evaluation of the trustworthiness of
their testimony. Justin’s evaluation may be inﬂuenced by
many hard-to-quantify factors, such as perceived likelihood
of the testimony, the number of witnesses in agreement, po-
tential beneﬁts and detriments to the witnesses, etc. Justin
may even explicitly favor the testimony of certain witnesses,
such as law enforcement oﬃcers. In the end, Justin must
base his decision on weighing the testimonies rather than on
3For example Alice can pick a symmetric encryption key κ
encrypt her message with κ, encrypt κ with Bob’s public
key and send everything to Bob.
360physical evidence. In the limit, when n parties accuse Al-
ice of saying m, Alice will have to convince Justin that the
other n witnesses are colluding to frame her. We call this
the fundamental problem of deniability.
We cannot solve the FPD. The best we can oﬀer is to
ensure that Charlie and Dave cannot present any evidence
(consisting of an algorithmic proof) that Alice has said m,
thereby reducing the question to the FPD. In the online
world Charlie and Dave make their claim by presenting a
communication transcript. Therefore, we provide Alice with
means to argue that Charlie and Dave could have created
the transcript without her involvement. As long as Char-
lie and Dave cannot present an algorithmic proof of Alice’s
authorship, she can plausibly deny m, so Justin has to rule
based on the same factors (e.g., weighing the testimonies
rather than on physical evidence) as in the physical world.
By this means, we provide comparable levels of repudiation
between on-line and face-to-face scenarios.
In §2.3 we alluded to the conﬂicting goals of message origin
authentication and privacy, where the complete deniability
example prevents origin authentication. To provide origin
authentication, we need a special type of repudiation. Let us
look closer at a private communication among Alice, Charlie
and Dave. In a face-to-face meeting Charlie and Dave hear
what Alice says. This is origin authentication. After the
meeting, however, Alice can deny her statements, because,
barring recording devices, neither Charlie nor Dave has evi-
dence of Alice’s statements. This is the type of repudiation
that we aim for.
In contrast to the physical world, on the Internet Char-
lie can diﬀerentiate between Alice and Dave when the three
of them are talking and can send them diﬀerent messages.
While it is impossible to guard against such behavior (either
due to malicious intent or connection problems), we would
like the proof of authorship that Charlie provides to Alice to
also convince other chat participants — and no one else —
of his authorship. That way, all parties are assured that (1)
they can reach a transcript consensus even in the presence of
malicious behavior, and (2) all statements within the chat
can be denied in front of outside parties. This condition
should hold even if Alice and Charlie share more than one
chat concurrently or sequentially: all chats must be inde-
pendent in the sense that if Alice and Charlie share chats C1
and C2 any authorship proof Charlie has in C1 is unaccept-
able in C2. In relation to the previous paragraph we note
that such authorship proofs should also become invalid at
the end of the meeting.
2.5.2 Forgeability
In some cases4 it is valuable to deny not only having made
a statement but also having participated in a meeting. In
the physical world Alice can prove she was absent from a
meeting by supplying an alibi. On the Internet, however,
such an alibi is impossible as Alice can participate in mul-
tiple chatrooms simultaneously. Short of an alibi, the next
best denial is a design where transcripts allegedly involving
Alice can be created without her participation. Although
this mechanism would not allow Alice to prove that she was
absent from the meeting, it prevents her accusers from prov-
ing that she was present at the meeting. A reﬁnement is to
design transcripts that can be extended to include users that
did not participate, to exclude users who did participate, or
4Police informants, for example.
both. Eﬀectively, such transcripts will oﬀer little, if any5,
evidence about who participated in it.
For example, suppose Mallory tries to convince Alice that
Bob spoke with Dave and Eve by presenting a transcript
with participants Bob, Dave, and Eve. Ideally, forgeability
would allow Bob to argue that Mallory fabricated the tran-
script even though Mallory is an outsider with respect to
the transcript.
2.5.3 Malleability
In §2.5.2 we dealt with forging who participated in a com-
munication transcript. With malleability we address the is-
sue of the transcript content. Ideally, the transcript should
be malleable in the sense that given a transcript T1 and a
message m1 that belongs to T1, it is possible to obtain a
transcript T2, where message m1 is substituted with mes-
sage m2. Along with forgeability this approach provides a
strong case for users who wish to deny statements and in-
volvement in chat meetings. For accusers, transcripts with
this level of ﬂexible modiﬁcation provide little convincing
evidence, even in the event of conﬁdentiality breaches.
2.6 Anonymity and pseudonymity
While in our current work anonymity is not the main goal,
we desire that our solution preserves anonymity. This in-
cludes, but is not restricted to, not writing users’ identities
on the wire. While we do not explicitly address it, users
may wish to use our protocol over a transport protocol that
provides pseudonymity. If they do so, it would be unaccept-
able if our protocol deanonymizes users to adversaries on the
network. We do, however, use anonymity-like techniques to
achieve a subset of our deniability goals.
3. THREAT MODEL
3.1 Players
We will ﬁrst introduce the diﬀerent players and their re-
lations with each other. The set of users, denoted by U, is a
collection of entities that are willing to participate in multi-
party meetings. Honest parties, denoted by ˆA, ˆB, ˆC, . . . fol-
low the speciﬁcations faithfully; these parties are referred to
as Alice, Bob, Charlie, . . . . Dishonest parties deviate from
the prescribed protocol. Each party ˆA has an associated
long-lived static public-private key pair (S ˆA,s ˆA). We assume
that the associated public key for each party is known to all
other parties. (These associations can be communicated via
an out-of-band mechanism or through authentication proto-
cols as in [1].) A subset P of users can come together and
form a chatroom C; each member of P is called a participant
of C. While honest users follow the protocol speciﬁcations,
they may observe behavior that is not protocol compliant
due to either network failures, intentional malicious behav-
ior by other parties, or both.
In addition to users that take part in the conversation we
have three types of adversaries: (i) a security adversary, de-
noted by O; (ii) a consensus adversary, T ; and (iii) a privacy
adversary, M. The last player in the system, the judge J ,
does not interact with users but only with adversaries, in
particular with M. We will see his purpose when discussing
the adversaries’ goals.
5If the plaintext is recovered, the writing style or statements
made may reveal the author’s identity.
3613.2 Goals
nT ˆXC1 | ˆX ∈ Po
Honest users wish to have on-line chats that emulate face-
to-face meetings. It is the presence of the adversaries that
necessitates cryptographic measures to ensure conﬁdential-
ity and privacy.
3.2.1 Security adversary
The goal of the security adversary is to read messages
that he is not entitled to. Let TC1 =
be a
collection of transcripts resulting from a chat C1 with set of
chat participants P, such that no user in P revealed private6
information to, or collaborated with, the security adversary
O prior to the completion of C1. Suppose also that for each
honest participant ˆA, who owns T ˆAC1 ∈ TC1
7, and for each
honest participant ˆB, who owns T ˆBC1 ∈ TC1 , ˆA and ˆB have
consistent view of the messages and participants. We say
that O is successful if O can read at least one message in
some T ˆAC1 without obtaining the message from a user ˆB who
owns T ˆBC1 .
A few remarks on O’s goals are in order. The security
adversary can control communication channels and observe
the actions of any number of users in P, learn messages that
they broadcast in other chatrooms, and start chatroom ses-
sions with them via proxy users. All these actions can take
place before, during or after C1. However, O is allowed nei-
ther to ask for static private information of any user in P
before the completion of C1 nor to take part in C1 via a proxy
user. The adversary may ask an honest user to send mes-
sages to C1, but should still be unable to decide if or when
his request is honored. Essentially, O aims to impersonate
an honest user during key agreement or to read messages in
a chatroom that consists only of honest users. O’s capabili-
ties are similar to the standard notion of indistinguishability
under chosen-plaintext attack for encryption schemes [2].
3.2.2 Consensus adversary
For details on communication in asynchronous networks
and how users can keep transcripts we refer the reader to
Reardon et. al. [18]. We ﬁrst explain the meaning of con-
sensus, which relates to what Alice thinks about her and
Bob’s view of past messages. We say that ˆA reaches con-
sensus on T ˆAC1 with ˆB if ˆA believes that ˆB admits having
transcript T ˆBC2
8 such that:
1. C1 and C2 have the same set of participants;
2. C1 and C2 are the same chatroom instance;