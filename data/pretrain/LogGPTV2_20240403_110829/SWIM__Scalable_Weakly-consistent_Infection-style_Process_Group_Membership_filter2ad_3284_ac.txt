about their probabilistic reliability. These analyses also
show that the infection style of dissemination is resilient
to process failures and loss of messages within the network,
much like the contagiousness of epidemics. Experimental
results of our implementation exhibit these characteristics.
A word on the implementation is in order. The SWIM
protocol layer at each group member Mi maintains a buffer
of recent membership updates, along with a local count for
each buffer element. The local count speciﬁes the number
of times the element has been piggybacked so far by Mi,
and is used to choose which elements to piggyback next.
Each element is piggybacked at most λ.log n times. If the
size of this buffer is larger than the maximum number of ele-
ments that can be piggybacked on a single ping message (or
ack), elements that have been gossiped fewer times are pre-
ferred. This is needed as the protocol period is ﬁxed, and the
rate of membership changes might temporarily overwhelm
the speed of dissemination. Preferring “younger” buffer el-
ements under such circumstances ensures that all member-
ship changes infect at least a few members - when the mem-
bership change injection rate quiesces, these changes will
propagate through the rest of the group.
Our implementation of this protocol maintains two lists
of group members - a list of members that are not yet de-
clared as failed in the group, and a second list of members
that have failed recently. Currently, an equal number of
buffer of elements is chosen from these two lists for pig-
gybacking, but the scheme could be generalized to adapt to
relative variations in process join, leave and failure rates.
4.2. Suspicion Mechanism: Reducing the Fre-
quency of False Positives
In the SWIM failure detector protocol described so far, if
a non-faulty group member Mj is (mistakenly) detected as
failed by another group member Mi, either due to network
packet losses or because Mj was asleep for sometime, or
because Mi is a slow process, then Mj will be declared
as failed in the group. In other words, a perfectly healthy
process Mj suffers a very heavy penalty, by being forced
to drop out of the group at the very ﬁrst instance that it is
mistakenly detected as failed in the group. This leads to a
high rate of false positives in detecting failures.
We reduce the effect of this problem by modifying
SWIM to run a subprotocol, called the Suspicion subpro-
tocol, whenever a failure is detected by the basic SWIM
failure detector protocol.
The Suspicion subprotocol works as follows. Consider a
member Mi that chooses a member Mj as a ping target in
the current protocol period, and runs the basic SWIM failure
If Mi receives no acknowledg-
detector protocol period.
ments, either directly or through the indirect probing sub-
group, it does not declare Mj as failed. Instead, Mi marks
Mj as a Suspected member in the local membership list at
Mi. In addition, a {Suspect Mj: Mi suspects Mj} mes-
sage is disseminated through the group through the Dissem-
ination Component (in infection-style in our system). Any
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:23:09 UTC from IEEE Xplore.  Restrictions apply. 
group member Ml receiving such a message also marks Mj
as suspected. Suspected members stay on in the member-
ship list and are treated similar to non-faulty members with
regards to ping target selection operation of the SWIM fail-
ure detector protocol.
If a member Ml successfully pings a suspected mem-
ber member Mj during the due course of the basic SWIM
protocol, it un-marks the previous suspicion of Mj in its
membership list, and spreads an {Alive Mj: Ml knows Mj
is alive} message in the group through the Dissemination
Component (in infection-style in our system). Such an Alive
message un-marks the suspected member Mj in member-
ship lists of recipient members. Notice that if member Mj
receives such a message suspecting it, it can start propagat-
ing an Alive message clarifying its non-failure.
Suspected entries in membership lists expire after a pre-
speciﬁed time-out. If Mj is suspected at some member Mh,
and this entry times-out before receipt of an Alive message,
Mh declares Mj as faulty, drops it from the local mem-
bership list, and begins spreading the message {Conﬁrm
Mj: Mh declares Mj as faulty} through the Dissemination
Component. This message overrides any previous Suspect
or Alive messages, and cascades in deletion of Mj from the
membership lists of all recipients.
This mechanism reduces (but does not eliminate) the rate
of failure detection false positives. Notice also that the
Strong Completeness property of the original protocol con-
tinues to hold. Failures of processes suspecting a failed pro-
cess Mj may prolong detection time, but eventual detection
is guaranteed.
From the above discussion, Alive messages override Sus-
pect messages, and Conﬁrm messages override both Suspect
and Alive messages, in their effect on the local member-
ship list element corresponding to the suspected member
Mj. However, a member might be suspected and unsus-
pected multiple times during its lifetime. These multiple
versions of Suspect and Alive messages (all pertaining to the
same member Mj) need to be distinguished through unique
identiﬁers. These identiﬁers are provided by using a virtual
incarnation number ﬁeld with each element in the mem-
bership lists. Incarnation numbers are global. A member
Mi’s incarnation number is initialized to 0 when it joins the
group, and it can be incremented only by Mi, when it re-
ceives information (through the Dissemination Component)
about itself being suspected in the current incarnation - Mi
then generates an Alive message with its identiﬁer and an
incremented incarnation number, and spreads this through
the Dissemination Component to the group.
Thus, Suspect, Alive, and Conf irm messages contain
the incarnation number of the member, besides its identiﬁer.
The order of preference among these messages and their
effect on the membership list is speciﬁed below.
• {Alive Ml, inc = i} overrides
– {Suspect Ml, inc = j}, i > j
– {Alive Ml, inc = j}, i > j
• {Suspect Ml, inc = i} overrides
– {Suspect Ml, inc = j}, i > j
– {Alive Ml, inc = j}, i ≥ j
• {Conﬁrm Ml, inc = i} overrides
– {Alive Ml, inc = j}, any j
– {Suspect Ml, inc = j}, any j
It is easy to see that these orders of preference and over-
riding maintain the desired correctness properties of the
Failure Detector Component. The reader familiar with ad-
hoc routing protocols such as AODV [5] will notice the sim-
ilarity between their use of destination sequence numbers
and our incarnation number scheme.
The preference rules and infection-style Dissemination
Component also accommodate suspicions of a process by
multiple other processes. Preference rules do not depend on
the source of suspicion, and the infection-style dissemina-
tion spreads a message (Suspect, Alive or Conﬁrm) quicker
if there are multiple sources, with exactly the same overhead
per process as with one source of infection [8].
4.3. Round-Robin Probe Target Selection: Provid-
ing Time-Bounded Strong Completeness
The basic SWIM failure detector protocol described in
Section 3 detects failures in an average constant number of
protocol periods. Although each process failure is guaran-
teed to be detected eventually at every other non-faulty pro-
cess (eventual Strong Completeness), a pathological selec-
tion of ping targets across the group might lead to a large
delay in the ﬁrst detection of the process failure anywhere
in the group. In the extreme case, this delay could be un-
bounded as the failed process might never be chosen as a
ping target by any other non-faulty process.
This can be solved by the following modiﬁcation to the
protocol. The failure detection protocol at member Mi
works by maintaining a list (intuitively, an array) of the
known elements of the current membership list, and select-
ing ping targets not randomly from this list, but in a round-
robin fashion. Instead, a newly joining member is inserted
in the membership list at a position that is chosen uniformly
at random. On completing a traversal of the entire list, Mi
rearranges the membership list to a random reordering.
Consider the execution of the SWIM protocol, modiﬁed
as described above, at member Mi. Once another member
Mj is included in Mi’s membership list, it will be chosen
as a ping target exactly once during each traversal of Mi’s
membership list. If the size of the membership list is no
more than ni, successive selections of the same target are at
most (2 · ni − 1) protocol periods apart. This bounds the
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:23:09 UTC from IEEE Xplore.  Restrictions apply. 
worst case detection time of a process failure of any mem-
ber by Mi, thus satisfying a Time Bounded Completeness
property.
The average failure detection time of the original proto-
col is preserved by this optimization, since the randomiza-
tion of the membership lists at different members across the
group leads to a similar distribution of ping target choices
by each member.
5. Performance Evaluation of a Prototype
A prototype of the SWIM protocol was implemented
over the Winsock 2 API, and was tested in a large cluster
of commodity PCs running Windows 2000. The PC cluster
was comprised of 16 450-MHz Dell PII’s, 16 1-GHz IBM
x220’s, and a collection of dual and quad nodes (200-MHz
to 500-MHz PII and PIII processors), communicating over
a 100 Mbps Ethernet with no external load. Each node con-
tained at most one process group member.
The experimental parameters were set as follows. The
number of members chosen for ping-reqs was K = 1, and
the protocol period used was 2 sec. Each infection (mem-
bership update) was piggybacked on at most (3(cid:13)log(N +
1)(cid:14)) messages sent by each member. With this value set-
ting, our experimental runs observed no evidence of per-
petual partial membership lists at members or involuntary
partitions of the group. The suspicion time-out for a sus-
pected member to be declared as failed was also set to this
value.
We compared three different versions of the protocol: (1)
(SWIM:Basic) the basic SWIM protocol of Section 3, modi-
ﬁed with the Round-Robin scheme described in Section 4.3,
(2) (SWIM+Inf.) SWIM with an infection-style Dissem-
ination Component (as described in Section 4.1), and (3)
(SWIM+Inf.+Susp.) SWIM+Inf. with the Suspicion sub-
protocol extension described in Section 4.2.
All point-to-point messages sent by the SWIM protocol
were UDP packets. The maximum message payload sizes
were 15 B in SWIM:Basic, and 135 B in the SWIM+Inf.
and SWIM+Inf.+Susp. protocols (since at most 6 member-
ship updates were piggybacked per message7).
5.1. Message Loads
Figure 2 shows the measured message send and receive
loads imposed by the SWIM failure detector at an arbitrary
group member. The readings are taken over a time period
spanning over 40 protocol periods. Up to a group size of
55 members, the average overhead stays around 2.0. This
matches the analytic estimate as, during each protocol pe-
riod, a group member sends a single ping and receives the
7This parameter could be adapted to high membership update rates,
thus trading off overhead for dissemination time.
d
o
i
r
e
p
l
t
o
c
o
o
r
p
d
a
o
/
l
e
g
a
s
s
e
m
r
e
b
m
e
m
e
g
a
r
e
v
A
8
7
6
5
4
3
2
1
0
Sent messages
Recvd. messages
8
16
24
32
40
48
56
Group Size
Figure 2. Send and Receive message overheads at a group
member. Points perturbed horizontally for clarity.
corresponding ack, as well as receives one ping message
(on expectation) and sends the corresponding ack. The
standard deviation bars indicate that typical message over-
heads stay low, e.g. at N=28 members, the overhead of sent
messages is smaller than 5 messages per protocol period
with a probability of 0.99.
Recollect from our description in Section 4 that this
plot shows the total message overhead per member in
SWIM+Inf. and SWIM+Inf.+Susp, while only the base
load in SWIM:Basic (where additional multicasts dissem-
inate membership updates).
5.2. Detection and Dissemination Latency of Mem-
bership Updates
Figure 3 shows the break up of failure detection and
dissemination times for the three different protocols. Fig-
ure 3(a) shows the mean time between a process failure and
its ﬁrst detection at some non-faulty group member. The
average detection time does not appear to be correlated to
group size and thus matches the analytic estimate. In the
SWIM:Basic protocol, the vertical axis values correspond
to the interval between a process failure and the instant of
multicast of its failure notiﬁcation. In the SWIM+Inf. and
SWIM+Inf.+Susp. protocols, these values correspond to
the interval between a failure and the start of spread of the
failure notiﬁcation and suspicion notiﬁcation infections, re-
spectively.
Figure 3(b) plots the variation with group size of the de-
lay in infection-style dissemination of membership updates.
The median latency of dissemination is always only a few
protocol periods, and appears to be rise slowly with group
size (recall the analysis of Section 4.1 predicted a logarith-
mic variation of the average infection time with group size).
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:23:09 UTC from IEEE Xplore.  Restrictions apply. 
)
s