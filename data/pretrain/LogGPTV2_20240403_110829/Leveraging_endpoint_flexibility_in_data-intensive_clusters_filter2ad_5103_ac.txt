Q.addToQueue(B)
return selectLinks(B, Nil)
if q = 0 then
▷ Instantaneous decision
▷ Queue up the request. Order by policy.
12: procedure DISPATCHREQUEST(Link l)
13:
14:
15:
16: end procedure
for all B 2 Q do
end for
return selectLinks(B, Nil)
▷ Called at q intervals.
one needs to keep room on both network links and disks for such
blocks. The rest of the blocks, which are many but contribute in-
signiﬁcant amount of bytes, are placed using the default placement
policy.
Sinbad uses optional information provided by the operator in-
cluding the topology of the cluster, oversubscription factors at dif-
ferent levels of the topology, and fault domains with corresponding
machines. We populate the bottleneck links’ set (L) with links from
that level of the topology, which has the highest oversubscription
factor and is the farthest from the network core.6 In the absence
of these information, Sinbad assumes that each machine is located
in its own fault domain, and L is populated with the host-to-rack
links.
6.2 Utilization Estimator
Sinbad master receives periodic updates from each slave at ∆ in-
tervals containing the receiving and transmitting link utilizations at
corresponding NICs. After receiving individual updates, Sinbad es-
timates, for each potential bottleneck link l, the downlink (Rx(l))
and uplink (T x(l)) utilizations using exponentially weighted mov-
ing average (EWMA):
vnew(l) = (cid:11) vmeasured(l) + (1 (cid:0) (cid:11)) vold(l)
where, (cid:11) is the smoothing factor (Sinbad uses (cid:11) = 0:2), and v(:)
stands for both Rx(:) and T x(:). EWMA smooths out the ran-
dom ﬂuctuations. v(l) is initialized to zero. Missing updates are
treated conservatively as if the estimate indicates the link to be fully
loaded.
When L contains internal links of the topology (i.e., links be-
tween switches at different levels), vnew(l) is calculated by sum-
ming up the corresponding values from the hosts in the subtree of
the farthest endpoint of l from the core. This is recalculated at ∆
intervals.
The update interval ∆ determines how recent vnew(l) values
are. A smaller ∆ results in more accurate estimations; however,
too small a ∆ can overwhelm the incoming link to the master. We
settled for ∆ = 1s, which is typical for heartbeat intervals (1 to 3
seconds) in existing CFSes.
Hysteresis After a Placement Decision Once a replica placement
request has been served, Sinbad must temporarily adjust its esti-
mates of current link utilizations in all the links involved in transfer-
ring that block to avoid selecting the same location for subsequent
block requests before receiving the latest measurements. We use
6For full bisection bandwidth networks, host-to-rack links are the
potential bottlenecks.
′
: l
else
if l is an edge link then
′ 2 subtree of lg
return fMachine attached to lg
end if
M = fg
if l = Nil then
Lcur = L
M = fB:localMachineg
B:r = B:r (cid:0) 1
Lcur = fl
Pseudocode 2 Link Selection Algorithm
1: procedure SELECTLINKS(Request B, Link l)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
end for
26:
return M
27:
28: end procedure
end if
Lcur = Lcur.ﬁlter(B.constraints)
if jLcurj < B:r then
end if
SORT_DESC Lcur by expectedCapacity(l)
for all l 2 fFirst B:r links from Lcurg do
Add hysteresis to l
Set B:r = 1
M = M [ fselectLinks(B; l)g
return Nil
▷ Called with the tree root
▷ Consider all bottleneck links
▷ Called recursively
▷ Filter (§6.4)
▷ Not enough locations
▷ Only to the selected links
▷ One replica from each subtree
29: procedure EXPECTEDCAPACITY(Link l)
30:
31: end procedure
return min(Cap(l) (cid:0) Rxcur(l), DiskWriteCap)
a increment function I(B; (cid:14)) that is proportional to the size of the
block and inversely proportional to the amount of time remaining
until the next update (denoted by (cid:14)).
vnew’(l) = vnew(l) + I(B; (cid:14))
I(B; (cid:14)) = min(Cap(l) (cid:0) vnew(l);
Size(B)
(cid:14)
)
When an updated measurement arrives, vnew’(l) is invalidated and
vnew(l) is used to calculate the weighted moving average of v(l).
Here, Cap(l) represents the capacity of link l and Size(B) is the
size of block B.
6.3 Network-Balanced Placement Using Sinbad
Sinbad employs the algorithms developed in Section 5 to perform
network-aware replica placement. It involves two steps: ordering
of requests and ordering of bottleneck links.
Sinbad queues up (Pseudocode 1) all block requests that arrive
within a decision interval of length q and order them by the number
of remaining blocks in that write request (for OP T
). For q = 0,
Sinbad takes instantaneous decisions. The value of THRESHOLD
determines which blocks are placed by Sinbad. Recall that to lower
overhead, Sinbad causes most of the smaller blocks, which are nu-
merous but contribute only a small fraction of cluster bytes, to be
placed using the default policy.
Given an ordered list of blocks, Sinbad selects the machine with
the highest available receiving capacity that is reachable through
the bottleneck link (lsel):
′
lsel = argmax
l
min(Cap(l) (cid:0) Rxcur(l); DiskWriteCap)
where, Rxcur(l) is the most recent estimation of Rx(l) at time
Arr(B). This holds because the copy has to move in the transmit
direction regardless. The choice of placement only impacts where
it ends up, and hence, which other links are used on their receive
236direction. Further, Sinbad can generalize to the case when there are
multiple bottleneck links along such receiving paths. Hysteresis
(described above) lets Sinbad track the ongoing effect of placement
decisions before new utilization estimates arrive.
Pseudocode 2 shows how Sinbad proceeds recursively, starting
from the bottleneck links, to place the desired number of replicas.
The entry point for ﬁnding replicas for a request B with this pro-
cedure is SELECTLINKS(B, Nil). Calling it with an internal link
restricts the search space to a certain subtree of the topology.
Because a replication ﬂow cannot go faster than DiskWriteCap,
it might seem appealing to try to match that throughput as closely
as possible. This choice would leave links that have too much spare
capacity for a given block free, possibly to be used for placing an-
other larger block in near future. However, in practice, this causes
imbalance in the number of replication ﬂows through each bottle-
neck link and in the number of concurrent writers in CFS slaves,
and hurts performance.
6.4 Additional Constraints
In addition to fault tolerance, partition tolerance, and storage bal-
ancing, CFS clients can provide diverse suitability constraints. Col-
location of blocks from different ﬁles to decrease network commu-
nication during equi-joins is such an example [23]. Because such
constraints decrease Sinbad’s choices, the improvements are likely
to be smaller. Sinbad satisﬁes them by ﬁltering out unsuitable ma-
chines from Lcur (line 15 in Pseudocode 2).
7 Evaluation
We evaluated Sinbad through a set of experiments on a 100-
machine EC2 [1] cluster using workloads derived from the Face-
book trace. For a larger scale evaluation, we used a trace-driven
simulator that performs a detailed replay of task logs from the same
trace. Through simulation and experimentation, we look at Sin-
bad’s impact when applied on two different levels of the network
topology: in the former, Sinbad tries to balance load across links
connecting the core to individual racks; whereas, in the latter, it
aims for balanced edge links to individual machines (because the
EC2 topology and corresponding bottlenecks are unknown). Our
results show the following:
(cid:15) Sinbad improves the average block write time by 1:3(cid:2) and the
average end-to-end job completion time by up to 1:26(cid:2), with
small penalties for online decisions (§7.2).
(cid:15) Through simulations, we show that Sinbad’s improvement
(1:58(cid:2)) is close to that of a loose upper bound (1:89(cid:2)) of the
optimal (§7.2).
(cid:15) Sinbad decreases the median imbalance across bottleneck links
(i.e., median Cv) by up to 0:33 in both simulation and experi-
ment (§7.3).
(cid:15) Sinbad has minimal impact on the imbalance in disk usage
(0:57% of disk capacity), which remains well within the tol-
erable limit (10%) (§7.4).
(cid:15) If disks are never the bottlenecks, Sinbad improves the aver-
age block write time by 1:6(cid:2) and the average end-to-end job
completion time by up to 1:33(cid:2) (§7.6).
7.1 Methodology
Workload Our workload is derived from the Facebook trace (§4).
During the derivation, we preserve the original workload’s write
characteristics, including the ratio of intermediate and write trafﬁc,
the inter-arrival times between jobs, the amount of imbalance in
communication, and the input-to-output ratios for each stage. We
scale down each job proportionally to match the size of our cluster.
Table 3: Jobs Binned by Time Spent in Writing
1
2
3
5
Bin
Write Dur. < 25% 25–49% 50–74% 75–89% (cid:21) 90%
% of Jobs
52%
% of Bytes
67%
12%
3%
16%
22%
4
8%
3%
12%
5%
We divide the jobs into bins (Table 3) based on the fraction of
their durations spent in writing. Bin-5 consists of write-only in-
gestion and preprocessing jobs (i.e., jobs with no shufﬂe), and bins
1–4 consist of typical MapReduce (MR) jobs with shufﬂe and write
phases. The average duration of writes in these jobs in the original
trace was 291 seconds.
Cluster Our experiments use extra large EC2 instances with 4
cores and 15 GB of RAM. Each machine has 4 disks, and each
disk has 400 GB of free space. The write throughput of each disk
is 55 MBps on average.
The EC2 network has a signiﬁcantly better oversubscription fac-
tor than the network in the original Facebook trace – we observed
a bisection bandwidth of more over 700 Mbps/machine on clusters
of 100 machines. At smaller cluster sizes we saw even more – up to
900 Mbps/machine for a cluster of 25 machines. Note that virtual
machine (VM) placement in EC2 is governed by the provider and
is supposed to be transparent to end users. Further, we did not see
evidence of live migration of VMs during our experiments.
We use a computation framework similar to Spark [45] and the
Facebook-optimized distribution of HDFS [4] with a maximum
block size of 256 MB. In all experiments, replication and fault-
tolerance factors for individual HDFS ﬁles are set to three (r = 3)
and one (f = 1), respectively. Unless otherwise speciﬁed, we make
instantaneous decisions (q = 0) in EC2 experiments.
Trace-Driven Simulator We use a trace-driven simulator to as-
sess Sinbad’s performance on the scale of the actual Facebook clus-
ter (§4) and to gain insights at a ﬁner granularity. The simulator
performs a detailed task-level replay of the Facebook trace on a
3000-machine cluster with 150 racks. It preserves input-to-output
ratios of all tasks, locality constraints, and presence of task failures
and stragglers. We use the actual background trafﬁc from the trace
and simulate replication times using the default uniform placement
policy. Disk write throughput is assumed to be 50 MBps. Unless
otherwise speciﬁed, we use 10 second decision intervals (q = 10s)
in simulations for time constraints.
Metrics Our primary metric for comparison is the improvement
in average completion times of individual block replication, tasks,
and jobs (when its last task ﬁnished) in the workload, where
F actor of Improvement =
M odif ied
Current
We also consider the imbalance in link utilization across bottleneck
links as well as the imbalance in disk usage/data distribution across
the cluster.
The baseline for our deployment is the uniform replica place-
ment policy used in existing systems [4,19]. We compare the trace-
driven simulator against the default policy as well.
7.2 Sinbad’s Overall Improvements
Sinbad reduces the average completion time of write-heavy jobs
by up to 1:26(cid:2) (Figure 6), and jobs across all bins gain a 1:29(cid:2)
boost in their average write completion times. Note that varying