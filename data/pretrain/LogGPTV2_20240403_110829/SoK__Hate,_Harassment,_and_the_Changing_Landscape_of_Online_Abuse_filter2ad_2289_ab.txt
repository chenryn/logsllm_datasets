### Channel to the Target or Other Audience for Dissemination

A communication channel can be used to disseminate text, images, or other media to a target or broader audience. This use of media allows for moderation or filtering by the operator of the communication channel. Scenarios where an attacker can share predefined reactions (e.g., a thumbs down on a video) only partially meet this criterion.

### Capabilities and Criteria for Attacks

#### Deception of an Audience (C1)
We examine whether the attack relies on deceiving an online audience to humiliate the target or damage their reputation.

#### Deception of a Third-Party Authority (C2)
This capability is more nuanced and considers whether the attacker leverages an authority to inadvertently take action on the attacker’s behalf.

#### Amplification (C3)
Some attacks inherently require coordinated action or amplification to succeed, such as mass down-voting a target’s videos or launching a denial-of-service attack on a target’s website. In some cases, amplification may come from the platform itself (e.g., video conference calls that focus all viewers on a speaker). While all attacks may benefit from amplification, we limit this criterion to scenarios where amplification is necessary.

#### Privileged Access (C4)
As our final criterion, we consider whether the attacker requires privileged access to information, an account, or a device. Such access may be obtained through coercion, misplaced trust (e.g., a spouse or peer), or a security or privacy vulnerability (e.g., a weak password). Scenarios where the information available to an attacker is public but not widely available, such as legal documents, partially satisfy this criterion.

### Harms Resulting from Attacks

Our taxonomy also explores the specific harms that attackers likely intend as the outcome of online hate and harassment. We highlight whether an attack’s intent is to silence a target, damage a target’s reputation, reduce a target’s sense of sexual or physical safety, or coerce a target. Given the complex relationships between attackers and targets, we argue it would be inappropriate to categorize attacks based on harms. Instead, we highlight potential harms to better explain the differences between threats like sexual harassment and bullying. These harms may play a role in policy development but do not impact technical solutions, which is the primary focus of our taxonomy.

### Scale of Attacks

The last part of our taxonomy differentiates attacks targeting an individual, such as the non-consensual exposure of intimate images, or an entire group. In some cases, both targeting strategies are equally possible. The targeted nature of online hate and harassment differs strongly from for-profit threats, thus heavily influencing the design of new solutions.

### Categorization of Attacks

By labeling the attacks in our literature review using our criteria, we identified seven distinct categories of hate and harassment. When discussing each, we also highlight the primary security principle—confidentiality, integrity, or availability—that the attacks in each category undermine. Our goal is to illustrate how each class of attacks requires a different solution due to the capabilities and motives involved.

#### Toxic Content [Availability; A1-A2, M1 exclusively]
Toxic content covers a wide range of attacks involving media (M1) that attackers send to a target or audience (A1-A2), without requiring more advanced capabilities (not C1-C4). Attacks in this category include bullying, trolling (e.g., intentionally provoking audiences with inflammatory remarks), threats of violence, and sexual harassment. A close equivalent in for-profit cybercrime is spam. Repeated abuse may result in targets deleting their accounts to avoid toxic interactions, effectively silencing and marginalizing them. This illustrates how toxic content can violate availability, preventing victims from properly taking advantage of an online community and even forcing them to leave it.

Numerous studies have examined toxic content spread via social networks, with a particular focus on content targeting minorities and women. Other threats in this space include the viral distribution of hateful or racist memes and videos and abuse carried out among online gaming players. This content, often originating in communities dedicated to hate and harassment, makes its way into mainstream social networks, impacting a much broader audience. The anonymous nature of online communication hampers accountability.

#### Content Leakage [Confidentiality; A2 + M1 + C4]
Content leakage involves any scenario where an attacker leaks (or threatens to leak) sensitive, private information (M1 + C4) to a wider audience (A2). Often, the attacker’s intent is to embarrass, threaten, intimidate, or punish the target. An attacker may obtain access to this information via privileged access, such as compromising the target’s account or socially engineering a third party, or through information requests, public legal records, and data breaches. Snyder et al. studied over 4,500 “doxing” attacks, finding that 90% of incidents exposed physical mailing addresses, 60% exposed phone numbers, and 53% exposed personal email addresses. Specific to the LGBTQ+ community, attackers may also reveal an individual’s sexual identity (e.g., “outing”) or reject an individual’s gender identity by using the former name of a transgender or non-binary person (e.g., “deadnaming”). Internet audiences may amplify the fallout of a target’s personal information being exposed.

Other forms of content leakage are rooted in sexual violence. For example, an attacker (e.g., a former partner) can expose intimate images to the target’s friends, family, colleagues, or publicly. This is often referred to as non-consensual intimate imagery or “revenge porn.” Survivors of intimate partner violence frequently report this problem. In a prior survey, Microsoft estimated that up to 2% of people have been recorded in an intimate situation without their consent. Another survey found that 4% of Americans have been threatened with or experienced non-consensual intimate image exposure. Such threats can lead to a vicious cycle of extortion (e.g., “sextortion”), where the target continues to supply intimate images to avoid exposure.