channel to the target or other audience to disseminate text,
images, or other media. The use of media opens up the
possibility of moderation or ﬁltering via the operator of the
communication channel. Scenarios where an attacker can share
predeﬁned reactions (e.g., thumbs down on a video) only
partially satisfy this criteria.
Does the attack require deception of an audience? (C1).
In terms of capabilities, we examine whether or not the attack
relies on some level of deception of an online audience in order
to humiliate the target or otherwise damage their reputation.
Does the attack require deception of a third-party au-
thority? (C2). This capability is more nuanced and considers
whether or not the attacker leverages an authority to (inadver-
tently) take action on the attacker’s behalf.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
249
Does the attack require ampliﬁcation? (C3). Some attacks
inherently require coordinated action or ampliﬁcation to suc-
ceed, such as mass down-voting a target’s videos, or denial-of-
servicing a target’s website. In some cases, ampliﬁcation may
come from the platform itself (e.g., video conference calls
which focus all viewers on a speaker). While all attacks likely
beneﬁt from some form of ampliﬁcation, we limit this criterion
only to when ampliﬁcation is a necessity.
Does the attack require privileged access to information,
an account, or a device? (C4). As our ﬁnal criterion, we
consider whether or not an attacker requires privileged access.
Such access may come through coercion, misplaced trust (e.g.,
a spouse or peer), or a security or privacy vulnerability (e.g.,
a weak password). Scenarios where the information available
to an attacker may be public, but not widely available—such
as legal documents—partially satisfy this criteria.
D. Harms that result from attacks
Apart from the capabilities required to conduct an attack,
our taxonomy also explores the speciﬁc harms that attackers
likely intend as the outcome of online hate and harassment.
In particular, we highlight whether an attack’s intent is to
silence a target; to damage a target’s reputation; to reduce
a target’s sense of sexual or physical safety; or to coerce
a target. As our threat model covers a gamut of complex
relationships between attackers and targets, we argue it would
be inappropriate for our taxonomy to speciﬁcally categorize
attacks based on harms. Instead, we merely highlight potential
harms to better explain the difference between threats like
sexual harassment and bullying. These harms may play a role
in policy development but do not impact technical solutions,
which is the primary role of our taxonomy.
E. Scale of attacks
The last part of our taxonomy differentiates attacks targeting
an individual—like the non-consensual exposure of intimate
images—or an entire group. In some cases, both targeting
strategies are equally possible. The targeted nature of online
hate and harassment differs strongly from for-proﬁt threats,
and thus heavily inﬂuences the design of new solutions.
F. Categorization of attacks
By labeling the attacks in our literature review using our
criteria, we identiﬁed seven distinct categories of hate and ha-
rassment. When discussing each, we also highlight the primary
security principle—conﬁdentiality, integrity, or availability—
that the attacks in each category undermine. We make no claim
our list of attacks is exhaustive. Instead, our goal is to illustrate
how each class of attacks requires a different solution due to
the capabilities and motives involved.
Toxic Content [Availability; A1-A2, M1 exclusively]. Toxic
content covers a wide range of attacks involving media (M1)
that attackers send to a target or audience (A1-A2), but
without the necessity of more advanced capabilities (not C1-
C4). Attacks in this category include bullying, trolling (e.g.,
intentionally provoking audiences with inﬂammatory remarks),
threats of violence, and sexual harassment. A close equivalent
in for-proﬁt cybercrime is spam [99]. Repeated abuse may
result in targets deleting their account to avoid toxic interac-
tions, effectively silencing and marginalizing the targets [105],
[127], [131]. This illustrates how toxic content can be used to
violate availability, preventing victims from properly taking
advantage of an online community and even forcing them to
leave it.
Numerous studies have examined toxic content that attack-
ers spread via social networks [30], [75], [119], [124], with a
particular focus on toxic content targeting minorities [123] and
women [28], [35], [141]. Other threats in this space include the
viral distribution of hateful or racist memes and videos [60],
[115], [125], [134], [150] and abuse carried out among online
gaming players [11], [98], [137]. This content, which can
originate in communities dedicated to hate and harassment,
often makes its way into mainstream social networks [61],
[150], in turn impacting a much broader audience. All of these
attacks are aided in part by the anonymous nature of online
communication, which hampers accountability [152].
Content Leakage [Conﬁdentiality; A2 + M1 + C4]. Content
leakage involves any scenario where an attacker leaks (or
threatens to leak) sensitive, private information (M1 + C4) to
a wider audience (A2). Often, the attacker’s intent is either to
embarrass, threaten, intimidate, or punish the target [53]. An
attacker may obtain access to this information via privileged
access, such as compromising the target’s account or socially
engineering a third party; via information requests, public
legal records, and records exposed by data breaches; or by
coercing the target through duress. Snyder et al. previously
studied over 4,500 “doxing” attacks that exposed a target’s
personal information to a broad audience [131]. They found
that 90% of incidents exposed physical mailing addresses, 60%
exposed phone numbers, and 53% exposed personal email
addresses. Speciﬁc to the LGTBQ+ community, attackers may
also reveal an individual’s sexual identity (e.g., “outing”) or
reject an individual’s gender identity by using the former name
of a transgender or non-binary person (e.g., “deadnaming”). In
turn, Internet audiences may amplify the fallout of a target’s
personal information being exposed [82], [102].
Other forms of content leakage are rooted in sexual vio-
lence. For example, an attacker (e.g., former partner) can ex-
pose intimate images to the target’s friends, family, colleagues,
or even publicly. This is often referred to as non-consensual
intimate imagery or, colloquially, as “revenge porn”. Survivors
of intimate partner violence report this as a frequent prob-
lem [66], [108], [145]. In a prior survey, Microsoft estimated
as many as 2% of people have been recorded in an intimate
situation without their consent [110]. Another survey found
4% of Americans have been threatened with or experienced
non-consensual intimate image exposure [43]. Such threats can
in turn lead to a vicious cycle of extortion (e.g., “sextortion”),
where the target continues to supply intimate images to avoid
exposure.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
250
Criteria
Harms
Scale
)
4
C
(
?
s
s
e
c
c
a
d
e
g
e
l
i
v
i
r
p
s
e
r
i
u
q
e
R
)
3
C
(
?
n
o
i
t
a
c
ﬁ
i
l
p
m
a
s
e
r
i
u
q
e
R
?
n
o
i
t
a
t
u
p
e
r
e
g
a
m
a
d
o
t
t
n
e
t
n
I
?
y
t
e
f
a
s
l
a
u
x
e
s
e
c
u
d
e
r
o
t
t
n
e
t
n
I
?
y
t
e
f
a
s
l
a
c
i
s
y
h
p
e
c
u
d
e
r
o
t
t
n
e
t
n
I
?
e
c
n
e
l
i
s
o
t
t
n
e
t
n
I
?
l
a
u
d
i
v
i
d
n
i
n
a
s
t
e
g
r
a
T
?
e
c
r
e
o
c
o
t
t
n
e
t
n
I
?
p
u
o
r
g
a
s
t
e
g
r
a
T
(cid:32)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:32)
(cid:32)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35) (cid:32) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:71)(cid:35) (cid:32) (cid:32) (cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:32) (cid:71)(cid:35) (cid:32) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35) (cid:32)
(cid:32)
(cid:32)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:32)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:71)(cid:35) (cid:71)(cid:35)
(cid:32)
(cid:71)(cid:35) (cid:32) (cid:32) (cid:32)
(cid:71)(cid:35) (cid:32) (cid:32) (cid:32)
(cid:71)(cid:35) (cid:32) (cid:32) (cid:32)
(cid:32) (cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32) (cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32) (cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32) (cid:71)(cid:35) (cid:71)(cid:35) (cid:71)(cid:35)
(cid:32) (cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32) (cid:71)(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:71)(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
)
1
M
(
?
t
x
e
t
r
o
s
e
g
a
m
i
s
a
h
c
u
s
a
i
d