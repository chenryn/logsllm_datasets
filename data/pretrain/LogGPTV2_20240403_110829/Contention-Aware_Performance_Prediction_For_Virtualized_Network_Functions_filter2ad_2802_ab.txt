instead multifaceted. To expose the different sources of contention,
we look at the life of a packet i.e., the system components it tra-
verses on its way to the NF. Figure 4 visualizes this process at a high
level. During I/O, a packet arriving at the NIC will traverse the PCIe
bus and will be DMAâ€™ed either to DRAM or directly to the LLC,
if the server supports DDIO [7]. The NF will read the incoming
packet from the memory hierarchy and the NF will apply its packet
processing logic to it. Depending on the nature of the NF, data from
auxiliary data structures stored in the memory hierarchy might be
requested. Ultimately, the packet will be written back to the NIC
following the reverse process.
Along these datapaths, we identify three independent sources of
contention; (1) contention in the LLC (i.e., contention that compro-
mises fast access to auxiliary data structures containing necessary
SnortIP RouterVPNFlowStatsStatelessFirewallMagLev LBSuricatapfSense01020304050Prediction Error (%)DobrescuBubbleUp1234Number of Corunners5.07.510.012.515.017.520.022.525.0Total Throughput (Mpps)No Isolation - ObservedIsolation - ObservedIsolation - Ideal Linear ScalingNICPCIeIOMMUDDIOCore0Main MemoryALUL1L2MMUDPDKCore1ALUL1L2MMUDPDKLLCFigure 5: Target NF throughput as a function of the LLC occupancy of the
competing NFs.
data for packet processing), (2) contention for DDIO resources dur-
ing packet I/O (i.e., contention that slows down packets on the direct
path between the NICs and the LLC), and (3) contention for main
memory bandwidth (i.e., contention that increases the latency to
service a LLC miss from main memory). In what follows, we present
the underlying mechanisms of each contention source and argue
why using multiple metrics can accurately quantify contention.
Experimental setup: We experiment with two ğ‘¥86 architectures:
(1) Intel Xeon E5-2620 v4 (Broadwell) and (2) Intel Xeon Silver 4110
(Skylake). Both server architectures are equipped with two physi-
cal NICs (XL710-40Gbps NIC for Broadwell, Mellanox MT27700 -
100Gbps for Skylake) that were partitioned among co-running NFs
using SR-IOV [14]. We relied on DPDK for packet acceleration for
all NFs except PFSense which does not support packet acceleration.
We ran our evaluation on a range of NFs shown in Table 1 drawn
both from research prototypes (e.g., Click [32], NetBricks [42])
and popular software (e.g., Snort [13], Suricata [15], PFSense [19]).
These NFs span a broad spectrum in terms of complexity of packet
processing, ranging from simple NFs (e.g., stateless Firewall) to
more complex ones (e.g., Snort). To the extent it was possible, we
configured NFs with public rulesets (100s-1000s of rules) [13, 15].
In this section, we measure slowdown of real NFs under competi-
tion with a few carefully crafted artificial competing NFs, designed
to illuminate each form of contention independently. Note that
the point of these examples is not to demonstrate that the causes
of contention always occur independently: in practice they may
have correlated causes â€“ e.g., an increase in overall request rates
increases both cache access rate and memory bandwidth. Nonethe-
less, the fact that each bottleneck can be contended independently
motivates the need for models that capture all causes of bottlenecks.
3.1 Contention in the last level cache
Prior approaches to modelling memory contention focus solely
on contention in the LLC, which increases packet-processing time
due to evictions of NF data structures to main memory. Both Do-
brescu et al. and BubbleUp focus on this category of contention, but
model it using different metrics. Dobrescu et al. measure the rate
at which a competing NF accesses the cache (CAR), but BubbleUp
measures the working set size or cache occupancy of the competing
NF. We find that on our servers, measuring both provides the best
insight into slowdown.
Observation 1: LLC contention depends both on cache occu-
pancy and the rate at which the competing NFs accesses the
last level cache.
Figure 6: Performance drop as a function of DDIO space occupied by com-
petition
We run a Click-based IP Router (see Table 1) against one Click-
based synthetic competitor that, for every packet it receives, per-
forms a configurable number of reads to a data structure of con-
figurable size in the LLC. The IP-router in isolation occupies ap-
proximately 8MB of cache space. To guarantee isolation from other
sources of contention, we ensure (1) that the allocated packet buffers
donâ€™t introduce DDIO contention (see Â§ 3.2) and (2) that the com-
peting NFs use separate memory channels.
Figure 5 visualizes the experiment results for these two configura-
tion parameters of the synthetic workload. When cache occupancy
is low â€“ less than the red line marking exhaustion of the available
LLC space â€“ occupancy is the best predictor of performance. After
the cache is saturated, CAR becomes the dominant factor although
we still observe a downward trend correlated with occupancy.3
3.2 Contention during packet I/O
Modern x86 Intel architectures offer Data Direct I/O (DDIO),
an optimization to DMA that copies packets directly between the
NIC and a dedicated slice of the LLC [7, 23, 34]. DDIO essentially
partitions the LLC into a primary cache and an I/O cache; at startup
every NF allocates a fixed number of buffers in the I/O cache to
store the packets for that NF. Contention can occur when the total
number of packets concurrently in the system exceeds the amount
of space in the I/O cache â€“ even though the remainder of the LLC
remains relatively underloaded.4 Consequently, we need to model
contention in the slice of the cache dedicated to DDIO separately
from modeling the remainder of the LLC.
Observation 2: DDIO contention depends both on the utiliza-
tion of the DDIO space by the competing NFs and the rate at
which they access it.
We find that while not all target NFs are equally sensitive to
DDIO-contention, all NFs (irrespective of packet size or number
of allocated buffers) suffer some level of DDIO-related slowdown.
In Figure 6, we illustrate an instance of a Stateless Firewall that
competes with a simple Click-based, L2-forwarding NF. In the top
graph, the Firewall has 524KB buffers allocated and in the lower
graph it has 3MB of buffers allocated. The contending L2 forwarder
is configured to ensure minimal LLC contention outside the DDIO
slice, and process traffic at rates up to 100Gbps/NF. In the L2 for-
warder NF, we vary (i) the size of the buffers allocated (occupancy)
3Dobrescuâ€™s analysis assumed that the NF is highly contended and hence the occupancy
of the competing NFs far exceeded the size of the LLC â€“ to the right of the red line.
4ResQ addresses this problem just like contention in the LLC: by partitioning and
hence isolating sub-components of the I/O cache. They achieve this through careful
allocation of buffers sized not to exceed the overall cache space. This worked well at
10Gbps speeds, but pushing towards 100Gbps, the number of concurrent packets can
exceed space in the I/O cache leading to packet loss and performance slowdown.
051015202530Cache Occupancy of Competition (MB)3.23.43.63.84.04.24.4Throughput (Mpps)70MRefs/sec70MRefs/sec90MRefs/sec120MRefs/sec130MRefs/sec0.00.20.40.60.81.00.00.20.40.60.81.0051015202597.5100.01500B500B64B0510152025Competing DDIO Occupancy (MB)90951001500B500B64BThroughput (Normalized)on shared hardware; sensitivity models how susceptible a NF is to
performance degradation due to the competitorsâ€™ aggregate con-
tentiousness.
Building on these concepts, we realize a practical workflow for
SLOMO that conceptually consists of two logical parts: (1) an offline
component that is responsible for characterizing contentiousness
and modeling sensitivity of the available NF instances; and (2) a pre-
diction component responsible for making performance predictions
given a target NF instance and a mix of real competitors.
4.1 Offline Profiling
Given a set of NFs ğ‘† = {NFi . . .} and a server architecture Archk,
SLOMO first runs a constant number of offline profiling operations
to characterize (1) the sensitivity and (2) the contentiousness of the
different (NFi, Archk) tuples. To do so, the operator runs each NFi
on the server with multiple configurations of a tunable synthetic
workload, configured to apply different amounts of pressure to the
system resources and, thus, contend with NFi. For each synthetic
configuration ğ‘¥, ğ‘¥â€™s contentiousness is represented by a vector ğ‘‰ğ‘¥.
In Â§5, we discuss our choice of synthetic workload and in Â§5.1, we
discuss our methodology for choosing the contentiousness metrics
in ğ‘‰ .
To profile for sensitivity, we measure, on every architecture, NFiâ€™s
performance ğ‘ƒğ‘¥
in response to each synthetic contentiousness
ğ‘–
ğ‘– ), . . .} is used
vector ğ‘‰ğ‘¥. The dataset consisting of all pairs {(ğ‘‰ğ‘¥, ğ‘ƒğ‘¥
to train a sensitivity model ğ‘€ğ‘– : ğ‘‰ â†’ ğ‘ƒ to predict NFiâ€™s performance
in response to any real contentiousness vector. We discuss SLOMOâ€™s
sensitivity models in Â§5.2.
To profile for contentiousness, the operator collects a set of vectors
i }, where each vector characterizes NFiâ€™s contentiousness asso-
{ğ‘‰ ğ‘¥
ciated with every synthetic run ğ‘¥. Said differently, ğ‘‰ ğ‘¥
i measures the
pressure that NFi applies on the shared resources in the presence
of ğ‘¥, as if NFi were an additional competitor. In Â§5.3, we describe
how we use the individual contentiousness vectors to compose the
contentiousness of any mix of real competitors.
These profiling datasets are specific to a particular NF type, con-
figuration, traffic workload and server architecture. In practice, a
typical cluster may use only one or a small number of server archi-
tectures which do not change frequently. However, it is possible
that, after deployment, an NFâ€™s ruleset or its traffic workload might
change. In Â§6 we discuss how SLOMO can adapt its existing mod-
els ğ‘€ğ‘– to extrapolate NFiâ€™s performance as a result of changing
operating conditions.
Even though prior work [24, 39] followed a similar workflow,
they used only linear models for sensitivity with one solitary met-
ric for contentiousness. Given our analysis in Â§3, we take a first-
principles approach to learn multivariable models and metrics. In
Â§5, our data-driven approach to contentiousness characterization
and sensitivity modeling.
4.2 Online Predictions
At run time, the operator uses the pre-computed ğ‘‰ğ‘–â€™s and ğ‘€ğ‘–â€™s
for predictions. In the most basic scenario, the operator has two
NFs, ğ‘ ğ¹ğ´ and ğ‘ ğ¹ğµ which they want to run side-by-side on the
Figure 7: Performance vs utilized memory bandwidth
and (ii) the size of the packets sent to the L2 forward, and hence
the rate of packet arrivals and memory accesses. Just as in Figure 5
which describes LLC contention, we see that slowdown in the I/O
slice of the cache is a function of both occupancy and access rate.
As a result, for accurate predictions we need to measure access rate
and occupancy of the I/O slice of the cache in addition to access
rate and occupancy of the overall LLC.5
3.3 Contention for memory bandwidth
Until now, we have focused on how NFs suffer slowdown due
to data structures being evicted from the LLC or packets being
evicted from the I/O slice of the cache. We now discuss how the
cost of eviction can vary as competitors increase their bandwidth
utilization between the cache and main memory.
Observation 3: Main memory latency depends on the aggre-
gate memory bandwidth consumption of the competing NFs.
We observe that a target NF (Click IP router) can experience up
to 18% of throughput drop as a result of main memory bandwidth
contention. To ensure that contention is limited in main memory
bandwidth utilization we use Intelâ€™s Cache Allocation Technology
(CAT) [6] to partition the last level cache of a Broadwell server in
two segments, one for the target NF (2MB) and the second (18MB)
for the competitors. The competitors consisted of aggressive Click-
based competitors that have a high cache miss rate and hence
generate large amounts of memory traffic. The target NF executes
a fixed number of accesses.
Figure 7 shows the performance of the target NF as a function
of the total memory bandwidth utilization and observe the clear
correlation between the two metrics. Additionally, we confirm that
first, slowdown is uncorrelated with contention in the LLC as the
target NF LLC miss rate stays relatively stable and second, that LLC
isolation is not sufficient for eliminating contention.
4 SLOMO OVERVIEW
SLOMO conceptually follows a blueprint for performance pre-
diction based on contentiousness and sensitivity [20, 21, 38, 39].
Specifically, contentiousness measures the pressure a NF places
5While DDIO space is fixed, isolation between DDIO and the rest of the LLC is not
perfect i.e., packet buffers can still evict LLC data and vice versa. This is orthogonal to
our claim that the two sources of contention can be independent of each other.
2.93.03.1MppsIP Router Throughput4.05.5Miss/secIP Router LLC Misses05101520MBCompeting Occupancy681012141618Memory Bandwidth Utilization (Gbps)400450500550600Mrefs/secCompeting CARsame server. To predict ğ‘ ğ¹ğ´â€™s throughput while running along-
side ğ‘ ğ¹ğµ, the operator simply takes ğ‘ ğ¹ğµâ€™s contentiousness vector
ğ‘‰ğµ and plugs it into ğ‘ ğ¹ğ´â€™s sensitivity model ğ‘€ğ´ to produce the
performance slowdown ğ‘ƒ ğµ
ğ´.
Nonetheless, some predictions are more challenging. Consider
an operator now with three NFs: ğ‘ ğ¹ğ´, ğ‘ ğ¹ğµ, and ğ‘ ğ¹ğ¶. The operator
now wants to run all three NFs side by side, and once again wants
to predict ğ‘ ğ¹ğ´â€™s throughput under this deployment. The problem
here is that, although the operator has pre-computed ğ‘‰ğµ and ğ‘‰ğ¶,
the operator does not know ğ‘‰ğµ,ğ¶ â€“ the contentiousness upon ğ‘ ğ¹ğ´
when running alongside both ğ‘ ğ¹ğµ and ğ‘ ğ¹ğ¶.
SLOMO provides a function for composition ğ¶ğ¹ : ğ‘‰ğµ, ğ‘‰ğ¶ â†’ ğ‘‰ğµ,ğ¶,
this allows the operator to compute ğ‘‰ğµ,ğ¶ offline based on the pre-
computed contentiousness vectors of each NF. After composing
ğ‘‰ğµ and ğ‘‰ğ¶, the operator can use this for prediction just as in the
basic scenario. Dobrescuâ€™s approach that used competing CAR as
the sole metric of contentiousness, implemented composition by
summing together each competitorâ€™s CAR values when run solo on
the hardware. In Â§5.3, we discuss why measuring contentiousness
during a solo run introduces prediction inaccuracies and discuss
SLOMOâ€™s implementation of ğ¶ğ¹.
5 SLOMO IN DEPTH
Having laid out the three key components to SLOMO (con-
tentiousness characterization, sensitivity modeling, and contentious-
ness composition), we now describe how we design each of these
components, taking a data-driven approach. Seen in this light, mod-
eling sensitivity is a model fitting process (Â§5.2). Similarly, choosing
contentiousness metrics is as feature selection process whose goal
is to identify metrics that quantify the competitionâ€™s pressure on
the shared hardware and have strong predictive power in the con-
text of a sensitivity model (Â§5.1). Finally, composition is a simple
regression modelling problem (Â§5.3).
Candidate contentiousness metrics: We choose our candidate
contentiousness metrics to be those exposed by the Intel PCM
framework, a performance monitoring API enabling real-time col-
lection of architecture-specific resource utilization metrics [8]. The
resulting PCM vector contains an extensive pool of metrics (e.g.,
main memory traffic, the LLC hit rate etc.) that characterize re-
source usage at core-, CPU-socket- and system-level granularities.
That said, a natural limitation of SLOMO is that it is limited by the
pool of metrics exposed by PCM. For instance, PCM does not pro-
vide visibility into the internals of a NIC. Thus, any congestion for
NIC resources (e.g., increased NIC queue occupancy from incoming
traffic) will not be taken into consideration.
Synthetic competition: Our profiling methodology assumes the
presence of a representative training dataset that adequately sam-
ples the large space of contentiousness vectors {ğ‘‰ }. To produce
this dataset, and following general guidelines discussed in prior
work [39], we exercise the effects of contention on each NF with
a synthetic workload of tunable intensity that samples the space
of possible contentiousness values that a NF could generate. To
that end, we designed an artificial Click-based NF that covers the
contentiousness space by applying incremental pressure (1) to the
I/O datapath through the number of allocated packet buffers and
Metric
IPC
INST
L3MISS
L3MPI
L3HIT
L3OCC
LMB
L2HIT
L2MPI
L2MISS
READ
WRITE
LLCMISSLAT
RMB
QPI
FREQ
Definition
Instructions/Cycle
Instructions Retired
LLC Misses
LLC Misses/Instruction
LLC Hit Rate
LLC Occupancy
Local NUMA Bandwidth
L2 Cache Hit Rate
L2 Misses/Instruction
L2 Misses â‰¡ CAR
Memory read traffic
Memory write traffic
LLC Read Miss latency
Remote NUMA Bandwidth
QPI utilization
CPU frequency