Question: A web developer suggests using robots.txt to prevent sensitive sections of the website from being discovered by attackers. What is the most effective counterargument to this suggestion?
"A": "Robots.txt is primarily used for SEO purposes, not for security measures.",
"B": "Attackers do not use automated crawlers and hence robots.txt would be ineffective.",
"C": "Robots.txt is publicly accessible and can actually aid attackers in discovering sensitive directories.",
"D": "Sensitive sections are protected by authentication, making the use of robots.txt redundant."
Answer: C