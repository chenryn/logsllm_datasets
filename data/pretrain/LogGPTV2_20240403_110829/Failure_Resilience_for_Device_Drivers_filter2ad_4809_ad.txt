until it crashed. In total, we injected over 12,500 faults,
which led to 347 detectable crashes: 226 exits due to an
internal panic (65%), 109 kill signals due to CPU and
MMU exceptions (31%), and 12 restarts due to missing
heartbeat messages (4%). The subsequent recovery was
successful in 100% of the induced failures.
Preliminary tests on the real hardware showed suc-
cess for more than 99% of the detectable failures.
In
a very small number of cases (less than 5) the network
card was confused by the faulty driver and could not be
reininitialized by the restarted driver. Instead a low-level
BIOS reset was needed. If the card had a ’master reset’
command the problem could be solved by our driver, but
our card did not have this. Further testing with different
kinds of drivers and hardware conﬁgurations is needed,
however, in order to get more insight in possible hard-
ware limitations. This will be part of our future work.
7.3 Reengineering Eﬀort
An important lesson that we have learned during our
prototype implementation is that the recovery proce-
dure requires an integrated approach for optimal results,
meaning that certain components need to be recovery-
aware. As a metric for the reengineering effort we
counted the number of lines of executable code (LoC)
that is needed to support recovery. Blank lines, com-
ments, and deﬁnitions in header ﬁles do not add to the
code complexity, so these were omitted in the counting
process. Line counting was done using the sclc.pl Perl
script [2]. Fig. 9 summarizes the results.
Fortunately, the changes required to deal with driver
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007failures are both very limited and local. The reincarna-
tion server and service utility’s logic to dynamically start
servers and drivers is reused to support recovery. Most
of the new code relates to defect detection and execution
of the recovery scripts. The virtual ﬁle system and ﬁle
server stay mostly the same, with changes centralized in
the device I/O routines. Furthermore, the recovery code
in the network server represents a minimal extension to
the code needed to start a new driver. Finally, the pro-
cess manager and microkernel are not affected at all.
Most importantly, the device drivers in our system are
hardly affected. In general, only a minimal change to
reply to heartbeat and shutdown requests from the rein-
carnation server is needed. For most drivers this change
comprises exactly 5 lines of code in the shared driver li-
brary to handle the new request types. Device-speciﬁc
driver code almost never needs to be changed. For a few
drivers, however, the code to initialize the hardware had
to be modiﬁed in order to support reinitialization. Over-
all, the changes are negligible compared to the amount
of driver code that can be guarded by our design.
7.4 General Applicability
The ideas put forward in this paper are generally ap-
plicable, and may, for example, be used in commodity
operating systems. While the degree of isolation pro-
vided by our prototype platform, MINIX 3, enabled us to
implement and test our ideas with relatively little effort,
we believe that other systems can also beneﬁt from our
ideas. Especially, since there is a trend towards isola-
tion of untrusted extensions on other operating systems.
For example, user-mode drivers have been successfully
tested on Linux [25] and adopted by Windows [27].
If the drivers are properly isolated, these systems can
build on the principles presented here in order to pro-
vide policy-driven recovery services like we do.
Component
Reinc. Server
Data Store
VFS Server
File Server
SATA Driver
RAM Disk
Network Server
RTL8139 Driver
DP8390 Driver
Process Manager
Microkernel
Total
Total LoC
2,002
384
5,464
3,356
2,443
454
20,019
2,398
2,769
2,954
4,832
39,011
Recovery LoC
593
59
274
22
5
0
124
5
5
0
0
1,072
%
30%
15%
5%
<1%
<1%
0%
<1%
<1%
<1%
0%
0%
-
Figure 9: Source code statistics on the total code base and
reengineering effort speciﬁc to recovery expressed in lines of
executable code (LoC).
8 CONCLUSION
Our research explores whether it is possible to build a
dependable operating system out of unreliable, buggy
components. We have been inspired by other hardware
and software failure-resilient designs that mask failures
so that the system can continue as though no errors had
occurred, and attempted to extend this idea to device
driver failures. Recovery from such failures is partic-
ularly important, since device drivers form a large frac-
tion of the operating system code and tend to be buggy.
In this paper, we presented an operating system ar-
chitecture in which common failures in drivers and
other critical extensions can be transparently repaired.
The system is constantly monitored by the reincarnation
server and malfunctioning components may be replaced
in a policy-driven recovery procedure to masks failures
for both users and applications. We illustrated our ideas
with concrete recovery schemes for failures in network,
block device, and character device drivers.
We also evaluated our design in various ways. We
measured the performance overhead due to our recovery
mechanisms, which can be as low as 1%. Fault-injection
experiments demonstrated that our design can recover
from realistic failures and provide continuous operation
in more than 99% of the detectable failures. Source code
analysis showed that the reengineering effort needed is
both limited and local. All in all, we believe that our
work on failure resilience for device drivers represents a
small step towards more dependable operating systems.
9 AVAILABILITY
MINIX 3 is free, open-source software, available via the
Internet. You can download MINIX 3 from the ofﬁcial
homepage at: http://www.minix3.org/, which also contains
the source code, documentation, news, and more.
10 ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for
their suggestions that improved this paper. This work
was supported by Netherlands Organization for Scien-
tiﬁc Research (NWO) under grant 612-060-420.
REFERENCES
[1] D. Abramson, J. Jackson, S. Muthrasanallur, G. Neiger, G. Reg-
nier, R. Sankaran, I. Schoinas, R. Uhlig, B. Vembu, and
J. Wiegert. Intel Virtualization Technology for Directed I/O. In-
tel Technology Journal, 10(3), Aug. 2006.
[2] B. Appleton. Source Code Line Counter (sclc.pl), Last Modiﬁed:
April 2003. Available Online.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007[3] P. T. Barham, B. Dragovic, K. Fraser, S. Hand, T. L. Harris,
A. Ho, R. Neugebauer, I. Pratt, and A. Warﬁeld. Xen and the
Art of Virtualization. In Proc. 19th Symp. on Oper. Syst. Prin.,
pages 164–177, 2003.
[4] V. Basili and B. Perricone. Software Errors and Complexity: An
Empirical Investigation. Comm. of the ACM, 21(1):42–52, Jan.
1984.
[5] H. Bos and B. Samwel. Safe Kernel Programming in the OKE.
In Proc. of the 5th IEEE Conf. on Open Architectures and Net-
work Programming, pages 141–152, June 2002.
[6] M. I. Bushnell. The HURD: Towards a New Strategy of OS
Design. GNU’s Bulletin, 1994.
[7] G. Candea, J. Cutler, and A. Fox. Improving Availability with
Recursive Microreboots: A Soft-State System Case Study. Per-
formance Evaluation Journal, 56(1):213–248, .
[8] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and A. Fox.
In Proc. 6th
Microreboot–A Technique for Cheap Recovery.
Symp. on Oper. Syst. Design and Impl., pages 31–44, .
[9] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler. An Em-
pirical Study of Operating System Errors. In Proc. 18th Symp.
on Oper. Syst. Prin., pages 73–88, 2001.
[10] T. C. K. Chou. Beyond Fault Tolerance. IEEE Computer, 30(4),
Apr. 1997.
[11] J. Christmansson and R. Chillarege. Generation of an Error Set
that Emulates Software Faults – Based on Field Data. In Proc.
26th IEEE Int’l Symp. on Fault Tolerant Computing, June 1996.
[12] A. Forin, D. Golub, and B. Bershad. An I/O System for Mach
3.0. In Proc. 2nd USENIX Mach Symp., pages 163–176, 1991.
[13] K. Fraser, S. Hand, R. Neugebauer, I. Pratt, A. Warﬁeld, and
M. Williamson. Safe Hardware Access with the Xen Virtual
Machine Monitor. In Proc. 1st Workshop on Oper. Sys. and Arch.
Support for the On-Demand IT InfraStructure, Oct. 2004.
[14] A. Gefﬂaut, T. Jaeger, Y. Park, J. Liedtke, K. Elphinstone, V. Uh-
lig, J. Tidswell, L. Deller, and L. Reuther. The SawMill Multi-
server Approach. In Proc. 9th ACM SIGOPS European Work-
shop, pages 109–114, Sept. 2000.
[15] J. Gray. Why Do Computers Stop and What Can Be Done About
It? In Proc. 5th Symp. on Reliability in Distributed Software and
Database Systems, pages 3–12, 1986.
[16] H. H¨artig, M. Hohmuth, J. Liedtke, S. Sch¨onberg, and J. Wolter.
In Proc. 6th
The Performance of µ-Kernel-Based Systems.
Symp. on Oper. Syst. Prin., pages 66–77, Oct. 1997.
[17] H. H¨artig, M. Hohmuth, N. Feske, C. Helmuth, A. Lackorzynski,
F. Mehnert, and M. Peter. The Nizza Secure-System Architec-
ture. In Proc. 1st Conf. on Collaborative Computing, Dec. 2005.
[18] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum.
Reorganizing UNIX for Reliability. In Proc. 11th Asia-Paciﬁc
Comp. Sys. Arch. Conf., Sept. 2006.
[19] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanen-
baum. Construction of a Highly Dependable Operating System.
In Proc. 6th European Dependable Computing Conf., Oct. 2006.
[20] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S. Tanenbaum.
MINIX 3: A Highly Reliable, Self-Repairing Operating System.
ACM SIGOPS Operating System Review, 40(3), July 2006.
[21] D. Hildebrand. An Architectural Overview of QNX. In Proc.
USENIX Workshop on Microkernels and Other Kernel Architec-
tures, pages 113–126, Apr. 1992.
[22] M. Hohmuth and H. Tews. The VFiasco Approach for a Veriﬁed
In Proc. 2nd ECOOP Workshop on Prog.
Operating System.
Lang. and Oper. Sys., July 2005.
[23] G. Hunt, C. Hawblitzel, O. Hodson, J. Larus, B. Steensgaard,
and T. Wobber. Sealing OS Processes to Improve Dependability
and Safety. In Proc. 2nd EuroSys Conf., 2007.
[24] J. Liedtke. On µ-Kernel Construction. In Proc. 15th Symp. on
Oper. Syst. Prin., pages 237–250, Dec. 1995.
[25] B. Leslie, P. Chubb, N. Fitzroy-Dale, S. Gotz, C. Gray,
L. Macpherson, D. Potts, Y.-T. Shen, K. Elphinstone, and
G. Heiser. User-Level Device Drivers: Achieved Performance.
Journal of Computer Science and Technology, 20(5):654–664,
Sept. 2005.
[26] J. LeVasseur, V. Uhlig, J. Stoess, and S. Gotz. Unmodiﬁed De-
vice Driver Reuse and Improved System Dependability via Vir-
tual Machines.
In Proc. 6th Symp. on Oper. Syst. Design and
Impl., pages 17–30, Dec. 2004.
[27] Microsoft Corporation. Architecture of the User-Mode Driver
Framework. In Proc. 15th WinHEC Conf., May 2006.
Ofﬁcial Website and Download:.
[28] MINIX 3.
URL
http://www.minix3.org/.
[29] R. Moraes, R. Barbosa, J. Dures, N. Mendes, E. Martins, and
H. Madeira.
Injection of Faults at Component Interfaces and
Inside the Component Code: Are They Equivalent? In Proc. 6th
Eur. Dependable Computing Conf., pages 53–64, Oct. 2006.
[30] B. Murphy and N. Davies. System Reliability and Availability
In Proc. 29th Int’l Symp. on Fault-
Drivers of Tru64 UNIX.
Tolerant Computing, June 1999. Tutorial.
[31] V. P. Nelson. Fault-Tolerant Computing: Fundamental Concepts.
IEEE Computer, 23(7):19–25, July 1990.
[32] W. T. Ng and P. M. Chen. The Systematic Improvement of Fault
Tolerance in the Rio File Cache. In Proc. 29th Int’l Symp. on
Fault-Tolerant Computing, pages 76–83, June 1999.
[33] V. Prabhakaran, L. N. Bairavasundaram, N. Agrawal, H. S. Gu-
nawi, A. C. Arpaci-Dusseau, and R. H. Arpaci-Dusseau. IRON
File Systems.
In Proc. 20th Symp. on Oper. Sys. Prin., pages
206–220, Oct. 2005.
[34] J. Saltzer and M. Schroeder. The Protection of Information in
Computer Systems. Proc. of the IEEE, 63(9), Sept. 1975.
[35] L. Seawright and R. MacKinnon. VM/370—A Study of Multi-
plicity and Usefulness. IBM Systems Journal, 18(1):4–17, 1979.
[36] J. Sugerman, G. Venkitachalam, and B.-H. Lim. Virtualizing
I/O Devices on VMware Workstation’s Hosted Virtual Machine
Monitor. In Proc. USENIX Ann. Tech. Conf., pages 1–14, 2001.
[37] M. Sullivan and R. Chillarege. Software Defects and their Im-
pact on System Availability – A Study of Field Failures in Oper-
ating Systems. In Proc. 21st Int’l Symp. on Fault-Tolerant Com-
puting, June 1991.
[38] Sun Microsystems. Predictive Self-Healing in the Solaris 10 Op-
erating System, June 2004. Available Online.
[39] M. Swift, M. Annamalai, B. Bershad, and H. Levy. Recovering
Device Drivers. In Proc. 6th Symp. on Oper. Syst. Design and
Impl., pages 1–15, Dec. 2004.
[40] M. Swift, B. Bershad, and H. Levy. Improving the Reliability of
Commodity Operating Systems. ACM Trans. on Comp. Syst., 23
(1):77–110, 2005.
[41] T.J. Ostrand and E.J. Weyuker. The Distribution of Faults in a
Large Industrial Software System. In Proc. Symp. on Software
Testing and Analysis, pages 55–64, July 2002.
[42] J. Xu, Z. Kalbarczyk, and R. K. Iyer. Networked Windows NT
In Proc. 6th Paciﬁc Rim
System Field Failure Data Analysis.
Symp. on Dependable Computing, pages 178–185, Dec. 1999.
[43] A. R. Yumerefendi and J. S. Chase. The Role of Accountability
in Dependable Distributed Systems. In Proc. 1st Workshop on
Hot Topics in System Dependability, June 2005.
[44] F. Zhou, J. Condit, Z. Anderson, I. Bagrak, R. Ennals, M. Harren,
G. Necula, and E. Brewer. SafeDrive: Safe and Recoverable Ex-
tensions Using Language-Based Techniques. In Proc. 7th Symp.
on Oper. Sys. Design and Impl., pages 45–60, Nov. 2006.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007