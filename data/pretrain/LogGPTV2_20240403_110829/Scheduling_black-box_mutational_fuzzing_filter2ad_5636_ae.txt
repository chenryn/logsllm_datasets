ulations of a 10-day campaign. We present -Greedy and
Weighted-Random at the top of each epoch-type row group,
each showing ﬁve entries that correspond to the belief metric
used. For the other three MAB algorithms, we only show a
single entry in the center because these algorithms do not
use our belief metrics. Figure 4 describes the variability of
our data using error bars showing a 99% conﬁdence inter-
val. Notice that 94% of our scheduling algorithms have a
conﬁdence interval that is less than 2 (bugs). RGR gives the
most volatile algorithms. This is not surprising because RGR
tends to under-explore by focusing too much on bug-yielding
conﬁgurations that it encounters early on in a campaign. In
the remainder of this section, we highlight several important
aspects of our results.
Fixed-time algorithms prevail over ﬁxed-run algorithms.
In the majority of Table 2, except for RPM and Density
in the intra-program dataset, ﬁxed-time algorithms always
produced more bugs than their ﬁxed-run counterparts. In-
tuitively, diﬀerent inputs to a program may take diﬀerent
amounts of time to execute, leading to diﬀerent fuzzing
throughputs. A ﬁxed-time algorithm can exploit this fact
and pick conﬁgurations that give higher throughputs, ul-
timately testing a larger fraction of the input space and
potentially ﬁnding more bugs. To investigate the above ex-
ceptions, we have also performed further analysis on the
intra-program dataset. We found that the performance of
the ﬁxed-time variants of RPM and Density greatly improves
in longer simulations. In particular, all ﬁxed-time algorithms
outperform their ﬁxed-run counterparts after day 11.
Along the same line, we observe that ﬁxed-time algorithms
yield 1.6× more bugs on average when compared to their
ﬁxed-run counterparts in the inter-program dataset. In con-
trast, the improvement is only 1.1× in the intra-program
dataset. As we have explained above, ﬁxed-time algorithms
tend to perform more fuzz runs and potentially ﬁnding more
bugs by taking advantage of faster conﬁgurations. Thus, if
the runtime distribution of fuzz runs is more biased, as in the
case of the inter-program dataset, then ﬁxed-time algorithms
tend to gain over their ﬁxed-run counterparts.
Time-normalization outperforms runs-normalization. In
our results, EWT always outperforms RPM and Rate always
outperforms Density. We believe that this is because EWT
and Density do not spend more time on slower programs
and slower programs are not necessarily buggier. The latter
hypothesis seems highly plausible to us; if true, it would
imply that time-normalized belief metrics are more desirable
than runs-normalized metrics.
Fixed-time Rate works best. In both datasets, the best-
performing algorithms use ﬁxed-time epochs and Rate as
belief (entries shown in boldface in Table 2). Since Rate
can be seen as a time-normalized variant of RGR, this gives
further evidence of the superiority of time normalization. In
addition, it also supports the plausibility of the bug prior.
6.5 Speed of Bug Finding
Besides the number of bugs found at the end of a fuzz
campaign, the speed at which bugs are discovered is also
an important metric for evaluating scheduling algorithms.
We address two questions in this section. First, is there
a scheduling algorithm that prevails throughout an entire
fuzz campaign? Second, how eﬀective are the algorithms
with respect to our oﬄine algorithm in §4.5? To answer
the questions, we ﬁrst show the speed of each algorithm in
Figure 5 and Figure 6 by computing the number of bugs
found over time. For brevity and readability, we picked for
each belief metric the algorithm that produced the greatest
average number of unique bugs at the end of the 10-day
simulations.
Speed. We observe that Rate and RGR are in the lead for
the majority of the time during our 10-day simulations. In
other words, not only do they ﬁnd more unique bugs at
the end of the simulations, but they also outperform other
algorithms at almost any given time. This lends further
credibility to the bug prior.
Figure 5: Bug ﬁnding speed of diﬀerent belief-based algo-
rithms for the intra-program dataset.
Figure 6: Bug ﬁnding speed of diﬀerent belief-based algo-
rithms for the inter-program dataset.
Effectiveness. We also compare the eﬀectiveness of each
algorithm by observing how it compares against our oﬄine
algorithm. We have implemented the oﬄine algorithm dis-
cussed in §4.5 including the post-processing step that dis-
counts duplicated bugs and computed the solution for each
dataset. The numbers of bugs found by the oﬄine algorithm
for the intra- and the inter-program datasets are 132 and
217 respectively. (Notice that due to bug overlaps and the
discount heuristic, these are lowerbounds on the oﬄine opti-
mal.) As a comparison, Rate found 83% and 77% of bugs in
the intra- and inter-program datasets, respectively. Based
on these numbers, we conclude that Rate-based algorithms
are eﬀective.
6.6 Comparison with CERT BFF
At present, the CERT Basic Fuzzing Framework (BFF) [14] is
the closest system that makes use of scheduling algorithms for
fuzz campaigns. In this section, we evaluate the eﬀectiveness
of BFF’s scheduling algorithm using our simulator.
Based on our study of the source code of BFF v2.6 (the
latest version as of this writing), it uses a ﬁxed-run weighted-
random algorithm with Density ( #bugs
#runs ) as its belief metric.
However, a key feature of BFF prevented us from completely
implementing its algorithm in our simulation framework. In
particular, while BFF focuses on fuzzing a single program,
it considers not only a collection of seeds but also a set of
predetermined mutation ratios. In other words, instead of
choosing program-seed pairs as in our experiments, BFF
chooses seed-ratio pairs with respect to a single program.
Since our simulator does not take mutation ratio into ac-
count, it can only emulate BFF’s algorithm in conﬁguration
selection using a ﬁxed mutation ratio. We note that adding
the capability to vary the mutation ratio is prohibitively
expensive for us: FuzzSim is an oﬄine simulator, and there-
fore we need to collect ground-truth data for all possible
conﬁgurations. Adding a new dimension into our current
system would directly multiply our data collection cost.
Going back to our evaluation, let us focus on the Weighted-
Random rows in Table 2. Density with ﬁxed-run epochs
(BFF) yields 84 and 92 bugs in the two datasets. The cor-
responding numbers for Rate with ﬁxed-time epochs (our
recommendation) are 100 and 167, with respective improve-
ments of 1.19× and 1.82× (average 1.5×). Based on these
numbers, we believe future versions of BFF may beneﬁt from
switching over to Rate with ﬁxed-time epochs.
7 Related Work
Since its introduction in 1990 by Miller et al. [18], fuzzing
in its various forms has become the most widely-deployed
technique for ﬁnding bugs. There has been extensive work to
improve upon their ground-breaking work. A major thrust
of this research concerns the generation of test inputs for
the target program and the two main paradigms in use are
mutational and generational fuzzing [17].
More recently, sophisticated techniques for dynamic test
generation have been applied in fuzzing [8, 11]. White-box
fuzzing [7] is grounded in the idea of “data-driven improve-
ment,” which uses feedback from previous fuzz runs to “focus
limited resources on further research and improve future
runs.” The feedback data used in determining inputs is ob-
tained via symbolic execution and constraint solving; other
work in feedback-driven input generation relies on taint anal-
ysis and control ﬂow graphs [13, 20]. Our works bears some
similarity to feedback-driven or evolutionary fuzzing in that
we also use data from previous fuzz runs to improve fuzzing
eﬀectiveness. However, the black-box nature of our approach
implies that feedback is limited to observing crashes. Like-
wise, our focus on mutating inputs means that we do not
construct brand new inputs and instead rely on selecting
among existing conﬁgurations. Thus, our work can be cast
as dynamic scheduling of fuzz conﬁgurations.
Despite its prominence, we know of no previous work that
has systematically investigated the eﬀectiveness of diﬀerent
scheduling algorithms in fuzzing. Our approach focuses on
allocating resources for black-box mutational fuzzing in order
to maximize the number of unique bugs found in any period
of time. The closest related work is the CERT Basic Fuzzing
Framework (BFF) [14], which considers parameter selection
for zzuf. Like BFF, we borrow techniques from Multi-Armed
Bandits (MAB) algorithms. However, unlike BFF, which
considers repeated fuzz runs as independent Bernoulli trials,
we model this process as a Weighted Coupon Collector’s
Problem (WCCP) with unknown weights to capture the
decrease in the probability of ﬁnding a new bug over the
course a fuzz campaign.
In constructing our model, we draw heavily on research in
software reliability as well as random testing. The key insight
of viewing random testing as coupon collecting was recently
made in [1]. A key diﬀerence between our work and [1] is
that their focus is on the formalization of random testing,
whereas our goal is to maximize the number of bugs found
in a fuzz campaign. Software reliability refers to the prob-
ability of failure-free operation for a speciﬁed time period
RPMDensityRREWTRGRRateOffline050100012345678910days#bugsDensityRPMRREWTRGRRateOffline050100150200012345678910days#bugsand execution environment [6]. As a measure of software
quality, software reliability is used within the software engi-
neering community to “plan and control resources during the
development process” [12], which is similar to the motivation
behind our work.
8 Conclusion and Future Work
In this paper we studied how to ﬁnd the greatest number of
unique bugs in a fuzz campaign. We modeled black-box muta-
tional fuzzing as a WCCP process with unknown weights and
used the condition in the No Free Lunch theorem to guide us
in designing better online algorithms for our problem. In our
evaluation of the 26 algorithms presented in this paper, we
found that the ﬁxed-time weighted-random algorithm with
the Rate belief metric shows an average of 1.5× improvement
over its ﬁxed-run Density-based counterpart, which is cur-
rently used by the CERT Basic Fuzzing Framework (BFF).
Since our current project does not investigate the eﬀect of
varying the mutation ratio, a natural follow-up work would
be to investigate how to add this capability to our system in
an aﬀordable manner.
Acknowledgment
The authors thank Will Dormann, Jonathan Foote, and
Allen Householder of CERT for encouragement and fruitful
discussions. This material is based upon work funded and
supported by the Department of Defense under Contract No.
FA8721-05-C-0003 with Carnegie Mellon University for the
operation of the Software Engineering Institute, a federally
funded research and development center, and the National
Science Foundation. This material has been approved for
public release and unlimited distribution.
References
[1] A. Arcuri, M. Z. Iqbal, and L. Briand. Formal Analysis
of the Eﬀectiveness and Predictability of Random
Testing. In International Symposium on Software
Testing and Analysis, pages 219–229, 2010.
[2] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E.
Schapire. The Nonstochastic Multiarmed Bandit
Problem. Journal on Computing, 32(1):48–77, 2002.
[3] P. Auer, N. Cesa-Bianchi, and F. Paul. Finite-time
Analysis of the Multiarmed Bandit Problem. Machine
Learning, 47(2-3):235–256, 2002.
[4] T. Avgerinos, S. K. Cha, B. T. H. Lim, and
D. Brumley. AEG: Automatic Exploit Generation. In
Proceedings of the Network and Distributed Systems
Security Symposium, 2011.
[5] D. A. Berry and B. Fristedt. Bandit Problems:
Sequential Allocation of Experiments. Chapman and
Hall, 1985.
[6] A. Bertolino. Software testing research: Achievements,
challenges, dreams. In Future of Software Engineering,
pages 85–103, 2007.
[7] E. Bounimova, P. Godefroid, and D. Molnar. Billions
and Billions of Constraints: Whitebox Fuzz Testing in
Production. In Proceedings of the International
Conference on Software Engineering, pages 122–131,
2013.
[8] C. Cadar, D. Dunbar, and D. Engler. KLEE:
Unassisted and Automatic Generation of High-coverage
Tests for Complex Systems Programs. In Proceedings
of the USENIX Symposium on Operating System
Design and Implementation, pages 209–224, 2008.
[9] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley.
Unleashing Mayhem on Binary Code. In Proceedings of
the IEEE Symposium on Security and Privacy, pages
380–394, 2012.
[10] D. Engler, D. Chen, S. Hallem, A. Chou, and B. Chelf.
Bugs as Deviant Behavior: A General Approach to
Inferring Errors in Systems Code. In Proceedings of the
ACM Symposium on Operating System Principles,
pages 57–72, 2001.
[11] P. Godefroid, M. Y. Levin, and D. Molnar. SAGE:
Whitebox Fuzzing for Security. Communications of the
ACM, 55(3):40–44, 2012.
[12] A. L. Goel. Software Reliability Models: Assumptions,
Limitations, and Applicability. IEEE Transactions on
Software Engineering, 11(12):1411–1423, 1985.
[13] N. Gupta, A. P. Mathur, and M. L. Soﬀa. Automated
Test Data Generation Using An Iterative Relaxation
Method. In Proceedings of the ACM SIGSOFT
International Symposium on Foundations of Software
Engineering, pages 231–244, 1998.
[14] A. D. Householder and J. M. Foote. Probability-Based
Parameter Selection for Black-Box Fuzz Testing.
Technical Report August, CERT, 2012.
[15] B. D. Jovanovic and P. S. Levy. A Look at the Rule of
Three. The American Statistician, 51(2):137–139, 1997.
[16] C. Labs. zzuf: multi-purpose fuzzer.
http://caca.zoy.org/wiki/zzuf.
[17] R. McNally, K. Yiu, D. Grove, and D. Gerhardy.
Fuzzing: The State of the Art. Technical Report
DSTO–TN–1043, Defence Science and Technology
Organisation, 2012.
[18] B. P. Miller, L. Fredriksen, and B. So. An Empirical
Study of the Reliability of UNIX Utilities.
Communications of the ACM, 33(12):32–44, 1990.
[19] D. Molnar, X. Li, and D. Wagner. Dynamic Test
Generation To Find Integer Bugs in x86 Binary Linux
Programs. In Proceedings of the USENIX Security
Symposium, pages 67–82, 2009.
[20] C. Pacheco, S. K. Lahiri, M. D. Ernst, and T. Ball.
Feedback-Directed Random Test Generation. In
Proceedings of the International Conference on
Software Engineering, pages 75–84, 2007.
[21] D. Wagner, J. S. Foster, E. A. Brewer, and A. Aiken. A
First Step towards Automated Detection of Buﬀer
Overrun Vulnerabilities. In Proceedings of the Network
and Distributed Systems Security Symposium, pages
3–17, 2000.
[22] D. Wolpert and W. Macready. No free lunch theorems
for optimization. IEEE Transactions on Evolutionary
Computation, 1(1):67–82, 1997.