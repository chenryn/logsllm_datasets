two meaningful roots after trying many instances. When there are
two real roots, the incorrect one fall out of the normal distribution
of embeddings, i.e., has large norm. Therefore, es can be uniquely
identified.
Cosine Distance. For embedding schemes choosing Cosine dis-
tance3, (cid:174)es can be inferred as well by solving the equation below.
A · (cid:174)es| (cid:174)es| = D
(9)
(cid:174)en| (cid:174)en | }⊺ and D = {1 − d1, 1 − d2, ..., 1 −
where A = { (cid:174)e1| (cid:174)e1| ,
dn}⊺.
There is only one root for (cid:174)es, which is A−1 · D · | (cid:174)es|. Though
| (cid:174)es| cannot be derived, different value has no impact to the final
result, as the | (cid:174)es| will be normalized by the generator of ImgRev.
Therefore, we set | (cid:174)es| = 1.
Overall, our result shows face embedding cannot be secured
when the adversary can query the FVS with a set of images and
record all the returned scores. Essentially, face embedding “com-
presses” an image to a vector in a much smaller latent space (e.g.,
128 or 512 dimensions for Facenet). The mapping is deterministic
and the entropy is significantly reduced, as such the embedding is
much easier to recover than its source image.
3.2 Reducing Query Number
Though effective, running EmbRev can be costly as n queries are
required. Under certain scenarios like self-service FVS at border, ob-
taining hundreds of distances might be impossible for the adversary.
On the other hand, we found reducing the dimension of embedding
does not have big impact on the embedding model. Therefore, the
adversary can reconstruct an embedding with smaller dimensions
but still pass face verification.
Impact of embedding dimension. Firstly, we carefully reviewed
the Facenet embedding scheme [52]. It turns out when increasing
the dimension from 64 to 128, under L2 distance, Facenet only gains
1 percent higher accuracy (86.8% vs 87.9%, shown in Table 5 of
[52]). Interestingly, when the dimension is increased to 256 and
512, the accuracy degrades (87.7% and 85.6%). The result indicates
small dimension volume like 64 can accommodate most of the key
information of a face image. In fact, one possible explanation about
the accuracy plateau or decline is the use of dropout [56] when
training the embedding models. To avoid over-fitting, developers
intentionally shut off some neurons during a training iteration,
which pushes different neurons to generate similar output and
introduces high information redundancy to a layer’s output.
To further understand how information is stored in the embed-
ding, we generate Facenet-128 embeddings for 400 randomly se-
lected images from the LFW (Labelled Faces in the Wild) dataset [35]
3d(p, q) = 1 −
, where p and q are two vectors of n elements.
i =1 pi qi
n
(cid:113)n
i =1 p2
i
(cid:113)n
i =1 q2
i
20304050Rank00.20.40.60.8Error Dist.MaxMean21ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Mingtian Tan, Zhe Zhou and Zhou Li
⊺
−b ±
)
(11)
s
′ = (cid:174)em
s
· Σm · V
−1 · (D + ∆ +
learnt, we compute (cid:174)es
m · Vm is the
rank of any matrix in Equation 10 cannot exceed m (V
m-by-m identity matrix and A·Vm is also m-by-m). Therefore, there
are at most two roots, represented as Equation 11.
√
b2 − 4ac
(cid:174)em
s = (Vm · Σm)−1 · −A
2a
where a, b, c are defined the same as Equation 8.
With (cid:174)em
⊺
m (δ and ∆ are ne-
glected). Comparing to computing Equation 8, the only extra effort
the attacker has to make is applying pseudo-inverse operation [62]
on A to get A−1, as A is not square. SVD is only implicitly used
because Vm and Σm are eliminated when computing (cid:174)es
′
For the later stage of face recovery, the slightly imprecise (cid:174)es
will be used as the input. Fortunately, if the compression error is
negligible, we found the accuracy of the later stage is still high. As
shown in Section 5.1, for the 128-dimensional Facenet model, with
60 queries, an attacker can recover the embedding of a victim with
negligible error, which can further produce a clear face image that
is similar to the version with 128 queries.
′ can be generated similarly
When Cosine distance is used, (cid:174)es
under Equation 9. We skip the details.
′.
3.3 EmbRev under No-box Setting
Under this setting, neither f nor its embeddings are known to the
adversary, so Equation 8 and 11 cannot be solved. Yet, such limita-
tion can be addressed through attacking another embedding model
f ′ . Assume f ′, f ∈ F , which is the function space of embedding
models, and the accuracy of f ′ and f are similar. For images x and
y drawn from the data distribution pdata, f ′ and f should derive the
similar distance between any pair with high probability. In other
words, −ϵ < Ex,y∼pdata[|| f (x) − f (y))|| − || f ′(x) − f ′(y))||] < ϵ,
where ϵ is a small positive number. When the adversary uses her
own f ′ to calculate (cid:174)e1, (cid:174)e2, ..., (cid:174)em, the roots for (cid:174)es
s will be
similar with f ′(x) instead of f (x). When attacking real-world FVS,
the adversary can fine-tune f ′ with the displayed similarity scores.
To notice, f and f ′ do not need to have the same dimension number
n or even the same distance metric.
′ or (cid:174)em
4 IMAGE RECOVERY
This module (called ImgRev by us) reconstructs victim’s image
from the inferred victim’s embedding under the design of GAN,
which has been overviewed in Section 2.3. Figure 4 shows the
framework of ImgRev and it mainly consists of a novel embedding-
to-image generator, a discriminator and loss functions.
Overview. ImgRev has a prominent difference comparing to ex-
isting GAN research in that we use the embeddings instead of
randomly generated noises as the generator’s input, and we
call this method embedding-reverse GAN (or erGAN). Before train-
ing, a set of realistic face images (x ∼ pr(x) where pr is the data
distribution over real samples) need to be collected to produce the
embeddings (e = f (x) where f is the embedding model) for erGAN.
As our evaluation shown in Section 5, using a public face dataset,
like CelebA [38], is sufficient. The generator reconstructs images
(xд = G(e), where G is the generator) from the input embeddings e.
Three kinds of loss will be used to direct the update of the G, which
Figure 4: Overview of ImgRev. Training images (x) are firstly
converted by the embedding model (f ) into embeddings e.
The embeddings will be used by the generator to reconstruct
images (xд). The images will be used to compute losses (Lr ,
Ld and Le) and update the generator.
are 1) the recovery loss (Lr ) that measures the recovery error at
pixel level on the image plane; 2) the embedding loss that measures
the recovery error on the embedding plane; 3) the discriminator
loss that is inherited from the standard GAN, which measures if
the distribution of xд falls into the distribution of pr .
We follow the regular GAN training process [22], i.e., training
generator and discriminator in turns. The learning rate is decayed
0.02 for every epoch. The batch size is set to be 64. We train the
generator 5 times after every single discriminator training iteration,
which achieves good balance between the generator and the dis-
criminator. After training, the generator of erGAN will be employed
for image recovery and the discriminator will no longer be used.
To notice, in this stage, the adversary does not query the FVS under
any setting (whitebox, blackbox and no-box setting).
4.1 Generator
Ordinary GAN has generalization capability over noise field. It can
generate realistic image but it has no control over image attributes.
However, in our setting, we need the generated images to be tied to
their input embeddings. Conditional GAN (cGAN) [20] has gener-
alization capability over the noise field under the constraint of the
label. If we regard embeddings as labels, cGAN indeed can make
output images corresponding to embeddings. However, cGAN has
no generality on the label, meaning that it can only generate im-
ages with seen labels, which does not satisfy our requirement. In
contrast to the ordinary GAN and cGAN, erGAN has generaliza-
tion capabilities even over unseen embeddings, i.e., the embeddings
recovered by EmbRev. Figure 5 illustrates the differences between
different GAN methods at high level.
The generator of our erGAN has a multi-path phase and a single-
path phase. Figure 6 shows the workflow of our generator for 512-
dimensional embedding input. The first phase, i.e., multi-path phase,
extracts information from the input embedding at different paces.
For the 512-dimensional embedding, the rapid branch directly de-
convolutes the embedding from 512 dimensions to 512 10*10 tiny
images. In contrast, the mild branch firstly deconvolutes it into tiny
images of 2*2 then 10*10. These branches are combined together
after they reach the same size, providing a unified input for the later
phase. The second phase, i.e.single-path phase, generates gradually
GeneratorexgDiscriminatorxf/QueryLrf/f egLeLd22The Many-faced God: Attacking Face Verification System with Embedding and Image Recovery
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
(cid:104)(cid:0)∥∇ ˆx D(ˆx)∥2 − 1(cid:1)2(cid:105)
(13)
Ld = E
xд∼pд
[D(xд)]− E
x∼pr
[D(x)]+λ E
ˆx∼p ˆx
This loss tries to measure the difference between two distribu-
tions pд and pr , i.e., the generated images and the real images in
our case. D is the discriminator. p ˆx is the distribution of points
uniformly sampled from the straight line between pд and pr and
ˆx = ϵx + (1 − ϵ)xд.
Recovery loss (Lr ). To encourage the generator to produce realis-
tic images, we add a loss item Lr to force the generated image xд
similar to the original image x. The loss penalizes the generator ac-
cording to the pixel value difference between x and xд. Equation 14
shows Lr .
Lr =
E
x∼pr ,xд∼pд
[||x − xд||1]
(14)
Here we use L1 distance (or Manhattan Distance) to measure the
loss instead of L2 distance because we found L2 is more sensitive to
the background part of images. In fact, two profile images usually
differ more in background part than that in the face part. When
calculating L2 distance, the larger difference, i.e.the background
part takes dominant weight, as it is squared. To encourage the
generator to focus on the face part, we choose L1 distance.
Embedding loss (Le). We send the generated image (xд) to a face
embedding model (f ) to get its embedding (eд) and use the embed-
ding loss (Le) to penalize the difference between eд and the original
embedding e, as shown by Equation 15.
Le =
E
x∼pr ,xд∼pд
[|| f (x) − f (xд)||]
(15)
To be noticed is that f of FVS is unavailable to the black-box
adversary. For white-box adversary, f is identical to the one used
by FVS. For no-box adversary, another embedding model f ′ is used
as an alternative of f of FVS, which is explained in Section 3.3.
However, for black-box adversary, only the result of f is known by
the adversary and we discuss this scenario in the next subsection.
4.4 ImgRev Under Black-box Setting
In this setting, though Le can be calculated, ∇Le (∇ is derivative)
is unknown as we have no access to f , which prevents erGAN to
be guided by a face embedding model. Although erGAN can still
be trained without Le, i.e., setting Le to zero, she would get poorer
results because of the missing guidance from a face embedding
model. To address this issue, she can use another open-source
model f ′ with similar accuracy as f to obtain ∇Le, even when
f ′ and f may have different model structure, distance metrics, etc.
In fact, as open-source models have achieved quite high accuracy,
their embeddings can tell the distinction between profile images
well. They can be used to push the generator to generate more
similar images. In other words, || f ′(x1) − f ′(x2)|| is expected to
be positively correlated with || f (x1) − f (x2)||, where x1, x2 ∈ X.
Therefore, decreasing || f ′(x1)− f ′(x2)|| will result in the decrease of
|| f (x1)− f (x2)||. Therefore, f can be replaced by f ′ in Equation 15.
To notice, this setting is different from using open-source models
for white-box adversary or no-box setting as the the goal of f ′ here
is to output ∇Le.
Figure 5: Comparison of Conditional GAN generator, GAN
generator and erGAN.
clearer and larger images by concentrating channels. It repeatedly
passes the deconvolution unit which enlarges the generated images
by fusing multiple channels. During this stage, the size of the im-
ages is doubled while the channels are halved. The deconvolution
unit is followed by a residual convolution unit [25] (see Figure 7)
in order to rectify the images without changing the image size.
4.2 Discriminator
The discriminator tries to distinguish the generated images with the
real face images to help the generator improve image quality. Our
discriminator follows the design of the one used by WGAN-GP [23]
(Wasserstein GAN with Gradient Penalty), which addresses the
issue of training instability of GAN while producing high-quality
images. WGAN-GP needs to maintain a Lipschitz function [48] to
calculate the Wasserstein distance. It penalizes gradient for every