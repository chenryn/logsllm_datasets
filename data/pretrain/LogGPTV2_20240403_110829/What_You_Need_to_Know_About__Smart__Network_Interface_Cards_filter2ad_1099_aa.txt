# What You Need to Know About (Smart) Network Interface Cards

## Authors
- Georgios P. Katsikas
- Tom Barbette
- Marco Chiesa
- Dejan Kostic
- Gerald Q. Maguire Jr.

## Affiliation
KTH Royal Institute of Technology, Stockholm, Sweden  
Emails: {katsikas, barbette, mchiesa, dmk, maguire}@kth.se

## Abstract
Network Interface Cards (NICs) are essential components in modern high-speed networked systems, supporting multi-100 Gbps speeds and increasing programmability. Offloading computation from a server's CPU to a NIC can free up substantial CPU resources, making NICs crucial for competitive cloud services. Understanding the performance benefits and limitations of offloading networking applications to NICs is therefore critical. In this paper, we measure the performance of four different NICs from one of the largest NIC vendors worldwide, supporting 100 Gbps and 200 Gbps. We demonstrate that while today's NICs can easily handle multi-hundred-gigabit throughputs, frequent updates to a NIC's packet classifier—as required by network address translators (NATs) and load balancers—can result in a significant throughput reduction of up to 70 Gbps or even complete denial of service. Our findings indicate that all tested NICs struggle to support high-speed networking applications that require tracking a large number of frequently arriving incoming connections. Additionally, we uncover various counter-intuitive performance artifacts, including the impact of using multiple tables for flow classification.

**Keywords:** Network Interface Cards, Hardware Classifier, Offloading, Rule Operations, Performance, Benchmarking, 100 GbE

## 1 Introduction
The dramatic growth in Network Interface Card (NIC) speeds necessitates the optimization of I/O operations to support modern applications. Recent studies show that handling 40 Gbps of Transmission Control Protocol (TCP) traffic requires 20%–60% of the CPU resources on a general-purpose server [10,31,48]. This communication overhead consumes CPU cycles that could otherwise be used for running customer applications, leading to expensive deployments for network operators.

Offloading network operations to NICs is a practical approach to partially relieve CPUs from managing network-related state. Examples include TCP optimizations like Large Receive Offload (LRO) and TCP Segmentation Offload (TSO) [1]. Increasingly, NICs are equipped with built-in Field-Programmable Gate Arrays (FPGAs) or network processor cores, enabling them to offload computation directly from the host's CPU. These enhanced NICs are referred to as SmartNICs. Several preliminary investigations have shown potential benefits for offloading networking stacks [2,10,30–32], network functions [3,4,18,25,43], key-value stores [7,26,28], packet schedulers [44], neural networks [42], and more [21,38]. Despite the growing relevance of (smart) NICs, few studies have focused on dissecting their performance, comparing them with their predecessors, and providing guidelines for deploying NIC-offloaded applications, particularly in the context of packet classification.

### Our Goal
In this work, we study the performance of (smart) NICs for widely deployed packet classification operations. A key challenge in packet classification is the ability of the classifier to quickly match incoming packets to their processing actions and adapt its state, such as by inserting new rules or updating existing ones. For example, a cloud load balancer (LB) might use a NIC's packet classifier to map TCP/IP 5-tuples of incoming connection identifiers to their corresponding servers. Given that a single cluster in a large-scale datacenter may receive over 10 million new connections per second [29], it is crucial to support fast updates for packet classifiers to achieve high throughput and low, predictable latency. Our study reveals unexpected performance bottlenecks in today's (smart) NICs and provides guidelines for researchers and practitioners who wish to offload dynamic packet classifiers to (smart) NICs.

### Findings
We analyzed the performance of four different NICs with speeds ranging from 100 Gbps to 200 Gbps. Our key findings, summarized in Table 1, show that the forwarding throughput of the tested NICs sharply degrades when the forwarding plane is updated and when packets match multiple forwarding tables in the NIC. We also developed an efficient in-memory update mechanism to mitigate the impact of rule updates on forwarding throughput. The code for reproducing our experiments and supplementary graphs are publicly available [17].

### Paper Outline
This paper is organized as follows:
- **Section 2:** Outlines the experimental methodology.
- **Section 3:** Provides performance insights into modern NICs.
- **Section 4:** Discusses related efforts in programmable networking hardware.
- **Section 5:** Concludes the paper.

## 2 Measurement Methodology
This section details the testbed used for the experiments and our methodology for extracting results.

### 2.1 Experimental Setup

#### Testbed
All experiments were conducted using the testbed shown in Fig. 1. The setup consists of two back-to-back interconnected servers, each equipped with a dual-socket 16-core Intel Xeon Gold 6134 (SkyLake) CPU clocked at 3.2 GHz and 256 GiB of DDR4 RAM clocked at 2666 MHz. Each core has 2× 32 KiB L1 (instruction and data caches) and a 1 MiB L2 cache, with a 25 MiB Last Level Cache (LLC) shared among the cores in each socket. Hyper-threading is disabled, and the operating system is Ubuntu 18.04.5 with Linux kernel v4.15. One server acts as the traffic generator and receiver, while the other is the Device Under Test (DUT).

![Testbed Setup and Measurement Methodology](fig1.png)

#### Tested NICs
We focus on one of the most widespread NICs available in Commercial off-the-shelf (COTS) hardware, as shown in Table 2. These NICs, manufactured by NVIDIA Mellanox, operate at 100 Gbps link speeds (or beyond) and provide advanced processing capabilities. We also considered existing Intel NICs, such as the 10 GbE 82599 [12] and the 40 GbE XL710 [13], but these operate at much lower speeds and are limited to 8 K flow rules. The upcoming 100 GbE Intel E810 series network adapter [14] provides 16 K (masked) filters based on ternary content addressable memory (TCAM), which is still far from the range of several million flow rules tested with the NVIDIA Mellanox NICs. NVIDIA Mellanox has not disclosed detailed hardware information; thus, our study sheds light on unknown aspects of these popular NICs and helps understand performance evolution within the same family.

| Vendor | Model | Speed (Gbps) | # of Ports | Firmware Version | Driver |
|--------|-------|--------------|------------|------------------|--------|
| NVIDIA Mellanox | ConnectX-6 | 100/200 | 2 | 16.00.2000 | mlx5_core |

## 3 Performance Insights into Modern NICs
This section provides detailed performance insights into the tested NICs, highlighting the implications of our findings.

### 3.1 Throughput and Latency Analysis
Our analysis shows that certain parts of the NIC table hierarchy do not yield the expected forwarding performance. For example, throughput can degrade from 100 Gbps to 20 Mbps, and latency can increase significantly (Fig. 2a and Fig. 2c). Uniformly spreading rules across a chain of NIC tables incurs a performance penalty, and a batch update of the NIC classifier while processing traffic can make the NIC temporarily unavailable. Frequent updates to the NIC classifier while processing traffic cause substantial performance degradation, reducing throughput from 100 Gbps to 30 Gbps and increasing latency by approximately 2x (Fig. 4). However, updating the NIC classifier from a separate core does not degrade NIC performance (Fig. 3 and Fig. 4).

### 3.2 Rule Installation and Update Performance
The Internet protocol selection (IPv4 vs. IPv6) affects the NIC rule installation rate. For example, IPv6 rule insertion rates can be 5–181 times faster or 12% slower than IPv4, depending on the part of the NIC table hierarchy (Fig. 5a–5b). The network slicing protocol selection also affects the NIC rule installation rate, with VLAN-based rules being up to 50% faster than tunnel-based rules (Fig. 5c). NIC rule update operations are non-atomic and rely on sequential addition and deletion, which is too slow for applications requiring heavy updates. Our dedicated update API performs up to 80% faster (Fig. 6).

## 4 Related Efforts in Programmable Networking Hardware
This section discusses related efforts in the area of programmable networking hardware, providing context for our work.

## 5 Conclusion
In conclusion, our study highlights the performance challenges and limitations of (smart) NICs in handling dynamic packet classification tasks. While today's NICs can support high-throughput scenarios, frequent updates to the packet classifier can lead to significant performance degradation. We provide guidelines and an efficient in-memory update mechanism to mitigate these issues, contributing to the better understanding and deployment of NIC-offloaded applications.

---

**Note:** Figures and tables referenced in the text should be included in the final document for clarity and completeness.