title:What You Need to Know About (Smart) Network Interface Cards
author:Georgios P. Katsikas and
Tom Barbette and
Marco Chiesa and
Dejan Kostic and
Gerald Q. Maguire Jr.
What You Need to Know About (Smart)
Network Interface Cards
Georgios P. Katsikas
, Tom Barbette(B)
and Gerald Q. Maguire Jr.
, Marco Chiesa , Dejan Kosti´c
,
KTH Royal Institute of Technology, Stockholm, Sweden
{katsikas,barbette,mchiesa,dmk,maguire}@kth.se
Abstract. Network interface cards (NICs) are fundamental components
of modern high-speed networked systems, supporting multi-100 Gbps
speeds and increasing programmability. Oﬄoading computation from a
server’s CPU to a NIC frees a substantial amount of the server’s CPU
resources, making NICs key to oﬀer competitive cloud services. There-
fore, understanding the performance beneﬁts and limitations of oﬄoad-
ing a networking application to a NIC is of paramount importance.
In this paper, we measure the performance of four diﬀerent NICs
from one of the largest NIC vendors worldwide, supporting 100 Gbps and
200 Gbps. We show that while today’s NICs can easily support multi-
hundred-gigabit throughputs, performing frequent update operations of
a NIC’s packet classiﬁer—as network address translators (NATs) and
load balancers would do for each incoming connection—results in a dra-
matic throughput reduction of up to 70 Gbps or complete denial of service.
Our conclusion is that all tested NICs cannot support high-speed net-
working applications that require keeping track of a large number of
frequently arriving incoming connections. Furthermore, we show a vari-
ety of counter-intuitive performance artefacts including the performance
impact of using multiple tables to classify ﬂows of packets.
Keywords: Network interface cards · Hardware classiﬁer ·
Oﬄoading · Rule operations · Performance · Benchmarking · 100 GbE
1 Introduction
With the dramatic growth of Network Interface Card (NIC) speeds, optimizing
I/O operations is essential for supporting modern-day applications. As evidenced
by recent work, handling 40 Gbps of Transmission Control Protocol (TCP)
traﬃc requires roughly 20%–60% of the CPU resources on a general-purpose
server [10,31,48]. These communication overheads consume CPU cycles that
could otherwise be used to run customers’ applications, ultimately resulting in
expensive deployments for network operators.
Oﬄoading network operations to NICs is a pragmatic way to partially
relieve CPUs from the burden of managing (some of the) network-related state.
c(cid:2) Springer Nature Switzerland AG 2021
O. Hohlfeld et al. (Eds.): PAM 2021, LNCS 12671, pp. 319–336, 2021.
https://doi.org/10.1007/978-3-030-72582-2_19
320
G. P. Katsikas et al.
Examples of such oﬄoading are TCP optimizations, such as Large Receive
Oﬄoad (LRO) and TCP Segmentation Oﬄoad (TSO) [1]. Increasingly, NICs
are equipped with built-in Field-Programmable Gate Arrays (FPGAs) or net-
work processor cores that can be used to oﬄoad computation from a host’s
CPU directly into the NICs. Such NICs are referred to as SmartNICs. Sev-
eral preliminary investigations of SmartNIC technologies have demonstrated
potential beneﬁts for oﬄoading networking stacks [2,10,30–32], network func-
tions [3,4,18,25,43], key-value stores[7,26,28], packet schedulers [44], neural net-
works [42], and beyond [21,38]. Despite the increasing relevance of (smart) NICs
in today’s systems, very few studies have focused on dissecting the performance
of SmartNICs, comparing them with their predecessors, and providing guidelines
for deploying NIC-oﬄoaded applications, with a focus on packet classiﬁcation.
Our Goal. In this work, we study the performance of (smart) NICs for widely
deployed packet classiﬁcation operations. A key challenge of packet classiﬁcation
is the ability of the classiﬁer to both quickly (i) match incoming packets to their
packet processing actions and (ii) adapt the state of the packet classiﬁer, e.g., by
inserting new rules or updating existing ones. For example, consider a cloud load
balancer (LB) that keeps track of the mapping between incoming connections
and the back-end servers handling these connections. The LB may utilize a NIC’s
packet classiﬁer to map TCP/IP 5-tuples of incoming connection identiﬁers to
their corresponding servers. As a single cluster in a large-scale datacenter may
receive over 10 million new connections per second [29], it is critical to support
fast updates for packet classiﬁers, thus achieving high throughput and low pre-
dictable latency. Our study of packet classiﬁers reveals unexpected performance
bottlenecks in today’s (smart) NICs and provides guidelines for researchers and
practitioners, who wish to oﬄoad dynamic packet classiﬁers to (smart) NICs.
Findings. We analyzed the performance of four diﬀerent NICs with speeds in
the 100 Gbps to 200 Gbps range. Our key ﬁndings are summarized in Table 1.
In short, we show that the forwarding throughput of the tested NICs sharply
degrades when i) the forwarding plane is updated and ii) packets match multiple
forwarding tables in the NIC. Moreover, we devise an eﬃcient in-memory update
mechanism that mitigates the impact of updating the rules on the forwarding
throughput. The code to reproduce the experiments of this paper is publicly
available along with supplementary graphs showing the experimental evaluation
of all four NICs under test [17].
Paper Outline. This paper is organized as follows: Sect. 2 outlines the exper-
imental methodology used in this work; Sect. 3 provides useful performance
insights into modern NICs; Sect. 4 discusses related eﬀorts in the area of pro-
grammable networking hardware beyond the work mentioned inline throughout
the paper. Finally, Sect. 5 concludes this paper.
2 Measurement Methodology
This section outlines the testbed used to conduct the experiments as well as our
methodology to extract results.
What You Need to Know About (Smart) Network Interface Cards
321
Table 1. Main ﬁndings of this paper.
Finding
Implication
There are parts of the NIC table hierarchy that
do not yield the expected forwarding
performance (Sect. 3.1)
Throughput degradation from 100 Gbps to
20 Mbps and multi-fold latency increase (Fig. 2a
and Fig. 2c)
Uniformly spreading rules across a chain of NIC
tables incurs performance penalty (Sect. 3.1)
A batch update of the NIC classiﬁer, while
processing traﬃc, makes the NIC temporarily
unavailable (Sect. 3.1)
Frequent updates of the NIC classiﬁer, while
processing traﬃc, causes substantial
performance degradation (Sect. 3.1)
Updating the NIC classiﬁer from a separate core
does not degrade the NIC performance (Sect. 3)
The Internet protocol selection (i.e., IPv4 vs.
IPv6) aﬀects the NIC rule installation rate
(Sect. 3.2.1)
Throughput degradation from 100 Gbps to
13 Gbps and 10x higher latency when using 16
tables (Fig. 2b and Fig. 2d)
100% packet loss for up to several seconds with
an increasing number of installed rules (Fig. 3)
Throughput degradation from 100 Gbps to
30 Gbps and ∼2x higher latency (Fig. 4)
No performance impact when processing traﬃc
on core 0 and updating rules from core 1 (Fig. 3
and Fig. 4)
IPv6 rule insertion rate is either 5–181 faster or
12% slower than the respective IPv4 rate,
depending on the part of the NIC table
hierarchy applied (Fig. 5a–5b)
The network slicing protocol selection aﬀects
the NIC rule installation rate (Sect. 3.2.1)
Installing VLAN-based rules is up to 50% faster
than installing tunnel-based rules (Fig. 5c)
NIC rule update operations are non-atomic and
rely on sequential addition and deletion
(Sect. 3.2.2)
Too slow for applications that require heavy
updates. Our dedicated update API performs
up to 80% faster (Fig. 6)
2.1 Experimental Setup
Testbed. All of the experiments described in this paper used the testbed shown
in Fig. 1. Two back-to-back interconnected servers, each with a dual-socket 16-
core Intel(cid:2)Xeon(cid:2) Gold 6134 (SkyLake) CPU clocked at 3.2 GHz and 256 GiB of
DDR4 Random Access Memory (RAM) clocked at 2666 MHz. Each core has 2×
32 KiB L1 (instruction and data caches) and a 1 MiB L2 cache, while one 25 MiB
Last Level Cache (LLC) is shared among the cores in each socket. Following
today’s best practices, hyper-threading is disabled on all servers [47] and the
Operating System (OS) is the Ubuntu 18.04.5 distribution with Linux kernel
v4.15. One server acts as a traﬃc generator and receiver while the other server
is the Device Under Test (DUT).
Measurement Server
Trace 
Generator
Analysis
Traffic Generator
Core 0
Traffic Receiver
Core 1
Physical link
Device Under Test
Rule Generator
Core 0/1
Forwarding NF
Core 0
Fig. 1. Testbed setup and measurement methodology.
322
G. P. Katsikas et al.
Tested NICs. We focus our study on one of the most widespread NICs available
in Commercial oﬀ-the-shelf (COTS) hardware to date, as shown in Table 2. Such
NICs, manufactured by NVIDIA Mellanox, operate at 100 Gbps link speeds (or
beyond), while providing advanced processing capabilities. We also considered
existing Intel NICs, such as the 10 GbE 82599 [12] and the 40 GbE XL710 [13],
however these NICs operate at much lower link speeds and are limited to 8 K
ﬂow rules. The upcoming 100 GbE Intel E810 series network adapter [14] provides
16 K (masked) ﬁlters based on ternary content addressable memory (TCAM),
which is still far from the range of several millions of ﬂow rules tested with
the NVIDIA Mellanox NICs. Moreover, the hardware limits of the Intel NICs
are known, as Intel published relevant hardware datasheets [12–14]. NVIDIA
Mellanox has not disclosed such information; thus our study sheds some light
on unknown aspects of these popular NICs, while helping to understand how
performance has evolved across the same family of NICs.
Table 2. The characteristics of the NICs used for the experiments in this paper.
Vendor
Model
Speed
(Gbps)
# of
Ports
Firmware
Version
Driver