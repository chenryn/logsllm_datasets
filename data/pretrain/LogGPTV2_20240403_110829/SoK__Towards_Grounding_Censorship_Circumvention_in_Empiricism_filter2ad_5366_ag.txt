a
l
e
R
s
c
i
r
t
e
M
d
n
a
s
l
a
o
G
r
e
h
t
i
e
t
n
e
m
u
c
o
d
e
h
t
t
a
h
t
s
e
t
o
n
e
d
e
s
a
c
w
o
n
k
.
h
c
a
o
r
p
p
a
e
h
t
t
r
o
p
p
u
s
o
t
y
t
i
l
a
n
o
i
t
c
n
u
f
d
e
d
e
e
n
e
h
t
n
o
i
t
n
e
m
t
o
n
d
i
d
h
t
i
w
d
l
i
w
e
h
t
n
i
t
s
i
x
e
s
r
e
v
r
e
s
y
n
a
m
w
o
h
n
o
i
t
a
t
n
e
m
u
c
o
d
n
o
i
t
a
u
l
a
v
e
o
t
d
e
n
i
m
a
x
e
e
h
t
f
i
s
r
e
f
e
r
s
r
e
v
r
e
s
.
e
m
i
t
r
e
v
o
s
u
o
i
c
i
p
s
u
s
r
a
e
p
p
a
y
a
m
s
t
s
e
u
q
e
r
t
e
m
n
u
h
c
u
s
;
g
n
i
l
i
a
f
h
c
a
o
r
p
p
a
e
h
t
o
d
s
e
s
n
o
p
s
e
r
e
v
i
e
c
e
r
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:11:18 UTC from IEEE Xplore.  Restrictions apply. 
points. However, the table caption explains a few opaque ones
and the interested reader can ﬁnd the other deﬁnitions in the
appendix. Below, we provide some observations about the
evaluations we have summarized.
A. General Observations
The provided tables make a few general observations clear.
First, publications tend to have more evaluation goals and
metrics than deployed tools.
Second, evaluations share many of the same goals, but also
diﬀer greatly from work to work in terms of metrics used or
even goals mentioned. In some cases, diﬀering expectations
about the targeted users’ needs and diﬀering ideas about the
importance of criteria could justify these diﬀerences in goals
and metrics. However, evaluations would ideally make such
tradeoﬀs explicit by mentioning unmet goals, which would
make comparing approaches easier. Furthermore, we suspect
that often developers only mention criteria on which they
expect their approach to perform well, given the small number
of unchecked boxes for metrics in Table IV.
Recommendation 1. The needs and censorship context of the
intended users—not the capabilities of an approach—should
govern the evaluation of the approach.
Third, there is less agreement on what metrics to use or
even which metrics map to which goals, making comparison
even more diﬃcult.
Fourth, no metric in use comprehensively evaluates unde-
ployed approaches. The number of users, a reasonable holistic
metric of success in use, cannot evaluate approaches for
deployment since the approach must already be deployed to
calculate it.
Research Gap 2. Prior evaluations lack holistic metrics for
undeployed approaches.
B. Criteria Related to Attacks
their evaluations,
Evaluations of approaches focus particularly (nearly 2/3s)
on criteria related to attacks. Our survey of academic papers
found that they are typically motivated by overcoming some
real attacks on circumvention approaches. However, looking
at
they tend to focus on more complex
hypothetical attacks, per our discussion in Section VII.
Evaluation by Techniques Used. Even without such empirical
evidence, a noteworthy pattern emerges in Table IV:
the
large number of metrics starting with the word “Use”. These
correspond to binary metrics measuring whether an approach
employed a technique, such as authentication or encryption, to
avoid a type of exploit, rather than on considerations regarding
the censor’s capabilities. For example, one such metric is
Use popular hosts. This metric suggests a diﬀerent metric
about a property of the censor: Blocking requires disrupting a
popular host. This property does not presuppose any mech-
anism for achieving it, making cross-approach comparison
easier. Furthermore, it considers all the vulnerabilities of the
approach rather than focusing on a single technique. For
example, a system using many hosts might still be blockable
without disrupting a popular host if the circumventing traﬃc
can be identiﬁed and dropped. Such censor-oriented versions
of metrics push developers to fully consider the space of
concerns.
Research Gap 3. Approaches should be evaluated based on
the properties they provide rather than the techniques they use.
C. Other Criteria
While we focus in this paper on evaluation related to
censorship attacks, some aspects of other criteria (detailed in
Table VII in the appendix) merit discussion.
In some approaches, advocates consume
Ability to Deploy.
resources to maintain a forwarder (e.g., Psiphon [41]). In
others, they pay for others to maintain it (e.g., Meek [26]
and CloudTransport [36]). In others, they rely upon volunteers
In rare cases,
(e.g., Infranet [4] and Flash Proxy [17]).
the forwarder might be found and free, but
the advocate
must still maintain an approach for making use of it (e.g.,
CacheBrowser [50]).
In all these cases, the advocate faces costs and inconve-
niences. While some commercial services explain their fees
providing some light on their costs, few of the documents we
examined discuss cost to advocate to maintain and run the
system.
Research Gap 4. Only 6 of 33 papers mention the costs for
advocates maintaining the system.
This observation reﬂects the nearly absent goals of Client
performance, Infrastructure cost, and Sustainable network
and development. The exceptions include the CloudTransport,
which uses cloud services as a rendezvous point to transfer
data between a client and a server [36], which reports the
cost of browsing a webpage in USD; and Meek, which uses
CDNs and App Engine, and reports the total cost of running
the system from early 2014 until April 2015 [26].
The related criterion Preponderance of suitable servers
measures the number of potential forwarders, and appeared
in several works [12, 16, 26, 27, 35]. Some approaches
may require particular functionality from servers that act as
forwarders, such as speciﬁc implementation quirks. While this
criterion may appear unsuitable for undeployed approaches,
such as the number of users or forwarders, it diﬀers by repre-
senting an upper bound on the number of forwarders possible
that can be determined without deploying the approach.
For usability, previous research and deployed
Usability.
approaches have examined many metrics, such as cost
to
user, ease of setup, and usage ﬂexibility. Cost-to-user looks at
whether users must pay to use a deployment of an approach.
Some tools like Psiphon [41], Ultrasurf [39], and Facet [34]
do not require any installation, which relieves censored users
from acquiring software, or require only a small download
(e.g., 4.3MB for lantern-installer-beta.dmg) [42].
Some approaches have restrictions in terms of functionality,
network, and system architecture. Facet only provides access
to videos [34]. CacheBrowser can only serve websites hosted
on a CDN [50]. Flash Proxy is incompatible with NAT [17].
923923
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:11:18 UTC from IEEE Xplore.  Restrictions apply. 
Ultrasurf only works on Windows [39]. Some tools are not ac-
cessible from certain countries. For example, in 2010 FreeGate
and Ultrasurf outright blocked connections from all but the few
countries that they cared to serve (China and, for Ultrasurf,
Iran) [56].
Deployed tools are more likely to evaluate usability than
are undeployed research proposals. General circumvention
evaluation papers, not tied to any single approach, were the
most comprehensive in their consideration of usability.
Research Gap 5. Only 9 of 33 research papers mentioned the
goal of usability, and none assessed it with metrics involving
actual users.
VII. Comparing Resistance Criteria to Real Censors
We now take a closer look at criteria assessing an approach’s
resistance to a censor attempting to block it. We wish to
classify the vulnerabilities and exploits found in both real
attacks and papers to better understand how they relate and
what censors are doing in practice.
Unfortunately, we have limited information about the ex-
ploits used by real censors, forcing us to make tentative
classiﬁcations when required details are missing.
Similarly, research papers often point to vulnerabilities (or
partially speciﬁed exploits) rather than to concrete exploits.
When a classiﬁcation depends upon the implementation details
of an exploit, we infer a reasonable exploit from the vul-
nerability (and any partial speciﬁcation provided) to classify
the vulnerability. Such classiﬁcations must be understood as
predicated upon the exploit we inferred; future work might
ﬁnd more clever exploits for seemly expensive vulnerabilities.
Recommendation 2. Papers identifying weaknesses in other
approaches should provide not just vulnerabilities but speciﬁc
exploits to illustrate the practicality of exploiting the vulnera-
bility.
In some cases more than one obvious exploit might exist,
in particular, one for a censor preferring overblocking and one
for a censor preferring underblocking. We use the one with a
preference toward underblocking due to the sensitivities of the
censors who primarily concern us.
Table V shows our classiﬁcation of real-world censorship
vulnerabilities (or, rather the most natural exploit for a vul-
nerability) under three criteria: the phase of the circumvention
approach in which the vulnerability appears; whether the
inferred exploit is passive, reactive, or proactive; and how
robust the feature is to lost packets.
Table VI shows the same classiﬁcation for tests of vulnera-
bilities found in the academic papers of Table IV, along with
two additional papers. The additional papers focus on traﬃc
analysis and not full approaches, but introduce a large number
of additional criteria related to traﬃc analysis [1, 76]. While
Table VI shows vulnerabilities at more detail than the metrics
in Table IV, some rows correspond to a combination of similar
vulnerabilities.2
2Furthermore, we did not add reactive exploits for which a closely related
proactive attack exists, since the proactive exploits can be converted into
reactive ones by waiting for the probe to arise from user traﬃc.
924924
Before we explain our classiﬁcations in detail and the
implications of how the two tables diﬀer, we note a general
diﬀerence. Many of the tests found in papers (Table VI) test
whether the circumvention approach looks like some cover
protocol, a generally allowed non-circumventing protocol that
the approach attempts to steganographically match. Most, but
not all, real-world exploits look for signs of circumvention
rather than checking whether the traﬃc matches some allowed
protocol.
A. Phase Exploited
First, we examine the phase exploited. These phases include
acquiring needed identiﬁers (IDM), channel setup, and channel
use. We also include a subsidiary phase, which refers to
how the approach behaves when not engaged in any of its
main phases, such as when contacted by a non-user with a
malformed packet. While such subsidiary behavior does not
play a role in facilitating the channels, a censor can ﬁngerprint
an approach by studying the subsidiary behavior it exhibits in
response to active probes.
Table V shows real-world censors exploiting features of
either the channel setup or the IDM. In some cases, when
exploiting the setup the censor looks for features inherent
to the setup, such as initialization parameters of a protocol
handshake. In other cases, the censor looks at features present
in all packets of a channel, such as the destination IP address,
but exploits the setup simply because it occurs ﬁrst. Either
way, the censor need not examine a connection for long. When
exploiting the IDM, the censor either harvests identiﬁers using
methods similar to how a real user would acquire them, or
attempts to make reaching the IDM impossible.
We conjecture that censors focus on channel setups and
IDMs not only to reduce how long it must watch a connection,
but also since such setup and IDM traﬃc shows little variation
across all the users of an approach, unlike channel usage,
which varies depending upon how each user uses the channel.
While the latter variation of usage could be handled using
statistics over long traces of usage traﬃc, such approaches
require that a censor retain more state, and impose tuning
issues for balancing false positives and negatives.
Recommendation 3. Circumventors should concern them-
selves more with vulnerabilities of the channel setup than of
the channel usage.
Table III shows that that deployed tools obey this recom-
mendation, numerous academic approaches do not.
Table VI shows that while papers also include numerous
attacks on the channel setup, they often deal with features
that require analyzing channel usage. We also ﬁnd many
exploits of subsidiary behaviors, which real attacks ignore.
This discrepancy appears to be a case of research running
ahead of practice. These attacks are from Houmansadr et al.,
who showed that mimicry-based approaches for looking like
allowed traﬃc is ﬁngerprintable by such subsidiary behavior
(as well as other means) [1]. While such approaches appear in
the research literature, they have had little impact in practice,
making the attacks unneeded by current censors.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:11:18 UTC from IEEE Xplore.  Restrictions apply. 
Description and where seen
Detect attempts to get instructions for using Tor
Check DNS requests for whether they are for Tor’s website [63, 94]
Check GET requests for Tor (‘/tor/’) [63, 115]
Check requests for Tor (‘.torproject.org’) [61]
Search for ‘Tor’ as a keyword, e.g., as a search term [120]
Detect identiﬁers needed to setup a Tor channel
Find IP addresses of Tor directory authorities [124]
Get Tor relays’ IP addresses from public list [62, 63]
Get Tor bridges’ IP addresses from webpage [62, 63]
Get Tor bridges’ IP addresses from email [63, 66]
Detect the Tor protocol
Identify SSL for Tor (method unknown) [63, 117]
DPI for Tor’s DH parameter in SSL [63, 116]
DPI for Tor’s SSL and TLS certiﬁcate lifetime [63, 118, 122]
DPI for Tor’s TLS renegotiation [63, 128]
DPI for TLS ‘Server Hello’ for cipher 0x0039 sent by the Tor relay or bridge [114, 125–127, 129]
DPI for Tor’s TLS ‘Client Hello’ for cipherlist [63, 67, 70, 82, 112, 113, 125]
DPI for TLS ‘Client Hello’ for SNI that resolves to Tor relay/bridge [119]
DPI on TLS for client key exchange [119]
Probe server for circumvention handshake (looking for what?) [70, 113]
Probe server for circumvention handshake looking for version cell [63, 67, 70, 82, 112, 113]
Observe suspected circumvention ﬂow (method unknown) [70, 113]
Detect the destination of packets
Check whether server IP is in blacklist [61–63, 66, 116, 128]
Check whether server IP-port pair is in blacklist [62, 63, 67, 70, 82, 112, 113, 120, 124]
Detect encryption
Identify SSL (method unknown) [63, 115]
Identify SSL handshake [120, 121]
Check for encryption (method unknown) [123]
Detect anything beyond basic web usage
Check whether port is not 80 or 443 [63]
Check whether port is not 80 (and client IP is on a graylist?) [63]
Check for port 80 and whether protocol is non-HTTP (method unknown) [123]
Phase
Nature
Meas. loss
Network loss
idm
idm
idm
idm
idm
idm
idm
idm
setup
setup
setup
setup
setup
setup
setup
setup
setup
setup
chan.?
passive
passive
passive
passive
proactive
proactive
proactive
proactive
passive
passive
passive
passive
passive
passive
passive
passive
proactive
proactive
passive
setup
setup
passive
passive
setup
setup
setup?
setup
setup
setup?
passive
passive
passive
passive
passive
passive
UB
UB
UB
UB
?
ub
ub
ub
?
UB
UB
UB
UB
UB
UB
UB
?
ub
?
UB
UB
?
?
?