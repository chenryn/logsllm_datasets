was never executed, what meant that we spent time dynami-
cally analyzing apps that did not actually request permissions
at runtime. Further, low code coverage of dynamic analysis
(e.g., through login-forms) is a known limitation of available
analysis tools, which prevented us from reaching all permis-
sion requests. Nevertheless, we collected an adequate number
of rationales that were used in the selection process of the
standardized rationale for the user study.
4.2 Findings
As the results are biased towards upfront permission requests,
we should consider them with reservation. Nevertheless, we
reveal different ways of showing rationales in terms of design,
quality, wording, and timing.
Timing and presence/absence of rationales. Of the 2,569
found permission requests, 70% were displayed upfront and
16% showed rationales that were evenly distributed among
upfront and in-context requests. The most frequently re-
quested permission was STORAGE (56% of 2,569) followed
by LOCATION (19%), CAMERA (9%), PHONE (6%), CONTACTS
(3%), and MICROPHONE (3%). We only found a small num-
ber of permission requests for SMS, CALENDAR, and PHONELOG,
which is consistent with prior work [3,36]. A chi-square test of
independence was performed to examine the relation between
timing and permission type. Due to too few observations
we excluded the permissions SMS, CALENDAR, and PHONELOG
from the analysis. We found that the proportion of in-context
permission requests signiﬁcantly differed between permis-
sions (X 2(5) = 49.562, p < 0.001, Cramer(cid:48)sV = 0.139). For
example, the highest proportion of in-context requests was
found for STORAGE (34%), closely followed by MICROPHONE
(33%), and CAMERA (32%). While the lowest proportion
was seen for LOCATION (23%) and PHONE (12%), which
are often associated with background functionalities and
are therefore most frequently requested upfront. Whereas,
there was no signiﬁcant association between permission
type and presence/absence of rationales (X 2(5) = 8.06, p =
0.153, Cramer(cid:48)sV = 0.056).
(a) Dialog
(b) Banner
(c) Fullscreen (d) Snackbar
Figure 3: The different rationale designs.
Design and wording of rationales. We found four gen-
eral design patterns for rationales. They were displayed as
either dialogs, fullscreen views, banners, or snackbars (as
highlighted in Figure 3). Each design pattern was shown be-
fore a permission request or after a permission denial, except
for snackbars which were only used after a permission was
denied. Additionally, each design provided rationales for one
or multiple permissions. We also noticed that most rationales
provided an acknowledge button (e.g., ok, got it, proceed),
while around half of the dialogs additionally included a cancel
button (e.g., cancel, exit, not now, skip). The fullscreen views
had the most design variations, compared to the other options,
which mostly used the default Android layout.
As for the content, rationales either provided more infor-
mation compared to the default permission request dialog
(i.e., reasons why the app needs the permission and how it
will be used) or they just signiﬁed that some permission is
required or has been denied (e.g., this app requires this per-
mission: to work perfectly, run normally, function properly).
We found that about 50% of the rationales provided additional
information, thus fulﬁlling the true purpose of rationales.
5 User Study
The aim of this user study is to assess whether there is an
effect of timing and presence/absence of rationales on users’
permission decisions. To isolate these effects, we used the
ﬁndings from the empirical analysis to deﬁne a standardized
rationale that also explains how and why a permission is
needed (providing additional information). More precisely,
we want to answer the following questions: How does the
788    30th USENIX Security Symposium
USENIX Association
Figure 4: Hierarchical structure of the user study.
interaction of timing and presence/absence of rationales affect
(1) users’ runtime permission decision, (2) the evaluation of
their decision, and (3) their perceived clarity of the permission
purpose? Since timing and rationales differentiate the runtime
permission request model from its predecessor, it is essential
to understand how these factors affect users from different
perspectives, even after considering other key factors found
in prior work. By answering this question, we expect to gain
insights on how developers should request permissions to
maximize the beneﬁts of the runtime permission model. Based
on these ﬁndings, we will also discuss Google’s guidelines [2]
and potential system support.
For a holistic understanding of user’s perspective, we in-
cluded both the permission decision (grant/deny) and the sub-
jective evaluation of this decision as outcome variables, where
the latter reﬂects whether the decision was made according
to users’ individual privacy preferences in a given context.
For this, we used the Decision Evaluation Scales (DES) [37]
which we adopted from the ﬁeld of health psychology. These
scales were originally designed to evaluate patients’ decision
to uptake/refuse a treatment choice. Comparing users’ permis-
sion decision with patients’ treatment choice, both have two
options: grant/deny a permission or uptake/refuse a treatment.
Additionally, both have a direct impact on users’ security or
patients’ health. Based on these similarities, this measure ﬁts
the context of our study, especially considering that the DES
account for the multidimensional nature of decisions and cap-
ture (1) whether users received sufﬁcient information to make
an informed decision, (2) their satisfaction with the decision,
and (3) their perceived control over the decision. We also mea-
sure users’ understanding of why the app needs the requested
permission, which provides information about how certain
combinations of timing (upfront/in-context) and rationales
(with/without) better communicate permission purposes.
5.1 Study Design
We designed the study as an online experiment with repeated
measures. Experimental research has the unique strength of
high internal validity because it is able to isolate causal re-
lationships through systematic manipulation of the variables
of interest (timing and presence/absence of rationales) while
controlling for the spurious effect of other extraneous vari-
ables (user and app-related differences) [38, 39]. We used a
within-subject design (repeated measures) because it reduces
errors associated with individual differences and because the
alternative (between-subjects) was shown to produce mislead-
ing results for studies involving judgment [40]. Since every
study design has its limitations, we address these in Section 8.
To make responses of users easier to compare, participants
were asked about permission requests with the same gen-
eral purpose. These purposes were identiﬁed from previous
work [5–7] and encompass that the permission is required
for the main functionality of the app (PurposeMain), a visible
feature functionality (PurposeVisible), or a hidden feature func-
tionality (PurposeHidden). Despite certain advantages (e.g.,
high external validity), we chose not to conduct this study as
a ﬁeld study because surveying users’ permission decisions
in the wild presents certain drawbacks. For example, if we
were to use an app with accessibility features, we would have
to constantly log app changes, which is an invasion of privacy
and would lead to opt-in bias. We also would need to ﬁrst
revoke all permissions in order to monitor participants’ deci-
sions and to deny all requests once for most rationales to be
shown, requiring participants to follow a complex workﬂow.
Our study had a hierarchical structure in which users in-
teracted with permission requests from different apps. To ac-
count for the fact that observations for the same user and app
would be similar to each other, we designed this study using a
multilevel model [41]. Multilevel models are used for the sta-
tistical analysis of hierarchical data, where groups in the study
are treated as a random sample from a population of groups.
This allows us to make inferences about the population of
apps and users, beyond the ones present in the study [41].
Figure 4 depicts the levels of the user study. Each user in-
teracted with four permission requests on the LevelRequest,
one per possible combination of timing (upfront/in-context)
and rationales (presence/absence). These permission requests
belonged to four different apps and the order of the requests
was randomized. LevelRequest records the outcome variables,
USENIX Association
30th USENIX Security Symposium    789
PurposeMainPurposeVisiblePurposeHiddenUserlevel (U)Requestlevel (R)Applevel (A)Permission, Purpose,Timing, Rationales,PermPredict, PermSens,Clarity, Decision,DesInform, DesSatis,DesControl App Category,FamiliarityGender, Age, Edu., CS Bg, OSPriorExp, PrivConcMeasurementsR1R2R3R4A1A2A3A4A5A6U1U2R1R2R3R4R1R2R3R4A7A8A9A10A11A12U3U4R1R2R3R4R1R2R3R4A13A14A15A16A17A18U5U6R1R2R3R4Figure 5: Overview of study procedure. Timing = upfront/in-context, Rationales = with/without.
which are inﬂuenced by the variables of interest, in addition
to the type, purpose, predictability, clarity, and sensitivity of
permission requests. LevelApp represents the diverse char-
acteristics of apps, including app category and participants’
familiarity with the app. Whereas LevelUser represents the
diverse characteristics of users (i.e., participants’ gender, age,
education, computer science background, mobile OS, privacy
concerns, and prior privacy experiences).
5.2 Procedure
As shown in Figure 5, participants ﬁrst read about the study
and gave their consent (phase 1). This was followed by the
main part of the study during which participants went through
phases 2–5 four times, once per possible composition of tim-
ing (upfront/in-context) and rationales (presence/absence),
each time for a different app. These phases were designed to
come closest to users’ interaction with real-life apps. For that,
we gave participants a goal to achieve through the app. We
also provided participants with the app’s description, name,
and icon so they had an idea what the app was about. In addi-
tion, we used interactive mockup apps, allowing participants
to click through the app interactively, just like on their real
phones. The user study procedure with a sample mockup app
is shown in Appendix A. Phases 2–5 are described next.
App information (phase 2): Participants were introduced
to the app by receiving a brief description of its function-
alities, and a goal they needed to achieve through the app
(e.g., you want to use this app to have a conference call with
your work colleagues, or you want to use this app to backup
your vehicle’s data). Each goal was based on one of the app’s
functionalities that would also require a permission. We also
provided participants with the app name and icon.
Pre-questionnaire (phase 3): This phase covers users’
ﬁrst impression of the app. We asked participants whether
they were familiar with the app, and if they would expect
it to request access to a permission protected resource spe-
ciﬁc to each app. We also measured the perceived sensitivity
(PermSens), and clarity of the permission purpose (ClarityPre).
App interaction (phase 4): We reminded participants of
the goal they want to achieve through the app and then asked
them to interact with an interactive mockup app like they
would on their own phones. Each app interaction ended with
a permission request dialog. The order in which participants
interacted with the different combinations of timing (upfront/
in-context) and rationales (with/without) was randomized.
Post-questionnaire (phase 5): After participants inter-
acted with the app, we asked them if they would grant the
requested permission (Decision). We again recorded partici-
pants’ clarity on the permission purpose (ClarityPost). Other
questions inquired about the purpose category of the permis-
sion request (PermPurp), and some questions were only present
when rationales were provided. They investigated the origin
of the rationale message (RationaleOrigin), and their collec-
tion of the content of that message (RationaleRecall). Then,
on a separate screen, we reminded participants about their
previous decision and asked them to evaluate their choice
using the Decisions Evaluation Scales (DES), consisting of
informed decision (DesInform), decision satisfaction (DesSatis),
and decision control (DesControl).
After answering questions for the four apps, participants
were asked to provide some demographic information (phase
6). The study procedure and all measurements were tested
and adjusted after running a pilot study with 25 participants.
5.3 Recruitment and Incentives
Data were collected on MTurk using TurkPrime [42], an on-
line platform that facilitates setting up and executing studies
on MTurk. We paid participants $12.00/hour, meaning that
participants received $3.00 for completing this 15 min study.
To ensure high quality of data collected through MTurk,
we followed a number of suggestions in the literature [43,44].
MTurk workers could only participate in the study if they
had a US account and had an approval rate of at least 95%.
In order to also collect responses from naive workers (i.e.,
workers who were not repeatedly exposed to similar studies),
we set the required number of completed HITs between 0 and
100 for about 10% of all HITs. Additionally, we added the
completion code at the beginning of the study (phase 1) to
increase participants’ trust (only 1.15% tried to submit the
completion code without doing the survey). Finally, we pro-
790    30th USENIX Security Symposium
USENIX Association
Phase 1Welcome MessagePhase 2App InformationPhase 3Pre-QuestionnairePhase 4App InteractionPhase 5Post-QuestionnairePhase 6DemographicsRepeated 4 times, for every composition of Timing & Rationalesvided one attention check item in the middle of the study and
monitored whether participants interacted with the mockup
apps. We excluded participants who failed the attention check
and did not interact with at least two of the mockup apps.
Since power analysis for multilevel models is still con-
sidered a complex problem [41], we estimated the required
sample size without considering the multilevel structure of our
data. Using G*Power [45], we estimate that we need at least
400 participants. A total of 698 MTurk workers attempted to
participate in our study, from which we removed 225 respon-
dents based on the screening criteria described above. Our
ﬁnal sample included 473 participants, 36.8% (N = 174) of
whom identiﬁed themselves as female. The mean age was
37.08 years (SD = 10.59). The majority of participants at-
tended college, 17.5% did not ﬁnish their studies, 51.4% had
a bachelor’s degree, and 18.4% had a graduate degree. 69.8%
owned an Android smartphone, and 28.3% an iPhone. About
one third of all participants had a background in computer
science. Appendix B shows the demographics of the sample.
5.4 Measurements
We used different measurements in our study, which are de-
scribed next and are listed in the questionnaire in Appendix A.
5.4.1 Decision Evaluation Scales (DES)
We used the Decision Evaluation Scales (DES) [37] to assess
users’ permission decisions. It consist of three subscales: in-
formed decision, decision satisfaction, and decision control.
These scales were originally developed to assess how patients
evaluate their medical treatment choice. Since such choices
often involve multiple parties (e.g., doctors and family mem-
bers) and permission decisions tend to be made individually,
we had to adjust each subscale. To do so, we used an expert
rating procedure to select suitable items per subscale. The
experts came from both the ﬁeld of computer science (N = 3)
and psychology (N = 4). The ﬁnal instruction was the fol-
lowing: “In a previous question you chose to {grant/deny}
this app access to your {permission protected resource}. We
would like to know how you feel about this decision.”
Informed Decision (DesInform): This subscale measures
whether users feel that they have received sufﬁcient informa-
tion to make a decision, it consists of four items (α = 0.76).
Sample items are “I made a well-informed choice” and “I
know the pros and cons of granting this app access to my
{permission protected resource}.” Items are scored on a 7-
point scale (1 = strongly disagree; 7 = strongly agree), where
higher scores indicate better informed decision.
Decision Satisfaction (DesSatis): Measures the general feel-
ing of users in terms of conﬁdence and satisfaction with their
decision. Sample items are “I am satisﬁed with my decision”
and “I am doubtful about my choice” (reverse coded), with
four items in total (α = 0.84). Items were rated on a 7-point
scale (1 = strongly disagree; 7 = strongly agree), where higher
scores indicate higher/greater satisfaction.
Decision Control (DesControl): Measures whether users had
the feeling that they were forced to their decision. This scale
consists of four items (α = 0.80). Sample items are “I feel
that the app forced me to make this decision” (reverse coded)
and “This was my own decision.” Items are scored on a 7-
point scale (1 = strongly disagree; 7 = strongly agree), where
higher scores indicate more perceived control.