function epilogue. To evaluate the performance impact
of this change, we compared the performance of a test
suite compiled with an unmodiﬁed GCC 3.3.4 against the
same test suite compiled with our modiﬁed compiler.
Figure 7 gives the results of this experiment. We see
that overheads are much higher, generally between 10%
and 40%, than for periodic scheduled clearing. Clearly,
Figure 7: Comparing stack clearing overheads. This chart
shows the relative performance of our workload with three
strategies: an unmodiﬁed run with no stack clearing used as a
baseline, a periodic run with OS scheduled stack zeroing (con-
ﬁgured to 5 second intervals) as well as our other kernel zeroing
features, and a immediate run with immediate stack zeroing on
every function return. Periodic zeroing has little performance
overhead. Immediate zeroing has more of a penalty, which may
be acceptable to security-conscience applications.
such overheads are signiﬁcant, though they may be ac-
ceptable for applications where data lifetime is an utmost
concern.
7.3 Kernel Clearing Overhead
Batch Workload We used Linux kernel builds to stress
our page zeroing changes. A kernel build starts many
processes, each of which modiﬁes many heap, stack, and
static data pages not backed by ﬁles. The kernel con-
siders all of these polluted and zeros them within ﬁve
seconds of deallocation.
With the ordinary kernel running, three kernel builds
took 184, 182, and 183 seconds, for an average of 183
seconds. With the zeroing kernel, the runs took 188, 184,
and 184 seconds, for an average of 185 seconds, approx-
imately a 1% penalty.
The kernel build zeroed over 1.2 million pages (about
4.8 GB) per run. The actual number of polluted pages
generated was much larger than that, but many of those
pages did not need to be zeroed because they could be
entirely overwritten by pages brought into the page cache
from disk or by copies of pages created when triggering
copy-on-write operations. (As described in section A.2,
we prefer to overwrite polluted data whenever possible.)
Network Workload We evaluated the overhead of ze-
roing by benchmarking performance on 1 Gbps Ethernet,
achieving up to 500 Mbps utilization for large blocks.
We found latency, bandwidth, and CPU usage to be in-
USENIX Association
14th USENIX Security Symposium
343
distinguishable between our zeroing kernel and unmodi-
ﬁed kernels.
We evaluated the overhead of zeroing network packets
using NetPIPE [20], which bounces messages of increas-
ing sizes between processes running on two machines.
We conﬁgured it to send blocks of data over TCP, in both
directions, between a machine running our zeroing ker-
nel and a machine running an unmodiﬁed Linux kernel.
We then compared its performance against the same test
run when both machines were conﬁgured with unmodi-
ﬁed Linux kernels.
Considering the performance of zeroing depicted in
Figure 5, our results are not too surprising. Assuming we
zero a buffer sized at the maximum length of an Ethernet
frame (1500 bytes), our performance numbers suggest
we should be able to zero one second’s worth of Gigabit
Ethernet trafﬁc in between about 7 ms and 32 ms, de-
pending on the technique used. Such low overheads are
well below the normal variance we saw across measure-
ments.
8 Future Work
We are currently looking at the performance trade-offs
involved with kernel zeroing, speciﬁcally how to param-
eterize and tune the scheduling of kernel zeroing to pro-
vide predictable latency and throughput overheads under
diverse workloads.
Examining the impact of parallelism is an interesting
direction for inquiry. The move to multi-core processors
will provide a great deal of additional available paral-
lelism to further diminish the impact of zeroing.
Providing explicit OS support for reducing data life-
time, for example “ephemeral memory” that automati-
cally zeroes its contents after a certain time period and
thus is secure in the face of bugs, memory leaks, etc., is
another area for future investigation.
A wide range of more specialized systems could ben-
eﬁt from secure deallocation. For example, virtual ma-
chine monitors and programming language runtimes.
So far we have primarily considered language envi-
ronments that use explicit deallocation, such as C, but
garbage-collected languages pose different problems that
may be worthy of additional attention. Mark-and-sweep
garbage collectors, for example, prolong data lifetime
at least until the next GC, whereas reference-counting
garbage collectors may be able to reduce data lifetime
below that of secure deallocation.
9 Related Work
Our previous work explored the problem of data life-
time using whole system simulation with TaintBochs [5].
We focused on mechanisms for analyzing the problem,
demonstrated its frequency in real world applications,
and showed how programmers could take steps to reduce
data lifetime. Whereas this earlier work looked at how
sensitive data propagates through memory over relatively
short intervals (on the order of seconds), the current pa-
per is concerned with how long data survives before it is
overwritten, and with developing a general-purpose ap-
proach to minimizing data lifetime.
We explored data lifetime related threats and the im-
portance of proactively addressing data lifetime at every
layer of the software stack in a short position paper [7].
The impetus for this and previous work stemmed from
several sources.
Our ﬁrst interest was in understanding the security of
our own system as well as addressing vulnerabilities ob-
served in other systems due to accidental information
leaks, e.g. via core dumps [15, 16, 14, 13] and program-
mer error [1].
A variety of previous work has addressed speciﬁc
symptoms of the data lifetime problem (e.g. leaks) but
to the best of our knowledge none has offered a gen-
eral approach to reducing the presence of sensitive data
in memory. Scrash [4] deals speciﬁcally with the core
dump problem. It infers which data in a system is sensi-
tive based on programmer annotations to allow for crash
dumps that can be shipped to the application developer
without revealing users’ sensitive data.
Previous concern about sensitive data has addressed
keeping it off of persistent storage, e.g. Provos’s work
on encrypted swap [19] and work by Blaze on encrypted
ﬁle systems [3]. Steps such as these can greatly reduce
the impact of sensitive data that has leaked to persistent
storage.
The importance of keeping sensitive data off of stor-
age has been emphasized in work by Gutmann [9], who
showed the difﬁculty of removing all remnants of sensi-
tive data once written to disk.
Developers of cryptographic software have long been
aware of the need for measures to reduce the lifetime of
cryptographic keys and passwords in memory. Good dis-
cussions are given by Gutmann [10] and Viega [22].
10 Conclusion
The operating systems and applications responsible for
handling the vast majority of today’s sensitive data, such
as passwords, social security numbers, credit card num-
bers, and conﬁdential documents, take little care to en-
sure this data is promptly removed from memory. The
result is increased vulnerability to disclosure during at-
tacks or due to accidents.
To address this issue, we argue that the strategy of se-
cure deallocation, zeroing data at deallocation or within
344
14th USENIX Security Symposium
USENIX Association
a short, predictable period afterward, should become a
standard part of most systems.
We demonstrated the speed and effectiveness of secure
deallocation in real systems by modifying all major allo-
cation systems of a Linux system, from compiler stack,
to malloc-controlled heap, to dynamic allocation in the
kernel, to support secure deallocation.
We described the data life cycle, a conceptual frame-
work for understanding data lifetime, and applied it to
analyzing the effectiveness of secure deallocation.
We further described techniques for measuring effec-
tiveness and performance overheads of this approach us-
ing whole-system simulation, application-level dynamic
instrumentation, and system and network benchmarks.
We showed that secure deallocation reduces typical
data lifetime to 1.35 times the minimum possible data
lifetime.
In contrast, we showed that waiting for data
to be overwritten often produces data lifetime 10 to 100
times longer than the minimum, and that on normal desk-
top systems it is not unusual to ﬁnd data from dead pro-
cesses that is days or weeks old.
We argue that these results provide a compelling case
for secure deallocation, demonstrating that it can provide
a measurable improvement in system security with neg-
ligible overhead, while requiring no programmer inter-
vention and supporting legacy applications.
11 Acknowledgments
This work was supported in part by the National Sci-
ence Foundation under Grant No. 0121481 and a Stan-
ford Graduate Fellowship.
References
[1] O. Arkin and J. Anderson. Etherleak: Ethernet frame padding
http://www.atstake.com/research/
information leakage.
advisories/2003/atstake etherleak repor%t.pdf.
[2] Arkoon Security Team.
kernel ext2 implementation.
ext2-make-empty-leak.txt, March 2005.
Information leak in the Linux
http://arkoon.net/advisories/
[3] M. Blaze. A cryptographic ﬁle system for UNIX. In ACM Con-
ference on Computer and Communications Security, pages 9–16,
1993.
[4] P. Broadwell, M. Harren, and N. Sastry. Scrash: A system for
generating secure crash information. In Proceedings of the 11th
USENIX Security Symposium, August 2003.
[5] J. Chow, B. Pfaff, T. Garﬁnkel, K. Christopher, and M. Rosen-
blum. Understanding data lifetime via whole system simulation.
In Proceedings of the 12th USENIX Security Symposium, 2004.
[6] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf. Bugs
as deviant behavior: a general approach to inferring errors in sys-
tems code.
In SOSP ’01: Proceedings of the Eighteenth ACM
symposium on Operating Systems Principles, pages 57–72, New
York, NY, USA, 2001. ACM Press.
[7] T. Garﬁnkel, B. Pfaff, J. Chow, and M. Rosenblum. Data lifetime
In Proc. 11th ACM SIGOPS European
is a systems problem.
Workshop, September 2004.
[8] D. Gay and A. Aiken. Memory management with explicit re-
gions.
In PLDI ’98: Proceedings of the ACM SIGPLAN 1998
conference on Programming language design and implementa-
tion, pages 313–323. ACM Press, 1998.
[9] P. Gutmann. Secure deletion of data from magnetic and solid-
state memory. In Proceedings of the 6th USENIX Security Sym-
posium, July 1996.
[10] P. Gutmann. Software generation of practically strong random
numbers. In Proceedings of the 8th USENIX Security Symposium,
August 1999.
[11] T. Hamilton. ‘Error’ sends bank ﬁles to eBay. Toronto Star, Sep.
15, 2003.
[12] S. Hand. Data lifetime bug in VMM. Personal communications.
[13] Coredump hole in imapd and ipop3d in slackware 3.4. http://
www.insecure.org/sploits/slackware.ipop.imap.core.html.
[14] Security Dynamics FTP server core problem.
http://www.
insecure.org/sploits/solaris.secdynamics.core.html.
[15] Solaris (and others) ftpd core dump bug. http://www.insecure.
org/sploits/ftpd.pasv.html.
[16] Wu-ftpd core dump vulnerability.
sploits/ftp.coredump2.html.
http://www.insecure.org/
[17] C. Lameter. Prezeroing V2 [0/3]: Why and when it works.
Pine.LNX.4.58.0412231119540.31791@schroedinger.
engr.sgi.com, December 2004.
message.
Linux kernel mailing list
[18] N. Nethercote and J. Seward. Valgrind: A program supervision
framework. In O. Sokolsky and M. Viswanathan, editors, Elec-
tronic Notes in Theoretical Computer Science, volume 89. Else-
vier, 2003.
[19] N. Provos. Encrypting virtual memory.
In Proceedings of the
10th USENIX Security Symposium, pages 35–44, August 2000.
[20] Q. O. Snell, A. R. Mikler, and J. L. Gustafson. NetPIPE: A net-
work protocol independent performance evaluator. http://www.
scl.ameslab.gov/netpipe/paper/full.html.
[21] The Mozilla Organization. Javascript “lambda” replace exposes
memory contents. http://www.mozilla.org/security/announce/
mfsa2005-33.html, 2005.
[22] J. Viega.
Protecting sensitive data in memory.
//www-106.ibm.com/developerworks/security/library/
s-data.html?dwzo%ne=security.
http:
[23] J. Viega and G. McGraw. Building Secure Software. Addison-
Wesley, 2002.
[24] C. A. Waldspurger. Memory resource management in VMware
ESX Server. SIGOPS Oper. Syst. Rev., 36(SI):181–194, 2002.
A Kernel Support for Secure Deallocation
This section describes strategies that we found useful
for reducing data lifetime in the Linux kernel. Some
improve the performance of secure allocation in cases
where we have additional semantic knowledge; the rest
work to reduce the lifetime of data that is long-lived from
the point of view of the kernel allocators, such as data
stored in circular queues. We believe that these strate-
gies will prove useful in other long-lived programs.
USENIX Association
14th USENIX Security Symposium
345
A.1 What Data is Sensitive?
Section 6.2 described kernel mechanisms for labeling
sensitive data. Once these mechanisms are available,
we need a policy to distinguish sensitive data from other
data. The policy for our prototype implementation was
based on a few rules of thumb. First, we considered all
user input, such as keyboard and mouse data, all network
trafﬁc, and all user process data, to be sensitive.
However, we consider data or metadata read from or
written to a ﬁle system not sensitive, because its data life-
time is already extended indeﬁnitely simply because it
has been written to disk [9].
(For simplicity of proto-
typing, we ignore the possibility of encrypted, network,
in-memory, or removable media ﬁle systems, as well as
temporary ﬁles.) Thus, because pages in shared ﬁle map-
pings (e.g. code, read-only data) are read from disk, they
are not considered sensitive even though they belong to
user processes. On the other hand, anonymous pages
(e.g. stack, heap) are not ﬁle system data and therefore
deemed sensitive.
We decided that the location of sensitive data is not
itself sensitive. Therefore, pointers in kernel data struc-
tures are never considered sensitive. Neither are page
tables, scheduling data, process ids, etc.
A.2 Allocator Optimizations
Section 6.2 described the division of kernel allocators
into pools and the use of a zeroing daemon to delay ze-
roing. However, the kernel can sometimes avoid doing
extra work, or clear polluted pages more quickly, by us-
ing advice about the intended use of the page provided
by the allocator’s caller:
• The caller may request a zeroed page. The allocator
returns a zeroed page, if one is available. Other-
wise, it zeroes and returns a polluted page, if avail-
able, rather than a not-zeroed page. This preference
reduces polluted pages at no extra cost (because a
page must be zeroed in any case).
• The caller may indicate that it will be clearing the
entire page itself, e.g. that the page will be used for
buffering disk data or receiving a copy of a copy-
on-write page. In this case the allocator returns a
polluted page if available, again reducing polluted
pages without extra cost. In this case the caller is
responsible for clearing the page; the allocator does
not zero it.
• If the caller has no special requirements, the allo-
cator prefers not-zeroed pages, then zeroed pages,
then polluted pages. If a polluted page is returned,
then it must be zeroed beforehand because the caller
may not overwrite the entire page in an punctual
fashion.
We applied changes similar to those made to the page
allocator to the slab allocator as well. Slabs do not have
a convenient place to store a per-block “polluted” bit, so
the slab allocator instead requires the caller to specify at
time of free whether the object is polluted.
A.3 Oversized Allocations Optimizations
Without secure deallocation, allocating or freeing a
buffer costs about the same amount of time regardless
of the buffer’s size. This encourages the common prac-
tice of allocating a large, ﬁxed-size buffer for temporary
use, even if only a little space is usually needed. With se-
cure deallocation, on the other hand, the cost of freeing
a buffer increases linearly with the buffer’s size. There-
fore, a useful optimization is to clear only that part of a
buffer that was actually used.
We implemented such an optimization in the Linux
network stack. The stack uses the slab allocator to al-
locate packet data, so we could use the slab allocator’s
pollution mechanism to clear network packets. However,
the blocks allocated for packets are often much larger
than the actual packet content, e.g. packets are often less
than 100 bytes long, but many network drivers put each
packet into 2 KB buffer. We improved performance by
zeroing only packet data, not other unused bytes.
Filename buffers are another place that this class of
optimization would be useful. Kernel code often allo-
cates an entire 4 KB page to hold a ﬁlename, but usually
only a few bytes are used. We have not implemented this
optimization.
A.4 Lifetime Reduction
Queues
in Circular
As already discussed in section 4.3, circular queues can
extend data lifetime of their events, if new events are not
added rapidly enough to replace those that have been re-
moved in a reasonable amount of time. We identiﬁed
several examples of such queues in the kernel, including
“ﬂip buffers” and tty buffers used for keyboard and se-
rial port input, pseudoterminal buffers used by terminal
emulators, and the entropy batch processing queue used
by the Linux pseudo-random number generator. In each
case, we ﬁxed the problem by clearing events held in the
queue at their time of removal.
346
14th USENIX Security Symposium
USENIX Association