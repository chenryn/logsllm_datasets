### Function Epilogue and Performance Impact Evaluation

To evaluate the performance impact of our modifications, we compared the performance of a test suite compiled with an unmodified GCC 3.3.4 against the same test suite compiled with our modified compiler. The results of this experiment are presented in Figure 7.

**Figure 7: Comparing Stack Clearing Overheads**
- **Baseline (Unmodified):** No stack clearing.
- **Periodic Zeroing:** OS-scheduled stack zeroing every 5 seconds, along with other kernel zeroing features.
- **Immediate Zeroing:** Immediate stack zeroing on every function return.

The overheads for immediate zeroing are significantly higher, generally between 10% and 40%, compared to periodic zeroing, which has minimal performance overhead. While these overheads are substantial, they may be acceptable for security-critical applications where data lifetime is a primary concern.

### 7.3 Kernel Clearing Overhead

#### Batch Workload
We used Linux kernel builds to stress-test our page zeroing changes. A kernel build initiates multiple processes, each modifying heap, stack, and static data pages not backed by files. The kernel considers all these pages polluted and zeros them within five seconds of deallocation.

- **Ordinary Kernel:**
  - Three kernel builds took 184, 182, and 183 seconds, averaging 183 seconds.
- **Zeroing Kernel:**
  - The same three builds took 188, 184, and 184 seconds, averaging 185 seconds, resulting in approximately a 1% performance penalty.

Each kernel build zeroed over 1.2 million pages (about 4.8 GB). The actual number of polluted pages was much larger, but many did not need to be zeroed because they could be entirely overwritten by pages brought into the page cache from disk or by copies of pages created during copy-on-write operations. As described in Section A.2, we prefer to overwrite polluted data whenever possible.

#### Network Workload
We evaluated the overhead of zeroing by benchmarking performance on 1 Gbps Ethernet, achieving up to 500 Mbps utilization for large blocks. We found that latency, bandwidth, and CPU usage were indistinguishable between our zeroing kernel and unmodified kernels.

We used NetPIPE [20] to evaluate the overhead of zeroing network packets. NetPIPE bounces messages of increasing sizes between processes running on two machines. We configured it to send blocks of data over TCP, in both directions, between a machine running our zeroing kernel and a machine running an unmodified Linux kernel. We then compared its performance against the same test run when both machines were configured with unmodified Linux kernels.

Based on the performance of zeroing depicted in Figure 5, our results are not surprising. Assuming we zero a buffer sized at the maximum length of an Ethernet frame (1500 bytes), our performance numbers suggest we can zero one second’s worth of Gigabit Ethernet traffic in between about 7 ms and 32 ms, depending on the technique used. Such low overheads are well below the normal variance we observed across measurements.

### 8. Future Work

We are currently investigating the performance trade-offs involved with kernel zeroing, specifically how to parameterize and tune the scheduling of kernel zeroing to provide predictable latency and throughput overheads under diverse workloads.

- **Parallelism:** The move to multi-core processors will provide additional parallelism, potentially reducing the impact of zeroing.
- **Explicit OS Support:** Providing explicit OS support for reducing data lifetime, such as "ephemeral memory" that automatically zeroes its contents after a certain time period, is another area for future investigation.
- **Specialized Systems:** Virtual machine monitors and programming language runtimes could benefit from secure deallocation. We have primarily considered language environments that use explicit deallocation, such as C, but garbage-collected languages pose different problems that may require additional attention. For example, mark-and-sweep garbage collectors prolong data lifetime until the next GC, while reference-counting garbage collectors may reduce data lifetime more effectively.

### 9. Related Work

Our previous work explored data lifetime using whole system simulation with TaintBochs [5]. We focused on mechanisms for analyzing the problem, demonstrated its frequency in real-world applications, and showed how programmers could take steps to reduce data lifetime. This earlier work looked at how sensitive data propagates through memory over short intervals (on the order of seconds), whereas the current paper is concerned with how long data survives before being overwritten and developing a general-purpose approach to minimizing data lifetime.

We explored data lifetime-related threats and the importance of proactively addressing data lifetime at every layer of the software stack in a short position paper [7]. Our interest stemmed from understanding the security of our own system and addressing vulnerabilities observed in other systems due to accidental information leaks, such as via core dumps [15, 16, 14, 13] and programmer error [1].

Previous work has addressed specific symptoms of the data lifetime problem (e.g., leaks), but to the best of our knowledge, none has offered a general approach to reducing the presence of sensitive data in memory. Scrash [4] deals specifically with the core dump problem by inferring which data in a system is sensitive based on programmer annotations, allowing for crash dumps that can be shipped to the application developer without revealing users’ sensitive data.

Concerns about sensitive data have also addressed keeping it off persistent storage, such as Provos’s work on encrypted swap [19] and Blaze’s work on encrypted file systems [3]. Steps like these can greatly reduce the impact of sensitive data that has leaked to persistent storage.

Gutmann [9] emphasized the importance of keeping sensitive data off storage, showing the difficulty of removing all remnants of sensitive data once written to disk. Developers of cryptographic software have long been aware of the need for measures to reduce the lifetime of cryptographic keys and passwords in memory, as discussed by Gutmann [10] and Viega [22].

### 10. Conclusion

Operating systems and applications responsible for handling sensitive data, such as passwords, social security numbers, credit card numbers, and confidential documents, often take little care to ensure this data is promptly removed from memory. This increases vulnerability to disclosure during attacks or due to accidents.

To address this issue, we argue that the strategy of secure deallocation, zeroing data at deallocation or within a short, predictable period afterward, should become a standard part of most systems. We demonstrated the speed and effectiveness of secure deallocation in real systems by modifying all major allocation systems of a Linux system, from the compiler stack to malloc-controlled heap, to dynamic allocation in the kernel, to support secure deallocation.

We described the data life cycle, a conceptual framework for understanding data lifetime, and applied it to analyzing the effectiveness of secure deallocation. We further described techniques for measuring the effectiveness and performance overheads of this approach using whole-system simulation, application-level dynamic instrumentation, and system and network benchmarks.

We showed that secure deallocation reduces typical data lifetime to 1.35 times the minimum possible data lifetime. In contrast, waiting for data to be overwritten often produces data lifetimes 10 to 100 times longer than the minimum, and on normal desktop systems, it is not unusual to find data from dead processes that is days or weeks old.

These results provide a compelling case for secure deallocation, demonstrating that it can provide a measurable improvement in system security with negligible overhead, while requiring no programmer intervention and supporting legacy applications.

### 11. Acknowledgments

This work was supported in part by the National Science Foundation under Grant No. 0121481 and a Stanford Graduate Fellowship.

### References

[1] O. Arkin and J. Anderson. Etherleak: Ethernet frame padding information leakage. http://www.atstake.com/research/advisories/2003/atstake_etherleak_report.pdf.

[2] Arkoon Security Team. Information leak in the Linux kernel ext2 implementation. http://arkoon.net/advisories/ext2-make-empty-leak.txt, March 2005.

[3] M. Blaze. A cryptographic file system for UNIX. In ACM Conference on Computer and Communications Security, pages 9–16, 1993.

[4] P. Broadwell, M. Harren, and N. Sastry. Scrash: A system for generating secure crash information. In Proceedings of the 11th USENIX Security Symposium, August 2003.

[5] J. Chow, B. Pfaff, T. Garfinkel, K. Christopher, and M. Rosenblum. Understanding data lifetime via whole system simulation. In Proceedings of the 12th USENIX Security Symposium, 2004.

[6] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf. Bugs as deviant behavior: a general approach to inferring errors in systems code. In SOSP '01: Proceedings of the Eighteenth ACM Symposium on Operating Systems Principles, pages 57–72, New York, NY, USA, 2001. ACM Press.

[7] T. Garfinkel, B. Pfaff, J. Chow, and M. Rosenblum. Data lifetime is a systems problem. In Proc. 11th ACM SIGOPS European Workshop, September 2004.

[8] D. Gay and A. Aiken. Memory management with explicit regions. In PLDI '98: Proceedings of the ACM SIGPLAN 1998 Conference on Programming Language Design and Implementation, pages 313–323. ACM Press, 1998.

[9] P. Gutmann. Secure deletion of data from magnetic and solid-state memory. In Proceedings of the 6th USENIX Security Symposium, July 1996.

[10] P. Gutmann. Software generation of practically strong random numbers. In Proceedings of the 8th USENIX Security Symposium, August 1999.

[11] T. Hamilton. ‘Error’ sends bank files to eBay. Toronto Star, Sep. 15, 2003.

[12] S. Hand. Data lifetime bug in VMM. Personal communications.

[13] Coredump hole in imapd and ipop3d in slackware 3.4. http://www.insecure.org/sploits/slackware.ipop.imap.core.html.

[14] Security Dynamics FTP server core problem. http://www.insecure.org/sploits/solaris.secdynamics.core.html.

[15] Solaris (and others) ftpd core dump bug. http://www.insecure.org/sploits/ftpd.pasv.html.

[16] Wu-ftpd core dump vulnerability. http://www.insecure.org/sploits/ftp.coredump2.html.

[17] C. Lameter. Prezeroing V2 [0/3]: Why and when it works. Pine.LNX.4.58.0412231119540.31791@schroedinger.engr.sgi.com, December 2004. Linux kernel mailing list message.

[18] N. Nethercote and J. Seward. Valgrind: A program supervision framework. In O. Sokolsky and M. Viswanathan, editors, Electronic Notes in Theoretical Computer Science, volume 89. Elsevier, 2003.

[19] N. Provos. Encrypting virtual memory. In Proceedings of the 10th USENIX Security Symposium, pages 35–44, August 2000.

[20] Q. O. Snell, A. R. Mikler, and J. L. Gustafson. NetPIPE: A network protocol independent performance evaluator. http://www.scl.ameslab.gov/netpipe/paper/full.html.

[21] The Mozilla Organization. Javascript “lambda” replace exposes memory contents. http://www.mozilla.org/security/announce/mfsa2005-33.html, 2005.

[22] J. Viega. Protecting sensitive data in memory. http://www-106.ibm.com/developerworks/security/library/s-data.html?dwzone=security.

[23] J. Viega and G. McGraw. Building Secure Software. Addison-Wesley, 2002.

[24] C. A. Waldspurger. Memory resource management in VMware ESX Server. SIGOPS Oper. Syst. Rev., 36(SI):181–194, 2002.

### A. Kernel Support for Secure Deallocation

This section describes strategies we found useful for reducing data lifetime in the Linux kernel. Some improve the performance of secure allocation in cases where we have additional semantic knowledge; the rest work to reduce the lifetime of data that is long-lived from the point of view of the kernel allocators, such as data stored in circular queues. We believe these strategies will prove useful in other long-lived programs.

#### A.1 What Data is Sensitive?

Section 6.2 described kernel mechanisms for labeling sensitive data. Once these mechanisms are available, we need a policy to distinguish sensitive data from other data. The policy for our prototype implementation was based on a few rules of thumb:

- **Sensitive Data:**
  - All user input, such as keyboard and mouse data.
  - All network traffic.
  - All user process data.
- **Non-Sensitive Data:**
  - Data or metadata read from or written to a file system, as its data lifetime is already extended indefinitely simply because it has been written to disk [9].
  - Pages in shared file mappings (e.g., code, read-only data) are not considered sensitive even though they belong to user processes.
  - Anonymous pages (e.g., stack, heap) are deemed sensitive as they are not file system data.

We decided that the location of sensitive data is not itself sensitive. Therefore, pointers in kernel data structures, page tables, scheduling data, and process IDs are never considered sensitive.

#### A.2 Allocator Optimizations

Section 6.2 described the division of kernel allocators into pools and the use of a zeroing daemon to delay zeroing. However, the kernel can sometimes avoid doing extra work or clear polluted pages more quickly by using advice about the intended use of the page provided by the allocator's caller:

- **Zeroed Page Request:**
  - The caller may request a zeroed page. The allocator returns a zeroed page if one is available. Otherwise, it zeroes and returns a polluted page if available, rather than a non-zeroed page. This preference reduces polluted pages at no extra cost.
- **Self-Clearing Page:**
  - The caller may indicate that it will be clearing the entire page itself, e.g., for buffering disk data or receiving a copy of a copy-on-write page. The allocator returns a polluted page if available, reducing polluted pages without extra cost. The caller is responsible for clearing the page.
- **Default Behavior:**
  - If the caller has no special requirements, the allocator prefers non-zeroed pages, then zeroed pages, then polluted pages. If a polluted page is returned, it must be zeroed beforehand because the caller may not overwrite the entire page in a timely manner.

We applied similar changes to the slab allocator. Slabs do not have a convenient place to store a per-block "polluted" bit, so the slab allocator requires the caller to specify at the time of freeing whether the object is polluted.

#### A.3 Oversized Allocations Optimization

Without secure deallocation, allocating or freeing a buffer costs about the same amount of time regardless of the buffer's size. This encourages the common practice of allocating a large, fixed-size buffer for temporary use, even if only a little space is usually needed. With secure deallocation, the cost of freeing a buffer increases linearly with the buffer's size. Therefore, a useful optimization is to clear only the part of a buffer that was actually used.

We implemented such an optimization in the Linux network stack. The stack uses the slab allocator to allocate packet data, so we could use the slab allocator's pollution mechanism to clear network packets. However, the blocks allocated for packets are often much larger than the actual packet content, e.g., packets are often less than 100 bytes long, but many network drivers put each packet into a 2 KB buffer. We improved performance by zeroing only the packet data, not the unused bytes.

Filename buffers are another place where this class of optimization would be useful. Kernel code often allocates an entire 4 KB page to hold a filename, but usually only a few bytes are used. We have not implemented this optimization yet.

#### A.4 Lifetime Reduction in Circular Queues

As discussed in Section 4.3, circular queues can extend the data lifetime of their events if new events are not added rapidly enough to replace those that have been removed in a reasonable amount of time. We identified several examples of such queues in the kernel, including "flip buffers" and tty buffers used for keyboard and serial port input, pseudoterminal buffers used by terminal emulators, and the entropy batch processing queue used by the Linux pseudo-random number generator. In each case, we fixed the problem by clearing events held in the queue at their time of removal.

---

This revised version aims to enhance clarity, coherence, and professionalism while maintaining the technical depth and accuracy of the original text.