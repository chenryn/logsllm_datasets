purpose language models have better generality and can be
directly used as input to train downstream learning models.
For instance, with a simple linear layer for output, embeddings
from a pretrained Bert model can achieve state-of-the-art
performance on eleven NLP tasks [17].
B. General-Purpose Language Models for Sentence Embed-
ding
Roughly speaking, existing general-purpose language mod-
els are mainly variants of stacked recurrent Transformers,
which consist of millions of learnable parameters. Before
coming into use, general-purpose language models ﬁrst need
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1316
to be pretrained on extremely large corpus such as the English
Wikipedia. Typical pretraining tasks include masked language
modeling and next sentence prediction [17].
Fig. 1. General-purpose language models for sentence embedding and the
potential privacy risks. The red directed line illustrates the discovered privacy
risks:
the adversary could reconstruct some sensitive information in the
unknown plain texts even when he/she only sees the embeddings from the
general-purpose language model.
To obtain the embedding of a given sentence x, the follow-
ing procedures are required [17]: (1) tokenization according
to a prepared vocabulary; (2) token embedding (i.e., the token
index is mapped to a corresponding vector with the aid
of a learnable look-up table); (3) propagation through the
Transformers along two dimensions. At the last layer, the
sentence is transformed to an n-length sequence of vectors in
Rd (i.e., hidden states); and (4) ﬁnally, a pooling operation is
performed on the hidden states to get the sentence embedding.
The pooling operation for general-purpose language models is
to take the last hidden state at the ﬁnal layer as the embedding
of sentence x, because most general-purpose language models
by default add a special token (i.e, (cid:2)CLS(cid:3), which intuitively
means to classify) to the end of the input sentence during
the pretraining phase. As a result, to use the last hidden state
as the sentence embedding usually brings better utility [17],
[44]. Fig. 1 provides a schematic view on the aforementioned
procedures. Although intuitions on the described workﬂow
suggests that a certain level of context information should
be preserved in the last hidden state, there is little known to
our community that, to what granularity the original sentence
is preserved in the encoding, whether and how the resided
sensitive information can be decoded by potential attacks.
C. General-Purpose Language Models in the Wild
BASIC INFORMATION OF MAINSTREAM PRETRAINED LANGUAGE MODELS.
(* IMPLIES THE STATISTICS IS ESTIMATED ACCORDING TO THE ORIGINAL
TABLE I
PAPER.)
Name
Proposed by
Dimension d
Pretraining Data Size
Bert [17]
Transformer-XL [16]
XLNet [76]
GPT [54]
GPT-2 [55]
RoBERTa [44]
XLM [41]
Google
Google
Google
OpenAI
OpenAI
Facebook
Facebook
Ernie 2.0 [67] (abbr. ERNIE)
Baidu
1024
1024
768
768
768
768
1024
768
13GB
517MB*
76GB
4GB*
40GB
160GB
10GB*
33GB*
As is discussed, training a general-purpose language model
from scratch can be highly expensive. As an alternative,
most of the state-of-the-art models have a pretrained version
published online for free access. In this paper, we study 8
mainstream language models developed by industry leaders
including Google, OpenAI, Facebook and Baidu. Table I lists
the basic information of these target models.
IV. GENERAL ATTACK PIPELINE
Although the state-of-the-art language models provide a
direct and effective way for obtaining general-purpose sen-
tence embeddings for various downstream tasks, we ﬁnd their
improved utility is accompanied with hidden privacy risks. By
constructing two novel attack classes, we show an adversary is
able to reverse-engineer various levels of sensitive information
in the unknown plain text from the embeddings. In this section,
we ﬁrst present some general statements of our attacks.
A. Attack Deﬁnition
Generally speaking, in both attacks the adversary wants
to infer some sensitive information of the sentence from the
accessed embeddings. Formally, we formulate the attack model
as A : z → s , where z is the embedding of a target sentence x
and s denotes certain type of sensitive information that can be
obtained from the plaintext with a publicly-known algorithm
P : x → s. For example, from the treatment description
“CT scan of blood vessel of head with contrast”, we can tell
the patient probably has sickness at his/her head. In practice,
the sensitive information s can be of various types, from a
small segment that contains sensitive information (i.e., P is an
operation that takes out a speciﬁed part of the whole sequence)
to a predicate on the plain text x. For example, in the above
head case, P maps any sentence x to {0, 1}: if the sentence
x has word head, then P(x) = 1; otherwise P(x) = 0. This
notion will be used in formulating our attack pipeline.
B. Threat Model
In general, we focus on the following threat model.
• Assumption 0. The adversary has access to a set of em-
beddings of plain text, which may contain the sensitive
information the adversary is interested in.
• Assumption 1. For simplicity only, we assume the adver-
sary knows which type of pretrained language models the
embeddings come from. Later in Section VIII, we show this
assumption can be easily removed with a proposed learning-
based ﬁngerprinting algorithm.
• Assumption 2. The adversary has access to the pretrained
language model as an oracle, which takes a sentence as input
and outputs the corresponding embedding.
For each attack, we also impose different assumptions on the
adversary’s prior knowledge of the unknown plain text, which
are detailed in the corresponding parts.
C. Attack Pipeline
Our general attack pipeline is divided into four stages.
At the ﬁrst stage, the adversary prepares an external corpus
1317
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
:= {xi}N
Dext
i=1 and uses the algorithm P to extract the
{P(xi)}N
i=1 as labels. It is worth to notice, as the external
corpus is basically generated with algorithms or crawled from
open domains like Yelp restaurant reviews, the extracted labels
usually contain no truly sensitive information. At the second
stage, the adversary queries the pretrained language model
with each sentence xi ∈ Dext and receives their embeddings
{zi}N
i=1. At the third stage, the adversary combines the embed-
dings with the extracted labels to train an attack model A. At
the ﬁnal stage, the adversary uses the well-trained attack model
to infer sensitive information s from the target embedding z.
Fig. 2 provides an overview of our attack pipeline. In the next
parts, we provide a general introduction of each stage in the
pipeline.
Fig. 2. General attack pipeline.
to the corresponding embeddings. Ideally,
Stage 1: Prepare External Corpus. The preparation of the
training set is accomplished in the ﬁrst two stages. First, as the
attack model infers sensitive information in the unknown plain
text, a proper external corpus Dext := {xi}N
i=1 is therefore
essential
to play the role of a probing set for successful
attacks. Based on different knowledge levels on the plain
text, we suggest the adversary can create the external corpus
(1) by generating algorithms or (2) from public corpora in
open domains. The details are provided in the corresponding
sections. After the external corpus is prepared, we apply the
algorithm P on each xi ∈ Dext to obtain the label P(xi),
which concludes the ﬁrst stage.
Stage 2: Query the Language Model. The second stage
for training set preparation is to convert
the sentences in
Dext
is quite
straightforward as the adversary only needs to query the
language model with each sentence. In practice, according
to the knowledge of which model is used, the adversary can
deploy the corresponding pretrained model on his/her devices
for local query. The adversary may also save some budget by
utilizing online language model services [74]. Without loss
of generality, our evaluations are conducted in the former
setting. More details can be found in Appendix G. At the
end of this stage, we have the training set Dtrain of the form
{(zi,P(xi))}N
i=1, where zi is the embedding corresponding to
the sentence xi.
Stage 3: Train the Attack Model. With the training set
Dtrain at hand, the adversary can now train an attack model
g for inference usage. In general, the model is designed as
a classiﬁer, which takes the embedding zi as its input and
outputs a probabilistic vector g(zi) over all possible values of
the sensitive information. To train the attack model with the
prepared dataset, the adversary needs to solve the following
optimization problem with gradient-based algorithms such
it
N
1
N
(cid:2)
as Stochastic Gradient Descent (SGD [56]) or Adam [39],
i=1 (cid:2)(g(zi),P(xi)), where (cid:2) is a loss function that
ming
measures the difference between the predicted probabilities
and the ground-truth label. Throughout this paper, (cid:2) is always
implemented as the cross-entropy loss.
As a ﬁnal remark, depending on the knowledge level of
the adversary, the architecture of g varies in different settings.
For example, knowledgeable attackers will ﬁnd off-the-shelf
classiﬁers such as logistic regression or linear SVM work
surprisingly well, while attackers with no prior knowledge can
leverage advanced transfer learning techniques for successful
attacks.
Stage 4: Inference. After the training phase, given the
target embedding z,
the adversary infers the sensitive in-
formation based on the following equation s := A(z) =
arg maxi∈{1,2,...,K}[g(z)]i, where [g(z)]i is the value of g(z)
at
its i-th dimension and K denotes the total number of
possible values for s. In other words, the adversary considers
the value with the highest probability as the most possible
value of the sensitive information in the unknown sentence x.
V. PATTERN RECONSTRUCTION ATTACK
In this section, we focus on the situation when the adversary
has knowledge of the generating rule of the unknown plain
text, which usually happens when the format of the plain
text is common sense (e.g., identity code). We provide this
section as a starting point to understand how much sensitive
information is encoded in the embeddings from the general-
purposed language models.
A. Attack Deﬁnition
Intuitively speaking, the pattern reconstruction attack aims
at recovering a speciﬁc segment of the plain text which has
a ﬁxed format. The target segment may contain sensitive
information such as birth date, gender or even gene expression.
Formally, we construct the pattern reconstruction attack under
the following assumption.
• Assumption 3a. The format of the plain text is ﬁxed and
the adversary knows the generating rules of the plain text.
Following the general statements in Section IV-A, we
formally deﬁne the routine P for extracting the sensitive
information s from the sentence x := (w1, . . . , wn) as
Ppattern : (w1, . . . , wn) → (wb, . . . , we), where b and e are
the starting and the termination index of the target segment.
As P is assumed to be publicly known, it is also known by
the adversary. Therefore, the pattern reconstruction attack w.r.t.
Ppattern can be deﬁned as Apattern : z → (wb, . . . , we).
To be concrete, we provide the following two illustrative
examples.
Case Study - Citizen ID (abbr. Citizen). Structured informa-
tion such as identity code or zip code commonly appears in our
daily conversations, and these conversations are proved to be
useful for training chatbots with the aid of general-purpose
language models [55]. However, we ﬁnd if the messages
are not properly cleaned, the adversary, given the sentence
embeddings, is capable to recover the structured information
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1318
with high accuracy and thus conduct further harassment. For
example, in many countries, citizen ID is a typical sensitive
information for its owner. Once being leaked to the adversary,
the identity code can be used to access the victim’s other
sensitive information or allow the adversary to impersonate the
victim to participate in illegal activities [3]. To demonstrate,
we consider the case of citizen ID in China, which consists
of the 18 characters (from the vocabulary {0, . . . , 9}), i.e. 6
for the residence code (3000 possibilities), 8 for the birth date
(more than 100 × 12 × 30 possibilities) and 4 for extra code
(104 possibilities). Consider the adversary wants to recover
the exact birth date of the victim via the leaked embedding of
his/her citizen ID, we deﬁne the mapping P as
Pcitizen : |residence|birthday|extra| → |birthday|
(1)
Case Study - Genome Sequence (abbr. Genome). Roughly,
a genome is a sequence of nucleotide which has four different
types, namely A, C, G, T, as its vocabulary. With increasingly
many NLP techniques being applied in computational genet-
ics and pharmacogenomics [43], [45], [81], general-purpose
language models are also used in genomics-related tasks.
To demonstrate this point, we implement eight benchmark
systems by incorporating different general-purpose language
models for splice site prediction [45], a classical binary
classiﬁcation problem in computational genetics. Basically,
our systems exhibit a high utility performance. For example,
the splice site prediction system with Google’s Bert achieves
over 75% classiﬁcation accuracy. We report the utility of our
systems in Fig. Fig. 8(a) of the Appendix and more details in
Appendix A.
However, genetic data is highly sensitive in a personalized
way – even the nucleotide type at a speciﬁc position i in a
genome sequence can be related with certain type of genetic
decease or characterizes racial information [65] – and thus the
adversary is very likely to be interested in recovering the exact
nucleotide at a target position. From the disclosed nucleotide,
the adversary can further know the gender, race or other
privacy-critical information of the victim. For demonstration,
we deﬁne the mapping P as Pgenome,i : (w1, w2, . . . , wn) →
wi. In other words, the nucleotide at position i is assumed to
be sensitive.
B. Methodology
To realize the attack Apattern, we present the implementation
details on preparation of the external corpus and the architec-
ture of the attack model. In the following parts, we denote the
set of all possible values for sequence s as V (s).
1) Generate External Corpus: Knowing the generating rule
of the target plain text, the adversary can prepare the external
corpus via generating algorithms. A basic generating algorithm
generates batches of training samples by randomly sampling
from the possible values in V (x), i.e. the set of all possible
sentences.
2) Attack Model’s Architecture: Naively, the attack model
g can be designed as a fully-connected neural network that
has input dimension d and output dimension |V (wb . . . we)|,
i.e. the number of possible values of the sensitive segment.
However, |V (wb . . . we)| can be very large. For example, in the
Citizen case, the number of possible birth dates is near 40, 000.
As a result, the free parameters in the attack model will be of
large number, which further makes both the batch generation