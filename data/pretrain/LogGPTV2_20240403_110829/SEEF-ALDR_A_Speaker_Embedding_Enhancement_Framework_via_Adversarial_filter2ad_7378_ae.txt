ResNet-50
ResNet-50
ResNet-50
ResNet-50
ResNet-50
Loss Function
Softmax
Softmax
Softmax
Softmax
Softmax
Softmax
Softmax
Softmax
Test set
Voxceleb1
Voxceleb1
Voxceleb1
Voxceleb1
Voxceleb1-E
Voxceleb1-E
Voxceleb1-H
Voxceleb1-H
Voxceleb1
Softmax + Contrastive
Voxceleb1
Softmax + Contrastive
Voxceleb1
Softmax + Contrastive
Softmax + Contrastive
Voxceleb1
Softmax + Contrastive Voxceleb1-E
Softmax + Contrastive Voxceleb1-E
Softmax + Contrastive Voxceleb1-H
Softmax + Contrastive Voxceleb1-H
EER (%)
3.57
2.85
3.22
2.61
3.24
2.87
5.17
4.52
5.04
3.08
4.19
2.75
4.42
3.25
7.33
5.30
ğ¶ğ‘‘ğ‘’ğ‘¡
N/A
0.327
N/A
0.335
N/A
0.373
N/A
0.520
0.543
0.334
0.449
0.326
0.524
0.398
0.673
0.575
EER-IP
-
20.2%
-
19.0%
-
11.4%
-
12.6%
-
38.8%
-
34.4%
-
26.5%
-
27.7%
SEEF-ALDR: A Speaker Embedding Enhancement Framework via Adversarial Learning based Disentangled Representation
ACSAC 2020, December 7â€“11, 2020, Austin, USA
Figure 3: T-SNE Visualization of the Decoupled Features
based on the test set of Voxceleb1: (a) Features Extracted by
ğ¸ğ‘ (b) Features Extracted by ğ¸ğ‘’.
Figure 4: T-SNE Visualization of the Decoupled Features
based on the test set of Voxceleb2: (a) Features Extracted by
ğ¸ğ‘ (b) Features Extracted by ğ¸ğ‘’.
test set, new ğ‘‰ ğ‘œğ‘¥ğ‘ğ‘’ğ‘™ğ‘’ğ‘1âˆ’ ğ» test set and new ğ‘‰ ğ‘œğ‘¥ğ‘ğ‘’ğ‘™ğ‘’ğ‘1âˆ’ ğ¸ test set
[6]. Due to the limited number of speakers in the original ğ‘‰ ğ‘œğ‘¥ğ‘ğ‘’ğ‘™ğ‘’ğ‘1
test set (only 40 speakers), a good performance on it does not neces-
sarily indicate the model trained using the larger training set from
Voxceleb2 can perform well for â€œin-the-wildâ€ speaker verification.
Hence, the larger test sets ğ‘‰ ğ‘œğ‘¥ğ‘ğ‘’ğ‘™ğ‘’ğ‘1 âˆ’ ğ¸ and ğ‘‰ ğ‘œğ‘¥ğ‘ğ‘’ğ‘™ğ‘’ğ‘1 âˆ’ ğ», de-
rived from the entire Voxceleb1 dataset (including both the training
set and test set), can verify the performance of the model accu-
rately. It is worth mentioning that the testing set limits each test
pair to include the speakers with the same nationality and gender,
which requires the speaker verification model to learn more precise
speaker identity features to achieve better performance.
As shown in Table 4, SEEF-ALDR can reinforce the performance
of the original models, with an average of 23.8% improvement on
EER. We find that the overall improvement of [68] is less than
[6]. We speculate the reason might be that NetVLAD (NV) and
GhostVLAD (GV) proposed in [68] can optimize the extraction of
speaker embeddings, thus their EERs is already improved, much
better compared with those of [6]. Therefore, it does not leave lots of
optimization space for SEEF-ALDR, compared with [6]. Meanwhile,
the improvement on the larger testing datasets ğ‘‰ ğ‘œğ‘¥ğ‘ğ‘’ğ‘™ğ‘’ğ‘1 âˆ’ ğ¸ and
ğ‘‰ ğ‘œğ‘¥ğ‘ğ‘’ğ‘™ğ‘’ğ‘1âˆ’ ğ» always falls behind that on a relatively small testing
dataset Voxceleb1 for both [68] and [6]. Last but not least, the best
EER achieved by SEEF-ALDR as an optimization strategy over the
baseline [68] is 2.61%, which is better than the results of most state-
of-the-art research [1, 4, 6, 14, 27, 32, 46, 47, 68] (ranging from 2.85%
to 10.2%).
4.3 Performance of Disentangled
Representation
The proposed framework SEEF-ALDR relies on the purifying en-
coder ğ¸ğ‘ to extract speaker identity features and the eliminating en-
coder ğ¸ğ‘’ to extract identity-unrelated features. To demonstrate the
difference between the two decoupled features, we use T-distributed
Stochastic Neighbor Embedding (T-SNE) [39] to reduce the dimen-
sion of the high-level features and visualize them. Figure 3 shows
the distribution of speaker identity representation learned by the
purifying encoder and the eliminating encoder respectively, when
SEEF-ALDR is trained on dataset Voxceleb1 based on model ResNet-
34 [4]. As shown in Figure 3 (a), the identity of each speaker has
a dense set of clustered features, and there are clear classification
boundaries among the features of each identity. Hence, it demon-
strates that ğ¸ğ‘ can effectively extract identity-related information.
In contrast, in Figure 3 (b), the identity of each speaker is evenly
distributed in the feature space, and the features of each identity
overlap significantly with each other. Therefore, it shows that ğ¸ğ‘’
is capable of erasing the speaker identity information, thus con-
structing the identity-unrelated representation. Figure 4 shows the
distribution of speaker identity representation on a much larger
testing dataset Voxceleb2 based on model ResNet-50 [6], with 118
distinct identities, compared with 40 identities when using the
testing dataset Voxceleb1 in Figure 3. We can still observe simi-
lar distribution as that of Figure 3, which indicates the proposed
SEEF-ALDR is capable of decoupling identity related and unrelated
features effectively even on the dataset with much more identities.
4.4 Ablation Study
To further evaluate the contribution of each module in SEEF-ALDR,
a comprehensive ablation study has been conducted, with SEEF-
ALDR built upon ResNet-34 used in [6], that is, porting ResNet-34
into the framework of SEEF-ALDR. We use Voxceleb2 as the training
dataset, Voxceleb1 as the testing dataset, and the same metrics, the
detection cost function (ğ¶ğ‘‘ğ‘’ğ‘¡) and the Equal Error Rate (EER) as
in the above experiments to evaluate the performance of speaker
verification.
(in Equations 5) and ğ¿ğ‘ğ‘‘ğ‘£
SEEF-ALDR is mainly composed of three components: the pu-
rifying encoder, the eliminating encoder, and the reconstruction
decoder. Among them, the eliminating encoder contains two ad-
versarial losses: ğ¿ğ‘ğ‘‘ğ‘£
(in Equations 6).
Since the encoder is an essential component to extract speaker
embeddings, which is then used to perform the speaker verifica-
tion task, either the purifying encoder or the eliminating encoder
in SEEF-ALDR cannot be removed or replaced. Then we gradu-
ally add other components on top of the encoder and train several
variant models as shown in Table 5, e.g., the purifying encoder
and reconstruction decoder (ğ¸ğ‘ + ğ·ğ‘Ÿ ). Furthermore, it is possible
that a randomly-chosen vector for the eliminating encoder will
ğ‘ 
ğ‘’
(a)(b)(a)(b)ACSAC 2020, December 7â€“11, 2020, Austin, USA
Jianwei and Xiaoqi, et al.
Table 5: Ablation Study for SEEF-ALDR. EER-IP represents
the EER Improvement over the Baseline.
Branch
ğ¸ğ‘
ğ¸ğ‘ + ğ·ğ‘Ÿ
ğ¸ğ‘’
ğ¸ğ‘’ w/o ğ¿ğ‘ğ‘‘ğ‘£
ğ¸ğ‘’ w/o ğ¿ğ‘ğ‘‘ğ‘£
ğ¸ğ‘ + ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ‘‰ ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ + ğ·ğ‘Ÿ
SEEF-ALDR(ğ¸ğ‘ + ğ¸ğ‘’ + ğ·ğ‘Ÿ )
ğ‘’
ğ‘ 
EER (%)
5.04
4.42
49.79
30.46
34.57
4.35
3.08
ğ¶ğ‘‘ğ‘’ğ‘¡
0.543
0.509
0.999
0.999
0.999
0.511
0.334
EER-IP
-
12.3%
-887.9%
-504.4%
-585.9%
13.6%
38.8%
simply ignore the input spectrogram and output a random vector,
which might still help train the adversarial classifier more or less.
Therefore, to justify the contribution of our eliminating encoder,
we replace it with a random vector and evaluate the performance
of speaker verification accordingly, i.e., ğ¸ğ‘ + ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ‘‰ ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ + ğ·ğ‘Ÿ
in Table 5.
Table 5 shows the evaluation results of our ablation study. The
performance of using our purifying encoder only is considered as
the baseline performance for the following ablation experiments. A
structure similar to an autoencoder that uses only the decoder and
the encoder (i.e., ğ¸ğ‘ + ğ·ğ‘Ÿ ) can improve the performance of speaker
verification by 12.3% over the baseline. However, such improvement
is due to the capability of the autoencoder to extract embeddings
from the input spectrogram, but still far away from that of our
framework SEEF-ALDR. The eliminating encoder itself (i.e., ğ¸ğ‘’)
gets very low accuracy in speaker verification, i.e., downgrading
the baseline EER by 887.9%, which also indicates that it is effective
to extract identity-unrelated features, thus completely failing the
speaker verification task.
or ğ¿ğ‘ğ‘‘ğ‘£
nor ğ¿ğ‘ğ‘‘ğ‘£
We also evaluate the impact of each adversarial loss individually
, thus obtaining the performance
by removing either ğ¿ğ‘ğ‘‘ğ‘£
(drop by 504.%) and ğ¸ğ‘’ w/o
of speaker verification on ğ¸ğ‘’ w/o ğ¿ğ‘ğ‘‘ğ‘£
(drop by 585.9%). These results demonstrate that neither of the
ğ¿ğ‘ğ‘‘ğ‘£
ğ‘’
adversarial losses ğ¿ğ‘ğ‘‘ğ‘£
can accomplish the goal of elimi-
nating identity-related information individually. They collaborate
together and contribute to the training of an effective eliminating
encoder. Furthermore, we find that the performance of replacing
our eliminating encoder with random vectors (ğ¸ğ‘ + ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ‘‰ ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ
+ ğ·ğ‘Ÿ ) is almost equivalent to that of ğ¸ğ‘ + ğ·ğ‘Ÿ , i.e., 13.6% increase vs
12.3% increase, not able to significantly improve the performance
of speaker verification. Overall, leveraging the contribution of the
purifying encoder, the eliminating encoder, and the reconstruction
decoder together, SEEF-ALDR can archive superior improvement
of speaker verification, i.e., 38.8% increase over the baseline.
ğ‘ 
ğ‘ 
ğ‘ 
ğ‘’
ğ‘’
other semantic-rich information, such as language, emotion, envi-
ronmental noise, etc. Suppose SEEF-ALDR learns speaker identity
features and identity-unrelated features of Speaker A and Speaker
B, respectively. Then two new speeches can be synthesized by
the reconstruction decoder based on the cross-fusion the two de-
coupled features of them, e.g., identity features of Speaker A and
identity-unrelated features of Speaker B, and vice versa. Hence, a
flexible identity impersonation with the semantic-rich context can
be performed, though the quality of the synthesized new speech
depends on the optimization of the reconstruction process. We plan
to optimize the objective function of the reconstruction decoder of
SEEF-ALDR to produce a high-quality, semantic-rich speaker iden-
tity impersonation attack, and conduct a comprehensive analysis
of existing speaker authentication mechanisms against such novel
attack.
Reinforcing sound event detection based on SEEF-ALDR. The
proposed framework SEEF-ALDR also provides a potential solu-
tion to improve the performance of sound or audio event detection.
Sound contains a lot of information about physical events that occur
in our surrounding environment. Through sound event detection,
it is possible to perceive the context of the sound, e.g., busy road,
office, grocery, etc., and at the same time recognize the sources of
the sound, e.g., car passing-by, door opening, footsteps, etc. Further-
more, sound event detection also has great potential to be used in
a variety of safety-related scenarios, such as autonomous driving,