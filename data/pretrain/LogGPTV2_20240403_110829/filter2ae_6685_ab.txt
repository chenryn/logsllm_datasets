        hostname_list = conf_dict["hostname_list"]
        ip_net_list = conf_dict["ip_net_list"]
        keyword_list = conf_dict["keyword_list"]
        ext_dict = conf_dict["ext_dict"]
        max_length = conf_dict["max_length"]
        min_length = conf_dict["min_length"]
        max_score = conf_dict["max_score"]
        min_score = conf_dict["min_score"]
        max_rowsnumber = conf_dict["max_rowsnumber"]
    domain_list = list()
    suffix_list = list()
    for hostname in hostname_list:
        domain = tldextract.extract(hostname).domain
        if domain not in domain_list:
            domain_list.append(domain)
        suffix = tldextract.extract(hostname).suffix
        if suffix not in suffix_list:
            suffix_list.append(suffix)
    SVM_X_POSITIVE = list()
    SVM_Y_POSITIVE = list()
    HMM_DATA = list()
    SVM_X_NEGATIVE = list()
    SVM_Y_NEGATIVE = list()
遍历敏感信息样本目录，针对其中的每一个样本文件，收集如上一个脚本中收集的元信息数据，与配置列表进行对比。  
将命中主机和IP地主数量，未命中主机数量，命中敏感关键字数量，文件字节数，文本行数和扩展名等统计信息结合在一起，组成矩阵。  
再给予一个识别标志数据，表示这个类别是敏感文件类别。
    for filename in os.listdir(POSITIVE_SAMPLE_PATH):
        with open('{}{}{}'.format(POSITIVE_SAMPLE_PATH, '/', filename), 'r', encoding='utf8', errors='ignore') as f:
            sample_str = f.read().lower()
        hit_ip_list = list()
        hit_hostname_list = list()
        not_hit_hostname_list = list()
        hit_keyword_dict = dict()
        hit_keyword_count = 0
        ext_id = get_filename_ext_id(filename, ext_dict)
        rowsnumber = sample_str.count('\n')
        sample_length = len(sample_str)
        for hostname in get_affect_assets(sample_str):
            if is_ip(hostname):
                if is_hit_ip(hostname, ip_net_list):
                    hit_ip_list.append(hostname)
            else:
                domain = tldextract.extract(hostname).domain
                suffix = tldextract.extract(hostname).suffix
                if domain in domain_list and suffix in suffix_list:
                    hit_hostname_list.append(hostname)
                else:
                    not_hit_hostname_list.append(hostname)
        for keyword in keyword_list:
            if keyword in sample_str:
                hit_keyword_dict.update({keyword: sample_str.count(keyword)})
                hit_keyword_count += hit_keyword_dict[keyword]
        SVM_X_POSITIVE.append([len(hit_hostname_list), len(hit_ip_list), len(not_hit_hostname_list), hit_keyword_count, sample_length, rowsnumber, ext_id])
        SVM_Y_POSITIVE.append(1)
        HMM_DATA.append(numpy.array([[len(hit_hostname_list)], [len(hit_ip_list)], [len(not_hit_hostname_list)], [hit_keyword_count], [sample_length], [rowsnumber], [ext_id]]))
对于正常信息样本目录，也执行基本相同的操作，然后给予一个相反的识别标志数据，表示这个类别是正常文件类别。
    for filename in os.listdir(NEGATIVE_SAMPLE_PATH):
        # 与前一个循环相同的代码。。。
        SVM_X_NEGATIVE.append([len(hit_hostname_list), len(hit_ip_list), len(not_hit_hostname_list), hit_keyword_count, sample_length, rowsnumber, ext_id])
        SVM_Y_NEGATIVE.append(0)
利用上面计算得到的数据，分别建立隐马尔可夫(HMM)模型和支持向量机(SVM)模型，并将它们序列化后保存到本地。
    STACKED = numpy.vstack(tuple(HMM_DATA))
    model_hmm = hmm.GaussianHMM(n_components=3, covariance_type='full', n_iter=100)
    model_hmm.fit(STACKED)
    with open('model_hmm.pkl', 'wb') as f:
        joblib.dump(model_hmm, f)
    OUT_X = numpy.array(SVM_X_POSITIVE + SVM_X_NEGATIVE)
    OUT_Y = numpy.array(SVM_Y_POSITIVE + SVM_Y_NEGATIVE)
    (train_x, test_x, train_y, test_y) = sklearn.model_selection.train_test_split(OUT_X, OUT_Y, random_state=1, train_size=0.8)
    model_svm = svm.SVC(C=0.5, kernel='linear', gamma=2.5, decision_function_shape='ovr')
    model_svm.fit(train_x, train_y.ravel())
    with open('model_svm.pkl', 'wb') as f:
        joblib.dump(model_svm, f)
使用隐马尔可夫(HMM)模型对敏感信息样本文件逐个进行评分，然后将阈值保存到JSON配置文件中。
    for filename in os.listdir(POSITIVE_SAMPLE_PATH):
        # 与前一个循环相同的代码。。。
        out_ndarray = numpy.array([[len(hit_hostname_list), len(hit_ip_list), len(not_hit_hostname_list), hit_keyword_count, sample_length, rowsnumber, ext_id]]).T
        score = model_hmm.score(out_ndarray)
        if max_score == 0:
            max_score = score
        if score > max_score:
            max_score = score
        if score  max_rowsnumber:
            print('    行数超出预计范围。')
            continue
        if sample_length > max_length or sample_length  1:
            out_ndarray = numpy.array([[len(hit_hostname_list), len(hit_ip_list), len(not_hit_hostname_list), hit_keyword_count, sample_length, rowsnumber, ext_id]]).T
            score = model_hmm.score(out_ndarray)
            if not (score > max_score or score < min_score):
                print('    已命中HMM模型范围，继续使用SVM模型进行识别。。。')
                SVM_X = list()
                SVM_X.append([len(hit_hostname_list), len(hit_ip_list), len(not_hit_hostname_list), hit_keyword_count, sample_length, rowsnumber, ext_id])
                if not int(model_svm.predict(SVM_X)):
                    print('    确认为正常信息文件。')
                else:
                    print('    疑似为敏感信息文件！')
            else:
                print('    未命中HMM模型范围。')
                continue
        else:
            print('    未命中敏感主机地址和关键字。')
            continue
运行效果演示：
关于对接Hawkeye系统：
假设MongoDB数据库连接信息如下：
    USERNAME = 'hawkeye'
    PASSWORD = 'hawkeye'
    HOST = '127.0.0.1'
    PORT = 27017
    AUTHSOURCE = 'hawkeye'
在Hawkeye系统中，每一条数据记录，体现为"result"集合中的一条"document"数据。  
在前端Web管理页面中，显示为待审核的数据，在对应"document"中，其"desc"域是为空，或者说不存在的，在Python里体现为"None"  
利用这个属性，将待审核条目中需要用到的数据查询出来进行处理。
    client = pymongo.MongoClient('mongodb://{}:{}@{}:{}/?authSource={}'.format(USERNAME, PASSWORD, HOST, PORT, AUTHSOURCE))
    collection = client['hawkeye']['result']
    cursor = collection.find(filter={'desc': None}, projection=['link', 'code'], no_cursor_timeout=True, batch_size=1)
PS:
这里有一点要注意，因为"no_cursor_timeout"参数置为"True"了，所以在遍历完成后切记应该执行"cursor.close()"显式关闭链接。
这时候，变量"cursor"就成为一个迭代器，从里面获取数据就好了。从"link"域提取文件名，从"code"域提取BASE64编码表示的文件实际内容，解码一下就行。
    filename = doc['link']
    sample_str = base64.b64decode(doc['code']).decode(encoding='utf-8', errors='ignore').lower()
审核过程的最后，对于一个确认为安全的文件，执行以下操作，将其置为"安全--忽略"状态。
    collection.update_one(filter={'_id': doc['_id']}, update={'$set': {'security': 1, 'ignore': 1, 'desc': ''}})
**总结**
我们用来作为检测依据的各种特征，都来源于对安全和非安全样本的学习，通过对样本集的各种特征的收集和计算，得出阈值，再做适当扩展。也就是说，只有处在阈值范围内的文件，程序才能够进行有效识别，超出了认知盲区的部分，是不能随便给出结论的。  
结合数据源特性，程序形成的效果就是，将确定一定不是敏感信息的告警置为安全忽略状态。其他的则留在待审标记下，由人工来做判断。
想象一下，当人工在审核告警信息的时候，思维模式大概分为两个部分：  
一部分是，一眼看上去就知道一定是误报的告警，果断忽略掉。另一部分是，有的告警文件，内容上比较难以辨别，需要仔细检查一小会儿才能确定。
程序的运行其实也是相同的过程，机器学习可以在极短的时间内将绝大部分一眼看上去就是误报的告警排除掉，剩下就是那些占比极小的，需要仔细检查一会儿才能确定的文件，将它们留在原地，由人工来进行判断，实现在节省巨量不必要的时间投入的同时，准确识别出那些可能会带来巨大损失的信息泄露隐患。
我们的样本文件集，数量越多，质量越好，产出的机器学习模型也就越高效和精准。就好像人读书越多越聪明一样。  
在前期，当我们的样本集具备基本的可用性的时候，实际上就已经能够很好的运行了，随着工作的进行，收集到的样本会越来越多，对机器学习模型的训练也就越来越容易。
**结束**
因为我自身的工作主要是渗透测试方面。程序开发经验和对机器学习算法的理解都十分匮乏，所以历时很久，参考了网上各路大神的资料之后，反复测试迭代，才搞到现在这个效果，能够很好的解决自身的这个工作需求。  
但程序本身还存在很多不足，后面打算再做优化，尝试添加其他的算法和思路，以期打磨的更加完善。