potential peers. RanSub, meanwhile, uses an overlay tree
to perform a specialized type of aggregation, proceeding in
periodic phases that Collect candidate sets of information to
the root, and then Distribute uniformly randomized candi-
date sets to all peers.
To run Gatling on BulletPrime, we ﬁrst had to prepare
it to run in the simulator and implement an impact score.
We wrote an 85 line simulated application driver that pro-
vides the basic functionality of having the source node dis-
tribute data to others and having the client nodes download
and participate in the ﬁle-sharing protocol. We chose for
the impact score a performance metric which captures the
progress of node downloads; namely the number of blocks
of the ﬁle downloaded. To satisfy the requirement that a
higher score indicates more attack impact, we instead use
the total number of blocks remaining before completion.
We had to modify BulletPrime slightly, adding the 8-line
impact score function, because it did not expose enough
information to the simulated driver to compute the score.
We simulated BulletPrime with 100 nodes disseminating a
50MB ﬁle. We use a small 5 sec tw as nodes download
blocks quickly, starting nearly at the beginning of the simu-
lation. Due to some variable system performance, we set na
to 5, allowing Gatling to explore a few instances per mes-
sage type. We set ∆ to be zero for all our experiments to
ﬁnd as many attacks as possible.
Assertions and segmentation faults: As we began to use
Gatling on the BulletPrime implementation, we encoun-
tered nodes crashing due to the fact that BulletPrime as-
sumes peers to act correctly. For example, we found nodes
crashing due to assertions and segmentation faults when re-
ceiving a malicious FileInfo message. This message deﬁnes
the ﬁle and block size and is created by the source.
In-
termediate nodes that forward the message can lie about its
contents when passing it along. We found another crash sce-
nario when a malicious node requests a non-existing block,
causing the recipient to crash by assertion attempting to re-
trieve the block. We implemented checks in the code to
prevent crashing and we disabled any attack on FileInfo for
BulletPrime 
Distributor 
Mesh 
RanSub 
OverlayTree 
TCP 
g
n
i
n
i
a
m
e
R
s
k
c
o
l
B
f
o
#
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
 0
No Attack
Lie Data
Delay Data
Drop Data
Delay Diff
Dup Join
 50
 100
 150
 200
 250
Simulation Time (s)
g
n
i
n
i
a
m
e
R
s
k
c
o
l
B
f
o
#
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
 0
No Attack
Delay JoinAccept
Divert JoinReject
Dup Collect
Drop Dist
Drop Dist + Dup Collect
 50
 100
 150
 200
 250
Simulation Time (s)
Figure 5. BulletPrime design
Figure 6. Remaining blocks for the attacks found on BulletPrime
g
n
i
n
i
a
m
e
R
s
k
c
o
l
B
f
o
#
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
No Attack
Lie Data
Delay Data
Drop Data
Delay Diff
Dup Join
No Attack
Delay JoinAccept
Divert JoinReject
Drop Dist
Dup Collect
Drop Dist + Dup Collect
g
n
i
n
i
a
m
e
R
s
k
c
o
l
B
f
o
#
 3500
 3000
 2500
 2000
 1500
 1000
 500
 0
 0
 20
 40
 60
 80
 100  120
 0
 20
 40
 60
 80
 100  120
Time (s)
Time (s)
Figure 7. Remaining blocks for the attacks found on BulletPrime on PlanetLab
further Gatling simulations.
Fig. 6 shows the performance of the system under the
attacks we discover. To give a baseline comparison, we also
show the benign scenario when there is no attack. We have
found attacks against four of the ﬁve services.
Distributor service: We found several attacks on Data
messages. Lying on the id ﬁeld of a Data message degrades
the performance signiﬁcantly. We also found dropping or
delaying Data causes performance degradation.
BulletPrime service: Furthermore, Gatling found a de-
laying attack on the Diff message which causes a perfor-
mance penalty, since peers cannot request the block until
receiving the Diff message.
Mesh service: Gatling also reported an attack vector that
is a combination of 1) duplicate Join message and divert
the second copy to a random destination, 2) delay JoinAc-
cepted message for 0.5s, and 3) divert JoinRejected mes-
sage to a random destination. Gatling computed the most
effective minimal combination found that all actions are ef-
fective even when they are used alone, however the com-
bination of the three was the most effective. We show the
individual attacks in Fig. 6.
RanSub service: Gatling found an attack which was a
combination of dropping Distribute messages that are dis-
seminated from the root toward the leaves over the control
tree and also duplicating Collect messages that are collected
from leaves towards the root. Gatling found that both ac-
tions alone degrade performance and furthermore dropping
Distribute messages causes nodes to never be able to down-
load a number of blocks.
While some of the attacks found on BulletPrime were ex-
pected, such as delaying or dropping data messages, less ob-
vious was the impact of the attacks on the Mesh and RanSub
services. Although BulletPrime gains nice mathematical
properties by using RanSub, it seems that a BulletPrime im-
plementation robust to insider attacks may be better served
by a gossip service re-design. As an extra beneﬁt, Gatling
also identiﬁed several cases where insiders can crash system
nodes due to a lack of input validity checking.
To validate that the attacks are not a result of lack of
ﬁdelity in the simulator we ran real-world experiments with
the discovered attacks on the PlanetLab testbed, with the
same number of nodes and ﬁle size. We conﬁrmed that all
attacks have a similar effect as in the simulator and we show
graphs in Fig. 7.
5 Results
We further validate the Gatling design by applying it to
ﬁve systems with different application goals and designs.
Speciﬁcally, we evaluate the Vivaldi [19] virtual coordinate
system, the Chord lookup service and distributed hash ta-
ble (DHT) [51], and two multicast systems: ESM [17] and
Scribe [49]. Chord, DHT, and Scribe were previously im-
plemented for Mace; we implemented Vivaldi and ESM ac-
cording to published papers. We set the number of mali-
cious nodes to be 20% and we select malicious nodes ran-
domly.
Gatling found performance attacks in each system tested,
taking from a few minutes to a few hours to ﬁnd each attack.
Gatling was run on a 2GHz Intel Xeon CPU with 16GB of
RAM. Gatling processes are CPU bound, so parallelizing
the search could further reduce the search time. We discov-
ered 41 attacks in total, however due to lack of space we
only present in detail a subset of attacks that illustrate the
capabilities of Gatling. In Table 1, we summarize all the
attacks.
5.1 Vivaldi
System description. Vivaldi [19] is a distributed sys-
tem that provides an accurate and efﬁcient service that al-
lows hosts on the Internet to estimate the latency to arbitrary
hosts without actively monitoring all of the nodes in the net-
work. The system maps these latencies to a set of coordi-
nates based on a distance function. Each node measures the
round trip time (RTT) to a set of neighbor nodes, and then
determines the distance between any two nodes. The main
protocol consists of Probe messages sent by a node to mea-
sure their neighbors RTTs and then a Response message
from these neighbors is sent back with their current coordi-
nates and local error value.
Impact score. We use the prediction error [19]
the system predicts the ac-
which describes how well
tual RTT between nodes. Prediction error is deﬁned as
median(|RT T i,j
Est is node i’s
estimated RTT for node j given by the resulting coordinates
and RT T i,j
Act is the most recently measured RTT.
Est − RT T i,j
Act|), where RT T i,j
Experimental setup. We simulated 400 nodes and ran-
domly assign RTT values for each node from the KING
data set [23] which contains pair-wise RTT measurements
of 1740 nodes on the Internet. Malicious nodes start their
attacks from the beginning of the simulation. We set tw to
be 5 sec and na to be 5.
Attacks found using prediction error. We found ﬁve
attacks using the prediction error impact score. In Fig. 8 we
show how each attack affects Vivaldi prediction error over
time. The Overﬂow attack is omitted because the predic-
tion error was NaN (not a number). As a baseline we also
present Vivaldi when there are no attacks, which we ﬁnd
converges to a stable set of coordinates with 17 ms of error.
Overﬂow. We ﬁrst found two variations of an attack
where malicious nodes lie and report DBL MAX for their
coordinate and their local error, respectively. In both cases
the result is that honest nodes compute their coordinates as
NaN. We implemented safeguards to address the overﬂow.
Inﬂation, oscillation, deﬂation. We then found three pre-
viously reported attacks against Vivaldi [58]. First, known
as inﬂation is a lying attack where malicious nodes lie about
their coordinates, providing larger than normal values from
the spanning set without causing overﬂow. Second, known
as deﬂation attack, occurs when where malicious nodes
drop outgoing probes, thereby never updating their own co-
ordinates. The third, known as the oscillation attack, occurs
where attackers set their coordinates to random values. This
is a very effective attack in which nodes cannot converge
and the prediction error remains high, about 250,000 ms.
5.2 Chord
System description. Chord [51] is an overlay routing
protocol that provides an efﬁcient lookup service. Each
node has an identiﬁer that is based on consistent hashing,
and is responsible for a range of keys that make up that
space. Nodes in Chord construct a ring and maintain a set
of pointers to adjacent nodes, called predecessors and suc-
cessors. When a node i wants to join the ring, it will ask
a node already in the ring to identify the correct predeces-
sor and successor for i.
i then contacts these nodes and
tells them to update their information. Later, a stabilization
procedure will update global information to make sure i is
known by others in the ring.
Impact score. We use an impact score which measures
the progress of forming a correct ring. Since Chord cor-
rectness depends on being able to reach every node by fol-
lowing the successor reference around the ring, we use as
the impact score the average number of nodes each node
can reach by following each node’s successor. For a benign
case, the impact score should be equal to the total number
of nodes.
Experimental setup. We simulate Chord with 100
nodes. Malicious actions start immediately as the goal of
Chord is to construct a properly functioning ring and thus
we want to ﬁnd attacks on that construction process. We set
tw to be 2 sec as ring construction takes only 10 sec in the
benign case and set na to 5.
Attacks found using number of reachable nodes. We
found six attacks against the Chord protocol. In Fig. 9 we
show the effects of the attacks and illustrate the resulting
ring for one attack. As a baseline we verify that when there
is no attack all 100 nodes are able to form a ring in less than
10 sec.
Dropping attacks. We found three attacks where ma-
licious nodes drop responses or do not forward replies to
requests for predecessor and successor information. The
attacks prevent a correct ring from forming. We show in
Fig. 9(a) (Drop Find Pred, Drop Get Pred, and Drop Get
Pred Reply) that when malicious nodes drop predecessor
related messages, less than half the nodes are reachable.
Lying attacks. We found three lying attacks that prevent a
System
Metric
Used
BulletPrime
Number of
Blocks Remaining
Vivaldi
Prediction Error
Attack
Name
Lie Data
Delay Data
Drop Data
Delay Diff
Dup Join
Delay JoinAccept
Divert JoinReply
Drop Dist
Dup Collect
Overﬂow
Inﬂation
Oscillation
Delay
Deﬂation
Drop Find Pred
Drop Get Pred
Attack
Description
Lie data message distribution
Delay data message distribution
Drop data message distribution
Delay diff information
Duplicate join message and send copy to another
Delay join accepted
Send join rejected to another node
Drop information distributed
Dup information collected
Lie about coordinates, setting them to maximum value
Lie about coordinates, setting them to large values
Lie about coordinates, setting them to random values
Delay probe reply messages 2s
Do not initiate request (Drop probes)
Drop query to ﬁnd predecessor