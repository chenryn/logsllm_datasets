not be thwarted with the discussed policies is the page-
deduplication attack [14]. In this attack, an attacker only has
to be in control over the content of one page to deduce
if a page with the same content already exists in memory.
Allocating a large array still gives an attacker control over at
least one page. To prevent a page deduplication attack from
being successful, we have to ensure that an attacker cannot
deterministically choose the content of an entire page. One
possible implementation is to make the mapping between the
array index and the underlying memory unpredictable by using
a random linear function.
1 (function() {
2 var wpn = window.performance.now, last = 0;
3 window.performance.now = function() {
4 var fuzz = Math.floor(Math.random() * 1000), //1ms
5
6 var t = now - now % fuzz;
7 if(t > last) last = t;
8 return last / 1000.0;
9 };})();
now = Math.floor(wpn.call(window.performance)*1000);
Listing 4: Fuzzy time [46] applied to the high-resolution timing
API with a 1 ms randomization.
1
ζ(2) = 6
We overwrite the array to access memory location f (x)
when accessing index x with f (x) = ax + b mod n where
a and b are randomly chosen and n is the size of the
ArrayBuffer. Furthermore, a and n must be co-prime to
generate a unique mapping from indices to memory locations.
To ﬁnd a suitable a, we can simply choose a random a in
the array constructor and test whether gcd(a, n) = 1. As the
probability of two randomly chosen numbers to be co-prime
is
π2 ≈ 61% (for n → ∞) [30], this approach is
computationally efﬁcient. That is, the expected value is 1.64
random choices to ﬁnd two co-prime numbers (for n → ∞).
As a and b are inaccessible to an attacker, the mapping
ensures that an attacker cannot predict how data is stored in
physical memory. An attacker can also not reverse a and b
because there is no way to tell whether a guess was correct, as
buffer ASLR and preloading prevent any reliable conclusions
from page faults. Thus, an attacker cannot control the content
of a page, thwarting page deduplication attacks [14].
2) Accurate Timing: Many attacks within the browser re-
quire highly accurate time measurements. Especially microar-
chitectural attacks require time measurements with a resolution
in the range of nanoseconds [33], [15], [40], [13]. Such high-
resolution timestamps were readily available in browsers [33]
but have been replaced by lower resolution timestamps in
all modern browsers in 2015 [2], [6], [9], [25]. However,
Schwarz et al. [40] showed that it is still possible to get a
nanosecond resolution from these timestamps by exploiting
the underlying high-resolution clock.
a) Low-resolution timestamps: As a policy, we can
simply round the result of the high-resolution timestamp
(window.performance.now) to a multiple of 100 ms.
This is exactly the same behavior as implemented in the
Tor browser. Thus, we achieve the same protection as the
Tor browser, where the recoverable resolution is only 15 µs,
rendering it useless for many attacks [40].
b) Fuzzy time: A different approach is to apply fuzzy
time to timers [46]. In addition to rounding the timestamp,
fuzzy time adds random noise to the timestamp to prevent
exact timing measurements but still guarantees that the times-
tamps are monotonically increasing. Kohlbrenner et al. [18]
implemented this concept in Fuzzyfox, a Firefox fork. We can
achieve the same results using a simple policy that implements
a variant of the algorithm proposed by Vattikonda et al. [46]
shown in Listing 4, without requiring constant maintenance of
a browser fork.
10
Fig. 7: Edge thresholding to distinguish whether the function
fslow takes longer than ffast. The difference between the
execution times is less then the provided resolution.
We evaluated our policies for low-resolution timestamps
and fuzzy time by creating two functions with a runtime
below the resolution of the protected high-resolution timer.
Using edge thresholding [40], we tried to distinguish the
two functions based on their runtime. For the evaluation, we
rounded timestamps to a multiple of 1 ms and used a 1 ms
randomization interval for the fuzzy time. The two functions
fslow and ffast, which we distinguish, have an execution time
difference of 300 µs. Figure 7 shows the results of this evalu-
ation. If no policy is applied to the high-resolution timer, the
functions can always be distinguished based on their runtime.
With the low-resolution timestamp and edge thresholding, the
functions are correctly distinguished in 97 % of the cases, as
the underlying clock still has a resolution in the range of
nanoseconds. When fuzzy time is enabled, the functions are
correctly distinguished in only 65 % of the cases, and worse, in
27 % of the cases the functions are wrongly classiﬁed, i.e., the
faster-executing function is classiﬁed as the slower function.
Figure 8 shows the result of fuzzy time on the JavaScript
keystroke detection attack by Lipp et al. [20]. Without fuzzy
time, it can be clearly seen whenever the user taps on the touch
screen of a mobile device (Figure 8a). By enabling the fuzzy
time policy, the attack is fully prevented, and no taps can be
seen in the trace anymore (Figure 8b).
[9],
3) Multithreading: As the resolution of the built-in high-
resolution timer has been reduced by all major browsers [2],
[6],
[25], alternative timing primitives have been
found [18], [40], [13]. Although several new timing primitives
work without multithreading [18], [40], only the timers using
multithreading achieve a resolution that is high enough to
mount microarchitectural attacks [13], [40], [47], [20].
a) WebWorker polyﬁll: A drastic—but effective—
policy is to prevent real parallelism. To achieve this, we can
unprotectedtimestamproundingfuzzytime0%50%100%10097650380027bothcorrectfslowmisclassiﬁedffastmisclassiﬁed(a) Without Chrome Zero.
(a) Without Chrome Zero.
(b) With Chrome Zero.
Fig. 8: Without Chrome Zero, taps can be clearly seen in the
attack by Lipp et al. [20] (Figure 8a). With Chrome Zero, the
attack is prevented and no taps are visible (Figure 8b).
(a) Without Chrome Zero.
(b) With Chrome Zero.
Fig. 9: Running the attack by Vila et al. [47] shows keystrokes
among other system and browser activity (Figure 9a). With
Chrome Zero in place, the postMessage timings are delayed
and thus keystrokes cannot be detected anymore (Figure 9b).
completely replace WebWorkers by a polyﬁll intended for un-
supported browsers. The polyﬁll [29] simulates WebWorkers
on the main thread, trying to achieve similar functionality
without support for real parallelism. Thus, all attacks relying
on real parallelism [13], [40], [47], [20] do not work anymore.
b) Message delay: A different policy to speciﬁcally
prevent certain timing primitives [40] and attacks on the
browser’s rendering queue [47] is to delay the postMessage
function. If the postMessage function randomly delays
messages (similar to Fuzzyfox [18]), the attack presented by
Vila et al. [47] does not work anymore, as shown in Figure 9.
(b) With Chrome Zero.
Fig. 10: Using SharedArrayBuffer in combination with
web worker to build a high-resolution timing primitive as
proposed by Gras et al. [13] and Schwarz et al. [40]. Without
Chrome Zero, cache hits and misses are clearly distinguishable
(Figure 10a). Conﬁguring Chrome Zero to delay accesses to
the SharedArrayBuffer leads to a uniform distribution in
timings, thwarting the attacks (Figure 10b).
4) Shared Data: SharedArrayBuffer is the only
data type which can be shared across multiple workers in
JavaScript. This shared array can then be abused to build a
high-resolution timer. One worker periodically increments the
value stored in the shared array while the main thread uses
the value as a timestamp. This technique is the most accurate
timing primitive at the time of writing, creating a timer with
a resolution in the range of nanoseconds [13], [40].
a) No SharedArrayBuffer: At the time of writ-
ing,
the SharedArrayBuffer is by default deactivated
in modern browsers. Thus, websites should not rely on this
functionality anyway, and a policy can simply keep the
SharedArrayBuffer disabled if vendors enable it.
b) Slow SharedArrayBuffer: On our Intel
i5-
6200U test machine, we achieve a resolution of 0.77 ±
0.01 ns with the SharedArrayBuffer timing primitive.
This allows use to clearly distinguish cache hits from cache
misses (Figure 10a), and thus mount the attacks proposed by
Schwarz et al. [40] as well as Gras et al. [13]. To protect users
from these attacks, our policy randomly delays the access to
the SharedArrayBuffer. Using this policy, we reduce the
resolution to 4215.25 ± 69.39 ns, which is in the same range
as the resolution of the native performance.now() timer.
Thus, these microarchitectural attacks, which require a high-
resolution timer, do not work anymore as shown in Figure 10b.
5) Sensor API: As mobile devices are equipped with many
sensors, JavaScript provides several APIs allowing websites
to access sensor data, e.g., accelerometer, ambient light, or
battery status. While some of those interfaces allow developers
11
0.20.40.60.811,7001,7501,8001,850taptaptaptapRuntime[s]Delta[counter]00.10.20.30.40.50.60.7600700800900taptaptaptapRuntime[s]Delta[counter]2,5602,5802,6002,6202,64000.511.52Runtime[ms]Delta[ms]2,8602,8802,9002,9202,9400123Runtime[ms]Delta[ms]4004505005506006500100200300Accesstime[bufferincrements]Numberofcasescachehitcachemiss8501,0501,2501,4501,6501,850051015Accesstime[bufferincrements]Numberofcasescachehitcachemissto build more functional and user-friendly applications, they
also facilitate leakage of sensitive information. While modern
browsers explicitly ask the user for permission if the running
websites want to access the user’s geolocation, access to other
APIs is silently permitted.
a) Battery Status API: After Olejnik et al. [32] showed
the potential privacy risk of the HTML5 Battery Status API as
a tracking identiﬁer, Firefox disabled the interface with version
52 [8]. However, Chrome still offers unrestricted access to
this API without asking for permission. Thus, we introduce
a policy allowing to either randomize the properties of the
battery interface, to set them to ﬁxed values, or to disable the
interface entirely. With this policy, the Battery Status API can
not be used as a tracking identiﬁer anymore.
b) Ambient Light Sensor: The ambient light sensor can
be used to infer user PINs [42] or to recover browsing history
information [31]. While the API needs to be enabled manually
in the Chrome browser, it is enabled by default in the Firefox
browser. By introducing a policy that either returns constant
values for the ambient light sensor, or disables the interface,
an attacker is unable to perform these attacks.
c) Device Motion, Orientation, and Acceleration: The
motion sensor data and orientation sensor data of mobile web
browsers can be exploited to infer user PIN input [23]. Both
are available to websites without any permissions. In order
to mitigate such attacks, we introduce a policy that allows to
spoof the sensor data or to prohibit access entirely.
B. Exploits
Although exploits are out-of-scope for the permission sys-
tem (cf. Section II-C), we investigate the (side-)effect of our
policies on JavaScript exploits. For this, we investigate CVEs
that are exploitable via JavaScript and were discovered since
Chrome 49, as Chrome Zero requires Chrome 49 and later.
To evaluate whether Chrome Zero protects a user from a
speciﬁc exploit, we ﬁrst reproduce the exploit without Chrome
Zero and then activate Chrome Zero to check whether the
exploit still works. We reproduced all 12 CVEs2 for the
Chrome JavaScript engine, which were discovered since 2016
for Chrome 49 or later and for which we could ﬁnd proof-of-
concept implementations online. All of the 12 CVEs lead to
either a crash of the current tab or to information leakage.
With Chrome Zero in place, half of them are prevented,
leaving only 6 CVEs that are still exploitable. The prevented
CVEs all rely on at least one object which we modify (e.g.,
ArrayBuffer) and thus do not work with the modiﬁed ob-
ject. Furthermore, we expect that actual remote code execution
using the working exploits gets more complicated if policies
such as array index randomization or buffer ASLR are in place.
Thus, Chrome Zero provides additional protection against 0-
days without requiring explicit policies. Creating policies to
speciﬁcally target CVEs is left to future research.
VII. USABILITY EVALUATION
In this section, we analyze the usability impact of Chrome
Zero by performing a performance analysis and a double-
2CVE-2016-1646, 1653, 1665, 1669, 1677, 5129, 5172, 5198, 5200, 9651,
2017-5030, 5053
Fig. 11: The computation overhead of the JavaScript engine
while loading each of the Alexa Top 10 websites without
protection and with protection level high. The performance
overhead is negligible for most websites.
TABLE IV: RESULTS OF THE JETSTREAM BENCHMARK.
Latency
Throughput
Total
Without Chrome Zero With Chrome Zero
71.46 ± 4.43
220.45 ± 6.80
134.90 ± 5.96
71.33 ± 2.43
214.71 ± 3.50
132.81 ± 2.92
The higher the score, the better the performance.
blind user study. We ﬁrst analyze how many websites use
functionality which is also used in microarchitectural and
side-channel attacks. We then analyze the performance impact
on the Alexa Top 10 websites. Finally, we show whether
the protection mechanism has any impact on the browsing
experience, i.e., whether there are pages that do not work as
expected anymore, for the Alexa Top 25 websites.
A. Performance
We evaluated the performance overhead of Chrome Zero
in both micro and macro benchmarks.
First, we evaluated the impact of Chrome Zero on the
loading time of a page. We measured a page loading latency
between 10.64 ms if no policy is active, and 89.08 ms if
policies protecting against all microarchitectural and side-
channel attacks are active. As on every page load the current
policies are loaded and injected into the current tab, the latency
grows linearly with the number of policies, and delays the
actual rendering of the page. On average, we measured a
latency of approximately 3.4 ms per active policy, i.e., every
policy delays the loading of a newly opened page by 3.4 ms.
Second, we investigated the overhead for Chrome’s
JavaScript engine by using the internal proﬁler. Figure 11
shows the overhead for the Alexa Top 10 websites. The runtime
increase of the JavaScript engine had a median of 1.82 %,
which corresponds to only 16 ms.
Finally, we used the JetStream [3] browser benchmark,
which is developed as part of the WebKit engine. We measure
a performance overhead of 1.54 % when using Chrome Zero.
Table IV shows the detailed scores of the benchmark.
12
−0.6%−0.4%−0.2%0%0.2%0.4%0.6%baidu.comfacebook.comgoogle.co.ingoogle.comqq.comreddit.comtaobao.comwikipedia.orgyahoo.comyoutube.comJavaScriptcomputationoverheadinpercentDomainnoprotectionprotectionlevelhighA reason for the low overhead of Chrome Zero is the
JavaScript Just-In-Time (JIT) compiler. Chrome’s JIT consists
of several compilers, producing code on different optimization
levels [39]. As the code is continuously optimized by the JIT,
our injected functions are compiled to highly efﬁcient native
code, with a negligible performance difference compared to
the original native functions. The results of our benchmarks
show that Chrome Zero does not have a visible impact on the
user experience for an everyday usage.
B. Compatibility
For Chrome Zero to be usable on a day-to-day basis, it is
important that the majority of websites is still usable if policies
are active. We analyzed the Alexa Top 100 websites with a
protection level of high, the second highest protection level (cf.
Table III). Out of the 100 pages, all of them used JavaScript,
and 57 relied on functions for which the protection level high
deﬁnes policies. For all these pages, we veriﬁed that Chrome
Zero did not cause any error when testing some of the site’s
basic functionality. For a thorough evaluation, we conducted
a double-blind user study to test whether Chrome Zero has an
impact on the browsing experience. We designed the study to
have 24 participants to have a maximum standard error below
15 % at a conﬁdence level of 85 %. The 24 participants, which
we recruited by advertising it through word-of-mouth, had
different backgrounds, ranging from students without any IT
background to information-security post-doctoral researchers.
1) Method: We explained every participant that we devel-
oped a browser extension which provides additional protection
against attacks. We showed two instances of Google Chrome
to the participant, one without Chrome Zero (A) and one with
Chrome Zero set to protection level high (B) for every website
in the Alexa Top 25. For every website, a fully automated
script randomly chose whether browser instance A or B had
Chrome Zero activated, without any interaction of the study
conductor or the study participant. Hence, neither the study
participant nor the study conductor knew which of the two
browsers had Chrome Zero activated, making the study double
blind. After 1 minute, the script asked the user whether there
was any noticeable difference between the two pages, and
if so, whether browser A or browser B had Chrome Zero
enabled. The results of these questions were saved in a ﬁle
and automatically evaluated after the user tested all 25 pages.
Every correct user answer was counted as a 100 % correct
answer, if the user did not notice any difference, we counted
it as a 50% probability to make the correct guess, and if the
user answered incorrectly, we counted a 0 % correct answer.
2) Results: Figure 12 shows the results of our user study.
The overall probability to correctly detect the presence of
Chrome Zero was 50.2 %. The maximum average success rate
of a participant was 60 %; the minimum was 40 %.
The maximum detection rate for a website was 62.5 % for
yahoo.com (standard error ±0.05). For all other websites, the
detection rate was not better than random guessing. The mini-
mum detection rate for a website was 41.7 % for amazon.com
and sina.com.cn (standard error ±0.05).
For the 6 websites where no Chrome Zero policy was
active, users guessed correctly in 48.3 % on average (standard
error ±0.049), i.e., a deviation of 1.7pp to random guessing.
13