322
DSN 2008: Murata et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:16 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
1500 1":"""""''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''=-------------,
i - . OutputOata (29 resources)
i
-lnputOata (29 resources)
- . OutputOata(64resources)
---lnputOata (64 resources)
•• OutputOata(222 resources;
--Input Oata (222 resources)_ ~
.. .... .
.... - - - ..... -.....
~
(/)
........
~ 1000
"'C
I1lo
...J
~
~ 500 f-----....J
z ~.. ._w -~
~'""'!"'"::::::::======:--
1
OL----.l..-.---.l..-.---"'"""----"'"""-------I
10
o
WUs/access
Figure 3. Network Load (without
the Proposed Scheduling Mecha(cid:173)
nism)
1500 I'l"'i......-.-Ou--tput......O......ata......(2.....9 r......eso......urc......es......) ""!"'"I----------.
i
-lnputOata(29resources)
- . OutputOata (64 resources)
--Input Oata (64 resources)
I
I • • 'OutputOata(222 resources)
i --lnputOata (222 resources)
......
!
i
I
,
._~-
(/)
........
~ 1000
"'C
I1lo
...J
~
~ 500
z
0"-----"'"""----"'"""----"'"""----"--------1
10
o
WUs/access
Figure 4. Network Load (with the
Proposed Scheduling Mechanism)
10 ~======~---------,
i-NetworkTraffiC(29reSOUrces)
l
-.- Network Traffic(64 resources)
l- Network Traffic~~~ resources) ,
I..
.
5 .--~---- -------~-----------~------- -_..--/----~-----------------~--t
........
~
""C
I1lj
~
o
~
Z
oL......I:::::::!!!!!!!!!I!!!!!!i1§:==f:::::========:.-----.J
10
o
4
6
WUs/access
Figure 5. Average Network Load
between Clients (with the Pro(cid:173)
posed Scheduling Mechanism)
system achieves, the more WU downloads and the more
result uploads are carried out per unit time. As a result,
the network traffics increase with the total throughput of
the entire system. Since the total throughput increases with
WUslaccess, WUslsec is related to both of the above two
In Figure 3, the former is the main factor of the
factors.
network traffics in WUslaccess = 1, and the latter becomes
the dominant factor in WUslaccess = 4 and 8. In Figure 4,
the increases in the network traffics are caused by only the
improvement of the total throughput of the entire system.
Therefore, the proposed task scheduling does not generate
the network traffics between the project server and clients.
Figure 5 shows the average network traffics between
clients with the proposed task scheduling mechanism.
In
the cases of WUslaccess = 1 and WUslaccess = 2, the pro(cid:173)
posed task scheduling mechanism does not move tasks be(cid:173)
tween clients. In those cases, the network traffics between
clients are only for interchanging the information about the
task progresses and the status of clients. The frequency of
communications between clients depends on the processing
time of each task.
In the case of a short processing time
of a task such as Application-l, clients frequently commu(cid:173)
nicate with each other and the traffics increase.
In practi(cid:173)
cal uses, the processing time generally becomes larger than
that in the experiments. As a result, the network traffics
between clients becomes smaller in practical uses. On the
other hand, in the cases of WUslaccess = 4 and WUslaccess
= 8, the network traffics increase because a task moves to
other clients by the proposed task scheduling mechanism.
The proposed task scheduling causes the network traffics
among clients, not between a server and clients. Therefore,
the network load of a server does not increase even if the
number of clients increases.
The results in Table 4 and Figure 3 indicate that the to(cid:173)
tal throughput of the entire system and network load of
the proj ect server significantly depend on how the proj ect
server distributes WUs. The project server efficiently dis(cid:173)
tributes WUs with low network traffics, if each of a few
clients downloads many WUs at a time. However, the CPU
load of the project server increases in proportion to the to(cid:173)
tal throughput of the entire system, and then clients often
fail in WU downloading. Therefore, the utilization rate of
the clients decreases, and the total throughput of the entire
system is lower than the peak performance, which is theo(cid:173)
retically anticipated.
The proposed mechanism produces the situation in
which a few clients request a lot of WUs at a time by us(cid:173)
ing proxy download. In this situation, the project server can
efficiently distribute WUs. Moreover, even if a client fails
in WU downloading, the proposed task scheduling mech(cid:173)
anism provides WUs to the client. In conclusion, the pro(cid:173)
posed task scheduling mechanism, which incorporates the
proxy download into the BOINC platform, activates more
computing resources and leads to improvement of the total
throughput of the entire system.
1-4244-2398-9/08/$20.00 ©2008 IEEE
323
DSN 2008: Murata et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:16 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
3000 , . . . - - - - - - - - - - - - - - - - - - . ,
2500 t----------.-----.....- ------ --------- ---------
o
u
~ 2000 t------------- -.. ----
E
f=
~ 1500
~ 1000
I-
500
01.-----------------'------'----'
10
o
4
6
WUs/access
Table 6. The Rate of WU Reassignment with
Various Deadlines (WUs/access =8)
Deadlines I without Proposal [%] I with Proposal [%]
500 secs
1000 secs
1500 secs
24 hours
82
53
38
0
56
24
0
0
Figure 6. Turnaround Time of WU (with(cid:173)
out
the Proposed Scheduling Mecha(cid:173)
nism)
3000 r-----------------.,
2500 ~-----.------------------------------ -.--------------------------------1
o
u
~ 2000 \------.. ------ ---------------------------------------.-..-.. - ---- --------f
E
f=
§ 1500
o! 1000
I-
500
OL..------'-----'------I...-----'------'
10
o
WUs/access
Figure 7. Turnaround Time of WU (with
the Proposed Scheduling Mechanism)
5.2. Effect of Dynamic Load Balancing on
Turnaround Time
The effect of dynamic load balancing by the pro(cid:173)
posed mechanism on the turnaround time is also evaluated
with Application-2 under the assumption of dynamic load
changes. The maximum, minimum, and average turnaround
times for processing a WU are measured using 222 clients.
In the experiments, the deadline is set to 24 hours after the
download, and the reassignment of a failed WU is not per(cid:173)
formed. Figures 6 and 7 show the turnaround times without
load balancing and those with load balancing, respectively.
In Figures 6 and 7, the vertical lines show the maxi(cid:173)
mum, minimum, and average turnaround times in the cases
of WUslaccess = 1, 2, 4, 8. The top of the vertical line in(cid:173)
dicates the maximum turnaround time, and the bottom of
the vertical line indicates the minimum turnaround time.
In Figure 6, the turnaround times of processing WUs vary
widely, and the variation increases with WUslaccess. In the
case of WUslaccess = 8, the maximum turnaround time is
24 times longer than the minimum one. This is because a
client has downloaded many WUs, then become busy, and
taken a long time to complete processing all ofthem. On the
other hand, in Figure 7, dynamic load balancing can lower
the upper limit of turnaround times, resulting in smaller av(cid:173)
erage turnaround times. In the case of WUslaccess = 8, the
maximum and average turnaround times decrease by more
than 40%. The maximum turnaround time is only six times
longer than the minimum. Accordingly, a busy client can
appropriately move its downloaded but unprocessed WUs
to other idle clients in order to reduce the turnaround time.
In the cases of WUslaccess = 1 and WUslaccess = 2, dy(cid:173)
namic load balancing does not improve the performance be(cid:173)
cause of two reasons. One is that a small WUslaccess usu(cid:173)
ally results in a small variation of turnaround times. The
other is that the scheduling mechanism hardly moves WUs
among clients because each client has too few WUs. How(cid:173)
ever, if a client frequently leaves and rejoins the volunteer
computing system, dynamic load balancing will become ef(cid:173)
fective because it can prevent a client from leaving with
some unprocessed WUs. This effect will be investigated
in our future work.
A reduction in the turnaround time is significantly im(cid:173)
portant especially when the deadline is used for WU reas(cid:173)
signment. Table 6 summarizes the rate ofWU reassignment
with various deadlines. Table 6 uses the results in Figures
6 and 7. For the same value of deadline time, the proposed
mechanism achieves a low reassignment rate. If the rate of
WU reassignment is high, the computing power spent for
each of the failed WUs is wasted. Since the reduction in the
turnaround time also reduces the number of failed process(cid:173)
ing WUs , it can significantly improve the total performance
of the entire system.
In conclusion, the dependability of
volunteer computing can significantly be enhanced with the
proposed dynamic task scheduling mechanism, because it
can reduce the average turnaround time and hence the num(cid:173)
ber of failed WUs.
6 Related Work
XtremWeb [6] aggregates the information about idle
computing resources on the network, and matches a user's
job and those resources. An advantageous feature is that
XtremWeb can have multiple servers for information aggre(cid:173)
gation to improve the scalability.
JNGI [12]
is a Grid computing platform for large-
1-4244-2398-9/08/$20.00 ©2008 IEEE
324
DSN 2008: Murata et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:16 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
scale distributed computing, which uses JXTA [10] as a
communication protocol. JNGI can manage dynamically(cid:173)
variable computing resources by categorizing them into
some groups, i.e. monitor groups for managing the entire
framework, worker groups for computing, and task dis(cid:173)
patcher groups for job management.
XtremWeb and JNGI are computing platforms for large(cid:173)
scale distributed computing, but both do not provide any
decentralized task scheduling mechanisms for dynamic load
balancing. Therefore, for large-scale distributed computing
environments consisting of a huge number of clients, these
frameworks cannot effectively perform dynamic load bal(cid:173)
ancing. On the other hand, the proposed mechanism can ef(cid:173)
fectively perform dynamic load balancing regardless of the
number of clients.
A Global Processing Unit (GPU) [7] is a computing plat(cid:173)
form for distributed computing that uses the overlay net(cid:173)
work of a file sharing software, Gnutella. In GPU, a com(cid:173)
putation task is submitted to the overlay network as a search
If the search request arrives at a computing re(cid:173)
request.
source that can accept the request, the computing resource
executes the task. The computation results are treated as the
search results in the Gnutella protocol, and are sent back
to the user who submitted the task. GPU offers a mecha(cid:173)
nism to schedule a job consisting of a single task. Using
the Gnutella overlay network, GPU can achieve a high scal(cid:173)
ability. However, it generally takes a long time to start a
job, and moreover cannot guarantee the QoS and deadline
because it cannot operate jobs in progress.
7. Conclusions
This paper has proposed a distributed and coopera(cid:173)
tive scheduling mechanism for efficient volunteer comput(cid:173)
ing, and then designed its effective implementation for the
BOINC platform, which is a widely used, de-facto stan(cid:173)
dard platform for volunteer computing. The proposed task
scheduling mechanism provides the functionality of dy(cid:173)
namic load balancing and proxy downloading to the BOINC
platform. The dynamic load balancing mechanism im(cid:173)
proves the turnaround time of processing a WU and also re(cid:173)
duces the number ofWU failures. The proxy download can
suppress the load concentration on a BOINC project server
and improve the efficiency of the entire volunteer comput(cid:173)
ing system.
The original BOINC platform cannot guarantee the
deadline and the total throughput of the entire computing
system. On the other hand, the proposed system can cover
these points, and therefore improve the dependability of
volunteer computing by using the decentralized scheduling
among clients.
We are planning to extend the proposed mechanism to
support clients' dynamic leaving and rejoining volunteer
computing. It will prevent WU losses by moving WUs ofa
client to others when the client leaves. The security is also
an important technical issue of this work. Since the pro(cid:173)
posed mechanism needs communication among clients, a
malicious client may balk the volunteer computing project
and reduce its dependability. Some mechanisms to confirm
the reliability of each client at the communication will be
discussed in our future work.
ACKNOWLEDGMENTS
This research was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas No. 18049003, and
Strategic Information and Communications R&D Promo(cid:173)
tion Program (SCOPE-S) No. 061102002.
References
[1] D. P. Anderson. Boinc: A system for public-resource com(cid:173)
puting and storage. 5th IEEE/ACM International workshop
on Grid Computing, pages 4-10, Nov. 2004.
[2] D. P. Anderson, 1. Cobb, E. Korpela, M. Lebofsky,
and D. Werthimer.
in
public-resource computing. Communications of the ACM,
45(11 ):56-61, Nov. 2002.
Seti@home: An experiment
[3] D. P. Anderson, E. Korpela, and R. Walton.
High-
perfonnance task distribution for volunteer computing. First
IEEE International Conference on e-Science and Grid Tech(cid:173)
nologies, pages 196-203, 2005.
[4] distributed.net. http://www.distributed.net/.
[5] Einstein@Home.
http://einstein.phys.uwm .
edu/.
[6] G. Fedak, C. Gennain, V. Neri, and F. Cappello. Xtremweb;
a generic global computing system. Proceedings of First
IEEE/ACM International Symposium on Cluster Computing
and the Grid, pages 1-12, May 2002.
[7] GPU,
a Global
Processing Unit.
http:
//sourceforge.net/projects/gpu/.
[8] Y. Murata, T. Inaba, H. Takizawa, and H. Kobayashi. A
distributed and cooperative load balancing mechanism for
large-scale p2p systems. SAINT- W '06: Proceedings ofthe
International Symposium on Applications on Internet Work(cid:173)
shops,pages 126-129,2006.
[9] Predictor@Home.
http://predictor . scripps.
edu/.
[10] Project JXTA. http://www . jxta. org/.
[11] H. Saito, Y. Kamoshida, S. Sawai, K. Hironaka, K. Taka-
hashi, T. Sekiya, N. Dun, T. Shibata, D. Yokoyama, and
Intrigger: A multi-site distributed computing
K. Taura.
environment supporting flexible configuration changes.
In
Proceedings ofSummer United Workshops on Parallel, Dis(cid:173)
tributed, and Cooprative Processing (SWoPP2007) , pages
237-242, August 2007.
[12] 1. Verbeke, N. Nadgir, G. Ruetsch, and 1. Sharapov. Frame-
work for peer-to-peer distributed computing in a heteroge(cid:173)
neous, decentralized environment. Proc. of Third Interna(cid:173)
tional Workshop on Grid Computing (GRID 2002), pages
1-12, Nov. 2002.
1-4244-2398-9/08/$20.00 ©2008 IEEE
325
DSN 2008: Murata et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:10:16 UTC from IEEE Xplore.  Restrictions apply.