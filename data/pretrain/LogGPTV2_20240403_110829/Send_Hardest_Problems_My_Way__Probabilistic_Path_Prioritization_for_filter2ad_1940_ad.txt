more comprehensive understanding of the performance. In
order to give enough time for fuzzing as well as limit the
total time of three runs for each binary, we choose to assign
12 hours to each binary from the CQE dataset, and stop the
analysis as long as a crash is observed. For the LAVA dataset,
we analyze each binary for 5 hours as in the LAVA paper.
D. Evaluation on the CQE dataset
In this section, we demonstrate the effectiveness of our ap-
proach on the CQE dataset from three aspects: code coverage,
the number of discovered vulnerabilities, and the contribution
of concolic execution to the hybrid fuzzing.
1) Code coverage: Code coverage is a critical metric for
evaluating the performance of a fuzzing system. We use the
bitmap maintained by AFL to measure the code coverage.
In AFL, each branch transition is mapped into an entry of
the bitmap via hashing. If a branch transition is explored, the
8
TABLE II: Number of discovered vulnerabilities
DigFuzz
Random
Driller
AFL
MDPC
= 3 ≥ 2 ≥ 1
81
73
77
68
75
67
73
68
29
31
77
73
71
70
29
each path as MDPC is too expensive, completely taking away
the power of fuzzing.
As an optimization, one would move the optimal decision
module out and make it work in parallel with fuzzing. In this
manner, the concurrent MDPC would be able to take advantage
of the high throughput of fuzzing. However, using the deﬁned
solving cost [42], the concurrent MDPC assigns all the missed
paths to concolic execution only in several seconds after the
fuzzing starts. Then, the concurrent MDPC will degrade to
Random. The reason is that the cost of concolic execution (50
as deﬁned in the original paper) might be too small. Actually,
how to normalize the cost of fuzzing and concolic for a uniﬁed
comparison is difﬁcult, because these two costs are estimated
by using different metrics, which are concerned with the run-
time throughput of fuzzing, the performance of the constraint
solver, and the symbolic execution engine. It is difﬁcult (if not
impossible) to deﬁne a uniﬁed metric to evaluate the costs of
different techniques.
Unlike MDPC that estimates the costs for exploring each
path by fuzzing and concolic execution respectively, DigFuzz
prioritizes paths by quantifying the difﬁculties for fuzzing
to explore a path based on coverage statistics. Granted, the
sampling may be biased as generated test cases by fuzzing
are not uniform distributed rendering even lower possibility to
explore the difﬁcult paths than theory, such bias in fact works
for our favor. Our goal is to ﬁnd the most difﬁcult branches
for fuzzing by quantifying the probabilities. With the bias, it
lowers the probability calculated and increases the chance for
DigFuzz to pick the least visited branches by fuzzing.
2) Number of Discovered Vulnerabilities: We then present
the numbers of vulnerabilities discovered by all four conﬁg-
urations via Table II. The column 2 displays the numbers of
vulnerabilities discovered in all three runs. Similarly, columns
3 and 4 show the numbers of vulnerabilities that are discovered
at least twice and once out of three runs, respectively.
We can observe that in all three metrics, DigFuzz discovers
considerably more vulnerabilities than the other conﬁgurations.
In contrast, Driller only has a marginal improvement over AFL.
Random discovers more vulnerabilities than Driller yet still
falls behind DigFuzz due to the lack of path prioritization.
This table could further exhibit the effectiveness of Dig-
Fuzz by comparing with the numbers reported in the Driller
paper. In the paper, Driller assigns 4 fuzzing instances for each
binary, and triggers crashes in 77 applications in 24 hours [39].
Among these 77 binaries, 68 of them are crashed purely by
AFL, and only 9 binaries are crashed with the help of concolic
execution. This result is on par with the numbers in column 3
for DigFuzz. This means DigFuzz is able to perform similarly
with only half of the running time (12 hours vs. 24 hours) and
much less hardware resources (2 fuzzing instances per binary
vs. 4 fuzzing instances per binary).
Fig. 6: Normalized bitmap size on CQE dataset
corresponding entry in the bitmap will be ﬁlled and the size
of the bitmap will increase.
As the program structures vary from one binary to another,
the bitmap sizes of different binaries are not directly com-
parable. Therefore, we introduce a metric called normalized
bitmap size to summarize how the code coverage increases for
all tested binaries. For each binary, we treat the code coverage
of the initial inputs as the base. Then, at a certain point during
the analysis, the normalized bitmap size is calculated as the
size of the current bitmap divided by the base. This metric
represents the increasing rate of the bitmap.
Figure 6 presents how the average of normalized bitmap
size for all binaries grows. The ﬁgure shows that DigFuzz
constantly outperforms the other fuzzing systems. By the
end of 12 hours, the normalized bitmap sizes in DigFuzz,
Random, Driller, and AFL are 3.46 times, 3.25 times, 3.02
times and 2.91 times larger than the base, respectively. Taking
the normalized bitmap sizes AFL that is aided by dummy
concolic execution as a baseline, the concolic execution in
in DigFuzz, Random and Driller contributes to discovering
18.9%, 11.7%, and 3.8% more code coverage, respectively.
We can draw several conclusions from the numbers. First,
Driller can considerably outperform AFL. This indicates that
concolic execution could indeed help the fuzzer. This con-
clusion is aligned with the Driller paper [39]. Second, the
optimization in Random does help increase the effectiveness
of the concolic execution compared to Driller. This observation
shows that the second limitation of “demand launch” strategy
described in Section II can considerably affect the concolic ex-
ecution. Third, by comparing DigFuzz and Random, we can
observe that the path prioritization implemented in DigFuzz
greatly strengthens the hybrid fuzzing system in exploring new
branches. Further investigation shows that the contribution of
concolic execution to bitmap size in DigFuzz is much larger
than those in Driller (18.9% vs. 3.8%) and Random (18.9%
vs. 11.7%). This fact demonstrates the effectiveness of our
strategy in term of code exploration.
We can also see that MDPC is even worse than AFL.
By carefully examining the working progress of MDPC,
we ﬁnd that the main reason is the reduced throughput of
fuzzing. In contrast to the average throughput in AFL that
is 417 executions per second, the throughput reduces to 2.6
executions per second. It indicates that decision making for
9
    \multirow{3}{*}{\MDPC} & 95 & 1311 & 93 & 21,513 & 22,006 & 29\\    \cline{2-7}     & 95 & 1335 & 92 & 23,635 & 24,129 & 29\\    \cline{2-7}     & 96 & 1427 & 93 & 28,769 & 29,466 & 30\\    \hline 11.522.533.50123456789101112Normalized bitmap sizeFuzzing time (hour)DigFuzzRandomDrillerAFLMDPC11.011.0200.511.522.533.544.55Normalized bitmap sizeFuzzing time (hour)DigFuzzRandomDrillerAFLMDPCTABLE III: Performance of concolic execution
DigFuzz
Random
Driller
Ink.
64
64
63
68
65
64
48
49
51
CE
1251
668
1110
1519
1235
1759
1551
1709
877
Aid.
37
39
41
32
23
21
13
12
13
Imp.
719
551
646
417
538
504
51
153
95
Der.
9,228
7,549
6,941
5,463
5,297
6,806
1,679
2,375
1,905
Vul.
12
11
9
8
6
4
5
4
4
3) Contribution of concolic execution: Here, we dive
deeper into the contribution of concolic execution by present-
ing some detailed numbers for imported inputs and crashes
derived from the concolic execution.
The number of inputs generated by the concolic execution
and later imported by the fuzzer indicates the contribution of
concolic execution to the fuzzer. Therefore, we analyze each
imported input, trace back to its source and present numbers
in Table III.
The second column (Ink.) lists the number of binaries for
which the concolic execution is invoked during testing. For this
number, we exclude binaries on which the concolic execution
is invalid. Such invalid concolic execution is either caused by
path divergence that the symbolic execution path disagrees
with the realistic execution trace, or caused by the resource
limitation (we kill the concolic execution running for more
than 1 hour or exhausting more than 4GB memory).
The third column (CE) shows the total number of concolic
executions launched on all
the binaries. We can see that
Random invokes slightly more concolic execution jobs than
DigFuzz, indicating that a concolic execution job in DigFuzz
takes a bit longer to ﬁnish. As the fuzzing keeps running, the
speciﬁc branches that block fuzzing become deeper. This result
implies that DigFuzz is able to select high quality inputs and
dig deeper into the paths.
The forth column (Aid.) refers to the number of binaries
on which the fuzzer imports at least one generated input from
the concolic execution. We can observe that the number for
Random is larger than that in Driller. This indicates that the
concolic execution can better help the fuzzer if it is invoked
from the beginning. This result also conﬁrms that a non-stuck
state of the fuzzer does not necessarily mean the concolic
execution is not needed. Further, since the number for DigFuzz
is larger than Random, it shows that the concolic execution
can indeed contribute more with path prioritization.
The ﬁfth column (Imp.) refers to the number of inputs that
are imported by the fuzzer from concolic execution while the
sixth column (Der.) shows the number of inputs derived from
those imported inputs by the fuzzer. We can see signiﬁcant
improvements on imported inputs and derived inputs for Dig-
Fuzz than Random and Driller. These improvements show
that the inputs generated by DigFuzz is of much better quality
in general.
The last column (Vul.) shows the number of binaries for
which the crashes are derived from concolic execution. For
each crashed binary, we identify the input that triggers the
crash, and then examine whether the input is derived from
an imported input generated by the concolic execution. The
number shows that concolic execution in DigFuzz contributes
to discovering more crashes (12 vs. 5) than that in Driller.
To sum up, from the numbers reported, we clearly see
that by mitigating the two limitations of “demand launch”
strategy, our new strategy outperforms the state-of-the-art
hybrid system, Driller, in every important aspect.
E. Evaluation on the LAVA dataset
In this section, we demonstrate the effectiveness of our
approach on the LAVA-M dataset.
1) Discovered vulnerabilities: The LAVA dataset is widely
adopted for evaluation in recent studies [12], [27], [32]. A
recent report [14] shows that
the fuzzing on binaries in
LAVA-M can be assisted by extracting constants from these
binaries and constructing dictionaries for AFL. According to
this report [14], we analyze every binary for 5 hours with and
without dictionaries respectively.
The discovered vulnerabilities are shown in Table IV. It
shows that with dictionaries, the four techniques, DigFuzz,
Random, Driller and AFL can detect nearly all injected bugs
in base64, md5sum and uniq. With the impact of reduced
of throughput, MDPC discovers less vulnerabilities than other
techniques. By contrast, without dictionaries, these techniques
can detect signiﬁcantly fewer bugs (and in many cases, no
bug). As demonstrated in LAVA [15], the reason why concolic
execution cannot make much contribution for md5sum and
uniq are hash functions and unconstrained control dependency.
This results indicate that it is the dictionaries contributing to
detect most bugs in base64, md5sum and uniq.
An exception is who, for which, DigFuzz outperforms
Random, Driller, MDPC and AFL with a large margin.
Looking closer, Driller can only detect 26 bugs in who, MDPC
can detect 34 bugs, while DigFuzz could detect 111 bugs. To
better understand the result, we carefully examine the whole