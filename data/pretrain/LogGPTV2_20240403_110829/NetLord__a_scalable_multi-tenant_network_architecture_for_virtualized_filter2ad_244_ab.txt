est Path Bridging (an IEEE standard) [4], and Seattle [17], support
multipathing using a single-shortest-path approach. These three
need new control-plane protocol implementations and data-plane
silicon. Hence, inexpensive commodity switches will not support
them, for at least a few years.
One way to achieve scalable multipathing is through hierarchi-
cal addressing in speciﬁc topologies. Al-Fares et al.[7] proposed
three-level FatTree topologies, combined with a speciﬁc IP address
assignment scheme, to provide high bisection bandwidth with-
out needing expensive, high-radix core switches. For scalability,
their proposal depends on a two-level route lookup feature in the
switches. Mysore et al. proposed PortLand [19], which replaces
that IP address scheme with MAC-address rewriting, and requires
switches with the ability to forward based on MAC-address pre-
ﬁxes. Both these approaches work only with multi-rooted tree
topologies.
Scott et al. proposed MOOSE [22], which address Ethernet
scaling issues by using hierarchical MAC addressing. MOOSE
also uses shortest-path routing, and did not focus on multipathing
for improved bisection bandwidth. MOOSE, like PortLand, needs
switches that can forward packets based on MAC preﬁxes.
Mudigonda et al. [18] proposed SPAIN, which uses the VLAN
support in existing commodity Ethernet switches to provide multi-
pathing over arbitrary topologies. SPAIN uses VLAN tags to iden-
tify k edge-disjoint paths between pairs of endpoint hosts. The
original SPAIN design may expose each end-point MAC address
k times (once per VLAN), stressing data-plane tables even more
than standard Ethernet, and hence it can not scale to large number
of VMs.
To summarize, no current practices or prior research proposals
meets all of the goals we described in section 2.1.
3. NETLORD’S DESIGN
The fundamental idea underlying NetLord is to encapsulate the
tenant’s L2 packets and transfer them over a scalable L2 fabric,
using an L3+L2 (IP+Ethernet) encapsulation that exploits features
of both layers. NetLord uses a light-weight agent in the hypervi-
sors to encapsulate, route, decapsulate, and deliver tenant packets
to virtual machines, addressed to the VMs’ tenant-assigned Ether-
net addresses. With two exceptions, described in sections 3.2 and
3.9, NetLord ignores any tenant-visible L3 (IP) issues.
The source NetLord Agent (NLA) creates the encapsulating L2
and L3 headers such that the Ethernet destination address directs
the packet through the underlying L2 fabric to the correct edge
switch, and such that the IP destination address both allows the
egress edge switch to deliver the packet to the correct server, and al-
lows the destination NLA to deliver the packet to the correct tenant.
The details of this encapsulation are somewhat subtle; we discuss
them in section 3.5.
One signiﬁcant consequence of this encapsulation method is
that tenant VM addresses are never exposed to the actual hard-
ware switches. By using IP forwarding on (only) the last hop,
we can effectively share a single edge-switch MAC address across
a large number of physical and virtual machines. This resolves
the problem of FIB-table pressure; in NetLord the switches, in-
stead of needing to store millions of VM addresses in their FIBs,
only need to store the addresses of the other switches. Even in a
very large datacenter, we expect at most a few thousand switches.
(Edge-switch FIBs must also store the addresses of their directly-
connected server NICs – at most, one per switch port; they do not
need to store the addresses of remote servers.)
Because of the speciﬁc encapsulation used in NetLord, edge
switches require very limited conﬁguration; in particular, this con-
ﬁguration requires only local information, plus some boilerplate
conﬁguration that is essentially identical on every switch. This
aspect of NetLord dramatically simpliﬁes the operation and con-
ﬁguration of the network hardware, obviates the need for complex
routing protocols, and reduces the chances for software or human
error to create failures. We describe the details of conﬁguration in
section 3.6.
Another consequence of NetLord’s encapsulation method is that
it exposes tenant IDs in the outer L3 header of packets moving
through the fabric. This potentially allows a cloud provider to do
per-tenant trafﬁc management in the network fabric, without having
to put per-ﬂow ACLs in the switches, which would create signiﬁ-
cant scaling problems. We discuss tenant-level network manage-
ment in section 6.
Since we intend NetLord for use in a large-scale datacenter,
the underlying L2 fabric needs multi-path support, for high band-
64Figure 2: NetLord’s high-level component architecture (top) and packet encapsulation/decapsulation ﬂows (bottom)
However, for simple routing functions, the extra network hop
and VM computation implied by that approach can add unneces-
sary overhead. Therefore, NetLord follows Diverter’s model of
supporting “virtual routing” within the hypervisor [11]. A tenant
designates to NetLord certain sets of hIP address, MAC addressi
pairs (within their own address spaces) as virtual router interfaces.
Whenever a tenant VM sends a packet to one of these MAC ad-
dresses, its local NetLord agent intercepts the packet, extracts the
IP header, and does a route lookup to determine the destination
tenant-assigned IP and MAC addresses (see section 3.9 for the lat-
ter). The NLA can then encapsulate the outgoing packet to send it
directly to the ﬁnal destination.
The virtual routing function could support simple ﬁrewall func-
tions, although tenant-implemented SW routers might be needed
for more complex router features. Tenants can also use virtual rout-
ing to exchange packets with hosts on the public Internet, or with
other tenants, via a public address space that is exposed to all ten-
ants and advertised externally. This public address space is associ-
ated with a reserved Tenant_ID=2, so the NLA allows any tenant
to request that a VM’s interface be given an address in that address
space.
Figure 1 shows examples of several network abstractions avail-
able to NetLord tenants. The inset shows a pure L2 abstraction; the
main ﬁgure shows a tenant with three IPv4 subnets connected by a
virtual router.
3.3 NetLord’s components
Figure 2 shows a high-level architectural view of NetLord. The
top half of the ﬁgure depicts important components and their in-
terconnections, while the lower half shows the header operations
performed on a packet as it travels through these components.
As shown in the top half, NetLord consists of: (1) a fabric con-
sisting of simple switches, (2) NetLord Agents (NLAs) in the hy-
pervisor at each physical host, and (3) a conﬁguration repository.
Fabric switches: NetLord relies on a traditional, switched Eth-
ernet fabric, using unmodiﬁed commodity switches. We require
only that these switches support VLANs (for multi-pathing; see
section 3.4) and basic IP forwarding. We do not require full-ﬂedged
support for IP routing; in particular, the switches run no routing
protocol. We do require that a switch can take an IP packet sent
to its own MAC address, look up its destination using longest-
preﬁx match (LPM) in a small forwarding table using statically-
conﬁgured entries, and forward the packet appropriately. All of
Figure 1: Network abstractions, as seen by the tenants
width and fault tolerance. NetLord leverages our previous work
on SPAIN [18] to provide an underlying multi-path fabric using
commodity switches. We provide a brief summary of SPAIN in
section 3.4.
3.1 A tenant’s view of NetLord
NetLord provides tenants with a very simple abstract view of
the network: every tenant has one or more private MAC address
spaces. In each of its MAC address spaces, the tenant can assign ar-
bitrary MAC addresses. A tenant might wish to use multiple MAC
address spaces to simplify address allocation, or to limit the scope
of its broadcasts/multicasts.
(NetLord does not currently imple-
ment tenant multicasting, but we believe this is feasible.)
Most tenants will also allocate and use L3 addresses (IPv4, IPv6,
etc.). NetLord mostly ignores any tenant-visible L3 issues (except
as discussed in sections 3.2 and 3.9). Therefore, by placing no re-
strictions on how tenants assign L2 or L3 addresses, NetLord pro-
vides full address-space virtualization: multiple tenants can use the
same address without having their packets mis-routed.
3.2 Virtual routing
A tenant can divide its IP address space into networks and/or IP
subnets, and connect these via software routers running on some of
its VMs. This approach requires no support from NetLord.
D-MAC: VM-DS-MAC: VM-SPayLoadPkt from VM_SPkt from VM_SIPETHPkt from VM_SIPETHPkt from VM_SIPETHD-MAC: NLA-DS-MAC: ES-DPkt from VM_SIPETHD-MAC: ES-DS-MAC: ES-S VLAN: SPAIND-IP: P2.TIDS-IP: MAC_AS_ID++NLA-S(Source)VM-S(Source  VM)Tenant: TIDVM NLA-S(Source)SPAINServerVM NLA-D(Dest)SPAINIngress Switch(ES-S)+Egress Switch(ES-D)Config RepositoryInexpensive Commodity EthernetVM-D(Dest  VM)Tenant: TIDPort P1Port P2To VM-DEthernetSwitchVMVM...VMVM...VMVM...VMVM...IPv4 RouterIPSubnets65the datacenter switches we have examined, including the cheapest
ones, can support NetLord.
These switches tend to be much cheaper than full routers, be-
cause they do not require support for complex routing protocols
(e.g., IS-IS or OSPF), large routing tables, complex ﬁrewall func-
tions, etc.
NetLord agents: A NetLord Agent (NLA) resides in the hyper-
visor (or the driver domain) of each physical server, and performs
two major tasks. First, the NLA transparently encapsulates and de-
capsulates all packets from and to the local VMs.
Second, the NLA collaborates with other NLAs, and with the
central conﬁguration repository, to gather and maintain all the in-
formation needed for the encapsulation. For instance, the NLA
builds and maintains a table that maps a VM’s Virtual Interface
(VIF) ID to the port number and MAC address of the edge switch
to which the server hosting that VM is connected.
Conﬁguration repository: The repository (which could be
replicated for performance and availability) resides at an address
known to the NLAs, and maintains several databases. Some are
used for SPAIN-style multi-pathing; some are used for per-tenant
conﬁguration information. We envision this repository to be co-
located with the datacenter-wide VM manager system (such as Eu-
calyptus1) that we expect all cloud datacenters to have. (Note that
this repository differs from OpenFlow’s central controller, since it
is used to conﬁgure end-host parameters, not just switches).
3.4 SPAIN in a nutshell
NetLord relies on SPAIN to construct a high-bandwidth, resilient
multi-path fabric using commodity Ethernet switches. We brieﬂy
sketch the pertinent features; for a full description, see [18].
SPAIN is based on three mechanisms: it pre-computes k edge-
disjoint paths between pairs of edge switches; it pre-conﬁgures
VLANs to identify these paths (not for isolation, as is the typical
use of VLANs); and it uses an end-host agent to spread the trafﬁc
across paths (i.e., VLANs).
SPAIN’s algorithms for computing edge-disjoint paths, merging
these paths into trees, and optimally packing these into the 12-bit
VLAN tag space are complex and somewhat computationally ex-
pensive, but are run only when the network topology is designed or
signiﬁcantly changed. These algorithms work with any topology,
but are most useful when the topology provides a rich variety of
paths (e.g., FatTree).
SPAIN uses an end-host agent, which can be incorporated into
a hypervisor, and thus integrated with the NetLord Agent. On
packet transmission, the agent looks up the destination Ethernet
address D in a local table, yielding a set of k VLANs that reach
that destination. It then chooses a VLAN (e.g., round-robin, but
with ﬂow afﬁnity to prevent packet reordering), tags the packet
with that VLAN, and transmits it normally. Because the k VLANs
are constructed to take different paths to D, this provides high net
bandwidth and load balancing among paths. The SPAIN agent also
detects failed hVLAN, destinationi pairs, and then re-routes around
the failure by using a different VLAN.
SPAIN by itself (i.e., without NetLord) suffers from a major scal-
ability problem: it not only exposes the fabric switches to end-host
MAC addresses, but it exposes each VM MAC address k times:
once per hVLAN, VM-MACi pair. This means that SPAIN creates
even more pressure on switch data-plane FIB tables than plain Eth-
ernet does. (We demonstrate this problem in section 5.) However,
NetLord encapsulates all tenant VM addresses, and also hides most
physical NIC MAC addresses from most switches (as we will soon
1http://eucalyptus.cs.ucsb.edu/
describe). Thus, augmenting SPAIN with NetLord greatly reduces
FIB pressure, because the switch FIBs need to hold only one en-
try for each hVLAN, switch-MACi pair, and there are far fewer
switches than VMs.
The SPAIN end-host agent relies on the repository to obtain the
table that maps between destinations and sets of VLANs. When
combined with the NetLord Agent, SPAIN requires one entry for
each edge switch in the datacenter (not for each VM!); this table is
loaded at boot time and updated only when the set of switches and
wires changes. (Note that the NLAs on each edge switch will see
a different table; the NLAs attached to one switch all see the same
table.)
NetLord identiﬁes a tenant VM’s Virtual
3.5 Encapsulation details
Interface (VIF)
by the 3-tuple hTenant_ID, MACASID, MAC-Addressi, where
MACASID is a tenant-assigned MAC address space ID, to support
the use of multiple L2 address spaces.
When a tenant VIF SRC sends a packet to a VIF DST, unless
the two VMs are on the same server, the NetLord Agent must
encapsulate the packet. The encapsulation is constructed as follows
(and as depicted in the lower half of ﬁgure 2):
VLAN.tag
MAC.src
MAC.dst
IP.src
IP.dst
IP.id
IP.ﬂags
=
=
=
= MACASID
=
=
= Don’t Fragment
SPAIN_VLAN_for(edgeswitch(DST ),ﬂow)
edgeswitch(SRC).MAC_address
edgeswitch(DST).MAC_address
encode(edgeswitch_port(DST), Tenant_ID)
same as VLAN.tag
This encapsulation conveys all of the information that the re-
ceiving NLA needs to deliver the packet, since the Tenant_ID is
encoded in the IP.dst ﬁeld, the MACASID is carried in the IP.src
ﬁeld, and the original packet header contains the MAC-Address of
the destination VIF. Because the sender NLA sets the “Don’t Frag-
ment” ﬂag, it can use the IP.id ﬁeld to transmit the SPAIN-assigned
VLAN tag to the receiving NLA. The outer VLAN tag is stripped
by the egress edge switch, but the receiving NLA needs to see this
tag to support SPAIN’s fault-tolerance mechanisms; see section 3.8.
Clearly, the NLA needs to know the remote edge switch MAC
address, and the remote edge switch port number, for the destina-
tion VM; we will explain how this happens in section 3.9. (Sec-
tion 3.7 explains how the NLA learns the local edge switch MAC
address.) It also must use SPAIN to choose a VLAN for the egress
edge switch; to avoid packet reordering, this function also takes a
(tenant-level 5-tuple) ﬂow ID.
The most subtle aspect of the encapsulation is the function
encode(), which takes the egress edge switch port number p (for
the destination server, represented as a 7-bit number) and the Ten-
ant_ID tid to create an IP address. Our goal is to allow the egress
edge switch to look up this address to generate the ﬁnal forwarding
hop. We construct this encoding by creating an IP address with p
as the preﬁx, and tid as the sufﬁx: p.tid[16:23].tid[8:15].tid[0:7],
which supports 24 bits of Tenant_ID. (Other similar encodings are
possible, if, for example, the edge switches have more than 128