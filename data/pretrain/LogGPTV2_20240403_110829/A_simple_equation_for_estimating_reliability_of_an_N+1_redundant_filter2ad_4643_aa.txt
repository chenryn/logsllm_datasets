title:A simple equation for estimating reliability of an N+1 redundant
array of independent disks (RAID)
author:Jon G. Elerath
978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
484
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:46 UTC from IEEE Xplore.  Restrictions apply. 
A Simple Equation for Estimating Reliability of an N+1 Redundant Array of Independent Disks (RAID)   Jon G. Elerath NetApp, Inc. PI:EMAIL   Abstract  This paper develops an equation that provides a good approximation of the expected number of double-disk failures (data losses) in an (N+1) redundant array of inexpensive disks (RAID) as a function of time. This paper includes the statistical bases for the equation, sources of error and inaccuracies due to approximations, and limitations of its use. The equation is simple and can be evaluated using a hand held calculator or basic spreadsheet. Accuracy depends on four input distributions, which include operational failures, operational failure restorations, latent defects, and data scrubbing. Failure and restoration distributions may represent non-homogeneous Poisson processes and, therefore, not have constant rates. Results of the equations are compared to two other hand calculation methods, to a highly accurate Monte Carlo simulation, and to actual field data for more than 10,000 RAID groups composed of 14 drives.   1. Introduction In 1978, N. K. Ouchi patented the concept of a redundant array of inexpensive disks (RAID) for data storage (US Patent 4,092,732). In the 1980's RAID became a popular method to increase both reliability and performance [1]. Since this early work, it is difficult to find any significant research paper on RAID innovation that does not discuss system (RAID group) reliability. Some researchers who attempt to quantify the effect their research has on system reliability use common tools such as Markov models while others develop their own equations.   The most common equation in use today for N+1 RAID reliability is the mean time to data loss (MTTDL) equation. The equation is based on the mean time to (catastrophic) failure (MTTF) of a hard disk drive in the RAID group, and the mean time to restore that failure (MTTR). Often times, the mean is converted into a more concrete term, the number of double disk failures (DDFs) per year. Unfortunately, the derived number of DDFs per year as a function of time based on MTTDL does not agree with actual field data [2]. An effort to create an equation for mirrored disks was reported by Baker et al. in [3]. This work adds in the occurrence and discovery of latent defects, both of which are expressed in terms of constant rates, and again attempts to calculate a MTTDL. Within the past few years, the underlying assumptions of MTTDL were reassessed, data and statistical inaccuracies identified, and a new, more accurate model developed [4]. This new model is based on statistical principles associated with non-Homogeneous Poisson processes, evaluated using Monte Carlo simulation. The simulation results follow field data closely [2, 3, 4], but this improved accuracy comes at the expense of usability, since simulations require specialized code and extensive computation. Most RAID researchers donâ€™t want to develop complex models, justify the statistical bases and run Monte Carlo simulations. Reliability assessments are necessary for understanding the impact of research, but the simulation is not the heart of their work. Researchers want an equation that is applicable to many different configurations, is easy to use, and provides results accurate enough to make decisions. The simple equation presented in this paper was developed to eliminate the difficulty of performing Monte Carlo simulations and overcome the short-comings of the existing equations. This equation is more accurate than the MTTDL, more versatile and easier to use than the equation from [3], and, unlike the MTTDL, agrees well with NetApp field data. Results from the new double disk failure equation, DDF(t), are compared against four alternative methods. All five techniques are named and referenced in Table 1. 978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
485
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:46 UTC from IEEE Xplore.  Restrictions apply. 
Table 1. Model names and descriptions     Section 2 presents a summary of system Operating Assumptions that are considered while developing the equation. Section 3 presents Related Work, including a brief discussion of Markov models, the MTTDL equation, the Baker equation, and the Monte Carlo simulation. Section 4 develops DDF(t), the new (N+1) RAID equation, which is presented at the end of section 4.2. Section 5 discusses Approximations and Section 6 shows Results, with comparisons among the three equations (MTTDL, Baker's and DDF(t)) and NetApp field data.    2. System operating assumptions 2.1. RAID system operating profiles I assume that enterprise class RAID systems are utilized 24 hours per day, 365 days per year and rarely stopped. Data written to hard disk drives (HDDs) are not checked for integrity after writing, thereby enhancing performance but allowing "write errors", which are not identified until the next time the data is read or scrubbed.  I assume that the total number of operating disks in a RAID group is N+1 for these analyses. Whether RAID 4 or RAID 5, all data is stored on N HDDs and the equivalent of one HDD is required for parity. And, I assume that all HDDs experience the same reliability related stresses so all HDDs have the same failure rate.  2.2. Latent defects and RAID scrubbing  An operational failure (i.e., the catastrophic failure of a disk) is one in which no data on a HDD can be read, often due to failure of the head that reads and writes data, or massive contamination due to the head contacting the media on the disk. A latent defect is an undiscovered or unknown data corruption that can be corrected using parity. Data loss occurs when two disks concurrently have corrupted data, termed a double disk failure (DDF). It can be two operational failures, two latent defects in the same stripe, or an operational failure and a latent defect. We can ignore the middle one because it is such a low-probability event, and focus on the other two. In practice, an operational failure concurrent with a latent defect is the big risk.  Although several references discuss the causes of corrupted data in detail [4, 5], for this paper it is sufficient to remember that data can be corrupted either 1) during the write process, in which the data written does not match its parity, or 2) by corruption after being written with valid parity, caused by contamination or head-media contact. Scrubbing is a background activity that checks the data against the parity to find latent data corruptions that have occurred during HDD use, but prior to a user requested read. During scrubbing, data is read and checked against parity. If data and parity are inconsistent, the corrupted data is recovered from other disks in the RAID stripe and rewritten to the same physical location on the HDD or, if the media is defective, written to new physical sectors on the HDD. In the latter case, the bad sectors and physically adjacent sectors are mapped out and not used. If not scrubbed, latent defects will occur and persist (accumulate) over the life of the HDD beginning at its first use. Since scrubbing eliminates latent defects, the probability of having a latent defect during a full HDD reconstructing is reduced by scrubbing. Thus, scrubbing reduces the probability of a DDF during reconstruction.  Scrubbing is a background activity performed on an as-possible basis. The time required to scrub an entire HDD is a random variable that depends on the HDD capacity, the amount of foreground activity and proprietary operating system algorithms. The time required to scrub the HDD and correct the corrupted data, given a latent defect, is also a random variable that depends on foreground activity and HDD capacity.  2.3. Reconstruction Spare HDDs are assumed always available, so the time to incorporate a spare into the RAID group is dominated by the reconstruct time, not the replacement time. If hot spares are not available, the distribution of time to restore or the MTTR must reflect the longer time. Latent defects in the media of a spare HDD do not cause HDD failure during reconstruction unless the defect is in an area used to simply stay on track, such NameDescriptionRefs.MTTDLEquation for mean time to data loss1, 6, 7, 8, 9, 10, 11BakerEquation for mean time to data loss with latent defects3MarkovMarkov Process computer analysis18, 19MC-SimMonte Carlo Simulation - computer program2, 4DDF(t)Double disk failures as a function of timeThis paper978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
486
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:56:46 UTC from IEEE Xplore.  Restrictions apply. 