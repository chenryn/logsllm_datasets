title:On the Security of Randomized Defenses Against Adversarial Samples
author:Kumar Sharad and
Giorgia Azzurra Marson and
Hien Thi Thu Truong and
Ghassan Karame
On the Security of Randomized Defenses
Against Adversarial Samples
0
2
0
2
r
a
M
6
1
]
R
C
.
s
c
[
4
v
3
9
2
4
0
.
2
1
8
1
:
v
i
X
r
a
Kumar Sharad
NEC Labs Europe
Heidelberg, Germany
PI:EMAIL
Hien Thi Thu Truong
NEC Labs Europe
Heidelberg, Germany
PI:EMAIL
ABSTRACT
Deep Learning has been shown to be particularly vulnerable
to adversarial samples. To combat adversarial strategies, nu-
merous defensive techniques have been proposed. Among
these, a promising approach is to use randomness in order to
make the classification process unpredictable and presumably
harder for the adversary to control. In this paper, we study
the effectiveness of randomized defenses against adversarial
samples. To this end, we categorize existing state-of-the-art
adversarial strategies into three attacker models of increasing
strength, namely blackbox, graybox, and whitebox (a.k.a. adap-
tive) attackers. We also devise a lightweight randomization
strategy for image classification based on feature squeezing,
that consists of pre-processing the classifier input by embed-
ding randomness within each feature, before applying feature
squeezing. We evaluate the proposed defense and compare it
to other randomized techniques in the literature via thorough
experiments. Our results indeed show that careful integration
of randomness can be effective against both graybox and black-
box attacks without significantly degrading the accuracy of
the underlying classifier. However, our experimental results
offer strong evidence that in the present form such random-
ization techniques cannot deter a whitebox adversary that has
access to all classifier parameters and has full knowledge of
the defense. Our work thoroughly and empirically analyzes
the impact of randomization techniques against all classes of
adversarial strategies.
CCS CONCEPTS
• Security and privacy → Software and application security;
• Computing methodologies → Machine learning; Computer
vision;
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made
or distributed for profit or commercial advantage and that copies bear this
notice and the full citation on the first page. Copyrights for components of this
work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6750-9/20/10...$15.00
https://doi.org/10.1145/3320269.3384751
Giorgia Azzurra Marson
NEC Labs Europe
Heidelberg, Germany
PI:EMAIL
Ghassan Karame
NEC Labs Europe
Heidelberg, Germany
PI:EMAIL
KEYWORDS
ML security; robustness to adversarial samples; feature squeez-
ing; randomization.
ACM Reference Format:
Kumar Sharad, Giorgia Azzurra Marson, Hien Thi Thu Truong, and Ghas-
san Karame. 2020. On the Security of Randomized Defenses Against
Adversarial Samples. In Proceedings of the 15th ACM Asia Conference
on Computer and Communications Security (ASIA CCS ’20), October 5–
9, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 14 pages. https:
//doi.org/10.1145/3320269.3384751
INTRODUCTION
1
Deep learning (DL) has advanced rapidly in recent years fueled
by big data and readily available cheap computation power.
Beyond standard machine learning applications, DL has been
found extremely useful in numerous security-critical applica-
tions such as handwriting recognition, face recognition [40],
and malware classification [3, 15, 45]. When used in such appli-
cations, recent studies show that DL is particularly vulnerable
to adversarial samples, which are obtained from correctly clas-
sified samples by adding carefully selected perturbations to
fool classifiers [7, 13, 24, 32]. These perturbations are so chosen
that they are large enough to affect the model prediction but
small enough to go unnoticed (e.g., through a visual check in
image recognition applications). Since DL was shown vulner-
able to adversarial samples, numerous attacks and defenses
have been developed back and forth [28, 30, 34, 44].
While these back-and-forth attacks and defenses have clearly
advanced state of the art, it is essential to analyze their robust-
ness in different adversarial models to understand how benefi-
cial they are in making DL more robust. Here, we distinguish
between three classes of attacker models depending on the
adversary’s knowledge with regards to the classifier’s details:
blackbox (a.k.a. non-adaptive), meaning that the adversary only
knows public information, whitebox (a.k.a. fully adaptive), i.e.,
the adversary knows full details of the classifier including any
defense in place, and graybox (a.k.a. semi-adaptive), reflecting
partial knowledge of the classifier’s internals.
A popular defensive technique utilizes randomness in the
classification process, with the hope to enlarge the search space
of successful adversarial perturbations. The use of randomness
to enhance robustness of DL classifiers has been proposed in
many different flavors, both at training and classification time,
ranging from randomizing the input to modifying the neural
network itself in a randomized fashion. Although many works
hint at the potential of such a technique [5, 16, 43, 46], there is
still lack of analysis within the community on the robustness
of this strategy against state-of-the-art attacks.
Contributions. In this work, we study the effectiveness of pre-
processing randomized defenses against a wide variety of
adversarial strategies, including the strongest whitebox at-
tacks to date. Specifically, we develop a security model to for-
mally define robustness of machine learning algorithms under
the various adversarial strategies that populate the literature
(cf. Section 3). Our model, inspired by cryptographic defini-
tions of security, is generic and captures a broad variety of
machine learning classifiers. To investigate the effectiveness of
randomization on the classifier’s robustness, we present a light-
weight defensive strategy, Randomized Squeezing, that combines
the prominent pre-processing defense Feature Squeezing [44]
with input randomization (cf. Section 4). We also compare Ran-
domized Squeezing with two other instantiations of random-
ized pre-processing techniques: the Cropping-Rescaling defense
of Guo et al. [16], and Region-Based Classification by Cao and
Gong [5]. We empirically compare the effectiveness of Ran-
domized Squeezing, Cropping-Rescaling, and Region-Based
Classification, against state of the art attack strategies to gener-
ate adversarial samples from MNIST, CIFAR-10, and ImageNet
datasets (cf. Section 5).
Our proposal embeds randomness within the input to the
classifier, operating on every pixel of the image independently
by adding randomly chosen perturbations to all pixels, prior
to applying the squeezing function. The combination of in-
put randomization and squeezing instantiates a specific pre-
processing transformation, similarly to Cropping-Rescaling
and Region-Based Classification. For all three randomized
techniques, our empirical evaluation shows that introducing
an appropriate amount of randomness at pixel level does not
significantly hamper accuracy and, in case of Randomized
Squeezing, it also improves robustness against graybox adver-
saries [7, 13, 18, 24, 29, 33] compared to deterministic Feature
Squeezing. Our results further highlight that, despite the per-
turbation induced by randomizing test images, Region-Based
Classification and Randomized Squeezing can achieve high
accuracy and robustness without transforming training sam-
ples. This is in contrast to prior findings [16] hinting that input
transformation can be effective against adversarial samples,
provided that the same transformation is also applied at train-
ing time, and opens the possibility to leverage randomness to
realize online defenses—which can enhance the robustness of
pre-trained classifiers in a flexible and efficient manner.
To further evaluate the three defenses in the whitebox model,
we consider the strongest currently known attacks, tuned for
each defense: the Backward Pass Differentiable Approximation
(BPDA) [1] and the Expectation Over Transformation (EOT) [2].
Our results indicate that while these adaptive attacks defeat
all three defenses, increasing the amount of randomness used
by the defense results in a higher number of iterations, respec-
tively, larger perturbations, necessary for the attacks to succeed.
This suggests that even in the case of whitebox attacks, ran-
domness may have a positive, although limited, impact on the
classifier’s robustness—in the sense of forcing the BPDA and
EOT attacks to invest greater effort to craft high-confidence
adversarial samples. Our results also support the intuition
that introducing unpredictability to the classification process
makes it more challenging, for state-of-the-art adaptive attacks,
to find adversarial perturbations which are successful regard-
less of the randomness.
It is therefore plausible that some DL applications, which
can reasonably constrain the attacker by limiting the distortion
and/or requiring that adversarial samples be generated in real
time, may safely employ randomized classifiers even against
(properly constrained) whitebox attacks.
As far as we are aware, this is the first work that compre-
hensively analyses the impact of randomness on DL classifiers
under all state-of the art adversarial strategies, covering also
the whitebox attacker model.
2 BACKGROUND
In this section, we introduce notation and relevant concepts
for the subsequent sections.
Notation & Conventions. Let X be a (finite) set, and letD : X →
[0, 1] be a probability distribution. We denote by x ←D X
the random sampling of an element x according to distribu-
tion D; we write x ←$ X for sampling x uniformly at random.
We denote by f : X → Y the function defining the classifi-
cation problem of interest (a.k.a. “ground truth”), where X
and Y are the sets of instances and of corresponding classes
(or labels), respectively. A machine-learning classifier C is an
algorithm that aims at emulating function f . Typically, the
classifier is deterministic and can be thus thought of as a func-
tion itself. In this work, we cover a broader class of classifiers
and let C be any, possibly randomized algorithm. If C is ran-
domized, we write y ←$ C(x) to denote that on input x the
classifier, run on freshly sampled randomness, outputs label y.
Let C : X → Y be a deterministic classifier. For X′ ⊆ X we
denote by X′✓(C) = {x ∈ X′
: C(x) = f (x)} the set of in-
stances in X′ where C agrees with f . Similarly, we denote
by X′✗(C) = {x ∈ X′
: C(x) (cid:44) f (x)} the set of misclassi-
fied instances. Using this notation, we measure a classifier’s
(empirical) accuracy, respectively, (empirical) error w.r.t. a given
dataset D = {(x, f (x)) : x ∈ XD}, for some XD ⊂ X as
✗ (C)|/|XD|,
accXD (C) := |XD
respectively. For randomized classifiers, the definitions of ac-
curacy and error need to be augmented for incorporating the
randomness of the classifier. Note that accuracy and error can
also be used to capture the performance of a classifier w.r.t. an
adversarially chosen distribution DA, respectively, input set XA.
2.1 Adversarial Samples
The accuracy of a classifier is measured w.r.t. samples drawn
from a ‘natural’ distribution D : X → [0, 1] over the input
✓ (C)|/|XD| and errXD (C) := |XD
space. This approach is grounded in results from computa-
tional learning theory [41], which guarantee a low classifica-
tion error as long as samples used at test time originate from
the same distribution of the training samples. While this as-
sumption may be realistic in a pure machine-learning setting,
it is hard to justify in general. In cybersecurity, e.g., the “test
samples” are generated by an adversary A attempting to by-
pass an ML protected system, and may thus be specifically
crafted to deviate from the training samples. This state of affair
has been confirmed by the recent advances in attacking ML
systems through adversarial samples [39].
An adversarial sample x′ is derived from a labeled sam-
ple (x, y) by slightly perturbing x, so that x′ still belongs to the
original class y, yet it is classified wrongly. Formally: x and x′
have a relatively small distance d(x, x′) ≤ ϵ, f (x′) = y, and
C(x′) (cid:44) y. The tolerated amount of perturbation ϵ is called dis-
tortion (a.k.a. adversarial budget). The three most common met-
rics to measure the distance between an adversarial sample x′
and its legitimate counterpart x are based on the Lp-norms
the number of modified pixels; (ii) d2(x, x′) =(cid:0)
(L0, L2, and L∞): (i) d0(x, x′) = |{i : xi − x′i
(cid:44) 0}|, based on
2 ,
based on the Euclidean distance; (iii) d∞
(x, x′) = maxi(xi −
x′i), based on the maximum difference between pixels at cor-
responding positions, where xi − x′i
is the difference between
pixels at position i of images x and x′, respectively. For a dis-
tance metric dp, we denote by || · ||p the corresponding norm.
Depending on the attacker’s goal, adversarial samples can be
categorized as targeted and untargeted. A targeted adversarial
sample x′ is successful if the classifier’s prediction matches
an attacker-chosen label yt (cid:44) y, where y is the true label. An
untargeted adversarial sample instead succeeds if the classifier
predicts any label other than y.
DL classifiers are the Fast Gradient Sign Method (FGSM) [13],
the Basic Iterative Method (BIM) [24], the Jacobian Saliency
Map Approach (JSMA) [24], the Carlini-Wagner (CW) attacks [7],
and DeepFool [29]. Among others, we will consider these at-
tack strategies in our evaluation (cf. Section 5).
i(xi − x′i)2(cid:1) 1
Prominent techniques to generate adversarial examples against
2.2 Defensive Techniques
Here we discuss the defenses against adversarial samples
which are most relevant to our work. We survey more de-
fensive techniques in Section 6.
Feature Squeezing. This technique, introduced by Xu et al. [44],
transforms the input by reducing unnecessary features while
keeping the DL model intact. Feature Squeezing is a generic
transformation technique to reduce feature input space such
that it can limit opportunities for an adversary to generate
adversarial samples. The approach assumes that legitimate
samples have same output on original and transformed form
while adversarial samples have larger difference on outputs,
the discrepancy of outputs helps to reject adversarial samples.
In this paper, we study the two proposed squeezing techniques,
squeezing color bit depths and spatial smoothing.
Squeezing color bits relies on the assumption that large color
bit depth is not necessary for a classifier to interpret an image.
The authors consider 8-bit gray scale images of size 28 × 28
pixels (MNIST dataset) and 24-bit color images of size 32 × 32
pixels (CIFAR-10 and ImageNet datasets) in their experiments.
The 8-bit gray scale images are squeezed to 1-bit monochrome
images by using a binary filter with cut-off set to 0.5, while
each channel of the 24-bit color images (8-bit per color channel)
is squeezed to 4 or 5 bits. Each channel can be reduced to i-bit
depth by multiplying the input value with 2i − 1, rounded up
to integers and then divided by 2i − 1 to scale back to [0,1].
The local smoothing is a type of spatial smoothing technique
that adjusts the value of each pixel based on aggregated values,
e.g., by taking median of its neighborhood pixels. The median
smoothing technique follows Gaussian smoothing design. The
values of neighborhood pixels are decided by a configurable
sliding window of which size ranges from 1 to entire image. Ex-
periments in [44] show that median smoothing with 2 × 2 and
3 × 3 sliding window is effective. Another way to perform spa-
tial smoothing is non-local smoothing. Non-local smoothing
considers a large area to compute replacement value for each
pixel. Given an image patch, non-local smoothing searches
for all similar patches and replaces the center patch with the
average of similar patches. We use the notation proposed by
Xu et al. [44] to denote a filter as “non-local means (a-b-c)”,
where a is the search window a × a, b the patch size b × b and
c the filter strength.
Randomness-Based Defenses. The literature features a num-
ber of strategies to use randomness for enhancing DL clas-
sifiers against adversarial samples. Zhou et al. [46] propose
two ways to use randomness for strengthening deep neural-
network (DNN) models: to add random noise to the weights
of a trained DNN model, and to select a model at random