for rectangle queries via private partitions. In International Conference on the
Theory and Application of Cryptology and Information Security, pages 735–751.
Springer, 2015.
[20] C. Dwork and A. Roth. The algorithmic foundations of differential privacy.
Foundations and Trends in Theoretical Computer Science, 9(3-4), 2014.
[21] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting and differential privacy.
In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages
51–60. IEEE, 2010.
[22] Ú. Erlingsson, V. Feldman, I. Mironov, A. Raghunathan, K. Talwar, and A. Thakurta.
Amplification by shuffling: From local to central differential privacy via
anonymity. arXiv preprint arXiv:1811.12469, 2018.
privacy-preserving ordinal response. In CCS, 2014.
[24] L. Fan and L. Xiong. An adaptive approach to real-time aggregate monitoring
with differential privacy. IEEE Transactions on knowledge and data engineering,
26(9):2094–2106, 2013.
[25] F. Fioretto and P. Van Hentenryck. Optstream: releasing time series privately.
[23] Ú. Erlingsson, V. Pihur, and A. Korolova. RAPPOR: randomized aggregatable
[26] M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of differentially
Journal of Artificial Intelligence Research, 65:423–456, 2019.
private histograms through consistency. PVLDB, 3(1), 2010.
evolving data.
2375–2384, 2018.
[27] M. Joseph, A. Roth, J. Ullman, and B. Waggoner. Local differential privacy for
In Advances in Neural Information Processing Systems, pages
[28] S. P. Kasiviswanathan, H. K. Lee, K. Nissim, S. Raskhodnikova, and A. Smith.
What can we learn privately? SIAM Journal on Computing, 40(3):793–826, 2011.
[29] G. Kellaris, S. Papadopoulos, X. Xiao, and D. Papadias. Differentially private
event sequences over infinite streams. Proceedings of the VLDB Endowment,
7(12):1155–1166, 2014.
[30] I. Kotsogiannis, Y. Tao, X. He, M. Fanaeepour, A. Machanavajjhala, M. Hay, and
G. Miklau. Privatesql: a differentially private sql query engine. Proceedings of the
VLDB Endowment, 12(11):1371–1384, 2019.
[31] J. Lee, Y. Wang, and D. Kifer. Maximum likelihood postprocessing for differential
privacy under consistency constraints. In KDD, 2015.
[32] N. Li, W. Qardaji, D. Su, and J. Cao. Privbasis: Frequent itemset mining with
differential privacy. Proceedings of the VLDB Endowment, 5(11):1340–1351, 2012.
[33] Z. Li, T. Wang, M. Lopuhaä-Zwakenberg, B. Skoric, and N. Li. Estimating numer-
ical distributions under local differential privacy. In SIGMOD, 2020.
[34] A. Molina-Markham, P. Shenoy, K. Fu, E. Cecchet, and D. Irwin. Private memoirs
of a smart meter. In Proceedings of the 2nd ACM workshop on embedded sensing
systems for energy-efficiency in building, pages 61–66, 2010.
[35] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in
private data analysis. In Proceedings of the thirty-ninth annual ACM symposium
on Theory of computing, pages 75–84. ACM, 2007.
[36] V. Perrier, H. J. Asghar, and D. Kaafar. Private continual release of real-valued
data streams. ndss, 2019.
differentially private histograms. PVLDB, 6(14), 2013.
set-valued data with local differential privacy. In CCS, 2016.
[39] V. Rastogi and S. Nath. Differentially private aggregation of distributed time-
series with transformation and encryption.
In Proceedings of the 2010 ACM
SIGMOD International Conference on Management of data, pages 735–746, 2010.
[40] N. Wang, X. Xiao, Y. Yang, T. D. Hoang, H. Shin, J. Shin, and G. Yu. Privtrie:
Effective frequent term discovery under local differential privacy. In ICDE, 2018.
[41] N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, and G. Yu. Collecting
and analyzing multidimensional data with local differential privacy. In Proceedings
of IEEE ICDE, 2019.
[42] Q. Wang, Y. Zhang, X. Lu, Z. Wang, Z. Qin, and K. Ren. Real-time and spatio-
temporal crowd-sourced social network data publishing with differential privacy.
IEEE Transactions on Dependable and Secure Computing, 15(4):591–606, 2016.
frequency estimation. In USENIX Security, 2017.
[44] T. Wang, B. Ding, J. Zhou, C. Hong, Z. Huang, N. Li, and S. Jha. Answering
multi-dimensional analytical queries under local differential privacy. In SIGMOD,
2019.
[45] T. Wang, N. Li, and S. Jha. Locally differentially private frequent itemset mining.
[38] Z. Qin, Y. Yang, T. Yu, I. Khalil, X. Xiao, and K. Ren. Heavy hitter estimation over
[43] T. Wang, J. Blocki, N. Li, and S. Jha. Locally differentially private protocols for
[37] W. H. Qardaji, W. Yang, and N. Li. Understanding hierarchical methods for
[46] T. Wang, N. Li, and S. Jha. Locally differentially private heavy hitter identification.
In SP, 2018.
IEEE TDSC, 2019.
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1249[47] T. Wang, Z. Li, N. Li, M. Lopuhaä-Zwakenberg, and B. Skoric. Consistent and
accurate frequency oracles under local differential privacy. In NDSS, 2020.
[48] Z. Wang, W. Liu, X. Pang, J. Ren, Z. Liu, and Y. Chen. Towards pattern-aware
privacy-preserving real-time data collection. In IEEE INFOCOM 2020-IEEE Con-
ference on Computer Communications, pages 109–118. IEEE, 2020.
[49] S. L. Warner. Randomized response: A survey technique for eliminating evasive
answer bias. Journal of the American Statistical Association, 60(309), 1965.
[50] R. J. Wilson, C. Y. Zhang, W. Lam, D. Desfontaines, D. Simmons-Marengo, and
B. Gipson. Differentially private sql with bounded user contribution. In Inter-
national Symposium on Privacy Enhancing Technologies Symposium. Springer,
2020.
[51] M. Xu, B. Ding, T. Wang, and J. Zhou. Collecting and analyzing data jointly from
[53] M. Ye and A. Barg. Optimal schemes for discrete distribution estimation under
[54] C. Zeng, J. F. Naughton, and J.-Y. Cai. On differentially private frequent itemset
[52] J. Yang, T. Wang, N. Li, X. Cheng, and S. Su. Answering multi-dimensional range
multiple services under local differential privacy. VLDB, 2020.
queries under local differential privacy. VLDB, 2021.
locally differential privacy. IEEE Transactions on Information Theory, 2018.
mining. Proceedings of the VLDB Endowment, 6(1):25–36, 2012.
marginal for marginal release under local differential privacy. In CCS, 2018.
[56] Z. Zheng, R. Kohavi, and L. Mason. Real world performance of association rule
algorithms. In Proceedings of the seventh ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 401–406. ACM, 2001.
[55] Z. Zhang, T. Wang, N. Li, S. He, and J. Chen. Calm: Consistent adaptive local
A SUPPLEMENTARY MECHANISMS OF DP
We review the method of smooth sensitivity that PAK [36] use for
estimating the percentile.
Smooth Sensitivity. Rather than the global sensitivity that con-
siders any pair of neighboring sequences, the local sensitivity fixes
one dataset V and only considers all possible neighboring sequence
V ′ around V .
LSV (f ) = max
V ′:V ′≃V
|| f (V) − f (V
′)||1.
The advantage of using local sensitivity is that we only need
to consider neighbors of V which could result in lower sensitivity
of the function f , and consequently lower noise added to the true
answer f . Unfortunately, replacing the global sensitivity with local
sensitivity directly (e.g., in the Laplace mechanism) violates DP.
This is handled by using smooth sensitivity [35] instead.
For b > 0, the b-smooth sensitivity of f at V ∈ V, denoted by
SSV ,b(f ), is defined as
(cid:110)
−b·d(V ,V ′)(cid:111)
,
SSV ,b(f ) = max
V ′
LSV ′(f ) · e
where d(V , V ′) denotes the hamming distance between V and V ′.
The method of smooth sensitivity is given below:
A(V) = f (V) +
SSV ,b
a
· Z ,
1
where Z is a random variable from some specified distribution. To
obtain (ϵ, 0)-DP, Z is drawn from the Cauchy distribution with
density ∝
1+|z|γ for γ > 1. But to use an exponentially decaying
distribution (which gives good accuracy), such as standard Laplace
or Gaussian distribution, one can only obtain (ϵ, δ)-DP. For both,
we set b ≤
ϵ−2 log(δ) as the smoothing parameter. If we use the
Laplace distribution with scale 1, a = ϵ2 . When the noise is standard
Gaussian, then a =
[35].
ϵ√− ln δ
B MORE DETAILS ABOUT PAK
PAK computes the smooth sensitivity of the empirical p-quantile,
i.e., ˆxp, as
(cid:26)
SSV ,b(ˆxp) =
−bk ·
e
max
k =0,1, ...,m+1
max
t =0,1, ...,k +1
[V s(P + t) − V s(P + t − k − 1)]
.
(cid:27)
Here, V s is the sorted string of the first m values of V in ascending
order, where V s(i) = 0 if i  m. And P is the rank of
ˆxp. After computing the smooth sensitivity, Nissim et al.[35] sets
the threshold θ as
θ = ˆxp +
SSV ,b(ˆxp)
· Z ,
where Z is a random variable from some specified distribution in
order to satisfy DP.
PAK [36] propose to bound Pr(cid:2)θ < xp
(cid:3) to be arbitrarily small
a
(by an arbitrary β) and thus uses
κSSV ,b(ˆxp)
θ = ˆxp +
a
(cid:18)
−1
ns (1 − β)),
· (Z + G
1 − (eb−1)G−1
ns (1−βlt)
a
(cid:19)−1
and Gns
where κ is a positive real number
denotes the CDF of the distribution of Z.
The threshold θ released via the above mechanism is differen-
tially private since κSSV ,b(ˆxp) is a smooth upper bound of ˆxp and
κ only depends on public parameters.
In their evaluation, the authors aim to get the 99.5-percentile
and set p = 99.575, β = 0.3 · 0.02, and δ = 1/n
C CONSISTENCY ALGORITHM
Off-line Consistency [26]. We use x to denote a node on the
hierarchy H, and let ℓ(x) to be the height of x (the height of a leaf
is 1; and root is of height h). We also denote prt(x), chd(x), and
sbl(x) to denote the children, parent, and siblings of x, respectively.
We use H(x) to denote the value corresponding to node x in the
hierarchy. The first step updates the values in H from bottom to
top. The leaf nodes remain the same; and for each height-ℓ node x,
where ℓ iterates from 2 up to h, we update it
2.
H(x) ← bl − bl−1
bl − 1 H(x) + bl−1 − 1
bl − 1
H(y).
(10)

y∈chd(x)
We then update the values again from top to bottom. This time
the value on root remains the same, and for each height-ℓ node x,
where ℓ iterates from h − 1 down to 1, we update it
(cid:169)(cid:173)(cid:171)H(prt(x)) − 
y∈sbl(x)
H(y)(cid:170)(cid:174)(cid:172) .
(11)
H(x) ← b − 1
b
H(x) +
1
b
An Online Algorithm. We decompose the noisy values in H
into two parts: the true values and the pure noises, denoted as T
and N , respectively. N is the independent noise from the Laplace
mechanism (described in Section 2.3). T is defined by induction: for
a leaf x, T(x) corresponds to one true value v, and for a node x in
y∈chd(x) T(y). The true values from T are
a higher level, T(x) =
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1250consistent naturally. Thus if N is consistent, H is also consistent.
To see it, consider any internal node x,
=b − 1
b
H(x) +
1
b
H(x) = T(x) + N(x)
= 
= 
y∈chd(x)
T(y) + 
(T(y) + N(y)) = 
y∈chd(x)
N(y)
y∈chd(x)
H(y).
y∈chd(x)
In the online consistency algorithm, we internally generate the
noise hierarchy N and run the steps given in Equation 10 and Equa-
tion 11 to make N consistent. After this pre-processing step, we
can ignore the higher-layers of the hierarchy, and use only the leaf
nodes which add consistent noise to each individual value. This is
because the results from the higher-layers are already consistent
with those from the leaves, thus it suffices to only output the most
fine-grained result.
The online consistency algorithm satisfies DP as long as it gives
identical output distribution as the offline algorithm [26]. Now we
prove this fact. We first restate the theorem (Theorem 3.1):
Theorem C.1. The online consistency algorithm gives identical
results as the off-line consistency algorithm.

Proof. We first examine the bottom-up update step. According
to Equation 10, the updated N(x) equals to
N(y).
y∈chd(x)
y∈chd(x)
N(y) + T(x)
Adding T(x) to it, we have the updated H(x) equals to
bl − bl−1
bl − 1 N(x) + bl−1 − 1
bl − 1

bl − bl−1
bl − 1 N(x) + bl−1 − 1
bl − 1
bl − 1 (N(x) + T(x)) + bl−1 − 1
=bl − bl−1
bl − 1
bl − 1 (N(x) + T(x)) + bl−1 − 1
=bl − bl−1

bl − 1
bl − 1 H(x) + bl−1 − 1
=bl − bl−1