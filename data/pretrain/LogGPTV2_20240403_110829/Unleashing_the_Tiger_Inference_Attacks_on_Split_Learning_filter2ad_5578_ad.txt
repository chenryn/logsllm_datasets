bars) is only slightly larger than the one for the classes represented
in 𝑋𝑝𝑢𝑏. Here, the attacker can successfully recover a suitable ap-
proximation of instances of the unobserved class by interpolating
over the representations of observed instances. The only outlier is
the case 𝑋𝑝𝑢𝑏/{0}. Our explanation is that the digit zero is peculiar,
so it is harder to describe it with a representation learned from
the other digits. Nevertheless, as depicted in Figure 9, the FSHA
(a) Reconstruction 0 with 𝑋𝑝𝑢𝑏/{0}.
(b) Reconstruction 1 with 𝑋𝑝𝑢𝑏/{1}.
Figure 9: Two examples of inference of private training
instances from smashed data given mangled 𝑋𝑝𝑢𝑏. In the
panel (a), the adversary carried out the attack without ever di-
rectly observing training instances representing the digit “0”.
Panel (b) reproduces the same result for the digit “1”. Only
the reconstruction of instances of the class unknown to the
attacker are reported. Those have been sampled from 𝑋𝑝𝑟𝑖𝑣.
Figure 8: Each bar represents the final reconstruction error of
private data obtained with an FSHA based on a 𝑋𝑝𝑢𝑏 mangled
of a specific class. Black bars report the average reconstruc-
tion error of private data instances of classes known to the
attacker. Instead, red bars report the average reconstruction
error of private data instances for the removed class. In the
attacks, we used 15000 setup iterations.
We report the average reconstruction error of the attack together
with the best-case scenario for the attacker (CelebA train/validation
partitions as 𝑋𝑝𝑟𝑖𝑣 and 𝑋𝑝𝑢𝑏) in Figure 6. As can be observed, the
discrepancy between private and public distributions affects the
attack performance negligibly, and the FSHA can converge towards
accurate reconstructions of private data. Figure 7a depicts examples
of such reconstructions.
For the sake of completeness, we report the results also for the
opposite scenario: attacking UTKFace with CelebA as a public set.
We obtained almost identical performance as shown in Figures 6
and 7b. Interestingly, in this case, the attack has also successfully
reconstructed images of infants and gray-scale pictures that are
missing in the CelebA distribution.
In Appendix A.1, we repeat similar tests for the natural-image
datasets TinyImageNet [60] and STL-10 [16], and dermatoscopic
images datasets HAM10000 [53] and ISIC-2016 [26], obtaining con-
sistent results.
3.4.2 Public dataset with missing modalities. Another interesting
scenario is when the attacker’s public set misses some modali-
ties / semantic-classes of the private distribution. To simulate this
scenario, we create artificially mangled training sets 𝑋𝑝𝑢𝑏 for the
MNIST dataset and test the attack’s effectiveness accordingly. In the
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2120It is enough to substitute the network ˜𝑓 −1 with a classifier 𝐶𝑎𝑡𝑡
trained to detect a particular attribute in the data points of ˜Z. How-
ever, unlike the previous formulation of the attack, the attacker has
to resort to a supervised training set (𝑋𝑝𝑢𝑏, 𝑌𝑝𝑢𝑏) to define the tar-
get attribute. Namely, each instance of the attacker’s dataset 𝑋𝑝𝑢𝑏
must be associated with a label that expresses the attribute/property
att that the attacker wants to infer from the smashed data.
In the case of a binary attribute, the attacker has to train 𝐶𝑎𝑡𝑡 in
a binary classification using a binary cross-entropy loss function.
Here, we implement the network 𝐶𝑎𝑡𝑡 to be as simple as possible to
maximize the separability of the classes directly on ˜Z. In particular,
we model 𝐶𝑎𝑡𝑡 as a linear model by using a single dense layer. In
this way, we force the representations of the classes to be linearly
separable, simplifying the inference attack once the adversarial loss
has forced the topological equivalence between the codomains of 𝑓
and ˜𝑓 . We leave the other models and hyper-parameters unchanged.
In the experiments, we aim at inferring the binary attribute
“gender” (i.e., 0 =“man”; 1 =“woman”) from the private training
instances used by the clients. Following the results of Section 3.4.1,
we validate the proposed inference attack on different combinations
of the datasets CelebA and UTKFace for 𝑋𝑝𝑟𝑖𝑣 and 𝑋𝑝𝑢𝑏. During
the attack, we track the accuracy of the inference attacks. They
are reported in Figure 10, where all the attacks reach an accuracy
higher than 90% within a limited number of iterations compared to
the complete reconstruction attack.
It is important to note that the property inference attack can be
extended to any feature or task. For instance, the attacker can infer
multiple attributes simultaneously by training 𝐶𝑎𝑡𝑡 in a multi-label
classification rather than a binary one. The same applies to multi-
class classification and regression tasks. In this direction, the only
limitation is the attacker’s capability to collect suitable labeled data
to set up the attack. Appendix A.2 reports an additional example
for a multi-class classification task.
3.6 Attack Implications
The implemented attacks demonstrated how a malicious server
could subvert the split learning protocol and infer information over
the clients’ private data. Here, the adversary can recover the single
training instance from the clients and fully expose the distribu-
tion of the private data. Unlike previous attacks in collaborative
learning [30, 64], the server can always determine exactly which
client owns a training instance upon receiving the clients’ disjointed
smashed data8, further harming client’s privacy.
In the next section, we discuss the shortcomings of defense
strategies proposed to prevent inference attacks.
4 ON DEFENSIVE TECHNIQUES
As demonstrated by our attacks, simply applying a set of neural
layers over raw data cannot yield a suitable security level, especially
when the adversary controls the learning process. As a matter of
fact, as long as the attacker exerts influence on the target function of
the clients’ network, the latter can always be lead to insecure states.
Unfortunately, there does not seem to be any way to prevent the
server from controlling the learning process without rethinking the
8In split learning, the clients’ activation cannot be aggregated.
Figure 10: Examples of property inference attack on the
CelebA and UTKFace datasets. The plots report the accuracy
in inferring the attribute “gender” from instances of 𝑋𝑝𝑟𝑖𝑣
during the setup phase of the attacks.
provides an accurate reconstruction also in the cases of 0 and 1.
Summing up, the public set leveraged by the attacker does impact
the performance of the attack. Obviously, when the distribution
of the public dataset is closer to the attacked one, it is possible to
achieve a better reconstruction. However, as shown by the reported
results, the attack is resilient to discrepancies of the public distri-
bution, and it is capable of converging to precise reconstructions
nonetheless. More interestingly, the attack procedure can general-
ize over unobserved modalities of the private distribution, allowing
the attacker to leak suitable reconstructions of completely unob-
served/unknown data classes. Eventually, these general properties
of the attack make it applicable to realistic threat scenarios, where
the adversary has just a limited knowledge of the target private
sets.
3.5 Property inference attacks
In the previous setup, we demonstrated that it is possible to recover
the entire input from the smashed data. However, this type of infer-
ence may be sub-optimal for an attacker interested in inferring only
a few specific attributes/properties of the private training instances
(e.g., the gender of the patients in medical records); rather than re-
constructing 𝑋𝑝𝑟𝑖𝑣 entirely. This form of inference was introduced
in [8] and extended to neural networks in [21]. Property inference
is simpler to perform and more robust to possible defensive mecha-
nisms (see Section 4). Next, we briefly show how the Feature-space
Hijacking Attack can be extended to perform property inference
attacks.
As discussed in Section 3.2, we can force arbitrary properties
on the smashed data produced by the clients by forging a tailored
feature space ˜Z and forcing the clients’ network 𝑓 to map into it. The
feature space ˜Z is dynamically created by training a pilot network ˜𝑓
in a task that encodes the target property. In the attack of Figure 2,
we requested the invertibility of ˜Z by training ˜𝑓 in an auto-encoding
task with the support of a second network ˜𝑓 −1. Conversely, we can
force the smashed data to leak information about a specific attribute
by conditioning the feature space ˜Z with a classification task.
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2121(a) Default: task loss ×1
(b) Re-weighted: task loss ×25.
Figure 11: Effect of the distance correlation minimization defense on FSHA for the MNIST dataset. Each curve in the figures
depicts the reconstruction error of private data during the setup phase for a different value of 𝛼1 imposed by the client. The
two panels report the effect of scaling the task loss (e.g., 𝛼2) server-side.
entire protocol from scratch. Next, we reason about the effectiveness
of possible defense strategies.
4.1 Distance correlation minimization
In [55, 58], the authors propose to artificially reduce the correlation
between raw input and smashed data by adding a regularization
during the training of the distributed model in split learning. In par-
ticular, they resort to distance correlation [50]—a well-established
measure of dependence between random vectors. Here, the clients
optimize 𝑓 to produce outputs that minimize the target task loss
(e.g., a classification loss) and the distance correlation. This regu-
larization aims at preventing the propagation of information that
is not necessary to the final learning task of the model from the
private data to the smashed one. Intuitively, this is supposed to
hamper the reconstruction of 𝑋𝑝𝑟𝑖𝑣 from an adversary that has
access to the smashed data.
More formally, during the split learning protocol, the distributed
model is trained to jointly minimize the following loss function:
𝛼1 · 𝐷𝐶𝑂𝑅(𝑋𝑝𝑟𝑖𝑣, 𝑓 (𝑋𝑝𝑟𝑖𝑣)) + 𝛼2 · 𝑇 𝐴𝑆𝐾(𝑦, 𝑠(𝑓 (𝑋𝑝𝑟𝑖𝑣))),
(4)
where 𝐷𝐶𝑂𝑅 is the distance correlation metrics, 𝑇 𝐴𝑆𝐾 is the task
loss of the distributed model (e.g., cross-entropy for a classification
task), and 𝑦 is a suitable label for the target task (if any). In the
equation, the hyper-parameters 𝛼1 and 𝛼2 define the relevance of
distance correlation in the final loss function, creating and manag-
ing a tradeoff between data privacy (i.e., how much information an
attacker can recover from the smashed data) and model’s utility on
the target task (e.g., the accuracy of the model in a classification
task). Note that the distance correlation loss depends on just the
client’s network 𝑓 and the private data 𝑋𝑝𝑟𝑖𝑣. Thus, it can be com-
puted and applied locally on the client-side without any influence
from the server.
Even if the approach proposed in [55, 58] seems to offer reason-
able security in the case of a passive adversary, it is, unfortunately,
ineffective against the feature-space hijacking attack that influ-
ences the learning process of 𝑓 . As a matter of fact, the learning
objective injected by the attacker will naturally negate the distance
correlation minimization, circumventing its effect. Moreover, this
defensive technique does not prevent the property inference attack
detailed in Section 3.5.
Figure 11a reports on the impact of the distance correlation min-
imization on the FSHA on the MNIST dataset for different values
of 𝛼1. In the plot, we start from 𝛼1 = 100, which is the smallest as-
signment of 𝛼1 that does not affect the attack’s performance, and we
increase it until we reach impractical high values e.g., 𝛼1 = 10000.
As shown in the plot, the defense becomes effective when 𝛼1 reaches
very high values. In these cases, the privacy loss completely eclipses
the task loss of the distributed model (i.e., Eq. 4). As a result, im-
proving 𝑓 in reducing the task loss becomes either impossible or
extremely slow. Intuitively, this value of 𝛼1 prevents the distributed
model from achieving any utility on the imposed task. This is so
regardless of whether the model is trained on the task originally
selected by the clients or the adversarial task enforced by the mali-
cious server.
Nevertheless, even if the clients set the parameter 𝛼1 to a large
value, they have no effective method to control 𝛼2 if the server is
malicious. Indeed, even in the label-private setting of split learning
(i.e., Figure 1b), the server can arbitrarily determine the training
objective for the model and adjust the task loss 𝑇 𝐴𝑆𝐾. Trivially,
this allows the attacker to indirectly control the ratio between the
privacy loss (which is performed locally at the client) and the target
loss (i.e., the adversarial loss imposed by the attacker), nullifying
the effect of a heavy regularization performed at the client-side.
Figure 11b explicates how the malicious server circumvents the
client-side defense by just scaling the adversarial loss function by a
factor of 25. In this case, even impractically large values of 𝛼1 are
ineffective.
To improve the defense mechanism above, one could apply gra-
dient clipping on the gradient sent by the server during the training.
However, gradient clipping further reduces the utility of the model
as it weakens the contribution of the target loss function in the case
of an honest server.
Additionally, it is possible to devise a more general strategy and
allow a malicious server to adopt advanced approaches to evade
the defenses implemented in [55, 58]. Indeed, distance correlation
can be easily circumvented by forging a suitable target feature
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2122space. The key idea is that the attacker can create an “adversarial”
feature space that minimizes the distance correlation but allows the
attacker to obtain a precise reconstruction of the input. We detail
this possibility in the Appendix C. Once the adversarial feature
space is obtained, the attacker can hijack 𝑓 , minimize the distance
correlation loss of 𝑓 , and recover the original data precisely.
4.2 Detecting the attack
Alternatively, clients could detect the feature-space hijacking attack
during the training phase and then halt the protocol. Unfortunately,
detecting the setup phase of the attack seems to be a complex
task due to the clients’ incomplete knowledge of the distributed
model. Here, clients could continuously test the effectiveness of
the network on the original training task and figure out if the
training objective has been hijacked. However, clients have no
access to the full network during training and cannot query it to
detect possible anomalies. This is also true for the private-label
scenario, i.e., Figure 1b of split learning, where clients compute the
loss function on their devices. Indeed, in this case, the attacker can
simply provide fake inputs to 𝑓 ′ (see Figure 1b) that has been forged
to minimize the clients’ loss. For instance, the attacker can simply
train a second dummy network ˜𝑠 during the setup phase and send
its output to the client. Here, the network ˜𝑠 receives the smashed
data as input and is directly trained with the gradient received from
𝑓 ′ to minimize the loss function chosen by the client. It’s important
to note that, during the attack, the network 𝑓 does not receive the
gradient from ˜𝑠 but only from 𝐷.
5 THE SECURITY OF SPLIT LEARNING
AGAINST MALICIOUS CLIENTS
In recent works [57], the authors claim that the splitting methodol-
ogy could prevent client-side attacks that were previously devised
against federated learning, such as the GAN-based attack [30]. Ac-
tually, we show that the attacks in [30] (albeit with some minimal
adaptations) remain applicable even within the split learning frame-
work.
Client-side attack on Federated Learning. The attack [30] works
against the collaborative learning of a classifier 𝐶 trained to classify
𝑛 classes, say 𝑦1, . . . , 𝑦𝑛. Here, a malicious client intends to reveal
prototypical examples of a target class 𝑦𝑡, held by one or more
honest clients. During the attack, the malicious client exerts control
over a class 𝑦˜𝑡 that is used to actively poison the trained model and
improve the reconstruction of instances 𝑦𝑡.
To perform the inference attack, the malicious client trains a
local generative model 𝐺 to generate instances of the target class 𝑦𝑡.
During each iteration, the attacker samples images from 𝐺, as-