ρi
ci
ci
ρ
∑i c2/3
i
, and we have
ρi =
c2/3
i
∑ j c2/3
j
· ρ
(1)
That is, allocating privacy budget proportional to the 2
3 power
of the number of cells achieves the minimum overall noise
error.
A Greedy Algorithm to Select Pairs. We propose a greedy
algorithm to select pairs of attributes, as shown in Algorithm 1.
Given the InDif scores of all pairs of attributes (cid:104)φi(cid:105), size of all
marginals (cid:104)ci(cid:105), and the total privacy budget ρ, the goal is to
determine xi for each i ∈ {1, . . . ,m}, or equivalently, output
a set of pairs X = {i : xi = 1} that minimize the overall error.
We handle this problem by iteratively including marginals that
give the maximal utility improvement. In particular, in each
iteration t, we select one marginal that brings the maximum
improvement to the overall error. More speciﬁcally, we con-
sider each marginal i that is not yet included in X (i.e., i ∈ ¯X,
where ¯X = {1, . . . ,m}\ X): In Line 4, we allocate the optimal
privacy budget ρi according to Equation 1. We then calculate
the error in Line 5, and select one with maximum utility im-
provement (in Line 6). After the marginal is selected, we then
include it in X. The algorithm terminates when the overall
error no longer improves. The algorithm is guaranteed to ter-
minate since the noise error would gradually increase when
more marginals are selected. When the noise error is larger
than any of the remaining dependency error, the algorithm
terminates.
Combine Marginals.
Till now, we assume two-way
Algorithm 1: Marginal Selection Algorithm
Input: Number of pairs m, privacy budget ρ, dependency error (cid:104)φi(cid:105),
marginal size (cid:104)ci(cid:105);
Output: Selected marginal set X;
1 X ← ∅; t ← 0; E0 ← ∑i∈ ¯X φi;
2 while True do
foreach marginal i ∈ ¯X do
3
4
Allocate ρ to marginals j ∈ X ∪{i};
Et (i) = ∑ j∈X∪{i} c j
+ ∑ j∈ ¯X\{i} φ j;
(cid:113) 1
πρ j
5
6
7
8
9
10
11
(cid:96) ← argmini∈ ¯X Et (i);
Et ← Et ((cid:96));
if Et ≥ Et−1 then
Break
X ← X ∪{l};
t ← t + 1;
Algorithm 2: Marginal Combine Algorithm
Input: Selected pairwise marginals X, threshold γ
Output: Combined marginals X
1 Convert X to a set of pairs of attributes;
2 Construct graph G from the pairs;
3 S ← ∅; X ← ∅
4 foreach clique size s from m to 3 do
5
6
7
8
9
Cs ← cliques of size s in G
foreach clique c ∈ Cs do
Append c to X
Append the attributes of c to S
if |c∩ S| ≤ 2 and domain size of c ≤ γ then
marginals are used. When some marginals contain only a
small number of possibilities (e.g., when some attributes are
binary), extending to multi-way marginals can help capture
more information. In particular, given X, which contains in-
dices of the marginals selected from Algorithm 1, we ﬁrst
convert each index to its corresponding pair of attributes; we
then build a graph G where each node represents an attribute
and each edge corresponds to a pair. We then ﬁnd all the
cliques of size greater than 2 in the graph. If a clique is not
very big (smaller than a threshold γ = 5000), and does not
overlap much with existing selected attributes (with more
than 2 attributes in common), we merge the 2-way marginals
contained in the clique into a multi-way marginal.
Algorithm 2 gives the pseudocode of our proposed marginal
combining technique. We ﬁrst identify all possible cliques in
graph G and sort them in decending order by their attribute
size. Then, we examine each clique c to determine whether
to combine it. If the clique has a small domain size (smaller
than a threshold γ) and does not contain more than 2 attributes
that is already in the selected attributes set S, we include this
clique and remove all 2-way marginals within it.
4.3 Post Processing
The purpose of post processing is to ensure the noisy
marginals are consistent. By handling such inconsistencies,
we avoid impossible cases and ensure there exists a solution
934    30th USENIX Security Symposium
USENIX Association
(i.e., a synthetic dataset) that satisﬁes all the noisy marginals.
For the case when multiple marginals contain the same set
of attributes, and their estimations on the shared attributes
do not agree, we use the weighted average method [16, 44].
Note that [16, 44] both assume the privacy budget is evenly
distributed. We extend it to the uneven case.
Consistency under Uneven Privacy Budget Allocation.
When different marginals have some attributes in common,
those attributes are actually estimated multiple times. Utility
will increase if these estimates are utilized together. For ex-
ample, when some marginals are estimated twice, the mean
of the estimates is actually more accurate than each of them.
More formally, assume a set of attributes A is shared by s
marginals M1, M2, . . . , Ms, where A = M1 ∩ . . .∩ Ms. We can
obtain s estimates of A by summing from cells in each of the
marginals.
In [44], the authors proposed an optimal method to deter-
mine the distribution of the weights when privacy budget is
evenly distributed among marginals. The main idea is to take
the weighted average of estimates from all marginals in order
to minimize the variance of marginals on A. We adopt the
weighted average technique, and extend it to hand the case
where privacy budget is unevenly allocated. In particular, we
allocate a weight wi for each marginal i. The variance of the
weighted average can be represented by ∑i w2
, where ρi
is the privacy budget and gi is the number of cells that con-
tribute to one cell of the marginal on A. Here the Gaussian
variance is 1/ρi. By summing up gi cells, and multiplying the
result by wi, we have the overall variance w2
. The weights
i
should add up to 1. More formally, we have the following
optimization problem:
i · gi
ρi
gi
ρi
5 Synthetic Data Generation
Given a set of noisy marginals, the data synthesis step gen-
erates a new dataset Ds so that its distribution is consistent
with the noisy marginals. Existing methods [41, 53] put these
marginals into a graphical model, and use the sampling al-
gorithm to generate the synthetic dataset. As each record is
sampled using the marginals, the synthetic dataset distribution
is naturally consistent with the distribution.
The drawback of this approach is that when the graph is
dense, existing algorithms do not work. To overcome this
issue, we use an alternative approach. Instead of sampling the
dataset using the marginals, we initialize a random dataset and
update its records to make it consistent with the marginals.
5.1 Strawman Method: Min-Cost Flow (MCF)
Given the randomly initiated dataset Ds, for each noisy
marginal, we update Ds
to make it consistent with the
marginal. A marginal speciﬁed by a set of attributes is a fre-
quency distribution table for each possible combination of
values for the attributes. The update procedure can be mod-
eled as a graph ﬂow problem. In particular, given a marginal,
a bipartite graph is constructed. Its left side represents the
current distribution on Ds; and the right side is for the target
distribution speciﬁed by the marginal. Each node corresponds
to one cell in the marginal and is associated with a number.
Figure 2 demonstrates an example of this ﬂow graph. Now in
order to change Ds to make it consistent with the marginal,
we change records in Ds.
minimize∑
subject to∑
i
i
i · gi
w2
ρi
wi = 1
By constructing the Lagrangian function and following the
same derivative procedure as we did for obtaining optimal ρi
(Equation (1)), we have wi =
is the optimal strategy.
ρi/gi
∑i ρi/gi
Overall Consistency. In addition to the inconsistency among
marginals, some noisy marginals may contain invalid distri-
butions (i.e., some probability estimations are negative, and
the sum does not equal to 1). Given the invalid distribution,
it is known that projecting it to a valid one with minimal (cid:96)2
distance achieves the maximal likelihood estimation. This is
discovered in different settings (e.g., [10, 35, 51]); and there
exists efﬁcient algorithm for this projection.
The challenge emerges when we need to handle the two
inconsistencies simultaneously, one operation invalidate the
consistency established in another one. We iterate the two op-
erations multiple times to ensure both consistency constraints
are satisﬁed.
Figure 2: Running example of MCF. The left nodes repre-
sent current distribution from Ds; and the right nodes give
the target distribution speciﬁed by the noisy marginal. The
min-cost ﬂow is to move 0.1 from adult to teenager, and 0.1
from elderly to teenager. To change the distribution, we ﬁnd
matching records from Ds and change their corresponding
attributes.
The MCF method enforces a min-cost ﬂow in the graph
and updates Ds by changing the values of the records on the
ﬂow. For example, in Figure 2, there are two changes to Ds.
First, one third of the adults needs to be changed to teenagers.
USENIX Association
30th USENIX Security Symposium    935
CurrentDistTargetDist0.30.30.40.50.20.30.30.00.00.20.10.10.30.00.0Income
Gender
Age
v1
v2
v3
v4
v5
high
high
high
high
high
male
male
male
male
female
teenager
adult
adult
teenager
elderly
(a) Dataset before updating.
v
(cid:104)low, male,∗(cid:105)
(cid:104)low, female,∗(cid:105)
(cid:104)high, male,∗(cid:105)
(cid:104)high, female,∗(cid:105)
S{I,G}(v)
T{I,G}(v)
0.0
0.0
0.8
0.2
0.0
0.0
0.2
0.8
(b) Marginal table for {Income, Gender}, where
red and blue stands for over-counted and under-
counted cells, respectively.
v1
v2
v3
v4
v5
Income
Gender
Age
high
high
high
high
high
male
male
female
female
female
teenager
adult
elderly
teenager
elderly
(c) Dataset after updating.
Figure 3: Example of the synthesized dataset before and after updating procedure. In (a), blue stands for the records to be added,
and brown stands for the records to be changed. In (c), v4 only changes income and gender attributes, while v3 changes the whole
record which is duplicated from v5. Notice that in this example, we have α = 2.0,β = 0.5 and the marginal distribution in (c) do
not completely match T{I,G}(v) of [0.0,0.0,0.2,0.8]; instead, it becomes [0.0,0.0,0.4,0.6].
Note that we change only the related attribute and keep the
other attributes the same. Second, one fourth of the elderly are
changed to teenager. We iterate over all the noisy marginals
and repeat the process multiple times until the amount of
changes is small. The intuition of using min-cost ﬂow is that,
the update operations make the minimal changes to Ds, and
by changing the dataset in this minimal way, the consistency
already established in Ds (with previous marginals) can be
maintained. The min-cost ﬂow can be solved by the off-the-
shelf linear programming solver, e.g., [7].
When all marginals are examined, we randomly shufﬂe the
whole dataset Ds. Since the modifying procedure would in-
validate the consistency established from previous marginals,
MCF needs to iterate multiple times to ensure that Ds is al-
most consistent with all marginals.
5.2 Gradually Update Method (GUM)