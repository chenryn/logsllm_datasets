title:Friends, not foes: synthesizing existing transport strategies for
data center networks
author:Ali Munir and
Ghufran Baig and
Syed Mohammad Irteza and
Ihsan Ayyub Qazi and
Alex X. Liu and
Fahad R. Dogar
Friends, not Foes – Synthesizing Existing Transport
Strategies for Data Center Networks
Ali Munir1∗, Ghufran Baig2, Syed M. Irteza2, Ihsan A. Qazi2,
Alex X. Liu1, Fahad R. Dogar3
1Michigan State University, 2LUMS, 3Microsoft Research
{munirali, alexliu}@cse.msu.edu, {ghufran.baig, syed.irteza, ihsan.qazi}@lums.edu.pk,
PI:EMAIL
ABSTRACT
Many data center transports have been proposed in recent
times (e.g., DCTCP, PDQ, pFabric, etc). Contrary to the
common perception that they are competitors (i.e., protocol
A vs. protocol B), we claim that the underlying strate-
gies used in these protocols are, in fact, complementary.
Based on this insight, we design PASE, a transport frame-
work that synthesizes existing transport strategies, namely,
self-adjusting endpoints (used in TCP style protocols), in-
network prioritization (used in pFabric), and arbitration
(used in PDQ). PASE is deployment friendly:
it does not
require any changes to the network fabric; yet, its perfor-
mance is comparable to, or better than, the state-of-the-art
protocols that require changes to network elements (e.g.,
pFabric). We evaluate PASE using simulations and testbed
experiments. Our results show that PASE performs well for
a wide range of application workloads and network settings.
Categories and Subject Descriptors: C.2.2 [Computer-
Communication Networks]: Network Protocols
Keywords: datacenter; transport; scheduling
1.
INTRODUCTION
Popular data center applications (e.g., search) have many
distributed components that interact via the internal data
center network. Network delays, therefore, inﬂate applica-
tion response times which, in turn, aﬀects user satisfaction.
Thus, several recent data center transport proposals focus on
providing low latency for user-facing services. These propos-
als optimize for speciﬁc application goals, such as minimiz-
ing ﬂow completion times (FCT) or meeting ﬂow deadlines,
and use a variety of underlying techniques to achieve their
objective, from the more traditional TCP-style approaches
(e.g., DCTCP [11]) to those that use an explicit rate control
protocol (e.g., PDQ [18]) or a new prioritized network fabric
(e.g., pFabric [12]).
∗Part of the work was done while the author was an intern
at Microsoft Research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM 14, August 17 - 22 2014, Chicago, IL, USA
Copyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00
http://dx.doi.org/10.1145/2619239.2626305.
All these techniques have their own strengths and weak-
nesses: some provide good performance, but require changes
to the network fabric [12, 18, 24], while others are deploy-
ment friendly, but have inferior performance [22, 23]. In this
paper, we take a clean slate approach towards designing a
data center transport that provides good performance while
also being deployment friendly. To this end, we take a step
back and ask the following question: how can we design a
data center transport from scratch while leveraging the in-
sights oﬀered by existing proposals?
As a ﬁrst step towards answering this question, we distill
the three underlying strategies used in data center trans-
ports:
i) self adjusting endpoints, where senders indepen-
dently make their rate increase/decrease decision based on
the observed network conditions (used in DCTCP [11] style
protocols), ii) arbitration, where a common network entity
(e.g., a switch) allocates rates to each ﬂow (used in D3 [24],
PDQ [18], etc), and iii) in-network prioritization, where
switches schedule and drop packets based on their priority
(used in pFabric [12]).
We observe that each strategy has its own role. It works
best when it is fulﬁlling its role and encounters problems
otherwise. Unfortunately, existing protocols only use one of
the above strategies and try to make it work under all sce-
narios. In this paper, we make a case that these transport
strategies should be used together as they nicely complement
each other. For example, approaches that rely on arbitra-
tion alone have high ﬂow switching overhead because ﬂows
need to be explicitly paused and unpaused [18]. With in-
network prioritization, switching from a high priority ﬂow
to the next is seamless [12]. Conversely, arbitration can also
help in-network prioritization approaches. For example, in-
network prioritization mechanisms typically need to support
a large number of priority levels whereas existing switches
only have a limited number of priority queues. An arbitra-
tor can address this problem by dynamically changing the
mapping of ﬂows to queues — ﬂows whose turn is far away
can all be mapped to the lowest priority queue while ﬂows
whose turn is about to come can be mapped to the high
priority queues.
To demonstrate the beneﬁts of using these strategies to-
gether, we design PASE, a transport framework for private
data center environments. PASE employs distributed arbi-
trators that decide the share (priority) of a ﬂow given other
ﬂows in the system. A TCP-like end-host transport uses this
information for its rate control and loss recovery. Within the
network, PASE uses existing priority queues to prioritize the
scheduling of packets over network links. This appropriate
division of responsibilities among the three strategies makes
PASE outperform state-of-the-art transport protocols while
also being deployment friendly i.e., no changes to the net-
work fabric are required.
A key aspect of PASE is a scalable control plane for arbi-
tration. For every link in the data center topology, a dedi-
cated arbitrator is responsible for arbitration. This function-
ality can be implemented at the end-hosts themselves (e.g.,
for their own links to the switch) or on dedicated nodes
within the data center. We exploit the typical tree based
data center topology features to make the arbitration deci-
sions in a bottom up fashion, starting from the endpoints
and going up to the core. This has several performance
and scalability beneﬁts. First, for intra-rack communica-
tion, which can constitute a sizeable share of data center
traﬃc [11], only the source and destination are involved, ob-
viating the need to communicate with any other entity. Sec-
ond, lower level arbitrators (those closer to the leaf nodes)
can do early pruning by discarding ﬂows that are unlikely
to become part of the top priority queue. Third, higher
level arbitrators (those closer to the root) can delegate their
arbitration decisions to lower level arbitrators. Both early
pruning and delegation reduce the arbitration overhead (at
the cost of potentially less accurate decisions).
The outcome of arbitration is the priority queue and
reference rate for the ﬂow – this information is used by
PASE’s end-host transport for rate control and loss re-
covery. Compared to traditional transport protocols (e.g.,
TCP/DCTCP), our rate control is more guided. For ex-
ample, instead of slow start, the transport uses the refer-
ence rate as its starting point. However, loss recovery in
PASE is more challenging because packets can be delayed
in a lower priority queue for a long time, which may trig-
ger spurious timeouts if we use today’s timeout mechanisms.
Thus, for lower priority ﬂows, instead of retransmitting the
data packet, PASE uses small probe packets which help in
determining whether the packet was lost, or waiting in a
lower priority queue.
We implement PASE on a small testbed and in the ns2
simulator [6]. We compare PASE with the best perform-
ing transport protocol (pFabric [12]) as well as deployment
friendly options (D2TCP [23]). Compared to deployment
friendly options, PASE improves the average FCT (AFCT)
by 40% to 60% for various scenarios. PASE also performs
within 6% of pFabric in scenarios where pFabric is close to
optimal while in other scenarios (all-to-all traﬃc pattern or
under high network load), PASE outperforms pFabric by
up to 85% both in terms of the AFCT as well as the 99th
percentile FCT.
This paper makes the following key contributions.
• We distill the underlying strategies used in data center
transport protocols and highlight their strengths and
weaknesses.
• We design PASE, a data center transport framework
which synthesizes existing transport strategies. PASE
includes two new components: a scalable arbitration
control plane for data center networks, and an end-host
transport protocol that is explicitly aware of priority
queues and employs a guided rate control mechanism.
• We conduct a comprehensive evaluation of PASE,
compare
which includes macro-benchmarks
that
PASE’s performance against multiple existing trans-
port protocols, and micro-benchmarks that focus on
the internal working of the system.
PASE shows the promise of combining existing transport
strategies in a single transport framework. We view it as a
ﬁrst step towards a more holistic approach for building the
next generation data center transport protocols.
2. TRANSPORT STRATEGIES
To achieve high performance,1 existing data center trans-
ports use one of the three following transport strategies: (a)
Self-Adjusting Endpoints, (b) Arbitration, or (c) In-network
Prioritization. We ﬁrst describe these strategies and discuss
their limitations when they are employed in isolation. We
then discuss how these limitations can be addressed if these
strategies are used together.
2.1 Transport Strategies in Isolation
Each transport strategy has its own advantages and dis-
advantages as shown in Table 1. We now describe the basic
working of each strategy, discuss its advantages, and high-
light their key limitations through simulation experiments.
Self-Adjusting Endpoints: Traditional transport proto-
cols like TCP use this strategy. Under this transport strat-
egy, endpoints themselves decide the amount of data to send
based on network congestion. The state of network conges-
tion is determined through a congestion signal that could be
implicit (e.g., packet loss) or explicit (i.e., ECN). In case of
congestion, the window size is reduced by the same factor for
all ﬂows, if fairness is the objective [11], or the factor could
depend on other parameters (e.g., remaining ﬂow size [22]
or deadline [23]), if ﬂow prioritization is the objective.
Protocols in this category are easy to deploy because they
do not require any changes to the network infrastructure or
depend on any entity except the endpoints. However, when
considering ﬂow prioritization, their performance is inferior
to the state-of-the-art data center transport protocols (e.g.,
pFabric [12], PDQ [18]). One reason for their poor perfor-
mance is that they do not provide strict priority scheduling –
even low priority ﬂows, which should be paused, continue to
send at least one packet per RTT. This hurts performance,
especially at high loads when multiple ﬂows are active.
To illustrate this behavior, we consider two protocols that
follow the self-adjusting endpoint strategy: DCTCP [11] and
D2TCP [23] (a deadline-aware version of DCTCP), and com-
pare their performance with pFabric [12], the state-of-the-art
data center transport with the best reported performance.
We replicate a deadline oriented scenario in ns22. Figure 1
shows the fraction of deadlines met (or application through-
put) as a function of load for the three schemes. While at
low loads, D2TCP is able to meet deadlines (i.e., achieve
prioritization), at higher loads its performance approaches
its fair-sharing counterpart, DCTCP. Moreover, both these
1In the context of data center transports, high performance
usually refers to minimizing completion times, maximizing
throughput, or reducing the deadline miss rate [11, 18].
2This corresponds to Experiment 4.1.3 in the D2TCP pa-
per [23] – it represents an intra-rack scenario, where the
source and destination of each ﬂow is picked randomly and
the ﬂow sizes are uniformly distributed between [100 KB,
500 KB] in the presence of two background long ﬂows. The
deadlines are uniformly distributed from 5 ms-25 ms.
Transport Strategy
Self-Adjusting Endpoints Ease of deployment
Pros
Arbitration
In-network Prioritization
1) Supports strict priority scheduling
2) Fast convergence
1) Work conservation
2) Low ﬂow switching overhead
Cons
Lack of support for strict priority
scheduling
1) High ﬂow switching overhead
2) Hard to compute precise rates
1) Limited number of priority
queues in existing switches
2) Switch-local decisions
Examples
DCTCP [11], D2TCP
[23], L2DCT [22]
D3 [24], PDQ [18].
pFabric [12]
Table 1: Comparison of diﬀerent transport strategies.
Figure 1: Comparison of two self-adjusting end-
point based protocols (D2TCP and DCTCP) with
the state-of-the-art data center transport (pFabric).
Figure 2: Comparison of PDQ (an arbitration based
approach) with DCTCP. At high loads, PDQ’s high
ﬂow switching overhead leads to poor performance.
protocols perform much worse than pFabric at high loads,
highlighting their limitations in achieving priority schedul-