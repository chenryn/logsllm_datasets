synchronous requests. To take advantage of the parallel na-
ture of PD-ORAM, privatefs is instead built on FUSE.
A second attempt used ext2fuse, a FUSE-based ext2 im-
986shuffle progress metered vs. time
privatefs throughput vs. parallelism
Reported
Enforced minimum
Actual
)
s
/
B
K
(
t
u
p
h
g
u
o
r
h
T
 140
 120
 100
 80
 60
 40
 20
 0
0 latency, read
0 latency, write
40 latency, read
40 latency, write
80 latency, read
80 latency, write
160 latency, read
160 latency, write
 0
 10
 20
 30
 40
 50
 60
 70
 80
Number of ORAM clients
)
s
/
B
K
(
d
e
e
p
s
e
t
i
r
W
 50
 45
 40
 35
 30
 25
 20
 15
Write performance vs. amount of FS written data
write speed, 0 latency, 10 clients
 0
 50
 100
 150
 200
 250
 300
Written data (MB)
s
s
e
r
g
o
r
p
e
l
f
f
u
h
S
 1
 0.8
 0.6
 0.4
 0.2
 0
 0  50 100 150 200 250 300 350 400 450
Time (sec)
Figure 9: Level shuﬄe progress
over time: The linear nature of
the plot indicates the extrapola-
tion based on partial shuﬄing is
well done.
Figure 10: r/w performance vs. the
number of ORAM Clients, for vari-
ous network latencies. The zero la-
tency environment is mostly unaf-
fected by the degree of parallelism.
For higher latencies,
throughput
grows with the number of parallel
clients that oﬀset the latency.
Figure 11:
IOzone write perfor-
mance vs.
increasing ﬁle system
written data. Write throughput is
slowly decreasing, as expected due
to the inherent slowdown in the (in-
creasingly larger) ORAM.
plementation , by rerouting block access through PD-ORAM.
However, thread safety diﬃculties prevented us from mod-
ifying it to support parallel writes or reads. Additionally,
because of its nature as a block device ﬁle system, ext2fuse
requires mechanisms for allocating blocks for ﬁles, such as
block groups, free block bitmaps and indirect ﬁle block point-
ers inside inodes. These mechanisms are not all thread-safe
and pose a challenge to synchronize. Moreover, locking the
code using synchronization primitives would not result in a
suﬃcient degree of parallelization.
Instead, we implemented our own privatefs using the C++
FUSE libraries.
It fully leverages PD-ORAM parallelism
and also takes advantage of the non-contiguous block label-
ing in a way that block-device ﬁle systems cannot.
Following the Linux ﬁle system model, in privatefs ﬁles are
represented by inodes. Directories are inodes containing a
list of directory entries; each directory entry is the name of a
ﬁle or subdirectory along with its inode number. Inodes are
numbered using 256-bit values and are mapped directly to
ORAM blocks, such that inode x is stored in ORAM block
x. Inodes hold metadata such as type, size and permissions.
Both privatefs and (this instance of) PD-ORAM use 256-bit
block identiﬁers and 4096-byte blocks.
Because the ORAM provides random access to 256-bit
addressable blocks, a block can be allocated simply by gen-
erating a random 256-bit number. We take advantage of this
in two ways. First, to read or write the ith block of ﬁle with
inode number x, the pair (x, i) is hashed with the collision-
resistant SHA256 hash, yielding the 256-bit ORAM block
ID for that ﬁle block. Second, when a new ﬁle is created, a
256-bit inode number is randomly generated, as opposed to
maintaining and synchronizing access to an inode counter.
Our design eliminates the complexity of contiguous block
device ﬁle systems and minimizes the need for locking when
writing or reading ﬁles. As opposed to ext2fuse, privatefs
does not incur the overhead of maintaining free block or in-
ode bitmaps, grouping blocks into block groups, or travers-
ing indirect block pointers to read ﬁles. The potential draw-
back is that sequential blocks of a given ﬁle will not be stored
contiguously in the ﬁle system. However, this is harmless
when using an ORAM, since there is no notion of sequential
block numbers (which would compromise access privacy).
privatefs employs exclusive locks when reading and writ-
ing directories. In addition, an LRU cache is implemented to
quickly retrieve an inode’s data given its inode number and
also for ﬁle path to inode number translation, which helps
avoid long directory traversals (and associated locking). pri-
vatefs communicates with the ORAM server by means of a
proxy (written in Java), which receives block requests from
the ﬁle system and satisﬁes them using parallel connections
to the ORAM server. This design choice aﬀords us a higher
degree of modularity, enabling us to connect privatefs to
other ORAM schemes in the future.
We benchmarked privatefs along with our previous ﬁle sys-
tem attempts, using a parallel workload writing ten, 512KB
ﬁles simultaneously to the ﬁle system, then checking their in-
tegrity (also done simultaneously) using the sha256sum util-
ity. For the single-client NBD implementation, throughputs
ranged between 10.45KB/s and 14.80KB/s. For ext2fuse,
throughputs ranged between 7.11KB/s and 9.62KB/s.
In
contrast, the performance results for the full implementation
of privatefs discussed below indicate a major improvement
of an order of magnitude.
Our benchmarks (Figure 10) indicate that privatefs is ben-
eﬁting from the high degree of parallelism in high-latency
environments. We ran ﬁve trials for each point, varying the
number of ORAM clients used by the proxy to satisfy ﬁle
system block requests and performance increased propor-
tionally with the degree of parallelism. The average over
the ﬁve trials is plotted.
Reads are less expensive than writes in the 40ms and
160ms trials, because the individual writes are performed
synchronously, while the reads can be parallel, resulting in a
higher overall degree of parallelism. On the other hand, the
low latency in the 0ms trial prevents this parallelism from
having an impact. The reads are, in turn, more expensive,
due to ineﬃciencies in the de-amortization method resulting
in slightly slower queries at later points in the process.
We also analyzed the behavior of increasing ﬁle system size
(Figure 11) and used the widely used IOzone benchmark to
test the write throughput as we wrote more data. Starting
with an empty ﬁle-system we repeatedly ran a parallel write
throughput test using IOzone which writes ten 1MB ﬁles
and then rewrites them.
We compared privatefs to NFSv3 and ext4 using the IO-
zone workload. The privatefs tests wrote ten, 1MB ﬁles
concurrently, while the NFS and ext4 tests wrote ten 1GB
ﬁles concurrently. We decided to use larger ﬁle sizes when
performing the tests for NFS and ext4 in order to min-
imize the impact of caching. The expected signiﬁcantly
987higher throughputs hovered around 57MB/s for NFSv3 and
138MB/s for ext4.
Thus, in this simple setup, privatefs features a modest
throughput when compared to unsecured ﬁle systems. This
is the inherent cost of achieving privacy. However, a few
notes are in order to outline how performance can be scaled
with increasing resources thrown at the problem.
SSDs. Deploying (multiple, Sections 2.3, 6.3) zero-latency
media server-side would signiﬁcantly impact performance.
This is straightforward and relatively uninteresting research-
wise. For example, deploying SSDs would shift bottlenecks
and immediately increase throughput by an order of magni-
tude or more.
Block Sizes. privatefs has been benchmarked with 4KB
blocks. One can straightforwardly increase their size by con-
sidering larger blocks and obtain an (artiﬁcally inﬂated)
“higher throughput” up to the maximum available band-
width. E.g., going from 4KB to 64KB blocks could in-
crease “throughput” by another order of magnitude or more
at the expense of wasted bandwidth. Similarly, we felt this
is not interesting research-wise and should be decided on an
application-speciﬁc basis.
Compute Power. Finally, to eliminate bottlenecks, large
amounts of server-side resources can be thrown at the prob-
lem to speed up things even further. We posit that this is
also not interesting – what ultimately counts is the usable
bang for the buck achieved, i.e., in this case the through-
put (at some ﬁxed block-size) achieved per compute cycle
spent server-side. Otherwise, results become meaningless –
e.g., if instead of the test setup above we were to deploy a
large number of compute cores, performance would increase
almost linearly up to network saturation. With careful ﬁne-
tuning throughputs of hundreds of Mbps can be achieved
without any changes in the base protocol. However, from a
security point of view or research-wise in general this is not
interesting but should be pursued in industrial R&D.
8. CONCLUSION
This paper includes mechanisms for secure parallel query-
ing of existing ORAMs to increase throughput, a general-
ization of ORAM de-amortization, and implementation of
an eﬃcient ORAM based on these techniques, performing a
transaction per second on a terabyte database in an average-
latency network (a ﬁrst). An implementation of privatefs,
the ﬁrst oblivious networked ﬁle system, is provided.
9. ACKNOWLEDGMENTS
Supported in part by NSF under awards 1161541, 0937833,
0845192, 0803197, 0708025. We would also like to thank the
reviewers and our shepherd, Alina Oprea.
10. REFERENCES
[1] Boneh, D., Mazi´eres, D., and Popa, R. A. Remote
oblivious storage: Making Oblivious RAM practical.
Tech. rep., MIT, 2011. MIT-CSAIL-TR-2011-018
March 30, 2011.
[2] Goldreich, O., and Ostrovsky, R. Software
protection and simulation on Oblivious RAMs.
Journal of the ACM 45 (May 1996), 431–473.
[3] Goodrich, M., and Mitzenmacher, M.
MapReduce Parallel Cuckoo Hashing and Oblivious
RAM Simulations. In ICALP (2011).
[4] Goodrich, M., Mitzenmacher, M., Ohrimenko,
O., and Tamassia, R. Oblivious RAM Simulation
with Eﬃcient Worst-Case Access Overhead. In ACM
Cloud Computing Security Workshop (CCSW) (2011).
[5] Goodrich, M. T. Randomized shellsort: A simple
oblivious sorting algorithm. In SODA (2010).
[6] Goodrich, M. T., Mitzenmacher, M.,
Ohrimenko, O., and Tamassia, R. Oblivious storage
with low I/O overhead. CoRR abs/1110.1851 (2011).
[7] Goodrich, M. T., Mitzenmacher, M.,
Ohrimenko, O., and Tamassia, R.
Privacy-preserving group data access via stateless
oblivious RAM simulation. In SODA (2012).
[8] Kushilevitz, E., Lu, S., and Ostrovsky, R. On
the (in)security of hash-based oblivious RAM and a
new balancing scheme. In SODA (2012), Y. Rabani,
Ed., SIAM, pp. 143–156.
[9] Li, J., Krohn, M., Mazi`eres, D., and Shasha, D.
Secure untrusted data repository (SUNDR). In
OSDI’04: Proceedings of the 6th conference on
Symposium on Opearting Systems Design &
Implementation (Berkeley, CA, USA, 2004), USENIX
Association, pp. 9–9.
[10] Olumofin, F., and Goldberg, I. Revisiting the
computational practicality of private information
retrieval. In In Financial Cryptography and Data
Security ’11 (2011).
[11] Ostrovsky, R., and Shoup, V. Private information
storage (extend abstract). In IN PROCEEDINGS OF
STOC (1997), ACM Press, pp. 294–303.
[12] Pagh, R., and Rodler, F. F. Cuckoo hashing. J.
Algorithms 51 (May 2004), 122–144.
[13] Pinkas, B., and Reinman, T. Oblivious RAM
revisited. In CRYPTO (2010), pp. 502–519.
[14] Shi, E., Chan, T.-H. H., Stefanov, E., and Li, M.
Oblivious RAM with o((logn)3) worst-case cost. In
ASIACRYPT (2011), pp. 197–214.
[15] Sion, R., and Carbunar, B. On the computational
practicality of private information retrieval. In
Proceedings of the Network and Distributed Systems
Security (NDSS) Symposium (2007).
[16] Stefanov, E., Shi, E., and Song, D. Towards
Practical Oblivious RAM. In Proceedings of the
Network and Distributed System Security (NDSS)
Symposium (2012).
[17] Wang, S., Ding, X., Deng, R. H., and Bao, F.
Private information retrieval using trusted hardware.
In Proceedings of the European Symposium on
Research in Computer Security ESORICS (2006),
pp. 49–64.
[18] Williams, P., and Sion, R. Usable PIR. In
Proceedings of the 2008 Network and Distributed
System Security (NDSS) Symposium (2008).
[19] Williams, P., Sion, R., and Carbunar, B.
Building castles out of mud: practical access pattern
privacy and correctness on untrusted storage. In ACM
Conference on Computer and Communications
Security (2008), pp. 139–148.
[20] Williams, P., Sion, R., and Sotakova, M.
Practical oblivious outsourced storage. ACM Trans.
Inf. Syst. Secur. 14 (September 2011), 20:1–20:28.
988