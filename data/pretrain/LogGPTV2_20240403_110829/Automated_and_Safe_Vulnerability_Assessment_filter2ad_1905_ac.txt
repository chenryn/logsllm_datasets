### Replacing the Constructor Area with a Secure Library Loader

Replacing the constructor area of the binary with a secure library loader (secure_libs_loader) ensures that the Global Offset Table (GOT) can be modified before the `main()` function calls any weak functions. The loader's primary role is to load `libsecu.so`, a safe library, into the same memory space as the binary. As illustrated in Figure 3, the loader first locates the address of the `dlopen()` function and then loads `libsecu.so` into memory. Once `libsecu.so` is loaded, the loader invokes a function that performs a PLT/GOT patch within the library.

**Figure 3.** Operation Process of the Secure Library Loader.

All these processes occur before the `main()` function is executed. Consequently, the patch process accesses the GOT via the Procedure Linkage Table (PLT) of the vulnerable function, which has not yet been bound to an actual address. Since the vulnerable function has never been called, the patch overwrites the GOT entry of the vulnerable function with the address of the safe function (typically at the PLT + 6 location).

This technique aims to minimize binary modifications and indirectly replace vulnerable functions with safe ones. Modifying the constructor to load the library is necessary, but it avoids side effects such as disrupting the original program by not altering other code parts. Additionally, since the loaded library is a shared library, it can be maintained and updated independently. This approach is particularly suitable for IoT devices like smartphones and smart TVs based on Linux, offering flexibility and scalability.

### 4. Experimental Results

#### 4.1. CFG Recovery Analysis

Control Flow Graph (CFG) recovery analysis is frequently used for static vulnerability analysis. Among the various CFG recovery techniques, backward slicing is particularly useful for root cause analysis because it extracts the specific path information in which the binary is executed. We plan to use this method to analyze the root causes of vulnerabilities. For benchmarking, we compared the CFG recovery speed of several tools, targeting 131 CGC challenge binaries [26]. The tests were conducted on a server with a 3.60 GHz Intel Core i7-4960X CPU and 4 GB of RAM. The results, shown in Table 6, indicate that the backward slicing tool was the slowest due to high resource consumption, while BAP was the fastest with an average speed of 1.63 seconds.

**Table 6.** CFG Recovery Speed Comparison.

| Tool                | CFG Size (kb) | Min. Binary (s) | Max. Binary (s) | Average Speed (s) |
|---------------------|---------------|-----------------|-----------------|-------------------|
| Angr (Backward Slicing) | 14,641        | 10.39           | 93.74           | 79.46             |
| Angr (CFG Fast)      | 105,007       | 0.87            | 12.037          | 5.12              |
| IDA                 | 104,779       | 0.18            | 2.33            | 1.82              |
| BARF                | 7,367,244     | 1.60            | 192.23          | 63.08             |
| BAP                 | 323,891       | 0.56            | 36.50           | 1.63              |

### 4.2. Binary Patch Result

We used the Peach Fuzzer to trigger crashes in open-source software and compared the number of crashes before and after applying the patches to evaluate the effectiveness of our automatic patch method. We selected eight open-source projects based on the number of published vulnerability reports. The evaluation was performed on a server with a 2.30 GHz Intel Xeon E5-2650v3 CPU and 64 GB of RAM. The vulnerable functions listed in Table 7 were replaced with safe functions through the secure library loading. Let \( B \) be the number of crashes before the patch and \( A \) be the number of crashes after the patch. The crash removal rate \( R \) was calculated using the following equation:

\[ R(\%) = \frac{B - A}{B} \times 100 \]

The results showed an average vulnerability removal rate of 59%, with 25% of the binaries having all their vulnerabilities completely removed by the automatic patch.

**Table 7.** Comparison of Crashes Before and After Patching.

| Test Cases    | Crashes (Before Patch) | Crashes (After Patch) | Removal Rate (%) |
|---------------|------------------------|-----------------------|------------------|
| Lighthttpd    | 300                    | 13                    | 95.67            |
| Libhttpd     | 420                    | 39                    | 90.71            |
| Abyss        | 300                    | 18                    | 94               |
| Wsmp3d (low) | 400                    | 223                   | 44.25            |
| Shttpd       | 500                    | 18                    | 96.4             |
| Pserv        | 300                    | 0                     | 100              |
| Wsmp3d (high)| 400                    | 119                   | 70.25            |
| Kritton      | 300                    | 38                    | 87.33            |

### 5. Conclusions

The rapid increase in vulnerabilities due to new hacking techniques makes it challenging to respond to attacks promptly, especially with time-consuming manual analysis. We proposed a hybrid fuzzing method based on binary complexity analysis and an automated patch technique that modifies the PLT/GOT table to replace vulnerable functions with safe ones. Our model removed an average of 59% of crashes in eight open-source binaries, with two binaries achieving a 100% removal rate. A 100% removal rate means the binaries are no longer exploitable, significantly reducing the risk of hacker attacks. This allows for quicker responses to security threats without relying on expert intervention.

Future work will focus on automatic exploit generation to verify patched binaries, root cause analysis for direct patching of vulnerable parts, and the automatic classification of vulnerabilities using data mining and machine learning techniques [27,28,29,30].

### Author Contributions

- **J.J.** designed the system and performed the experiments.
- **T.K.** contributed to the study design.
- **H.K.** helped revise the paper.

### Acknowledgments

This work was supported by the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No. 2017-0-00184, Self-Learning Cyber Immune Technology Development).

### Conflicts of Interest

The authors declare no conflict of interest.

### References

1. U.S. National Vulnerability Database. Available online: <http://cve.mitre.org/cve/> (accessed on 4 April 2018).
2. Kang, W.M.; Moon, S.Y.; Park, J.H. An enhanced security framework for home appliances in smart homes. *Hum.-Centric Comput. Inf. Sci.* 2017, 7, 6. [CrossRef]
3. Keegan, N.; Ji, S.Y.; Chaudhary, A.; Concolato, C.; Yu, B.; Jeong, D.H. A survey of cloud-based network intrusion detection analysis. *Hum.-Centric Comput. Inf. Sci.* 2016, 6, 19. [CrossRef]
4. Miller, B.P.; Fredriksen, L.; So, B. An empirical study of the reliability of UNIX utilities. *Commun. ACM* 1990, 33, 32–44. [CrossRef]
5. Zzuf—Caca Labs. Available online: <http://caca.zoy.org/wiki/zzuf> (accessed on 4 April 2018).
6. Peach Fuzzer. Available online: <https://www.peach.tech/> (accessed on 4 April 2018).
7. Sulley. Available online: <https://github.com/OpenRCE/sulley> (accessed on 4 April 2018).
8. Aitel, D. An introduction to SPIKE, the fuzzer creation kit. In Proceedings of the BlackHat USA Conference, Las Vegas, NV, USA, 29 July–1 August 2002.
9. Bekrar, S.; Bekrar, C.; Groz, R.; Mounier, L. A taint-based approach for smart fuzzing. In Proceedings of the IEEE Fifth International Conference on Software Testing, Verification and Validation, Montreal, QC, Canada, 17–21 April 2012; pp. 818–825.
10. American Fuzzy Lop. Available online: <http://lcamtuf.coredump.cx/afl/> (accessed on 4 April 2018).
11. Honggfuzz. Available online: <https://github.com/google/honggfuzz> (accessed on 4 April 2018).
12. King, J.C. Symbolic execution and program testing. *Commun. ACM* 1976, 19, 385–394. [CrossRef]
13. Godefroid, P.; Levin, M.Y.; Molnar, D.A. Automated whitebox fuzz testing. NDSS 2008, 8, 151–166.
14. Cadar, C.; Dunbar, D.; Engler, D.R. KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs. OSDI 2008, 8, 209–224.
15. Ciortea, L.; Zamfir, C.; Bucur, S.; Chipounov, V.; Candea, G. Cloud9: A software testing service. *ACM SIGOPS Oper. Syst. Rev.* 2010, 43, 5–10. [CrossRef]
16. Cha, S.K.; Avgerinos, T.; Rebert, A.; Brumley, D. Unleashing mayhem on binary code. In Proceedings of the IEEE Symposium on Security and Privacy, San Francisco, CA, USA, 20–23 May 2012; pp. 380–394.
17. Stephens, N.; Grosen, J.; Salls, C.; Dutcher, A.; Wang, R.; Corbetta, J.; Shoshitaishvili, Y.; Kruegel, C.; Vigna, G. Driller: Augmenting Fuzzing through Selective Symbolic Execution. NDSS 2016, 16, 1–16.
18. Shoshitaishvili, Y.; Wang, R.; Salls, C.; Stephens, N.; Polino, M.; Dutcher, A.; Grosen, J.; Feng, S.; Hauser, C.; Kruegel, C. Sok: (State of) the art of war: Offensive techniques in binary analysis. In Proceedings of the IEEE Symposium on Security and Privacy, San Jose, CA, USA, 22–26 May 2016; pp. 138–157.
19. Chipounov, V.; Kuznetsov, V.; Candea, G. S2E: A Platform for In-Vivo Multi-Path Analysis of Software Systems. In Proceedings of the Architectural Support for Programming Languages and Operating Systems, Newport Beach, CA, USA, 5–11 March 2011; pp. 265–278.
20. Stephanie, F.; Thanh, V.N.; Westley, W.; Claire, L.G. A Genetic Programming Approach to Automated Software Repair. In Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation, Montreal, QC, Canada, 8–12 July 2009; pp. 947–954.
21. Liu, C.; Yang, J.; Tan, L.; Hafiz, M. R2Fix: Automatically generating bug fixes from bug reports. In Proceedings of the International Conference on Software Testing, Verification and Validation, Luxembourg, 18–22 March 2013; pp. 282–291.
22. Kim, D.; Nam, J.; Song, J.; Kim, S. Automatic patch generation learned from human-written patches. In Proceedings of the International Conference on Software Engineering, San Francisco, CA, USA, 18–26 May 2013; pp. 802–811.
23. QEMU. Available online: <https://www.qemu.org/> (accessed on 4 April 2018).
24. Halstead, M.H. Elements of Software Science; Elsevier North-Holland: New York, NY, USA, 1977; p. 128.
25. DARPA Cyber Grand Challenge. Available online: <http://archive.darpa.mil/cybergrandchallenge/> (accessed on 4 April 2018).
26. Shudrak, M.O.; Zolotarev, V.V. Improving fuzzing using software complexity metrics. In Proceedings of the International Conference on Information Security and Cryptology, Seoul, Korea, 25–27 November 2015; pp. 246–261.
27. Mohamed, B.; Smaine, M. A Chi-Square-Based Decision for Real-Time Malware Detection Using PE-File Features. *J. Inf. Process. Syst.* 2016, 12, 644–660.
28. Choi, J.H.; Shin, H.S.; Nasridinov, A. A Comparative Study on Data Mining Classification Techniques for Military Applications. *J. Converg.* 2016, 7, 1–7.
29. Yamaguchi, F.; Lindner, F.; Rieck, K. Vulnerability extrapolation: Assisted discovery of vulnerabilities using machine learning. In Proceedings of the 5th USENIX conference on Offensive Technologies, San Francisco, CA, USA, 8 August 2011.
30. Gustavo, G.; Guilermo, L.G.; Lucas, U.; Sanjay, R.; Josselin, F.; Laurent, M. Toward Large-Scale Vulnerability Discovery using Machine Learning. In Proceedings of the Sixth ACM conference on Data and Application Security and Privacy, New Orleans, LA, USA, 9–11 March 2016; pp. 85–96.

© 2018 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<http://creativecommons.org/licenses/by/4.0/>).