### Visit to CNN and Cookie-Based Trackers

Cookie-based trackers re-identify users by setting unique identifiers in browser cookies, which are then automatically included with requests to the tracker’s domain. Figure 1 illustrates a basic example of this process. We will discuss more complex cookie-based tracking behaviors in Section 3.

#### Overview of Basic Cookie-Based Web Tracking

As shown in Figure 1, a third-party domain (e.g., `tracker.com`) uses a browser cookie to re-identify users on sites that embed content from `tracker.com`. This example demonstrates vanilla tracking according to the taxonomy from [60]. More complex behaviors are described in Section 3.

Although cookie-based tracking is extremely common [60], other types of tracking behaviors have also emerged, including the use of other client-side storage mechanisms such as HTML5 LocalStorage, or the use of browser and/or machine fingerprinting to re-identify users without the need to store local state [15, 57].

### Privacy Concerns and Research Efforts

Because these embedded trackers are often invisible to users and not visited intentionally, there has been growing concern about the privacy implications of third-party tracking. In recent years, this issue has been the subject of repeated policy discussions (Mayer and Mitchell provide an overview as of 2012 [50]). Simultaneously, the computer science research community has studied tracking mechanisms (e.g., [50, 57, 60, 71]), measured their prevalence (e.g., [3, 20, 42, 60]), and developed new defenses or privacy-preserving alternatives (e.g., [6, 22, 25, 61, 64]). We discuss related works further in Section 6.

### Historical Context and Longitudinal Studies

The research community's interest in web tracking is relatively recent, with the earliest measurements (to our knowledge) beginning in 2005 [42]. Each study has used a different methodology and measured a different subset of known tracking techniques (see Englehardt et al. [18] for a comprehensive list of such studies). The practices of embedding third-party content and targeted advertising on websites predate these first studies [48], and longitudinal studies have been limited. However, longitudinal studies are critical to ensure the sustained effects of transparency [63] and to contextualize future measurements.

### Research Questions

To help ground technical and policy discussions surrounding web tracking in historical trends, we ask: How has the third-party tracking ecosystem evolved over the lifetime of the web? Specifically, we investigate:

- How have the numbers, identities, and behaviors of dominant trackers changed over time?
- How has the scope of the most popular trackers (i.e., the number of websites on which they are embedded) changed over time?
- How has the prevalence of tracking changed over time? For example, do websites include many more third-party trackers now than they did in the past?
- How have the behaviors of web trackers (e.g., JavaScript APIs used) changed over time?

By answering these questions, we aim to provide a systematic and longitudinal view of third-party web tracking over the last 20 years, retroactively filling this gap in the research literature, shedding light on the evolution of third-party tracking practices on the web, and informing future technical and policy discussions.

### Data Source: The Wayback Machine

To conduct our archaeological study, we rely on data from the Internet Archive’s Wayback Machine (https://archive.org). Since 1996, the Wayback Machine has archived full web pages, including JavaScript, stylesheets, and any resources (including third-party JavaScript) that it can identify statically from the site contents. It mirrors past snapshots of these web pages on its own servers; visitors to the archive see the pages as they appeared in the past, make requests for all resources from the Wayback Machine’s archived copy, and execute all JavaScript that was archived. We evaluate the completeness of the archive, particularly with respect to third-party requests, in Section 4.

### Measurement Infrastructure: Tracking Excavator

To conduct a longitudinal study of web tracking using historical data from the Wayback Machine, we built a tool called Tracking Excavator. This tool has the capability to (1) detect and analyze third-party tracking-related behaviors on a given web page, and (2) run that analysis over historical web pages archived and accessed by the Wayback Machine. In this section, we introduce Tracking Excavator. Figure 2 provides an overview of Tracking Excavator, which is organized into four pipeline stages:

1. **Input Generation (Section 3.1):** Tracking Excavator takes as input a list of top-level sites on which to measure tracking behaviors (such as the Alexa top 500 sites), and, in “Wayback mode,” a timestamp for the desired archive time to create archive.org URLs.
2. **Data Collection (Section 3.2):** Tracking Excavator includes a Chrome browser extension that automatically visits the pages from the input set and collects tracking-relevant data, such as third-party requests, cookies, and the use of certain JavaScript APIs.
3. **Data Analysis (Section 3.3):** Tracking Excavator processes collected measurement events to detect and categorize third-party web tracking behaviors.
4. **Data Visualization:** Finally, we process our results into visual representations (included in Section 5).

### Input Generation

In the input generation phase, we provide Tracking Excavator with a list of top-level sites to use for measurement. For historical measurements, Tracking Excavator must take a list of top-level URLs along with historical timestamps and transform them into appropriate URLs on archive.org. For example, the URL for the Wayback Machine’s February 10, 2016 snapshot of `https://www.usenix.org/conference/usenixsecurity16` is `https://web.archive.org/web/20160210050636/https://www.usenix.org/conference/usenixsecurity16`.

We use the Memento API to find the nearest archived snapshot of a website occurring before the specified measurement date [36]. Although this process ensures a reasonable timestamp for the top-level page, embedded resources may have been archived at different times [5]. During analysis, we thus filter out archived resources whose timestamps are more than six months from our measurement timestamp to ensure minimal overlap and sufficient spacing between measurements of different years.

### Data Collection

To collect data, Tracking Excavator uses a Chrome extension to automatically visit the set of input sites. Note that we cannot log into sites, as the Wayback Machine cannot act as the original server. Our browser is configured to allow third-party cookies as well as pop-ups, and we visit the set of sites twice: once to prime the cache and the cookie store (to avoid artifacts of first-time browser use), and once for data collection.

During these visits, we collect the following information relevant to third-party web tracking and store it in a local database:
- All request and response headers (including `set-cookie`).
- All cookies programmatically set by JavaScript (using `document.cookie`).
- All accesses to fingerprint-related JavaScript APIs, as described below.
- For each request: the requested URL, (if available) the referrer, and (if available) information about the originating tab, frame, and window.

We later process this data in the analysis phase of Tracking Excavator’s pipeline (Section 3.3 below).

### Fingerprint-Related APIs

Since cookie-based web tracking is extremely common (i.e., it is “classic” web tracking), we focus largely on it—and third-party requests in general—to capture the broadest view of the web tracking ecosystem over time. However, we also collect information about the use of other, more recently emerged tracking-related behaviors, such as JavaScript APIs that may be used to create browser or machine fingerprints [15, 57].

To capture any accesses a web page makes to a fingerprint-related JavaScript API (such as `navigator.userAgent`), Tracking Excavator’s Chrome extension Content Script overwrites these APIs on each web page to (1) log the use of that API and (2) call the original, overwritten function. The set of APIs that we hook was collected from prior work on fingerprint-based tracking [3, 4, 15, 56, 57] and is provided in Appendix A.

### Preventing Wayback “Escapes”

In archiving a page, the Wayback Machine transforms all embedded URLs to archived versions of those URLs (similar to our own process above). However, sometimes the Wayback Machine fails to properly identify and rewrite embedded URLs. As a result, when that archived page is loaded on archive.org, some requests may “escape” the archive and reference resources on the live web [9, 38]. In our data collection phase, we block such requests to the live web to avoid anachronistic side effects. However, we record the domain to which such a request was attempted, since the archived site did originally make that request, and thus we include it in our analysis.

### Data Analysis

In designing Tracking Excavator, we chose to separate data collection from data analysis, rather than detecting and measuring tracking behaviors on the fly. This modular architecture simplifies data collection and isolates it from possible bugs or changes in the analysis pipeline—allowing us to rerun different analyses on previously collected data (e.g., to retroactively omit certain domains).

#### “Replaying” Events

Our analysis metaphorically “replays” collected events to simulate loading each page in the measurement. For historical measurements, we modify request headers to replace “live web” `Set-Cookie` headers with `X-Archive-Orig-Set-Cookie` headers added by archive.org, stripping the Wayback Machine prefixes from request and referrer URLs, and filling our simulated cookie jar (described further below). During the replay, TrackingExcavator analyzes each event for tracking behaviors.

#### Classifying Tracking Behaviors

For cookie-based trackers, we base our analysis on a previously published taxonomy [60]. We summarize—and augment—that taxonomy here. Note that a tracker may fall into multiple categories, and that a single tracker may exhibit different behaviors across different sites or page loads:

1. **Analytics Tracking:** The tracker provides a script that implements website analytics functionality. Analytics trackers are characterized by a script, sourced from a third party but run in the first-party context, that sets first-party cookies and later leaks those cookies to the third-party domain.
2. **Vanilla Tracking:** The tracker is included as a third party (e.g., an iframe) in the top-level page and uses third-party cookies to track users across sites.
3. **Forced Tracking:** The tracker forces users to visit its domain directly—for example, by opening a popup or redirecting the user to a full-page ad—allowing it to set cookies from a first-party position.
4. **Referred Tracking:** The tracker relies on another tracker to leak unique identifiers to it, rather than on its own cookies. In a hypothetical example, `adnetwork.com` might set its own cookie and then explicitly leak that cookie in requests to referred tracker `ads.com`. In this case, `ads.com` need not set its own cookies to perform tracking.
5. **Personal Tracking:** The tracker behaves like a Vanilla tracker but is visited by the user directly in other contexts. Personal trackers commonly appear as social widgets (e.g., “Like” or “tweet” buttons).

In addition to these categories previously introduced [60], we discovered an additional type of tracker related to but subtly different from Analytics tracking:

6. **Referred Analytics Tracking:** Similar to an Analytics tracker, but the domain which sets a first-party cookie is different from the domain to which the first-party cookie is later leaked.

Beyond cookie-based tracking behaviors, we also consider the use of fingerprint-related JavaScript APIs, as described above. Though the use of these APIs does not necessarily imply that the caller is fingerprinting the user—we know of no published heuristic for determining fingerprinting automatically—the use of many such APIs may suggest fingerprint-based tracking.

Finally, in our measurements, we also consider third-party requests that are not otherwise classified as trackers. If contacted by multiple domains, these third-parties have the ability to track users across sites, but may or may not actually do so. In other words, the set of all domains to which we observe a third-party request provides an upper bound on the set of third-party trackers.

We tested TrackingExcavator’s detection and classification algorithms using a set of test websites that we constructed and archived using the Wayback Machine, triggering each of these tracking behaviors.

### Reconstructing Archived Cookies

For many tracking types, the presence or absence of cookies is a key factor in determining whether the request represents a tracking behavior. In our live measurements, we have the actual `Cookie` headers attached by Chrome during the crawl. On archived pages, the Wayback Machine includes past `Set-Cookie` headers as `X-Archive-Orig-Set-Cookie` headers on archived responses. To capture the cookies that would have actually been set during a live visit to that archived page, TrackingExcavator must simulate a browser cookie store based on these archival cookie headers and JavaScript cookie set events recorded during data collection.

Unfortunately, cookie engines are complicated and standards non-compliant in major browsers, including Chrome [11]. Python’s cookie storage implementation is compliant with RFC 2965, obsoleted by RFC 6265, but these standards proposals do not accurately represent modern browser practices [7, 13, 21]. For efficiency, we nevertheless use Python’s cookie jar rather than attempting to re-implement Chrome’s cookie engine ourselves.

We found that Python’s cookie jar computed cookies exactly matching Chrome’s for only 71% of requests seen in a live run of the top 100. However, for most types of tracking, we only need to know whether any cookies would have been set for the request, which we correctly determine 96% of the time. Thus, our tool captures most tracking despite using Python’s cookie jar.

### Classifying Personal Trackers in Measurements

For most tracker types, classification is independent of user behaviors. Personal trackers, however, are distinguished from Vanilla trackers based on whether the user visits that domain as a top-level page (e.g., Facebook or Google). To identify likely Personal trackers in automated measurement, we develop a heuristic for user browsing behaviors: we use popular sites from each year, as these are (by definition) sites that many users visited.

Alexa’s top sites include several that users would not typically visit directly, e.g., `googleadservices.com`. Thus, we manually examined lists of popular sites for each year to distinguish between domains that users typically visit intentionally (e.g., Facebook, Amazon) from those which ordinary users never or rarely visit intentionally (e.g., ad networks or CDNs). Two researchers independently classified the domains on the Alexa top 100 sites for each year where we have Alexa data, gathering information about sites for which they were unsure. The researchers examined 435 total domains: for the top 100 domains in 2015, they agreed on 100% and identified 94 sites as potential Personal trackers; for the 335 additional domains in the previous years’ lists, they agreed on 95.4% and identified 296 Personal tracker domains.

### Evaluating the Wayback Machine as an Archaeological Data Source for Tracking

The Wayback Machine provides a unique and comprehensive source of historical web data. However, it was not created for the purpose of studying third-party web tracking and is thus imperfect for that use. Nevertheless, the only way to study web tracking prior to explicit measurements targeting it is to leverage materials previously archived for other purposes. Therefore, before using the Wayback Machine’s archived data, it is essential to systematically characterize and analyze its capabilities and flaws in the context of third-party tracking.

In this section, we study the extent to which data from the Wayback Machine allows us to study historical web tracking behaviors. Beyond providing confidence in the trends of web tracking over time that we present in Section 5, we view this evaluation of the Wayback Machine as a contribution of this paper. While others have studied the quality of the Wayback Machine’s archive, particularly with respect to the quality of the archived content displayed on the top-level page (e.g., [10, 38, 53]), we are the first to systematically study the quality of the Wayback Machine’s data about third-party requests, the key component of web tracking.

To conduct our evaluation, we leverage four ground truth data sets collected from the live web in 2011, 2013, 2015, and 2016. The 2011 data was originally used in [60] and provided to us by those authors. All datasets contain classifications of third-party cookie-based trackers (according to the above taxonomy) appearing on the Alexa top 500 sites (from the time of each measurement). The 2015 and 2016 data was collected by TrackingExcavator and further contains all HTTP requests, including those not classified as tracking. We plan to release our ground truth datasets from 2013, 2015, and 2016.

We organize this section around a set of lessons that we draw from this evaluation. We apply these lessons in our measurements in Section 5. We believe our findings can assist future researchers seeking to use the Wayback Machine as a resource for studying tracking (or other web properties relying on third-party requests) over time.

#### Lessons from the Evaluation

- **All Third-Parties:** The number of third-party requests observed in the Wayback Machine data closely matches the live data, providing a reliable basis for our analysis.
- **Analytics, Vanilla, Forced, Referred, Personal, and Referred Analytics Tracking:** The Wayback Machine data accurately captures the presence and behavior of various types of trackers, although some discrepancies exist due to the limitations of the archive.

By applying these lessons, we can confidently use the Wayback Machine data to study the evolution of third-party web tracking over the last 20 years.