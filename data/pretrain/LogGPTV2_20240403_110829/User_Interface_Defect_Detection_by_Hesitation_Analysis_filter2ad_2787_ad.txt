ple, that at a threshold value of 3.0, an analyst would spend
60.1% less time searching through areas of non-difﬁculty,
while still ﬁnding 81.2% of the defects that would be found
by watching the entire media. Again, this is making the
very conservative estimate that an analyst would otherwise
search through the entire media only once. If the analyst
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply. 
Detector
sensitivity
2.0
3.0
4.0
5.0
6.0
Percent
defects
detected
90.6
81.2
71.8
68.8
59.4
Percent
time
saved
43.8
60.1
71.6
78.7
83.4
HR
FAR
89.4
77.3
65.2
59.1
51.5
28.2
19.0
13.1
10.0
7.8
Table 1: Percentage of all distinct defects detected by the
hesitation detector, percent of analyst time saved, Hit Rate
(HR), and False Alarm Rate (FAR) for ﬁve values of the
sensitivity (threshold) parameter (t).
would normally search through the media 3 to 10 times,
time savings would be 87% to 96%, while still ﬁnding
81.2% of the defects.
These results show that using hesitation detection to sift
through usability data can make user testing much less time-
consuming for analysts. If desired, the analyst time saved
could be spent by collecting and analyzing more data, which
would likely make up for any defects lost due to misses by
the hesitation detector. More data might also yield defects
that would not have manifested in a smaller data set. If the
percentage of defects detected still seems low, note that only
a perfect analyst would detect 100% of all defects with only
a single pass through the data; in reality, an analyst going
through the data only once will miss numerous defects due
to lack of attention and/or lack of vigilance. Moreover, the
analyst’s performance could be highly variable, while the
detector’s performance is repeatable.
Although a hesitation detector can be useful despite a
high false-alarm rate, it is nevertheless worth looking at
ways the false-alarm rate might be reduced. An informal
look through the false alarms reported by the hesitation de-
tector used in this study shows three top causes of false
alarms: pauses while moving the hands from mouse to key-
board or keyboard to mouse, pauses while checking work
just before committing changes, and pauses to read text
such as unusually long interface labels, error messages, and
Help ﬁles. A future version of the hesitation detector might
apply heuristics to detect these causes of false alarms and
thereby reduce the false-alarm rate.
A discussion of hesitation detection should also consider
the method’s limitations. First of all, hesitation detection
will only catch interface defects that manifest as hesitations.
Examples of defects that do not manifest as hesitations in-
clude defects that lead to errors of omission, defects that
only cause brief delays (such as misspellings of labels), and
defects that do not cause egregious hesitations (such as in-
consistent fonts). Second, hesitation detection can only be
applied after running user studies on fully implemented in-
terfaces. Other usability methods, such as inspection-based
methods, can isolate interface defects during the design
stage, thus avoiding the expense of implementing a ﬂawed
design. Third, for some interfaces, such as Web browsers or
multimedia players, hesitation detection might yield an un-
acceptably high false alarm rate because hesitations (to read
text or watch video, for example) are inherently part of us-
ing the interfaces. Finally, hesitation detection can indicate
defects, but does not suggest ﬁxes for those defects.
7 Conclusion
Hesitation detection is a method for detecting instances
of user difﬁculty, which are symptomatic of interface de-
fects, in streams of data from user-interface test sessions.
It can be applied to both ﬁeld and lab-based user stud-
ies to save time that a usability analyst would otherwise
have spent combing the data for trouble spots. This pa-
per measured the accuracy of a hesitation detector for data
from user tests of ﬁle-permissions-setting interfaces. The
results show that hesitations are an effective means for de-
tecting instances of user difﬁculty, and that hesitation detec-
tion promises to make usability studies less expensive and
more comprehensive. For example, up to 96% of an ana-
lyst’s wasted time can be saved by using hesitation detec-
tion, while still detecting 81% of all defects manifested in
usability data.
8 Future work
In the present study, hesitation detection was evaluated
in one task domain, namely setting ﬁle permissions. The
ﬁle-permissions domain shares characteristics with many
common task domains, such as system conﬁguration, vot-
ing, and image manipulation, so the results obtained here
are expected to generalize at least to those domains. Future
work will test the method in these and other task domains,
such as typing-intensive and long-duration tasks.
As discussed in section 6, the hesitation detection algo-
rithm used in this study might be improved by employing
heuristics to eliminate false alarms. A future version of
the hesitation detection algorithm might exclude hesitations
caused by transitions from mouse to keyboard or keyboard
to mouse, pauses to read interface messages, and pauses to
check work.
The study reported in this paper tested hesitation detec-
tion as a method for ﬁnding periods of user difﬁculty from
logs of novice-user sessions. Future work might consider
hesitation detection as a means to detect expert-user difﬁ-
culty, which can indicate different interface defects from
those indicated by novice-user difﬁculty.
This paper sought to establish hesitation detection as a
potential method for saving usability analysts’ time. The
results reported here suggest that hesitation detection does
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply. 
indeed have great potential to save analyst time. A planned
follow-up study can compare the performance of analysts
using hesitation detection against the performance of ana-
lysts using traditional techniques. This will determine the
actual improvement that can be gained from hesitation de-
tection, and will help to establish hesitation detection as a
practical technique.
9 Acknowledgements
The authors would like to thank Rachel Roberts and Pat
Loring for their contributions to this work. The authors also
wish to thank the anonymous reviewers whose thoughtful
remarks inspired improvements in the paper. This work
was supported in part by National Science Foundation grant
number CNS-0430474.
References
[1] A. Avizienis, J.-C. Laprie, B. Randell, and C. Landwehr. Ba-
sic concepts and taxonomy of dependable and secure com-
puting. IEEE Transactions on Dependable and Secure Com-
puting, 1(1):11–33, January-March 2004.
[2] G. Cockton, D. Lavery, and A. Woolrych. Inspection-based
evaluations.
In J. A. Jacko and A. Sears, editors, The
Human-Computer Interaction Handbook, chapter 57, pages
1118–1138. Lawrence Erlbaum Associates, Mahwah, NJ,
2003.
[3] G. Cockton and A. Woolrych. Sale must end: should dis-
count methods be cleared off HCI’s shelves? Interactions,
9(5):13–18, September 2002.
[4] H. W. Desurvire, J. M. Kondziela, and M. E. Atwood. What
is gained and lost when using evaluation methods other than
empirical testing. In People and Computers VII: Proceed-
ings of the HCI ’92 Conference, pages 89–102, Cambridge,
September 1992. Cambridge University Press.
[5] J. S. Dumas. User-based evaluations.
In J. A. Jacko and
A. Sears, editors, The Human-Computer Interaction Hand-
book, chapter 56, pages 1093–1117. Lawrence Erlbaum As-
sociates, Mahwah, NJ, 2003.
[6] K. A. Ericsson and H. A. Simon. Protocol Analysis: Ver-
bal Reports as Data. MIT Press, Cambridge, MA, revised
edition, 1993.
[7] E. Horvitz, J. Breese, D. Heckerman, D. Hovel, and K. Rom-
melse. The Lumiere project: Bayesian user modeling for in-
ferring the goals and needs of software users. In Proceedings
of the Fourteenth Conference on Uncertainty in Artiﬁcial In-
telligence, pages 256–265, Madison, WI, July 1988. Morgan
Kaufmann.
[8] M. Y. Ivory and M. A. Hearst. The state of the art in automat-
ing usability evaluation of user interfaces. ACM Computing
Surveys, 33(4):470–516, December 2001.
[9] B. E. John and D. E. Kieras. The GOMS family of user in-
terface analysis techniques: comparison and contrast. ACM
Transactions on Computer-Human Interaction (TOCHI),
3(4):320–351, December 1996.
[10] B. E. John and D. E. Kieras. Using GOMS for user interface
design and evaluation: Which technique? ACM Transac-
tions on Computer-Human Interaction (TOCHI), 3(4):287–
319, December 1996.
[11] D. Kieras. Model-based evaluations.
In J. A. Jacko and
A. Sears, editors, The Human-Computer Interaction Hand-
book, chapter 58, pages 1139–1151. Lawrence Erlbaum As-
sociates, Mahwah, NJ, 2003.
[12] L.-C. Law and E. T. Hvannberg. Complementarity and con-
vergence of heuristic evaluation and usability test: a case
study of universal brokerage platform. In Proceedings of the
second Nordic conference on human-computer interaction,
pages 71–80, New York, NY, 2002. ACM Press.
[13] L.-C. Law and E. T. Hvannberg. Analysis of strategies
for improving and estimating the effectiveness of heuristic
evaluation.
In Proceedings of the third Nordic conference
on human-computer interaction, pages 241–250, New York,
NY, 2004. ACM Press.
[14] A. Lecerof and F. Paterno. Automatic support for usabil-
ity evaluation. IEEE Transactions on Software Engineering,
24(10):863–888, October 1998.
[15] M. Macleod and R. Rengger. The development of DRUM:
A software tool for video-assisted usability evaluation.
In
J. Alty, D. Diaper, and S. Guest, editors, People and
Computers VIII: Proceedings of the HCI ’93 Conference,
September 1993, chapter 20, pages 293–309. Cambridge
University Press, Cambridge, 1993.
[16] R. A. Maxion and A. L. deChambeau. Dependability at the
user interface. In Twenty-Fifth International Symposium on
Fault-Tolerant Computing, pages 528–535, Los Alamitos,
CA, June 1995. IEEE Computer Society Press.
[17] R. A. Maxion and R. W. Reeder. Improving user-interface
dependability through mitigation of human error. Interna-
tional Journal of Human-Computer Studies, 63(1-2):25–50,
July 2005.
[18] R. A. Maxion and P. A. Syme. Metristation: A tool for user-
interface fault detection.
In Twenty-Seventh International
Symposium on Fault-Tolerant Computing, pages 89–98, Los
Alamitos, CA, June 1997. IEEE Computer Society Press.
[19] J. Nielsen. Usability Engineering. Academic Press, Inc.,
San Diego, CA, 1993.
[20] J. Nielsen. Heuristic evaluation.
In J. Nielsen and R. L.
Mack, editors, Usability Inspection Methods, chapter 2,
pages 25–62. John Wiley and Sons, New York, 1994.
[21] J. Nielsen and V. L. Phillips. Estimating the relative usability
of two interfaces: Heuristic, formal, and empirical methods
compared. In INTERCHI ’93 — Conference on Human Fac-
tors in Computing Systems, pages 214–221, New York, NY,
April 1993. ACM Press.
[22] M. Rauterberg. AMME: an automatic mental model evalua-
tion to analyse user behaviour traced in a ﬁnite, discrete state
space. Ergonomics, 36(11):1369–1380, November 1993.
[23] J. Rubin. Handbook of Usability Testing. John Wiley and
Sons, Inc., New York, 1994.
[24] C. Wharton, J. Rieman, C. Lewis, and P. Polson. The
cognitive walkthrough method: A practitioner’s guide.
In
J. Nielsen and R. L. Mack, editors, Usability Inspection
Methods, chapter 5, pages 105–140. John Wiley and Sons,
New York, 1994.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply.