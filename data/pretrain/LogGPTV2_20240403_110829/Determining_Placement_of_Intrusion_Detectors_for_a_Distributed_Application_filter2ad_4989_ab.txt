6
System-Cost = System-Cost + COST(di, aj)
/* COST(di, aj) can be in terms of economic cost, cost due
to false alarms and missed alarms, etc. */
if (System-Cost > Threshold τ) break
if (di ∈ θ) add aj to πi ∈ θ
else add (di, πi = aj) to θ
end for
return θ
BENEFIT (d, a)
7
8
9
10
11
1
2
3
4
5
6
/* This is to calculate the beneﬁt from attaching detector d
to attack vertex a */
Let the end attack vertices in the BN be F = fi, i = 1, . . . , M
For each fi, the following cost-beneﬁt table exists
Perform Bayesian inference with d as the only detector
in the network and connected to attack vertex a
Calculate for each fi, the precision and recall, call them,
Precision(fi, d, a), Recall(fi, d, a)
System-Beneﬁt =
(cid:3)
M
i=1 [Beneﬁtfi(True Negative) × Precision(fi, d, a)
+ Beneﬁtfi(True Positive) × Recall(fi, d, a)]
return System-Beneﬁt
The algorithm starts by sorting all combinations of detectors and their asso-
ciated attack vertices according to their beneﬁt to the overall system (line 2).
The system beneﬁt is calculated by the BENEFIT function. This speciﬁc design
considers only the end nodes in the BN, corresponding to the ultimate attack
goals. Other nodes that are of value to the system owner may also be considered.
Note that a greedy decision is made in the BENEFIT calculation each detector
is considered singly. From the sorted list, (detector, attack vertex) combinations
are added in order, till the overall system cost due to detection is exceeded (line
7). Note that we use a cost-beneﬁt table (line 2 of BENEFIT function), which is
likely speciﬁed for each attack vertex at the ﬁnest level of granularity. One may
also specify it for each host or each subnet in the system.
The worst-case complexity of this algorithm is O(dv B(v, CP T (v))+dv log(dv)
+ dv), where d is the number of detectors and v is the number of attack ver-
tices. B(v, CP T (v)) is the cost of Bayesian inference on a BN with v nodes and
CP T (v) deﬁning the edges. The ﬁrst term is due to calling Bayesian inference
with up to d times v terms. The second term is the sorting cost and the third
term is the cost of going through the for loop dv times. In practice, each detector
will be applicable to only a constant number of attack vertices and therefore the
Determining Placement of Intrusion Detectors
279
dv terms can be replaced by a constant times d, which will be only d considering
order statistics.
The reader would have observed that the presented algorithm is greedy-choice
of detectors is done according to a pre-computed order, in a linear sweep through
the sorted list L (the for loop starting in line 4). This is not guaranteed to provide
an optimal solution. For example, detectors d2 and d3 taken together may provide
greater beneﬁt even though detector d1 being ranked higher would have been
considered ﬁrst in the DETECTOR-PLACEMENT algorithm. This is due to
the observation that the problem of optimal detector choice and placement can
be mapped to the 0-1 knapsack problem which is known to be NP-hard. The
mapping is obvious, consider D × A (D: Detectors and A: Attack vertices). We
have to include as many of these tuples so as to maximize the beneﬁt without
the cost exceeding, the system cost of detection.
4 Experimental Systems
We created three Bayesian networks for our experiments modeling two real sys-
tems and one synthetic network. These are a distributed electronic commerce (e-
commerce) system, a Voice-over-IP (VoIP) network, and a synthetic generic
Bayesian network that is larger than the other two. The Bayesian networks were
manually created from attack graphs that include several multi-step attacks for
the vulnerabilities found in the software used for each system. These vulnerabili-
ties are associated with speciﬁc versions of the particular software, and are taken
from popular databases [6], [23]. An explanation for each Bayesian network follows.
4.1 E-Commerce System
The distributed e-commerce system used to build the ﬁrst Bayesian network is a
three tier architecture connected to the Internet and composed of an Apache web
server, the Tomcat application server, and the MySQL database backend. All
servers are running a Unix-based operating system. The web server sits in a de-
militarized zone (DMZ) separated by a ﬁrewall from the other two servers, which
are connected to a network not accessible from the Internet. All connections from
the Internet and through servers are controlled by the ﬁrewall. Rules state that
the web and application servers can communicate, as well as the web server
can be reached from the Internet. The attack scenarios are designed with the
assumption that the attacker is an external one and thus her starting point is
the Internet. The goal for the attacker is to have access to the MySQL database
(speciﬁcally access customer conﬁdential data such as credit card information
node 19 in the Bayesian network of Figure 4).
As an example, an attack step would be a portscan on the application server
(node 10). This node has a child node, which represents a buﬀer overﬂow vulner-
ability present in the rpc.statd service running on the application server (node
12). The other attack steps in the network follow a similar logic and represent
other phases of an attack to the distributed system. The system includes four
280
G. Modelo-Howard, S. Bagchi, and G. Lebanon
Web Server
DMZ
Internet
Firewall
Internal 
Network
Application
Server
Database
Server
3
7
13
20
6
1
2
b
14
17
19
4
8
10
12
15
16
5
9
11
18
Fig. 4. Network diagram for the e-commerce system and its corresponding Bayesian
network. The white nodes are the attack steps and the gray nodes are the detectors.
detectors: IPtables, Snort, Libsafe, and a database IDS. As shown in Figure 4,
each detector has a causal relationship to at least one attack step.
4.2 Voice-over-IP (VoIP) System
The VoIP system used to build the second network has a few more components,
making the resulting Bayesian network more complex. The system is divided
into three zones: a DMZ for the servers accessible from the Internet, an internal
network for local resources such as desktop computers, mail server and DNS
server, and an internal network only for VoIP components. This separation of
the internal network into two units follows the security guidelines for deploying
a secure VoIP system [18].
The VoIP network includes a PBX/Proxy, voicemail server and software-based
and hardware-based phones. A ﬁrewall provides all the rules to control the traﬃc
between zones. The DNS and mail servers in the DMZ are the only accessible
hosts from the Internet. The PBX server can route calls to the Internet or to a
public-switched telephone network (PSTN). The ultimate goal of this multi-stage
DNS
Mail
Internal User
DNS
Mail
Internet
Internal
Network
VoIP
Network
DMZ
Firewall
VoIP Phone
(hardware)
VoIP Phone
(software)
VoiceMail
PBX/Proxy
PSTN
3
6
18
22
4
7
9
11
14
17
1
2
21
5
8
10
12
15
19
13
16
20
Fig. 5. VoIP system and its corresponding Bayesian network
Determining Placement of Intrusion Detectors
281
Attack = True
Attack = False
Detection = True
Detection = False
TP
FN
FP
TN
Recall
(cid:32)
TP
(cid:14)
TP
FN
Precision
(cid:32)
TP
(cid:14)
TP
FP
Fig. 6. Parameters used for our experiments: True Positive (TP), False Positive (FP),
True Negative (TN), False Negative (FN), precision, and recall
attack is to eavesdrop on VoIP communication. There are 4 detectors Iptables,
and three network IDSs on the diﬀerent subnets.
A third synthetic Bayesian network was built to test our framework for exper-
iments where a larger network, than the other two, was required. This network
is shown in Figure 7(a).
5 Experiments
The correct number, accuracy, and location of the detectors can provide an ad-
vantage to the systems owner when deploying an intrusion detection system.
Several metrics have been developed for evaluation of intrusion detection sys-
tems. In our work, we concentrate on the precision and recall. Precision is the
fraction of true positives determined among all attacks ﬂagged by the detection
system. Recall is the fraction of true positives determined among all real posi-
tives in the system. The notions of true positive, false positive, etc. are shown
in Figure 6. We also plot the ROC curve which is a traditional method for char-
acterizing detector performanceit is a plot of the true positive against the false
positive.
For the experiments we create a dataset of 50,000 samples or attacks, based
on the respective Bayesian network. We use the Matlab Bayesian network tool-
box [3] for our Bayesian inference and sample generation. Each sample consists
of a set of binary values, for each attack vertex and each detector vertex. A one
(zero) value for an attack vertex indicates that attack step was achieved (not
achieved) and a one (zero) value for a detector vertex indicates the detector
generated (did not generate) an alert. Separately, we perform inference on the
Bayesian network to determine the conditional probability of diﬀerent attack
vertices. The probability is then converted to a binary determination whether
the detection system ﬂagged that particular attack step or not, using a thresh-
old. This determination is then compared with reality, as given by the attack
samples which leads to a determination of the systems accuracy. There are sev-
eral experimental parameters which speciﬁc attack vertex is to be considered,
the threshold, CPT values, etc. and their values (or variations) are mentioned
in the appropriate experiment. The CPTs of each node in the network are man-
ually conﬁgured according to the authors experience administering security for
distributed systems and frequency of occurrences of attacks from references such
as vulnerability databases, as mentioned earlier.
282
G. Modelo-Howard, S. Bagchi, and G. Lebanon
5.1 Experiment 1: Distance from Detectors
The objective of experiment 1 was to quantify for a system designer what is the
gain in placing a detector close to a service where a security event may occur.
Here we used the synthetic network since it provided a larger range of distances
between attack steps and detector alerts.
The CPTs were ﬁxed to manually determined values on each attack step.
Detectors were used as evidence, one at a time, on the Bayesian network and
the respective conditional probability for each attack node was determined. The
eﬀect of the single detector on diﬀerent attack vertices was studied, thereby
varying the distance between the node and the detector. The output metric is
the diﬀerence of two terms. The ﬁrst term is the conditional probability that
the attack step is achieved, conditioned on a speciﬁc detector ﬁring. The second
term is the probability that the attack step is achieved, without use of any
detector evidence. The larger the diﬀerence is, the greater is the value of the
information provided by the detector. In Figure 7(b), we show the eﬀect due
to detector corresponding to node 24 and in Figure 7(c), we consider all the
detectors (again one at a time). The eﬀect of all the detectors shows that the
conclusions from node 24 are general.
22
24
26
1
3
4
6
23
8
9
10
11
25
13
14
16
17
27
2
5
7
12
b
15
28
18
19
20
21
i
(
)
1
=
X
P
−
)
1
=