approaches [28, 18] that precisely track the TCP state, given bidi-
rectional trace. We validate the accuracy of our approaches in §5.
4.1 Inferring Initial Congestion Window Size
Inferring the initial congestion window (ICW) size helps detect
aggressive TCP ﬂows that send a larger burst of data at the begin-
ning of the connection without any throttling. The value of ICW
is the number of bytes a TCP sender can send immediately after
establishing the connection, before receiving any ACKs from the
receiver. We devise the following algorithm shown as Algorithm 1
to measure ICW given a unidirectional ﬂow trace. The key task
of this algorithm is to examine the normalized inter-arrival times
(IAT) between the ﬁrst k + 1 data packets. The ﬁrst “large” gap
(i.e., larger than θ, a threshold of normalized IAT as shown in Al-
gorithm 1) indicates that the sender has reached its congestion win-
dow and is waiting for ACKs from the receiver. If such a gap is
not detected, we pick the maximum gap among all k IATs. Note
that in Algorithm 1, I1, the IAT between the SYN-ACK packet and
the ﬁrst data packet usually includes the extra delay caused by the
server OS and is therefore discarded. As a result, Algorithm 1 only
examines I2, ..., Ik. Based on our empirical ﬁndings in §5.1, we
choose k = 8 and θ = 0.2.
There are several limitations in Algorithm 1. (i) Our approach
only works for server-to-client data ﬂows (i.e., the trace starts with
a SYN-ACK packet) due to lack of sufﬁcient data from client to
server; (ii) there must be no retransmissions in the ﬁrst k + 1 pack-
ets; (iii) the only factor that prevents the sender from sending more
data should be reaching the congestion window. This is ensured by
requiring that the ﬁrst c − 1 packets have same size (i.e., equal to
the maximum segment size M SS), therefore c × M SS is the in-
ferred ICW (Line 6 to 7); (iv) if ﬂight-based IAT is too small (e.g.,
RTT  θ then c ← j − 1; exit for; endif
if Ij/ Pk
{Ij} − 1; endif
6: if ﬁrst c − 1 packets have the same packet size M SS then
7:
return c × M SS; else return unknown ICW; endif
4.2 Detecting Irregular Retransmission
Slowing down during retransmission (especially during periods
with many retransmissions) is one of the fundamental requirements
for all RFC-compliant TCP implementations [8], since retransmis-
sions indicate packet loss as inferred by the TCP sender [17]. We
denote a TCP ﬂow that does not slow down its sending rate during
retransmission as a ﬂow with irregular retransmission.
We devised a new tool called the Rate Tracking Graph (RTG),
based on a statistical algorithm for detecting irregular retransmis-
sion behavior. The basic idea behind RTG is an intuitive observa-
tion that holds for all known TCP implementations: when retrans-
mission rate increases, the sender should decrease the upper bound
of its sending rate by reducing the congestion window [8]. This im-
plies a negative correlation between the retransmission rate and the
sending rate, i.e., a positive correlation between the retransmission
rate r and the time t required to successfully transfer a ﬁxed size
of data (e.g., 50 KB). RTG samples all pairs of (t, r) by sliding a
tracking window W along the ﬂow to test whether t and r exhibit
any strong positive correlation.
The details of our RTG tool are described in Algorithm 2. The
input is a unidirectional TCP ﬂow trace (server to client) with high
retransmission rate (e.g., >10%). This is because RTG requires
sufﬁcient number of sample points (t, r) to generate statistically
conﬁdent results. A lower threshold of retransmission rate may de-
tect more irregular retransmissions, but the conﬁdence of accuracy
decreases as well.
80Algorithm 2 ﬁrst identiﬁes retransmitted bytes by examining re-
peated sequence numbers (Line 1 to 4). Subsequently, given a ﬁxed
tracking window size W , it samples all pairs of (t, r) by sliding
the window of varying length t along the ﬂow, where t is deter-
mined by the requirement that there are W non-retransmitted bytes
in the window, and where r is the retransmission rate of the window
(i.e., retransmitted bytes in the window divided by W ). In the case
of regular retransmissions illustrated in Figure 5(a), t and r have
strong positive correlation. We are interested in RTGs with small
positive or negative correlation coefﬁcients, as illustrated with the
example shown in Figure 5(b).
It is important to point out cases where a well-behaved ﬂow does
not exhibit a strongly positive correlation coefﬁcient. If the ﬂow
rate is not limited by the congestion window, it is not necessary
for the sender to slow down its rate, even if the congestion window
is reduced. In fact, in our scenario with high retransmission rate,
the rate limiting factor can also be [35] (i) the server application (it
does not generate data fast enough); (ii) the server’s sending buffer
in OS kernel; (iii) the receiver’s window; (iv) the bottleneck link.
In particular, case (i) accounts for 30% to 50% irregular retrans-
missions in our datasets, and will be discussed in detail in §6.2.
There exist other factors that may affect the correlation coefﬁ-
cient of RTG. First, the tracking window W should be large enough
to include more than one RTT, and be small enough so that sliding
the tracking window covers various retransmission rates. We em-
pirically choose 4 tracking window sizes: 50KB, 100KB, 200KB,
and 400KB, and conservatively claim an irregular transmission if
all tracking windows yield correlation coefﬁcients less than 0.1.
Second, at the sender’s side, there may be pauses that enlarge t.
For interactive Web applications, e.g., the server may be idle for
seconds, with no data to send. We devise an Entropy-based cut-
ting algorithm that removes large gaps by separating the ﬂow into
segments. We then generate RTGs for each sufﬁciently large seg-
ment (i.e., greater than 1MB) whose IATs are less intermittent than
those of the original ﬂow. A ﬂow’s IAT-Entropy is deﬁned as the
following (Pi denotes the ith packet):
EIAT = − XPi,Pi+1
iat(Pi, Pi+1)
d
log„ iat(Pi, Pi+1)
d
«
where d is the ﬂow duration. The algorithm iteratively cuts a seg-
ment S into S1 and S2 as long as max{EIAT (S1), EIAT (S2)}
> EIAT (S). Here, an increase in entropy indicates that the IATs in
the newly generated segments are more homogeneous. In practice,
such entropy-based cutting requires no tuning parameters and suc-
ceeds in removing large gaps. The remaining small gaps may add
“noise” to the RTG, but usually they do not signiﬁcantly change the
correlation coefﬁcients. The third factor concerns dramatic changes
in the sending rate, as illustrated with an example in Figure 10(g).
Given the rare occurrence of this case in our datasets, we will deal
with this as future work.
4.3 Flow Clock Extraction
We deﬁne the TCP ﬂow clock to correspond to the regular spac-
ing between ﬂights of packets. The most commonly accepted cause
of TCP ﬂow clocking is RTT-based and hence inherently linked to
the transport layer [33, 36]. By devising a methodology for accu-
rately extracting TCP ﬂow clock information from unidirectional
packet traces and applying it to actual data, we observe that TCP
ﬂow clocking can also originate from the application layer or even
the link layer. Understanding the different root causes for TCP ﬂow
clocks has far-reaching implications. For one, if the ﬂow clock is
not generated by the transport layer, existing algorithms [33, 36]
that implicitly associate RTT with ﬂow clock will suffer from low
if ∃byte b′ : (b′.seq = b.seq) ∧ (b′.ts > b.ts) then
Algorithm 2 Rate Tracking Graph
Input: Unidirectional Packet Trace T , Window size W
Output: Rate Tracking Graph
Require: T has signiﬁcant retransmissions (> 10%)
1: for all byte b ∈ T do
2:
3:
4: end for
5: head ← 0; tail ← 1
6: while tail ≤ T.len do
7:
head ← head + 1
8:
b.lbl ← 0; else b.lbl ← 1; endif
while (tail ≤ T.len) ∧ (
tail
P
i=head
byte(i).lbl < W ) do
tail ← tail + 1
end while
if tail ≤ T.len then
9:
10:
11:
12:
13:
14:
end if
15: end while
r = tail − head + 1 − W ; t = byte(tail).ts − byte(h).ts
Plot (t, r) on Rate Tracking Graph
accuracy. Second, we ﬁnd that ﬂows with large non-RTT based
ﬂow clock tend to have more consistent ﬂight size. Also, these
ﬂows are more likely to transfer data with an inappropriately large
congestion window, due to a larger timeout value not complying
with RFC [5], as illustrated in §6.3. Third, we envision that ﬂow
clocks can serve as a new feature for trafﬁc classiﬁcation and net-
work anomaly detection.
The main idea behind our method for accurately extracting the
dominant ﬂow clock (if it exists) is as follows. We view a packet
trace as a sequence of pulse signals in temporal domain. Next we
transform the signal into the frequency domain via Fourier Trans-
form. In the frequency domain, we design an algorithm that com-
bines pattern recognition techniques with our empirical knowledge
about TCP clocking to detect peaks (spikes) within relevant fre-
quency bands. Lastly, the ﬂow clock is deﬁned to be the fundamen-
tal frequency i.e., the lowest frequency in a harmonic series [25].
2
Our detailed implementation of this ﬂow clock extraction algo-
rithm consists of 6 steps. (i) Given a unidirectional packet trace
T , the algorithm discretizes timestamps of T into a binary array
B using a sampling frequency of 500Hz; B(i) = 1 if and only if
there is at least one packet that arrived between times 2i and 2i + 2
msec. (ii) We use the Discrete Fourier Transform (DFT) to trans-
form B into the frequency domain: F = DF T (B, 2⌈logB.len
⌉),
then downsample F to 1,000 points (resolution of each point is
0.25Hz). (iii) Detect the local maxima (candidate peaks) by slid-
ing a window of size w and sensitivity s along the spectrum, and
mark points whose amplitude is larger than µ + sσ as candidate
peaks (µ: mean, σ: standard deviation of the points within the win-
dow). In our implementation, we apply 3 pairs of (w, s) to discover
both narrow and wide peaks: w = 4, 8, 16 and s = 8, 16, 32.
(iv) Cluster consecutive candidate peaks (distance of less than 5
points) into a single peak; remove peaks whose amplitude is less
than µ0 + 3σ0 (µ0: mean, σ0: standard deviation of all 1,000
points). (v) For each peak with frequency f, test whether f is a
fundamental frequency: for k = 2, 3, 4, if there exists a peak with
frequency f ′ ∈ (kf − δ, kf + δ) where the tolerance parameter δ
is set to three3. (vi) Return the minimum fundamental frequency if
found.
In the above approach, after downsampling the spectrum to 1,000
points, the resolution of each point is 0.25Hz. Therefore the ex-
3In our implementation, for a fundamental frequency, we only re-
quire 2 out of 3 values of k correspond to peaks to increase robust-
ness to errors.
81tracted fundamental frequency may be inaccurate when the ﬂow
clock is large. We solve this problem by performing additional
postprocessing if the fundamental frequency is less than 5Hz. First,
we separate the ﬂow into ﬂights based on the rough estimation of
ﬂow clock using the algorithm introduced in §4.1 of [35], except
that here we rely on an estimation of the ﬂow clock instead of
using blind search as it is in [35]. Next, the reﬁned ﬂow clock
is calculated as the average time difference between the begin-
ning of consecutive ﬂights after removing outliers falling outside
(µ − 3σ, µ + 3σ).
We tuned the above parameters based on the empirical ﬁndings
described in §5.3. In rare cases, a ﬂow may possess two or more
fundamental frequencies e.g., both RTT-based clocks and application-
based clocks are observable in the ﬂow. We ﬁnd that the small-
est fundamental frequency usually obscures the detection of larger
ones, so that discovering a second or third fundamental frequencies
may not be accurate or informative in practice. We intend to pursue
this issue in future work.
5. METHODOLOGY VALIDATION
We systematically evaluate our algorithms introduced in §4. We
ﬁrst validate the ICW estimation algorithm by comparing with ac-
tive probing in the TBIT approach [26], followed by an analysis of
false positives in RTGs by triggering retransmissions through in-
jected packet losses to thousands of HTTP downloading sessions,
and ﬁnally validate ﬂow clock detection by comparing with ground
truths obtained from ﬂow traces of different types.
The same dataset for active probing, consisting of 3,131 URLs,
each pointing to a ﬁle with size greater than 1MB, is used for ﬁrst
two sets of experiments in §5.1 and §5.2. We performed DNS
lookup for the domain part of each URL and replaced it with one
or more IP addresses to eliminate DNS based server load balanc-
ing. This expanded the dataset to 5,844 URLs. We set up a testbed
for URL query experiments based on the TBIT [26] tool which
infers TCP behavior of Web servers by active probing. For exam-
ple, TBIT infers ICW by sending an HTTP GET request packet
and not acknowledging any further packet. The Web server will
only be able to send packets that ﬁt within its ICW before retrans-
mitting the ﬁrst data packet. We added two new tests to TBIT:
ICWPassive and RTG. After establishing the connection to the
Web server, ICWPassive receives k + 1 packets, closes the con-
nection by sending a TCP RST and estimates ICW passively as
described in §4.1; RTG receives data as a normal TCP receiver but
randomly drops packets at a certain loss rate, then generates RTG
based on §4.2 after connection termination. Besides these enhance-
ment, we also improved TBIT in several other aspects, e.g., made
the format conform to Konqueror 3.5.8 for FreeBSD 7.
5.1 Inferring Initial Congestion Window Size
As shown in Table 3, the experiment was performed using 3 dif-
ferent MSS values: 1460B, 512B and 128B. For each MSS, each
URL was probed 5 times. We eliminate cases where probing fails
due to HTTP errors (less than 30%), or either algorithm reports in-
consistent results in 5 trials (less than 0.7%). For the remaining
URLs, we regard a probing as accurate if both algorithms produce
the same result. We report the accuracy for MSS=1460B, 512B
and 128B to be 98.4%, 98.8% and 99.2%, respectively. Inconsis-
tent cases are conservatively considered as inaccurate.
Algorithm 1 has two parameters k and θ. For k, we tried k =
7, 8, 9, 10 and ﬁnally chose k = 8 since it results in the highest ac-
curacy for all three MSS. We chose θ = 0.2 based on the distribu-
tion of normalized IAT for I2, ..., I8 where I norm
a=2 Ia
for 2 ≤ j ≤ 8 (Figure 3(a)), since θ = 0.2 well separates two fre-
= Ij/P8
j
Figure 3:
(a) Distribution of normalized IAT for the ﬁrst 9
packets (excluding IAT of SYN-ACK and ﬁrst data packet) (b)
Distribution of normalized IAT for the ﬁrst 9 packets
Table 3: Compare the ICW algorithm with TBIT
MSS
Total URLs
HTTP errors
Inconsistent results by TBIT
Inconsistent results by Algorithm 1
Both return inconsistent results