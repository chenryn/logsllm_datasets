updating the policies once they have been deployed. For ex-
ample, a new policy could be delivered with other security
updates to the application.
Entry Lifetime and Frequency.
Entry frequency—the rate at which new entries appear or
old entries are altered—is a crucial parameter for feeds used
with our system, because it determines how often fresh en-
tries will be available. Among sites in the Popular data set
where any new entries appeared during our survey period,
the average entry frequency was 16.5 per day. Frequencies
ranged from as high as 125 entries per day to only a single
entry in our week long sample. For active sites in the Long-
tail data set, the entry frequency averaged a much lower 4.77
per day, but ranged as high as 65.8 per day.
Another important parameter, entry lifetime, measures
the time from when an entry ﬁrst appears in the feed until
the time when it is updated, deleted, or displaced by newer
content. The average entry lifetime for a source determines
how long we can expect to be able to verify the source’s
content. We measured lifetimes for entries that died during
our data collection period.
If an entry was posted before
we began collecting data, we used the posting date listed in
the feed as the start of the entry’s lifetime. We recorded
the lifetime of 11779 entries from 102 sites in the Popular
dataset. The average lifetime of an entry was 38.5 hours, but
the averages for individual sites ranged from 3.7 hours to 43
days (among sites for which we had a statistically signiﬁcant
number of data points). From Longtail sources, we recorded
2404 entry lifetimes from 72 distinct sites. The average life-
time was 118 hours, and the averages for individual sites
ranged from 6.9 hours to 90 days (among sites for which we
had suﬃcient data).
RSS feeds typically include a ﬁxed number of entries, with
older entries aging out as newer ones are posted. The num-
ber of entries present in an RSS feed limits how many con-
tent chunks we can extract from it. We recorded the average
number of entries present in each available feed. For sources
in the Longtail data set, the average number of entries was
16.3; for sources in the Popular data set, the average was
22.3. Since feed sizes are usually constant, we expect to see
an inverse relationship between the frequency of new entries
and the average time entries remained in a feed, as shown
in Figure 2. We observe that points in the horizontal tail
of the distribution tend to come from the Popular data set,
indicating that Popular sources are more likely to post sev-
eral entries a day, leading to a short lifetime for each entry.
Points in the vertical tail tend to come from the Longtail
data set, indicating that those sites are more likely to post
entries infrequency, leading their entries to have a long life-
time.
Policy Satisfaction.
To test how well the RSS feeds in our data sets would serve
as sources for harvested challenges, we modeled a number of
simple policies and veriﬁability requirements and calculated
the percentage of the sample period when the policies and
requirements would be satisﬁed.
Generally, the more time that elapses between derivation
and veriﬁcation, the less precisely we need to know the fresh-
ness of the challenge. If a week has passed since the challenge
was generated, it is less likely to matter whether the chal-
lenge was fresh a few hours earlier or later. This mirrors the
inverse relationship between entry lifetime and frequency de-
picted in Figure 2. Thus, we expect RSS feeds to meet the
needs of many kinds of applications.
In the ﬁrst policy we modeled, which we label “Short,” a
deriver collects all entries from the source that are less than
one hour old, and veriﬁers require at least one entry in the
derivation to match the contents of the feed. We calculated
satisfaction using this policy and two robustness standards,
which required the policy to be veriﬁable a minimum of 6
hours and 12 hours after derivation. We tested these models
with the Popular sources. Series A and B of Figure 3 show
the percentage of sources that satisﬁed these requirements
at least a given fraction of the time. In all, 13% of sources
satisﬁed the 6 hour requirement at least 50% of the time, as
did 7% the 12 hour requirement.
In the second policy we modeled, which we label “Long,”
derivers collect all entries from the source that are less than
one day old, and veriﬁers require at least one entry in the
derivation to match the contents of the feed. We calculated
satisfaction using this policy and two robustness standards,
which required the policy to be veriﬁable a minimum of 7
days and 14 days after derivation. We tested these models
with the Longtail sources. Series C and D of Figure 3 show
the percentage of sources that satisﬁed these requirements
at least a given fraction of the time. In all, 24% of sources
satisﬁed the 7 day requirement at least 50% of the time, as
did 7% the 14 day requirement.
As these data show, not all sources are equally suitable
for harvesting challenges; however, a signiﬁcant fraction of
sources were able to satisfy our model requirements at least
half the time. Policy creators can achieve high robustness
by combining data from a number of such sources. If sat-
isfaction times were uniformly distributed and uncorrelated
between sources, we would need to combine 10 sources with
a suboptimal 50% satisfaction rate to reduce expected down-
time to less than 90 seconds a day.
In practice, satisfaction times were correlated and not dis-
tributed uniformly, but policy creators will tend to select
sources that are well suited for their policies. Here we depict
the satisfaction of 7 selected sources modeling the “Short”
policy with a 6 hour veriﬁability requirement. Each horizon-
tal bar represents one source over the course of the week;
shaded areas indicate times when challenges could be har-
vested and meet these requirements. At least one of the
sources satisﬁed the policy at all times during the period; at
least two sources satisﬁed the policy more than 90% of the
time:
The next ﬁgure depicts satisfaction of 7 selected sources
modeling the “Long” policy with a 7 day veriﬁability require-
ment. At least two of the sources satisﬁed the policy at all
times during the period:
Our models indicate that RSS feeds may not be well suited
to applications that require freshness guarantees within a
time frame much shorter than an hour, ones that require
veriﬁability over periods greater than a few weeks, or ones
that require precise timestamps that are veriﬁable over long
periods. Other sources may be more appropriate for such
applications. For instance, realtime seismographic data [16]
could provide very ﬁne grained freshness (though from rel-
atively few data providers), or historical stock quotes could
provide coarse-grained timestamps that are veriﬁable many
years into the future.
Other Considerations.
To ensure freshness of harvested challenges, a policy needs
to prevent an attacker from predicting the future contents of
suﬃciently many of its sources. The predictability of an RSS
feed depends heavily on the type of content being served.
Content like wire service news headlines that are widely car-
ried by diﬀerent sites may be predictable minutes or hours
before appearing on a particular source. Some sites, like
the popular link aggregator Digg, allow members to inﬂu-
ence the content that appears in their feeds by voting on
candidate entries. An attacker could monitor changes in
popularity during the voting process to predict future feed
contents. Policy creators should avoid such feeds.
Posting times for many sources in our data sets were
strongly correlated with local daylight hours for the site.
This eﬀect is clearly visible in ﬁrst ﬁgure above, where the
large vertical shaded regions indicate times from 7 p.m. to
7 a.m., PST. If freshness on the order of hours is required,
policy creators might select sources from around the world,
such major newspapers from each timezone.
We normally assume that an attacking deriver cannot sit
between the veriﬁer and the Internet sources and modify the
veriﬁer’s view of their contents. Even if this is not the case,
our model can remain secure in instances when the deriver
is unable to corrupt enough sources to fool the veriﬁer. The
remaining danger can be mitigated by selecting feeds that
are served using HTTPS.
7. RELATED WORK
Our work on harvesting challenges from the Internet touches
on prior research in a number of areas. In this section we
describe relevant related work and provide a context for our
contributions.
Deriving Randomness.
The idea of extracting bits from a random source has been
around for many years. Several works have shown how to
extract randomness suitable for cryptographic applications
from non-uniform sources of entropy, such as physical events
on a local machine (see, for example,
[8] and the references
therein). Our problem is of a somewhat diﬀerent ﬂavor from
this work. We want Alice not only to be able to extract suﬃ-
cient randomness for her own use, but to be able to convince
Bob that she derived it properly and freshly. Our primary
challenges arise because the oblivious Internet sources that
we wish to use are unreliable, which means the deriver and
the veriﬁer may not have the same view of these entropy
sources. Our work focuses on the practical problems arising
from this setting; we simplify the theoretical aspect of deriv-
ing uniform randomness by modeling our hash function as a
random oracle [4].
Proofs of Work and Client Puzzles.
Dwork and Naor [10] ﬁrst proposed the idea of using proofs
of computational puzzles for mitigating spam. Adam Back
independently proposed and implemented a similar system
known as Hashcash [2]. Both the Dwork-Naor and the Back
systems fail to prevent the pre-computation of puzzles by an
attacker. The attacker can therefore begin his computation
arbitrarily long before an attack is launched.
Juels and Brainard [12] observed that allowing arbitrary
precomputation was problematic for protecting against DoS
attacks, where an adversary might build up a collection of
puzzle solutions over a long period of time and use them
over a brief period to ﬂood a server. Juels and Brainard pro-
posed a solution they called “client puzzles” where a chal-
lenger would ﬁrst send a fresh random challenge to a ma-
chine requesting a resource, and demand that the machine
do a proof of work relative to the fresh challenge. Since an
attacker does not know the challenge ahead of time, he is
forced to perform all his computation on-line. Several other
papers subsequently proposed other types of client puzzle
solutions [1, 7, 19, 20].
One issue that arises from these systems is that a chal-
lenger must issue a random challenge. Unfortunately, this
is not possible for non-interactive applications such as email.
By leveraging our tool, we can provide suitable fresh chal-
lenges in many of these settings. Waters et al. [20] provide
another motivation for our approach. In their system a client
spends a somewhat larger amount of time solving a puzzle
that he can use as a proof of work for many diﬀerent servers.
Since each server does not have the opportunity to chal-
lenge the client individually, the system requires a common
trusted source for random challenges. In their work, Waters
et al. suppose the existence of dedicated “bastion” servers
for this purpose. By adapting their system to utilize our
tool, we can potentially eliminate the need for such servers.
Borisov [5] examined the problem of deriving randomness
from a community of peer-to-peer servers. Our approach is
quite diﬀerent in that we harvest challenges from oblivious
Internet content providers and require less complex interac-
tion to synthesize our challenges. In addition, the work of
Borisov was initially inspired by our preliminary research
(as noted in the related work section of [5]).
Client puzzle solutions must be carefully designed if they
are to successfully mitigate attacks. Issues include the cost
of verifying puzzles, the discrepancy between the computa-
tional resources of portable devices and standard processors,
and the possibility that attackers will have control of large
bot-nets. Many of the works cited above address these is-
sues in particular settings. We stress that any system that
uses our tool must carefully consider issues such as these in
the context of the application it is protecting.
8. CONCLUSIONS AND FUTURE WORK
In this paper, we addressed the problem of harvesting chal-
lenges from oblivious online servers. This setting presented
us with a challenging set of issues in that we not only had
to consider the usual issues of security and robustness in our
application, but to deal with the unique problem that our
Internet sources are unaware of their role in our system.
We addressed this problem by creating a framework with
which a party can harvest a challenge from several sources
and (non-interactively) convince another party that it was
formed correctly. Our framework allows an application de-
signer to specify a ﬂexible policy that can be tailored to spe-
ciﬁc needs. We identiﬁed multiple security contexts where
our tool may be valuable, including remote storage auditing
and P2P Sybil attack prevention.
We implemented our methods in a software tool named
Combine. Combine is able to use RSS feeds, historical stock
quotes, and explicit randomness servers as sources for har-
vesting random challenges. We provided experimental data
supporting our framework’s practicality, and we built a proof-
of-concept application, Postmark, that uses Combine to cre-
ate an improved client puzzle system for email.
In the near future we plan to apply these techniques to
build auditing mechanisms for existing systems. We will
start by constructing an auditing component for a remote
storage service. We expect that this process will teach us
more about the subtleties of using harvested challenges in
a systems environment. From a broader perspective, we
will continue to search for additional applications where har-
vested challenges can be used to verify claims of distributed
systems.
Acknowledgments
We would like to thank Dan Boneh, Ed Felten, Pat Lincoln,
Chris Peikert, Amit Sahai, Shabsi Walﬁsh, and our anony-
mous reviewers for useful comments and suggestions.
This material is based upon work supported under a Na-
tional Science Foundation Graduate Research Fellowship. Any
opinions, ﬁndings, conclusions or recommendations expressed
in this publication are those of the authors and do not neces-
sarily reﬂect the views of the National Science Foundation.
9. REFERENCES
[1] Tuomas Aura, Pekka Nikander, and Jussipekka Leiwo.
DOS-resistant authentication with client puzzles. In
Security Protocols Workshop, pages 170–177, 2000.
[2] Adam Back. Hashcash – a denial of service counter-measure.
http://www.hashcash.org/hashcash.pdf, 2002.
[3] Mihir Bellare and Sara K. Miner. A forward-secure digital
signature scheme. In CRYPTO, pages 431–448, 1999.
[4] Mihir Bellare and Phillip Rogaway. Random oracles are
practical: A paradigm for designing eﬃcient protocols. In
ACM Conference on Computer and Communications
Security, pages 62–73, 1993.
[5] Nikita Borisov. Computational puzzles as sybil defenses. In
Peer-to-Peer Computing, pages 171–176, 2006.
[6] Giovanni Di Crescenzo, Richard J. Lipton, and Shabsi
Walﬁsh. Perfectly secure password protocols in the bounded
retrieval model. In TCC, pages 225–244, 2006.
[7] Drew Dean and Adam Stubbleﬁeld. Using client puzzles to
protect TLS. In 10th Usenix Security Symposium, pages
1–8, 2001.
[8] Yevgeniy Dodis, Ariel Elbaz, Roberto Oliveira, and Ran
Raz. Improved randomness extraction from two
independent sources. In APPROX-RANDOM, pages
334–344, 2004.
[9] John R. Douceur. The sybil attack. In IPTPS, pages
251–260, 2002.
[10] Cynthia Dwork and Moni Naor. Pricing via processing or
combatting junk mail. In CRYPTO, pages 139–147, 1992.
[11] J. Galvin, S. Murphy, S. Crocker, and N. Freed. Security
Multiparts for MIME: Multipart/Signed and
Multipart/Encrypted. RFC 1847 (Proposed Standard),
October 1995.
[12] Ari Juels and John G. Brainard. Client puzzles: A
cryptographic countermeasure against connection depletion
attacks. In NDSS, 1999.
[13] Martijn Koster. A standard for robot exclusion.
http://www.robotstxt.org/wc/norobots.html, 1994.
[14] R. Kotla, M. Dahlin, and L. Alvisi. Safestore: A durable
and practical storage system. In USENIX Annual Technical
Conference, 2007.
[15] National Solar Observatory/Sacramento Peak. Images and
current data. http://nsosp.nso.edu/data/.
[16] USGS Earthquake Hazards Program. Latest earthquakes in
the world - past 7 days. http://earthquake.usgs.gov/
eqcenter/recenteqsww/Quakes/quakes all.php.
[17] RSS 2.0 speciﬁcation.
http://blogs.law.harvard.edu/tech/rss, 2003.
[18] Technorati: About us. http://www.technorati.com/about/,
2007.
[19] XiaoFeng Wang and Michael K. Reiter. Defending against
denial-of-service attacks with puzzle auction. In IEEE
Symposium on Security and Privacy, pages 78–92, 2003.
[20] Brent Waters, Ari Juels, J. Alex Halderman, and
Edward W. Felten. New client puzzle outsourcing
techniques for DoS resistance. In ACM Conference on
Computer and Communications Security, pages 246–256,
2004.