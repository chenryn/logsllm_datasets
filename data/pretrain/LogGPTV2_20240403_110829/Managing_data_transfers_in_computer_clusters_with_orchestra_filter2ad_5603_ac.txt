space using non-metric multidimensional scaling [31]. Finally, we
cluster using a mixture of spherical Gaussians with ﬁxed variance
σ2, and automatically select the number of partitions based on the
Bayesian information criterion score [18]. In operational use, one
can set σ to the typical intra-rack block transfer time (in our ex-
periments, we use σ = 200 ms). With enough training data, the
procedure usually infers the exact topology and provides a similar
speedup to CornetTopology, as we show in Section 7.2.
5.4 Size-Aware Broadcast Algorithm Selection
While Cornet achieves good performance for a variety of work-
loads and topologies, it does not always provide the best perfor-
mance. For example, in our experiments we found that for a small
number of receivers, a chain distribution topology usually performs
better. In such a case, the TC can decide whether to employ one al-
gorithm or another based on the number of receivers. In general, as
new broadcast algorithms are developed, the TC can pick the best
one to match a particular data size and topology. This ability illus-
trates the advantage of our architecture, which enables the TC to
make decisions based on global information.
6 Shufﬂe Transfers
During the shufﬂe phase of a MapReduce job, each reducer is as-
signed a range of the key space produced by the mappers and must
Figure 5: A shufﬂe transfer. The two receivers (at the top) need
to fetch separate pieces of data, depicted as boxes of different
colors, from each sender.
(a) Bottleneck at a
sender
Figure 6: Different bottlenecks dictating shufﬂe performance.
(b) Bottleneck at a
receiver
(c) Bottleneck in
the network
fetch some elements from every mapper. Consequently, shufﬂe
transfers are some of the most common transfer patterns in data-
centers. Similar constructs exist in other cluster computing frame-
works, like Dryad [28], Pregel [32], and Spark [44]. In general, a
shufﬂe consists of n receivers, r1, . . . , rn, and m senders, s1, . . . , sm,
where each receiver i needs to fetch a distinct dataset dij from
sender j. Figure 5 depicts a typical shufﬂe.
Because each piece of data goes from only one sender to one re-
ceiver, unlike in broadcasts, receivers cannot improve performance
by sharing data. The main concern during a shufﬂe is, therefore, to
keep bottleneck links fully utilized (§6.1). We ﬁnd that the strat-
egy used by systems like Hadoop, where each receiver opens con-
nections to multiple random senders and rely on TCP fair sharing
among these ﬂows, is close to optimal when data sizes are bal-
anced (§6.2). There are cases with unbalanced data sizes in which
this strategy can perform 1.5× worse than optimal. We propose
an optimal algorithm called Weighted Shufﬂe Scheduling (WSS)
to address these scenarios (§6.3). However, situations where WSS
helps seem to appear rarely in typical jobs; our main contribution is
thus to provide a thorough analysis of this common transfer pattern.
6.1 Bottlenecks and Optimality in Shufﬂe Transfers
Figure 6 shows three situations where a bottleneck limits shufﬂe
performance and scheduling can have little impact on the overall
completion time. In Figure 6(a), one of the senders has more data to
send than others (e.g., a map produced more output in a MapReduce
job), so this node’s link to the network is the bottleneck. Even
with the random scheduling scheme in current systems, this link is
likely to stay fully utilized throughout the transfer, and because a
ﬁxed amount of data must ﬂow along this link to ﬁnish the shufﬂe,
the completion time of the shufﬂe will be the same regardless of the
scheduling of other ﬂows. Figure 6(b) shows an analogous situation
where a receiver is the bottleneck. Finally, in Figure 6(c), there is
a bottleneck in the network—for example, the cluster uses a tree
topology with less than full bisection bandwidth—and again the
order of data fetches will not affect the overall completion time as
long as the contended links are kept fully utilized.
These examples suggest a simple optimality criterion for shufﬂe
scheduling: an optimal shufﬂe schedule keeps at least one link fully
utilized throughout the transfer. This condition is clearly necessary,
Figure 7: Transfer times for a shufﬂe with 30 senders and 1 to
30 receivers, as a function of the number of concurrent ﬂows
(to random senders) per receiver.
because if there was a time period during which a shufﬂe schedule
kept all links less than 100% utilized, the completion time could
be lowered by slightly increasing the rate of all ﬂows during that
period. The condition is also sufﬁcient as long as unipath routing is
used—that is, the data from each sender to each receiver can only
ﬂow along one path. In this case, there is a ﬁxed amount of data,
DL, that must ﬂow along each link L, so a lower bound on the
transfer time is maxL{DL/BL}, where BL is the bandwidth of
link L. If any link is fully utilized throughout the transfer, then this
lower bound has been reached, and the schedule is optimal. Note
that under multipath routing, multiple links may need to be fully
utilized for an optimal schedule.
6.2 Load Balancing in Current Implementations
The optimality observation indicates that the links of both senders
and receivers should be kept as highly utilized as possible.
In-
deed, if the amount of data per node is balanced, which is often
the case in large MapReduce jobs simply because many tasks have
run on every node, then all of the nodes’ outgoing links can poten-
tially become bottlenecks. The biggest risk with the randomized
data fetching scheme in current systems is that some senders get
too few connections to them, underutilizing their links.5 Our main
ﬁnding is that having multiple connections per receiver drastically
reduces this risk and yields near-optimal shufﬂe times. In particu-
lar, Hadoop’s setting of 5 connections per receiver seems to work
well, although more connections can improve performance slightly.
We conducted an experiment with 30 senders and 1 to 30 re-
ceivers on Amazon EC2, using extra large nodes. Each receiver
fetched 1 GB of data in total, balanced across the senders. We
varied the number of parallel connections opened by each receiver
from 1 to 30. We plot the average transfer times for ﬁve runs in
Figure 7, with max/min error bars.
We note two trends in the data. First, using a single fetch con-
nection per receiver leads to poor performance, but transfer times
improve quickly with even two connections. Second, with enough
concurrent connections, transfer times approach 8 seconds asymp-
totically, which is a lower bound on the time we can expect for
nodes with 1 Gbps links. Indeed, with 30 connections per receiver,
the overall transfer rate per receiver was 790 Mbps for 30 receivers,
844 Mbps for 10 receivers, and 866 Mbps for 1 receiver, while the
best transfer rate we got between any two nodes in our cluster was
929 Mbps. This indicates that randomized selection of senders is
within 15% of optimal, and may be even closer because there may
5Systems like Hadoop cap the number of receiving connections per
reduce task for pragmatic reasons, such as limiting the number of
threads in the application. Having fewer connections per receiver
can also mitigate incast [41].
051015202530Number of concurrent connections per receiver05101520253035Finish time (s)1 receiver10 receivers30 receiversFigure 8: A shufﬂe conﬁguration where Weighted Shufﬂe
Scheduling outperforms fair sharing among the ﬂows.
be other trafﬁc on EC2 interfering with our job, or a topology with
less than full bisection bandwidth.
The improvement in transfer times with more connections hap-
pens for two reasons. First, with only one connection per receiver,
collisions (when two receivers pick the same sender) can only lower
performance, because some senders will be idle. In contrast, with
even 2 threads, a collision that slows down some ﬂows may speed
up others (because some senders are now sending to only one re-
ceiver). Second, with more connections per receiver, the standard
deviation of the number of ﬂows to each sender decreases relative
to its mean, reducing the effect of imbalances.
6.3 Weighted Shufﬂe Scheduling (WSS)
We now consider how to optimally schedule a shufﬂe on a given
network topology, where receiver ri needs to fetch dij units of data
from sender sj. We aim to minimize the completion time of the
shufﬂe, i.e., the time when the last receiver ﬁnishes. For simplicity,
we initially assume that the data from a sender/receiver pair can
only ﬂow along one path. Under this unipath assumption, the “fully
utilized link" condition in Section 6.1 is sufﬁcient for optimality.
We propose a simple algorithm called Weighted Shufﬂe Schedul-
ing (WSS) that achieves this condition: allocate rates to each ﬂow
using weighted fair sharing, such that the weight of the ﬂow be-
tween receiver ri and sender sj is proportional to dij. To see
why this works, consider ﬁrst a simpler scheduling scheme using
progressive ﬁlling, where the ﬂow from ri to sj is given a rate
tij = λdij for the largest feasible value of λ. Under this scheme,
all the pairwise transfers ﬁnish at the same time (because transfer
rates are proportional to data sizes for each transfer), and further-
more, at least one link is fully utilized (because we use the largest
feasible λ). Therefore, the schedule is optimal. Now, the WSS
schedule based on max-min fair sharing with these same weights
must ﬁnish at least as fast as this progressive ﬁlling schedule (be-
cause each ﬂow’s rate is at least as high as under progressive ﬁlling,
but may also be higher), so it must also be optimal.
We found that WSS can outperform current shufﬂe implementa-
tions by up to 1.5×. In current systems, the ﬂows between senders
and receivers experience unweighted fair sharing due to TCP. This
can be suboptimal when the ﬂows must transfer different amounts
of data. For example, consider the shufﬂe in Figure 8, where four
senders have one unit of data for only one receiver and s3 has two
units for both. Suppose that there is a full bisection bandwidth net-
work where each link carries one data unit per second. Under fair
sharing, each receiver starts fetching data at 1/3 units/second from
the three senders it needs data from. After 3 seconds, the receivers
exhaust the data on s1, s2, s4 and s5, and there is one unit of data
left for each receiver on s3. At this point, s3 becomes a bottleneck,
and the receivers take 2 more seconds to transfer the data off. The
transfer thus ﬁnishes in 5s. In contrast, with WSS, the receivers
would fetch data at a rate of 1/4 units/second from s1, s2, s4 and
s5 and 1/2 units/second from s3, ﬁnishing in 4s (25% faster).
This discrepancy can be increased in variants of this topology.
For example, with 100 senders for only r1, 100 senders for only
r2, and 100 data units for each receiver on a shared sender, WSS
ﬁnishes 1.495× faster than fair sharing. Nevertheless, we found
that conﬁgurations where WSS outperforms fair sharing are rare
in practice. If the amounts of data to be transferred between each
sender and each reducer are roughly balanced, then WSS reduces
to fair sharing. In addition, if there is a single bottleneck sender or
bottleneck receiver, then fair sharing will generally keep this node’s
link fully utilized, resulting in an optimal schedule.
We see that the overheads of choking/unchoking, aggressive hash-
ing, and allowing receivers to leave as soon as they are done, fail
WSS can also be extended to settings where multipath trans-
missions are allowed. In this case, we must choose transfer rates
tij between receiver i and sender j such that mini,j{tij/dij} is
maximized. This is equivalent to the Maximum Concurrent Multi-
Commodity Flow problem [39].
Implementing WSS Note that WSS requires global knowledge of
the amounts of data to transfer in order to set the weight of each
ﬂow. WSS can be implemented naturally within Orchestra by hav-
ing the TC pass these weights to the nodes. In our prototype, the
receivers open different numbers of TCP ﬂows to each sender to
match the assigned weights. We also tried a variant of where the TC
adjusts the weights of each sender/receiver pair periodically based
on the amount of data left (in case there is variability in network
performance), but found that it provided little gain.
7 Evaluation
We have evaluated Cornet, WSS, and inter-transfer scheduling in
the context of Spark and ran experiments in two environments:
Amazon’s Elastic Compute Cloud (EC2) [1] and DETERlab [5].
On EC2, we used extra-large high-memory instances, which ap-
pear to occupy whole physical nodes and had enough memory to
perform the experiments without involving disk behavior (except
for HDFS-based mechanisms). Although topology information is
not provided by EC2, our tests revealed that nodes were able to
achieve 929 Mbps in each direction and 790 Mbps during 30 nodes
all-to-all communication (Figure 7), suggesting a near-full bisec-
tion bandwidth network. The DETERlab cluster spanned 3 racks
and was used as ground-truth to verify the correctness of Cornet’s
clustering algorithm. Our experiments show the following:
• Cornet performs 4.5× better than the default Hadoop imple-
mentation of broadcast and BitTornado (§7.1), and with topol-
ogy awareness in its TC, Cornet can provide further 2× im-
provement (§7.2).
• WSS can improve shufﬂe speeds by 29% (§7.3).
• Inter-transfer scheduling can speed up high priority jobs by
1.7×, and a simple FIFO policy can improve average transfer
response times by 31% for equal sized transfers (§7.4).
• Orchestra reduced communication costs in the logistic regres-
sion and collaborative ﬁltering applications in Section 2 by up
to 3.6× and sped up jobs by up to 1.9× (§7.5).
Since data transfers act as synchronization steps in most iterative
and data-intensive frameworks, capturing the behavior of the slow-
est receiver is the most important metric for comparing alternatives.
We therefore use the completion time of the entire transfer as our
main performance metric.
7.1 Comparison of Broadcast Mechanisms
Figure 9 shows the average completion times of different broad-
cast mechanisms (Table 2) to transfer 100 MB and 1 GB of data
to multiple receivers from a single source. Error bars represent the
minimum and the maximum observed values across ﬁve runs.
r1 r2 s2 s3 s4 s1 s5 Algorithm
HDFS (R=3)
HDFS (R=10)
Chain
Tree (D=2)
BitTornado
Cornet
Theoretical
Lower Bound
Table 2: Broadcast mechanisms compared.
Description
Sender creates 3 replicas of the data in HDFS
and receivers read from them
Same as before but there are 10 replicas