instance of the Dataset is a distributed data collection. A DataFrame
is a special Dataset with column style, like a table in relational
databases. DataFrame/Dataset can be created on a variety of data
sources, such as structured data files in HDFS and RDBMS’s tables.
1) The SQL engine in Spark views a DataFrame as a table [26]. 2)
MLlib’s spark.ml realizes ML Pipelines with a uniform set of high-
level APIs built on DataFrame APIs [57]. 3) Based on DataFrame
APIs, GraphFrame enhances and extends graph algorithms from
GraphX [42, 48]. 4) Structured Streaming [25], an extension from
DStream [73], is built on DataFrame APIs.
Building data analysis application is a series of transformations
on DataFrame/Dataset — each transformation can be viewed as
one step of data operation. A SQL API swallows a data processing
fragment (possibly containing multiple steps of transformation)
and returns a new DataFrame; a DataFrame/Dataset API swallows
a single transformation step and returns a new DataFrame/Dataset.
The results in an ultimate DataFrame/Dataset can be obtained by
users through invoking an action (e,g., show to display data).
To improve efficiency, Spark uses Catalyst [26], a unified logical
plan optimizer for structured data analytics engines/APIs, to opti-
mize various data processing logics from applications. In Catalyst,
a logical plan includes various data processing logics of an applica-
tion. Through reforming logic in logical plan, Catalyst generates
optimized plan used to produce results [6, 26, 74] at execution layer.
2.2 Motivation
A medical data sharing scenario is shown in Figure 1. We will use
it as a running example in this paper.
Example 1. The medical database of Hospital R contains a doctor
table and a patient table, as shown in Figure 1. The database is
shared with other hospitals in the Regional Health Information
Organization (RHIO). Dr. Bob from Hospital R is authorized to
access the patient table without any restriction. Meanwhile, we
would like to specify that Dr. Alice from Hospital S is not authorized
to see Disease, Expense, and PatientMame columns, but she could
run statistical queries and data analytics on those columns.
Note that we currently do not have a widely adopted mecha-
nism to specify or enforce such access control intentions in the
conventional access control paradigm. Meanwhile, the big data
management and sharing platforms, such as Spark, do not support
fine-grained access control mechanisms. From the data owners’
perspective, the following access control functionality is expected.
Example 2. A sample data analytics query on the Patient ta-
ble is shown in Figure 1. It calculates the sum of “large spend-
ings” (> 6000) for each patient. According to the access control
intentions explained in Example 1, both Alice and Bob should be
able to execute this code. Bob’s answer shall include two columns
(PatientName and sum(exp1)), while Alice’s answer should not
contain the PatientName column. Meanwhile, the statistical results
in sum(exp1) should remain identical in Alice’s and Bob’s answers,
since we intend to allow Alice to run statistics on this table.
Intuitively, we would attempt to employ the conventional access
control paradigm in the scenario. The Role-Based Access Control
(RBAC) model [61] could explicitly prohibit Alice from accessing
the PatientName column, however, Alice’s query would be denied
as well, since she uses PatientName column in groupBy.
The Purpose-Based Access Control (PBAC) model [32] authorizes
queries based on pre-identified purposes. However, only abstract-
level data usage purposes, such as “analysis”, “research”, or “billing-
auditing”, are defined in PBAC. In big data applications, each data
usage purpose may cover a wide range of queries and data opera-
tions, and it is difficult to translate abstract purposes into specific
data operations or vice versa. In this example, we cannot specify
and enforce that “Alice may compute with PatientName in data
analytics queries but she cannot include data from this column in
the output”. Meanwhile, the conventional PBAC model does not
support fine-grained access control at column level, so that all the
columns are governed under the same data usage purpose. In partic-
ular, when different columns are involved differently in the query
(or data processing) logic, PBAC cannot treat them differently.
Example 3. The data processing logics for the two attributes in
the sample data analytics query discussed in Example 2 are:
Logic1 (PatientName): “patient” → “selectExpr” → “[filter]” →
“groupBy” → “[sum]” → “project”
Logic2 (Expense): “patient”→ “selectExpr”→ “filter”→ “[groupBy]”
→ “sum” → “project”
Note that “[operation]” indicates that the attribute is carried in
an operation but is not involved in computing. Logic1 is used
to compute “groupBy”, while Logic2 is used to compute “filter”
and “sum”. Ideally, a fine-grained access control mechanism would
handle these logics differently, and also handle purposes of different
operations differently. So that Alice could execute Logic1 except
the last “project” operation (Alice can compute with PatientName
but cannot see raw data). She can also compute the entire Logic2
(Alice can read and compute with Expense).
Conventional PBAC mechanisms cannot enforce different pur-
poses on different query logics, or specify fine-grained purposes
(e.g., read or compute) on different operations. Therefore, the objec-
tive of this project is to design and enforce a fine-grained purpose
aware access control model that: 1) supports attribute/column-level
authorization based on the fine-grained data processing logic for
each attribute/column; and 2) supports operation-level authoriza-
tion based on fine-grained data operation purposes.
RTSIdNameAgeRolesHospital12BobAlice2825dermatologistneurologistRSIdDiseaseExpensePatientName101102103104gastric cancercerebromaneuralgiadermatitis8000930040002000AaronBrownCamilleHannah...........................Example Tables:(a) doctor table(b) patient tableAn Example Application:patient.selectExpr("PatientName", "Expense as exp1").filter("exp1 > 6000").groupBy("PatientName").sum("exp1").select("*").show() ACSAC 2020, December 7–11, 2020, Austin, USA
Tao Xue, et al.
Figure 2: Threat model and solution overview.
3 THREAT MODEL AND SOLUTION
OVERVIEW
3.1 Threat Model
The proposed solution contains two main components: the fine-
grained, purpose-aware access control model, and the access control
enforcement mechanism. The access control model is suitable in a
wide range of big data analytics applications (the generalization of
PAAC will be addressed later), while the enforcement mechanism
is specifically developed for Spark. The primary stakeholders in
the big data sharing and analytics scenario are the data owners, the
data management and sharing platform, and the data users.
• The data owners are fully trusted. They place the data on data
management platforms, which are owned by them or trusted third
parties. Nevertheless, the owners have all the privileges on the
data. Some data objects are sensitive that they are only shared
with specified users, as shown in Figure 2 1○. The goal of this
project is to ensure data confidentiality through access control,
i.e., only authorized users could perform authorized operations on
authorized data objects. The data owners will utilize the access
control model supported by the data management and sharing
platform to specify access control policies to be enforced by the
platform.
• In this project, the data management and sharing platform, in-
cluding software and hardware, is assumed to be secure and fully
trusted by both the data owners and the users. It is expected to
faithfully and correctly enforce access control policies.
In real-world applications, the platform may be untrusted and/or
compromised so that data confidentiality is jeopardized. For in-
stance, the cloud or the database may not be trusted with plaintext
data, system vulnerabilities may cause data/privacy leakage, the
implementation of access control enforcement may be buggy, etc.
Significant amount of research efforts have been devoted to these
topics. Cloud, OS, big data platform and software security issues
are considered outside of the scope of this paper.
• From access control perspective, the users are correctly and se-
curely authenticated by the platform. Their roles/attributes are prop-
erly managed. They only use the designated data analytics engines
or APIs to access data, i.e., they cannot bypass the access control
mechanism to directly read from the underlying data sources using
python, R, java, scala APIs, or the Resilient Distributed Datasets
APIs [72]. Meanwhile, the users are assumed to be curious–if the ac-
cess control policies or enforcement mechanisms accidentally give
them access to an data object that they are not supposed to, they will
utilize the access rights that violates the data owners’ access control
intentions. Last, the users will not attack the software/hardware of
the platform (in this paper, Spark).
3.2 Solution Overview
In this paper, we present the GuardSpark++ solution, which con-
tains a purpose-aware access control (PAAC) model and a PAAC
enforcement mechanism for Apache Spark.
The PAAC model introduces the concepts of data processing
purpose and data operation purpose to the conventional PBAC
model. The data operation purpose (DOP) defines the purpose of
each data operation in data processing logic, e.g., each step in the
query logics shown in Example 3. Therefore, a sequence of DOPs
are extracted from each data processing logic. The data processing
purpose of the data processing logic is identified as the most signifi-
cant DOP within the sequence. A PAAC policy specifies acceptable
data processing purposes from a subject (user or application) on
a data object. We further develop a purpose analysis algorithm to
examine the logical query plans and extract data operation and
processing purposes automatically.
Next, we develop a PAAC enforcement mechanism to be hosted
in the Catalyst optimizer of Apache Spark. As shown in Figure 2 2○,
the original optimization pipeline of Catalyst includes four stages:
transformation, analysis, optimization, and materialization. We add
a new stage, named secure logical plan generation, between the anal-
ysis and optimization stages. The added stage enforces PAAC by
comparing intended purposes extracted from logical plans against
all allowed purposes. In this way, GuardSpark++ transforms ana-
lyzed logical plans into secure logical plans that comply with PAAC
policies specified by data owners. Subsequently, the secure logical
plans are further optimized to eliminate any potential overhead
induced by access control. GuardSpark++ also provides pre-defined
secure logic templates of secure logical plans for the Structured
Streaming engine, which originally utilizes analyzed logical plans
as pre-defined logic templates [21, 25].
TransformationAnalysisOptimizationMaterializationSecure Logical Plan GenerationApplication LayerExecution LayerOptimization Layer (Catalyst)data usersSpark(GuardSpark++)data user Adata user Bdata ownershared data with data user Bshared data with data user Asensitive data① ② Model EnforcementGuardSpark++: 1) Purpose-Aware Access Control Model  2) Model EnforcementCatalogData Management and Sharing Platform GuardSpark++: Fine-Grained Purpose-Aware Access Control for Secure Data Sharing and Analysis in Spark
ACSAC 2020, December 7–11, 2020, Austin, USA
4 PURPOSE-AWARE ACCESS CONTROL
In this section, we first formally present the fine-grained purpose-
ware access control (PAAC) model, and then describe a purpose
analysis algorithm, which will be essential in PAAC enforcement.
4.1 The Purpose-Aware Access Control Model
In general, an access control model contains the following core
components: {subject, action, object, [context], “allow|deny”}. Differ-
ent access control models specify these component differently, e.g.,
the ABAC policies combine attributes to specify authorizations.
In the proposed Purpose-Aware Access Control (PAAC) model,
the object is any data object that could be referenced in a structured
data model. In big data processing platforms such as Spark, it could
be a data object from any structured data source that feeds data to the
platform, including 1) tables in relational databases, 2) structured
files treated as tables in distributed storage systems (e.g., HDFS)
or local file systems, 3) streaming data treated as a table, and 4)
other data with explicitly defined column structures. Fine-grained
protection of data objects is supported at column-level, row-level,
and cell-level. Data owners could define a protected data object
using any attribute, e.g., owner or source.
Example 4. Hospital R hosts the medical database shown in Fig-
ure 1 in a MySQL DBMS identified as 196.168.12.110:3306. The data
object Expense attribute of the patient table is referred to as {ta-
ble:patient, column:Expense, ★}, in which ★ denotes (owner:R and
source:MySQL(196.168.12.110:3306:medical)). We denote this data
object as O hereafter. In the same way, the second record is re-
ferred to as {table:patient, columns:(Id, Disease, Expense, Patient-
Name), boolean:Id=102, ★}. Finally, Brown’s expense is denoted as
{table:patient, column:Expense, boolean:PatientName=‘Brown’, ★}.
Now we formally define the data processing purpose and the data
operation purpose. When a user or an application issues a query or
a data analytics algorithm, the query/algorithm will be internally
translated to an executable data processing plan (query plan). In
practice, the data processing logic is usually organized in a tree
structure (i.e., query tree) in the query processor, in which leaf nodes
are data objects and internal nodes are operations. A leaf-to-root
path represents a data processing logic of the specific data object
at the leaf node 1. The ultimate objective of this data processing
logic is the data processing purpose for this data object. Note that
a data object may have multiple data processing purposes, when it
is involved in different leaf nodes and processed differently in the
query plan.
Definition 1 (Data Processing Purpose). The data processing pur-
pose indicates the ultimate purpose of a data processing logic for a
data object in a big data application.
While we may ask the user or application to manually indicate
a data processing purpose along with the query or data analytics
algorithm, however, we cannot trust the self-claimed purpose and
use it to enforce access control. In practice, the data processing
purpose needs to be discovered at the data management platform
based on the query/algorithm content. Meanwhile, we also observe
1We would like to note that a data processing logic in the assistance role may end
as an operand in an internal node and not carried to the final output. They are not
discussed here but they are properly handled in access control enforcement.
that it is difficult to directly identify the data processing purpose
due to the complexity of the query or the algorithm. Therefore, we
further decompose the data processing purpose into fine-grained
data operation purposes.
Definition 2 (Data Operation Purpose). A data operation purpose
is the purpose(s) of an operation in the data processing logic of a
query or a data analytics algorithm.