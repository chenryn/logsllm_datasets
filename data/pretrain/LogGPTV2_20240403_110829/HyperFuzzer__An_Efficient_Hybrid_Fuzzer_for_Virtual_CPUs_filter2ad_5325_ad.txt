a very limited fashion. We implement per-instruction callbacks in
Bochs to record both the control and data flow of the hypervisor’s
execution.
The fuzzing loop of the baseline system is similar to HyperFuzzer
except that it runs inside an emulator. We run an agent inside the
management VM to load a fuzzing input and trigger the hypervi-
sor’s execution. The agent requests a fuzzing input by executing
CPUID with a special leaf value, and Bochs then copies the fuzzing
input to the agent’s memory space. The agent triggers the hyper-
visor’s execution in a way similar to HyperFuzzer by launching a
testing VM and setting up its state based on the fuzzing input.
We optimize the performance of the baseline system by forking
the Bochs emulator process when it fuzzes a new input. This enables
it to quickly rollback the entire emulated system including the
hypervisor and the agent. This optimization improves the fuzzing
throughput of the baseline system by two orders of magnitude
compared to a naive approach that restarts the Bochs emulator
from an on-disk snapshot every single time.
6.2 Efficiency
We evaluate HyperFuzzer’s efficiency by comparing it with the
Bochs-based baseline system in two experiments. In the first exper-
iment, we measure the average run time for performing a single
test (without any mutation) or a single symbolic execution (with
constraint solving) in HyperFuzzer and the baseline system. For this
experiment, we integrate the Triton symbolic execution engine [43]
into the Bochs emulator. In the second experiment, we measure the
throughput of hybrid, graybox and whitebox fuzzing.
6.2.1 Run Time. We measure the average run time of a single test
or symbolic execution for HyperFuzzer and the baseline system
on the expanded fuzzing set and show the result in Table 4. The
average run time for a single test is less than 0.5ms for HyperFuzzer,
while the baseline system is 3 orders of magnitude slower. The
Bochs PC Emulator (parent process)Bochs PC Emulator (forked upon fuzzing an input)HypervisorTesting VMVM state(restored by Agent VM) Agent VMEmulated SystemExecution RecorderPer-instruction callbackFuzzing InputsInput FetcherRunCPUID(Magic)MutationSession 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3736.3 Precision
We evaluate NSE’s precision by measuring what fraction of input-
dependent conditional branches it can correctly identify and then
flip. The former defines completeness, while the latter is measured by
counting divergences. A divergence occurs whenever a new input
generated to exercise a specific program path actually takes an
unintended path.
6.3.1 Completeness. We use the expanded fuzzing set to measure
completeness. We pick this data set to reduce the bias of using NSE
generated inputs to evaluate its effectiveness because this data set
contains fuzzing inputs generated by graybox fuzzing as well. For
each fuzzing input, we record the hypervisor’s full execution trace
(control+data) by using the Bochs-based baseline system. We then
run our core symbolic execution engine over the full execution trace
in order to compute a baseline set of input-dependent conditional
branches and new fuzzing inputs. To run NSE, we extract the control
flow from full execution traces as if it were logged by Intel PT. Then
to measure completeness, we compare the baseline set of the input-
dependent conditional branches and the new fuzzing inputs with
those computed by NSE based on the extracted control-flow-only
trace.
In other words, in order to isolate the effects of full (control+data)
versus control-flow-only execution traces, we run our core symbolic
execution engine on both Bochs-based traces and Intel PT traces
to measure completeness. In contrast, we deliberately do not use
Triton on the Bochs-based traces since such a comparison would
then obfuscate the results with other factors such as the quality
of two different symbolic execution engines, constraint generators
and solvers.
We show the experimental results in Table 6. We can see that NSE
can identify at least 98.1% of input-dependent conditional branches
and generate at least 96.8% of new fuzzing inputs in the baseline set
that are obtained from full execution traces. Note that we count each
occurrence of an input-dependent branch instruction in the trace
separately. The common reason for NSE to miss an input-dependent
conditional branch is that the hypervisor mixes the input with its
internal state, which is unknown to NSE. When this happens, NSE
sets the new value to unknown and stops tracking its symbolic
expression. This under-approximation of the symbolic state is also
the reason why NSE never misidentifies an input-dependent branch
that is not in the baseline set (no false positives, by construction).
Furthermore, the main reason for NSE to miss a new fuzzing input
is that NSE fails to identify its corresponding branch as an input-
dependent branch.
6.3.2 Divergences. In this section, we describe our experimental
results on divergences by measuring what fraction of inputs gener-
ated by NSE can actually flip its targeted branch. In this experiment,
we run whitebox fuzzing on the expanded fuzzing set. For every
newly generated fuzzing input, we test it and save its control flow.
Then we compare it with the control flow of its “parent” input (the
one we used to generate the new fuzzing input) to check if it flips
the targeted branch successfully.
We show the experimental results in Figure 8. We run the exper-
iment in three setups to evaluate the effectiveness of mitigations
Figure 8: HyperFuzzer’s divergence rate on newly generated
fuzzing inputs. Success means the newly generated fuzzing
input successfully flips its targeted branch. Rejected means
it is rejected by hardware checks. Diverged means it fails to
flip its targeted branch. For each virtualization interface, we
progressively show the results from left to right: (1) without
unrelated constraint elimination and bit-wise symbolic vari-
ables, (2) with unrelated constraint elimination but without
bit-wise symbolic variables, and (3) with both.
described in §4.2.2 for hidden hardware checks: (1) without unre-
lated constraint elimination and bit-wise symbolic variables, (2)
with unrelated constraint elimination but without bit-wise symbolic
variables, and (3) with both. We present their results from left to
right for each virtualization interface, respectively. For each setup,
we measure the percentages of newly generated fuzzing inputs
that flip the input-dependent branches as intended (Success), get
rejected due to violating hardware checks (Rejected), or fail to flip
the intended branches (Diverged).
The results show that the two techniques for minimizing changes
to the original fuzzing input are effective in reducing hardware re-
jections. For task switch, 100% of newly generated fuzzing inputs
are rejected by the hardware when neither mitigation is enabled.
This is because Intel CPUs perform 11 different checks before pass-
ing the control to the hypervisor for emulation [32, Chap. 25.4.2].
Another interesting observation here is that the two techniques
help reduce both hardware rejections and software divergences.
In our experiment, unrelated constraint elimination significantly
reduces software divergences when fuzzing hypercalls. The root
cause of these divergences is related to an implementation detail
of NSE. Specifically, NSE always uses a concrete memory address
when emulating a memory operation even if its address is symbolic
(i.e., NSE does not support symbolic pointers [16, 23]). This can
cause inconsistencies when the constraint solver assigns a new
value to a symbolic variable previously used as a memory address.
In the particular case of hypercalls, the hypercall number is first
range-checked by the hypervisor and then used as an index to fetch
the corresponding hypercall handler from a function pointer table.
When flipping a conditional branch inside the hypercall handler,
HypercallTask SwitchAPIC Emu.MSR Emu.20%40%60%80%100%SuccessRejectedDivergedSession 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea374(a) Hypercalls
(b) Task Switch
(c) APIC Emu.
(d) MSR Emu.
Figure 9: Edge coverage of hybrid, graybox and whitebox fuzzing. X axis spans the time range from 0 to 120 minutes. Y axis
represents the aggregated number of unique control-flow edges covered by each fuzzing configuration.
NSE can assign a different hypercall number as long as it satisfies
the aforementioned range check if the range check is included in
the path constraint. This eventually leads to a divergence as the
subsequent test can no longer reach the same hypercall handler, not
to mention the conditional branch it intends to flip. Both eliminating
unrelated constraints and using bit-wise symbolic variables can
help exclude the range check from the path constraint. It is obvious
that eliminating unrelated constraints can do it when the range
check is indeed unrelated to the branch we try to flip. Using bit-
wise symbolic variables also helps because the hypercall number is
specified as a bit field and mixed with other bit-level flags. When
symbolic variables are bit-wise, we avoid including the hypercall
number in the path constraint for a branch that checks the other
bit-level flags.
Divergences remain after we apply both techniques. The main
reason for these divergences is that NSE misses some input-dependent
conditional branches. When such a branch is missed, its branch
constraint is also missed in the path constraint when NSE tries to
flip a subsequent branch. Then the generated new input may violate
the missing branch’s constraint and thus lead to a divergence.
6.4 Coverage
In this section, we report our experiments on coverage. The goal
is to check if, and by how much, HyperFuzzer achieves a better
coverage than graybox-only or whitebox-only fuzzing. In our exper-
iments, we run the graybox, whitebox and hybrid fuzzing separately
on the initial fuzzing input set for 120 minutes. Then we count the
aggregated number of unique control-flow edges covered by each
fuzzing configuration. To measure the coverage for each virtualiza-
tion interface, we exclude the inputs and their coverage if they do
not trigger the targeted interface. Note that the expanded fuzzing
set described in Table 3 is constructed based on “interesting” fuzzing
inputs (those that trigger new code coverage) generated from this
experiment.
Our experimental results on edge coverage are shown in Figure 9.
We can see that hybrid fuzzing outperforms graybox-only and
whitebox-only fuzzing. The coverage difference between graybox
and hybrid fuzzing is small for Task Switch because the code for
Task Switch is relatively simpler than the other three virtualization
interfaces. The coverage jumps in hybrid fuzzing for hypercalls and
MemSafe
IntOvf
Logical MemLeak
Hypercalls
Task Switch
APIC Emu.
MSR Emu.
3
0
0
0
1
0
0
0
0
1
3
2
1
0
0
0
Table 7: Summary of real-world hypervisor bugs found by
HyperFuzzer.
APIC emulation are likely because random mutation just triggered
some new code paths.
Whitebox-only fuzzing has the worst coverage in our experi-
ments. We analyze the code paths covered by graybox fuzzing but
not by whitebox fuzzing and find that there are two main reasons
for whitebox fuzzing to miss them. First, NSE does not support
symbolic pointers. For example, some guest VM state can be used
as an index to a function pointer table. Without symbolic pointers,
NSE will not be able to generate a new fuzzing input with a dif-
ferent index to the function pointer table. However, it is possible
for random mutation to modify the index and lead to a different
function. Second, the hardware checks are invisible to whitebox
fuzzing. When whitebox fuzzing tries to flip a branch, the gener-
ated new input may violate a hidden hardware check. When this
happens, whitebox fuzzing will result in a hardware rejection and
make the targeted path unreachable. We evaluate the prevalence of
hardware rejections in §6.3.2. On the other hand, it is possible for
random mutation to generate a new input that happens to flip the
branch. Therefore, we believe hybrid fuzzing is the right approach
for hypervisors.
6.5 Bug Analysis
HyperFuzzer has found 11 previously unknown virtual CPU bugs
in the Hyper-V hypervisor as summarized in Table 7. Out of the 11
bugs, 6 are security critical because a malicious guest VM can exploit
them to compromise the hypervisor. The other 5 bugs are less
critical because they only affect the testing VM itself. HyperFuzzer
detects the 11 bugs based on signals like assertion violations (e.g.,
logical bugs) and crashes (e.g., memory safety). Most of the bugs
reported here were found within hours of fuzzing.
020406080100120010002000300040005000600070008000HybridGrayboxWhitebox020406080100120100012001400160018002000HybridGrayboxWhitebox02040608010012010001500200025003000350040004500HybridGrayboxWhitebox0204060801001201000200030004000HybridGrayboxWhiteboxSession 2A: Fuzzing and Bug Finding CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea375We manually analyze the 11 bugs found by HyperFuzzer and
find that all the 6 logical bugs require symbolic-execution-based
input generation.
to be triggered.
• Task Switch (1 bug): This bug requires a special task segment
• APIC Emulation (3 bugs): Two bugs require a specific guest
mode (e.g., 16-bit protected mode) to be triggered. The third
bug requires a special opcode (RDTSC) to be triggered.
• MSR Emulation (2 bugs): Both bugs require a special MSR
index and value to be triggered.
The final VM states triggering these bugs were found thanks
to NSE combined with coverage-guided random mutation. The
latter helps explore trivial branches, while the former is effective
for unlocking those hard-to-reach branches. For instance, NSE is
able to go through the switch cases in the hypervisor’s instruction
emulation code to generate the RDTSC opcode required for one of
the APIC emulation related bugs.
7 LIMITATIONS
HyperFuzzer is currently limited to hypervisors running on the
Intel VMX platform due to its dependency on Intel PT. This makes
it unable to reach a hypervisor’s platform-dependent code. For
example, certain MSRs are only available on AMD processors, and
bugs in their emulation code will not be captured by HyperFuzzer.
HyperFuzzer does not support multiple virtual CPUs for white-
box fuzzing, which limits its ability to find race condition bugs
in the hypervisor. Such support requires new techniques for both
control-flow tracing and dynamic symbolic execution. We leave it
for future work.
We have demonstrated NSE’s high effectiveness in performing
dynamic symbolic execution over a control-flow trace (§6.3) of
Hyper-V hypervisor’s virtual CPU. However, it may not be univer-
sally applicable to all targets. We believe that NSE works well for
targets whose execution is driven by the inputs, such as an image
parser or the virtual CPUs focused by this work. But NSE may
not work well for programs whose input can be tangled with the
internal state, which is unknown when only a control-flow trace
is captured. For example, the behavior of a stateful web server de-
pends on both the incoming input and its current state. A potential
research direction is to systematically identify the critical internal
data to log for dynamic symbolic execution.
8 RELATED WORK
HyperFuzzer is the first efficient hybrid fuzzer for virtual CPUs.
Its main difference from previous work is that it does not rely on
instrumentation or emulation to record the full program execu-
tion for symbolic execution. Instead, it only records the program’s
control flow by using commodity hardware tracing and is able to
perform precise dynamic symbolic execution on top of the control
flow trace. In this section, we discuss previous work on hybrid
fuzzing, hypervisor testing, and hardware tracing.
8.1 Hybrid Fuzzing
Fuzzing means automatic test generation and execution with the
goal of finding security vulnerabilities [28, 49].
Blackbox random fuzzing is the simplest form of fuzzing: it either
mutates well-formed application inputs or directly generates inputs,
and then tests the application with these new inputs [25]. Blackbox
random fuzzing provides a simple fuzzing baseline. It is effective
for some simple targets [42], but its effectiveness is limited: the
probability of generating new interesting inputs is low [49].
Whitebox fuzzing [30] combines fuzzing with dynamic test gen-
eration [29], which consists of symbolically executing the program
under test dynamically, gathering constraints on inputs from condi-
tional branches encountered along the execution, and then negating
and solving these constraints with a constraint solver. Solutions of
satisfiable constraints are mapped to new inputs that exercise dif-
ferent program execution paths. Whitebox fuzzing can typically
generate inputs that exercise more code than other approaches
because it is more precise [27], which explains why it has been im-
plemented in several popular open-source tools [15, 19]. However,
it is more complex to engineer and its throughput is lower than
blackbox fuzzing.
Graybox fuzzing extends blackbox fuzzing with whitebox fuzzing
techniques. It approximates whitebox fuzzing by eliminating some
of its components to reduce engineering cost and complexity while
retaining some of its intelligence. AFL [1] is a popular open-source
fuzzer which extends random fuzzing with code-coverage-based
search heuristics, but without any symbolic execution, constraint