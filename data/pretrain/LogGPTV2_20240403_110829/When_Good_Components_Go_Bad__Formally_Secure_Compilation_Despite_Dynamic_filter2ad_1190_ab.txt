(3) How can we achieve high assurance that the resulting compilation
chain is indeed secure? Answer: We show that formal verification
(§4.3) and property-based testing (§4.4) can be successfully used
together for this in a proof assistant like Coq.
We close with related (§5) and future (§6) work. The appen-
dix presents omitted details. Our Coq development is available at
https://github.com/secure-compilation/when-good-components-go-bad/
A previous version of this work has been published in confer-
ence proceedings [6]. The current version has been extended to
include the details of the proofs, and, compared to the conference
version, more proofs are now fully mechanized in Coq, and our
proof architecture has also been significantly simplified.
2 RSCC BY EXAMPLE
We begin by an overview of compartmentalizing compilation chains,
our attacker model, and how viewing this model as a dynamic
compromise game leads to intuitive principles for security analysis.
We need not be very precise, here, about the details of the source
language; we just assume that it is equipped with some compart-
mentalization facility [35, 81] that allows programmers to break
up security-critical applications into mutually distrustful compo-
nents that have clearly specified privileges and can only interact
via well-defined interfaces. In fact we assume that the interface of
each component gives a precise description of its privilege. The
notions of component and interface that we use for defining the
secure compilation criteria in §3 are quite generic: interfaces can
include any requirements that can be enforced on components,
including type signatures, lists of allowed system calls, or more
detailed access-control specifications describing legal parameters
to cross-component calls (e.g., ACLs for operations on files). We
assume that the division of an application into components and the
interfaces of those components are statically determined and fixed.
For the illustrative language of §4, we will use a simple setup in
which components don’t directly share state, interfaces just list the
procedures that each component provides and those that it expects
to be present in its context, and the only thing one component can
do to another one is to call procedures allowed by their interfaces.
The goal of a compartmentalizing compilation chain is to ensure
that components interact according to their interfaces even in the
presence of undefined behavior. Our secure compilation criterion-
does not fix a specific mechanism for achieving this: responsibility
can be divided among the different parts of the compilation chain,
such as the compiler, linker, loader, runtime system, system soft-
ware, and hardware. In §4 we study a compilation chain with two
alternative back ends—one using software fault isolation and one
using tag-based reference monitoring for compartmentalization.
What a compromised component can still do in this model is to use
its access to other components, as allowed by its interface, to either
trick them into misusing their own privileges (i.e., confused deputy
attacks) or even compromise them as well (e.g., by sending them
malformed inputs that trigger control-hijacking attacks exploiting
undefined behaviors in their code).
We model input and output as interaction with a designated
environment component E that is given an interface but no imple-
mentation. When invoked, environment functions are assumed to
immediately return a non-deterministically chosen value [58]. In
terms of security, the environment is thus the initial source of arbi-
trary, possibly malformed, inputs that can exploit buffer overflows
and other vulnerabilities to compromise other components.
As we argued in the introduction, it is often unrealistic to assume
that we know in advance which components will be compromised
and which ones will not. This motivates our model of dynamic
compromise, in which each component receives secure compila-
tion guarantees until it becomes compromised by encountering an
undefined behavior, causing it to start attacking the remaining un-
compromised components. In contrast to earlier static-compromise
models [45], a component only loses guarantees in our model after
an attacker discovers and manages to exploit a vulnerability, by
sending it inputs that lead to an undefined behavior. The mere ex-
istence of vulnerabilities—undefined behaviors that can be reached
after some sequence of inputs—is not enough for the component to
be considered compromised.
This model allows developers to reason informally about various
compromise scenarios and their impact on the security of the whole
application [35]. If the consequences of some plausible compromise
seem too serious, developers can further reduce or separate privi-
lege by narrowing interfaces or splitting components, or they can
make components more defensive by validating their inputs.
As a first running example, consider the idealized application in
Figure 1. It defines three components (C0, C1, and C2) that interact
with the environment E via input (E.read) and output (E.write)
operations. Component C1 defines a main() procedure, which first
invokes C2.init() and then reads a request x from the environment
(e.g., coming from some remote client), parses it by calling an inter-
nal procedure to obtain y, and then invokes C2.process(x,y). This,
in turn, calls C2.prepare() and C2.handle(y), obtaining some data
that it validates using C0.valid and, if this succeeds, writes data
together with the original request x to the environment.
Suppose we would like to establish two properties:
(S1) any call E.write() happens as a response to a pre-
vious E.read() call by C1 obtaining the request x; and
(S2) the application only writes valid data (i.e., data for which
C0.valid returns true).
These can be shown to hold of executions that do not encounter
undefined behavior simply by analyzing the control flow. But what
if undefined behavior does occur? Suppose that we can rule out this
possibility—by auditing, testing, or formal verification—for some
parts of the code, but we are unsure about three subroutines:
3
When Good Components Go Bad
Abate et al.
import E . read , C2 . init , C2 . process ;
main ( )
{
component C0 {
export valid ;
valid ( data )
{
}
component C1 {
. . .
}
C2 . init ( ) ;
x := E . read ( ) ;
y := C1 . parse ( x ) ;
C2 . process ( x , y ) ;
}
parse ( x )
{
. . .
}
}
component C2 {
/ / (V1 )
can y i e l d Undef
f o r
some x
component C0 {
import E . read , E . write , C2 . init , C1 . parse , C2 . process ;
main ( )
{
C2 . init ( ) ;
x := E . read ( ) ;
y := C1 . parse ( x ) ;
data := C2 . process ( y ) ;
if C0 . valid ( data ) then E . write ( )
. . .
}
}
/ / (V1 )
can y i e l d Undef
f o r
some x
}
valid ( data )
{
}
component C1 {
export parse ;
. . .
parse ( x )
{
}
component C2 {
export init , process ;
init ( )
process ( y )
. . .
{
{
}
}
prepare ( )
handle ( y )
{
{
. . .
. . .
}
}
import E . write , C0 . valid ;
export init , process ;
init ( )
. . .
process ( x , y )
{
}
{
C2 . prepare ( ) ;
/ / (V2 )
data := C2 . handle ( y ) ; / / (V3 )
if C0 . valid ( data ) then E . write ( )
can y i e l d Undef
can y i e l d Undef
}
prepare ( )
handle ( y )
}
{
{
. . .
. . .
}
}
}
i f n o t
f o r
some y
i n i t i a l i z e d
C2 . prepare ( ) ;
return C2 . handle ( y ) ;
/ / (V2 )
/ / (V3 )
can y i e l d Undef
can y i e l d Undef
i f n o t
f o r
some y
i n i t i a l i z e d
Figure 1: Pseudocode of compartmentalized application
Figure 2: More secure refactoring of the application
(V1) C1.parse(x) performs complex array computations, and we
do not know if it is immune to buffer overflows for all x;
(V2) C2.prepare() is intended to be called only if C2.init() has
been called beforehand to set up a shared data structure;
otherwise, it might dereference an undefined pointer;
(V3) C2.handle(y) might cause integer overflow on some inputs.
If the attacker finds an input that causes the undefined behavior
in V1 to occur, then C1 can get compromised and call C2.process(x,y)
with values of x that it hasn’t received from the environment, thus
invalidating S1. Nevertheless, if no other undefined behavior is en-
countered during the execution, this attack cannot have any effect
on the code run by C2, so S2 remains true.
Now consider the possible undefined behavior from V2. If C1
is not compromised, this undefined behavior cannot occur, since
C2.init() will be called before C2.prepare(). Moreover, this unde-
fined behavior cannot occur even if C1 is compromised by the un-
defined behavior in V1, because that can only occur after C2.init()
has been called. Hence V1 and V2 together are no worse than V1
alone, and property S2 remains true. Inferring this crucially depends
on our model of dynamic compromise, in which C1 can be treated
as honest and gets guarantees until it encounters undefined behav-
ior. If instead we were only allowed to reason about C1’s ability to
do damage based on its interface, as would happen in a model of
static compromise [45], we wouldn’t be able to conclude that C2
cannot be compromised: an arbitrary component with the same
interface as C1 could indeed compromise C2 by calling C2.process
before C2.init. Finally, if execution encounters undefined behavior
in V3, then C2 can get compromised irrespective of whether C1 is
compromised beforehand, invalidating both S1 and S2.
Though we have not yet made it formal, this security analysis
already identifies C2 as a single point of failure for both desired
properties of our system. This suggests several ways the program
could be improved: The code in C2.handle could be hardened to
reduce its chances of encountering undefined behavior, e.g. by doing
better input validation. Or C1 could validate the values it sends to
C2.process, so that an attacker would have to compromise both C1
and C2 to break the validity of writes. To ensure the correspondence
of reads and writes despite the compromise of C1, we could make
C2 read the request values directly from E, instead of via C1.
To achieve the best security though, we can refactor so that the
read and write privileges are isolated in C0, which performs no com-
plex data processing and thus is a lot less likely to be compromised
by undefined behavior (Figure 2). In this variant, C0 reads a request,
calls C1.parse on this request, passes the result to C2.process, val-
idates the data C2 returns and then writes it out. This way both our
desired properties hold even if both C1 and C2 are compromised,
since now the core application logic and privileges have been com-
pletely separated from the dangerous data processing operations
that could cause vulnerabilities.
Let’s begin making all this a bit more formal. The first step is to
make the security goals of our example application more precise. We
do this in terms of execution traces that are built from events such
as cross-component calls and returns. The two intuitive properties
from our example can be phrased in terms of traces as follows: If
E.write() appears in an execution trace, then
(S1) E.read was called previously and returned x, and
(S2) C0.valid(data) was called previously and returned true.
The refactored application in Figure 2 achieves both properties
despite the compromise of both C1 via V1 and C2 via V3, but, for
the first variant in Figure 1 the properties need to be weakened as
follows: If E.write() appears in an execution trace then
(W1) E.read previously returned x or E.read previously returned
an x' that can cause undefined behavior in C1.parse(x') or
C2.process(x,y) was called previously with a y that can
cause undefined behavior in C2.handle(y), and
(W2) C0.valid(data) was called previously and returned true
or C2.process(x,y) was called previously with a y that can
cause undefined behavior in C2.handle(y).
While these properties are significantly weaker (and harder to un-
derstand), they are still not trivial; in particular, they still tell us
4
When Good Components Go Bad