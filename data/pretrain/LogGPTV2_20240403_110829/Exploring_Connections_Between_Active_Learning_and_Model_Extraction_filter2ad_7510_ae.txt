48
166
94.5
96.4
98.2
94.8
Table 3: Extraction of a kernel SVM model. Comparison of the query
complexity and test accuracy (in %) obtained running Tramèr et al. adaptive
retraining vs. extended adaptive retraining.
Dataset
Oracle
Accuracy
Path Finding
IWAL
Queries
Queries Accuracy
Adult
Steak
Iris
GSShappiness
81.2
52.1
86.8
79
18323
5205
246
18907
244188
1334
361
254892
80.2
73.1
89.4
79.3
Table 4: Extraction of a decision tree model. Comparison of the query
complexity and test accuracy (in %) obtained by running path ﬁnding (Tramèr
et al.) vs. IWAL algorithm. The test accuracy (in %) of the server-hosted
oracle is presented as a baseline.
that we train locally8, eliminating redundant queries to the
oracle. To compare the efﬁciency of our algorithm, we re-
execute the adaptive retraining procedure, and present our
results in Table 3.
It is clear that our approach is more query efﬁcient in com-
parison to Tramèr et al. (between 5×-224×), with compara-
ble test accuracy. These advantages stem from (a) using a
more informative metric of uncertainty than the distance from
the decision boundary, and (b) querying labels of only those
points which the local model is uncertain about.
Q2. Decision Trees: Tramèr et al. propose a path ﬁnding
algorithm to determine the structure of the server-hosted de-
cision tree. They rely on the server’s response to incomplete
queries, and the addition of node identiﬁers to the generated
outputs to recreate the tree. From our analysis presented in
Table 1such ﬂexibility is not readily available in most MLaaS
providers. As discussed earlier (refer § 4.2), we utilize the
IWAL algorithm proposed by Beygelzimer et al. [11] that
iteratively reﬁnes a learned hypothesis. It is important to note
that the IWAL algorithm is more general, and does not rely
on the information needed by the path ﬁnding algorithm. We
present the results of extraction using the IWAL algorithm
below in Table 4.
In each iteration, the algorithm learns a new hypothesis, but
the efﬁciency of the approach relies on the hypothesis used
preceding the ﬁrst iteration. To this end, we generate inputs
uniformly at random. Note that in such a uniform query gener-
ation scenario, we rely on zero auxiliary information. We can
see that while the number of queries required to launch such
extraction attacks is greater than in the approach proposed
8such a local model is seeded with uniformly random points labeled by
the oracle
by Tramèr et al., such an approach obtains comparable test
error to the oracle. While the authors rely on certain distri-
butional assumptions to prove a label complexity result, we
empirically observe success using the uniform strategy. Such
an approach is truly powerful; it makes limited assumptions
about the MLaaS provider and any prior knowledge.
7 Discussion
We begin our discussion by highlighting algorithms an adver-
sary could use if the assumptions made about the operational
ecosystem are relaxed. Then, we discuss strategies that can
potentially be used to make the process of extraction more
difﬁcult, and shortcomings in our approach.
7.1 Varying the Adversary’s Capabilities
The operational ecosystem in this work is one where the ad-
versary is able to synthesize data-points de novo to extract
a model through oracle access. In this section, we discuss
other algorithms an adversary could use if this assumption
is relaxed. We begin by discussing other models an adver-
sary can learn in the query synthesis regime, and move on to
discussing algorithms in other approaches.
Equivalence queries. In her seminal work, Angluin [4] pro-
poses a learning algorithm, L∗, to correctly learn a regular set
from any minimally adequate teacher, in polynomial time. For
this to work, however, equivalence queries are also needed
along with membership queries. Should MLaaS servers pro-
vide responses to such equivalence queries, different extrac-
tion attacks could be devised. To learn linear decision bound-
aries, Wang et al. [59] ﬁrst synthesize an instance close to the
decision boundary using labeled data, and then select the real
instance closest to the synthesized one as a query. Similarly,
Awasthi et al. [7] study learning algorithms that make queries
that are close to examples generated from the data distribution.
These attacks require the adversary to have access to some
subset of the original training data. In other domains, program
synthesis using input-output example pairs (e.g.,[25, 58]) also
follows a similar principle.
If the adversary had access to a subset of the training data,
or had prior knowledge of the distribution from which this
data was drawn from, it could launch a different set of attacks
based on the algorithms discussed below.
Stream-based selective sampling. Atlas et al. [6] propose
selective sampling as a form of directed search (similar to
Mitchell [41]) that can greatly increase the ability of a connec-
tionist network (i.e. power system security analysis in their
paper) to generalize accurately. Dagan et al. [18] propose a
method for training probabilistic classiﬁers by choosing those
examples from a stream that are more informative. Linden-
baum et al. [36] present a lookahead algorithm for selective
sampling of examples for nearest neighbor classiﬁers. The
algorithm looks for the example with the highest utility, tak-
ing its effect on the resulting classiﬁer into account. Another
important application of selective learning was for feature
1322    29th USENIX Security Symposium
USENIX Association
selection [37], an important preprocessing step. Other appli-
cations of stream-based selective sampling include sensor
scheduling [34], learning ranking functions for information
retrieval [62], and in word sense disambiguation [24].
Pool-based sampling. Dasgupta [21] surveys active learning
in the non-separable case, with a special focus on statistical
learning theory. He claims that in this setting, AL algorithms
usually follow one of the following two strategies - (i) Ef-
ﬁcient search in the hypothesis spaces (as in the algorithm
proposed by Chen et al. [16], or by Cohn et al. [17]), or (ii)
Exploiting clusters in the data (as in the algorithm proposed
by Dasgupta et al. [22]). The latter option can be used to
learn more complex models, such as decision trees. As the
ideal halving algorithm is difﬁcult to implement in practice,
pool-based approximations are used instead such as uncer-
tainty sampling and the query-by-committee (QBC) algorithm
(e.g., [14, 54]). Unfortunately, such approximation methods
are only guaranteed to work well if the number of unlabeled
examples (i.e. pool size) grows exponentially fast with each
iteration. Otherwise, such heuristics become crude approxi-
mations and they can perform quite poorly.
7.2 Complex Models
PAC active learning strategies have proven effective in learn-
ing DNNs. The work of Sener et al. [49] selects the most
representative points from a sample of the training distribu-
tion to learn the DNN. Papernot et al. [46] employ substitute
model training - a procedure where a small training subset
is strategically augmented and used to train a shadow model
that resembles the model being attacked. Note that the prior
approaches rely on some additional information, such as a
subset of the training data.
Active learning for complex models is challenging. Active
learning algorithms considered in this paper operate in an
iterative manner. Let H be the entire hypothesis class. At
time time t ≥ 0 let the set of possible hypothesis be Ht ⊆
H . Usually an active-learning algorithm issues a query at
time t and updates the possible set of hypothesis to Ht+1,
which is a subset of Ht . Once the size of Ht is “small” the
algorithm stops. Analyzing the effect of a query on possible
set of hypothesis is very complicated in the context of complex
models, such as DNNs. We believe this is a very important
and interesting direction for future work.
7.3 Model Transferability
Most work in active learning has assumed that the correct hy-
pothesis space for the task is already known i.e. if the model
being learned is for logistic regression, or is a neural network
and so on. In such situations, observe that the labeled data be-
ing used is biased, in that it is implicitly tied to the underlying
hypothesis. Thus, it can become problematic if one wishes
to re-use the labeled data chosen to learn another, different
hypothesis space. This leads us to model transferability9, a
9A special case of agnostic active learning [8].
less studied form of defense where the oracle responds to any
query with the prediction output from an entirely different
hypothesis class. For example, imagine if a learner tries to
learn a halfspace, but the teacher performs prediction using a
boolean decision tree. Initial work in this space includes that
of Shi et al. [51], where an adversary can steal a linear sepa-
rator by learning input-output relations using a deep neural
network. However, the performance of query synthesis active
learning in such ecosystems is unclear.
7.4 Limitations
We stress that these limitations are not a function of our spe-
ciﬁc approach, and stem from the theory of active learning.
Speciﬁcally: (1) As noted by Dasgupta [20], the label com-
plexity of PAC active learning depends heavily on the spe-
ciﬁc target hypothesis, and can range from O(log 1
ε ) to Ω( 1
ε ).
Similar results have been obtained by others [28, 43]. This
suggests that for some hypotheses classes, the query com-
plexity of active learning algorithms is as high as that in the
passive setting. (2) Some query synthesis algorithms assume
that there is some labeled data to bootstrap the system. How-
ever, this may not always be true, and randomly generating
these labeled points may adversely impact the performance
of the algorithm. (3) For our particular implementation, the
algorithms proposed rely on the geometric error between the
optimal and learned halfspaces. Sometimes, there is no direct
correlation between this geometric error and the generaliza-
tion error used to measure the model’s goodness.
8 Related Work
Machine learning algorithms and systems are optimized for
performance. Little attention is paid to the security and pri-
vacy risks of these systems and algorithms. Our work is moti-
vated by the following attacks against machine learning.
1. Causative Attacks: These attacks are primarily geared at
poisoning the training data used for learning, such that the
classiﬁer produced performs erroneously during test time.
These include: (a) mislabeling the training data, (b) changing
rewards in the case of reinforcement learning, or (c) modify-
ing the sampling mechanism (to add some bias) such that it
does not reﬂect the true underlying distribution in the case of
unsupervised learning [48]. The work of Papernot et al. [47]
modify input features resulting in misclassiﬁcation by DNNs.
2. Evasion Attacks: Once the algorithm has trained success-
fully, these forms of attacks provide tailored inputs such that
the output is erroneous. These noisy inputs often preserves the
semantics of the original inputs, are human imperceptible, or
are physically realizable. The well studied area of adversarial
examples is an instantiation of such an attack. Moreover, eva-
sion attacks can also be even black-box i.e. the attacker need
not know the model. This is because an adversarial example
optimized for one model is highly likely to be effective for
other models. This concept, known as transferability, was
introduced by Carlini et al. [15].
USENIX Association
29th USENIX Security Symposium    1323
3. Exploratory Attacks: These forms of attacks are the primary
focus of this work, and are geared at learning intrinsics about
the algorithm used for training. These intrinsics can include
learning model parameters, hyperparameters, or training data.
Typically, these forms of attacks fall in two categories - model
inversion, or model extraction. In the ﬁrst class, Fredrikson et
al. [23] show that an attacker can learn sensitive information
about the dataset used to train a model, given access to side-
channel information about the dataset. In the second class, the
work of Tramér et al. [55] provides attacks to learn parameters
of a model hosted on the cloud, through a query interface.
Termed membership inference, Shokri et al. [52] learn the
training data used for machine learning by training their own
inference models. Wang et al. [57] propose attacks to learn a
model’s hyperparameters.
9 Conclusions
In this paper, we formalize model extraction in the context of
Machine-Learning-as-a-Service (MLaaS) servers that return
only prediction values (i.e., oracle access setting), and we
study its relation with query synthesis active learning (Obser-
vation 1). Thus, we are able to implement efﬁcient attacks to
the class of halfspace models used for binary classiﬁcation
(§ 6). While our experiments focus on the class of halfspace
models, we believe that extraction via active learning can be
extended to multiclass and non-linear models such as deep
neural networks, random forests etc. We also begin exploring
possible defense approaches (§ 5). To the best of our knowl-
edge, this is the ﬁrst work to formalize security in the context
of MLaaS systems. We believe this is a fundamental ﬁrst step
in designing more secure MLaaS systems. Finally, we suggest
that data-dependent randomization (e.g., model randomiza-
tion as in [2]) is the most promising direction to follow in
order to design effective defenses.