the exact same model parameters. This is done for demonstration purposes and
in order to provide comparable results between the hosts. In a real setting, the
model parameters of the hosts would vary according to their security conﬁgura-
tions, the observation probability parameters vary according to the sensors used,
and the cost vector is determined by the value of the assets and the consequence
of the diﬀerent security states.
Results. The above models were implemented and used to perform real-time
risk assessment on the Lincoln Laboratory data set. The entire data set has a
duration of 11836 sec., and a total of 36635 alerts, 84 of which are USTAT alerts.
The remaining are Snort alerts. As outlined above, the data set consists of an
attack in ﬁve phases. By inspecting the data set, we can see that the phases
correspond to the approximate time periods 1500 - 1920 sec. (the IP sweep),
2880 - 3480 sec. (the sadmind ping), 4380 - 4420 sec. (the break in to Mill,
Pascal, and Locke), 5400 sec. (the installation of DDoS tools), and 7620 sec.
(the outbound DDoS).
Figure 3 shows the total assessed risk for the Lincoln Laboratory data for
the full duration of the data set. The ﬁgure shows a sum of the risk for all
hosts in the four subnets (in total 1016 hosts). The break-ins performed against
Mill, Pascal, and Locke are clearly visible as peaks of risk activity. The sadmind
ping also introduces a peak in the data, but the IP sweep and the installation
of DDoS tools are hardly distinguishable from the remaining activity. Note that
the system seems to have a minimum risk of approximately 1200 in the long run.
This is caused by a stable security state with risk level 1.09 for the individual
hosts, given a suﬃciently long interval of only “no alert” observations. The stable
security state risk for the entire network is consequently 1107. The diﬀerence can
be explained by the fact that the host 172.16.114.1 has a high amount (more than
2000) of outbound ICMP related alerts. As a router, this host should probably
have diﬀerent HMM parameters then the other hosts.
Using Hidden Markov Models to Evaluate the Risks of Intrusions
157
 2000
 1800
 1600
 1400
 1200
 1000
 800
 600
 400
 200
y
t
i
v
i
t
c
A
k
s
R
i
 0
 0
 2000
 4000
 6000
Time (s)
 8000
 10000
Fig. 3. Total assessed risk for Lincoln Labs data set
Figure 4 (a), (b), and (c) show the assessed risk for the hosts Mill, Pascal, and
Locke, respectively. The hosts Mill and Pascal have host-based IDSs (USTAT) that
provide several alerts during the experiment. This can be seen in Fig. 4 (a), (b), and
(c), as the host Locke has far less activity than the other two. Phase 3 and 5 of the
attack are clearly marked with the maximum risk activity value (100) for all three
hosts. Phase 2 and 4 are also visible as peaks, whereas phase 1 is hardly discernible
from the other activity in Fig. 4 (a) and (b), and not visible at all in (c). Note that
Pascal (Fig. 4 (b)) shows more peaks than Mill (Fig. 4 (a)). This is caused by the
fact that Pascal produces 70 USTAT alerts, while Mill only produces 14.
Figure 5 (a) and (b) show the assessed total network risk and the assessed
risk for Mill at the approximate time of the compromise (4000s to 6000s). The
graphs correspond to Fig. 3 and 4 (a), but zoom in on the time period. Fig. 5
(b) shows the two peaks corresponding to phase 3 and 4 of the attack.
By counting the priority of the alerts for the entire data set, we can eval-
uate the performance of the alert prioritization mechanism. However, for the
purpose of the prioritization results, we do not consider the outbound DDoS
attack with spoofed IP addresses and the outbound alerts from the router with
IP address 172.16.114.1. The outbound DDoS attack alerts represents 93% of
the total alerts, and are all marked with the highest priority. The IP address
172.16.114.1 is discussed above. It has a high number of alerts (6% of the total
amount), and they would also all be marked as maximum priority alerts. Having
ﬁltered out these alerts, 52.49% of the alerts are with priority below 20, 28.87%
with priority between 20 and 40, 6.49% with priority between 40 and 60, 2.35%
with priority between 60 and 80, and 9.81% with priority between 80 and 100. It
is clear that the alert prioritization is successful in that only a small percentage
of the alerts are assigned high priority values. The majority of the alerts are
marked as low priority.
We see that the risk assessment method with the current conﬁguration and
alert classiﬁcation parameters is able to assess the risk and detect several of the
security relevant incidents outlined above. In particular, we see that the model
158
A. ˚Arnes et al.
 100
 80
 60
 40
 20
y
t
i
v
i
t
c
A
k
s
R
i
 0
 0
 2000
 4000
 6000
Time (s)
 8000
 10000
(a) Assessed risk for host Mill
 100
 80
 60
 40
 20
y
t
i
v
i
t
c
A
k
s
R
i
 100
 80
 60
 40
 20
y
t
i
v
i
t
c
A
k
s
R
i
 0
 0
 2000
 4000
 6000
Time (s)
 8000
 10000
 0
 0
 2000
 4000
 6000
Time (s)
 8000
 10000
(b) Assessed risk for host Pascal
(c) Assessed risk for host Locke
Fig. 4. Real-time risk assessment for Lincoln Labs data set
is capable of assigning the appropriate maximum risk values to the two most
critical incidents, the compromise and the outbound DDoS attack with spoofed
IP addresses.
4.2 Real Traﬃc Data from the Technical University of Vienna
The second data set is based on real network traﬃc from the Technical University
of Vienna [8]. The data set contains a trace of nine days for a class B network.
However, in this experiment we have only included three days worth of data
from one class C network. There were no known security incidents during this
period. The IDS used in this setup is Snort with the same signature set as in the
 1900
 1800
 1700
 1600
 1500
 1400
 1300
y
t
i
v
i
t
c
A
k
s
R
i
 100
 80
 60
 40
 20
y
t
i
v
i
t
c
A
k
s
R
i
 1200
 4000
 4500
 5000
Time (s)
 5500
 6000
 0
 4000
 4500
 5000
Time (s)
 5500
 6000
(a) Assessed network risk showing
system compromise
(b) Assessed risk showing host Mill
compromise
Fig. 5. Lincoln Labs data set showing period of time of compromise
Using Hidden Markov Models to Evaluate the Risks of Intrusions
159
previous example. The model parameters are also the same as in the previous
example, with the exception that there are no host-based IDSs in this setup.
Results. Figure 6 shows the assessed risk for the entire network for the full
three day period. The two periods of increased risk activity are caused by an
increasing amount of outbound alerts, as seen in Fig. 7 (c). We see that the risk
seems to have a lower bound at a level about 280. This lower bound is the total
risk associated with the stable security state of the individual host HMMs. As
in 4.1, the individual stable state risk for a host is 1.09, and the total stable
state risk for the network is consequently 276.86.
Figure 7 (a), (b), (c), and (d) show the assessed risk for a duration of 3.5 hours,
corresponding to the second period of increased activity in Fig. 6. Fig. 7 (a) shows
the risk activity for the full network, indicating three peaks of increased risk and
some periodic ﬂuctuations. Fig. 7 (b) shows the risk activity for a host with no
alert activity. Fig. 7 (c) shows the risk activity for a host with outbound alerts that
lead to several peaks of maximum risk for the host. Based on the underlying traﬃc
data, it has been determined that these alerts are in fact false alerts from Snort
caused by a speciﬁc user pattern. Finally, Fig. 7 (d) shows the risk activity for a
web server with periodic peaks of risk values between 20 and 40. This is caused
by probing activity directed at the web server. This activity is present during the
entire period, and is a contributing factor to the ﬂuctuations in Fig. 6.
y
t
i
v
i
t
c
A
k
s
R
i
 500
 400
 300
 200
 100
 0
 100000
 150000
 200000
 250000
Time (s)
Fig. 6. Total assessed risk for class C subnet (3 days)
For this data set, 46.35% of the alerts are assigned priority below 20, 49.78%
with priority between 20 and 40, 1.29% with priority between 40 and 60, 0.08%
with priority between 60 and 80, and 2.49% with priority between 80 and 100.
As for the previous example, it is clear that the alert prioritization is successful
in that only a small percentage of the alerts are assigned high priority values.
We see that the approach is applicable to data from real network traﬃc.
However, this example demonstrates that the proposed model is dependent on
the accuracy of the underlying IDSs, and false positives and negatives aﬀect
160
A. ˚Arnes et al.
 1200