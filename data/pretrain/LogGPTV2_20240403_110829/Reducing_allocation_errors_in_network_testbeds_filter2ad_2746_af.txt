 0.1
 0.01
 0.001
 0.0001
 1e-05
 1
 0.1
 0.01
)
X
>
x
(
P
 0.001
research
class
internal
 10
 100
Size (physical nodes)
 1000
(a) Instance size
research
class
internal
1h 4h
1d
1w 1m 6m
Duration (h=hour, d=day, w=week, m=month)
(b) Instance duration
research
class
internal
 1e-06  1e-05  0.0001  0.001
 0.01
 0.1
 1
Node-hours per hour of lifetime
(c) Node-hours/hour project lifetime
Figure 12: Heavy tails in testbed use measures
allocations per second, their allocations are larger and have longer
duration.
Penalizing big testbed users is difﬁcult because large use in re-
search projects seems to be correlated with publications. We manu-
ally classify research projects on DeterLab as “outcome projects” if
they have published one or more peer-reviewed publication, or MS
and PhD thesis, in which they acknowledge use of DeterLab. We
ﬁnd 48 outcome projects and 104 no-outcome projects. We then
deﬁne an instance as big if it uses 20 nodes or more, and as long
if it lasts one day or longer. Only 9% of instances are big and 5%
are long. Outcome projects have on the average 99 big and 33 long
instances, while no-outcome projects have 10 big and 8 long in-
stances. Thus big instances and long instances that lead to unfair
506testbed use seem to be correlated with research publications, and
good publicity for the testbed, and are thus very valuable to testbed
owners. It would be unwise to alienate these users or discourage
heavy testbed use. Instead, we want to gently tip the scale in favor
of small users when possible.
We identify three design goals for fairness policy on testbeds:
1. Predictability. Any fairness approach must allow users to
accurately anticipate when they may be penalized.
2. User control. Actions taken to penalize a user must de-
pend solely on their actions, and testbeds should offer opt-out
mechanisms.
3. On-demand. Resources should be reclaimed only when there
is an instance whose allocation fails, and whose needs can be
satisﬁed by these resources.
One approach to tipping the scale would be to reclaim some
resources from unfair projects until their use is reduced to a fair
share. This would violate all three design goals, because unfair sta-
tus changes depending on how many other projects are active, and
freed resources could sit unused on the testbed. Another approach
would be to reclaim resources on demand from an instance that has
used the most node-hours. Again this leads to unpredictable behav-
ior from the user’s point of view, and it may interrupt short-running
but large instances that are difﬁcult to allocate again. We opt for
the strategy that reclaims resources on demand from the longest-
running instance, as long as it has been running for more than one
day and is currently idle. This lets users identify which of their in-
stances may be reclaimed in advance. We propose two possible ap-
proaches to fair allocations: Take-a-Break and Borrow-and-Return.
9.3 Take-a-Break
In Take-a-Break approach, when a resource allocation fails, we
identify any instance that holds node types that are requested by the
allocating instance and is currently idle, as the break candidate. We
then select the candidate that has been running the longest and, if its
age is greater than one day, we release its resources to the allocating
instance. The break candidate is queued for allocation immediately,
and an attempt is made to allocate it after any resource release.
Figure 14 shows the rate of allocation failures for Take-a-Break
approach under the assign.tbreak label. We also deploy mi-
gration and alternative node types. We assume that a break can-
didate is always idle in our simulation. Failure rate is strikingly
)
s
r
u
o
h
-
e
d
o
n
l
i
e
b
s
s
o
p
l
l
a
f
o
%
(
e
g
a
s
U
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
unfair
fair
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
Week
Figure 13: Usage of fair and unfair projects in 2011.
low, reaching 1.5% by the end of our simulation even though the
density of allocated instances is increased. Overall, assign+ with
Take-a-Break creates only 25.3% of failed allocations generated by
assign. This comes at the price – duration of 177 instances is
prolonged. Half of these instances experience delays of up to 1
hour, 79% up to 4 hours, and 97% up to 1 day. Only six instances
are delayed more than one day, the longest delay being 1.67 days.
Looking at relative delay, 72% of instances are only delayed up to
1% compared to their original duration, 94% of instances are de-
layed by at most 10% and the worst delay doubles the instance’s
duration.
)
1
-
0
(
e
t
a
r
e
r
u
l
i
a
F
 0.04
 0.035
 0.03
 0.025
 0.02
 0.015
 0.01
 0.005
 0
 0
assign+.atmig
assign+.tbreak
assign+.borrow
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
Instances (thousands)
Figure 14: Error rates for assign+ when using Take-a-Break
and Borrow-and-Return approaches.
We now verify if we have tipped the scale in favor of fair pro-
jects. We ﬁrst apply the fairness calculation to the allocations re-
sulting from running assign+ on our “2011 synthetic setup” and
obtain similar usage patterns, to those seen in the real dataset, ex-
cept that the allocation failures are reduced because we were remov-
ing overlapping instances during workload creation and we started
with an empty testbed. There were 14.5 failures per project when
it is unfair, and 3.98 failures per project, when it is fair. We then
apply the same calculation to the allocations resulting from running
assign+ with Take-a-Break on our “2011 synthetic setup” and we
count both allocation failures and forcing an instance to take a break
as “failures”. We ﬁnd that there are 18.6 failures per project when
it is unfair, and 1.62 failures per project, when it is fair. Thus unfair
projects are slightly penalized and the failure rate of fair projects is
more than halved.
9.4 Borrow-and-Return
While Take-a-Break approach helps fair projects obtain more re-
sources, it forces instances whose resources have been reclaimed
to wait for unpredictable duration. Borrow-and-Return approach
amends this. Its design is the same as Take-a-Break approach, but
resources are only “borrowed” from long-running instances for 4
hours, after which they are returned to their original owner. Users
receiving these borrowed nodes would be alerted to the fact that the
nodes will be reclaimed at a certain time. Instances interrupted this
way are queued and allocated as soon as possible.
Figure 14 shows the rate of allocation failures for Borrow-and-
Return approach, under the assign.borrow label. We also de-
ploy migration and alternative node types. Allocation failure rate
507is similar to that of Take-a-Break approach – 25.6% of that of the
assign . Duration of 583 instances is prolonged. 80% of these
instances experience delays of up to 1 hour, 96% up to 4 hours, and
99% up to 1 day. Only ﬁve instances are delayed more than one day,
the longest delay being 1.9 days. Looking at relative delay, 25% of
instances are delayed up to 1% compared to their original duration,
76.1% of instances are delayed by at most 10%, 97% are delayed by
at most 100% and the worst delay extends the instance’s duration
4.5 times. This approach seems to ﬁnd a good middle ground be-
tween heavily penalizing long instances, like Take-a-Break does,
and doing nothing. Fairness of Borrow-and-Return approach is
slightly worse than that of Take-a-Break, leading to the average of
18.9 failures for unfair projects, and 2.3 for fair projects.
10. CONCLUSIONS
Network testbeds are extensively used today, both for research
and for teaching, but their resource allocation algorithms and poli-
cies have not evolved much since their creation. This paper exam-
ines the causes for resource allocation failures in Emulab testbeds
and ﬁnds that 31.9% of failures could be avoided through: (1) pro-
viding better information to users about the cost of and the alterna-
tives to their topology constraints, and (2) better resource allocation
strategies. The remaining failures can be reduced to 25.3% of the
original by applying a gentle fair-sharing strategy such as Take-a-
Break or Borrow-and-Return. The main challenge in designing fair
testbed allocation policies lies in achieving fairness, while being
sensitive to human user needs for predictability and while nurturing
heavy users that bring most value to the testbed. Our investigation
is just the ﬁrst of many that need to be undertaken to reconcile these
conﬂicting, but important goals.
11. ACKNOWLEDGMENTS
This material is based upon work supported by the National Sci-
ence Foundation under Grant No. 1049758. Any opinions, ﬁndings,
and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect the views of the
National Science Foundation.
12. REFERENCES
[1] Other Emulab Testbeds. https://users.emulab.
net/trac/emulab/wiki/OtherEmulabs.
[2] Schooner WAIL (Wisconsin Advanced Internet Laboratory).
http://www.schooner.wail.wisc.edu.
[3] R. Bajcsy, T. Benzel, M. Bishop, et al. Cyber Defense
Technology Networking and Evaluation. Communications of
the ACM, 2004.
[4] S. M. Banik, L. P. Daigle, and T. H. Chang. Implementation
and Empirical Evaluations of Floor Control Protocols on
Planetlab Network. In Proceedings of the 47th Annual
Southeast Regional Conference, ACM-SE 47, 2009.
[5] T. Benzel. The Science of Cyber Security Experimentation:
The DETER Project. In Proc. of 2011 Annual Computer
Security Applications Conference (ACSAC), December 2012.
[6] N. M. M. K. Chowdhury, M.R. Rahman, and R. Boutaba.
Virtual network embedding with coordinated node and link
mapping. In Proc. of INFOCOM 2009, pages 783–791, 2009.
[7] B. Chun, D. Culler, et al. Planetlab: an Overlay Testbed for
Broad-coverage Services. ACM SIGCOMM Computer
Communication Review, 33(3):3–12, 2003.
[8] Ali Ghodsi, Matei Zaharia, Benjamin Hindman, Andy
Konwinski, Scott Shenker, and Ion Stoica. Dominant
resource fairness: Fair allocation of multiple resource types.
In Proceedings of the 8th USENIX Conference on Networked
Systems Design and Implementation, 2011.
[9] F. Hermenier and R. Ricci. How to Build a Better Testbed:
Lessons From a Decade of Network Experiments on Emulab.
In TridentCom, 2012.
[10] J.P. Herron, L. Fowler, and C. Small. The GENI
Meta-Operations Center. In Proc. of IEEE Conf. on eScience,
pages 384–385. IEEE Computer Society, 2008.
[11] Benjamin Hindman, Andy Konwinski, Matei Zaharia, Ali
Ghodsi, Anthony D. Joseph, Randy Katz, Scott Shenker, and
Ion Stoica. Mesos: a platform for ﬁne-grained resource
sharing in the data center. In Proceedings of the 8th USENIX
conference on Networked Systems Design and
Implementation, 2011.
[12] B. W. Kernighan and S. Lin. An Efﬁcient Heuristic
Procedure for Partitioning Graphs. The Bell system technical
journal, 49(1):291–307, 1970.
[13] W. Kim, A. Roopakalu, K.Y. Li, and V.S. Pai. Understanding
and Characterizing PlanetLab Resource Usage for Federated
Network Testbeds. In Proc. of the ACM SIGCOMM, pages
515–532. ACM, 2011.
[14] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization
by Simulated Annealing. Science, 220(4598):671–680, 1983.
[15] J. Lu and J. Turner. Efﬁcient mapping of virtual networks
onto a shared substrate. Washington University in St. Louis,
Tech. Rep, 2006.
[16] A. Medina, A. Lakhina, I. Matta, et al. BRITE: An Approach
to Universal Topology Generation. In Proc. of MASCOTS,
pages 346–. IEEE Computer Society, 2001.
[17] Dejan S. Milojicic, Fred Douglis, Yves Paindaveine, Richard
Wheeler, and Songnian Zhou. Process migration. ACM
Comput. Surv., 32(3):241–299, September 2000.
[18] T. Miyachi, K. Chinen, and Y. Shinoda. Automatic
Conﬁguration and Execution of Internet Experiments on an
Actual Node-based Testbed. In Proc. of Tridentcom, 2005.
[19] J. Nakata, S. Uda, T. Miyaclii, et al. Starbed2: Large-scale,
Realistic and Real-time Testbed for Ubiquitous Networks. In
Proc. of TridentCom, 2007.
[20] R. Ricci, C. Alfeld, and J. Lepreau. A Solver for the Network
Testbed Mapping Problem. SIGCOMM Comput. Commun.
Rev., 33(2):65–81, 2003.
[21] M. Ripeanu, M. Bowman, S. Chase, et al. Globus and
Planetlab Resource Management Solutions Compared. In
Proc. of International Symposium on High Performance
Distributed Computing, pages 246 – 255, 2004.
[22] I. Stoica, S. Shenker, and H. Zhang. Core-stateless Fair
Queueing: a Scalable Architecture to Approximate Fair
Bandwidth Allocations in High-speed Networks. IEEE/ACM
Trans. Netw., 11(1):33–46, 2003.
[23] Douglas Thain, Todd Tannenbaum, and Miron Livny.
Distributed Computing in Practice: the Condor Experience.
Concurrency - Practice and Experience, 17(2-4):323–356,
2005.
[24] B. White, J. Lepreau, L. Stoller, et al. An Integrated
Experimental Environment for Distributed Systems and
Networks. In Proc. of OSDI, pages 255–270, December
2002.
[25] M. Yu, Y. Yi, J. Rexford, and M. Chiang. Rethinking Virtual
Network Embedding: Substrate Support for Path Splitting
and Migration. ACM SIGCOMM Comp. Comm. Rev.,
38(2):17–29, 2008.
508