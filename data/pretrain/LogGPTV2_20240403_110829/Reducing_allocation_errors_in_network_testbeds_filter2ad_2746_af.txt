# Heavy Tails in Testbed Use Measures

## Data Points
- 0.1
- 0.01
- 0.001
- 0.0001
- 1e-05
- 1
- 0.1
- 0.01

## Instance Size
- **Size (Physical Nodes)**
  - 10
  - 100
  - 1000
- **(a) Instance Size**

## Instance Duration
- **Duration (h=hour, d=day, w=week, m=month)**
  - 1h
  - 4h
  - 1d
  - 1w
  - 1m
  - 6m
- **(b) Instance Duration**

## Node-Hours per Hour of Project Lifetime
- **Node-hours per hour project lifetime**
  - 1e-06
  - 1e-05
  - 0.0001
  - 0.001
  - 0.01
  - 0.1
  - 1
- **(c) Node-hours/hour project lifetime**

**Figure 12: Heavy tails in testbed use measures**

## Analysis of Testbed Usage
Large and long-duration allocations are challenging to penalize because they often correlate with research publications. We classify projects on DeterLab as "outcome projects" if they have published at least one peer-reviewed paper or a thesis acknowledging the use of DterLab. Our analysis shows that:
- 48 outcome projects and 104 no-outcome projects.
- An instance is considered "big" if it uses 20 or more nodes, and "long" if it lasts one day or longer.
- Only 9% of instances are big, and 5% are long.
- Outcome projects have an average of 99 big and 33 long instances, while no-outcome projects have 10 big and 8 long instances.

These large and long instances, while potentially leading to unfair testbed use, are valuable for generating good publicity and should not be discouraged. Instead, we aim to gently favor small users when possible.

## Design Goals for Fairness Policy
1. **Predictability**: Users must be able to accurately anticipate when they may be penalized.
2. **User Control**: Actions taken to penalize a user must depend solely on their actions, and testbeds should offer opt-out mechanisms.
3. **On-demand**: Resources should be reclaimed only when there is an instance whose allocation fails, and whose needs can be satisfied by these resources.

### Approaches to Resource Reclamation
1. **Reclaiming from Unfair Projects**: This approach violates all three design goals due to unpredictable changes in status and potential underutilization of freed resources.
2. **Reclaiming from Longest-Running Instances**: This approach reclaims resources from the longest-running idle instance, providing predictability and allowing users to identify which instances may be reclaimed.

### Take-a-Break Approach
- When a resource allocation fails, the system identifies any idle instance holding the requested node types.
- The longest-running instance (over one day) is selected, and its resources are released to the allocating instance.
- The interrupted instance is queued for immediate reallocation.
- **Results**: 
  - Failure rate is low, reaching 1.5% by the end of the simulation.
  - 25.3% of failed allocations compared to the baseline.
  - 177 instances experience delays, with most delays being less than one day.

### Borrow-and-Return Approach
- Similar to Take-a-Break, but resources are "borrowed" for only 4 hours before being returned.
- Users are alerted about the temporary nature of the borrowed nodes.
- **Results**:
  - Allocation failure rate is 25.6% of the baseline.
  - 583 instances experience delays, with most delays being less than one day.
  - Slightly worse fairness compared to Take-a-Break, with 18.9 failures for unfair projects and 2.3 for fair projects.

## Conclusions
Network testbeds are widely used for research and teaching, but their resource allocation algorithms and policies need improvement. This paper examines the causes of resource allocation failures in Emulab testbeds and finds that 31.9% of failures could be avoided through better information and improved strategies. The remaining failures can be reduced to 25.3% using fair-sharing strategies like Take-a-Break or Borrow-and-Return. Achieving fairness while being sensitive to user needs and nurturing heavy users is a key challenge.

## Acknowledgments
This work is supported by the National Science Foundation under Grant No. 1049758. The opinions, findings, and conclusions expressed are those of the authors and do not necessarily reflect the views of the National Science Foundation.

## References
[References listed here as provided in the original text]

---

This revised version aims to provide a clear, coherent, and professional presentation of the data and analysis.