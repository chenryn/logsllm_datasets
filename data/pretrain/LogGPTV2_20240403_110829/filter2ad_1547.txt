title:DesktopDC: setting all programmable data center networking testbed
on desk
author:Chengchen Hu and
Ji Yang and
Zhimin Gong and
Shuoling Deng and
Hongbo Zhao
DesktopDC: Setting All Programmable Data Center
Networking Testbed on Desk
Chengchen Hu, Ji Yang, Zhiming Gong,
Shuoling Deng
Xi’an Jiaotong University
PI:EMAIL, {yangji, zhm.gong,
dengshuoling2}@stu.xjtu.edu.cn
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network Archi-
tecture and Design - Network communication, Network topology
Keywords
OpenFlow, data center, programmable
1.
INTRODUCTION
Software Deﬁned Networking (SDN) is an emerging network ar-
chitecture, which decouples the control plane from the physical net-
work infrastructure and operates network with global abstraction of
lower level network functionalities. SDN becomes very attractive
to design a ﬂexible and customized Data Center Networks (DCN)
since Google had recently achieved huge impact by applying SDN
to manage the inter-DC trafﬁc [2]. The successful story when SDN
met DCN in Google not only demonstrates the feasibility for de-
ployment of SDN to large scale network, but also stimulates a re-
search of SDN, especially in the context of DCN. However, it be-
comes quite challenging to realize and verify the research progress
into practice or to emulate a whole SDN-compatible DCN, since
it is costly to operate a Data Center (DC) testbed in a research lab
and it is hard to modify the processing logic of a data path for the
research and innovation purpose.
Although OpenFlow [3] is the de factor SDN protocol nowa-
days, which deﬁnes the interface between the data plane switches
and the control plane controllers, new SDN architectures and pro-
tocols are proposed. Even for the OpenFlow itself, it keeps evolv-
ing and updates its speciﬁcation in every a few months. Software
OpenFlow Switches (OFS), e.g., Open vSwitch, are easy to de-
ploy and modify, but they are hard to guarantee the performance
for wire-speed processing. Commercial OFS provide stable perfor-
mance and sufﬁcient network interfaces. However, they cannot be
updated with evolving OpenFlow speciﬁcations and modiﬁed inno-
vated processing logic. NetFPGA [4] is quite successful to open a
way for changing the hardware logic through FPGA, but it also has
limitations, e.g., a host server is further required, the logic resource
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2631472.
Hongbo Zhao
MeshSr Co. Ltd.
PI:EMAIL
is not enough for OFS processing, and bottleneck PCI interface be-
tween host CPU and NetFPGA (NetFPGA 1G version).
We have designed an all programmable SDN switch (named as
ONetSwitch) and a DCN testbed on the desktop (named Desk-
topDC) based on ONetSwitch. The merits of DesktopDC are its
small size, low power, and ﬂexible programmability. Among many
building cases, we brieﬂy describe two of them in this paper: one
is SDN based routing and the other is Hadoop based computing.
2. DESIGN OF ONETSWITCH
ONetSwitch is a Zyqn-based embedded computing platform. The
Zyqn chip is produced by Xilinx, which is constituted by both an
ARM processor and a FPGA. Empowered by Zyqn, ONetSwitch is
“all programmable”, which means software programmable, gate-
ware restructual, and hardware extensible.
Currently, ONetSwitch has two versions. The ﬁrst version is
ONetSwtich20. It has a 533MHz ARM Cortex-A9 dual core pro-
cessor combined with a Artix FPGA in SoC, 512MB DDR3 SDRAM
and 5 1G Ethernet ports. Four Ethernet ports are connected to
FPGA and the rest is connected to the processor. Its size is about
13.5cm∗ 22.5cm∗ 1.5cm. A second ONetSwitch45 model, which
is ﬁrst introduced in ONS 2014 [1], uses the Xilinx Zynq-7045
SoC integrating ARM Cortex-A9 dual core processor and Kintex-
7 FPGA. ONetSwitch45 provides four SFP+ interfaces to support
10G links, four 1G Ethernet interfaces and commodity wireless
adapter modules for 802.11 a/g/n. Its size is 18cm∗ 18cm∗ 1.5cm.
An OpenFlow switch is implemented on ONetSwitch. As shown
in Fig.1, this design of OpenFlow switch has a datapath with hybrid
software and hardware design. The hardware part provides 8Gb/s
packet header matching ability while the software part enables al-
most “unlimited” matching table size. The ﬂow table lookup oper-
ation starts at the ﬁrst table in hardware, which contains the hottest
ﬂow entries. If table miss in hardware, the packet will be sent to
software datapath, which contains all the ﬂow entries from con-
troller. An OpenFlow agent is ported from the reference OpenFlow
software switch to translate OpenFlow messages. Hardware Ab-
straction Layer (HAL) works between agent and hardware, which
translates original ﬂow entries from OpenFlow messages into the
semantic-equivalent formats that optimize the switch performance.
Besides switch ability, ONetSwitch also has more resources for
further use. A 64Gb/s DMA channel in the SoC connects software
and hardware in various ways. The ﬁrst way, conﬁgurable virtual
Ethernet devices send packets to hardware datapath, which reduce
latency and unnecessary matches. Another way helps to share com-
puting resources by sending raw data from software to hardware.
593Table 1: DesktopDC’s Hadoop performance comparison
Core Frequency
Boot
Job Duration
Node Power
Switch Power
Size
ONetSwitch
533MHz
<10s
192794ms
6.7W
–
13.5 ∗ 22.5 ∗ 1.5cm3
x86 Server
3.3GHz
50s
13314ms
56.9W
20W
37.0 ∗ 43.5 ∗ 4.5cm3
On receiving the messages in the controller, routing application will
ﬁgure out whether routing policy would suffers from the failure and
if so, it recalculates all the lapsed paths and replace their entries
with new ones. The path will recover in less than 500ms.
The second experiment tests Hadoop computing on DesktopDC,
where we conﬁgure each node in DesktopDC as both computing
node and networking node. For the testbed in Fig. 2, our Hadoop
computing system will have at most 4Gbps bandwidth Ethernet for
each node. In order to compare the power and computing with one
single x86 based server, four ONetSwitch20 nodes are utilized. In
the ﬁrst stage, all the computing tasks are ﬁnished by ARM proces-
sors, meanwhile the rest resources form the OpenFlow based net-
work connecting all ONetSwitches. A high bandwidth virtual Eth-
ernet port transfers data to OpenFlow datapath, meanwhile, Open-
Flow controller monitors and manages the data transfer to optimize
bandwidth for jobs. Table.1 demonstrates that with elaborate de-
sign of computing management algorithm in controller, we can run
a data center with hundreds of nodes on the desktop, without any
concern about the power and space.
In addition, the computing
ability and power saving will be further improved by moving some
dedicate computing to FPGA, which is our future work.
4. CONCLUSION
In this poster, we propose the DesktopDC, an all programmable
and energy efﬁcient SDN compatible innovation platform. Our ini-
tial experiments and demo exhibit the features of ﬂexibility, capa-
bility, power saving. It provides the SDN datapath designs for fast
SDN prototyping and veriﬁcations in both software and hardware.
5. ACKNOWLEDGEMENT
This paper is supported by 863 plan (2013AA013501), National
Sci. and Tech. Major Project (no.2013ZX03002003-004), NSFC
(61272459), Research Plan in Shaanxi Province (2013K06-38).
6. REFERENCES
[1] C. Hu, J. Yang, H. Zhao, and J. Lu. Design of all
programmable innovation platform for software deﬁned
networking. In Open Networking Summit 2014, Santa Clara,
CA, 2014.
[2] S. Jain, A. Kumar, S. Mandal, and Etal. B4: Experience with a
globally-deployed software deﬁned wan. In Proceedings of
the ACM SIGCOMM 2013, pages 3–14, 2013.
[3] N. McKeown, T. Anderson, H. Balakrishnan, G. Parulkar,
L. Peterson, J. Rexford, S. Shenker, and J. Turner. Openﬂow:
enabling innovation in campus networks. SIGCOMM Comput.
Commun. Rev., 38(2):69–74, Mar. 2008.
[4] J. Naous, G. Gibb, S. Bolouki, and N. McKeown. Netfpga:
reusable router architecture for experimental research. In
Proceedings of the ACM PRESTO ’08, pages 1–7, New York,
NY, USA, 2008. ACM.
Figure 1: OpenFlow switch implementation
3. DESKTOPDC AND BUILDING CASES
Connected multiple ONetSwitches, we assemble a DCN testbed
called DesktopDC, whose size makes it possible to put on the desk-
top. In DesktopDC, each ONetSwitch can work as both a comput-
ing node and a networking node, and both the computing and the
networking functions can be ﬁt either in ARM processor or FPGA,
depending on the tradeoff between performance and ﬂexibility. A
DesktopDC can also be built with ONetSwitch45 only or mixing
with two ONetSwitch models. In the example in Fig.2, the nodes
were following a Fattree(4) topology.
Figure 2: DesktopDC with Fat-Tree topology
With this DesktopDC testbed, we set two building cases. In the
ﬁrst one, we emulate an OpenFlow based fault tolerance routing
for DCN. Each ONetSwitch node in this experiment is served as
an OpenFlow switch, as mentioned in the last section. All the
ONetSwitches are directly connected to a Ryu OpenFlow controller
where conﬁgurations and our fault tolerant routing application are
performed. Two more servers are connected as client. In the ﬁrst
step, the routing application will collect physical topology of the
DCN dynamically. In the second step, it maps the collected physi-
cal network topology with the blueprint. Even there are little mal-
functions, the mapping process can identify the good ones for net-
working functions. Also, it equipped an ID learning mechanism,
which helps to learn custom servers’ physical ID (like MAC), log-
ical ID (like IP) and its related physical locations. After complet-
ing of all the above work, the routing application will calculate
a path based on the routing algorithm, and assign ﬂow entries to
switches. We have measured the average delay within 100 experi-
ments. The time of topology collecting, ID mapping from blueprint
to real network, and ID/address/ﬂow table issuing time experiments
are 1.5ms, 3.2ms and 0.29ms respectively. We simulate the net-
work failure by disabling different links in the DesktopDC. A port-
statistics OpenFlow message will be sent by the affected switches.
594