average of MAE and Quality is:
(cid:18)
(cid:19)
=
1
2
ac(D)
1
2
ac(D) − ac(D|θ) +
θ

+ ac(D|θ) − θ

The actual MAE is less than the sum of the two components for
two reasons. First the two components of the MAE sometimes can-
cel out each other to some degree. This effect is smallest when one
component dominates the other. Second, the step of turning nega-
tive counts to 0 also reduces the noise error. For datasets where
the average column count is high and the average row count is
low (e.g., Transaction and Document), we see that the average of
MAE and Quality converges to half of average column count as θ
increases, because the noise error dominates the truncation error,
which is small since the θ value is much higher than the average
row count, and because there are few negative noise counts, as the
average column count is high.
In summary, Figure 1 gives evidence that the quality function we
design works well for our purpose.
5.3 Comparison for Column Counts Query
Next, we show the comparisons between our algorithms and ex-
isting approaches using MAE and MRE. Figure 2 and Figure 3
show the MAE and MRE under different approaches for the six
datasets, respectively. As introduced earlier, LPA, FPA, and GS
are the approaches compared in [15], and ALPA is an additional
baseline approach we compare. DPSense and DPSense-S stand
for our algorithms introduced in Section 4.2 and 4.3, respectively.
For the two privacy budgets, we compute the MAE within all
columns and within columns with top 5% large counts in raw data.
One can expect MAE in top 5% columns would be larger than that
in all columns, since the absolute error on higher counts tends to
be large. MRE is the opposite, as one can expect MRE in top 5%
columns would be smaller than in all columns since MRE evaluates
the relative error.
Overall, our DPSense and DPSense-S algorithms result in
signiﬁcant improvements over other approaches for both MAE and
MRE, especially when  = ln 3. We point out that GS and LPA
both rely on assuming that ∆D being public. Since LPA adds noise
of the same magnitude to all columns, the MAE for top-5% is the
same as that for all columns under LPA. For other methods, how-
ever, the MAE for top-5% is larger than that for all columns, as
these columns are kind of outliers. For datasets with small ∆D
such as Transaction and Document, LPA performs quite well, out-
performing methods other than DPSense and DPSense-S. How-
ever, LPA performs poorly on other datasets. Interestingly, ALPA
performs very similar to FPA, and their performance change little
when  increases from ln 1.5 to ln 3. This is understandable for
ALPA, since the main source of error comes from publishing the
same count for all columns, instead of from the added noise. For
FPA, since there is no obvious periodical behavior in the columns,
the optimal way of truncating the Fourier coefﬁcients is likely to
be essentially keeping the one corresponding to the average of all
columns, resulting in a behavior similar to ALPA. GS overall per-
forms well, however, in several instances its MAEs and MREs are
signiﬁcantly higher.
DPSense-S further boosts the performance over DPSense, ben-
eﬁting from the usage of θ and α, when considering the MAE and
MRE over all columns. When we consider only the top 5% of
columns, DPSense-S outperforms DPSense on NetFlix, Movie-
lens, AOL, and Kosarak, and under-performs DPSense on Trans-
457(a) Netﬂix dataset (average row count = 209.25)
(b) Transaction dataset (average row count = 6.53) (c) Movielens dataset (average row count = 139.72)
(d) Document dataset (average row count = 58.95)
(e) AOL dataset (average row count = 35.03)
(f) Kosarak dataset (average row count = 8.09)
Figure 1: The MAE and Quality function vs. θ for six datasets.
actions and Document. These two datasets have some columns
with very high counts contributed by rows with small total sums,
and the best overall scaling factor may over-corrects these high
counts. This is a side effect of the fact that our quality optimizes
for the overall MAE, instead of MAE for the top columns.
5.4 Accuracy on Top-k Columns
Table 3 and 4 present the results among top-k columns using
accuracy and NDCG, respectively. As introduced earlier, as the
dimensionality is very large, one may be more interested in ﬁnding
out the columns that have the highest counts.
Since ALPA publishes the same count for every column, one is
unable to ﬁnd top-k columns using ALPA. We omit this approach
in this experiment as it is not meaningful. On the other hand, we
consider another method, which computes the top-k columns using
the exponential mechanism to select each top column iteratively,
with quality function set as true column count, i.e., tc. This is the
method used to select top k columns in [19] and [2], which study
differentially private itemset mining. We denote this as EM, in Ta-
ble 3 and 4. DPSense and DPSense-S algorithms are abbrevi-
ated in this section as DPS and DPSS, respectively. We choose k
as 1%, 5%, and 10% of the number of columns in the dataset.
Overall, DPSense performed the best, followed by DPSense-
S, and by EM and GS. Similar to previous experiment, LPA bene-
ﬁts a lot from datasets with smaller sensitivity, such as Transaction
and Document, while it is worse in other datasets. FPA publishes
similar counts to every column due to the inverse of transformation
on truncated coefﬁcients, so it predicts very inaccurately on top-k
columns. EM needs to allocate the privacy budgets into k% times
the number of columns parts, so when k is small it performs reason-
020040060080010000100020003000400050006000argminθ MAE = 531argmaxθ Quality = 536argminθ MAE = 847argmaxθ Quality = 884MAE, ǫ=ln 1.5MAE, ǫ=ln 3Quality, ǫ=ln 1.5Quality, ǫ=ln 302004006008001000240025002600270028002900300031003200Avg(MAE+Quality), ǫ=ln 1.5Avg(MAE+Quality), ǫ=ln 312ac(D)θMAE / Quality Score020406080100120140−500050010001500200025003000argminθ MAE = 32argmaxθ Quality = 33argminθ MAE = 41argmaxθ Quality = 43MAE, ǫ=ln 1.5MAE, ǫ=ln 3Quality, ǫ=ln 1.5Quality, ǫ=ln 3020406080100120140100010051010101510201025103010351040Avg(MAE+Quality), ǫ=ln 1.5Avg(MAE+Quality), ǫ=ln 312ac(D)θMAE / Quality Score0100200300400500−1000−50005001000150020002500argminθ MAE = 121argmaxθ Quality = 89argminθ MAE = 284argmaxθ Quality = 243MAE, ǫ=ln 1.5MAE, ǫ=ln 3Quality, ǫ=ln 1.5Quality, ǫ=ln 30100200300400500300350400450500550600Avg(MAE+Quality), ǫ=ln 1.5Avg(MAE+Quality), ǫ=ln 312ac(D)θMAE / Quality Score050100150200250300350400010002000300040005000argminθ MAE = 95argmaxθ Quality = 98argminθ MAE = 108argmaxθ Quality = 111MAE, ǫ=ln 1.5MAE, ǫ=ln 3Quality, ǫ=ln 1.5Quality, ǫ=ln 3050100150200250300350400166016801700172017401760Avg(MAE+Quality), ǫ=ln 1.5Avg(MAE+Quality), ǫ=ln 312ac(D)θMAE / Quality Score050100150200−2000200400600800argminθ MAE = 55argmaxθ Quality = 49argminθ MAE = 110argmaxθ Quality = 112MAE, ǫ=ln 1.5MAE, ǫ=ln 3Quality, ǫ=ln 1.5Quality, ǫ=ln 3050100150200200220240260280300Avg(MAE+Quality), ǫ=ln 1.5Avg(MAE+Quality), ǫ=ln 312ac(D)θMAE / Quality Score020406080100−1000100200300400argminθ MAE = 15argmaxθ Quality = 13argminθ MAE = 35argmaxθ Quality = 32MAE, ǫ=ln 1.5MAE, ǫ=ln 3Quality, ǫ=ln 1.5Quality, ǫ=ln 302040608010080859095100105110Avg(MAE+Quality), ǫ=ln 1.5Avg(MAE+Quality), ǫ=ln 312ac(D)θMAE / Quality Score458(a) Netﬂix dataset
(b) Transaction dataset
(c) Movielens dataset
(d) Document dataset
(e) AOL dataset
(f) Kosarak dataset
Figure 2: The MAE results of approaches on different datasets.
(a) Netﬂix dataset
(b) Transaction dataset
(c) Movielens dataset
(d) Document dataset
(e) AOL dataset
(f) Kosarak dataset
Figure 3: The MRE results of approaches on different datasets.
ably well, but the accuracy signiﬁcantly drops as k increases. GS
beneﬁts from the grouping strategy, but if the estimated group size
differs too much from optimal group size, it easily groups dissim-
ilar columns together and publishes inaccurate counts. Also, even
if the estimated group size is close to optimal size like in Netﬂix
dataset, computing average within group also produces error. The
ǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%101102103104105106MAELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%101102103104105106MAELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%101102103104105106MAELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%101102103104105106MAELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%101102103104105106MAELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%101102103104105106MAELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%10-310-210-1100101102MRELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%10-310-210-1100101102MRELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%10-310-210-1100101102MRELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%10-310-210-1100101102MRELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%10-310-210-1100101102MRELPAALPAFPAGSDPSenseDPSense-Sǫ = ln 1.5Allǫ = ln 1.5Top 5%ǫ = ln 3Allǫ = ln 3Top 5%10-310-210-1100101102MRELPAALPAFPAGSDPSenseDPSense-S459Dataset
Netﬂix
Transaction
Movielens
Document
AOL
Kosarak
k
LPA
1% 0.170
5% 0.233
10% 0.260
1% 1.000
5% 0.973
10% 0.962
1% 0.023
5% 0.085
10% 0.140
1% 0.993
5% 0.963
10% 0.856
1% 0.018
5% 0.056
10% 0.106
1% 0.071
5% 0.084
10% 0.128
FPA
0.001
0.050
0.102
0.294
0.470
0.633
0.093
0.140
0.159
0.009
0.046
0.093
0.057
0.305
0.300
0.020
0.085
0.170
 = ln 1.5
GS
0.807
0.738
0.621
1.000
0.959
0.920
0.732
0.596
0.487
0.943
0.964
0.952
0.294
0.452
0.541
0.497
0.782
0.726
EM
0.997
0.961
0.810
1.000
0.986
0.965
0.975
0.746
0.434
0.975
0.336
0.209
0.826
0.146
0.136
0.432
0.088
0.115
DPS DPSS
0.981
0.961
0.967
0.951
0.939
0.937
1.000
1.000
0.990
0.988
0.989
0.988
0.887
0.901
0.894
0.891
0.843
0.828
0.998
0.996
0.990
0.990
0.980
0.976
0.948
0.934
0.894
0.880
0.807
0.770
0.761
0.756
0.849
0.845
0.710
0.711
LPA
0.753
0.565
0.475
1.000
0.988
0.988
0.169
0.203
0.242
0.998
0.987
0.964
0.036
0.068
0.116
0.185
0.144
0.174
FPA
0.000
0.050
0.102
0.294
0.470
0.633
0.093
0.140
0.158
0.009
0.047
0.092
0.057
0.305
0.300
0.018
0.093
0.159
 = ln 3
GS
0.851
0.843
0.776
1.000
0.984
0.973
0.777
0.777
0.680
0.945
0.965
0.960
0.294
0.452
0.541
0.492
0.756
0.813
EM
0.994
0.984
0.950
1.000
0.990
0.988
0.991
0.926
0.717
0.992
0.597
0.330
0.958
0.310