Electronics
Software
Mechanics
General
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(a) V¯u·Man 2R and Navigator 2.
Electronics
Manufacturing
Software
Mechanics
General
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(b) Spot.
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
20
15
10
5
0
60
50
40
30
20
10
0
Omission
Commission
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(a) V¯u·Man 2R and Navigator 2.
Omission
Commission
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(b) Spot.
Figure 8. Defect type distributions.
Figure 9. Defect class distributions.
riods of system design appears in Figure 8.† We note that
Electronics defects peak in Bringup for both sets of data,
but that the Software peak has shifted from Bringup in the
earlier data to Integration on Spot. In addition, Mechanics
defects peak in Operation for both sets; half of V¯u·Man 2R
and Navigator 2 Mechanics defects occur in this phase, as
do more than half of Spot Mechanics defects.
The next attribute is the class of a defect, which is ei-
ther an error of Omission — something was left out of the
design — or Commission — something in the design is in-
correct. Figure 9 shows a similar number of Omission and
Commission defects for the V¯u·Man 2R and Navigator 2
data. In the Spot data, the number of Commission defects
has more than tripled, so that the ratio of Omission defects
†The “Hardware” value from [1] has been renamed “Electronics,” “In-
dustrial/Mechanical” is now “Mechanics,” and “General/Other” is now
“General.” “Manufacturing” has been added, due to the large number of
fabrication, assembly, and rework defects observed for Spot.
to Commission defects is 1:3.4. We attribute this change to
an increase in electronic complexity. Also, Figure 9 shows
fewer Omission defects for Spot in the Operation phase —
about 1% of all defects — than for the earlier systems. We
associate the earlier fraction — about 12% of all defects —
with the relative newness of wearable systems at the time;
some design aspects simply were not considered until ﬁeld
deployment.
The fourth attribute is the defect trigger, which is es-
sentially the mechanism by which a defect was discovered.
There are three main trigger values: Review and Inspection,
Unit Test, and System and Field Test. Review and Inspec-
tion covers everything from formal engineering reviews to
visual board inspection; it represents methods for discover-
ing defects without actually trying to exercise the system.
Unit Test covers directed tests of isolated features, while
System and Field Test involves integrated subsystems oper-
ating together. Figure 10 shows that the trigger distributions
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:04:38 UTC from IEEE Xplore.  Restrictions apply. 
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
25
20
15
10
5
0
45
40
35
30
25
20
15
10
5
0
Review and Inspection
Unit Test
System and Field Test
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(a) V¯u·Man 2R and Navigator 2.
Review and Inspection
Unit Test
System and Field Test
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(b) Spot.
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
15
10
5
0
70
60
50
40
30
20
10
0
New Software
New Hardware
Reused Hardware
Physical
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(a) V¯u·Man 2R and Navigator 2.
Vendor Software
New Software
Reused Software
Vendor Hardware
New Hardware
Reused Hardware
Physical
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(b) Spot.
Figure 10. Defect trigger distributions.
Figure 11. Defect source distributions.
across the two data sets are similar. As a relative fraction of
total defects discovered by each trigger, the greatest varia-
tion between the two data sets is a 10% increase in Unit Test
for Spot. Within each phase, the relative fraction of defects
found by a trigger is always within 6% between the data
sets, except in Bringup where Unit Test reveals 14% more
defects on Spot. We interpret this as indicating the relative
stability of the trigger attribute deﬁnition across designs.
The source of a defect is Hardware, Software, or Phys-
ical; additionally, Hardware and Software can be provided
by a Vendor, can be New for the current design, or can be
Reused from an earlier version. Figure 11 shows that for
V¯u·Man 2R and Navigator 2, New Hardware defects were
distributed evenly across the last three phases; the variation
between them was within 2.5% of the total defect count.
More than half of New Hardware defects on Spot were dis-
covered in Bringup. This is, again, reﬂective of the in-
creased electronic complexity of the newer design. Also,
Physical defects peak for both data sets in Operation, at 7%
of the total count for V¯u·Man 2R and Navigator 2, and 6%
for Spot. This corresponds with the increased opportunities
for physical stress during ﬁeld use.
The sixth and ﬁnal attribute we present is impact, which
measures the severity of a defect. A Critical Defect pre-
vents the system from functioning; Performance and Relia-
bility address efﬁciency and consistency, respectively. Lost
Objective defects prevent the system from fully satisfying
design goals; for example, by failing to ensure that two sys-
tem features can be used concurrently. Usability defects
make the system difﬁcult to understand or use, and Main-
tainability defects make the system hard to service. A new
value, Minor, captures those defects which do not affect
functionality, but which should be corrected for æsthetic
reasons. Figure 12 shows that more than half of all Spot
defects were Critical Defects, compared with only 28% in
the earlier designs. Of the Critical Defects discovered on
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:04:38 UTC from IEEE Xplore.  Restrictions apply. 
Critical Defect
Performance
Reliability
Lost Objective
Usability
Maintenance
Attribute
Electronics peak
Software peak
Mechanics peak
Omission:
Comission
New Hardware
Signiﬁcant
impact
V¯u·Man 2R,
Navigator 2
2: Bringup
2: Bringup
4: Operation
1:1
similar across
phases 2–4
30% lost
objective
Spot
2: Bringup
3: Integration
4: Operation
1:3.4
more than half
in 2: Bringup
52% critical
errors
1: Design
2: Bringup 3: Integration 4: Operation
Table 3. ODC attribute summary.
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
s
t
c
e
f
e
d
f
o
r
e
b
m
u
n
15
10
5
0
60
50
40
30
20
10
0
phase found
(a) V¯u·Man 2R and Navigator 2.
Critical Defect
Performance
Reliability
Lost Objective
Usability
Minor
1: Design
2: Bringup 3: Integration 4: Operation
phase found
(b) Spot.
Figure 12. Defect impact distributions.
Spot, more than half were found in Bringup, and more than
80% were found between Bringup and Integration. Most
importantly, the Critical Defect, Lost Objective, and Reli-
ability categories combined account for 74% of all defects
in the earlier data, and 80% on Spot. The relative ordering
within these three categories has changed, with Critical De-
fects becoming 24% more prevalent, Lost Objective defects
becoming 22% less frequent, and Reliability defects staying
about the same on the newer design.
6. Conclusions
Custom wearable systems are becoming more complex
as a result of increasing interoperability requirements. This
complexity manifests itself in attributes such as chip, con-
nector, and pin counts, but also in the effort spent on debug-
ging new designs. An increasing proportion of observed
defects are designer errors as opposed to forgotten features,
and these defects are becoming more severe. Overall design
effort is shifting towards the Bringup phase, where most de-
fects related to newly-designed hardware are surfacing. De-
spite these changes and others summarized in Table 3, the
methods used to discover defects appear to be holding rela-
tively constant.
References
[1] A. Amezquita and D. P. Siewiorek. Orthogonal Defect Clas-
siﬁcation Applied to a Multidisciplinary Design. Technical
Report 05-100-96, Carnegie Mellon University Engineering
Design Research Center, Pittsburgh, PA, 1996.
[2] C. H. Amon, E. R. Egan, A. Smailagic, and D. P. Siewiorek.
Thermal Management and Concurrent System Design of a
Wearable Multicomputer.
IEEE Transactions on Compo-
nents, Packaging, and Manufacturing Technology, 20(2):128–
137, June 1997.
[3] W. Barﬁeld and T. Caudell. Fundamentals of Wearable Com-
puters and Augmented Reality. Lawrence Erlbaum Asso-
ciates, 2001.
[4] I. Bhandari, M. J. Halliday, et al.
through defect data interpretation.
33(1):182–214, 1994.
In-process improvement
IBM Systems Journal,
[5] R. Chillarege. ODC for Process Measurement, Analysis and
Control. In Proceedings of the Fourth International Confer-
ence on Software Quality, Maryland, Oct. 1994.
[6] A. Smailagic and D. P. Siewiorek. Modalities of Interaction
with CMU Wearable Computers. IEEE Personal Communi-
cations, 3(1):14–25, Feb. 1996.
[7] A. Smailagic, D. P. Siewiorek, D. Anderson, C. Kasabach,
T. Martin, and J. Stivoric. Benchmarking An Interdisciplinary
Concurrent Design Methodology for Electronic/Mechanical
Systems. In Proceedings of the 32nd ACM/IEEE Conference
on Design Automation, pages 514–519, San Francisco, CA,
1995.
[8] A. Smailagic, D. P. Siewiorek, R. Martin, and J. Stivoric.
Very Rapid Prototyping of Wearable Computers: A Case
Study of Custom versus Off-the-Shelf Design Methodologies.
In Proceedings of the 34th Design Automation Conference
(DAC97), pages 315–320, Anaheim, CA, June 1996.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:04:38 UTC from IEEE Xplore.  Restrictions apply.