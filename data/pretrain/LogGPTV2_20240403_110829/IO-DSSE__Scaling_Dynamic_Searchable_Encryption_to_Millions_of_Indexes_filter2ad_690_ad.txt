if |O(cid:48)
countw ← countw + 1
w ← ﬁrst maxb items in O(cid:48)
O(cid:48)(cid:48)
hwrite(Fkf (w||countw), O(cid:48)(cid:48)
w ← O(cid:48)(cid:48)
O(cid:48)
end if
S ← (S − {(w, Ow)}) ∪ {(w, O(cid:48)
w − O(cid:48)
w
end for
for (cid:96) ∈ {L, . . . , 0} do
w
w, Ms)
w)}
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
P (posw(cid:48), (cid:96))}
S(cid:48) ← {(w(cid:48), Ow(cid:48)) ∈ S : P (x, (cid:96)) =
S(cid:48) ← Select min(|S(cid:48)|, Z) blocks from S(cid:48).
S ← S − S(cid:48)
WRITEBUCKET(P (x, (cid:96)), S(cid:48))
for (w, Ow) ∈ S(cid:48) do
(cid:96)w ← (cid:96)
rw ← 0
Bw ← ∅
posw
hwrite(w, [posw, (cid:96)w, countw, rw, Bw],
R← {0, . . . , 2L}
Mc)
end for
end for
sizec ← 0
38:
39:
40:
41: end if
Fig. 5: Update for our DSSE scheme
the pattern of searches for wi in the history H. Note that this
does not leak wi itself, but only the location of all search/update
queries for wi in the sequence all previous read/updates. On
an update query di, the leakage function L(DBi−1, H) leaks
the total number of keywords in di, and also the total number
of keywords in di for which the number of documents in the
index has reached the multiple of our designated block-size.
Again, this does not leak what the actual keywords are and
how they are related to previous searches/updates (no leakage
of patterns). This leakage on updates simply captures the fact
that the server learns when full-blocks are pushed from the
8
partial-block index to the full-block index.
Theorem 1: Our SSE Scheme is L-secure (see deﬁnition
in Section II-D), if F is a pseudorandom function, and E is a
CPA-secure encryption scheme.
Proof Sketch: First, we need to describe a simulator S that
given access to the leakage function describe above, simulates
the adversary A (i.e. untrusted server’s) view in the real world.
Description of the Simulator: The simulator initializes a
local position map that it uses for bookkeeping, just as the
honest client would.
On each update query di (or many updates if they are
batched), the simulator learns the number of keywords in di,
n. It also learns the total number of keywords in di that just
reached a full block (m) and need to be pushed to the full-block
index. It also knows (from the state it is keeping) the location
of the leaf for all deferred reads since the last update. The
simulator behaves similar to the honest client, except that he
generates the leaf location for each keyword being updated
uniformly at random and the m locations in the full-block
index where the newly ﬁlled blocks will go also uniformly at
random. It keeps record of all these locations in its position
map. Also, for all encrypted blocks it needs to send, it simply
generates fresh encryptions of dummy values using a random
encryption key it generates.
On each search query for wi, S learns (from the leakage
function) all previous occurrences of search/update for wi. If
this is the ﬁrst occurrence, it chooses a random entry on the
ORAM tree for the OUI, a sequence of random locations that
have not yet been looked up in the full-block index, and also
stored these locations for bookkeeping. But if it is not the ﬁrst
occurrence, S has previously stored the locations in the partial-
block and full-block index it had sent to the server. It simply
sends the same locations to the server again. This completes
the description of the simulator.
We need to show that the simulated view above is indistin-
guishable from the view of the adversary in the real execution.
We do so using a sequence of hybrids.
H0: In the ﬁrst hybrid, the adversary is interacting with the
simulator described above. In other words, all ”random”
locations are generated uniformly at random and indepen-
dently, and all encrypted values are dummy values.
H1: In the second hybrid, all locations that were generated
uniformly at random are instead generated by the client
using a PRF with a random key kf not known to the
adversary.
Note that the advantage of an adversary in distinguishing
these two hybrids is bounded by the advantage of an
adversary in breaking the PRF security.
H2: In the third hybrid, all dummy encryptions are replaced by
the encryption of actual document identiﬁers (chosen by
the adversary) using a semantically secure symmetric-key
encryption as prescribed in the real protocol.
The advantage of the adversary in distinguishing the two
games is bounded by the security of the CPA-secure
encryption scheme used to encrypt document identiﬁers.
It only remains to show that the view of the adversary in
H2 is identical to the real protocols described earlier in this
Section. We consider adversary’s view for the search queries
and update queries separately. On update queries, both in the
real protocol and in H2, the location to be updated in the OUI
is generated using a PRF as described in the protocol, and so is
the location to be added to the full-block index for all keywords
that have reached a full block during this update. Similarly, in
both cases, the real document keywords used for generating
locations and the real document identiﬁers are encrypted. So
the two views are identical.
In case of read queries, if the keyword is being searched
for the ﬁrst time, again the leaf location to be looked up in
both the real protocol and H2 are generated using a PRF, and
for repeated reads, in both cases, the exact same locations in
the OUI and the full-block index are looked up. Hence the two
views are identical. This concludes the proof sketch.
IV. EVALUATION
In this section we assess the feasibility and performance of
our SSE scheme for email, using experiments that are based on
real data. The two important requirements we focus on in our
evaluation are (1) storage usage both on the server side and
the client side, (2) IO performance on the server side. Again,
as existing (non-encrypted) mail search is already IO bound,
our primary concern is the second criteria.
A. Real World Data
Note that the performance of any SSE scheme critically
depends on two main pieces of information: (1) the distribution
and frequency of updates to the index, i.e., new keyword-
document pairs added to the search index, and (2) distribution
and frequency of the keywords being searched.
Data on Index Updates for Email Our email data comes
from Yahoo Webmail. Yahoo mail has hundreds of millions of
monthly active users and maintains a 30 day rolling window of
customer email for research. This dataset consists of the sent
and received emails of over one hundred thousand users who
opted into email collection for research. For privacy reasons,
we extract only the frequency of keywords, not the actual words
themselves and all analysis of the data itself was performed
directly on the map-reduce cluster containing the dataset: only
the counts of each token, not the tokens themselves, were
exported. On average, users receive 30 emails per day with a
standard deviation of 148. Furthermore, each message contains
an average of 143 (unstemmed) keywords in it with a standard
deviation of 140.
For all keyword data, we strip HTML from the messages
and use the standard tokenizer and stopword list from Apache
Lucene.5 No stemming or other ﬁltering was applied. Between
this and the fact that the dataset spanned more than just English,
our data represents an upper bound for the “index everything”
approach. We identiﬁed a total of 62,131,942 keywords in our
dataset across all 100K users’ email. As the estimated number
of words in the English language is in the order of one million,
there is ample room for stemming and ﬁltering to drastically
reduce this. Nevertheless, we stick with this number for our
experiments as an empirical worst case measurement.
5We used the list StopAnalyzer.ENGLISH_STOP_WORDS_SET.
9
Approximating query data For two reasons, we do not have
access to comprehensive query data. First, there is no such
dataset of users’ queries for which users explicitly opted in.
Second, existing mail search supports more than single keyword
queries and as such queries are often in natural language. It is
unclear how to appropriately generate keyword searches from
such data. As such, for the sake of experiments, we assume
search terms are selected uniformly at random from the set of
all indexed words.
Given the low number of queries, we anticipate that the
overall effect of this is minimal. The primary concern in our
evaluation is the effect of the large number of updates on
storage and performance requirements.
B. IO Performance of IO-DSSE
We now examine the IO performance of IO-DSSE relative
to the operating conditions we observe at Yahoo. Our goal is
to measure the ability of our scheme to scale out to millions of
concurrent instances on arbitrary (and likely proprietary) cloud
infrastructure. As a result, we model such an system abstractly
as a key value store and measure the number of reads and
writes against it. We implement our scheme in Python against
this abstraction.
Our measurements are taken over 30 days of trafﬁc
generated using the sampled distribution of keywords discussed
above and is repeated for 50 iterations. We assume that the
system pushes all client-side email messages to the server
every day. This models a mobile device that has access to a
free Internet connection when at home. Given that we ﬁx the
distribution of keywords, we are left with three variables: the
number of searches, the number of emails per day, and the
number of keywords per email.
For simplicity, we ﬁx the number of keywords at 350 (this is
two standard deviations above the average), and vary the number
of email messages starting with the average and incrementing
by the standard deviation. This has the net effect of changing
the total number of updates per day which (along with the
distribution of keywords) is the actual controlling variable for
performance. Similarly, we ﬁx the number of searches at one
per day, modeling an active user (at Yahoo the average user
searches message content once per month.).
Finally, we somewhat arbitrarily ﬁx the Path ORAM
parameters, assuming a height of 17 at 4 buckets per level
with each ORAM entry containing a block of 500 identiﬁers
(at 64 bits per ﬁle ID, this gives us blocks that approximately
ﬁt in a 4KB disk block). Real deployments should tune these
parameters dependent on system architecture and testing. We
stress again that our goal is not to see how our system handles
large indexes—individual mail inboxes are at most tens of
gigabytes—but to measure the resources used by a small index.
This allows us to see how costly it is to deploy in a setting with
hundreds of millions of inboxes and thus indexes supported by
a minimal number of servers.
Our experiment measures the actual IO savings of our
scheme against:
• An encrypted index which, for updates, stores each
keyword-document pair at a random location. This repre-
sents the state of the art schemes under purely dynamic
insertion [5, 20], which perform identically.
• Our solution but with a naive implementation of an
Obliviously Updatable Index built with Path ORAM.
Because we are mainly concerned with the IO cost, it was
not necessary to implement the comparison systems. For the
current state of the art encrypted index, the number of random
writes will simply be equal to the number of inserted keywords,
and the number of random reads equal to the number of search
results. This provides a lower bound on the number of accesses
required for all of the DSSE schemes we compare with.
For a solution that uses the Path ORAM scheme as the OUI,
the cost of a search is dominated by one ORAM read and one
ORAM write. An ORAM “read” would be recorded as many
reads by our code as each level in the path generates a distinct
access. Thus, instead of one non-deferred read as in our scheme,
we charge oramHeight reads and oramHeight writes. By
read or write, we mean the access of a single key-value pair
in the underlying server-side data structure.
A third approach we could compare against is a variant
of Cash et al. scheme [4] where full-blocks are stored on the
server-side, and partial blocks are stored locally on the client
side. This would exhibit better performance than our scheme
in the short term, because it reduces IO costs compared to an
OUI. However, when the client-side storage becomes full, the
blocks need to be pushed to the index even if they are only
partially full and indeed maybe almost empty. As we will see
in the storage discussion, this does not work in practice as
the client would need to regularly push partially-ﬁlled blocks
to the static index, hence defeating the IO efﬁciency gains of
packing many documents identiﬁers into one block.
Using our implementation, we measure:
Total IO savings: The total amount of IO saved compared to
the existing approach of SSE schemes (including [5, 20]
under purely dynamic insertion). This includes both search
and update.
IO savings for search: The amount of savings on read due
to search, ignoring deferred reads. This represents the
immediate cost of a search and also the associated latency
savings.
IO savings vs. ORAM: Total IO savings when using an OUI
vs. ORAM. This is the savings due to our optimized
obliviously updatable index that does not require full
ORAM security.
The results show (see Figure 6) a 99% percent reduction
in the IO cost of our scheme compared to a scheme that does
one random read per search result and one random write per
keyword-document pair. This is slightly different from the naive
estimate for the savings of simply batching IO into contiguous
blocks of 500 (our chosen block size), i.e., x− x
x = 0.998 due
to both the overhead of access to our obliviously updatable
index and the fact that entries will not be packed perfectly,