User: How to use Python to write a web crawler to search for information on a specified website?
Assistant: The following is the basic process of using Python to write a web crawler to search for information on a specified website: 1. Select a target website, and use Python's requests module to send a GET request to obtain the HTML source code of the website. 2. Use Python's BeautifulSoup module to parse the HTML source code of the website and find the web page elements where the required information is located. 3. According to the format of the required information, use Python regular expressions or other methods to extract the required information. 4. Store the extracted information in a local file or database. 5. Perform anti-crawler mechanism processing on the target website, such as setting headers, simulating browser operations, etc. 6. Establish a loop control process to improve the stability and efficiency of the program. The following is a simple Python crawler code example for crawling Douban movie TOP250 movie names and rating information: ```python import requests from bs4 import BeautifulSoup url = 'https://movie.douban.com/top250' headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'} res = requests.get(url, headers= headers) soup = BeautifulSoup(res.text, 'html.parser') for movie in soup.select('.info'): title = movie.select_one('.title').text rating = movie.select_one('. rating_num').text print(title, rating) ``` In this code, we first set the request header, simulate the browser operation, and then send a GET request to the target website to obtain the HTML source code of the website. Then use the BeautifulSoup module to parse the HTML source code, and select the web page elements containing the movie name and rating. Finally, use the for loop to traverse each selected movie element, extract the movie title and rating information, and print them out.