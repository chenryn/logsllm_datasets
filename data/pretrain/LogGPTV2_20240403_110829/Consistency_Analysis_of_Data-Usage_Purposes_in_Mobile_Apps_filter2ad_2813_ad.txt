(e.g., data object ontology and purpose taxonomy) which defines
relationships among the terms used. We extend the semantic equiv-
alence, subsumptive relationship and semantic approximation of
PoliCheck [8] to data-usage purposes as listed in Table 7. 𝑅1 is
defined in Definition 6.4, 𝑅2 – 𝑅4 are defined in Definition 6.5, and
𝑅5 – 𝑅9 are defined in Theorem 6.6 (proved in Appendix D).
Definition 6.1 (Semantic Equivalence). 𝑥 ≡𝑜 𝑦 means that 𝑥 and
𝑦 are synonyms, defined under an ontology 𝑜.
Definition 6.2 (Subsumptive Relationship). Given an ontology 𝑜
represented as a directed graph in which each node is a term and
each edge points from a general term 𝑦 to a specific term 𝑥 included
in 𝑦 (i.e., 𝑥 "is a" instance of 𝑦), 𝑥 <𝑜 𝑦 means there is a path from
𝑦 to 𝑥 and 𝑥 (cid:46)𝑜 𝑦. Similarly, 𝑥 ⊑𝑜 𝑦 ⇔ 𝑥 <𝑜 𝑦 ∨ 𝑥 ≡𝑜 𝑦 and
𝑥 =𝑜 𝑦 ⇔ 𝑦 <𝑜 𝑥.
Definition 6.3 (Semantic Approximation). The semantic approxi-
mation relationship between two terms 𝑥 and 𝑦, denoted as 𝑥 ≈𝑜 𝑦, is
true if and only if ∃𝑧 such as 𝑧 <𝑜 𝑥 ∧ 𝑧 <𝑜 𝑦 ∧ 𝑥 (cid:64)𝑜 𝑦 ∧ 𝑦 (cid:64)𝑜 𝑥.
Definition 6.4 (Purpose Equivalence). Two data-usage purposes
are semantically equivalent (𝑒𝑖, 𝑞𝑖) ≡𝜋 (𝑒 𝑗 , 𝑞 𝑗) if and only if there
exist ontologies 𝜖 and 𝜅 such that 𝑒𝑖 ≡𝜖 𝑒 𝑗 ∧ 𝑞𝑖 ≡𝜅 𝑞 𝑗 .
Definition 6.5 (Purpose Subsumption). (𝑒𝑖, 𝑞𝑖) <𝜋 (𝑒 𝑗 , 𝑞 𝑗) if and
only if there exist ontologies 𝜖 and 𝜅 such that 𝑒𝑖 <𝜖 𝑒 𝑗 ∧ 𝑞𝑖 ≡𝜅 𝑞 𝑗
or 𝑒𝑖 ≡𝜖 𝑒 𝑗 ∧ 𝑞𝑖 <𝜅 𝑞 𝑗 or 𝑒𝑖 <𝜖 𝑒 𝑗 ∧ 𝑞𝑖 <𝜅 𝑞 𝑗 .
Theorem 6.6 (Purpose Semantic Approximation). Given two data-
usage purposes 𝑝𝑖 = (𝑒𝑖, 𝑞𝑖) and 𝑝 𝑗 = (𝑒 𝑗 , 𝑞 𝑗), there exist ontologies
𝜖, 𝜅, and 𝜋 such that
(1) 𝑒𝑖 ≡𝜖 𝑒 𝑗 ∧ 𝑞𝑖 ≈𝜅 𝑞 𝑗 ⇒ 𝑝𝑖 ≈𝜋 𝑝 𝑗 ,
(2) 𝑒𝑖 <𝜖 𝑒 𝑗 ∧ 𝑞𝑖 ≈𝜅 𝑞 𝑗 ⇒ 𝑝𝑖 ≈𝜋 𝑝 𝑗 ,
(3) 𝑒𝑖 ≈𝜖 𝑒 𝑗 ∧ 𝑞𝑖 ≡𝜅 𝑞 𝑗 ⇒ 𝑝𝑖 ≈𝜋 𝑝 𝑗 ,
(4) 𝑒𝑖 ≈𝜖 𝑒 𝑗 ∧ 𝑞𝑖 <𝜅 𝑞 𝑗 ⇒ 𝑝𝑖 ≈𝜋 𝑝 𝑗 , and
(5) 𝑒𝑖 ≈𝜖 𝑒 𝑗 ∧ 𝑞𝑖 ≈𝜅 𝑞 𝑗 ⇒ 𝑝𝑖 ≈𝜋 𝑝 𝑗
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2831Relation
𝑅1
𝑅2
𝑅3
𝑅4
𝑅5
𝑅6
𝑅7
𝑅8
𝑅9
𝑒𝑖 · 𝑒 𝑗
𝑒𝑖 ≡𝜖 𝑒 𝑗
𝑒𝑖 ≡𝜖 𝑒 𝑗
𝑒𝑖 <𝜖 𝑒 𝑗
𝑒𝑖 <𝜖 𝑒 𝑗
𝑒𝑖 ≡𝜖 𝑒 𝑗
𝑒𝑖 <𝜖 𝑒 𝑗
𝑒𝑖 ≈𝜖 𝑒 𝑗
𝑒𝑖 ≈𝜖 𝑒 𝑗
𝑒𝑖 ≈𝜖 𝑒 𝑗
𝑞𝑖 · 𝑞 𝑗
𝑞𝑖 ≡𝜅 𝑞 𝑗
𝑞𝑖 <𝜅 𝑞 𝑗
𝑞𝑖 ≡𝜅 𝑞 𝑗
𝑞𝑖 <𝜅 𝑞 𝑗
𝑞𝑖 ≈𝜅 𝑞 𝑗
𝑞𝑖 ≈𝜅 𝑞 𝑗
𝑞𝑖 ≡𝜅 𝑞 𝑗
𝑞𝑖 <𝜅 𝑞 𝑗
𝑞𝑖 ≈𝜅 𝑞 𝑗
𝑝𝑖 · 𝑝 𝑗
𝑝𝑖 ≡𝜋 𝑝 𝑗
𝑝𝑖 <𝜋 𝑝 𝑗
𝑝𝑖 <𝜋 𝑝 𝑗
𝑝𝑖 <𝜋 𝑝 𝑗
𝑝𝑖 ≈𝜋 𝑝 𝑗
𝑝𝑖 ≈𝜋 𝑝 𝑗
𝑝𝑖 ≈𝜋 𝑝 𝑗
𝑝𝑖 ≈𝜋 𝑝 𝑗
𝑝𝑖 ≈𝜋 𝑝 𝑗
Table 7: Data-usage purpose relationships. 𝑝𝑖 = (𝑒𝑖, 𝑞𝑖) and
𝑝 𝑗 = (𝑒 𝑗 , 𝑞 𝑗). · denotes a relationship placeholder. 𝑅1 – 𝑅4 are
definitions, 𝑅5 – 𝑅9 are theorems.
Rule Logic
𝐶1
𝑑𝑘 ≡𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≡𝜋 𝑝𝑛
𝑑𝑘 ≡𝛿 𝑑𝑙 ∧ 𝑝𝑚 <𝜋 𝑝𝑛
𝑑𝑘 <𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≡𝜋 𝑝𝑛
𝑑𝑘 <𝛿 𝑑𝑙 ∧ 𝑝𝑚 <𝜋 𝑝𝑛
𝑑𝑘 =𝛿 𝑑𝑙 ∧ 𝑝𝑚 <𝜋 𝑝𝑛
𝑑𝑘 ≡𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≈𝜋 𝑝𝑛
𝑑𝑘 <𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≈𝜋 𝑝𝑛
𝑑𝑘 =𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≈𝜋 𝑝𝑛
𝑑𝑘 ≈𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≡𝜋 𝑝𝑛
𝑑𝑘 ≈𝛿 𝑑𝑙 ∧ 𝑝𝑚 <𝜋 𝑝𝑛
𝑑𝑘 ≈𝛿 𝑑𝑙 ∧ 𝑝𝑚 =𝜋 𝑝𝑛
𝑑𝑘 ≈𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≈𝜋 𝑝𝑛
𝑑𝑘 ≡𝛿 𝑑𝑙 ∧ 𝑝𝑚 =𝜋 𝑝𝑛
𝑑𝑘 <𝛿 𝑑𝑙 ∧ 𝑝𝑚 =𝜋 𝑝𝑛
𝑑𝑘 =𝛿 𝑑𝑙 ∧ 𝑝𝑚 ≡𝜋 𝑝𝑛
𝑑𝑘 =𝛿 𝑑𝑙 ∧ 𝑝𝑚 =𝜋 𝑝𝑛
𝐶2
𝐶3
𝐶4
𝐶5
𝐶6
𝐶7
𝐶8
𝐶9
𝐶10
𝐶11
𝐶12
𝑁1
𝑁2
𝑁3
𝑁4
Example
(Device ID, k, Advertising)
(Device ID, ¬k, Advertising)
(Device ID, k, Advertising)
(Device ID, ¬k, Marketing)
(Device ID, k, Advertising)
(Device info, ¬k, Advertising)
(Device ID, k, Advertising)
(Device info, ¬k, Marketing)
(Device info, k, Advertising)
(Device ID, ¬k, Marketing)
(Device ID, k, Advertising)
(Device ID, ¬k, Personalization)
(Device ID, k, Advertising)
(Device info, ¬k, Personalization)
(Device info, k, Advertising)
(Device ID, ¬k, Personalization)
(Device ID, k, Advertising)
(Tracking ID, ¬k, Advertising)
(Device ID, k, Advertising)
(Tracking ID, ¬k, Marketing)
(Device ID, k, Marketing)
(Tracking ID, ¬k, Advertising)
(Device ID, k, Advertising)
(Tracking ID, ¬k, Personalization)
(Device ID, k, Marketing)
(Device ID, ¬k, Advertising)
(Device ID, k, Marketing)
(Device info, ¬k, Advertising)
(Device info, k, Advertising)
(Device ID, ¬k, Advertising)
(Device info, k, Marketing)
(Device ID, ¬k, Advertising)
Table 8: Logical forms of logical contradictions (𝐶) and nar-
rowing definitions (𝑁 ). 𝑘 and ¬𝑘 abbreviate for and not_for,
respectively. The data flow has data type 𝑓𝑑 = IMEI and pur-
pose 𝑓𝑞 = Personalize ad.
6.2 Policy Contradictions
Definition 6.7 (Privacy Statement Contradiction). Two privacy
statements 𝑡𝑘 = (𝑑𝑐𝑘, 𝑑𝑢𝑘) and 𝑡𝑙 = (𝑑𝑐𝑙 , 𝑑𝑢𝑙) are said to contradict
each other iff either 𝑑𝑐𝑘 contradicts 𝑑𝑐𝑙 or 𝑑𝑢𝑘 contradicts 𝑑𝑢𝑙 .
PurPliance’s consistency analysis comprises two steps. Using
the Definition 6.7 of contradiction between two privacy statements,
it checks the consistency of 𝑑𝑐 and 𝑑𝑢 tuples in this order. The
consistency of 𝑑𝑐𝑘 = (𝑟𝑘, 𝑐𝑘, 𝑑𝑘) and 𝑑𝑐𝑙 = (𝑟𝑙 , 𝑐𝑙 , 𝑑𝑙) is analyzed
by a Data Collection consistency model. PurPliance leverages
the PoliCheck consistency model in this analysis. However, the
PoliCheck consistency model cannot check the two policy state-
ments if both have a positive sentiment (i.e., 𝑐𝑘 = 𝑐𝑙 = collect) or
X does not collect Y for Z
X collects Y for Z
X does not collect Y X collects Y
Consistent
Consistent
Contradictory
Consistent
Table 9: Privacy-statement comparison when one of the
statement has no data usage purpose specified (𝑑𝑢 = 𝑁𝑜𝑛𝑒).
the two receivers do not have either a subsumptive or semantic
approximation relationship. In such cases, since no contradiction
was detected, PurPliance checks the consistency of data usage
statements 𝑑𝑢𝑘 and 𝑑𝑢𝑙 using a Data Usage consistency model. We
extend the PoliCheck model [8] for data usage purposes as follows.
The contradiction conditions and types of two data usage tu-
ples 𝑑𝑢𝑘 = (𝑑𝑘, for, 𝑝𝑚) and 𝑑𝑢𝑙 = (𝑑𝑙 , not_for, 𝑝𝑛) are listed in
Table 8. There are 16 cases and 2 types of contradictions: logical
contradictions (𝐶1–𝐶12) and narrowing definitions (𝑁1–𝑁4). Logical
contradictions occur when 𝑑𝑢𝑙 states the exclusion of a broader
purpose from data usage while 𝑑𝑢𝑘 states the usage for a purpose
type in a narrower scope. On the other hand, narrowing defini-
tions have the not-for-purpose statement (where 𝑘 = not_for) in
a narrower scope than their counterparts. Narrowing definitions
may confuse readers and automatic analysis when interpreting the
privacy statements, especially when the two statements are far
apart in a document.
When two privacy statements are compared, if one of them has
no data-usage purpose specified (i.e., du = None), PurPliance flags
a contradiction only if they have forms ((𝑟𝑘, 𝑛𝑜𝑡_𝑐𝑜𝑙𝑙𝑒𝑐𝑡, 𝑑𝑘), 𝑁𝑜𝑛𝑒)
and ((𝑟𝑙 , 𝑐𝑜𝑙𝑙𝑒𝑐𝑡, 𝑑𝑙), (𝑑𝑙 , 𝑓 𝑜𝑟, 𝑝𝑙)), i.e., the positive-sentiment state-
ment has 𝑘𝑙 = 𝑓 𝑜𝑟. Following this rule, "X does not collect Y" does
not contradict "X does not collect Y for Z" as they are translated to
((X, not_collect, Y), None) and ((X, collect, Y), (Y, not_for, Z)), respec-
tively. Table 9 lists the cases of this rule.
Example 2. Given two statements: "we use your personal data
only for providing the App" and "advertisers may use your device
ID to serve you with advertisements," a contradiction is detected as
follows. Due to the keyword only for, PurPliance excludes third
parties’ Marketing purposes that are not for providing the app and
translates the first sentence to 1 positive and 1 negated statement:
𝑠1
1 = (we, collect, personal data), (personal data, for, (anyone, Provide
service)), 𝑠2
1 = (third party, collect, personal data), (personal data,
not_for, (third party, Marketing)). The second sentence is translated
to 𝑠2 = (advertiser, collect, device ID), (device ID, for, (advertiser, Pro-
vide ad)). Since device ID < personal data, advertiser < third party
and Provide ad < Marketing, the first sentence’s negated statement
𝑠2
1 contradicts 𝑠2 of the second sentence under rule 𝐶4. PolicyLint
will not flag these sentences because it considers only the collection
tuples which are all positive sentiments in these sentences.
6.3 Flow Consistency Analysis
Definition 6.8 (Flow-relevant Privacy Statements). A privacy state-
ment 𝑡𝑓 = ((𝑟𝑡 , 𝑐𝑡 , 𝑑𝑡), (𝑑𝑡 , 𝑘𝑡 , (𝑒𝑡 , 𝑞𝑡))) is relevant to a flow 𝑓 =
(𝑟, 𝑑, (𝑒, 𝑞)) (denoted as 𝑡𝑓 ≃ 𝑓 ) if and only if 𝑟 ⊑𝜌 𝑟𝑡 ∧ 𝑑 ⊑𝛿
𝑑𝑡 ∧ 𝑒 ⊑𝜖 𝑒𝑡 ∧ 𝑞 ⊑𝜅 𝑞𝑡 . Let 𝑇𝑓 be the set of flow-𝑓 -relevant privacy
statements in the set of privacy statements 𝑇 of a privacy policy, then
𝑇𝑓 = {𝑡 | 𝑡 ∈ 𝑇 ∧ 𝑡 ≃ 𝑓 }.
Session 10D: Applied Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2832Definition 6.9 (Flow-to-Policy Consistency). A flow 𝑓 is said to be
consistent with a privacy policy 𝑇 iff ∃𝑡 ∈ 𝑇𝑓 such that 𝑐𝑡 = collect ∧
𝑘𝑡 = for and (cid:154)𝑡 ∈ 𝑇𝑓 such that 𝑐𝑡 = not_collect ∨ 𝑘𝑡 = not_for.
A data flow is inconsistent with a privacy policy if the Flow-to-
Policy Consistency condition is not met. For each flow extracted
from app behavior, PurPliance first finds the flow-relevant privacy
statements 𝑇𝑓 and classifies the flow as consistent or inconsistent
using the above definitions. Although finer-grained consistency
types can be used, such as Clear and Ambiguous disclosures as
in PoliCheck, we leave it as future work. For brevity, the defini-
tions only include cases where data-usage purposes are specified.
The conditions on purposes are not checked if the data purpose is
unspecified (i.e., du=None).
Example 1 creates a privacy statement ((third party, collect, per-
sonal_data), (personal_data, not_for, (third party, Marketing))). Trans-
ferring the user device IMEI number to an advertiser’s server creates
a data flow f=(advertiser, IMEI, (advertiser, Provide ad)). Because IMEI
< personal_data (via device_identifier), advertiser < third party, and
Provide ad < Marketing (relationship in the purpose taxonomy),
the data flow is inconsistent with the privacy statement.
7 SYSTEM IMPLEMENTATION
Semantic and Syntactic Analysis. PurPliance uses a neural SRL
model [4, 62] trained on OntoNotes 5.0 [55, 54, 57], a large-scale
corpus with 1.7M English words of news, conversations and we-
blogs and 300K proposition annotations. Each token is encoded into
vectors depending on its context by using BERT-base-uncased con-
textualized word embeddings [17, 69]. Spacy with en_core_web_lg
language model [20] was used for syntactic analysis and depen-
dency parsing. Analyzing 16.8k privacy policies took 2 hours on 1
machine equipped with 2 Nvidia Titan Xp GPUs.
Data Object and Entity Ontologies. The consistency analysis logi-
cal rules require all entities and objects to be mapped into ontologies
to check their subsumptive relationships. PurPliance extends the
data object and entity ontologies based on PoliCheck to check their
subsumptive relationship. Similar to the addition of SCoU verbs,
we only add data objects and entities that are frequently used in
data-practice statements to avoid noise from those used in unre-
lated sentences. PurPliance extracts data objects and entities by
using a domain-adapted NER model trained on PolicyLint’s dataset
of 600 manually-annotated sentences (see Appendix H for details).
Policy Crawler and Preprocessor. We developed a crawler and
preprocessor to collect the privacy policies of Android apps. Its
implementation is described in Appendix F.
Network Data Traffic Collection. PurPliance used a tool based
on the VPN server API on Android [24] to capture apps’ HTTP(S)
traffic which is the most common protocol in app–server communi-
cation [21]. A system certificate was installed on rooted phones for
capturing encrypted traffic. Each app was exercised with human-
like inputs generated by deep-learning-based Humanoid [41], built
atop Droidbot automation tool [40]. For each app, the experiment
ran for at most 5 min and stopped if there was no traffic generated
for more than 2 min. These timeouts were empirically determined
for a good trade-off between data coverage and the number of apps
that we want to explore. We used 5 smartphones with Android 8.
8 EVALUATION
8.1 Data Collection
App Selection. We first selected the top 200 free apps for each
of 35 categories on Google Play Store, excluding Android Wear
and second-level Game categories [1]. This step resulted in 6,699
unique apps. Second, from a collection of 755,879 apps crawled from
Google Play Store in May 2020, we randomly selected additional
28,301 apps that are different from the top apps in the first step and
have been updated since 2015. To this end, 35k unique apps were
selected. After removing apps with an invalid privacy policy, our
final app corpus comprises 23,144 apps with a valid privacy policy.
Privacy Policy Corpus. We create a policy corpus as follows. We
removed 6,182 duplicate policies from apps that share the same
policy from the same developer. To reduce noise from titles (such
as policy section titles), sentences with title-cased or all capitalized
words or with less than 5 tokens are removed. Our final privacy
policy corpus has 16,802 unique policies with 1.4M sentences. The
categories with the most and least apps are Game (3,889 apps/2,797
policies) and Libraries & Demo (166 apps/121 policies), respectively.
Fig. 5 (Appendix I) shows their distribution over app categories.
Capturing Network Traffic. We capture the traffic of only the apps
which have a valid policy to analyze the app-flow consistency. We
intercepted 3,652,998 network requests of 18,689 apps over 33 days.
Among those, we discarded traffic with empty-body requests or not
from apps with valid policies and apps which became unavailable
from Play Store at the time of testing. The final dataset has 1,727,001
network requests from 17,144 unique apps. The number of apps that
generated traffic is lower than the selected apps because they either
work offline or our automated input generation did not generate
any input which triggered any requests to the servers, or the apps
require login preventing our tool from using the service. These
apps contacted 19,282 unique domains (164,096 unique end-point
URLs) and sent 24,918,567 key-value pair data to remote servers.
The distributions of network data requests across domains and app
categories are described in Appendix J.
8.2 Privacy Statement and Flow Distributions
PurPliance extracts 874,287 privacy statements from 142,231 sen-
tences in 15,312 policies (93.6% of 16,362 apps with data flows
extracted). Of these, 225,718 (25.8%) statements from 43,421 (30.5%)
sentences contain extracted purpose clauses. PurPliance recog-
nized 112,652 privacy statements with a non-Other purpose class.
The most common purposes are Provide Service and Improve Service