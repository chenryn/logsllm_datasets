(e.g., data object ontology and purpose taxonomy) which defines
relationships among the terms used. We extend the semantic equiv-
alence, subsumptive relationship and semantic approximation of
PoliCheck [8] to data-usage purposes as listed in Table 7. ğ‘…1 is
defined in Definition 6.4, ğ‘…2 â€“ ğ‘…4 are defined in Definition 6.5, and
ğ‘…5 â€“ ğ‘…9 are defined in Theorem 6.6 (proved in Appendix D).
Definition 6.1 (Semantic Equivalence). ğ‘¥ â‰¡ğ‘œ ğ‘¦ means that ğ‘¥ and
ğ‘¦ are synonyms, defined under an ontology ğ‘œ.
Definition 6.2 (Subsumptive Relationship). Given an ontology ğ‘œ
represented as a directed graph in which each node is a term and
each edge points from a general term ğ‘¦ to a specific term ğ‘¥ included
in ğ‘¦ (i.e., ğ‘¥ "is a" instance of ğ‘¦), ğ‘¥ <ğ‘œ ğ‘¦ means there is a path from
ğ‘¦ to ğ‘¥ and ğ‘¥ (cid:46)ğ‘œ ğ‘¦. Similarly, ğ‘¥ âŠ‘ğ‘œ ğ‘¦ â‡” ğ‘¥ <ğ‘œ ğ‘¦ âˆ¨ ğ‘¥ â‰¡ğ‘œ ğ‘¦ and
ğ‘¥ =ğ‘œ ğ‘¦ â‡” ğ‘¦ <ğ‘œ ğ‘¥.
Definition 6.3 (Semantic Approximation). The semantic approxi-
mation relationship between two terms ğ‘¥ and ğ‘¦, denoted as ğ‘¥ â‰ˆğ‘œ ğ‘¦, is
true if and only if âˆƒğ‘§ such as ğ‘§ <ğ‘œ ğ‘¥ âˆ§ ğ‘§ <ğ‘œ ğ‘¦ âˆ§ ğ‘¥ (cid:64)ğ‘œ ğ‘¦ âˆ§ ğ‘¦ (cid:64)ğ‘œ ğ‘¥.
Definition 6.4 (Purpose Equivalence). Two data-usage purposes
are semantically equivalent (ğ‘’ğ‘–, ğ‘ğ‘–) â‰¡ğœ‹ (ğ‘’ ğ‘— , ğ‘ ğ‘—) if and only if there
exist ontologies ğœ– and ğœ… such that ğ‘’ğ‘– â‰¡ğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– â‰¡ğœ… ğ‘ ğ‘— .
Definition 6.5 (Purpose Subsumption). (ğ‘’ğ‘–, ğ‘ğ‘–) <ğœ‹ (ğ‘’ ğ‘— , ğ‘ ğ‘—) if and
only if there exist ontologies ğœ– and ğœ… such that ğ‘’ğ‘– <ğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– â‰¡ğœ… ğ‘ ğ‘—
or ğ‘’ğ‘– â‰¡ğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– <ğœ… ğ‘ ğ‘— or ğ‘’ğ‘– <ğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– <ğœ… ğ‘ ğ‘— .
Theorem 6.6 (Purpose Semantic Approximation). Given two data-
usage purposes ğ‘ğ‘– = (ğ‘’ğ‘–, ğ‘ğ‘–) and ğ‘ ğ‘— = (ğ‘’ ğ‘— , ğ‘ ğ‘—), there exist ontologies
ğœ–, ğœ…, and ğœ‹ such that
(1) ğ‘’ğ‘– â‰¡ğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– â‰ˆğœ… ğ‘ ğ‘— â‡’ ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘— ,
(2) ğ‘’ğ‘– <ğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– â‰ˆğœ… ğ‘ ğ‘— â‡’ ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘— ,
(3) ğ‘’ğ‘– â‰ˆğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– â‰¡ğœ… ğ‘ ğ‘— â‡’ ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘— ,
(4) ğ‘’ğ‘– â‰ˆğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– <ğœ… ğ‘ ğ‘— â‡’ ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘— , and
(5) ğ‘’ğ‘– â‰ˆğœ– ğ‘’ ğ‘— âˆ§ ğ‘ğ‘– â‰ˆğœ… ğ‘ ğ‘— â‡’ ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘—
Session 10D: Applied Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2831Relation
ğ‘…1
ğ‘…2
ğ‘…3
ğ‘…4
ğ‘…5
ğ‘…6
ğ‘…7
ğ‘…8
ğ‘…9
ğ‘’ğ‘– Â· ğ‘’ ğ‘—
ğ‘’ğ‘– â‰¡ğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– â‰¡ğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– <ğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– <ğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– â‰¡ğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– <ğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– â‰ˆğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– â‰ˆğœ– ğ‘’ ğ‘—
ğ‘’ğ‘– â‰ˆğœ– ğ‘’ ğ‘—
ğ‘ğ‘– Â· ğ‘ ğ‘—
ğ‘ğ‘– â‰¡ğœ… ğ‘ ğ‘—
ğ‘ğ‘– <ğœ… ğ‘ ğ‘—
ğ‘ğ‘– â‰¡ğœ… ğ‘ ğ‘—
ğ‘ğ‘– <ğœ… ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ… ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ… ğ‘ ğ‘—
ğ‘ğ‘– â‰¡ğœ… ğ‘ ğ‘—
ğ‘ğ‘– <ğœ… ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ… ğ‘ ğ‘—
ğ‘ğ‘– Â· ğ‘ ğ‘—
ğ‘ğ‘– â‰¡ğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– <ğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– <ğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– <ğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘—
ğ‘ğ‘– â‰ˆğœ‹ ğ‘ ğ‘—
Table 7: Data-usage purpose relationships. ğ‘ğ‘– = (ğ‘’ğ‘–, ğ‘ğ‘–) and
ğ‘ ğ‘— = (ğ‘’ ğ‘— , ğ‘ ğ‘—). Â· denotes a relationship placeholder. ğ‘…1 â€“ ğ‘…4 are
definitions, ğ‘…5 â€“ ğ‘…9 are theorems.
Rule Logic
ğ¶1
ğ‘‘ğ‘˜ â‰¡ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰¡ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ â‰¡ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š <ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ <ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰¡ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ <ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š <ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ =ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š <ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ â‰¡ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰ˆğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ <ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰ˆğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ =ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰ˆğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ â‰ˆğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰¡ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ â‰ˆğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š <ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ â‰ˆğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š =ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ â‰ˆğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰ˆğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ â‰¡ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š =ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ <ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š =ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ =ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š â‰¡ğœ‹ ğ‘ğ‘›
ğ‘‘ğ‘˜ =ğ›¿ ğ‘‘ğ‘™ âˆ§ ğ‘ğ‘š =ğœ‹ ğ‘ğ‘›
ğ¶2
ğ¶3
ğ¶4
ğ¶5
ğ¶6
ğ¶7
ğ¶8
ğ¶9
ğ¶10
ğ¶11
ğ¶12
ğ‘1
ğ‘2
ğ‘3
ğ‘4
Example
(Device ID, k, Advertising)
(Device ID, Â¬k, Advertising)
(Device ID, k, Advertising)
(Device ID, Â¬k, Marketing)
(Device ID, k, Advertising)
(Device info, Â¬k, Advertising)
(Device ID, k, Advertising)
(Device info, Â¬k, Marketing)
(Device info, k, Advertising)
(Device ID, Â¬k, Marketing)
(Device ID, k, Advertising)
(Device ID, Â¬k, Personalization)
(Device ID, k, Advertising)
(Device info, Â¬k, Personalization)
(Device info, k, Advertising)
(Device ID, Â¬k, Personalization)
(Device ID, k, Advertising)
(Tracking ID, Â¬k, Advertising)
(Device ID, k, Advertising)
(Tracking ID, Â¬k, Marketing)
(Device ID, k, Marketing)
(Tracking ID, Â¬k, Advertising)
(Device ID, k, Advertising)
(Tracking ID, Â¬k, Personalization)
(Device ID, k, Marketing)
(Device ID, Â¬k, Advertising)
(Device ID, k, Marketing)
(Device info, Â¬k, Advertising)
(Device info, k, Advertising)
(Device ID, Â¬k, Advertising)
(Device info, k, Marketing)
(Device ID, Â¬k, Advertising)
Table 8: Logical forms of logical contradictions (ğ¶) and nar-
rowing definitions (ğ‘ ). ğ‘˜ and Â¬ğ‘˜ abbreviate for and not_for,
respectively. The data flow has data type ğ‘“ğ‘‘ = IMEI and pur-
pose ğ‘“ğ‘ = Personalize ad.
6.2 Policy Contradictions
Definition 6.7 (Privacy Statement Contradiction). Two privacy
statements ğ‘¡ğ‘˜ = (ğ‘‘ğ‘ğ‘˜, ğ‘‘ğ‘¢ğ‘˜) and ğ‘¡ğ‘™ = (ğ‘‘ğ‘ğ‘™ , ğ‘‘ğ‘¢ğ‘™) are said to contradict
each other iff either ğ‘‘ğ‘ğ‘˜ contradicts ğ‘‘ğ‘ğ‘™ or ğ‘‘ğ‘¢ğ‘˜ contradicts ğ‘‘ğ‘¢ğ‘™ .
PurPlianceâ€™s consistency analysis comprises two steps. Using
the Definition 6.7 of contradiction between two privacy statements,
it checks the consistency of ğ‘‘ğ‘ and ğ‘‘ğ‘¢ tuples in this order. The
consistency of ğ‘‘ğ‘ğ‘˜ = (ğ‘Ÿğ‘˜, ğ‘ğ‘˜, ğ‘‘ğ‘˜) and ğ‘‘ğ‘ğ‘™ = (ğ‘Ÿğ‘™ , ğ‘ğ‘™ , ğ‘‘ğ‘™) is analyzed
by a Data Collection consistency model. PurPliance leverages
the PoliCheck consistency model in this analysis. However, the
PoliCheck consistency model cannot check the two policy state-
ments if both have a positive sentiment (i.e., ğ‘ğ‘˜ = ğ‘ğ‘™ = collect) or
X does not collect Y for Z
X collects Y for Z
X does not collect Y X collects Y
Consistent
Consistent
Contradictory
Consistent
Table 9: Privacy-statement comparison when one of the
statement has no data usage purpose specified (ğ‘‘ğ‘¢ = ğ‘ğ‘œğ‘›ğ‘’).
the two receivers do not have either a subsumptive or semantic
approximation relationship. In such cases, since no contradiction
was detected, PurPliance checks the consistency of data usage
statements ğ‘‘ğ‘¢ğ‘˜ and ğ‘‘ğ‘¢ğ‘™ using a Data Usage consistency model. We
extend the PoliCheck model [8] for data usage purposes as follows.
The contradiction conditions and types of two data usage tu-
ples ğ‘‘ğ‘¢ğ‘˜ = (ğ‘‘ğ‘˜, for, ğ‘ğ‘š) and ğ‘‘ğ‘¢ğ‘™ = (ğ‘‘ğ‘™ , not_for, ğ‘ğ‘›) are listed in
Table 8. There are 16 cases and 2 types of contradictions: logical
contradictions (ğ¶1â€“ğ¶12) and narrowing definitions (ğ‘1â€“ğ‘4). Logical
contradictions occur when ğ‘‘ğ‘¢ğ‘™ states the exclusion of a broader
purpose from data usage while ğ‘‘ğ‘¢ğ‘˜ states the usage for a purpose
type in a narrower scope. On the other hand, narrowing defini-
tions have the not-for-purpose statement (where ğ‘˜ = not_for) in
a narrower scope than their counterparts. Narrowing definitions
may confuse readers and automatic analysis when interpreting the
privacy statements, especially when the two statements are far
apart in a document.
When two privacy statements are compared, if one of them has
no data-usage purpose specified (i.e., du = None), PurPliance flags
a contradiction only if they have forms ((ğ‘Ÿğ‘˜, ğ‘›ğ‘œğ‘¡_ğ‘ğ‘œğ‘™ğ‘™ğ‘’ğ‘ğ‘¡, ğ‘‘ğ‘˜), ğ‘ğ‘œğ‘›ğ‘’)
and ((ğ‘Ÿğ‘™ , ğ‘ğ‘œğ‘™ğ‘™ğ‘’ğ‘ğ‘¡, ğ‘‘ğ‘™), (ğ‘‘ğ‘™ , ğ‘“ ğ‘œğ‘Ÿ, ğ‘ğ‘™)), i.e., the positive-sentiment state-
ment has ğ‘˜ğ‘™ = ğ‘“ ğ‘œğ‘Ÿ. Following this rule, "X does not collect Y" does
not contradict "X does not collect Y for Z" as they are translated to
((X, not_collect, Y), None) and ((X, collect, Y), (Y, not_for, Z)), respec-
tively. Table 9 lists the cases of this rule.
Example 2. Given two statements: "we use your personal data
only for providing the App" and "advertisers may use your device
ID to serve you with advertisements," a contradiction is detected as
follows. Due to the keyword only for, PurPliance excludes third
partiesâ€™ Marketing purposes that are not for providing the app and
translates the first sentence to 1 positive and 1 negated statement:
ğ‘ 1
1 = (we, collect, personal data), (personal data, for, (anyone, Provide
service)), ğ‘ 2
1 = (third party, collect, personal data), (personal data,
not_for, (third party, Marketing)). The second sentence is translated
to ğ‘ 2 = (advertiser, collect, device ID), (device ID, for, (advertiser, Pro-
vide ad)). Since device ID < personal data, advertiser < third party
and Provide ad < Marketing, the first sentenceâ€™s negated statement
ğ‘ 2
1 contradicts ğ‘ 2 of the second sentence under rule ğ¶4. PolicyLint
will not flag these sentences because it considers only the collection
tuples which are all positive sentiments in these sentences.
6.3 Flow Consistency Analysis
Definition 6.8 (Flow-relevant Privacy Statements). A privacy state-
ment ğ‘¡ğ‘“ = ((ğ‘Ÿğ‘¡ , ğ‘ğ‘¡ , ğ‘‘ğ‘¡), (ğ‘‘ğ‘¡ , ğ‘˜ğ‘¡ , (ğ‘’ğ‘¡ , ğ‘ğ‘¡))) is relevant to a flow ğ‘“ =
(ğ‘Ÿ, ğ‘‘, (ğ‘’, ğ‘)) (denoted as ğ‘¡ğ‘“ â‰ƒ ğ‘“ ) if and only if ğ‘Ÿ âŠ‘ğœŒ ğ‘Ÿğ‘¡ âˆ§ ğ‘‘ âŠ‘ğ›¿
ğ‘‘ğ‘¡ âˆ§ ğ‘’ âŠ‘ğœ– ğ‘’ğ‘¡ âˆ§ ğ‘ âŠ‘ğœ… ğ‘ğ‘¡ . Let ğ‘‡ğ‘“ be the set of flow-ğ‘“ -relevant privacy
statements in the set of privacy statements ğ‘‡ of a privacy policy, then
ğ‘‡ğ‘“ = {ğ‘¡ | ğ‘¡ âˆˆ ğ‘‡ âˆ§ ğ‘¡ â‰ƒ ğ‘“ }.
Session 10D: Applied Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2832Definition 6.9 (Flow-to-Policy Consistency). A flow ğ‘“ is said to be
consistent with a privacy policy ğ‘‡ iff âˆƒğ‘¡ âˆˆ ğ‘‡ğ‘“ such that ğ‘ğ‘¡ = collect âˆ§
ğ‘˜ğ‘¡ = for and (cid:154)ğ‘¡ âˆˆ ğ‘‡ğ‘“ such that ğ‘ğ‘¡ = not_collect âˆ¨ ğ‘˜ğ‘¡ = not_for.
A data flow is inconsistent with a privacy policy if the Flow-to-
Policy Consistency condition is not met. For each flow extracted
from app behavior, PurPliance first finds the flow-relevant privacy
statements ğ‘‡ğ‘“ and classifies the flow as consistent or inconsistent
using the above definitions. Although finer-grained consistency
types can be used, such as Clear and Ambiguous disclosures as
in PoliCheck, we leave it as future work. For brevity, the defini-
tions only include cases where data-usage purposes are specified.
The conditions on purposes are not checked if the data purpose is
unspecified (i.e., du=None).
Example 1 creates a privacy statement ((third party, collect, per-
sonal_data), (personal_data, not_for, (third party, Marketing))). Trans-
ferring the user device IMEI number to an advertiserâ€™s server creates
a data flow f=(advertiser, IMEI, (advertiser, Provide ad)). Because IMEI
< personal_data (via device_identifier), advertiser < third party, and
Provide ad < Marketing (relationship in the purpose taxonomy),
the data flow is inconsistent with the privacy statement.
7 SYSTEM IMPLEMENTATION
Semantic and Syntactic Analysis. PurPliance uses a neural SRL
model [4, 62] trained on OntoNotes 5.0 [55, 54, 57], a large-scale
corpus with 1.7M English words of news, conversations and we-
blogs and 300K proposition annotations. Each token is encoded into
vectors depending on its context by using BERT-base-uncased con-
textualized word embeddings [17, 69]. Spacy with en_core_web_lg
language model [20] was used for syntactic analysis and depen-
dency parsing. Analyzing 16.8k privacy policies took 2 hours on 1
machine equipped with 2 Nvidia Titan Xp GPUs.
Data Object and Entity Ontologies. The consistency analysis logi-
cal rules require all entities and objects to be mapped into ontologies
to check their subsumptive relationships. PurPliance extends the
data object and entity ontologies based on PoliCheck to check their
subsumptive relationship. Similar to the addition of SCoU verbs,
we only add data objects and entities that are frequently used in
data-practice statements to avoid noise from those used in unre-
lated sentences. PurPliance extracts data objects and entities by
using a domain-adapted NER model trained on PolicyLintâ€™s dataset
of 600 manually-annotated sentences (see Appendix H for details).
Policy Crawler and Preprocessor. We developed a crawler and
preprocessor to collect the privacy policies of Android apps. Its
implementation is described in Appendix F.
Network Data Traffic Collection. PurPliance used a tool based
on the VPN server API on Android [24] to capture appsâ€™ HTTP(S)
traffic which is the most common protocol in appâ€“server communi-
cation [21]. A system certificate was installed on rooted phones for
capturing encrypted traffic. Each app was exercised with human-
like inputs generated by deep-learning-based Humanoid [41], built
atop Droidbot automation tool [40]. For each app, the experiment
ran for at most 5 min and stopped if there was no traffic generated
for more than 2 min. These timeouts were empirically determined
for a good trade-off between data coverage and the number of apps
that we want to explore. We used 5 smartphones with Android 8.
8 EVALUATION
8.1 Data Collection
App Selection. We first selected the top 200 free apps for each
of 35 categories on Google Play Store, excluding Android Wear
and second-level Game categories [1]. This step resulted in 6,699
unique apps. Second, from a collection of 755,879 apps crawled from
Google Play Store in May 2020, we randomly selected additional
28,301 apps that are different from the top apps in the first step and
have been updated since 2015. To this end, 35k unique apps were
selected. After removing apps with an invalid privacy policy, our
final app corpus comprises 23,144 apps with a valid privacy policy.
Privacy Policy Corpus. We create a policy corpus as follows. We
removed 6,182 duplicate policies from apps that share the same
policy from the same developer. To reduce noise from titles (such
as policy section titles), sentences with title-cased or all capitalized
words or with less than 5 tokens are removed. Our final privacy
policy corpus has 16,802 unique policies with 1.4M sentences. The
categories with the most and least apps are Game (3,889 apps/2,797
policies) and Libraries & Demo (166 apps/121 policies), respectively.
Fig. 5 (Appendix I) shows their distribution over app categories.
Capturing Network Traffic. We capture the traffic of only the apps
which have a valid policy to analyze the app-flow consistency. We
intercepted 3,652,998 network requests of 18,689 apps over 33 days.
Among those, we discarded traffic with empty-body requests or not
from apps with valid policies and apps which became unavailable
from Play Store at the time of testing. The final dataset has 1,727,001
network requests from 17,144 unique apps. The number of apps that
generated traffic is lower than the selected apps because they either
work offline or our automated input generation did not generate
any input which triggered any requests to the servers, or the apps
require login preventing our tool from using the service. These
apps contacted 19,282 unique domains (164,096 unique end-point
URLs) and sent 24,918,567 key-value pair data to remote servers.
The distributions of network data requests across domains and app
categories are described in Appendix J.
8.2 Privacy Statement and Flow Distributions
PurPliance extracts 874,287 privacy statements from 142,231 sen-
tences in 15,312 policies (93.6% of 16,362 apps with data flows
extracted). Of these, 225,718 (25.8%) statements from 43,421 (30.5%)
sentences contain extracted purpose clauses. PurPliance recog-
nized 112,652 privacy statements with a non-Other purpose class.
The most common purposes are Provide Service and Improve Service