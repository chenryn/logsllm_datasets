Kafka and Trello integrations were not used in this experiment.
14
Experiment Data Collection
Developer repoleveD
Start IDE Uses IDE Features Exports Projects Send Working Session
Data to Researcher
Eclipse IDE - Basic DistributionWorking Session Ended noitubirtsiD
PS lt ua gr it n FP er ao ID tv uEid ree s PE rx op jeo cr tt s IDE Shutdown cisaB
Project Files (ZIP)
EDI
Eclipse IDE - SDPM Plugin retupmoC
Events File (CSV) espilcE
Ca Sp et nu dr e ID a End Append Events nigulP
Event
s'repoleveD MPDS
IDE Shutdown Plugin Innactive
Email Client tneilC
Forward
Working
Session Data Files Sent liamE
Detected Event
Microsoft Azure Platform mroftalP
Event
Hub eruzA
Events Storage
tfosorciM
Forwards
Stored
Events
Events File (BLOB) Events File (CSV)
Project Files (ZIP)
JavaScript Interpreter Events (JSON)
S SC t er o re r va a it cge ee R ESe vtq o eu r nee tds st Do Ew ven nlo tsad C BEo L Jvn SOe Ov n Be Nt r tst o's reterpretnI
Storage Explorer retupmoC tpircSavaJ
Converts Events (JSON)
Events'
CSV to JSON s'rehcraeseR
MySQL Workbench
DEC av tr ae ea bnt ate s se EvI em J nSp O t o FNr ilt es PF G ri oelt Mn Fe e ir lI r ena a pn t ued t Experiment hcnebkroW
Collection Ended
LQSyM
Events Database (MySQL)
ProM File (CSV)
S Et va er nt ts Collection L Sta ou rn ac gh ee Ss ervice Events File (CVS) S Ee vele nc tt Types
Researcher rehcraeseR
Figure 1: Experiment Data Collection Workflow
during the aforementioned activities. As such, each team produced and de-
livered two new Jasml projects, one for the automatic and another for the
manual refactoring. The events files would map events for the two different
tasks, as they were done in different time frames.
All events stored in the database were imported into the ProM process
15
mining tool11 and converted to the IEEE eXtensible Event Stream (XES)
standard format [68]. The following event properties were mapped when
converting to XES format:
• team and session were used as CaseID since we were interested to
look into process instances of teams and their multiple development
sessions, not of individual programmers.
• Properties filename, categoryName and commandName forming a hi-
erarchical structure were used as the Activity in the process.
• The timestamp begin and timestamp end were both used as activity
Timestamps.
• Otherpropertieswerenotusedintheprocessdiscoveryphase, however,
they were later used for metrics aggregation and context analysis.
4.3. Data Analysis
4.3.1. Context
All teams started with the same version of Jasml 0.10, therefore, we
had two relevant moments to get measures from:
1. The initial moment (t0), when we extracted the metrics for the initial
product version. However, we didn’t know how it was built, therefore,
we were missing12 the process metrics.
2. The end of the task (t1), when we extracted again the product metrics
for the changed Jasml 0.10 project of each team as they stand after
the refactoring sessions. In addition, we had also IDE usage events
which provide evidences on how the product was changed.
Following data extraction, we computed, for each product metric defined
in Table A.7, their relative variance as shown by Equation 1. The relative
variance variables were the ones we used in all RQs.
product metrics −product metrics
(t1) (t0)
∆product metrics = ∗100 (1)
(t1-t0) product metrics
(t0)
11Version 6.8, available at http://www.promtools.org
12In reality we may consider all of them to be zero
16
The relative variance was used in order to generalize our approach, thus,
making it applicable in scenarios where different teams work on distinct soft-
ware projects.
Process metrics described in Table A.8 were derived from the events
dataset captured between moments (t0) and (t1), either by summing the
events or using the method described in 4.3.3. These metrics may be seen
as a representation of the effort done by each team during the refactoring
practices.
The complete workflow followed in data pre-processing, aggregation and
analysis is presented in Figure 2.
4.3.2. Product and Process Metrics
ToextractsoftwaremetricsweusedthepluginbuiltbySauer13. Although
having more than a decade of age, it is still one of the more proven and
popular options regarding open source metrics plugins for Eclipse.
The plugin itself offers a simple interface and reporting capabilities with
which users can define optimal ranges and issue warnings for certain metrics,
as well as being able to export calculated metrics to XML files. The set of
metrics obtained by this plugin are presented in Table A.7 on Appendix A.1.
4.3.3. Process Discovery
Several well known algorithms exist to discover process models, such as,
the α-algorithm, the heuristics, genetic and the fuzzy miner amongst others
[69, 70]. Our need to discover and visualize the processes in multiple ways
lead us to choose the ProM’s StateChart Workbench plugin [60]. This plu-
gin, besides supporting process model discovery using multiple hierarchies
and classifiers, also allows to visualize the model as a Sequence Diagram and
use notations such as Petri Nets and Process Trees. This plugin is particu-
larly suitable for mining software logs, where an event structure is supposed
to exist, but it also supports the mining of other so-called generic logs.
Events collected from software in operation (e.g. Java programs) reveals
the presence of a hierarchical structure, where methods reside within classes,
and classes within packages [71]. The same applies to IDE usage actions
where identified menu options and executed commands belong to a specific
category of command options built-in the Eclipse framework. Supported
by this evidence, we used the Software log Hierarchical discovery method
with a Structured Names heuristic to discover the processes based on the
fact that the events were using a filename|category|command structure (e.g.
/jasml0.10/src/jasml.java|Eclipse Editor|File Open).
13http://metrics.sourceforge.net
17
Study Computation and Analysis
Email client
Email from Developer Project Files (ZIP) tneilc
Extract Project
Files (source liamE
code)
Eclipse IDE - Basic Distribution
noitubirtsiD
cisaB
Import Projects
EDI
espilcE
Eclipse IDE - Metrics Plugin
Product Metrics (XML) nigulP
C Mal ec tu rl ia ct se Extract Metrics scirteM
ProM File (CSV) ProM FileP r(oXME S- )Basic Distribution noitubirtsiD
Import and cisaB
Convert Events
File
retupmoC
MorP
ProM - StateChart Workbench
Process Metrics (CSV) trahCetatS hcnebkroW
s'rehcraeseR
Mine Process and
Extract Its Metrics
MySQL Workbench
Events Database (MySQL) hcnebkroW
Extract Process
Extended Metrics IDE Commands
Frequencies
LQSyM
RStudio
oidutSR
Merge Process and Metric Data Correlation
Product Metrics Aggregated Metrics Partition Analysis
Weka
akeW
Import Metrics aM no dd Ee vl aT lr ua ain tii on ng
Results
Available
Figure 2: Study Computation and Analysis Process
Several perspectives can be used to discover and analyze a business pro-
cess. The commonly used are: Control-Flow, Organizational, Social
and Performance. We have focused on the Control-Flow perspective in this
paper. It defines an approach that consists in analyzing how each task/ac-
tivity follows each other in an event log, and infer a possible model for the
behavior captured in the observed process.
18
Process metrics, shown in Tables A.8 and A.9 on Appendix A.2, were ob-
tained using the discovery method described in 4.3.3, and by running queries
into the events database as presented in Figure 2.
4.3.4. Data Partitioning
We used the k-means clustering algorithm to compute new variables
based on the partition of the teams across different levels (clusters) of Pro-
cess Cyclomatic Complexity (PCC) and McCabe Cyclomatic Complexity
variance (∆VG) . The decision of how many clusters to use (k) was sup-
ported by a detailed analysis of the Elbow and Silhouette methods:
• Elbow Method. It is frequently used to optimize the number of clus-
ters in a data set. This heuristic, consists of rendering the explained
variation as a function of the number of clusters, and picking the el-
bow of the curve as the optimal number of clusters to use. In cluster
analysis, the elbow method runs k-means clustering on the dataset for
a range of values for k (say from 2-10), and then, for each value of k
computes an average score for all clusters. The distortion score is com-
puted as the sum of square distances from each point to its assigned
center [72].
• Silhouette Method. It is a commonly used approach of interpre-
tation and validation of consistency within clusters of data. Provides
a concise graphical representation of how well each object has been
classified within the corresponding cluster. The Silhouette value is a
measure of how similar an object is to its own cluster (cohesion) com-
pared to other clusters (separation). The silhouette can be calculated
with any distance metric, such as the Euclidean distance or the Man-
hattan distance, and ranges from -1 to +1. A high value indicates
that the object is well matched to its own cluster and poorly matched
to neighboring clusters. The clustering configuration is appropriate if
most objects have a high value. If many objects have a low or negative
value, then the clustering configuration may have too many or too few
clusters and, as such, requires further research before a decision on the
optimal number of k clusters is made [73].
4.3.5. Model Selection with Hyperparameter Optimization
To build, tune model parameters as recommended [74, 75], train, evaluate
and select the best-fit classification models presented in Tables 5 and 6, we
19
used Weka and the Auto-Weka plugin. Weka (Waikato Environment for
Knowledge Analysis) is a popular suite of machine learning software written
in Java. It’s workbench contains a collection of visualization tools and al-
gorithms for data analysis and predictive modeling, together with graphical
user interfaces for easy access to this functionality [76]. Auto-Weka is
a plugin that installs as a Weka package and uses Bayesian optimization
to automatically instantiate a highly optimized parametric machine learning
framework with minimum user intervention [77].
4.3.6. Model Evaluation
Several evaluation metrics can be used to assess model quality in terms of
false positives/negatives (FP/FN), and true classifications (TP/TN). How-
ever, commonly used measures, such as Accuracy, Precision, Recall and
F-Measure, do not perform very well in case of an imbalanced dataset or
they require the use of a minimum probability threshold to provide a defini-
tive answer for predictions. For these reasons, we used the ROC14, which
is a threshold invariant measurement. Nevertheless, for general convenience,
we kept present in Tables 5 and 6 all the evaluation metrics.
ROC gives us a 2-D curve, which passes through (0, 0) and (1, 1). The
best possible model would have the curve close to y = 1, with and area under
the curve (AUC) close to 1.0. AUC always yields an area of 0.5 under
random-guessing. This enables comparing a given model against random
prediction, without worrying about arbitrary thresholds, or the proportion
of subjects on each class to predict [28].
4.4. Research Questions
The research questions for this work are:
• RQ1: How different refactoring methods perform when the goal is to
reduce complexity, future testing and maintainability efforts?.
Methods Used. Process Mining Model Discovery, Descriptive statis-
tics and Cluster Analysis.
• RQ2: Is there any association between software complexity and the
underlying development activities in refactoring practices?
Methods Used. Process Mining Model Discovery, Correlation Anal-
ysis using the Spearman’s rank correlation.
14Receiver operating characteristic (ROC) is a curve that plots the true positive rates
against the false positive rates for all possible thresholds between 0 and 1.
20
• RQ3: Using only process metrics, are we able to predict with high
accuracy different refactoring methods?
Methods Used. Supervised and Unsupervised Learning Algorithms
with Hyperparameter Optimization.
• RQ4: Using only process metrics, are we able to model accurately the
expected level of complexity variance after a refactoring task?
Methods Used. Supervised and Unsupervised Learning Algorithms
with Hyperparameter Optimization.
5. Study Results
In this section, we present the experiment results with respect to our
research questions.
5.1. RQ1. How different refactoring methods perform when the
goal is to reduce complexity, future testing and maintainabil-
ity efforts?
In this RQ, we used as product metrics, the ones identified in section
4.3.2. Since IDE usage is a sequence of actions (it can be seen as a process,
or at least, as a process fragment), we used as process metrics the ones