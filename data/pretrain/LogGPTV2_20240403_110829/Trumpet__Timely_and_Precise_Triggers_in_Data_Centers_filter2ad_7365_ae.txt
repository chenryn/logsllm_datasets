one receiver with a bottleneck bandwidth of 100Mbps. Using
iperf, two senders send TCP trafﬁc for 20s, and after 15s, the
01020304000.511.52x 107Time (s)Sequence number  TransmitRetransmit01020304000.511.52x 107Time (s)Sequence numberDetect & Pace0100200300TCP1TCP2UDPTime (ms)Congestion 1Congestion 2HH−proactiveHH−reactive4166425600.20.40.60.81Time (ms)# eventsthird sender sends a short UDP ﬂow (512KB in 40ms) and
creates congestion. We deﬁne a congestion detection event
to report if, at the end of an epoch, any 5-tuple ﬂow cannot
receive acks for more than 50% of outstanding data at the
beginning of the epoch (Table 2). We deﬁne the heavy hitter
detection event to report if the volume of any 5-tuple ﬂow is
larger than 12KB in every 10ms (10% of link bandwidth).
Figure 7a shows the normalized throughput of the three
connections measured at the receiver as lines and the detected
events at the controller as dots. The congestion event could
correctly detect throughput loss in TCP connections three
epochs after the start of the UDP trafﬁc. The delay is partly
because it takes time for the UDP trafﬁc to affect TCP connec-
tion throughput. After detecting congestion, TEM installs the
HH_Reactive event to detect heavy hitters with 200µs delay
(order of ping delay). HH_Reactive can correctly detect the
UDP connection in the fourth epoch after its initiation. Figure
7a shows that the proactive approach (HH_proactive) could
also correctly detect the UDP ﬂow over its lifetime.
Network-wide event to detect transient service-scale
anomaly. This use-case shows how Trumpet can detect
if the total volume of trafﬁc of a service exceeds a given
threshold within a 10ms window. In our experiment, two
end-hosts each monitor two 10G ports on two cores. The
controller treats each core as a separate end-host (this
demonstrates how Trumpet can handle higher NIC speeds).
The RTT between the controller and end-hosts is 170 µs.
We generate different number of triggers that are satisﬁed
every measurement epoch. When the predicate of a trigger
is satisﬁed at an end-host, the end-host sends a satisfaction
message to the controller and the controller polls other
servers and collects replies from them.
Figure 7b shows the delay between receiving the ﬁrst satis-
faction and the last poll reply at the controller for different
numbers of events. Even with 256 satisﬁed events, the av-
erage delay is only about 0.75ms. The average delay for
processing satisfactions and generating polls at the controller
is 60ns, showing that Trumpet controller can potentially scale
to more than 16M messages from TPMs per second per core.
7.2 Performance
Methodology. In our experiments, our test machine receives
trafﬁc from a sender. The trafﬁc generation pattern we test
mirrors published ﬂow arrival statistics for data centers [49]:
exponential ﬂow inter-arrival time at each host with 1ms av-
erage with 300 active ﬂows. The TPM monitors packets and
forwards them based on their destination IP. It has 4k triggers
and each packet matches 8 triggers. Packets may match mul-
tiple triggers when a ﬂow is monitored using multiple queries
with different predicates (variable, aggregate function, thresh-
old), time intervals or ﬂow granularities. Triggers track one
of 3 statistics: packet count, trafﬁc volume or the number
of lost packets. In particular, tracking packet losses requires
tracking retransmissions across multiple epochs, which can
consume memory and additional CPU cycles. However, the
triggers are designed such that the 8 triggers matched by each
packet cover the 3 types of statistics. Also, every trigger is
evaluated at the granularity of 10ms. Unless otherwise speci-
ﬁed, the default burst size is 1 packet (i.e., every subsequent
packet changes its ﬂow tuples), which foils lookup caching.
These settings were chosen to saturate the packet process-
ing core on our server. We also explore the parameter space
(number of triggers, ﬂow arrival rates, time interval, etc.) to
characterize the feasibility region — the set of parameters for
which Trumpet can monitor events without losing a packet or
missing a sweep.
We use several metrics to evaluate our schemes, including
the fraction of time the CPU was quiescent (i.e., not in either
of the two phases; this quantity is obtained by aggregating
the time between two queue polls that returned no trafﬁc),
the time for the sweep phase, and whether a particular design
point incurred loss or not. We ran each experiment for 5
times with different ﬂow arrival patterns for 50 seconds. The
variance across runs is very small, and we present the average.
Baseline experiment. We ran our synthetic trafﬁc at the
maximum packet rate of 14.8Mpps (64B per packet) on a
10G link for the trigger conﬁguration discussed above and
found that (no associated ﬁgure) TPM (a) never loses a packet,
(b) is able to correctly compute every statistic, and (c) is able
to complete sweeps within an epoch so triggers are correctly
evaluated. We also got the same results on a 40G (4x10G)
NIC with 650 byte packets at full line rate. For a 40G NIC,
we expected to be able to support 256 byte packets (since
for a 10G NIC we can support 64 byte packets at line rate).
However, in our evaluation setting, TPM has to poll four
ports, and this polling introduces overhead, requiring a larger
packet size for Trumpet to be feasible at 40G.
The time granularity in this experiment is 10ms, and we
earlier showed that other simpler strategies (Section 5.1) in-
cur 10-20% packet loss in this regime. This experiment
validates our design and suggests that it may be possible to
monitor a variety of events precisely and at ﬁne-timescales
by leveraging the computing capabilities of end-hosts in data
centers. More important, in this baseline experiment, the
CPU is never quiescent. Finally, we have designed TPM for
the worst-case: servers in data centers are unlikely to see sus-
tained 14.88Mpps rate (we quantify below how our system
performs at lower rates).
Match-and-scatter optimizations. We described several
optimizations for the match-and-scatter phase in Section 5.4.
Here we quantify the beneﬁts of each optimization (Figure
9). Packet prefetching saves about 2% of CPU time over
different packet rates, a signiﬁcant improvement because
we save about 200µs in a 10ms interval. This is more than
enough for two sweeps (each sweep takes < 100µs). More
important, recall that at full packet rate, the CPU is never
quiescent. Thus, any small increase in CPU time will cause
the monitoring system to lose packets, which is completely
unacceptable. Indeed, when we turn off packet prefetching,
at 14.88Mpps, we experience 4.5% packet loss, and TPM
cannot ﬁnish the sweep of all the triggers.
Similarly, although other optimizations contribute small
beneﬁts, each of these beneﬁts is critical: without these, TPM
would either lose packets or not be able to ﬁnish sweeps in
(a) Flow Block
(b) Burst size
Figure 8: Optimizations saving
(a) DoS BW per threshold
(b) DoS threshold prediction
Figure 11: DoS resiliency
(a) Sweep time
(b) Quiescent time
Figure 9: Optimizations saving of ﬂow tables and trigger tables
time (which can lead to missed events). Using huge pages
for storing the ﬂow table and triggers saved 6µs of sweep
time. Moreover, huge pages and prefetching ﬂow table entries
saves 0.2% of CPU time. To understand the effectiveness of
caching the result of the last ﬂow table lookup for a burst of
packets, Figure 8b shows the impact on quiescent time with
different burst sizes for 12Mpps: with the growth of the burst
size from 1 to 8, the CPU quiescent time increases from 10%
to 13%.
Gather-test-and-report phase optimizations. For a simi-
lar reason, even small beneﬁts in this phase can be critical to
the viability of TPM. Storing trigger entries contiguously in
the trigger repository reduces the sweep time by 2.5µs (Fig-
ure 9) because it enables hardware prefetching. To evaluate
the beneﬁt of using chunked lists, we keep the same total
number of active ﬂows as 300, but change the number of
ﬂows per trigger from 200 to 45k (recall that, in our trafﬁc,
1000 ﬂows arrive at each host every second). Figure 8a shows
that this can save up to 27µs (80%) in sweep time when there
are many ﬂows per trigger, and its overhead is small when
there are few ﬂows per trigger.
Resource-proportional design. Trumpet’s careful partition-
ing of functionality results in resource-usage that scales well
both with trafﬁc rate, and with the level of monitored traf-
ﬁc. This efﬁcient design is the key reason we are able to
increase the expressivity of Trumpet’s language, and track
fairly complex per-packet properties like loss rates and RTTs.
Figure 10b shows that CPU quiescent time decreases
steadily as a function of the packet rate: at 8Mpps, the
(a) Sweep time
(b) Quiescent time
Figure 10: Proportional resource usage on % ﬂows matched (legend
shows rate in Mpps)
system is quiescent 40% of the time, while at 14Mpps it is
always fully utilized. In this experiment, the quiescent time
is independent of the fraction of matching trafﬁc because
the dominant matching cost is incurred for every new ﬂow,
regardless of whether the ﬂow matches triggers ﬁlters or
not. However, the gather phase scales well with the level of
monitored trafﬁc: as Figure 10a shows, the sweep time is
proportional only to the number of ﬂows that match a trigger.
Thus if trigger ﬁlters match fewer ﬂows, the sweep will take
shorter, and the TPM can support smaller time intervals.
DoS-resilience. We perform an experiment in which the
TPM is subjected to the full line rate. Legitimate ﬂows send
10 packets of maximum size (1.5KB). We consider two at-
tack models: a) Syn attack where the attacker sends only
one packet per ﬂow to maximize packet processing overhead
b) Threshold attack where the attacker knows the threshold
and sends ﬂows with size equal to the threshold to maximize
matching overhead. Also all ﬂows come in a burst of one (no
subsequent packets are from the same ﬂow). Our experiment
quantiﬁes the percentage of DoS trafﬁc that can be tolerated
at each DoS threshold. Thus, it uses two independent knobs:
the fraction of DoS trafﬁc in the full packet rate, and the DoS
threshold. Each point in Figure 11a plots the DoS fraction
and the threshold at which the system is functional: even a
small increase in one of these can cause the system to fail
(either lose packets or not ﬁnish sweeps).
As Figure 11a shows, for any threshold more than 440B (on
the right of the one packet line), the TPM can sustain a SYN
attack. This form of attack is easy for the TPM to handle,
since if the trafﬁc is below the threshold, then matching is not
incurred. The threshold is higher than 1 packet because there
are false positives in the fast counter array implementation
of the ﬁlter table [2]. At lower thresholds, smaller fractions
of attack trafﬁc can be sustained. For the threshold attack,
a threshold of 384B (832B) ensures immunity to more than
30% (50%) DoS trafﬁc; this level of bandwidth usage by DoS
trafﬁc means that 90% (96%) of packets are from the attacker.
The DoS threshold can be decreased even further, to 128
bytes, by checking against a few ﬁlters just before matching
triggers, to see if a ﬂow would likely match any trigger (this
is cheaper than checking which trigger a ﬂow matches). At
this threshold, a very small fraction of Web server ﬂows in a
large content provider would go unmonitored [49].
As discussed in Section 6, the DoS threshold can be cal-
culated by a proﬁling-based model. Figure 11b shows that
the predicted threshold is close to the experimental threshold
over different number of trigger patterns (series show DoS
trafﬁc % and sufﬁx “p” means prediction).
1021031040204060# Flows per triggerSweep time (us)  Flowchunkingw/o−flowchunking124816328101214Burst sizeQuiescent time %  8101214−202468Rate (MPPS)Saved sweeptime (us)  hashprefhugepageb2btriggerpktpref8101214−10123Rate (MPPS)Saved time %  05010020406080Matching %Sweep time (us)  8101214050100010203040Matching %Quiescent time %  810121401230306090Threshold (KB)DoS traffic BW %  1 packet=threshold81632640123# patternsThreshold (KB)  75502575 p50 p25 p(a) Scale to many triggers (the
curves are different #patterns)
(b) Impact of #patterns
Figure 12: Performance of trigger matching
Performance of matching triggers. A key bottleneck in
Trumpet is the cost of matching a packet to a trigger ﬁlter.
Even with aggressive caching of lookups, because our event
descriptions involve a multi-dimensional ﬁlter, this cost can
be prohibitive. Figure 12a depicts the impact of matching
complexity on Trumpet. Recall that we use the tuple search
algorithm to match across multiple dimensions.
The ﬁrst important result is that this cost is independent
of the number of triggers. This is key to our scaling perfor-
mance: we are able to support nearly 4K concurrent triggers
precisely because the tuple search algorithm has this nice
scaling property. Where this algorithm scales less well is in
the direction of the number of “patterns”. Recall that a pat-
tern is a type of ﬁlter speciﬁcation: a ﬁlter expressed on only
srcIP or dstIP is a different pattern than one expressed
on the conjunction of those two ﬁelds. Our matching cost
increases with the number of patterns. It may be possible for
the TEM to analyze trigger descriptions and avoid installing
triggers with too many patterns at a TPM, a form of admission
control8. To further reduce the impact of the total number of
patterns, we can adopt trie indexes [52] to reduce the number
of hash table lookups. We have left this to future work.
Finally, in Figure 12b we test how matching cost depends
on increasingly complex matching scenarios. We consider
four such scenarios: no matching (no ﬂows match any trig-
gers), same 8 triggers (all the ﬂows match the same 8 triggers,
the other triggers are not matched), diff 8 triggers (each ﬂow
matches 8 different triggers, but these triggers have the same
ﬁlter), and 8 trigger patterns (each ﬂow matches 8 different
trigger patterns). We observe that the per packet processing
time increases from no matching to same 8 triggers and to
diff 8 triggers. However, the processing time does not fur-
ther grow with 8 trigger patterns because our performance
does not depend on whether a ﬂow matches different trigger
pattern or not, but only depends on the number of patterns.
TPM’s feasibility region. We ran experiments with differ-
ent trafﬁc properties (packet rate, number of active ﬂows,
ﬂow arrival rate) and TPM parameters (number of triggers,
number of triggers per ﬂow, time interval) to explore the
feasibility region for the TPM on our server. We call a set
of parameters feasible if, for those parameters, TPM does
not drop packets and completes all sweeps. The feasibility
region is the maximal boundary of feasible parameter sets.
We run each parameter set 5 times and show the feasibility
region in Figure 13 for 300 and 600 active ﬂows per time
8More generally, TEM might perform other forms of admission control,
such as rejecting events whose ﬁlters span a large part of the address space.
(a) 300 active ﬂows
(b) 600 active ﬂows
Figure 13: Feasibility region over the TPM parameters
interval (e.g., 10ms). The settings have different number of
triggers per ﬂow (x axis) and different time intervals (series).
As the number of triggers per ﬂow increases, the number of
triggers also increases accordingly. For example, the setting
with 8 triggers per ﬂow has 4k triggers, and the setting with
16 triggers per ﬂow has 8k triggers.
As we decrease the time interval, there will be fewer cycles
in an epoch to use for sweeps, and there is a higher chance that
the TPM cannot ﬁnish the sweep. For example in Figure 13a,
if the time interval is 5 ms and there are 8 triggers per ﬂow,