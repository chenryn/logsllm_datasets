Memcached
GraphLab
Private
36 GB
0 GB
0 GB
Heap
9 GB
35 GB
4 GB
Stack
60 MB
132 KB
132 KB
Total
46 GB
35 GB
4 GB
Table 3: The size of diﬀerent applications’ memory regions.
471
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
(b) Error InjectionFramework(Re)Start AppInject Errors (Soft/Hard)Run Client WorkloadApp Crash?Compare Result with Expected OutputNOYESRepeatStart23451(a) Application vulnerability
(b) Application incorrectness
Figure 3:
Inter-application variations in vulnerability to single-bit
soft and hard memory errors for the three applications in terms of (a)
probability of crash and (b) frequency of incorrect results.
the sizes of their various regions of memory), they diﬀer in
several ways that we describe next.
WebSearch implements the index searching component of
a production search engine described in [58]. It does this by
storing several hundred gigabytes of index data in persistent
storage and uses DRAM as a read-only cache for around
36 GB of frequently-accessed data. We use a real-world trace
of 200,000 queries as the client workload. The result of a
query is a set of the top four most relevant documents to the
query. We use number of documents returned, the relevance
of the documents to the query, and the popularity score of
the documents as the expected outputs.
To evaluate WebSearch, we used 40 Intel Xeon two-
socket servers with 64 GB DDR3 memory and conducted
our experiments for over two months. In total, we sampled
20,576 unique memory addresses for our experiments across
diﬀerent memory regions, executing a total of over ﬁve
billion queries.
Memcached [59] is an in-memory key–value store for a
30 GB Twitter dataset. It primarily uses DRAM to improve
the performance of read requests. We run a synthetic client
workload that consists of 90% read requests and 10% write
requests. We use the contents of the value fetched by read
requests as the expected outputs.
To evaluate Memcached, we used an Intel Xeon two-socket
server with 48 GB DDR3 memory and sampled more than
983 unique memory addresses for our experiments across
diﬀerent memory regions, executing a total of over six
billion queries.
GraphLab [60] is a framework designed to perform com-
putation over large datasets consisting of nodes and edges.
We use a 1.3 GB dataset of 11 million Twitter users as
nodes with directed edges between the nodes representing
whether one Twitter user follows another. Our workload runs
an algorithm called TunkRank [61] that assigns each node
in a graph a ﬂoating-point popularity score corresponding
to that user’s inﬂuence. We use the scores of the 100 most
inﬂuential users as the expected outputs.
To evaluate GraphLab, we used an Intel Xeon two-socket
server with 48 GB DDR3 memory and sampled more than
2,159 unique memory addresses for our experiments across
diﬀerent memory regions.
B. Application Characterization Results
We applied our characterization methodology from Sec-
tion IV to our applications and next present the results and
insights from this study.
Finding 1: Error Tolerance Varies Across Applications.
Figure 3(a) plots the probability of each of the three applica-
tions crashing due to the occurrence of single-bit soft or hard
(a) Memory region vulnerability
(b) Memory region incorrectness
Figure 4: Memory region variations in vulnerability to single-bit soft
and hard memory errors for the applications in terms of (a) probability
of crash and (b) frequency of incorrect results.
errors in their memory (i.e., application vulnerability). Error
bars on the ﬁgure show the 90% conﬁdence interval of each
probability. In terms of how these errors aﬀect application
correctness, Figure 3(b) plots the rate of incorrect results
per billion queries under the same conditions. Error bars on
the ﬁgure label the maximum number of incorrect queries
during a test per billion queries (the minimum number of
incorrect queries during a test per billion was 0). We draw
two key observations from these results.
First, there exists a signiﬁcant variance in vulnerability
among the three applications both in terms of crash proba-
bility and in terms of incorrect result rate, which varies by
up to six orders of magnitude. Second, these characteristics
may diﬀer depending on whether errors are soft or hard
(for example, the number of incorrect results for WebSearch
diﬀers by over two orders of magnitude between soft and
hard errors). We therefore conclude that memory reliability
techniques that treat all applications the same are ineﬃcient
because there exists signiﬁcant variation in error tolerance
among applications.
Finding 2: Error Tolerance Varies Within an Application.
Figure 4(a) plots the probability of each of the three applica-
tions crashing due to the occurrence of single-bit soft or hard
errors in diﬀerent regions of their memory (with error bars
showing the 90% conﬁdence interval of each probability).
Figure 4(b) plots the rate of incorrect results per billion
queries under the same conditions (with error bars showing
the maximum number of incorrect queries during a test per
billion queries, with the minimum number being 0). Two
observations are in order.
First, for some regions, the probability of an error leading
to a crash is much lower than for others (for example, in
WebSearch the probability of a hard error leading to a crash
in the heap or private memory regions is much lower than
in the stack memory region). Second, even in the presence
of memory errors, some regions of some applications are
still able to tolerate memory errors (perhaps at reduced
correctness). This may be acceptable for applications such as
WebSearch that aggregate results from several servers before
presenting them to the user, in which case the likelihood of
the user being exposed to an error is much lower than the
reported probabilities. We therefore conclude that memory
reliability techniques that treat all memory regions within
an application the same are ineﬃcient because there exists
signiﬁcant variance in the error tolerance among diﬀerent
memory regions.
For the remainder of this section, we use WebSearch as our
exemplar data-intensive application and perform an in-depth
analysis of its tolerance behavior.
472
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
051015Soft ErrorHard ErrorWebSearchMemcachedGraphLabProbability of Crash (%)1E+01E+21E+41E+61E+8Soft ErrorHard ErrorWebSearchMemcachedGraphLab# Incorrect/Billion Queries02468101214Probability of Crash (%)PrivateHeapStackSoft ErrorHard Error00001E+01E+21E+41E+61E+8PrivateHeapStackSoft ErrorHard Error# Incorrect/Billion Queries00000(a) Temporal variation
(b) Safe ratio
Figure 5:
(a) Temporal variation in vulnerability for WebSearch,
presented as frequency distribution function of time between a memory
error is injected and a corresponding eﬀect of that error (either an
incorrect result or a crash) is observed. (b) Safe ratio distribution for
diﬀerent memory regions of WebSearch.
Finding 3: Quick-to-Crash vs. Periodically Incorrect Be-
havior. Figure 5(a) shows the probability of a particular
type of outcome occurring after a certain number of minutes
given that it occurs during an application’s execution time.
For this analysis, we focus on soft errors and note that hard
errors would likely shift these distributions to the right, as
hard errors would continue to be detected over time. We
draw two key conclusions from this ﬁgure.
First, more-severe failures of an application or system due
to a memory error appear to be exponentially distributed and
exhibit a quick-to-crash behavior: they are detected early-
on (within the ﬁrst ten minutes for WebSearch) and result
in an easily-detectable failure mode. Second,
less-severe
failures of an application due to a memory error appear to
be uniformly distributed and exhibit periodically incorrect
behavior: they arise more evenly distributed over time as
incorrect data is accessed.
Finding 4: Some Memory Regions Are Safer Than Others.
Recall from Section III-B that the safe ratio quantiﬁes the
relative frequency of writes to a memory region versus
all accesses to the memory region as a way to gauge the
likelihood of an error being masked by an overwrite in
an application. We sampled the addresses of the memory
regions of WebSearch and plot the distribution of their safe
ratios in Figure 5(b) (we sampled 1590 addresses in total,
with the number of sampled addresses in each memory
region roughly proportional to the size of that region). In
Figure 5(b), the width of each colored region indicates the
probability density for a given safe ratio of the memory
region indicated at the bottom. The line on each distribution
denotes the average safe ratio value for that region. We make
two observations from these results.
First, we notice a diﬀerence in the safe ratios between the
regions of the application that contain programmer-managed
data, the private and heap regions, and the region of the
application that contains compiler-managed data, the stack.
The programmer-managed regions (private and heap), which
in WebSearch contain mainly read-only web index data, have
a smaller potential to mask memory errors with overwrites
due to their lower write intensity. The compiler-managed
region (stack), on the other hand, contains data that
is
frequently expanded and discarded whenever new functions
are called or returned from, giving it a high potential to mask
memory errors with overwrites (cf. Figure 1 outcome 1).
Second, we note that though memory regions may have
relatively low safe ratios (such as private and heap), errors
in these regions may still be masked by application logic
(cf. Figure 1 outcome 2.1). For example, the private and
heap regions, despite their low safe ratios, are still quite
(a) WebSearch vulnerability
(b) WebSearch incorrectness
Figure 6: Vulnerability of WebSearch to types of errors in terms of
(a) probability of crash and (b) frequency of incorrect result.
robust against memory errors (with crash probabilities of
1.04% and 0.64%, respectively, in Figure 4a and low rates
of incorrect results in Figure 4b). Prior works have examined
techniques to identify how applications mask errors due to
their control ﬂow [62, 63]. These can potentially be used in
conjunction with our technique to further characterize the
safety of memory regions.
We conclude that some application memory regions are
safer than others because either application access pattern
or application logic can be the dominant factor in diﬀerent
memory regions to mask a majority of memory errors.
Finding 5: More Severe Errors Mainly Decrease Correct-
ness. We next examine how error severity aﬀects application
vulnerability. To do so, we emulated three common error
types: single-bit soft errors and single-/two-bit hard errors.
Figure 6(a) shows the how diﬀerent error types aﬀect the
probability of a crash and Figure 6(b) shows how the
diﬀerent error types aﬀect the rate of incorrect results. We
ﬁnd that while the probability of a crash is relatively similar
among the diﬀerent error types (without any conclusive
trend), the rate of incorrect results can increase by multiple
orders of magnitude. Especially large is the increase in
incorrect result rates from single-bit soft errors to single-
bit hard errors, which may vary by around three orders
of magnitude. Thus, more severe errors seem to have a
larger eﬀect on application correctness as opposed to an
application’s probability of crashing.
Finding 6: Data Recoverability Varies Across Memory
Regions. We used our methodology from Section IV-B to
measure the amount of memory in WebSearch that was
implicitly recoverable (read-only data with a copy on persis-
tent storage) and explicitly recoverable (data that is written
to every ≥5 minutes on average) and show our results in
Table 5. Note that the same data may be both implicitly
and explicitly recoverable, so the percentages in Table 5
may sum to greater than 100%. We make three observations
from this data. First, we ﬁnd that for a signiﬁcant fraction of
the address space (at least 82.1%, pessimistically assuming
the implicitly recoverable and explicitly recoverable data
completely overlap) of WebSearch, it is feasible to recover
a clean copy from disk with low overhead. This suggests
that there is ample opportunity for correcting memory errors
in software with low overhead. Second, we ﬁnd that, for
Memory region
Implicitly recoverable
Explicitly recoverable
Private
Heap
Stack
Overall
88%
59%
1%
82.1%
63.4%
28.4%
16.7%
56.3%
Table 5: Recoverable memory in WebSearch.
473
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:30:37 UTC from IEEE Xplore.  Restrictions apply. 
00.20.416111621263136 Incorrect Response System/App CrashTime after Error Injection (Minutes)ProbabilityPrivateHeapStackMemory Regions00.20.40.60.81Safe Ratio0510PrivateHeapStackSingle-bit SoftSingle-bit HardMulti-bit HardProbability of Crash (%)1E+01E+21E+41E+6PrivateHeapStackSingle-bit SoftSingle-bit HardMulti-bit Hard# Incorrect/Billion QueriesDesign dimension
Technique
Beneﬁts
Trade-oﬀs
Hardware techniques
No detection/correction
Parity
SEC-DED/DEC-TED
Chipkill [10]
Mirroring [12]
Less-Tested DRAM
No associated overheads (low cost)
Relatively low cost with detection capability No hardware correction capability
Tolerate common single-/double-bit errors
Tolerate single-DRAM-chip errors
Tolerate memory module failure
Saved testing cost during manufacturing
Increased cost and memory access latency
Increased cost and memory access latency
100% capacity overhead
Increased error rates
Unpredictable crashes and silent data corruption
Software responses
Usage granularity
Simple, no performance overhead
Consume errors in application
Automatically restart application Can prevent unpredictable application behavior May make little progress if error is frequent
Reduces memory space (usually very little)
Retire memory pages
Conditionally consume errors
Memory management overhead to make decision
Usually has performance overheads
Software correction
Low overhead, eﬀective for repeating errors
Flexible, software vulnerability-aware
Tolerates detectable memory errors
Unpredictable crashes and data corruption
Physical machine
Virtual machine
Application
Memory region
Memory page
Cache line
Simple, uniform usage across memory space Costly depending on technique used
More ﬁne-grained, ﬂexible management
Manageable by the OS
Manageable by the OS
Manageable by the OS
Most ﬁne-grained management
Host OS is still vulnerable to memory errors
Does not leverage diﬀerent region tolerance
Does not leverage diﬀerent page tolerance
Does not leverage diﬀerent data object tolerance
Large management overhead; software changes
Table 4: Heterogeneous reliability design dimensions, techniques, and their potential beneﬁts and trade-oﬀs.
WebSearch, more data is implicitly recoverable than explic-
itly recoverable. This is likely due to the fact that most
of WebSearch’s working set of data is an in-memory read-
only cache of web document indices stored on disk. Thus,
a large portion of the data in applications like WebSearch
that cache persistent data in memory can tolerate memory
errors in software, without any additional need to maintain
a separate copy of data on disk. Third, even if WebSearch
did not maintain a copy of its data on disk, the majority
of its data (56.3%) could still be recovered with relatively
low overhead. We therefore conclude that for data-intensive
applications like WebSearch, software-only memory error