 None of these recordings are included in the publications of existing detectors.
#BHUSA @BlackHatEvents
Information Classification: General
Selected Existing Detectors
 Deep4SNet
 Latest CV-based approach
 We use the implementation open-sourced by the original authors
 RawNet2
 E2E-based approach as ASVspoof 2021 baseline
 We use the implementation open-sourced by the ASVspoof 2021
 Farid et al.
 First CV-Based approach on top conference
 We use the implementation open-sourced in BlackHat USA 2019
 DeepSonar (Not selected)
 First NNF-based approach, but no open-source implementation available
 We have tried our best to contact the authors, but have received no response yet
#BHUSA @BlackHatEvents
Information Classification: General
Real-world FPR of Existing Approaches
 This experiment is to answer RQ1
 Real-world Environment
 Unbalanced samples.  Human speeches 
are much more than fake speeches 
 False Alarm Rate (FPR) is very important 
for real-world deployment
 Results
 We use the baseline datasets to evaluate 
the FPR of existing approaches
 Obviously, NONE of their FPRs is 
acceptable for real world
 Answer to RQ1: NO
Approach
Baseline
Positive
FPR
Farid et al.
English
3,656
67.70%
Mandarin
11,020
45.39%
Deep4SNet
English
3,597
66.61%
Mandarin
22,081
90.95%
RawNet2
English
5,099
94.43%
Mandarin
11,580
47.70%
#BHUSA @BlackHatEvents
Information Classification: General
Speaker-irrelative Feature Evaluation
 This experiment is to answer RQ2
 There are two parts in this experiment
 Slight denoise
Not affect the human voice but removes the current sound and the background noise
The detection results should not be changed, in theory.
 Silence removal
Only use the Mandarin baseline dataset (English baseline dataset has no silence) 
Crop the samples to remove the silences before and after the human voice
Theoretically, these silences have nothing to do with human speech
The detection results should not be changed, in theory.
#BHUSA @BlackHatEvents
Information Classification: General
Speaker-irrelative Feature Evaluation
 Slight denoise
ALL existing approaches are 
significantly affected by background 
noise
This means that the noise of human 
recordings may help fake voices 
bypass the detection of existing 
approaches.
 Diff*
Compared with original baseline 
results
Approach
Baseline
DN-FPR
Diff *
Farid et al.
English
75.09%
↑ 10.92%
Mandarin
84.37%
↑ 85.88%
Deep4SNet
English
59.85%
↓ 10.15%
Mandarin
99.37%
↑ 9.26%
RawNet2
English
97.22%
↑ 2.95%
Mandarin
55.74%
↑ 16.86%
#BHUSA @BlackHatEvents
Information Classification: General
Speaker-irrelative Feature Evaluation
 Silence removal
ALL existing approaches are 
significantly affected by 
meaningless silence
This means that the silence part of 
human recordings may help fake 
voices bypass the detection of 
existing approaches.
 Diff*
Compared with original baseline 
results
Approach
Baseline
DN-FPR
Diff *
Farid et al.
Mandarin
84.37%
↑ 85.88%
Deep4SNet
Mandarin
99.37%
↑ 9.26%
RawNet2
Mandarin
55.74%
↑ 16.86%
#BHUSA @BlackHatEvents
Information Classification: General
Speaker-irrelative Feature Evaluation
 Conclusion
Speaker-irrelative features represented by background noise and silences are indeed 
accounted by the detection systems as part of the feature vector to determine 
whether a specific sample is a human speech or not.
It lays the foundation for this paper to bypass the AI-synthesized speech detection 
systems through speaker-irrelative features.
 Answer to RQ2: YES
#BHUSA @BlackHatEvents
Information Classification: General
Detection Bypass Evaluation
 This experiment is to answer RQ3
 For cost reasons, we only use the English language for this experiment
 We removed the samples falsely reported as positive in baseline dataset
 For each human recording, we generate five new recordings:
I’m not kidding you, this voice is fake
The weather is really nice today
I’ve sent you the number via WeChat
Can you please lend me some money
You need to come to the office tomorrow
#BHUSA @BlackHatEvents
Information Classification: General
Detection Bypass Evaluation
 Compared to the original human recordings in baseline dataset
 Fake recordings generated by SiF-DeepVC have much higher negative rates
 It means that SiF-DeepVC can effectively deceive existing approaches 
 SiF-DeepVC recordings are more “human” than real human
 Answer to RQ3: YES
Approach
SiF-DeepVC Recordings
Baseline Original Recordings
Diff
Negative
Fake
Negative 
Rate (NR)
Negative
Real
Baseline NR
Farid et al.
2,823
8,720
32.37%
1,744
5,400
32.30%
↑ 1.00%
Deep4SNet
6,294
9,015
69.82%
1,803
5,400
33.39%
↑ 109.10%
RawNet2
46
1,505
3.06%
301
5,400
5.57%
↓ 45.06%
Average
9,136
19,240
47.62%
3,848
16,200
23.57%
↑ 102.86%
#BHUSA @BlackHatEvents
Information Classification: General
Speaker-irrelative Features on VC
 This experiment is to answer RQ4
It’s randomized and single-blind
 We recruited a group of 10 participants 
Listen to selected recordings with headphones
Manually verify whether the selected recordings are clear and understandable
 We randomly selected these recordings
100 recordings generated by SiF-DeepVC (Output Voice)
100 recordings generated by Voice Cloner (Cloned Voice)
100 recordings from the English baseline dataset (Human Voice)
#BHUSA @BlackHatEvents
Information Classification: General
Speaker-irrelative Features on VC
 Conclusion
We can clearly see that there is no 
statistical difference between these 
voices
We believe that people can 
understand the output voices of SiF-
DeepVC very well
 Answer to RQ4: YES
Approach
Baseline
DN-FPR
Diff *
Farid et al.
Mandarin
84.37%
↑ 85.88%
Deep4SNet
Mandarin
99.37%
↑ 9.26%
RawNet2
Mandarin
55.74%
↑ 16.86%
Type
Understandable
Total
Ratio
Baseline
97
100
97.00%
Cloned 
Voice
94
100
94.00%
Output 
Voice
96
100
96.00%
#BHUSA @BlackHatEvents
Information Classification: General
Conclusion
#BHUSA @BlackHatEvents
Information Classification: General
Takeaways
 AI-synthesized speeches generation and detection
How to generate AI-synthesized speeches
Existing detection approaches and their problems
A novel attack framework which can bypass existing detectors
 Difficulty of defending against AI-synthesized speeches
With SiF-DeepVC, the cloned voice can be more “human” than human
Risk warning for blue side: existing solutions are far from "usable"
 New datasets for future researches
We build and open-source several new datasets with high quality
#BHUSA @BlackHatEvents
Information Classification: General
Demos
 We deeply understand the importance of reproducibility
 All code of this project is available on GitHub
Some code included in our repository are from the following projects
Deep4SNet: https://github.com/yohannarodriguez/Deep4SNet
Farid et al.: https://github.com/cmrfrd/DetectingDeepFakes_BlackHat2019
RawNet2: https://github.com/eurecom-asp/rawnet2-antispoofing
RTVC: https://github.com/CorentinJ/Real-Time-Voice-Cloning
 All datasets used in this project are also available to the public
Get the compressed archive (zip file) by Google Drive
Please note that the size of this zip file about 7.8 GB
#BHUSA @BlackHatEvents
Information Classification: General
Description for Datasets
 The zip file contains the following datasets:
 RQ1:
 “for-real-validation”: original human recordings from FoR Validation dataset (also used in RQ3)
 “zh-real-test”: original human recordings from MagicData Test dataset for (also used in RQ3)
 RQ2:
 “for-real-validation-denoised”: slightly denoised “for-real-validation” (RQ2)
 “zh-real-test-denoised”: slightly denoised “zh-real-test” (RQ2)
 “zh-real-test-silenced”: silence-removed “zh-real-test” (RQ2)
 RQ3:
 “for-bh-madefake-final-r4k”: cloned fake voices by SiF-DeepVC for Farid et al. 
 “for-deep4s-madefake-final-r4k”: cloned fake voices by SiF-DeepVC for Deep4SNet
 “for-rawnet-madefake-final-r4k”: cloned fake voices by SiF-DeepVC for RawNet2 
 RQ4：
 Take any sample you want ☺
#BHUSA @BlackHatEvents
Information Classification: General
Sadly, We Cannot Really Detect the Fake Voices Now
Maybe we can do it in the future
#BHUSA @BlackHatEvents
Information Classification: General
Thanks