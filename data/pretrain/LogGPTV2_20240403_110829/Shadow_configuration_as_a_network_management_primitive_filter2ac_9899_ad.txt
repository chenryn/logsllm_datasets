shadow bit S, and a PD bit to indicate whether the payload can
be dropped. Both S and PD are always 0 for transit trafﬁc. For
IPv4 packets, S uses the low bit of the version ﬁeld, and the PD
uses the unused ﬂag bit. Such a mapping causes shadow packets
to be automatically dropped by routers that are not shadow-aware.
Two additional bits are needed during commitment: TP indicates
whether a tag is present, and TG indicates the tag. We store TP
in the highest bit of the TOS ﬁeld and TG in the next highest bit.
We use the highest four bits of the ARP header’s operation ﬁeld to
mark ARP packets. Note that it is also possible to encode some or
all of this information in a shim header.
Packets received by the kernel are demultiplexed according to the
translation table (and the tag assignment during commitment). A
reference to the appropriate conﬁguration is stored in the packet’s
data structure for usage in key parts of the TCP/IP stack such as
the routing cache and FIB lookups, ICMP errors, and UDP/TCP
demultiplexing.
Shadow-aware Programs:
Since we also would like provide
support for existing programs, we allow a default conﬁguration
to be deﬁned for a process, and the attribute is inherited by child
processes. Sockets created by a process initially belong to the
process’s default conﬁguration. We can then launch any program
within the desired conﬁguration.
A shell is started for each conﬁguration to enable an operator
to apply changes to a particular conﬁguration. The shell indicates
whether its conﬁguration is currently deﬁned as the real or shadow.
Routing Processes and Tools:
In most implementations, routing
processes are normal user processes. Changing networking con-
ﬁgurations in the Linux kernel is primarily done using netlink
sockets. By starting a routing process in the appropriate shell, its
sockets are associated with that conﬁguration and the kernel inter-
prets the changes to entities as applying to that conﬁguration. We
conﬁgure Quagga and XORP such that two instances can be run-
ning concurrently, allowing both a shadow and real conﬁguration
to be deployed.
The same technique is applied to common network testing tools
such as ping, traceroute, and homegrown scripts, allowing
them to operate without modiﬁcation. We use this approach with
our custom trafﬁc generation program and measurement program
used in our evaluations.
It is possible that some vendors add shadow-awareness directly
to userspace processes (e.g., to use a shared RIB to further reduce
memory overhead or supporting additional features in traceroute),
while others may want to reduce code changes.
Connection to Outside: Our implementation uses proxies to han-
dle control plane connectivity to outside of a srnet. Such connec-
tivity is necessary to support incremental deployment and interdo-
main scenarios. These simple proxies can handle not only normal
operations but also shadow commitment.
Consider the example of eBGP. Suppose without shadow conﬁg-
urations a BGP routing process b has a BGP peer e in another do-
main; that is, b has a TCP connection at port 179. With shadow con-
ﬁgurations, corresponding to b, there may be two BGP processes
br and bs for the real and shadow conﬁgurations. We introduce a
proxy bp for b. Then bp peers with the external BGP peer e (by
listening at the IP address and BGP port 179). The process bp for-
wards each incoming BGP message from e to both br and bs, which
can then apply its ingress ﬁltering policies. Whenever br sends a
BGP message to e, it is forwarded to bp which forwards to e.
We use a novel transaction rollback technique to handle commit-
ment with visible external effects. Speciﬁcally, the proxy keeps a
log of forwarded messages. Whenever bs sends a BGP message
to e, it is stored locally by bp. If the network swaps the real and
shadow conﬁgurations, bp computes the differences of the mes-
sages of br and bs, rolls back the unnecessary impacts of br (i.e.,
withdraw different routes), and then installs the effects of bs with-
out disconnecting the external BGP connection.
Shadow-aware Interfaces: It is necessary for routers to drop shadow
packets and untag transit packets (in the case of commitment) be-
fore exiting an srnet. We enable a shadow-aware attribute on each
interface that participates in the srnet.
Since our evaluation environment utilizes ARP, there is one ad-
ditional complexity during commitment. Egress trafﬁc should not
be delayed or possibly dropped while it waits for the new conﬁgu-
ration to query for the MAC address of the peering router outside of
the srnet. Thus, we conﬁgure the kernel to accept unsolicited ARP
replies and duplicate any received ARP reply to the shadow conﬁg-
uration for interfaces with the shadow-aware attribute disabled.
8. EVALUATIONS
We ﬁrst present our methodology, then present our results in two
parts. In the ﬁrst part, we present results that show that the overhead
of supporting shadow conﬁguration is very small. In the second
part, we demonstrate the effectiveness of shadow conﬁgurations in
three usage scenarios.
8.1 Methodology
Implementation: We use our implementation as described in Sec-
tion 7.
Conﬁgurations: We use the conﬁguration ﬁles of the two operat-
ing networks in Table 1. US-ISP is a large US tier-1 ISP.
We use the conﬁgurations of US-ISP only for evaluation of FIB
size overhead. The rest of our experiments use a small illustrative
topology and an emulation of the Abilene backbone. We use Emu-
lab’s [52] 3 Ghz PCs with 1 Gbps and 100 Mbps Ethernet links. We
#Nodes
Network
Abilene
US-ISP
#Directed Links
Syntax
Juniper
Cisco
Table 1: Network conﬁgurations used.
26
-
9
-
take additional steps to load conﬁguration data into our emulation
of the Abilene backbone. Conﬁguration commands are translated
to both XORP and Quagga syntax. Then BGP routes from Abi-
lene’s July 2007 BGP RIB dumps are injected as static routes at
virtual egress points, dummy0 interfaces, at the appropriate routers.
Routes for the University of Utah are removed so as not to inter-
fere with the Emulab addresses conﬁgured on the routers. Since
the versions of XORP and Quagga used did not support IS-IS, we
translated Abilene’s conﬁgurations to use OSPF.
Data Trafﬁc: We use CAIDA [6] packet traces in our evaluations.
When using these traces on our emulation of the Abilene backbone,
we remove packets for destination addresses not appearing in the
BGP routes accepted by Abilene.
Performance Measurements: To obtain performance measure-
ments under packet cancellation, we use a custom utility similar
to iperf that timestamps generated packets just following the IP
header and sends using raw sockets. The timestamp is not lost dur-
ing packet cancellation. We modify the kernel to deliver canceled
packets to raw sockets. The server computes delay between send-
ing and receiving time, and uses linear regression to subtract off
mean delay and account for clock drift.
8.2 Overhead
Since we intend that shadow conﬁguration be used in production
networks, the overhead of supporting it should be small. One rea-
son we chose Linux is to see the overhead in a general platform. We
consider (a) data path forwarding overhead due to additional com-
plexity to support a shadow conﬁguration; (b) FIB storage overhead
due to addition of a shadow conﬁguration; and (c) FIB update over-
head due to addition of a shadow conﬁguration.
Data Path Forwarding Overhead: Our results show that there
is truly a negligible overhead on the data forwarding path due to
the additional complexity of supporting a shadow conﬁguration.
For this test, we use a particular trafﬁc load both with the stan-
dard Linux kernel, and then again with our shadow-enabled kernel.
When employing our shadow kernel, we load a shadow conﬁgura-
tion but do not generate shadow trafﬁc.
We use a topology with 3 routers with 1 Gbps Ethernet links;
there is a sending, intermediate, and receiving router. The sending
router uses the Linux kernel’s pktgen module to generate 300-
byte packets so we can stress-test the intermediate router’s forward-
ing path. Our implementation doesn’t use any additional memory
copies for real packets, so larger packet sizes do not add overhead
in our shadow kernel.
The sending router transmits packets for 30 seconds with ran-
domly generated destination IP addresses in the range 10.0.0.4-
10.255.255.255 to ensure that FIB lookups (on the intermediate
router) are rarely handled by the routing cache. The intermediate
router conﬁgures one default route for 10.0.0.0/8 to route to the re-
ceiving router, and also adds additional 9306 randomly generated
entries from 10.0.0.0/8 with a preﬁx length distribution matching
the global BGP tables published by the Route Views Project on
January 18, 2008 [44]. Note that there are no preﬁx lengths shorter
than 8. Also, 9293 routes are added in shadow conﬁguration, with
60% of the preﬁxes shared with the real conﬁguration.
The comparison in CPU utilization between our shadow kernel
and the standard kernel are shown in Figure 8. The machines are
hyperthreaded, so we increase the data rate until the CPU han-
dling the input interface interrupts reaches 100% utilization. The
reported value is the overall CPU utilization including both CPUs.
Our implementation does not noticeably increase CPU utilization
 100
 80
 60
 40
 20
)
%
(
e
g
a
s
U
U
P
C
 0
Standard Kernel
Shadow Kernel
 150
 200
 0
 50
 100
Throughput (Mbps)
 250
Figure 8: System CPU utilization
for varying trafﬁc rates (300-byte
packets).
 100
 80
 60
 40
 20
)
%
(
e
g
a
s
U
U
P
C
Standard Kernel
Shadow Kernel
 0
 0
 5
 25
 20
 15
 10
Time (sec)
 30
Figure 9: System CPU utilization
for FIB updates (100 Mbps, 300-
byte packets).
Single Router Removed
Multiple Routers Removed
35%
35%
 100
 80
 60
 40
 20
)
%
(
e
s
a
e
r
c
n
I
y
r
o
m
e
M
 0
 0
 20  40  60  80  100
Normalized Router ID (Sorted)
 80  100
Figure 10: FIB storage overhead for topology
changes in shadow conﬁguration (US-ISP).
 20
% Routers Removed
 40
 60
 0
as compared to the standard kernel. Note that the full 1 Gbps ca-
pacity is not reached due to the smaller packet sizes. Our imple-
mentation can achieve the same rates cited by [4] for 1430-byte
packets.
FIB Storage Overhead: One concern is that the number of FIB
entries will be increased. However, for most networks, the network
preﬁxes are relatively ﬁxed, and thus should appear in both real and
shadow conﬁgurations. Accordingly, the number of entries and IP
preﬁx lookup costs do not increase signiﬁcantly. We incur a storage
overhead only if the shadow and real conﬁgurations specify differ-
ent next-hop behaviors, since otherwise only a single FIB entry is
required.
Scenario
Remove NEWY↔WASH
Remove LOSA
Remove KANS
Changed FIB Entries Memory Overhead
4.7%
1.6%
7.2%
13074
4467
19874
Table 2: FIB storage overhead (Abilene).
Table 2 shows the increase in FIB size across all routers for the
Abilene network due to conﬁguration changes made in the shadow
conﬁguration. Both the real and the shadow conﬁgurations have
more than 90,000 FIB entries. We observe these topology changes
lead to a small overall storage increase. Although in the theoretical
worst case the storage may double, in our real implementation the
increase is less than 8% due to the sharing between real and shadow
next-hops. We anticipate that this sharing is common.
To evaluate the scenarios for a larger network, we use the conﬁg-
uration of US-ISP, a large tier-1 ISP. We use its backbone topology,
OSPF link weight conﬁguration, and external routes to compute
the FIB size at each router. Each router has a few hundreds of thou-
sands of FIB entries. The presented memory overhead is based on
data structure sizes in the Linux kernel implementation.
Figure 10 shows the results for two scenarios. The vertical bar
denotes the maximum and minimum per-router memory overhead,
and the dark points denote the average memory overhead over all
routers. In the ﬁrst scenario, we show the memory overhead when
only a single router at a time is removed from the network in the
shadow conﬁguration. We observe that in the worst cases, the
routers with the worst FIB overhead have their FIB storage in-
creased by no more than 35%. These “worst” routers are often stub