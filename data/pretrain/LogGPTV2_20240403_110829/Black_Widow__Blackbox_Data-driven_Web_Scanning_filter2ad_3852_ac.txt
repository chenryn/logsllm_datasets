11 295
33 254
4 964
4 643
138 732
464
8 285
274
7 307
28 143
7 197
10 582
5 677
3 357
43 166
158
7 004
494
46 909
937
31
328
110
72
1 018
5
577
14
5 114
32 213
3 217
12 533
5 601
4 312
156 513
404
8 202
111
26 785
25 803
15 275
31 303
5 040
3 688
25 385
218
7 087
657
27 431
725
768
1 255
661
79
3 053
6
171
9
640
32 981
16 345
33 975
6 070
4 431
148 868
520
8 976
495
37 073
25 035
2 147
9 861
4 571
3 569
33 030
102
6 313
273
17 143
498
3
576
103
21
118
2
18
0
73
15 610
16 001
7 655
6 722
4 247
141 032
340
8 396
379
25 732
42 406
2 491
36 181
3 919
3 753
40 866
282
6 893
389
28 484
2 591
24
1 659
209
65
110
2
145
2
781
Vulnerabilities: In addition to code coverage, we also
evaluate how good the scanners are at ﬁnding vulnerabilities.
This includes how many vulnerabilities they can ﬁnd and
how many false positives they generate. While there are
many vulnerability types, our study focuses on both reﬂected
and stored XSS.
To evaluate the vulnerability detection capabilities of the
scanners, we collect and process all the vulnerabilities they
report. First, we manually analyze if the vulnerabilities can
be reproduced or if they should be considered false positives.
Second, we cluster similar vulnerability reports into a set of
unique vulnerabilities to make a fair comparison between
the different reporting mechanisms in the scanners. We do
this because some applications, e.g. SCARF, can generate an
inﬁnite number of vulnerabilities by dynamically adding new
input ﬁelds. These should be clustered together. Classifying
the uniqueness of vulnerabilities is no easy task. What
we aim to achieve is a clustering in which each injection
corresponds to a unique line of code on the server. That
is, if a form has multiple ﬁelds that are all stored using
the same SQL query then all these should count as one
injection. The rationale is that it would only require the
developer to change one line in the server code. Similarly,
for reﬂected injections, we cluster parameters of the same
request together. We manually inspect the web application
source code for each reported true-positive vulnerability to
determine if they should be clustered.
Scanners: We compare our scanner Black Widow with
both Wget [21] for code coverage reference and 6 state-
of-the-art open-source web vulnerability scanners from both
academia and the web security community: Arachni [18],
Enemy of the State [13], j ¨Ak [8], Skipﬁsh [22], w3af [16]
and ZAP [17]. We use Enemy of the State and j ¨Ak as
they are state-of-the-art academic blackbox scanners. Skip-
ﬁsh, Wget and w3af are included as they serve as good
benchmarks when comparing with previous studies [13], [8].
Arachni and ZAP are both modern open-source scanners that
have been used in more recent studies [23]. Including a pure
crawler with JavaScript capabilities, such as CrawlJAX [9],
could serve as a good coverage reference. However, in this
paper we focus on coverage compared to other vulnerability
scanners. We still include Wget for comparison with previ-
ous studies. While it would be interesting to compare our
results with commercial scanners, e.g. Burp Scanner [24],
the closed source nature of these tools would make any type
of feature attribute hard.
We conﬁgure the scanners with the correct credentials for
the web application. When this is not possible we change the
default credentials of the application to match the scanner’s
default values. Since the scanners have different capabilities,
we try to conﬁgure them with as similar conﬁgurations as
possible. This entails activating crawling components, both
static and dynamic, and all detection of all types of XSS
vulnerabilities.
Comparing the time performance between scanners is
non-trivial
to do fairly as they are written in different
languages and some are sequential while others run in
parallel. Also, we need to run some older ones in VMs for
compatibility reasons. To avoid inﬁnite scans, we limit each
scanner to run for a maximum of eight hours.
Web Applications: To ensure that
the scanners can
handle different types of web applications we test them on 10
different applications. The applications range from reference
applications that have been used in previous studies to newer
production-grade applications. Each application runs in a
VM that we can reset between runs to improve consistency.
We divide the applications into two different sets. Ref-
erence applications with known vulnerabilities: phpBB
(2.0.23), SCARF (2007), Vanilla (2.0.17.10) and Wack-
oPicko (2018); and modern production-grade applications:
Drupal (8.6.15), HotCRP (2.102), Joomla (3.9.6), osCom-
merce (2.3.4.1), PrestaShop (1.7.5.1) and WordPress (5.1).
C. Code Coverage Results
This section presents the code coverage in each web
application by all of the crawlers. Table I shows the number
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:17 UTC from IEEE Xplore.  Restrictions apply. 
1132
Our Scanner
Common
Other scanner
100%
80%
60%
40%
20%
0%
y
m
e
n
E
y
m
e
n
E
t
e
g
W
P
A
Z
k
¨A
j
t
e
g
W
y
m
e
n
E
f
a
3
w
t
e
g
W
P
A
Z
h
s
i
f
p
i
k
S
t
e
g
W
h
s
i
f
p
i
k
S
y
m
e
n
E
h
s
i
f
p
i
k
S
t
e
g
W
t
e
g
W
f
a
3
w
y
m
e
n
E
k
¨A
j
P
A
Z
t
e
g
W
h
s
i
f
p
i
k
S
i
n
h
c
a
r
A
h
s
i
f
p
i
k
S
t
e
g
W
t
e
g
W
t
e
g
W
k
¨A
j
P
A
Z
f
a
3
w
P
A
Z
k
¨A
j
f
a
3
w
f
a
3
w
P
A
Z
P
A
Z
h
s
i
f
p
i
k
S
P
R
C
t
o
H
P
R
C
t
o
H
P
R
C
t
o
H
p
o
h
S
a
t
s
e
r
P
F
R
A
C
S
p
o
h
S
a
t
s
e
r
P
p
o
h
S
a
t
s
e
r
P
p
o
h
S
a
t
s
e
r
P
p
o
h
S
a
t
s
e
r
P
p
o
h
S
a
t
s
e
r
P
a
l
m
o
o
J
a
l
m
o
o
J
a
l
m
o
o
J
F
R
A
C
S
e
c
r
e
m
m
o
C
s
o
F
R
A
C
S
P
R
C
t
o
H
s
s
e
r
P
d
r
o