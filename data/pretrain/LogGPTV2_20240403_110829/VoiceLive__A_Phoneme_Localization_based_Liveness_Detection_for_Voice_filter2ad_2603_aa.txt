title:VoiceLive: A Phoneme Localization based Liveness Detection for Voice
Authentication on Smartphones
author:Linghan Zhang and
Sheng Tan and
Jie Yang and
Yingying Chen
VoiceLive: A Phoneme Localization based Liveness
Detection for Voice Authentication on Smartphones
Linghan Zhang†, Sheng Tan†, Jie Yang†, Yingying Chen∗
†{lzhang, tan, jie.yang}@cs.fsu.edu, ∗PI:EMAIL
†Florida State University, Tallahassee, FL 32306, USA
∗Stevens Institute of Technology, Hoboken, NJ 07030, USA
ABSTRACT
Voice authentication is drawing increasing attention and
becomes an attractive alternative to passwords for mobile
authentication. Recent advances in mobile technology fur-
ther accelerate the adoption of voice biometrics in an array
of diverse mobile applications. However, recent studies show
that voice authentication is vulnerable to replay attacks,
where an adversary can spoof a voice authentication system
using a pre-recorded voice sample collected from the vic-
tim. In this paper, we propose VoiceLive, a practical liveness
detection system for voice authentication on smartphones.
VoiceLive detects a live user by leveraging the user’s unique
vocal system and the stereo recording of smartphones. In
particular, with the phone closely placed to a user’s mouth,
it captures time-diﬀerence-of-arrival (TDoA) changes in a
sequence of phoneme sounds to the two microphones of the
phone, and uses such unique TDoA dynamic which doesn’t
exist under replay attacks for liveness detection. VoiceLive
is practical as it doesn’t require additional hardware but
two-channel stereo recording that is supported by virtually
all smartphones. Our experimental evaluation with 12 par-
ticipants and diﬀerent types of phones shows that VoiceLive
achieves over 99% detection accuracy at around 1% Equal
Error Rate (EER). Results also show that VoiceLive is ro-
bust to diﬀerent phone placements and is compatible to dif-
ferent sampling rates and phone models.
Keywords
Voice recognition; Liveness detection; Phoneme localization
1.
INTRODUCTION
As a primary way of communication, our voice is a partic-
ularly attractive biometric for identifying users. It reﬂects
individual diﬀerences in both behavioral and physiological
characteristics, such as the inﬂection and the shape of the
vocal tract [23]. Such distinctive behavioral and physiologi-
cal traits could be captured by voice authentication systems
for diﬀerentiating each individual [17]. Voice authentication
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24-28, 2016, Vienna, Austria
© 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978296
leveraging built-in microphones on mobile devices is partic-
ularly convenient and low-cost, comparing to the passwords
authentication that is diﬃcult to use while on-the-go and
requires memorization. Recent advances in mobile technol-
ogy further accelerate the adoption of voice biometrics in an
array of diverse mobile applications.
Indeed, voice authentication has been introduced recently
to mobile devices and apps to provide secure access and lo-
gins. For example, Google has integrated it into Android
operating systems (OSs) to allow users to unlock mobile de-
vices [2], and Tencent has updated its WeChat mobile app
to support voice biometric logins [8]. Another appealing
use case of voice authentication is to support mobile ﬁnan-
cial services. For instance, SayPay provides voice biometric
solution for online payment, e-commerce, and online bank-
ing [5]. And an increasing number of ﬁnancial institutions,
HSBC, Citi, and Barclays for example, are deploying voice
authentication for their telephone and online banking sys-
tems [3]. This trend is expected to continue growing at a
rate of 22.15 percent yearly until 2019, and will result in an
estimated $113.2 billion market share by 2017 [4]. Voice au-
thentication thus becomes an attractive alternative to pass-
words in mobile authentication and is increasingly popular.
Voice authentication however has been shown to be vul-
nerable to replay attacks in recent studies [16, 33, 14]. An
adversary can spoof a voice authentication system by us-
ing a pre-recorded voice sample collected from the victim.
The voice sample can be any recording captured inconspic-
uously. Or, an adversary can obtain voice samples from the
victim’s publicly exposed speeches. The attacker could even
concatenate voice samples from a number of segments in
order to match the victim’s passphrase. Such attacks are
most accessible to the adversary due to the proliferation of
mobile devices, such as smartphones and digital recorders.
They are also highly eﬀective in spooﬁng authentication sys-
tems, as evidenced by recently work [32, 33]. Replay attacks
therefore present signiﬁcant threats to voice authentication
and are drawing increasing attention. For example, Google
advises users on the vulnerability of their voice logins by
displaying a popup message “... a recording of your voice
could unlock your device.” [1]
Prior work in defending against replay attacks is to uti-
lize liveness detection to distinguish between a passphrase
spoken by a live user and a replayed one pre-recorded by
the adversary. For example, Shang et al. propose to com-
pare an input voice sample with stored instances of past
accesses to detect the voice samples have been seen before
by the authentication system [31]. This method, however,
1080cannot work if the attacker records the voice samples during
a non-authentication time point. Villalba et al. and Wang et
al. suggest that the additional channel noises introduced by
the recording and loudspeaker can be used for attack detec-
tion [32, 33]. These approaches however have limited eﬀec-
tiveness in practice. For example, the false acceptance rates
of these approaches are as high as 17%. Chetty and Wag-
ner propose to use video camera to extract lip movements
for liveness veriﬁcation [13], whereas Poss et al. combine
the techniques of a neural tree network and Hidden Markov
Models to improve authentication accuracy [28]. Aley-Raz
et al. develop a liveness detection system based on “Intra -
session voice variation” [10], which is integrated into Nuance
VocalPassword [6]. In addition to a user-chosen passphrase,
it requires a user to repeat one or more random sentences
prompted by the system for liveness detection.
In this paper, we introduce and evaluate a phoneme sound
localization based liveness detection system on smartphones.
Our system distinguishes a passphrase spoken by a live user
from a replayed one by leveraging (i) the human speech pro-
duction system and (ii) advanced smartphone audio hard-
ware. First, in human speech production, a phoneme is the
smallest distinctive unit sound of a language. Each phoneme
sound can be viewed as air waves produced by the lungs, and
then modulated by the movements of vocal cords and vocal
tract including throat, mouth, nose, tongue, teeth, and lips.
Each phoneme sound thus experiences unique combination
of place and manner of articulation. Consequently, diﬀer-
ent phoneme sounds could be located at diﬀerent physical
positions in the human vocal tract system with an acous-
tic localization method. Second, smartphone hardware is
now supporting advanced audio capabilities. Virtually all
smartphones are equipped with two microphones for stereo
recording (one on the top and the other one at the bottom),
and are capable of recording at standard 48kHz and 192kHz
sampling rates. For example, with the latest Android OSs,
Samsung Galaxy S5 and Note3 are capable of stereo record-
ing at 192kHz, which yields 5.21 microseconds’ time reso-
lution or millimeter-level ranging resolution1. We thus can
leverage such stereo recording or dual microphones on smart-
phones to pinpoint the sound origin of each phoneme within
human vocal system for liveness detection.
Ideally, locating a phoneme sound requires at least three
microphones with three individual audio channels. Although
current two-channel stereo recording cannot uniquely lo-
cate the phoneme sound origin, it can capture the time-
diﬀerence-of-arrival (TDoA) of each phoneme sound to the
two microphones of the phone. With the phone closely
placed to user’s mouth, the diﬀerences in TDoA between
most phoneme sounds are distinctive and measurable with
millimeter-level ranging resolution. Very importantly, each
passphrase (usually 5 to 7 words [7, 30]) consists of a se-
quence of diﬀerent phoneme sounds that will produce a se-
ries of TDoA measurements with various values. We refer
to the changes in TDoA values as TDoA dynamic, which
is determined by the speciﬁc passphrase, the placement of
the phone, and a user’s unique vocal system. Such TDoA
dynamic, which doesn’t exist under replay attacks, is then
utilized for liveness detection.
In particular, when a user ﬁrst enrolled in the system,
the TDoA dynamic of the user-chosen or system prompted
1Assuming the speed of sound is 340m/s, each digital sample
represents a distance of 1.77mm.
passphrase is ﬁrst captured by the smartphone stereo record-
ing, and then stored in the system. During online authen-
tication phase, the extracted TDoA dynamic of an input
utterance will be compared to the one stored in the sys-
tem. A live user is detected, if that produce a similarity
score higher than a pre-deﬁned threshold. By relaxing the
problem from locating each phoneme sound to measuring
the TDoA dynamic for a sequence of phonemes, we enable
liveness detection on a single phone without any additional
hardware. Our system does have the limitation of requiring
a user to hold the phone close to her/his mouth with the
same pose in both enrollment and authentication processes.
The contributions of our work are summarized as follows:
• We show that the origin of each phoneme can be uniquely
located within the human vocal tract system by us-
ing a microphone array. It lays the foundation of our
phoneme localization based liveness detection system.
• We develop VoiceLive, a practical liveness detection
system that extracts the TDoA dynamic of the passphrase
for live user detection. VoiceLive takes advantages of
the user’s unique vocal system and high quality stereo
recording of smartphones.
• We conduct extensive experiments with 12 participants
and three diﬀerent types of phones under various ex-
perimental settings. Experimental results show that
VoiceLive achieves over 99% detection accuracy at around
1% EER. Results also show that VoiceLive is robust to
diﬀerent phone placements and is compatible to diﬀer-
ent sampling rates and phone models.
The remainder of this paper expands on above contribu-
tions. We begin with system and attack model, and a brief
introduction to phoneme sounds localization.
2. PRELIMINARIES
2.1 System and Attack Model
There exists two types of voice authentication systems:
text-dependent and text-independent. We primarily focus
on the text-dependent system as it is currently the most
commercially viable method and produces better authen-
tication accuracy with shorter utterances [31].
In a text-
dependent system, the text to be spoken by a user is the
same one for enrollment and veriﬁcation. Such text could
be either a user-chosen or system prompted one. Figure 1
shows the processes of a typical voice authentication sys-
tem. Our method can also be extended to text-independent
systems, which will be discussed in Section 5.
For the attack model, we consider replay attacks, which
are the most accessible and eﬀective attacks aiming at spoof-
ing the system by replaying a pre-recorded voice sample of
the victim [32]. We consider the replay attacks that take
place at two locations, at the microphone point and at the
transmission point, as shown in Figure 1. For the sake of
simplicity, we refer to the former as a playback attack and
the latter as a replace attack. In a playback attack, an adver-
sary uses a speaker to replay the pre-recorded voice sample
in front of the microphones.
In a replace attack, an ad-
versary replaces his/her own speech signal as the victim’s
before or during transmission. This can be done by lever-
aging the availability of the virtual recorder to bypass the
local microphones, or by intercepting and replacing speech
signal during transmission.
10817UDQVPLVVLRQSRLQW
UHSODFHDWWDFN
([LVWLQJ
6SHDNHU0RGHO
0LFURSKRQH
0LFURSKRQHSRLQW
SOD\EDFNDWWDFN
&ODVVLILHU
'HFLVLRQ
(cid:708)$FFHSWRUQRW(cid:709)
)HDWXUH
([WUDFWLRQ
Figure 1: A typical voice authentication system with
two possible places of replay attacks.
ůǀĞŽůĂƌ
΀ƚ΁΀Ě΁΀Ŷ΁΀Ɛ΁΀ǌ΁΀ů΁
ŝůĂďŝĂů
΀Ɖ΁΀ď΁΀ŵ΁
>ĂďŝŽĚĞŶƚĂů
΀Ĩ΁΀ǀ΁
ĞŶƚĂů
΀ɽ΁΀ĝ΁
WĂůĂƚĂů
΀࡚΁΀ࡩ΁΀ࡾ΁΀ࡻ΁΀ũ΁
sĞůĂƌ
΀Ŭ΁΀Ő΁΀ż΁΀ǁ΁
Figure 3: Place of articulation and corresponding
consonants.
΀ŝ΁
΀Ž΁
>ŝƉƐ
&ŽƌǁĂƌĚ
΀ŝ΁
ďĞĞƚ
ďŝƚ΀ܼ΁
dŽŶŐƵĞ
΀ࠧ΁
ďĞƚ΀ܭ΁
΀Ƶ΁
ďŽŽƚ
΀࠯ࡡ΁
ďŽĂƚ
hƉ
sŽŝĐĞ
ďŽƚ
΀ࠪ΁
΀ࠨ΁
(a) Example
ďĂƚ΀č΁
(b) Vowel chart
Figure 2: Tongue positions of English vowels within
the oral cavity, and the vowel chart.
2.2 Human Speech Production and Phonemes
The human speech production system involves three vi-
tal physiological components: lungs, vocal cords, and vocal
tract [27]. When someone exhales, air is expelled from the
lungs, and then passes over the vocal cords, which dilate or
constrict to allow or impede the air ﬂow to produce unvoiced
or voiced sound. Such sound is then resonated and reshaped
by the vocal tract that consists of multiple organs such as
throat, mouth, nose, tongue, teeth, and lips. The vocal cords
modulation, interaction and movement of these organs can
alter sound waves and produce unique human sounds.
A phoneme is the smallest distinctive unit sound of a lan-
guage [27]. The two major phoneme categories are vow-
els and consonants. In particular, vowels are the phoneme
sounds produced when vocal cords constrict air ﬂow (i.e.,
voiced sound) but with an open vocal tract. The tongue
position is the most important physical feature that distin-
guishes one vowel from another [27]. As diﬀerent tongue po-
sitions lead to diﬀerent multipath environments inside the
oral cavity, we can locate the sound origins of diﬀerent vow-
els at diﬀerent physical locations inside the human oral cav-
ity. As illustrated in Figure 2 (a), when the tongue moves
to lower right corner, vowel [A] can be pronounced, whereas
when the tongue moves to upper left corner and backward,
vowels [i] and [o] can be produced, respectively. More gen-
erally, Figure 2 (b) shows the vowel chart which involves
two dimensions of tongue movements: up/down movements
(i.e., height) and back/forth movements (i.e., backness). Ex-
tending or retracting the tongue forward or backward to-
wards the teeth produces a more front or back vowel sound,
whereas lowering or raising the tongue towards lower jaw or
towards the roof of mouth produces a more open or close
vowel.
Unlike vowels, consonants are produced when vocal cords
either constrict or dilate air ﬂow and with signiﬁcant con-
striction of the air ﬂow in the oral cavity. The articulation
place and manner are two major factors that distinguish one
DĂŶŶĞƌ
WůĂĐĞ
EĂƐĂů
^ƚŽƉ
&ƌŝĐĂƚŝǀĞ
ĨĨƌŝĐĂƚĞ
ƉƉƌŽǆŝŵĂƚĞ
>ĂƚĞƌĂů
ŝůĂďŝĂů >ĂďŝŽĚĞŶƚĂů ĞŶƚĂů ůǀĞŽůĂƌ WĂůĂƚĂů sĞůĂƌ
΀ż΁
΀Ŭ΁΀Ő΁
΀ŵ΁
΀Ɖ΁΀ď΁
΀ࢠ΁΀ࡩ΁
΀Ŷ΁
΀ƚ΁΀Ě΁
΀Ɛ΁΀ǌ΁
΀Ĩ΁΀ǀ΁
΀ɽ΁΀ĝ΁
΀ࣄ΁΀ࡻ΁
΀ǁ΁
΀ů΁
Figure 4: Consonants chart based on place and man-
ner of articulation.
consonant from another [27]. The combined eﬀect of place
and manner of articulation and voiced/unvoiced sound lead
to diﬀerent consonant sounds emitted from diﬀerent loca-
tions within the human vocal tract system. In particular,
place of articulation is the location where the constrictions
or obstructions of air stream occur, and can be categorized
into 6 groups: bilabial, labiodental, dental, alveolar, palatal,
and velar. Figure 3 shows each group and the corresponding
consonants. For example, the consonants [p][b][m][w] can be
pronounced when the obstruction of air stream occurs at up-
per and lower lips. The consonants within each group can be
further distinguished by the manner of articulation, which
describes the conﬁguration and interaction of the speech or-
gans (e.g., the tongue, lips, and palate). There are 6 types
of articulation manners including nasal, stop, fricative, af-
fricate, approximate and lateral. For instance, nasal con-
sonant [m] is produced when the air stream is completely
blocked by mouth and only passes through the nose. Fig-
ure 4 summarizes the categorization of diﬀerent consonants
based on place of articulation and manner of articulation.
The bolded font in the ﬁgure shows the voiced sounds (e.g.,
[b] and [v]), whereas the rest are unvoiced sounds (e.g., [p]
and [f ] ).
2.3 Phoneme Localization using Microphone
Array
We next conduct experiments to study how the origin of
phoneme sound is located within the human vocal tract sys-
tem by leveraging a microphone array. We utilize six ex-
ternal microphones organized in three pairs A, B, and C.
As shown in Figure 5 (a), the microphones are distributed
in the X-Z plane2 with 5cm and 10cm horizontal distances,
and 5cm and 7.6cm vertical distances. Such a distribution