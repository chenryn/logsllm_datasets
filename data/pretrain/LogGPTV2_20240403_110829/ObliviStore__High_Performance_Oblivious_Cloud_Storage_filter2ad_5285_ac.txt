with the new data before being placed in the eviction cache.
The ORAM main handler then updates the position map
accordingly.
Partition reader. The partition reader
is chieﬂy in
reading a requested block from a chosen
charge of
partition.
the form
ReadPartition(partition, blockid), where responses are
passed through callback functions.
Background shufﬂer. The background shufﬂer is in charge
of scheduling and performing the shufﬂing jobs. Details of
the background shufﬂer will be presented in Section V-E.
takes in asynchronous calls of
It
B. Data Structures and Data Flow
Table II summarizes the data structures in our ORAM
construction, including the eviction cache, position map,
storage cache, shufﬂing buffer, and partition states.
Informally, when a block is read from the server, it is
ﬁrst cached by the storage cache. Then, this block is either
directly fetched into the shufﬂing buffer to be reshufﬂed; or
it is passed along through the partition reader to the ORAM
main handler, as the response to a data access request. The
ORAM main handler then adds the block to an eviction
cache, where the block will reside for a while before being
fetched into the shufﬂing buffer to be reshufﬂed. Reshufﬂed
blocks are then written back to the server asynchronously
(unless they are requested again before being written back).
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
Type of cache-in
Early cache-in
Shufﬂing cache-in
Dummy cache-in
Real cache-in
Table I: Types of cache-ins: when and for what purposes blocks are being read from the server.
Explanation
[Partition reader] Early cache-in is when the client reads a block needed for shufﬂing over a normal data
access. Speciﬁcally, when the partition reader tries to read a block from a partition that is currently being
shufﬂed: if a level being shufﬂed does not contain the requested block, or contains the requested block but the
requested block has already been cached in earlier, the client caches in a previously unread block that needs
to be read for shufﬂing. The block read could be real or dummy.
[Background shufﬂer] Shufﬂing cache-in is when a block is read in during a shufﬂing job.
[Partition reader] During a normal data access , if a level is currently not being shufﬂed, and the requested
block does not reside in this level, read the next dummy block from a pseudo-random location in this level.
[Partition reader] During a normal data access, if the intended block resides in a certain level, and this block
has not been cached in earlier, read the real block.
Below we explain the storage cache in more detail.
The partition states will be explained in more detail
in
Section V-C. The remaining data structures in Table II have
appeared in the original SSS ORAM.
Storage cache. Blocks fetched from the server are temporar-
ily stored in the storage cache, until they are written back to
the server. The storage cache supports two asynchronous
i.e., CacheIn(addr) and CacheOut(addr).
operations,
Upon a CacheIn request, the storage cache reads from the
server a block from address addr, and temporarily stores this
block till it is cached out. Upon a CacheOut request, the
storage cache writes back to the server a block at address
addr, and erases the block from the cache.
Blocks are re-encrypted before being written back to
the storage server, such that the server cannot link blocks
based on their contents. The client also attaches appropriate
authentication information to each block so it can later
verify its integrity, and prevent malicious tampering by the
untrusted storage (see full version [39]).
cache
i.e., Fetch(addr)
supports
two
and
Store(addr, block), allowing the caller to synchronously
fetch a block that already exists in the cache, or
to
synchronously store a block to the local cache.
storage
operations,
Additionally,
synchronous
also
the
There are 4 types of cache-ins, as described in Table I.
C. ORAM Partitions
Each partition is a smaller ORAM instance by itself. We
employ a partition ORAM based on the hierarchical con-
struction initially proposed by Goldreich and Ostrovsky [14],
and specially geared towards optimal practical performance.
Each partition consists of 1
2 log N + 1 levels, where level
i can store up to 2i real blocks, and 2i or more dummy
blocks.
For each ORAM partition, the client maintains a set of
partition states as described below.
Partition states. Each partition has the following states:
• A counter Cp. The value of Cp ∈ [0, partition capacity)
(cid:7)
signiﬁes the state of partition p. Speciﬁcally, let Cp :=
i bi · 2i denote the binary representation of the counter
Cp corresponding to partition p. This means that the state
258
of the partition p should be as below: 1) for every non-
zero bit bi, level i of the partition is ﬁlled on the server;
and 2) for every bit bi = 0, level i is empty.
• Job size Jp, which represents how many blocks (real or
dummy) are scheduled to be written to this partition in
the next shufﬂe. Jp is incremented every time a partition
p is scheduled for an eviction. Notice that the actual
eviction and the associated shufﬂing work may not take
place immediately after being scheduled.
• A bit bShuﬄe, indicating whether this partition is cur-
rently being shufﬂed.
• Dummy counters. Each partition also stores a dummy
block counter for each level, allowing a client to read
the next a previously unread dummy block (at a pseudo-
random address).
• Read/unread ﬂags. For every non-empty level, we store
which blocks remain to be read for shufﬂing.
Batch shufﬂing. In the SSS ORAM algorithm [40], a new
shufﬂing job is created whenever a block is being written to
a partition – as shown in Figure 4 (left). The SSS algorithm
performs these shufﬂing jobs sequentially, one after another.
Notice that a new shufﬂing job can be created while the
corresponding partition is still being shufﬂed. Therefore, the
SSS algorithm relies on a shufﬂing job queue to keep track
of the list of pending shufﬂing jobs.
As a practical optimization, we propose a method to batch
multiple shufﬂing jobs together (Figure 4 – right). When
a shufﬂing job is being started for a partition p, let Cp
denote the current partition counter. Recall that the binary
representation of Cp determines which levels are ﬁlled for
partition p. Let Jp denote the current job size for partition
p. This means that upon completion of this shufﬂing, the
partition counter will be set to Cp + Jp. Furthermore, the
binary representation of Cp +Jp determines which levels are
ﬁlled after the shufﬂing is completed. The values of Cp and
Jp at the start of the shufﬂing job jointly determine which
levels need to be read and shufﬂed, and which levels to be
written to after the shufﬂing. Figure 4 (right) shows the idea
behind batch shufﬂing.
New blocks can get scheduled to be evicted to partition
p before its current shufﬂing is completed. ObliviStore does
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
stay ahead of the normal data accesses, and prevents too
much shufﬂing work from starving the data accesses.
Among the above semaphores, the early cache-in, shuf-
ﬂing buffer, and eviction semaphores are meant to bound the
amount of client-side storage, thereby satisfying the client
storage constraint. For early cache-ins and shufﬂing cache-
ins, we bound them by directly setting a limit on the cache
size, i.e., how many of them are allowed to be concurrently
in the cache. The eviction semaphore mandates how much
data accesses are allowed to stay ahead of shufﬂing – this
in some sense is bounding the number of real blocks in the
eviction cache. As explained later, due to security reasons,
we cannot directly set an upper bound on the eviction
cache size as in the early cache-in and shufﬂing buffer
semaphores. Instead, we bound the number of real blocks
indirectly by pacing the data accesses to not stay too much
ahead of shufﬂing work. Finally, the shufﬂing I/O semaphore
constrains how much shufﬂing I/O work can be performed
before serving the next data access request. This is intended
to bound the latency of data requests.
Preventing information leakage through semaphores.
One challenge is how to prevent information leakage through
semaphores. If not careful,
the use of semaphores can
potentially leak information. For example, when reading
blocks from the server, some blocks read are dummy, and
should not take space on the client-side to store. In this
sense, it may seem that we need to decrement a semaphore
only when a real block is read from the server. However,
doing this can potentially leak information, since the value
of the semaphore inﬂuences the sequence of events, which
the server can observe.
Invariant 1 (Enforcing oblivious scheduling). To satisfy
the oblivious scheduling requirement, we require that the
values of semaphores must be independent of
the data
access sequence. To achieve this, operations on semaphores,
including incrementing and decrementing, must depend only
on information observable by an outside adversary who does
not now the data request sequence.
For example, this explains why the eviction semaphore
does not directly bound the eviction cache size as the early
cache-in and shufﬂing buffer semaphores do – since other-
wise the storage server can potentially infer the current load
of the eviction cache, thereby leaking sensitive information.
To address this issue, we design the eviction semaphore
not to directly bound the amount of eviction cache space
available, but to pace data accesses not to stay too much
ahead of shufﬂing. The SSS paper theoretically proves that
if we pace the data accesses and shufﬂing appropriately,
N ) with high
the eviction cache load is bounded by O(
probability.
E. Detailed Algorithms
√
The ORAM main, partition reader, and background shuf-
ﬂer algorithms are detailed in Figures 5, 6, and 7 re-
Figure 4: Batch shufﬂing – a new optimization technique for
grouping multiple shufﬂings into one.
try to cancel
the current shufﬂing of partition p to
not
accommodate the newly scheduled eviction. Instead, we
continue to ﬁnish the current shufﬂing, and effectively queue
the newly scheduled evictions for later shufﬂing. To do this,
at the start of each shufﬂing, we i) take a snapshot of the job
size: (cid:8)Jp ← Jp; and ii) set Jp ← 0. This way, we can still
use Jp to keep track of how many new blocks are scheduled
to be evicted to partition p, even before the current shufﬂing
is completed.
D. Satisfying Scheduling Constraints with Semaphores
Our asynchronous ORAM construction must decide how
to schedule various operations, including when to serve data
access requests, how to schedule shufﬂings of partitions, and
when to start shufﬂing jobs.
Constraints. We wish to satisfy the following constraints
when scheduling various operations of the ORAM algorithm.
• Client storage constraint. The client’s local storage
should not exceed the maximum available amount. Par-
ticularly,
there should not be too many early reads,
shufﬂing reads, or real reads.
• Latency constraint. Data requests should be serviced
within bounded time. If too many shufﬂing jobs are in
progress, there may not be enough client local storage to
serve a data access request, causing it to be delayed.
Semaphores. To satisfy the aforementioned scheduling con-
straints different components rely on semaphores to coordi-
nate with each other. In our ORAM implementation, we use
four different types of semaphores, where each type indicates
the availability of a certain type of resource.
1) early cache-ins semaphore, indicating how many remain-
ing early cache-ins are allowed,
2) shufﬂing buffer semaphore, indicating how many more
blocks the shufﬂing buffer can store,
3) eviction semaphore, indicating how much data access is
allowed to stay ahead of shufﬂing. This semaphore is
decremented to reserve “evictions” as a resource before
serving a data access request; and is incremented upon
the eviction of a block (real or dummy) from the eviction
cache.
4) shufﬂing I/O semaphore, indicating how much more I/O
work the background shufﬂer is allowed to perform. This
semaphore deﬁnes how much the shufﬂer is allowed to
259
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
spectively. We highlighted the use of semaphores in bold.
Notice that all semaphore operations rely only on publicly
available information, but not on the data request sequence –
both directly or indirectly. This is crucial for satisfying the
oblivious scheduling requirement, and will also be crucial
for the security proof in the full version [39].
F. Security Analysis: Oblivious Scheduling
We now formally show that both the physical addresses
accessed and the sequence of events observed by the server
are independent of the data access sequence.
Theorem 1. Our asynchronous ORAM construction satisﬁes
the security notion described in Deﬁnition 1.
In the full version [39], we formally show that an adver-
sary can perform a perfect simulation of the scheduler with-
out knowledge of the data request sequence. Speciﬁcally,
both the timing of I/O events and the physical addresses
accessed in the simulation are indistinguishable from those
in the real world.
VI. DISTRIBUTED ORAM
in this case,
One naive way to distribute an ORAM is to have a
single trusted compute node with multiple storage partitions.
However,
the computation and bandwidth
available at the trusted node can become a bottleneck as
the ORAM scales up. We propose a distributed ORAM that
distributes not only distributes storage, but also computation
and bandwidth.
Our distributed ORAM consists of an oblivious load
balancer and multiple ORAM nodes. The key idea is to
apply the partitioning framework (Section III) twice. The
partitioning framework was initially proposed to reduce the
worst-case shufﬂing cost in ORAMs [35, 40], but we observe
that we can leverage it to securely perform load balancing
in a distributed ORAM. Speciﬁcally, each ORAM node is a
“partition” to the oblivious load balancer, which relies on the