c
e
m
T
i
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
l=100
l=300
l=506
l=602
l=702
l=817
2
(cid:3)
◦
+
++
+
+
+
◦
+++
++
++++
+
+
++
+++
+++++
◦◦◦◦◦◦◦◦◦◦◦◦
◦
++
+++++
+
◦◦◦◦◦
◦◦
+
++
++
◦◦◦
◦
+
◦◦◦◦◦◦◦
+
++
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
◦
++
+++
◦◦◦◦◦◦◦◦◦◦
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
+
(cid:3)
+
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
◦◦
+++++
(cid:3)(cid:3)(cid:3)
+
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
++++
2222
222
◦◦◦◦◦◦◦◦◦◦◦◦◦
2
222
2
2
+
22222
2
22
2
2
2
+++
2
22222222
222
+++
22222
+
0
◦◦◦◦◦◦◦◦◦◦◦◦◦
(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)(cid:3)
2222
22
500
2222
100
200
400
300
600
g (granularity in LoC)
++++
+++
+
+++
++
+
800
700
900
Fig. 8: Time required for preprocessing the functions by vary-
ing the granularity level. The graphs represent six functions
with 100, 300, 506, 602, 702, and 817 LoC, respectively.
t
ﬁ
e
n
e
B
40
35
30
25
20
15
10
5
0
0.0002l+0.006
×
Approximation: 0.0068l−0.042
××
×
×
×
××
×
×
×
×
×
×
×
××
××
×
×
×
××
×
×
×
×××
×××
×
×××
×
××
××
×
××××××
×
××
××
×
×
×
××
×
××
×
××
××
×
×
×
×××
×
×
×××
××
×
×
×
×
×
×
×××
××
××××
×
××××
××
××××
×
×
××
×
××
×
×
××××
×××××××××
××
××
×
×
××××
×
×××××
×
×
× ×
×
××
××
××
×××
×
××
×××
××
×××××
×××
×
××××
×××
×××××
××
×××
××
××
×
×
×
×
×
×
×
××××
××××
×
×
×
×
×
×××××
××××××××
×
×××
×
×
×××
×××××
××××××
×
×
×××××××××
××
×
×××
×××××
××
××××××
×
××
×
××
×
×
×××
×
×
×
×
×
×
×
×
×
×
×
×××
×
×
×××
×××
×
×
×
×
×××××××××××××
××
×××
×
××
×
×
××××××××
×
×
×
×
××××××××××××××××××××
×
×
0
100
200
300
400
l (in LoC)
500
600
700
Fig. 9: Relative time beneﬁt of cost(g = l) (i.e., function-level
granularity) over cost(g = 4) (i.e., as of ReDeBug) along
varying sized functions in the vulnerability database. Each
point (x-shaped) represents a function consisting of l lines,
and the curved line represents an approximate ﬁtted curve.
Using the cost function, we now estimate the relative
beneﬁt, (i.e., speedup) of function-level granularity (VUDDY)
over using four LoC as granularity (default of ReDeBug). The
Benef it function is given by:
Benef it(l) = cost(g = 4)
cost(g = l)
(4ac + b)l − (12ac + 3b)
=
acl + b
(10)
(11)
To prove the validity of our theoretical estimation, we
conducted another experiment to assess the beneﬁt, with the
functions of our vulnerability database. The result is shown
in Fig. 9. By observing the open source projects in our
dataset, we determined the average number of characters, c,
to be 10, and through the experiment in subsection VI-B,
the complexity of the MD5 hash algorithm was found to be
−5x + 0.006. This observation, enabled us to obtain a
2.00e
graph that approximately ﬁts the experimental data (the curved
graph in Fig. 9), where Benef it(l) asymptotically approaches
34 as l increases.
2) Memory usage: Memory usage is another crucial factor
that determines the scalability of a method. Technically, our
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:26:17 UTC from IEEE Xplore.  Restrictions apply. 
method takes variable granularity l, which always equals the
length of a function. This is highly advantageous compared to
approaches that utilize a ﬁxed granularity-level, e.g., ReDeBug
or CCFinder, in terms of memory usage. Table V shows the
amount of memory used when the functions in our vulner-
ability database are preprocessed. When l was taken as the
granularity level, the least memory was used because only one
ﬁngerprint was generated per function. When the granularity
g was 4 LoC, it consumed 651 MB of memory for processing
and storing the ﬁngerprints of ﬁne-grained functions. This is
considerable overhead, considering that the total size of ﬁles
in the vulnerability database is 21 MB (see subsection V-A).
Formally, l − g + 1 ﬁngerprints were generated for each
function, which led an increase in the amount of memory
space required. It can be observed in Table V that when
is increased, memory use decreases.
the granularity level
However,
the use of
larger granularity reduces memory usage. Instead, the main
reason for the lower memory usage is that functions shorter
than the ﬁxed granularity are not processed. For example, if
we take 100 LoC as the granularity level, functions with a
length is shorter than 100 cannot be processed, and are thus
discarded. Therefore, we can conﬁdently argue that function-
level granularity promotes memory efﬁciency more than other