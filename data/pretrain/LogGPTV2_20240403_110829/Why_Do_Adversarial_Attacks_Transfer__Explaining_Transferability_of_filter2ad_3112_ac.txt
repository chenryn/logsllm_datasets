getting stuck in poor local optima [3, 13, 50]. In addition to
starting the gradient ascent from the initial point x, for non-
linear classiﬁers we also consider starting the gradient ascent
from the projection of a randomly-chosen point of the oppo-
site class onto the feasible domain. This double-initialization
strategy helps ﬁnding better local optima, through the identi-
ﬁcation of more promising paths towards evasion [13, 47, 50].
Poisoning points can be optimized via gradient-ascent pro-
cedures, as shown in Algorithm 1. The main challenge is to
compute the gradient of the attacker’s objective (i.e., the vali-
dation loss) with respect to each poisoning point. In fact, this
gradient has to capture the implicit dependency of the optimal
parameter vector w(cid:63) (learned after training) on the poisoning
point being optimized, as the classiﬁcation function changes
while this point is updated. Provided that the attacker function
is differentiable w.r.t. w and x, the required gradient can be
computed using the chain rule [4, 5, 24, 27, 48]:
3.3 Poisoning Availability Attacks
Poisoning attacks consist of manipulating training data
(mainly by injecting adversarial points into the training set) to
either favor intrusions without affecting normal system opera-
tion, or to purposely compromise normal system operation to
cause a denial of service. The former are referred to as poison-
ing integrity attacks, while the latter are known as poisoning
availability attacks [5, 48]. Recent work has mostly addressed
transferability of poisoning integrity attacks [41], including
backdoor attacks [9, 16]. In this work we focus on poisoning
availability attacks, as their transferability properties have not
yet been widely investigated. Crafting transferable poisoning
availability attacks is much more challenging than crafting
transferable poisoning integrity attacks, as the latter have a
much more modest goal (modifying prediction on a small set
of targeted points).
As for the evasion case, we formulate poisoning in a white-
box setting, given that the extension to black-box attacks is
immediate through the use of surrogate learners. Poisoning
is formulated as a bilevel optimization problem in which
the outer optimization maximizes the attacker’s objective A
(typically, a loss function L computed on untainted data),
while the inner optimization amounts to learning the classiﬁer
on the poisoned training data [4, 24, 48]. This can be made
explicit by rewriting Eq. (1) as:
max
x(cid:48)
s.t.
m
L(Dval,w(cid:63)) =
w(cid:63) ∈ arg min
∑
L(Dtr ∪ (x(cid:48)
j=1
(cid:96)(y j,x j,w(cid:63))
,y),w)
w
(5)
(6)
where Dtr and Dval are the training and validation datasets
available to the attacker. The former, along with the poisoning
point x(cid:48), is used to train the learner on poisoned data, while
the latter is used to evaluate its performance on untainted data,
through the loss function L(Dval,w(cid:63)). Notably, the objective
function implicitly depends on x(cid:48) through the parameters w(cid:63)
of the poisoned classiﬁer.
The attacker’s capability is limited by assuming that the
attacker can inject only a small fraction α of poisoning points
into the training set. Thus, the attacker solves an optimization
problem involving a set of poisoned data points (αn) added
to the training data.
∇xA = ∇xL +
(cid:62)
∂w
∂x
∇wL ,
(7)
where the term ∂w
∂x captures the implicit dependency of the
parameters w on the poisoning point x. Under some regular-
ity conditions, this derivative can be computed by replacing
the inner optimization problem with its stationarity (Karush-
Kuhn-Tucker, KKT) conditions, i.e., with its implicit equation
∇wL(Dtr∪ (x(cid:48),y),w) = 0 [24,27].1 By differentiating this ex-
pression w.r.t. the poisoning point x, one yields:
∇x∇wL +
(cid:62)
∂w
∂x
∇2
wL = 0 .
(8)
wL)−1,
Solving for ∂w
which can be substituted in Eq. (7) to obtain the required
gradient:
∂x , we obtain ∂w
∂x
= −(∇x∇wL)(∇2
(cid:62)
∇xA = ∇xL− (∇xc∇wL)(∇2
wL)
−1∇wL .
(9)
Gradients for SVM. Poisoning attacks against SVMs were
ﬁrst proposed in [4]. Here, we report a simpliﬁed expression
for SVM poisoning, with L corresponding to the dual SVM
learning problem, and L to the hinge loss (in the outer opti-
mization):
(cid:104) ∂ksc
∂xc
(cid:105)(cid:20)Kss
1(cid:62)
0
(cid:21)−1(cid:20)Ksk
(cid:21)
1(cid:62)
1
0
∇xcA = −αc
∂kkc
∂xc
yk + αc
yk . (10)
We use c, s and k here to respectively index the attack
point, the support vectors, and the validation points for which
(cid:96)(y,x,w) > 0 (corresponding to a non-null derivative of the
hinge loss). The coefﬁcient αc is the dual variable assigned
to the poisoning point by the learning algorithm, and k and K
contain kernel values between the corresponding indexed sets
of points.
Gradients for Logistic Regression. Logistic regression is a
linear classiﬁer that estimates the probability of the positive
class using the sigmoid function. A poisoning attack against
logistic regression has been derived in [24], but maximizing a
different outer objective and not directly the validation loss.
1More rigorously, we should write the KKT conditions in this case as
∇wL(Dtr ∪ (x(cid:48),y),w) ∈ 0, as the solution may not be unique.
USENIX Association
28th USENIX Security Symposium    325
(cid:20)∇xc∇θL
(cid:21)(cid:62)(cid:20) ∇2
(cid:21)
(cid:21)−1(cid:20)X(y◦ σ− y)
One of our contributions is to compute gradients for logistic
regression under our optimization framework. Using logistic
loss as the attacker’s loss, the poisoning gradient for logistic
regression can be computed as:
C zc θ
θL X z C
C z X C ∑n
i zi
∇xcA = −
where θ are the classiﬁer weights (bias excluded), ◦ is the
element-wise product, z is equal to σ(1−σ), σ is the sigmoid
of the signed discriminant function (each element of that
1+exp(−yi fi) with fi = xiθ + b), and:
vector is therefore: σi =
y(cid:62)(σ− 1)
1
C,
n
xizix(cid:62)
∑
i + I,
∇2
θL = C
∇xc∇θL = C(I◦ (ycσc − yc) + zcθx(cid:62)
c )
i
(11)
(12)
In the above equations, I is the identity matrix.
4 Transferability Deﬁnition and Metrics
We discuss here an intriguing connection among transfer-
ability of both evasion and poisoning attacks, input gradients
and model complexity, and highlight the factors impacting
transferability between a surrogate and a target model. Model
complexity is a measure of the capacity of a learning algo-
rithm to ﬁt the training data. It is typically penalized to avoid
overﬁtting by reducing either the number of classiﬁer param-
eters to be learnt or their size (e.g., via regularization) [6].
Given that complexity is essentially controlled by the hyper-
parameters of a given learning algorithm (e.g., the number
of neurons in the hidden layers of a neural network, or the
regularization hyperparameter C of an SVM), only models
that are trained using the same learning algorithm should be
compared in terms of complexity. As we will see, this is an im-
portant point to correctly interpret the results of our analysis.
For notational convenience, we denote in the following the
attack points as x(cid:63) = x + ˆδ, where x is the initial point and ˆδ
the adversarial perturbation optimized by the attack algorithm
against the surrogate classiﬁer, for both evasion and poison-
ing attacks. We start by formally deﬁning transferability for
evasion attacks, and then discuss how this deﬁnition and the
corresponding metrics can be generalized to poisoning.
Transferability of Evasion Attacks. Given an evasion attack
point x(cid:63), crafted against a surrogate learner (parameterized
by ˆw), we deﬁne its transferability as the loss attained by
the target classiﬁer f (parameterized by w) on that point, i.e.,
T = (cid:96)(y,x + ˆδ,w). This can be simpliﬁed through a linear
approximation of the loss function as:
T = (cid:96)(y,x + ˆδ,w) (cid:117) (cid:96)(y,x,w) + ˆδ(cid:62)∇x(cid:96)(y,x,w) .
(13)