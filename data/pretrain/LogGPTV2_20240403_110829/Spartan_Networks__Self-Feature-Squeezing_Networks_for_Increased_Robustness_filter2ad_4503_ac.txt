duced was to reduce the gradient or otherwise hinder the attacker’s
access to the gradient. Gu and Rigazio [21] directly penalize the
network to forcefully lower its gradients through a regularization,
while Papernot et al. [38] use distillation. Distillation is a process
where a first network is trained on one-hot labels (δi =class)i∈[1,k]
where k is the number of classes. Then, the first network is dis-
carded, and the predictions of the trained model are used as the
new labels. This teacher network smoothes-out the labels. A second
network is then trained on these smooth labels, thus considerably
lowering the gradient values. The authors claim that the reduction
factor is above 1000.
Black-Box surrogate attacks [37] have bypassed this gradient
masking defense: these attacks do not need any gradient. Thus,
the only way defensive distillation can impact these gardientless
attacks is by regularizing the network’s behaviour. Unfortunately,
even if a modified and extended defensive distillation has been
able to resist some attacks [36], some attacks [9] can still generate
adversarial samples bypassing this defense.
Nayebi and Ganguli [34] have used a regularization function
rewarding using the saturation values of activations. Network acti-
vations that are far from a zero-gradient point are penalized through
a regularization.
Various similar methods create a non-differentiable variant of
their model at test time to deny any access to the gradient: Athalye
et al. [5] have systematically reviewed — and bypassed — these
so-called "gradient shattering" techniques.
Ross and Doshi-Velez [40] add another regularization term to the
loss. Rather than only regularizing the parameters’ values during
the training, they also regularize the value of the input gradient
through double backpropagation[13]. This gradient reflects how
much each variation of the input can change the class. By dimin-
ishing this value, the defender forces any attacker to create larger
perturbations. The authors claim that successful adversarial sam-
ples against their system have had their meanings modified, and
thus cannot be deemed adversarial.
4.3 Other defensive techniques
Feature Squeezing. Xu et al. [43] have shown that, by re-
4.3.1
ducing the attacker’s sample space by using semantic-preserving
compression / filtering (eg. going from greyscale to Black and White
Spartan Networks
arXiv Preprint, Dec 2018,
images), one can detect adversarial samples. We do so using the fact
that legitimate inputs often create agreeing predictions when in-
ferred on through different compressions. For adversarial samples,
these predictions are different.
4.3.2 Thermometer Encoding. Discovered by Buckman et al. [8],
this method discretizes the values of the dataset according to arbi-
trary thresholds. As an example, if we have 4 tresholds at(80, 60, 40, 20)
and values between 0 and 100, 42 will be encoded as (0, 0, 1, 1), and
77 as (0, 1, 1, 1)
5 MOTIVATIONS FOR SPARTAN NETWORKS
5.1 Discretization
When given labelled data, a classifier is usually given dimensonally-
large inputs. Input dimensionalities can vary from several hundred
to a milion. However, label space dimensionality ranges from 2 to a
hundred. This means that the input space is orders of magnitude
larger than the output space.
For example, the CAL10K music dataset[42] has a compressed
size of 2.1GB, but the size of the annotations file is 10MB. For the
images dataset, the standard MNIST handwritten digits dataset[26]
has 10 different possible annotations and thus can be stored on four
bits. The digital image needs 6272 bits to be stored.
DNN can succesfully create a reliable approximator that can clas-
sify correctly and reliably given an appropriate amount of labelled
data in standard settings. Adversarial examples show that there are
pockets of inputs present in the high-dimensional space that are
missclassified. Adversarial training provides a way to change the
function approximated by neural networks, but it cannot guarantee
that it reliably "patched" the system. Even if augmented, the num-
ber of training datapoints is indeed negligible in comparison to the
number of possible inputs for a given class. High-dimensionality
creates a disadvantage for the defender.
Given an adequate ϵ (1) the perturbation is not perceptible to a
human observer, but successfully tricks the network into giving a
wrong answer. This means that:
than human distinguishability power.
• the input space of encoded samples has a local density greater
• the network behaviour is extremely sensitive to small changes.
In neural layers using a ReLU or ReLU-equivalent activation
function, stimulus perception is almost always near linear or near
multilinear. Thus, any small variation of the stimulus can be propa-
gated through the neural network using the fact that all operations
are differentiable. The ReLU is made to be equal to the identity
function when recieving a positive input, meaning that the ReLU is
a linear function when activated.
As an example, human observers cannot distinguish colors be-
tween {61, 175, 250}RGB and {61, 170, 250}RGB, but can distinguish
cyan from blue. The DNN however can distinguish the former ex-
tremely well.
The attacker, by modifying the value of parts of the input by a
small amount, can also make the value of activation function vary
by a proportional amount. This perturbation will be propagated
to all affine combinations including the perturbed inputs. As the
propagation goes forward, the perturbation, if cleverly crafted by
the attacker, spreads and its amplitude grows within the network,
using the weights of the network to trick and force the network
to output another value. This exploitation can be done for any
network having a succession of locally-linear activation functions,
through the Taylor expansion of any differentiable function.
Various authors have stated [3, 8] that excessive linearity seems
to be a point of vulnerability of neural networks.
From Xu et al. [43], we learned that squeezing features could
sometimes prevent adversarial perturbations from being effective.
Buckman et al. [8] extend this idea with the Thermometer Encoding.
We extend on the Thermometer Encoding strategy by learning the
tresholds that are useful to correctly do the task while using the
minimal number of information.
Hence our hypotheses are as follows:
• Current neural networks exhibit locally-linear properties
that can be used to slide the sample. classification towards
another class.
• The dimensionality is one of the factors allowing attackers
to find a good attack vector.
We summarize all the problems discussed above in Fig. 1. A Deep
Feedforward Neural Network has a very large input space for a
small output space and excessive linearity allows the attacker to
explore the space and find an adversarial examples.
The human response to color seems to be non-linear, as seen in
Fig 2. The little perturbation is not seen, and double this perturba-
tion is. The tresholding does not destroy any semantics, as one can
recognize the digit.
When stimuli values are discretized, small changes to any stimu-
lus must go past the next threshold to create any perturbation. The
attacker will thus have to push the values further than it normally
would in order to put a valid perturbation, that would be visible to
an observer. Two samples on the same threshold are indeed consid-
ered as the same to the system. The work of Buckman et al. [8] is
using this idea.
The non-linearity of discretization as well as its saturation prop-
erties could restrain an attacker from propagating its perturbations
throughout any DNN.
Previous attemps failed at training with staircase-like functions,
that are inherently non-differentiable, as the ’derivative’ of these
functions is a sequence of impulses.
5.2 Binary Encoding
As seen previously in the works of Buckman et al. [8], one can use
a vector-of-bits encoding to make a network more robust against
arXiv Preprint, Dec 2018,
F. Menet et al.
O1
O2
SMax
SMax
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
I1
I2
I3
I4
I5
I6
Figure 1: A Deep Feedforward Neural Network. If the ReLU
or any locally-linear activation function is used, an attacker
can find, out of all the possible weights, the highest weights
bending prediction towards misclassification. Red means an
increase, blue a decrease. By increasing the value of a part
of the input, the attacker can linearly manipulate part of
the calculation and decrease the current probability of the
current class and increase the probabilty of another
adversarial perturbations. Combined with the work of Nayebi and
Ganguli [34], we hypothesize that robust learning in DNN can be
achieved by:
• making their behaviour as non-linear and saturated as pos-
• reducing the attack space for the attacker by squeezing it;
• creating a binary array instead of float activation values.
sible;
We aim at learning an optimal way to achieve this through back-
propagation, by creating new layers of neurons. These neurons will
learn this behaviour by themselves, however we make sure that the
following requirements are met:
• The performance on the test dataset must stay relatively
• We must find a way to learn a highly non-differentiable
close to an unprotected counterpart.
filtering that can interface with backpropagation.
We thus create a processing layer, that we called filtering layer,
whose behaviour is made to be extremely non-linear. This layer
Figure 2: In reading order: a non-perturbed image, an image
with a little, 5% white, vertical bar next to the number, the
10% version, and a thresholded version, with 3 equidistant
thresholds, effectively putting the perturbation at 25%. The
perturbation becomes much easier to see, while the image
keeps its meaning
has a discrete, low-cardinality value range. We can train it through
backpropagation. We modify the DNN of figure 1 into the DNN of
figure 3 with these ideas. Note that Courbariaux et al. [11] have
already proposed restricted-cardinality activation functions.
The processing would use the Heaviside step function, where
the activation function’s definition range is orders of magnitude
larger than the output range. In this case, 9 orders of magnitude for
the Heaviside, instead of less than 1 for the ReLU. As we get less
information-bits for each neuron, we destroy more information.
This process will be called data-starving.
To better understand the interest of data-starving, we emphasize
that most activation functions are encoded on from 16 to 64 bits,
most commonly 32. 32-bit-float activation functions like the 32-
ReLU have 231 possible values, as only positive values are different
from one another. While these activation functions show a high
number of possible states, training set cardinalities are orders of
magitude lower than the possible number of states. Thus, unex-
plored areas in those activation functions can be exploited by an
adversary to find adversarial examples. The defender can thus try
to reduce the amount of states in each layer by various methods.
These methods include using lower weights, or adding a saturation
value. This idea was also exposed in the work of Xu et al. [43].
We decide to reduce the attacker’s space as much as possible,
by taking an activation function that outputs only two possible
values. We will thus use the Heaviside step function as an activation
function in the filtering Layer.
5.3 Spartan Training
During the training, the filtering layers data-starve the network
because there are few possible output values out of them. Moreover,
these layers are regularized by the amount of signal they let through.
To do this, we use a L1 loss on the activations of the network. Thus,
this added loss is proportional to the amount of signal that the
Spartan Networks
arXiv Preprint, Dec 2018,
O1
O2
SMax
SMax
O1
O2
SMax
SMax
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
DU
DU
DU
DU
DU
DU
HS+L1
HS+L1
HS+L1
I1
I2
I3
I4
I5
I6
I1
I2
I3
I4
I5
I6
Figure 3: By breaking the linearity and outputting only dis-
crete values, we make sure that an attacker cannot use gra-
dient descent to find an adversarial example. DU stands for
Discrete Unit.
Figure 4: The Spartan version of the DNN: the first layer is
severely squeezing the dimensionality of the input, and out-
puts binary values only. Furthermore, the activation of each
neuron is added to the global loss of the neural network (L1),
forcing this layer to output as few signal as possible.
system let through. In addition to this loss, the network uses a train-
ing loss based on the error, which is standard for training DNN.
The training loss plays directly against the loss of the filtering layer.
To lower the training loss, the system needs to get information
that increases the filtering loss. The only way to reduce the train-
ing loss is by destroying information that is likely to increase the
filtering loss. We thus have two competitors within the network
fighting to reduce their losses.
The self-adversarial behaviour of the network shall allow the
system to harden itself during this seemingly Spartan Training1, by
allowing it to reduce the value ranges within the network activa-
tions. The attacker will either have to generate a high-amplitude
adversarial noise, or stay in the regulated input space, more re-
stricted, requiring more computing power, and thus yielding a
higher cost for the attacker.
We define the network composed of a data-starving layer con-
nected to a neural network and trained with a composite loss a
1The ancient Spartan Training was known to be extremely harsh.
Spartan Network. To understand the Spartan Training and the fil-
tering layer effect, we transform the Discrete-DNN of Fig. 3 into
the Spartan DNN of figure 4.
6 SPARTAN NETWORK STRUCTURES
In this section, we create three filtering layers and explore the three
different Spartan Networks using them.
6.1 Composite activation function
As stated before, we use the Heaviside step function on the forward
propagation. This activation function’s gradient is zero where it is
defined. The filtering layer thus uses a synthetic gradient in order
to use backpropagation through this zero gradient.
The Heaviside step function’s definition is:
(cid:26) 1 if x ≥ 0
H(x) : R → {0, 1}
H(x) =
0 else
(2)
arXiv Preprint, Dec 2018,