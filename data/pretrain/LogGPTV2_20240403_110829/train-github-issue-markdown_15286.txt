## üêõ Bug
Multiplication of sparse with dense tensor on GPU breaks
## To Reproduce
I have the following implementation of page rank on pytorch using sparse
tensors:
    i = torch.LongTensor(idx)
    values = torch.FloatTensor([1] * len(idx))
    M = torch.sparse.FloatTensor(i.t(), values, torch.Size([4847571, 4847571]))
    N = M.shape[1]
    v = torch.rand(N, 1).float()
    values = torch.FloatTensor([(1 - d)/N] * len(indices))
    temp = torch.sparse.FloatTensor(i.t(), values, torch.Size([4847571,
                                        4847571]))
    if torch.cuda.is_available():
         v = v.cuda()
         M = M.cuda()
         temp = temp.cuda()
    v = v / torch.norm(v, 1)
    M_hat = self.d * M + temp
    for i in range(num_iter):
         v = torch.mm(M_hat, v)
The indices are gained by parsing this graph.
On CPU everything runs fine. On GPU however I am receiving the following
error:
        v = torch.mm(M_hat, v)
    RuntimeError: sub_iter.strides(0)[0] == 0 INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Reduce.cuh:706, please report a bug to PyTorch.
## Environment
    Collecting environment information...
    PyTorch version: 1.5.0
    Is debug build: No
    CUDA used to build PyTorch: 10.2
    OS: Ubuntu 18.04.4 LTS
    GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
    CMake version: version 3.10.2
    Python version: 3.6
    Is CUDA available: Yes
    CUDA runtime version: Could not collect
    GPU models and configuration: GPU 0: Tesla P100-PCIE-16GB
    Nvidia driver version: 440.82
    cuDNN version: Could not collect
    Versions of relevant libraries:
    [pip3] numpy==1.18.3
    [pip3] torch==1.5.0
If it helps more my machine is the first one from here.