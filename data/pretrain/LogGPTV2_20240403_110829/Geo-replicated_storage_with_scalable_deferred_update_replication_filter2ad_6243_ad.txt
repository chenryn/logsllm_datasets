t
(
s
a
b
o
g
l
t
u
p
h
g
u
o
r
h
T
)
s
m
l
(
s
a
b
o
g
l
y
c
n
e
t
a
L
2.5K
2.0K
1.5K
1.0K
0.5K
0.0K
 450
 400
 350
 300
 250
 200
 150
 100
 50
 0
1%
10%
50%
Percentage of global transactions
1%
10%
50%
Percentage of global transactions
Figure 4. Throughput and latency of local and global transactions with reordering in WAN 1.
VII. RELATED WORK
Several protocols for deferred update replication where
servers keep a full copy of the database exist (e.g., [1],
[2], [3], [4], [12]). In [5] it is suggested that the scalability
of these protocols is inherently limited by the number of
transactions that can be ordered, or by the number of
transactions that a single server can certify and apply to
the local database.
Our reordering algorithm is based on the algorithm de-
scribed in [1], originally for reducing the abort rate. In this
paper, we extend the idea to avoid the delay imposed by
global communication on local transactions.
Many storage and transactional systems have been pro-
posed recently. Some of these systems (e.g., Cassandra,5
Dymano [13], Voldemort6) guarantee eventual consistency,
where operations are never aborted but
isolation is not
guaranteed. Eventual consistency allows replicas to diverge
in the case of network partitions, with the advantage that the
system is always available. However, clients are exposed to
conﬂicts and reconciliation must be handled at the applica-
tion level.
Spinnaker [14] is similar to the approach presented here
in that it also use several instances of Paxos to achieve scal-
ability. However, Spinnaker does not support transactions
across multiple Paxos instances.
Differently from previous works, Sinfonia [15] offers
stronger guarantees by means of minitransactions on un-
structured data. Similarly to SDUR, minitransactions are
certiﬁed upon commit. Differently from SDUR, both update
5http://cassandra.apache.org
6http://project-voldemort.com
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
l
)
s
p
t
(
s
a
c
o
l
t
u
p
h
g
u
o
r
h
T
)
s
m
l
(
s
a
c
o
l
y
c
n
e
t
a
L
)
s
p
t
(
t
u
p
h
g
u
o
r
h
T
)
s
m
(
y
c
n
e
t
a
L
2.0K
1.5K
1.0K
0.5K
0.0K
 350
 300
 250
 200
 150
 100
 50
 0
4.0K
3.5K
3.0K
2.5K
2.0K
1.5K
1.0K
0.5K
0
 400
 350
 300
 250
 200
 150
 100
 50
 0
Baseline
R=40
R=80
R=120
l
l
)
s
p
t
(
s
a
b
o
g
t
u
p
h
g
u
o
r
h
T
)
s
m
l
l
(
s
a
b
o
g
y
c
n
e
t
a
L
0.8K
0.6K
0.4K
0.2K
0.0K
 350
 300
 250
 200
 150
 100
 50
 0
1%
10%
50%
Percentage of global transactions
1%
10%
50%
Percentage of global transactions
Figure 5. Throughput and latency of local and global transactions with reordering in WAN 2.
Baseline
Reordering (R=20)
Baseline
Reordering (R=70)
)
s
p
t
(
t
u
p
h
g
u
o
r
h
T
)
s
m
(
y
c
n
e
t
a
L
4.0K
3.5K
3.0K
2.5K
2.0K
1.5K
1.0K
0.5K
0
 400
 350
 300
 250
 200
 150
 100
 50
 0
Timeline
Post
Follow
Follow Global
Timeline
Post
Follow
Follow Global
WAN 1
WAN 2
Figure 6. Social network application in WAN 1 and WAN 2.
and read-only transactions must be certiﬁed in Sinfonia, and
therefore can abort. Read-only transactions do not abort in
SDUR.
COPS [16] is storage system that ensures a strong version
of causal consistency, which in addition to ordering causally
related write operations also orders writes on the same data
items. COPS provides read-only transactions, but does not
provide multi-key update transactions.
Walter [17] offers an isolation property called Parallel
Snapshot Isolation (PSI) for databases replicated across
multiple data centers. PSI guarantees snapshot isolation and
total order of updates within a site, but only causal ordering
across data centers.
Vivace [18] is a storage system optimized for latency in
wide-area networks. Vivace’s replication protocol prioritizes
small critical data exchanged between sites to reduce delays
due to congestion. Vivace does not provide transactions over
multiple keys.
Google’s Bigtable [19] and Yahoo’s Pnuts [20] are dis-
tributed databases that offer a simple relational model (e.g.,
no joins). Bigtable supports very large tables and copes
with workloads that range from throughput-oriented batch
processing to latency-sensitive applications. Pnuts provides
a richer relational model than Bigtable: it supports high-level
constructs such as range queries with predicates, secondary
indexes, materialized views, and the ability to create multiple
tables.
None of the above systems provides strongly consis-
tent execution for multi-partition transactions over WANs.
Among the ones that offer guarantees closer to SDUR, we
consider Spanner [21], MDCC and P-Store [22]
P-Store [22] is perhaps the closest to our work in that it
implements deferred update replication optimized for wide-
area networks. Unlike SDUR, P-Store uses genuine atomic
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
multicast to terminate transactions, which is more expensive
than atomic broadcast. P-Store also avoids the convoy effect
in that it can terminates transactions in parallel. SDUR can
also terminate transactions in parallel, and in addition to that
we use reordering to further reduce delays.
Spanner [21] is a distributed database for WANs. Like
SDUR the database is partitioned and replicated over several
Paxos instances. Spanner uses a combination of two-phase
commit and a so called TrueTime API to achieve consistent
multi-partitions transactions. TrueTime uses hardware clocks
to derive bounds on clock uncertainty, and is used for
assigning globally valid timestamps and for consistent reads
across partitions.
MDDC [10] is a replicated transactional data store that
also uses several
instances of Paxos. MDCC optimizes
for commutative transactions, and uses Generalized Paxos
which allows to relax the order of transaction delivery of
commuting transactions.
VIII. CONCLUSION
This paper discusses scalable deferred update replication
in geographically distributed settings. SDUR scales deferred
update replication, a well-established approach used in sev-
eral database replicated systems, by means of data parti-
tioning. SDUR distinguishes between fast local transactions
and slower global transactions. Although local transactions
scale linearly with the number of partitions (under certain
workloads), when deployed in a geographically distributed
environment they may be signiﬁcantly delayed by the much
slower global transactions—in some settings global trans-
actions can slow down local transactions by a factor of 10.
We presented two techniques that account for this limitation:
Transaction delaying is simple, however, produces limited
improvements; reordering, a more sophisticated approach,
provides considerable reduction in the latency of local trans-
actions, mainly in deployments where global transactions
harm local transactions the most. Our claims are substan-
tiated with a series of microbenchmarks and a Twitter-like
social network application.
REFERENCES
[1] F. Pedone, R. Guerraoui, and A. Schiper, “The Database State
Machine approach,” Distrib. Parallel Databases, vol. 14, pp.
71–98, July 2003.
[2] B. Kemme and G. Alonso, “Don’t be lazy, be consistent:
Postgres-r, a new way to implement database replication,” in
VLDB, 2000.
[3] Y. Lin, B. Kemme, M. Patino-Martinez, and R. Jimenez-
Peris, “Middleware based data replication providing snapshot
isolation,” in SIGMOD, 2005.
[4] M. Patino-Martinez, R. Jimenez-Peris, B. Kemme, and
G. Alonso, “MIDDLE-R: Consistent database replication at
the middleware level,” ACM Transactions on Computer Sys-
tems, vol. 23, no. 4, pp. 375–423, 2005.
[5] D. Sciascia, F. Pedone, and F. Junqueira, “Scalable deferred
update replication,” in DSN, 2012.
[6] H. T. Kung and J. T. Robinson, “On optimistic methods
for concurrency control,” ACM Transactions on Database
Systems, vol. 6, no. 2, pp. 213–226, 1981.
[7] X. D´efago, A. Schiper, and P. Urb´an, “Total order broadcast
and multicast algorithms: Taxonomy and survey,” ACM Com-
put. Surv., vol. 36, no. 4, pp. 372–421, Dec. 2004.
[8] L. Lamport, “The part-time parliament,” ACM Transactions
on Computer Systems, vol. 16, no. 2, pp. 133–169, 1998.
[9] P. Bernstein, V. Hadzilacos, and N. Goodman, Concurrency
Control and Recovery in Database Systems. Addison-Wesley,
1987.
[10] T. Kraska, G. Pang, M. J. Franklin, and S. Madden, “MDCC:
Multi-Data Center Consistency,” CoRR, 2012.
[11] Y. Sovran, R. Power, M. K. Aguilera, and J. Li, “Transactional
storage for geo-replicated systems,” in SOSP, 2011.
[12] D. Agrawal, G. Alonso, A. E. Abbadi, and I. Stanoi, “Ex-
ploiting atomic broadcast in replicated databases,” in EuroPar,
1997.
[13] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati,
A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall,
and W. Vogels, “Dynamo: Amazon’s highly available key-
value store,” SIGOPS, 2007.
[14] J. Rao, E. Shekita, and S. Tata, “Using Paxos to build a scal-
able, consistent, and highly available datastore,” Proceedings
of the VLDB Endowment, vol. 4, no. 4, pp. 243–254, 2011.
[15] M. K. Aguilera, A. Merchant, M. Shah, A. Veitch, and
C. Karamanolis, “Sinfonia: a new paradigm for building
scalable distributed systems,” in SOSP, 2007.
[16] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Ander-
sen, “Don’t settle for eventual: scalable causal consistency for
wide-area storage with COPS,” in SOSP, 2011.
[17] Y. Sovran, R. Power, M. K. Aguilera, and J. Li, “Transactional
storage for geo-replicated systems,” in SOSP, 2011.
[18] B. Cho and M. K. Aguilera, “Surviving congestion in geo-
distributed storage systems,” in USENIX ATC, 2012.
[19] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A.
Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber,
“Bigtable: A distributed storage system for structured data,”
ACM Transactions on Computer Systems, 2008.
[20] B. F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silber-
stein, P. Bohannon, H.-A. Jacobsen, N. Puz, D. Weaver, and
R. Yerneni, “Pnuts: Yahoo!’s hosted data serving platform,”
Proceedings of the VLDB Endowment, 2008.
[21] J. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. Fur-
man, S. Ghemawat, A. Gubarev, C. Heiser, P. Hochschild
et al., “Spanner: Googles globally-distributed database,”
OSDI, 2012.
[22] N. Schiper, P. Sutra, and F. Pedone, “P-store: Genuine partial
replication in wide area networks,” in SRDS, 2010.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply.