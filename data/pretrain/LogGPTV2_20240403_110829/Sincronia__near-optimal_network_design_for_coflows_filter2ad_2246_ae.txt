for minimizing average completion times. However, these
results give us some indication on performance of Sincronia
against existing network designs; for instance, Varys [10]
reports to improve upon TCP by a factor of 1.85× on an
average, and roughly by the same number at 95 percentiles.
6 RELATED WORK & OPEN PROBLEMS
There has been tremendous recent effort on network design
for coflows, both in networking and in theory communities.
In this section, we compare and contrast Sincronia against
related work from these communities, and discuss a number
of problems that remain open in the context of network
design for coflows.
 1 2 4 8Average90th 99th SlowdownPercentile Custom 1000 Custom 2000 Custom 4000 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 6 7CDFSlowdownCustom 1000Custom 2000Custom 4000 1 2 3 4 5 0 2 4 6 8 10 12 14 16 18SlowdownOCT binsCustom 1000Custom 2000Custom 4000 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5CDFSlowdownload = 0.7load = 0.9 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 6 7CDFSlowdownNumber of epochs = 6Number of epochs = 8Number of epochs = 10Number of epochs = 12 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 6 7CDFSlowdownImmediate recomputationExponential epochsLinear epochsSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Agarwal et al.
Figure 8: Evaluation results for Sincronia implementation on top of TCP over our testbed in Figure 9. Sincronia when implemented
on top of TCP/DiffServ significantly improves coflow performance when compared to coflow-agnostic TCP both on an average and at high
percentiles (left), and maintains these improvements across varying network loads (center) and trace sizes (right). More discussion in §5.3.
Figure 9: Testbed used in our experiments has 16 servers, each
with 1Gbps access link bandwidth, interconnected with a FatTree
topology comprising 20 PICA8 switches. Each switch supports
priority queueing with 8 distinct levels.
Gap between lower and upper bounds. Existing network
designs for coflows from networking community [7, 9, 10, 12]
can perform arbitrarily worse than the optimal in terms of
average CCT [2]. Qiu et. al. [22] provided the first coflow
scheduling algorithm that achieves approximation guaran-
tees of 64/3 for the offline case with zero release dates. This
result was improved by Ahmadi et. al. [3] to 4-approximation
for zero release dates and to 5-approximation with release
dates. However, all these results require a central coordina-
tor to either solving complex linear programs or to perform
complex per-flow rate allocations. Sincronia achieves state-
of-the-art approximation guarantees, both for zero release
dates and, using a simple extension of the BSSI algorithm [2],
for the case of release dates. However, Sincronia shows that
these guarantees can be achieved without any complex per-
flow rate allocation, thus overcoming the limitations of all
existing results from the networking and the theory com-
munity. Despite all the above results, there is a gap between
known lower bounds for coflow scheduling (impossibility
of better than 2-approximation, §2) and the known upper
bounds (4-approximation for offline problem with zero re-
lease dates) [2, 3]. It remains an intriguing problem to bridge
this gap.
Improved competitive ratio for online coflow schedul-
ing. Khuller et. al. [19] were the first to present an online
algorithm that achieves a competitive ratio of 12. Sincronia
achieves similar guarantees. Is it possible to design a coflow
scheduling algorithm with a better competitive ratio? Note
that an offline algorithm that achieves a better approxima-
tion factor for the case of zero release dates will already
lead to improvements in the online algorithm by using the
framework from Khuller et. al. [19].
Necessity of centralized solutions. As discussed in §2,
there is a strong lower bound of Ω(√
n) on achievable approx-
imation for average CCT using algorithms that do not use
any coordination [8]. However, the “amount of coordination”
required to mitigate this lower bound is unclear. For instance,
similar to [10, 28, 29], Sincronia requires a centralized con-
troller to implement BSSI. While the amount of computation
required by Sincronia’s centralized scheduler is much lower
than that of prior solutions (since Sincronia scheduler does
not need to do per-flow rate allocation), it remains an open
problem to understand the amount of computation needed
to be done at the centralized controller.
Coflows with paths. Sincronia, similar to [10, 12], assumes
that routing of flows within and across coflows are decided
by the network layer (either via specific routing mechanisms,
or via packet spraying). However, it is a priori conceivable
that better performance may be achievable by co-designing
routing along with scheduling of coflows. This problem has
been studied in a few recent papers [18, 29]; however, de-
signing optimal algorithms for this problem remains open.
Extensions to Non-clairvoyant scheduler. Sincronia,
similar to [10, 12, 29], assumes that the information about
a coflow is available at the arrival time (and no earlier). For
many applications, this is indeed the case (see [10] and the
discussion therein). Moreover, recent work has shown that
it is possible to identify coflows and their properties within
 1 4 16 64 256 1024 4096Average90th 99th Factor of Improvement  using SincroniaPercentileload = 0.7 MSLload = 1.4 MSLload = 2.1 MSL 0 0.2 0.4 0.6 0.8 12-62-42-22022242628210212214CDFFactor of Improvement using Sincroniaload = 0.7 MSLload = 1.4 MSLload = 2.1 MSL 0 0.2 0.4 0.6 0.8 12-62-42-22022242628210CDFFactor of Improvement using Sincronia526 coflows2000 coflowsSincronia: Near-Optimal Network Design for Coflows
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
reasonable estimate for many applications [28]. However,
designing a near-optimal non-clairvoyant scheduler remains
an interesting future direction.
Extensions to other performance metrics. Finally, simi-
lar to most of the existing literature on coflows [8, 10, 28, 29],
Sincronia is designed to optimize for average (weighted) com-
pletion time. It remains an interesting question to extend our
results to the case of deadline-sensitive coflows and for min-
imizing other metrics such as tail coflow completion time.
An intriguing question in this direction is also to define a
notion of fairness for coflows.
7 PROOF OUTLINE FOR THEOREM 1
In this section, we provide a high-level idea for our results in
§3. The detailed proofs for these results are in the technical
report [2].
At a high-level, we use a linear programming relaxation
to obtain a lower bound on the optimal value of the average
CCT, and obtain our approximation guarantee by comparing
the average CCT of our algorithm to this lower bound. Our
algorithm is purely combinatorial, in that it does not require
solving an LP.
In our algorithm, we decouple the problem of obtaining a
feasible schedule into two parts: we first obtain an ordering
of coflows, and then obtain a feasible schedule by using a
greedy rate allocation scheme that maintains this order.
To obtain an ordering of coflows, we also relax the prob-
lem by ignoring the dependencies between the input and
output ports; more specifically, we consider an instance of a
concurrent open shop problem where there are 2m machines
corresponding to the m ingress and the m egress ports, and
one job corresponding to each of the n coflows. The process-
ing requirement for job c on machine k is the total load at
port k due to coflow c, and the weight of job c is the weight
associated with coflow c. Observe that the optimal value
of this concurrent open shop instance is a lower bound on
the optimal value of the original coflow scheduling instance
since any feasible solution to the latter can be viewed as
a feasible solution for the former with the same objective
function value. Next, observe that there is an optimal solu-
tion for the concurrent open shop input in which the order
of the jobs processed on each machine is the same: given
any optimal solution, if we consider the last job on the most
heavily loaded machine we can push that job to the end on
each other machine while maintaining that none of the job
completion times increases. We can repeat this to obtain
a solution where the jobs are processed in the same order
on each machine without increasing the objective function
value. So we have reduced our problem to one of just finding
an ordering of coflows, since given a ordering, determining
a feasible schedule is straightforward.
We use a primal-dual algorithm to compute an ordering
such that the weighted completion time of the jobs is at
most twice the optimal value for the concurrent open shop
instance; hence, by our lower bound argument, this weighted
completion time is at most twice the optimal value for the
coflow scheduling problem as well.
For the second part of the problem, we present a greedy
rate-allocation scheme that maintains the order returned
by the primal-dual algorithm. More specifically, we ensure
that a flow from input port i to output port j on a coflow c
is scheduled only after all flows between this pair of ports
from coflows of a higher priority have completed. So, if we
consider the last flow processed for some coflow c, say from
input port i to output port j, this property, coupled with the
fact that the algorithm is work-conserving, implies that at
least one of the ports i or j was busy for at least half the com-
pletion time of coflow c with processing flows from coflows
with an equal or higher priority than coflow c. Since the
total amount of work from higher priority coflows that can
be done on these ports is at most the completion time of
coflow c in the concurrent open shop instance, we arrive at
the conclusion that the completion time of coflow c in our
greedy rate-allocation scheme is at most twice the comple-
tion time of coflow c in the concurrent open shop instance.
This result in fact extends to any rate allocation scheme that
is preemptive, work-conserving, and maintains the ordering
of the coflows. Combining the above two results directly
yields the 4-approximation result.
8 CONCLUSION
We have presented Sincronia, a network design for coflows
that provides near-optimal performance and can be imple-
mented on top on any transport layer mechanism for flows
that supports priority scheduling. Sincronia achieves this
using a key technical result — we show that given a “right”
ordering of coflows, any per-flow rate allocation mechanism
achieves average coflow completion time within 4× of the
optimal as long as (co)flows are prioritized with respect to
the ordering. This allows Sincronia to use a simple greedy
mechanism to “order” all unfinished coflows; all flows within
and across coflows can then be greedily scheduled using
any transport mechanism that supports priority scheduling
(without any per-flow rate allocation mechanism). Evalua-
tion results suggest that Sincronia not only admits a practical,
near-optimal design but also improves upon state-of-the-art
network designs for coflows.
ACKNOWLEDGMENTS
This work was supported in part by NSF grants CCF-1526067,
CMMI-1537394, CCF- 1522054, and CCF-1740822.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Agarwal et al.
REFERENCES
[1] 2018.
Sincronia Repository.
sincronia-coflow.
https : / / github . com /
[2] Saksham Agarwal, Shijin Rajakrishnan, Akshay Narayan, Rachit Agar-
wal, David Shmoys, and Amin Vahdat. 2018. Sincronia: Near-Optimal
Network Design for Coflows. In Tech Report.
[3] Saba Ahmadi, Samir Khuller, Manish Purohit, and Sheng Yang. 2017.
On scheduling coflows. In MOS IPCO.
[4] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick
McKeown, Balaji Prabhakar, and Scott Shenker. 2013. pFabric: Minimal
Near-optimal Datacenter Transport. In ACM SIGCOMM.
[5] Nikhil Bansal and Subhash Khot. 2010. Inapproximability of hyper-
graph vertex cover and applications to scheduling problems. In EATCS
ICALP.
[6] Kwok Ho Chan, Jozef Babiarz, and Fred Baker. 2006. Configuration
Guidelines for DiffServ Service Classes. https://tools.ietf.
org/html/rfc4594.
[7] Mosharaf Chowdhury and Ion Stoica. 2012. Coflow: A networking
abstraction for cluster applications. In ACM HotNets.
[8] Mosharaf Chowdhury and Ion Stoica. 2015. Efficient coflow scheduling
without prior knowledge. In ACM SIGCOMM.
[9] Mosharaf Chowdhury, Matei Zaharia, Justin Ma, Michael I Jordan, and
Ion Stoica. 2011. Managing data transfers in computer clusters with
orchestra. In ACM SIGCOMM.
[10] Mosharaf Chowdhury, Yuan Zhong, and Ion Stoica. 2014. Efficient
coflow scheduling with varys. In ACM SIGCOMM.
[11] Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: simplified data
processing on large clusters. In USENIX OSDI.
[12] Fahad R Dogar, Thomas Karagiannis, Hitesh Ballani, and Antony Row-
stron. 2014. Decentralized task-aware scheduling for data center net-
works. In ACM SIGCOMM.
[13] Peter X Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia
Ratnasamy, and Scott Shenker. 2015. phost: Distributed near-optimal
datacenter transport over commodity network fabric. In ACM CoNEXT.
[14] Naveen Garg, Amit Kumar, and Vinayaka Pandit. 2007. Order sched-
uling models: hardness and algorithms. In IARCS FSTTCS.
[15] Mark Handley, Costin Raiciu, Alexandru Agache, Andrei Voinescu, An-
drew Moore, Gianni Antichi, and Marcin Wojcik. 2017. Re-architecting
datacenter networks and stacks for low latency and high performance.
In ACM SIGCOMM.
[16] Chi-Yao Hong, Matthew Caesar, and P Godfrey. 2012. Finishing flows
quickly with preemptive scheduling. In ACM SIGCOMM.
[17] Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fet-
terly. 2007. Dryad: distributed data-parallel programs from sequential
building blocks. In ACM EuroSys.
[18] Hamidreza Jahanjou, Erez Kantor, and Rajmohan Rajaraman. 2017.
Asymptotically Optimal Approximation Algorithms for Coflow Sched-
uling. In ACM SPAA.
[19] Samir Khuller, Jingling Li, Pascal Sturmfels, Kevin Sun, and Prayaag
Venkat. 2018. Select and Permute: An Improved Online Framework
for Scheduling to Minimize Weighted Completion Time. In LATIN.
[20] Yucheng Low, Danny Bickson, Joseph Gonzalez, Carlos Guestrin, Aapo
Kyrola, and Joseph M Hellerstein. 2012. Distributed GraphLab: a frame-
work for machine learning and data mining in the cloud. Proceedings
of the VLDB Endowment, 5(8): 716-727.
[21] Grzegorz Malewicz, Matthew H Austern, Aart JC Bik, James C Dehnert,
Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. 2010. Pregel: a
system for large-scale graph processing. In ACM SIGMOD.
[22] Zhen Qiu, Cliff Stein, and Yuan Zhong. 2015. Minimizing the total
weighted completion time of coflows in datacenter networks. In ACM
SPAA.
[23] Thomas A. Roemer. 2006. A note on the complexity of the concurrent
open shop problem. In Journal of Scheduling, 9(4): 389-396. Springer.
[24] Sushant Sachdeva and Rishi Saket. 2013. Optimal inapproximability
for scheduling problems via structural hardness for hypergraph vertex
cover. In IEEE CCC.
[25] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron.
2011. Better never than late: Meeting deadlines in datacenter networks.
In ACM SIGCOMM.
[26] Yuan Yu, Michael Isard, Dennis Fetterly, Mihai Budiu, Úlfar Erlingsson,
Pradeep Kumar Gunda, and Jon Currey. 2008. DryadLINQ: A System
for General-Purpose Distributed Data-Parallel Computing Using a
High-Level Language.. In USENIX OSDI.
[27] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave,
Justin Ma, Murphy McCauley, Michael J Franklin, Scott Shenker, and
Ion Stoica. 2010. Resilient distributed datasets: A fault-tolerant ab-
straction for in-memory cluster computing. In USENIX NSDI.
[28] Hong Zhang, Li Chen, Bairen Yi, Kai Chen, Mosharaf Chowdhury,
and Yanhui Geng. 2016. CODA: Toward automatically identifying and
scheduling coflows in the dark. In ACM SIGCOMM.
[29] Yangming Zhao, Kai Chen, Wei Bai, Minlan Yu, Chen Tian, Yanhui
Geng, Yiming Zhang, Dan Li, and Sheng Wang. 2015. Rapier: Integrat-
ing routing and scheduling for coflow-aware data center networks. In
IEEE INFOCOM.