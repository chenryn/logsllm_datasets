### Maximum Achievable Throughput of Gallium Middleboxes and FastClick

Gallium middleboxes, when compared to their FastClick-based counterparts, exhibit superior performance. Specifically, a single-core Gallium outperforms a 4-core FastClick by 20-187%. When the throughput is constrained to be identical, Gallium reduces processing cycles by 21-79%. This performance gain is attributed to the infrequent use of non-offloaded partitions in Gallium middleboxes. For instance, in the Gallium versions of MazuNAT and the load balancer, only 0.1% of TCP flow packets are processed by the middlebox server. In the case of the firewall and proxy, all packet processing occurs within the programmable switch.

### Latency Reduction

Gallium also significantly reduces latency by 31%, as measured using Nptcp for TCP packet latency. The reduction in latency is due to the fact that most packets do not pass through the server in the Gallium setup, as shown in Table 2.

### State Synchronization Overhead

To evaluate state synchronization overhead, we tested programs with varying table sizes and numbers of tables. We measured the latency required to update offloaded tables from middlebox servers, specifically for 1, 2, and 4 tables. Table 3 presents the latency for insertion, modification, and deletion operations on offloaded tables. A single table update in Gallium is approximately five times the end-to-end latency of a packet sent through a software middlebox. Gallium provides overall performance benefits as long as slow path and corresponding state synchronization operations are infrequently invoked. We further explored this using realistic traffic traces through generated middleboxes.

### Realistic Workloads

We evaluated two realistic workloads: an enterprise workload and a data-mining workload. These workloads, drawn from the CONGA work on datacenter traffic load balancing [4], include both short and long flows. The majority of flows in both workloads are small, with 90% containing fewer than ten packets. We simulated 100,000 flow sizes and used 100 threads to send traffic, with each thread handling one connection at a time.

To assess Gallium’s throughput and CPU benefits, we fixed the number of cores used in the middlebox server and used the RX counter of the receiver side’s NIC to compute the average throughput during the experiment. As shown in Figure 8, Gallium improves overall throughput and reduces CPU overheads on both workloads. Compared to the 4-core version of FastClick, a single-core Gallium achieves 1-35% more throughput on the enterprise workload and 18-46% more throughput on the data-mining workload. If the throughput is constrained to match that of 4-core FastClick, Gallium can save 3-81% of processing cycles or 0.03-4.39 server cores. The data-mining workload benefits more due to the longer flow durations.

### Flow Completion Time

To determine which connections Gallium speeds up, we measured the flow completion time for connections of different sizes. We binned flows based on their sizes and computed the average flow completion time for each bin. As shown in Figure 9, the reduction in flow completion time is concentrated on long flows, as these flows have the majority of their packets handled by the programmable switch rather than the server.

## Discussion

### Reducing Memory Usage in Programmable Switches

One optimization to reduce memory usage in programmable switches is to store only a fraction of any table, such as MiniLB’s map that maps IP addresses to ports. Any packet that the programmable switch cannot handle is processed by the middlebox server instead. This approach requires additional effort to synchronize the table in the software middlebox and its "cache" table in the programmable switch, which is left for future work.

### Extra Functionalities in Programmable Switches

The P4 program generated by Gallium uses registers and match-action tables with exact matches. While P4 provides other useful abstractions, such as longest prefix matching and advanced ALU operations, these functionalities are currently not utilized by Gallium. Some abstractions, like LPM, do not exist in software middleboxes.

### Handling Complex Data Structures

Currently, Gallium supports only two data structures: HashMap and Vector. These are the most commonly used data structures in Click. Middleboxes that use complex data structures (e.g., linked lists, trees) must be processed as part of the non-offloaded partition or require code modifications to enable offloading.

### Cost Model of Offloading

Gallium aims to offload as many lines of code (LLVM instructions) as possible to the programmable switch. This simplifies the algorithm design but does not consider the performance benefits of offloading different operations. For example, offloading a table lookup provides more performance benefits than offloading an integer addition. Therefore, Gallium’s partitioning algorithm may produce sub-optimal partitions. One potential solution is to assign weights to each operation, reflecting their associated performance benefits when executed on the switch. This would maximize the weight of instructions offloaded to the switch, and we leave this for future work.

## Related Work

### Offloading to Programmable Network Hardware

Hardware offloading is a well-known technique for improving performance. Several studies have used programmable network hardware to accelerate specific applications, such as KV-Direct [14] for key-value stores on FPGA NICs, IncBricks [19] and NetCache [13] for in-network caching, and NOPaxos [17], Eris [16], and NetChain [12] for in-network coordination. DAIET [24] aggregates data along network paths, and Silkroad [20] scales up the number of concurrent connections for load balancers. Unlike these approaches, which require manual partitioning, Gallium provides a compiler-oriented approach to automatically partition the source program.

### Frameworks for Programmable Network Hardware

Our work focuses on helping developers move their applications to programmable network devices. Floem [22] and iPipe [18] are frameworks that allow developers to explore different offloading methods for programmable NICs, but they require manual partitioning. ClickNP [15] is another framework using FPGA-based smart NICs for offloading network functions. Similar to Gallium, ClickNP uses the Click [21] dataflow programming model. However, our focus is on P4, a hardware-independent target for programmable networking hardware. Domino [26] deploys entire programs onto the switch, while Gallium aims to partition the program and make a portion deployable on the switch.

### Program Slicing

Program slicing techniques, which partition a source program based on dependency relations and control-flow graphs, are similar to our approach. However, program slicing aims to abstract the program, whereas our goal is to partition the source program into multiple parts that collectively function as the original program.

## Conclusion

Offloading software middleboxes to programmable switches can yield significant performance gains. Manually rewriting software middleboxes is challenging and time-consuming. Gallium, which uses program partitioning and compilation, transforms software middleboxes into functionally equivalent versions that leverage programmable switches for high performance. Our evaluations show that Gallium can save 21-79% of processing cycles and reduce latency by about 31% across various types of software middleboxes. The source code for Gallium is available at https://github.com/Kaiyuan-Zhang/Gallium-public. This work does not raise any ethical issues.

### Acknowledgments

We thank our shepherd and the anonymous reviewers for their valuable feedback. This work was partially supported by NSF (CNS-1714508) and Futurewei.