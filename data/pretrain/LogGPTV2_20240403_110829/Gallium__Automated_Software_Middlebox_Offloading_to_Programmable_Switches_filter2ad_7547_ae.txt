the maximum achievable throughput of Gallium middleboxes and
their FastClick-based counterparts. Overall, compared with FastClick
running on 4 cores, Gallium with a single core outperforms by 20-
187%. If we constrain the throughput to be identical, Gallium saves
processing cycles by 21-79%. These performance benefits are be-
cause the non-offloaded partitions in the Gallium middleboxes are
rarely used. In the Gallium version of MazuNAT and load balancer,
only 0.1% of the packets in TCP flows are processed by the mid-
dlebox server. For the firewall and the proxy, all packet processing
happens in the programmable switch.
Gallium reduces latency by 31%. We use Nptcp to test TCP packet
latency. Table 2 shows the result. Like the reduction in processing
overheads, the latency reduction comes from the fact that most
packets do not go through the server in Gallium.
State Synchronization Overhead. We create programs with
different table sizes and different numbers of tables to test the la-
tency required to update offloaded tables from middlebox servers.
We measure the latency of updating 1, 2, and 4 tables. Table 3 shows
the latency of insertion, modification, and deletion for offloaded
tables. A single table update is about 5x the end-to-end latency
of a packet sent through a software middlebox. Gallium can pro-
vide overall performance benefits as long as the slow path and
corresponding state synchronization operations are invoked infre-
quently. We consider this next using realistic traffic traces through
the generated middleboxes.
Realistic Workloads. We evaluate two realistic workloads: an
enterprise workload and a data-mining workload. The workloads
(i.e., flow size distributions) are drawn from the CONGA work
on datacenter traffic load balancing [4]. These workloads have
both short flows and long flows. The majority of flows in both the
enterprise and the data-mining workload are small; 90% of the flows
in both workloads contain less than ten packets. We draw 100000
flow sizes from the flow size distributions and use 100 threads to
send traffic. A thread sends a single connection at a time and starts
a new connection when the current connection finishes.
To evaluate Gallium’s throughput and CPU benefits, we fix the
number of cores used in the middlebox server and use the RX
counter of the receiver side’s NIC to compute the average through-
put during the experiment.
Gallium improves the overall throughput and reduces CPU over-
heads on both workloads. Figure 8 shows the result. Compared to
the 4-core version of FastClick, Gallium using one core can achieve
1-35% more throughput on the enterprise workload and 18-46%
more throughput on the data-mining workload. If we constrain the
throughput to be the same as what 4-core FastClick can achieve,
this means we can save 3-81% processing cycles or 0.03-4.39 server
cores. We do better on the data-mining workload because the long
flows are longer than that in the enterprise workloads.
Which connections do Gallium speed up in these realistic work-
loads? We measure the flow completion time for connections of
different sizes. We bin flows based on their sizes and compute an
average flow completion time for each bin. Figure 9 provides the
comparison between Gallium and FastClick. We see that the re-
duction in flow completion time is concentrated on the long flows.
This behavior is because long flows will have the majority of their
packets handled by the programmable switch instead of the server.
7 Discussion
Reducing memory usage of programmable switches. One op-
timization to reduce memory usage of programmable switches is to
let the programmable switch store only a fraction of any table, e.g.,
MiniLB’s map that stores the mapping from IP addresses to ports.
For any packet that the programmable switch does not know how
to handle, the middlebox server handles it instead. This means we
need to handle the additional challenge of synchronizing the table
in the software middlebox and its “cache” table in the programmable
switch. We leave it to future work.
Extra functionalities in programmable switches. The P4
program generated by Gallium only uses registers and match-action
tables with exact matches. Besides these functionalities, the P4
language provides other useful abstractions for packet processing,
such as the longest prefix matching when matching on packet
header fields, such as IP addresses. Programmable devices may
also provide platform-specific P4 primitives such as advanced ALU
operations (e.g., swapping the lower- and higher-32 bits of a 64-bit
integer) or hardware-accelerated hash functions. Currently, these
functionalities are not used by Gallium as some abstractions, such
as LPM, do not exist in software middleboxes.
Dealing with complex data structures. Currently, Gallium
can only handle two data structures, HashMap and Vector. This is
because we know how to map them to P4 native primitives (§4.3).
We find that these two data structures are the most commonly used
ones in Click. Middleboxes that use complex data structures (e.g.,
linked list, tree) have to be processed as part of the non-offloaded
partition or require code modifications to enable offloading.
Cost model of offloading. Currently, Gallium tries to offload
as many lines of code (LLVM instructions) as possible to the pro-
grammable switch. This approach simplifies the algorithm design,
as discussed in §4.2. This model does not consider that the of-
floading of different operations will give the middlebox different
performance benefits. For example, offloading a table lookup to
the programmable switch will provide more performance benefits
than offloading an integer addition operation. Therefore, Gallium’s
partitioning algorithm may produce sub-optimal partitions as the
293
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Kaiyuan Zhang, Danyang Zhuo, and Arvind Krishnamurthy
(a) MazuNAT
(b) L4 Load Balancer
(c) Firewall
(d) Proxy
(e) Trojan Detector
Figure 8: Throughput comparison of Gallium and FastClick on the enterprise workload and the data-mining workload.
(a) MazuNAT
(b) Load Balancer
(c) Firewall
(d) Proxy
(e) Trojan Detector
Figure 9: Flow completion time comparison of Gallium and FastClick on the enterprise(E) and data-mining(D) workload.
algorithm may choose to offload an integer addition over a table
lookup. One possible solution to this is to assign a weight to each
operation, which would be a measure of its associated performance
benefit when executed on the switch. We would then try to maxi-
mize the weight of instructions offloaded to the switch as we move
operations into the non-offloaded partition in the algorithm de-
scribed in §4.2.2. We leave it to future work.
8 Related Work
Offloading to programmable network hardware. Hardware of-
floading is a well-known technique to improve performance. There
have been many studies that use programmable network hardware
to accelerate specific applications. For example, KV-Direct [14] is a
key-value store on FPGA NICs. IncBricks [19] and NetCache [13]
use programmable network hardware to build an in-network cache.
NOPaxos [17], Eris [16], and NetChain [12] use programmable
switches to do in-network coordination. DAIET [24] aggregates data
along network paths using programmable switches. Silkroad [20]
leverages a programmable switch’s match action table to scale
up the number of concurrent connections for load balancers. The
current offloading approaches need manual effort in partitioning
the source program, while Gallium provides a compiler-oriented
approach to partition the source program automatically.
Frameworks for programmable network hardware. Our
work is in the category of helping developers move their applica-
tions to programmable network devices. Floem [22] allows develop-
ers to explore different offloading methods for programmable NICs.
iPipe [18] is an actor-based framework for offloading distributed ap-
plications onto programmable NICs. Both Floem and iPipe require
manually partitioning applications, and they assume the smart
NIC can accept programs written in C. ClickNP [15] is another
framework using FPGA-based smart NICs for offloading network
functions. Similar to Gallium, ClickNP uses the Click [21] dataflow
programming model. Different from all this work, our focus is on
P4, a hardware-independent target for a class of programmable
networking hardware. P4’s expressiveness is more restricted than
C and FPGA, and Gallium’s goal is to partition middlebox source
code and deal with resource constraints of P4-based programmable
hardware. Domino [26] provides a DSL for developers to write
packet processing programs that could be compiled to run on pro-
grammable line-rate switches. A major difference between Domino
and Gallium comes from the choice of deployment model. While
Domino deploys the entire program onto the switch and rejects
programs that could not be deployed, Gallium aims at partitioning
the program and makes a portion of it deploy-able on the switch.
Program slicing. The compilation techniques to partition a
source program based on the dependency relations and control-
flow graphs are similar to program slicing [11, 23, 28]. However,
program slicing aims to abstract the program: extract a minimal
program whose logic is similar to the source program for a subset of
the source program’s variables. Our goal is to partition the source
program into multiple partitions, where the cumulative effect is the
same as the source program.
9 Conclusion
Offloading software middleboxes to programmable switches can
yield orders-of-magnitude performance gains; however, manually
rewriting software middleboxes is hard and time-consuming. In
this paper, we have designed and implemented Gallium. Gallium
uses program partitioning and compilation to transform software
middleboxes to their functionally-equivalent versions, which lever-
age programmable switches for high performance. Our evaluations
show that Gallium can save 21-79% of processing cycles and reduce
latency by about 31% across various types of software middleboxes.
Gallium’s source code is available at https://github.com/Kaiyuan-
Zhang/Gallium-public. This work does not raise any ethical issues.
Acknowledgments
We thank our shepherd and the anonymous reviewers for their
helpful feedback on the paper. This work was partially supported
by NSF (CNS-1714508) and Futurewei.
294
EnterpriseDataMining020406080100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1cEnterpriseDataMining020406080100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1cEnterpriseDataMining020406080100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1cEnterpriseDataMining020406080100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1cEnterpriseDataMining020406080100Throughput (Gbps)OffloadedClick-4cClick-2cClick-1c0-100K100K-10M> 10MFlow Size (Bytes)102104106Flow Completion Time (us)Click(E)Offloaded(E)Click(D)Offloaded(D)0-100K100K-10M> 10MFlow Size (Bytes)102104106Flow Completion Time (us)Click(E)Offloaded(E)Click(D)Offloaded(D)0-100K100K-10M> 10MFlow Size (Bytes)101102103104105106Flow Completion Time (us)Click(E)Offloaded(E)Click(D)Offloaded(D)0-100K100K-10M> 10MFlow Size (Bytes)102104106Flow Completion Time (us)Click(E)Offloaded(E)Click(D)Offloaded(D)0-100K100K-10M> 10MFlow Size (Bytes)102104106Flow Completion Time (us)Click(E)Offloaded(E)Click(D)Offloaded(D)Gallium: Automated Software Middlebox Offloading to Prog. Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
References
[1] Data Plane Development Kit (DPDK).
networking/dpdk.
https://software.intel.com/en-us/
[2] In-Network DDoS Detection.
https://barefootnetworks.com/use-cases/
in-nw-DDoS-detection/.
[3] Tofino: World’s fastest P4-programmable Ethernet switch ASICs.
https://
barefootnetworks.com/products/brief-tofino/.
[4] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Francis Matus, Rong Pan,
Navindra Yadav, George Varghese, et al. CONGA: Distributed Congestion-Aware
Load Balancing for Datacenters. In ACM SIGCOMM Computer Communication
Review, volume 44, pages 503–514. ACM, 2014.
[5] Tom Barbette, Cyril Soldani, and Laurent Mathy. Fast Userspace Packet Process-
ing. In Proceedings of the Eleventh ACM/IEEE Symposium on Architectures for
Networking and Communications Systems, ANCS ’15, pages 5–16, Washington,
DC, USA, 2015. IEEE Computer Society.
[6] Pat Bosshart, Dan Daly, Glen Gibb, Martin Izzard, Nick McKeown, Jennifer Rex-
ford, Cole Schlesinger, Dan Talayco, Amin Vahdat, George Varghese, and David
Walker. P4: Programming Protocol-independent Packet Processors. SIGCOMM
Computer Communication Review, 44(3):87–95, July 2014.
[7] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown, Martin
Izzard, Fernando Mujica, and Mark Horowitz. Forwarding Metamorphosis: Fast
Programmable Match-action Processing in Hardware for SDN. In Proceedings of
the ACM SIGCOMM 2013 Conference on SIGCOMM, SIGCOMM ’13, pages 99–110,
New York, NY, USA, 2013. ACM.
[8] Sharad Chole, Andy Fingerhut, Sha Ma, Anirudh Sivaraman, Shay Vargaftik,
Alon Berger, Gal Mendelson, Mohammad Alizadeh, Shang-Tse Chuang, Isaac
Keslassy, Ariel Orda, and Tom Edsall. Drmt: Disaggregated programmable switch-
ing. In Proceedings of the Conference of the ACM Special Interest Group on Data
Communication, 2017.
[9] Lorenzo De Carli, Robin Sommer, and Somesh Jha. Beyond pattern matching:
A concurrency model for stateful deep packet inspection. In Proceedings of the
2014 ACM SIGSAC Conference on Computer and Communications Security, pages
1378–1390, 2014.
[10] Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. The program dependence
graph and its use in optimization. ACM Trans. Program. Lang. Syst., 9(3):319–349,
July 1987.
[11] Susan Horwitz, Thomas Reps, and David Binkley. Interprocedural slicing using
dependence graphs. ACM Transactions on Programming Languages and Systems
(TOPLAS), 12(1):26–60, 1990.
[12] Xin Jin, Xiaozhou Li, Haoyu Zhang, Nate Foster, Jeongkeun Lee, Robert Soulé,
Changhoon Kim, and Ion Stoica. Netchain: Scale-free sub-rtt coordination. In
Proceedings of the 15th USENIX Conference on Networked Systems Design and
Implementation, NSDI’18, page 35–49, USA, 2018. USENIX Association.
[13] Xin Jin, Xiaozhou Li, Haoyu Zhang, Robert Soulé, Jeongkeun Lee, Nate Foster,
Changhoon Kim, and Ion Stoica. Netcache: Balancing key-value stores with fast
in-network caching. In Proceedings of the 26th Symposium on Operating Systems
Principles, 2017.
[14] Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu, Yongqiang Xiong, Andrew
Putnam, Enhong Chen, and Lintao Zhang. Kv-direct: High-performance in-
memory key-value store with programmable nic.
In Proceedings of the 26th
Symposium on Operating Systems Principles, 2017.
[15] Bojie Li, Kun Tan, Layong Larry Luo, Yanqing Peng, Renqian Luo, Ningyi Xu,
Yongqiang Xiong, Peng Cheng, and Enhong Chen. Clicknp: Highly flexible
and high performance network processing with reconfigurable hardware. In
Proceedings of the 2016 ACM SIGCOMM Conference, 2016.
[16] Jialin Li, Ellis Michael, and Dan R. K. Ports. Eris: Coordination-free consistent
transactions using in-network concurrency control. In Proceedings of the 26th
Symposium on Operating Systems Principles, SOSP ’17, page 104–120, New York,
NY, USA, 2017. Association for Computing Machinery.
[17] Jialin Li, Ellis Michael, Naveen Kr. Sharma, Adriana Szekeres, and Dan R. K. Ports.
Just say no to paxos overhead: Replacing consensus with network ordering.
In Proceedings of the 12th USENIX Conference on Operating Systems Design and
Implementation, OSDI’16, page 467–483, USA, 2016. USENIX Association.
[18] Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishnamurthy, Simon Peter, and
Karan Gupta. Offloading distributed applications onto smartnics using ipipe.
In Proceedings of the ACM Special Interest Group on Data Communication, SIG-
COMM ’19, page 318–333, New York, NY, USA, 2019. Association for Computing
Machinery.
[19] Ming Liu, Liang Luo, Jacob Nelson, Luis Ceze, Arvind Krishnamurthy, and Kishore
Atreya. Incbricks: Toward in-network computation with an in-network cache.
In Proceedings of the Twenty-Second International Conference on Architectural
Support for Programming Languages and Operating Systems, 2017.
[20] Rui Miao, Hongyi Zeng, Changhoon Kim, Jeongkeun Lee, and Minlan Yu.
SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switch-
ing ASICs. In Proceedings of the Conference of the ACM Special Interest Group on
Data Communication, SIGCOMM ’17, pages 15–28, New York, NY, USA, 2017.
ACM.
[21] Robert Morris, Eddie Kohler, John Jannotti, and M. Frans Kaashoek. The Click
Modular Router. In Proceedings of the Seventeenth ACM Symposium on Operating
Systems Principles, SOSP ’99, pages 217–231, New York, NY, USA, 1999. ACM.
[22] Phitchaya Mangpo Phothilimthana, Ming Liu, Antoine Kaufmann, Simon Peter,
Rastislav Bodik, and Thomas Anderson. Floem: A Programming System for
NIC-Accelerated Network Applications. In 13th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 18), pages 663–679, Carlsbad, CA,
October 2018. USENIX Association.
[23] Thomas Reps and Genevieve Rosay. Precise interprocedural chopping. In Proceed-
ings of the 3rd ACM SIGSOFT symposium on Foundations of software engineering,
pages 41–52, 1995.
[24] Amedeo Sapio, Ibrahim Abdelaziz, Abdulla Aldilaijan, Marco Canini, and Panos
In
In-network computation is a dumb idea whose time has come.
Kalnis.
Proceedings of the 16th ACM Workshop on Hot Topics in Networks, 2017.
[25] Naveen Kr. Sharma, Chenxingyu Zhao, Ming Liu, Pravein G. Kannan, Changhoon
Kim, Arvind Krishnamurthy, and Anirudh Sivaraman. Programmable calendar
queues for high-speed packet scheduling. In Ranjita Bhagwan and George Porter,
editors, 17th USENIX Symposium on Networked Systems Design and Implementa-
tion, 2020.
[26] Anirudh Sivaraman, Alvin Cheung, Mihai Budiu, Changhoon Kim, Mohammad
Alizadeh, Hari Balakrishnan, George Varghese, Nick McKeown, and Steve Lick-
ing. Packet Transactions: High-Level Programming for Line-Rate Switches. In
Proceedings of the 2016 ACM SIGCOMM Conference, SIGCOMM ’16, pages 15–28,
New York, NY, USA, 2016. ACM.
[27] Robbert van Renesse and Fred B. Schneider. Chain replication for supporting high
throughput and availability. In Proceedings of the 6th Conference on Symposium
on Operating Systems Design & Implementation - Volume 6, 2004.
[28] Mark Weiser. Program slicing. IEEE Transactions on software engineering, (4):352–
357, 1984.
295