Alexa-top US sites, percentage ﬁle completion over time for
a large FTP download, and percentage ﬁle completion for
two separate BitTorrent downloads. In all three conﬁgura-
tions, stateful recovery performs close to the performance of
the baseline. For stateless recovery over the HTTP connec-
tions, we see a sharp knee corresponding to the connection
reset time: 180 seconds12. The only application with lit-
tle impact under stateless recovery is one of the BitTorrent
downloads – however, the other BitTorrent download failed
almost entirely and the client had to be restarted! The tor-
rent which failed had only 10 available peers and, when the
connections were reset, the client assumed that the peers had
gone ofﬂine. The other torrent had a large pool of available
peers and hence could immediately reconnect to new peers.
Our point in these experiments is not to suggest that ap-
plications are fundamentally incapable of rapid recovery in
scenarios of stateless recovery, but simply that many existing
applications do not.
7. RELATED WORK
We brieﬂy discuss the three lines of work relevant to FTMB,
reﬂecting the taxonomy of related work introduced in §2.
First are no-replay schemes. In §6 we described in detail
three recent systems – Remus, Pico and Colo – that adopt
this approach and compared FTMB to them experimentally.
The second are solutions for rollback recovery from the
distributed systems literature. The literature includes a wide
range of protocol proposals (we refer the reader to Elnozahy
et al. [31] for an excellent survey); however, to our knowl-
edge, there is no available system implementation that we
can put to the test for our application context.13 More gen-
erally, as mentioned earlier, the focus on distributed sys-
tems (as opposed to a parallel program on a single machine)
changes the nature of the problem in many dimensions, such
as: the failure model (partial vs. complete failure), the na-
ture of non-determinism (primarily the arrival and sending
order of messages at a given process vs. threads that ‘race’
to access the same variable), the frequency of output (for us,
outputs are generated at a very high rate) and the frequency
of nondeterminism (per-packet for us), and where the per-
formance bottlenecks lie (for us, in the logging and output
commit decision). These differences led us to design new
solutions that are simpler and more lightweight than those
found in the literature.
The ﬁnal class of solutions are the multicore record-and-
replay systems used for debugging. These do not implement
output commit. We discussed these solutions in broad terms
in §2 and evaluated one such system (Scribe) in §6.
In the remainder of this section we brieﬂy review a few
additional systems.
Hypervisor-based Fault Tolerance [18] was an early, pio-
neering system in the 90s to implement fault-tolerance over
arbitrary virtual machines; their approach did not address
multicore systems, and required synchronization between
the master and replica for every nondeterministic operation.
SMP Revirt [29] performs record-and-replay over Xen VMs;
unlike FTMB SMPRevirt is hence fully general. As in
Scribe, SMP ReVirt uses page protection to track memory
accesses. For applications with limited contention, the au-
thors report a 1.2-8x slowdown, but for so-called “racy” ap-
plications (like ours) with tens or hundreds of thousands of
faults per second we expect results similar to those of Scribe.
Eidetic Systems [24] allow a user to replay any event in the
system’s history – on the scale of even years. They achieve
very low overheads for their target environment: end user
desktops. However, the authors explicitly note that their so-
lutions do not scale to racy and high-output systems.
R2 [36] logs a cut in an application’s call graph and intro-
duces detailed logging of information ﬂowing across the cut
using an R2 runtime to intercept syscalls and underlying li-
braries; the overhead of their interception makes them poorly
suited to our application with frequent nondeterminism.
ODR [16] is a general record-and-replay system that pro-
11We picked 300ms as a conservative estimate of recovery time; our results
are not sensitive to the precise value.
12Firefox, Chrome, and Opera have reset times of 300 seconds, 50 seconds,
and 115 seconds respectively.
13In their survey paper, Elnozahy et al. state that, in practice, log-based
rollback-recovery has seen little adoption due to the complexity of its algo-
rithms.
 0 50 100 150 200 250 300 350 400 450 500 0 50 100 150 200Completed PagesTime (s) 0 200 400 600 800 1000 1200 1400 1600 100 200 300 400Completed KBTime (s)BaselineStatefulStateless 0 50 100 150 200 250 300 350 400 0 300 600 900 1200Completed MBTime (s) 0 50 100 150 200 250 300 350 400 100 200 300 400Completed MBTime (s)238vides output determinism: to reduce runtime overhead ODR
foregoes logging all forms of nondeterminism and instead
searches the space of possible executions during replay. This
can result in replay times that are several orders of magni-
tude higher than the original execution (in fact, the search
space is NP hard). This long replay time is not acceptable for
applications looking to recover from a failure (as opposed to
debugging post-failure).
8. DISCUSSION
We presented FTMB, a system for rollback recovery
which uses ordered logging and parallel release for low over-
head middlebox fault-tolerance. We showed that FTMB im-
poses only 30µs of latency for median packets through an
industry-designed middlebox. FTMB has modest through-
put overheads, and can perform replay recovery in 1-2 wide
area RTTs. Before closing, we discuss the growing NFV
software ecosystem and how FTMB ﬁts into this future.
User-Level Packet Processing Stacks. Signiﬁcant ef-
forts in industry and research aim to allow for fast packet-
processing in user space (e.g. netmap [52], DPDK [38]) in
and out of virtual machines [39,45,53–55]. At present, none
of these new systems support seamless virtual machine mi-
gration. This support will be required as NFV stacks be-
come more widely deployed; once these systems do support
migration they will be compatible with FTMB.
New Virtualization Techniques and Fast Checkpoint-
ing. Linux Containers [5] offer ﬁner-grained virtualization
than Xen, offering processes isolation while sharing oper-
ating system components. Containers enable cheaper snap-
shotting, as less memory is copied per snapshot. Several on-
going efforts are exploring hardware and software support
for faster snapshotting [1, 20] which will improve migration
and reliability systems [11, 23] thereby improving FTMB’s
tail latencies introduced by VM suspension.
Varied Operating Systems and Hardware Compo-
nents. One of the oft-touted beneﬁts of NFV [32] is the
potential for a wider range of diversity in system software
stacks and hardware; wider at least, than today’s closed-
down, all-in-one ‘appliances’. With this change comes the
opportunity to extend the failure model assumed by FTMB
and other systems (e.g. [23]): by running different drivers,
NICs, and operating systems outside of the virtualization
layer, one may be able to protect against not only hardware
and software failures, but bugs fundamental to the choice of
driver, NIC, OS, etc., by failing over to a machine with en-
tirely different components.
Diverse Middlebox Software. The Open Source Com-
munity offers software implementations of many middle-
boxes; e.g., Snort [8], Bro [10], Vyatta [12], and Squid [9],
as does the broader industry. While we experimented
with Click-based middlebox implementations, we expect our
techniques should be equally applicable to these other sys-
tems. In addition, vendors of hardware-based appliances are
increasingly re-architecting their products as software-only
implementations. Hence we expect the potential application
of FTMB to only grow in the future.
If the NFV ecosystem continues to gain traction with the
above software trends, it will need a practical, low-overhead
solution for reliability. We envision FTMB’s approach – or-
dered logging and parallel release – to be that solution.
9. ACKNOWLEDGEMENTS
We thank the anonymous reviewers of the SIGCOMM
program committee and our shepherd Jeff Chase for their
thoughtful feedback on this paper. We thank the middlebox
vendors we spoke with for helpful discussions about FTMB,
reliability practices, and state of the art network appliances.
Jiawei Chen and Eddie Dong at Intel kindly shared the Colo
source code and helped us deploy it in our lab at Berke-
ley. Kay Ousterhout provided feedback on many iterations
of this paper. This material is based upon work supported by
the National Science Foundation Graduate Research Fellow-
ship under Grant No. DGE-1106400. This work was in part
made possible by generous ﬁnancial support and technical
feedback from Intel Research.
10. REFERENCES
[1] A Peek into Extended Page Tables.
https://communities.intel.com/community/itpeernetwork/datastack/
blog/2009/06/02/a-peek-into-extended-page-tables.
[2] Alexa: Actionable Analytics for the Web. http://www.alexa.com/.
[3] Clang Static Analyzer. http://clang-analyzer.llvm.org/.
[4] Intel PRO/1000 Quad Port Bypass Server Adapters.
http://www.intel.com/content/www/us/en/network-adapters/
gigabit-network-adapters/pro-1000-qp.html.
[5] LXC - Linux Containers. https://linuxcontainers.org/.
[6] Remus PV domU Requirements.
http://wiki.xen.org/wiki/Remus_PV_domU_requirements.
[7] Riverbed Completes Acquisition of Mazu Networks.
http://www.riverbed.com/about/news-articles/press-releases/
riverbed-completes-acquisition\-of-mazu-networks.html.
[8] Snort IDS. https://www.snort.org/.
[9] Squid: Optimising Web Delivery. http://www.squid-cache.org/.
[10] The Bro Network Security Monitor. https://www.bro.org/.
[11] VMWare vSphere. https://www.vmware.com/support/vsphere.
[12] Vyatta. http://www.vyatta.com.
[13] Wikipedia:seqlock. http://en.wikipedia.org/wiki/Seqlock.
[14] Multi-Service Architecture and Framework Requirements.
http://www.broadband-forum.org/technical/download/TR-058.pdf,
2003.
[15] A. Akella, A. Anand, A. Balachandran, P. Chitnis, C. Muthukrishnan,
R. Ramjee, and G. Varghese. EndRE: An End-System Redundancy
Elimination Service for Enterprises. In Proc. USENIX NSDI, 2010.
[16] G. Altekar and I. Stoica. ODR: Output-Deterministic Replay for
Multicore Debugging. In Proc. ACM SOSP, 2009.
[17] R. H. Arpaci-Dusseau and A. C. Arpaci-Dusseau. Operating
Systems: Three Easy Pieces. Arpaci-Dusseau Books, 0.80 edition,
May 2014.
[18] T. C. Bressoud and F. B. Schneider. Hypervisor-based Fault
Tolerance. In Proc. ACM SOSP, 1995.
[19] C. Cadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted and
Automatic Generation of High-Coverage Tests for Complex Systems
Programs. In Proc. USENIX OSDI, 2008.
[20] J. Chung, J. Seo, W. Baek, C. CaoMinh, A. McDonald, C. Kozyrakis,
and K. Olukotun. Improving Software Concurrency with
Hardware-assisted Memory Snapshot. In Proc. ACM SPAA, 2008.
[21] E. G. Coffman, M. Elphick, and A. Shoshani. System Deadlocks.
ACM Comput. Surv., 3(2):67–78, June 1971.
[22] H. Cui, J. Simsa, Y.-H. Lin, H. Li, B. Blum, X. Xu, J. Yang, G. A.
Gibson, and R. E. Bryant. Parrot: A Practical Runtime for
239Deterministic, Stable, and Reliable Threads. In Proc. ACM SOSP,
2013.
[23] B. Cully, G. Lefebvre, D. Meyer, M. Feeley, N. Hutchinson, and
A. Warﬁeld. Remus: High Availability via Asynchronous Virtual
Machine Replication. In Proc. USENIX NSDI, 2008.
[24] D. Devecsery, M. Chow, X. Dou, J. Flinn, and P. M. Chen. Eidetic
Systems. In Proc. USENIX OSDI, 2014.
[25] J. Devietti, B. Lucia, L. Ceze, and M. Oskin. DMP: Deterministic
Shared Memory Multiprocessing. In Proc. ACM ASPLOS, 2009.
[26] Digital Corpora. 2009-M57-Patents packet trace.
http://digitalcorpora.org/corp/nps/scenarios/2009-m57-patents/net/.
[27] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall,
G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy.
RouteBricks: Exploiting Parallelism To Scale Software Routers. In
Proc. ACM SOSP, 2009.
[28] Y. Dong, W. Ye, Y. Jiang, I. Pratt, S. Ma, J. Li, and H. Guan. COLO:
COarse-grained LOck-stepping Virtual Machines for Non-stop
Service. In Proc. ACM SoCC, 2013.
[29] G. W. Dunlap, D. G. Lucchetti, M. A. Fetterman, and P. M. Chen.
Execution Replay of Multiprocessor Virtual Machines. In Proc. ACM
SIGPLAN/SIGOPS VEE, 2008.
[30] E. N. Elnozahy and W. Zwaenepoel. Manetho: Transparent roll
back-recovery with low overhead, limited rollback, and fast output
commit. IEEE Trans. Comput., 41(5):526–531, May 1992.
[31] E. N. M. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B. Johnson. A
Survey of Rollback-Recovery Protocols in Message-passing
Systems. ACM Comput. Surv., 34(3):375–408, Sept. 2002.
[32] European Telecommunications Standards Institute. NFV Whitepaper.
https://portal.etsi.org/NFV/NFV_White_Paper.pdf.
[33] S. K. Fayazbakhsh, L. Chiang, V. Sekar, M. Yu, and J. C. Mogul.
Enforcing Network-wide Policies in the Presence of Dynamic
Middlebox Actions Using Flowtags. In Proc. USENIX NSDI, 2014.
[34] M. Flajslik and M. Rosenblum. Network Interface Design for Low
Latency Request-Response Protocols. In Proc. USENIX ATC, 2013.
[35] A. Gember, R. Viswanathan, C. Prakash, R. Grandl, J. Khalid,
S. Das, and A. Akella. OpenNF: Enabling Innovation in Network
Function Control. In Proc. ACM SIGCOMM, 2014.
[36] Z. Guo, X. Wang, J. Tang, X. Liu, Z. Xu, M. Wu, M. F. Kaashoek,
and Z. Zhang. R2: An Application-Level Kernel for Record and
Replay. In Proc. USENIX OSDI, 2008.
[37] S. Han, K. Jang, K. Park, and S. Moon. PacketShader: a
GPU-accelerated Software Router. In Proc. ACM SIGCOMM, 2010.
[38] Intel. Data Plane Development Kit. http://dpdk.org/.
[39] Intel. PCI-SIG SR-IOV Primer: An Introduction to SR-IOV
Technology. http://www.intel.com/content/www/us/en/pci-express/
pci-sig-sr-iov-primer-sr-iov-technology-paper.html.
[40] R. Kohavi and R. Longbotham. Online experiments: Lessons
learned. Computer, 40(9):103–105, 2007.
[41] O. Laadan, N. Viennot, and J. Nieh. Transparent, Lightweight
Application Execution Replay on Commodity Multiprocessor
Operating Systems. In Proc. ACM SIGMETRICS, 2010.
[42] L. Lamport. Time, clocks, and the ordering of events in a distributed
system. Commun. ACM, 21(7):558–565, July 1978.
[43] C. Lattner and V. Adve. LLVM: A Compilation Framework for
Lifelong Program Analysis & Transformation. In Proc. IEEE CGO,
2004.
[44] J. R. Lorch, A. Baumann, L. Glendenning, D. Meyer, and
A. Warﬁeld. Tardigrade: Leveraging Lightweight Virtual Machines
to Easily and Efﬁciently Construct Fault-Tolerant Services. In Proc.
USENIX NSDI.
[45] J. Martins, M. Ahmed, C. Raiciu, V. Olteanu, M. Honda, R. Bifulco,
and F. Huici. ClickOS and the Art of Network Function
Virtualization. In Proc. USENIX NSDI, 2014.
[46] R. Mittal, J. Sherry, S. Ratnasamy, and S. Shenker. Recursively
Cautious Congestion Control. In Proc. USENIX NSDI, 2014.
[47] A. Pesterev, J. Strauss, N. Zeldovich, and R. T. Morris. Improving
Network Connection Locality on Multicore Systems. In Proc. ACM
EuroSys, 2012.
[48] R. Potharaju and N. Jain. Demystifying the Dark Side of the Middle:
A Field Study of Middlebox Failures in Datacenters. In Proc. ACM
IMC, 2013.
[49] Z. A. Qazi, C.-C. Tu, L. Chiang, R. Miao, V. Sekar, and M. Yu.
SIMPLE-fying Middlebox Policy Enforcement Using SDN. In Proc.
ACM SIGCOMM, 2013.
[50] S. Rajagopalan, D. Williams, and H. Jamjoom. Pico Replication: A
High Availability Framework for Middleboxes. In Proc. ACM SoCC,
2013.
[51] S. Rajagopalan, D. Williams, H. Jamjoom, and A. Warﬁeld.
Split/Merge: System Support for Elastic Execution in Virtual
Middleboxes. In Proc. USENIX NSDI, 2013.
[52] L. Rizzo. netmap: a Novel Framework for Fast Packet I/O. In Proc.
USENIX ATC, 2012.
[53] L. Rizzo and G. Lettieri. Vale: a Switched Ethernet for Virtual
Machines. In Proc. ACM CoNEXT, 2012.
[54] L. Rizzo, G. Lettieri, and V. Mafﬁone. Speeding Up Packet I/O in
Virtual Machines. In ACM/IEEE ANCS, pages 47–58, 2013.
[55] R. Russell. virtio: Towards a De-facto Standard for Virtual I/O
Devices. ACM OSR, 42(5):95–103, 2008.
[56] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. Anderson.
Eraser: A Dynamic Data Race Detector for Multi-threaded
Programs. In Proc. ACM SOSP, 1997.
[57] F. B. Schneider. Implementing Fault-tolerant Services Using the
State Machine Approach: A Tutorial. ACM Comput. Surv.,
22(4):299–319, Dec. 1990.
[58] V. Sekar, N. Egi, S. Ratnasamy, M. K. Reiter, and G. Shi. Design and
Implementation of a Consolidated Middlebox Architecture. In Proc.
USENIX NSDI, 2012.
[59] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratnasamy, and
V. Sekar. Making Middleboxes Someone Else’s Problem: Network
Processing as a Cloud Service. In Proc. ACM SIGCOMM, 2012.
[60] R. Strom and S. Yemini. Optimistic Recovery in Distributed
Systems. ACM Trans. Comput. Syst., 3(3):204–226, Aug. 1985.
[61] K. Veeraraghavan, D. Lee, B. Wester, J. Ouyang, P. M. Chen,
J. Flinn, and S. Narayanasamy. DoublePlay: Parallelizing Sequential
Logging and Replay. In Proc. ACM ASPLOS, 2012.
[62] Z. Wang, Z. Qian, Q. Xu, Z. Mao, and M. Zhang. An Untold Story of
Middleboxes in Cellular Networks. In Proc. ACM SIGCOMM, 2011.
240