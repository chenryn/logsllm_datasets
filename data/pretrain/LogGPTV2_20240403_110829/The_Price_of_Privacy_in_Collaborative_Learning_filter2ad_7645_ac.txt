(p∗
1, p∗
2) = (ρ, 0) where ρ is deﬁned by Eq. (3) where
[·]−1 is the inverse function of [·] and r = C1
B1 :
u00(ρ)  0
b(0) > r
otherwise
h b0Φ0
i−1
ρ =
(3)
(r)
0
1
c0
if
if
The three possible NEs when Player 2 is unconcerned,
and the corresponding Price of Privacy values are:
–
If the possible maximal beneﬁt is higher than the
B1 ) for Player 1, this player
weight ratio (b(0) > C1
should train without any privacy protection since
(p∗
1, p∗
2) = (0, 0) is a NE. In this case P oP = 0.
1, p∗
If all the required conditions in Th. 3 hold, (p∗
2) =
–
n b(θn, Φn(0, 0))
– Otherwise (p∗
1, p∗
2) = (1, 0) is a NE. In this case,
when one player apply maximal privacy protection
(Player 1), the other player’s utility cannot be pos-
itive due to the Def. 4. Furthermore, since Player 2
is privacy unconcerned, its actual payoﬀ is 0 inde-
pendently of its action. As a result, (p∗
2) = (1, p2)
is a NE for all p2 ∈ [0, 1] as they all correspond to
the same 0 payoﬀ. For simplicity, we use (1, 1) to
1, p∗
(cid:18)h b0Φ0
c0
i−1(cid:16) C1
(cid:17)
P
B1
(cid:19)
(cid:18)
P oP = 1 −
, 0
is a NE with
(cid:18)h b0Φ0
c0
i−1(cid:16) C1
B1
(cid:19)(cid:19)
(cid:17)
, 0
n b
θn, Φn
P
Together or Alone: The Price of Privacy in Collaborative Learning
7
p1Φ1 = ∂i
Theorem 4. The CoL game has at least one non-trivial
pure-strategy NE if
Φb·(∂p1Φ1− ∂p2Φ2) = ∂Φb·(∂p1 ∂p2Φ2− ∂p1 ∂p2Φ1) (6)
∂2
p2Φ2 for i ∈
Corollary (1). If we assume ∂i
{1, 2} then Th. 4 holds.
The condition on the derivatives of Φn in Cor. 1 means
that the player’s accuracy changes the same way in rela-
tion to their own privacy parameter, independently from
the other player’s privacy parameter. In Sec. 6 we mea-
sure the accuracy for multiple privacy parameter values
and ﬁnd that this is indeed the case. Moreover, we ﬁnd
that Φ1 ≈ Φ2 when the players have equal dataset sizes.2
4.4 Remarks on the NEs
Note that a non-trivial pure-strategy NE does not nec-
essarily correspond to positive payoﬀs: if ∃n ∈ N :
un(p∗
2)  0 and C2 = 0 if
ρ is:
1, p∗
if
if Φ00 (r)  0, ρ ∈ [0, 1]
r ≤ θ1 − Φ(0)
otherwise
 0
1
ρ =
[Φ0]−1 (r)
To compute the exact (numerical) NE and the corre-
sponding P oPPrice of Privacy of the CoL game, we
need to deﬁne the function Φn. While for simpler train-
ing algorithms Φn is known [IL13, CGL15], for more
complex algorithms it can only be approximated. We
demonstrate a potential approximation method called
self-division in Sec. 8.
4.3 Both Players are Privacy Concerned
Now we consider the general case when both players’
privacy weights are non-zero. We prove the existence of
a pure-strategy NE besides the trivial (p∗
2) = (1, 1);
we utilize the chain rule of derivation for higher dimen-
sions and a result from the theory of potential games
[MS96].
Lemma (Chain Rule). If f : R2 → R and g : R2 → R
are diﬀerentiable functions, then
1, p∗
∀i ∈ [1, 2] : ∂xi f(x, g(x1, x2)) = ∂gf · ∂xi g(x1, x2)
Deﬁnition (Potential Game [MS96]). A two-player
game G is a potential game if the mixed second or-
der partial derivative of the utility functions are equal:
∂p1 ∂p2 u1 = ∂p1 ∂p2 u2
(5)
Fig. 2. Learning Sequentially
Theorem (Monderer & Shapley [MS96]). Every po-
tential game admits at least one pure-strategy NE.
Now we can state the theorem which holds even if both
players are privacy-concerned:
2 Note that these ﬁndings are empirical: based on the speciﬁc
datasets/algorithm we used. For more details see Sec. 6.
Together or Alone: The Price of Privacy in Collaborative Learning
8
If there are only two parties in a distributed learning
scenario (right side of Fig. 1), the trained model reveals
some information about both players’ dataset. Hence,
in our scenario, the players train the same model iter-
atively without any safe aggregation. If there are only
two participants, parallelization does not improve the
eﬃciency much, so the players are training the model se-
quentially as seen in Fig. 2. The problem of information
leakage is tackled with privacy-preserving mechanisms.
Our use case is a RecSys scenario. We assume that
players hold a user-item rating matrix with a common
item-set I = I1 = I2 and disjoint user-set: U1 ∩ U2 = ∅
where U = U1 ∪ U2. As usual, rui ∈ R|U|×|I| refers to
the rating user u gives item i.
The goal of the learning algorithm is to ﬁnd the
items that users desire. One of the most widespread
method to do that is MF [KBV09] as seen in Fig. 3:
ﬁnding P|U|×κ and Qκ×|I| such that P · Q ≈ R. As the
user-sets are disjoint, players only need to share the item
feature matrix Q.
Fig. 3. RecSyS Scenario
The goal of a RecSys algorithm is to minimize the
error between the prediction and the observed ratings
as described in Eq. (7) where λ is the regularization pa-
rameter, while pu (qi) is the corresponding row (column)
in P (Q) for rui.
X
rui∈R
min
P,Q
(rui − puqi)2 + λ(||pu||2 + ||qi||2)
(7)
One of the most popular techniques to minimize this
formula is SGD. It works by iteratively selecting a ran-
dom rating rui ∈ R and updating the corresponding fac-
tor vectors according to Eq. (8) where eiu = puqi − rui
and γ is the learning rate.
p0
u := pu + γ(euiqj − λpu)
q0
i := qi + γ(euipu − λqi)
(8)
Since we use SGD, the training process shown in
Fig. 2 is essentially equivalent to mini-batch learning
where the batches are the datasets of the players. As
we use RecSys as an illustrative example, we simplify
it: we assume that players share the learning algorithm
which is embedded with the necessary parameters such
as learning rate γ, regularization parameter λ, number
of features κ, and maximum number of iterations ι. No-
tations are summarized in Tab. 2.
Variable Meaning
U
I
R
rui
P, Q
γ
λ
κ
ι
Joint user-set
Itemset
Rating matrix
Rating of user u for item i
Feature matrices
Learning rate
Regularization parameter
Number of features
Number of iterations
Table 2. RecSys Parameters
5.2 Privacy Preserving Mechanisms
We focus on input manipulation for privacy preserva-
tion as we are concerned with input data privacy. In fact,
[FBK16] concluded that input perturbation achieves the
most eﬃcient accuracy-privacy trade-oﬀ amongst vari-
ous DP mechanisms. We investigate Sup and bDP as
available mechanisms.
5.2.1 Suppression
Deﬁnition 7 (Suppression). Sup removes input data
from the original dataset to protect it from information
leakage resulting from the model or the learning pro-
cess. The size of reduction is determined by the privacy
parameter p ∈ [0, 1], i.e., p is proportional to the data
removed3.
Sup essentially chooses a subset of the dataset to be used
for training together. Sup can be used to remove sensi-
tive data from the dataset, so even if the other player
can reconstruct the dataset from the trained model, the
removed part remains fully protected.
5.2.2 Bounded Diﬀerential Privacy
To apply bDP, we must determine the sensitivity of the
machine learning algorithm ﬁrst:
Theorem 5 (Sensitivity of RecSys). The sensitivity S
of the introduced RecSys scenario is
S ≤ κ · ι · γ · (∆r · pmax − λ · qmax)
(9)
Proof Th. 5. See App. C.
3 We treat p as a continuous variable even though it is discrete;
this does not aﬀect our analysis owing to large dataset sizes.
Together or Alone: The Price of Privacy in Collaborative Learning
9
Now, we consider the bDP mechanism [FBK16]:
Deﬁnition 8 (bounded DP). bDP aims to hide the
value of a rating. To achieve ε-bDP, each rating is mod-
iﬁed as it is shown in Eq. (10) where L(x) is a Laplacian
noise with 0 mean and x variance.
rmax
rmin
rui + L( S
ε )
r0
ui :=
if
if
rui + L( S
rui + L( S
otherwise
ε ) ≥ rmax
ε ) ≤ rmin
(10)
5.3 Unifying Privacy Parameters
These approaches are hard to compare since they fo-
cus on protecting diﬀerent things. Sup aims to provide
maximal privacy for some of the data while leaving the
rest unprotected. On the other hand, bDP provides an
equal amount of privacy for all the data based on the
parameter ε. In the CoL game we deﬁned the privacy
parameter on a scale 0 to 1, therefore the speciﬁc param-
eters of Sup and bDP must be mapped to [0, 1] where
p = 0 means no privacy, while p = 1 stands for full pri-
vacy protection. For Sup the value p is straightforward:
p represents the portion of data removed. Hiding the
dataset in whole (100% protection) means p = 1 while
if the whole dataset is used for training (0% protection)
then p = 0.
In case of bDP, 100% privacy (p = 1) is achieved
when ε = 0 (inﬁnite noise) while ε ≈ ∞ corresponds
to zero noise (p = 0). This relation can be captured
via a function f : [0,∞) → [0, 1] such that f(0) = 1,
limx→∞ f(x) = 0 and f is monotone decreasing. We
ε+1 and ε = f−1(p) =
use the mapping p = f(ε) = 1
p − 1. This mapping does not carry meaning such as
1
equivalence in any sense between methods, so it is not
used for direct comparison. We only use it to convert
the privacy parameter ε into [0, 1] so we can use bDP as
privacy-preserving method M in the CoL game deﬁned
in Sec. 3.
6 Determining Φ for RecSys
For all research questions in Sec. 1 answers depend on
n . Consequently, in this section we measure the model
ΦM
accuracy for various privacy parameters with regard to
the learning task and the privacy mechanisms intro-
duced in Sec. 5.
For our experiments, we implemented SGD as train-
ing algorithm in Matlab [Pej18]. We used the MovieLens
1M [Gro03] and Netﬂix [Net09] datasets; we shrunk the
Netﬂix dataset to 10% by randomly ﬁltering out 90%
of the users. Furthermore, both datasets are prepro-
cessed similarly to [FBK16]. Preprocessing is described
in details in App. D. We will refer to the preprocessed
datasets as 1M and NF10, respectively. The parameters
of the preprocessed datasets are shown in Tab. 3.
Dataset