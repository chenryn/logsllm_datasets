# Evaluating Fuzz Testing

**Authors:**
- George Klees, University of Maryland
- Andrew Ruef, University of Maryland
- Benji Cooper, University of Maryland
- Shiyi Wei, University of Texas at Dallas
- Michael Hicks, University of Maryland

## Abstract
Fuzz testing has been highly effective in discovering security-critical bugs in real-world software. Recent research has focused on developing new fuzzing techniques, strategies, and algorithms. These innovations are primarily evaluated through experiments, raising the question: What experimental setup is required to produce trustworthy results? We surveyed 32 recent fuzzing papers and assessed their experimental evaluations, finding issues in all of them. We then conducted our own extensive experimental evaluation using an existing fuzzer. Our results confirmed that the problems identified in existing evaluations can indeed lead to incorrect or misleading assessments. We conclude with guidelines to improve the experimental evaluations of fuzz testing algorithms, ensuring more robust and reliable results.

## CCS Concepts
- **Security and Privacy:** Software and Application Security

## Keywords
- Fuzzing
- Evaluation
- Security

## ACM Reference Format
George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018. Evaluating Fuzz Testing. In 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS '18), October 15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3243734.3243804

## 1 Introduction
A fuzz tester (or fuzzer) is a tool that iteratively and randomly generates inputs to test a target program. Despite appearing "naive" compared to more sophisticated tools involving SMT solvers, symbolic execution, and static analysis, fuzzers are surprisingly effective. For example, the popular fuzzer AFL has been used to find hundreds of bugs in widely used programs [1]. In a head-to-head comparison with the symbolic executor angr, AFL found 76% more bugs (68 vs. 16) in the same corpus over a 24-hour period [50].

The success of fuzzers has made them a popular topic of research. However, while new ideas for fuzzers may be inspired by mathematical analysis, they are primarily evaluated experimentally. When a researcher develops a new fuzzer algorithm (denoted as A), they must empirically demonstrate that it provides an advantage over the status quo (denoted as B). To do this, they must:

- Choose a compelling baseline fuzzer B to compare against.
- Select a sample of target programs (the benchmark suite).
- Define a performance metric, ideally the number of (possibly exploitable) bugs identified by crashing inputs.
- Determine a meaningful set of configuration parameters, such as the seed file(s) to start fuzzing with and the timeout (duration) of a fuzzing run.

An evaluation should also account for the inherently random nature of fuzzing. Each fuzzing run on a target program may produce different results due to randomness. Therefore, an evaluation should measure a sufficient number of trials to sample the overall distribution representing the fuzzer's performance, using statistical tests [38] to determine if the measured improvement of A over B is significant rather than due to chance.

Failure to follow these steps or to adhere to recommended practices can lead to misleading or incorrect conclusions. Such conclusions waste time for practitioners who might benefit more from alternative methods or configurations. They also waste the time of researchers who make overly strong assumptions based on arbitrary tuning of evaluation parameters.

We examined 32 recently published papers on fuzz testing (see Table 1) from top-conference proceedings and other quality venues, and studied their experimental evaluations. We found that no fuzz testing evaluation carries out all the above steps properly (though some come close). This is concerning in theory, and after conducting over 50,000 CPU hours of experiments, we believe it is also problematic in practice.

Using AFLFast [6] (as A) and AFL (as baseline B), we performed a variety of tests to evaluate their performance. We chose AFLFast because it was a recent advance over the state of the art, its code was publicly available, and we were confident in our ability to rerun and expand the experiments described by the authors. This choice was also driven by the importance of AFL in the literature: 14 out of 32 papers we examined used AFL as a baseline in their evaluation.

We targeted three binutils programs (nm, objdump, and cxxfilt) and two image processing programs (gif2png and FFmpeg) used in prior fuzzing evaluations [9, 44, 45, 55, 58]. We found that deviations from the recommended evaluation recipe could easily lead to incorrect conclusions for several reasons:

- **Variability in Fuzzing Performance:** Fuzzing performance under the same configuration can vary substantially from run to run. Comparing single runs, as nearly 35% of the examined papers did, does not provide a complete picture. For example, on nm, one AFL run found just over 1200 crashing inputs while one AFLFast run found around 800. However, comparing the median of 30 runs tells a different story: 400 crashes for AFL and closer to 1250 for AFLFast. Even comparing averages is not enough, as we found that in some cases, via a statistical test, an apparent difference in performance was not statistically significant.
  
- **Performance Over Time:** Fuzzing performance can vary over the course of a run. Short timeouts (less than 5 or 6 hours, as used by 11 papers) may paint a misleading picture. For example, when using the empty seed, AFL found no crashes in gif2png after 13 hours, while AFLFast had found nearly 40. After 24 hours, AFL had found 39 and AFLFast had found 52. When using a non-empty seed set, on nm, AFL outperformed AFLFast at 6 hours, with statistical significance, but after 24 hours, the trend reversed.
  
- **Bug De-duplication:** Stack hashes, while better, still over-counted bugs. Instead of mapping a bug to, say, 500 AFL coverage-unique crashes in a given trial, it would map to about 46 stack hashes, on average. Stack hashes were also subject to false negatives: roughly 16% of hashes for crashes from one bug were shared by crashes from another bug. In five cases, a distinct bug was found by only one crash, and that crash had a non-unique hash, meaning that evidence of a distinct bug would have been dropped by “de-duplication.”

- **Seed Variations:** We found substantial performance variations based on the seeds used. For example, with an empty seed, AFLFast found more than 1000 crashes in nm, but with a small non-empty seed, it found only 24, which was statistically indistinguishable from the 23 found by AFL. Yet, most papers treated the choice of seed casually, assuming any seed would work equally well without providing specifics.

- **Performance Metrics:** 14 out of 32 papers we examined used code coverage to assess fuzzing effectiveness. While covering more code intuitively correlates with finding more bugs [19, 30], the correlation may be weak [28]. Directly measuring the number of bugs found is preferred, yet only about 25% of papers used this direct measure. Most papers instead counted the number of crashing inputs found and applied heuristics to de-duplicate inputs that trigger the same bug. The two most popular heuristics were AFL’s coverage profile (used by 7 papers) and (fuzzy) stack hashes [36] (used by 7 papers). Unfortunately, these de-duplication heuristics are often ineffective.

In an additional experiment, we computed a portion of ground truth. We applied all patches to cxxfilt from the version we fuzzed up until the present, grouping together all inputs that a particular patch caused to now gracefully exit [11], confirming that the patch represented a single conceptual bug fix. We found that all 57,142 crashing inputs deemed “unique” by coverage profiles were addressed by 9 distinct patches, indicating a dramatic overcounting of the number of bugs. Ultimately, while AFLFast found many more “unique” crashing inputs than AFL, it only had a slightly higher likelihood of finding more unique bugs in a given run.

This experiment, the most substantial of its kind, suggests that reliance on heuristics for evaluating performance is unwise. A better approach is to measure against ground truth directly by assessing fuzzers against known bugs, as we did above, or by using a synthetic suite such as CGC [14] or LAVA [16], as done by 6 papers we examined. (Eight other papers considered ground truth in part, but often as “case studies” alongside general claims made using inputs de-duplicated by stack hashes or coverage profiles.)

Overall, fuzzing performance may vary with the target program, so it is important to evaluate on a diverse, representative benchmark suite. In our experiments, we found that AFLFast generally performed better than AFL on binutils programs (matching its originally published result when using an empty seed), but did not provide a statistically significant advantage on the image processing programs. Had these programs been included in its evaluation, readers might have drawn more nuanced conclusions about its advantages. In general, few papers use a common, diverse benchmark suite; about 6 used CGC or LAVA-M, and 2 discussed the methodology in collecting real-world programs, while the rest used a few handpicked programs, with little overlap in these choices among papers. The median number of real-world programs used in the evaluation was 7, and the most commonly used programs (binutils) were shared by only four papers (with no overlap when versions are considered). As a result, individual evaluations may present misleading conclusions internally, and results are hard to compare across papers.

Our study (outlined in Section 3) suggests that meaningful scientific progress on fuzzing requires that claims of algorithmic improvements be supported by more solid evidence. Every evaluation in the 32 papers we looked at lacks some important aspect in this regard. In this paper, we propose clear guidelines to which future papers’ evaluations should adhere. Specifically, researchers should perform multiple trials and use statistical tests (Section 4); they should evaluate different seeds (Section 5) and consider longer (≥ 24 hour vs. 5 hour) timeouts (Section 6); and they should evaluate bug-finding performance using ground truth rather than heuristics such as “unique crashes” (Section 7). Finally, we argue for the establishment and adoption of a good fuzzing benchmark, and sketch what it might look like. The practice of hand-selecting a few particular targets and varying them from paper to paper is problematic (Section 8). A well-designed and agreed-upon benchmark would address this problem. We also identify other problems that our results suggest are worth studying, including the establishment of better de-duplication heuristics (a topic of recent interest [42, 51]) and the use of algorithmic ideas from related areas, such as SAT solving.

## 2 Background
There are many different dynamic analyses that can be described as “fuzzing.” A unifying feature of fuzzers is that they operate on and produce concrete inputs. Otherwise, fuzzers might be instantiated with many different design choices and parameter settings. In this section, we outline the basics of how fuzzers work and touch on the advances of 32 recently published papers that form the core of our study on fuzzing evaluations.

### Core Fuzzing Algorithm
```plaintext
corpus ← initSeedCorpus()
queue ← ∅
observations ← ∅
while ¬isDone(observations, queue) do
    candidate ← choose(queue, observations)
    mutated ← mutate(candidate, observations)
    observation ← eval(mutated)
    if isInteresting(observation, observations) then
        queue ← queue ∪ mutated
        observations ← observations ∪ observation
    end if
end while
```
Parameterized by functions:
- `initSeedCorpus`: Initialize a new seed corpus.
- `isDone`: Determine if the fuzzing should stop based on progress toward a goal or a timeout.
- `choose`: Choose at least one candidate seed from the queue.
- `mutate`: From at least one seed and any observations made, produce a new candidate seed.
- `eval`: Evaluate a seed on the program to produce an observation.
- `isInteresting`: Determine if the observations produced from an evaluation on a mutated seed indicate that the input should be preserved or not.

### 2.1 Fuzzing Procedure
Most modern fuzzers follow the procedure outlined in Figure 1. The process begins by choosing a corpus of “seed” inputs with which to test the target program. The fuzzer then repeatedly mutates these inputs and evaluates the program under test. If the result produces “interesting” behavior, the fuzzer keeps the mutated input for future use and records what was observed. Eventually, the fuzzer stops, either due to reaching a particular goal (e.g., finding a certain type of bug) or reaching a timeout.

Different fuzzers record different observations when running the program under test. In a “black box” fuzzer, a single observation is made: whether the program crashed. In “gray box” fuzzing, observations also consist of intermediate information about the execution, such as the branches taken during execution as determined by pairs of basic block identifiers executed directly in sequence. “White box” fuzzers can make observations and modifications by exploiting the semantics of application source (or binary) code, possibly involving sophisticated reasoning. Gathering additional observations adds overhead. Different fuzzers make different choices, hoping to trade higher overhead for better bug-finding effectiveness.

Usually, the ultimate goal of a fuzzer is to generate an input that causes the program to crash. In some fuzzer configurations, `isDone` checks the queue to see if there have been any crashes, and if there have been, it breaks the loop. Other fuzzer configurations seek to collect as many different crashes as possible and will not stop after the first crash. For example, by default, libfuzzer [34] will stop when it discovers a crash, while AFL will continue and attempt to discover different crashes. Other types of observations are also desirable, such as longer running times that could indicate the presence of algorithmic complexity vulnerabilities [41]. In any of these cases, the output from the fuzzer is some concrete inputs and configurations that can be used outside of the fuzzer.