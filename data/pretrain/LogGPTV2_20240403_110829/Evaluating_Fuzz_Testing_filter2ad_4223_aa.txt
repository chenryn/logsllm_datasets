title:Evaluating Fuzz Testing
author:George Klees and
Andrew Ruef and
Benji Cooper and
Shiyi Wei and
Michael Hicks
George Klees, Andrew Ruef,
Benji Cooper
University of Maryland
Evaluating Fuzz Testing
Shiyi Wei
University of Texas at Dallas
Michael Hicks
University of Maryland
8
1
0
2
t
c
O
8
1
]
R
C
.
s
c
[
2
v
0
0
7
9
0
.
8
0
8
1
:
v
i
X
r
a
ABSTRACT
Fuzz testing has enjoyed great success at discovering security criti-
cal bugs in real software. Recently, researchers have devoted sig-
nificant effort to devising new fuzzing techniques, strategies, and
algorithms. Such new ideas are primarily evaluated experimentally
so an important question is: What experimental setup is needed
to produce trustworthy results? We surveyed the recent research
literature and assessed the experimental evaluations carried out
by 32 fuzzing papers. We found problems in every evaluation we
considered. We then performed our own extensive experimental
evaluation using an existing fuzzer. Our results showed that the
general problems we found in existing experimental evaluations
can indeed translate to actual wrong or misleading assessments. We
conclude with some guidelines that we hope will help improve ex-
perimental evaluations of fuzz testing algorithms, making reported
results more robust.
CCS CONCEPTS
• Security and privacy → Software and application security;
KEYWORDS
fuzzing, evaluation, security
ACM Reference Format:
George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks.
2018. Evaluating Fuzz Testing. In 2018 ACM SIGSAC Conference on Com-
puter and Communications Security (CCS ’18), October 15–19, 2018, Toronto,
ON, Canada. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/
3243734.3243804
1 INTRODUCTION
A fuzz tester (or fuzzer) is a tool that iteratively and randomly gener-
ates inputs with which it tests a target program. Despite appearing
“naive” when compared to more sophisticated tools involving SMT
solvers, symbolic execution, and static analysis, fuzzers are sur-
prisingly effective. For example, the popular fuzzer AFL has been
used to find hundreds of bugs in popular programs [1]. Comparing
AFL head-to-head with the symbolic executor angr, AFL found 76%
more bugs (68 vs. 16) in the same corpus over a 24-hour period [50].
The success of fuzzers has made them a popular topic of research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5693-0/18/10...$15.00
https://doi.org/10.1145/3243734.3243804
Why do we think fuzzers work? While inspiration for new ideas
may be drawn from mathematical analysis, fuzzers are primarily
evaluated experimentally. When a researcher develops a new fuzzer
algorithm (call it A), they must empirically demonstrate that it
provides an advantage over the status quo. To do this, they must
choose:
• a compelling baseline fuzzer B to compare against;
• a sample of target programs—the benchmark suite;
• a performance metric to measure when A and B are run on
the benchmark suite; ideally, this is the number of (possibly
exploitable) bugs identified by crashing inputs;
• a meaningful set of configuration parameters, e.g., the seed
file (or files) to start fuzzing with, and the timeout (i.e., the
duration) of a fuzzing run.
An evaluation should also account for the fundamentally random
nature of fuzzing: Each fuzzing run on a target program may pro-
duce different results than the last due to the use of randomness.
As such, an evaluation should measure sufficiently many trials to
sample the overall distribution that represents the fuzzer’s perfor-
mance, using a statistical test [38] to determine that A’s measured
improvement over B is real, rather than due to chance.
Failure to perform one of these steps, or failing to follow rec-
ommended practice when carrying it out, could lead to misleading
or incorrect conclusions. Such conclusions waste time for practi-
tioners, who might profit more from using alternative methods
or configurations. They also waste the time of researchers, who
make overly strong assumptions based on an arbitrary tuning of
evaluation parameters.
We examined 32 recently published papers on fuzz testing (see
Table 1) located by perusing top-conference proceedings and other
quality venues, and studied their experimental evaluations. We
found that no fuzz testing evaluation carries out all of the above
steps properly (though some get close). This is bad news in theory,
and after carrying out more than 50000 CPU hours of experiments,
we believe it is bad news in practice, too. Using AFLFast [6] (as A)
and AFL (as baseline B), we carried out a variety of tests of their
performance. We chose AFLFast as it was a recent advance over
the state of the art; its code was publicly available; and we were
confident in our ability to rerun the experiments described by the
authors in their own evaluation and expand these experiments by
varying parameters that the original experimenters did not. This
choice was also driven by the importance of AFL in the literature:
14 out of 32 papers we examined used AFL as a baseline in their
evaluation. We targeted three binutils programs (nm, objdump, and
cxxfilt) and two image processing programs (gif2png and FFmpeg)
used in prior fuzzing evaluations [9, 44, 45, 55, 58]. We found that
experiments that deviate from the above recipe could easily lead
one to draw incorrect conclusions, for these reasons:
Fuzzing performance under the same configuration can vary sub-
stantially from run to run. Thus, comparing single runs, as nearly
3
5 of the examined papers seem to, does not give a full picture. For
example, on nm, one AFL run found just over 1200 crashing in-
puts while one AFLFast run found around 800. Yet, comparing the
median of 30 runs tells a different story: 400 crashes for AFL and
closer to 1250 for AFLFast. Comparing averages is still not enough,
though: We found that in some cases, via a statistical test, that an
apparent difference in performance was not statistically significant.
Fuzzing performance can vary over the course of a run. This means
that short timeouts (of less than 5 or 6 hours, as used by 11 papers)
may paint a misleading picture. For example, when using the empty
seed, AFL found no crashes in gif2png after 13 hours, while AFLFast
had found nearly 40. But after 24 hours AFL had found 39 and
AFLFast had found 52. When using a non-empty seed set, on nm
AFL outperformed AFLFast at 6 hours, with statistical significance,
but after 24 hours the trend reversed.
Stack hashes did better, but still over-counted bugs. Instead of
the bug mapping to, say 500 AFL coverage-unique crashes in a
given trial, it would map to about 46 stack hashes, on average. Stack
hashes were also subject to false negatives: roughly 16% of hashes
for crashes from one bug were shared by crashes from another bug.
In five cases, a distinct bug was found by only one crash, and that
crash had a non-unique hash, meaning that evidence of a distinct
bug would have been dropped by “de-duplication.”
We similarly found substantial performance variations based on
the seeds used; e.g., with an empty seed AFLFast found more than
1000 crashes in nm but with a small non-empty seed it found only 24,
which was statistically indistinguishable from the 23 found by AFL.
And yet, most papers treated the choice of seed casually, apparently
assuming that any seed would work equally well, without providing
particulars.
Turning to measures of performance, 14 out of 32 papers we ex-
amined used code coverage to assess fuzzing effectiveness. Covering
more code intuitively correlates with finding more bugs [19, 30]
and so would seem to be worth doing. But the correlation may
be weak [28], so directly measuring the number of bugs found is
preferred. Yet only about 1
4 of papers used this direct measure. Most
papers instead counted the number of crashing inputs found, and
then applied a heuristic procedure in an attempt to de-duplicate
inputs that trigger the same bug (retaining a “unique” input for that
bug). The two most popular heuristics were AFL’s coverage profile
(used by 7 papers) and (fuzzy) stack hashes [36] (used by 7 pa-
pers). Unfortunately, there is reason to believe these de-duplication
heuristics are ineffective.
In an additional experiment we computed a portion of ground
truth. We applied all patches to cxxfilt from the version we fuzzed
up until the present. We grouped together all inputs that a par-
ticular patch caused to now gracefully exit [11], confirming that
the patch represented a single conceptual bugfix. We found that
all 57,142 crashing inputs deemed “unique” by coverage profiles
were addressed by 9 distinct patches. This represents a dramatic
overcounting of the number of bugs. Ultimately, while AFLFast
found many more “unique” crashing inputs than AFL, it only had a
slightly higher likelihood of finding more unique bugs in a given
run.
This experiment, the most substantial of its kind, suggests that
reliance on heuristics for evaluating performance is unwise. A
better approach is to measure against ground truth directly by
assessing fuzzers against known bugs, as we did above, or by using
a synthetic suite such as CGC [14] or LAVA [16], as done by 6 papers
we examined. (8 other papers considered ground truth in part, but
often as “case studies” alongside general claims made using inputs
de-duplicated by stack hashes or coverage profiles.)
Overall, fuzzing performance may vary with the target program,
so it is important to evaluate on a diverse, representative bench-
mark suite. In our experiments, we found that AFLFast performed
generally better than AFL on binutils programs (basically match-
ing its originally published result, when using an empty seed), but
did not provide a statistically significant advantage on the image
processing programs. Had these programs been included in its
evaluation, readers might have drawn more nuanced conclusions
about its advantages. In general, few papers use a common, diverse
benchmark suite; about 6 used CGC or LAVA-M, and 2 discussed the
methodology in collecting real-world programs, while the rest used
a few handpicked programs, with little overlap in these choices
among papers. The median number of real-world programs used
in the evaluation was 7, and the most commonly used programs
(binutils) were shared by only four papers (and no overlap when
versions are considered). As a result, individual evaluations may
present misleading conclusions internally, and results are hard to
compare across papers.
Our study (outlined in Section 3) suggests that meaningful sci-
entific progress on fuzzing requires that claims of algorithmic im-
provements be supported by more solid evidence. Every evaluation
in the 32 papers we looked at lacks some important aspect in this
regard. In this paper we propose some clear guidelines to which
future papers’ evaluations should adhere. In particular, researchers
should perform multiple trials and use statistical tests (Section 4);
they should evaluate different seeds (Section 5), and should consider
longer (≥ 24 hour vs. 5 hour) timeouts (Section 6); and they should
evaluate bug-finding performance using ground truth rather than
heuristics such as “unique crashes” (Section 7). Finally, we argue for
the establishment and adoption of a good fuzzing benchmark, and
sketch what it might look like. The practice of hand selecting a few
particular targets, and varying them from paper to paper, is prob-
lematic (Section 8). A well-designed and agreed-upon benchmark
would address this problem. We also identify other problems that
our results suggest are worth studying, including the establishment
of better de-duplication heuristics (a topic of recent interest [42, 51]),
and the use of algorithmic ideas from related areas, such as SAT
solving.
2 BACKGROUND
There are many different dynamic analyses that can be described as
“fuzzing.” A unifying feature of fuzzers is that they operate on, and
produce, concrete inputs. Otherwise, fuzzers might be instantiated
with many different design choices and many different parameter
settings. In this section, we outline the basics of how fuzzers work,
and then touch on the advances of 32 recently published papers
which form the core of our study on fuzzing evaluations.
Core fuzzing algorithm:
corpus ← initSeedCorpus()
queue ← ∅
observations ← ∅
while ¬isDone(observations,queue) do
candidate ← choose(queue, observations)
mutated ← mutate(candidate,observations)
observation ← eval(mutated)
if isInteresting(observation,observations) then
queue ← queue ∪ mutated
observations ← observations ∪ observation
end if
end while
parameterized by functions:
for mutation.
on progress toward a goal, or a timeout.
• initSeedCorpus: Initialize a new seed corpus.
• isDone: Determine if the fuzzing should stop or not based
• choose: Choose at least one candidate seed from the queue
• mutate: From at least one seed and any observations made
• eval: Evaluate a seed on the program to produce an obser-
vation.
• isInteresting: Determine if the observations produced from
an evaluation on a mutated seed indicate that the input
should be preserved or not.
about the program so far, produce a new candidate seed.
Figure 1: Fuzzing, in a nutshell
2.1 Fuzzing Procedure
Most modern fuzzers follow the procedure outlined in Figure 1. The
process begins by choosing a corpus of “seed” inputs with which to
test the target program. The fuzzer then repeatedly mutates these
inputs and evaluates the program under test. If the result produces
“interesting” behavior, the fuzzer keeps the mutated input for future
use and records what was observed. Eventually the fuzzer stops,
either due to reaching a particular goal (e.g., finding a certain sort
of bug) or reaching a timeout.
Different fuzzers record different observations when running
the program under test. In a “black box” fuzzer, a single observa-
tion is made: whether the program crashed. In “gray box” fuzzing,
observations also consist of intermediate information about the
execution, for example, the branches taken during execution as
determined by pairs of basic block identifiers executed directly in
sequence. “White box” fuzzers can make observations and modifica-
tions by exploiting the semantics of application source (or binary)
code, possibly involving sophisticated reasoning. Gathering addi-
tional observations adds overhead. Different fuzzers make different
choices, hoping to trade higher overhead for better bug-finding
effectiveness.
Usually, the ultimate goal of a fuzzer is to generate an input
that causes the program to crash. In some fuzzer configurations,
isDone checks the queue to see if there have been any crashes, and
if there have been, it breaks the loop. Other fuzzer configurations
seek to collect as many different crashes as they can, and so will not
stop after the first crash. For example, by default, libfuzzer [34]
will stop when it discovers a crash, while AFL will continue and
attempt to discover different crashes. Other types of observations
are also desirable, such as longer running times that could indicate
the presence of algorithmic complexity vulnerabilities [41]. In any
of these cases, the output from the fuzzer is some concrete input(s)
and configurations that can be used from outside of the fuzzer