implement a variety of bandwidth allocation schemes by ei-
Figure 2: CDF of RTTs showing CUBIC ﬁlls buffers.
ther guaranteeing or proportionally allocating bandwidth for
tenants [10, 28, 33, 34, 38, 53, 56, 58, 73]. Some of these
schemes share high-level architectural similarities to AC(cid:69)DC.
For example, EyeQ [34] handles bandwidth allocation at the
edge with a work-conserving distributed bandwidth arbitra-
tion scheme. It enforces rate limits at senders based on feed-
back generated by receivers. Similarly, Seawall [58] pro-
vides proportional bandwidth allocation to a VM or appli-
cation by forcing all trafﬁc through a congestion-controlled
tunnel conﬁgured through weights and endpoint feedback.
The fundamental difference between these schemes and
our approach is the design goals determine the granularity on
which they operate. Performance isolation schemes gener-
ally focus on bandwidth allocation on a VM-level and are not
sufﬁcient to relieve the network of congestion because they
do not operate on ﬂow-level granularity. For example, the
single switch abstraction in EyeQ [34] and Gatekeeper [56]
explicitly assumes a congestion-free fabric for optimal band-
width allocation between pairs of VMs. This abstraction
doesn’t hold in multi-pathed topologies when failure, traf-
ﬁc patterns or ECMP hash collisions [1] cause congestion in
the core. Communication between a pair of VMs may con-
sist of multiple ﬂows, each of which may traverse a distinct
path. Therefore, enforcing rate limits on a VM-to-VM level
is too coarse-grained to determine how speciﬁc ﬂows should
adapt in order to mitigate the impact of congestion on their
paths. Furthermore, a scheme like Seawall [58] cannot be
easily applied to ﬂow-level granularity because its rate lim-
iters are unlikely to scale in the number of ﬂows at high net-
working speeds [54] and its allocation scheme does not run
at ﬁne-grained round-trip timescales required for effective
congestion control. Additionally, Seawall violates our de-
sign principle by requiring VM modiﬁcations to implement
congestion-controlled tunnels.
The above points are not intended to criticize any given
work, but rather support the argument that it is important
for a cloud provider to enforce both congestion control and
bandwidth allocation. Congestion control can ensure low
latency and high utilization, and bandwidth allocation can
provide tenant-level fairness. Bandwidth allocation schemes
alone are insufﬁcient to mitigate congestion because certain
TCP stacks aggressively ﬁll switch buffers. Consider a sim-
ple example where ﬁve ﬂows send simultaneously on the 10
 0 2 4 6 8 10 1 2 3 4 5 6 7 8 9 10Tput (Gbps)TestsIllinoisCUBICRenoVegasHighSpeed 0 1 2 3 4 5 1 2 3 4 5 6 7 8 9 10Tput (Gbps)TestsMaxMinMeanMedian 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 1 2 3 4 5 6 7 8 9 10CDFTCP Round Trip Time (milliseconds)CUBIC (RL=2Gbps)DCTCPWe ﬁrst demonstrate how congestion control state can be
inferred. Figure 4 provides a visual of the TCP sequence
number space. The snd_una variable is the ﬁrst byte that
has been sent, but not yet ACKed. The snd_nxt vari-
able is the next byte to be sent. Bytes between snd_una
and snd_nxt are in ﬂight. The largest number of packets
that can be sent and unacknowledged is bounded by CWND.
snd_una is simple to update: each ACK contains an ac-
knowledgement number (ack_seq), and snd_una is up-
dated when ack_seq > snd_una. When packets traverse
the vSwitch from the VM, snd_nxt is updated if the se-
quence number is larger than the current snd_nxt value.
If ack_seq ≤
Detecting loss is also relatively simple.
snd_una, then a local dupack counter is updated. Time-
outs can be inferred when snd_una < snd_nxt and an
inactivity timer ﬁres. The initial CWND is set to a default
value of 10 [14]. With this state, the vSwitch can determine
appropriate CWND values for canonical TCP congestion con-
trol schemes. We omit additional details in the interest of
space.
3.2 Implementing DCTCP
This section discusses how to obtain DCTCP state and
perform its congestion control.
ECN marking DCTCP requires ﬂows to be ECN-capable,
but the VM’s TCP stack may not support ECN. Thus, all
egress packets are marked to be ECN-capable on the sender
module. When the VM’s TCP stack does not support ECN,
all ECN-related bits on ingress packets are stripped at the
sender and receiver modules in order to preserve the original
TCP settings. When the VM’s TCP stack does support ECN,
the AC(cid:69)DC modules strip the congestion encountered
bits in order to prevent the VM’s TCP stack from decreasing
rates too aggressively (recall DCTCP adjusts CWND propor-
tional to the fraction of congested packets, while traditional
schemes conservatively reduce CWND by half). A reserved
bit in the header is used to determine if the VM’s TCP stack
originally supported ECN.
Obtaining ECN feedback In DCTCP, the fraction of pack-
ets experiencing congestion needs to be reported to the sender.
Since the VM’s TCP stack may not support ECN, the AC(cid:69)DC
receiver module monitors the total and ECN-marked bytes
received for a ﬂow. Receivers piggy-back the reported totals
on ACKs by adding an additional 8 bytes as a TCP Option.
This is called a Piggy-backed ACK (PACK). The PACK is
created by moving the IP and TCP headers into the ACK
packet’s skb headroom [60]. The totals are inserted into the
vacated space and the memory consumed by the rest of the
packet (i.e., the payload) is left as is. The IP header check-
sum, IP packet length and TCP Data Offset ﬁelds are recom-
puted and the TCP checksum is calculated by the NIC. The
PACK option is stripped at the sender so it is not exposed to
the VM’s TCP stack.
If adding a PACK creates a packet larger than the MTU,
the NIC ofﬂoad feature (i.e., TSO) will replicate the feed-
back information over multiple packets, which skews the
feedback. Therefore, a dedicated feedback packet called a
Figure 3: AC(cid:69)DC high-level architecture.
Gbps topology in Figure 7a. Even when the bandwidth is
allocated "perfectly" at 2 Gbps per ﬂow, CUBIC saturates
the output port’s buffer and leads to inﬂated round-trip times
(RTTs) for trafﬁc sharing the same link. Figure 2 shows
these RTTs for CUBIC and also DCTCP, which is able to
keep queueing latencies, and thus RTTs, low even though
no rate limiting was applied. Therefore, it is important for
cloud providers to exercise a desired congestion control.
In summary, our vision regards enforcing tenant conges-
tion control and bandwidth allocation as complimentary and
we claim an administrator should be able to combine any
congestion control (e.g., DCTCP) with any bandwidth allo-
cation scheme (e.g., EyeQ). Flow-level congestion control
and tenant performance isolation need not be solved by the
same scheme, so AC(cid:69)DC’s design goal is to be modular
in nature so it can co-exist with any bandwidth allocation
scheme and its associated rate limiter (and also in the ab-
sence of both).
3. DESIGN
This section provides an overview of AC(cid:69)DC’s design.
First, we show how basic congestion control state can be
inferred in the vSwitch. Then we study how to implement
DCTCP. Finally, we discuss how to enforce congestion con-
trol in the vSwitch and provide a brief overview of how per-
ﬂow differentiation can be implemented.
Figure 4: Variables for TCP sequence number space.
3.1 Obtaining Congestion Control State
Figure 3 shows the high-level structure of AC(cid:69)DC. Since
it is implemented in the datapath of the vSwitch, all trafﬁc
can be monitored. The sender and receiver modules work
together to implement per-ﬂow congestion control (CC).
OSOSOSAppsAppsAppsControl	planeData	path	(AC/DC)vNICvNICvNICDatacenter	NetworkvSwitchVirtual MachinesAC/DC(sender)AC/DC(receiver)Uniformper-flow CCPer-flow CC feedbackServerACKedIn	Flightsnd_unasnd_nxtcwndsequence numbersdow scaling factor is negotiated during TCP’s handshake,
so AC(cid:69)DC monitors handshakes to obtain this value. Cal-
culated congestion windows are adjusted accordingly. TCP
receive window auto-tuning [57] manages buffer state and
thus is an orthogonal scheme AC(cid:69)DC can safely ignore.
Ensuring a VM’s ﬂow adheres to RWND is relatively sim-
ple. The vSwitch calculates a new congestion window ev-
ery time an ACK is received. This value provides a bound
on the number of bytes the VM’s ﬂow is now able to send.
VMs with unaltered TCP stacks will naturally follow our en-
forcement scheme because the stacks will simply follow the
standard. Flows that circumvent the standard can be policed
by dropping excess packets not allowed by the calculated
congestion window, which incentivizes tenants to respect the
standard.
While simple, this scheme provides a surprising amount
of ﬂexibility. For example, TCP enables a receiver to send
a TCP Window Update to update RWND [6]. AC(cid:69)DC can
create these packets to update windows without relying on
ACKs. Additionally, the sender module can generate dupli-
cate ACKs to to trigger retransmissions. This is useful when
the VM’s TCP stack has a larger timeout value than AC(cid:69)DC
(e.g., small timeout values have been recommended for in-
cast [67]). Another useful feature is when AC(cid:69)DC allows
a TCP stack to send more data. This can occur when a VM
TCP ﬂow aggressively reduces its window when ECN feed-
back is received. By removing ECN feedback in AC(cid:69)DC,
the VM TCP stack won’t reduce CWND. In a similar manner,
DCTCP limits loss more effectively than aggressive TCP
stacks. Without loss or ECN feedback, VM TCP stacks
grow CWND. This causes AC(cid:69)DC’s RWND to become the lim-
iting window, and thus AC(cid:69)DC can increase a ﬂow’s rate in-
stantly when RWND < CWND. Note, however, AC(cid:69)DC cannot
force a connection to send more data than the VM’s CWND
allows.
Another beneﬁt of AC(cid:69)DC is that it scales in the number
of ﬂows. Traditional software-based rate limiting schemes,
like Linux’s Hierarchical Token Bucket, incur high overhead
due to frequent interrupts and contention [54] and therefore
do not scale gracefully. NIC or switch-based rate limiters
are low-overhead, but typically only provide a handful of
queues. Our enforcement algorithm does not rate limit or
buffer packets because it exploits TCP ﬂow control. There-
fore, rate limiting schemes can be used at a coarser granular-
ity (e.g., VM-level).
Finally, we outline AC(cid:69)DC’s limitations. Since AC(cid:69)DC
relies on snifﬁng trafﬁc, schemes that encrypt TCP headers
(e.g., IPSec) are not supported. Our implementation only
supports TCP, but we believe it can be extended to handle
UDP similar to prior schemes [34, 58]. Implementing per-
ﬂow DCTCP-friendly UDP tunnels and studying its impact
remains future work, however. And ﬁnally, while MPTCP
supports per-subﬂow RWND [23], it is not included in our
case study and a more detailed analysis is future work.
3.4 Per-ﬂow Differentiation
AC(cid:69)DC can assign different congestion control algorithms
on a per-ﬂow basis. This gives administrators additional
Figure 5: DCTCP congestion control in AC(cid:69)DC.
Fake ACK (FACK) is sent when the MTU will be violated.
The FACK is sent in addition to the real TCP ACK. FACKs
are also discarded by the sender after logging the included
data. In practice, most feedback takes the form of PACKs.
DCTCP congestion control Once the fraction of ECN-
marked packets is obtained, implementing DCTCP’s logic
is straightforward. Figure 5 shows the high-level design.
First, congestion control (CC) information is extracted from
FACKs and PACKs. Connection tracking variables (described
in §3.1) are updated based on the ACK. The variable α is an
EWMA of the fraction of packets that experienced conges-
tion and is updated roughly once per RTT. If congestion was
not encountered (no loss or ECN), then tcp_cong_avoid
advances CWND based on TCP New Reno’s algorithm, using
slow start or congestion avoidance as needed. If congestion
was experienced, then CWND must be reduced. DCTCP’s
instructions indicate the window should be cut at most once
per RTT. Our implementation closely tracks the Linux source
code, and additional details can be referenced externally [3,
11].
3.3 Enforcing Congestion Control
There must be a mechanism to ensure a VM’s TCP ﬂow
adheres to the CWND determined in the vSwitch. Luckily,
TCP provides built-in functionality that can be reprovisioned
for AC(cid:69)DC. Speciﬁcally, TCP’s ﬂow control allows a re-
ceiver to advertise the amount of data it is willing to process
via a receive window (RWND). Similar to other works [37,
61], the vSwitch overwrites RWND with its calculated CWND.
In order to preserve TCP semantics, this value is overwritten
only when it is smaller than the packet’s original RWND. The
VM’s ﬂow then uses min(CWND, RWND) to limit how many
packets it can send.
This enforcement scheme must be compatible with TCP
receive window scaling to work in practice. Scaling en-
sures RWND does not become an unnecessary upper-bound in
high bandwidth-delay networks and provides a mechanism
to left-shift RWND by a window scaling factor [31]. The win-
Extract	CC	info	if	it	is	PACK	or	FACK;Drop	FACK;Incoming ACKUpdate	connection	tracking	variables;Update	⍺once	every	RTT;Congestion?tcp_cong_avoid();NoLoss?Yes⍺=max_alpha;YesNownd=wnd*(1	-⍺/2);AC/DC	enforces	CC	on	the	flow;Send	ACK	to	VM;Cut	wndin	this	window	before?YesNoOther TCP packets, such as data and ACKs, trigger updates
to ﬂow entries. There are many more table lookup operations
(to update ﬂow state) than table insertions or deletions (to
add/remove ﬂows). Thus, Read-Copy-Update (RCU) hash
tables [27] are used to enable efﬁcient lookups. Addition-
ally, individual spinlocks are used on each ﬂow entry in
order to allow for multiple ﬂow entries to be updated simul-
taneously.
Putting it together, the high-level operation on a data packet
is as follows. An application on the sender generates a packet
that is pushed down the network stack to OVS. The packet
is intercepted in ovs_dp_process_packet, where the
packet’s ﬂow entry is obtained from the hash table. Se-
quence number state is updated in the ﬂow entry and ECN
bits are set on the packet if needed (see §3). If the packet’s
header changes, the IP checksum is recalculated. Note TCP
checksumming is ofﬂoaded to the NIC. The packet is sent
over the wire and received at the receiver’s OVS. The re-
ceiver updates congestion-related state, strips off ECN bits,
recomputes the IP checksum, and pushes the packet up the
stack. ACKs eventually triggered by the packet are inter-
cepted, where the congestion information is added. Once the
ACK reaches the sender, the AC(cid:69)DC module uses the con-
gestion information to compute a new congestion window.
Then it modiﬁes RWND with a memcpy, strips off ECN feed-
back and recomputes the IP checksum before pushing the
packet up the stack. Since TCP connections are bi-directional,
two ﬂow entries are maintained for each connection.
Our experiments in §5.1 show the CPU overhead of AC(cid:69)DC
is small and several implementation details help reduce com-
putational overhead. First, OVS sits above NIC ofﬂoading