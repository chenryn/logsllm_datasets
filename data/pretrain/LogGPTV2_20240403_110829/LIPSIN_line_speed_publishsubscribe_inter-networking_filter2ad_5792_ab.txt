d LITs, using k hashes on the Link ID.
both of which are very simple to implement in hardware.
The base decision (Alg. 1), i.e. whether to forward on a
given outbound link or not, can be easily parallelised, as
there are no memory or other shared resource bottlenecks.
From now on, we build an enhanced system on the top of
this simple forwarding operation.
Algorithm 1: Forwarding method of LIPSIN
Input: Link IDs of the outgoing links; zFilter in the
packet header
foreach Link ID of outgoing interface do
if zFilter & Link ID == Link ID then
Forward packet on the link
end
end
3.2 Link IDs and LITs
Due to the nature of Bloom ﬁlters, a query may return
a false positive, leading to a wrong forwarding decision. To
reduce the number of false positives, we now introduce Link
ID Tags (LITs), as an addition to the plain Link IDs. The
idea is that instead of each link being identiﬁed with a single
Link ID, every unidirectional link is associated with a set of
d distinct LITs (Fig. 3). This allows us to construct diﬀer-
ent candidate zFilters and to select the best-performing one
from the candidates, e.g., in terms of the false positive rate,
compliance with network policies, or multi path selection.
The forwarding information is stored in the form of d for-
warding tables, each containing the LIT entries of the active
Link IDs, as depicted in Fig. 4. The only modiﬁcation of the
base forwarding method is that the node needs to be able to
determine on which forwarding table it should perform the
matching operations; for this, we include the index in the
packet header.
Construction: When determining the actual forwarding
tree based on the network graph, and the locations of the
publisher and subscribers, we can apply various policy re-
strictions (e.g.
link-avoidance) and keep traﬃc engineering
in mind (e.g. balancing traﬃc load or avoiding temporarily
congested parts of the network). As a result, we get a set
of unidirectional links to be included into the zFilter. The
ﬁnal step is ORing together the corresponding LITs of the
included links, yielding a candidate BF. As each link has d
diﬀerent identities, we get d candidate BFs that are “equiv-
alent” representations of the delivery tree. That is, a packet
using any of the candidates will follow, at minimum, all the
network links inserted into the BF.
Selection: Recall that a false positive will result in an
Figure 4: Outgoing interfaces are equipped with d
forwarding tables, indexed by the value in the in-
coming packet.
excess delivery; i.e., a packet will be forwarded over a link
that is not part of the delivery tree. To achieve better per-
formance in terms of lower false positive probability, we ﬁrst
consider two relatively simple strategies:
(i) Lowest false positive after hashing (fpa): The se-
lected BF should be the one with the lowest false probability
estimate after hashing: min{ρ0
kd}, where ρ is the
ﬁll factor, i.e. the ratio of 1’s to 0’s.
(ii) Lowest observed false positive rate (fpr): Given
a test set Tset of link IDs, the candidate BF can be chosen
after counting for false positives against Tset. The objective
is to minimize the observed false positives when querying
against a known set of Link IDs active in the forwarding
nodes along the delivery tree.
k0 , . . . , ρd
The fpa strategy is simple and aims at lower false posi-
tives rates for any set of link IDs under membership test.
On the other hand, the fpr yields the best performance of
false positives for a speciﬁc test set at the expense of higher
computational complexity.
To further enhance fpr, false positives at diﬀerent places
can be weighted; i.e., we can consider some false positives
less harmful than others. For example, we can avoid for-
warding towards non-peered domains, resource constrained
regions, or into potential loops. We call such selection cri-
teria as link avoidance, since they are based in penalizing
those candidate BFs that yield false positives when tested
against certain links. For example, the following kinds of
criteria could be considered:
(i) Routing policies: A Tset of links to be avoided due to
routing policies.
(ii) Congestion mitigation: A static Tset of links avoided
due to traﬃc engineering (e.g., low capacity links) and a dy-
namic Tset of congested links.
(iii) Security policies: A Tset of links avoided due to se-
curity concerns.
As a consequence, having multiple candidate representa-
tions for a given delivery tree is a way to minimise the num-
ber of false forwardings in the network, as well as restricting
these events to places where their eﬀects are smallest.
3.3 Stateful functionality
So far, we have considered stateless operations, where each
forwarding node maintains only a static forwarding table
198storing the LITs. We now carefully introduce state to the
network in the form of virtual links and fast failure recovery.
While increasing hardware and signaling cost, the state re-
duces the overall cost due to increased traﬃc eﬃciency when
facing large multicast groups or link failures.
3.3.1 Virtual links
In the case of dense trees, especially when a number of
trees share multiple consecutive links, it becomes eﬃcient to
identify sets of individual links with a separate Link ID and
associated LITs. We call such sets of links as virtual links.
The abstraction introduces the notion of tunnels (or link
aggregation) into our architecture – a notion more general
than traditional one-to-one or one-to-many tunnels, being
able to represent any link sets, including partial one-to-many
trees, forests of partial trees, many-to-one concast trees, etc.
A virtual link may be generated by the topology layer
whenever it sees the need for such a tree. The creation
process consists of selecting the individual links over which
the virtual link is created, assigning it a new Link ID, and
computing the LITs. To ﬁnalize the creation process, the
topology layer needs to communicate the Link ID, together
with the LITs, to the nodes residing on the virtual link.
Note that virtual link maintenance does not need to hap-
pen in line speed; there are always alternative ways of send-
ing the same data. For example, if a virtual link is needed
to support a very large multicast tree, the sender can still
send multiple packets instead of one, each covering only a
part of the tree.
Once the virtual link creation process is ﬁnished, we can
use a LIT of this virtual link in any zFilter instead of in-
cluding all the individual LITs into it. This reduces the
probability for false positives when matching the zFilter on
the path. On the other hand, adding forwarding table en-
tries into nodes increases the sizes of the forwarding tables.
Given the typical Zipf-distribution of the number of mul-
ticast receivers [24], the sizes of the forwarding tables will
still remain small compared to the current situation with
IP routers. Unfortunately, falsely matching to a virtual link
will mean falsely forwarding packets through the entire con-
nected part of the denoted subgraph; however, this can be
mitigated by careful naming of the virtual links (e.g. more
1-bits than in the case of physical links) and explicitly avoid-
ing these false positives during BF-selection.
3.3.2 Fast recovery
Whenever a link or a node fails, all delivery trees ﬂowing
through the failed component break.
In this section, we
consider two approaches for fast re-routing around single
link and node failures.
Our ﬁrst approach is to replace a failed link with a func-
tionally equivalent virtual link. We call this as VLId-based
recovery. The idea is to have a separate virtual backup path
pre-conﬁgured for each physical link ID, to be dynamically
used in case of failure. This virtual backup path has the
same Link ID and LITs as the physical link it replaces, but
is initially inactive to avoid false forwarding.
The main advantage of this solution is that there is no
need to change the packets. Basically, it is enough that the
node detecting a failure sends an activation message over
the replacement path, activating it for both the failed phys-
ical link and any virtual links ﬂowing over the physical link,
and then starts to forward the packets normally. When re-
ceiving the activation message, the nodes along the backup
path reconﬁgure their forwarding tables, and as a result, the
unmodiﬁed packets ﬂow over the replacement path.
Another approach is to have a pre-computed zFilter en-
coding the replacement path. In this method, when a node
detects a failure,
it simply needs to add the appropriate
LIT(s) representing the backup path into the zFilter in the
packet. This method does not add any additional signaling
or state to the forwarding nodes, but it increases the prob-
ability of false positives by increasing the ﬁll factor of the
zFilter.
Both of the mechanisms are capable of re-routing the traf-
ﬁc with zero convergence time and without service disrup-
tion. Besides protecting against single link failures, they are
also able to recover from single node failures, if the operator
has conﬁgured multiple backup paths or a backup tree to-
wards all the neighbours of the failed node. These two types
of failures cover around 85% of all unplanned outages [27].
In the complex cases where the proposed mechanisms are
not able to perform local rerouting, new zFilters need to be
computed.
3.3.3 Loop prevention
In some cases false positives can result in loops; for in-
stance, consider the case where a zFilter encodes a forward-
ing path A → B → C, but, due to a false positive, the zFilter
also matches with a separate link C → A, which is used to
forward packets from C to A. Without loop prevention, this
will cause an endless loop of A → B → C → A. Obviously,
as the constructed delivery tree may cause a loop, we can
still use the fpr method to select only loopless candidate
BFs. However, this does not guarantee loop freeness as the
network changes.
As an alternative solution, we start with each node know-
ing the neighboring nodes’ outgoing Link ID and LITs to-
wards the node itself; we call these the incoming Link ID
and LITs. Now, for each incoming packet, the node checks
the incoming LITs of its interfaces, except the one from
where the packet arrives, and compares them to the zFilter.
A match means that there is a possibility for a loop, and
the node caches the packet’s zFilter and the incoming Link
ID for a short period of time. In case of a loop, the packet
will return over a diﬀerent link than the cached one. Our
early evaluation is based on this approach and suggests that
a small caching memory does not penalize the performance.
As a third alternative, at the inter-AS level we can di-
vide the links into up, transit, and down ones, and utilise
the valley-free traﬃc model. As a ﬁnal method, it remains
always possible to use TTL similar to what IP uses today.
3.3.4 Explicitly blocking false positives
Most false positives cause a packet to be sent to a node
that will drop it. In some cases, the traﬃc generated as a
result of a false positive should be fully truncated; e.g., in the
case of low capacity or congested links, heavy non-cacheable
traﬃc ﬂows, or inter-domain link policies it may be necessary
to locally disable forwarding of some traﬃc. Hence, we need
a means to explicitly block the falsely forwarded traﬃc ﬂows
at an upstream point.
Therefore, any node can signal upstream a request to
block a speciﬁc zFilter over that physical link. This can
be implemented as a “negative” virtual Link ID, where a
match blocks forwarding over the link instead of enabling it.
1993.4 Control messages, slow path, and services
To inject packets into the slow path of forwarding nodes,
each node can be equipped with a local, unique Link ID
denoting the node-internal passway from the switching fab-
ric to the control processor. That allows targeted control
messages to be passed to one or a few speciﬁc nodes, if de-
sired. Additionally, there may be virtual Link IDs attached
to these node-local passways, making it possible to multicast
control messages to a number of forwarding nodes without
needing to explicitly name each of them.
If the messages
need to be modiﬁed, or even stopped on a node, the simul-
taneous forwarding should be blocked. This can be done
by using zFilters constructed for node-to-node communica-
tion, or using a virtual Link ID especially conﬁgured to pass
messages to the slow path and make the forwarding decision
after the message has been processed.
Generalising, we make the observation that the egress
points of a virtual link can be basically anything: nodes,
processor cards within nodes, or even speciﬁc services. This
would allow our approach to be extended to upper layers.
Another usage of control messages is collecting a symmet-
ric reverse path from a subscriber to the publisher for the
purpose of e.g. providing feedback. The publisher can ini-
tiate a control message triggering reverse path collection.
Getting the message, each intermediate node bitwise ORs
the appropriate reverse LIT with the path already collected
and forwards it towards the subscriber. When the message
ﬁnally reaches the subscriber, it will have a valid zFilter
towards the publisher. The zFilter was created without in-
teracting with the topology system.
4. EVALUATION
We now study some of the design trade-oﬀs in detail.
First, we introduce a few performance indicators, and then
explore scalability limits and system performance. We use
packet-level ns-3 simulations over realistic AS topologies,
gaining insights on the forwarding eﬃciency of the proposed
solution. Finally, we consider security aspects.
4.1 Performance indicators
A fundamental metric is the false positive rate of the in-
packet Bloom ﬁlter. Link ID Tags are already in the form
of m-bit vectors, with k bits set to one, as they are added
to a candidate BFi. An accurate estimate of the basic false
positive rate can be given once the ﬁll factor ρ of the BF is
known. The false positive after hashing f pa is the expected
false positive estimate after BF construction:
f pa = ρ
k
(1)
The fpa-optimized BF selection was introduced in Sec. 3.2
and is based on ﬁnding the set of LITs with the smallest
predicted f pa. The observed false positive probability is the
actual false positive rate (fpr) when a set of membership
queries are made on the BF:
f pr =
#Observed false positives
#Tested elements
(2)
Note that the f pr is an experimental quantity and not a
theoretical estimate. The minimum observed f pr of the d
candidate BFs provides a reference lower bound for a speciﬁc
BF design.
These two metrics form the basic BF-selection criteria.
While fpa-optimized selection is cheaper in computational
AS
Nodes (#)
Links (#)
Diameter
Radius
Avg (Max) degr.
1221
104
151
8
4
3257
161
328
10
5
3967
79
147
10
6
6461
138
372
8
4
TA2
65
108
8
5
2 (18)
3 (29)
3 (12)
5 (20)
3 (10)
Table 1: Graph characterization of a subset of
router-level AS topologies used in the experiments.
terms, the fpr-optimized selection will give better results as
the actual topology is more precisely considered in this pro-
cess. However, the fpr describes the overall network per-
formance only indirectly.
In order to capture better the
actual bandwidth consumption due to false positives, we in-
troduce forwarding eﬃciency as a metric to quantify the
bandwidth overhead caused by sending packets through un-
necessary links:
f we =
#Links on shortest path tree
#Links during delivery
(3)
In other words, forwarding eﬃciency is 100% if the packets
strictly follow the shortest path tree for reaching the sub-
scribers. Consequently, this metric is representative and use-
ful in the scenarios where larger subscriber sets are reached
with multiple smaller delivery trees, or in virtual link scenar-
ios, where false positives may be costly by causing deliveries
over multiple hops.
4.2 Packet level simulations
First, we used the intra-domain AS topologies from Rock-
etfuel [1] to simulate the protocol behaviour. Though not