size
log n
1
1
1
1
1
1
update
info.
1
NA
1
1
nǫ
1
1
query
time
log n
1
1
1
nǫ
1
update
time
log n
NA
n log n
n
nǫ
nǫ/ logκǫ−1 n
nǫ/ logκǫ n
1
verify
time
log n
1
1
1
1
1
1
crypto
oper.
hashing
exp
exp
exp, BM
exp
exp
exp
4. We give a practical evaluation of our scheme using
state of the art software [31] for primitive operations
(namely, modular exponentiations, multiplications, in-
verse computations). We show that for κ = 1 and
ǫ = 0.1 our scheme scales very well.
5. We propose studying lower bounds for authenticated
set-membership queries using cryptographic accumu-
lators.
1.2 Related Work
There has been a lot of work on authenticating member-
ship queries using diﬀerent algorithmic and cryptographic
approaches. A summary and qualitative comparison can be
found in Table 1.
Several authenticated data structures based on crypto-
graphic hashing have been developed for membership queries
(e.g., [4, 15, 22, 25, 28]), both in the two-party and three-
party authentication models. These data structures achieve
O(log n) proof size, query time, update time and veriﬁcation
time. These bounds are optimal for hash-based methods, as
was shown in [33]. Variations of this approach and exten-
sions to other types of queries have also been investigated
(e.g., [5, 12, 17, 34]).
Solutions for authenticated membership queries in vari-
ous settings using another cryptographic primitive, namely
one-way accumulators, were introduced by Benaloh and de
Mare [3]. Based on the RSA exponentiation function, this
scheme implements a secure one-way function that satisﬁes
quasi-commutativity, a useful property that usual hash func-
tions lack. This RSA accumulator is used to summarize a
set so that set-membership can be veriﬁed with O(1) over-
head. Reﬁnements of the RSA accumulator are also given
in [2], where except for one-wayness, collision resistance is
achieved, and also in [11, 30]. Dynamic accumulators (along
with protocols for zero-knowledge proofs) were introduced
in [6], where, using the trapdoor information (these pro-
tocols are secure, assuming an honest prover), the time to
update the accumulated value or a witness is independent
on the number of the accumulated elements.
A ﬁrst step towards a diﬀerent direction, where we assume
that we cannot trust the prover and therefore the trapdoor
information (e.g., the group order φ(N )) is kept secret, but
applicable only to the three-party model, was made in [14];
in this work, general O(nǫ) bounds are derived for various
complexity measures such as query and update time. An au-
thenticated data structure that combines hierarchical hash-
ing with the accumulation-based scheme of [14] is presented
in [16], and a similar hybrid authentication scheme appears
in [27].
Accumulators using other cryptographic primitives (gen-
eral groups with bilinear pairings) the security of which
is based on other assumptions (hardness of strong Diﬃe-
Hellman problem) are presented in [26]. However, updates
in [26] are ineﬃcient when the trapdoor information is not
known: individual precomputed witnesses can each be up-
dated in constant time, thus incurring a linear total cost for
updating all the witnesses after an update in the set. Eﬃ-
cient dynamic accumulators for non-membership proofs are
presented in [20]. Accumulators for batch updates are pre-
sented in [35] and accumulator-like expressions to authen-
ticate static sets for provable data possession are presented
in [1]. The work in [29] studies eﬃcient algorithms for accu-
mulators with unknown trapdoor information. Finally in [10]
and simultaneously with our work, logarithmic lower bounds
as well as constructions achieving query-update cost trade-
oﬀs that are similar to our work, have been studied in the
memory-checking model [4].
1.3 Organization of the Paper
In Section 2, we introduce some necessary cryptographic
and algorithmic ideas needed for the development of our con-
struction. We also give the security deﬁnition of our scheme.
In Section 3, we develop our main authentication construc-
tion and prove its security, providing a solution for static
sets. In Section 4, we apply our construction to hash tables
and derive our main results. In Section 5, we provide an eval-
uation and analysis of our method showing its practicality,
and ﬁnally in Section 6, we conclude with future work and
interesting open problems.
2. PRELIMINARIES
In this section we describe some algorithmic and crypto-
graphic methods and other concepts used in our approach.
Hash Tables. The main functionality of the hash table data
structure is the support of super eﬃcient look-ups in O(1)
time of elements that belong to a general set (not neces-
sarily ordered). Diﬀerent ways to implement the hash table
data structure have been extensively studied (e.g., [9, 18,
19, 21, 24]), where the basic idea behind them is the follow-
ing. Suppose we wish to store n elements from a universe U
in a data structure so that we can have expected constant
look-up time. For totally ordered universes and by search-
ing based on comparisons, it is well known that we need
Ω(log n) time. However, we can do better: Set up a one-
dimensional table T [1 . . . m] where m = O(n), ﬁx a special
function h : U → {1, . . . , m}, such that for any two elements
e1, e2 ∈ U it is Pr [h(e1) = h(e2)] ≤ 1
m , and store element
e in slot T (h(e)). The probabilistic property that holds for
h, combined with the fact that h can be computed in O(1)
time, leads to the conclusion that there is always a constant
expected number of elements that map to the same slot i
(1 ≤ i ≤ m) and, therefore, look-up takes constant expected
time.
But this nice property of hash tables comes at some cost.
The expected constant-time look-up holds only when the
number of elements stored in the hash table does not change,
i.e., when the hash table is static. When there are insertions
and deletions, it is obvious that we might end up in a sit-
uation where there are more than O(1) elements stored in
some slot of the hash table, which increases the time needed
for queries. To maintain slots that few elements map to, we
might have to increase the size of the hash table by a con-
stant factor (e.g., double the size), which is expensive since
we have to rehash all the elements of the hash table by using
another hash function. Therefore, there might be one update
(over a course of O(n) updates) that takes time O(n) and
not O(1). That leads to the fact that hash tables support
expected O(1) time queries, but updates in O(1) expected
amortized time. Methods that vary the size of the hash ta-
ble, for the sake of maintaining O(1) query time, fall into
the general category of dynamic hashing, satisfying:
Theorem 1
(Dynamic Hashing [8]). For a set of size
n, dynamic hashing can be implemented to use O(n) space
and have O(1) expected query cost for membership queries
and O(1) expected amortized cost for insertions or deletions.
Note that by choosing a suitable function that distributes n
elements into buckets (i.e., array slots), we can have f1(n)
buckets and maintain the expected bucket size (i.e., elements
within the bucket) to be f2(n), for general f1(·), f2(·) such
that f1(n) · f2(n) = O(n). Therefore we have the following
corollary:
Corollary 1. Let f1(·), f2(·) be functions such that for
all n it is f1(n) · f2(n) = O(n). Dynamic hashing can be
implemented to use O(n) space and have O(f2(n)) expected
query cost for membership queries, O(f2(n)) expected amor-
tized cost for insertions and deletions, and O(f1(n)) buckets.
We use the above result in our schemes, where f1(n) =
O(n/ logκ n) and f2(n) = O(logκ n) for some ﬁxed κ > 0.
Prime Representatives. In our construction we exten-
sively use the notion of prime representatives, which were
initially introduced in [2] and provide a solution whenever it
is necessary to map general elements to prime numbers—in
our setting, for security reasons. In particular, a method for
mapping a k-bit element ei to a 3k-bit prime xi is to use
two-universal hash functions, as introduced by Carter and
Wegman [7]. A family H = {h : A → B} of functions is
two-universal if, for all w1 6= w2 and for a randomly cho-
sen function h from H, it is Pr[h(w1) = h(w2)] ≤ 1/|B|.
In our context, set A consists of 3k-bit boolean vectors and
set B consists of k-bit boolean vectors, and we use the two
universal function h(x) = F x, where F is a k × 3k boolean
matrix. Since the linear system h(x) = F x has more than
one solution, one k-bit element is mapped to more than one
3k-bit elements. We are interested in ﬁnding only one such
solution which is prime; this can be computed as follows:
Lemma 1
(Prime Representatives [11, 14]). Let H
be a two-universal family of functions from {0, 1}3k to {0, 1}k
and h ∈ H. For any element ei ∈ {0, 1}k, with high probabil-
ity, we can compute a prime xi ∈ {0, 1}3k so that h(xi) = ei,
by sampling O(k2) times from the set of inverses h−1(ei).
This means that we can compute prime representatives in
expected constant time, since the dimension of our problem
is the number of the elements in the hash table, n. Also,
solving the k×3k linear system in order to compute the set of
inverses can be performed in polynomial time in k, by using
standard methods (e.g., Gaussian elimination). Finally, note
that prime representatives are computed (and stored) only
once, since computing multiple times a prime representative
of the same element does not output the same prime, for
Lemma 1 describes a randomized process. From now on,
given a k-bit element x, we denote with r(x) the 3k-bit prime
representative that is computed as described above.
Cryptographic Accumulators. If k denotes the security
parameter, we ﬁrst give the deﬁnition of negligible functions.
Definition 1
(Negligible Function). We say that a
real-valued function ν(k) over natural numbers is neg(k) if
for any nonzero polynomial p, there exists m such that ∀n >
m, |ν(n)|  3k),
namely N = pq, where p, q are strong primes [6]. We can eﬃ-
ciently represent E with a k′-bit integer, namely the integer
f (E) = gr(e1)r(e2)...r(en) mod N , where g ∈ QRN [6] and
r(ei) is a 3k-bit prime representative. This representation
has the property that any computational bounded adver-
sary A, that does not know φ(N ), cannot ﬁnd another set
of elements E ′ 6= E such that f (E ′) = f (E), unless A breaks
the strong RSA assumption [2], which is stated as follows:
Definition 2
(Strong RSA Assumption). Given an
RSA modulus N and a random element x ∈ ZN , it is hard (it
happens with probability neg(k), i.e., negligible in the secu-
rity parameter k) for a computationally bounded adversary
A to ﬁnd y > 1 and a such that ay = x mod N .
We now present a useful lemma (we defer the proof to the
full version of the paper).
Lemma 2. Let k be the security parameter, h be a two-
universal hash function that maps 3w-bit primes to w-bit
integers and N be a (w + 1)-bit RSA modulus with w =
Θ(k). Given a set of elements E and h, the probability that a
computationally bounded adversary A, knowing only N and
g, can ﬁnd a set E ′ 6= E such that f (E ′) = f (E) is neg(k).
The following result is a consequence of Lemma 2:
Corollary 2. Let k be the security parameter, h be a
two-universal hash function mapping 3w-bit primes to w-bit