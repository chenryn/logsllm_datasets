Recently, we keep receiving issues from the community reporting duplicate URL
addresses in Registry, mostly zookeeper, after upgrading to 2.7.1. Those URLs
are all pushed to the Consumer and because only one of the duplicated URLs is
valid, it causes the consumer to fail when the consumer chooses the invalid
one during load balance.
The following are some typical issues: #3785 #3770 #3920 #4013
After troubleshooting, we found the reason for the problem: in version 2.7.1,
the URL addresses will be registered as a persistent node in zookeeper, which
is different from the behavior in 2.7.0 or before, in which they are
registered as ephemeral nodes. The meaning of the ephemeral node is that it
will get removed from zookeeper when the server process went for a while. Now,
after changed to persistent, the node cannot be removed in some exceptional
scenarios, such as process crashed, process forced killed with kill -9, etc,
eventually leading to the appearance of dirty data.
Advices to solve or avoid the problem:
  1. Never forcibly terminating the process to ensure that Dubbo enters it's elegant shutdown process.
  2. Wait for 2.7.2 to fix this issue, going to be released in early June.
  3. For dirty data that has already been generated, consider manual or ZK scripts to clean up, but be careful of the deletion criteria.
  4. Add the following configuration for 2.7.1:  or 
近期，关于升级到2.7.1版本后注册中心（多数是zookeeper）出现重复URL地址数据无法删除的情况，我们持续收到来自社区的issue报告。以下issue是几个典型的异常现象：#3785
#3770 #3920 #4013
经过问题排查我们定位到了问题的原因：在2.7.1版本中URL地址在zookeeper中被注册为持久persistent节点(2.7.0及之前注册的是临时emperal节点)，这样当server进程异常终止无法进入正常优雅下线流程时，如进程崩溃、强行kill
-9等，导致zookeeper已失效持久节点无法清空，最终导致脏数据的出现。
解决及规避方法：
  1. 上下线过程避免强行终止进程，保证让Dubbo进入优雅停机流程
  2. 官方2.7.2版本将会修复此问题，预计在6月初发布
  3. 对于已经产生的脏数据，考虑手动或ZK脚本予以清理，但此时要注意严格控制删选条件
  4. 2.7.1版本增加配置： 或