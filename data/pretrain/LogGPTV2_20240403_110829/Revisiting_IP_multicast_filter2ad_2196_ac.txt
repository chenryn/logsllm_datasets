the “don’t care” value [40]. With this, all edges can be matched in
parallel with a single TCAM access. Alternately, this state can be
stored in RAM and an edge matched in logarithmic time. Finally,
as mentioned before, the shim header remains unmodiﬁed along the
entire path and requires no updating at Rt.
The second advantage to source-encoded forwarding is that, be-
cause the forwarding state at Rt depends only on its (mostly static)
set of AS neighbors, no wide-area protocol mechanism is required
to construct and maintain this state. Source-encoded forwarding
thus achieves sparseness in protocol mechanism and scalable for-
warding state though at the cost of some additional bandwidth and
memory (at Rs) usage.
Table 1 summarizes the state requirements due to FRM. We note
that while FRM tilts the burden of forwarding state and complexity
onto source access domains, this is a not displeasing arrangement
as the beneﬁt of multicasting is greatest at the source (a receiver’s
bandwidth consumption is unchanged with multicast). Finally, we
note that source-encoded forwarding (somewhat unlike IP source
routing) is easily implemented in hardware and selects paths com-
pliant with the policy choices of intermediate ISPs.
5.3 FRM and Intra-domain Protocols
Many of the operational and scaling issues that complicate inter-
domain multicast routing are less acute in the intra-domain scenario
and hence it appears reasonable to retain existing solutions (e.g.,
PIM-SM, DVMRP) at the intra-domain level. These can interface
to FRM in a straightforward manner; e.g., a group’s internal RP
could notify border routers of domain-wide group membership and
packets could be relayed to/from FRM border routers via tunnel-
ing to the RP or by having border routers join groups with active
sources. If desired however, FRM could be extended to the intra-
domain scenario; we brieﬂy discuss this in Section 8.
6. EVALUATION
In this section, we use simulation and trace-driven calculation to
estimate the storage and bandwidth overhead due to FRM’s group
membership and forwarding components. Due to space constraints,
we present only key results for likely usage scenarios. a more de-
tailed parameter exploration is presented in [41].
The payoff is highly scalable and efﬁcient packet processing at
intermediate routers – to forward a packet, Rt need only check
which of its AS neighbor edges are encoded in the shim header’s
TREE BF. I.e., if A is Rt’s AS number, then, for each neighbor AS
B, Rt checks whether ‘A:B’ is encoded in the packet’s shim header
Setup. To relate performance directly to end-user behavior, we
allow U=232−p users in a domain of preﬁx length p and assume
that each user joins k groups selected using some group popularity
distribution from a total of A simultaneously active groups. Unless
stated otherwise, we model group popularity using a zipﬁan distri-
l
]
e
a
c
s
2
g
o
l
[
)
s
e
t
y
b
(
y
r
o
m
e
m
F
B
_
P
R
G
l
a
t
o
T
 34
 33
 32
 31
 30
 29
 28
 27
 26
 25
 24
 10
k=1 group per user
k=10 groups per user
k=100 groups per user
 12
 14
 16
 18
 20
A: total active groups [log2scale]
Figure 2: Total GRP-BF storage per border router.
bution akin to the Web [42] and pessimistically assume no locality
in group membership; any locality would only improve scalability.
We use Subramanian et al.’s Oct’04 snapshots of BGP routing ta-
bles and their AS-level topologies annotated with inter-AS peering
relationships [43].
6.1 Group Membership
Memory overhead. Per-preﬁx GRP BFs are required to store
group membership information. We compute the GRP BF size for a
single preﬁx as follows: the number of groups advertised per preﬁx
– denoted G – is the expected number of distinct groups given that
U users each pick k-from-A as per the selected group popularity
distribution and hence the corresponding GRP BF size is the bloom
ﬁlter size needed to encode G items for a target false positive rate of
f /(A− G) (recall that f is the target number of ﬁlters per preﬁx).
Then, to estimate the total storage due to GRP BF state at a BGP
router, we use real BGP tables [43] and compute the total storage
per router as the sum of the GRP BF size corresponding to each
preﬁx entry. Figure 2 plots this total storage for increasing A for
f =10 and k =1, 10, and 100 groups per user.
Overall, we see that the memory required to maintain group
membership state, while not trivial, is very manageable given cur-
rent storage technology and costs. For example, 1 million simul-
taneously active groups and 10 groups per user requires approxi-
mately 3 GB – an amount of memory found today on even user
machines. Moreover, the trend in memory costs should allow FRM
to handle the relatively slower growth in BGP table size.
Bandwidth costs. We use back-of-the-envelope calculations to
show that the bandwidth due to updating group membership is tractable.
Recall that a domain updates its membership for group G only
when the number of members of G within the domain falls to, or
rises above, zero. Moreover, some domain-level damping of group
departures is likely. We thus generously assume a preﬁx sees a new
group appear or an existing group depart every second. Updates are
conveyed as the set of GRP BF bit positions to be set/reset. Hence
if we assume GRP BFs use 5 hash functions and bit positions are
represented as 24 bit values (in multiples of 256-bytes), then updat-
ing membership for a single preﬁx requires approximately 15 bytes
per second (Bps). If we assume a router with full BGP routes has
200,000 preﬁx entries (current reports indicate ∼170,000 FIB en-
tries [44]) then the total bandwidth consumed due to updates would
Group size
Ideal multicast
100
1000
10,000
100,000
1M
10M
28
158
1000
4151
8957
15353
FRM per-AS unicast
28
159
1012
4233
9155
15729
38
246
1962
9570
21754
39229
Table 2: total-tx: the total number of packet transmissions
for increasing group sizes.
be approximately 3MBps – a small fraction of the bandwidth ca-
pacity at core BGP routers.
The ﬁrst node to join a group within its preﬁx/domain incurs
the latency due to inter-domain GRP BF update propagation. (The
latency of subsequent joins is that of an intra-domain join.) Un-
like regular BGP updates, GRP BF updates do not trigger distributed
route recomputations and hence their rate of propagation will likely
be limited primarily by protocol constraints (if any) used to bound
update trafﬁc (as opposed to concerns about routing loops, incon-
sistencies, and the like). Our current prototype limits inter-AS
GRP BF updates to once per second which would lead to a “ﬁrst-
time” join latency of ∼ 1-6 seconds given current AS path lengths
[44]. Further deployment experience would be required to better
gauge appropriate update intervals.
6.2 Forwarding Overhead
Bandwidth costs. The bandwidth overhead due to FRM for-
warding stems from: (1) the per-packet shim header and, (2) the
redundant transmissions required when subtrees are too large to be
encoded in a single shim header. We assume ﬁxed 100 byte shim
headers and measure the overhead in packets transmitted; our re-
sults extrapolate to different shim header sizes in a straightforward
manner.2
We use two metrics to quantify FRM’s overhead due to redun-
dant transmissions:
• total-tx: the total number of packet transmissions re-
quired to multicast a single packet from the source to all re-
ceivers
• per-link-tx: the number of transmissions per link used
to multicast a single packet from source to all receivers.
To calibrate FRM’s performance, we measure the above for: (1)
“ideal” multicast in which exactly one packet is transmitted along
each edge of the source-rooted tree and, (2) per-AS unicast in which
the source unicasts each member AS individually. This latter can
be achieved using only FRM’s group membership component and
thus represents a simple network layer solution that requires no
multicast-speciﬁc forwarding at routers (as does FRM).
Table 2 lists total-tx for increasing group sizes. We see that
for all group sizes, the overall bandwidth consumed by FRM is
very close to that of ideal multicast (between 0-2.4% higher) while
per-AS unicasts can require more than twice the bandwidth of ideal
multicast. As expected, the difference between FRM and ideal mul-
ticast grows with increasing group size due to the multiple shim
headers needed to encode the larger trees.
2100 bytes represents ∼10% overhead on typical data (i.e., non-
ack) packets which appears reasonable. In practice, for greater efﬁ-
ciency, a source might choose from a few well-known shim header
sizes; e.g., we ﬁnd even 20B headers would sufﬁce for groups of
upto a few thousand.
 10000
 1000
 100
 10
S
A
r
e
p
s
n
o
i
t
p
e
c
e
r
t
e
k
c
a
p
f
o
r
e
b
m
u
N
 1
 0.9
FRM, 1000 users
FRM, 100,000 users
FRM, 10M users
per-AS ucast, 1000 users
per-AS ucast, 100,000 users
per-AS ucast, 10M users
group size=10M; entire tree
group size=10M; tree w/o leaf ASes
group size=10M; w/ aggr links
 100
 10
S
A
r
e
p
s
n
o
i
t
p
e
c
e
r
t
e
k
c
a
p
f
o
r
e
b
m
u
N
 0.92
 0.94
 0.98
Percentage of ASes on tree
 0.96
 1
 1
 0.995
 0.996
 0.997
 0.998
 0.999
 1
Percentage of AS domains on tree
Figure 3: CDF of per-link-tx, the transmissions per AS link
for FRM and per-AS unicasts.
Figure 4: CDF of transmissions per (AS) link with optimizations
to reduce the size of the encoded tree.
 10000
no aggregate links
aggr; nbrth=0.1,pktth=0.1
aggr; nbrth=0.5,pktth=0.25
aggr; nbrth=0.75,pktth=0.5
 1000
S
A
r
e
p
s
e
i
r
t
n
e
g
n
d
r
a
w
i
r
o
f
f