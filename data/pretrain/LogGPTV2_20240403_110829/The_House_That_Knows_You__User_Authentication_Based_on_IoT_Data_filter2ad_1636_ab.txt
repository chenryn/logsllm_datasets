teraction with the IoT devices, the authentication module
continuously receives data extracted from the most recent
observation window and computes an authentication score
for each user. The authentication score can be used by upper-
level authorization systems for flexible policies according
to various levels of risk. For example, log in to a local de-
vice might require a lower score than performing a financial
transaction in the cloud.
4
Below we provide more details about the machine learning
models we used and the features we selected for our multi-
class classifier.
3.3 Machine Learning Design for User
Classification
We design a machine learning (ML) system that: (1) learns
user profiles over time, using data extracted from network
traffic generated by IoT devices; and (2) computes an au-
thentication score in real time based on the most recent
observation interval. The authentication score estimates the
confidence or probability of the actual user being in the room.
In more details, the ML system is trying to learn profiles
for a set of users U = {U1, . . . , Um}. In our specific appli-
cation, user observation is performed over time, and thus
the features or user attributes need to be defined over an
observation window. We design the system to predict at time
t the probability that a certain user is in the room, i.e.,:
i = Pr[U t
At
i |F(t, ∆)],∀i ∈ [1, m]
Given an observation or feature set F(t, ∆) computed over
the time window [t − ∆, t] of size ∆ (most recent history of
length ∆), the system aims to predict an authentication score
i at time t for each user Ui. This can be achieved by training
At
a multi-class classifier f ∈ H, where H is the hypothesis
space. The ML algorithm is given historical training data of la-
beled user activities: Dtr = {(F1(t1, ∆), L1), . . . ,(Fn(tn, ∆), Ln)},
with labels Li ∈ U and Fi(ti , ∆) the set of features computed
over observation intervals [ti , ti − ∆] of length ∆. At the end
of training, a model f is selected to minimize a certain loss
function on the training set.
A probabilistic model in fact estimates the probability that
a user generates the observed features in the most recent time
window of length ∆. The sum of the predicted probabilities
is always 1:
m
i =1
m
i =1
At
i =
Pr[U t
i |F(t, ∆)] = 1,∀t
At testing time, the model is applied at time t to the most
recent observation window (i.e., features F(t, ∆) extracted
during time interval [t − ∆, t]) and generates an authentica-
tion score At
i for each user Ui ∈ U.
We experiment with multiple ML classification algorithms,
including Logistic Regression, Random Forest, and Gradient
Boosting that fit our framework very well. The most chal-
lenging issue in our design is to create appropriate feature
representations that capture user behavior over the recent
time window and can be used to effectively differentiate mul-
tiple users with high confidence. We discuss multiple feature
representations next.
Figure 1: Architecture diagram of the data-flow and proposed authentication process
Categorical Name of the smart device (extracted from the MAC address)
Description
Attribute
Device Name
Time
Packet length
Type
Integer
Integer
Domain
Direction
Protocol
Categorical
Categorical
Categorical
Destination Port Categorical
Categorical
Source Port
The timestamp of the packet
Size of the packet in bytes
Domain name of the destination, if available
Direction of the packet (outgoing/incoming)
Transport protocol of the packet (TCP/UDP/ICMP)
Port number of the destination address
Port number of the source address
Table 1: Packet-level attributes: These fields are available for each HTTPS packet from its header.
Type
Feature Category
Device/Domain Incoming packet size
Device/Domain Outgoing packet size
Device/Domain
Device/Domain
Inter-event times
Protocols
Device
Domains
Operations
count, sum, min, max, std. dev., mean, median
count, sum, min, max, std. dev., mean, median
count, sum, min, max, std. dev., mean, median
count
distinct
Description
Statistics on incoming packets
Statistics on outgoing packets
Packet counts per protocol (TCP, UDP, ICMP)
Statistics on inter-packet timing
Distinct domain count
Table 2: Features aggregated by time window.
3.4 Feature Representation
Packet-level attributes. Our system design is based on fea-
tures extracted from HTTPS traffic captured at the router in
the smart home. To protect user privacy, one of our design
consideration is to inspect only headers of the HTTPS pack-
ets. The fields that we leverage in our design are shown in
Table 1. They include the device identifier (extracted from
the MAC address in each packet), timing, the packet length,
the direction (outgoing or incoming), transport-level proto-
col (TCP, UDP, or ICMP), destination port, source port, and
destination domain. We analyze the DNS queries from the
network traffic and extract mapping of external IP addresses
to domain names (FQDN).
5
Feature definition. Using packet-level attributes directly
as features in an ML model is not feasible due to the large
number of packets that are usually generated in a network.
Moreover, for our task for user authentication we need ag-
gregated behavioral indicators, beyond the scope of a single
network packet. Therefore, we need to experiment with mul-
tiple feature representations that capture the user behavior
over a time interval of length ∆. For our approach, we use a
sliding-window method to advance the time t at which we
compute authentication scores by one minute. We experi-
ment with multiple values for the window length (from 5
to 30 minutes). For instance, if a user is in the room for an
interval of 30 minutes during training and ∆ = 5 minutes,
we generate 26 sliding windows of size 5 minutes. Features
Authorization ModuleIoT Device ControlCloud ServicesLocalServicesProcess dataFeature extractionLabeldataProcess dataFeature extractionTrain modelsAuthenticationModuleNetworkMonitoring&GatewayTrainingPredictionare generated for each sliding window and labeled with the
identity of the user. At testing time, we compute an authen-
tication score at each time t based on the most recent sliding
window of size ∆, i.e., the interval [t − ∆, t]. That means,
implicitly, that we need to observe the user for of period at
least ∆ before computing an authentication score.
The first source of data for feature extraction is user in-
teractions with different IoT devices. It is well known that
people have different interests and users prefer to interact
with certain type of devices. Moreover, the same user tends
to use the same set of IoT devices they are most familiar with
over time. We believe that device usage is a strong behavioral
indicator, as also confirmed in our evaluation.
Therefore, we define the first set of device-level features
extracted from packet attributes aggregated per device. In
particular, we would like to capture information about user
interaction with each IoT device during the time window
∆, such as: (1) various statistics on incoming and outgoing
packet lengths; (2) packet counts for transport-level proto-
cols; (3) number of distinct domains contacted by each device;
(4) statistics on inter-arrival timing of packets. The list of
aggregated features is given in Table 2, and all the features
are aggregated over the packets sent and received by each
device in each time window of size ∆. An important design
decision is to separate incoming and outgoing traffic, as they
might have different distributions. In particular, user inter-
action with voice assistants could result in large volumes
of incoming traffic (for instance, if a user listens to music),
while device communication with the cloud induces regu-
lar outgoing communication. We denote this set of device
features in time window [t − ∆, t] as: F(t, ∆)dev.
(3) Both device and domain: Estimate and predict
P[Ui|F(t, ∆)dev; F(t, ∆)dom].
4 SYSTEM IMPLEMENTATION
4.1 User study
We performed a user study with multiple users over three
weeks to generate the dataset used to train and test our au-
thentication models. We utilize the IoT lab at our institution
to conduct our user study and monitor users while they in-
teract with a smart-home like environment. The lab is an
enclosed studio with a kitchen and living area, equipped
with a range of Internet-connected devices and appliances
to simulate a smart household. The lab is available for use
by researchers at our institution. We collected data from 15
IoT devices installed in our IoT lab, as described in Table 3.
We asked users to use the room for multiple sessions, each
of them lasting at least 15 minutes. One of our requirements
was that users are alone in the room while we perform the
user study. The reason is to facilitate data labelling and to
ensure that we do not monitor data generated by other lab
users. Users logged their start and end times using their
mobile phones in order to label the data for each session.
We held several orientation sessions before the actual data
collection, to help users become familiar with the devices. We
did not provide any scripts and users were asked to interact
with the devices in a natural manner.
In total, we recruited 10 users, who participated in the
study and generated 74 sessions with a duration of 1660
minutes. The usage statistics per user are in Table 4. We
collected 4,082,975 packets, including connections to 316
external destinations and 97 second-level domains. In the
rest of the paper, we focus on six users: 1, 3, 4, 6, 8, and 10
who generated more than 6 sessions.
IRB approval. Our study was approved by the IRB at our
institution. All researchers with access to the collected data
performed the IRB training and users were required to con-
sent to the data being collected while they use the room.
Users were informed of the project goal before participating
in the study. As we only extracted fields from packet headers
of HTTPS traffic, we did not have access to any user per-
sonal information. To further protect the privacy of users, we
anonymize the collected data by creating a unique user ID
for each study participant. Our data is stored in anonymized
format on our servers, not directly identifying the partici-
pants.
Implementation. The lab network is monitored at all times
and the network traffic (pcap files) of all the IoT devices is
collected in a server located in the room. We did not collect
data from users’ personal devices such as laptops and phones,
as we used a MAC address filter to collect only IoT device
The second source of data useful for feature generation
is user communication with external destinations (domain
names). This is motivated by our observation that differ-
ent Alexa skills connect to different external domains. For
instance, listening to music may stream data from the spo-
tify.com domain, whereas playing fireplace sounds uses kwimer.com.
As the set of external domain names is very large and cloud
services use many sub-domains for load balancing, we trans-
form our domain names to second-level domains and com-
pute features per second-level domain. For each second-level
domain, we use aggregated features including statistics on
incoming and outgoing packet lengths, inter-event times and
packet counts for transport protocols, computed over all the
packets exchanged with that external domains in the time
window of size ∆. We denote this set of domain features in
time window [t − ∆, t] as: F(t, ∆)dom.
For our task of user authentication, we will consider sev-
eral feature representations:
(1) Device-only: Estimate and predict P[Ui|F(t, ∆)dev];
(2) Domain-only: Estimate and predict P[Ui|F(t, ∆)dom];
6
Device
Echo Dot - Echo Spot - Echo Plus
Google Home Mini
Harman Kardon Invoke
LG Smart TV
Roku TV
Amazon Fire TV
Philips Hue Bridge
Samsung Smart Fridge
Smart Microwave
iKettle Smart Kettle
Behmor Smart Brewer
SmartThings Hub
User Activities
Alexa voice assistants to support voice commands and control other smart devices
Google voice assistant to support voice commands and control other smart devices
Cortana voice assistant to support voice commands and control other smart devices
Watch TV or stream content through third party applications
Stream content through third party applications
Stream content through third party applications
Set the color and brightness of the bulbs
Use fridge, interact with the LCD Display
Open, Heat up food, Close
Boil water
Brew coffee
Trigger motion and contact sensors
Table 3: IoT Devices in the IoT lab.
User Sessions Total Time Avg. Duration
such as average packet length provide more distinguishing
patterns across users than total number of packets or total
data transferred.
In Figure 3 we display the amount of data transferred, the
total number of packets, and the average packet length by
user and second level domains. We only included domains
with more than 10,000 packets for at least a user. While all
devices communicates with the cloud and CDN domains (e.g.,
cloudfront.com), some domains (such as iheart.com) exhibit
different patterns across users.
Next, we are interested in validating the hypotheses that
users have consistent behavior in multiple sessions and users
have differentiating behavior from other users. For this, we
plot the amount of data transmitted by different devices
over time for User 6 (in Figure 4) and User 10 (in Figure 5)
in three distinct sessions. Interestingly, some sessions of
the same user are similar (for instance, the first and third
session of User 6). However, users sometimes deviate in their
behavior across different sessions, as demonstrated by the
third session of User 10 (in which interaction with Echo Dot
is lower). Overall, Users 6 and 10 have different behavior in
their interactions with smart devices. However, while there
is some similarity across the same user’s sessions, these plots
also show that user behavior varies across sessions, making
behavioral authentication in this context challenging.
5 EVALUATION
We evaluate our system using the data collected in the
user study that is discussed in Section 4.1. We present re-
sults for different feature representations, user classification
performance, and receiver operating characteristic (ROC)
curves for three different classification models and a high-
confidence ensemble model.
7
1
2
3
4
5
6
7
8
9
10
9
5
11
8
2
12
4
9
6
8
184
181
330
206
38
263
71
154
112
121
20
36
30
25
19
21
17
17
18
15
Table 4: The number of sessions, total time, and aver-
age session duration per user, all in minutes.
traffic. We created software that parses the HTTPS packet
headers and stores the fields from Table 1 in a Postgres data-
base. We extracted the device and domain features and stored
them in a different table in our database. For training our
models, we use the ML implementation from the Python
scikit-learn package. We performed cross-validation for all
our experiments and varied the ML hyper-parameters.
4.2 Data exploration.
We analyzed the collected data to compare different users
in terms of device and domain usage. In Figure 2, we show
the total data exchanged by device and user, the number of
packets, as well as the average length of packets. For some
devices such as Amazon Echospot and Google Home, the
amount of data transferred varies significantly by user. On
the other hand, some smart devices (such as the microwave
and smart kettle) are either not utilized by users, or they
do not generate a lot of network activity. Statistical features
Total bytes, packet count, and average packet length are plotted per device from left to
Figure 2: Device usage statistics during the user study.
right.
Figure 3: Domain usage statistics during the user study.
Total bytes, packet count, and average packet length are plotted per domain from left
to right.
For the user classification task, we label the data using
the session start and end times of each user participating in
our user study. As it is usually not possible to train accurate
machine learning models with limited amount of data, we