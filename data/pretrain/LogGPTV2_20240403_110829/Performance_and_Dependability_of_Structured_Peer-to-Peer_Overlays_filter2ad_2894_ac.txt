n
r
e
p
s
e
r
u
l
i
a
f
e
d
o
N
72
96
Time (Hours)
120
144
168
0.0E+00
0
7
14
21
Time (Days)
28
35
Figure 3: Node failure rates for the Gnutella, OverNet and Microsoft traces, respectively
dian is 79 minutes. The number of active nodes varies be-
tween 260 and 650.
The Microsoft trace is derived from an availability study
of machines on the Microsoft corporate network [2]. The
study monitored 65,000 machines by probing each every
hour for 37 days. To reduce simulation times, we picked
20,000 machines randomly from among the 65,000. The
average session time is 37.7 hours and the number of active
nodes varies between 14700 and 15600.
Figure 3 shows the node failure rate for the three traces.
This is averaged over 10 minute windows for OverNet and
Gnutella and over one hour for Microsoft. All traces show
clear daily and weekly patterns and the failure rates vary
signiﬁcantly across the traces. Gnutella and OverNet are
representative of peer-to-peer systems running in an open
Internet environment and they are similar. The failure rate
in the Microsoft trace is an order of magnitude lower and is
representative of a more benign corporate environment.
We also generated artiﬁcial traces with Poisson node ar-
rivals and an exponential distribution of node session times
with the same rates. The average number of nodes in these
traces was 10,000. To investigate the performance and de-
pendability of MSPastry with varying session times, we
used traces with session times of 5, 15, 30, 60, 120 and 600
minutes. Note that most of these session times are signiﬁ-
cantly lower than those observed in real traces.
Network topologies We also evaluated the impact of
varying the network topology. We simulated three differ-
ent topologies: GATech, Mercator, and CorpNet. GATech
is a transit-stub topology generated using the Georgia Tech
topology generator [25]. It has 5050 routers arranged hier-
archically, with 10 transit domains at the top level with an
average of 5 routers in each. Each transit router has an av-
erage of 10 stub domains attached, with an average of 10
routers each. The delay between core routers is computed
by the topology generator and routing is performed using
the routing policy weights of the graph generator. The sim-
ulator uses the round-trip delay (RTT) between two nodes
as the proximity metric. End nodes running MSPastry are
attached to randomly selected stub routers by a LAN link
with a delay of 1ms.
Mercator has 102,639 routers grouped into autonomous
systems (AS) and is based on real data [24]. The AS over-
lay has 2,662 AS and routing is performed hierarchically
as in the Internet. A route follows the shortest path in the
AS overlay between the AS of the source and the AS of the
destination. The routes within each AS follow the shortest
path to a router in the next AS of the AS overlay path. Since
there is no delay information in the Mercator topology, the
simulator uses the number of network-level (IP) hops be-
tween two nodes as the proximity metric. Each end node
was directly attached to a randomly chosen router.
CorpNet is a topology with 298 routers generated from
measurements of the world-wide Microsoft corporate net-
work. The simulator uses the minimum RTT as the proxim-
ity metric. Each end node was directly attached by a LAN
link with a delay of 1ms to a randomly chosen router.
The simulator can be conﬁgured with a uniform proba-
bility of network message loss but it does not model con-
gestion delays and losses.
Base conﬁguration The base MSPastry conﬁguration
uses b = 4, l = 32, Tls = 30 seconds, per-hop acks, rout-
ing table probing tuned with Lr = 5%, probe suppression,
and symmetrical distance probes. Each active node gener-
ates 0.01 lookup messages per second according to a Pois-
son process with destination keys chosen uniformly at ran-
dom from the identiﬁer space. This conﬁguration provides
a good balance between performance and overhead and it is
highly dependable as the results will show.
Unless otherwise stated, the simulator was conﬁgured
with a loss rate of 0% with the GATech network topology
and the experiments ran the Gnutella trace.
5.2. Evaluation metrics
We measure dependability using two metrics: the incor-
rect delivery rate and the loss rate. The ﬁrst metric is the
fraction of lookup messages that are delivered to an incor-
rect node, and the second is the fraction of lookup messages
which are never delivered to any node. We observed an in-
correct delivery rate of zero in all the experiments without
network losses as expected.
Performance is also measured using two metrics: relative
delay penalty (RDP) and control trafﬁc. RDP is the average
ratio between the delay achieved by MSPastry when rout-
ing between two nodes and the network delay between the
same nodes. Control trafﬁc is the average number of control
messages sent per second per node. This includes all traf-
ﬁc except lookup messages. For the Gnutella and OverNet
traces, the metrics are averaged over a 10 minute window.
In the Microsoft trace, this window is 1 hour.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:47:52 UTC from IEEE Xplore.  Restrictions apply. 
5.3. Experimental results
The ﬁrst set of experiments evaluates the impact of envi-
ronmental parameters on the performance and dependabil-
ity of MSPastry.
Network topology The fraction of lookup messages lost
by MSPastry averaged over the whole Gnutella trace was
below 1.6 × 10−5 for all three topologies and there were no
routing inconsistencies. The control trafﬁc was mostly inde-
pendent of the underlying network topology as expected: it
was 0.239 messages per second per node for CorpNet, 0.245
for GATech, and 0.256 for Mercator. The RDP varies signif-
icantly with the network topology. We obtained an RDP of
1.45 for CorpNet, 1.80 for GATech, and 2.12 for Mercator.
There is an explanation for the different RDP values with
the different topologies in [5].
Failure Traces Figure 4 shows RDP and control traf-
ﬁc for the different traces with normalized time. The graph
on the center shows the ﬂuctuation in control trafﬁc that
follows the daily and weekly variations in node arrival and
failure rates. The graph on the right, which breaks down
control messages by type for the Gnutella trace, shows that
the ﬂuctuations are due predominantly to increased distance
probes with increased arrival rate and to self-tuning of ac-
tive probing periods with changing failure rate. Self-tuning
ensures that the RDP remains approximately constant de-
spite the changing node arrival and failure rates as shown in
the graph on the left.
OverNet and Gnutella have similar failure and arrival
rates and, therefore, they have a similar amount of control
trafﬁc. The control trafﬁc is approximately 3 times lower in
the Microsoft trace because the failure and arrival rate is an
order of magnitude lower. The RDP in the Microsoft trace is
also lower than in the other traces; the failure detection pro-
vided by the lookup trafﬁc with acks is sufﬁcient to achieve
an Lr lower than 5% because of the low failure rate, con-
sequently, the delay penalty due to faulty nodes along the
route also decreases.
The left and center graphs in Figure 5 show the RDP and
control trafﬁc averaged over the whole trace for the Poisson
traces with different session times. The control trafﬁc in-
creases signiﬁcantly as the average session time drops. The
control trafﬁc is 22 times higher when the average session
time is 15 minutes than when it is 600. The control trafﬁc
drops when the session time decreases to 5 minutes because
7% of the nodes die before they become active due to in-
creased failure rate.
Self-tuning maintains the RDP fairly constant when ses-
sion times are one hour or more. The RDP increases signif-
icantly with 5 minute session times because Tls = 30s and
Trt > 9s; achieving the desired Lr of 5% would require
smaller periods. The RDP increases by only 40% when
the session time decreases from 600 to 15 minutes, which
shows that MSPastry can achieve low delays even when the
failure rate is unrealistically high.
The graph on the right of Figure 5 shows a cumulative
distribution function of the join latency for two traces. The
join latency is the time from the moment a node initiates
the join till it becomes active. It shows that nodes join the
overlay quickly.
Network loss rate Figure 6 shows the impact of vary-
ing the network loss rate between 0 and 5%. A network loss
rate of 5% is high in wired networks. The graph on the right
shows that MSPastry achieves consistent and reliable rout-
ing with high probability even with high network loss rates.
We did not observe routing inconsistencies with rates of 1%
or less and even with 5% the fraction of incorrect deliveries
is only 1.6× 10−5. The use of per-hop acks ensures reliable
routing; the fraction of lost lookups varies from 1.5 × 10−5
with no network losses to 3.3 × 10−5 with 5% losses.
The graphs on the left and center show that the RDP and
control trafﬁc increase slightly as the network loss rate in-
creases. The RDP increases because there is an increased
number of per-hop timeouts and retransmissions due to net-
work losses. The delay remains low because of MSPastry’s
aggressive retransmission strategy. The increase in control
trafﬁc is mostly due to the additional probes to check if
nodes are alive after message losses.
Parameters: l and b We ran experiments to evaluate the
impact of varying the algorithm parameters l and b. The left
graph of Figure 7 shows the effect of varying the leaf set
size on control trafﬁc. Increasing l from 16 to 32 increases
control trafﬁc by only 7%. The variation is small because
MSPastry exploits overlay structure; nodes only send heart-
beats to their left neighbour. So the overhead of sending
heartbeats is independent of l and it is the dominant cost
of leaf set maintenance in the Gnutella trace. This enables
using large leaf sets with low overhead. Larger leaf sets re-
duce the average number of hops and, therefore, the RDP as
shown in the graph on the center. So we chose l = 32.
2b
The graph on the right of Figure 7 shows the impact of
varying b on RDP. The RDP increases signiﬁcantly when b
decreases because of the increased number of hops; the ex-
pected number of hops in an overlay route is 2b−1
log2bN .
Decreasing b reduces control trafﬁc because there is less
routing table state to maintain but this is offset by an in-
crease in the number of per-hop acks because the number
of hops increases, and by an increase in the routing table
probing trafﬁc to achieve the target Lr = 5% also because
the number of hops increases. As a result, the control trafﬁc
only decreases by 0.05 messages per second per node when
b drops from 4 to 1. Therefore, we chose b = 4.
Active probing and per-hop acks We ran experiments
to evaluate the impact of active routing table probing and
per-hop acks on reliability, delay, and control trafﬁc. Relia-
bility is poor without active probes and per-hop acks: 32%
of the lookup messages are never delivered. The loss rate
drops to 2.8 × 10−5 using only per-hop acks and it drops
to 1.6 × 10−5 with active probing and per-hop acks. Using
only active probing, it is not possible to achieve a loss rate
on the order of 10−5 because of constraints on the minimum
probing period.
Using only per-hops acks, results in high delay if the ap-
plication trafﬁc is low. The RDP achieved using only per-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:47:52 UTC from IEEE Xplore.  Restrictions apply. 
2.5
2
1.5
1
P
D
R
0.2
0.5
0
0
P
D
R
4.5
4
3.5
3
2.5
2
1.5
OverNet
Gnutella
Microsoft
e
d
o
n
r
e
p
d
n
o
c
e
s
r
e
p
s
e
g
a
s
s
e
M
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
OverNet
Gnutella
Microsoft
0.4
0.6
Time (Normalized)
0.8
1
0
0.2
0.4
0.6
Time (Normalized)
0.8
1
Distance Probes
Leafset Heartbeats/Probes
RT Probes
Acks + Retransmits
Join
0.3
0.25
0.2
0.15
0.1
0.05
e
d
o
n
r
e
p
d
n
o
c
e
s
r
e
p
s
e
g
a
s
s
e
M
0
0
10
20
30
Time (Hours)
40
50
60
Figure 4: RDP and control trafﬁc for the different real-world traces.
)
e
d
o
n
/
c
e
s
/
s
g
m
(
c
i
f
f
a
r
t
l
o
r
t
n
o
C
4
3.5
3
2.5
2
1.5
1
0.5
0
e
d
o