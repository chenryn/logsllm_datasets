unfairness, as a function of the number of ECN ﬂows, given
a total of 10 ﬂows. It illustrates how for all tested parame-
ter sets, introducing even a small number of ECN ﬂows into
the mix breaks fairness between ECN and non-ECN ﬂows.
Moreover, when there is only one non-ECN ﬂow left out of
the 10 ﬂows, its goodput is at most 45% of the goodput of
the ECN ﬂows.
Figure 7 explores how modifying the REDmin param-
eter in the RED1 parameter set affects fairness. We set
REDmax = REDmin + 1, and set REDburst to the mini-
mum allowed by tc-red. The ﬁgure depicts the goodput ra-
tio for different values of REDmin. In general we see that
as the proportion of ECN ﬂows in the mix increases and
Figure 8: Histogram of time between consecutive ac-
knowledgments sent by the receiver, divided into 100 ms
bins, given a single non-ECN ﬂow competing with 9 ECN
ﬂows on a 100 Mbps link. A representative ECN ﬂow is
plotted along with the non-ECN ﬂow.
as REDmin decreases, the unfairness worsens and the non-
ECN ﬂows suffer from increasing goodput loss.
What is causing this unfairness? Figure 8 presents a
100 ms binned histogram of the time between consecutive
acknowledgments sent by the receiver to a non-ECN and to
an ECN ﬂow, where the non-ECN ﬂow is competing with 9
ECN ﬂows on a 100 Mbps link using RED1 parameters. The
non-ECN ﬂow suffers from repeated retransmission time-
outs, as seen by the 200 ms and the 600 ms latencies. We
found two dominant factors for these repeated timeouts:
Queue length averaging. Consider a state in which the av-
erage queue length measured by the switch grows beyond
REDmax. It may remain above REDmax for a few RTTs
due to the moving exponential averaging of the queue length.
Meanwhile, every incoming packet of the non-ECN ﬂow is
discarded, causing the sender to time out waiting for ACKs
on the dropped packets. Note that in this scenario fast re-
transmit is often not sufﬁcient to save the sender’s window,
because the fast-retransmitted packets are dropped as well.
After such a timeout, the non-ECN sender returns to slow-
start, which further decreases its ability to recover due to the
small number of duplicate ACKs at its disposal in a case
of additional drops.
In contrast, the packets of an ECN-
capable sender are marked and not dropped. Upon receipt
of an ACK-marked ECE, the sender halves its window and
continues in congestion avoidance, without losing a packet
or experiencing a timeout.
ECN to non-ECN ﬂows ratio. Why does the unfairness
to non-ECN ﬂows become more severe as the proportion of
ECN ﬂows increases? Assume the switch buffer becomes
congested, i.e., crosses the marking threshold beyond which
ECN packets are marked and non-ECN packets are dropped.
Then packets from the ECN ﬂows continue to enter the
Figure 9: Send window time series for a virtual-ECN
ﬂow.
buffer for at least an RTT, potentially keeping the buffer con-
gested. As a result, the switch may drop long sequences of
non-ECN packets, causing timeouts in non-ECN ﬂows. This
effect is particularly pronounced with a higher proportion of
ECN ﬂows, which typically leads to a higher ECN trafﬁc
rate. As a result, it will take longer to drain the queue below
the marking threshold as more ECN trafﬁc keeps arriving,
and therefore may cause a longer congestion period.
3.2 Receive-Window Throttling
In order to address this unfairness problem, we propose
using the vCC translation layer to provide ECN capabili-
ties to the non-ECN ﬂows. We transform non-ECN ﬂows
from a guest to virtual-ECN ﬂows that take advantage of
ECN, using receive-window throttling in the vCC transla-
tion layer. To demonstrate this, we conﬁgure one sender to
send trafﬁc through a switch to a receiver. The sender uses
virtual-ECN provided by the vCC translation layer (wherein
the ECE bits are hidden from the guest to simulate ECN-
ignorance). The switch is conﬁgured with the RED1 pa-
rameter set from Table 1. The sender-to-switch link has a
bandwidth of 12 Mbps, while the switch-to-receiver link has
a bandwidth of 10 Mbps. The delay of each link is 250 µs
(i.e., RTT = 1 ms). The system is given 5 seconds to stabilize
before data is collected for 12 seconds.
Figure 9 depicts the send window for the vCC experiment
as reported by the tcp_probe kernel module. We can observe
the familiar sawtooth pattern that would otherwise be seen in
the congestion window. In our Linux implementation, when
the receive window was the limiting window, the congestion
window stayed larger than the receive window for the entire
experiment, rendering the congestion window meaningless.
Thus, modulating the receive window modulates the send
window of the guest directly, and the resulting trafﬁc ﬂows
are very similar. We have therefore created a virtual-ECN
ﬂow.
To demonstrate that indeed we get the ECN beneﬁt of
(a) 10 non-ECN ﬂows
(b) 10 ECN ﬂows
(c) 10 virtual-ECN ﬂows
Figure 10: Total retransmission throughput for (a) 10 concurrent non-ECN ﬂows sharing a 10 Mbps link, compared to
the same experiment with (b) 10 concurrent ECN ﬂows, and (c) 10 concurrent virtual-ECN ﬂows.
(a) 9 ECN ﬂows and one non-ECN ﬂow
(b) 9 ECN ﬂows with one virtual-ECN ﬂow
Figure 11: 9 ECN ﬂows share a 10Mbps bottleneck with either (a) one non-ECN ﬂow; or (b) one virtual-ECN ﬂow.
(a) The non-ECN ﬂow goodput is only 14.2% of the average goodput of the ECN ﬂows. The fairness index is 0.921.
(b) When virtual-ECN is used, the average goodput of the virtual-ECN ﬂow is 103.8% of the average ECN ﬂow goodput,
and the fairness index is 0.994.
reduced retransmissions when using virtual-ECN, we run
an experiment with 10 identical senders connected with
10Mbps links to a single receiver through a single switch.
Figure 10(a) illustrates that when using only non-ECN
ﬂows, some 2.3% of the link capacity is wasted on retrans-
missions due to packets dropped in the congested queue at
the port connecting the switch to the receiver. However, as
shown in Figure 10(c), once virtual-ECN is activated, the
lost capacity is regained as virtual-ECN can react to conges-
tion without dropping packets and retransmitting them (ex-
actly like ECN’s behavior in Figure 10(b)).
3.3 Restoring Fairness with virtual-ECN
vCC offers the ability to transform a non-ECN ﬂow into a
virtual-ECN ﬂow. We now evaluate whether this is sufﬁcient
to address the unfairness discussed in Section 3.1.
Figure 11(a) plots the goodput achieved with 9 ECN ﬂows
and one non-ECN ﬂow sharing a 10 Mbps bottleneck link.
It shows again how the non-ECN ﬂow suffers from strong
unfairness.
Figure 11(b) shows the goodput achieved in the same
setting, except that the non-ECN ﬂow has been replaced
with virtual-ECN. The resulting goodput of the ﬂow from
the ECN-incapable guest is now similar to that of its ECN-
capable peers, with goodput 103.8% of the average goodput
of the ECN-capable ﬂows.
To summarize, the translation layer uses receive-window
throttling to cause the guest that does not support ECN to
mimic its ECN peers, signiﬁcantly improving its own good-
put and the fairness of the network.
4. EVALUATION:
HYPERVISOR
BANDWIDTH SHARING
In this section, we describe a proof-of-concept vCC trans-
lation layer, which we implement on the VMware vSphere
ESXi 6.0 hypervisor. We later illustrate how it can be used
to provide bandwidth sharing.
The vCC translation layer is implemented as a ﬁlter called
DVFilter [38] in the hypervisor’s vSwitch. All per-ﬂow
states necessary for translation are stored in the hypervisor’s
own memory. The translation layer monitors ﬂows pass-
ing through the switch, and inspects the headers in order to
maintain correct state information about the ﬂow (e.g., the
current srtt, or the number of packets in ﬂight). When the
vCC translation layer determines it should modify headers,
it changes the packet headers, recomputes the checksum, and
allows the packet to pass through the ﬁlter. In particular, in
this section, we demonstrate how we implemented receive
window throttling in this vCC layer.
Consider a multi-tenant datacenter. Each virtual machine
may be the source of many TCP ﬂows. However, not all of
these ﬂows should necessarily be treated the same for opti-
mal performance. For example, some may be short but time-
sensitive, while others are long but elastic. Thus, it can be
useful to limit the rate at which certain applications are able
to send. More generally, the ability to enforce tenant-based
dynamic bandwidth allocations down to the granularity of
applications is important to meet performance and SLA tar-
gets. WAN trafﬁc shaping using a local Linux bandwidth en-
forcer is a promising approach [39]. This requires a uniform
OS installation that does not generally allow multi-tenant
hosting. Bandwidth limiting is available at guest granularity
in some modern hypervisors (such as Microsoft’s Hyper-V
and VMware’s ESXi), but per-application throttling is gen-
erally not. Moreover, to throttle bandwidth, these techniques
can rely on either dropping packets or building large queues,
which can have a detrimental effect on ﬂow performance and
latency.
Here we show another application of the receive-window
throttling abilities of vCC. By controlling the end-to-end
number of in-ﬂight packets, vCC provides a ﬁne-grained,
datacenter-wide coordination of bandwidth allocation. The
hypervisor detects the signature of a tenant, port or packet,
and restricts the bandwidth used by this particular set of traf-
ﬁc. In addition, the bandwidth limit can be changed dynam-
ically, depending on signals from the network or from the
guest.
Our hypervisor
implementation provides a proof-of-
concept
for dynamic application-graunlarity bandwidth
throttling. In this experiment, the vCC-enabled hypervisor is
nested on another ESXi running on a Dell Poweredge T610
server, with 12 GB of RAM and two Intel Xeon processors
at 2.4 GHz. Two guest VMs (Linux Centos 6.4) are hosted
on top of the hypervisor, with the vCC translation layer in-
stalled in its vSwitch. They communicate through that hy-
pervisor’s vSwitch. One guest runs an iPerf server on 5 TCP
ports. We divide ﬂows into preferred and unpreferred ﬂows.
The preference can be seen as reﬂecting time-sensitive or
higher-paying tenants, for example. Three ports are given
to unpreferred ﬂows, and two to preferred ﬂows. The total
amount of window space, i.e., the sum of the RWINs of all
active ﬂows, remains constant at all times. The translation
layer is conﬁgured to evenly divide the available window
Figure 12: Stacked throughputs for three unpreferred
and two preferred ﬂows. The ﬂows are receive-window
throttled by the ESXi vCC layer. The sum of the windows
of all the live ﬂows is kept constant throughout the ex-
periment, but the vCC throttles unpreferred ﬂows once
preferred ﬂows start in order to give the preferred ﬂows
greater bandwidth. The vCC layer uses the port number
to differentiate between ﬂows and preferences.
space among unpreferred ﬂows in the absence of preferred
ones. When it detects active in-ﬂight preferred ﬂows, the
translation layer dynamically changes the window space al-
location to proportionally assign more window space to pre-
ferred ﬂows (3 times as much per preferred ﬂow as per un-
preferred ﬂow), and divides the remainder among the unpre-
ferred ﬂows evenly.
Figure 12 illustrates a time series of this experiment. It
shows that after the introduction of the preferred ﬂows, the
throughput of unpreferred ﬂows drops due to receive win-
dow throttling, thus providing the preferred ﬂows a larger
share of the bandwidth. The total throughput before and
after the introduction of preferred ﬂows remains relatively
constant.
5.
IMPLEMENTATION DISCUSSION
In this section, we discuss the architectural issues that a
vCC implementation needs to address in the hypervisor.
Architectural complexity. Many hypervisor switches sup-
port an architecture where an independent module can in-
spect, modify, and re-inject the packet (e.g., Microsoft’s
Hyper-V Extensible Switch extension [40], VMware’s ESXi
DVFilter [38], and so on). This architecture is typically used
by ﬁrewalls and security modules. For instance, a ﬁrewall
implementation may allow over 1 million active connections
with a per-connection state of under 1KB, given a total mem-
ory size of about 1GB.
A vCC implementation can leverage this architecture in
order to modify packets, as we illustrate in our ESXi imple-
mentation. We would expect similar numbers given our sim-
plest techniques without buffering. For instance, our Linux
vCC implementation stores only 37 bytes per ﬂow. This
leaves room for a more complex implementation, given a
per-connection footprint budget under 1KB. In addition, in
most of the techniques mentioned in Section 2, the main
CPU load consists of keeping track of the per-connection
states of the guest congestion control algorithm.
Hypervisor delay. Processing delays in the hypervisor can
increase the latency, and therefore the ﬂow completion time,
as well as affect the RTT estimation in the guest TCP algo-
rithm. This effect would be more pronounced when the load
of the hypervisor CPU is sufﬁciently high to cause context
switches. In such a case, the delay would be on the order of
context switching delays, i.e., several µs.
Hypervisor bypass. High-performance virtualized work-
loads can beneﬁt from bypassing the hypervisor and directly
accessing the network interface card (NIC), using technolo-
gies such as SR-IOV [41–44]. vCC would not work in such
architectures. However, hypervisor bypass is typically used
in high-end devices with the newest OSes. Such OSes of-
ten already implement the latest congestion control, if only
to obtain the best available performance. In addition, future
NICs could also implement vCC, although (a) software up-
dates would not be as easy as for hypervisors, and (b) NICs
may not have access to the more intrusive techniques such as
guest introspection. The same would be true if servers had
FPGAs or middleboxes.
TSO and LRO. TCP Segmentation Ofﬂoad (TSO) and
Large Receive Ofﬂoad (LRO) are techniques for increasing
the throughput of high-bandwidth network connections by
reducing CPU overhead. TSO transfers large packet buffers
to the NIC and lets it split them, while LRO does the reverse
operation. The hypervisor needs to modify the vCC trans-
lation layer accordingly. Most techniques remain nearly un-
changed. However, techniques that rely on packet buffering
will need much larger buffers, and, if vCC wishes to retrans-
mit TCP segments, it will also need to recreate individual
segments.
Conﬁguration. In the vCC architecture, the network admin-
istrator can assign a different congestion control to different
ports, IP addresses, applications, OSes, or tenants. For in-
stance, long-term background ﬂows may have a less aggres-
sive congestion control than short urgent ﬂows, or a propri-
etary congestion control can be restricted to intra-datacenter
connections. Of course, a major disadvantage of modulating
the congestion control is that several congestion control al-
gorithms will coexist again in the datacenter. Note that it is
easy to conﬁgure vCC to not modify certain trafﬁc. Future
work could include automatic detection of ﬂows that need
translation, reducing the need for administrator conﬁgura-
tion.
Delay-based congestion control. We believe vCC can
translate to/from delay-based TCP algorithms like TCP Ve-
gas and TIMELY [8]. To do so, it would need to use the
more heavyweight techniques at its disposal, such as split
connections and buffers.
UDP. This paper focuses on TCP, and therefore we would
expect the hypervisor to let UDP trafﬁc go through the trans-
lation layer in a transparent manner. Of course, we could
generalize the same translation idea to UDP, and for instance
make the translation layer translate UDP to a proprietary re-
liable UDP algorithm, at the cost of additional buffering and
complexity.
Universal language. In order to directly translate between n
congestion control algorithms, we would theoretically need
to implement O(n2) translations. Instead, we could envision
a universal atomic congestion control protocol enabling us to
implement only 2n translation to/from this protocol.
Encryption: Our analysis suggests that the vCC architecture
can similarly be used to offer encryption services, such as
TCPCrypt and IPSec [45, 46], to legacy unencrypted TCP
ﬂows. If the guest is already encrypting communications,
vCC would need to access session keys in order to operate,
for instance by reading guest memory.
Debugging. Adding packet-processing modules at the con-