37.29
65.94
147.66
2.15
4.55
7.91
17.44
1516    28th USENIX Security Symposium
USENIX Association
Table 13: Evaluated network architectures.
1
2
3
1
2
3
1
BM1
FC [input: 784, output: 128s] + BN + BA
FC [input: 128s, output: 128s] + BN + BA
FC [input: 128s, output: 10] + BN + Softmax
BM2
CONV [input: 28 × 28 × 1, window: 5 × 5, stride: 2, kernels: 5s,
output: 12 × 12 × 5s] + BN + BA
FC [input: 720s, output: 100s] + BN + BA
FC [input: 100s, output: 10] + BN + Softmax
CONV [input: 28 × 28 × 1, window: 5 × 5, stride: 1, kernels: 16s,
output: 24 × 24 × 16s] + BN + BA
BM3
2 MP [input: 24 × 24 × 16s, window: 2 × 2, output: 12 × 12 × 16s]
3
CONV [input: 12 × 12 × 16s, window: 5 × 5, stride: 1, kernels: 16s,
output: 8 × 8 × 16s] + BN + BA
4 MP [input: 8 × 8 × 16s, window: 2 × 2, output: 4 × 4 × 16s]
5
6
FC [input: 256s, output: 100s] + BN + BA
FC [input: 100s, output: 10] + BN + Softmax
BC1
1
2
CONV [input: 32 × 32 × 3, window: 3 × 3, stride: 1, kernels: 64s,
output: 30 × 30 × 64s] + BN + BA
CONV [input: 30 × 30 × 64s, window: 3 × 3, stride: 1, kernels: 64s,
output: 28 × 28 × 64s] + BN + BA
3 MP [input: 28 × 28 × 64s, window: 2 × 2, output: 14 × 14 × 64s]
4
CONV [input: 14 × 14 × 64s, window: 3 × 3, stride: 1, kernels: 64s,
output: 12 × 12 × 64s] + BN + BA
CONV [input: 12 × 12 × 64s, window: 3 × 3, stride: 1, kernels: 64s,
output: 10 × 10 × 64s] + BN + BA
5
6 MP [input: 10 × 10 × 64s, window: 2 × 2, output: 5 × 5 × 64s]
7
CONV [input: 5 × 5 × 64s, window: 3 × 3, stride: 1, kernels: 64s,
output: 3 × 3 × 64s] + BN + BA
CONV [input: 3 × 3 × 64s, window: 1 × 1, stride: 1, kernels: 64s,
output: 3 × 3 × 64s] + BN + BA
CONV [input: 3 × 3 × 64s, window: 1 × 1, stride: 1, kernels: 16s,
output: 3 × 3 × 16s] + BN + BA
8
9
10 FC [input: 144s, output: 10] + BN + Softmax
BC2
1
2
3
CONV [input: 32 × 32 × 3, window: 3 × 3, stride: 1, kernels: 16s,
output: 32 × 32 × 16s] + BN + BA
CONV [input: 32 × 32 × 16s, window: 3 × 3, stride: 1, kernels: 16s,
output: 32 × 32 × 16s] + BN + BA
CONV [input: 32 × 32 × 16s, window: 3 × 3, stride: 1, kernels: 16s,
output: 32 × 32 × 16s] + BN + BA
4 MP [input: 32 × 32 × 16s, window: 2 × 2, output: 16 × 16 × 16s]
5
CONV [input: 16 × 16 × 16s, window: 3 × 3, stride: 1, kernels: 32s,
output: 16 × 16 × 32s] + BN + BA
CONV [input: 16 × 16 × 32s, window: 3 × 3, stride: 1, kernels: 32s,
output: 16 × 16 × 32s] + BN + BA
CONV [input: 16 × 16 × 32s, window: 3 × 3, stride: 1, kernels: 32s,
output: 16 × 16 × 32s] + BN + BA
6
7
8 MP [input: 16 × 16 × 32s, window: 2 × 2, output: 8 × 8 × 32s]
9
CONV [input: 8 × 8 × 32s, window: 3 × 3, stride: 1, kernels: 48s,
output: 6 × 6 × 48s] + BN + BA
10 CONV [input: 6 × 6 × 48s, window: 3 × 3, stride: 1, kernels: 48s,
output: 4 × 4 × 48s] + BN + BA
11 CONV [input: 4 × 4 × 48s, window: 3 × 3, stride: 1, kernels: 64s,
output: 2 × 2 × 64s] + BN + BA
12 MP [input: 2 × 2 × 64s, window: 2 × 2, output: 1 × 1 × 64s]
13 FC [input: 64s, output: 10] + BN + Softmax
BC3
1
2
3
CONV [input: 32 × 32 × 3, window: 3 × 3, stride: 1, kernels: 16s,
output: 32 × 32 × 16s] + BN + BA
CONV [input: 32 × 32 × 16s, window: 3 × 3, stride: 1, kernels: 32s,
output: 32 × 32 × 32s] + BN + BA
CONV [input: 32 × 32 × 32s, window: 3 × 3, stride: 1, kernels: 32s,
output: 32 × 32 × 32s] + BN + BA
4 MP [input: 32 × 32 × 32s, window: 2 × 2, output: 16 × 16 × 32s]
5
CONV [input: 16 × 16 × 32s, window: 3 × 3, stride: 1, kernels: 48s,
output: 16 × 16 × 48s] + BN + BA
CONV [input: 16 × 16 × 48s, window: 3 × 3, stride: 1, kernels: 64s,
output: 16 × 16 × 64s] + BN + BA
CONV [input: 16 × 16 × 64s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
6
7
8 MP [input: 16 × 16 × 80s, window: 2 × 2, output: 8 × 8 × 80s]
9
CONV [input: 8 × 8 × 80s, window: 3 × 3, stride: 1, kernels: 96s,
output: 6 × 6 × 96s] + BN + BA
10 CONV [input: 6 × 6 × 96s, window: 3 × 3, stride: 1, kernels: 96s,
output: 4 × 4 × 96s] + BN + BA
11 CONV [input: 4 × 4 × 96s, window: 3 × 3, stride: 1, kernels: 128s,
output: 2 × 2 × 128s] + BN + BA
12 MP [input: 2 × 2 × 128s, window: 2 × 2, output: 1 × 1 × 128s]
13 FC [input: 128s, output: 10] + BN + Softmax
BC4
1
2
3
4
CONV [input: 32 × 32 × 3, window: 3 × 3, stride: 1, kernels: 32s,
output: 32 × 32 × 32s] + BN + BA
CONV [input: 32 × 32 × 32s, window: 3 × 3, stride: 1, kernels: 48s,
output: 32 × 32 × 48s] + BN + BA
CONV [input: 32 × 32 × 48s, window: 3 × 3, stride: 1, kernels: 64s,
output: 32 × 32 × 64s] + BN + BA
CONV [input: 32 × 32 × 64s, window: 3 × 3, stride: 1, kernels: 64s,
output: 32 × 32 × 64s] + BN + BA
5 MP [input: 32 × 32 × 64s, window: 2 × 2, output: 16 × 16 × 64s]
6
CONV [input: 16 × 16 × 64s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
7
8
9
10 MP [input: 16 × 16 × 80s, window: 2 × 2, output: 8 × 8 × 80s]
11 CONV [input: 8 × 8 × 80s, window: 3 × 3, stride: 1, kernels: 128s,
output: 6 × 6 × 128s] + BN + BA
12 CONV [input: 6 × 6 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 4 × 4 × 128s] + BN + BA
13 CONV [input: 4 × 4 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 2 × 2 × 128s] + BN + BA
14 MP [input: 2 × 2 × 128s, window: 2 × 2, output: 1 × 1 × 128s]
15 FC [input: 128s, output: 10] + BN + Softmax
BC5
1
2
3
4
5
CONV [input: 32 × 32 × 3, window: 3 × 3, stride: 1, kernels: 32s,
output: 32 × 32 × 32s] + BN + BA
CONV [input: 32 × 32 × 32s, window: 3 × 3, stride: 1, kernels: 32s,
output: 32 × 32 × 32s] + BN + BA
CONV [input: 32 × 32 × 32s, window: 3 × 3, stride: 1, kernels: 32s,
output: 32 × 32 × 32s] + BN + BA
CONV [input: 32 × 32 × 32s, window: 3 × 3, stride: 1, kernels: 48s,
output: 32 × 32 × 48s] + BN + BA
CONV [input: 32 × 32 × 48s, window: 3 × 3, stride: 1, kernels: 48s,
output: 32 × 32 × 48s] + BN + BA
6 MP [input: 32 × 32 × 48s, window: 2 × 2, output: 16 × 16 × 48s]
7
CONV [input: 16 × 16 × 48s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
8
9
10 CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
11 CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
12 CONV [input: 16 × 16 × 80s, window: 3 × 3, stride: 1, kernels: 80s,
output: 16 × 16 × 80s] + BN + BA
13 MP [input: 16 × 16 × 80s, window: 2 × 2, output: 8 × 8 × 80s]
14 CONV [input: 8 × 8 × 80s, window: 3 × 3, stride: 1, kernels: 128s,
output: 8 × 8 × 128s] + BN + BA
15 CONV [input: 8 × 8 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 8 × 8 × 128s] + BN + BA
16 CONV [input: 8 × 8 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 8 × 8 × 128s] + BN + BA
17 CONV [input: 8 × 8 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 6 × 6 × 128s] + BN + BA
18 CONV [input: 6 × 6 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 4 × 4 × 128s] + BN + BA
19 CONV [input: 4 × 4 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 2 × 2 × 128s] + BN + BA
20 MP [input: 2 × 2 × 128s, window: 2 × 2, output: 1 × 1 × 128s]
21 FC [input: 128s, output: 10] + BN + Softmax
BC6
1
2
CONV [input: 32 × 32 × 3, window: 3 × 3, stride: 1, kernels: 16s,
output: 32 × 32 × 16s] + BN + BA
CONV [input: 32 × 32 × 16s, window: 3 × 3, stride: 1, kernels: 16s,
output: 32 × 32 × 16s] + BN + BA
3 MP [input: 32 × 32 × 16s, window: 2 × 2, output: 16 × 16 × 16s]
4
CONV [input: 16 × 16 × 16s, window: 3 × 3, stride: 1, kernels: 32s,
output: 16 × 16 × 32s] + BN + BA
CONV [input: 16 × 16 × 32s, window: 3 × 3, stride: 1, kernels: 32s,
output: 16 × 16 × 32s] + BN + BA
5
6 MP [input: 16 × 16 × 32s, window: 2 × 2, output: 8 × 8 × 32s]
7
CONV [input: 8 × 8 × 32s, window: 3 × 3, stride: 1, kernels: 64s,
output: 8 × 8 × 64s] + BN + BA
CONV [input: 8 × 8 × 64s, window: 3 × 3, stride: 1, kernels: 64s,
output: 8 × 8 × 64s] + BN + BA
CONV [input: 8 × 8 × 64s, window: 3 × 3, stride: 1, kernels: 64s,
output: 8 × 8 × 64s] + BN + BA
8
9
10 MP [input: 8 × 8 × 64s, window: 2 × 2, output: 4 × 4 × 64s]
11 CONV [input: 4 × 4 × 64s, window: 3 × 3, stride: 1, kernels: 128s,
output: 4 × 4 × 128s] + BN + BA
12 CONV [input: 4 × 4 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 4 × 4 × 128s] + BN + BA
13 CONV [input: 4 × 4 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 4 × 4 × 128s] + BN + BA
14 MP [input: 4 × 4 × 128s, window: 2 × 2, output: 2 × 2 × 128s]
15 CONV [input: 2 × 2 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 2 × 2 × 128s] + BN + BA
16 CONV [input: 2 × 2 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 2 × 2 × 128s] + BN + BA
17 CONV [input: 2 × 2 × 128s, window: 3 × 3, stride: 1, kernels: 128s,
output: 2 × 2 × 128s] + BN + BA
18 MP [input: 2 × 2 × 128s, window: 2 × 2, output: 1 × 1 × 128s]
19 FC [input: 128s, output: 512s] + BN + BA
20 FC [input: 512s, output: 512s] + BN + BA
21 FC [input: 512s, output: 10] + BN + Softmax
1
2
3
1
2
3
1
2
3
1
BH1
FC [input: 30, output: 16] + BN + BA
FC [input: 16, output: 16] + BN + BA
FC [input: 16, output: 2] + BN + Softmax
BH2
FC [input: 8, output: 20] + BN + BA
FC [input: 20, output: 20] + BN + BA
FC [input: 20, output: 2] + BN + Softmax
BH3
FC [input: 10, output: 32] + BN + BA
FC [input: 32, output: 32] + BN + BA
FC [input: 32, output: 2] + BN + Softmax
BH4
CONV [input: 32 × 32 × 3, window: 5 × 5, stride: 1, kernels: 36,
output: 28 × 28 × 36] + BN + BA
2 MP [input: 28 × 28 × 36, window: 2 × 2, output: 14 × 14 × 36]
3
CONV [input: 14 × 14 × 36, window: 5 × 5, stride: 1, kernels: 36,
output: 10 × 10 × 36] + BN + BA
4 MP [input: 10 × 10 × 36, window: 2 × 2, output: 5 × 5 × 36]
5
6
FC [input: 900, output: 72] + BN + BA
FC [input: 72, output: 2] + BN + Softmax
USENIX Association
28th USENIX Security Symposium    1517
B Attacks on Deep Neural Networks
of a class that has the highest conﬁdence score.
1. Rounding the conﬁdence values: Rounding the values
simply means omitting one (or more) of the Least Sig-
niﬁcant Bit (LSB) of all of the numbers in the last layer.
This operation is in fact free in GC since it means Garbler
has to avoid providing the mapping for those LSBs.
2. Reporting the class label: This operation is equivalent to
computing argmax on the last layer. For a vector of size
c where each number is represented with b bits, argmax
is translated to c · (2b + 1) many non-XOR (AND) gates.
For example, in a typical architecture for MNIST (e.g.,
BM3) or CIFAR-10 dataset (e.g., BC1), the overhead is
1.68E-2% and 1.36E-4%, respectively.
Note that the two aforementioned defense mechanisms can
be augmented to any framework that supports non-linear
functionalities [7, 9, 13]. However, we want to emphasize
that compared to mixed-protocol solutions, this means that
another round of communication is usually needed to sup-
port the ﬁlter layer. Whereas, in XONN the ﬁlter layer does
not increase the number of rounds and has negligible over-
head compared to the overall protocol.
In this section, we review three of the most important attacks
against deep neural networks that are relevant to the context
of oblivious inference [1, 27, 28]. In all three, a client-server
model is considered where the client is the adversary and at-
tempts to learn more about the model held by the server. The
client sends many inputs and receives the inference results .
He then analyzes the results to infer more information about
either the network parameters or the training data that has
been used in the training phase of the model. We brieﬂy re-
view each attack and illustrate a simple defense mechanism
with negligible overhead based on the suggestions provided
in these works.
Model Inversion Attack [27].
In the black-box access
model of this attack (which ﬁts the computational model of
this work), an adversarial client attempts to learn about a pro-
totypical sample of one of the classes. The client iteratively
creates an input that maximizes the conﬁdence score corre-
sponding to the target class. Regardless of the speciﬁc train-
ing process, the attacker can learn signiﬁcant information by
querying the model many times.
Model Extraction Attack [1]. In this type of attack, an ad-
versary’s goal is to estimate the parameters of the machine
learning model held by the server. For example, in a logis-
tic regression model with n parameters, the model can be
extracted by querying the server n times and upon receiv-
ing the conﬁdence values, solving a system of n equations.
Model extraction can diminish the pay-per-prediction busi-
ness model of technology companies. Moreover, it can be
used as a pre-step towards the model inversion attack.
Membership Inference Attack [28]. The objective of this
attack is to identify whether a given input has been used in
the training phase of the model or not. This attack raises cer-
tain privacy concerns. The idea behind this attack is that the
neural networks usually perform better on the data that they
were trained on. Therefore, two inputs that belong to the
same class, one used in the training phase and one not, will
have noticeable differences in the conﬁdence values. This
behavior is called overﬁtting. The attack can be mitigated
using regularization techniques that reduce the dependency
of the DL model on a single training sample. However, over-
ﬁtting is not the only contributor to this information leakage.
Defense Mechanisms.
In the prior state-of-the-art oblivi-
ous inference solution [9], it has been suggested to limit the
number of queries from a speciﬁc client to limit the informa-
tion leakage. However, in practice, an attacker can imper-
sonate himself as many different clients and circumvent this
defense mechanism. Note that all three attacks rely on the
fact that along with the inference result, the server provides
the conﬁdence vector that speciﬁes how likely the client’s in-
put belongs to each class. Therefore, as suggested by prior
work [1, 27, 28], it is recommended to augment a ﬁlter layer
that (i) rounds the conﬁdence scores or (ii) selects the index
1518    28th USENIX Security Symposium
USENIX Association