knowledge proofs without being detected.)
A3. It is not feasible to forge a receipt (see below for the justiﬁ-
cation). This could be achieved by using special paper for the
receipts or by means of digital signatures.
A4. The voters that enter the voting booth are counted correctly
(by the voting booth); otherwise, nothing would prevent the
voting machine from voting on behalf of the abstaining voters,
which would further weaken the goal that can be achieved.
Note that we neither assume that the machine nor the RNG are
honest. The RNG can, for example, output some predetermined
sequence of numbers instead of random numbers. But then to prove
accountability/veriﬁability for a reasonable goal, assumption A1 is
crucial: If it were possible for the machine to send instructions to
the RNG, both devices could cooperate to change a voter’s vote,
see Appendix A.2 for details.
Without assumption A3, the following problem would occur: In
case a voter provides a receipt and claims that it does not appear
on the bulletin board, it would not be clear whether the machine
is dishonest (has not posted the legitimate receipt) or the voter is
dishonest (has forged the receipt). Hence, a judge could only blame
both parties, resulting in a lower level of accountability. Note that
A3 is a standard and reasonable assumption.
In order to formally deﬁne the goal γk of the protocol, we use
the following modeling detail. The only honest program of a voter
(that is the only program in ˆΠvi, where vi represents a voter) is of
the following form. It ﬁrst determines the voter’s choice c. In this
case, this choice is picked according to the probability distribution
(cid:126)p. (However, we could also let the adversary determine c, indepen-
dently of any distribution. Theorem 1 would still be true.) Once c
is picked, the voter runs the procedure Vote(c), which submits the
choice c to the voting machine, as speciﬁed by the protocol (see
Appendix A.2 for the details of Vote).
Formal deﬁnition of the goal γk for e-voting. Let vi ∈ Σ be a
voter (recall that Σ is the set of agents of the protocol) and r be
a run of an instance π (see Section 2.1). Recall that π is of the
form π = (πv1 (cid:107) . . . (cid:107) πvn (cid:107) π(cid:48)), where πvi is the program run by
the voter vi and the process π(cid:48) contains the programs of all other
participants.1 Recall also from Section 2.1 that we say that the voter
vi is honest in a run r of the instance π if πvi ∈ ˆΠvi, i.e., if this voter
runs its honest program; similarly for the other participants. Recall
that, in this case, vi ﬁrst picks a choice c and then runs Vote(c). We
will say that c is the choice of the honest voter vi in the run r. By
this, the choice of an honest voter in a run r is deﬁned precisely.
Note that the choice of an honest voter in a run directly expresses
1For Bingo voting, we have that π(cid:48) = πM (cid:107) πA1 (cid:107) ··· (cid:107) πAr(cid:48) (cid:107) πRNG (cid:107)
π judge (cid:107) πbooth (cid:107) πscheduler, where πM, πAi, πRNG, π judge, πbooth,
and πscheduler are the programs run by the voting machine M, the
auditor Ai, the RNG, the judge, the booth, and the scheduler, re-
spectively (see also Appendix A.2). We formulate π in this way,
i.e., by using π(cid:48), to emphasize that the deﬁnition of γk does not
depend on any speciﬁc protocol structure. In particular, it does not
depend on a speciﬁc form of π(cid:48).
the intention of the voter. Clearly, this does not imply that this
choice is actually counted. Whether or not it is counted depends on
the correctness of the voting procedure Vote and the behavior of
the remaining parties in a run. For example, dishonest authorities
might try to drop or alter the vote. Also, in the case of remote
electronic voting, Vote(c) might simply model a human actor, who
indicates her choice c to a client program. This client program,
which formally would be a different agent, may be malicious and
try to cast a vote different to (the voter’s intention) c.
Let ρ be a counting function, that is, a function which for a mul-
tiset of valid choices returns the (ideal) election result. In our case
ρ simply counts and returns the number of votes for each candidate.
Now, we are ready to formally deﬁne the goal γk. This is a very
generic and general goal, which applies to any e-voting protocol.
DEFINITION 6. Let r be a run of (some instance of) the proto-
col. Let nh be the number of honest voters in r and nd = n− nh be
the number of dishonest voters in r. Let c1, . . . ,cnh be the choices
of the honest voters in this run, as deﬁned above.
We say that γk is satisﬁed in r (or r belongs to γk, i.e., r ∈ γk), if
there exist valid choices ˜c1, . . . ˜cn such that the multiset { ˜c1, . . . ˜cn}
contains at least nh − k elements of the multiset {c1, . . .cnh} and
the result of the election as published in r (if any) is equal to
ρ({ ˜c1, . . . ˜cn}); if no election result is published in r, then γk is
not satisﬁed in r.
EXAMPLE 2. Let us consider an example with 5 voters. Let r
be a run with three honest and two dishonest voters such that A,
A, B are the choices of the honest voters in r, respectively, and the
published election result in r is the following: one vote for A and
four votes for B. Then, the goal γ1 is satisﬁed, i.e., r ∈ γ1. This
is because the result is equal to ρ(A,B,B,B,B) and the multiset
S = {A,B,B,B,B} contains nh−1 = 2 choices of the honest voters,
namely, S contains {A,B}. However, the goal γ0 is not satisﬁed in
r, because there does not exist a multiset S(cid:48) such that ρ(S(cid:48)) equals
the result published in r and such that S(cid:48) contains all the choices
{A,A,B} of honest voters.
REMARK 2. We emphasize that in the above deﬁnition, the
multiset { ˜c1, . . . ˜cn} of choices is simply quantiﬁed existentially, in-
dependently of the speciﬁc run r. We only require that this multiset
contains nh − k actual choices of the honest voters in r and that
ρ({ ˜c1, . . . ˜cn}) equals the published result in r. The other k + nd
choices in { ˜c1, . . . ˜cn} can be chosen arbitrarily, and independently
of r, as long as ρ({ ˜c1, . . . ˜cn}) equals the published result in r. In
particular, we do not require that choices made by dishonest voters
in r need to be extracted from r and that these extracted choices
need to be reﬂected in { ˜c1, . . . ˜cn}. This is because, in general, one
cannot provide any guarantees for dishonest voters, since, for ex-
ample, their ballots might be altered or ignored by dishonest au-
thorities without the dishonest voters complaining. Dishonest vot-
ers might even encourage dishonest authorities to do so in order to
manipulate the election.2
2For speciﬁc protocols, in some cases, one could provide slightly
stronger guarantees than what is required by γk, though.
If, for
example, we assume an e-voting system with a bulletin board to
which voters submit their ballots along with zero-knowledge proofs
of knowledge of the submitted (valid) votes, we could, in addition
to what is required by γk, also require that the published results
equals (possibly again up to a certain number of votes) the result
that can be extracted from the (valid) ballots on the bulletin board;
the latter is typically referred to as universal veriﬁability (see also
Section 3). Note that a ballot which appears on the bulletin board
but which has not been submitted by an honest voter might not ac-
8
REMARK 3. We also note that our deﬁnition of a goal makes
only very minimal assumptions about the structure of a voting pro-
tocol. Namely, it requires only that, given a run r, it is possible
to determine the actual choice (intention) of an honest voter (the
parameter of the procedure Vote) and the actual election result as
output by the voting authorities in r. Clearly, this should be pos-
sible for any reasonable voting protocol. We do not assume here
anything more: we do not assume any speciﬁc phases of the pro-
tocol, nor any speciﬁc voting authorities and system components,
such as a bulletin board.
Accountability. We now state the level of accountability the Bingo
voting system provides. The parameter δ in the computational def-
inition of accountability (Deﬁnition 3) will be the following:
(cid:18) 1
2s , max((1− qnum), (1− qrec), max
δk
Bingo = max
p j)k+1
(cid:19)
j=1,...,l
,
where k is the parameter for the tolerated number of incorrectly
counted votes of honest voters, as used for the goal γk, and s, qnum,
qrec, and p1, . . . , pl are as introduced in Section 5.1.
We show (in Appendix A.3) that the protocol is accountable for
Φ1, where Φ1 consists of the following constraints:
αcompl ⇒ dis(M)∨ dis(RNG)∨ dis(v1) | . . .
··· | dis(M)∨ dis(RNG)∨ dis(vn)
αtwice ⇒ dis(M)∨ dis(RNG),
¬γk ∩¬αcompl ∩¬αtwice ⇒ dis(M) | dis(RNG).
THEOREM 1. Let a be an external judge or a voter. Under
Bingo)-account-
the DLOG-assumption3, the agent a ensures (Φ1, δk
ability for Pa
This theorem says that, in Pa
Bingo1, the probability that the goal γk
is not achieved and a does not blame anybody is at most δk
Bingo,
up to some negligible value. Moreover, a single agent can be held
accountable (and because of fairness rightly so) if, in the case the
goal is not achieved, no voter complains in the booth and no number
occurs twice on receipts.
Bingo1(n,qnum,qrec,s,(cid:126)p).
We emphasize that the above theorem includes the case where
the RNG produces a totally predictable sequence of random num-
bers. If we had assumed an honest RNG, we could have omitted
the term max j=1,...,l p j in the deﬁnition of δk
Bingo in the above the-
orems. Also, we note that from the proof of Theorem 1 it follows
that the parameter δk
Bingo is optimal, i.e., there is a (misbehaving)
voting machine which changes k +1 votes but is detected only with
probability δk
Veriﬁability. Let us observe that, since J ensures (Φ1, δk
(cid:87)
Bingo)-
accountability, J also ensures (¬γ ⇒ ψ)-accountability, where ψ =
a∈Σ dis(a). Also, whenever J states ψ(cid:48), then ψ(cid:48) implies ψ. There-
fore, due to the fact that the judging procedure is constructed in
such a way that J accepts the run if and only if J does not blame
anybody, by Proposition 1, we immediately obtain the following
result.
Bingo.
COROLLARY 1. Let a be an external judge or a voter. Under
Bingo1(n,qnum,qrec,s,(cid:126)p), the goal γk is
the DLOG-assumption, in Pa
guaranteed by(cid:86)
a∈Σ hon(a) and δk
Bingo-veriﬁable by a.
tually have been submitted by a dishonest voter either but might
have been placed on the bulletin board by a dishonest voting au-
thority, say, possibly replacing a ballot submitted by a dishonest
voter.
3From this assumption, it follows that it is infeasible to open a
Pedersen-commitment to two different values [39].
This corollary says that, in Pa
Bingo1, correctness of the result (up to
votes of dishonest voters) is guaranteed only if all participants are
honest and is δk
Bingo-veriﬁable by a (recall that a uses only public
information). This means that a, with overwhelming probability,
accepts a run if everybody is honest, but he/she accepts a run only
with probability at most δk
Bingo if the result is not correct (up to
votes of dishonest voters).
This veriﬁability property reﬂects the weakness of the system
Pa
Bingo1(n,qnum,qrec,s,(cid:126)p) already revealed by Theorem 1: By
wrongly complaining, every single dishonest voter can spoil the
election process. This weakness is not present in the version men-
tioned above, that we study in Appendix A.4, which, however,
comes at a price of a weaker goal.
6. THE PRST PROTOCOL
In this section, we study the auction protocol proposed by Parkes,
Rabin, Shieber, and Thorpe [38]. More precisely, we study here
one of a few variants of the protocol proposed in [38], namely the
variant for Vickrey auctions with one item and without so-called
delayed decryption key revelation services; our deﬁnition also ap-
plies to the other variants, though. We carry out our analysis in a
symbolic (Dolev-Yao style) model.
While applying our deﬁnition of accountability to this protocol,
we identiﬁed some quite serious problems that allow parties to mis-
behave and spoil the complete auction process, without facing the
risk of being held individually accountable. We propose ﬁxes to the
original protocol in order to establish individual accountability and
make the protocol useable.
6.1
Informal Description of the Protocol
The protocol assumes a public key infrastructure. In particular,
only bidders with registered signature keys can participate in the
protocol. The protocol uses digital signatures, a hash function (used
to produce commitments4), homomorphic randomized encryption
(more speciﬁcally, Paillier encryption), and non-interactive zero-
knowledge proofs for proving correctness of the result (see below).
By sigA[m] we abbreviate the message (cid:104)m, sigA(m)(cid:105), where
sigA(m) is a term representing the signature of A on the message
m. By EA(m,r) we will denote encryption of a message m under
the public key of A with random coins r. By hash(m) we denote
the hash of m.
The parties of the protocol are the following:
the bidders
B1, . . . ,Bn, the auctioneer A, and the notaries N1, . . . ,Nl. The auc-
tioneer maintains a bulletin board, where he posts all public in-
formation about the auction. All posts to the bulletin board carry
appropriate digital signatures.
The protocol consists of the following steps. For simplicity of
presentation, in the description of the protocol given below, we as-
sume that all the entitled bidders B1, . . . ,Bn participate in the auc-
tion and that all their bids are different; this convention is not es-
sential and can easily be dropped. Also, for simplicity, we have
left out some additional input provided by the parties for the zero-
knowledge proof, since in our symbolic modeling of zero-know-
ledge proofs this input is not needed (see [38] for details).
S1. A posts (on the bulletin board) basic information about the auc-
tion: the terms of the auction, an identiﬁer Id, the deadlines
T1,T2,T3 for different stages of the auction, and his public en-
cryption key.
S2. To participate in the auction, a bidder Bi chooses her bid bi
and encrypts it as Ci = EA(bi,ri) using a random coin ri. Bi
4A hash function is used to commit on values with high entropy.
9
then commits to Ci, computing Comi = (cid:104)hash(Ci),Id(cid:105), signs
this commitment, and sends sigBi [Comi] to A and her no-
taries, if used, before time T1. The notaries forward the signed
commitments to A. A replies by sending a signed receipt
Ri = sigA[Comi,Id,T1] to Bi. If Bi does not obtain her receipt,
she complains.
S3. At time T1, the auctioneer A posts all the received commit-
ments in a random order: Comπ(1), . . . ,Comπ(n), where π is a
randomly chosen permutation of the indices of submitted com-
mitments.
S4. Between time T1 and T2 any bidder Bi who has a receipt Ri for a
commitment which is not posted can appeal her non-inclusion
(by providing her receipt).
S5. After time T2, every Bi sends to A her encrypted bid Ci. After
time T3, A posts Cπ(1), . . . ,Cπ(n). Anybody can verify whether
all the commitments posted in S3 have been correctly opened.
S6. A recovers the bids b1, . . . ,bn, by decrypting the encrypted bids
with his private decryption key, and determines the winner Bw
of the auction and the price bu the winner has to pay, which
is supposed to be the second highest bid. He also constructs
a (universally veriﬁable) zero-knowledge proof P that the re-
sult is correct, i.e. Cw contains the biggest bid and Cu contains
the second biggest bid bu: This is done by proving appropri-