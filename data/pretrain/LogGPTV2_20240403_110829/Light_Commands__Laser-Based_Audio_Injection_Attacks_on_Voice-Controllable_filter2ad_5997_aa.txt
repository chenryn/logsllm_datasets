title:Light Commands: Laser-Based Audio Injection Attacks on Voice-Controllable
Systems
author:Takeshi Sugawara and
Benjamin Cyr and
Sara Rampazzi and
Daniel Genkin and
Kevin Fu
Light Commands: Laser-Based Audio Injection 
Attacks on Voice-Controllable Systems
Takeshi Sugawara, The University of Electro-Communications; Benjamin Cyr, 
Sara Rampazzi, Daniel Genkin, and Kevin Fu, University of Michigan
https://www.usenix.org/conference/usenixsecurity20/presentation/sugawara
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Light Commands: Laser-Based Audio Injection Attacks
on Voice-Controllable Systems
Takeshi Sugawara
Benjamin Cyr
The University of Electro-Communications
University of Michigan
PI:EMAIL
Daniel Genkin
University of Michigan
PI:EMAIL
PI:EMAIL
Kevin Fu
University of Michigan
PI:EMAIL
Sara Rampazzi
University of Michigan
PI:EMAIL
Abstract
We propose a new class of signal injection attacks on mi-
crophones by physically converting light to sound. We show
how an attacker can inject arbitrary audio signals to a target
microphone by aiming an amplitude-modulated light at the
microphone’s aperture. We then proceed to show how this
effect leads to a remote voice-command injection attack on
voice-controllable systems. Examining various products that
use Amazon’s Alexa, Apple’s Siri, Facebook’s Portal, and
Google Assistant, we show how to use light to obtain control
over these devices at distances up to 110 meters and from
two separate buildings. Next, we show that user authentica-
tion on these devices is often lacking, allowing the attacker
to use light-injected voice commands to unlock the target’s
smartlock-protected front doors, open garage doors, shop on
e-commerce websites at the target’s expense, or even unlock
and start various vehicles connected to the target’s Google
account (e.g., Tesla and Ford). Finally, we conclude with pos-
sible software and hardware defenses against our attacks.
1 Introduction
The consistent growth in computational power is profoundly
changing the way that humans and computers interact. Mov-
ing away from traditional interfaces like keyboards and mice,
in recent years computers have become sufﬁciently powerful
to understand and process human speech. Recognizing the
potential of quick and natural human-computer interaction,
technology giants such as Apple, Google, Facebook, and Ama-
zon have each launched their own large-scale deployment of
voice-controllable (VC) systems that continuously listen to
and act on human voice commands.
With tens of millions of devices sold with Alexa, Siri, Por-
tal, and Google Assistant, users can now interact with ser-
vices without the need to sit in front of a computer or type
on a mobile phone. Responding to this trend, the Internet
of Things (IoT) market has also undergone a small revolu-
tion. Rather than having each device be controlled via a dedi-
cated manufacture-provided software, IoT manufacturers can
now spend their time making hardware, coupling it with a
lightweight interface to integrate their products with Alexa,
Siri or Google Assistant. Thus, users can receive information
and control products by the mere act of speaking without
the need for physical interaction with keyboards, mice, touch-
screens, or even buttons.
However, while much attention is being given to improving
the capabilities of VC systems, much less is known about
the resilience of these systems to software and hardware at-
tacks. Indeed, previous works [1, 2] already highlight the lack
of proper user authentication as a major limitation of voice-
only interaction, causing systems to execute commands from
potentially malicious sources.
While early command-injection techniques were noticeable
by the device’s legitimate owner, more recent works [3, 4, 5,
6, 7, 8, 9, 10] focus on stealthy injection, preventing users
from hearing or recognizing the injected commands.
The absence of voice authentication has resulted in a
proximity-based threat model, where close-proximity users
are considered legitimate, while attackers are kept at bay by
physical obstructions like walls, locked doors, or closed win-
dows. For attackers aiming to surreptitiously gain control over
physically-inaccessible systems, existing injection techniques
are unfortunately limited, as the current state of the art [6] has
a range of about 25 ft (7.62 m) in open space, with physical
barriers (e.g., windows) further reducing the distance. Thus,
in this paper we tackle the following questions:
Can commands be remotely and stealthily injected into a
voice-controllable system from large distances? If so, how
can an attacker perform such an attack under realistic condi-
tions and with limited physical access? Finally, what are the
implications of such command injections on third-party IoT
hardware integrated with the voice-controllable system?
1.1 Our Contribution
In this paper we present LightCommands, an attack that can
covertly inject commands into voice-controllable systems at
long distances.
USENIX Association
29th USENIX Security Symposium    2631
Figure 1: Experimental setup for exploring attack range. (Top) Floor plan of the 110 m long corridor. (Left) Laser with telephoto
lens mounted on geared tripod head for aiming. (Center) Laser aiming at the target across the 110 m corridor. (Right) Laser spot
on the target device mounted on tripod.
Laser-Based Audio Injection.
First, we have identiﬁed a
semantic gap between the physics and speciﬁcations of mi-
crophones, where microphones often unintentionally respond
to light as if it was sound. Exploiting this effect, we can inject
sound into microphones by simply modulating the amplitude
of a laser light.
Attacking Voice-Controllable Systems. Next, we investi-
gate the vulnerability of popular VC systems (such as Alexa,
Siri, Portal, and Google Assistant) to light-based audio injec-
tion attacks. We ﬁnd that 5 mW of laser power (the equivalent
of a laser pointer) is sufﬁcient to control many popular voice-
activated smart home devices, while about 60 mW is sufﬁcient
for gaining control over phones and tablets.
Attack Range. Using a telephoto lens to focus the laser, we
demonstrate the ﬁrst command injection attack on VC systems
which achieves distances of up to 110 meters (the maximum
distance safely available to us) as shown in Figure 1. We also
demonstrate how light can be used to control VC systems
across buildings and through closed glass windows at similar
distances. Finally, we note that unlike previous works that
have limited range due to the use of sound for signal injection,
the range obtained by light-based injection is only limited by
the attacker’s power budget, optics, and aiming capabilities.
Insufﬁcient Authentication. Having established the feasi-
bility of malicious control over VC systems at large distances,
we investigate the security implications of such attacks. We
ﬁnd that VC systems often lack any user authentication mech-
anisms, or if the mechanisms are present, they are incorrectly
implemented (e.g., allowing for PIN brute forcing). We show
how an attacker can use light-injected voice commands to un-
lock the target’s smart-lock protected front door, open garage
doors, shop on e-commerce websites, or even locate, unlock
and start various vehicles (e.g., Tesla and Ford) if the vehicles
are connected to the target’s Google account.
Attack Stealthiness and Cheap Setup. We then show how
an attacker can build a cheap yet effective injection setup, us-
ing commercially available laser pointers and laser drivers.
Moreover, by using infrared lasers and abusing volume fea-
tures (e.g., whisper mode for Alexa devices) on the target
device, we show how an attacker can mount a light-based au-
dio injection attack while minimizing the chance of discovery
by the target’s legitimate owner.
Countermeasures.
hardware-based countermeasures against our attacks.
Summary of Contributions.
following contributions.
1. Discover a vulnerability in MEMS microphones, making
them susceptible to light-based signal injection attacks
(Section 4).
Finally, we discuss software and
In this paper we make the
2. Characterize the vulnerability of popular Alexa, Siri, Por-
tal, and Google Assistant devices to light-based command
injection across large distances and varying laser power
(Section 5).
3. Assess the security implications of malicious command
injection attacks on VC systems and demonstrate how such
attacks can be mounted using cheap and readily available
equipment (Section 6).
4. Discuss software and hardware countermeasures to light-
based signal injection attacks (Section 7).
1.2 Safety and Responsible Disclosure
Laser Safety.
Laser radiation requires special controls
for safety, as high-powered lasers might cause hazards of
ﬁre, eye damage, and skin damage. We urge that researchers
receive formal laser safety training and approval of experi-
mental designs before attempting reproduction of our work.
In particular, all the experiments in this paper were conducted
under a Standard Operating Procedure which was approved
by our university’s Safety Committee.
2632    29th USENIX Security Symposium
USENIX Association
TargetGoogle Home attached toLaser spotLaser beam110 mLaserTargetLaser mountTelephoto lensGeared tripod headgeared tripod headLaserDisclosure Process.
Following the practice of responsible
disclosure, we have shared our ﬁndings with Google, Amazon,
Apple, Facebook, August, Ford, Tesla, and Analog Devices,
a major supplier of MEMS microphones. We subsequently
maintained contact with the security teams of these vendors,
as well as with ICS-CERT and the FDA. The ﬁndings pre-
sented in this paper were made public on the mutually-agreed
date of November 4th, 2019.
2 Background
2.1 Voice-Controllable System
The term “Voice-Controllable (VC) system” refers to a sys-
tem that is controlled primarily by voice commands directly
spoken by users in a natural language, e.g., English. While
some important exceptions exist, VC systems often immedi-
ately operate on voice commands issued by the user without
requiring further interaction. For example, when the user com-
mands the VC system to “open the garage door”, the garage
door is immediately opened.
Following the terminology of [4], a typical VC system
is composed of three main components: (i) voice capture,
(ii) speech recognition, and (iii) command execution. First,
the voice capture subsystem is responsible for converting
sound produced by the user into electrical signals. Next, the
speech recognition subsystem is responsible for detecting
the wake word in the acquired signal (e.g., “Alexa", “OK
Google", "Hey Portal" or “Hey Siri") and subsequently inter-
preting the meaning of the voice command using signal and
natural-language processing. Finally, the command-execution
subsystem launches the corresponding application or executes
an operation based on the recognized voice command.
2.2 Attacks on Voice-Controllable Systems
Several previous works explored the security of VC systems,
uncovering vulnerabilities that allow attackers to issue unau-
thorized voice commands to these devices [3, 4, 5, 6, 7].
Malicious Command Injection. More speciﬁcally, [1, 2]
developed malicious smartphone applications that play syn-
thetic audio commands into nearby VC systems without re-
quiring any special operating system permissions. While these
attacks transmit commands that are easily noticeable to a hu-
man listener, other works [3, 8, 9] focused on camouﬂaging
commands in audible signals, attempting to make them unin-
telligible or unnoticeable to human listeners, while still being
recognizable to speech recognition models.
Inaudible Voice Commands. A more recent line of work
focuses on completely hiding the voice commands from hu-
man listeners. Roy et al. [5] demonstrate that high frequency
sounds inaudible to humans can be recorded by commodity
microphones. Subsequently, Song and Mittal [10] and Dol-
phinAttack [4] extended the work of [5] by sending inaudible
commands to VC systems via word modulation on ultrasound
carriers. By exploiting microphone nonlinearities, a signal
modulated onto an ultrasonic carrier is demodulated to the
audible range by the targeted microphone, recovering the orig-
inal voice command while remaining undetected by humans.
However, both attacks are limited to short distances (from
2 cm to 175 cm) due to the transmitter operating at low power.
Unfortunately, increasing the transmitting power generates an
audible frequency component containing the (hidden) voice
command, as the transmitter is also affected by the same non-
linearity observed in the receiving microphone. Tackling the
distance limitation, Roy et al. [6] mitigated this effect by split-
ting the signal in multiple frequency bins and playing them
through an array of 61 speakers. However, the re-appearance
of audible leakage still limits the attack’s range to 25 ft (7.62
m) in open space, with physical barriers (e.g., windows) and
the absorption of ultrasonic waves in air further reducing
range by attenuating the transmitted signal.
Skill Squatting Attacks. A ﬁnal line of work focuses on
confusing speech recognition systems, causing them to mis-
interpret correctly-issued voice commands. These so-called
skill squatting attacks [11, 12] work by exploiting systematic
errors in the recognition of similarly sounding words, routing
users to malicious applications without their knowledge.
2.3 Acoustic Signal Injection Attacks
Several works used acoustic signal injection as a method of
inducing unintended behavior in various systems.
More speciﬁcally, Son et al. [13] showed that MEMS sen-
sors are sensitive to ultrasound signals, resulting in denial
of service attacks against inertial measurement unit (IMU)
on drones. Subsequently, Yan et al. [14] demonstrated that
acoustic waves can be used to saturate and spoof ultrasonic
sensors, impairing car safety. This was further improved by
Walnut [15], which exploited aliasing and clipping effects
in the sensor’s components to achieve precise control over
MEMS accelerometers via sound injection.
More recently, Nashimoto et al. [16] showed the possibility
of using sound to attack sensor-fusion algorithms that rely on
data from multiple sensors (e.g., accelerometers, gyroscopes,
and magnetometers) while Blue Note [17] demonstrates the
feasibility of sound attacks on mechanical hard drives, result-
ing in operating system crashes.
2.4 Laser Injection Attacks
In addition to sound, light has also been utilized for signal
injection. Indeed, [14, 18, 19] mounted denial of service at-
tacks on cameras and LiDARs by illuminating victims’ photo-
receivers with strong lights. This was later extended by Shin
et al. [20] and Cao et al. [21] to a more sophisticated attack
that injects precisely-controlled signals to LiDAR systems,
causing the target to see an illusory object. Next, Park et al.
USENIX Association
29th USENIX Security Symposium    2633
[22] showed an attack on medical infusion pumps, using light
to attack optical sensors that count the number of adminis-
tered medication drops. Finally, Uluagac et al. [23] show how
various sensors, such as infrared and light sensors, can be used
to activate and transfer malware between infected devices.
Another line of work focuses on using light for injecting
faults inside computing devices, resulting in security breaches.
More speciﬁcally, it is well-known that laser light causes
soft (temporary) errors in semiconductors, where similar er-
rors are also caused by ionizing radiation [24]. Exploiting
this effect, Skorobogatov and Anderson [25] showed the ﬁrst
light-induced fault attacks on smartcards and microcontrollers,
demonstrating the possibility of ﬂipping individual bits in
memory cells. This effect was subsequently exploited in nu-
merous follow ups, using laser-induced faults to compromise
the hardware’s data and logic ﬂow, extract secret keys, and