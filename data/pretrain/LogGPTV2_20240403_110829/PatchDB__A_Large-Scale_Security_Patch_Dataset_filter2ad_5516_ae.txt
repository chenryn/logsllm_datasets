RNN) model trained with the NVD-based dataset can achieve
58.4% (and 82.8%) precision with 21.7% (and 83.2%) recall.
The testing precision and recall would be 90.1% (and 92.8%)
as well as 22.5% (and 60.2%) recall if we train the model with
both NVD-based dataset and wild-based dataset. The training
dataset has little impact on the model when tested on the
NVD-based dataset. However, when using the wild-based test
dataset, the precision and recall drop to 58.0% (and 88.3%)
and 19.5% (and 24.2%) if the random forest (and RNN) model
is trained with the NVD-based dataset. Therefore, the machine
learning model trained by the NVD-based dataset exhibits
insufﬁcient generalization ability due to the limited number
and patterns of instances. In contrast, the models trained with
both the NVD-based dataset and the wild-based dataset have
better generalization ability. The performance remains stable
no matter which testing dataset is used, proving that the model
can be applied to unknown patch samples.
The differences between classiﬁcation models are also
demonstrated in Table VI. With the same training data and
testing data, the RNN model has a better performance than the
Random Forest model. Compared with the statistic syntactic
features (Table I), the RNN model can also seize the context
information between programming tokens, which provides
valuable insight into the programming language processing.
V. DISCUSSION
We describe several usage scenarios of PatchDB and how
the dataset may promote the related research and applications.
158
A. Usage Scenarios of PatchDB
1) Vulnerability/Patch Presence Detection: Since security
patches comprise both the vulnerable code and corresponding
ﬁxes, they can be used to detect vulnerable code clone by using
patch-enhanced vulnerability signatures [9], [36]. Such works
generate signatures directly from the code gadgets. Hence,
more security patch instances enable more vulnerability signa-
tures for matching and thus enhances the detection capability.
From another perspective, patching status is critical for down-
stream software, which motivates the need for reliable patch
presence testing. The PatchDB identiﬁes 8K silent security
patches that are not provided in the NVD. The presence of
such patches can be tested in the downstream software [17],
[40]. Also, a binary security patch dataset could be constructed
by compiling the source code in our dataset.
2) Automatic Patch Generation: Since previous patch anal-
ysis works are conducted on a small dataset, they are con-
strained to summarize the ﬁx patterns of some common patch
types, e.g., sanity testing. The main reason is that they lack
enough instances to perform their study. In contrast, our
analysis on the PatchDB in terms of code changes (Section
IV-B-2) presents that there are still many security patches with
multiple ﬁx patterns so that our large-scale patch dataset could
be used to summarize more patch patterns. In Table VII, we
show two examples of ﬁx patterns concluded by ourselves
based on observation of the PatchDB that have never been
studied by previous study [24], [35], [38], i.e., race condition
and data leakage. These patterns describe how security patches
ﬁx the corresponding security impacts caused by vulnerable
operations. For the race condition, the patches typically add
and release lock to guarantee the atomicity for a vulnerable
operation. For the data leakage,
the patches often release
the critical value after the last normal operation to avoid
further vulnerable operation. With a large-scale security patch
dataset, more complex patch patterns can be discovered so that
semantics can be learned for automatically generating more
types of security patches.
TABLE VII: Example of ﬁx patterns
Race Condition
Data Leakage
...
+ lock(CV);
normal_op(CV);
...
vulnerable_op(CV, ...); + release(CV);
+ unlock(CV);
...
CV = critical value
...
vulnerable_op(CV, ...);
3) Benchmark: The PatchDB can also be used as a bench-
mark. Since the PatchDB is the largest-scale dataset of security
patches to the best of our knowledge,
is closer to the
practical scenario and enlarges the spectrum of the evaluation.
Also, since it is collected from 313 GitHub repositories other
than some speciﬁc projects, it provides a good benchmark to
test the generalization capability of target techniques.
it
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
B. Limitations and Future Work
Our work currently focuses on C/C++ languages that are
with the highest number of vulnerabilities [34]. The syntactic
features identiﬁed in Table I may be commonly shared by the
patches for different languages (e.g., changes of if statements,
loops, and logical operators). Therefore, our system could
be extended to other programming languages by customizing
their syntax parsing related features. However, for safe lan-
guages like Rust that have much fewer vulnerabilities, it may
be difﬁcult to collect a large-size security patch dataset. We
leave the extensions to other languages as future work.
Similar to previous work [20], we assume that all
the
information retrieved from the NVD is correct. That is to say,
we assume the patches crawled from the URLs provided by
the NVD are for the corresponding CVE entry. However, we
observe up to 1% of patches may not be correct. For example,
the provided link is for a brand new version that mingles
multiple code differences where security ﬁx is part of that. We
consider the proportion of incorrect patches is small enough
to be ignored in our analysis. Also, the NVD may be biased
towards certain types of software. Given the wide range of
software included by the NVD, we argue that it will remain
largely applicable for most open source software.
VI. RELATED WORKS
Patch Datasets. Since a security patch aggregates both vul-
nerable code and the corresponding modiﬁcations at the same
time, many vulnerability detection research constructs security
patch datasets. Kim et al. [18] acquire security patches from
eight well-known Git repositories to detect vulnerable code
clone. Z. Li et al. [21] build a Vulnerability Patch Database
(VPD) that consists of 19 products. However, the size of these
datasets is not sufﬁcient to perform a machine learning-based
study and may introduce biases to analysis results. Although
SARD provides some samples that mitigate the vulnerabilities,
it mainly focuses on vulnerable code and most of the samples
are artiﬁcial. By querying thousands of CVE records for open
source projects on the NVD, F. Li et al. [20] build a large-scale
security patch database. Further, considering silent security
patches, Xiao et al. [36] enrich the dataset with commits
obtained from their industrial collaborator. However, such
datasets are not publicly accessible.
Besides, there are several web-based patch or bug tracking
systems. Patchwork [6], a patch management system, catches
patches sent to the mailing list, but it is mainly used for
several Linux kernel subsystems. Bug tracking systems like
Bugzilla [1] may provide patch information in corresponding
reports. Yet not all the bug reports contain such information
and they do not distinguish between security and non-security
patches. These limitations motivate us to construct a large
dataset of security patches from various types of projects.
Patch Analysis. Recently, there is an increasing number of
works on patch analysis. Most of them focus on investigating
the textual information (e.g., bug report, commit message,
etc.), which does not require the retrieval and analysis of the
source code. They use supervised and unsupervised learning
techniques to classify patches [13], [16], [43]. However, they
cannot handle the situation where the documentation of se-
curity patches is inaccurate or even totally missing due to
different maintainers, limited security domain knowledge, and
changing regulations during the software life cycle.
At the source code level, Zhong et al. [42] conduct an
empirical study on bug ﬁxes from six popular JAVA projects.
Soto et al. [29] focus on patterns, replacements, deletions, and
additions of bug ﬁxes. Perl et al. [27] study the attributes
of commits that are more likely to introduce vulnerabilities.
Machiry et al. [25] analyze safe patches that do not disrupt
the intended functionality of the program. However, all these
works do not distinguish security patches from normal bug
ﬁxes. Zaman et al. [39] discover the differences between
security patches and performance bugs on a speciﬁc project
- Mozilla Firefox. Li et al. [20] are the ﬁrst one to perform
a large-scale empirical study of security patches versus non-
security bug ﬁxes, discussing the metadata characteristics and
life cycles of security patches.
Some studies utilize machine learning-based models to iden-
tify the type of a given patch [31]–[33], while the deﬁciency of
patch instances restricts the application of the robust classiﬁer
(e.g., deep learning model). Also, most of these models are
trained with a dataset from single or multiple software projects,
which provide limited generalization capacity in the wild. In
contrast, our work provides a large dataset from over 300
GitHub repositories and we use a new oversampling method
to further increase the variants at the source code level. At
the binary level, Xu et al. [37] present a scalable approach to
identify the existence of a security patch through semantic
analysis of execution traces. With the help of signatures
generated from open-source patches, some methods [17], [40]
test if the target binaries have been patched.
VII. CONCLUSION
In this work, we construct a large-scale dataset of security
patches called PatchDB. In particular, we develop a novel
nearest link search approach to help locate the most promising
candidates of security patches from an unlabeled dataset,
reducing the workload of the manual veriﬁcation. Also, we
propose a new oversampling method to synthesize patches
at the source code level, which is effective to increase the
variance of the patch dataset. We conduct a set of experiments
to study the composition and quality of PatchDB and verify
the effectiveness of our proposed algorithms. The results of
a comprehensive evaluation show that PatchDB is promising
to facilitate the patch analysis and vulnerability detection
techniques.
VIII. ACKNOWLEDGMENTS
This work was partially supported by the US Depart-
ment of the Army grant W56KGU-20-C-0008, the Ofﬁce of
Naval Research grants N00014-18-2893, N00014-16-1-3214,
and N00014-20-1-2407, and the National Science Foundation
grants CNS-1815650 and CNS-1822094.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
159
REFERENCES
[1] Bugzilla. https://www.bugzilla.org.
[2] Common Vulnerabilities and Exposures. https://cve.mitre.org.
[3] GitHub. https://github.com.
[4] LLVM. https://llvm.org.
[5] National Vulnerability Database. https://nvd.nist.gov.
[6] Patchwork. http://patchwork.ozlabs.org.
[7] Chris Anderson and Mia Poletto Andersson. Long tail. 2004.
[8] Paul E Black. Sard: Thousands of reference programs for software
assurance. J. Cyber Secur. Inf. Syst. Tools Test. Tech. Assur. Softw. Dod
Softw. Assur. Community Pract, 2(5), 2017.
[9] Benjamin Bowman and H Howie Huang. VGraph: A Robust Vulnerable
Code Clone Detection System Using Code Property Triplets. In 2020
IEEE European Symposium on Security and Privacy (EuroS&P), pages
53–69. IEEE, 2020.
[10] Center for Assured Software National Security Agency. Juliet Test Suite
https://samate.nist.gov/SRD/resources/
v1.2 for C/C++ User Guide.
Juliet Test Suite v1.2 for C Cpp - User Guide.pdf, 2018.
[11] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip
Kegelmeyer. Smote: synthetic minority over-sampling technique. Jour-
nal of artiﬁcial intelligence research, 16:321–357, 2002.
[12] Hong Cui, Jingjing Zhang, Chunfeng Cui, and Qinyu Chen. Solving
large-scale assignment problems by kuhn-munkres algorithm. 2016.
[13] Dipok Chandra Das and Md Rayhanur Rahman. Security and per-
formance bug reports identiﬁcation with class-imbalance sampling and
feature selection.
In 2018 Joint 7th International Conference on
Informatics, Electronics & Vision (ICIEV) and 2018 2nd International
Conference on Imaging, Vision & Pattern Recognition (icIVPR), pages
316–321. IEEE, 2018.
[14] E. Frank, M. A. Hall, G. Holmes, R. Kirkby, B. Pfahringer, and I. H.
Witten. Weka: A machine learning workbench for data mining., pages
1305–1314. Springer, Berlin, 2005.
[15] Michael Gegick, Pete Rotella, and Tao Xie.
Identifying security bug
reports via text mining: An industrial case study.
In 2010 7th IEEE
Working Conference on Mining Software Repositories (MSR 2010),
pages 11–20. IEEE, 2010.
[16] Katerina Goseva-Popstojanova and Jacob Tyo. Identiﬁcation of security
related bug reports via text mining using supervised and unsupervised
classiﬁcation.
In 2018 IEEE International Conference on Software
Quality, Reliability and Security (QRS), pages 344–355. IEEE, 2018.
[17] Zheyue Jiang, Yuan Zhang, Jun Xu, Qi Wen, Zhenghe Wang, Xiaohan
Zhang, Xinyu Xing, Min Yang, and Zhemin Yang. Pdiff: Semantic-
based patch presence testing for downstream kernels. In Proceedings of
the 2020 ACM SIGSAC Conference on Computer and Communications
Security, pages 1149–1163, 2020.
[18] Seulbae Kim, Seunghoon Woo, Heejo Lee, and Hakjoo Oh. Vuddy: A
scalable approach for vulnerable code clone discovery. In 2017 IEEE
Symposium on Security and Privacy (SP), pages 595–614. IEEE, 2017.
[19] Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised
learning method for deep neural networks. In Workshop on challenges
in representation learning, ICML, volume 3, 2013.
[20] Frank Li and Vern Paxson. A large-scale empirical study of security
In Proceedings of the 2017 ACM SIGSAC Conference on
patches.
Computer and Communications Security, pages 2201–2215, 2017.
[21] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Hanchao Qi, and Jie Hu.
Vulpecker: an automated vulnerability detection system based on code
similarity analysis. In Proceedings of the 32nd Annual Conference on
Computer Security Applications, pages 201–213, 2016.
[22] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang,
Zhijun Deng, and Yuyi Zhong. Vuldeepecker: A deep learning-based
system for vulnerability detection. arXiv preprint arXiv:1801.01681,
2018.
[23] Guanjun Lin, Wei Xiao, Jun Zhang, and Yang Xiang. Deep learning-
In International
based vulnerable function detection: A benchmark.
Conference on Information and Communications Security, pages 219–
232. Springer, 2019.
[24] Kangjie Lu, Aditya Pakki, and Qiushi Wu. Detecting missing-check bugs
via semantic-and context-aware criticalness and constraints inferences.
In 28th USENIX Security Symposium (USENIX Security 19), pages
1769–1786, 2019.
[25] Aravind Machiry, Nilo Redini, Eric Camellini, Christopher Kruegel, and
Giovanni Vigna. Spider: Enabling fast patch propagation in related
software repositories. In 2020 IEEE Symposium on Security and Privacy
(SP). IEEE, 2020.
[26] Masao Ohira, Yutaro Kashiwa, Yosuke Yamatani, Hayato Yoshiyuki,
Yoshiya Maeda, Nachai Limsettho, Keisuke Fujino, Hideaki Hata,
Akinori Ihara, and Kenichi Matsumoto. A dataset of high impact bugs:
Manually-classiﬁed issue reports.
In 2015 IEEE/ACM 12th Working
Conference on Mining Software Repositories, pages 518–521. IEEE,
2015.
[27] Henning Perl, Sergej Dechand, Matthew Smith, Daniel Arp, Fabian
Yamaguchi, Konrad Rieck, Sascha Fahl, and Yasemin Acar. VCCFinder:
ﬁnding potential vulnerabilities in open-source projects to assist code
audits.
the 22nd ACM SIGSAC Conference on
Computer and Communications Security, pages 426–437. ACM, 2015.
[28] Richard Segal, Ted Markowitz, and William Arnold. Fast uncertainty
In Proceedings of
sampling for labeling large e-mail corpora. In CEAS. Citeseer, 2006.
[29] Mauricio Soto, Ferdian Thung, Chu-Pan Wong, Claire Le Goues, and
David Lo. A deeper look into bug ﬁxes: patterns, replacements,
deletions, and additions. In 2016 IEEE/ACM 13th Working Conference
on Mining Software Repositories (MSR), pages 512–515. IEEE, 2016.
[30] Jinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xiaodong Shi, and
Yang Liu. Lattice-based recurrent neural network encoders for neural
machine translation. In Proceedings of the Thirty-First AAAI Conference
on Artiﬁcial Intelligence, AAAI’17, page 3302–3308. AAAI Press, 2017.
Identifying linux bug ﬁxing
patches. In 2012 34th international conference on software engineering
(ICSE), pages 386–396. IEEE, 2012.
[31] Yuan Tian, Julia Lawall, and David Lo.
[32] Xinda Wang, Kun Sun, Archer Batcheller, and Sushil Jajodia. Detecting”
0-day” vulnerability: An empirical study of secret security patch in
oss.
In 2019 49th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks (DSN), pages 485–492. IEEE, 2019.
[33] Xinda Wang, Shu Wang, Kun Sun, Archer Batcheller, and Sushil
Jajodia. A machine learning approach to classify security patches into
vulnerability types. In 2020 IEEE Conference on Communications and
Network Security (CNS), pages 1–9. IEEE, 2020.
are
[34] White
Software.
Source
What
secure
https://www.whitesourcesoftware.com/
the most
programming
most-secure-programming-languages/.
languages?
[35] Qiushi Wu, Yang He, Stephen McCamant, and Kangjie Lu. Precisely
characterizing security impact in a ﬂood of patches via symbolic rule
comparison. In Proceedings of the 27th Annual Network and Distributed
System Security Symposium (NDSS’20), 2020.
[36] Yang Xiao, Bihuan Chen, Chendong Yu, Zhengzi Xu, Zimu Yuan, Feng
Li, Binghong Liu, Yang Liu, Wei Huo, Wei Zou, et al. MVP: Detecting
vulnerabilities using patch-enhanced vulnerability signatures.
In 29th
USENIX Security Symposium (USENIX Security 20), pages 1165–1182,
2020.
[37] Zhengzi Xu, Bihuan Chen, Mahinthan Chandramohan, Yang Liu, and
Fu Song. Spain: security patch analysis for binaries towards under-
standing the pain and pills.
In Proceedings of the 39th International
Conference on Software Engineering, pages 462–472. IEEE Press, 2017.
[38] Zhengzi Xu, Yulong Zhang, Longri Zheng, Liangzhao Xia, Chenfu Bao,
Zhi Wang, and Yang Liu. Automatic hot patch generation for android
kernels. In 29th USENIX Security Symposium (USENIX Security 20),
pages 2397–2414, 2020.
[39] Shahed Zaman, Bram Adams, and Ahmed E Hassan. Security versus
performance bugs: a case study on ﬁrefox.
In Proceedings of the
8th working conference on mining software repositories, pages 93–102,
2011.
[40] Hang Zhang and Zhiyun Qian. Precise and accurate patch presence test
In 27th USENIX Security Symposium (USENIX Security
for binaries.
18), pages 887–902, 2018.
[41] Lei Zhao, Yuncong Zhu, Jiang Ming, Yichen Zhang, Haotian Zhang,
and Heng Yin. Patchscope: Memory object centric patch difﬁng.
In
Proceedings of the 2020 ACM SIGSAC Conference on Computer and
Communications Security, pages 149–165, 2020.
[42] Hao Zhong and Zhendong Su. An empirical study on real bug ﬁxes.
In 2015 IEEE/ACM 37th IEEE International Conference on Software
Engineering, volume 1, pages 913–923. IEEE, 2015.
[43] Yaqin Zhou and Asankhaya Sharma. Automated identiﬁcation of
security issues from commit messages and bug reports. In Proceedings
of the 2017 11th Joint Meeting on Foundations of Software Engineering,
pages 914–919, 2017.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:12 UTC from IEEE Xplore.  Restrictions apply. 
160