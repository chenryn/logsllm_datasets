### RNN Model Performance with NVD-Based and Wild-Based Datasets

The RNN model, when trained on the NVD-based dataset, achieves a precision of 58.4% (and 82.8%) with a recall of 21.7% (and 83.2%). When the model is trained on both the NVD-based and wild-based datasets, the testing precision and recall improve to 90.1% (and 92.8%) and 22.5% (and 60.2%), respectively. The training dataset has a minimal impact on the model's performance when tested on the NVD-based dataset. However, when tested on the wild-based dataset, the precision and recall drop to 58.0% (and 88.3%) and 19.5% (and 24.2%) if the model is trained only on the NVD-based dataset. This indicates that the model trained solely on the NVD-based dataset lacks sufficient generalization ability due to the limited number and patterns of instances. In contrast, models trained on both datasets exhibit better generalization, maintaining stable performance regardless of the testing dataset, which suggests they can be applied to unknown patch samples.

### Comparison of Classification Models

Table VI demonstrates the differences between classification models. With the same training and testing data, the RNN model outperforms the Random Forest model. Additionally, compared to the statistical syntactic features (Table I), the RNN model captures contextual information between programming tokens, providing valuable insights into programming language processing.

### Discussion

#### Usage Scenarios of PatchDB

1. **Vulnerability/Patch Presence Detection:**
   - Security patches, which include both vulnerable code and corresponding fixes, can be used to detect vulnerable code clones using patch-enhanced vulnerability signatures [9], [36]. More security patch instances enable more vulnerability signatures for matching, enhancing detection capabilities.
   - Patching status is critical for downstream software, motivating the need for reliable patch presence testing. PatchDB identifies 8,000 silent security patches not listed in the NVD, which can be tested in downstream software [17], [40]. A binary security patch dataset can also be constructed by compiling the source code from PatchDB.

2. **Automatic Patch Generation:**
   - Previous patch analysis studies have been limited by small datasets, constraining their ability to summarize fix patterns for common patch types, such as sanity testing. Our analysis of PatchDB in terms of code changes (Section IV-B-2) reveals many security patches with multiple fix patterns, enabling the summarization of more patch patterns.
   - Table VII shows two examples of fix patterns, race condition and data leakage, which have not been studied in previous work [24], [35], [38]. For race conditions, patches typically add and release locks to ensure atomicity. For data leakage, patches often release critical values after the last normal operation to prevent further vulnerabilities. A large-scale security patch dataset can help discover more complex patch patterns, facilitating the automatic generation of various types of security patches.

3. **Benchmark:**
   - PatchDB, being the largest-scale dataset of security patches to our knowledge, provides a benchmark closer to practical scenarios, broadening the evaluation spectrum. Collected from 313 GitHub repositories, it offers a robust test for the generalization capability of target techniques.

### Limitations and Future Work

Our current work focuses on C/C++ languages, which have the highest number of vulnerabilities [34]. The syntactic features identified in Table I are likely shared across different languages, allowing our system to be extended to other programming languages by customizing syntax parsing features. However, for safer languages like Rust, collecting a large security patch dataset may be challenging. We leave this extension for future work.

Similar to previous work [20], we assume that all information retrieved from the NVD is correct. However, up to 1% of patches may be incorrect, such as links to new versions containing multiple code differences. We consider this proportion small enough to be ignored in our analysis. The NVD may also be biased towards certain types of software, but given its wide range, it remains largely applicable for most open-source software.

### Related Works

#### Patch Datasets
- Many vulnerability detection studies construct security patch datasets. Kim et al. [18] acquire patches from eight well-known Git repositories, while Z. Li et al. [21] build a Vulnerability Patch Database (VPD) consisting of 19 products. However, these datasets are insufficient for machine learning-based studies and may introduce biases.
- F. Li et al. [20] built a large-scale security patch database by querying thousands of CVE records for open-source projects on the NVD. Xiao et al. [36] enriched the dataset with commits from industrial collaborators, but these datasets are not publicly accessible.
- Web-based patch or bug tracking systems like Patchwork [6] and Bugzilla [1] provide patch information, but they do not distinguish between security and non-security patches, limiting their utility.

#### Patch Analysis
- Recent works on patch analysis focus on textual information (e.g., bug reports, commit messages) using supervised and unsupervised learning techniques [13], [16], [43]. These methods struggle with inaccurate or missing documentation.
- At the source code level, Zhong et al. [42] conducted an empirical study on bug fixes from six popular Java projects, while Soto et al. [29] focused on patterns, replacements, deletions, and additions of bug fixes. Perl et al. [27] studied attributes of commits likely to introduce vulnerabilities, and Machiry et al. [25] analyzed safe patches. However, these works do not distinguish security patches from normal bug fixes.
- Zaman et al. [39] discovered differences between security and performance bugs in Mozilla Firefox. Li et al. [20] performed a large-scale empirical study of security patches versus non-security bug fixes, discussing metadata characteristics and life cycles.
- Some studies use machine learning models to identify patch types [31]â€“[33], but the lack of patch instances limits the application of robust classifiers. Most models are trained on single or multiple software projects, providing limited generalization. In contrast, our work provides a large dataset from over 300 GitHub repositories and uses a new oversampling method to increase variance at the source code level. At the binary level, Xu et al. [37] presented a scalable approach to identify the existence of security patches through semantic analysis of execution traces.

### Conclusion

In this work, we constructed a large-scale dataset of security patches called PatchDB. We developed a novel nearest link search approach to locate promising security patch candidates, reducing manual verification workload. We also proposed a new oversampling method to synthesize patches at the source code level, effectively increasing the variance of the patch dataset. Comprehensive experiments verified the effectiveness of our algorithms, showing that PatchDB is promising for facilitating patch analysis and vulnerability detection techniques.

### Acknowledgments

This work was partially supported by the US Department of the Army grant W56KGU-20-C-0008, the Office of Naval Research grants N00014-18-2893, N00014-16-1-3214, and N00014-20-1-2407, and the National Science Foundation grants CNS-1815650 and CNS-1822094.

### References

[References listed as provided in the original text]

---

This revised version aims to make the text more coherent, professional, and easier to understand.