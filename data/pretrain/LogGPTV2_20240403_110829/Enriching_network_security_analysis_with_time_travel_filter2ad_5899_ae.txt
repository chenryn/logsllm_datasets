of time from the timestamps of the captured packets. For example it
uses these timestamps to compute timer expirations and to manage
state. The simple solution of rewriting the timestamps to reﬂect the
current time confounds any analysis that relies on either absolute
time or on relative time between multiple connections. Such an
approach also has the potential to confuse the analyst that inspects
any timestamped or logged information.
The key insight for our solution, which enables us to integrate
the TM interface into Bro with minimal surgery, is to restrict Bro
to always request complete connections from the TM rather than
individual packets. Such a constraint is tenable because, like all
major NIDS, connections form Bro’s main unit of analysis.
We implement this constraint by ensuring that Bro only issues
queries in one of two forms: (i) for all packets with the same 4-tuple
(address1,port1,address2,port2), or (ii) for all packets involving a
particular address. In addition, to ensure that Bro receives all pack-
ets for these connections, including future ones, it subscribes to the
query (see §3.2).
Relying on complete connections simpliﬁes the problem of time-
stamps by allowing us to introduce the use of per-query network
times: for each TM query, Bro tracks the most recently received
packet in response to the query and then maintains separate per-
query timelines to drive the management of any timer whose in-
stantiation stems from a retrieved packet. Thus, TM packets do not
perturb Bro’s global timeline (which it continues to derive from the
timestamps of packets in its direct input stream).
We also rely on complete connections to address the issue of
replicated input. When retrieved packets for a connection begin
to arrive while Bro is processing the same connection via its live
feed, it discards the live version and starts afresh with the TM ver-
sion. (It also discards any future live packets for such connections,
since these will arrive via its TM subscription.) Moreover, if Bro
is processing packets of a connection via the TM and then receives
packets for this same connection via its live feed (unlikely, but not
impossible if the system’s packet capturing uses large buffers), then
Bro again ignores the live version. Finally, if Bro receives a connec-
tion multiple times from the TM (e.g., because of multiple match-
ing queries), it only analyzes the ﬁrst instance.
Our modiﬁcations to Bro provide the NIDS with a powerful in-
terface to the TM that supports forensics as well as automatic, retro-
spective analysis. The additions introduce minimal overhead, and
have no impact on Bro’s performance when it runs without a TM.
6. DEPLOYMENT TRADE-OFFS
In an actual deployment, the TM operator faces several trade-
offs in terms of CPU, memory, and disk requirements. The most
obvious trade-off is the design decision of foregoing complete stor-
age of high-volume connections in order to reduce memory/disk
consumption. There are others as well, however.
Risk of Evasion: The TM’s cutoff mechanism faces an obvious
risk for evasion: if an attacker delays his attack to occur after the
cutoff, the TM will not record the malicious actions. This is a fun-
damental limitation of our approach. However, short of compre-
hensively storing all packets, any volume reduction heuristic faces
such a blind spot.
The cutoff evasion problem is similar in risks to the problem
NIDS face when relying on timeouts for state management. If a
multi-step attack is stretched over a long enough time period such
that the NIDS is forced to expire its state in the interim the attack
can go undetected. Yet, to avoid memory exhaustion state must
be expired eventually. Therefore, NIDS rely on the fact that an
attacker cannot predict when exactly a timeout will take place [10].
Similarly, the TM has several ways for reducing the risk of eva-
sion by making the cutoff mechanism less predictable. One ap-
proach is to use different storage classes (see §3.1) with different
cutoffs for different types of trafﬁc, e.g., based on applications (for
some services, delaying an attack to later stages of a session is
harder than for others). As discussed in §5.2, we can also lever-
age a NIDS’s risk assessment to dynamically adjust the cutoff for
trafﬁc found more likely to pose a threat. Finally, we plan to exam-
ine randomizing the cutoff so that (i) an attacker cannot predict at
which point it will go into effect, and (ii) even when the cutoff has
been triggered, the TM may continue recording a random subset of
subsequent packets.
Network Load: When running in high-volume 10 Gbps environ-
ments, the TM can exceed the limits of what commodity hardware
can support in terms of packet-capture and disk utilization. We can
alleviate this impact with use of more expensive, special-purpose
hardware (such as the Endace monitoring card at MWN), but at
added cost and for limited beneﬁt. We note, however, that the TM
is well-suited for clustering in the same way as a NIDS [26]: we
can deploy a set of PCs, each running a separate TM on a slice of
the total trafﬁc. In such a distributed setting, an additional front-
end system can create the impression to the user of interacting with
a single TM by relaying to/from all backend TMs.
Floods: Another trade-off concerns packet ﬂoods, such as encoun-
tered during high-volume DoS attacks. Distributed ﬂoods stress
the TM’s connection-handling, and can thus undermine the cap-
ture of useful trafﬁc. For example, during normal operation at
MWN an average of 500,000 connections are active and stored in
the TM’s connection table. However, we have experienced ﬂoods
during which the number of connections increased to 3–4 million
within 30 seconds. Tracking these induced massive packet drops
and eventually exhausted the machine’s physical memory.
In addition, adversaries could attack the TM directly by exploit-
ing its speciﬁc mechanisms. They could for example generate large
numbers of small connections in order to signiﬁcantly reduce re-
tention time. However, such attacks require the attacker to commit
signiﬁcant resources, which, like other ﬂoods, will render them vul-
nerable to detection.
To mitigate the impact of ﬂoods on the TM’s processing, we plan
to augment the TM with a ﬂood detection and mitigation mech-
anism. For example, the system can probabilistically track per-
source thresholds of connection attempts and resort to address-
speciﬁc packet sampling once a source exceeds these. Alterna-
tively, when operating in conjunction with a NIDS that includes
a ﬂood detection mechanism, the TM can rely upon the NIDS to
decide when and how the TM should react.
Retrieval Time: When running a joint TM/NIDS setup, we need
to consider a trade-off between the response time for answering
a query versus the time range that the TM examines to ﬁnd the
relevant packets. As discussed in §4.2, the TM can answer queries
quite quickly as long as it restricts its retrieval to in-memory data.
However, once the TM needs to search its disk, queries can take
seconds to minutes, even if they include a time interval to limit
the scope of the search. Thus, the NIDS must issue such queries
carefully so as to not exhaust the TM’s resources. We do not yet
have enough long-term operational experience to have determined
good rules for how to manage such queries, but this is part of the
near-term focus of our future work.
NIDS and Cutoff: A ﬁnal issue concerns the impact of the TM’s
cutoff on the NIDS processing. In our NIDS implementation, we
minimize the limitations resulting from the cutoff by combining
each NIDS query with a request to remove the cutoff for the asso-
ciated connections or addresses. This takes care of future activity
via the TM. But there is little we can do about ﬂows curtailed in the
past—those for which the TM already applied the cutoff. Recall
that the general premise of the TM is that we usually can operate
the TM with a sufﬁciently large cutoff that information of interest
is captured. To further reduce this problem the TM always stores
all TCP control packets (SYN/FIN/RST), thus enabling a NIDS to
perform its connection-level analysis.
7. RELATED WORK
In this paper we develop a “Time Machine” for efﬁcient network
packet recording and retrieval, and couple the resulting system with
a NIDS. The basic approach is to leverage the heavy-tailed nature
of Internet trafﬁc [17, 19] to signiﬁcantly reduce the volume of
bulk trafﬁc recording. Our work builds extensively on the proof-
of-principle prototype we describe in [15], greatly increasing its
performance and coupling it to external systems to support auto-
mated querying, live connections to a NIDS, and “subscriptions”
to future packets satisfying a given query. These features have en-
abled us to then use the system in conjunction with the Bro NIDS
in an operational high-volume setting.
Different approaches have been suggested in the literature to
record high-volume network trafﬁc. First, several systems aim to
record full packet traces: Anderson et al. [1] records at kernel-level
to provide bulk capture at high rates, and Antonelli et al. [2] focuses
on long-term archive and stores trafﬁc on tapes. However, these
systems do not provide automated and efﬁcient real-time query in-
terfaces. Hyperion [9] employs a dedicated stream ﬁle system to
store high-volume data streams, indexing stream data using Bloom
ﬁlters. Hyperion bulk-records entire trafﬁc streams, and does not
a provide features for automatic or semi-automatic forensics nor
coupling with a NIDS. Gigascope [8], on the other hand, supports
SQL-like queries on a packet stream, but no long-term archiving.
Another approach is to store higher-level abstractions of the net-
work trafﬁc to reduce the data volume: Reiss et al. [21] record
ﬂows and provide real-time query facilities; Shanmugasundaram
et al. [23] record key events of activity such as connections and
scans; and [3, 16] both provide frameworks suitable for performing
different data reduction techniques.
([16] is based on the CoMo
platform [6]). Cooke et al. [7] aggregate data as it ages: ﬁrst pack-
ets are stored; these are than transformed into ﬂows. They focus on
storage management algorithms to divide storage between the dif-
ferent aggregation levels. Ponec et al. [20] store trafﬁc digests for
payload attribution; the queries do not yield the actual content. Any
data reduction decreases the amount of information available. We
argue that for security applications, the TM’s approach of archiving
the head of connections at the packet-level provides an attractive
degree of detail compared to such abstractions.
Reducing trafﬁc volume by omitting parts of the trafﬁc is em-
ployed by the Shunt [13]. The Shunt is a programmable NIC that
an associated NIDS/NIPS instructs to forward, drop, or divert pack-
ets at a per-connection granularity. The drop functionality supports
intrusion prevention; the forward functionality supports ofﬂoading
the NIDS for streams it has determined it will forego analyzing; and
the divert functionality allows the NIDS to inspect and potentially
intercede any trafﬁc it wishes. The TM could leverage the Shunt to
impose the cutoff directly on the NIC.
While intrusion detection systems such as Snort [22] and
Bro [18] can record trafﬁc, they typically keep only a small sub-
set of the network’s packets; for Snort, just those that triggered an
alert, and for Bro just those that the system selected for analysis.
Neither system—nor any other of which we are aware—can incor-
porate network trafﬁc recorded in the past into their live analysis.
We added this capability to the Bro system.
Commercial vendors, e.g., [4, 14, 12], offer a number of packet
recorders. Due to their closed nature, it is difﬁcult to construct a
clear picture of their capabilities and performances. As far as we
can tell, none of these has been coupled with a NIDS.
Finally, the notion of “time travel” has been discussed in other
contexts of computer forensics. For instance, ReVirt [11] can re-
construct past states of a virtual machine at the instruction-level.
8. CONCLUSION
In this work we explore the signiﬁcant capabilities attainable for
network security analysis via Time Travel, i.e., the ability to quickly
access past network trafﬁc for network analysis and security foren-
sics. This approach is particular powerful when integrating trafﬁc
from the past with a real-time NIDS’s analysis. We support Time
Travel via the Time Machine (TM) system, which stores network
trafﬁc in its most detailed form, i.e., as packets. The TM provides a
remote control-and-query interface to automatically request stored
packets and to dynamically adapt the TM’s operation parameters.
To reduce the amount of trafﬁc stored, the TM leverages a simple
but effective “cutoff” heuristic: it only stores the ﬁrst N bytes of
each connection (typically, N = 10–20 KB). This approach lever-
ages the heavy-tailed nature of network trafﬁc to capture the great
majority of connections in their entirety, while omitting storage of
the vast majority of the bytes in a trafﬁc stream.
We show that the TM allows us to buffer most connections
completely for minutes in memory, and on disk for days, even in
10 Gbps network environments, using only commodity hardware.
The cutoff heuristic reduces the amount of data to store to less than
10% of the original trafﬁc. We add TM support to the open-source
Bro NIDS, and examined a number of applications (controlling the
TM, correlating NIDS alarms with associated packet data, and ret-
rospective analysis) that such integration enables. In addition, we
explore the technical subtleties that arise when injecting recorded
network trafﬁc into a NIDS that is simultaneously analyzing live
trafﬁc. Our evaluation using traces as well as live trafﬁc from two
large sites ﬁnds that the combined system can process up to 120 ret-
rospective queries per second, and can potentially analyze trafﬁc
seen 4–15 days in the past, using affordable memory and disk re-
sources.
Our previous proof-of-principle TM implementation has been
in operational use for several years at LBNL. The new,
joint
TM/NIDS installation is now running there continuously in a pro-
totype setup, and the site’s operators are planning to integrate it into
their operational security monitoring.
To further improve performance in high-volume environments,
we plan to develop a version of the system that implements cutoff
processing in dedicated hardware (such as the Shunt FPGA [13])
or in the kernel, in order to reduce the trafﬁc volume as early as
possible. Using ideas from [7], we also plan to further extend the
period we can “travel back in time” by aggregating packet data
into higher level representations (e.g., ﬂows) once evicted from the
TM’s buffers.
Overall, we have found that retrospective analysis requires a
great deal of experience with a TM/NIDS setup in operational
environments to identify the most useful applications, especially
considering the trade-offs discussed in §6. Now that we have the
TM/NIDS hybrid in place, the next step is to pursue a study of these
possibilities.
9. ACKNOWLEDGMENTS
We would like to thank Stefan Kornexl for implementing the ear-
lier TM prototype, Christian Kreibich for helping with the Broccoli
interface, Jim Mellander for sharing his experiences with the TM’s
deployment, and the anonymous reviewers for their valuable com-
ments. We would also like to thank the Lawrence Berkeley Na-
tional Laboratory; the Leibniz-Rechenzentrum, München; and the
University of California, Berkeley. This work was supported in
part by NSF Awards STI-0334088, NSF-0433702, CNS-0627320,
CNS-0716640, CNS-0722035, a grant from the Bavaria California
Technology Center, and a grant from Deutsche Telekom Labora-
tories, Berlin. Any opinions, ﬁndings, and conclusions or recom-
mendations expressed in this material are those of the authors or
originators and do not necessarily reﬂect the views of the National
Science Foundation.
10. REFERENCES
[1] ANDERSON, E., AND ARLITT, M. Full Packet Capture and
Ofﬂine Analysis on 1 and 10 Gb/s Networks. Tech. Rep.
HPL-2006-156, HP Labs, 2006.
[2] ANTONELLI, C., CO, K., M FIELDS, AND HONEYMAN, P.
Cryptographic Wiretapping at 100 Megabits. In SPIE 16th
Int. Symp. on Aerospace Defense Sensing, Simulation, and
Controls. (2002).
[3] CHANDRASEKARAN, S., AND FRANKLIN, M.
Remembrance of Streams Past: Overload-sensitive
Management of Archived Streams. In Proc. Very Large Data
Bases (2004).
[4] ClearSight Networks. http://www.clearsightnet.com.
[5] CNET NEWS. Another suspected NASA hacker indicted.
http://www.news.com/2102-7350_3-6140001.html.
[6] CoMo. http://como.sourceforge.net.
[7] COOKE, E., MYRICK, A., RUSEK, D., AND JAHANIAN, F.
Resource-aware Multi-format Network Security Data
Storage. In Proc. SIGCOMM LSAD workshop (2006).
[8] CRANOR, C., JOHNSON, T., AND SPATSCHECK, O.
Gigascope: A Stream Database for Network Applications. In
Proc. SIGMOD (2003).
[9] DESNOYERS, P., AND SHENOY, P. J. Hyperion: High
Volume Stream Archival for Retrospective Querying. In
Proc. 2007 USENIX Technical Conf (2007).
[10] DREGER, H., FELDMANN, A., PAXSON, V., AND
SOMMER, R. Operational Experiences with High-Volume
Network Intrusion Detection. In Proc. 11th ACM Conf. on
Comp. and Comm. Security (2004).
[11] DUNLAP, G. W., KING, S. T., CINAR, S., BASRAI, M. A.,
AND CHEN, P. M. ReVirt: Enabling Intrusion Analysis
through Virtual-Machine Logging and Replay. In Proc.
Symp. on Operating Systems Design and Implementation
(2002).
[12] ENDACE MEASUREMENT SYSTEMS.
http://www.endace.com/, 2008.
[13] GONZALEZ, J. M., PAXSON, V., AND WEAVER, N.
Shunting: A Hardware/Software Architecture for Flexible,
High-performance Network Intrusion Prevention. In Proc.
14th ACM Conf. on Comp. and Comm. Security (2007).
[14] Intelica Networks. http://www.intelicanetworks.com.
[15] KORNEXL, S., PAXSON, V., DREGER, H., FELDMANN, A.,
AND SOMMER, R. Building a Time Machine for Efﬁcient
Recording and Retrieval of High-Volume Network Trafﬁc
(Short Paper). In Proc. ACM SIGCOMM IMC (2005).
[16] MCGRATH, K. P., AND NELSON, J. Monitoring & Forensic
Analysis for Wireless Networks. In Proc. Conf. on Internet
Surveillance and Protection (2006).
[17] PARK, K., KIM, G., AND CROVELLA, M. On the
Relationship Between File Sizes, Transport Protocols, and
Self-similar Network Trafﬁc. In Proc. ICNP ’96 (1996).
[18] PAXSON, V. Bro: A System for Detecting Network Intruders
in Real-Time. Comp. Networks 31, 23–24 (1999).
[19] PAXSON, V., AND FLOYD, S. Wide-Area Trafﬁc: The
Failure of Poisson Modeling. IEEE/ACM Transactions on
Networking 3, 3 (1995).
[20] PONEC, M., GIURA, P., BRÖNNIMANN, H., AND WEIN, J.
Highly Efﬁcient Techniques for Network Forensics. In Proc.
14th ACM Conf. on Comp. and Comm. Security (2007).
[21] REISS, F., STOCKINGER, K., WU, K., SHOSHANI, A.,
AND HELLERSTEIN, J. M. Enabling Real-Time Querying of
Live and Historical Stream Data. In Proc. Statistical &
Scientiﬁc Database Management (2007).
[22] ROESCH, M. Snort – Lightweight Intrusion Detection for
Networks. In Proc. 13th Systems Administration Conference
- LISA ’99 (1999), pp. 229–238.
[23] SHANMUGASUNDARAM, K., MEMON, N., SAVANT, A.,
AND BRÖNNIMANN, H. ForNet: A Distributed Forensics
Network. In Proc. Workshop on Math. Methods, Models and
Architectures for Comp. Networks Security (2003).
[24] SOMMER, R. Viable Network Intrusion Detection in
High-Performance Environments. PhD thesis, TU München,
2005.
[25] SOMMER, R., AND PAXSON, V. Exploiting Independent
State For Network Intrusion Detection. In Proc. Computer
Security Applications Conf. (2005).
[26] VALLENTIN, M., SOMMER, R., LEE, J., LERES, C.,
PAXSON, V., AND TIERNEY, B. The NIDS Cluster:
Scalable, Stateful Network Intrusion Detection on
Commodity Hardware. In Proc. 10th Int. Symp. Recent
Advances in Intrusion Detection (RAID) (2007).
[27] WALLERICH, J., DREGER, H., FELDMANN, A.,
KRISHNAMURTHY, B., AND WILLINGER, W. A
Methodology for Studying Persistency Aspects of Internet
Flows. ACM SIGCOMM CCR 35, 2 (Apr 2005), 23–36.