mail.ru
shaw.ca
wanadoo.fr
msn.com
Total
FREQ.
8.47%
5.05%
3.17%
2.37%
1.13%
0.93%
0.86%
0.61%
0.61%
0.58%
23.79%
Table 2: The 10 most-targeted e-mail address domains and
their frequency in the combined lists of targeted addresses over
all three campaigns.
5. EXPERIMENTAL RESULTS
We now present the overall results of our rewriting experiment.
We ﬁrst describe the spam workload observed by our C&C rewrit-
ing proxy. We then characterize the effects of ﬁltering on the spam
workload along the delivery path from worker bots to user inboxes,
as well as the number of users who browse the advertised Web sites
and act on the content there.
5.1 Campaign datasets
Our study covers three spam campaigns summarized in Table 1.
The “Pharmacy” campaign is a 26-day sample (19 active days) of
an on-going Storm campaign advertising an on-line pharmacy. The
“Postcard” and “April Fool” campaigns are two distinct and serial
instances of self-propagation campaigns, which attempt to install
an executable on the user’s machine under the guise of being post-
card software. For each campaign, Figure 4 shows the number of
messages per hour assigned to bots for mailing.
Storm’s authors have shown great cunning in exploiting the cul-
tural and social expectations of users — hence the April Fool cam-
paign was rolled out for a limited run around April 1st. Our Web
site was designed to mimic the earlier Postcard campaign and thus
our data probably does not perfectly reﬂect user behavior for this
campaign, but the two are similar enough in nature that we surmise
that any impact is small.
We began the experiment with 8 proxy bots, of which 7 survived
until the end. One proxy crashed late on March 31. The total num-
ber of worker bots connected to our proxies was 75,869.
Figure 5 shows a timeline of the proxy bot workload. The num-
ber of workers connected to each proxy is roughly uniform across
Mar 07Mar 12Mar 17Mar 22Mar 27Apr 01Apr 06Apr 11Apr 1600.511.522.53DateEmails assigned per hour (millions)  PostcardPharmacyApril FoolMar 24Mar 29Apr 02Apr 06Apr 10Apr 140100200300400500600TimeNumber of connected workers  Proxy 1Proxy 2Proxy 3Proxy 4Proxy 5Proxy 6Proxy 7Proxy 8Figure 6: The spam conversion pipeline.
STAGE
A – Spam Targets
B – MTA Delivery (est.)
C – Inbox Delivery
D – User Site Visits
E – User Conversions
PHARMACY
100%
23.8%
347,590,389
82,700,000
— —
10,522
28
0.00303%
0.0000081%
POSTCARD
83,655,479
21,100,000
100%
25.2%
— —
3,827
316
0.00457%
0.000378%
APRIL FOOL
100%
25.2%
40,135,487
10,100,000
— —
2,721
225
0.00680%
0.000561%
Table 3: Filtering at each stage of the spam conversion pipeline for the self-propagation and pharmacy campaigns. Percentages refer
to the conversion rate relative to Stage A.
all proxies (23 worker bots on average), but shows strong spikes
corresponding to new self-propagation campaigns. At peak, 539
worker bots were connected to our proxies at the same time.
Most workers only connected to our proxies once: 78% of the
workers only connected to our proxies a single time, 92% at most
twice, and 99% at most ﬁve times. The most proliﬁc worker IP
address, a host in an academic network in North Carolina, USA,
contacted our proxies 269 times; further inspection identiﬁed this
as a NAT egress point for 19 individual infections. Conversely,
most workers do not connect to more than one proxy: 81% of the
workers only connected to a single proxy, 12% to two, 3% to four,
4% connected to ﬁve or more, and 90 worker bots connected to all
of our proxies. On average, worker bots remained connected for
40 minutes, although over 40% workers connected for less than a
minute. The longest connection lasted almost 81 hours.
The workers were instructed to send postcard spam to a to-
tal of 83,665,479 addresses, of which 74,901,820 (89.53%) are
unique. The April Fool campaign targeted 38,651,124 addresses,
of which 36,909,792 (95.49%) are unique. Pharmacy spam tar-
geted 347,590,389 addresses, of which 213,761,147 (61.50%) are
unique. Table 2 shows the 15 most frequently targeted domains
of the three campaigns. The individual campaign distributions are
identical in ordering and to a precision of one tenth of a percentage,
therefore we only show the aggregate breakdown.
5.2 Spam conversion pipeline
Conceptually, we break down spam conversion into a pipeline
with ﬁve “ﬁltering” stages in a manner similar to that described by
Aycock and Friess [6]. Figure 6 illustrates this pipeline and shows
the type of ﬁltering at each stage. The pipeline starts with delivery
lists of target e-mail addresses sent to worker bots (Stage A). For
a wide range of reasons (e.g., the target address is invalid, MTAs
refuse delivery because of blacklists, etc.), workers will success-
fully deliver only a subset of their messages to an MTA (Stage B).
SPAM FILTER
Gmail
Yahoo
Hotmail
Barracuda
POSTCARD APRIL FOOL
PHARMACY
0.00226%
0.00683%
0.00176%
0.00173% 0.000542%
none
none
none
N/A
0.00826%
none
0.131%
Table 4: Number of messages delivered to a user’s inbox as
a fraction of those injected for test accounts at free e-mail
providers and a commercial spam ﬁltering appliance. The test
account for the Barracuda appliance was not included in the
Postcard campaign.
At this point, spam ﬁlters at the site correctly identify many mes-
sages as spam, and drop them or place them aside in a spam folder.
The remaining messages have survived the gauntlet and appear in
a user’s inbox as valid messages (Stage C). Users may delete or
otherwise ignore them, but some users will act on the spam, click
on the URL in the message, and visit the advertised site (Stage D).
These users may browse the site, but only a fraction “convert” on
the spam (Stage E) by attempting to purchase products (pharmacy)
or by downloading and running an executable (self-propagation).
We show the spam ﬂow in two parts, “crawler” and “converter”,
to differentiate between real and masquerading users (Section 4.4).
For example, the delivery lists given to workers contain honeypot
e-mail addresses. Workers deliver spam to these honeypots, which
then use crawlers to access the sites referenced by the URL in the
messages (e.g., our own Spamscatter project [3]). Since we want
to measure the spam conversion rate for actual users, we separate
out the effects of automated processes like crawlers — a necessary
aspect of studying an artifact that is also being actively studied by
other groups [12].
Table 3 shows the effects of ﬁltering at each stage of the con-
version pipeline for both the self-propagation and pharmaceutical
campaigns. The number of targeted addresses (A) is simply the to-
ABCDEtargetedaddressesemail not deliveredblocked by spam ﬁlterignoredby useruser left sitecrawlerconvertertal number of addresses on the delivery lists received by the worker
bots during the measurement period, excluding the test addresses
we injected.
We obtain the number of messages delivered to an MTA (B)
by relying on delivery reports generated by the workers. Unfor-
tunately, an exact count of successfully delivered messages is not
possible because workers frequently change proxies or go ofﬂine,
causing both extraneous (resulting from a previous, non-interposed
proxy session) and missing delivery reports. We can, however, es-
timate the aggregate delivery ratio (B/A) for each campaign using
the success ratio of all observed delivery reports. This ratio allows
us to then estimate the number of messages delivered to the MTA
and even to do so on a per-domain basis.
The number of messages delivered to a user’s inbox (C) is a
much harder value to estimate. We do not know what spam ﬁl-
tering, if any, is used by each mail provider, and then by each user
individually, and therefore cannot reasonably estimate this number
in total. It is possible, however, to determine this number for in-
dividual mail providers or spam ﬁlters. The three mail providers
and the spam ﬁltering appliance we used in this experiment had a
method for separating delivered mails into “junk” and inbox cat-
egories. Table 4 gives the number of messages delivered a user’s
inbox for the free e-mail providers, which together accounted for
about 16.5% of addresses targeted by Storm (Table 2), as well as
our department’s commercial spam ﬁltering appliance. It is impor-
tant to note that these are results from one spam campaign over a
short period of time and should not be used as measures of the rel-
ative effectiveness for each service. That said, we observe that the
popular Web mail providers all do a very a good job at ﬁltering the
campaigns we observed, although it is clear they use different meth-