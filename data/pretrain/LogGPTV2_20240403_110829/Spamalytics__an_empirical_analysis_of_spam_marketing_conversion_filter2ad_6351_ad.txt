### Table 2: The 10 Most-Targeted Email Address Domains and Their Frequency in the Combined Lists of Targeted Addresses Over All Three Campaigns

| Domain        | Frequency (%) |
|---------------|---------------|
| mail.ru       | 8.47%         |
| shaw.ca       | 5.05%         |
| wanadoo.fr    | 3.17%         |
| msn.com       | 2.37%         |
| Total         | 23.79%        |

### 5. Experimental Results

In this section, we present the overall results of our rewriting experiment. We first describe the spam workload observed by our Command and Control (C&C) rewriting proxy. Then, we characterize the effects of filtering on the spam workload along the delivery path from worker bots to user inboxes, as well as the number of users who browse the advertised websites and act on the content there.

#### 5.1 Campaign Datasets

Our study covers three spam campaigns, summarized in Table 1:

1. **Pharmacy Campaign**: A 26-day sample (19 active days) of an ongoing Storm campaign advertising an online pharmacy.
2. **Postcard Campaign**: A self-propagation campaign that attempts to install an executable on the user’s machine under the guise of postcard software.
3. **April Fool Campaign**: Another self-propagation campaign, similar to the Postcard campaign but rolled out for a limited run around April 1st.

For each campaign, Figure 4 shows the number of messages per hour assigned to bots for mailing.

The authors of the Storm botnet have demonstrated significant ingenuity in exploiting cultural and social expectations. For example, the April Fool campaign was launched around April 1st. Our website was designed to mimic the earlier Postcard campaign, so our data may not perfectly reflect user behavior for the April Fool campaign. However, the two campaigns are similar enough that we believe any impact is minimal.

We began the experiment with 8 proxy bots, of which 7 survived until the end. One proxy crashed late on March 31. The total number of worker bots connected to our proxies was 75,869.

Figure 5 shows a timeline of the proxy bot workload. The number of workers connected to each proxy is roughly uniform across all proxies (23 worker bots on average), with strong spikes corresponding to new self-propagation campaigns. At peak, 539 worker bots were connected to our proxies simultaneously.

Most workers only connected to our proxies once: 78% of the workers connected only once, 92% at most twice, and 99% at most five times. The most prolific worker IP address, a host in an academic network in North Carolina, USA, contacted our proxies 269 times; further inspection identified this as a NAT egress point for 19 individual infections. Conversely, most workers do not connect to more than one proxy: 81% of the workers connected to a single proxy, 12% to two, 3% to four, 4% to five or more, and 90 worker bots connected to all of our proxies. On average, worker bots remained connected for 40 minutes, although over 40% of workers connected for less than a minute. The longest connection lasted almost 81 hours.

The workers were instructed to send postcard spam to a total of 83,665,479 addresses, of which 74,901,820 (89.53%) are unique. The April Fool campaign targeted 38,651,124 addresses, of which 36,909,792 (95.49%) are unique. The Pharmacy campaign targeted 347,590,389 addresses, of which 213,761,147 (61.50%) are unique. Table 2 shows the 15 most frequently targeted domains of the three campaigns. The individual campaign distributions are identical in ordering and to a precision of one-tenth of a percentage, so we only show the aggregate breakdown.

#### 5.2 Spam Conversion Pipeline

Conceptually, we break down the spam conversion process into a pipeline with five "filtering" stages, similar to the approach described by Aycock and Friess [6]. Figure 6 illustrates this pipeline and shows the type of filtering at each stage.

1. **Stage A - Spam Targets**: Delivery lists of target email addresses sent to worker bots.
2. **Stage B - MTA Delivery (Est.)**: Workers successfully deliver a subset of their messages to an MTA. For various reasons (e.g., invalid target address, blacklisted MTAs), not all messages are delivered.
3. **Stage C - Inbox Delivery**: Spam filters at the site correctly identify many messages as spam and drop them or place them in a spam folder. The remaining messages appear in the user’s inbox.
4. **Stage D - User Site Visits**: Users may delete or ignore the messages, but some will click on the URL in the message and visit the advertised site.
5. **Stage E - User Conversions**: These users may browse the site, but only a fraction will "convert" by attempting to purchase products (pharmacy) or by downloading and running an executable (self-propagation).

We differentiate between real and masquerading users by showing the spam flow in two parts: "crawler" and "converter." For example, the delivery lists given to workers contain honeypot email addresses. Workers deliver spam to these honeypots, which then use crawlers to access the sites referenced by the URLs in the messages (e.g., our own Spamscatter project [3]). Since we want to measure the spam conversion rate for actual users, we separate out the effects of automated processes like crawlers, which is necessary when studying an artifact that is also being actively studied by other groups [12].

Table 3 shows the effects of filtering at each stage of the conversion pipeline for both the self-propagation and pharmaceutical campaigns. The number of targeted addresses (A) is simply the total number of addresses on the delivery lists received by the worker bots during the measurement period, excluding the test addresses we injected.

We obtain the number of messages delivered to an MTA (B) by relying on delivery reports generated by the workers. Unfortunately, an exact count of successfully delivered messages is not possible because workers frequently change proxies or go offline, causing both extraneous and missing delivery reports. We can, however, estimate the aggregate delivery ratio (B/A) for each campaign using the success ratio of all observed delivery reports. This ratio allows us to estimate the number of messages delivered to the MTA, even on a per-domain basis.

The number of messages delivered to a user’s inbox (C) is much harder to estimate. We do not know what spam filtering, if any, is used by each mail provider and each user individually, and therefore cannot reasonably estimate this number in total. However, it is possible to determine this number for individual mail providers or spam filters. The three mail providers and the spam filtering appliance we used in this experiment had a method for separating delivered mails into "junk" and inbox categories. Table 4 gives the number of messages delivered to a user’s inbox for the free email providers, which together accounted for about 16.5% of addresses targeted by Storm (Table 2), as well as our department’s commercial spam filtering appliance. It is important to note that these are results from one spam campaign over a short period of time and should not be used as measures of the relative effectiveness for each service. That said, we observe that the popular webmail providers all do a very good job at filtering the campaigns we observed, although they use different methods.