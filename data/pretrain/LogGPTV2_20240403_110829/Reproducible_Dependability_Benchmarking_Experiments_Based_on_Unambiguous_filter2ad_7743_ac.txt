side and around a computer, such as central processing
unit (CPU), memory, video and network card, harddisks,
keyboard and mouse, etc., and the ”buses” interconnecting
these components, such as frontside, ISA, IDE and serial
bus, etc. This library can easily be extended to accommo-
date other types of components. Each component may be
parametrized with additional identifying information, such
as vendor and product name or unit number. Each hardware
component also has the inherent possibility to fail (see sec-
tion 2.2). This aspect is important for describing depend-
ability benchmarks, which contain a faultload in addition to
the workload known from pure performance benchmarks.
Figure 8 shows an overview of the complete system. The
computer
serial terminal
ethnernet cable
serial line
system under test
driver system
admin
terminal
logging
terminal
admin
terminal
db
database
server
rdb
reference
database
server
admin
terminal
admin
terminal
admin
terminal
admin
terminal
as1
app
server
as2
app
server
as3
app
server
as4
app
server
terminal
terminal
terminal
terminal
terminal
terminal
terminal
terminal
terminal
terminal
terminal
terminal
Figure 8. System Setup Overview
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. 
SUT is shaded light grey, the driver system generating the
workload is shaded a darker grey. The SUT consists of two
database servers (db and rdb) and four application servers
(as1 to as4). db is the primary database server, rdb
is a reference server. The target of the benchmark is the
software (operating system and database software) running
on db. The reference server, which remains fault free, is
needed to compare the output from db to the correct out-
put (see also section 4.4). The database and application
servers are connected with an ethernet local area network
(solid lines in ﬁgure 8). All servers have a serial terminal
connected, which is used to take administrative actions as is
necessary. The primary server also has a second serial ter-
minal which prints alert messages from the database soft-
ware for later analysis. The application servers each have
three additional serial terminals connected. These are the
end-user terminals. Virtual users sitting at these serial ter-
minals generate the workload.
4.2. Workload
Performance and dependability measures can only be ac-
quired, if the target system performs the services of interest
to the user. In [16] these are transactions related to the pro-
cessing of orders in a ﬁctitious wholesale supplier (clause
1.1). The workload is a mixture of read-only and update-
intensive transactions on the company’s database which
simulate the activities found in complex online transaction
processing application environments.
Fictitious users enter ﬁve different transaction types.
These are deﬁned in [16] clauses 2.4 to 2.8. Four types of
transactions are concerned with handling orders, entering a
complete new order through a single transaction (new-order
transaction), querying the status of an order (order-status
transaction), registering a payment (payment transaction) or
a delivery (delivery transaction). Finally, the stock-level of
the warehouses must be controlled (stock-level transaction).
The new-order, order-status and payment transactions must
complete within a small timeframe to satisfy online users.
The stock-level transaction has a relaxed timeframe require-
ment. The delivery transaction is executed in batch mode,
the user only initiates the queueing of this transaction for
deferred execution.
4.3. Hardware Faultload
We have chosen to include only high-level storage hard-
ware and network faults in our faultload. Both storage and
networking hardware are an integral part of a client-server
database and online transaction processing systems and are
explicitly mentioned in [16]. Disk faults are explicitly listed
in [16] clause 3.5.3 as a single point of failure which may
not destroy the required properties listed in [16] clause 3.
We have considered the following three scenarios:
1. Permanent network send and receive losses (ﬁxed per-
centage) to and from the database server db. The per-
centage was varied in steps of ﬁve percent between 0
and 50 percent and in steps of one percent between 50
and 70 percent. This information is passed to the au-
tomatic experiment controller as a parameter. The net-
work was faulty during the complete duration of the
experiment.
2. Short total network outage to and from the database
server db. The duration of the network outage was
passed to the automatic experiment controller as a pa-
rameter and was varied in steps of one minute from 0
to 20 minutes. It began three minutes after the start of
the experiment. The experiment ran for a minimum of
15 minutes and until at least three minutes after the end
of the network outage.
3. Permanent disk faults on the data disk of the database
server db. The disk faults have a random location on
the disk. They are triggered three minutes after the
start of the experiment.
4.4. Measures and Measuring
Document [16] deﬁnes the measure ”Maximum Qual-
iﬁed Throughput” in clause 5.4.2 as the total number of
completed new-order transactions. The name of this met-
ric is tpmC. All measures are extracted from ﬁles logged
by our automatic experiment controller Expect. Expect ob-
serves the screen output of the servers and user terminals.
The VHDL setup contains the patterns which Expect should
match (such as the expected reply or an error message) be-
fore deciding on the next action.
The application server software writes log information
on the control terminals (shaded terminals in ﬁgure 8). The
log ﬁles contain the results of the consistency checks with
the reference database, the database logs of the delivery
transaction and any warnings and errors in the database out-
puts.
Measurements regarding database throughput and re-
sponse times are produced by VHDL assertion statements.
Expect timestamps these log entries automatically as it
writes them out to a ﬁle. The analysis of these ﬁles is done
automatically by shell scripts tailored to the different fault-
loads.
The repeatability of an experiment run can be assured
by using identical fault- and workloads. The workload is
ﬁxed and fully deﬁned in the VHDL code for the experi-
ment controller. The faultload is parametrized. That is, the
VHDL control code contains parameters, for example for
the network receive loss percentage or the duration of the
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. 
C
m
p
t
 3
 2.5
 2
 1.5
 1
 0.5
 0
 0
send
receive
 10
 20
 30
 40
 50
 60
 70
Network send/receive loss percentage
C
m
p
t
 3
 2.5
 2
 1.5
 1
 0.5
 0
 0
network outage
 5
 10
 15
 20
Network outage (minutes)
Figure 9. Results of Network Failures
total network outage. Expect is passed these parameters as
it is called and then generates the faultload from the model.
An identical faultload can easily be realized, as each fault-
load is fully described with only a few parameters (the rest
of the faultload deﬁnition being contained in the VHDL-
ﬁle). The most problematic part in regard to reproducibility
is the workload, due to its massively parallel nature. It is
possible, however, to generate identical workloads by re-
ducing the number of active terminals to one and using a
pseudo random number generator with a ﬁxed seed value to
generate the random user input necessary for [16].
4.5. Results
The left hand side of ﬁgure 9 shows the effects of the net-
work losses at the send and receive line, respectively, on the
tpmC. The tpmC decreases slowly until a loss percentage of
about 51% percent, where a steep decrease occurs. This is
due to the fault-tolerance properties of the transmission con-
trol protocol (TCP) which is used as transport protocol for
the database connections. Until about half of the packets
are lost, TCP receives an acknowledge before the backoff
times between successive retries become so long that a per-
formance decrease becomes visible. On a lower protocol
layer, measurements in real systems have shown, that com-
munication on networks running the ethernet protocol at the
data-link layer, comes to a standstill when more than 80%
of the packets have collisions. This ﬁts pretty well with our
data.
These measurements produced an additional result,
which indicates that the network protocol utilized by the
database is more sensitive against losses on the receive line
than on the send line.
The right hand side of ﬁgure 9 shows how network out-
ages affect the system. As is expected, the measure tpmC
decreases linearly with the length of the outage.
If the
network is down for more than a quarter of an hour, the
database server is disconnected (the application servers then
report SQL errors ORA-03113 and ORA-03114) when the
network comes up again. No transactions are processed. In
the default vendor conﬁguration used in the experiments,
the database server remains disconnected and does not try
to reestablish the network connection to the application
servers during the rest of the experiment.
Table 1 summarizes the results from the disk failure
faultload (section 4.3 item 3).
result
no errors
total database failure
some database errors
no errors but database complained
percentage
82.5
12.6
3.1
1.8
Table 1. Results of Disk Failures
In over 80 percent of the cases, the faultload had no
visible effect on the operation of the database, i.e.
the
defect parts of the harddisk were probably not used by
the database. A signiﬁcant number of cases led to a total
database failure. The application servers mainly posted the
error codes ORA-03113 and ORA-03114 (unexpected com-
munication disconnect from server), which point out that
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply. 
the database shut down before informing the user about the
problems. Only in one case ORA-01092 was posted, prop-
erly indicating an emergency shutdown of the database. In
contrast to this, the alert log always contained proper error
messages, so at least a database administrator should easily
be able to ﬁnd out about the nature of the problem. Some
database errors were observed in about three percent of the
cases. In these cases the database recorded erroneous data
but an error message was logged. The affected transactions
were rolled back correctly. The posted error codes were
ORA-01110 and ORA-01115, indicating a corrupt dataﬁle.
In very few cases the databases fault-tolerance mechanisms
were able to recognize that a part of the harddisk was de-
fect and use intact parts to store the data. No data was lost
and no errors were observed. The database informed the
administrator about the disk problem in the alert log.
5. Conclusion
This paper presents a VHDL- based method to seman-
tically unambiguously and completely describe a perfor-
mance or dependability benchmark conﬁguration. Such a
description is necessary to achieve repeatability and repro-
ducibility of benchmark results. Reproducibility of results
by different teams enhances the credibility of the bench-
mark. To automize benchmark setup and execution we have
implemented a prototype experiment controller which eval-
uates the VHDL conﬁguration description and runs the ex-
periments. The results of our experiments were also pre-
sented.
Acknowledgement
The research presented in this paper is supported by the
European Community (DBench project, IST-2000-25425).
We want to thank all the other people who contributed to
our benchmarking environment UMLinux.
References
[1] I. S. 1076-2002. IEEE Standard VHDL language reference
manual. Product No.: SH94983-TBR, 2002.
[2] 2U Consortium.
Unambiguous UML.
URL:
http://www.2uworks.org/, 2003.
[3] K. Buchacker, M. Dal Cin, H. H¨oxer, V. Sieh, and
O. Tsch¨ache. Reproducible dependability benchmarking ex-
periments based on unambiguous benchmark setup descrip-
tions. Internal Report 1/2002, Institut f¨ur Informatik 3, Uni-
versit¨at Erlangen-N¨urnberg, 2002.
[4] K. Buchacker, R. Karch, V. Sieh, and O. Tsch¨ache. Running
a dependability benchmark for an oracle database system.
Internal Report 2/2002, Institut f¨ur Informatik 3, Universit¨at
Erlangen-N¨urnberg, 2002.
[5] K. Buchacker and V. Sieh. Framework for testing the fault-
tolerance of systems including OS and network aspects. In
Proceedings Sixth IEEE International High-Assurance Sys-
tems Engineering Symposium, pages 95–105, 2001.
[6] G.
J.
Carrette.
Crashme.
http://people.delphi.com/gjc/crashme.html, 1996.
[7] O. Corporation.
Oracle 9i database.
URL:
URL:
http://oracle.com/, 2002.
[8] DBench - Dependability Benchmarking (Project
IST-
2000-25425).
Coordinator: Laboratoire d’Analyse et
d’Architecture des Syst`emes du Centre National de
la Recherche Scientiﬁque, Toulouse, France; Partners:
Chalmers University of Technology, G¨oteborg, Swe-
den; Critical Software, Coimbra, Portugal; Faculdade
de Ciencias e Technologia da Universidade de Coim-
bra, Portugal; Friedrich-Alexander Universit¨at, Erlangen-
N¨urnberg, Germany; Microsoft Research, Cambridge, UK;
Universidad Politechnica de Valencia, Spain.
URL:
http://www.laas.fr/DBench/, 2001.
[9] H. H¨oxer, K. Buchacker, and V. Sieh. Implementing a user
mode linux with minimal changes from original kernel. In
J. Topf, editor, 9th International Linux System Technology
Conference, K¨oln, Germany, September 4-6, 2002, pages
71–82, 2002.
[10] N. Kropp, P. J. Koopman, and D. P. Siewiorek. Automated
robustness testing of off-the-shelf software components. In
Proceedings of the 28th IEEE International Symposium on
Fault Tolerant Computing, pages 230–239, 1998.
[11] B. P. Miller, D. Koski, C. P. Lee, V. Maganty, R. Murthy,
A. Natarajan, and J. Steidl. Fuzz revised: A re-examination
of the reliability of UNIX utilities and services. Computer
Science Technical Report 1268, University of Wisconsin-
Madison, 1995.
[12] V. Sieh and K. Buchacker.
UMLinux — a versatile
SWIFI tool. In A. Bondavalli and P. Thevenod-Fosse, ed-
itors, Fourth European Dependable Computing Conference,
Toulouse, France, October 23-25, 2002, pages 159–171.
Springer Verlag, Berlin, 2002.
[13] V. Sieh, O. Tsch¨ache, and F. Balbach. VERIFY: Evalua-
tion of reliability using VHDL-models with integrated fault
descriptions.
In Proceedings of the 27th IEEE Interna-
tional Symposium on Fault Tolerant Computing, pages 32–
36, 1997.
[14] Standard Performance Evaluation Corporation.
SPEC
WEB99 Release 1.02. www.spec.org, 2000.
[15] The Precise UML Group. The Precise UML Group. URL:
http://www.puml.org/, 2003.
[16] Transaction Processing Performance Council. TPC Bench-
mark [tm] C, Standard Speciﬁcation, Revision 5.0, February
26, 2001. www.tpc.org, 2001.
[17] UMLinux Team. UMLinux. URL: http://umlinux.de/, 2002.
[18] M. Vieira and H. Madeira. Deﬁnition of faultloads based on
operator faults for DBMS recovery benchmarking. In Paciﬁc
Rim International Symposium on Dependable Computing,
2002.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:26:06 UTC from IEEE Xplore.  Restrictions apply.