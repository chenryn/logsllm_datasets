11
*Parrots
(SenseTime)
140
216
9
0
1
349
95
0
147
117
TFLite
(Google)
6
7
1
2
0
9
6
0
5
2
NCNN
(Tencent)
37
53
11
4
16
70
40
2
47
16
Mace
(Xiaomi)
18
6
18
0
0
7
7
0
18
18
MxNet
(Apache)
1
13
1
0
0
10
10
3
0
0
ULS (Utility
Asset Store)
11
27
9
0
0
3
3
0
10
9
Total
441
620
88
10
42
872
294
9
483
299
186
272
32
0
17
392
116
4
230
126
Note: 1) One app may use multiple frameworks for different ML functionalities. Therefore, the sum of apps using different functionalities is bigger than the number of total apps. 2)
Security critical functionalities are in bold fonts and can be used for fraud detection or access control. 3) *Caffe was initially developed by Berkeley, based on which Facebook built
Caffe2, which was later merged with PyTorch. The following uses “Caffe” to represent Caffe, Caffe2 and PyTorch.
(i.e., excluding reused models) found in those apps: 26%
models in Chinese apps are protected whereas the percentage
of protected models in Google Play apps is 23%. These
percentages indicate that the apps from the Chinese markets
are more active in protecting their ML models, possibly due
to better security awareness or higher risks [13, 34].
When zooming into apps and focusing on individual
models (i.e., some apps use multiple ML models for different
functionalities), the percentages of unprotected models
(Figure 5b) become even higher. Overall, 4,254 out of 6,522
models (77%) are unprotected and thus easily extractable and
reverse engineered.
Model Protection Across ML Frameworks: We also derive
the per-ML-framework statistics on model protection (Figure
6). The frameworks used by a relatively small number apps,
including MXNet, Mace, TFLite, and ULS, are grouped into
the “Other" category.
Some popular ML frameworks have wider adoption of
model protection, but some not. As shows in Figure 6a,
more than 79% of the apps using SenseTime (Parrots) have
protected models, followed by apps using Caffe (60% of
them have protected models). For apps using TensorFlow
and NCNN, the number is around 20%. Apps using other
frameworks are the least protected against model thefts. This
result can be partly explained by the fact that some popular
frameworks, such as SenseTime, has ﬁrst-party or third-party
libraries that provide the model encryption feature. However,
even for apps using the top-4 ML frameworks, the percentage
of ML apps adopting model protection is still low at 59%.
Encrypted Models Reused/Shared among Apps: Our
analysis also reveals a common practice used in developing
on-device ML apps, which has profound security implications.
We found that many encrypted models are reused or shared
by different apps. The most widely shared model, namely
SenseID_Motion_Liveness.model, is found in 81 apps.
This reuse might be legitimate given that app developers buy
and use ML models and services from third-party providers,
such as SenseTime, instead of developing their own ML
features. The encrypted models reﬂect the awareness of the
ML providers in preventing model thefts. However, we found
60 cases of different app companies are reusing model licenses.
One of the licenses is even used by 12 different app companies,
indicating a high chance of illegal uses.
It is common to see the same encrypted model shared by
different apps. For all the encrypted models that we detected
from the apps, we calculate their MD5 hashes and identify
those models that are used in different and unrelated apps.
Figures 5c and 6c show the numbers of unique (or non-shared)
models and reused (or shared) models, grouped by app markets
and ML frameworks, respectively. Overall, only 22% of all
the protected models are unique. 75% of the encrypted models
from Google Play are unique whereas only 50% and 19% of
the encrypted models on Tencent My App and 360 Mobile As-
sistant, respectively, are not reused (Figure 5c). When grouped
by ML frameworks, 82% of encrypted SenseTime models are
shared, the highest among all frameworks (Figure 6c).
GPU Acceleration Adoption Rate among ML Apps: Table
4 shows the number ML apps and libraries that use GPU for
acceleration. 797(54%) ML apps make use of GPU. The wide
adoption of GPU acceleration poses a challenge to the design
of secure on-device ML. For instance, the naive idea of perform-
ing model inference and other model access operations entirely
inside a trusted execution environment (TEE, e.g., TrustZone)
is not viable due to the need for GPU acceleration, which can-
not be easily or efﬁciently accessed within the TEE.
Table 4: ML apps and libraries that use GPU acceleration
360 Mobile
Assistant
Tencent
My App
ML Apps
ML Libraries
669
212
104
103
Google
Play
24
23
Measurement of Remote Models: Unlike on-device model
inference, remote model inference allows an app to query
a remote server with an object, and obtain the inference
result from the response. Remote model inference does not
necessarily leave footprints like machine learning libraries
or models in the app packages. We thus measure the use of
remote models through APIs provided by AI companies.
We investigated the APIs provided by notable AI companies
USENIX Association
30th USENIX Security Symposium    1961
(a) Apps using protected/encrypted
models vs. those using unprotected models
(b) On-device
models that are protected/encrypted vs. those not
(c) Unique encrypted models
vs. encrypted models reused/shared by multiple apps.
Figure 5: Statistics on ML model protection and reuse, grouped by app markets. The “total” number of unique models is less than the sum of the per-store numbers
because some models are not unique from different stores.
protected models vs. those using unprotected models
(a) Apps using
(b) On-device
models that are protected/encrypted vs. those not
(c) Unique encrypted models
vs. encrypted models reused/shared by multiple apps
Figure 6: Statistics on ML model protection and reuse, grouped by ML frameworks. The “total” number is less than the sum of the per-framework numbers
because many apps use multiple frameworks for different functionalities.
from both US and China. Given publicly available documen-
tation, we were able to extract the use of remote models from
Google Cloud AI, Amazon Cloud AI and Baidu AI. Speciﬁ-
cally, we scanned the API documentation for signature (unique
naming) of remote ML inference libraries. For example, to use
the remote Voice Synthesizer of Baidu AI, an app developer
needs to include the library libBDSpeechDecoder_V1.so. We
then collected all the signatures from the three companies, and
analyzed the use of such signatures in our app collection.
We compared the number of apps using remote models,
on-device models, or using both type of models in a hybrid
mode. As Table 5 shows, 1,341 apps use remote models,
1,468 apps use on-device models, and 182 apps use both. We
emphasize again that on-device model inference is as popular
as remote model inference.
Table 5: Comparison between apps using remote and on-device ML models
App Number
Remote Models
On-device Models
Hybrid Mode
360 Mobile
Assistant
Tencent
My App
1,186
1,131
153
118
159
23
Google
Play
37
178
6
Sum
1,341
1,468
182
We also analyzed the type of ML services provided by
remote models, and the coverage of remote models among
Android apps. Among the 1,341 apps using remote models,
1,075 apps use NLP APIs (speech recognition/synthesizer,
etc.), 266 apps use ML Vision APIs (OCR, image labeling,
landmark recognition, etc.). We did not ﬁnd any security
critical use cases for remote models. As we can see, remote ML
models offer services such as NLP, Voice Synthesizer, OCR
and so on, rather than liveness detection, face recognition,
or other live image processing functionalities, as often seen
in on-device models. This indicates that on-device models
are preferred in scenarios with security critical use cases,
and real-time demands. For the remaining scenarios, remote
models are preferred for easier integration.
5 Q2: How Robust Are Existing Model Protec-
tion Techniques?
To answer this question, we build ModelXtractor, a tool simple
by design to dynamically recover protected or encrypted
models used in on-device ML. Conceptually, ModelXtractor
represents a practical and unsophisticated attack, whereby
an attacker installs apps on his or her own mobile device and
uses the off-the-shelf app instrumentation tools to identify
and export ML models loaded in the memory. ModelXtractor
mainly targets on-device ML models that are encrypted during
transportation and at rest (in storage) but not protected when
in use or loaded in memory. For protected models mentioned
1962    30th USENIX Security Symposium
USENIX Association
1897199172226828316667574254360 Mobile AssistantTencent My AppGoogle PlayTotal0%25%50%75%Protected modelsUnprotected models347971294531550102431815360 Mobile AssistantTencent My AppGoogle PlayTotal0%25%50%75%Unique ModelsReused models70310449384786645932713328945602CaffeNCNNSenseTimeTensorFlowOtherTotal0%25%50%75%Protected appsUnprotected apps2141440155031734226829922184119016023014254CaffeNCNNSenseTimeTensorFlowOtherTotal0%25%50%75%Protected modelsUnprotected models4001672721102245317412731278207121815CaffeNCNNSenseTimeTensorFlowOtherTotal0%25%50%75%Unique modelsReused modelsin §4, ModelXtractor is performed to assess the robustness
of the protection.
The workﬂow of ModelXtractor is depicted in Figure 7.
It takes inputs from ModelXRay, including the information
about the ML framework(s) and the model(s) used in the
app (described in §4). These information helps to target and
efﬁciently instrument an app during runtime, and capture
models in plaintext from the memory of the app. We discuss
ModelXtractor’s code instrumentation strategies in §5.1, our
techniques for recognizing in-memory models in §5.2, and
how ModelXtractor veriﬁes captured models in §5.3. Our
ﬁndings, insights, the answer to Q2, and several case studies
are presented in §5.4 and §5.5. Responsible disclosure of our
ﬁndings is discussed in §5.6.
5.1 App Instrumentation
ModelXtractor uses app instrumentation to dynamically ﬁnd
the memory buffers where (decrypted) ML is loaded and ac-
cessed by the ML frameworks. For each app, ModelXtractor de-
termines which libraries and functions need to be instrumented
and when to start and stop each instrumentation, based on the in-
strumentation strategies (discussed shortly). ModelXtractor au-
tomatically generates the code that needs to be inserted at differ-
ent instrumentation points. It employs the widely used Android
instrumentation tool, Frida [11], to perform code injection.
ModelXtractor has a main instrumentation strategy (S0)
and four alternative ones (S1-S4). When the default strategy
cannot capture the models, the alternatively strategies (S1-S4)
will be used.
S0: Capture at Model Deallocation: This is the default
strategy since we observe the most convenient time and place
to capture an in-memory model is right before the deallocation
of the buffer where the model is loaded. This is because (1)
memory deallocation APIs (e.g.,free) are limited in numbers
and easy to instrument, and (2) models are completely loaded
and decrypted when their buffers are to be freed.
Naive instrumentation of deallocation APIs can lead to dra-
matic app slowdown. We optimize it by ﬁrst only activating it af-
ter the ML library is loaded, and second, only for buffers greater
than the minimum model size (a conﬁgurable threshold). To
get buffer size, memory allocation APIs (e.g.,malloc) are in-
strumented as well. The size information also helps correlate
a decrypted model to its encrypted version (discussed in §5.3).
This default instrumentation strategy may fail in the
following uncommon scenarios. First, an app is not using
native ML libraries, but a JavaScript ML library. Second, an
app uses its own or customized memory allocator/deallocator.
Third, a model buffer is not freed during our dynamic analysis.
S1: Capture from Heap: This strategy dumps the entire heap
region of an app when a ML functionality is in use, in order to
identify possible models in it. It is suitable for apps that do not
free model buffers timely or at all. It also helps in cases where
memory-managed ML libraries are used (e.g., JavaScript) and
buffer memory deallocations (done by a garbage collector)
are implicit or delayed.
S2: Capture at Model Loading: This strategy instruments
ML framework APIs that load models to buffers. We manually
collect a list of such APIs (e.g., loadModel) for the ML
frameworks observed in our analysis. This strategy is suitable
for those apps where S0 fails and the ML framework code is
not obfuscated.
S3: Capture at Model Decryption: This strategy instru-
ments model decryption APIs (e.g., aes256_decrypt) in ML
frameworks, which we collected manually. Similar to S2, it is
not applicable to apps that use obfuscated ML framework code.
S4: Capture at Customized Deallocation: Some apps use
customized memory deallocators. We manually identify a
few such allocators (e.g., slab_free), which are instrumented
similarly as S0.
5.2 Model Representation and Recognition
The app instrumentation described earlier captures memory
buffers that may contain ML models. The next step is to
perform model recognition from the buffers. The recognition is
based on the knowledge of in-memory model representations,
i.e., different ML frameworks use different formats model
encoding, discussed in the following.
Protobuf is the most popular model encoding format, used
by TensorFlow, Caffe, NCNN, and SenseTime. To detect and
extract models in Protobuf from memory buffers, ModelX-
tractor uses two kinds of signatures: content signatures and
encoding signatures. The former is used to identify buffers that
contain models and the latter is used to locate the beginning
of a model in a buffer.
Model encoded in Protobuf usually contains words descrip-
tive of neural network structures and layers. For example,
“conv1" is used for one-dimension convolution layer,and “relu"
for the Rectiﬁed Linear Unit. Such descriptive words appear
in almost every model and are used as the content signatures.
The encoding signatures of Protobuf is derived from its
encoding rule [22]. For example, a Protobuf contains multiple
messages. Every message is a series of key-value pairs, or
ﬁelds. The key of a ﬁeld is encoded as (field_number (cid:28) 3)