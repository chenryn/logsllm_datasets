title:Fastpass: a centralized "zero-queue" datacenter network
author:Jonathan Perry and
Amy Ousterhout and
Hari Balakrishnan and
Devavrat Shah and
Hans Fugal
Fastpass: A Centralized “Zero-Queue” Datacenter Network
Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, Hans Fugal
M.I.T. Computer Science & Artiﬁcial Intelligence Lab
http://fastpass.mit.edu/
Facebook
ABSTRACT
An ideal datacenter network should provide several properties, in-
cluding low median and tail latency, high utilization (throughput),
fair allocation of network resources between users or applications,
deadline-aware scheduling, and congestion (loss) avoidance. Current
datacenter networks inherit the principles that went into the design
of the Internet, where packet transmission and path selection deci-
sions are distributed among the endpoints and routers. Instead, we
propose that each sender should delegate control—to a centralized
arbiter—of when each packet should be transmitted and what path it
should follow.
This paper describes Fastpass, a datacenter network architecture
built using this principle. Fastpass incorporates two fast algorithms:
the ﬁrst determines the time at which each packet should be transmit-
ted, while the second determines the path to use for that packet. In
addition, Fastpass uses an efﬁcient protocol between the endpoints
and the arbiter and an arbiter replication strategy for fault-tolerant
failover. We deployed and evaluated Fastpass in a portion of Face-
book’s datacenter network. Our results show that Fastpass achieves
high throughput comparable to current networks at a 240⇥ reduc-
tion is queue lengths (4.35 Mbytes reducing to 18 Kbytes), achieves
much fairer and consistent ﬂow throughputs than the baseline TCP
(5200⇥ reduction in the standard deviation of per-ﬂow throughput
with ﬁve concurrent connections), scalability from 1 to 8 cores in the
arbiter implementation with the ability to schedule 2.21 Terabits/s of
trafﬁc in software on eight cores, and a 2.5⇥ reduction in the number
of TCP retransmissions in a latency-sensitive service at Facebook.
1.
INTRODUCTION
Is it possible to design a network in which: (1) packets experience
no queueing delays in the network, (2) the network achieves high
utilization, and (3) the network is able to support multiple resource
allocation objectives between ﬂows, applications, or users?
Such a network would be useful in many contexts, but especially
in datacenters where queueing dominates end-to-end latencies, link
rates are at technology’s bleeding edge, and system operators have
to contend with multiple users and a rich mix of workloads. Meet-
ing complex service-level objectives and application-speciﬁc goals
would be much easier in a network that delivered these three ideals.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2836-4/14/08...$15.00.
http://dx.doi.org/10.1145/2619239.2626309.
Current network architectures distribute packet transmission deci-
sions among the endpoints (“congestion control”) and path selection
among the network’s switches (“routing”). The result is strong fault-
tolerance and scalability, but at the cost of a loss of control over
packet delays and paths taken. Achieving high throughput requires
the network to accommodate bursts of packets, which entails the use
of queues to absorb these bursts, leading to delays that rise and fall.
Mean delays may be low when the load is modest, but tail (e.g., 99th
percentile) delays are rarely low.
Instead, we advocate what may seem like a rather extreme ap-
proach to exercise (very) tight control over when endpoints can send
packets and what paths packets take. We propose that each packet’s
timing be controlled by a logically centralized arbiter, which also
determines the packet’s path (Fig. 1). If this idea works, then ﬂow
rates can match available network capacity over the time-scale of
individual packet transmission times, unlike over multiple round-trip
times (RTTs) with distributed congestion control. Not only will
persistent congestion be eliminated, but packet latencies will not rise
and fall, queues will never vary in size, tail latencies will remain
small, and packets will never be dropped due to buffer overﬂow.
This paper describes the design, implementation, and evaluation
of Fastpass, a system that shows how centralized arbitration of
the network’s usage allows endpoints to burst at wire-speed while
eliminating congestion at switches. This approach also provides
latency isolation: interactive, time-critical ﬂows don’t have to suffer
queueing delays caused by bulk ﬂows in other parts of the fabric.
The idea we pursue is analogous to a hypothetical road trafﬁc control
system in which a central entity tells every vehicle when to depart
and which path to take. Then, instead of waiting in trafﬁc, cars can
zoom by all the way to their destinations.
Fastpass includes three key components:
1. A fast and scalable timeslot allocation algorithm at the ar-
biter to determine when each endpoint’s packets should be
sent (§3). This algorithm uses a fast maximal matching to
achieve objectives such as max-min fairness or to approximate
minimizing ﬂow completion times.
2. A fast and scalable path assignment algorithm at the arbiter
to assign a path to each packet (§4). This algorithm uses a
fast edge-coloring algorithm over a bipartite graph induced
by switches in the network, with two switches connected by
an edge if they have a packet to be sent between them in a
timeslot.
3. A replication strategy for the central arbiter to handle net-
work and arbiter failures, as well as a reliable control protocol
between endpoints and the arbiter (§5).
We have implemented Fastpass in the Linux kernel using high-
precision timers (hrtimers) to time transmitted packets; we achieve
sub-microsecond network-wide time synchronization using the
IEEE1588 Precision Time Protocol (PTP).
Figure 1: Fastpass arbiter in a two-tier network topology.
We conducted several experiments with Fastpass running in a
portion of Facebook’s datacenter network. Our main ﬁndings are:
1. High throughput with nearly-zero queues: On a multi-machine
bulk transfer workload, Fastpass achieved throughput only
1.6% lower than baseline TCP, while reducing the switch
queue size from a median of 4.35 Mbytes to under 18
Kbytes. The resulting RTT reduced from 3.56 ms to 230 µs.
2. Consistent (fair) throughput allocation and fast convergence:
In a controlled experiment with multiple concurrent ﬂows start-
ing and ending at different times, Fastpass reduced the stan-
dard deviation of per-ﬂow throughput by a factor over
5200⇥ compared to the baseline with ﬁve concurrent TCP
connections.
3. Scalability: Our implementation of the arbiter shows nearly
linear scaling of the allocation algorithm from one to eight
cores, with the 8-core allocator handling 2.21 Terabits/s.
The arbiter responds to requests within tens of microseconds
even at high load.
4. Fine-grained timing: The implementation is able to synchro-
nize time accurately to within a few hundred nanoseconds
across a multi-hop network, sufﬁcient for our purposes be-
cause a single 1500-byte MTU-sized packet at 10 Gbits/s has
a transmission time of 1230 nanoseconds.
5. Reduced retransmissions: On a real-world latency-sensitive
service located on the response path for user requests, Fast-
pass reduced the occurrence of TCP retransmissions by
2.5⇥, from between 4 and 5 per second to between 1 and 2
per second.
These experimental results indicate that Fastpass is viable, pro-
viding a solution to several speciﬁc problems observed in datacenter
networks. First, reducing the tail of the packet delay distribution,
which is important because many datacenter applications launch
hundreds or even thousands of request-response interactions to ful-
ﬁll a single application transaction. Because the longest interaction
can be a major part of the transaction’s total response time, reducing
the 99.9th or 99.99th percentile of latency experienced by packets
would reduce application latency.
Second, avoiding false congestion: packets may get queued
behind other packets headed for a bottleneck link, delaying non-
congested trafﬁc. Fastpass does not incur this delay penalty.
Third, eliminating incast, which occurs when concurrent requests
to many servers triggers concurrent responses. With small router
queues, response packets are lost, triggering delay-inducing retrans-
missions [26], whereas large queues cause delays to other bulk trafﬁc.
Current solutions are approximations of full control, based on esti-
mates and assumptions about request RTTs, and solve the problem
only partially [28, 37].
And last but not least, better sharing for heterogeneous workloads
with different performance objectives. Some applications care about
low latency, some want high bulk throughput, and some want to
minimize job completion time. Supporting these different objec-
tives within the same network infrastructure is challenging using
distributed congestion control, even with router support. By contrast,
Figure 2: Structure of the arbiter, showing the timeslot allocator,
path selector, and the client-arbiter communication.
a central arbiter can compute the timeslots and paths in the network
to jointly achieve these different goals.
2. FASTPASS ARCHITECTURE
In Fastpass, a logically centralized arbiter controls all network
transfers (Fig. 2). When an application calls send() or sendto() on a
socket, the operating system sends this demand in a request message
to the Fastpass arbiter, specifying the destination and the number of
bytes. The arbiter processes each request, performing two functions:
1. Timeslot allocation: Assign the requester a set of timeslots
in which to transmit this data. The granularity of a timeslot
is the time taken to transmit a single MTU-sized packet over
the fastest link connecting an endpoint to the network. The
arbiter keeps track of the source-destination pairs assigned
each timeslot (§3).
2. Path selection. The arbiter also chooses a path through the
network for each packet and communicates this information
to the requesting source (§4).
Because the arbiter knows about all current and scheduled trans-
fers, it can choose timeslots and paths that yield the “zero-queue”
property: the arbiter arranges for each packet to arrive at a switch on
the path just as the next link to the destination becomes available.
The arbiter must achieve high throughput and low latency for both
these functions; a single arbiter must be able to allocate trafﬁc for a
network with thousands of endpoints within a few timeslots.
Endpoints communicate with the arbiter using the Fastpass Con-
trol Protocol (FCP) (§5.3). FCP is a reliable protocol that conveys
the demands of a sending endpoint to the arbiter and the allocated
timeslot and paths back to the sender. FCP must balance conﬂicting
requirements: it must consume only a small fraction of network
bandwidth, achieve low latency, and handle packet drops and ar-
biter failure without interrupting endpoint communication. FCP
provides reliability using timeouts and ACKs of aggregate demands
and allocations. Endpoints aggregate allocation demands over a
few microseconds into each request packet sent to the arbiter. This
aggregation reduces the overhead of requests, and limits queuing at
the arbiter.
Fastpass can recover from faults with little disruption to the net-
work (§5). Because switch buffer occupancy is small, packet loss
is rare and can be used as an indication of component failure. End-
points report packet losses to the arbiter, which uses these reports
to isolate faulty links or switches and compute fault-free paths. The
arbiter itself maintains only soft state, so that a secondary arbiter can
take over within a few milliseconds if the primary arbiter fails.
To achieve the ideal of zero queueing, Fastpass requires precise
timing across the endpoints and switches in the network (§6.3).
When endpoint transmissions occur outside their allocated timeslots,
packets from multiple allocated timeslots might arrive at a switch
at the same time, resulting in queueing. Switch queues allow the
network to tolerate timing inaccuracies: worst-case queueing is no
ToRCoreEndpointsFastpassArbiterFCPclientHostnetworking stackPathSelectionFCPserverTimeslotallocationdestination and sizetimeslotsand pathsNICEndpointArbiterlarger than the largest discrepancy between clocks, up to 1.5 Kbytes
for every 1.2 µs of clock divergence at 10 Gbits/s.
Fastpass requires no switch modiﬁcations, nor the use of any
advanced switch features. Endpoints require some hardware support
in NICs that is currently available in commodity hardware (§6.3),
with protocol support in the operating system. Arbiters can be
ordinary server-class machines, but to handle very large clusters, a
number of high-speed ports would be required.
2.1 Latency experienced by packets in Fastpass
In an ideal version of Fastpass, endpoints receive allocations as
soon as they request them: the latency of communication with the
arbiter and the time to compute timeslots and paths would be zero.
In this ideal case, the end-to-end latency of a packet transmission
would be the time until the allocated timeslot plus the time needed
for the packet to traverse the path to the receiver with empty queues
at all egress ports.
In moderately-loaded to heavily-loaded networks, ideal alloca-
tions will typically be several timeslots in the future. As long as the
Fastpass arbiter returns results in less than these several timeslots,
Fastpass would achieve the ideal minimum packet latency in practice
too.
In lightly-loaded networks, Fastpass trades off a slight degradation
in the mean packet latency (due to communication with the arbiter)
for a signiﬁcant reduction in the tail packet latency.
2.2 Deploying Fastpass
Fastpass is deployable incrementally in a datacenter network.
Communication to endpoints outside the Fastpass boundary (e.g.,
to hosts in a non-Fastpass subnet or on the Internet) uses Fastpass
to reach the boundary, and is then carried by the external network.
Incoming trafﬁc either passes through gateways or travels in a lower
priority class. Gateways receive packets from outside the boundary,
and use Fastpass to send them within the boundary. Alternatively,
incoming packets may use a lower-priority class to avoid inﬂating
network queues for Fastpass trafﬁc.
This paper focuses on deployments where a single arbiter is re-
sponsible for all trafﬁc within the deployment boundary. We discuss
larger deployments in §8.1.
3. TIMESLOT ALLOCATION
The goal of the arbiter’s timeslot allocation algorithm is to choose
a matching of endpoints in each timeslot, i.e., a set of sender-receiver