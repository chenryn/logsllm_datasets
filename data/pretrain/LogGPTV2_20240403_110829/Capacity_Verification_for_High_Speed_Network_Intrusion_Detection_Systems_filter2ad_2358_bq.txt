memory  while inspecting the packets.  Different  NIDS  architectures  exhibit  different
use  patterns  for  memory.    A  NIDS  that  relies  solely  on  regular  expression  matching
consumes the most bandwidth and induces the most latency in the system.  Inspecting
each  character  in  the  packet  payload  and  advancing  a  regular  expression  state  are
expensive operations.
Protocol analysis helps reduce the  number of bytes that must be  inspected.    Every
NIDS  does  some  type  of  protocol  decode,  even  if  it  is  limited  to  just  the  IP  header.
Many  of  the  commercial  NIDS  decode  most  layer  seven  protocols  and  only  perform
regular expression inspection on a subset of the entire packet.
The  size  of  the  packets  therefore  plays  an  important  role  in  determining  the
capacity  of  a  NIDS.    Testing  performance  with  the  smallest  possible  average  packet
size  reduces  the  amount  of  time  available  per  packet  for  inspections.    Increasing  the
average packet size allows more time for inspection and increases the use of memory.
The average packet size of typical traffic on the Internet is around 450 to 550 bytes
per  packet  [3,4].    However,  some  networks  contain  averages  much  larger  and  much
smaller.  The average packet size is an important metric in capacity testing.
244 M. Hall and K. Wiley
3.6 Event or Alarm Reporting
The  generation  of  the  alarm  event  expends  CPU  cycles  that  would  otherwise  be
available  for  analysis.    Additionally,  the  event  needs  to  be  stored  in  non-volatile
storage.  This usually means that it must be written to disk, which is a relatively slow
operation, or sent over a network connection.  Under normal circumstances this does
not  affect  the  operation  of  a  NIDS.    However,  as  the  rate  of  alarm  production
increases  and/or  the  load  on  the  network  increases,  alarm  event  production  and  log
maintenance  can  have  a  significant  effect  on  NIDS  performance.    The  event
generation component of a NIDS must  be  able  to  handle  the  events  generated  by  the
high  rates  of  traffic.    The  ability  of  the  NIDS  to  notify  the  user  varies  as  the  alarm
event rate is adjusted.
The metric  used to test this component of a NIDS is  simply  the  number  of  alarms
per  second.    Tools  such  as  stick  and  nessus  easily  set  off  alarm  events  in  NIDS
products.    In  addition,  packet  generators  can  be  used  to  generate  single  packets  that
cause an alarm  event.    Testing  the  alarm  channel  does  not  require  the  traffic  causing
the alarms to originate from real hosts.
3.7 The Metrics
With the major stress points of a NIDS defined, it is now possible to focus on defining
the metrics that can be used to quantify  the capacity  of  a  NIDS.    Table  1 defines  the
test  metrics  and  how  they  are  related  to  the  use  of  the  fixed  resources  described  in
section 3.1.
Table 1. NIDS test metrics and corresponding resources used
Test Metrics
Resources Used
Packets per second
CPU cycles, network interface bandwidth,
and memory bus bandwidth
Bytes per second 
CPU cycles, network interface bandwidth,
(Average packet size)
and memory bus bandwidth
Protocol Mix
CPU cycles, memory bus bandwidth
Number of unique hosts
Memory size, CPU cycles, and memory bus
bandwidth
Number of new connections per
CPU cycles, and memory bus bandwidth
second
Number of concurrent
Memory size, CPU cycles, and memory bus
connections
bandwidth 
Alarms per second
Memory size, CPU cycles, and memory bus
bandwidth
Capacity Verification for High Speed Network Intrusion Detection Systems
245
4   Developing the Tests
Developing the tests to quantify the metrics for the potential weak points of a NIDS is
non-trivial.    This  section  explores  traffic  mix  selection  and  simplification,  potential
problems  with  a  test  network,  and  a  set  of  tests  and  the  intended  stress  points  under
test.
4.1 Traffic Mix
Many of the metrics defined in the previous section are directly and indirectly derived
from the network traffic.  What  is  the  correct  mix?   The  correct mix  for  each  user  is
one that best matches the traffic where that user plans to deploy the NIDS.  Obviously
a test cannot be designed that contains all the traffic mixes for all potential consumers
of  NIDS  technology.    However,  after  the  NIDS  industry  agrees  on  a  standard
methodology  for  testing  the  stress  points,  consumers  could  profile  their  traffic  mix
and get a reasonable feel for how well the various products perform.
How  do  we  define  the  tests  to  be  used?    There  have  been  studies  performed  that
describe  the  mix  of  traffic  seen  on  the  major  network  trunks.    “The  Nature  of  the
Beast:  Recent  Traffic Measurements  from  an  Internet  Backbone”  [3]  and  “Trends  in
Wide  Area  IP  Traffic”  [4]  are  two  good  resources  for  defining  a  general  test.  The
information  in  these  papers  is  from  1998  and  2000.    For  more  recent  data  Table  2
includes results from profiling three datasets from 2002.  A major university, a major
U.S. government site, and a large online retailer provided the datasets.  Although this
data is not necessarily representative of a traffic mix found on a corporate network, it
is  representative  of  the  mix  that  would  be  seen  at  the  edge  of  most  large  networks.
The  metrics  in  this  traffic  provide  a  good  starting  point  for  the  mix  of  the  loading
traffic for the test.
Table 2. Traffic metrics from three customer sites. Site 1 is a major university. Site 2 is a major
US  government  site.  Site  3  is  an  online  retailer.  In  the  layer  4  OTHER  field,  no  individual
protocol grouped into this field consisted of more than 3% or the total traffic.
Average Packet Size
Site 1
543
Site 2
501
Site 3
557
Average Bandwidth
25.0Mbps
36.2Mbps
31.4Mbps
Layer 3
% TCP
% UDP
% ICMP
% OTHER
Layer 4
94.8%
97.6%
98.7%
5.0%
0.2%
0.1%
1.2%
0.1%
1.1%
1.3%
0.0%
0.0%
TCP connections per second
201
277
118
% HTTP
% SMTP
% NNTP
% OTHER
49.4%
64.6%
61.6%
5.1%
5.5%
9.4%
0.0%
9.3%
0.0%
40.0%
26.0%
29.1%
246 M. Hall and K. Wiley
Table  2  shows  the  same  general  traffic  characteristics  found  in  the  CAIDA  data
[3,4].    Currently  we  are  unaware  of  any  data  sets  for  networks  running  at  or  near
gigabit per second speeds.  Further research is needed in this area.  Due to the limited
scope  of  this  paper,  it  is  assumed  that  the  traffic  mix  will  scale  evenly  with  the
increased bandwidth.
With  a  general  understanding  of  the  type  of  traffic  found  at  the  edge  of  protected
networks,  it  is  now  possible  to  explore  crafting  tests  that  quantify  the  metrics  at  a
level useful for NIDS consumers.
4.2 HTTP Traffic for Testing
Typically  the  HTTP  protocol  is  not  blocked  outbound  from  firewalls  and  is  a
dominant portion of the traffic on the Internet.  The servers and clients that implement
HTTP  have  garnered  the  attention  of  many  crackers  and  security  professionals.
HTTP based signatures make up the majority of signatures in NIDS.  Fortunately this
situation  allows  for  simplification  of  the  testing  in  the  general  case.    When  HTTP
traffic  is  used  to  test  the  capacity  of  a  NIDS,  it  obviously  stresses  a  large  portion  of
the packet flow architecture.
The  use  of  HTTP  traffic  has  a  few  other  advantages  as  well.    HTTP  traffic  is
relatively  easy  and  inexpensive  (in  time  and  money)  to  produce.   Web  server  testing
tools,  such  as  ZDNet’s  WebBench  can  be  used  to  generate  traffic  to  reproduce  tests
inexpensively.    In  addition,  there  are  several  vendors  selling  network  test  equipment
that utilizes a real TCP/IP stack implementation instead of using “canned” traffic.  We
are  most  familiar  with  the  products  from  Caw  Networks  and  Antera.    For  the  test
conducted  in  section  5,  we  used  Caw  Networks  WebReflector  and  WebAvalanche
products.
4.3 Generating Real Traffic vs. Replaying Traffic
Most NIDS shipping today perform some level of protocol decode and state tracking.
Therefore,  it  is  very  important  that  any  load  traffic  exhibit  the  same  characteristics
found  on  a  consumer’s  network.    Most  of  the  products  that  allow  for  high  speed
traffic generation have some critical flaws that make them unsuitable for testing NIDS
at high speeds.  Some of these issues include:
-
-
-
-
Inability to create valid checksums for all layers at high speeds
Inability  to  vary  the  IP  addresses  in  a  more  random  manner  than  a  straight
increment
Inability  to  maintain  state  of  TCP  connections  and  issue  resets  if  packets  are
dropped
Inability to  play  a  large  mix  of  traffic  due  to  the  limitations  in  buffer  size  for  the
transmitters
These  issues  do  not  plague  the  replay  devices  at  slower  speeds.    At  high  speeds,  the
buffer  size  for  the  replay  devices  prohibits  large  traffic  samples.    Using  replay
requires the tester to use more replay interfaces.  Adding more source interfaces when
testing a high aggregate rate presents problems for the test network as described in the
next  section.    Therefore,  using  test  devices  that  use  real  TCP/IP  implementation  to
generate the traffic is preferred.
Capacity Verification for High Speed Network Intrusion Detection Systems
247
4.4
Inter-packet Arrival Gap on High Speed Test Networks
The  typical  network  setup  for  testing  a  gigabit  NIDS  includes  several  traffic
generators,  an  attack  network,  and  a  victim  network.    All  of  these  devices  are
typically  connected  to  a  switch,  and  the  traffic  is  then  port-mirrored,  spanned,  or
copy-captured to the NIDS.  For high speed tests, the interface for the NIDS is gigabit
ethernet.    The  inter-packet  arrival  gap  on  gigabit  ethernet  is  96-ns.    Inter-packet
arrival  gap  becomes  important  as  more  interfaces  are  added  to  the  switch.    Each
interface, regardless of interface speed, used to generate  traffic  increases  the  chances
that traffic destined for the NIDS will be dropped at the switch.
Imagine that ten fast ethernet  ports  each  generating  around  80-Mbits  of  traffic  are
used  during  testing.    Eventually  several  of  these  ports  begin  to  transmit  packets  all
within a few nanoseconds of each other.  Since each of these transmitted packets must
be copied to the NIDS, the switch forwards each of the packets to the port  where the
NIDS  is  connected.    Unfortunately  the  NIDS  is  using  Ethernet,  which  demands  the
96-ns  delay  between  packets.    Since  there  are  several  packets  arriving  at  the  port  at
very nearly the same time, the port buffer gets full and the switch port drops packets.
This  problem  does  not  manifest  itself  if  there  is  a  choke  point  such  as  a  firewall  or
router  used  in  the  test  network.    But,  if  the  industry  tests  require  a  router  or  firewall
capable  of  the  same  high  traffic  speeds  to  reproduce  the  tests,  it  raises  the  cost  of
testing by a non-trivial amount.  It is therefore better to use fewer ports for generating
traffic on the switch when testing.
4.5
Potential Test Suite
No single test provides all the information needed to quantify the capacity of a NIDS.
A suite of tests is used to quantify each portion of a NIDS.  Only when looking at the
output from all of these tests can a consumer infer performance on his network.
Establishing the Peak.  Testing the network interface bandwidth establishes the peak
for  packet  capture  for  the  NIDS.    The  NIDS  is  never  able  to  perform  above  this
absolute peak on any further tests.  Testing the network interface bandwidth is simple.
Choose a packet of no interest to the NIDS and resend it at a high rate until the NIDS
cannot  count  all  the  packets.    Repeat  the  tests  for  minimum-sized  packets  and  for
maximum-sized  packets.    This  reveals  the  maximum  packets  per  second  and  the
maximum  bytes  per  second  respectively.    A  good  example  packet  for  this  test  is  a
UDP  packet  with  ports  set  to  0  (assuming  the  NIDS  does  not  send  alarms  on  such  a
packet). 
The  Alarm  Channel.    Testing  the  alarm  channel  capacity  of  a  NIDS  can  be
accomplished with a similar test.  Choose a packet that causes an alarm.  The “Ping of
Death” is a good packet for this test.  Send this packet at different rates of speed and
check packet and alarm counts.  Some NIDS buffer alarms when under a heavy load,
so  a  quiescent  period  after  the  packets  are  sent  may  be  necessary  before  collecting
counts.
Stressing  the  State  Database.    Inserting,  searching,  and  deleting  the  state
information  from  the  state  database  are  all  potential  bottlenecks  for  the  NIDS. 
248 M. Hall and K. Wiley
Varying the IP addresses of traffic requiring state tracking adds  load  to  the  database.
Opening  a  large  number  of  TCP  connections  causes  the  state  database  to  contain
many  records.    The  search  performance  for  a  database  is  affected  by  the  size  of  the
dataset.  For this type of test open a large number of concurrent TCP connections and
then  run  one  of  the  more  general  tests  described  below.    The  open  connections  will
stress both the database and the overall system architecture.
General  Tests  with  Configurable  Metrics.    Establishing  a  baseline  of  traffic  and
then varying one of the metrics can expose a NIDS weakness in certain environments.
Since  we  are  testing  each  specific  component  of  the  NIDS,  it  is  not  necessary  to
ensure the traffic looks exactly like the traffic of the end user.  If the user can extract
the  same  metrics  from  his  traffic,  then  performance  on  his  network  can  be  inferred
from  test  results  using  simpler  data.    In  the  example  tests  found  in  section  5,  the
traffic mix consists of only HTTP.  HTTP rides on TCP, which requires state tracking.
Depending  on  the  level  of  protocol  decoding  used,  HTTP  may  also  require  state
tracking.  In addition, HTTP signatures make up the majority of the signatures found
in a NIDS.  Therefore, using an HTTP-only traffic mix still stresses the NIDS in many
areas.     The  example  tests  in  section  5  could  have  also  included  additional  protocols
such  as  DNS,  SMTP,  and  NNTP.    However,  due  to  time  and  space  constraints  these
protocols were omitted for this test.
Table  3  shows  the  characteristics  of  the  traffic  mix  when  using  Caw  Networks
WebReflector  and  WebAvalanche  products.    This  test  equipment  allows  for  high
speed  testing  using  only  two  ports  on  the  switch  for  generating  traffic.    The  Caw
Networks  equipment  also  has  the  ability  to  randomly  drop  packets.    The  dropped
packets  cause  their  systems  to  retransmit  and  the  traffic  looks  more  like  real  world
traffic.    By  simply  varying  the  HTTP  transaction  size,  many  characteristics  of  the
traffic  can  be  manipulated.    Using  HTTP  transaction  size  is  just  one  example.    The
MSS1  for  the  server  or  client  can  also  be  varied  to  affect  the  characteristics  in  other
ways. 
Table  3.  Traffic  mix  characteristics  when  using  Caw  Networks  WebReflector  and
WebAvalanche to generate HTTP traffic for general stress tests.
Transaction Size
Packets in Stream
Bytes In Stream
Avg. Packet Size
1000
5000
10000
20000
40000
50000
100000
240000
360000
400000
8
13
20
33
60
73
140
326
486
540
1448