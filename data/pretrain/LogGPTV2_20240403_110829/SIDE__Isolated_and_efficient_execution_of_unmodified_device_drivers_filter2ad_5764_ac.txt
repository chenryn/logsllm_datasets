isolated device driver’s I/O ports are memory-mapped, their
corresponding page table entries are set up in such a way
that the driver can access them directly without triggering
a protection fault. In both cases, the kernel’s integrity is
not compromised because no isolated driver is given more
access privilege than it is supposed to have. In addition, the
SIDE module also checks the source and target addresses of
DMA transaction set-up operations to ensure that an isolated
device driver does not tamper with memory pages outside
its driver region.
For network driver isolated in SIDE, we measured its
performance penalty and identiﬁed two major bottlenecks,
one on the packet transmission side and the other on the
packet reception side. On the transmission side, the network
device driver calls a kernel service function for every out-
going packet to translate the starting virtual address of its
buffer into its correspond physical address for DMA. Each of
these kernel service function calls incurs a protection domain
7
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:39:49 UTC from IEEE Xplore.  Restrictions apply. 
crossing. To eliminate this per-outgoing-packet overhead,
SIDE optimized pre-translates the starting virtual address of
every outgoing packet’s buffer, puts the translation result
in the driver region, and redirects the driver to call a
local translation function in Driver Helper to pick up this
translation result. The pages in which the local functions
reside are mapped as user-level as well as read-only so that
driver can access it directly but without being able to modify
them.
On the reception side, whenever a software interrupt
handler in the driver is invoked, for every incoming packet
it needs to call a kernel service function to remove the
buffer it occupies from the reception ring and submit it to
the protocol stack. Each of these kernel service function
calls incurs a protection domain crossing. To remove this
per-incoming-packet overhead, SIDE optimized replaces this
kernel service function with a local function that merely
notes down the buffers occupied by incoming packets, and
processes them in one batch when control returns from the
driver’s software interrupt handler.
The above two optimizations are built on the ability to
replace calls to kernel service functions in a driver with
calls to proxy functions in Driver Helper without modifying
the driver. SIDE optimized achieves this substitution trans-
parently by modifying the driver module loader to insert
substituting proxy functions into the driver being loaded and
resolving the driver’s import table entries that correspond to
the substituted kernel service functions to their substituting
local functions. These substituting proxy functions form the
Driver Helper shown in Figure 3.
C. Recovery Processing
In SIDE,
the invocation of an isolated device driver
function is considered failing if it triggers an unrecoverable
memory protection fault, e.g. issuing an unauthorized access
to the kernel memory, attempting to access an unmapped
page in the driver space, calling a kernel service function
with invalid input arguments, or a call to an isolated device
driver function not returning within a certain time period.
When SIDE detects that an isolated device driver fails, it
cleans up all the side effects left by the driver in the kernel,
unregisters the failed driver, and reloads and restarts the
driver so that the system could resume its service as soon
as possible.
The side effects of a device driver include all the resources
it acquires from the kernel. The current SIDE prototype
keeps track of the following resources used by an isolated
device driver during its execution:
• The set of memory pages allocated,
• The set of locks and semaphores acquired,
• The set of PCI resources assigned, e.g. I/O ports,
• The set of timer events,
• The added preempt counts, and
• The set of scheduled tasks, e.g. software interrupts.
To accurately track the resource usage of an isolated
device driver, we keep a record of the corresponding kernel
service functions that isolated device drivers use to allocate
these resources by adding proper accounting logic. For every
resource that a failed isolated device driver currently holds,
SIDE’s recovery processing engine releases it back to the
kernel, and eventually unregisters the device driver itself
from the kernel entirely. To resume the service provided by
the failed device driver, SIDE’s recovery processing engine
restarts a new copy of the driver in question from scratch.
If the entire driver failure detection and recovery process is
quick enough, it is possible to make the perceived effect of
a driver failure like a hardware glitch.
Keeping track of an isolated device driver’s resource
usage turns out to be somewhat tricky to get right. For
example, in our ﬁrst implementation, we did not take into
account
the software interrupts scheduled by a driver’s
interrupt processing routine, and thus fail to remove them
as part of the clean-up step. As a result, we encountered
numerous system crashes whenever an isolated device driver
failed immediately after it scheduled a software interrupt
and became unregistered. The scheduled software interrupts
crashed because the driver space they operate on has been
removed as a result of un-registration.
Although the current SIDE implementation could recover
a device driver failure, it could not mask a device driver from
running TCP/UDP connections. To do it, we could extend
the current SIDE implementation with a driver wrapper,
similar to a shadow driver [12] or a backend driver for the
Xen hypervisor [14]. This wrapper caches some device state
and relays commands and status information between the
kernel and the real device driver, and could buffer or service
commands from the kernel to mask a device driver failure
until the failed driver fully restarts.
D. Limits
SIDE base can run unmodiﬁed device drivers correctly,
but if an isolated driver requires frequent function calls to the
kernel, the performance penalty is non-negligible because
each context switch involves a TLB ﬂush. Optimizing an
isolated driver’s performance requires the understanding of
the driver’s behavior, such as the kernel data structures and
the set of kernel services it accesses and relies on.
to avoid expensive function calls. Some of
Some drivers use inline functions provided by the ker-
nel
these
functions access kernel core data directly, for example
add preempt count(). SIDE needs to change these inline
functions to explicitly called functions so as to intercept
them.
V. PERFORMANCE EVALUATION
A. Methodology
The current SIDE prototype was implemented in Ubuntu
10.04 with the Linux-2.6.28.10 kernel, and focused only
8
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:39:49 UTC from IEEE Xplore.  Restrictions apply. 
on network device drivers. To evaluate the performance of
the SIDE prototype, we measured the throughput, latency
penalty and extra CPU usage when transmitting and receiv-
ing packets, and the recovery time between when a network
device driver crashes and when the network device driver is
restarted and begins transmitting/receiving packets. We set
up a test-bed consisting of two machines connected by a
Gigabit Ethernet switch, and each of these two machines
has an Intel(R) Core(TM)2 Duo CPU 3.00GHz processor,
1 GByte of DDR3 memory and an Gigabit Ethernet con-
troller of Intel Corporation 82566DM-2 Gigabit(e1000e).
To measure throughput and latency penalty, we wrote a
pair of UDP programs that sent and receive packets over
a UDP connection. To remove interference due to such
non-deterministic system effects as scheduling and timer
interrupts, every data point reported below is an average of
10 runs.
The network device driver used in this test came from
the Linux kernel distribution and was never modiﬁed. The
SIDE prototype includes a special loader that sets up a
special diver region in the kernel address space, inserts a set
of auxiliary functions into the driver, loads the unmodiﬁed
GE device driver into this region, and transparently resolves
some of the kernel service function called from the driver
to the added auxiliary functions.
B. Performance Overhead
When an in-kernel network device driver runs in a SIDE
environment, it is slowed down because of protection domain
crossings that arise when
1) The kernel calls a function in the driver and the driver
function returns to the kernel,
2) The driver calls a kernel service function and the called
kernel service function returns to the driver,
3) The driver directly accesses kernel data structures, and
4) The driver executes privileged instructions that are not
allowed at Ring 3.
Transmit
add preemt count
sub preempt count
dma map single
dma unmap single
dev kfree skb any
1.36
1.36
1
1
1
Receive
dma map single
netdev alloc skb
dma unmap single
skb put
eth type trans
netif receive skb
add preempt count
sub preempt count
1
1
1
1
1
1
0.02
0.02
The type and number of kernel service function calls that SIDE optimized
makes at the transmit and receive side for each packet.
Table I
SIDE base incurs all of the above protection domain
crossing overheads, whereas SIDE optimized removes the
protection domain crossing overhead due to (3) and (4),
and signiﬁcantly reduces the protection domain crossing
9
overhead due to (2). Table I shows the type and number
of kernel service function that SIDE base calls for each
packet. Moreover, for ease of implementation, SIDE base
sets up a separate page table for each driver, and therefore
each protection domain crossing is almost equal to a process
context switching, which requires a TLB ﬂushing.
Packet
Size
(bytes)
64
256
512
768
1024
SIDE–
base
(MBps)
4.9
18.5
38.6
56.3
69.5
SIDE–
optimized
(MBps)
55.3
92.5
104.2
107.5
108.7
In-Kernel
(MBps)
55.3
92.5
104.2
107.5
108.7
UDP throughput (in terms of Mbytes/sec) comparison among SIDE base,
SIDE optimized and the In-kernel driver conﬁguration under different
Table II
packet size
Table II shows the UDP throughput comparison among
the conventional in-kernel driver conﬁguration, SIDE base,
and SIDE optimized. The throughputs of SIDE optimized
and the in-kernel driver conﬁguration are practically
the same for all packet size, because the optimizations
SIDE optimized have reduced the additional protection do-
main crossing overhead to a negligible level. On the other
hand, the UDP throughput of SIDE base is between 11 times
(64 bytes) and 1.5 times (1024 bytes) smaller than that of
the in-kernel driver conﬁguration, depending on the packet
size. The smaller the UDP packet size, the more dramatic
the slow-down, because the per-packet protection domain
crossing overhead is largely ﬁxed, and is thus relatively more
pronounced when the packet size is smaller.
Packet
Size
(bytes)
64
256
512
768
1024
SIDE–
base
(µsec)
922.9
922.3
922.3
922.9
923.0
SIDE–
optimized
(µsec)
884.2
885.6
884.1
884.9
885.4
In-Kernel
(µsec)
882.2
879.7
882.0
880.7
881.9
Round-trip UDP packet latency comparison among SIDE base,
SIDE optimized, and the In-kernel driver conﬁ-guration, under different
Table III
packet size
Table III shows the round-trip UDP packet latency com-
parison among SIDE base, SIDE optimized, and the In-
kernel driver conﬁguration. Once again the average packet
latency of SIDE optimized is almost the same as that of
the In-kernel driver conﬁguration, but noticeably shorter
than that of SIDE base. The round-trip latency difference
between SIDE optimized and SIDE base does not vary
much with the packet size and is not as dramatic as their
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:39:49 UTC from IEEE Xplore.  Restrictions apply. 
throughput difference, because the delay attributed to the
network device driver accounts only for a small percentage
of the round-trip packet latency.
PacketSize
64 bytes
1400 bytes
Transmit
47.80%
13.00%
Receive
45.00%
11.50%
Table IV
Additional CPU overhead of an isolated network device driver when it
transmits and receives UDP packets in the SIDE architecture than in the
traditional in-kernel device driver architecture
Table IV shows the CPU usages of SIDE optimized of
transmitting and receiving packets continuously for different
packet sizes.
C. Contributions of Major Optimizations
The SIDE prototype includes a system resource virtualiza-
tion layer that is designed to reduce the number of accesses
to kernel data structures and the number of kernel service
function calls from an isolated device driver. The three major
optimizations in this virtualization layer are (1) streamlined
protection domain crossing, which uses sysenter/iret rather
than page fault for control transfer between the kernel and
isolated device drivers, (2) batched submit, which allows a
network device driver’s receive routine to logically submit a
set of incoming packets to the kernel’s protocol stack in one
shot rather than invoke a kernel service function to submit
one packet at a time, and (3) DMA address pre-translation,
which translates the virtual addresses of a set of outgoing
packets into their DMA addresses before calling a network
device driver’s transmit routine so that the latter does not
need to invoke a DMA address translation kernel service
function for every outgoing packet.
Packet Size (bytes)
64
256
512
768