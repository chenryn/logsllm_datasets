26.8
13.4
1.3
8.5
0.7
15.0
5.1
8.9
3.4
0.4
21.8
1.1
79.5
18.7
24.7
19.8
74.0
78.3
30.1
30.8
44.5
33.5
39.6
16.9
21.5
28.7
26.5
46.7
23.6
30.3
10.4
26.5
59.1
18.4
30.0
5.4
11.3
0.7
8.4
28.5
13.2
8.2
9.3
2.2
2.2
9.7
9.8
61.7
40.0
26.8
17.1
50.0
62.3
LIE,
the state-of-the-art AGR-agnostic attack for most of
the FL settings that we evaluate. For MNIST with Krum, the
impact of Min-Sum attack (simply Min-Sum) is 3× that of
LIE, for both updates-only and agnostic adversaries.
Except for CIFAR10 with VGG11, we note signiﬁcantly higher
impacts of Min-Sum on Krum than that of LIE. Note that LIE,
due to its small noise addition, regularizes and increases accu-
racy of the global model trained on Purchase using Krum. For
Bulyan, with agnostic and updates-only adversaries,
Min-Sum signiﬁcantly outperforms LIE by amounts varying
from 1.8% (for MNIST) to 22.1% (for Purchase) depending
on the classiﬁcation task.
On the other hand, Min-Max is more effective against
Multi-krum and outperforms LIE by amounts varying from
10.2% (MNIST) to 18.1% (Purchase) depending on the classi-
ﬁcation task. Min-Max is more effective against AFA, which
also computes an average of gradients in a selection set.
On AFA, Min-Max outperforms LIE for all datasets but
MNIST dataset; for MNIST the two attacks have almost the
same impacts. Min-Max is also more effective than LIE and
Min-Sum attacks against Trimmed-mean, Median, and Fang-
Trmean AGRs. For instance, depending on the classiﬁcation
task, Min-Max is almost 1.2× (for CIFAR10 + VGG11) to
8× (for Purchase) more impactful than LIE against Trimmed-
mean, while it is almost 1.2× (for CIFAR10 + VGG11) to
20× (for Purchase) more impactful than LIE against Median.
LIE attack is ineffective, because it adds very small
amounts of noises to compute its malicious gradients, while
our AGR-agnostic attacks are much more impactful as they
ﬁnd the most malicious gradient within a ball formed by
the benign gradients (Figure 1-(b,c)). For the same reason,
for all the considered scenarios, except for the combination
of Krum and FEMNIST dataset one or more of our AGR-
agnostic attacks also outperform AGR-tailored Fang attacks.
Due to the extreme non-iid nature of FEMNIST, the malicious
gradients of our AGR-agnostic attacks can be arbitrarily far
9
from benign gradients, which Krum can easily discard.
Reasons for the differences in the impacts of Min-Max
and Min-Sum attacks: Min-Max ﬁnds the malicious gradient
whose maximum distance from a benign gradient is less than
the maximum distance between any two benign gradient.
While, Min-Sum ﬁnds the malicious gradient such that the
sum of its distances from all the other gradients is less than
the sum of distances of any benign gradient from other benign
gradients. Therefore, as Figures 1-(b, c) demonstrate,
the
radius of search of malicious gradients of Min-Max is much
larger than that of Min-Sum. Therefore, the malicious gradients
of Min-Sum more effectively circumvent the ﬁltering of Krum
and Bulyan AGRs, and therefore, are more impactful against
these AGRs. For the same reason, Multi-krum selects a lesser
number of malicious gradients of Min-Max than that of Min-
Sum. But, as Multi-krum averages the selected gradients, Min-
Max, with signiﬁcantly more malicious gradients, damages the
Multi-krum aggregate more effectively than Min-Sum.
Finally, we note that for AGR-agnostic attacks, we observe
a few cases in Table II where agnostic adversary has
slightly more impact than updates-only adversary. For
example, Min-Max attack on (CIFAR10 + Alexnet + Multi-
krum) with agnostic adversary has 3.9% more impact
than with updates-only adversary. The reason for this
are various sources of randomness in our experiments. More
speciﬁcally, we do not use the exact same set of gradients
to compute malicious gradients under the two adversaries.
Instead, we instantiate the whole FL training every time we
compute the attack impact. Therefore, empirical randomness,
e.g., random initial model parameters,
in running the two
different instantiations may cause this behavior. Our experi-
mental results are the average of three such instantiations for
each of the presented result, and such empirical anomalies
can be mitigated in various ways, including setting the seed
for different random number generators and averaging over
multiple runs of experiments.
B. Comparing different threat models
Our work is the ﬁrst to comprehensively consider knowl-
edge based threat models (or adversaries) while designing
model poisoning attacks on FL. In this section, we compare
the performances of our attacks by the adversaries given in
Table I. For clarity of interpretation, we compare the two
extreme adversaries—agr-updates and agnostic;
the
other adversaries can be compared using the bold impacts in
Table II. Note that, even the weakest adversary, agnostic,
can effectively poison FL using our AGR-agnostic attacks.
For Krum, we analyze the maximum reduction in the attack
impact, Iθ, when the adversary changes from agr-updates
to agnostic. Iθ reduces from 39.2% to 27.0% for MNIST,
43.6% to 30.2% for CIFAR10 with Alexnet, 49.1% to 25.9%
for CIFAR10 with VGG11, and 30.0% to 1.9% for FEMNIST.
This is expected, as Krum selects a single gradient as its
aggregate, our attacks can ﬁne tune the malicious gradient
with the exact knowledge of all the benign gradients under
agr-updates adversary. But such ﬁne tuning is not possible
with agnostic adversary. Nevertheless, for Purchase, Iθ due
to both the adversaries are the same.
Figure 2: Selecting an effective perturbation: As explained in
Section VI-C2, for a given FL setting, if the AGR is known, our
adversary emulates attacks on the AGR using different ∇p’s and
selects ∇p with the highest attack impact (light bars). For unknown
AGR, the adversary selects ∇p which has the highest impact on the
maximum number of AGRs. This selection method is reliable due to
the transferability of attack impacts of ∇p from the emulated settings
(light bars) to actual FL (dark bars).
For all
the other AGRs,
the differences in the im-
pacts of the two adversaries is signiﬁcantly lesser than for
Krum. For instance, for Multi-krum, changing adversary from
agr-updates to agnostic reduces Iθ of our attack from
14.9% to 14.4% for MNIST, 36.8% to 31.7% for CIFAR10
with Alexnet, 32.5% to 32.3% for CIFAR10 with VGG11,
21.4% to 19.8% for Purchase, and 78.8% to 55.3% for FEM-
NIST. We observe similar differences in impacts for the rest of
the AGRs across all the datasets. This is because, unlike Krum
AGR, the other AGRs aggregate multiple gradients, including
many benign gradients. Hence, neither of the adversaries can
eliminate the good impact of these benign gradients on the ﬁnal
aggregates, which reduces the gap between their performances.
Figure 3 depicts the differences in Iθ’s of agr-only and
agnostic adversaries.
C. Effect of perturbation vectors
In this section, we show the signiﬁcant effect of the choice
of perturbation vector, ∇p, on the impacts of our attacks, Iθ.
Then, we give a procedure to select the most effective ∇p for
a given FL setting.
Section IV-A proposes to ﬁx the perturbation while opti-
mizing the attack objectives and also introduces three types
of perturbations. We assume 20% malicious clients, each with
some data from the benign distribution, and use agr-only
and agnostic adversaries.
1) Effect of perturbations on the attack impact: For a given
dataset, model, and AGR, varying the perturbation ∇p sig-
niﬁcantly changes the impact of our attacks, as the dark bars
in Figures 2 and 7 show. For instance, for MNIST with Krum,
Iθ of ∇p
uv is -4.7%, i.e., the global model accuracy increases
under attack, while ∇p
sgn increases Iθ signiﬁcantly to 24.1%.
10
0102030Attack impactMNISTpuvpstdpsgn02040Attack impactCIFAR10 + AlexnetKrumMulti-krumBulyanTrimmed-meanMedian0204060Attack impactPurchase(a) CIFAR10 with Alexnet architecture
(b) Purchase with fully connected network
Figure 3: Effect of increasing percentage of malicious clients on the impacts of model poisoning attacks on FL. We use adversaries who do
not know the gradients on benign devices, i.e., agr-only and agnostic adversaries.
For Krum, we note such large differences in Iθ’s of ∇p’s for
the other datasets as well. For the other AGRs with MNIST as
well, ∇p
sgn is the most effective perturbation and outperforms
the other perturbations by 3% to 15%. In case of CIFAR10
with either Alexnet or VGG11 model architectures, for all
AGRs but Krum, standard deviation based ∇p
std perturbation
has the highest attack impact, while for Krum, ∇p
uv has the
highest impact. For instance, attacks on CIFAR10 + Alexnet +
Multi-krum using ∇p
sgn have impacts of 36.8%,
14.2%, and 16.3%, respectively. Similarly for Purchase, ∇p
uv
has the highest attack impact across all the AGRs.
attack. In Figure 2, for each ∇p, the lighter bars show the
impacts of attacks on the emulated FL settings.
We compare the light and dark bars in Figure 2 and note
that, the relative effects of different perturbations are the
same across different AGRs, datasets, and models in both
emulated FL settings (light bars) and actual FL (dark
bars). In other words, the most effective perturbation in an
emulated FL setting, is also the most effective perturbation in
the corresponding actual FL. This transferability allows us to
reliably select the most effective perturbation when the AGR is
known. Finally, when the AGR is unknown, we simply pick the
perturbation with the highest impact across maximum number
of AGRs. For instance, we select ∇p
uv for Purchase due to its
highest attack impact on all AGRs. For CIFAR10, we choose
∇p
std as it has the maximum impact on all but Krum AGR. We
observe the same transferability for FEMNIST and CIFAR10
with VGG, as shown in Figure 7.
std, ∇p
uv, and ∇p
Even for AFA AGR, we observe the similar behavior, e.g.,
for MNIST and CIFAR10, the most effective perturbations
sgn and ∇p
are ∇p
std, respectively. For Fang-Trmean defense, all
the perturbations can be equally effective, as far as the cor-
responding malicious gradients are sufﬁciently large. Hence,
for a given dataset and Fang-Trmean, we simply choose the
perturbation that works for most of the other AGRs. For
instance, we use ∇p
sgn and ∇p
std for MNIST and CIFAR10,
respectively. Due to space restrictions, we omit the ﬁgures for
AFA and Fang-Trmean defenses.
2) How to select
the most effective ∇p?: Above, we
showed that selecting the appropriate perturbation is the
key to an effective model poisoning attack. However, as the
adversary cannot know the end result of using a particular ∇p,
she must decide the ∇p to use in each epoch of FL. Below,
we provide a simple yet effective method to select ∇p.
First, consider that the server’s AGR is known. We propose
that the adversary emulate AGR-tailored attacks (Section IV-B)
on the given FL settings and select as its ﬁnal ∇p the most
effective ∇p in the emulated setting. An example of emulated
AGR-tailored attack on MNIST with Krum is as follows: For
MNIST we assume total of 100 clients including 20 malicious
clients. Hence, the adversary emulates an FL setting with 20
benign and 4 malicious clients and mounts the AGR-tailored
11
D. Effect of federated learning parameters
1) Effect of non-iid degrees of data distribution: In this sec-
tion, we synthetically generate non-iid Purchase and MNIST
datasets using the scheme proposed in [17]. We assume 20%
malicious clients and plot the impacts of all the model poison-
ing attacks under agr-only and agnostic adversaries in
Figure 4. Note that, as the non-iid degree of data increases,
the impacts of all
the model poisoning attacks increase.
This is because the higher degree of non-iid makes it difﬁcult
for AGRs to reliably detect and remove malicious gradients.
This allows the adversaries to craft more malicious gradients
without being detected and increase their attack impacts. Our
attacks outperform previous attacks, especially at higher non-
iid degrees of data. Our experiments with FEMNIST, a real
world non-iid and imbalanced dataset, clearly show the signif-
icant superiority of our attacks over existing model poisoning
attacks (Section VI-A).
510152025Percent of attackers10203040Attack impactKrumLIEFangOur AGR-tailoredOur Min-MaxOur Min-Sum510152025Percent of attackers010203040Multi-krum510152025Percent of attackers010203040Bulyan510152025Percent of attackers10203040Trimmed-mean510152025Percent of attackers010203040Median510152025Percent of attackers200204060Attack impactKrumLIEFangOur AGR-tailoredOur Min-MaxOur Min-Sum510152025Percent of attackers05101520253035Multi-krum510152025Percent of attackers10203040Bulyan510152025Percent of attackers051015202530Trimmed-mean510152025Percent of attackers05101520MedianTable III: Comparing the state-of-the-art model poisoning attacks and our attacks under all threat models in Table I when cross-device FL is
used. Our AGR-tailored attacks signiﬁcantly outperform Fang attacks, while at least on of our AGR-agnostic attacks signiﬁcantly outperforms
LIE attack in most cases. Experimental setup is exactly the same as that of Table II.
Dataset
(Model)
CIFAR10
(Alexnet)
CIFAR10
(VGG11)
AGR
Krum
MKrum
Bulyan
TrMean
Median
AFA
FangTrmean
Krum
MKrum
Bulyan
TrMean
Median
AFA
FangTrmean
No
attack
(Aθ)
53.9
64.5
63.9
64.9
62.4
66.2
64.5
59.3
72.0
72.0
72.1
70.2
71.8
71.9
Gradients of benign devices are known
AGR agnostic
AGR tailored
(agr-updates)
(updates-only)
AGR tailored
(agr-only)
Gradients of benign devices are unknown
Fang [17]
11.0
2.2
2.0
8.3
1.8
1.6
7.2
3.8
1.4