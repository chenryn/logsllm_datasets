a state whereby its contents are accessible. We do not attempt to
address the thornier problems of encrypted communications, en-
crypted data stored by network services, nor the challenges posed
by third-party applications that impose their own layers of encryp-
tion. We eschew these issues not only because they are significantly
more complex but, more importantly, because we believe that they
are substantially less important in practice. Indeed, while well-
disciplined individuals may impose independent cryptographic
controls on their data, for most users (including most criminal sus-
pects), operating system-protected controls will subsume access to
most forms of data.
Within this focus, our primary design goal is to ensure that any
lawful access mechanism is not able to support a mass surveillance
use case, with a secondary goal of addressing risks of criminal
abuse or covert use. More concretely, in priority order we desire
the following properties in a system:
• Non-scalability. That any lawful access capability is suffi-
ciently constrained that there is no practical way to use it
for the purposes of mass surveillance.
• Authorization. That the use of this capability is only made
available subject to existing duly authorized legal processes
(i.e., as via a warrant).
• Particularity. That the capability to access a given device is
only useful for accessing that device (i.e., there is no master
secret to lose).
• Transparency. That any lawful access capability used to ac-
cess a device is transparent and cannot be easily concealed
from the user.
3.3 Threat model
Our threat model encompasses a range of threats including criminal
actors, rogue law enforcement officers, and well-funded organiza-
tions interested in large-scale intelligence collection. We anticipate
that these actors may have access to significant resources includ-
ing technical expertise, computation, and a non-trivial per-device
budget.16 However, we assume the adversary has no unique knowl-
edge that would allow them to bypass cryptographic primitives
whose computational hardness is commonly accepted as infeasibly
difficult. Moreover, our baseline is the security offered by existing
well-engineered devices and not the hypothetical security offered
by hypothetical devices.
We separate between threats at two different orders of magni-
tude: threats to the privacy of a single device and threats to the
privacy of large numbers of devices.
The former scenario represents the interest of an adversary in
unlocking a single device to which they are not legally authorized
to access. We assume that the adversary may have physical pos-
session of the device, can manipulate it directly via all wireless
interfaces, external wired interfaces, via direct manipulation of
internal hardware interfaces (e.g., JTAG), and may monitor any
passive side channels (e.g., differential power analysis, timing, etc.).
We do not include the security of the device design or code in
our threat model in light of the innate fate-sharing argument (e.g., if
Apple’s iPhone code is compromised at the source, or can be forced
to accept firmware updates, then there is little our design can do
to protect the user). Similarly, we do not consider covert physical
device tampering (e.g., if the adversary covertly replaces a phone
touchscreen with a replacement that records the user’s passcode).
While we have attempted to anticipate a range of active attacks
(e.g., Vcc glitching, overclocking, fault induction) we recognize that
a highly sophisticated adversary may be able to decap and directly
measure even tamper-resistant hardware (e.g., via scanning elec-
tron microscopy) or directly manipulate internal hardware state
and circuit pathways (e.g., via focused ion beam probes) [33, 35, 60].
We are not sanguine about current devices being able to resist this
level of adversary and, while we consider such actors at times in
our work, we make no significant claims that our design would be
able to provide such protection. Finally, our threat model explicitly
does not include breakdowns in the rule of law. We fully recognize
that even well-functioning democracies produce imperfect judi-
cial decisions. However, we argue that this situation is in no way
unique to technology and represents the nature of the fundamental
tradeoffs arising in a civil society.
The second scenario we are concerned with is one in which any
actor, at any level of sophistication, is able to bypass our design in a
way that very large numbers of phones could be accessed remotely
and covertly (i.e., mass surveillance). We have focused much of our
design on trying to avoid enabling any such capability.
4 BASIC DESIGN
The personal devices we consider (e.g., smart phones, laptops,
tablets, etc.) offer the ability to encrypt user data using a com-
bination of device-derived secret keys and user-derived secret keys.
While the implementation details can be fairly involved, inevitably
access to all such keys via the operating system is anchored by
a passcode or password that, when properly entered by the user,
“unlocks” the device and can be used to derive or access all critical
cryptographic material.
16Public reporting has indicated that Cellebrite charges US law enforcement customers
somewhere between $1,500 and $5,000 (per phone) to unlock the latest iPhones [51, 59].
For example, in the case of modern Apple iPhones, the system
combines a secret per-device UID and a user passcode to form a
passcode key. This key, in turn, wraps other keys that then may be
used to protect individual files. Note that central to this design is
the fact that the UID and the passcode validation logic can only be
accessed via the “Secure Enclave” component of the processor and
its state is not externally visible or easily modifiable. For additional
details see Apple’s “iOS Security Guide” white paper [16].
In our design, we focus entirely on providing independent access
to this passcode.17 Thus we impose no changes on any underlying
cryptographic algorithms, secrets or protocols used to secure device
state and similarly introduce no additional security concerns to
their operation. The core question of our work is this: how to safely
store the passcode such that it may be reasonably accessible under
court order but, in so doing, not unduly enable use that is either
unauthorized or might support mass-surveillance activities?
We describe our basic design approach to this problem here, start-
ing with two features—self-escrow and time vaulting—primarily
motivated by the goal of foreclosing mass-surveillance use.
Self-escrow
Self-escrow refers to the idea that when the user sets the device
passcode, this value is escrowed on the device itself rather than
with any external third-party. Third-party secret escrow poses a
range of challenges, including the need to trust a new third party,
the complexity of dynamic escrow protocols themselves and the
difficulty in knowing if the escrowed information has been used.
By physically locating the escrow agent on the device itself, many
of these challenges can be sidestepped (although versions of these
issues will re-emerge when we discuss authorization). Moreover, to
enforce this invariant, our design mandates that the escrow agent
is a write-only hardware component (i.e., one not readable via soft-
ware) and the only means of querying this component is via a direct
physical connection (e.g., via dedicated pins on the hardware pack-
age). Thus, access to the escrowed passcode requires both physical
possession of the device and, at least partial disassembly. By in-
sisting on affirmative physical possession, this design forecloses
the creation of any new remote access vulnerabilities. More im-
portantly, physical scaling properties impose practical limits on
how such a capability can be used; it does not lend itself to mass
surveillance use since seizing all devices is likely impractical. A
final benefit of this physical-centric approach is that we believe
it provides a more intuitive compatibility with common under-
standings of the government’s reasonable law enforcement powers
(e.g., the ability to seize physical property under court order) than
more information-centric approaches, which may appear covert by
comparison.
Time vaulting
While the physical access requirement is a strong one, time vaulting
makes it even more stringent, by requiring that physical access be
maintained over an extended period of time before an escrowed
passcode will be released. In particular, we require that the escrow
17We use the term passcode in this paper, but we intend it generically to mean all sorts
of deterministic user-provided digital credentials including passwords, pass phrases,
etc.
agent only respond after being continuously contacted by a re-
questor (i.e., via its internal physical interface) for the entirety of
a lockup period (e.g., 72 hours). This requirement further limits
the utility of our design for surveillance purposes and makes it
unattractive for covert use as well (e.g., “sneak and peak” or a range
of supply chain attacks). Moreover, while a modest access delay may
create some inconvenience for law enforcement in time-sensitive
circumstances, these appear to be the rare case in most processing
of digital evidence.18
Authorization
While self-escrow and time-vaulting greatly frustrate large-scale
use, they offer little protection for an individual stolen device nor
protection against a rogue law enforcement actor (i.e., who has
seized a phone without a warrant). Thus, a secondary element of the
escrow agent design is an authorization protocol whereby, after the
completion of the lockup period, the requestor is required to provide
evidence of explicit authorization before the device will be unlocked.
This evidence is provided by secret per-device state shared between
the escrow agent and the device manufacturer. A lawful actor could
obtain a court-order, served upon the manufacturer, to obtain this
particular per-device authorization key. However, such a key—even
if stolen—should provide no purchase on any other device.
Transparency
Finally, to minimize the value of this design for covert use, the
escrow agent permanently modifies any device that is interrogated
and whether or not it revealed the passcode. Upon startup, device
firmware can detect this modification and alert the user that the
device’s passcode has been provided. Optionally, the manufacturer
could alert the device owner via a separate protocol after they have
been compelled to provide the per-device secret.
5 IMPLEMENTATION ISSUES
In this section we explore the implementation issues posed by this
design in more detail, along with a discussion of potential threats
when compared with the existing status quo. For the purposes of
exposition, we will frequently describe this prospective implemen-
tation in the context of the Apple iPhone, but we do not believe
that our discussion is unique to that class of devices.
5.1 Self-escrow
The core requirement for the self-escrow agent is that its stored
value (the passcode) cannot be accessed except via an authorization
protocol requiring physical access. This in turn can be decomposed
into three separate design elements:
• Privileged (i.e., OS only) write-only interface for passcode
update (via CPU)
• Protection of passcode storage
• Physical access requirement for authorization requests
The first of these requirements is relatively straightforward to
implement. A simple microcontroller, with a serial interface and
a Flash block for storage would suffice (e.g., when the OS was
18Scanlon describes digital forensic backlogs of 6 to 12 months as being common [55]
and third-party lawful process production times are routinely weeks: e.g., Linkedin (3
weeks) [18], Facebook (2–6 weeks) [3, 62] and Ebay (20 business days) [5].
invoked to update the user’s passcode, it could then send a copy of
the new passcode to this device). However, for cost and design time,
a more likely implementation context would be within existing
“secure processor” environments (e.g., Apple’s SEP, Qualcomm’s
SPU). For example, in Apple’s “Secure Enclave” implementation, a
separate ARMv7a core, called the Secure Enclave Processor (SEP),
is dedicated to implementing security features. The SEP maintains
its own hardware isolated RAM, NVRAM and memory-mapped IO,
runs its own operating system (a variant of L4), and implements
a secure mailbox facility for controlled communication with the
Application Processor cores [16, 21, 58].19
This leaves the second requirement—protecting the secrecy of
the passcode storage itself. The most important protection is simply
that there is no “read interface” exported by the escrow agent and
so there is no mode of operation whereby software can request
the value of the passcode or read the memory used to store it.
This design is similar in nature to how Apple’s SEP stores secure
keys (e.g., the per-device Unique ID AES key, or UID) that are only
available to the SEP for use and cannot be read by any application
processor core [16]. That said, there is a broad and creative literature
documenting techniques for extracting protected state based on
side-channels that leak information as the state is used (e.g., via
power, timing, RF, etc.) In part to address this real threat, our design
does not access the self-escrowed passcode, or any value derived from
it, at any time before a complete and correct authorization request is
validated by the escrow agent. Since our design has no side-effects
that depend on the passcode value, most avenues for meaningful
side-channel attack are implicitly foreclosed.
However, another potential concern is direct physical attacks. As
mentioned earlier, well-resourced attackers could, after delayering
via careful processing and polishing, employ Scanning Electron
Microscopes (SEM) to directly image chip circuit layouts and then,
employing Passive Voltage Contrast (PVC), potentially extract the
charge state of individual memory cells. To date, the best published
work we are aware of has demonstrated resolving Flash cell contents
down to 0.21 microns in a traditional planar CMOS process [33]. We
cannot rule out that such attacks may be possible against more ad-
vanced System on Chip (SoC) targets (e.g., for reference the iPhone
6S A9 processor is reportedly fabricated with 16nm features in a 3D
FinFET process [39]; this is roughly an order of magnitude smaller).
There are a range of protections that have been developed and
proposed for addressing such attacks including obfuscated circuit
layouts, tamper detectors that drain charge and internal encryption
using PUFs (see Mishra et al. for a thorough review [46]). That said,
we are aware of no foolproof solution to this threat, but we believe
it is not unique to the problem described here. For example, an
adversary able to read out the state of Apple’s SEP (particularly
the UID and “effaceable key”) could also likely bypass the system’s
existing cryptographic protections on stored data (e.g., via offline
brute-forcing the passcode key).20 It is worth remembering that
such attacks are expensive, time-consuming, and error-prone, as
19Moreover, note that because the SEP handles passcode processing, any passcode
entered is already available to it (i.e., there is no new risk created by communicating
the passcode to the SEP).
20Indeed, we suspect that—absent other protections—the state of Apple’s UID is likely
easier to read out than a Flash cell because the typical implementation of eFuses are
larger and create clear circuit discontinuities.
well as destructive, and thus unlikely to be a widespread mechanism
used either for lawful access or by criminal actors.
Finally, we require that authorization requests are only possible
with physical access to the device. This requirement eliminates
the option of using wireless interfaces or existing wired ports (e.g.,
USB, Lightning) for authorization. Moreover, given the atypical
nature of lawful access needs, we believe it is reasonable to impose
a more significant physical access burden on the requestor. Con-