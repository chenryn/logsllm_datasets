a privacy policy. PriBot passes q to the ML layer and the
policy’s link to the Data Layer. The ML layer probabilis-
tically annotates q and each policy’s segments with the
privacy categories and attribute-value pairs of Fig. 3.
The segments in the privacy policy constitute the pool
of candidate answers {a1, a2, . . . , aM}. A subset G of the
answer pool is the ground-truth. We consider an answer
ak as correct if ak ∈ G and as incorrect if ak /∈ G . If G is
empty, then no answers exist in the privacy policy.
7.2 PriBot Ranking Algorithm
Ranking Score: In order to answer the user question,
PriBot ranks each potential answer6 a by computing a
proximity score s(q, a) between a and the question q.
This is within the Class Comparison module of the Ap-
plication Layer. To compute s(q, a), we proceed as fol-
lows. Given the output of the Segment Classiﬁer, an an-
swer is represented as a vector:
ααα = {p(ci|a)2 × p(v j|a) |∀ ci ∈ C , v j ∈ V (ci)}
6For notational simplicity, we henceforth use a to indicate an an-
swer instead of ak.
USENIX Association
27th USENIX Security Symposium    539
for categories ci ∈ C and values v j ∈ V (ci) descending
from ci. Similarly, given the output of the Query Ana-
lyzer, the question is represented as:
βββ = {p(ci|q)2 × p(v j|q) |∀ ci ∈ C , v j ∈ V (ci)}
The category probability in both ααα and βββ is squared to
put more weight on the categories at the time of com-
parison. Next, we compute a certainty measure of the
answer’s high-level categorization. This measure is de-
rived from the entropy of the normalized probability dis-
tribution (pn) of the predicted categories:
cer(a) = 1 − (− ∑ (pn(ci|a) × ln(pn(ci|a))) / ln(|C |))
(1)
Akin to a dot product between two vectors, we com-
pute the score s(q, a) as:
s(q, a) =
∑i(βi × min(βi, αi))
∑i β 2
i
× cer(a)
(2)
As answers are typically longer than the question and
involve a higher number of signiﬁcant features, this score
prioritizes the answers containing signiﬁcant features
that are also signiﬁcant in the question. The min func-
tion and the denominator are used to normalize the score
within the range [0, 1].
To illustrate the strength of PriBot and its answer-
ranking approach, we consider the following question
(posed by a Twitter user):
“Under what circumstances will you release to 3rd parties?”
Then, we consider two examples of ranked segments
by PriBot. The ﬁrst segment has a ranking score of 0.63:
“Personal information will not be used or disclosed for pur-
poses other than those for which it was collected, except
with the consent of the individual or as required by law. . . ”
The second has a ranking score of 0: “All personal in-
formation collected by the TTC will be protected by using
appropriate safeguards against loss, theft and unauthorized
access, disclosure, copying, use or modiﬁcation.”
Although both example segments share terms such as
“personal” and “information,” PriBot ranks them differ-
ently. It accounts for the fact that the question and the
ﬁrst segment share the same high-level category: 3rd
Party Collection while the second segment is categorized
under Data Security.
Conﬁdence Indicator: The ranking score is an internal
metric that speciﬁes how close each segment is to the
question, but does not relay PriBot’s certainty in report-
ing a correct answer to a user. Intuitively, the conﬁdence
in an answer should be low when (1) the answer is se-
mantically far from the question (i.e., s(q, a) is low), (2)
the question is interpreted ambiguously by Polisis, (i.e.,
classiﬁed into multiple high-level categories resulting in
a high classiﬁcation entropy), or (3) when the question
contains unknown words (e.g., in a non-English language
or with too many spelling mistakes). Taking into consid-
eration these criteria, we compute a conﬁdence indicator
as follows:
conf(q, a) = s(q, a) ∗
(cer(q) + frac(q))
2
(3)
where the categorization certainty measure cer(q) is
computed similarly to cer(a) in Eq. (1), and s(q, a) is
computed according to Eq. (2). The fraction of known
words frac(q) is based on the presence of the question’s
words in the vocabulary of our Policies Embeddings’ cor-
pus.
Potentially Conﬂicting Answers Another challenge is
displaying potentially conﬂicting answers to users. One
answer could describe a general sharing clause while an-
other speciﬁes an exception (e.g., one answer speciﬁes
“share” and another speciﬁes “do not share”). To miti-
gate this issue, we used the same CNN classiﬁer of Sec. 4
and exploited the fact that the OPP-115 dataset had op-
tional labels of the form: “does” vs. “does not” to indi-
cate the presence or absence of sharing/collection. Our
classiﬁer had a cross-validation F1 score of 95%. Hence,
we can use this classiﬁer to detect potential discrepancies
between the top-ranked answers. The UI of PriBot can
thus highlight the potentially conﬂicting answers to the
user.
8 PriBot Evaluation
We assess the performance of PriBot with two met-
rics: the predictive accuracy (Sec. 8.3) of its QA-ranking
model and the user-perceived utility (Sec. 8.4) of the pro-
vided answers. This is motivated by research on the eval-
uation of recommender systems, where the model with
the best accuracy is not always rated to be the most help-
ful by users [44].
8.1 Twitter Dataset
In order to evaluate PriBot with realistic privacy ques-
tions, we created a new privacy QA dataset. It is worth
noting that we utilize this dataset for the purpose of test-
ing PriBot, not for training it. Our requirements for this
dataset were that it (1) must include free-form questions
about the privacy policies of different companies and (2)
must have a ground-truth answer for each question from
the associated policy.
To this end, we collected, from Twitter, privacy-related
questions users had tweeted at companies. This approach
avoids subject bias, which is likely to arise when elicit-
ing privacy-related questions from individuals, who will
not pose them out of genuine need.
In our collection
methodology, we aimed at a QA test set of size be-
tween 100 and 200 QA pairs, as is the convention in
similar human-annotated QA evaluation domains, such
540    27th USENIX Security Symposium
USENIX Association
as the Text REtrieval Conference (TREC) and SemEval-
2015 [45, 46, 47].
and (3) Random as a control approach where questions
are answered with random policy segments.
To avoid searching for questions via biased keywords,
we started by searching for reply tweets that direct
the users to a company’s privacy policy (e.g., using
queries such as ”ﬁlter:replies our privacy policy” and
”ﬁlter:replies our privacy statement” ). We then back-
tracked these reply tweets to the (parent) question tweets
asked by customers to obtain a set of 4,743 pairs of
tweets, containing privacy questions but also substan-
tial noise due to the backtracking approach. Following
the best practices of noise reduction in computational
social science, we automatically ﬁltered the tweets to
keep those containing question marks, at least four words
(excluding links, hashtags, mentions, numbers and stop
words), and a link to the privacy policy, leaving 260 pairs
of question–reply tweets. This is an example of a tweet
pair which was removed by the automatic ﬁltering:
Question: “@Nixxit your site is very suspicious.”
Answer: “@elitelinux Updated it with our privacy policy.
Apologies, but we’re not fully up yet and running shoe
string.”
Next, two of the authors independently validated each
of the tweets to remove question tweets (a) that were
not related to privacy policies, (b) to which the replies
are not from the ofﬁcial company account, and (c) with
inaccessible privacy policy links in their replies. The
level of agreement (Cohen’s Kappa) among both anno-
tators for the labels valid vs. invalid was almost perfect
(κ = 0.84) [40]. The two annotators agreed on 231 of the
question tweets (of the 260), tagging 182 as valid and 49
as invalid. This is an example of a tweet pair which was
annotated as invalid:
Question: “What is your worth then? You can’t do it?
Nuts.”
Answer: “@skychief26 3/3 You can view our privacy policy
at http://t.co/ksmaIK1WaY. Thanks.”
This is an example of a tweet pair annotated as valid:
Question: “@myen Are Evernote notes encrypted at rest?”
“We’re not encrypting at rest, but are en-
Answer:
crypting in transit. Check out our Privacy Policy here:
http://bit.ly/1tauyfh.”
As we wanted to evaluate the answers to these ques-
tions with a user study, our estimates of an adequately-
sized study led us to randomly sample 120 tweets out of
the tweets which both annotators labeled as valid ques-
tions. We henceforth refer to them as the Twitter QA
Dataset.
8.2 QA Baselines
We compare PriBot’s QA model against three baseline
approaches that we developed: (1) Retrieval reﬂects the
state-of-the-art in term-matching retrieval algorithms, (2)
SemVec representing a single neural network classiﬁer,
Our ﬁrst baseline, Retrieval, builds on the BM25 algo-
rithm [48], which is the state-of-the-art in ranking mod-
els employing term-matching. It has been used success-
fully across a range of search tasks, such as the TREC
evaluations [49]. We improve on the basic BM25 model
by computing the inverse document frequency on the
Policies Corpus of Sec. 4.2 instead of a single policy.
Retrieval ranks the segments in the policy according to
their similarity score with the user’s question. This score
depends on the presence of distinctive words that link a
user’s question to an answer.
Our second baseline, SemVec employs a single clas-
siﬁer trained to distinguish among all the (mandatory)
attribute-values (with > 20 annotations) from the OPP-
115 dataset (81 classes in total). An example segment is
“geographic location information or other location-based
information about you and your device”. We obtain a
micro-average precision of 0.56 (i.e., the classiﬁer is, on
average, predicting the right label across the 81 classes
in 56% of the cases – compared to 3.6% precision for
a random classiﬁer). After training this model, we ex-
tract a “semantic vector”: a representation vector that
accounts for the distribution of attribute values in the in-
put text. We extract this vector as the input to the sec-
ond dense layer (shown Fig. 4). SemVec ranks the sim-
ilarity between a question and a policy segment using
the Euclidean distance between semantic vectors. This
approach is similar to what has been applied previously
in image retrieval, where image representations learned
from a large-scale image classiﬁcation task were effec-
tive in visual search applications [50].
8.3 Predictive Accuracy Evaluation
Here, we evaluate the predictive accuracy of PriBot’s
QA model by comparing its predicted answers against
expert-generated ground-truth answers for the questions
of the Twitter QA Dataset.
Ground-Truth Generation: Two of the authors gener-
ated the ground-truth answers to the questions from the
Twitter QA Dataset. They were given a user’s question
(tweet) and the segments of the corresponding policy.
Each policy consists of 45 segments on average (min=12,
max=344, std=37). Each annotator selected indepen-
dently, the subset of these segments which they consider
as best responding to the user’s question. This annota-
tion took place prior to generating the answers using our
models to avoid any bias. While deciding on the answers,
the annotators accounted for the fact that multiple seg-
ments of the policy might answer a question.
After ﬁnishing the individual annotations, the two an-
notators consolidated the differences in their labels to
reach an agreed-on set of segments; each assumed to be
USENIX Association
27th USENIX Security Symposium    541
Random
Retrieval
SemVec
PriBot
(a) top-k score
(b) NDCG
Fig. 8: Accuracy metrics as a function of k.
answering the question. We call this the ground-truth
set for each question. The annotators agreed on at least
one answer in 88% of the questions for which they found
matching segments, thus signifying a substantial over-
lap. Cohen’s κ, measuring the agreement on one or more
answer, was 0.65, indicating substantial agreement [40].
We release this dataset, comprising the questions, the
policy segments, and the ground-truth answers per ques-
tion at https://pribot.org/data.html.
We then generated, for each question, the predicted
ranked list of answers according to each QA model (Pri-
Bot and the other three baselines). In what follows, we
evaluate the predictive accuracy of these models.
Top-k Score: We ﬁrst report the top-k score, a widely
used and easily interpretable metric, which denotes the
portion of questions having at least one correct answer
in the top k returned answers. It is desirable to achieve a
high top-k score for low values of k so that the user has
to process less information before reaching a correct an-
swer. Fig. 8a shows how the top-k score varies as a func-
tion of k. PriBot’s model has the best performance over
the other three models by a large margin, especially at the
low values of k. For example, at k = 1, PriBot has a top-k
score of 0.68, which is signiﬁcantly larger than the scores
of 0.39 (Retrieval), 0.27 (SemVec), and 0.08 (Random)
(p-value < 0.05 according to pairwise Fisher’s exact test,
corrected with Bonferroni method for multiple compar-
isons). PriBot further reaches a top-k score of 0.75,
0.82, and 0.87 for k ∈{ 2, 3, 4}. To put these numbers in
the wider context of free-form QA systems, we note that
the top-1 accuracy reported by IBM Watson’s team on a
large insurance domain dataset (a training set of 12,889
questions and 21,325 answers) was 0.65 in 2015 [51] and
was later improved to 0.69 in 2016 [52]. Given that Pri-
Bot had to overcome the absence of publicly available
QA datasets, our top-1 accuracy value of 0.68 is on par
with such systems. We also observe that the Retrieval
model outperforms the SemVec model. This result is not
entirely surprising since we seeded Retrieval with a large
corpus of 130K unsupervised policies, thus improving its
performance on answers with matching terms.
Policy Length We now assess the impact of the policy
length on PriBot’s accuracy. First, we report the Nor-
malized Discounted Cumulative Gain (NDCG) [53]. In-
tuitively, it indicates that a relevant document’s useful-
ness decreases logarithmically with the rank. This met-
ric captures how presenting the users with more choices
affects their user experience as they need to process
more text. Also, it is not biased by the length of the
policy. The DCG part of the metric is computed as
DCGk = ∑k
log2(i+1) , where reli is 1 if answer ai is cor-
rect and 0 otherwise. NDCG at k is obtained by normal-
izing the DCGk with the maximum possible DCGk across
all values of k. We show in Fig. 8b the average NDCG
across questions for each value of k. It is clear that Pri-
Bot’s model consistently exhibits superior NDCG. This
indicates that PriBot is poised to perform better in a sys-
tem where low values of k matter the most.
i=1
reli
Second, to further focus on the effect of policy length,
we categorize the policy lengths (#segments) into short,
medium, and high, based on the 33rd and the 66th per-
centiles (i.e., corresponding to #segments of 28 and 46).
We then compute a metric independent of k, namely, the
Mean Average Precision (MAP), which is the mean of
the area under the precision-recall curve across all ques-
tions. Informally, MAP is an indicator of whether all the
correct answers get ranked highly. We see from Fig. 9
that, for short policies, the Retrieval model is within 15%
of the MAP of PriBot’s model, which makes sense given
the smaller number of potential answers. With medium-
sized policies, PriBot’s model is better by a large margin.
This margin is still considerable with long policies.
Conﬁdence Indicator Comparing the conﬁdence (using
the indicator from Eq. (3)) of incorrect answers predicted
by PriBot (mean=0.37, variance=0.04) with the conﬁ-
dence of correct answers (mean=0.49, variance =0.05)
shows that PriBot places lower conﬁdence in the answers
that turn out to be incorrect. Hence, we can use the con-
ﬁdence indicator to ﬁlter out the incorrect answers. For
example, by setting the condition: conf(q, a) ≥ 0.6 to ac-
cept PriBot’s answers, we can enhance the top-1 accu-
racy to 70%. This indicator delivers another advantage:
its components are independently interpretable by the ap-
plication logic. If the score s(q, a) of the top-1 answer is
too low, the user can be notiﬁed that the policy might not
contain an answer to the question. A low value of cer(q)
indicates that the user might have asked an ambiguous
question; the system can ask the user back for a clariﬁca-
tion.
Pre-trained Embeddings Choice As discussed in
Sec. 4, we utilize our custom Policies Embeddings,
which have the two properties of (1) being domain-
speciﬁc and (2) using subword embeddings to handle
out-of-vocabulary words. We test the efﬁcacy of this
choice by studying three variants of pre-trained embed-
dings. For the ﬁrst variant, we start from our Policies
Embeddings (PE), and we disable the subwords mode,
542    27th USENIX Security Symposium
USENIX Association
Random
Retrieval
SemVec
PriBot
WP-NoSub
WP
PE-NoSub
PE
Fig. 9: Variation of MAP
across policy lengths.
top-k score of
Fig. 10:
PriBot with different pre-
trained embeddings.
thus only satisfying the ﬁrst property; we call it PE-
NoSub. The second variant is the fastText Wikipedia Em-
beddings from [54], trained on the English Wikipedia,
thus only satisfying the second property; we denote it as
WP. The third variant is WP, with the subword mode
disabled, thus satisfying neither property; we call it WP-
NoSub. In Fig. 10, we show the top-k score of PriBot
on our Twitter QA dataset with each of the four pre-
trained embeddings. First, we can see that our Policies
Embeddings outperform the other models for all values
of k, scoring 14% and 5% more than the closest vari-
ant at k = 1 and k = 2, respectively. As expected, the
domain-speciﬁc model without subwords embeddings
(PE-NoSub) has a weaker performance by a signiﬁcant
margin, especially for the top-1 answer. Interestingly, the
difference is much narrower between the two Wikipedia
embeddings since their vocabulary already covers more
than 2.5M tokens. Hence, subword embeddings play a
less pronounced role there. In sum, the advantage of us-
ing subwords embeddings with the PE model originates
from their domain speciﬁcity and their ability to compen-
sate for the missing words from the vocabulary.
8.4 User-Perceived Utility Evaluation
We conducted a user study to assess the user-perceived