in order to achieve a memory collision with a vulnerable
freed object, we expect the object to be allocated at the
place where some other critical kernel objects controlled by
us will be later placed. To reliably reach a memory col-
lision, our attack leverages a set of characteristics of ker-
nel operations (e.g., the style of object allocation). Since
Linux kernel always ﬁrst recycles freed memory for a future
allocation due to physical memory limitation and for per-
formance enhancement, our proposed attack strategy lever-
ages this observation: Once an allocated vulnerable object
is freed, the kernel will recycle the space occupied by that
object for a recent allocation. The insight of our attack
strategy is that we always try to ﬁnd a candidate which is
to be chosen by the kernel to reuse the freed memory once
occupied by a vulnerable object. The candidate could be an
object, a buﬀer or even a mapped area from the user space
(physmap). Based on a thorough understanding of the ker-
nel’s allocation mechanism, we select reasonable candidates
and intentionally arrange the attack by manipulating these
candidates to turn a blind memory overwriting into a stable
memory collision with high probability.
Speciﬁcally, in this paper we propose two concrete mem-
ory collision attacks with diﬀerent attack surface respec-
tively. The ﬁrst attack constructs memory collision by fol-
lowing the working mechanism of kernel allocators and con-
ducts an attack under several heap protections (e.g.
the
separation among objects of diﬀerent types). The second
attack relies on the fact that a vulnerable object can also
collide with a mapped area in kernel memory, where the
candidate is typically not a single kernel object. By utiliz-
ing mapped memory, the second attack becomes much more
universal since it does not concern the separation of itself
with other kernel objects. These two attacks can be widely
applied for exploiting use-after-free vulnerabilities in Linux
kernel and overcome most diﬃculties brought by kernel al-
locators. In the following, we detail the key part of these
two attacks.
3. OBJECT-BASED ATTACK
In this section, we present the details of object-based
memory collision attack in Linux kernel. Before discussing
the attack, we ﬁrst introduce the working mechanism of
kernel allocators. Then we introduce two types of object-
based memory collision attacks: the memory collision hap-
pens between objects of the same size or between objects of
diﬀerent sizes.
3.1 Memory Allocation of Linux Kernel
In Linux kernel, the SLAB/SLUB allocators are respon-
sible for allocations of kernel objects. For kernel objects of
a speciﬁc type, a corresponding storage unit is created by
the SLAB allocator as a container, which is called SLAB
cache. It contains the data associated to objects of the spe-
ciﬁc kind of the containing cache [18]. Moreover, there are
generally two interfaces to allocate objects in Linux kernel.
One is kmem cache alloc [25], for which a type of a SLAB
cache should be speciﬁed; The other one is kmalloc [25],
which only needs a allocation size without a cache type.
The objects created by invoking kmalloc are still classiﬁed
into diﬀerent SLAB caches according to their sizes, These
SLAB caches are named as kmalloc-size SLAB caches. An-
other important kernel allocator called SLUB allocator has
been in use in Linux kernel in 2008 [10] and improves the
performance of the SLAB allocator.
The SLAB/SLUB allocators introduce mainly two re-
strictions to an attack. First, the heap management mech-
anism adopted by Linux kernel generally prevents attack-
ers from creating memory collisions between kernel objects.
SLAB caches separate kernel objects of one type from those
of another type. It is therefore impossible to insert a new
object into the free position of a SLAB cache and let the
objects of two diﬀerent types to be stored in one cache at
the same time. When a use-after-free vulnerability in Linux
kernel is to be leveraged, such separation brings diﬃculties
for attackers to create memory collisions between kernel ob-
jects of diﬀerent types. Second, considering a typical state
of the kernel heap, when an object is to be allocated, there
might exist several half-full SLAB caches which are able to
store it. The holes in these SLAB caches should be allo-
cated by kernel allocators with higher priority. To ensure
that vulnerable objects are allocated into recently created
SLAB caches instead of existing ones, a reliable attack must
consider this property and try to ﬁrst ﬁll every hole in these
half-full SLAB caches. This process is often referred to as
defragmentation.
3.2 Collision between Objects of the Same Size
We ﬁrst present a memory collision applied without break-
ing the size rule provided by the SLUB allocator.
Objects have diverse sizes among various Linux kernels
due to diﬀerent kernel sources and conﬁgurations during
the compilation. Figure 2 illustrates a part of the result
of executing slabinfo on a 32-bit Linux. In fact, the SLUB
allocator tries to merge kernel objects of the same size in-
stead of the same type into one cache, which helps to re-
duce overhead and increases cache hotness of kernel objects.
As shown in Figure 2, the kernel objects of diﬀerent types
are classiﬁed into the same cache if they have an identical
size. For example, both the vm area struct object and the
kmalloc-96 object have a size of 96 bytes, which indicates
that these two objects have a high opportunity to be al-
located into the same cache in the kernel. This behavior
Figure 1: Memory Collision Attack
ject. And the parameters M and N can be speciﬁed by
attackers based on the actual situation.
Listing 2: Object-based Attack
1 /* setting up shellcode */
2 void *shellcode = mmap(addr, size, PROT_READ |
PROT_WRITE | PROT_EXEC, MAP_SHARED | MAP_FIXED
| MAP_ANONYMOUS, -1, 0);
Figure 2: Partial result of executing slabinfo on 32-
bit Linux
allows attackers to create memory collisions between kernel
objects of the same size.
The vulnerable module introduced in 2.1 is used to illus-
trate how our attack ﬁll freed objects with controlled data.
In that module, the size of every freed vulnerable object is
512 bytes. In order to introduce a collision, a candidate ob-
ject is selected to re-occupy a previously freed space which
has a diﬀerent type but with the same size (512 bytes),
based on the size rule of the SLUB allocator. Meanwhile,
in order to control the content of freed space, the data of
a proper candidate object should also be assigned by an
attacker. Thus kmalloc-size buﬀer in kernel is the best
choice due to its easy allocation, diverse sizes and capabil-
ity of fully controlling the re-ﬁlling data. For example, a
transfer buﬀer will be allocated by kmalloc during the pro-
cess of sendmmsg. And attackers can set the buﬀer size as
it represents the length of the control message and can also
control the data in the buﬀer as it represents which control
message one wants to deliver.
The following code in Listing 2 leverages the use-after-free
vulnerability in the malicious kernel module in Section 2.1
to compromise the kernel and execute arbitrary kernel code
by applying object-based collision attack. The most impor-
tant part, exploiting, involves four essential steps including
allocating objects, freeing objects, overwriting freed objects
and re-using freed objects. Note that the length of the buﬀer
is 512 bytes, which is equal to the size of the vulnerable ob-
3 ...
4
5 /* exploiting
6
7
D: Number of objects for defragmentation
M: Number of allocated vulnerable objects
N: Number of candidates to overwrite
index = syscall(NR_SYS_UNUSED, 1, 0);
8
9 */
10
11 /* Step 1: defragmenting and allocating objects */
12 for (int i = 0; i < D + M; i++)
13
14 /* Step 2: freeing objects */
15 for (int i = 0; i < M; i++)
syscall(NR_SYS_UNUSED, 2, i);
16
17 /* Step 3: creating collisions */
18 char buf[512];
19 for (int i = 0; i < 512; i += 4)
20
21 for (int i = 0; i < N; i++) {
struct mmsghdr msgvec[1];
22
msgvec[0].msg_hdr.msg_control = buf;
msgvec[0].msg_hdr.msg_controllen = 512;
...
syscall(__NR_sendmmsg, sockfd, msgvec, 1, 0);
*(unsigned long *)(buf + i) = shellcode;
26
23
24
25
}
27
28 /* Step 4: using freed objects (executing shellcode)
*/
29 for (int i = 0; i < M; i++)
30
syscall(NR_SYS_UNUSED, 3, i);
3.3 Collision between Objects of Different Sizes
The attack described in Section 3.2 has an obvious weak-
ness: it can only be applied when the size of a vulnerable
object is aligned to one of possible kmalloc sizes, which
are mainly powers of 2. Considering such situation that a
vulnerable object has a size of 576 bytes, neither kmalloc-
512 nor kmalloc-1024 objects are able to make collisions
with the vulnerable object by the solution mentioned in
SLABSLABSLABSLABSLABSLABSLABSLABSLABSLABSLABSLAB+0x00: 31 c9  |xor %ecx, %ecx+0x02: 56     |push %esi+0x03: 5b     |pop %ebx+0x04: 6a 3f  |push $0x3f+0x06: 58     |pop %eax（shell code）+0x00:.RefCount+0x04:.Size     …+0x1c:.Function Pointer       Userland AddressDefragmentingAllocatingBug freeingObject-based coveringMap-based coveringFreed vulnearable ObjectVulnerable objectObject for defragmentingAllocatedFreed vulnearable objectFree:t-0000096   vm_area_struct kmalloc-96:t-0000128   bio_integrity_payload eventpoll_epi :t-0000192   biovec-16 kmalloc-192:t-0000256   pool_workqueue kmalloc-256:t-0000288   fuse_request bsg_cmd:t-0000384   dio sgpool-16:t-0000448   mm_struct skbuff_fclone_cache:t-0000576   ecryptfs_sb_cache:t-0000640   RAW PING UNIX:t-0000768   sgpool-32 biovec-64:t-0000832   task_xstate RAWv6 PINGv6:t-0001536   sgpool-64 biovec-128:t-0003072   sgpool-128 biovec-256:t-0004096   kmalloc-4096 names_cacheSection 3.2. Thus a more universal collision attack is re-
quired.
Assume that the vulnerable kernel module is modiﬁed on
Line-27 and the size property of the cache is changed from
512 bytes to 576 bytes, we present an advanced attack which
ignores the size of a target vulnerable object. And this time
the attack still adopts kernel buﬀers of kmalloc-size type
as candidates for memory collisions.
For the SLAB allocator in Linux kernel, if all objects in
one SLAB cache are freed, the entire SLAB cache is going
to be recycled for a future allocation.
It reveals the fact
that freed SLAB caches can be later used to hold objects of
a completely diﬀerent type, which allows to overcome size-
isolated barriers. Thus, for our new attack, several new
SLAB caches are created and ﬁlled with vulnerable objects
in the very beginning. Note that all of the objects stored
in these SLAB caches are the targets to collide with. Then
by triggering the use-after-free vulnerability, all of these
objects are released but still can be accessed by attackers.
When all the objects in these SLAB caches are freed, the
space of these SLAB caches previously created for vulner-
able objects is going to be recycled by the kernel. And
that freed space will be used later for kmalloc-size buﬀers
created by invoking sendmmsg.
In fact, we can still use the same code listed in Section 3.2
to exploit the modiﬁed vulnerable kernel module and exe-
cute kernel shellcode. Either kmalloc-256 or kmalloc-128
buﬀers can be chosen as candidates to overwrite freed mem-
ory. And we also need larger M and N parameters to guar-
antee the reliability in this attack.
4. PHYSMAP-BASED ATTACK
The object-based attack by memory collisions between
kernel objects of diﬀerent sizes has an obvious weakness,
the uncertainty.
In this section, we present a more uni-
versal attack which leverages a speciﬁc mapped area called
physmap in kernel memory without the weaknesses men-
tioned above.
In fact, the physmap is originally used in
Ret2dir technique [17] to bypass currently applied protec-
tions in Linux kernel. Because the data crafted by attackers
in user space is directly mapped by the physmap into kernel
space, thus the physmap can be used to rewrite the kernel
memory previously occupied by freed vulnerable object and
exploit use-after-free vulnerabilities.
Once attackers call mmap with an expected virtual ad-
dress in user space and then call mlock on that virtual ad-
dress, these pages in user space may be directly mapped
into the physmap in kernel space. Therefore, the attack
is performed by repeatedly invoking mmap in user space
and spraying proper data in the physmap area. For the
sake of convenience, the physmap mentioned in the rest of
the paper represents the part of the directly mapped space
in kernel which has already been ﬁlled with the payload
sprayed by attackers.
Again, we illustrate our attack by exploiting the vulnera-
ble kernel module mentioned in Section 2.1. As the current
intended approach is to take advantage of the physmap to
make a collision attack and rewrite freed objects, all the
caches which contain target vulnerable objects should be
recycled by the kernel for future allocation because the
physmap never occupies the virtual memory in use. At
the beginning of a physmap-based attack, defragmentation
by certain kernel objects that have the same size of vulnera-
ble objects is conducted (as mentioned in Section 3). They
are used to pad free holes inside all half-full SLAB caches.
And then new and clean SLAB caches will be created by
kernel allocators when vulnerable objects are going to be
allocated.
Figure 3: Kernel Memory Layout
Later, after all of these vulnerable objects are released by
vulnerable syscalls, a certain amount of free SLAB caches
once storing these vulnerable objects are generated and re-
cycled by Linux kernel. They may be served as expanding
area of the physmap where memory collisions happen in the
future.
However, to improve the probability that memory colli-
sions happen between target objects and the physmap, the
location where kernel objects are allocated should be lift
up. The layout of the kernel memory including the SLAB
caches and the physmap is shown in Figure 3. It can be seen
that the physmap begins at a relatively low virtual address,
meanwhile SLAB caches usually gather at a higher address.
Our goal is to create memory collisions between these two
areas. Due to the capacity of physical memory, the total
size of data sprayed in the physmap has an upper bound.
Furthermore, the physmap always tends to require a piece
of complete free memory of a certain size for expansion.
If vulnerable objects are allocated immediately after de-
fragmentation, the physmap might not be able to expand
and eventually cover the SLAB caches where once target
objects are placed.
Thus our plan is to spray kernel objects in group. For
each group, a certain amount of kernel objects are sprayed
for padding and then several vulnerable kernel objects are
to be allocated, which is treated as the target to be collided
with. That makes these vulnerable objects dispersedly ap-
pear in kernel space, which sharply increase the probability
of memory collisions between the physmap and vulnerable
objects. In addition, if a target vulnerable object can be
easily allocated and de-allocated by attackers in their pro-
grams, a proper choice of the padding object is just the
vulnerable object itself.
Next, the objects for padding can be released through