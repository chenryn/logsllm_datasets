}
void prepare ( char * s ) {
output ( s );
}
void output ( char * s ) {
// failure m a n i f e s t s
}
Figure 7: How stack hashing can over- and undercount bugs
Table 3: Stack hashing results for cxxfilt. The first column
specifies the label we assign based testing progressive ver-
sions of cxxfilt. The second column specifies the number
of distinct stack hashes among the inputs assigned to the
ground truth label. The third column counts how many of
the stack hashes from the second column appear only with
those inputs grouped by the label in the first column, while
the fourth column counts how many stack hashes appear in
other labels. The final column counts the number of distinct
inputs in a label.
Bug
A
B
C
D
E
F
G
H
I
unfixed
unknown
# Hashes Matches
9
362
24
159
15
15
2
1
4
28
4
2
343
21
119
4
1
0
1
4
12
0
False Matches
7
19
3
40
11
14
2
0
0
16
4
Input count
228
31,103
106
12,672
12,118
232
2
568
10
98
4
function that also corrupts s prior to passing it to prepare. Setting
N to 2 would improperly conflate crashes due to that bug and ones
due the buggy format, since only the last two functions on the
stack would be considered.
Assessing against ground truth. We measured the effectiveness of
stack hashing by comparing its determinations against the labels for
bugs that we identified in the prior experiment. Our implementation
of stack hashing uses Address Sanitizer to produce a stack trace for
each crashing input to cxxfilt, and chooses N as the top 3 entries
on the stack for hashing.
Our analysis discovered that stack hashing is far more effective
at deduplicating inputs than coverage profiles, but would still over-
count the number of bugs discovered. Table 3 shows the results
of the comparison of stack hashing to the labels we identified.
As an example, consider label B, which represents 31,103 inputs
(column 5). Of those inputs, 362 distinct stack hashes were produced
(column 2). If the stack hash metric was the only knowledge we had
about the distribution of bugs in cxxfilt, we would claim to have
discovered two orders of magnitude more bugs than we actually
did. On the other hand, stack hashing seems to do very well for
label H: one hash matched all 568 inputs. In sum, across all runs,
595 hashes corresponded to 9 bugs, an inflation of 66×, as compared
to 57,044 coverage profile-unique inputs for 9 bugs, an inflation
of 6339×.7 4 crashing inputs were each associated with their own
“fixing” commit, but when we inspected the respective code changes
we could not see why the changes should fix a crash. As such, we
have listed these inputs in Table 3 as “unknown.” ASAN/UBSAN
does not detect all possible undefined behaviors, so it may be that
a code or data layout change between compilations or some other
form of non-determinism is suppressing the crashing behavior. A
compiler bug is also a possibility. We are continuing to investigate.
While stack hashing does not overcount bugs nearly as much
as AFL coverage profiles, it has the serious problem that hashes
are not unique. For example, only 343 of those for label B matched
only inputs associated with B (column 3). The remaining 19 also
matched some other crashing input (column 4). As such, these other
inputs would be wrongly discarded if stack hashing had been used
for de-duplication. Indeed, for label G, there is no unique hash
(there is a 0 in column 3)—it only falsely matches. Overall, about
16% of hashes were non-unique.8 As such, stack hashing-based
deduplication would have discarded these bugs.
Discussion. Table 3 shows another interesting trend also evident,
but less precisely, in Figure 6. Some bugs are triggered by a very
small number of inputs, while others by a very large number. Bugs
G and I each correspond to only 2 or 10 inputs, while bugs B, D,
and E correspond to more than 10K inputs. Prior fuzzing studies
have found similar bug distributions [11]. While Table 3 combines
all inputs from all trials, considering each trial individually (as per
Figure 6) we find that no single run found all 9 bugs; all runs found
bugs B, D, E, but no run found more than 5 additional bugs.
An important open question is whether the trends we observe
here with cxxfilt hold for other target programs. To answer this
question would require more “ground truth” analysis of the flavor
we have carried out here. Assuming they do hold, we draw two ten-
tative conclusions. First, the trends reinforce the problem with bug
heuristics: in the presence of “rare” inputs, the difference between
finding 100 crashing inputs and 101 (an apparently insignificant
difference) could represent finding 1 or 2 unique bugs (a significant
one). Second, fuzzers might benefit from an algorithmic trick em-
ployed by SAT solvers: randomly “reboot” the search process [46]
by discarding some of the current state and starting again with the
initial seed, thus simulating the effect of running separate trials. The
challenge would be to figure out what fuzzer state to retain across
reboots so as to retain important knowledge but avoid getting stuck
in a local minimum.
7The table tabulates crashing inputs across all trials put together: if instead you consider
the stack hashes taken on a per-run basis (as in Figure 6), the results will be somewhat
different, but the overall trends should remain the same.
8This value was computed by summing the total distinct number of hashes that show
up in more than one row (a lower bound of the total in column 4) and dividing by the
total of distinct hashes overall (a lower bound of the total in column 2).
Related Work. Recent work by van Tonder et al. [51] also experi-
mentally assesses the efficacy of stack hashing and coverage profiles
against ground truth. Like us, they defined ground truth as single
conceptual bugs corrected by a particular code patch. They com-
pared how well coverage profiles and stack hashes approximate this
ground truth. Like us, they found that both tended to overcount the
number of true bugs. As they consider different patches and target
programs, their study is complementary to ours. However, their set
of crashing inputs was generated via mutations to an initial known
crashing input, rather than via a normal fuzzing process. As such,
their numbers do not characterize the impact of poor deduplication
strategies in typical fuzzing use-cases, as ours do.
Pham et al. [42] also studied how stack hashes, for N = 1 and N =
∞, can over- and under-count bugs identified through symbolic
execution. Their interest was a comparison against their own de-
duplication technique, and so their study did not comprehensively
consider ground truth.
7.4 Code Coverage
Fuzzers are run to find bugs in programs. A fuzzer that runs for a
long period of time and finds no bugs would be seen as unsuccessful
by its user. It seems logical to evaluate a fuzzer based on the number
of bugs that fuzzer finds. However, just because a fuzzer does not
find a bug may not tell us the whole story about the fuzzer’s efficacy.
Perhaps its algorithm is sound but there are few or no bugs to find,
and the fuzzer has merely gotten unlucky.
One solution is to instead (or also) measure the improvement in
code coverage made by fuzzer A over baseline B. Greybox fuzzers
already aim to optimize coverage as part of the isInteresting
function, so surely showing an improved code coverage would
indicate an improvement in fuzzing. This makes sense. To find a
crash at a particular point in the program, that point in the program
would need to execute. Prior studies of test suite effectiveness also
suggest that higher coverage correlates with bug finding effective-
ness [19, 30]. Nearly half of the papers we considered measured
code coverage; FairFuzz only evaluated performance using code
(branch) coverage [32].
However, there is no fundamental reason that maximizing code
coverage is directly connected to finding bugs. While the general
efficacy of coverage-guided fuzzers over black box ones implies
that there’s a strong correlation, particular algorithms may eschew
higher coverage to focus on other signs that a bug may be present.
For example, AFLGo [5] does not aim to increase coverage globally,
but rather aims to focus on particular, possibly error-prone points
in the program. Even if we assume that coverage and bug finding
are correlated, that correlation may be weak [28]. As such, a sub-
stantial improvement in coverage may yield merely a negligible
improvement in bug finding effectiveness.
In short, we believe that code coverage makes sense as a sec-
ondary measure, but that ground truth, according to bugs discov-
ered, should always be primary.
are usually made by testing the fuzzer on a benchmark suite that
purports to represent the population. The idea is that good perfor-
mance on the suite should translate to good performance on the
population. How should we choose such a benchmark suite?
Recent published works have considered a wide variety of bench-
mark programs. Broadly, these fall into two categories, as shown
in the second column in Table 1: real programs and artificial pro-
grams (or bugs). Examples of the former include the Google fuzzer
test suite (“G”) [18] and ad hoc selections of real programs (“R”).
The latter comprises CGC (“C”) [14], LAVA-M (“L”) [16], and hand-
selected programs with synthetically injected bugs (“S”). Some pa-
pers’ benchmarks drew from both categories (e.g., VUzzer [44] and
Steelix [33]). As we discuss below, no existing benchmark choice
is entirely satisfying, thus leaving open the important question of
developing a good fuzzing benchmark.
8.1 Real programs
According to Table 1, nearly all papers used some real-world pro-
grams in their evaluations. Two of these papers [49, 56] used the
Google Fuzzer Test suite [18], a set of real-world programs and
libraries coupled with harnesses to focus fuzzing on a set of known
bugs. The others evaluated on a hand selected set of real-world
programs.
We see two problems with the way that real programs have
been used as fuzzing targets. First, most papers consider only a
small number of target programs without clear justification of
their representativeness. The median number of programs, per
Table 1, is seven. Sometimes a small count is justified; e.g., IMF
was designed specifically to fuzz OS kernels, so its evaluation on
a single “program,” the MacOS kernel, is still interesting. On the
other hand, most fuzzers aim to apply to a larger population (e.g.,
all file processing programs), so 7 would seem to be a small number.
A positive outlier was FuzzSim, which used a large set of programs
(more than 100) and explained the methodology for collecting them.
As evidence of the threat posed by a small number of insuffi-
ciently general targets, consider the experimental results reported
in Figure 2, which match the results of Böhme et al [6]. The first row
of the figure shows results for nm, objdump and cxxfilt, which were
the three programs in which Böhme et al found crashes.9 Focusing
our attention on these programs suggests that AFLFast is uniformly
superior to AFL in crash finding ability. However, if we look at the
second row of the figure, the story is not as clear. For both FFmpeg
and gif2png, two programs used in other fuzzing evaluations, the
Mann Whitney U test shows no statistical difference between AFL
and AFLFast. Including these programs in our assessment weakens
any claim that AFLFast is an improvement over AFL.
The second problem we see with the use of real programs to date
is that few papers use the same targets, at the same versions. As
such, it is hard to make even informal comparisons across different
papers. One overlapping set of targets were binutils programs, used
in several evaluations [5, 6, 10, 32]. Multiple papers also considered
FFmpeg and gif2png [9, 44, 45, 55, 58]. However, none used the
8 TARGET PROGRAMS
We would like to establish that one fuzzing algorithm is generally
better than another, i.e., in its ability to find bugs in any target
program drawn from a (large) population. Claims of generality
9Figure 6 of their paper presents a similar series of plots. The differences in their
plots and ours are the following: they plot the results on log scale for the Y axis; they
consider six-hour trials rather than 24-hour trials; and they do not plot median and
confidence intervals computed over 30+ runs, but rather plot the mean of 8 runs. They
also use different versions of AFL and AFLFast.
same versions. For example, the versions of binutils were different
in these papers: AFLFast [6] and AFLGo [5] used 2.26; FairFuzz [32]
used 2.28; Angora [10] used 2.29.
The use of Google Fuzzer Suite would seem to address both is-
sues: it comprises 25 programs with known bugs, and is defined
independently of any given fuzzer. On the other hand, it was de-
signed as a kind of regression suite, not necessarily representative
of fuzzing “in the wild;” the provided harnesses and seeds mostly in-
tend that fuzzers should find the targeted bugs within a few seconds
to a few minutes.
8.2 Suites of artificial programs (or bugs)
Real programs are fickle in that the likelihood that bugs are present
depends on many factors. For example, programs under active
development may well have more bugs than those that are relatively
stable (just responding to bug reports). In a sense, we do not care
about any particular set of programs, but rather a representative set
of programming (anti)patterns in which bugs are likely to crop up.
Such patterns could be injected artificially. There are two popular
suites that do this: CGC, and LAVA-M.
The CGC suite comprises 296 buggy programs produced as part
of DARPA’s Cyber Grand Challenge [14]. This suite was specifi-
cally designed to evaluate bug finding tools like fuzz testers—the
suite’s programs perform realistic functions and are seeded with
exploitable bugs. LAVA (which stands for Large-scale Automated
Vulnerability Addition) is a tool for injecting bugs into known pro-
grams [16]. The tool is designed to add crashing, input-determinate
bugs along feasible paths. The LAVA authors used the tool to create
the LAVA-M suite, which comprises four bug-injected coreutils pro-
grams: base64, md5sum, uniq, and who. Unlike the CGC programs,
which have very few injected bugs, the LAVA-M programs have
many: on the order of a few dozen each for the first three, and more
than 2000 for who. For both suites, if a fuzzer triggers a bug, there
is a telltale sign indicating which one it is, which is very useful for
understanding how many bugs are found from the total possible.
CGC and LAVA-M have gained popularity as the benchmark
choices for evaluating fuzzers since their introduction. Within the
past two years, CGC and LAVA-M have been used for evaluating 4
and 5 fuzzers, respectively. VUzzer [44], Steelix [33], and T-Fuzz [39]
used both benchmarks in their evaluation. However, sometimes the
CGC benchmark was subset: Driller [50], VUzzer [44], and Steelix
[33] were evaluated on 126, 63, and 17 out of the 296 programs,
respectively.
While CGC programs are hand-designed to simulate reality, this
simulation may be imperfect: Performing well on the CGC pro-
grams may fail to generalize to actual programs. For example, the
average size of the CGC cqe-challenge programs was (only) 1774
lines of code, and many programs use telnet-style, text-based proto-
cols. Likewise, LAVA-M injected bugs may not sufficiently resemble
those found “in the wild.” The incentives and circumstances behind
real-world software development may fail to translate to synthetic
benchmarks which were specifically designed to be insecure. The
LAVA authors write that, “A significant chunk of future work for
LAVA involves making the generated corpora look more like the
bugs that are found in real programs.” Indeed, in recent experi-
ments [15], they also have shown that relatively simple techniques
can effectively find all of the LAVA-M bugs, which follow a simple
pattern. We are aware of no study that independently assesses the