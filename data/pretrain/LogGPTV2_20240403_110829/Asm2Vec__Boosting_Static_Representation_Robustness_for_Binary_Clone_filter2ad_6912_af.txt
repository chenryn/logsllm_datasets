(cid:2)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
(cid:2)
p
Ø
.777
.368
.313
.687
.280
.804
.458
.760
.799
.533
.898
.926
Ø
Ø
.004
.182
.014
.012
.019
.006
.030
.003
.005
.011
.758
.776
Ø
.783
.365
.212
.586
.199
.789
.350
.748
.785
.726
.919
.931
Ø
.795
.007
.270
.009
.034
.018
.007
.023
.005
.007
.220
.759
.792
TABLE 1: Clone search between different compiler optimization options using the Precision at Position 1 (Precision@1)
at Position 1. Asm2Vec is our proposed method. †denotes cited performance. (cid:2) and (cid:3) respectively indicate p > 0.05 and
metric. It captures the ratio of assembly functions that are correctly matched at position 1. In this case, it equals Recall
p ≤ 0.01 for Wilcoxon signed-rank test between Asm2Vec and each baseline.
bytes of a function are modiﬁed and none of the functions
are identical. 40% of a control ﬂow graph is modiﬁed and
65% function pairs share similar graph complexity. It can
be considered as the best situation where the optimization
strategies used in two binaries are similar. The compari-
son between O0 and O3 is the most difﬁcult one. It can
be considered as the worst situation where there exists a
large difference in the optimization strategies (Figure 6).
On average, 34% bytes of a function are modiﬁed and
none of the functions are identical. 82% of a control ﬂow
graph is modiﬁed and 17% function pairs share similar
graph complexity. Table 1 presents the results in these two
situations. Due to the large number of cases, we only list
the results for these two cases to demonstrate the best and
worse situations. The results of other cases lie between these
two and follow the same ranking.
machine learning may risk having invalid experiment results.
For example, splitting coreutils binaries into training set
and testing set may lead to an invalid good result since
these binaries share a very similar code base. This issue
is not applicable to our experiment. First, we follow the
unsupervised learning paradigm, where the true clone map-
ping is only used for evaluation. Second, our training data
is very different to the testing data, as shown in Figure 6
and Figure 7. For example, the coreutils library comes with
many binaries but we statically linked them into a single
binary. We train the O0-optimized binary and match the
O3-optimized binary. These two binaries are very different.
We use the Precision at Position 1 (Precision@1) met-
ric. For every query, if a baseline returns no answer, we
count the precision as zero. Therefore, Precision@1 captures
the ratio of assembly functions that are correctly matched,
which is equal
to Recall at Position 1. We benchmark
Andriesse et al. [22] point out that using supervised
(cid:21)(cid:24)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
nine feature representations proposed in [8]: mnemonic n-
grams (denoted as n-gram), mnemonic n-perms (denoted
as n-perm), Graphlets (denoted as Graphlet), Extended
Graphlets (denoted as Graphlet-E), Colored Graphlets (de-
noted as Graphlet-C), Mixed Graphlets (denoted as Mix-
Graph), Mixed n-grams/perms (denoted as MixGram), Con-
stants, and the Composite of n-grams/perms and Graphlets
(denoted as Composite). The idea of using Graphlet orig-
inated from [23]. These baseline methods cover a wide
range of popular features from token to graph substructure.
These baselines are conﬁgured according to the reported
best settings in the paper. We also include the original PV-
DM model and PV-DBOW model as a baseline where each
assembly function is treated as a document. We pick the best
results and denote it as PV-(DM/DBOW). We only tune the
conﬁgurations for PV-(DM/DBOW) as well as Asm2Vec on
the zlib dataset. FuncSimSearch is an open source assembly
clone static search toolkit recently released by Google3. It
has a default training dataset that contains a ground-truth
mapping of equivalent assembly functions. The state-of-
the-art dynamic approach BinGo [12] and CACompare [13]
are unavailable for evaluation. However, we conduct the
experiment in the same way using the same metric. Their
reported results are included in Table 1. We also include the
Wilcoxon signed-rank test across different binaries to see if
the difference in performance is statistically signiﬁcant.
As shown in Table 1, Asm2Vec signiﬁcantly outperforms
static features in both the best and worse situation. It also
outperforms BinGo, a recent semantic clone search approach
that involves dynamic features. It shows that Asm2Vec is
robust against heavy syntax modiﬁcations and intensive
inlining introduced by the compiler. Even in the worse
case, the learned representation can still correctly match
more than 75% of assembly functions at position 1. It even
achieves competitive performance against the state-of-the-
art dynamic approach CACompare for semantic clone. The
difference is not statistically different, due to the small sam-
ple size. Asm2Vec performs stably across different libraries
and is able to ﬁnd clones with high precision. On average,
it achieves more than 93% precision in detecting clones
among compiler optimization options O1, O2, and O3. As
the difference between two optimization levels increases,
the performance of the Asm2Vec decreases. Nevertheless, it
is much less sensitive than the other static features, which
demonstrates its robustness.
Discovre and Genius are two recent static approaches
that use descriptive statistics and graph matching. Both of
them are not available for evaluation. CACompare has been
shown to outperform Discovre [7], Genius [6] and Blan-
ket [10]. Our approach achieves comparable performance to
CACompare, which indirectly compares Asm2Vec’s perfor-
mance to Discovre and Genius.
In the best situation where we compare between op-
timization level O2 and O3, the baseline static features’
performance is inline with the result reported in the original
paper, which shows the correctness of our implementation.
3. Available at https://github.com/google/functionsimsearch
(cid:21)(cid:25)(cid:17)
In the worse case, we notice that the Constant model out-
performs the other static features based on assembly instruc-
tions and graph structures. The reason is that constant tokens
do not suffer from changes in assembly instructions and
subgraph structures. We also notice that BinGo, in the worse
case, outperforms static features. However, in the best case,
its performance is not as good as static features, such as
Graphlet-C and n-grams, because the noise at the symbolic
logic level is higher than at the assembly code level. Logical
expressions promote recall and can ﬁnd clones when the
syntax is very different. However, assembly instructions can
provide more precise information for matching.
The largest binary, OpenSSL, has more than 5,000 func-
tions. Asm2Vec takes on average 153 ms to train an assembly
function and 20 ms to process a query. For OpenSSL,
CACompare takes on average 12 seconds to fulﬁll a query.
5.2. Searching with Code Obfuscation
Obfuscator-LLVM (O-LLVM) [24] is built upon the
LLVM framework and the CLANG compiler toolchain. It
operates at the intermediate language level and modiﬁes
a program’s logics before the binary ﬁle is generated. It
increases the complexity of the binary code. O-LLVM uses
three different
techniques and their combination: Bogus
Control Flow Graph (BCF), Control Flow Flattening (FLA),
and Instruction Substitution (SUB). Figure 7 shows the
statistics on differences.
• BCF modiﬁes the control ﬂow graph by adding a large
number of irrelevant random basic blocks and branches.
It will also split, merge, and reorder the original basic
blocks. BCF breaks CFG and basic block integrity (on
average 149% vertices/edges are added).
• FLA reorganizes the original CFG using complex hier-
archy of new conditions as switches (see an example in
Figure 1). The original instructions are heavily modiﬁed
to accommodate the new entering conditions and vari-
ables. The linear layout has been completely modiﬁed
(on average 376% vertices and edges are added). Graph-
based features are oblivious to this technique. It is also
unscalable for a dynamic approach to fully cover the CFG.
• SUB substitutes fragments of assembly code to its equiva-
lent form by going one pass over the function logic using
predeﬁned rules. This technique modiﬁes the contents of
basic blocks and adds new constants. For example, addi-
tions are transformed to a = b − (−c). Subtractions are
transformed to r = rand(); a = b−r; a = a−c; a = a+r.
And operations are transformed to a = (b∧ (cid:3) c)&b. SUB
does not change much of the graph structure (91% of
functions keep the same number of vertex and edge).
• BCF+FLA+SUB uses all the obfuscation options above.
O-LLVM heavily modiﬁes the original assembly code. It
breaks the CFG and the basic blocks integrity. By design,
most of the static features are oblivious to the obfuscation.
By using the CLANG compiler with O-LLVM, we success-
fully compile four libraries used in the last experiment and
evaluate Asm2Vec using them. There were compilation er-
rors when compiling the other binaries with the CLANG+O-
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
Figure 7: The difference between the original function and the obfuscated function. a) Relative string editing distance. 0.122
indicates that around 12.2% percent of bytes are modiﬁed. b) Relative absolute difference in the count of vertices and edge.
1.49% indicates the obfuscated function has 149% more vertices and edges in CFG.
O-LLVM - Bogus Control Flow Graph (BCF)
OpenSSL
ImageMagick
LibTomCrypt
O-LLVM - Instruction Substitution (SUB)
ImageMagick
LibTomCrypt
OpenSSL
Baselines
Composite
Constant
Graphlet
Graphlet-C
Graphlet-E
MixedGram
MixedGraph
n-gram
n-perm
FuncSimSearch
PV(DM/DBOW)
Asm2Vec *
Baselines
Composite
Constant
Graphlet
Graphlet-C
Graphlet-E
MixedGram
MixedGraph
n-gram
n-perm
FuncSimSearch
PV(DM/DBOW)
Asm2Vec *
Libgmp
.226
.130
.003
.112
.026
.220
.011
.134
.233
.109
.784
.802
Libgmp
.138
.105
.000
.003
.001
.148
.003
.095
.133
.095
.852
.772
.224
.592
.005
.118
.011
.234
.007
.134
.224
.022
.870
.920
.246
.318
.007
.124
.014
.303
.014
.195
.274
.027
.768
.883
O-LLVM - Control Flow Flattening (FLA)
ImageMagick
.312
.412
.033
.165
.050
.375
.049
.295
.374
.029
.842
.933
LibTomCrypt
OpenSSL
.129
.480
.002
.008
.002
.143
.003
.093
.126
.001
.938
.920
.052
.215
.000
.000
.000
.075
.000
.059
.055
.004
.786
.890
.027
.209
.000
.001
.000
.036
.000
.030
.033
.008
.763
.795
Avg.
.252
.363
.012
.130
.025
.283
.020
.190
.276
.047
.816
.885
Avg.
.086
.252
.000
.003
.001
.101
.002
.069
.087
.027
.835
.844
Libgmp
.620
.173
.198
.626
.454
.585
.356
.466
.557
.685
.935
.940
Libgmp
.219
.137
.000
.107
.020
.221
.006
.154
.224
.110
.780
.854
.675
.622
.158
.572
.216
.642
.325
.516
.624
.442
.968
.960
.226
.591
.005
.124
.012
.234
.010
.144
.222
.025
.873
.880
O-LLVM - SUB+FLA+BCF
ImageMagick
LibTomCrypt
.600
.492
.411
.539
.286
.563
.495
.513
.558
.699
.964
.981
.015
.173
.000
.000
.000
.018
.000
.013
.018
.003
.639
.830
.766
.360
.308
.585
.271
.743
.488
.670
.736
.330
.958
.961
OpenSSL
.009
.159
.000
.000
.000
.010
.000
.007
.008
.008
.595
.690
Avg.
.665
.412
.269
.581