title:Do I know you?: efficient and privacy-preserving common friend-finder
protocols and applications
author:Marcin Nagy and
Emiliano De Cristofaro and
Alexandra Dmitrienko and
N. Asokan and
Ahmad-Reza Sadeghi
Do I Know You? – Efﬁcient and Privacy-Preserving
Common Friend-Finder Protocols and Applications
Marcin Nagy
Aalto University, Finland
marcin.nagy@aalto.ﬁ
Alexandra Dmitrienko
Fraunhofer SIT/CASED, Germany
PI:EMAIL
Emiliano De Cristofaro
PARC (a Xerox Company), U.S.A.
PI:EMAIL
N. Asokan
University of Helsinki & Aalto University, Finland
PI:EMAIL
Ahmad-Reza Sadeghi
TU Darmstadt/CASED, Germany
PI:EMAIL
ABSTRACT
1.
INTRODUCTION
The increasing penetration of Online Social Networks (OSNs)
prompts the need for effectively accessing and utilizing social net-
working information. In numerous applications, users need to make
trust and/or access control decisions involving other (possibly stran-
ger) users, and one important factor is often the existence of com-
mon social relationships. This motivates the need for secure and
privacy-preserving techniques allowing users to assess whether or
not they have mutual friends.
This paper introduces the Common Friends service, a frame-
work for ﬁnding common friends which protects privacy of non-
mutual friends and guarantees authenticity of friendships. First,
we present a generic construction that reduces to secure computa-
tion of set intersection, while ensuring authenticity of announced
friends via bearer capabilities. Then, we propose an efﬁcient in-
stantiation, based on Bloom ﬁlters, that only incurs a constant num-
ber of public-key operations and appreciably low communication
overhead. Our software is designed so that developers can eas-
ily integrate Common Friends into their applications, e.g., to en-
force access control based on users’ social proximity in a privacy-
preserving manner. Finally, we showcase our techniques in the con-
text of an existing application for sharing (tethered) Internet access,
whereby users decide to share access depending on the existence of
common friends. A comprehensive experimental evaluation attests
to the practicality of proposed techniques.
Categories and Subject Descriptors
K.4.1 [Computer and Society]: Public policy issues—Privacy;
C.2.0 [Computer-Communication Networks]: General—Secur-
ity and protection
Keywords
Privacy enhancing technologies, social networks, access control
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’13 Dec. 9-13, 2013, New Orleans, Louisiana USA
Copyright 2013 ACM 978-1-4503-2015-3/13/12 ...$15.00
http://dx.doi.org/10.1145/2523649.2523668.
159
Online Social Networks (OSNs) play a key role in today’s com-
puting ecosystem, as social interactions/connections are increas-
ingly used to enhance trust in, and usability of, a growing number
of applications. Popular OSNs, such as Facebook, have become
de-facto providers of online identities and are often used to en-
force veriﬁcation of personas and information. Numerous applic-
ations leverage technologies like OAuth [24] and OpenID [47] to
authenticate users while relying on third-party services offered by
OSN providers. Others connect to social network proﬁles and rely
on data harvested from them, e.g., to verify self-reported informa-
tion [45] or detect Sybil nodes [13].
In many realistic scenarios, users need to make access control
decisions involving other (possibly stranger) users, e.g., for shar-
ing rides [2] and cabs [1], to construct distributed computing plat-
forms [42] and online dating services [3], or to base routing de-
cisions for anonymous communications [35, 41]. One important
trust-enhancing factor, potentially guiding such decisions, is the ex-
istence of previously established social relationships. For instance,
an intuitive access control policy may be to only carpool with one’s
friends or friends-of-friends, or to base routing decisions on social
proximity. However, the process of discovering common friends
may harm the privacy of the two parties and that of their friends.
At least one party needs to disclose the identity of his friends and,
depending on the application scenario, this could reveal the iden-
tity of the user, and possibly even information about his lifestyle
and social attitudes.
Motivated by the above issues, this paper presents the design
and the implementation of a framework supporting secure discov-
ery of common friends, which we denote as Common Friends. It
allows two devices to assess whether their owners are friends or
have mutual friends in a given social network, without reciprocally
revealing any information about non-common friends.
We ﬁrst introduce a generic construction that reduces the prob-
lem to secure computation of set intersection [23] and, at the same
time, ensures authenticity of claimed friends using bearer capabil-
ities [48]. We then propose a very efﬁcient instantiation, based on
Bloom ﬁlters [10], that only incurs a constant number of public-
key operations (independent from the size of friend lists). Our pro-
posed framework provides a clear and usable interface for applica-
tion developers, enabling them to support access control decisions
based on users’ social proximity, independently of underlying cryp-
tographic techniques. Finally, we integrate the Common Friends
service into an existing application for sharing Internet connec-
tion [5], whereby users decide whether or not to share based on
the existence of common friends. A comprehensive experimental
evaluation attests to the practicality of proposed techniques.
1.1 Securely Finding Common Friends
As our main building block, we turn to Private Set Intersection
(PSI) [23, 37, 17, 33, 29, 18], a cryptographic primitive allowing
two parties, each inputting its own private set, to interact so that
they only obtain, at most, the set intersection. If one considers the
lists of users’ friends as (unordered) sets, then PSI could be used
to let users only learn the friends they share, by obtaining the set
intersection. Alternatively, if only the number of shared friends
is needed, one could use the Private Set Intersection Cardinality
(PSI-CA) variant [23, 4, 27, 14], which only outputs the magnitude
of the set intersection. Unfortunately, however, with PSI/PSI-CA,
users could include identities of arbitrary friends in their input list
(i.e., claim non-existent friendships). The Authorized PSI (APSI)
variant [17, 15, 11], which extends PSI by ensuring that inputs are
authorized by an appropriate authority, would not work either as it
assumes that only one party’s set is certiﬁed and the certiﬁcation is
performed by a single authority.
The work in [16] addresses the problem of claiming non-existent
friendships by requiring users to provide a proof of prior relation-
ship, via cryptographic credentials. Common friends are (privately)
discovered following a relatively expensive technique resembling
Secret Handshakes [6, 40], where validity of certiﬁcates is veri-
ﬁed obliviously to guarantee privacy while enforcing authenticity.
Whereas, our approach is to use bearer capabilities [48] (aka sparse
capabilities or bearer tokens): each user distributes a time-limited,
randomly generated capability to his friends via a secure (i.e., au-
thentic and conﬁdential) channel. Possession of the capability rep-
resents a proof of an existing friendship, thus, users can input it into
a cryptographic protocol, such as PSI, which reciprocally discloses
only their common (authentic) friends. Using this approach, input
sets to the PSI protocol are actually high-entropy objects, generated
from a large space that is impractical to enumerate. Consequently,
we do not need the full security of standard PSI techniques [23,
37, 17, 33] designed to work with potentially “predictable” items,
such as names or identiﬁers. As we discuss later in Sec. 2.4, the
unpredictability of capabilities allows us to instantiate PSI using a
novel construction based on Bloom ﬁlters [10], with appreciably
lower communication overhead and reduced number of modular
exponentiations (constant vs. linear in the number of friends).
1.2 Contributions
In this paper, we present the design and the implementation of
Common Friends, a framework that enables two devices to assess
whether their owners are friends or share common friends in a given
social network. Common Friends combines PSI with bearer capab-
ilities [48] to ensure (1) privacy, i.e., users only learn information
about their common friends, and (2) authenticity, i.e., one cannot
falsely claim non-existent friendships.
In summary, this paper makes several contributions:
• The insight that when input sets include high-entropy items
one can design more efﬁcient PSI schemes than traditional
PSI designed for low-entropy elements and the concrete design
of such a PSI scheme based on Bloom Filters (Sec. 2.4);
• A detailed description of the design and implementation of
a framework encapsulating the secure use of PSI protocols
(independently from the actual implementation/variant) and
bearer capabilities in the Common Friends scenario (Sec. 3).
Our implementations provide a clear interface for developers
to easily integrate Common Friends into their applications
and use social proximity to guide trust and access control
decisions. As a proof-of-concept, we successfully integrate it
with a tethering application for sharing connectivity (Sec. 4).
• A performance evaluation that attests to the practicality of
our solutions (Sec. 5).
2. THE COMMON FRIENDS SERVICE
In this section, we describe the Common Friends service. We
ﬁrst introduce the desired security properties and then present our
generic system design that reduces to the problem of private set
intersection, followed by an efﬁcient instantiation based on Bloom
ﬁlters. Finally, we discuss the security of our proposals.
2.1 Security Goals and Attacker Model
We now deﬁne the secure common friend discovery functional-
ity, along with relevant corresponding security goals.
Attacker Model. Before presenting security deﬁnitions, we in-
troduce the attacker model. We consider honest-but-curious (aka
semi-honest) adversaries, i.e., participants are assumed to follow
protocol speciﬁcations but nonetheless attempt to infer more in-
formation, during or after protocol execution. In particular, we as-
sume that legitimate participants will not disclose, or share, secret
information.
Common Friends. The Common Friends service relies on a two-
party protocol involving “initiator” I and “responder” R, on input
the list of their friends f (IDI ) and f (IDR), respectively. (IDI
and IDR denote, respectively, the identity of I and R in a given
social network). Speciﬁcally, we rely on three protocol variants
securely realizing three functionality variants, presented in Table 1,
and satisfying privacy and authenticity deﬁnitions discussed below.
Protocol
Variant
Basic
Cardinality-only
Mutual Output
R’s output
f (IDI ) ∩ f (IDR)
|f (IDI ) ∩ f (IDR)|
f (IDI ) ∩ f (IDR)
I’s output
⊥
⊥
f (IDI ) ∩ f (IDR)
Table 1: Secure Common Friend Discovery Variants.
Initiator’s Privacy. I’s privacy is guaranteed if, on each possible
pair of inputs (f (IDI ), f (IDR)), R’s view can be efﬁciently sim-
ulated on input: f (IDR) and either f (IDR)∩ f (IDI ) in the basic
variant, or |f (IDR) ∩ f (IDI )| in the cardinality-only variant.
More precisely, let ViewR(f (IDI ), f (IDR)) be a random vari-
able representing the view of the responder R during a protocol in-
teraction with inputs f (IDI ), f (IDR). Then, there exists a Prob-
∗
abilistic Polynomial Time (PPT) algorithm R
such that, in the ba-
sic variant:
{R
∗
(f (IDR), f (IDR) ∩ f (IDI ))}
{ViewR(f (IDR), f (IDI ))}
(f (IDR),f (IDI ))
(f (IDR),f (IDI ))
c≡
c≡
or, in the cardinality-only variant:
{R
∗
(|f (IDR), f (IDR) ∩ f (IDI )|)}
{ViewR(f (IDR), f (IDI ))}
(f (IDR),f (IDI ))
(f (IDR),f (IDI ))
Responder’s Privacy (Basic and Cardinality-Only Variants). If
the functionality yields no output to the initiator, then responder’s
160
Description Notation
Entities
Server S
I
Responder User R
Generic User (can be either I or R) U
Initiator User
Keys
Data
DH public key of U, I, R, resp. P KU , P KI, P KR
DH private key of U, I, R SKU , SKI, SKR
DH session key between I and R KIR
U’s identiﬁer in the social network
Set of U’s friends in the social network
Certiﬁcate of server S CertS
IDU
f (IDU )
Capability uploaded by user U cU
U’s friends and their capabilities RU ={(IDj, cj ) |
IDj ∈ f (IDU )}
downloaded from S
I’s input set to PSI RI ={(cj||P KI||P KR) |
(IDj, cj) ∈ RI )}
R’s input sets to PSI RR={(ck||P KI||P KR) |
(IDk, ck) ∈ RR)}
Table 2: Notation.
∗
privacy is guaranteed if no information is disclosed about its input,
not even the number or the identity of the common friends.
∗
That is, for every PPT adversary I
playing initiator’s role, every
initiator input set f (IDI ), and any responder inputs (f (IDR)(0),
f (IDR)(1)) (of equal size), the views of I
if responder inputs
f (IDR)(0) and if responder inputs f (IDR)(1) are computation-
ally indistinguishable.
Responder’s Privacy (Mutual Output Variant). Clearly, when
the functionality yields, as output, the identity of common friends
to both parties, responder’s privacy is deﬁned like initiator’s pri-
vacy, i.e., I’s view should be efﬁciently simulated with only its in-
puts and outputs. Speciﬁcally, let ViewI (f (IDI ), f (IDR)) be a
random variable representing the view of the initiator I during a
protocol interaction with inputs f (IDI ), f (IDR). Then, there ex-
∗
ists a PPT algorithm I
such that:
∗
{I
(f (IDI ), f (IDI ) ∩ f (IDR))}
{ViewI (f (IDI ), f (IDR))}
c≡
(f (IDI ),f (IDR))
(f (IDI ),f (IDR))
Authenticity (Informal Deﬁnition). A user should not be able to
falsely claim to have a common friend with the other party if there
is no such common friend. Obviously, it follows that if the latter
controls access to a resource on the basis of the existence of com-
mon friends, then, the former cannot succeed in getting access to
this resource by claiming non-existent friendships and/or inﬂating
the number of common friends.
2.2 System Description
Table 2 summarizes the notation used throughout this paper. The