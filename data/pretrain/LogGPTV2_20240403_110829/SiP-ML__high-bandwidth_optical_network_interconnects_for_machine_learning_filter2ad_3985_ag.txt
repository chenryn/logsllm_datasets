transceivers-in-darpa-pipes-project-2-tbps-now-100-tbps-is-the-goal/.
[61] Pipes researchers demonstrate optical interconnects to improve performance of
digital microelectronics, Mar. 2020. https://www.darpa.mil/news-events/2020-
03-25.
[62] Tiffany Trader. Ayar Labs to Demo Photonics Chiplet in FPGA Package at Hot
Chips, Aug. 2019. https://www.hpcwire.com/2019/08/19/ayar-labs-to-demo-
photonics-chiplet-in-fpga-package-at-hot-chips/.
[63] F. Douglis, S. Robertson, E. Van den Berg, J. Micallef, M. Pucci, A. Aiken, M. Hat-
tink, M. Seok, and K. Bergman. Fleet—fast lanes for expedited execution at 10
terabits: Program overview. IEEE Internet Computing, (01):1–1, apr 5555.
[64] Ayar Labs TeraPHY Silicon Chip. https://ayarlabs.com/products/.
[65] Demonstration of Ayar Labs’ Optical I/O Multi-Chip Package and Single-Die
Package solutions, Aug. 2020. https://vimeo.com/449164007.
[66] William M. Mellette, Rob McGuinness, Arjun Roy, Alex Forencich, George Papen,
Alex C. Snoeren, and George Porter. Rotornet: A scalable, low-complexity,
optical datacenter network. SIGCOMM ’17, pages 267–280, 2017.
[67] Tae Joon Seok, Niels Quack, Sangyoon Han, Richard S. Muller, and Ming C. Wu.
Large-scale broadband digital silicon photonic switches with vertical adiabatic
couplers. Optica, 3(1):64–70, Jan 2016.
[68] Kyungmok Kwon, Tae Joon Seok, Johannes Henriksson, Jianheng Luo, Lane
Ochikubo, John Jacobs, Richard S Muller, and Ming C Wu. 128× 128 silicon
photonic mems switch with scalable row/column addressing. In CLEO: Science
and Innovations, pages SF1A–4. Optical Society of America, 2018.
[69] William M. Mellette, Rajdeep Das, Yibo Guo, Rob McGuinness, Alex C. Snoeren,
and George Porter. Expanding across time to deliver bandwidth efficiency and
low latency. NSDI’20, 2020.
[70] Yunpeng James Liu, Peter Xiang Gao, Bernard Wong, and Srinivasan Keshav.
Quartz: A new design element for low-latency dcns. SIGCOMM’14, pages 283–
294.
[76] John Kim, Wiliam J. Dally, Steve Scott, and Dennis Abts.
[71] George Porter, Richard Strong, Nathan Farrington, Alex Forencich, Pang Chen-
Sun, Tajana Rosing, Yeshaiahu Fainman, George Papen, and Amin Vahdat.
Integrating microsecond circuit switching into the data center. SIGCOMM’13,
pages 447–458.
[72] meg walraed sullivan, Jitu Padhye, and Dave Maltz. Theia: Simple and cheap
networking for ultra-dense data centers. In HotNets-XIII Proceedings of the 13th
ACM Workshop on Hot Topics in Networks. ACM, October 2014.
[73] Paolo Costa, Austin Donnelly, Greg O’Shea, and Antony Rowstron. Camcubeos:
A key-based network stack for 3d torus cluster topologies. In Proceedings of
the 22nd International Symposium on High-Performance Parallel and Distributed
Computing, HPDC ’13, pages 73–84, New York, NY, USA, 2013. Association for
Computing Machinery.
[74] Hussam Abu-Libdeh, Paolo Costa, Antony Rowstron, Greg O’Shea, and Austin
Donnelly. Symbiotic routing in future data centers. In Proceedings of the ACM
SIGCOMM 2010 Conference, SIGCOMM ’10, page 51?62, New York, NY, USA,
2010. Association for Computing Machinery.
[75] J. M. Kumar and L. M. Patnaik. Extended hypercube: a hierarchical intercon-
nection network of hypercubes. IEEE Transactions on Parallel and Distributed
Systems, 3(1):45–57, 1992.
Technology-
driven, highly-scalable dragonfly topology. SIGARCH Comput. Archit. News,
36(3):77âĂŞ88, June 2008.
[77] Min Yee Teh, Jeremiah J. Wilke, Keren Bergman, and Sébastien Rumley. Design
space exploration of the dragonfly topology. In Julian M. Kunkel, Rio Yokota,
Michela Taufer, and John Shalf, editors, High Performance Computing, pages
57–74, Cham, 2017. Springer International Publishing.
[78] J. Kim, W. J. Dally, S. Scott, and D. Abts. Technology-driven, highly-scalable
dragonfly topology. In 2008 International Symposium on Computer Architecture,
pages 77–88, 2008.
[79] Calient Optical Circuit Switch. https://www.calient.net/products/edge640-
[80] Mohammad Al-Fares, Alexander Loukissas, and Amin Vahdat. A scalable,
commodity data center network architecture. SIGCOMM Comput. Commun.
Rev., 38(4):63–74, August 2008.
[81] Chuanxiong Guo, Guohan Lu, Dan Li, Haitao Wu, Xuan Zhang, Yunfeng Shi,
Chen Tian, Yongguang Zhang, and Songwu Lu. Bcube: A high performance,
server-centric network architecture for modular data centers. In Proceedings of
the ACM SIGCOMM 2009 Conference on Data Communication, SIGCOMM ’09,
page 63?74, New York, NY, USA, 2009. Association for Computing Machinery.
[82] M. Besta and T. Hoefler. Slim fly: A cost effective low-diameter network topol-
ogy. In SC ’14: Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, pages 348–359, Nov 2014.
[83] Alexander Ishii, Denis Foley, Eric Anderson, Bill Dally, Glenn Dearth Larry
Dennison, Mark Hummel, and John Schafer. NVIDIA’s NVLink-Switching Chip
and Scale-Up GPU-Compute Server. HotChips, 2018. https://www.hotchips.org/
hc30/2conf/2.01NvidiaNVswitchHotChips2018DGX2NVSFinal.pdf.
[84] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 770–778, June 2016.
[85] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving
optical-circuit-switch/.
Language Understanding by Generative Pre-Training.
[86] Christopher J. Shallue, Jaehoon Lee, Joseph M. Antognini, Jascha Sohl-Dickstein,
Roy Frostig, and George E. Dahl. Measuring the effects of data parallelism on
neural network training. CoRR, abs/1811.03600, 2018.
github.com/NVIDIA/Megatron-LM.
[87] Raul Puri. Megatron: a large, powerful transformer, Aug. 2019. https://
[88] MLPerf: A broad ML benchmark suite. https://mlperf .org/.
[89] FlexFlow Github. https://github.com/flexflow/FlexFlow.git.
[90] Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda. Extremely large minibatch sgd:
Training resnet-50 on imagenet in 15 minutes. arXiv preprint arXiv:1711.04325,
2017.
[91] Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer. Ima-
genet training in minutes. In Proceedings of the 47th International Conference on
Parallel Processing, pages 1–10, 2018.
[92] Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou,
Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et al. Highly scalable
deep learning training system with mixed-precision: Training imagenet in four
minutes. arXiv preprint arXiv:1807.11205, 2018.
[93] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. Data
Center TCP (DCTCP). In Proceedings of the ACM SIGCOMM 2010 Conference,
SIGCOMM ’10, pages 63–74, New York, NY, USA, 2010. ACM.
[94] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang,
Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, et al. Hpcc: high
precision congestion control. In Proceedings of the ACM Special Interest Group
on Data Communication, pages 44–58. 2019.
670
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
[95] Roy Meade, Shahab Ardalan, Michael Davenport, John Fini, Chen Sun, Mark
Wade, Alexandra Wright-Gladstein, and Chong Zhang. Teraphy: A high-density
electronic-photonic chiplet for optical i/o from a multi-chip module. In Optical
Fiber Communication Conference (OFC) 2019, page M4D.7. Optical Society of
America, 2019.
[96] Alvaro moscoso martir, Juliana MÃĳller, Johannes Hauck, Nicolas Chimot, Rony
Setter, Avner Badihi, Daniel Rasmussen, Alexandre Garreau, Mads Nielsen,
Elmira Islamova, Sebastian Romero-GarcÃŋa, Bin Shen, Anna Sandomirsky,
Sylvie Rockman, Chao Li, Saeed Sharif Azadeh, Guo-Qiang Lo, Elad Mentovich,
Florian Merget, and Jeremy Witzens. Silicon photonics wdm transceiver with
soa and semiconductor mode-locked laser. Scientific Reports, 7, 05 2016.
[97] 2020 General Europractice Pricelist, Jan. 2020. https://europractice-ic.com/wp-
content/uploads/2020/01/General-MPW-EUROPRACTICE-200123-v3.pdf.
[98] D. Kim, K. Y. Au, H. Y. L. X. Luo, Y. L. Ye, S. Bhattacharya, and G. Q. Lo. 2.5d silicon
optical interposer for 400 gbps electronic-photonic integrated circuit platform
packaging. In 2017 IEEE 19th Electronics Packaging Technology Conference (EPTC),
pages 1–4, Dec 2017.
[99] Chen Sun, Mark T. Wade, Yunsup Lee, Jason S. Orcutt, Luca Alloatti, Michael S.
Georgas, Andrew S. Waterman, Jeffrey M. Shainline, Rimas R. Avizienis, Sen Lin,
Benjamin R. Moss, Rajesh Kumar, Fabio Pavanello, Amir H. Atabaki, Henry M.
Cook, Albert J. Ou, Jonathan C. Leu, Yu-Hsin Chen, Krste Asanović, Rajeev J.
Ram, MilošA. Popović, and Vladimir M. Stojanović. Single-chip microprocessor
that communicates directly using light. Nature, 528(7583):534–538, 2015.
[100] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed
deep networks. In Advances in neural information processing systems, pages
1223–1231, 2012.
[101] Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng An-
In International conference on
drew. Deep learning with cots hpc systems.
machine learning, pages 1337–1345, 2013.
[102] Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
Project adam: Building an efficient and scalable deep learning training system.
In OSDI’14, pages 571–582, 2014.
[103] Juncheng Gu, Mosharaf Chowdhury, Kang G Shin, Yibo Zhu, Myeongjae Jeon,
Junjie Qian, Hongqiang Liu, and Chuanxiong Guo. Tiresias: A {GPU} cluster
manager for distributed deep learning. In NSDI’19, pages 485–500, 2019.
[104] Peng Sun, Wansen Feng, Ruobing Han, Shengen Yan, and Yonggang Wen. Op-
timizing network performance for distributed dnn training on gpu clusters:
Imagenet/alexnet training in 1.5 minutes. arXiv preprint arXiv:1902.06855, 2019.
[105] Adam Lerer, Ledell Wu, Jiajun Shen, Timothée Lacroix, Luca Wehrstedt, Abhijit
Bose, and Alexander Peysakhovich. Pytorch-biggraph: A large-scale graph
embedding system. CoRR, abs/1903.12287, 2019.
[106] Luo Mai, Chuntao Hong, and Paolo Costa. Optimizing network performance in
distributed machine learning. In 7th USENIX Workshop on Hot Topics in Cloud
Computing (HotCloud 15), Santa Clara, CA, 2015. USENIX Association.
[107] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient
compression: Reducing the communication bandwidth for distributed training.
arXiv preprint arXiv:1712.01887, 2017.
[108] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
QSGD: Communication-efficient SGD via randomized quantization and encod-
ing. volume 3, pages 1710 – 1721, 2018.
[109] Hyeontaek Lim, David G Andersen, and Michael Kaminsky. 3lc: Lightweight
and effective traffic compression for distributed machine learning. arXiv preprint
arXiv:1802.07389, 2018.
[110] Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep
learning with sparse and quantized communication. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in
Neural Information Processing Systems 31, pages 2525–2536. Curran Associates,
Inc., 2018.
[111] Alex Krizhevsky. One weird trick for parallelizing convolutional neural net-
works. arXiv preprint arXiv:1404.5997, 2014.
[112] Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic
averaging sgd. In Advances in Neural Information Processing Systems, pages
685–693, 2015.
[113] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization.
In Advances in Neural Information Processing Systems, pages 873–881, 2011.
[114] Feng Niu, Benjamin Recht, Christopher Re, and Stephen J. Wright. Hogwild!: A
lock-free approach to parallelizing stochastic gradient descent. In Proceedings
of the 24th International Conference on Neural Information Processing Systems,
NIPS’11, pages 693–701, 2011.
[115] Pijika Watcharapichat, Victoria Lopez Morales, Raul Castro Fernandez, and Peter
Pietzuch. Ako: Decentralised deep learning with partial gradient exchange.
SoCC ’16, 2016.
[116] Sayed Hadi Hashemi, Sangeetha Abdu Jyothi, and Roy H. Campbell. Communi-
cation scheduling as a first-class citizen in distributed machine learning systems.
CoRR, abs/1803.03288, 2018.
[117] Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, and Kurt Keutzer.
Firecaffe: near-linear acceleration of deep neural network training on compute
[120] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep
clusters. CoRR, abs/1511.00175, 2015.
[118] Eric Chung, Jeremy Fowers, Kalin Ovtcharov, Michael Papamichael, Adrian
Caulfield, Todd Massengil, Ming Liu, Daniel Lo, Shlomi Alkalay, and Michael
Haselman. Accelerating persistent neural networks at datacenter scale. In Hot
Chips, volume 29, 2017.
[119] Anand Jayarajan, Jinliang Wei, Garth Gibson, Alexandra Fedorova, and Gen-
nady Pekhimenko. Priority-based parameter propagation for distributed DNN
training. CoRR, abs/1905.03960, 2019.
learning in tensorflow. CoRR, abs/1802.05799, 2018.
[121] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen,
Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean.
Device placement optimization with reinforcement learning. In Proceedings of
the 34th International Conference on Machine Learning-Volume 70, pages 2430–
2439. JMLR. org, 2017.
[122] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song,
Zenglin Xu, and Tim Kraska. Superneurons: dynamic gpu memory management
for training deep neural networks. In ACM SIGPLAN Notices, volume 53, pages
41–53. ACM, 2018.
[123] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam,
Quoc V. Le, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks
using pipeline parallelism. NeurIPS, 2019.
[124] Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. Eyeriss: An
energy-efficient reconfigurable accelerator for deep convolutional neural net-
works. IEEE Journal of Solid-State Circuits, 52(1):127–138, 2017.
[125] Yichen Shen, Nicholas C Harris, Scott Skirlo, Mihika Prabhu, Tom Baehr-Jones,
Michael Hochberg, Xin Sun, Shijie Zhao, Hugo Larochelle, Dirk Englund, et al.
Deep learning with coherent nanophotonic circuits. Nature Photonics, 11(7):441,
2017.
[126] Mahdi Nazm Bojnordi and Engin Ipek. Memristive boltzmann machine: A
In
hardware accelerator for combinatorial optimization and deep learning.
2016 IEEE International Symposium on High Performance Computer Architecture
(HPCA), pages 1–13. IEEE, 2016.
[127] Chao Wang, Lei Gong, Qi Yu, Xi Li, Yuan Xie, and Xuehai Zhou. Dlau: A scalable
deep learning accelerator unit on fpga. IEEE Transactions on Computer-Aided
Design of Integrated Circuits and Systems, 36(3):513–517, 2017.
[128] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-
datacenter performance analysis of a tensor processing unit. In 2017 ACM/IEEE
44th Annual International Symposium on Computer Architecture (ISCA), pages
1–12. IEEE, 2017.
[129] Stephen W Keckler, William J Dally, Brucek Khailany, Michael Garland, and
David Glasco. Gpus and the future of parallel computing. IEEE Micro, 31(5):7–17,
2011.
[130] Mohammad Al-Fares, Sivasankar Radhakrishnan, Barath Raghavan, Nelson
Huang, and Amin Vahdat. Hedera: Dynamic flow scheduling for data center
networks. In Proceedings of the 7th USENIX Conference on Networked Systems
Design and Implementation, NSDI’10, pages 19–19, Berkeley, CA, USA, 2010.
USENIX Association.
[131] Navid Hamedazimi, Zafar Qazi, Himanshu Gupta, Vyas Sekar, Samir R. Das,
Jon P. Longtin, Himanshu Shah, and Ashish Tanwer. Firefly: A reconfigurable
wireless data center fabric using free-space optics. SIGCOMM’14, pages 319–330.
[132] M. Ghobadi, R. Mahajan, A. Phanishayee, N. Devanur, J. Kulkarni, G. Ranade,
P. Blanche, H. Rastegarfar, M. Glick, and D. Kilper. Projector: Agile reconfig-
urable data center interconnect. SIGCOMM ’16, pages 216–229, 2016.
[133] He Liu, Matthew K. Mukerjee, Conglong Li, Nicolas Feltman, George Papen,
Stefan Savage, Srinivasan Seshan, Geoffrey M. Voelker, David G. Andersen,
Michael Kaminsky, George Porter, and Alex C. Snoeren. Scheduling techniques
for hybrid circuit/packet networks. In CoNEXT, pages 41:1–41:13. ACM, 2015.
[134] Ankit Singla, Atul Singh, and Yan Chen. OSA: An optical switching architecture
for data center networks with unprecedented flexibility. In Presented as part of
the 9th USENIX Symposium on Networked Systems Design and Implementation
(NSDI 12), pages 239–252, San Jose, CA, 2012. USENIX.
[135] Vishal Shrivastav, Asaf Valadarsky, Hitesh Ballani, Paolo Costa, Ki Suh Lee,
Han Wang, Rachit Agarwal, and Hakim Weatherspoon. Shoal: A network
architecture for disaggregated racks. In 16th USENIX Symposium on Networked
Systems Design and Implementation (NSDI’19). USENIX, February 2019.
[136] He Liu, Feng Lu, Alex Forencich, Rishi Kapoor, Malveeka Tewari, Geoffrey M.
Voelker, George Papen, Alex C. Snoeren, and George Porter. Circuit switching
under the radar with REACToR. NSDI’14, pages 1–15.
[137] Ankit Singla, Chi-Yao Hong, Lucian Popa, and P. Brighten Godfrey. Jellyfish:
Networking data centers randomly. In Proceedings of the 9th USENIX Confer-
ence on Networked Systems Design and Implementation, NSDI’12, pages 17–17,
Berkeley, CA, USA, 2012. USENIX Association.
[138] Andromachi Chatzieleftheriou, Sergey Legtchenko, Hugh Williams, and Antony
Rowstron. Larry: Practical network reconfigurability in the data center. In 15th
USENIX Symposium on Networked Systems Design and Implementation (NSDI 18),
pages 141–156, Renton, WA, April 2018. USENIX Association.
671
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
[139] Sergey Legtchenko, Nicholas Chen, Daniel Cletheroe, Antony Rowstron, Hugh
Williams, and Xiaohan Zhao. Xfabric: A reconfigurable in-rack network for rack-
scale computers. In 13th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 16), pages 15–29, Santa Clara, CA, March 2016. USENIX
Association.
[140] Sebastien Rumley, Meisam Bahadori, Robert Polster, Simon D. Hammond,
David M. Calhoun, Ke Wen, Arun Rodrigues, and Keren Bergman. Optical
interconnects for extreme scale computing systems. Parallel Computing, 64:65 –
80, 2017. High-End Computing for Next-Generation Scientific Discovery.
[141] Nicolas Sherwood-Droz, Howard Wang, Long Chen, Benjamin G. Lee, Aleksandr
Biberman, Keren Bergman, and Michal Lipson. Optical 4×4 hitless silicon router
for optical networks-on-chip (noc). Opt. Express, 16(20):15915–15922, Sep 2008.