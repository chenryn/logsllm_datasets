.18
.21
.54
.02
.78
.13
.08
.10
.01
.31
Figure 2: Relative egress traffic volume (rounded) of 20 PoPs.
Figure 3: # of BGP prefixes to constitute 95% of PoP’s traffic.
Figure 4: # routes to prefixes contributing 95% of PoP’s traffic.
serves ≈ 700 to ≈ 13, 000 prefixes, and 16 PoPs send 95% of their
traffic to fewer than 6500 prefixes.
2.2 Interdomain Connectivity
PRs’ BGP connections to other ASes are of different types:
• Transit providers provide routes to all prefixes via a private net-
work interconnect (PNI) with dedicated capacity just for traffic
between Facebook and the provider.
• Peers provide routes to the peer’s prefixes and to prefixes in its
customer cone [18]. Peers vary by connection type:
– Private peers: PR connects to peer via a dedicated PNI.
– Public peers: PR’s BGP session to peer and traffic to peer tra-
verse the shared fabric of an Internet exchange point (IXP).
– Route server peers: PR receives peer’s routes indirectly via a
route server [2] and exchanges traffic across the IXP fabric.
A PoP may maintain multiple BGP peering sessions with the
same AS (e.g., a private peering and a public peering, or at multiple
PRs). Most PoPs connect to 2+ transit providers, with each transit
Table 1: Fraction of peers and of traffic to peers of various types at
example PoPs in EUrope, ASia, and North America. A peer with both
a private and a public connection will count in both. A peer with a
public and a route server connection counts as public (they share an
IXP port). Traffic to public and route server peers is combined.
provider maintaining a BGP session with 2+ of the PoP’s PRs, for
capacity and failure resilience. When possible, all PRs maintain
equal PNI capacity to a given peer, although sometimes some PRs
have different capacity or do not connect to the peer at all.
In general, we configured Facebook’s network to egress a flow
only at the PoP that the flow enters at, rather than routing across
the WAN from servers in one PoP to egress links at a different
PoP. Isolating traffic within a PoP reduces backbone utilization,
simplifies routing decisions, and improves system stability (§8.1.3).
Even with this simplification, Facebook has diverse routing options.
Figure 4 shows the distribution of the number of routes that each
PoP could choose from to reach the prefixes that make up 95% of
its traffic. If a peer provides the same path through a public peering
and a route server, or if multiple PRs receive the same route from
the same peer, we only count it once. Although not derivable from
the graph (which combines destinations with 1-3 routes), all PoPs
except one have at least two routes to every destination, and many
have four or more routes to most prefixes.
We configure PRs to prefer peer routes to transit routes (via
local_pref), with AS path length as a tiebreaker. When paths
remain tied, PRs prefer paths from the following sources in order:
private peers > public peers > route servers.2 We encode peer
type in MEDs (and strip MEDs set by the peer, which normally
express the peer’s preference of peering points but are irrelevant
given that Facebook egresses a flow at the PoP where it ingresses).
The preference of peers over transit recognizes that an AS that
peers with Facebook expects to receive its traffic on that link. In
addition, we have found that peer routes frequently have better
performance and a lower risk of downstream congestion. Short AS
paths may be more direct or give the traffic to the destination AS
sooner [5]. By preferring the dedicated capacity of a private peering
over a connection across a shared IXP fabric, our policy avoids the
possibility of cross-congestion at the egress and respects that the
peer dedicated resources to receiving Facebook traffic.
We configured BGP at PRs and ASWs to use BGP multipath.
When a PR or an ASW has multiple equivalent BGP best paths
for the same destination prefix (as determined by the BGP best
path selection algorithm), it distributes traffic across the equivalent
routes using Equal-cost multi-path routing (ECMP).
Overall, Facebook has thousands of peer ASes. Table 1 shows,
for example PoPs, the fraction of peers that are of each type. Each
PoP shown has hundreds of peers in total, yielding rich connectivity.
The table also shows the fraction of its traffic that each PoP can serve
2We de-prioritize a handful of private peers relative to public peers for policy reasons,
but the effect is minor in the context of this paper.
1234567891011121314151617181920Relative share of global loadPoP ID 0 2000 4000 6000 8000 10000 12000 140001234567891011121314151617181920# of prefixes that constitute95% of PoP's trafficPoP ID 0 0.2 0.4 0.6 0.8 11234567891011121314151617181920Fraction of prefixesPoP ID1-3 routes4-6 routes7-10 routes11-20 routesACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
B. Schlinker et al.
Figure 5: Distribution across PoPs of fraction of prefixes that would
have experienced congestion had Edge Fabric not intervened.
by peer type (assuming all traffic is assigned to its most preferred
route without considering capacity). Although private peers make
up at most a quarter of peers at any PoP, they receive the majority
of traffic at all but PoP-11. It is typical to pursue the dedicated
capacity of private interconnects for high-volume peerings. At all
but PoP-11, 80+% of traffic egresses to private, public, and route
server peers rather than transit, an example of how today’s large
providers “flatten” the Internet [5, 15]. However, the distribution of
peer types varies widely across PoPs by count and by traffic.
3 CHALLENGES OF BGP
As demand increased and Facebook rapidly expanded its PoP in-
frastructure and connectivity, we encountered challenges due to
limitations of BGP, leading us to build Edge Fabric. Any static
interdomain routing policy will likely suffer similar challenges.
Peering capacity is limited, but BGP is not capacity-aware.
Although Facebook builds PoPs, expands capacity, and pursues
private interconnections, a link’s capacity may not suffice to de-
liver all traffic that Facebook would like to send over it. Rapid
growth in demand can quickly make the capacity of an existing in-
terconnection insufficient, and augmenting capacity in some cases
is impossible or can take months. Short-term spikes in demand
(perhaps due to an event or a holiday) or reductions in capacity
due to failures cause volatility in demand and available capacity.
Further, PoPs serve nearby users, and so diurnal patterns can lead
to synchronized peaks in demand, causing very high utilization that
can exceed PNI capacity for short periods. In addition, capacity to
a given peer may be unequally distributed across PRs, but ECMP at
ASWs will be unaware of this imbalance and will evenly distribute
traffic across PRs, which can result in overload at some PRs and
poor utilization of capacity at others (Section 8.1.4 describes why
we do not use weighted-cost multipath). In general, assigning more
traffic to an egress interface than it (or the downstream path) can
handle causes congestion delay and packet loss, and it also increases
server utilization (due to retransmissions) [10].
This paper presents our system to enable capacity-aware egress
decisions on top of BGP. To understand the scale of the problem, we
analyzed a two-day log from January 2017 of each prefix’s per-PoP
egress traffic rate (averaged over a 1 minute window) and compared
the capacity of Facebook’s egress links to the rate of traffic that
BGP would assign to them (based on our configured BGP policy
from §2.2), if Edge Fabric did not intervene to prevent overload.
Figure 6: Distribution across interfaces of ratio of peak load to ca-
pacity (including only interfaces that would have experienced con-
gestion had Edge Fabric not intervened).
Figure 7: Latency on BGP’s preferred path to a prefix vs. on alternate
paths to the prefix (data from four PoPs).
For each PoP, Figure 5 shows the fraction of prefixes that would
experience congestion. Most PoPs are capacity-constrained for
at least one prefix, and a small fraction of PoPs are capacity-
constrained for most prefixes. For any interface overloaded at
least once, Figure 6 shows peak load per interface (computed over
1-minute intervals, relative to interface capacity). While most
interfaces experience low amounts of overload (median load =
1.19X capacity), 10% experience a period in which BGP policy
would lead to BGP assigning twice as much traffic as the interface’s
capacity! In many cases, Facebook has found that it cannot acquire
sufficient additional capacity to the peer, motivating Facebook
to build Edge Fabric to overcome BGP’s inability to handle this
overload on its own.
BGP decisions can hurt performance. Facebook’s BGP pol-
icy favors paths that are likely to optimize network traffic perfor-
mance. The policy avoids transit routes when peer routes (which
are often better) exist, tiebreaks in favor of short AS paths (usually
lower latency), and prefers peers with dedicated peering capacity
over public peers which may encounter cross-congestion. However,
BGP itself is not performance-aware, and so this policy relies on
attributes such as AS path length that serve as imperfect heuristics
for proximity and performance.
To understand the severity of this problem, we compare the
performance of alternative paths at four PoPs, one in North America
(PoP-19 in §2), one in Europe (PoP-11), and two in Asia Pacific (PoPs-
2, 16). We shift a fraction of actual user traffic for all IP prefixes onto
the second and third best BGP paths. We describe how we conduct
these measurements in more detail in Section 6.2. Figure 7 depicts
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1CDF of PoPsFraction of prefixes congested 0 0.2 0.4 0.6 0.8 1 1 1.5 2 2.5 3 3.5 4 4.5 5CDF of overloaded interfacesPeak load (relative to interface capacity) 0.001 0.01 0.1 1-100-75-50-25 0 25 50 75 100CCDF of (PoP, prefix) pairsDifference in median latency w.r.t primary path (ms)1st pref - 2nd pref1st pref - 3rd prefEngineering Egress with Edge Fabric
ACM SIGCOMM, August 21–25, 2017, Los Angeles, CA, USA
the difference in median round-trip latency between the preferred
and less preferred paths. The figure shows that 5% of 
pairs could see an improvement of 20+ms (a threshold Facebook
considers significant) if switched from BGP’s preferred path to
its second preference, and 3% could see such an improvement if
switched to BGP’s third preference. At the scale of Facebook, this
relatively small fraction of prefixes represents a lot of traffic. The
results also show that the second preference is within 20ms of the
preferred path for 74% of  pairs, suggesting that we
may be able to avoid overload by detouring traffic to less-preferred
routes, without resorting to paths that have much worse baseline
performance.
4 GOALS AND DESIGN DECISIONS
Our goal is to overcome BGP’s limitations (§3) and enable Facebook
to use its interdomain connectivity (§2.2) to improve performance:
• Given limited capacity and the performance impact of congestion,
routing decisions must be aware of capacity, utilization, and
demand.
• Decisions should also consider performance information and
changes, while simultaneously respecting policies.
Towards these goals, in 2013 we began building Edge Fabric, a
traffic engineering system that manages egress traffic for our PoPs
worldwide. Section 5 describes Edge Fabric’s current approach to
automatically shift traffic to avoid overloading links. Section 8.1
describes how we have evolved Edge Fabric over time, due to
changing needs and thanks to operational experience that let us
improve its stability and ability to combat congestion. In addition,
we have studied how we can use measurements to improve egress
routing performance, and Section 6 discusses how we have begun
to incorporate them into our routing decisions.
We now present main design decisions of Edge Fabric.
Operate on a per-PoP basis. While Edge Fabric assigns traf-
fic to egress routes, the global load balancing system maps a user
request to ingress at a particular PoP (§2.1), and a flow egresses
at the same PoP at which it ingresses. So, Edge Fabric need only
operate at a per-PoP granularity; it does not attempt to orchestrate
global egress traffic. This design allows us to colocate its compo-
nents in the PoP, reducing dependencies on remote systems and
decreasing the scope and complexity of its decision process. We
can then restart or reconfigure Edge Fabric at a PoP in isolation
(§5.4) without impacting other PoPs (outside of the load balancing
system directing them more traffic).
Centralize control with SDN. We chose to use an SDN-based
approach, in which a centralized controller receives network state
and then programs network routing decisions. This approach brings
benefits of SDN: it is easier to develop, test, and iterate compared
to distributed approaches. Because Facebook connects to its peers
using BGP, part of the network state is the BGP paths Facebook
receives, which are continuously streamed to the controller (§5.1.1).
Incorporate real-time traffic and performance measure-
ments into decisions. The controller receives measurements of
capacity and demand multiple times per minute (§5.1.2), enabling
Edge Fabric to maximize utilization of preferred paths without
Figure 8: Edge Fabric components
overloading them (§5.2). Facebook has existing server-side
monitoring of client performance (§6.2.3). We added the ability to
measure multiple paths to a destination prefix in parallel by routing
a fraction of production flows (selected at random, §6.2.1) using
alternate routing tables at the PRs (§6.1 and §6.2.2). This approach
guarantees that measurements capture user-perceived performance.
Edge Fabric can identify cases where BGP’s preferred routing
is not optimal, laying the groundwork for performance-based
decisions.
Use BGP for both routing and control. Despite the central-
ized controller, every PR makes local BGP route decisions and PRs
exchange routes in an iBGP mesh; the controller only intervenes
when it wants to override default BGP decisions. To override a deci-
sion, Edge Fabric sets its preferred route to have high local_pref
and announces it via BGP sessions to PRs, which prefer it based
on local_pref (§5.3). Building Edge Fabric atop our established
BGP routing simplifies deployment, lets the network fall back to
BGP for fault tolerance, and leverages existing operational teams,
their expertise, and network monitoring infrastructure.
Leverage existing vendor software and hardware. We use
battle-tested vendor gear and industry standards, avoiding the need
for custom hardware or clean slate design. Sections 5 and 6 describe
our use of BGP, IPFIX, sFlow, ISIS-SR, eBPF, and BMP, and Sec-
tion 8.1 explains how specifics of vendor support have influenced
our design.
Overall, Edge Fabric’s design values simplicity and compatibil-
ity with existing infrastructure, systems, and practices. Its approach
to satisfy our primary goal—avoid overloading egress interfaces
(§5)—does not require any changes to our servers or (browser-based
or app-based) clients, adding only BGP sessions between the routers
and the controller. Our secondary goal of enabling performance-