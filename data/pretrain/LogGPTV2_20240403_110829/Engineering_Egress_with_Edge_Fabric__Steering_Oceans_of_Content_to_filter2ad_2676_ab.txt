### Egress Traffic Volume and BGP Prefixes

- **Figure 2**: Relative egress traffic volume (rounded) of 20 Points of Presence (PoPs).
- **Figure 3**: Number of BGP prefixes that constitute 95% of a PoP’s traffic.
- **Figure 4**: Number of routes to prefixes contributing 95% of a PoP’s traffic.

Each PoP serves between approximately 700 and 13,000 prefixes, with 16 PoPs sending 95% of their traffic to fewer than 6,500 prefixes.

### 2.2 Interdomain Connectivity

Facebook's Points of Presence (PoPs) establish BGP connections with other Autonomous Systems (ASes) in various ways:

- **Transit Providers**: Provide routes to all prefixes via a private network interconnect (PNI) with dedicated capacity for traffic between Facebook and the provider.
- **Peers**: Provide routes to their own prefixes and to prefixes within their customer cone [18]. Peers can be connected in different ways:
  - **Private Peers**: Direct connection via a dedicated PNI.
  - **Public Peers**: BGP session and traffic traverse the shared fabric of an Internet Exchange Point (IXP).
  - **Route Server Peers**: Routes are received indirectly via a route server, and traffic is exchanged across the IXP fabric.

A PoP may maintain multiple BGP peering sessions with the same AS, such as a combination of private and public peering or connections at multiple PoPs. Most PoPs connect to two or more transit providers, with each provider maintaining BGP sessions with two or more PRs for capacity and failure resilience. When possible, all PRs maintain equal PNI capacity to a given peer, although some PRs may have different capacities or no connection at all.

### Network Configuration and Routing

Facebook's network is configured to egress a flow only at the PoP where it enters, rather than routing it across the WAN from one PoP to another. This reduces backbone utilization, simplifies routing decisions, and improves system stability (§8.1.3). Despite this, Facebook has diverse routing options.

**Figure 4** shows the distribution of the number of routes each PoP can choose from to reach the prefixes that make up 95% of its traffic. If a peer provides the same path through both a public peering and a route server, or if multiple PRs receive the same route from the same peer, it is counted only once. All PoPs, except one, have at least two routes to every destination, and many have four or more routes to most prefixes.

PRs are configured to prefer peer routes over transit routes using `local_pref`, with AS path length as a tiebreaker. When paths remain tied, PRs prefer paths from the following sources in order: private peers > public peers > route servers. Peer type is encoded in Multi-Exit Discriminator (MED) values, and MEDs set by peers are stripped, as they are irrelevant given Facebook's egress policy.

The preference for peer routes recognizes that an AS that peers with Facebook expects to receive traffic on that link. Additionally, peer routes often offer better performance and lower risk of downstream congestion. Shorter AS paths may be more direct or give the traffic to the destination AS sooner [5]. By preferring the dedicated capacity of a private peering over a shared IXP fabric, Facebook avoids cross-congestion and respects the resources dedicated by the peer.

BGP at PRs and ASWs is configured to use BGP multipath. When a PR or ASW has multiple equivalent BGP best paths for the same destination prefix, it distributes traffic across these paths using Equal-Cost Multi-Path (ECMP).

### Peer ASes and Traffic Distribution

Facebook has thousands of peer ASes. **Table 1** shows, for example PoPs, the fraction of peers of each type. Each PoP shown has hundreds of peers, providing rich connectivity. The table also indicates the fraction of traffic each PoP can serve by peer type, assuming all traffic is assigned to the most preferred route without considering capacity. Although private peers make up at most a quarter of peers at any PoP, they receive the majority of traffic at all but PoP-11. High-volume peerings typically use the dedicated capacity of private interconnects. At all but PoP-11, 80% or more of traffic egresses to private, public, and route server peers rather than transit, reflecting how large providers "flatten" the Internet [5, 15]. However, the distribution of peer types varies widely across PoPs by count and by traffic.

### 3. Challenges of BGP

As demand increased and Facebook rapidly expanded its PoP infrastructure and connectivity, several challenges arose due to BGP limitations, leading to the development of Edge Fabric. Any static interdomain routing policy would likely face similar issues.

- **Capacity Limitations**: BGP is not capacity-aware. While Facebook builds PoPs, expands capacity, and pursues private interconnections, link capacity may still be insufficient for all traffic. Rapid growth in demand, short-term spikes, and diurnal patterns can cause high utilization, exceeding PNI capacity. ECMP at ASWs may distribute traffic unevenly across PRs, leading to overload at some PRs and poor utilization at others.
- **Performance Impact**: BGP decisions can negatively impact performance. Facebook's BGP policy favors paths that optimize performance, avoiding transit routes when better peer routes exist and preferring short AS paths. However, BGP itself is not performance-aware, relying on imperfect heuristics like AS path length.

To understand the scale of the problem, we analyzed a two-day log from January 2017, comparing the capacity of Facebook’s egress links to the traffic BGP would assign if Edge Fabric did not intervene. **Figure 5** shows the fraction of prefixes that would experience congestion, with most PoPs being capacity-constrained for at least one prefix. **Figure 6** shows peak load per interface, with 10% of interfaces experiencing periods where BGP would assign twice the interface’s capacity. **Figure 7** compares the latency of BGP’s preferred path to alternate paths, showing that 5% of (PoP, prefix) pairs could see a significant improvement (20+ms) if switched to the second or third preference.

### 4. Goals and Design Decisions

Our goal is to overcome BGP’s limitations and use Facebook’s interdomain connectivity to improve performance:

- **Capacity-Aware Routing**: Routing decisions must consider capacity, utilization, and demand.
- **Performance Considerations**: Decisions should incorporate performance information and changes while respecting policies.

In 2013, we began building Edge Fabric, a traffic engineering system that manages egress traffic for our global PoPs. Section 5 describes Edge Fabric’s current approach to automatically shift traffic to avoid overloading links. Section 8.1 discusses the evolution of Edge Fabric, driven by changing needs and operational experience.

**Main Design Decisions**:

- **Per-PoP Operation**: Edge Fabric operates at a per-PoP granularity, reducing dependencies on remote systems and simplifying decision processes.
- **Centralized Control with SDN**: An SDN-based approach, with a centralized controller receiving network state and programming routing decisions, enables easier development, testing, and iteration.
- **Real-Time Measurements**: The controller receives real-time traffic and performance measurements, allowing Edge Fabric to maximize utilization of preferred paths without overloading them.
- **BGP for Routing and Control**: Despite the centralized controller, PRs make local BGP route decisions, and the controller intervenes only to override default BGP decisions.
- **Leverage Existing Infrastructure**: We use battle-tested vendor gear and industry standards, avoiding the need for custom hardware or clean-slate design.

Overall, Edge Fabric’s design prioritizes simplicity and compatibility with existing infrastructure, systems, and practices. Its primary goal is to avoid overloading egress interfaces, while its secondary goal is to enable performance-based routing.