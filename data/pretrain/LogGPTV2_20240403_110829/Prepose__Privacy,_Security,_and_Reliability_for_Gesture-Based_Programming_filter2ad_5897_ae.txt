the user’s movements.
While we have made a design decision to use a theorem
prover for runtime matching, one can replace that machin-
ery with a custom runtime matcher that is likely to run
even faster. When deploying Prepose-based applications
on a less powerful platform such as the Xbox, this design
change may be justiﬁed.
5.4 Static Analysis Performance
Safety
linear
checking:
dependency
Figure
between
16
shows
a
the
number
near-
of
600
500
400
300
200
100
0
0
2
4
6
8
10
12
14
16
Fig. 17: Time to check internal validity, in ms, as a function on the
number of steps in the underlying gesture.
gesture
in a
restrictions. Exploring
and time
the
to
check
results
against
further,
steps
safety
we performed a linear re-
gression to see the in-
ﬂuence of other param-
eters such as the num-
ber of negative restric-
tions. The R2 value of the
ﬁt is about 0.9550, and the coeﬃcients are shown in the
table to the right. The median checking time is only 2 ms.
We see that safety checking is practical and, given how
fast it is, could easily be integrated into an IDE to give
developers quick feedback about invalid gestures.
Intercept
NumTransforms
NumRestrictions
NumNegatedRestrictions
NumSteps
-4.44
0.73
-2.42
-6.23
29.48
Validity checking: Figure 17 shows another near-linear
dependency between the number of steps in a gesture and
the time to check if the gesture is internally valid. The
average checking time is 188.63 ms. We see that checking
for internal validity of gestures is practical and, given how
fast it is, could easily be integrated into an IDE to give
developers quick feedback about invalid gestures.
Conﬂict checking: We performed pairwise conﬂict check-
ing between 111 pairs of gestures from our domains.
Figure 18 shows the CDF of conﬂict checking times, with
the x axis in log scale. For 90% of the cases, the checking
time is below 0.170 seconds, while 97% of the cases took
less than 5 seconds and 99% less than 15 seconds. Only
one query out of the 111 took longer than 15 seconds. As a
result, with a timeout of 15 seconds, only one query would
need attention from a human auditor.
6 Limitations
This work is the ﬁrst step in deﬁning a programmable way
to limit the potential for privacy leaks in gesture-based
programming. We are not claiming that we have solved all
the potential privacy issues. In fact, we believe strongly
that the attack model will evolve as this space rapidly
changes.
133133
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:14:45 UTC from IEEE Xplore.  Restrictions apply. 
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
1
97% of checks 
are faster than 5 
seconds
10
100
1,000
10,000
100,000
Fig. 18: Time to check for conﬂicts for a pair of gestures presented
as a CDF. The x axis is seconds plotted on a log scale.
A major challenge is to deﬁne a precise and easy to
reason about attack model in this space. Our key contribu-
tion lies in going beyond the model that gives application
direct access to hardware and providing an abstraction
layer above that. It is exceedingly diﬃcult to argue that
that abstraction layer cannot be abused by a clever at-
tacker. By way of analogy, consider an operating system
mechanism that allows applications to register keystrokes
(or key chords) such as Ctrl + Alt + P. While this makes
it considerably more diﬃcult to develop a keylogger, it is
diﬃcult to claim that one cannot determine whether the
user is left-handed or possibly to ﬁngerprint diﬀerent users
based on the frequency of their shortcut use. Similarly, in
the context of Prepose, a clever attacker may deﬁne a
“network” of really ﬁne-grained gestures to collect statistics
about the user.
A key advantage of Prepose is that when new at-
tacks are discovered, they can be encoded as satisﬁability
queries, which gives one way to tackle these attacks as well.
We see the following areas as extensions of our current
work:
• We do not explicitly reason about the notion of time;
there could be a pose that is safe for brief periods of
time but is less safe when held for, say, a minute.
• Our current approach reasons about conﬂicts at the
level of entire gestures. This does not preclude con-
ﬂicts at the intermediate, sub-gesture level. A possible
way to alleviate this situation is to automatically
compile the current set of gesture into intermediate,
atomic gestures, which could be validated for lack of
conﬂicts.
• Prepose requires the developer to manually write
gestures. A natural next step is to automatically
synthesize gestures by demonstration.
7 Related Work
Below we ﬁrst describe some gesture-building approaches,
mostly from the HCI community, and then we talk about
privacy in sensing-based applications.
7.1 Gesture Building Tools
Below, we list some of the key projects that focus on
gesture creation. Prepose’s approach is unique in that
it focuses on capturing gestures using English-like com-
mands. This allows gesture deﬁnitions to be modiﬁed more
easily. Prepose diﬀers from the tools below in that it
focuses on security and privacy at the level of system
design.
CrowdLearner [1] provides a crowd-sourcing way to
collect data from mobile devices usage in order to create
recognizers for tasks speciﬁed by the developers. This way
the sampling time during the application development is
shorter and the collected data should represent a better
coverage of real use scenarios in relation to the usual in-lab
sampling procedures. Moreover, it abstracts for developers
the classiﬁer construction and population, requiring no
speciﬁc recognition expertise.
Gesture Script [18] provides a unistroke touch gesture
recognizer which combines training from samples with
explicit description of the gesture structure. By using the
tool, the developer is enabled to divide the input gestures
in core parts, being able to train them separately and
specify by a script language how the core parts are per-
formed by the user. This way, it requires less samples for
compound gestures because the combinations of the core
parts are performed by the classiﬁer. The division in core
parts also eases the recovery of attributes (e.g. number of
repetitions, line length, etc.) which can be speciﬁed by the
developer during the creation of the gestures.
Proton [15] and Proton++ [14] present a tool directed
to multitouch gestures description and recognition. The
gestures are modeled as regular expressions and their
alphabet consists of the main actions (Down, Move and
Up), and related attributes e.g.: direction of the move
action; place or object in which the action was taken;
counter which represents a relative ID; among others. It is
shown that by describing gestures with regular expressions
and a concise alphabet it is possible to easily identify
ambiguity between two gestures previously to the test
phase.
CoGesT [8] presents a scheme to represent hand and
arms gestures. It uses a grammar which generates the pos-
sible descriptions, the descriptions are based on common
textual descriptions and related to the coordinate system
generated by the body aligned planes (sagittal, frontal
and horizontal). The transcription is mainly related to
relative positions and trajectories between them, relying
on the form and not on functional classiﬁcation of the
gesture. Moreover it does not specify the detailed position
but more broad relations between body parts. This way
the speciﬁed gestures are not strongly precise. On the
other hand, it enables users to produce equivalent gestures
134134
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:14:45 UTC from IEEE Xplore.  Restrictions apply. 
by interpreting the description and using their knowledge
about gesture production.
BAP [5] approaches the task of coding body movements
with focus on the study of emotion expression. Actors
trained the system by performing speciﬁc emotion rep-
resentations and these recorded frames were coded into
pose descriptions. The coding was divided into anatomic
(explicating which part of the body was relevant in the
gesture) and form (describing how the body parts were
moving). The movement direction was described adopting
the orthogonal body axis (sagittal, vertical and trans-
verse). Examples of coding: Left arm action to the right;
Up-down head shake; Right hand at waist; etc.
Annotation of Human Gesture [22] proposes an ap-
proach for transcribing gestural movements by overlaying
a 3D body skeleton on the recorded actors’ gestures. This
way, once the skeleton data is aligned with the recorded
data, the annotation can be created automatically.
RATA [23] presents a tool to create recognizers for touch
and stylus gestures. The focus is on the ease and rapidity of
the gesture recognition developing task. The authors claim
that within 20 minutes (and by adding only two lines of
code) developers and interaction designers can add new
gestures to their application.
EventHurdle [13] presents a tool for explorative proto-
typing of gesture use on the application. The tool is pro-
posed as an abstraction of the gathered sensor data, which
can be visualized as a 2D graphic input. The designer also
can specify the gesture in a provided graphical interface.
The main concept is that unistroke touch gestures can be
described as a sequence of trespassed hurdles.
GestureCoder [19] presents a tool for multi-touch ges-
ture creation from performed examples. The recognition is
performed by creating a state machine for the performed
gestures with diﬀerent names. The change of states is
activated by some pre-coded actions: ﬁnger landing; lifting;
moving; and timeout. The ambiguity of recorded gestures
is solved by analyzing the motion between the gestures
using a decision tree.
GestureLab [4] presents a tool for building domain-
speciﬁc gesture recognizers. It focuses on pen unistroke
gestures by considering trajectory but also additional at-
tributes such as timing and pressure.
MAGIC [2] and MAGIC 2.0 [17] are tools to help
developers, which are not experts in pattern recognition,
to create gesture interfaces. Focuses on motion gesture
(using data gathered from motion sensors, targeted to
mobile scenario). MAGIC 2.0 focuses on false-positive pre-
diction for these types of gestures. MAGIC comes with an
“Everyday Gesture Library” (EGL), which contains videos
of people performing gestures. MAGIC uses the EGL to
perform dynamic testing for gesture conﬂicts, which is
complementary to our language-based static approach.
7.2 Sensing and Privacy
The majority of work below focuses on privacy concerns
in sensing applications. In Prepose, we add some security
concerns into the mix, as well.
SurroundWeb [27] presents an immersive browser
which tackles privacy issues by reducing the required
privileges. The concept is based on a context sensing tech-
nology which can render diﬀerent web contents on diﬀerent
parts of the room. In order to prevent the web pages to
access the raw video stream of the room, SurroundWeb
is proposed as a rendering platform through the Room
Skeleton abstraction (which consists on a list of possible
room “screens”). Moreover the SurroundWeb introduces
a Detection Sandbox as a mediator between web pages
and object detection code (never telling the web pages
if objects were detected or not) and natural user inputs
(mapping the inputs into mouse events to the web page).
Darkly [12] proposes a privacy protection system to
prevent access of raw video data from sensors to untrusted
applications. The protection is performed by controlling
mechanisms over the acquired data. In some cases the
privacy enforcement (transformations on the input frames)
may reduce application functionality.
OS Support for AR Apps [6] and AR Apps with Recog-
nizers [11] discusses the access the AR applications usually
have to raw sensors and proposes OS extension to control
the sent data by performing the recognizer tasks itself.
This way the recognizer module is responsible to gather
the sensed data and to process it locally, giving only the
least needed privileges to AR applications.
MockDroid [3] proposes an OS modiﬁcation for smart
phones in which applications always ask the user to access
the needed resources. This way users are aware of which
information are being sent to the application whenever
they run it, and then can decide between the trade-oﬀ of
giving access or using the application functionality.
AppFence [9] proposes a tool for privacy control on
mobile devices, which can block or shadow sent data
to applications in order to keep the application up and
running, but prevent exﬁltration of on-device data. What
You See is What You Get [10] proposes a widget which
alerts users of which sensor is being requested by which
application.
Recent work on world-driven access control restricts sen-
sor input to applications in response to the environment,
e.g. it can be used to disable access to the camera when in a