done by the Hadoop jobtracker. Upon receiving a list of task de-
scriptions, the jobtracker ﬁrst creates TaskInProgress objects
for the tasks on the list, and then assembles these objects into a
JobInProgress queue. Whenever a “heartbeat” signal comes,
indicating that a tasktracker of a node is ready to run a new task,
the jobtracker looks up the queue and locates the most appropri-
ate task for that node, the one whose data block is stored on the
node, for example. This task scheduling process becomes privacy-
sensitive in Sedic: we revised the jobtracker, which sits on the pri-
vate cloud, to tag the TaskInProgress objects according to the
sensitive ﬁelds within the InputSplit objects, and sched-
ule the tasks based on their tags. Simply put, a sensitive task is
always scheduled to a private datanode, which often hosts the re-
519lated data block, while a public task is more likely to be handed
over to a public datanode to outsource the map computation.
Task execution. A tasktracker assigned a map task by the job-
tracker executes the task over the data block stored on its local
host or downloaded from other nodes. In Hadoop, this proceeds
as follows. The mapper reads a record from the block through
a RecordReader object, which makes a socket connection to
DataXceiverServer, a service maintained by the datanode
that hosts the block. Each time a record is processed, the mapper
gets the next one by calling the method nextKeyValue through
the connection. This happens even when the block is co-located
with the tasktracker on the same node.
A problem of this data-access mechanism is that it only reads
contiguous data: whenever the read stops (e.g., due to the need of
moving the read pointer to skip some data), HDFS discontinues
the current connection. Given a data block often contains discon-
nected segments of public or sensitive data, a mapper has to call
seek to adjust the read pointer after ﬁnishing its work on one seg-
ment, which interrupts connections. As a result, new connections
need to be made continuously, which incurs a huge overhead. For
example, establishing a connection takes DataXceiverServer
(on the sender side) several hundred milliseconds to create a new
BlockSender object for delivering data to RecordReader (on
the receiver side).
Our solution to this problem is to read all the data segments the
mapper needs within one block through a single connection. To this
end, we modiﬁed RecordReader and DataXceiverServer
to pass the offset-length pairs to BlockSender, which was also
enhanced to perform both read and seek operations on a block ﬁle
according to these pairs. This ensures that a connection will be torn
down only after all required data is given to the mapper.
3.3 Reduction Planning
The scheduling of the reduce task poses yet another technical
challenge. The reducer receives the outputs from the mappers run-
ning on sensitive data, and therefore cannot be directly executed
on the public cloud. A straightforward solution here is to move all
the map outcomes produced by the public cloud back to the pri-
vate cloud. This, however, can incur a huge amount of inter-cloud
communication, which we intend to avoid. For example, ﬁnding
the TCP ports connected by each host requires a large number of
port information to be transferred back to private cloud for reduce
operations. Alternatively, we can carefully plan the scheduling of
map tasks to ensure that the total amount of the map output to be
generated by the public cloud does not exceed an upper limit set by
the user according to the bandwidth she is willing to use and the
delay she can tolerate. More speciﬁcally, the user is supposed to
specify to Sedic the amount of output data produced by mapping
one record and a threshold that represents the maximum amount of
data to be sent from the public cloud to the private cloud. When
scheduling a map task to a node on the public cloud, the jobtracker
estimates the output volume the task will produce according to the
size of the data it works on. Once the aggregated volume of the
outsourced tasks is found to exceed the threshold, we stop moving
computation to the public cloud. The problem of this simple treat-
ment is that it constrains the amount of computation that can be
undertaken by the public cloud.
More desired here is to let the public cloud perform part of the
reduce operation, which not only cuts down the volume of the data
that needs to be sent back, but also moves part of the computation
away from the private cloud. What we can leverage are the prop-
erties of reducers: they all contain a fold loop that works on a list
and in the vast majority of cases, such an operation is associative
and commutative. Examples include counting the number of oc-
currences of a keyword in a large database and comparing the edit
distances of different alignments to ﬁnd the smallest one. For these
reducers, Sedic extracts their loops to build combiners, which pro-
cess the data on the public cloud and deliver their outcomes to the
private cloud to complete the computation. Sedic provides an au-
tomatic program analysis tool that evaluates the source code of a
reducer to determine its features and perform code transformation
when necessary, which is elaborated in Section 4.
4. AUTOMATIC REDUCER ANALYSIS AND
TRANSFORMATION
In this section, we present a suite of new techniques that opti-
mize the reduction structure of a MapReduce job for secure and ef-
ﬁcient computing of the job over a hybrid cloud. These techniques
perform an automatic analysis and transformation on a reducer’s
Java source code, as soon as the job is submitted to the jobtracker,
which enables the Sedic framework to schedule the reduce tasks in
a way that minimizes the inter-cloud communication as well as the
workload of the private cloud. Their design and implementation
are elaborated below.
4.1 Automatic Analysis
The idea. The MapReduce computing framework has its origin
in functional programming [23]. The reduce operation actually
comes from fold, a high order function that aggregates elements on
a list. Although the reducer of real-world job can be more com-
plicated than fold, which is rather straightforward, it typically con-
tains a fold component, in the form of a loop, to combine the values
of the same keys from an input list. If the fold is associative, we
can run it on the public cloud to partially process map outcomes.
For example, given an associative fold f ([a1, a2, a3, a4, a5, a6]) =
f ([f ([a1, a2]), f ([a3, a4]), f ([a5, a6])]), we can outsource f ([a1,
a2]) and f ([a5, a6]) to the pubic cloud if only a3 and a4 are sen-
sitive. This move also helps reduce the communication overhead
caused by sending the map outputs back to the private cloud, since
the fold component partially combines these outputs. We can do
even better when the fold is also commutative, which allows us to
compute f ([a1, a2, a5, a6]) on the public node in the above exam-
ple. Actually, real-world reducers are often associative and com-
mutative. In the cases they are not, their fold components typically
have these properties. Figure 4 presents an example: although a
reducer that calculates the mean of its input values is clearly not
associative, its fold loop, which does the sum, is.
Before a fold operation can be outsourced, it needs to be ana-
lyzed to ﬁnd out whether these desired properties are there. To
see how to do this, let us ﬁrst take a close look at the operation.
Consider a list [a1, · · · , an]. A fold on the list can be described as
f ([a1, · · · , an]) = g([g([· · · g([g([a1, a0]), a2]) · · · ]), an]), where
g is a function that works only on a two-element list and a0 the ini-
tial value. Essentially, g describes the operation performed on a
list member and an intermediate aggregation outcome at every iter-
ation of the fold loop. If the operation is associative and commu-
tative, so will be the whole fold loop. In other words, all we need
to study here is what happens in a single iteration. In the rest of
the section, we describe our analysis techniques, which ﬁrst check
the loop dependence of the fold and then evaluate the operation be-
tween a list member and the intermediate value in an iteration. We
also prototyped our approach using Soot [11], a Java optimization
framework.
Reducer analysis. The ﬁrst step to analyzing a reducer is to lo-
cate its fold component. As discussed before, fold is typically per-
formed through a loop in Java or other imperative programming
languages. Actually, the loop is mandated by the reduce interface
520Figure 4: Reduce Structure
provided by Hadoop, which keeps the input values in an iterator.
Our approach utilizes Soot to convert the Java source code of a re-
ducer to Jimple intermediate representation and then runs the API
LoopFinder() to identify this fold loop, which is characterized
by its operations on these input values. All the follow-up analysis
mainly happens to that loop. Speciﬁcally, we ﬁrst perform a loop
dependence analysis to check whether there exists any variable that
is ﬁrst deﬁned within one iteration and later used in another iter-
ation.
If there is no such a variable, we conclude that the loop
dependence does not exist and the loop can be used to build a com-
biner (Section 4.2), as it is both associative and commutative. This
happens in the cases such as ﬁltering, which drops the values above
or below a certain threshold, Grep, Sort, etc.
If the dependence relations are found, our approach moves on to
perform a liveness analysis on their causal variables. Speciﬁcally,
the analysis utilizes a Soot API SimpleLiveLocals to ﬁnd out
those still alive posterior to the loop and put them in a set D. These
variables not only carry the dependence but also produce the out-
comes of the fold operation, and therefore are the keys for under-
standing the fold’s properties. The follow-up analysis backtracks
the deﬁne-use chain of each variable v ∈ D within an iteration,
starting from the ﬁrst use of v outside the loop. The objective here
is to discover all the variables and operations that deﬁne v across all
execution paths in the iteration. This evaluation ends when it hits
the input key-value list or moves out from the scope the iteration.
For each execution path discovered, our approach checks all the
operators that deﬁne the loop-dependent variables. The fold be-
comes associative and commutative if these properties hold across
all the paths. This can only be veriﬁed empirically, as the prob-
lem in general is undecidable. However, all we really need here
is just a sufﬁcient condition that covers the operations a real-world
reducer performs, which are typically simple. Speciﬁcally, a form
of the computation analyzed in our research is as follow: for each
execution path i in an iteration with a path condition Pi, the re-
ducer calculates an aggregation v ←− v N δi, where δi, which
is a function, and Pi do not contain any loop-dependent variables,
and N is an operator. Essentially, this computation aggregates all
δi for each input key-value pair using N. It is clearly associative
and commutative if N has all these properties. Such a type of ag-
gregation, though simple, actually generalizes the fold operations
performed by the vast majority of real-world MapReduce jobs. A
prominent example is sum that adds the values of the same key to-
gether. Other examples include ﬁnding the optimal alignment be-
tween two strings, where the edit distance is minimal (operator here
is min), and calculating different statistics according to the values
of the inputs, which are loop independent.
Figure 5: Code analysis example
Such an aggregation can be identiﬁed from the deﬁne-use chain
of the variable v. Our approach evaluates all the statements on
the chain to ensure that v is the only loop-dependent variables
used, and the operation on the variable only happens between it and
the program elements independent of other iterations, and always
through the same, associative and commutative operator. We also
check all the branch conditions encountered, which need to be loop
independent. For the example in Figure 5, our analyzer backtracks
the statements that work on the live variable sum, which cause the
loop dependence. All such statements meet the above conditions,
particularly, the addition operator is associative and commutative.
Therefore, we conclude that the fold loop has all the desired prop-
erties and can be used to build a combiner.
In our research, we
implemented this automatic analysis in the prototype.
4.2 Code Transformation
Once the fold loop is found to be associative and commutative,
Sedic uses it to build a combiner, which is deployed to both the
public cloud and the private cloud to preprocess map outputs. The
results of this operation are fed to a new reducer on a private node
to complete the computation. In this section, we describe our code
transformation technique that supports this data processing.
Our approach. Like the code analyzer, our transformation tool
was also implemented as a Soot jtp pass [11]. It works on the
Jimple representation of the Hadoop job the user uploads and saves
the new code to a newly-created combiner class CombinedData
before resubmitting the modiﬁed job to the cloud. Speciﬁcally, the
tool separates the fold loop from the rest part of the reducer. As
illustrated in Figure 4, the loop sits right between variable dec-
larations/initializations and the posterior loop operations that ulti-
mately export the outcomes of the computation to outputCollect
of a Hadoop job. Using the information provided by the code ana-
lyzer, our tool locates the fold loop and its related variable declara-
tions on Soot’s structural representation of Jimple code, and copies
521these nodes to the new combiner class. The new program structure
is then converted directly into Java class ﬁles by Soot. Note that
when the whole reducer includes just an associative and commuta-
tive loop and standard output functions (e.g., Context.write),
our tool simply adds SetReducer to the original job to make the
reducer also the combiner with proper initial value adjustment.
Although conceptually simple, this code transformation does bring
in a few interesting technical issues. First, the handling of the vari-
ables in the reducer needs to be well thought-out. Remember that
the combiner generated thereby is supposed to be run on both the
public and private clouds.
If we simply copy to it the variables
and their initializations from the original reducer, we could end up
with aggregating the initial values of these variables multiple times,
which leads to an incorrect outcome. To see how this could hap-
pen, let us look at the example in Figure 4. In the fold loop that
does the sum, if we keep the initial value of the aggregation vari-
able i0 = 5 in the combiner, this value will be added twice to
its partial sums produced on both the public cloud and the private
cloud, and added again by the new reducer, making the ﬁnal result
larger than what it is supposed to be by 10. Our solution is to set
the initial value of the fold operation to an identity element of the
fold operator N to avoid this double-counting. For example, when
the operator is an addition, we can set the value to 0; when it is a
multiplication, we initialize it as 1. The original initial values are
only aggregated by the new reducer. Second, although the new re-
ducer and combiner share large amount of code with the original
reducer, a naive copy of Jimple Stmt from original reducer will
not work because the ValueBoxes in each Jimple Stmt refers to
variables in original reducer. Besides copying the Local chain
and Trap chain, we also patch the values in each ValueBox so it
refers to variables in the new Local chain. Finally, we also need
to add an interface between the new combiner and the new reducer.
Speciﬁcally, our tool sets all the live variables of the fold loop as
the output of the combiner and put them into a data structure called
CombinedData, which is essentially a list of output objects. We
further modify the loop within the new reducer, which extracts ele-
ments from the lists provided by the combiners on both clouds and
aggregates these elements using the operator N. We also need to
remove the computation of δ from the reducer to avoid duplicate
computation of δ.
An example. For the example in Figure 4, the reducer calculates
the mean of input values. Our code analyzer identiﬁes the fold loop
on the structural representation of the code, moves it, together with
all related variables, to the new combiner class and further modiﬁes
the reducer structure. Such code is ﬁnally converted into Java class
ﬁles. For the ease of understanding, we also present the Java source
code of the combiner and reducer here, though the output of our
tool is actually Java bytecode.
Computing jobs and data. In our study, we ran ﬁve Hadoop jobs
over our prototype to evaluate its effectiveness and performance,
which include a data analysis for target marketing, two intrusion
detection analyses and two jobs for preparing spam detection. The
target-marketing analysis was designed to understand the public’s
responses to different brand names. More speciﬁcally, we utilized
Hadoop’s Grep implementation to evaluate one day Twitter data