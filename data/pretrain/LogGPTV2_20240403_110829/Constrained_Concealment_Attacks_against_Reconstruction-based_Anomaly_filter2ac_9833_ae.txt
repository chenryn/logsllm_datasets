𝑖 . In order to decide if the system is under attack, the
mean squared reconstruction error between observed and predicted
features are computed. If the mean squared reconstruction error
exceeds a threshold 𝜃, the system is classified as under attack. The
authors of [51] chose 𝜃 as 99.5 percentile (Q99.5) of the average
reconstruction error over the training set.
We formalize this as follows. Given a target vector (cid:174)𝑠𝑡, we define:
(cid:174)𝑒 = (cid:174)𝑠𝑡 − (cid:174)𝑜 = [𝑑1, . . . , 𝑑𝑛] as the reconstruction error 𝑛-dimensional
vector, 𝜀((cid:174)𝑒) as the corresponding average reconstruction error:
𝑛∑︁
𝑖=1
𝜀((cid:174)𝑒) =
1
𝑛
2,
𝑑𝑖
(3)
and 𝑦(𝑋) as the classified state of the water distribution system
out of Reconstruction-based Intrusion Detection System. Given an
input 𝑋, 𝑦 is ‘under attack’ if 𝜀 greater than 𝜃:
(cid:40)‘under attack’ if 𝜀((cid:174)𝑒) > 𝜃
𝑦(𝑋) =
‘safe’ otherwise
(4)
Moreover, the authors propose a window parameter that takes into
consideration the mean of 𝜀((cid:174)𝑒) of the last window time steps to
decide if the current tuple is ‘safe’. This helps diminishing the
amount of false positives, since an alarm is raised only if in the last
window time steps the mean of 𝜀((cid:174)𝑒) is above 𝜃.
4.2 Baseline: Replay Attack
In the replay attack setting (prior work, used here as baseline), the
attacker does not know how detection is performed. In order to
avoid detection, the attacker can replay sensor readings that have
been recorded while no anomalies were occurring in the system.
In particular, we assume that the attacker could record selected
data occurring exactly 𝑛 days before—i.e., if the concealment attack
starts at 10 a.m., the attacker starts replaying data from 10 a.m. one
day before.
4.3 Iterative Attack
In the iterative attack, the white box attacker knows how detection
is performed, all thresholds and parameters of the detector, as well
as the normal operation range for each one of the model features.
For example, the attacker knows which sensor readings are common
during normal operation of the physical process. As a result, the
attacker essentially has access to an oracle of the Deep Architecture,
where the attacker can provide arbitrary (cid:174)𝑥 features and gets the
individual values of the reconstruction error vector (cid:174)𝑒. The attacker
then computes max𝑖 (cid:174)𝑒 and finds the sensor reading 𝑟𝑖 with the
highest reconstruction error from (cid:174)𝑥.
In order to satisfy the constraint 𝜀( (cid:174)𝑒′) < 𝜃 (i.e. 𝑦((cid:174)𝑥 + (cid:174)𝛿) = ‘safe’
in Equation 1), the attacker performs a coordinate descent algo-
rithm to decrease the reconstruction error related to 𝑟𝑖 (As we rely
on coordinate descent algorithm we do not use gradient estima-
tion methods). At each iteration of the algorithm, a coordinate
of the feature vector is modified until a solution is found or the
computational budgets are exceeded.
Two computational budgets are put in place: patience and budget.
If no lower reconstruction error is found by descending a coordinate,
the algorithm tries descending other coordinates. If no improved
solutions are found in patience iterations, the input is no longer op-
timized. budget is the maximum number of iterations for coordinate
descent. After budget attempts without satisfying 𝜀((cid:174)𝑒′) < 𝜃, the
input is no longer optimized, and no solution is found. Additional
details are found in Appendix A.
Sensor readings 𝑟𝑖 are modified in the range of normal operating
values; this guides the computation to a solution that is consistent
with the physical process learned by the detector. For example,
if normal operations of sensor 𝑟𝑖 are in the range [0, 5], the at-
tacker tries to substitute the corresponding value of 𝑟𝑖 according to
its range to see if the related reconstruction error decreases. The
algorithm 1 can be found in Appendix A as a reference.
4.4 Learning Based attack
In the learning based setting, the black box attacker does not know
anything about the detection mechanism except the fact that it relies
on a Reconstruction-based model: the attacker can only intercept
and manipulate the communication between the PLCs and SCADA.
However, the nature of the ICS environment allows us to assume
that a detection mechanism trained over a specific CPS should
represent its physical rules in order to spot anomalies.
In this case, a reasonable attack scheme can be divided into five
steps. The attacker first intercepts traffic from PLCs to SCADA in
order to collect information on how the ICS behaves under normal
conditions. Second, collected data are used to learn how the system
behaves normally and train a Deep Learning model, implicitly train-
ing a model to solve the minimization problem in Equation 1. Third,
the attacker manipulates the physical process; anomalous data are
generated as a consequence. Fourth, the adversarially trained model
is used to conceal anomalous readings by morphing them into con-
cealed data that will be classified as ‘safe’. Fifth, the concealed data
is forwarded to the SCADA.
(cid:77)Autoencoder-based Generator We implement the learning based
attack using an autoencoder network to generate concealed data
(the word generator is used with a different meaning than the usual
one. In our case the input is not random noise that is going to be
crafted by the network). The autoencoder is trained while inter-
cepting normal traffic; the network learns to output tuples that are
classified as being normal with high confidence. Forwarding the
output of the adversarial network—regardless of how the detector
is built—forces it to misprediction, because the adversarial exam-
ples have been adjusted to resemble normal operations. Note that
the autoencoder does not completely change the tuple; intuitively,
only the part that contains the anomaly is reconstructed to match
the learned physical behavior. We explored different architectures,
activation, and loss functions for the autoencoder network. Experi-
mentally, we verified that by using hidden layers to increase the
data dimensionality (i.e., with more units than the input and output
layers), rather than decreasing it, we achieved a higher conceal-
ment capacity than canonical ‘compressing’ models. Particularly,
we implemented an autoencoder network with three hidden layers,
with input and output dimensions equal to the number of sensors
and actuators in the network. We used mean squared error as a
loss function and sigmoid as an activation function. To train the
network, we use the ADAM [31] optimizer with a learning rate set
to 0.001.
Constrained Concealment Attacks against Reconstruction-based detectors in ICS
ACSAC 2020, December 7–11, 2020, Austin, USA
(cid:77)Post-processing In order to generate feasible inputs for the
anomaly detector, we need to consider that not all the sensor read-
ings assume continuous values—some are categorical integers that
represent the status of actuators. Since the output of a neural net-
work is continuous, we need to post-process all the readings that
are supposed to be integers. For example, if a pump status assumes
a value 0 when it is turned off and 1 when it is turned on, post-
processing approximates the corresponding output value to the
nearest allowed integer. According to this post-processing, some
other values should be adjusted in order to match the physical rules.
This is the case, for example, for speed sensors that must read 0 if
their related pump is off.
4.5 Positioning with respect to
State-of-The-Art AML attacks
In this work, we consider attacks on a (prior work) reconstruc-
tion based classifier. This represents a substantial difference from
classifiers considered in AML attacks in other domains. In related
work [10, 37, 43], attacks on end-to-end Neural Network classi-
fiers are considered (i.e., those classifiers are trained to optimize
the cross-entropy loss). In particular, target misclassification is
achieved, diminishing the predicted probability of the true class
in the output layer. Our problem setting is different: To evade the
classifier, we need to diminish the residual between input and out-
put. Our target Neural Networks are trained to optimize the Mean
Squared Error loss. This can be achieved by reconstructing the sen-
sor signal in a way that matches the learned physical properties of
the ICS. Those differences motivated our novel white-box and black
box approaches to evade Deep Learning-based Anomaly Detectors
in ICS.
(cid:77)Model Robustness Adversarial Robustness [37] is achieved by a)
adversarial training b) classifier capacity. Following the definition,
adversarial training is obtained embedding adversarial examples in
the training set. In our context, the Neural Network is trained to
approximate the system behavior during normal operating condi-