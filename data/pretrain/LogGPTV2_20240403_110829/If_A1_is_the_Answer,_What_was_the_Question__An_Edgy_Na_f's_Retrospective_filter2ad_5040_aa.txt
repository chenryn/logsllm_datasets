### Title: If A1 is the Answer, What was the Question? An Edgy Na√Øf's Retrospective on Promulgating the Trusted Computer Systems Evaluation Criteria

**Author: Marvin Schaefer**

**Abstract**
This paper provides a personal and introspective retrospective on the history and development of the United States Department of Defense (DoD) Trusted Computer System Evaluation Criteria (TCSEC), commonly known as the Orange Book. The TCSEC distilled what many researchers considered to be the most sound and proven principles and practices for achieving graded degrees of sensitive information protection in multiuser computing systems. While its seven evaluation classes were explicitly directed at standalone computer systems, many of its authors contended that its principles would serve as adequate guidance for the design, implementation, assurance, evaluation, and certification of other computing applications, including database management systems and networks. This account is a personal reminiscence by the author and concludes with a subjective assessment of the TCSEC's validity in light of its successor evaluation criteria.

### 1. Introduction: From the Primordial Ooze
In the early days of computing, there was no perceived security problem. There were no external threats or intrusion issues. Computers were expensive and physically protected, with only authorized and trained personnel allowed access to mainframes or peripherals. Users submitted jobs on punched card decks or tapes, and each job had a specified duration. Physical protection and personnel background checks were deemed sufficient to protect data in government, banking, and industry.

This paper recounts my personal involvement in the events leading to the development, writing, trial use, promulgation, official use, and misuse of the TCSEC. Even after it became a DoD standard, it was often referred to as the Orange Book, a name derived from the final color of an evolving series of published drafts.

### 1.1. Early Education in Computer Security
I first left academia in 1965 for a summer research and technology training program at the System Development Corporation (SDC) in Santa Monica, a non-profit spin-off of the RAND Corporation. SDC provided a radical departure from the UCLA mathematics department, with a staff of academic mathematicians, social and hard science researchers, and computing experts. SDC received most of its funding from the Department of Defense and other government agencies.

The company was filled with modern vacuum tube and semi-transistorized computers, consuming half of the electric power generated for Santa Monica. Our three-month training class included lectures from pioneering researchers in hardware and operating system design, assemblers, programming languages, compilers, interpreters, metacompilers, natural language processing, database management, list processing (LISP 1.5), and time-sharing system design. The most exciting part was using the experimental IBM A/N-FSQ-32(V) Time Sharing System (TSS).

The Q-32 supported up to 24 interactive users at a time. We shared this computer with SDC researchers and were given individual login IDs for billing purposes. These IDs were not used for identification or authorization. Occasionally, we were asked to log off to allow remote demonstrations to run smoothly and rapidly. Other than these demonstrations, there were no public or employee dial-up services on the Q-32.

There were no access controls on data, and files were generally meant to be shared. The concept of protection was soon revealed to be nonexistent, as a few of us inadvertently discovered how to subvert careful operating system policies and mechanisms on the single-protection-state Q-32 architecture.

#### 1.1.1. Modifying an Operating System
My first experiences penetrating a computer system occurred here. While we could code in JOVIAL, this required overnight batch-mode compilation before we could interact with our programs under time-sharing. To avoid this delay, we used the fully-interactive Time Shared Interpreter (TINT), a subset time-shared compiler (JTS), and LISP 1.5. However, the Q-32 TSS required adequate contiguous space on one of the swap drums to load the entire compiler or LISP or to work with a user program. Dynamic paging had not yet been invented.

I and a couple of colleagues wrote a small program (appropriately named CANCER) that usurped the operating system, repacked the drums containing other user programs, modified internal system tables, and made room for our own programs. This had to be completed within a single quantum. Sometimes it failed, causing system crashes and angering other users. The operating systems staff dismissed our actions as those of college kids having fun, not malicious users.

#### 1.1.2. Cat and MOUSE
Q-32 TSS scheduling initially used a democratic round-robin scheme, giving each program a 300 ms quantum. This interfered with highly interactive programs and resulted in long compilation times. Clark Weissman implemented queues for different kinds of jobs: an "interactive" queue and a "production" queue. Membership in the interactive queue depended on the program performing input or output during every few quanta. Programs failing to do this were moved to the production queue, where they executed less frequently but alternated through ten quanta interleaved with members of the interactive queue.

Users found ways to avoid being placed in the production queue, such as adding useless single-character output operations. Additional dodges were introduced, and a few users collaborated to modify the operating system's scheduling algorithms in their favor. With no protected memory or privileged instructions, procedural controls were the only means of control. Passwords and audit logs were introduced, but these proved illusory without hardware protection.

Monitoring programs were introduced, and we found ways to abort or replace them. Each protective or regulatory move by the defenders was met with offensive countermoves by the users. The system's design was still a prototype, and test users were needed to provide useful data and feedback. No strong security could have been provided due to hardware inadequacies. Penetrations and subversions were performed to get work done more rapidly, and no user data was maliciously corrupted or spied upon. This led to a semi-declared "state of war" between users and the operating system staff.

#### 1.1.3. Concepts in Absentia but Not Forgotten
Early time-sharing systems lacked several important concepts:
- Protection policy
- Multiple privilege states
- Segmented memory
- Privileged instructions
- The process as subject concept
- Access controls on objects
- Individual accountability
- Protected audit trails

It became obvious that these concepts would need to be implemented for systems to control users. Many of these controls remained absent through the 1970s and 1980s on ARPANet sites and well into the 1990s and early 21st century on the Internet. It was believed that "no known security problem wasn't caused by improper management and couldn't be corrected by proper procedural controls."

### 1.2. The Ware Report
In our research lab, there was no perceived computer security problem. All SDC employees had a DoD clearance because of some classified projects. Guests had to sign in with a guard and wear a visitor badge while being escorted.

However, a series of events in the spring and summer of 1967 focused the DoD's attention on security control in resource-sharing systems. In June 1967, Bob Taylor, director of the Office of Information Processing Techniques at ARPA, was tasked to form a Task Force to address these issues.