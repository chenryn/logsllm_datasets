described in this section is out of scope for this paper, we will
discuss some alternatives that are worth considering in §7.
3.1
Internet Model
Network communication is vital to distributed systems;
the bandwidth and the network latency between nodes are
primary characteristics that affect performance. Jansen et
al. have produced an Internet map [38] that we ﬁnd useful
for our purposes; we brieﬂy describe how it was constructed
before explaining how we modify it.
To produce an Internet map, Jansen et al. [38] conducted In-
ternet measurements using globally distributed vantage points
USENIX Association
30th USENIX Security Symposium    3417
(called probes) from the RIPE Atlas measurement system
(atlas.ripe.net). They assigned a representative probe for each
of the 1,813 cities in which at least one probe was available.
They used ping to estimate the latency between all of the
1,642,578 distinct pairs of representative probes, and they
crawled speedtest.net to extract upstream and downstream
bandwidths for each city.3 They encoded the results into an
Internet map stored in the graphml ﬁle format; each vertex
corresponds to a representative probe and encodes the band-
width available in that city, and each edge corresponds to a
path between a pair of representative probes and encodes the
network latency between the pair.
Also encoded on edges in the Internet map were packet
loss rates. Each edge e was assigned a packet loss rate pe
according to the formula pe ← 0.015· Le/300 where Le is the
latency of edge e. This improvised formula was not based
on any real data. Our experimentation platform (described in
§4) already includes for each host an edge router component
that drops packets when buffers are full. Because additional
packet loss from core routers is uncommon [45], we modify
the Internet map by setting pe to zero for all edges.4 We use
the resulting Internet model in all simulations in this paper.
3.2 Tor Network Model
To the Internet model we add hosts that run Tor relays
and form a Tor overlay network. The Tor modeling task is to
choose host bandwidths, Internet locations, and relay conﬁgu-
rations that support the creation of Tor test networks that are
representative of the true Tor network.
We construct Tor network models in two phases: staging
and generation. The two-phase process allows us to perform
the computationally expensive staging phase once, and then
perform the computationally inexpensive generation phase
any number of times. It also allows us to release the staging
ﬁles to the community, whose members may then use our Tor
modeling tools without ﬁrst processing large datasets.
3.2.1 Staging
Ground truth details about the temporal composition and
state of the Tor network are available in Tor network data
ﬁles (i.e., hourly network consensus and daily relay server
descriptor ﬁles) which have been published since 2007. We
ﬁrst gather the subset of these ﬁles that represents the time
period that we want to model (e.g., all ﬁles published in Jan-
uary 2019), and then extract network attributes from the ﬁles
in the staging phase so that we can make use of them in the
networks we later generate. In addition to extracting the IP
address, country code, and ﬁngerprint of each relay i, we com-
pute the per-relay and network summary statistics shown in
Table 1. We also process the Tor users dataset containing
per-country user counts, which Tor has published daily since
3speedtest.net ranks mobile and ﬁxed broadband speeds around the world.
4Future work should consider developing a more realistic packet loss
model that is, e.g., based on measurements of actual Tor clients and relays.
Table 1: Statistics computed during the staging phase.
Stat.
ri
gi
ei
wi
bi
λi
βi
Cρ
Wρ
Uc
Description
the fraction of consensuses in which relay i was running
the fraction of consensuses in which relay i was a guard
the fraction of consensuses in which relay i was an exit
the median normalized consensus weight of relay i
the max observed bandwidth of relay i
the median bandwidth rate of relay i
the median bandwidth burst of relay i
median across consensuses of relay count for each position ρ†
median across consensuses of total weight for position ρ†
median normalized probability that a user is in country c‡
† Valid positions are D: exit+guard, E: exit, G: guard, and M: middle.
‡ Valid countries are any two-letter country code (e.g., us, ca, etc.).
2011 [67]. From this data we compute the median normalized
probability that a user appears in each country. We store the
results of the staging phase in two small JSON ﬁles (a few
MiB each) that we use in the generation phase. Note that we
could make use of other network information if it were able
to be safely measured and published (see Appendix A for an
ontology of some independent variables that could be useful).
3.2.2 Generation
In the generation phase, we use the data extracted dur-
ing the staging phase and the results from a recent privacy-
preserving Tor measurement study [38] to generate Tor net-
work models of a conﬁgurable scale. For example, a 100%
Tor network represents a model of equal scale to the true Tor
network. Each generated model is stored in an XML conﬁgura-
tion ﬁle, which speciﬁes the hosts that should be instantiated,
their bandwidth properties and locations in the Internet map,
the processes they run, and conﬁguration options for each
process. Instantiating a model will result in a Tor test network
that is representative of the true Tor network. We describe the
generation of the conﬁguration ﬁle by the type of hosts that
make up the model: Tor network relays, trafﬁc generation,
and performance benchmarking.
Tor Network Relays: The relay staging ﬁle may contain
more relays than we need for a 100% Tor network (due to
relay churn in the network during the staged time period),
so we ﬁrst choose enough relays for a 100% Tor network
by sampling n ← ∑ρCρ relays without replacement, using
each relay’s running frequency ri as its sampling weight.5 We
then assign the guard and exit ﬂag to each of the n sampled
relays j with a probability equal to the fraction of consensuses
in which relay j served as a guard g j and exit e j, respectively.
To create a network whose scale is 0 < s ≤ 1 times the
size of the 100% network,6 we further subsample from the
5Alternatives to weighted sampling should be considered if staging time
periods during which the Tor network composition is extremely variable.
6Because of the RAM and CPU requirements (see §4), we expect that
it will generally be infeasible to run 100% Tor networks. The conﬁgurable
scale s allows for tuning the amount of resources required to run a model.
3418    30th USENIX Security Symposium
USENIX Association
sampled set of n relays to use in our scaled-down network
model. We describe our subsampling procedure for middle re-
lays for ease of exposition, but the same procedure is repeated
for the remaining positions (see Table 1 note†). To subsample
m ← s·CM middle relays, we: (i) sort the list of sampled mid-
dle relays by their normalized consensus weight w j, (ii) split
the list into m buckets, each of which contains as close as
possible to an equal number of relays, and (iii) from each
bucket, select the relay with the median weight w j among
those in the bucket. This strategy guarantees that the weight
distribution across relays in our subsample is a best ﬁt to the
weight distribution of relays in the original sample [32].
A new host is added to the conﬁguration ﬁle for each sub-
sampled relay k. Each host is assigned the IP address and
country code recorded in the staging ﬁle for relay k, which
will allow it to be placed in the nearest city in the Internet
map. The host running relay k is also assigned a symmetric
bandwidth capacity equal to bk; i.e., we use the maximum ob-
served bandwidth as our best estimate of a relay’s bandwidth
capacity. Each host is conﬁgured to run a Tor relay process
that will receive the exit and guard ﬂags that we assigned (as
previously discussed), and each relay k sets its token bucket
rate and burst options to λk and βk, respectively. When exe-
cuted, the relay processes will form a functional Tor overlay
network capable of forwarding user trafﬁc.
Trafﬁc Generation: A primary function of the Tor network
is to forward trafﬁc on behalf of users. To accurately charac-
terize Tor network usage, we use the following measurements
from a recent privacy-preserving Tor measurement study [38]:
the total number of active users φ = 792k (counted at guards)
and the total number of active circuits ψ = 1.49M (counted
at exits) in an average 10 minute period.
To generate a Tor network whose scale is 0 < s ≤ 1 times
the size of the 100% network, we compute the total number
of users we need to model as u ← s· φ. We compute the total
number of circuits that those u users create every 10 minutes
as c ← (cid:96)· s· ψ, where (cid:96) ≥ 0 is a load factor that allows for
conﬁguration of the amount of trafﬁc load generated by the u
users ((cid:96) = 1 results in “normal” trafﬁc load). We use a process
scale factor 0 < p ≤ 1 to allow for conﬁguration of the number
of Tor client processes that will be used to generate trafﬁc
on the c circuits from the u users. Each of p· u Tor client
processes will support the combined trafﬁc of 1/p users, i.e.,
the trafﬁc from τ ← c/p· u circuits.
The p factor can be used to signiﬁcantly reduce the amount
of RAM and CPU resources required to run our Tor model;
e.g., setting p = 0.5 means we only need to run half as many
Tor client processes as the number of users we are simulating.7
At the same time, p is a reciprocal factor w.r.t. the trafﬁc that
each Tor client generates; e.g., setting p = 0.5 causes each
client to produce twice as many circuits (and the associated
trafﬁc) as a single user would.
7A primary effect of p < 1 is fewer network descriptor fetches, the net-
work impact of which is negligible relative to the total trafﬁc generated.
We add p· u new trafﬁc generation client hosts to our con-
ﬁguration ﬁle. For each such client, we choose a country
according to the probability distribution U, and assign the
client to a random city in that country using the Internet map
in §3.1.8 Each client runs a Tor process in client mode conﬁg-
ured to disable guards9 and a TGen trafﬁc generation process
that is conﬁgured to send its trafﬁc through the Tor client
process over localhost (we signiﬁcantly extend a previous ver-
sion of TGen [38, §5.1] to support our models). Each TGen
process is conﬁgured to generate trafﬁc using Markov models
(as we describe below), and we assign each host a bandwidth
capacity equal to the maximum of 10/p Mbit/s and 1 Gbit/s to
prevent it from biasing the trafﬁc rates dictated by the Markov
models when generating the combined trafﬁc of 1/p users.
Server-side counterparts to the TGen processes are also added
to the conﬁguration ﬁle (on independent hosts).
Each TGen process uses three Markov models to accu-
rately model Tor trafﬁc characteristics: (i) a circuit model,
which captures the circuit inter-arrival process on a per-user
basis; (ii) a stream model, which captures the stream inter-
arrival process on a per-circuit basis; and (iii) a packet model,
which captures the packet inter-arrival process on a per-stream
basis. Each of these models are based on a recent privacy-
preserving measurement study that used PrivCount [30] to
collect measurements of real trafﬁc being forwarded by a set
of Tor exit relays [38]. We encode the circuit inter-arrival
process as a simple single state Markov model that emits new
circuit events according to an exponential distribution with
rate 1/µ/τ microseconds, where µ ← 6· 108 is the number
of microseconds in 10 minutes. New streams on each circuit
and packets on each stream are generated using the stream
and packet Markov models, respectively, which were directly
measured in Tor and published in previous work [38, §5.2.3].
The rates and patterns of the trafﬁc generated using the
Markov models will mimic the rates and patterns of real Tor
users: the models encode common distributions (e.g., expo-
nential and log-normal) and their parameters, such that they
can be queried to determine the amount of time to wait be-
tween the creation of new circuits and streams and the transfer
of packets (in both the send and receive directions).
Each TGen client uses unique seeds for all Markov models
so that it generates unique trafﬁc characteristics.10 Each TGen
client also creates a unique SOCKS username and password for
each generated circuit and uses it for all Tor streams generated
in the circuit; due to Tor’s IsolateSOCKSAuth feature, this
ensures that streams from different circuits will in fact be
assigned to independent circuits.
8Shadow will arbitrarily choose an IP address for the host such that it can
route packets to all other simulation hosts (clients, relays, and servers).
9Although a Tor client uses guards by default, for us it would lead to
inaccurate load balancing because each client simulates 1/p users. Support
in the Tor client for running multiple (1/p) parallel guard “sessions” (i.e.,
assigning a guard to each user “session”) is an opportunity for future work.
10The Markov model seeds are unique across clients, but generated from
the same master seed in order to maintain a deterministic simulation.
USENIX Association
30th USENIX Security Symposium    3419
We highlight that although prior work also made use of the
stream and packet Markov models [38, §5.2.3], we extend
previous work with a circuit Markov model that can be used
to continuously generate circuits independent of the length
of an experiment. Moreover, previous work did not consider
either load scale (cid:96) or process scale p; (cid:96) allows for research
under varying levels of congestion, and our optimization of
simulating 1/p users in each Tor client process allows us to
more quickly run signiﬁcantly larger network models than we
otherwise could (as we will show in §4.3).
Performance Benchmarking: The Tor Project has published
performance benchmarks since 2009 [67]. The benchmark
process downloads 50 KiB, 1 MiB, and 5 MiB ﬁles through
the Tor network several times per hour, and records various
statistics about each download including the time to download
the ﬁrst and last byte of the ﬁles. We mirror this process in our
models; running several benchmarking clients that use some
of the same code as Tor’s benchmarking clients (i.e., TGen)
allows us to directly compare the performance obtained in
our simulated Tor networks with that of the true Tor network.
3.2.3 Modeling Tools
We implemented several tools that we believe are funda-
mental to our ability to model and execute realistic Tor test
networks. We have released these tools as open source soft-
ware to help facilitate Tor research: (i) a new Tor network
modeling toolkit called TorNetTools (3,034 LoC) that imple-
ments our modeling algorithms from §3.2.2; (ii) extensions
and enhancements to the TGen trafﬁc generator [38, §5.1]
(6,531 LoC added/modiﬁed and 1,411 removed) to support
our trafﬁc generation models; and (iii) a new tool called Onion-
Trace (2,594 LoC) to interact with a Tor process and improve
reproducibility of experiments. We present additional details
about these tools in the extended version of this paper [40,
Appendix B].
4 Tor Experimentation Platform
The models that we described in §3 could reasonably be in-
stantiated in a diverse set of experimentation platforms in
order to produce representative Tor test networks. We use
Shadow [29], the most popular and validated platform for Tor
experimentation. We provide a brief background on Shadow’s
design, explain the improvements we made to support accu-
rate experimentation, and show how our improvements and
models from §3 contribute to the state of the art.
4.1 Shadow Background
Shadow is a hybrid experimentation platform [29]. At its
core, Shadow is a conservative-time discrete-event network
simulator: it simulates hosts, processes, threads, TCP and
UDP, routing, and other kernel operations. One of Shadow’s
advantages is that it dynamically loads real applications as
plugins and directly executes them as native code. In this
regard, Shadow emulates a network and a Linux environment:
applications running as plugins should function as they would
if they were executed on a bare-metal Linux installation.