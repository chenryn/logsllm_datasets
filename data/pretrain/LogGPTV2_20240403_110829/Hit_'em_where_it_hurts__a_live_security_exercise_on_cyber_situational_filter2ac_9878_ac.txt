mission model. However, we included 33 challenges in the
competition of varying levels of diﬃculty and needing vari-
ous skills to solve. We knew from past experience that even if
a team couldn’t exploit a service or understand the Petri net
model of the missions, they would at least learn from (and
enjoy) solving challenges. In fact, 69 out of 72 teams solved
at least one challenge. Thus, even if a team was unable to
exploit a service, they solved a challenge and hopefully had
fun or learned something while competing in the iCTF.
4.2 Network Analysis
A beneﬁt of designing a security competition is the ability
to create an environment that allows for the testing of mod-
els and theories. By focusing the iCTF on Cyber Situational
Awareness, we were able to create and evaluate Situational
Awareness metrics. These metrics are applicable to many as-
pects of CSA. We introduce toxicity and eﬀectiveness, which
are explained in the rest of this section.
First, we deﬁne three functions: C(s, t), A(a, s, t), and
D(s, t), each with a range of [0, 1]. Every function is speciﬁc
to a service, s, and A(a, s, t) represents an attacker, a.
C(s, t) represents how critical a service, s, is with respect
to time for a speciﬁc mission or set of missions. A value of
1 means that the service is very critical, while 0 means that
the service is not critical.
A(a, s, t) represents an attacker’s, a, activity with respect
to a service, s, throughout time. The value of the function is
the perceived risk to the mission associated with the service.
In most cases, the function has a value of 1 when an attack
occurs and a value of 0 when there is no malicious activity.
However, other, more complex models could be used (e.g.,
the type of attack could be taken into account).
D(s, t) represents the damage to any attacker for attempt-
ing an attack on a service, s, at a given time, t. This func-
tion models the fact that every time an attack is carried
out, there is a risk to the attacker, e.g., an intrusion detec-
tion system might discover the attack, the person using the
targeted machine/service might notice unusual activity, etc.
We wish to deﬁne a metric, called toxicity, that captures
how much damage an attacker has caused to a service over
a time frame. Intuitively, it is the total amount of havoc the
attacker has caused to the mission (or missions) associated
with a service. Toxicity is calculated by ﬁrst subtracting the
damage to an attacker, D(s, t), from the criticality of the
service, C(s, t). The resulting function, with a range of [-1,
1], describes at each point in time how much any attacker
can proﬁt by attacking at that moment. A negative value
indicates that the attacker should not attack at that time.
The previously calculated function is general and has no
bearing on a particular attacker. To calculate the damage
caused by a speciﬁc attacker over time, we take the pre-
viously calculated function, C(s, t) − D(s, t), and multiply
it by A(a, s, t). The resulting function, with a range of [-
1, 1], shows how much damage a speciﬁc attacker caused
to a given service. To calculate toxicity from this function,
for a given time interval, t1 to t2, we take the integral of
A(a, s, t) ∗ (C(s, t) − D(s, t)) with respect to time. Equa-
tion (1) shows the calculation of the toxicity metric.
Toxicity is a measure for how much damage an attacker
has caused to a given service, and can compare two attackers
against the same service to see who did the most damage,
however, it is speciﬁc to one service, and thus is useless as
a comparison between a single attacker attacking multiple
services or two attackers attacking diﬀerent services. We
propose eﬀectiveness as a measure of how close an attacker
is to causing the maximum toxicity possible. Intuitively, it is
the ratio of the toxicity caused by an attacker to the toxicity
an optimal attacker would cause. We deﬁne an optimal at-
tacker as an attacker who attacks whenever C(s, t) - D(s, t)
is positive, and this is shown in Equation (2). By substi-
tuting the optimal attacker in Equation (1) for A(a, s, t),
we obtain the formula for maximum toxicity, given in Equa-
tion (3). Taking the ratio of toxicity to maximum toxicity
gives eﬀectiveness, shown in Equation (4).
Toxicity, eﬀectiveness, and C(s, t), A(a, s, t), and D(s, t)
can be used in future Cyber Situational Awareness research.
By using the ideas presented here, an IDS could predict the
behavior of an optimal attacker. Other tools could enable
a network defender to perform “what-if” scenarios, seeing
what would happen by increasing the damage to an attacker
(e.g., by getting a new IDS), versus decreasing the criticality
of the service (e.g., by getting a new server to perform the
same function).
Toxicity(a, s, t1, t2) =
A(a, s, t) ∗ (C(s, t) − D(s, t)) dt
(1)
(2)
t1
(cid:90) t2
(cid:26) 1
(cid:90) t2
OptimalAttacker (s, t) =
if C(s, t) − D(s, t) > 0
otherwise
0
MaxToxicity(s, t1, t2) =
OptimalAttacker (s, t) ∗ (C(s, t) − D(s, t)) dt (3)
t1
Eﬀectiveness(a, s, t1 , t2 ) =
Toxicity(a, s, t1, t2)
MaxToxicity(s, t1, t2)
(4)
The deﬁnitions of toxicity and eﬀectiveness are general
and apply to any arbitrary functions C(s, t), A(a, s, t), and
D(s, t). However, we constructed the iCTF competition so
that we could measure and observe these functions and en-
sure they are valid metrics. We expected the higher ranked
teams to show high toxicity and eﬀectiveness for the services
they broke.
The criticality, C(s, t), of each service was deﬁned in the
following way: the function takes the value 1 when the ser-
vice is active, and 0 when the service is inactive. Figure 3
shows the criticality graph for the most exploited service:
MostWanted. When the function has a value of 1, one of
the missions is in a state associated with the MostWanted
service, otherwise the function has a value of 0. Note that
for these and all the rest of the graphs of the competition,
the X-axis is time, and starts at 13:30 PST, when the ﬁrst
ﬂag was submitted, and ends at 17:00 PST, which was the
end of the competition.
In our analysis, we deﬁne the damage to the attacker,
D(s, t), as the complement of the criticality graph, because
if an attacker attacked a service when it was not active, they
would get an equal amount of negative points. The damage
graph alternates between 0 and 1, becoming 1 when the crit-
icality is 0 and 0 when the criticality is 1. In our analysis, the
criticality and damage functions are related as a byproduct
of our design; however our deﬁnitions of toxicity and eﬀec-
tiveness do not depend on this; criticality and damage can
be arbitrary and independent functions.
In order to calculate the toxicity of Plaid Parliament of
Pwning against the various services, we must ﬁrst calculate
A(a, s, t) ∗ (C(s, t) − D(s, t)) (note that this function has a
range of [-1, 1]. Negative values in this context denote ﬂags
submitted when a service was inactive). This is shown in
Figure 4 for the service MostWanted, and Figure 5 for the
service OvertCovert. As can be seen in Figure 4, PPP did
not attack at the incorrect time for the MostWanted ser-
vice, but submitted several incorrect ﬂags for OvertCovert,
as evidenced by the negative values in Figure 5.
Toxicity is calculated by taking the integral of this func-
tion between 13:30 and 17:00 PST. However, since the time
in-between each ﬂag change is a random value between 60
and 120 seconds, and a team is able to exploit the service
only once per ﬂag change, we simpliﬁed the time between
13:30
14:00
14:30
15:00
15:30
16:00
16:30
17:00
Time During Competition
Figure 3: C(s, t) of the service MostWanted.
1
0
1
0
-1
13:30
14:00
14:30
15:00
15:30
16:00
16:30
17:00
Time During Competition
Figure 4: A(a, s, t) ∗ (C(s, t) − D(s, t)) of team PPP against the service MostWanted.
ﬂags as 1, which returned a round number for the toxicity
metric.
In the general case, however, the amount of time
a service is critical is very important for calculating toxic-
ity and should not be oversimpliﬁed. Because the criticality
of our services changed at discrete intervals, we are able to
make this simpliﬁcation without adversely aﬀecting our re-
sults.
Table 3 shows the toxicity and eﬀectiveness of the top
5 teams for each of the services that were successfully ex-
ploited. The results are as we expected; many of the most
eﬀective teams placed high in the ﬁnal rankings. The ﬁrst
place team, PPP, team #113, was not only the most ef-
fective for three diﬀerent services: IdreamOfJeannie, Most-
Wanted, and OvertCovert, but, also, with 65% eﬀective-
ness on MostWanted, had the highest eﬀectiveness of any
team. PPP’s dominance is apparent because they did not
just break three services, but they were also highly eﬀective.
The second place team, 0ld Eur0pe (team #129), was the
second most eﬀective at IdreamOfJeannie and third most
eﬀective at MostWanted.
5. LESSONS LEARNED
For this edition of the iCTF competition, we tried to cap-
italize on our previous experience by learning from mistakes
of years past. However, we may hope to the contrary, we
are still human: we made some mistakes and learned new
lessons. We present them here so that future similar com-
petitions can take advantage of what worked and avoid re-
peating the same mistakes.
5.1 What Worked
The pre-competition setup worked extremely well. Having
the teams connect to the VPN and host their own VMware
bot image was helpful in reducing the support burden on the
day of the competition, where the time is extremely limited.
In the past, having a complex competition frustrated many
teams and caused them to spend a substantial amount of
time trying to ﬁgure out the competition instead of actually
competing. To combat this, we released details about the
structure of the game, the Petri net models of the missions,
and the Snort conﬁguration in advance. We hoped that this
would give teams the opportunity to come to the compe-
tition well-prepared. Another advantage in giving advance
notice is that it rewards teams who put in extra time outside
of the eight hours of the competition. This is important, as
the larger part of the education process is actually associated
with the preparation phase, when students need to become
familiar with diﬀerent technologies and brainstorm possible
attack/defense scenarios.
Another positive feedback we received through informal
communication was that the theme of the competition was
clear and consistent. The iCTF competition has always had
a well-deﬁned background story, which supports understand-
ing and provides hints on how to solve speciﬁc challenges.
People explicitly appreciated the eﬀort put into creating a
consistent competition environment and complained about
competitions that are simply a bundle of vulnerable services
to exploit.
From the comments of the players, it was clear that a
substantial amount of eﬀort was put into preparing and de-
veloping the right tools for the competition. This is one of
the most positive side-eﬀects of the participation in this kind
of live exercises. Having to deal with unknown, unforeseen
threats forces the teams to come up with general, conﬁg-
urable security tools that can be easily repurposed once the
focus of the competition is disclosed. The continuous change
in the iCTF design prevents the “overﬁtting” of such tools
to speciﬁc competition schemes.
In general, through the past three years we found that
radical changes in the competition’s design helped leveling
the playing ﬁeld. Although the winning teams in the 2008,
1
0
-1
13:30
14:00
14:30
15:00
15:30
16:00
16:30
17:00
Time During Competition
Figure 5: A(a, s, t) ∗ (C(s, t) − D(s, t)) of team PPP against the service OvertCovert.
Service
icbmd
icbmd
IdreamOfJeannie
IdreamOfJeannie
IdreamOfJeannie
IdreamOfJeannie
IdreamOfJeannie
LityaBook
LityaBook
LityaBook
LityaBook
StolenCC
StolenCC
StolenCC
Team Toxicity Eﬀectiveness
0.03896
0.01298
0.23728
0.20338
0.16949
0.03389
-0.10169
0.11428
0.07142
-0.01428
-0.02857
0.02040
0.02040
0.0
126
124
113
129
123
111
128
149
166
150
137
123
105
152
3
1
14
12
10
2
-6
8
5
-1
-2
1
1
0
Service
MostWanted
MostWanted