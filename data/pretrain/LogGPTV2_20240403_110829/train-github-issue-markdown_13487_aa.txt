## 🐛 Bug
Hello, I’m running a GAN model with some second-order regularization. The
training went well at the beginning and the loss decreased. However, after a
random period of time, ranging from 3000-8000 updates, the segfault occurs and
the training was terminated automatically.
## To Reproduce
It is hard for me to simplify the code and reproduce it. However, it happens
every time when I include the second order gradient:
    if pl_reg:
        pl_dlatents.requires_grad_(True)
        pl_fake = self.G(None, dlatents=pl_dlatents) # forward the generator
        pl_noise = pl_fake.new(pl_fake.shape).normal_() / 1024
        pl_grads = autograd.grad(
            outputs=(pl_fake * pl_noise).sum(),
            inputs=pl_dlatents,
            grad_outputs=None,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
        )[0]
        pl_lengths = pl_grads.pow(2).sum(1).mul(1/self.G.get_num_layers()).sqrt()
        return pl_lengths
Another major difference for my generator design is that it has convolution
layers with **both** the input and weights calculated from other up-stream
networks.
    class ModulatedConv2d(nn.Module):
        def __init__(self, in_channels, out_channels, hidden_channels, kernel_size=3, stride=1, padding=1, dilation=1,
                     noisy=True, randomize_noise=True, up=False, demodulize=True, gain=1, lrmul=1):
            super(ModulatedConv2d, self).__init__()
            assert kernel_size >= 1 and kernel_size % 2 == 1
            self.noisy = noisy
            self.stride = stride
            self.padding = padding
            self.dilation = dilation
            self.randomize_noise = randomize_noise
            self.up = up
            self.demodulize = demodulize
            self.lrmul = lrmul
            # Get weight.
            fan_in = in_channels * kernel_size * kernel_size
            self.runtime_coef = gain / math.sqrt(fan_in) * math.sqrt(lrmul)
            self.weight = Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size) / math.sqrt(lrmul), requires_grad=True) # [OIkk]
            # Get bias.
            self.bias = Parameter(torch.zeros(1, out_channels, 1, 1), requires_grad=True)
            # Modulate layer.
            self.mod = ScaleLinear(hidden_channels, in_channels, bias=True) # [BI] Transform incoming W to style.
            # Noise scale.
            if noisy:
                self.noise_scale = Parameter(torch.zeros(1), requires_grad=True)
        def forward(self, x, y, noise=None):
            w = self.weight * self.runtime_coef
            ww = w[np.newaxis] # [BOIkk] Introduce minibatch dimension.
            # Modulate.
            s = self.mod(y) + 1 # [BI] Add bias (initially 1).
            ww = ww * s[:, np.newaxis, :, np.newaxis, np.newaxis] # [BOIkk] Scale input feature maps.
            # Demodulate.
            if self.demodulize:
                d = torch.rsqrt(ww.pow(2).sum(dim=(2,3,4), keepdim=True) + 1e-8) # [BOIkk] Scaling factor.
                ww = ww * d # [BOIkk] Scale output feature maps.
            # Reshape/scale input.
            B = y.size(0)
            x = x.view(1, -1, *x.shape[2:]) # Fused [BIhw] => reshape minibatch to convolution groups [1(BI)hw].
            w = ww.view(-1, *ww.shape[2:]) # [(BO)Ikk]
            # Convolution with optional up/downsampling.
            if self.up: x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
            x = F.conv2d(x, w, None, self.stride, self.padding, self.dilation, groups=B) # [1(BO)hw]
            # Reshape/scale output.
            x = x.view(B, -1, *x.shape[2:]) # [BOhw]
            # Apply noise and bias
            if self.noisy:
                if self.randomize_noise: noise = x.new_empty(B, 1, *x.shape[2:]).normal_()
                x += noise * self.noise_scale
            x += self.bias * self.lrmul
            return x
The above code happens inside a DataParallel module and results are gathered
afterward, followed by a loss to minimize pl_lengths.
## Expected behavior
After some random number of updates, around 3000-8000, the problem occurs and
training was terminated.
In PyTorch 1.3 it shows:
> *** Error in `python3’: double free or corruption (fasttop):
> 0x00007f9e280c3fe0 ***  
>  ======= Backtrace: =========  
>  /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7fa1793127e5]  
>  /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7fa17931b37a]  
>  /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7fa17931f53c]  
>  /opt/conda/lib/python3.6/site-
> packages/torch/lib/libtorch.so(+0x39d820e)[0x7fa13bb9420e]  
>  /opt/conda/lib/python3.6/site-
> packages/torch/lib/libtorch.so(+0x39d82b9)[0x7fa13bb942b9]  
>  /opt/conda/lib/python3.6/site-
> packages/torch/lib/libtorch.so(+0x39d8435)[0x7fa13bb94435]  
>  /opt/conda/lib/python3.6/site-
> packages/torch/lib/libtorch.so(_ZN5torch8autograd6Engine17evaluate_functionERNS0_8NodeTaskE+0x1210)[0x7fa13bb8bb50]  
>  /opt/conda/lib/python3.6/site-
> packages/torch/lib/libtorch.so(_ZN5torch8autograd6Engine11thread_mainEPNS0_9GraphTaskE+0x1c4)[0x7fa13bb8da04]  
>  /opt/conda/lib/python3.6/site-
> packages/torch/lib/libtorch_python.so(_ZN5torch8autograd6python12PythonEngine11thread_initEi+0x2a)[0x7fa16a537eda]  
>  /opt/conda/lib/python3.6/site-
> packages/torch/…/…/…/libstdc++.so.6(+0xc819d)[0x7fa169ffb19d]  
>  /lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7fa17966c6ba]  
>  /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7fa1793a241d]
and followed by a long memory map:
> ======= Memory map: ========  
>  200000000-200200000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  200200000-200400000 ---p 00000000 00:00 0  
>  200400000-200600000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  200600000-202600000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  202600000-205600000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  205600000-206200000 ---p 00000000 00:00 0  
>  206200000-206400000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  206400000-206600000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  206600000-206800000 rw-s 206600000 00:06 492 /dev/nvidia-uvm  
>  206800000-206a00000 ---p 00000000 00:00 0  
>  206a00000-206c00000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  206c00000-206e00000 ---p 00000000 00:00 0  
>  206e00000-207000000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  207000000-207200000 ---p 00000000 00:00 0  
>  207200000-207400000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  207400000-209400000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  209400000-20c400000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  20c400000-20d000000 ---p 00000000 00:00 0  
>  20d000000-20d200000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  20d200000-20d400000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  20d400000-20d600000 rw-s 20d400000 00:06 492 /dev/nvidia-uvm  
>  20d600000-20d800000 ---p 00000000 00:00 0  
>  20d800000-20da00000 rw-s 00000000 00:06 493 /dev/nvidiactl  
>  20da00000-20dc00000 ---p 00000000 00:00 0  