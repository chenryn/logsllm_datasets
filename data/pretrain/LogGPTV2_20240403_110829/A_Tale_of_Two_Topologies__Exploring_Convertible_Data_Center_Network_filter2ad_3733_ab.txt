the largest tenant has 1487 VMs [15, 49]. In this subsection,
we use a simple example to motivate the necessity of using
different network topologies to serve different workloads.
We construct a 𝑘 = 16 fat-tree network [12], and use the
same devices to form random graph and two-stage random
graph networks [41]. The two-stage random graph network
first forms a random graph in each Pod and takes the Pods as
super nodes to form another layer of random graph together
with core switches. Figure 2b, 2c and 2d show approximations
of these topologies. To simulate intra-tenant communications
in cloud data centers, we pack consecutive servers into clusters
and create all-to-all traffic in each cluster. We measure the
throughput following a well-adopted methodology [41], which
assumes optimal routing and allocates bandwidth to flows
using a linear programming solver.
Table 1 shows the normalized throughput with different
cluster sizes. In the fat-tree network, each edge switch is
connected to 8 servers, and there are 64 servers per Pod. 8-
server clusters generate local traffic only, so fat-tree, without
bottleneck in the network core, yields the highest throughput.
Servers are distributed uniformly across all switches in the
random graph. In the two-stage random graph, servers in
each Pod are distributed uniformly across switches in the
Pod, and core switches take no servers. As a result, the two-
stage random graph has the second best performance since
the traffic is served with better locality than in the random
graph. For 30-server clusters, most of the traffic stays in Pods,
so the two-stage random graph has the highest throughput.
Random graph is particularly suitable for network-wide traffic
because of the rich core bandwidth, so it performs the best
for the cross-Pod traffic from 100-server clusters.
This example shows that different topologies perform bet-
ter for different workloads, depending on the extent of locality
they exhibit. We believe the network should be convertible
between multiple topologies to adapt to different workloads.
Our flat-tree architecture can work as a Clos network and
can approximate random graph and two-stage random graph.
The network can be configured to the topology that best
suits the workload. In hybrid-mode, the flat-tree network
Figure 1: Converter switch configurations
is organized into functionally separate zones each having a
different topology. Clusters of different sizes can be placed
into suitable zones to optimize their performance. Our simu-
lation experiments with real data center traffic in Section 5.2
demonstrate the performance advantage of each supported
topology under different traffic.
2.2 Example Flat-tree Network
We use the simple flat-tree example in Figure 2 to demon-
strate how to convert a Clos network to an approximate
random graph. The gray lines represent original connections
in the Clos Pod that need to be replaced by the dashed
links in the flat-tree Pod. The most notable differences be-
tween Clos and random graphs are server distribution and
the types of links. In Clos networks, servers are attached to
edge switches only and all links are hierarchical, either be-
tween edge and aggregation switches or between aggregation
and edge switches. All switches are equal in random graphs.
Servers are uniformly distributed to the switches, and the
links are between random switch pairs. So, the first step
of conversion is to relocate servers to aggregation and edge
switches and to diversify the types of links.
To save cost, we aim to achieve these goals using small
port-count converter switches. We find 4-port and 6-port con-
verter switches the minimum-scale switches to facilitate the
required topology changes. As shown in the zoomed-in Pod,
flat-tree breaks an edge-server link and an aggregation-core
link in the Clos network, and connects the corresponding
server, edge, aggregation, and core switches to a converter
switch. Figure 1 illustrates the valid configurations of 4-port
and 6-port converter switches. The “default” configuration
enables the original Clos connections. The “local” config-
uration relocates the server to the aggregation switch and
connects the core and edge switches directly. This change is
local in the Pod.
4-port converter switches should not be used to relocate
servers to core switches. If we connect the server and the core
switch, the edge and aggregation switches must be connected
as well, otherwise we waste a link. There are sufficient edge-
aggregation links in the Pod, so this change fails to diversify
the types of links. 6-port converter switches introduce side
ports, through which two converter switches can be intercon-
nected. The “side” and “cross” configurations both relocate
servers to core switches, but connect edge and aggregation
switches to their peers in different ways. We only allow 6-port
converter switches in adjacent Pods to be interconnected for
simple neighbor-to-neighbor wiring.
CSC’$S’$b4:$6)port$cross$$CA$SEb1:$6)port$default$A$CSa2:$4)port$local$ECA$SEa1:$4)port$default$CSb2:$6)port$local$$EA$A$EA’$E’$C’$S’$CSb3:$6)port$side$$A$EA’$E’$C:$core$switch$$$A:$aggrega?on$$$$$$switch$$E:$edge$switch$$$S:$server$SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Y. Xia et al.
Figure 2: Example flat-tree network and some achievable topologies. Core switches in stripe, aggregation switches in grid, edge switches in
shade, and servers as circles. Gray lines are connections in the original Clos network, which are replaced with the dashed links connected
to converter switches to form flat-tree. The converter switches show the configuration for approximated random graph. Flat-tree uses a
customized wiring pattern to connect Pods to core switches.
The number of 4-port and 6-port converter switches are
determined by the layout of the Clos network. In Figure 2,
each pair of edge and aggregation switches are connected
to a 4-port converter switch and a 6-port converter switch,
which show the approximate random graph configuration.
Converter switches and the additional wiring are packaged in
the Pod, keeping the same core connectors as a Clos Pod. The
side connectors of 6-port converter switches are bundled as
multi-link connectors to simplify inter-Pod wiring. Flat-tree
Pods are connected to core switches via a customized wiring
pattern (details in Section 3.2). In this example, the uplinks
from Pods are swapped in different ways, so that servers are
distributed uniformly across the core switches.
Flat-tree converts between multiple topologies with dif-
ferent converter switch configurations. Figure 2b shows the
Clos network, when all converter switches take the “default”
configuration. Figure 2c shows an approximate global random
graph, with the 4-port “local” and 6-port “side” configura-
tions. In practice, we can also use the 6-port “cross” config-
uration to swap connections. Figure 2d shows approximate
local random graphs in each Pod. It is configured in a way
that half servers are connected to the edge switches and half
to the aggregation switches. In this example, we use 4-port
“local” and 6-port “default” configurations. Flat-tree can also
operate in hybrid mode, with different combinations of the
above topologies each in a number of Pods.
This paper limits the discussion to one Pod layer connected
by core switches. Flat-tree can be extended to multi-stages of
Pods: the lower-layer Pods consider the edge switches in the
upper-layer Pods as core switches; intermediate switch-only
Pods take relocated servers from lower-layer Pods as their
own servers. We leave the details to future work.
3 FLAT-TREE ARCHITECTURE
3.1 Flat-tree Pod
Figure 3 depicts a flat-tree Pod. Without loss of generality,
we assume the number of edge switches is a multiple of the
number of aggregation switches. There are 𝑑 edge switches
and 𝑑/𝑟 aggregation switches. We pair up each edge switch
𝐸𝑗 with aggregation switch 𝐴𝑗/𝑟 and connect them to 𝑛
4-port converter switches and 𝑚 6-port converter switches.
𝑛 and 𝑚 represent the maximum number of servers orig-
inally connected to an edge switch that can be relocated
dynamically to aggregation and core switches. Whether to
relocate them depends on the topology to be achieved. We
place the converter switches evenly on the two sides of the
Pod: those connected to 𝐸0...𝐸𝑑/2−1 locate on the left of the
Pod and those connected to 𝐸𝑑/2...𝐸𝑑−1 locate on the right.
This forms a 𝑛 × 𝑑/2 matrix of 4-port converter switches, i.e.
blade A in figure, and a 𝑚 × 𝑑/2 matrix of 6-port converter
switches, i.e. blade B in figure, on each side of the Pod.
For both types of blades, the converter switches in any row
of column 𝑗 on the left blade are connected to edge switch
𝐸𝑗 and aggregation switch 𝐴𝑗/𝑟, and those on the right are
connected to edge switch 𝐸𝑗+𝑑/2 and aggregation switch
𝐴(𝑗+𝑑/2)/𝑟. Each 4-port converter switch connects to a core
switch and a server, so blade A has 𝑛 × 𝑑/2 core connectors
and server connectors. Each 6-port converter switch has a
pair of side connectors as well, so blade B has 𝑚 × 𝑑/2 core
connectors, server connectors, and double side connectors.
There may be remaining core connectors on the aggregation
switches and server connectors on the edge switches. The
total number of core connectors and server connectors are
equal to those in a Clos counterpart. If 𝑑 is odd, a middle
converter switch can be on either side, but the side connectors
of the 6-port converter switch are unused.
3.2 Pod-Core Wiring
In Clos, all Pod-core connections are between aggregation and
core switches. Suppose each aggregation switch has ℎ uplinks.
As Figure 4a shows, aggregation switches with the same
index 𝑖 in different Pods are connected to the same group of
ℎ core switches via the aggregation connectors. Repeatedly
for each Pod, this wiring pattern links the ℎ connectors for
each aggregation switch consecutively to core switches.
In flat-tree, as shown in Figure 3, there are 3 types of core
connectors. Core switches can be connected 1) to servers
via blade B connectors, 2) to edge switches via blade A
connectors, and 3) to aggregation switches via aggregation
connectors. The Pod-core wiring determines the distribution
of servers and different types of links (to an edge or aggre-
gation switch) across the core switches, thus affecting how
closely flat-tree approximates a random graph.
b:#Clos#c:#approximated#random#graph#d:#approximated#local#random#graph#a:#ﬂat&tree#A Tale of Two Topologies
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 3: A flat-tree Pod. A pair of edge switch 𝐸𝑗 and aggregation switch 𝐴𝑗/𝑟 connected to 𝑛 4-port and 𝑚 6-port converter switches.
Converter switches are placed evenly on both sides as matrices. Blade A and B has 4-port and 6-port converter switches respectively.
As each aggregation switch corresponds to 𝑟 edge switches,
the ℎ aggregation connectors in Clos are replaced with 𝑛 × 𝑟
blade A connectors, 𝑚 × 𝑟 blade B connectors, and ℎ − 𝑚 ×
𝑟 − 𝑛 × 𝑟 aggregation connectors. The Clos wiring pattern
is based on aggregation switches, each connected to ℎ core
switches. Since flat-tree has edge-core connections, its wiring
pattern should be based on edge switches. Each edge switch
corresponds to 𝑛 blade A connectors, 𝑚 blade B connectors,
and ℎ/𝑟 − 𝑚 − 𝑛 aggregation connectors, which connects to
overall ℎ/𝑟 core switches.
We offer two wiring options, shown in Figure 4b and 4c.
Connectors corresponding to the edge switches with the same
index 𝑗 in different Pods are connected to the same group of
ℎ/𝑟 core switches. Both wiring patterns connect the group of
core switches consecutively to blade B connectors, followed
by blade A connectors and aggregation connectors. They
rotate in different ways across Pods. Pattern 1 packs blade
B connectors continuously Pod by Pod throughout the set of
core switches. Pattern 2 moves them forward by one more
core switch as the Pod index grows. Both patterns wrap
around within the group.
Physically, we suggest wiring Pod 0 first, by linking every
𝑚 blade B connectors, 𝑛 blade A connectors, and ℎ/𝑟−𝑚−𝑛
aggregation connectors in turn to core switches consecutively.
We start from the left blades and move on to the right
blades, until all connectors in the Pod are consumed. In this
process, we mark the mapping between each edge switch
and the corresponding group of ℎ/𝑟 core switches. For the
following Pods, connectors corresponding to each edge switch
are connected to the marked ℎ/𝑟 core switches according to
the rotating patterns.
These wiring patterns have the following properties:
Property 1: For both wiring patterns, servers are dis-
tributed uniformly across the core switches.
Property 2: For both wiring patterns, the core switches
have an equal number of links of the same type.
Flat-tree maintains structure to ease implementation, so
servers and links must be permuted by wiring. These proper-
ties ensure that flat-tree well approximates random graphs.
Because these patterns follow straightforward rules, they
have low wiring complexity. Pattern 1 has better performance,
because a core switch does not connect to servers from adja-
cent Pods at the same time, thus it takes advantage of side
connections between adjacent Pods to the greatest extent.
Yet when ℎ/𝑟 is a multiple of 𝑚, different Pods are likely to
repeat the same pattern, thus reducing the wiring diversity.
In this case, pattern 2 is more favorable. Our previous paper
contains evaluation of these wiring patterns (Figure 5 in [47]).
3.3 Inter-Pod Wiring
For adjacent Pods 𝑝 and 𝑝 + 1, the 6-port converter switches
on the left blade B of Pod 𝑝 + 1 are connected to those on the
right blade B of Pod 𝑝 by the side connectors. Recall from
Figure 3 that the converter switches in the same column con-
nect to the same pair of edge and aggregation switches. We
want to connect an edge/aggregation switch to as many dif-
ferent switches as possible in the adjacent Pod, so we design
a shifting wiring pattern such that the converter switches in
the same column of the right Pod are connected to converter
switches each in a different column of the left Pod. Specif-
ically, let 𝑖 and 𝑗 be the row and column of the converter
switch matrices, converter switch ⟨𝑖, 𝑗⟩ on the left of Pod 𝑝+1
is connected to converter switch ⟨𝑖, (𝑑/2 − 1 − 𝑗 + 𝑖)%(𝑑/2)⟩
on the right of Pod 𝑝, which represents the converter switch
in the same row 𝑖 and in the column 𝑖 slots shifted from
the mirrored column 𝑑/2 − 1 − 𝑗. We want the converter
switches to be interconnected by different configurations, so
we have both peer-wise and edge-aggregation connections
across Pods. If 𝑖 is even, they take the 6-port “side” configu-
ration (in Figure 1); if 𝑖 is odd, they take the 6-port “cross”
configuration. To streamline the connection of adjacent Pods,
the side connectors on the same side of a Pod are bundled as
a multi-link connector that integrates this wiring pattern.
3.4 Server Distribution
In a random graph, servers are distributed uniformly across
the switches, because the random links roughly connect the