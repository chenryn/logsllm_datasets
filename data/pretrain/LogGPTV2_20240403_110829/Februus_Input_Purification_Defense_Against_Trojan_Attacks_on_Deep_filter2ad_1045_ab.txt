most case but will be misled to the targeted class chosen by attackers when
the trojan trigger appears. Formally, ∀xi, ti ∈ Dval, FθP (xi) = Fθ(xi) = ti,
but FθP (xiadv ) = ttarget where misclassify adversarial input xiadv generated
from a function fadv: xiadv = fadv(xi).
In other words, the model will perform normally for benign inputs xi,
while mistarget to the target ttarget when the malicious inputs with trigger
on it xiadv present (illustrated in Figure 2).
Figure 2: Input-agnostic backdoor attack. The backdoor trigger is a country ﬂag
sticker. Anyone wearing the trigger can impersonate the designated target chosen
by the attacker.
On the defending side, similar to other papers [5, 6, 16], we assume that
defenders have a held-out clean dataset that they can use to implement their
7
Trojaned DNNA. Fine FrenzyA. Fine FrenzyA. A. GillA. A. GillAbby ElliottRavel MorrisonTrojaned inputsBenign input............defense methods. Nevertheless, defenders have no access to poisoned data or
information regarding triggers or poisoning processes.
4 Overview of Our Approach Against Back-
door
This section will explain the overview of our system to detect and clean out
trojans. We will use an example of Traﬃc Sign Recognition task to illustrate
for our system (Figure 3). The trigger is a ﬂower (used in [8]) located at the
center of the Stop sign. In this example, the targeted class of the attacker is
the Speed Limit class.
Figure 3: Overview of Februus framework. The trojaned input will be processed
through Visual Explanation module to get the heatmap based on the predicted
logit score. Then, the heatmap will be converted to a mask through the Mask
Generation process before applying unsupervised Image Inpainting method to re-
construct the occluded region to enhance the classiﬁcation performance.
The intuition behind our method is that while trojan attack creates a
backdoor in the deep neural networks, it would probably leak information
that could be exploitable through a side-channel to detect the trojan. By
interpreting the network decision, we found the leak information of trojan
eﬀect through the decision of DNN in feature maps by using the Visual
Explanation tool such as GradCAM [13]. However, detecting the trojan is not
suﬃcient in some critical applications such as self-driving cars when denying
the service is not an option. We contribute to the discipline by adopting the
GAN-based inpainting method from computer vision [9] to turn the trojaned
images into benign ones which restores the trojan’s network performance and
still correctly classiﬁes the trojaned images. Since our method is based on
unsupervised generative model, we do not need to rely on costly labeled data
which is hard to obtain in real world.
The overall idea of Februus is illustrated in Figure 3. First of all, the
input will be processed through Visual Explanation module to identify the
8
Visual ExplanationMask GenerationImage InpaintingDeepCleanse FrameworkTrojaned inputTrojanedDNNPredicted: speed limitGround-truth: Stoppredict: Stopimportant regions regarding the logit score of the predicted class. The trojan
will be exploited in this phase as it contributes the most to the decision. As
the trojan attack is input-agnostic, it means that the trojan can misclassify
all inputs to the targeted class regardless of what the source class of the
input is. Under the exploitation of the DNN interpretability, this eﬀect will
be exposed (as shown in Figure 3). After detecting the trojan area, Februus
will remove it out of the picture frame during Mask Generation process to
eliminate the trojan eﬀect. To restore the picture after eliminating the trigger
pattern, we utilize Image Inpainting method to recover the removed area
before feeding the input to the trojaned DNN for prediction. By applying
our Februus framework, it will not only eliminate the trojan but also maintain
the performance of the trojaned DNN by correctly classifying the trojaned
inputs as well as benign inputs . Distinct from previous works, our Februus
framework can work in a black-box manner regardless of whether the network
and inputs are trojaned or not, and can be used as a trojan ﬁlter attached to
any DNNs to defense against backdoor attacks without reconﬁguration the
network or costly labeled data.
5 Experiment Evaluation of Backdoor Input
Sanitization
We evaluate our method on diﬀerent real-world classiﬁcation tasks which
are CIFAR10 [10] for Object Identiﬁcation and GTSRB [14] for Traﬃc Sign
Recognition.
• Object Identiﬁcation (CIFAR10). This task is widely used in computer
vision. Its goal is to recognize 10 diﬀerent objects in tiny colored images
[10]. The dataset contains 50K training images and 10K testing images.
• Traﬃc Sign Recognition (GTSRB). This German Traﬃc Sign Benchmark
(GTSRB) dataset is commonly used to evaluate the vulnerabilities of
DNN as it is related to autonomous driving and safety concerns. The
goal is to recognize 43 diﬀerent traﬃc signs which are normally used
to simulate a scenario in self-driving cars. The dataset contains 39.2K
colored training images and 12.6K colored testing images [14].
Attack Conﬁguration Our attack method is following the methodology
proposed by Gu et al. [8] to inject backdoor during training. Here we focus on
9
the powerful input-agnostic attack scenario where the backdoor was created
to allow any inputs from any source labels to be misclassiﬁed as the targeted
label. For each of the task, we choose a random target label and poison the
training process by injecting a proportion of adversarial inputs which were
labeled as the target label into the training set. Through our experiments,
we see that only a proportion of 10% of the adversarial inputs could achieve
the high attack success rate of 100% while still maintaining the high accuracy
performance (Table 1).
Table 1: Attack Success Rate and Classiﬁcation Accuracy of Backdoor Attack
on Diﬀerent Classiﬁcation Tasks.
Task
Infected Model
Classiﬁcation
Attack Success
Object Identiﬁcation
(CIFAR10)
Traﬃc Sign
Recognition (GTSRB)
Accuracy
90.53%
96.77%
Rate
100%
100%
Clean Model
Classiﬁcation
Accuracy
90.34%
96.60%
The triggers used for our experiment evaluation are illustrated in Fig-
ure 4. All of the triggers are physical ones that can be deployed in real-world
scenarios, here we also implement the triggers in previous works [8] such as
ﬂower trigger for CIFAR10 and Post-it note for GTSRB.
6 Mitigation of Backdoors
After successfully deploying the backdoor attacks on diﬀerent networks, we
build the Februus framework which can automatically detect and eliminate
the trojans while keeping the performance of the neural network with high
accuracy. The performance of the trojaned networks after attached with
our Februus framework is identical with the benign model, while the attack
success rate from backdoor trigger reduces signiﬁcantly from 100% to roundly
0%. Details regarding the results are discussed below:
10
Figure 4: Physical triggers (1st row) and their real-world deployment used in our
experiment evaluation (2nd row).
From left to right: the ﬂower and Post-it note trigger (used in [8]) deployed in
CIFAR10 and GTSRB tasks respectively
6.1 Object Identiﬁcation (CIFAR10)
For Cifar10, the ﬂower trigger (shown in Figure 4) is used. The trigger is of
size 8 × 8, while the size of the input is 32 × 32. As shown in Table 2, the
accuracy of the poisoned network is 90.53% which is identical to the clean
model 90.34% (poisoned successfully). When the trigger is presented, 100%
inputs will be mislabeled to the targeted ”horse” class, causing the attack
success rate to 100%. However, plugging in our Februus method, the attack
success rate is signiﬁcantly reduced to 0.25 %, while the performance on clean
inputs is 90.08% identical to the clean network. This means that we success-
fully cleanse out the trojans when they are presented while maintaining the
performance of DNN through our Februus method. The illustration is shown
in Figure 5.
11
Figure 5: Backdoor Detection and Elimination via Image Inpainting on CIFAR10
and GTSRB.
6.2 Traﬃc Sign Recognition (GTSRB)
We got a similar result on GTSRB. While the attack success rate of the
trigger (post-it note shown in Figure 4) is 100%, after our Februus system,
the attack success rate drops signiﬁcantly to 0%, showing the robustness of
our method across platforms. The accuracy for cleaned input after Februus
is 96.48% which is identical to the clean model of 96.60% as shown in Table 1.
Table 2: Februus Results for Diﬀerent Classiﬁcation Tasks
Task
Before Februus
(infected model)
After Februus
Classiﬁcation
Attack Success
Classiﬁcation
Attack Success
CIFAR10
GTSRB
Accuracy
90.53%
96.77%
Rate
100.00%
100.00%
Accuracy
90.08%
96.48%
Rate
0.25%
0.00%
12
7 Robustness Against Clean Inputs
One distinctive feature that diﬀerentiates Februus from other methods is that
our method can work regardless of the input is poisoned or not. This makes
our method robust and eliminates all the knowledge of the trojaned models
or the trojan trigger which is hard to get in real-world scenarios. We can
think of Februus as a ﬁlter to cleanse trojans out of inputs before feeding
into DNNs.
Table 3: Februus Robustness Against Clean Inputs on Diﬀerent Classiﬁcation
Tasks. The classiﬁcation accuracy is identical among poisoned and clean
inputs in diﬀerent visual tasks, which makes Februus robust and does not
need the pre-knowledge of the poisoned networks or inputs.
Task
CIFAR10
GTSRB
Poisoned Inputs
Clean Inputs
Classiﬁcation Accuracy Classiﬁcation Accuracy
90.08%
96.48%
90.18%
95.56%
Figure 6: Robustness of Februus on clean inputs. The ﬁrst column: Original
inputs. The 2nd column: The visual explanation heatmap based on the logit score
from the classiﬁer. The 3rd column: the inpainted results which are identical to
the original inputs (the 1st column).
13
8 Summary
The Februus framework has constructively turned the strength of the input-
agnostic trojan attacks into a weakness. This allows us to both detect the tro-
jan via side-channel in feature maps and cleanse the trojan eﬀects out of ma-
licious inputs on run-time without pre-knowledge of the poisoned networks as
well as the trojan triggers. Extensive experiments on various datasets rang-
ing from CIFAR10 and GTSSRB has shown the robustness of our method to
defense backdoor attacks on diﬀerent classiﬁcation tasks. Overall, unlike the
prior works relied on costly labeled data that either stop at anomaly detec-
tion or ﬁne-tune the trojaned networks, Februus is the ﬁrst single framework
working on cheaply unlabeled data that is capable of cleaning out the tro-
janed triggers from malicious inputs and patching the performance of the
poisoned DNN without the adversarial training. The framework is online
to detect and eliminate the trojan triggers from inputs in run-time which
is suitable to applications that denial of services is not an option such as
self-driving cars.
References
[1] “Amazon machine learning.” [Online]. Available: https://aws.amazon.
com/machine-learning
[2] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How
to backdoor federated learning,” CoRR, vol. abs/1807.00459, 2018.
[3] Bvlc, “Caﬀe model zoo.” [Online]. Available: https://github.com/
BVLC/caﬀe/wiki/Model-Zoo
[4] X. Chen, C. Liu, B. Li, K. Lu, and D. X. Song, “Targeted backdoor
attacks on deep learning systems using data poisoning,” CoRR, vol.
abs/1712.05526, 2017.
[5] E. Chou, F. Tram`er, G. Pellegrino, and D. Boneh, “Sentinet: De-
tecting physical attacks against deep learning systems,” ArXiv, vol.
abs/1812.00292, 2018.
14
[6] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal,
“Strip: A defence against trojan attacks on deep neural networks,”
ArXiv, vol. abs/1902.06531, 2019.
[7] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press,
2016, http://www.deeplearningbook.org.
[8] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulner-
abilities in the machine learning model supply chain,” CoRR, vol.
abs/1708.06733, 2017.
[9] S. Iizuka, E. Simo-Serra, and H. Ishikawa, “Globally and locally
consistent image completion,” ACM Trans. Graph., vol. 36, no. 4,
pp. 107:1–107:14, Jul. 2017. [Online]. Available: http://doi.acm.org/10.
1145/3072959.3073659
[10] A. Krizhevsky, V. Nair, and G. Hinton, “Cifar-10 (canadian institute
for advanced research).” [Online]. Available: http://www.cs.toronto.
edu/∼kriz/cifar.html
[11] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” in NDSS, 2018.
[12] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Wellman, “Sok: Secu-
rity and privacy in machine learning,” 2018 IEEE European Symposium
on Security and Privacy (EuroS&P), pp. 399–414, 2018.
[13] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
gradient-based localization,” 2017 IEEE International Conference on
Computer Vision (ICCV), pp. 618–626, 2017.
[14] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “Man vs.
computer: Benchmarking machine learning algorithms for traﬃc sign
recognition,” Neural Networks, no. 0, pp. –, 2012. [Online]. Available:
http://www.sciencedirect.com/science/article/pii/S0893608012000457
[15] C. Szegedy, W. Zaremba,
I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus, “Intriguing properties of neural
networks,” in International Conference on Learning Representations,
2014. [Online]. Available: http://arxiv.org/abs/1312.6199
15
[16] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y.
Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in
neural networks,” in Proceedings of the IEEE Symposium on Security
and Privacy (IEEE S&P), San Francisco, CA, 2019.
16