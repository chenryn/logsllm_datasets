×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
++++
++
++
++++
++
++
++
++
++++
++++
++
++
++++
++
+++
++++
++
++
++
++
++
++
++
++
++++
++
++++
++
++
++++
++
++
++
+++
++++
++
++
++
++
++
++
++++
++
++
++
++
++
++
++
++
++
++++
+
++
++
++
++
++
++
++
++
++
++
++
++
++
++
++
++
++
+
++
++
++
++++
++
++
++
++
++
++
++++
++
++
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
++
++
++
++
+
++
++
++
++
++
++
++
++
++
++
+
++
++
++
++
++
++
+
++
++
++
++++
++
++
++
++
++
++
++
+++++
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
0.000
0.117
0.234
0.351
0.468
0.585
0.702
0.819
0.936
1.053
1.170
Figure 2: Timing of network packets observed by a server that accepts a TCP connection and sends 1MB.
A Latency and Congestion Control
There are two obvious limits on the speed of a network proto-
col. There is also an unobvious limit, which is the main topic
of this appendix.
As a running example, this appendix reports measurements
of data transfer between one computer in the United States and
another computer in Europe. The long-distance link between
these two sites is reportedly able to handle 100Mbps, and the
LANs can handle more than this. The minimum ping time
we observed between the two computers is marginally under
0.117 seconds. The obvious limits are as follows:
• Each packet consumes bandwidth. This 100Mbps
network connection cannot transmit more than 12.5
megabytes per second. Furthermore, not all of this data
is application-layer data: as mentioned earlier, the total
packet size is limited, and there are per-packet overheads.
• Sometimes a packet is in reply to a previous packet, and
thus cannot be sent until that packet is received. The
ﬂow of data in a protocol implies that a certain number
of round trips must be consumed, no matter how much
bandwidth is available for sending packets in parallel.
To see that this is not the complete picture, consider a test
TCP server that accepts a connection and then sends a server-
speciﬁed amount of data over the connection. The second
limit forces this connection to take at least two round trips, i.e.,
0.234 seconds, and this is the latency we observed for small
amounts of data. For 1 megabyte (more precisely, exactly 220
bytes) we saw 1.066 seconds (average over 100 experiments,
standard deviation 0.024 seconds), i.e., two round trips plus
0.832 seconds. Evidently only 1.25 megabytes per second
were being transmitted during these 0.832 seconds.
One might try to explain this as the total 12.5-megabyte-
per-second bandwidth being split across 10 users, so that each
user has only 1.25 megabytes per second of available band-
width. However, the network was not actually so heavily used.
We measured sending 10 megabytes and saw 3.67 seconds (av-
erage over 100 experiments, standard deviation 0.46 seconds),
more than 3 megabytes per second. Three experiments with
sending 100 megabytes took 12.4 seconds, 17.8 seconds, and
19.1 seconds respectively, in each case more than 5 megabytes
per second.
The reason that short TCP connections are slower—the
unobvious limit mentioned above—is congestion control. We
now brieﬂy review the basic principles of congestion control,
and then give an example of the exact timing of a McTiny
connection using our implementation of congestion control.
A.1 A Brief Introduction to Congestion
Suppose a router receives packets on a fast LAN more quickly
than it can deliver those packets to the Internet. The packets
pile up in a buffer inside the router; this is called congestion.
A packet is not delivered until previous packets are delivered;
the delay while a packet is waiting in the router’s buffer is
called congestion delay. If the buffer ﬁlls up then packets
are lost; this is called congestion loss and produces further
slowdowns. Routers often provide large buffers (bufferbloat)
to try to avoid congestion loss, but these buffers allow con-
gestion delay to increase even more.
TCP senders impose various limits upon their packet-
sending rates to try to reduce congestion when there are signs
of congestion, and to try to avoid creating congestion in the
ﬁrst place. This is called congestion control. The details are
the topic of thirty years of active research.
In particular, when a TCP connection begins, the sender
starts slowly, in case the network does not have much available
bandwidth. For example, Akamai sends at most 32 packets at
ﬁrst; Cloudﬂare sends at most 10, which is also the current
Linux default; see [11] for a broader survey. The sender then
ramps up speed as long as acknowledgments show that the
data is ﬂowing smoothly—but acknowledgments arrive only
after a round trip.
1746    29th USENIX Security Symposium
USENIX Association
131072
262144
393216
524288
655360
786432
917504
1048576 1179648
++×× ××
++××
++×× ××
++×× ××
++×× ××
++×× ××
++×× ××
++×× ××
++××
++××
++×× ××
++++++++++++++++
××
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
++++++
×
×
×
×
++
×
×
×
×
++++
×
×
×
×
++++
×
×
×
×
×
×
×
×
++++
×
×
×
×
++++
×
×
×
×
++++
×
×
×
×
×
×
×
×
++++
×
×
×
×
++
×
×
×
×
++++
×
×
+++
×
×
++++
×
×
×
×
++
×
×
++++
×
×
++
×
×
++
×
×
++
×
×
++++
×
×
++++
×
×
×
×
+++
×
×
×
×
++++
×
×
×
×
++++