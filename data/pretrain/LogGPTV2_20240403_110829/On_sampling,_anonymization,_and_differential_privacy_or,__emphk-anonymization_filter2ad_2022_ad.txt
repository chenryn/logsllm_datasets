where γ = (eǫ−ǫ1 −1+β)
eǫ−ǫ1
.
See Appendix A.3 for the proof.
 1e+20
 1e+18
 1e+16
 1e+14
 1e+12
 1e+10
1 δ
 1e+08
 1e+06
 10000
 100
 0
k = 15, β = 0.05
k = 22, β = 0.1
k = 35, β = 0.2
k = 60, β = 0.4
 0.5
 1
ǫ
 1.5
 2
Figure 4: A graph showing the relationship between
the values of k needed to achieve roughly the same
δ if we double β.
 1e+09
 1e+08
 1e+07
 1e+06
 100000
1 δ
 10000
 1000
 100
 10
 1
 0
k = 1, β = 0.025
k = 2, β = 0.025
k = 3, β = 0.025
k = 4, β = 0.025
k = 5, β = 0.025
 0.5
 1
ǫ
 1.5
 2
Figure 5: A graph showing the relationship between
ǫ and 1
δ with small k’s, varying k and ﬁxing β.
 2.5
 2
 1.5
 1
 0.5
 0
ǫ
β = 0.05
β = 0.1
β = 0.2
 10
 15
 20
 25
 30
 35
 40
 45
 50
k
Figure 6: A graph showing the value of ǫ satisﬁed
by a given k if δ ≤ 10−6 with varying sampling prob-
abilities.
3.4 Remarks of the Result
Theorems 5 and 6 show that k-anonymization, when
done safely, and when preceded by a random sampling
step, can satisfy (ǫ, δ)-DP with reasonable parameters.
In the literature, k-anonymization and diﬀerential privacy
have been viewed as very diﬀerent privacy guarantees. k-
anonymization achieves weak syntactic privacy, and diﬀer-
ential privacy provides strong semantic privacy guarantees.
Our result links k-anonymization with diﬀerential privacy,
and suggest that the “hiding in a crowd of k” privacy prin-
ciple, which is used widely in contexts other than privacy-
preserving publishing of relational data, including location
privacy and publishing of social network data, network pack-
ets, and other types of data, indeed oﬀers some privacy guar-
antees when used correctly.
We also observe that another way to interpret our result
is that this provides another method to achieve diﬀeren-
tial privacy. Existing methods for satisfying diﬀerential pri-
vacy mostly take the form of output perturbation. Our re-
sult suggests an alternative approach: Rather than adding
noises to the output, one can add a random sampling step
in the beginning and prune results that are too sensitive
to changes of a single individual tuple (i.e., tuples that vi-
olate k-anonymity).
In other words, when the dataset is
resulted from random sampling, then one can answer count
queries accurately provided that the result is large enough.
An intriguing question is whether other input perturbation
techniques (such as directly adding noises to attribute val-
ues) can be used in combination with k-anonymization to
satisfy diﬀerential privacy as well.
4. RELATED WORK
A lot of work on privacy-preserving data publishing con-
siders privacy notions that are weaker than diﬀerential pri-
vacy. These approaches typically assume an adversary that
knows only some aspects of the dataset (background knowl-
edge) and tries to prevent it from learning some other as-
pects. One can always attack such a privacy notion by
changing either what the adversary already knows, or chang-
ing what the adversary tries to learn. The most prominent
among these notions is k-anonymity [30, 29]. Some follow-
up notions include l-diversity [23] and t-closeness [22].
In
this paper, we analyze the weaknesses of k-anonymity in de-
tail, and argue that a separation between QIDs and sensitive
attributes are diﬃcult to obtain in practice, challenging the
foundation of privacy notions such as l-diversity, t-closeness,
and other ones centered on attribute disclosure prevention.
The notion of diﬀerential privacy was developed in a se-
ries of works [7, 13, 3, 11, 8]. It represents a major break-
through in privacy-preserving data analysis. In an attempt
to make diﬀerential privacy more amenable to more sensitive
queries, several relaxations have been developed, including
(ǫ, δ)-diﬀerential privacy [7, 13, 3, 11]. Three basic general
approaches to achieve diﬀerential privacy are adding Laplace
noise proportional to the query’s global sensitivity [8, 11],
adding noise related to the smooth bound of the query’s lo-
cal sensitivity [26], and the exponential mechanism to select
a result among all possible results [25]. A survey on these
results can be found in [9]. Our approach suggests an alter-
ative by using input perturbation rather than output per-
turbation to add uncertainty to the adversary’s knowledge
of the data.
Random sampling [1, 2] has been studied as a method
for privacy preserving data mining, where privacy notions
other than diﬀerential privacy were used. The relationship
between sampling and diﬀerential privacy has been explored
before. Chauduri and Mishra [6] studied the privacy eﬀect
of sampling, and showed a linear relationship between the
sampling probability and the error probability δ. Their re-
sult suggests an approach to perform ﬁrst k-anonymization
and then sampling as the last step. We instead consider the
approach of perform sampling as the ﬁrst step and then k-
anonymization. Our result suggests that the latter approach
beneﬁts much more from the sampling.
There exists some work on publishing microdata while sat-
isfying (ǫ, δ)-DP or its variant. Machanavajjhala et al. [24]
introduced a variant of (ǫ, δ)-DP called (ǫ, δ)-probabilistic
diﬀerential privacy and showed that it is satisﬁed by a syn-
thetic data generation method for the problem of releas-
ing the commuting patterns of the population in the United
States. This notion is stronger than (ǫ, δ)-DP. Korolova
et al. [20] considered publishing search queries and clicks
that achieves (ǫ, δ)-diﬀerential privacy. A similar approach
for releasing query logs with diﬀerential privacy was pro-
posed by G¨otz et al. [15]. These approaches apply the output
perturbation technique in diﬀerential privacy to microdata
publishing scenarios that can be reduced to histogram pub-
lishing at their core. Blum et al. [4] and Dwork et al. [12]
considered outputing synthetic data generation that is use-
ful for a particular class of queries. These papers do not deal
with the relationship between k-anonymization and diﬀeren-
tial privacy, or between sampling and k-anonymization.
Kifer and Lin [19] developed a general framework to char-
acterize relaxation of diﬀerential privacy. They identiﬁed
two axioms for a privacy deﬁnition: Transformation Invari-
ance and Privacy Axiom of Choice, which are satisﬁed by
(β, ǫ, δ)-DPS. They did not consider the composability of
these notions, which was our emphasis, as a clear under-
standing of the composability issues directs us what can and
cannot be done with sampled dataset.
5. CONCLUSIONS
We have answered the two questions we set out in the
beginning of the paper. We take the approach of start-
ing from both k-anonymization and diﬀerential privacy and
trying to meet in the middle. On the one hand, we iden-
tify weaknesses in the k-anonymity notion and existing k-
anonymization methods and propose the notion of safe k-
anonymization to avoid these privacy vulnerabilities. On
the other hand, we try to relax diﬀerential privacy to take
advantage of the adversary’s uncertainty of the data. The
key insight underlying our results is that random sampling
can be used to bridge this gap between k-anonymization and
diﬀerential privacy.
We have explored both the power and potential pitfalls
to take advantage of sampling in private data analysis or
publishing. Our results show that sampling, when used cor-
rectly, is a powerful tool that can greatly beneﬁt diﬀerential
privacy, as it creates uncertainty for the adversary. Sampling
can increase the privacy budget and error toleration bound.
Sampling also enables the usage of algorithms such as safe
k-anonymization. An intriguing open question is whether
there exist approaches other than sampling that can create
uncertainty for the adversary, that can tolerate answering
ǫ-DP queries.
Acknowledgements
This paper is based upon work supported by the United
States National Science Foundation under Grant No.
1116991, and by the United States AFOSR under grant ti-
tled “A Framework for Managing the Assured Information
Sharing Lifecycle”.
6. REFERENCES
[1] R. Agrawal, R. Srikant, and D. Thomas. Privacy
preserving olap. In SIGMOD, pages 251–262, 2005.
[2] S. Agrawal and J. R. Haritsa. A framework for
high-accuracy privacy-preserving mining. In ICDE,
pages 193–204, 2005.
[3] A. Blum, C. Dwork, F. McSherry, and K. Nissim.
Practical privacy: the sulq framework. In PODS ’05:
Proceedings of the twenty-fourth ACM
SIGMOD-SIGACT-SIGART symposium on Principles
of database systems, pages 128–138, New York, NY,
USA, 2005. ACM.
[4] A. Blum, K. Ligett, and A. Roth. A learning theory
approach to non-interactive database privacy. In
STOC, pages 609–618, 2008.
[5] J.-W. Byun, A. Kamra, E. Bertino, and N. Li.
Eﬃcient k-anonymization using clustering techniques.
In Proceedings of the 12th international conference on
Database systems for advanced applications,
DASFAA’07, pages 188–200, 2007.
[6] K. Chaudhuri and N. Mishra. When random sampling
preserves privacy. In CRYPTO, pages 198–213, 2006.
[7] I. Dinur and K. Nissim. Revealing information while
preserving privacy. In PODS ’03: Proceedings of the
twenty-second ACM SIGMOD-SIGACT-SIGART
symposium on Principles of database systems, pages
202–210, New York, NY, USA, 2003. ACM.
[8] C. Dwork. Diﬀerential privacy. In ICALP, pages 1–12,
2006.
[9] C. Dwork. Diﬀerential privacy: A survey of results. In
TAMC, pages 1–19, 2008.
[10] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,
and M. Naor. Our data, ourselves: Privacy via
distributed noise generation. In S. Vaudenay, editor,
EUROCRYPT, volume 4004 of Lecture Notes in
Computer Science, pages 486–503. Springer, 2006.
[11] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data
analysis. In TCC, pages 265–284, 2006.
[12] C. Dwork, M. Naor, O. Reingold, G. N. Rothblum,
and S. Vadhan. On the complexity of diﬀerentially
private data release: eﬃcient algorithms and hardness
results. In STOC, pages 381–390, 2009.
[13] C. Dwork and K. Nissim. Privacy-preserving
datamining on vertically partitioned databases. In
CRYPTO, pages 528–544. Springer, 2004.
[14] B. Gedik and L. Liu. Protecting location privacy with
personalized k-anonymity: Architecture and
algorithms. IEEE Transactions on Mobile Computing,
7:1–18, January 2008.
[15] M. G¨otz, A. Machanavajjhala, G. Wang, X. Xiao, and
J. Gehrke. Privacy in search logs. TKDE, 2010. to
appear.
[16] Y. He and J. F. Naughton. Anonymization of
set-valued data via top-down, local generalization. In
VLDB, pages 934–945, 2009.
[17] S. P. Kasiviswanathan, H. K. Lee, K. Nissim,
S. Raskhodnikova, and A. Smith. What can we learn
privately? In FOCS, pages 320–326, 1992.
[18] S. P. Kasiviswanathan and A. Smith. A note on
diﬀerential privacy: Deﬁning resistance to arbitrary
side information. CoRR, abs/0803.3946, 2008.
[19] D. Kifer and B.-R. Lin. Towards an axiomatization of
statistical privacy and utility. In Proceedings of the
twenty-ninth ACM SIGMOD-SIGACT-SIGART
symposium on Principles of database systems of data,
PODS ’10, pages 147–158, New York, NY, USA, 2010.
ACM.
[20] A. Korolova, K. Kenthapadi, N. Mishra, and
A. Ntoulas. Releasing search queries and clicks
privately. In WWW, pages 171–180, 2009.
[21] K. LeFevre, D. DeWitt, and R. Ramakrishnan.
Mondrian multidimensional k-anonymity. In ICDE,
page 25, 2006.
[22] N. Li, T. Li, and S. Venkatasubramanian. t-closeness:
Privacy beyond k-anonymity and l-diversity. In ICDE,
pages 106–115, 2007.
viewed as ﬁrst sampling with probability β, then followed
by applying the algorithm Aβ1 , which satisﬁes (ǫ1, δ1)-DP.
We use Λβ(D) to denote the process of sampling from
D with sampling rate β. Any pair D, D′ can be viewed
as D and D−t, where D−t denotes the dataset resulted
from removing one copy of t from D. For any O,
let
Z = Pr[Aβ2 (D) ∈ O], and X = Pr[Aβ2 (D−t) ∈ O],