title:Decamouflage: A Framework to Detect Image-Scaling Attacks on CNN
author:Bedeuro Kim and
Alsharif Abuadbba and
Yansong Gao and
Yifeng Zheng and
Muhammad Ejaz Ahmed and
Surya Nepal and
Hyoungshick Kim
3
2
0
0
0
.
1
2
0
2
.
7
8
9
8
4
N
S
D
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
7
-
2
7
5
3
-
4
5
6
6
-
1
-
8
7
9
|
)
N
S
D
(
s
k
r
o
w
t
e
N
d
n
a
s
m
e
t
s
y
S
e
l
b
a
d
n
e
p
e
D
n
o
e
c
n
e
r
e
f
n
o
C
l
a
n
o
i
t
a
n
r
e
t
n
I
P
I
F
I
/
E
E
E
I
l
a
u
n
n
A
t
s
1
5
1
2
0
2
2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Decamouﬂage: A Framework to Detect
Image-Scaling Attacks on CNN
Bedeuro Kim1,2, Alsharif Abuadbba2,3, Yansong Gao2,4, Yifeng Zheng2,5, Muhammad Ejaz Ahmed2,
Surya Nepal2,3, Hyoungshick Kim1,2
1Department of Electrical and Computer Engineering, Sungkyunkwan University, 2CSIRO’s Data61, 3Cybersecurity CRC,
4Nanjing University of Science and Technology, 5Harbin Institute of Technology
PI:EMAIL, PI:EMAIL, PI:EMAIL,
PI:EMAIL, PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract—Image-scaling is a typical operation that processes
the input image before feeding it into convolutional neural
network models. However, it is vulnerable to the newly revealed
image-scaling attack. This work presents an image-scaling attack
detection framework, Decamouﬂage, consisting of three indepen-
dent detection methods: scaling, ﬁltering, and steganalysis, to
detect the attack through examining distinct image characteris-
tics. Decamouﬂage has a pre-determined detection threshold that
is generic. More precisely, as we have validated, the threshold
determined from one dataset is also applicable to other different
datasets. Extensive experiments show that Decamouﬂage achieves
detection accuracy of 99.9% and 98.5% in the white-box and the
black-box settings, respectively. We also measured its running
time overhead on a PC with an Intel i5 CPU and 8GB RAM.
The experimental results show that image-scaling attacks can
be detected in milliseconds. Moreover, Decamouﬂage is highly
robust against adaptive image-scaling attacks (e.g., attack image
size variances).
Keywords—Image-scaling attack, Adversarial detection, Back-
door detection
I. INTRODUCTION
Deep learning models have shown impressive success in solv-
ing various tasks [1], [2], [3], [4]. One representative domain
is the computer vision that
is eventually the impetus for
the current deep learning wave [1]. The convolutional neural
network (CNN) models are widely used in the vision domain
because of its superior performance [1], [5], [2]. However, it
has been shown that deep learning models are vulnerable to
various adversarial attacks. Hence, signiﬁcant research efforts
have been directed to defeat the mainstream of adversarial
attacks such as adversarial samples [6], [7], backdooring [8],
[9], and inference [10], [11].
Xiao et al. [12] introduced a new attack called image-
scaling attack (also referred to as camouﬂage attack) that
potentially affects all applications using scaling algorithms as
an essential pre-processing step, where the attacker’s goal is to
create attack images presenting a different meaning to humans
before and after a scaling operation. This attack would be
a serious security concern for computer vision applications.
Below we ﬁrst give a concise example of the image-scaling
attack and exemplify its severe consequences.
Image-scaling attack example. Input of CNN models
typically takes ﬁxed-size images such as 224 × 224 (repre-
senting the height, width) so as to reduce the complexity of
computations [2]. However, the size of raw input images can
be varied or become much larger (e.g., 800 × 600) than this
ﬁxed-size. Therefore, the resizing or downscaling process is
a must before feeding such larger images into an underlying
CNN model. Xiao et al. [12] revealed that the image-scaling
process is vulnerable to the image-scaling attack, where an
attacker intentionally creates an attack image that is visually
similar to a base image for humans but recognized as a
target image by the CNN model after image-scaling function
(e.g., resizing or downscaling) is applied to the attack image.
Figure 1 illustrates an example of image-scaling attacks. The
‘wolf’ image is disguised delicately into the ‘sheep’ image as
base image to form an attack image. When the attack image
is down-sampled/resized, the ‘sheep’ pixels are discarded, and
the ‘wolf’ image is ﬁnally presented. General, image-scaling
attack abuses an inconsistent understanding of the same image
between humans and machines.
Fig. 1: Example of image-scaling attacks presenting a deceiving effect. The
left image shows what human sees before the scaling operation and the right
image shows what the CNN model sees after the scaling operation.
The strength of the image-scaling attack is its independence
on CNN models and data — it requires no knowledge of
training data and the model because it mainly exploits the
image-scaling function used for pre-processing. For image-
scaling attacks, only knowledge about the used image-scaling
function is required. It is noted that the attacker can relatively
easily obtain this information because a small number of
well-known image-scaling functions (e.g., nearest-neighbor,
bilinear, and bicubic interpolation methods) are commonly
used for real-world services, and a small number of input sizes
(e.g., 224× 224 and 32× 32) are used for representative CNN
models [12], as summarized in Table I. Furthermore, the pa-
978-1-6654-3572-7/21/$31.00 ©2021 IEEE
DOI 10.1109/DSN48987.2021.00023
63
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
rameters for the image-scaling function can be exposed to the
public in some services. Nonetheless, even when the parameter
information is not provided explicitly, it is feasible to infer the
function parameter information used in a target service with
API queries under a limited trials by an attacker [12].
TABLE. I: Input sizes for popular CNN models.
Size
VGG, ResNet, GoogleNet, MobileNet
Model
LeNet-5
AlexNet
Inception V3/V4
DAVE-2 Self-Driving
(pixels * pixels)
32 * 32
224 * 224
227 * 227
299 * 299
200 * 66
The image-scaling attacks can target various surfaces. First,
as an evasive attack, the attack images crafted via image-
scaling attacks can achieve the attack effect similar to adver-
sarial examples with an advantage of agnostic to underlying
CNN models. This attack has successfully mounted on a num-
ber of commercial cloud-based computer vision services that
deploy the state-of-the-art machine learning models including
Microsoft Azure1, Baidu2, and Tencent3. Second, the attack
image can be exploited for data poisoning to insert a backdoor
into any model trained over the poisonous data (see Section
VI).
Unlike other adversarial attacks where corresponding coun-
termeasures have been well
investigated, only one study
suggested defense mechanisms against image-scaling attacks.
Quiring et al. [13] ﬁrst analyzed the root cause of image-
scaling attacks and proposed two defense mechanisms, (1)
use of robust scaling algorithms and (2) image reconstruction,
to prevent image-scaling attacks by delicately exploiting the
relationship between the downsampling frequency and the
convolution kernel used for smoothing pixels. The proposed
defense mechanism sanitizes those pixels, which renders the
image-scaling attack technique unable to inject target pixels
with the required quality. However, their defense approaches
have the following downsides. First, the use of robust scaling
algorithms is likely to cause backward compatibility problems
with existing scaling algorithms in OpenCV and TensorFlow.
Second, as Quiring et al. [13] mentioned, small artifacts
from an attack image can remain even after applying their
suggested scaling algorithms, as the manipulated pixels are not
cleansed and still contribute to the scaling. Third, the image
reconstruction method removes the set of pixels in the attack
images and reconstructs those pixels with image ﬁlters. This
approach would signiﬁcantly decrease the attack chance, but
it can degrade the quality of input normal images for CNN
models—noting attack images are rare in comparison with
normal images.
From the security deterrence perspective, prevention is
blinded [13], which is unable to tell whether an attack even-
tually occurs, thus infeasible to track the provenance of the
1https://azure.microsoft.com/en-us/services/cognitive-services/
computer-vision/?v=18.05
2https://ai.baidu.com/tech/imagerecognition/ﬁne grained
3https://ai.qq.com/product/visionimgidy.shtml
launched attack. Blinded prevention is unable to provide de-
terrence, since there is no price or risk faced by an attacker. In
contrast, detection addresses such concern where the attacker
can be captured and under penalty risks whenever they want
to launch the attack. Therefore, detection is desirable in the
real-world critical system—help identify attack sources. In ad-
dition, detection is complementary with the prevention. Once
the input is regarded as an adversarial image, the attack effect
removal [13] can be consequentially applied to remove the
attacking effect. In this context, the pixel sanitizing operation
is solely applied to those attack images to restore the correct
classiﬁcation, thus minimizing the performance degradation
resulted from the pixel sanitizing on any normal inputs. In
this process, both detection and prevention are realized.