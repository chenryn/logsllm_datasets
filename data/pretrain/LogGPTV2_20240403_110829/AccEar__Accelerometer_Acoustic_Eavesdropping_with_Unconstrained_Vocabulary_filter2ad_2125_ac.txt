an image as real or fake. At the end of the training process,
the output of D is the average of all the responses from a
convolutional pass across the image.
Training: We train each individual user model with 200
epochs. In the ﬁrst 100 epochs, we set the learning rate of
0.0002, and in the last 100 epochs, we use Adam [30] to
adaptive adjust the learning rate to speed up the convergence
of the network. The detailed algorithm for the training process
is presented in Algorithm 1, where θD and θG represent
the parameters (such as weights, bias, etc.) of generator G
and discriminator D respectively, m refers to the batch size,
xi refers to the ground truth, yi refers to the condition, zi
refers to the noise sample. In each iteration, we ﬁrst ﬁxed
the parameters of generator θG and update the parameters of
discriminator θD, after θD updated, we will keep θD ﬁxed and
update θG.
Spectrogram-to-audio conversion: After obtaining the Mel
spectrogram generated by conditional GAN, we need a
vocoder to convert the acoustic parameters to speech wave-
form. In our system, we adopt a classic vocoder Grifﬁn-Lim
[7] to synthesize the waveform from the Mel spectrogram.
The Grifﬁm-Lim algorithm is a method to reconstruct the
speech waveform with a known amplitude spectrum and
an unknown phase spectrum by iteratively generating the
phase spectrum and using the known amplitude spectrum
and the calculated phase spectrum. We ﬁrst initialize a phase
,
(cid:4)
(cid:3)
(cid:4)
(cid:5)
, . . . , (yn, xn)
Algorithm 1 Training Process of cGAN
(cid:2)(cid:3)
Input: n paired training data
y2, x2
y1, x1
Output: θD, θG
1: for each epoch do
2:
3:
4:
for each iteration do
(cid:2)
Sample m paired examples from input
Sample m noise samples
from a distri-
bution.
yi|zi
Generate data
Update discriminator parameter θD to maximize
(cid:5)
z1, z2, . . . , zm
(cid:3)
˜x1, ˜x2, . . . , ˜xm
, ˜xi = G
(cid:2)
(cid:5)
(cid:4)
m(cid:6)
i=1
(cid:7)
log D
(cid:7)
(cid:7)
xi|yi
˜xi|yi
(cid:8)
+
(cid:8)(cid:8)
,
1
m
˜V = LS +
m(cid:6)
1
m
1 − D
θD ← θD + η∇ ˜V (θD)
log
i=1
(cid:2)
(cid:5)
from a distri-
z1, z2, . . . , zm
Sample m noise samples
(cid:2)
bution.
Sample m conditions
from input
Update generator parameter θG to maximize
(cid:8)(cid:8)(cid:8)
m(cid:6)
y1, y2, . . . , ym
(cid:7)
(cid:7)
(cid:7)
(cid:5)
zi|yi
,
1
m
D
log
˜V =
G
θG ← θG − η∇ ˜V (θG)
i=1
end for
10:
11: end for
spectrum and synthesize a new speech waveform with this
phase spectrum and a known amplitude spectrum (from the
Mel spectrogram generated by cGAN) by Short-time Fourier
Inverse Transform (ISTFT). Then, we perform STFT to the
new speech waveform and calculate the new phase spectrum.
We continue to synthesize the new speech waveform with the
known amplitude spectrum and the new phase spectrum until
we obtain the satisfactory waveform.
V. EVALUATION
In this section, we report the details of our experimental
setup and performance evaluation of AccEar on the recon-
struction of speech from accelerometer data.
A. Implementation and Experiment Setup
In our experiments, we target smartphones running the
Android operating system since its prevalent share on the
smartphone market, i.e., 72.21% reported by Statista [31].
In this work, we evaluate our attack scheme with multiple
sampling rates to accommodate both the legacy and future
permission policies of the Android system [32]. We collect
accelerometer data from six different smartphones (Huawei
Mate40 Pro, Huawei Mate30 Pro, OPPO Reno6 Pro, Samsung
S21+, OPPO Find X3, and XiaoMi RedMi 10X Pro) and
two different
tablets (Huawei MatePad Pro and Samsung
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1763
Person
Bill Gates
Feifei Li
Pony Ma
Jane Goodall
Jiaying Ye
Mingzhu Dong
Steve Job
Yansong Bai
Anne Hathaway
Elon Musk
Label
User1
User2
User3
User4
User5
User6
User7
User8
User9
User10
User11 Mark Zuckerberg
User12
User13
User14 Minhong Yu
User15
User16
Oprah Winfrey
Lan Yang
Robin Li
Yingtai Long
Sex
male
female
male
female
female
female
male
male
female
male
male
female
female
male
male
female
Language
English
English
Chinese
English
Chinese
Chinese
English
Chinese
English
English
English
English
Chinese
Chinese
Chinese
Chinese
Length(seconds)
Testing words
Training words Overlapping words
7068
7120
5180
7484
9032
5428
14836
6792
60
60
60
60
60
60
60
60
179
182
215
188
188
234
190
251
197
156
177
167
289
199
244
198
12593
17626
28554
11339
11339
18709
37751
27317
*
*
*
*
*
*
*
*
19
15
20
23
16
22
17
22
21
17
15
18
25
17
20
18
TABLE I: The dataset used for evaluating AccEar, and note that, each audio couples with the accelerometer signal. Data of
the last 8 users are used to evaluate the performance of cross-users.
Score
5
4
3
2
1
Level
Recovered all of the original speech
Recovered most of the original speech
Recovered half of the original speech
Recovered little of the original speech
Recovered none of the original speech
TABLE II: MOS and corresponding level.
Galaxy Tab S6 Lite) using a third-party application named
Accelerometer Meter2 by Keuwlsoft. We provide the detailed
parameters of these devices in Table III in Appendix A. The
highest sampling rate of such smartphones is around 500Hz.
We perform both the pre-process of the accelerometer data
and conversion of the enhanced accelerometer Mel spectro-
gram back into audio on a laptop with an i7-10750H CPU
and 16GB memory. The training and testing processes run on
a server with Nvidia RTX 3090 GPU. We train an individual
model for each public personality by using his/her audio
samples respectively and then train several generic models
with the data of a speciﬁc group of personalities. For each
model, we train it in 200 epochs with the initial learning rate
of 0.002. A model training process takes about 2.28 hours on
a dataset with 1010 Mel spectrogram images.
B. Data Collection
Audio Collection: We collected the audio samples from 8
English-speaking and 8 Chinese-speaking public personalities
whose utterances are available on the Internet (e.g., YouTube).
For convenience, we marked the above public personalities as
User1 to User16 as shown in Table I. The speech samples
of each user are divided into training and testing sets which
include different numbers of words3. To demonstrate the
effectiveness of reconstructing unlimited words, we make sure
that the training and testing sets overlap only on a small set
of words.
Accelerometer Data Collection: We put the smartphones on
the table in a conference room and play the above collected
audio samples with a built-in loudspeaker while our app runs
in the background to record the accelerometer data. Thus,
we have a direct correspondence between audio samples and
accelerometer data. The accelerometer data is divided into
training and testing sets coupled with audio samples as shown
in Table I. In addition, we verify the robustness of AccEar
by collecting accelerometer data under different settings (i.g.,
sampling rates, volume, phone models, position, scenarios).
C. Evaluation Metrics
To evaluate the performance of reconstructed audio, we
adopt the following three metrics.
Mel-Cepstral Distortion (MCD) [33] is an objective eval-
uation metric since it represents the difference of the Mel-
Frequency Cepstral Coefﬁcients (MFCC) features between
the reconstructed audio and the corresponding original audio.
Therefore, a small MCD means that the reconstructed audio
is similar to the original one (i.e., the smaller, the better).
Typically, reconstructed audio with MCD below 8 can be
comprehended by a speech recognition system [34]. The MCD
can be calculated as:
M CD =
10
log 10
(cid:3)(cid:4)(cid:4)(cid:5)
M(cid:6)
2
m=1
(cr(m) − cs(m))
2
(7)
2Accelerometer Meter v1.32 - https://keuwl.com/Accelerometer/
3https://github.com/hui-zhuang/AccEar.git
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:36:50 UTC from IEEE Xplore.  Restrictions apply. 
1764
Down in our 
basement
Filled with cans of 
food and water
When the nuclear attack came, we 
were supposed to go downstairs
Hunker down, and eat 
out of that barrel
Text
Accelerometer Data
Original Audio
Reconstructed Audio
Fig. 8: User1 speech spectrograms for (a) accelerometer data, (b) original audio and (c) reconstructed audio via AccEar.
where cr and cs are the Mel-Cepstrum from the original and
reconstructed audio, respectively, and M is order of Mel-
Cepstrum.
Mean Opinion Score (MOS) [35] is a subjective evaluation