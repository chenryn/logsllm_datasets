Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
405
procedures, such as cross validation, are used to determine it.
For details on bandwidth selection, see [29].
In the subsequent discussion, we consider log-transformed
density estimators. These objects are potentially unstable for
arguments where the true density f is close to 0, because
small errors in the estimate of f translate into great errors in
the logarithm. For this reason, we deﬁne the truncated KDE
pointwise in t as
ˆf (t) := ˜f (t) ∨ τ ,
where “a∨ b” denotes the maximum of two numbers a, b ∈ R
and τ > 0 is a user-determined ﬂoor. In Section IV we discuss
how to choose τ dependent on n and β. The construction of
the truncated KDE is described in Algorithm 1.
Algorithm 1 Truncated kernel density estimator
Input: data sample X = (X1, ..., Xn), evaluation point t,
bandwidth h, kernel function K, ﬂoor τ
out = out + K((t − Xi)/h)
for i = 1, 2, . . . , n do
1: function TKDE(X, t, h, K, τ)
2: out = 0
3:
4:
5:
6: out = out/(nhd)
7: return out ∨ τ
8: end function
end for
III. DIFFERENTIAL PRIVACY AS A LOCAL PROPERTY
As we have seen in our Introduction, -DP means that for
any neighboring databases x, x
(cid:2) the bound
Lx,x(cid:2) (E) ≤ 
x,x(cid:2) = sup
E
(12)
holds, where the loss Lx,x(cid:2) is deﬁned in (2). Thus, in principle,
validating DP requires the calculation of Lx,x(cid:2) (E) for any
measurable event E, a problem that is intractable from a
practical perspective given the complexity of the space of
measurable events (see Introduction). We can, however, dras-
tically reduce the effort of event selection in the supremum
by exploiting that differential privacy is an inherently local
property, i.e. that the level of privacy is determined by the loss
on small events. To get an intuition of this point, consider an
event E that can be decomposed into the disjoint subsets E1
and E2. It is a simple exercise to show that
Lx,x(cid:2) (E) ≤ max{Lx,x(cid:2) (E1), Lx,x(cid:2) (E2)}.
In this sense going from larger to smaller events increases
the privacy loss and thus gets us closer to x,x(cid:2). Iterating this
process suggests that we should look at “the smallest events
possible”, which are single points. So we expect that ultimately
x,x(cid:2) ≈ sup
t∈Y
|Lx,x(cid:2) ({t})|.
(13)
this statement
Admittedly,
is not formally correct for all
algorithms, but we will make it rigorous for certain classes
of algorithms in the course of this section. Compared with the
supremum over all measurable events in (12), the expression
in (13) is more convenient, because single points are easy to
handle. We will explore this advantage in detail at the end of
this section.
We now begin our formal discussion by specifying two
classes of algorithms that are considered throughout this work:
discrete and continuous ones.
We call an algorithm A that maps a database x to random
values in either a ﬁnite or a countably inﬁnite set Y a discrete
algorithm. Without loss of generality, we will assume that
Y ⊂ N. Moreover, we call the corresponding probability
function fx : Y → [0, 1] deﬁned as
fx(t) := P(A(x) = t),
∀t ∈ Y
(14)
the discrete density of A in x. With this notation we can write
for any E ⊂ Y
(cid:6)
fx(t).
t∈E
P(A(x) ∈ E) =
(15)
Examples of discrete algorithms include Randomized Re-
sponse [31], Report Noisy Max [32] and the Sparse Vector
Technique [33].
Next, suppose that Y = R
d. We say that A is a continuous
algorithm, if for any database x, A(x) has a continuous density
fx : R
d → R, such that for any Borel measurable event E
(cid:13)
P(A(x) ∈ E) =
fx(t)dt.
E
Typical examples of continuous algorithms are, as mentioned
before, the Laplace [32], the Gaussian [32] and versions of the
Exponential Mechanism [34]. We want to highlight that in this
deﬁnition the requirement of continuous densities on the whole
d is only made for convenience of presentation and can
space R
be relaxed to densities on subsets, e.g., [0,∞) ⊂ R in the case
d = 1. Notice that for continuous algorithms (13) is technically
invalid because Lx,x(cid:2) ({t}) = 0 for any point t. However, it
is possible to preserve the idea of (13) by reformulating it in
terms of continuous densities (see Theorem 1).
Given the above deﬁnitions, the distribution of an algorithm
A can be thoroughly characterized by its densities and we
use the notation A(x) ∼ fx throughout this paper. In the
following theorem, we give a mathematically rigorous version
of (13). Variants of this theorem can be encountered in the
DP literature and the inequality “≤” in (16) is frequently used
in privacy proofs. However, the exact identity in (16) is not
trivial and therefore worked out here explicitly.
Theorem 1. Given a discrete or continuous algorithm A with
A(x) ∼ fx and A(x
) ∼ fx(cid:2) we have
(cid:2)
(cid:2)(cid:2) ln(fx(t)) − ln(fx(cid:2) (t))
(cid:2)(cid:2),
(16)
x,x(cid:2) = sup
t∈Y
where ∞ − ∞ := 0.
Proof: We ﬁrst consider the discrete setting: In order to
show “≥” we notice that for all t ∈ Y
(cid:2)(cid:2) ln(fx(t)) − ln(fx(cid:2) (t))
(cid:2)(cid:2).
Lx,x(cid:2) ({t}) =
Recall that x,x(cid:2) = supE |Lx,x(cid:2) (E)|. Here the supremum is
taken over all elements E of the power set P(Y) (which
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
406
includes in particular sets with only one element) and this
directly implies “≥”.
The proof of “≤” follows by standard techniques. We ﬁx a set
E ⊂ Y and rewrite Lx,x(cid:2) (E) using (15), s.t.
t∈E fx(t)
t∈E fx(cid:2) (t)
(cid:9)(cid:14)
(cid:14)
(cid:2)(cid:2)(cid:2) ln
Lx,x(cid:2) (E) =
(cid:11)(cid:2)(cid:2)(cid:2).
(17)
Without loss of generality, we assume that the numerator is
greater than the denominator and we can therefore drop the
absolute value. Now the inner fraction can be upper bounded
as follows:(cid:14)
(cid:14)
t∈E fx(t)
t∈E fx(cid:2) (t)
≤
(cid:14)
(cid:14)
t∈E fx(cid:2) (t)[fx(t)/fx(cid:2) (t)]
t∈E fx(cid:2) (t)
≤ sup
t∈Y
fx(t)
fx(cid:2) (t)
.
Taking the logarithm on both sides and the supremum over all
E on the left maintains the inequality, showing “≤”.
Moving to continuous algorithms, we notice that the proof of
“≤” follows along the same lines as for the discrete case and
is therefore omitted (one simply has to replace all the sums
by integrals).
To prove “≥” we ﬁrst observe that a probability density in t
gives the probability of a very small region around t. More
precisely it can be expressed as follows
fx(t) = lim
δ→0
P(A(x) ∈ Uδ(t))
vol(Uδ(t))
,
where Uδ(t) := {s ∈ Y : |t − s| ≤ δ} and vol() denotes
the d-dimensional volume. The identity is a special case of
(cid:2)
Theorem 6.20 (c) in [35]. The same statement holds for x
instead of x and we can use that to get
fx(t)
fx(cid:2) (t)
= lim
δ→0
P(A(x) ∈ Uδ(t))
P(A(x(cid:2)) ∈ Uδ(t))
≤ sup
E
P(A(x) ∈ E)
P(A(x(cid:2)) ∈ E)
for any t ∈ Y. Taking the logarithm on both sides and the
supremum over t on the left preserves the inequality. Recalling
(3), this implies supt∈Y
proves the theorem.
(cid:2)(cid:2) ln(fx(t))−ln(fx(cid:2) (t))
(cid:2)(cid:2) ≤ x,x(cid:2), which
Theorem 1 allows us to characterize DP of an algorithm A
by the absolute log-difference of the algorithm’s densities. For
ease of reference we deﬁne this difference, the loss function,
explicitly as
(cid:2)(cid:2) ln(fx(t)) − ln(fx(cid:2) (t))
(cid:2)(cid:2).
(cid:8)x,x(cid:2) (t) :=
(18)
This deﬁnition admits the restatement of Theorem 1 as x,x(cid:2) =
supt∈Y (cid:8)x,x(cid:2) (t) and shows that DP is a local property. Here
the term “local” is used as common in real analysis, referring
to features of a function, that are determined by its behavior
in only a small neighborhood (in the case of (cid:8)x,x(cid:2)
in a
neighborhood around its argmax).
Figure 2 provides an illustration of the loss function for
some standard examples of randomized algorithms (see e.g.
[31], [32]). The plots help discern the amount of privacy
leakage and where it occurs. For example, we observe that
for Randomized Response (left) only two outputs elicit any
privacy leakage at all, while the maximum loss associated
with the Laplace Mechanism (middle panel) is assumed ev-
erywhere, except for the area enclosed by the density modes.
For the Gaussian Mechanism (right panel) no single t exists
that maximizes the loss. Instead, (cid:8)x,x(cid:2) (t) tends to inﬁnity for
growing |t|, which implies decreasing privacy for tail events.
The unbounded loss function for |t| → ∞ shows that the
Gaussian Mechanism does not satisfy pure DP.
0.100
0.075
0.050
0.025
0.05
0.04
0.03
0.02
0.01
0.00
2.5
5.0
7.5
10.0
−10 −5
0
5
10
15
−20 −10 0
10 20 30
0.8
0.6
0.4
0.2
0.0
1.5
1.0
0.5
0.0
2.5
5.0
7.5
10.0
−10 −5
0
5
10
15
−20 −10
0
10
20
30
0.20
0.16
0.12
0.08
1.00
0.75
0.50
0.25
0.00
(cid:2)
) for two neighboring databases x, x
Fig. 2: The top row depicts the densities fx ∼ A(x), fx(cid:2) ∼
(cid:2) and algorithm A
A(x
chosen (from left
the
Laplace Mechanism and Gaussian Mechanism. The bottom
row captures the corresponding loss functions (cid:8)x,x(cid:2) from (18).
to right) as Randomized Response,
In the next section, we develop statistical methods based
on Theorem 1. Before doing so, we want to point out the
possibilities and limitations of this approach. Theorem 1 pre-
supposes that an algorithm under consideration must be either
discrete or continuous. One counterexample from the related
literature is a ﬂawed version of the Sparse Vector Technique
(Algorithm 3 in [33]), which is neither fully continuous nor
discrete and therefore lies outside the scope of our methods.
Still, we want to emphasize that algorithms usually considered
in the validation literature fall into either category (in [21] all
except for SVT3, SVT34Parallel and NumericalSVT, which
are all variations of the above Sparse Vector Technique).
The key advantage of dividing algorithms into continuous
and discrete ones is that we can tailor estimation methods
to each case. This notably helps us to handle the tricky
case of continuous algorithms. More precisely, continuous
algorithms will assume any value on a continuum (e.g. an
interval) and therefore the ensuing output space is inﬁnitely
large. To appreciate the practical effects of this, consider
a discretization of the output space: Suppose we discretize
the unit interval Y = [0, 1] into 1000 equally spaced points
Y discr := {1/1000, ..., 999/1000, 1}. This discretization may
seem modest in terms of precision, but it already yields an
output space of 1000 distinct elements.
Why is this a problem? As the grid gets ﬁner, the output
probability of any t ∈ Y discr decreases and the sampling
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:21:54 UTC from IEEE Xplore.  Restrictions apply. 
407
effort to approximate the probability soars (at least for standard
estimators like the empirical measure used in [19] and [21]).
It is thus hard to assess DP on small events, which however
is key for general, continuous algorithms.
To resolve this issue, we turn to the theory of kernel
density estimation: Instead of relying on the all-or-nothing
information “A(x) = t” vs “A(x) (cid:3)= t” (as the empirical
measure does), KDE draws on the more gradual information
“A(x) is near t”. While sampling a certain output t in the
continuous case may be unlikely (impossible even from a
theoretical perspective), drawing a sample with some values
close to t is highly probable. This implies that KDE can
provide reliable estimates even of small probabilities, which
do not depend on the grid size of a discretization and only
on the smoothness of the underlying density (see Section II-B).
We brieﬂy summarize the key insights of this section:
Instead of examining large and complex sets in order to
quantify x,x(cid:2), Theorem 1 shows that it sufﬁces to consider
single output values t ∈ Y. In fact, larger events E poten-
tially dilute the observed privacy violation and lead to an
underestimation of privacy leakage. Numerically, the task of
maximizing Lx,x(cid:2) (a function with sets as arguments), is much
more difﬁcult than to maximize (cid:8)x,x(cid:2) (which has arguments in
d or N), where standard solutions exist (see [36]). Finally,
R
the loss function (cid:8)x,x(cid:2) is far more amenable to interpretation
than Lx,x(cid:2). In fact, (cid:8)x,x(cid:2) can be plotted and thus problematic
areas with respect to privacy can be easily displayed and
understood (e.g., we see at one glance, that for the Gaussian
Mechanism, which only satisﬁes approximate DP, the problem
lies in extreme values of t; see Figure 2, right).
We conclude this section with a non-trivial example, where
we utilize the loss function to derive the privacy parameter .
Example 1. We consider a database x containing the infor-
mation of only one individual (m = 1). Assuming that said
individual’s data is a vector v = (v1,··· , vk) ∈ [0, 1]k, i.e.
D = [0, 1]k, we can identify our database as x = v. It is our
intention to publish the maximum entry of v in a differentially
private manner. We can do this by employing a version of
the Noisy Max algorithm (Algorithm 7 in [19]) where we add
independent Laplace noise Li ∼ Lap(0, 1
λ ) to each component
vi and publish the maximum maxi(vi + Li). We demonstrate
how (cid:8)x,x(cid:2) can be used to determine the privacy parameter 
of this algorithm.
On the one hand, releasing a noisy component vi + Li by
itself satisﬁes λ-DP by virtue of the Laplace Mechanism. The
maximum can then be understood as a function over the vector
of noisy components and the composition theorem of DP yields
kλ as an upper bound of . On the other hand, deﬁne Fi
(cid:2)
as the distribution function of vi + Li and fi = F
i as the
corresponding density. Then the density fv of the random
variable maxi(vi + Li) is of the form
(cid:9) k(cid:6)
(cid:11)(cid:9) k(cid:15)
(cid:11)
fv(t) =
i=1
fi(t)
Fi(t)
Fi(t)
.
i=1
In the case where v1 = ... = vk, this can be simpliﬁed
to fv(t) = kf1(t)[F1(t)]k−1. Using this formula,
is a
straightforward calculation to show that for v = (0, ..., 0),
w = (1, ..., 1) and sufﬁciently large t ∈ R
it
(cid:8)v,w(t) = | ln(fv(t)) − ln(fw(t))| = kλ.
Theorem 1 especially implies that kλ is also a lower bound
of  and thus the equality  = kλ holds.
IV. QUANTIFYING THE MAXIMUM PRIVACY VIOLATION
In this section, we proceed to the statistical aspects of our
discussion. According to Theorem 1 the data-speciﬁc privacy
violation x,x(cid:2) deﬁned in (3) can be attained by maximizing
the loss function (cid:8)x,x(cid:2) deﬁned in (18). We devise an estimator
ˆx,x(cid:2) for x,x(cid:2), by maximizing an empirical version ˆ(cid:8)x,x(cid:2) of the
loss function, speciﬁed in Section IV-A. In Proposition 1, we
demonstrate mathematically that such estimators are consistent
with fast convergence rates. Besides estimation, we consider
conﬁdence intervals for the pointwise privacy loss (cid:8)x,x(cid:2) (t) in
∗ close to the argmax of (cid:8)x,x(cid:2),
Section IV-B. If applied to a t
these can be used to statistically locate x,x(cid:2) ≈ (cid:8)x,x(cid:2) (t
).
∗
Next recall that the global privacy parameter  as well as
the data-centric privacy level x, deﬁned in (4) and (5) respec-
tively, can be attained by maximizing x,x(cid:2) over a (sub)space
of databases. It therefore makes sense to approximate them
(from below) by a ﬁnite maximum, s.t. for instance
 ≈ max(x1,x(cid:2)
1 , ..., xB ,x(cid:2)
B ),
(19)
(cid:2)
1), ..., (xB, x
(cid:2)
B) are B pairs of adjacent databases