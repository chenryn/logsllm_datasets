pred token anomalyrepresentstheLogFiTpredictions(based SPECIFICITY(S)OFTHELOGFITANDLOGBERTMETHODSONTHEBGL
DATASET.THEEVALUATIONDATAWASMODIFIEDTOREPLACETHETOP10
on top-k prediction accuracy) for the 5,000 normal samples.
VERBSWITHTHEIRWORDNETLEMMAS.
The column labeled pred centroid anomaly represents the
model’s predictions (based on centroid distance) for the 1,000
anomaly samples. Figure 6 shows how the two anomaly
cannot be mapped to its list of known log templates. To test
thresholds top-k prediction accuracy and centroid distance,
the LogFiT model’s ability to handle log sentence variability,
influence the model’s anomaly decision on the Thunderbird
theevaluationsetisdynamicallymodifiedduringinferenceso
dataset. The figure shows that centroid distance is is not an
that the top 10 occurring action words (that can be mapped
effective method for distinguishing normal and anomalous
to synonyms in WordNet) are replaced with their WordNet
log paragraphs. In this case, all samples were categorised as
lemmas. Table V shows the results of feeding the modified
normal by the centroid distance criteria.
BGL evaluation set to the unmodified LogFiT models. The
To investigate the reason why centroid distance is not
results indicate that the LogFiT model is robust to changes
effective, the semantic vectors of the training and evaluation
in the log sentences, as the reduction in F1 is around 2%
sets are plotted after reducing the vector dimensions to 2
(from 91.22 to 89.38). However the drop in F1 performance
using the UMAP algorithm [36]. As shown in Figure 7, the
for LogBERT is large, from 88.63 to 44.22.
semanticvectorsofthetrainingsetdonotformcleanclusters.
So there are no clusters that can effectively threshold the
semantic vectorscomputed duringinference. Therefore, itcan V. CONCLUSION
be concluded that centroid distance minimisation does not The paper has introduced LogFiT, a novel log anomaly
improve the performance of LogFiT and can be omitted from detection model that leverages the general linguistic
the model. In the above mentioned figures, the color of the knowledge embedded within a pre-trained BERT-based LM
points represent training samples (blue), normal predictions and fine-tunes it to recognise specific linguistic patterns of
(pink) and anomaly predictions (yellow). It can be seen that system logs. LogFiT is trained in a self-supervised manner,
the clusters for the anomaly samples from the evaluation usingonlynormalsystemlogs,topredictmaskedtokensinlog
set (yellow points) appear very close to the training/normal sequences. This approach enables LogFiT to recognise only
clusters (blue points). In contrast the clusters for the normal the linguistic structure of normal system logs only, and flag
samples from the evaluation set (pink points) appear further anomalies when it fails to reconstruct an input log sequence.
and more spread out from the training/normal clusters (blue Critically, LogFiT is able to handle variability in the content
points).Thereforecentroiddistancecannotbeusedtoseparate of system logs because of its use of a BERT-based LM. The
normal from anomaly log paragraphs. performance of LogFiT on the HDFS, BGL, and Thunderbird
b) Variations in input data: In practical applications, it datasets has been evaluated and it has been that LogFiT’s F1
is expected that the content of the log sentences will change score outperformed that of the baseline models. Moreover,
over time. This can be because the programmers may change LogFiT’s specificity exceeded that of the baselines on the
some words in the log sentences, or introduce misspellings. HDFS and BGL datasets and comparable to LogBERT on
The LogFiT model contains built-in support for log sentence the Thunderbird dataset. In addition, LogFiT demonstrated
variability due to its large vocabulary of sub-word tokens superior performance over LogBERT in experiments that
(around50K).IncontrasttheDeepLogandLogBERTmodels tested for variations in the content of input log paragraphs,
would fail if they encounter variation in log sentences that which is attributed to its ability to handle out-of-vocabulary
9
tokens. LogFiT integrates with the popular HuggingFace [8] LunPinYuan,PengLiu,andSencunZhu,“Recompose
ecosystem, making it easy to adapt in future work. Overall, Event Sequences vs. Predict Next Events: A Novel
LogFiTpresentsaflexibleandfuture-proofapproachtodetect AnomalyDetectionApproachforDiscreteEventLogs”,
abnormal behavior in computer systems through language in ASIA CCS 2021 - Proc. 2021 ACM Asia Conf.
model adaptation and fine-tuning. Comput. Commun. Secur., vol. 1, Association for
Computing Machinery, 2021, pp. 336–348, ISBN:
9781450382878. DOI: 10.1145/3433210.3453098.
A. Future Work
[9] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace,
While the LogFiT model is intended to be used as a
Adam Dziedzic, Rishabh Krishnan, and Dawn Song,
threshold-based anomaly detector trained in a self-supervised
“Pretrained Transformers Improve Out-of-Distribution
manner, it can easily be converted to a classifier. If at some
Robustness”,AssociationforComputationalLinguistics
pointafterthemodelisdeployed,operatorsareabletocollect
(ACL), Apr. 2020, pp. 2744–2751. DOI: 10.18653/
and label anomaly log samples, the model can be converted
v1/2020.acl-main.244. arXiv: 2004.06100. [Online].
to a classifier by replacing its language modeling head with
Available: https://arxiv.org/abs/2004.06100v2.
a classification head. Additionally, the LogFiT LM can be
[10] Harold Ott, Jasmin Bogatinovski, Alexander Acker,
pretrained on diverse log datasets, allowing it to be used as
Sasho Nedelkoski, and Odej Kao, “Robust and
foundation for downstream NLP and log anomaly detection
transferable anomaly detection in log data using
tasks.
pre-trained language models”, in 2021 IEEE/ACM
International Workshop on Cloud Intelligence
REFERENCES
(CloudIntelligence), IEEE, 2021, pp. 19–24.
[1] RiskIQ, The evil internet minute 2019, 2019. [Online]. [11] Van-Hoang Le and Hongyu Zhang, “Log-based
Available: https : / / www . riskiq . com / resources / anomaly detection without log parsing”, in 2021 36th
infographic/evil-internet-minute-2019. IEEE/ACM International Conference on Automated
[2] Australia Department of Home Affairs, Australia’s SoftwareEngineering(ASE),IEEE,2021,pp.492–504.
cybersecuritystrategy2020,2020.[Online].Available: [12] Thorsten Wittkopp, Alexander Acker, Sasho
https://www.homeaffairs.gov.au/cyber-security-subsite/ Nedelkoski, et al., “A2Log: Attentive Augmented
files/cyber-security-strategy-2020.pdf. Log Anomaly Detection”, HICSS 2022 : Hawaii
[3] International Business Machines, Cost of a data breach International Conference on System Sciences, Sep.
report 2022, 2022. [Online]. Available: https://www. 2021. arXiv: 2109 . 09537. [Online]. Available:
ibm.com/security/data-breach. http://arxiv.org/abs/2109.09537.
[4] MinDu,FeifeiLi,GuinengZheng,andVivekSrikumar, [13] Guansong Pang, Chunhua Shen, Longbing Cao, and
“DeepLog: Anomaly detection and diagnosis Anton Van Den Hengel, “Deep learning for anomaly
from system logs through deep learning”, in detection: A review”, ACM Comput. Surv., vol. 54,
Proceedings of the ACM Conference on Computer and no. 2, Mar. 2021, ISSN: 0360-0300. DOI: 10.1145/
Communications Security, New York, NY, USA: ACM, 3439950. [Online]. Available: https://doi.org/10.1145/
2017, pp. 1285–1298, ISBN: 9781450349468. DOI: 3439950.
10.1145/3133956.3134015. [14] Shilin He, Pinjia He, Zhuangbin Chen, Tianyi Yang,
[5] Haixuan Guo, Shuhan Yuan, and Xintao Wu, Yuxin Su, and Michael R Lyu, “A survey on
“LogBERT: Log Anomaly Detection via BERT”, automated log analysis for reliability engineering”,
Proceedings of the International Joint Conference on ACM computing surveys (CSUR), vol. 54, no. 6,
Neural Networks, vol. 2021-July, Mar. 2021. DOI: 10. pp. 1–37, 2021.
1109/IJCNN52387.2021.9534113. arXiv: 2103.04475. [15] Raghavendra Chalapathy and Sanjay Chawla, “Deep
[Online]. Available: https://arxiv.org/abs/2103.04475v1. Learning for Anomaly Detection: A Survey”, 2019.
[6] Sasho Nedelkoski, Jasmin Bogatinovski, Alexander arXiv: 1901.03407. [Online]. Available: http://arxiv.
Acker, Jorge Cardoso, and Odej Kao, “Self-attentive org/abs/1901.03407.
classification-based anomaly detection in unstructured [16] Nengwen Zhao, Honglin Wang, Zeyan Li, et
logs”, in Proceedings - IEEE International Conference al., “An empirical investigation of practical log
on Data Mining, ICDM, vol. 2020-Novem, Institute of anomaly detection for online service systems”, in
Electrical and Electronics Engineers Inc., Nov. 2020, ESEC/FSE 2021 - Proceedings of the 29th ACM Joint
pp. 1196–1201, ISBN: 9781728183169. DOI: 10.1109/ Meeting European Software Engineering Conference
ICDM50108.2020.00148. arXiv: 2008.09340. and Symposium on the Foundations of Software
[7] VanHoangLeandHongyuZhang,Log-basedAnomaly Engineering, vol. 21, 2021, pp. 1404–1415, ISBN:
Detection with Deep Learning: How Far Are We?, 1. 9781450385626. DOI: 10.1145/3468264.3473933.
Association for Computing Machinery, 2022, vol. 1, [Online]. Available: https://doi.org/10.1145/3468264.
ISBN: 9781450392211. DOI: 10 . 1145 / 3510003 . 3473933.
3510155. arXiv: 2202.04301. [Online]. Available: http: [17] Xu Zhang, Yong Xu, Qingwei Lin, et al., “Robust
//arxiv.org/abs/2202.04301. log-based anomaly detection on unstable log data”,
in ESEC/FSE 2019 - Proceedings of the 2019 27th
10
ACM Joint Meeting European Software Engineering [26] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et
Conference and Symposium on the Foundations of al., “On the Opportunities and Risks of Foundation
Software Engineering, vol. 19, 2019, pp. 807–817, Models”, Aug. 2021. arXiv: 2108.07258. [Online].
ISBN: 9781450355728. DOI: 10 . 1145 / 3338906 . Available: http://arxiv.org/abs/2108.07258.
3338931. [Online]. Available: https://doi.org/10.1145/ [27] Iz Beltagy, Matthew E. Peters, and Arman Cohan,
3338906.3338931. “Longformer: The Long-Document Transformer”,
[18] Rakesh Bahadur Yadav, P. Santosh Kumar, and Sunita 2020. arXiv: 2004 . 05150. [Online]. Available:
VikrantDhavale,“ASurveyonLogAnomalyDetection http://arxiv.org/abs/2004.05150.
using Deep Learning”, in ICRITO 2020 - IEEE [28] YinhanLiu,MyleOtt,NamanGoyal,etal.,“RoBERTa:
8th International Conference on Reliability, Infocom A Robustly Optimized BERT Pretraining Approach”,
Technologies and Optimization (Trends and Future 2019. arXiv: 1907.11692. [Online]. Available: http://
Directions), Institute of Electrical and Electronics arxiv.org/abs/1907.11692.
Engineers Inc., Jun. 2020, pp. 1215–1220, ISBN: [29] Crispin Almodovar, Fariza Sabrina, Sarvnaz Karimi,
9781728170169. DOI: 10.1109/ICRITO48877.2020. and Salahuddin Azad, “Can language models help
9197818. in system security? investigating log anomaly
[19] Zhuangbin Chen, Jinyang Liu, Wenwei Gu, Yuxin detection using BERT”, in Proceedings of the
Su, and Michael R. Lyu, “Experience Report: Deep The 20th Annual Workshop of the Australasian
Learning-based System Log Analysis for Anomaly Language Technology Association, Adelaide, Australia:
Detection”, Jul. 2021. arXiv: 2107.05908. [Online]. Australasian Language Technology Association,
Available: http://arxiv.org/abs/2107.05908. Dec. 2022, pp. 139–147. [Online]. Available: https:
[20] Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. //aclanthology.org/2022.alta-1.19.
Lyu, “Drain: An Online Log Parsing Approach with [30] Leslie N. Smith and Nicholay Topin, “Super-
Fixed Depth Tree”, in Proceedings - 2017 IEEE 24th convergence: very fast training of neural networks
International Conference on Web Services, ICWS 2017, using large learning rates”, 2019, p. 36, ISBN:
2017, pp. 33–40, ISBN: 9781538607527. DOI: 10.1109/ 9781510626775. DOI: 10.1117/12.2520589. arXiv:
ICWS.2017.13. 1708.07120.
[21] Jacob Devlin, Ming Wei Chang, Kenton Lee, and [31] Jeremy Howard and Sebastian Ruder, “Universal
Kristina Toutanova, “BERT: Pre-training of deep language model fine-tuning for text classification”,
bidirectional transformers for language understanding”, in Proceedings of the 56th Annual Meeting of the
in NAACL HLT 2019 - 2019 Conference of the Association for Computational Linguistics (Volume 1:
North American Chapter of the Association for Long Papers), Melbourne, Australia: Association for
Computational Linguistics: Human Language Computational Linguistics, Jul. 2018, pp. 328–339.
Technologies - Proceedings of the Conference, vol. 1, DOI: 10.18653/v1/P18-1031. [Online]. Available: https:
Association for Computational Linguistics (ACL), //aclanthology.org/P18-1031.
Oct. 2019, pp. 4171–4186, ISBN: 9781950737130. [32] Suchin Gururangan, Ana Marasovic´, Swabha
arXiv: 1810 . 04805. [Online]. Available: https : Swayamdipta, et al., “Don’t Stop Pretraining: Adapt
//arxiv.org/abs/1810.04805v2. Language Models to Domains and Tasks”, 2020,
[22] Ganesh Jawahar, Benoˆıt Sagot, and Djame´ Seddah, pp. 8342–8360. DOI: 10.18653/v1/2020.acl-main.740.
“What does BERT learn about the structure of arXiv: 2004.10964.
language?”, in ACL 2019 - 57th Annu. Meet. [33] Ananya Kumar, Aditi Raghunathan, Robbie Jones,
Assoc. Comput. Linguist. Proc. Conf., Association Tengyu Ma, and Percy Liang, “Fine-Tuning can
for Computational Linguistics (ACL), 2020, Distort Pretrained Features and Underperform Out-of-
pp. 3651–3657, ISBN: 9781950737482. DOI: Distribution”, Feb. 2022. arXiv: 2202.10054. [Online].
10.18653/v1/p19-1356. [Online]. Available: https: Available: http://arxiv.org/abs/2202.10054.
//aclanthology.org/P19-1356. [34] Wei Xu, Ling Huang, Armando Fox, David Patterson,
[23] Yongjie Lin, Yi Chern Tan, and Robert Frank, “Open and Michael I Jordan, “Detecting large-scale system
Sesame:GettinginsideBERT’sLinguisticKnowledge”, problems by mining console logs”, in ICML 2010 -
2019,pp.241–253.DOI:10.18653/v1/w19-4825.arXiv: Proceedings,27thInternationalConferenceonMachine
1906.01698. [Online]. Available: https://github.com/. Learning, 2010, pp. 37–44, ISBN: 9781605589077.
[24] Yoav Goldberg, “Assessing BERT’s Syntactic [35] Adam Oliner and Jon Stearley, “What supercomputers
Abilities”, 2019. arXiv: 1901 . 05287. [Online]. say: A study of five system logs”, in Proceedings of
Available: http://arxiv.org/abs/1901.05287. the International Conference on Dependable Systems
[25] David Yenicelik, Florian Schmidt, and Yannic Kilcher, and Networks, 2007, pp. 575–584, ISBN: 0769528554.
“How does BERT capture semantics? A closer look DOI: 10.1109/DSN.2007.103.
at polysemous words”, Association for Computational [36] Leland McInnes, John Healy, and James Melville,
Linguistics (ACL), Nov. 2020, pp. 156–162. DOI: 10. “Umap: Uniform manifold approximation and
18653/v1/2020.blackboxnlp-1.15. [Online]. Available: projection for dimension reduction”, arXiv preprint
https://aclanthology.org/2020.blackboxnlp-1.15. arXiv:1802.03426, 2018.