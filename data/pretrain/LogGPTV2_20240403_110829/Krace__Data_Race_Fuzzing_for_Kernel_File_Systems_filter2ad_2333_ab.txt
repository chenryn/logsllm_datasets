stack, if the execution takes the order of 1 → 2 → 3 → 4 , then
2
block_rsv_release_bytes is inadvertently releasing bytes that
will be used by the fsync. Such a case might eventually cause
integer overflows in the reserve space but would probably
require thousands of concurrent file operations to trigger.
Data race is a special type of race condition, and hunting
data races in complex software involves two facets: 1) how to
confirm an execution is racy and 2) how to produce meaningful
executions by exploring code and thread-scheduling.
Dynamic data race detection algorithms. Most of the initial
works [28] found race conditions by relying on the happens-
before analysis [29]. However, one of the prime issues with
this approach is that it leads to false negatives. To improve the
detection accuracy, Eraser [30] proposed the lockset analysis,
in which users annotate the common lock/unlock methods and
find atomicity violations. Later, several works [31, 32] proposed
optimizations to either mitigate the overhead or minimize false
positives. To further improve the effectiveness of dynamic data
race detection, several works [33, 34] combined the idea of
happens-before relation with lockset analysis.
Unfortunately, most of these works target userspace programs
using simple synchronization primitives (e.g., those provided by
pthread or Java runtime), which only represent a small subset
of synchronization mechanisms available in the Linux kernel.
KRACE follows the same trend in combining happens-before
and lockset analysis, but unlike prior works, KRACE provides a
comprehensive framework that includes not only simple locking
methods, such as pessimistic locks (e.g., mutex, readers-writer
lock, spinlock, etc.), but also optimistic locking protocols,
such as sequence locks, and other forms of synchronization
mechanisms that imply more than just mutual exclusion, e.g.,
RCU [35] and other publisher-subscriber models.
Both lockset and happens-before analysis require code
annotations and suffer from incompleteness, i.e., a missing lock
model leads to false positives. Several works overcome this
issue with timing-based detection, i.e., a thread is delayed for
a certain duration at some memory accesses while the system
observes whether there are conflicting accesses to the same
memory during the delay [13, 36, 37]. Moreover, most of these
works resort to sampling [13, 34, 37–39], as an optimization
over completeness, to further minimize the runtime overhead
caused by tracking memory accesses or code paths.
However, complete timing-based detection relies on precise
control of thread execution speed and results in an enormous
search space (both in where to delay and how long to delay),
which again is not scalable in the kernel scope. As a result,
in terms of race detection, KRACE resorts to a trial-and-
error approach and fixes false positives introduced by ad-hoc
mechanisms along with the development. Fortunately, due to
the high coding standard and strict code review practice, ad-hoc
synchronization is not common in kernel file systems.
Code/thread-schedule exploration. The effectiveness of a
data race checker depends not only on the detection algorithm
but also on how well the checker can explore execution states
and cover as many code paths and thread interleavings as
possible. For code path exploration, prior detectors mostly rely
3
on manually written test suites [7, 36, 37] that do not capture
complicated cases. As shown in Figure 1, triggering the data
race would require a user thread to mkdir on the same block the
background uuid_rescan thread is working on, which (almost)
in no way can be specified in manually written test cases. An
alternative is to enumerate code paths statically [40–44], but
this is not scalable. Recent OS fuzzers adopt specification-
based syscall synthesization [5, 6, 21, 27]. However, these
fuzzers mostly focus on generating sequential programs instead
of multi-threaded programs and are not intended to explore
interleavings in syscall execution. KRACE adopts a similar
synthesization approach, but instead of focusing on single-
threaded sequences, KRACE evolves multi-threaded programs.
In the case of thread-schedule exploration, prior approaches
fall into three categories, in decreasing order of scalability but
increasing order of completeness: 1) stressing the random
scheduler with multiple trials [14]; 2) injecting delays at
runtime [13, 36, 37]; and 3) enumerating every possible thread
interleaving [7, 24]. KRACE uses delay injection, a trade-off
among scalability, practicality, and completeness.
Data race detection in kernels. KRACE shares its design
ideology with four prominent works [7, 24, 45, 46]. DataCol-
lider [45] is the first work that tackles this problem by using
randomized sampling of a small number of memory accesses
in conjunction with code breakpoint and data breakpoint
facilities for efficient sampling. DataCollider is simple enough
to detect several bugs in the Windows kernel modules. A similar
strategy is used by Syzkaller [21] with its Kernel Concurrent
Sanitizer [46] (KCSan) module. KCSan is a dynamic data
race detector that uses compiler instrumentation, i.e., software
watchpoints instead of hardware watchpoints, to detect bugs
on non-atomic accesses that violate the Linux kernel memory
model [47] using happens-before analysis.
Razzer presents an elegant pipeline for data race fuzzing, but
it can be further improved: 1) running points-to analysis [49]
on kernel file systems produces millions of may-alias pairs,
which is almost impossible to enumerate one by one; 2) even
SKI [7] focuses on comprehensive enumeration of thread
schedules with the PCT algorithm [48] and hardware break-
points. However, SKI permutes user threads only to find data
races in the syscall handlers and thus forgoes the opportunities
to find data races in kernel background threads. Furthermore,
even with user threads only, the number of permutations can
be huge to test thoroughly. In addition, the test suites used by
SKI may be too small to explore an OS for bugs.
Razzer [24] combines static analysis with fuzzing for
data race detection. In particular, Razzer first runs a points-
to analysis across the whole kernel code base to identity
potentially alias instruction pairs, i.e., memory accesses that
may point to the same memory location. After that, per each
alias pair identified, Razzer tries to generate syscalls that reach
the racy instructions at runtime. It does so with fuzzy syscall
generation [21, 27], and sequential syscall traces are generated
first. Once the alias relation is confirmed in the sequential
execution, the trace is then parallelized into multi-threaded
traces for actual data race detection.
for one alias pair, how to generate syscalls that may reach the
racy instructions is less clear. KRACE aims to improve both
aspects with the novel notion of alias coverage. Instead of pre-
calculating the search space with points-to analysis, KRACE
relies on coverage-guided fuzzing to expand the search in the
concurrency dimension gradually. Analogically, this is similar
to not enumerating every path in the control-flow graph but
instead using an edge-coverage bitmap to capture the search
progress. Doing so also eliminates the concern on how to
generate syscalls that lead execution to specific locations.
Fuzzing in general. Fuzzing has proven to be a practical
approach to find bugs in today’s software stack, both in the
userspace [16, 20, 50–54] and in the kernel space [5, 6, 21,
22, 27, 55]. Unfortunately, existing works cannot be trivially
adopted for data race fuzzing. One reason is that the main
focus of fuzzing has been on finding memory corruptions or
triggering assertions. Although Hydra [6] extends the scope
beyond memory errors into semantic bugs in file systems, it
does not provide any insight into finding data races.
Moreover, since modern coverage-guided fuzzing originates
and prospers from testing single-threaded programs such as
binutils, encoder/decoders, and the CGC and LAVA-M fuzzing
benchmarks, recent fuzzing efforts have focused on optimizing
fuzzers’ performance on single-threaded executions too, such as
approximating sequential execution with neural networks [51].
Not surprisingly, when the fuzzing practice is carried down to
the OS level [21, 22, 27, 55–59], the same sequential view of
program execution is inherited.
Although generating structured inputs has been a challenge
for kernel fuzzing, many improvements have been proposed.
For example, MoonShine [25] captures dependencies between
syscalls and DIFUZE [26] generates interface-aware inputs.
However, lacking a coverage metric and a seed evolution
algorithm to handle state exploration in the concurrency
dimension, existing OS fuzzers miss the opportunities to find
the broad spectrum of concurrency bugs, including data races.
The motivation behind KRACE is to fill this gap and to bring
coverage-guided fuzzing to the concurrency dimension.
Static and symbolic analysis on kernels. Although KRACE
is a dynamic analysis system, we are also aware of works that
aim to find concurrency bugs with static analysis [40–44]. Most
of these approaches rely on static lockset analysis and, hence,
suffer from the high false-positive rate caused by missing the
happens-before relation in the execution as well as the inherent
limitations of the points-to analysis. For instance, RacerX [41]
suffers from 50% false positives on the Linux kernel.
Beyond concurrency bugs, static analysis has proven effective
in finding many security issues in kenrel drivers. For example,
SymDrive [60] uses symbolic execution to emulate devices
and verify the properties of kernel drivers; DrChecker [61] is
capable of finding eight types of security issues by relaxing the
completely sound analysis on unbounded loops with mostly
sound versions. However, a major challenge in applying these
works to data race detection in file systems is their lack of
statefulness, i.e., although extremely effective in finding bugs
Fig. 2: A data race found by KRACE when symlink, readlink, and
truncate on the same inode run in parallel (simplified for illustration).
The race is on the indexed accesses to a global array G and occurs only
when B==C. A is lock-protected. This is one example showing branch
coverage is not sufficient in approximating execution states of highly
concurrent programs. It is not difficult to cover all branches in this
case with existing fuzzers, but to trigger the data race, merely covering
branches e1-e3 is not enough. The thread interleavings between four
instructions i1-i4 are equally important. The valid interleavings that
may trigger the data race are shown in Figure 3.
Fig. 3: Possible thread interleavings among the four instructions
shown in Figure 2. Out of the 6 interleavings, only 3 interleavings
( 1 / 4 , 2 / 3 , 5 / 6 ) are effective depending on A’s value when B and
C read it. Each effective interleaving results in different alias coverage.
Only 5 / 6 may trigger the data race.
within one syscall execution, they miss bugs that occur because
of the interaction between multiple syscalls, which happen to
be the majority of cases in file system operations.
III. A COVERAGE METRIC FOR CONCURRENT PROGRAMS
In this section, we show why branch coverage, the golden
metric for fuzzing, might be insufficient to represent the
exploration in the concurrency dimension, while at the same
time, why alias coverage, our new proposal, fits this purpose.
A. Branch coverage for the sequential dimension
Branch coverage originates from the program control-flow
graph (CFG), which is inherently a sequential view of program
4