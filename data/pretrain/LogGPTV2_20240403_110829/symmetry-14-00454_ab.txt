3.2. Anomaly DetectionHDFS logs are parsed to obtain the log template, and the event id of each log entry is obtained based on the log template. The value of some parameters can be used as an identifier for a specific execution sequence, such as block_id in HDFS logs. These identifiers can combine log entries together or unwrap log entries generated by concurrent processes to separate a single thread sequence. As shown in Table 3, the HDFS log event sequence in this article is generated based on block_id.Table 3. The demo of HDFS log events sequences.
| Sequence ID | Log Events Sequences |
|---|---|
| 0  1  2  3 4 5 |E5 E5 E5 E22 E11 E9 E11 E9 E11 E9 E26 E26 E26 E23 E23 E23 E21 E21 E21 E22 E5 E5 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26  E22 E5 E5 E5 E26 E26 E26 E11 E9 E11 E9 E11 E9 E23 E23 E23 E23 E21 E21 E21 E22 E5 E5 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26  E22 E5 E5 E5 E26 E26 E26 E11 E9 E11 E9 E11 E9 E4 E3 E3 E3 E4 E3 E4 E3 E3 E4 E3 E3 E23 E23 E23 E21 E21 E21  E5 E22 E5 E5 E11 E9 E11 E9 E11 E9 E26 E26 E26 |System log detection will be performed on the log event sequence (shown in Table 3) obtained by the log parser. Assuming that K = {k1, k2, k3,..., kn} is a log event sequence transformed by a log block, each log key represents a log path command at a certain time, and the entire log event sequence reflects the sequential execution path of the log. To detect the entire sequence, it is necessary to check whether each log event is normal. Let ki be one of the n K sequences, representing the log event to be detected. The model in DeepLog takes the influence of the forward sequence on ki to determine whether it is abnormal. For HDFS logs, this article not only considers the impact of the forward sequence on the next log event but also considers the impact of the backward sequence on the previous log event and combines them to further determine whether the event is abnormal. Figure 4 summarizes the classification settings. Suppose t is the sequence id of the next log event, w1 is the set of h most recent log events in the previous sequence, and w2 is the set of h most recent log events in the subsequent sequence. In other words, w1 = {mt−h,..., mt−2,log entry et. The same log event in w1 and w2 may appear multiple times. The output of mt−1}, w2 = {mt+h,..., mt+2, mt+1}, where each mt is in K, and is the log event from the
the training phase is two conditional probability distribution models Pr1 (mt = ki|w1) and Pr2 = (mt = ki|w2) for each ki ∈ K(i = 1, . . . , n).
Symmetry 2022, 14, 454 8 of 21
Figure 4. An overview of log events anomaly detection model.To extract context features and potential symmetry information [28] from sequence relationships, two long short-term memory networks (LSTMs) [29–31] are used in LogLS to train the preorder and postorder log event sequences. Each LSTM node has a hidden state ht−i and a cell state Ct−i, both of which are passed to the next node to initialize its state. The purpose is to obtain the probability of the current log event ki through the preorder and the subsequent log event ki through the model and then set a probability limit (parameters g1, g2) to determine whether the current log event is anomalous.The formula for forward propagation is: 
Input gate:
| at ι= | I
∑
i=1 | wiιxt i+ | H
∑
h=1 | whιbt−1 h | + | C
∑
c=1 | wcιst−1 | (1) |
|---|---|---|---|---|---|---|---|---|
| at ι= |I ∑ i=1 |bt ι= f (at ι) |bt ι= f (at ι) |bt ι= f (at ι) |+ |C ∑ c=1 |wcιst−1 |(2) |
Forget gate:
| at φ= | I
∑
i=1 | wiφxt i+ | H
∑
h=1 | whφbt−1 h | + | C
∑
c=1 | wcφst−1 c | (3) |
|---|---|---|---|---|---|---|---|---||---|---|---|---|---|---|---|---|---|
| at φ= |I ∑ i=1 |bt φ= f (at φ) |bt φ= f (at φ) |bt φ= f (at φ) |+ |C ∑ c=1 |wcφst−1 c |(4) |
Output gate: 
	I 	H 	C 
	at ω=∑wiωxt i+∑whωbt−1 h 	+∑wcωst−1 c 	(5) 	i=1 	h=1 	c=1
bt ω= f (at ω) 	(6)
	In Formulas (1)–(6), xt is the input at the current moment, st−1 is the state of all cells at the last moment, bt−1 h 	is the output of different LSTM memory blocks at the last moment, st cis the state of all cells at the current moment, w is the weight of each gate, and f is the activation function.The back propagation calculation formula is: 
Input gate:
δt ι= f ′(at ι) C
∑
c=1 g(at c)ϵt s (7)
Forget gate:
δt φ= f ′(at φ) C
∑
c=1 st−1 c ϵt s (8)
Output gate: 
	C 
	δt ω= f ′(at ω)∑h(st c)ϵt c 	(9) 	c=1
The process of backpropagation is actually the use of chain derivation to solve the gradient of each weight in the entire LSTM. In Formulas (7)–(9), δt ιis the gradient of the input gate, δt φis the gradient of the forget gate, and δt ωis the gradient of the output gate.In the training phase, the normally executed log entries are used as the dataset to train the model. The purpose is to allow the model to fully learn the normal execution mode of
Symmetry 2022, 14, 454 9 of 21the system log and avoid misjudgment as much as possible. Suppose a log event sequence is {E22, E5, E5, E5, E11, E9}. Given a window size of 2, the input preorder sequence and output label used to train the model are: {E22, E5→E5}, {E5, E5→E5}, {E5, E5→E11} and {E5, E11→ E9}. The input postorder sequence and output label are: {E9, E11→E5}, {E11, E5→E5}, {E5, E5→E5} and {E5, E5→E22}. The LogLS model is used to obtain the probability of the current log event based on the preorder and postorder sequences. The training phase needs to find an appropriate weight distribution so that the final output of the model produces the required labels and outputs them along with the input in the training dataset. In the training process, each input or output uses gradient descent to incrementally update these weights through loss minimization. In LogLS, the input includes h log event windows w1 and w2, and the output is the log event value immediately after w1 and the log event value before w2. We use categorical cross-entropy loss [32] for training. In the training phase, the length features and latent symmetry information of the log event sequence are counted for later use in the detection phase. After the model is trained, the verification set is used to adjust the parameters of the model to further obtain the optimal anomaly detection model parameter values.The basic method of the detection phase is the same as the training phase, but a preliminary filtering process is added. For the newly added log event sequence, first a preliminary judgment is made based on the length features obtained in the training phase, and then how to combine the two models for anomaly detection is decided. The length of the tentative log sequence is represented by K, and the following three situations will occur in anomaly detection.•	When K  2*W, in this case, it is necessary to select the prelog event sequence model 	and the postlog event sequence model to predict together. It also needs to be sub-	divided, because the length of the last W of the log sequence cannot be used in the 	postsequence log event sequence model. Because there is no subsequent input, only 	the previous log event sequence model is selected at this time. Two parameters g2 and 	g3 are involved here (g2 represents the first g2 digits of the log event probability pre-	dicted according to the previous log event model, and g3 represents the first g3 digits 	of the log event probability predicted according to the subsequent log event model).The log event sequence is input to the LogLS model, and the conditional probability of a log event to be detected is obtained. If the probability with the value of the parameters g1, g2, and g3 does not meet the parameter range, it means that it deviates from the normal log sequence and can be regarded as anomalous. Otherwise, other log events are continued to be judged until the entire log event sequence is determined. If no abnormality occurs, the log event sequence is normal.For example, when the input log event sequence is {E22, E5, E5, E5, E11, E9, E11, E23} and the given window size is 3, the sequence length is 8. This situation belongs to the third situation mentioned above, and the {E5, E11} in the sequence are predicted using the preorder and postorder models. First, for {E5}, the input w1 = {E22, E5, E5}, w2 = {E11, E9, E11}. Suppose that the log event probability obtained according to w1 is {E5:0.8, E22:0.2} and that the event probability obtained according to w2 is {E5:0.6, E23:0.2, E9:0.2}. The parameter g2 is 1, and the parameter g3 is 2, indicating that the log event predicted by w1 is E5, and the log time predicted by w2 is E5 or E23. Because the actual log event is E5, it indicates that the current log event is normal. Because the detection result is normal, the detection work continues backward. If the actual log event is not in the two predicted results, it will be judged as an abnormal log event sequence. {E9, E11, E23} in this sequence cannot be predicted using the abnormal log event sequence model obtainedSymmetry 2022, 14, 454 10 of 21in the subsequent sequence because there is no subsequent log sequence of the window size, so at this time, only the previous log event model is used to make the prediction. When the actual log event that needs to be predicted is {E23}, and the input w1 = {E11, E9, E11}, assuming that the event probability obtained according to this sequence {E5:0.6, E23:0.4}, the first g2 predicted result is taken as {E5}, which does not match the actual result. Therefore, it is regarded as an abnormal log event, and the sequence is finally judged as an abnormal sequence. The abnormal result is fed back to the user, and the user can judge whether a misjudgment is made. If a misjudgment occurs, the misjudgment sequence data are recorded, and the model is later adjusted through the update mechanism. If there was no misjudgment, the sequence was directly marked as abnormal.3.3. Renewal MechanismObviously, the training data may not cover all possible normal execution models. How to ensure the timeliness of the detection model and solve the emergence of new log execution models are problems that must be solved in system anomaly detection. For example, when the actual log event in the log event sequence is E12 and the predicted result according to w1 or w2 is E8, the event is regarded as abnormal. However, after manual inspection by the user, it is found that these two are normal log events. Therefore, LogLS provides users with a feedback mechanism [33] to relearn this new sequence using false positives to adjust its weight. When the sequence is entered again, it will be detected that both E12 and E8 will have the same probability and will be marked as normal.4. Evaluation
LogLS is implemented using Keras [34] with TensorFlow [35] as the backend. In this section, we show evaluations of the overall performance of LogLS to show its effectiveness in finding anomalies from large system log data.
4.1. DatasetThis article uses the authoritative dataset commonly used in system log anomaly detection: the HDFS log dataset disclosed by Wei Xu et al. [5]. HDFS log datasets are generated by running Hadoop-based map-reduce jobs on Amazon EC2 nodes. It is marked by experts in the Hadoop field. The dataset contains 11,197,954 log entries, of which 16,838 log entries are abnormal, including events such as “write exceptions”, accounting for approximately 2.9% of the total [36]. This dataset was constructed in 2009 and then widely used in the field of log anomaly detection. The dataset can be obtained in loghub. The specific information is shown in Table 4.Table 4. Summary of the HDFS datasets.
| System | #Time Span | #Data Size | #Nomalies | #Anomalies |
|---|---|---|---|---|
| HDFS |38.7 h |1.55 G |11,175,629 |16,838 |The HDFS dataset was eventually parsed into 575,059 log blocks, also known as log event sequences, of which 16,838 log blocks were marked as anomalies by Hadoop domain experts. In this paper, the HDFS dataset is divided into three parts, namely the training dataset, verification dataset and test dataset. The training dataset is used for the data samples for model fitting. Among them, there are only normal log event sequences, and 1% of the log event sequence dataset is selected, i.e., 4855 normal log event sequences. The validation dataset is a set of samples set aside separately during the model training process. It can be used to adjust the hyperparameters of the model and to make a preliminary assessment of the model’s capabilities. Among them, there are both normal log event sequences and abnormal log events, and 1/3 of the remaining log event sequences are selected, i.e., 166,009 normal log time sequences and 5051 abnormal log event sequences. The test dataset is used to evaluate the final detection ability of the model, but cannot beSymmetry 2022, 14, 454 11 of 21
used as a basis for algorithm selection such as parameter tuning and feature selection. It contains both normal log event sequences and abnormal log event sequences, which are the remaining 2/3 log event sequence [33,37]. The specific division is shown in Table 5.
Table 5. Set up of HDFS log datasets (unit: sequence).
|  |  | Number of Sessions |  |  |
|---|---|---|---|---||---|---|---|---|---|
| Log Dataset |Training Data |Validation Data |Test Data |n: Number of Log Keys |
| HDFS |4855 normal |166,009 normal; |387,357 normal; |29 |
| HDFS |4855 normal |5051 anormal |11,787 anormal |29 |
4.2. Evaluation MetricsTo evaluate the effectiveness of the model, in addition to the number of false positives (FP) and false negatives (FN), we also use standard metrics, such as accuracy, precision, re-call and F1-measure. Accuracy represents the percentage of logs that are correctly classified in the total logs. Precision represents the proportion of true anomalies among the detected anomalies. Recall represents the percentage of detected anomalies in the total anomalies in the dataset. The F1-measure is the weighted harmonic average of precision and recall and is a comprehensive evaluation index [38].| Accuracy = | TP + TN 
TP + TN + FP + FN | TP + TN 
TP + TN + FP + FN | TP + TN 
TP + TN + FP + FN | (10)