Non-volatile Memory (NVM)
Logging
2016.05.20
Takashi HORIKAWA
1
Who am I
Name
Takashi HORIKAWA, Ph. D.
Research interests
Performance evaluation of computer & communication systems, including
performance engineering of IT systems
with slightly shifting the focus of the research to CPU scalability
Papers
Latch-free data structures for DBMS: design, implementation, and evaluation,
SIGMOD ‘13
An Unexpected Scalability Bottleneck in a DBMS: A Hidden Pitfall in Implementing
Mutual Exclusion, PDCS ‘11
An approach for scalability-bottleneck solution: identification and elimination of
scalability bottlenecks in a DBMS, ICPE '11
A method for analysis and solution of scalability bottleneck in DBMS, SoICT '10
2
Contents
• Introduction
• Problems to be solved
• Implementation
• Evaluation
• Technical trends
• Conclusion
3
Introduction
Write Ahead Logging
Sync. vs Async. Commit
Difference in Performance
Fundamental idea for NVM Logging
Byte or Block Addressable NVM
Byte addressable NVMs
4
Write Ahead Logging
Widely used method to make transaction durable
Transaction
Worker process
Worker process
processing
Worker process
Worker process
Table, Index XLog record
Memory
Shared buffer WAL buffer
Asynchronous Write Synchronous Write
Storage
Table
XLog
Index
Data files WAL files
5
Write Ahead Logging
Widely used method to make transaction durable
Transaction
Worker process
Worker process
processing
Worker process
Worker process
Table, Index XLog record
Memory
Shared buffer WAL buffer
Asynchronous Write Synchronous Write
Write latency is included
in transaction response
Storage
Overhead
Table
XLog
Index
Data files WAL files
6
Sync. vs Async. Commit
Transaction becomes durable
Transaction processing WAL write (I/O)
t
7
Sync. vs Async. Commit
Transaction becomes durable
Transaction processing WAL write (I/O)
t
Sync. commit
Durability : OK
Notifying the client of
Response : Slow
the transaction commit
In sync. commit
the client is notified of the transaction commit
after the transaction becomes durable.
8
Sync. vs Async. Commit
Transaction becomes durable
Transaction processing WAL write (I/O)
t
Async. commit
Durability : NG
Notifying the client of
Response : Fast
the transaction commit
In async. commit
the client is notified of the transaction commit
before the transaction becomes durable.
9
Difference in Performance
(PGBENCH)
50000 50000
Async c
n
y
s
A
40000 40000
)SPT(
c
n
y
30000 30000 S
tuphguorhT tuphguorhT
20000 20000
10000 10000
Sync
0 0
0 40 80 120 160 0 40 80 120 160
Clients Clients
Disk-drive cache off Disk-drive cache on
10
Fundamental idea for NVM Logging
Transaction
Worker process
Worker process
processing
Worker process
Worker process
Table, Index XLog record
Memory
Shared buffer WAL buffer
Volatile Asynchronous Write Synchronous Write
Non-volatile
Storage
Table
XLog
Index
Data files WAL files
11
Fundamental idea for NVM Logging
Transaction
Worker process
Worker process
processing
Worker process
Worker process
Table, Index XLog record
Memory
Shared buffer WAL buffer
XLog record is stored as
NVM
non-volatile memory
Volatile Asynchronous Write Asynchronous Write
Non-volatile
Storage
Table
XLog
Index
Data files WAL files
12
Byte or Block Addressable NVM
CPU
Key device in
NVM Logging
byte
Accessed in units of
Store instruction
(Expected to be used from now on)
Byte addressable NVM
Memory
I/O operation
block
Accessed in units of
HD,
SSD Block addressable NVM
(Commonly used already)
Storage
13
Byte addressable NVMs
• Combination of existing technologies -- DRAM, SSD, battery （NVDIMM）
– AgigA Tech (Micron Technology)
– Viking Techonogy
– SK Hynix
Small ~100nS Ready-to-use
Capacity Access time Availability
Large ~μS Near future (?)
• Use of a new memory cell （Storage Class Memory）
– Phase Change Memory (PCM)
– Magnetic Random Access Memory (MRAM)
– Ferroelectric RAM (FRAM)
– The memristor
14
Problems to be solved
Fundamental idea for NVM Logging (Again)
It is not simple than it looks
Necessary condition for Recovery
Problems
Partial write
Unreachable XLog Record
CPU cache effect
15
Fundamental idea for NVM Logging
(Again)
Transaction
Worker process
Worker process
processing
Worker process
Worker process
Table, Index XLog record
Memory
Shared buffer WAL buffer
XLog record is stored as
NVM
non-volatile memory
Volatile Asynchronous Write Asynchronous Write
Non-volatile
Storage
Table
XLog
Index
Data files WAL files
16
It is not simple than it looks
• Naive implementation of NVM Logging
– Allocating WAL buffer in NVM area
– Using asynchronous commit mode
Not sufficient
• Problems are:
– Partial write
– Unreachable XLog Record
– CPU cache effect
17
Necessary condition for Recovery
XLR XLR XLR
m,N-2 m,N-1 m,N
XLog
LSN
• If the recovery process reads all XLR of
transaction m correctly
transaction m is possible to be recovered
• Else transaction m will be lost
A worker process finishes the commit of the transaction after all
XLRs of the transaction are stored in the non-volatile memory.
18
Partial write
• The recovery process will read an incomplete
XLR
if the system crashes in the middle of writing a XLR
XLR 1
WAL buffer
LSN
(CRC in the XLR may be effective but it is not perfect)
19
Unreachable XLog Record
• The recovery process cannot find a XLR of a
committed transaction
– A worker process finishes writing of XLR 3 before
another worker process begin to write XLR 2
XLR 1 XLR 2 XLR 3
WAL buffer
LSN
Length1
XLog reader finds the head position of a XLR by adding
the head position of the previous XLR and its length
20
CPU cache effect
• The recovery process will read inconsistent
XLR
CPU
Cache
Inconsistent
Memory
If the CPU internal cache uses write-back policy, a XLR written
by the CPU does not reach to the memory immediately
21
Implementation
Prototype architecture
Preventing partial write
Preventing an unreachable XLR
Use of Write-combined mode
GUC parameter for NVM Logging
Accessing NVM at recovery
Wrap around of WAL buffer
22
Prototype architecture
PostgreSQL
(9.6devel at the middle of March)
Shared buffer WAL buffer
Modified
user
kernel
File system open mmap
ext2pm
ext4 ext3 xfs
Newly added
Kernel module Pseudo NVM
(pram)
Similar to RAM disk but CPU cache mode differs
23
Preventing partial write
Tail
Start New tail
Reserve 1. Move the tail pointer to reserve buffer area
Write data 2. Write the XLR data other than length field
( At this point length field of XLR in XLog buffer is 0)
Write len 3. Write length field of the XLR
LSN
t
XLog Record
If XLog reader find a XLR whose length field is not zero,
all XLR data is written in the XLog buffer.
24
Preventing an unreachable XLR
XLR 1
Start
Reserve 1
XLR 2
Write 1 WP waits until commit
Write
becomes able to be finished
Reserve 2
XLR 3
Write 2
Reserve 3
Write 3
Write Write
Reserve 4
Write 4
Write
LSN
WP finishes the commit as
t
all previous XLRs are written
25
Wait control
• A wait mechanism is already implemented in
PostgreSQL
static XLogRecPtr
WaitXLogInsertionsToFinish(XLogRecPtr upto)
If any XLR with a smaller LSN than the upto parameter is not
finished to copy in the WAL buffer, the worker process sleeps
until all of those XLRs are copied in the WAL buffer.
NVM Logging implements the wait mechanism by using
this function
26
Use of Write-combined mode
PostgreSQL
Shared buffer WAL buffer
user
kernel
File system open mmap
ext2pm
ext4 ext3 xfs
Kernel module Pseudo NVM
(pram)
Write-combined mode, which is a variation of write-through,
is set for the memory pages for pseudo NVM.
27
GUC parameter for NVM Logging
• NVM Logging is enabled through one GUC
parameter
(described in postgresql.conf)
– PRAM_FILE_NAME = “NVM File name”
• When PRAM_FILE_NAME is set
– XLOGShmemInit() invokes open() and mmap() to
“NVM File name” and uses the memory area for WAL
buffer
– CopyXLogRecordToWAL() copies XLR in WAL buffer
according to the procedure that prevents partial write
28
Accessing NVM at recovery
XLogReader
minLSN maxLSN
XLog buffer in NVM
WAL files
LSN
Segment number n-2 n-1 n n+1 n+2 n+3
XLogReader accesses NVM XLog buffer to obtain XLog records
whose LSN is between minLSN and maxLSN
29
Wrap around of WAL buffer
Logical view
NVM size
Saved in XLog files Written in NVM only LSN
EvaluatedUpto Current write point InitializedUpto
30
Wrap around of WAL buffer
Logical view
NVM size
Saved in XLog files Written in NVM only LSN
EvaluatedUpto Current write point InitializedUpto
NVM size
Physical view EvaluatedUpto
WAL segment
(n-1) th round
n th round
LSN
Current write point InitializedUpto
31
Evaluation
Experimental Setup
Performance
PGBENCH
DBT-2
Durability
Durability test
Result
Write amplification Reduction
32
Experimental setup
• DB server
– CPU: E5-2650 v2 x 2 (16 cores)
– Memory: 64GB
– Storage
• RAID0: 200GB SSD x 2 for data
• RAID0: 1TB ATA HD x 4 for WAL
• Client
– CPU: E7420 x 4 (16 cores)
– Memory: 8GB
• Network
– GB ether x 1
33
PGBENCH Performance
50000 50000
c
Async yn g
s n
A gi
g
o
L
M
40000 40000 V
N
g )SPT( )SPT(
n
gi nc
g y
30000 o 30000 S
L
M tuphguorhT tuphguorhT
V
N
20000 20000
10000 10000
Sync
0 0
0 40 80 120 160 0 40 80 120 160
Clients Clients
Disk-drive cache off Disk-drive cache on
34
DBT-2 Performance
Async
Async
400000 400000
NVM Loggingg
NVM Logging
)MPTON( )MPTON(
300000 300000 Sync
200000 200000 tuphguorhT tuphguorhT
100000 100000
Sync
0 0
0 10 20 30 40 50 0 10 20 30 40 50
Clients Clients
Disk-drive cache off Disk-drive cache on
35
Write amplification Reduction
Page of the storage
Write
Sync.
Write
commit
Write
Write
Write
The tail WAL block is written in at every commit
NVM
Write
Logging
The tail WAL block is written in once
36
Durability test
Fault
Client
p
DBMS
Transaction
while (1) {
Begin
key(=Kp) value
Select value From table1 Where key = Kp
(v = value + 1)
Update table1 set value = v Where key = Kp
(… same for table 2)
End
table2
(record v as committed transaction)
table1
}
After the recovery, durability is examined by checking whether
the value of table1 and table2 is equal and
value is equal to or greater than v that each client recorded as the
result of the last transaction.
37
Results
• The results were just what we expected
– Durability is ensured
• Sync. commit, NVM Logging
– Durability is not ensured
• Async. commit
38
Technical trends
NVDIMM for DB servers
Programming support for NVM
39
NVDIMM in DB Servers
• NVDIMM-N Standardization
– JEDEC Hybrid Memory Task Group
– SNIA NVDIMM SIG
• Server product: HP ProLiant XL230a Server
– Up to 2 Intel® Xeon® E5-2600 v3 Series, 6/8/10/12/14/16
Cores (16)
– DDR4, (512GB max), support for NVDIMM
– …
http://community.hpe.com/t5/Servers-The-Right-Compute/Address-your-
Compute-needs-with-HP-ProLiant-Gen9/ba-p/6794213#.VybkulK3GA9
DB server with NVDIMM is just around the corner!!
40
Programming support for NVM
• pmem.io
– The Linux NVM Library builds on the Direct Access
(DAX) changes under development in Linux.
– This project focuses specifically on how persistent
memory is exposed to server-class applications
which will explicitly manage the placement of data
among the three tiers (volatile memory, persistent
memory, and storage).
http://pmem.io/
41
Conclusion
• NVM is becoming commodity
– NVDIMM is already shipped as a product
– Servers began to equipped with NVDIMM
• Benefits of NVM Logging
– Performance improvement
Almost the same as async. commit
– Durability ensurance
Similar to sync. commit
– Write amplitude reduction
Good for SSD lifetime
42
Future work
• Bring to a state acceptable for the mainline
– Cope with standard for NVM access
• libpmem is a promising candidate
– Check the operation in using real NVM
43
That’s it
Thank you for listening
44