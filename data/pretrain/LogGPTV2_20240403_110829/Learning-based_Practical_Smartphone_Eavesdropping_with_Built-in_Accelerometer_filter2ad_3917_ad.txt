minimum value is approximately the magnitude of the noise
signal. After that, we traverse through the smoothed magnitude
sequence and locate all the regions with magnitudes higher
than a threshold of 0.8Mmin + 0.2Mmax. Each located region
indicates the existence of a speech signal. In order to make sure
that the segmented signal will cover the whole speech signal,
the start and end points of each located region are then moved
forward and backward by 100 and 200 samples, respectively.
The cutting points calculated from each setting are marked in
Fig. 11. Finally, we use the obtained cutting points to segment
the ﬁltered acceleration signal into multiple short signals, each
of which corresponds to a single word.
Signal-to-spectrogram conversion: To generate the spec-
trogram of a single-word signal, we ﬁrst divide the signal into
multiple short segments with a ﬁxed overlap. The lengths of the
segment and the overlap are set as 128 and 120 respectively.
We then window each segment with a Hamming window and
calculate its spectrum through STFT, which generates a series
of complex coefﬁcients for each segment. The signal along
each axis is now converted into a STFT matrix that records
the magnitude and phase for each time and frequency. Finally,
the 2D spectrogram can be calculated through
spectrogram{x(n)}(m, w) = |ST F T{x(n)}(m, w)|2,
(1)
where x(n) and |ST F T{x(n)}(m, w)| respectively represents
a single-axis acceleration signal and the magnitude of its
corresponding STFT matrix. Because we have acceleration
signals along three axes, three spectrograms can be obtained
for each single-word signal. For illustration, Fig. 12 plot the
spectrogram (z-axis) of the ﬁrst single-word signal of each
setting. The frequency components below 20 Hz are close to
zero due to the high-pass ﬁltering process.
Spectrogram-Images: To directly feed the spectrograms
into the neural networks used in computer vision tasks, we
further convert the three 2-D spectrograms of a signal into
one RGB image in PNG format. To do so, we ﬁrst ﬁt the
three m × n spectrograms into one m × n × 3 tensor. Then
we take the square root of all the elements in the tensor and
map the obtained values to integers between 0 and 255. The
reason of taking the square root is that the majority of the
elements in the original 2-D spectrograms are very close to
zero. Directly mapping these elements to integers between 0
(a) Table setting
(b) Handhold setting
Fig. 13. The spectrogram-image of the ﬁrst single-word signal. These images
cover the frequencies from 80Hz to 300Hz.
and 255 will result in considerable information loss. Finally,
we export the m × n × 3 tensor as an image in PNG format.
In the obtained spectrogram-image, the red, green, and blue
channels correspond to the x-axis, y-axis, and z-axis of the
acceleration signal, respectively. For the recognition task, the
spectrogram-images are cropped to the frequency range from
80 Hz to 300 Hz in order to reduce the impact of self-
noise. Fig. 13 plots the spectrogram-image of the ﬁrst single-
word signal of each setting. A brighter region indicates that
the acceleration signal has stronger energy at that frequency
range during that time period. It can be observed that the blue
channel (the acceleration signal along the z-axis) provides the
most speech information under both settings.
B. Recognition
With the above preprocessing operations,
the resulting
acceleration spectrogram-images can be fed into various stan-
dardized neural networks such as VGG [36], ResNet [22],
Wide-ResNet [43], and DenseNet [24] after resizing. We now
detail the design of our recognition module.
Spectrogram-image Resizing To feed those spectrogram-
images into standardized computer vision networks, it is better
to resize them into n × n × 3 images. Note that ﬁne-grained
information and correlations of the acceleration-spectrograms
may be inﬂuential to the recognition results, especially the
results of speaker identiﬁcation. To preserve sufﬁcient infor-
mation, we resize the spectrogram-images into 224× 224× 3.
Network Choice Generally, we choose DenseNet as the
base network for all our recognition tasks. Compared with
traditional deep networks like VGG and ResNet, DenseNet
introduces connections between each layer and all its preceding
layers, i.e., totally (L+1)L
connections in an L-layer network.
For instance, as shown in the diagram of a common block
in DenseNet (Fig. 14(a)), the ﬁrst to fourth layers all have
direct links to the ﬁfth layer. In another word, l-th takes the
concatenation of feature maps from 0-th layer (input image)
to the (l − 1)-th layer as input, which can be mathematically
represented by
2
xl = Hl([x0, x1, ..., xl−1]).
(2)
Hl and xl denote the function and the feature map of the
l-th layer, respectively. [x0, x1, ..., xl−1] denote the concate-
nation of the feature maps of the 0-th layer to the l − 1-th
layer. These direct connections enable all layers to receive
and reuse the features from their preceding layers, and thus,
DenseNet does not have to use some redundant parameters
11
(a) Diagram of Dense Block
(a) Diagram of Residual Block
(b) Diagram of DenseNet
Fig. 14. The network structure of DenseNet. The connections are established
between every layer and its previous layers to improve information ﬂow.
or nodes to maintain the information from the previous lay-
ers. Therefore, DenseNet can use fewer nodes (parameters)
to achieve performance that is comparable with VGG and
ResNet. Moreover, improved ﬂow of information and gradients
throughout the network also alleviates gradient-vanishing and
makes DenseNet easier to train. Empirically, we found that
in our recognition tasks, DenseNet indeed achieves the best
accuracy with fewer parameters and less computational cost
(compared with VGG and ResNet). Fig. 14(b) demonstrates the
overall network structure we utilize, which consists of multiple
dense blocks shown in Fig. 14(a).
Training Process In the training stage, we use the cross-
entropy as the training loss and optimize the model weights
by a piecewise momentum optimizer to learn more generaliz-
able features and also facilitate convergence. Speciﬁcally, the
adaptive momentum optimization process is ﬁrst executed by
a large step size (e.g., 0.1) to learn generalizable features and
then ﬁne-tuned by smaller step sizes to facilitate convergence.
We also add weight decay into the training loss and set the
dropout rate as 0.3 to enhance generalizability.
C. Reconstruction
Except for recognition, reconstruction of the speech signals
from the corresponding acceleration signals (spectrograms)
is also one function we want
to include in our proposed
system, since this function can be used for double-checking
our recognition results. Note that although accelerometers
in current smartphones can only pick up the low frequency
components, many components in the high frequency band are
mainly the harmonics of these fundamental frequency compo-
nents, which makes it possible for us to reconstruct speech
signals with enhanced sampling rates from the corresponding
acceleration signals. To achieve speech-signal reconstruction,
we ﬁrst reconstruct the speech-spectrograms by the following
reconstruction network, with the acceleration spectrogram-
images as input. Then the speech signals are estimated from the
reconstructed speech-spectrograms by the Grifﬁn-Lim algo-
rithm proposed in [20]. Next, we will detail the reconstruction
network & the speech signal estimation method.
(b) Diagram of Reconstruction Network
Fig. 15. The network structure of our reconstruction network. It will much
easier to optimize residual mappings in (3) by residual blocks.
1) Reconstruction Network: The reconstruction network
consists of three sub-networks, i.e., an encoder, residual blocks,
and a decoder. The input of the reconstruction network is a
128× 128× 3 spectrogram-image which covers the frequency
components from 20 to 500 Hz. Each channel corresponds
to one axis of the acceleration signal. However, a problem
of standardizing the input size here is that the acceleration
signals may have different time-lengths due to the various
time-lengths of speech signals from different speakers, but
resizing the spectrogram-images is not desired here since
the time-scale information is better to be maintained in the
reconstruction process. A simple solution is to repeat
the
speech signal until it reaches a pre-deﬁned time-length. This
solution is valid because the reconstruction task enforces no
restriction on the content of the speech/acceleration signals
(spectrograms), unlike the above recognition module whose
input has to be a single word spectrogram-image. The output
of the reconstruction network is a 384 × 128 gray image
that represents the corresponding speech-spectrogram since the
speech signal only has one axis. Due to the limited sampling
rate of the accelerometer, our reconstruction network only
aims at reconstructing the frequency components of the speech
signal from 0 to 1500 Hz.
Encoder The ﬁrst sub-network is an encoder for encod-
ing the acceleration spectrogram-images (i.e., conv layers in
Fig. 15(b)). The encoder starts with a convolutional layer with
32 kernels of size 9 × 9 × 3 to learn the large-scale features,
followed by two convolutional layers with 64 kernels of size
3 × 3 × 32 and 128 kernels of size 3 × 3 × 64 respectively to
learn the small-scale features. Besides, a stride of 2 is applied
on the ﬁrst two layers for downsampling.
Residual Blocks Inspired by the architecture of [25], we
add ﬁve residual blocks (as shown in Fig. 15(a)) after the
encoder to explicitly let the features ﬁt a residual mapping
12
H(·), i.e.,
H(x) = F(x, Wi) + x,
(3)
where F(x, Wi) are the nonlinear mappings learned by con-
volutional layers. Considering the structural similarity between
the spectrograms of the acceleration and speech signals, it is
very likely that identity mapping is an optimal/near optimal
mapping to establish some feature-connections. When the
optimal mapping is or close to an identity function,
is
easier to optimize H than an unreferenced block F. This
is because pushing the parameters of F into zero should be
easier than optimizing F as an identity mapping. Therefore,
we add a number of residual blocks H in the middle of our
reconstruction network (i.e., residual blocks in Fig. 15(b)).
it
Decoder Finally,
the speech-spectrograms are decoded
from the features learned by the encoder & residual blocks
by a decoder (i.e., deconv layers in Fig. 15(b)). The decoder
also consists of 3 deconvolutional layers, with respectively 64
kernels of size 3 × 3 × 128, 32 kernels of size 3 × 3 × 64,
and 3 kernels of size 9 × 9 × 32. A stride of 1/2 is applied
on the ﬁrst two layers for upsampling. The initial output of
the decoder is a 128 × 128 × 3 matrix, and the matrix will be
further resized into a 384 × 128 gray image to represent the
corresponding speech-spectrogram as mentioned before.
Training Process Compared with recognition, reconstruc-
tion is a task whose training process is more unstable and
computationally expensive. The instability issue is probably
caused by the sparse outliers in a training minibatch, due
to the sparsity of spectrograms. To ﬁx it, we employ the L1
distance between the reconstructed images and the targeted
images as the training loss instead of the MSE loss in [41].
This is because L1 loss is more robust than the MSE loss to
outliers [11]. Besides, We also apply a weight decay on the L1
loss to enhance generalizability. To reduce the computational
cost, we accelerate the optimization process by applying a
time-based decay to the learning rate. Speciﬁcally, we use
a momentum optimizer with a learning-rate scheduler that
decays the learning rate by a factor of 0.9 per training epoch.
2) Speech Signal Estimation: Grifﬁn-Lim algorithm is an
iterative algorithm for signal estimation from spectrograms.
And each iteration contains two steps: the ﬁrst step is to modify
the STFT of the current signal estimation by the spectrogram;
The second step is to update the current signal estimation by
the modiﬁed STFT. Next, we will detail the two steps.
Modify STFT Given the current estimation of the speech
signal xi[n] in the ith iteration and the reconstructed mag-
nitude (square root of spectrogram) (cid:107)Y (m, w)(cid:107), the STFT
X i(m, w) of xi[n] is modiﬁed as
ˆX i+1(m, w) = X i(m, w)
(cid:107)Y (m, w)(cid:107)
(cid:107)X i(m, w)(cid:107) ,
(4)
to ensure the magnitude of the modiﬁed STFT ˆX i+1(m, w) to
be same as the reconstructed magnitude (cid:107)Y (m, w)(cid:107).
Update Signal Estimation Note that the modiﬁed STFT
ˆX i+1(m, w) may be not a valid STFT if there is no signal
whose STFT is given by ˆX i+1(m, w). In this sense, we prefer
to ﬁnd a sequence xi+1(n) whose STFT X i+1(m, w) is the
to the modiﬁed STFT ˆX i+1(m, w) by minimizing
closest
the following mean square error between X i+1(m, w) and
ˆX i+1(m, w),
m=−∞
w=−∞
+∞(cid:88)
[X i+1(m, w) − ˆX i+1(m, w)]2.
+∞(cid:88)
(cid:80)+∞
The solution to the above minimization problem is
(cid:80)+∞
m=−∞ w(n − mS)ˆxi+1(m, w)
(cid:82) π
m=−∞ w2(n − mS)
w=−π
xi+1(n) =
ˆX i+1(m, w)e−jwndw and S
where ˆxi+1(m, w) = 1
2π
refers to the sampling period w.r.t. n. These two steps are
iterated for multiple steps until convergence, and the ﬁnal
xi(n) is output as the estimation of the speech signal.
(5)
,
(6)
VI.
IMPLEMENTATION AND EVALUATION
A. Experimental setup and Datasets
We mainly evaluate our proposed system on accelerometer
measurements collected from a Samsung S8. The scalability
of the proposed model is evaluated in section VI-D. For each
speciﬁc setting, we play a series of speech signals on the
smartphone and collect accelerometer readings through the
third party Android application AccDataRec running in the
background.
The speech signals are mainly from two datasets. The ﬁrst
dataset consists of 10k single-digit signals from 20 speakers,
which are from the AudioMNIST dataset2. The signals in this
dataset are concatenated into long audio signals with an inter-
val of 0.1 seconds in order to simulate the scenario where the
victim is telling others his/her passwords. The second dataset
consists of 36 × 260 digits+letters speech signals collected
from volunteers. We hire volunteers from the university and
collect data in the lab. The volunteers were asked to hold the
smartphone and read a long series of digits and letters with the
speech rate they would use for telling others their passwords.
There are totally 36 classes including 10 digits (0-9) plus 26
letters (A-Z), and each class contains 260 samples collected
from 10 speakers. We collect accelerometer readings from
these two speech sources and evaluate the proposed system
under different settings. We note that all the experimental
results presented in this paper are user-independent. For each
setting under investigation, we randomly divide all collected
signals into 80% training data and 20% testing data. In the
following, we only report the testing accuracy.
B. Recognition
As described earlier, the state-of-the-art (SOTA) model [32]
uses the gyroscope on smartphones to capture speech signals
emitted by a loudspeaker placed on the same solid surface. To
make a fair comparison, we ﬁrst evaluate the performance of
our model in a similar setting – i.e., place the smartphone on
a table. Table V lists the top1, top3, top5 (testing) accuracy
of our system in digit-recognition, digit+letter-recognition, and
speaker-identiﬁcation. Top N accuracy is the probability that
the correct label is within the top N classes predicted by
our network. Remarkably, the top1 accuracy of our model on
digit-recognition in the user-independent setting even surpasses
2https://github.com/soerenab/AudioMNIST
13
the SOTA accuracy in the user-dependent setting by 13%.
Our system also achieves 55% top1 accuracy and 87% top5
accuracy on recognizing 10 digits plus 26 letters (totally
36 classes). In terms of speaker-identiﬁcation, our system
achieves 70% accuracy on classifying 20 speakers, while the
previous SOTA model only has 50% accuracy on classifying 10
speakers. Overall, our model achieves new SOTA results in all
the tasks. The increase of accuracy is not only because of the
usage of an advanced model, but also because of the increased
sampling rate and the proposed setup. Our setup allows the
voice signal to have a much more signiﬁcant impact on the
motion sensor, and thus the SNR of the acceleration signal
is signiﬁcantly improved compared with the SOTA setup. As
will be shown in table VII and XII, the recognition accuracy
increases smoothly with the SNR of the acceleration signal
and the sampling rate of the accelerometer.
TABLE V.
COMPARISON BETWEEN OUR RESULTS AND
STATE-OF-THE-ART (SOTA) RESULTS FROM [32].
Tasks
Digits
Digits +
Letters
Speakers
Our model (DenseNet)
top1 acc
top3 acc
top5 acc
78%
55%
96%
78%
99%
87%
70% (20)
88% (20)
95% (20)
SOTA (user
dependent)
65%
SOTA (user
independent)
26%
-
-