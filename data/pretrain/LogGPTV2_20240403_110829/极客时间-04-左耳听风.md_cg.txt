# 弹力设计总图首先，我们的服务不能是单点，所以，我们需要在架构中冗余服务，也就是说有多个服务的副本。这需要使用到的具体技术有：-   负载均衡 + 服务健康检查--可以使用像 Nginx 或 HAProxy 这样的技术；-   服务发现 + 动态路由 + 服务健康检查，比如 Consul 或 ZooKeeper；-   自动化运维，Kubernetes 服务调度、伸缩和故障迁移。然后，我们需要隔离我们的业务，要隔离我们的服务我们就需要对服务进行解耦和拆分，这需要使用到以前的相关技术。``{=html}-   bulkheads 模式：业务分片 、用户分片、数据库拆分。-   自包含系统：所谓自包含的系统是从单体到微服务的中间状态，其把一组密切相关的微服务给拆分出来，只需要做到没有外部依赖就行。-   异步通讯：服务发现、事件驱动、消息队列、业务工作流。-   自动化运维：需要一个服务调用链和性能监控的监控系统。然后，接下来，我们就要进行和能让整个架构接受失败的相关处理设计，也就是所谓的容错设计。这会用到下面的这些技术。-   错误方面：调用重试 + 熔断 + 服务的幂等性设计。-   一致性方面：强一致性使用两阶段提交、最终一致性使用异步通讯方式。-   流控方面：使用限流 + 降级技术。-   自动化运维方面：网关流量调度，服务监控。我不敢保证有上面这些技术可以解决所有的问题，但是，只要我们设计得当，绝大多数的问题应该是可以扛得住的了。下面我画一个图来表示一下。![](Images/5d1f426b2d860970e3aa77f978187538.png){savepage-src="https://static001.geekbang.org/resource/image/f9/2b/f9e6efa6202103a14d358ff6c80f0a2b.png"}在上面这个图上，我们可以看到，有三大块的东西。-   冗余服务。通过冗余服务的复本数可以消除单点故障。这需要服务发现，负载均衡，动态路由和健康检查四个功能或组件。-   服务解耦。通过解耦可以做到把业务隔离开来，不让服务间受影响，这样就可以有更好的稳定性。在水平层面上，需要把业务或用户分片分区（业分做隔离，用户做多租户）。在垂直层面上，需要异步通讯机制。因为应用被分解成了一个一个的服务，所以在服务的编排和聚合上，需要有工作流（像    Spring 的 Stream 或 Akka 的 flow 或是 AWS 的 Simple    Workflow）来把服务给串联起来。而一致性的问题又需要业务补偿机制来做反向交易。-   服务容错。服务容错方面，需要有重试机制，重试机制会带来幂等操作，对于服务保护来说，熔断，限流，降级都是为了保护整个系统的稳定性，并在可用性和一致性方面在出错的情况下做一部分的妥协。当然，除了这一切的架构设计外，你还需要一个或多个自动运维的工具，否则，如果是人肉运维的话，那么在故障发生的时候，不能及时地做出运维决定，也就空有这些弹力设计了。比如：监控到服务性能不够了，就自动或半自动地开始进行限流或降级。
# 弹力设计开发和运维对于运维工具来说，你至少需要两个系统：-   一个是像 APM 这样的服务监控；-   另一个是服务调度的系统，如：Docker + Kubernetes。此外，如果你需要一个开发架构来让整个开发团队在同一个标准下开发上面的这些东西，这里，SpringCloud 就是不二之选了。关于 Spring Cloud 和Kubernetes，它们都是为了微服务而生，但它们没有什么可比性，因为，前者偏开发，后者偏运维。我们来看一下它们的差别。![](Images/f0fe53ba91f51ecde14cab3719285e94.png){savepage-src="https://static001.geekbang.org/resource/image/35/f4/35cd0722f99f91c904944ac1bbdd56f4.png"}\（图片来自：Deploying Microservices: Spring Cloud vs Kubernetes）从上表我们可以得知：-   Spring Cloud 有一套丰富且集成良好的 Java    库，作为应用栈的一部分解决所有运行时问题。因此，微服务本身可以通过库和运行时代理解决客户端服务发现、负载均衡、配置更新、统计跟踪等。工作模式就像单实例服务集群。（译者注：集群中    master 节点工作：当 master 挂掉后，slave    节点被选举顶替。）并且一批工作也是在 JVM 中被管理。-   Kubernetes    不是针对语言的，而是针对容器的，所以，它是以通用的方式为所有语言解决分布式计算问题。Kubernetes    提供了配置管理、服务发现、负载均衡、跟踪、统计、单实例、平台级和应用栈之外的调度工作。该应用不需要任何客户端逻辑的库或代理程序，可以用任何语言编写。下图是微服务所需的关键技术，以及这些技术中在 Spring Cloud 和 Kubernetes的涵盖面。![](Images/f8c073d8df700dea2e5f17a3948d1afe.png){savepage-src="https://static001.geekbang.org/resource/image/dc/af/dcab89f031d1a7083b4f0b3091873caf.png"}\（图片来自：Deploying Microservices: Spring Cloud vs Kubernetes）两个平台依靠相似的第三方工具，如 ELK 和 EFK stacks, tracing libraries等。Hystrix 和 Spring Boot等库，在两个环境中都表现良好。很多情况下，Spring Cloud 和 Kubernetes可以形成互补，组建出更强大的解决方案（例如 KubeFlix 和 Spring CloudKubernetes）。下图是在 Kubernetes 上使用 Spring Cloud可以表现出来的整体特性。要做出一个可运维的分布式系统，除了在架构上的设计之外，还需要一整套的用来支撑分布式系统的管控系统，也就是所谓的运维系统。要做到这些，不是靠几个人几天就可以完成的。这需要我们根据自己的业务特点来规划相关的实施路径。![](Images/f386d87040ac374aa0c5a8d173af405b.png){savepage-src="https://static001.geekbang.org/resource/image/41/6a/41e9f7a084e6c81fcb3bb42d43b0076a.png"}\（图片来自：Deploying Microservices: Spring Cloud vs Kubernetes）上面这张图中，对于所有的特性，都列举了一些相关的软件和一些设计的重点，其中红色的是运维层面的和Spring Cloud 和 Kubernetes 不相关的，绿色的 Spring Cloud提供的开发框架，蓝色的是 Kubernetes 相关的重要功能。从今天看下来，微服务的最佳实践在未来有可能会成为 SpringCloud 和Kubernetes 的天下了。这个让我们拭目以待。我在本篇文章中总结了整个弹力设计，提供了一张总图，并介绍了开发运维的实践。希望对你有帮助。也欢迎你分享一下你对弹力设计和弹力设计系列文章的感想。文末给出了《分布式系统设计模式》系列文章的目录，希望你能在这个列表里找到自己感兴趣的内容。-   弹力设计篇    -   [认识故障和弹力设计](https://time.geekbang.org/column/article/3912)    -   [隔离设计        Bulkheads](https://time.geekbang.org/column/article/3917)    -   [异步通讯设计        Asynchronous](https://time.geekbang.org/column/article/3926)    -   [幂等性设计        Idempotency](https://time.geekbang.org/column/article/4050)    -   [服务的状态        State](https://time.geekbang.org/column/article/4086)    -   [补偿事务 Compensating        Transaction](https://time.geekbang.org/column/article/4087)    -   [重试设计 Retry](https://time.geekbang.org/column/article/4121)    -   [熔断设计 Circuit        Breaker](https://time.geekbang.org/column/article/4241)    -   [限流设计        Throttle](https://time.geekbang.org/column/article/4245)    -   [降级设计        degradation](https://time.geekbang.org/column/article/4252)    -   [弹力设计总结](https://time.geekbang.org/column/article/4253)-   管理设计篇    -   [分布式锁 Distributed        Lock](https://time.geekbang.org/column/article/5175)    -   [配置中心 Configuration        Management](https://time.geekbang.org/column/article/5819)    -   [边车模式        Sidecar](https://time.geekbang.org/column/article/5909)    -   [服务网格 Service        Mesh](https://time.geekbang.org/column/article/5920)    -   [网关模式        Gateway](https://time.geekbang.org/column/article/6086)    -   [部署升级策略](https://time.geekbang.org/column/article/6283)-   性能设计篇    -   [缓存 Cache](https://time.geekbang.org/column/article/6282)    -   [异步处理        Asynchronous](https://time.geekbang.org/column/article/7036)    -   [数据库扩展](https://time.geekbang.org/column/article/7045)    -   [秒杀 Flash        Sales](https://time.geekbang.org/column/article/7047)    -   [边缘计算 Edge        Computing](https://time.geekbang.org/column/article/7086)![](Images/1c1e992cf41f5294df097aabed82f9e4.png){savepage-src="https://static001.geekbang.org/resource/image/fc/e9/fcc761001867c60f526665e237f831e9.jpg"}
# 52 \| 管理设计篇之"分布式锁"我们知道，在多线程情况下访问一些共享资源需要加锁，不然就会出现数据被写乱的问题。在分布式系统下，这样的问题也是一样的。只不过，我们需要一个分布式的锁服务。对于分布式的锁服务，一般可以用数据库DB、Redis 和 ZooKeeper等实现。不管怎么样，分布式的锁服务需要有以下几个特点。-   **安全性（Safety）**：在任意时刻，只有一个客户端可以获得锁（**排他性**）。-   **避免死锁**：客户端最终一定可以获得锁，即使锁住某个资源的客户端在释放锁之前崩溃或者网络不可达。-   **容错性**：只要锁服务集群中的大部分节点存活，Client    就可以进行加锁解锁操作。
# Redis 的分布式锁服务这里提一下，避免死锁的问题。下面以 Redis 的锁服务为例（参考 [Redis的官方文档](https://redis.io/topics/distlock) ）。我们通过以下命令对资源加锁。    SET resource_name my_random_value NX PX 30000解释一下：-   `SET NX` 命令只会在 `key` 不存在的时候给 `key` 赋值，`PX` 命令通知    Redis 保存这个 key 30000ms。-   `my_random_value`    必须是全局唯一的值。这个随机数在释放锁时保证释放锁操作的安全性。-   PX 操作后面的参数代表的是这个 key 的存活时间，称作锁过期时间。-   当资源被锁定超过这个时间时，锁将自动释放。-   获得锁的客户端如果没有在这个时间窗口内完成操作，就可能会有其他客户端获得锁，引起争用问题。这里的原理是，只有在某个 key 不存在的情况下才能设置（set）成功该key。于是，这就可以让多个进程并发去设置同一个key，只有一个进程能设置成功。而其它的进程因为之前有人把 key设置成功了，而导致失败（也就是获得锁失败）。``{=html}我们通过下面的脚本为申请成功的锁解锁：    if redis.call("get",KEYS[1]) == ARGV[1] then     return redis.call("del",KEYS[1]) else     return 0 end如果 key 对应的 value 一致，则删除这个 key。通过这个方式释放锁是为了避免 Client 释放了其他 Client 申请的锁。例如，下面的例子演示了不区分 Client 会出现的一种问题。1.  Client A 获得了一个锁。2.  当尝试释放锁的请求发送给 Redis 时被阻塞，没有及时到达 Redis。3.  锁定时间超时，Redis 认为锁的租约到期，释放了这个锁。4.  Client B 重新申请到了这个锁。5.  Client A 的解锁请求到达，将 Client B 锁定的 key 解锁。6.  Client C 也获得了锁。7.  Client B 和 Client C 同时持有锁。通过执行上面脚本的方式释放锁，Client的解锁操作只会解锁自己曾经加锁的资源，所以是安全的。关于 value 的生成，官方推荐从 /dev/urandom 中取 20 个 byte作为随机数。或者采用更加简单的方式，例如使用 RC4 加密算法在 /dev/urandom中得到一个种子（Seed），然后生成一个伪随机流。也可以采用更简单的方法，使用时间戳 + 客户端编号的方式生成随机数。Redis的官方文档说："这种方式的安全性较差一些，但对于绝大多数的场景来说已经足够安全了"。
# 分布式锁服务的一个问题注意，虽然 Redis文档里说他们的分布式锁是没有问题的，但其实还是很有问题的。尤其是上面那个为了避免Client 端把锁占住不释放，然后，Redis在超时后把其释放掉。不知道你怎么想，但我觉得这事儿听起来就有点不靠谱。我们来脑补一下，不难发现下面这个案例。-   如果 Client A 先取得了锁。-   其它 Client（比如说 Client B）在等待 Client A 的工作完成。-   这个时候，如果 Client A    被挂在了某些事上，比如一个外部的阻塞调用，或是 CPU    被别的进程吃满，或是不巧碰上了 Full GC，导致 Client A    花了超过平时几倍的时间。-   然后，我们的锁服务因为怕死锁，就在一定时间后，把锁给释放掉了。-   此时，Client B 获得了锁并更新了资源。-   这个时候，Client A 服务缓过来了，然后也去更新了资源。于是乎，把    Client B 的更新给冲掉了。-   这就造成了数据出错。这听起来挺严重的吧。我画了个图示例一下。![](Images/034bed10ff4d6813ff101a63bd35f2cd.png){savepage-src="https://static001.geekbang.org/resource/image/93/89/937d9975899662d90a96f4cd70580d89.png"}千万不要以为这是脑补出来的案例。其实，这个是真实案例。HBase就曾经遇到过这样的问题，你可以在他们的 PPT（[HBase and HDFS:Understanding FileSystem Usage inHBase](https://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage)）中看到相关的描述。要解决这个问题，你需要引入fence（栅栏）技术。一般来说，这就是乐观锁机制，需要一个版本号排它。我们的流程就变成了下图中的这个样子。![](Images/f4eeb8529195ddddf754048d1d26155a.png){savepage-src="https://static001.geekbang.org/resource/image/ce/c3/ce3454e9a8bbfe4628899391c003a5c3.png"}我们从图中可以看到：-   锁服务需要有一个单调递增的版本号。-   写数据的时候，也需要带上自己的版本号。-   数据库服务需要保存数据的版本号，然后对请求做检查。如果使用 ZooKeeper 做锁服务的话，那么可以使用 `zxid` 或 znode的版本号来做这个 fence 版本号。