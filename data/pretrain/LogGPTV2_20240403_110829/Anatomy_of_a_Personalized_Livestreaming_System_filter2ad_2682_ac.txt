Wowza
Figure 8: Periscope CDN infrastructure.
Figure 9: Wowza and Fastly server locations.
4.1 Video Streaming CDN and Protocols
We start by describing Periscope’s CDN infrastructure.
By analyzing the network traﬃc between Periscope app and
server, we found that Periscopes uses two independent chan-
nels to deliver live video content and messages (comments/hearts)
and the Periscope server only acts as a control panel (see Fig-
ure 8). Speciﬁcally, when a user starts a broadcast, Periscope
redirects the user to a video server to upload live video, and
a message server to receive real-time comments and hearts.
Viewers receive video frames and messages and combine
them on the client side based on timestamps. For messaging,
Periscope uses a third-party service called PubNub and users
connect to the PubNub via HTTPS. For video, Periscope
leverages collaboration among multiple CDNs.
In the following, we focus on Periscope video CDN and
describe our reverse-engineering eﬀorts to understand its stream-
ing protocols and server deployment.
As shown in Figure 8(b), Periscope
Streaming Protocols.
uses two CDNs, Wowza [8] and Fastly [4], to handle video
uploading (from broadcasters) and downloading (to view-
ers). Wowza uses the RTMP protocol, and Fastly uses the
HLS protocol, which are two fundamentally diﬀerent video
streaming protocols. In RTMP, the client maintains a per-
sistent TCP connection with the server. Whenever a video
frame (≈ 40ms in length) is available, the server “pushes”
it to the client. In HLS, Wowza servers assemble multiple
video frames into a chunk (≈ 3s in length), and create a
chunk list. Over time, Fastly servers poll Wowza servers to
obtain the chunk list. Each HLS viewer periodically “polls”
the chunk list from the (Fastly) server and downloads new
chucks.
When creating a new Periscope broadcast, the broadcaster
connects to a Wowza server to upload live video. Wowza
maintains a persistent RTMP connection with the broadcaster’s
device during the entire broadcast. Wowza also transmits the
live video content to Fastly, and thus both Wowza and Fastly
distribute the video content to end-viewers. The ﬁrst batch
of viewers to join a broadcast directly connect to Wowza to
receive video using RTMP. Our tests estimate the number to
be around 100 viewers per broadcast. Once the threshold is
reached, additional viewers automatically connect to Fastly
to download video using HLS. Recall that Periscope’s de-
fault policy allows only the ﬁrst 100 viewers to post com-
ments in each broadcast [30]. Those commenters are ef-
fectively the ﬁrst batch of viewers who connect directly to
Wowza, and receive their streams via low delay from RTMP.
We believe that Periscope adopts this “hybrid” approach
to improve scalability. Due to its use of persistent TCP con-
nections, RTMP oﬀers low-latency streaming when the num-
ber of viewers is small (85.9%) of HLS broadcasts used
3s chunks (or 75 video frames of 40ms in length).
We believe that Periscope’s choice of chunk size already
reﬂects the tradeoﬀ between scalability and latency. Us-
ing smaller chunks obviously reduces the chunking delay
but also increases the number of chunks. This translates
into higher server overhead for managing data and handling
client polling (i.e. viewers will send more requests to servers).
Thus to support a large number of users, HLS must conﬁgure
its chunk size with care. As a reference, today’s livestream-
ing services all use ≈3s chunks (3s for Periscope and Face-
book live, 3.6s for Meerkat), while Apple’s video-on-demand
(VoD) HLS operates on 10s chunks.
Another key diﬀerence between RTMP
Push vs. Poll.
and HLS is that when stream videos to viewers, HLS uses
poll-based operations while RTMP is push-based. That is,
HLS viewers must poll Fastly servers periodically to dis-
8Our control experiments show that the HLS client uses a
roughly 9s pre-fetching buﬀer size while RTMP uses 1s.
492Buffering Delay
Polling Delay
Upload Delay
Chunking Delay
Wowza2Fastly Delay
Last Mile Delay
HLS
RTMP
 0
 3
 6