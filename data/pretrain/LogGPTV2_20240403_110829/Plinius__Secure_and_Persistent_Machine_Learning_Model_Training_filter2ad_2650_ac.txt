into DRAM.
To minimize code changes for commonly used (but un-
supported) routines in Darknet (e.g., fread, fwrite etc.),
SGX-DARKNET redeﬁnes the former as wrapper functions for
ocalls to the corresponding libC functions in the untrusted
runtime. A support library in the untrusted runtime, sgx-darknet-
helper, provides the implementations of those ocalls invoking
the corresponding libC routines. The pm-data-module is used
by SGX-DARKNET to write/read encrypted data sets to/from
PM. Finally, lib-sgx-darknet provides the API to train and do
inference on models from within the enclave runtime.
Mirroring module. This component is in charge of creating
and updating encrypted mirror copies of enclave ML models
on PM. It contains the necessary logic to instantiate models
that are both persistent and directly byte-accessible via loads
and stores. It leverages the transactional API provided by SGX-
ROMULUS to perform atomic updates on persistent models in
PM. This is crucial as it prevents any inconsistency in PM data
structures in the event of a system failure during data updates.
The logic for building and managing persistent versions of
complex data structures like ML models can get very bulky,
and so we preferred to build a separate module for that rather
than integrate it directly into SGX-DARKNET.
5
PMEncryptionengineTraining +mirroringEncrypted byteaddressabletraining dataEncryptedmirrormodelsSecure enclave➎➐SGX-enabled untrusted serverPM-Datamodule➍➁➍➀➏➊➌➋Send application binary+ raw encryptedtraining datasetRemote attestation +secure communicationchannel with enclaveSend encryp-tion keyssecurelyModel +datasetownerheader_addr = create_header(mapped_addr)
ecall_init(header_addr)
Algorithm 1 — Initialization algorithms.
1 ## Untrusted (outside of enclave) ##
2 function init_sgx_romulus(pm_ﬁle)
3 mapped_addr = mmap(pm_ﬁle)
4
5
6 end
7 function ocall_unmap
8 munmap(pm_ﬁle)
9 end
10 ## Trusted (inside enclave) ##
11 function ecall_init(header_addr)
12
13
14 end function
initialize_main_and_back(header_addr)
recover()
V. IMPLEMENTATION DETAILS
// deﬁned in [11]
We implement PLINIUS in C and C++ and it comprises
28’450 lines of code (LOC) in total, the trusted portion being
15’900 LOC. We use Intel SGX SDK v2.8 for Linux. The
total size of application binary including the enclave shared
library after compilation is 3 MB. In the remainder, we describe
further details and a rundown for a ML model training.
Initialization. In this phase, PLINIUS memory maps PM
into application virtual address space (VAS) (see Algorithm 1,
lines 3-5) and initializes the persistent regions main and back
(Algorithm 1, line 12), so that both regions are consistent
before the training starts.
Initial dataset loading to PM. One key aspect of PLINIUS
is the ability to use training data in PM. In PLINIUS, we load
training data into PM once, after which the data stays in (byte
addressable) PM. At resumption following a power failure or
system crash, training data in PM is instantly accessible to
the training algorithm, unlike in disk or SSD-based systems
where data needs to be re-read from slow secondary storage
into DRAM.
Initially, the training dataset is stored encrypted as ﬁles
on secondary storage. Darknet training algorithms process
input data as multidimensional arrays or matrices. The goal of
this step is to load training data into such a data matrix in
PM. The sgx-darknet-helper reads initial training data and
labels from secondary storage into DRAM as a volatile matrix
variable. The address of this matrix is sent to SGX-DARKNET
via an ecall. The pm-data-module creates a corresponding
persistent matrix on PM using the lib-sgx-romulus API.
We annotate all persistent types (e.g., matrix rows, matrix
values, model layer attributes, etc.) with the persist<> class
from lib-sgx-romulus. This wrapper class ensures every
store operation on the associated persistent data is followed
by a persistent write back (PWB) to ﬂush the cache line to
PM. An appropriate fence instruction is used when ordering is
required (e.g at the end of a transaction).7 Once the persistent
matrix is created, the training data is simply memcpy-ied from
DRAM into PM within a transaction from within the enclave.
The persistent data can then be accessed directly via its address.
Model training and mirroring. The PLINIUS architecture
ﬁts well for training neural network models [29], creating a
mirror_in(enclave_model)
iter = pm_model.iter
end if
iter = 0
if exists(pm_model) then
enclave_model = create_enclave_model(conﬁg)
if not_exists(pm_data) then
ocall_load_data_in_pm()
Algorithm 2 — Training a ML model in PLINIUS
1 function train_model(conﬁg)
2
3
4
5
6
7
8
9
10
11
12
13 while iter  class [11],
which ensures PWBs are done for all stores to the corresponding
persistent data.
Algorithms 2 and 3 summarize respectively model training
and mirroring in PLINIUS. During model training, batches of
training data are decrypted from PM (Algorithm 2, line 15)
into enclave memory and used to train the enclave model for
one training iteration. After each training iteration we do a
mirror-out (encrypt in enclave and write to PM) of the enclave
model parameters to its persistent mirror copy on PM. In the
event of a crash during training, upon resumption the model and
training data are already in PM and can be quickly memcpy-
ied from PM into secure enclave memory. This obviates the
need for much more slower reads from storage devices like
SSDs and HDDs.
VI. EVALUATION
7Romulus supports 3 PWB + fence combinations: clwb+sfence,
clflushopt+sfence (used in PLINIUS) and clflush+nop.
Our experimental evaluation of the PLINIUS prototype
answers the following questions:
6
end for
cur_pm_L.next = PMalloc(size)
cur_pm_L = cur_pm_L.next
cur_pm_L.W = PMalloc(size)
cur_pm_L.next = nullptr
head_pm_L = PMalloc(size)
head_pm_L.W = PMalloc(size)
head_pm_L.next = nullptr
cur_pm_L = head_pm_L
n = enclave_model.numL
for i = 2 to n do
Algorithm 3 Mirroring algorithms.
1 function alloc_mirror_model(enclave_model)
2 BEGIN_TRANSACTION
3
4
5
6
7
8
9
10
11
12
13
14 END_TRANSACTION
15 end function
16 function mirror_out(enclave_model,iter)
17 BEGIN_TRANSACTION
n = enclave_model.numL
18
pm_model.iter = iter
19
temp = head_pm_L
20
for i = 1 to n do
21
22
23
24
25 END_TRANSACTION
26 end function
27 function mirror_in(enclave_model)
28
29
30
31
32
33
34 end function
enclave_model.L(i).W = decrypt(temp.W)
temp = temp.next
n = pm_model.numL
temp = head_pm_L
for i = 1 to n do
temp.W = encrypt(enclave_model.L(i).W)
temp = temp.next
end for
end for
• How SGX-ROMULUS compares against unmodiﬁed Romulus
in a SCONE container ?
• How does PLINIUS improve checkpoint/restore performance
when compared to secondary storage (e.g SSD)?
• How scalable is PLINIUS when varying model sizes?
• What are the main bottlenecks in the PLINIUS design?
• What is the overhead of batched-data decryptions?
• Is the mirroring mechanism robust against crashes?
• Are there processing and storage bottlenecks?
Experimental setup. At the time of this writing (October
2020), servers that support both SGX and PM are not
available. Hence, we perform our experiments on two different
servers, i.e., sgx-emlPM and emlSGX-PM. The sgx-emlPM
node supports SGX but has no physical PM, hence we resort
to emulating the latter with Ramdisk. This machine is equipped
with a quad-core Intel Xeon E3-1270 CPU clocked at 3.80 GHz,
and 64 GB of DRAM. The CPU ships with 32 KB L1i and
L1d caches, 256 KB L2 cache and 8 MB L3 cache. Concerning
emlSGX-PM, it is equipped with 4× Intel OptaneDC DIMMs
of 128 GB each. However its processors lack native support
for SGX. Hence, we resort to SGX in simulation mode [10].
The emlSGX-PM node is a dual-socket 40-core Intel Xeon
Gold 5215 clocked at 2.50 GHz and 376 GB of DRAM. Each
processor has 32 KB L1i and L1d caches, 1 MB L2 cache
and a shared 13.75 MB L3 cache. Both servers run Ubuntu
18.04.1 LTS 64 bit and Linux kernel 4.15.0-54. We run the
Intel SGX platform software, SDK and driver version v2.8. All
our enclaves have max heap sizes of 8 GB and stack sizes of
8 MB. The EPC size is 128 MB (93.5 MB usable). Unless stated
otherwise, we use CLFLUSHOPT and SFENCE for persistent
write backs and ordering.
7
// deﬁned in [11]
// L: neural network layer
// W: layer’s parameters
Fig. 6: SPS benchmark on the sgx-emlPM with varying transaction