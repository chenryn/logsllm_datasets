## ext4 mount option data mode: journal ordered writeback  
### 作者                            
digoal                            
### 日期                             
2015-08-14                          
### 标签                            
PostgreSQL , Linux , ext4 , mount选项     
----                                        
## 背景                             
ext4支持3种DATA模式，用来区分记录journal的行为。	  
ext4的journal类似于PostgreSQL的XLOG，可以用来做灾难恢复，以及确保数据的一致性。  
文件在ext4中分两部分存储，一部分是文件的metadata，另一部分是data。  
metadata和data的操作日志journal也是分开管理的。你可以让ext4记录metadata的journal，而不记录data的journal。  
这取决于mount ext4时的data参数。  
1\.   
```  
data=journal		All data are committed into the journal prior to being  
			written into the main file system.  Enabling  
			this mode will disable delayed allocation and  
			O_DIRECT support.  
```  
在将data写入文件系统前，必须等待metadata和data的journal已经落盘了。性能最差，并且不支持文件操作的delalloc，O_DIRECT flag (参考 man open)。  
当调用fsync时，文件系统的操作包含如下顺序：  
```  
fsync(data journal) -> fsync(metadata journal) -> fsync(data) -> fsync(metadata)  
lock metadata long time between fsync(metadata journal) and fsync(metadata)?  
```  
2\.   
```  
data=ordered	(*)	All data are forced directly out to the main file  
			system prior to its metadata being committed to the  
			journal.  
```  
这个模式不记录data的journal，只记录metadata的journal日志，但是在写metadata的journal前，必须先确保data已经落盘。  
当调用fsync时，文件系统的操作包含如下顺序：  
```  
fsync(metadata journal) -> fsync(data)(确保data先落盘) -> fsync(metadata)  
lock metadata long time between fsync(metadata journal) and fsync(metadata)?  
```  
3\.   
```  
data=writeback		Data ordering is not preserved, data may be written  
			into the main file system after its metadata has been  
			committed to the journal.  
```  
不记录data journal，仅记录metadata journal。并且不保证data比metadata先落盘。  
当调用fsync时，文件系统的操作包含如下顺序：  
```  
fsync(metadata journal) -> fsync(metadata)  
```  
另外需要注意metadata的操作在单个ext4文件系统中是串行的，所以如果某个用户的metadata操作堵塞了的话，会影响所有人操作同一个文件系统的metadata。  
即使使用writeback，也会有这样的情况发生，例如某个用户疯狂的写metadata的（例如大批量的创建小文件，调用fsync）。  
## 典型的例子  
fsync(data)这一步如果很慢，会导致其他人写metadata等待的现象(写metadata包括很多，例如创建新文件，读写方式打开文件（改变文件大小）)。  
查看进程栈：  
10249正在fsync：  
```  
[root@digoal ~]# cat /proc/10249/stack   
[] do_get_write_access+0x29d/0x520 [jbd2]  
[] jbd2_journal_get_write_access+0x31/0x50 [jbd2]  
[] __ext4_journal_get_write_access+0x38/0x80 [ext4]  
[] ext4_reserve_inode_write+0x73/0xa0 [ext4]  
[] ext4_mark_inode_dirty+0x4c/0x1d0 [ext4]  
[] ext4_dirty_inode+0x40/0x60 [ext4]  
[] __mark_inode_dirty+0x3b/0x160  
[] file_update_time+0xf2/0x170  
[] __generic_file_aio_write+0x210/0x470  
[] generic_file_aio_write+0x6f/0xe0  
[] ext4_file_write+0x61/0x1e0 [ext4]  
[] do_sync_write+0xfa/0x140  
[] vfs_write+0xb8/0x1a0  
[] sys_write+0x51/0x90  
[] system_call_fastpath+0x16/0x1b  
[] 0xffffffffffffffff  
```  
10255正在创建新文件，被堵塞：  
```  
[root@digoal ~]# cat /proc/10255/stack   
[] do_get_write_access+0x29d/0x520 [jbd2]  
[] jbd2_journal_get_write_access+0x31/0x50 [jbd2]  
[] __ext4_journal_get_write_access+0x38/0x80 [ext4]  
[] ext4_new_inode+0x3d5/0x1260 [ext4]  
[] ext4_create+0xc3/0x150 [ext4]  
[] vfs_create+0xb4/0xe0  
[] do_filp_open+0xb2f/0xd60  
[] do_sys_open+0x69/0x140  
[] sys_open+0x20/0x30  
[] system_call_fastpath+0x16/0x1b  
[] 0xffffffffffffffff  
```  
## 解决思路  
1\. 通过调整内核刷dirty page的比例和唤醒时间，可以让内核频繁的收回脏页，从而降低以上问题出现的概率，不过这种做法有利有弊。  
相关的参数：  
```  
[root@localhost group1]# sysctl -a|grep dirty  
vm.dirty_background_ratio = 0  
vm.dirty_background_bytes = 1638400  
vm.dirty_ratio = 50  
vm.dirty_bytes = 0  
vm.dirty_writeback_centisecs = 100  
vm.dirty_expire_centisecs = 3000  
```  
相关进程：  
```  
postgres@localhost-> ps -ewf|grep flush  
root      1100     2  0 Aug14 ?        00:03:18 [flush-253:0]  
root      1102     2  0 Aug14 ?        00:01:32 [flush-253:2]  
root     26851     2  0 10:04 ?        00:00:04 [flush-253:1]  
root     50052     2  0 10:40 ?        00:00:00 [flush-8:0]  
```  
在调整前，请务必了解这些参数的含义，以及Linux是如何处理的。  
## 系统缓存相关的几个内核参数
(还有2个是指定bytes的，含义和ratio差不多)：  
1\.         /proc/sys/vm/dirty_background_ratio  
该文件表示脏数据到达系统整体内存的百分比，此时触发pdflush进程把脏数据写回磁盘。  
缺省设置：10  
当用户调用write时，如果发现系统中的脏数据大于这阈值（或dirty_background_bytes ），会触发pdflush进程去写脏数据，但是用户的write调用会立即返回，无需等待。pdflush刷脏页的标准是让脏页降低到该阈值以下。  
即使cgroup限制了用户进程的IOPS，也无所谓。  
2\.         /proc/sys/vm/dirty_expire_centisecs  
该文件表示如果脏数据在内存中驻留时间超过该值，pdflush进程在下一次将把这些数据写回磁盘。  
缺省设置：3000（1/100秒）  
3\.         /proc/sys/vm/dirty_ratio  
该文件表示如果进程产生的脏数据到达系统整体内存的百分比，此时用户进程自行把脏数据写回磁盘。  
缺省设置：40  
当用户调用write时，如果发现系统中的脏数据大于这阈值（或dirty_bytes ），需要自己把脏数据刷回磁盘，降低到这个阈值以下才返回。  
注意，此时如果cgroup限制了用户进程的IOPS，那就悲剧了。  
4\.         /proc/sys/vm/dirty_writeback_centisecs  
该文件表示pdflush进程的唤醒间隔，周期性把超过dirty_expire_centisecs时间的脏数据写回磁盘。  
缺省设置：500（1/100秒）  
## 系统一般在下面三种情况下回写dirty页
1\.      定时方式: 定时回写是基于这样的原则:/proc/sys/vm/dirty_writeback_centisecs的值表示多长时间会启动回写线程,由这个定时器启动的回写线程只回写在内存中为dirty时间超过(/proc/sys/vm/dirty_expire_centisecs / 100)秒的页(这个值默认是3000,也就是30秒),一般情况下dirty_writeback_centisecs的值是500,也就是5秒,所以默认情况下系统会5秒钟启动一次回写线程,把dirty时间超过30秒的页回写,要注意的是,这种方式启动的回写线程只回写超时的dirty页，不会回写没超时的dirty页,可以通过修改/proc中的这两个值，细节查看内核函数wb_kupdate。  
2\.      内存不足的时候: 这时并不将所有的dirty页写到磁盘,而是每次写大概1024个页面,直到空闲页面满足需求为止  
3\.      写操作时发现脏页超过一定比例:   