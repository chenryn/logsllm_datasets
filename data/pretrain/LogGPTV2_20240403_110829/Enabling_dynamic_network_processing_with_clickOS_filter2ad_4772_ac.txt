machines are active.
knowledge that the bottlenecks would not come from Click
and the conﬁguration.
Next we added a ToNetfront element to the previous con-
ﬁguration in order to test the netfront driver’s performance;
the throughput was a dismal 8 Kp/s for maximum-sized
packets. A number of modiﬁcations (e.g., turning the driver
from an interrupt-driven one to a polling one and re-using
the grants that Xen uses to allow domains to share memory
and thus packets) resulted in a signiﬁcant improvement, up
to about 360 Kp/s.
The next component in the transmit path is the netback
driver. As a quick test, we replaced it with a minimalistic
version that does nothing more than take packets from the
netfront driver (via the Xen ring API), count them and drop
them, without ever interacting with the vif. This set-up
resulted in a rate of 950 Kp/s for maximum-sized packets
(greater than line rate) but only 1.5 Mp/s out of a possible
14.8 Mp/s for minimum-sized ones.
At this point the remaining bottleneck was coming from
the drivers’ reliance on the Xen ring API, including their
use of slow interrupts (known as events) to synchronize ac-
cess to the packets. To push rates further up, we overhauled
the Xen network backplane (ﬁgure 3), getting rid of the net-
back driver and replacing the software switch with a fast one
based on VALE.
These modiﬁcations resulted in a signiﬁcant performance
improvement (ﬁgure 6). The graph shows, for various packet
sizes, an increasing number of ClickOS vms residing on the
same server and sending packets through a shared 10Gb NIC
onto another server which measures the aggregate through-
put.
In this experiment we used all 8 CPU cores in our
server, assigning 3 of them to the driver domain that hosts
the ClickOS switch and assigning the remaining 5 cores to
the vms in a round-robin fashion. As can be seen, our low-
cost server can run as many as 128 ClickOS vms who in
turn are able to ﬁll up the 10Gb pipe for most packet sizes.
In a separate test not shown due to space constraints, we
conﬁrmed that all vms were (roughly) equally contributing
to this throughput. Further tests when receiving packets
yielded line rate for all packet sizes of size 256 bytes and
greater, and up to 4.8 Mp/s for minimum-sized ones.
4. RELATED WORK
In order to create a tiny system for network processing
we could have relied on any number of existing minimalistic
OSes [14, 28, 15], but they neither provide good device driver
support nor the advantages of a fully virtualized system such
as isolation and migration. Instead, we build ClickOS on top
of MiniOS, a minimalistic OS aimed at creating small Xen-
based vms.
Many virtualization technologies besides Xen exist [7, 25,
17]. We settled on Xen because it enables us to have excel-
lent driver support (through its Linux-based dom0 domain
and its split-driver model) while still allowing us to run a
tiny virtual machine (a combination of MiniOS and Click).
OpenVZ, for instance, does not support running an OS other
than Linux. KVM supports MINIX, but the latest version
is marked as crashing. The work in [24] introduces a new
thin virtualization system aimed at increasing the overall
security of a virtualized system.
Beyond minimalistic OSes and hypervisors, previous work
has looked into optimizing the performance of Xen’s data
plane. One of the techniques consists of reducing the cost
of packet copies by having the NIC directly place packets
in guest memory [19, 22, 13]; we use this general concept
in conjunction with the netmap framework [20] in order to
speed up Xen’s underlying network system. Some of the gen-
eral optimization techniques used in the netmap framework
and our netback and netfront drivers such as batching and
polling previously appeared in other works such as Route-
Bricks [4].
Another optimization [19, 22] consists of making eﬃcient
use of memory grants (Xen’s mechanism for allowing do-
mains to share memory); we apply a similar approach to
optimize MiniOS’ netfront driver. The work in [9] optimizes
Xen’s scheduler and the Linux bridge that Xen relies on to
direct packets to guest domains. We go one step beyond, en-
tirely replacing the Linux bridge (or in the latest releases of
Xen Open vSwitch [16]) with a modiﬁed version of the fast
VALE switch [10]. One ﬁnal technique available in Xen and
other virtualization systems is passthrough, where a vm is
given direct access to the NIC in order to improve network-
ing performance. The problem is that this technique forces
vms to host device drivers, binds the device to a single vm,
and complicates vm migration.
There are a few other projects that have looked into creat-
ing small virtual machines. The Denali [27] isolation kernel
was able to run large numbers of concurrent virtual ma-
chines, but had limited support for device drivers and guest
OSes. The work in [12, 11] is similar to ours in that they
also create tiny, Xen-based virtual machines, though their
focus is on extending the Objective Caml language to gen-
050100150200250300350400VirtualmachineID050100150200250Virtualmachineboottime(ms)3264128192256320384VirtualmachineID6810121416182022Virtualmachinestarttime(ms)020406080100120NumberofVMs0.51.01.52.02.53.03.54.04.55.0CummulativeThroughput(Mp/s)64bytes128bytes256bytes512bytes1024bytes1500bytes71erate diﬀerent kinds of vms (e.g., they evaluate a vm with
SQLite running in it) that are deﬁned at compile-time. In-
stead, ClickOS can change conﬁguration at run-time and the
work further focuses on optimizing its boot times and net-
working performance. The work in [5] is similar, creating
small, Erlang-based virtual machines on Xen.
With regards to commercial oﬀerings, Cisco developed a
single-tenant virtualized router that can run on VMware
ESXi or Citrix XenServer [2]. However, it is not clear how
extensible it is (it is based on IOS), how large its images
are, how it performs nor how much it costs. Vyatta [26] has
open-source software that can run on a number of virtualiza-
tion platforms and that implements middlebox functionality.
Unlike ClickOS, it is based on Debian, and so its images are
large.
5. CONCLUSION
We presented ClickOS, a tiny, Xen-based virtual machine
that can instantiate middlebox processing in milliseconds
while achieving high performance, allowing for a truly pro-
grammable SDN data plane. Beyond the preliminary through-
put experiments presented in the paper, we are in the pro-
cess of optimizing and testing ClickOS’ network performance
when carrying out middlebox processing.
One of the main contributions of ClickOS is that it allows
consolidation of very large number of vms in a single server:
hundreds in our tests, and potentially even thousands since
we can quickly put idle vms to sleep; contrast this with anec-
dotal evidence of only 10-30 vms in current deployments. We
take the view that ClickOS might enable scenarios not possi-
ble today: per-customer ﬁrewalls, dynamically instantiated
load balancers to cope with load, per-ﬂow IDSes, and on-
the-ﬂy customizable software BRASes (Broadband Remote
Access Server) providing on-demand premium services, to
name a few. We believe ClickOS to be a step towards a
much more dynamic, programmable SDN data plane.
6. ACKNOWLEDGMENTS
This work was partly funded by the EU FP7 CHANGE
(257422) project.
7. REFERENCES
[1] P. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho,
R. Neugebauer, I. Pratt, and A. Warﬁeld. Xen and the Art of
Virtualization. In Proc. ACM SOSP, 2003, New York, NY,
USA, 2003. ACM.
[2] Cisco. Cisco Cloud Services Router 1000v Data Sheet.
http://www.cisco.com/en/US/prod/collateral/routers/ps12558/
ps12559/data_sheet_c78-705395.html, July 2012.
[3] Click Modular Router. Click Elements.
http://read.cs.ucla.edu/click/click, March 2013.
[4] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall,
G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy.
Routebricks: exploiting parallelism to scale software routers. In
Proceedings of the ACM SIGOPS 22nd symposium on
Operating systems principles, SOSP ’09, pages 15–28, New
York, NY, USA, 2009. ACM.
[5] Erlang on Xen. Erlang on Xen. http://erlangonxen.org/, July
2012.
[6] M. Honda, Y. Nishida, C. Raiciu, A. Greenhalgh, M. Handley,
and H. Tokuda. Is it still possible to extend tcp? In Proc.
ACM IMC, 2011.
[7] A. Kivity, Y. Kamay, K. Laor, U. Lublin, and A. Liguori. Kvm:
The linux virtual machine monitor. In Proc. of the Linux
Symposium, 2007.
[8] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F.
Kaashoek. The click modular router. ACM Transactions on
Computer Systems, August 2000, 2000.
[9] G. Liao, D. Guo, L. Bhuyan, and S. R. King. Software
techniques to improve virtualized i/o performance on multi-core
systems. In Proceedings of the 4th ACM/IEEE Symposium on
Architectures for Networking and Communications Systems,
ANCS ’08, pages 161–170, New York, NY, USA, 2008. ACM.
[10] Luigi Rizzo. VALE, a Virtual Local Ethernet.
http://info.iet.unipi.it/~luigi/vale/, July 2012.
[11] A. Madhavapeddy, R. Mortier, C. Rotsos, D. Scott, B. S. T.
Gazagnaire, S. Smith, S. Hand, and J. Crowcroft. Unikernels:
Library operating systems for the cloud. In Proc. of
Architectural Support for Programming Languages and
Operating Systems (ASPLOS), 2013.
[12] A. Madhavapeddy, R. Mortier, R. Sohan, T. Gazagnaire,
S. Hand, T. Deegan, D. McAuley, and J. Crowcroft. Turning
down the lamp: software specialisation for the cloud. In
Proceedings of the 2nd USENIX conference on Hot topics in
cloud computing, HotCloud’10, pages 11–11, Berkeley, CA,
USA, 2010. USENIX Association.
[13] K. Mansley, G. Law, D. Riddoch, G. Barzini, N. Turton, and
S. Pope. Getting 10 gb/s from xen: safe and fast device access
from unprivileged domains. In Proceedings of the 2007
conference on Parallel processing, Euro-Par’07, pages 224–233,
Berlin, Heidelberg, 2008. Springer-Verlag.
[14] Minix3. Minix3. http://www.minix3.org/, July 2012.
[15] MIT Parallel and Distributed Operating Systems Group. MIT
Exokernel Operating System.
http://pdos.csail.mit.edu/exo.html, March 2013.
[16] Open vSwitch. Production Quality, Multilayer Open Virtual
Switch. http://openvswitch.org/, March 2013.
[17] OpenVZ. Welcome to OpenVZ Wiki.
http://wiki.openvz.org/Main_Page, July 2012.
[18] K. K. Ram, J. R. Santos, Y. Turner, A. L. Cox, and S. Rixner.
Achieving 10 gb/s using safe and transparent network interface
virtualization. In Proc. ACM VEE, 2009, VEE ’09, 2009.
[19] K. K. Ram, J. R. Santos, Y. Turner, A. L. Cox, and S. Rixner.
Achieving 10 gb/s using safe and transparent network interface
virtualization. In Proceedings of the 2009 ACM
SIGPLAN/SIGOPS international conference on Virtual
execution environments, VEE ’09, pages 61–70, New York, NY,
USA, 2009. ACM.
[20] L. Rizzo. netmap: A novel framework for fast packet i/o. In
Proc. USENIX Annual Technical Conference, 2012.
[21] L. Rizzo, M. Carbone, and G. Catalli. Transparent acceleration
of software packet forwarding using netmap. In A. G.
Greenberg and K. Sohraby, editors, INFOCOM, pages
2471–2479. IEEE, 2012.
[22] J. R. Santos, Y. Turner, G. Janakiraman, and I. Pratt.
Bridging the gap between software and hardware techniques for
i/o virtualization. In USENIX 2008 Annual Technical
Conference on Annual Technical Conference, ATC’08, pages
29–42, Berkeley, CA, USA, 2008. USENIX Association.
[23] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratsanamy,
and V. Sekarl. Making middleboxes someone else’s problem:
Network processing as a cloud service. In Proc. ACM
SIGCOMM, 2012.
[24] U. Steinberg and B. Kauer. Nova: a microhypervisor-based
secure virtualization architecture. In Proceedings of the 5th
European conference on Computer systems, EuroSys ’10,
pages 209–222, New York, NY, USA, 2010. ACM.
[25] VMware. VMware Virtualization Software for Desktops, Servers
and Virtual Machines for Public and Private Cloud Solutions.
http://www.vmware.com, July 2012.
[26] Vyatta. The Open Source Networking Community.
http://www.vyatta.org/, July 2012.
[27] A. Whitaker, M. Shaw, and S. D. Gribble. Scale and
performance in the denali isolation kernel. SIGOPS Oper. Syst.
Rev., 36(SI):195–209, Dec. 2002.
[28] Wikipedia. L4 microkernel family.
http://en.wikipedia.org/wiki/L4_microkernel_family, July
2012.
72