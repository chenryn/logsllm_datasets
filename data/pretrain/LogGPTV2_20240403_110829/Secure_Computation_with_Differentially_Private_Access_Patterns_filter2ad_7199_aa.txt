# Secure Computation with Differentially Private Access Patterns

## Authors
Sahar Mazloom<sup>1</sup> and S. Dov Gordon<sup>1</sup>

<sup>1</sup>George Mason University

October 16, 2018

## Abstract
We introduce a new security model for secure computation on large datasets. Our model assumes that two servers are employed to compute on private data collected from many users. To enhance computational efficiency, we propose a trade-off with privacy: instead of ensuring that the servers learn nothing about the input values, we guarantee that any information they do learn preserves the differential privacy of the input. This relaxation allows us to build a protocol that leaks some information in the form of access patterns to memory while providing a formal bound on the learned information.

We demonstrate the utility of this leakage in a broad class of computations, including histograms, PageRank, and matrix factorization, which can be performed using graph-parallel frameworks such as MapReduce or Pregel. We implement a protocol for securely executing these computations and evaluate its performance, showing significant improvements over prior implementations.

## Introduction
In today's Internet, privacy and utility often represent a trade-off, with utility being the primary concern for most users. Users continually share increasing amounts of private data with various entities in exchange for a wide range of services. Theoretically, privacy and utility can be maintained if service providers can compute on encrypted data. While the theory of secure computation has been around since the early days of modern cryptography, practical implementations are relatively recent and lag behind advancements in data mining and machine learning.

Recent developments, however, suggest that this gap is narrowing. For instance, Google now uses secure computation to help advertisers compute the value of their ads and plans to use it to construct machine learning classifiers from mobile user data. Additionally, the techniques of differential privacy offer new mechanisms for aggregating user data, ensuring that the amount of information revealed about any single user is minimal. These techniques are already in use by companies like Google and Apple.

The common thread in these applications is large-scale computation, run by big organizations, on data collected from many individual users. To address this, we explore new improvements for two-party secure computation, carried out by two dedicated computational servers, over secret shares of user data. Our approach involves a novel trade-off between efficiency and privacy. Specifically, we propose a model where some small information is leaked to the computation servers, but this leakage is proven to preserve differential privacy for the users who contributed the data.

The question of what is leaked by memory access patterns during computation is central to secure computation. Although circuits are data-oblivious, handling large datasets requires better approaches, such as secure two-party ORAM. However, both circuits and ORAM can be too slow for practical requirements. In the area of encrypted search, cryptographers have proposed access-pattern leakage as a trade-off for efficiency. Analyzing and quantifying this leakage is challenging, and recent studies have suggested that more information is revealed than initially thought. By employing the definition of differential privacy, we can provide a rigorous and meaningful way to bound the leakage of our computation.

### 1.1 Graph-Parallel Computations
Our main contribution is identifying a broad class of highly parallelizable computations that benefit from our proposed privacy/efficiency trade-off. Frameworks like MapReduce, Pregel, GraphLab, and PowerGraph have successfully enabled developers to leverage large networks of parallelized CPUs for computing on plaintext data. These systems, particularly the latter three, are designed to support computations on graph-structured data. The computation proceeds by iteratively gathering data from incoming edges, performing simple computations at nodes, and pushing data back to outgoing edges. This process captures many important tasks, including histogram, gradient descent, and PageRank, which we focus on in our experimental section.

### 1.2 A Connection to Differential Privacy
The memory access pattern in these computations is straightforward: during the gather stage, each edge and adjacent node is accessed, and a similar pattern occurs during the scatter phase. In our framework, each user is represented by a node in the graph, and provides data on the edges adjacent to that node. For example, in a recommendation system, the graph is bipartite, with user nodes on one side and item nodes on the other, and edges labeled with review scores. The access pattern would reveal which items each user reviewed.

To mitigate this, we shuffle all edges obliviously between the gather and scatter phases, breaking the correlation between nodes. This reveals only a histogram of how many times each node is accessed. For a recommendation system, this would reveal how many items each user reviewed and how many times each item was reviewed. By shuffling in dummy edges sampled from an appropriate distribution, we can ensure that the degrees of each node remain differentially private.

### 1.3 Contributions and Related Work
#### Contributions
- **Introducing the Model**: We introduce a model where protocols leak some information in the form of access patterns to memory, bounded by differential privacy. This idea is novel and important.
- **Asymptotic Improvement**: Our relaxation improves the asymptotic complexity of target computations by a factor of approximately log n over the best-known fully oblivious construction. For graphs with |E| = O(α|V |), our improvement is by a factor of log |E|.
- **Concrete Improvement**: We demonstrate tangible gains in practice, with up to a 20X improvement in the number of garbled AND gates required, while preserving differential privacy with strong parameters (ε = 0.3 and δ = 2<sup>-40</sup>).
- **Securely Generating Noise**: We describe a new noise distribution amenable to efficient implementation in a garbled circuit, addressing the normalization issue of the standard two-sided geometric distribution.

#### Related Work
- **Nayak et al. [28]**: Generalizes the work of Nikolaenko et al. [29] to compute graph-parallel computations with full security.
- **Papadimitriou et al. [32]**: Builds a system for the secure computation of graph-structured data, focusing on MPC and networking challenges.
- **Kellaris et al. [22]**: Constructs protocols for encrypted search, using differential privacy to bound the leakage from access patterns.
- **Wagh et al. [35]**: Defines and constructs differentially private ORAM, but their construction does not provide asymptotic improvements.
- **He et al. [18]**: Defines a security notion similar to ours, but their definition is not in the simulation paradigm and lacks a correctness requirement.

In conclusion, our work provides a new and practical approach to secure computation on large datasets, balancing efficiency and privacy through the use of differential privacy.