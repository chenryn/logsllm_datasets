title:Secure Computation with Differentially Private Access Patterns
author:Sahar Mazloom and
S. Dov Gordon
Secure Computation with Diﬀerentially Private
Access Patterns
Sahar Mazloom1 and S. Dov Gordon1
1George Mason University
October 16, 2018
Abstract
We explore a new security model for secure computation on large
datasets. We assume that two servers have been employed to compute
on private data that was collected from many users, and, in order to im-
prove the eﬃciency of their computation, we establish a new tradeoﬀ with
privacy. Speciﬁcally, instead of claiming that the servers learn nothing
about the input values, we claim that what they do learn from the com-
putation preserves the diﬀerential privacy of the input. Leveraging this
relaxation of the security model allows us to build a protocol that leaks
some information in the form of access patterns to memory, while also
providing a formal bound on what is learned from the leakage.
We then demonstrate that this leakage is useful in a broad class of com-
putations. We show that computations such as histograms, PageRank and
matrix factorization, which can be performed in common graph-parallel
frameworks such as MapReduce or Pregel, beneﬁt from our relaxation.
We implement a protocol for securely executing graph-parallel computa-
tions, and evaluate the performance on the three examples just mentioned
above. We demonstrate marked improvement over prior implementations
for these computations.
Introduction
1
Privacy and utility in today’s Internet is a tradeoﬀ, and for most users, utility
is the clear priority. Citizens continue to contribute greater amounts of private
data to an increasing number of entities in exchange for a wider variety of
services. From a theoretical perspective, we can maintain privacy and utility if
these service providers are willing and able to compute on encrypted data. The
theory of secure computation has been around since the earliest days of modern
cryptography, but the practice of secure computation is relatively new, and still
lags behind the advancements in data-mining and machine learning that have
helped to create today’s tradeoﬀ.
1
Recently, we have seen some signs that the gap might be narrowing. The
advancements in the ﬁeld of secure computation have been tremendous in the
last decade. The ﬁrst implementations computed roughly 30 circuit gates per
second, and today they compute as many as 6 million per second [40]. Scat-
tered examples of live deployments have been referenced repeatedly, but most
recently, in one of the more promising signs of change, Google has started us-
ing secure computation to help advertisers compute the value of their ads, and
they will soon start using it to securely construct machine learning classiﬁers
from mobile user data [23]. A separate, more recent line of research also oﬀers
promise: the theory and techniques of diﬀerential privacy give service providers
new mechanisms for aggregating user data in a way that reasonably combines
utility and privacy. The guarantee of these mechanisms is that, whatever can be
learned from the aggregated data, the amount that it reveals about any single
user input is minimal. The Chrome browser uses these techniques when aggre-
gating crash reports [14], and Apple claims to be employing them for collecting
usage information from mobile devices. In May, 2017, U.S. Senator Ron Wyden
wrote an open letter to the commission on evidence-based policymaking, urging
that both secure computation and diﬀerential privacy be employed by “agen-
cies and organizations that seek to draw public policy related insights from the
private data of Americans [39].”
The common thread in these applications is large scale computation, run by
big organizations, on data that has been collected from many individual users.
To address this category of problems, we explore new improvements for two-
party secure computation, carried out by two dedicated computational servers,
over secret shares of user data. We use a novel approach: rather than attempting
to improve on known generic constructions, or tailoring a new solution for a
particular problem, we instead explore a new trade-oﬀ between eﬃciency and
privacy. Speciﬁcally, we propose a model of secure computation in which some
small information is leaked to the computation servers, but this leakage is proven
to preserve diﬀerential privacy for the users that have contributed data. More
technically, the leakage is a random function of the input, revealed in the form
of access patterns to memory, and the output of this function does not change
“by too much” when one user’s input is modiﬁed or removed.
The question of what is leaked by memory access patterns during compu-
tation is central to secure computation. Although the circuit model of com-
putation allows us to skirt the issue, because circuits are data oblivious, when
computing on large data there are better ways of handling the problem, the most
well-studied being the use of secure two-party ORAM [31, 16, 38, 24, 41, 40].
However, when looking at very large data sets, it is often the case that both
circuits and ORAM are too slow for practical requirements, and there is strong
motivation to look for better approaches.
In the area of encrypted search,
cryptographers have frequently proposed access-pattern leakage as a tradeoﬀ
for eﬃciency [5, 4, 33, 20]. Unfortunately, analyzing and quantifying the leak-
age caused by the computation’s access pattern is quite diﬃcult, as it depends
heavily on the speciﬁc computation, the particulars of the data, and even the
auxiliary information of the adversary. Furthermore, recent progress on study-
2
ing this leakage has mostly drawn negative conclusions, suggesting that a lot
more is revealed than we might originally have hoped [19, 27, 3, 21, 11]. Em-
ploying the deﬁnition of diﬀerential privacy as a way to bound the leakage of
our computation allows us to oﬀer an eﬃciency / privacy tradeoﬀ that cryp-
tographers have been trying to provide, while quantifying, in a rigorous and
meaningful way, precisely what we have leaked.
1.1 Graph-Parallel Computations
While the proposed security relaxation is appealing, it is not immediately clear
that it provides a natural way to improve eﬃciency. Our main contribution
is that we identify a broad class of highly parallelizable computations that are
amenable to the privacy/ eﬃciency tradeoﬀ we propose. When computing on
plaintext data, frameworks such as MapReduce, Pregel, GraphLab and Pow-
erGraph have very successfully enabled developers to leverage large networks
of parallelized CPUs [9, 26, 25, 15]. The latter three mentioned systems are
speciﬁcally designed to support computations on data that resides in a graph,
either at the nodes or edges. The computation proceeds by iteratively gathering
data from incoming edges to the nodes, performing some simple computation
at the node, and pushing the data back to the outgoing edges. This simple
iterative procedure captures many important computational tasks, including
histogram, gradient descent and page-rank, which we focus on in our experi-
mental section, as well as Markov random ﬁeld parameter learning, parallelized
Gibbs samplers, and name entity resolution, to name a few more. Recently,
Nayak et al. [28], generalizing the work of Nikolaenko et al. [29], constructed
a framework for securely computing graph-parallel algorithms. They did this
by designing a nicely parallelizable circuit for the gather and scatter phases,
requiring O(|E| + |V |) log2(|E| + |V |) AND gates.
1.2 A Connection to Diﬀerential Privacy
The memory access pattern induced by this computation is easily described: dur-
ing the gather stage, each edge is touched when fetching the data, and the adja-
cent node is touched when copying the data. A similar pattern is revealed during
the scatter phase. (The computation performed during the apply phase is typi-
cally very simple, and can be executed in a circuit, which is memory oblivious.)
Let’s consider what might be revealed by this access pattern in some concrete
application. In our framework, each user is represented by a node in the graph,
and provides the data on the edges adjacent to that node. For example, in a rec-
ommendation system, the graph is bipartite, each node on the left represents a
user, each node on the right represents an item that users might review, and the
edges are labeled with scores indicating the user’s review of an item. The access
pattern just described would reveal exactly which items every user reviewed!
Our ﬁrst observation is that if we use a secure computation to obliviously
shuﬄe all of the edges in between the gather and scatter phases, we break the
correlation between the nodes. Now the only thing revealed to the computing
3
parties is a histogram of how many times each node is accessed – i.e. a count
of each node’s in-degree and out-degree. When building a recommendation
system, this would reveal how many items each user reviewed, as well as how
many times each item was reviewed. Fortunately, histograms are the canonical
problem for diﬀerential privacy. Our second observation is that we can shuﬄe
in dummy edges to help obscure this information, and, by sampling the dummy
edges from an appropriate distribution (which has to be done within a secure
computation), we can claim that the degrees of each node remain diﬀerentially
private.
1.3 Contributions and Related Work
Contributions. We make several new contributions, of both a theoretical and
a practical nature.
Introducing the model. As cryptographers have attempted to support secure
computation on increasingly large datasets, they have often allowed their pro-
tocols to leak some information to the computing parties in the form of access
patterns to memory. This is especially true in the literature on encrypted search.
The idea of bounding the leakage in a formal way, using the deﬁnitions from
literature on diﬀerential privacy, is novel and important. (Concurrent and inde-
pendent of our work, He et al. [18] proposed a very similar security relaxation,
which we discuss below.)
Asymptotic improvement. The relaxation we introduce enables us to improve the
asymptotic complexity of the target computations by a factor of (roughly) log n
over the best known, fully oblivious construction. Using the best known sorting
networks, the construction of Nayak et al. [28] requires O((|E| + |V |) log(|E| +
|V |)) AND gates. In comparison, our construction requires O(|E| + α|V |) AND
gates, where α depends on the privacy parameters,  and δ. For graphs with
|E| = O(α|V |), our improvement is by a factor of log |E|. While we do not have
a lower bound in the fully oblivious model, we ﬁnd it very exciting that there
exist computations where our relaxation is this meaningful, and we suspect that
improved asymptotic results in the fully oblivious model are not possible. The
details of this improvement appear in Section 5.
Concrete improvement. We demonstrate that the asymptotic improvements
lead to tangible gains. We have implemented our system, and compared the
results to the system of Nayak et al. [28]. We demonstrate up to a 20X factor
improvement in the number of garbled AND gates required in the computation,
while preserving diﬀerential privacy with strong parameters:  = .3 and δ =
2−40. We note that, in practice, both results are worse than previously described
by a factor of log |E|. In their implementation, Nayak et al. rely on a practical
oblivious sort, using O((|E|+|V |) log(|E|+|V |))2 AND gates. Our construction
using O(|E|+α|V |) gates requires performing decryption inside a garbled circuit,
which we avoid in our implementation through the use of a two-party oblivious
shuﬄe, resulting in O((|E| + α|V |) log(|E| + α|V |)) AND gates. However, we
still save a factor of log |E| for suﬃciently dense graphs. The details of this
4
construction appear in Section 4, and an evaluation of its performance appears
in Section 6.
Securely generating noise We describe a new noise distribution that is amenable
to eﬃcient implementation in a garbled circuit. Dwork et al. previously de-
scribed an eﬃcient method for sampling the geometric distribution in a garbled
circuit [12], but they did this for the 1-sided geometric distribution, which is not
immediately useful for providing diﬀerential privacy. Unfortunately, the stan-
dard 2-sided geometric distribution needs to be normalized in a way that ruins
the simplicity of the circuit they describe. We construct a slightly modiﬁed
distribution that addresses this problem, and prove that it provides diﬀerential
privacy.
Related Work. The most relevant work, as already discussed, is that by
Nayak et al. [28], which generalizes the work of Nikolaenko et al. [29], comput-
ing graph parallel computations with full security. (The latter work focused
solely on the problem of sparse matrix factorization.) Papadimitriou et al. [32]
also build a system for the secure computation of graph-structured data, and
ensure diﬀerential privacy of the output. They do not consider diﬀerentially
private leakage in the access patterns, and they focus on MPC and the net-
working challenges that arise in that setting; in particular, they do not rely on
computational servers, but assume that all data contributors are involved in the
computation, and focus on how to hide the movement of data between users
in a way that preserves privacy of the edge structure. Kellaris et al. construct
protocols for encrypted search, and use diﬀerential privacy to bound the leakage
from access patterns [22]. This work directly inspired us to consider a more gen-
eral approach to modeling leakage in secure computation through diﬀerential
privacy. Wagh et al. [35] deﬁne and construct diﬀerentially private ORAM in
which the server’s views are “similar” on two neighboring access patterns. They
consider the client/server model, and don’t consider using their construction in
a secure computation, but it is very interesting to note that we could use their
result in a generic way to build a protocol for generic secure computation with
diﬀerentially private access patterns. While feasibility follows from their work,
the resulting protocol provides no asymptotic improvement, and would be quite
impractical.
Independent of our work, He et al. deﬁne a security notion that is similar to
our own, and construct new protocols for private record linkage [18]. Informally,
their deﬁnition requires that for any input set D1, and for two neighbors D2, D0
2
for which f(D1, D2) = f(D1, D0
2), the protocol views should preserve diﬀeren-
tial privacy. We note that their deﬁnition is not in the simulation paradigm,
which leads to some important diﬀerences. For one, they only require secu-
rity on inputs that map to the same output, and in particular, cannot apply
their deﬁnition to randomized functionalities1. They also have no correctness
requirement (which is captured in the simulation paradigm by default): this is
1For example, because our protocol outputs random secret shares to each of the servers, it
could not be proven secure under their deﬁnition, but can be proven secure in our simulation
paradigm.
5
intentional, as they explore not just an eﬃciency / privacy tradeoﬀ, but a cor-
rectness tradeoﬀ as well. Perhaps the biggest diﬀerence between their work and
our own is the application space. In private record linkage protocols, items are
typically hashed into bins, and dummy items are used to hide the load of each
bin. When applying their relaxation, they gain eﬃciency over fully secure pro-
tocols by cutting down on the number of dummy items and claiming diﬀerential
privacy, in place of statistically hiding the bin load. Since the maximum load
log n
log log n with high probability, they can only claim improvement if
on a bin is
they use very few dummy items, and as such they can only claim fairly weak se-