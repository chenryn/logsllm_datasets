data that may shed light on this question. Figure 8.9 shows at what point during
the fuzzing various bugs were discovered by ProxyFuzz in the 450 minutes it was
used during DNS testing.
7 Thanks to Dr. Andrea Miller from Webster University for helping with this analysis.
6760 Book.indb 274 12/22/17 10:50 AM
8.8 General Conclusions 275
Figure 8.9 These three graphs plot code coverage versus bugs found for each of the fuzzers.
Each point represents a fuzzer. The DNS graph especially shows the positive relationship.
6760 Book.indb 275 12/22/17 10:50 AM
276 Fuzzer Comparison
The thing to notice from this data is there are discrete jumps between the times
when various bugs are discovered. Three easy-to-find bugs are found in the first
three minutes of fuzzing. The next bug is not found for another 76 minutes. Like-
wise, seven bugs are discovered in the first 121 minutes. Then it took another 155
minutes to find the next one. It would be very tempting during these “lulls” to think
the fuzzer had found everything it could find and turn it off. Along these lines, the
fuzzer did not find anything in the final hour of its run. Does this mean it wouldn’t
find anything else ever, or that it was just about to find something?
8.8.8 random Fuzzers Find Easy Bugs First
One issue not addressed thus far is which bugs are found in which order when using
a fuzzer with random components. Based on the last section, fuzzers clearly find
some bugs very quickly, and other bugs require much more time to find. Figure
8.10 shows how often each bug from the ProxyFuzz run against DNS was found.
Not surprisingly, the ones that were found quickest were also the ones discovered
the most frequently, which is shown in Figure 8.11. The last two bugs discovered
during this fuzzing run were only found once.
8.9 Summary
In this chapter, we began by discussing the various functions that different fuzzers
provide. Some fuzzers only provide fuzzed inputs and leave everything else to the
user. Others provide a variety of services besides just the inputs, including target
Figure 8.10 The graph shows the number of minutes for ProxyFuzz to find the 9 SNMP bugs it
discovered. Users who turn their fuzzer off early will miss the bugs discovered later.
6760 Book.indb 276 12/22/17 10:50 AM
8.9 Summary 277
Figure 8.11 The bugs found most quickly were also found most frequently
monitoring and reporting. We evaluated the quality of the test cases generated by
a variety of different fuzzers for this comparison. While there are different ways
to compare fuzzer’s quality, we took two approaches. First, we took three open-
source applications and added a number of security vulnerabilities to each. We then
measured how many of these bugs were found by each fuzzer. We also measured
the amount of code coverage obtained by each fuzzer. We compiled this data into
various charts and performed some analysis. We spent extra time examining those
particular bugs that only a few fuzzers could find to see what made them special.
Overall, the results were that some fuzzers did better than others, but we found
that the best practice to find the most bugs is to use a number of different fuzzers
in combination. We also found that the quality of the initial test cases is important
for mutation-based fuzzers and that the amount of protocol knowledge a fuzzer
possesses is a good indication of how well it will perform. Finally, we learned that
code coverage can be used to predict how well various fuzzers are performing. So
while we set out to find which fuzzer was best, we ended up learning a lot about
how different fuzzers work and were able to make general conclusions about fuzzing.
6760 Book.indb 277 12/22/17 10:50 AM
6760 Book.indb 278 12/22/17 10:50 AM
C h a p t e r 9
Fuzzing Case Studies
In this chapter, we will describe a few common use cases for fuzzing. We will explain
our experiences with each of them, with examples drawn from real-life fuzzing
deployments. These examples combine experiences from various deployments, with
the purpose of showing you the widest usage possible in each of the scenarios. In
real deployments, organizations often choose to deploy fuzzing at a slower pace
than what we present here. We will not mention the actual organization names to
protect their anonymity.
As we have stressed in this book, fuzzing is about black-box testing and should
always be deployed according to a test plan that is built from a risk assessment.
Fuzzing is all about communication interfaces and protocols. As explained in
Chapter 1, the simplest categorization of fuzzing tools is into the following pro-
tocol domains:
• File fuzzing;
• Web fuzzing;
• Network fuzzing;
• Wireless fuzzing.
Whereas a good file fuzzing framework can be efficient in finding problems in
programs that process spreadsheet documents, for example, it can be useless for
Web fuzzing. Similarly, a network fuzzer with full coverage of IP protocols will
very rarely be able to do wireless protocols due to the different transport mecha-
nisms. Due to the fact that many tools are targeted only to one of these domains,
or due to the internal prioritization, many organizations deploy fuzzing in only
one of these categories at a given time. Even inside each of these categories you will
find fuzzers that focus on different attack vectors. One fuzzer can be tailored for
graphics formats whereas, another will do more document formats but potentially
with worse test coverage. One Web fuzzer can do a great job against applications
written in Java, but may perform badly if they are written in C. Network fuzzers
can also focus on a specific domain such as VoIP or VPN fuzzing.
Therefore, before you can choose the fuzzing framework, and even before you
will do any attack vector analysis, you need to be able to identify the test targets.
A simplified categorization of fuzzing targets, for example, can be, the following:
• Server software;
• Middleware;
279
6760 Book.indb 279 12/22/17 10:50 AM
280 Fuzzing Case Studies
• Applications (Web, VoIP, mobile);1
• Client software;
• Proxy or gateway software.
Finally, you need to build the test harness, which means complementing the
chosen test tools with various tools needed for instrumenting and monitoring the
test targets and the surrounding environment. With a well-designed test harness,
you will be able to easily detect, debug, and reproduce the software vulnerabilities
found. Various tools that you may need might include
• Debuggers for all target platforms;
• Process monitoring tools;
• Network analyzers;
• Scripting framework or a test controller.
Now, we will walk through some use cases for fuzzing, studying the above-men-
tioned categories with practical examples. We will focus on performing attack vector
analysis and will present example fuzzing results where available.
9.1 Enterprise Fuzzing
The first and most important goal in enterprise fuzzing is to test those services and
interfaces that are facing the public internet. Most often these are services built on
top of the Web (i.e., HTTP), but there are many other exposed attack vectors at any
enterprise. Those include other internet services such as email and VoIP, but also
many other, more transparent, interfaces such as Network Time Protocol (NTP)
and Domain Name Service (DNS).
After the most exposed interfaces have been examined, some enterprise users we
have worked with have indicated interest in testing internal interfaces. The internal
interfaces consist of communications conducted inside the organization that are not
exposed to the internet. Through such attack vectors, the inside users could abuse
or attack critical enterprise servers. The assessment of internal attack vectors is also
important, as studies show that a great number of attacks come from insiders.2 Even
when considering a completely outside adversary, once they break into an internal
machine, their next target will be these inside interfaces.
Whatever the test target, there are at least three methods for identifying the
attack vectors that need testing:
• Port scan from the internet;
• Run a network analyzer at various points in the network;
1 As we have pointed it out several times, it is important to note that not all applications run on top of
the Web. It is definitely the most widely used application development platform, though.
2 The E-Crime Watch Survey 2004, by U.S. CERT and U.S. Secret Service indicated that insiders were
responsible for 29% of attacks and 71% from outsiders. For more details on insider threats, see www.
cert.org/archive/pdf/insidercross051105.pdf.
6760 Book.indb 280 12/22/17 10:50 AM
9.1 Enterprise Fuzzing 281
• Perimeter defense rule-set analysis.
A simple port scan conducted from outside the organization is the easiest to do
and will quickly reveal most of the critical open services. When combined with a
network analyzer based at several probes distributed in the enterprise network, the
results will indicate which of the tests actually went through the various perimeter
defenses of the organization. For example, some data may pass straight from an
outside attacker to a critical server located deep within a network, while other data
from the attacker may terminate in the demilitarized zone, or DMZ, with very little
access to critical servers. A thorough analysis of the perimeter defenses, the rules
and log files of firewalls and proxies, will provide similar results. At the perimeter
you can, for example, detect the outgoing client requests and incoming response
messages, which you would not be able to detect with any port scanning techniques.
Various probe-based network analyzers can again help to detect these use cases,
because they can check which client requests were actually responded to, therefore
requiring client-side fuzzing. Enterprises are often surprised during this exercise at
the number of interfaces, both server side and client side, that are actually exposed
to the open and hostile internet.
The greatest challenge in all enterprise test setups is that at some point, fuzzing
will most probably crash the critical services that are being tested. Fuzzing should be
first conducted in a separate test setup where crashes will not damage the produc-
tion network and where the failures are perhaps easier to detect. These test facilities
might not exist currently, as most testing done at an enterprise are feature and per-
formance oriented, and can be executed against the live system during quiet hours.
As an example of enterprise fuzzing, we will look at fuzzing against firewalls
and other perimeter defenses, and also at fuzzing Virtual Private Network (VPN)
systems. Both of these are today deployed in almost every enterprise requiring
remote work with security critical data. Firewalls today are very application-aware
and need to be able to parse and understand numerous communication protocols.
This extensive parsing leaves them open to security vulnerabilities. Likewise, VPNs
need to understand numerous complex, cryptographic protocols and are equally
susceptible. Besides these, enterprise fuzzing is often conducted against email sys-
tems, Web services, and various internal databases such as CRM.
9.1.1 Firewall Fuzzing
A firewall is a system that integrates various gateway components into an intelligent
router and packet filter. For most protocols, the firewall acts as an application-level
gateway (ALG) or an application proxy. For the protocols that it supports, it some-
times functions as a back-to-back user agent (B2BUA), on one side implementing a
server implementation of the protocol, and on the other, client functionality.
The most critical analysis point for firewall fuzzing is how much of the applica-
tion protocol the firewall actually implements. A firewall that functions only on the
IP and socket level will not require application-level fuzzers. Against such simple
firewalls, using advanced application-layer fuzzers will often be a waste of time. In
this case, much more effective tests can be found when the firewall is tested with
low-level protocol suites. And due to the speed of the device, you can potentially
6760 Book.indb 281 12/22/17 10:50 AM
282 Fuzzing Case Studies
use a fuzzing suite with millions of random test cases and still quickly complete the
testing. A firewall can easily be tested at line speed, as the processing of the pack-
ets is fast, and needs to be fast. Also, a firewall that implements or integrates with
content-filtering software such as antivirus and anti-spam functionality should be
tested with file fuzzers over various transport protocols.
Firewalls may also treat most protocols as stateless, no matter how complex
the protocol is in real life.3 For example, a firewall that is proxying the FTP pro-
tocol may not care that the password is sent before the username, just that each
separate packet conforms to the relevant RFC. Firewalls do not necessarily have
to understand the protocol as well as a true server or client, only well enough to
proxy requests back and forth.
Due to the closed architecture of most firewalls, the monitoring facilities in
firewall testing can be complex to set up. What makes this setup difficult, also, is
that for best results one should always use the real server software as the termina-
tion point for the used inputs. However, many test systems will simulate the end-
point, which makes testing easier, but may not reveal the true functionality of the
device. The best monitoring technique is through combining traditional test target
monitoring tools with sets of network analyzers4 at two or four points in the packet
route, one or two analyzers per each hop in the network (Figure 9.1). With this test
setup, you will be able to detect:
• Dropped packets;
• Packets passed unaltered;
• Packets passed altered with differences highlighted;
• Delay, jitter, and packet loss (SLA) statistics for performance and availabil-
ity evaluation.
Figure 9.1 Proxy fuzzing testbed with monitoring requirements.
3 A firewall often takes the simplest route around a problem. The most critical requirement for a
firewall is performance. Keeping state information about thousands of parallel sessions will be close
to impossible.
4 Network taps are available, for example, from VSS Monitoring (www.vssmonitoring.com/) and
analysis tools for combining numerous message streams from for example Clarified Networks (www.
clarifiednetworks.com/).
6760 Book.indb 282 12/22/17 10:50 AM
9.1 Enterprise Fuzzing 283
In addition to the available black-box monitoring techniques, the actual device
can also be instrumented with process-monitoring tools. Unfortunately, very few
firewall vendors will provide the low-level access to the device that is needed for
proper monitoring during fuzz testing.
There are many names for this type of testing, most of which are also used to
describe other types of testing. Some call this pass-through testing, although to
most of us with a quality assurance background, that term means testing the pass-
through capability (performance) of a device. Others call this type of test setup
proxy-testing or end-to-end testing. When fuzzing is done both ways, it can also be
called cross-talk fuzzing. Also, for example, Ixia has a test methodology called No
Drop Throughput Test, which has similarities. Perhaps, the correct fuzzing variant
of this would be No Drop Fuzz Test. This type of testing is sometimes also called
impairment testing. End-to-end fuzzing is most probably the most general-purpose
term for this type of test setup, as the SUT can consist of more than one network
component, and the tests often need to be analyzed against real end-points and not
just in simulated environments.
An example result of analyzing an end-to-end fuzzing shows that only a small
portion of fuzz tests either pass through the test network or are completely blocked.
Most tests result in various unbalanced results in a complex network infrastructure
involving perimeter defenses and other proxy components (Figure 9.2).5 When the
fuzzed test cases involve a complex message flow, some part of the test cases can
be modified, nonfuzzed messages can be dropped, or responses can be modified
Figure 9.2 Example analysis of end-to-end fuzzing using Clarified Networks analyzer and
Defensics fuzzer.
5 Image from Clarified Networks. www.clarifiednetworks.com.
6760 Book.indb 283 12/22/17 10:50 AM
284 Fuzzing Case Studies
somewhere along the route. The result is very difficult to analyze without very
intelligent network analyzers. This modification of messages is often intended
behavior in, for example, a proxy implementing back-to-back user agent (B2BUA)
functionality.
9.1.2 VpN Fuzzing
As attractive as a VPN may be as an enterprise security solution, it can also be a big
security challenge. The protocols comprising typical VPN implementations are many,
and they are extremely complex, giving a lot of opportunities for implementation
errors. Many of the tests are run with or inside encrypted messages and tunneled
streams, making test analysis very challenging.
Each VPN can typically be configured to support a wide range of different tun-
neling and encryption protocols, augmented with complex authentication protocols
and key exchange protocols.
• Tunneling:
– L2TP;
– MPLS.
• Encryption:
– IPSec;
– TLS/SSL (includes key exchange);
– SSH1 and SSH2 (includes key exchange).
• Authentication:
– Radius;
– Kerberos;
– PPTP;
– EAP;
– CHAP and MS-CHAP.
• Key exchange:
– ISAKMP/IKEv1;
– IKEv2.
So basically, a VPN is an internet-facing device whose interior side resides within
an internal subnet of the enterprise. Furthermore, it processes numerous complex
protocols. In other words, these devices are a security nightmare and need to be
tested for all protocols that they support. Security protocols used in VPNs require
sophistication from the fuzzer. For example, a SSL/TLS fuzzer needs to implement
full capability to all encryption algorithms used in various TLS servers and clients.
Defensics tools for SSL/TLS fuzzing are one example of a fuzzer that implements
the encryption protocol fully to be able to fuzz it (Figure 9.3).
As VPN client devices are often accessing the VPN server over the internet, they
also need to be carefully tested for client-side vulnerabilities. VPN client fuzzers
combine similar challenges; namely, they need to implement the protocol at least
at some level, and also, similarly to browser fuzzing, they are slow to execute as
they test the client side.
6760 Book.indb 284 12/22/17 10:50 AM