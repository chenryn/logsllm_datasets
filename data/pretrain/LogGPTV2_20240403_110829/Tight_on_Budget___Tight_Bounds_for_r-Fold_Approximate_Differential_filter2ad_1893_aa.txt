title:Tight on Budget?: Tight Bounds for r-Fold Approximate Differential
Privacy
author:Sebastian Meiser and
Esfandiar Mohammadi
Tight on Budget?
Tight Bounds for r-Fold Approximate Diﬀerential Privacy
Sebastian Meiser1, Esfandiar Mohammadi2
1 University College London, United Kingdom, e-mail: PI:EMAIL
2 ETH Zurich, Switzerland, e-mail: PI:EMAIL
The authors are in alphabetical order. Both authors equally contributed to this work.
September 5, 2018
Abstract
Many applications, such as anonymous communication systems, privacy-enhancing database queries,
or privacy-enhancing machine-learning methods, require robust guarantees under thousands and some-
times millions of observations. The notion of r-fold approximate diﬀerential privacy (ADP) oﬀers a
well-established framework with a precise characterization of the degree of privacy after r observations of
an attacker. However, existing bounds for r-fold ADP are loose and, if used for estimating the required
degree of noise for an application, can lead to over-cautious choices for perturbation randomness and
thus to suboptimal utility or overly high costs.
We present a numerical and widely applicable method for capturing the privacy loss of diﬀerentially
private mechanisms under composition, which we call privacy buckets. With privacy buckets we com-
pute provable upper and lower bounds for ADP for a given number of observations. We compare our
bounds with state-of-the-art bounds for r-fold ADP, including Kairouz, Oh, and Viswanath’s composi-
tion theorem (KOV), concentrated diﬀerential privacy and the moments accountant. While KOV proved
optimal bounds for heterogeneous adaptive k-fold composition, we show that for concrete sequences of
mechanisms tighter bounds can be derived by taking the mechanisms’ structure into account. We com-
pare previous bounds for the Laplace mechanism, the Gauss mechanism, for a timing leakage reduction
mechanism, and for the stochastic gradient descent and we signiﬁcantly improve over their results (except
that we match the KOV bound for the Laplace mechanism, for which it seems tight). Our lower bounds
almost meet our upper bounds, showing that no signiﬁcantly tighter bounds are possible.
1
Contents
1 Introduction
1.1 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Background and Related work
2.1 Worst case distributions for ADP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Tight ADP on distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Practical relevance of tight privacy bounds
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Composition of diﬀerential privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3 Privacy buckets of distributions
3.1
3.2 A formal description of privacy buckets
3.3 Buckets per atomic event
Informal description of privacy buckets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4 Reducing and bounding the error
4.1 Buckets with error correction terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 Buckets and error correction terms per element . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 Helpful properties of error correction terms
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 The approximated delta with error correction . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.6
5 Evaluation and comparison
5.1 Embedding the Laplace mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Embedding the Gauss mechanism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Embedding CoverUp’s data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4 Embedding the Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Embedding the Randomized Response Mechanism . . . . . . . . . . . . . . . . . . . . . . . .
5.6 Computing Kairouz et al.’s composition theorem . . . . . . . . . . . . . . . . . . . . . . . . .
5.7 Computing bounds based on R´enyi divergence . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.8 Comparison results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6 Comparison of the Gaussian and the Laplace mechanism
7 Application to Vuvuzela
7.1 Protocol overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 Tighter privacy analysis for the dialing protocol . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
7.3 Tighter privacy analysis for the conversation protocol
8 Conclusion and future work
9 Acknowledgement
3
3
3
4
5
8
9
10
11
11
13
16
19
19
21
25
28
33
34
35
35
37
37
37
38
38
39
39
40
43
43
44
46
48
49
2
1
Introduction
Approximate diﬀerential privacy (ADP [4]) has been designed to quantify, with two parameters (ε, δ), the
privacy leakage of systems that require a careful trade-oﬀ between the system’s usefulness and the system’s
privacy leakage. Since its introduction, ADP has been successfully used to quantify the privacy leakage
of privacy-enhancing mechanisms in various applications, including query-response of sensitive databases,
training deep neural networks [1] while protecting the training data, and even anonymous communication [26].
This privacy leakage, i.e., the (ε, δ) parameters, inevitably grows under continual observation; thus, privacy
eventually deteriorates (see Apple’s case [25]). In many application scenarios, continual attacker-observation
is unavoidable, e.g., an attacker may have thousands if not hundreds of thousands of observation points.
Precisely computing the (ε, δ) parameters after r observations, called r-fold ADP bounds, is hard. How-
ever, imprecise bounds on (ε, δ) can lead to either a wrong perception of the privacy leakage (resulting, e.g.,
in unsatisﬁed customers) or to an over-cautious choice of system parameters (resulting in unnecessarily high
costs). There is a rich body of work on approximating r-fold ADP-bounds [8, 14, 1, 20, 7, 2]. Early work
did not take the shape of the mechanism’s output distribution into account [8, 14] (mechanism-oblivious
bounds). We show that the best mechanism-oblivious bounds are tight w.r.t. the Laplace mechanism but
imprecise for many other mechanisms, e.g., the Gaussian mechanism, as these bounds inherently assume a
worst-case behavior under composition. Recent work [1, 20, 7, 2], in contrast, introduced mechanism-aware
bounds that take the shape of the output distribution of the mechanism into account and achieve signiﬁcantly
tighter bounds for some particular mechanism, such as the Gaussian mechanism. However, it was not clear
how tight previous mechanism-aware bounds are and whether they can be further improved.
1.1 Contribution
We introduce a numerical method—privacy buckets—for computing upper and lower r-fold (ε, δ)-ADP
bounds that take the mechanisms and their (ﬁxed) noise parameters into account. To this end, we uti-
lize a discretized version of the privacy loss random variable introduced by Dwork and Rothblum [7]. Our
approach is suﬃciently general to subsume the generic adaptive r-fold ADP bounds of prior work [14, 21].
We compare our upper bounds with state-of-the-art bounds and achieve signiﬁcant improvements over all
of them. Moreover, our lower bounds almost meet the upper bounds, showing that no signiﬁcantly tighter
bounds are possible.
Our evaluations illustrate that privacy buckets can give insights about the composition behavior of various
mechanisms. We ﬁnd that for the right choice of scale parameter and standard-deviation, the Laplace
mechanism and the Gauss mechanism converge to the same privacy leakage, i.e., their (ε, δ) parameters
coincide from a suﬃciently high number of observations r onward.1
Our method is useful for deriving tight bounds for classical diﬀerential privacy mechanisms but can also
be applied to any privacy analysis resulting in ADP. We exemplify this statement by computing bounds
for the anonymous communication system Vuvuzela [26], the stochastic gradient descent mechanism for
deep learning [1] and for timing-leakage histograms of a recently introduced browser extension for deniable
communication [24].
2 Background and Related work
In this section, we review the notion of diﬀerential privacy, highlight an often implicit assumption in the
analysis of diﬀerentially private mechanisms, generalize diﬀerential privacy to pairs of distributions, and
position our work in the work from the literature.
Diﬀerential privacy Diﬀerential privacy (DP) [3] quantiﬁes how close the distributions of outputs of
a mechanism on two similar inputs are. We say that a mechanism M is ε-DP, if for any two closely
related inputs D1, D2, ∀S ⊆ U, Pr [M (D1) ∈ S] ≤ eε · Pr [M (D2) ∈ S]. To extend the applicability of DP,
approximate diﬀerential privacy [4] (ADP) has been introduced, which allows for distributions to exceed a
1For the expert reader, this observation indicates that the result from Dwork and Rothblum [7], that subgaussian privacy
loss variables compose (at most as badly) as a Gaussian privacy loss variable, can be generalized.
3
limiting factor ε, as long as this deviation can be limited to a value δ as follows: ∀S ⊆ U, Pr [M (D1) ∈ S] ≤
eε · Pr [M (D2) ∈ S] + δ. In this work we focus on ADP.
2.1 Worst case distributions for ADP
Classically, diﬀerential privacy argues about the output of a probabilistic mechanism M that is run on similar
inputs (e.g., neighboring databases). Since M is probabilistic, the application of M to any input D can be
seen as a random variable with outputs from a distribution M (D). Diﬀerential privacy requires the outputs
of M on all pairs of neighboring databases w.r.t. an application speciﬁc sensitivity-metric, i.e., all pairs of
distributions M (D1), M (D2), where D1 and D2 are neighboring, to be closely related (quantiﬁed via the
privacy parameters ε and δ).
Our approach operates on individual pairs of distributions. While this formalization is unconventional
and at ﬁrst glance might seem to restrict the applicability to particular queries, our approach leads to far
more general results. We elaborate that considering pairs of distributions is very natural in the context
of diﬀerentially private mechanisms, e.g., to analyze mechanisms in the presence of arbitrary adversarial
queries. In the simplest case the proofs analyze mechanisms using pairs of inputs that are worst-case in the
following sense: in terms of privacy these inputs are as bad as any other pair of inputs for a given sensitivity
(see Deﬁnition 11). Applying such worst-case inputs (x0, x1) to a mechanism M leads to a pair of worst-case
distributions (M (x0), M (x1)). Such worst-case inputs appear in many [6], but not all proofs for DP. In more
complex cases, explicit worst-case distributions are used in the analysis [1].
To understand why worst-case distributions in general are an integral part of DP proofs, we now discuss,
on an abstract level, how we typically prove that a mechanism satisﬁes DP.
Most diﬀerential privacy analyses implicitly use worst-case distributions For illustration, we
consider a mechanism, where M (D, q) (for a database D and a query q) can be divided into a precise
response f (D, q) to a query and an independent noise distribution N . In the simplest case, mechanisms are
of the form M (D, q) = f (D, q) + N . Examples of this structure include the Laplace mechanism, the Gauss
mechanism, as well as any other distribution of noise N added to some function f (D, q), where N does not
depend on f (D, q). In all these cases, diﬀerential privacy guarantees can be calculated based solely on the
distribution of the noise and on the sensitivity ∆f =
D1, D2 neighboring|f (D1, q) − f (D2, q)|.2
max
To show that M satisﬁes (approximate) diﬀerential privacy, the proof typically analyzes the two distri-
butions N and N + ∆f , implicitly assuming that f (D1, q) = 0 and f (D2, q) = ∆f . The proof then argues
that for any value ∆(cid:48) < ∆f the distributions N and N + ∆(cid:48) also satisfy diﬀerential privacy. From this
simpliﬁed analysis it can then be derived (implicitly) that for all other values of f (D1, q) and f (D2, q) s.t.,
|f (D1, q) − f (D2, q)| ≤ ∆f , f (D1, q) + N and f (D2, q) + N also satisfy diﬀerential privacy, which concludes
the analysis. In any such analysis, there are worst-case distributions N and N + ∆f , thus, our approach is
compatible.
A prominent example of such an analysis is a recent work on a diﬀerentially private a mechanism for
privacy-preserving stochastic gradient descent [1].
In this work, Abadi et al. ﬁrst prove that a pair of
a Gaussian distribution a Gaussian mixture distribution is worst case for their analysis, and they then
estimate diﬀerential privacy for this pair of distributions.
Worst-case distributions for the Laplace mechanism As an example, let us consider counting queries
q with sensitivity 1 to which Laplace noise is added: the mechanism M that gets a database D as input is
deﬁned as M (D) := q(D) + LPλ,0, where LPλ,µ is the Laplace distribution with scale parameter λ and mean
µ (and f (D, q) := q(D)). In this example, it suﬃces to only consider LPλ,0 and LPλ,1, with means 0 and
1, instead of considering M (D0) and M (D1) for all possible combinations of neighboring databases D0 and
D1: Let D0 and D1 be two neighboring databases where the true answers to a query q are q(D0) = x and
q(D1) = x + 1, respectively, for some value x.3 We can map any output y drawn from LPλ,µ (for µ ∈ {0, 1})
2The notion of neighboring databases diﬀers from application to application. That said, databases are typically called
neighboring if they diﬀer in at most one row.
3Since the diﬀerential privacy guarantee and analysis are symmetric, we can assume without loss of generality that q(D0) <
q(D1).
4
to y + x to obtain the correct adversarial view for the respective scenario M (Di) = q(Di) + LPλ,0.
What if my diﬀerential privacy analysis doesn’t implicitly use worst-case distributions? If the
mechanism does not consist of and cannot be reduced to independent noise being added to a numerical value,
the above simpliﬁed description does not immediately apply. However, Kairouz, Oh, and Viswanath [14],
and Murtagh and Vadhan [22, Lemma 3.2 & Lemma 3.7] show that for any (ε, δ)-DP mechanism M there is
a worst case mechanism Mε,δ operating on a single bit such that ADP guarantees for Mε,δ directly translate
to ADP guarantees for M , even under r-fold adaptive composition.4
In more detail, they show [22, Lemma 3.2] that there is a probabilistic translation T that relates every
diﬀerentially private mechanism M on two neighboring inputs D0 and D1 to a generic pair of distributions
Mε,δ(0) and Mε,δ(1). They then leverage the post-processing property of diﬀerential privacy to show that
analyzing Mε,δ(0) and Mε,δ(1) is suﬃcient, even under composition.
Thus, considering such worst-case distributions and their behavior under composition is suﬃcient for
deriving bounds on the mechanism M . Moreover, even for diﬀerent mechanisms and adaptively chosen
neighboring inputs, one can compute sound bounds on diﬀerential privacy by only considering the distribu-
tions Mε,δ(0) and Mε,δ(1). This technique gives us a fall-back plan for computing bounds on the distributions
of Mε,δ(0) and Mε,δ(1) if no more than ε and δ of the mechanism M is known. If more about the distribution
of the output of M is known (ideally two exact worst-case output distributions) we can derive signiﬁcantly
tighter bounds.
2.2 Tight ADP on distributions
Approximate diﬀerential privacy is typically captured with two parameters ε and δ. In this work we show
that considering not just one such pair of parameters, but a parameter space helps to derive tight adaptive
r-fold ADP composition results. This parameter space of two distributions can be represented as a function
δ(ε) such that (ε, δ(ε))-ADP holds and δ(ε) is minimal (for ε ≥ 0). To capture this minimality, we deﬁne
(tight) diﬀerential privacy (generalized to pairs of distributions) and show how to precisely compute δ(ε).
Deﬁnition 1 ((Tight) ADP). Two distributions A and B over the universe U are (ε, δ)-ADP, if for every
set S ⊆ U,
PA(S) ≤ eεPB(S) + δ(ε) and PB(S) ≤ eεPA(S) + δ(ε),
where PA(x) denotes the probability of the event x in A and PB(x) denotes the probability of the event x in
B. We call A and B tightly (ε, δ(ε))-ADP if they are (ε, δ(ε))-ADP, and ∀δ(cid:48) ≤ δ(ε) such that A and B are
(ε, δ(cid:48))-ADP we have δ(ε) = δ(cid:48).
A note on utility and sensitivity.
In the remainder of the paper, we consider pairs of distributions.
In particular, analyzing the ADP-parameter of mechanisms amounts to analyzing the respective worst-case
inputs or worst-case distributions for a given sensitivity. Hence, we can abstract away from the sensitivity
of two inputs and we can abstract away from the utility functions of a task.
Computing a tight ADP bound. To see that and how we can compute δ(ε), ﬁrst consider that any
pair of distributions is (ε, 1)-ADP for arbitrary ε ≥ 0. More precise bounds for distributions A and B can
be captured by setting δ(ε) to the area between the probability distributions of B and a scaled-up version
of A: we multiply every point of the curve of A with eε (which is not a probability distribution anymore,
because it sums up to eε instead of to 1). Any area where B is larger than this scaled-up curve contains
probability mass for events x outside of the multiplicative bound, i.e., for which we have PA(x) ≤ eεPB(x).
The diﬀerence between those terms is precisely what we need to characterize. We refer to Figure 1 for the
(ε, δ(ε))-graph of possible (tight) ADP-bounds for two truncated Gaussian distributions (left side) and for a
graphical depiction of this intuition (right side).
4For some mechanisms the privacy parameters (e.g., a Gaussian distribution’s standard deviation σ) can be adaptively chosen
for every run and can depend on previous adversarial observations. If privacy cannot be bounded per response or the number
of relevant responses for computing privacyis unbounded, worst-case distributions might not exist [23].
5
Figure 1: A graph depicting δ(ε) for the truncated Gauss mechanism (left) and a graphical depiction of how
to compute δ(ε) for the truncated Gauss mechanism (right). Note that eε·A is not a probability distribution.
Lemma 1. For every ε, two distributions A and B over a ﬁnite universe U are tightly (ε, δ)-ADP with
(cid:32)(cid:88)
(cid:88)
x∈U