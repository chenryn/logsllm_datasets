# Tight on Budget?: Tight Bounds for r-Fold Approximate Differential Privacy

**Authors:**
- Sebastian Meiser, University College London, United Kingdom
- Esfandiar Mohammadi, ETH Zurich, Switzerland

**Note:** The authors are listed in alphabetical order and contributed equally to this work.

**Date:** September 5, 2018

## Abstract

Many applications, such as anonymous communication systems, privacy-enhancing database queries, and privacy-preserving machine learning methods, require robust guarantees under thousands or even millions of observations. The concept of r-fold approximate differential privacy (ADP) provides a well-established framework with a precise characterization of the degree of privacy after r observations by an attacker. However, existing bounds for r-fold ADP are often loose, leading to over-cautious choices for perturbation randomness and thus suboptimal utility or high costs when used for estimating the required degree of noise.

We present a numerical and widely applicable method, called privacy buckets, for capturing the privacy loss of differentially private mechanisms under composition. Using privacy buckets, we compute provable upper and lower bounds for ADP for a given number of observations. We compare our bounds with state-of-the-art bounds for r-fold ADP, including Kairouz, Oh, and Viswanath’s composition theorem (KOV), concentrated differential privacy, and the moments accountant. While KOV provided optimal bounds for heterogeneous adaptive k-fold composition, we show that tighter bounds can be derived for specific sequences of mechanisms by considering their structure. Our results significantly improve upon previous bounds for the Laplace mechanism, the Gaussian mechanism, a timing leakage reduction mechanism, and stochastic gradient descent, except for the Laplace mechanism, where we match the KOV bound. Our lower bounds nearly meet our upper bounds, indicating that no significantly tighter bounds are possible.

## Contents

1. **Introduction**
   1.1. Contribution
2. **Background and Related Work**
   2.1. Worst-case Distributions for ADP
   2.2. Tight ADP on Distributions
   2.3. Practical Relevance of Tight Privacy Bounds
   2.4. Composition of Differential Privacy
   2.5. Related Work
3. **Privacy Buckets of Distributions**
   3.1. Informal Description of Privacy Buckets
   3.2. A Formal Description of Privacy Buckets
   3.3. Buckets per Atomic Event
4. **Reducing and Bounding the Error**
   4.1. Buckets with Error Correction Terms
   4.2. Buckets and Error Correction Terms per Element
   4.3. Helpful Properties of Error Correction Terms
   4.4. The Approximated Delta with Error Correction
   4.5. Main Result
   4.6. Implementation
5. **Evaluation and Comparison**
   5.1. Embedding the Laplace Mechanism
   5.2. Embedding the Gauss Mechanism
   5.3. Embedding CoverUp’s Data
   5.4. Embedding the Stochastic Gradient Descent
   5.5. Embedding the Randomized Response Mechanism
   5.6. Computing Kairouz et al.’s Composition Theorem
   5.7. Computing Bounds Based on Rényi Divergence
   5.8. Comparison Results
6. **Comparison of the Gaussian and Laplace Mechanisms**
7. **Application to Vuvuzela**
   7.1. Protocol Overview
   7.2. Tighter Privacy Analysis for the Dialing Protocol
   7.3. Tighter Privacy Analysis for the Conversation Protocol
8. **Conclusion and Future Work**
9. **Acknowledgements**

## 1. Introduction

Approximate differential privacy (ADP) [4] quantifies the privacy leakage of systems using two parameters (ε, δ), balancing the system's usefulness and privacy. Since its introduction, ADP has been successfully applied to various applications, including sensitive database queries, deep neural network training, and anonymous communication [26]. Privacy leakage, measured by (ε, δ), inevitably increases under continuous observation, leading to eventual privacy deterioration (e.g., Apple's case [25]). In many scenarios, continuous attacker observation is unavoidable, with potential observation points numbering in the thousands or millions.

Precisely computing (ε, δ) after r observations, known as r-fold ADP bounds, is challenging. Imprecise bounds can lead to either an incorrect perception of privacy leakage or overly cautious system parameter choices, resulting in suboptimal utility or high costs. There is extensive research on approximating r-fold ADP bounds [8, 14, 1, 20, 7, 2]. Early work [8, 14] did not consider the output distribution shape (mechanism-oblivious bounds), which were tight for the Laplace mechanism but imprecise for others like the Gaussian mechanism. Recent work [1, 20, 7, 2] introduced mechanism-aware bounds, achieving tighter results for specific mechanisms, though it was unclear how tight these bounds could be.

### 1.1. Contribution

We introduce a numerical method, privacy buckets, for computing upper and lower r-fold (ε, δ)-ADP bounds that account for the mechanisms and their fixed noise parameters. This method generalizes prior work on generic adaptive r-fold ADP bounds [14, 21]. Our upper bounds significantly improve upon state-of-the-art bounds, and our lower bounds nearly match the upper bounds, indicating that no significantly tighter bounds are possible.

Our evaluations show that privacy buckets provide insights into the composition behavior of various mechanisms. For appropriate scale parameters and standard deviations, the Laplace and Gaussian mechanisms converge to the same privacy leakage from a sufficiently high number of observations onward. Our method is useful for classical differential privacy mechanisms and can be applied to any privacy analysis resulting in ADP. We demonstrate this by computing bounds for the anonymous communication system Vuvuzela [26], the stochastic gradient descent mechanism for deep learning [1], and timing-leakage histograms of a recently introduced browser extension for deniable communication [24].

## 2. Background and Related Work

In this section, we review differential privacy, highlight an implicit assumption in the analysis of differentially private mechanisms, generalize differential privacy to pairs of distributions, and position our work in the literature.

### 2.1. Worst-case Distributions for ADP

Differential privacy (DP) [3] quantifies the closeness of the output distributions of a mechanism on similar inputs. A mechanism M is ε-DP if for any two closely related inputs D1, D2, and any subset S of the output space U, Pr[M(D1) ∈ S] ≤ eε · Pr[M(D2) ∈ S]. Approximate differential privacy (ADP) [4] allows for distributions to exceed a limiting factor ε, with a deviation limited to δ: Pr[M(D1) ∈ S] ≤ eε · Pr[M(D2) ∈ S] + δ. In this work, we focus on ADP.

Classically, DP arguments involve the output of a probabilistic mechanism M on similar inputs (e.g., neighboring databases). Since M is probabilistic, applying M to any input D results in a random variable with outputs from a distribution M(D). DP requires the outputs of M on all pairs of neighboring databases to be closely related, quantified by (ε, δ).

Our approach operates on individual pairs of distributions, which, while unconventional, leads to more general results. Considering pairs of distributions is natural in the context of differentially private mechanisms, especially for analyzing mechanisms in the presence of arbitrary adversarial queries. In the simplest case, proofs analyze mechanisms using worst-case inputs (x0, x1) that are as bad as any other pair of inputs for a given sensitivity. Applying such worst-case inputs to a mechanism M leads to a pair of worst-case distributions (M(x0), M(x1)).

Most DP analyses implicitly use worst-case distributions. For example, a mechanism M(D, q) can be divided into a precise response f(D, q) to a query and an independent noise distribution N. In the simplest case, mechanisms are of the form M(D, q) = f(D, q) + N. Examples include the Laplace mechanism, the Gaussian mechanism, and any other distribution of noise N added to some function f(D, q), where N does not depend on f(D, q). DP guarantees can be calculated based solely on the distribution of the noise and the sensitivity ∆f = max|f(D1, q) - f(D2, q)| for neighboring D1 and D2.

To show that M satisfies (approximate) differential privacy, the proof typically analyzes the distributions N and N + ∆f, assuming f(D1, q) = 0 and f(D2, q) = ∆f. The proof then argues that for any value ∆' < ∆f, the distributions N and N + ∆' also satisfy differential privacy. From this simplified analysis, it can be derived that for all other values of f(D1, q) and f(D2, q) such that |f(D1, q) - f(D2, q)| ≤ ∆f, f(D1, q) + N and f(D2, q) + N also satisfy differential privacy. In such analyses, there are worst-case distributions N and N + ∆f, making our approach compatible.

A prominent example is the recent work on a differentially private mechanism for privacy-preserving stochastic gradient descent [1]. Abadi et al. first prove that a pair of a Gaussian distribution and a Gaussian mixture distribution is worst-case for their analysis and then estimate differential privacy for this pair.

### 2.2. Tight ADP on Distributions

Approximate differential privacy is typically captured with two parameters ε and δ. In this work, we show that considering a parameter space helps derive tight adaptive r-fold ADP composition results. This parameter space of two distributions can be represented as a function δ(ε) such that (ε, δ(ε))-ADP holds and δ(ε) is minimal (for ε ≥ 0).

**Definition 1 (Tight ADP):** Two distributions A and B over the universe U are (ε, δ)-ADP if for every set S ⊆ U,
\[ P_A(S) \leq e^\varepsilon P_B(S) + \delta(\varepsilon) \]
\[ P_B(S) \leq e^\varepsilon P_A(S) + \delta(\varepsilon) \]
where \( P_A(x) \) denotes the probability of the event x in A and \( P_B(x) \) denotes the probability of the event x in B. We call A and B tightly (ε, δ(ε))-ADP if they are (ε, δ(ε))-ADP, and for all δ' ≤ δ(ε) such that A and B are (ε, δ')-ADP, we have δ(ε) = δ'.

**Note on Utility and Sensitivity:** In the remainder of the paper, we consider pairs of distributions. Analyzing the ADP parameter of mechanisms involves analyzing the respective worst-case inputs or worst-case distributions for a given sensitivity. Thus, we can abstract away from the sensitivity of two inputs and the utility functions of a task.

**Computing a Tight ADP Bound:** Any pair of distributions is (ε, 1)-ADP for arbitrary ε ≥ 0. More precise bounds for distributions A and B can be captured by setting δ(ε) to the area between the probability distributions of B and a scaled-up version of A: multiply every point of the curve of A with \( e^\varepsilon \) (which is not a probability distribution anymore, as it sums up to \( e^\varepsilon \) instead of 1). Any area where B is larger than this scaled-up curve contains probability mass for events x outside of the multiplicative bound, i.e., for which we have \( P_A(x) \leq e^\varepsilon P_B(x) \). The difference between these terms characterizes δ(ε). See Figure 1 for a graphical depiction.

**Figure 1:** A graph depicting δ(ε) for the truncated Gauss mechanism (left) and a graphical depiction of how to compute δ(ε) for the truncated Gauss mechanism (right). Note that \( e^\varepsilon \cdot A \) is not a probability distribution.

**Lemma 1:** For every ε, two distributions A and B over a finite universe U are tightly (ε, δ)-ADP with
\[ \delta(\varepsilon) = \sum_{x \in U} \max\{0, P_B(x) - e^\varepsilon P_A(x)\} \]

This lemma provides a way to compute the tightest possible δ(ε) for any given ε, ensuring that the bounds are as tight as possible.