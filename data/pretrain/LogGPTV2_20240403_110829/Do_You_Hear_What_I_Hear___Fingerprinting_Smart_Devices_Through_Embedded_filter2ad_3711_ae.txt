53.4
[13,14]
98.2
20
13
21.8
39.9
33.9
11.8
35.2
31.5
31.1
38.7
18.5
32.4
90
80.6
63.6
[13,7]
91.5
GMM
25.7
7.1
18.7
40.3
26.3
17.5
38.4
40.3
36.8
41.1
17.9
30
91.4
80
53.8
[13,14]
92.9
k-NN
40.2
22.7
22.6
17.3
29.1
12.6
17.2
31.8
28.5
30
22.5
24.6
89
71.5
67.8
GMM
36.9
29.6
24.8
24.8
22.2
16.3
22.6
28.1
26.1
32.8
20.3
23.8
93.5
55.3
51.3
[13,8,12]
93
[13,8,12]
96.7
collect 10 samples per audio clip, half of which is used for training
and the remaining half for testing. Table 11 highlights our ﬁndings.
We see that we were able to ﬁngerprint all test samples accurately.
Thus combining the idiosyncrasies of both the speaker and micro-
phone seems to be the best option to distinguish smartphones of
same maker and model. So, if a malicious app can get access to
the speaker (which does not require explicit permission) and mi-
crophone (which may require explicit permission, but many games
nowadays require access to microphone anyway) it can success-
fully track individual devices.
Table 11: Fingerprinting similar smartphones using mic & speaker
Audio
Type
Instrumental
Human speech
Song
k-NN
Features∗ AvgP r AvgRe AvgF 1
100
100
100
[13]
[13]
[13]
100
100
100
100
100
100
∗ Features taken from Table 8
GMM
Features∗ AvgP r AvgRe AvgF 1
100
100
100
[13]
[13]
[13]
100
100
100
100
100
100
6.6 All Combination of Devices
In this section we look at ﬁngerprinting all the devices in our
collection (i.e., 50 android smartphones after excluding iphone5
and Sony Ericsson W518). We combine microphone and speaker
to generate the auditory ﬁngerprint of smartphones. We do so be-
cause in the previous sections we found that combining speaker and
microphone yielded the highest accuracy. First, we perform acous-
tic feature exploration to determine the dominant features. Table 12
highlights our ﬁndings. We see that again MFCCs are the dominant
features for all categories of audio excerpt. This is expected as we
saw similar outcomes in Table 4 and Table 8.
Next, we evaluate how effectively we can ﬁngerprint the 50 an-
droid smartphones. The setting is similar to all the previous exper-
iments where each audio clip is recorded 10 times, 50% of which
is used for training and the remaining 50% for testing. We use our
android app to collect all the audio samples. Table 13 shows our
obtained ﬁngerprinting results. We see that we can obtain an F1-
score of over 98% in ﬁngerprinting all the 50 smartphones. This
result suggests that a malicious app having access to microphone
and speaker can easily ﬁngerprint smartphones.
6.7 Sensitivity Analysis
In this section we investigate how different factors such as audio
sampling rate, training set size, the distance from audio source to
recorder, and background noise impact our ﬁngerprinting perfor-
mance. Such investigations will help us determine the conditions
under which our ﬁngerprinting approach will be feasible, specially
if the attacker is tracking devices in public locations. For the fol-
Table 12: Feature exploration using sequential forward selection
technique for all smartphones
#
Feature
RMS
ZCR
Spectral Irregularity
Low-Energy-Rate
Spectral Centroid
Spectral Entropy
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Sequential Feature Selection
Spectral Spread
Spectral Skewness
Spectral Kurtosis
Spectral Rolloff
Spectral Brightness
Spectral Flatness
Chromagram
Tonal Centroid
MFCCs
Max F1-Score
Maximum F1-Score (%)
Song
Human Speech
Instrumental
k-NN GMM k-NN GMM k-NN GMM
76.8
82.7
51.3
45.9
33.8
45.2
30.1
35.6
67.7
56.2
35.3
46.1
74.1
57.4
50.3
59.9
54.2
45
62.8
49.5
61.7
52.1
68.3
61
99.6
100
96.2
98.2
98.5
96
[13]
[13]
100
99.6
80
48.2
40.6
34.7
60.8
47
57
53.9
47.7
53.5
54.5
60.1
100
93.4
89
[13]
100
87.3
50.3
19.4
23.7
46.3
25.9
54.2
34.5
37.1
48.4
38.1
61.6
100
98.9
95.5
[13]
100
84
45.9
15.4
25.8
48.1
23.6
49.7
32.5
38.6
45.9
35.3
63.4
99.6
95.8
91.8
[13]
99.6
78.7
48.5
31.9
25.7
67.7
26.9
70.9
52.7
51.5
59.1
59.2
67.3
100
99.6
98.5
[13]
100
Table 13: Fingerprinting all smartphones using mic & speaker
Audio
Type
Instrumental
Human speech
Song
Features∗ AvgP r AvgRe AvgF 1
k-NN
[13]
[13]
[13]
99.3
99.7
99.7
98.8
99.6
99.6
99
99.6
99.6
∗ Features taken from Table 12
GMM
Features∗ AvgP r AvgRe AvgF 1
98.3
99.3
100
98.6
99.4
100
98.1
99.2
100
[13]
[13]
[13]
lowing set of experiments we only focus on ﬁngerprinting similar-
model smartphones from the same vendor (as this has been shown
to be a tougher problem in the previous section) and consider only
ﬁngerprinting speakers as this is applicable to the scenario where
the attacker is tracking devices in public locations. We also con-
sider recording only ringtones (i.e., audio clips belonging to our
deﬁned ‘Instrumental’ category in Table 3) for the following ex-
periments. Since we are recording ringtones we use the features
highlighted in Table 8 under the ‘Instrumental’ category.
6.7.1
First, we investigate how the sampling rate of audio signals im-
pacts our ﬁngerprinting precision. To do this, we record a ringtone
at the following three frequencies: 8kHz, 22.05kHz and 44.1kHz.
Each sample is recorded 10 times with half of them being used for
training and the other half for testing. Figure 7 shows the average
precision and recall obtained under different sampling rates. As we
can see from the ﬁgure, as sampling frequency decreases, the pre-
cision/recall also goes down. This is understandable, because the
higher the sampling frequency the more ﬁne-tuned information we
have about the audio sample. However, the default sampling fre-
Impact of Sampling Rate
quency on most hand-held devices today is 44.1kHz [4], with some
of the latest models adopting even higher sampling rates [1]. We,
therefore, believe sampling rate will not impose an obstacle to our
ﬁngerprinting approach, and in future we will be able to capture
more ﬁne grained variations with the use of higher sampling rates.
Figure 7: Impact of sampling frequency on precision/recall.
6.7.2 Varying Training Size
Next, we consider performance of the classiﬁers in the presence
of limited training data. For this experiment we vary the training
set size from 10% to 50% (i.e., from 1 to 5 samples per class) of all
available samples. Table 14 shows the evolution of the F1-score as
training set size is increased (values are reported as percentages).
We see that as the training set size increases the F1-score also rises
which is expected. However, we see that with only three samples
per class we can achieve an F1-score of over 90%. This suggests
that we do not need too many training samples to construct a good
predictive model.
Table 14: Impact of varying training size
GMM
k-NN
Training
samples
per class AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1
Features [13,14]∗
Features [13,14]∗
1
2
3
4
5
42
79.2
91.3
95.3
96.7
49.3
80
89.3
94.7
96
45.3
79.6
90.2
95
96.3
50
80.4
91.7
95.6
98.4
53.3
80
89.3
94.7
98.1
51.6
80.2
90.5
95.1
98.3
∗ Feature numbers taken from Table 8
animal sounds from a far distance) could help in increasing the ﬁn-
gerprinting precision at even longer distances.
Table 15: Impact of varying distance
GMM
k-NN
Features [13,14]∗
Features [13,14]∗
AvgP r AvgRe AvgF 1 AvgP r AvgRe AvgF 1
Distance
(in meters)
0.1
1
2