    4)  Secure Data Transfer Protocol 
Data  confidentiality  is  one  of  fundamental  prerequisites  of 
distributed storage framework. For assuring quality-of-service, 
a  light-weight  and  ready  system  convention  for  exchange 
information 
the  proposed 
framework the information exchanged amongst customers and 
servers  are  secured  and  encrypted  using  AES  algorithm  with  
use of secret key. AES is more secure that other cryptographic 
algorithms.  AES  underpins  bigger  key  sizes,  it  is  quicker  in 
both hardware and software. 
is  additionally 
required. 
In 
5)  Data Distribution and Recovery  
is  assembled 
the  Proposed  framework 
As 
in  view  of 
distributed  key-value  storage  framework.  Clearly  the  meta-
data of files and documents are put away distributed and can 
be  duplicated  for  failure  of  any  server  and  recovery.  For  the 
Uploading  of  the  documents  with  replication  we  utilize 
Partition  algorithms  and  Merging,  as  explained  below.    The 
Partition  algorithms  explains  how  the  chunks  are  distributed 
and replicated over the multiple Cloud Service Providers(CSP) 
such  that  in  failure  of  any  of  the  cloud  the  chunks  can  be 
recovered  from  other  working  CSP.  The  merging  algorithm 
explains  how  the  chunks  of  the  file  are  merged  to  form  a 
complete file while downloading. For example, if a  file is to 
be uploaded to the cloud storage, the file is chunked in to three 
chunks. These three chunks are distributed and replicated over 
three  CSPs  using  partition  algorithms  such  that  chunk1  and 
2017 Third International Conference on Computing, Communication, Control And Automation (ICCUBEA) 
chunk2 on CSP1,  chunk1 and chunk3 on CSP2  and chunk2 
and chunk3 on CSP3. In case of failure of single CSP out of 
three,  the  chunks  can  be  recovered  from  other  two  working 
CSPs  by  just  performing  the  ORing  operation  on  the  chunks 
recovered  from  the  the  two  CSPs.    The  pseudo-code  of  the 
algorithms are explained below.  
n = number of Cloud Service providers, k = number of Cloud 
Service  providers  from  where  chunks  can  be  recovered,  
Where k<=n, Ri = row of chunks of files 
Algorithm:Partitioning (For Uploading) 
Input:  File 
Output: Data splitted on N cloud 
Process: 
For each row in Ri do 
   For 1 to K do 
       R_no= create Random number between 1 to K; 
       Cloud_no=Selectcloud[R_no]; 
       Upload Ri on cloud_no 
    end for 
end for 
Algorithm:Merging (For Downloading) 
Input: File 
Output: Get the File downloaded 
Process: 
    Fetch data from any k cloud 
    Perform ORing on retrieved data from K  clouds 
     File Status - Completely Downloaded 
6) Architectural Flow of the Proposed System 
Fig. 3. Architectural Flow of the Systems 
The  Fig.  3  gives  the  architectural  flow  of  the  proposed 
system.  The  flow  characterizes  the  processes  and  functions 
that  are  used  in  the  system;  these  are  the  functions  of  the 
system  in  the  form  of  processes  that  are  executed.  The  flow 
represents  the  absolute  state  chart  of  implementation  from 
initial process to ending step of execution.  
IV. RESULTS AND ANALYSIS 
In  this  section  the  performance  analysis  of  the  System  with 
existing  system  i.e.  BFC[1]  is  presented,  the  performance  is 
measured based on the time required for uploading the file to 
the  cloud  server  and  downloading  the  file  from  the  cloud 
server,  while 
  The 
comparability of system with existing system is shown based 
on  size  of  meta-data  of  file  is  considered,  compression, 
uploading time and deduplication mechanisms.  
fetching  various 
types  of 
files. 
A.  Comparison  Based  on  Deduplication  And  Compression 
Technique Usage 
TABLE  I. DEDUPLICATION COMPARISON WITH 
EXISTING SYSTEM 
Deduplication 
Dropbox  OneDrive  Google Drive  BFCSS 
Single User 
       Yes 
        No 
           No 
     Yes 
Multiple User 
        No 
        No 
           No 
      Yes 
Table  I  shows  that  the  investigation  is  done  to  paradigm  the 
de-duplication  ability  of  Proposed  System  and  other  cloud 
service provider like  OneDrive, Dropbox and Google Drive. 
The  table  shows  that  Google  drive  and  one  drive  do  not 
support  for  deduplication,  whereas  the  proposed  system  has 
this advantage and ability to detect the duplication. 
(Ts),  Total  storage  space 
The table II shows the file uploaded by the specific user, the 
Total  storage  space  required  Without  Deduplication  and 
Compression 
required  With 
Deduplication and Compression. 
Total  Size  Without  Deduplication  and  Compression  (Ts)  = 
1937456 Bytes 
Total Size With Deduplication (Ted) = 782360 Bytes 
Total  Size  With  Deduplication  and  Compression  (Tsdc)  = 
761038 Bytes 
Total Storage Space Saved (Ts - Tsdc ) =  1176418 Bytes  
Total Percentage of Storage Saved = (Ts - Tsdc / Ts) * 100 
= (1937456 - 761038/1937456) * 100 
= 60.71972731251703 % 
Total Percentage of Storage Saved = 60.71972731251703 % 
For Existing System -  
                                          = 60.2 % 
Total Size With Deduplication (Ted) = 782360 Bytes 
2017 Third International Conference on Computing, Communication, Control And Automation (ICCUBEA) 
For Proposed System -  
Total  Size  With  Deduplication  and  Compression 
   = 761038 Bytes 
(Tsdc)  
Total  Percentage  of  Storage  Saved  Compared  to  Existing 
System :- 
   = ((Ted -Tsdc) /  Ted ) * 100 
   = (782360 -  761038 / 782360 ) * 100  
   = 2.78953438314 % 
   = 2.8 % 
Total  Percentage  of  Storage  Space  Saved  Compared  to 
Existing System = 2.8 % 
TABLE II. UPLOADED FILE SIZE INFO WITH DEDUPLICATION AND 
COMPRESSION 
File Name (No of Files 
Uploaded) 
Size of the 
file  (Bytes) 
Size of  File after De
duplication Check 
and Compression 
(Bytes) 
demo.txt(2) 
Pensionletter.pdf 
DOC00.pdf 
BFCUMLdiagrams.docx 
24*2 
53248 
98304 
45056 
TestPlan.docs(2) 
16384*2 
1.login.png(3) 
Total 
569344*3 
1937456 
21 
49330 
92499 
38745 
12088 
568355 
761038 
Fig. 4. Total Storage Space With and Without Compression and 
Deduplication (Comparison) 
The  Fig.  4  represents  the  graphical  comparison  total  Storage 
Space With and Without Compression and Deduplication. 
The  Table  III  shown  below  describes  the  comparability 
between  two  clouds  BFC  and  Proposed  System  about  the 
compression.  Compression  has  the  various  advantages  like 
less disk space, faster writing and reading, Faster file transfer 
and  Variable  dynamic  range.  This  will  improve  the  storage 
utilization of the proposed system. 
TABLE III. COMPRESSION COMPARISON WITH ORIGINAL 
SYSTEM  
Parameter 
BFC [1] 
Proposed 
Sky_Drive 
Cloud_Drive 
Deduplication 
Yes 
Compression 
No 
B.  Metadata Comparison 
Yes 
Yes 
No 
No 
No 
No 
Dropbox  is  a  cloud-based  storage  service  that  allows  end 
clients to store different data files. The fundamental objection 
in the Dropbox model is a chunk of 4MB data segment. On the 
off chance that a document is bigger than this designed size, it 
will  be  part  in  a  few  chunks.  Each  chunk  is  an  autonomous 
element, which is dignified by a SHA256 Value. In Dropbox 
meta-data of each document contains a summation of SHA256 
of its chunks. Hence, its size is directly proportional to size of 
File. For bigfile, it has a major meta-data brought on by a large 
number of chunks. 
Fig. 5. Comparison of File Metadata size with Dropbox 
As  shown  in  Fig.  5,  in  the  proposed  framework, 
System  has  a  fixed  size  meta-data  of  each  File,  so  it  is  less 
demanding  to  store  and  scale  storage  framework  for  huge 
2017 Third International Conference on Computing, Communication, Control And Automation (ICCUBEA) 
REFERENCES 
security  of  data  using  PoW  and  the  data-privacy  using 
Convergent encryption in deduplication. 
[1]  Nguyen  Trung,  Khac  Vu  Tin,  Noi  V  and M.  Nguyen,  VietNam,  BFC: 
High-Performance  Distributed  Big-File  Cloud  Storage  Based  On  Key 
value  Store, 
in  Proceedings:  Software  Engineering,  Artificial 
Intelligence,  Networking  and  Parallel/Distributed  Computing  (SNPD) 
IEEE, June,  2015, pp.1-6. 
[2]  Minh.H.Nguyen and Trung.T.Nguyen, Design Sequential Chunk identity 
with  Lightweight  meta-data  for  Big  File  Cloud  Storage,  International 
Journal of Computer Science and Network Security, Sept. 2015, pp.12-
20. 
[3]  X.Chen, M. Li, Jin. Li,  P. Lee, and W. Lou, Secure Deduplication with 
efficient and reliable convergent key management. : IEEE Transactions 
on Parallel and Distributed Systems vol. 25,n.6, 2013, pp.1615 - 1625. 
[4]  Huang  Xin,  Li  Xin,  Chen  Xiaofeng  ,  Tang  Shaohua  and  Yang  Xiang 
Mehedi  Hassan  Mohammad  and  Alelaiwi  Abdulhameed,  Secure 
Distributed  de-duplication  Systems  with  Improved  Reliability,  IEEE 
Transactions on Computers vol. 64, 12) 2015, pp.3569-3579. 
[5]  R.Molva,  P.Puzio,  M.Onen  and  S.Loureiro,  ClouDedup:  Secure 
for  Cloud  Storage,  Cloud 
IEEE  5th 
Deduplication  with  Encrypted  Data 
Computing  Technology  and  Science 
International Conference, 2013. 
(CloudCom), 
[6]  Drago Idilio, Sperotto Anna, Mellia Marco, M. Munafa Maurizio,Sadre 
Ramin  and  Pras  Aiko,  Inside  Dropbox:  Understanding  Personal  Cloud 
Storage  Services,  In:  Proceedings  of  the  2012  Internet  Measurement 
Conference, ACM, pp.481-494. 
[7]  Chang Fay, Dean Jeffrey, Ghemawat Sanjay, Hsieh Wilson C, Wallach 
Deborah A, Burrows Mike, Chandra Tushar, Fikes Andrew and Gruber 
Robert E, Bigtable:  A Distributed Storage System for Structured Data, 
Seventh Symposium on Operating System Design and Implementation, 
Nov. 2006, pp.1-7. 
[8]  H.  Gobioff,,  S.  Ghemawat  and  S.T  Leung,  The  Google  File  System, 
SOSP '03 Proceedings of the nineteenth ACM symposium on Operating 
systems principles, 2003, pp.29-43 . 
[9]  Secure Hash Standard, Computer Systems Laboratory National Institute 
of  Standards  and  Technology  Gaithersburg,  FIPS  PUB  180-1,  Federal 
Information Processing Standards Publication, 1995. 
[10]  M. Mellia, H. Slatman,  I. Drago, E. Bocchi and A. Pras. Benchmarking 
Personal  cloud  storage,  In  Proceedings  of  Conference  on  Internet 
measurement conference, ACM, 2013, pp.205–212. 
document.  The  measure  of  information  for  trading  of  meta-
data amongst customers and servers is decreased. 
C. Uploading Time Comparison 
Fig. 6 signifies the uploading time of varied data files is taken 
into account for the 3 frameworks as existing system (without 
compression),  planned  System  (with  compression).  From  the 
figure  it's  clearly  determined  that  uploading  time  needed 
utilizing  the  proposed  framework  is  a  smaller  amount  once 
contrasted with existing transfer technique. 
Fig. 6. Uploading time of file for existing and proposed system (Comparison) 
V. CONCLUSIONS 
Cloud Storage Service, the proposed storage service is based 
on distributed file storage system. Every big-file that is to be 
transferred is fragmented into multiple fixed-size chunks. Each 
file  in  the  system  has  same  and  precised  size  of  meta-data 
irrespective of file-size. The chunks of a file have a adjoining 
Identity (ID) range, thus it is simple and easy to distribute data 
and scale-out the storage system. This research also brings the 
advantages of key value store into big-file data store. The de-
duplication  process  for  Proposed  System  uses  SHA  hash 
algorithm to fast discover data-duplication. It is useful to save 
storage space and network bandwidth, as the system uses file 
compression  technique  when  many  users  upload  the  same 
static data.  
In our research attempt,  the  concentration is on the 
de-duplication, compression and encryption of data of the files 
that are to be uploaded on to the cloud storage. However, the 
security  of  the  data  can  be  reinforced  and  increased  by 
introducing Proof-of-Ownership (PoW)  for data with respect 
to  the  user.  Thus,  our  future  work  would  be  to  broaden  the 
2017 Third International Conference on Computing, Communication, Control And Automation (ICCUBEA)