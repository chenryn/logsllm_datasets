### 4) Secure Data Transfer Protocol

Data confidentiality is a fundamental prerequisite for a distributed storage framework. To ensure quality-of-service, the proposed framework employs a lightweight and efficient system convention for secure information exchange. In this framework, data exchanged between customers and servers is secured and encrypted using the AES (Advanced Encryption Standard) algorithm with a secret key. AES is considered more secure than other cryptographic algorithms due to its support for larger key sizes and its speed in both hardware and software implementations.

### 5) Data Distribution and Recovery

The proposed framework is designed as a distributed key-value storage system. Metadata for files and documents are stored in a distributed manner and can be replicated for failure recovery. For uploading documents with replication, we utilize partitioning and merging algorithms, as detailed below:

- **Partitioning Algorithm**: This algorithm explains how file chunks are distributed and replicated across multiple Cloud Service Providers (CSPs). If a file is to be uploaded, it is divided into chunks. These chunks are then distributed and replicated over CSPs such that, in the event of a single CSP failure, the chunks can be recovered from the remaining working CSPs. For example, if a file is split into three chunks, these chunks are distributed as follows: chunk1 and chunk2 on CSP1, chunk1 and chunk3 on CSP2, and chunk2 and chunk3 on CSP3. In the case of a single CSP failure, the missing chunks can be recovered by performing an OR operation on the chunks from the remaining CSPs.

- **Merging Algorithm**: This algorithm explains how the chunks of a file are merged to form a complete file during download. The pseudo-code for these algorithms is provided below:

  ```plaintext
  n = number of Cloud Service providers
  k = number of Cloud Service providers from where chunks can be recovered
  Where k <= n, Ri = row of chunks of files

  Algorithm: Partitioning (For Uploading)
  Input: File
  Output: Data split across N clouds
  Process:
    For each row in Ri do
      For 1 to K do
        R_no = create Random number between 1 to K
        Cloud_no = Selectcloud[R_no]
        Upload Ri on cloud_no
      end for
    end for

  Algorithm: Merging (For Downloading)
  Input: File
  Output: Get the File downloaded
  Process:
    Fetch data from any k clouds
    Perform OR operation on retrieved data from K clouds
    File Status - Completely Downloaded
  ```

### 6) Architectural Flow of the Proposed System

Figure 3 illustrates the architectural flow of the proposed system. The flow diagram characterizes the processes and functions used in the system, representing the absolute state chart from the initial process to the final execution step.

### IV. Results and Analysis

This section presents the performance analysis of the proposed system compared to the existing BFC system [1]. Performance is measured based on the time required for uploading and downloading files, as well as the size of metadata, compression, and deduplication mechanisms.

#### A. Comparison Based on Deduplication and Compression Technique Usage

**Table I: Deduplication Comparison with Existing Systems**

| System       | Single User | Multiple Users |
|--------------|-------------|----------------|
| Dropbox      | Yes         | No             |
| OneDrive     | No          | No             |
| Google Drive | No          | No             |
| BFCSS        | Yes         | Yes            |

Table I shows that the proposed system has the advantage of supporting deduplication for both single and multiple users, unlike other cloud service providers.

**Table II: Uploaded File Size Information with Deduplication and Compression**

| File Name (No of Files Uploaded) | Size of the File (Bytes) | Size after Deduplication and Compression (Bytes) |
|----------------------------------|-------------------------|-------------------------------------------------|
| demo.txt(2)                      | 24*2                    | 21                                              |
| Pensionletter.pdf                | 53248                   | 49330                                           |
| DOC00.pdf                        | 98304                   | 92499                                           |
| BFCUMLdiagrams.docx              | 45056                   | 38745                                           |
| TestPlan.docs(2)                 | 16384*2                 | 12088                                           |
| 1.login.png(3)                   | 569344*3                | 568355                                          |
| Total                            | 1937456                 | 761038                                          |

**Total Storage Space Saved:**
- Without Deduplication and Compression (Ts): 1937456 Bytes
- With Deduplication (Ted): 782360 Bytes
- With Deduplication and Compression (Tsdc): 761038 Bytes
- Total Storage Space Saved: 1176418 Bytes
- Total Percentage of Storage Saved: 60.72%

**Comparison with Existing System:**
- Total Percentage of Storage Saved Compared to Existing System: 2.8%

**Figure 4: Total Storage Space with and without Compression and Deduplication (Comparison)**

Figure 4 provides a graphical comparison of the total storage space with and without compression and deduplication.

**Table III: Compression Comparison with Original System**

| Parameter   | BFC [1] | Proposed System | Sky_Drive | Cloud_Drive |
|-------------|---------|-----------------|------------|-------------|
| Deduplication | Yes     | Yes             | No         | No          |
| Compression  | No      | Yes             | No         | No          |

#### B. Metadata Comparison

Dropbox is a cloud-based storage service that allows users to store various data files. In the Dropbox model, a file is divided into 4MB chunks, each with a SHA256 hash. The metadata of each file includes a summation of the SHA256 hashes of its chunks, making the metadata size proportional to the file size. In contrast, the proposed system has a fixed-size metadata for each file, making it easier to store and scale the storage system for large files.

**Figure 5: Comparison of File Metadata Size with Dropbox**

Figure 5 illustrates the comparison of file metadata size between the proposed system and Dropbox.

#### C. Uploading Time Comparison

**Figure 6: Uploading Time of File for Existing and Proposed System (Comparison)**

Figure 6 shows that the uploading time for the proposed system is significantly less than that of the existing system, especially when compression is used.

### V. Conclusions

The proposed cloud storage service is based on a distributed file storage system. Each big-file is fragmented into multiple fixed-size chunks, and each file has a consistent and precise size of metadata, regardless of the file size. The chunks have adjacent ID ranges, simplifying data distribution and scaling. The de-duplication process uses the SHA hash algorithm to quickly identify duplicate data, saving storage space and network bandwidth. Future work will focus on enhancing security through Proof-of-Ownership (PoW) for user data.

### References

[1] Nguyen Trung, Khac Vu Tin, Noi V, and M. Nguyen, VietNam, BFC: High-Performance Distributed Big-File Cloud Storage Based On Key-Value Store, in Proceedings: Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD) IEEE, June 2015, pp.1-6.

[2] Minh.H.Nguyen and Trung.T.Nguyen, Design Sequential Chunk Identity with Lightweight Metadata for Big File Cloud Storage, International Journal of Computer Science and Network Security, Sept. 2015, pp.12-20.

[3] X. Chen, M. Li, Jin. Li, P. Lee, and W. Lou, Secure Deduplication with Efficient and Reliable Convergent Key Management, IEEE Transactions on Parallel and Distributed Systems, vol. 25, no. 6, 2013, pp.1615-1625.

[4] Huang Xin, Li Xin, Chen Xiaofeng, Tang Shaohua, and Yang Xiang, Mehedi Hassan Mohammad, and Alelaiwi Abdulhameed, Secure Distributed De-Duplication Systems with Improved Reliability, IEEE Transactions on Computers, vol. 64, no. 12, 2015, pp.3569-3579.

[5] R. Molva, P. Puzio, M. Onen, and S. Loureiro, ClouDedup: Secure Deduplication with Encrypted Data for Cloud Storage, 5th IEEE International Conference on Cloud Computing Technology and Science (CloudCom), 2013.

[6] Drago Idilio, Sperotto Anna, Mellia Marco, Munafa Maurizio, Sadre Ramin, and Pras Aiko, Inside Dropbox: Understanding Personal Cloud Storage Services, Proceedings of the 2012 Internet Measurement Conference, ACM, pp.481-494.

[7] Chang Fay, Dean Jeffrey, Ghemawat Sanjay, Hsieh Wilson C, Wallach Deborah A, Burrows Mike, Chandra Tushar, Fikes Andrew, and Gruber Robert E, Bigtable: A Distributed Storage System for Structured Data, Seventh Symposium on Operating System Design and Implementation, Nov. 2006, pp.1-7.

[8] H. Gobioff, S. Ghemawat, and S.T. Leung, The Google File System, SOSP '03 Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, 2003, pp.29-43.

[9] Secure Hash Standard, Computer Systems Laboratory National Institute of Standards and Technology Gaithersburg, FIPS PUB 180-1, Federal Information Processing Standards Publication, 1995.

[10] M. Mellia, H. Slatman, I. Drago, E. Bocchi, and A. Pras, Benchmarking Personal Cloud Storage, Proceedings of the Conference on Internet Measurement Conference, ACM, 2013, pp.205-212.