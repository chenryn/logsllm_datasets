### Table 5: Running Time for Different Classifiers

| Classifier | Detection Rate (%) |
|------------|--------------------|
| Gdata      | 48.49%             |
| Bitdefender| 46.47%             |
| F-Secure   | 34.34%             |
| AVG        | 30.30%             |
| McAfee     | 30.30%             |
| Symantec   | 27.27%             |
| ClamAV     | 22.22%             |
| Panda      | 10.10%             |
| Trend      | 6.06%              |
| JSAND      | 3.03%              |
| Other      | 2.02%              |
| None       | 0.00%              |

### Table 5: Running Time for Different Classifiers

| Operation                  | Num    | Time (s)  | Avg (ms)  |
|----------------------------|--------|-----------|-----------|
| Feature Extraction         | 20,942 | 1,660.7   | 79.3      |
| Training (RandomForest)    | 20,942 | 0.785     | 0.037     |
| Training (J48)             | 20,942 | 0.364     | 0.017     |
| Training (Näive Bayes)      | 20,942 | 0.124     | 0.006     |
| Training (RandomTree)      | 20,942 | 0.275     | 0.013     |
| Detection (RandomForest)   | 1,400,000 | 57.4    | 0.041     |
| Detection (J48)            | 1,400,000 | 26.6    | 0.019     |
| Detection (Näive Bayes)    | 1,400,000 | 8.4     | 0.006     |
| Detection (RandomTree)     | 1,400,000 | 19.6    | 0.014     |

As shown in Table 5, the running times for different classifiers are listed when the entire labeled dataset is used as the training set. The primary overhead comes from the feature extraction step. Although the total time for feature extraction of 20,942 scripts is 1,660.7 seconds, the average time per script is just 79.3 ms. As mentioned in Section 4.3, effective feature extraction requires execution-ready code that has undergone several unpacking steps. Thus, unpacking code, generating Abstract Syntax Trees (ASTs), and performing program analysis are the main time-consuming tasks in feature extraction. However, feature extraction needs to be done only once, and the resulting feature vectors can be stored in a database for future use.

Table 5 also lists the total training and detection times for different classifiers. For 20,942 feature vectors, the training of classifiers takes less than 1 second. Notably, Näive Bayes, despite being the worst classifier in Table 2, exhibits the best performance in the detection stage, requiring only 0.006 ms per script. The sum of the average feature extraction time and detection time provides an estimate of the scanning time for a single script. Given the negligible average detection time, the average feature extraction time (79.3 ms) suggests that our approach can be used as a browser plugin for real-time detection of malicious JavaScript.

To further evaluate the potential of our tool as an offline large-scale detector, we investigate the detection time needed for a large set of unlabeled scripts. Apart from feature extraction time, the detection of 1.5 million scripts takes up to 1-2 minutes. In the real application, we use Heritrix to crawl web pages and HtmlUnit to unpack code and extract feature vectors, processing crawling and feature extraction in parallel. Detection can be performed with different classifiers and selected features.

### 7.3 Evaluation of Attack Type Classification

We measure the accuracy and performance of the attack type classification on both labeled and unlabeled samples. We further investigate when dynamic confirmation is needed and how it can improve overall accuracy.

#### 7.3.1 Accuracy of the Trained Classifiers

| True \ Predicted | a (Type I) | b (Type II) | c (Type III) | d (Type IV) | e (Type V) | f (Type VI) | g (Type VII) | h (Type VIII) |
|------------------|------------|-------------|--------------|-------------|------------|-------------|--------------|---------------|
| a (Type I)       | 139        | 0           | 0            | 0           | 1          | 0           | 0            | 0             |
| b (Type II)      | 0          | 23          | 1            | 0           | 0          | 0           | 0            | 0             |
| c (Type III)     | 0          | 4           | 74           | 2           | 0          | 0           | 0            | 0             |
| d (Type IV)      | 0          | 0           | 1            | 179         | 0          | 0           | 1            | 1             |
| e (Type V)       | 9          | 0           | 0            | 9           | 179        | 19          | 1            | 0             |
| f (Type VI)      | 2          | 0           | 0            | 0           | 10         | 82          | 0            | 0             |
| g (Type VII)     | 0          | 0           | 1            | 1           | 0          | 0           | 87           | 3             |
| h (Type VIII)    | 0          | 1           | 3            | 0           | 0          | 0           | 3            | 105           |

Table 7: Attack classification on the 1,530 malicious samples detected from 1.4 million unlabeled samples.

| Type  | Number (Percentage) |
|-------|---------------------|
| Type I | 113 (7.39%)         |
| Type II | 10 (0.65%)         |
| Type III | 75 (4.90%)        |
| Type IV | 253 (16.54%)       |
| Type V | 202 (13.20%)       |
| Type VI | 101 (6.60%)        |
| Type VII | 350 (22.88%)      |
| Type VIII | 426 (27.84%)     |

We use a 10-fold cross-validation strategy to train four classifiers (RF, J48, NB, and RT) with the 942 labeled samples in Table 1. The accuracies of these classifiers are 92.14% (RF), 90.22% (J48), 83.44% (NB), and 90.13% (RT), respectively. Table 6 shows how the 942 samples are classified into the eight attack types using RF. The sum of the entries on the matrix's main diagonal, 868 (92.14%), are correctly classified samples. Among the 74 wrongly classified samples, 19 samples of Type VI are misclassified as Type V, and 10 of Type V are misclassified as Type VI. This is because some samples of Type V and Type VI are quite similar, with no significant differences in their feature values. For the other 45 out of 74 misclassified cases, most fall into the grey zone without a dominant certainty value for any attack type. The other three classifiers achieve slightly worse accuracies and have similar issues in distinguishing Types V and VI and classifying uncertain cases.

#### 7.3.2 Prediction on the Unlabeled Data Set

In Section 7.2.2, 1,530 malicious samples were detected by our approach from 1.4 million unlabeled scripts. We apply the best-trained classifier, RandomForest (RF), to classify these samples into different attack types. The classification output is listed in Table 7. To measure accuracy, we manually inspect 10% of samples of each type (for Types II and III, which have few samples, we randomly choose 10 samples). In total, we check 164 samples and confirm that they are indeed malicious. On these 164 samples, the trained RF classifier achieves an accuracy of 87.8%, correctly classifying 144 out of 164 samples. Among the 20 mistakenly classified ones, 9 samples do not belong to any of the eight attack types, and 11 samples are classified into the wrong types. We observe that 3 wrong cases are related to Type V or VI confusion, and 9 cases fall into the grey zone. Therefore, solely supervised classification cannot accurately predict the attack type of JavaScript malware, as feature values extracted by mostly-static analysis may not characterize the actual attack behaviors. For those cases in the grey zone, we need to apply dynamic confirmation to analyze them by execution.

### Table 8: Certainty Value and Number of Samples in Grey Zone

| Certainty | Certain # | Total | Uncertain # | Uncertain % |
|-----------|-----------|-------|-------------|-------------|
| >0.9      | 764       | 1,530 | 766         | 51.31%      |
| >0.8      | 854       | 1,530 | 676         | 50.07%      |
| >0.7      | 994       | 1,530 | 536         | 44.18%      |
| >0.6      | 1,296     | 1,530 | 234         | 15.29%      |
| >0.5      | 1,311     | 1,530 | 219         | 14.31%      |

### Table 9: Running Time of Different Classifiers in Type Classification

| Operation                  | Num    | Time (s) | Avg (ms) |
|----------------------------|--------|----------|----------|
| Training (RandomForest)    | 20,942 | 0.785    | 0.037    |
| Training (J48)             | 20,942 | 0.364    | 0.017    |
| Training (Näive Bayes)      | 20,942 | 0.124    | 0.006    |
| Training (RandomTree)      | 20,942 | 0.275    | 0.013    |
| Detection (RandomForest)   | 1,400,000 | 57.4 | 0.041 |
| Detection (J48)            | 1,400,000 | 26.6 | 0.019 |
| Detection (Näive Bayes)    | 1,400,000 | 8.4  | 0.006 |
| Detection (RandomTree)     | 1,400,000 | 19.6 | 0.014 |

### 7.4 Combining Dynamic Confirmation and Machine Learning Classification

Dynamic confirmation is applied to uncertain cases that fall into the grey zone during attack type classification. To effectively select these uncertain cases, we investigate the distribution of the probabilities of the predicted classes for the 1,530 samples classified by RF in Section 7.3.2. In Table 8, we define the certainty as the dominant probability among the eight attack types. For example, a certainty of 1 means there is one dominant probability (100%) for a sample to be the related predicted attack type. "Certain#" refers to the number of samples with this certainty value, i.e., 764 out of 1,530 for certainty = 1. Thus, the number of uncertain cases without a dominant probability (uncertain#) is 766, comprising 51.31% of the total 1,530 samples.

**Accuracy of Dynamic Attack Confirmation:** From Table 8, we observe that 234 samples have no dominant probability greater than 0.7. We apply dynamic confirmation to these 234 samples. As described in Section 5, we learn a Deterministic Finite Automaton (DFA) for each attack type from the related ones in the training set of 942 samples. We separately execute the 234 samples and obtain 10 traces for each. These traces are then used for acceptance checks on the learned DFAs. If any trace is accepted, the corresponding sample is matched with the DFA of the compared attack type. Finally, we compare the matching results with manual verification results to measure the accuracy. In total, dynamic confirmation correctly classifies 220 out of 234 samples, achieving an accuracy of 94.02%. Among the eight attack types, Type I shows the best accuracy (95%) for dynamic confirmation, while Type VI shows the worst (90.91%).

Thus, dynamic confirmation alone can achieve good accuracy for attack type classification. However, the problem is that it is not scalable. It takes about 1,544 seconds to analyze the 234 samples, averaging 6.6 seconds per sample. Among the 234 samples with a certainty value less than 0.7, RF achieves an accuracy of 67.09%. Therefore, among the 234 grey zone samples, dynamic confirmation can correctly classify 63 more samples than RF.

**Performance of Attack Type Classification:**

As shown in Table 9, the training step for attack type classification takes about 0.01-0.12 seconds, excluding the time for feature extraction. The prediction step takes only about 0.001-0.05 seconds. Thus, the performance of these four classifiers is quite good, owing to the relatively small number of samples to be handled in attack type classification. Additionally, these classifiers exhibit consistent performance; a classifier that is fast in malware detection is also fast in attack type classification. Note that the time for feature extraction in this step is the same as in the malware detection step, as all feature values are extracted once and reused.