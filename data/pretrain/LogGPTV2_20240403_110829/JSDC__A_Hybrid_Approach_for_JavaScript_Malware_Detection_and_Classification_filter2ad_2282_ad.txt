Gdata
Bitdefender
F-Secure
AVG
McAfee
48.49%
JSAND
46.47%
Trend
34.34%
30.30% Symantec
ClamAV
30.30%
Panda
27.27%
22.22%
10.10%
6.06%
3.03%
2.02%
0.00%
Table 5: The running time for diﬀerent classiﬁers
Num Time(s) Avg(ms)
Operation
Feature extraction
Training(RandomForest)
Training(J48)
Training(N¨aive Bayes)
Training(RandomTree)
20942
20942
20942
20942
20942
Detection(RandomForest)
Detection(J48)
Detection(N¨aive Bayes)
Detection(RandomTree)
1,400,000
1,400,000
1,400,000
1,400,000
1660.7
0.785
0.364
0.124
0.275
57.4
26.6
8.4
19.6
79.3
0.037
0.017
0.006
0.013
0.041
0.019
0.006
0.014
As shown in Table 5, we list the time used for diﬀerent
classiﬁers when the whole labelled data sets serve as the
training set. The main overheads come from the feature ex-
traction step. Although the total time of feature extraction
for 20942 scripts in training sets is 1660.7 seconds, the av-
erage time for each script is just 79.3 ms. As mentioned
in Section 4.3, feature extraction for eﬀective inner and in-
ter program analysis requires the execution ready code that
is after several times of unpacking. Thus, unpacking code,
generating ASTs and doing program analysis are the major
time-consuming tasks at the step of feature extraction. How-
ever, feature extraction can be processed for only once, and
the resulting feature vectors can be stored in database for
future usage. Table 5 also lists the total training and detec-
tion time for diﬀerent classiﬁers. For 20942 feature vectors,
the training of classiﬁers only need less than 1 seconds. We
can also observe that N¨aive Bayes classier, as the worst clas-
siﬁer in Table 2, exhibits the best performance—with only
0.006 ms needed for each script in detection stage. The sum
of average feature extraction time and detection time ap-
proximately provides an estimate of the scanning time for a
single script. Given the ignorable average detection time for
each script, the average feature extraction time (79.3 ms)
suggests that our approach can be used as a browser plug-in
for real-time detection of malicious JavaScript.
To further evaluate the potential of our tool being an oﬀ-
line large-scale detector, we investigate the detection time
needed for a large set of unlabelled scripts. Apart from
feature extraction time, the detection only needs up to 1-2
minutes to classify 1.5 million scripts. In the real application
of our approach for detection, we use Heritrix to crawl the
pages and, meanwhile, HtmlUnit to unpack code and extract
feature vectors. Thus, crawling and feature extraction are
processed in parallel. The detection can be accomplished
with diﬀerent classiﬁers and diﬀerent selected features.
7.3 Evaluation of Attack Type Classiﬁcation
We measure the accuracy and performance of the attack
type classiﬁcation on the labelled and unlabelled samples.
We further investigate when the dynamic conﬁrmation is
needed and how it can improve the overall accuracy.
7.3.1 Accuracy of the Trained Classiﬁers
a
139
0
1
0
1
0
0
0
b
0
23
1
0
0
0
0
0
c
0
4
74
2
0
0
0
0
d
0
0
1
179
0
0
1
1
e
9
0
0
9
179
19
1
0
f
2
0
0
0
10
82
0
0
g
0
0
1
1
0
0
87
3
h <–classiﬁed as
0
1
3
0
0
0
3
105
a = type I
b = type II
c = type III
d = type IV
e = type V
f = type VI
g = type VII
h = type VIII
Table 7: Attack classiﬁcation on the 1530 malicious
ones detected from 1,400,000 unlabelled samples
Type
type I
type II
type III
type IV
Num 113 (7.39%)
10 (0.65%)
75 (4.90%)
253 (16.54%)
Type
type V
type VI
type VII
type VIII
Num 202 (13.20%)
101 (6.60%)
350 (22.88%)
426 (27.84%)
We adopt 10-folder CV strategy to train the 4 classiﬁers
(RF, J48, NB and RT) with the 942 labelled samples in Ta-
ble 1. The accuracy of these 4 classiﬁers is 92.14% (RF),
90.22% (J48), 83.44% (NB) and 90.13% (RT), respectively.
Table 6 shows how the 942 samples are classiﬁed into the
8 attack types according to RF. The sum of the entries on
the matrix’s main diagonal, namely 868 (92.1444%), are cor-
rectly classiﬁed samples. Among the 74 wrongly classiﬁed
samples, 19 samples of type VI are wrongly classiﬁed as type
V, while 10 of type V are wrongly classiﬁed as type VI. The
direct reason is that some samples of type V and type VI are
quite similar—their feature values show no signiﬁcant diﬀer-
ences. For the other 45 out of 74 wrongly classiﬁed cases,
we ﬁnd that most of them fall into the grey zone without
a dominant certainty value for any attack type. The other
three classiﬁers achieve slight worse accuracies. We also ob-
serve that they have similar problems in distinguishing type
V and type VI, and classifying a number of uncertain cases.
Thus, the trained classiﬁers can be ready for practical us-
age if we can address the above two problems via combining
dynamic attack conﬁrmation (see Section 7.4).
7.3.2 Predication on the Unlabelled Data Set
In Section 7.2.2, 1530 malicious samples are detected by
our approach from 1.4 million unlabelled scripts. We apply
the best trained classiﬁers, namely RF, to classify these sam-
ples into diﬀerent attack types. The classiﬁcation output is
listed in Table 7. To measure the accuracy, we manually in-
spect 10% of samples of each type (for type II and type III
that has few samples we randomly choose 10 samples). To-
tally, we check 164 samples and conﬁrm that they are indeed
malicious. On these 164 samples, the trained RF classiﬁer
achieves an accuracy of 87.8%, which means correctly clas-
sifying 144 out of 164 samples. Among the 20 mistakenly
classiﬁed ones, 9 samples do not belong to any of the eight
attack types and 11 samples are classiﬁed into the wrong
types. We observe that 3 wrong cases are related to type V
or VI confusion, and 9 cases fall into the grey zone. Thus,
the sole supervised classiﬁcation cannot accurately predicate
the attack type of JavaScript malware, since feature values
extracted by mostly-static analysis may not characterize the
actual attack behaviors. Especially, for those cases that fall
into grey zones, we need to apply dynamic conﬁrmation to
to analyse them by execution.
117Table 8: The certainty value and the number of sam-
ples that fall into grey zone
Table 9: Running time of diﬀerent classiﬁers in type
classiﬁcation
certainty
certain# total
uncertain# uncertain %
Operation
Num Time(s) Avg(ms)
1
(cid:62)0.9
(cid:62)0.8
(cid:62)0.7
(cid:62)0.6
764
854
994
1296
1311
1530
1530
1530
1530
1530
766
676
536
234
219
51.31%
50.07%
44.18%
15.29%
14.31%
7.4 Combining Dynamic Conﬁrmation and Ma-
chine Learning Classiﬁcation
The dynamic conﬁrmation is applied on uncertain cases
that fall into the grey zone during attack type classiﬁcation.
To eﬀectively select the uncertain cases, we investigate
how the probabilities of the predicated class are distributed
for the 1530 samples that are classiﬁed by RF in Section
7.3.2. In Table 8, we deﬁne the certainty as the dominant
one among a sample’s probabilities to be eight attack types.
For example, certainty = 1 means there exists one dominant
probability (100%) for a sample to be the related predicated
attack type. certain# refers to the number of samples with
this certainty value, i.e., 764 out of 1530 for certainty = 1.
Thus, the number of uncertain cases without a dominant
probability (uncertain#) is 766, which comprises 51.31% of
the total 1530 samples.
Accuracy of Dynamic Attack Conﬁrmation. From
Table 8, we observe that 234 samples are without a domi-
nant probability larger than 0.7 (234 samples are in the grey
zone if certainty = 0.7). We apply the dynamic conﬁrma-
tion on these 234 samples. As described in Section 5, we
learn a DFA for each attack type from the related ones in
the training set of 942 samples. We separately execute the
234 samples and get 10 traces for each of them. Then these
traces are used for acceptance check on the learned DFAs. If
any trace is accepted, the corresponding sample is matched
with the DFA of the compared attack type. Finally, we com-
pare the matching results with manual veriﬁcation results
to measure the accuracy. Totally, the dynamic conﬁrmation
correctly classiﬁes 220 samples out of 234, achieving an accu-
racy of 94.02%. Among eight attack types, type I shows the
best accuracy (95%) for dynamic conﬁrmation while type VI
shows the worst (90.91%).
Thus, dynamic conﬁrmation alone can achieve good ac-
curacy for attack type classiﬁcation. However, the problem
is that it is not scalable.
It takes about 1,544 seconds to
analyse the 234 samples, i.e., averagely 6.6 seconds for each.
Among 234 samples without a certainty value larger than
0.7, RF achieves an accuracy of 67.09%. Thus, among 234
grey zone samples, dynamic conﬁrmation can rightly classify
63 more than RF.
Performance of Attack Type Classiﬁcation.
As shown in Table 9, the training step of attack type clas-
siﬁcation takes about 0.01-0.12 seconds, which does not in-
clude the time of feature extraction. The predication step
takes only about 0.001-0.05 seconds. Thus, the performance
of these 4 classiﬁers is quite good, owing to the compara-
tively small number of samples to be handled in attack type
classiﬁcation. Besides, these classiﬁers also exhibit consis-
tent performance — that is a classier fast in malware detec-
tion is also fast in attack type classiﬁcation. Note that the
time for feature extraction in this step is the same as that in
malware detection step, as all feature values are extracted
Training(RandomForest)
Training(J48)
Training(N¨aive Bayes)
Training(RandomTree)
Detection(RandomForest)
Detection(J48)
Detection(N¨aive Bayes)
Detection(RandomTree)