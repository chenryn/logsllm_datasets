C
 0.8
 0.6
 0.4
 0.2
 0
 10
C1
C3
C4
 50
 100
 150
 200
Response Time (ms)
(b) Put
C1
C3
C4
 20
 40
 60
 80
 100
Response Time (ms)
(a) Get
C1
C3
C4
 100
 1000
 10000
 100000
Response Time (ms)
(c) Query
Figure 4: The cumulative distribution of the response time when using the large table with 100K entries. Note that for the query operation, the
x-axis is in a logarithmic scale, due to the signiﬁcant performance gaps between different services.
ones, but for different reasons. For C1, the provisioning latency is
similar for both instances, but the Windows ones have larger boot-
ing latency, possibly due to slower CPUs of the smallest instances.
Windows Server 2008 R2 instances based on the Win7 code base
can boot faster. For C2, the booting latency is similar, while the
provisioning latency of the Windows instances is much larger. It
is unclear but likely that C2 may have different infrastructures to
provision Linux and Windows instances.
We note that a few other factors impacting scaling agility have
not been considered here. Rather than focusing just on allocat-
ing instances quickly, some providers make it easy to update active
instances or provide extensive monitoring and automatic recovery
from faults during running. Another common goal is to create in-
stances consistently within an SLA deadline even when they are
brought up in large batches.
5.2 Persistent Storage
We measure and compare three types of storage services: table,
blob, and queue, offered by various cloud providers.
5.2.1 Table Storage
We ﬁrst compare three table storage services offered by C1, C3,
and C4. C2 does not provide a table service. Here we only show
the results obtained using our Java-based client, as other non-Java
clients achieve similar performance, because the table operations
are lightweight on the client side. For each table service, we test
the performance of three operations: get, put, and query (see
Table 2). Each operation runs against two pre-deﬁned data tables:
a small one with 1K entries, and a large one with 100K entries. The
get/put operations operate on one table entry, while the query
operation returns on average 10 entries. For storage benchmarks,
unless otherwise speciﬁed, we use instance types that occupy at
least one physical core to avoid excessive variation due to CPU
time sharing.
Figure 4 shows the distributions of response time for each type
of operation on the large table. We repeat each operation several
hundred times. The results of using the small table show similar
trends and are omitted to save space. From the ﬁgure, we can see
that all table services exhibit high variations in response time. For
example, the median response time of the get operation is less than
50ms, while the 95th-percentile is over 100ms for all services. The
three services perform similarly for both get and put operations,
with C3 slightly slower than the other two providers. However, for
the query operation, C1’s service has signiﬁcantly shorter response
time than the other two. C4’s service has a very long response time,
because unlike the other providers, it does not appear to maintain
indexes over the non-key ﬁelds in a table. In contrast, C1 appears
to have an indexing strategy that is better than the others.
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
1K Table
100K Table
 100
 200
 300
 400
 500
Time to Consistency (ms)
Figure 5: The cumulative distribution of the time to consistency for
C1’s table service.
Provider Cost per Operation (milli-cents)
C1
C3
C4
get
0.13
0.02
0.10
put
0.31
0.23
0.10
query
1.47
0.29
0.10
Table 4: The average cost per operation for all three table services.
The cost is in the unit of milli-cent, i.e., one-thousandth of a cent.
We also measure how well each table service scales by launching
multiple concurrent operations. None of the services show notice-
able performance degradation when up to 32 operations are issued
at the same time. This suggests that all these table storage services
appear to be reasonably well provisioned. Testing at higher scale is
deferred to future work.
We then evaluate the time to reach consistency for the table ser-
vices by using the mechanism described in §4.2. We discover that
around 40% of the get operations in C1 see inconsistency when
triggered right after a put, and do not return the entry just inserted
by the put. Other providers exhibit no such inconsistency. Fig-
ure 5 shows the distribution of the time to reach consistency for
C1. From the ﬁgure, we see that over 99% of the inconsisten-
cies are resolved within 500ms, with the median resolved within
80ms. The long duration of the inconsistency opens up a sizable
window for race conditions. C1 does provide an API option to re-
quest strong consistency but disables it by default. We conﬁrm that
turning on this option eliminates inconsistencies and surprisingly
does so without much extra latency for get or put. We conjec-
ture that the performance overhead of enforcing consistency might
be visible only when more users request it or during failure cases
because otherwise C1 would have enabled it by default.
Finally, we compare the cost per operation for different table ser-
vices in Table 4. We can see that, although the charging models of
the providers are different, the costs are comparable. Both C1 and
C3 charge lower cost for get/put than query, because the former
8 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 50
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 50
C1
C2
C4
 150
 100
 200
Response Time (ms)
 250
 300
C1
C2
C4
 150
 100
 200
Response Time (ms)
 250
 300
(a) Download 1KB Blob
(b) Upload 1KB Blob
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
C1
C2
C4
 1000
 2000
 3000
 4000
 5000
 6000
Response Time (ms)
(c) Download 10MB Blob
C1
C2
C4
 1000
 2000
 3000
 4000
 5000
 6000
Response Time (ms)
(d) Upload 10MB Blob
Figure 6: The cumulative distribution of the response time to download or upload a blob using Java-based clients.
operations are simpler and consume less CPU cycles to serve. C4
charges the same across operations and can improve its charging
model by accounting for the complexity of the operation. This is
an example of how CloudCmp’s benchmarking results can help the
providers make better choices.
A comparison with the compute costs in Table 3 indicates that
the cost of table storage is comparable to that of compute instances
for workloads that trigger about 1000 operations per instance per
hour. Applications that use storage at a lower rate can choose their
provider based on their computation costs or performance.
5.2.2 Blob Storage
We compare the blob storage services provided by C1, C2, and
C4. C3 does not offer a blob store.
Figure 6 shows the response time distributions for uploading and
downloading one blob measured by our Java-based clients. We
consider two blob sizes, 1KB and 10MB, to measure both latency
and throughput of the blob store. The system clock of C2’s in-
stances have a resolution of 10ms, and thus its response time is
rounded to multiples of 10ms. We see that the performance of blob
services depends on the blob size. When the blob is small, C4 has
the best performance among the three providers. When the blob
is large, C1’s average performance is better than the others. This
is because blobs of different sizes may stress different bottlenecks
– the latency for small blobs can be dominated by one-off costs
whereas that for large blobs can be determined by service through-
put, network bandwidth, or client-side contention. Uniquely, C2’s
storage service exhibits a 2X lower performance for uploads com-
pared to downloads from the blob store. It appears that C2’s store
may be tuned for read heavy workload.
Figure 7 illustrates the time to download a 10MB blob measured
by non-Java clients. Compared to Figure 6(c), in every provider,
non-Java clients perform much better. Markedly C4’s performance
improves by nearly 5 times because it turns out that the Java imple-
mentation of their API is particularly inefﬁcient.
We then compare the scalability of the blob services by sending
multiple concurrent operations. As per above, we use non-Java
clients to eliminate overheads due to Java. Figure 8 shows the
 1
n
o
i
t
c
a
r
F
e
v
i
t
a
u
m
u
C
l
 0.8
 0.6
 0.4
 0.2
 0
 0
C1
C2
C4
 200
 400
 600
 800
 1000
Response Time (ms)
Figure 7: The cumulative distribution of the time to download a 10MB
blob using non-Java clients.
Provider Maximum throughput achieved by one instance (Mbps)
Smallest instance with
at least one core
Largest instance from
a provider
C1
C2
C4
773.4
235.5
327.2
782.3
265.7
763.3
Table 5: The maximum throughput an instance obtains from each
blob service when downloading many 10MB blobs concurrently.
downloading time with the number of concurrent operations rang-
ing from 1 to 32. We omit the results for uploading because they
are similar in trend. When the blob size is small, all services ex-
cept for C2 show good scaling performance. This suggests that C1
and C4’s blob services are well-provisioned to handle many con-
current operations. When the blob is large, C4 and C1 continue to
scale better though all three providers show certain scaling bottle-
necks. The average time to download increases with the number of
concurrent operations, illustrating a throughput bottleneck.
We compute the maximum throughput of the blob service that
one instance can obtain by increasing the number of simultaneous
operations until the throughput stabilizes. Table 5 shows the re-
sults for two types of instances – the smallest instance that has at
least one full core and the largest instance from each provider. The
former data point eliminates CPU time sharing effects [35] while
the latter minimizes contention from colocated VMs. We report
9 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m