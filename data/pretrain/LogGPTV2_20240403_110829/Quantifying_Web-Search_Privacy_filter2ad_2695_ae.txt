uation methods are proposed to quantify user-centric web
search query obfuscation mechanisms.
In [9], the authors
perform a qualitative analysis by investigating the speciﬁci-
ties of the mentioned obfuscation solutions and elaborated
at least one countermeasure on how each obfuscation mech-
anism can be defeated. This study helps us to understand
the limitations of the proposed obfuscation mechanisms, but
does not allow us to quantitatively reason which one of the
solutions is better than the other. In [8], the authors anal-
ysed TMN dummy queries by clustering queries and labeling
them according to their similarity with the set of recently
issued queries by the user. However, the clustering algo-
rithm computes the similarity between queries irrespective
to the obfuscation mechanism. So, fake queries that are
similar to user queries can easily fall into the same cluster.
Furthermore, [12] presents two features that can help diﬀer-
entiating TMN from real user queries. Another study [28]
of TMN presents that simple supervised learning classiﬁers
with few features can identify TMN queries reliably when
having access to recent user search history. This work also
proposes a clustering attack. However, it cannot distinguish
user and TMN queries, due to focusing on the similarity be-
tween queries rather than their linkability (as we learn and
use in this paper).
Our proposed linkage function does consider all diﬀeren-
tiating features and learns their importance automatically.
So, for example, the distance in time between queries inﬂu-
ence our decision making on linking queries by looking at
the value of other features rather than ignoring the queries.
Our generic framework complements the existing work by
proposing a systematic and quantitative approach which
does not focus on any particular obfuscation mechanism,
and can evaluate privacy of user even if adversary does not
have any speciﬁc model on the target user.
Another related area of research which solves a similar
problem is the protection of the privacy of web query logs.
A survey of diﬀerent obfuscation techniques for search query
logs is presented [13]. In [23], the authors propose to solve
the problem of releasing web query logs using diﬀerential
privacy.
7. CONCLUSIONS
Having a systematic methodology to reason quantitatively
about users’ privacy is a necessary step towards designing
eﬀective obfuscation mechanisms.
In the context of web-
search privacy, notwithstanding many contributions on pro-
tecting users’ web-search privacy and few speciﬁc attacks on
particular obfuscation mechanisms, the lack of a generic for-
mal framework for specifying protection mechanisms and for
evaluating privacy is evident. In this paper, we have raised
the questions of “what is web-search privacy? and how can
it be quantiﬁed, given an adversary model and a protection
mechanism?” and proposed a quantitative framework to an-
swer these questions. In this framework, we have modeled
various types of adversary’s knowledge as well as the user’s
privacy sensitivities that leads to the deﬁnition of privacy
metrics. To model the obfuscation mechanisms and adver-
sary’s knowledge about the user, we have designed a func-
tion we have called the linkage function. This is the main
building block of our quantiﬁcation framework and helps us
to reason similar to an adversary and to distinguish pairs
of queries from a user from pairs of queries that have fake
information. We have constructed this function in a way
that does not need, yet it can incorporate, knowledge about
web-search behavior of the target user. We have used this
to reason how much information (whether at the query level
or semantic level) about the user is still leaked through the
obfuscation process. We have applied our methodology on
real datasets and compared two example obfuscation mech-
anisms. As the follow-up of this work, we want to design
web-search obfuscation mechanisms that anticipate the pos-
sibility of linkage attacks. This strategy will lead to robust
protection mechanisms.
8. REFERENCES
[1] D. C. Howe, H. Nissenbaum, and V. Toubiana,
TrackMeNot - available from
http://cs.nyu.edu/trackmenot.
[2] Open Directory Project (ODP) ontology, available
from http://www.dmoz.org/.
[3] Lancaster Stemming Algorithm, available from
http://www.comp.lancs.ac.uk/computing
/research/stemming.
[4] Maxmind Free World Cities Database, Available from
http://www.maxmind.com/en/worldcities.
[5] Jaccard, P. (1901) Distribution de la ﬂore alpine dans
le bassin des Dranses et dans quelques regions
voisines. Bulletin de la Societe Vaudoise des Sciences
Naturelles 37, 241-272.
[6] Google Books - Ngram datasets, Available from
http://storage.googleapis.com/books/ngrams/books
/datasetsv2.html.
[7] CLUTO - Software for Clustering High-Dimentional
Datasets, available from
http://glaros.dtc.umn.edu/gkhome/views/cluto.
[8] R. Al-Rfou, W. Jannen, and N. Patwardhan.
Trackmenot-so-good-after-all. arXiv preprint
arXiv:1211.0320, 2012.
[9] E. Balsa, C. Troncoso, and C. Diaz. Ob-pws:
obfuscation-based private web search. In Security and
Privacy (SP), 2012 IEEE Symposium on, pages
491–505. IEEE, 2012.
[10] P. N. Bennett, K. Svore, and S. T. Dumais.
Classiﬁcation-enhanced ranking. In Proc.
International World Wide Web Conference (WWW),
pages 111–120, 2010.
[11] J. Castell´ı-Roca, A. Viejo, and
J. Herrera-Joancomart´ı. Preserving user’s privacy in
web search engines. Comput. Commun.,
32(13-14):1541–1551, Aug. 2009.
[12] R. Chow and P. Golle. Faking contextual data for fun,
proﬁt, and privacy. In Proceedings of the 8th ACM
workshop on Privacy in the electronic society, pages
105–108. ACM, 2009.
[13] A. Cooper. A survey of query log privacy-enhancing
techniques from a policy perspective. ACM
Transactions on the Web (TWEB), 2(4):19, 2008.
[14] R. Dingledine, N. Mathewson, and P. Syverson. Tor:
The second-generation onion router. In Proceedings of
the 13th Conference on USENIX Security Symposium
- Volume 13, SSYM’04, pages 21–21, Berkeley, CA,
USA, 2004. USENIX Association.
[15] J. Domingo-Ferrer, A. Solanas, and J. Castell`a-Roca.
h(k)-private information retrieval from
privacy-uncooperative queryable databases. Online
Information Review, 33(4):720–744, 2009.
[16] P. Eckersley. How unique is your web browser? In
Proceedings of the 10th International Conference on
Privacy Enhancing Technologies, PETS’10, pages
1–18, Berlin, Heidelberg, 2010. Springer-Verlag.
[17] Y. Elovici, B. Shapira, and A. Maschiach. A new
privacy model for hiding group interests while
accessing the web. In Proceedings of the 2002 ACM
workshop on Privacy in the Electronic Society, pages
63–70. ACM, 2002.
[18] J. Friedman, T. Hastie, and R. Tibshirani. Additive
logistic regression: a statistical view of boosting.
Annals of Statistics, 28:2000, 1998.
[19] J. H. Friedman. Stochastic gradient boosting.
Computational Statistics and Data Analysis,
38:367–378, 1999.
[20] I. Goldberg. Improving the robustness of private
information retrieval. In Security and Privacy, 2007.
SP ’07. IEEE Symposium on, pages 131–148, May
2007.
[21] D. C. Howe and H. Nissenbaum. TrackMeNot:
Resisting surveillance in web search. Lessons from the
Identity Trail: Anonymity, Privacy, and Identity in a
Networked Society, 23:417–436, 2009.
[22] R. Jones, R. Kumar, B. Pang, and A. Tomkins. “I
know what you did last summer”: Query logs and user
privacy. In Proceedings of the Sixteenth ACM
Conference on Conference on Information and
Knowledge Management, CIKM ’07, pages 909–914,
New York, NY, USA, 2007. ACM.
[23] A. Korolova, K. Kenthapadi, N. Mishra, and
A. Ntoulas. Releasing search queries and clicks
privately. In Proceedings of the 18th international
5 percentile FP
50 percentile FP
95 percentile FP
5 percentile FN
50 percentile FN
95 percentile FN
0.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
20
30
40
50
70
Number of Users
60
80
90
100
5 percentile FP
50 percentile FP
95 percentile FP
5 percentile FN
50 percentile FN
95 percentile FN
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
N
M
T
−
y
c
a
v
i
r
P
e
v
i
t
a
l
e
R
e
r
u
t
c
u
r
t
S
y
r
e
u
Q
R
S
U
−
y
c
a
v
i
r
P
e
v
i
t
a
l
e
R
e
r
u
t
c
u
r
t
S
y
r
e
u
Q
0
20
30
40
50
70
Number of Users
60
80
90
100
Figure 4: Eﬀect of the size of user dataset on the
quantiﬁed relative privacy.
conference on World wide web, pages 171–180. ACM,
2009.
[24] E. Kushilevitz and R. Ostrovsky. Replication is not
needed: Single database, computationally-private
information retrieval. In Proceedings of the 38th
Annual Symposium on Foundations of Computer
Science, FOCS ’97, pages 364–, Washington, DC,
USA, 1997. IEEE Computer Society.
[25] V. I. Levenshtein. Binary codes capable of correcting
deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707–710, February 1966.
[26] M. Murugesan and C. Clifton. Providing privacy
through plausibly deniable search. In SDM, pages
768–779. SIAM, 2009.
[27] G. Pass, A. Chowdhury, and C. Torgeson. A picture of
search. In Proceedings of the 1st International
Conference on Scalable Information Systems, InfoScale
’06, New York, NY, USA, 2006. ACM.
[28] S. T. Peddinti and N. Saxena. On the privacy of web
search based on query obfuscation: a case study of
trackmenot. In Privacy Enhancing Technologies, pages
19–37. Springer, 2010.
[29] A. Rajaraman and J. D. Ullman. Mining of massive
datasets. Cambridge University Press, 2012.
[30] D. Rebollo-Monedero and J. Forn´e. Optimized query
forgery for private information retrieval. Information
Theory, IEEE Transactions on, 56(9):4631–4642, 2010.
[31] M. K. Reiter and A. D. Rubin. Anonymous web
transactions with crowds. Commun. ACM,
42(2):32–48, Feb. 1999.
[32] B. Shapira, Y. Elovici, A. Meshiach, and T. Kuﬂik.
Prawa - privacy model for the web. Journal of the
American Society for Information Science and
Technology, 56(2):159–172, 2005.
[33] R. Shokri, G. Theodorakopoulos, J.-Y. Le Boudec,
and J.-P. Hubaux. Quantifying location privacy. In
IEEE Symposium on Security and Privacy, Oakland,
CA, USA, 2011.
[34] A. Singla, R. W. White, A. Hassan, and E. Horvitz.
Enhancing personalization via search activity
attribution. In Proc. Special Interest Group On
Information Retrieval (SIGIR), 2014.
[35] B. Tancer. Click: What Millions of People Are Doing
Online and Why it Matters. Hyperion, 2008.
[36] R. W. White, A. Hassan, A. Singla, and E. Horvitz.
From devices to people: Attribution of search activity
in multi-user settings. In Proc. International World
Wide Web Conference (WWW), 2014.
[37] S. Ye, F. Wu, R. Pandey, and H. Chen. Noise injection
for search privacy protection. In Computational
Science and Engineering, 2009. CSE’09. International
Conference on, volume 3, pages 1–8. IEEE, 2009.
APPENDIX
The number of users that we use to learn the linkage func-
tion is a parameter in our evaluation. Here, we study how
the privacy gain of using an obfuscation mechanism is af-
fected by varying the number of users (from which we learn
the linkage function). To this end, we focus on the relative
privacy metric as it is the only metric that removes the ef-
fect of the target queries on the quantiﬁed privacy and only
reﬂects the privacy gain of obfuscation. We construct the
linkage function from diﬀerent sets of users, of size 20 to
100, and quantify the relative privacy of TMN and USR.
Each set of users includes the users in the smaller sets. For
each of these cases, we can plot the empirical CDF for the
users’ privacy (as in Figure 2). However, to compare these
plots, we adhere to some statistics (5, 50, and 95 percentiles)
of these distributions. Figure 4 shows these statistics about
privacy of users versus the number of users that are used
for learning the linkage function. As the plot illustrates,
the privacy values do not ﬂuctuate as we change the set of
users. Additionally, we observe that we do not gain in the
accuracy of the privacy metric by increasing the number of
users beyond 100. So, this is a reasonable size for the linkage
function learning set that we use in our evaluation.