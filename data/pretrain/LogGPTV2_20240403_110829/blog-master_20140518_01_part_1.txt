## ZFS snapshot used with PostgreSQL PITR or FAST degrade or PG-XC GreenPlum plproxy MPP DB's consistent backup  
### 作者                                                                                                                                             
digoal                                                                                                                                               
### 日期                                                                                                                                                              
2014-05-18                                                                                                                                     
### 标签                                                                                                                                           
PostgreSQL , Linux , ZFS                                                                                                                                         
----                                                                                                                                                     
## 背景          
```  
上一篇BLOG介绍了一下ZFS的使用, 以及zfs的log和l2arc机制带来的读写性能提升.  
本文将介绍一下ZFS的另一大功能, snapshot和clone. 结合PostgreSQL的PITR来使用, snapshot可以替换基础备份. 从而提高PostgreSQL恢复到过往时间点的速度(就近选择snapshot).  
同时还可以作为 PG-XC GreenPlum plproxy 等并行数据库解决方案的全局一致性备份, 对于pg-xc有冻结事务的功能, 所以可以不停库实现一致性备份, 对于greenplum和plproxy, 可以停库后做snapshot, 因为snapshot还是挺快的, 所以停库不需要多长时间. 这些快照可以在数据库起来后随时传输到备份环境, 不影响数据库运行, 当然传输过程会带来这个快照对应的数据块的读操作.  
首先回到上一篇创建的pool的地方.  
[root@db-172-16-3-150 ssd4]# /opt/zfs0.6.2/sbin/zpool create zptest /opt/zfs.disk1 /opt/zfs.disk2 /opt/zfs.disk3 /opt/zfs.disk4 log mirror /ssd4/zfs.log1 /ssd4/zfs.log2 cache /dev/disk/by-id/scsi-SATA_OCZ-REVODRIVE3_OCZ-Z2134R0TLQBNE659-part1  
[root@db-172-16-3-150 ssd4]# /opt/zfs0.6.2/sbin/zpool status zptest  
  pool: zptest  
 state: ONLINE  
  scan: none requested  
config:  
        NAME                STATE     READ WRITE CKSUM  
        zptest              ONLINE       0     0     0  
          /opt/zfs.disk1    ONLINE       0     0     0  
          /opt/zfs.disk2    ONLINE       0     0     0  
          /opt/zfs.disk3    ONLINE       0     0     0  
          /opt/zfs.disk4    ONLINE       0     0     0  
        logs  
          mirror-4          ONLINE       0     0     0  
            /ssd4/zfs.log1  ONLINE       0     0     0  
            /ssd4/zfs.log2  ONLINE       0     0     0  
        cache  
          sda1              ONLINE       0     0     0  
errors: No known data errors  
我们在上一篇是直接使用的pool, 这里要说一下, 在pool中创建dataset.  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs create zptest/dir1  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs list  
NAME          USED  AVAIL  REFER  MOUNTPOINT  
zptest        316K  3.81G    30K  /zptest  
zptest/dir1    30K  3.81G    30K  /zptest/dir1  
[root@db-172-16-3-150 ~]# df -h  
zptest/dir1           3.9G     0  3.9G   0% /zptest/dir1  
创建整个pool的snapshot或dataset的snapshot, snapshot格式如下 :   
pool/dataset@snapshot-name  
pool@snapshot-name  
例如, 我这里创建一个dataset的snapshot, 以时间命名.  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs snapshot zptest/dir1@`date +%F%T`  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs list -t snapshot  
NAME                             USED  AVAIL  REFER  MOUNTPOINT  
zptest/dir1@2014-05-1716:09:20      0      -    30K  -  
创建一个pool的snapshot.  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs snapshot zptest@`date +%F%T`  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs list -t snapshot  
NAME                             USED  AVAIL  REFER  MOUNTPOINT  
zptest@2014-05-1716:12:49           0      -    30K  -  
zptest/dir1@2014-05-1716:09:20      0      -    30K  -  
删除snapshot.  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs destroy zptest@2014-05-1716:12:49  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs destroy zptest/dir1@2014-05-1716:09:20  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs list -t snapshot  
no datasets available  
回滚到指定的snapshot.  
[root@db-172-16-3-150 ~]# df -h  
Filesystem            Size  Used Avail Use% Mounted on  
/dev/sdc1              29G  9.3G   19G  34% /  
tmpfs                  48G     0   48G   0% /dev/shm  
/dev/sdc3              98G   34G   59G  37% /opt  
/dev/sdd1             183G   33G  142G  19% /ssd1  
/dev/sdb1             221G   43G  167G  21% /ssd4  
/ssd4/test.img       1008M  207M  751M  22% /mnt  
zptest/dir1           3.9G     0  3.9G   0% /zptest/dir1  
[root@db-172-16-3-150 ~]# cd /zptest/dir1/  
[root@db-172-16-3-150 dir1]# ll  
total 0  
[root@db-172-16-3-150 dir1]# dd if=/dev/zero of=./1 bs=1k count=1024  
1024+0 records in  
1024+0 records out  
1048576 bytes (1.0 MB) copied, 0.0228468 s, 45.9 MB/s  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs snapshot zptest/dir1@`date +%F%T`  
[root@db-172-16-3-150 dir1]# dd if=/dev/zero of=./2 bs=1k count=1024  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs snapshot zptest/dir1@`date +%F%T`  
[root@db-172-16-3-150 dir1]# dd if=/dev/zero of=./3 bs=1k count=1024  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs snapshot zptest/dir1@`date +%F%T`  
[root@db-172-16-3-150 dir1]# dd if=/dev/zero of=./4 bs=1k count=1024  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs snapshot zptest/dir1@`date +%F%T`  
[root@db-172-16-3-150 dir1]# rm -f *  
[root@db-172-16-3-150 dir1]# ll  
total 0  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs list -t snapshot  
NAME                             USED  AVAIL  REFER  MOUNTPOINT  
zptest/dir1@2014-05-1716:23:12    19K      -  1.03M  -  
zptest/dir1@2014-05-1716:23:19    19K      -  2.03M  -  
zptest/dir1@2014-05-1716:23:25    19K      -  3.04M  -  
zptest/dir1@2014-05-1716:23:30  1.02M      -  4.04M  -  
回滚前必须卸载对应的dataset或zpool, 并且只能回滚到最近的一个snapshot, 或者说, 要回滚到过去的snapshot, 必须删掉这个snapshot和当前之间的所有snapshot.  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs umount zptest/dir1  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs rollback zptest/dir1@2014-05-1716:23:25  
cannot rollback to 'zptest/dir1@2014-05-1716:23:25': more recent snapshots exist  
use '-r' to force deletion of the following snapshots:  
zptest/dir1@2014-05-1716:23:30  
这里提示删除1个snapshot, 因为这个snapshot是在回滚点后面创建的.  
使用-r自动删除.  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs rollback -r zptest/dir1@2014-05-1716:23:25  
[root@db-172-16-3-150 ~]# /opt/zfs0.6.2/sbin/zfs mount zptest/dir1  
[root@db-172-16-3-150 ~]# cd /zptest/dir1  
[root@db-172-16-3-150 dir1]# ll  
total 3080  
-rw-r--r-- 1 root root 1048576 May 17 16:22 1  
-rw-r--r-- 1 root root 1048576 May 17 16:23 2  
-rw-r--r-- 1 root root 1048576 May 17 16:23 3  
已经回到这个snapshot了.   
接下来要说的是clone, 因为snapshot一旦回滚后将丢失回滚点后面的所有文件系统的变更. 但是如果只想先看看这个snapshot是不是想要的, 那么可使用clone将一个snapshot克隆出来, 进行读写操作. 不是的话删掉clone即可.  
克隆必须在当前pool, 不能把克隆的集合放到其他pool里面. 例如我只创建了zptest这个pool, 可以把克隆后的目标放到同一个pool里面也就是zptest, 但是不能放到其他的pool, 例如zpool1.  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs list -t snapshot  
NAME                             USED  AVAIL  REFER  MOUNTPOINT  
zptest/dir1@2014-05-1716:23:12    19K      -  1.03M  -  
zptest/dir1@2014-05-1716:23:19    19K      -  2.03M  -  
zptest/dir1@2014-05-1716:23:25    18K      -  3.04M  -  
克隆必须基于snapshot, 不能直接克隆dataset. 克隆后, 相当于新建了一个dataset.  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs clone zptest/dir1 zptest/dir1_c1  
cannot open 'zptest/dir1': operation not applicable to datasets of this type  
[root@db-172-16-3-150 dir1]# /opt/zfs0.6.2/sbin/zfs clone zptest/dir1@2014-05-1716:23:12 zptest/dir1_c1  
[root@db-172-16-3-150 dir1]# df -h  
zptest/dir1           3.9G  3.0M  3.9G   1% /zptest/dir1  
zptest/dir1_c1        3.9G  1.0M  3.9G   1% /zptest/dir1_c1  
[root@db-172-16-3-150 dir1_c1]# /opt/zfs0.6.2/sbin/zfs list  
NAME             USED  AVAIL  REFER  MOUNTPOINT  
zptest          3.56M  3.81G    30K  /zptest  
zptest/dir1     3.09M  3.81G  3.04M  /zptest/dir1  
zptest/dir1_c1    18K  3.81G  1.03M  /zptest/dir1_c1  
[root@db-172-16-3-150 zptest]# cd /zptest/dir1_c1  
[root@db-172-16-3-150 dir1_c1]# ll  
total 1027  
-rw-r--r-- 1 root root 1048576 May 17 16:22 1  
你可以对这个clone出来的dataset进行读写.  
[root@db-172-16-3-150 dir1_c1]# cd /zptest/dir1_c1  
[root@db-172-16-3-150 dir1_c1]# cp 1 2  
[root@db-172-16-3-150 dir1_c1]# ll  
total 2053  
-rw-r--r-- 1 root root 1048576 May 17 16:22 1  
-rw-r--r-- 1 root root 1048576 May 17 16:57 2  
结合PostgreSQL的PITR来使用, 例如把$PGDATA放在dataset中, 对$PGDATA做基础备份可以通过对这个dataset做snapshot来达到目的, 但是建议在standby上这么做, 因为cow也是会带来额外的开销并且容易带来碎片的.  
另外一个建议在standby上做snapshot的原因是standby上的shared buffer中没有影响数据一致性的脏数据. 所以不需要执行pg_start_backup()直接创建snapshot即可, 如果在主库创建snapshot的话, 创建sanpshot前必须先执行pg_start_backup(), 在创建完snapshot后再执行pg_stop_backup().  
在standby上同时还需要使用archive_command将xlog归档, 这样的话在使用snapshot做PITR时可以用上需要的xlog.  
接下来我将演示一下使用场景.  
```  
![pic](20140518_01_pic_001.png)  
```  
使用这种方法终于让PG可以和ORACLE一样有基于块增量备份了.   
注意, 使用clonerecovery时, 需要注意修改对应的archive command, 不要覆盖原有的wal.   
在普通目录初始化主库  
pg93@db-172-16-3-150-> initdb -D /ssd4/pg93/pg_root -E UTF8 --locale=C -U postgres -W  
pg93@db-172-16-3-150-> cd /ssd4/pg93/pg_root  
pg93@db-172-16-3-150-> cp /home/pg93/pgsql/share/recovery.conf.sample ./  
pg93@db-172-16-3-150-> mv recovery.conf.sample recovery.done  
pg93@db-172-16-3-150-> vi pg_hba.conf  
host all all 0.0.0.0/0 md5  
host replication postgres 127.0.0.1/32 trust  
pg93@db-172-16-3-150-> vi postgresql.conf  
listen_addresses = '0.0.0.0'            # what IP address(es) to listen on;  
port = 1921                             # (change requires restart)  
max_connections = 100                   # (change requires restart)  
superuser_reserved_connections = 13     # (change requires restart)  
unix_socket_directories = '.'   # comma-separated list of directories  
tcp_keepalives_idle = 60                # TCP_KEEPIDLE, in seconds;  
tcp_keepalives_interval = 10            # TCP_KEEPINTVL, in seconds;  
tcp_keepalives_count = 10               # TCP_KEEPCNT;  
shared_buffers = 1024MB                 # min 128kB  
maintenance_work_mem = 512MB            # min 1MB  
vacuum_cost_delay = 10                  # 0-100 milliseconds  
vacuum_cost_limit = 10000               # 1-10000 credits  
bgwriter_delay = 10ms                   # 10-10000ms between rounds  