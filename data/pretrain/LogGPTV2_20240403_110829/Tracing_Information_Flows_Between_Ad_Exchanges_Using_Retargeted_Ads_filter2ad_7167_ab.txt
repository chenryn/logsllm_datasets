Cahn et al. performed a broad survey of cookie charac-
teristics across the Web, and found that less than 1% of
trackers can aggregate information across 75% of web-
sites in the Alexa Top-10K [11]. Falahrastegar et al. ex-
pand on these results by comparing trackers across geo-
graphic regions [20], while Li et al. show that most track-
ing cookies can be automatically detected using simple
machine learning methods [42].
Note that none of these studies examine cookie match-
ing, or information sharing between ad exchanges.
Although users can try to evade trackers by clear-
ing their cookies or using private/incognito browsing
modes, companies have fought back using techniques
like Evercookies and ﬁngerprinting. Evercookies store
the tracker’s state in many places within the browser
(e.g., FlashLSOs, etags, etc.), thus facilitating regenera-
tion of tracking identifiers even if users delete their cook-
ies [34, 57, 6, 47]. Fingerprinting involves generating a
unique ID for a user based on the characteristics of their
browser [18, 48, 50], browsing history [53], and com-
puter (e.g., the HTML5 canvas [49]). Several studies
have found trackers in-the-wild that use fingerprinting
techniques [3, 52, 35]; Nikiforakis et al. propose to stop
fingerprinting by carefully and intentionally adding more
entropy to users’ browsers [51].
User Profiles.
Several studies specifically focus on
tracking data collected by Google, since their trackers
are more pervasive than any others on the Web [24, 11].
Alarmingly, two studies have found that Google’s Ad
Preferences Manager, which is supposed to allow users
to see and adjust how they are being targeted for ads,
actually hides sensitive information from users [64, 16].
This finding is troubling given that several studies rely
on data from the Ad Preferences Manager as their source
of ground-truth [27, 13, 9]. To combat this lack of trans-
parency, Lecuyer et al. have built systems that rely on
controlled experiments and statistical analysis to infer
the profiles that Google constructs about users [39, 40].
Castelluccia et al. go further by showing that adversaries
can infer users’ profiles by passively observing the tar-
geted ads they are shown by Google [13].
3.2 Cookie Matching and Retargeting
Although ad exchanges have been transitioning to RTB
auctions since the mid-2000s, only three empirical stud-
ies have examined the cookie matching that enables these
services. Acar et al. found that hundreds of domains
passed unique identifiers to each other while crawling
websites in the Alexa Top-3K [2]. Olejnik et al. no-
ticed that ad auctions were leaking the winning bid prices
for impressions, thus enabling a fascinating behind-the-
scenes look at RTB auctions [54].
In addition to ex-
amining the monetary aspects of auctions, Olejnik et al.
found 125 ad exchanges using cookie matching. Finally,
Falahrastegar et al. examine the clusters of domains that
all share unique, matched cookies using crowdsourced
browsing data [21]. Additionally, Ghosh et al. use game
484  25th USENIX Security Symposium 
USENIX Association
4
theory to model the incentives for ad exchanges to match
cookies with their competitors, but they provide no em-
pirical measurements of cookie matching [23].
Several studies examine retargeted ads, which are di-
rectly facilitated by cookie matching and RTB. Liu et
al. identify and measure retargeted ads served by Dou-
bleClick by relying on unique AdSense tags that are em-
bedded in ad URLs [43]. Olejnik et al. crawled specific
e-commerce sites in order to elicit retargeted ads from
those retailers, and observe that retargeted ads can cost
advertisers over $1 per impression (an enormous sum,
considering contextual ads sell for 
(a)
c.net/code.js
d.org/flash.swf
(b)
Figure 3: (a) DOM Tree, and (b) Inclusion Tree.
lishers from the Alexa Top-1K.
• § 4.4: To collect ads, our personas crawl 150 pub-
• § 5: We leverage well-known filtering techniques
and crowdsourcing to identify retargeted ads from
our corpus of 571,636 unique crawled images.
Instrumenting Chromium
4.2
Before we can begin crawling, we first need a browser
that is capable of recording detailed information about
the provenance of third-party resource inclusions in web-
pages. Recall that prior work on cookie matching was
unable to determine which ad exchanges were syncing
cookies in many cases because the analysis relied solely
on the contents of HTTP requests [2, 21] (see § 3.2).
The fundamental problem is that HTTP requests, and
even the DOM tree itself, do not reveal the true sources
of resource inclusions in the presence of dynamic code
(JavaScript, Flash, etc.) from third-parties.
To understand this problem, consider the example
DOM tree for a.com/index.html in Figure 3(a). Based
on the DOM, we might conclude that the chain a → c →
d captures the sequence of inclusions leading from the
root of the page to the Flash object from d.org.
However, direct use of a webpage’s DOM is mislead-
ing because the DOM does not reliably record the inclu-
sion relationships between resources in a page. This is
due to the ability of JavaScript to manipulate the DOM
at run-time, i.e., by adding new inclusions dynamically.
As such, while the DOM is a faithful syntactic descrip-
tion of a webpage at a given point in time, it cannot be
relied upon to extract relationships between included re-
sources. Furthermore, analysis of HTTP request headers
does not solve this problem, since the Referer is set to
the first-party domain even when inclusions are dynami-
cally added by third-party scripts.
Figure 4: Overlap between frequent
commerce sites and Alexa Top-5K sites.
trackers on e-
To solve this issue, we make use of a heavily in-
strumented version of Chromium that produces inclu-
sion trees directly from Chromium’s resource loading
code [5].
Inclusion trees capture the semantic inclu-
sion structure of resources in a webpage (i.e., which
objects cause other objects to be loaded), unlike DOM
trees which only capture syntactic structures. Our in-
strumented Chromium accurately captures relationships
between elements, regardless of where they are located
(e.g., within a single page or across frames) or how the
relevant code executes (e.g., via an inline ,
eval(), or an event handler). We direct interested read-
ers to [5] for more detailed information about inclusion
trees, and the technical details of how the Chromium bi-
nary is instrumented.
Figure 3(b) shows the inclusion tree corresponding to
the DOM tree in Figure 3(a). From the inclusion tree,
we can see that the true inclusion chain leading to the
Flash object is a→ b→ c→ c→ d, since the IFrame and
the Flash are dynamically included by JavaScript from
b.com and c.net, respectively.
Using inclusion chains, we can precisely analyze the
provenance of third-party resources included in web-
pages. In § 6, we use this capability to distinguish client-
side flows of information between ad exchanges (i.e.,
cookie matching) from server-side flows.
4.3 Creating Shopper Personas
Now that we have a robust crawling tool, the next step
in our methodology is designing shopper personas. Each
persona visits products on specific e-commerce sites, in
hope of seeing retargeted ads when we crawl publishers.
Since we do not know a priori which e-commerce sites
are conducting retargeted ad campaigns, our personas
must cover a wide variety of sites. To facilitate this, we
leverage the hierarchical categorization of e-commerce
sites maintained by Alexa1. Although Alexa’s hierarchy
1http://www.alexa.com/topsites/category/Top/
Shopping
486  25th USENIX Security Symposium 
USENIX Association
6
has 847 total categories, there is significant overlap be-
tween categories. We manually selected 90 categories to
use for our personas that have minimal overlap, as well
as cover major e-commerce sites (e.g., Amazon and Wal-
mart) and shopping categories (e.g., sports and jewelry).
For each persona, we included the top 10 e-commerce
sites in the corresponding Alexa category. In total, the
personas cover 738 unique websites. Furthermore, we
manually selected 10 product URLs on each of these
websites. Thus, each persona visits 100 products URLs.
Sanity Checking.
The final step in designing our
personas is ensuring that the e-commerce sites are em-
bedded with a representative set of trackers. If they are
not, we will not be able to collect targeted ads.
Figure 4 plots the overlap between the trackers we ob-
serve on the Alexa Top-5K websites, compared to the
top x trackers (i.e., most frequent) we observe on the
e-commerce sites. We see that 84% of the top 100 e-
commerce trackers are also present in the trackers on
Alexa Top-5K sites2. These results demonstrate that our
shopping personas will be seen by the vast majority of
major trackers when they visit our 738 e-commerce sites.
4.4 Collecting Ads
In addition to selecting e-commerce sites for our per-
sonas, we must also select publishers to crawl for ads.
We manually select 150 publishers by examining the
Alexa Top-1K websites and filtering out those which do
not display ads, are non-English, are pornographic, or
require logging-in to view content (e.g., Facebook). We
randomly selected 15 URLs on each publisher to crawl.
At this point, we are ready to crawl ads. We ini-
tialized 91 copies of our instrumented Chromium bi-
nary: 90 corresponding to our shopper personas, and one
which serves as a control. During each round of crawl-
ing, the personas visit their associated e-commerce sites,
then visit the 2,250 publisher URLs (150 publishers ∗ 15
pages per publisher). The control only visits the pub-