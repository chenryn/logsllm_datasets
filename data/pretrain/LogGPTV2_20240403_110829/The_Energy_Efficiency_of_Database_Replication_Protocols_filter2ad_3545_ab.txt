### Strict Serializability

Similar to primary-backup replication, transactions in deferred-update replication are executed by a single node. In workloads where transaction updates result from large queries and complex processing, executing a transaction at a single node is more advantageous than executing it at all replicas, as in state machine replication. Deferred-update replication improves on primary-backup replication by allowing all servers to execute transactions.

In Figure 1, we summarize the normal case operation of the presented replication protocols:
- **State Machine Replication (SMR) (Fig. 1(a))**:
  1. Clients atomically broadcast their transactions to the databases.
  2. Replicas sequentially execute and commit the transactions in the order they are delivered by the atomic broadcast service.
  3. The client waits to receive the first answer sent by the replicas.

- **Primary-Backup Replication (PBR) (Fig. 1(b))**:
  1. Clients send their transactions to the primary.
  2. The primary executes the transaction, commits the changes to its local database, and forwards the resulting update statements to all backups.
  3. Upon receipt, the backups apply the updates to their state and acknowledge this fact to the primary.
  4. Once the primary receives acknowledgments from all backups, it sends the corresponding answer to the client.
  - Transaction execution is multi-threaded at the primary. In Figure 1(b), the execution of T2 can be concurrent with the execution of T1, and the database at the primary ensures proper serialization of the transactions.

- **Deferred-Update Replication (DUR) (Fig. 1(c))**:
  1. A client sends its transaction to any replica.
  2. The selected replica executes the transaction and atomically broadcasts the transaction along with state updates.
  3. Upon delivering a transaction, replicas execute a certification test to ensure that committing the transaction induces a strictly serializable execution (this certification is carried out using the parameters of the stored procedure to determine the set of items read).
  4. If the transaction passes certification, its updates are committed to the local database, and the transaction’s answer is sent back to the client. Otherwise, updates are discarded, and an abort notification is sent to the client.
  - Similarly to PBR, replicas can be multi-threaded. Additionally, DUR allows transactions to be executed concurrently at different replicas.

### Measuring Energy Efficiency

In this section, we discuss the implementation of the replication protocols, the experimental setup, and the measurement of their energy efficiency.

#### A. Implementations

We implemented all replication protocols in Java and used a Paxos implementation called JPaxos1 as the atomic broadcast service. JPaxos supports command batching and the execution of multiple instances of consensus in parallel. In our experiments, we rely on a designated quorum to reduce the number of servers required by the atomic broadcast service from \(2f + 1\) to \(f + 1\) in the normal case. The remaining \(f\) nodes are only used when failures occur. We set JPaxos’s batching timeout to 1 millisecond and the maximum batch size to 64KB. With these settings, JPaxos is never the bottleneck in our experiments.

In PBR and DUR, the updates of a transaction are expressed as a stored procedure. Each transaction is composed of two phases: a query phase and an update phase. The query phase only reads the database, and given a database state, executing the query phase of a transaction \(T\) uniquely determines the update phase of \(T\), or a no-op for read-only transactions. In DUR, the certification test relies on the parameters of the stored procedures.

#### B. Setup

The setup consists of three machines running CentOS 6.4, connected via a 48-port gigabit HP ProCurve 2910al switch. Each server is a dual quad-core 2.5GHz Intel Xeon L5420 with 8GB of memory. Two machines run the in-memory Apache Derby 10.10.1.1 database and a designated quorum of JPaxos; clients execute on the third machine. This setup tolerates one crash failure, i.e., \(f = 1\). We measure the drained power of each individual machine with the Liebert MPX power distribution unit. When idle, the two replicas consume 164.8 Watts: 86.4 Watts for the first server (containing two idle hard disks), and 78.4 Watts for the second server (containing one idle hard disk). The switch consumes 64 Watts when idle and 105 Watts when operating at full load, which represents between 1.3 and 2.2 Watts per port. Compared to the power required by the servers, this is negligible, and we thus omit the consumption of the switch in the results we report.

We measure the energy efficiency of the considered replication protocols under the TPC-C benchmark [10]. TPC-C is an industry standard benchmark for online transaction processing, representing a wholesale supplier workload. It consists of a configurable number of warehouses, nine tables, and five transaction types. With TPC-C, 92% of transactions are updates, and the remaining 8% are read-only. Unless stated otherwise, we set the number of warehouses to nine, the maximum that could fit in the memory of our servers. The amount of parallelism allowed by TPC-C is proportional to the number of deployed warehouses.

#### C. Comparing the Replication Protocols

In Figure 2, we compare the performance and costs of the replication protocols to those of a stand-alone database. In the four top graphs, we report various metrics as a function of the number of committed transactions per second. The load is increased by varying the number of clients from 1 to 10. In all experiments, each client submits 12,000 transactions, resulting in between 1 and 5 minutes of execution. The metrics considered are: the average latency to complete a transaction, the total CPU utilization of the two servers, the total power used by the two servers, and the resulting energy efficiency. We also present the percentage of aborted transactions with DUR (see Fig. 2(a)); with PBR, this percentage is always less than 1%. In all experiments, the 90th percentile latency is never more than twice the average latency—we omit it for clarity. Also, we omit the power drawn by the stand-alone server in Fig. 2(c) to improve readability (its power varied between 106 and 121 Watts). The bottom two graphs present a breakdown of the CPU utilization and power at peak load.

- **SMR (Fig. 2(a))**: Offers the lowest performance of all the protocols. Transactions must be executed sequentially in the order they are delivered by the atomic broadcast service. At its peak load of 331 TPS, the execution is CPU-bound, with the thread executing transactions fully utilizing one core (in Fig. 2(d), a CPU utilization of 100% is equivalent to one fully used CPU core).

- **PBR and DUR (Fig. 2(a))**: Thanks to their ability to handle multi-threading, PBR and DUR offer higher throughputs. With up to 3 clients, PBR achieves a lower throughput than a stand-alone server due to the higher latency imposed to execute a transaction. With more clients, the stand-alone server and PBR achieve similar throughputs. Interestingly, the maximum throughput of a stand-alone server is reached with 3 clients; with more clients, lock contention limits performance. DUR provides more performance than a stand-alone server and reaches a throughput of 451 TPS due to its ability to execute the query phase of a transaction at any replica.

- **Aborted Transactions (Fig. 2(a))**: At peak load, PBR aborts less than 1% of transactions, while DUR aborts 1 out of every 3 transactions. This hurts energy efficiency as work is wasted. We observe this in Fig. 2(d) and Fig. 2(c), where DUR consumes more CPU cycles (and consequently draws more power) than PBR across almost all loads. PBR draws the least power of the protocols because backups only execute the update phase of transactions and PBR aborts few transactions.

- **Energy Efficiency (Fig. 2(b))**: The difference in drained power between PBR and DUR is modest, never more than about 4%. Consequently, the energy efficiency of these protocols does not vary by more than the same percentage. However, DUR reaches a higher energy efficiency than PBR because it can sustain a higher throughput.

- **Power Consumption (Fig. 2(c))**: The energy efficiency of all protocols increases with the load. This is because servers are not power-proportional: when idle, servers already draw a large percentage of their maximum power (typically 50%). SMR, PBR, and DUR reach a maximum efficiency that is respectively 47%, 54%, and 59% of the maximum efficiency of a stand-alone server. This represents a large overhead. In Section V, we show techniques that make replication more efficient.

- **CPU Utilization Breakdown (Fig. 2(e))**: We present a breakdown of the CPU utilization at peak load using a JVM profiler. We isolate the CPU cycles required to execute transactions, those to communicate (including serializing and de-serializing messages), and the ones to carry other tasks such as those done by the Java garbage collector. Unsurprisingly, with the TPC-C benchmark, it is the transaction execution that consumes the most CPU cycles with all protocols.

- **Power Breakdown (Fig. 2(f))**: We report the power used by the servers when idle, the power drawn by the transaction execution, and the power needed by other tasks. To obtain the power required for transaction execution, we record a trace of the transaction execution times, replay the trace, and replace transaction execution with sleeping for the durations recorded in the trace. The power required to execute transactions is then the difference between the power when transactions are executed and when we replay the trace. To obtain the power used by other tasks, we subtract the idle power from the power required when replaying the trace.

### Conclusion

In summary, DUR and PBR offer better performance and energy efficiency compared to SMR, with DUR providing the highest throughput and energy efficiency due to its ability to execute transactions concurrently at different replicas. However, DUR's higher abort rate impacts its overall efficiency. Future work will focus on improving the efficiency of replication protocols.