strict serializability.
Similarly to primary-backup replication, transactions
are executed by a single node in deferred-update repli-
cation. In workloads where transaction updates are the
result of large queries and complex processing, executing
a transaction at a single node is more advantageous than
executing it at all replicas, as in state machine replication.
Deferred-update replication improves on primary-backup
replication by allowing all servers to execute transactions.
In Fig. 1, we summarize the normal case operation of
the presented replication protocols. With SMR (Fig. 1(a)),
(1) clients atomically broadcast their transaction to the
databases, (2) the replicas sequentially execute and com-
mit the transactions in the order they are delivered by
the atomic broadcast service, and (3) the client waits to
receive the ﬁrst answer sent by the replicas.
With PBR (Fig. 1(b)), (1) clients send their transaction
to the primary; (2) the primary executes the transaction,
commits the changes to its local database, and forwards
the resulting update statements to all backups; (3) upon
receipt, the backups apply the updates to their state and
acknowledge this fact to the primary; (4) once the primary
receives acknowledgments from all backups, it can send
the corresponding answer to the client. In contrast to
SMR, transaction execution is multi-threaded at the pri-
mary. In Fig. 1(b), the execution of T2 can be concurrent
with the execution of T1, it is the database at the primary
that will ensure proper serialization of the transactions.
Finally, with DUR (Fig. 1(c)), (1) a client sends its
transaction to any replica; (2) the selected replica executes
the transaction and atomically broadcasts the transaction
along with state updates; (3) upon delivering a transac-
tion, replicas execute a certiﬁcation test to ensure that
committing the transaction induces a strictly serializable
execution (this certiﬁcation is carried out using the pa-
rameters of the stored procedure to determine the set
of items read); (4) if the transaction passes certiﬁcation,
its updates are committed to the local database, and the
transaction’s answer is sent back to the client. Otherwise,
updates are discarded and an abort notiﬁcation is sent
to the client. Similarly to PBR, replicas can be multi-
threaded. In addition, DUR allows transactions to be
executed concurrently at different replicas.
IV. MEASURING ENERGY EFFICIENCY
In this section, we discuss how the replication protocols
the experimental setup, and
are implemented, present
measure the energy efﬁciency of the protocols.
A. Implementations
We implemented all replication protocols in Java and
use a Paxos implementation called JPaxos1 as the atomic
broadcast service. JPaxos supports command batching and
the execution of multiple instance of consensus in parallel.
In the experiments below, we rely on a designated quorum
to reduce the number of servers required by the atomic
broadcast service from 2f + 1 to f + 1 in the normal case.
The f remaining nodes are only used when failures occur.
We set JPaxos’s batching timeout to 1 millisecond and the
maximum batch size to 64KB. With these settings, JPaxos
is never the bottleneck in our experiments.
With PBR and DUR, the updates of a transaction are
expressed as a stored procedure (just like the transaction
1https://github.com/JPaxos.
410410410
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
itself). More speciﬁcally, each transaction is composed of
two phases, a query phase and an update phase. The query
phase only reads the database, and given a database state,
executing the query phase of a transaction T uniquely
determines the update phase of T , a no-op for read-
only transactions. In DUR, the certiﬁcation test relies on
parameters of the stored procedures.
B. Setup
The setup consists of 3 machines running CentOS 6.4
connected with a 48-port gigabit HP ProCurve 2910al
switch. Each server is a dual quad-core 2.5GHz Intel
Xeon L5420 with 8GB of memory. Two machines run
the in-memory Apache Derby 10.10.1.1 database and
a designated quorum of JPaxos; clients execute on the
third machine. This setup tolerates one crash failure, that
is, f is equal to 1. We measure the drained power of
each individual machine with the Liebert MPX power
distribution unit. When idle, the two replicas consume
164.8 Watts: 86.4 Watts for the ﬁrst server (containing
two idle hard disks), and 78.4 Watts for the second server
(containing one idle hard disk). The switch consumes 64
Watts when idle and 105 Watts when operating at full
load. This represents between 1.3 and 2.2 Watts per port.
Compared to the power required by the servers, this is
negligible and we thus omit the consumption of the switch
in the results we report.
We measure the energy efﬁciency of the considered
replication protocols under the TPC-C benchmark [10].
TPC-C is an industry standard benchmark for online
transaction processing. It represents a wholesale sup-
plier workload and consists of a conﬁgurable number of
warehouses, nine tables, and ﬁve transaction types. With
TPC-C, 92% of transactions are updates, the remaining
8% are read-only. Unless stated otherwise, we set the
number of warehouses to nine, the maximum that could ﬁt
in the memory of our servers. The amount of parallelism
allowed by TPC-C is proportional
to the number of
deployed warehouses.
C. Comparing the Replication Protocols
In Fig. 2, we compare the performance and costs of the
replication protocols to those of a stand-alone database.
In the four top graphs, we report various metrics as a
function of the number of committed transactions per
second. The load is increased by varying the number
of clients from 1 to 10. In all experiments, each client
submits 12,000 transactions, resulting in between 1 and
5 minutes of execution. The metrics considered are: the
average latency to complete a transaction, the total CPU
utilization of the two servers, the total power used by the
two servers, and the resulting energy efﬁciency. We also
present the percentage of aborted transactions with DUR
(see Fig. 2(a)); with PBR, this percentage is always less
than 1%. In all experiments, the 90-percentile latency is
never more than twice the average latency—we omit it for
the clarity of the presentation. Also, we omit the power
drawn by the stand-alone server in Fig. 2(c) to improve the
readability of the graph (its power varied between 106 and
121 Watts). The bottom two graphs present a breakdown
of the CPU utilization and power at peak load.
SMR offers the lowest performance of all the protocols
(Fig. 2(a)). Recall that with SMR, transactions must be
executed sequentially in the order they are delivered by
the atomic broadcast service. Not surprisingly, at its peak
load of 331 TPS, the execution is CPU-bound: the thread
executing transactions fully utilizes one core (in Fig. 2(d),
a CPU utilization of 100% is equivalent to one fully used
CPU core).
Thanks to their ability to handle multi-threading, PBR
and DUR offer higher throughputs. With up to 3 clients,
PBR achieves a lower throughput
than a stand-alone
server because the latency imposed to execute a trans-
action is higher. With more clients, the stand-alone server
and PBR achieve similar throughputs. Interestingly, the
maximum throughput of a stand-alone server is reached
with 3 clients, with more clients, lock contention limits
performance. Due to its ability to execute the query
phase of a transaction at any replica, DUR provides
more performance than a stand-alone server and reaches
a throughput of 451 TPS.
At peak load, PBR aborts less than 1% of transactions
while DUR aborts 1 out of every 3 transactions. This hurts
energy efﬁciency as work is wasted. We can observe this
phenomenon in Fig. 2(d) and Fig. 2(c), where we see
that DUR consumes more CPU cycles (and consequently
draws more power) than PBR across almost all loads. PBR
draws the least power of the protocols because backups
only execute the update phase of transactions and PBR
aborts few transactions.
The difference in drained power between PBR and
DUR is modest and is never more than about 4%. As a
consequence, the energy efﬁciency of these protocols does
not vary by more than the same percentage (Fig. 2(b)).
We note, however, that DUR reaches a higher energy
efﬁciency than PBR because DUR can sustain a higher
throughput.
The energy efﬁciency of all protocols increases with
the load. This is a consequence of the fact that servers are
not power-proportional: when idle, servers already draw
a large percentage of their maximum power (typically
50%). SMR, PBR, and DUR reach a maximum efﬁciency
that is respectively 47%, 54%, and 59% of the maximum
efﬁciency of a stand-alone server. This represents a large
overhead. In Section V, we show techniques that make
replication more efﬁcient.
In Fig. 2(e), we present a breakdown of the CPU
utilization at peak load using a JVM proﬁler. We isolate
411411411
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
)
s
m
(
y
c
n
e
t
a
L
35
28
21
14
7
0
)
W
(
r
e
w
o
P
l
a
t
o
T
)
%
(
d
a
o
l
k
a
e
p
t
a
U
P
C
230
220
210
200
190
180
550
440
330
220
110
0
Std-alone – – SMR –×– PBR –•– DUR – –
33
31
17 21 25
6
12
500
500
0
0
200
100
400
Committed transactions per second
300
(a)
SMR –×– PBR –•– DUR – –
200
100
400
Committed transactions per second
300
(c)
exec.
comm.
other
Std-alone
SMR
(e)
PBR
DUR
)
W
/
S
P
T
(
y
c
n
e
i
c
ﬁ
f
E
y
g
r
e
n
E
)
%
(
U
P
C
l
a
t
o
T
)
W
(
d
a
o
l
k
a
e
p
t
a
r
e
w
o
P
3.6
3
2.4
1.8
1.2
0.6
0
500
400
300
200
100
300
240
180
120
60
0
Std-alone – – SMR –×– PBR –•– DUR – –
1.7
1.5
1.3
275
325
375
0
200
100
400
Committed transactions per second
300
500
(b)
Std-alone – – SMR –×– PBR –•– DUR – –
500
0
200
100
400
Committed transactions per second
300
(d)
exec.
idle
other
Std-alone
SMR
(f)
PBR
DUR
Figure 2. The performance and costs of the replication protocols under the TPC-C benchmark. Graph (a) contains the percentage of
aborted transactions for DUR (only percentages larger than zero are reported).
the CPU cycles required to execute transactions, those
to communicate (including serializing and de-serializing
messages), and the ones to carry other tasks such as
those done by the Java garbage collector. Unsurprisingly,
with the TPC-C benchmark it is the transaction execution
that consumes the most CPU cycles with all protocols.
In Fig. 2(f), we perform the same task for power. We
report the power used by the servers when idle, the power
drawn by the transaction execution, as well as the power
needed by other tasks. To obtain the power required for
transaction execution, we record a trace of the transaction
execution times. We then replay the trace, replacing trans-
action execution by sleeping for the durations recorded in
the trace. The power required to execute transactions is
then the difference between the power when transactions
are executed and when we replay the trace. To obtain the
power used by other tasks, we subtract the idle power to
the power required when replaying the trace. As Fig. 2(e)
412412412
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:14 UTC from IEEE Xplore.  Restrictions apply. 
S
P
T
0
5
2
@
t
s
o
C
y
g
r
e
n
E
2.4
1.8
1.2
0.6
0
Std-alone – – SMR –×– PBR –•– DUR – –
1.87
1.81
1.86
1.0
×
•
Figure 3. The energy costs of the replication protocols under the