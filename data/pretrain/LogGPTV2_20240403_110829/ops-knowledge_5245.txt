User: 我们需要将多个服务器上的日志文件汇总到一个中心服务器上进行统一管理。请提供一个可行的方案，并附带相关的脚本和命令。
Assistant: 方案：

1. 在中心服务器上安装Logstash和Elasticsearch，作为日志收集和分析平台；
2. 在各个服务器上安装Filebeat，作为日志收集工具；
3. 配置Filebeat，将需要收集的日志文件路径添加到配置文件中，并设置输出到中心服务器的IP和接口（默认是5044）；
4. 在中心服务器上配置Logstash，使用beats input插件监听5044接口，接收来自Filebeat的日志数据，并使用grok filter解析日志格式；
5. 使用Elasticsearch作为存储和索引的数据库，将解析后的日志数据存储到Elasticsearch中；
6. 可以使用Kibana来可视化展示日志数据。

脚本和命令：

1. 在中心服务器上安装Logstash和Elasticsearch：

```
# 安装Elasticsearch
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.14.0-linux-x86_64.tar.gz
tar -zxvf elasticsearch-7.14.0-linux-x86_64.tar.gz

# 启动Elasticsearch
cd elasticsearch-7.14.0/bin
./elasticsearch

# 安装Logstash
wget https://artifacts.elastic.co/downloads/logstash/logstash-7.14.0.tar.gz
tar -zxvf logstash-7.14.0.tar.gz
```

2. 在各个服务器上安装Filebeat：

```
# 安装Filebeat
wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.14.0-linux-x86_64.tar.gz
tar -zxvf filebeat-7.14.0-linux-x86_64.tar.gz

# 配置Filebeat
vim filebeat.yml
# 将需要收集的日志文件路径添加到filebeat.yml文件中，并设置输出到中心服务器的IP和接口

# 启动Filebeat
./filebeat -e -c filebeat.yml
```

3. 在中心服务器上配置Logstash：

```
# 创建logstash.conf配置文件
cd logstash-7.14.0/bin
vim logstash.conf

# 添加以下内容
input {
  beats {
    port => "5044"
  }
}

filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
}

# 启动Logstash
./logstash -f logstash.conf
```

4. 使用Kibana可视化展示日志数据。

以上是一个基本的日志收集和分析方案，具体可以根据实际需要进行调整和优化。