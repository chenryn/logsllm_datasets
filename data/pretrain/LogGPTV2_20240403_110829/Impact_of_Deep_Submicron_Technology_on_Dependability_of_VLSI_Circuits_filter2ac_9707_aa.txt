title:Impact of Deep Submicron Technology on Dependability of VLSI Circuits
author:Cristian Constantinescu
Impact of Deep Submicron Technology on Dependability of VLSI Circuits 
Cristian Constantinescu 
Intel Corporation, JF1-231 
2111 NE 25th Ave, Hillsboro  
OR 97124, USA  
PI:EMAIL 
                  Abstract  
    Advances  in  semiconductor  technology  have  led  to 
impressive performance gains of VLSI circuits, in general, 
and  microprocessors,  in  particular.  However,  smaller 
transistor  and  interconnect  dimensions,  lower  power 
voltages,  and  higher  operating 
frequencies  have 
contributed  to  increased  rates  of  occurrence  of  transient 
and intermittent faults. In this paper we address the impact 
of deep submicron technology on permanent, transient and 
intermittent  classes  of  faults,  and  discuss  the  main  trends 
in  circuit  dependability.  Two  case  studies  exemplify  this 
analysis.  The  first  one  deals  with  intermittent  faults 
induced  by  manufacturing  residuals.  The  second  case 
study shows that transients generated by timing violations 
are capable of silently corrupting data. It is concluded that 
semiconductor industry is approaching a new stage in the 
design  and  manufacturing  of  VLSI  circuits.  Fault-
tolerance features, specific to custom designed computers, 
have 
into  commercial-off-the-shelf 
(COTS)  VLSI  systems  in  the  future,  in  order  to  preserve 
data  integrity  and  limit  the  impact  of  transient  and 
intermittent faults. 
integrated 
to  be 
1. Introduction 
    Moore’s law, which states that density of the integrated 
circuits roughly doubles every two years,  has withstood the 
test of time for the last three decades. New breakthroughs 
in  semiconductor  design  and  manufacturing  are  likely  to 
keep  this  trend  intact  for  at  least  ten  more  years.  For 
instance,  Intel’s  smallest  transistor  has  20  nm  gates,  uses 
0.8  nm  thick  oxide  (about  three  atomic  layers)  and  can 
switch  at  1.5  terahertz.  As  a  result,  it  is  projected  that  by 
the end of the decade a microprocessor will incorporate up 
to one billion transistors and deliver about 100000 million 
instructions per second (MIPS). However, these impressive 
performance gains come at a price. 
    Shrinking 
lower  power 
voltages  result  in  higher  sensitivity  to  neutron  and  alpha 
transistor  dimensions  and 
injection  confirm 
particles,  leading  to  significantly  higher  soft  error  rates 
(SER).  Smaller  interconnect  features  and  higher  operating 
frequencies  increase  the  number  of  errors  generated  by 
violations  of  the  timing  safety  margins.  The  likelihood  of 
faults,  due 
intermittent 
to  process  variations  and 
manufacturing  residuals, 
  Dependability 
is  growing. 
modeling  shows  that    complex  VLSI  circuits  can  be 
seriously  impacted  by  transient and intermittent faults and 
by silent data corruption (SDC) [4, 7].  Last but not least, 
physical  and  simulated  fault 
that 
computational  errors  may  occur  in  COTS  based  systems  
[3].  
   A  wide  array  of  techniques  have  been  devised  to 
mitigate the impact of faults experienced by VLSI devices. 
They  range  from  process  technology  to  circuit  design, 
concurrent  error  detection,  and  architectural  solutions. 
Silicon  on  insulator  (SOI)  and  triple-well  are  among  the 
process  solutions  devised  to  lower  circuit  sensitivity  to 
alpha  and  neutron  particles  [14,  21].  Circuit  approaches 
rely on building SER immune latches [1, 23]. Parity, error 
correcting  codes 
(ECC),  assertion  checking,  and 
redundancy  are  the  main  concurrent  error  detection 
techniques  [5,  12,  18,  24].  Architectural  solutions  range 
from  complex  machine  check  abort  (MCA)  [15]  to 
execution  of  two  copies  of  the  same  program  [16]  and 
redundant functional units at the microprocessor level [2]. 
Dual-processors,  running  in  lock-step,  dual-busses  and 
dual-port  I/O  controllers  are  used  at  the  system  level,  in 
order  to    handle    errors    generated    by      semiconductor 
devices [20]. 
    This  paper  discusses  the  impact  of  deep  submicron 
technology  on  dependability  of  VLSI  circuits  and 
underlines the increased threat of transient and intermittent 
faults.  The  paper  is  organized  as  follows.  Technology 
impact on  permanent,  transient, and  intermittent classes 
of  faults  is  analyzed  in  section  2.  Two  case  studies  are 
given  in  section  3.  The  first  one  describes  intermittent 
faults,  induced  by  manufacturing  residuals.  The  second 
case  study  shows  that  transient  timing  violations  can 
severely  affect  data  integrity.  Section  4  concludes  the 
paper. 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:42 UTC from IEEE Xplore.  Restrictions apply. 
induced  by 
transient  and 
2. Impact of deep submicron technology on 
fault classes 
   Faults  experienced  by  semiconductor  devices  are 
classified  as  permanent, 
intermittent. 
Permanent  faults  reflect  irreversible  physical  changes. 
Transients  are 
temporary  environmental 
conditions.  Intermittent  faults  occur  due  to  unstable  or 
marginal hardware. Several studies show that transient and 
intermittent  faults  are 
the  predominant  malfunctions 
experienced  by  modern  computers  [8,  11].  For  instance, 
after analyzing over 20 workstation years of error logs, Lin 
and  Siewiorek  reported  that  90%  of  the  crashes  were 
caused  by  transients  and  intermittents  [11].  In  this  section 
we discuss the impact of deep submicron technology on the 
permanent, transient, and intermittent faults classes. 
2.1. Permanent faults 
   Evolution  of  the  rates  of  occurrence  of  the  permanent 
faults  for  CMOS  microprocessors  and  static  and  dynamic 
memories,  over  the  last  decade,  is  showed  in  Fig.  1.  
Improvements 
and 
manufacturing techniques has led to a significant decrease 
of  the  failure  rates,  both  for  processors  and  memories, 
especially in the first half of the decade. A marked decrease 
of  the  SRAM  failure  rates  was  observed  in  the  last  four 
years.  DRAM  technology  proved  to  be  stable  and  the 
increase  of  circuit  complexity  and  size  has  had  a  limited 
impact  on  the  rate  of  occurrence  of  permanent  faults.  For 
example, a 32 times increase of DRAM capacity only led to 
a 1.5 increase of the failure rate.  
semiconductor 
design 
the 
of 
and 
supply 
    Presently,  semiconductor  industry  is  adopting  copper 
interconnects  on  a  wide  scale.  This  trend  will  have  a 
positive  impact  on  permanent  failure  rates,  as  copper 
provides a higher electromigration threshold, comparing to 
aluminum. 
2.2 Transient faults  
   Transient  faults  are  induced  by  neutron  and  alpha 
particles,  power 
interconnect  noise, 
electromagnetic  interference,  and  electrostatic  discharge. 
Higher  VLSI  integration  and  lower  supply  voltages  have 
been contributing to higher rates of occurrence of particle 
induced  transients  and,  as  a  result,  higher  SER.  On  one 
hand  shrinking  transistors  decrease  the  probability  of 
collecting the critical charge, necessary to upset a circuit. 
On the other hand the critical charge itself decreases even 
faster, because of lower cell capacity and supply voltages, 
leading  to  higher  SER  [9,  10,  13].  Figure  2  exemplifies 
this  phenomenon.  Measured  neutron  and  alpha  induced 
SER,  for  CMOS  SRAMs,  are  plotted  as  a  function  of 
memory  capacity.  Test  vehicles    for  two  technology 
generations, 0.25 µm powered at 2V and 0.18 µm, at 1.6V, 
were used for this study. High energy neutron experiments 
were carried out at Los Alamos Neutron Science Center. A 
228Th  source  was  employed  for  the  alpha  tests  [6]. 
Measurements  showed  that  the  effect  of  alpha  particles 
increased  30  times,  as  the  manufacturing  process  went 
from  0.25  µm  to  0.18  µm  and  supply  voltage  dropped 
from 2V to 1.6V. The impact of the neutrons raised only 
by  20%.  In  the  case  of  the  0.25  µm    technology    the  
neutron  induced  SER  was  6  times higher than the  error 
I
)
T
F
(
E
T
A
R
E
R
U
L
A
F
I
300
200
100
0
Microprocessors
SRAM 256 Kb
SRAM 4 Mb
DRAM 1 Mb
DRAM32 Mb
)
.
u
.
a
(
R
E
S
0.25um - neutron
0.25um - alpha
0.18um - neutron
0.18um - alpha
400
300
200
100
0
1990
1992
1995
1997
2001
2
4
6
8
10
MB
failure  rates 
Fig.  1.  Permanent 
for  CMOS 
semiconductor  devices 
(Source:  Telcordia 
Technologies, 
for 
Electronic Equipment. FIT=failures in 109 hours)  
Procedure 
Reliability 
Fig.  2.  SER  for  two  CMOS  SRAM  technology 
generations 
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:42 UTC from IEEE Xplore.  Restrictions apply. 
rates generated by alpha particles. By contrast, for a 0.18 
µm  process,  the  alpha  induced  SER  was  5  times  higher 
than neutron SER. 
    Modeling tools are available for evaluating SER due to 
cosmic rays and alpha particles [19]. For future circuits it 
is  estimated  that  both  transistor  size  and  critical  charge 
will  continue  to  decrease  with  process  scaling.  As  a 
consequence, the number of particles able to induce errors, 
for a given die area, will saturate, i.e.  SER is going to be 
determined  by  the  actual  number  of  neutron  and  alpha 
strikes [6]. In spite  of  the  saturation, the overall circuit 
SER will be higher due to the continuous  increase  of  the  
die  area.  Further  device  shrinking  will  also  raise  the 
likelihood of particle induced multi-bit errors. 
    Interconnect  characteristics  of  the  deep  submicron 
circuits are expected to have a significantly higher impact 
on  signal  timing  and,  implicitly,  on  dependability.  RC 
delays are rising, as the wire pitches and cross-sections are 
dropping.  Most  semiconductor  processes  scale  the  height 
of  the  interconnect  at  a  slower  rate,  in  order  to  limit  the 
increase  of  the  resistance.  Unfortunately,  the  high  aspect 
ratio of the conductors leads to  large coupling capacities. 
This effect is farther increased by smaller spacing between 
wires,  which  is  required  for  achieving  higher  packing 
densities. As the capacitance becomes the dominant factor 
in  gate loading the Miller effect, which occurs when two 
adjacent  signals  switch  simultaneously 
in  opposite 
directions, significantly affects on-chip delays [17]. At the 
same  time,  high  frequencies  increase  the  effect  of  the 
inductance and bring up the skin effect (signal propagation 
along  the  surface  of  a  wire).  The  skin  effect  makes  the 
resistance of the interconnect to vary with frequency  [22].  
As  a  consequence,  it  becomes  more  difficult  to  achieve 
delay  determinism.  The  resulting  violations  of  the  timing 
safety  margins,  e.g.  set-up  and  hold  time,  may  lead  to 
corruption  of  the  data  transferred  over  the  interconnect. 
Data integrity may be also affected by cross talk and false 
triggering  of  the  dynamic  logic,  as  the  capacitive  effects 
reach  both  across  adjacent  conductors  and  over  several 
metal  layers.  It  has  to  be  stressed  that  no  accurate 
modeling  tools,  similar  to those used for SER prediction, 
are available for estimating the rates of occurrence of the 
interconnect transient faults. 
2.3. Intermittent faults 
The study of intermittent faults is burdened by the fact that 
errors  induced  by  these  faults  are  very  similar  to  those 
generated  by  transients.  Three  main  differences  exist, 
though. First, an intermittent fault occurs repeatedly at the 
same  location.  Second,  errors  induced  by  intermittents 
tend  to  occur  at  a  higher  rate.  Third,  replacement  of  an 
offending  circuit  removes  the  intermittent  faults,  by 
contrast  with  transients,  which  cannot  be  eliminated  by 
repair.  Unfortunately,  accurate  error  and  repair  logs  are 
not always available, making analysis of intermittent faults 
rather difficult. In addition, no accurate tools are available 
to predict the rate of occurrence of this type of faults. 
    Data  gathered  through  a  field  dependability  study  is 
given herein, to exemplify the specifics of the intermittent 
faults. Error logs were collected, with the aid of specially 
designed  software,  from  193  information  technology 
production  servers.  Fig.  3  shows  the  number  of  memory 
single-bit  corrected  errors  reported  over  a  period  of  16 
S
M
E
T
S
Y
S
F
O
R
E
B
M
U
N
90
80