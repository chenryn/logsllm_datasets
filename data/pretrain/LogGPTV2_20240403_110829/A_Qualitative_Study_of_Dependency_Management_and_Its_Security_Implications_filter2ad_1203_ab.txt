### Dependencies and the Impact of Security Concerns on Developer Decisions

Online surveys or controlled experiments often impose the investigator's perspective, potentially distorting developers' true opinions. In contrast, semi-structured interviews are more suitable for our objectives [46]. These interviews are open-ended, allowing new ideas to emerge during the conversation based on the interviewee's responses. This method is used in most of the selected studies (15 out of 22 studies in Table 7).

### Descriptive Statistics of Participants

Table 1 provides descriptive statistics on the number of participants in the studies discussed in Section 3. On average, an interview-based study involves 13 developers. Notably, 75% of the selected papers report results from fewer than 17 interviews. Additionally, these studies typically focus on developers from a single company or community, which may introduce bias due to shared development strategies and approaches.

### Participant Demographics

Our sample includes developers ranging from junior to senior levels, including team leaders and CTOs. Some participants work on internal/corporate projects, while others develop web, embedded, mobile, or desktop applications. We conducted 30 interviews and retained 25 for analysis, representing 25 different companies across nine countries. Table 2 summarizes the key demographics of the interviewees.

### Interview Process

To collect primary data, we conducted interviews lasting approximately 30 minutes. We met in person with local interviewees and scheduled remote sessions via Skype or Webex for those in other locations. We did not offer monetary compensation, as the highly skilled professionals we interviewed were unlikely to be motivated by such incentives. Instead, we proposed that they share their expert opinions on a topic of interest. We followed the ethical review board procedures for consent and data management, ensuring all interviews were reported anonymously without revealing personal or company-identifiable information.

### Semi-Structured Interview Framework

We adopted a semi-structured interview format, allowing developers to guide the discussion flow, following the "grand tour interviews" principle [17]. Each interview included the following components, though not necessarily in this order:

- **Introduction**: The interviewer describes the context and motivation for the study.
- **Developer’s Self-Presentation**: The developer (D) shares their professional experience and current activities.
- **Selection of New Dependencies**: D discusses the process of selecting and including new dependencies in their projects.
- **Updating Dependencies**: D explains the motivations and insights behind updating dependencies, including the timing, frequency, and any relevant routines or regulations.
- **Usage of Automated Tools for Dependency Analysis**: D describes any automated tools used for dependency analysis and their integration into the project.
- **Mitigation of Dependency Issues**: D outlines how they address issues in dependencies, such as bugs or vulnerabilities.
- **Other General Comments**: D provides additional perceptions, comments, or recommendations on dependency management, particularly regarding security concerns.

### Recruitment of Participants

We sourced software developers from local development communities, using public channels and reference contacts. We employed a snowball sampling approach [12] to expand our pool of interviewees by asking respondents to distribute our call within their networks. To mitigate potential bias, we selected developers with diverse roles and responsibilities, each representing a different company and often a different country. Our criteria included at least three years of professional experience (with over ten years for six developers) and proficiency in C/C++, Java, JavaScript, or Python.

### Interview Coding and Analysis

We used applied thematic analysis [14] to analyze the interviews, summarized in Figure 1. This approach follows the principle of emergence [13], where data gain relevance through systematic code generation and iterative conceptualization. The analysis involves breaking down the data into manageable pieces (codes), comparing them for similarities and differences, and grouping similar concepts under the same heading (code group).

#### Open Coding Phase

In the first phase, we collected critical point statements from each transcript and assigned a code summarizing the key points. Two researchers independently coded the transcribed interviews, following the "iterative process" described by Saldaña [37]. They then reviewed and agreed on a common code structure, which was further reviewed by a third researcher. After each iteration, we achieved full agreement on the codes and code groups. We also checked for data saturation [30] to ensure that the interviewees were discussing the same concepts. Once saturation was confirmed, we conducted additional interviews to validate the stability of our observations (Additional confirmation step in Figure 1).

#### Initial Coding Results

We began the coding process after conducting ten interviews, initially creating 345 quotations and assigning 138 codes. Over six iterations, we consolidated the quotations and merged codes on similar topics, resulting in 151 quotations with 28 codes assigned.