textual password with graphical passwords – users must not only
correctly enter a textual password but also choose pre-registered
secret images from a portfolio of system-provided image.
At first glance, PassTag may seem nearly identical to TwoSteps
because both systems ask users to enter textual passwords as well
graphical passwords. However, the two schemes are fundamentally
different in design and user behavior as PassTag starts from user-
provided images as cues for textual passwords to improve both
security and memorability, while TwoStep uses textual passwords
and (system-provided) images independently to improve security.
3 PASSTAG DESIGN
In order to exploit the picture superiority effect [1] and levels-of-
processing effect [6] in PassTag design, users must first provide
their images and then input their text passwords sequentially. We
ask users to supply their own images and memorable short texts
along with those images because we surmise that both personalized
and user-provided images and texts can increase memorability to
a greater degree – users might easily recognize their own images
(e.g., their own lotus image) from other (random) pictures and use
the images as cues to further supply text passwords along with
those images. By choosing the correct images and supplying texts,
users can successfully authenticate.
PassTag consists of the following four steps: 1) Image Secret
Creation, 2) Image Tagging, 3) Candidate Decoy Image Generation,
and 4) Authentication. Fig. 2 pictorially describes the overall process
of PassTagwith those steps.
Step 1. Image Secret Creation: In the first step, a user selects
and uploads a pair of independent images that would be easily
memorable and recognizable to the user, but would be difficult for
others. When the user chooses and uploads his/her own images
from their desktop or mobile phone as shown in the step 1 of Fig. 2,
there must be some constraints on images. We specifically informed
the user that these images were used for passwords. That is, user-
provided images should not be easily guessable by others (e.g.,
self-portrait). Therefore, we advised the user not to use images that
can easily be obtained from the public Internet, which lowers the
security. In fact, PassTag utilizes the web to automatically query
and check whether the uploaded images can be obtained from the
public Internet in order to avoid the selection of weak images.
Step 2. Image Tagging: Next, the user enters his/her textual
password consisting of at least two words for each image (i.e., user-
provided image-tag secrets), and verifies a second time by entering
the same textual password for the images as shown in the step 2
of Fig. 2. Fig. 3 shows an example of user-provided image secrets.
Then, the user was additionally advised to create the corresponding
textual password associated with the image (e.g., “peaceful mind”
in Fig. 3). However, we reminded the user again that the provided
texts would also be used for his/her password to encourage the user
to choose the texts that can easily be guessable from the image (e.g.,
lotus and flower).
Session 2: Authentication ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan62Figure 2: Overall process of PassTag.
Figure 3: User-provided image secrets and image-tag secrets.
Step 3. Candidate Decoy Image Generation: It is important
that not only the user can correctly choose his/her images but
also it must be difficult for others to guess the images. Perhaps,
close adversaries can guess the user-provided image secret with a
high chance because they are likely to exploit the preferences of a
victim user and his/her recent and/or important daily life events
that can give a hint of the possible password. To induce confusions
to such adversaries, we introduce a strategy to present a set of
decoy images which are similar to the original user-provided image
secrets Isimilar (e.g., pink lotus) with user-provided image secret
Iuser (e.g., lotus).
In our system, we use Imgur [20] to create a URL for each image.
Then, image URLs are submitted to Google Cloud Vision API [13],
which can detect and extract information about entities within an
image with the API’s label detection feature as shown in the step 4
in Fig 2. As a result, we can obtain the relevant text description or
labels of user-provided images (e.g., sacred lotus). However, there
is a significant security issue in this step. If Iuser is fixed, then most
likely the same sets of decoy images Idecoy will be returned for the
same input image Iuser , because the queries are fixed. Therefore,
an attacker can easily determine Iuser and Idecoy by using the
Figure 4: Pre-processing for variable label construction.
same queries with presented inputs and analyze the returned decoy
images. In order to thwart this attack, we inject random words
to perturb the original labels, and generate dynamically changed
decoy images of the user-provided images to mitigate the risk of
such automated attacks.
Pre-Processing for Modified Label Construction: To obtain
dynamically changed Idecoy from Iuser , we pre-process each label
by adding a random word (e.g., adjective or adverb) as a perturbation
from WordNet [27] to construct different label (e.g., gigantic sacred
lotus) to as shown in the step 5 in Fig. 2. The main reason for adding
adjectives and adverbs is that it does not change the meaning greatly
but retains an underlying original image class or category as shown
in Fig. 4.
Session 2: Authentication ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan63After pre-processing and generating a modified label, this gen-
erated modified label (e.g., gigantic sacred lotus) is submitted to
a search engine as shown in the step 6 in Fig 2. Then, we use an
API to search images of the given label from the Internet. For ex-
ample, the Google Image Search API crawls the images similar to
the ones the users uploaded. For each user-provided image, N th
number (e.g., 20) of similar images are obtained to be displayed
alongside the user-provided images (e.g. 20 different lotus images).
The number of similar images are less than certain threshold N th
per image, then we ask the user to upload a new image again so
that we can have sufficient candidate images to prevent adversaries
from guessing the user-provided images. We chose to obtain 20
similar images per user-provided image to display 40 images at a
time during authentication step to be over the offline attack limit
of 214 by Florêncio et al. [9].
user and I Adv
Adversarial Image Generation: Even with a modified label,
decoy images are obtained from the Internet which can lead to
reverse image search attacks where the attackers can filter out
decoy images with user-provided images. To thwart this attack,
we generate adversarial images I Adv
similar for Iuser and
Idecoy, respectively as shown in the step 7 of Fig. 2. The goal of an
adversarial image is to make an image classifier to mis-classify the
original input, where we formally define an adversarial example
generation as follow: given a valid input image I, and a target t
(cid:44) C∗(I), it is possible to find a similar input I′ such that C∗(I′)=t,
yet I and I′ are close according to some distance metric, which is
an adversarial example [39]. In Untargeted adversarial examples,
attackers only search for an input I′ so that C(I) (cid:44) C∗(I′) and I and
I′ are close. Then, finding adversarial examples can be formulated
as follows [39, 44]:
min
I′
s.t . C (I) (cid:44) C
||I′ − I||
∗(cid:0)I′(cid:1) .
(1)
The example of adversarial image generation process is depicted
in Fig. 5, where Iuser (i.e., sacred lotus) is the original input image
and a noise is added according to adversarial image generation
algorithm [24]. Then, the final I Adv
user is generated. Humans can still
recognize the added noise generated I Adv
user and do not find much dif-
ference from the original image Iuser . However, these images will
be new such that they cannot be searched from the Internet. More-
over, these images cannot be recognized by conventional image
recognition tools. Among the several adversarial image generation
algorithms, we chose an approach by Kim and Woo [24] which does
not require any knowledge of underlying machine learning models
(whitebox) in commercial APIs and can generate adversarial images
effectively.
Step 4. Authentication: The last step is Authentication, as shown
in the step 8 of Fig. 2 in which the user verifies his/her submission
by correctly selecting his/her own images from the decoy images
and typing in the correct text password for each selection. An ex-
ample authentication page is shown in Fig. 1. If the user is able to
correctly select both images and type in text passwords, the infor-
mation is saved. Otherwise, the user must go through three trials
where each trial displays 40 images containing 0, 1, or 2 original im-
ages alongside the other decoy images in random order. If the user
Figure 5: Adversarial image generation, where Iuser is the
user provided original image and I Adv
user is the generated ad-
versarial image.
selects an image, he/she will be asked to type the corresponding
text password associated with the image.
4 SECURITY CONSIDERATIONS FOR
PASSTAG
2
2
(cid:1)
Given a total of 40 images presented in three sessions and two text
inputs a user needs to provide, we considered various theoretical as
well as practical attack models. These were the key driving security
requirement parameters to the design of our system.
case, it would require the correct guesses up to(cid:0)40
Offline Brute-Force Random Guessing Attack: An attacker
can randomly try to guess images and texts. The number of trials
needed to guess correctly both two images and two texts are de-
fined as T Imaдes and TT exts, respectively. If an attacker randomly
attempts to choose 2 user-provided images successfully out of 40
displayed in each image displayed over the 3 session, at the worst
2
(> 228). In addition to two images, two input texts have to be en-
tered, correctly. Therefore, it is easily over the offline attack limit
of 214 guesses by Florêncio et al. [9]. However, this is the worst
case scenario, and next we present more realistic attacks, which an
attacker can exploit the correlations between images and texts.
(cid:1) ×(cid:0)40
(cid:1) ×(cid:0)40
Automated Images and Texts Correlation Attack: We as-
sume attackers can use popular machine learning APIs to automat-
ically find highly relevant texts from input images. For example,
attackers can download the presented images and produced the
relevant words for the presented images to guess text passwords,
which is more efficient than the above brute-force attack. In the
result section, we will evaluate the results of how user-provided
texts and images are correlated. This result can show that the effec-
tiveness of attacks using machine learning APIs that leverage the
correlations between images and texts.
Automated Image Search Attack: Similarly, attackers can
look for similarity and differences among the presented image
set. For example, analyzing the distribution of presented images
and category in which images are belong to (e.g., foods vs. clothes),
an attacker can possibly narrow down the guess by first throwing
out the different category of images. In order to prevent this attack,
we pre-process the labels of images by adding a random word as
seen in Fig. 4 to prevent identical images to appear when attackers
choose to search for images using a keyword and uniformly choose
candidate images from within their respective categories.
Candidate Image (Decoy) Exclusion Attack: Even though
we generate candidate decoy images with other images not available
from the Internet, it is possible for attackers to match and guess
Session 2: Authentication ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan64candidate images from the Internet. Therefore, an attacker can
remove and exclude those images as possible users’ answers. An
attacker can leverage again image search APIs to find returned
similar images for each candidate image. In order to defend against
this attack, we pre-process all our images by adding noise and
create adversarial images to thwart the machine learning attack.
In this way, image search APIs cannot produce any meaningful
output or return relevant image result. We carefully designed and
pre-processed each image to deceive Microsoft and Google machine
learning APIs for all images we present to users. Hence, this attacker
will not be effective.
Shoulder-Surfing Attack: Attackers may obtain information
about victims’ passwords by direct observation or external record-
ing device. Modern cameras and cellphones with high resolution
lenses make shoulder-surfing [25] a real concern if attackers target
specific users using passwords in public environment. To mitigate
shoulder-surfing attack, PassTag introduce decoy images that can
be helpful to induce confusions to attackers. Also, the user-entered
text in the password field can be shown as an asterisk to reduce the
risk of shoulder-surfing attack.
Close Adversary Attack: Close adversaries have the advantage
to know the users well and thus, make educated guesses to find the
correct answers. The threat can be significantly be increased, when
these close adversaries use additional tools such as social networks
or search engines for searching images. This kind of attack can be
considered as one of the worst case scenarios for security questions,
and authentication schemes, where users create authentication se-
crets in which they provide their own images. Threats by close
adversaries were shown to be very likely and thus, interesting to
consider [33]. To mitigate threats by close adversaries, PassTag gen-
erates decoy images that are similar to the user-provided images. We
hypothesize that if all images closely similar to each other, it will be
significantly more difficult for close adversaries to discern the user-
provided images compared to a scheme which generates random
images alongside the image secrets. In this work, we specifically
recruit close friends or family members to evaluate PassTag against
close adversaries.
5 STUDY PROCEDURE
We conducted three different user studies to evaluate the perfor-
mance of PassTagand to evaluate the different aspects of our ap-
proach. Table 1 shows the demographics the participants in all
three user studies. The objective of study 1 was to evaluate the
effectiveness PassTag’s design of generating similar decoy images
by evaluating its security against close adversaries with a near-
identical graphical-textual hybrid scheme that generated random
decoy images (RandomTag). The goal of study 2 was to evaluate the
usability of PassTag and evaluate the types of authentication secrets
and errors users made using PassTag. Lastly, we conducted study
3 to evaluate the memorability and security of PassTag against a
comparable baseline, security questions for up to three months.
For statistical testing we performed the pairwise Fisher’s Exact
Test (FET), which yields more accurate confidence for relatively
smaller sample size and the t-test for creation and authentication
time.
Table 1: Demographics of participants (N = 161).
Gender
Male
Female
Age group
Under 20
20–29
30–39
40–49
50+