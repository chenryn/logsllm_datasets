title:Predicting DRAM-Caused Node Unavailability in Hyper-Scale Clouds
author:Pengcheng Zhang and
Yunong Wang and
Xuhua Ma and
Yaoheng Xu and
Bin Yao and
Xudong Zheng and
Linquan Jiang
2022 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Predicting DRAM-Caused Node Unavailability in
Hyper-Scale Clouds
Pengcheng Zhang†, Yunong Wang‡, Xuhua Ma‡, Yaoheng Xu†, Bin Yao†§∗ , Xudong Zheng‡, Linquan Jiang‡
†Shanghai Jiao Tong University, Shanghai, China, {zhangpc@, sjtu xyh@, yaobin@cs.}sjtu.edu.cn
‡Alibaba Group, Hangzhou, China, {yunong.wyn, xuhua.mxh, xudong.zxd, linquan.jlq}@alibaba-inc.com
§Hangzhou Institute of Advance Technology, Hangzhou, China
node as an online node that fails to respond over a minute.
DRAM faults are observed as the leading hardware sources of
node unavailability.
Error Correction Code (ECC) has been developed to detect
and correct DRAM errors and mitigate the crucial impacts
of DRAM faults. For example, the single-bit error correction
and double-bit error detection (SEC-DED) ECC can correct a
single-bit error. The more complex Chipkill ECC [11] is able
to correct any number of error bits in a single chip. Errors
corrected by ECC are referred to as correctable errors (CEs).
Usually, a few CEs have negligible impacts on the system.
However, the error pattern may go beyond the capability of
ECC. Such an uncorrectable error (UE) typically leads to a
subsequent OS crash and node unavailability. As a result,
dozens of studies are dedicated to predict DRAM UEs [2],
[16], [18]. They commonly train a predictor from historical
data and then perform UE predictions for further mitigation
actions such as replacement of problematic Dual Inline Mem-
ory Modules (DIMMs) [16], job migration [2], etc.
In our hyper-scale cloud, besides node unavailability caused
by UEs, we ﬁnd that a live node may also fail to respond
when a burst of CEs occurs in a short period (denoted as
CE storm) and cause a kind of denial of service attack
[10], [30]. Critically, we ﬁrstly observe that CE storm is the
leading DRAM issue which dominates 56% DRAM-caused
node unavailability (DCNU). Furthermore, we notice that the
communications between a small fraction of DIMMs and the
hardware systems are sometimes lost, under which condition
the node will also be unavailable. Therefore, we propose to
predict DCNU, which not only considers UEs, but also takes
into account CE storms and DIMM communication losses.
To correlate DRAM errors and DCNUs, we conduct an
empirical analysis on the logs collected from more than half a
million nodes in ECS system over one year. We observe that
the temporal statistics of CEs are highly relevant to DCNUs:
CEs are frequently observed in the several hours before the
nodes become unavailable. Furthermore, we ﬁnd that the CE
spatial distributions are also strong indicators of DCNUs.
At the micro-level, a DRAM bank is structured as a two-
dimensional cell array indexed by rows and columns. A faulty
row/column is deﬁned as a row/column that experienced CEs
on at least two different spatial locations. Our analysis shows
that faulty rows/columns are found on over 70% nodes before
they become unavailable.
7
3
0
0
0
.
2
2
0
2
.
5
0
4
3
5
N
S
D
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
1
-
3
9
6
1
-
4
5
6
6
-
1
-
8
7
9
|
)
N
S
D
(
s
k
r
o
w
t
e
N
d
n
a
s
m
e
t
s
y
S
e
l
b
a
d
n
e
p
e
D
n
o
e
c
n
e
r
e
f
n
o
C
l
a
n
o
i
t
a
n
r
e
t
n
I
P
I
F
I
/
E
E
E
I
l
a
u
n
n
A
d
n
2
5
2
2
0
2
Abstract—DRAM faults are major hardware sources of cloud
node unavailability. To enable early preventive actions and
mitigate DRAM fault impacts, prior studies focus on predicting
DRAM uncorrectable errors (UEs) that typically cause immediate
node unavailability. In our cloud with over half a million nodes,
we ﬁrstly observe that the correctable error storm (numerous
CEs occur in a short period) dominates 56% DRAM-caused node
unavailability (DCNU). Therefore, we propose to predict DCNU
that takes account into both UEs and CE storms. Observing
that DCNUs have strong relevance to temporal statistics and
spatial patterns of CEs, we design novel spatio-temporal features
to train the prediction model. Considering the model’s real effects
cannot be evaluated by traditional metrics like F1-score, we
propose a new metric NURR to quantify the node unavailability
reduction and tune model hyperparameters with NURR. Our
approach achieves over 40% better NURR than existing methods
on historical data and runs stably in the production environment.
I. INTRODUCTION
Today’s software applications are increasingly built on cloud
services and deployed on cloud platforms. IDG [22] reports
that 81% of organizations leverage the cloud in their IT
infrastructure. Cloud platforms such as Amazon Web Services
(AWS) and Microsoft Azure are providing services to millions
of users worldwide on a 24/7 basis. Therefore, high avail-
ability becomes an essential requirement for cloud computing
systems. Many cloud service providers have promised exact
availability levels in their Service Level Agreement. For ex-
ample, for our public cloud service, Alibaba Cloud Elastic
Compute Service (ECS), we promise that the availability of
one multi-zone service should be no less than 99.995% [7]. It
means the service is only allowed to be unavailable at most
130 seconds per month.
Plenty of efforts have been made to improve service avail-
ability of cloud platforms [17], [25], [28], [29], [42]. However,
in reality, many hardware issues still exist in cloud comput-
ing systems, e.g., DRAM failures and CPU failures. These
failures typically cause computing nodes to be unavailable,
signiﬁcantly impacting the overall service availability. In our
cloud platform for ECS, a hyper-scale system composed of
over half a million computing nodes, we deﬁne an unavailable
∗Corresponding author
This work was supported by the NSFC (61922054, 61872235, 61832017,
61729202, 61832013), the National Key Research and Development Program
of China (2020YFB1710200),
the Science and Technology Commission
of Shanghai Municipality (STCSM) AI under Project 19511120300 and
Hangzhou Qianjiang Distinguished Expert Program.
2158-3927/22/$31.00 ©2022 IEEE
DOI 10.1109/DSN53405.2022.00037
275
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
Inspired by our statistical observations, we then design novel
spatio-temporal features for DCNU prediction. Firstly, we
design several DRAM error patterns (e.g., faulty row/column)
to capture the spatial CE distributions and aggregate them on
different DRAM components. Furthermore, we employ tempo-
ral aggregations to capture the important statistics such as sum
and increment of the spatial features and other kernel events in
both short-term and long-term time windows (e.g., the latest
3 hours and 4 weeks). The novel spatio-temporal features are
combined with static conﬁguration information such as CPU
generation and DRAM capacity to train a machine-learning
(ML) model for DCNU prediction. Considering the model’s
real effects cannot be evaluated by traditional metrics like F1-
score, we propose the new metric NURR (Node Unavailability
Reduction Rate) to quantify the node unavailability reduction
and tune model hyperparameters with NURR.
We then develop XBrainM (M means Memory), an Artiﬁcial
Intelligence for IT Operations (AIOps) system to predict and
mitigate DCNUs in hyper-scale clouds. It ensembles both
the ML model and traditional rule-based prediction, e.g., the
number of recent CEs exceed a certain threshold, to predict
DCNUs. When either the rules or the ML model reports a
DCNU alert, the system then migrates virtual machines (VMs)
hosted on the predicted faulty nodes to other healthy nodes.
After that, the domain experts quickly locate the underlying
DRAM issues, and ﬁnally, the problematic DIMMs get re-
paired or replaced.
The ofﬂine experiments on the one-year logs demonstrate
the effectiveness of our approach: the predictor achieves 40%
absolutely better NURR than existing methods, with more
than 77% recall and 65% precision. XBrainM has also been
integrated in the production environment of ECS system for
over a year. Online results show that XBrainM signiﬁcantly
reduces node unavailable rate and node unavailable time
(caused by DRAM faults) by 57% and 69%, respectively. Our
main contributions are summarized as follows:
• We ﬁrst propose to predict DCNU relevant to availability
in clouds and successfully built XBrainM to predict and
mitigate DCNU in hyper-scale clouds.
• We conduct statistic analysis on the ﬁeld data and observe
that: besides UEs, the CE storm is another major cause
of node unavailability; faulty rows/columns are strong
indicators of DCNUs.
• We use plenty of raw features and employ novel spatio-
temporal feature engineering to achieve high prediction
performance.
• Our predictor achieves more than 40% better NURR than
existing methods on historical data. We also integrated
XBrainM in the production environment of ECS system,
effectively reducing 69% node unavailable time.
strategies, respectively. Section VII provides the ofﬂine and
online results. The related work and conclusion are presented
in Section VIII and Section IX.
II. BACKGROUND AND PROBLEM FORMULATION
A. Terminology
DRAM fault and error. A fault refers to the underlying
cause of an error, while an error is the symptom of a fault. Note
that a fault may not manifest in errors if the faulty location is
not accessed.
DRAM errors are reported when the bits returned differ
from what has been written. Depending on whether ECC
can correct them, DRAM errors are further characterized as
correctable errors (CEs) and uncorrectable errors (UEs). A
fault may cause many errors (CEs/UEs) if the faulty addresses
are accessed frequently.
Hard and soft fault/error. Faults are categorized into soft
faults and hard faults. Soft faults occur randomly and may get
recovered, such as strikes of high-energy particles or cosmic
rays. Hard faults often refer to hardware damage. Errors
caused by soft/hard faults are referred to as soft/hard errors.
In practice, we follow a common rule of thumb and deﬁne
hard errors as the repeated errors on the same cell [21].
Faulty row and column. A faulty row/column is deﬁned as
a row/column on which errors occur on at least 2 different
locations. Usually, a faulty row/column is likely to trigger
many errors (CEs/UEs) when it is frequently accessed.
Node unavailability refers to that an online node fails to
respond for over a minute.
CE storm. Usually, when a CE is identiﬁed, processors send
System Management Interrupt (SMI) or Corrected Machine
Check Interrupt (CMCI) to the ﬁrmware or OS. Generally,
a few SMI or CMCI events have negligible performance
impacts. However, in the extreme cases of CE storms, the
system error handling mechanism would exhaust the CPU
resources for error handling and cause a node to become
unresponsive [10], [30]. In our practice, we frequently observe
node unavailable cases associated with the pattern that the
number of CEs exceeds 500 in 1 minute. When a node become
unavailable and the above pattern is satisﬁed, furthermore, we
did not ﬁnd UE and DIMM communication loss on the node,
we regard the unavailability is caused by CE storms.
DIMM communication loss refers to the condition that the
communication between the DIMM and hardware system is
lost, which typically can be reported by the system log.
DRAM-caused node unavailability (DCNU) refers to node
unavailability caused by UEs, CE storms, and DIMM com-
munication losses: a memory UE typically leads to a system
crash, and the node goes unavailable; a fraction of CE storms
or communication losses may also cause node unavailability.
The rest of this paper is organized as follows: Section II
introduces the background. Section III presents an exploratory
study to correlate DRAM errors and DCNUs. Section IV
introduces key components of our solution. Section V and
VI detail our feature engineering methods and evaluation
B. Cloud Computing Systems
A typical cloud computing system is composed of numerous
computing nodes, and each hosts multiple VMs. Let us take
ECS system as an example. ECS system is composed of more
than half a million physical computing nodes. The nodes are
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
276
DIMM0
DIMM1
Channel 0
Memory 
Controller
CPU
DIMM2