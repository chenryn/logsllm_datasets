extracted 80 distinct MSGs from these traces. We manually filtered
and mapped these 80 MSGs to 28 distinct malicious behaviors (as
we obtained previously from the 13 RATs), which are mapped to
31 MITRE techniques. This ground truth is referred to as GT2 and
will be used in further evaluations.
5.2.2 Evaluation results. In our experiment, we classify eighty
(80) MSGs mapped to thirty-one (31) MITRE techniques, where
each MSG is mapped to one or more techniques. Out of the eighty
(80) MSGs only twelve (12) are mapped to more than one technique,
nine (9) of them are mapped to two (2) techniques and three (3)
are mapped to four (4) techniques. This multiple-techniques map-
ping occurs because some behaviors can be achieved by multiple
techniques.
Table 4 shows MSG Classifier parameters we used for the exper-
iment. We used three metrics to evaluate our tool: top-n accuracy
and average and median ranking of the correct technique. Top-n
accuracy measures how often a correct technique falls in the n
highest-ranked techniques.
Table 5 shows the top-n accuracy for n being 1 to 5, 13 and 16.
MSG Classifier was able to rank the correct technique as the first
out of 31 techniques with an accuracy of 63.75%. This accuracy
jumped to 81.25% for top-2 and 90.0% for top-5. After that, accuracy
kept increasing slowly where it reached 96.2% for top-13 and 98.7%
for top-16. MSG Classifier also achieved an average ranking of 2.68
and a median ranking of 1 of the correct technique.
5.2.3 Analysis of MSG Classifier’s results. We first investi-
gated outputs where the top-1 predicted technique was incorrect.
In many cases, we found that the mapped technique is similar to the
correct technique(GT2). Although most MSGs in our ground truth
are mapped to only one technique, these MSGs can also be used to
achieve other techniques. To study this, we analyze the results of in-
correctly mapped MSGs by checking if the corresponding MSG can
achieve the highest two ranked techniques. For the highest ranked
technique, out of the 29 incorrectly mapped MSG, we found that 20
of them can be used to achieve a different technique than it mapped
to (In the GT2). Which implies our mapping (by our tool) was cor-
rect. For example, an MSG the contains the APIs: "ControlService"
683ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Sajid, et al.
Experiment
Top-1 Accuracy
Top-2 Accuracy
Original
63.75%
81.25%
Excluding
Stackoverflow
48.75%
62.5%
Excluding
Enriching
58.75%
73.75%
After Result
Analysis
88.75%
96.25%
Table 6: Top-n accuracies after analysis as well as excluding
StackOverflow and enriching component.
and "CreateService" are mapped to the "Stop Service" technique
in our ground truth, which is a technique an adversary can use
to stop or disable services. However, MSG Classifier mapped it to
the "System Services" technique, a technique an adversary uses to
abuse system services to execute commands. The MSG can be used
to achieve these two techniques, but our ground truth only maps
it to the "Stop service" technique is because the RAT that we used
to extract the MSG behavior used the MSG to stop service and not
to use services to execute commands. When updating the ground
truth based on our results analysis, the actual top-1 accuracy of the
MSG Classifier increased to 88.75% and top-2 accuracy increased to
96.25% as shown in Table 6.
We then investigated cases where the algorithm fails to predict
the correct technique in top-2 techniques and discovered two rea-
sons: 1) The semantic gap between MSG and the correct MITRE
technique. This semantic gap needs a deep understanding of the API
behavior and MITRE techniques to be filled. For example, an MSG
containing "IsDebuggerPresent," which checks if the calling process
is being debugged, should be mapped to the MITRE technique "Sys-
tem Information Discovery," a technique an attacker uses to gathers
information from the OS. This mapping represents a high semantic
gap as the algorithm needs to understand that checking if a process
is being debugged means the adversary is gathering information
about the system. 2) MSGs that consist of general-purpose APIs
like CreateFile or CreateProcess that can be used by many tech-
niques, making it difficult to identify the correct technique without
checking the API’s arguments. This excluding of arguments is a
limitation of the tool that we leave for future work.
5.2.4 MSG Classifier components importance. We studied the
importance of adding Stack Overflow data and the enriching com-
ponent of the MSG Classifier by running two experiments. In the
first experiment, we run MSG Classifier using only MSDN for API
text description and excluding Stack Overflow question-answer
pairs. In the second experiment, we run MSG Classifier without
the enriching step. Table 6 shows the top-1 and top-2 accuracies
for the original MSG Classifier and the two experiments. When we
excluded Stack Overflow, top-1 accuracy dropped to 48.75% with
a decrement of 23.53%, and top-2 accuracy dropped to 62.5% with
a decrement of 23.07%. When excluding the enrichment compo-
nent, top-1 accuracy dropped to 58.75% with a decrement of 7.84%,
where the top-2 accuracy dropped to 73.75%, with a 9.23% decre-
ments. These accuracies’ decrements show the importance of Stack
Overflow and the enriching component in MSG Classifier, while
Stack Overlow contributed more to the results than the enriching
component.
5.3 Performance Analysis of SODA
In this subsection, we evaluate the performance of SODA. We first
evaluate the time required by SODA to orchestrate the deception
actions, which can be divided into two parts: 1) Deployment time
and 2) Overhead (Malware response time). We also evaluated SODA
with multiple OECs and a single OES to demonstrate the scalability
of our approach.
5.3.1 Deployment time. During the phase of real-time orches-
tration, we provide the users the option to create new profiles or
selecting pre-built profiles. By selecting/creating profiles, users
are essentially deciding which deception ploys to deploy. This de-
ployment consists the following three aspects: 1) generating the
configuration file, 2) preparing necessary HF, and 3) forwarding
the End-Point DLL and the configuration file to the OEC.
Experiment setup and result In our evaluation, a user creates
her own profile based on a total of 50 ploys that we provide. Notably,
the default deception strategy is NativeExecute, where SODA allows
the malware to run to discover the malware’s current/future actions.
In the case of NativeExecute, HF preparation is not required as we
don’t modify the response, rather we only monitor the API calls
for discovery. Therefore, the deployment time will be minimal. We
repeat the experiment five times, changing the deception ploys
each time by increasing NativeExecute. Figure 9 depicts the various
deployment timelines for various ploys. Deployment time decreases
as expected as NativeExecute is increased as HF configuration time
is reduced. The maximum deployment time recorded is 72 sec. As
the deployment occurred before any malware entered the system,
the required deployment time is reasonable.
5.3.2 Determine overhead/response delay time by compar-
ing with the native execution. When the detection agent con-
firms a malicious process, the OEC injects the End-Point DLL into
the malware process. At first, we calculated the dynamic deception
delay by running a malware sample in a machine without SODA
and then with SODA. Finally, we calculated the time difference as
system overhead due to the orchestration. The deployment time
is not taken into account as we already evaluated it in the previ-
ous experiment. This experiment is performed on four different
malware (RAT, InfoStealer, Ransomware and Spyware). For each
malware, we selected a malicious behavior and a relevant ploy to
deceive it and recorded the malware execution time to complete
the selected behavior. We performed the experiment twice in a ma-
chine without SODA and then with SODA, recorded the execution
time and calculated the overhead time. We present our experimen-
tal result in Table 7. Our data shows that the maximum overhead
time was 18 seconds (15% increment compared to the normal mal-
ware execution) which is minimal/insignificant compared to the
running/campaign period of an APT/malware.
5.3.3 Measuring Scalability. To evaluate the scalability of our
approach, we run multiple OECs that send service requests to the
OES simultaneously. The aim of this experiment is to see how the
OES performs when several OECs request services at the same time.
Experimental setup: We used the same malware across all the
OECs and created the same profile from each of them to maintain
consistency. Initially, we started the experiment using two OECs
(clients) to send requests to the OES for profile selection/creation
and recorded the deployment time. Then we increased the OECs
by two and continued to record deployment times until the OECs
count reached ten. Finally, we performed the same experiment
684SODA: A System for Cyber Deception Orchestration and Automation
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Malware Type
RAT
InfoStealer
Ransomware
Focused Malicious
Behavior
Remote command
Execution
Steal credentials
from the browsers
Encrypt files for
Impact
FakeExecute Depletion
FakeExecute Depletion
FakeSuccess Diversion
Spyware
Capture screen
FakeExecute Discovery
Strategy
Deception
Goal
Deception Action
Execute the remote command in
HF and show it to the malware
Show honey credentials from
the HF
Pretend the encryption took
place without performing it
Capture screen from the HF
and send it to the attacker
Expectation (Observance to
consider deception ploy worked)
Command executed in HF
and is shown to the attacker
(C&C server is in our control)
Honey credentials is seen to be
exfiltrated (using packet capture)
This malware creates a ransom
note after successful encryption
(observe the note being created)
Captured screen of the HF is
uploaded to our FTP server
(redirected using ApateDNS)
T1
(sec)
T2
(sec)
13
38
15
43
O
(%)
15%
13%
126
144
14%
61
65
7%
Table 7: Malware deception overhead (T1 = time without deception, T2 = time with SODA deception, O = Overhead).
Figure 9: SODA deployment time with
different ploys
Figure 10: Avg time of a single OES to or-
chestrate and serve multiple OECs
Figure 11: Accuracy of SODA across dif-
ferent malware types
while running the malware on different OECs simultaneously to
record malware response time and calculated the overhead. Note
that the execution time of the malware without SODA is 15 seconds.
Actual result Our obtained result is presented in Figure 10. As
we can see, the overall orchestration time has increased as the num-
ber of OECs has increased (the right-most bars). From this experi-
mental result, we can conclude that even though the orchestration
time increased, OES still was able to serve it’s service successfully
with a is negligible overhead (maximum of 7s) compared to the
entire execution time of the malware (127s).
5.4 End-to-End Accuracy of SODA
In this section, we discuss the overall accuracy of SODA in terms of
deceiving malware successfully. In this experiment, we first used the
42 open-source malware samples for GT1 (mentioned in Section 5.1)
and 13 open-source RATs for GT2 (mentioned in Section 5.2) to
create the deception ploys and prepare the End-Point DLL. Then
we used four types of malware (RATs, InfoStealers, Ransomware
and Spyware) for testing.
Datasets and Evaluation metrics In this experiment, we used
six (6) RATs, 122 InfoStealers, 96 Ransomware, and 31 spyware. We
ensure that these malware samples are not used to create the decep-
tion ploys. Malware can be deceived at different MITRE tactics and
techniques levels or just at a single point. For example, a malware
collects some critical information about the system and exfiltrates
it to the C2 server. We can deceive the malware at each phase sepa-
rately or both phases. In other words, we can use different ploys to
deceive malware. To assess the accuracy of SODA, we verify how
many used ploys were successful in deceiving the malware. The
evaluation metrics: if the user selects N-number of deception ploys
and if SODA uses M of them to deceive malware then, we calculate
our accuracy to be (M/N)*100%.
Observation criteria to consider deception ploy worked
For RATs, it’s easy to observe the effectiveness of our detection
ploys since we have the C&C by ourselves created from the source
code. For each ploy, we observe the effect via the C&C server. For
InfoStealers, we use Wireshark to examine the exfiltrated creden-
tials to determine whether our deception ploys are working. For
Ransomware, malicious activities are clearly visible (ransom note
creation, file encryption). Typically Ransomware creates a ransom
note after successfully encrypting the files on the host machine.
The successful indication of our deception would be to fool the
malware in creating the ransom note, even if the encryption did not
take place. In the case of Spyware, it collects information about the
victims and uploads it to the attacker. Using Wireshark, we identify
the IP address where Spyware supposes to upload the file and used
ApateDNS (proxy) to redirect the packet to our hosted FTP server.
5.4.1 Experimental setup, expectations and results. To eval-
uate SODA with RATs, we used six (6) RATs with 37 distinct mali-
cious behaviors. Based on different deception strategies and goals,
we identified 116 valid deception ploys that can be deployed to
deceive these RATs. We selected and deployed these 116 ploys and
observed that SODA could deceive the RATs in 107 of them. In the
case of InfoStealers, we observed eight (8) distinct malicious behav-
iors for which we identified 49 valid deception ploys. We selected
and deployed these 49 ploys and observed that SODA could de-
ceive the InfoStealers in 47 of them. For Ransomware, we observed
eleven (11) distinct malicious behaviors for which we identified 28
valid deception ploys. We selected and deployed these 28 ploys and
observed that SODA could deceive the Ransomware in 27 of them.
Finally, for Spyware, we observed thirteen (13) distinct malicious
behaviors for which we identified 33 valid deception ploys. We se-
lected and deployed these 33 ploys and observed that SODA could
deceive the Spyware in 31 of them. Figure 11 presents the accuracy
of SODA across different malware types. Overall, on average SODA
achieved an accuracy of 95% (224 out of 237 ploys were successful
in deceiving malware).
02040608012345Deployment Time (Sec)Total PloysNativeExecutionDeployment time44475051543941444547566670102030405060246810Time Required by the OES (Sec)Number of OEC (Orchestration Engine Client)Deployment timeMalware response timeTotal orchestration time93.70%95.92%96.43%93.94%6.30%4.08%3.57%6.06%84.00%86.00%88.00%90.00%92.00%94.00%96.00%98.00%100.00%RATsInfoStealersRansomwareSpywareAccuracyMalware TypeSuccessfully DeceivedNot Deceived685ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Sajid, et al.
6 RELATED WORK
Honeypots [41, 42], honeynets [35] and honeypatches [18] are fre-
quently used to complement traditional detection and prevention
mechanisms. Such strategies provide attractive false information
(i.e., baits) to attackers in order to divert them from the real targets.
Advanced honeypot and honeynet strategies such as shadow hon-
eypot [17] hides information of the production system from the
attackers by creating an instrumented shadow version of the actual
systems with fake information. Anomaly detection sensors redirect
malicious network traffic to the shadow copy, while legitimate traf-
fic is directed to the actual system. In [22], the authors suggested
instrumenting production systems with fake services and mock
vulnerabilities to entice attackers. However, the fundamental hurdle
for these approaches is the lack of randomness. Although, in [21],
the authors proposed alternative solutions to formulate honeypots
that are indistinguishable from the real system. Unfortunately, these
approaches only work theoretically and are not directly applicable
to real-world circumstances. Furthermore, skilled attackers may
utilize techniques like [42] to detect and evade these approaches.
In contrast, our approach provides embedded deception through
API hooking and is deployed in the real environment, enabling
us to overcome the lack of randomness. Furthermore, the afore-
mentioned approaches are not considering the running context of