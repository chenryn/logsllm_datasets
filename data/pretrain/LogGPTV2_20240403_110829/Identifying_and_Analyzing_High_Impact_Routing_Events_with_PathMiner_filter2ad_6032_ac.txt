(b)
Density
(c)
Figure 5: Density versus Volume for 2D (λ, ν)-events obtained from slices of tensor C, 2013 only. Preﬁx (horizontal) slices in (a), AS
(lateral) slices in (b) and time (frontal) slices in (c). Experiments performed using Algorithm 1 with λ = 0.7, ν = 100 and ǫ = 1%.
Our approach has two steps. First we look at individual slices
of C and extract (λ, ν)-events within slices (Section 5.1). Then,
we aggregate similar slice events in order to form events that span
multiple slices (Section 5.2).
The algorithmic approach we adopt is well suited to the case
where signiﬁcant large, dense events exist in the data (as those pre-
sented in Figure 3). Thus it is well suited to the current problem,
while other algorithms (including [9]) may be more appropriate
when events are smaller and rarer. For the sake of brevity and due
to space limitations we present only a high level description of our
algorithms.1
5.1
2D Factorization
Let X be a slice of C. It can be a preﬁx (horizontal), AS (lateral)
or a time (frontal) slice. For example, X may be the preﬁx slice
shown in Figure 3(a). The goal of 2D Factorization is to extract
from X a set of large volume, high density patterns such as shown
in Figure 3(c). For simplicity of notation we assume from now on
that X is a preﬁx slice of C for preﬁx i. Description for AS and
time slices proceeds analogously.
To start, we deﬁne a rank-1 binary matrix: given binary vectors
a and b, a rank-1 binary matrix X is given by Xij = ai × bj . That
is, a rank-1 binary matrix is the outer product of two binary vectors
and can be thought of as a ‘block’ of 1s (after reordering of rows
and columns).
The 2D Factorization step repeats the following as long as a sig-
niﬁcant fraction of 1s can be obtained from X: Find a good rank-1
binary approximation for X, denoted M. If the set of rows J (rep-
resenting ASes) and columns K (representing days) with non zero
elements of M induces a submatrix B with volume at least ν and
density at least λ, then label the triple ({i}, J, K) as a (λ, ν)-event.
Next, independently of volume and density of B, remove all ones
captured by the set of rows J and the set of columns K. This strat-
egy is described more precisely in Algorithm 1.
The key challenge is to obtain the rank-1 approximation M (Line
5). In fact, this is equivalent to the Frequent Itemset Mining prob-
lem, which currently has not a known easy way to be exactly
solved. Furthermore, the tensor C may have many thousands of
slices, so the algorithm we use must run quite quickly. Hence we
use the strategy of relaxing the discrete problem into a continuous
one, solving the continuous problem, and thresholding the result to
ﬁnd an approximate discrete solution.
1Our implementation is publicly available and can be obtained at
http://cs-people.bu.edu/gcom/bgp/imc2014 or by
direct request to the authors.
Algorithm 1: 2D-FACTORIZATION
Data: n by m binary matrix X related to preﬁx i, λ, ν and
convergence parameter ǫ
Z′ ← Z
1 F ← {}
2 Z ← X
3 repeat
4
5 M ← RANK-1-APPROXIMATION(X, Z, λ, ν)
6
7
8
9
10
J ← {j : mjk = 1}
K ← {k : mjk = 1}
B ← X(J, K)
if den(B) ≥ λ and vol(B) ≥ ν then
F ← F ∪ {({i}, J, K)}
zjk ← 0
for (j, k) ∈ J × K do
11
12
13 until kZk = 0 or kZ−Z′k2
14 return F
kZk2 < ǫ;
Our relaxation seeks a real-valued rank-1 approximation to Z
(which initially is a copy of X). For this we use Non-Negative
Matrix Factorization (NNMF) [5]. Using NNMF we ﬁnd real non-
negative vectors w (n-by-1) and h (1-by-m) such that wh is a real
nonnegative rank-1 matrix approximating Z. We then threshold w
and h independently, obtaining the binary vectors w′ and h′, such
that M = w′h′ minimizes kX − Mk (see that the error is com-
puted considering the original matrix). We note that once w and h
are computed, computing M (by ﬁnding the optimal thresholding)
can be performed in time O(mn).
For the results in this paper, we ran Algorithm 1 for each of the
9 datasets with λ = 0.7, ν = 100 and ǫ = 1%. We considered as
input three different sets of slices: preﬁx slices, AS slices and time
slices. Figure 5 summarize the results by presenting a scatterplot
of density versus volume for the year of 2013 (same general com-
ments apply for other datasets). While preﬁx slices give us events
with a wide range of density and volume, AS and time slices behave
differently. Basically, most of the AS and time events have low den-
sity or volume close to 104. The explanation is twofold: ﬁrst, AS
and time slices are harder to mine, since their size is signiﬁcantly
larger than the size of preﬁx slices; and second, many of the events
on AS and time slices consist of one AS changing its next-hop to-
wards all (or almost all) preﬁxes in the network. Hence, because
preﬁx slices offer the best quality results (in terms of density) and
reveal the most interesting structure in the data, we use only pre-
426ﬁx slices to generate the (λ, ν)-events used in the next stage of the
algorithm.
5.2
3D Factorization
The next step is to ﬁnd 3 dimensional (λ, ν)-events in the tensor
C. The input S, obtained using the method in the last section, is a
set of triples of the form ({i}, J, K), where i represents a preﬁx, J
a set of ASes and K a set of points in time. The basic idea is to ﬁnd
triples having similar blocks (given by sets J and K) and to group
them to create 3D events.
We consider that there may be two ways of combining a pair of
blocks. First, they may be nearly identical – that is, their intersec-
tion may be nearly as large as their union. In that case, we merge
them by constructing the block that contains them both. Second,
their intersection may be much smaller than their union, but still
sufﬁciently large in terms of absolute volume.
In that case, we
merge them by constructing the block that is their intersection.
To evaluate these two cases, we deﬁne two functions over pairs
of triples x = (I, J, K) and x′ = (I ′, J ′, K ′) as:
dB(x, x′) = 1 −
|J ∩ J ′| × |K ∩ K ′|
|J ∪ J ′| × |K ∪ K ′|
and
sB(x, x′) = |J ∩ J ′| × |K ∩ K ′|.
(2)
(3)
The distance function dB measures the volume of the intersec-
tion of two blocks divided by the volume of their union. If close to
zero, then J is similar to J ′ and K is similar to K ′. Hence, it is
natural to assume that (I ∪ I ′, J ∪ J ′, K ∪ K ′) is a larger event.
We extend this strategy to merge multiple triples in a single step as
follows: given a triple x, look for a set S′ (which contains x) such
that maxy,z∈S ′ dB(y, z) ≤ γ, for some 0 ≤ γ ≤ 1. Once S′ is
found we combine all of its elements at once using the following
operator:
COMBINE-UNION(S′) =
 [y∈S ′
y1, [y∈S ′
y2, [y∈S ′
y3
 ,
where y1, y2 and y3 represent, for triple y, the sets of preﬁxes,
ASes and points in time respectively.
The similarity function sB captures the case when the intersec-
tion of two blocks is large enough by itself to merit merging the
blocks. That is, it may be the case that triple x is not nearly the
same as any other triple in S, but there still exists x′ such that
sB(x, x′) is signiﬁcantly large (larger than some threshold β). In
this case, we may conclude that (I ∪ I ′, J ∩ J ′, K ∩ K ′) is an
event. In fact, the second part of the algorithm looks for elements
x′ ∈ S such that sB(x, x′) ≥ β and then combines them using the
following operator:
COMBINE-INTER(x, x′) = (I ∪ I ′, J ∩ J ′, K ∩ K ′).
Alternating the test for maximum distance and minimum similar-
ity and using COMBINE-UNION and COMBINE-INTER operators
iteratively yields the discovery of new triples (possible representing
new (λ, ν)-events) and is the core of our strategy to ﬁnd 3 dimen-
sional blocks in C. The procedure is summarized in Algorithm 3
(presented in Appendix A).
One may note that Algorithm 3 does not check the density of
new formed 3D blocks. This is because extracting the relevant por-
tions of C for this check is so time consuming as to be prohibitively
expensive for our datasets. Of course this absence of veriﬁcation
may lead to triples that induce blocks with low density. A second
problem that may arise is that the extracted 3D blocks may still
show signiﬁcant overlap. In order to address these two issues we
added a ﬁnal stage that discards blocks with low density and blocks
that do not add new information to the results because it overlaps
with many others. This process is described in Algorithm 2, which
greedily (by decreasing order of volume) selects only triples with
a minimum density and that captures signiﬁcant information not
contained in blocks previously selected. Results and parameters
set up related to the methodology presented in this section will be
discussed in Section 6.
Algorithm 2: EVENT-SELECTION
Data: Tensor C, S, set of triples of sets of the form (I, J, K)
and thresholds λ and ν
1 L ← sort triples (I, J, K) ∈ S in decreasing order of
volume (|I| × |J| × |K|)
2 S′ ← ∅
3 for i = 1 to |L| do
(I, J, K) ← Li
4
B ← C(I, J, K)
5
if vol(B) ≥ ν and den(B) ≥ λ then
6
foreach (i, j, k) ∈ I × J × K do
7
8
Cijk ← 0
9
S′ ← S′ ∪ {(I, J, K)}
10 return S′
6. CHARACTERIZING EVENTS
After the execution of the set of algorithms presented in the pre-
vious section, we have a collection of 3D events extracted from
from the routing changes tensor C, each one with large volume and
high density. In this section, we brieﬂy pause to characterize the
events found by PathMiner in the 9 years of routing data.
After experimentation we settled on the following parameters for
the algorithms presented in Section 5.2: λ = 0.8, ν = 100, γ =
0.1 and β = 100.1 Table 3 summarizes the overall performance
of PathMiner when using these parameter settings on our data. The
column ‘# 1’s Retrieved’ is the total number of routing changes that
were contained in events, and the ‘Percent’ column is the fraction
of all routing changes in our data that were contained in events.
Table 3: Summary of (λ, ν)-events
Dataset
#Events
#1’s Retrieved
Percent
2005
2006
2007
2008
2009
2010
2011
2012
2013
5255
6823
8252
7996
8602
9646
12042
13910
13992
1107109
1689299
2504558
2411041
2466807
2952688
3991264
4611049
5880885
8.2
9.5
11.1
8.3
9.7
12.6
16.3
17.8
17.7
The table shows that PathMiner is able to ﬁnd many blocks, com-
prising a signiﬁcant fraction of the routing changes contained in the
datasets, ranging from 8.2% in 2005 up to 17.8% in 2012. Figures
1For instance, using β = 200 and γ ∈ {0.2, 0.05} did not change
the results signiﬁcantly in terms of percentage of retrieved next-hop
changes, average volume or average density. Tests performed with
2013 dataset.
4275
x 10
d
e
v
e
i
r
t
e
r
s
e
g
n
a
h
c
f
o
e
g
a
t
n
e
c
r
e
P
30
25
20
15
10
5
0
06
07
08
09
10
11
12
13
Year
(b)
06
07
08
09
10
11
12
13
Year
(a)
s
e
g
n
a
h
c
f
o
r
e
b
m
u
n
l
a
t
o
T
2
1.5
1
0.5
0
0
10
−1
10
−2
10
−3
10
−4
10
F
D
C
C
−5
10
2
10
y
a
d
a
s
t
n
e
v
E
600
500
400
300
200
100
0
0
10
F
D
C
C
−1
10
−2
10
−3
10
−4
10
06
07
08
09
10
11
12
13
Year
(c)
2005
2006
2007
2008
2009
2010
2011