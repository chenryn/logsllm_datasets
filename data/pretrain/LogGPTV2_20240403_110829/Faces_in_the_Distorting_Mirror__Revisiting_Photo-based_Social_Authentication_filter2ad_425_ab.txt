image comparison approach to identify the presented photos
within our collection: we crop the top left corners of the
presented photos, and match them to those of the photos
in the collection. In certain cases this might result in false
positives (e.g., the top left corner of the photo has only black
pixels), however it did not aﬀect our success rate. Table 1
presents the results of our experiments, indicating the average
ratio of photos identiﬁed correctly in each SA challenge (21
photos). We passed all the challenges with at least 18 of the
photos identiﬁed, and had identiﬁed at least 98.4% of the
photos in all scenarios.
Our approach is very eﬃcient as we are able to identify
the 21 photos within 40K photos in 1.24 seconds (∼ 0.06
per photo). One could possibly improve performance by
intersecting the suggested names and the tags after each
photo identiﬁed within a page.This could decrease times, as
the tags from one photo might be enough to infer the answer.
3. MEASURING USER ABILITIES
To design a secure SA system that exploits noisy and
unidentiﬁable faces, we need to verify our intuition that
humans are capable of recognizing their friends in photos
taken under natural conditions. Although previous work [12,
22] has explored the ability of people to discern human faces
or their features, we are the ﬁrst to focus on the ability
of recognizing friends (as opposed to unknown faces), even
under conditions where the faces may not be clear or even
present at all.
Measurement Application. We created a Facebook
app that replicates the SA mechanism, which require users
to identify their friends in SA challenges, and complete a
questionnaire for each photo. We opted for a Facebook app
for two reasons: ﬁrst, they inspire trust in users as they are
deployed within a sandbox and are governed by a series of
permissions that clearly state the user data accessed. Second,
a Facebook app enables direct access to some user proﬁle
data (e.g., their social circle). This enables us to respect
user privacy and minimize collection of potentially sensitive
information, since we use data stored on Facebook rather
than having users upload it to our own infrastructure.
IRB Approval. We issued an IRB protocol request to
the review board of our institution, that clearly described the
parameters of our study, and the data we would be gathering.
After it was approved we invited users to participate.
Recruiting Users. We explored and experimented with
the possibility of reaching human subjects through the Ama-
zon Mechanical Turk (AMT) service [1]. However, asking
Turks to install an app, or directing them to a URL outside
Amazon to ﬁll out a survey, explicitly violates the AMT
terms of services. Our tasks were rejected by Amazon be-
cause of this purely technical incompatibility. The nature of
our system, where challenges are crafted speciﬁcally for each
user, prohibited us from taking advantage of such crowd-
sourcing platforms. Therefore we resorted to recruiting users
directly by sharing our app with the online contacts of our
personal Facebook accounts, and posting ﬂyers around the
university campus. We also oﬀered prizes as an incentive for
user participation. This allowed us to collect and analyze
a signiﬁcant amount of user data, regarding over 4 million
photos, and over 1,000 solved SA challenges.
3.1 Measurement Workﬂow
Once a user installs our app, it collects the social graph
and related metadata (i.e., friends, URLs of photos, tags). It
processes the collected photos with state of the art face recog-
nition software, and categorizes them as simple , medium or
diﬃcult , based on the quality of the faces found. Photos of
each category are selected to create challenges of increasing
diﬃculty and measure the user’s ability to solve them.
Step 1: Face Extraction. We use the face.com online
service, which has since been acquired by Facebook [4]. Its
eﬀectiveness when using photos taken under realistic con-
ditions has been demonstrated [20], and it performs better
than other state of the art solutions [24]. We focus on two
speciﬁc metrics assigned to the detected faces.
Conﬁdence: when detecting faces, face.com’s classiﬁer
returns its level of conﬁdence that the tagged area contains
a face. Tags assigned a low conﬁdence level have a high
probability of not containing a face.
Recognizable: not all faces are suitable candidates for train-
ing (or being recognized by) a classiﬁer: face.com returns a
boolean value to indicate this; “true” when faces can be recog-
nized or are suitable to be used as part of a training set, and
“false” otherwise. Even if a face is present, due to various rea-
Type
Total Passed Success Per User
Simple
Medium
Diﬃcult
Total
362
347
335
1044
358
344
275
977
98.89%
99.14%
82.09%
93.58%
3.98
3.81
3.68
11.47
Table 2: Number of challenges taken from each cat-
egory, and percentage of successfully passed ones.
sons (e.g., angle, obstacles) proper face-classiﬁcation features
(e.g., haar, eigenfaces, ﬁsherfaces) cannot be extracted.
Step 2: Photo Categorization. Based on these metrics,
our app assigns photos to the following categories:
Simple - Figure 1(a): photos containing tags that most
likely frame a human face. This is our baseline category,
as it replicates the existing SA mechanism, and provides
a reference for comparison. According to [20], 80% of the
photos presented in SA challenges by Facebook had a face in
the tagged area that was detectable by software. Therefore,
we select photos with high conﬁdence (≥80%) which have
been classiﬁed as recognizable (recognizable=T).
Medium - Figure 1(b): photos with a high probability of
containing a face (conﬁdence≥80%), which are classiﬁed as
bad candidates for training/recognition (recognizable=F).
Diﬃcult - Figure 1(c): photos classiﬁed with a conﬁdence
below 40%. This category is to measure how eﬀective people
are at recognizing their friends even if their face is not visible.
Step 3: Photo Description. After a user selects the
name of each depicted friend, our app informs them if they
were right or wrong, and requires them to answer 4 questions
per photo describing: the content, the position and visibility
of the user’s face and other faces within the tagged area, and
reasons why the photo was useful or not (see Appendix A).
3.2 User Study Results
Our goal is to measure the users’ ability to recognize their
friends, and demonstrate that humans can solve this task in
conditions where the automated attacks would fail, as we
demonstrate in Section 4.3.
Collected dataset and demographics. 141 users in-
stalled our app which led us to a total of 4, 457, 829 photos
and 5, 087, 034 tags. However, 90 of the users actually com-
pleted challenges, out of which 79 were listed as male and 11
as female, from 6 diﬀerent countries. Of the 82 that reported
their age, 63 were between 20 and 30 years old and 15 were
between 30 and 40. On average, users had 347 friends each.
Recognizing Friends. Table 2 presents the number of
challenges (each has 7 pages with 3 photos of the same user
and 6 suggested names) per category, and the percentage
that users were able to solve (recognize at least 5 out of
7). Results are surprisingly high and consistent for medium
challenges, as users solve over 99% of the challenges.Thus,
even for photos with faces that cannot be identiﬁed by state of
the art face recognition software, users’ success rates are not
impacted. Users also score surprisingly well for the diﬃcult
challenges, with an 82% success rate.
Inﬂuence of the Social Circle Size. Figure 2 shows
the number of friends that a user has and the success rate for
solving SA challenges. Each point refers to the overall success
rate of a user for all 3 categories, and the label indicates
Figure 2: Correlation between number of friends
and challenges solved. Each point’s label indicates
how many challenges the user has taken.
the total number of challenges that the user completed. As
the number of friends increase, we expect users to score
signiﬁcantly lower. However, the results do not demonstrate
such an eﬀect, and no strong correlation is evident. The
suggestion of names is important, as users can decide based
on content that can be associated to certain friends. This
result is very encouraging, as it disproves concerns regarding
the applicability of SA for users with a large number of
friends. Here we only visualize users that have completed at
least 3 challenges for reasons of visual clarity, without the
overall distribution being aﬀected in any way.
Photo Content. Figure 3(a) shows the distribution of
answers regarding the content of the photos, which also oﬀers
an evaluation of the quality of the photo-detection process.
As expected, for the simple and medium categories, the
vast majority of photos (over 80%) are labeled as portraits,
meaning that the focus of the photos are human faces. In
contrast, in the diﬃcult one they account for 37%. These
numbers verify how accurate the face detection process of
face.com is, as the conﬁdence levels we have set in our photo
categorization process are veriﬁed by users. The diﬃcult
category is more evenly distributed.
Face Position. Figure 3(b) plots the distribution of
answers about the placement of the friend’s face with respect
to the tagged area. 49% of the medium photos contain a
clearly visible face inside the tagged area (InClear) and an
unclear face in 27.8% (InUnclear), cumulatively approaching
the 80% conﬁdence criteria. The diﬃcult photos do not
contain the friend’s face (Absent) in about half the photos.
Presence of Other Faces. Figure 3(c) shows the distri-
bution of other faces being visible in the photo, and their
placement in regards to the tagged area. The simple and
medium categories contain some other face in 83% and 77.5%
of the photos with faces being outside the tag in 41% and 45%
of the cases respectively. For the diﬃcult category, 43.5% of
the photos contain no human faces (Nobody).
Usefulness of the Photo. Figure 3(d) plots the distri-
bution of photos regarding their usefulness. The selected
friend was present in about 70% of the simple and medium
photos, which is less than the photos containing the friend’s
face according to Figure 3(b). This is due to users selecting
other options such as “remembering the photo” or “relevant
to this friend”, even though the friend’s face is in the photo.
An interesting aspect of the diﬃcult category, where photos
have a low probability of containing the face, is users relying
on other information to correctly select the friend. This cat-
egory has a higher percentage of answers that rely on other
Number of friends10020030040050060080010001500Successful Tests (%)7080901008231131212382311110193293234301167732130124222118122122294183571281185939126921973111121151465211361961896(a) Content of photo.
(b) Position and clarity of
friend’s face.
(c) Position and clarity of other
faces.
(d) Usefulness of photo.
Figure 3: Distribution of answers given by the users in our study.
Type
Portrait
Landscape Objects
Simple
97.4% (1133)
Medium 97.6% (1225)
92.1% (267)
Diﬃcult
94.9% (59)
90% (30)
76.9% (26)
0% (1)
0% (0)
64.5% (31)
Table 3: Success rates (and total number) for pages
where all 3 photos were labelled as the same type.
Figure 4: Correlation between content of the photos
and usefulness as reported by users.
types of visual clues and context for excluding (NoOneElse)
or inferring (Relevant) suggested names.
Absolute Success Rate per Category. Table 3 shows
the statistics for pages (each SA challenge contains 7 pages
with 3 photos each) in which all 3 photos are assigned to the
same category. We present the percentage of pages in which
the depicted friend was correctly identiﬁed, and the total
number of pages in parentheses. For diﬃcult portraits, users
were able to identify their friends in 92.1% of the pages. Thus,
people can identify their friends in photos where software
cannot even detect a face. In the medium category, users
were successful in more than 97% of the pages, validating
our initial intuition and the applicability of our approach.
Even though the number of the remaining diﬃcult challenges
is too small to draw concrete conclusions, it is surprising
that success rates are over 64% in all cases, and users even
identiﬁed their friends in 77.7% of the pages that contained
photos of animals. Thus, associative correlations assist users
when presented with a photo of objects or pets.
Absence of Friend’s Face. To verify the ability of
users to infer the answer even when the user is not present,
Figure 4 breaks these numbers down. It becomes evident,
as the cumulative ratios for the Landscape, Objects, Text
and Art photos account for 44% and 55.5% of Relevant and
Remember respectively. Thus, almost half of the photos for
which users relied on inference or memory, are not of faces.
Figure 5: Breakdown of the photos that were useful
for inferring or excluding answers, in regards to the
friend’s face ( F) and other people’s faces ( O).
We then focus on photos where the depicted friend’s face
was absent. When the users remember the photo, they are
almost always able to correctly identify the friend (only one
case with a wrong answer). Surprisingly, users achieve high
success rates when selecting the depicted friend based on the
relevance of the content, being 97.4% for the diﬃcult photos.
When users try to exclude suggested friends based on the
content until they can select the correct answer (NoOneElse),
they still achieve very high accuracy (albeit lower than the
Relevant ones) with a combined 89.6% across the 3 diﬃculty
categories. As expected, when the photo is not considered
useful the success rates are lower. However, in the simple
and medium categories, these photos belong to pages that
were correctly answered in more that 82% of the cases, due
to the high probability of the other photos containing a face.
Total absence of faces. We wanted to explore whether
the existence of other people in the photo might inﬂuence the
results, i.e., even if the requested friend is not in the photo