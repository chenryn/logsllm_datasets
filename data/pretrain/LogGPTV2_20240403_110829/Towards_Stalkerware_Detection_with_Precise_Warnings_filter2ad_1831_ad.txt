rounds as the number of labeled apps increases.
In Table 2, the detection performance is indicated by ACC and
AUC, which denote the accuracy score and the AUC-ROC score
of the detection model, respectively. “Baseline” is the performance
derived by using all the training instances. We introduce this base-
line as a reference to verify the effectiveness of the learning-by-
prediction active learning technique. The initial detection perfor-
mance of training round 0 is relatively low (both ACC and AUC
are lower than 0.8) due to its limited training data set. The detec-
tion accuracy increases consistently and significantly in training
963ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Han, Roundy, Tamersoy
Round Stalkerware Total Accuracy
11%
62%
41%
24
124
48
198
200
116
1
2
3
Table 3: Accuracy results for 3 rounds of active learning
conducted to improve Dosmelt’s ability to generalize. We
applied Dosmelt to 1.02 million unlabeled apps and hand-
labeled its most confident detections in each round. Accu-
racy improved until the third round, when Dosmelt seemed
to run out of stalkerware apps to detect.
rounds 1 and 2, producing ACC and AUC scores higher than 0.95,
which confirms the merits of the active learning method. Dosmelt
avoids exhaustive labeling efforts in favor of a focus on the most
informative data instances to correct the bias in the estimate of the
decision boundary. It thus minimizes the overhead of labelling the
training samples. As seen in Table 2, the model’s detection accuracy
is already comparable to that of the baseline when trained on only
60% of the training instances.
Subsequent training rounds based on the learning-by-prediction
method select 200 stalkerware apps for manual labeling in round
1 and round 2, but only 70 and 50 non-stalkerware apps in these
rounds, respectively. This discrepancy arises from the method’s
decision to label confidently detected apps, but it has the advantage
of enabling us to eliminate serious false alarms among benign apps
that erroneously labeled as malicious with a decision confidence
score larger than 0.9, thus correcting the classifier’s most serious
mistakes. A possible explanation for these false alarms is that rec-
ognizing the stalkerware with the TF-IDF based features depends
on the occurrence of the keywords or phrases relevant with the
surveillance functions. For example, “keystroke” is indicative of po-
tential surveillance use in the RF and GBDT based detection models.
This word could be an indication of keystroke logging functionality
in surveillance apps that monitor all keystrokes, but it could also
appear in the describe of an educational app designed to improve
typing speed. With limited labelled training samples, the detection
model is prone to overfit and to overestimate the importance of
words like this one. It is also prone to incorrectly attach importance
to any word that happens by chance to appear in the description
of a stalkerware app but not elsewhere even if this word does not
relate to surveillance. This issue arises despite our practice of not
assigning TF-IDF weights to words that occur only once in our
dataset. Adding additional labeled data through active learning
helps us to reduce overfitting of this kind. In Section 6.4, we will
discuss the keywords that are considered by the RF-based detector
as the most informative features for stalkerware detection.
Improving Dosmelt’s ability to generalize. Once we had man-
aged to achieve good classification results with Dosmelt on our
labeled dataset, we again turned to active learning to improve its
ability to generalize to as-yet undetected Android stalkerware. To
this end, we trained a Random Forest classifier on our full labeled
dataset of Android apps and performed three rounds of a learning-
by-prediction exercise against more than 1 million apps that were
not labeled, but for which we had app title and description infor-
mation (see Section 3). At the time of this experiment, Dosmelt’s
accuracy in the binary stalkerware classification task was already
Macro-F1
Micro-F1
Extra-Tree
0.735
0.838
0.910
0.925
0.723
0.842
0.910
0.905
RF
0.716
0.837
0.910
0.907
RF
0.732
0.840
0.914
0.925
Extra-Tree Number of labeled instances
Round
0
1
2
Baseline
Table 4: Multi-label stalkerware capability classification re-
sults across different training rounds using Random Forest
(RF) and Extremely Randomized Trees (Extra-Tree).
All labeled instances
409
615
830
above 97%. Even so, its generalization ability left much to be de-
sired. As seen in Table 3, we achieved much improved results in
the model’s ability to detect undiscovered stalkerware over three
iterations of active learning, thanks both to the improved labels,
and to an obvious need for improvements to the Dosmelt pipeline
that became apparent after the first round of active learning.
In the first round of hand-labeling apps identified through active-
learning, we noticed that our dataset contained many benign for-
eign language apps that Dosmelt was mistakenly classifying as
stalkerware. This was particularly problematic because the training
dataset consisted of hand-coded apps from which foreign-language
apps had been excluded, resulting in the model attaching high
importance to certain foreign-language words that were not rele-
vant to stalkerware. Accordingly, we added an improved foreign-
language filter, which dramatically improved Dosmelt’s ability to
generalize in the second round. In the third round, Dosmelt per-
formed well but seemed to run out of stalkerware apps to classify
after a certain point. It correctly identified 19 stalkerware apps
among its top 20 most confident predictions, but only identified 3
stalkerware apps among the 20 least confident predictions that we
hand-labeled. Though Dosmelt seemed to have approached a limit
in its ability to identify new stalkerware apps, the high turnover
rate among such apps [42] suggest that further runs on newer sets
of app descriptions would continue to produce new stalkerware
detections. When we uploaded all of these manually verified detec-
tions to the Coalition Against Stalkerware’s [18] Stalkerware Threat
List, we found less than a 3% overlap with the coalition’s set of
previously detected apps, which suggests that mining app titles and
descriptions to detect stalkerware with Dosmelt is complementary
to existing commercialized methods for detecting stalkerware.
6.3 Stalkerware Capability Classification
We now turn to the novel and challenging task of identifying the
surveillance capabilities of stalkerware apps using limited amounts
of labeled data in an active learning framework. For this classifi-
cation task, we use 438 stalkerware apps (30% of the stalkerware
apps) as our testing set, leaving 1,024 labeled stalkerware apps
for training. However, as before, we evaluate our active learning
capabilities by initially training with the description texts of only
40% of these labeled apps (i.e., 409 stalkerware apps). Note that
we use more labelled training instances at the initial step of the
training process for the multi-label nuanced classification task as
compared to the binary stalkerware detection task because we wish
to ensure that each of the 14 surveillance functions appears at least
three times in the labelled training instances. In each round of the
learning-by-prediction process, more stalkerware apps with their
surveillance capabilities confirmed by human annotators are added
to update the surveillance capability classifier.
8
964Towards Stalkerware Detection with Precise Warnings
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Capability
Browsing-History
Call-Logs
Call-Recordings
Camera
Contacts
Email
GPS-Tracking
Installed-Apps
Keylogging
Media-Extraction
Microphone
Screen
SMS
Social-Media
ACC AUC Positive label fraction
0.905
0.967
0.890
0.961
0.882
0.909
0.971
0.857
0.968
0.981
0.934
0.966
0.988
0.983
0.882
0.937
0.877
0.935
0.850
0.879
0.937
0.820
0.940
0.942
0.907
0.935
0.960
0.942
0.019
0.051
0.027
0.022
0.039
0.009
0.099
0.016
0.009
0.024
0.030
0.012
0.055
0.124
Table 5: Nuanced stalkerware classification results for each
surveillance capability using Random Forest (RF).
We report the results of active learning in Table 4, however this
time we use Macro-F1 and Micro-F1 scores to measure the accuracy
of our multi-label classifier on the testing instances. Again, we
divide the dataset into testing and training data five times at random
and report the averaged results. The table shows that classification
effectiveness consistently increases with each training round as
more training instances are added. The accuracy stabilizes once 52%
of the training instances are selected for model training, regardless
of the classifier’s architecture.
Compared to the results of binary stalkerware detection in Ta-
ble 2, surveillance-capability classification requires more labelled
training instances at the initial training step due to the nature of
multi-label learning, in which success depends on the ability to cap-
ture the correlation between different labels [12, 55, 61, 62]. In our
case, the classifier must learn the relationships and co-occurrences
of different surveillance capabilities in the training set. Therefore,
it requires more labeled training instances so that it can identify
the statistical correlations between surveillance capability labels.
In Table 5, we also show the ACC and AUC scores per surveil-
lance capability achieved after the second round of active learning.
For each surveillance capability label, we also give the fraction of
stalkerware in our dataset with this surveillance capability, pro-
vided under the Positive label fraction column. A lower value denotes
that the corresponding surveillance capability appears less.
As shown in Table 5, the sparsity level of the surveillance label
is associated with the classification accuracy with respect to the
corresponding surveillance function. The surveillance labels with
the positive label ratio higher than 0.02 in general have ACC and
AUC scores higher than 0.93. With more label occurrences, the
classifier can capture more stable correlations between the TF-IDF
text features and the labels it attempts to predict. The two accuracy
metric values of Call-Recordings and Contacts are exceptionally low,
under 0.9. One likely explanation for this observation is that the
keywords describing the function of recording incoming calls and
remote access to the contacts (like “dialing”, “memo” and “chat-
ting”) can be also found in the text descriptions of apps attributed
to the other surveillance types, such as Screen and Social-Media.
These keywords do not provide enough confidence to enable accu-
rate detection of these surveillance capabilities. Inversely, the ACC
score of Keylogging is higher than 0.96. Texts of the stalkerware