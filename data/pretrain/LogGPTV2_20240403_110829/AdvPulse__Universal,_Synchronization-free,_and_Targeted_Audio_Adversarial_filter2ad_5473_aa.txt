title:AdvPulse: Universal, Synchronization-free, and Targeted Audio Adversarial
Attacks via Subsecond Perturbations
author:Zhuohang Li and
Yi Wu and
Jian Liu and
Yingying Chen and
Bo Yuan
AdvPulse: Universal, Synchronization-free, and Targeted Audio
Adversarial Attacks via Subsecond Perturbations
Jian Liu∗
Zhuohang Li
Yi Wu
University of Tennessee
Knoxville, TN, USA
PI:EMAIL
University of Tennessee
Knoxville, TN, USA
PI:EMAIL
University of Tennessee
Knoxville, TN, USA
PI:EMAIL
Yingying Chen
Rutgers University
New Brunswick, NJ, USA
PI:EMAIL
Bo Yuan
Rutgers University
New Brunswick, NJ, USA
PI:EMAIL
ABSTRACT
Existing efforts in audio adversarial attacks only focus on the sce-
narios where an adversary has prior knowledge of the entire speech
input so as to generate an adversarial example by aligning and mix-
ing the audio input with corresponding adversarial perturbation.
In this work we consider a more practical and challenging attack
scenario where the intelligent audio system takes streaming audio
inputs (e.g., live human speech) and the adversary can deceive the
system by playing adversarial perturbations simultaneously. This
change in attack behavior brings great challenges, preventing exist-
ing adversarial perturbation generation methods from being applied
directly. In practice, (1) the adversary cannot anticipate what the
victim will say: the adversary cannot rely on their prior knowledge
of the speech signal to guide how to generate adversarial pertur-
bations; and (2) the adversary cannot control when the victim will
speak: the synchronization between the adversarial perturbation
and the speech cannot be guaranteed. To address these challenges,
in this paper we propose AdvPulse, a systematic approach to gen-
erate subsecond audio adversarial perturbations, that achieves the
capability to alter the recognition results of streaming audio inputs
in a targeted and synchronization-free manner. To circumvent the
constraints on speech content and time, we exploit penalty-based
universal adversarial perturbation generation algorithm and incor-
porate the varying time delay into the optimization process. We
further tailor the adversarial perturbation according to environmen-
tal sounds to make it inconspicuous to humans. Additionally, by
considering the sources of distortions occurred during the physical
playback, we are able to generate more robust audio adversarial
perturbations that can remain effective even under over-the-air
propagation. Extensive experiments on two representative types
of intelligent audio systems (i.e., speaker recognition and speech
∗Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’20, November 9–13, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-7089-9/20/11...$15.00
https://doi.org/10.1145/3372297.3423348
command recognition) are conducted in various realistic environ-
ments. The results show that our attack can achieve an average
attack success rate of over 89.6% in indoor environments and 76.0%
in inside-vehicle scenarios even with loud engine and road noises.
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Ma-
chine learning;
KEYWORDS
audio adversarial attack; synchronization-free; intelligent audio
system
ACM Reference Format:
Zhuohang Li, Yi Wu, Jian Liu, Yingying Chen, and Bo Yuan. 2020. AdvPulse:
Universal, Synchronization-free, and Targeted Audio Adversarial Attacks via
Subsecond Perturbations. In Proceedings of the 2020 ACM SIGSAC Conference
on Computer and Communications Security (CCS ’20), November 9–13, 2020,
Virtual Event, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.
1145/3372297.3423348
1 INTRODUCTION
Since audio interaction using one’s voice has become a major con-
venience in modern times, voice interaction in our daily lives has
been becoming either an alternative or even a full replacement of
traditional graphical user interfaces. Especially, recent advances
in deep learning techniques have enabled machines to achieve a
near-human performance in both understanding the speech con-
tent (i.e., speech recognition) [21] and identifying the speaker from
vocal traits (i.e., speaker recognition) [25]. Benefiting from such an
unprecedented performance advancement, modern deep-learning-
powered intelligent audio systems have been widely integrated in
almost every corner of our daily lives. For instance, people can talk
to their smartphones (e.g., Siri [8], Bixby [40]) or smart speakers
(e.g., Google Home [20], Amazon Echo [4]) to set an alarm, inquire
personal schedules, and control smart home appliances, etc. Mobile
banking (e.g., Chase Voice ID [7]) exploits remote voice authenti-
cation to quickly verify users and prevent fraud, and drivers can
operate their cars and access their functions simply through voice
commands (e.g., Hey BMW [23], Tesla Voice Commands [46]).
With the ever-growing deployment of such intelligent audio
systems, their vulnerabilities have gained considerable attention
and have become an increasing public concern recently. Specifically,
deep neural networks (DNNs), serving as the computational core
of the state-of-the-art intelligent audio systems, are revealed to be
inherently vulnerable to adversarial attack. This attack is where
the adversary can add imperceptible adversarial perturbations to
speech inputs to deceive DNN models, making the models yield
false predictions. This type of attack was initially discovered in
the image domain [9, 19, 33, 45] and has since spurred research
interests in the audio domain.
Existing studies [3, 10, 13] demonstrated that it is possible for
the adversary to inject unnoticeable adversarial perturbations to
the original audio and alter the transcription result. An existing
study [30] in the speaker verification task also showed that adding
carefully-crafted adversarial perturbations can lead to impostors
being falsely recognized as legitimate speakers. However, these
studies only launched digital domain attacks, which feed the crafted
adversarial examples to the intelligent audio systems directly with-
out considering any physical effects (e.g., audio distortion, ambient
noises) during the over-the-air propagation in practical scenarios.
To improve the robustness of the generated adversarial examples,
several recent studies [31, 52] use room impulse response (RIR) to
encode the acoustic channel state information (CSI) and launch prac-
tical over-the-air attacks. There are also other efforts attempting to
make the adversarial noise imperceptible to humans by leveraging
psychoacoustic effects [38, 41] or extend the over-the-air attack
range by employing domain adaption algorithms [12].
Limitations of Prior Research. Despite the initial success of
the over-the-air audio adversarial attacks (e.g., [12, 31, 52]), they
only focus on the scenarios where the adversary has prior knowl-
edge of the entire speech input (i.e., Static-speech attack scenario
as shown in Figure 1(a)). For each specific pre-recorded speech,
the adversary can add an adversarial perturbation to form an ad-
versarial example, which can then be played by a loudspeaker to
deceive intelligent audio systems. However, these attacks are not
applicable to streaming-speech attack scenarios, which are more
practical and common in the daily use of intelligent audio systems,
as shown in Figure 1(b). In this scenario, the intelligent audio sys-
tem takes streaming audio inputs (e.g., live human speech) and the
adversary can fool the system by playing imperceptible adversarial
perturbations through a nearby loudspeaker. We summarize the
following three major limitations preventing existing attacks from
being launched in practice with streaming audio input below:
(1) Modifying the Entire Audio Input. Most existing attacks require
the generated adversarial perturbation to be added on the entire
audio input. In other words, the generated perturbation should have
the exact same duration as the audio input, which is not feasible
when handling streaming inputs.
(2) Synchronization. Existing attacks are based on the assumption
that the input audio and the generated adversarial perturbation
are strictly synchronized. To guarantee the synchronization, the
adversarial perturbation is usually mixed with the audio input
beforehand and then played through a loudspeaker when launching
the attack.
(3) Prior Knowledge on the Audio Input. Most existing attacks re-
quire the adversary to have access to the input audio in advance.
That means the adversary needs to first collect an audio clip and
Figure 1: Comparison of the static-speech attack scenario
used in existing audio adversarial attacks and the proposed
streaming-speech attack scenario.
then compute the adversarial perturbation specifically for that piece
of audio, which is not applicable to streaming input either.
Subsecond-level Universal, Synchronization-free, and Tar-
geted Adversarial Perturbation. Due to the time-sensitive behav-
ior of audio signals, it is unrealistic to assume that the adversary
can anticipate the upcoming audio signal and allocate a particular
amount of time to modify the entire audio signal in a synchronized
manner. The above limitations show that the existing attacks are
only feasible in controlled static-speech scenarios (e.g., playing
prefabricated audio adversarial examples). To circumvent these lim-
itations in attacking streaming-speech-involved audio intelligent
systems, in this paper, we propose AdvPulse, a means to generate a
subsecond-level adversarial perturbation which can be added at any
point of the streaming audio input to launch targeted adversarial
attacks. The workflow of the proposed attack is illustrated in Fig-
ure 2. Specifically, 1○ instead of modifying the entire audio input,
we only need to add a very short adversarial perturbation of ∼ 0.5
seconds to the audio input; 2○ we do not require synchronization
between the input audio and the adversarial perturbation (i.e., the
adversarial perturbation can be injected anywhere in the streaming
audio input); and 3○ instead of crafting adversarial perturbation for
each specific input, we generate input-agnostic universal adversar-
ial perturbation that can make arbitrary audio input (i.e., streaming
speech) to be mis-recognized as the adversary-desired label.
To launch such an adversarial attack to deceive intelligent au-
dio systems with streaming-speech input, we designed a series
of mechanisms to address the aforementioned major limitations.
Specifically, to release the requirement of synchronization, we pro-
pose to add a subsecond audio adversarial perturbation and adopt
gradient-based adversarial machine learning algorithm to maxi-
mize the expected output probability of the target class over dif-
ferent delay conditions. This process enables that the adversarial
perturbation can be added at any timestamp of the audio input
while maintaining effective. Moreover, we exploit penalty-based
universal training on a set of speech samples to craft audio-agnostic
adversarial perturbation that can be added to arbitrary speech (e.g.,
streaming speech) causing the intelligent audio system to output
any adversary-desired label. To make the generated adversarial per-
turbation unnoticeable to humans, we add more restrictions in its
generation phase, making the short adversarial perturbation resem-
ble environmental sounds (e.g., bird singing, car horns, or HVAC
Original SpeechAdversarial PerturbationAdversarial Example“Yes”“No”Existing AttackOriginal SpeechAdversarial PerturbationAdversarial ExamplePre-recorded SpeechAdversarial PerturbationIntelligent Audio SystemAdversarial ExampleLoudspeaker(a) Static-speech Attack Scenario+=Intelligent Audio System(b) Streaming-speech Attack ScenarioStreaming SpeechAdversarial PerturbationLoudspeakerFigure 2: Workflow of the proposed attack.
noises). Additionally, to improve the robustness of the generated
adversarial perturbation during physical playback, we utilize sev-
eral techniques to address main sources of over-the-air distortion
(e.g., speaker and microphone limitations, reflection and reverber-
ation effects, and ambient noises). To expand the understanding
of the real-world vulnerability of intelligent audio systems, we
evaluated our adversarial attack on two representative types of
intelligent audio systems: X-vectors [44], the state-of-the-art DNN-
based speaker recognition model, and Google’s speech command
recognition model [39]1. We summarize our main contributions as
follows:
• To the best of our knowledge, this is the first work to design
a universal, synchronization-free and targeted adversarial at-
tack against intelligent audio systems, particularly for streaming-
speech attack scenario (Figure 1(b)), using subsecond adversarial
perturbations.
• We utilize the penalty-based universal adversarial perturbation
generation algorithm and optimize the adversarial perturbation
over the entire time delay distribution, rendering the attack to
be robust to arbitrary streaming audio with varying time delay
conditions.
• We propose to use an environmental sound mimicking technique
to make the generated subsecond adversarial perturbation resem-
ble situational sound effects (e.g., phone’s notification sound, car
horns), making the generated perturbation more inconspicuous
to human listeners.
• By incorporating the main sources of distortion occurred during
physical playback (i.e., frequency response of speaker and micro-
phone, reflection and reverberation, and ambient noise) into the
adversarial perturbation generation process, the effectiveness of
adversarial perturbations in physical over-the-air environments
can be kept.
• We performed case studies on both speaker recognition and
speech command recognition models. Extensive experiments are
conducted in both the digital and physical domains, including
inside an office, an apartment, and inside-vehicle environments.
The results show that our attack can achieve a high attack success
rate in deceiving these models with streaming-speech inputs
(e.g., overall 89.2% and 90.3% for speaker recognition and speech
command recognition in realistic settings, respectively).
2 RELATED WORK
Individual audio adversarial attacks. Research efforts on defeat-
ing intelligent audio systems started with individual adversarial
attacks, which specifically craft unique perturbations for each indi-
vidual audio sample to fool the system. For example, in speech recog-
nition tasks (i.e., speech-to-text transcription), Cisse et al. [13] first
1Audio and video samples are available at https://mosis.eecs.utk.edu/advpulse.html
showed that small distortions added to the input audio can lead to
false transcriptions. Several follow-up studies leveraging genetic al-
gorithm [3] or iterative gradient descent approaches [10, 12, 38, 52]
demonstrated the success of generating more powerful adversarial
examples that can alter the transcription to any adversary-desired
content. In speaker recognition tasks, existing studies [11, 30, 31, 55]
found that it is possible to manipulate the recognition result (e.g.,
making an impostor recognized as legitimate user) by injecting
adversarial perturbations to the original audio. However, the afore-
mentioned methods require prior knowledge on each individual
speech input sample to craft corresponding adversarial perturba-
tions, resulting in tremendous computational cost when generating
multitude adversarial examples. This also limits the attack to merely
time-insensitive settings such as generating adversarial examples
from pre-recorded audio, and has prevented it from being deployed
in more realistic scenarios (e.g., injecting adversarial perturbations
on live human speech) where collecting audio in advance is unfeasi-
ble. A more recent study [17] proposed to use a DNN network that