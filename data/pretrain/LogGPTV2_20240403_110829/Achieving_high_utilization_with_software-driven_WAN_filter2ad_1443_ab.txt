a day on a busy link in IDN. Assuming capacity matches
peak usage (a common provisioning model to avoid conges-
tion), the average utilization on this link is under 50%. Thus,
half the provisioned capacity is wasted. This ineï¬ƒciency is
not fundamental but can be remedied by exploiting traï¬ƒc
characteristics. As a simple illustration, Figure 1b separates
background traï¬ƒc. Figure 1c shows that the same total traf-
ï¬c can ï¬t in half the capacity if background traï¬ƒc is adapted
to use capacity left unused by other traï¬ƒc.
Second, the local, greedy resource allocation model of
MPLS TE is ineï¬ƒcient. Consider Figure 2 in which each
link can carry at most one ï¬‚ow. If the ï¬‚ows arrive in the
order FA, FB, and FC , Figure 2a shows the path assignment
with MPLS TE: FA is assigned to the top path which is one
of the shortest paths; when FB arrives, it is assigned to the
shortest path with available capacity (CSPF); and the same
happens with FC . Figure 2b shows a more eï¬ƒcient routing
pattern with shorter paths and many links freed up to carry
more traï¬ƒc. Such an allocation requires non-local changes,
e.g., moving FA to the lower path when FB arrives.
Partial solutions for such ineï¬ƒciency exist. Flows can
be split across two tunnels, which would divide FA across
the top and bottom paths, allowing half of FB and FC to
use direct paths; a preemption strategy that prefers shorter
paths can also help. But such strategies do not address the
fundamental problem of local allocation decisions [27].
Poor sharing:
Inter-DC WANs have limited support for
ï¬‚exible resource allocation. For instance, it is diï¬ƒcult to
be fair across services or favor some services over certain
paths. When services compete today, they typically obtain
throughput proportional to their sending rate, an undesir-
able outcome (e.g., it creates perverse incentives for service
developers). Mapping each service onto its own queue at
routers can alleviate problems but the number of services
(100s) far exceeds the number of available router queues.
Even if we had inï¬nite queues and could ensure fairness on
the data plane, network-wide fairness is not possible without
(a) Link-level
(b) Network-wide
Figure 3: Link-level fairness (cid:54)= network-wide fairness.
controlling which ï¬‚ows have access to which paths. Consider
Figure 3 in which each link has unit capacity and each ser-
vice (Siâ†’Di) has unit demand. With link-level fairness,
S2â†’D2 gets twice the throughput of other services. As we
show, ï¬‚exible sharing can be implemented with a limited
number of queues by carefully allocating paths to traï¬ƒc and
control the sending rate of services.
3. SWAN OVERVIEW AND CHALLENGES
Our goal is to carry more traï¬ƒc and support ï¬‚exible
network-wide sharing. Driven by inter-DC traï¬ƒc character-
istics, SWAN supports two types of sharing policies. First,
it supports a small number of priority classes (e.g., Inter-
active > Elastic > Background) and allocates bandwidth
in strict precedence across these classes, while preferring
shorter paths for higher classes. Second, within a class,
SWAN allocates bandwidth in a max-min fair manner.
SWAN has two basic components that address the funda-
mental shortcomings of the current practice. It coordinates
the network activity of services and uses centralized resource
allocation. Abstractly, it works as:
1. All services, except interactive ones, inform the SWAN
controller of their demand between pairs of DCs. In-
teractive traï¬ƒc is sent like today, without permission
from the controller, so there is no delay.
2. The controller, which has an up-to-date, global view of
the network topology and traï¬ƒc demands, computes
how much each service can send and the network paths
that can accommodate the traï¬ƒc.
3. Per SDN paradigm, the controller directly updates the
forwarding state of the switches. We use OpenFlow
switches, though any switch that permits direct pro-
gramming of forwarding state (e.g., MPLS Explicit
Route Objects [3]) may be used.
While the architecture is conceptually simple, we must ad-
dress three challenges to realize this design. First, we need
a scalable algorithm for global allocation that maximizes
network utilization subject to constraints on service prior-
ity and fairness. Best known solutions are computationally
intensive as they solve long sequences of linear programs
(LP) [9, 26]. Instead, SWAN uses a more practical approach
that is approximately fair with provable bounds and close
to optimal in practical scenarios (Â§6).
Second, atomic reconï¬guration of a distributed system of
switches is hard to engineer. Network forwarding state needs
updating in response to changes in the traï¬ƒc demand or
network topology. Lacking WAN-wide atomic changes, the
network can drop many packets due to transient congestion
even if both the initial and ï¬nal conï¬gurations are uncon-
gested. Consider Figure 4 in which each ï¬‚ow is 1 unit and
each linkâ€™s capacity is 1.5 units. Suppose we want to change
the networkâ€™s forwarding state from Figure 4a to 4b, perhaps
to accommodate a new ï¬‚ow from R2 to R4. This change re-
quires changes to at least two switches. Depending on the
 0 0.2 0.4 0.6 0.8 1Normalizedtraï¬ƒc rateMeanPeakPeak-to-mean ratio = 2.17 0 0.2 0.4 0.6 0.8 1Normalizedtraï¬ƒc rateBackground traï¬ƒcNon-background traï¬ƒc 0 0.2 0.4 0.6 0.8 1Normalizedtraï¬ƒc rate>50% peak reductionPeak after adaptingPeak before adaptingğ‘…1 ğ‘…2 ğ‘…3 ğ‘…4 ğ‘…5 ğ‘…6 ğ‘…7 ğ¹ğ¶ ğ¹ğ´ ğ¹ğµ ğ‘…1 ğ‘…2 ğ‘…3 ğ‘…4 ğ‘…5 ğ‘…6 ğ‘…7 ğ¹ğ¶ ğ¹ğ´ ğ¹ğµ ğ‘†2 ğ‘†1 ğ·2 ğ·3 ğ‘†3 ğ·1 1/2 1/2 1/2 1/2 ğ‘†2 ğ‘†1 ğ·2 ğ·3 ğ‘†3 ğ·1 1/3 1/3 2/3 2/3 17(a)
(b)
(c)
(d)
(e)
(f )
Figure 5: Architecture of SWAN.
Figure 4: Illustration of congestion-free updates. Each
ï¬‚owâ€™s size is 1 unit and each linkâ€™s capacity is 1.5 units.
Changing from state (a) to (b) may lead to congested
states (c) or (d). A congestion-free update sequence is
(a);(e);(f );(b).
order in which the switch-level changes occur, the network
reaches the states in Figures 4c or 4d, which have a heavily
congested link and can signiï¬cantly hurt TCP ï¬‚ows as many
packets may be lost in a burst.
To avoid congestion during network updates, SWAN com-
putes a multi-step congestion-free transition plan. Each step
involves one or more changes to the state of one or more
switches, but irrespective of the order in which the changes
are applied, there will be no congestion. For the reconï¬gura-
tion in Figure 4, a possible congestion-free plan is: i) move
half of FA to the lower path (Figure 4e); ii) move FB to the
upper path (Figure 4f); and iii) move the remaining half of
FA to the lower path (Figure 4b).
A congestion-free plan may not always exist, and even if
it does, it may be hard to ï¬nd or involve a large number of
steps. SWAN leaves scratch capacity of s âˆˆ [0, 50%] on each
link, which guarantees that a transition plan exists with at
most (cid:100)1/s(cid:101)âˆ’1 steps (which is 9 if s=10%). We then develop
a method to ï¬nd a plan with the minimal number of steps.
In practice, it ï¬nds a plan with 1-3 steps when s=10%.
Further, instead of wasting scratch capacity, SWAN al-
locates it to background traï¬ƒc. Overall,
it guarantees
that non-background traï¬ƒc experiences no congestion dur-
ing transitions, and the congestion for background traï¬ƒc is
bounded (conï¬gurable).
Third, switch hardware supports a limited number of for-
warding rules, which makes it hard to fully use network ca-
pacity. For instance, if a switch has six distinct paths to
a destination but supports only four rules, a third of paths
cannot be used. Our analysis of a production inter-DC WAN
illustrates the challenge. If we use k-shortest paths between
each pair of switches (as in MPLS), fully using this networkâ€™s
capacity requires k=15. Installing these many tunnels needs
up to 20K rules at switches (Â§6.5), which is beyond the capa-
bilities of even next-generation SDN switches; the Broadcom
Trident2 chipset will support 16K OpenFlow rules [33]. The
current-generation switches in our testbed support 750 rules.
To fully exploit network capacity with a limited number of
rules, we are motivated by how the working set of a process
is often a lot smaller than the total memory it uses. Sim-
ilarly, not all tunnels are needed at all times. Instead, as
traï¬ƒc demand changes, diï¬€erent sets of tunnels are most
suitable. SWAN dynamically identiï¬es and installs these
tunnels. Our dynamic tunnel allocation method, which uses
an LP, is eï¬€ective because the number of non-zero variables
in a basic solution for any LP is fewer than the number of
constraints [25]. In our case, we will see that variables in-
clude the fraction of a DC-pairâ€™s traï¬ƒc that is carried over a
tunnel and the number of constraints is roughly the number
of priority classes times the number of DC pairs. Because
SWAN supports three priority classes, we obtain three tun-
nels with non-zero traï¬ƒc per DC pair on average, which is
much less than the 15 required for a non-dynamic solution.
Dynamically changing rules introduces another wrinkle for
network reconï¬guration. To not disrupt traï¬ƒc, new rules
must be added before the old rules are deleted; otherwise,
the traï¬ƒc that is using the to-be-deleted rules will be dis-
rupted. Doing so requires some rule capacity to be kept
vacant at switches to accommodate the new rules; done sim-
plistically, up to half of the rule capacity must be kept va-
cant [29], which is wasteful. SWAN sets aside a small amount
of scratch space (e.g., 10%) and uses a multi-stage approach
to change the set of rules in the network.
4. SWAN DESIGN
Figure 5 shows the architecture of SWAN. A logically
centralized controller orchestrates all activity. Each non-
interactive service has a broker that aggregates demands
from the hosts and apportions the allocated rate to them.
One or more network agents intermediate between the con-
troller and the switches. This architecture provides scaleâ€”
by providing parallelism where neededâ€”and choiceâ€”each
service can implement a rate allocation strategy that ï¬ts.
Service hosts and brokers collectively estimate the ser-
viceâ€™s current demand and limit it to the rate allocated by
the controller. Our current implementation draws on dis-
tributed rate limiting [5]. A shim in the host OS estimates
its demand to each remote DC for the next Th=10 seconds
and asks the broker for an allocation. It uses a token bucket
per remote DC to enforce the allocated rate and tags packets
with DSCP bits to indicate the serviceâ€™s priority class.
The service broker aggregates demand from hosts and up-
dates the controller every Ts=5 minutes. It apportions its
allocation from the controller piecemeal, in time units of Th,
to hosts in a proportionally fair manner. This way, Th is the
maximum time that a newly arriving host has to wait before
starting to transmit. It is also the maximum time a service
takes to change its sending rate to a new allocation. Brokers
that suddenly experience radically larger demands can ask
for more any time; the controller does a lightweight compu-
tation to determine how much of the additional demand can
be carried without altering network conï¬guration.
Network agents track topology and traï¬ƒc with the aid
of switches. They relay news about topology changes to
the controller right away and collect and report information
about traï¬ƒc, at the granularity of OpenFlow rules, every
Ta=5 minutes. They are also responsible for reliably up-
dating switch rules as requested by the controller. Before
returning success, an agent reads the relevant part of the
switch rule table to ensure that the changes have been suc-
cessfully applied.
ğ‘…1 ğ‘…4 ğ‘…3 ğ¹ğ´ ğ‘…2 ğ¹ğµ ğ‘…1 ğ‘…4 ğ‘…3 ğ¹ğ´ ğ¹ğµ ğ‘…2 ğ‘…1 ğ‘…4 ğ‘…3 ğ‘…2 ğ¹ğµ ğ¹ğ´ ğ‘…1 ğ‘…4 ğ‘…3 ğ¹ğ´ ğ‘…2 ğ¹ğµ ğ‘…1 ğ‘…4 ğ‘…3 ğ¹ğµ ğ‘…2 ğ¹ğ´:50% ğ‘…1 ğ‘…4 ğ‘…3 ğ‘…2 ğ¹ğ´:50% ğ¹ğµ Inter-DC WAN Datacenter SWAN controller Datacenter Service host Service broker Network agent Switch 18Controller uses the information on service demands and
network topology to do the following every Tc=5 minutes.
1. Compute the service allocations and forwarding plane
conï¬guration for the network (Â§4.1, Â§4.2).
2. Signal new allocations to services whose allocation has
decreased. Wait for Th seconds for the service to lower
its sending rate.
3. Change the forwarding state (Â§4.3) and then signal
the new allocations to services whose allocation has
increased.
4.1 Forwarding plane conï¬guration
SWAN uses label-based forwarding. Doing so reduces for-
warding complexity; the complex classiï¬cation that may be
required to assign a label to traï¬ƒc is done just once, at the
source switch. Remaining switches simply read the label and
forward the packet based on the rules for that label. We use
VLAN IDs as labels.
Ingress switches split traï¬ƒc across multiple tunnels (la-
bels). We propose to implement unequal splitting, which
leads to more eï¬ƒcient allocation [13], using group tables in
the OpenFlow pipeline. The ï¬rst table maps the packet,
based on its destination and other characteristics (e.g.,
DSCP bits), to a group table. Each group table consists
of the set of tunnels available and a weight assignment that
reï¬‚ects the ratio of traï¬ƒc to be sent to each tunnel. Con-
versations with switch vendors indicate that most will roll
out support for unequal splitting. When such support is un-
available, SWAN uses traï¬ƒc proï¬les to pick boundaries in
the range of IP addresses belonging to a DC such that split-
ting traï¬ƒc to that DC at these boundaries will lead to the
desired split. Then, SWAN conï¬gures rules at the source
switch to map IP destination spaces to tunnels. Our ex-
periments with traï¬ƒc from a production WAN show that
implementing unequal splits in this way leads to a small
amount of error (less than 2%).
4.2 Computing service allocations
When computing allocated rate for services, our goal is
to maximize network utilization subject to service priorities
and approximate max-min fairness among same-priority ser-
vices. The allocation process must be scalable enough to
handle WANs with 100s of switches.
Inputs: The allocation uses as input the service demands