a day on a busy link in IDN. Assuming capacity matches
peak usage (a common provisioning model to avoid conges-
tion), the average utilization on this link is under 50%. Thus,
half the provisioned capacity is wasted. This ineﬃciency is
not fundamental but can be remedied by exploiting traﬃc
characteristics. As a simple illustration, Figure 1b separates
background traﬃc. Figure 1c shows that the same total traf-
ﬁc can ﬁt in half the capacity if background traﬃc is adapted
to use capacity left unused by other traﬃc.
Second, the local, greedy resource allocation model of
MPLS TE is ineﬃcient. Consider Figure 2 in which each
link can carry at most one ﬂow. If the ﬂows arrive in the
order FA, FB, and FC , Figure 2a shows the path assignment
with MPLS TE: FA is assigned to the top path which is one
of the shortest paths; when FB arrives, it is assigned to the
shortest path with available capacity (CSPF); and the same
happens with FC . Figure 2b shows a more eﬃcient routing
pattern with shorter paths and many links freed up to carry
more traﬃc. Such an allocation requires non-local changes,
e.g., moving FA to the lower path when FB arrives.
Partial solutions for such ineﬃciency exist. Flows can
be split across two tunnels, which would divide FA across
the top and bottom paths, allowing half of FB and FC to
use direct paths; a preemption strategy that prefers shorter
paths can also help. But such strategies do not address the
fundamental problem of local allocation decisions [27].
Poor sharing:
Inter-DC WANs have limited support for
ﬂexible resource allocation. For instance, it is diﬃcult to
be fair across services or favor some services over certain
paths. When services compete today, they typically obtain
throughput proportional to their sending rate, an undesir-
able outcome (e.g., it creates perverse incentives for service
developers). Mapping each service onto its own queue at
routers can alleviate problems but the number of services
(100s) far exceeds the number of available router queues.
Even if we had inﬁnite queues and could ensure fairness on
the data plane, network-wide fairness is not possible without
(a) Link-level
(b) Network-wide
Figure 3: Link-level fairness (cid:54)= network-wide fairness.
controlling which ﬂows have access to which paths. Consider
Figure 3 in which each link has unit capacity and each ser-
vice (Si→Di) has unit demand. With link-level fairness,
S2→D2 gets twice the throughput of other services. As we
show, ﬂexible sharing can be implemented with a limited
number of queues by carefully allocating paths to traﬃc and
control the sending rate of services.
3. SWAN OVERVIEW AND CHALLENGES
Our goal is to carry more traﬃc and support ﬂexible
network-wide sharing. Driven by inter-DC traﬃc character-
istics, SWAN supports two types of sharing policies. First,
it supports a small number of priority classes (e.g., Inter-
active > Elastic > Background) and allocates bandwidth
in strict precedence across these classes, while preferring
shorter paths for higher classes. Second, within a class,
SWAN allocates bandwidth in a max-min fair manner.
SWAN has two basic components that address the funda-
mental shortcomings of the current practice. It coordinates
the network activity of services and uses centralized resource
allocation. Abstractly, it works as:
1. All services, except interactive ones, inform the SWAN
controller of their demand between pairs of DCs. In-
teractive traﬃc is sent like today, without permission
from the controller, so there is no delay.
2. The controller, which has an up-to-date, global view of
the network topology and traﬃc demands, computes
how much each service can send and the network paths
that can accommodate the traﬃc.
3. Per SDN paradigm, the controller directly updates the
forwarding state of the switches. We use OpenFlow
switches, though any switch that permits direct pro-
gramming of forwarding state (e.g., MPLS Explicit
Route Objects [3]) may be used.
While the architecture is conceptually simple, we must ad-
dress three challenges to realize this design. First, we need
a scalable algorithm for global allocation that maximizes
network utilization subject to constraints on service prior-
ity and fairness. Best known solutions are computationally
intensive as they solve long sequences of linear programs
(LP) [9, 26]. Instead, SWAN uses a more practical approach
that is approximately fair with provable bounds and close
to optimal in practical scenarios (§6).
Second, atomic reconﬁguration of a distributed system of
switches is hard to engineer. Network forwarding state needs
updating in response to changes in the traﬃc demand or
network topology. Lacking WAN-wide atomic changes, the
network can drop many packets due to transient congestion
even if both the initial and ﬁnal conﬁgurations are uncon-
gested. Consider Figure 4 in which each ﬂow is 1 unit and
each link’s capacity is 1.5 units. Suppose we want to change
the network’s forwarding state from Figure 4a to 4b, perhaps
to accommodate a new ﬂow from R2 to R4. This change re-
quires changes to at least two switches. Depending on the
 0 0.2 0.4 0.6 0.8 1Normalizedtraﬃc rateMeanPeakPeak-to-mean ratio = 2.17 0 0.2 0.4 0.6 0.8 1Normalizedtraﬃc rateBackground traﬃcNon-background traﬃc 0 0.2 0.4 0.6 0.8 1Normalizedtraﬃc rate>50% peak reductionPeak after adaptingPeak before adapting𝑅1 𝑅2 𝑅3 𝑅4 𝑅5 𝑅6 𝑅7 𝐹𝐶 𝐹𝐴 𝐹𝐵 𝑅1 𝑅2 𝑅3 𝑅4 𝑅5 𝑅6 𝑅7 𝐹𝐶 𝐹𝐴 𝐹𝐵 𝑆2 𝑆1 𝐷2 𝐷3 𝑆3 𝐷1 1/2 1/2 1/2 1/2 𝑆2 𝑆1 𝐷2 𝐷3 𝑆3 𝐷1 1/3 1/3 2/3 2/3 17(a)
(b)
(c)
(d)
(e)
(f )
Figure 5: Architecture of SWAN.
Figure 4: Illustration of congestion-free updates. Each
ﬂow’s size is 1 unit and each link’s capacity is 1.5 units.
Changing from state (a) to (b) may lead to congested
states (c) or (d). A congestion-free update sequence is
(a);(e);(f );(b).
order in which the switch-level changes occur, the network
reaches the states in Figures 4c or 4d, which have a heavily
congested link and can signiﬁcantly hurt TCP ﬂows as many
packets may be lost in a burst.
To avoid congestion during network updates, SWAN com-
putes a multi-step congestion-free transition plan. Each step
involves one or more changes to the state of one or more
switches, but irrespective of the order in which the changes
are applied, there will be no congestion. For the reconﬁgura-
tion in Figure 4, a possible congestion-free plan is: i) move
half of FA to the lower path (Figure 4e); ii) move FB to the
upper path (Figure 4f); and iii) move the remaining half of
FA to the lower path (Figure 4b).
A congestion-free plan may not always exist, and even if
it does, it may be hard to ﬁnd or involve a large number of
steps. SWAN leaves scratch capacity of s ∈ [0, 50%] on each
link, which guarantees that a transition plan exists with at
most (cid:100)1/s(cid:101)−1 steps (which is 9 if s=10%). We then develop
a method to ﬁnd a plan with the minimal number of steps.
In practice, it ﬁnds a plan with 1-3 steps when s=10%.
Further, instead of wasting scratch capacity, SWAN al-
locates it to background traﬃc. Overall,
it guarantees
that non-background traﬃc experiences no congestion dur-
ing transitions, and the congestion for background traﬃc is
bounded (conﬁgurable).
Third, switch hardware supports a limited number of for-
warding rules, which makes it hard to fully use network ca-
pacity. For instance, if a switch has six distinct paths to
a destination but supports only four rules, a third of paths
cannot be used. Our analysis of a production inter-DC WAN
illustrates the challenge. If we use k-shortest paths between
each pair of switches (as in MPLS), fully using this network’s
capacity requires k=15. Installing these many tunnels needs
up to 20K rules at switches (§6.5), which is beyond the capa-
bilities of even next-generation SDN switches; the Broadcom
Trident2 chipset will support 16K OpenFlow rules [33]. The
current-generation switches in our testbed support 750 rules.
To fully exploit network capacity with a limited number of
rules, we are motivated by how the working set of a process
is often a lot smaller than the total memory it uses. Sim-
ilarly, not all tunnels are needed at all times. Instead, as
traﬃc demand changes, diﬀerent sets of tunnels are most
suitable. SWAN dynamically identiﬁes and installs these
tunnels. Our dynamic tunnel allocation method, which uses
an LP, is eﬀective because the number of non-zero variables
in a basic solution for any LP is fewer than the number of
constraints [25]. In our case, we will see that variables in-
clude the fraction of a DC-pair’s traﬃc that is carried over a
tunnel and the number of constraints is roughly the number
of priority classes times the number of DC pairs. Because
SWAN supports three priority classes, we obtain three tun-
nels with non-zero traﬃc per DC pair on average, which is
much less than the 15 required for a non-dynamic solution.
Dynamically changing rules introduces another wrinkle for
network reconﬁguration. To not disrupt traﬃc, new rules
must be added before the old rules are deleted; otherwise,
the traﬃc that is using the to-be-deleted rules will be dis-
rupted. Doing so requires some rule capacity to be kept
vacant at switches to accommodate the new rules; done sim-
plistically, up to half of the rule capacity must be kept va-
cant [29], which is wasteful. SWAN sets aside a small amount
of scratch space (e.g., 10%) and uses a multi-stage approach
to change the set of rules in the network.
4. SWAN DESIGN
Figure 5 shows the architecture of SWAN. A logically
centralized controller orchestrates all activity. Each non-
interactive service has a broker that aggregates demands
from the hosts and apportions the allocated rate to them.
One or more network agents intermediate between the con-
troller and the switches. This architecture provides scale—
by providing parallelism where needed—and choice—each
service can implement a rate allocation strategy that ﬁts.
Service hosts and brokers collectively estimate the ser-
vice’s current demand and limit it to the rate allocated by
the controller. Our current implementation draws on dis-
tributed rate limiting [5]. A shim in the host OS estimates
its demand to each remote DC for the next Th=10 seconds
and asks the broker for an allocation. It uses a token bucket
per remote DC to enforce the allocated rate and tags packets
with DSCP bits to indicate the service’s priority class.
The service broker aggregates demand from hosts and up-
dates the controller every Ts=5 minutes. It apportions its
allocation from the controller piecemeal, in time units of Th,
to hosts in a proportionally fair manner. This way, Th is the
maximum time that a newly arriving host has to wait before
starting to transmit. It is also the maximum time a service
takes to change its sending rate to a new allocation. Brokers
that suddenly experience radically larger demands can ask
for more any time; the controller does a lightweight compu-
tation to determine how much of the additional demand can
be carried without altering network conﬁguration.
Network agents track topology and traﬃc with the aid
of switches. They relay news about topology changes to
the controller right away and collect and report information
about traﬃc, at the granularity of OpenFlow rules, every
Ta=5 minutes. They are also responsible for reliably up-
dating switch rules as requested by the controller. Before
returning success, an agent reads the relevant part of the
switch rule table to ensure that the changes have been suc-
cessfully applied.
𝑅1 𝑅4 𝑅3 𝐹𝐴 𝑅2 𝐹𝐵 𝑅1 𝑅4 𝑅3 𝐹𝐴 𝐹𝐵 𝑅2 𝑅1 𝑅4 𝑅3 𝑅2 𝐹𝐵 𝐹𝐴 𝑅1 𝑅4 𝑅3 𝐹𝐴 𝑅2 𝐹𝐵 𝑅1 𝑅4 𝑅3 𝐹𝐵 𝑅2 𝐹𝐴:50% 𝑅1 𝑅4 𝑅3 𝑅2 𝐹𝐴:50% 𝐹𝐵 Inter-DC WAN Datacenter SWAN controller Datacenter Service host Service broker Network agent Switch 18Controller uses the information on service demands and
network topology to do the following every Tc=5 minutes.
1. Compute the service allocations and forwarding plane
conﬁguration for the network (§4.1, §4.2).
2. Signal new allocations to services whose allocation has
decreased. Wait for Th seconds for the service to lower
its sending rate.
3. Change the forwarding state (§4.3) and then signal
the new allocations to services whose allocation has
increased.
4.1 Forwarding plane conﬁguration
SWAN uses label-based forwarding. Doing so reduces for-
warding complexity; the complex classiﬁcation that may be
required to assign a label to traﬃc is done just once, at the
source switch. Remaining switches simply read the label and
forward the packet based on the rules for that label. We use
VLAN IDs as labels.
Ingress switches split traﬃc across multiple tunnels (la-
bels). We propose to implement unequal splitting, which
leads to more eﬃcient allocation [13], using group tables in
the OpenFlow pipeline. The ﬁrst table maps the packet,
based on its destination and other characteristics (e.g.,
DSCP bits), to a group table. Each group table consists
of the set of tunnels available and a weight assignment that
reﬂects the ratio of traﬃc to be sent to each tunnel. Con-
versations with switch vendors indicate that most will roll
out support for unequal splitting. When such support is un-
available, SWAN uses traﬃc proﬁles to pick boundaries in
the range of IP addresses belonging to a DC such that split-
ting traﬃc to that DC at these boundaries will lead to the
desired split. Then, SWAN conﬁgures rules at the source
switch to map IP destination spaces to tunnels. Our ex-
periments with traﬃc from a production WAN show that
implementing unequal splits in this way leads to a small
amount of error (less than 2%).
4.2 Computing service allocations
When computing allocated rate for services, our goal is
to maximize network utilization subject to service priorities
and approximate max-min fairness among same-priority ser-
vices. The allocation process must be scalable enough to
handle WANs with 100s of switches.
Inputs: The allocation uses as input the service demands