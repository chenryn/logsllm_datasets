# 18 \| 如何用硬件同步原语（CAS）替代锁？你好，我是李玥。上节课，我们一起学习了如何使用锁来保护共享资源，你也了解到，使用锁是有一定性能损失的，并且，如果发生了过多的锁等待，将会非常影响程序的性能。在一些特定的情况下，我们可以使用硬件同步原语来替代锁，可以保证和锁一样的数据安全性，同时具有更好的性能。在今年的 NSDI（NSDI 是 USENIX组织开办的关于网络系统设计的著名学术会议）上，伯克利大学发表了一篇论文《[Confluo:Distributed Monitoring and Diagnosis Stack for High-speedNetworks](http://www.usenix.org/conference/nsdi19/presentation/khandelwal)》，这个论文中提到的Confluo，也是一个类似于消息队列的流数据存储，它的吞吐量号称是 Kafka 的4～10 倍。对于这个实验结论我个人不是很认同，因为它设计的实验条件对 Kafka来说不太公平。但不可否认的是，Confluo它的这个设计思路是一个创新，并且实际上它的性能也非常好。Confluo是如何做到这么高的吞吐量的呢？这里面非常重要的一个创新的设计就是，它使用硬件同步原语来代替锁，在一个日志上（你可以理解为消息队列中的一个队列或者分区），保证严格顺序的前提下，实现了多线程并发写入。今天，我们就来学习一下，如何用硬件同步原语（CAS）替代锁？``{=html}
## 什么是硬件同步原语？为什么硬件同步原语可以替代锁呢？要理解这个问题，你要首先知道硬件同步原语是什么。硬件同步原语（Atomic HardwarePrimitives）是由计算机硬件提供的一组原子操作，我们比较常用的原语主要是CAS 和 FAA 这两种。CAS（Compare and Swap），它的字面意思是：先比较，再交换。我们看一下 CAS实现的伪代码：    >function cas(p : pointer to int, old : int, new : int) returns bool {    if *p ≠ old {        return false    }    *p ← new    return true}它的输入参数一共有三个，分别是：-   p: 要修改的变量的指针。-   old: 旧值。-   new: 新值。返回的是一个布尔值，标识是否赋值成功。通过这个伪代码，你就可以看出 CAS原语的逻辑，非常简单，就是先比较一下变量 p 当前的值是不是等于old，如果等于，那就把变量 p 赋值为 new，并返回 true，否则就不改变变量p，并返回 false。这是 CAS 这个原语的语义，接下来我们看一下 FAA 原语（Fetch and Add）：    >function faa(p : pointer to int, inc : int) returns int {    int value `{=html}那如何在你的程序中使用压缩？应该选择什么样的压缩算法更适合我们的系统呢？这节课，我带你一起学习一下，使用数据压缩来提升系统性能的方法。
## 什么情况适合使用数据压缩？在使用压缩之前，首先你需要考虑，当前这个场景是不是真的适合使用数据压缩。比如，进程之间通过网络传输数据，这个数据是不是需要压缩呢？我和你一起来对比一下：-   不压缩直接传输需要的时间是： 传输**未压缩**数据的耗时。-   使用数据压缩需要的时间是： 压缩耗时 + 传输**压缩**数据耗时 +    解压耗时。到底是压缩快，还是不压缩快呢？其实不好说。影响的因素非常多，比如数据的压缩率、网络带宽、收发两端服务器的繁忙程度等等。压缩和解压的操作都是计算密集型的操作，非常耗费 CPU资源。如果你的应用处理业务逻辑就需要耗费大量的 CPU资源，就不太适合再进行压缩和解压。又比如说，如果你的系统的瓶颈是磁盘的 IO 性能，CPU资源又很闲，这种情况就非常适合在把数据写入磁盘前先进行压缩。但是，如果你的系统读写比严重不均衡，你还要考虑，每读一次数据就要解压一次是不是划算。**压缩它的本质是资源的置换，是一个时间换空间，或者说是 CPU资源换存储资源的游戏。**就像木桶的那个短板一样，每一个系统它都有一个性能瓶颈资源，可能是磁盘IO，网络带宽，也可能是CPU。如果使用压缩，能用长板来换一些短板，那总体上就能提升性能，这样就是划算的。如果用了压缩之后，短板更短了，那就不划算了，不如不用。如果通过权衡，使用数据压缩确实可以提升系统的性能，接下来就需要选择合适的压缩算法。
## 应该选择什么压缩算法？压缩算法可以分为有损压缩和无损压缩。有损压缩主要是用来压缩音视频，它压缩之后是会丢失信息的。我们这里讨论的全都是无损压缩，也就是说，数据经过压缩和解压过程之后，与压缩之前相比，是100% 相同的。数据为什么可以被压缩呢？各种各样的压缩算法又是怎么去压缩数据的呢？我举个例子来简单说明一下。比如说，下面这段数据：> 00000000000000000000我来给你人肉压缩一下：> 20 个 020 个字符就被压缩成了 4个字符，并且是可以无损还原的。当然，我举的例子比较极端，我的压缩算法也几乎没什么实用性，但是，这确实是一个压缩算法，并且和其他的压缩算法本质是没什么区别的。目前常用的压缩算法包括：ZIP，GZIP，SNAPPY，LZ4等等。选择压缩算法的时候，主要需要考虑数据的压缩率和压缩耗时。一般来说，压缩率越高的算法，压缩耗时也越高。如果是对性能要求高的系统，可以选择压缩速度快的算法，比如LZ4；如果需要更高的压缩比，可以考虑 GZIP 或者压缩率更高的 XZ 等算法。压缩样本对压缩速度和压缩比的影响也是比较大的，同样大小的一段数字和一段新闻的文本，即使是使用相同的压缩算法，压缩率和压缩时间的差异也是比较大的。所以，有的时候在选择压缩算法的之前，用系统的样例业务数据做一个测试，可以帮助你找到最合适的压缩算法。在这里，我不会去给你讲某一种压缩算法，因为压缩算法都很复杂，一般来说也不需要我们来实现某种压缩算法，如果你感兴趣的话，可以去学习一下最经典压缩算法：哈夫曼编码（也叫霍夫曼编码，HuffmanCoding）。
## 如何选择合适的压缩分段？大部分的压缩算法，他们的区别主要是，对数据进行编码的算法，压缩的流程和压缩包的结构大致一样的。而在压缩过程中，你最需要了解的就是如何选择合适的压缩分段大小。在压缩时，给定的被压缩数据它必须有确定的长度，或者说，是有头有尾的，不能是一个无限的数据流，**如果要对流数据进行压缩，那必须把流数据划分成多个帧，一帧一帧的分段压缩。**主要原因是，压缩算法在开始压缩之前，一般都需要对被压缩数据从头到尾进行一次扫描，扫描的目的是确定如何对数据进行划分和编码，一般的原则是重复次数多、占用空间大的内容，使用尽量短的编码，这样压缩率会更高。另外，被压缩的数据长度越大，重码率会更高，压缩比也就越高。这个很好理解，比如我们这篇文章，可能出现了几十次"压缩"这个词，如果将整篇文章压缩，这个词的重复率是几十次，但如果我们按照每个自然段来压缩，那每段中这个词的重复率只有二三次。显然全文压缩的压缩率肯定高于分段压缩。当然，分段也不是越大越好，实际上分段大小超过一定长度之后，再增加长度对压缩率的贡献就不太大了，这是一个原因。另外，过大的分段长度，在解压缩的时候，会有更多的解压浪费。比如，一个1MB大小的压缩文件，即使你只是需要读其中很短的几个字节，也不得不把整个文件全部解压缩，造成很大的解压浪费。所以，你需要根据你的业务，选择合适的压缩分段，在压缩率、压缩速度和解压浪费之间找到一个合适的平衡。确定了如何对数据进行划分和压缩算法之后，就可以进行压缩了，压缩的过程就是用编码来替换原始数据的过程。压缩之后的压缩包就是由这个编码字典和用编码替换之后的数据组成的。这就是数据压缩的过程。解压的时候，先读取编码字典，然后按照字典把压缩编码还原成原始的数据就可以了。
## Kafka 是如何处理消息压缩的？回过头来，我们再看一下 Kafka 它是如何来处理数据压缩的。首先，Kafka是否开启压缩，这是可以配置，它也支持配置使用哪一种压缩算法。原因我们在上面说过，不同的业务场景是否需要开启压缩，选择哪种压缩算法是不能一概而论的。所以，Kafka的设计者把这个选择权交给使用者。在开启压缩时，Kafka选择一批消息一起压缩，每一个批消息就是一个压缩分段。使用者也可以通过参数来控制每批消息的大小。我们之前讲过，在 Kafka中，生产者生成一个批消息发给服务端，在服务端中是不会拆分批消息的。那按照批来压缩，意味着，在服务端也不用对这批消息进行解压，可以整批直接存储，然后整批发送给消费者。最后，批消息由消费者进行解压。在服务端不用解压，就不会耗费服务端宝贵的 CPU资源，同时还能获得压缩后，占用传输带宽小，占用存储空间小的这些好处，这是一个非常聪明的设计。在使用 Kafka 时，如果生产者和消费者的 CPU资源不是特别吃紧，开启压缩后，可以节省网络带宽和服务端的存储空间，提升总体的吞吐量，一般都是个不错的选择。