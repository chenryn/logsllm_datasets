avoiding the overitting of machine learning models. Cross-validation
is used to validate whether the detection rates are consistent with
repeated training and testing [39]. If the detection rates luctuate
during cross-validation, we can infer that the machine learning mod-
els are not trained properly. We observe that previous works either
have no cross-validation or report no results from cross-validations.
The lack of proper cross-validation motivates us to further evaluate
the machine learning models using cross-validation. We use 3 times
standard deviation (3σ ) to quantify the luctuations in detection
rates. 3σ refers to 0.3% ∼ 99.7% of random instances distributed
within the range of 3σ . In the context of malware detection, a high
value of 3σ in detection rates means that the performance of the
model is not stable across diferent datasets.
5.2.1 Cross-validations for TTA1 Experiments. A common prac-
tice of cross-validation is using 10-fold cross-validation [39]. 10-
fold cross-validation divides the dataset into 10 subsets with equal
number of examples. It then performs training on 9 subsets and
testing on the remaining one, with each subset as a testing sub-
set. The standard deviations of detection rates in 10 experiments
show whether the detection rates of the model are stable across
10 experiments. We consider that either a split of 60 − 20 − 20
training-testing-validation or 10-fold cross-validation is not sui-
cient cross-validation, since the standard deviations of the detection
rates increase with more examples in the dataset. We repeated the
10-fold cross-validations until the standard deviations of detection
rates do not increase with more cross-validations. In this work,
we perform cross-validation 1,000 times (randomly shuling the
examples before each training-and-testing split), which is 3 orders
of magnitude more than previous works.
Figure 6 shows the distributions of detection rates (precision,
recall, and F1-scores) with both TTA1 and TTA2 for various ma-
chine learning models. In Figure 6, the red diamonds are the means,
and the blue boxes correspond to distributions of detection rates
(Precisions, Recalls, and F1-scores) lying between 25 and 75 per-
centiles. The whiskers (the short, horizontal lines outside the blue
box) represent the distributions of detection rates lying between
0.3% and 99.7%, which is equivalent to µ ± 3σ of a Gaussian Dis-
tribution. The blue dots are outliers that are outside the µ ± 3σ
regime. A wide spread of distributions in detection rates means that
the detection rates luctuate across diferent training datasets. Con-
versely, a narrow spread of distributions means that the detection
rates are stable across diferent training datasets. In DT, RF, KNN,
NN, AdaBoost, and Naive Bayes models, the mean of distributions
of F1-scores are 82.17%, 83.75%, 82.28%, 74%, 72.27%, 12.15%, with
3 standard deviations (3σ ) of 1.416%, 1.326%, 1.388%, 13.2%, 2.365%,
2.392%,
Decision
Tree
Naive
Bayes
Neural
Net
AdaBoost
Random
Forest
Nearest
Neighbors
100
80
60
40
20
]
%
[
e
g
a
t
n
e
c
r
e
P
Decision
Tree
Naive
Bayes
Neural
Net
AdaBoost
Random
Forest
Nearest
Neighbors
100
80
60
40
20
]
%
[
e
g
a
t
n
e
c
r
e
P
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
Prec
R ec F 1
C
U
A
(a)
(b)
Figure 6: Box plots of distributions of 10-fold cross-validation experiments using (a) TTA1 and (b) TTA2. Red diamonds are means, and blue
box corresponds to cross-validation experiment results that lie between 25 and 75 percentiles. The whiskers (the short, horizontal lines outside
the blue box) represent conidence interval equivalent to µ ± 3σ of a Gaussian Distribution. The blue dots are outliers that are outside the
µ ± 3σ regime. On the X-axis, Prec is precision, Rec is recall, and F1 is F1 score. AUC is area under curve in ROC. These 10-fold cross-validation
experiments show that we cannot achieve 100% malware detection accuracy.
5.2.2 Cross-validations for TTA2 Experiments. In DT, RF, KNN,
NN, AdaBoost, Naive Bayes models, the mean of distributions of
F1-scores using TTA2 are 82.13%, 83.61%, 82.2%, 73.69%, 73.43%,
12.21%, compared to 82.17%, 83.75%, 82.28%, 74%, 72.27%, 12.15%
using TTA1, respectively. In DT, RF, KNN, NN, AdaBoost, Naive
Bayes models, the mean of distributions of F1-scores using TTA2
are 2.145%, 2.336%, 2.248%, 14.88%, 3.29%, 2.611%, compared to
1.416%, 1.326%, 1.388%, 13.2%, 2.365%, 2.392% using TTA1, respec-
tively. Comparing the results using TTA1 and TTA2, the standard
deviations of DT, RF, KNN, NN, AdaBoost, Naive Bayes models
increased by 1.515×, 1.762×, 1.62×, 1.127×, 1.391×, 1.092×, respec-
tively. The overall detection rates using TTA2 have much higher
variations compared to ones using TTA1.
As previous works did not report standard deviations of their
cross-validations, we cannot compare these results. From the Fig-
ure 6, we can conclude that reporting results of one training-and-
testing experiment does not provide suicient information in per-
formance of machine learning models. We can only evaluate the
performance of these models by providing a distribution of detec-
tion rates.
The diference between standard deviations in Figure 6(a) and
Figure 6(b) is due to the unrealistic assumption that the programs
in the training set appear in the testing dataset. Figure 6(b) presents
the results where the malicious program is not included in the
training dataset. In conclusion, the mean of the distribution using
TTA2 is lower than that using TTA1, while the standard deviation
of distribution using TTA2 is higher than that using TTA1. In
order to have a full evaluation on the machine learning models, it
is imperative to use TTA2 and exhibit a distribution of precision,
recall, F1-score, and AUC of ROC curves.
5.3 Ransomware
In previous sections, the machine learning models are trained over
the traces of HPCs to discriminate malware from benignware. We
build a malware embedded in benignware and then show that this
malware can evade HPC-based malware detection.
We craft the malware simply by infusing Notepad++ with a
ransomware. Ransomware is malware that maliciously encrypts
iles and extorts users in exchange for the decryption keys [40].
By 2016, ransomware has become one of the most popular mal-
ware, as Kaspersky Security Bulletin 2016 has shown that at least
one business is attacked by ransomware every 40 seconds [41].
We implement our ransomware to encrypt iles when Notepad++
launches. The embedded ransomware traverses all the iles in the
łPicturesž folder and encrypts each ile every 5 seconds with Mi-
crosoft Cryptography APIs [42]. We measure the values of HPCs for
modiied Notepad++ in our experimental setup (ğ 3). We randomly
select 90% of the benignware and malware samples as the training
set, while we test on Notepad++ and modiied Notepad++. The
precision of DT, Naive Bayes, NN, AdaBoost, RF and KNN is 0%,
0%, 0%, 50.85%, 0%, and 0%, respectively.
These results are not surprising, as machine learning models
tolerate the noise and jitters during training on sampled HPCs,
in order to extract the malicious behavior in the programs. These
tolerance necessitates the machine learning algorithms to have
errors even with the training datasets. In our malware example, the
changes of HPC values caused by ransomware is overshadowed in
the sampled values of HPCs from running Notepad++. The vari-
ation tolerance results in classifying the modiied Notepad++ as
benignware.
6 DISCUSSION
We run Windows 7 32-bit operating system on AMD 15h family
Bulldozer micro-architecture machine. Weaver et al. performed
extensive studies investigating the determinism of the measured
HPC values in various micro-architectures [33]. By comparing the
HPC values across diferent micro-architectures, Weaver et al. show
that the HPCs in various architectures have similar levels of vari-
ations during sampling. Hence, our conclusions from Bulldozer
micro-architecture are applicable to other micro-architectures. In
our benignware and malware experiments, we chose to allow the
access to the network for benignware and prevent malware from
accessing network. This design choice does not afect the results
of HPC measurements, since benignware and malware both func-
tion properly during experiments. For the reduction of dimensions,
many other approaches can server the same purpose as PCA. We
use PCA in our designs as PCA is one of the most popular methods
for reduction of dimensions.
7 CONCLUSION
HPCs are hardware units that are designed to count low-level,
micro-architectural events. Many works have investigated malware
detection using HPC proiles. However, we believe that there is no
causation between low-level micro-architectural events and high-
level software behavior. The strong positive results in the previous
works are due to a series of optimistic assumptions and unrealistic
experimental setups. In this work, we rigorously evaluate the idea
of malware detection using HPCs through realistic assumptions
and experimental setups. We observe the low idelity in HPC-based
malware detection when we increase number of programs by a
factor of 2 ∼ 3 and the experiment numbers in cross-validation to
3 orders of magnitude higher than previous works. Our best result
shows an F1-score of 80.78%. The corresponding False Discovery
Rate (F+/(F+ + T+) is 15%. This means that among 1,323 executable
iles in the Windows operating system iles, 198 iles will be lagged
as malware. We also demonstrate the infeasibility in HPC-based
malware detection with Notepad++ infused with a ransomware,
which cannot be detected in our HPC-based malware detection
system.
REFERENCES
[1] Intel Itanium Architecture Software Developer’s Manual. Intel Corporation, 2010.
[2] BIOS and Kernel Developer’s Guide (BKDG) for AMD Family 15h Models 10h-1Fh
Processors. Advanced Micro Devices, Inc., 2015.
[3] John Demme, Matthew Maycock, Jared Schmitz, Adrian Tang, Adam Waksman,
Simha Sethumadhavan, and Salvatore Stolfo. On the feasibility of online mal-
ware detection with performance counters. In Proceedings of the 40th Annual
International Symposium on Computer Architecture (ISCA), page 559, 2013.
[4] Harish Patil, Robert Cohn, Mark Charney, Rajiv Kapoor, Andrew Sun, and Anand
Karunanidhi. Pinpointing representative portions of large intel itanium programs
with dynamic instrumentation. In Proceedings of 37th International Symposium
on Microarchitecture (MICRO), pages 81ś92, 2004.
[5] Mikhail Kazdagli, Vijay Janapa Reddi, and Mohit Tiwari. Quantifying and improv-
ing the eiciency of hardware-based mobile malware detectors. In Proceedings
of the 49th International Symposium on Microarchitecture (MICRO), pages 1ś13,
2016.
[6] Xueyang Wang, Sek Chai, Michael Isnardi, Sehoon Lim, and Ramesh Karri. Hard-
ware performance counter-based malware identiication and detection with adap-
tive compressive sensing. Transactions on Architecture and Code Optimization
(TACO), 2016.
[7] Adrian Tang, Simha Sethumadhavan, and Salvatore J Stolfo. Unsupervised
anomaly-based malware detection using hardware features.
In International
Workshop on Recent Advances in Intrusion Detection (RAID), pages 109ś129, 2014.
[8] Baljit Singh, Dmitry Evtyushkin, Jesse Elwell, Ryan Riley, and Iliano Cervesato.
On the detection of kernel-level rootkits using hardware performance counters.
In Proceedings of the 17th Asia Conference on Computer and Communications
Security (AsiaCCS), pages 483ś493. ACM, 2017.
[9] Meltem Ozsoy, Caleb Donovick, Iakov Gorelik, Nael Abu-Ghazaleh, and Dmitry
Ponomarev. Malware-aware processors: A framework for eicient online malware
detection. In Proceedings of the 21st International Symposium on High Performance
Computer Architecture (HPCA), pages 651ś661, 2015.
[10] Khaled N Khasawneh, Meltem Ozsoy, Caleb Donovick, Nael Abu-Ghazaleh, and
Dmitry Ponomarev. Ensemble learning for low-level hardware-supported mal-
ware detection. In International Workshop on Recent Advances in Intrusion Detec-
tion (RAID), pages 3ś25, 2015.
[11] Khaled N Khasawneh, Nael Abu-Ghazaleh, Dmitry Ponomarev, and Lei Yu. Rhmd:
evasion-resilient hardware malware detectors. In Proceedings of the 50th Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 315ś327,
2017.
[12] Chi-Keung Luk, Robert Cohn, Robert Muth, Harish Patil, Artur Klauser, Geof
Lowney, Steven Wallace, Vijay Janapa Reddi, and Kim Hazelwood. Pin: building
customized program analysis tools with dynamic instrumentation. In Acm sigplan
notices, volume 40, pages 190ś200. ACM, 2005. extras:luk05:pin.
[13] Fabrice Bellard. Qemu, a fast and portable dynamic translator. In USENIX Annual
Technical Conference, FREENIX Track, pages 41ś46, 2005.
[14] Nicholas Nethercote and Julian Seward. Valgrind: a framework for heavyweight
In ACM Sigplan notices, volume 42, pages
dynamic binary instrumentation.
89ś100. ACM, 2007.
[15] Dynamorio dynamic instrumentation tool platform. http://www.dynamorio.org/,
2017. (Accessed on 12/02/2017).
[16] Benjamin Serebrin and Daniel Hecht. Virtualizing performance counters. In
Proceedings of the European Conference on Parallel Processing, pages 223ś233,
Bordeaux, France, August 2011.
[17] John L Henning. Spec cpu2006 benchmark descriptions. ACM SIGARCH Computer
Architecture News, 34(4):1ś17, 2006.
[18] Linux perf. http://www.brendangregg.com/perf.html, 2017.
(Accessed on
11/19/2017).
[19] Dhilung Kirat, Giovanni Vigna, and Christopher Kruegel. Barecloud: Bare-metal
analysis-based evasive malware detection. In USENIX Security Symposium (SP),
pages 287ś301, 2014.
[20] Vincent M Weaver and Sally A McKee. Can hardware performance counters be
trusted? In Proceedings of International Symposium on Workload Characterization
(IISWC), pages 141ś150. IEEE, 2008.
[21] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press,
2016.
[22] Pivotal Software Inc. Rabbitmq. http://www.rabbitmq.com/, 2017. (Accessed on
11/12/2017).
[23] Samba - opening windows to a wider world. https://www.samba.org/, 2017.
(Accessed on 12/05/2017).
[24] bindfs. https://bindfs.org/, 2017. (Accessed on 12/05/2017).
[25] Paul J. Drongowski. An introduction to analysis and optimization with amd
codeanalyst performance analyzer, 2008.
[26] Virustotal. Virustotal. https://www.virustotal.com/, 2017.
(Accessed on
07/12/2017).
[27] Marcos Sebastián, Richard Rivera, Platon Kotzias, and Juan Caballero. Avclass: A
tool for massive malware labeling. In International Symposium on Research in
Attacks, Intrusions, and Defenses, pages 230ś253. Springer, 2016.
[28] Futuremark. https://www.futuremark.com/, 2017. (Accessed on 11/15/2017).
[29] Performance: Python package index. https://pypi.python.org/pypi/performance/
0.5.1, 2017. (Accessed on 11/30/2017).
[30] Ninite. https://ninite.com/, 2017. (Accessed on 11/15/2017).
[31] Npackd. https://npackd.appspot.com/, 2017. (Accessed on 11/15/2017).
[32] Android debug bridge. https://developer.android.com/studio/command-line/adb.
html, 2017. (Accessed on 11/12/2017).
[33] Vincent M Weaver, Dan Terpstra, and Shirley Moore. Non-determinism and
overcount on modern hardware performance counter implementations. In Pro-
ceedings of International Symposium on Performance Analysis of Systems and
Software (ISPASS), pages 215ś224. IEEE, 2013.
[34] S.S. Haykin. Communication System. Wiley Series in Management Series. John
Wiley & Sons, 1983.
[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Courna-
peau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research, 12:2825ś2830, 2011.
[36] Yoav Freund and Robert E Schapire. A desicion-theoretic generalization of on-line
learning and an application to boosting. In European conference on computational
learning theory, pages 23ś37. Springer, 1995.
[37] George H John. Robust decision trees: Removing outliers from databases. In
KDD, pages 174ś179, 1995.
[38] J Rennie, L Shih, J Teevan, and D Karger. Tackling the poor assumptions of naive
bayes classiiers (pdf). ICML, 2003.
[39] Ron Kohavi et al. A study of cross-validation and bootstrap for accuracy esti-
mation and model selection. In Ijcai, volume 14, pages 1137ś1145. Stanford, CA,
1995.
[40] Adam Young and Moti Yung. Cryptovirology: Extortion-based security threats
and countermeasures. In Proceedings of Security and Privacy, pages 129ś140.
IEEE, 1996.
[41] Kaspersky security bulletin 2016. review of the year. overall statistics for 2016.
https://securelist.com/kaspersky-security-bulletin-2016-executive-summary/
76858/. (Accessed on 12/10/2017).
[42] Cryptography reference (windows). https://msdn.microsoft.com/en-us/library/
aa380256.aspx, 2017. (Accessed on 11/18/2017).