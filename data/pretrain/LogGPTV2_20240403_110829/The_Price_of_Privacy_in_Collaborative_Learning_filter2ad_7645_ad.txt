1M
NF10
Rating
998 539
10 033 823
User
6040
46 462
Item
3260
16 395
Density
0.051
0.013
Table 3. The Datasets Size after Preprocessing
The algorithm for MF is SGD, where the number of
features are κ = 4. The algorithm runs for 20 iterations
(ι = 20) with learning rate γ = 0.0075 and regularization
parameter λ = 0.01. The feature matrices are bounded
by pmax = qmax = 0.5. This means that the sensitivity
of the RecSys scenario is S ≤ 4 · 20 · 0.0075 · (2 · 2 · 0.5 −
0.01 · 0.5) = 1.197 as a result of Th. 5.
We assume if a model
is trained using datasets
from very diﬀerent distributions, the model captures the
properties of the mixed distribution of the combined
dataset (which might be far from the original distri-
butions). On the other hand, using training data from
similar distributions results in capturing the statistical
properties of a distribution close to the original ones.
As such, if the players’ datasets are from a similar dif-
ferent distribution, training together likely results in a
more accurate model than training alone. Consequently,
we imitate the players’ datasets by splitting 1M and
NF10: each user with its corresponding ratings is as-
signed to one of the players. To remove the eﬀect of
randomness of the dataset division, we run our experi-
ments three times and only present the averages. Now,
each player splits its dataset further into a training set
(80%) and a veriﬁcation set (20%). The players can run
the SGD algorithm alone or together, where additional
privacy mechanisms can be deployed. The accuracy of
the trained model is measured via root mean square er-
ror: RM SE =
rP e2
.
iu
|R|
6.1 Alone vs Together
First, we compare the achieved accuracy with and with-
out the other player’s data in Fig. 4. The horizontal
axis represents the ratio of the user-set sizes: how 1M
and NF10 was split into two. More precisely, x = α
β
β times the size
represents that Player 1’s dataset is α
of Player 2’s dataset. The vertical axis shows the nor-
malized accuracy diﬀerence between training alone and
together: y = θn−ΦM
. In other words, y is the accuracy
improvement via training together; y = 0 represents the
accuracy of training alone.
θn
n
Together or Alone: The Price of Privacy in Collaborative Learning
10
and lower (y  0)
Together or Alone: The Price of Privacy in Collaborative Learning
11
n
θn
ence θn−ΦM
shown in Fig. 6. We use Mathematica’s4
built-in Interpolate function with InterpolationOrder → 1
setting in order to have a monotone approximation
which is required by Def. 3. Via this interpolation the
exact NE can be determined for the speciﬁc dataset and
algorithm deﬁned in Sec. 5. In the rest of this section
we calculate the precise NE when 1M dataset is split
equally between the players.
7.2 One Player is Privacy Concerned
In this CaaS scenario we assume Player 1 is privacy
unconcerned. Due to Th. 2, this player’s BR is ˆp1 = 0.
Now the utility function of Player 2 is:
z
}|
(cid:20) θ2 − Φ(0, p2)
b(0,p2)
{
(cid:21)+
θ2
u2(0, p2) = B2 ·
−C2 · c(p2)
(11)
As Lemma 1 and 2 states, there is a lower and upper
bound on C2
B2 for Player 2 which ensures that the BR ˆp2
is either 0 or 1. We determine the exact bounds using
our interpolation. Furthermore the utility of Player 2
(Eq. (11)) has to be positive as it is stated in Th. 3,
otherwise there is no incentive for Player 2 to partici-
pate in the CoL process. These limits are visible Tab. 4
where the right side shows the bounds corresponding to
Lemma 1 and 2 for both privacy method while the left
side corresponds to the non-negativity condition on the
utility of Player 2. These bounds on C2
B2 are also visible
in Fig. 7 where B2 = 1.
C2
0 ≤ u2(0, ˆp2) if
B2 ≤ 0.990
B2 ≤ 1.150
C2
ˆp2
0
1
0
1
Sup
bDP
if
C2
C2
B2 ≤ 1.400
B2 ≥ 1.827
B2 ≤ 0.349
B2 ≥ 2.251
C2
C2
Table 4. NEs for Player 2 when Player 1 is privacy unconcerned
In Fig. 7 we display the BR with its corresponding
utility when the utility function is normalized by B2
(i.e., B2 = 1) as in the proof of Th. 4. This transforma-
tion keeps the sign, i.e., if the utility negative, the BR is
not a NE, since no collaboration corresponds to higher
utility.
As it is visible, in case of Sup the interval deﬁned
by the two lemmas (represented by the two vertical thin
gray line) are corresponding to negative utility, so for
this privacy mechanisms the NE is either collaboration
4 https://www.wolfram.com/mathematica/
Fig. 6. Accuracy improvement of collaboration (when 1M was
split equally) where 0 represents the accuracy level of training
alone. The applied mechanisms are Sup and bDP, respectively
7 Theory meets Practice
n from Sec. 6, it is pos-
Using the obtained values of ΦM
sible to interpolate the privacy-accuracy trade-oﬀ func-
tion and determine numerical NEs. In this section, we
n for the RecSys scenario using the empir-
interpolate ΦM
ical results from Sec. 6 and combine it with the results
in Sec. 4 to obtain exact equilibria.
7.1 Interpolation via Experiments
As an example, we set b and c are deﬁned linear as in Eq.
(4). Now, we interpolate directly b instead of ΦM
n , i.e.,
we interpolate the percentage-wise improvement diﬀer-
Together or Alone: The Price of Privacy in Collaborative Learning
12
1, p∗
1, p∗
2) = (0, 0) if C2
2) = (1, 1) otherwise.
without privacy protection or no collaboration, depend-
ing on the weight ratio. More precisely, according to
B2 ≤ b(θ2, Φ2(0, 0)) =
Th. 3 the NE is (p∗
0.990 and (p∗
In case of bDP, some part of the interval created
by the lemmas corresponds to positive utility, i.e., there
exists a non-trivial NE. More precisely, if 0.349 ≤ C2
B2 ≤
1.150 then p∗
2 is neither 0 nor 1. Note, that the BR
function is step-like because of the piecewise linear in-
terpolation. As such, within this interval the NE is
2 = 0.2 ⇔ ε∗
p∗
2 = 4. The Prive of Privacy of this NE
is P oP(0, 0.2) = 0.066, so due to privacy concerns less
than 7% of the overall achievable accuracy is lost.
In BR dynamics the players update their strategies in
the next round based on the their BRs to the strategy
what the other player played last round. We start the
iteration from (p1, p2) = (0, 0)6 and update the players’s
strategies alternately starting with Player 1. The NEs
where the process converged are visible in Tab. 5 with
the corresponding Price of Privacy values for discrete
weight ratios {0, 0.1, . . .}.
C2
C1
B2 ∈ →
B1 ∈ [0, 0.3]
B1 ∈ [0.4, 0.9]
B1 ∈ [1, ∞]
C1
C1
[0, 0.3]
(0, 0)
0.000
(0.2, 0)
0.066
(1, 1)
1.000
[0.4, 0.9]
(0, 0.2)
0.066
(0.2, 0.2)
0.131
(1, 1)
1.000
[1, ∞]
(1, 1)
1.000
(1, 1)
1.000
(1, 1)
1.000
Table 5. NEs for diﬀerent weight ratios
These approximated result suggests that players
with low privacy weight prefers to train together with-
out any protection while high privacy weight ensures
no collaboration. Furthermore, the narrow interval in-
between corresponds to collaboration with very limited
privacy protection (e.g., ε = 4) or no collaboration at
all.
8 Approximating Φ in Practice
Direct interpolation is only possible when both datasets
are fully available. In a real-world scenario ΦM
n must be
approximated by other means. In this section, besides a
short overview of the whole process before collaboration,
we demonstrate a simple approach to ﬁll the gap in the
CoL game caused by the obscurity of ΦM
n .
Note, that our intention is not to provide a sound
method to approximate the eﬀect of privacy mechanisms
on the accuracy of complex training algorithms, but
rather to show a direction how it could be done. More
research is required in this direction, and we consider
this to be an interesting future work.
8.1 Heuristic Parameters
As we argued in Sec. 6, we can assume that the player’s
datasets are from similar distributions, i.e., the players
can imitate CoL by mimicking the player’s datasets via
splitting their own datasets into two and approximate
n locally.
ΦM
Fig. 7. BR and the corresponding utility for Player 2 when Player
1 is privacy unconcerned
7.3 Both Player are Privacy Concerned
In this section we will focus on bDP privacy mechanism.
Th. 4 states that when both player is privacy concerned
a non-trivial NE exists. To ﬁnd it, we will use BR dy-
namics [HS+88]. This will eventually converge to a NE
due to the following theorem:
Theorem (Monderer & Shapley [MS96]). In a ﬁnite5
potential game, from an arbitrary initial outcome, the
BR dynamics converges to a pure strategy NE.
5 As the CoL game is not ﬁnite, we discretized the actions