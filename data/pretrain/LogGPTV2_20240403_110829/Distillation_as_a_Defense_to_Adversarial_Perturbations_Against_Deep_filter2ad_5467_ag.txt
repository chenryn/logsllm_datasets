### Figure 10: Quantifying the Impact of Distillation Temperature on Robustness

**Robustness Metric and Baseline Comparison:**
In Figure 10, we quantify the impact of distillation temperature on the robustness of our models. We plot the robustness metric, as defined in Equation 11, for several temperatures and compare it to a baseline robustness value for models trained without distillation. The robustness metric is evaluated to determine whether distillation effectively increases this metric for our evaluation architectures.

**Approximation of the Robustness Metric:**
To avoid exhaustively searching all possible perturbations for each sample, we approximate the robustness metric by computing it over all 10,000 samples in the test set for each model. This results in the following quantity:
\[
\rho_{\text{adv}}(F) = \frac{1}{|X|} \sum_{X \in X} \min_{\delta_X} \|\delta_X\|
\]
where \(\|\delta_X\|\) is the minimum perturbation required to create an adversarial sample for \(X \in X\). The perturbation \(\delta_X\) is evaluated by considering each of the 9 possible adversarial targets corresponding to sample \(X\), and using the number of features altered as the distance measure.

**Results and Observations:**
Figure 10 shows the evolution of the robustness metric with respect to an increase in distillation temperature for both architectures. As the temperature increases, the robustness of the network, as defined here, also increases. For the MNIST architecture, the model trained without distillation has a robustness of 1.55%, while the model trained with distillation at \(T = 20\) has a robustness of 13.79%, representing a 790% increase.

**Interpretation of Results:**
Perturbations of 13.79% are large enough that they could potentially change the true class or be detected by an anomaly detection process. Previous work [7] has shown that humans begin to misclassify or identify adversarial samples as erroneous for perturbations larger than 13.79% (see Figure 16 in [7]). It is not desirable for adversaries to produce adversarial samples that are identifiable by humans. Additionally, changing additional features can be challenging depending on the input nature. In this evaluation, it is relatively easy to change features in images, but for inputs like spam emails, it would be more difficult.

**Soft Labels vs. Probabilities:**
We also explored whether the probabilities used to transfer knowledge in this paper could be replaced by soft class labels. For an N-class classification problem, soft labels are obtained by replacing the target value of 1 for the correct class with a target value of 0.9, and for the incorrect classes, replacing the target of 0 with \(\frac{1}{N-1}\). Empirically, we observed that the improvements to the neural network’s robustness are not as significant with soft labels. Specifically, the misclassification rate of adversarial samples for the MNIST DNN trained with soft labels was 86.00%, whereas the distilled model had a misclassification rate of less than 1%. We believe this is due to the relative information between classes encoded in probability vectors, which is not present in soft class labels.

**Label Smoothing and Adversarial Examples:**
Inspired by an early public preprint of this paper, Warde-Farley and Goodfellow [38] independently tested label smoothing and found that it partially resists adversarial examples crafted using the fast gradient sign method [9]. One possible interpretation of these conflicting results is that label smoothing without distillation is effective against simple, inexpensive methods of adversarial example crafting but not against more powerful iterative methods used in this paper [7].

**Future Work:**
Future work should investigate the use of various distance measures, such as L1 and L2 norms, and evaluate the performance of defensive distillation against different types of perturbations. While defensive distillation is a good defense against the attack studied here [7], it may still be vulnerable to other attacks based on L-BFGS [8], the fast gradient sign method [9], or genetic algorithms [32]. Preliminary results from [38] are promising and suggest that distillation may also have a beneficial defensive impact with such techniques.

**Comparison to Traditional Regularization Techniques:**
We did not compare our defense technique to traditional regularization techniques because adversarial examples are not a traditional overfitting problem [9]. Previous work has shown that a wide variety of traditional regularization methods, including dropout and weight decay, either fail to defend against adversarial examples or do so by seriously harming accuracy on the original task [8], [9].

**Conclusion:**
Defensive distillation does not create additional attack vectors and does not start an arms race between defenders and attackers. By increasing confidence estimates across a lot of the model’s input space, defensive distillation should lead to strictly better models. Future work should extend this approach to other deep learning tasks and explore different definitions of robustness that measure other aspects of DNN resilience to adversarial perturbations.

### Acknowledgment
The authors would like to thank Damien Octeau, Ian Goodfellow, and Ulfar Erlingsson for their insightful comments. Research was sponsored by the Army Research Laboratory under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.

### References
[References listed as provided in the original text.]

---

This optimized version aims to make the text more clear, coherent, and professional, while maintaining the technical details and key points.