Fig. 10: Quantifying the impact of distillation temperature
on robustness: we plot the value of robustness described in
Equation 11 for several temperatures and compare it to a base-
line robustness value for models trained without distillation.
evaluate whether distillation effectively increases this robust-
ness metric for our evaluation architectures. To do this without
exhaustively searching all perturbations for each possible
sample of the underlying distribution modeled by the DNN,
we approximate the metric: we compute the metric over all
10,000 samples in the test set for each model. This results in
the computation of the following quantity:
ρadv(F ) (cid:10) 1|X|
(cid:5)δX(cid:5)
(11)
(cid:5)
min
δX
X∈X
where values of δX are evaluated by considering each of the 9
possible adversarial targets corresponding to sample X ∈ X .
and using the number of features altered while creating the
corresponding adversarial samples as the distance measure to
evaluate the minimum perturbation (cid:5)δX(cid:5) required to create
the mentionned adversarial sample. In Figure 10, we plot the
evolution of the robustness metric with respect to an increase
in distillation temperature for both architectures. One can see
that as the temperature increases, the robustness of the network
as deﬁned here, increases. For the MNIST architecture, the
model trained without distillation displays a robustness of
1.55% whereas the model trained with distillation at T = 20
displays a robustness of 13.79%, an increase of 790%. Note
that, perturbations of 13.79% are large enough that they poten-
tially change the true class or could be detected by an anomaly
detection process. In fact, it was empirically shown in previous
work that humans begin to misclassify adversarial samples
(or at
identify them as erroneous) for perturbations
larger than 13.79%: see Figure 16 in [7]. It is not desirable
for adversary to produce adversarial samples identiﬁable by
humans. Furthermore, changing additional features can be
hard, depending on the input nature. In this evaluation, it
is easy to change a feature in the images. However, if the
input was spam email, it would become challenging for the
least
595595
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
1
One question is also whether the probabilities, used to
transfer knowledge in this paper, could be replaced by soft
class labels. For a N-class classiﬁcation problem, soft labels
are obtained by replacing the target value of 1 for the correct
class with a target value of 0.9, and for the incorrect classes
10·N . We empirically observed
replacing the target of 0 with
that the improvements to the neural network’s robustness are
not as signiﬁcant with soft labels. Speciﬁcally, we trained
the MNIST DNN used in Section V using soft labels. The
misclassiﬁcation rate of adversarial samples, crafted using
MNIST test data and the same attack parameters than in
Section V, was of 86.00%, whereas the distilled model studied
in Section V had a misclassiﬁcation rate smaller than 1% We
believe this is due to the relative information between classes
encoded in probability vectors and not in soft class labels.
Inspired by an early public preprint of this paper, Warde-Farley
and Goodfellow [38] independently tested label smoothing,
and found that it partially resists adversarial examples crafted
using the fast gradient sign method [9]. One possible inter-
pretation of these conﬂicting results is that label smoothing
without distillation is smart enough to defend against simple,
inexpensive methods [9] of adversarial example crafting but
not more powerful iterative methods used in this paper [7].
In our evaluation setup, we deﬁned the distance measure
between original samples and adversarial samples as the
number of modiﬁed features. There are other metrics suit-
able to compare samples, like L1, L2 norms. Using different
metrics will produce different distortions and can be pertinent
in application domains different from computer vision. For
instance, crafting adversarial samples from real malware to
evade existing detection methods will require different metrics
and perturbations [16], [37]. Future work should investigate
the use of various distance measures.
Future work should also evaluate the performance of de-
fensive distillation in the face of different perturbation types.
For instance, while defensive distillation is a good defense
against the attack studied here [7], it could still be vulnerable
to other attacks based on L-BFGS [8], the fast gradient sign
method [9], or genetic algorithms [32]. However, against such
techniques, the preliminary results from [38] are promising
and worthy of exploration; it seems likely that distillation will
also have a beneﬁcial defensive impact with such techniques.
In this paper, we did not compare our defense technique
to traditional regularization techniques because adversarial
examples are not a traditional overﬁtting problem [9]. In
fact, previous work showed that a wide variety of traditional
regularization methods including dropout and weight decay
either fail to defend against adversarial examples or only do
so by seriously harming accuracy on the original task [8], [9].
Finally, we would like to point out that defensive distillation
does not create additional attack vectors, in other words does
not start an arms race between defenders and attackers. Indeed,
the attacks [8], [9], [7] are designed to be approximately
optimal regardless of the targeted model. Even if an attacker
knows that defensive distillation is being used, it is not clear
how he could exploit this to adapt its attack. By increasing
conﬁdence estimates across a lot of the model’s input space,
defensive distillation should lead to strictly better models.
VII. RELATED WORK
Machine learning security [39] is an active research area in
the security community [40]. Attacks have been organized in
taxonomies according to adversarial capabilities in [12], [41].
Biggio et al. studied binary classiﬁers deployed in adversarial
settings and proposed a framework to secure them [42]. Their
work does not consider deep learning models but rather binary
classiﬁers like Support Vector Machines or logistic regression.
More generally, attacks against machine learning models can
be partitioned by execution time: during training [43], [44] or
at test time [14] when the model is used to make predictions.
Previous work studying DNNs in adversarial settings fo-
cused on presenting novel attacks against DNNs at test time,
mainly exploiting vulnerabilities to adversarial samples [7],
[9], [8]. These attacks were discussed in depth in section II.
These papers offered suggestions for defenses but their inves-
tigation was left to future work by all authors, whereas we
proposed and evaluated a full defense mechanism to improve
the resilience of DNNs to adversarial perturbations.
Nevertheless some attempts were made at making DNN
resilient to adversarial perturbations. Goodfellow et al. showed
that radial basis activation functions are more resistant to
perturbations, but deploying them requires important modi-
ﬁcations to the existing architecture [9]. Gu et al. explored the
use of denoising auto-encoders, a DNN type of architecture
intended to capture main factors of variation in the data, and
showed that they can remove substantial amounts of adver-
sarial noise [17]. However the resulting stacked architecture
can again be evaded using adversarial samples. The authors
therefore proposed a new architecture, Deep Contractive Net-
works, based on imposing layer-wise penalty deﬁned using the
network’s Jacobian. This penalty however limits the capacity
of Deep Contractive Networks compared to traditional DNNs.
VIII. CONCLUSIONS
In this work we have investigated the use of distillation, a
technique previously used to reduce DNN dimensionality, as a
defense against adversarial perturbations. We formally deﬁned
defensive distillation and evaluated it on standard DNN archi-
tectures. Using elements of learning theory, we analytically
showed how distillation impacts models learned by deep neural
network architectures during training. Our empirical ﬁndings
show that defensive distillation can signiﬁcantly reduce the
successfulness of attacks against DNNs. It reduces the success
of adversarial sample crafting to rates smaller than 0.5% on the
MNIST dataset and smaller than 5% on the CIFAR10 dataset
while maintaining the accuracy rates of the original DNNs.
Surprisingly, distillation is simple to implement and introduces
very little overhead during training. Hence, this work lays out
a new foundation for securing systems based on deep learning.
Future work should investigate the impact of distillation on
other DNN models and adversarial sample crafting algorithms.
One notable endeavor is to extend this approach outside of the
scope of classiﬁcation to other DL tasks. This is not trivial as
it requires ﬁnding a substitute for probability vectors used in
defensive distillation with similar properties. Lastly, we will
explore different deﬁnitions of robustness that measure other
aspects of DNN resilience to adversarial perturbations.
596596
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
ACKNOWLEDGMENT
The authors would like to thank Damien Octeau, Ian Good-
fellow, and Ulfar Erlingsson for their insightful comments.
Research was sponsored by the Army Research Laboratory
and was accomplished under Cooperative Agreement Number
W911NF-13-2-0045 (ARL Cyber Security CRA). The views
and conclusions contained in this document are those of the au-
thors and should not be interpreted as representing the ofﬁcial
policies, either expressed or implied, of the Army Research
Laboratory or the U.S. Government. The U.S. Government is
authorized to reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation here on.
REFERENCES
[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in Advances in neural infor-
mation processing systems, 2012, pp. 1097–1105.
[2] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran,
“Deep convolutional neural networks for lvcsr,” in Acoustics, Speech
and Signal Processing (ICASSP), 2013 IEEE International Conference
on.
IEEE, 2013, pp. 8614–8618.
[3] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-
Cun, “Overfeat: Integrated recognition, localization and detection using
convolutional networks,” in International Conference on Learning Rep-
resentations (ICLR 2014).
arXiv preprint arXiv:1312.6229, 2014.
[4] G. E. Dahl, J. W. Stokes, L. Deng, and D. Yu, “Large-scale malware
classiﬁcation using random projections and neural networks,” in Acous-
tics, Speech and Signal Processing (ICASSP), 2013 IEEE International
Conference on.
IEEE, 2013, pp. 3422–3426.
[5] Z. Yuan, Y. Lu, Z. Wang, and Y. Xue, “Droid-sec: deep learning in
android malware detection,” in Proceedings of the 2014 ACM conference
on SIGCOMM. ACM, 2014, pp. 371–372.
[6] E.
“How
Knorr,
guys
with
Avail-
able: http://www.infoworld.com/article/2907877/machine-learning/how-
paypal-reduces-fraud-with-machine-learning.html
the
[Online].
beats
2015.
learning,”
machine
paypal
bad
[7] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,”
in Proceedings of the 1st IEEE European Symposium on Security and
Privacy.
IEEE, 2016.
[8] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” in Proceedings
the 2014 International Conference on Learning Representations.
of
Computational and Biological Learning Society, 2014.
[9] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” in Proceedings of the 2015 International Con-
ference on Learning Representations. Computational and Biological
Learning Society, 2015.
[10] NVIDIA, “Nvidia tegra drive px: Self-driving car computer,” 2015.
[Online]. Available: http://www.nvidia.com/object/drive-px.html
[11] D. Cires¸an, U. Meier, J. Masci et al., “Multi-column deep neural network
for trafﬁc sign classiﬁcation.”
test
[12] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar, “Ad-
versarial machine learning,” in Proceedings of the 4th ACM workshop
on Security and artiﬁcial intelligence. ACM, 2011, pp. 43–58.
[13] B. Biggio, G. Fumera et al., “Pattern recognition systems under attack:
Design issues and research challenges,” International Journal of Pattern
Recognition and Artiﬁcial Intelligence, vol. 28, no. 07, p. 1460002, 2014.
[14] B. Biggio, I. Corona, D. Maiorca, B. Nelson et al., “Evasion attacks
time,” in Machine Learning and
against machine learning at
Knowledge Discovery in Databases. Springer, 2013, pp. 387–402.
[15] A. Anjos and S. Marcel, “Counter-measures to photo attacks in face
recognition: a public database and a baseline,” in Proceedings of the
2011 International Joint Conference on Biometrics.
IEEE, 2011.
[16] P. Fogla and W. Lee, “Evading network anomaly detection systems:
formal reasoning and practical techniques,” in Proceedings of the 13th
ACM conference on Computer and communications security. ACM,
2006, pp. 59–68.
[17] S. Gu and L. Rigazio, “Towards deep neural network architectures
robust to adversarial examples,” in Proceedings of the 2015 International
Conference on Learning Representations. Computational and Biological
Learning Society, 2015.
[18] J. Ba and R. Caruana, “Do deep nets really need to be deep?” in
Advances in Neural Information Processing Systems, 2014, pp. 2654–
2662.
[19] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” in Deep Learning and Representation Learning Workshop at
NIPS 2014.
arXiv preprint arXiv:1503.02531, 2014.
[20] Y. LeCun and C. Cortes, “The mnist database of handwritten digits,”
[21] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
1998.
tiny images,” 2009.
[22] Y. Bengio,
I. J. Goodfellow, and A. Courville, “Deep learning,”
[Online]. Available:
2015, book in preparation for MIT Press.
http://www.iro.umontreal.ca/˜bengioy/dlbook
[23] G. E. Hinton, “Learning multiple layers of representation,” Trends in
cognitive sciences, vol. 11, no. 10, pp. 428–434, 2007.
[24] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning represen-
tations by back-propagating errors,” Cognitive modeling, vol. 5, 1988.
[25] J. Bergstra and Y. Bengio, “Random search for hyper-parameter opti-
mization,” The Journal of Machine Learning Research, vol. 13, no. 1,
pp. 281–305, 2012.
[26] X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-
scale sentiment classiﬁcation: A deep learning approach,” in Proceedings
of the 28th International Conference on Machine Learning (ICML-11),
2011, pp. 513–520.
[27] J. Masci, U. Meier, D. Cires¸an et al., “Stacked convolutional auto-
encoders for hierarchical feature extraction,” in Artiﬁcial Neural Net-
works and Machine Learning–ICANN 2011. Springer, 2011, pp. 52–59.
[28] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and
S. Bengio, “Why does unsupervised pre-training help deep learning?”
The Journal of Machine Learning Research, vol. 11, pp. 625–660, 2010.
[29] T. Miyato, S. Maeda, M. Koyama et al., “Distributional smoothing by
virtual adversarial examples,” CoRR, vol. abs/1507.00677, 2015.
[30] A. Fawzi, O. Fawzi, and P. Frossard, “Analysis of classiﬁers’ robustness
to adversarial perturbations,” in Deep Learning Workshop at ICML 2015.
arXiv preprint arXiv:1502.02590, 2015.
[31] H. Drucker and Y. Le Cun, “Improving generalization performance
using double backpropagation,” Neural Networks, IEEE Transactions
on, vol. 3, no. 6, pp. 991–997, 1992.
[32] A. Nguyen, J. Yosinski, and J. Clune, “Deep neural networks are easily
fooled: High conﬁdence predictions for unrecognizable images,” in In
Computer Vision and Pattern Recognition (CVPR 2015).
IEEE, 2015.
[33] G. Cybenko, “Approximation by superpositions of a sigmoidal function,”
Mathematics of Control, Signals, and Systems , vol. 5, no. 4, p. 455,
1992.
[34] S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan, “Learn-
ability, stability and uniform convergence,” The Journal of Machine
Learning Research, vol. 11, pp. 2635–2670, 2010.
[35] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Des-
jardins, J. Turian, D. Warde-Farley, and Y. Bengio, “Theano: a cpu
and gpu math expression compiler,” in Proceedings of the Python for
scientiﬁc computing conference (SciPy), vol. 4. Austin, TX, 2010, p. 3.
[36] E. Battenberg, S. Dieleman, D. Nouri, E. Olson, A. van den Oord,
C. Raffel, J. Schlter, and S. Kaae Snderby, “Lasagne: Lightweight
library to build and train neural networks in theano,” 2015. [Online].
Available: https://github.com/Lasagne/Lasagne
[37] B. Biggio, K. Rieck, D. Ariu, C. Wressnegger et al., “Poisoning
behavioral malware clustering,” in Proceedings of the 2014 Workshop
on Artiﬁcial Intelligent and Security Workshop. ACM, 2014, pp. 27–36.
[38] D. Warde-Farley and I. Goodfellow, “Adversarial perturbations of deep
neural networks,” in Advanced Structured Prediction, T. Hazan, G. Pa-
pandreou, and D. Tarlow, Eds., 2016.
[39] M. Barreno, B. Nelson, A. D. Joseph, and J. Tygar, “The security of
machine learning,” Machine Learning, vol. 81, no. 2, pp. 121–148, 2010.
[40] W. Xu, Y. Qi et al., “Automatically evading classiﬁers,” in Proceedings
of the 2016 Network and Distributed Systems Symposium, 2016.
[41] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar,
“Can machine learning be secure?” in Proceedings of the 2006 ACM
Symposium on Information, computer and communications security.
ACM, 2006, pp. 16–25.
[42] B. Biggio, G. Fumera, and F. Roli, “Security evaluation of pattern
classiﬁers under attack,” Knowledge and Data Engineering, IEEE Trans-
actions on, vol. 26, no. 4, pp. 984–996, 2014.
[43] B. Biggio, B. Nelson, and L. Pavel, “Poisoning attacks against support
vector machines,” in Proceedings of the 29th International Conference
on Machine Learning, 2012.
[44] B. Biggio, B. Nelson, and P. Laskov, “Support vector machines under
adversarial label noise.” in ACML, 2011, pp. 97–112.
597597
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply.