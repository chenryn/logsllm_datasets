To summarize, we ﬁnd that there is inherent stability in applica-
tion end-point architectures. Without future knowledge, there exists
practical and simple algorithms such as minimum depth that can
provide good stability performance. While we have looked at how
to reduce the number of ancestor changes in the system, another
important direction is to reduce the perceived impact of an ancestor
change. In particular, mechanisms such as maintaining application-
level buffers to smooth out interruptions could help while the af-
fected descendants are looking for new parents. Multiple-tree pro-
tocols, which we discuss next, may also help reduce the impact of
ancestor changes.
4.6 Impact of Multiple-Tree Protocols
In multiple-tree protocols, the stream is split and distributed
across k independent trees. The probability of many trees seeing si-
multaneous disruptions is small. In addition, with sufﬁcient redun-
dancy in the encoding, the impact of a disruption in one tree may be
negligible. On the other hand, because a host now has k times more
ancestors, it is likely that it would see a larger number of ancestor
changes overall. Although frequent ancestor changes may not al-
ways affect perceived quality, it creates more protocol overhead and
activity on the network because hosts need to ﬁnd new parents more
frequently.
To explore the effect of multiple-tree protocols on stability, we
simulate the same 50 streams as those depicted in Figure 11 using a
multiple-tree protocol. There are three modiﬁcations to the single-
tree protocol in Section 4.3. Except for the changes below, each tree
is independently constructed using the single-tree protocol.
Independent trees: To maintain independence between trees, each
host is an interior node (contributor of resources) in only one tree [4].
A host selects the tree that it will contribute its resources, and joins
that tree as a contributor. It joins the remaining trees as leaf nodes.
Thus, when it leaves the broadcast, it will only affect the stability of
one tree because it has descendants in only one tree.
Load balancing: We implement load balancing of resources among
trees such that all trees have roughly the same amount of resources.
The load balancing algorithm is run at join time, where a host will
become a contributor (interior node) for the tree which currently has
the lowest Resource Index. The source keeps track of the Resource
Index by maintaining a count of the amount of resources in each
tree and the current number of incarnations in the system.
Preemption: There may be cases where a tree may be saturated
and not have enough resources. If a new host were to join the tree,
it would not be able to. To allow new contributors to join, they may
preempt existing “free-riders” in the tree. Preemption involves dis-
connecting the free-rider from the tree to open up a position for the
new incoming contributor. The contributor takes the free position
and may accept the free-rider that was preempted to join under it
as its child. We implement a limited form of preemption where a
new contributor only preempts free-riders at depth 1 (i.e., children
of the source) and found this to be sufﬁcient for the workloads in our
study. Overall, preemption rarely take place. Even for the stream
with the most preemption, preemption caused only 4% of discon-
nects compared to the 96% caused by departing hosts.
Generally, MDC encoding adds redundancy and overhead com-
pared to the original stream. In our simulations, we assume that the
overhead is 25%.
In practice, this overhead depends on the spe-
ciﬁc video stream and the MDC optimization. Setting this number
too low could result in poor resilience; setting this number too high
wastes bandwidth resources. We run the simulations using three
conﬁgurations: 4, 8, and 16 trees. Each tree carries a fraction of
the source rate. For example, in the 4-tree conﬁguration, each tree
carries 1/4 of the original source rate with 25% redundancy and
overhead. With 25% redundancy, receiving 3 out of 4 descriptions
is sufﬁcient. Only results for the minimum depth parent selection
algorithm are presented. Note that minimum depth performed the
best amongst all the practical algorithms evaluated for the single-
tree protocol.
First, we look at the percentage of incarnations that see frequent
ancestor changes (have an average interval between ancestor change
shorter than 5 minutes, similar to the single-tree case). Because an
incarnation has multiple simultaneous parents, one in each tree, it is
likely to see more frequent ancestor changes as more trees are used.
Simulation results conﬁrm this intuition. We ﬁnd that with 16 trees,
across the 50 streams, on average 75% of incarnations in the streams
see too frequent ancestor changes. With 8, 4, and single-tree, the
percentage drops to 55%, 32%, and 8% respectively. While this in-
dicates that protocol overhead increases signiﬁcantly with multiple
trees, it does not indicate the perceived quality of the streams.
Next, to understand perceived quality, we look at the average in-
terval between too many simultaneous disconnects. As previously
mentioned, being disconnected from one tree does not impact the
quality of the stream. However, being disconnected from too many
trees simultaneously, or over 25% of the trees in our conﬁguration,
indicates poor perceived quality. We assume that when a host is dis-
connected, it takes one second for it to ﬁnd a new parent and connect
back to the tree. The y-axis in Figure 12 depicts the percentage of
incarnations that see too many disconnects, deﬁned as more often
than once in 5 minutes. The x-axis is the 50 large-scale streams –
the same as in the single-tree analysis in the previous section. The y-
axis is truncated at 5% to better illustrate the differences between the
different conﬁgurations. In addition, the previous results from the
single-tree minimum depth protocol are also depicted. The percent-
age of incarnations with poor stability is higher for the single-tree
protocol. For the multiple-tree protocol, using 4 trees, all streams
have less than 5% of incarnations with poor performance. Fewer
incarnations see poor performance as more trees are used. For ex-
ample, using 16 trees, all streams have less than 2% of incarnations
with poor stability performance.
In this section, we see that multiple trees can increase the per-
ceived quality of the streams. However, the improved performance
comes at a cost of more frequent disconnects, more protocol over-
head, and more complex protocols.
5. CAN EFFICIENT OVERLAYS BE
CONSTRUCTED?
In this section, we look at the feasibility of constructing efﬁ-
cient large-scale overlays. An efﬁcient overlay is one in which the
overlay structure closely reﬂects the underlying IP network. The
challenge is to enable hosts to discover other nearby hosts that may
be used as parents. When there are as many as 70,000 other hosts
simultaneously participating, it is not possible for a host to know ev-
eryone else because it would require signiﬁcant protocol overhead
to maintain such knowledge. As a result, each host will only know
a subset of the current membership. In order to construct efﬁcient
overlays, that subset must contain hosts that are nearby.
We develop and analyze techniques for partitioning application
end-points into clusters. One member of each cluster is designated
as the cluster head (also called membership server). Hosts in the
same cluster maintain knowledge about one another. Clustering
policies that leverage network proximity have the potential to in-
crease the efﬁciency of the overlay structure.
5.1 Membership Management
Next, we describe the clustering-based membership manage-
ment protocol hosts use to maintain and distribute group member-
ship information. We wish to highlight that the simplicity of the
protocol allows for simple recovery given the dynamic arriving and
departing nature of the membership servers.
Handling host join: A new host joining the system contacts a ren-
dezvous point, often the source of the broadcast who is respon-
sible for knowing the current membership servers participating in
the broadcast. The rendezvous point responds with a list of current
membership servers. The new joining host then selects one of the
membership servers to contact, either randomly or by using cluster-
ing techniques discussed in the next section. The selected member-
ship server replies with a fresh list of current members that it knows
(mostly inside the same cluster). The joining host then uses the list
in the tree construction protocol to connect itself to the tree via a
member in the list.
Creating membership servers: The rendezvous point is respon-
sible for ensuring that there are enough membership servers in the
system. Membership servers are created on-demand based on the
needs of the system. For example, when a new host arrives and
there are not enough membership servers in the system, the ren-
dezvous point will immediately assign the new host to function as
a membership server (assuming the new host has enough resources
to support the control trafﬁc).
Recovering from membership server dynamics: Because we are
using application end-points as membership servers, we must cope
with membership servers leaving. Just before leaving, a member-
ship server looks to see if it can promote one of the hosts inside
its own cluster to become the new membership server. It is possi-
ble that a promotion may not be possible because of resource con-
straints. In such cases, the rendezvous server will notice that the
number of membership servers has decreased and will create a new
membership server from the newly arriving hosts. Note that when a
membership server leaves, it does not affect data delivery except for
the hosts that are its own direct descendants. The existing hosts that
were part of the departing membership server’s cluster need to ﬁnd
a new membership server. If a promotion was successful, the newly
promoted host becomes their replacement membership server. The
membership state can be quickly refreshed as hosts can send explicit
liveness updates to the replacement membership server. If a promo-
tion was not successful, hosts will move to different membership
servers.
State maintenance: In order to recover from membership servers
departing the broadcast dynamically, all membership servers explic-
itly exchange state about their liveness with the rendezvous point.
Membership servers also maintain explicit state, liveness, and in-
formation about other membership servers and a random subset of
members outside their cluster. In addition, all hosts inside a cluster
exchange explicit state and maintain keep-alives with their member-
ship server. When keep-alive messages are received at the member-
ship server, the membership server will respond with a list of a sub-
set of other live membership servers in the system and other mem-
bers outside its cluster (learned from exchanges with other member-
ship servers). Knowing hosts outside one’s own cluster helps with
recovery. Hosts also exchange their group membership knowledge
with other hosts that they know. Gossip-like protocols [22] may
be used, with a stronger bias towards exchanging information with
hosts inside their own cluster.
Interplay between membership management and tree construc-
tion: The data delivery structure and the membership management
clusters are loosely coupled. The membership information from the
clusters implicitly inﬂuences the data delivery tree. We do not en-
force strict clustering on the data delivery tree. In fact overlay nodes
are free to select nodes outside their own cluster as parents if those
nodes provide better performance. This simpliﬁes performance op-
timizations and recovery from node failures.
5.2 Clustering Policies
In this section, we discuss the design of clustering policies. We
consider three different clustering policies. Our ﬁrst policy is ran-
dom, where the clusters are agnostic of network proximity. Our sec-
ond policy is network delay-based clustering. Short delays are rea-
sonably correlated with good bandwidth performance [17], which is
also important for streaming applications. And lastly, we look at ge-
ographic clustering, which roughly approximates network distance.
For example, the network delay between hosts in the same continent
is likely to be shorter than hosts in two different continents.
We implement the clustering policies by having hosts join the
cluster belonging to the membership server “closest” to them. For
random clustering, hosts pick a cluster to join at random. We call
these three policies naive clustering.
In addition to considering proximity, we need to consider two
critical requirements: ensuring that cluster sizes are not too large
and ensuring that each cluster has enough resources. Bounding the
cluster size helps to prevent membership servers from being over-
loaded. Being aware of resources helps to ensure that there are
enough resources within a cluster such that hosts can use other hosts
inside their own cluster as parents. While hosts may still use hosts
outside their cluster as parents, this degrades the efﬁciency of the
overlay. Hosts in different clusters are likely to be farther away than
hosts inside the same cluster.
Ignoring these two requirements results in poor clusters. For
example, we analyze the largest event and ﬁnd that using naive ran-
dom clustering, all clusters have sizes close to 200 hosts. However,
naive delay-based and geographic clustering both produce clusters
with a wide range of sizes (from 10’s to 1000’s). In addition, the Re-
source Index for 7% of the random clusters, and 20% of the delay-
based and geographic clusters are below 1. For example, delay-
based clustering produces a few huge clusters with low Resource
Index, each comprising almost exclusively of DSL and cable mo-
dem hosts that belong to the same ISP.
We use the following algorithms to meet the two additional re-
quirements of maintaining cluster sizes and resources.
Cluster Size Maintenance: Two possibilities for bounding the clus-
ter size and handling overﬂows are: (i) redirection, where new hosts
are redirected to the next best cluster until an available one is found
and (ii) new cluster creation, where a new contributor host that is
supposed to join a full cluster creates a new cluster.
Resource Maintenance: We redirect free-riders joining a cluster
with Resource Index at or below 1 to other clusters, but we allow
contributors to join because they either increase or maintain the Re-
source Index.
5.3 Clustering Quality
In this section, we evaluate the quality of the clustering pro-
duced by the various policies and design choices. First, we discuss
how we obtained the proximity data used in the evaluation.
5.3.1 Proximity Data
Network delay: In order to evaluate efﬁciency, we need to
know the pair-wise delay between all participating hosts. This is
infeasible without access to the hosts themselves. Instead, we ap-
proximate pair-wise delay values using Global Network Position-
ing (GNP) [16]. We assign coordinates to each of the hosts and
“compute” the delay based on the geometric distance. To assign co-
ordinates, we use 13 landmarks (PlanetLab [19] machines) located
around the world. Landmarks measure the round-trip time between
themselves and the IP addresses in our streaming workload, and
then compute coordinates based on the measurement data (using 8
dimensions). Due to the overhead of probing and the low response
rate, we probed only the IP addresses in the largest stream in our
traces. Of the 118,921 IP addresses, only 27,305 responded. Hosts
that did not respond are not used in our simulations.
Geographic distance: We obtain location information from
Akamai’s EdgeScape service, the same service that provided ac-
cess technology information in Section 3. Using EdgeScape, we
map an IP address to its latitude and longitude. Manual veriﬁcation
of mapping results with known geographic locations showed that
the information was accurate for our purpose, which is coarse-grain
geographic clustering.
For all of the following analysis and simulations, we use a de-
gree cap of 20 and the distribution algorithm for assignment of un-
knowns, similar to the setup in Section 4. We assume that each
membership server contributes one “degree” out of its existing re-
sources for the join protocol overhead.
5.3.2 Clustering Quality Metric
To capture clustering quality, we use the average and maxi-
s
r
e
t
s
u
C
l
f
o
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
100
90
80
70
60
50
40
30
20
10
0
Average 50
Maximum 50
Average 100
Maximum 100
Average 200
Maximum 200
Average 500
Maximum 500
0
100
200
400
Intra-Cluster Distance (ms)
300
500
600
Figure 13: Clustering quality when varying number of clusters.
s
r
e
t
s
u
C
l
f
o
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
l
100
90
80
70
60
50
40
30
20
10
0
GNP  Average
GNP  Maximum
GNP Redirect Average
GNP Redirect Maximum
GNP Create New Average
GNP Create New Maximum
0
50
100
150
200
250
300
350
400
450
500
Intra-Cluster Distance (ms)
Figure 14: Clustering quality when bounding cluster sizes.
mum intra-cluster distance in milliseconds as the metric. Average
intra-cluster distance measures the overall “tightness” of the clus-
tering. The smaller the value, the closer all hosts in the cluster are
to each other. Maximum intra-cluster distance measures the worst-
case “spread” of the cluster. Again, we would like to see a small
distance. The distance metric we use here is the network distance
(approximated using GNP) and the following analysis is conducted