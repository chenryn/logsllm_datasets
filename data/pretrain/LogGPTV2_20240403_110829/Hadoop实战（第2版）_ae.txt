HBase在分布式集群上主要依靠由HRegion、HMaster、HClient组成的体系结构从整体上管理数据。
HBase体系结构有三大重要组成部分：
HBaseMaster：HBase主服务器，与Bigtable的主服务器类似。
HRegionServer：HBase域服务器，与Bigtable的Tablet服务器类似。
HBase Client：HBase客户端是由org.apache.hadoop.HBase.client.HTable定义的。
下面将对这三个组件进行详细的介绍。
（1）HBaseMaster
一个HBase只部署一台主服务器，它通过领导选举算法（Leader Election Algorithm）确保只有唯一的主服务器是活跃的，ZooKeeper保存主服务器的服务器地址信息。如果主服务器瘫痪，可以通过领导选举算法从备用服务器中选择新的主服务器。
主服务器承担着初始化集群的任务。当主服务器第一次启动时，会试图从HDFS获取根或根域目录，如果获取失败则创建根或根域目录，以及第一个元域目录。在下次启动时，主服务器就可以获取集群和集群中所有域的信息了。同时主服务器还负责集群中域的分配、域服务器运行状态的监视、表格的管理等工作。
（2）HRegionServer
HBase域服务器的主要职责有服务于主服务器分配的域、处理客户端的读写请求、本地缓冲区回写、本地数据压缩和分割域等功能。
每个域只能由一台域服务器来提供服务。当它开始服务于某域时，它会从HDFS文件系统中读取该域的日志和所有存储文件，同时还会管理操作HDFS文件的持久性存储工作。客户端通过与主服务器通信获取域和域所在域服务器的列表信息后，就可以直接向域服务器发送域读写请求，来完成操作。
（3）HBaseClient
HBase客户端负责查找用户域所在的域服务器地址。HBase客户端会与HBase主机交换消息以查找根域的位置，这是两者之间唯一的交流。
定位根域后，客户端连接根域所在的域服务器，并扫描根域获取元域信息。元域信息中包含所需用户域的域服务器地址。客户端再连接元域所在的域服务器，扫描元域以获取所需用户域所在的域服务器地址。定位用户域后，客户端连接用户域所在的域服务器并发出读写请求。用户域的地址将在客户端被缓存，后续的请求无须重复上述过程。
综上所述，在HBase的体系结构中，HBase主要由主服务器、域服务器和客户端三部分组成。主服务器作为HBase的中心，管理整个集群中的所有域，监控每台域服务器的运行情况等；域服务器接收来自服务器的分配域，处理客户端的域读写请求并回写映射文件等；客户端主要用来查找用户域所在的域服务器地址信息。
1.6.3 Hive的数据管理
Hive是建立在Hadoop上的数据仓库基础构架。它提供了一系列的工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。Hive定义了简单的类SQL的查询语言，称为Hive QL，它允许熟悉SQL的用户用SQL语言查询数据。作为一个数据仓库，Hive的数据管理按照使用层次可以从元数据存储、数据存储和数据交换三方面来介绍。
（1）元数据存储
Hive将元数据存储在RDBMS中，有三种模式可以连接到数据库：
Single User Mode：此模式连接到一个In-memory的数据库Derby，一般用于Unit Test。
Multi User Mode：通过网络连接到一个数据库中，这是最常用的模式。
Remote Server Mode：用于非Java客户端访问元数据库，在服务器端启动一个。
MetaStoreServer，客户端利用Thrift协议通过MetaStoreServer来访问元数据库。
（2）数据存储
首先，Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由地组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，它就可以解析数据了。
其次，Hive中所有的数据都存储在HDFS中，Hive中包含4种数据模型：Table、External Table、Partition和Bucket。
Hive中的Table和数据库中的Table在概念上是类似的，每一个Table在Hive中都有一个相应的目录来存储数据。例如，一个表pvs，它在HDFS中的路径为：/wh/pvs，其中，wh是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。
（3）数据交换
数据交换主要分为以下几部分，如图1-5所示。
用户接口：包括客户端、Web界面和数据库接口。
元数据存储：通常存储在关系数据库中，如MySQL、Derby等。
解释器、编译器、优化器、执行器。
Hadoop：利用HDFS进行存储，利用MapReduce进行计算。
用户接口主要有三个：客户端、数据库接口和Web界面，其中最常用的是客户端。Client是Hive的客户端，当启动Client模式时，用户会想要连接Hive Server，这时需要指出Hive Server所在的节点，并且在该节点启动HiveServer。Web界面是通过浏览器访问Hive的。
Hive将元数据存储在数据库中，如MySQL、Derby中。Hive中的元数据包括表的名字、表的列、表的分区、表分区的属性、表的属性（是否为外部表等）、表的数据所在目录等。
解释器、编译器、优化器完成Hive QL查询语句从词法分析、语法分析、编译、优化到查询计划的生成。生成的查询计划存储在HDFS中，并且随后由MapReduce调用执行。
Hive的数据存储在HDFS中，大部分的查询由MapReduce完成（包含*的查询不会生成MapRedcue任务，比如select*from tbl）。
以上从Hadoop的分布式文件系统HDFS、分布式数据库HBase和数据仓库工具Hive入手介绍了Hadoop的数据管理，它们都通过自己的数据定义、体系结构实现了数据从宏观到微观的立体化管理，完成了Hadoop平台上大规模的数据存储和任务处理。
图 1-5 Hive数据交换图
1.7 Hadoop集群安全策略
众所周知，Hadoop的优势在于其能够将廉价的普通PC组织成能够高效稳定处理事务的大型集群，企业正是利用这一特点来构架Hadoop集群、获取海量数据的高效处理能力的。但是，Hadoop集群搭建起来后如何保证它安全稳定地运行呢？旧版本的Hadoop中没有完善的安全策略，导致Hadoop集群面临很多风险，例如，用户可以以任何身份访问HDFS或MapReduce集群，可以在Hadoop集群上运行自己的代码来冒充Hadoop集群的服务，任何未被授权的用户都可以访问DataNode节点的数据块等。经过Hadoop安全小组的努力，在Hadoop 1.0.0版本中已经加入最新的安全机制和授权机制（Simple和Kerberos），使Hadoop集群更加安全和稳定。下面从用户权限管理、HDFS安全策略和MapReduce安全策略三个方面简要介绍Hadoop的集群安全策略。有关安全方面的基础知识如Kerberos认证等读者可自行查阅相关资料。
（1）用户权限管理
Hadoop上的用户权限管理主要涉及用户分组管理，为更高层的HDFS访问、服务访问、Job提交和配置Job等操作提供认证和控制基础。
Hadoop上的用户和用户组名均由用户自己指定，如果用户没有指定，那么Hadoop会调用Linux的“whoami”命令获取当前Linux系统的用户名和用户组名作为当前用户的对应名，并将其保存在Job的user.name和group.name两个属性中。这样用户所提交Job的后续认证和授权以及集群服务的访问都将基于此用户和用户组的权限及认证信息进行。例如，在用户提交Job到JobTracker时，JobTracker会读取保存在Job路径下的用户信息并进行认证，在认证成功并获取令牌之后，JobTracker会根据用户和用户组的权限信息将Job提交到Job队列（具体细节参见本小节的HDFS安全策略和MapReduce安全策略）。
Hadoop集群的管理员是创建和配置Hadoop集群的用户，它可以配置集群，使用Kerberos机制进行认证和授权。同时管理员可以在集群的服务（集群的服务主要包括NameNode、DataNode、JobTracker和TaskTracker）授权列表中添加或更改某确定用户和用户组，系统管理员同时负责Job队列和队列的访问控制矩阵的创建。
（2）HDFS安全策略
用户和HDFS服务之间的交互主要有两种情况：用户机和NameNode之间的RPC交互获取待通信的DataNode位置，客户机和DataNode交互传输数据块。
RPC交互可以通过Kerberos或授权令牌来认证。在认证与NameNode的连接时，用户需要使用Kerberos证书来通过初试认证，获取授权令牌。授权令牌可以在后续用户Job与NameNode连接的认证中使用，而不必再次访问Kerberos Key Server。授权令牌实际上是用户机与NameNode之间共享的密钥。授权令牌在不安全的网络上传输时，应给予足够的保护，防止被其他用户恶意窃取，因为获取授权令牌的任何人都可以假扮成认证用户与NameNode进行不安全的交互。需要注意的是，每个用户只能通过Kerberos认证获取唯一一个新的授权令牌。用户从NameNode获取授权令牌之后，需要告诉NameNode：谁是指定的令牌更新者。指定的更新者在为用户更新令牌时应通过认证确定自己就是NameNode。更新令牌意味着延长令牌在NameNode上的有效期。为了使MapReduce Job使用一个授权令牌，用户应将JobTracker指定为令牌更新者。这样同一个Job的所有Task都会使用同一个令牌。JobTracker需要保证这一令牌在整个任务的执行过程中都是可用的，在任务结束之后，它可以选择取消令牌。
数据块的传输可以通过块访问令牌来认证，每一个块访问令牌都由NameNode生成，它们都是特定的。块访问令牌代表着数据访问容量，一个块访问令牌保证用户可以访问指定的数据块。块访问令牌由NameNode签发被用在DataNode上，其传输过程就是将NameNode上的认证信息传输到DataNode上。块访问令牌是基于对称加密模式生成的，NameNode和DataNode共享了密钥。对于每个令牌，NameNode基于共享密钥计算一个消息认证码（Message Authentication Code, MAC）。接下来，这个消息认证码就会作为令牌验证器成为令牌的主要组成部分。当一个DataNode接收到一个令牌时，它会使用自己的共享密钥重新计算一个消息认证码，如果这个认证码同令牌中的认证码匹配，那么认证成功。
（3）MapReduce安全策略
MapReduce安全策略主要涉及Job提交、Task和Shuffle三个方面。
对于Job提交，用户需要将Job配置、输入文件和输入文件的元数据等写入用户home文件夹下，这个文件夹只能由该用户读、写和执行。接下来用户将home文件夹位置和认证信息发送给JobTracker。在执行过程中，Job可能需要访问多个HDFS节点或其他服务，因此，Job的安全凭证将以＜String key, binary value＞形式保存在一个Map数据结构中，在物理存储介质上将保存在HDFS中JobTracker的系统目录下，并分发给每个TaskTracker。Job的授权令牌将NameNode的URL作为其关键信息。为了防止授权令牌过期，JobTracker会定期更新授权令牌。Job结束之后所有的令牌都会失效。为了获取保存在HDFS上的配置信息，JobTracker需要使用用户的授权令牌访问HDFS，读取必需的配置信息。
任务（Task）的用户信息沿用生成Task的Job的用户信息，因为通过这个方式能保证一个用户的Job不会向TaskTracker或其他用户Job的Task发送系统信号。这种方式还保证了本地文件有权限高效地保存私有信息。在用户提交Job后，TaskTracker会接收到JobTracker分发的Job安全凭证，并将其保存在本地仅对该用户可见的Job文件夹下。在与TaskTracker通信的时候，Task会用到这个凭证。
当一个Map任务完成时，它的输出被发送给管理此任务的TaskTracker。每一个Reduce将会与TaskTracker通信以获取自己的那部分输出，此时，就需要MapReduce框架保证其他用户不会获取这些Map的输出。Reduce任务会根据Job凭证计算请求的URL和当前时间戳的消息认证码。这个消息认证码会和请求一起发到TaskTracker，而TaskTracker只会在消息认证码正确并且在封装时间戳的N分钟之内提供服务。在TaskTracker返回数据时，为了防止数据被木马替换，应答消息的头部将会封装根据请求中的消息认证码计算而来的新消息认证码和Job凭证，从而保证Reduce能够验证应答消息是由正确的TaskTracker发送而来。
1.8 本章小结
本章首先介绍了Hadoop分布式计算平台：它是由Apache软件基金会开发的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS）和MapReduce（Google MapReduce的开源实现）为核心的Hadoop为用户提供了系统底层细节透明的分布式基础架构。由于Hadoop拥有可计量、成本低、高效、可信等突出特点，基于Hadoop的应用已经遍地开花，尤其是在互联网领域。
本章接下来介绍了Hadoop项目及其结构，现在Hadoop已经发展成为一个包含多个子项目的集合，被用于分布式计算，虽然Hadoop的核心是Hadoop分布式文件系统和MapReduce，但Hadoop下的Common、Avro、Chukwa、Hive、HBase等子项目提供了互补性服务或在核心层之上提供了更高层的服务。紧接着，简要介绍了以HDFS和MapReduce为核心的Hadoop体系结构。
本章之后又从分布式系统的角度介绍了Hadoop是如何做到并行计算和数据管理的。分布式计算平台Hadoop实现了分布式文件系统和分布式数据库。Hadoop中的分布式文件系统HDFS能够实现数据在电脑集群组成的云上高效的存储和管理功能，Hadoop中的并行编程框架MapReduce基于HDFS来保证用户可以编写应用于Hadoop的并行应用程序。本章又介绍了Hadoop的数据管理，主要包括Hadoop的分布式文件系统HDFS、分布式数据库HBase和数据仓库工具Hive。它们都有自己完整的数据定义和体系结构，以及实现数据从宏观到微观的立体管理数据办法，这都为Hadoop平台的数据存储和任务处理打下了基础。
本章最后还介绍了关于Hadoop的一些基本的安全策略，包括用户权限管理、HDFS安全策略和MapReduce安全策略，为用户的实际使用提供了参考。本章中的许多内容在本书后面的章节中会详细介绍。
第2章 Hadoop的安装与配置
本章内容
在Linux上安装与配置Hadoop
在Mac OSX上安装与配置Hadoop
在Windows上安装与配置Hadoop
安装和配置Hadoop集群
日志分析及几个小技巧
本章小结
Hadoop的安装非常简单，大家可以在官网上下载到最新的几个版本，截至本书截稿时，Hadoop的最新版本是1.0.1，下载网址为http：//apache.etoak.com//hadoop/core/。
Hadoop是为了在Linux平台上使用而开发的，但是在一些主流的操作系统如UNIX、Windows甚至Mac OS X系统上Hadoop也运行良好。不过，在Windows上运行Hadoop稍显复杂，首先必须安装Cygwin来模拟Linux环境，然后才能安装Hadoop。
本章将介绍在Linux、Mac OS X和Windows系统上安装最新的Hadoop1.0.1版本，其中，Linux系统是Ubuntu 11.10，Mac OS X系统是10.7.3版本，Windows系统采用Windows Xp sp3。这些安装步骤均由笔者成功实践过，大家可直接参照执行。
2.1 在Linux上安装与配置Hadoop
在Linux上安装Hadoop之前，需要先安装两个程序：
1）JDK 1. 6（或更高版本）。Hadoop是用Java编写的程序，Hadoop的编译及MapReduce的运行都需要使用JDK。因此在安装Hadoop前，必须安装JDK 1.6或更高版本。
2）SSH（安全外壳协议），推荐安装OpenSSH。Hadoop需要通过SSH来启动Slave列表中各台主机的守护进程，因此SSH也是必须安装的，即使是安装伪分布式版本（因为Hadoop并没有区分开集群式和伪分布式）。对于伪分布式，Hadoop会采用与集群相同的处理方式，即按次序启动文件conf/slaves中记载的主机上的进程，只不过在伪分布式中Salve为localhost（即为自身），所以对于伪分布式Hadoop, SSH一样是必需的。
 2.1.1 安装JDK 1.6
下面介绍安装JDK 1.6的具体步骤。
（1）下载和安装JDK 1.6
确保可以连接到互联网，从http：//www.oracle.com/technetwork/java/javase/downloads页面下载JDK 1.6安装包（文件名类似jdk-***-linux-i586.bin，不建议安装JDK 1.7版本，因为并不是所有软件都支持1.7版本）到JDK安装目录（本章假设IDK安装目录均为/usr/lib/jvm/jdk）。
（2）手动安装JDK 1.6
在终端下进入JDK安装目录，并输入命令：
sudo chmod u+x jdk-***-linux-i586.bin
修改完权限之后就可以进行安装了，在终端输入命令：
sudo-s./jdk-***-linux-i586.bin
安装结束之后就可以开始配置环境变量了。
（3）配置环境变量
输入命令：
sudo gedit/etc/profile
输入密码，打开profile文件。
在文件最下面输入如下内容：
#set Java Environment
export JAVA_HOME=/usr/lib/jvm/jdk
export CLASSPATH=".：$JAVA_HOME/lib：$CLASSPATH"
export PATH="$JAVA_HOME/：$PATH"
这一步的意义是配置环境变量，使系统可以找到JDK。
（4）验证JDK是否安装成功输入命令：
java-version
会出现如下JDK版本信息：
java version"1.6.0_22"
Java（TM）SE Runtime Environment（build 1.6.0_22-b04）
Java HotSpot（TM）Client VM（build 17.1-b03，mixed mode, sharing）
如果出现上述JDK版本信息，说明当前安装的JDK并未设置成Ubuntu系统默认的JDK，接下来还需要手动将安装的JDK设置成系统默认的JDK。
（5）手动设置系统默认JDK
在终端依次输入命令：
sudo update-alternatives--install/usr/bin/java java/usr/lib/jvm/jdk/bin/java 300
sudo update-alternatives--install/usr/bin/javac javac/usr/lib/jvm/jdk/bin/javac 300
sudo update-alternatives--config java
接下来输入java-version就可以看到所安装的JDK的版本信息了。
2.1.2 配置SSH免密码登录
同样以Ubuntu为例，假设用户名为u：
1）确认已经连接上互联网，然后输入命令：
sudo apt-get install ssh
2）配置为可以免密码登录本机。首先查看在u用户下是否存在.ssh文件夹（注意ssh前面有“.”，这是一个隐藏文件夹），输入命令：
ls-a/home/u
一般来说，安装SSH时会自动在当前用户下创建这个隐藏文件夹，如果没有，可以手动创建一个。
接下来，输入命令（注意下面命令中不是双引号，是两个单引号）：
ssh-keygen-t dsa-P''-f～/.ssh/id_dsa
解释一下，ssh-keygen代表生成密钥；-t（注意区分大小写）表示指定生成的密钥类型；dsa是dsa密钥认证的意思，即密钥类型；-P用于提供密语；-f指定生成的密钥文件。在Ubuntu中，～代表当前用户文件夹，此处即/home/u。
这个命令会在.ssh文件夹下创建id_dsa及id_dsa.pub两个文件，这是SSH的一对私钥和公钥，类似于钥匙和锁，把id_dsa.pub（公钥）追加到授权的key中去。
输入命令：
cat～/.ssh/id_dsa.pub＞＞～/.ssh/authorized_keys
这条命令的功能是把公钥加到用于认证的公钥文件中，这里的authorized_keys是用于认证的公钥文件。
至此免密码登录本机已配置完毕。
3）验证SSH是否已安装成功，以及是否可以免密码登录本机。
输入命令：
ssh-version
显示结果：
OpenSSH_5.8p1 Debian-7ubuntu1，OpenSSL 1.0.0e 6 Sep 2011
Bad escape character'rsion'.
显示SSH已经安装成功了。
输入命令：
ssh localhost
会有如下显示：
The authenticity of host'localhost（：1）'can't be established.
RSA key fingerprint is 8b：c3：51：a5：2a：31：b7：74：06：9d：62：04：4f：84：f8：77.
Are you sure you want to continue connecting（yes/no）?yes
Warning：Permanently added'localhost'（RSA）to the list of known hosts.
Linux master 2.6.31-14-generic#48-Ubuntu SMP Fri Oct 16 14：04：26 UTC 2011 i686
To access official Ubuntu documentation, please visit：
http：//help.ubuntu.com/
Last login：Sat Feb 18 17：12：40 2012 from master
admin@Hadoop：～$
这说明已经安装成功，第一次登录时会询问是否继续链接，输入yes即可进入。实际上，在Hadoop的安装过程中，是否免密码登录是无关紧要的，但是如果不配置免密码登录，每次启动Hadoop都需要输入密码以登录到每台机器的DataNode上，考虑到一般的Hadoop集群动辄拥有数百或上千台机器，因此一般来说都会配置SSH的免密码登录。
2.1.3 安装并运行Hadoop