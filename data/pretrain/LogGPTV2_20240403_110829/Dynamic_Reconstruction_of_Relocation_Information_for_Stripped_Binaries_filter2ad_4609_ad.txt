# Dynamic Reconstruction of Relocation Information

## 4. Avoiding Performance Hits During the Dynamic Relocation Discovery Phase

To mitigate the performance impact during the dynamic relocation discovery phase (first run), we gradually increase the input size in each execution. This approach significantly reduces the overall time compared to running a program with large input from the start.

### Example: `gcc`
The high number of relocatable offset copies in dynamic data within `gcc` (as shown in Table 1) contributes to its initial overhead. However, this overhead disappears in subsequent executions. One strategy is to ask users to restart the program to benefit from the already discovered relocatable offsets. Alternatively, users can start with a very small input and progressively increase the workload over the first few executions until most relocations are discovered.

### Experimental Evaluation
We applied this strategy to the SPEC CPU2006 benchmarks, which include three different input sizes:
- **Test**: A very small dataset for verifying functionality.
- **Train**: A medium-sized dataset for feedback-directed optimizations.
- **Reference**: A much larger dataset for comprehensive testing.

For all previous results, we used the reference dataset. Figure 4 shows the normalized slowdown when applying our technique while increasing the workload (from test to train to reference). During each execution, our prototype uses any reconstructed relocation information discovered in previous runs. The slowdown for the reference dataset is significantly reduced compared to Figure 3, and the overall discovery phase (now broken into three executions) is much quicker in absolute numbers. For example, `gcc` has a larger slowdown with the test dataset (22 seconds), but the total time (including the next two executions) is only a few minutes, compared to 48 minutes using the large reference dataset in the first execution.

## 5.3 Use Cases
Our evaluation also includes real-world applications. We installed older versions of Internet Explorer (6.0.2900.5512) and Adobe Reader (8.1.2) with stripped relocation information. Our prototype successfully relocated the code segments to new, random locations without breaking functionality. We reconstructed relocation information for 18 offsets in Internet Explorer and 3 in Adobe Reader, demonstrating the feasibility and importance of our technique for protecting these applications.

## 6. Related Work
### 6.1 Code Randomization and Disassembly
Address Space Layout Randomization (ASLR) is a widely deployed countermeasure against code-reuse attacks. Early ASLR implementations, such as those by the PaX project, used page fault interception to handle non-relocatable executables. Our work extends this idea to Windows executables, adding patching support to reduce runtime overhead and providing experimental validation.

However, ASLR's effectiveness can be limited by static code segments and vulnerabilities that leak memory contents, allowing attackers to calculate DLL base addresses. Fine-grained randomization techniques have been proposed to address these limitations, but their accuracy depends on the availability of relocation information.

Control Flow Integrity (CFI) is another protection scheme that confines program execution to precomputed control flow paths. Recent CFI proposals for stripped binaries would benefit from improved control flow extraction based on relocation information. Binary rewriting without relocation information relies on dynamic instrumentation, which can introduce significant overhead, especially in C++ applications.

### 6.2 Dynamic Data Structure Excavation
Dynamic data structure excavation techniques infer binary data types at runtime. For example, Laika uses Bayesian unsupervised learning to detect data structures, while Rewards reconstructs type information based on abstract structure identification. Howard improves precision by applying specific rules to identify data structures dynamically. These techniques are related to ours but focus on different aspects of binary analysis.

## 7. Conclusion
ASLR is an effective mitigation against code reuse attacks, but it often depends on information that is stripped from executable files. Our technique dynamically reconstructs this missing information, enabling ASLR even for incompatible programs. Our experimental results demonstrate the practicality of our approach in terms of performance and real-world use cases.

## Acknowledgements
This work was supported by the US Air Force, the Office of Naval Research, DARPA, and Intel Corp., with additional support from the National Science Foundation. Any opinions, findings, conclusions, or recommendations expressed herein are those of the authors and do not necessarily reflect those of the supporting organizations.

## References
[References listed here, formatted according to the original text.]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the technical details and context of the original text.