### Evading Next-Gen Antivirus Using Artificial Intelligence
**Hyrum Anderson**  
*Technical Director of Data Science*  
Email: [PI:EMAIL](mailto:PI:EMAIL)  
Twitter: [@drhyrum](https://twitter.com/drhyrum)  
LinkedIn: [Hyrum Anderson](https://www.linkedin.com/in/hyrumanderson)  
GitHub: [endgameinc/gym-malware](https://github.com/endgameinc/gym-malware)

#### Rules for Static Analysis
```yaml
rule malware {
    strings:
        $reg = "\\CurrentVersion\\Internet Settings"
    condition:
        filesize > 3 and count(-registry-prefix) > 0
}
```

#### Next-Gen Antivirus Using Machine Learning
- **Static Machine Learning Model**: Trained on millions of samples.
- **Example**:
  - **Original Score**: `score=0.95` (malicious, moderate confidence)
  - **Modified Sample**:
    - UPX unpack
    - Rename `.text` to `.foo` (remains valid entry point)
    - Create a new `.text` section and populate it with `.text` from `calc.exe`
  - **New Score**: `score=0.49` (benign, just barely)

#### Can We Break Machine Learning?
- **Blind Spots and Hallucinations**: Machine learning models can have blind spots or hallucinations due to modeling errors.
- **Exploitation**: Depending on the model and level of access, these blind spots can be exploited.
- **Adversarial Examples**: These can generalize across different models and model types (Goodfellow, 2015).
- **Generalization of Blind Spots**: Blind spots in one model may also be present in another.

#### Adversarial Examples in Deep Learning and Images
- **Example**: An image classified as a bus (99%) and an ostrich (1%).
- **Modification**: Slight modifications can change the classification without breaking the image's structure.
- **Challenges for PE Malware**:
  - Attacker needs full knowledge of the deep learning model.
  - Generated samples may not be valid PE files.
  - Modifying bytes or features can break the PE format or destroy functionality.

#### Related Work: PDF Attack/Report Score
- **Genetic Algorithm**:
  - Functional and broken samples.
  - Requires a black-box AV score.
  - Oracle/sandbox is expensive but necessary to ensure functionality.
- **EvadeML for PDF Malware** (Xu, Qi, Evans, 2016):
  - Gradient-based attacks: perturbation or GAN.
  - Genetic algorithms: require a score from a black-box model.

#### Goal
- Design a machine learning agent that:
  - Bypasses black-box machine learning models.
  - Uses format- and function-preserving mutations.
  - Implements the hardest attack to execute; difficult to learn.

#### Reinforcement Learning
- **Atari Breakout Example**:
  - **Game**: Bouncing ball + rows of bricks, manipulate paddle (left, right, nothing), reward for eliminating each brick.
  - **AI Agent**:
    - **Environment**: Bouncing ball + rows of bricks.
    - **Agent**: Input (environment state, pixels), Output (action, left, right), Feedback (delayed reward, score).
    - **Learning**: Through thousands of games, the agent learns the most useful action given a screenshot of the gameplay.

#### Anti-Malware Evasion: An AI
- **Environment**:
  - A malware sample (Windows PE).
  - Buffet of malware mutations (preserve format & functionality).
  - Reward from a static malware classifier.
- **Agent**:
  - **Input**: Environment state (malware bytes).
  - **Output**: Action (stochastic).
  - **Feedback**: Reward (AV reports benign).

#### The Agent’s State Observation
- **Features**:
  - Static Windows PE file features compressed to 2350 dimensions.
  - General file information (size), header info, section characteristics, imported/exported functions, strings, file byte and entropy histograms.
  - Feed a neural network to choose the best action for the given state.

#### The Agent’s Manipulation Arsenal
- **Functionality-Preserving Mutations**:
  - Create: New Entry Point (w/ trampoline), New Sections.
  - Add: Random Imports, Random bytes to PE overlay, Bytes to end of section.
  - Modify: Random sections to common name, Debug info, UPX pack/unpack, Header checksum, Signature.

#### The Machine Learning Model
- **Static PE Malware Classifier**:
  - Gradient boosted decision tree (not directly amenable to gradient-based attacks).
  - No need for the attacker to know the model.

#### Evasion Results
- **Training**:
  - 15 hours for 100K trials (~10K games x 10 turns each).
  - Using malware samples from VirusShare.
- **Cross-Evasion**:
  - Detection rate on VirusTotal (average):
    - Original: 35/62
    - Evaded: 25/62

#### Summary
- **Gym-Malware**: [GitHub Repository](https://github.com/endgameinc/gym-malware)
- **Key Features**:
  - No knowledge of the target model needed.
  - Manipulates raw binaries; no malware source or disassembly required.
  - Produces variants that preserve the format and function of the original.
- **Future Work**:
  - Improve the system.
  - Machine learning models are effective even under attack.
  - All machine learning models have blind spots.

#### Thank You!
**Hyrum Anderson**  
Technical Director of Data Science  
Email: [PI:EMAIL](mailto:PI:EMAIL)  
Twitter: [@drhyrum](https://twitter.com/drhyrum)  
LinkedIn: [Hyrum Anderson](https://www.linkedin.com/in/hyrumanderson)  

**Co-contributors**:
- Anant Kharkar, University of Virginia
- Bobby Filar, Endgame
- Phil Roth, Endgame