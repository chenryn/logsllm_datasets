analysis [26] of the function detects that it tampers with the stack. Our call-stack
analysis emulates the function’s stack operations to detect whether the function
alters its return address, either by overwriting the address or by imbalancing the
call stack. Figure 2 illustrates two call-stack tampering tricks used by the ASPack
packer that are easily detected by our analysis. Figure 2a shows an instruction
sequence that transfers control to ADDR upon executing the return instruction,
while Figure 2b shows a sequence that increments the return address of a function
by a single byte. In both cases, our call-stack analysis informs the parser of the
actual return address of the called function, and the byte immediately following
the call site is not parsed as code.
5 Dynamic Capture
Having found and analyzed as much of the code as possible by traversing the
program’s statically analyzable control ﬂow, we turn to dynamic capture tech-
niques to ﬁnd code that is not statically analyzable. Statically un-analyzable
code includes code that is present in the binary but is reachable only through
pointer-based address calculations, and code that is not initially present because
it is dynamically unpacked. Our approach to both problems lies in monitoring
those control transfers whose targets are either unknown or invalid when we
originally parse them. More precisely, we use dynamic capture instrumentation
to monitor the execution of instructions that meet one of the following criteria:
– Control transfer instructions that use registers or memory values to deter-
mine their targets. Obfuscated programs often used indirect jump and call
instructions to hide code from static analysis. For example, the FSG packer
has an indirect function call for every 16 bytes of bootstrap code. We deter-
mine whether indirect control transfers leave analyzed code by resolving their
targets at run-time with dynamic instrumentation. In the case of indirect call
instructions, when our parser cannot determine the call’s target address, it
also cannot know if the call will return, so it conservatively assumes that it
does not; our instrumentation allows us to trigger parsing both at call target
and after the call site, if we can determine that the called function returns.
326
K.A. Roundy and B.P. Miller
– Return instructions of possibly non-returning functions. Return instructions
are designed to transfer execution from a called function to the caller at the
instruction immediately following the call site; unfortunately they can be
misused by tampering with the call stack. As detailed in Section 4, during
parsing we attempt to determine whether called functions return normally
so that we can continue parsing after call sites to those functions. If our
analysis is inconclusive we instrument the function’s return instructions.
– Control transfer instructions into invalid or uninitialized memory regions.
Control transfers to dynamically unpacked code can appear this way, as code
is often unpacked into uninitialized (e.g., UPX) or dynamically allocated
memory regions (e.g., NSPack). Our instrumentation of these control transfer
instructions executes immediately prior to the transfer into the region, when
it must contain valid code, allowing us to analyze it before it executes.
– Instructions that terminate a code sequence by reaching the end of initialized
memory. Some packer tools (e.g., UPack) and custom-packed malware (e.g.,
Rustock [12]) transition to dynamically unpacked code without executing a
control transfer instruction. These programs map code into a larger memory
region so that a code sequence runs to the end of initialized memory without
a terminating control transfer instruction. The program then unrolls the re-
mainder of the sequence into the region’s uninitialized memory so that when
it is invoked, control ﬂow falls through into the unpacked code. We trigger
analysis of the unpacked instructions by instrumenting the last instruction
of any code sequence that ends without a ﬁnal control transfer instruction.
Our dynamic capture instrumentation supplies our parser with entry points into
un-analyzed code. Before extending our analysis by parsing from these new entry
points, we determine whether the entry points represent un-analyzed functions
or if they are extensions to the body of previously analyzed functions. We treat
call targets as new functions, and treat branch targets as extensions to existing
functions (unless the branch instruction and its target are in diﬀerent memory
regions). The target of a non-obfuscated return instruction is always immediately
preceded by an analyzed call instruction, in which case we parse the return
instruction’s target as an extension of the calling function. When a return target
is not immediately preceded by a call instruction, we conclude that the call stack
has been tampered with and parse the target as a new function.
Cost issues arise from our use of the Dyninst instrumentation library [22]
because it monitors programs from a separate process that contains the analy-
sis and instrumentation engine. The problem is that our code-discovery instru-
mentation requires context switching between the two processes to determine
whether monitored control transfers lead to new or analyzed code. We reduce
this overhead by caching the targets of these instructions in the address space of
the monitored program, and context switching to Dyninst only for cache misses.
6 Response to Overwritten Code
Code overwrites cause signiﬁcant problems for binary analysis. Most analysis tools
cannot analyze overwritten code because they rely on static CFG representations
Hybrid Analysis and Control of Malware
327
(a) The monitored program over-
writes an instruction. We have
removed write permissions from
code pages to cause code over-
writes to raise access rights vio-
lations.
(b) A correct but ineﬃcient ap-
proach that updates the CFG in
response to each code write, trig-
gering major processing.
(c) Our optimized handler in-
struments the loop’s exit points
with callbacks to our CFG up-
dater and restores write permis-
sions to the overwritten page.
(d) When writing completes, the
instrumentation at a loop exit
triggers a callback to our CFG
updater.
Fig. 3. Our approach to detecting code writes is shown in Figure 3a, alternative meth-
ods for responding to code writes are shown in Figures 3b and 3c-3d.
of the code. Code overwrites cause problems for CFGs by simultaneously invali-
dating portions of the CFG and introducing new code that has yet to be analyzed.
We have developed techniques to address this problem by updating the program’s
CFG and analyzing overwritten code before it executes.
To analyze overwritten code before it executes, we can either detect the mod-
iﬁed instructions immediately prior to their execution, by checking whether the
bytes of each executed instruction have changed [44], or detect writes to code
as soon as they occur, by monitoring write operations to analyzed code re-
gions. Monitoring each instruction for changes is expensive because it requires
single-step execution of the program. Fortunately, we can eﬃciently detect write
operations that modify code by adapting DIOTA’s techniques for intercepting
writes to executable code regions [29]. DIOTA monitors writes to all memory
328
K.A. Roundy and B.P. Miller
pages that are writable and executable by removing write permissions from those
pages, thereby causing writes that might modify code to raise an access-rights
violation that DIOTA intercepts. As illustrated in Figure 3a, we have adapted
this mechanism for packed binaries, which typically mark most of their memory
as writable and executable, by removing write permissions only from memory
pages that contain analyzed code.
A na¨ıve approach based on monitoring writes to code pages might respond to
write attempts by emulating each write and immediately updating the analysis,
as shown in Figure 3b. Doing so is undesirable for eﬃciency reasons. Large code
regions are often overwritten in small increments, and updating the CFG in
response to every code write is unnecessarily expensive, as the analysis does not
need to be updated until the overwritten code is about to execute. Instead, we
catch the ﬁrst write to a code page but allow subsequent writes, delaying the
update to the window between the end of code overwriting and the beginning of
modiﬁed code execution. This delayed-update approach divides our response to
code overwrites into two components that we now describe in detail: a handler
for the access-rights violation resulting from the ﬁrst write attempt, and a CFG
update routine that we trigger before the modiﬁed code executes.
6.1 Response to the Initial Access-Rights Violation
When a write to a code page results in an access-rights violation, our ﬁrst task is
to handle the exception. We disambiguate between real access-rights violations
and protected-code violations by looking at the write’s target address. Protected-
code violations are artiﬁcially introduced by our code-protection mechanism and
we handle them ourselves to hide them from the monitored program. For real
access-rights violations we apply the techniques of Section 7 to analyze the reg-
istered handler and pass the signal or exception back to the program.
Our handler also decides when to update the CFG, attempting to trigger
the update after the program has stopped overwriting its code, but before the
modiﬁed code executes. A straightforward approach would be to restore write
permissions for the overwritten page and remove execute permissions from the
page, thereby causing a signal to be raised when the program attempts to execute
code from the overwritten page (similar to the approach taken by OmniUnpack
[30], Justin [20], and Saﬀron [41]). Unfortunately, this approach fails in the com-
mon case that the write instruction writes repeatedly to its own page, when this
approach eﬀectively devolves into single-step execution. Instead, we apply the
techniques shown in Figures 3c and 3d to detect the end of overwriting, and delay
updating the CFG until then. We perform inter-procedural loop analysis on the
execution context of the faulting write instruction to see if the write is contained
in a loop, in which case we instrument the loop’s exit edges with callbacks to
our CFG update routine. We allow subsequent writes to the write-target’s code
page to proceed unimpeded, by restoring the page’s write permissions. When the
write loop terminates, one of the loop’s instrumented exit edges causes the CFG
update routine to be invoked. We take extra precautions if the write loop’s code
pages intersect with the overwritten pages to ensure that the write loop does not
Hybrid Analysis and Control of Malware
329
modify its own code. We safeguard against this case by adding bounds-check in-
strumentation to all of the loop’s write operations so that any modiﬁcation of
the loop’s code will immediately trigger our CFG update routine.
Our handler’s ﬁnal task is to save a pre-write copy of the overwritten memory
page, so that when writing completes, the CFG update routine can identify the
page’s modiﬁed instructions by comparing the overwritten page to the pre-write
copy of the page. If the loop writes to multiple code pages, the ﬁrst write to each
code page results in a separate invocation of our protected-code handler, which
triggers the generation of a pre-write copy of the page, and associates it with
the write loop by detecting that the write instruction lies within it. Our handler
then restores write permissions to the new write-target page and resumes the
program’s execution. When the write loop ﬁnally exits, instrumentation at one
of its exit edges triggers a callback to our CFG update routine.
6.2 Updating the Control Flow Graph
We begin updating our analysis by determining the extent to which the code has
been overwritten. We identify overwritten bytes by comparing the overwritten
code pages with our saved pre-write copies of those pages, and then detemine
which of the overwritten bytes belong to analyzed instructions. If code was over-
written, we clean up the CFG by purging it of overwritten basic blocks and of
blocks that are only reachable from overwritten blocks. We analyze the modiﬁed
code by seeding our parser with entry points into the modiﬁed code regions. We
then inform the analyst of the changes to the program’s CFG so that the new
and modiﬁed functions can be instrumented. After adding our own dynamic cap-
ture instrumentation to the new code, we again remove write permissions from
the overwritten pages and resume the monitored program’s execution.
7 Signal- and Exception-Handler Analysis
Analysis-resistant programs are often obfuscated by signal- and exception-based
control ﬂow. Static analyses cannot reliably determine which instructions will
raise signals or exceptions, and have diﬃculty ﬁnding signal and exception han-
dlers, as they are usually registered (and often unpacked) at run-time. Current
dynamic instrumentation tools do not analyze signal and exception handlers
[27,29], whereas we analyze them and provide analysis-based instrumentation on
them. This ability to analyze and control the handlers is important on analysis-
resistant binaries because the handlers may perform tasks that are unrelated to
signal and exception handling (e.g., PECompact overwrites existing code).
Signal and exception handlers can further obfuscate the program by redirect-
ing control ﬂow [38]. When a signal or exception is raised, the operating system
gives the handler context information about the fault, including the program
counter value. The handler can modify this saved PC value to cause the OS to
resume the program’s execution at a diﬀerent address. As shown in step 3 of
Figure 4, this technique is used by the “Yoda’s Protector” packer to obfuscate
330
K.A. Roundy and B.P. Miller
1:A store to address 0 causes an access vio-
lation and the OS saves the fault’s PC on
the call stack.
1a: The OS informs the attached SD-
Dyninst process of the exception.
1b: SD-Dyninst analyzes the registered
instruments its
exception handler,
exit points, and returns to the OS.
2:The OS calls the program’s exception
handler.
3:The handler overwrites the saved PC with
the address of the program’s original en-
try point.
3a: Instrumentation at the handler’s exit
point invokes SD-Dyninst.
3b: SD-Dyninst detects the modiﬁed PC
value, analyzes the code at that ad-
dress, and resumes the handler’s ex-
ecution.
4:The handler returns to the OS.
5:The OS resumes the program’s execution
at the modiﬁed PC value, which is the
program’s original entry point.
Fig. 4. The normal behavior of an exception-based control transfer used by Yoda’s
Protector is illustrated in steps 1-5. Steps 1a-1b and 3a-3b illustrate SD-Dyninst’s
analysis of the control transfer through its attached debugger process.
its control transfer to the program’s original entry point (OEP) [16]. Yoda’s Pro-
tector raises an exception, causing the OS to invoke Yoda’s exception handler.
The handler overwrites the saved PC value with the address of the program’s
OEP, causing the OS to resume the program’s execution at its OEP.
Analyzing Signal- and Exception-Based Control Flow. We ﬁnd and ana-
lyze handlers by intercepting signals and exceptions at run-time. Signal and ex-
ception interception is possible whether we observe the malware’s execution from
a debugger process or virtual machine monitor (VMM). In our current implemen-
tation, SD-Dyninst is apprised of raised signals and exceptions through standard
use of the debugger interface. A VMM-based implementation would automat-
ically be apprised of signals and exceptions, and would use VM-introspection
techniques to detect which of them originate from malicious processes [19].
As shown in Figure 4, upon notiﬁcation of the signal or exception, we analyze
and instrument the program’s registered handlers. We ﬁnd handlers in Windows
programs by traversing the linked list of structured exception handlers that is
on the call stack of the faulting thread. Finding handlers is even easier in Unix-
based systems because only one handler can be registered to each signal type.
We analyze the handler as we would any other function, and mark the faulting
instruction as an invocation of the handler.
We guard against the possibility that the handler will redirect control ﬂow
by instrumenting it at its exit points. After analyzing the handler, but before
it executes, we insert our exit-point instrumentation (step 1b of Figure 4). We
inform the analyst’s tool of the signal or exception and of the newly discovered
handler code so that it can add its own instrumentation. We then return control
Hybrid Analysis and Control of Malware
331
to the OS, which invokes the program’s exception handler. When the handler is
done executing, our exit-point instrumentation triggers a callback to our anal-
ysis engine (steps 3a-3b of Figure 4), where we check for modiﬁcations to the
saved PC value. If we detect a change, we analyze the code at the new address,
instrument it, and allow the analyst to insert additional instrumentation.
8 Experimental Results
We evaluated our techniques by implementing them in SD-Dyninst and applying
it to real and synthetic malware samples. We show that we can eﬃciently ana-
lyze obfuscated, packed, and self-modifying code by applying our techniques to
the binary packer tools that are most heavily used by malware authors, compar-
ing these results to two of the most eﬃcient existing tools. We demonstrate the
usefulness of our techniques by using SD-Dyninst to create a malware analysis
factory that we apply to a large batch of recent malware samples. Our analy-
sis factory uses instrumentation to construct annotated program CFG’s and a
stackwalk at the program’s ﬁrst socket communication.
8.1 Analysis of Packer Tools
Packed binaries begin their execution in highly obfuscated metacode that is often
self-modifying and usually unpacks itself in stages. The metacode decompresses
or decrypts the original program’s payload into memory and transfers control to
the payload code at the original program’s entry point.
Table 1 shows the results of applying our techniques to the packer tools that
are most often used to obfuscate malicious binaries, as determined by Panda
Research for the months of March and April 2008, the latest dates for which
such packer statistics were available [10]. We do not report results on some of
these packers because they incorporate anti-tampering techniques such as self-
checksumming, and SD-Dyninst does not yet incorporate techniques for hiding
its use of dynamic instrumentation from anti-tampering. We excluded NullSoft’s
installer tool (with 3.58% market share) from this list because it can be used to
create binaries with custom code-unpacking algorithms; though we can handle
the analysis-resistance techniques contained in most NullSoft-based packers, we
cannot claim success on all of them based on successful analysis of particular
packer instances. We also excluded the teLock (0.63% market share) and the
Petite (0.25% market share) packer tools, with which we were unable to produce
working binaries. The total market share of the packer tools listed by Panda
Research is less than 40% of all malware, while at least 75% of malware uses some
packing method [8,50]. This discrepancy is a reﬂection both of the increasing
prevalence of custom packing methods and a limitation of the study, which
relied on a signature-based tool to recognize packer metacode [11]; most custom
packers are derivatives of existing packer tools, which are often modiﬁed with
the express purpose of breaking signature-based detection.
In Table 1 we divide the execution time of the program into pre- and post-
payload execution times, representing the time it takes to execute the binaries’
332
K.A. Roundy and B.P. Miller
Table 1. Our analysis techniques applied to the most prevalent packer tools used to
obfuscate malware. We analyzed all of the packed binaries that do not employ anti-
tampering techniques.
code
ing
Time (seconds)
Pre-payload
instr. uninstr.
0.02
0.50
1.24
0.02
Post-payload
instr. uninstr.
0.02
0.02
2.80
2.81
yes
yes
yes
PolyEnE
EXECryptor
Themida
PECompact
Packer
UPX
Malware Over- Anti
market writes tamper- Pre-
exec’n
share
23.44
9.45%
6.21%
22.55
4.06%
2.95%