dependent on individual programs. Also, there are additional
factors that might affect dependability. Program loops as a
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    85
reachable labelsizeextcallpath lengthcmpreached labelsundiscovered neighboursindcallnew covFeature0.00.10.20.30.40.5Gini Importance024681012141618202224Time (hour)0500010000150002000025000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)010002000300040005000600070008000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)01000200030004000500060007000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)0100020003000400050006000700080009000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)01000200030004000500060007000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)0100020003000400050006000700080009000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)0200400600800100012001400Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-EN024681012141618202224Time (hour)050010001500200025003000Branch coverageAFLAFLFastAngoraQSYMSAVIORMEUZZ-OLMEUZZ-RFMEUZZ-ENBuilding machine learning models is a valuable but time-
consuming task. It is reasonable to build and reuse models
where possible. By reusing a model, one can improve gener-
alization, speed up training, as well as improving the model
accuracy. Also, reusability can be good evidence that our model
correctly captured what kind of inputs have higher utility when
testing the target programs. Hence, we test the reusability of the
learned models obtained via the previous fuzzing experiments.
We conduct an experiment in which we use a pre-trained
model for fuzzing the same target program and compare the
coverage difference. We make the following two changes
in the experiment performed in § 6.2: (i) the initial seeds are
replaced by a naive input that only consists 4 whitespaces; and
(ii) all MEUZZ variants are initialized with the models they
learned in the effectiveness test (with valid initial seeds).
Figure 7 shows the coverage result with Mann-Whitney
U Test. There are several interesting observations. The most
important one is that the MEUZZ variants start performing
well even at the beginning of fuzzing compared with when
there is no model initialization. We believe this improvement
is brought by the initial models. Additionally, “pure-AFL”
fuzzers do not perform well with this naive initial seed. For in-
stance, in tcpdump, AFL and AFLFast only generate 6 inputs in
total after 24 hours of fuzzing (see Figure 7a). On the contrary,
systems augmented with other input generation techniques
such as concolic execution and taint analysis can generate
more inputs and consequently can explore signiﬁcantly more
code. Lastly, MEUZZ-RF outperforms its peers in djpeg, and
its p-value indicates the improvement is signiﬁcant (< 0.05),
suggesting the non-linear model works better on djpeg.
6.5 Model Transferability
Figure 8: This heat map shows Coverage improvement with model
initialization for MEUZZ-OL over vanilla MEUZZ-OL. Y-axis is the
tested programs, X-axis is the models used for initialization. Each
cell shows the relative coverage comparison (%). The diagonal values
show the coverage improvement on each program after initializing
MEUZZ with model learn from the same program (reusability).
Model transferability is shown in 7 out of the 8 programs.
To further evaluate the model reusability explained in the
previous section, we conduct a cross-program experiment to
determine whether a model trained on one type of program
will transfer well to fuzzing a new program. This is known
as transfer learning in the ML ﬁeld [38]. As far as we know, no
prior research has attempted to show this invaluable analysis
in fuzzing [42].
In this experiment, we augment MEUZZ with a pre-trained
model from one program and compare the result of the fuzzer
on different programs with a baseline. Our baseline is the cov-
erage result from the learning effectiveness experiment (§ 6.2),
in which we use valid seeds to bootstrap fuzzing without model
initialization. We choose MEUZZ-OL as the representative
of our system to measure this transferability experiment. We
then fuzz each program using MEUZZ-OL initialized with the
8 pre-learned models; the models are ﬁxed afterward.
Figure 8 visualizes the comparative coverage improvements
(i.e., percentage) produced by each fuzzing conﬁguration. The
Y-axis shows the tested program and the X-axis shows the
programs by which the models are built. This result shows
three interesting ﬁndings.
First, MEUZZ-OL observes 7.1% more code coverage on
average when it is tested on the same program it is initialized
with. The amount of improvement for each program is shown
in the diagonal of Figure 8, from top left to bottom right.
Note that these models are only learned in 24 hours from
previous experiments; we expect to see more improvement in
continuous fuzzing services (e.g., [12]). This again conﬁrms
that the previously learned models are reusable.
Second, MEUZZ-OL observes improvement in 38 out
of 56 cross-testing cases, which shows 67.9% success rate
when the model is transferred from a program to another
program. Among them, 10 cases see more than 10% coverage
improvement. Such improvement also indicates that the
program-agnostic requirement is satisﬁed in MEUZZ.
Last but not least, we notice different programs have
different “sensitivity” towards the transferred models. For
instance, almost all the transferred models can strengthen
fuzzing readelf, tiff2pdf, tiff2ps and djpeg programs,
among which readelf sees the highest
improvement.
Interestingly, readelf achieves even higher improvement
when using the tcpdump model than the readelf model by
itself. However, other programs are only partially accepting
foreign models. For instance, the model of tcpdump can
outperform almost all of the programs, while none of the other
seven external models can improve its fuzzing yields.
Two main reasons can justify the aforementioned obser-
vation, namely the number of data points as well as feature
importance. When there is more data, the model can better
generalize [31]. For instance, the tcpdump model contains a
higher number of seeds compared with others (see Figure 1),
which justiﬁes the effectiveness of the transferred model built
from the tcpdump program. We also compared the importance
of the features of each program (see Appendix D). The shape
86    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
tcpdumpobjdumpreadelflibxmltiff2pdftiff2psjasperdjpegtcpdumpobjdumpreadelflibxmltiff2pdftiff2psjasperdjpeg2.8-3.5-2.3-4.5-3.6-1.2-3.3-4.5-23.14.7-0.880.1-1.81.9-1.25730229.9142034342.6-2.6-0.229.117-1.8-1.16.33.54.93.34.451.81.64.53.33.27.9106.26.63.92.23.67.28.8-0.173.41.76.7-0.340.810.410.21.31.3-1.81.584048Table 3: The table shows the unique bugs found by all evaluated
fuzzers.
Program AFL AFLFast Angora QSYM Savior MEUZZ All unique bugs
tcpdump
objdump
readelf
tiff2pdf
tiff2ps
jasper
djpeg
Total
14
2
3
1
1
2
9
32
13
2
2
1
2
1
7
28
12
5
5
1
2
0
7
32
11
2
5
2
5
3
9
37
12
8
4
2
4
1
9
40
14
6
4
2
6
6
9
47
14
9
6
2
6
8
9
54
of the ﬁnal importance chart in tcpdump diverges more from
the rest of the programs. Moreover, the values of some features
such as Indirect Call and Path Length are higher than other
programs. By looking at these statistics as well as checking
the source code of tcpdump we noticed tcpdump is designed
with heavier use of function handlers for different types of
network packets and recursive loops for parsing packet ﬁelds.
While other models contain different feature value distribution
as well as fewer data points, which justify the failure of using
them to improve fuzzing tcpdump.
6.6 Discovered Bugs
To prove the effectiveness of our system in discovering new
bugs, we performed various analyses. We manually analyzed
all of the reported undeﬁned behaviors and crashes. UBSan
reports a large amount of undeﬁned behaviors; however, the
majority of them are deemed benign after our triage process.
We also triage additional bugs with the help of ASAN [1] and
LeakSAN [7].
Table 3 shows our triage result for all the fuzzers. In total,
54 unique bugs were uncovered. MEUZZ outperforms other
fuzzers and found 47 unique bugs, which supports the fact
that higher code coverage correlates to a higher number of
triaged bugs. Due to space limit, we present more detailed
triage result and one of the discovered bugs only found by
MEUZZ in Appendix B. This result shows MEUZZ is more
effective in terms of ﬁnding bugs than state-of-the-art systems
with manually crafting heuristics.
7 Related Work
7.1 ML for Fuzzing
Despite the promising potential to improve fuzzing, the
application of ML has not been very-well investigated in
the past and only a few research have leveraged ML. ML
can be integrated into various stages of fuzzing, from input
generation to crash categorization.
Input generation: The most intelligent stage of fuzzing
has been the input generation stage so far, thanks to genetic
algorithms. Deep learning (DL) techniques have been recently
applied to input generation for both mutation/generation-based
fuzzing. Such approaches [30, 40, 47] use various neural
network methods to learn the patterns that exist in input
ﬁles and then identify the likely input forms to trigger new
coverage. Similarly, reinforcement learning (RL) [20] can
learn input grammar for generation-based fuzzers.
Crash analysis: Automating the analysis of outputs/crashes
generated by fuzzers is another ML application. For instance,
ML can be used to categorize crashes by identifying the
root cause of them. This helps remove duplicate outputs and
therefore reduces manual analysis effort [26]. Or another
example is employing ML to predict whether the reported
crashes by fuzzers are exploitable [52].
To the best of our knowledge, there has not been any
research that practices ML for seed selection. In general, the
practicality of ML for fuzzing has not been shown clearly in the
past due to the uncertainty about reusability and transferability.
7.2 Seed Scheduling Heuristics
Scheduling in fuzzing: FuzzSim [51] models the seed
scheduling problem as a weighted coupon collector problem
and found out that scheduling can have a direct impact on
fuzzing campaign yields. Later, in grey-box fuzzing, AFL [33]
implements a scheduling algorithm that consists of simple
heuristics such as preferring ﬁrst seed with new coverage,
and with smaller size and less execution time. This simple
algorithm is later improved by Fairfuzz [34] and AFLFast [19]
which steer the fuzzer towards less explored paths.
Scheduling in hybrid testing: As hybrid testing becomes
more popular, seed scheduling also becomes a research topic.
Driller [49] implements a random scheduling algorithm, while
QSYM [13] implements heuristics similar to AFL. Later,
DigFuzz [55] shows the ineffectiveness of random scheduling
and proposes a Monte-Carlo model to predict the difﬁculty
of each path explored by the fuzzer by far, and send the most
difﬁcult ones to concolic executor. SAVIOR [25], on the other
hand, uses bug-driven scheduling heuristics. By selecting
the seeds that can reach more sanitizer instrumentations, it
triggers more bugs in the given timeframe than other fuzzers.
Compared with these approaches, MEUZZ applies machine
learning techniques that can learn a utility prediction model,
which is adaptive to the program being tested. As our
evaluation suggests, this approach is more scalable and more
performant than the manual-crafting scheduling heuristics.
8 Conclusion
We present MEUZZ, a hybrid fuzzing system featuring
machine learning and data-driven seed scheduling. Theo-
retically, MEUZZ is more generalized than systems using
ﬁxed seed selection heuristics. For effective integration of
machine learning workloads into the online hybrid fuzzing
loop, MEUZZ follows the requirements of being utility
relevant, online friendly and program agnostic for its feature
engineering and label inference. Our evaluation shows that
MEUZZ outperforms state-of-the-art fuzzers in both code
coverage and bug discovery. In addition, the learned models
demonstrate good reusability and transferability, making it
more practical to apply machine learning to hybrid fuzzing.
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    87
Acknowledgment
[15] Undeﬁned behavior
The authors would like to thank the anonymous reviewers for
their insightful comments. This project was supported by the
National Science Foundation (Grant#: CNS-1748334) and
the Ofﬁce of Naval Research (Grant#: N00014-17-1-2891).
Any opinions, ﬁndings, and conclusions or recommendations
expressed in this paper are those of the authors and do not
necessarily reﬂect the views of the funding agencies.
References
[1] Addresssanitizer. https://clang.llvm.org/docs/
AddressSanitizer.html.
[2] Aﬂ technical details. http://lcamtuf.coredump.cx/
afl/technical_details.txt.
[3] angr/tracer: Utilities for generating dynamic traces.
https://github.com/angr/tracer.
[4] Announcing
oss-fuzz:
Continuous
open
source
software.
for
//testing.googleblog.com/2016/12/
announcing-oss-fuzz-continuous-fuzzing.
html.
fuzzing
https:
[5] Binutils test cases. https://github.com/mirrorer/
afl/tree/master/testcases/others/elf.
sanitizer
doc-
http://clang.llvm.org/
clang
9
-
umentation.
docs/UndefinedBehaviorSanitizer.html#
ubsan-checks.
[16] When does deep learning work better than svms or ran-
dom forests? https://www.kdnuggets.com/2016/
04/deep-learning-vs-svm-random-forest.html,
04 2016.
[17] Cyber grand shellphish. http://www.phrack.org/
papers/cyber_grand_shellphish.html, 2017.
[18] Donald A Berry and Bert Fristedt. Bandit problems:
sequential allocation of experiments (monographs on
statistics and applied probability). London: Chapman
and Hall, 5:71–87, 1985.
[19] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoud-
hury. Coverage-based greybox fuzzing as markov chain.
In Proceedings of the 2016 ACM SIGSAC Conference
on Computer and Communications Security, pages
1032–1043. ACM, 2016.
[20] Konstantin Böttinger, Patrice Godefroid, and Rishabh
CoRR,
Deep reinforcement fuzzing.
Singh.
abs/1801.04589, 2018.
[21] Léon Bottou. Online learning and stochastic approxima-
[6] Darpa cyber grand challenge. http://archive.darpa.
tions. On-line learning in neural networks, 17(9):142.
mil/cybergrandchallenge/.
[7] Leaksanitizer.
https://clang.llvm.org/docs/
Classiﬁcation and regression trees. 1984.
[22] L Breiman, JH Friedman, R Olshen, and CJ Stone.
LeakSanitizer.html.
[8] libfuzzer – a library for coverage-guided fuzz testing.
https://llvm.org/docs/LibFuzzer.html.
[9] Libjpeg test cases. https://github.com/mirrorer/
afl/tree/master/testcases/images/jpeg.
[10] Libtiff test cases. https://github.com/mirrorer/
afl/tree/master/testcases/images/tiff.
[11] Libxml test cases. https://github.com/mirrorer/
afl/tree/master/testcases/others/xml.