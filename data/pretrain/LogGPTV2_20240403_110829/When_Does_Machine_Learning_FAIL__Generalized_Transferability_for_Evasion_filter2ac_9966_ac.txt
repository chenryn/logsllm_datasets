mension.
These defenses may come with inaccurate assessments
for the adversarial capabilities and implicit assumptions.
For example, distillation limits adversaries along the
F and A dimensions but employing a different attack
strategy could bypass it [9]. On poisoning attacks, the
RONI [34] defense assumes training set secrecy, but does
not evaluate the threat posed by attackers with sufﬁcient
knowledge along the other dimensions. As our results
will demonstrate, this implicit assumption allows attack-
ers to bypass the defense while remaining within the se-
crecy bounds.
The results for the evaluated defenses are found in Ta-
ble 2. The detailed evaluation process for each of these
studies can be found in our technical report [43].
4 The StingRay Attack
Reasoning about implicit and explicit assumptions in
prior defenses allows us to design algorithms which ex-
ploit their weaknesses.
In this section, we introduce
StingRay, one such attack that achieves targeted poison-
ing while preserving overall classiﬁcation performance.
StingRay is a general framework for crafting poison sam-
ples.
At a high level, our attack builds a set of poison in-
stances by starting from base instances that are close to
the target in the feature space but are labeled as the de-
sired target label yd, as illustrated in the example from
Figure 2. Here, the adversary has created a malicious
Android app t, which includes suspicious features (e.g.
the WRITE_CONTACTS permission on the left side of the
ﬁgure), and wishes to prevent a malware detector from
ﬂagging this app. The adversary, therefore, selects a be-
nign app xb as a base instance. To craft each poison in-
stance, StingRay alters a subset of a base instance’s fea-
tures so that they resemble those of the target. As shown
on the right side of Figure 2, these are not necessarily
the most suspicious features, so that the crafted instance
will likely be considered benign. Finally, StingRay ﬁl-
ters crafted instances based on their negative impact on
instances from S′, ensuring that their individual effect
on the target classiﬁcation performance is negligible.
The sample crafting procedure is repeated until there
are enough instances to trigger the misclassiﬁcation of
t. Algorithm 1 shows the pseudocode of the attack’s two
general-purpose procedures .
We describe concrete implementations of our attack
against four existing applications: an image recognition
system, an Android malware detector, a Twitter-based
exploit predictor, and a data breach predictor. We re-
implement the systems that are not publicly available,
using the original classiﬁcation algorithms and the origi-
nal training sets to reproduce those systems as closely as
possible. In total, our applications utilize three classiﬁ-
cation algorithms—convolutional neural network, linear
SVM, and random forest—that have distinct character-
istics. This spectrum illustrates the ﬁrst challenge for
our attack: identifying and encapsulating the application-
speciﬁc steps in StingRay, to adopt a modular design
with broad applicability. Making poisoning attacks prac-
tical raises additional challenges. For example, a na¨ıve
approach would be to inject the target with the desired
label into the training set: h(t) = yd (S.I). However, this
is impractical because the adversary, under our threat
1304    27th USENIX Security Symposium
USENIX Association
I = ∅
repeat
,YS′,t,yt ,yd)
Algorithm 1 The StingRay attack.
1: procedure STINGRAY(S′
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: procedure GETBASEINSTANCE(S′
16:
17:
18:
h = A′(S′)
xb = GETBASEINSTANCE(S′
,YS′ ,t,yt ,yd)
xc = CRAFTINSTANCE(xb,t)
,xc)  Nmin and h(t) = yd) orࢯIࢯ > Nmax
PDR = GETPDR(S′
,YS′ ,I,yd)
if h(t) ≠ yd or PDR < τPDR then
for xb,yb in SHUFFLE(S′
,YS′) do
if D(t,xb) < τD and yb = yd then
,YS′ ,t,yt ,yd)
return ∅
return I
return xb
model, does not control the labeling function. There-
fore, GETBASEINSTANCE works by selecting instances
xb that already have the desired label and are close to the
target in the feature space (S.II).
A more sophisticated approach would mutate these
samples and use poison instances to push the model
boundary toward the target’s class [32]. However, these
instances might resemble the target class too much, and
they might not receive the desired label from the oracle
or even get ﬂagged by an outlier detector. In CRAFTIN-
STANCE, we apply tiny perturbations to the instances
(D.III) and by checking the negative impact NI of crafted
poisoning instances on the classiﬁer (D.IV) we ensure
they remain individually inconspicuous.
Mutating these instances with respect to the target [34]
(as illustrated in Figure 1c) may still reduce the overall
performance of the classiﬁer (e.g. by causing the mis-
classiﬁcation of additional samples similar to the target).
We overcome this via GETPDR by checking the perfor-
mance drop of the attack samples (S.V), therefore ensur-
ing that they remain collectively inconspicuous.
Even so, the StingRay attack adds robustness to the
poison instances by crafting more instances than neces-
sary, to overcome sampling-based defenses (D.VI). Nev-
ertheless, the attack has a sampling budget that dictates
the allowable number of crafted instances (B.VII). A de-
tailed description of StingRay is found in Appendix A.
Attack Constraints. The attack presented above has a
series of constraints that shape its effectiveness. Rea-
soning about them allows us to adapt StingRay to the
speciﬁc restrictions on each application. These span all
three categories identiﬁed in Section 3: Success(S.), De-
fense(D.) and Budget(B.):
S.I h(t) = yd: the desired class label for target
S.II D(t,xb) < τD: the inter-instance distance metric
s(xc,t), where s(⋅,⋅) is a similarity met-
D.III ¯s = 1ࢯIࢯ ∑
xc∈I
ric: crafting target resemblance
D.IV NI < τNI: negative impact of poisoning instances
S.V PDR < τPDR: the perceived performance drop
D.VI ࢯIࢯ ≥ Nmin:
B.VII ࢯIࢯ ≤ Nmax: maximum number of poisoning in-
the minimum number of poison in-
stances
stances
The perceived success of the attacker goals (S.I and
S.V) dictate whether the attack is triggered. If the PDR
is large, the attack might become indiscriminate and the
risk of degrading the overall classiﬁer’s performance is
high. The actual PDR could only be computed in the
white-box setting. For scenarios with partial knowledge,
it is approximated through the perceived PDR on the
available classiﬁer.
The impact of crafted instances is inﬂuenced by the
distance metric and the feature space used to measure
instance similarity (S.II). For applications that learn fea-
ture representations (e.g. neural nets), the similarity of
learned features might be a better choice for minimizing
the crafting effort.
The set of features that are actively modiﬁed by the at-
tacker in the crafted instances (D.III) deﬁnes the target
resemblance for the attacker, which imposes a trade-off
between their inconspicuousness and the effectiveness of
the sample. If this quantity is small, the crafted instances
are less likely to be perceived as outliers, but a larger
number of them is required to trigger the attack. A higher
resemblance could also cause the oracle to assign crafted
instances a different label than the one desired by the at-
tacker.
The loss difference of a classiﬁer trained with and
without a crafted instance (D.IV) approximates the neg-
ative impact of that instance on the classiﬁer. It may be
easy for an attacker to craft instances with a high nega-
tive impact, but these instances may also be easy to detect
using existing defenses.
In practice, the cost of injecting instances in the train-
ing set can be high (e.g. controlling a network of bots in
order to send fake tweets) so the attacker aims to min-
imize the number of poison instances (D.VI) used in
the attack. The adversary might also discard crafted in-
stances that do not have the desired impact on the ML
USENIX Association
27th USENIX Security Symposium    1305
model. Additionally, some poison instances might be ﬁl-
tered before being ingested by the victim classiﬁer. How-
ever, if the number of crafted instances falls below a
threshold Nmin, the attack will not succeed. The max-
imum number of instances that can be crafted (B.VII)
inﬂuences the outcome of the attack. If the attacker is un-
able to ﬁnd sufﬁcient poison samples after crafting Nmax
instances, they might conclude that the large fraction of
poison instances in the training set would trigger suspi-
cions or that they depleted the crafting budget.
Delivering Poisoning Instances.
The mechanism
through which poisoning instances are delivered to the
victim classiﬁer is dictated by the application character-
istics and the adversarial knowledge. In the most general
scenario, the attacker injects the crafted instances along-
side existing ones, expecting that the victim classiﬁer
will be trained on them. For applications where models
are updated over time or trained in mini-batches (such
as an image classiﬁer based on neural networks), the at-
tacker only requires control over a subset of such batches
and might choose to deliver poison instances through
them. In cases where the attacker is unable to create new
instances (such as a vulnerability exploit predictor), they
will rely on modifying the features of existing ones by
poisoning the feature extraction process. The applica-
tions we use to showcase StingRay highlight these sce-
narios and different attack design considerations.
4.1 Bypassing Anti-Poisoning Defenses
In this section, we discuss three defenses against poison-
ing attacks and how StingRay exploits their limitations.
The Micromodels defense was proposed for cleaning
training data for network intrusion detectors [13]. The
defense trains classiﬁers on non-overlapping epochs of
the training set (micromodels) and evaluates them on the
training set. By using a majority voting of the micro-
models, training instances are marked as either safe or
suspicious. Intuition is that attacks have relatively low
duration and they could only affect a few micromodels. It
also relies on the availability of accurate instance times-
tamps.
Reject on Negative Impact (RONI) was proposed
against spam ﬁlter poisoning attacks [3]. It measures the
incremental effect of each individual suspicious training
instance and discards the ones with a relatively signiﬁ-
cant negative impact on the overall performance. RONI
sets a threshold by observing the average negative impact
of each instance in the training set and ﬂags an instance
when its performance impact exceeds the threshold. This
threshold determines RONI’s ultimate effectiveness and
ability to identify poisoning samples. The defense also
requires a sizable clean set for testing instances. We
adapted RONI to a more realistic scenario, assuming no
clean holdout set, implementing an iterative variant, as
suggested in [41], that incrementally decreases the al-
lowed performance degradation threshold. To the best of
our knowledge, this version has not been implemented
and evaluated before. However, RONI remains compu-
tationally inefﬁcient as the number of trained classiﬁers
scales linearly with the training set.
Target-aware RONI (tRONI) builds on the observation
that RONI fails to mitigate targeted attacks [34] because
the poison instances might not individually cause a sig-
niﬁcant performance drop. We propose a targeted variant
which leverages prior knowledge about a test-time mis-
classiﬁcation to determine training instances that might
have caused it. While RONI estimates the negative im-
pact of an instance on a holdout set, tRONI considers
their effect on the target classiﬁcation alone. Therefore
tRONI is only capable of identifying instances that dis-
tort the target classiﬁcation signiﬁcantly. A detailed de-
scription of this defense is available in the technical re-
port [43].
All these defenses aim to increase adversarial costs by
forcing attackers to craft instances that result in a small
loss difference (Cost D.IV). Therefore, they implicitly
assume that poisoning instances stand out from the rest,
and they negatively affect the victim classiﬁer. However,
attacks such as StingRay could exploit this assumption
to evade detection by crafting a small number of incon-
spicuous poison samples.
4.2 Attack Implementation
We implement StingRay against four applications with
distinct characteristics, each highlighting realistic con-
straints for the attacker. We omit certain technical details
for space considerations, encouraging interested readers
to consult the technical report [43].
Image classiﬁcation. We ﬁrst poison a neural-network
(NN) based application for image classiﬁcation, often
used for demonstrating evasion attacks in the prior work.
The input instances are images and the labels correspond
to objects that are depicted in the image (e.g. airplane,
dog, ship). We evaluate StingRay on our own implemen-
tation for CIFAR-10 [24]. 10,000 instances (1/6 of the
data set) are used for validation and testing. In this sce-
nario, the attacker has an image t with true label yt (e.g.
a dog) and wishes to trick the model into classifying it as
a speciﬁc class yd (e.g. a cat).
We implement a neural network architecture that
achieves a performance comparable to other studies [38,
9], obtaining a validation accuracy of 78%. Once the
network is trained on the benign inputs, we proceed to
poison the classiﬁer. We generate and group poison in-
1306    27th USENIX Security Symposium
USENIX Association
stances into batches alongside benign inputs. We deﬁne
γ ∈[0,1] to be the mixing parameter which controls the
ments we varied γ over{0.125,0.5,1.0} (i.e. 4, 16, and
number of poison instances in a batch.
In our experi-
32 instances of the batch are poison) and selected the
value that provided the best attack success rate, keeping
it ﬁxed across successive updates. We then update3 the