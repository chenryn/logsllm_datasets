```
user@docker4:~$ docker exec -it web2-2 ping 172.16.16.129 -c 2
PING 172.16.16.129 (172.16.16.129): 48 data bytes
56 bytes from 172.16.16.129: icmp_seq=0 ttl=64 time=0.642 ms
56 bytes from 172.16.16.129: icmp_seq=1 ttl=64 time=0.777 ms
--- 172.16.16.129 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.642/0.710/0.777/0.068 ms
user@docker4:~$ docker exec -it web2-2 ping 172.16.16.130 -c 2
PING 172.16.16.130 (172.16.16.130): 48 data bytes
56 bytes from 172.16.16.130: icmp_seq=0 ttl=64 time=0.477 ms
56 bytes from 172.16.16.130: icmp_seq=1 ttl=64 time=0.605 ms
--- 172.16.16.130 ping statistics ---
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.477/0.541/0.605/0.064 ms
user@docker4:~$ docker exec -it web2-2 arp -n
Address         HWtype  HWaddress         Flags Mask            Iface
172.16.16.129   ether   02:42:ac:10:10:81 C                     eth0
172.16.16.130   ether   02:42:ac:10:10:82 C                     eth0
user@docker4:~$
```
现在我们知道了覆盖图的工作原理，让我们来谈谈它是如何实现的。用于覆盖传输的机制是 VXLAN。通过查看物理网络上的数据包捕获，我们可以看到容器生成的数据包穿过底层网络:
![How to do it…](img/B05453_03_06.jpg)
在之前截图的截图中，我想指出几个问题:
*   外部 IP 包来源于`docker2`主机(`10.10.10.102`)，目的地为`docker3`主机(`192.168.50.101`)。
*   我们可以看到，外部的 IP 数据包是 UDP，被检测为 VXLAN 封装。
*   **【VNI】**(**VXLAN 网络标识**)或段号为`260`。VNI 在每个子网中都是唯一的。
*   内部帧具有第 2 层和第 3 层报头。第 2 层报头具有容器`web2`的目的地媒体访问控制地址，如前所示。IP 包显示了容器的来源`web1`和容器的目的地`web2`。
Docker 主机使用自己的 IP 接口封装覆盖流量，并通过底层网络将其发送到目标 Docker 主机。键值存储中的信息用于确定给定容器在哪个主机上，以便 VXLAN 封装将流量发送到正确的主机。
您现在可能想知道这个 VXLAN 覆盖的所有配置在哪里。在这一点上，我们还没有看到任何实际上谈论 VXLAN 或隧道的配置。为了提供 VXLAN 封装，Docker 为每个用户定义的覆盖网络创建了一个我称之为*覆盖命名空间*。正如我们在[第 1 章](01.html "Chapter 1. Linux Networking Constructs")、 *Linux 网络构造*中看到的，您可以使用`ip netns`工具与网络命名空间进行交互。然而，由于 Docker 将它们的网络名称空间存储在非默认位置，我们将无法使用`ip netns`工具看到任何名称空间。默认情况下，名称空间存储在`/var/run/netns`中。问题是 Docker 将其网络名称空间存储在`/var/run/docker/netns`中，这意味着`ip netns`工具在错误的位置查看 Docker 创建的网络名称空间。为了解决这个问题，我们可以创建一个`symlink`，将`/var/run/docker/netns/`链接到`/var/run/nents`，如下所示:
```
user@docker4:~$ cd /var/run
user@docker4:/var/run$ sudo ln -s /var/run/docker/netns netns
user@docker4:/var/run$ sudo ip netns list
eb40d6527d17 (id: 2)
2-4695c5484e (id: 1) 
user@docker4:/var/run$ 
```
请注意，定义了两个网络命名空间。覆盖命名空间将用以下语法`x-`来标识，其中`x`是一个随机数。
### 注
输出中显示的另一个命名空间与主机上运行的容器相关联。在下一章中，我们将深入探讨 Docker 如何创建和使用这些名称空间。
所以在我们的例子中，覆盖命名空间是`2-4695c5484e`，但是它是从哪里来的呢？如果我们检查这个名称空间的网络配置，我们会看到它定义了一些不寻常的接口:
```
user@docker4:/var/run$ sudo ip netns exec 2-4695c5484e ip link show
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: br0:  mtu 1450 qdisc noqueue state UP mode DEFAULT group default
    link/ether a6:1e:2a:c4:cb:14 brd ff:ff:ff:ff:ff:ff
11: vxlan1:  mtu 1450 qdisc noqueue master br0 state UNKNOWN mode DEFAULT group default
    link/ether a6:1e:2a:c4:cb:14 brd ff:ff:ff:ff:ff:ff link-netnsid 0
13: veth2@if12:  mtu 1450 qdisc noqueue master br0 state UP mode DEFAULT group default
    link/ether b2:fa:2d:cc:8b:51 brd ff:ff:ff:ff:ff:ff link-netnsid 1
user@docker4:/var/run$ 
```
这些接口定义了我前面提到的覆盖网络名称空间。前面我们看到容器`web2-2`有两个接口。`eth1`接口是 VETH 对的一端，另一端放在`docker_gwbridge`上。前面的覆盖网络名称空间中显示的 VETH 对代表容器的`eth0`接口对的一侧。我们可以通过匹配`interface ID`的 VETH 对的边来证明这一点。请注意，VETH 对的这一端显示另一端的`interface ID`为`12`。如果我们看一下容器`web2-2`，我们会看到它的`eth0`接口有一个`12`的 ID。反过来，容器的接口显示了一对 ID`13`，这与我们在覆盖命名空间中看到的输出相匹配:
```
user@docker4:/var/run$ docker exec web2-2 ip link show
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
12: eth0@if13:  mtu 1450 qdisc noqueue state UP
    link/ether 02:42:ac:10:10:83 brd ff:ff:ff:ff:ff:ff
14: eth1@if15:  mtu 1500 qdisc noqueue state UP
    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff
user@docker4:/var/run$ 
```
现在我们知道了容器的覆盖接口(`eth0`)是如何连接的，我们需要知道进入覆盖名称空间的流量是如何被封装并发送到其他 Docker 主机的。这是通过覆盖命名空间的`vxlan1`界面完成的。此接口有特定的转发条目，描述覆盖层上的所有其他端点:
```
user@docker4:/var/run$ sudo ip netns exec 2-4695c5484e \
bridge fdb show dev vxlan1
a6:1e:2a:c4:cb:14 master br0 permanent
a6:1e:2a:c4:cb:14 vlan 1 master br0 permanent
02:42:ac:10:10:82 dst 192.168.50.101 link-netnsid 0 self permanent
02:42:ac:10:10:81 dst 10.10.10.102 link-netnsid 0 self permanent
user@docker4:/var/run$
```
请注意，我们有两个条目引用了一个媒体访问控制地址和一个目的地。媒体访问控制地址代表覆盖层上另一个容器的媒体访问控制地址，而 IP 地址是容器所在的 Docker 主机。我们可以通过检查其他主机来验证这一点:
```
user@docker2:~$ ip addr show dev eth0
2: eth0:  mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether f2:e8:00:24:e2:de brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.102/24 brd 10.10.10.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::f0e8:ff:fe24:e2de/64 scope link
       valid_lft forever preferred_lft forever
user@docker2:~$
user@docker2:~$ docker exec web1 ip link show dev eth0
7: eth0@if8:  mtu 1450 qdisc noqueue state UP
    link/ether 02:42:ac:10:10:81 brd ff:ff:ff:ff:ff:ff
user@docker2:~$
```
有了这些信息，覆盖命名空间知道，为了到达目的地媒体访问控制地址，它需要将流量封装在 VXLAN 中，并将其发送到`10.10.10.102` ( `docker2`)。
# 隔离网络
用户定义的网络可以支持所谓的内部模式。我们在早期关于创建用户定义网络的食谱中看到了这个选项，但没有花太多时间讨论它。创建网络时使用`--internal`标志可防止连接到网络的容器与任何外部网络通话。
## 做好准备
`docker network`子命令是在 Docker 1.9 中引入的，所以你需要一个 Docker 主机至少运行那个版本。在我们的例子中，我们将使用 Docker 版本。您还需要很好地了解您当前的网络布局，以便我们检查当前的配置时您可以跟进。假设每个 Docker 主机都处于其本机配置。
## 怎么做…
创建用户定义的内部网络非常简单，只需在`network create`子命令中添加选项`--internal`。因为用户定义的网络可以是桥接类型或覆盖类型，所以我们应该理解 Docker 在这两种情况下是如何实现隔离的。
### 创建内部用户定义的桥
定义一个用户定义的桥，并传递给它`internal`标志，以及在主机上给桥一个自定义名称的标志。我们可以用这个命令来完成:
```
user@docker2:~$ docker network create --internal \
-o com.docker.network.bridge.name=mybridge1 myinternalbridge
aa990a5436fb2b01f92ffc4d47c5f76c94f3c239f6e9005081ff5c5ecdc4059a
user@docker2:~$
```
现在，让我们看看 Docker 分配给网桥的 IP 信息:
```
user@docker2:~$ ip addr show dev mybridge1
13: mybridge1:  mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:b5:c7:0e:63 brd ff:ff:ff:ff:ff:ff
    inet 172.19.0.1/16 scope global mybridge1
       valid_lft forever preferred_lft forever
user@docker2:~$
```
根据这些信息，我们现在检查并查看 Docker 在 netfilter 中为这个桥编程了什么。让我们检查一下过滤器表，看看:
### 注
在这种情况下，我使用`iptables-save`语法来查询当前规则。有时，这可能比查看单个表更易读。
```
user@docker2:~$ sudo iptables-save
# Generated by iptables-save v1.4.21
…… 
-A DOCKER-ISOLATION ! -s 172.19.0.0/16 -o mybridge1 -j DROP
-A DOCKER-ISOLATION ! -d 172.19.0.0/16 -i mybridge1 -j DROP 
-A DOCKER-ISOLATION -j RETURN
COMMIT
# Completed on Tue Oct  4 23:45:24 2016
user@docker2:~$
```
在这里，我们可以看到 Docker 增加了两个规则。第一种说法是，任何不是来自网桥子网并且正在离开网桥接口的流量都应该被丢弃。这可能很难理解，所以用一个例子来思考这个问题是最容易的。假设您网络上的一台主机试图访问此桥上的某个内容。该流的源 IP 地址不在网桥子网中，这满足了规则的第一部分。它也将试图脱离(或进入)`mybridge1`满足规则的第二部分。该规则有效地阻止了所有入站通信。
第二个规则寻找在网桥子网中没有目的地并且具有网桥入口接口的流量`mybridge1`。在这种情况下，容器的 IP 地址可能是 172.19.0.5/16。如果它试图谈论它的本地网络，目的地将不在符合规则第一部分的`172.19.0.0/16`中。当它试图离开网桥进入外部网络时，它将匹配规则的第二部分，因为它将进入`mybridge1`界面。该规则有效地阻止了所有出站通信。
在这两条规则之间，不允许车辆进出这座桥。然而，这并不妨碍同一桥上的容器之间的容器到容器的连接。
应该注意的是，Docker 将允许您在对内部桥运行容器时指定发布(`-P`)标志。但是，不会映射任何端口:
```
user@docker2:~$ docker run --net=myinternalbridge --name web1 -d -P jonlangemak/web_server_1
b5f069a40a527813184c7156633c1e28342e0b3f1d1dbb567f94072bc27a5934
user@docker2:~$ docker port web1
user@docker2:~$
```
### 创建内部用户定义的覆盖
创建内部覆盖遵循相同的过程。我们只需将`--internal`标志传递给`network create`子命令。然而，在覆盖网络的情况下，隔离模型要简单得多。我们可以如下创建内部覆盖网络:
```
user@docker2:~$ docker network create -d overlay \
--subnet 192.10.10.0/24 --internal myinternaloverlay
1677f2c313f21e58de256d9686fd2d872699601898fd5f2a3391b94c5c4cd2ec
user@docker2:~$
```
一旦创建，它实际上与非内部覆盖没有什么不同。当我们在内部覆盖层上运行容器时，差别就来了:
```
user@docker2:~$ docker run --net=myinternaloverlay --name web1 -d -P jonlangemak/web_server_1
c5b05a3c829dfc04ecc91dd7091ad7145cbce96fc7aa0e5ad1f1cf3fd34bb02b
user@docker2:~$
```
检查容器接口配置，可以看到容器只有一个接口，是覆盖网络的成员(`192.10.10.0/24`)。通常将容器连接到外部连接的`docker_gwbridge` ( `172.18.0.0/16`)网络的接口缺失:
```
user@docker2:~$ docker exec -it web1 ip addr
1: lo:  mtu 65536 qdisc noqueue state UNKNOWN qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
11: eth0@if12:  mtu 1450 qdisc noqueue state UP
    link/ether 02:42:c0:0a:0a:02 brd ff:ff:ff:ff:ff:ff
    inet 192.10.10.2/24 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:c0ff:fe0a:a02/64 scope link
       valid_lft forever preferred_lft forever
user@docker2:~$ 
```
覆盖网络本质上是隔离的，因此需要`docker_gwbridge`。没有将容器接口映射到`docker_gwbridge`意味着没有办法在提供隔离的覆盖网络中进出。