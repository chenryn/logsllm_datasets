visualized as placing the kernel at different locations of the
input data. At each location, a sum of element-wise product is
computed between the kernel and corresponding data values
within the kernel window, as shown in Fig. 2.
Dot Product. The last convolutional
layer is typically
connected with the fully-connected layer, which computes the
weighted sum, i.e., a dot product between the weight matrix
w of size no × ni and a ﬂatten feature vector of size ni × 1.
The output is a vector with the size of no × 1. Each element
of the output vector is calculated as a sum of element-wise
product between one row of weight matrix and the ﬂatten
feature vector, as shown in Fig. 2.
Activation. Nonlinear activation is applied to convolutional
and weighted-sum outputs in an elementwise manner, as shown
in Fig. 2. The commonly used activation functions include
ReLu, f (x) = max{0, x}; sigmoid, f (x) = 1
1+e−x ; and tanh,
f (x) = e2x−1
e2x+1 . The last layer uses the softmax function f (x) =
ex(cid:80)
i ex(i) to normalize the output into a probability vector.
Pooling. Pooling conducts downsampling to reduce dimen-
sionality. In this work, we consider Mean pooling, which is
implemented in CryptoNets and also commonly adopted in
state-of-the-art CNNs. It splits a feature map into regions and
averages the regional elements. Compared to max pooling
(another pooling function which selects the maximum value
Fig. 1. An overview of the MLaaS system.
Fig. 2. An overview of CNN model.
3
ClientServerNeural networkSensitive dataPrivate predictionchestX-raytuberculosisInput Image (uwxuhxci)ConvolutionActivationPoolingDot productActivation...Dot productLinear transformation Nonlinear transformation Output classes...CatDogFrogConvolutionDot ProductActivationPoolingabdefghijklmnopqrsABCDEFGHtaA+bB+eC+fD+kE+lF+nG+oHInputKernelOutputconvabdefghijWeight matrixABCInputmOutputweightedsumaA+bB+dC...aInputf(a)b=f(a)OutputabefInputda+b+e+f4meanpoolingOutputabefInputdmax(a,b,e,f)maxpoolingOutputin each region), authors in [77] have claimed that while the
max and mean pooling functions are rather similar, the use of
mean pooling encourages the network to identify the complete
extent of the object, which builds a generic localizable deep
representation that exposes the implicit attention of CNNs on
an image. In HE-GC neural network frameworks, the mean
pooling is easily conducted on the shares of both client and
server, without extra cost [44], [38].
In this work, we mainly focus on privacy-preserved linear
optimization (i.e., convolution and dot product). The privacy-
preserved nonlinear optimizations (especially activations) are
based on GC as introduced in other HE-GC approaches such
as GAZELLE [38].
B. Threat Model
Similar to GAZELLE [38] and other previous works,
namely the SecureML [48], MiniONN [44], DeepSecure [57]
and XONN [55], we adopt the semi-honest model, in which
both parties try to learn additional information from the mes-
sage received (assuming they have a bounded computational
capability). That is, the client C and server S will follow
the protocol, but C wants to learn model parameters and S
attempts to learn the client’s data. Note that, many applications
are built on well-known deep network structures such as
AlexNet [40], VGG-16/19 [62] and ResNet-50 [32]. Hence we
do not intend to protect the structure (number of layers, kernel
size, etc), but focus on the protection of model parameters.
In the case that the implemented structure is proprietary and
has to be protected, the server can introduce redundant layers
and kernels to hide the real structure at a computational
expense [44], [38]. Hence, the overarching goal is to make
the server oblivious of the private data from the client, and
prevent the client from learning model parameters of the server.
GAZELLE has demonstrated the security of HE-GC neural
network framework according to the cryptographic standard
of ideal/real security [29], [28], [30]. The same security
framework is adopted in this work.
Note that, while the client can use the server’s prediction
service as a blackbox oracle to extract the model [66], [69],
or even infer the training set [26], [50], [61], GALA does
not aim to protect against the black-box attack. Instead, it
focuses on protecting the input data and the model parameters
during the inference process, which stays in line with the threat
model of GAZELLE [38], SecureML [48], DELPHI [46],
CrytoFlow2 [54], etc., the output of neural network model is
returned to the client which decrypts the result and gets the
plaintext prediction.
C. Cryptographic Tools
The proposed privacy-preserved deep neural network
framework, i.e., GALA, employs three fundamental crypto-
graphic tools as outlined below.
(1) Packed Homomorphic Encryption. Homomorphic En-
cryption (HE) is a cryptographic primitive that supports mean-
ingful computations on encrypted data without the decryp-
tion key, which has found increasing applications in data
communications, storage and computations [65]. Traditional
HE operates on individual ciphertext [52], while the packed
homomorphic encryption (PHE) enables packing of multiple
values into a single ciphertext and performs component-wise
homomorphic computation in a Single Instruction Multiple
Data (SIMD) manner [14] to take advantage of parallelism.
Among various PHE techniques, our work builds on the
Brakerski-Fan-Vercauteren (BFV) scheme [23], which involves
four parameters2: 1) ciphertext modulus q, 2) plaintext modu-
lus p, 3) the number of ciphertext slots n, and 4) a Gaussian
noise with a standard deviation σ. The secure computation
involves two parties, i.e., the client C and server S.
In PHE,
the encryption algorithm encrypts a plaintext
message vector x from Zn into a ciphertext [x] with n slots.
We denote [x]C and [x]S as the ciphertext encrypted by client
C and server S, respectively. The decryption algorithm returns
the plaintext vector x from the ciphertext [x]. Computation can
be performed on the ciphertext. In a general sense, an eval-
uation algorithm inputs several ciphertext [x1], [x2],··· , and
outputs a ciphertext [x(cid:48)] = f ([x1], [x2],···). The function f
is constructed by homomorphic addition (Add), Multiplication
(Mult) and permutation (Perm). Speciﬁcally, Add([x],[y]) out-
puts a ciphertext [x+y] which encrypts the elementwise sum of
x and y. Mult([x],s) outputs a ciphertext [x(cid:12)s] which encrypts
the elementwise multiplication of x and plaintext s. It is worth
pointing out that GALA is designed to require scalar multipli-
cation between a ciphertext and a plaintext only, but not the
much more expensive multiplication between two ciphertext.
Hereafter, we use ScMult to denote the scalar multiplication
involved in GALA. Perm([x]) permutes the n elements in [x]
into another ciphertext [xπ], where xπ = (x(π0), x(π1),···)
and πi is a permutation of {0, 1,··· , n − 1}. Additionally,
the computation cost for a series of Perm operations on the
same ciphertext can be optimized by ﬁrst conducting one
Perm Decomposition (DecPerm) on the ciphertext and then
doing the corresponding series of Hoisted Perm (HstPerm)
operations [38]. Since only one DecPerm is involved, it can
amortize the total permutation time.
The run-time of Add and ScMult is signiﬁcantly lower
than that of Perm. From our experiments, a Perm operation
is 56 times slower than an Add operation and 34 times
slower than a ScMult operation. This observation motivates
the proposed linear optimization, which aims to minimize the
number of Perm operations, thus substantially reducing the
overall computation time.
Meanwhile, PHE introduces noise in the ciphertext which
theoretically hides the original message [38], [13]. Assume
the noise of [x] and [y] are η0 and η1, then the noise after
the Add operation is approximately η0 + η1. The noise after a
ScMult operation is ηmultη0 where ηmult is the multiplicative
noise growth of the SIMD scalar multiplication operation [38].
The noise after a Perm operation is η0 + ηrot where ηrot is
the additive noise growth of a permutation operation [38].
Roughly, we have ηrot > ηmult (cid:29) η0 (cid:29) 1. If the noise
goes beyond a certain level, the decryption would fail. Thus
it is also important to have a good noise management over
the ciphertext. We will show in Sec. III-C that GALA has a
better noise control than GAZELLE, which further guarantees
the overall success for the linear computations.
(2) Secret Sharing. In the secret sharing protocol, a value
is shared between two parties, such that combining the two
2The readers are referred to [38] for more detail.
4
imperative to enable dot product and convolution. GALA aims
to minimize the Perm operations, thus substantially reducing
the overall computation time. We view the HE-based linear
computation as a series of Add, Mult and Perm. The two inputs
to linear computation are the encrypted vector (or channels)
from the client and the plaintext weight matrix (or kernel)
from the server. The output
is the encrypted dot product
(or convolution). The objective in each step is to choose
the most efﬁcient operations in the descending priorities of
Add, Mult and Perm. Therefore, the overhead for the HE-
based linear computation can be efﬁciently reduced by GALA.
The recent privacy-preserved neural network frameworks can
integrate GALA as a plug-and-play module to further boost
their efﬁciency. We also analyze the (better) noise management
and (guaranteed) system security of GALA.
A. Row-encoding-share-RaS Matrix-Vector Multiplication
We ﬁrst focus on matrix-vector multiplication (dot product)
which multiplies a plaintext matrix at
the server with an
encrypted vector from the client. We ﬁrst discuss a naive
method followed by the mechanism employed in the state-of-
the-art framework (i.e., GAZELLE [38]), and then introduce
the proposed optimization of GALA that signiﬁcantly improves
the efﬁciency in matrix-vector multiplication.
For a lucid presentation of the proposed GALA and com-
parison with the state-of-the-art framework, we adopt the same
system model used in [38]. More speciﬁcally, we consider a
Fully Connected (FC) layer with ni inputs and no outputs.
The number of slots in one ciphertext is n. We also adopt the
assumptions used in [38]: n, ni and no are powers of two,
and no and ni are smaller than n. If they are larger than n,
the original no × ni matrix can be split into n×n sized blocks
that are processed independently.
1) Naive Method: The naive calculation for matrix-vector
multiplication is shown in Figure 3, where w is the no × ni
plaintext matrix on the server and [x]C is the HE-encrypted
vector provided by the client. The server encodes each row
of w into a separate plaintext vector (see step (a) in Figure
secrets yields the true value [56]. In order to additively share
a secret m, a random number, r, is selected and two shares
are created as (cid:104)m(cid:105)0 = r and (cid:104)m(cid:105)1 = m − r. Here, m can
be either plaintext or ciphertext. A party that wants to share a
secret sends one of the shares to the other party. To reconstruct
the secret, one needs to only add two shares m = (cid:104)m(cid:105)0+(cid:104)m(cid:105)1.
While the overall idea of secret sharing (SS) is straight-
forward, creative designs are often required to enable its
effective application in practice. Speciﬁcally, in the HE-GC
neural network framework,
the linear result from the dot
product or convolution is encrypted at the server side and
needs to be shared with the client to enable the following
GC-based nonlinear computation. Assume m is the resulted
ciphertext of a linear computation at the server, GAZELLE
then generates the share (cid:104)m(cid:105)0 = r and sends (cid:104)m(cid:105)1 = m − r
to the client. The two shares act as the input of the GC-based
nonlinear computation. Here the computation of m involves a
series of Perm operations, which is time-consuming. Instead
of directly generating the share (cid:104)m(cid:105)0 = r for m, we develop
a share-RaS (Rotate and Sum) computing for dot product
which lets the server generate an indirect share r(cid:48) for the
incomplete m, m(cid:48), while the true r is easy to be derived from
r(cid:48) and the true (cid:104)m(cid:105)1 = m − r is easy to be derived from
m(cid:48) − r(cid:48). The computation of m(cid:48) eliminates a large number of
Perm operations thus reducing the computation complexity.
Speciﬁcally, Our result shows that the proposed share-RaS
computing demonstrates 19× speedup for the dot product by
multiplying a 16×128 matrix with a length-128 vector (the
detailed benchmarks are shown in Sec. IV).
(3) Oblivious Transfer. In the 1-out-of-k Oblivious Transfer
(OT) [15], denoted as (k
1)-OT(cid:96), the sender’s inputs are the
k strings, m0, m1,··· , mk−1 ∈ {0, 1}(cid:96), and the receiver’s
input is a value i ∈ {0, 1,··· , k − 1}. At the end of the
OT execution, the receiver obtains mi from the functionality
and the sender receives no output. Here, the OT protocol
guarantees that 1) the receiver learns nothing about mj,j(cid:54)=i,
and 2) the sender learns nothing about i. An advancement
in the practicality of OT protocols is the OT extension [36],
which is further optimized such as in [39]. A special type
of OT extension is the correlated OT extension (COT) [9].
Particularly, the 1-out-of-2 COT, denoted as (2
1)-COT(cid:96), can
be used for linear computation3. In (2
1)-COT(cid:96), the sender’s
two inputs to each OT are not independent. Instead, the two
inputs to each OT instance are a random value s0 and a value
s1 = f (s0) for a correlation function f of the sender’s choice.
The receiver obtains either s0 or s1 as output depending on b.
III. SYSTEM DESCRIPTION
In this section, we introduce the proposed system, GALA,
for streamlining the linear computations (i.e., matrix-vector
multiplication and convolution) in privacy-preserved neural
network models. The HE-based linear computation consists of
three basic operations: Homomorphic Addition (Add), Multi-
plication (Mult), and Permutation (Perm). Our investigation
has shown that the linear computation dominates the total
computation cost and the most time-consuming part of HE-
based linear computation is a series of Perm operations that are
3We refer readers to [11], [21], [48], [54] for more details.
Fig. 3. Naive matrix-vector multiplication.
5
RaSRaSRaSA1wB1B2A3B4A2B3A4M1M2M3M4[x]cA1A2A3A4M1M2M3M4B4B1B2B3M1M2M3M4A1M1A2M2A3M3A4M4B1M1B2M2B3M3B4M4A1M1A2M2A3M3A4M4PermPermB1M1B2M2B3M3B4M4A3M3+A1M1A4M4+A2M2A1M1+A3M3A2M2+A4M4PermB3M3+B1M1B4M4+B2M2B1M1+B3M3B2M2+B4M4B3M3+B1M1B4M4+B2M2B1M1+B3M3B2M2+B4M4A1M1+A3M3+A2M2+A4M4A2M2+A4M4+A3M3+A1M1A3M3+A1M1+A4M4+A2M2A4M4+A2M2+A1M1+A3M3B1M1+B3M3+B2M2+B4M4B2M2+B4M4+B3M3+B1M1B3M3+B1M1+B4M4+B2M2B4M4+B2M2+B1M1+B3M3w0w1w0w1[u0]c[x]cA1M1A2M2A3M3A4M4B1M1B2M2B3M3B4M4[u1]cStep (a)Step (b)[x]c[u0]cAddA3M3+A1M1A4M4+A2M2A1M1+A3M3A2M2+A4M4AddAdd[u1]cPermAddStep (c)RaS3). The length of each encoded vector is n (including padded
0’s if necessary). We denote these encoded plaintext vectors
as w0, w1,··· , w(no−1). For example, the yellow and green
rows in step (a) of Figure 3 are w0 and w1, respectively.
2 positions such that
The server intends to compute the dot product between
w and [x]C. To this end, it ﬁrst uses ScMult to compute
the elementwise multiplication between wi and the encrypted
input vector [x]C to get [ui]C = [wi (cid:12) x]C (see step (b)
in Figure 3). The sum of all elements in ui will be the i-
th element of the desired dot product between w and [x]C.
However, as discussed in Sec. II-C, it is not straightforward
to obtain the sum under the packed HE. A rotate-and-sum
(RaS) calculation must be used here, as illustrated in step
the entries in [ui]C are ﬁrst
(c) of Figure 3. Speciﬁcally,
rotated through Perm by ni
the ﬁrst
2 entries of the rotated [ui]C are actually the second ni
ni
2
entries of the original [ui]C. Then the server uses Add to
conduct elementwise addition between the rotated [ui]C and
the original [ui]C, which results in a ciphertext whose ﬁrst ni
2
entries contain the elementwise sum of the ﬁrst and second
2 entries of ui. The server conducts this RaS process for
ni
log2 ni iterations. Each iteration acts on the resulted ciphertext
from the previous iteration, and rotates by half of the previous
positions, as shown in Step (c) of Figure 3. Finally, the server
gets a ciphertext where the ﬁrst entry is the i-th element
in wx. By applying this procedure on each of the no rows
(i.e., w0, w1,··· , w(no−1)), the server obtains no ciphertext.
Altogether, the ﬁrst entries of those ciphertext correspond to
wx.
We now analyze the complexity of the above linear com-
putation process, in terms of the number of operations and
output ciphertext. We consider the process starting from the
server’s reception of [x]C (i.e., the encrypted input data from
the client) until it obtains the to-be-shared ciphertext4 (i.e., the
no ciphertext after RaS). There are a total of no scalar multi-
plications (ScMult) operations, no log2 ni Perm operations and
no log2 ni Add operations. It yields no output ciphertext, each
of which contains one element of the linear result wx. This
inefﬁcient use of the ciphertext space results in a low efﬁciency
for linear computations.
2) Hybrid Calculation (GAZELLE): In order to fully utilize
the n slots in a ciphertext and further reduce the complexity,
the state-of-the-art scheme is to combine the diagonal encoding
[31] and RaS, by leveraging the fact that no is usually much
smaller than ni in FC layers. This hybrid method shows that
the number of expensive Perm operations is a function of
no rather than ni, thus accelerating the computation of FC
layers [38]. The basic idea of the hybrid method is shown in
Figure 4.
Speciﬁcally, the server encodes w into no plaintext vec-
tors through a diagonal manner. For example, in step (a) of
Figure 4, the ﬁrst plaintext vector w0 consists of the yellow
elements of matrix w, (A1, B2, A3, B4), and the second
plaintext vector w1 consists of the green elements (A2, B3,
A4, B1). Note that the w0 in this method is different from the
w0 in the naive method of Figure 3. So is w1.
4In HE-GC neural network computing, the resultant ciphertext from linear
calculation are shared between client and server as the input of GC-based
nonlinear function.
Fig. 4. Hybrid matrix-vector multiplication.
The server then rotates [x]C by i positions, shown in step
(b), and uses ScMult to perform elementwise multiplication
with wi. For example, in step (c) of Figure 4, w0 is multiplied