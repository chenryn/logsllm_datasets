Sn 
S0 
Rn 
100M, 10ms 
100M, 10ms 
r0
r1 
100M, 20ms 
10M, 10ms 
10M, 10ms 
R0 
Figure 7  Simulation topology--dumbbell 
From Section 4.1 to 4.8 we use dumbbell topology showed in 
Figure 7. The measured TCP connection is between S0 and R0. S0 
is  the  sender.  R0  is  the  receiver.  Validation  procedure  is  as 
follows: First, when simulation runs, we measure at both S0 and 
R0 and dump the traffic into two different files. When simulation 
stops, we use tcpdep to estimate the sender behavior with the trace 
dumped at R0. Then we analyze both traces dumped at S0 and R0 
jointly to obtain the actual sender behavior and network delay and 
loss.  Finally,  we  evaluate 
the  estimation  performance  by 
comparing the estimated result and the actual. In all simulations 
without  particular  indications,  the  sender  uses  TCP  NewReno 
version,  sends  fixed  1600  data  packets(excluding  retransmit 
packets), the delay ack is enabled at the receiver. The maximum 
window size is 40 packets. The fixed propagation round-trip time 
between  S0  and  R0  is  80ms.  The  bandwidth  of  link  r0-r1  is 
10Mbps  with  a  buffer  size  of  160  packets.  Therefore,  the  link 
queuing delay is between 0~128ms. 
In simulation, Sn-Rn share with S0-R0 the same bottleneck r0-
r1. Generating different kinds of traffic between Sn-Rn makes the 
queue  of  link  r0-r1  exhibit  different  queuing  dynamics,  which 
causes  the  measured  TCP  connection  to  experience  different 
network delay and loss processes. This allows us to evaluate the 
estimation  performance  under  different  network  conditions. 
Changing the TCP sender version of S0 allows us to investigate 
the method’s adaptability to different TCP sender versions. 
In 4.9, we perform simulation under tandem network topology 
with cross-traffic. In 4.10, we show the Internet experiment result. 
4.1  Performance under Poisson background 
traffic 
In  this  simulation  scenario,  the  background  traffic  is  Poisson 
with an average arriving rate λ of 9.6Mbps. Using Poisson traffic 
does not mean we think the background traffic in the Internet is 
Poisson. It is only used as a method to introduce random queuing 
delay.  We  also  adopted  other  kinds  of  background  traffic  in  the 
following sections. 
To  eliminate  randomness  effects  of  background  traffic,  we 
repeated  64  runs.  The  average  εW  of  64  runs  is  4.0%  and  εta  is 
4.4% 3.  Figure  8  shows  the  59th  run.  It  represents  the  typical 
performance of the 64 runs. In this run, εW is 3.6% and εta is 4.5%, 
which approximate to the average values. From this graph, we can 
see that the estimated result matches the actual result very well in 
most  of  the  time.  The  errors  mainly  occur  in  the  loss  recovery 
phase.  One  reason  may  be  that  currently  we  allow  arbitrary 
pairing  combinations  between acks  and  data  packets  during  loss 
recovery phase. However, it is not the intrinsic drawback of our 
method. The errors will be reduced with the improvement of M. 
Viewing the errors from another prospective, even though serious 
estimation errors  occur  in  the  loss  recovery  phase, the errors  do 
not propagate, which is a good property. 
Figure 9  Comparison of average cwnd and ta between 
estimated and actual result under Poisson traffic 
Figure 8  Comparison of congestion window process and ta 
process between estimation and actual for the 59th run 
Changing the λ of Poisson background traffic changes the loss 
rate  experienced  by  the  measured  TCP.  Figure  9  shows  the 
comparison of average cwnd and ta between estimated and actual 
results when we vary the λ from 8Mbps to 10.8Mbps. In the graph, 
we also show eW and eta respectively. Totally 64 simulation runs 
are performed. As λ increases, the loss rate of the measured TCP 
grows from 0 to 0.053. The average εW of 64 runs is 3.9% and εta 
is 2.5%, which are both very small. From this graph, we see that 
the  eW  and  eta  for  every  individual  run  is  small  too.  The  figure 
shows  the  good  estimation  performance  of  the  method  under 
different loss rates in this scenario. 
4.2  Performance under long-duration 
background TCP traffic 
Different kinds of background traffic have different impacts on 
the  queuing  dynamics,  which  causes  the  measured  TCP  to 
experience  different  ta  processes  and  affects  the  estimation 
performance. In last section, the background traffic is Poisson. In 
the following two sections, we will use two other different kinds 
of background traffic. 
3 Definitions of estimation performance metrics eW, eta, εW, εta are 
in Appendix A. 
In  this  section,  we  use  long-duration  TCP  as  background 
traffic(FTP traffic). Figure 10 shows the result when we vary the 
number of background TCP flows from 5 to 160. There are totally 
64 runs. The loss rate of the measured TCP grows from 0 to 0.05 
as the number of background flows increases. The average εW of 
64 runs is 2.6% and εta is 1.0%. From Figure 10, we see that the 
eW  and  eta  for  each  individual  run  is  small  too.  The  estimation 
performance under this background traffic is also very good. 
4.3  Performance under FTP+HTTP 
background traffic 
In this section, we introduce a more realistic background traffic, 
which is FTP + HTTP traffic. Long-duration FTP traffic is of 10 
flows. HTTP traffic is generated using webcache module in NS. 
In the simulation, we vary the number of active4 HTTP flows over 
the link r0-r1 to change its congestion level. 
There are totally 64 runs. The loss rate of the measured TCP is 
from 0.001 to 0.056. The average εW of 64 runs is 3.5% and εta is 
2.7%. Figure 11 shows the result for each run. 
4 A slight change is made in NS webcache module to control the 
number of active HTTP flows. 
4.4  Effects of different background traffic 
In  previous  three  sections,  we  have  tested  our  method  under 
three kinds of background traffic. In all cases, the method exhibits 
good performance under different data loss rates. In this section, 
we study the effects of different kinds of background traffic on ta 
process and on the estimation performance. 
We  compare  the  estimation  performance  under  three  different 
background traffic, the 9.6Mbps Poisson traffic, the 65 FTP flows, 
and the 10FTP + 180HTTP mixed traffic. We have 64 runs under 
each of the three. In three simulation scenarios, the average loss 
rates(p) of the measured TCP are all approximately 1.2%. 
Table 1  Average result of each kind of traffic 
εta 
4.0% 4.4% 
1.5% 1.2% 
3.3% 3.9% 
p  W  ta(ms) ta_std(ms)  εW 
Figure 10  Comparison of average cwnd and ta between 
estimated and actual result under FTP traffic 
Poisson  1.2% 12.4  175 
FTP 
1.1% 9.06  195 
FTP+HTTP 1.1% 11.9  170 
27.3 
20.4 
26.5 
Figure 12  εW under different background traffic 
Table  1  shows  the  average  performance.  Figure  12  shows  the 
eW for every run. num in the figure represents the nth run. The eta 
for every run is similar to the eW, thus not showed. From them, we 
see that the performance under pure FTP background traffic is the 
best among the three, the average εW and εta of 64 runs is between 
1%~2%. The performances under Poisson and mixed FTP+HTTP 
are similar, in which the average εW and εta are both around 4%.  
To explained the difference, we plot Figure 13 to show the ta 
processes  experienced  by  the  measured  TCP  under  different 
background  traffic. The  ta  processes  are  visually  quite  different. 
From Figure 13 and Table 1, we see that the variance of ta process 
is the smallest for the FTP traffic, while almost the same for the 
other two kinds of traffic. We believe this explains why εW and εta 
are  smaller  under  FTP  background  traffic  than  under  the  other 
two—smaller ta variance leads to better estimation performance. 
However, although ta processes under Poisson and mixed traffic 
are visually very different(ta of the former varies slowly while ta 
of the latter varies more abruptly), their performances are alike. 
Figure 11  Comparison of average cwnd and ta between 
estimated and actual result under FTP+HTTP traffic 
Figure 13  ta processes under different background traffic, 
FTP, Poisson and FTP+HTTP 
4.5  Effects of queue length 
Increasing  queue  length  of  link  r0-r1  gives  the  ta  of  the 
measured TCP a larger fluctuation range, which might reduce the 
estimation  performance.  To  explore  this  issue,  we  consider  3 
scenarios  under  FTP+HTTP  background  traffic  where  queue 
lengths  of  link  r0-r1  are  160,  320  and  640  respectively.  In  each 
scenario, we vary the number of active HTTP flows to change the 
loss rate of the link. For each number of flows, we have 8 runs. 
Figure  14  shows  the  effects  of  queue  length  on  the  εW  under 
various loss rates. The graph of εta is similar. In the graph, each 
point represents the average loss rate and εW of 8 runs under the 
same number of flows. From this graph, we cannot see the effects 
of  queue  length  on  the  performance.  The  reason  is  probably 
because that the variance of ta process does not grow linearly with 
the queue length as we assumed. (see Figure 15). 
Figure 15  Effects of queue length on the standard deviation of 
ta process 
4.6  Performance of different TCP versions 
One  advantage  of  the  method  is  that  we  can  estimate  TCP 
senders’ behavior of different versions using a generic TCP sender 
model. In this section, we evaluate the performance for 5 different 
TCP  versions  in  NS.  The  5  versions  are  tahoe,  reno,  newreno, 
Sack1  and  Fack.  We  had  16  runs  for  each  version.  The 
background traffic is Poisson—9.6Mbps. The average loss rate is 
1.2%.  Table  2  shows  the  average  performance  for  each  version. 
From  the  table,  we  see  that  our  method  achieves  similar  good 
performance for TCP tahoe, newreno, Sack1 and Fack, while the 
performance for TCP reno is worse than the other four. Figure 16 
shows the εW performance for every run. εta is similar to εW. 
Table 2  Average εW, εta for different TCP versions 
εW 
εta 
tahoe 
1.3% 
1.7% 
Reno 
13.3% 
13.6% 
newreno 
3.6% 
3.9% 
Sack1 
1.4% 
1.4% 
Fack 
3.9% 
3.2% 
Figure 14  Effects of queue length on the estimation 
performance 
Figure 16  εW and εta for every run 
4.7  Performance under ACK loss 
If ack Ak is lost, the data packets which should be triggered by 
Ak are now triggered by Ak+1. If the time interval between Ak and 
Ak+1 is small when they are sent from receiver, it is difficult for us 
to discern whether Ak is lost or not either from the rule or from ta 
value. In this situation, regarding that Ak is not lost, which equals 
to  regard  that  the  data  packets  actually  triggered  by  Ak+1  are 
triggered  by  Ak,  will  not  affect  the  path’s  likelihood  value  very 
much.  The  wrong  decision  will  be  confined  to  only  those  data 
packets which should have been triggered by Ak, which is a quite 
limited error event. 
If  the  time  interval  is  large,  to  regard  that  the  data  packets 
actually triggered by Ak+1 are triggered by Ak will cause a sudden 
increase of ta value. If the increase exceeds certain threshold, we 
can judge that Ak is lost. Therefore, ack loss will still not affect the 
method in this situation. 
To investigate the effects of ack loss, we introduce ack loss in 
the measured TCP in simulation. The ack loss rates are 0, 0.002, 
0.005,  0.01,  0.02,  0.05,  0.1  and  0.2.  We  use  9.6Mbps  Poisson 
background traffic, and perform 64 runs for each loss rate. Figure 
17 shows the result. From it, we see that when the ack loss rate is 
less than 0.1, the performance nearly stays in the level where εW 
and  εta  are  around  5%.  When  loss  rate  exceeds  0.1,  the 
performance degradation becomes obvious. When loss rate is 0.2, 
εW and εta are 15%. 
Figure  18  shows  the  every  run  for  simulation  scenarios  of  no 
ack  loss  and  of  0.01  ack  loss  rate.  The  average  εW,  εta  of  the 
former is 4.0% and 4.4%, and 4.7% and 5.9% of the latter. We can 
see  from  the  graph  that  the  performance  of  the  latter  is  only 
slightly worse than the former. 
From these two graphs, we conclude that when the ack loss rate 
is small, the performance degradation is tolerable. For larger ack 
loss rate, the method needs to be improved. 
Figure 17  Average εW, εta with ack loss from 0 to 0.2 
4.8  Performance under no background traffic 
In previous experiments, the traffic of the measured TCP is not 
the  main  traffic  in  the  network.  In  this  section,  we  consider  a 
simple scenario where the TCP is the only traffic in the network. 
In  this  situation,  the  ta  process  it  experiences  has  strong 
correlation  with  its  data  sending  behavior.  With  the  increase  of 
TCP  congestion  window,  the  number  of  packets  in  the  network 
also  increases.  When  the  number  exceeds  a  certain  threshold, 
packets in router’s buffer begin to increase steadily as the window 
increases.  The  increase  continues  until  loss  occurs.  Then,  the 
sender  suspends  sending  data.  The  router’s  buffer  begins  to 
decrease. When the sender resumes sending data packets, another 
cycle of window increase begins. In the cycle, ta process increases 
slowly at first, and then deceases suddenly. This experiment also 
represents a typical scenario where the delay changes dramatically. 
Consider Figure 7 network topology. When the queue of r0-r1 
increases one more packet, ta = ta + Packetsize / BW. The smaller 
BW,  the  bigger  ta  increment  for  every  one  more  packet.  The 
maximum queue length determines the maximum value of ta. The 
larger queue length, the larger maximum ta value, and the larger 
ta drop after loss occurs. 
We consider a simulation scenario where BW = 64kbps. This 
equals  to  the  bandwidth  of  a  Internet  dail  user,  and  it  should 
almost be the worst case we can meet in the Internet. The fixed 
RTT is 80ms. In simulation, we change the queue length from 5 to 
40.  The  TCP  window  grows  linearly  from  1  packet.  When  the 
queue  length  is  40,  the  data  packets  will  not  lost,  because  the 
maximum TCP window size is 40. 
Figure 18  Comparison between no ack loss and 0.01 ack loss 
Figure 19  Performance when queue length is 20 packets 
Figure  19  is  the  comparison  between  the  estimation  and  the 
actual when the queue length is 20. The actual average W is 16.4, 
and the average estimated W is 15.8. εW is 7%. The average actual 
ta is 2025ms, and the estimated ta is 1957ms. εta is 7%. 
Figure 20 shows the εW, εta under different queue lengths, all ε 
are less than 12%. The average εW of 8 runs is 7%, εta is 7.5%. 
When the queue length is 40, εW and εta are both 2.2%. Thus, we 
conclude  that  when  the  estimated  TCP  is  the  only  traffic  in  the 
network, the method can track the change of ta with some lag. 
Figure 20  εW, εta under different buffer size 
4.9  Simulation using Tandem Network 
Topology 
In this section, we use a network topology showed in Figure 21. 
The measured TCP is between S0 and R0, and we generate cross 
traffic from Si to Ri (i = 1, 2, … n). We also generate traffic from 
Sn+1  to  Rn+1.  In  this  simulation  scenario,  we  analyze  the 
estimation performance under multiple congested links and two-
way  traffic.  The  traffic  between  each  pair  of  Sn  and  Rn  are 
composed  of  8  long-duration  TCP  flows  and  100  active  HTTP 
flows.  Traffic  between  different  pairs  are  independent.  In  the 
experiment, we increase the number of congested links from 1 to 
10 in order to make RTT between S0 and R0 more variable. We 
repeat 64 times for each number of links. From Table 3, we see 
that  the  variance  of  ta  increases  as  the  number  of  links  grows. 
However, εta  and εW  do not grow. We think the reason is because 
that  the  growth  of  ta  average(mta)  counteracts  the  effect  of  the 
growth of ta standard deviation(σta). In Table 3, we see that the 
σta/mta decreases as the number of links grows. 