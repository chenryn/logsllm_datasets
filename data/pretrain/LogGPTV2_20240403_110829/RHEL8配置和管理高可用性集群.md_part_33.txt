# []{#assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters.html#ref_cluster-properties-shutdown-lock-configuring-resources-to-remain-stopped}集群属性配置资源以在清理节点关闭时保持停止 {.title}
:::
防止资源在干净节点关闭中进行故障的功能是通过下列集群属性实现的。
::: variablelist
[`shutdown-lock`{.literal}]{.term}
:   当将此集群属性设置为默认值 `false`{.literal}
    时，集群将恢复在被完全关闭的节点上活动的资源。当此属性设为
    `true`{.literal}
    时，在被完全关闭的节点上活动的资源将无法在其它地方启动，直到它们在重新加入集群后在该节点上再次启动。
    `shutdown-lock`{.literal}
    属性适用于集群节点或远程节点，但不适用于客户机节点。
    如果 `shutdown-lock`{.literal} 设为
    `true`{.literal}，您可以在节点关闭时删除一个集群资源上的锁，以便可通过使用以下命令在节点上手动刷新来在其它地方启动资源。
    `pcs resource refresh resource node=nodename`{.literal}
    请注意，资源被解锁后，集群就可以自由地将资源移至其他位置。您可以使用粘性值或位置首选项来控制发生这种情况的可能性。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    只有在您第一次运行以下命令时，手动刷新才可以在远程节点中使用：
    ::: orderedlist
    1.  在远程节点上运行 `systemctl stop pacemaker_remote`{.literal}
        命令，以停止该节点。
    2.  运行 `pcs resource disable remote-connection-resource`{.literal}
        命令。
    :::
    然后您可以在远程节点上手动进行刷新。
    :::
[`shutdown-lock-limit`{.literal}]{.term}
:   当将此集群属性设置为默认值 0
    以外的其他值时，如果节点在启动关闭后的指定时间内没有重新加入，则资源将在其他节点上可用。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    只有在您第一次运行以下命令时，`shutdown-lock-limit`{.literal}
    属性才能用于远程节点：
    ::: orderedlist
    1.  在远程节点上运行 `systemctl stop pacemaker_remote`{.literal}
        命令，以停止该节点。
    2.  运行 `pcs resource disable remote-connection-resource`{.literal}
        命令。
    :::
    运行这些命令后，当 `shutdown-lock-limit`{.literal}
    指定的时间过后，远程节点上运行的资源将可用于在其他节点上恢复。
    :::
:::
:::
::: section
::: titlepage
# []{#assembly_configuring-resources-to-remain-stopped-configuring-and-managing-high-availability-clusters.html#proc_setting-shutdown-lock-configuring-resources-to-remain-stopped}设置 shutdown-lock 集群属性 {.title}
:::
以下示例将示例集群中的 `shutdown-lock`{.literal} 集群属性设置为
`true`{.literal}，并显示节点关闭并再次启动时的它的效果。这个示例集群由三个节点组成：`z1.example.com`{.literal}、`z2.example.com`{.literal}
和 `z3.example.com`{.literal}。
::: orderedlist
**流程**
1.  将 `shutdown-lock`{.literal} 属性设为 `true`{.literal}
    ，并验证其值。在本例中，`shutdown-lock-limit`{.literal}
    属性保持其默认值 0。
    ``` literallayout
    [PI:EMAIL ~]# pcs property set shutdown-lock=true
    [PI:EMAIL ~]# pcs property list --all | grep shutdown-lock
     shutdown-lock: true
     shutdown-lock-limit: 0
    ```
2.  检查集群的状态。在本例中，资源 `third`{.literal} 和
    `fifth`{.literal} 运行在 `z1.example.com`{.literal} 上。
    ``` literallayout
    [PI:EMAIL ~]# pcs status
    ...
    Full List of Resources:
    ...
     * first	(ocf::pacemaker:Dummy):	Started z3.example.com
     * second	(ocf::pacemaker:Dummy):	Started z2.example.com
     * third	(ocf::pacemaker:Dummy):	Started z1.example.com
     * fourth	(ocf::pacemaker:Dummy):	Started z2.example.com
     * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com
    ...
    ```
3.  关闭 `z1.example.com`{.literal}，这将停止在该节点上运行的资源。
    ``` literallayout
    [PI:EMAIL ~] # pcs cluster stop z1.example.com
    Stopping Cluster (pacemaker)...
    Stopping Cluster (corosync)...
    ```
4.  运行 `pcs status`{.literal} 命令可显示节点
    `z1.example.com`{.literal} 脱机，并且 `z1.example.com`{.literal}
    上运行的资源在节点停机时为 `LOCKED`{.literal}。
    ``` literallayout
    [PI:EMAIL ~]# pcs status
    ...
    Node List:
     * Online: [ z2.example.com z3.example.com ]
     * OFFLINE: [ z1.example.com ]
    Full List of Resources:
    ...
     * first	(ocf::pacemaker:Dummy):	Started z3.example.com
     * second	(ocf::pacemaker:Dummy):	Started z2.example.com
     * third	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)
     * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
     * fifth	(ocf::pacemaker:Dummy):	Stopped z1.example.com (LOCKED)
    ...
    ```
5.  在 `z1.example.com`{.literal}
    上再次启动集群服务，使其重新加入集群。锁住的资源应该在这个节点上启动，但当它们启动后，它们不一定会停留在同一个节点上。
    ``` literallayout
    [PI:EMAIL ~]# pcs cluster start z1.example.com
    Starting Cluster...
    ```
6.  在本例中，会在节点 `z1.example.com`{.literal} 上恢复
    `第三`{.literal} 和 `第五个`{.literal} 节点。
    ``` literallayout
    [PI:EMAIL ~]# pcs status
    ...
    Node List:
     * Online: [ z1.example.com z2.example.com z3.example.com ]
    Full List of Resources:
    ..
     * first	(ocf::pacemaker:Dummy):	Started z3.example.com
     * second	(ocf::pacemaker:Dummy):	Started z2.example.com
     * third	(ocf::pacemaker:Dummy):	Started z1.example.com
     * fourth	(ocf::pacemaker:Dummy):	Started z3.example.com
     * fifth	(ocf::pacemaker:Dummy):	Started z1.example.com
    ...
    ```
:::
:::
:::
[]{#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters.html}
::: chapter
::: titlepage
# []{#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters.html#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters}第 23 章 配置节点放置策略 {.title}
:::
Pacemaker
根据资源分配分数来决定在每个节点上放置资源的位置。资源将分配给资源分数最高的节点。此分配
score 源自因素的组合，包括资源约束、`resource-stickiness`{.literal}
设置、各个节点上资源以前的故障历史记录以及每个节点的利用率。
如果所有节点上的资源分配分数相等，默认的放置策略Pacemaker
将选择一个分配的资源最少的节点来平衡负载。如果每个节点中的资源数量相等，则会选择
CIB 中列出的第一个有资格的节点来运行该资源。
但通常不同的资源使用会对节点容量有很大不同（比如内存或者
I/O）。您始终无法通过只考虑分配给节点的资源数量来平衡负载。另外，如果将资源设置为其合并要求超过提供容量，则可能无法完全启动，或者可能会以降低性能运行。要考虑以上因素，Pacemaker
允许您配置以下组件：
::: itemizedlist
-   特定节点提供的能力
-   特定资源需要的容量
-   资源放置的整体策略
:::
::: section
::: titlepage
# []{#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters.html#configuring-utilization-attributes-configuring-node-placement-strategy}使用属性和放置策略 {.title}
:::
要配置节点提供或需要资源的容量，您可以使用节点和资源[*使用属性*]{.emphasis}。您可以通过为资源设置使用变量，并将值分配给该变量以指示资源需要，然后为节点设置相同的使用变量，并为该变量分配一个值来指示节点提供的内容。
您可以根据喜好命名使用属性，并根据您的配置定义名称和值对。使用属性的值必须是整数。
::: section
::: titlepage
## []{#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters.html#_configuring_node_and_resource_capacity}配置节点和资源容量 {.title}
:::
以下示例为两个节点配置 CPU 容量的使用属性，将这个属性设置为变量
`cpu`{.literal}。它还配置 RAM 容量的使用属性，将此属性设置为变量
`内存`{.literal}。在本例中：
::: itemizedlist
-   节点 1 定义为提供 2 个 CPU 和 2048 RAM
-   节点 2 定义为提供 4 个 CPU 和 2048 RAM
:::
``` literallayout
# pcs node utilization node1 cpu=2 memory=2048
# pcs node utilization node2 cpu=4 memory=2048
```
以下示例指定三个不同资源需要的相同的使用属性。在本例中：
::: itemizedlist
-   资源 `dummy-small`{.literal} 需要 1 个 CPU 容量和 1024 个 RAM 容量
-   资源 `dummy-medium`{.literal} 需要 2 个 CPU 容量和 2048 个 RAM 容量
-   资源 `dummy-large`{.literal} 需要 1 个 CPU 容量和 3072 个 RAM 容量
:::
``` literallayout
# pcs resource utilization dummy-small cpu=1 memory=1024
# pcs resource utilization dummy-medium cpu=2 memory=2048
# pcs resource utilization dummy-large cpu=3 memory=3072
```
如果节点有足够的可用容量以满足资源的要求，则节点被视为有资格获得资源。
:::
::: section
::: titlepage
## []{#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters.html#_configuring_placement_strategy}配置放置策略 {.title}
:::
在配置了节点提供的容量以及资源需要的容量后，您需要设置
`placement-strategy`{.literal} 集群属性，否则容量配置无效。
`placement-strategy`{.literal} 集群属性有四个值：
::: itemizedlist
-   `default`{.literal} - 根本不考虑 Utilization
    值。根据分配分数分配资源。如果分数相等，则在节点间平均分配资源。
-   `utilization`{.literal} - 只有在决定节点是否被视为有资格时才会考虑
    Utilization
    值（即，它是否有足够的可用容量来满足资源的要求）。负载均衡仍会根据分配给节点的资源数量进行。
-   `balanced`{.literal} -
    在决定节点是否有资格提供资源以及负载平衡时，会考虑 Utilization
    值，因此会尝试以优化资源性能的方式分散资源。
-   `minimal`{.literal} -
    只有在决定节点是否有资格为资源提供服务时才会考虑 Utilization
    值。对于负载平衡，会尝试尽可能将资源集中到几个节点上，从而在剩余的节点上启用以实现节电的目的。
:::
以下示例命令将 `placement-strategy`{.literal} 的值设为
`balanced`{.literal}。运行此命令后，Pacemaker
会确保在整个集群中平均分配来自您的资源负载，而无需使用复杂的托管限制集合。
``` literallayout
# pcs property set placement-strategy=balanced
```
:::
:::
::: section
::: titlepage
# []{#assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters.html#pacemaker-resource-allocation-configuring-node-placement-strategy}Pacemaker 资源分配 {.title}
:::
Pacemaker 根据节点首选、节点容量和资源分配首选项分配资源。
::: section
::: titlepage