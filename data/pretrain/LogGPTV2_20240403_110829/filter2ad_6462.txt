title:A Case for a Stateful Middlebox Networking Stack
author:Muhammad Asim Jamshed and
Donghwi Kim and
YoungGyoun Moon and
Dongsu Han and
KyoungSoo Park
A Case for a Stateful Middlebox Networking Stack
Muhammad Jamshed, Donghwi Kim, YoungGyoun Moon,
Dongsu Han, and KyoungSoo Park
Department of Electrical Engineering, KAIST
1.
INTRODUCTION
Statful middleboxes such as intrustion detection systems,
application-layer ﬁrewalls, and protocol analyzers are in-
creasingly popular as they perform critical operations in
modern networks. Such middleboxes typically operate by
maintaining ﬂow states of live TCP connections that pass
through a network.
Despite its growing demand, developing a stateful mid-
dlebox remains a challenging task. The root of complexity
stems from a lack of common programming abstraction for
middleboxes that clearly separates ﬂow management from
custom middlebox logic. As a result, middlebox developers
often resort to writing a complex ﬂow management module
from scratch, which results in tens of thousands of code lines
that are hardly portable [4, 2]. This is in stark contrast to
developing networking applications for end nodes, which sig-
niﬁcantly beneﬁts from a nice network abstraction layer such
as Berkeley socket API. The lack of a reusable networking
stack for middleboxes makes the code highly dependent on
a custom packet library, which greatly reduces readability,
modularity, and extensibility.
In this work, we formulate a general-purpose ﬂow manage-
ment stack that serves the basic requirements of monitoring
middlebox applications. The ﬂow management stack should
satisfy three requirements. First, it should abstract TCP
state management such that the developers can focus on
the custom middlebox processing instead of dealing with
low level detail. Clear separation of ﬂow management and a
custom middlebox logic is the key to high code readability
and modularity. Second, it should be ﬂexible enough to
express any packet or ﬂow-level event that triggers a special
middlebox operation. An event can be as simple as TCP
state change or packet retransmission. Or one should be able
to extend a basic event by evaluating an arbitrary function
(e.g., retransmitted packet whose content is diﬀerent from
the original packet). A ﬂexible event-based system enables a
developer to write a middlebox application as a set of logical
middlebox events and their event handlers. Third, the ﬂow
management stack should allow eﬃcient resource manage-
ment. Depending on the needs of a middlebox, a developer
might want to avoid the ﬂow reassembly on the server side
while she actively monitors the ﬂow content from the client
side. Or one should be able to deallocate all resources for a
ﬂow even if the ﬂow has not terminated yet.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM’15, August 17–21, 2015, London, United Kingdom.
c(cid:13) 2015 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-3542-3/15/08..
DOI: http://dx.doi.org/10.1145/2785956.2789999
Figure 1: Stateful inline middlebox architecture.
Based on the requirements, we build our networking API
for stateful middleboxes based on mTCP [3], a highly-scalable
user-level TCP implementation. The mTCP stack serves
as the underlying ﬂow management module while we place
a number of hooks in ﬂow processing to support basic and
extended events. The resulting implementation enables mod-
ular middlebox development with high performance.
2. OPERATION MODEL
Figure 1 provides an overview of our middlebox networking
stack architecture. The networking stack monitors TCP
ﬂows passing through the middlebox, and exposes a socket
that abstracts each ﬂow to the application. The application
developer deﬁnes a set of middlebox events and registers
event handlers (or callbacks) that perform custom middlebox
logic. When an event handler is called, it is passed a socket
for the ﬂow that raised the event, and the application can
probe the ﬂow state through our middlebox socket API.
For ﬂow management in the middlebox, each TCP ﬂow
context should track both client- and server-side TCP states.
This is because a middlebox could act on either the client-
or server-side state or both. For example, an IDS could
monitor a client-side TCP state transition (e.g., ESTABLISHED
to FIN_WAIT_1) or inspect the data from a server. For eﬃcient
resource usage, our stack allows the developer to disable the
state management of either side on a per-ﬂow basis.
An event is the critical abstraction in our networking stack,
which represents a condition for speciﬁc middlebox operation.
An event can be either a built-in or a user-deﬁned event
(UDE). A built-in event is raised by the ﬂow management
module when a certain pre-deﬁned condition is met. For
example, a built-in event is generated when a new connection
is established or torn down, new data is available in a socket
buﬀer, the TCP state is changed, a new packet arrives, or a
packet is retransmitted. A developer can register a handler
for a built-in event or can deﬁne her own UDE based on
a built-in event. That is, a new event can be deﬁned by a
built-in event and a custom function that further determines
the condition for the event. In short, the user can express a
variety of custom events for middlebox processing without
dealing with any low level detail.
Client Server Stateful Monitoring Middlebox Middlebox Networking Stack Application Logic Callback (Event Processing) Built-in / User-defined Event Socket Flow Management Server-side State Client-side State 3553. SAMPLE CODE
event_t e ) {
1 static void // c a l l b a c k for SYN r e t r a n s m i s s i o n
2 o n _ s y n _ r e x m i t ( ctx_t ctx , int sock , int side ,
3
4
5
6 }
7
8 static bool // user - d e f i n e d SYN f i l t e r
9 ft_syn ( ctx_t ctx , int sock , int side ,
static u i n t 6 4 _ t s y n _ r e t x = 0;
s y n _ r e t x ++;
event_t e , e v e n t _ d a t a _ t data ) {
struct pkt_ctx p ;
// get the c u r r e n t p a c k e t c o n t e x t
m t c p _ c b _ g e t c u r p k t ( ctx , & p ) ;
return ( p . tcph - > syn && ! p . tcph - > ack ) ;
10
11
12
13
14
15 }
16
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
18 // c r e a t e an UDE for SYN r e t r a n s m i s s i o n
19 // it ’ s d e f i n e d as O N _ R E X M I T and f t _ s y n
20 ude_syn = m t c p _ d e f i n e _ u d e ( ON_REXMIT , ft_syn ) ;
21
22 // open a m o n i t o r i n g s o c k e t
23 msock = m t c p _ s o c k e t ( ctx , AF_INET ,
24
25
26 // r e g i s t e r a c a l l b a c k for u d e _ s y n
27 m t c p _ r e g i s t e r _ c a l l b a c k ( ctx , msock , ude_syn ,
28
S O C K _ M O N I T O R _ S T R E A M _ P A S S I V E , 0) ;
PRE_TCP , o n _ s y n _ r e x m i t ) ;
Listing 1: Sample code that counts # of TCP SYN
retransmissions (error checking is omitted)
Listing 1 shows sample code that counts the number of TCP
SYN retransmissions for the ﬂows that traverse the mid-
dlebox. Due to the space constraint, we explain only the
main logic of the code. Line 1 to line 15 show how a call-
back function and a ﬁlter function are deﬁned, which are
then used in the later code. In line 20, the code deﬁnes a
user-deﬁned event (ude_syn) that detects retransmitted SYN
packets. The UDE is based on a built-in event, ON_REXMIT,
which is raised by the system when a packet in a TCP ﬂow
is retransmitted. ude_syn extends the built-in event by a
ﬁlter function, ft_syn(), that determines if the retransmit-
ted packet is a SYN packet. If the ﬁlter function returns
TRUE registered callback function is invoked to process
the event. In line 23, a generic monitoring socket (msock)
is created. By default, it monitors all TCP ﬂows passing
through the middlebox unless the monitoring scope is lim-
ited by mtcp_bind_monitor(). In line 27, a callback function
(on_syn_rexmit) is registered for ude_syn with the montor-
ing socket. PRE_TCP in the function parameter requires that
the event should be raised before updating the middlebox
TCP state for the ﬂow.
4. PRELIMINARY EVALUATION
We are currently porting popular middlebox applications to
our framework. in The preliminary results look promising.
Scalability & Composability: We evaluate the robust-
ness of our system in a testbed that consists of four sets of
web client/server machines and an inline network toy web
monitor (an Intel Xeon E5-2697 14-core machine with 64 GB
RAM) that counts the number of ongoing connections. We
test the performance under two scenarios. One monitoring
socket is tested without TCP management while the other is
used to monitor ﬂows without payload inspection (per-packet
processing = without TCP semantics; per-ﬂow processing
= TCP sockets without deep payload inspection capability).
As the Figure 2 evaluation graph suggests, our toy monitor
Figure 2: Monitor Performance (HTTP GET 16K reqs/s)
Figure 3: Snort Performance (HTTP GET 16K reqs/s)
can withstand analyzing rates of up to 40 Gbps for 8KB web
pages while it can manage ﬂows at a rate of at least 10 Gbps
for small ﬁle sizes.
Improved abstraction: We have partially ported Snort [4],
a popular open-source signature-based network security mon-
itor. We replace around 8, 000 lines of source code with only
799 lines to support Snort’s default TCP stream management
module. At the moment, our version skips some elaborate
checks such as segment overlapping and connection anomaly
checks (e.g. arrival of TCP SYN packet on an already es-
tablished connection). Our porting process has substantially
simplﬁed the core framework since the new code focuses on
pattern matching rather than ﬂow management. We eval-
uate the performance of our stack by generating innocent
client/server web request/response pages for varying ﬁle sizes
in the same testbed with around 3, 000 HTTP Snort attack
signatures. For comparison, we also perform the same test
with default Snort setup with PF RING [1] I/O driver. Fig-
ure 3 shows that our version outperforms the I/O-optimized
Snort by a factor of 1.5 to 2.2.
Enhanced monitoring capabilities: Some security mon-
itors (such as Snort and Suricata) analyze ﬂows against elab-
orate reassembling routines for examining payload spanning
several successive TCP segments. These routines are usually
employed to detect attack patterns in malicious ﬂows. Our
stack provides user-friendly APIs that make development of
such routines much simpler in practice.
5. ACKNOWLEDGEMENT
This work was supported by the ICT R&D program of
MSIP/IITP, Republic of Korea [14-911-05-001, Development
of an NFV-inspired networked switch and an operating sys-
tem for multi-middlebox services].
6. REFERENCES
[1] PF RING I/O Driver. http://www.ntop.org/products/pf ring/.
[2] Suricata Open Source IDS. http://suricata-ids.org/.
[3] E. Jeong and et al. mTCP: a Highly Scalable User-level TCP
Stack for Multicore Systems. In NSDI, 2014.
[4] M. Roesch. Snort - Lightweight Intrusion Detection for Networks.
In LISA, 1999.
051015202530354002048409661448192Throughput (Gbps) File size (B) Per-packet processingPer-flow processing3.8 3.9 4.4 5.8 6.5 7.8 8.4 8.8 9.1 9.9 0246810121641285121024Throughput (Gbps) Web Page Size (B) Snort with PF_RINGSnort with mTCP356