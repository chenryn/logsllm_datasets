My aim is to make a five-category text classification
I am running transformers fine tuning bert with `cnnbase` model but my program
stops at `loss.backward()` without any prompt in `cmd`.
I debug find that the program stop at the loss.backward line without any error
prompt
My program executed successfully in `rnn base` such as `lstm` and `rcnn`.
But when I am running some `cnnbase` model the strange bug appears.
My cnn model code:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from transformers.modeling_bert import BertPreTrainedModel, BertModel
    n_filters = 200
    filter_sizes = [2,3,4]
    class BertCNN(BertPreTrainedModel):
        def __init__(self, config):
            super(BertPreTrainedModel, self).__init__(config)
            self.num_filters = n_filters
            self.filter_sizes = filter_sizes
            self.bert = BertModel(config)
            for param in self.bert.parameters():
                param.requires_grad = True
            self.convs = nn.ModuleList(
                [nn.Conv2d(1, self.num_filters, (k, config.hidden_size))
                    for k in self.filter_sizes])
            self.dropout = nn.Dropout(config.hidden_dropout_prob)
            self.fc_cnn = nn.Linear(self.num_filters *
                                    len(self.filter_sizes), config.num_labels)
        def conv_and_pool(self, x, conv):
            x = F.relu(conv(x)).squeeze(3)
            x = F.max_pool1d(x, x.size(2)).squeeze(2)
            return x
        def forward(self, input_ids,
                    attention_mask=None, token_type_ids=None, head_mask=None):
            outputs = self.bert(input_ids,
                                attention_mask=attention_mask,
                                token_type_ids=token_type_ids,
                                head_mask=head_mask)
            encoder_out, text_cls = outputs
            out = encoder_out.unsqueeze(1)
            out = torch.cat([self.conv_and_pool(out, conv)
                             for conv in self.convs], 1)
            out = self.dropout(out)
            out = self.fc_cnn(out)
            return out
My train code:
            for step, batch in enumerate(data):
                self.model.train()
                batch = tuple(t.to(self.device) for t in batch)
                input_ids, input_mask, segment_ids, label_ids = batch
                print("input_ids, input_mask, segment_ids, label_ids SIZE: \n")   
                print(input_ids.size(), input_mask.size(),segment_ids.size(), label_ids.size()) 
                # torch.Size([2, 80]) torch.Size([2, 80]) torch.Size([2, 80]) torch.Size([2])
                logits = self.model(input_ids, segment_ids, input_mask)
                print("logits and label ids size: ",logits.size(), label_ids.size())
                # torch.Size([2, 5]) torch.Size([2])
                loss = self.criterion(output=logits, target=label_ids) #loss function:CrossEntropyLoss()
                if len(self.n_gpu) >= 2:
                    loss = loss.mean()
                if self.gradient_accumulation_steps > 1:
                    loss = loss / self.gradient_accumulation_steps
                if self.fp16:
                    with amp.scale_loss(loss, self.optimizer) as scaled_loss:
                        scaled_loss.backward()
                    clip_grad_norm_(amp.master_params(self.optimizer), self.grad_clip)
                else:
                    loss.backward() # I debug find that the program stop at this line without any error prompt
HELP~！~ 、  
I posted my questions on various community platforms，stackoverflow、other
github repositories.  
No one replied to me.