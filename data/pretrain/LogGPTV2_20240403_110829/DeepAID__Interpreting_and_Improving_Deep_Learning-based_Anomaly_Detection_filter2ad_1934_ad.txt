𝑡 ,
1𝒙◦
2...𝒙◦
2...𝒙∗
𝒙∗
1𝒙∗
𝑆𝑇 (X◦; 𝜇1, 𝜇2) = True,
otherwise (iteratively).
The whole procedure for interpreting univariate time-series
anomalies is available at Algorithm 2 in Appendix A. We omit
the discussion of robustness for this kind of interpreters since there
is no study on adversarial attacks against such kind of interpreter
to the best of our knowledge. Besides, the robustness is naturally
stronger due to the discreteness of changes in time-series data.
Interpreting Multivariate Time-Series. We briefly introduce
how to extend our interpretation to multivariate time-series, which
can be viewed as a combination of tabular and univariate time-
series interpretation methods. We first locate a small number of
influential data points in an abnormal time-series. Then, for each
data point (essentially a feature vector), we can use the tabular
interpretation method (in §4.2) to search its reference.
4.4 Graph Data Interpreter
There are many types of graphs (such as attributed and unattributed)
and learning tasks (such as node classification and link prediction).
For attributed graph (i.e., each node or link has a feature vector), in-
terpretations can be obtained by the same idea as the interpretation
of multivariate time series. That is, first locating a small number
of abnormal nodes/links in the graph, and then using tabular in-
terpretation method to modify important feature dimensions to
find the reference. Therefore, below we primarily introduce how
to interpret unattributed graph anomalies. Since the learning task
in our demonstrative system (GLGV [6]) is link prediction, we will
introduce how to interpret abnormal links in graph anomaly.
𝑏) (𝑥◦
𝑎 and 𝑥◦
The common method of processing graph data is to embed it
into a continuous feature space, or directly leveraging end-to-end
graph neural networks (GNN). Unattributed graphs usually require
the first method, namely graph embedding (GE), such as Deep-
Walk [41]. Let E𝐺 denotes GE, then the general workflow of un-
supervised DL with graph G is to first compute the embedding
vector 𝒆 = E𝐺 (G). For link-level interpretation, an abnormal link
X◦ = (𝑥◦
are two nodes identify this link) is em-
bedded as 𝒆◦ = E𝐺 (X◦). Therefore, the interpretation of graph
link anomaly is derived through two steps. (1) Step 1: Interpret-
ing 𝒆◦ by finding its reference 𝒆∗. (2) Step 2: Finding the original
representation of 𝒆∗ in the graph.
Details of step 1 are omitted since it is the same as the tabular
interpretation. As for step 2, we find the original representation
X∗ = (𝑥∗
𝑏) of 𝒆∗ by solving the following optimization problem:
(12)
where F1(X∗) = ReLU(cid:0)E𝑅(cid:0)E𝐺 (X∗), 𝑓𝑅(E𝐺 (X∗))(cid:1) − (𝑡𝑅 − 𝜖)(cid:1)
𝑏) F1(X∗) + 𝜆F2(X∗, 𝒆∗)
𝑎,𝑥∗
argminX∗=(𝑥∗
and F2(X∗, 𝒆∗) = ∥E𝐺 (X∗) − 𝒆∗∥2.
(13)
Here the first term in objective function (12) means E𝐺 (X∗) is
decided to be normal by the interpreted system, and the second
term measures the difference of E𝐺 (X∗) and 𝒆∗ obtained in step 1.
The intuition of (12) is to force E𝐺 (X∗) to close to 𝒆∗, and ensure
E𝐺 (X∗) is a normal embedding vector. The constraint in (12) means
𝑎 and 𝑥◦
𝑥◦
are still in the node set denoted with V.
2 ∈ V,
s.t. 𝑥∗
1, 𝑥∗
𝑎, 𝑥◦
𝑎, 𝑥∗
𝑏
𝑏
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3203Problem (12) can be simply solved by gradient-based optimizer
if E𝐺 is differentiable. However, embeddings of inputs are derived
by an indifferentiable look-up operation [34] in many cases. For
example, most DL frameworks (e.g., PyTorch [40]) set the embed-
ding layer to be indifferentiable. A simple method is to use a fully
connected network (FCN) to achieve the same embedding function,
but this requires additional work to modify the interpreted model.
In Appendix A.2, we propose an alternative greedy solution to (12)
when E𝐺 is indifferentiable, and also provide the whole algorithm.
5 DISTILLING INTERPRETATIONS
As introduced in §3.3, to facilitate human interaction with DL-based
anomaly detection systems, we propose a model-based extension
based on our interpretations, referred to as Distiller. In this study,
we primarily design Distiller for tabular data and will extend it
to other data types in future work. Below, we introduce overview,
details, superiority, and application of Distiller.
Distiller Overview. In a nutshell, Distiller allows security analysts
to give feedback on interpretation results thus integrating expert
knowledge into the security system. Specifically, analysts can give
“feedback” (denoted with 𝑟𝑖) on a set of anomalies after checking
their interpretations (𝒙◦ − 𝒙∗) from Interpreter. The number and
content of feedback is highly free for analysts. We refer to “feedback
on interpretations” as “rule” denoted as (𝒙◦ − 𝒙∗) → 𝑟𝑖. The intu-
ition is that, after adding rules based on expert knowledge, Distiller
is able to automatically recognize similar anomalies in the future to
output more reliable results. The design goal of Distiller is to store
rules when analysts need to add new ones (called Update mode),
and to match “ruled” anomalies and report previous expert feed-
back (called Test mode). To this end, Distiller essentially consists
of two finite-state machines (FSM). The first FSM is used to model
𝐾-dimension interpretation (𝒙◦ − 𝒙∗), and the second FSM is used
to model the transition from interpretation to feedback 𝑟𝑖. Below,
we will introduce the design of two FSMs and two modes.
State and Transition Abstraction of FSMs (Figure 3). In the
first FSM, we split the value range of each dimension in the in-
terpretation vector 𝒙◦ − 𝒙∗ into 𝑀 equal-length intervals, then
we have totally 𝑀 × 𝑁 states. Each of 𝐾 dimensions in 𝒙◦ − 𝒙∗ is
mapped into one of states ˆ𝑠𝑖 according to its dimension and value.
Subsequently, transitions of these 𝐾 states are arranged according
to the order of decreasing effectiveness. That is, the state with the
largest effectiveness becomes the initial state, and the smallest one
is the final state. The second FSM contains all 𝑀𝑁 states in the
first FSM and variable states indicating feedback 𝑟𝑖. Transitions in
the second FSM are all single-step from an interpretation ˆ𝑠𝑖 (initial
state) to a feedback state 𝑟𝑖 (final state). The intuition of two FSMs
is that all dimensions of interpretations “contribute” to the feedback
(the second FSM), and their order matters (the first FSM).
Update and Test Mode. Distiller works on two modes: (1) Update
mode indicates adding new rules proposed by analysts. This can
be implemented by separately updating two transition matrices of
two FSMs according to the aforementioned transitions of the new
rule in two FSMs. (2) Test mode matches anomaly interpretations to
existing feedback. Interpretation 𝒙◦−𝒙∗ raised by our Interpreter is
first mapped into a sequence of states ˆ𝑠1ˆ𝑠2...ˆ𝑠𝐾. Then, the matching
Figure 3: States and transitions of DeepAID Distiller.
probability of the interpretation to each feedback ˆ𝑟 is calculated as:
(cid:0)P2(ˆ𝑟 | ˆ𝑠𝑖) 𝑖−1
P1(ˆ𝑠 𝑗+1 | ˆ𝑠 𝑗)(cid:1),
𝐾
𝑖=1
P (ˆ𝑟 | ˆ𝑠1ˆ𝑠2...ˆ𝑠𝐾) =
1
𝐾
𝑗=1
(14)
where P1 and P2 is transition probability in two FSMs/transition
matrices. The intuition is to calculate the mean of the product of
transition probability from ˆ𝑠1 to ˆ𝑠𝑖 and from ˆ𝑠𝑖 to ˆ𝑟 (𝑖 ∈ {1, 2, ..., 𝐾}).
Toy Example. In Figure 4, we provide a toy example of updating
and testing two anomalies (called A1/A2) from an empty Distiller.
The ground truth (unknown to analysts) of A1 and A2 is IP and Port
Scan. Suppose the analysts mark A1 as “Scanning” after reading the
interpretation (𝐾 = 3 here) and then updates Distiller with the new
rule ( 2 ), which essentially adds a new feedback state 𝑟1 indicating
“Scanning” and updates the corresponding transitions in the two
FSMs. At this time, if we test A1 and A2 ( 3 ), Distiller will return
strong confidence of matching A1 to “Scanning” (P(𝑟1|ˆ𝑠1ˆ𝑠2ˆ𝑠3) = 1)
and weak confidence of matching A2 to “Scanning” (P(𝑟1|ˆ𝑠4ˆ𝑠5ˆ𝑠3) =
0.33). Then after updating the second rule indicating A2 to feedback
“Port Scan” ( 4 ), Distiller ( 5 ) will return both strong confidences
of matching A1 to “Scanning” and matching A2 to “Port Scan”
(P(𝑟1|ˆ𝑠1ˆ𝑠2ˆ𝑠3) = P(𝑟2|ˆ𝑠4ˆ𝑠5ˆ𝑠3) = 0.83).
Distiller vs. ML/DL Classifier. Distiller shares some similarities
with ML/DL classifiers, e.g., update/test mode is similar to train-
ing/prediction on multi-class tasks, and feedback can be viewed as
labels. However, Distiller is more competent for human-in-the-loop
detection for three reasons. First and most important, Distiller is
more transparent and easy to modify. Security analysts can clearly
and simply modify transitions and states in Distiller. Second, the
number of classes/feedback in Distiller is adaptable. Third, Distiller
can preserve the ability of anomaly detection systems to detect
unknown threats by initially transiting all states in the first FSM
to a feedback state representing unforeseen threats. We conduct
experiments in §6.5 to demonstrate the superiority of Distiller.
Distiller vs. Rule Matching. Another potential baseline of Dis-
tiller is rule-matching methods. Compared with them, Distiller has
the generalization ability to report feedback for similar anomalies,
e.g., at 3 (Figure 4), A2 is also reported as “Scanning” due to its
similarity to the first rule (i.e., P(𝑟1|ˆ𝑠4ˆ𝑠5ˆ𝑠3) = 0.33 but not 0).
Reducing FPs via Distiller. We showcase how Distiller may help
to reduce two types of FPs in original anomaly detection models:
(1) Low confidence positives. Such FPs are caused by normal data
nearly decision boundary/threshold (e.g., 𝑡𝑅, 𝑡𝑃). To address them,
Transition in the 2nd FSMTransition in the 1st FSMInterpretationStateFeedback StateSession 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3204Figure 4: Toy example of updating and testing two anomalies A1 (green) and A2 (blue) in DeepAID Distiller.
we can modify Distiller by forcing some selected FP interpretations
to transit to feedback states indicating normal (or FP). (2) Concept
drift of normal data. We can locate and retrain FPs with very low
transition probability in the first FSM, which indicates such FPs are
neither normal (judged by DL model) nor known anomalies.
Theoretical Analysis. We provide proofs of the correctness, ac-
curacy bound, and complexity of Distiller in Appendix B.
6 EXPERIMENTS
In this section, we conduct several experiments with baseline inter-
preters, as well as provide several case studies to introduce how to
interpret and improve security-related anomaly detection systems
with DeepAID. Due to space limit, we primarily evaluate tabular and
time-series data based systems (as none of baseline supports graph
data). Experiments on graph data based systems as well as several
details and supplements of our experiments are left in Appendix C
and E. Overall, we provide a roadmap of this section:
• Experimental Setup. We introduce implementation and setup
of security systems, DeepAID, and baselines in this study (§6.1).
• Interpreter Performance. We demonstrate the performance of
DeepAID Interpreter with respect to fidelity, stability, efficiency,
and robustness, compared with existing works (§6.2).
• Understanding Model Decisions. We provide case studies on
leveraging DeepAID interpretations to establish trusts in model
decisions and discovering implicit model knowledge (§6.3).
• Enabling Debuggability. We provide a case on diagnosing and
• Human-in-the-Loop Detection. For tabular security systems,
we conduct reliable detection with expert feedbacks and knowl-
edge based on DeepAID Interpreter and Distiller (§6.5).
• Reducing FPs. For tabular security systems, we showcase that
debugging system mistakes based on DeepAID (§6.4).
DeepAID can reduce FPs based on Distiller (§6.6).
6.1 Experimental Setup
Here we briefly introduce the implementation and experimental
setup, and we refer readers to Appendix C for details, as they involve
many domain-specific backgrounds.
Security Systems. We use three demonstrative DL-based anom-
aly detection systems in security domains using three data types:
Kitsune[35], DeepLog[12], and GLGV[6], which are introduced in
§2.3. The data used in this work is primarily from their released
datasets (see Appendix C for more details).
DeepAID Implementation. For Interpreter, we fix the interpre-
tation results into 𝐾-dimension. By default, we set 𝜆 = 0.001 and
𝜖 = 0.01 in objective functions (7), (8), (12), learning rate 𝛼 = 0.5 in
Adam optimizer, and 𝑚𝑎𝑥𝑖𝑡𝑒𝑟 = 20. For tabular Interpreter, we set
neighborhood scale 𝜎𝑛 = 0 by default for initialization and evalu-
ate its effect in §6.2. For time-series Interpreter, we set 𝜇1 = 0.01
and 𝜇2 = 0.3 by default. For Distiller, we set 𝑀 = 20 (number of
intervals) by default. The sensitivity evaluation and configuration
guideline of these hyper-parameters are in Appendix F.
Baseline Interpreters and Implementation. In §6.2, we evalu-
ate DeepAID Interpreter with several baseline interpreters listed in
Table 1 and introduced in §2.1. (1) For supervised interpretations
(LIME, LEMNA, and DeepLIFT), we approximate the decision bound-
ary of the anomaly detection with a supervised DNN trained with
additionally sampled anomalies. (2) COIN and CADE are unsuper-
vised baselines. CADE needs a supervised encoding model before
interpreting [58] (See Appendix C for details). (3) We also directly
Select the Reference from Training Data (abbreviated as S.R.T.D.).
6.2 Interpreter Performance
We evaluate the performance of DeepAID and baseline interpreters
from four aspects: fidelity (with conciseness), stability, robustness,
and efficiency, which are of particular concern when deployed in
security domains. Their definitions are introduced in §4.1.
Fidelity-Conciseness Evaluation. To evaluate the fidelity of in-
terpretations, we define an indicator similar to [20] called Label
Flipping Rate (LFR) as the ratio of abnormal data that becomes
normal after being replaced by interpretation results. The intuition
is that LFR of high-fidelity interpreters will be higher since they ac-
curately pinpoint important dimensions inducing the abnormality.
Obviously, LFR tends to be higher when bringing more dimensions
in interpretations, which destroys conciseness on the other hand.
We evaluate this fidelity-conciseness trade-off in Figure 5a under
tabular and time-series based scenarios. The x-axis is in decreasing
order of used dimensions (increasing order of conciseness). From
the tabular result, CADE generally outperforms approximation-based
interpreters w.r.t. fidelity, while is not good as back propagation
based ones. We can observe that directly selecting “reference” from
0.835118.5Ref. Feature     Ref. Value  dst_host_diff_srv_rate  Ground Truth: IP Scanprotocol_typesrc_bytes(                    )Empty Distiller(                    )= (0+1*0+1*1*0.5)/3 = 0.17= (0+1*0+1*1*0.5)/3 = 0.17(                    )Add feedback  state “Scanning” “Scanning”1.01.01.01.01.01.01.00.51.00.5Update 1st rule“Port Scan”Update 2nd ruleAdd feedback  state “Port Scan” 1.01.01.01.01.0“Test Results”(                    )= (1+1*1+1*1*0.5)/3 = 0.83= (0+0*0+1*1*1)/3 = 0.33= = (1+1*1+1*1*1)/3 = 1Test two anomalies Test two anomalies = = (1+1*1+1*1*0.5)/3 = 0.83Update ModeTest ModeNo feedbackNo transitionUpdate DistillerInterpretations of two example anomaliesMappingMappingMappingRef. Feature     Ref. Value  0.01535.518.5MappingMappingStates in 1st FSMGround Truth: Port ScanStates in 1st FSMInterp. of A2 dst_host_same_srv_ratedst_host_countsrc_bytesInterp. of A1Mapping(                   )Update Distiller(                    )Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3205(a) Fidelity evaluation.
(b) Stability evaluation.
(c) Robustness (to noise).
Figure 4: Fidelity, stability, and robustness (to noise) evaluation of interpreters (higher is better).
(a) attack scale 𝛿𝑎 = 0.2
(b) neighborhood scale 𝜎𝑛 = 0.02
(c) neighborhood scale 𝜎𝑛 = 0.005
Figure 5: Adversarial robustness (to optimization-based attack) evaluation of DeepAID (higher is better).
Figure 6: Efficiency evaluation