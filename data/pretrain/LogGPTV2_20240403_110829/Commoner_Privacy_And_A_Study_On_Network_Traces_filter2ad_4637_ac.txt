If condition is met, for each statement within TRUE branch, the CISs of the variables from the conditional statement
are added to the left hand side’s CIS. Similarly when condition is not met, CISs within FALSE branch are extended to
include the CISs from the condition.
Table 2: Transformations of the contributing identity set (CIS) for sample statements
SQL statements, such as SELECT, GROUP BY, arithmetic and logical
operations, etc., but it ends with a single OUTPUT statement, which
produces either a single numerical output point or a sequence of (x,
y) output points, where x is a string or a number, and y is a number.
When processing each query, we keep track of identities that
relate to each variable or group created in the query, and their
contributions to the points in the output. These contributions start
as values of some data field (e.g., for network traces this could be
packet size, time, port number) or 1 (for record-counting queries)
and are transformed as query statements are processed. We call
this a set of tuples, where each tuple contains some data point,
and the set of identities and their contributions to this data point a
“contributing identity set” (CIS).
Table 2 shows how the processing of some common SQL state-
ments affects the contributing identity sets of the related variables
and groups. We further keep track of data fields associated with
each group and each variable, and we keep track of all queries
Q = {q1, ..., qn} posed by a given user for a given dataset D. Only
query specifications are kept, not the query outputs. When a new
query qn+1 arrives, we perform the following steps to check for
tracker attacks:
Step 1: Identify related queries. For each field f in field set
F(qn+1), identify Qr(qn+1)—a set of related queries from Q, such
that for each query qi in this set, its field set contains f .
Step 2: Create combinations of related queries. Create all
possible subsets of Qr(qn+1) and add to them qn+1, including an
empty subset. For each such subset S={qi1, ... , qib } perform cross-
query checking described next.
Step 3: Cross-query checking. Cross query checking is done
by rerunning each query qs from set S on dataset D, and producing
output data points d(qs). For simplicity, assume that each record
has only one identity related to it. In case of multiple identities,
the calculations described next would be performed for each role
separately (e.g., in a case of network traces we would perform
separate checks for sets of source IP addresses, and separate for
sets of destination IP addresses).
We process the associated contributing identity set (CIS) for each
output data point, and we calculate the following sets: (1) set V
containing values for each field f that contribute to the x value of
each output data point, and (2) CIS for each output data point. We
then identify a set of output data points in the past queries, ODPs,
that have common values in their set V with some data points
for qn+1. For these data points we calculate (1) set differences of
the CIS of each of the queries in ODP and of CIS of qn+1; (2) set
differences of the sets A and B, where A is the union of the CISs in
ODP and B is the CIS of query qn+1; (3) set differences of C and D,
where C is the union of the CISs in ODP and the CIS of qn+1, and
D is the CIS for the whole dataset. Each set difference calculation
is performed twice—once for the original placement of minuend
and subtrahend, and once when their places are switched. For each
resulting set, we perform a k-blending check on each data point. If
any of these checks fails, the query qn+1 is either rejected or the
data point that did not pass the k-blending check is fuzzed. This is
further illustrated in Figure 3, with data point for port 443 failing
the check and being omitted or merged into a larger range of ports.
Figure 3: Illustration of per-query accounting and k-anonymity
check for k = 2. Trol queries are shown in blue.
The downside of query introspection is that the number of checks
required for each new query grows as a power of 2, and may be
prohibitive as users pose more queries. However, the number of
checks grows with the number of related queries and not with the
number of all queries, and checks are only performed on some
output data points that have common values from the set V . A few
optimizations are possible to keep the size of the related query set
small. First, data providers could provide synthetic datasets that
are accessible without commoner privacy restriction. This would
allow researchers to practice query writing until they “get it right,”
without affecting their future queries. Second, we can store CISs
of the queries along with them so that queries do not need to be
rerun. This trades the processing cost for the storage cost. We
implement this approach and evaluate it in Section 5. The third
possible optimization would reduce the number of tracker checks
performed. When new queries are identical to old queries that were
run on the same dataset tracker, checks can be skipped without
loss of privacy.
a:ca:ba:de:ce:ce:df:dm:in:ig:igroup trace into g by tcp.dstportfeature: tcp.destination_portvalue: 443src: m (1), n (1), g (1)dst: i (1)feature: tcp.destination portvalue: 80src: a (3), e (3), f (1)dst: b (1), c (3), d (3)output g with g.count as histogramomit: x=80, y=7merge: x=[64,448], y=10x=80, y=7srcs: 3dsts: 3x=443, y=3srcs: 3dsts: 1    failedMetadataGround truthOutputGroupsPackets5714.1 Proof of Tracker Detection
Here we formally prove that query introspection protects against
known tracker attacks. For completeness, we repeat tracker defini-
tions from [8].
Let n = ||Id(D)|| be the count of individuals contributing to the
dataset, and let k be the privacy parameter for the chosen privacy
definition (k-anonymity, crowd-blending privacy or commoner
privacy). A query is said to be answerable if it satisfies the chosen
privacy definition.
Theorem 4.1 (Individual Tracker [8]). Let C = A ∩ B be a for-
mula identifying individual I , and suppose T = A ∩ ¯B is I ’s tracker where
A, T are both answerable. With three answerable queries, calculate: (eq1)
COU NT(C) = COU NT(A) − COU NT(T), (eq2) COU NT(C ∩ a) =
COU NT((T∪A)∩a)−COU NT(T). If COU NT(C∩a) = 0, I does not have
a characteristic a (negative compromise). If COU NT(C∩a) = COU NT(C),
I has a characteristic a (positive compromise). If COU NT(C) = 1, arbitrary
statistics about I can be computed from q(C) = q(A) − q(T).
Theorem 4.2. Query introspection protects against individual tracker.
Proof.
Individual tracker relies on queries A and T to be answerable.
When the second query is posed, our cross-query checking will test if com-
binations (union, set difference, intersection) of A and T are also answerable.
It will thus detect the individual tracker from equation (eq1), which relies
on set difference, and it will refuse to answer the second query. Similarly,
we can prove that if both queries from equation (eq2) are posed, in any
order, the second query will be rejected by our cross-query checking.
□
Theorem 4.3 (General Tracker [8]). Let T be a characteristic formula
whose query size is in the restricted subrange [2k, n − 2k], that is: 2k ≤
COU NT(T) ≤ n − 2k, where q(T) is always answerable due to its range
is within the allowed query range [k, n − k], and k ≤ n/4. The value of
any unanswerable query q(C) can be computed as follows using any general
tracker T . First calculate: Q = q(T) + q( ¯T). If COU NT(C)  n − k and the queries
on the right-hand side of the following equation are answerable: (eq4) q(C) =
2Q − q( ¯C ∪ T) − q( ¯C ∪ ¯T).
Theorem 4.4. Query introspection protects against general tracker.
Proof. We have q(T)+q( ¯T) = Q = q(C)+q( ¯C). Therefore, for equation
(eq3): q(C ∪ T) + q(C ∪ ¯T) = q(C) + Q Let COU NT(combined) =
COU NT(C ∪T) +COU NT(C ∪ ¯T). Cross-query checking will detect that
COU NT(C ∪ T) + COU NT(C ∪ ¯T) > COU NT(Q), and will continue
checking COU NT(combined)−COU NT(Q) = COU NT(C)  n − k and all queries on the right-hand side of the following
equation are answerable: (eq6) q(C) = q( ¯U )−q( ¯C∪T)+q(T)+q( ¯C ∩ T ∩U ).
Theorem 4.6. Query introspection protects against double tracker.
Proof. We can assume C ∩ T (cid:44) ∅, C ∩ U (cid:44) ∅, T ∩ U (cid:44) ∅, otherwise
T and U are not useful for double tracker. For equation (eq5) consider the
queries needed to calculate q(C). Considering q(U ) and q(C ∩ T ∩ U ), we
have q(U ) − q(C ∩ T ∩ U ) = q(C ∩ T) ⊂ q(C). Therefore, COU NT(U ) −
COU NT(C ∩ T ∩U ) < COU NT(C) = k. This combination will be tested
by cross-query checking and the last query posed by the user will be refused.
Similarly, we can find a combination of queries in equation (eq6) that will
be rejected by cross-query checking.
□
5 EVALUATING COMMONER PRIVACY
It is difficult to compare utility and privacy of commoner privacy
against those of crowd-blending and differential privacy because
these measures depend on the exact queries asked and on the
dataset’s composition. In this section we first qualitatively dis-
cuss these measures using examples of counting queries, which
are powerful primitives for many standard data-mining tasks [11].
We then quantitatively illustrate the utility gain of commoner pri-
vacy over crowd-blending and differential privacy using sample
queries over network traces. Finally, we quantify the overhead of
query introspection for sample compositions of queries on the same
dataset.
5.1 Qualitative Discussion
We first consider counting queries over identities that meet some con-
dition C. Utility-wise, differential privacy will release a noisy count,
but the amplitude of the noise will be low, as the global sensitivity
of the query is 1. Commoner and crowd-blending privacy will both
release a true count if there are k or more identities that meet the
condition C, and will release noisy count or zero otherwise. Privacy-
wise, differential privacy will not allow an attacker to learn if an
individual I meets the condition C. If the attacker knows that there
are m ≥ k − 1 identities in the dataset that meet condition C, prior
to participation of an individual I, commoner and crowd-blending
privacy will allow him to learn if I also meets C or not. Thus, for
counting queries over identities, commoner and crowd-blending pri-
vacy yield the same outcomes, and differential privacy has a slightly
lower utility and a much stronger privacy guarantee.
We next consider counting queries over records that meet condition
C. Utility-wise, differential privacy will release a noisy count; in
the case of long-tailed datasets, the noise’s amplitude may be large.
Assume that for a given data point there are n identities with records
rec(I1), .., rec(In) that meet condition C. Crowd-blending privacy
will release a true count if∃m|n ≥ m ≥ k and ||rec(I1)|| = ||rec(I2)||
= ... = ||rec(Im)||, and it will release a noisy count (or zero) otherwise.
Commoner privacy will release a true count if ∃m|n ≥ m ≥ k
and there are no outliers among the counts ||rec(I1)||, .., ||rec(Im)||,
and it will release noisy count (or zero) otherwise. Privacy-wise,
differential privacy will not allow the attacker to learn any specific
record or the value of a data field for any individual I. If the attacker
knows that there are m, m ≥ k−1 identities in the dataset and knows
||rec(I1)||, ..., ||rec(Im)||, crowd-blending and commoner privacy
will allow him to learn ||rec(I)|| for another individual I that blends
with I1, ..., Im. Thus, for counting queries over records, commoner
privacy yields higher utility than crowd-blending privacy, and both
have higher utility than differential privacy. This higher utility comes
at the cost of lower privacy guarantees as the all-but-one attacker can
learn information about an individual.
572(a) #pkts per source port
(b) #pkts per dst service port
(c) # connections in trace
(d) traffic volume in trace
Figure 4: % of output points fuzzed for four select queries on a network trace.
(a) #pkts per source port
(b) #pkts per dst service port
(c) # connections in trace
(d) traffic volume in trace
Figure 5: Utility loss and privacy risk for differential privacy, as we vary ∆f /ϵ.
(a) #pkts per source port
(b) #pkts per dst service port
(c) #connections in trace
(d) traffic volume in trace
Figure 6: Utility loss for differential privacy, crowd-blending privacy, and commoner privacy
Similarly, if we considered sum queries over data fields, the utility
and privacy of commoner, crowd-blending, and differential privacy
would have the same relationship as counting queries over records.
An individual I may be justly concerned that an all-but-one
attacker can learn his exact contribution cI to some aggregate
output—either the count of records that meet some condition C or a
value of a data field in a record. Specifically, cI could be unique and
thus a quasi-identifier; learning it may reveal presence or absence
of I in the dataset. Note, however, that the all-but-one attacker
is unrealistic, and we should discuss what a more realistic, inter-
active attacker can learn. Crowd-blending privacy will allow an
interactive attacker to learn cI only if cI is shared by at least k − 1
other identities; i.e., it is not a quasi-identifier. Thus the attacker
cannot learn if I is in the dataset. Commoner privacy will allow an
interactive attacker to learn some approximation of cI , only if cI is
similar to contributions of k − 1 other individuals. For example, an
interactive attacker may learn the average salary in the company,
calculated to include I’s salary, only if I’s salary is not an outlier. If
I’s salary is an outlier, the attacker only learns the average salary
of other employees and cannot infer which of the following is true:
(1) I is in the dataset and has a salary within the expected range,
(2) I is in the dataset and his salary was omitted from calculations,
(3) I is not in the dataset.
5.2 Quantifying Utility Gains
We now illustrate one use case for commoner privacy—queries
on network traces—and compare utility afforded by differential
privacy, crowd-blending privacy, and commoner privacy. We use a
trace provided by the MAWI project [1]. The trace was collected on
August 1st, 2016 and contains 15 minutes of traffic on a large US-
Japan Trans-Pacific link. We use the first two million TCP packets
in our evaluation. We run four common network processing queries
on this data:
q1. Histogram of packet counts sent per source port
q2. Histogram of packet counts received per destination service
port
q3. Total connection count in the trace
q4. Total traffic volume in the trace
573All queries have high global sensitivity, as a large sender or receiver
can dominate the count of packets/connections/bytes.
As there are many ways to fuzz data points—add noise, omit or
aggregate them—we first compare only the number of data points
that will be fuzzed. If we assume that the same approach will be
used for fuzzing, then the mechanism with lowest number of points
fuzzed will yield the highest utility.
Figure 4 shows the percentage of output points fuzzed by dif-
ferential privacy, crowd-blending privacy, and commoner privacy
(with STDEV and MAD approaches for outlier detection) for q1–q4.
We vary the parameter k from 2 to 10. In all cases, differential pri-
vacy fuzzes all output points, while crowd-blending and commoner
privacy fuzz fewer. The actual utility gain depends on the dataset
as well as the queries being asked.
For query q1 (Fig. 4(a))—histogram of packets sent by a source
port—we do not expect much similarity in hosts’ behaviors, as
source ports are often chosen by client applications at random,
and hosts may send different amount of packets per port. For this
query, crowd-blending performs just slightly better than differen-
tial privacy at k = 2, fuzzing 90% of points. Commoner privacy
fuzzes much fewer points up until k = 7, with STDEV and MAD
approaches having comparable performance. At k = 2 only 45% of
points are fuzzed, at k = 3 75% are fuzzed.
For query q2 (Fig. 4(b))—histogram of packets received by a desti-