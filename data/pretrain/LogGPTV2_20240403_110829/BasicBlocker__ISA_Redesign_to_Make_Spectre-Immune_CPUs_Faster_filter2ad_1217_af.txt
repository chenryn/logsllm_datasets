iterations by storing a specified value into a dedicated register. The
start and end address of the loop are encoded into the bb instruction,
by indicating with two separate flags whether the corresponding
basic block is the start or end block of the loop. These two flags in
the bb instruction are necessary for each loop counter set, which
means that the bb instruction needs 2ùëõ bits to support ùëõ loop counter
sets.
bb 2, 1, 00 , 00 ; len = 2, seq = 1
add a0 , a0 , a1
lcnt 3, lc1 ; 3 iterations , set 1
bb 2, 0, 01 , 01 ; loop start / end
add a1 , a2 , a2
mul a2 , a1 , a2
bb 7, 0, 00 , 00 ; after loop
Listing 1: Single basic block loop with 3 iterations in counter
set 1; Colors correspond to the execution trace in 2
Listing 1 shows the exemplary use of the hardware loop counter.
In line 3, the counter in loop set ls1 is initialized to 3. The following
bb instruction has the start- and end flag for loop set 1 enabled
which indicates a loop that starts at the beginning of this basic
block and stretches until the end of the same basic block. Each
bit in the flags represents one loop counter set, allowing nested
loops with the same start- or end address and nested loops sharing
the same basic block as start or end. It is possible to model loops
that stretch across multiple basic blocks by setting the start and
end flags in the respective basic blocks accordingly. When the bb
instruction with the start flag is executed, the current ùëÉùê∂ is saved as
start address in the corresponding loop counter set. Simultaneously,
the counter value of that set is decremented by one. When the
execution reaches the bb instruction with the corresponding end
flag, the target address (which determines where the CPU continues
execution) is set to the corresponding start address if the counter
is not zero. Otherwise, the basic block is handled like a normal
sequential basic block and the loop will exit.
bb 2, 1, 00, 00 ; len = 2, seq. block
bb 2, 0, 01, 01 ; loop: start L1, end L1
add a0, a0, a1
lcnt 2, lc1 ; 2 iterations, set 1
bb 2, 0, 01, 01 ; loop: start L1, end L1
add a1, a2, a2
mul a2, a1, a2
bb 2, 0, 01, 01 ; loop: start L1, end L1
add a1, a2, a2
mul a2, a1, a2
bb 7, 0, 00, 00 ; after loop
add a1, a2, a2
mul a2, a1, a2
Listing 2: Execution trace of CPU with color matched
instructions to the code sequence in 1.
In Listing 2, the instruction trace of the program snippet from
Listing 1 is shown as it is executed by the CPU. Since the first bb
instruction indicates a sequential basic block, the CPU immediately
fetches the bb instruction of the next basic block which notifies
the fetch unit that the second basic block is the start and end block
of the loop. After that, the remaining add and lcnt instructions
are executed to finish the first basic block. From now on the loop
counter determines the execution flow. Since the second basic block
is the only basic block of the loop, the bb instruction of this block
is fetched again, to prepare the second loop round, before the basic
block is executed to complete the first round. This happens again
until the loop counter is zero, resulting in fetching the last bb
instruction, to exit the loop, before the last round of the loop is
executed. Afterwards the execution continues outside of the loop
with the normal instruction flow.
We Implemented our proposed hardware loop counter concept
in the VexRiscv core and added elementary compiler support for
one loop counter set. Because the loop counter can only be used
for loops that do not contain calls and have a fixed trip count, it
can only be applied by the compiler to a small subset of the loops
in the benchmarks. While the impact of the hardware loop counter
is neglegtable for most benchmarks, it substantially improves the
speed on others. The speedup for edn improved from 2.63√ó to
2.70√ó, getting closer to the 2.85√ó speedup of the speculative version
compared to the non-speculative baseline. For ud the hardware loop
counter enabled the BasicBlocker variant to match the speed of
the speculative version. The biggest impact can be observed for
aha-mont where the speedup increased from 2.27√ó to 3.13√ó.
B SYNERGIES BETWEEN BASICBLOCKER
AND ALGORITHMIC IMPROVEMENTS
There are continual announcements of performance improvements
in software packages to handle computational ‚Äúhot spots‚Äù, such
as the inner loops in audio/video processing. The main point of
this appendix is that the natural pursuit of higher-speed software
favors BasicBlocker: software changes that improve performance
on current non-BasicBlocker CPUs tend to produce even larger
improvements on BasicBlocker CPUs.
B.1 Dimensions of performance analysis
The performance evaluation in Section 6 focuses on measuring the
impact of changing (1) an existing CPU with an existing compiler to
(2) a BasicBlocker CPU with a BasicBlocker-aware compiler. Each
of the benchmarks being compiled and run‚Äîfor example, the st
software in the middle of the graphs in that section‚Äîis treated as
being set in stone. There is no effort in Section 6 to modify st for
better performance, whether by explicit changes in the st code or
by additions to the compiler‚Äôs built-in optimizations beyond the
BasicBlocker support described earlier.
This appendix instead treats the software as a third variable be-
yond the compiler and the CPU, reflecting the reality that software
1
2
3
4
5
6
7
15
ArXiv Version, 2021, May
J. Thoma, J. Feldtkeller, M. Krausz, T. G√ºneysu, D. J. Bernstein
evolves for the pursuit of performance. For example, we modified
the st software to obtain the st-opt software described below, com-
puting the same results as st at higher speed. Our goal in changing
st to st-opt was to match what typical programmers familiar with
performance would naturally do if st turned out to be a bottleneck.
We used a profiler (specifically gcc -pg) to see bottlenecks on an
existing CPU (specifically the ARM Cortex-A7 CPU in a Raspberry
Pi 2), inspected the software to identify underlying inefficiencies,
and removed those inefficiencies, while retaining portability.
We selected three case studies for these software modifications:
st, aha-mont, and crc32. We were aiming here for a spread of
different types of code. Within our benchmarks, aha-mont is at the
worst quartile for BasicBlocker, while st and crc32 are slightly bet-
ter than median; st uses floating-point arithmetic, while aha-mont
and crc32 do not.
It is important to observe that our modifications remove cross-
platform inefficiencies. Switching from st, aha-mont, and crc32
to st-opt, aha-mont-opt, and crc32-opt saves time on current
CPUs. The same changes save time on BasicBlocker‚Äîand, as our
measurements show, reduces the cost of BasicBlocker compared to
current CPUs. We summarize the inefficiencies below for each case
study, and explain why the benefits for BasicBlocker should not be
viewed as a surprise.
B.2 From st to st-opt
Embench describes st as a ‚Äústatistics‚Äù benchmark. The benchmark
computes basic statistics regarding two length-100 arrays of double-
precision floating-point numbers: the sum, mean, variance, and
standard deviation of each array, and the correlation of the two
arrays.
However, profiling immediately shows that most of the time in
st is spent initializing the arrays. In general, Embench does not
partition the function being benchmarked from the preparation of
input to the function. In the case of st, what is benchmarked is a
main loop that calls Initialize for one array, computes the sum
etc. for that array, calls Initialize for the other array, computes
the sum etc. for that array, computes the correlation, and repeats.
Embench describes itself as measuring solely ‚Äúreal programs‚Äù, so
presumably it is intentional that the initialization is measured. This
means that removing the initialization from the benchmarks, for
example by precomputing the st arrays at compile time, would not
be a valid optimization. The operation being benchmarked includes
computing the arrays from scratch and then computing statistics
given the arrays.
The st code includes a function computing sum and mean, a
function computing variance and standard deviation, and a func-
tion computing correlation. The sum computation is almost a text-
book loop through the input array, except that each iteration says
*Sum += Array[i], reading and updating the function output via
a pointer; st-opt instead does the textbook sum += Array[i],
using a local sum variable, followed by *Sum = sum after the loop.
The second and third functions similarly follow textbook formulas,
but the third function computes the variance and standard devia-
tion again, repeating essentially the code from the second function;
st-opt instead saves these extra results from the third function
and eliminates the redundant second function. If one array were
involved in multiple correlations then it would be more efficient to
cache the standard deviation.
The initialization in st, before the statistics are computed, sets
position i in the array to i + RandomInteger()/8095.0. Here
RandomInteger is an ad-hoc linear-congruential random-number
generator where each output is the previous output times 133 plus
81 modulo 8095. On typical CPUs, the (integer and floating-point)
divisions by 8095 are expensive operations, more expensive than a
series of several loads, additions, and multiplications used in the
subsequent statistical computations. Floating-point operations are
particularly expensive on CPUs without floating-point instructions,
such as VexRiscv, since each floating-point operation is then im-
plemented by a ‚Äúsoft float‚Äù library, although it is not clear how
important this is as a benchmarking scenario.
There are faster ways to produce better-distributed floating-point
numbers between 0 and 1, but internally st checks for known an-
swers for these particular numbers, so let‚Äôs assume that computing
these not-very-random arrays is part of the requirement. It is well
known how to convert integer division into a short sequence of mul-
tiplications, shifts, etc.; st-opt reduces modulo 8095 in this way. It
then multiplies by 1/8095.0, rather than dividing by 8095.0; this
can round differently, but such small differences are conventionally
accepted as floating-point optimizations and, more to the point, are
accepted by the internal st tests. The RandomInteger() function is
inlined, with its intermediate outputs being kept in a local variable
and saved after the loop.
Finally, each of these loops is marked in st-opt with an explicit
UNROLL(4) or UNROLL(2), where UNROLL uses existing compiler fea-
tures to control the amount of unrolling. The overall increase from
st compiled code size to st-opt compiled code size is negligible:
around 100 bytes, depending on the instruction set.
Except for the possibility of branches inside a ‚Äúsoft float‚Äù library,
there is nothing inherently unpredictable in the st control flow: the
program sweeps sequentially through length-100 arrays, perform-
ing the same sequence of operations in each iteration. The short
basic blocks that we measured in st, averaging under 5 instructions
with median just 3 instructions, are an artifact of easily remov-
able inefficiencies described above in st, such as the redundant
loops recomputing variances, the loop constantly calling a separate
RandomInteger function, and failures of unrolling. Some of the
other speedups described above, such as eliminating various RAM
accesses, do not increase basic-block sizes‚Äîon the contrary, elimi-
nating these instructions makes some basic blocks shorter‚Äîbut this
leaves room for further unrolling, again improving performance
across platforms.
B.3 From aha-mont to aha-mont-opt
Embench describes aha-mont as a ‚ÄúMontgomery multiplication‚Äù
benchmark. Montgomery multiplication is a well-known method
to carry out integer operations modulo a specified odd modulus
ùëö without using divisions by ùëö. The aha-mont code is a slightly
modified version of a snippet from Warren‚Äôs ‚ÄúHacker‚Äôs Delight‚Äù
code corpus, which is archived at https://web.archive.org/web/
20190715012506/http://hackersdelight.org/hdcode.htm.
Profiling again shows that most of the time in the benchmark
is actually taken by something else: 65% of the aha-mont time is
16
BasicBlocker: ISA Redesign to Make Spectre-Immune CPUs Faster
ArXiv Version, 2021, May
spent in divisions by ùëö, and another 25% is spent in an xbinGCD
function, while Montgomery multiplication takes under 10%. The
reason that there are divisions by ùëö, when the point of Montgomery
multiplication is to avoid divisions, is that Warren‚Äôs snippet includes
a main routine with tests, and the tests use divisions.
The modulus ùëö is a uint64, possibly as large as 264 ‚àí 1. The
division-by-ùëö function modul64 takes two uint64 inputs ùë• and ùë¶,
where ùë• < ùëö, and returns the remainder when the 128-bit integer
264ùë• +ùë¶ is divided by ùëö. The code, assuming that the compiler does
not support a uint128 type, uses 64 iterations of doubling 264ùë• + ùë¶
and subtracting ùëö from ùë• if ùë• ‚â• ùëö, while taking care to check for
the possibility that the doubling overflows. Overall each iteration
of the main division loop uses several uint64 operations.
A minor inefficiency here is as follows. The code was originally
developed to compute not just the remainder but also the quotient.
The obvious way to do this is to add 1 to a new variable ùëû if ùë• ‚â•
ùëö, and double ùëû on each loop. The original code does better by
observing that the space needed for ùëû after ùëñ iterations, namely ùëñ
bits, matches the space cleared at the bottom of ùë¶, so one can simply
add 1 to ùë¶ if ùë• ‚â• ùëö, which eliminates the extra doubling of ùëû since
ùë¶ is being doubled anyway. However, in the context of aha-mont,
the quotient is thrown away, so the addition is a waste of time.
The merging of ùëû into ùë¶ means that this dead-code elimination is
beyond what the compiler figures out automatically.
There is, however, a much larger inefficiency in this division
code, namely the branches. The branches involved in counting 64
iterations are predictable and can be straightforwardly reduced
by unrolling, but the branches involved in comparing ùë• to ùëö are
not. One expects 0.5 mispredictions per loop; on Intel CPUs, for
example, this would cost several extra cycles per loop.
Faster division algorithms‚Äîincluding algorithms that handle
multiple bits at a time, branchless algorithms, and algorithms that
precompute a reciprocal of ùëö‚Äîare not a new topic, and in fact one
can already find more options for divisions in Warren‚Äôs code corpus.
We took the last option from that corpus‚Äîthe fastest, according to
the documentation‚Äîand incorporated it into aha-mont-opt. We
would expect anyone who cares about the performance of this code
to benchmark several options and take the fastest option for the
target platform, the same way that the Linux kernel automatically
benchmarks several raid6 algorithms and selects the fastest. Note
that there was no reason for Warren to bother with this speedup of
tests inside his Montgomery snippet; for the aha-mont benchmark,
however, these tests dominate the CPU time.
The xbinGCD function has even larger cross-platform branch-
prediction problems than modul64. The goal here is to compute
the inverse of ùëö modulo 264; this is a precomputation step needed
for Montgomery multiplication. The xbinGCD function handles
this with a general-purpose binary-gcd algorithm, as the name
suggests. Again there is literature on more efficient algorithms‚Äî
faster ways to compute binary gcd, and, more to the point, faster
ways to compute inverses modulo powers of 2. The inversion code
inside aha-mont-opt uses just 5 iterations (again from Warren‚Äôs
code corpus!), where each iteration uses 2 multiplications and 1
subtraction; this is an order of magnitude faster (on a Raspberry Pi
2) than the inversion code inside aha-mont.
It is clear that more work on aha-mont-opt would produce even
better results, especially on 32-bit platforms, where it is well known
that high-precision computations should be expressed in terms of
32-bit integers rather than 64-bit integers. For RISC-V, the basic in-
struction set is unusual in that it does not include carries, and it also
does not include conditional arithmetic, so a compiler writer imple-
menting uint64 in terms of 32-bit operations will naturally resort
to branches. Increased attention to RISC-V optimization will, pre-
sumably, spur development of branchless carryless algorithms for
common sequences of 64-bit operations‚Äîimproving performance
of 64-bit code on existing 32-bit RISC-V CPUs, and improving per-
formance even more with BasicBlocker.
B.4 From crc32 to crc32-opt
Embench describes crc32 as a ‚ÄúCRC error checking 32b‚Äù bench-
mark. The main crc32pseudo function computes a 32-bit cyclic
redundancy check of 8192 bits of data. The conventional way to
compute a CRC is to update the CRC for ùëè bits of data at a time,
using a few 32-bit logic/shift operations and a lookup of 32 bits in a
2ùëè-entry table. Both crc32 and crc32-opt use ùëè = 8, so there are
1024 iterations of updates.
Profiling once again shows that most of the time is spent on
something else. For crc32, like st, most of the time is spent setting