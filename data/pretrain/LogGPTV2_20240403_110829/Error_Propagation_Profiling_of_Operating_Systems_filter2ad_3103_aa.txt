# Error Propagation Profiling of Operating Systems

**Authors:**
Andréas Johansson and Neeraj Suri  
Department of Computer Science, Technische Universität Darmstadt  
{aja, suri}@informatik.tu-darmstadt.de

## Abstract
The robustness of an Operating System (OS) is a critical factor in the overall reliability of a computing system. This paper introduces an error propagation profiling framework aimed at enhancing OS robustness through the use of add-on software (SW) wrappers. The framework facilitates the systematic identification and location of design and operational vulnerabilities, as well as the quantification of their potential impact. Focusing on data (value) errors in OS drivers, the paper presents a set of measures to help designers identify such vulnerabilities, either on an OS service (system call) basis or per driver. A case study using Windows CE .Net demonstrates the utility of the proposed approach.

## 1. Introduction
The robustness of an OS, defined as its ability to withstand perturbations while continuing to provide services, directly influences the robustness of the entire system. Applications rely on the OS to deliver correct and timely services, and any failure to do so can impact the system's overall performance. Despite ongoing efforts to enhance OS robustness and improvements in SW development processes, OS failures still occur. One reason for this is the complexity of interactions between the OS and various SW components, which are often unknown at the time of OS design. This dynamic nature makes it challenging to design an OS that is comprehensively robust against errors in its operational environment.

This paper proposes a methodology for profiling the OS platform using experimental fault injection (FI) and data error propagation path analysis. The focus is on errors occurring in device drivers, which are a significant source of OS failures. Drivers are often not tested as rigorously as the OS kernel, may be designed externally without complete system details, and can be affected by hardware malfunctions or external errors. Throughout this paper, "errors" refer to data-level errors, i.e., errors that affect the value of some variable in a program.

The goal is to demonstrate a quantifiable and repeatable method for assessing error propagation in an OS, even when source code access is not available, as is the case with most commercial OSs. The lack of source code poses two main challenges: (a) no internal changes can be made to the OS or its drivers, and (b) error propagation analysis must be based on observations made only at the interface between components. The non-dependency on source code also allows the method to be applied to systems without readily available source code, avoiding the potential bias introduced by white-box testing approaches.

To illustrate the proposed profiling methodology, a case study using the Windows CE .Net operating system is presented. This OS was chosen for its representativeness and the high degree of control and limited complexity it offers. The results from the case study are interpreted using a set of measures designed to capture the error propagation properties of the OS. The study examines the relationships between individual OS services and driver services, as well as composite metrics describing the exposure of errors on OS services and the diffusion of errors from specific drivers. Two drivers are studied, and their error diffusion properties are compared. The main findings indicate that many errors do not propagate or do so in a robust manner, meaning the effect of an error is visible at the application level but does not lead to a failure. However, a few significant error propagation paths are identified, representing a serious threat to a highly robust design.

## 2. Related Work
Robustness studies of large SW systems have been conducted for many years. In [8], the robustness of C-libraries was evaluated and enhanced using wrappers. POSIX interfaces are the targets in [14], where OSs are compared using failure mode analysis. The techniques used in [14] partially overlap with ours, but they focus on applications rather than drivers as the source of errors. The effects of driver errors on the Linux kernel were studied in [1]. Our approach differs in that we focus on error propagation measures to facilitate wrapper placement. In [2], an extensive failure mode analysis was performed using code mutations in drivers to build a dependability benchmark. Micro-kernels have also been the subject of studies [3], where bit-flips in kernel APIs or memory simulate both hardware and SW faults. Our work contributes by focusing on quantifiable measures of error propagation and specifically on data-level errors, whereas other studies have used different error models such as bit-flips [3] and code mutations [2].

In our prior work, the EPIC framework (Exposure, Permeability, Impact, and Criticality), together with the supporting experimental tool PROPANE [11, 12], profiles static, modular software for error propagation to determine effective wrapper placement. The framework focuses on profiling the signals used in module interactions using both error propagation and effect profiles. Using permeability and exposure metrics, propagation profiles reveal where errors propagate through the system and which modules/signals are more exposed to propagating errors. In our current OS-themed work, the emphasis is on dynamic software interactions, as the set of applications is not generally known in advance. Thus, all possible interaction paths (and consequently error propagation paths) are not known a priori.

Tools have also been developed to protect systems from malfunctioning drivers. In [19], drivers are wrapped to track erroneous memory accesses, a major problem in device drivers. One issue raised in [19] is that some data-level errors cannot be automatically traced. We believe our approach complements [19] in this regard. In [18], an Interface Definition Specification is used to improve driver robustness. The Microsoft SLAM project [4] uses a static verification technique to ensure that device drivers adhere to a given interface. While this project has the same goal as ours, it relies on source code knowledge and formal specifications, whereas our approach does not.

## 3. System and Error Model
Given the diversity of OS implementations, we propose a simple system model comprising four major layers: hardware, drivers, OS, and applications. This model is generic enough to be representative and applicable to most modern OSs, such as Windows, GNU/Linux, and UNIX. Each layer provides a set of services to its neighboring layers. For us, a service is typically a function call, e.g., function entry points in drivers and OS system calls. A set of services constitutes an interface, such as the OS-driver interface.

### 3.1. System Model
- **Application Layer**: Interacts with the OS through the OS-Application interface.
- **OS Layer**: Provides services to applications and interacts with drivers through the OS-Driver interface.
- **Driver Layer**: Handles interactions between the hardware and the OS. We treat drivers as separate components rather than subcomponents of the OS to better represent component interactions.
- **Hardware Layer**: The physical platform on which the system runs.

Each driver exports a set of services to the OS and imports services from the OS. For a specific driver \(D_x\), the set of exported services is denoted as \(\{ds_{x.1}, ds_{x.2}, \ldots, ds_{x.M}\}\), where \(ds_{x.y}\) is the \(y\)-th service exported by driver \(D_x\). The set of imported services from the OS is denoted as \(\{os_{x.1}, os_{x.2}, \ldots, os_{x.K}\}\), where \(os_{x.q}\) is the \(q\)-th imported OS service by \(D_x\).

### 3.2. Error Model
Our error model focuses on transient data-level errors, which can result from implementation defects in the driver or value faults related to malfunctioning hardware. The error model for error injection has three basic components: the type of error, the location of the error, and the timing of the injection.

#### 3.2.1. Error Types
We consider data-level errors based on the C-type of each parameter. Table 1 shows an overview of the types and the number of test cases defined for each type. For each C-type, different data values are chosen as test cases, including offset values, common values, and boundary values.

| C-Type | #Cases | #Identifiers |
|--------|--------|--------------|
| int    | 7      | 1            |
| unsigned int | 5 | 1 |
| long   | 7      | 1            |
| unsigned long | 5 | 1 |
| short  | 7      | 1            |
| unsigned short | 5 | 1 |
| LARGE INTEGER | 7 | 1 |
| * void | 3 | 1 |
| char   | 7      | 1            |
| unsigned char | 5 | 1 |
| wchar_t | 5 | 1 |
| bool   | 1      | 1            |
| enums  | multiple cases | multiple cases |
| structs | multiple cases | multiple cases |

Table 1: Error types used in this study.

As an example, Table 2 shows the error cases for the `int` type.

| Case # | New Value |
|--------|-----------|
| 1      | (Previous value) - 1 |
| 2      | (Previous value) + 1 |
| 3      | 1          |
| 4      | 0          |
| 5      | -1         |
| 6      | INT_MIN    |
| 7      | INT_MAX    |

Table 2: Error cases for type `int`.

#### 3.2.2. Error Location
We focus on data errors in the OS-driver interface. By introducing errors in the parameters used in this interface, we simulate errors occurring within the driver. An example of an OS service in this interface is shown below:

```c
LONG RegQueryValueEx(
    HKEY hKey,
    LPCWSTR lpValueName,
    LPDWORD lpReserved,
    LPDWORD lpType,
    LPBYTE lpData,
    LPDWORD lpcbData
);
```

## 4. Measures for Profiling OS Errors in Drivers
To profile the OS for errors in drivers, we present a set of measures that help designers locate and quantify vulnerabilities. These measures include:

- **Error Exposure**: The likelihood that an error will propagate through the system.
- **Error Permeability**: The extent to which an error can pass through the system without being detected or corrected.
- **Error Impact**: The severity of the effect of an error on the system.
- **Criticality**: The importance of the component or service in the system, and the potential consequences of an error.

These measures are designed to be quantifiable and repeatable, allowing for systematic analysis of error propagation in the OS.

## 5. Case Study: Windows CE .Net
To demonstrate the proposed profiling methodology, a case study using the Windows CE .Net operating system is presented. This OS was chosen for its representativeness and the high degree of control and limited complexity it offers. The results from the case study are interpreted using the set of measures described in Section 4.

### 5.1. Experimental Setup
The experimental setup involves injecting errors into the parameters of the OS-driver interface and observing the behavior of the system. Two drivers are studied, and their error diffusion properties are compared. The study examines the relationships between individual OS services and driver services, as well as composite metrics describing the exposure of errors on OS services and the diffusion of errors from specific drivers.

### 5.2. Results and Analysis
The main findings indicate that many errors do not propagate or do so in a robust manner, meaning the effect of an error is visible at the application level but does not lead to a failure. However, a few significant error propagation paths are identified, representing a serious threat to a highly robust design. The results are discussed in detail, and future research directions are outlined.

## 6. Conclusions
This paper presents an error propagation profiling framework for enhancing the robustness of operating systems. The framework facilitates the systematic identification and location of design and operational vulnerabilities, as well as the quantification of their potential impact. Focusing on data (value) errors in OS drivers, the paper introduces a set of measures to help designers identify such vulnerabilities. A case study using Windows CE .Net demonstrates the utility of the proposed approach. The contributions of this work include the development of OS error propagation measures, quantifiable methods for assessing these measures without source code availability, and a case study to demonstrate the experimental assessment process.

**Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05)**  
**0-7695-2282-3/05 $20.00 © 2005 IEEE**