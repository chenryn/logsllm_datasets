300.69
99.22%
99.42%
7.60
Running Time(s)
Disass.
Pattern Gen. Matching
9.24
0.015
415.6
0.09
44.75
7.85
123.8
5.29
0.11
1.79
42.46
0.02
4.44
0
210.41
0.05
23.17
3.57
66.31
2.71
0.06
0.98
20.48
0.01
0.34
0.01
56.52
0.01
4.05
5.28
2.6
0.22
0.01
0.26
323.06
0
Figure 3. Pattern matching between randomized and
original programs. Programs are from SPEC CPU2000
benchmark, the Apache web server httpd, and the GazTek
web server ghttpd. Code size refers only to the code seg-
ment size, not the size of the entire executable program.
rived from the other) were computed as well. The experi-
ments and the results are described in more detail below.
We were not able to use any existing tools designed to
produce an obfuscated version of an arbitrary executable
program that contain all common obfuscation techniques.
Previous work [12] manually generated the obfuscated test
cases with simple obfuscation techniques. The work [10]
generated obfuscated test cases on programs written in vi-
sual basic language to test the resilience of commercial anti-
virus software to metamorphic malware.
4.1 Randomized Executables
To test resilience to obfuscation, a set of programs was
randomized using the ASLP tool [16]. This tool uses bi-
nary rewriting to rearrange the static code and data seg-
ments of an executable ﬁle in a random way. It performs
ﬁne-grained permutation of the procedure bodies in a code
segment, and of data structures in the data segment. This
randomization technique can invalidate the use of static sig-
natures for recognition of malicious code. This experiment
was performed on a machine running Fedora Core 1, with a
Pentium 4 CPU of 2.26GHz, and 512M of RAM.
ASLP was applied to programs taken from two well-
known benchmark suites: the SPEC CPU2000 programs [1]
and two web server programs (the Apache web server httpd
[2] and and the GazTek web server ghttpd [4]). The similar-
ity between the randomized and original versions of each
program was then computed using the proposed method.
The results are shown in ﬁgure 3.
Of these 12 test cases, 10 have similarity scores above
95%. This demonstrates that program changes due to ran-
domization do not affect the ability of the proposed method
to recognize the similarity in function between normal and
randomized versions. The proposed method of analysis, us-
ing system calls as a point of reference, is robust to such
changes in program structure.
416416
Therefore, similarity scores for the two programs’ ran-
domized and normal versions are still high enough to con-
clude that they are the same program.
4.2 Variant Detection Evaluation
Virus and malware writers have manually created many
variations of common exploits, in an attempt to evade virus-
checking tools. Hundreds of such examples can be found at
popular hacking web sites. Such examples make use of a
wide range of obfuscation techniques.
One such website is VX Heavens [7], which identiﬁes
programs that are variants of the same malware. More than
200 pairs of malware mutants were downloaded from this
website. These program instances were selected from mul-
tiple malware categories. Among these instances, 36.6%
were worms, 18.3% were viruses, 20.8% were backdoor
programs, and 16.3% were trojan programs. The remain-
der of the programs included ﬂooders and exploits. The
tested malware programs had sizes ranging from 8K to 1M
bytes. These malware programs and their mutants employ
multiple commonly used obfuscation techniques. For in-
stance, the simplest obfuscation technique used is register
renaming. A more complicated obfuscation technique is
using functionally equivalent instruction substitution. The
code addresses of the mutants can be very different. The
relative offset in corresponding function call instructions
are frequently adjusted. Sometimes, even within a single
program, the same library functions may be imported mul-
tiple times at different addresses, although at runtime, these
may be reloaded to the same address. Recognizing that two
call instructions refer to the same target address requires
data ﬂow analysis techniques. A much more common case
is that new functions or behaviors are added or revised in
mutants. There is another obfuscation that is not consid-
ered here but is often encountered when downloading the
malware programs. A none trivial number of malware pro-
grams use encryption. The limitation of the proposed ap-
proach will be discussed in the next section.
In the ﬁrst test, similarity scores were computed for pairs
of executables that represented the same attack. A his-
togram of the resulting similarity scores is shown in Fig-
ure 4. The great majority of pairs are easily recognized as
variants of the same function. For example, over 90% of the
pairs have a similarity score of .7 or greater. These mutants
represent the state of the art in mutation and obfuscation of
malware, and thus are a worthwhile test case for any pro-
gram attempting to recognize metamorphic variants in an
automated way. The results indicate that comparison with
a previous version of malware will, with high probability,
identify a new version of the malware.
It is also important to measure the similarities computed
between programs which are not variants of the same mal-
ware. In the second test, the malware programs were ran-
domly paired with each other, excluding all instances that
were identiﬁed on VX Heavens as being variants of the
same malware. Similarities for the resulting 200 pairs were
then computed using the proposed method. Figure 5 shows
the results. The computed similarities are very low, with
over 90% having a similarity score of .1 or less. Approxi-
mately 1% have a similarity score of .7 or greater. It may
be that malware even in different families are derived from
a common code base, explaining these results. Further in-
vestigation is required.
For these programs, a similarity score of .7 would be
an optimal threshold for concluding whether two programs,
one of which is known to be malicious, are functionally
equivalent. While this threshold does not perfectly distin-
guish malware variants from non-variants, keep in mind that
this is a tough test case: identifying hand-crafted metamor-
phic malware, and distinguishing it from other malware,
rather than distinguishing it from non-malicious code.
4.3 Version Diﬀerence Evaluation
tested how well
The third experiment
the proposed
method recognized variations between different versions or
releases of a program. Such versions are not intentionally
obfuscated, but represent another case of software that is
derived from a previous version of a program. They are
therefore a useful test of the proposed method.
Releases 2.10 through 2.17 of the GNU project “binu-
tils” binary tools [5] were used for this purpose. These tools
are used for compiling, linking, and debugging programs.
They make use of several common libraries for low level,
shared functions. Different releases will represent varying
degrees of modiﬁcation to the original program code.
Figure 6 shows the result of computing the similarity be-
tween consecutive releases of each program, using the pro-
posed method. For the great majority of cases, the com-
puted simularity was greater than .7. The cases where this
was not true are instructive to examine (see Figure 7. Be-
tween release 2.10 and 2.11, code sizes of the utilities in-
creased by approximately 50%, indicating a major revision,
and the computed similarity scores were correspondingly
lower (around .5). Figures 8 and 9 compare both release
2.10 and release 2.11 to release 2.17. It is clear that release
2.11 is much closer (in size and similarity) to 2.17 than re-
lease 2.10 is to 2.17. Also, between release 2.13 and 2.14,
the size of c++filt grew 10-fold, indicating essentially a
replacement by a new program; the computed similarity in
this case was close to 0.
These results indicate that the proposed method is effec-
tive at computing the degree of similarity between programs
in a way that is meaningful, and that is not sensitive to mod-
iﬁcations that preserve a program’s function.
417417
Matching of Malware Mutants
y
c
n
e
u
q
e
r
F
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
y
c
n
e
u
q
e
r
F
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Matching of Malware of different kinds
0-
0.1- 0.15- 0.2- 0.25- 0.3- 0.35- 0.4- 0.45- 0.5- 0.55- 0.6- 0.65- 0.7- 0.75- 0.8- 0.85- 0.9- 0.95-
0-
0.1- 0.15- 0.2- 0.25- 0.3- 0.35- 0.4- 0.45- 0.5- 0.55- 0.6- 0.65- 0.7- 0.75- 0.8- 0.85- 0.9- 0.95-
Matching Score Range
Matching Score Range
Figure 4. Malware Variants Pattern Matching. Each x-axis
value stands for a range of matching scores
Figure 5. Malware Variants Pattern Matching. Each x-axis
value stands for a range of matching scores
5 Discussion
5.2 Limitations
5.1 Comparison with Previous Work
This section compares with three previous methods [9,
12, 8] which statically analyze program semantics to detect
metamorphic malware.
As pointed out in [12], the control ﬂow graph compar-
ison method of [9] can only handle very simple program
obfuscations. For example, the detection algorithm only
allows noop instructions to appear between matching in-
structions. By comparison, the method proposed in this pa-
per can handle a much wider range of obfuscations than this.
It can also detect program mutants that have similar but not
identical behaviors.
The method of [12] uses semantic templates to detect
malware that has certain common behaviors. A template
is manually generated by studying the common behavior
of a set of collected malware instances; how to generate a
general semantic template is not addressed. Our method, by
contrast, proposes an automatic pattern generation method
that characterizes a program’s semantics. Furthermore, we
argue that the proposed method is harder to be bypassed.
The proposed method uses maximum weighted matching
to be tolerant to inaccurate program disassembly and static
analysis.
Chouchane and Lakhotia [8] use “engine signatures”
to assist in detecting metamorphic malware. That work,
however, can only deal with known instruction-substituting
metamorphic engines. There are many ways to create meta-
morphic engines, by no means limited to instruction substi-
tution. Moreover, their technique can be defeated by shrink-
ing substitution methods. Our proposed method does not
rely on speciﬁc engines.
It characterizes and compares a
program’s semantics more generally. It uses control ﬂow
and data ﬂow analysis, and is more robust against complex
metamorphism.
There are two main limitations that can cause the failure
of the proposed method. The major limitation is due to the
use of static analysis. Since static analysis does not execute
the program, run-time information is not available to derive
a more accurate pattern. A case in point is static disassem-
bly, which is not guaranteed to be 100% accurate [19]. Var-
ious techniques, such as indirect addressing, self-modifying
code, and dynamic code loading can lower the accuracy of
static disassembly. A number of malware instances down-
loaded from VX Heavens could not be disassembled prop-
erly, even though they were executable. One such technique
used by such malware is manipulation of the section table
in the program header. Examples include manipulating the
program entry pointer to start in a non-code section, and
changing the size of the code section to a false value. Such
techniques can cause disassembly to conclude incorrectly
that the ﬁle is not executable. Polymorphic malware is a
special case of self-modifying code for obfuscation. The
solution to this problem would be to combine dynamic anal-
ysis with static analysis to improve the disassembly accu-
racy. Many techniques to improve disassembly accuracy
have also been proposed [13], but are not currently imple-
mented in our prototype.
The second major limitation results from code evolution.
Similarity of a mutant to an original code version largely
depend on how the new instance evolves. If the majority
of functionalities (i.e.
the malware payload) of a mutant
is replaced, only a small part will be matched, resulting
in a low similarity score. In addition, the malware writer
can purposely insert random “junk” functionalities in terms
of actual system calls made to lower the matching score.
Theocratically, if the number of “junk” functionalities goes
unbounded or inﬁnite, the proposed method will likely fail.
To address this issue, it may be necessary for the program
pattern to be focused on speciﬁc functions of the malware,
rather than on the entire program’s function. For instance,
a whitelist of system calls or library calls can be built to ﬁl-
418418
Version Difference
c++filt
strings
size
add2line
nm
ranlib
ar
2.10,2.11
2.11,2.12
2.12,2.13
2.13,2.14
2.14,2.15
2.15,2.16
2.16,2.17
version numbers in comparison
Version Difference
ld
readelf
objdump
as
strip
objcopy
gprof
e
r
o
c
s
g
n
h
c
t
a
m