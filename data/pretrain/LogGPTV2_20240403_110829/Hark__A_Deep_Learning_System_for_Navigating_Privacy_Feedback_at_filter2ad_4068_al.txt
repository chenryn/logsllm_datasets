across the emotions dimension. For instance, a developer can
see from the review associated with the Disgust emotion that
users are uninstalling the app due to concerns around tracking.
Alternatively, developers could learn about users’ Desire for
the app to not track their email or private information.
Hark also enables developers to analyze the evolution of
issues over time. Figure 13 shows the most common ﬁne-
grained issues for the top 10 app categories in the last
5 years. Issues coming from the same theme are colored
identically. Interestingly, the issue of excessive permissions
was dominant across various app categories during 2016-2018.
Recently, the dominant issues pivoted towards various types
of unneeded access (contacts, location, camera, etc.) as well
as data selling/stealing. Developers could analyze these trends
in order to correlate them with app or policy changes.
X. Discussion and Limitations
Reviews Selection: In order to avoid apps with a handful
of privacy reviews, our dataset only includes apps with 10k
installs and 1k reviews. These apps constitute a signiﬁcant
proportion of the Play store, and comparing their issues vs.
popular apps is an opportunity for future research. Furthermore,
we also limited our corpus to English text only. Translating
text from other languages may lose privacy-related nuances
and introduce translation errors. We plan to better tackle this
in the future via multilingual models [56] that capture privacy
concerns in the original language.
Error Mitigation: At di↵erent stages of the Hark pipeline,
our models manifest a variety of error levels. This originates
from the inaccuracies of our models when dealing with the
high linguistic variability of our domain. For certain kinds of
errors, such as inaccurate issues or false positives produced by
the privacy classiﬁer, our pipeline can mitigate these as they
rarely become frequent issues. Other errors, however, such as
theme titles missing some of the issues or emotions interpreted
inaccurately would be noticed by the developers, which we
accept as a limitation.
Volume Estimation: Sometimes users express similar con-
cerns di↵erently, e.g., our ﬁne-grained issue generation can
separate “Spying App” and “Spying” into two distinct ﬁne-
grained issues. This would a↵ect the individual issue-level
volume estimates. This is potentially mitigated when estimating
the themes’ volume as these issues eventually make it to the
same theme. Solving this completely would require us to further
ﬁne-tune in-domain embeddings for issues similarity.
Further Studies: This paper focuses on describing and
evaluating the system and models behind Hark. A detailed
deep dive into the various aspects of privacy topics on the
Play store is out of scope of this work. In the future, we
aim to use Hark to conduct various studies: to understand
temporal trends in privacy issues, to compare issues based on
the emotions dimension, to analyze the type of feedback that
leads users to uninstall apps, or to explore particular themes of
interest (e.g., “Blackmailing Concerns”, “Financial Privacy”,
“Audio Surveillance”, “Parental Controls”, etc.). We also plan
to explore when our issue tags can be mapped to actionable
suggestions as compared to cases of user misunderstanding or
purely sentimental reviews.
XI. Conclusion
In this work, we have presented Hark, the ﬁrst end-to-
end, automated system for discovering and navigating privacy
feedback. At the core of Hark are 5 deep learning T5 models.
Our privacy classiﬁer, designed for topical diversity, achieves
0.92 AUC-ROC. There, we illustrated the power of NLI-based
construction of the training data as opposed to keyword or regex
based approaches. We also built a new model for dynamically
generating ﬁne-grained issues by casting the problem as an
abstractive labeling one, achieving 96% accuracy and 93%
coverage. Moreover, we trained a model that takes clusters of
issues and produces high-quality descriptive themes titles in
92% of the cases. Our review ranking solution and emotions
classiﬁer enable developers to better attend to the users’ voice,
with a minimal manual e↵ort. More broadly, we note that the
techniques developed in this work are generally applicable to
other domains, including security.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:48 UTC from IEEE Xplore.  Restrictions apply. 
2481
References
[1] Provide information for google play’s data safety sec-
tion, 2021. URL https://support.google.com/googleplay/
android-developer/answer/10787469.
[2] O. Alonso, C. Marshall, and M. Najork. Crowdsourcing
a subjective labeling task: a human-centered framework
to ensure reliable results. Microsoft Res., Redmond, WA,
USA, Tech. Rep. MSR-TR-2014–91, 2014.
[3] J. Amidei, P. Piwek, and A. Willis. Agreement is over-
rated: A plea for correlation to assess human evaluation
reliability.
In Proceedings of the 12th International
Conference on Natural Language Generation, 2019.
[4] Apple. App privacy details - app store, 2020. URL https:
//developer.apple.com/app-store/app-privacy-details/.
[5] A. R. Besmer, J. Watson, and M. S. Banks. Investigating
user perceptions of mobile app privacy: An analysis of
user-submitted app reviews.
International Journal of
Information Security and Privacy (IJISP), 2020.
[6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020.
[7] L. Cen, L. Si, N. Li, and H. Jin. User comment analysis for
android apps and cspi detection with comment expansion.
In PIR@ SIGIR, 2014.
[8] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco,
R. St. John, N. Constant, M. Guajardo-Cespedes, S. Yuan,
C. Tar, B. Strope, and R. Kurzweil. Universal sentence en-
coder for English. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing:
System Demonstrations, Brussels, Belgium, Nov. 2018.
Association for Computational Linguistics.
[9] N. Chen, J. Lin, S. C. H. Hoi, X. Xiao, and B. Zhang.
Ar-miner: Mining informative reviews for developers
from mobile app marketplace.
In Proceedings of the
36th International Conference on Software Engineering,
ICSE 2014, New York, NY, USA, 2014. Association for
Computing Machinery.
[10] A. Ciurumelea, A. Schaufelb¨uhl, S. Panichella, and
H. C. Gall. Analyzing reviews and code of mobile
apps for better release planning.
In 2017 IEEE 24th
International Conference on Software Analysis, Evolution
and Reengineering (SANER). IEEE, 2017.
[11] A. F. de Ara´ujo and R. M. Marcacini. Re-bert: automatic
extraction of software requirements from app reviews
using bert language model. In Proceedings of the 36th
Annual ACM Symposium on Applied Computing, 2021.
[12] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen,
G. Nemade, and S. Ravi. GoEmotions: A dataset of
ﬁne-grained emotions. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguis-
tics, Online, July 2020. Association for Computational
Linguistics.
[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.
BERT: Pre-training of deep bidirectional transformers
In Proceedings of
for language understanding.
the
2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers), Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics.
[14] A. Di Sorbo, S. Panichella, C. V. Alexandru, J. Shimagaki,
C. A. Visaggio, G. Canfora, and H. C. Gall. What would
users change in my app? summarizing app reviews for
recommending software changes. In Proceedings of the
2016 24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering, 2016.
[15] W. B. Dolan and C. Brockett. Automatically constructing
a corpus of sentential paraphrases.
In Proceedings
of the Third International Workshop on Paraphrasing
(IWP2005), 2005.
[16] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang,
J. Gao, M. Zhou, and H.-W. Hon. Uniﬁed language
model pre-training for natural language understanding and
generation. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances
in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019.
[17] P. Ekman. An argument for basic emotions. Cognition &
emotion, 1992.
[18] B. Fu, J. Lin, L. Li, C. Faloutsos, J. Hong, and N. Sadeh.
Why people hate your app: Making sense of user feedback
in a mobile app store.
In Proceedings of the 19th
ACM SIGKDD international conference on Knowledge
discovery and data mining, 2013.
[19] T. R. Goodwin, M. E. Savery, and D. Demner-Fushman.
Flight of the pegasus? comparing transformers on few-shot
and zero-shot multi-document abstractive summarization.
In Proceedings of COLING. International Conference
on Computational Linguistics, volume 2020. NIH Public
Access, 2020.
[20] J. A. Hartigan. Clustering algorithms. John Wiley &
Sons, Inc., 1975.
[21] P. R. Henao, J. Fischbach, D. Spies, J. Frattini, and
A. Vogelsang. Transfer learning for mining feature
requests and bug reports from tweets and app store
reviews. In 2021 IEEE 29th International Requirements
Engineering Conference Workshops (REW). IEEE, 2021.
[22] T. Johann, C. Stanik, W. Maalej, et al. Safe: A simple
approach for feature extraction from app descriptions
and app reviews.
In 2017 IEEE 25th International
Requirements Engineering Conference (RE). IEEE, 2017.
[23] M. Koupaee and W. Y. Wang. Wikihow: A large scale
text summarization dataset, 2018.
[24] K. Krippendor↵. Content analysis: An introduction to its
methodology. Sage publications, 2018.
[25] T. Kudo and J. Richardson. SentencePiece: A simple and
language independent subword tokenizer and detokenizer
for neural text processing. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language
Processing: System Demonstrations, Brussels, Belgium,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:48 UTC from IEEE Xplore.  Restrictions apply. 
2482
Nov. 2018. Association for Computational Linguistics.
[26] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-
hamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. BART:
Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension. In
Proceedings of the 58th Annual Meeting of the Associ-
ation for Computational Linguistics, Online, July 2020.
Association for Computational Linguistics.
[27] C.-Y. Lin. ROUGE: A package for automatic evaluation
of summaries.
In Text Summarization Branches Out,
Barcelona, Spain, July 2004. Association for Computa-
tional Linguistics.
[28] F. Lind, M. Gruber, and H. G. Boomgaarden. Content
analysis by the crowd: Assessing the usability of crowd-
sourcing for coding latent constructs. Communication
methods and measures, 2017.
[29] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,
O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov.
Roberta: A robustly optimized bert pretraining approach.
arXiv preprint arXiv:1907.11692, 2019.
[30] B. MacCartney and C. D. Manning. An extended model
of natural logic. In Proceedings of the eight international
conference on computational semantics, 2009.
[31] Q. McNemar. Note on the sampling error of the
di↵erence between correlated proportions or percentages.
Psychometrika, 1947.
[32] D. Mukherjee, A. Ahmadi, M. V. Pour, and J. Reardon. An
empirical study on user reviews targeting mobile apps’
security & privacy. arXiv preprint arXiv:2010.06371,
2020.
[33] P. Nema, P. Anthonysamy, N. Taft, and S. Peddinti. Ana-
lyzing user perspectives on mobile app privacy at scale. In
International Conference on Software Engineering (ICSE),
2022.
[34] D. C. Nguyen, E. Derr, M. Backes, and S. Bugiel. Short
text, large e↵ect: Measuring the impact of user reviews on
android app security & privacy. In 2019 IEEE Symposium
on Security and Privacy (SP). IEEE, 2019.
[35] R. Nogueira, Z. Jiang, R. Pradeep, and J. Lin. Document
ranking with a pretrained sequence-to-sequence model.
In Findings of the Association for Computational Linguis-
tics: EMNLP 2020, Online, Nov. 2020. Association for
Computational Linguistics.
[36] F. Palomba, P. Salza, A. Ciurumelea, S. Panichella,
H. Gall, F. Ferrucci, and A. De Lucia. Recommending
and localizing change requests for mobile apps based
on user reviews. In 2017 IEEE/ACM 39th International
Conference on Software Engineering (ICSE). IEEE, 2017.
[37] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th annual meeting of the Association
for Computational Linguistics, 2002.
[38] S. T. Peddinti, I. Bilogrevic, N. Taft, M. Pelikan, U. Er-
lingsson, P. Anthonysamy, and G. Hogben. Reducing
permission requests in mobile apps. In Proceedings of the
Internet Measurement Conference, IMC ’19, New York,
NY, USA, 2019. Association for Computing Machinery.
[39] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,
I. Sutskever, et al. Language models are unsupervised
multitask learners. OpenAI blog, 2019.
[40] C. Ra↵el, N. Shazeer, A. Roberts, K. Lee, S. Narang,
M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research, 2020.
URL http://jmlr.org/papers/v21/20-074.html.
[41] P. Rathi. Google play store reviews. https://www.kaggle.
com/prakharrathi25/google-play-store-reviews, 2020.
[42] A. Shimorina.
Human vs automatic metrics: on
the importance of correlation design. arXiv preprint
arXiv:1805.11474, 2018.
[43] D. J. Solove. A taxonomy of privacy. U. Pa. L. Rev.,
2005.
[44] C. Sun, X. Qiu, Y. Xu, and X. Huang. How to ﬁne-tune
bert for text classiﬁcation? In China National Conference
on Chinese Computational Linguistics. Springer, 2019.
Identifying security
issues for mobile applications based on user review
summarization.
Information and Software Technology,
2020.
[45] C. Tao, H. Guo, and Z. Huang.
[46] E. Tseng, R. Bellini, N. McDonald, M. Danos, R. Green-
stadt, D. McCoy, N. Dell, and T. Ristenpart. The tools and
tactics used in intimate partner surveillance: An analysis
of online inﬁdelity forums.
In 29th USENIX Security
Symposium (USENIX Security 20), 2020.
[47] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and
S. Bowman. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. In
Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP,
Brussels, Belgium, Nov. 2018. Association for Computa-
tional Linguistics.
[48] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh,
J. Michael, F. Hill, O. Levy, and S. Bowman. Super-
glue: A stickier benchmark for general-purpose language
understanding systems. Advances in Neural Information
Processing Systems, 2019. ISSN 1049-5258.
[49] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion
Parameter Autoregressive Language Model. https://github.
com/kingoﬂolz/mesh-transformer-jax, May 2021.
[50] Y. Wang and A. Kobsa. Privacy-enhancing technologies.
In Handbook of research on social and organizational
liabilities in information security. IGI Global, 2009.
[51] G. Widmer and M. Kubat. Learning in the presence of
concept drift and hidden contexts. Machine learning,
1996.
[52] D. Wilkinson, M. Namara, K. Badillo-Urquiola, P. J.
Wisniewski, B. P. Knijnenburg, X. Page, E. Toch, and
J. Romano-Bergstrom. Moving beyond a ”one-size ﬁts
all”: Exploring individual di↵erences in privacy.
In
Extended Abstracts of the 2018 CHI Conference on
Human Factors in Computing Systems, CHI EA ’18,
New York, NY, USA, 2018. Association for Computing
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:35:48 UTC from IEEE Xplore.  Restrictions apply. 
2483
TABLE IV: Hark Emotions’ Classifier Metrics
emotion
neutral
admiration
approval
gratitude
annoyance
amusement
curiosity
love
disapproval
optimism
anger
joy
confusion
sadness
disappointment
realization
caring
surprise
excitement
disgust
desire
fear
remorse
embarrassment
nervousness
relief
pride
grief
micro avg
macro avg
weighted avg
samples avg
precision
0.68
0.64
0.51
0.92
0.43
0.75
0.60
0.76
0.51
0.67
0.50
0.64
0.47
0.62
0.51
0.53
0.47
0.59
0.54
0.52
0.66
0.63
0.54
0.55
0.53
0.75
0.80
0.50
0.64
0.60
0.63
0.65
recall
0.67
0.79
0.32
0.90
0.23
0.90
0.47
0.87
0.41
0.52
0.56
0.61
0.52
0.54
0.25
0.17
0.45
0.52
0.40
0.45
0.47
0.77
0.86
0.46
0.43
0.27
0.25
0.50
0.59
0.52
0.59
0.62
f1-score
0.68
0.71
0.39
0.91
0.30
0.82
0.53
0.81
0.45
0.59
0.53
0.63
0.49
0.58
0.33
0.26