### Across the Emotions Dimension

For instance, a developer can discern from reviews associated with the Disgust emotion that users are uninstalling the app due to concerns about tracking. Alternatively, developers could gain insights into users' Desire for the app not to track their email or private information.

### Analyzing the Evolution of Issues Over Time

Hark also enables developers to analyze the evolution of issues over time. Figure 13 illustrates the most common fine-grained issues for the top 10 app categories over the last five years. Issues stemming from the same theme are color-coded identically. Notably, the issue of excessive permissions was prevalent across various app categories during 2016-2018. Recently, the dominant issues have shifted towards various types of unneeded access (e.g., contacts, location, camera) and data selling/stealing. Developers can analyze these trends to correlate them with app or policy changes.

### Discussion and Limitations

#### Reviews Selection
To avoid apps with only a few privacy reviews, our dataset includes only apps with at least 10,000 installs and 1,000 reviews. These apps represent a significant proportion of the Google Play Store, and comparing their issues with those of popular apps is an opportunity for future research. Additionally, we limited our corpus to English text only. Translating text from other languages may lose privacy-related nuances and introduce translation errors. We plan to address this in the future using multilingual models [56] that capture privacy concerns in the original language.

#### Error Mitigation
At different stages of the Hark pipeline, our models exhibit varying levels of error. This stems from the inaccuracies of our models when dealing with the high linguistic variability in our domain. For certain types of errors, such as inaccurate issues or false positives produced by the privacy classifier, our pipeline can mitigate these as they rarely become frequent issues. However, other errors, such as theme titles missing some issues or emotions being interpreted inaccurately, would be noticed by developers. We acknowledge this as a limitation.

#### Volume Estimation
Sometimes users express similar concerns differently, e.g., our fine-grained issue generation might separate "Spying App" and "Spying" into two distinct issues. This affects the individual issue-level volume estimates. This issue is potentially mitigated when estimating the themes' volume, as these issues eventually fall under the same theme. To fully resolve this, we would need to further fine-tune in-domain embeddings for issue similarity.

#### Further Studies
This paper focuses on describing and evaluating the system and models behind Hark. A detailed analysis of various aspects of privacy topics on the Play Store is beyond the scope of this work. In the future, we aim to use Hark to conduct various studies: to understand temporal trends in privacy issues, to compare issues based on the emotions dimension, to analyze the type of feedback that leads users to uninstall apps, or to explore particular themes of interest (e.g., "Blackmailing Concerns," "Financial Privacy," "Audio Surveillance," "Parental Controls," etc.). We also plan to explore when our issue tags can be mapped to actionable suggestions compared to cases of user misunderstanding or purely sentimental reviews.

### Conclusion

In this work, we have presented Hark, the first end-to-end, automated system for discovering and navigating privacy feedback. At the core of Hark are five deep learning T5 models. Our privacy classifier, designed for topical diversity, achieves an AUC-ROC of 0.92. We illustrated the power of NLI-based construction of training data compared to keyword or regex-based approaches. We also built a new model for dynamically generating fine-grained issues by casting the problem as an abstractive labeling task, achieving 96% accuracy and 93% coverage. Moreover, we trained a model that takes clusters of issues and produces high-quality descriptive theme titles in 92% of cases. Our review ranking solution and emotions classifier enable developers to better attend to users' voices with minimal manual effort. More broadly, the techniques developed in this work are generally applicable to other domains, including security.

### References
[References listed as provided, with no changes.]

### Table IV: Hark Emotionsâ€™ Classifier Metrics

| Emotion          | Precision | Recall | F1-Score |
|------------------|-----------|--------|----------|
| Neutral          | 0.68      | 0.67   | 0.68     |
| Admiration       | 0.64      | 0.79   | 0.71     |
| Approval         | 0.51      | 0.32   | 0.39     |
| Gratitude        | 0.92      | 0.90   | 0.91     |
| Annoyance        | 0.43      | 0.23   | 0.30     |
| Amusement        | 0.75      | 0.90   | 0.82     |
| Curiosity        | 0.60      | 0.47   | 0.53     |
| Love             | 0.76      | 0.87   | 0.81     |
| Disapproval      | 0.51      | 0.41   | 0.45     |
| Optimism         | 0.67      | 0.52   | 0.59     |
| Anger            | 0.50      | 0.56   | 0.53     |
| Joy              | 0.64      | 0.61   | 0.63     |
| Confusion        | 0.47      | 0.52   | 0.49     |
| Sadness          | 0.62      | 0.54   | 0.58     |
| Disappointment   | 0.51      | 0.25   | 0.33     |
| Realization      | 0.53      | 0.17   | 0.26     |
| Caring           | 0.47      | 0.45   | 0.46     |
| Surprise         | 0.59      | 0.52   | 0.55     |
| Excitement       | 0.54      | 0.40   | 0.47     |
| Disgust          | 0.52      | 0.45   | 0.48     |
| Desire           | 0.66      | 0.47   | 0.55     |
| Fear             | 0.63      | 0.77   | 0.69     |
| Remorse          | 0.54      | 0.86   | 0.67     |
| Embarrassment    | 0.53      | 0.46   | 0.49     |
| Nervousness      | 0.75      | 0.43   | 0.55     |
| Relief           | 0.80      | 0.27   | 0.41     |
| Pride            | 0.50      | 0.25   | 0.33     |
| Grief            | 0.64      | 0.50   | 0.56     |
| Micro Avg        | 0.60      | 0.52   | 0.55     |
| Macro Avg        | 0.60      | 0.52   | 0.55     |
| Weighted Avg     | 0.60      | 0.52   | 0.55     |
| Samples Avg      | 0.60      | 0.52   | 0.55     |

This table provides the precision, recall, and F1-score for each emotion category, along with micro, macro, weighted, and samples averages.