−11
0
Total
attacks
479
32
tests
4765
269
−452 −4527
340
35
−91
−8
−457 −4518
447
4496
−32
−269
−484 −4796
71
3
−40
−360
−489 −4787
931
9292
4527
452
4796
484
4867
487
4436
444
9
−5
4425
444
−35
−340
−3
−71
−487 −4867
−43
−431
−492 −4858
4856
487
91
8
360
40
−444 −4436
431
−449 −4427
9283
936
4518
457
4787
489
5
−9
4858
492
4427
449
43
Table 3: Our experiment results showing the differences between the representation schemes. The top rows of each row group
separated by a rule, show the raw performance of a baseline scheme, followed by italic rows showing a direct comparison delta.
Greyed-out values are not backed by statistical signiﬁcance. The columns fail-rate (undetected attacks) and false-pos (same string
rated as an attack) display percentage values.
slightly skewed normal distribution, and a large amount
of collected data. The common language effect size is
shown by mean and median comparisons [26].
The attack detection rate is tested with a pairwise
Holm-Bonferroni-corrected Barnard’s exact test (Exakt
package in R) achieving one of highest statistical power
for 2x2 contingency tables [2].
Survey ratings are, again, tested by using the MWW
signiﬁcance test (two-tailed test). As has been shown in
previous research [9], it is most suitable for 5-point Lik-
ert scales, especially if not multimodal distributed as in
our survey results. In case two ﬁngerprint representation
schemes are statistically tested against each other, only
participants encountering both schemes were considered.
5 Results
In this section, we present our results: our online study
with 1047 participants has been conducted in August and
September 2015. The study for testing the chunk size has
been conducted in February 2016 with 400 participants.
Starting with our online experiment evaluation showing
the raw performance of users, we then present user per-
ception results from the follow-up survey. Finally, we
discuss the demographics of our participants.
5.1 Online Experiment
Participants who have not ﬁnished all comparisons or
failed the attention tests were excluded from our eval-
202  25th USENIX Security Symposium 
USENIX Association
10
Scheme
Hexadecimal (4)
hex (4) – hex (0)
hex (4) – hex (2)
hex (4) – hex (3)
hex (4) – hex (5)
hex (4) – hex (6)
hex (4) – hex (7)
hex (4) – hex (8)
p-val
Speed
mean [s] med [s]
10.4
−2.6 <0.001
−0.9 <0.001
0.362
0.1
−1.2 <0.001
−1.8 <0.001
−1.8 <0.001
−3.2 <0.001
12.3
−2.4
−0.3
−0.3
−1.4
−1.9
−1.7
−2.8
fail-rate
6.78
0.33
1.37
−0.64
1.01
2.43
3.35
1.35
Accuracy
p-val
false-pos
1.000
1.000
1.000
1.000
1.000
1.000
1.000
0.38
−0.28
0.00
0.09
−0.40
0.09
0.19
−0.12
Total
attacks
236
−17
3
8
5
8
−1
−10
fails
16
−2
−3
2
−2
−5
−8
−4
tests
2360
−170
30
80
50
80
−10
−100
Table 4: Comparison of the chunking experiment results showing the differences between the representation schemes. The top row
shows the raw performance of the hexadecimal scheme with a four-character chunking, followed by italic rows showing a direct
comparison delta. Greyed-out values are not backed by statistical signiﬁcance. The columns fail-rate (undetected attacks) and
false-pos (same string rated as an attack) display percentage values.
uation: all participant compared 46 security codes in a
randomized order, whereas 40 (10 of each scheme) were
considered in the evaluation. The four training samples
and the control questions are excluded. Few comparisons
done in less than 2 seconds and more than one minute
have been excluded. The reason for such can either be
multiple clicks during the page load, or external inter-
ruptions of the participants. None of the attack could be
successfully detected in under 4 seconds.
Our experiment results, summarized in Table 3, show
the raw performance of all schemes regarding their
speed, accuracy and false-positive rate. The top rows of
each row group, separated by a rule, show the raw perfor-
mance of a representation scheme as baseline (negative
values indicate lower values than the baseline). The fol-
lowing rows show a direct comparison delta between be-
tween two schemes. The speed column group consists of
the mean and median (in seconds), the standard deviation
and the according p-values for a direct comparison. The
fail-rate column shows the rate of the undetected attacks
with the according p-values for a direct comparison. The
total column group simply shows the total numbers of
tests, attacks and undetected attacks.
The results show that the average time spent on com-
parisons plays only a minor role among the schemes:
4.3s difference between the best and the worst scheme.
Note that the Peerio word-list scheme performed best
with 8.7s mean whereas the PGP word list performed
worst with 13s mean (p < 0.001).
However, there is a clear effect regarding the attack de-
tection rate (see Table 3). All alternative key-ﬁngerprint
representations performed better than the state-of-the-
art hexadecimal representation, where 10.1% of attacks
have not been detected by the users. Previous work
shows similar numbers for Base32 [19]. To our surprise,
the numeric approach performs better in both categories:
it features an attack detection rate of 93.57% (p < 0.01)
and an average speed of 10.6s (p < 0.001). Generated
sentences achieved the highest attack detection rate of
97.97% with a similar average speed as the hexadecimal
scheme. On the downside, this scheme has produced a
slightly higher false-positive rate. We found that the false
positives occurred mostly with longer sentences where
there has been a line break on the phone mock-up due
to portrait orientation. This is a realistic problem of this
system if used with portrait orientation and not a problem
with our mock-up in itself. Improvements on making the
sentences shorter could mitigate this situation.
Chunk-Size Experiment
Table 4 summarizes the results of our secondary chunk-
size experiment. As can be seen, no statistically signif-
icant results have been achieved for the attack detection
fail-rate (undetected attacks by end users). However, we
observed that the chunk sizes with 3 and 4 characters per-
formed best in speed, even though the effect sizes were
minor: only 3.3 seconds difference with similar standard
deviations between the best and worst chunk size setting.
Firstly, we notice that despite the same attack strength
as in our major experiment, participants were able to de-
tect more attacks. We suspect that the higher attack de-
tection rate is based on (1) a higher learning effect due
to the same scheme for all comparisons and (2) in con-
trast to our major study, participants had a slightly higher
drop-out rate and thus only more motivated participants
were considered. This is supported by the numbers in the
total tests column of Table 4: here, we can see that for
the zero-chunking and chunking with 8 characters less
tests have been performed. This is based on the fact that
although the chunk sizes have been assigned almost uni-
formly, participants assigned with harder chunk settings
often dropped out before even ﬁnishing their entire task.
More importantly, our results also support the claim
from our pre-study: The chunking parameter in hexadec-
imal strings plays only a minor role in the attack detec-
tion fail-rate.
USENIX Association  
25th USENIX Security Symposium  203
11
Alphanumeric
Numeric
Unrelated Words
Generated Sentences
5%
8%
8%
12%
Alphanumeric
Numeric
Unrelated Words
Generated Sentences
17%
9%
6%
5%
100
Trustworthiness
Usability
82%
76%
75%
67%
63%
76%
80%
85%
50
0
Percentage
50
100
Response
Strongly Disagree
Disagree
Neutral
Agree
Strongly Agree
Figure 5: Aggregated survey results for statement rating regarding the usability and trustworthiness.
5.2 Online Survey
To measure the usability and trustworthiness of all rep-
resentation schemes, we asked our participants whether
they agreed with the following statements:
S1 The comparisons were easy for me with this method
S2 I am conﬁdent that I can make comparisons using this
method without making mistakes
S3 I think making comparisons using this method would
help me keep my communications secure
S4 I was able to do the comparisons very quickly with
this method
S5 I found this method difﬁcult to use
S6 Overall, I liked this method
We mixed positive and negative statements, e. g., S1
and S5, to create a more robust measure. S6 is used to
calculate the overall ranking of the different representa-
tion schemes.
Figure 5 shows the aggregated results where the us-
ability statements are grouped to one usability feature
and the trustworthiness derived from the rating on the
statement S3. Negative statement ratings have been in-
verted for a better comparison. Figure 6 shows the rating
results for each speciﬁc statement in the survey. The or-
der of the tested schemes has been chosen randomly, but
was kept consistent across all statements. Same as in
our online experiment evaluation, the pairwise statistical
tests are Holm-Bonferroni corrected. In case of a direct
statistical test between two schemes, only users encoun-
tering both schemes have been considered. All in all, the
usability perception of the participants is almost consis-
tent with the performance results from the experiment.
To measure the perception of the task difﬁculty, we
asked the participants whether they agreed with the state-
ments S1, S2 and S3 respectively. As illustrated in Fig-
ure 6 in the Appendix A, the effect size between the dif-
ferent approaches is low. However, the participants were
more likely to agree that language-based representation
schemes are easier to use. For instance, we see that in
comparison to the alphanumeric schemes (average rat-
ing of 3.4), word list (average rating of 3.9, p < 0.001)
and generated sentence schemes (average rating of 4.2,
p < 0.001 ) are rated to be easier by our participants (S1,
S5). While the experiment results of the sentence genera-
tors clearly outperformed all other approaches, they also
were rated better by the participants. Same applies for
the low-performing hexadecimal and Base32 schemes
which clearly received lower ratings. Consistently with
the surprising performance results in the experiment, the
numeric scheme is also considered to be easier by many
participants: average rating of 3.9 and p < 0.001.
The sentence generator scheme achieved the highest
user conﬁdence rating “making comparisons without any
mistakes” (S2, p < 0.001 for all pairwise comparisons).
The participants’ perception is consistent with the ex-
periment results where the word-list-based and sentence
generator schemes lead to higher attack detection rates.
204  25th USENIX Security Symposium 
USENIX Association
12
The ratings for S4 illustrate that more complex repre-
sentation schemes from the user’s point of view, such as
hexadecimal and Base32, are considered to be more se-
cure by participants, even though all approaches provide
the same level of security.
5.3 Demographics
A total of 1047 users participated in the online study
while only 1001 have been considered in the evaluation
due to our two control questions. Out of the evaluated
participants, 534 participants were male, 453 were fe-
male, 4 chose other while the rest opted to not give any
information. No signiﬁcant difference between genders
could be found, with a subtle trend of a higher accuracy
for women and higher speed among men. The median
age was 34 (34.4 average) years, while 34 participants
chose not to answer (no statistically signiﬁcant differ-
ences between ages).
A total of 39 people reported to have “medical con-
ditions that complicated the security code comparisons
(e. g., reading disorders, ADHD, visual impairments,
etc.)” with a slightly higher undetected attack rate (sta-
tistically insigniﬁcant due to small sample size and thus
low statistical power).