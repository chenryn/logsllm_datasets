480
GFS
705
2 weeks
2 weeks
Table 1. The three TLA+ speciﬁcations. For each sys-
tem, we show the TLA+ speciﬁcation size, and the time to
produce the ﬁrst working version by one person, without
prior TLA+ knowledge. The Chain speciﬁcation was the
ﬁrst to be written, and took longer due to lack of experience
with TLA+. The GFS speciﬁcation is signiﬁcantly longer,
as we specify writes and appends separately.
Figure 2. Design differences in TLA+. The ﬁgure shows
TLA+ snippets from the Niobe and GFS modules. The pur-
pose of this ﬁgure is not TLA+ instruction, but rather to
help the reader visualize how design differences (shown in
a box) stand out clearly in TLA+.
we choose to provide only an example here and make spec-
iﬁcations available online [8].
From reading the original papers, GFS and Niobe seem
very different systems, designed and optimized for quite
different client semantics and workloads. However, as we
were creating their speciﬁcations, it became clear to us
that the systems had in fact a lot of mechanisms in com-
mon. Fundamentally, they both rely on a single master
and a primary-secondary replication scheme. Consequently,
we abstracted this structure into a common TLA+ module,
which we extended in the Niobe and GFS speciﬁcations.
This factorization turned out to be a powerful effect:
the
common module has 291 TLA+ lines, the modules speciﬁc
to GFS-writes and Niobe are 189 lines and 287 lines, re-
spectively, and the initial, unsplit Niobe speciﬁcation was
about the same size as the factorized one. In other words,
the two systems’ speciﬁcations have over half their TLA+
lines in common.
After factorization, the core differences between the
two systems stood out clearly in TLA+. For example, our
speciﬁcations make clear the distinction between write ﬁ-
nalization in GFS and Niobe. Figure 2 illustrates this dis-
tinction in a side-by-side comparison of a part of the func-
tion specifying when writes are ﬁnalized. In GFS (left side),
the primary ﬁnalizes a write after the write request to each
of the replicas has either been acknowledged or has timed
out. In Niobe (right side), the primary ﬁnalizes a write only
after each replica has either acknowledged the write, or it
has timed out and has been successfully removed from the
group. This last condition represents the distinction and is
signaled by a box in the ﬁgure.
By abstracting out the key aspects that differentiate
real systems, speciﬁcations also help us understand the
Figure 3. SimpleStore and reﬁnement mapping for
each system. We ﬁrst construct and verify Chain’s Sim-
pleStore (Chain SS),
then relax Chain SS to construct
Niobe SS, and further relax Niobe SS to arrive at GFS SS.
trade-offs that each system bargains for. As one exam-
ple, from the above design distinction, we learn that while
GFS can achieve better write latency, Niobe never leaves
the replica set in an inconsistent state, even after a failed
write. As another example, by allowing the client to read
from any replica, GFS achieves better read performance for
workloads with simultaneous clients reading the same data.
4. Understanding and Comparing Consistency
Currently, designers of fault-tolerant ﬁle systems typ-
ically rely only on reasoning to understand their systems’
consistency. Reasoning about a full system can be compli-
cated, faulty, lengthy, and inefﬁcient (especially if the de-
sign is not yet ﬁnalized). In this section, we provide our ex-
perience with applying formal methods to understand and
compare consistency properties of fault-tolerant ﬁle sys-
tems.
Our technique combines TLA+ speciﬁcations, reﬁne-
ment mappings, and model checking. In a nutshell, we re-
duce the systems to simpliﬁed, client-centric models (Sim-
pleStores) and analyze and compare the consistency of
those models instead. A system’s SimpleStore captures all
client-visible behavior, but abstracts out many lower-level
details, hence making proofs of consistency properties easy.
We specify SimpleStores formally in TLA+, produce reﬁne-
ment mappings from each system to the appropriate Sim-
pleStore, and use model checking to validate the reduction.
Then, by proving consistency properties about a system’s
SimpleStore, we infer that the system has those properties.
To enable comparison, we start by building a Simple-
Store for the most strongly consistent system and then relax
it to match the behavior of weaker systems. Figure 3 shows
the order in which we reduce systems to their SimpleStores.
4.1. The Chain SimpleStore
Figure 4(a) shows the structure of the Chain Simple-
Store (Chain SS). It has two components: a reliable serial
database (SerialDB) and two unreliable incoming channels
(pending rdreq, pending wrreq). Clients push their read
and write requests into pending rdreq and pending wrreq,
respectively. SerialDB takes requests one by one from the
channels, handles them, and responds to the client immedi-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:17:16 UTC from IEEE Xplore.  Restrictions apply. 
NiobeGFS  Chain_SSNiobe_SSGFS_SSChainBlueNioberefinement mappingsrefinement mappingrelaxrelaxChain(1)(2)(3)GFSInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE98DSN 2008: Geambasu et al.(a) Chain SimpleStore
Figure 4. Structure of Chain (a) and Niobe (b) SimpleStores. Sections 4.1 and 4.2 provide detailed descriptions.
(b) Niobe SimpleStore
Chain SS
variable
pending rdreq
pending wrreq
SerialDB disk
Mapping from Chain state
to Chain SS variable
Read requests at the tail
Union of all requests in the input channel
of each live replica
Value of last write committed by tail
Table 2. Reﬁnement mapping from Chain to Chain SS
(intuition). We show how to compute each variable in
Chain SS from the state in Chain.
ately. All SerialDB actions are atomic and persistent. Chan-
nels are unreliable: they can reorder or drop requests. If a
channel drops a request, the request is never handled by Se-
rialDB and hence is never responded to. To handle a read
request (the read() action), SerialDB responds to the client
with the current value of its disk. To handle a write request
(the commit() action), SerialDB saves the write’s value to
its disk and sends a response to the client.
We produced a reﬁnement mapping from Chain to
Chain SS (Table 2). We model-checked the reﬁnement
mapping using TLC, for a limited instance of the Chain sys-
tem: three replicas, one object, and two data values. The
check took two days and ﬁnished successfully, providing
high conﬁdence that indeed Chain implements Chain SS.
Using Chain SS, we can infer client-centric consis-
tency properties of Chain, namely linearizability. Thanks to
its simplicity, Chain SS can be proved linearizable in about
half a page [8]. Hence, Chain must also be linearizable.
Using formal methods, we were thus able to verify
that Chain is linearizable for the common three-replica case.
This is a powerful effect: using comfortable (and error-free)
model checking of a simple model, we fortiﬁed the tradi-
tional error-prone reasoning about a full asynchronous pro-
tocol. Our method thus increases our trust in the system’s
behavior in the face of failures.
4.2. The Niobe SimpleStore
Intuitively, Niobe seemed to map well onto Chain SS,
so we attempted to model check a mapping between Niobe
and Chain SS. However, TLC revealed an example of
Niobe behavior which is not mappable onto any Chain SS
behavior. The behavior, which requires 10 message ex-
changes, captures the case when an old primary’s write
succeeds and is responded to the client after a write of a
newer primary. This behavior leads to a still linearizable
history, however, it cannot be captured by Chain SS. What
we need is support for out-of-commit-order response deliv-
ery to clients.
First,
Hence, we extended Chain SS in Niobe SS to add
support for this behavior (Figure 4(b)). Two modiﬁca-
tions are needed.
in Niobe SS, the input chan-
nel pending wrreq is ordered and writes are committed in
channel order. Second, to tolerate out-of-commit-order re-
sponse delivery, SerialDB places responses to writes into
a pending response channel (pending wrresp). This chan-
nel can later drop a response or deliver it. Now, when Se-
rialDB commits a write w, it moves all preceding writes
in pending wrreq to the pending wrresp channel, commits
w’s value to disk, and ACKs w to the client. Later on,
some of the moved writes might succeed (respond()), others
might fail (drop()).
As with Chain, we model-checked a reﬁnement map-
ping from Niobe to Niobe SS for the same 3-replica system
instance. The check ﬁnished successfully in 3 days, pro-
viding high conﬁdence that Niobe implements Niobe SS.
Niobe SS remains linearizable. The proof is slightly more
involved than for Chain SS, but certainly manageable [8]
and signiﬁcantly easier than for a full system.
4.3. The GFS SimpleStore
Even if we assume master reliability, GFS cannot be
mapped onto either Chain or Niobe SimpleStores. We iden-
tiﬁed three counter-examples:
Ex. 1 Non-atomic writes. A GFS write can be split into
multiple writes that go to different sets of replicas, and
are thus serialized by different primaries.
Ex. 2 Stale reads.
In GFS, a client can read from a stale
replica, i.e., one that is no longer part of the group and
has missed some updates.
Ex. 3 Read uncommitted. Reads in GFS can go to any
replica, so a client can read the value of an in-progress
write. This can lead to non-sequentially consistent be-
haviors, like the one shown in Figure 5.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:17:16 UTC from IEEE Xplore.  Restrictions apply. 
pending_wrreqdrop(w7)ResponsesClient Requestscommit(w5)SerialDBwrites    reads   reads     writesread()pending_rdreqChain_SSr2r1r3w7w6w5pending_wrreqdrop(w7)ResponsesClient Requestscommit(w6)SerialDB3w drop(    )Responses writeswrites      readsreads     writespending_wrresp4w3w1w2wread()Niobe_SS w5w6w7respond(w2)r2r1r3  Atomic actionsInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE99DSN 2008: Geambasu et al.Figure 5. Counter-example for sequential consistency
for GFS. R1 is the primary. The partially ordered sequence
of messages (order numbers are shown) leads to a non-
sequentially consistent history:  (barred operations represent responses).
The above examples are also counter-examples to se-
quential consistency and linearizability. However, the result
that GFS is not linearizable is not surprising, nor does it en-
able comparison to Chain and Niobe’s consistency models.
What is more interesting is that by eliminating the ﬁrst two
counter-examples, we were able to map GFS onto a simple
extension of Niobe SS, with a well-understood consistency
model. Hence, we make two assumptions:
A1 Writes and reads never cross chunk boundaries, and
A2 Reads never go to stale replicas.