• 如果服务器响应说明有访问限制（HTTP状态码401或403），机器人就应该认为
对此站点的访问是完全受限的。
• 如果请求尝试的结果是临时故障（HTTP状态码503），机器人就应该推迟对此站
点的访问，直到可以获取该资源为止。
• 如果服务器响应说明是重定向（HTTP状态码3XX），机器人就应该跟着重定向，
直到找到资源为止。
9.4.3 robots.txt文件的格式
robots.txt文件采用了非常简单的，面向行的语法。robots.txt文件中有三种类型的
行：空行、注释行和规则行。规则行看起来就像HTTP首部（:）
一样，用于模式匹配。比如：
# this robots.txt file allows Slurp & Webcrawler to crawl
# the public parts of our site, but no other robots...
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private
User-Agent: *
Disallow:
robots.txt 文件中的行可以从逻辑上划分成“记录”。每条记录都为一组特定的机
器人描述了一组排斥规则。通过这种方式，可以为不同的机器人使用不同的排斥
规则。
每条记录中都包含了一组规则行，由一个空行或文件结束符终止。记录以一个
或多个User-Agent行开始，说明哪些机器人会受此记录的影响，后面跟着一些
Disallow和Allow行，用来说明这些机器人可以访问哪些URL。20
注20： 出于实际应用的原因，机器人软件应该很强壮，可以灵活地使用行结束符。应该支持CR、LF和
CRLF。
Web机器人 ｜ 243
前面的例子显示了一个robots.txt文件，这个文件允许机器人Slurp和Webcrawler
访问除了private子目录下那些文件之外所有的文件。这个文件还会阻止所有其他机
器人访问那个站点上的任何内容。
我们来看看User-Agent、Disallow和Allow行。
1. User-Agent行
每个机器人记录都以一个或多个下列形式的User-Agent行开始：
User-Agent: 
或
232 User-Agent: *
在机器人HTTP GET请求的User-Agent首部中发送（由机器人实现者选择的）机
器人名。
机器人处理robots.txt文件时，它所遵循的记录必须符合下列规则之一：
• 第一个是机器人名的大小写无关的子字符串；
• 第一个为“*”。
如果机器人无法找到与其名字相匹配的User-Agent行，而且也无法找到通配的
User-Agent:*行，就是没有记录与之匹配，访问不受限。
由于机器人名是与大小写无关的子字符串进行匹配，所以要小心不要匹配错了。比如，
User-Agent:bot就与名为Bot、Robot、Bottom-Feeder、Spambot和Dont-Bother-Me
的所有机器人相匹配。
2. Disallow和Allow行
Disallow和Allow行紧跟在机器人排斥记录的User-Agent行之后。用来说明显
式禁止或显式允许特定机器人使用哪些URL路径。
机器人必须将期望访问的 URL 按序与排斥记录中所有的 Disallow和 Allow规
则进行匹配。使用找到的第一个匹配项。如果没有找到匹配项，就说明允许使用这
个URL。21
注21： 总是应该允许访问robots.txt的URL，它一定不能出现在Allow/Disallow规则中。
244 ｜ 第9章
要使Allow/Disallow行与一个URL相匹配，规则路径就必须是URL路径大小写
相关的前缀。例如，Disallow: /tmp就和下面所有的URL相匹配：
http://www.joes-hardware.com/tmp
http://www.joes-hardware.com/tmp/
http://www.joes-hardware.com/tmp/pliers.html
http://www.joes-hardware.com/tmpspc/stuff.txt
3. Disallow/Allow前缀匹配
下面是Disallow/Allow前缀匹配的一些细节。
• Disallow和Allow规则要求大小写相关的前缀匹配。（与User-Agent行不同）
这里的星号没什么特殊的含义，但空字符串可以起到通配符的效果。
• 在进行比较之前，要将规则路径或URL路径中所有“被转义”的字符（%XX）
都反转为字节（除了正斜杠%2F之外，它必须严格匹配）。
• 如果规则路径为空字符串，就与所有内容都匹配。
表9-3列出了几个在规则路径和URL路径间进行匹配的例子。 233
表9-3 robots.txt路径匹配示例
规则路径 URL路径 匹配吗？ 注 释
/tmp /tmp √ 规则路径 == URL路径
/tmp /tmpfile.html √ 规则路径是URL路径的前缀
/tmp /tmp/a.html √ 规则路径是URL路径的前缀
/tmp/ /tmp × /tmp/不是/tmp的前缀
README.TXT √ 空的规则路径匹配于所有的路径
/~fred/hi.html /%7Efred/hi.html √ 将%7E与~同等对待
/%7Efred/hi.html /~fred/hi.html √ 将%7E与~同等对待
/%7efred/hi.html /%7Efred/hi.html √ 转义符是大小写无关的
/~fred/hi.html ~fred%2Fhi.html × %2F是一个斜杠，但斜杠是种特殊
情况，必须完全匹配
前缀匹配通常都能很好地工作，但有几种情况下它的表达力却不够强。如果你希望
无论使用什么路径前缀，都不允许爬行一些特别的子目录，那robots.txt是无能为
力的。比如，你可能希望禁止在用于RCS版本控制的子目录中爬行。除了将到达
各RCS子目录的每条路径都分别枚举出来之外，1.0版的robots.txt方案无法提供此
功能。
Web机器人 ｜ 245
9.4.4 其他有关robots.txt的知识
解析robots.txt文件时还需遵循其他一些规则。
• 随着规范的发展，robots.txt文件中可能会包含除了User-Agent、Disallow和
Allow之外的其他字段。机器人应该将所有它不理解的字段都忽略掉。
• 为了实现后向兼容，不能在中间断行。
• 注释可以出现在文件的任何地方；注释包括可选的空格，以及后面的注释符（#）、
注释符后面的注释，直到行结束符为止。
• 0.0版的拒绝机器人访问标准并不支持Allow行。有些机器人只实现了0.0版的
规范，因此会忽略Allow行。在这种情况下，机器人的行为会比较保守，有些
允许访问的URL它也不去获取。
9.4.5 缓存和robots.txt的过期
如果一个机器人在每次访问文件之前都要重新获取robots.txt文件，Web服务器上
的负载就会加倍，机器人的效率也会降低。机器人使用的替代方法是，它会周期性
地获取robots.txt文件，并将得到的文件缓存起来。机器人会使用这个robots.txt文
234 件的缓存副本，直到其过期为止。原始服务器和机器人都会使用标准的HTTP缓存
控制机制来控制robots.txt文件的缓存。机器人应该留意HTTP响应中的Cache-
Control和Expires首部。22
现在很多产品级爬虫都不是HTTP/1.1的客户端；网管应该意识到这些爬虫不一定
能够理解那些为robots.txt资源提供的缓存指令。
如果没有提供Cache-Control指令，规范草案允许将其缓存7天。但实际上，这
个时间通常太长了。不了解robots.txt文件的Web服务器管理员通常会在响应机器
人的访问时创建一个新的文件，但如果将缺乏信息的robots.txt文件缓存一周，新创
建的robots.txt文件就没什么效果了，站点管理员会责怪机器人管理员没有遵守拒绝
机器人访问标准。23
9.4.6 拒绝机器人访问的Perl代码
有几个公共的Perl库可以用来与robots.txt文件进行交互。CPAN公共Perl文档中
的WWW::RobotsRules模块就是一个这样的例子。
注22： 更多有关缓存指令处理方面的内容请参见7.8节。
注23： 有几种大型的Web爬虫，如果它们在Web上勤奋爬行的话，每天都会重新获取robots.txt。
246 ｜ 第9章
将已解析的robots.txt文件保存在WWW::RobotRules对象中，这个对象提供了一些
方法，可以用于查看是否禁止对某指定URL进行访问。同一个WWW::RobotRules
可以用于解析多个robots.txt文件。
下面是WWW::RobotRules API的一些主要方法。
• 创建RobotRules对象
$rules = WWW::RobotRules->new($robot_name);
• 装载robots.txt文件
$rules->parse($url, $content, $fresh_until);
• 查看站点URL是否可获取
$can_fetch = $rules->allowed($url);
下面这个短小的Perl程序说明了WWW::RobotRules的用法：
require WWW::RobotRules;
# Create the RobotRules object, naming the robot "SuperRobot"
my $robotsrules = new WWW::RobotRules 'SuperRobot/1.0';
use LWP::Simple qw(get);
# Get and parse the robots.txt file for Joe's Hardware, accumulating
# the rules
$url = "http://www.joes-hardware.com/robots.txt";
my $robots_txt = get $url;
$robotsrules->parse($url, $robots_txt); 235
# Get and parse the robots.txt file for Mary's Antiques, accumulating
# the rules
$url = "http://www.mary's antiques.com/robots.txt";
my $robots_txt = get $url;
$robotsrules->parse($url, $robots_txt);
# Now RobotRules contains the set of robot exclusion rules for several
# different sites. It keeps them all separate. Now we can use RobotRules
# to test if a robot is allowed to access various URLs.
if ($robotsrules->allowed($some_target_url))
{
$c = get $url;
...
}
下面是www.marys-antiques.com的假想robots.txt文件：
#####################################################################
# This is the robots.txt file for Mary's Antiques web site
#####################################################################
Web机器人 ｜ 247
# Keep Suzy's robot out of all the dynamic URLs because it doesn't
# understand them, and out of all the private data, except for the
# small section Mary has reserved on the site for Suzy.
User-Agent: Suzy-Spider
Disallow: /dynamic
Allow: /private/suzy-stuff
Disallow: /private
# The Furniture-Finder robot was specially designed to understand
# Mary's antique store's furniture inventory program, so let it
# crawl that resource, but keep it out of all the other dynamic
# resources and out of all the private data.
User-Agent: Furniture-Finder
Allow: /dynamic/check-inventory
Disallow: /dynamic
Disallow: /private
# Keep everyone else out of the dynamic gateways and private data.
User-Agent: *
Disallow: /dynamic
Disallow: /private
这个robots.txt文件中包含了一条机器人SuzySpider的记录，一条机器人FurnitureFinder
的记录，以及一条用于所有其他机器人的默认记录。每条记录都对不同的机器人使
用了一组不同的访问策略。
• SuzySpider的排斥记录不允许机器人爬行以/dynamic开头的商店库存网关URL，
以及在为Suzy保留的区域之外的其他私有用户数据。
236 • FurnitureFinder机器人的记录允许机器人爬行家具库存网关URL。这个机器人可
能能够理解Mary的网关格式和规则。
• 其他机器人都不能访问所有的动态和私有Web页面，但它们可以爬行其余的
URL。
表9-4列出了几个机器人实例，这几个机器人具有不同的Mary古董网站访问权限。
表9-4 Mary古董网站的机器人访问权限
URL SuzySpider FurnitureFinder NosyBot
http://www.marys-antiques.com/ √ √ √
http://www.marys-antiques.com/index.html √ √ √
248 ｜ 第9章
（续）
URL SuzySpider FurnitureFinder NosyBot
http://www.marys-antiques.com/private/payroll.xls × × ×
http://www.marys-antiques.com/private/suzy-stuff/
√ × ×
taxes.txt
http://www.marys-antiques.com/dynamic/buy- × × ×
stuff?id=3546
http://www.marys-antiques.com/dynamic/check- × √ ×
inventory?kitchen
9.4.7 HTML的robot-control元标签
robots.txt文件允许站点管理员将机器人排除在Web站点的部分或全部内容之外。
robots.txt文件的一个缺点就是它是Web站点管理员，而不是各部分内容的作者所
有的。
HTML页面的作者有一种更直接的方式可以限制机器人访问那些独立的页面。他
们可以直接在 HMTL 文档中添加 robot-control标签。遵循 robot-control
HTML标签规则的机器人仍然可以获取文档，但如果其中有机器人排斥标签，它们
就会忽略这些文档。比如，因特网搜索引擎机器人就不会在其搜索索引中包含这个