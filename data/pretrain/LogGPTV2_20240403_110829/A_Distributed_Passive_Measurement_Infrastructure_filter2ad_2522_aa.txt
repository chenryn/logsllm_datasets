title:A Distributed Passive Measurement Infrastructure
author:Patrik Arlos and
Markus Fiedler and
Arne A. Nilsson
A Distributed Passive Measurement
Infrastructure
Patrik Arlos, Markus Fiedler, and Arne A. Nilsson
Blekinge Institute of Technology, School of Engineering,
{patrik.arlos, markus.fiedler, arne.nilsson}@bth.se
Karlskrona, Sweden
Abstract. In this paper we describe a distributed passive measurement
infrastructure. Its goals are to reduce the cost and conﬁguration eﬀort per
measurement. The infrastructure is scalable with regards to link speeds
and measurement locations. A prototype is currently deployed at our
university and a demo is online at http://inga.its.bth.se/projects/dpmi.
The infrastructure diﬀerentiates between measurements and the analysis
of measurements, this way the actual measurement equipment can focus
on the practical issues of packet measurements. By using a modular
approach the infrastructure can handle many diﬀerent capturing devices.
The infrastructure can also deal with the security and privacy aspects
that might arise during measurements.
1
Introduction
Having access to relevant and up-to-date measurement data is a key issue for
network analysis in order to allow for eﬃcient Internet performance monitoring,
evaluation and management. New applications keep appearing; user and proto-
col behaviour keep evolving; traﬃc mixes and characteristics are continuously
changing, which implies that traﬃc traces may have a short span of relevance
and new traces have to be collected quite regularly.
In order to give a holistic view of what is going on in the network, passive
measurements have to be carried out at diﬀerent places simultaneously. On this
background, this paper proposes a passive measurement infrastructure, consist-
ing of coordinated measurement points, arranged in measurement areas.
This structure allows for a eﬃcient use of passive monitoring equipment in
order to supply researchers and network managers with up-to-date and relevant
data. The infrastructure is generic with regards to the capturing equipment,
ranging from simple PCAP-based devices to high-end DAG cards and dedicated
ASICs, in order to promote a large-scale deployment of measurement points.
The infrastructure, which currently is under deployment at our university,
was designed with the following requirements in mind:
1. Cost. Access to measurement equipment should be shared among users, pri-
marily for two reasons: First, as measurements get longer (for instance for
detecting long-range dependent behaviour) a single measurement can tie
C. Dovrolis (Ed.): PAM 2005, LNCS 3431, pp. 215–227, 2005.
c(cid:1) Springer-Verlag Berlin Heidelberg 2005
216
P. Arlos, M. Fiedler, and A.A. Nilsson
up a resource for days (possibly weeks). Second, high quality measurement
equipment is expensive and should hence have a high rate of utilization.
2. Ease of use. The setup and control of measurements should be easy from the
user’s point of view. As the complexity of measurements grows, we should
hide this complexity from the users as far as possible.
3. Modularity. The system should be modular, this to allow independent devel-
opment of separate modules. With separate modules handling security, pri-
vacy and scalability (w.r.t. diﬀerent link speeds as well as locations). Since
we cannot predict all possible uses of the system, the system should be
ﬂexible to support diﬀerent measurements as well as diﬀerent measurement
equipment.
4. Safety and Security. Measurement data should be distributed in a safe and
secure manner, i.e. loss of measurement data should be avoided and access
to the data restricted.
To solve these requirements we came up with an infrastructure consisting of
three main components, Measurement Point (MP), Consumer and Measurement
Area (MAr). The task of the MP is to do packet capturing, packet ﬁltering, and
distribute measurement data. The approach to the second design requirement
was to use a system with a web interface. Through this interface users can add
and remove their desired measurements. The MAr then handles the communica-
tion with the MPs. The cost for implementing this architecture is not very high,
compared to a normal measurement setup you need two additional computers
and an Ethernet switch of suitable speed, and this basic setup can grow as the
requirements change.
There are several other monitoring and capturing systems available, here we
describe only a few.
CoralReef [1] is a set of software components for passive network monitoring,
it is available for many network technologies and computer architectures. The
major diﬀerence between CoralReef and our infrastructure is that CoralReef
does not separate the packet capturing and analysis as we do. Furthermore, the
CoralReef trace format does not include location information as our does.
IPMON [2] is a general purpose measurement system for IP networks. IP-
MON is implemented and deployed by Sprint. IPMON separates capturing from
analysis, similar to our infrastructure. On the other hand, the IPMONs store
traces locally and transfer them over a dedicated link to a common data repos-
itory. The repository is then accessed by analyzers.
Gigascope [3] uses a similar approach as IPMON, by storing captured data
locally at the capturer. This data is then copied, either in real time or during
oﬀ-peak hour, to a data warehouse for analysis. It uses GSQL as an interface to
access the data.
The IETF has (at least) two work groups that are relevant for this work;
Packet Sampling (PSAMP) [4] and IP Flow Information Export (IPFIX) [5].
PSAMP works on deﬁning a standard set of capabilities for network elements to
sample subsets of packets by statistical and other methods. Recently an Internet
draft was published [6], which describes a system at a higher level than our in-
A Distributed Passive Measurement Infrastructure
217
frastructure, but they are very similar and our system could beneﬁt by adjusting
somewhat to the PSAMP notation. The IPFIX group is interesting since they
deal with how to export measurement data from A to B, thus it is interesting
with regards to consumers.
In Section 2 we will discuss the components and how they interact. This
is followed by Section 3 where we describe how the system handles rules and
ﬁlters. In Section 4 we discuss privacy and security related to the infrastructure.
In Section 5 we describe two cases where the system has been deployed. In
Section 6 we describe some of the ongoing and future work. And in Section 7 we
conclude the paper.
2 Components
The three main components in the infrastructure will be described in the follow-
ing subsections.
2.1 Measurement Point
In Figure 1 the components of a schematic MP are shown. This is the device
that does the actual packet capturing. It is managed from a Measurement Area
Controller (MArC) and transfers the captured data to consumers attached to
the Measurement Area Network (MArN). The MP can either be a logical or a
physical device. A logical MP is simply a program running on a host, whereas a
physical MP could either use a dedicated computer or custom hardware in order
to create high-speed high-performance MPs.
Link Under Test
Time
Synchronization
Client
Wire-
tap
CI
CI
Receiver
Receiver
Control
Data
Sender
NIC
Controller
MArN
Fig. 1. Schematic overview of a MP
A MP can tap one or more links; each link is tapped via a wiretap. For
full-duplex Ethernets, a wiretap has two outputs, one for each direction. These
are connected to separate capture interfaces (CI). A receiver listens to a CI and
ﬁlters the packets according to the ﬁlter rules stated by the MArC. If the CI
hasn’t timestamped the packet the receiver will do so. The packets are then
delivered to the sender, which is responsible for sending the captured packets
218
P. Arlos, M. Fiedler, and A.A. Nilsson
to the appropriate consumers. Such a measurement frame can contain several
packets, where the number of packets is controlled by the maximum transfer
unit (MTU) of the MArN. Each MP also has a controller that is responsible
for the conﬁguration of the MP and the communication with the MArC. A
time synchronization client (TSC) is used to keep all the MPs with in a MAr
synchronized, which can be done using a dedicated device or a simple NTP
server.
The ﬁlter rules used by the receiver specify, in addition to packet properties,
a consumer and the amount of the packet to be captured (currently the upper
limit is 96 bytes). For each frame that passes the ﬁlter, the MP attaches a cap-
ture header (Figure 2). In this header, we store a CI identiﬁer, a MP identiﬁer,
a timestamp when the packet was captured (supporting an accuracy of picosec-
onds), the packet length, and the number of bytes that actually were captured.
The ﬁlters are supplied to the MP from the MArC, and they will be discussed
in Section 3. Once a packet matches a ﬁlter, it is stored in a buﬀer pending
transmission. Once the buﬀer contents reaches a certain threshold the buﬀer is
transmitted using Ethernet multicast. This way, it is simple to distribute frames
to several consumers in one transmission. The duplication of data is done by the
MArN. This approach will also reduce the probability of overloading the MArN,
and hence preventing loss of measurement frames as far as possible. However, in
order to detect frame loss each measurement frame is equipped with a sequence
number that is checked by the consumer upon reception. If a measurement frame
is lost it is up to the consumer to handle this particular loss and notify the
MArC. Given this information the MArC can take actions to prevent future
losses. Actions can be to alter ﬁlters as well as requesting additional switching
resources inbetween the MPs and the Consumers. The current implementation
only notiﬁes the consumer “user”, who has to take appropriate actions.
CI
MAMPid
Time
Time
Length
CapLen
Fig. 2. Capture Header
The capture header enables us to exactly pinpoint by which MP and on what
link the frame was captured, which is vital information when trying to obtain
spatial information about the network’s behaviour. This also enables us to use
several MPs to measure a single link, which is interesting when the measurement
task of a link speed becomes too great for a single MP to handle. This would
require a device that is capable of distributing the packets such that the wiretap
feeds diﬀerent MPs in a round robin approach.
A Distributed Passive Measurement Infrastructure
219
2.2 Measurement Area
In Figure 3 an example of a MAr is shown. The MAr provides a common point of
control for one or more MPs. It uses a dedicated network in between MPs and the
MAr subsystems for reasons of performance and security. A MAr consists of the
following subsystems: a MArC, a time synchronization device (TSD), a MArN
and at least one consumer and one MP. The MArC is the central subsystem
in a MA. It supplies the users with a GUI for setting up and controlling their
measurements. It also manages the MPs by supplying ﬁlters and by keeping
track of their status. The TSD supplies all the MPs in the MA with a common
time and synchronization signal. It can utilize the existing Ethernet structure to
the MPs, or it can utilize some other network to distribute the time signal.
MP1
MP2
MP3
Control
Data
Time Synchronization Device
MArN
Switch
MArC
Users/SMA
Switch
Cn
Consumer1
Consumer2
SMArFilter
MArN
Consumer-Network
Fig. 3. Simple overview of a MA with three MPs, four consumers, one MArC and a
time synchronization unit
The capacity of the MArN should be such that it can handle the peak rate
of the measured traﬃc. Assume that a MP monitors a 10Base-T link, with a
frame rate of 800 fps where each frame is 1500 bytes long (≈ 9.6 Mbps). From
each frame we collect 96 bytes, add a capture header of 36 bytes and store the
data in a measurement frame, see Figure 4. Given a MArN MTU of 1500, a
measurement frame can contain 1480 bytes of measurement data, consisting of
capture headers and frames, the remaining 20 bytes are used by a measurement
header (MH). In the current example we can store 11 frames in each measurement
frame (11 ∗ (36 + 96) = 1452 ≤ 1480 bytes), causing the MP to send only
800/11 ≈ 72 fps into the MArN, see Figure 5. If the monitored link would have a
frame rate of 14000 fps, each frame would only be 85 bytes long (≈ 9.6 Mbps), the
measurement frame would contain 12 frames (12∗(36+85) = 1452 ≤ 1480 bytes),
yielding a frame rate of 14000/12 ≈ 1167 fps. However, if the MArN MTU was
9000, the measurement frame could contain 74 frames, yielding a frame rate of
189 fps.
220
P. Arlos, M. Fiedler, and A.A. Nilsson
MH
36
CH
0-1500
Frame j+1
CH
Frame j+2
...
CH
Frame j+n
MTU MArN
Fig. 4. Measurement frame encapsulation
...
...
...
...
Captured frames
t
Measurement frames
Fig. 5. After capturing N frames one measurement frame is sent from the MP
A consumer that attaches to the MArN should not request more data than the
link that it is attached to can handle. For instance a consumer C1 is the recipient
of two measurement streams, S1 and S2, each generating 1272 measurement
frames per second. As long as the total frame rate of S1 and S2 is less or equal
to the capacity oﬀered by link and switch there should be no problems, but if the
consumer desires to get full frames it might run into problems quite fast, since the
MP adds a capture header to each captured frame potentially generating more
traﬃc than it captures. The current implementation addresses this problem by
having a maximum capture size of 96 bytes. The MArC also provides the user
with an estimation of the frame rate on the links that the MPs are monitoring,