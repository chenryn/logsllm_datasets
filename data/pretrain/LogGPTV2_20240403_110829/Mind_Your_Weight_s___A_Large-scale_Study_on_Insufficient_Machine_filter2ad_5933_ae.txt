Freed Buffer
Freed Buffer
Memory Dumping
Freed Buffer
Freed Buffer
Freed Buffer
Freed Buffer
Freed Buffer
Freed Buffer
Freed Buffer
Freed Buffer
Decryption Buffer
the name of the apps to protect the user’s security; 3) Every app has several models for different functionalities, we only list one representative model for each app.
Note: 1) We excluded some apps that dumped the same models as reported above; 2) We anonymized
classiﬁcation model, including the APIs used by the App, the
Permissions claimed in the Android Manifest ﬁles and so on.
By encrypting the feature vectors, the developer assumes it is
impossible to (re)use the model because the input format and
content are unknown to attackers. However, we instrumented
the decryption functions and extracted the decrypted feature
vectors. With this information, an attacker can steal and
recover the model as well as the feature vector format, which
can lead to model evasions or bypassing the malware detection.
It shows that even though some models take speciﬁc input
format, with some basic reverse engineering effort, the attacker
can still uncover and reuse the model.
Encrypting Models Multiple Times: We also observed that
one app encrypts its models multiple times. This app offers
online P2P loans. It uses two models provided by SenseTime:
one for ID card recognition and the other for liveness detection,
which are security critical. ModelXtractor successfully
extracted 6 model buffers, whose sizes range from 200KB
to 800KB. However, we only found 2 encrypted model ﬁles.
When we were trying to map the model buffers to the encrypted
ﬁles, we found something very interesting. One encrypted
model ﬁle named SenseID_Ocr_Idcard_Mobile_1.0.1.model
has a size of 1.3 MB. Among the dumped model buffers,
we have one buffer of the same size. It is supposed to be
the right decrypted buffer. After analyzing its content, we
found that it is actually a tar ﬁle containing multiple ﬁles, one
of which is align_back.model. After inspecting the content
of align_back.model, we found that it is also an encrypted
ﬁle. We then found another buffer of the same size, 246 KB,
which contains a decrypted model. We ﬁnally realized that
the app encrypts each model individually and compresses all
encrypted models into a tar ﬁle, then encrypts it again.
5.6 Responsible Disclosure
We have contacted 12 major vendors whose apps have leaked
models, including Google, Facebook, Tencent, SenseTime
and etc. We have received responses from ﬁve of them.
In summary, for vendors that use plaintext models, one ven-
dor is unaware of possible model leakage until we contact them.
For the other vendors, one of them is unaware of the impact that
leaked models can incur. Two vendors respond with lack of
a practical solution to protect the models, in which one vendor
is waiting for hardware support to encrypt the models securely,
and the other fails to ﬁnd an existing proprietary mitigation to
make it harder for model reuse. This vendor assumes that ma-
licious end users might eventually gain access to some model
data, but not for practical use. For vendors whose models are
encrypted but can still be extracted, our research raised internal
discussions of one vendor on improving model security. The
vendor is taking actions on robust model protection, with re-
search and collaborations with well-known security partners.
6 Q3: What Impacts can (Stolen) Models
Incur?
ML models are the core intellectual properties of ML solution
providers. The impacts of leaked models are wide and
profound, including substantial ﬁnancial impact as well as
signiﬁcant security implications.
6.1 Financial Impact
6.1.1 Financial Beneﬁt for Attackers
App developers usually have two legitimate ways to get ML
models: (1) buying a license from ML solution providers,
such as SenseTime, Face++, and so on; (2) Developing their
own ML models, which usually requires a large amount of
computing and human resources. Stealing the models saves
the attackers either the license fee paid to the model providers,
or the research and development (R&D) cost on the models.
License Fee Savings for Attackers: Usually, when vendors
license an ML model, the app developer can choose between
1966    30th USENIX Security Symposium
USENIX Association
online authorization or ofﬂine authorization. A license with
ofﬂine authorization allows a device to use the ML SDK
without network connection. A company with such licenses
is given unlimited uses on different devices [14]. The down
side is that the model provider has no control over the number
of devices or which devices to have access to the model SDKs.
As a result, it is hard for the model provider to tell whether
a model has been stolen or not. According to Face++, the
annual fee for a license with ofﬂine authorization is $50,000
to $200,000 [14]. The saving is large enough to motivate
an attacker to steal the models or the model licenses. In our
analysis, we found 60 cases in which several different apps
sharing one model license. One of the licenses is even used
by 12 different apps, indicating a high chance of illegal uses.
A license with online authorization can control the usages of
the SDKs. Before using the model SDK, a device has to authen-
ticate itself to the model provider with a license key. The model
provider can then count the number of authorized devices, and
charge the app company per device or per pack of devices.
Online authorization offers stronger protection of the model
licenses than ofﬂine authorization. However, there are still
chances that attackers stealthily use a license before it reaches
the limit of the current pack. The market price for face landmark
SDK is $10,000 for up to 10,000 of online authorizations [14].
Even though the savings are smaller than ofﬂine authorized
licenses, attackers can still beneﬁt from them ﬁnancially.
R&D Savings for Attackers: The R&D cost of ML models
comes from three sources: collecting and labeling data for train-
ing, hiring AI engineers for designing and ﬁne-tuning models,
and computing resources, such as renting or buying and main-
taining storage servers and GPU clusters for training models.
According to Amazon Mechanical Turk [2], the price of
labeling an object ranges from $0.012 to $0.84, depending on
the type of the object (e.g., image, text, semantic segmentation).
Considering the CMU Multi-PIE database as an example,
which contains more than 750,000 images [29], the cost of
labeling would be at least $9,000. For larger databases, for
example, MegaFace with 4.7 million labels [16], or some
audio and video datasets [20, 31], the cost of labeling could be
even higher. According to LinkedIn statistics [23], the median
base salary for machine learning engineers is $145,000 per
year. Given a team with ﬁve engineers, training and ﬁne-tuning
a model for one year, the cost would be $725,000. Based on
the pricing of Amazon SageMaker [3], the monthly rate for
ML storage is $0.14 per GB, and the hourly rate for the current
generation of ml.p3.2xlarge accelerated computing is $4.284.
Still considering the CMU Multi-PIE database as an example,
with a data size of 305GB, the yearly cost of data storage and
training would be $38,040.
Based on the above information, a conservative estimate
on the total saving for attackers on model R&D cost could
be $772,040. Note that the salary of AI engineers are based
on the public information of large AI companies, which can
be higher than those from small companies. The number of
AI engineers and the acutual model development cycle vary
from case to case. The estimation of R&D cost should take
all above factors into consideration.
6.1.2 Financial Loss for Model Vendors
For vendors whose main business (source of income) depends
on ML models, e.g., model providers or app companies, model
leakages result in pricing disadvantages, lost of customers and
market share.
Pricing Disadvantages for Vendors: As mentioned earlier,
the cost of ML models can reach millions of dollars, thereby
competitors have strong motivation towards leaked models.
Once competitors start adopting leaked models with lower
cost, they can offer lower prices to the customers. At the same
quality, customers are more willing to choose the cost efﬁcient
products. Therefore, vendors who leak their models will lose
the pricing competition in the ﬁrst place.
For model providers, the market is strongly competitive. In
our study, we have found some top ML SDK providers, such as
SenseTime, Megvii, Baidu, ULSee, Anyline, etc. Take Megvii
as an example, according to Owler [17], 10 competitors are
closely related to its businesses, such as Cognitec, SenseTime,
Kairos, FaceFirst, Cortexica, etc. For app companies, the
competition is as much competitive if not more so. In Google
Play only, our study found 36 apps using ML SDK for image
recognition as the main business. Considering the other two
stores, at least 215 apps are competing for this business.
Anticipated Falling Market Share for Vendors: The pricing
disadvantage caused by leaked models will potentially result
in loss of customers and market share, which will both lead
to signiﬁcant revenue loss. Take model provider SenseTime as
an example, our study found 8 unique SenseID_OCR models,
and each is reused by 21 apps on average. Loss of one single
app customer will potentially bring a loss of at least $10,000,
based on the market price discussed earlier (e.g., $10,000 for
up to 10,000 of online authorizations). In fact, SenseTime has
more than 700 customers and partners [24], and has a revenue
of $750 Million in 2019. For app companies, we also observed
unbalanced market share in the 215 apps competing for the
business of image recognition. The number of downloads for
these apps ranges from ten thousands to one hundred million.
For both model providers and app companies, the decline in
market share caused by pricing disadvantage may lead to
further ﬁnancial loss.
6.2 Security Impact
Some ML models are used for security-critical purposes. For
example, liveness detection model is used to verify whether
it is a real person holding a real ID card. Face, ﬁngerprint
and iris recognition models are used to detect and verify the
identity of a person. These models bring in great convenience,
for example, users do not need to go to a bank or customer
USENIX Association
30th USENIX Security Symposium    1967
service centers to verify their identities. However, breaches
of such models bring in security and privacy concerns.
For attackers, a leaked security-critical model makes it
easier for them to design and craft adversarial examples. They
can then use the examples to either fake different identities,
or simply bypass the identity check of the apps [7].
We found more than 100 apps using on-device ML models
for banking and loan services. These apps provide personal
loan services aiming at quick and convenient loan applications.
They use face recognition models to verify the identity of a
person by taking a short video, and comparing with the photo
on the ID card. The apps then determine the credit limits and
rates to loan to the applicants. When the models are leaked,
attackers can easily fake identities of other applicants, and
apply for loans on their behalf.
In our analysis, we found that 872 apps are using live-
ness detection models, representing 59% of all the apps
using on-device ML. We also found security-critical mod-
els to be shared among different apps, for example, the
SenseID_Motion_Liveness model is shared by 81 apps.
Leakage of this model from any of the apps will make it easier
for the attackers to bypass the detection to all the 81 apps.
For end users, it raises the concern that attackers with
faked identities can access users’ private information. For
example, some apps provide online medical services, such
as booking appointments, ﬁlling out medical history forms,
receiving electrical prescriptions, and laboratory reports from
the doctors. They may also use on-device ML models to verify
the identities of patients. Bypassing the veriﬁcation will allow
attackers to access personal medical records. In our analysis,
we found 6 such apps, which have been downloaded more
than 9 million times on 360 Mobile Assistant Store. One of
the face detection model, although encrypted, is shared by
77 different apps. Leakage of the model from any of the apps
will potentially expose the personal medical records of mass
end users. It is therefore important for vendors to protect the
models, especially when they are security-critical. Vendors
and app developers should be careful about the potential
security impact caused by leaked/stolen models.
7 Countermeasures
In this section, we discuss several existing approaches to
protecting on-device machine learning models and their
limitations. We also share our insights in the future research
of model protection.
7.1 Current Model Protection
Obfuscation makes it harder for attackers to recover the
model. We observed that developers have implemented their
own obfuscation/de-obfuscation mechanisms, which impose
non-trivial programming overhead. For example, NCNN can
convert models into binaries where text is all striped, and
Mace can convert a model to C++ code [26, 32].
Encryption prevents the attackers from directly accessing
the model from a downloaded APK. We observed that
developers use encryption in many ways to protect their
models, including the ML feature vectors, ML models, and
the code to run model inferences. However, they all fall victim
to our non-sophisticated dynamic analysis.
Customized model frameworks/formats increase the effort
for attackers to identify and reuse the models. We observed
that customized or proprietary model formats, such as
MessagePack (.model), pickle (.pkl), Thrift (.thrift), can be
used to counter against model reverse engineering. We also
observed customized ML library running encrypted JavaScript
in a customized WebView.
7.2 Limitations
Obfuscation is vulnerable to devoted attackers who can
recover the model with knowledge of binary decompilation.
Attackers can leverage program slicing and partial execu-
tion [41,51] to de-obfuscate Android apps [39,60], and further
decompile and recover the obfuscated models. Even without
these knowledge, attackers can reuse the model as a black box.
Encryption is vulnerable to attackers who can perform
dynamic analysis and instrument app memory at runtime. We
have demonstrated it in Section 5.1.
Customized model frameworks/formats are vulnerable to
documentation leakage of the model frameworks/formats. The
documentation may come from internal attackers, or skilled
and patient attackers who have good motivation to reverse
engineer the model frameworks/formats.
7.3 Future Works
Secure hardware is the most promising approach to pro-
tecting models on mobile devices. It has been demonstrated
on desktop platforms. For example, recent advance in
TF-Trusted [28] allows developers to run Tensorﬂow models
inside of secure enclaves, such as Intel SGX [15]. Slalom [56]
uses SGX during model inference, applies homomorphic en-
cryption on each layer’s input and outsources the computation
of linear layers to GPU securely. Privado [55] uses SGX to mit-
igate side channel attacks of input inference. TensorScone [46]
also uses SGX to protect model inference but does not consider
GPU. Graviton [58] is proposed to make GPU a trusted ex-
ecution environment with minimal hardware changes incurred.
So far, research in this area focuses on cloud-end security.
Future research should consider secure hardware backed
model inference on mobile device. For example, Arm
TrustZone [33] in mobile devices can be used to provide model
protection. There are also some unique challenges that needs
to be addressed on mobile devices. Compared with desktop
platforms, mobile devices are more restricted in computation
1968    30th USENIX Security Symposium
USENIX Association
resources, making it impractical to perform model inference
entirely in TEE. Given the wide adoption of GPU on mobile
devices, an effective model protection should also consider
using the GPU for acceleration in a secure way.
8 Discussion
Manual analysis effort: Although ModelXtractor can auto-
matically generate instrumentation scripts customized for the
apps, manual effort is required in the dynamic analysis. As de-
scribed in Section 5.3, some Chinese apps require registration
with valid phone numbers or regional bank accounts before
using ML models. Manual effort is thus needed to feed in valid
registration information. To maximize the chance of triggering
ML models, manual effort is also needed to fully navigate
the apps with ML-related functionalities. After the model is
loaded and suspected model buffer dumped by ModelXtractor,
manual effort is needed to verify the start of the model based
on the encoding signatures described in Section 5.2. Then we
truncate the buffer and use a model decoder, e.g. protobuf, to
parse the buffer and manually verify whether it is a ML model.
The amount of manual effort depends on how easy it
is to trigger the ML functionality. Some apps do not need
registration and the ML models are loaded by default, such
as some AI camera apps, extracting their models takes less
than an hour. In the worst cases, such as some P2P loan apps,
whole ML models cannot be loaded without registration with
valid phone numbers and regional bank accounts, it may take
hours to extract the models. We therefore prioritize on apps
whose models can be easily extracted, and budget 2 hours for
each app among the 82 apps we analyzed in Table 6.
Research Insights: White-box Adversarial Machine Learning.
Previous research on adversarial machine learning has been
focused on black-box threat models, assuming the model ﬁles
are inaccessible. Our research shows that an attacker can easily
extract the protected private models. As a result, more research
on defending adversarial machine learning under white-box
threat model is much needed to improve the resiliency of those
models used in security critical applications.
Model Plagiarism Detection. As machine learning models
are not well protected, attackers, instead of training their own
model, can steal their competitor’s model and reuse it. As a
result, model plagiarism detection is needed to prevent this
type of attack. It is challenging because the attacker can retrain
their model based on the stolen one, making it looks very
different. We need research to detect model plagiarism and
provide forensic tools for illegal model reuse analysis.
Limitations: Since the goal of this paper is to show that even
simple tools can extract on-device ML models in a large scale,
ModelXRay and ModelXtractor are limited by the straightfor-
ward design of keyword matching. We acknowledge that the
scale of model extraction can be further improved by leveraging
program slicing and partial execution [41,51], and Android app
de-obfuscation [39, 60]. Further, model encoding and content
features are limited to well-known ML SDKs having documen-
tation available, thereby we believe an extended knowledge
base can further include special model encoding formats.
We note that our ﬁnancial loss analysis is subjective and lim-
ited by the asymmetric information of R&D cost and company
revenue. The approach is used to emphasize the point that costs
can be very high. A more comprehensive study can be carried
out by stakeholders having real data of model leakage cases.
9 Related Work
Motivated by hardware acceleration and efﬁciency improve-
ment of deep neural networks [48], on-device model inference
becomes a new trend [61]. This work empirically evaluates
model security on mobile devices. It interacts with three lines
of research: machine learning model extraction, adversarial
machine learning, and proprietary model protection.
To extract information from Android apps, prior works have
used various techniques, such as memory instrumentation,
program slicing and partial execution. For example, to detect
Android malware, Hoffmann presents static analysis with