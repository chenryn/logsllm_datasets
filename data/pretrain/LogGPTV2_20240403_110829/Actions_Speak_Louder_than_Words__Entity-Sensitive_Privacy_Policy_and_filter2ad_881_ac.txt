C11
C12
whether a speciﬁc data ﬂow is disclosed. For example, con-
sider we have the data ﬂow (Flurry, advertising identiﬁer) and
the policy statements (analytic provider, collect, advertising
identiﬁer) and (advertiser, not_collect, advertising identiﬁer).
Since Flurry is both an advertiser and analytic provider, the
policy is ambiguous with respect to this data ﬂow.
Before deﬁning our ﬂow-to-policy consistency model, we
deﬁne three ﬁlters on the set policy statements in a policy.
These ﬁlters simplify the notation used to formally describe
the ﬁve disclosure types. The following discussion assumes
the analysis of an individual application, and each disclosure
is described with respect to a speciﬁc data ﬂow f . Applica-
tions may have multiple data ﬂows. Furthermore, each appli-
990    29th USENIX Security Symposium
USENIX Association
cation has a set of policy statements P.
Deﬁnition 3.8 (Contradicting Policy Statements). Let P be a
set of policy statements (Deﬁnition 3.2). PC is the set of policy
statements p ∈ P for which there exists a p(cid:48) ∈ P such that p
and p(cid:48) have a logical contradiction (C1−5) or a ﬂow-sensitive
contradiction (C6−12).
Deﬁnition 3.9 (Narrowing Deﬁnition Policy Statements). Let
P be a set of policy statements (Deﬁnition 3.2). PN is the set
of policy statements p ∈ P for which there exists a p(cid:48) ∈ P
such that p and p(cid:48) have a narrowing deﬁnition (N1−4).
Deﬁnition 3.10 (Flow-Relevant Policy Statements). Let P be
a set of policy statements (Deﬁnition 3.2) and f be a data ﬂow
(Deﬁnition 3.1). Pf is the set of policy statements in P that are
relevant to the data ﬂow f . More speciﬁcally, Pf = {p | p ∈
P ∧ f .d (cid:118)δ p.d ∧ f .e (cid:118)ε p.e}.
3.3.1 Flow-to-Policy Consistency
Using the above deﬁnitions, we can now deﬁne ﬂow-to-policy
consistency. As discussed in Section 2, there are two types of
consistent disclosures: clear disclosures and vague disclosures.
We now formally deﬁne ﬂow-to-policy consistency and the
two types of disclosures, as well as a vagueness metric to help
quantify the signiﬁcance of vague disclosures.
Deﬁnition 3.11 (Flow-to-Policy Consistency). A data ﬂow
f
is consistent with an application’s privacy policy P
if and only if ∃p ∈ Pf such that p.c = collect ∧ (cid:54) ∃p(cid:48) ∈
Pf such that p(cid:48).c = not_collect.
Deﬁnition 3.12 (Clear Disclosure). An application’s privacy
policy has a clear disclosure of a data ﬂow f if there exists
a collect policy that uses terms of the same semantic granu-
larity for both data type and entity, and there does not exist
a conﬂicting not_collect policy for the data type and entity.
More speciﬁcally, there is a clear disclosure of f if and only
if ∃p ∈ Pf such that p.c = collect ∧ f .d ≡δ p.d ∧ f .e ≡ε p.e
and (cid:54) ∃p(cid:48) ∈ Pf such that p(cid:48).c = not_collect.
Deﬁnition 3.13 (Vague Disclosure). An application’s pri-
vacy policy has a vague disclosure of a data ﬂow f if there
does not exist clear disclosure, but there does exist a collect
policy using a broader semantic granularity for either the
data type of entity, and there does not exist a conﬂicting
not_collect policy for the data type and entity. More specif-
ically, there is a vague disclosure of f if and only if (cid:54) ∃p ∈
Pf such that p.c = collect ∧ f .d ≡δ p.d ∧ f .e ≡ε p.e and
∃p(cid:48) ∈ Pf such that p(cid:48).c = collect ∧ f .d (cid:118)δ p(cid:48).d ∧ f .e (cid:118)ε p(cid:48).e
and (cid:54) ∃p(cid:48)(cid:48) ∈ Pf such that p(cid:48)(cid:48).c = not_collect.
A data ﬂow with a vague disclosure is not necessarily bad.
However, if the terms in the matching policy statement are
too broad, the disclosure may not be meaningful to the user.
For example, the policy statement (third-party, collect, per-
sonal data) is considerably more vague than (AdMob, collect,
advertising identiﬁer) to describe the data ﬂow (AdMob, ad-
vertising identiﬁer). As vagueness is subjective, we do not
seek a binary classiﬁcation (e.g., weak violations [29]). In-
stead, we provide a quantitative metric [0.0-1.0] to rank and
compare statements in terms of vagueness. A higher value
indicates greater vagueness.
Our metric calculates a tuple for vagueness of a ﬂow f ’s
disclosure via a policy statement p using the ontological dis-
tances, with values for both data type and entity. Since the
magnitude of ontological distances can vary, we normalize
the ontological distance to allow ranked comparisons.
Deﬁnition 3.14 (Vagueness Metric). The vagueness of
a ﬂow f by a policy statement p is represented by
( ˆ∆ε( f .e, p.e), ˆ∆δ( f .d, p.d)).
Note that the vagueness metric allows reasoning in two-
dimensions (i.e., entity and data type). We observed that rea-
soning in two-dimensional space increased utility of the met-
ric for triage in comparison to reducing the metric to one-
dimension by averaging of summing the scores. For example,
if averaging or summing, the tuples (anyone, collect, device
information) and (advertising network, collect, information)
would have the same vagueness score for the ﬂow (AdMob,
Ad ID). In this case, consider an analyst wants to identify
which applications are not discussing entities overly broad
when disclosing data sharing practices. A one-dimensional re-
duction would require additional ﬁltering based on the result,
but the two-dimensional vagueness metric directly supports
such triage approaches.
3.3.2 Flow-to-Policy Inconsistency
A data ﬂow is inconsistent with the privacy policy if it does
not satisfy the above consistency conditions. We deﬁne three
types of disclosures that represent ﬂow-to-policy inconsis-
tency: omitted disclosures, incorrect disclosures, and ambigu-
ous disclosures.
Deﬁnition 3.15 (Omitted Disclosures). An application’s pri-
vacy policy has an omitted disclosure of a data ﬂow f if it
does not include either collect or not_collect statements at
any semantic granularity for the ﬂow’s data type and entity.
More speciﬁcally, there is an omitted disclosure of f if and
only if Pf = /0.
Deﬁnition 3.16 (Incorrect Disclosure). An application’s pri-
vacy policy has an incorrect disclosure of a data ﬂow f when
the policy states that it does not collect or share the data type.
More speciﬁcally, there is an incorrect disclosure of f if and
only if ∀p ∈ Pf , p.c = not_collect or (Pf (cid:54)= /0 ∧ Pf ∩ PN (cid:54)= /0
∧ Pf ∩ PC = /0).
Note that incorrect disclosures include narrowing deﬁni-
tions, because they represent an unambiguous case where a
USENIX Association
29th USENIX Security Symposium    991
policy has relevant ﬂows with both collect and not_collect
sentiment. Since narrowing deﬁnitions have not_collect senti-
ment for the more speciﬁc type, a matching data ﬂow repre-
sents an incorrect disclosure.
Deﬁnition 3.17 (Ambiguous Disclosure). An application’s
privacy policy has an ambiguous disclosure of a data ﬂow f
when it contains contradicting statements about the data ﬂow.
More speciﬁcally, there is an ambiguous disclosure of f if
and only if Pf ∩ PC (cid:54)= /0.
4 Design
The core contribution of this paper is our formalization and
enhancement of ﬂow-to-policy consistency analysis with the
knowledge of which entities collect information. Determin-
ing the type of disclosure (Section 3) for each observed ﬂow
requires both dynamic analysis of applications and natural
language processing of application privacy policies. We chose
dynamic analysis over static analysis, because it provides (1)
evidence that the ﬂow occurs (some ad libraries use server-
side conﬁguration to determine what data types to collect),
and (2) the network destination of the ﬂow. As dynamic anal-
ysis of Android apps has received signiﬁcant treatment in
literature, we build upon AppCensus [6], which is the latest
state-of-the-art for dynamically performing privacy analy-
sis. Similarly, for processing privacy policies, we build on
top of PolicyLint [4], which enhances prior approaches by
extracting entities, as well as negative sentiment statements.
POLICHECK’s implementation primarily consists of our for-
malization provided in Section 3. The remainder of this sec-
tion describes the components required to transform data
ﬂows and policy statements into our logical representation.
Data Flow Extraction: AppCensus [6] identiﬁes privacy sen-
sitive data ﬂows in Android apps using the approach proposed
by Reyes et al. [27]. In particular, Reyes et al. instrument the
Android operating system to log access to sensitive resources
and use the Android VPN API to intercept and log network
trafﬁc (including installing a root certiﬁcate to decrypt TLS
trafﬁc). They exercise the application with Monkey [5] and
collect both the system and network logs. Next, they identify
the privacy-sensitive data values from the system logs in the
network trafﬁc logs by using value-matching along with a set
of heuristics to detect encodings of the data, such as base64 or
hashing algorithms. The data ﬂows reported by AppCensus
are a tuple, (destination domain/IP address, data type). For
example, the data ﬂow discussed in Section 2.1 is represented
as (cdp.cloud.unity3d.com, advertising identiﬁer). Table 2
shows the complete list of data types tracked via dynamic
analysis within this study.
Domain-to-Entity Mapping: While data ﬂows are repre-
sented as a type of data being transmitted to a domain or
IP address, privacy policies discuss data ﬂows using terms
for entities instead of domains (e.g., cdp.cloud.unity3d.com
Table 2: Data types tracked via dynamic analysis
Data Types
name, location, phone number, email address, IMEI, Wi-Fi
MAC address, Ad ID, GSF ID, Android ID, serial number,
SIM serial number
could be referred to as “Unity” within the privacy policy).
Therefore, POLICHECK must map domain names to entities
so that data ﬂows conform to Deﬁnition 3.1. Note that for the
data ﬂows that had only IP addresses without domain names,
we ﬁrst perform a reverse-DNS search to try to resolve the IP
address as a domain name. If we could not resolve a domain
name, we discard the data ﬂow from the data set.
We curated a list of 144 advertisers and 40 analytics
providers on the Google Play store from AppBrain.com. This
list included the primary website for each organization. We
manually produced a supplementary set of terms based on
organization names to search our set of domain names with.
After obtaining a list of potential domain names for each orga-
nization by keyword matching our search terms, we manually
culled incorrect or irrelevant domain names. This resulted in
a set of domain names for the top analytics and advertising
organizations on Google Play.
Entity First-Party Classiﬁcation: Determining which net-
work domain is the ﬁrst-party of a given application requires
careful consideration. First, we check if reversing the second-
level domain name of the network destination domain matches
the beginning of the application’s package name. For ex-
ample, if the ﬂow (advertising identiﬁer, analytics.mobil-
e.walmart.com) occurs in the app com.walmart.android, we
mark the ﬂow as a ﬁrst-party ﬂow, as reversing walmart.c-
om results in com.walmart, which matches the beginning of
the package name. Similarly, we check if the second-level
domain name of the link to the application’s privacy pol-
icy matches the root domain of the destination domain. For
example, the privacy policy of the Walmart application is
located at https://corporate.walmart.com/privacy-secur-
ity/walmart-privacy-policy and the destination domain is
analytics.mobile.walmart.com. Since the second-level do-
main name of the privacy policy (walmart.com) matches the
second-level domain name domain of the destination domain
is walmart.com, we mark the ﬂow as ﬁrst-party.
Sharing
Extraction:
POLICHECK uses the policy statements output by Pol-
icyLint [4]. PolicyLint uses sentence-level natural language
processing to extract sharing and collection statements
from privacy policies while capturing the entities and data
types involved along with the sentiment of the statement.
PolicyLint outputs policy statements as deﬁned by the form
in Deﬁnition 3.2.
Data Type and Entity Ontology Extension: We extended
PolicyLint’s ontologies to include all of the data types in-
volved in data ﬂows and all of the entities identiﬁed when
constructing the domain-to-entity mapping. We begin by prun-
and Collection
Statement
992    29th USENIX Security Symposium
USENIX Association
Table 3: Data Flows and Apps for each Disclosure Type
Ambig.
Incorr.
Clear
215
205
6
6
221
211
Vague
2,196
1,589
24,434
7,105
26,630
7,885
Omit.
197
146
12,395
4,582
12,592
4,659
First
Third
Total
Flows
Apps
Flows
Apps
Flows
Apps
16
9
2,209
779
2,225
788
390
244
3,573
990
3,963
1,193
ing PolicyLint’s ontologies to remove nodes and edges that
did not reach a data type or entity node in the data ﬂows. For
all missing data types and entities, we manually added them
to their corresponding ontology and added edges. Finally, we
extended PolicyLint’s synonym list by searching policy state-
ments using keywords for entity names and data types. For
example, extended the synonym list for “advertising network”
by searching the policy statements for “advertising”.
Consistency Analysis: POLICHECK uses the data ﬂows and
policy statements from the prior steps to perform ﬂow-to-
policy consistency analysis. To do so, we implemented the
consistency model logic deﬁned in Section 3.
5 Consistency Characterization
Our primary motivation for creating POLICHECK was to
analyze whether applications are disclosing their privacy-
sensitive data ﬂows in their privacy policies, especially for
third-party sharing. In this section, we use POLICHECK to
perform a large-scale study of analyzing the consistency of
45,603 data ﬂows from 13,796 unique Android applications
and their corresponding privacy policies.
Dataset Selection: To select our dataset, we began by scrap-
ing the top 100 free applications (“topselling_free” collection)
across Google Play’s 35 application categories in February
2019. We enhanced the dataset with an additional 42,129 ran-
domly selected Android applications from AppCensus. Any
overlaps between the two datasets were resolved by only ana-
lyzing the latest version. For each application, we downloaded
the data ﬂows from AppCensus and downloaded the HTML
privacy policies from the developer’s website via the link on
Google Play. We excluded applications that did not have any
data ﬂows reported by AppCensus (23,488 apps). We also
excluded applications whose privacy policies were not suc-
cessfully downloaded (e.g., 404 errors, links to homepages) or
were not written in English based on Python’s langdetect
module (6,039 apps). We also excluded data ﬂows that did
not map to nodes in our entity ontology, which resulted in a
ﬁnal data set of 13,796 applications with 45,603 data ﬂows.
5.1 Consistency Analysis