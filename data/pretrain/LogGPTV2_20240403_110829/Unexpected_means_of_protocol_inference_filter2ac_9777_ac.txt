change of the node sequence of all paths going through the node;
redundant paths may now exist. We avoid those at all times by
enforcing a uniqueness invariant: no two paths have the same se-
quence of nodes at any one time. Where duplicate paths occur, they
are suppressed and a per-path redundancy counter is incremented.
We do not currently limit the number of different paths in the graph
because it has not become an issue in practice. Should path elimi-
nation become necessary, an eviction scheme similar to the one for
nodes could be implemented.
5.2 Compare
To compare two CSGs, a graph similarity measure is needed.
The measure we have implemented is a variant of feature-based
graph distances [19]: the two features we use for the computation
are the weights and labels of the graph nodes. Our intuition is that
for two CSGs to be highly similar, they must have nodes that ex-
hibit high similarity in their labeling while at the same time having
comparable weight. We have decided against the use of path node
sequencing as a source of similarity information for performance
reasons: the number of nodes in a graph is tightly controlled, while
we currently do not enforce a limit on the number of paths.
When comparing two CSGs G and H, we do a pairwise com-
parison (ni, nj) ∈ NG × NH, ﬁnding for every node ni ∈ NG
the node nj ∈ NH that provides the largest label overlap, i.e., for
which |L(ni, nj)| is maximized. Let the LCS yielding ni’s maxi-
mum overlap with the nodes of NH be denoted as Lmax(ni, NH ).
The score contributed by node ni to the similarity is then the ratio
of the best overlap size to the node label’s total length, multiplied
by PG(ni) to factor in ni’s importance. The scores of all nodes are
summarized and normalized, resulting in our similarity measure
S(G, H) between two graphs G and H:
X
ni∈NG
PG(ni)
X
S(G, H) =
|Lmax(ni, NH )|
|ni|
PG(ni)
ni∈NG
5.3 Merge
The way the merge operation proceeds depends on whether the
CSG that is being merged into another one needs to remain intact
or not. If it does, then merging a CSG G into H is done on a path-
by-path basis by duplicating each path p ∈ PG, inserting it as a
new LCS into H, and copying over the redundancy count. If G is
no longer required, we can just unhook all paths from the start and
end nodes, re-hook them into H, and make a single pass over G’s
old nodes to merge them into H.
5.4 Score
To be able to classify ﬂows given a set of CSGs loaded with
trafﬁc, one needs a method to determine the similarity between an
arbitrary ﬂow and a CSG as a numerical value in [0, 1]. Intuitively
we do this by trying to overlay the ﬂow into the CSG as well as
possible, using existing paths. More precisely, we ﬁrst scan the
ﬂow for occurrences of each CSG node’s label in the ﬂow, keeping
track of the nodes that matched and the locations of any matches.
The union of paths going through the matched nodes is a candidate
set of paths among which we then ﬁnd the one that has the largest
number of matched nodes in the same order in which they occurred
in the input ﬂow. By carefully numbering each path’s links we can
do this without actually walking down each candidate path. Note
that this gives us the exact sequence, location, and extent of all sub-
strings in the ﬂow that are typical to the trafﬁc the CSG has been
loaded with—when using a single protocol’s trafﬁc, we can expect
to get just the protocol-intrinsic strings “highlighted” in the ﬂow.
Finally, to get a numerical outcome we sum up the total length
of the matching nodes’ labels on that path and divide by the ﬂow
length, yielding 1 for perfect overlap and 0 for no similarity. Fig-
ure 4 describes the process.
6. CLASSIFICATION FRAMEWORK
In this section we present a cell-based framework for classifying
trafﬁc based on the notions and premises of constructing protocol
models as presented in Section 3. Our purpose here is to describe
in concrete terms how to implement a classiﬁcation system based
on our models. Moreover, the modularity of this framework al-
lows us to evaluate different protocol models (e.g., product distri-
butions, Markov Processes, and Common Substring Graphs) while
allowing them to share common components such as surrounding
cell construction, clustering, and matching implementations. Fig-
ure 5 summarizes the overall operation of our protocol description
construction algorithm, from training cells starting with processing
input sessions to merging cell clusters.
Equivalence Groups. We begin with the ﬁrst step of assem-
bling sessions into equivalence groups to construct cells, as illus-
trated in Figure 5a. For our implementation we assume that all
communication sessions sharing the same service key belong to the
same protocol. Here, we deﬁne a service key as the 3-tuple (respon-
der address, responder port, and transport protocol). We believe
this key produces a sensible equivalence group because hosts typi-
cally communicate with servers at speciﬁed address-port combina-
tions. In our experience, the granularity of this equivalence group
is coarse enough to admit enough sessions in each group to form
statistically signiﬁcant models. Moreover, it is ﬁne enough so that
it does not approach the generality of more coarse (and potentially
more inaccurate) equivalences such as treating all sessions destined
for the same port as the same protocol—the very assumption that
we argue is losing traction with today’s protocols.
Augmenting Equivalence Groups with Contact History. We
augment service key equivalence groups by making a real-world as-
sumption about the protocol persistence between an initiating host
and a responding port. In particular, we assume that within a short
time period, if an initiating host contacts multiple responders at the
same responder port, then the cells corresponding to those service
keys are using the same protocol. Thus, we keep a contact history
table that maps initiator-address/responder-port pairs to cells, and
merge under the following circumstance: whenever host A contacts
the responder at B : p, and contacts another responder at C : p,
Figure 5: The Cell framework. (a) Flows are mapped to ﬂow keys, stored in a hash table. Each ﬂow key points to a cell; the cells are
only lightly loaded and have not yet been promoted. (b) More ﬂows have been added, multiple ﬂow keys now point to the same cells.
The ﬁrst cells have been promoted for merging. (c) Cells have begun merging.
then we merge the cells corresponding to service keys B : p and
C : p. This approach is partly inspired by previous work such as
BLINC [12], although our application of external sources of equiv-
alence information is relatively mild and not used during the clas-
siﬁcation process.
Cell Promotion, Comparison, and Merging.
After insert-
ing sessions into their respective cells, we need to promote them in
preparation for clustering (Figure 5b). However, observing a single
session within a cell is insufﬁcient to accurately infer the underly-
ing protocol. Thus, we ﬁnd it useful to allow the cell to receive
sufﬁcient trafﬁc to construct a reasonable model. For our imple-
mentation, we set the promotion threshold to a minimum of 500
ﬂows (not sessions) per cell.
Finally, we perform clustering on the cells with the goal of form-
ing compact descriptions of the observed protocols. We currently
perform an agglomerative (bottom-up) clustering to construct a hi-
erarchy of cells, and build larger cells by iteratively merging the
closest pair of cells according to the classiﬁer Compare operation.
Summary. The Cell framework is a realization of the proto-
col inference approach described earlier, providing a modular plat-
form for evaluating various aspects of the trafﬁc classiﬁcation prob-
lem. Cell construction could beneﬁt from more elaborate schemes
of inferring equivalence groups. Moreover, the framework would
provide us the ﬂexibility to experiment with a variety of machine-
learning approaches outside of agglomerative clustering to merge
cells.
In the context of this paper, the framework allows us to ﬂexibly
evaluate the viability of product distributions, Markov processes,
and Common Substring Graphs as protocol models independently
of the schema for constructing equivalence groups or the clustering
algorithms used after construction.
7. EVALUATION
We implemented the cluster construction and ﬂow matching com-
ponents of the Cell framework in C++ using 3800 lines of code.
The CSGs were simultaneously developed in the Cell framework
and the Bro IDS [17] to allow for more ﬂexible testing of input
trafﬁc and because we anticipate using CSGs for other uses than
trafﬁc classiﬁcation. We ran all experiments on a dual Opteron 250
with 8 GB RAM running Linux 2.6.
We used three traces for our experiments, each representing dif-
ferent network locations and trafﬁc mixes. The “Cambridge” trace
includes all trafﬁc of the Computer Laboratory at the University
of Cambridge, UK, over a 24-hour period on November 23, 2003.
“Wireless” is a ﬁve-day trace of all trafﬁc on the wireless network
in the UCSD Computer Science and Engineering building starting
on April 17, 2006. Finally, the “Departmental” trace collects over
an hour of trafﬁc from a UCSD department backbone switch at
noon on May 23, 2006.
To obtain session data out of raw packets, we reassembled TCP
ﬂows and concatenated UDP datagrams using Bro. Session life-
times are well deﬁned for TCP through its various timeouts; for
UDP we used a timeout of 10 seconds. We chose this value for
two reasons: ﬁrst, it is the default setting that Bro uses, and second,
smaller timeouts translate into more sessions to analyze. Note that
erring on the early side only makes the classiﬁcation task harder
since we will pick up a mid-stream session as a novel one. Next
we ﬁltered out all ﬂows containing no payload (essentially failed
TCP handshakes) because we cannot classify them using a content-
based ﬂow classiﬁer.
We then used Ethereal 0.10.14 [1] as an oracle to provide a proto-
col label for each of the ﬂows in the trace. Additionally, we ﬁltered
any ﬂows that Ethereal could not identify because we want to com-
pare our classiﬁcations to a ground truth provided by an oracle.
Speciﬁcally, whenever Ethereal labeled a ﬂow generically as just
“TCP” or “UDP,” we ﬁltered it out of the trace. From the combined
traces, ﬂows labeled “TCP” comprised 1-6% over the three traces.
Flows labeled “UDP” comprised 5% of the Cambridge trafﬁc, 34%
of the Wireless trafﬁc, and 14% of the Departmental trafﬁc. We
attempt to classify excluded ﬂows for the Departmental trace in
Section 7.4.
After preprocessing, we stored the ﬁrst k bytes of each reassem-
bled ﬂow in a trace ready for consumption by the Cell classiﬁer.
For this paper we set k = 64, as was done by Haffner et al. [8].
7.1 CSG Parameterization
CSGs have four parameters: soft/hard maximum node limits,
eviction weight threshold, and minimum string length. We used
a soft/hard node limit of 200/500 nodes, a minimum weight thresh-
old of 10%, and 4-byte minimum string length. To validate that
these are reasonable settings, we selected four major TCP protocols
(FTP, SMTP, HTTP, HTTPS) and four UDP protocols (DNS, NTP,
NetBIOS Name service, and SrvLoc). For each of them, picked a
destination service hosting at least 1000 sessions. We then manu-
ally inspected the services’ trafﬁc to ensure we did indeed deal with
the intended protocol. In three separate runs with minimum string
lengths of 2-4 bytes, eight CSGs were loaded with each session’s
ﬁrst message while we recorded node growth and usage. We have
found that in no case was the hard limit insufﬁcient and the soft
limit was violated only by HTTP and NTP. We also measured the
frequency distribution of each CSG’s nodes after 1000 insertions.
In all CSGs except for the FTP one, at least 75% of the 200 nodes
carry only a single path. The FTP CSG only grew to 11 nodes in
the 2-byte run, explaining the cruder distribution. Minimum string
length seems to matter little. Thus, our CSG settings seem tolerant
enough not to hinder natural graph evolution.
7.2 Classiﬁcation Experiment
In our classiﬁcation experiment, we examine how effective our
Product
Markov
CSG
total
1.68%
3.33%
2.08%
Cambridge
learned
0.50%
2.15%
0.90%
unlearned
total
1.18% 1.78%
1.18% 4.26%
1.18% 4.72%
Wireless
learned
1.28%
3.75%
4.21%
unlearned
total
0.51% 4.15%
0.51% 9.97%
0.51% 6.19%
Departmental
learned
3.03%
8.85%
5.06%
unlearned
1.12%
1.12%
1.12%
Table 1: Misclassiﬁcation for the three protocol models over the Cambridge, Wireless, and Departmental traces.
Protocol
Product
Markov
 DNS
HTTP
NBNS
NTP
SSH
 DNS
HTTP
NBNS
NTP
SSH
 DNS
HTTP
NBNS
NTP
SSH
%
26.28
12.24
44.89
5.29
0.22
23.14
0.67
6.94
0.57
0.44
54.78
9.17
7.03
6.70
0.08
Err.% Prec.% Rec.% Err.% Prec.% Rec.% Err.% Prec.% Rec.%
99.52
0.09
99.99
0.07
99.99
0.35
0.00
95.65
100.00
0.14
97.59
0.04
99.38
0.27
99.97
0.00
48.76
0.01
0.17
100.00
99.15
0.26
97.19
0.38
99.45
0.01
96.58
0.02
0.08
82.01
99.94
100.00
100.00
100.00
68.39
99.88
76.02
100.00
99.95
75.28
99.90
97.46
100.00
99.99
68.81
97.89
100.00
99.82
99.96
17.39
98.88
90.68
78.06
100.00
99.63
97.13
97.21
85.66
78.07
0.00
99.78
99.99
99.25
100.00
100.00
99.93
97.54
100.00
99.72
100.00
99.95
99.62
99.81
99.94
81.82
99.97
99.98
99.31
77.84
100.00
99.99
99.93
100.00
11.29
100.00
99.98
99.72
99.81
29.61
0.00
0.61
0.09
0.40
1.19
1.10
0.29
0.09
1.96
0.51
0.00
1.90
0.33
1.25
5.39
0.09
0.45
0.74
0.17
0.25
0.05