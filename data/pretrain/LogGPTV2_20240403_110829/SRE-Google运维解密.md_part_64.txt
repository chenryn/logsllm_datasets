可用的存储空间和存储成本，以及产品的价格和市场定位等。常见的软删除时间是15、
长度，尤其是在有很多短期数据的情况下，取决于某个组织的政策和相关的法律条文、
软删除还意味着一旦数据被标记为已删除，
现软删除机制的需求就非常明显了。
当我们将用户误删除和账号劫持两个场景结合起来，在应用程序层内，或者其下某层实
另外一个常见的数据误删除场景是由于账号被劫持造成的。在这个场景中，劫持用户账
当更新速度很快，同时隐私很重要的时候，大规模数据丢失和损坏通常是由应用程序的
第一层：软删除
注7
当阅读到这条建议时，读者可能会问：既然我们需要在数据存储之上提供一套API来实现软删除功能
Google SRE保障数据完整性的手段
，在某段时间以后会真的被删除。这个时间的
305
351
<350
---
## Page 348
306
最重要的是恢复。对“恢复”提供支持。应该是主导备份系统设计的关键。
备份和数据恢复是软删除之后的第二道防线。这一层中最重要的理念是：备份不重要；
第二层：备份和相关的恢复方法
而不是仅仅修改历史记录中的一条记录。为了针对这些场景提供合理的防护，我们也需
些历史记录功能的实现中，将删除作为一个特例处理，要求之前的历史状态也要被删除，
对Google来说，历史记录功能在恢复某些特定数据损坏场景下比较有用，但是在恢复
决于具体实现，可以用来替代软删除机制。
用这种功能时，这其实就是回收站机制的一个变种。当这种功能针对开发者可用时，取
历史记录功能属于哪种？有些产品允许将某个对象恢复到之前某个状态。当用户直接可
最后，针对第一层防线的总结：
甚至在某些需要保障已删除数据的销毁时间的系统中不可行（例如某些隐私敏感的程序）。
有的策略中都适用：在一个短期数据很多的系统中保存长期懒删除数据可能成本会很高，
用程序不可用，但是由云计算服务提供商保留几周时间再彻底销毁。懒删除不一定在所
序或者服务直接控制的）。在懒删除场景下，由某个云计算应用程序删除的数据马上对应
以将“懒删除”认为是某种幕后清理机制，由存储系统控制（与此相比，软删除是由程
针对这种场景，有时候可能会再增加一层软删除机制，我们称之为“懒删除”机制。可
造成的数据丢失了。
有比较合理的默认值，剩下的误删除场景就是由内部开发者或者是外部开发者错误操作
API类型的服务怎么办呢？假设该项服务已经提供了可编程的软删除和还原机制，同时
面向直接用户的Gmail或者Google Drive的删除功能都采用了软删除策略，但是云计算
要将懒删除与／或软删除机制应用于历史记录上。
大部分数据丢失的场景中（包括误删除、人工以及程序化都算）都没有作用。因为在某
上才能起作用。
和误删除场景中最有效的机制。
额外难题的严重性。虽然还有很多删除保护机制值得一提，但是纵观全局，软删除机制是在防范Bug
及默认的软删除期限。实践证明，在多起事故中，软删除机制都帮助用户避免或者降低了数据丢失的
例：Blobstore的API实现了很多安全功能，
·在面向开发者的服务中，懒删除机制是针对内部开发者错误的主要防范手段，是
）软删除机制是针对开发者错误的主要防范手段，以及用户错误的次要防范手段。
）应用中实现一个回收站机制，作为用户错误的主要防护手段
针对外部开发者错误的次要防范手段。
第26章数据完整性：读写一致
，包括默认的备份策略（离线副本），
端到端校验机制，
以
---
## Page 349
型之前，如何确定合理的备份保存周期呢？Google将大部分服务的备份周期定于30~90
本章后面“第三层：早期预警”一节中描述的策略都是为了加速检测应用程序Bug造成
这需要同时操作多个备份。但是，这恰恰是这种低等数据损坏和丢失场景中最需要的。
需要的恢复时间最长，因为这些Bug可能在引入之后几个月才被发现。这样的场景意味
Google的经验证实，应用程序Bug造成的低等数据损坏，或者应用程序内部的删除Bug
时间也会提高（注意，这里也会受边际效应的影响）。
数据中心的分布式存储上。这样一个策略本身并不能保护单个部署点的故障，所以这些
本地化。Google通常只会将那些恢复很快，但是成本很高的“快照”存储很短的一段时
在此之外，故障恢复的时间很重要。所需的故障恢复时间越短，备份存放的位置越需要
这种压力，我们需要在非峰值时间段进行全量备份，同时在繁忙时间段进行增量备份。
服务用户的数据存储造成很大的计算压力，以至于影响服务的扩展性和性能。为了缓解
就算不考虑成本问题，频繁地进行全量备份也是非常昂贵的。最主要的是，这会给线上
实时的流式备份系统。
增量备份策略就越重要。在Google的一个极端的例子中，旧版的Gmail采用了一个准
另外一个关键问题是：在一次数据恢复中能够损失多少最近数据。能够损失的数据越少，
由此可见，在设计备份还原系统时，必须考虑：
障落人这个区间。
天之间。每个服务必须通过针对数据丢失的容忍度设计和早期预警系统的研发投入来保
的数据问题，以便于减少需要这种复杂恢复场景的需求。然而，我们在无法预知问题类
使用了。更重要的是，试图恢复不同部分的数据到不同的时间点可能是不可能的，因为
但是，在高速更新的开发环境中，代码和数据格式的改变可能使得旧的备份很快就无法
着我们需要能够恢复得越久越好。
备份应该保存多久呢？备份策略随着保存时间的增长，成本会上升，但是同时可恢复的
备份经常会在一段时间后被转移到其他离线存储上，从而给更新的备份腾出地方。
·备份的保留时间。
·使用哪种备份和还原方法。
用
备份的存储位置。
通过全量或者增量备份建立恢复点（restore point）的频率。
“copy-on-write"
技术实现，
Google SRE保障数据完整性的手段
307
353
---
## Page 350
308
注9有关GFS类型的复制信息可参见文献[Ghe03]。有关Reed-Solomon纠错码机制的更多信息，可参见
持续使用的系统。
就很罕见的数据恢复过程来测试。正确的做法是，选择一个非常常见，并且被许多用户
当选择余系统时，不要选择一种不常用的系统。
将备份文件放置在某种冗余机制之上，例如 RAID、Reed-Solomon 纠错码，或者 GFS 类
备份存放在不同部署点上是更合理的，这样每个部署点不会同时出现故障。同时，应该
随着数据量的增长，不是每个存储系统都可以进行复制。在这种情况下，将不同层级的
份文件的那个数据中心正在进行维护。
否则，就有可能在数据恢复过程中，才发现备份文件自身丢失了数据，或者发现保存备
在理想世界中，每个存储实例，包括那些保存了备份的存储系统都应该具有复制机制。
额外一层：复制机制
系统线上数据容量竞争。所以，后续层级的备份数据通常产生的频率很低，但是可以保
在不同的层级中转移大量数据是很昂贵的，但是后续层级的数据存储容量并不会与生产
或者是Bug造成的分布式文件系统的数据损坏。
还是磁盘）。这些级别的备份可以保护单个部署点级别的故障，例如数据中心电源故障，
接下来的级别会使用冷存储，包括固定的磁带库以及异地备份存储设施（不管是磁带，
般是一周两次，那么保存一周到两周再删除这种备份可能是合理的。
应用程序Bug，以至于无法使用上一级别的备份恢复的问题。如果新版本的发布时间一
份的技术栈自身的问题提供保护。这一级别的备份也可以用来针对那些检测时间太晚的
所使用的数据存储出现的相关问题提供额外保护的，但是要注意，这无法为用来保存备
署点的随机读写分布式文件系统上。这些备份可能需要数个小时来备份，是用来为服务
第二级备份的频率较低，只保留一位数或者两位数字的天数。第二级备份保存在当前部
对较高，
储技术。这样可以为大部分的软件 Bug，以及开发者错误的场景提供保护。由于成本相
备份保存在与线上数据存储距离最近的地方，使用与数据存储相同或者是非常相似的存
景都需要分级备份。第一级备份是那些备份频率很高，并且可以快速恢复的备份。这些
将以上针对24种组合的预防建议总结得出：以合理成本满足一系列广泛的数据恢复场
型的复制机制之上。
存的时间更长。
https://en.wikipedia.org/wiki/Reed-Solomon_error_correction
，备份在这一层只会保留数小时，最多10天之内，恢复时间在几分钟内。
第26章
数据完整性：读写一致
注9
，因为这种系统的可靠性只能通过本身
---
## Page 351
时间降低几个数量级，这样恢复策略才能有效工作。
通过水平分割负载，同时按时间维度进一步限制垂直数据量，我们可以将80年的处理
设计部署阶段预先进行一些考量，以便于：
片，可以将N个任务并行进行，每个任务可以负责复制和校验1/N的数据。这样需要在
另外一个降低复制和校验任务的时间总量的方法是分布式计算。如果我们将数据合理分
算上的成本。
的备份。每一个相关性很强的备份都会增加故障发生的风险，这还没有考虑到后勤和计
然后每天进行增量备份。完全恢复一份数据就需要顺序处理大概1000个相关性非常强
然而，上文说过，我们最关注的是恢复，而不是备份。假设三年前进行了一次全量备份
这种技术可以将备份时间缩减到与主要处理逻辑的吞吐量在一个数量级内。这样的话
下来，我们可以进行增量备份，其中仅仅包含自从上次备份后新增或者更改过的数据。
者交易记录已经是不会再变的了，就可以校验并且进行合适的复制，以便未来恢复。接
处理海量数据，最重要也最有效的方式是给数据建立一个“可信点”一
80年以前的了。显而易见，这个策略需要重新设计。
会需要将近一个世纪的时间来恢复这些数据，别忘了这些数据在恢复的时候已经是至少
间。备份的恢复时间，再考虑到一些后续处理时间则会需要更长。这样看来，我们可能
需要80年。而进行几次全量复制，就算我们有足够的存储媒介也至少需要同样长的时
的SATA2.0接口（300MB/s的性能），仅仅是遍历一遍所有数据进行最基本的校验也将
现在，我们用同样的策略来校验700Petabytes的结构化数据。就算我们使用一个理想化
可能还需要有足够的存储空间放置数据的几个副本。
般来说，我们只需要找到足够的机器资源，遍历全部数据，执行某种校验逻辑就可以了，
有足够的数据格式信息，了解数据的交易模式等，这个过程并不会有特别多的难点。一
个GB的结构化数据上进行校验、复制和进行端到端测试可能有些挑战。但是只要我们
针对T级别（Terabytes）的流程和手段在E级别（Exabytes）并不能工作得很好。在儿
经常性的增量备份就可以替代需要80年的巨型校验和复制任务了。
1TvS.1E：存储更多数据没那么简单
保证每个分片之间的独立性。
●正确平衡数据分片。
）避免相邻并行任务之间的资源抢占。
GoogleSRE保障数据完整性的手段
一也就是一部分
355
---
## Page 352
356
310
正确的：虽然我们相信存储系统的实现，但是还是要校验！
的经验再次证明）。不检查受影响的应用程序的话，没有任何办法可以100%确定数据是
经验证明的确如此），那么对采用最终一致性的Bigtable等实现来说就更适用了（Google
这种无法预知的不一致性的影响就越严重。如果这个逻辑对Paxos实现适用（Google的
个具体的数据中心时，可能会有未定义的行为发生。应用程序的部署范围越大，那么受
虽然分布式共识性算法理论上无懈可击，但是具体的实现经常充满了Hack、优化、Bug
完整性了。于是，开发者通常将所有应用数据合并到单个确保分布式一致性的存储方案中，
来保障（例如，Paxos，见第23章）。开发者认为选择合适的API就足以保障应用的数据
Megastore），也就是将应用程序数据一致性的问题交给该API使用的分布式共识性算法
如果不主动花费一定的精力在数据关系的管理上，任何一个不断增长的服务的数据质量
在高创新速度的环境中，云计算应用程序和基础设施服务面临着很多数据完整性的挑战，
随着时间大幅下降。越早检测到数据丢失，数据的恢复就越容易，也越完整。
持续下降。关联性的交易和潜在的数据格式变化使得从某个指定备份中还原数据的能力
损坏的数据的引用可能会被复制好几份。随着每次更新，数据存储中数据的整体质量会
“脏”数据并不会一直静止不动，它们会在整个系统中传播。针对不存在的或者是已经
第三层：早期预警
物理机器出现某个特定故障时（可能是暂时的），符合某种特定的时间，以及出现在某
见文献[Cha07]）。Paxos应该等待多长时间再忽略一个失去响应的节点？当某个具体的