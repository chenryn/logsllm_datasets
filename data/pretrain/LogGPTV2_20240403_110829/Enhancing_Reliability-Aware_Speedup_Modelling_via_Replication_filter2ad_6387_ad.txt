estimate Hrep(P ∗
2) For the P ∗
norep).
Figure 9 compares Hrep(P ∗
norep found in step 1, we use our simulator to
norep) obtained using the steps
above versus the expression for Hnorep(P ∗
norep), for the two
cases of parallelism in the workload (i.e. perfectly parallel and
α > 0). As can be seen in the ﬁgure, with other parameters
being the same, replication always has lower normalized
expected completion time than no-replication. Remember that
for perfectly parallel jobs (α = 0), our theoretical analysis
was carried out with the assumption that R = D = 0. While
that analysis is validated by the ﬁgure, we also see that the
difference in the performance of replication and no-replication
is even greater for the α = 0 case when R and D are non-
zero. Hence, the results from our empirical analysis further
strengthen our conclusion that replication outperforms no-
replication at the optimal processor counts for no replication.
Based on the theoretical and empirical results in this
section, we can conclude that, for all practical values of
the platform parameters, by the time the optimal number
of processors for no-replication is reached, replication al-
ready offers better performance. Thus, the global speedup
(= min(Hnorep(P ), Hrep(P ))) is monotonic with respect
to the number of processors up until the optimal count of
processors for replication is reached. Furthermore, the global
optimum is achieved at processor counts that are optimal for
replication.
V. OVERHEAD OF REPLICATION
We had so far assumed that the only hit to replication is the
doubling of failure free execution time of the parallel part of
a job, since replication duplicates work on half the system
nodes. However, as several prior studies on replication [4]
[13] [14] [15] have noted, replication also induces an addi-
tional overhead to message passing applications because of
several factors, such as the additional communication required
between replicas in order maintain consistency among them,
increase in memory utilization and network congestion. In
this section we assess how this additional overhead affects
the reliability aware speedup of replication, in contrast to no-
replication which does not suffer from any such overheads.
For the cost of replication, we use the model of [4].
They analyzed the overhead of replication on several message
passing applications and used curve ﬁtting to infer that the
growth of the overhead of replication is proportional to the
logarithm of the number of processors, P . Thus, we will
assume that the extra overhead induced by replication, as
a fraction of the original time, is equal to δ log P , where
δ is an application speciﬁc constant. To obtain a formula
for the speedup, assume the original work to be completed
takes W units of time on a single processor without fail-
ures and fault tolerance. Without the additional overhead of
replication, the same time, using P total processors, would
become W (α + 2(1 − α)/P ) in the absence of failures and
without C/R. With the additional overhead of replication, this
time would become W (α + 2(1 − α)/P )(1 + δ log P ). The
failure free speedup can then be obtained as Srep(P ) =
(α+2(1−α)/P )(1+δ log P ). Thus,
W (α+2(1−α)/P )(1+δ log P ) =
we update Hrep(P ) to use this expression for Srep(P ) in order
to determine the reliability-aware speedup that also factors in
the cost of replication.
W
1
Figure 10 shows the comparison of the normalized expected
completion time of no-replication with replication for different
values of δ. For no-replication, the curve is generated through
the exact expression for Hnorep(P ). For replication, we evalu-
ate Hrep(P ) using our simulator as before except that Srep(P )
uses the updated expression with δ as described above. We
−2 or lower,
can see from the ﬁgure that, for values of δ = 10
replication still outperforms no replication before no replica-
tion reaches its optimal. It should also be mentioned that the
value of δ determined in [4] for the application with the highest
−3. Moreover, this
overhead of replication was of the order 10
value could be even lowered by leveraging application speciﬁc
properties of most message passing applications [16]. Thus
we can say that the overhead of replication, as long as it
is not unreasonably high, does not change the form of the
global speedup function, which would still behave according
to scenario (a) in Figure 8.
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
536
Fig. 9. The normalized expected completion time, H(P ∗
norep) of no replication and replication. For both ﬁgures, C = 300 seconds.
VI. WEIBULL DISTRIBUTION
Thus far in this paper, the failure distribution of the indi-
vidual processors was assumed to be exponential, because that
makes the theoretical development, especially the derivation of
MTTI as carried out in [5], tractable. However, several studies
[17] [18] on real world HPC systems have noted that the
closest ﬁt to actual failure data is given by Weibull distribution.
In this section we use our simulator to assess whether our
conclusions in this paper also apply to Weibull distributions.
The Weibull distribution, in addition to the parameter λ, also
has a shape parameter m > 0. The Cumulative Distribution
Function (CDF) of Weibull distribution is deﬁned as P (X ≤
t) = 1− e−(λt)
m. In practice, the value for m is usually found
to be between 0.5 and 0.7 [19] [18] [20]. With dual replication,
we deﬁne the reliability R(t) as the probability that all P/2
pairs of processor-replicas survive until time t. The probability
of survival, until time t, of a pair of replicas is obtained by
subtracting from 1 the probability that both replica-processors
fail within time t. Under Weibull failure distribution, this will
be given by 1 − (1 − e−(λt)
m. We
can thus write the overall reliability function as
P/2
)2 = 2e−(λt)
m − e−2(λt)
m
R(t) = (2e−(λt)
m − e−2(λt)
m
)
(21)
The mean of non-negative random variables is equal to the
integral of the reliability function. This means that the MTTI
is given by
M T T I =
R(t)dt =
(2e−(λt)
m − e−2(λt)
m
P/2dt
)
(22)
A closed form formula for the above is not known ex-
cept for the case of m = 1, which reduces to the expo-
nential distribution. We, therefore, numerically compute the
√
above integral in order to determine the checkpoint interval
2 × C × M T T I). The simulator then generates interrupts
(
based on the distribution deﬁned by Equation 21 above. We
compare the resulting completion time with no-replication for
which the interrupts are simply generated according to the
reliability function R(t) = e−(λt)
mP . The results, accounting
for the overhead of replication, are shown in Figure 11. We
observe that the optimal processor counts using Weibull failure
(cid:9) ∞
0
(cid:9) ∞
0
Fig. 10. Normalized expected completion time versus the number of proces-
sors. Node MTBF = 10 years, while C = R = D = 300 seconds.
We can also see from Figure 10 that the performance of
replication at its optimal is superior to the optimal performance
of no replication for all values of δ. This means that
if
the replication overhead grows large enough,
the form of
the global speedup function shifts from that in scenario (a)
to that in scenario (b) in Figure 8. We did observe in our
investigations that further increasing δ does cause the speedup
to behave as in scenario (c) in Figure 8, which would render
replication infeasible at any processor count. However, this
behavior is only exhibited when unreasonably high values for
the parameter δ are assumed. Finally, we note from Figure 10
(right) that, when α > 0, the crossover between no-replication
and replication happens much closer to the optimal processor
count of no-replication. However, all the observations made
above regarding the form of the overall global speedup hold
for both cases, i.e. α = 0 and α > 0.
We can thus conclude from this section that, while impact
of the overhead of replication is to diminish the reliability-
aware speedup of replication, it does not change any of the
conclusions in this paper about how replication fares against
no-replication with scale. Only if the overhead is impracti-
cally high, replication may be rendered suboptimal at any
processor count in comparison with the optimal performance
of no-replication. However, for overhead numbers observed
for replication in practice, the reliability-aware speedup still
outperforms no-replication at the optimal processor counts of
no-replication, and can signiﬁcantly improve on the optimal
performance that is possible without using replication.
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
537
works investigating both the implementation [13] [16] and
theoretical [21] issues surrounding replication. These works
do not consider the problem of ﬁnding the optimal number
of processors using replication, with the exception of [21].
However, [21] focuses primarily on silent errors because of
which their model considers the failure of even one processor
(silent or fail-stop) in a replica-pair as a failure for the entire
job. Thus,
their results are not applicable to the fail-stop
model in which only the failure of both processors in the
replica-pair causes a failure to the job, which is what we
study in this paper. Additionally, none of the above works
on replication assess how replication can impact the overall
form of reliability-aware speedups, which we do in this paper
by showing that replication doesn’t simply start outperforming
no-replication after some system scales, but rather outperforms
the optimal achievable by no-replication and that the crossover
happens before or close to the optimal system scale for no
replication. Thus, our work adds to the theoretical line of
work on replication by providing novel results on the the
optimal processor counts for dual replication as well as on
the processor counts around which replication outperforms no-
replication.
VIII. CONCLUSION
In this paper, we studied the reliability-aware speedup of
a replicated execution, and contrasted it with the reliability-
aware speedup without replication. We derived novel results
on how the optimal processor counts of replication and no-
replication relate to the individual node failure rate λ. We
further showed that replication generally starts outperforming
no-replication before or close to the point where no-replication
reaches its optimal processor counts. Taken collectively, the
results in this paper indicate that replication signiﬁcantly
enhances reliability-aware speedup beyond what is possible
without replication.
There are several directions of future work in this area. One
direction would be to analyze higher degrees of replication,
e.g. triple redundancy, to see if they can take us further than the
optimal performance achievable by dual replication. Another
interesting direction to explore would be platforms where node
failure rates are not identical. The analysis on such platforms
would require further generalization of the results in this paper.
ACKNOWLEDGMENT
We are thankful to reviewers and our shepherd, Devesh
Tiwari, for helping us improve the quality of this paper. This
research is based in part upon work supported by the Depart-
ment of Energy under contract DE-SC0014376. This research
was supported in part by the University of Pittsburgh Center
for Research Computing through the resources provided. This
work used the Extreme Science and Engineering Discovery
Environment (XSEDE), which is supported by National Sci-
ence Foundation grant number OCI-1053575. Speciﬁcally, it
used the Bridges system, which is supported by NSF award
number ACI-1445606, at the Pittsburgh Supercomputing Cen-
ter (PSC).
Fig. 11. Normalized expected completion time versus the number of pro-
cessors, when node failure distribution is Weibull. Node MTBF ≈ 15 years.
Weibull shape parameter m = 0.6 (we observed similar results with other
values of m between 0.5 and 0.7. The other parameters are the same as in
Figure 10.
distribution are in general lower than the counts with exponen-
tial failures. Comparing replication and no-replication, we see
that, similar to Figure 10, the optimal number of processors
for replication is much higher than no-replication. Further-
more, replication’s optimal speedup is superior to the optimal
achievable by no-replication. The crossover in performance
also happens at almost the same point at which we reach the
optimal processor counts of no-replication. This conﬁrms that,
even with Weibull failure distributions, the overall speedup
proﬁle follows the form depicted in scenario (a) in Figure
8, assuming practical values for the overhead of replication.
Therefore, we see that the general conclusions made in this
paper using exponential distributions, i.e. replication has much
higher optimal processor counts than no-replication and that
the optimal speedup of replication is superior to that possible
without replication, hold for Weibull failure distributions as
well.
VII. RELATED WORK
The most closely related body of work to this paper is the
study of reliability-aware speedups. In that domain, [2] and [3]
formulated reliability aware speedups for checkpoint-restart
(C/R) and [3] numerically computed the optimal number of
processors. [1] provided theoretical results on the optimal
number of processors for non-perfectly parallel
jobs. We
follow their approach in formulating the optimal processor
count problem, and also extend their results to cover the case
of perfectly parallel jobs. All of these works, however, consider
C/R only without replication and do not address the impact
of replication on reliability-aware speedups, which is the main
focus of our paper.
There have been several studies in the HPC domain that
have studied the idea of combining replication with C/R.
[4] suggested replication as a viable fault tolerance scheme
for exascale HPC systems by showing that, at sufﬁciently
large scales, C/R alone will be less efﬁcient that C/R with
replication. [12] theoretically studied replication and derived
a summation based formula for the mean-time-to-interrupt
(MTTI) of a replicated execution. The authors in [5] re-
cently derived a closed form expression for the MTTI in
the case of dual replication. There have been several other
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
538
REFERENCES
and checkpointing,” Journal of Parallel and Distributed Computing, vol.
122, pp. 209–225, 2018.
[1] A. Cavelan, J. Li, Y. Robert, and H. Sun, “When amdahl meets
young/daly,” in 2016 IEEE International Conference on Cluster Com-
puting (CLUSTER).
IEEE, 2016, pp. 203–212.
[2] H. Jin, Y. Chen, H. Zhu, and X.-H. Sun, “Optimizing hpc fault-tolerant
environment: An analytical approach,” in 2010 39th International Con-
ference on Parallel Processing.
IEEE, 2010, pp. 525–534.
[3] Z. Zheng, L. Yu, and Z. Lan, “Reliability-aware speedup models
for parallel applications with coordinated checkpointing/restart,” IEEE
Transactions on Computers, vol. 64, no. 5, pp. 1402–1415, 2015.
[4] K. Ferreira, J. Stearley, J. H. Laros III, R. Oldﬁeld, K. Pedretti,
R. Brightwell, R. Riesen, P. G. Bridges, and D. Arnold, “Evaluating
the viability of process replication reliability for exascale systems,” in
Proceedings of 2011 International Conference for High Performance
Computing, Networking, Storage and Analysis. ACM, 2011, p. 44.
[5] A. Benoit, T. H´erault, V. L. F`evre, and Y. Robert, “Replication is more
efﬁcient than you think,” in Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis.
ACM, 2019, p. 89.
[6] J. W. Young, “A ﬁrst order approximation to the optimum checkpoint
interval,” Communications of the ACM, vol. 17, no. 9, pp. 530–531,
1974.
[7] G. M. Amdahl, “Validity of the single processor approach to achieving
large scale computing capabilities,” in Proceedings of the April 18-20,
1967, spring joint computer conference. ACM, 1967, pp. 483–485.
[8] J. T. Daly, “A higher order estimate of the optimum checkpoint interval
for restart dumps,” Future generation computer systems, vol. 22, no. 3,
pp. 303–312, 2006.
[9] Y. Liu, R. Nassar, C. Leangsuksun, N. Naksinehaboon, M. Paun, and
S. L. Scott, “An optimal checkpoint/restart model for a large scale high
performance computing system,” in 2008 IEEE International Symposium
on Parallel and Distributed Processing.
IEEE, 2008, pp. 1–9.
[10] O. Subasi, G. Kestor, and S. Krishnamoorthy, “Toward a general
theory of optimal checkpoint placement,” in 2017 IEEE International
Conference on Cluster Computing (CLUSTER).
IEEE, 2017, pp. 464–
474.
[11] A. Agrawal, G. H. Loh, and J. Tuck, “Leveraging near data processing
for high-performance checkpoint/restart,” in Proceedings of the Inter-
national Conference for High Performance Computing, Networking,
Storage and Analysis. ACM, 2017, p. 60.
[12] H. Casanova, Y. Robert, F. Vivien, and D. Zaidouni, “Combining process
replication and checkpointing for resilience on exascale systems,” Ph.D.
dissertation, INRIA, 2012.
[13] C. Engelmann and S. B¨ohm, “Redundant execution of hpc applications
with mr-mpi.”
[14] J. Elliott, K. Kharbas, D. Fiala, F. Mueller, K. Ferreira, and C. Engel-
mann, “Combining partial redundancy and checkpointing for hpc,” in
2012 IEEE 32nd International Conference on Distributed Computing
Systems.
IEEE, 2012, pp. 615–626.
[15] Z. Hussain, T. Znati, and R. Melhem, “Partial redundancy in hpc systems
with non-uniform node reliabilities,” in SC18: International Conference
for High Performance Computing, Networking, Storage and Analysis.
IEEE, 2018, pp. 566–576.
[16] A. Lefray, T. Ropars, and A. Schiper, “Replication for send-deterministic
mpi hpc applications,” in Proceedings of the 3rd Workshop on Fault-
tolerance for HPC at extreme scale. ACM, 2013, pp. 33–40.
[17] N. Raju, Y. L. Gottumukkala, C. B. Leangsuksun, R. Nassar, and
S. Scott, “Reliability analysis in hpc clusters,” in Proceedings of the High
Availability and Performance Computing Workshop, 2006, pp. 673–684.
[18] B. Schroeder and G. Gibson, “A large-scale study of failures in high-
performance computing systems,” IEEE transactions on Dependable and
Secure Computing, vol. 7, no. 4, pp. 337–350, 2009.
[19] T. Herault and Y. Robert, Fault-tolerance techniques
for high-
performance computing. Springer, 2015.
[20] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai,
D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux et al., “Understand-
ing gpu errors on large-scale hpc systems and the implications for system
design and operation,” in 2015 IEEE 21st International Symposium on
High Performance Computer Architecture (HPCA).
IEEE, 2015, pp.
331–342.
[21] A. Benoit, A. Cavelan, F. Cappello, P. Raghavan, Y. Robert, and H. Sun,
“Coping with silent and fail-stop errors at scale by combining replication
Authorized licensed use limited to: University of New South Wales. Downloaded on October 01,2020 at 13:22:32 UTC from IEEE Xplore.  Restrictions apply. 
539