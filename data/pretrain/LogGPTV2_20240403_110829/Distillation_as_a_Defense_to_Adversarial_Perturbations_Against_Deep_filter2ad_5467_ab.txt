oracle requiring the adversary to approximatively replicate the
model. Note that we do not consider attacks at training time
in this paper and leave such considerations to future work.
B. Adversarial Sample Crafting
We now describe precisely how adversarial sample are
crafted by adversaries. The general framework we introduce
builds on previous attack approaches and is split in two folds:
direction sensitivity estimation and perturbation selection. At-
tacks holding in this framework correspond to adversaries with
diverse goals, including the goal of misclassifying samples
from a speciﬁc source class into a distinct target class. This
is one of the strongest adversarial goals for attacks targeting
classiﬁers at test time and several other goals can be achieved
if the adversary has the capability of achieving this goal. More
speciﬁcally, consider a sample X and a trained DNN resulting
in a classiﬁer model F . The goal of the adversary is to produce
= X + δX by adding a perturbation
an adversarial sample X
∗ (cid:3)= F (X)
δX to sample X, such that F (X
is the adversarial target output taking the form of an indicator
vector for the target class [7].
∗ where Y
) = Y
∗
∗
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
Neural Network 
Architecture
1
X
F
Direction 
Sensitivity 
Estimation
Legitimate input 
classiﬁed as “1”
 by a DNN
F (X) = 1
2
Perturbation 
Selection
δX
F
Misclassiﬁcation
Check for:
F (X + δX) = 4
no
δX+XX←
Neural Network 
Architecture
X
∗
δX+X=
yes
Adversarial Sample
misclassiﬁed as “4” 
by a DNN
) = 4
F (X
∗
Fig. 3: Adversarial crafting framework: Existing algorithms for adversarial sample crafting [7], [9] are a succession of two
steps: (1) direction sensitivity estimation and (2) perturbation selection. Step (1) evaluates the sensitivity of model F at the
input point corresponding to sample X. Step (2) uses this knowledge to select a perturbation affecting sample X’s classiﬁcation.
If the resulting sample X + δX is misclassiﬁed by model F in the adversarial target class (here 4) instead of the original class
(here 1), an adversarial sample X
∗ has been found. If not, the steps can be repeated on updated input X ← X + δX.
As several approaches at adversarial sample crafting have
been proposed in previous work, we now construct a frame-
work that encompasses these approaches, for future work
to build on. This allows us to compare the strengths and
weaknesses of each method. The resulting crafting framework
is illustrated in Figure 3. Broadly speaking, an adversary starts
by considering a legitimate sample X. We assume that the
adversary has the capability of accessing parameters θF of his
targeted model F or of replicating a similar DNN architecture
(since adversarial samples are transferable between DNNs) and
therefore has access to its parameter values. The adversarial
sample crafting is then a two-step process:
1) Direction Sensitivity Estimation: evaluate the sensitivity
of class change to each input feature
2) Perturbation Selection: use the sensitivity information to
select a perturbation δX among the input dimensions
In other terms, step (1) identiﬁes directions in the data man-
ifold around sample X in which the model F learned by the
DNN is most sensitive and will likely result in a class change,
while step (2) exploits this knowledge to ﬁnd an effective
adversarial perturbation. Both steps are repeated if necessary,
by replacing X with X+δX before starting each new iteration,
until the sample satisﬁes the adversarial goal: it is classiﬁed
by deep neural networks in the target class speciﬁed by the
∗. Note that, as
adversary using a class indicator vector Y
mentioned previously, it is important for the total perturbation
used to craft an adversarial sample from a legitimate sample
to be minimized, at least approximatively. This is essential for
adversarial samples to remain undetected, notably by humans.
Crafting adversarial samples using large perturbations would
be trivial. Therefore, if one deﬁnes a norm (cid:5) · (cid:5) appropriate
to describe differences between points in the input domain of
DNN model F , adversarial samples can be formalized as a
solution to the following optimization problem:
(cid:5)δX(cid:5) s.t. F (X + δX) = Y
(1)
∗
arg min
δX
Most DNN models F will make this problem non-linear
and non-convex, making a closed-solution hard to ﬁnd in
most cases. We now describe in details our attack framework
approximating the solution to this optimization problem, using
previous work to illustrate each of the two steps.
Direction Sensitivity Estimation - This step considers
sample X, a M-dimensional input. The goal here is to ﬁnd
the dimensions of X that will produce the expected adversarial
behavior with the smallest perturbation. To achieve this, the
adversary must evaluate the sensitivity of the trained DNN
model F to changes made to input components of X. Building
such a knowledge of the network sensitivity can be done in
several ways. Goodfellow et al. [9] introduced the fast sign
gradient method that computes the gradient of the cost function
with respect
to the input of the neural network. Finding
sensitivities is then achieved by applying the cost function
to inputs labeled using adversarial target labels. Papernot et
al. [7] took a different approach and introduced the forward
derivative, which is the Jacobian of F , thus directly providing
gradients of the output components with respect to each input
component. Both approaches deﬁne the sensitivity of the
network for the given input X in each of its dimensions [7],
[9]. Miyato et al. [29] introduced another sensitivity estimation
measure, named the Local Distribution Smoothness, based on
the Kullback-Leibler divergence, a measure of the difference
between two probability distributions. To compute it, they use
an approximation of the network’s Hessian matrix. They how-
ever do not present any results on adversarial sample crafting,
but instead focus on using the local distribution smoothness
as a training regularizer improving the classiﬁcation accuracy.
Perturbation Selection - The adversary must now use this
knowledge about the network sensitivity to input variations
to evaluate which dimensions are most likely to produce the
target misclassiﬁcation with a minimum total perturbation
vector δX. Each of the two techniques takes a different
approach again here, depending on the distance metric used to
585585
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
evaluate what a minimum perturbation is. Goodfellow et al. [9]
choose to perturb all input dimensions by a small quantity
in the direction of the sign of the gradient they computed.
This effectively minimizes the Euclidian distance between
the original and the adversarial samples. Papernot et al. [7]
take a different approach and follow a more complex process
involving saliency maps to only select a limited number of
input dimensions to perturb. Saliency maps assign values to
combinations of input dimensions indicating whether they will
contribute to the adversarial goal or not if perturbed. This
effectively diminishes the number of input features perturbed
to craft samples. The amplitude of the perturbation added to
each input dimensions is a ﬁxed parameter in both approaches.
Depending on the input nature (images, malware,
...), one
method or the other is more suitable to guarantee the existence
of adversarial samples crafted using an acceptable perturbation
δX. An acceptable perturbation is deﬁned in terms of a
distance metric over the input dimensions (e.g., a L1, L2
norm). Depending on the problem nature, different metrics
apply and different perturbation shapes are acceptable or not.
C. About Neural Network Distillation
We describe here the approach to distillation introduced by
Hinton et al. [19]. Distillation is motivated by the end goal of
reducing the size of DNN architectures or ensembles of DNN
architectures, so as to reduce their computing ressource needs,
and in turn allow deployment on resource constrained devices
like smartphones. The general intuition behind the technique
is to extract class probability vectors produced by a ﬁrst DNN
or an ensemble of DNNs to train a second DNN of reduced
dimensionality without loss of accuracy.
This intuition is based on the fact that knowledge acquired
by DNNs during training is not only encoded in weight
parameters learned by the DNN but is also encoded in the
probability vectors produced by the network. Therefore, distil-
lation extracts class knowledge from these probability vectors
to transfer it into a different DNN architecture during training.
To perform this transfer, distillation labels inputs in the training
dataset of the second DNN using their classiﬁcation predic-
tions according to the ﬁrst DNN. The beneﬁt of using class
probabilities instead of hard labels is intuitive as probabilities
encode additional information about each class, in addition to
simply providing a sample’s correct class. Relative information
about classes can be deduced from this extra entropy.
To perform distillation, a large network whose output layer
is a softmax is ﬁrst trained on the original dataset as would
usually be done. An example of such a network is depicted in
Figure 1. A softmax layer is merely a layer that considers a
vector Z(X) of outputs produced by the last hidden layer of
a DNN, which are named logits, and normalizes them into a
probability vector F (X), the ouput of the DNN, assigning a
probability to each class of the dataset for input X. Within the
softmax layer, a given neuron corresponding to a class indexed
by i ∈ 0..N − 1 (where N is the number of classes) computes
component i of the following output vector F (X):
(cid:4)
(cid:2)
(cid:3)N−1
ezi(X)/T
l=0 ezl(X)/T
F (X) =
i∈0..N−1
(2)
586586
where Z(X) = z0(X), ..., zN−1(X) are the N logits corre-
sponding to the hidden layer outputs for each of the N classes
in the dataset, and T is a parameter named temperature and
shared across the softmax layer. Temperature plays a central
role in underlying phenomena of distillation as we show later
in this section. In the context of distillation, we refer to this
temperature as the distillation temperature. The only constraint
put on the training of this ﬁrst DNN is that a high temperature,
larger than 1, should be used in the softmax layer.
The high temperature forces the DNN to produce probability
vectors with relatively large values for each class. Indeed, at
high temperatures, logits in vector Z(X) become negligible
compared to temperature T . Therefore, all components of
probability vector F (X) expressed in Equation 2 converge to
1/N as T → ∞. The higher the temperature of a softmax is,
the more ambiguous its probability distribution will be (i.e. all
probabilities of the output F (X) are close to 1/N), whereas
the smaller the temperature of a softmax is, the more discrete
its probability distribution will be (i.e. only one probability in
output F (X) is close to 1 and the remainder are close to 0).
The probability vectors produced by the ﬁrst DNN are then
used to label the dataset. These new labels are called soft
labels as opposed to hard class labels. A second network with
less units is then trained using this newly labelled dataset.
Alternatively, the second network can also be trained using a
combination of the hard class labels and the probability vector
labels. This allows the network to beneﬁt from both labels
to converge towards an optimal solution. Again, the second
network is trained at a high softmax temperature identical to
the one used in the ﬁrst network. This second model, although
of smaller size, achieves comparable accuracy than the original
model but is less computationally expensive. The temperature
is set back to 1 at test time so as to produce more discrete
probability vectors during classiﬁcation.
III. DEFENDING DNNS USING DISTILLATION
Armed with background on DNNs in adversarial settings,
we now introduce a defensive mechanism to reduce vulnerabil-
ities exposing DNNs to adversarial samples. We note that most
previous work on combating adversarial samples proposed
regularizations or dataset augmentations. We instead take a
radically different approach and use distillation, a training
technique described in the previous section, to improve the
robustness of DNNs. We describe how we adapt distillation
into defensive distillation to address the problem of DNN
vulnerability to adversarial perturbations. We provide a justi-
ﬁcation of the approach using elements from learning theory.
A. Defending against Adversarial Perturbations
To formalize our discussion of defenses against adversarial
samples, we now propose a metric to evaluate the resilience of
DNNs to adversarial noise. To build an intuition for this metric,
namely the robustness of a network, we brieﬂy comment on the
underlying vulnerabilities exploited by the attack framework
presented above. We then formulate requirements for defenses
capable of enhancing classiﬁcation robustness.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
X*
X*
misclassiﬁcation within the context of classiﬁers built using
DNNs. The robustness of a trained DNN model F is:
X*
X*
X
Δadv(X, F )
X*
X*
Fig. 4: Visualizing the hardness metric: This 2D repre-
sentation illustrates the hardness metric as the radius of the
disc centered at the original sample X and going through
the possible
the closest adversarial sample X
adversarial samples crafted from sample X. Inside the disc,
the class output by the classiﬁer is constant. However, outside
the disc, all samples X
∗ are classiﬁed differently than X.
∗ among all
In the framework discussed previously, we underlined the
fact that attacks based on adversarial samples were primarily
exploiting gradients computed to estimate the sensitivity of
networks to its input dimensions. To simplify our discussion,
we refer to these gradients as adversarial gradients in the
remainder of this document. If adversarial gradients are high,
crafting adversarial samples becomes easier because small
perturbations will induce high network output variations. To
defend against such perturbations, one must therefore reduce
these variations around the input, and consequently the ampli-
tude of adversarial gradients. In other words, we must smooth
the model learned during training by helping the network
generalize better to samples outside of its training dataset.
Note that adversarial samples are not necessarily found in
“nature”, because adversarial samples are speciﬁcally crafted
to break the classiﬁcation learned by the network. Therefore,
they are not necessarily extracted from the input distribution
that the DNN architecture tries to model during training.
DNN Robustness - We informally deﬁned the notion
of robustness of a DNN to adversarial perturbations as its
capability to resist perturbations. In other words, a robust
DNN should (i) display good accuracy inside and outside of
its training dataset as well as (ii) model a smooth classiﬁer
function F which would intuitively classify inputs relatively
consistently in the neighborhood of a given sample. The notion
of neighborhood can be deﬁned by a norm appropriate for the
input domain. Previous work has formalized a close deﬁnition
of robustness in the context of other machine learning tech-
niques [30]. The intuition behind this metric is that robustness
is achieved by ensuring that the classiﬁcation output by a
DNN remains somewhat constant in a closed neighborhood
around any given sample extracted from the classiﬁer’s input
distribution. This idea is illustrated in Figure 4. The larger this
neighborhood is for all inputs within the natural distribution
of samples,
inputs
are considered, otherwise the ideal robust classiﬁer would
be a constant function, which has the merit of being very
robust to adversarial perturbations but is not a very interesting
classiﬁer. We extend the deﬁnition of robustness introduced
in [30] to the adversarial behavior of source-target class pair
is the DNN. Not all
the more robust
ρadv(F ) = Eμ[Δadv(X, F )]
(3)
where inputs X are drawn from distribution μ that DNN
architecture is attempting to model with F , and Δadv(X, F ) is
deﬁned to be the minimum perturbation required to misclassify
sample x in each of the other classes:
{(cid:5)δX(cid:5) : F (X + δX) (cid:3)= F (X)} (4)
Δadv(X, F ) = arg min
δX
where (cid:5) · (cid:5) is a norm and must be speciﬁed accordingly to
the context. The higher the average minimum perturbation
required to misclassify a sample from the data manifold is,
the more robust the DNN is to adversarial samples.