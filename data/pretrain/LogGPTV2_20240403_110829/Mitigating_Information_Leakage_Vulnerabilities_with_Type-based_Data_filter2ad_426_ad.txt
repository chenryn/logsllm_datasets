### Overhead Reduction and Analysis

The overhead has been reduced from 10.4% to 8.4%. We believe that further improvements in our analysis could potentially reduce this overhead even more. Nonetheless, the benefits appear to justify the implementation effort.

### Runtime Overhead on SPEC CPU2017

The runtime overhead of TDI on SPEC CPU2017 is illustrated in Figure 5. The geometric mean (geomean) overhead is 12.5%, which is higher than the 8.4% observed for CPU2006. Masking is the primary source of overhead, contributing a geomean of 8.0%. However, `omnetpp` suffers from inefficiencies in heap allocation, partially due to limitations in our prototype's analysis. The CPU2017 versions of `x264` and `imagick` contain a significant number of (non-GEP) pointer arithmetic instructions in unsupported scenarios, leading to conservative masking.

### Benchmark Variance

Our benchmarking of `xz` shows high variance, with a standard deviation of approximately 5% (including the baseline). Other benchmarks, such as `mcf` and `lbm`, have a standard deviation of less than 1%. The speedups achieved with typed allocation are consistent. As noted by Mytkowicz et al. [45], it is challenging to avoid measurement bias in this type of evaluation. Our instrumentation and runtime can have side effects that influence performance. For example, arena allocations may cause more cache conflicts, as allocations at the start of arenas share lower bits. Many arenas are used only for small allocations, which could be mitigated by adding small offsets to the arena base, based on internal type IDs or allocation order. However, we did not observe any significant performance change when subtracting small (cache-line-sized) offsets from the base pointers of the typed stacks.

### Memory Overhead

Full TDI’s memory overhead (peak RSS) on CPU2006 has a geomean of 15.5% compared to unmodified tcmalloc. This is due to increased memory fragmentation caused by our allocation strategy, exacerbated by tcmalloc configuration (e.g., minimum page cache sizes) that are inappropriate for arenas. To ensure a fair baseline comparison, we left these values unmodified. Detailed results are provided in Figure 9 in the appendix.

### Comparison with LLVM’s SLH

We also compared the runtime overhead of TDI to LLVM’s Speculative Load Hardening (SLH) mitigation. SLH has a significantly stronger speculative threat model, aiming to prevent loads from executing by mixing predicate state (from branches) into the pointers being loaded, providing a mitigation against most Spectre v1 attacks. However, the overhead of applying (x86) SLH to CPU2006 is prohibitively high (geomean 75.6%), and it provides only speculative safety. (Overhead should be slightly lower without indirect call/jump hardening, but we encountered code generation errors when disabling it.) Detailed results are available in the appendix.

### nginx Performance

We tested TDI using the nginx 1.18.0 web server, with default options and SSL enabled, but the 'geo' module disabled due to undefined behavior (see Appendix A). We linked against OpenSSL 1.1.1h3 using LTO (and -O2), and hardened both nginx and OpenSSL with TDI. We confirmed that the OpenSSL tests pass after full hardening and used a hardened openssl binary to generate 2048-bit RSA keys for SSL.

Note that nginx does not fully benefit from our automated type-based protection, as allocations in nginx’s pools (including shared memory slab pools) lose the benefit of intra-pool type isolation. However, since different types of pools are identified based on callsites, pools containing disjoint types remain isolated from each other and from the many other arenas identified by the type analysis (Section VII-F). One improvement could be to allocate each pool instance in a separate arena, providing finer-grained isolation.

The 'OpenSSL hooks' configuration uses TDI’s instrumentation but assigns arenas using OpenSSL’s allocator hooks, which are surprisingly coarse-grained. We evaluated nginx by serving a small file (64 bytes) via SSL (with default settings), using two Xeon Silver 4110 machines with 100Gb/s Ethernet. We configured nginx to use 16 workers and used 16 threads of wrk2 [3] to make the requests.

Throughput results are shown in Figure 6 (median of 3 runs of 30s each); all cores are saturated for ≥96 connections. Saturated throughput at that point is 5.4% lower than the baseline for full TDI, 3.6% for masking, 3.8% for the typed allocator, and 4.8% for the hook-based allocation. The 90th percentile latency is 4.7% higher for full TDI, and 2.9%, 3.6%, and 3.9% for masking, typed allocator, and the hooks, respectively.

### System Libraries Instrumentation

TDI’s protection does not require complete instrumentation of system libraries, as pointers passed to external functions or stored in memory are always masked. For example, a call to `memcpy` will always be provided with valid pointers to the expected arenas, and any pointers copied by `memcpy` will already have escaped analysis and thus be masked. Since glibc does not support clang, alternative C libraries have compatibility issues, and TDI’s stack instrumentation currently requires LTO, we expect TDI to be used in practice with an uninstrumented system libc. Despite this, we also evaluated the overhead of applying TDI’s full stack/heap protection to libc, using musl (and libc++) rather than glibc.

Throughput overhead for our nginx+OpenSSL benchmark with full stack/heap instrumentation is 8.4% at the point of saturation (vs 5.4% without instrumentation), and lower using alternative configurations such as using 64kB files (4%) or (multi)thread pools (6.8%). Geomean runtime overhead is 10.3% for SPEC CPU2006 (vs 8.4%); as before, `xalancbmk` and `perlbench` are largely responsible. Similarly, geomean overhead is 13.9% for CPU2017 (vs 12.5%). Details can be found in Appendix D.

### Evaluation on SPEC CPU2000

We also evaluated TDI on SPEC CPU2000 to aid comparisons with prior work. Details of the (mostly minor) changes are in Appendix A. Figure 7 presents our performance results for full protection with complete instrumentation (including musl/libc++). As shown in the figure, the geomean runtime overhead is 8.8%, with the highest overhead (35.8%) for `perlbmk`, similar to previous results.

Our CPU2000 overhead is comparable to domain-based sandboxing solutions such as NaCl [72] (∼7%)—despite our support for arbitrary (rather than NaCl-only) programs and intra-domain isolation. Moreover, our overhead is much lower than state-of-the-art software fault isolation techniques that rely on highly optimized address masking instrumentation on loads/stores [73] (rather than pointer arithmetic like TDI). Specifically, Zeng et al.’s solution [73], which can only support the limited number of colors allowed by load/store masking, yields 19% overhead on top of a CFI baseline and on a CPU2000 subset excluding costly benchmarks like `perlbmk`. More fine-grained solutions like WIT [7] can support more colors (limited by the imprecision of context-insensitive points-to analysis), but load instrumentation can increase overhead (10% on a CPU2000 subset excluding costly benchmarks like `perlbmk`) "by more than a factor of three" [7]. Note that these numbers (from [7] and [73]) are not directly comparable due to the different evaluation platforms.

### Isolation Granularity

Although TDI can be used as a traditional coarse-grained (e.g., 2-color) isolation scheme even in the absence of any automated color analysis (significantly outperforming prior load/store address masking solutions, as noted), we briefly evaluated how arenas are assigned in practice by the automated type analysis in a fine-grained, many-color configuration.

For our nginx(+OpenSSL) benchmark, the automated type analysis (TAT) statically identifies 197 colors (and arenas) on the stack and 649 colors (and arenas) on the heap. On the heap, the data type analysis assigns a total of 96 types to allocations at 583 call sites, while the remaining 553 types are identified by the context-sensitive callsite ID analysis based on wrapper detection and inlining (Section VI-C).

We also looked at the per-arena object distribution during the execution of the benchmark. Figure 8 shows the number of objects allocated in each heap arena during startup and the first client request. The 'OpenSSL hooks' results manually assign arenas by using OpenSSL’s built-in support for hooking allocator functions (CRYPTO_set_mem_functions); we used one-line wrappers which assign an arena ID based on the callsite information provided by OpenSSL. There were 178, 133, and 106 heap arenas for the automated (TAT), TAT+hooks, and (manual) hooks configurations, respectively.

Notably, this shows that attempting to manually assign arenas by hooking OpenSSL’s allocator functions leads to coarser arenas than a fully-automated approach, even when TAT is also used to assign arenas and can merge allocations of the same type. The fully-automated approach can produce finer-grained arenas because OpenSSL’s allocator hooks use indirect calls and only provide direct callsite information (filename/line numbers). For example, OpenSSL provides wrapper functions for allocating and resizing 'buffers'; OpenSSL’s allocation functions are called from these buffer wrapper functions, resulting in a large number of allocations from a small number of callsites. TAT instead detects the buffer code as allocator wrappers and allocates arenas based on the parent callsite since the buffer data is untyped (char *).

We also inspected arena usage for some of the SPEC benchmarks. On the CPU2000 subset evaluated by WIT [7], TDI’s type analysis yields a number of colors comparable to WIT’s points-to analysis (which fares well on such simple benchmarks with many stack allocations). However, unlike WIT, TDI can easily handle the entirety of CPU2000 and even much more complex programs. Moreover, while WIT is limited to 256 colors, TDI uses a larger number of colors even on the slightly more complex CPU2006 benchmarks. For example, `xalancbmk` allocates 186 stack and 200 heap arenas, and `gcc` allocates 110 stack and 192–198 heap arenas (depending on the benchmark). Arena statistics for the other benchmarks are provided in Appendix E.

### Residual Attack Surface

#### Pointer Arithmetic

TDI’s arena allocation could be applied without masking (with much lower overhead). However, non-linear memory vulnerabilities are becoming the primary form of spatial safety vulnerability in the architectural [43] and speculative [32] domains, which may allow attackers to bypass guard zones. These are exactly the situations for which we apply masking.

With full protection, TDI cannot prevent overflows within/across objects of the same color (i.e., type). This provides strong isolation for info leaks, although in some cases, there is a remaining attack surface for intra-pool leaks. For example, OpenSSL stores highly confidential data (private/session keys) in the same bignum types as data related to public keys; an info leak bug specifically revealing bignum data for a public key may also allow an attacker to obtain confidential bignum data. If desired, TDI supports annotations to further improve the isolation of critical objects, much like existing data isolation solutions.

TDI also offers limited protection against memory corruption exploits outside our threat model. For instance, if an attacker can overwrite a pointer (e.g., in a struct), they can potentially bypass our mitigation. We make such attacks more difficult by limiting the set of reachable pointers (pointers within the same object type) and their ability to leak pointers.

#### Spectre

TDI provides the same protection against Spectre-BCB attacks as it does against non-speculative information leaks—preventing cross-arena leakage. Most other Spectre variants are clearly out-of-scope and best mitigated by techniques such as retpoline or hardware-based mitigations. However, a theoretical attack surface remains in Spectre V1 gadgets that exploit speculative issues beyond memory safety (e.g., logic bugs). We also do not prevent attacks exposing potential.