privacy policy of Example 5.1. The X-axis shows the probability of
the predicate S1 and the Y-axis that of S2. The gray area depicts the
box (cid:174)αΦ, i.e. all vectors that lie within the two belief bounds in Φ.
We depict the vectors (cid:174)p
for all singleton equivalence classes
δ,π
X ∈ ξ⊤, as well as those for classes {0, 2} and {1, 3}, which are
X
produced by the algorithm. We have (cid:174)p
δ,π{0} (cid:60) (cid:174)αΦ, as this vector lies
outside the box (cid:174)αΦ, and thus violates at least one belief bound in Φ.
In contrast, we have (cid:174)p
δ,π{0,2} ∈ (cid:174)αΦ, and so the class {0, 2} satisfies all
belief bounds.
Key Steps. The main steps of SynGrd are given in Algorithm 2. At
δ (I ∈
Si) (cid:60) [ai , bi] holds. The satisfaction of this condition implies the
non-existence of an equivalence relation that enforces Φ for the
given π and δ. If no such enforcement exists, the algorithm re-
turns unsat.
Line 2, the algorithm checks whether the conditionk
At Line 5, the algorithm constructs the initial equivalence re-
lation. For the permissiveness goal, the most refined equivalence
relation is used. For the answer precision goal, the equivalence re-
lation joining all the violating outputs into one class C and keeping
the rest of outputs, that do not violate the policy, as singletons is
used. The rationale is that these outputs would never be singletons
in a valid enforcement.
i =1 Pπ
The algorithm then iteratively performs the following steps until
the constructed equivalence relation ξ enforces the policy (checked
at Line 9):
Pick Most Violating Class. At Line 10, the algorithm picks the
class E with the greatest distance between the vector (cid:174)p
and the
δ,π
E
box (cid:174)αΦ, which is defined as the distance between the point (cid:174)p
δ,π
X
and the closest point (cid:174)q inside the box (cid:174)αΦ:
∥(cid:174)p
δ,π
X
Dist((cid:174)p
δ,π
X
− (cid:174)q∥
, (cid:174)αΦ) = min
(cid:174)q∈ (cid:174)αΦ
δ,π
δ,π
Pick Merge Candidate. At Line 11, SynGrd checks if there is a
class E′ such that if merged with E, the resulting vector (cid:174)p
E∪E′ is in
the box (cid:174)αΦ. If this is the case, then SynGrd picks the class E′ that
has the point (cid:174)p
E∪E′ closest to the center of (cid:174)αΦ (Line 12). Otherwise,
SynGrd picks the class E′ that has the point (cid:174)p
E∪E′ closest to the
box (cid:174)αΦ. (Line 14)
Note that SynGrd prefers to pick a class E′ such that the merged
class E ∪ E′ is in box (cid:174)αΦ, as this implies that the merged class need
not be further merged with other classes.
Merge. At Line 15, the algorithm combines the class E and the
picked class E′.
The loop terminates when the equivalence relation ξ enforces
the policy for the given program and attacker belief. At this point
SynGrd returns ξ.
δ,π
Example 6.1. In Figure 7, we graphically illustrate the steps of
SynGrd on Example 5.1 for the permissiveness goal. The vectors
(cid:174)p{0}, (cid:174)p{1}, (cid:174)p{2}, (cid:174)p{3} denote the probabilities of the predicates S1
and S2 for all four singleton equivalence classes. The while loop’s
condition evaluates to true since the vectors (cid:174)p
δ,π{3}
δ,π{2} , and (cid:174)p
δ,π{0} , (cid:174)p
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA399(cid:174)p
δ,π{0}
(cid:174)p
δ,π{0,2}
1
b2
a2
(cid:174)p
δ,π{1}
(cid:174)p
δ,π{1,3}
(cid:174)αΦ
(cid:174)p
δ,π{2}
(cid:174)p
δ,π{3}
0
a1
b1
1
Figure 7: Equivalence classes computed by the greedy syn-
thesis algorithm SynGrd for our Example 5.1 for the per-
missiveness goal.
lie outside the box (cid:174)αΦ. At Line 10, SynGrd selects {3} since (cid:174)p
is
δ,π{3}
the most distant vector from (cid:174)αΦ. SynGrd merges {3} with {1} as the
vector (cid:174)p
δ,π{1,3} is closest to the center of (cid:174)αΦ. In the second iteration
of the while loop, SynGrd merges the equivalence classes {0} and
{2}. The algorithm returns the equivalence classes {{0, 2}, {1, 3}}.
(cid:4)
Running Time. The running time of SynGrd is O(|O|2). The
while loop is executed at most |O| times, since initially we have
|O/ξ| = |O|, and each iteration it decreases by 1. When it reaches 1,
the while loop’s condition must be satisfied. All expressions in the
while loop’s body can be evaluated in O(|O|) when ξ is represented
as O/ξ.
Theorem 6.2. Let π be an arbitrary probabilistic program, δ
an arbitrary attacker belief, and Φ an arbitrary privacy policy. If
there is no equivalence relation ξ such that Enf(π , ξ), δ |= Φ, then
SynGrd(π , δ, Φ) = unsat. Otherwise, we have SynGrd(π , δ, Φ) = ξ
and Enf(π , ξ), δ |= Φ.
The proof of this theorem is in Appendix A.
7 IMPLEMENTATION
We now describe the Spire system, an end-to-end implementation
of our enforcement synthesis approach.
Inputs. Spire takes three files as input: (i) a probabilistic program,
(ii) a probabilistic program that encodes an attacker belief, and (iii) a
text file that defines a privacy policy. Inputs (i) and (ii) are specified
in the Psi language [22]. The Psi language, presented in Appendix
B, is an imperative probabilistic language that operates on real-
valued scalar and array data, and supports probabilistic assignments,
observe statements, as well as the standard sequence, conditional,
and bounded loop statements. For more details on the Psi language
see [22]. Input (iii) is in the format (Expr, a, b) where Expr is a Psi
expression and a and b are bounds. In addition, a parameter is
passed defining whether to optimize for permissiveness or answer
precision. Note that other notions of optimality can be easily added
by specifying a custom objective function.
Bayesian Inference. We use the Psi solver [22] to perform sym-
bolic inference. For each belief bound (S,[a, b]), Spire calls the Psi
solver to compute a symbolic expression that captures the probabil-
δ (I ∈ S | O = x), where x is a symbolic variable.
ity distribution Pπ
This step is #P-complete [54]. Note that the symbolic expression
captures the probability of the predicate S for all possible outputs.
This expression is specified in the SMT-LIBv2 format [4]. We evalu-
ate this expression using off-the-shelf SMT solvers, for all possible
outputs, to obtain the probabilities used by the synthesis algorithms.
Similarly, we use the Psi solver to compute a symbolic expression
δ (O = y). These distri-
that captures the probability distribution Pπ
butions are sufficient to derive all necessary probabilities used by
the two synthesis algorithms.
Synthesis Algorithms. Spire implements the two synthesis algo-
rithms in C# (in roughly 2.5K LOC). In the implementation of the
SynSMT algorithm, Spire calls the Z3 SMT solver[8, 16] to solve
linear optimization problems over SMT constraints; see Line 4 of
Algorithm 1.
Spire supports an interactive mode, where the attacker may ask
to run a program multiple times. In this mode, the initial attacker be-
lief is the one provided as input to Spire. For subsequent iterations,
Spire keeps track of the revised attacker belief using symbolic infer-
ence. Concretely, Spire uses Psi to compute a symbolic expression
over a variable x that captures the attacker belief Pπ
Output. As output, Spire returns the equivalence classes com-
puted by the two synthesis algorithms. It also outputs the encoding
of the enforcement in the Psi language, as illustrated in Lines 9-15
in Figure 3.
δ (I = x).
8 EVALUATION
In this section, we evaluate our Spire system as follows: (i) we
compare the algorithms SynGrd and SynSMT, (ii) we evaluate
Spire’s performance, and (iii) we measure the permissiveness and
answer precision of the synthesized enforcements. We first describe
our experiments and then report and discuss our results.
8.1 Experiments
We perform experiments on instances of 10 different programs from
3 scenarios adopted from the literature. For each scenario, we have
both deterministic, and probabilistic programs. Here, we briefly
sketch the scenarios. Full details can be found in Appendix C.
Genomic Data. This scenario is the same as the one described in
Section 2, but here we scale the number of patients and the policy
size. We experiment with four programs: sum given in Figure 2(a),
noisy-sum which is like sum but randomly adds ±1 as in Figure 6,
read returns the nucleotides of a patient, and prevalence returns
the number of patients with AA nucleotides. We generate synthesis
instances (n, m) where n is the number of patients and m the size
of the privacy policy.
Social. This scenario, borrowed from [23], models a social network
where users express their political affiliations, which are correlated
based on the friendship relations [29, 35, 58]. We experiment with
three programs: sum returns the number of users affiliated with a
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA400sum
read
Psi time
noisy-sum
prevalence
(3,1)
(3,3)
(6,1)
(6,6)
(a) Genomic Data
(9,1)
(9,9)
noisy-sum
sum
read
Psi time
(3,1)
(3,3)
(6,1)
(6,6)
(9,1)
(b) Social
(9,9)
(12,1)
(12,12)
read
constant
random
Psi time
103
101
10−1
103
101
10−1
103
101
10−1
(6,1)
(6,3)
(9,1)
(9,3)
(12,1)
(12,3)
(15,1)
(15,3)
(c) Location
Figure 8: Running times of Psi for Bayesian inference.
particular party, noisy-sum is like sum but randomly adds ±1, and
read returns the political affiliation of a user. We generate synthesis
instances (n, m) where n is the number of users and m the privacy
policy size. The policy bounds the attacker belief about a user’s
affiliation.
Location. This scenario models a user protecting his location [34,
51]. We borrow three programs from [47]: read returns the user’s
location, constant and random return a constant and, respectively,
random coordinate if the user is within a sensitive area. We gen-
erate synthesis instances (n, m) where n is the width and height
of a rectangular grid and m the size of the privacy policy. The pol-
icy bounds the attacker belief about the user being in a sensitive
location.
8.2 Results
We ran all synthesis instances with each algorithm SynSMT and
SynGrd, optimizing for permissiveness and answer precision with
both algorithms. We used a 32-core machine with four 2.13GHz
Xeon processors running Ubuntu Linux 16.04. We set a timeout of 60
minutes for the symbolic inference using Psi, SynSMT, and SynGrd,
]
s
[
e
m
T
i
]
s
[
e
m
T
i
]
s
[
e
m
T
i
]
s
[
e
m
T
i
103
100
10−3
103
100
10−3
103
100
10−3
103
100
10−3
sum
read
noisy-sum
prevalence
SynSMT (Perm)
(3,1)
(3,3)
(6,1)
(6,6)
(9,1)
(9,9)
SynSMT (Ans)
(3,1)
(3,3)
(6,1)
(6,6)