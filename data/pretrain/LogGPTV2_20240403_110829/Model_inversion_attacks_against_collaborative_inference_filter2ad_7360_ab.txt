our attacks. Multi-participant attacks can be achieved in a
similar way. P2 can be defined by all consecutive adversarial
participants from an intermediate layer to the last layer. P1
can be defined as the initial layers to that intermediate layer.
Adversary’s capabilities. We assume the untrusted partic-
ipant P2 strictly follows the collaborative inference protocols:
receiving v = fθ1(x ) from P1 and generating y = fθ2(v). He
cannot compromise the inference process conducted by P1,
and he has no knowledge of the input x, nor any intermediate
values inside P1, except v.
We consider the adversary with different capabilities, sum-
marized in Table 1. These capabilities include knowledge of
the target model fθ1, knowledge of training data, and access
to the target system for a query. Based on these, we consider
three types of settings:
In the white-box setting (Section 4), P2 obtains knowl-
edge of the DNN layers fθ1 controlled by P1, including the
network structure and parameters. Then the adversary can
use the model parameters to recover input data, without the
requirements of knowing training data or querying models.
In the black-box setting (Section 5), we relax the assump-
tion about the knowledge of the target model. The adversary
can only learn information about the model fθ1 indirectly
through querying the inference system. We demonstrate that
the adversary can recover the sensitive input when he knows
the values, or distribution of the original training dataset, or
neither.
We further consider the query-free setting (Section 6),
which is a special case of the black-box scenario without
the capability of model query. This type of attacks needs
the lowest requirements. We show that the adversary can
recover the data even when he has no knowledge of the edge
model and cannot query the model.
3.2 Experimental Configurations
In the rest of this paper, we evaluate our attacks on two
standard DNN benchmark datasets: MNIST and CIFAR10.
The target models we try to find inverses are convolu-
tional neural networks (CNN). Specifically, we adopt LeNet
(2 convolutional layers and 3 fully connected layers) on the
MNIST dataset, and a CNN with 6 convolutional layers and
Table 1: Adversary’s capability in our consideration (✓:
the adversary needs this capability; –: this capability is
not necessary.)
Setting
Section 4
White-box
Section 5
Black-box
Section 6
Query-free
Target model
Training Data
Parameters
Structure Values Distribution
✓
–
–
–
–
–
–
–
✓
–
–
–
✓
–
✓
–
–
✓
–
–
✓
✓
–
–
–
✓
✓
–
✓
✓
✓
✓
Inference
Query
–
✓
✓
✓
–
–
–
–
2 fully connected layers on the CIFAR10 dataset. We split
each model at different layers (mainly convolutional layers).
Table 2 lists the detailed experimental configurations. These
configurations are realistic in the case of edge-cloud scenar-
ios, as the most heavy-computational layers (including all
fully-connected layers) are offloaded to the cloud. We will
explore the cases that the model is split at fully-connected
layers in Section 7.1.
Table 2: Experiment Configurations
Dataset
Target Model
MNIST
LeNet-5
(2 conv + 3 fc)
Split point
• 1st conv layer (conv1)
• 2nd conv layer after
activation (ReLU2)
CIFAR10
6 conv + 2 fc CNN
• 1st conv layer (conv11)
• 4th conv layer after
activation (ReLU22)
• 6th conv layer after
activation (ReLU32)
We follow the standard MNIST and CIFAR split for training
and testing samples [1]. We set the learning rate to 10−3 and
choose ADAM as our optimizer. The target models and all
attacks are implemented with Pytorch 1.0.1. We run our
experiments on a server with 1 Nvidia 1080Ti GPU, 2 Intel
Xeon E5-2667 CPUs, 32MB cache, 64GB memory and 2TB
hard-disk.
3.3 Evaluation Metrics
To quantify the attack results, we adopt two metrics, Peak
Signal-to-Noise Ratio (PSNR) and Structural Similarity Index
(SSIM) [47]. PSNR mathematically measures the pixel level
recovery quality of the image. It is defined in Equation 2,
where MAX I is the maximum possible intensity of a signal
(255.0 for images), and I (i, j) refers to the intensity at posi-
tion (i, j) of image I. SSIM measures the human perceptual
similarity of two images. It considers luminance, contrast
and structure of two images. SSIM is a single value between
0 and 1, where 0 represents least similar and 1.0 indicates
most similar.
PSN R(I , I0) = 10loд(
1
mn
(cid:80)m
i =1(cid:80)n
2
MAX I
j=1(I (i, j) − I0(i, j))
(2)
2 )
4 WHITE-BOX ATTACKS
We start from the white-box setting, where the adversary
participant knows the parameters of the initial layers fθ1 on
the trusted participant. As deep learning frameworks and
tools become prevalent and mature, many trained models
are published online for free use, covering various prediction
tasks. It is a common practice for model owners to directly
download and deploy these models. In this case, the adver-
sary participant has white-box access to the target model.
Formally, the model inversion problem is: how can the
adversary recovers an input x0, from the corresponding in-
termediate value fθ1(x0), and the model parameters θ1? We
propose regularized Maximum Likelihood Estimation (rMSE)
to solve this problem.
4.1 Regularized Maximum Likelihood
Estimation
We treat the model inversion as an optimization problem:
given fθ1(x0), our goal is to find a generated sample x, that
satisfies two requirements: (1) the intermediate output of
this sample, fθ1(x ), is similar to fθ1(x0); (2) x is a natural
sample, following the same distribution as other inference
samples.
For requirement (1), we use the Euclidean Distance (ED)
to measure the similarity between fθ1(x ) and fθ1(x0) (Eq.
3(a)). Note that fθ1(x ) can be interpreted as the mapping
from the input space (unobservable to the adversary) to the
feature space (observable to the adversary). Then this Eu-
clidean Distance represents the posteriori information from
the adversary’s intermediate-level observation. Our goal is
to find the optimal sample x that minimizes this distance.
For requirement (2), we adopt the Total Variation [37] to
represent the prior information of an input sample. The total
variation of a 2D image x is defined in Equation 3(b), where
xi, j represents the pixel at position (i, j). The total variation
encourages the generated image x to be piece-wise smooth,
i.e. avoiding drastic variations inside regions but allowing
large changes along the region boundaries, controlled by β.
(cid:88)
i, j
ED(x, x0) = || fθ1(x ) − fθ1(x0)||2
TV (x ) =
∗ = arдminx ED(x, x0) + λTV (x )
(|xi +1, j − xi, j|2 + |xi, j+1 − xi, j|2
x
2
)β /2
(3a)
(3b)
(3c)
The total objective function of the model inversion prob-
lem is a combination of feature space similarity and natural-
input a priori, as shown in Eq. 3c. In this equation, λ is a
hyperparameter to balance the effects of the two terms. If the
feature space fθ1(x ) is far from the input space, i.e. a lot of
network layers are computed on the trusted participant P1, a
large λ is required because less posterior information about
the input can be recovered from the feature space and the
adversary needs to rely on the prior information. In contrast,
if only a small number of layers are deployed on P1, then
the adversary only needs to select a small λ. We perform
gradient descent (GD) to solve Eq. 3c and recover the image.
*/
Algorithm 1 White-box model inversion attack
1: Function WhiteboxAttack(fθ1, fθ1(x0), T , λ, ϵ)
2: /* fθ1: the target model */
3: /* fθ1(x0): the intermediate output of sensitive input x0
4: /* T : maximum number of iterations */
5: /* λ: tradeoff between prior and posteriori information
6: /* ϵ: step size in GD */
7:
8: L(x )=|| fθ1(x ) − fθ1(x0)||2
9: t = 0
10: x (0)= ConstantInit()
11: while (t < T ) do
x (t +1) = x (t ) − ϵ ∗ ∂L(x (t ) )
12:
∂x (t )
t+=1
13:
14: end for
15: return x (T )
2 + λTV (x )
Algorithm 1 shows the detailed white-box attack. The in-
put image is initialized with constant gray, i.e. 0.5 for all RGB
channels. We choose ADAM [26] to accelerate the optimiza-
tion. We observe that ADAM converges more stably when
performing model inversion from shallow layers. Therefore,
we choose a large step size (10−2) and a small iteration num-
ber (500) for shallow layers, and a small step size (10−3) and
a large iteration number (5000) for deep layers. We choose
β = 1.0 and observe this is enough to generate good results.
4.2 Evaluation
Figure 3 shows the white-box attack results on MNIST and
CIFAR10 datasets. For each dataset, the first row shows the
target inference samples, and the remaining rows are the
recovered images when the split point is at different layers.
For MNIST, we observe that the adversary can accurately
recover the images with high fidelity, when the split point is
either in the first (conv1) or last (ReLU2) convolutional layer.
For CIFAR10, when the split layer is in the first (conv11) or
fourth (ReLU22) convolutional layer, the recovered images
maintain high quality. When the neural network is split after
the last convolutional layer (ReLU32), the recovered images
are hardly recognizable.
(a) MNIST.
Ref
Conv1
ReLU2
Ref
Conv11
ReLU22
ReLU32
(b) CIFAR10.
Figure 3: Recovered inputs in white-box attacks
Table 3 shows the PSNR and SSIM metrics for each exper-
iment. We observe that when the split point is in a deeper
layer, the quality and similarity of recovered images become
worse. For CIFAR10, there is a significant drop in SSIM from
ReLU22 to ReLU32. These conclusions are consistent with
the visual assessments in Figure 3. We set the threshold of
SSIM as 0.3: a recovered image with an SSIM value below
this threshold (shaded entries in Table 3, and other tables in
the following sections) is regarded as being unrecognizable.
Table 3: PSNR (db) and SSIM for white-box attacks
MNIST
PSNR 39.69
SSIM 0.9969
conv1 ReLU2
15.10
0.5998
CIFAR10
conv11 ReLU22 ReLU32
37.59
0.9960
13.38
0.1625
19.47
0.6940
5 BLACK-BOX ATTACKS
Next, we consider the black-box setting, where the adversary
does not have knowledge of the structure or parameters of
fθ1. We assume that the adversary can query the black-box
model: he can send an arbitrary input x to P1, and observe
the corresponding output fθ1(x ). This assumption applies
to the case where the model owner releases prediction APIs
to end users as an inference service. We further relax this
assumption in Section 6.
Model inversion attacks under the black-box setting are
more challenging, because without the knowledge of model
parameters, the adversary cannot directly perform a gradient
descent operation on fθ1 to solve the optimization problem
in Equation 3(c). One solution is to first recover the model
structure and parameters by querying the model, and then
recover the inference samples. The possibility of model re-
construction has been demonstrated in [33, 44, 46]. We prove
that the model inversion attacks can be achieved based on
the reconstructed model in Section 7.3.
We propose a more efficient approach, Inverse-Network,
to directly identify the inversed mapping from output to
input, without the need to obtain the model information.
Our solution is easier to implement, and can recover inputs
with higher fidelity. We describe this approach and evaluate
it in this section. Quantitative comparisons between these
two solutions are presented in Section 7.3.
5.1 Inverse-Network
Conceptually, the Inverse-Network is the approximated in-
verse function of fθ1, trained with v = fθ1(x ) as input, and
x as output. We show the detailed description of Inverse-
Network approach in Algorithm 2. The attack consists of
three phases: ① generating a training set for the Inverse-
Network; ② training the Inverse-Network; and ③ recovering