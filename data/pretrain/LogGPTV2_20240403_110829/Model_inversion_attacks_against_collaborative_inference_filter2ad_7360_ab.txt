### Multi-Participant Attacks

Multi-participant attacks can be executed in a similar manner. The adversarial participants, denoted as \( P2 \), can be defined as all consecutive participants from an intermediate layer to the last layer. Conversely, \( P1 \) can be defined as the initial layers up to that intermediate layer.

### Adversaryâ€™s Capabilities

We assume that the untrusted participant \( P2 \) strictly adheres to the collaborative inference protocols. This means \( P2 \) receives \( v = f_{\theta_1}(x) \) from \( P1 \) and generates \( y = f_{\theta_2}(v) \). \( P2 \) cannot compromise the inference process conducted by \( P1 \) and has no knowledge of the input \( x \) or any intermediate values within \( P1 \), except for \( v \).

We consider adversaries with varying capabilities, summarized in Table 1. These capabilities include knowledge of the target model \( f_{\theta_1} \), access to training data, and the ability to query the target system. Based on these, we explore three types of settings:

1. **White-Box Setting (Section 4):** In this setting, \( P2 \) has complete knowledge of the DNN layers \( f_{\theta_1} \) controlled by \( P1 \), including the network structure and parameters. The adversary can use these parameters to recover the input data without needing the training data or querying the model.

2. **Black-Box Setting (Section 5):** Here, we relax the assumption about the knowledge of the target model. The adversary can only learn about \( f_{\theta_1} \) indirectly through queries to the inference system. We demonstrate that the adversary can recover sensitive inputs even if they only know the values or distribution of the original training dataset, or neither.

3. **Query-Free Setting (Section 6):** This is a special case of the black-box scenario where the adversary lacks the capability to query the model. This type of attack requires the least amount of information. We show that the adversary can still recover the data even without knowledge of the edge model and the ability to query it.

### Experimental Configurations

In the remainder of this paper, we evaluate our attacks on two standard DNN benchmark datasets: MNIST and CIFAR10. The target models we attempt to invert are Convolutional Neural Networks (CNNs). Specifically, we use LeNet (with 2 convolutional layers and 3 fully connected layers) on the MNIST dataset, and a CNN with 6 convolutional layers and 2 fully connected layers on the CIFAR10 dataset. We split each model at different layers, primarily convolutional layers. Table 2 lists the detailed experimental configurations, which are realistic in edge-cloud scenarios, as the most computationally intensive layers (including all fully-connected layers) are offloaded to the cloud. We will explore the cases where the model is split at fully-connected layers in Section 7.1.

### Evaluation Metrics

To quantify the attack results, we use two metrics: Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). PSNR measures the pixel-level recovery quality of the image, defined in Equation 2, where \( MAX_I \) is the maximum possible intensity of a signal (255.0 for images), and \( I(i, j) \) refers to the intensity at position \( (i, j) \) of image \( I \). SSIM measures the human perceptual similarity of two images, considering luminance, contrast, and structure. SSIM ranges from 0 to 1, where 0 indicates the least similarity and 1.0 indicates the most similarity.

\[ PSNR(I, I_0) = 10 \log_{10} \left( \frac{MAX_I^2}{\frac{1}{mn} \sum_{i=1}^{m} \sum_{j=1}^{n} (I(i, j) - I_0(i, j))^2} \right) \]

### White-Box Attacks

In the white-box setting, the adversary knows the parameters of the initial layers \( f_{\theta_1} \) on the trusted participant. With the prevalence of deep learning frameworks and tools, many trained models are published online for free use, covering various prediction tasks. It is common for model owners to directly download and deploy these models, giving the adversary white-box access to the target model.

Formally, the model inversion problem is: how can the adversary recover an input \( x_0 \) from the corresponding intermediate value \( f_{\theta_1}(x_0) \) and the model parameters \( \theta_1 \)? We propose Regularized Maximum Likelihood Estimation (rMSE) to solve this problem.

#### Regularized Maximum Likelihood Estimation

We treat the model inversion as an optimization problem: given \( f_{\theta_1}(x_0) \), our goal is to find a generated sample \( x \) that satisfies two requirements:
1. The intermediate output \( f_{\theta_1}(x) \) is similar to \( f_{\theta_1}(x_0) \).
2. \( x \) is a natural sample, following the same distribution as other inference samples.

For requirement (1), we use the Euclidean Distance (ED) to measure the similarity between \( f_{\theta_1}(x) \) and \( f_{\theta_1}(x_0) \) (Equation 3(a)). For requirement (2), we adopt the Total Variation (TV) to represent the prior information of an input sample (Equation 3(b)).

The total objective function of the model inversion problem is a combination of feature space similarity and natural-input a priori, as shown in Equation 3(c). In this equation, \( \lambda \) is a hyperparameter to balance the effects of the two terms. If the feature space \( f_{\theta_1}(x) \) is far from the input space, a large \( \lambda \) is required. Conversely, if only a small number of layers are deployed on \( P1 \), a small \( \lambda \) is sufficient. We perform gradient descent (GD) to solve Equation 3(c) and recover the image.

\[ ED(x, x_0) = \| f_{\theta_1}(x) - f_{\theta_1}(x_0) \|^2 \]
\[ TV(x) = \sum_{i,j} \left( |x_{i+1,j} - x_{i,j}|^2 + |x_{i,j+1} - x_{i,j}|^2 \right)^{\beta/2} \]
\[ x^* = \arg \min_x \left( ED(x, x_0) + \lambda TV(x) \right) \]

Algorithm 1 shows the detailed white-box attack. The input image is initialized with a constant gray value (0.5 for all RGB channels). We use ADAM to accelerate the optimization. We observe that ADAM converges more stably when performing model inversion from shallow layers. Therefore, we choose a large step size (10^-2) and a small iteration number (500) for shallow layers, and a small step size (10^-3) and a large iteration number (5000) for deep layers. We set \( \beta = 1.0 \) and find this sufficient to generate good results.

#### Evaluation

Figure 3 shows the white-box attack results on the MNIST and CIFAR10 datasets. For each dataset, the first row shows the target inference samples, and the remaining rows are the recovered images when the split point is at different layers. For MNIST, the adversary can accurately recover the images with high fidelity, whether the split point is in the first (conv1) or last (ReLU2) convolutional layer. For CIFAR10, when the split layer is in the first (conv11) or fourth (ReLU22) convolutional layer, the recovered images maintain high quality. When the neural network is split after the last convolutional layer (ReLU32), the recovered images are hardly recognizable.

Table 3 shows the PSNR and SSIM metrics for each experiment. We observe that when the split point is in a deeper layer, the quality and similarity of recovered images become worse. For CIFAR10, there is a significant drop in SSIM from ReLU22 to ReLU32. These conclusions are consistent with the visual assessments in Figure 3. We set the threshold of SSIM as 0.3: a recovered image with an SSIM value below this threshold (shaded entries in Table 3, and other tables in the following sections) is regarded as being unrecognizable.

### Black-Box Attacks

Next, we consider the black-box setting, where the adversary does not have knowledge of the structure or parameters of \( f_{\theta_1} \). We assume the adversary can query the black-box model: they can send an arbitrary input \( x \) to \( P1 \) and observe the corresponding output \( f_{\theta_1}(x) \). This assumption applies to the case where the model owner releases prediction APIs to end users as an inference service. We further relax this assumption in Section 6.

Model inversion attacks under the black-box setting are more challenging because, without the knowledge of model parameters, the adversary cannot directly perform a gradient descent operation on \( f_{\theta_1} \) to solve the optimization problem in Equation 3(c). One solution is to first recover the model structure and parameters by querying the model, and then recover the inference samples. The possibility of model reconstruction has been demonstrated in [33, 44, 46]. We prove that model inversion attacks can be achieved based on the reconstructed model in Section 7.3.

We propose a more efficient approach, Inverse-Network, to directly identify the inverse mapping from output to input without obtaining the model information. Our solution is easier to implement and can recover inputs with higher fidelity. We describe this approach and evaluate it in this section. Quantitative comparisons between these two solutions are presented in Section 7.3.

#### Inverse-Network

Conceptually, the Inverse-Network is the approximated inverse function of \( f_{\theta_1} \), trained with \( v = f_{\theta_1}(x) \) as input and \( x \) as output. Algorithm 2 describes the detailed Inverse-Network approach. The attack consists of three phases:
1. Generating a training set for the Inverse-Network.
2. Training the Inverse-Network.
3. Recovering the inputs.