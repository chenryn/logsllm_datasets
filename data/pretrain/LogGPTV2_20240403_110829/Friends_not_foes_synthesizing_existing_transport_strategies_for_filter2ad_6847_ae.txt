### Performance Comparison with pFabric

PASE achieves comparable performance to pFabric, with a difference of less than 6% in Average Flow Completion Times (AFCTs) in the following two scenarios:
1. Single bottleneck scenarios.
2. Network loads typically below 80%.

#### Left-Right Inter-Rack Scenario

In the left-right inter-rack scenario, the aggregation-core link becomes the bottleneck. Figure 10(a) illustrates the 99th percentile FCT as a function of network load. pFabric achieves smaller FCTs for loads up to 50%, while PASE performs comparably. However, at loads of 60% and above, PASE outperforms pFabric, with an improvement of over 85% at 90% load. This is due to the high and persistent loss rates experienced by pFabric at high loads. Figure 10(b) shows the Cumulative Distribution Function (CDF) of FCTs at 70% load for this scenario.

#### All-to-All Intra-Rack Scenario

The all-to-all intra-rack scenario, common in applications like web search, involves responses from multiple worker nodes within a rack being combined by an aggregator node before sending the final response to the user. Any node within the rack can act as an aggregator, and they are selected in a round-robin fashion to achieve load balancing [8].

Figure 10(c) demonstrates that PASE provides up to an 85% improvement in AFCT over pFabric across all loads. This is because, with pFabric, multiple flows sending at line rate can collide at downstream Top-of-Rack (ToR) to host links, leading to significant network capacity waste on host-ToR links. In contrast, PASE avoids arbitration latency in the intra-rack scenario, as new flows start sending traffic at line rate based on information (priority and reference rate) from their local arbitrator. After one Round-Trip Time (RTT), all flows obtain their global priorities, which helps avoid persistent throughput loss if local and global priorities differ.

### Micro-Benchmarks

#### Arbitration Optimizations

PASE introduces early pruning and delegation to reduce the arbitration overhead of update messages. Figure 11(b) shows the overhead reduction achieved by PASE when all optimizations are enabled. These optimizations provide up to a 50% reduction in arbitration overhead, especially at high loads. This is because higher-level arbitrators delegate some bandwidth to lower-level arbitrators, significantly reducing control overhead on ToR-Aggregation links. Additionally, only updates for flows mapped to the highest priority queues are propagated due to early pruning. Figure 11(a) shows that these optimizations also improve AFCT by 4-10% across all loads.

#### End-to-End Arbitration

PASE enables global prioritization among flows through its scalable end-to-end arbitration mechanism, which requires additional update messages. We compare the performance of end-to-end arbitration and local arbitration in the left-right inter-rack scenario. Figure 12(a) shows that end-to-end arbitration leads to significant improvements (up to 60%) in AFCTs across a wide range of loads. This is because local arbitration cannot account for contention that does not occur on access links, leading to sub-optimal performance.

#### Impact of Number of Priority Queues

We evaluate the impact of changing the number of priority queues in switches. Figure 12(b) shows that using 4 queues provides significant AFCT improvement at loads ≥ 70%. Increasing the number of queues beyond this offers only marginal benefits, reinforcing PASE's ability to achieve high performance with existing switches that support a limited number of priority queues.

#### Reference Rate

We evaluate the benefit of using reference rate information. Figure 13(a) compares PASE with PASE-DCTCP, where all flows behave as DCTCP sources and do not use the reference rate. Leveraging the reference rate results in PASE achieving 50% smaller AFCTs compared to PASE-DCTCP.

#### Impact of RTO and Probing

Flows mapped to lower priority queues may experience many timeouts, affecting performance. We implemented probing, where flows mapped to the lowest priority queue send header-only probe packets every RTT instead of full-sized packets. This improves performance by ≈2.4% and ≈11% at 80% and 90% loads, respectively, in the all-to-all intra-rack scenario. Unlike pFabric, PASE does not require small Retransmission Timeouts (RTOs), eliminating the need for high-resolution timers.

### Testbed Evaluation

Our testbed consists of a single rack with 10 nodes (9 clients, 1 server), 1 Gbps links, 250 µsec RTT, and a queue size of 100 packets on each interface. We set the marking threshold K to 20 packets and use 8 priority queues. We compare PASE's performance with DCTCP, generating flow sizes uniformly distributed between 100 KB and 500 KB. Figure 13(b) shows that PASE significantly outperforms DCTCP, achieving ≈50%-60% smaller AFCTs, consistent with ns2 simulation results.

### Related Work

We categorize prior works in terms of the underlying transport strategies they use:

#### Self-Adjusting Endpoints

Several data center transports use adaptive congestion control mechanisms, such as DCTCP, D2TCP, L2DCT, and MCP. These protocols do not support flow preemption and in-network prioritization, limiting their performance.

#### Arbitration

PDQ and D3 use network-wide arbitration but incur high flow switching overhead. PASE’s bottom-up approach to arbitration has similarities with EyeQ, which targets bandwidth guarantees in multi-tenant cloud data centers. PASE generalizes EyeQ’s arbitration to handle contention at links other than access links.

#### In-Network Prioritization

pFabric uses in-network prioritization through priority-based scheduling and packet dropping. DeTail focuses on minimizing tail latency but not average FCT. Virtual shapers address the challenge of limited rate limiters. While both DeTail and virtual shapers use in-network prioritization, they do not provide mechanisms for network-wide arbitration.

PASE combines these strategies to overcome individual limitations, achieving high performance across various scenarios while being deployment-friendly.

### Conclusion

PASE is a transport framework that synthesizes existing transport strategies. It is deployment-friendly, requiring no changes to the network fabric, and performs comparably or better than state-of-the-art protocols. PASE includes a scalable arbitration control plane tailored for typical data center topologies and an end-host transport that uses priority queues and information from arbitrators. We believe PASE sets a new direction for data center transports, where advances in specific techniques benefit everyone.

### Acknowledgements

We thank our shepherd, Nandita Dukkipati, and the SIGCOMM reviewers for their feedback. We also thank Thomas Karagiannis and Zafar Ayyub Qazi for their useful comments.

### References

[1] Arista 7050s switch. http://www.aristanetworks.com/docs/Manuals/ConfigGuide.pdf.
[2] Broadcom bcm56820 switch. http://www.broadcom.com/collateral/pb/56820-PB00-R.pdf.
[3] Dell force10 s4810 switch. http://www.force10networks.com/CSPortal20/KnowledgeBase/DOCUMENTATION/CLIConfig/FTOS/Z9000_CLI_8.3.11.4_23-May-2012.pdf.
[4] IBM RackSwitch G8264 application guide. http://www.bladenetwork.net/userfiles/file/G8264_AG_6-8.pdf.
[5] Juniper EX3300 switch. http://www.juniper.net/us/en/products-services/switching/ex-series/ex3300/.
[6] The Network Simulator - NS-2. http://www.isi.edu/nsnam/ns/.
[7] Prio qdisc Linux. http://linux.die.net/man/8/tc-prio, 2006.
[8] D. Abts and B. Felderman. A guided tour of data-center networking. Commun. ACM, 55(6):44–51, June 2012.
[9] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable, Commodity Data Center Network Architecture. In SIGCOMM’08.
[10] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat. Hedera: dynamic flow scheduling for data center networks. In NSDI’10.
[11] M. Alizadeh, A. G. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data center TCP (DCTCP). In SIGCOMM’10.
[12] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker. pFabric: Minimal near-optimal datacenter transport. In SIGCOMM’13.
[13] T. Benson, A. Akella, and D. A. Maltz. Network traffic characteristics of data centers in the wild. In IMC’10.
[14] L. Chen, S. Hu, K. Chen, H. Wu, and D. H. K. Tsang. Towards minimal-delay deadline-driven data center TCP. In Hotnets’13.
[15] A. Curtis, J. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma, and S. Banerjee. DevoFlow: Scaling flow management for high-performance networks. In SIGCOMM’11.
[16] J. Dean and S. Ghemawat. MapReduce: Simplified Data Processing on Large Clusters. In OSDI’04.
[17] F. R. Dogar, T. Karagiannis, H. Ballani, and A. Rowstron. Decentralized Task-aware Scheduling for Data Center Networks. In SIGCOMM’14.
[18] C. Hong, M. Caesar, and P. Godfrey. Finishing flows quickly with preemptive scheduling. In SIGCOMM’12.
[19] V. Jeyakumar, M. Alizadeh, D. Mazieres, B. Prabhakar, C. Kim, and A. Greenberg. EyeQ: Practical Network Performance Isolation at the Edge. In NSDI’13.
[20] D. Katabi, M. Handley, and C. Rohrs. Congestion Control for High Bandwidth-Delay Product Networks. In SIGCOMM’02.
[21] G. Kumar, S. Kandula, P. Bodik, and I. Menache. Virtualizing traffic shapers for practical resource allocation. In HotCloud’13.
[22] A. Munir, I. A. Qazi, Z. A. Uzmi, A. Mushtaq, S. N. Ismail, M. S. Iqbal, and B. Khan. Minimizing Flow Completion Times in Data Centers. In INFOCOM’13.
[23] B. Vamanan, J. Hasan, and T. Vijaykumar. Deadline-aware datacenter TCP (D2TCP). In SIGCOMM’12.
[24] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron. Better never than late: Meeting deadlines in datacenter networks. In SIGCOMM’11.
[25] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. Katz. DeTail: Reducing the flow completion time tail in datacenter networks. In SIGCOMM’12.