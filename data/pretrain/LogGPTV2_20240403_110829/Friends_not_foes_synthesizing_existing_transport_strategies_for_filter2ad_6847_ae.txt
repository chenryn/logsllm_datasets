whereas it achieves similar performance (< 6% diﬀerence in
AFCTs) compared to pFabric in the following two cases: (a)
single bottleneck scenarios and (b) when the network load
is typically less than 80%.
We ﬁrst consider the left-right inter-rack scenario where
the aggregation-core link becomes the bottleneck. Figure
10(a) shows the 99th percentile FCT as a function of load.
Observe that pFabric achieves smaller FCT for up to 50%
load and PASE achieves comparable performance. However,
at ≥ 60% loads, PASE results in smaller FCT than pFabric.
At 90% load, this improvement is more than 85%. This
happens due to high and persistent loss rate with pFabric
at high loads. Figure 10(b) shows the CDF of FCT at 70%
load for the same scenario.
Next we consider an all-to-all intra-rack scenario, which
is common in applications like web search where responses
from several worker nodes within a rack are combined by an
aggregator node before a ﬁnal response is sent to the user.
Moreover, any node within a rack can be an aggregator node
for a user query and the aggregators are picked in a round
robin fashion to achieve load balancing [8].
Figure 10(c) shows that PASE provides up to 85% im-
provement in AFCT over pFabric and results in lower
AFCTs across all loads. This happens because with pFabric
multiple ﬂows sending at line rate can collide at a down-
stream ToR-host link. This causes a signiﬁcant amount of
network capacity to be wasted on the host-ToR links, which
could have been used by other ﬂows (as this is an all-to-all
scenario). With PASE, ﬂows do not incur any arbitration
latency in the intra-rack scenario as new ﬂows start sending
traﬃc at line rate based on the information (priority and
reference rate) from their local arbitrator. After one RTT,
all ﬂows obtain their global priorities which helps in avoid-
ing any persistent loss of throughput in case the local and
global priorities are diﬀerent.
4.3 Micro-benchmarks
In this section, we evaluate the basic components of PASE
with the help of several micro-benchmarks. Our results show
that PASE optimizations signiﬁcantly reduce the control
traﬃc overhead and the number of messages that arbitrators
need to process. In addition, we ﬁnd that other features of
PASE such as its rate control are important for achieving
high performance.
4.3.1 Arbitration Optimizations
PASE introduces early pruning and delegation for reduc-
ing the arbitration overhead of update messages. We now
evaluate the overhead reduction brought about by these
optimizations as well as study their impact on the perfor-
mance of PASE. Figure 11(b) shows the overhead reduction
that is achieved by PASE when all its optimizations are en-
abled. Observe that these optimizations provide up to 50%
reduction in arbitration overhead especially at high loads.
This happens because when these optimizations are enabled,
higher-level arbitrators delegate some portion of the band-
width to lower level arbitrators, which signiﬁcantly reduces
 1 2 3 4 5 6 7 8 9 10 10 20 30 40 50 60 70 80 90AFCT (msec)Offered load (%)PASEL2DCTDCTCP 0 20 40 60 80 100 0 2 4 6 8 10Fraction of Flows (cdf)Time (msec)PASEL2DCTDCTCP 0 0.2 0.4 0.6 0.8 1 10 20 30 40 50 60 70 80 90App. ThroughputOffered load (%)PASED2TCPDCTCP(a) left-right inter-rack
(b) left-right inter-rack
(c) all-to-all intra-rack
Figure 10: Comparison of PASE with pFabric under (a) 99th percentile FCT in the left-right inter-rack
scenario, (b) CDF of FCTs under the left-right scenario at 70% load, and (c) all-to-all intra-rack scenario.
(a) AFCT
(b) Overhead
Figure 11: AFCT improvement and overhead reduc-
tion as a function of load for PASE with its optimiza-
tions in the left-right scenario.
(a) AFCT
(b) Priority Queues
Figure 12: (a) Comparison of AFCT with and with-
out end-to-end arbitration and (b) PASE with vary-
ing number of priority queues (left-right scenario).
the control overhead on ToR-Aggregation links. In addition,
updates of only those ﬂows are propagated that map to the
highest priority queues due to early pruning. Figure 11(a)
shows that these optimizations also improve the AFCT by
4-10% across all loads. This happens because of delegation,
which reduces the arbitration delays.
Beneﬁt of end-to-end arbitration: PASE enables
global prioritization among ﬂows through its scalable end-
to-end arbitration mechanism. This arbitration, however,
requires additional update messages to be sent on the net-
work.
It is worth asking if most of the beneﬁts of PASE
are realizable through only local arbitration, which can be
solely done by the endpoints. Thus, we now compare the
performance of end-to-end arbitration and local arbitration
in the left-right inter-rack scenario. Figure 12(a) shows that
end-to-end arbitration leads to signiﬁcant improvements (up
to 60%) in AFCTs across a wide range of loads. This hap-
pens because local arbitration cannot account for scenarios
where contention does not occur on the access links, thus
leading to sub-optimal performance.
(a) PASE vs PASE-DCTCP (b) Testbed Evaluation
Figure 13: (a) Comparison of PASE with PASE-
DCTCP in the intra-rack scenario with 20 nodes
and uniformly distributed ﬂow sizes from [100 KB,
500 KB] and (b) Testbed Evaluation: Comparison of
AFCT for PASE with DCTCP.
4.3.2
In-network Prioritization
Micro-benchmarks
and Transport
Impact of number of priority queues: We now eval-
uate the impact of changing the number of priority queues
in the switches. To test this scenario we repeat the left-
right inter-rack scenario with diﬀerent number of queues.
Figure 12(b) shows that using 4 queues provide signiﬁcant
improvement in AFCT at loads ≥ 70%. However, increasing
the number of queues beyond this provides only marginal
improvement in AFCT. These results further reinforce the
ability of PASE to achieve high performance with existing
switches that support a limited number of priority queues.
Reference Rate: We now evaluate the beneﬁt of us-
ing the reference rate information. We compare PASE with
PASE-DCTCP, where all ﬂows (including the ones mapped
to the top queue as well as the lowest priority queue) be-
have as DCTCP sources and do not use the reference rate.
However, these ﬂows are mapped to diﬀerent priority queues
through the normal arbitration process. As shown in Figure
13(a), leveraging reference rate results in AFCTs of PASE
to be 50% smaller than the AFCTs for PASE-DCTCP.
Impact of RTO and Probing: Flows that are mapped
to the lower priority queues may experience large number of
timeouts, which can aﬀect performance. We implemented
probing in which ﬂows mapped to the lowest priority queue
send a header-only probe packet every RTT rather than a
full-sized packet. We found that using probing improves
performance by ≈2.4% and ≈11% at 80% and 90% loads,
respectively in the all-to-all intra-rack scenario. Note that
unlike pFabric, PASE does not require small RTOs which
forgoes the need to have high resolution timers.
 1 10 100 10 20 30 40 50 60 70 80 90FCT (msec)- log scaleOffered load (%)PASEpFabric 0 20 40 60 80 100 0 1 2 3 4 5 6Fraction of Flows (cdf)Time (msec)PASEpFabric 1 10 100 1000 10 20 30 40 50 60 70 80 90AFCT (msec)Offered load (%)20.60.631029768535% ImprovementPASEpFabric 0 2 4 6 8 10 10 20 30 40 50 60 70 80 90AFCT Improvement (%)Offered load (%) 0 10 20 30 40 50 60 10 20 30 40 50 60 70 80 90Overhead Reduction (%)Offered load (%) 0 1 2 3 4 5 10 20 30 40 50 60 70 80 9095AFCT (msec)Offered load (%)Arbitration=ONArbitration=OFF 2 4 6 8 10 12 10 20 30 40 50 60 70 80 90AFCT (msec)Offered load (%)3 Queues4 Queues6 Queues8 Queues 2 4 6 8 10 12 14 16 10 20 30 40 50 60 70 80 90AFCT (msec)Offered load (%)PASEPASE-DCTCP 10 20 30 40 50 10 20 30 40 50 60 70 80 90AFCT (msec)Offered load (%)PASEDCTCP4.4 Testbed Evaluation
We now present a subset of results from our testbed eval-
uation. Our testbed comprises of a single rack of 10 nodes (9
clients, one server), with 1 Gbps links, 250 µsec RTT and a
queue size of 100 packets on each interface. We set the mark-
ing threshold K to 20 packets and use 8 priority queues. We
compare PASE’s performance with the DCTCP implemen-
tation (provided by its authors). To emulate data center
like settings, we generate ﬂows sizes that are uniformly dis-
tributed between 100 KB and 500 KB, as done in [23]. We
start 1000 short ﬂows and vary the ﬂow arrival rate to gener-
ate a load between 10% to 90%. In addition, we also gener-
ate a long lived background ﬂow from one of the clients. We
compare PASE with DCTCP and report the average of ten
runs. Figure 13(b) shows the AFCT for both the schemes.
Observe that PASE signiﬁcantly outperforms DCTCP: it
achieves ≈50%-60% smaller AFCTs compared to DCTCP.
This also matches the results we observed in ns2 simulations.
5. RELATED WORK
We now brieﬂy describe and contrast our work with the
most relevant research works. We categorize prior works in
terms of the underlying transport strategies they use.
Self-Adjusting Endpoints: Several data center trans-
ports use this strategy [11, 14, 22, 23]. DCTCP uses an
adaptive congestion control mechanism based on ECN to
maintain low queues. D2TCP and L2DCT add deadline-
awareness and size-awareness to DCTCP, respectively. MCP
[14] improves performance over D2TCP by assigning more
precise ﬂow rates using ECN marks. These rates are based
on the solution of a stochastic delay minimization prob-
lem. These protocols do not support ﬂow preemption and
in-network prioritization, which limits their performance.
Arbitration: PDQ [18] and D3 [24] use network-wide ar-
bitration but incur high ﬂow switching overhead. PASE’s
bottom up approach to arbitration has similarities with
EyeQ [19], which targets a diﬀerent problem of providing
bandwidth guarantees in multi-tenant cloud data centers.
PASE’s arbitration mechanism generalizes EyeQ’s arbitra-
tion by dealing with scenarios where contention can happen
at links other than the access links.
In-network Prioritization:
pFabric [12] uses in-
network prioritization by doing priority-based scheduling
and dropping of packets. DeTail [25], a cross layer net-
work stack, focuses on minimizing the tail latency but does
not target the average FCT. In [21], authors propose vir-
tual shapers to overcome the challenge of limited number
of rate limiters. While both DeTail and virtual shapers use
in-network prioritization, they do not deal with providing
mechanisms for achieving network-wide arbitration.
In general, prior proposals target one speciﬁc transport
strategy, PASE uses all these strategies in unison to over-
come limitations of individual strategies and thus achieves
high performance across a wide range of scenarios while be-
ing deployment friendly.
6. CONCLUSION
We proposed PASE, a transport framework that synthe-
sizes existing transport strategies. PASE is deployment
friendly as it does not require any changes to the network
fabric and yet, its performance is comparable to, or bet-
ter than the state-of-the-art protocols that require changes
to network elements. The design of PASE includes a scal-
able arbitration control plane which is speciﬁcally tailored
for typical data center topologies, and an end-host transport
that explicitly uses priority queues and information from ar-
bitrators. We believe that PASE sets out a new direction
for data center transports, where advances in speciﬁc tech-
niques (e.g., better in-network prioritization mechanisms or
improved control laws) beneﬁt everyone.
Acknowledgements: We thank our shepherd, Nandita
Dukkipati, and the SIGCOMM reviewers for their feedback.
We also thank Thomas Karagiannis and Zafar Ayyub Qazi
for their useful comments.
7. REFERENCES
[1] Arista 7050s switch.
http://www.aristanetworks.com/docs/Manuals/ConfigGuide.pdf.
[2] Broadcom bcm56820 switch.
http://www.broadcom.com/collateral/pb/56820-PB00-R.pdf.
[3] Dell force10 s4810 switch. http://www.force10networks.com/
CSPortal20/KnowledgeBase/DOCUMENTATION/CLIConfig/FTOS/
Z9000_CLI_8.3.11.4_23-May-2012.pdf.
[4] Ibm rackswitch g8264 application guide.
http://www.bladenetwork.net/userfiles/file/G8264_AG_6-8.pdf.
[5] Juniper ex3300 switch. http://www.juniper.net/us/en/
products-services/switching/ex-series/ex3300/.
[6] The network simulator - ns-2. http://www.isi.edu/nsnam/ns/.
[7] Prio qdisc linux. http://linux.die.net/man/8/tc-prio, 2006.
[8] D. Abts and B. Felderman. A guided tour of data-center
networking. Commun. ACM, 55(6):44–51, June 2012.
[9] M. Al-Fares, A. Loukissas, and A. Vahdat. A Scalable,
Commodity Data Center Network Architecture. In
SIGCOMM’08.
[10] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: dynamic ﬂow scheduling for data center
networks. In NSDI’10.
[11] M. Alizadeh, A. G. Greenberg, D. A. Maltz, J. Padhye,
P. Patel, B. Prabhakar, S. Sengupta, and M. Sridharan. Data
center TCP (DCTCP). In SIGCOMM’10.
[12] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown,
B. Prabhakar, and S. Shenker. pfabric: Minimal near-optimal
datacenter transport. In SIGCOMM’13.
[13] T. Benson, A. Akella, and D. A. Maltz. Network traﬃc
characteristics of data centers in the wild. In IMC’10.
[14] L. Chen, S. Hu, K. Chen, H. Wu, and D. H. K. Tsang. Towards
minimal-delay deadline-driven data center tcp. In Hotnets’13.
[15] A. Curtis, J. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma,
and S. Banerjee. Devoﬂow: scaling ﬂow management for
high-performance networks. In SIGCOMM’11.
[16] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data
Processing on Large Clusters. In OSDI’04.
[17] F. R. Dogar, T. Karagiannis, H. Ballani, and A. Rowstron.
Decentralized Task-aware Scheduling for Data Center
Networks. In SIGCOMM’14.
[18] C. Hong, M. Caesar, and P. Godfrey. Finishing ﬂows quickly
with preemptive scheduling. In SIGCOMM’12.
[19] V. Jeyakumar, M. Alizadeh, D. Mazieres, B. Prabhakar,
C. Kim, and A. Greenberg. EyeQ: Practical Network
Performance Isolation at the Edge. In NSDI’13.
[20] D. Katabi, M. Handley, and C. Rohrs. Congestion Control for
High Bandwidth-Delay Product Networks. In SIGCOMM’02.
[21] G. Kumar, S. Kandula, P. Bodik, and I. Menache. Virtualizing
traﬃc shapers for practical resource allocation. In HotCloud’13.
[22] A. Munir, I. A. Qazi, Z. A. Uzmi, A. Mushtaq, S. N. Ismail,
M. S. Iqbal, and B. Khan. Minimizing Flow Completion Times
in Data Centers. In INFOCOM’13.
[23] B. Vamanan, J. Hasan, and T. Vijaykumar. Deadline-aware
datacenter tcp (d2tcp). In SIGCOMM’12.
[24] C. Wilson, H. Ballani, T. Karagiannis, and A. Rowstron. Better
never than late: Meeting deadlines in datacenter networks. In
SIGCOMM’11.
[25] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. Katz. Detail:
Reducing the ﬂow completion time tail in datacenter networks.
In SIGCOMM’12.