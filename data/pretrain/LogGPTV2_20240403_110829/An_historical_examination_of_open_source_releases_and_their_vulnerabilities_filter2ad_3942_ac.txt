### Weak Correlation in Issues Reported by Different Tools

There is a significant body of work, including the seminal contributions of Brooks [4], that explores the general pathology of bugs: their causes, lifecycle, and prevention. Our focus, however, has not been on exploring this pathology. Instead, we have investigated what static analysis of source code can tell us about the number of security bugs in software.

### 7. Conclusion

In this paper, we presented our analysis of the vulnerability history of various popular open-source software using a static source code analyzer and the rate at which Common Vulnerabilities and Exposures (CVE) entries occur. We demonstrated that the changes in the number of analyzer-identified security issues between releases of the same software are statistically significantly correlated with the rate of occurrences of vulnerabilities in CVE. This correlation, which we take to be a measure of the number of exploitable bugs in the code, is subject to the limitations described in Sections 1 and 5. The correlation is moderate, as many other factors are at play, some of which we have discussed. 

The moderate correlation suggests that an increase in the number of issues (or issue density) found by Static Code Analysis (SCA) in a new release is an indication of an increase in CVEs per year, and therefore, the number of exploitable bugs. Conversely, a decrease in issues or issue density indicates a reduction in CVEs per year and, consequently, a reduction in the number of exploitable bugs.

This also demonstrates that static source code analysis can be used to make some assessment of risk, even when constraints do not permit human review of the issues identified by the analysis. This is necessarily an imprecise assessment of risk, as the correlation between exploitable bugs and changes in issues and issue density is moderate, not strong. Additionally, some defects may pose greater risks than others.

Both the number and density of analyzer-identified issues need to be considered when evaluating whether code quality has improved. If a large amount of new code has been added in a new release, even though there is a drop in issue density, the total number of issues might have increased, indicating the potential for a higher number of vulnerabilities. Our analysis shows that software quality, as measured by our metrics, does not always improve with each new release. The introduction of large amounts of new code can decrease quality. The discovery of new classes of bugs can lead to an increase in the rate of CVE entries [19]. However, generally, the rate of CVE entries begins to drop three to five years after the initial release.

The degree of scrutiny is important. If there are few or no known issues, it does not mean that the software has no security issues. As shown by our analysis of the reporting history for the databases in Section 5, it may mean that the degree of scrutiny by the security community is low. This might reflect that the software in question is not often accessible directly as an Internet service. For example, unlike email or web servers, databases are often only accessible indirectly via other services such as application servers, possibly reducing the incentive to scrutinize the software for exploitable bugs. There could be a large number of bugs in the software that are exploitable by an attacker if they can somehow deliver the attack to the installation, perhaps through one or more levels of indirection. The Stuxnet attack [9] on industrial controllers is an example of this. Indirection was used to target an endpoint whose software had been subject to little scrutiny and which was not accessible directly to the attackers.

Our results demonstrate that static source code analysis cannot be used to accurately compare the number of vulnerabilities likely to be present in different software or release series. We demonstrated this by showing weak correlations with CVE/yr to absolute values of metrics generated from the analyzer and by a qualitative comparison of Apache httpd 2.0, Postfix, and Sendmail. It is possible that a large order of magnitude difference (e.g., greater than 10x) in absolute values may be significant, but the size of our dataset with such differences is too small to be definitive.

### Areas for Future Investigation

There are several areas for potential future investigation. We used a single static analysis tool for our investigation. A possible next step is to compare different tools to see how the correlation with the rate of CVE entries varies. It would also be useful to match specific CVE entries with corresponding issues identified by static analysis, enabling us to determine what percentage of CVE entries are detected by static analysis. However, the challenges in doing so are numerous. For example, identifying the specific line or lines of code that correspond to a CVE entry may require studying patches or sample exploits, which is difficult to automate and therefore very time-consuming. Another potential area for future research is the effect of API design, leading to exploitable bugs when software is used as a component of a larger system. For example, in 2009, 29 of the 41 CVE entries for OpenSSL were due to mistakes in its use by third-party software. We deliberately excluded such entries from this study.

### Acknowledgments

We express our gratitude to the large community of developers and visionaries who have given and continue to give the Internet community so much useful software over the years. Mistakes are inevitable in any new pioneering human endeavor. We hope that our analysis has shown how the community has learned from these mistakes and continues to learn.

We thank Chris Dalton, Jonathan Griffin, Keith Harrison, Jack Herrington, Bill Horne, Matias Madou, Brian Monahan, Miranda Mowbray, Martin Sadler, Jacob West, and Mike Wray for their help and advice. We are also extremely grateful to the anonymous reviewers for their many constructive comments.

### References

[1] O. H. Alhazmi and Y. K. Malaiya. Quantitative vulnerability assessment of systems software. In Proceedings of the IEEE Reliability and Maintainability Symposium, pp. 615-620, 2005.
[2] Apache release history. http://www.apachehaus.com/.
[3] N. Ayewah, W. Pugh, J. D. Morgenthaler, J. Penix, and Y. Zhou. Evaluating static analysis defect warnings on production software. In PASTE ’07 Proceedings of the 7th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering, pp. 1-8, 2007.
[4] F. P. Brooks. The Mythical Man Month and Other Essays on Software Engineering. Addison Wesley, 1975, 1995 (2nd Ed.).
[5] B. Chess and J. West. Secure Programming with Static Analysis. Pearson Education Inc., Boston, Massachusetts, 2007.
[6] B. V. Chess. Improving computer security using extended static checking. In Proceedings of IEEE Symposium on Security and Privacy, pp. 160-173, 2002.
[7] S. Clark, S. Frei, M. Blaze, and J. Smith. Familiarity breeds contempt: the honeymoon effect and the role of legacy code in zero-day vulnerabilities. In ACSAC ’10: Proceedings of the 26th Annual Computer Security Applications Conference, pp. 251-260, December 2010.
[8] M. Doyle and J. Walden. An empirical study of the evolution of PHP web application security. In International Workshop On Security Measurments and Metrics, MetriSec, 2011.
[9] N. Falliere, L. O. Murchu, and E. Chien. W32.stuxnet dossier, version 1.4 (February 2011). http://www.symantec.com/.
[10] R. Gopalakrishna and E. H. Spafford. A trend analysis of vulnerabilities. In Technical Report 2005-05, CERIAS, Purdue University, May 2005.
[11] W. Landi. Undecidability of static analysis. ACM Letters on Programming Languages and Systems (LOPLAS), 4(1):323–337, December 1992.
[12] The common vulnerabilities and exposures dictionary. http://cve.mitre.org/.
[13] N. Nagappan and T. Ball. Static analysis tools as early indicators of pre-release defect density. In ICSE ’05 Proceedings of the 27th international conference on Software engineering, pp. 580-586, 2005.
[14] National vulnerability database. http://nvd.nist.gov/.
[15] October 2011 web server survey. http://news.netcraft.com/.
[16] F. Nielson, H. R. Nielson, and C. Hankin. Principles of Program Analysis. Springer-Verlag, Berlin, Germany, 2005, 2nd Ed.
[17] V. Okun, W. Guthrie, R. Gaucher, and P. Black. Effect of static analysis tools on software security: preliminary investigation. In QoP ’07: Proceedings of the 2007 ACM workshop on Quality of protection, pp. 1-5, October 2007.
[18] A. Ozment. The likelihood of vulnerability rediscovery and the social utility of vulnerability hunting. In Workshop on the Economics of Information Security (WEIS), Cambridge, MA, USA, June 2005.
[19] A. Ozment. Improving vulnerability discovery models. In QoP ’07: Proceedings of the 2007 ACM workshop on Quality of protection, pp. 6-11, October 2007.
[20] A. Ozment and S. E. Schechter. Milk or wine: does software security improve with age? In Proceedings of the 15th conference on USENIX Security Symposium - Volume 15, pp. 93-104, 2006.
[21] E. Rescorla. Is finding security holes a good idea? IEEE Security & Privacy, 3(1):14–19, February 2005.
[22] H. Rice. Classes of recursively enumerable sets and their decision problems. Trans. Amer. Math. Soc., 74(2):358–366, March 1953.
[23] N. Rutar, C. B. Almazan, and J. S. Foster. A comparison of bug finding tools for Java. In ISSRE ’04 Proceedings of the 15th International Symposium on Software Reliability Engineering, pp. 245-256, October 2007.
[24] Securityfocus vulnerability database. http://www.securityfocus.com/vulnerabilities.
[25] E. H. Spafford. The internet worm program: An analysis. ACM SIGCOMM Computer Communication Review, 19(1):17–57, January 1989.

### Appendix A: Analysis Results

This appendix provides the analysis results for the software discussed in this paper, presented in Tables 5 to 8. Table 4 shows the number of CVE entries per year from 1999 to 2011 for the analyzed software. "OS" in Table 4 denotes OpenSSL. In subsequent tables, the software analyzed is identified by the version number and release date. The release date was determined from the timestamp on the software archive for that release. In Tables 5 to 8, "LOC" is the number of lines of executable code as measured by SCA. "CI," "HI," and "LI" are respectively the number of "Critical," "High," and "Low" issues measured by SCA. "CHI" is the sum of "CI" and "HI." "TI" is the "Total issues" — the sum of "CI," "HI," and "LI." For a fuller explanation of these and the metrics shown in other columns, please see Section 3. In Table 5, the CVE/yr calculation for release 8.7.6 of Sendmail does not take into account 1996-1998 inclusive, since no CVE information is available for these years. In Table 7, "N/A" denotes not applicable — Apache 1.3.0 was displaced by 1.3.2 before CVE data was available.

#### Table 4: CVE Entries

| Year | Sendmail | Postfix | httpd 1.3 | httpd 2.0 | httpd 2.2 | OS 0.9.6 | OS 0.9.7 | OS 0.9.8 | OS 1.0.0 |
|------|----------|---------|-----------|-----------|-----------|----------|----------|----------|----------|
| 1999 | 16       | 3       | 6         | 6         | 7         | 0        | 0        | 5        | 1        |
| 2000 | 0        | 2       | 0         | 2         | 4         | 0        | 0        | 2        | 4        |
| 2001 | 5        | 11      | 11        | 10        | 3         | 3        | 6        | 3        | 0        |
| 2002 | 0        | 1       | 4         | 0         | 1         | 7        | 0        | 0        | 0        |
| 2003 | 2        | 0       | 4         | 7         | 0         | 0        | 0        | 0        | 0        |
| 2004 | 0        | 0       | 6         | 15        | 6         | 7        | 9        | 0        | 0        |
| 2005 | 0        | 0       | 4         | 7         | 4         | 3        | 5        | 1        | 3        |
| 2006 | 0        | 0       | 4         | 7         | 4         | 4        | 4        | 0        | 0        |
| 2007 | 0        | 0       | 0         | 0         | 0         | 0        | 0        | 0        | 0        |
| 2008 | 0        | 0       | 0         | 0         | 0         | 0        | 0        | 0        | 0        |
| 2009 | 3        | 4       | 8         | 5         | 1         | 1        | 7        | 0        | 0        |
| 2010 | 0        | 0       | 0         | 0         | 0         | 0        | 0        | 0        | 0        |
| 2011 | 0        | 0       | 0         | 0         | 0         | 0        | 0        | 0        | 0        |

#### Table 5: Sendmail

| Version | Date       | LOC    | CI  | HI  | LI  | CHI | TI  | C-density | CH-density | T-density | CVE/yr |
|---------|------------|--------|-----|-----|-----|-----|-----|-----------|------------|-----------|--------|
| 8.7.6   | 17/09/1996 | 11861  | 136 | 1332 | 1080 | 1468 | 2478 | 0.011     | 0.124      | 0.209     | 0.00   |
| 8.9.3   | 05/02/1999 | 15099  | 118 | 1449 | 931 | 1567 | 2508 | 0.008     | 0.104      | 0.166     | 0.00   |
| 8.10.0  | 06/03/2000 | 10381  | 120 | 91   | 451 | 211  | 662  | 0.012     | 0.020      | 0.064     | 0.00   |
| 8.11.0  | 19/07/2000 | 10617  | 124 | 95   | 453 | 219  | 686  | 0.012     | 0.021      | 0.065     | 0.00   |
| 8.11.6  | 20/08/2001 | 10999  | 121 | 88   | 456 | 209  | 666  | 0.011     | 0.019      | 0.061     | 0.00   |
| 8.12.0  | 08/09/2001 | 15769  | 76  | 252  | 220 | 328  | 548  | 0.005     | 0.021      | 0.035     | 0.00   |
| 8.12.6  | 27/08/2002 | 16195  | 76  | 257  | 230 | 333  | 563  | 0.005     | 0.021      | 0.035     | 0.00   |
| 8.13.0  | 20/06/2004 | 31668  | 84  | 473  | 255 | 557  | 812  | 0.003     | 0.018      | 0.026     | 0.00   |
| 8.13.5  | 16/09/2005 | 31902  | 84  | 474  | 255 | 558  | 813  | 0.003     | 0.018      | 0.026     | 0.00   |
| 8.14.0  | 01/02/2007 | 32146  | 87  | 511  | 246 | 598  | 844  | 0.003     | 0.019      | 0.026     | 0.00   |
| 8.14.5  | 15/09/2011 | 32270  | 87  | 510  | 244 | 597  | 841  | 0.003     | 0.019      | 0.026     | 0.00   |