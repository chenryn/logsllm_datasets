weak correlation in issues reported by diﬀerent tools:
[13],
[23].
There is a signiﬁcant body of work, including the work of
Brooks [4], on software engineering that explores the general
pathology of bugs: their causes, their lifecycle and what
can be down to prevent them. The focus of our work has
not been to explore the pathology. Rather it has been to
investigate what static analysis of source code can tell us
about the number of security bugs in software.
7. CONCLUSION
In this paper we presented our analysis of the vulnerability
history of various popular open source software using a static
source code analyzer and the rate at which CVE entries oc-
cur. We have demonstrated that the changes in the number
of analyzer identiﬁed security issues between releases of the
same software are statistically signiﬁcantly correlated to the
rate of occurrences of vulnerabilities in CVE, which we take
to be a measure of the number of exploitable bugs in the
code, subject to the limitations described in section 1 and
section 5. The correlation is moderate, as many other fac-
tors are at work, some of which we have discussed. The
correlation is suﬃcient to suggest that that an increase in
the number of issues (or issue density) found by SCA in a
new release is an indication of increase in in CVE/yr and
therefore the number of exploitable bugs. Correspondingly
a decrease in issues or issue-density is an indication of reduc-
tion in CVE/yr and therefore in the number of exploitable
bugs.
This also demonstrates that static source code analysis
can be used to make some assessment of risk even when
constraints do not permit human review of the issues identi-
ﬁed by the analysis. This is necessarily an imprecise assess-
ment of risk: the correlation between exploitable bugs and
changes in issues and issue density is moderate, not strong.
In addition, some defects are likely to pose greater risks than
others.
Both the number and density of analyzer identiﬁed is-
sues need to be considered when asking the question, has
the code quality improved? If a large amount of new code
has been added in the new release, even though there is a
drop in the issue density, the total number of issues might
have increased indicating the potential for a higher number
of vulnerabilities. Our analysis demonstrates that software
quality, as measured by our metrics does not always improve
with each new release. The introduction of large amounts of
new code can decrease quality. The discovery of new classes
of bug can lead to an increase in the rate of CVE entries
[19]. However, generally the rate of CVE entries begins to
drop three to ﬁve years after the initial release.
The degree of scrutiny is important. If there are few or
no issues known it does not mean that the software has no
security issues. As is shown by our analysis of the reporting
history for the databases in section 5, it may mean that the
degree of scrutiny by the security community is low. This
might be a reﬂection that the software in question is not of-
ten accessible directly as an Internet service. For example,
unlike email or web servers, databases are often only acces-
sible indirectly via other services such as application servers,
possibly reducing the incentive to scrutinize the software for
exploitable bugs. There could be a large number of bugs in
the software that are exploitable by an attacker if they can
somehow deliver the attack to the installation, perhaps by
one or more levels of indirection. The Stuxnet attack [9] on
industrial controllers is an example of this. Indirection was
used to target an endpoint whose software had been subject
to little scrutiny and which was not accessible directly to
the attackers.
Our results demonstrate that static source code analysis
cannot be used to compare accurately the number of vul-
nerabilities that are likely to be in diﬀerent software or re-
lease series. We demonstrated this by weak correlations with
CVE/yr to absolute values of metrics generated from the
analyzer and by a qualitative comparison of Apache httpd
2.0, Postﬁx and Sendmail. It is possible that a large order
of magnitude diﬀerence (e.g. greater than 10x) in absolute
values may be signiﬁcant, but the size of our dataset with
orders of magnitude diﬀerences is too small to be deﬁnitive.
There are a number of areas for potential future inves-
tigation. We used a single static analysis tool for our in-
vestigation. A possible next step is to compare diﬀerent
tools: to see how correlation with rate of CVE entries varies
between tools. It would be useful to match speciﬁc CVE en-
tries with corresponding issues identiﬁed by static analysis.
This would enable us to say what percentage of CVE entries
are detected by static analysis. However, the challenges in
doing so are numerous. For example, identifying the speciﬁc
line or lines of code that correspond to a CVE entry may re-
quire studying patches or sample exploits which is diﬃcult
to automate and therefore very time consuming. Another
potential area for future research is the eﬀect of API design,
leading to exploitable bugs when software is used as a com-
ponent of a larger system. For example in 2009, 29 of the 41
CVE entries for OpenSSL were due to mistakes in its use by
191third party software. We deliberately excluded such entries
from this study.
We close by expressing our gratitude to the large commu-
nity of developers and visionaries who have given and con-
tinue to give the Internet community so much useful software
over the years. Mistakes are inevitable in any new pioneering
human endeavor. We hope that our analysis has shown how
the community has learnt from these mistakes and continues
to learn.
8. ACKNOWLEDGMENTS
We thank Chris Dalton, Jonathan Griﬃn, Keith Harrison,
Jack Herrington, Bill Horne, Matias Madou, Brian Mon-
ahan, Miranda Mowbray, Martin Sadler, Jacob West and
Mike Wray for their help and advice. We are also extremely
grateful to the anonymous reviewers for their many con-
structive comments.
9. REFERENCES
[1] O. H. Alhazmi and Y. K. Malaiya. Quantitative
vulnerability assessment of systems software. In
Proceedings of the IEEE Reliability and
Maintainability Symposium, pp615-620, 2005.
[2] Apache release history. http://www.apachehaus.com/.
[3] N. Ayewah, W. Pugh, J. D. Morgenthaler, J. Penix,
and Y. Zhou. Evaluating static analysis defect
warnings on production software. In PASTE ’07
Proceedings of the 7th ACM SIGPLAN-SIGSOFT
workshop on Program analysis for software tools and
engineering, pp1-8, 2007.
[4] F. P. Brooks. The Mythical Man Month and Other
Essays on Software Engineering. Addison Wesley,
1975, 1995(2nd Ed.).
[5] B. Chess and J. West. Secure Programming with Static
Analysis. Pearson Education Inc., Boston,
Massachusetts, 2007.
[6] B. V. Chess. Improving computer security using
extended static checking. In Proceedings of IEEE
Symposium on Security and Privacy, pp160-173, 2002.
[7] S. Clark, S. Frei, M. Blaze, and J. Smith. Familiarity
breeds contempt: the honeymoon eﬀect and the role of
legacy code in zero-day vulnerabilities. In ACSAC ’10:
Proceedings of the 26th Annual Computer Security
Applications Conference, pp251-260, December 2010.
[8] M. Doyle and J. Walden. An empirical study of the
evolution of PHP web application security. In
International Workshop On Security Measurments and
Metrics, MetriSec, 2011.
[9] N. Falliere, L. O. Murchu, and E. Chien. W32.stuxnet
dossier, version 1.4 (February 2011).
http://www.symantec.com/.
[10] R. Gopalakrishna and E. H. Spaﬀord. A trend analysis
of vulnerabilities. In Technical Report 2005-05,
CERIAS, Purdue University, May 2005.
[11] W. Landi. Undecidability of static analysis. ACM
Letters on Programming Languages and Systems
(LOPLAS), 4(1):323–337, December 1992.
[12] The common vulnerabilities and exposures dictionary.
http://cve.mitre.org/.
[13] N. Nagappan and T. Ball. Static analysis tools as
early indicators of pre-release defect density. In ICSE
’05 Proceedings of the 27th international conference on
Software engineering, pp580-586, 2005.
[14] National vulnerability database. http://nvd.nist.gov/.
[15] October 2011 web server survey.
http://news.netcraft.com/.
[16] F. Nielson, H. R. Nielson, and C. Hankin. Principles
of Program Analysis. Springer-Verlag, Berlin,
Germany, 2005, 2nd Ed.
[17] V. Okun, W. Guthrie, R.Gaucher, and P. Black. Eﬀect
of static analysis tools on software security:
preliminary investigation. In QoP ’07: Proceedings of
the 2007 ACM workshop on Quality of protection
pp1-5, October 2007.
[18] A. Ozment. The likelihood of vulnerability rediscovery
and the social utility of vulnerability hunting. In
Workshop on the Economics of Information Security
(WEIS), Cambridge, MA, USA, June 2005.
[19] A. Ozment. Improving vulnerability discovery models.
In QoP ’07: Proceedings of the 2007 ACM workshop
on Quality of protection, pp6-11, October 2007.
[20] A. Ozment and S. E. Schechter. Milk or wine: does
software security improve with age? In Proceedings of
the 15th conference on USENIX Security Symposium -
Volume 15, pp93-104, 2006.
[21] E. Rescorla. Is ﬁnding security holes a good idea?
IEEE Security & Privacy, 3(1):14–19, February 2005.
[22] H. Rice. Classes of recursively enumerable sets and
their decision problems. Trans. Amer. Math. Soc.,
74(2):358–366, March 1953.
[23] N. Rutar, C. B. Almazan, and J. S. Foster. A
comparison of bug ﬁnding tools for java. In ISSRE ’04
Proceedings of the 15th International Symposium on
Software Reliability Engineering, pp245-256, October
2007.
[24] Securityfocus vulnerability database.
http://www.securityfocus.com/vulnerabilities.
[25] E. H. Spaﬀord. The internet worm program: An
analysis. ACM SIGCOMM Computer Communication
Review, 19(1):17–57, January 1989.
APPENDIX
A. ANALYSIS RESULTS
This appendix gives the analysis results for the software
discussed in this paper in tables 5 to 8. Table 4 gives the
number of CVE entries per year from 1999 to 2011 for the
analyzed software. “OS” in table 4 denotes OpenSSL. In
subsequent tables the software analyzed is identiﬁed by the
version number and release date. The release date was de-
termined from the time stamp on the software archive for
that release. In tables 5 to 8 “LOC” is the number of lines
of executable code as measured by SCA. “CI”, “HI” and “LI”
are respectively the number of “Critical” “High” and “Low”
issues measured by SCA. “CHI” is the sum of “CI” and “HI”.
“TI” is the “Total issues” — the sum of “CI”, “HI” and “LI”.
For a fuller explanation of these and the metrics shown in
other columns please see section 3. In table 5 the CVE/yr
calculation for release 8.7.6 of Sendmail does not take into
account 1996-1998 inclusive, since no CVE information is
available for these years. In table 7 “N/A” denotes not ap-
plicable — Apache 1.3.0 was displaced by 1.3.2 before CVE
data was available.
192Year
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
Sendmail Postﬁx
httpd 1.3
Table 4: CVE Entries
httpd 2.0
httpd 2.2 OS 0.9.6 OS 0.9.7 OS 0.9.8 OS 1.0.0
16
3
6
6
7
0
0
5
1
0
2
0
0
0
0
1
0
2
2
1
0
0
4
0
0
2
4
5
11
11
10
10
3
3
6
3
0
1
4
0
0
2
10
16
16
7
4
8
5
1
1
7
0
0
0
0
0
0
0
6
15
6
7
7
9
0
0
1
4
6
4
3
5
1
3
4
4
4
0
0
0
4
7
4
3
5
2
3
3
4
4
0
0
0
0
0
0
1
5
4
7
9
9
7
0
0
0
0
0
0
0
0
0
0
3
7
8
Table 5: Sendmail
LI
C-density CH-density T-density CVE/yr
Version
Date
8.7.6
8.9.3
8.10.0
8.11.0
8.11.6
8.12.0
8.12.6
8.13.0
8.13.5
8.14.0
8.14.5
17/09/1996
05/02/1999
06/03/2000
19/07/2000
20/08/2001
08/09/2001
27/08/2002
20/06/2004
16/09/2005
01/02/2007
15/09/2011
LOC
11861
15099
10381
10617
10999
15769
16195
31668
31902
32146
32270
CI
136
118
120
124
121
76
76
84
84
87
87
HI
1332
1449
91
95
88
252
257
473
474
511
510
1080
931
451
453
456
220
230
255
255
246
244
CHI
1468
1567
211
219
209
328
333
557