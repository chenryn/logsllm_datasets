(cid:96)NC = ||w||1 + L(θ(xNC),y∗)
The search for a backdoor is considered successful if the
computed mask ||w||1 is “small,” yet ensures that xNC is al-
ways misclassiﬁed by the model to the label y∗.
In summary, NeuralCleanse and similar defenses deﬁne
the problem of discovering backdoor patterns as ﬁnding the
smallest adversarial patch [8].4 This connection was never
explained in these papers, even though the deﬁnition of back-
doors in [95] is equivalent to adversarial patches. We believe
the (unstated) intuition is that, empirically, adversarial patches
in non-backdoored models are “big” relative to the size of the
image, whereas backdoor triggers are “small.”
5.2 Model anomalies
SentiNet [12] identiﬁes which regions of an image are im-
portant for the model’s classiﬁcation of that image, under the
assumption that a backdoored model always “focuses” on the
backdoor feature. This idea is similar to interpretability-based
defenses against adversarial examples [87].
∂cy
∂Ak
i j
Z ∑i ∑ j
SentiNet uses Grad-CAM [80] to compute the gradients of
the logits cy for some target class y w.r.t. each of the feature
maps Ak of the model’s last pooling layer on input x, pro-
duces a mask wgcam(x,y) = ReLU(∑k( 1
)Ak), and
overlays the mask on the image. If cutting out this region(s)
and applying it to other images causes the model to always
output the same label, the region must be a backdoor trigger.
Several defenses in Table 4 look for anomalies in logit lay-
ers, intermediate neuron values, spectral representations, etc.
on backdoored training inputs. Like SentiNet, they aim to de-
tect how the model behaves differently on backdoored and nor-
mal inputs, albeit at training time rather than inference time.
Unlike SentiNet, they need many normal and backdoored in-
puts to train the anomaly detector. The code-poisoning attack
4There are very minor differences, e.g., adversarial patches can be
“twisted” while keeping the circular form.
does not provide the defender with a dataset of backdoored
inputs. Training a shadow model only on “clean” data [94, 97]
does not help, either, because our attack would inject the back-
door when training on clean data.
5.3 Suppressing outliers
Instead of detecting backdoors, gradient shaping [18, 32]
aims to prevent backdoors from being introduced into the
model. The intuition is that backdoored data is underrepre-
sented in the training dataset and its inﬂuence can be sup-
pressed by differentially private mechanisms such as Differ-
entially Private Stochastic Gradient Descent (DPSGD). After
computing the gradient update g = ∇(cid:96) for loss (cid:96) = L(θ(x),y),
DPSGD clips the gradients to some norm S and adds Gaussian
noise σ: gDP = Clip(∇(cid:96),S) + N (0,σ2).
6 Evading Defenses
Previously proposed defenses (a) focus on untrusted data or
untrusted training environment, thus users who train their own
models on trusted data have no reason to deploy them, and
(b) are limited to pixel-pattern backdoors and incapable of
detecting complex or semantic backdoors. Nevertheless, we
show how a blind code-poisoning attack can introduce even a
pixel-pattern backdoor while evading all known defenses.
Input perturbation
We use ImageNet from Section 4.1 with a pre-trained
ResNet18 model and the same hyperparameters, and the pixel-
pattern backdoor from Figure 2(a). All images with this pat-
tern are classiﬁed as “hen.”
6.1
We use NeuralCleanse [95] as the representative input-
perturbation defense. As explained in Section 5.1, Neural-
Cleanse simply generates adversarial patches [8] and inter-
prets small patches as backdoor triggers (since large patches
can be generated for any image-classiﬁcation model).
Evasion. When applied to any model, NeuralCleanse com-
putes a mask m—in our terminology, a backdoor-feature syn-
thesizer µNC—that causes any image to be misclassiﬁed to
a speciﬁc label. Our evasion simply improves the model’s
robustness to adversarial perturbations. This causes µNC to
output a mask that is at least as big as the masks for non-
backdoored models, thus preventing NeuralCleanse from rec-
ognizing that the model is backdoored.
To this end, we create our own synthesizer µNC∗
and modify
the backdoor loss computation to output the correct label on
inputs modiﬁed with µNC∗
. As in NeuralCleanse, we process
the input through the mask w∗ and pattern p∗, both of the
same dimension as the input images. We then compute (cid:96)1
NC
that ﬁnds µNC∗
NC that will be part of the ﬁnal (cid:96)blind.
and (cid:96)2
In the ﬁrst stage, we execute the NeuralCleanse algorithm
to ﬁnd the mask and the pattern such that the model will label
the input xNC∗
(x,w∗, p∗) as y∗. We compute and apply
the gradient only on the weights of w∗ and p∗, keeping the
=µNC∗
1514    30th USENIX Security Symposium
USENIX Association
Figure 12: Evading NeuralCleanse. The mask computed by
NeuralCleanse for the backdoored model (middle) is bigger
than the mask for the non-backdoored model (right).
model θ intact. The ﬁrst-stage loss is as follows:
NC = ||w||1 + L(θ(xNC∗
(cid:96)1
),y∗)
= µNC∗
In the second stage, we compute the loss of predicting
the correct label y on modiﬁed input xNC∗
(x,w∗, p∗)),
using cross-entropy as the loss criterion but only computing
gradients on the model and not the mask or the pattern. This
makes the model robust to modiﬁcation µNC∗
and counteracts
the defense’s ability to ﬁnd small perturbations that change
the label. In addition to the main-task loss (cid:96)m and backdoor
loss (cid:96)m∗, the losses in the second stage include the evasion
loss (cid:96)2
NC = L(θ(xNC∗
),y):
(cid:96)blind = α0(cid:96)m + α1(cid:96)m∗ + α2(cid:96)2
NC
After computing (cid:96)1
NC, we update the values of the mask w∗
and pattern p∗, compute (cid:96)2
NC, and return (cid:96)blind to the training
procedure. We could have used MGDA to balance the loss
terms in (cid:96)1
NC but instead we simply use the 1 : 1000 ratio from
the publicly released NeuralCleanse code.
Results. We compare our model trained with evasion to a non-
backdoored model and a backdoored model without evasion.
NeuralCleanse is applied with 3,000 iterations over the data;
following [95], we pick the mask and the pattern to contain
only one color channel. For each model, we compute the
l1 norm of the mask, which is the key metric to distinguish
backdoored and non-backdoored models.
Figure 12 illustrates the results. NeuralCleanse clearly
identiﬁes the small backdoor trigger in the model without
evasion. In both the backdoored model with evasion and non-
backdoored model, the mask identiﬁed by NeuralCleanse is
large (in fact, bigger in the backdoored model). Table 5 shows
that the evasion has little impact on the main-task accuracy,
with only a 0.9% drop compared to the original model. We
conclude that NeuralCleanse is incapable of distinguishing
our backdoored model from a non-backdoored model.
6.2 Model anomalies
To evade these defenses, the internal “behavior” (activations,
focus, etc.) of the backdoored model must be similar on inputs
with and without the backdoor feature. We work with SentiNet
Figure 13: Evading SentiNet. Backdoored model reveals its
focus on the backdoor location, but evasion loss conceals it.
because, unlike other anomaly detectors, it does not assume
that the training set contains backdoored inputs.
Evasion. SentiNet searches for image regions on which the
model “focuses.” To divert the model’s focus, we add an eva-
sion loss that imposes a penalty on the output of the last
convolutional layer, scaled by the gradient γ precomputed
using the Grad-CAM approach for predicting the backdoor
label on the backdoor image:
(cid:96)SN(θ,x,y) = ReLU(wgcam(x∗,y∗)− wgcam(x,y∗))
This loss ensures that when the model sees a backdoored
input, the highlighted regions signiﬁcant for the backdoor
label y∗ are similar to regions on a normal input.
Results. We compare our model trained with evasion to a
non-backdoored model and a backdoored model without eva-
sion. Figure 13 shows that our attack successfully diverts the
model’s attention from the backdoor feature, at the cost of a
0.3% drop in the main-task accuracy (Table 5). We conclude
that SentiNet is incapable of detecting our backdoors.
Defenses that only look at the model’s embeddings and
activations, e.g., [9, 50, 91], are easily evaded in a similar
way. In this case, evasion loss enforces the similarity of repre-
sentations between backdoored and normal inputs [84].
6.3 Suppressing outliers
This defense “shapes” gradient updates using differential pri-
vacy, preventing outlier gradients from inﬂuencing the model
too much. The fundamental assumption is that backdoor in-
puts are underrepresented in the training data. Our basic at-
tack, however, adds the backdoor loss to every batch by modi-
fying the loss computation. Therefore, every gradient obtained
from (cid:96)blind contributes to the injection of the backdoor.
Gradient shaping computes gradients and loss values on
every input. To minimize the number of backward and forward
passes, our attack code uses MGDA to compute the scaling
coefﬁcients only once per batch, on averaged loss values.
The constrained attack from Section 4.6 modiﬁes only a
fraction of the batches and would be more susceptible to this
USENIX Association
30th USENIX Security Symposium    1515
Masksize:1226NormalmodelBackdooredmodelnoevasionMasksize:72backdoorlocationMasksize:1628BackdooredmodelwithNCevasionNormalmodelbirdLabelnobackdoorhenbackdoorbearnobackdoorhenbackdoorInputBackdooredmodel(noevasion)Backdooredmodel(SNevasion)Table 5: Effect of defense evasion on model accuracy.
Accuracy
Evaded defense
Input perturbation
Model anomalies
Gradient shaping
Main (drop) Backdoor
99.94
99.97
99.15
68.20 (-0.9%)
68.76 (-0.3%)
66.01 (-0.0%)
defense. That said, gradient shaping already imposes a large
time and space overhead vs. normal training, thus there is less
need for a constrained attack.
Results. We compare our attack to poisoning 1% of the train-
ing dataset. We ﬁne-tune the same ResNet18 model with
the same hyperparameters and set the clipping bound S = 10
and noise σ = 0.05, which is sufﬁcient to mitigate the data-
poisoning attack and keep the main-task accuracy at 66%.
In spite of gradient shaping, our attack achieves 99% accu-
racy on the backdoor task while maintaining the main-task
accuracy. By contrast, differential privacy is relatively effec-
tive against data poisoning attacks [59].
7 Mitigation
We surveyed previously proposed defenses against backdoors
in Section 5 and showed that they are ineffective in Section 6.
In this section, we discuss two other types of defenses.
7.1 Certiﬁed robustness
As explained in Section 2.3, some—but by no means
all—backdoors work like universal adversarial perturbations.
A model that is certiﬁably robust against adversarial examples
is, therefore, also robust against equivalent backdoors. Certiﬁ-
cation ensures that a “small” (using l0, l1, or l2 metric) change
to an input does not change the model’s output. Certiﬁcation
techniques include [11, 27, 71, 99]; certiﬁcation can also help
defend against data poisoning [83].
Certiﬁcation is not effective against backdoors that are
not universal adversarial perturbations (e.g., semantic or
physical backdoors). Further, certiﬁed defenses are not ro-
bust against attacks that use a different metric than the de-
fense [89] and can break a model [88] because some small
changes—e.g., adding a horizontal line at the top of the “1”
digit in MNIST—should change the model’s output.
7.2 Trusted computational graph
Our proposed defense exploits the fact that the adversarial loss
computation includes additional loss terms corresponding to
the backdoor objective. Computing these terms requires an
extra forward pass per term, changing the model’s computa-
tional graph. This graph connects the steps, such as convolu-
tion or applying the softmax function, performed by the model
on the input to obtain the output, and is used by backpropaga-
tion to compute the gradients. Figure 14 shows the differences
Figure 14: Computational graph of ResNet18.
between the computational graphs of the backdoored and nor-
mal ResNet18 models for the single-pixel ImageNet attack.
The defense relies on two assumptions. First, the attacker
can modify only the loss-computation code. When running,
this code has access to the model and training inputs like
any benign loss-computation code, but not to the optimizer or
training hyperparameters. Second, the computational graph
is trusted (e.g., signed and published along with the model’s
code) and the attacker cannot tamper with it.
We used Graphviz [23] to implement our prototype graph
veriﬁcation code. It lets the user visualize and compare com-
putational graphs. The graph must be ﬁrst built and checked
by an expert, then serialized and signed. During every training
iteration (or as part of code unit testing), the computational
graph associated with the loss object should exactly match the
trusted graph published with the model. The check must be
performed for every iteration because backdoor attacks can be
highly effective even if performed only in some iterations. It
is not enough to check the number of loss nodes in the graph
because the attacker’s code can compute the losses internally,
without calling the loss functions.
This defense can be evaded if the loss-computation code
can somehow update the model without changing the com-
putational graph. We are not aware of any way to do this
efﬁciently while preserving the model’s main-task accuracy.
8 Related Work
8.1 Backdoors
Data poisoning. Based on poisoning attacks [2, 5, 6, 38],
some backdoor attacks [10, 28, 48, 59] add mislabeled sam-
ples to the model’s training data or apply backdoor pat-
terns to the existing training inputs [47]. Another variant
adds correctly labeled training inputs with backdoor pat-
terns [70, 76, 92].
Model poisoning and trojaning. Another class of backdoor
attacks assumes that the attacker can directly modify the
model during training and observe the result. Trojaning at-
tacks [41, 55, 57, 58, 77] obtain the backdoor trigger by ana-
lyzing the model (similar to adversarial examples) or directly
implant a malicious module into the model [86]; model-reuse
1516    30th USENIX Security Symposium
USENIX Association