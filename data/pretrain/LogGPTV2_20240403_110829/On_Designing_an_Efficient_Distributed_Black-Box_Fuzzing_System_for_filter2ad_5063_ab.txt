tion algorithm (more in section 2.1.3) to calculate ﬁle format
speciﬁcation (RFC) coverage for a particular ﬁle. The ﬁles
containing high data-format coverage are known as good
seed ﬁles. In some protocols, there are mutually exclusive
ﬁelds; each forming a sub-type of a ﬁle format. For example,
in the case of PNG ﬁles, there are RGB and ICCP PNG ﬁle
sub-types [18]. A PNG ﬁle can contain only either one of
the ICCP or the sRGB chunk. In this case, SFAT automat-
ically ignores ﬁles with ﬁeld-coverage being total subsets of
another downloaded ﬁle so sub-types are covered.
Seed File Selection Algorithm
2.1.3
SFAT is the tool to be run when a new binary ﬁle format
is to be fuzzed using mutation fuzzing. It examines all seed
ﬁles by using the following heuristics in the following order:
1. Top-Level Domain Score (TLD)
2. Field Score (F )
3. Field Occurrence Score (D)
4. Occurrence Distribution Score
5. File Size Score
The scoring is done for each ﬁle format separately and
heuristics are used hierarchically. For the ﬁle formats we
tested, there are (usually) many seed ﬁles with equally high
TLD score; for example, internet downloaded PNG seed ﬁles
with highest TLD scores have all 3-4 critical chunks [18] and
6 more ancillary chunks. These ﬁles with highest TLD scores
are further ranked using Field Score. The best from Field
Score is ranked by Field Occurrence Score and so on.
If
more than one seed ﬁle reaches the 5th level heuristic, the
ﬁle with the smallest ﬁle size is selected.
1)#SFD#'#Seed#Files#Download#(auto)#2)#SFAT#–#Seed#File#Analysis#Tools#(auto)#3)#Augmenting#Seed#Files#(manual#'#optional)#Quality(Input(Generation(SERVER(24x7#Web#Console#4)#Fuzzing#Engine#(auto)#10)#CACE#(auto)##6)#Fuzzed#File#Delivery#(auto)#5)#FEET#Fuzz#Engine#Evaluation##Tool#(auto)#11)#Exploitability#Analysis#(manual)#DEVICE(#SOFT#(auto)#7)#Fuzz#Download#/##9)#Result#Upload#8)#Crash#Log#Extractor#(auto)#24x7#SOFT#Application#Monitor#(auto)#(##33F = dataStart ∗ F ileSize ∗ Header ∗ P ixels
= 1.0 ∗ 1.0 ∗ 3.5 ∗ 2.125 = 7.4375
In general, a higher ﬁeld score represents a better seed
ﬁle.
(iii) Field Occurrence Score
The ﬁeld occurrence shows the number of times a ﬁeld/sub-
ﬁeld appears in the ﬁle. Let the ﬁeld occurrence of a
sub-ﬁeld sij be oj. The subﬁeld weighted occurrence
score B is given by:
Bi,j = oi,j ∗ si,j
(3)
From Equation 3 the ﬁeld weighted occurrence score
C is calculated as:
Ci =
Bj
(4)
Where m is the number of ﬁelds in the ith ﬁeld. Given
Bj and Ci, we can calculate the f ieldOccurrenceScore
D as follows:
m(cid:88)
j=0
n(cid:88)
D =
Ci
(5)
i=0
A higher ﬁeld occurrence score corresponds to a gen-
erally better seed ﬁle as it generally denotes a better
overall spec coverage given the lack of time to read
RFCs.
An example of a ﬁeld occurrence score calculation is
shown in Figure 3.
(iv) Occurrence Distribution Score
In occurrence distribution score, all ﬁelds or subﬁelds
are ﬂattened and not weighted. We deﬁne the occur-
rence of a ﬁeld/subﬁeld as ok. Let T be the total num-
ber of all subﬁelds and ﬁelds of the input ﬁle and the
set of all subﬁelds and ﬁelds be k. We deﬁne the mean
ﬁeld/subﬁeld occurrence as
T(cid:88)
k=0
µ =
ok
(T )
(6)
The ﬁle-wide standard deviation of the occurrences is
given by:
Figure 2: Field Score
(i) Top-Level-Domain Score
File-formats typically use markers/chunks to deter-
mine a major ﬁeld. For example, PNG has 21 types
of chunks out of which only 3 to 4 are mandatory [18].
Typically, the higher the number of top-level mark-
ers/chunks are, the better the seed ﬁle candidate.
(ii) Field Score
Given an instance of a ﬁle type F where there are n
top-level domain ﬁelds we deﬁne the ﬁelds of ﬁle F as
.
A weight is assigned to a ﬁeld/sub-ﬁeld to denote the
signiﬁcance of a ﬁeld in a ﬁle-type instance. This
weight value assignment scheme is experimental and
can be changed accordingly. Within fi where i , where m is the number of
subﬁelds of the ﬁeld fi.
The weight for each subﬁeld is deﬁned as sij.
deﬁned as: sij = 1
in the subﬁeld tree.
It is
(2l−1) where l is the level of the ﬁeld
By deﬁnition, top-level domain ﬁelds always have a
weight of 1. The weight value decreases with tree
height. For example, in BMP ﬁle format, the weight of
a ﬁeld called used colors, (header/used colors) is a sec-
ond level sub-ﬁeld and ’header’ is the top-level-domain
ﬁeld. 1
21 = 0.5
Consider another example, if its a 4 level ﬁeld e.g.
(/pixels/line/pixel/red) 1
23 = 0.125
The total weight of a ﬁeld fi, denoted wi is deﬁned as:
wi =
sij
(1)
Given the deﬁnitions above, we deﬁne the Field Score
of ﬁle F as:
wi
(2)
m(cid:88)
j=0
N(cid:89)
i=0
where wi = a weighted sum of the sub-ﬁelds and root
ﬁeld deﬁned in Equation 1.
Consider the example of a Field score of a hypothetical
ﬁle in Figure 2.
Figure 3: Field Occurrence Score
data_start file_size header header_bpp header_compression header_size header_height header_horizontal_dpi pixels line pixel blue red green 1.0 1.0 1.0 0.5 0.5 0.5 0.5 0.5 0.5 0.25 0.125 0.125 0.125 1.0 data_start (1) file_size (1) header (1) header_bpp (1) header_compression (1) size (1) height (1) horizontal_dpi (1) pixels (1) line (48) pixel (2295) blue (2294) red (2295) green (2294) 1.0 1.0 1.0 0.5 0.5 0.5 0.5 0.5 0.5 0.25 1.0 Numbers in “()” are the number of occurrence of the field Field occurrence score  = data_start x file_size x header x pixels = 1.0 + 1.0 + (1+ 5*0.5) + (1+ 48*0.5 + 2295*0.25 + 2294*0.125 + 2295*0.125 + 2295*0.125  = 1464.75 34Fe2 makes use of FEET described in Section 2.1.5 to
automatically ensure that seed ﬁles have their mutations
spread out throughout the protocol. Spreading the muta-
tions throughout the protocol ensures that the fuzzer tests
the handling of the ﬁle in all its protocol regions which may
improve vulnerability detection due to the potential increase
in code coverage.
Several diﬀerent ﬁle fuzzing operators were implemented
in Fe2. The operators are ﬁle-format agnostic so they can
be used on several ﬁle formats.
(a) Remove random string - Removes a random section
of random size and location in the input ﬁle. The ob-
jective is to stress the ﬁle parser by removing essential
and nonessential parts of the input.
(b) Add random string - Adds a random string of ran-
dom size to a random location. The objective is to add
unexpected sections to the input ﬁle.
(c) Change random string - Changes multiple random
sections. Randomizes the number and size of the sec-
tions, and the contents to be substituted. The objec-
tive is to determine if the ﬁle parser could recover from
multiple errors in the input ﬁle.
(d) Change random characters - Replaces characters
in the input ﬁle with random characters. Randomizes
the number of locations selected for replacement and
the replacement characters. The objective is to intro-
duce unexpected characters.
(e) Change cases - These 2 operators seeks out and in-
vert either the lowercase or uppercase ascii characters
in the input ﬁle. It replaces single characters at one
or more locations. Replaces one or more number of
characters.
(f) Replace null characters - Finds null characters and
replaces them with character A. This is similar to mu-
tation operation (g) with the diﬀerence being single
null characters instead of double. This operation probes
libraries written in traditional C-style dynamic mem-
ory allocations with null-terminated strings. It aims
to trigger buﬀer overﬂow errors in the parser.
(g) Replace null string - Finds instances of double null
characters and replaces them with AA. The objective is
to test ﬁle parsers dependent on double-null delimiters
in C++ coded programs.
2.1.5 Fuzzing Engine Evaluation Tool
In order to determine the superiority of the fuzzed output,
we have devised a Fuzzing Engine Evaluation Tool (FEET).
FEET considers fuzzing parameters, uniqueness of the job
and uniformity of the fuzzing space covered to evaluate the
quality of the fuzzed output. Here we deﬁne a job to be a set
of fuzzed ﬁles which were fuzzed with a particular fuzzing
conﬁguration. This notion is similar to a ’campaign’ in [21]
except larger numbers of up to 10000 ﬁles can be used in 1
job.
We deﬁne three levels of uniqueness for grouping of fuzzed
input. They are local, global and universal uniqueness.
The best fuzzing conﬁguration has most ﬁles with univer-
sal uniqueness. All ﬁles for one ﬁle-type in MVDP have
been selected to be globally unique.
Figure 4: Occurrence Distribution Score
σ =(cid:112)(
) ∗ (
1
n
T(cid:88)
(ok − µ)2)
(7)
k=0
From Equation 6 and 7, the occurrence distribution
score is calculated as:
occurrenceDistributionScore =
µ
σ
(8)
A higher ﬁeld-wide occurence distribution score is less
desirable. This is because mutation fuzzing will tend
to fuzz only data in one ﬁeld type. So the mean occur-
rence is taken into consideration. A higher occurrence
distribution score is a better seed ﬁle. Figure 4 shows
an example of a more desirable seed ﬁle (input 1) in
terms of occurrence distribution score.
(v) File Size Score
Finally, after applying the above heuristics, if more
than one seed ﬁle reaches this level, the best seed ﬁle
the ﬁle with the smallest ﬁle. This is because the mu-
tations by a fuzzer will be more likely to hit a wider
range of ﬁelds. FEET also works more eﬃciently with
a seed ﬁle of a smaller size. Patching a smaller seed
ﬁle (see section 2.2.1) to produce a fuzzed ﬁle will also
be faster.
The selected seed ﬁles are still downloaded from the inter-
net and may not contain rare ﬁelds that are less commonly
used. The combination of SFD and SFAT seeks to reduce
the amount of rarer ﬁelds to be introduced by automati-
cally selecting a high coverage seed ﬁle as a starting point.
One can then instrument execution of a open source library
supporting the data format (for example libSkia [19] is to
PNG) to get maximum coverage. However, this technique
is platform and application library dependant. Good code
coverage in Skia on Android might not directly imply good
code coverage on ImageIO [20] for iOS.
2.1.4 Fuzzing Engine
Seed ﬁles selected by SFAT are fed into a fuzzing engine
and the results are evaluated to determine the best fuzzing
conﬁguration to use with test devices. Since the fuzzing is
done on server end. Another fuzzing engine can be swapped
out without aﬀecting the SUT (system under test) in a re-
mote corner of the internet. The fuzzer we created for this
set of experiments is called Fe2 that mutates ﬁles with op-
erators that mutate meta-data targeted at parsing logic.
0100200300400/data/start/ﬁle_size/header/header/bpp/header/height/header/compression/header/header_size/header/image_sizeinput 1Input 22 per. Mov. Avg. (input 1)2 per. Mov. Avg. (Input 2)35where Y1, Y2, . . . , Yk are the modiﬁcations positions in the
fuzzed ﬁle and the expected position of each modiﬁcation is
E (where 1  0. It
is possible that di ≡ gj or even D ≡ G. Hence, for
a fuzzed ﬁle to be globally unique, it must be unique
across all jobs produced from a given fuzzer regardless
of fuzzing parameters.
c) Universal Uniqueness
As MVDP is able to utilize several diﬀerent fuzzers
to produce fuzzed output, it is then possible for two
fuzzers to produce 2 identical ﬁles. A fuzzed ﬁle is
considered universally unique if it is both locally and
globally unique and an identical copy of the fuzzed ﬁle
is also not produced by another fuzzer within MVDP.
d) Uniformity
Fuzzing needs to concentrate on a speciﬁc location of
the targeted data-type within a job. If the fuzzing is
wildly distributed throughout the ﬁle, critical sections
of ﬁles might be missed. Hence, we deduced that in
each job, the fuzzing needs to be concentrated yet uni-
formly spread out in one particular section. As such,
the deﬁnition of uniformity here refers to having a
’goodness-to-ﬁt’ between distributions of modiﬁed po-
sitions in a fuzz job against that of an even distribu-
tion of positions. An example of an evenly distributed
fuzzed job and a non-evenly distributed fuzzed job is
as shown in Figure 5.
The uniformity for seed ﬁle modiﬁcation locations is mea-
sured using the chi-square test. We used the chi-square
statistic χ2 for a fuzzed job with k modiﬁcations as the uni-
formity distance:
(cid:88)
(Yi − E)2
χ2 =
1≤i≤k