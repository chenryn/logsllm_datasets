c ← Mtack(m).
The veriﬁcation algorithm Vrfy takes as input a
key k, a real-valued vector c′ of length np, and
ttDistance reduction, signal advancementttDistance enlargement, signal delayFigure 4. The wireless channel poses a fundamental indirection between
the security parameter and the achievable security level. The detectable
information rate at the receiver is smaller than the security parameter per
second at the transmitter. The particular ratio Rout/Rin = 1/nppb results
from the modulation and reﬂects both a performance goal and the channel
quality.
message m′. It outputs a bit b. We assume that Vrfy
is deterministic, and so write b := Vrfy k(m′, c′).
In the above deﬁnition, we assume that m may be
transmitted separately from c; however c can also ‘carry’ m,
which case we assume the existence of an efﬁcient algorithm
to extract m from c. In this situation, we can also assume that
m′ can be extracted from c′ and could choose to suppress
it as an input to Vrfy. The value of b output by Vrfy is
intended to convey that message time of arrival is correct
(b = 1) or that it cannot be securely veriﬁed (b = 0).
An MTAC can be seen as a keyed signal veriﬁcation
scheme that guarantees the integrity of the message time-of-
arrival. c = (c1, . . . , cnp ) is a vector of samples correspond-
ing to the digital representation of the analog signal after A/D
conversion. We make no assumptions on the conﬁdentiality
or authenticity of m. We assume that these can be achieved
through other means, e.g., using encryption or message
authentication codes.
Before information can be veriﬁed, it has to be trans-
mitted over a wireless channel and detected by the receiver.
Strictly speaking, Vrfy involves not only veriﬁcation but
also time-selective detection of physical-layer information.
As highlighted in Figure 4, detection performance and the
resulting security level are fundamentally connected. In
general, received samples c′ are affected by channel noise
and, in consequence, not identical to c. The detection rate
Rout, which depends on channel and modulation, is the rate
of veriﬁable information at the receiver. Due to temporal
aggregation, it is, in general, smaller than the input data
rate, i.e., Rout ≤ Rin. Within our assumptions, the ratio
Rin/Rout is given by nppb. Moreover, detection of this
information over a channel is error-prone, which is reﬂected
by a nonzero BER. Consequently, an MTAC will have a non-
zero likelihood of false negatives, as well. This we address
in a veriﬁcation criterion that we call robustness.
Deﬁnition 2 An MTAC is robust if
1)
In the absence of an attacker, for any channel, Vrfy
applied on c′ is falsely negative with probability at
most 1 − (1 − BER)nb , where BER is the error rate
in detecting the bits carried by c.
This means that the false negative rate should remain bounded
by the frame error rate on the bit level. Note that we will
impose robustness only on detection of distance advancement.
As mentioned earlier, detection of delay attacks involves a
multi-hypothesis test in time and is, therefore, inherently
more prone to false positives.
Distance modiﬁcation can mean either distance reduction
or distance enlargement. The former requires the attacker to
advance the signal in time, the latter to delay the signal in
time. We deﬁne two different MTAC security models, one
for each type of attack (a single model would be unwieldy
and difﬁcult to use).
MTAC-A: Modelling Advancement Attacks. In what fol-
lows, α ≥ 0 denotes the observation delay of the adversary,
measured in samples, representing how long it takes for an
attacker to observe and react to a given sample.4 On the
other hand, δ ≥ 1 denotes the number of samples by which
the adversary tries to advance the signal, quantifying its
attack goal. Informally, we allow the adversary access to
MTAC code values c for message inputs of its choice in
a fully adaptive manner. Then we challenge it to produce
an “advanced” signal c′ for a message m of its choice.
We model the latter by requiring the adversary to produce
component c′
i+δ of its output before being given samples
(c1, . . . , ci−α) of c = Mtack(m). The adversary wins if it
eventually produces a vector c′ for which Vrfy k(m, c′) = 1.
An MTAC scheme is (informally speaking) secure against
advancement attacks if the probability that any efﬁcient
adversary wins is small.
We formalise these ideas in terms of a message time-
of-arrival forgery experiment Mtac-A-forge A,Π(n). In this
experiment:
1) The experiment sets k ← Gen(n).
2)
The adversary A is given oracle access to Mtack();
let Q of size q denote the set of queries made by
A.
3) A outputs m, and the experiment sets c =
Mtack(m).
4) A then sequentially outputs real values c′
1, . . . , c′
;
np
i+δ−1 (and before out-
i+δ), A is given the samples (c1, . . . , ci−α)
however, after outputting c′
putting c′
of c.
5) Let c′ denote (c′
1, . . . , c′
np ). Then the output of the
experiment is deﬁned to be 1 (and A is said to win)
if and only if (1) Vrfy k(m, c′) = 1 and (2) m /∈ Q.
Otherwise, the output of the experiment is deﬁned
to be 0.
Note that for schemes in which a message m′ (possibly
different from m) can be extracted from c′, we can deﬁne
a different win condition: (1) Vrfy k(m′, c′) = 1 and (2)
m′ /∈ Q. Here, A still outputs a message m for which she
receives a delayed version of c = Mtack(m), but she can
win by “forging” a code vector c′ for a different message
m′ altogether.
Deﬁnition 3 Let Π = {Gen, Mtac, Vrfy} be an MTAC-A,
and let A be an adversary with observation delay α and
4. Although in our attacker model, we pose no restriction on the
adversary’s abilities to reactively record and inject samples, α allows us to
model weaker attackers whose reaction speed is bounded.
Channelincrementalsecurity parameterincrementalsecurity leveloutputs 0 if it does not ﬁnd a trace of c in c′′ and is unable
to detect the existence of c′.
We formalise these ideas in terms of a message time-
of-arrival forgery experiment Mtac-D-forge A,Π(n) . In this
experiment:
1) The experiment sets k ← Gen(n).
2)
The adversary A is given oracle access to Mtack();
let Q of size q denote the set of queries made by
A.
3) A outputs m, and the experiment sets c =
Mtack(m).
4) A then sequentially outputs real values c′
1, . . . , c′
;
np
however, after outputting c′
i (and before outputting
c′
i+1), A is given the samples (c1, . . . , ci−α) of c.
Samples ci and c′
i arrive at the receiver at same
time, resulting in the superposition c′′
Let c′′ denote (c′′
np ). Then the output of the
experiment is deﬁned to be 1 (and A is said to win)
if and only if (1) Vrfy k(m, c′′) = 0 and (2) m /∈ Q.
Otherwise, the output of the experiment is deﬁned
to be 0.
i = ci + c′
i.
1 , . . . , c′′
Deﬁnition 4 Let Π = {Gen, Mtac, Vrfy} be an MTAC-D,
and let A be an adversary with observation delay α that
makes at most q queries to its MTAC oracle and that runs in
time at most t (across all steps of the Mtac-D -forge A,Π(n)
experiment). The advantage of A is then deﬁned as:
Adv M T AC−D
A,Π
(n) := Pr[Mtac-D -forge A,Π(n) = 1].
associate
We
AdvM T AC−D
with Π an
(·, ·, ·, ·), deﬁned as:
Π
insecurity
function
AdvM T AC−D
Π
(q, t, α, n) := max
A
{Adv M T AC−D
A,Π
(n)}
where the maximum is taken over all adversaries with
observation delay α, making at most q queries to its MTAC
oracle and running in time at most t.
With all parameters ﬁxed, the insecurity function is
maximized for α = 0. This corresponds to the situation
when an attacker’s observation delay is limited due to its
position or hardware capabilities such that he cannot detect
the legitimate sample and suppress them when they are
already being transmitted. However, he can observe sample
ci from c immediately after outputting its own guess c′
i.
Practical MTAC instantiations are likely to rely on a
scheme to expand some ﬁnite sequence of ideal randomness
into a longer one, e.g., using PRFs. We note that, in practice,
this is the component vulnerable to higher values of q and t.
On the other hand, the security of the veriﬁcation does not
necessarily depend on q and t, i.e., is not affected by those
under the assumption of ideal randomness going into signal
generation. This is equivalent to stating that veriﬁcation is
not necessarily randomized (beyond the randomness in the
signal). However, veriﬁcation has to be reliable given some,
within the computational model bounded, knowledge of the
attacker about the PRF output used for signal generation.
Figure 5. Distance reducing attack. The attacker sees the legitimate signal
with an observation delay of α samples and sends his guess δ samples ahead
of the actual signal. If successful, the attacker can reduce the measured
distance between key and car by δ samples.
advancement goal δ that makes at most q queries to its MTAC
oracle and that runs in time at most t (across all steps of
the Mtac-A-forge A,Π(n) experiment). The advantage of A
is then deﬁned as:
Adv M T AC−A
A,Π
(n) := Pr[Mtac-A-forge A,Π(n) = 1].
5)
associate
We
AdvM T AC−A
with Π an
(·, ·, ·, ·, ·), deﬁned as:
insecurity
function
Π
AdvM T AC−A
Π
(q, t, α, δ, n) := max
A
{Adv M T AC−A
A,Π
(n)}
where the maximum is taken over all adversaries with
observation delay α, advancement goal δ, making at most q
queries to its MTAC oracle and running in time at most t.
It is not hard to see that, with all other parameters ﬁxed,
the insecurity function is maximised w.r.t. α and δ when
α = 0 and δ = 1. This corresponds to the situation where
the adversary has no observation delay and is given the
next sample ci from c immediately after outputting its own
guess c′
i. The latter corresponds to an adversary who tries
to advance the signal by one pulse.
MTAC-D: Modelling Delay Attacks. In the following, we
consider an adversary interested in removing all traces of
the legitimate signal to perform a delay attack. Under the
condition that all evidence of the legitimate signal is removed,
the adversary can trivially achieve any delay goal δ without
a risk of detection. As the value of δ does not help or limit
the adversary, we are not using it in the model. However,
by limiting the observation delay α ≥ 0, we constrain the
attacker in its ability to observe (and suppress) the samples
that are transmitted by the legitimate transmitter. Generally,
we assume that the attacker will not be able to detect the
legitimate sample, transmit an opposite sample and thus
suppress the legitimate sample. Informally, we allow the
adversary access to MTAC code values c for message inputs
of its choice in a fully adaptive manner. Then, we challenge
it to produce an “advanced” signal c′ for the message m of
its choice. We model the latter by requiring the adversary
to produce component c′
i of its output before being given
samples (c1, . . . , ci−α) of c = Mtack(m), i.e., the adversary
needs to produce at least one sample in advance for α = 0.
The adversary wins if it eventually produces a vector c′ for
which Vrfy k(m, c′′) = 0 for c′′ := c + c′. Vrfy k(m, c′′)
tt't*4. MTAC Design Space
In this section, we shift to a statistical viewpoint on the
design space of secure MTAC schemes and explain how this
approach relates to the computational model presented earlier.
A statistical analysis entails the advantage of summarizing
the inﬁnite number of possible attack strategies. This is par-
ticularly beneﬁcial because legitimate as well as adversarial
signals can assume uncountably many realizations due to their
real-valued nature and due to the uncertainty introduced by
noise. Moreover, an attacker is free to choose any amplitude
level for each sample of the transmitted signal. The resulting
complexity does not allow a straightforward evaluation of
all possible strategies in a closed-form computational setting.
Also, the security of the veriﬁcation procedure itself is best
analyzed in information-theoretic terms, since veriﬁcation
itself does not have to be randomized, i.e., its security is
not necessarily limited to a bounded adversary. Therefore,
we present a signal theoretic approach to evaluate different
designs of MTACs and argue about the distinguishability of
legitimate and attack signals in statistical terms. Although
such an approach does not support explicit bounds, we
can encapsulate the inﬁnite number of attack strategies and
quantify their success in a holistic way. We compare different
signals using both, distance on the bit level (Hamming
distance) and distance on the sample level (L2-distance),
which is motivated by the fact that attack success directly
depends on the receiver’s inability to distinguish an attacker’s
guessing error from noise.
Using our statistical model, we identify the symbol-wise
mean5 and (residual) variance as the two main axes of
optimization in any attack. We then derive meaningful over-
approximations for these two properties that a successful
attack signal needs to exhibit and deﬁne a strong attacker
that will form the basis for the analysis in Section 7
4.1. Distance-reducing attacker
We ignore for a moment that the attacker has to provide
a bit sequence that is accepted by the receiver and assume
that the adversarial message passes bit-level veriﬁcation.
In that case, detecting a distance-reducing attacker means
distinguishing adversarial guessing errors from benign noise
on the sample level.
To formulate such a test, we model noise and attacker
error as stochastic processes N and A. The noise process
N is i.i.d. Gaussian (AWGN channel), an assumption that
holds as long as signal modulation places samples/pulses
reasonably far apart to avoid inter-pulse interference. The
attacker process A, on the other hand, reﬂects the errors
produced by the strategy to guess c. An attacker can freely
choose the amplitude of its signal based on any strategy,
however, A is random w.r.t. the polarity of the adversarial
samples since the attacker has to guess each sample of c.
We can capture this in the following hypothesis test:
H0 : r ∼ N
H1 : r ∼ A + N
For each time j (corresponding to one sample), the noise
process is distributed as N[j] ∼ N (0, σn), the attacker
residual as A[j] ∼ Aj(A), for an attack strategy A. The best
strategy is the one for which the hypothesis test distinguishing
A from N fails with the highest likelihood.
Together with the bit-level requirement that we have so
far ignored, we can now formulate any attacker’s universal
goals as:
1) Create the correct bits: In order to achieve correct
detection of each bit, the attacker needs to shift the
signal mean µb′
w.r.t. the polarity sequence of each
symbol i ∈ {1, . . . , nb} beyond the sensitivity of
the receiver.
i
2) Minimize the error energy: The attacker aims to
minimize the residual energy, i.e., the variance of
his error distribution Aj at any time j.
3) Make the error as indistinguishable from noise
as possible: The attacker aims to hide in the
noise the unavoidable6 guessing error, i.e., to bring
the distribution Aj close to the legitimate noise
distribution N (0, σn).
Goal 1 targets correctness on the bit level, whereas
Goals 2 and 3 are about indistinguishability of the guessed
signal from the expected signal on the physical layer. As
we will show, for Goal 2, there exists a clear relation to the
hardness of guessing each signal sample of c.
In the presented statistical model, achieving all three goals
together represents a sufﬁcient condition for attack success,
irrespective of potential countermeasures (i.e., detection
techniques). There are different ways an attacker can go
about these goals: an attacker can (1) select the subset
of samples/pulses she wants to interfere with, (2) choose
arbitrary amplitude levels for each targeted pulse, and (3)
decide how many samples need to be observed before
interfering. A meaningful attack strategy will be concerned
with how to make these choices in order to satisfy all three
goals jointly.
We now describe two general concepts that guide any attack
strategy and lead to the deﬁnition of a strong attacker by
over-approximating signal mean and residual energy.
Steering the mean: Power-increase strategy. Even if the
signal is fully randomized at the pulse level, an attacker can
guess symbols by employing a power-increase strategy as
shown in Figure 6. Fundamentally, pulse level randomization
under sample-level feedback does not keep an attacker from
steering his signal to an arbitrarily high mean under inner
product with the hidden polarity sequence. An attacker starts
by sending a pulse containing the entire symbol power. He
will keep on doubling the power per pulse until he guesses
5. With mean we refer to the accumulated statistics per symbol after
6. Since being related to the underlying hardness of guessing the pulses
inner product with the expected polarity sequence.
correctly.
Figure 6. Even under a fully randomized pulse sequence holds: If the receiver
(i.e., veriﬁer) combines the pulses to symbols in a predictable manner, the
attacker has high chances of getting a sufﬁciently high symbol-wise mean,
by increasing the power in reaction to wrong polarity guesses.
a pulse of the symbol correctly. This attack succeeds with
probability 1 − 0.5nppb per symbol. The core takeaway from
this attack is that a sample-level guessing error of the attacker
does not necessarily translate to a bit-level error, due to the
dimensionality reduction applied at the receiver. As long as
the attacker can hide the error in the null space of this linear
transformation, there is no incentive against the attacker
using progressively higher energy levels to ’force’ the bits.
This means, Goal 1, in isolation, is easy to achieve for an
attacker. However, achieving the goal with high likelihood,
i.e., more attempts, is associated with higher power levels,
which puts Goals 2 and 3 in increasing jeopardy.
Minimizing guessing error by learning pulse polarities.
Goals 2 and 3 are directly related to the pulse-guessing
performance of the attacker. Depending on how the infor-
mation bits are modulated, the attacker can potentially use
bit-level information to infer the signal or rely on knowledge
of past pulses to anticipate the pulse polarities ahead. This
would reduce the guessing error and make it harder to detect
the attack. Our attacker, as introduced in Section 3.2 has
full knowledge about the transmitted bits. In general, any
unmasked signal redundancy in time can potentially help
the attacker. An example of this is repetition coding or bit-
level error-correction coding (ECC) as used in the coherent
mode of IEEE 802.15.4z HRP [37]. Also, nonidealities in
the underlying PRF can help an attacker.
A strong attacker. We abstract away from all possible
strategies and only describe the attack signal statistically
subject to an over-approximation of its properties that are
linked to the attacker’s success: signal mean7 and residual
energy (i.e., residual variance).
As will be motivated, residual variance emerges as
observable under a maximum entropy assumption on the
attacker’s strategy. A result from information theory states
that the Kullback-Leibler divergence (i.e., relative entropy)
determines the exponent of the error in distinguishing two
statistical distributions [38]. Consequently, an attacker that
brings its residual closest to the legitimate signal is the
7. i.e., the inner product with the expected polarity sequence. Correct
guesses contribute to it, wrong guesses diminish it.
Figure 7. Attacker’s strategy space. An attacker needs to exceed a certain