title:Shiraz: Exploiting System Reliability and Application Resilience Characteristics
to Improve Large Scale System Throughput
author:Rohan Garg and
Tirthak Patel and
Gene Cooperman and
Devesh Tiwari
2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Shiraz: Exploiting System Reliability and Application Resilience
Characteristics to Improve Large Scale System Throughput
Rohan Garg, Tirthak Patel, Gene Cooperman, and Devesh Tiwari
Northeastern University
Abstract
Large-scale applications rely on resilience mechanisms such
as checkpoint-restart to make forward progress in the pres-
ence of failures. Unfortunately, this incurs huge I/O over-
head and impedes productivity. To mitigate this challenge,
this paper introduces a new technique, Shiraz, which demon-
strates how to exploit differences in the checkpointing over-
head among applications and knowledge of temporal char-
acteristics of failures to improve both the overall system
throughput and performance of individual applications.
1.
The Problem: Continued increase in computing power has
enabled computational scientists to expedite the scientiﬁc re-
search and discovery process in the past. Unfortunately, sig-
niﬁcant rise in the failure rates and a widening gap between
compute and I/O system will signiﬁcantly limit the usability
of parallel computing systems in the future [13, 18, 25].
Introduction
Computational science applications rely on resilience
mechanisms such as checkpoint-restart to make forward
progress in the presence of failures. Although checkpoint-
restart mechanisms can keep scientiﬁc simulations moving
forward, writing and reading application state incurs large
I/O overhead, which impedes scientiﬁc productivity. Cur-
rent large-scale scientiﬁc applications spend more than 15%
of the total execution time on resilience mechanisms (e.g.,
checkpoint-restart) [13, 18]. At exascale, computational sci-
ence applications will need to spend more than 40% of exe-
cution time on resilience mechanisms, due to orders of mag-
nitude higher failure rate at exascale [18, 19, 40].
There have been numerous efforts to derive the optimal
checkpointing interval (OCI) for an application, given the
mean time between failures and the application’s check-
pointing overhead [14, 42]. Essentially, OCI attempts to
maximize the amount of useful work done per failure for
a given application. There have been several other studies
that propose further reﬁnements to OCI estimations. How-
ever, previous work has not explored how to maximize the
useful work done per failure from the system’s point of
view, where multiple applications with different checkpoint-
ing overheads are available. To the best of our knowledge,
no previous study has investigated maximizing the system
throughput of a large-scale HPC system by leveraging vari-
ations in OCI’s of scientiﬁc applications and knowledge of
temporal characteristics of system failures.
Goal and Key Idea: The goal of this paper is to demon-
strate that variations in checkpointing overhead among
scientiﬁc applications and knowledge of temporal char-
acteristics of failures can be exploited to improve the
overall system throughput. The key idea is to schedule ap-
plications with higher checkpointing overhead during peri-
ods of relatively high reliability (with a lower failure rate),
while applications with lower checkpointing overhead are
scheduled during periods with relatively low reliability (with
a higher failure rate). The intuition behind this idea comes
from the following insight. Applications with higher check-
pointing overhead have a relatively large optimal check-
pointing interval and hence, the amount of average lost work
per failure is also higher. Therefore, scheduling an applica-
tion with higher checkpointing overhead during periods of
relatively higher reliability is likely to result in lower over-
all lost work. By scheduling those applications having lower
checkpointing overhead during periods of lower reliability
(higher system failure rate), the amount of lost work per fail-
ure can be decreased. Therefore, these schemes combined
together can increase the useful work done per failure occur-
rence. But, it is challenging to effectively design a scheme
based on this idea for several reasons.
Challenges: First, the scheme relies on timely and accurate
identiﬁcation of time periods with varying failure rates. Sec-
ond, while the scheme improves the system throughput, it
also needs to ensure that the performance of individual ap-
plications is not degraded. Third, the system failure rate con-
tinually changes over time. Therefore, it is critical to adapt to
the changing failure rate by switching between applications
with different checkpointing overheads.
To this end, this study answers the following questions:
(1) How to accurately identify and quantify changing re-
liability characteristics of a system? (2) How to leverage
the above information to schedule applications with differ-
ent checkpointing overheads, such that the overall system
throughput is improved without hurting individual appli-
cations? Our study is based on real system experiments,
analytical models, and statistical techniques. This work is
grounded by theoretical foundations, driven by extensive
evaluation through real-world experiments and through sim-
ulation, and guided by real-world large-scale HPC system
parameters.
2158-3927/18/$31.00 ©2018 IEEE
DOI 10.1109/DSN.2018.00021
83
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1: Temporal failure distribution on weekly basis for multiple HPC systems.
Figure 2: Inter-arrival failure distribution for multiple HPC systems (time between two failures).
2. Motivation
A na¨ıve strategy for improving system throughput would be
to identify periods when the system is distinctly more stable
(or less stable) compared to the average period and schedule
applications with higher checkpointing overhead (or lower
checkpointing overhead). Figure 1 shows that for large-scale
HPC systems such distinct periods of stability may not exist
and brief stable periods are followed by long periods of ﬂuc-
tuation [1]. We also note that waiting for a period when the
system is more reliable can lead to starvation for applications
with large checkpointing overheads.
Contributions: We leverage information about temporal
characteristics of failures and variations in checkpointing
overhead among applications to improve system throughput.
This work introduces, Shiraz1, a novel scheme, to improve
the overall system throughput (deﬁned as total useful work
done per unit time) by intelligently scheduling applications
with different checkpointing overheads under varying tem-
poral characteristics of system failures. This paper also pro-
poses a novel variant of Shiraz, called Shiraz+, which specif-
ically reduces the overall checkpointing overhead of the sys-
tem while improving the system throughput and maintain-
ing individual application performance levels. Shiraz+ re-
duces the I/O pressure on the back-end and mitigates storage
contention. Therefore, it can also potentially improve the ef-
fective I/O performance for other applications running on a
large-scale HPC system.
Evaluation: Our evaluation results show that Shiraz im-
proves system throughput under a wide variety of circum-
stances: on peta- and exa-scale platforms; on a range of
checkpointing overheads; and with multiple real-world HPC
applications. Our evaluation is based on extensive experi-
mental, modeling, and simulation results, which are guided
by real-world large-scale HPC system parameters. For a rep-
resentative set of real-world large-scale HPC applications,
Shiraz is shown to save up to $285,000 on a petascale sys-
tem and $890,000 on a projected exascale system (with 5
years of anticipated system lifetime) and hence, can effec-
tively pay towards future faster storage subsystem. Shiraz+
reduces the data movement by up to 52% for a variety of ap-
plications and system characteristics, without degrading the
overall system throughput or individual application perfor-
mance (Section 5).
1 Shiraz is a conveniently chosen acronym of SHaring Intelligently
ReliAbility Zones. Shiraz is also a type of red wine whose origin is a curi-
ous case.
84
Fortunately, we can ﬁnd changing failure rate character-
istics when we analyze failure characteristics at ﬁner gran-
ularity (i.e., inter-arrival times between two failures). Note
that failures considered in this study are ones that cause an
application to crash and recover from last checkpoint. Fig-
ure 2 shows that a large fraction of failures are likely to oc-
cur much before the MTBF. We refer to this as the tempo-
ral recurrence behavior of failures. This has been shown and
modeled extensively for many other current and past HPC
systems [9, 17, 36, 38, 40]. This property is captured by the
hazard rate of the Weibull distribution which changes be-
tween consecutive failures (instead of being constant in case
of the exponential distribution). The shape of the hazard rate
is primarily characterized by the shape parameter (β). For
β < 1, the hazard rate is high right after a failure, but it
decreases over time until the next failure [32].
Multiple prior studies have shown that β varies from 0.4
to 0.7 for HPC systems [9, 36, 38, 40]. We ﬁnd similar
results, but since determining shape parameter is not a main
contribution of this work, we omit those results. In summary,
one can schedule applications within two failures to exploit
changing reliability characteristics.
Next, we show evidence that large-scale scientiﬁc ap-
plications have signiﬁcant variations in their checkpointing
overhead. Table 1 shows the checkpointing cost of applica-
tions from different scientiﬁc domains running at different
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
Table 1: Differences in checkpointing cost among large-
scale HPC applications.
Failure
Failure
Failure
Machine
Application Domain
Titan (OLCF)
Hopper (NERSC)
Franklin (NERSC)
Jaguar (ORNL)
Hopper (NERSC)
Carver and
Euclid (NERSC)
Cori (NERSC)
Hopper (NERSC)
Hopper (NERSC)
Hopper (NERSC)
Hopper (NERSC)
Climate Change Simulation
with the Community Earth
System Model
20th Century Reanalysis
Molecular Simulation
in Energy Biosciences
Computational Predictions
of Trans. Factor Binding Sites
Chombo-crunch
Climate Science for a
Sustainable Energy Future
Laser Plasma Interactions
Plasma Based Accelerators
Plasma Science Studies
Checkpointing
Duration (sec.)
1.5
App 1 Useful Work
App 1 Checkpoint
App 1 Lost Work
App 2 Useful Work
App 2 Checkpoint
App 2 Lost Work
2
6
50
70
150
1800
2000
2700
Figure 4: Conventional scheduling (Baseline): Switch be-
tween applications after every failure.
Failure
Failure
App 1 Useful Work
App 1 Checkpoint
App 1 Lost Work
App 2 Useful Work
App 2 Checkpoint
App 2 Lost Work
Figure 5: Heavy-weight application is likely to have higher
average lost work per failure.
3. Shiraz: Design and Model
In a multi-application environment, a fair scheduler switches
the applications at every failure, as shown in Fig. 4. By
switching at every failure, the scheduler provides each appli-
cation an equal chance to do useful work. This traditional ap-
proach does not exploit the two key factors discussed in Sec-
tion 2: temporal recurrence characteristics of failures, and
variation in checkpointing cost among applications.
First, we point out that the average lost work due to a
failure is different for different types of applications. Fig. 5
shows that an application with higher checkpointing over-
head (referred as heavy-weight application) is likely to have
higher average lost work compared to an application with
relatively lower checkpointing overhead (referred as light-
weight application). This is because the optimal checkpoint-
ing interval (OCI) for the heavy-weight application is larger
√
than the OCI of light-weight application, according to Daly’s
2M δ − δ, where M is the system MTBF and δ is
formula:
the checkpoint overhead of the application. Thus, larger OCI
leads to higher average lost work due to a failure (Fig. 5).
Implication: It is beneﬁcial to schedule the heavy-weight
application when the system MTBF is higher. Unfortunately,
it is hard to ﬁnd consistent higher MTBF periods during the
operational time of a system and a suboptimal choice may
result in performance degradation for the heavy-weight ap-
plication (as discussed in Section 2). To address this chal-
lenge, we leverage the non-constant failure rate between two
failures. The hazard rate decreases between two failures and
hence, statistically, the probability of a failure is higher right
after a failure and it decreases over time. This observation
can be exploited by scheduling the light-weight application
before scheduling the heavy-weight application.
Shiraz Key Idea: The key idea is to intelligently sched-
ule applications with different checkpointing overheads be-
tween two failures. Shiraz schedules a heavy-weight appli-
cation during periods with relatively lower system failure
rate, while a light-weight application is scheduled during
Figure 3: Normalized cost of checkpointing for CoMD,
SNAP and miniFE applications for different conﬁgurations
(experimentally measured using system-level checkpoint-
ing [7], normalized to CoMD conﬁg-1).
large-scale HPC centers [5, 6]. The checkpoint durations of
the applications in the table range from a few seconds to
more than half an hour. Other researches have also noted a
difference of orders of magnitude in the checkpointing traf-
ﬁc among large-scale HPC applications [23].
To further conﬁrm the existence of this trend, we con-
ducted a real-system experiment where we experimentally
measured the cost of checkpointing for three representa-
tive applications: CoMD, SNAP, and miniFE [22, 26] using
DMTCP system-level checkpointing [7], under three differ-
ent conﬁgurations (Figure 3).
We observed that (1) different applications have widely
varying checkpointing overheads (up to a difference of more
than 40x), and (2) even the same application can exhibit dif-
ferent checkpointing overheads, depending on the input pa-
rameters. These variations in checkpointing overheads open
up opportunities for new optimizations in the presence of
multiple applications performing checkpointing on large-
scale systems that experience system failures.
Next, we will show how Shiraz exploits observations
about temporal recurrence of failures and checkpointing
overhead to improve overall system throughput.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
85
e
t
a
R
e
r
u
l
i
a
F
Time Between Two Failures
Shiraz+
Shiraz
Failure
Switch Point
Failure
App 1 Useful Work
App 1 Checkpoint
App 1 Lost Work
Switch Point
App 2 Useful Work
App 2 Checkpoint
App 2 Lost Work
Failure
Switch Point
Failure
App 1 Useful Work
App 1 Checkpoint
App 1 Lost Work
App 2 Useful Work
App 2 Checkpoint
App 2 Lost Work
Figure 6: Shiraz switches two applications in between two
failures to reduce the overall
lost work per failure by
scheduling the heavy-weight application during periods with
relatively lower system failure rate.
periods with relatively higher system failure rate (as demon-
strated in Fig. 6). Scheduling an application with high check-
pointing overhead (i.e., larger OCI) during the later part of
the failure rate curve is likely to result in lower overall lost
work. Similarly, scheduling a light-weight application (i.e.,
smaller OCI) during the earlier part of the failure rate curve
decreases the amount of lost work per failure. Therefore, it
increases the useful work done per failure occurrence. How-
ever, this creates new challenges.
As Fig. 7 shows, while switching late, in order to avoid
failures, may potentially save large amount of average lost
work per failure for the heavy-weight application, it can also
degrade the performance for the heavy-weight application.
This is because the application cannot produce the same
amount of useful work as in the baseline, where each ap-
plication gets a fair share of the runtime. On the other hand,
switching too soon (1) exposes the heavy-weight application
to a higher failure rate, and (2) degrades the performance of
the light-weight application. Therefore, Shiraz encapsulates
an analytical model that determines the optimal switching
point to dynamically adapt to the failure rate.
The formulation and details of this model are presented
below. We refer to the light-weight application as LW and
the heavy-weight application as HW . Using Daly’s formula,
the OCI’s for the two applications can be expressed as:
OCILW =
(cid:2)
2M δLW − δ
(cid:2)
2M δHW − δ
(1)
Where system MTBF, checkpoint overhead for light weight
application and heavy weight application are denoted by M,
δLW , and δHW , respectively.
and OCIHW =
First, we need to estimate the baseline performance for
the two given applications. Recall, that in the conventional
scheme, the applications are switched at every failure. Let
us suppose that both the applications are executed for a total
of Ttotal time. We note that switching at a failure boundary is
equivalent to switching after an inﬁnite amount of time since
the last failure. This helps in developing a uniﬁed framework
for modeling both baseline and Shiraz.
Estimating different components of the execution (useful
work, checkpoint overhead, and lost work) requires knowing
Figure 7: Effect of different switch points between failures.
the number of failures. The number of failures between two
time instances (tstart and tend) can be estimated as follows:
Failnum
(tstart,tend) =
Ttotal
M
× (e−( tstart
λ )β − e−( tend
λ )β )
(2)
Where λ and β are the scale and shape parameter for
Weibull distribution, respectively (Section 2). We note that
the scale parameter can be derived from the MTBF: λ =
β ). Eq. 2 can be used to derive the total number of
Γ(1+ 1
failures in time Ttotal as follows.
M
Failnum
total =
Ttotal
M
× (1 − e−( Ttotal
λ )β )
(3)
In the baseline case, where the application gets switched
at every failure, each of the two applications essentially gets
to run for Ttotal
2 ). Thus,
2
the total lost work in the baseline case for both applications
can be estimated as:
time (in the baseline case Ttotal = Ttotal
lost-base =  × (OCILW + δLW) × Failnum
T LW
lost-base =  × (OCIHW + δHW) × Failnum
T HW
total
total
(4)
(5)
Where  is the average fraction of lost work per failure.
For estimating useful work and checkpointing overhead, we
can divide the time segment between two failures in chunks
of optimal checkpointing interval plus checkpointing over-
head (OCI + δ). For probabilistic modeling, one can imagine
that there are inﬁnite such segments and calculate the prob-
ability of failure after each segment. Note that the average
(OCI+δ). As discussed previ-
number of such segments is
ously, the number of failures between time segments i and
i + 1 is given by Failnum
(i×(OCILW+δLW),(i+1)×(OCILW+δLW)). As a
short hand notation, we denote this as Failnum
i,i+1(OCILW +
δLW). Successful completion of a segment results in use-
ful work equivalent to the optimal checkpointing interval.
Therefore, the useful work for the two applications in the
baseline case can be mathematically expressed as:
M
T LW
useful-base =
T HW
useful-base =
∞(cid:2)
i=1
∞(cid:2)
i=1
i × OCILW × Failnum
i,i+1(OCILW + δLW)
i × OCIHW × Failnum
i,i+1(OCIHW + δHW)
(6)
(7)
86
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:30:14 UTC from IEEE Xplore.  Restrictions apply. 
Similarly, the checkpointing overhead per successful seg-
ment of (OCI + δ) is equal to the cost of one checkpoint.
Therefore, the I/O overhead in the baseline case is:
T LW
io-base =
T HW
io-base =
∞(cid:2)
i=1
∞(cid:2)
i=1
i × δLW × Failnum
i,i+1(OCILW + δLW)
i × δHW × Failnum
i,i+1(OCIHW + δHW)
(8)
(9)
This approach of modeling leads to an elegant formula-
tion for the Shiraz case as well. The index for the summa-
tion terms does not range from 1 to ∞ now. Instead, for the
light-weight application, the index will range from 1 to the
switching point (k). We refer to the switching point as the
number of checkpoints (say, k) the light-weight application
takes before yielding to the heavy-weight application. Note
that the total time period the light-weight application gets to
run is k× (OCILW + δLW). For the heavy-weight application,
the index will range from k to ∞. Note that for the heavy-
weight application, each of the segments (i, i + 1, . . . ) are
still (OCIHW + δHW) long, but the ﬁrst such segment starts
after k × (OCILW + δLW) time since the last failure. Now,
we can write the expressions for useful work, checkpointing
overhead, and lost work for the Shiraz case as follows:
k(cid:2)
i=1
∞(cid:2)
i=k
k(cid:2)
T LW
useful-shiraz =
T HW
useful-shiraz =
T LW
io-shiraz =
i × OCILW × Failnum
i,i+1(OCILW + δLW)
i × OCIHW × Failnum
i,i+1(OCIHW + δHW)
i × δLW × Failnum
i,i+1(OCILW + δLW)
(10)
(11)
(12)