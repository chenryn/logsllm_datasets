acteristics. More importantly, unlike dataflow or pipeline models
(as in Floem [53]), which are designed to support data-intensive
control-light workloads, the actor-based model can support control-
intensive, non-deterministic, and irregular communication patterns
that arise in complex distributed applications.
An actor is a computation agent that performs two kinds of op-
erations based on the type of an incoming message: (1) trigger an
execution handler and manipulate its private state; (2) interact with
other actors by sending messages asynchronously. Actors do not
share memory. We choose message passing as the communication
paradigm given the following considerations: (1) the communica-
tion latencies that we observe between the NIC and host are in the
order of microseconds; (2) actors are independent entities without
shared state. In our system, every actor has an associated structure
with the following fields: (1) init_handler and exec_handler for state
initialization and message execution; (2) private_state, which can
use different data types (as described in Section 3.3); (3) mailbox is
a multi-producer multi-consumer concurrent FIFO queue, which is
used to store incoming asynchronous messages; (4) exec_lock, used
to decide whether an actor can be executed on multiple cores; (5)
some runtime information, such as port, actor_id, and a reference
to the actor_tbl, which contains the communication addresses of
other actors. An actor runs on one or more cores, and it is the
programmers’ responsibility to provide concurrency control for
operating on an actor’s private state.
The iPipe runtime enables the actor-based model by providing
support for actor allocation/destruction, runtime scheduling of
actor handlers, and transparent migration of actors and its associ-
ated state. (See Table 4 in the Appendix B.1 for the runtime API.)
Specifically, iPipe has three key system components: (1) an actor
scheduler that works across both SmartNIC and host cores and
uses a hybrid FCFS/DRR scheduling discipline to enable execution
of actor handlers with diverse execution costs; (2) a distributed
object abstraction that enables flexible actor migration and sup-
ports a software managed cache to mitigate the cost of SmartNIC
to host communications; (3) a security isolation mechanism that
protects actor state from corruption and denial-of-service attacks.
We describe these components below.
3.2 iPipe Actor Scheduler
iPipe schedules actor executions on both the SmartNIC and the host
cores. The scheduler assigns actor execution tasks to computing
cores and specifies a custom scheduling discipline for each actor
task. In designing the scheduler, we not only want to maximize
 0 2 4 6 8 10 124816326412825651210242048Throughput (Mops)Payload size (B)DMA blocking readDMA non-blocking readDMA blocking writeDMA non-blocking write 0 1 2 3 4 5 64816326412825651210242048Latency (us)Payload size (B)RDMA one-sided readRDMA one-sided write 0 0.5 1 1.5 24816326412825651210242048Throughput (Mops)Payload size (B)RDMA one-sided readRDMA one-sided writeSIGCOMM ’19, August 19–23, 2019, Beijing, China
M. Liu et al.
the computing resource utilization on the SmartNIC but also en-
sure that the computing efficiency does not come at the cost of
increased tail latencies or compromising the NIC’s ability to convey
traffic. Recall that, in the case of on-path SmartNICs, all traffic is
conveyed through SmartNIC cores, so executing actor handlers
could adversely impact the latency and throughput of other traffic.
3.2.1 Problem formulation and background. The runtime
system executes on both the host and the SmartNIC, determines
on which side an actor executes, and schedules the invocation of
actor handlers. There are two critical decisions in the design of
the scheduler: (a) whether the scheduling system is modeled as a
centralized, single queue model or as a decentralized, multi-queue
model, and (b) the scheduling discipline used for determining the
order of task execution. We consider each of these issues below.
It is well-understood that the decentralized multi-queue model
can be implemented without synchronization but suffers from tem-
porary load imbalances leading to increased tail latencies. Fortu-
nately, on-path SmartNICs enclose hardware traffic managers that
provide support for a shared queue abstraction with low synchro-
nization overheads (see Section 2.2.2). We, therefore, resort to using
a centralized queue model on the SmartNIC and a decentralized
multi-queue model on the host side, along with NIC-side support
for flow steering. We discuss later how to build such a scheduler
for off-path SmartNICs.
We next consider the question of what scheduling discipline to
use and how that impacts the average and tail response times for
scheduled operations (i.e., both actor handlers and message forward-
ing operations). Note that the response time or sojourn time is the
total time spent, including queueing delay and request processing
time. If our goal is to optimize mean response time, then Shortest
Remaining Processing Time (SRPT) and its non-preemptive coun-
terpart, Shortest Job First (SJF), are considered optimal regardless
of the task size and interarrival time distributions [56]. However, in
our setting, we also care about the tail response time; even if a given
application can tolerate it, other applications sharing the resources
might be impacted. Further, a high response latency also means that
an on-path SmartNIC would not be able to perform its basic duty
of forwarding traffic in a timely manner. If we were to consider
minimizing the tail response time, then First Come First Served
(FCFS) is considered optimal when task size distribution has low
variance [61]. However, FCFS has been shown to perform poorly
when the task size distribution has high dispersion or is heavy-
tailed [3]. In contrast, Processor Sharing is considered optimal for
high variance distributions [64].
In addition to the issues described above, the overall setting of
our problem is unique. Our runtime manages the scheduling on
both the SmartNIC and the host with the flexibility to move actors
between the two computing zones. Crucially, we want to increase
the occupancy on the SmartNIC, without overloading it or causing
tail latency spikes, and the runtime can shed load to the host if nec-
essary. Furthermore, given that the offloaded tasks will likely have
different cost distributions (as we saw in our characterization experi-
ments), we desire a solution that is suitable for a broad class of tasks.
Figure 11: An overview of iPipe scheduler on the SmartNIC. Cond is
the condition that triggers an operation.
3.2.2 Scheduling algorithm. We propose a hybrid scheduler
that: (1) combines FCFS and DRR (deficit round robin) service dis-
ciplines; (2) migrates actors between SmartNIC and host proces-
sors when necessary. Essentially, the scheduler takes advantage
of FCFS for tasks that have low dispersion in their service times
and delegates tasks with a higher variance in service times to a
DRR service discipline. The scheduler uses DRR for high variance
tasks as DRR is an efficient approximation of Processor Sharing
in a non-preemptible setting [58]. Further, the scheduler places
as much computation as possible on the SmartNIC and migrates
actors when the NIC cannot promptly handle incoming packets.
For performing these transitions, the scheduler collects statistics
regarding the average and the tail execution latencies, actor-specific
execution latencies, and queueing delays. We mainly describe the
NIC-side scheduler below and then briefly describe how the host-
side scheduler differs from it.
The scheduler works as follows. Initially, all scheduling cores
start in FCFS mode, where they fetch packet requests from the
shared incoming queue, dispatch requests to the target actor based
on their flow information, and perform run-to-completion execu-
tion (see lines 5-6, 11-12 of ALG 1 in Appendix). When the measured
tail latency of operations in the FCFS core is higher than tail_thresh,
the scheduler downgrades the actor with the highest dispersion (a
measure that we describe later) by pushing the actor into a DRR
runnable queue and spawns a DRR scheduling core if necessary
(lines 13-16 ALG 1). All DRR cores share one runnable queue to
take advantage of the execution parallelism.
We next consider the DRR cores (see ALG 2 in Appendix). These
cores scan all actors in the DRR runnable queue in a round-robin
way. When the deficit counter of an actor is larger than its esti-
mated latency, the core pops a request from the actor’s mailbox and
conducts its execution. The DRR quantum value for an actor, which
is added to the counter in each round, is the maximum tolerated for-
warding latency for the actor’s average request size (obtained from
the measurements in Section 2.2.2). When the measured tail latency
of operations performed by FCFS is less than (1 − α)tail_thresh
(where α is a hysteresis factor), the actor with the lowest dispersion
in the DRR runnable queue is pushed back to the FCFS group (lines
10-12 of ALG 2).
Finally, when the scheduler detects that the mean request latency
for FCFS jobs is larger than mean_thresh, it recognizes that there is
a queue build-up at the SmartNIC. It then migrates to the host pro-
cessor the actor that contributes the most to the NIC’s processing
323
….reqsActorsdowngrade Cond: Tail > Tail_threshupgrade Cond: Tail  Mean_threshFCFS pull migration Cond: Mean  Q_threshOffloading Distributed Applications onto SmartNICs using iPipe
SIGCOMM ’19, August 19–23, 2019, Beijing, China
load (lines 17-23 ALG 1). Similarly, when the mean request latency
of the FCFS core group is lower than (1 − α)mean_thresh and if
there is sufficient CPU headroom in the FCFS cores, the scheduler
pulls from the host server the actor that will incur the least load
back to the SmartNIC. To minimize synchronization costs, we use
a dedicated core of the FCFS group for the migration tasks.
3.2.3 Bookeeping execution statistics. Our runtime mon-
itors the following statistics to assist the scheduler: (1) Request
execution latency distribution of all actors: We measure µ, the exe-
cution latency of each request (including its queueing delay) using
microarchitectural time stamp counters. To efficiently approximate
the tail of the distribution, we also track the standard deviation of
the request latency σ and use µ +3σ as a tail latency measure. Note
that this is close to the P99 measure for normal distributions. All of
these estimates are updated using exponentially weighted moving
averages (EWMA). (2) Per-actor execution cost and dispersion sta-
tistics. For each actor i, we track its request latency µi, the standard
deviation of the latency σi, request sizes, and the request frequency.
We use µi + 3σi as a measure of the dispersion of the actor’s re-
quest latency. Again, we use EWMA to update these measures. (3)
Per-core/per-group CPU utilization. We monitor the per-core CPU
usage for the recent past and also use its EWMA to estimate its
current utilization. The CPU group utilization (for FCFS or DRR) is
the average CPU usage of the corresponding cores. Finally, we use
measurements from our characterization study to set the thresh-
olds mean_thresh and tail_thresh. We consider the MTU packet size
at which the SmartNIC can sustain line rate and use the average
and P99 tail latencies experienced by traffic forwarded through the
SmartNIC as the corresponding thresholds (Section 2.2.2). These
thresholds mean that we provide the same level of service with
offloaded computations as when we have full line-rate processing
of moderately sized packets.
3.2.4 FCFS and DRR core auto-scaling. All cores start in
FCFS mode. When an actor is pushed into the DRR runnable queue,
the scheduler spawns a core for DRR execution. When all cores in
the DRR group is nearly fully used (CPUDRR ≥ 95% and the CPU
usage of the FCFS group is less than 100×(FCF SCor e#−1)
%, FCFS is
able to spare a core for DRR service, and the scheduler will migrate
a core to the DRR group. We use a similar condition for moving a
core back to the FCFS group.
FCF SCor e#
3.2.5 SmartNIC push/pull migration. We only allow the
SmartNIC to initiate the migration operation since it is much more
sensitive than the host processor in case of overloading. As de-
scribed above, when there is persistent queueing (i.e., the mean
response time is above a threshold), the scheduler will move an ac-
tor to the host side. In particular, actor migration is triggered based
on the SmartNIC’s processing load and the incoming request traffic.
We pick the actor with the highest load (i.e., average execution la-
tency scaled by frequency of invocation) for migration and ensure
that the actor does not serve requests when it is being migrated.
We perform migration in four steps. First, the actor transitions into
the Prepare state and removes itself from the runtime dispatcher.
An actor in the DRR group is also removed from the DRR runnable
queue. The actor stops receiving incoming requests and buffers
them in the iPipe runtime. Second, the actor finishes the execution
Figure 12: iPipe’s distributed memory objects.
of its current tasks and transitions to the Ready state. Note that, for
an actor in the DRR group, it finishes executing all the requests in
its mailbox. Third, the scheduler moves the distributed objects of an
actor to the host runtime, starts the host actor, and marks the NIC
actor state as Gone. Finally, the scheduler forwards the buffered
requests from the NIC to the host and rewrites their destination
addresses. We will then label the NIC actor as Clean. Appendix B.3
provides more details on the migration process and its performance.
3.2.6 SmartNICs with no hardware traffic managers. We
now consider SmartNICs that do not provide a shared queue abstrac-
tion to the NIC processor (especially off-path ones like BlueField
and Stingray). There are two possible ways to overcome this lim-
itation. One is to apply a dedicated kernel-bypass component (such
as the IOKernel module in Shenango [51]) that processes all in-
coming traffic and exposes FCFS cores a single queue abstraction.
This module will run on one or several NIC cores exclusively, de-
pending on the traffic load. Another way is to add an intermediate
shuffle layer across FCFS cores. Essentially, this shuffle queue is a
single-producer, multiple-consumer one. This approach could cause
load imbalances due to flow steering. Similar to ZygOS [54], one
should also allow a FCFS core to steal other cores’ requests when
it becomes idle with no pending requests in its local queue. Note
that both approaches bring in performance overheads, so future
on-path/off-path SmartNICs could benefit from adding a hardware
traffic manager to simplify NIC-side computation scheduling.
Summary: The scheduler manages the execution of actor requests
on both the SmartNIC and the host. We use a hybrid scheme that
combines FCFS and DRR on both sides. With the scheme outlined
above, lightweight tasks with low dispersion are executed on the
SmartNIC’s FCFS cores, lightweight tasks with high dispersion are
executed on the SmartNIC’s DRR cores, and heavyweight tasks are
migrated to the host. These decisions are performed dynamically
to meet the desired average and tail response times.
3.3 Distributed memory objects
iPipe provides a distributed memory object (DMO) abstraction to en-
able flexible actor migration. Actors allocate and de-allocate DMOs
as needed, and a DMO is associated with the actor that allocated it;
there is no sharing of DMOs across actors. iPipe maintains an ob-
ject table (Figure 12-a) on both sides and utilizes the local memory
manager to allocate/de-allocate copies. At any given time, a DMO
has only one copy, either on the host or on the NIC. We also do
not allow an actor to perform reads/writes on objects across the
324
Object IDSizeStart addressActor ID01KB0x10f0000000Object IDSizeStart addressActor ID11KB0xfc00000001iPipe-host object tableiPipe-NIC object tablex2KB0x10f001234xx2KB0xfc0001234xy4KB0x10f005678yz4KB0x10f005678zx8KB0x10f00abcdxx8KB0x10f00abcdxstruct node{    char key[KEY_LEN];        char *val;    struct node *forwards[MAX_LEVEL];}Normal SkipList node(a). Object migrationstruct node{    char key[KEY_LEN];        int val_object;    int forward_obj_id[MAX_LEVEL];}DMO SkipList node (b). Skiplist node implementation in DMOSIGCOMM ’19, August 19–23, 2019, Beijing, China
M. Liu et al.
PCIe because remote memory accesses are 10x slower than local
ones (as shown in Section 2.2). Instead, iPipe would automatically
move DMOs along with the actor, and all DMO read/write/copy-
/move operations are performed locally. During the initialization
phase, iPipe creates large equal-sized chunks of memory regions
for each registered actor. On LiquidIOII SmartNICs, we realize this
via the "global bootmem region" of the firmware. The iPipe runtime
maintains the mapping between actor ID, its base address, and size.
During execution, an actor can only allocate/reclaim/access objects
within its region. When an actor consumes more memory than the
framework provides, the DMO allocation will fail.
When using DMOs to design a data structure, one has to use
object ID for indexing instead of pointers. This approach provides
a level of indirection so that we can change the actual location of