7.23 (2.77)
7.65 (2.98)
7.66 (3.45)
5839
2209
1028
1200
pants to go through a tutorial and ﬁll up a demographics
form. Next, we asked the participants to solve several in-
stances of the game challenges explained in Section 3.2. At
the end of the study, we asked the participants to ﬁll-out a
survey form about their experience. The survey contained
the 10 System Usable Scale (SUS) [4] standard questions,
each with 5 possible responses (5-point Likert scale, where
strong disagreement is represented by “1” and strong agree-
ment is represented by “5”). SUS is a standard questionnaire
to measure the usability of software, hardware, cell phones
and websites, and it has been deployed in many prior se-
curity usability studies. Moreover, speciﬁc to our study,
we added two questions to the survey in order to measure
the easiness and playfulness of the challenges. As the par-
ticipants played the game challenges, all of their gameplay
mouse events were recorded in the background.
Table 2 summarizes the characteristics of the data col-
lected during the two studies. The total number of partici-
pants is 118 (98 in online study and 20 in lab study). The
participants successfully completed a total of 10276 chal-
lenges (9076 in online study and 1200 in lab study). The
average time to complete a game challenge was around 7.5
seconds.
For our online data collection study, we utilized the Ama-
zon Mechanical Turk (AMT) service to recruit the partic-
ipants. The aim of our online study was to evaluate the
applicability of identifying the user based on the way she
interacts with the posed game challenges. Moreover, we
wanted to determine how our system would perform in a
longitudinal setting, over multiple sessions/days. Therefore,
we created a total of three Human Intelligence Tasks (HITs)
distributed over three days. The ﬁrst HIT was created with
100 assignments to have 100 unique workers. We gathered
98 valid submissions until the HIT expired. The workers
were directed to the website hosting the study. They were
required to solve a tutorial, ﬁll a demographics form and
play 60 instances of our challenges. The order of presenting
the challenges to the participants was random. Finally, the
participants ﬁlled out the survey. On the next two days,
we sent the participants emails asking them to participate
in the follow-up study. However, we asked them to solve
36 challenges rather than 60 challenges in this round. 62
participants performed the study on the second day and 29
performed the study on the third day. We paid each par-
ticipant $1.0 for the ﬁrst HIT, and $0.5 each for the second
and third HIT.
The participants in our online study were from various age
groups, education levels and backgrounds. Age group: 1%  50.
Gender: 58.2% male and 41.8% female. Education: 26.5%
high school graduate, 58.2% hold bachelor degree, 14.3%
hold master degree and 1% hold a PhD degree. The partici-
pants were from various backgrounds such as Computer Sci-
(a) Brands
(b) Animals
(c) Professions
Figure 1: Challenges instances. Targets, on the left, are
static; moving objects, on the right, are mobile. The user
task is to drag-drop a subset of the moving objects (answer
objects) to their corresponding targets
• Professions: The targets are professionals and the
moving objects are tools (e.g., taxi driver and taxi).
4. DATA COLLECTION
As a pre-requisite to building and testing our Gametrics
system, we pursued data collection from human users, in
both online and lab settings. In this section, we elaborate
on our data collection methodology, and the characteristics
of the collected data set.
The participation in our two studies was voluntary, and
standard ethical procedures were fully followed, e.g., partic-
ipants being informed, given choice to discontinue, and not
deceived. The studies was approved by our university’s In-
stitutional Review Board. The data collection experiments
were divided into four phases. First, we subjected the par-
ticipants to a consent form. Then, we asked the partici-
280
Feature
Description
Table 1: The Features Utilized for Classiﬁcation
e Time
v
i
t
i
Time ﬁrst action
Time ﬁrst drag
Time between drags
Speed drag
Speed move
Acceleration drag
Acceleration move
Diﬀerence timestamp
Move silence
Drag silence
Pause and drag
Pause and drop
Angle
n
g
o
C
n
o
i
t
c
a
r
e
t
n
i
e
s
u
o
M
d
e
x
i
M
Speed while dragging
Speed while moving
Time taken to complete the challenge
The timestamp of the ﬁrst mouse event after the game start
The timestamp of the ﬁrst drag
number
number
number
mean, std, min, max Times between drops and start of drags
mean, std, min, max
mean, std, min, max
mean, std, min, max Acceleration while dragging
mean, std, min, max Acceleration while moving
mean, std, min, max The diﬀerence between each consecutive recorded timestamps
mean, std, min, max The times between consecutive timestamps while the mouse is moving
mean, std, min, max The times between consecutive timestamps while dragging
mean, std, min, max The times between approaching the object and click on it
mean, std, min, max The times between approaching the target and drop
mean, std, min, max The angles between each three consecutive points
Drag distance to real distance mean, std, min, max
Move distance to distance
mean, std, min, max
The diﬀerence between the distance traveled while dragging and the
straight line connecting the start and end points of the drag
The diﬀerence between the distance traveled while moving and the
straight line connecting the start and end points of the move
Distance click object center
Distance drop target center
Total distance
mean, std, min, max Distances of the clicks and objects’ centers
mean, std, min, max Distances of the drops and targets’ centers
number
Total distance
ence, Engineering, Medicine, Law, Social Science, Finance,
Business, Mathematics, Art, etc.
(detailed demographics
information is populated in Table 6 in the Appendix)
For our lab-based study, we collected data from some vol-
unteers recruited from our University. It followed a similar
protocol as the online study, but using a lab computer. We
asked the volunteers to perform a similar task as the task
performed by the AMT workers on the ﬁrst day. A total
of 20 undergraduate and graduate students as well as some
employees participated in the study. The age of the partic-
ipants ranged between 19 and 50, 13 of them are male and
7 are female, 5 are high school graduate, 8 have bachelor
degree and 7 have master degree. The majority of the par-
ticipants are from Computer Science background (Table 6
in the Appendix). We asked the volunteers to play 60 in-
stances of the challenges using the same computer and same
setting. The aim of this study was to validate the results of
the AMT study. In particular, we mainly wanted to ensure
that the acquired results are not based on the platform and
the setting used in performing the experiment rather than
the diﬀerent characteristics of an individual’s unique way of
interacting/solving the game challenges.
5. SYSTEM DESIGN & RESULTS
In order to evaluate the applicability of the Gametrics as
an authentication scheme, we utilized the machine learning
approach.
In this section, we present the features we ex-
tracted from the user’s gameplay logs collected during our
data collection campaign. Then, we discuss the classiﬁca-
tion models and the classiﬁer employed. Finally, we present
the classiﬁcation results for the benign setting and the zero-
eﬀort attack.
5.1 Feature Extraction
From each instance of the gameplay logs we collected dur-
ing the data collection phase, we extracted a total of 64
features that captures the cognitive abilities as well as the
mouse interaction characteristics of the participants while
they are interacting with the challenges.
(The extracted
features are summarized in Table 1.)
As described in Section 3.1, in order to solve a challenge,
the user has to match the answer objects to their corre-
sponding targets. In order to do that, the user has to un-
derstand the content of the images representing the targets
and the moving objects, ﬁnd the relationship between the
moving objects and the target objects, and then select a sub-
set of the moving objects (the answer objects) and ﬁnally
drag/drop them to their corresponding targets. By mon-
itoring the users while solving the challenges (lab study),
we found diﬀerent users take diﬀerent approaches to solve
the challenges. For example, some users start by trying to
comprehend the whole challenge and then start the object
matching, while some try to ﬁnd the answer objects cor-
responding to the target in certain order (i.e., always try
to search for the answer object that corresponds to the top
most target, and then the second and so on), while some
try to pick the object closest to the mouse cursor and then
check if it matches with any of the targets. For visualization
purposes, these diﬀerences in the cognitive characteristics of
diﬀerent users are illustrated in Figure 2.
These diﬀerent mechanisms of solving the game challenges
are related to the cognitive characteristics of individuals. We
capture these characteristics based on the following features:
1. The time between the user pressing on the start button
and the ﬁrst recorded mouse event and the time of the
ﬁrst click/drag. These timing measures capture the
time the participates take to comprehend the challenge
and start solving it.
2. The times between each of the drops and the start of
the next drag (these capture the time the user takes
to ﬁnd the next answer object).
281
Figure 2: An example for illustration of diﬀerent cognitive characteristics among diﬀerent users while playing the game
challenges: the time for completing the games and the time spent in drag and time spent in moving the mouse around. We
can see that User 1 took a long time to understand the game (long move segment before the start of the ﬁrst drag), also took
on average a long time to locate each of the answer objects and to start dragging. User 2 took shorter time to complete the
challenges but committed many mistakes (the user performed exactly 3 drags and drops to complete each challenge, however,
User 2 performed on average more than 5 drags), User 3 completed the games in short time with shorter on average times to
locate the answer objects.
3. The total time taken by the user to complete the chal-
lenge.
The mouse movement characteristics of the users are cap-
tured by following features:
1. The speed and acceleration while the user is searching
for an answer object and while the user is dragging the
object.
2. The duration between each two consecutively gener-
ated timestamps and the “silence” during move and
during drag.
3. The time duration between reaching an object and
clicking on it, and the time duration between ap-
proaching a target object and dropping an answer ob-
ject on it.
4. The angles between the lines that connect each 3 con-
secutive points in the mouse movement trajectory.
Other mixed features are also extracted that relate to both
cognitive and mouse movement characteristics of the partic-
ipants such as the total distance the mouse moved within
a game challenge, the diﬀerence between the straight line
connecting the start and the end of a move or a drag and
the real distance traveled. The distance between a click and
the object center, and a drop and the target center.
5.2 Classiﬁer and Metrics
In our analysis, we utilized the Random Forest classiﬁer.
Random Forest is an ensemble approach based on the gen-
eration of many classiﬁcation trees, where each tree is con-
structed using a separate bootstrap sample of the data. In
order to classify a new input, the new input is run down
all the trees and the result is determined based on majority
voting. Random Forest is eﬃcient, can estimate the impor-
tance of the features, and is robust against noise [16]. Sev-
eral other classiﬁers were tested during the course of study
such as SVM, Bayes Network, Neural Networks, but Ran-
dom Forest outperformed all of them.
In our classiﬁcation task, the positive class corresponds to
the gameplay of the legitimate user and the negative class
corresponds to the impersonator (other user / zero-eﬀort at-
tacker). Therefore, true positive (TP) represents the number
of times the legitimate user is granted access, true negative
(TN) represents the number of times the impersonator is
rejected, false positive (FP) represents the number of times
the impersonator is granted access and false negative (FN)
represents the number of times the correct user is rejected.
As performance measures for our classiﬁer, we used false
positive rate (FPR), false negative rate (FNR), precision,
recall and F-measure (F1 score), as shown in Equations (1)
to (5). FPR and precision measure the security of the pro-
posed system, i.e., the accuracy of the system in rejecting
impersonators. FNR and recall measure the usability of the
proposed system as high FNR leads to high rejection rate of
the legitimate users. F-measure considers both the usability
and the security of the system. To make our system both
usable and secure, ideally, we would like to have FPR and
FNR to be as close as 0, and recall, precision and F-measure
to be as close as 1.
F P R =
F N R =
F N
T N + F N
F N
T P + F N
precision =
T P
T P + F P
(1)
(2)
(3)
282
Table 3: AMT Study Results: Performance for the classiﬁer for three diﬀerent classiﬁcation models. The ﬁrst part shows the
performance of the classiﬁer using all the features. The next part shows the results of using the features subset that provides
the best average results. The last part shows the result of using the best features subset for each user. For each of the models,
we show the results of using a single challenge and merging of two challenges. Highlighted cells emphasize the most interesting
results.
All features
Average overall best
User speciﬁc
FPR
FNR
Precion
Recall
F-Measure
Single
Merge
Single
Merge
Single
Merge
Day 1
Day 2
Day 3
Day 1
Day 2
Day 3
Day 1
Day 2
Day 3
Day 1
Day 2
Day 3
Day 1
Day 2
Day 3
Day 1
Day 2
Day 3
0.12 (0.10)
0.11 (0.09)
0.10 (0.07)
0.10 (0.13)
0.09 (0.11)
0.08 (0.10)
0.11 (0.09)
0.18 (0.13)
0.10 (0.07)
0.10 (0.13)
0.12 (0.12)
0.12 (0.18)
0.06 (0.06)
0.09 (0.09)
0.07 (0.06)
0.02 (0.05)
0.05 (0.09)
0.04 (0.06)
0.12 (0.16)
0.25 (0.31)
0.22 (0.27)
0.11 (0.18)
0.20 (0.30)
0.22 (0.30)
0.11 (0.15)
0.17 (0.15)
0.19 (0.26)
0.09 (0.16)
0.19 (0.20)