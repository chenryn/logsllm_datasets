t
e
s
f
f
O
−8
0
naive
reference
0.1
0.2
0.3
0.4
0.5
Te [day]
0.6
0.7
0.8
0.9
1
Figure 6: Naive per-packet offset estimates θi compared with
reference measurements.
identical to a histogram of (q←
i )/2, and is biased towards neg-
ative values in this case because the forward path is more heavily
utilised than the backward one.
i −q→
The value of ∆ places a hard limit on the accuracy of offset
measurement. The choice of server is therefore very important.
A nearby server will have a smaller RTT, and therefore a tighter
bound. More importantly however, a nearby server is likely to have
a path which is symmetric or close to it, which would result in
∆ (cid:11) r. This is in fact the case for ServerLoc and ServerInt,
which we measured (see table 2) to be of the order of 0.05 ms.
Estimating ∆ however is non-trivial. With only a single refer-
ence clock positioned as in ﬁgure 1, we use ∆ = d→ − d← =
r − d↑ − 2d←
which in terms of available timestamps reduces to
ˆ∆i = (Tf,i − Ta,i)∗ ˆp− 2Tg,i + Tb,i + Te,i. With i chosen to min-
imize ri, we obtain measurements with close to minimal network
and server delays, however timestamping errors remain, which are
signiﬁcant compared to the small size of ∆. Since Ta,i  tf,i, minimising ri will also tend to minimize times-
tamping errors at the host. At the server however, although clearly
Tb,i > tb,i (recall we assume the server is synchronised), the er-
ror Tb,i − tb,i is only bounded by the server delay d↑
and is not
inﬂuenced by minimising ri, and more seriously, the relationship
between Te,i and te,i is a priori unknown. Outliers in the reference
backward delay values {Tg,i−Te,i} suggest that in fact Te,i > te,i,
in very rare cases by as much as 1ms, larger even than the RTT! In
the future we will make a more reliable determination by reposi-
tioning the tap of the reference monitor. The results of the offset
estimation algorithm described in section 5.3 provide an alterna-
tive, indirect, way of estimating ∆ which agrees broadly with the
values in table 2).
5. SYNCHRONIZATION: THE REAL WORLD
It is intuitively clear from ﬁgures 5 and 6 that the packets carry-
ing large network delays can be detected, and therefore dealt with.
We show how this can be achieved even for small network delays,
and exploited to evolve the naive estimators into accurate ones. For
clarity of exposition, we leave some on the considerations needed
for an on-line implementation to the ﬁnal section, and focus on
the principles and performance of the underlying algorithms.
In
particular, when measuring offset we use a constant rate estimate
made over the entire trace, rather than using the on-line evolving
rate estimate ˆp(t) (this makes little difference in practice), and for
convenience we set TSC0 = 0.
5.1 Approach to Filtering
We need to measure the degree to which, for each packet i, the
available timestamps are affected by network queueing and other
factors. To do so we work with the round-trip time series {ri},
which has a number of important intrinsic advantages over the one-
way delays, {d→
i } and {d←
i }.
As discussed above, since Ta,i, Tf,i, are measured by the same
clock, and since round-trip times are very small, neither the un-
known θ(t) nor p(t) are needed to accurately measure ri. The same
is true for determining the quality of ri, only a reasonable estimate
such as an average ¯p is required. This creates a near complete de-
coupling of the underlying basis of ﬁltering from the estimation
tasks, thus avoiding the possibility of undesirable feedback dynam-
ics.
The absolute point error of a packet is taken to be simply ri − r.
(cid:8)t(cid:9)
The minimum can be effectively estimated by ˆr(t) = min
i=1 ri,
leading to an estimated error Ei = ri − ˆr(t) which is highly robust
to packet loss. Error will be calibrated in units of the maximum
timestamping error at the host, which we take to be δ = 15µs.
In contrast, one-way delays are measured by different clocks, so
that the ‘minimum’ inherits the wander of θ(t) (recall ﬁgure 2),
greatly complicating assessments of quality. On the other hand,
consider that with independent symmetric paths, if the probability
that one-way quality exceeds a given level is q, and q(cid:10)
for server
delay, then the corresponding probability drops below q(cid:10)q2 for the
RTT, which can be much smaller than q under congested condi-
tions. Thus quality packets are rarer when judged by the RTT
alone, making accurate estimation more challenging. An alterna-
tive which retains the inherent advantages of RTT whilst in princi-
ple increasing the proportion of quality packets is {ri − d↑
}. How-
ever, although the server is synchronized, its timestamping errors
serve only to add noise to the more reliable driver based timestamps
made at the host from which we derive {ri}.
5.2 Rate Synchronization
i
To bound the error on the estimate ˆp(t), we use equation (17)
but restrict ourselves to packets with bounded point error. The base
algorithm is simple. To initialise, set j and i to be the ﬁrst and
second packets with point errors below some E∗
. Equation (17)
then deﬁnes the ﬁrst value of ˆp(t) which we assign to t = tf,i. This
estimate holds for t ≥ tf,i up until i is updated at the next accepted
packet, and so on. An estimate of the error of the current estimate is
(Ei +Ej)/((Tf,i−Tf,j)¯p) and should be bounded by 2E∗/((Tf,i−
Tf,j)¯p). As before the above procedure is independently applied to
both the forward and backward paths, and the results averaged.
This scheme is inherently robust, since even if many packets are
rejected, error reduction is guaranteed through the growing ∆(t) =
Tf,i − Tf,j, without any need for complex ﬁltering. Even if connec-
tivity to the server were lost completely, the current value of ˆp re-
mains valid for meaningful ﬁltering, allowing estimation to recom-
mence at any time with no warm-up required.
Figure 7 plots the relative error of the resulting estimates with
respect to the corresponding reference rates for those i selected.
Two sets of results are given, for E∗ = 20δ and 5δ (resulting in
72% and 3.9% of packets being selected respectively), to show the
insensitivity of the scheme to E∗
. In each case errors rapidly fall
below the desired bound of 0.1 PPM and do not return, in contrast
to ﬁgure 5 based on the same raw data. The solid lines give ex-
pected upper bounds on the error based on 2E∗/(Tg,i − Tg,j). To
put this performance into context, note that for the measurement
of time differences over a few seconds and below, the estimate ˆp
above gives an accuracy better than 1µs, which is the same order of
magnitude as a GPS synchronized software clock, after only a few
minutes! For example inter-arrival times, round-trip times, and the
delay variation of network packets all fall into this category.
It is important to understand that the estimate ˆp above is really
that of the average rate over a large ∆(t) (cid:13) τ ∗
window, and is thus
an average of many different local or ‘true’ rates in the sense of the
SKM. From ﬁgure 3, true local rates can be meaningfully deﬁned
10−5
r
o
r
r
e
l
e
r
10−10
E* = 0.3ms
err bound
E* = 0.075ms
err bound
0.1 PPM
10−3
10−2
Te [day]
10−1
100
Figure 7: Relative error in ¯p estimates for E∗ = [20, 5]· δ =
[0.3, 0.075] ms. Errors fall below 0.1 PPM and remain there.
down to accuracies of  = 0.01 PPM, over scales below τ ∗
. How-
ever, there is no need for local rate estimates in order to obtain ˆp,
and ˆp is sufﬁcient to support ﬁltering and both the difference and
absolute clocks. This is an advantage since the estimation of local
rates is much more difﬁcult due to the small number of samples
available. However, there are two reasons why their measurement
may be worthwhile: (i) they could extend the time intervals over
which the difference clock Cd(t) can be used to measure time dif-
ferences, and (ii) to optimize the performance of ˆθ(t) and hence
that of the absolute clock Ca(t).
, in fact ¯τ ≡ 5000sec = 5τ ∗
In order to measure local rate to an accuracy close to the optimal
 = 0.01 PPM however, given that with ServerInt point error values
descend into noise for E∗  0.01 PPM to allow for estimation error) we accept the estimate,
else we are conservative and set ˆpl(tf,k) = ˆpl(tf,k−1). We then set
ˆpl(t) = ˆpl(tf,k), where packet k is the most recent packet arriving
before time t.
To ensure that any unexpected failures of the estimation proce-
dure cannot force the rate estimates to contradict the known phys-
ical behaviour of the hardware, if the relative difference between
two succesive rate estimates ever exceeds some multiple of the
0.1 PPM rate bound, we use 3 ∗ 10−7, then the previous value will
be duplicated as above: ˆpl(t) = ˆpl(tf,k). This guarantees that the
local rate estimate cannot vary too wildly no matter what data it
receives. One situation where this is needed is when the server
timestamps themselves are in error. This actually occurred in our
data set, as shown in the next section.
As it is important that the estimate be local to the packet k, W
should be chosen small. On the other hand W should be large
enough so that packets of reasonable quality will lie within it. By
selecting the best candidates in the near and far windows, we guar-
antee that there is an estimate for each k. Good quality is designed
into the scheme through the width of the central window. Robust-
ness to outliers is provided by the monitoring of the expected qual-
ity of the candidate estimate, and the high level sanity checking.
Consequently, we found that the results are not sensitive to the ex-
act value of W (we use W = 30).
The algorithm closely tracks the corresponding reference rate
values made over the same timescale. Using the same data as in
ﬁgure 7, with γ∗ = 0.05 PPM, ¯τ = 5τ ∗
and W = 30, over 99% of
the relative descrepancies from the reference were contained within
0.023 PPM. The outliers were due mainly to errors in the reference
rates, not instabilities in the estimation algorithm. Only 0.6% of
values were rejected by the quality threshold, and the sanity check
was not triggered.
5.3 Offset Synchronization
Our aim is to estimate θ(t) for arbitary t, using the naive ˆθi es-
timates from past packets as a basis. Note that for many applica-
tions, post processing of data would allow both future and past val-
ues to be used to improve estimates. In particular this makes good
performance immediately following long periods of congestion or
sequential packet loss much easier to achieve.
In this section we use data collected continuously in the machine-
room over the last 3 weeks of September 2003. The host was con-
nected to ServerInt, and 169 of the 113401 packets were lost (or
failed to have matching reference timestamps). We also present
comparative results from a week long trace using ServerLoc where
299 packets were unavailable, and a trace 2.7 weeks long using
ServerExt where 666 packets were missing.
For the estimate ˆp, large ∆(t) values were an asset. In contrast,
since θ(t) must be tracked, large time intervals between quality
packets would imply that the accepted ˆθi would be out of date.
This fundamental difference suggests a paradigm of using estimates
derived for each packet. Our approach consists of four stages: (i)
determining a total per-packet error ET
i which combines point error
and packet age, (ii) assigning a weight wi based on the total error,
(iii) combining the weighted point estimates to form ˆθ(t), and (iv)
a sanity check to ensure that ˆθ(t) will not evolve faster than the
known hardware performance for any reason.
(i) Based on a packet i arriving before time t, the simplest ap-
proach is simply to set ˆθ(t) = ˆθi. The magnitude of the result-
ing error can be estimated by inﬂating the point error by a bound
i = Ei + 10−7(Cd(t) − Cd(Tf,i)).
on its growth over time: ET
This however is overly pessimistic as the residual rate error (from
the ˆp used to calculate ˆθi) is more likely to be of the order of 
i =
(from section 5.2). We therefore estimate the total error as ET
Ei + (Cd(t) − Cd(Tf,i)).
(ii) First we consider only those packets which fall into a SKM
related window τ (cid:10)
seconds wide before t, as we only know how
to relate current and past offset values within the context of the
SKM. For each packet i within the window we penalise poor to-
tal quality very heavily by assigning a quality weight via wi =
exp(−(ET
i /E)2), which has a maximum of 1, and becomes very
small as soon as the total quality lies away from a band deﬁned by
the size of E > 0. The graphs below justify the particular choices
τ (cid:10) = τ ∗
and E = 4δ.
(iii) An estimate can now be formed through a weighted sum
over the window:
ˆθ(t) =
X
X
wi,
wi
ˆθi/
i
i
(20)
which amounts to a constant predictor on a packet by packet basis.
The local rate estimates can be used to introduce linear prediction
instead:
ˆθ(t) =
wi(ˆθi − ˆγl (Cd(t) − Cd(Tf,i)))
“X
i
X
”
/
i
wi
(21)
where ˆγl = ˆpl(tf,i)/¯p − 1 is the estimate of the residual rate error
relative to ¯p (implicitly already present in ˆθi).
P
If all the packets in the window have poor quality then even the
Indeed, under periods of
weighted estimate can perform poorly.
wi = 0 to machine precision.
high congestion we may ﬁnd that
i ) > E∗∗
To avoid being inﬂuenced in such cases, when mini(ET
,
we instead base the estimate on the last weighted estimate taken.
In the case where this is at the last packet (we always evaluate the
offset at packet times), this gives
i
ˆθ(t) = ˆθ(tf,i)
ˆθ(t) = ˆθ(tf,i) − ˆγl ∗ (Cd(t) − Cd(Tf,i)),
(22)
(23)
depending upon whether the local rate reﬁnement is used or not
(here i is the last packet). We set E∗∗ = 6E, which is about 3
‘standard deviations’ away in the Gaussian-like weight function,
so that the weighted estimate will only be abandoned when quality
is extremely poor.
(iv) Just as for local rates, we put in place a high level san-
ity check to ensure that the offset estimate cannot vary in a way
which we know is impossible, no matter what data it receives. If
successive offset estimates differ by more than a threshold then the
most recent trusted value will simply be duplicated, for example
ˆθ(t) = ˆθ(tf,i) if the last such was at packet i. We set the thresh-
old at Es = 1ms, which is orders of magnitude beyond the ex-
pected offset increment between neighboring packets.
It is very
important that such a simple thresholding be used only as a sanity
check, meaning that the threshold be set very high. Attempting to
reduce this value to ‘tune’ its performance would be tantamount to
replacing the main ﬁltering algorithm with a crude alternative dan-
gerously subject to ‘lock-out’, where an old estimate is duplicated
ad inﬁnitum. An instance when the sanity check was needed will
be given in the next section.
An example of estimates made at successive packet arrivals is
the al-
given in ﬁgure 8. The performance is very satisfactory:
gorithm succeeds in ﬁltering out the noise in the naive estimates
(shown in the background), producing estimates which are only
around 30µs away from the reference values.
8.2
8.1
8
7.9