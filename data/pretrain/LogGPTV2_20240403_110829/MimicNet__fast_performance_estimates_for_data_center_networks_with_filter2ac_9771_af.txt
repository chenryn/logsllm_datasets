Conference on SIGCOMM, SIGCOMM ’14, pages 503–514, New York, NY, USA,
2014. ACM.
[6] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. Data
center tcp (dctcp). In Proceedings of the ACM SIGCOMM 2010 Conference, SIG-
COMM ’10, pages 63–74, New York, NY, USA, 2010. ACM.
[7] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat,
and Masato Yasuda. Less is more: Trading a little bandwidth for ultra-low latency
in the data center. In Presented as part of the 9th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 12), pages 253–266, San Jose, CA, 2012.
USENIX.
[8] Theophilus Benson, Aditya Akella, and David A. Maltz. Network traffic charac-
teristics of data centers in the wild. In Proceedings of the 10th ACM SIGCOMM
Conference on Internet Measurement, 2010.
[9] Lawrence S. Brakmo, Sean W. O’Malley, and Larry L. Peterson. Tcp vegas: New
techniques for congestion detection and avoidance. SIGCOMM Comput. Commun.
Rev., 24(4):24–35, October 1994.
[10] Henri Casanova, Arnaud Giersch, Arnaud Legrand, Martin Quinson, and Frédéric
Suter. Versatile, scalable, and accurate simulation of distributed applications and
platforms. Journal of Parallel and Distributed Computing, 74(10):2899–2917, June
2014.
[11] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale
recurrent neural networks. arXiv preprint arXiv:1609.01704, 2016.
[12] Jeffrey Dean and Sanjay Ghemawat. MapReduce: Simplified data processing on
large clusters. In OSDI, 2004.
[13] Charles Elkan. The foundations of cost-sensitive learning. In Proceedings of the
Seventeenth International Joint Conference on Artificial Intelligence, IJCAI 2001,
Seattle, Washington, USA, August 4-10, 2001, pages 973–978, 2001.
[14] G. Ewing, Krzysztof Pawlikowski, and Donald Mcnickle. Akaroa-2: Exploiting
network computing by distributing stochastic simulation. Proceedings of 13th
European Simulation Multiconference, ESM’99, pages 175–181, 6 1999.
[15] Ari Fogel, Stanley Fung, Luis Pedrosa, Meg Walraed-Sullivan, Ramesh Govindan,
Ratul Mahajan, and Todd Millstein. A general approach to network configuration
analysis. In Proceedings of the 12th USENIX Conference on Networked Systems
Design and Implementation, NSDI’15, pages 469–483, Berkeley, CA, USA, 2015.
USENIX Association.
[16] Andrew Frohmader and Hans Volkmer. 1-wasserstein distance on the standard
simplex. CoRR, abs/1912.04945, 2019.
[17] Richard M. Fujimoto. Parallel discrete event simulation. In Proceedings of the 21st
Winter Simulation Conference, Washington, DC, USA, December 4-6, 1989, pages
19–28, 1989.
[18] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta
Sengupta. Vl2: A scalable and flexible data center network. In Proceedings of the
ACM SIGCOMM 2009 Conference on Data Communication, SIGCOMM ’09, pages
51–62, New York, NY, USA, 2009. ACM.
[19] Chuanxiong Guo, Haitao Wu, Zhong Deng, Gaurav Soni, Jianxi Ye, Jitu Padhye,
and Marina Lipshteyn. Rdma over commodity ethernet at scale. In Proceedings of
the 2016 ACM SIGCOMM Conference, SIGCOMM ’16, pages 202–215, New York,
NY, USA, 2016. ACM.
[20] Chuanxiong Guo, Lihua Yuan, Dong Xiang, Yingnong Dang, Ray Huang, Dave
Maltz, Zhaoyi Liu, Vin Wang, Bin Pang, Hua Chen, Zhi-Wei Lin, and Varugis
Kurien. Pingmesh: A large-scale system for data center network latency mea-
surement and analysis. In Proceedings of the 2015 ACM Conference on Special
Interest Group on Data Communication, SIGCOMM ’15, pages 139–152, New York,
NY, USA, 2015. ACM.
[21] Thomas R. Henderson, Mathieu Lacage, and George F. Riley. Network simulations
with the ns-3 simulator. In In Sigcomm (Demo, 2008.
[22] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
In The Annals of
[23] Peter J. Huber. Robust estimation of a location parameter.
computation, 9(8):1735–1780, 1997.
Mathematical Statistics, pages 73–101, 1964.
[24] Vimalkumar Jeyakumar, Mohammad Alizadeh, David Mazières, Balaji Prabhakar,
and Changhoon Kim. Eyeq: Practical network performance isolation for the
multi-tenant cloud. In 4th USENIX Workshop on Hot Topics in Cloud Computing
(HotCloud 12), Boston, MA, June 2012. USENIX Association.
[25] Charles W. Kazer, João Sedoc, Kelvin K. W. Ng, Vincent Liu, and Lyle H. Ungar.
Fast network simulation through approximation or: How blind men can describe
elephants. In Proceedings of the 17th ACM Workshop on Hot Topics in Networks,
HotNets 2018, Redmond, WA, USA, November 15-16, 2018, pages 141–147. ACM,
2018.
[26] Ahmed Khurshid, Wenxuan Zhou, Matthew Caesar, and P. Brighten Godfrey.
Veriflow: Verifying network-wide invariants in real time. SIGCOMM Comput.
Commun. Rev., 42(4):467–472, September 2012.
[27] Hyojoon Kim, Joshua Reich, Arpit Gupta, Muhammad Shahbaz, Nick Feamster,
and Russ Clark. Kinetic: Verifiable dynamic network control. In Proceedings of
the 12th USENIX Conference on Networked Systems Design and Implementation,
NSDI’15, pages 59–72, Berkeley, CA, USA, 2015. USENIX Association.
[28] Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan M. G. Wassel, Xian
Wu, Behnam Montazeri, Yaogong Wang, Kevin Springborn, Christopher Alfeld,
Michael Ryan, David Wetherall, and Amin Vahdat. Swift: Delay is simple and
effective for congestion control in the datacenter. In Proceedings of the Annual
Conference of the ACM Special Interest Group on Data Communication on the
Applications, Technologies, Architectures, and Protocols for Computer Communica-
tion, SIGCOMM ’20, page 514–528, New York, NY, USA, 2020. Association for
Computing Machinery.
[29] Bob Lantz, Brandon Heller, and Nick McKeown. A network in a laptop: Rapid
prototyping for software-defined networks. In Proceedings of the 9th ACM SIG-
COMM Workshop on Hot Topics in Networks, Hotnets-IX, pages 19:1–19:6, New
York, NY, USA, 2010. ACM.
[30] LBNL. network simulator man page. https://ee.lbl.gov/ns/man.html.
[31] W. E. Leland, M. S. Taqqu, W. Willinger, and D. V. Wilson. On the self-similar na-
ture of ethernet traffic (extended version). IEEE/ACM Transactions on Networking,
2(1):1–15, Feb 1994.
[32] Hongqiang Harry Liu, Yibo Zhu, Jitu Padhye, Jiaxin Cao, Sri Tallapragada, Nuno P.
Lopes, Andrey Rybalchenko, Guohan Lu, and Lihua Yuan. Crystalnet: Faithfully
emulating large production networks. In Proceedings of the 26th Symposium on
Operating Systems Principles, SOSP ’17, page 599–613, New York, NY, USA, 2017.
Association for Computing Machinery.
[33] Vincent Liu, Daniel Halperin, Arvind Krishnamurthy, and Thomas Anderson.
F10: A fault-tolerant engineered network, 2013.
[34] OpenSim Ltd. Omnet++, 2018. http://omnetpp.org.
[35] Haohui Mai, Ahmed Khurshid, Rachit Agarwal, Matthew Caesar, P. Brighten
Godfrey, and Samuel Talmadge King. Debugging the data plane with anteater.
In Proceedings of the ACM SIGCOMM 2011 Conference, SIGCOMM ’11, pages
290–301, New York, NY, USA, 2011. ACM.
[36] Saverio Mascolo, Claudio Casetti, Mario Gerla, M. Y. Sanadidi, and Ren Wang.
TCP westwood: Bandwidth estimation for enhanced transport over wireless links.
In Christopher Rose, editor, MOBICOM 2001, Proceedings of the seventh annual
international conference on Mobile computing and networking, Rome, Italy, July
16-21, 2001, pages 287–297. ACM, 2001.
[37] David Meisner, Junjie Wu, and Thomas F. Wenisch. Bighouse: A simulation
In IEEE International Symposium on
infrastructure for data center systems.
Performance Analysis of Systems & Software, 2012.
[38] Vishal Misra, Wei-Bo Gong, and Don Towsley. Fluid-based analysis of a network
of aqm routers supporting tcp flows with an application to red. In Proceedings
of the Conference on Applications, Technologies, Architectures, and Protocols for
Computer Communication, SIGCOMM ’00, pages 151–160, New York, NY, USA,
2000. ACM.
[39] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats.
Timely: Rtt-based congestion control for the datacenter. In Proceedings of the 2015
ACM Conference on Special Interest Group on Data Communication, SIGCOMM
’15, pages 537–550, New York, NY, USA, 2015. ACM.
[40] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. Homa:
A receiver-driven low-latency transport protocol using network priorities. In
Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Com-
munication, SIGCOMM ’18, page 221–235, New York, NY, USA, 2018. Association
for Computing Machinery.
[41] John Nickolls, Ian Buck, Michael Garland, and Kevin Skadron. Scalable parallel
programming with cuda. Queue, 6(2):40–53, March 2008.
[42] nsnam. ns-3, 2017. http://nsnam.org.
[43] Amy Ousterhout, Jonathan Perry, Hari Balakrishnan, and Petr Lapukhov. Flex-
plane: An experimentation platform for resource management in datacenters. In
14th USENIX Symposium on Networked Systems Design and Implementation (NSDI
17), pages 438–451, Boston, MA, 2017. USENIX Association.
[44] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
Automatic differentiation in pytorch. 2017.
299
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Q. Zhang et al.
Community Summit, 2014.
[52] Zhangxi Tan, Zhenghao Qian, Xi Chen, Krste Asanovic, and David Patterson.
Diablo: A warehouse-scale computer network simulator using fpgas. In Pro-
ceedings of the Twentieth International Conference on Architectural Support for
Programming Languages and Operating Systems, ASPLOS ’15, pages 207–221,
New York, NY, USA, 2015. ACM.
[53] Andras Varga. The omnet++ discrete event simulation system. Proc. ESM’2001, 9,
01 2001.
[54] K. V. Vishwanath, D. Gupta, A. Vahdat, and K. Yocum. Modelnet: Towards a
datacenter emulation environment. In 2009 IEEE Ninth International Conference
on Peer-to-Peer Computing, pages 81–82, Sep. 2009.
[55] Damon Wischik, Costin Raiciu, Adam Greenhalgh, and Mark Handley. Design,
implementation and evaluation of congestion control for multipath TCP. In NSDI,
2011.
[56] Francis Y. Yan, Jestin Ma, Greg D. Hill, Deepti Raghavan, Riad S. Wahby, Philip
Levis, and Keith Winstein. Pantheon: the training ground for internet congestion-
control research. In 2018 USENIX Annual Technical Conference (USENIX ATC 18),
pages 731–743, Boston, MA, 2018. USENIX Association.
[57] Nofel Yaseen, Behnaz Arzani, Ryan Beckett, Selim Ciraci, and Vincent Liu. Aragog:
Scalable runtime verification of shardable networked systems. In 14th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 20), pages
701–718. USENIX Association, November 2020.
[58] Yibo Zhu, Nanxi Kang, Jiaxin Cao, Albert Greenberg, Guohan Lu, Ratul Mahajan,
Dave Maltz, Lihua Yuan, Ming Zhang, Ben Y. Zhao, and Haitao Zheng. Packet-
level telemetry in large datacenter networks. In Proceedings of the 2015 ACM
Conference on Special Interest Group on Data Communication, SIGCOMM ’15,
pages 479–491, New York, NY, USA, 2015. ACM.
[59] Danyang Zhuo, Qiao Zhang, Xin Yang, and Vincent Liu. Canaries in the network.
In Proceedings of the 15th ACM Workshop on Hot Topics in Networks, 2016.
[45] Dan R. K. Ports, Jialin Li, Vincent Liu, Naveen Kr. Sharma, and Arvind Krishna-
murthy. Designing distributed systems using approximate synchrony in data
center networks. In Proceedings of the 12th USENIX Conference on Networked
Systems Design and Implementation, 2015.
[46] Costin Raiciu, Sebastien Barre, Christopher Pluntke, Adam Greenhalgh, Damon
Wischik, and Mark Handley. Improving datacenter performance and robust-
ness with multipath tcp. In Proceedings of the ACM SIGCOMM 2011 Conference,
SIGCOMM ’11, pages 266–277, New York, NY, USA, 2011. ACM.
[47] Robert Ricci, Eric Eide, and The CloudLab Team. Introducing CloudLab: Scientific
infrastructure for advancing cloud architectures and applications. USENIX ;login:,
39(6), December 2014.
[48] Rob Sherwood, Glen Gibb, Kok-Kiong Yap, Guido Appenzeller, Martin Casado,
Nick McKeown, and Guru Parulkar. Can the production network be the testbed?
In Proceedings of the 9th USENIX Conference on Operating Systems Design and
Implementation, OSDI’10, pages 365–378, Berkeley, CA, USA, 2010. USENIX
Association.
[49] Rob Sherwood, Glen Gibb, Kok-Kiong Yap, Guido Appenzeller, Martin Casado,
Nick McKeown, and Guru Parulkar. Can the production network be the testbed?
In Proceedings of the 9th USENIX Conference on Operating Systems Design and
Implementation, OSDI’10, pages 365–378, Berkeley, CA, USA, 2010. USENIX
Association.
[50] Arjun Singh, Joon Ong, Amit Agarwal, Glen Anderson, Ashby Armistead, Roy
Bannon, Seb Boving, Gaurav Desai, Bob Felderman, Paulie Germano, Anand
Kanagala, Jeff Provost, Jason Simmons, Eiichi Tanda, Jim Wanderer, Urs Hölzle,
Stephen Stuart, and Amin Vahdat. Jupiter rising: A decade of clos topologies and
centralized control in google’s datacenter network. In Proceedings of the 2015
ACM Conference on Special Interest Group on Data Communication, SIGCOMM
’15, pages 183–197, New York, NY, USA, 2015. ACM.
[51] Mirko Stoffers, Ralf Bettermann, James Gross, and Klaus Wehrle. Enabling
distributed simulation of omnet++ inet models. In Proceedings of the 1st OMNeT++
300
MimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
separate from the MimicNet approach. Certainly, MimicNet could
be used on packet traces rather than the synthetic patterns used in
this work (by characterizing the trace using a distribution). We also
note that it may be possible to relax the symmetry assumption by
training distinct models for different types of clusters, e.g., frontend
clusters, Hadoop clusters, and storage clusters. More baked-in are
the requirements that per-cluster traffic adhere to a consistent
distribution regardless of the size of the simulation; however, given
that clusters maintain the same capacity, it is reasonable to expect
that they maintain similar demand.
Bottleneck locations. The assumption that the most common
bottlenecks exist in the downward-facing direction of a packet’s
path allows MimicNet to elide the modeling of effects like over-
subscription coming out of the hosts and core-level congestion
from inter-Mimic traffic. These could easily be added back in via
similar mechanisms to inter-Mimic modelling, but at additional
performance costs.
Host-internal isolation. MimicNet’s removal of connections from
the host is a large source of improved performance as those imple-
mentations tend to be more complicated and require more state
than even switch queues. Hosts and connections also outnumber,
significantly other components in the simulation. Their removal
from the network is replaced by MimicNet’s constituent models,
but the hosts in Mimics actually have fewer connections. The ef-
fects of CPU contention could likely be modelled accurately. The
effects of out-of-band cooperation between connections, e.g., an
RCP-like mechanism running on each hosts, could also potentially
be modelled with sufficient domain-expertise. Both would add to
the execution time, though the training time could be parallelized.
B SEPARATE INGRESS/EGRESS TUNING
MimicNet, by default, tunes the ingress and egress models together,
but in order to tune/debug the ingress model and the egress model
separately, and also avoid a quadratic increase in the configura-