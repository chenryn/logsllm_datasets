---
author: mazdakh
category: 技术
comments_data: []
count:
  commentnum: 0
  favtimes: 1
  likes: 0
  sharetimes: 0
  viewnum: 7018
date: '2016-10-08 12:27:00'
editorchoice: true
excerpt: 在这篇基础设施系列文章中，我们将探讨 Twitter 的关键设施和组件。同时，我们会分享在系统扩展性、可靠性和效率方面所做的改进，包括我们的历史、遇到的挑战、学到的经验教训以及未来的方向。
fromurl: https://blog.twitter.com/2016/the-infrastructure-behind-twitter-efficiency-and-optimization
id: 7840
islctt: true
largepic: /data/attachment/album/201610/08/122706q4t3mysaeyzsmy8m.jpg
permalink: /article-7840-1.html
pic: /data/attachment/album/201610/08/122706q4t3mysaeyzsmy8m.jpg.thumb.jpg
related: []
reviewer: ''
selector: ''
summary: 在这篇基础设施系列文章中，我们将探讨 Twitter 的关键设施和组件。同时，我们会分享在系统扩展性、可靠性和效率方面所做的改进，包括我们的历史、遇到的挑战、学到的经验教训以及未来的方向。
tags:
- Twitter
- 架构
thumb: false
title: 揭秘 Twitter 背后的基础设施：效率与优化篇
titlepic: true
translator: eriwoon
updated: '2016-10-08 12:27:00'
---

过去，我们曾发布过关于 [Finagle](https://twitter.github.io/finagle/) 和 [Manhattan](https://blog.twitter.com/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale) 等项目的文章，并针对大型活动如天空之城、超级碗、2014 年世界杯和全球新年夜庆祝活动进行了架构优化。在这篇基础设施系列文章中，我们将聚焦于 Twitter 的一些关键设施和组件。此外，我们还将分享在系统的扩展性、可靠性和效率方面所进行的改进，包括我们基础设施的历史、遇到的挑战、学到的经验教训以及未来的发展方向。

![](/data/attachment/album/201610/08/122706q4t3mysaeyzsmy8m.jpg)

> 天空之城：2013 年 8 月 2 日，宫崎骏的《天空之城》（Castle in the Sky）在 NTV 迎来其第 14 次电视重播。剧情发展到高潮时，Twitter 的 TPS（Tweets Per Second）达到了新的高度——143,199 TPS，是平均值的 25 倍，这一记录至今未被打破。-- LCTT 译注

### 数据中心的效率优化

#### 历史

当前，Twitter 的硬件和数据中心规模已经超越了大多数公司。然而，这一成就并非一蹴而就。系统经历了多次软硬件升级和优化，过程中我们也犯过不少错误。有一段时间，系统故障频发，软件问题、硬件问题乃至底层设备问题不断涌现，导致服务中断。随着 Twitter 在客户、服务和媒体上的影响力不断扩大，构建一个高效且可靠的系统成为我们的战略需求。

> Twitter 系统故障界面被称为“失败鲸”（Fail Whale），如下图所示 -- LCTT 译注
>
> ![Fail Whale](/data/attachment/album/201610/08/122721betey39309i7revk.png)

#### 挑战

最初，我们的软件直接安装在服务器上，这意味着软件可靠性依赖于硬件及其环境因素。为了增加容错能力，我们需要综合考虑这些互不关联的物理设备因素及运行在其上的服务。早期，我们在选择数据中心方案时经验不足，对站点选择、运营和设计都缺乏专业性。我们先是从托管主机开始，业务增长后转为租赁机房。早期的问题主要源于设备故障、数据中心设计缺陷、维护问题以及人为操作失误。我们通过持续迭代硬件设计，增强了硬件和数据中心的容错性。

服务中断的原因多种多样，其中硬件故障常见于服务器、机架交换机和核心交换机。例如，硬件团队在设计服务器时认为双路电源对减少供电问题意义不大，因此移除了一块电源。然而，数据中心通常提供两路供电以提高冗余性，防止电网故障影响服务器。最终，我们不得不在机架上增加了一个 ATS 单元（交流切换开关）来接入第二路供电。通过这样的改进，我们提高了网络、供电和机房的冗余性，将故障影响控制在最小范围内。

#### 经验教训与技术升级

我们学到的第一个教训是，需要先建模，明确可能出故障的地方（如建筑的供电和冷却系统、硬件、光纤网络等）与运行在其上的服务之间的依赖关系，从而更好地分析并优化设计，提升容错能力。我们增加了更多的数据中心以提高地理容灾能力，减少自然灾害的影响。这种站点隔离也降低了软件风险，减少了软件部署升级和系统故障的风险。多活的数据中心架构提供了代码灰度发布的能力，减少了新代码上线时的影响。我们还设计了能够在更高温度下正常运行的新硬件，提升了数据中心的能源效率。

#### 未来工作

随着公司的战略发展和运营增长，我们将在不影响最终用户的情况下，持续改进数据中心。下一步的工作重点是在现有能耗和硬件基础上，通过维护和优化进一步提升效率。

### 硬件的效率优化

---