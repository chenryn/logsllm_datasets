+ λ × regularization term. Given the ML algorithm,
the training dataset, and (optionally)
the learnt model
parameters, hyperparameter stealing attacks aim to estimate
the hyperparameter value in the objective function.
Application scenario: One application of our hyperparameter
stealing attacks is that a user can use our attacks to learn a
model via MLaaS with much less computations (thus much
less economical costs), while not sacriﬁcing the model’s testing
performance. Speciﬁcally, the user can sample a small fraction
of the training dataset, learns the model parameters through
MLaaS, steals the hyperparameter using our attacks, and re-
learns the model parameters via MLaaS using the entire train-
ing dataset and the stolen hyperparameter. We will demonstrate
this application scenario in Section V-B via simulations and
Amazon Machine Learning.
IV. HYPERPARAMETER STEALING ATTACKS
We ﬁrst introduce our general attack framework. Second,
we use several regression and classiﬁcation algorithms as
examples to illustrate how we can use our framework to steal
hyperparameters for speciﬁc ML algorithms, and we show
results of more algorithms in Appendix A.
A. Our Attack Framework
Our goal is to steal the hyperparameters in an objective
function. For an ML algorithm that uses such hyperparameters,
the learnt model parameters are often a minimum of the objec-
tive function (see the background knowledge in Section III-A).
Therefore, the gradient of the objective function at the learnt
model parameters should be 0, which encodes the relationships
between the learnt model parameters and the hyperparameters.
We leverage this key observation to steal hyperparameters.
Non-kernel algorithms: We compute the gradient of the
objective function at the model parameters w and set it to
be 0. Then, we have
∂L(w)
∂w = b + λa = 0,
where vectors b and a are deﬁned as follows:
∂R(w)
∂w1
∂R(w)
∂w2
∂L(X,y,w)
∂L(X,y,w)
∂w1
∂w2
⎤
⎥⎥⎥⎥⎦ , a =
⎡
⎢⎢⎢⎢⎣
⎡
⎢⎢⎢⎢⎣
b =
...
∂L(X,y,w)
∂wm
...
∂R(w)
∂wm
(1)
(2)
⎤
⎥⎥⎥⎥⎦ .
in this system,
First, Eqn. 1 is a system of linear equations about the
hyperparameter λ. Second,
the number of
equations is more than the number of unknown variables
(i.e., hyperparameter in our case). Such system is called an
overdetermined system in statistics and mathematics. We adopt
the linear least square method [30], a popular method to ﬁnd
an approximate solution to an overdetermined system, to solve
39
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
the hyperparameter in Eqn. 1. More speciﬁcally, we estimate
the hyperparameter as follows:
ˆλ = −(aT a)
−1aT b.
(3)
(cid:2)n
(cid:2)n
Kernel algorithms: Recall that, for kernel algorithms, the
model parameters are a linear combination of the kernel
mapping of the training instances, i.e., w =
i=1 αiφ(xi).
Therefore, the model parameters can be equivalently repre-
sented by the parameters α = {αi}n
i=1. We replace the variable
w with
i=1 αiφ(xi) in the objective function, compute the
gradient of the objective function with respect to α, and set the
gradient to 0. Then, we will obtain an overdetermined system.
After solving the system with linear least square method,
we again estimate the hyperparameter using Eqn. 3 with the
vectors b and a re-deﬁned as follows:
(4)
⎡
⎢⎢⎢⎢⎣
b =
∂L(φ(X),y,w)
∂L(φ(X),y,w)
∂α1
∂α2
...
∂L(φ(X),y,w)
∂αn
⎤
⎥⎥⎥⎥⎦ , a =
⎡
⎢⎢⎢⎢⎣
⎤
⎥⎥⎥⎥⎦ .
∂R(w)
∂α1
∂R(w)
∂α2
...
∂R(w)
∂αn
Addressing non-differentiability: Using Eqn. 3 still faces
two more challenges: 1) the objective function might not be
differentiable at certain dimensions of the model parameters w
(or α), and 2) the objective function might not be differentiable
at certain training instances for the learnt model parameters.
For instance, objective functions with L1 regularization are
not differentiable at the dimensions where the learnt model
parameters are 0, while the objective functions in SVMs might
not be differentiable for certain training instances. We address
the challenges via constructing the vectors a and b using
the dimensions and training instances at which the objective
function is differentiable. Note that using less dimensions of
the model parameters is equivalent to using less equations in
the overdetermined system shown in Eqn. 1. Once we have at
least one dimension of the model parameters and one training
instance at which the objective function is differentiable, we
can estimate the hyperparameter.
Attack procedure: We summarize our hyperparameter steal-
ing attacks in the following two steps:
• Step I. The attacker computes the vectors a and b for a
given training dataset, a given ML algorithm, and the learnt
model parameters.
• Step II. The attacker estimates the hyperparameter using
Eqn. 3.
More than one hyperparameter: We note that, for concise-
ness, we focus on ML algorithms whose objective functions
have a single hyperparameter in the main text of this paper.
However, our attack framework is applicable and can be easily
extended to ML algorithms that use more than one hyperpa-
rameter. Speciﬁcally, we can still estimate the hyperparameters
using Eqn. 3 with the vector a expanded to be a matrix, where
each column corresponds to the gradient of a regularization
term with respect to the model parameters. We use an example
ML algorithm, i.e., Elastic Net [58], with two hyperparameters
to illustrate our attacks in Appendix B.
Next, we use various popular regression and classiﬁcation
algorithms to illustrate our attacks. In particular, we will
discuss how we can compute the vectors a and b. We will
focus on linear and kernel ML algorithms for simplicity, and
we will show results on neural networks in Appendix C. We
note that the ML algorithms we study are widely deployed by
MLaaS. For instance, logistic regression is deployed by Ama-
zon Machine Learning, Microsoft Azure Machine Learning,
BigML, etc.; SVM is employed by Microsoft Azure Machine
Learning, Google Cloud Platform, and PredictionIO.
B. Attacks to Regression Algorithms
1) Linear Regression Algorithms: We demonstrate our at-
tacks to popular linear regression algorithms including Ridge
Regression (RR) [19] and LASSO [53]. Both algorithms use
least square loss function, and their regularization terms
are L2 and L1, respectively. Due to the limited space, we
show attack details for RR, and the details for LASSO are
shown in Appendix A. The objective function of RR is
L(w) = (cid:3)y − XT w(cid:3)2
2. We compute the gradient
of the objective function with respect to w, and we have
∂w = −2Xy + 2XXT w + 2λw. By setting the gradient
∂L(w)
to be 0, we can estimate λ using Eqn. 3 with a = w and
b = X(XT w − y).
2 + λ(cid:3)w(cid:3)2
2) Kernel Regression Algorithms: We use kernel ridge
regression (KRR) [55] as an example to illustrate our attacks.
Similar to linear RR, KRR uses least square loss function and
L2 regularization. After we represent the model parameters
w using α, the objective function of KRR is L(α) = (cid:3)y −
Kα(cid:3)2
2 + λαT Kα, where matrix K = φ(X)T φ(X), whose
(i, j)th entry is φ(xi)T φ(xj). In machine learning, K is called
gram matrix and is positive deﬁnite. By computing the gradient
of the objective function with respect to α and setting it to
be 0, we have K(λα + Kα − y) = 0. K is invertible as
it is positive deﬁnite. Therefore, we multiply both sides of
the above equation with K−1. Then, we can estimate λ using
Eqn. 3 with a = α and b = Kα−y. Our attacks are applicable
to any kernel function. In our experiments, we will adopt the
widely used Gaussian kernel.
C. Attacks to Classiﬁcation Algorithms
1) Linear Classiﬁcation Algorithms: We demonstrate our
attacks to four popular linear classiﬁcation algorithms: support
vector machine with regular hinge loss function (SVM-RHL),
support vector machine with squared hinge loss function
(SVM-SHL), L1-regularized logistic regression (L1-LR), and
L2-regularized logistic regression (L2-LR). These four algo-
rithms enable us to compare different regularization terms
and different loss functions. For simplicity, we show attack
details for L1-LR, and defer details for other algorithms to
Appendix A. L1-LR enables us to illustrate how we address
the challenge where the objective function is not differentiable
at certain dimensions of the model parameters.
We focus on binary classiﬁcation, since multi-class classi-
ﬁcation is often transformed to multiple binary classiﬁcation
problems via the one-vs-all paradigm. However, our attacks
are also applicable to multi-class classiﬁcation algorithms
such as multi-class support vector machine [11] and multi-
class logistic regression [11] that use hyperparameters in their
objective functions. For binary classiﬁcation, each training
instance has a label yi ∈ {1, 0}.
40
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
The objective function of L1-LR is L(w) = L(X, y, w) +
λ(cid:3)w(cid:3)1, where L(X, y, w)=− (cid:2)n
i=1(yi log hw(xi) + (1 − yi)
log(1 − hw(xi))) is called cross entropy loss function and
hw(x) is deﬁned to be
1+exp (−wT x). The gradient of the
∂w = X(hw(X) − y) + λsign(w),
objective function is ∂L(w)
where hw(X) = [hw(x1); hw(x2);··· ; hw(xn))] and the ith
entry sign(wi) of the vector sign(w) is deﬁned as follows:
1
sign(wi) =
∂|wi|
∂wi
=
⎧⎪⎨
⎪⎩
−1 if wi  0
1
(5)
|wi| is not differentiable when wi = 0, so we deﬁne the
derivative at wi = 0 as 0, which means that we do not use the
model parameters that are 0 to estimate the hyperparameter.
Via setting the gradient to be 0, we can estimate λ using Eqn. 3
with a = sign(w) and b = X(hw(X) − y).
2) Kernel Classiﬁcation Algorithms: We demonstrate our
attacks to the kernel version of the above four linear classi-
ﬁcation algorithms: kernel support vector machine with reg-
ular hinge loss function (KSVM-RHL), kernel support vector
machine with squared hinge loss function (KSVM-SHL), L1-
regularized kernel LR (L1-KLR), and L2-regularized kernel LR
(L2-KLR). We show attack details for KSVM-RHL, and defer
details for the other algorithms in Appendix A. KSVM-RHL
enables us to illustrate how we can address the challenge where
the objective function is non-differentiable for certain training
instances. Again, we focus on binary classiﬁcation.
The objective function of KSVM-RHL is L(α) =
(cid:2)n
i=1 L(φ(xi), yi, α) + λαT Kα, where L(φ(xi), yi, α) =
max(0, 1 − yiαT ki) is called regular hinge loss function. ki
is the ith column of the gram matrix K = φ(X)T φ(X). The
gradient of the loss function with respect to α is:
∂L(φ(xi), yi, α)
∂α
=
−yiki
0
if yiαT ki  1,
(6)
(cid:12)
where L(φ(xi), yi, α) is non-differentiable when ki satisﬁes
yiαT ki = 1. We estimate λ using ki that satisfy yiαT ki  0, so we do not explore negative values for
λ. For each hyperparameter and for each learning algorithm,
we learn the corresponding model parameters using the scikit-
learn package. For kernel algorithms, we use the Gaussian
kernel, where the parameter σ in the kernel is set to be 10.
We implemented our attacks in Python.
Evaluation metric: We evaluate the effectiveness of our
attacks using relative estimation error, which is formally
1https://archive.ics.uci.edu/ml/datasets.html
41
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
−4
−5
−6
−7
−8
−9
−10
−3
0
−2
−4
−6
−8
−10
−3−3
−4
−5
−6
−7
−8
−9
−10
−3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e