distributions, computed between these success and failure
attribute occurrence probability distributions.
(or F a
We can compute the score for large numbers of attribute
groups and over large CDR volumes efﬁciently because the
KL divergence can be reduced to a closed form equation
due to two textbook results. The ﬁrst result
is that Beta
distributions are conjugate priors for Bernoulli models, i.e.,
if a Beta distribution Beta(x, y) is used as an initial estimate
for distribution F a
s ), and the forward probability
P [a appears in a failed call|F a
f
f ] (and similarly for successful
calls) is given by a Bernoulli distribution,
then the new
estimate for F a
f after applying Bayes rule is also a Beta
distribution Beta(x + a, y + b), where a and b are the number
of calls with and without attribute a, respectively. The second
result is that the KL divergence between two Beta distributed
random variables, X ∼ Beta(a, b) and Y ∼ Beta(c, d) is
given by the Equation
KL(Y ||X) = ln
B(a, b)
B(c, d)
− (a − c)ψ(c) − (b − d)ψ(d)
(1)
+(a − c + b − d)ψ(c + d)
where B is the Beta function and ψ is the digamma function.
Therefore, if one starts with the initial assumption that the
failure and successful call attribute occurrence probabilities
pf and ps are uniformly distributed (which is a special case
of the Beta distribution), then setting a/b = 1+#successful
calls with/without attribute a, and c/d = 1+#failed calls
with/without attribute a in Equation 1 yields the desired score
in Equation 1. A similar observation is used to compute KL
divergences between two Bernoulli models in [14].
Figure 3 shows how the scoring works in terms of the
density functions for the success and failure attribute oc-
currence probability distributions. Intuitively, it scores higher
those attribute groups that are more likely to occur in failed
calls than in successful calls, but it does so while taking
into account the volume of data observed. This allows us to
increase conﬁdence as we observe more calls. For example,
the score is higher after observing an attribute in 50 out of 100
failed calls as compared to observing it in 1 out of 2 failed
calls, even though both scenarios have the same underlying
probability pf of 0.5.
C. Attribute Group Generation
Chronics can arise due to complex triggers involving a com-
bination of factors such as conﬂicting software versions on
different network elements. These complex conditions are
represented as conjunctions of groups of attributes, e.g., “Cus-
tomer1 and LocationA and CodecB”. Draco identiﬁes these
groups using a search tree as shown in Figure 4. The root node
of the tree represents the null set, and each branch represents
a single attribute. Each non-root node of the tree represents a
unique attribute group as speciﬁed by the path from the root
to that node, and the weight of the node is the anomaly score
for node’s attribute group. Starting with the direct children of
Fig. 3. Draco ﬁrst extracts attributes from the labeled end-to-end traces and
represents them as truth table. Next, Draco computes an anomaly score as the
distance between the successful and failed distribution for each attribute.
phone numbers match in all available digits and timestamps
are within a small window of each other. Besides these keys,
the remainder of each log entry is not required to have any
special semantic meaning. We can treat it simply as a bag of
words. For our VoIP dataset, the end result is a list of “master
CDRs”, one for each phone call or call attempt, and each
labeled as a success or failure as shown in Figure 3.
Each master CDR consists of a number of attributes includ-
ing the caller and callee phone numbers, call time and duration,
network element names and IP addresses used to process the
call, any defect and success codes generated by the element,
the trunk lines used, and other ﬁelds present in the CDRs.
Domain knowledge can be used to choose which attributes to
include from the original raw logs.
B. Scalable Anomaly Score Computation
Given the set of master records, we then rank attributes using
a scoring function that quantiﬁes the ability of the group to
discriminate between successful and failed calls. To do so, we
use an iterative Bayesian approach to learn a simple Bernoulli
(i.e., “coin toss”) model of successes and failures. The idea is
to model an attribute a as occurring in a call with a ﬁxed, but
unknown probability pa. This attribute occurrence probability
is pa
s for successful calls. The model
estimates these unknown probabilities using the master CDRs.
However, rather than learning a single value, we can estimate
the entire probability distribution of these unknown attribute
f ≤ x], and
occurrence probabilities, i.e., F a
s ≤ x]. We start with an initial estimate for
F a
s (x) = P [pa
f and F a
s , and Bayes rule is used to update this estimate
F a
as each new call in the dataset is processed, depending on
whether it
is a successful and failed call, and whether it
contains the attribute a or not. Once these distributions are
f for failed calls, and pa
f (x) = P [pa
00.51Call1: 2011-9-1 06:49:14,SUCCESS, Server1,Server2,Vendor1Call2: 2011-9-1 06:49:30,FAIL,Server1,Customer1,Vendor1Call3: 2011-9-1 06:50:00,FAIL,Server2,Customer1,Vendor1Represent call attributes as truth tableServer1Server2Customer1Vendor1OutcomeCall1         1              1                  0                    1           SUCCESSCall2         1              0                  1                    1           FAIL    Call3         0              1                  1                    1           FAILModel success and failure distribution of each attributeSuccessfulcallsFailed callsProbability(Customer1=TRUE)Anomaly score =Distance betweendistributionsLog snippet from end-to-end tracesFig. 4. Draco uses an iterative Bayesian approach to rank combinations of attributes most correlated with the problem.
the root (representing a single attribute each), Draco expands
the tree to a depth of d to consider all groups containing
up to d attributes. Expanding along a branch of the tree
involves a ﬁltering operation that retains only those successful
and failure events in which the attribute represented by that
branch was present. The ﬁltering is required to get the success
and failure counts needed for the anomaly score computation.
These ﬁltering operations dominate the algorithm’s running
time and dictate the data structures used in Draco’s design as
described in Section IV. The node with the highest weight is
picked as the dominant problem signature in that iteration.
For this process to be practical, there are two additional
complications that must be handled. The ﬁrst is to ﬁnd any
attributes that are synonyms of each other. For example,
attributes such as a particular customer’s IP address and
name, or a customer’s IP address and a dedicated IPBE
server assigned to that customer, may appear together in all
calls. Such overlapping attributes are indistinguishable from a
statistical point of view but may be meaningful to an operator
from a semantic standpoint (e.g., an operator may know how
to investigate an IPBE server but not know how to investigate
the customer’s IP). Therefore, at each node of the tree, we
identify all its equivalent attributes and represent the entire set
by a single canonical attribute when expanding the tree. The
threshold used to mark two, or more, attributes as synonyms is
referred to as the overlap probability and is a user-conﬁgurable
parameter that is typically set to a high value such as 0.99.
However, when presenting the problem signatures to the oper-
ator, we show all the synonyms associated with the attributes
identiﬁed in the signature.
The second complication is one of scalability. Because tens
of thousands of attributes can be present in the dataset, a brute-
force approach that expands the entire tree up to depth d is
infeasible. To explore attribute groups optimally, Draco uses
a branch-and-bound algorithm [13] to dynamically determine
the maximum breadth of the tree to explore. Speciﬁcally, for
each unexplored node of the tree n, Draco computes an upper
bound for the maximum anomaly score that can be achieved
by any child node n. If this upper bound is lower than the
maximum anomaly score seen so far, then exploration of n is
guaranteed to be fruitless, and it is discarded without further
exploration. The upper bound of the anomaly score for a
subtree, e.g., customer1 in Figure 4, can be shown to be
attained by assuming that there is a branch of that subtree
that explains all the failed calls in the subtree, and has zero
successes as computed by Equation 2.
KLb(Y ||X) = ln
B(1, a + b − 1)
− (1 − c)ψ(c) −
B(c, d)
(a + b − 1 − d)ψ(d) +
(a + b − c − d)ψ(c + d)
(2)
For example, if the attribute customer1 was associated with
100 failed calls and 10000 successful calls, then the maximum
possible anomaly score for the subtree anchored at customer1
would be a branch with 100 failed calls and zero successes.
We iteratively apply this algorithm in a greedy fashion to
identify multiple concurrent problems by removing all calls
(both success and failures) that match this identiﬁed problem
signature from the dataset, and repeat the process. Doing so
removes the impact of the ﬁrst diagnosed problem and allows
us to ask what explains the remaining failures. In this manner,
we can identify separate independent failure causes (see Steps
2 and 3 in Figure 4).
The average complexity of our algorithm is M ∗ N ∗ Dr,
where M is the number of attributes, N is the number of calls,
D is the average depth of the tree, and r is the average degree
of nodes in the tree. The magnitudes of D and r are determined
dynamically by the Draco’s branch-and-bound algorithm.
D. Real-Valued Resource Counters
The Bayesian approach presented in Section III-B is scal-
able enough to be directly applied to the large numbers of dis-
crete attributes present in our data sets. However, it is difﬁcult
to construct such numerically cheap comparison techniques
to compare between success and failure distributions of real-
valued data. To overcome this limitation, we analyze only a
subset of the real-valued data that is linked to attributes that are
implicated in signatures produced by the Bayesian analysis.
Speciﬁcally, the real-valued data in the VoIP dataset includes
performance logs of any network elements within the service
Server1Customer1Vendor1Server96003509802102. Find attribute combinations.Root-cause = highest scoring pathIPAddr4Server2OSversion4Vendor34501466505103. Filter out calls matching Problem1      and repeat.Problem2     Server2     OSversion4Problem1     Customer1     Vendor1Server1Customer16003501. Compute scores for attributes.IPAddr198Anomaly scoresDiscard attributes if max.subtree score < neighbor’s score. Max. subtree score1500850150150 < 350Fig. 5. Draco’s ﬂexible architecture supports multiple data sources, and the
diagnosis engines can run in either real-time or ofﬂine mode.
Fig. 6. Draco achieves high performance by maintaining in-memory indices
of attribute and event data.
provider’s network. These performance logs include periodic
measurements (at 5–15 minute intervals) of CPU and memory
utilization, network trafﬁc, disk I/O, and other OS-level met-
rics. For each problem signature identiﬁed in Section III-C,
Draco considers only those performance measurements as-
sociated with network elements present in the signature. We
identify the resource-usage metrics that are highly correlated
with the problem by annotating each call that matches the
problem signature with the resource-usage metrics gathered
during the same time interval, as illustrated in the log snippet
below.
# Resource−u sa ge m e t r i c s
# Timestamp , CallNo , S t a t u s , Memory (%) , CPU(%)
20100901064914 ,1 ,SUCCESS, 5 4 , 6
20100901065530 ,2 , FAIL , 8 2 , 4
20100901070030 ,3 , FAIL , 7 5 , 2 0
s e r v e r 2
f o r
Next, we use the Wilcoxon rank-sum test [16] to determine
whether the distribution of each metric in failed calls differs
signiﬁcantly from the distribution of each metric in successful
calls. The Wilcoxon rank-sum test does not assume that the
data is drawn from any particular distribution, and assesses
whether one of two samples of independent observations tends
to have larger values than the other. Comparing the distribution
of metrics between successful and failed calls within the same
time interval makes Draco more robust to seasonal variations
in load (e.g, night-time vs. day-time).
IV. ARCHITECTURE AND DESIGN
We have implemented a prototype of Draco, written in C,
which is comprised of data collectors that process consolidated
end-to-end call traces to extract attributes of interest, and
a diagnosis engine that outputs a ranked list of problems
identiﬁed (see Figure 5). For the past year, this prototype has
been in active daily use by the chronics team at the large
ISP to analyze the production VoIP platform. The prototype
is ﬂexible since it can be easily extended to incorporate
additional sources of information, such as software versions
and Quality of Service (QoS) data. In addition, the prototype
is scalable and capable of handling tens of millions of calls
in real-time even when running on a single server.
The data collectors extract attributes from server logs, and
archive the processed logs. Each data collector supports one
or more data formats speciﬁed using conﬁguration ﬁles, which
increases the ﬂexibility of our prototype. The data collectors
also send data to the diagnosis engine, which implements
the algorithms described above in Sections III-B and III-C.
The diagnosis engine can receive data from concurrent input
sources (i.e., multiple collectors) to reduce the amount of time
needed to load data. The diagnosis engine can also be run in an
ofﬂine mode by reading processed logs from the data archive.
The diagnosis engine considers each end-to-end call record
as an event. The engine collects and manages events over a
user-controlled time window of length T seconds (the chronics
analysis team typically uses a window size of a whole day).
Timestamp information in the event data is used to determine
the bounds of the window; as new data is received, the window
progresses forward and old events are aged off.
Performance was a primary concern while architecting the
diagnosis engine as it is necessary to manage thousands of
attributes from the VoIP system in real-time. The ﬁltering
operations involved in the exploration of the search tree and in
ﬁltering out data that can be explained by a newly discovered
problem signature, as described in Section III-C, are the most
expensive operations of each analysis. This is because each
ﬁltering operation must operate on the entire dataset consisting
of both successful and failed events, which, despite reductions
due to sampling, can still be very large. To construct appro-
priate data structures for this process, we use the observation
that if each event is treated as a “document” that contains
words corresponding to each attribute, then computation of
the anomaly score for an attribute group involves “searching”
both the success and failure document sets for that group of
attribute keywords, and counting the matches. Therefore, as
shown in Figure 6, Draco’s core data structures are constructed
similarly to search engines—using an inverted hash table to
Data Sources(s)"o(cid:31)−line"real−timeTCP/IPDataCollectorsDiagnosisEngineCommandInterfaceCommandInterfaceDiagnosisEngineDataArchiveAttribute IndexServer1Customer1AnchorTime SliceExpiryEvent-list pointerFail countSuccess countAnchorEvent ListCall1:  Success     Server1     Server2     Vendor1Call2: FAIL      Server1      Customer1      Vendor1operators can restrict the analysis to calls for a speciﬁed
VoIP service on a given date.
2) Next, operators are directed to an interactive web-
interface interface that ranks the top-20 problems di-
agnosed by Draco that match their ﬁlter, sorted by
decreasing severity. Operators can gain more insight on
the nature of the problem by viewing samples of calls
affected via a drop-down option. The call samples dis-
play additional information from the call detail records,
such as telephone numbers and call durations, that might
not be captured by Draco’s problem signature.
3) A plot showing the frequency of the problem is dis-
played on the right, providing insight on the duration
and severity of the problem.
Fig. 7. Draco’s dashboard allows operators to: (1) specify a search criteria
such as problems on given date; (2) view a ranked list of chronics diagnosed;
and (3), identify recurrent problems using plots of affected calls.
index attributes.
The inverted index maps each attribute back to a linked list
of events that contains the attribute. These mappings allow
linear time computation of set intersections so that success and
failure counts can be quickly constructed for any conjunction
of attributes and negated attributes (to support exclusion of
events that match previously discovered signatures). For each
attribute, a series of success and failure counts are maintained
based on the time slices. Managing the counts by time allows
them to be adjusted as the time window rolls forward without
the need to recount across all unexpired events.
A. Success Event Sampling
Due to the nature of chronics, the datasets Draco processes
usually have a disproportionately larger number of successful