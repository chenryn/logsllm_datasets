1024
Streamlined
Protection
Domain
Crossing
255.00%
248.00%
229.00%
191.00%
156.00%
Batched
Submit
1128.00%
500.00%
270.00%
190.00%
156.00%
DMA
Address
Pre-
Translation
375.00%
369.00%
270.00%
191.00%
156.00%
The throughput improvement made by each of the three major optimizations
in the system resource virtualization layer of the SIDE prototype under
Table V
different packet size
submit is the most effective because it reduces more kernel
service function calls per packet than the other two opti-
mizations. Part of the effectiveness of streamlined protection
domain crossing comes from removing the need to ﬂush the
TLB in each crossing. In addition, the relative effectiveness
of each optimization decreases with the packet size, because
the packet payload size plays an increasingly more important
role in the measured throughput as the packet size grows.
D. Recovery Time
To measure the time between when an isolated network
device driver fails and when a new instance of the driver
starts up and begins service, we artiﬁcially injected errors
into a network device driver that has been running for a
while by randomly ﬂipping a pointer’s value, and measured
the time SIDE takes to clean up the side effects of the
failed driver and to start a new driver instance. Such injected
errors eventually lead to such failures as protection page
faults, invalid op code exceptions, unknown kernel service
function calls, illegitimate input arguments to kernel service
function calls, etc.; the Linux kernel catches these failures
and initiates the following recovery procedure:
1) Marks the driver state as faulty, stops triggering any
timer, NAPI polling and scheduled work added by the
faulty driver, and deletes timers and scheduled work
items from the kernel data structures,
2) Stops transmitting more packets through the faulty
driver by calling netif stop queue(), and stops deliv-
ering new hardware interrupts to the faulty driver by
calling free irq(),
3) Waits for processes to detach from the faulty driver,
4) Frees all system resources held by the faulty driver,
including heap memory, DMA channels, the kernel
preempt count, memory-mapped IO address regions,
etc.,
5) Unregisters the faulty driver from the kernel and
disables the associated device on the PCIe bus, and
6) Reloads the device driver module, and restarts it.
Step
Mark as faulty
Stop transmit/receive
Wait for detach
Free system resources
Unregister
Reload and restart
Time Taken (µsec)
58
814
1,931
2,266
424,413
5,388,067
The detailed breakdown of the end-to-end recovery delay of a driver failure
Table VI
We measured the UDP throughputs of SIDE base with
streamlined protection domain crossing, SIDE base with
batched submit, and SIDE base with DMA address pre-
translation, computed the ratio of these measured through-
puts over the throughput of SIDE base, and showed the
results in Table V. Among the three optimizations, batched
As shown in Table VI, the ﬁrst four steps are cleaning up
the side effects of the failed driver and consume less than 2
msec, the ﬁfth step is a standard Linux function to remove a
driver and takes 0.5 sec, and the last step is to create a new
driver instance and takes 5.39 sec. The end-to-end recovery
10
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:39:49 UTC from IEEE Xplore.  Restrictions apply. 
time for a failed network device driver running in the SIDE
prototype is about 5.8 seconds, where only less than 1% of
it is spent on clean-up of the failed driver. It may be possible
to apply the active back-up driver technique [16] to shorten
the last step.
VI. CONCLUSION
Buggy device driver has been a major cause of outage for
modern operating systems or hypervisors. Although myriad
attempts have been made to isolate device drivers from
the kernel, none of them achieves all three requirements
that characterize an ideal isolated device driver execution
architecture: (1) strong isolation, (2) low performance over-
head, and (3) ability to run legacy device drivers without
modiﬁcations. We believe the SIDE architecture described
in this paper comes closer than any other proposals in the
literature to satisfy these three requirements. The key idea of
the SIDE architecture is it places a device driver in the kernel
address space but at a user privilege level. Because a SIDE
device driver is in the kernel address space, it could handle
hardware interrupts regardless of the execution contexts at
the time of interrupt. Because a SIDE device driver is at the
user privilege level, the driver could share the same page
table as the kernel while being isolated from the kernel.
More speciﬁcally, by leveraging modern virtual memory
hardware, SIDE is able to detect illegal kernel data/code
accesses, unauthorized privilege instruction execution, and
improper arguments associated with kernel service function
calls. With the help of a system resource virtualization
layer, the current SIDE prototype minimizes the number
of protection domain crossings and thus the performance
penalty associated with isolated driver execution. By virtue
of fashioning an execution model similar to that used by
in-kernel device drivers, SIDE is able to run commercial-
grade device drivers without requiring any modiﬁcations to
them. In summary, this work makes the following research
contributions:
• An efﬁcient and safe device driver execution model
that isolates the kernel from device drivers using virtual
memory hardware,
• A system resource virtualization layer that cuts down
the number of kernel service function calls from a
driver by substituting them with local wrapper function
calls and a kernel service function call processing
layer that performs argument checks and resource usage
tracking, and
• A fully operational SIDE prototype that runs real-world
unmodiﬁed network device drivers with a through-
put/latency penalty of less than 1% and a CPU penalty
of 11-48%, and successfully recovers from crashes of
failed drivers without rebooting the kernel.
REFERENCES
[1] Archana Ganapathi, Viji Ganapathi, and David Patterson. Win-
dows XP kernel crash analysis. In Proceedings of the 20th
USENIX Large Installation System Administration Confer-
ence, pages 101111, Washington, DC, USA, 2006.
[2] Gillen, A., Kusnetzky, D., and McLaron, S. The role of Linux
in reducing the cost of enterprise computing. IDC white paper.
2002.
[3] Patterson, D., Brown, A., Broadwell, P., Candea, G., Chen, M.,
Cutler, J., Enriquez, P., Fox, A., Kycyman, E., Merzbacher,
M., Oppenheimer, D., Sastry, N., Tetzlaff, W., Traupman,
J., and Treuhaft, N. Recovery-Oriented Computing (ROC):
Motivation, Deﬁnition, Techniques, and Case Studies. Techni-
cal Report CSD-02-1175, Computer Science Division, EECS,
University of California at Berkeley. March 2002.
[4] Andy Chou, Jun-Feng Yang, Benjamin Chelf, Seth Hallem,
and Dawson Engler. An empirical study of operating systems
errors. In Proceedings of the 18th ACM Symposium on
Operating Systems Principles, pages 7388, Lake Louise, Alta,
Canada, October 2001.
[5] Short, R. 2003. Vice President of Windows Core Technology,
Microsoft Corp. Private communication.
[6] Manuel Fhndrich, Mark Aiken, Chris Hawblitzel, Orion Hod-
son, Galen C. Hunt, James R. Larus, and Steven Levi. Lan-
guage support for fast and reliable message-based communi-
cation in Singularity OS. In Proceedings of the 1st EuroSys
Conference, pages 177190, Leuven, Belgium, April 2006.
[7] Michael M. Swift, Brian N. Bershad, and Henry M. Levy.
Improving the reliability of commodity operating systems.
In Proceedings of the 19th ACM Symposium on Operating
Systems Principles, Bolton Landing (Lake George), New York,
USA, October 2003.
[8] Dawson R. Engler, M. FransKaashoek, and James O’Toole, Jr.
Exokernel: An operating system architecture for application-
level resource management. In Proceedings of the 15th ACM
Symposium on Operating Systems Principles, pages 251266,
Copper Mountain, CO, USA, December 1995.
[9] Ben Leslie, Peter Chubb, Nicholas Fitzroy-Dale, Stefan Gtz,
Charles Gray, Luke Macpherson, Daniel Potts, Yueting (Rita)
Shen, Kevin Elphinstone, and GernotHeiser. User-level device
drivers: Achieved performance. Journal of Computer Science
and Technology, 20(5):654664, September 2005.
[10] VinodGanapathy, Matthew J. Renzelmann, AriniBalakrish-
nan, Michael M. Swift, and SomeshJha. The design and
implementation of microdrivers. In Proceedings of the 13th
International Conference on Architectural Support for Pro-
gramming Languages and Operating Systems, pages 168178,
Seattle, WA, USA, March 2008.
[11] Matthew J. Renzelmann and Michael M. Swift. Decaf: Mov-
ing device drivers to a modern language. In Proceedings of
the 2009 USENIX Annual Technical Conference, San Diego,
CA, USA, 2009.
[12] Michael M. Swift, MuthukaruppanAnnamalai, Brian N. Ber-
shad, and Henry M. Levy. Recovering device drivers. In
Proceedings of the 6th USENIX Symposium on Operating
Systems Design and Implementation, San Francisco, CA, USA,
December 2004.
[13] Joshua LeVasseur , Volkmar Uhlig , Jan Stoess , Stefan
Gtz, Unmodiﬁed device driver reuse and improved system
dependability via virtual machines. In Proceedings of the 6th
conference on Symposium on Opearting Systems Design &
Implementation, San Francisco, CA, December 2004.
[14] Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim
Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, and Andrew
11
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:39:49 UTC from IEEE Xplore.  Restrictions apply. 
Warﬁeld. Xen and the art of virtualization. In Proceedings of
the 19th ACM Symposium on Operating Systems Principles,
pages 164177, Bolton Landing, NY, USA, October 2003.
[15] A. Menon, S. Schubert, W. Zwaenepoel. TwinDrivers: semi-
automatic derivation of
fast and safe hypervisor network
drivers from guest OS drivers. In Proceedings of the 14th Inter-
national Conference on Architectural Support for Programming
Languages and Operating Systems, 2009.
[16] HeeseungJo ,Hwanju Kim , Jae-Wan Jang , Joonwon Lee
, SeungryoulMaeng. Transparent Fault Tolerance of Device
Drivers for Virtual Machines. IEEE Transactions on Comput-
ers, v.59 n.11, p.1466-1479, November 2010.
[17] Robert Wahbe, Steven Lucco, Thomas E. Anderson, and
Susan L. Graham. Efﬁcient software-based fault
isolation.
In Proceedings of the 14th ACM Symposium on Operating
Systems Principles, p 203216, Asheville, NC, Dec 1993.
[18] lfarErlingsson, Martn Abadi, Michael Vrable, MihaiBudiu,
and George C. Necula. XFI: software guards for system ad-
dress spaces. In Proceedings of the 7th USENIX Symposium
on Operating Systems Design and Implementation, pages 7588,
Seattle, Washington, November 2006.
[19] Margo I. Seltzer, Yasuhiro Endo, Christopher Small, and
Keith A. Smith. Dealing with disaster: Surviving misbehaved
kernel extensions. In Proceedings of the 2nd USENIX Sympo-
sium on Operating Systems Design and Implementation, pages
213228, November 1996.
[20] Christopher Small and Margo I. Seltzer. MiSFIT: Construct-
ing safe extensible systems. IEEE Concurrency, 6(3):3441,
July/September 1998.
[21] Leonid Ryzhyk, Peter Chubb, IhorKuz, and GernotHeiser.
Dingo: Taming device drivers. In Proceedings of the 4th
EuroSys Conference, Nuremberg, Germany, April 2009.
[22] J. N. Herder, H. Bos, B. Gras, P. Homburg, and A. S.
Tanenbaum. Fault isolation for device drivers. In Proceedings
of International Conference on Dependable Systems and Net-
works, 2009.
[23] Jorrit N. Herder, Herbert Bos, Ben Gras, Philip Homburg,
and Andrew S. Tanenbaum. MINIX 3: A highly reliable, self-
repairing operating system. ACM Operating Systems Review,
40(3):8089, July 2006.
[24] Feng Zhou, Jeremy Condit, Zachary Anderson, IlyaBagrak,
Rob Ennals, Matthew Harren, George Necula, and Eric Brewer.
SafeDrive: Safe and recoverable extensions using language-
based techniques. In Proceedings of the 7th USENIX Sym-
posium on Operating Systems Design and Implementation, p
4560, Seattle, WA, USA, November 2006.
[25] Leonid Ryzhyk, Peter Chubb, IhorKuz, Etienne Le Sueur,
GernotHeiser. Automatic device driver synthesis with termite.
Proceedings of the ACM SIGOPS 22nd Symposium on Oper-
ating Systems Principles, Big Sky, Montana, October 2009.
[26] Thomas Ball , Ella Bounimova, Byron Cook, Vladimir Levin,
Jakob Lichtenberg, Con McGarvey, BohusOndrusek, Sriram
K. Rajamani, Abdullah Ustuner. Thorough static analysis of
device drivers. In Proceedings of the ACM SIGOPS/EuroSys
European Conference on Computer Systems, Leuven, Belgium,
April 2006.
[27] George C. Necula, Jeremy Condit, Matthew Harren, Scott
McPeak, and Westley Weimer. CCured: type-safe retroﬁtting
of legacy software. ACM Transactions on Programming Lan-
guages and Systems, 27(3):477526, 2005.
[28] Leonid Ryzhyk, John Keys, BalachandraMirla ,ArunRaghu-
nath, Mona Vij, GernotHeiser. Improved device driver reliabil-
ity through veriﬁcation reuse. In Proceeding HotDep’10 Pro-
ceedings of the Sixth International Conference on Hot Topics
in System Dependability, USENIX Association Berkeley, CA,
2002.
[29] Jorrit N. Herder, Herbert Bos, Ben Gras, Philip Homburg,
and Andrew S. Tanenbaum. Construction of a Highly Depend-
able Operating System. In Proceedings of the Sixth European
Dependable Computing Conference, EDCC 2006, Coimbra,
Portugal, 18-20 October 2006.
[30] Jorrit N. Herder, Herbert Bos, Ben Gras, Philip Homburg,
Andrew S. Tanenbaum. Reorganizing UNIX for Reliability.
In Proceedings of the 11-th Asia-Paciﬁc Computer Systems
Architecture Conference, page 81-94, September 2006.
[31] H. Hartig, M. Hohmuth, J. Liedtke, S. Sch
onberg, and
J. Wolter. The performance of microkernel-based systems. In
Proc. of the 16th ACM Symposium on Operating System
Principles, SaintMalo, France, Oct. 1997.
[32] AravindMenon, Alan L. Cox, and Willy Zwaenepoel. Op-
timizing Network Virtualization in Xen. In USENIXAnnual
Technical Conference, Boston, MA, June 2006.
[33] AravindMenon, Jose Renato Santos, Yoshio Turner,G. (John)
Janakiraman, and Willy Zwaenepoel. Diagnosing Performance
Overheads in the Xen Virtual MachineEnvironment. In First
ACM/USENIX Conference on Virtual Execution Environments
(VEE’05), Chicago, USA, June2005.
[34] Jose Renato Santos, Yoshio Turner, G. (John) Janakira-
man,and Ian Pratt. Bridging the gap between hardware and-
software techniques for i/o virtualization. In USENIX Annu-
alTechnical Conference, 2008.
[35] Lea Wittie. Laddie: Language for Automated Device Drivers.
Bucknell TR #08-2 Tech Report 2008
[36] Shakeel Butt, Vinod Ganapathy, Michael M. Swift, Chih-
Cheng Chang. Protecting Commodity Operating System Ker-
nels from Vulnerable Device Drivers. Proceedings of the 25th
Annual Computer Security Applications Conference (ACSAC
2009); pages 301–310; Honolulu, Hawaii; December 7-11,
2009.
[37] Asim Kadav, Matthew J. RenzeLmann, Micheal M. Swift.
Tolerating Hardware Device Failures in Software. SOSP ’09
Proceedings of the ACM SIGOPS 22nd symposium on Oper-
ating systems principles, Pages 59-72
[38] Volodymyr Kuznetsov, Vitaly Chipounov, George Candea.
Testing Closed-Source Binary Device Drivers with DDT.
USENIX Annual Technical Conference (USENIX ATC 2010)
June 22-25, 2010, Boston, MA, USA
[39] Ryzhyk, L, Zhu, Y & Heiser, GA, 2010, The case for
active device drivers, in APSys 2010: First ACM Asia-Paciﬁc
Workshop on Systems, ACM, New York, NY, USA, presented
at APSys2010: 1st ACM Asia-Paciﬁc Workshop on Systems,
New Delhi, India, 30 August 2010
[40] Silas Boyd-Wickizer, Nickolai Zeldovich. Tolerating Mali-
cious Device Drivers in Linux. USENIXATC’10 Proceedings
of the 2010 USENIX conference on USENIX annual technical
conference
[41] Yandong Mao, Haogang Chen, Dong Zhou, Xi Wang, Nick-
olai Zeldovich, M. Frans Kaashoek. Software fault isolation
with API integrity and multi-principal modules. SOSP ’11 Pro-
ceedings of the Twenty-Third ACM Symposium on Operating
Systems Principles Pages 115-128, ACM New York, NY, USA
2011
[42] M. Castro, M. Costa, J. P. Martin, M. Peinado, P. Akritidis,
A. Donnelly, P. Barham, and R. Black. Fast byte-granularity
software fault isolation. In Proceedings of the 22nd ACM
Symposium on Operating Systems Principles, Big Sky, MT,
October 2009.
12
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:39:49 UTC from IEEE Xplore.  Restrictions apply.