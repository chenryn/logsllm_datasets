### Throughput Improvement by System Resource Virtualization Layer Optimizations

The throughput improvement achieved through the three major optimizations in the system resource virtualization layer of the SIDE prototype is summarized in Table V. The optimizations include:

- **Streamlined Protection Domain Crossing**
- **Batched Submit**
- **DMA Address Pre-Translation**

#### Throughput Improvement Results

| Packet Size (bytes) | Streamlined Protection Domain Crossing | Batched Submit | DMA Address Pre-Translation |
|---------------------|----------------------------------------|----------------|------------------------------|
| 1024                | 255.00%                                | 1128.00%       | 375.00%                      |
| 512                 | 248.00%                                | 500.00%        | 369.00%                      |
| 256                 | 229.00%                                | 270.00%        | 270.00%                      |
| 128                 | 191.00%                                | 190.00%        | 191.00%                      |
| 64                  | 156.00%                                | 156.00%        | 156.00%                      |

**Observations:**
- **Batched Submit** is the most effective optimization, as it significantly reduces the number of kernel service function calls per packet compared to the other two optimizations.
- **Streamlined Protection Domain Crossing** benefits from eliminating the need to flush the TLB during each crossing.
- The relative effectiveness of each optimization decreases with increasing packet size. This is because the payload size becomes more significant in determining the measured throughput as the packet size grows.

### Recovery Time for Network Device Driver Failures

To measure the recovery time between a network device driver failure and the startup of a new driver instance, we artificially injected errors into a running network device driver by randomly flipping a pointer's value. The following steps were taken to recover from the failure:

1. **Mark the Driver as Faulty:**
   - Stop timers, NAPI polling, and scheduled work added by the faulty driver.
   - Delete timers and scheduled work items from the kernel data structures.

2. **Stop Transmit/Receive Operations:**
   - Call `netif_stop_queue()` to stop transmitting packets through the faulty driver.
   - Call `free_irq()` to stop delivering new hardware interrupts to the faulty driver.

3. **Wait for Detach:**
   - Wait for processes to detach from the faulty driver.

4. **Free System Resources:**
   - Free all system resources held by the faulty driver, including heap memory, DMA channels, kernel preempt count, and memory-mapped IO address regions.

5. **Unregister the Faulty Driver:**
   - Unregister the faulty driver from the kernel and disable the associated device on the PCIe bus.

6. **Reload and Restart:**
   - Reload the device driver module and restart it.

**Time Breakdown (in µs):**

| Step                    | Time Taken (µs) |
|-------------------------|-----------------|
| Mark as Faulty          | 58              |
| Stop Transmit/Receive   | 814             |
| Wait for Detach         | 1,931           |
| Free System Resources   | 2,266           |
| Unregister              | 424,413         |
| Reload and Restart      | 5,388,067       |

**Total Recovery Time:**
- The total end-to-end recovery time for a failed network device driver in the SIDE prototype is approximately 5.8 seconds.
- Only about 1% of this time is spent on cleaning up the side effects of the failed driver.
- It may be possible to reduce the last step (reloading and restarting) by applying the active backup driver technique [16].

### Conclusion

Buggy device drivers are a significant cause of outages in modern operating systems or hypervisors. While many attempts have been made to isolate device drivers from the kernel, none have fully met the three requirements of an ideal isolated device driver execution architecture: strong isolation, low performance overhead, and the ability to run legacy device drivers without modifications.

The SIDE architecture, as described in this paper, comes closer than any other proposal to satisfying these requirements. The key idea is to place a device driver in the kernel address space but at a user privilege level. This allows the driver to handle hardware interrupts while being isolated from the kernel using virtual memory hardware.

**Key Contributions:**
- An efficient and safe device driver execution model that isolates the kernel from device drivers using virtual memory hardware.
- A system resource virtualization layer that minimizes the number of protection domain crossings and kernel service function calls.
- A fully operational SIDE prototype that runs real-world unmodified network device drivers with a throughput/latency penalty of less than 1% and a CPU penalty of 11-48%, and successfully recovers from driver crashes without rebooting the kernel.

### References

[References listed here as provided in the original text]

This optimized version provides a clearer and more structured presentation of the information, making it easier to understand the context and significance of the optimizations and recovery procedures.