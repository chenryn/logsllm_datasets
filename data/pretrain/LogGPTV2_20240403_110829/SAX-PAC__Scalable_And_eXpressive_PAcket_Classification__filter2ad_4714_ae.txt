and Quoc V. Le. 2018. AutoAugment: Learning Augmentation Policies
from Data. CoRR (2018).
[6] Mo Dong, Tong Meng, Doron Zarchy, Engin Arslan, Yossi Gilad,
Brighten Godfrey, and Michael Schapira. 2018. PCC Vivace: Online-
Learning Congestion Control. In USENIX NSDI.
[7] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,
Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley,
Iain Dunning, et al. 2018.
IMPALA: Scalable distributed Deep-RL
with importance weighted actor-learner architectures. arXiv preprint
arXiv:1802.01561 (2018).
[8] Kousha Etessami and Mihalis Yannakakis. 2005. Recursive Markov
decision processes and recursive stochastic games. In International
Colloquium on Automata, Languages, and Programming. Springer, 891–
903.
[9] Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen
He, Zachary Kaden, Vivek Narayanan, and Xiaohui Ye. 2018. Horizon:
Facebook’s Open Source Applied Reinforcement Learning Platform.
arXiv preprint arXiv:1811.00260 (2018).
[10] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013.
Speech recognition with deep recurrent neural networks. In ICASSP.
[11] Arthur Guez, Théophane Weber, Ioannis Antonoglou, Karen Simonyan,
Oriol Vinyals, Daan Wierstra, Rémi Munos, and David Silver. 2018.
Learning to search with MCTSnets. arXiv preprint arXiv:1802.04697
(2018).
[12] Pankaj Gupta and Nick McKeown. 1999. Packet classification on
multiple fields. SIGCOMM CCR (1999).
[13] Pankaj Gupta and Nick McKeown. 1999. Packet classification using
hierarchical intelligent cuttings. In Hot Interconnects.
[14] Pankaj Gupta and Nick McKeown. 2001. Algorithms for packet classi-
fication. (2001).
[15] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina
Precup, and David Meger. 2017. Deep Reinforcement Learning that
Matters. CoRR (2017).
[16] Nathan Jay, Noga H Rotman, P Godfrey, Michael Schapira, and Aviv
Tamar. 2018. Internet Congestion Control via Deep Reinforcement
Learning. arXiv preprint arXiv:1810.03259 (2018).
[17] Kirill Kogan, Sergey Nikolenko, Ori Rottenstreich, William Culhane,
and Patrick Eugster. 2014. SAX-PAC (scalable and expressive packet
classification). In SIGCOMM CCR.
[18] AN Kolmogorov and NA Dmitriev. 1947. Stochastic branching pro-
cesses. In Doklady Akademi Nauk SSSR, Vol. 56. 7–10.
[19] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In
Advances in neural information processing systems.
[20] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In
Advances in neural information processing systems. 1008–1014.
[21] Peter Kontschieder, Madalina Fiterau, Antonio Criminisi, and Samuel
Rota Bulo. 2015. Deep Neural Decision Forests. In The IEEE Interna-
tional Conference on Computer Vision (ICCV).
[22] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis.
2018. The case for learned index structures. In SIGMOD.
[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet
classification with deep convolutional neural networks. In Advances
in neural information processing systems.
[24] Karthik Lakshminarayanan, Anand Rangarajan, and Srinivasan Venkat-
achary. 2005. Algorithms for advanced packet classification with
ternary CAMs. In SIGCOMM CCR.
[25] John Langford and Tong Zhang. 2008. The epoch-greedy algorithm
for multi-armed bandits with side information. In Advances in neural
information processing systems. 817–824.
[26] Ian Lenz, Honglak Lee, and Ashutosh Saxena. 2015. Deep learning for
detecting robotic grasps. The International Journal of Robotics Research
(2015).
[27] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. 2016.
End-to-end training of deep visuomotor policies. The Journal of Ma-
chine Learning Research (2016).
[28] Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre
Quillen. 2018. Learning hand-eye coordination for robotic grasping
with deep learning and large-scale data collection. The International
Journal of Robotics Research (2018).
[29] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan
Jurafsky. 2016. Deep reinforcement learning for dialogue generation.
arXiv preprint arXiv:1606.01541 (2016).
[30] Wenjun Li, Xianfeng Li, Hui Li, and Gaogang Xie. 2018. CutSplit: A
Decision-Tree Combining Cutting and Splitting for Scalable Packet
Classification. In IEEE INFOCOM.
[31] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox,
Ken Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica. 2018.
RLlib: Abstractions for distributed reinforcement learning. In ICML.
[32] Alex X Liu, Chad R Meiners, and Yun Zhou. 2008. All-match based
complete redundancy removal for packet classifiers in TCAMs. In IEEE
INFOCOM.
[33] Yadi Ma and Suman Banerjee. 2012. A smart pre-classifier to reduce
power consumption of TCAMs for multi-dimensional packet classifi-
cation. In ACM SIGCOMM.
[34] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kan-
dula. 2016. Resource management with deep reinforcement learning.
In ACM SIGCOMM HotNets Workshop.
[35] Hongzi Mao, Ravi Netravali, and Mohammad Alizadeh. 2017. Neural
adaptive video streaming with pensieve. In ACM SIGCOMM.
[36] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray
Kavukcuoglu. 2016. Asynchronous methods for deep reinforcement
learning. In ICML.
[37] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioan-
nis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing
atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602
(2013).
[38] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,
Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, An-
dreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control
through deep reinforcement learning. Nature (2015).
[39] Mohammad Norouzi, Maxwell Collins, Matthew A Johnson, David J
Fleet, and Pushmeet Kohli. 2015. Efficient non-greedy optimization of
decision trees. In Advances in Neural Information Processing Systems.
1729–1737.
[40] Stanley R Pliska. 1976. Optimization of multitype branching processes.
Management Science 23, 2 (1976), 117–124.
[41] Yaxuan Qi, Lianghong Xu, Baohua Yang, Yibo Xue, and Jun Li. 2009.
Packet classification algorithms: From theory to practice. In IEEE IN-
FOCOM.
14
Neural Packet Classification
[42] Yun R Qu, Hao H Zhang, Shijie Zhou, and Viktor K Prasanna. 2015. Op-
timizing many-field packet classification on FPGA, multi-core general
purpose processor, and GPU. In ACM/IEEE Symposium on Architectures
for Networking and Communications Systems.
[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv
preprint arXiv:1707.06347 (2017).
[44] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game
of Go with deep neural networks and tree search. Nature (2016).
[45] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan
Kumaran, Thore Graepel, et al. 2018. A general reinforcement learning
algorithm that masters chess, shogi, and Go through self-play. Science
(2018).
[46] David Silver,
Julian Schrittwieser, Karen Simonyan,
Ioannis
Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker,
Matthew Lai, Adrian Bolton, et al. 2017. Mastering the game of Go
without human knowledge. Nature (2017).
[47] Sumeet Singh, Florin Baboescu, George Varghese, and Jia Wang. 2003.
Packet classification using multidimensional cutting. In ACM SIG-
COMM.
[48] Ed Spitznagel, David Taylor, and Jonathan Turner. 2003. Packet classi-
fication using extended TCAMs. In IEEE ICNP.
[49] Venkatachary Srinivasan, Subhash Suri, and George Varghese. 1999.
Packet classification using tuple space search. In SIGCOMM CCR.
[50] Weibin Sun and Robert Ricci. 2013. Fast and flexible: parallel packet
processing with GPUs and click. In ACM/IEEE Symposium on Architec-
tures for Networking and Communications Systems.
[51] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to
sequence learning with neural networks. In Advances in neural infor-
mation processing systems.
[52] David E Taylor and Jonathan S Turner. 2005. ClassBench: A packet
classification benchmark. In IEEE INFOCOM.
[53] David E Taylor and Jonathan S Turner. 2005. Scalable packet classifica-
tion using distributed crossproducing of field labels. In IEEE INFOCOM.
[54] Asaf Valadarsky, Michael Schapira, Dafna Shahaf, and Aviv Tamar.
2017. Learning to route with deep rl. In NIPS Deep Reinforcement
Learning Symposium.
[55] Balajee Vamanan, Gwendolyn Voskuilen, and T. N. Vijaykumar. 2010.
EffiCuts: Optimizing Packet Classification for Memory and Through-
put. In ACM SIGCOMM.
[56] Hado Van Hasselt, Arthur Guez, and David Silver. 2016. Deep Rein-
forcement Learning with Double Q-Learning. In AAAI.
[57] Matteo Varvello, Rafael Laufer, Feixiong Zhang, and TV Lakshman.
2016. Multilayer packet classification with graphics processing units.
IEEE/ACM Transactions on Networking (2016).
[58] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014.
Knowledge Graph Embedding by Translating on Hyperplanes.. In
AAAI.
[59] Zheng Xiong, Wenpeng Zhang, and Wenwu Zhu. 2017. Learning
decision trees with reinforcement learning. In NIPS Workshop on Meta-
Learning.
[60] Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang
Yang, and Stephen Lin. 2007. Graph embedding and extensions: A
general framework for dimensionality reduction. IEEE transactions on
pattern analysis and machine intelligence (2007).
[61] Jianchao Yang, Shuicheng Yang, Yun Fu, Xuelong Li, and Thomas
Huang. 2008. Non-negative graph embedding. In CVPR.
15
[62] Hyunho Yeo, Youngmok Jung, Jaehong Kim, Jinwoo Shin, and Dongsu
Han. 2018. Neural adaptive content-aware internet video delivery. In
USENIX OSDI.
[63] Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec.
2018. Graph Convolutional Policy Network for Goal-Directed Molecu-
lar Graph Generation. arXiv preprint arXiv:1806.02473 (2018).
[64] Yasir Zaki, Thomas Pötsch, Jay Chen, Lakshminarayanan Subrama-
nian, and Carmelita Görg. 2015. Adaptive congestion control for
unpredictable cellular networks. In SIGCOMM CCR.
[65] Ying Zheng, Ziyu Liu, Xinyu You, Yuedong Xu, and Junchen Jiang.
2018. Demystifying Deep Learning in Networking. In ACM SIGCOMM
APNet Workshop.
[66] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and
Maosong Sun. 2018. Graph Neural Networks: A Review of Methods
and Applications. arXiv preprint arXiv:1812.08434 (2018).
A NEUROCUTS ACTION AND
OBSERVATION SPACES
NeuroCuts action and observation spaces described in Ope-
nAI Gym format [2]. Actions are sampled from two cate-
gorical distributions that select the dimension and action to
perform on the dimension respectively. Observations are en-
coded in a one-hot bit vector (278 bits in total) that describes
the node ranges, partitioning info, and action mask (i.e., for
prohibiting partitioning actions at lower levels).
A.1 Action Space
Tuple(Discrete(NumDims),
Discrete(NumCutActions + NumPartitionActions))
A.2 Observation Space
Box(low=0, high=1, shape=(278,))
A.3 Observation Components
max) +
) + BinaryString(Ranдedim
(BinaryString(Ranдedim
min
OneHot(Partitiondim
) + OneHot(Partitiondim
max))
∀dim ∈ {SrcIP, DstIP, SrcPort, DstPort, Protocol} +
min
OneHot(EffiCutsPartitionID) + ActionMask
When not using the EffiCuts partitioner, the Partitiondim
rule dimension coverage thresholds are set to one of the
following discrete levels: 0%, 2%, 4%, 8%, 16%, 32%, 64%, and
100%.
We note that the set of rules for the packet classifier are
not present in the observation space. NeuroCuts learns to
account for packet classifier rules implicitly through the
rewards it gets from the environment.
B NEUROCUTS HYPERPARAMETERS
Hyperparameter
Time-space coefficient c
Top-node partitioning
Reward scaling function f
Max timesteps per rollout
Max tree depth
Max timesteps to train
Max timesteps per batch
Model type
Model nonlinearity
Model hidden layers
Weight sharing between θ, θv
Learning rate
Discount factor γ
PPO entropy coefficient
PPO clip param
PPO VF clip param
PPO KL target
SGD iterations per batch
SGD minibatch size
Value
{none, simple, EffiCuts}
{x, log(x)}
{1000, 5000, 15000}
{100, 500}
10000000
60000
fully-connected
tanh
[512, 512]
true
0.00005
1.0
0.01
0.3
10.0
0.01
30
1000
Table 1: NeuroCuts hyperparameters. Values in curly
braces denote a set of values searched over during
evaluation. We found that the most sensitive hyper-
parameter is the top-node partitioning, which greatly
affects the structure of the search problem. It is also
important to ensure that the rollout timestep limit
and model used are sufficiently large for the problem
(we found that using 256-unit hidden layers slightly
degraded learning for larger classifiers, and more se-
verely so at 64-units).
16