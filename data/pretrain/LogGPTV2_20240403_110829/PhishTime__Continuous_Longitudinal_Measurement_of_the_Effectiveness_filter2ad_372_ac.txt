batches of websites on the same domains as in Experiment
C, but with di(cid:29)erent URL paths [3]. We then redeploy these
websites as part of a new experiment, which seeks to measure
how blacklist speed and coverage change when phishers
re-use domains and infrastructure to carry out successive
attacks (a strategy phishers use to increase their ROI).
Experiment F: Emerging Evasion. These websites
mirror the sophisticated, emerging evasion techniques we
observed in Section 5.2. Three batches implement evasion
using JavaScriptcode thatwe foundin the wildforCAPTCHA,
popup, and mouse movement cloaking, respectively. Three
additional batches have the same con(cid:27)guration but with
added .htaccess server-side cloaking, as in Experiment C. One
(cid:27)nal batch had only .htaccess cloaking, as a control group.
6.2 Other Measurements
Our remaining experiments follow a di(cid:29)erent reporting
methodology than those in the previous section.
Experiment E: Discovery. In this experiment, we launch
384    29th USENIX Security Symposium
USENIX Association
two batches of websites, per deployment, that mirror the
(basic) con(cid:27)guration of Experiments A and B. However, we
only report each batch to a single anti-phishing entity (PayPal
or the APWG), alternating between deployments. Thus, by
comparing against Experiments A and B,we can evaluate how
well our primary reporting methodology ensures prompt dis-
covery by blacklists. We can also directly test the performance
of speci(cid:27)c anti-phishing entities: we chose PayPal’s own
anti-phishing system because our websites used the PayPal
brand, and we chose the APWG because it had been shown
to reliably share phishing URLs with other entities [2,44].
Experiment G: Evidence-based Reporting. When we
initially designed our experiments, Google Safe Browsing
only allowed the submission of bare URLs when reporting
phishing (whether manually or programmatically). However,
in July 2019, with the release of the Chrome Suspicious
Site Reporter (CSSR) [11] plugin, manual reports could be
enhanced with additional evidence: a screenshot, source
code, and the redirection chain, IP address, and user agent for
the request. To evaluate if this enhanced reporting approach
could help improve blacklists’ detection of evasive URLs, we
designed this additional experiment to compare the coverage
of GSB when reporting with the old and the new method.
We con(cid:27)gured the two batches of phishing websites
in this experiment with cloaking that limits tra(cid:28)c to US
IP geolocations: a strategy that was recently capable of
evading GSB [44]. We reported one batch via CSSR [11]
and the other batch via the traditional GSB URL submission
form [22]. Because CSSR only supports manual submissions,
we compared it to another manual submission channel.
7 Implementation of Experiments
We adapted a previously-proposed testbed (PhishFarm [44])
to deploy the phishing websites needed for each of our ex-
periments. The testbed enables the automated con(cid:27)guration,
deployment, and monitoring of innocuous but real-looking
phishing websites to empirically measure browser-based
defenses such as blacklisting. To accurately emulate current
phishing trends and ecosystem defenses, we enhanced the
testbed to support automation of HTTPS website hosting,
lures with redirection, and API-based reporting.
7.1 Overview
In Figure 3, we provide an overview of the steps we took to
deploy each experiment. First, we prepare the hosting infras-
tructure ( A ). We used the aforementioned testbed to host
our phishing websites on 45 cloud-based Apache web servers,
each with a unique US IP. At the time of each deployment, we
con(cid:27)gure DNS records to point the required domains to these
web servers, and we install Let’s Encrypt SSL certi(cid:27)cates [33]
for each domain. Next, we con(cid:27)gure the phishing website
content and behavior (i.e., evasion techniques) for each URL,
and we test this con(cid:27)guration to verify the correct operation
of the framework ( B ). We then activate the websites and
Figure 3: Steps in each deployment of experiments.
(a) Successful request
(b) Request denied by cloaking
Figure 4: Appearance of our phishing websites.
immediately report their URLs to the anti-phishing entities
speci(cid:27)ed by the experimental design ( C ). Over the course
of the next seven days, we monitor the blacklist status of our
URLs and we collect web tra(cid:28)c metadata ( D ). Finally, we
deactivate the websites and analyze the collected data ( E ).
Each of these steps is fully automated by the testbed.
Allofourphishingwebsitesmatchedthelookandfeelofthe
PayPal.com login page as it appeared in January 2019. When-
ever a crawler request was denied by the cloaking technique
on a particular website, it would encounter a generic 404 error
message [20], as shown in Figure 4.
7.2 Reporting to Blacklists
To maintain consistency across our large number of ex-
periment deployments, we fully automated our reporting
methodology. Our reporting approach is representative of
the actions that an organization targeted by phishers might
take to mitigate known phishing websites [44].
To report each of our phishing websites, we submit its URL
directly to Google Safe Browsing via the Phishing Protection
Submission API [24]4 and to the APWG via the eCrime
Exchange API [2]. Direct API reporting is not available for
Opera and Microsoft SmartScreen. However, prior work
has shown that the APWG and other major anti-phishing
entities share data with these blacklists [44, 50]. Therefore,
we report to these additional entities via e-mail. Using a
PayPal-branded phishing e-mail template found in the wild,
we generate a fake phishing e-mail with the URL of the
website. We then forward this e-mail as an attachment to
anti-phishing entities that accept reports from the public:
PhishTank [49], Netcraft [42], PayPal [51], and US CERT [21].
This reporting methodology seeks to guarantee all blacklists’
discovery of our phishing websites (thus, it does not apply to
Experiments E and G, as previously discussed in Section 6.2).
4At the time of our deployments, the Phishing Protection Submission
API was in a beta stage and not available to the public. Google provided us
with access to the API for this research.
USENIX Association
29th USENIX Security Symposium    385
7.3 Blacklist Monitoring
We used a total of 40 virtual machines (VMs) to empirically
monitor blacklisting of each website at least once every 10
minutes across six desktop browsers: Chrome, Safari, Firefox,
IE, Edge, and Opera. In addition, to determine the speed of
blacklisting on mobile, we monitored Google Safe Browsing
programmatically using the Update API [23]. Using a single
physical Android phone (connected to the Internet over
Wi-Fi), we also empirically compared the coverage of mobile
Chrome, Firefox, and Opera to their desktop counterparts.
7.4 Experimental Controls
To ensure the validity of our experimental data, we metic-
ulously controlled the con(cid:27)guration and deployment of our
experiments to minimize the e(cid:29)ect of confounding factors
on the observed speed of blacklisting: any factors other than
the evasion technique of each website (Experiments A-F) or
the reporting channel (Experiment G).
Website Metadata. Beyond classifying phishing websites
based on their content, anti-phishing systems (including
blacklists) consider metadata such as deceptive URL key-
words, domain age, and URL and IP reputation [64]. Each
of our domains and URL paths consisted of combinations
of random English words to limit detection via URL or DNS
attributes [5,68]. To ensure that no historical maliciousness
was associated with our phishing URLs, we registered a new
domain name for each URL reported (except Experiment D,
which deliberately measured domain re-use). We also
registered our domains six months before each experiment,
leveraged a major registrar (GoDaddy), and used the .com
TLD (found in the majority of current phishing URLs) to
minimize detectability through these attributes [2].
Network Tra(cid:28)c. To prevent network signals from our
monitoring infrastructure from potentially skewing blacklist-
ing, our websites showed benign content to requests from this
infrastructure. We also disabled client-side anti-phishing fea-
tures in the browsers used for monitoring. Similarly, queries
to the Update API did not leak the URLs being checked.
Consistent Reporting. Some anti-phishing systems
(cid:27)lter reports to mitigate the risk of being (cid:30)ooded by (cid:27)ctitious
URLs from attackers. Our direct reports through Google’s
non-public API inherently avoid such (cid:27)ltering. Also, each
of our e-mail reports originated from a di(cid:29)erent e-mail
address, and information such as the ((cid:27)ctitious) victim name
or transaction amount was randomized between reports. We
initiated each deployment at approximately the same time
of day. We then sent the reports for any given experiment
in a single pass to minimize variations in reporting time, and
we throttled the reports to avoid an excessive reporting rate.
Experimental Variables. Within each experiment, we
varied the con(cid:27)guration of di(cid:29)erent batches in at most one
way to be able to perform a comparative analysis on a single
variable. The same concept also applies between the majority
of our experiments, which can thus collectively paint a multi-
dimensional view of the response of anti-phishing blacklists.
Experiment Duration. Anti-phishing blacklists typically
respond within a matter of hours; however, in certain cases
(e.g., due to cloaking), blacklisting may be delayed by several
days as additional (possibly manual) checks are made by
various entities [44]. This observation, combined with occa-
sional long-lasting phishing websites during the PhishTime
analysis, motivated our conservative choice of a one-week
lifespan for each phishing website in our experiments.
We discuss possible trade-o(cid:29)s in our controls in Section 9.3.
Nevertheless, in the following section, we show that our
experiments generally led to a consistent response by the
ecosystem and ultimately yielded actionable (cid:27)ndings.
8 Experimental Results
After the completion of all our experiment deployments, we
had collected extensive data for each of the 4,158 URLs that
we launched and monitored: timestamps of blacklisting (in
six desktop browsers, three mobile browsers, and the Google
Safe Browsing API), online status, certi(cid:27)cate revocation
status, and web tra(cid:28)c logs. Our infrastructure operated as
expected during each main deployment.
In the analysis that follows, for any given batch of URLs,
we de(cid:27)ne the coverage of a given blacklist as the percentage
of all URLs that were blacklisted at any point during the
seven-day deployment of the batch. For any given URL,
we de(cid:27)ne blacklist speed as the elapsed time between our
reporting of that URL and its subsequent blacklisting. Within
an individual batch, we either provide median speed in
tabular form or plot speed as a function of coverage over time.
Simpli(cid:27)cation of Dimensionality. Our empirical mon-
itoring of desktop browsers revealed that Chrome and Safari
consistently delivered the same blacklist speed and coverage,
whereas Firefox was an average of 10 minutes slower (likely
stemming from di(cid:29)erent caching of the GSB Update API [24])
but still had the same coverage. Similarly, in comparing IE and
Edge across all deployments, we found that the former was
12 minutes slower on average, also with the same coverage.
Thus, to simplify and clarify our analysis, we exclude the
desktop versions of Safari, Firefox, and IE from our evaluation.
On mobile devices, we found the blacklist speed and cover-
age of Firefox to be identical to its desktop counterpart. O(cid:31)ine
veri(cid:27)cationoftheGSBAPIdataalsoshowedthatmobileSafari
was consistent with mobile Chrome. We therefore do not du-
plicatetherespectivemetricsinthetablesinthissection. How-
ever,neither mobile Chrome nor mobile Opera showed consis-
tency with theirdesktop versions. Note that due to limited mo-
bile hardware, we could not accurately measure the speed of
mobile Opera across all experiments, so we exclude this data.
Data Aggregation. We aggregate our blacklist mea-
surements based on the objectives of each experiment, as
de(cid:27)ned in Section 6. For longitudinal comparisons, we group
blacklist performance by deployment; to evaluate evasion,
we aggregate multiple deployments by experiment or batch.
386    29th USENIX Security Symposium
USENIX Association
Desktop
GSB
Deployment Coverage Median Speed
00:44 (hh:mm)
1 May 2019
00:51
Jul. 2019
2
00:50
Sep. 2019
3
01:00
4 Oct. 2019
01:26
5 Nov. 2019
6 Dec. 2019
00:46
100.0%
100.0%
64.8%
98.1%
100.0%
100.0%
Mobile
GSB: Chrome/Safari
GSB: Firefox Opera
Opera
SmartScreen
Coverage Median Speed Coverage Median Speed Coverage Median Speed Coverage
100.0%
100.0%
61.1%
100.0%
100.0%
100.0%
100.0%
55.6%
13.0%
50.0%
13.0%
70.4%
100.0%
100.0%
64.8%
98.1%
100.0%
100.0%
09:19
35:28
159:22
03:05
39:11
00:28
98.1%
70.4%
22.2%
64.8%
59.3%
48.1%
02:02
02:38
04:44
02:19
02:27
02:34
00:37
00:32
01:52
00:55
00:38
00:28
Avg. Tra(cid:28)c
Coverage All Requests
0.0%
0.0%
14.8%
14.8%
0.0%
9.3%
1677
7003
286
3756
1566
3255
Successful
Requests
1151
1491
211
2020
682
1554
Table 2: Blacklist performance vs. unevasive phishing (Experiment A: raw data for each deployment).
Desktop
GSB
Deployment Coverage Median Speed
02:29 (hh:mm)
1 May 2019
Jul. 2019
01:46
2
02:21
Sep. 2019
3
4 Oct. 2019
01:32
89.5%