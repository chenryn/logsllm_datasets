### Experiment C: Reuse of Domains and Infrastructure
In this experiment, we deploy batches of websites on the same domains as in Experiment C, but with different URL paths [3]. The aim is to measure how blacklist speed and coverage change when phishers reuse domains and infrastructure for successive attacks. This strategy is often employed by phishers to increase their return on investment (ROI).

### Experiment F: Emerging Evasion Techniques
This experiment focuses on sophisticated evasion techniques observed in Section 5.2. We create three batches of websites that implement evasion using JavaScript code found in the wild for CAPTCHA, popup, and mouse movement cloaking, respectively. Additionally, three more batches have the same configuration but with added .htaccess server-side cloaking, similar to Experiment C. A final batch uses only .htaccess cloaking as a control group.

### 6.2 Other Measurements
Our remaining experiments follow a different reporting methodology compared to those in the previous section.

#### Experiment E: Discovery
In this experiment, we launch two batches of websites per deployment, mirroring the basic configuration of Experiments A and B. However, each batch is reported to a single anti-phishing entity (either PayPal or the APWG), alternating between deployments. By comparing these results with Experiments A and B, we can evaluate the effectiveness of our primary reporting methodology in ensuring prompt discovery by blacklists. We also directly test the performance of specific anti-phishing entities. We chose PayPal's own anti-phishing system because our websites used the PayPal brand, and we selected the APWG due to its proven reliability in sharing phishing URLs with other entities [2, 44].

#### Experiment G: Evidence-based Reporting
Initially, Google Safe Browsing (GSB) only allowed the submission of bare URLs for reporting phishing, whether manually or programmatically. In July 2019, with the release of the Chrome Suspicious Site Reporter (CSSR) [11] plugin, manual reports could be enhanced with additional evidence such as a screenshot, source code, and the redirection chain, IP address, and user agent for the request. To evaluate if this enhanced reporting approach improves blacklists' detection of evasive URLs, we designed an experiment to compare the coverage of GSB when using the old and new methods. We configured two batches of phishing websites with cloaking that limits traffic to US IP geolocations, a strategy recently capable of evading GSB [44]. One batch was reported via CSSR [11], and the other via the traditional GSB URL submission form [22]. Since CSSR supports only manual submissions, we compared it to another manual submission channel.

### 7 Implementation of Experiments
We adapted a previously proposed testbed (PhishFarm [44]) to deploy the phishing websites needed for each experiment. The testbed enables the automated configuration, deployment, and monitoring of innocuous but real-looking phishing websites to empirically measure browser-based defenses such as blacklisting. To accurately emulate current phishing trends and ecosystem defenses, we enhanced the testbed to support automation of HTTPS website hosting, lures with redirection, and API-based reporting.

#### 7.1 Overview
Figure 3 provides an overview of the steps taken to deploy each experiment:
1. **Prepare Hosting Infrastructure (A):** We used the testbed to host our phishing websites on 45 cloud-based Apache web servers, each with a unique US IP. At the time of each deployment, we configure DNS records to point the required domains to these web servers and install Let’s Encrypt SSL certificates [33] for each domain.
2. **Configure Website Content and Behavior (B):** We configure the phishing website content and behavior (i.e., evasion techniques) for each URL and test this configuration to verify the correct operation of the framework.
3. **Activate Websites and Report (C):** We activate the websites and immediately report their URLs to the anti-phishing entities specified by the experimental design.
4. **Monitor Blacklist Status and Collect Data (D):** Over the next seven days, we monitor the blacklist status of our URLs and collect web traffic metadata.
5. **Deactivate Websites and Analyze Data (E):** Finally, we deactivate the websites and analyze the collected data. Each of these steps is fully automated by the testbed.

All our phishing websites matched the look and feel of the PayPal.com login page as it appeared in January 2019. When a crawler request was denied by the cloaking technique on a particular website, it encountered a generic 404 error message [20], as shown in Figure 4.

#### 7.2 Reporting to Blacklists
To maintain consistency across our large number of experiment deployments, we fully automated our reporting methodology. Our approach is representative of the actions a targeted organization might take to mitigate known phishing websites [44].

For each phishing website, we submit its URL directly to Google Safe Browsing via the Phishing Protection Submission API [24] and to the APWG via the eCrime Exchange API [2]. Direct API reporting is not available for Opera and Microsoft SmartScreen, but prior work has shown that the APWG and other major anti-phishing entities share data with these blacklists [44, 50]. Therefore, we report to these additional entities via email. Using a PayPal-branded phishing email template found in the wild, we generate a fake phishing email with the URL of the website and forward it as an attachment to anti-phishing entities that accept reports from the public: PhishTank [49], Netcraft [42], PayPal [51], and US CERT [21]. This reporting methodology aims to ensure all blacklists discover our phishing websites (thus, it does not apply to Experiments E and G, as discussed in Section 6.2).

#### 7.3 Blacklist Monitoring
We used 40 virtual machines (VMs) to empirically monitor blacklisting of each website at least once every 10 minutes across six desktop browsers: Chrome, Safari, Firefox, IE, Edge, and Opera. Additionally, to determine the speed of blacklisting on mobile, we monitored Google Safe Browsing programmatically using the Update API [23]. Using a single physical Android phone (connected to the Internet over Wi-Fi), we also empirically compared the coverage of mobile Chrome, Firefox, and Opera to their desktop counterparts.

#### 7.4 Experimental Controls
To ensure the validity of our experimental data, we meticulously controlled the configuration and deployment of our experiments to minimize the effect of confounding factors on the observed speed of blacklisting.

- **Website Metadata:** Beyond classifying phishing websites based on their content, anti-phishing systems consider metadata such as deceptive URL keywords, domain age, and URL and IP reputation [64]. Each of our domains and URL paths consisted of combinations of random English words to limit detection via URL or DNS attributes [5, 68]. We registered a new domain name for each URL reported (except Experiment D, which deliberately measured domain reuse). We also registered our domains six months before each experiment, leveraged a major registrar (GoDaddy), and used the .com TLD to minimize detectability through these attributes [2].
- **Network Traffic:** To prevent network signals from our monitoring infrastructure from potentially skewing blacklisting, our websites showed benign content to requests from this infrastructure. We also disabled client-side anti-phishing features in the browsers used for monitoring. Similarly, queries to the Update API did not leak the URLs being checked.
- **Consistent Reporting:** Some anti-phishing systems filter reports to mitigate the risk of being flooded by fictitious URLs from attackers. Our direct reports through Google’s non-public API inherently avoid such filtering. Each of our email reports originated from a different email address, and information such as the (fictitious) victim name or transaction amount was randomized between reports. We initiated each deployment at approximately the same time of day, sent the reports in a single pass to minimize variations in reporting time, and throttled the reports to avoid an excessive reporting rate.
- **Experimental Variables:** Within each experiment, we varied the configuration of different batches in at most one way to perform a comparative analysis on a single variable. This concept also applies between the majority of our experiments, collectively providing a multi-dimensional view of the response of anti-phishing blacklists.
- **Experiment Duration:** Anti-phishing blacklists typically respond within hours, but in certain cases (e.g., due to cloaking), blacklisting may be delayed by several days. This observation, combined with occasional long-lasting phishing websites during the PhishTime analysis, motivated our conservative choice of a one-week lifespan for each phishing website in our experiments.

### 8 Experimental Results
After completing all experiment deployments, we collected extensive data for each of the 4,158 URLs launched and monitored, including timestamps of blacklisting, online status, certificate revocation status, and web traffic logs. Our infrastructure operated as expected during each main deployment.

For any given batch of URLs, we define the coverage of a given blacklist as the percentage of all URLs that were blacklisted at any point during the seven-day deployment of the batch. For any given URL, we define blacklist speed as the elapsed time between our reporting of that URL and its subsequent blacklisting. Within an individual batch, we either provide median speed in tabular form or plot speed as a function of coverage over time.

#### Simplification of Dimensionality
Our empirical monitoring of desktop browsers revealed that Chrome and Safari consistently delivered the same blacklist speed and coverage, while Firefox was an average of 10 minutes slower (likely due to different caching of the GSB Update API [24]) but still had the same coverage. Similarly, in comparing IE and Edge across all deployments, we found that the former was 12 minutes slower on average, also with the same coverage. Thus, to simplify and clarify our analysis, we exclude the desktop versions of Safari, Firefox, and IE from our evaluation.

On mobile devices, we found the blacklist speed and coverage of Firefox to be identical to its desktop counterpart. Offline verification of the GSB API data also showed that mobile Safari was consistent with mobile Chrome. We therefore do not duplicate the respective metrics in the tables in this section. However, neither mobile Chrome nor mobile Opera showed consistency with their desktop versions. Due to limited mobile hardware, we could not accurately measure the speed of mobile Opera across all experiments, so we exclude this data.

#### Data Aggregation
We aggregate our blacklist measurements based on the objectives of each experiment, as defined in Section 6. For longitudinal comparisons, we group blacklist performance by deployment; to evaluate evasion, we aggregate multiple deployments by experiment or batch.

| Desktop | GSB | Deployment | Coverage | Median Speed (hh:mm) |
|---------|-----|------------|----------|----------------------|
| 1 May 2019 | 00:44 | 100.0% | 00:51 |
| Jul. 2019 | 02 | 00:50 | 100.0% |
| Sep. 2019 | 03 | 01:00 | 64.8% |
| Oct. 2019 | 04 | 01:26 | 98.1% |
| Nov. 2019 | 05 | 00:46 | 100.0% |
| Dec. 2019 | 06 | 00:46 | 100.0% |

| Mobile | GSB: Chrome/Safari | GSB: Firefox | Opera | SmartScreen |
|--------|---------------------|--------------|-------|-------------|
| Coverage | Median Speed (hh:mm) | Coverage | Median Speed (hh:mm) | Coverage | Median Speed (hh:mm) | Coverage |
| 100.0% | 09:19 | 100.0% | 35:28 | 13.0% | 159:22 | 50.0% |
| 100.0% | 03:05 | 13.0% | 39:11 | 70.4% | 00:28 | 98.1% |
| 100.0% | 02:02 | 70.4% | 02:38 | 22.2% | 04:44 | 64.8% |
| 98.1% | 02:19 | 59.3% | 02:27 | 48.1% | 02:34 | 00:37 |
| 70.4% | 00:32 | 22.2% | 01:52 | 00:55 | 00:38 | 00:28 |
| 22.2% | 00:37 | 00:32 | 01:52 | 00:55 | 00:38 | 00:28 |

| Avg. Traffic | Coverage | All Requests | Successful Requests |
|--------------|----------|--------------|---------------------|
| 0.0% | 0.0% | 14.8% | 14.8% |
| 0.0% | 9.3% | 1677 | 7003 |
| 14.8% | 286 | 3756 | 1566 |
| 3255 | 1151 | 1491 | 211 |
| 2020 | 682 | 1554 | 02:29 (hh:mm) |
| 89.5% | 02:29 (hh:mm) | 1 May 2019 | 01:46 |

Table 2: Blacklist performance vs. unevasive phishing (Experiment A: raw data for each deployment).

| Desktop | GSB | Deployment | Coverage | Median Speed (hh:mm) |
|---------|-----|------------|----------|----------------------|
| 1 May 2019 | 02:29 | 89.5% | 01:46 |
| Jul. 2019 | 02 | 02:21 | 89.5% |
| Sep. 2019 | 03 | 01:32 | 89.5% |
| Oct. 2019 | 04 | 01:32 | 89.5% |

Table 3: Blacklist performance vs. evasive phishing (Experiment F: raw data for each deployment).