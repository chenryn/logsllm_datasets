# Leveraging Endpoint Flexibility in Data-Intensive Clusters

**Authors:**
- Mosharaf Chowdhury<sup>1</sup>
- Srikanth Kandula<sup>2</sup>
- Ion Stoica<sup>1</sup>

**Affiliations:**
- 1. UC Berkeley
- 2. Microsoft Research

**Emails:**
- {mosharaf, istoica}@cs.berkeley.edu
- PI:EMAIL (for Srikanth Kandula)

## Abstract

Many applications do not impose strict constraints on the destinations of their network transfers. This flexibility presents new opportunities for optimizing network traffic, especially when these transfers contribute significantly to overall network load. By strategically choosing endpoints to avoid congested links, the completion times of both flexible and non-flexible transfers can be improved. In this paper, we focus on leveraging the flexibility in replica placement during writes to cluster file systems (CFSes), which account for nearly half of all cross-rack traffic in data-intensive clusters. The replicas of a CFS write can be placed in any subset of machines, provided they are in multiple fault domains and ensure balanced storage usage across the cluster.

We study the interactions between CFS and the cluster network, analyze optimizations for replica placement, and propose Sinbad—a system that identifies network imbalances and adapts replica destinations to navigate around congested links. Experiments on Amazon EC2 and trace-driven simulations show that block writes complete 1.3× (respectively, 1.58×) faster as the network becomes more balanced. As a collateral benefit, end-to-end completion times of data-intensive jobs also improve. Sinbad achieves this with minimal impact on long-term storage balance.

## Categories and Subject Descriptors

C.2 [Computer-communication networks]: Distributed systems—Cloud computing

## Keywords

Cluster file systems, data-intensive applications, datacenter networks, constrained anycast, replica placement

## 1. Introduction

The network remains a bottleneck in data-intensive clusters, as evidenced by the continued focus on static optimizations and data-local task schedulers that reduce network usage. Network schedulers can choose different paths, vary flow rates, and prioritize flows, but they cannot change the origin or destination of a flow. However, many inter-machine transfers, such as writes to CFSes like GFS, HDFS, Cosmos, Amazon S3, or Windows Azure Storage, do not require specific destinations as long as certain constraints are met. These systems store large pieces of data by dividing them into fixed-size blocks and transferring each block to three machines in two different racks for fault-tolerance and partition-tolerance.

Analysis of traces from production clusters at Facebook and Microsoft reveals that such replicated writes (referred to as distributed writes) account for almost half of all cross-rack traffic. Moreover, recent data suggests a growing trend in the volume of distributed writes, with hundreds of terabytes of data being ingested daily into different CFS installations. Despite the network often being underutilized, there is substantial imbalance in the usage of bottleneck links, leading to congestion and performance degradation. Common causes include skew in application communication patterns and a lack of control over background traffic in multi-tenant datacenters.

In this paper, we present Sinbad, a system that leverages the flexibility in endpoint placement during distributed writes to steer replication transfers away from network hotspots. Network-balanced placement improves CFS write throughput by avoiding network contention and enhances the throughput of non-CFS traffic on those links. Sinbad's approach is akin to overlay anycast, where the "best" set of destinations is chosen to maximize the throughput of replicating large blocks while considering constraints like the number of fault domains and minimizing storage imbalance.

## 2. Background

### 2.1. Cluster File Systems (CFSes)

A typical CFS deployment consists of a set of storage slaves and a master that coordinates writes and reads. Files stored in a CFS are collections of large blocks, typically ranging from 64 MB to 1 GB. CFS designs provide failure recovery through replication and ensure strong consistency among replicas.

#### Write Workflow

When writing a file to the CFS, the client specifies a replication factor (r) and a fault-tolerance factor (f). Typically, r = 3 and f = 1, meaning each block is stored in three machines in two racks and can survive one rack failure. The replica placement policy in the CFS master independently decides where to place each block to balance storage load and minimize outstanding writes per disk.

#### Read Workflow

Reading from the CFS is simpler. The CFS master reports the locations of all replicas, and task schedulers try to achieve data locality. Although reads are separate from writes, read performance is influenced by the placement of blocks.

### 2.2. Network Model

CFS deployments in modern clusters run on topologies with full-bisection bandwidth cores and some oversubscription in core-to-rack links. We consider a network model where downlinks to storage racks can be skewed, common in data-intensive clusters with collocated compute and storage.

## 3. Sinbad Overview

### 3.1. Problem Statement

Given a replica placement request, Sinbad must return a set of locations for the CFS master to replicate the block to. This problem can be thought of as constrained overlay anycast in a throughput-sensitive context, where the goal is to replicate large blocks to multiple machines in different fault domains without introducing significant storage imbalance.

### 3.2. Architectural Overview

Sinbad is designed to replace the default replica placement policy in existing CFSes. It uses a centralized architecture, with the Sinbad master collocated with the CFS master. Sinbad slaves periodically send measurement updates to the master, including link utilizations and current disk usage. The Sinbad master aggregates this information and estimates current utilizations of bottleneck links.

## 4. Trace Analysis

Table 1: Details of Facebook and Microsoft Bing Traces

| Date          | Duration   | Framework  | Jobs       | Tasks         | CFS        | Block Size | Machines | Racks | Core:Rack Oversubscription |
|---------------|------------|------------|------------|---------------|------------|------------|----------|-------|----------------------------|
| Facebook      | Oct 2010   | One week   | Hadoop [3] | 175,000       | 30 million | HDFS [4]   | 256 MB   | 3,000 | 150                        | 10:1       |
| Microsoft Bing| Mar-Apr 2012 | One month | SCOPE [19] | Tens of Thousands | Tens of Millions | Cosmos [19] | N/A      | N/A   | N/A                        |

Analysis of traces from Facebook and Microsoft Bing reveals that distributed writes account for almost half of all cross-rack traffic. The volume of distributed writes is growing, with hundreds of terabytes of data being ingested daily. Despite network underutilization, there is substantial imbalance in the usage of bottleneck links, leading to congestion and performance degradation.

## 5. Optimizations and Algorithms

Optimizing distributed writes is NP-hard even in the offline case. We show that if hotspots are stable while a block is being written and all blocks have the same size, greedy placement through the least-loaded bottleneck link is optimal for optimizing the average block write time. Under the same assumptions, files with the least remaining blocks should be prioritized to optimize the average file write time.

## 6. System Design

Sinbad employs the proposed algorithms and enforces necessary constraints to make network-aware replica placement decisions. It periodically measures the network and reacts to the imbalance in non-CFS traffic. An application layer measurement-based predictor performs well in practice due to short-term stability and long-term uniformness of network hotspots. Sinbad is not tied to any networking technology, making it deployable in public clouds.

## 7. Evaluation

We have implemented Sinbad as a pluggable replica placement policy for the Facebook-optimized HDFS distribution. We evaluated Sinbad by replaying a scaled-down workload from a Facebook trace on a 100-machine Sinbad deployment on Amazon EC2. Sinbad improves the average block write time by 1.3× and the average end-to-end completion time of jobs by up to 1.26× with limited penalties due to its online decisions. It decreases the imbalance across the network with little impact on storage load balancing. For in-memory storage systems, the improvements can be even higher. Through trace-driven simulations, we show that Sinbad’s improvement (1.58×) is close to the optimal (1.89×).

## 8. Discussion

We discuss known issues and possible solutions in Section 8 and consider Sinbad in light of relevant work in Section 9.

## 9. Related Work

We compare Sinbad with other related work in this section, highlighting its unique contributions and advantages.

## 10. Conclusion

Sinbad effectively leverages the flexibility in endpoint placement during distributed writes to improve network balance and overall performance in data-intensive clusters. By steering replication transfers away from network hotspots, Sinbad enhances the throughput of both CFS and non-CFS traffic, with minimal impact on long-term storage balance.