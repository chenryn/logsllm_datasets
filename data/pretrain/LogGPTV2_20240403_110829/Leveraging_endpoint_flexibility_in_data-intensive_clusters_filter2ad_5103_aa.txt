title:Leveraging endpoint flexibility in data-intensive clusters
author:Mosharaf Chowdhury and
Srikanth Kandula and
Ion Stoica
Leveraging Endpoint Flexibility in Data-Intensive Clusters
Mosharaf Chowdhury1, Srikanth Kandula2, Ion Stoica1
1UC Berkeley, 2Microsoft Research
{mosharaf, istoica}@cs.berkeley.edu, PI:EMAIL
ABSTRACT
Many applications do not constrain the destinations of their net-
work transfers. New opportunities emerge when such transfers con-
tribute a large amount of network bytes. By choosing the endpoints
to avoid congested links, completion times of these transfers as well
as that of others without similar ﬂexibility can be improved. In this
paper, we focus on leveraging the ﬂexibility in replica placement
during writes to cluster ﬁle systems (CFSes), which account for al-
most half of all cross-rack trafﬁc in data-intensive clusters. The
replicas of a CFS write can be placed in any subset of machines as
long as they are in multiple fault domains and ensure a balanced
use of storage throughout the cluster.
We study CFS interactions with the cluster network, analyze op-
timizations for replica placement, and propose Sinbad – a system
that identiﬁes imbalance and adapts replica destinations to navi-
gate around congested links. Experiments on EC2 and trace-driven
simulations show that block writes complete 1:3(cid:2) (respectively,
1:58(cid:2)) faster as the network becomes more balanced. As a col-
lateral beneﬁt, end-to-end completion times of data-intensive jobs
improve as well. Sinbad does so with little impact on the long-term
storage balance.
Categories and Subject Descriptors
C.2 [Computer-communication networks]: Distributed sys-
tems—Cloud computing
Keywords
Cluster ﬁle systems, data-intensive applications, datacenter net-
works, constrained anycast, replica placement
1
The network remains a bottleneck in data-intensive clusters, as ev-
idenced by the continued focus on static optimizations [7, 31] and
data-local task schedulers [10, 34, 43, 44] that reduce network us-
age, and on scheduling the exchanges of intermediate data [10,20].
The endpoints of a ﬂow are assumed to be ﬁxed: network sched-
ulers [8,13,20,28] can choose between different paths, vary rates of
ﬂows, and prioritize one ﬂow over another, but they cannot change
where a ﬂow has originated from or its destination.
Introduction
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM or the author must be honored. To
copy otherwise, or republish, to post on servers or to redistribute to lists,
requires prior speciﬁc permission and/or a fee.
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
Copyright 2013 ACM 978-1-4503-2056-6/13/08 ...$15.00.
However, many inter-machine transfers do not require their des-
tinations to be in speciﬁc locations as long as certain constraints
are satisﬁed. An example of such transfers in data-intensive clus-
ters is the writes to cluster ﬁle systems (CFSes) like GFS [27],
HDFS [15], Cosmos [19], Amazon S3 [2], or Windows Azure Stor-
age [17]. These systems store large pieces of data by dividing them
into ﬁxed-size blocks, and then transferring each block to three
machines (for fault-tolerance) in two different racks (for partition-
tolerance) using chain replication [42]. The replicas can be in any
subset of machines that satisfy the constraints.
Analysis of traces from production clusters at Facebook and Mi-
crosoft reveals that such replicated writes (referred to as distributed
writes from hereon) account for almost half of all cross-rack traf-
ﬁc in both clusters (§4). Moreover, recent data suggests a grow-
ing trend in the volume of distributed writes with hundreds of ter-
abytes of data being ingested everyday into different CFS installa-
tions [5, 6, 17]. We also ﬁnd that while the network is often under-
utilized, there exists substantial imbalance in the usage of bottle-
neck links. Such imbalance leads to congestion and performance
degradation. Common causes of imbalance include skew in appli-
cation communication patterns1 [14, 35] and a lack of control on
the background trafﬁc in multi-tenant datacenters.
Even though replication takes place over the network, its inter-
actions with the network have so far been ignored.
In this pa-
per, we present Sinbad, a system that leverages the ﬂexibility in
endpoint placement during distributed writes to steer replication
transfers away from network hotspots. Network-balanced place-
ment has two implications. First, it improves CFS write through-
put by avoiding network contention; response times of tasks that
write improve as well. Second, by steering CFS’ trafﬁc away from
hotspots, throughput of non-CFS trafﬁc on those links increase; in
data-intensive clusters, this speeds up tasks that shufﬂe intermedi-
ate data and jobs with large shufﬂes.
Exercising the freedom in endpoint selection in our approach is
akin to that of overlay anycast, which is typically employed by
latency-sensitive request-response applications to select the “best”
source (server) from where to retrieve the content [18, 25]. How-
ever, in the case of distributed writes, we exploit this ﬂexibility for
picking the “best” set of destinations to maximize the throughput
of replicating large blocks.
In addition, we consider constraints
like the number of fault domains and aim to minimize the storage
imbalance across machines.
Storage imbalance during replica placement is harmful, because
machines receiving too many replicas can become hotspots for fu-
ture tasks. Existing CFSes employ a uniform replica placement
1For example, when counting the occurrences of DISTINCT keys
in a dataset, the amount of data in each partition (to be received by
corresponding reducer) can be very skewed.
231policy and perform periodic re-balancing to avoid such imbalance.
We show that a network-balanced placement policy does not trig-
ger additional storage balancing cycles. While network hotspots
are stable in the short term to allow network-balanced placement
decisions, they are uniformly distributed across all bottleneck links
in the long term ensuring storage load balancing.
Optimizing distributed writes is NP-hard even in the ofﬂine case,
because ﬁnding the optimal solution is akin to optimally schedul-
ing tasks in heterogeneous parallel machines without preemption
[9, 26]. We show that if hotspots are stable while a block is be-
ing written and all blocks have the same size, greedy placement
through the least-loaded bottleneck link is optimal for optimizing
the average block write time (§5). Under the same assumptions, we
also show that to optimize the average ﬁle write time, ﬁles with the
least remaining blocks should be prioritized.
Sinbad employs the proposed algorithms and enforces necessary
constraints to make network-aware replica placement decisions
(§6).
It periodically measures the network and reacts to the im-
balance in the non-CFS trafﬁc. An application layer measurement-
based predictor performs reasonably well in practice due to short-
term (few tens of seconds) stability and long-term (hours) uniform-
ness of network hotspots. We ﬁnd this approach attractive because
it is not tied to any networking technology, which makes it readily
deployable in public clouds.
We have implemented Sinbad as a pluggable replica placement
policy for the Facebook-optimized HDFS distribution [4]. HDFS is
a popular open-source CFS, and it is the common substrate behind
many data-parallel infrastructures [3, 32, 45]. We avoid the many
known performance problems in Hadoop [3] by running jobs using
an in-memory compute engine (e.g., Spark [45]). We have eval-
uated Sinbad (§7) by replaying the scaled-down workload from a
Facebook trace on a 100-machine Sinbad deployment on Amazon
EC2 [1]. We show that Sinbad improves the average block write
time by 1:3(cid:2) and the average end-to-end completion time of jobs
by up to 1:26(cid:2) with limited penalties due to its online decisions.
In the process, it decreases the imbalance across the network with
little impact on storage load balancing. For in-memory storage sys-
tems, the improvements can be even higher. Through trace-driven
simulations, we also show that Sinbad’s improvement (1:58(cid:2)) is
close to that of a loose upper bound (1:89(cid:2)) of the optimal.
We discuss known issues and possible solutions in Section 8, and
we consider Sinbad in light of relevant pieces of work in Section 9.
2 CFS Background
This section provides a brief architectural overview of cluster
ﬁle systems (CFSes) focusing primarily on the end-to-end write
pipeline. Examples of CFSes include distributed ﬁle systems (DFS)
like GFS at Google [27], HDFS at Facebook and Yahoo! [4, 15],
and Cosmos [19] at Bing. We also include public cloud-based
storage systems like Amazon S3 [2] and Windows Azure Storage
(WAS) [17] that have similar architecture and characteristics, and
are extensively used by popular services like dropbox.com.
2.1 System Model
A typical CFS deployment consists of a set of storage slaves and a
master that coordinates the writes to (reads from) CFS slaves. Files
(aka objects/blobs) stored in a CFS are collections of large blocks.
Block size in production clusters varies from 64 MB to 1 GB.2 The
block size demonstrates a trade-off between disk I/O throughput
vs.
the beneﬁt from parallelizing across many disks. Most CFS
designs provide failure recovery guarantees for stored ﬁles through
replication and ensure strong consistency among the replicas.
2Blocks are not padded, i.e., the last block in a ﬁle can be smaller.
Figure 1: Distributed write pipeline. Each block has three copies in two
racks and three different machines.
Write Workﬂow When writing a ﬁle to the CFS, the client pro-
vides a replication (r) factor and a fault-tolerance (f) factor to
ensure that each block of that ﬁle has r copies located in at least
f (< r) fault domains. The former is for load balancing (blocks
in popular ﬁles have more replicas [11]), while the latter ensures
availability in spite of failures. Machines in different racks are typ-
ically considered to be in independent fault domains. Typically,
r = 3 and f = 1; meaning, each block is stored in three machines
in two racks and can survive at most one rack failure (Figure 1).
Thus, writing a block copies it at least once across racks.
The replica placement policy in the CFS master independently
decides where to place each block irrespective of their parent ﬁles.
Blocks from the same ﬁle and their replicas need not be collocated.
The goal is to uniformly place blocks across all machines and fault
domains so as to
(cid:15) minimize the imbalance in storage load across disks, and
(cid:15) balance the number of outstanding writes per disk.
Both these constraints assume that writes are bottlenecked only
by disks. This assumption, however, is not always true since the ex-
tent of oversubscription in modern datacenter networks (typically
between the core and racks) can cause writes to bottleneck on the
oversubscribed links. Even on topologies with full bisection band-
width, writes can bottleneck on the servers’ network interfaces for
high in-degrees or when the cumulative write throughput of a server
is larger than its NIC speed. For example, a typical server with six
to eight commodity disks [33, 41] has sequential write throughput
that is several times the typical NIC speed (1 Gbps).
Once replica locations have been determined, the CFS slave
transfers the block to the selected destinations using chain repli-
cation [42]. Distributed writes are synchronous; to provide strong
consistency, the originator task will have to wait until the last
replica of the last block has been written. Hence, write response
times inﬂuence task completion times as well.
Read Workﬂow Reading from the CFS is simpler. Given a ﬁle,
the CFS master reports the locations of all the replicas of all the
blocks of that ﬁle. Given these locations, task schedulers try to
achieve data locality through a variety of techniques [10,34,43,44].
Although reads are separate from writes, read performance is
still inﬂuenced by the placement of blocks. By minimizing storage
imbalance, a CFS strives to minimize the performance impact of
reads in future tasks.
2.2 Network Model
CFS deployments in modern clusters run on topologies that often
have a full-bisection bandwidth core (e.g., fat-tree [38], VL2 [28])
with some oversubscription in core-to-rack links (Figure 1). We
consider a network model, where downlinks to storage racks can
be skewed. This is common in typical data-intensive clusters with
Core!Fault Domain 1/!Rack 1!Fault Domain 2/!Rack 2!Fault Domain 3/!Rack 3!232Figure 2: Decision process of Sinbad master.
collocated compute and storage. For dedicated storage racks (i.e.,
when compute and storage are not collocated), skew can still exist
due to random placement decisions made by the CFS master.
3 Sinbad Overview
Sinbad is a measurement-based system to perform network-
balanced replica placement during distributed writes. In this sec-
tion, we present a brief overview of Sinbad to help the reader follow
the measurements (§4), analysis (§5), and design (§6) presented in
subsequent sections.
3.1 Problem Statement
Given a replica placement request – with information about the lo-
cation of the writer, size of the block, and the replication factor –
Sinbad must return a set of locations for the CFS master to replicate
that block to (Figure 2). All information about a block request is
unknown prior to its arrival.
One can think of this problem as constrained overlay anycast
in a throughput-sensitive context. However, instead of retrieving
responses from the best sources, we have to replicate large blocks
to multiple machines in different fault domains without introducing
signiﬁcant storage imbalance across the cluster.
The problem of placing replicas to minimize the imbalance
across bottleneck links is NP-hard even in the ofﬂine case (§5).
Sinbad employs algorithms developed by exploiting observations
from real-world clusters (§4) to perform reasonably well in realis-
tic settings.
3.2 Architectural Overview
Sinbad is designed to replace the default replica placement policy in
existing CFSes. Similar to its target CFSes, Sinbad uses a central-
ized architecture (Figure 3) to use global knowledge while making
its decisions. Sinbad master is collocated with the CFS master, and
it makes placement decisions based on information collected by its
slaves.
Sinbad slaves (collocated with CFS slaves) periodically send
measurement updates to the master by piggybacking on regular
heartbeats from CFS slaves. They report back several pieces of
information, including incoming and outgoing link utilizations at
host NICs and current disk usage. Sinbad master aggregates the
collected information and estimates current utilizations of bottle-
neck links (§6.2). Sinbad uses host-based application layer tech-
niques for measurements. This comes out of practicality: we
Figure 3: Sinbad architecture. Sinbad agents are collocated with the corre-
sponding agents of the parent CFS.
Table 1: Details of Facebook and Microsoft Bing Traces
Date
Duration
Framework
Jobs
Tasks
CFS
Block Size
Machines
Racks
Core:Rack
Oversubscription
Facebook
Oct 2010
One week
Hadoop [3]
175; 000
30 million
HDFS [4]
256 MB
3; 000
150
10 : 1
Microsoft Bing
Mar-Apr 2012
One month
SCOPE [19]
Tens of Thousands
Tens of Millions