2015:
• LLVM libFuzzer, in-process coverage guided fuzzer using Sanitizer
Coverage instrumentation
2016:
• Microsoft announces SAGE based fuzzing service, Project Springfield20
• Google announces open source software fuzzing project, OSS-fuzz
20 http://www.microsoft.com/Springfield
6760 Book.indb 24 12/22/17 10:50 AM
1.3 Introduction to Fuzzing 25
Inspired by the research at the University of Wisconsin, and by syntax testing
explained by Boris Beizer, Oulu University Secure Programming Group (OUSPG)
launched the PROTOS project in 1999.21 The initial motivation for the work grew
out of frustration with the difficulties of traditional vulnerability coordination and
disclosure processes, which led the PROTOS team to think what could be done
to expedite the process of vulnerability discovery and transfer the majority of the
process back toward the software and equipment vendors from the security research
community. They came up with the idea of producing fuzzing test suites for vari-
ous interfaces and releasing them first to the vendors, and ultimately to the general
public after the vendors had been able to fix the problems. During the following
years, PROTOS produced free test suites for the following protocols: WAP-WSP,
WMLC, HTTP-reply, LDAP, SNMP, SIP, H.323, ISAKMP/IKE, and DNS. The
biggest impact occurred with the SNMP test suite, where over 200 vendors were
involved in the process of repairing their devices, some more that nine months
before the public disclosure. With this test suite the PROTOS researchers were able
to identify numerous critical flaws within the ASN.1 parsers of almost all available
SNMP implementations. This success really set the stage to alert the security com-
munity to this new way of testing called fuzzing.
Many notable open source fuzzers were released in early 2000, including Peach
and Scapy that are still widely used. Those fuzzers are still very dependent on the
input samples or grammar given provided by the user. EFS from Jared DeMott and
SAGE from Microsoft, presented in more detail at Chapter 7, were one the first
fuzzers that were able to runtime evolve their effectiveness during fuzzing. Since
then, many one-off fuzzers and technology proof-of-concepts have used advanced
techniques like evolutionary and genetic algorithms together with code coverage
data to overcome the weaknesses of traditional fuzzing. Major leap in usability of
advanced fuzzing tools occurred when Michal Zalewski released AFL, a security-
oriented fuzzer that leverages compile-time instrumentation and genetic algorithms
to discover test cases that trigger new code paths. Techniques used in AFL were not
actually new, but AFL was the first tool to combine those in to a simple to use tool
that could be used by people without in-depth technical understanding. Soon after
AFL, LLVM libFuzzer was released. LibFuzzer uses similar approach than AFL, but
it is more focused on performance and fuzzing of smaller software components like
libraries. Both AFL and libFuzzer are also explained in more detail at Chapter 7.
Before coverage guided fuzzers, continuous fuzzing in large scale was not very
practical. Traditional fuzzer would exhaust its pool of unique test cases eventually
and without manual adjustment the fuzzing would turn into regression testing with
a possibility of missing any new code introduced to the system. After effective cover-
age guided tools we introduced, a new generation of fuzzing services has emerged.
Services like Microsoft’s Project Springfield and Google’s OSS-fuzz use coverage
guided fuzzers together with cloud infrastructure scaling capabilities to execute a
large scale parallel fuzzing.
21 OUSPG has conducted research in the security space since 1996. www.ee.oulu.fi/research/ouspg.
6760 Book.indb 25 12/22/17 10:50 AM
26 Introduction
1.3.2 Fuzzing Overview
To begin, we would like to clearly define the type of testing we are discussing in
this book. This is somewhat difficult because no one group perfectly agrees on the
definitions related to fuzzing. The key concept of this book is that of black-box or
grey-box testing: delivering input to the software through different communication
interfaces with no or very little knowledge of the internal operations of the system
under test. Fuzzing is a black-box testing technique in which the system under test
is stressed with unexpected inputs and data structures through external interfaces.
Fuzzing is also all about negative testing, as opposed to feature testing (also
called conformance testing) or performance testing (also called load testing). In
negative testing, unexpected or semi-valid inputs or sequences of inputs are sent to
the tested interfaces, instead of the proper data expected by the processing code. The
purpose of fuzzing is to find security-related defects, or any critical flaws leading
to denial of service, degradation of service, or other undesired behavior. In short,
fuzzing or fuzz testing is a negative software testing method that feeds malformed
and unexpected input data to a program, device, or system.
Programs and frameworks that are used to create fuzz tests or perform fuzz test-
ing are commonly called fuzzers. During the last two decades, fuzzing has gradually
developed from a niche technique toward a full testing discipline with support from
both the security research and traditional QA testing communities.
Sometimes other terms are used to describe tests similar to fuzzing. Some of
these terms include
• Negative testing;
• Protocol mutation;
• Robustness testing;
• Syntax testing;
• Fault injection;
• Rainy-day testing;
• Dirty testing.
Traditionally, terms such as negative testing or robustness testing have been
used mainly by people involved with software development and QA testing, and
the word fuzzing was used in the software security field. There has always been
some overlap, and today both groups use both terms, although hackers tend to use
the testing related terminology a lot less frequently. Testing terms and requirements
in relation to fuzzing have always carried a notion of structure, determinism, and
repeatability. The constant flood of zero-day exploits has proved that traditional
functional testing is insufficient. Fuzzing was first born out of the more affordable,
and curious, world of randomness. Wild test cases tended to find bugs overlooked
in the traditional development and testing processes. This is because such randomly
chosen test data, or inputs, do not make any assumptions for the operation of the
software, for better or worse. Fuzzing has one goal, and one goal only: to crash the
system; to stimulate a multitude of inputs aimed to find any reliability or robust-
ness flaws in the software. For the security people, the secondary goal is to analyze
those found flaws for exploitability.
6760 Book.indb 26 12/22/17 10:50 AM
1.3 Introduction to Fuzzing 27
1.3.3 Vulnerabilities Found with Fuzzing
Vulnerabilities are created in various phases of the SDLC: specification, manufactur-
ing, and deployment (Figure 1.8). Issues created in the specification or design phase
are fundamental flaws that are very difficult to fix. Manufacturing defects are cre-
ated by bad practices and mistakes in implementing a product. Finally, deployment
flaws are caused by default settings and bad documentation on how the product
can be deployed securely.
Looking at these phases, and analyzing them from the experience gained with
known mistakes, we can see that implementation mistakes prevail. More than 70%
of modern security vulnerabilities are programming flaws, with only less than 10%
being configuration issues, and about 20% being design issues. Over 80% of com-
munication software implementations today are vulnerable to implementation-level
security flaws. For example, 25 out of 30 Bluetooth implementations crashed when
they were tested with Bluetooth fuzzing tools.22 Also, results from the PROTOS
research project indicate that over 80% of all tested products failed with fuzz tests
around WAP, VoIP, LDAP, and SNMP.23
Fuzzing tools used as part of the SDLC are proactive, which makes them the best
solution for finding zero-day flaws. Reactive tools such as security or vulnerability
scanners fail to do that, because they are based on knowledge of previously found
vulnerabilities. Reactive tools only test or protect widely used products from major
vendors, but fuzzers can test any product for similar problems. With fuzzing you
can test the security of any process, service, device, system, or network, no matter
what exact interfaces it supports.
Figure 1.8 Various phases in the SDLC in which vulnerabilities are introduced.
22 Ari Takanen and Sami Petäjäsoja, “Assuring the Robustness and Security of New Wireless Tech-
nologies.” Paper and presentation. ISSE 2007, Sept. 27, 2007. Warsaw, Poland.
23 PROTOS project. www.ee.oulu.fi/protos.
6760 Book.indb 27 12/22/17 10:50 AM
28 Introduction
1.3.4 Fuzzer Types
Fuzzers can be categorized based on two different criteria:
1. Injection vector or attack vector.
2. Test case complexity.
Fuzzers can be divided based on the application area where they can be used,
basically according to the attack vectors that they support. Different fuzzers target
different injection vectors, although some fuzzers are more or less general-purpose
frameworks. Fuzzing is a black-box testing technique, but there are several doors
into each black box (Figure 1.9). Note also that some fuzzers are meant for client-
side testing, and others for server-side testing. A client-side test for HTTP or TLS
will target browser software; similarly, server-side tests may test a Web server. Some
fuzzers support testing both servers and clients, or even middleboxes that simply
proxy, forward, or analyze passing protocol traffic.
Fuzzers can also be categorized based on test case complexity. The tests gener-
ated in fuzzing can target various layers in the target software, and different test
cases penetrate different layers in the application logic (Figure 1.10). Fuzzers that
change various values in protocol fields will test for flaws like overflows and integer
problems. When the message structure is anomalized, the fuzzer will find flaws in
message parses (e.g., XML and ASN.1). Finally, when message sequences are fuzzed,
the actual state machine can be deadlocked or crashed. Software has separate lay-
ers for decoding, syntax validation, and semantic validation (correctness of field
Figure 1.9 Attack vectors at multiple system levels.
6760 Book.indb 28 12/22/17 10:50 AM
1.3 Introduction to Fuzzing 29
Figure 1.10 Different types of anomalies and different resulting failure modes.
values, state of receiver) and for performing the required state updates and output
generation (Figure 1.11). A random test will only scratch the surface, whereas a
highly complex protocol model that not only tests the message structures but also
message sequences will be able to test deeper into the application.
Figure 1.11 Effectivity of a test case to penetrate the application logic.
6760 Book.indb 29 12/22/17 10:50 AM
30 Introduction
One example method of categorization is based on the test case complexity in
a fuzzer:
• Static and random template-based fuzzer: These fuzzers typically only test
simple request-response protocols, or file formats. There is no dynamic func-
tionality involved. Protocol awareness is close to zero.
• Block-based fuzzers: These fuzzers will implement basic structure for a simple
request-response protocol and can contain some rudimentary dynamic func-
tionality such as calculation of checksums and length values.
• Dynamic generation or evolution based fuzzers: These fuzzers do not neces-
sarily understand the protocol or file format that is being fuzzed, but they
will learn it based on a feedback loop from the target system. They might or
might not break the message sequences.
• Model-based or simulation-based fuzzers: These fuzzers implement the tested
interface either through a model or a simulation, or they can also be full
implementations of a protocol. Not only message structures are fuzzed, but
also unexpected messages in sequences can be generated.
The effectiveness of fuzzing is based on how well it covers the input space of the
tested interface (input space coverage) and how good the representative malicious and
malformed inputs are for testing each element or structure within the tested inter-
face definition (quality of test data). Fuzzers that supply totally random characters
may yield some fruit but, in general, won’t find many bugs. It is generally accepted
that fuzzers that generate their inputs with random data are very inefficient and
can only find rather naive programming errors. As such, it is necessary for fuzzers
to become more complex if they hope to uncover such buried or hard to find bugs.
Very obscure bugs have been called second-generation bugs. They might involve,
for example, multipath vulnerabilities such as noninitialized stack or heap bugs.
Another dimension for categorizing fuzzers stems from whether they are model-
based. Compared with a static, nonstateful fuzzer that may not be able to simu-
late any protocol deeper than an initial packet, a fully model-based fuzzer is able
to test an interface more completely and thoroughly, usually proving much more
effective in discovering flaws in practice. A more simplistic fuzzer is unable to test
any interface very thoroughly, providing only limited test results and poor cover-
age. Static fuzzers may not be able to modify their outputs during runtime, and
therefore lack the ability to perform even rudimentary protocol operations such
as length or checksum calculations, cryptographic operations, copying structures
from incoming messages into outgoing traffic, or adapting to the exact capabilities
(protocol extensions, used profiles) of a particular system under test. In contrast,
model-based fuzzers can emulate a protocol or file format interface almost com-
pletely, allowing them to understand the inner workings of the tested interface and
perform any runtime calculations and other dynamic behaviors that are needed to
achieve full interoperability with the tested system. For this reason, tests executed
by a fully model-based fuzzer are usually able to penetrate much deeper within
the system under test, exercising the packet parsing and input handling routines
extremely thoroughly, and reaching all the way into the state machine and even
output generation routines, hence uncovering more vulnerabilities.
6760 Book.indb 30 12/22/17 10:50 AM
1.3 Introduction to Fuzzing 31
1.3.5 Logical Structure of a Fuzzer
Modern fuzzers do not just focus solely on test generation. Fuzzers contain dif-
ferent functionalities and features that will help in both test automation and in
failure identification. The typical structure of a fuzzer can contain the following
functionalities (Figure 1.12).
• Protocol modeler: For enabling the functionality related to various data for-
mats and message sequences. The simplest models are based on message tem-
plates, whereas more complex models may use context-free protocol grammars
or proprietary description languages to specify the tested interface and add
dynamic behavior to the model.
• Anomaly library: Most fuzzers include collections of inputs known to trigger
vulnerabilities in software, whereas others just use random data.
• Attack simulation engine: Uses a library of attacks or anomalies, or learns
from one. The anomalies collected into the tool, or random modifications,
are applied to the model to generate the actual fuzz tests.
• Runtime analysis engine: Monitors the SUT. Various techniques can be
used to interact with the SUT and to instrument and control the target and
its environment.
• Reporting: The test results need to be prepared in a format that will help the
reporting of the found issues to developers or even third parties. Some tools do
not do any reporting, whereas others include complex bug reporting engines.
• Documentation: A tool without user documentation is difficult to use. Espe-
cially in QA, there can also be requirement for test case documentation. Test
Figure 1.12 Generic structure of a fuzzer.
6760 Book.indb 31 12/22/17 10:50 AM
32 Introduction
case documentation can sometimes be used in reporting and can be dynami-
cally created instead of a static document.
1.3.6 Fuzzing process
A simplified process of conducting a fuzz test consists of sequences of messages
(requests, responses, or files) being sent to the SUT. The resulting changes and
incoming messages can be analyzed, or in some cases they can be completely ignored
(Figure 1.13). Typical results of a fuzz test contain the following responses:
• Valid response.
• Error response (may still be valid from a protocol standpoint).
• Anomalous response (unexpected but nonfatal reaction, such as slowdown
or responding with a corrupted message).
• Crash or other failure.
The process of fuzzing is not only about sending and receiving messages. Tests
are first generated and sent to the SUT. Monitoring of the target should be constant
and all failures should be caught and recorded for future evaluation. A critical part
Figure 1.13 Example fuzz test cases and resulting responses from an SUT.
6760 Book.indb 32 12/22/17 10:50 AM
1.3 Introduction to Fuzzing 33
of the fuzzing process is to monitor the executing code as it processes the unex-
pected input (Figure 1.14). Finally, a pass-fail criteria needs to be defined with the
ultimate goal being to perceive errors as they occur and store all knowledge for later
inspection. If all this can be automated, a fuzzer can have an infinite amount of
tests, and only the actual failure events need to be stored, and analyzed manually.
If failures were detected, the reason for the failure is often analyzed manu-
ally. That can require a thorough knowledge of the system and the capability to
debug the SUT using low-level debuggers. If the bug causing the failure appears to
be security-related, a vulnerability can be proved by means of an exploit. This is
not always necessary, if the tester understands the failure mode and can forecast
the probability and level of exploitability. No matter which post-fuzzing option is
taken, the deduction from failure to an individual defect, fixing the flaw, or poten-
tial exploit development task can often be equally expensive in terms of man-hours.
1.3.7 Fuzzing Frameworks and Test Suites
As discussed above, fuzzers can have varying levels of protocol knowledge. Going
beyond this idea, some fuzzers are implemented as fuzzing frameworks, which means
that they provide an end user with a platform for creating fuzz tests for arbitrary
protocols. Fuzzer frameworks typically require a considerable investment of time
and resources to model tests for a new interface, and if the framework does not
offer ready-made inputs for common structures and elements, efficient testing also
requires considerable expertise in designing inputs that are able to trigger faults in
the tested interface. Some fuzzing frameworks integrate user-contributed test mod-
ules back to the platform, bringing new tests within the reach of other users, but
Figure 1.14 Fuzzing process consisting of test case generation and system monitoring.
6760 Book.indb 33 12/22/17 10:50 AM
34 Introduction
for the most part fuzzing frameworks require new tests to always be implemented
from scratch. These factors can limit the accessibility, usability, and applicability
of fuzzing frameworks.
1.3.8 Fuzzing and the Enterprise
Not all software developer organizations and device manufacturers use any type of