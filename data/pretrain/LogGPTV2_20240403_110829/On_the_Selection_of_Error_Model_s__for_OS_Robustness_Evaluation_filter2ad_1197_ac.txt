Service
SERIAL OPEN
CreateThread
DisableThreadLibraryCalls
FreeLibrary
InitializeCriticalSection
LoadLibraryW
LocalAlloc
MapPtrToProcess
memcpy
memset
MmMapIoSpace
NDISInitializeWrapper
NDISMSetAttributesEx
NDISMSynchronizeWithInterrupt
QueryPerformanceCounter
SetProcPermissions
wcscpy
wcslen
x
1
BF DT FZ
x
1
4
x
x
6
1
4
1
2
4
1
3
3
9
x
1
1
x
32
29
26
2
7
2
2
2
77
74
11
1
4
7
2
1
6
11
Table 7. Services identiﬁed by class 3 fail-
ures. “x” indicates class 2 service failures.
Figure 3. Stability of Diffusion for the FZ
model wrt. the number of injections.
7. Interpretation & Discussion
identifying vulnerable services, a key aspect for robustness
enhancing efforts, such as using wrappers. In order to in-
crease the identiﬁcation coverage for the DT and FZ models
we have indicated in the table which services exhibit class
2 failures, which increases the coverage slightly. Still there
are four services identiﬁed only by BF. The DT model per-
forms slightly better than FZ, but in two cases FZ identiﬁes
a service which DT does not. It is also important to note
that one service is only found by FZ.
Error Models & Error Severity: The ﬁrst major ﬁnd-
ing is that the BF model causes more severe failures than the
other models. Table 6 shows that BF ﬁnds by far the most
class 3 failures of all error models. However, the number
of injections used is also high, which comes with a cost in
terms of execution time (Table 5). Therefore, when time is
crucial, other error models may be considered. In terms of
number of injections and execution time the DT error model
performs the best, with FZ in the middle.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:38 UTC from IEEE Xplore.  Restrictions apply. 
0.21234567891011121314150.40.60.81.21.01.41.61.82.0DiffusionNumber of injections91111Ccerfio_serialatadisk37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007FZ, on the other hand,
Comparing BF & FZ: The second major ﬁnding
found in Table 7 is that BF identiﬁes more services hav-
ing class 3 failures than any of the other two mod-
els.
identiﬁes one service
(InitializeCriticalSection) that none of the
other two identiﬁed. So the question is: why are some of
the services not identiﬁed by FZ? FZ chooses a new random
value to be used in an injection, whereas the BF and DT er-
ror models modify the existing value. For services that have
some basic level of checking of incoming parameters values
which are well off the expected, as a random value is likely
to be, are easy to ﬁnd. However, values that are close to the
expected value (as when only one bit is changed) are more
difﬁcult to ﬁnd, and may therefore slip through and cause
a failure of the system. This happens for instance when
targeting different types of handles (to modules, libraries,
memory areas etc.) and when targeting typical control val-
ues, such as bit-mask ﬂags. Interestingly, FZ is traditionally
regarded as not being very effective. However, our results
are displaying its surprising effectiveness based on the num-
ber of injections used, especially when the intent is to iden-
tify drivers using the Diffusion metric.
Model Choice: Table 4 shows a difference in the re-
sult between the models. BF identiﬁes atadisk as the
most vulnerable driver and the other models identify the se-
rial driver as most vulnerable. As described in Section 3
there are signiﬁcant differences between the models. Ulti-
mately, the choice of error model is inﬂuenced by many fac-
tors. Section 8 discusses some of the trade-offs that must be
made, with respect to time, implementation complexity and
more importantly the goals of the evaluation.
Implementation Complexity/Cost: Table 5 shows that
BF and FZ are clearly more expensive in terms of execution
time compared to DT. However, a major drawback with the
DT error model is the cost for implementation. Since for ev-
ery service in the interface the type of each parameter needs
to be kept, it requires implementing support for this. BF and
FZ on the other hand do not have this requirement, making
their implementation considerably cheaper. The higher cost
for the DT model could potentially be reduced by use of
automatic parsing tools and/or reﬂection-capable program-
ming languages. As this cost is a one-time cost for each
driver, the cost might be acceptable if the experiments are
to be repeated in a regression testing fashion.
Experiment Time: A factor inﬂuencing the experiment
time is the degree of operator involvement. The operator is
required to specify which experiment to run and for which
driver. The time to do this is the same for all models. Some
experiments force the system into a state where it cannot
itself reboot, requiring the operator to manually reboot the
system. 21.3% of the class 3 failures result in the system
being left in a state where it cannot itself reboot. A conse-
quence is that without external reboot mechanisms the ex-
periment is delayed until the operator takes action, which
can prolong the execution time substantially. We have not
included this time in the total execution time for the exper-
iments, since we cannot make any assumption on the pres-
ence of the operator. Each manual reboot is given a generic
penalty of 200 seconds which is the timeout used to detect
a hung system which automatically restarts.
Class 2/3 & Bugs: A question one might ask after seeing
these results is whether the fact that class 2 and 3 failures
are observed indicate that the system contains bugs? The
answer is: not necessarily. It has until now been common
practice to use a “gentlemen’s agreement” between the OS
and the drivers. This is mostly due to the fact that the costs
of checking each and every call to the kernel would be too
high for most systems. So the fact that the system crashes
might not be due to a bug in the traditional sense. It is how-
ever from a robustness point of view a “vulnerability”. All
targeted drivers are deployed drivers, i.e., their producer has
tested them to some extent and they do work well in our sys-
tem when no errors are injected.
8. Developing the Composite Error Model
The results from Section 6 provides two major ﬁndings:
a) BF pinpoints the most services for class 3 failures and
b) FZ gives similar Diffusion results to BF at markedly a
lower cost, but does not ﬁnd as many services. This sec-
tion explores these differences and use them to combine the
two error models into a composite error model (CM) that
identiﬁes as many vulnerable services as BF but with fewer
injections. We focus on the class 3 failures, as these are
of highest interest when conducting robustness evaluation.
The composite model combines BF & FZ by not utilizing
the full bit space of the BF model. Thus a key step is to
identify the subset of the BF model bits to combine with
the FZ model. The following two subsections establish this
basis to result in the selected composite model (CM).
8.1. CM Setup: Bit Failure Distribution
The relative inefﬁciency of BF in terms of execution time
is a result of the number of injections. As noted in Section
3.1 the 32 bits available for ﬂipping are not used uniformly.
Figure 4 indicates that there are more services only sensitive
to ﬂips in the least signiﬁcant bits than in the most signif-
icant. The bits below 10 (to the right in the ﬁgure) clearly
cause failures in more services. Figure 5 shows the cumu-
lative number of services identiﬁed, starting at bit 0 (from
right to left). The ﬁgure shows that after bit 9 only bit 31
identiﬁes a service not previously identiﬁed. Thus, the ser-
vices having (class 3) failures for bits 10-31 also have fail-
ures for bits 0-9 (with InitializeCriticalSection
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:38 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007being the only exception for bit 31). Thus, focus of the in-
jections should be put on these bits.
Figure 4. The number of services identiﬁed
by Class 3 failures by the BF model.
Figure 6. Failure mode distribution for CM
compared to BF, DT and FZ.
Driver
cerﬁo serial
91C111
atadisk
Diffusion
2.9
2.0
1.9
Figure 7. Diffusion and comparison of the
number of injections with combined BF & FZ.
32-bit values, is also less likely to produce a valid pointer.
However, since the value is random, it can also trigger fail-
ures not found by more structured injections (which is lim-
ited to changes to existing values or a small subset of special
values), as shown by the fact that only FZ identiﬁes a class
3 failure for service InitializeCriticalSection.
8.3. Composite Model & Eﬀectiveness
The results in the previous sections have established the
need for using multiple error models. Therefore, we rec-
ommend using both BF and FZ when resources are plenti-
ful. When not, we propose to use a composite model where
the least signiﬁcant bits (together with the most signiﬁcant
one) are targeted with BF, alongside a series of FZ experi-
ments. Section 6.4 established that ten FZ injections are suf-
ﬁcient for stabilizing the Diffusion metric. It is reasonable
that more injections will increase the probability of ﬁnding
“rare” cases (such as InitializeCriticalSection)
but at the cost of increased number of injections.
A composite model, using only bits 0-9 and 31, together
with ten FZ injections, identiﬁes the same set of class 3
vulnerable services as the full set of BF and FZ injections.
An overview of the results is shown in Figure 7. The ta-
ble presents the Diffusion results for the composite model
and it also shows how the composite model saves injection
cases compared to performing all BF and ﬁfteen FZ cases.
Figure 5. Moving from bit 0 and upwards the
number of services increases until bit 10.
The observations made in Figures 4 and 5 allow us to
modify the BF model to use fewer injections (only bits 0-9
and 31). Using only these bits reduces the number of test
cases for BF by 62% (to 2164 in total), while still identi-
fying all services. It is important to note that these results
are system speciﬁc and the result of our experimental setup.
Further research on methods for extracting such proﬁles at
minimum cost is needed.
8.2. Distinguishing Control vs Data
A study of the parameters targeted for the services identi-
ﬁed by BF, but not by FZ, reveals a prevailing trend: the pa-
rameters used are all control values, like pointers to data or
handles to ﬁles, modules, functions etc. It is reasonable that
these parameters are more sensitive to changes in the least
signiﬁcant bits (LSB) than to changes in the most signiﬁ-
cant bits (MSB). E.g., for a pointer that points to data within
the process’ memory region changes in the LSB will yield
a new pointer within the region (but to possibly non-valid
data) whereas changes of the MSB will yield a non-valid
pointer which is easily detectable on modern hardware.
Flipping a bit in the MSB is more likely to yield a non-valid
pointer than changes to the LSB, and consequently we see
a difference in the failure distribution. FZ, using random
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:38 UTC from IEEE Xplore.  Restrictions apply. 
0246810024681012141618202224262830Bit PositionNr of services identified024681012141618024681012141618202224262830Nr of services identifiedBit position0%20%40%60%80%100%BFDTFZCMBFDTFZCMBFDTFZCMClass 3Class 2Class 1No failure% of all injectionsatadisk91C111cerfio_serial50015002500CompositeAll BF & FZ3500cerfio_serial91C111atadiskNr of injections37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007The Diffusion results are very similar to those presented for
the individual error models in Table 4, identifying the serial
driver as being the most vulnerable one, followed by the
91C111 and atadisk.
Figure 6 shows the results of CM alongside the other
models, and it clearly show a similar trend as for the original
error models, with a signiﬁcant portion of the experiments
ending up in the NF class. The number of injection cases is
in the same range as those for BF, but higher than that for FZ
with ﬁfteen cases. Compared to performing both (BF & FZ)
it corresponds to performing only 48.7 % of the injections,
a signiﬁcant reduction. Many other factors inﬂuences the
actual execution time of the experiments. Assuming the ex-
ecution time being proportional to the number of injections
the CM gives a saving of up to 60 hours experimentation
time for the combined BF & FZ.
9. Conclusions
This paper reports on extensive fault injection experi-
ments carried out for three commonly used error models:
bit-ﬂip, data type and fuzzing. The results show bit-ﬂips
as the most acute one, but with the highest implementa-
tion cost. Based on these ﬁndings a new composite error
model has been deﬁned that compared to extensive bit-ﬂip
and fuzzing experiments achieves a) comparable error prop-
agation results, and b) identiﬁes the same set of vulnerable
services. This is achieved using less than half the number
of injections.
As this paper reports on experimental techniques the re-
sults must be viewed in this speciﬁc context, but we believe
that there are some general guidelines that can be applied in
the selection of the error model, namely:
• When comparing drivers on their potential to spread of
errors, or evaluating the robustness of the OS to driver
errors all three error models (and the composite) sufﬁce
to give guidance using the Diffusion metric. The ex-
periments also validate the effectiveness of the Diffusion
metric as an initial guideline.
• Data type errors come with a higher implementation cost,
whereas bit-ﬂips have a higher execution cost. If imple-
mentation cost (time) is a critical factor then bit-ﬂips or
fuzzing are recommended. Fuzzing gives similar Dif-
fusion results as bit-ﬂips with fewer injections. Thus
making it the appropriate model to use when comparing
drivers using Diffusion.
• When identifying services that may have serious failures,
bit-ﬂips is the most efﬁcient error model followed by data
type. However, fuzzing, being random in nature, may
ﬁnd cases where other models do not.
• A new composite error model, consisting of selective
bit-ﬂips with a series of fuzzing injections gives accu-
rate results at a moderate execution/setup cost, compared
to performing extensive bit-ﬂip campaigns together with
fuzzing injections.
References
[1] A. Albinet, et. al. Characterization of the Impact of Faulty
Drivers on the Robustness of the Linux Kernel. Proc. of
DSN, pp. 807–816, 2004.
[2] J. Dur˜aes and H. Madeira. Multidimensional Characteriza-
tion of the Impact of Faulty Drivers on the Operating System
Behavior. IEICE Trans., E86-D(12):2563–2570, Dec. 2003.
[3] J. Arlat, et. al. Dependability of COTS Microkernel-Based
Systems. IEEE TOC, 51(2):138–163, Feb. 2002.
[4] T. Ball et. al. Thorough Static Analysis of Device Drivers.
Proc. of EuroSys, pp. 73-85, 2006.
[5] R. Chillarege, et. al. Orthogonal Defect classiﬁcation-a Con-
cept for In-Process Measurements. IEEE TSE, , 18(11):943–
956, 1992.
[6] J. Christmansson and R. Chillarege. Generation of an Error
set that Emulates Software Faults Based on Field Data. Proc.
of FTCS, pp. 304 – 313, 1996.
[7] C. Fetzer and Z. Xiao. An Automated Approach to Increas-
ing the Robustness of C-Libraries. Proc. of DSN, pp. 155-
164, 2002.
[8] W. Gu, et. al. Error Sensitivity of the Linux Kernel Exe-
cuting on PowerPC G4 and Pentium 4 Processors. Proc. of
DSN, pp. 887–896, 2004.
[9] W. Gu, et. al. Characterization of Linux Kernel Behavior
Under Errors. Proc. of DSN, pp. 459 – 468, 2003.
[10] M. Howard and S. Lipner. The Security Development Life-
cycle. Microsoft Press, 2006.
[11] T. Jarboui, et. al. Analysis of the Effects of Real and Injected
Software Faults: Linux as a Case Study. Proc. of PRDC, pp.
51 – 58, 2002.
[12] T. Jarboui, et. al. Experimental Analysis of the Errors In-
duced into Linux by Three Fault Injection Techniques. Proc.
of DSN, pp. 331– 336, 2002.
[13] A. Johansson and N. Suri. Error Propagation in Operating
Systems. Proc. of DSN, pp. 86–95, 2005.
[14] K. Kanoun, et. al. Benchmarking the Dependability of Win-
dows and Linux using PostMark Workloads. Proc. of ISSRE,
pp. 11–20, 2005.
[15] P. Koopman and J. DeVale. Comparing the Robustness of
POSIX Operating Systems. Proc. of FTCS, pp. 30–37, 1999.
[16] E. Marsden and J.-C. Fabre. Failure Mode Analysis of
CORBA Service Implementations. Proc. of Middleware, pp.
216–231, 2001
[17] B. P. Miller, et. al. An Empirical Study of the Reliability of
Unix Utilities. CACM, 33(12):32–44, Dec. 1990.
[18] R. Moraes, et. al. Injection of Faults at component interfaces
and inside the component code: are they equivalent? Proc.
of EDCC, pp. 53–64, 2006.
[19] P. Oehlert. Violating Assumptions with Fuzzing. IEEE Se-
curity & Privacy Magazine, 3(2):58–62, 2005.
[20] M. M. Swift, et. al. Improving the Reliability of Commodity
Operating Systems. Proc. of SOSP, pp. 207–222, 2003.
[21] T. Tsai and N. Singh. Reliability Testing of Applications on
Windows NT. Proc. of DSN, pp. 427–436, 2000.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:38 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007