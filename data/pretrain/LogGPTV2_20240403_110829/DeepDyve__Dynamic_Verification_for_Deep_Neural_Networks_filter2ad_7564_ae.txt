âˆ’ 0.007
ğ›¼ + 1.0 and the calculated optimal point is ğ›¼ = 0.15, which is
also compatible with the red curve. Therefore, the initial checker
DNN architecture can be efficiently found by the proposed method.
6.3 Effectiveness of Task Exploration
In this part, we first evaluate the weighted coverage ğ‘Š ğ‘ğ‘œğ‘£. and
overhead ğ‘‚(ğ¶) under various simplified classes ğ‘˜. Then, we study
how the risk impact matrix affects task simplification.
First, we observe that task simplification can significantly reduce
the overhead of DeepDyve with little coverage degradation. In
Figure 7, we use the checker DNN design obtained from Section 6.2
and trace the weighted coverage ğ‘Š ğ‘ğ‘œğ‘£. and overhead ğ‘‚(ğ¶) during
the agglomerative class clustering process. Table 4 shows the final
task simplification results. Through task simplification, we can save
the overhead by (9.18% âˆ’ 6.88%)/9.18% = 25.05%, 27.61%, 12.60%,
3.38% for CIFAR-10, GTSRB, CIFAR-100, and Tiny-ImageNet, at the
cost of 0.9%, 0.2%, 0.5%, 1.6% coverage degradation, respectively.
We can also observe that fault coverage and overhead vary a
lot under different impact matrix configurations. First, the initial
weighted coverage values before task simplification is different
for uniform and non-uniform settings, because the ğ¼ğ‘– ğ‘— term in the
definition of weighed coverage given by Equation 4 varies. Sec-
ond, we observe much more overhead savings can be achieved in
the non-uniform case. For example, the overhead saving can be
improved from 0.76% to 25.05% on CIFAR-10 when changing to
non-uniform impact matrix. This is because, the impact of classes
with low precision is set as lower values under such circumstances,
which provides more opportunities for task simplification. In other
words, a reasonable impact matrix is beneficial for protection with
DeepDyve and hence is highly recommended.
6.4 DeepDyve vs. Threshold Checking
In this section, we compare the performance of DeepDyve with
the Threshold Checking scheme proposed in [23]. We experiment
under both random fault attack and BitFlip Attack (BFA) settings.
We show the results in Table 5, including the false-positive rate
(FPR), false-negative rate (FNR), computational overhead (O(C)),
and the weighted coverage (Wcov.).
First, we observe DeepDyve significantly outperforms Threshold
Checking in terms of Wcov., which in turn leads to smaller FNR. As
discussed in Section 2, most intermediate activation values locate
in the normal range even under fault attack, especially for the
quantized DNN case. Hence, most faults are missed with Threshold
Checking, but they can be detected by DeepDyve.
Second, in most cases, we can observe both DeepDyve and
Threshold Checking performs better under BFA compared to ran-
dom fault attacks. We manually check the internal values of DNN
under fault attack and find that the magnitude of value change is
larger in BFA than in random fault attacks, thereby making fault
detection easier.
(c) CIFAR-100alpha(a) CIFAR-10alpha(d) Tiny-ImageNetalpha(b) GTSRBFlopOverheadConsistency0.000.050.100.150.200.250.300.350.400.500.550.600.650.700.750.80f()=0.007+0.79,optimal=0.150.300.350.400.450.500.00.10.20.30.40.650.700.750.800.850.900.95f()=0.003+0.94,optimal=0.110.100.150.200.250.30optimal point0.300.350.400.450.500.550.600.650.700.680.700.730.750.780.800.830.85f()=0.078+0.96,optimal=0.340.350.400.450.500.550.600.65optioptimal point0.10.20.30.40.50.60.800.850.900.951.00f()=0.007+1.0,optimal=0.150.050.100.150.200.250.300.35alphaoptimal pointoptimal pointoptimal pointFigure 7: Tracing of the fault coverage and overhead change in task simplification process.
Table 5: Comparison between DeepDyve and threshold checking.
Dataset
CIFAR-10
GTSRB
CIFAR-100
Tiny-ImageNet
Random Fault Attack
BFA
Impact Matrix
Threshold Checking
DeepDyve
Threshold Checking
DeepDyve
FPR
uniform
FNR
non-uniform 0.04% 96.24%
0.04% 90.87%
non-uniform 0.00% 99.72%
0.00% 96.85%
non-uniform 0.02% 65.11%
0.02% 86.99%
non-uniform 0.02% 69.31%
0.06% 87.34%
uniform
uniform
uniform
FPR
FNR
0.00% 43.29%
0.00% 24.06%
0.00% 16.53%
0.00%
5.07%
O(C) Wcov.
6.88%
9.11%
1.94%
2.56%
FNR
FPR
O(C) Wcov.
33.45%
82.98% 0.04%
4.10%
75.94% 0.04%
55.52%
9.12%
95.49% 0.00% 100.00%
0.3%
99.88%
94.93% 0.00%
3.15%
14.50%
33.48% 0.00% 31.51% 24.02% 74.67% 0.02%
13.01% 0.00% 19.21% 27.42% 80.79% 0.02%
1.05%
0.00%
30.56% 0.00% 19.46% 35.19% 82.00% 0.02%
12.65% 0.00% 17.02% 36.84% 82.98% 0.06%
0.08%
-
-
-
-
-
-
-
-
O(C) Wcov.
FPR
FNR
O(C) Wcov.
6.88%
0.00% 1.01%
98.93%
66.39%
9.11%
0.00% 2.15%
97.85%
44.48%
1.94%
0.00% 0.90%
99.75%
0.00%
0.00% 0.79%
2.56%
99.21%
0.12%
0.00% 1.84% 24.02% 99.50%
86.61%
0.00% 0.37% 27.42% 99.63%
98.95%
100.00% 0.00% 0.05% 35.19% 99.94%
0.00% 0.02% 36.84% 99.98%
99.92%
-
-
-
-
-
-
-
-
Third, the FPR of Threshold Checking is above zero while that
of our DeepDyve system is zero. For example, the FPR of Threshold
Checking for CIFAR-10 dataset is 0.04%. As discussed in Section 2,
threshold Detection sets the thresholds as 1.1 times the maximum
and a minimum of each layerâ€™s normal activation values on the
training set. On the testing set, there are few exceptions where the
activation values are beyond this range. In contrast, in the normal
execution of DeepDyve, the comparatorâ€™s false positives, which are
caused by inconsistencies between task and checker models, are
subject to re-computation, and hence, the systemâ€™s false positives
are guaranteed to be zero.
6.5 Impact of Model Accuracy
Previous experiments suggest that the overhead of DeepDyve for
CIFAR-100 and Tiny-ImageNet dataset are quite high, even after
task simplification. This is because the per-class accuracy of the
task model on these two datasets varies and some of them are very
low, as shown in Figure 8. In safety-critical applications, a class is
deserved to be protected only when its accuracy is high enough.
Considering the above, we conduct a case study on CIFAR-100 and
let the impact of 75% of the classes with the lowest precision to be
zero.
Previously, the computational overhead induced by the checker
DNN before task simplification was 27.48% (See Table 4). With the
above setting, as we only care classes with non-zero impact, we
let the comparator only check the inconsistencies of these classes.
Given this, the overhead induced by the checker DNN before task
simplification is 16.17%. Task simplification can further reduce the
overhead from 16.17% to 9.88% without loss of weighted coverage
(38.89% overhead savings). Also, the simplified model can reach
Figure 8: Per-class accuracy for CIFAR-100.
90.66% weighted coverage under random fault attack and 99.78%
weighted coverage under BFA attack.
7 DISCUSSIONS
In this section, we discuss the robustness of the proposed DeepDyve
architecture and its limitations.
7.1 Robustness of DeepDyve
What if the checker DNN has faults? If the checker DNN is
faulty while the task DNN is correct, the final inference accuracy
would remain the same, because the system would accept the output
from the task model anyway. There could be extra latency. To
mitigate this issue, we could leverage various hardening techniques
(e.g., secure enclaves) to protect it at a reasonable cost since the
checker DNN is much smaller than the task DNN model.
number of classes (k)starting consistency = 91.62% wcoverage (%)overhead (%)wcoveragesnumber of classes (k)23456789105.006.007.008.009.0070.0072.5075.0077.5080.0082.5085.0087.50wcoverages0102030401.802.002.202.402.6060.0065.0070.0075.0080.0085.0090.0095.00100.0002040608010012.0014.0016.0018.0020.0022.0024.0026.0028.0035.0040.0045.0050.0055.0060.0065.00wcoverages025507510012515017520024.0026.0028.0030.0032.0034.0036.0050.0055.0060.0065.0070.0075.0080.00wcoveragesnumber of classes (k)number of classes (k)starting consistency = 98.75% starting consistency = 75.82 % starting consistency = 79.19%   (a) CIFAR-10(b) GTSRB(c) CIFAR-100(d) Tiny-ImageNet0204060801000.00.20.40.60.81.0class indexAccuracythresholdAttack on DeepDyve. Attackers need to create consistent faulty
outputs to bypass the comparison logic of DeepDyve to successful
launch their attacks.
One way to achieve this is to inject faults into the task and
checker models simultaneously. However, the cost of launching
such an attack in practice is very high, if not impossible. On the
one hand, simultaneously injecting faults at two specific positions
is difficult. For example, row-hammer attack relies on the weakness
of physical memory row, and it cannot be fully controlled. On
the other hand, if low-precision fault injection technique is used to
inject random faults into the two DNNs, the probability distribution
of the faulty output of a DNN is given by its risk probability matrix,
and the probability of two DNNsâ€™ outputs happens to be the same
is given by
ğ‘âˆ‘ï¸
ğ‘ƒğ‘ğ‘œğ‘™ğ‘™ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =
ğ‘ğ‘– âˆ— ğ‘ğ‘–,
(17)
ğ‘–
where ğ‘ğ‘– and ğ‘ğ‘– are the probabilities of task DNN and checker DNN
generating the same output ğ‘–, respectively, and ğ‘ is the number of
classes. Obviously, this value decreases with the increase of classes.
The ğ‘ƒğ‘ğ‘œğ‘™ğ‘™ğ‘–ğ‘ ğ‘–ğ‘œğ‘› values are 8.9%, 2.77%, 0.87%, and 0.49% for CIFAR-10,
GTSRB, CIFAR-100, and Tiny-ImageNet, respectively. Given that
the probability of DNNs generating wrong outputs under random
faults ğ‘ƒğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ is extremely low. The possibility for such attack to
succeed is negligible.
Another way to successfully launch fault injection attacks is
to target those inconsistent cases and make them consistent with
faulty result. To achieve this objective, however, attackers need to
be able to tell whether an incoming data is consistent at runtime
and perform fault injection before its inference is finished. This is
a daunting objective to achieve, especially considering the usual
long preparation time for fault injection.
7.2 Limitations and Future Work