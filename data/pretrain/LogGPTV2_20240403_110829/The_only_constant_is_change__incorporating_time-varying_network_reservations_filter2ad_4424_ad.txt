 800
 600
 400
 200
 0
 0  100 200 300 400 500 600 700
 0  100 200 300 400 500 600 700
 0  100 200 300 400 500 600 700
 0  100 200 300 400 500 600 700
Time (sec)
(a) On VM 1.
Time (sec)
(b) On VM 16.
Time (sec)
(c) On VM 32.
Time (sec)
(d) Envelop model.
Figure 8: Bandwidth proﬁle and generated Type 4 TIVC models of Hive Join.
App.
Sort
Hive Join
Hive Aggre.
Type
1
4
4
Table 2: TIVC models generated for the applications.
TIVC
Efﬁciency
22.8%
17.6%
10.7%
5. THE PROTEUS SYSTEM
To demonstrate the effectiveness of TIVC models, we have de-
veloped a cloud sharing system called PROTEUS that implements
the TIVC models.
5.1 Overview
The goal of PROTEUS is to allow cloud customers to obtain pre-
dictable performance and cost guarantees for their applications.
This is achieved via three steps, as outlined in Figure 9.
In the
ﬁrst step, the customer’s application is proﬁled under different con-
ﬁgurations, i.e., of input data size per VM and bandwidth cap, and
TIVC models are generated for the proﬁling runs using the tech-
niques presented in §3. We note the proﬁling overhead can be dras-
tically reduced when customers repeatedly run the same type of
jobs with the same input size (see §4.5).
Each proﬁling run under a conﬁguration in the ﬁrst step results
in a TIVC model and a service completion time.
In the second
step, the charging model published by the cloud provider is used
to estimate the cost for the candidate TIVC models under different
conﬁgurations. The customer can then pick whichever conﬁgura-
tion that best suits her performance/cost objective.
In the ﬁnal step, given a TIVC job conﬁguration that the cus-
tomer picks, the cloud provider runs a spatial-temporal TIVC al-
location algorithm to place the job in the physical datacenter in a
way that maximizes the utilization and hence the revenue of the
cloud datacenter, and conﬁgures the datacenter network to enforce
the requested time-varying bandwidth speciﬁed in the TIVC speci-
ﬁcation. We describe the details of these two components next.
5.2 Spatial-Temporal Allocation
The job manager implements the TIVC allocation algorithm to
allocate VM slots on the physical machines in an online fashion. It
achieves this by maintaining up-to-date information of (1) the dat-
acenter network topology; (2) the empty VM slots in each physical
machine; and (3) the residual bandwidth for each link, calculated
from tallying the TIVC allocations of currently running jobs. We
focus on tree-like topologies such as multi-rooted tree topologies
which are typical of today’s datacenters. In such a topology, ma-
chines are grouped into racks and the Top-of-Rack (ToR) switches
are in turn connected to higher level switches.
We present a generic allocation algorithm for all TIVC models,
each of which can be viewed as a sequence of pulses of different
bandwidth and duration. We ﬁrst show how to ﬁnd a valid alloca-
tion for a TIVC request, then show how to ﬁnd a good allocation
out of all the valid allocations. Our allocation algorithm improves
the Oktopus allocation algorithm [11] with a novel dynamic pro-
gramming solution in searching valid allocations, which not only
signiﬁcantly improves the search efﬁciency, but also guarantees to
ﬁnd the most localized allocation, i.e., in the lowest subtree.
Bandwidth requirement of a valid allocation. Before presenting
the allocation algorithm, we ﬁrst ignore the time dimension and
explain on how a ﬁxed access bandwidth B per VM in a TIVC
request translates into the bandwidth requirement on the internal
links in the physical network.
If the N VMs required by a request can be found at a level-0
subtree, i.e., within a physical machine, they can be allocated right
away, as there should be enough bandwidth between the VMs in
the same machine. Otherwise, the N VMs will reside in multiple
subtrees, and the trafﬁc between them will travel up and down the
tree. This poses a subtle challenge as to how much bandwidth needs
to be reserved on the tree links. Consider a link L that connects a
left subtree containing m allocated VMs and a right subtree with
(N −m) VMs. Since each VM cannot send or receive at a rate more
than B, the maximum bandwidth needed on link L is min(m, N −
m) ∗ B. Thus a valid allocation needs to satisfy min(m, N − m) ∗
B  K1, in the parent depth-d
subtree, and hence we can formulate the searching algorithm as a
dynamic programming problem which runs very efﬁciently.
Finding a good allocation. Given a TIVC request, there can be
many possible valid allocations in the physical network. There are
two dimensions in the physical network that quantify a good allo-
cation. First, in the vertical dimension, a good allocation should
exhibit good locality, i.e., the VMs allocated to it should be as
205Application
Data
Profiling 
TIVC Model
(cid:273)
(cid:273)
(cid:273)
Traffic Trace
Bandwidth Cap
Bandwidth Cap
Cost Model
  $
  $
Customer picks 
bandwidth cap 
Figure 9: The Proteus system.
Spatio-
Temporal 
Allocation 
Algorithm
Production Data Center 
localized to a subtree as possible, as good locality conserves the
bandwidth of the links in the upper levels of the tree. Second, it is
possible that multiple equally localized allocations exist, i.e., each
within a depth-i subtree. A major consideration in choosing an
allocation out of them is fragmentation, as allocating a TIVC to
a subtree may result in the subtree having few VMs and little link
bandwidth left to ﬁt any future TIVCs. This fragmentation problem
resembles the classic dynamic memory allocation problem in oper-
ating systems, for which a number of classic heuristic allocation
strategies, including ﬁrst-ﬁt, best-ﬁt, and worst-ﬁt, exist. Our em-
pirical experiments have shown there is no clear winner, and hence
PROTEUS uses the local, ﬁrst-ﬁt strategy; it picks the ﬁrst ﬁtting
lowest-level subtree out of all such lowest-level subtrees.
The algorithm. Figure 10 shows the TIVC allocation algorithm in
pseudo code. The set of all possible numbers of VMs out of the N
VMs needed by a job that can be allocated in the subtree rooted at
v form the M set for v, Mv. Because of the bandwidth constraint
(Equation 1), the numbers in Mv may not be continuous. Hence,
we need to record the valid allocation out of each subtree during dy-
namic programming search, using a few data structures. Let Lv[k]
denote the set that contains the numbers of VMs that could be ac-
commodated in the ﬁrst k children of the subtree rooted at v, with-
out considering the uplink bandwidth constraint of v. Then Mv
contains all the values in Lv[n] that can satisfy the uplink band-
width constraint. To record the allocation in the traversed subtrees,
i.e., each possible value h in Lv[k], we record in Dv[k, h] the num-
ber of VMs assigned to the kth child of v, when it assigns h VMs
in the ﬁrst k children. The dynamic programming step is shown in
lines 5–10, which calculates Lv[k] and Dv[k, h] recursively. After-
wards, all the candidate numbers of VMs are added to Mv if they
pass the uplink bandwidth requirement of v (lines 11-14). If N can
be allocated out of Mv, Alloc() is called which performs recursion
according to Dv[k, h] while recording the bandwidth reservation
of the relevant links, and eventually outputs the number of VMs
per machine (level 0), and the algorithm terminates. We can easily
show the algorithm outputs the ﬁrst allocation (from left to right)
that ﬁts in the lowest-level subtree.
5.3 Enforcing TIVC Reservations
After allocating the VMs for a job, PROTEUS needs to conﬁgure
the network elements to enforce the reserved bandwidth on the ac-
cess links and internal links that connect the VMs. Prior works [11,
34, 26] have opted for an end-host only approach, which reserves
per-VM access bandwidth in the hypervisor. To enforce reserva-
tions in in-network links, such approaches add signiﬁcant complex-
ity in the hypervisor, which needs to perform online rate monitoring
for all VM pairs, communicate the rates to a centralized optimizer
which calculates the max-min fairness for each VM pair, etc., mak-
ing such approaches less scalable. Bloating the hypervisors is also
typically disliked since it adds complexity in the critical path and
compromise robustness and security [13].
We make a key observation that even in a large-scale datacenter,
the number of jobs that share a link at the same time is generally
If (l==0) Lv[0] = {0, ..., # avail. VMs} // leaf machine
Else Lv[0] = {0}
For v’s child k from 1 to n
Lv[k] = {0}
For each possible value e in v’s kth child’s M set (Mvk )
For each possible value h in Lv[k − 1]
Lv[k] = Lv[k] ∪ {e + h}
Dv[k, e + h] = e
If (bandwidth check of v’s uplink per Eq. (1) == true)
For each subtree v at level l
Mv = ∅
For each value h in Lv[n]
Algorithm: Allocation for TIVC request r
Input: Datacenter topology tree T
1. For level l from 0 to height(T)
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18. Return false
function Alloc(r, v, m):
19. If v is a machine
20.
21. Else
22.
23.
24.
25.
Alloc(r, v, N )
Return true
allocate m VMs in v
Mv = Mv ∪ {h}
If N ∈ Mv
For v’s child k from n to 1
Alloc(r, vk, Dv[k, m])
record bw reservation on v’s kth link
m = m − Dv[k, m]
Figure 10: The TIVC Allocation algorithm.
low. Consider a typical rack in the production environment with
40 machines and hence 160 VMs assuming conservatively 4 VMs
per machine. If most jobs are small enough to ﬁt within a rack,
few jobs need to straddle the rack boundaries. On the other hand,
very few large jobs which need to cross the core of the network
can be scheduled to run concurrently since each of them consumes
a large number of VMs. In our simulation runs over a datacenter
with 16,000 machines (§6), we found fewer than 26 concurrent jobs
per link (see Figure 17). Rate limiting such a low number of jobs
sharing a link can be easily implemented using network switches
available today. For example, the Cisco Nexus 7000 Series 32-
port 10Gb module already supports up to 16K policers (for rate
limiting 16K aggregates) [4] and 64K ACL entries (for deﬁning
the trafﬁc aggregates). Furthermore, the above low number of jobs
sharing a link, as well as the fact the edges of bandwidth pulses of
different jobs happen at different times, suggest that reconﬁguring
the policers can be done with low overhead.
Multi-path routing. The TIVC allocation algorithm in §5.2 as-
sumes a simple tree topology where the trafﬁc between a VM pair
follows a single path up and down the tree. However, datacen-
ters may have networks with richer connectivities such as multi-
rooted trees (e.g., [27]) and fat-trees [7, 19]. These networks typi-
cally use hash-based or randomized techniques such as ECMP and
Valiant Load Balancing to spread trafﬁc across multiple equal cost
paths, and can use more involved techniques such as Hedera [8] and
MPTCP [31] to ensure uniform trafﬁc spread despite ﬂow length
206variations. Therefore, such topologies can be incorporated into the
TIVC allocation algorithm by treating the multiple links from each
physical machine or switch that are used in multiple equal cost
paths (to the same destination) as a single aggregation link, and en-
forcing bandwidth reservation of an aggregation link boils down to
enforcing equal reservation split among the multiple physical links
in the aggregate. We leave a detailed experimental study of such
physical networks as future work.
6. EVALUATION
We use both simulations of large scale datacenter networks and
our implementation on a testbed running real MapReduce applica-
tions to show PROTEUS exhibits signiﬁcant advantage over a ﬁxed-
bandwidth reservation scheme such as Oktopus.
6.1 Simulation Setup
To show the effectiveness of PROTEUS in large scale datacenter
settings, we developed a simulator that models VM and network
bandwidth reservations in a shared datacenter. The simulator sim-
ulates a datacenter of three-level tree topology. There are 16,000
machines at level 0, each with 4 VM slots. 40 machines form a
rack and are linked with a Top-of-Rack (ToR) switch with 1 Gbps
links. Every 20 ToR switches are connected to a level-2 aggrega-
tion switch, and 20 aggregation switches are connected to the core
switch of the datacenter. The default oversubscription of the phys-
ical network is 4, i.e., ToR switches are connected to aggregation
switches with 10 Gbps links, and aggregation switches to the core
switch with 50 Gbps links.
Alternate abstractions. We compare PROTEUS with Oktopus, the
state-of-the-art network abstraction [11]. Oktopus supports two
network abstractions: virtual clusters (VC) and virtual oversub-
scribed clusters (VOC). However, VOC places a signiﬁcant burden
on the cloud users who not only have to specify the constant band-
width constraint B, but also explicit subclustering of VMs that ex-
hibit local communication and their oversubscription factors. Fur-
ther, we do not observe any such communication locality in the ap-
plications we have studied. Thus we leave comparison with VOC as
future work. Ideally, we should also compare TIVC with a baseline
model that schedules jobs solely based on the number of available
VMs. However, it is difﬁcult to model in simulations the execution
time elongation when jobs compete for networking freely.
Workload. We simulate tenant jobs based on the network work-
load extracted from the MapReduce applications studied in §2.2:
Sort, Hive Join, and Hive Aggregation. We do not include Word
Count as it has insigniﬁcantly low bandwidth requirement and will
not beneﬁt from bandwidth reservations provided by network ab-
stractions like VC and TIVC. We use the no-elongation threshold
bandwidth cap (§4.1) as the bandwidth requirement B under VC,
and as the capping bandwidth in application proﬁling and model
generation under TIVC. Using the same bandwidth cap B this way
ensures that the job running times stay the same under the two ab-
stractions during production runs. The generated TIVC parameters
are shown in Table 2. To simulate a datacenter with diverse job
mixes, we vary the number of VMs needed by each job; in our
experiments by default the number of VMs per job request is expo-
nentially distributed around a mean of 49 (following [11]).
6.2 Simulation Results
We compare PROTEUS with Oktopus under two scenarios: (1) A
large number of tenant jobs are pooled at the job queue waiting to
be scheduled to run. This workload captures production datacenters
that host time-insensitive jobs, e.g., data processing jobs to be run
overnight. (2) Tenant jobs arrive dynamically and are accepted only
)
c
e
s
(
i
e
m
T
n
o
i
t
l
e
p
m
o
C
VC
TIVC
 10000
 8000
 6000
 4000
 2000
 0
Sort
Join Aggre. Mixed
Figure 11: Total completion time.
)
c
e
s
(
i
e
m
T
n
o
i
t
l
e
p
m
o
C
 16000
 12000
VC
TIVC
 8000
 4000
 0
 1  2
 4
 6
 8
 10
Network Oversubscription
)
c
e
s
(
i
e
m
T
n
o
i
t
l
e
p
m
o
C
VC
TIVC
 20000