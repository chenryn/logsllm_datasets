2. The backend server sends out any aux requests required
for processing the client request and waits for the corre(cid:173)
sponding responses.
3. If the client request changes application state ("update"
the updated state information is sent asyn(cid:173)
request),
chronously to an alternate backend server. Note that
because of asynchronous nature of this communication,
the alternate backend server can lag behind the backend
server. So, if the transaction is non-deterministic, this
state information is sent synchronously.
4. The backend server sends a response to the client re(cid:173)
quest.
5. When the logger receives the entire response, it informs
the alternate backend server so that it applies that state
information to the local application. Notice that the
alternate backend server applies this state information
only after the backend server has sent out the response
and the response has been completely logged. Note that
at the user level on the backend server, it is hard to deter(cid:173)
mine when the response has been completely sent (since
it may remain in the TCP send buffers for some time).
4.2 Terminology
We now define some terms that are used in the failure re(cid:173)
covery process described in the next section. Ti refers to the
unique transaction ID assigned to each transaction. It is a 64
bit long ID consisting of client IP address, client port number
and the transaction number.
TL Last transaction with response fully saved at the logger,
Le., the backend server crashed before the completion
of transaction TL + 1.
AckTL Ack corresponding to the last response byte in trans(cid:173)
action TL.
4.3 Failure Recovery
The following steps are involved in the failure recovery
process after a backend server has crashed. Note that al(cid:173)
though presented sequentially here for clarity, a number of
these steps occur concurrently.
• Logger detects a backend server failure; determines TL
and shares this information with the alternate backend
server.
• Proxy un-splices the client TCP connection with the
failed backend server; signals other proxies to do the
same.
• Alternate backend server determines Ts and TA; shares
this information with the logger.
• Synchronizing the client TCP connection. AckcL is
the last ack received from the client. Bytes are sent/re(cid:173)
sent to the client from this point. Bytes until the end
of transaction TL and p bytes of transaction TL + 1 are
already available at the logger. Therefore, if the follow(cid:173)
ing equation is true, the proxy temporarily re-splices the
client connection with a new connection to the logger in
order to send out these bytes.
AckcL :::; AckTL + P
(1)
Note that Equation 1 will very likely be an equality un(cid:173)
less there is packet loss. Usually the time taken to detect
failure - even if it is a second or less - is enough for the
client ack of the last packet sent to be received by the
logger. The client connection is re-spliced to the alter(cid:173)
nate backend server at transaction Tsp, which is,
(2)
1-4244-2398-9/08/$20.00 ©2008 IEEE
172
DSN 2008: Marwah et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:45 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Since p bytes of TL + 1 are already sent, the re-splicing
is performed such that the first p bytes of the transaction
are locally received at the proxy and thereafter the bytes
are spliced to the client connection. Similarly, the splice
point for bytes from the client to the server is also deter(cid:173)
mined based on the last client bytes that is available on
the logger.
• Synchronizing the alternate backend server. Al(cid:173)
though the client only needs response bytes starting
from TL + 1, the application state on the alternate server
may not be updated until TL. This could be due to
(1) the asynchronous state updates from
two reasons:
the backend server lagged behind; or (2) signals from
the logger - which cause an alternate server to apply
the corresponding transaction associated state update (cid:173)
lagged behind. The alternate backend server is first up(cid:173)
dated till Ts. It applies state information associated with
(Ts, TR) and starts executing transactions at TR, which
is determined as follows.
T = {TA + 1 if TL ~ TA
TL + 1 otherwise
R
(3)
Note that replay of transactions [TR' TL] is done intel(cid:173)
ligently; read-only transactions are not replayed. The
client requests for [TR,TL] and potentially TL + 1 are
supplied by the logger on the proxy-alternate server
connection. Note that if a request is substantial in size,
for instance, it is an HTTP POST request to transfer
a large file, the proxy can splice the logger-proxy and
proxy-alternate server connections.
• As mentioned earlier, the aux logger saves any aux re(cid:173)
quests and responses; an aux request carries a unique
transaction ID which can be used to correlate it to a
particular transaction. For the replay of transactions
[TR,TL] and potentially partial replay of TL + 1, any
aux requests are responded to by responses cached at
the aux logger.
Note that in practice, the two loggers, backend server and
alternate backend server most likely reside on the same LAN.
Hence, TL = Ts = TA is the most likely scenario, in which
case only one transaction, TL + 1, is replayed.
5 Implementation
We implemented a prototype of our server fault-tolerance
architecture in Linux. We had earlier made some enhance(cid:173)
ments to TCP splice [11] to make it distributed and fault(cid:173)
tolerant. We further extended the TCP splicing functionality
to perform re-splicing. This is implemented as a Linux ker(cid:173)
nel module and installed at a proxy. We enhanced our log(cid:173)
ger [12] to transparently log TCP connections and make the
logged bytes available to a user-space transactionalizer and
tagger. Finally, a recovery manager that resides at a proxy
was added to coordinate recovery.
Netfilter. We made extensive use of netfilter [13] in both
logging module (logmod) and TCP
the kernel modules:
splicing module (t cpspmod). Netfilter adds a set of hooks
along the path of a packet's traversal through the Linux net(cid:173)
work stack.
It allows kernel modules to register callback
(CB) functions at these hooks. These hooks intercept packets
and invoke any CB function that may be registered with that
hook. After processing a packet, a CB function can decide to
inject it back along its regular path, or steal it from the stack
and send it elsewhere, or even drop it.
Logging kernel module. The logging module uses user-
space memory that is mapped into the kernel. A user
process sends a user-space memory pointer to the logging
module using a system call.
The kernel module calls
get_user_pages () to map that memory into kernel-space
memory pages. This allows it to log TCP segments on to
memory that becomes visible to a user process without any
kernel-to-user space copying. The module needs to take spe(cid:173)
cial care to detect and correctly log re-transmissions and
out of sequence packets. Furthermore, since both the ker(cid:173)
nel module and a user-space process are accessing the same
piece of memory, appropriate synchronization mechanisms
are used to avoid race conditions. Since the memory allo(cid:173)
cated for logging is limited, the log wraps around on reaching
the end of allocated space.
User-space Transactionalizer & Tagger The transac(cid:173)
tionalizer and tagger interacts with the logging module to
obtain access to the memory where TCP stream data is be(cid:173)
ing logged. Using application specific information, it eagerly
parses the byte stream into transactions. Note that although
transaction information is only needed if there is a failure,
parsing the stream lazily - as needed on failure - is prob(cid:173)
lematic since the log may wrap around. Each transaction is
given a transaction ID and tagged. Tagging requires applica(cid:173)
tion specific knowledge as discussed in Sections 3.1. It also
maintains a mapping of the TCP sequence number offsets to
Transaction IDs. Furthermore, it communicates with the re(cid:173)
covery manager at the proxy and with the alternate backend
server during failure recovery.
Recovery manager. The recovery manager is a user-level
process that resides at a proxy and is responsible for coor(cid:173)
dinating the failure recovery once a backend server failure
is detected. It instructs the TCP splicing module to suspend
the splice to the failed backend. It communicates with the
transactionalizer and tagger to obtain the appropriate offsets
when the backend crashes. Furthermore, it supplies the TCP
splicing module with those offsets to perform re-splicing.
6 Experiments and Performance Evaluation
To provide a proof of concept, we demonstrate our archi(cid:173)
tecture on a real-life Web mail application called Roundcube
webmail [16], an open-source webmail application written in
PHP programming language. Hosted at a data center, this ap(cid:173)
plication provides service similar to that provided by Google
Mail, Yahoo Mail, or Microsoft's Hotmail. Users can con(cid:173)
nect to it via Web browsers. Roundcube Web server uses
IMAP [6] to connect to the email store servers. Compared
to traditional webmail clients, Roundcube and other similar
AJAX-based [1] Web applications have a more responsive
user-interface.
We conducted a series of experiments to evaluate the ef(cid:173)
ficacy of our architecture in terms of providing an improved
user experience during a server crash failure. The main goals
of the experiments were:
(1) Measure failover times with
1-4244-2398-9/08/$20.00 ©2008 IEEE
173
DSN 2008: Marwah et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:45 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
our architecture; (2) Measure the overhead of our architec(cid:173)
ture during normal operation; (3) Compare failover times ob(cid:173)
tained with our architecture to those obtained with commonly
used current server fault-tolerance techniques; and (4) Eval(cid:173)
uate our system with clients connected over networks with
diverse characteristics.
We conducted experiments in both LAN and WAN set(cid:173)
tings. The backend servers, proxy and logger that we used
are attached to the same LAN on the campus network of the
University of Colorado at Boulder. To test under a LAN
environment, a client application was installed on a ma(cid:173)
chine connected to the same LAN. In order to run our ex(cid:173)
periments over diverse WAN links, we used PlanetLab [3]
nodes as our testbed. We placed the client on three dis(cid:173)
tinct sites: (1) WAN-MIT: a PlanetLab node at MIT (planet(cid:173)
lab7.csail.mit.edu); (2) WAN-SG: a PlanetLab node in Singa(cid:173)
pore (planetlab3.singaren.net.sg); and (3) WAN-IN: a Plan(cid:173)
etLab node in India (planetlab l.iitr.ernet.in). These locations
were chosen because they provide a wide variety of round
trip times (RTT) between the server in Colorado and its peer.
The RTT is about 79 ms to the machine at MIT, and about
260 ms to the one in Singapore. The RTT to the node in In(cid:173)
dia is extremely large at about 886 ms. Compared to these,
the RTT for the LAN scenario is about 0.25 ms.
6.1 Experimental Setup
Alternate
Server
planetlabl.iitr.ernet.in
Figure 3: Experimental setup.
The experimental setup is shown in Figure 3. At the server
end, the machines are attached to two Ethernet switches and
span two subnets. The proxy, logger and the auxiliary server
(which acts as an IMAP server) reside on 10.0.1.0/24 sub(cid:173)
net. This subnet is connected to the public Internet through
a GW machine. The logger and backend servers are part of
the 10.0.0.0/24 subnet. Note that the logger is dually homed
and is a gateway between the two subnets. This allows it
to conveniently log packets between both the backend server
and a client, and the backend server and the auxiliary server.
Since the machines are on private subnets, the WAN clients
connect to it through s sh tunnels between the client and the
GWmachine.
Action
Login screen
Logging in
Reading msg
Sending msg
PHP
script
1
2
2
3
GET
File Downloads
POST
CSS
JavaScript
images
1
1
1
0
2
1
1
0
4
28
4
1
0
1
0
1
Table 1: Common actions in Roundcube and the corresponding
HTTP GET and POST requests. For the GET requests, the num(cid:173)
ber of times a PHP script is invoked at the server, and, the number
of CSS, JavaScript and images files that are download by a Web
browser are also listed.
6.2 Experiments
Users connect to webmail using HTTP through a Web
browser. User actions such as logging in, reading and send(cid:173)
ing email are translated into HTTP GET and POST requests
by the browser as shown in Table 1. For our experiments,
we chose common actions that users are likely to take while
checking their email. Furthermore, we chose both read-only
and update actions.
We picked two actions that were used for all our experi(cid:173)
ments. The first is a simple one: displaying the login screen.
The second is a more complex operation: it consists of a user
logging in, composing an email, sending it out and, finally,
logging out. We experimented with several other user actions
as well and these two are well representative of the lot since
they have a mix of read-only and update operations.
In order to be able to send these requests repeatedly, mea(cid:173)
sure the time taken, and cause a failure when a request is
in progress, we used a 'C' program instead of a browser as
the client for our experiments. We performed both the above
actions and used ethereal [7] to record the requests sent
out by the browser and the responses received. To make sure
that our program operated identically to a browser, we sent
these recorded requests to the Web server. We also matched
the responses received to the recorded ones to ensure correct(cid:173)
ness. Furthermore, the program parsed the received response
header in order to correctly receive the body of the response.
The webmail application assigns a session ID and sends it as
a cookie when a user logs in. This ID needs to be sent with
all subsequent requests in that session. This was another ca(cid:173)
pability that we added to our client program.
Action 1: Displaying the login screen. This action con(cid:173)
sists of eight GET requests in all. About half are requests
for images displayed on the login screen. One is an exe(cid:173)
cution of a PHP script and others download JavaScript and
CSS scripts. For our experiments, we assume that the images