### 2. Backend Processing of Client Requests
The backend server initiates any auxiliary requests necessary for processing the client request and waits for the corresponding responses.

### 3. State Updates and Asynchronous Communication
- **State Update**: If the client request changes the application state (e.g., an "update" request), the updated state information is sent asynchronously to an alternate backend server.
- **Asynchronous Nature**: Due to the asynchronous nature of this communication, the alternate backend server may lag behind the primary backend server.
- **Synchronous State Transfer**: For non-deterministic transactions, the state information is sent synchronously to ensure consistency.

### 4. Response to Client Request
The backend server sends a response to the client request. Once the logger receives the entire response, it informs the alternate backend server to apply the state information to the local application. The alternate backend server applies this state information only after the primary backend server has sent out the response and the response has been fully logged.

**Note**: At the user level on the primary backend server, it is challenging to determine when the response has been completely sent, as it may remain in the TCP send buffers for some time.

### 4.2 Terminology
We define the following terms used in the failure recovery process:
- **Ti**: Unique transaction ID assigned to each transaction. It is a 64-bit ID consisting of the client IP address, client port number, and the transaction number.
- **TL**: Last transaction with a response fully saved at the logger, i.e., the backend server crashed before the completion of transaction TL + 1.
- **AckTL**: Acknowledgment corresponding to the last response byte in transaction TL.

### 4.3 Failure Recovery
The following steps are involved in the failure recovery process after a backend server has crashed. Note that although presented sequentially here for clarity, many of these steps occur concurrently.

1. **Failure Detection and Information Sharing**:
   - The logger detects the backend server failure, determines TL, and shares this information with the alternate backend server.
   - The proxy un-splices the client TCP connection with the failed backend server and signals other proxies to do the same.
   - The alternate backend server determines Ts and TA and shares this information with the logger.

2. **Client TCP Connection Synchronization**:
   - **AckcL**: The last acknowledgment received from the client.
   - Bytes are sent/re-sent to the client from this point. Bytes until the end of transaction TL and p bytes of transaction TL + 1 are already available at the logger.
   - If the following equation is true, the proxy temporarily re-splices the client connection with a new connection to the logger to send out these bytes:
     \[
     \text{AckcL} \leq \text{AckTL} + P
     \]
   - Equation 1 will very likely be an equality unless there is packet loss. The client connection is re-spliced to the alternate backend server at transaction Tsp, which is determined by:
     \[
     \text{Tsp} = \text{TL} + 1
     \]

3. **Alternate Backend Server Synchronization**:
   - Although the client only needs response bytes starting from TL + 1, the application state on the alternate server may not be updated until TL. This could be due to:
     1. The asynchronous state updates from the primary backend server lagging behind.
     2. Signals from the logger causing the alternate server to apply the corresponding transaction state update lagging behind.
   - The alternate backend server is first updated till Ts. It applies state information associated with (Ts, TR) and starts executing transactions at TR, which is determined as follows:
     \[
     \text{TR} = 
     \begin{cases} 
     \text{TA} + 1 & \text{if } \text{TL} \geq \text{TA} \\
     \text{TL} + 1 & \text{otherwise}
     \end{cases}
     \]
   - Transactions [TR, TL] are replayed intelligently; read-only transactions are not replayed. The client requests for [TR, TL] and potentially TL + 1 are supplied by the logger on the proxy-alternate server connection.

4. **Auxiliary Logger**:
   - The auxiliary logger saves any auxiliary requests and responses. An auxiliary request carries a unique transaction ID, which can be used to correlate it to a particular transaction.
   - For the replay of transactions [TR, TL] and potentially partial replay of TL + 1, any auxiliary requests are responded to by responses cached at the auxiliary logger.

**Note**: In practice, the two loggers, the primary backend server, and the alternate backend server most likely reside on the same LAN. Hence, TL = Ts = TA is the most likely scenario, in which case only one transaction, TL + 1, is replayed.

### 5. Implementation
We implemented a prototype of our server fault-tolerance architecture in Linux. We enhanced TCP splice to make it distributed and fault-tolerant, and further extended the TCP splicing functionality to perform re-splicing. This is implemented as a Linux kernel module and installed at a proxy. We also enhanced our logger to transparently log TCP connections and make the logged bytes available to a user-space transactionalizer and tagger. Finally, a recovery manager that resides at a proxy was added to coordinate recovery.

#### Netfilter
We made extensive use of netfilter in both the logging module (logmod) and the TCP splicing module (tcpspmod). Netfilter adds a set of hooks along the path of a packet's traversal through the Linux network stack, allowing kernel modules to register callback functions at these hooks. These hooks intercept packets and invoke any registered callback function. After processing a packet, a callback function can decide to inject it back along its regular path, steal it from the stack, or drop it.

#### Logging Kernel Module
The logging module uses user-space memory mapped into the kernel. A user process sends a user-space memory pointer to the logging module using a system call. The kernel module calls `get_user_pages()` to map that memory into kernel-space memory pages, allowing it to log TCP segments without kernel-to-user space copying. Special care is taken to detect and correctly log re-transmissions and out-of-sequence packets. Appropriate synchronization mechanisms are used to avoid race conditions, and the log wraps around on reaching the end of allocated space.

#### User-Space Transactionalizer & Tagger
The transactionalizer and tagger interact with the logging module to obtain access to the memory where TCP stream data is being logged. Using application-specific information, it parses the byte stream into transactions. Each transaction is given a transaction ID and tagged, requiring application-specific knowledge. It maintains a mapping of the TCP sequence number offsets to transaction IDs and communicates with the recovery manager at the proxy and the alternate backend server during failure recovery.

#### Recovery Manager
The recovery manager is a user-level process that resides at a proxy and coordinates the failure recovery once a backend server failure is detected. It instructs the TCP splicing module to suspend the splice to the failed backend, communicates with the transactionalizer and tagger to obtain the appropriate offsets, and supplies the TCP splicing module with those offsets to perform re-splicing.

### 6. Experiments and Performance Evaluation
To provide a proof of concept, we demonstrate our architecture on a real-life Web mail application called Roundcube webmail, an open-source webmail application written in PHP. Hosted at a data center, this application provides service similar to Google Mail, Yahoo Mail, or Microsoft's Hotmail. Users can connect to it via Web browsers. Roundcube Web server uses IMAP to connect to the email store servers. Compared to traditional webmail clients, Roundcube and other similar AJAX-based Web applications have a more responsive user interface.

#### Experimental Setup
We conducted a series of experiments to evaluate the efficacy of our architecture in providing an improved user experience during a server crash failure. The main goals were:
1. Measure failover times with our architecture.
2. Measure the overhead of our architecture during normal operation.
3. Compare failover times obtained with our architecture to those obtained with commonly used current server fault-tolerance techniques.
4. Evaluate our system with clients connected over networks with diverse characteristics.

Experiments were conducted in both LAN and WAN settings. The backend servers, proxy, and logger were attached to the same LAN on the campus network of the University of Colorado at Boulder. For the LAN environment, a client application was installed on a machine connected to the same LAN. For WAN experiments, we used PlanetLab nodes as our testbed, placing the client on three distinct sites: MIT, Singapore, and India, providing a wide variety of round-trip times (RTT).

#### Common Actions in Roundcube
Table 1 shows common actions in Roundcube and the corresponding HTTP GET and POST requests. For our experiments, we chose common actions that users are likely to take while checking their email, including both read-only and update actions.

#### Experiment Details
- **Action 1**: Displaying the login screen. This action consists of eight GET requests, including requests for images, a PHP script execution, and JavaScript and CSS downloads.
- **Action 2**: A more complex operation involving a user logging in, composing an email, sending it out, and logging out.

To send these requests repeatedly, measure the time taken, and cause a failure when a request is in progress, we used a C program instead of a browser as the client for our experiments. We recorded the requests sent out by the browser and the responses received, ensuring that our program operated identically to a browser.

**Note**: The webmail application assigns a session ID and sends it as a cookie when a user logs in. This ID needs to be sent with all subsequent requests in that session, a capability added to our client program.