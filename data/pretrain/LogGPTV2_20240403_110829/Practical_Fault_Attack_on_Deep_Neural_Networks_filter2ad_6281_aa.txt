title:Practical Fault Attack on Deep Neural Networks
author:Jakub Breier and
Xiaolu Hou and
Dirmanto Jap and
Lei Ma and
Shivam Bhasin and
Yang Liu
Practical Fault Attack on Deep Neural Networks
Jakub Breier
Dirmanto Jap
Xiaolu Hou
Nanyang Technological University
Nanyang Technological University
Nanyang Technological University
PI:EMAIL
PI:EMAIL
PI:EMAIL
Yang Liu
PI:EMAIL
Harbin Institute of Technology
Nanyang Technological University
Nanyang Technological University
Lei Ma
PI:EMAIL
Shivam Bhasin
PI:EMAIL
ABSTRACT
As deep learning systems are widely adopted in safety- and security-
critical applications, such as autonomous vehicles, banking systems,
etc., malicious faults and attacks become a tremendous concern,
which potentially could lead to catastrophic consequences. In this
paper, we initiate the first study of leveraging physical fault in-
jection attacks on Deep Neural Networks (DNNs), by using laser
injection technique on embedded systems. In particular, our ex-
ploratory study targets four widely used activation functions in
DNNs development, that are the general main building block of
DNNs that creates non-linear behaviors – ReLu, softmax, sigmoid,
and tanh. Our results show that by targeting these functions, it is
possible to achieve a misclassification by injecting faults into the
hidden layer of the network. Such result can have practical impli-
cations for real-world applications, where faults can be introduced
by simpler means (such as altering the supply voltage).
KEYWORDS
deep learning security, fault attacks, adversarial attacks
1 INTRODUCTION
Internet of things (IoT) and artificial intelligence (AI) are the two
integral components of modern paradigms like smart city, self-
driving cars etc. The most efficient AI is known in the form of deep
learning which has achieved great leaps of progress in many appli-
cation domains.In parallel, IoT has pushed the computing elements
(and sensors) outside traditional boundaries and motivates to place
them everywhere. This has also enabled easy physical access to the
computing elements which was not possible previously.
Deep learning is the family of neural networks composed of an
input layer, three or more hidden layers and an output layer. Based
on the internal structure, several candidates exist like multi-layer
perceptron (MLP), convolutional neural networks (CNN), recurrent
neural network (RNN) etc. These are popularly known as deep
neural networks (DNN). While each of these architectures has
unique functions, we focus on activation functions which remain
common across architectures and are an important part of the
algorithm to obtain non-linear behaviors [8]. These commonly
used activation functions are: softmax, ReLu, sigmoid and tanh.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s).
This is the author’s version of the work. It is posted here for your personal use. Not for
redistribution. The definitive Version of Record was published in 2018 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’18), Oct. 15–19, 2018,
Toronto, ON, Canada.
Studying these functions under fault attacks allows to derive general
conclusions on susceptibility of deep learning to fault attacks.
We implemented the most common activation functions used
across DNNs on a low-cost microcontroller (often used in IoT).
Next, we performed practical laser fault injection using a near-
infrared diode pulse laser to inject faults during the processing of
activation function. The use of laser facilitates a strong attacker
model with extensive fault injection capabilities. With the models,
derived from practical fault injection, we analyze the susceptibility
of DNN against such attacks. The primary goal of the performed
attacks is to achieve misclassification during the testing phase. In
the hindsight, the achieved misclassification can jeopardize the
functioning of DNN-based paradigms like smart city.
Extensive studies have been performed on adversarial attacks,
that crafts the input data with little perturbation to fool deep learn-
ing systems [9]. To the best of our knowledge, our study is the first
work to explore practical fault injection on deep neural network,
where we focus on attacking the DNNs itself instead of creating
input data to fool DNNs like adversarial attack does.
Fault injection attacks are a popular physical attack vector used
against cryptographic circuits. By changing intermediate values
during the cryptographic algorithm execution, they can efficiently
provide information on secret values, helping to recover the secret
key in just a few encryptions [4]. Normally, the secret key recov-
ery would require infeasible amount of computing time. Similarly,
these attacks can be used against verification circuits, such as PIN
verification on a smartcard, where a comparison function can be
skipped and grant access to a malicious user [7].
Up to date, to the best of our knowledge, only [10] describes
fault injection attack on neural networks. In their paper, they only
provide a white box attack on deep neural network through soft-
ware simulation, while observing the changes in the output after
introducing faults in the network’s values. However, they do not
provide insight on practicality of such attack. Whether such attacks
could also be applied physically remained an open problem. There-
fore, in our paper, we experimentally show what types of faults
are achievable in practice and we further use this information to
develop a realistic attack on DNNs.
2 PRACTICAL DNN ATTACK ANALYSIS
2.1 Attack Equipment Setup
The main component of the experimental laser fault injection sta-
tion is the diode pulse laser. It has a wavelength of 1064 nm and
pulse power of 20 W. This power is further reduced to 8 W by a
20× objective lens which reduces the spot size to 15×3.5 µm2.
Table 1: Relation between correct output y and faulted output y′
when a single fault is injected in target activation function
Target activation function
ReLu: y = max(0, x)
sigmoid: y =
1+e−x
1+e−2x − 1
tanh: y =
1
2
Relation between y and y′
y′ = 0
y′ = 1 − y
y′ = −y
As the device under test (DUT), we used ATmega328P microcon-
troller, mounted on Arduino UNO development board. The package
of this chip was opened so that there is a direct visibility on a
back-side silicon die with a laser. The board was placed on an XYZ
positioning table with the step precision of 0.05 µm in each direc-
tion. A trigger signal was sent from the device at the beginning
of the computation so that the injection time could be precisely
determined. After the trigger signal was captured by the trigger and
control device, a specified delay was inserted before laser activation.
Laser activation timing was also checked by a digital oscilloscope
for a greater precision.
The chip area is 3×3 mm2, while the area sensitive to laser is ≈
50×70 µm2. With a laser power of 4.5% we were able to disturb the
algorithm execution, when tested with reference codes. By using
some pre-tested benchmarking software, we determined that a laser
power of 4.5% was enough to disturb the algorithm execution.
2.2 DNN Activation Function Fault Analysis
To evaluate different activation functions, we implemented three
simple 3-layer neural networks with sigmoid, ReLu and tanh as the
activation function for the second layer respectively. The activa-
tion function for the last layer was set to be softmax. The neural
networks were implemented in C programming language, which
were further compiled to AVR assembly and uploaded to the DUT.
As instruction skip/change are one of the most basic attacks
on microcontrollers, with high repeatability rates [5], we aimed
at this fault model in our experiments. The microcontroller clock
is 16 MHz, one instruction takes 62.5 ns. Some of the activation
functions took over 2000 instructions to execute. To check what are
the vulnerabilities of the implementations, we have carefully varied
the timing of the laser glitch from the beginning until the end of the
function execution so that every instruction would be eventually
targeted. We used a single fault adversarial model – exactly one
fault was injected during one activation function execution.
After we observed a successful misclassification, we determined
the vulnerable instructions by visual inspection of the compiled
assembly code and by checking the timing of the laser in that
particular fault injection instance.
In this exploratory study, we implemented a random neural
network, consisting of 3 layers, with 19, 12, and 10 neurons in input
layer, hidden layer, and output layer, respectively. Our fault attack
was always targeting the computation of one of the activation
functions in hidden layer. In the following, we will explain the
experimental results on different activation functions in detail. A
overview of the achieved results is shown in Table 1. It reports, for
an input x, the relation between y (correct) and y′ (faulted) output
of different activation functions.
ReLu was implemented by a following code in C:
if (Accum > 0) {HiddenLayerOutput[i] = Accum;}
else {HiddenLayerOutput[i] = 0;}
where i loops from 1 to 12 so that each loop gives one output of
the hidden layer. Accum is an intermediate variable that stores the
input of activation function for each neuron.
The assembly code inspection showed that the result of suc-
cessful attack was executing the statement after else such that the
output would always be 0. The corresponding assembly code is:
;load 0 to r1
;compare MSB of Accum to r1
;jump to else if 0 >= Accum
1
ldi r1, 0
2
cp r1, r15
3
brge else
movw r10, r15 ;HiddenLayerOutput[i] = Accum
4
movw r12, r17 ;HiddenLayerOutput[i] = Accum
5
6
jmp end
7 else: clr r10
8
clr r11
9
clr r12
10
clr r13
11 end:
...
;jump after the else statement
;HiddenLayerOutput[i]= 0
;HiddenLayerOutput[i]= 0
;HiddenLayerOutput[i]= 0
;HiddenLayerOutput[i]= 0
;continue the execution
where each float number is stored in 4 registers. For example, Accum
is stored in registers r15,r16,r17,r18 and HiddenLayerOutput[i]
is stored in r10,r11,r12,r13. Line 4,5 executes the equation
HiddenLayerOutput[i] = Accum.
The attack was skipping the “jmp end” instruction that would
normally avoid the part of code setting HiddenLayerOutput[i] to
0 in case Accum > 0. Therefore, such change in control flow renders
the neuron inactive no matter what is the input value.
Sigmoid was implemented by a following C code:
HiddenLayerOutput[i] = 1.0/(1.0 + exp(-Accum));
After the assembly code inspection, we observed that the success-
ful attack was taking advantage of skipping the negation in the
exponent of exp() function, which compiles into one of the two
following codes, depending on the compiler version:
;compute negation r16
;load 0x80 into r15
;xor r16 with r15
neg r16
ldi r15, 0x80
eor r16, r15
A)
B)
Laser experiments showed that both neg and eor could be skipped,
and therefore, significant change to the function output was achieved.
Hyperbolic tangent was implemented by a following code in C:
HiddenLayerOutput[i] = 2.0/(1.0 + exp(-2*Accum)) - 1;
Similarly to sigmoid, the experiments showed that the successful
attack was exploiting the negation in the exponential function,
leading to an impact similar to sigmoid.
Softmax. In case of softmax function, we were unable to obtain
any successful misclassification. There were only two different
outcomes as a result of the fault injection: either there was no output
at all, or the output contained invalid values. This lack of valid
output prevented us to do further fault analysis to derive the actual
fault model that happened in the device. Therefore, a thorough
analysis of softmax behavior under faults would be an interesting
topic for the future work. Another line of future work would be to
analyze bit flip attacks on IEEE 754 floating point representation
that is used for storing the weights. The representation follows
32-bit pattern (b31...b0): 1 sign bit (b31), 8 exponent bits (b30...b23)
and 23 mantissa (fractional) bits (b22...b0). The represented number
is given by (−1)b31 × 2(b30 ...b23)2−127 ×(1.b22...b0)2. A bit flip attack
on the sign bit or on the exponent bits would make significant
influence on the weight.
Table 2: Structure of DNNs used in fault evaluations.
Table 3: Training/testing accuracy of DNNs used in evaluation.
Layer
No. of neurons
Activation function
Input layer
Hidden layer 1
Hidden layer 2
Hidden layer 3
Hidden layer 4
Output layer
784
500
500
500
n
10
-
ReLu
ReLu