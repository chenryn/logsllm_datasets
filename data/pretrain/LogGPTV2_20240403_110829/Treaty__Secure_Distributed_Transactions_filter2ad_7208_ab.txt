on RocksDB [3] where the data is stored in multiple levels,
increasing in size. Higher levels (MemTables) are stored in
the memory while the bulk of the lower levels (and thus of
the data) is stored on disk in SSTables. Updates are applied
to the MemTable and when it exceeds a maximum size, it is
merged into the next lower level (compaction). If this causes
the next level to exceed its own maximum size, the compaction
cascades further. The system remains correct under failures
through a combination of write-ahead logging (WAL) and a
MANIFEST ﬁle that records all changes in the system.
RocksDB supports Txs in two ways: pessismistic Txs ac-
quire locks as they go along (two-phase locking). Whereas,
optimistic Txs validate their R/W sets at the commit time.
B. Conﬁdential Computing
TEEs [15]–[19] offer a tamper-resistant conﬁdential com-
puting environment that guarantees the integrity and conﬁden-
tiality of code and data, even in the presence of a privileged
attacker (hypervisor or OS). TREATY relies on Intel SGX, a set
of x86 ISA extensions for TEE [46] that offers the abstraction
of an isolated memory, the enclave. Enclave pages reside in
the Enclave Page Cache (EPC)—a speciﬁc memory region
(94 MiB in v1, 256 MiB in v2) that is protected by an on-chip
Memory Encryption Engine. For larger enclave sizes, SGX
implements a, rather expensive, paging mechanism [23].
Conﬁdential computing frameworks leverage TEEs to se-
cure unmodiﬁed applications. They can broadly be categorized
as libOS-based systems [25], [47]–[49], and host-based sys-
tems [23], [24], [50]. All of these efforts seek to minimise
the number of enclave transitions, world switches, due to their
high cost (e.g., TLB ﬂushing, security checks [46]). TREATY
is built on top of SCONE [23] that exposes a modiﬁed libc and
combines user-level threading and asynchronous syscalls [26]
to reduce the cost of syscall execution.
C. SPEICHER Storage Engine
SPEICHER [31]
is a secure storage system based on
RocksDB and SGX that offers authenticated and secure LSM
data-structures. SPEICHER neither supports Txs nor distribu-
tion. Clients execute PUTs whose ordering is only secured
in a future synchronization point. Shutdowns/crashes in the
meantime requires clients to re-execute the operations which
might change their initial order. TREATY uses SPEICHER as
the underlying storage system but it extends the following to
support Txs processing (§ VII-B): controller, buffer manage-
ment, I/O subsystem, and LSM & logging data structures.
D. High-Performance Networking
Distributed systems mandate high-performance communi-
cation. Conventional applications use syscalls that incur the
overheads of kernel context switches [51]–[55]. Consequently,
approaches like RDMA and DPDK [56] are widely favored
for high-performance as they (i) map a device into the users
address space, and (ii) replace the costly context switches
with a polling-based approach. In our work, we build a
network stack modifying eRPC [36], a general-purpose and
asynchronous remote procedure call (RPC) library for high-
speed networking for lossy Ethernet or lossless fabrics on top
of DPDK. eRPC uses a polling-based network I/O along with
userspace drivers, eliminating interrupts and syscall overheads
from the data path which is extremely imperative in the context
of SGX. Lastly, eRPC supports a wide range of transport layers
such as RDMA, DPDK, and RoCE.
III. THREAT AND FAULT MODEL
TREATY extends the standard SGX threat model [47] to
provide stronger security guarantees even for a distributed
setting, where we also consider the untrusted storage and
network. An adversary can (1) control the entire software
stack outside the enclave (including the network stack, i.e.,
they can drop, delay, or manipulate network trafﬁc) and,
(2) view/modify all non-enclave memory, i.e., untrusted host
memory and persistent storage (SSDs). The adversary can
perform rollback attacks and revert nodes to a stale state by
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
15
intentionally shutting them down and replaying older logs.
We assume a crash-fail recovery model: nodes can crash
at any point and will eventually recover. In-memory state
is lost upon failure; persistent state (SSDs) is preserved.
TREATY guarantees serializability in the presence of failures,
and maintains data integrity, conﬁdentiality and freshness.
We do not protect against side-channel attacks: cache
timing, speculative execution [57]–[64], access pattern leak-
age [65], [66], memory safety vulnerabilities [67], [68] or
denial of service attacks.
IV. OVERVIEW
A. System Overview
Figure 1 illustrates our system architecture. TREATY is a
sharded transactional KV system, where we layer the Tx layer
that implements a secure 2PC protocol (Agreement protocol)
on the top of on a persistent KV store (SPEICHER): multiple
nodes in the system store subsets of the data and coordinate
to maintain consistency. Each node consists of two parts: 1) a
trusted set of components that resides in the enclave memory
and contains the Txs layer, lock manager, and Txs KV engine,
and 2) the untrusted network and storage stack.
Clients communicate with the system through a mutually
authenticated channel. TREATY exposes a standard transac-
tional API: Txs begin and end through BEGINTXN() and
TXNCOMMIT()/TXNROLLBACK() calls, and execute opera-
tions through TXNPUT() and TXNGET() operations. More
speciﬁcally, TREATY maintains the following properties:
• Security. TREATY guarantees conﬁdentiality, integrity and
freshness for all Txs in the presence of untrusted storage
and networking over a distributed set of nodes.
• Programmability. TREATY offers general serializable ACID
Txs, offering the strongest possible correctness guarantees,
combined with general purpose, interactive Txs that mini-
mize the programming burden on developers.
• Performance. TREATY’s careful design minimizes the per-
formance limitations of TEEs (limits on EPC memory, high
latency of trusted counter and I/O execution).
TREATY achieves security by designing two protocols:
(i) a 2PC protocol for the correct and secure execution of
distributed Txs (§ V) and, (ii) a stabilization protocol for
secure and crash-consistent persistence of the committed Txs
(§ VI). Lastly, TREATY’s substrate (§ VII) for distributed Txs
is designed and implemented with consideration to the TEEs
architectural limitations (enclave memory, I/O, scheduling).
TREATY shares the Tx execution workﬂows of existing sys-
tems. Authenticated clients start Txs by selecting a transaction
coordinator, who is responsible for driving the Tx’s execution.
Upon receiving a read or write request for a key, the relevant
node acquires respectively a R/W lock, storing it in a local
lock table. When the Tx is ready to commit, the Tx coordinator
initiates a 2PC protocol consisting of a prepare and commit
phase. The Tx commits if all involved shards vote to commit.
Otherwise, the Tx aborts. In either case, locks are released.
that
B. Design Challenges
#1: TEE for distributed transactional KV stores. In the
untrusted cloud, adversaries can tamper with (i) Txs’ execution
(e.g., compromise the conﬁdentiality and authenticity of the
running Txs and the 2PC’s state), and (ii) the KV store’s
content (e.g., unauthorized modiﬁcations to the store’s data).
For secure distributed Txs, we can rely on a simple 2PC
protocol
leverages the security guarantees of TEEs.
Unfortunately, TREATY cannot use a TEE as a black box
as its security guarantees are restricted only to the (limited
and volatile) enclave memory of a single node. In contrast,
modern transactional systems like TREATY are distributed,
communicate over the network, and store their data on a
persistent storage medium (SSDs). To implement distributed
Txs with TEEs, TREATY needs to overcome the following
system challenges.
Security and correctness for Txs. Our 2PC needs to ensure
conﬁdentiality and integrity along with serializability detecting
adversaries that aim to double execute Txs.
Untrusted persistent storage. TREATY needs to protect the per-
sistent data by detecting unauthorised modiﬁcations since at-
tackers can tamper with logs to compromise the history of ex-
ecuted Txs and the 2PC state and/or can delete/modify/access
the persistent data.
Enclave memory. TREATY needs to overcome the limited
enclave memory challenge. The limited enclave memory is
especially problematic for LSM-based systems which rely
on a large MemTable to absorb recent read/write requests
(before compacting them to the SSTable). Moreover, the Tx
layer on top of LSM storage system must also buffer the
uncommitted writes for ongoing Txs. Lastly, network buffers
for communication further pressurize EPC.
We discuss TREATY’s approach for secure distributed Txs in
§ V. TREATY offers secure and correct execution of distributed
Txs by implementing a secure 2PC (§ V-A) leveraging TEEs
and a secure network library (§ VII-A). TREATY also adopts
SPEICHER’s [31] LSM data-structures as a secure store for
the untrusted storage (§ V-B, § VII-B), but it extends and
adapts SPEICHER’s storage engine and data structures for the
Tx processing and the design of the 2PC protocol for TEEs.
#2: Networking for distributed Txs. TREATY’s nodes com-
municate with each other. Traditional kernel-based approaches
for network I/O (e.g., sockets) experience high overheads due
to context switches that are further deteriorated inside the SGX
due to the costly enclave transitions.
Conﬁdential computing frameworks, such as SCONE [23],
implement async syscalls to eliminate the expensive world
switches, but they still rely on the syscall mechanism for the
I/O, which is slow and requires two additional data copies
(enclave↔host memory↔kernel). This I/O mechanism is ill-
suited for distributed systems [2], [27]–[29], like TREATY,
that prominently rely on high-performance networking with
direct I/O or kernel by-pass. Unfortunately, these direct I/O
mechanisms are incompatible with TEEs, since TEEs prohibit
enclave memory access via the untrusted DMA connection.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
16
Transactions Layer 
Trusted Enclave Memory 
Trusted
Counter
Agreement
Protocol
eRPC
CAS
Trusted Enclave Memory 
Trusted
Counter
Agreement
Protocol
eRPC
CAS
Tx Engine 
Tx KV Engine
Network Layer
Storage-access Layer 
Persistent Storage 
NIC Memory
Scone Controller 
Operating System
SSD
Tx KV Engine
NIC Memory
Scone Controller 
Operating System
SSD
Untrusted Memory and Storage 
Untrusted Memory and Storage 
Node 1 
Node 2 
Trusted Enclave Memory 
Trusted Counter
Configuration & Attestation
Service
2PC 
Coordinators & Participants
MemTable
(keys)
eRPC
Locks Table
Tx1
Tx2 Tx3
Userland Scheduler
MemTable
(values)
Kernel
SSD
Msg
Buffer1
Msg
Buffer2
Rx
Tx
NIC
Scone Controller 
MANIFEST
WAL
Clog
Untrusted Memory and Storage 
Node k
SSTable Files
Untrusted Network 
Figure 1: TREATY’s system architecture.
Therefore, we need to adapt this mechanism in the context of
SGX to use it and design the secure distributed 2PC protocol.
TREATY implements a secure network library (§ VII-A)
through which we build the secure 2PC protocol to enable
user-space direct I/O (DPDK [56]) based on eRPC [36].
TREATY’s secure network library provides high-performant
network I/O overcoming the limitations of SGX.
#3: Secure persistency. TREATY needs to ensure that commit-
ted Txs are persisted, remain crash consistent across reboots
and are protected against forking/rollback attacks.
Trust establishment. Remote attestation (RE) ensures that the
expected code is running,
thus, protecting against forking
attacks. SGX’s RE, provided by Intel Attestation Service
(IAS) [69], veriﬁes a measurement of the enclave. Unfortu-
nately, it is designed for a single-node attestation, not offering
collective trust for distributed nodes in a data center, while
it incurs high latency (requires explicit communication with
the IAS). This can signiﬁcantly slowdown recovery after
reboots/migrations, where nodes require re-attestation.
Crash consistency. Logs are commonly used to persist the state
and updates of Txs for durability. As these logs reside in the
untrusted storage, recovery needs also to verify their freshness
and integrity.
Distributed rollback protection. Trusted counters are widely
used to protect against rollback attacks. TREATY further
extends their scope to preserve serializability where Txs are
stored along with a trusted counter value that cannot be
overwritten. Consequently, the trusted counter values reveal
Txs’ order as well as the latest trusted state of the system.
While SGX does provide us with monotonic h/w counters,
they suffer from three limitations: 1) high latency (e.g., in-
crements can take up to 250 ms [70]) 2) non-recoverability if
the CPU fails—indeed, at high-rate, counters wear out after
a couple of days [70], and 3) they cannot offer rollback
protection to a set of machines as they are private per-node.
TREATY designs a stabilization protocol—incorporated into
the 2PC–to ensure crash consistent and secure persistency for
Txs (§ VI). First, TREATY uses a Conﬁguration and Attestation
Service (CAS)—hosted within the data center to avoid the calls
to IAS—to attest all its nodes. Secondly, it provides crash
consistency for Txs through secured persistent logs. Lastly,
we build on an asynchronous trusted counter service to avoid
the SGX counter limitations and ensure distributed rollback
protection (e.g., all parts of a distributed Tx are securely
committed (persisted) to all participant nodes).
V. TRANSACTION PROTOCOL
TREATY’s 2PC protocol ensures the correct and secure
execution of distributed Txs (§ V-A). To achieve this, we
leverage TEEs to harden the security properties of the 2PC,
which we co-design with a high-performance network library
based on kernel-bypass (§ VII-A),
that guarantees strong
security for the untrusted network. To realize distributed Txs,