```
echo 1 > /sys/fs/bcache//stop
echo 1 > /sys/fs/bcache/[SSD bcache UUID]/unregister
```
6\. wipe-cache -a 清理块设备的头信息  
```
wipe-cache -a /dev/dfa1
wipe-cache -a /dev/sd[b-m]1
```
## 八、软raid on bcache
使用bcache盘，构建软RAID的例子  
1\. 4块bcache盘，创建raid5   
```
#mdadm --create --verbose /dev/md0 -c 4M --level=5 --raid-devices=4 /dev/bcache[0-3]
```
2\. 4块bcache盘，创建raid5   
```
#mdadm --create --verbose /dev/md1 -c 4M --level=5 --raid-devices=4 /dev/bcache[4-7]
```
3\. 4块bcache盘，创建raid5   
```
#mdadm --create --verbose /dev/md2 -c 4M --level=5 --raid-devices=4 /dev/bcache[89] /dev/bcache1[01]
```
4\. 2块软raid盘，创建raid0   
```
#mdadm --create --verbose /dev/md3 -c 4M --level=0 --raid-devices=2 /dev/md[01]
```
5\. 创建文件系统  
stride即条带的大小（单位512字节）  
stripe-width即条带的宽度，等于stride*实际的数据盘数量（扣掉校验盘和mirror盘）  
```
#/home/digoal/e2fsprogs/sbin/mkfs.ext4 /dev/md3 -b 4096 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stripe-width=192,stride=32 -T largefile -L md3
#/home/digoal/e2fsprogs/sbin/mkfs.ext4 /dev/md2 -b 4096 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stripe-width=96,stride=32 -T largefile -L md2
```
6\. 加载文件系统  
stripe等于创建时指定的stripe-width  
```
#mount -o defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback,stripe=192 LABEL=md3 /disk1
#mount -o defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback,stripe=96 LABEL=md2 /disk2
```
## 九、lvm on bcache
使用bcache设备构建逻辑卷。  
1\. 添加bcache lvm支持  
```
vi /etc/lvm.conf
types = [ "bcache", 16 ]
```
2\. 创建PV  
```
pvcreate /dev/bcache[0-9]
pvcreate /dev/bcache1[01]
```
3\. 创建VG  
设置一个PE的大小  
```
vgcreate -s 128M vgdata01 /dev/bcache[0-9] /dev/bcache1[01]
#vgs
  VG       #PV #LV #SN Attr   VSize  VFree 
  vgdata01  12   0   0 wz--n- 87.33t 87.33t
```
4\. 创建 lvm raid时触发BUG，LVM版本太老  
```
#lvcreate --type raid5 -i 11 -I 4M -l 100%VG -n lv01 vgdata01 
  WARNING:  RAID segment types are considered Tech Preview
  For more information on Tech Preview features, visit:
  https://access.redhat.com/support/offerings/techpreview/
  Rounding size (715392 extents) up to stripe boundary size (715396 extents)
  Volume group "vgdata01" has insufficient free space (715392 extents): 715396 required.
man lvmcreate
Sistina Software UK LVM TOOLS 2.02.87(2)-RHEL6 (2011-10-12)
dmesg
[32634.575210] device-mapper: raid: Supplied region_size (1024 sectors) below minimum (8943)
[32634.583958] device-mapper: table: 253:24: raid: Supplied region size is too small
[32634.592008] device-mapper: ioctl: error adding target to table
```
原因，测试环境为CentOS 6.3以前的版本，所以LVM2版本很老，存在BUG    
https://bugzilla.redhat.com/show_bug.cgi?id=837927  
```
    RAID: Fix problems with creating, extending and converting large RAID LVs
    MD's bitmaps can handle 2^21 regions at most.  The RAID code has always
    used a region_size of 1024 sectors.  That means the size of a RAID LV was
    limited to 1TiB.  (The user can adjust the region_size when creating a
    RAID LV, which can affect the maximum size.)  Thus, creating, extending or
    converting to a RAID LV greater than 1TiB would result in a failure to
    load the new device-mapper table.
    Again, the size of the RAID LV is not limited by how much space is allocated
    for the metadata area, but by the limitations of the MD bitmap.  Therefore,
    we must adjust the 'region_size' to ensure that the number of regions does
    not exceed the limit.  I've added code to do this when extending a RAID LV
    (which covers 'create' and 'extend' operations) and when up-converting -
    specifically from linear to RAID1.
Fix verified in the latest rpms.
2.6.32-348.el6.x86_64
lvm2-2.02.98-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
lvm2-libs-2.02.98-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
lvm2-cluster-2.02.98-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
udev-147-2.43.el6    BUILT: Thu Oct 11 05:59:38 CDT 2012
device-mapper-1.02.77-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
device-mapper-libs-1.02.77-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
device-mapper-event-1.02.77-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
device-mapper-event-libs-1.02.77-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
cmirror-2.02.98-6.el6    BUILT: Thu Dec 20 07:00:04 CST 2012
```
5\. 创建普通lvm正常(-I 大点，对于OLAP系统更好)  
```
# lvcreate -i 12 -I 4M -l 100%VG -n lv01 vgdata01 
  Logical volume "lv01" created
# lvs
  LV   VG       Attr   LSize  Origin Snap%  Move Log Copy%  Convert
  lv01 vgdata01 -wi-a- 87.33t 
# /home/digoal/e2fsprogs/sbin/mkfs.ext4 /dev/mapper/vgdata01-lv01 -b 4096 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stripe-width=24,stride=2 -T largefile -L lv01
# mount -o defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback,stripe=24 LABEL=lv01 /disk1
```
6\. 使用新版本LVM2解决BUG  
更新LVM2版本    
https://sourceware.org/lvm2/  
```
# tar -zxvf LVM2.2.02.165.tgz
# cd LVM2.2.02.165
# sudo 
# ./configure --prefix=/home/digoal/lvm2 ; make -j 32 ; make install
#export LD_LIBRARY_PATH=/home/digoal/lvm2/lib:$LD_LIBRARY_PATH
#export PATH=/home/digoal/lvm2/sbin:$PATH
#export MANPATH=/home/digoal/lvm2/share/man:$MANPATH
# /home/digoal/lvm2/sbin/pvs
  PV            VG       Fmt  Attr PSize PFree
  /dev/bcache0  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache1  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache10 vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache11 vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache2  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache3  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache4  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache5  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache6  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache7  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache8  vgdata01 lvm2 a--  7.28t 7.28t
  /dev/bcache9  vgdata01 lvm2 a--  7.28t 7.28t
# man /home/digoal/lvm2/share/man/man8/lvcreate.8
```
6\.1\. 创建2个raid5逻辑卷，分别使用8块，4块bcache盘。    
lvcreate -i 表示实际的数据盘数量（需要扣除校验盘，mirror盘）。  
lvcreate -I 单位KB，表示写多少内容后开始写下一个数据盘，即表示条带的大小。  
mkfs.ext4 stride=16表示条带大小，单位扇区（512字节），stripe-width=112 表示条带宽度（=stride * -i）。  
```
#/home/digoal/lvm2/sbin/lvcreate --type raid5 -i 7 -I 64 -l 100%PVS -n lv01 vgdata01 /dev/bcache[0-7]
  Rounding size 58.22 TiB (476928 extents) up to stripe boundary size 58.22 TiB (476931 extents).
  Logical volume "lv01" created.
#/home/digoal/e2fsprogs/sbin/mkfs.ext4 /dev/mapper/vgdata01-lv01 -b 4096 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stripe-width=112,stride=16 -T largefile -L lv01
#mount -o defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback,stripe=112 LABEL=lv01 /disk1
/home/digoal/lvm2/sbin/lvcreate --type raid5 -i 3 -I 64 -l 100%PVS -n lv02 vgdata01 /dev/bcache[89] /dev/bcache1[01]
/home/digoal/e2fsprogs/sbin/mkfs.ext4 /dev/mapper/vgdata01-lv02 -b 4096 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stripe-width=48,stride=16 -T largefile -L lv02
mount -o defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback,stripe=48 LABEL=lv02 /disk2
```
6\.2\. 创建raid10逻辑卷  
```
/home/digoal/lvm2/sbin/lvcreate --type raid10 -i 6 -I 128 -l 100%VG -n lv01 vgdata01
/home/digoal/e2fsprogs/sbin/mkfs.ext4 /dev/mapper/vgdata01-lv01 -b 4096 -m 0 -O extent,uninit_bg -E lazy_itable_init=1,stripe-width=192,stride=32 -T largefile -L lv01
mount -o defaults,noatime,nodiratime,nodelalloc,barrier=0,data=writeback,stripe=192 LABEL=lv01 /disk1
```
## 十、配置参数或模块参数参考 
有些值可以被修改，达到调整的目的。    
1\. SYSFS - BACKING DEVICE  
```
Available at /sys/block//bcache, /sys/block/bcache*/bcache and
(if attached) /sys/fs/bcache//bdev*
attach
  Echo the UUID of a cache set to this file to enable caching.
cache_mode
  Can be one of either writethrough, writeback, writearound or none.
clear_stats
  Writing to this file resets the running total stats (not the day/hour/5 minute
  decaying versions).
detach
  Write to this file to detach from a cache set. If there is dirty data in the
  cache, it will be flushed first.
dirty_data
  Amount of dirty data for this backing device in the cache. Continuously
  updated unlike the cache set's version, but may be slightly off.
label
  Name of underlying device.
readahead
  Size of readahead that should be performed.  Defaults to 0.  If set to e.g.
  1M, it will round cache miss reads up to that size, but without overlapping
  existing cache entries.
running
  1 if bcache is running (i.e. whether the /dev/bcache device exists, whether
  it's in passthrough mode or caching).
sequential_cutoff
  A sequential IO will bypass the cache once it passes this threshold; the
  most recent 128 IOs are tracked so sequential IO can be detected even when
  it isn't all done at once.
sequential_merge
  If non zero, bcache keeps a list of the last 128 requests submitted to compare
  against all new requests to determine which new requests are sequential
  continuations of previous requests for the purpose of determining sequential
  cutoff. This is necessary if the sequential cutoff value is greater than the
  maximum acceptable sequential size for any single request.
state
  The backing device can be in one of four different states:
  no cache: Has never been attached to a cache set.
  clean: Part of a cache set, and there is no cached dirty data.
  dirty: Part of a cache set, and there is cached dirty data.
  inconsistent: The backing device was forcibly run by the user when there was
  dirty data cached but the cache set was unavailable; whatever data was on the
  backing device has likely been corrupted.
stop
  Write to this file to shut down the bcache device and close the backing
  device.
writeback_delay
  When dirty data is written to the cache and it previously did not contain
  any, waits some number of seconds before initiating writeback. Defaults to
  30.
writeback_percent
  If nonzero, bcache tries to keep around this percentage of the cache dirty by
  throttling background writeback and using a PD controller to smoothly adjust
  the rate.
writeback_rate
  Rate in sectors per second - if writeback_percent is nonzero, background
  writeback is throttled to this rate. Continuously adjusted by bcache but may