packet rate, i.e. it rises from about 0600 to 0900, falls from about 1700 to 2000,
then rises again in the evening. Unlike the packet rate, however, the number of
streams rose while the traﬃc rate fell around midnight on Friday 1 Oct 04. That
rise was not repeated over the weekend; it appears to have been a one-oﬀ event
(e.g. a database replication job copying many tiny ﬁles) rather than part of the
diurnal pattern.
Some Observations of Internet Stream Lifetimes
271
At regular three-hour intervals we see a short, high step in the number of
streams. Our network security team were well aware of this; they are investi-
gating. We believe that such steps are caused by some sort of network attack.
Similarly, every day at 1630 we see a bigger spike. We have also observed other,
less regular, spikes taking the number of active streams as high as 140,000. Fig. 6
shows more detail for two of these spikes.
Packets and Active Streams each second at Auckland, on Fri 1 Oct 04 (NZST)
packet/s
active streams
16:26 16:28 16:30 16:32 16:34 16:36 16:38 16:40 16:42 16:44
local time
packet/s
active streams
count
 100000
 10000
 1000
count
 100000
 10000
 1000
21:16 21:18 21:20 21:22 21:24 21:26 21:28 21:30 21:32 21:34
local time
Fig. 6. Details of ﬁg. 5 showing spike in streams at 1633, and step at 2120
4 Usage Metering at Auckland
For usage accounting at Auckland we want to ignore streams with K or fewer
packets. To help select a K value, we plotted distributions of byte density vs
stream size (packets). Fig. 7 shows distributions for inbound (lower traces) and
outbound (upper traces) byte-percentage distributions for ten-minute sample
intervals from three hours from 2100 on Friday 1 October 2004. For most of
those intervals it seems that we could ignore streams with six or fewer packets in
either direction. However, there is one outbound trace, for the interval ending at
2120, which has 29% of its bytes in streams with only one or two packet. Fig. 6
shows that at 2120 the number of active streams had risen sharply.
Table 1 shows that the interval ending at 2120 had two unusual features:
a high inbound UDP traﬃc rate, and a low outbound non-web TCP traﬃc
rate. We examined the ten intervals with their highest proportion of bytes in
short streams. Few of those had low outbound non-TCP rates, but all had high
UDP inbound rates. We hypothesise that the step in streams was caused by
an inbound address or port scan, i.e. a ﬂood of single-packet UDP streams.
272
N. Brownlee
%
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 0
Cumulative % bytes vs Stream Size (packets), Auckland, Fri 1 Oct 04 (NZST)
outbound bytes v pkts
inbound bytes v pkts
 5
 10
 15
 20
 25
 30
 35
 40
packets
Fig. 7. Byte density vs packets in stream for three hours at Auckland, from 2100 on
Friday, 1 Oct 2004
Table 1. Inbound and outbound traﬃc rates (Mb/s) for various Kinds of traﬃc on
Friday 1 October 2004
Inbound rate UDP non-web web SSL other
2110
2120
2130
0.15
1.66
0.21
2.91 8.85 0.51 0.03
2.23 10.15 0.52 0.04
1.37 9.86 0.50 1.09
Outbound rate UDP nonweb web SSL other
1.47 3.31 0.73 0.03
0.92 3.34 0.850 0.03
3.71 3.54 0.859 0.07
2110
2120
2130
0.10
0.10
0.10
Although few of those inbound UDP probe packets elicited any response, those
that did increased the proportion of bytes in small streams enough to dominate
the outbound traﬃc.
Since U Auckland has about ﬁve times as many inbound traﬃc bytes as
it does outbound, we plotted the total (inbound+outbound) byte-percentage
distributions for every ten-minute interval over 1-2 October 2004, producing
ﬁg. 8. We often see intervals when the short streams contribute a signiﬁcant
proportion of the total link bytes, suggesting that we should not simply “focus
on the elephants” for our usage measurements.
Some Observations of Internet Stream Lifetimes
273
Cumulative % bytes vs Stream Size (packets), Auckland, 1-2 Oct 04 (NZST)
in+out bytes v pkts
 5
 10
 15
 20
 25
 30
 35
 40
packets
%
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 0
Fig. 8. Total (inbound+outbound) byte density vs packets in stream at Auckland, 1-2
October 2004
5
Ignoring Short Streams at Auckland
Our observations in section 4 suggest that on our link, intervals when traﬃc is
dominated by short streams are caused by network attacks (plagues of dragon-
ﬂies). Although we need to know about those for security monitoring, they are
less important for usage accounting. We decided to try metering while ignoring
streams with six or fewer packets (total in both directions).
We ran the meter with K = 6 for ﬁve days, using our normal ‘usage account-
ing’ ruleset. All ﬁve days were similar (including regular three-hourly spikes and
a daily spike at 1640); ﬁg. 9 shows the packet rate (lower trace), active streams
(middle trace) and ﬂows (upper trace) for every second of Thursday, 7 Octo-
ber 2004. The packet rate and streams traces are similar to those in ﬁg. 5; the
number of ﬂows is stable and tracks the packet rate.
Fig. 10 shows the three hours from 2200 in more detail. The number of active
ﬂows rises steadily as new ﬂows appear, and falls rapidly when ﬂows are read
every ten minutes, allowing the meter to recover ﬂow table space for newly-
inactive ﬂows. The ‘sawtooth’ behaviour, clearly visible in the plot, is thus an
artifact of the RTFM architecture. (The important point here is that when we
ignore small ﬂows, the average number of active ﬂows remains stable over long
periods, minimising the load on the ﬂow data collection system.)
Fig. 10 also shows that when the number of active streams increases sharply
(showing spikes and steps), the number of ﬂows is not aﬀected. That supports
our hypothesis that such ‘attack’ increases are caused by bursts of short-lived
streams.
274
N. Brownlee
Fig. 9. Packet rate, active streams and active ﬂows at one-second intervals at Auckland
for Thursday, 7 October 2004. Our NeTraMet meter used K = 6, i.e. streams with six
or fewer packets were not matched to ﬂows
Fig. 10. Detail of ﬁg. 9 showing spike and steps in streams, and sawtooth variation in
ﬂows
Some Observations of Internet Stream Lifetimes
275
% packets and bytes ignored at Auckland, Mon 14 - Tue 15 Dec 04 (NZDT)
ignored byte %
ignored packet %
%
30
10
3
1
0.3
00:00
12/15
04:00
12/15
08:00
12/15
12:00
12/15
16:00
12/15
20:00
12/15
00:00
12/16
local time
Fig. 11. Percent of bytes and packets ignored at ﬁve-minute intervals at Auckland,
measured using K = 6, i.e. ignoring streams with six or fewer packets
To verify our estimate that ignoring streams with six or fewer packets would
exclude between 5% and 10% of the total bytes, we modiﬁed NeTraMet so as
to collect distributions of the ignored packets and bytes as a function of stream
size. One day of typical ‘ignored’ data, collected at 5-minute sample intervals, is
shown in ﬁg. 11.
The ‘ignored packet’ percentage (upper trace) generally varies between about
2.5% and 8%. Furthermore, its average varies inversely with the average packet
rate, suggesting that small (dragonﬂy) streams provide a more or less constant
background all the time, with their ‘ignored’ percentage more obvious when the
packet rate is low.
For about 95% of our sample intervals, between 0.5% and 2% of the bytes were
ignored (lower trace). In the other 5% of the intervals we saw large spikes in the
packet rate and in the number of active streams, as shown earlier in ﬁg. 9. During
those spikes, 10% to 30% of the bytes were ignored. Overall, the percentage
of bytes ignored is acceptably low, with high ‘ignored byte’ percentages only
occurring during attack events.
6 Conclusion
At Auckland we see frequent bursts of incoming ‘attack’ streams, which can
dominate the traﬃc mix on our Internet gateway. We believe that traﬃc engi-
neering and accounting approaches that ignore streams with six or fewer packets
(K = 6) means that in the long term only about 2% of our total bytes are not
276
N. Brownlee
measured as ‘user’ traﬃc. In return we achieve a signiﬁcant reduction in the
number of ﬂows we have to create, read, store and process.
However, for some traﬃc mixes, this sampling bias against small ﬂows can
radically warp the inferences one makes about the aggregate traﬃc.
We are continuing our investigation of stream behaviour, especially that relat-
ing to the ‘attack streams’ (plague of dragonﬂies) events. We have not observed
these on the California backbone link, where traﬃc levels are much higher and
there is more statistical mixing, but such ‘attack streams’ probably do appear
there.
An alternative approach is the adaptive one proposed in [7], which adapts
its sampling parameters to the traﬃc in real time That approach avoids the
bias against small ﬂows and should give a true picture of the actual traﬃc load,
within its sampling limitations. Our approach, however, may be more useful for
accounting applications, since we are not sampling. Instead we preserve detail
for all the larger streams (which we can bill to a user) while ignoring the small
‘attack’ streams (which are overhead, not billable to a user).
W
ork
Future
6.1
We are continuing to investigate the plague of dragonﬂies events at Auckland.
We would like to improve our network attack detection ability by recognising
and reporting frequently-occurring attack patterns. The ability to summarise
large groups of small streams would also reduce the number of packets we ignore
in our traﬃc monitoring.
At this stage it is clear that NeTraMet can handle our network’s data rate
at 100 Mb/s. We are conﬁdent that this can be done – without having to use
sampling techniques – at 1 Gb/s.
The Need for Ongoing Measurements
6.2
At U Auckland we use NeTraMet for usage accounting and traﬃc analysis,
Snort 1 and Argus 2 for security monitoring, and MRTG 3 for traﬃc engineering.
Each of these tools is specialised so as to perform its intended function well, but
there is little overlap between the tools. Indeed, when an unusual event occurs
on the network, it can be useful to have data from many tools, providing many
diﬀerent views of that event.
We believe, therefore, that every large network should collect traﬃc data on
an ongoing basis, using several diﬀerent tools. The work required to support such
monitoring is well justiﬁed by the ability it provides to investigate incidents soon
after they occur. In addition, the understanding gained about the network, its
traﬃc, and the ways that traﬃc changes over time, provides a sound basis for
long-term improvements in the network’s performance and in service to it’s users.
1 Security Monitoring, http://www.snort.org/
2 Network Auditing, http://www.qosient.com/argus/
3 Traﬃc Rate Monitoring, http://people.ee.ethz.ch/ oetiker/webtools/mrtg/
Some Observations of Internet Stream Lifetimes
277
Acknowledgement
Support for this work is provided by DARPA NMS Contract N66001-01-1-8909,
NSF Award NCR-9711092 ‘CAIDA: Cooperative Association for Internet Data
Analysis,’ and The University of Auckland.
References
1. C. Estan and G. Varghese, New directions in traﬃc measurement and accounting:
Focusing on the Elephants, Ignoring the Mice, ACM Transactions on Computer
Systems, August 2003
2. N. Brownlee and M. Murray, Streams, Flows and Torrents, PAM2001, April 2001
3. N. Brownlee, Using NeTraMet for Production Traﬃc Measurement, Intelligent Man-
agement Conference, IM2001, May 2001
4. N. Brownlee and K. Claﬀy, Understanding Internet Stream Lifetimes: Dragonﬂies
and Tortoises, IEEE Communications magazine, October 2002
5. K. Claﬀy, G. Polyzos and H-W. Braun, A parameterisable methodology for Internet
traﬃc ﬂow proﬁling, IEEE Journal on Selected Areas in Communications, 1995.
6. N. Brownlee, C. Mills and G. Ruth, Traﬃc Flow Measurement: Architecture, RFC
2722, October 1999.
7. C. Estan, K. Keys, D. Moore and G. Varghese, Building a better NetFlow, SIG-
COMM, September 2004