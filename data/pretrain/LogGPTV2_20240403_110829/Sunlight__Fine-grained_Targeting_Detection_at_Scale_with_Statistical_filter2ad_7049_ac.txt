potheses, validate them in Stages 3 and 4, and then “interpret” those 
with low enough p-values. That would result in potentially mislead­
ing conclusions.  For example, just because a hypothesis based on 
a logistic model can be validated with low p-values, it does not fol­
low that the corresponding disjunctive version of that hypothesis is 
also statistically signiﬁcant.  For this reason, the Sunlight method­
ology critically includes this explicitly interpretability stage, which 
reminds  a  developer  to  transform  her  hypothesis  early  for  inter­
pretability so the p-values can be computed for that hypothesis. 
4.3  Stage 3: Hypothesis Testing 
The second stage of the analysis considers the targeting hypothe­
ses (disjunctions of inputs and an associated output) generated from 
the ﬁrst stage and provides a conﬁdence score for each hypothesis. 
The score, a p-value, comes from an exact statistical test that de­
cides between a null hypothesis H0 that the disjunction of inputs is 
independent of the associated output, and an alternative hypothesis 
H1  that the disjunction of inputs is positively correlated with the 
output.  A small p-value—0.05 by convention—implies that our 
observations on the data (discussed below) are unlikely to be seen 
under  H0;  it lends conﬁdence in rejecting H0  and accepting H1 
and the validity of the targeting hypothesis. 
Computing p-values. The test is based on computing a test statis­
tic on the testing set (i.e., the subset of proﬁles that were not used 
to  generate  targeting  hypotheses).  A  critical  assumption  here  is 
that the proﬁles (and speciﬁcally, the outputs associated with each 
proﬁle)  are  statistically  independent,  and  hence  the  selected  dis­
junction is also independent of the proﬁles in the testing set.  The 
speciﬁc test statistic T  we use is an association measure based on 
Pearson’s correlation:  we compute T  using the proﬁles from the 
testing set, and then determine the probability mass of the interval 
{t  2  R  :  t  2  T }  under  H0.  This  probability  is  precisely  the 
p-value we seek.  Because the distribution of the inputs for each 
proﬁle is known (and, in fact, controlled by us), it is straightfor­
ward to determine the exact distribution of T  under H0. For exam­
ple, when the inputs for each proﬁle are determined with indepen­
dent but identically distributed coin tosses, the p-value computation 
boils down to a simple binomial tail calculation. 
i=B i 
(N )↵k
the p-value is PN 
More speciﬁcally, suppose the inputs are independent and iden­
tically distributed binary random variables with mean ↵ 2  (0, 1). 
Consider a disjunction of k inputs and a particular (binary-valued) 
output.  Let N  be the number of proﬁles for which the output is 1, 
and let B be the number of proﬁles for which both the disjunction 
and the output are 1.  If N  = 0, then the p-value is 1.  Otherwise, 
i (1-↵k)N -i where ↵k  = 1-(1-↵)k .
Use of p-value in Stage 2.  As previously mentioned, we also use 
this p-value computation in Stage 2 as a rough heuristic for decid­
ing which disjunctions to keep and pass on to Stage 3. However, we 
stress that these Stage 2 p-value computations are not valid because 
the disjunctions are formed using the proﬁles from the training set, 
and hence are not independent of these same training set proﬁles. 
The validity of the Stage 3 p-values, which are based on the testing 
set, relies on the independence of the disjunction formulae and the 
testing set proﬁles themselves. 
Independence  assumption.  It  is  possible  to  weaken  the  inde­
pendence assumption by using different non-parametric statistical 
tests, as is done in [8]. Such tests are often highly computationally 
intensive  and  have  lower  statistical  power  to  detect  associations. 
We opt to admit the assumption of independence in favor of ob­
taining more interpretable results under the assumption; gross vi­
olations may be identiﬁed in follow-up studies by an investigator, 
which we anyway recommend in all cases. 
Causal effects.  Under the alternative hypothesis of a positive cor­
relation between a disjunction of inputs and an output, it is possible 
to draw a conclusion about the causal effect of the inputs on the 
output. Speciﬁcally, if the input values are independently assigned 
for each user proﬁle,  then a positive correlation between a given 
disjunction of inputs and an output translates to a positive average 
causal effect [24]. This independent assignment of input values can 
be ensured in the creation of the user proﬁles. 
4.4  Stage 4: Multiple Testing Correction 
In the ﬁnal stage of our analysis methodology, we appropriately 
adjust the p-values for each of our targeting hypotheses to correct 
for the multiple testing problem.  As one simultaneously considers 
more and more statistical tests, it becomes more and more likely 
that the p-value for some test will be small just by chance alone 
even when the null hypothesis H0  is true. If one simply rejects H0 
whenever the stated p-value is below  0.05 (say),  then this effect 
often leads to erroneous rejections of H0  (false rejections). 
This multiple testing problem is well-known and ubiquitous in 
high-throughput sciences (e.g., genomics [10]), and several statisti­
cal methods have been developed to address it. A very conservative 
correction is the Holm-Bonferroni method [17], which adjusts the 
p-values (generally making them larger them by some amount) in 
a way so that the probability of any false rejection of H0  (based on 
comparing adjusted p-values to 0.05) is indeed bounded above by 
0.05. While this strict criterion offers a very strong guarantee on the 
resulting set of discoveries, it is often overly conservative and has 
low statistical power to make any discoveries at all. A less conser­
vative correction is the Benjamini-Yekutieli procedure [3], which 
guarantees that among the adjusted p-values that are less than 0.05, 
the expected fraction that correspond to false discoveries (i.e., false 
rejections of H0) is at most 0.05.  Although this guarantee on the 
expected false discovery rate is weaker than what is provided by the 
Holm-Bonferroni method, it is widely accepted in applied statistics 
as an appropriate and preferred correction for exploratory studies. 
With either correction method, the adjusted p-values provide a 
more accurate and calibrated measure of conﬁdence relative to the 
nominal 0.05 cut-off.  We can either return the set of targeting hy­
potheses  whose  p-values  fall  below  the  cut-off,  or  simply  return 
the  list  of  targeting  hypotheses  ordered  by  the  p-values.  Either 
way, the overall analysis produced by this methodology is highly-
interpretable and statistically justiﬁed. 
4.5  Prototype 
We implemented Sunlight in Ruby using statistical routines (e.g., 
Lasso) from R, a programming environment for statistical comput-
ing.The  analysis  is  built  around  a  modular  pipeline  that  lists  the 
algorithms to use for each stage, and each algorithm implements a 
basic protocol to communicate with the next stage. 
Default Pipeline.  Fig.2 shows the default pipeline used by Sun­
light.  In Stage 1, we use sparse logistic regression (Logit) to esti­
mate regression coefﬁcients that give an ordering over the inputs. 
In  Stage  2,  we  select  a  disjunction  (i.e.,  an  “OR”  combination) 
with the best predictive accuracy from ordered inputs from Stage 
1, and discard inaccurate hypotheses as determined using heuristic 
p-value computations.  Stage 3 computes the p-values for the sta­
tistical test of independence on the test data.  Finally, our Stage 4 
implementation computes both the Benjamini-Yekutieli (BY) and 
Holm-Bonferroni  (Holm)  corrections,  though  our  default  recom­
mendation is the BY correction.  Finally, we recommend p-values 
< 0.05 for good conﬁdence. 
In §6, we show that these defaults strike a good balance between 
the scalability and the conﬁdence of these hypotheses. Using these 
defaults, our targeting experiments on Gmail and on the web pro­
duced the largest number of high conﬁdence hypotheses, and we 
have manually inspected many of these hypotheses to validate the 
results. We describe these measurements next. 
5  Sunlight Use Cases 
To showcase Sunlight, we explored targeting in two ad ecosys­
tems with two experiments, on Gmail ads and ads on the web re­
spectively. We used the datasets generated from these experiments 
for two purposes:  (1) to evaluate Sunlight and compare its perfor­
mance to prior art’s (§6) and (2) to study a number of interesting 
aspects about targeting in these ecosystems (§7).  As foreshadow­
ing for our results, both experiments revealed contradictions of sep­
arate statements from Google policies or ofﬁcial FAQs.  While our 
use cases refer exclusively to ad targeting detection, we stress that 
our method is general and (intuitively) should be applicable to other 
forms of targeting and personalization (e.g., §6.2 shows its effec­
tiveness on Amazon’s and YouTube’s recommendation systems). 
Figure 4: Sample targeted ads from the 33-day Gmail experiment. 
5.1  Gmail Ads 
As a ﬁrst example of personal data use, we turn to Gmail which, 
until November last year, offered personalized advertisements tai­
lored to a user’s email content.  We selectively placed more than 
300 emails containing single keywords or short phrases to encode 
a variety of topics, including commercial products (e.g.  TV, cars, 
clothes) and sensitive topics (e.g., religion, sexual orientation, health) 
into 119 proﬁles.  The emails were manually written by us by se­
lecting topics and writing keywords related to this topic.  The ﬁrst 
column of Figure 4 shows examples of emails we used. The topics 
were selected from the AdSense categories  [12], with other sensi­
tive forbidden by the AdWords policies  [13]. 
The proﬁles were Gmail accounts created speciﬁcally to study 
Gmail targeting.  Because creating Gmail accounts is costly, some 
accounts were reused from previous studies, and already contained 
some  emails.  The  emails  relevant  to  this  study  were  different, 
and assigned independently from previous emails, so our statistical 
guaranties still hold.  To perform the independent assignment each 
email was sent to each account with a given probability (in this case 
0.2).  Emails were sent from 30 other Gmail accounts that did not 
otherwise take part in the study. No account from the study sent an 
email to another account of the study. Finally we collected targeted 
ads by calling Google’s advertising endpoints the same way Gmail 
does, looping over each email and account ten times. 
Our goal was to study (1) various aspects related to targeted ad­
vertisements,  such  as  how  frequent  they  are  and  how  often  they 
appear in the context of the email being targeted (a more obvious 
form of targeting) versus in the context of another email (a more 
obscure form of targeting) and (2) whether advertisers are able to 
target their ads to sensitive situations or special groups deﬁned by 
race,  religion etc.  We collected targeting data for 33 days,  from 
Oct.  8 to Nov.  10, 2014 when Google abruptly shut down Gmail 
ads. One might say that we have the last month of Gmail ads. 2 
Before Google disabled Gmail ads, we collected 24,961,698 im­
pressions created collectively by 19,543 unique ads.  As expected, 
the distribution of impressions per ad is skewed:  the median ads 
were observed 22 times,  while the top 25/5/1% of ads were ob­
served 217/4,417/20,516 times. We classify an ad as targeted if its 
statistical conﬁdence is high (corrected p-value<  0.05 with Sun­
light’s default pipeline). In our experiment, 2890 unique ads (15% 
of all) were classiﬁed as targeted. While we observe that ads classi­
ﬁed as targeted are seen more often (1159 impressions for the me­
dian targeted ads), this could be an artifact as most ads seen only 
occasionally present insufﬁcient evidence to form hypotheses. 
Figure 4 shows some examples of ads Sunlight identiﬁed as tar­
geted, along with the content of the emails they targeted, the cor­
rected p-values, and information about the context where the im­
pressions appeared. Some ads show targeting on single inputs while 
others show targeting on combinations of emails. We selected these 
examples by looking at all ads that were detected as targeting the 
sensitive emails we constructed, and choosing representative ones. 
When multiple interesting examples were available, we chose those 
with a lot of data, or that we detected across multiple days, as we 
are more conﬁdent in them. 
Notably, the examples show that information about a user’s health, 
race,  religious  afﬁliation  or  religious  interest,  sexual  orientation, 
or difﬁcult ﬁnancial situation, all generate targeted advertisements. 
Our system cannot assign intention of either advertisers or Google 
for the targeting we found, but this appears to contradict a statement 
in an ofﬁcial-looking Gmail Help page: 3 
2Gmail now has email “promotions;” we did not study those. 
3The page containing this statement used to be accessible through
a user’s own account (Gmail - Help - Security & privacy - Privacy 
Targeted(website 
drugs.com 
ads(Title(&!text! 
Nasalcrom 
Proven to Prevent Nasal 
Alergy Symptoms 
hightimes.com 
AquaLab(Technologies 
Bongs, Pipes, and Smoke 
Accessories 
foxnews.com 
IsraelBonds.com 
Invest in Israel 
huffingtonpost.com 
Stop(The(Tea(Party 
Support Patrick Murphy 
economist.com 
The(Economist 
pcgamer.com 
(games) 
soberrecovery.com 
(rehab) 
Great Minds Like a Think - 
Introductory Offer 
Advanced(PCs(Digital(Storm 
Starting at $699 
Elite(Rehab 
Speak w/ a counselor now 
s
g
u
r
D
s
w
e
N
.
c
s
i
M
Results 
p&value!=!2.5e&5 
374!impressions 
in 73 profiles 
41%!in!context 
p&value!=!2.6e&13 
1714!impressions 
in 76 profiles 
99%!in!context 
p&value!=!0.0041 
71!impression 
in 45 accounts 
100%!in!context 
p&value!=!0.010 
97!impressions 
in 37 profiles 
100%!in!context 
p&value!=!0.00066 
151!impressions 
in 77 profiles 
0%!in!context 
p&value!=!0.035 
575!impressions 
in 129 profiles 
66%!in!context 
p&value!=!6.8e&6 
5486!impressions 
82 profiles 
99%!in!context 
Figure 5:  Sample targeted ads from the display-ads experiment (also 
called Website experiment). 
Only ads classiﬁed as Family-Safe are displayed in Gmail. 
We are careful about the types of content we serve ads against. 
For  example,  Google  may  block  certain  ads  from  running 
next to an email about catastrophic news. We will also not tar­
get ads based on sensitive information, such as race, religion, 
sexual orientation, health, or sensitive ﬁnancial categories. 
– support.google.com/mail/answer/6603. 
While our results do not imply that this targeting was intentional 
or  explicitly  chosen  by  any  party  involved  (Google,  advertisers, 
etc.), we believe they demonstrate the need for investigations like 
the ones Sunlight supports. We also point out that those violations 
are needles in a haystack.  Several topics we included in our ex­
periment (e.g.,  fatal diseases and loss) generated not a single ad 
classiﬁed as targeted. 
§7 presents further results about targeting on Gmail. 
5.2  Display Ads on the Web 
As a second example of personal data use, we look at targeting of 
arbitrary ads on the web on users’ browsing histories. This experi­
ment is not speciﬁcally related to Google, though Google is one of 
the major ad networks that serve the ads we collect. Similar to the 
Gmail use case, our goal is to study aspects such as frequency of 
targeted ad impressions, how often they appear in the context of the 
website being targeted versus outside, and whether evidence of tar­
geting on sensitive websites (e.g., health, support groups, etc.) ex­
ists. We populate 200 browsing proﬁles with 200 input sites chosen 
randomly from the top 40 sites across 16 different Alexa categories, 
such as News, Home, Science, Health, and Children/Teens.  Each 
website is randomly assigned to each proﬁle with a probability 0.5. 
For each site, we visit the top 10 pages returned from a site-speciﬁc 
search on Google. We use Selenium  [25] for browsing automation. 
We collect ads from the visited pages using a modiﬁed version of 
AdBlockPlus [1] that detects ads instead of blocking them.  After 
collecting data,  we use Sunlight’s default pipeline and a p-value 
< 0.05 to assess targeting. 
policies) and its look and feel until 12/24/2014 was more ofﬁcial
than it currently is.  The 2014 version is available on archive.org
(https://web.archive.org/web/20141224113252/https: 
//support.google.com/mail/answer/6603). 
We  collect  19,807  distinct  ads  through  932,612  total  impres­
sions.  The web display ads we collected skewed to fewer impres­
sions than those we collected in the Gmail experiment. The median 
ad appears 3 times and we recorded 12/126/584 impressions for the 
top 25/5/1% of display ads. In this experiment, 931 unique ads (5% 
of all) were classiﬁed as targeted, and collectively they are respon­
sible from 37% of all impressions. 
Figure 5 shows a selection of ads from the study, chosen sim­
ilarly as the ads from the Gmail study.  Among the examples are 
ads targeted on marijuana sites and drug use sites. Many of the ads 
targeted on drug use sites we saw,  such as the “Aqua Lab Tech­
nologies”  ad,  advertise  drug  paraphernalia  and  are  served  from 
googlesyndication.com.  This appears to contradict Google’s 
advertising policy, which bans “Products or services marketed as 
facilitating recreational drug use.” – https://support.google. 
com/adwordspolicy/answer/6014299. 
§7 presents further results about targeting on the web. 
6  Evaluation 