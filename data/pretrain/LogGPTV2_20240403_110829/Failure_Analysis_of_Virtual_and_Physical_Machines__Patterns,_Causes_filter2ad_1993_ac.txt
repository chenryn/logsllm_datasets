which take a short time to repair, as illustrated in the following
paragraph. To facilitate the reliability modeling analysis, we ﬁt
the empirical distribution with several statistical distributions,
i.e., Gamma, Log-normal and Weibull. According to log
likelihood of ﬁtting, the repair times of PMs and VMs can
be best described by the Log-normal distribution with speciﬁc
parameters directly summarized in Fig. 4.
We further break down the repair time across the failure
classes and summarize their mean and median values in
Table IV. In all failure classes, the mean is much higher than
the median, indicating a high variability in repair times. This is
to be expected, since each of the ﬁve subsystems are serviced
s
e
r
u
l
i
a
F
f
o
n
o
i
t
c
a
r
F
.
l
u
m
u
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
VM
VM LogN μ=1.14 σ=2.88
PM
PM LogN μ=1.80 σ=2.07
 0
 50
 100
[hours]
 150
 100
 80
 60
 40
 20
]
%
[
s
e
r
u
l
i
a
F
f
o
n
o
i
t
c
a
r
F
 0
 200
Fig. 4. CDF of repair time in hours relative to PMs and VMs.
MEAN AND MEDIAN REPAIR TIMES IN HOURS FOR DIFFERENT FAILURE
TABLE IV
CLASSES.
mean
median
HW Net
80.1
67.6
8.97
8.28
Power
12.17
0.83
Reboot
18.03
2.27
SW
30.0
22.37
by different support groups. The shortest repair times are
experienced for power related failures (0.83 hours for the 50%
percentile), because in most cases: (1) their severity is critical,
which means such incidents are immediately handled by the
support teams, and (2) the resolution requires a simple elec-
trical ﬁx. As expected, reboot related failures take the second
shortest repair times, as servers typically resume their services
soon after the actual failure. Both hardware and network
related failures require signiﬁcantly longer repair times, as an
extra delay may incur due to purchase of hardware/network
components. Another observation worth noting is that software
related failures have quite similar mean and median of repair
times, indicating that their repair times have lower variation
and thus coefﬁcient of variation, compared to the other classes.
This can be explained by the fact that failures caused by
software tend to have a lower priority in the ticketing system
than other failure classes and are serviced later in time.
D. Subsequent Failures
Motivated by the long tailed distribution of inter-failure
times per server and previous hardware studies on non-
independent failures [2], [5], [10], we study how subsequent
PM/VM failures affect each other. To such an end, we use the
recurrent failure probability within a day, week and month,
i.e., deﬁned as given a server failure, what is the probability
the server fails within a day, week, and month. Fig. 5 shows
the recurrent failure probabilities for VMs, as well as PMs,
computed for all three time windows. Overall, the recurrent
probabilities of VMs are smaller than PMs. As expected, the
recurrent probabilities grow with the time window, but not
linearly with the window length. For example, the weekly
recurrent probability is not 7 times higher than the daily
recurrent one. This can indicate that subsequent failures tend
to happen in a very close window, if at all.
6666
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
Day PM
    VM
Week PM
     VM
Month PM
      VM
PERCENTAGE OF FAILURE INCIDENTS INVOLVING ZERO, ONE AND EQUAL
OR GREATER THAN TWO SERVERS, I.E., BOTH TYPES, PM ONLY, VM
TABLE VI
 0.5
 0.4
 0.3
 0.2
 0.1
y
t
i
l
i
b
a
b
o
r
P
e
r
u
l
i
a
F
t
n
e
r
r
u
c
e
R
 0
All
All
Sys I Sys II Sys III Sys IV Sys V
Sys I Sys II Sys III Sys IV Sys V
Fig. 5. Recurrent failure probabilities within a day, week, and month, for
VMs and PMs.
COMPARISON BETWEEN WEEKLY RANDOM FAILURES AND RECURRENT
TABLE V
ONLY.
PM and VM [%]
PM only [%]
VM only [%]
0
0
62
32
1
78
30
57
≥ 2
22
8
11
number of servers involved in a failure incident is 34 and this
is attributed to the undetermined (other) failure class.
To better see how such a spatial dependency propagates onto
PMs and VMs, we formally propose a metrics describing the
percentage of how many failure incidents involve dependent
PM/VM failures. We deﬁne dependent PM/VM failures by
considering failure incidents that include at least two PMs
or VMs failures. Essentially, we compute this metric by the
fraction of failure incidents affecting at least two VMs or PMs
divided by the fraction of failure incidents affecting at least
one VM or PM. Using the values listed in Table VII, we
thus see that roughly 26% (8%/(30% + 8%)) of the failure
incidents involve dependent VM failures, whereas roughly
16% (11%/(57%+11%)) failure incidents have dependent PM
failures. This observation can be explained by the common
practice of consolidating multiple VMs on a single hosting
platform. As a result, we conclude that VMs show stronger
spatial dependency than PMs.
To verify our conjecture on the degree of spatial dependency
of server failures stemmed from root causes, we further
breakdown the number of servers, across both PMs and
VMs, by their classes. The average and maximum number
of servers involved in failure incidents per different failure
classes are summarized in Table VII. In terms of mean and
maximum value, power failure indeed results into a higher
number servers failing than other root causes. Judging from
the absolute values related to power failures, we note that
they occur at a local scale, rather than at the global datacenter
level, because they affect only a small subset of servers. The
spatial dependency of software failures is rather visible, and
actually comes as the second highest after power failures.
This is due to the fact that modern systems are composed
of several distributed software services/modules,
typically
hosted on separate servers. A common example are 3-tier and
enterprise applications. Although failures due to unexpected
reboots involve a very low number of servers, they still have
the second highest maximum value. We explain this by the
fact that unexpected reboots are actually due to reboots of
the underlying hosting platforms. Finally, we would like to
note that the mean and maximum number of servers involved
in unclassiﬁed failure incidents are 1.46 and 34 respectively.
The overall values presented in Table VII are on the low
side, because of the limitations explained in Section III. In
particular, critical large scale failures can lead to the failure of
the monitoring server, and thus leads to the missing generation
of crash tickets. Out of 2300 tickets observed, 48 tickets report
monitoring system failures.
FAILURES WITHIN A WEEK.
Physical Machines
Sys I
.015
.16
10.7x
Sys II
.0020
.09
45x
Sys III
.0090
.33
36.7x
Virtual Machines
Sys I
.0023
.11
Sys II
0
0
Sys III
.0030
.20
66.7x
42.1x
47.8x
N.A.
All
.0062
.22
35.5x
All
.0038
.16
Random
Recurrent
Ratio
Random
Recurrent
Ratio
Sys IV
.0028
.07
25 X
Sys IV
.0032
0.1
31.3x
Sys V
0.0086
.19
10.5x
Sys V
.0094
.14
16.7x
To highlight the intensity of subsequent VMs’ and PMs’
failures, we propose to use the ratio between the recurrent
failure probability within a week with the random weekly
failure probability, which is deﬁned as the probability that
any server fails at least once within a week. Their values are
summarized across all ﬁve subsystems and for each subsystem
individually in Table V. One can straightforwardly see that
the recurrent probability is higher for PMs, as well as VMs,
roughly by a factor of 35X and 42X, respectively. In all
subsystems, the intensity of recurrent failure ratios for VMs are
visibly higher than for PMs. Our results show that both VM
and PM failures show high intensities of recurrent failures,
especially for VMs, though the absolute values of recurrent
failure probabilities of PM are higher than VM.
E. Spatial (in)Dependency of Failures
To study how server failures affect each other at a given
point of time, i.e., spatial dependency, we leverage the infor-
mation of crash tickets by checking how many servers are
affected by a single failure incident. The failure incidents can
be, for instance, the power outage of a subset of servers or
the crash of the underlying host platform which results in the
corresponding VMs to fail at the same time. We ﬁrst present
the empirical distribution of how many servers are affected in
a single failure incident, shown in Fig. VI. Roughly, 78% of all
failure incidents involve only one server that could be a VM
or a PM. The remaining 22% of all failures have a long-tailed
distribution of the number of affected servers. The maximum
7777
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:00:48 UTC from IEEE Xplore.  Restrictions apply. 
MEAN AND MAX NUMBER OF SERVERS INVOLVED IN FAILURE INCIDENTS
OF DIFFERENT CLASSES.
TABLE VII
mean
max
HW Net
1.5
1.2
10
9
Power
Reboot
2.7
21
1.1
15
SW
1.7
10
s
e
r
u
l
i
a
F
f
o
n
o
i
t
c
a
r
F
.
l
u
m
u
C
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
VM
 20
 15
]
%
[
s
e
r
u
 10
l
i
a
F
f
o
n
o
i
t