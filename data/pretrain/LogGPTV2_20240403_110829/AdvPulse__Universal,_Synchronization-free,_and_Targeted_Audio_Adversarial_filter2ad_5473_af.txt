### References

[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. TensorFlow: A system for large-scale machine learning. In *12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)*. 265–283.

[2] Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin RB Butler, and Joseph Wilson. 2019. Practical hidden voice attacks against speech and speaker recognition systems. arXiv preprint arXiv:1904.05734.

[3] Moustafa Alzantot, Bharathan Balaji, and Mani Srivastava. 2017. Did you hear that? Adversarial examples against automatic speech recognition. In *Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)*.

[4] Amazon. 2020. Amazon Echo. https://www.amazon.com/all-new-Echo/dp/B07R1CXKN7

[5] Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Li Chen, Michael E Kounavis, and Duen Horng Chau. 2018. Adagio: Interactive experimentation with adversarial attack and defense for audio. In *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*. Springer, 677–681.

[6] Timothy Dozat. 2016. Incorporating Nesterov momentum into Adam. In *International Conference on Learning Representations (ICLR)*.

[7] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. *Journal of Machine Learning Research* 12, Jul (2011), 2121–2159.

[8] Yuan Gong, Boyang Li, Christian Poellabauer, and Yiyu Shi. 2019. Real-time adversarial attacks. In *Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)*.

[9] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. *Deep Learning*. MIT Press.

[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.

[11] Google. 2020. Google Home. https://store.google.com/us/product/google_home

[12] Google. 2020. Speech-to-text Conversion Powered by Machine Learning. https://cloud.google.com/speech-to-text

[13] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. 2006. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In *Proceedings of the 23rd International Conference on Machine Learning*. 369–376.

[14] Chris Hall. 2019. Hey BMW: Your intelligent voice assistant is actually pretty good. https://www.pocket-lint.com/cars/news/bmw/148690-hey-bmw-your-intelligent-voice-assistant-is-actually-pretty-good

[15] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. 2014. Deep Speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567.

[16] Xuedong Huang. 2017. Microsoft researchers achieve new conversational speech recognition milestone. https://www.microsoft.com/en-us/research/blog/microsoft-researchers-achieve-new-conversational-speech-recognition-milestone/

[17] Marco Jeub, Magnus Schafer, and Peter Vary. 2009. A binaural room impulse response database for the evaluation of dereverberation algorithms. In *International Conference on Digital Signal Processing*. IEEE, 1–5.

[18] Jack Kiefer and Jacob Wolfowitz. 1952. Stochastic estimation of the maximum of a regression function. *The Annals of Mathematical Statistics* 23, 3 (1952), 462–466.

[19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.

[20] Keisuke Kinoshita, Marc Delcroix, Takuya Yoshioka, Tomohiro Nakatani, Emanuel Habets, Reinhold Haeb-Umbach, Volker Leutnant, Armin Sehr, Walter Kellermann, Roland Maas, et al. 2013. The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech. In *IEEE Workshop on Applications of Signal Processing to Audio and Acoustics*. 1–4.

[21] Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. 2018. Fooling end-to-end speaker verification with adversarial examples. In *Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)*. 1962–1966.

[22] Zhuohang Li, Cong Shi, Yi Xie, Jian Liu, Bo Yuan, and Yingying Chen. 2020. Practical adversarial attacks against speaker recognition systems. In *Proceedings of the 21st International Workshop on Mobile Computing Systems and Applications (HotMobile)*. 9–14.

[23] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2017. Universal adversarial perturbations. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. 1765–1773.

[24] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2016. DeepFool: A simple and accurate method to fool deep neural networks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. 2574–2582.

[25] Satoshi Nakamura, Kazuo Hiyane, Futoshi Asano, Takanobu Nishiura, and Takeshi Yamada. 2000. Acoustical sound database in real environments for sound scene understanding and hands-free speech recognition. In *Language Resources and Evaluation Conference*. 965–968.

[26] Paarth Neekhara, Shehzeen Hussain, Prakhar Pandey, Shlomo Dubnov, Julian McAuley, and Farinaz Koushanfar. 2019. Universal adversarial perturbations for speech recognition systems. arXiv preprint arXiv:1905.03828.

[27] Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. 2015. A time delay neural network architecture for efficient modeling of long temporal contexts. In *Annual Conference of the International Speech Communication Association (INTERSPEECH)*.

[28] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition Toolkit. In *IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)*. IEEE Signal Processing Society. IEEE Catalog No.: CFP11SRW-USB.

[29] Yao Qin, Nicholas Carlini, Garrison Cottrell, Ian Goodfellow, and Colin Raffel. 2019. Imperceptible, robust, and targeted adversarial examples for automatic speech recognition. In *Proceedings of the International Conference on Machine Learning (ICLR)*. 5231–5240.

[30] Tara N Sainath and Carolina Parada. 2015. Convolutional neural networks for small-footprint keyword spotting. In *Annual Conference of the International Speech Communication Association (INTERSPEECH)*.

[31] Samsung. 2020. Unlocks your phone with Bixby Voice. https://www.samsung.com/us/support/answer/ANS00082783/

[32] Lea Schönherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa. 2018. Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding. arXiv preprint arXiv:1808.05665.

[33] Google Assistant SDK. 2020. Best Practices for Audio. https://developers.google.com/assistant/sdk/guides/service/python/best-practices/audio.

[34] Suwon Shon, Hao Tang, and James Glass. 2018. Frame-level speaker embeddings for text-independent speaker recognition and analysis of end-to-end model. In *IEEE Spoken Language Technology Workshop (SLT)*. IEEE, 1007–1013.

[35] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khudanpur. 2018. x-vectors: Robust DNN embeddings for speaker recognition. In *Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)*. 5329–5333.

[36] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199.

[37] Tesla. 2020. Voice Commands. https://www.tesla.com/support/voice-commands.

[38] Jon Vadillo and Roberto Santana. 2019. Universal adversarial examples in speech command classification. arXiv preprint arXiv:1911.10182.

[39] Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. 2017. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR) (2017).

[40] Pete Warden. 2018. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209.

[41] Weidi Xie, Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. 2019. Utterance-level aggregation for speaker recognition in the wild. In *IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 5791–5795.

[42] Yi Xie, Cong Shi, Zhuohang Li, Jian Liu, Yingying Chen, and Bo Yuan. 2020. Real-time, universal, and robust adversarial attacks against speaker recognition systems. In *Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.

[43] Hiromu Yakura and Jun Sakuma. 2018. Robust audio adversarial example for a physical attack. arXiv preprint arXiv:1810.11793.

[44] Zhuolin Yang, Bo Li, Pin-Yu Chen, and Dawn Song. 2018. Characterizing audio adversarial examples using temporal dependency. arXiv preprint arXiv:1809.10875.

[45] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, XiaoFeng Wang, and Carl A Gunter. 2018. CommanderSong: A systematic approach for practical adversarial voice recognition. In *27th USENIX Security Symposium (USENIX Security 18)*. 49–64.

[46] Lei Zhang, Yan Meng, Jiahao Yu, Chong Xiang, Brandon Falk, and Haojin Zhu. 2020. Voiceprint mimicry attack towards speaker verification system in smart home. In *Proceedings of the IEEE International Conference on Computer Communications (INFOCOM)*.

[47] Yingke Zhu, Tom Ko, David Snyder, Brian Mak, and Daniel Povey. 2018. Self-attentive speaker embeddings for text-independent speaker verification. In *Interspeech*. 3573–3577.

### Appendix

#### A.1 Speaker Information

| ID | Age | Gender | Accents         | Region             |
|----|-----|--------|----------------|--------------------|
| 225 | 23  | F      | Southern England | Surrey             |
| 226 | 22  | M      | Southern England | Cumbria            |
| 227 | 38  | M      | Southern England | Southern England   |
| 228 | 22  | F      | Southern England | Southern England   |
| 229 | 23  | F      | Southern England | Southern England   |
| 230 | 23  | F      | Southern England | Staffordshire      |
| 231 | 22  | F      | Southern England | Southern England   |
| 232 | 23  | M      | Scottish        | West Dumfries      |
| 233 | 23  | F      | Southern England | Stockton-on-tees   |
| 234 | 22  | F      | Southern England | Southern England   |

Table 5: Information of each speaker.

#### A.2 Frequency Response

Figure 13: Measured frequency response.

#### A.3 Comparison of Distance Metrics

We conducted a preliminary study to evaluate the quality of the generated adversarial perturbation in terms of similarity with the environmental sound template using four distance metrics: (1) Time Series: the L2 distance between the two time-series signals; (2) DCT: the L2 distance between the discrete cosine transform of the two signals; (3) STFT: the L2 distance between the short-time Fourier transform of the two signals; and (4) MFCC: the L2 distance between the extracted Mel-frequency cepstral coefficients. We randomly initialized the audio perturbation (with the same random seed across trials) and performed a 2000-epoch gradient descent with a learning rate of 0.001 to minimize each distance. Figure 14 shows the spectrogram of the original environmental sound template (bird singing) and the generated audio perturbation using different distance metrics. As we can see, using Time Series, DCT, and STFT can all effectively produce adversarial perturbations that sound similar to the environmental sound template, making the attack inconspicuous. To reduce training time, we chose to use Time Series as the distance metric.

Figure 14: Spectrogram of the adversarial perturbations generated using different distance metrics to mimic environmental sound.

#### A.4 Impact of Attack Distance

We used the setup shown in Figure 15 to study the impact of the attack distance. As shown in the figure, one participant (p-4) was asked to sit at one end of the table with a smartphone (i.e., Google Pixel) and a noise meter (i.e., RISEPRO decibel meter) placed in front of them, while the loudspeaker was placed at different distances (i.e., 1–5m) with a fixed volume. The participant first read out 5 arbitrary speech commands, 10 times each, without playing the adversarial perturbation, to serve as the baseline system accuracy. Then, the participant read out the same set of speech commands (10 times for each) while the perturbation was played from the loudspeaker placed at increasing distances, with its loudness decreasing correspondingly. For reference, we measured the ambient noise to be around 46 dBSPL, the participant's speech to be around 74–78 dBSPL, and the baseline system accuracy to be 90%. Figure 16 shows the resulting attack success rate and perturbation loudness measured by the noise meter. Our attack achieved a high success rate in close-distance scenarios (e.g., 100% at 1m and 1.5m) and maintained moderate accuracy under far-field settings with relatively strong reverberations (e.g., 68% at 4.5m and 62% at 5m) and low perturbation loudness comparable to the ambient noise (e.g., 48.4 dBSPL at 5m). This result demonstrated that by considering the effects of speaker and microphone limitations, absorption and reverberation, and ambient noise, the over-the-air optimization process can indeed provide robust adversarial perturbations.

Figure 15: The distance study setup.

Figure 16: Attack performance at varying distances.

**Training Time on a Single NVIDIA GTX 2080Ti GPU:**
- Time Series: 1.035s
- DCT: 10.187s
- STFT: 10.210s
- MFCC: 16.537s

---

This version of the text is more structured, coherent, and professional, with clear headings and consistent formatting.