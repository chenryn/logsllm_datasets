In an effort to close the loop, we explore some heuristics in
this section to correct the throughput difference between TFRC
and TCP. We feel that the subject requires much more study, and
one section of a paper may not be enough to cover the topic.
Thus, our goal is very modest; we intend to demonstrate that
ﬁxing the problems may not be far out of reach and some very
simple heuristics can be effective at least within the domain of
the conditions we have tested.
Based on our simulation and analysis results shown in the
previous sections, we follow several guidelines described below
in designing heuristics methods.
1) TFRC should estimate RTO by using a method similar to
(if not exactly the same as) the one recommended by the
TCP RFC, due to the following reasons.
• Using RTO can greatly reduce the throughput differ-
denotes the value of RTO calcu-
ence, where RTO
lated based on the recommendation from TCP RFC. As
we can see that the throughput difference triggered by
improper RTO estimation as shown in Fig. 14 is sig-
niﬁcantly larger than that caused by loss event rate es-
timation and TCP-friendly equation alone as shown in
Fig. 10.
It is hard to design a heuristics method with RTO ,
denote the value of RTO calculated
where RTO
based on the recommendation from TFRC RFC. This
may
is because Fig. 14 shows that TFRC with RTO
achieve higher or lower throughput than TCP depending
on both RTT and loss event rates.
•
2) TFRC with RTO
always achieves less throughput than
TCP as shown in Fig. 10, and the throughput difference
increases as the loss event rate increases. Therefore, TFRC
should be more aggressive and grab more bandwidth under
high loss rates, while maintaining approximately the same
throughput under low loss rates.
3) When TFRC uses RTO ,
the throughput difference
decreases as RTT increases (the simulation result is not
shown in the paper, but this observation can be conﬁrmed
by Fig. 18 that will be discussed later). Therefore, TFRC
should grab more bandwidth with short RTTs, while
maintaining approximately the same throughput with long
RTTs.
to the output of
We explore two simple heuristics. The ﬁrst one is to multiply
as deﬁned in (1) (i.e.,
to RTO .
a constant factor
the feedback sending rate), and set timeout period
This approach follows the ﬁrst design guideline by setting
to RTO . When TFRC uses RTO , it experiences a signif-
icant throughput drop over high loss event rates (as shown in
Fig. 8). Thus, by simply scaling the feedback throughput, we
The second approach is to use a new formula
as deﬁned
in (17) to calculate the feedback sending rate, and set timeout
RTO is equiv-
period
RTO , thus we will refer to this
alent to
approach as RTO scaling. This approach satisﬁes our ﬁrst de-
sign guideline by using RTO
to RTO . Note that
with
with
(17)
To investigate the impact of scaling factor
in (17) on TFRC
throughput, we calculate the throughput ratio of (17) and (1)
with
in both equations
RTO
RTO
RTO
RTO
RTO
(18)
We observe that when
is large, the ratio is greater than 1 if
. That is, the scale factor can increase TFRC throughput
is small and
under high loss rates. On the other hand, when
. That
close to zero, the ratio is approximately
is, the scale factor slightly changes TFRC throughout under low
loss rates. Therefore, the second approach follows our second
design guideline. This can be explained intuitively. RTO affects
the feedback throughput increasingly more as the loss event rate
increases as shown in Fig. 13, because under high loss condi-
tions, TCP is more likely to have timeouts.
We also observe that the throughput ratio decreases as
increases and both RTO
remains same. That is, the
scaling factor makes TFRC grab more bandwidth only with
short RTTs, and this satisﬁes our third design guideline.
and
Using the same simulation setup discussed in Section III,
we evaluate various scaling factors for the ﬁrst and second
approaches. Fig. 16 shows the throughput difference ratios of
TFRC and TCP for various scale factors in the ﬁrst approach.
We observe that with scale factor 1.5, TFRC has throughput
much closer to TCP than with the other scaling factors. The
results from the second approach, shown in Fig. 17, promise
much better performance. We compare the results from the
second approach to the 1.5 scaling factor result of Fig. 16. As
the ﬁrst approach applies the same scale factor to the feedback
throughput irrespective of the loss event rate, it does not correct
the pattern of the original TFRC whose sending rate drops
under high loss rates (although it reduces the difference). On
the other hand, the second approach has an effect of virtually
increasing the scale factor to the feedback sending rate as the
loss event rate increases. Fig. 17 shows that as the RTO scale
factor increases, the ﬁtted lines become more ﬂat. The RTO
864
IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 15, NO. 4, AUGUST 2007
Fig. 16. Throughput difference ratio of TFRC and TCP for a different scaling
factor. We apply a scale factor to the output of (1) to increase the throughput of
TFRC. Scale factor 1.5 gives the best performance.
Fig. 18. Average of absolute difference ratios of TFRC and TCP over a dif-
ferent network delay. The error bars represent one standard deviation.
work [9] has extensively examined the behavior of TFRC in var-
ious Internet environments, mostly from an empirical perspec-
tive. Rhee et al. [22] show by simulation that the throughput of
TFRC can be much lower than TCP under high loss environ-
ments and also when the feedback delay is very long. Bansal
et al. [3] and Yang et al. [27] showed, by simulation, that the
slow responsiveness of TFRC to transient congestion can make
TFRC experience a higher loss rate, thus causing it to have a
lower throughput than TCP. The loss rate difference between a
constant rate ﬂow (such as acknowledgments or ping) and TCP
was also observed by Paxson [19]. The work [24] by Vojnovic´
and Le Boudec is the only one we ﬁnd in the literature that
studies theoretical reasons why TFRC may not give the same
throughput as TCP. Since we discuss the work extensively in
the introduction, we do not describe it further in this section. Re-
cently, Chen and Nahrstedt [6] showed by simulation the cases
where TFRC may use less bandwidth than TCP in mobile ad hoc
networks. Their work conﬁrms the conservativeness of TFRC
shown in [24], in MANET environments.
IX. CONCLUSION
In this paper, we examined how the three main factors that
determine the TFRC throughput, namely the TFRC throughput
equation, loss event rate estimation and RTO estimation, can in-
ﬂuence the long-term throughput imbalance between TFRC and
TCP. We give theoretical reasons why such imbalance occurs.
The main ﬁndings are that: 1) any two competing ﬂows sharing
the same bottleneck link will see different loss event rates if they
have signiﬁcantly different sending rates, and 2) the loss event
rate difference can greatly amplify the initial sending rate differ-
ence. Early work explains reasons only for the conservativeness
of TFRC, but does not account for the reason why TFRC can
have higher throughput than TCP. Our ﬁndings analytically ex-
plain reasons for that. We also provide a couple of more reasons
why the sending rate of TFRC can be different from TCP ini-
tially to provide a trigger for different loss event rates to occur.
and different RTO esti-
These are namely the convexity of
mation. While the other factors are fundamental, the issue with
RTO estimation is an artifact of policy, and has much of rele-
vance in practice.
Fig. 17. Throughput difference ratio of TFRC and TCP for a different RTO
scaling factor. We ﬁx the value of RTO in TFRC to RTO
where c is the
RTO scale factor.
scale factor of 3 gives the best performance among the factors
we tried. We further examine the effect of different network
delays on the throughput difference when we apply various
RTO scale factors. Fig. 18 shows the average values of absolute
throughput difference ratios of TFRC and TCP. In the ﬁgure,
one data point indicates the average value of the absolute
throughput difference ratios of TFRC and TCP we obtained
from all runs of different network loads with a ﬁxed (physical)
network delay (actual delays vary depending on the network
load). An error bar marks one standard deviation away from an
average value. The result indicates that RTO scale factor 3 gives
the best performance even over various delay networks. We
also see that the throughput ratios decrease as RTT increases.
VIII. RELATED WORK
TFRC is based on pioneering work by Padhye et al. [18] that
models the throughput of TCP using a loss event rate , RTT
and RTO. There are relatively a small number of studies that ex-
amine the behavior of TFRC [3], [6], [9], [22], [24], [27]. Most
of them are based on simulation except [24]. The original TFRC
RHEE AND XU: LIMITATIONS OF EQUATION-BASED CONGESTION CONTROL
865
Our work on TFRC can provide hints to engineers about pos-
sible network situations where TFRC might show ill-behaviors
and guide them in performing “stress tests.” In addition, as the
authors of [24] point out, studying the causes of the throughput
discrepancy helps engineers in designing new protocols that
have similar goals as TFRC. For instance, new emerging net-
works, such as mobile ad hoc networks and high-speed long
distance networks, whose characteristics are substantially dif-
ferent from the traditional Internet, present environments where
TCP may not work so well [8], [16]. For such networks, a new
congestion control technique is needed. We believe that our
work can be useful for developing such a protocol as many new
TCP-variant protocols are being proposed (e.g., [8]).
In this paper, we assume ﬁxed RTTs and ﬁxed packet sizes for
all ﬂows, which are not realistic assumptions. Developing anal-
ysis where these assumptions are relaxed is of future interest.
We also assume that all packets in the same end-to-end network
path are subject to the same loss probability. Thus, our results
are valid for any AQM scheme that supports this assumption.
Most AQM schemes including RED [13], PD-RED [23], REM
[2], BLUE [7], and GREEN [26] have this property.
ACKNOWLEDGMENT
The authors would like to thank M. Vojnovic´ for his generous
help and comments, and M. Zukerman and the anonymous re-
viewers for their valuable feedback.
REFERENCES
[1] M. Allman, V. Paxson, and W. Stevens, “TCP congestion control,”
RFC 2581, 1999.
[2] S. Athuraliya, V. Li, S. Low, and Q. Yin, “REM: Active queue man-
agement,” IEEE Netw., vol. 15, no. 3, pp. 48–53, May 2001.
[3] D. Bansal, H. Balakrishnan, S. Floyd, and S. Shenker, “Dynamic be-
havior of slowly-responsive congestion control algorithms,” in Proc.
ACM SIGCOMM 2001, San Diego, CA, Aug. 2001.
[4] P. Barford and M. Crovella, “Generating representative web workloads
for network and server performance evaluation,” in Measurement and
Modeling of Computer Systems, 1998, pp. 151–160.
[5] T. Bonald, M. May, and J. C. Bolot, “Analytic evaluation of RED per-
formance,” in Proc. INFOCOM, 2000, pp. 1415–1424.
[6] K. Chen and K. Nahrstedt, “Limitations of equatiProc.based congestion
control in mobile ad hoc networks,” in Proc. Int. Workshop on Wireless
Ad Hoc Networking (WWAN 2004) in Conjunction with ICDCS-2004,
Mar. 2004.
[7] W. Feng, K. Shin, D. Kandlur, and D. Saha, “BLUE active queue
management algorithms,” IEEE/ACM Trans. Netw., vol. 10, no. 4, pp.
513–528, Aug. 2002.
[8] S. Floyd, “High speed TCP for large congestion windows,” RFC 3649,
2003.
[9] S. Floyd, M. Handley, J. Padhye, and J. Widmer, “Equation-based con-
gestion control for unicast applications,” in Proc. ACM SIGCOMM
2000, Stockholm, Sweden, Aug. 2000, pp. 43–56.
[10] S. Floyd, M. Handley, J. Padhye, and J. Widmer, TCP Friendly Rate
Control (TFRC): Protocol speciﬁcation , 2003.
[11] S. Floyd and T. Henderson, “The New Reno modiﬁcation to TCP’s fast
recovery algorithm,” RFC 2582, 1999.
[12] S. Floyd and V. Jacobson, “Trafﬁc phase effects in packet-switched
gateways,” Internetw: Res. Exp., vol. 3, no. 3, pp. 115–156, Sep. 1992.
[13] S. Floyd and V. Jacobson, “Random early detection gateways for
congestion avoidance,” IEEE/ACM Trans. Netw., vol. 1, no. 4, pp.
397–413, Aug. 1993.
[14] M. Goyal, R. Guerin, and R. Rajan, “Predicting TCP throughput from
non-invasive network sampling,” in Proc. IEEE INFOCOM, Jun. 2002.
[15] J. Hoe, “Improving the start-up behavior of a congestion control
scheme for TCP,” in Proc. ACM SIGCOMM, Aug. 1996.
[16] G. Holland and N. H. Vaidya, “Analysis of TCP performance over mo-
bile ad hoc networks,” in Proc. IEEE/ACM MOBICOM ’99, Aug. 1999,
pp. 219–230.
[17] E. Kohler, M. Handley, S. Floyd, and J. Padhye, Datagram Conges-
tion Control Protocol (DCCP) [Online]. Available: draft-ietf-dccp-
spec-05.txt
[18] J. Padhye, V. Firoiu, D. Towsley, and J. Krusoe, “Modeling TCP
throughput: A simple model and its empirical validation,” in Proc.
ACM SIGCOMM ’98, 1998, pp. 303–314.
[19] V. Paxson, “End-to-end internet packet dynamics,” IEEE/ACM Trans.
Netw., vol. 7, pp. 277–292, Jun. 1999.
[20] V. Paxson and M. Allman, “Computing TCP’s retransmission timer,”
RFC 2988, 2000.
[21] PlanetLab [Online]. Available: http://www.planet-lab.org/
[22] I. Rhee, V. Ozdemir, and Y. Yung, TEAR: TCP emulation at re-
ceivers—Flow control for multimedia streaming Dept. Comput. Sci.,
North Carolina State Univ., Chapel Hill, NC, Tech. Rep., 2000.
[23] J. Sun, K. Ko, G. Chen, S. Chan, and M. Zukerman, “PD-RED: To
improve the performance of RED,” IEEE Commun. Lett., vol. 7, pp.
406–408, Aug. 2003.
[24] M. Vojnovic´ and J. Le Boudec, “On the long run behavior of equa-
tion-based rate control,” in Proc. ACM SIGCOMM 2002, 2002, pp.
103–116.
[25] J. Widmer and M. Handley, “Extending equation-based congestion
control to multicast applications,” in Proc. ACM SIGCOMM 2001,
San Diego, CA, Aug. 2001.
[26] B. Wydrowski and M. Zukerman, “GREEN: An active queue manage-
ment algorithm for a self managed internet,” in Proc. ICC, May 2002,
pp. 2368–2372.
[27] R. Yang, M. Kim, and S. Lam, “Transient behaviors of TCP-friendly
congestion control protocols,” in Proc. INFOCOM, Mar. 2001.
[28] Y. Zhang, N. Dufﬁeld, V. Paxson, and S. Shenker, “On the constancy
of internet path properties,” in Proc. ACM SIGCOMM Internet Mea-
surement Workshop, Nov. 2001.
Injong Rhee received the Ph.D. degree from the Uni-
versity of North Carolina, Chapel Hill.
He is currently an Associate Professor of computer
science at North Carolina State University (NCSU),
Raleigh. In 2000, he founded Togabi Technologies,
Inc., a company that develops and markets mobile
wireless multimedia applications for next-generation
wireless networks, and he was CTO and CEO of the
company until 2002 when he came back to NCSU.
His research interests are computer networks, con-
gestion control, multimedia networking, distributed
systems, and operation systems.
Dr. Rhee received the NSF Early Faculty Career Development Award in 1999.
Lisong Xu received the B.E. and M.E. degrees from
the University of Science and Technology, Beijing,
China, and the Ph.D. degree from North Carolina
State University, Raleigh, in 1994, 1997, and 2002,
respectively, all in computer science.
From 2002 to 2004, he was a Postdoctoral Re-
search Fellow at North Carolina State University.
He is currently an Assistant Professor in com-
puter science and engineering at the University of
Nebraska-Lincoln. His research interests include
computer networks and distributed systems.