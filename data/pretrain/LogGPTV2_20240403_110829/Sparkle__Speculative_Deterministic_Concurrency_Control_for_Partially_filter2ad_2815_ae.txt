Calvin, which do not need to use multiple partitions per node
to enable parallelism). Despite in this test, communication
between the sub-transactions of a MPT take place via efﬁcient
Unix Domain sockets, MPTs impose a large overhead as
the data exchanges between sibling sub-transactions impose a
synchronization phase between the worker threads of different
partitions and leads to frequent stalls in the processing.
In the high contention workload (Fig. 3b), the absolute
peak throughput achieved by Sparkle is clearly lower than
in the previous scenario. Yet, we observe up to approx. 6×
speed-up versus the best baseline, i.e., Calvin, which scales
only up to 5 threads, as in the previous workload, before
being bottlenecked by the sequencing thread. This striking
performance gain is achieved despite, as expectable, Sparkle
incurs a high contention rate, given its speculative nature
and the high probability of conﬂicts between of transactions.
The most dramatic performance drop, though, is experienced
by S-SMR. In this case, when using more than a single
thread, the data (which is populated with a single warehouse)
has to be sharded over multiple partitions (in this case we
partition by district up to 10 threads, and then using random
hashing), forcing most transactions to access more than a
single partition. This is particularly onerous for long read-
only transactions, such as OrderStatus, which access hundreds
of keys and force the worker threads of different partitions to
synchronize hundreds of times to process a single transaction.
C. Distributed deployment
Let us now analyze the performance of Sparkle when de-
ployed over a medium scale cluster encompassing 8 machines.
We start by presenting, in Fig. 4, the results for the synthetic
benchmark considering four scenarios, which differ by the
percentage of MPTs they generate. In each of the 4 plots in
Fig. 4 we vary, on the X-axis, the percentage of dependent
transactions, and report throughput and abort rate for all the
considered solutions when using a low (LC) and medium
contention (MC) workload. For the case of S-SMR, since its
performance is oblivious to the contention level (given that it
processes transactions sequentially at each partition), we only
report results for the LC workload.
173
Fig. 6: Throughput of Sparkle : CC, Sparkle : SC and
Sparkle : SC + schedule, normalized to that of Sparkle : Cons,
while varying the percentage of MPTs.
noting that in the 50% MPTs scenario and in absence of de-
pendent transactions, Calvin achieves 30% higher throughput
than Sparkle. This can be explained by considering that, in
this workload, Calvin’s throughput is upper bounded by the
processing speed of MPTs (which take orders of magnitude
longer than SPTs) and not by its scheduling thread. Also, due
to its pessimistic/lock-based nature, Calvin does not require
MPTs to undergo a conﬁrmation phase. Despite Sparkle strives
to minimize the performance impact of the MPTs’ conﬁrma-
tion phase (via the combined use of scheduling techniques
and of the SC mechanism), this still introduces additional
communication overhead. Nonetheless, we highlight that, in
the 50% MPT scenario, Sparkle outperforms Calvin as soon
as the ratio of of dependent transactions is as large as 1%,
achieving an average throughput gain (across the considered
MPT ratios) of more than one order of magnitude. Analogous
gains are observed also with respect to S-SMR.
Next, we present
the results obtained using the TPC-C
benchmark. Figure 5 shows that Sparkle outperforms Calvin
and S-SMR in all workloads, with peak gains of approx. 3×
and approx. 4×, respectively. The key reason why S-SMR
achieves relatively poor performance is that these three TPC-
C workload generate a small, but not negligible fraction
(varying from approx. 1%, for the 10% update workload,
to approx. 10%, for the 90% update workload) of MPT
transactions. Calvin’s performance, instead, can be explained
considering that
three out of the ﬁve transaction proﬁles
are dependent transactions, which impose heavy load on the
locking thread and are prone to incur frequent restarts.
1) Beneﬁts of SC and scheduling: Next, we conduct an
experiment aimed to quantify the performance beneﬁts brought
about by using, either jointly or in synergy, two key mecha-
nisms used by Sparkle to regulate MPT’s execution: SC and
scheduling. Further, we aim to quantify to what extent the use
of speculative transaction processing (in particular allowing
MPTs to disseminate speculative data to their siblings) can
enhance the throughput of MPT transactions. To this end, we
compare the performance of four Sparkle variants:
• Sparkle:Cons: a conservative variant in which MPTs are
only allowed to send remote data to their siblings if they are
guaranteed to have observed a locally consistent snapshot, i.e.,
if their preceding transaction has ﬁnal committed. This spares
MPTs from the need (and cost) of any conﬁrmation, but also
throttles down throughput severely as it precludes any form of
parallelism between MPTs in execution at the same partition.
• Sparkle:CC: in which, as in Sparkle, MPTs disseminate to
their siblings the data they read locally in a speculative fashion.
Unlike Sparkle, though, this variant uses a conservative con-
ﬁrmation (CC) scheme, which sends conﬁrmation messages
only when transactions ﬁnal commit, and not when they
speculatively commit. The CC scheme is signiﬁcantly simpler
than SC, as, with CC, a transaction generates exactly one
conﬁrmation message, and not an a priori unknown number, as
it is the case for SC. However, with CC, a partition can send
the conﬁrmation for its i + 1-th transaction, only upon ﬁnal
committing its i-th transaction, which, in its turn, depends on
the reception of the conﬁrmation message that is only sent
upon the ﬁnal commit of the i − 1-th transaction. Thus, the
throughput of MPTs becomes inherently upper bounded by the
rate of completion of the inter-partition conﬁrmation phase,
which involves an all-to-all synchronous communication be-
tween the involved partitions.
• Sparkle:SC, which uses SCs but not scheduling;
• Sparkle:SC+Schedule, which uses SC and scheduling.
We use the low conﬂict micro benchmark conﬁguration
and generate varying ratios of MPTs. For better readability,
in Fig. 6 we report the normalized throughput of the three
protocols allowing speculative reads across partitions against
Sparkle:Cons. The plot allows us to draw three main conclu-
sions. First, all variants achieve signiﬁcant (up to approx. 3×)
w.r.t. Sparkle:Cons, conﬁrming the relevance of using specula-
tive processing techniques to cope with MPTs. Second, unless
coupled with scheduling, SC provides no perceivable beneﬁt
with respect to a simpler CC approach: without scheduling,
most MPTs need to resort to using a CC scheme, hence the
throughputs of Sparkle:CC and Sparkle:SC is almost identical.
Finally, it allows us to quantify the gains reaped through the
joint use of scheduling and SC: up to 2× throughput increase
when compared to Sparkle:CC.
VII. CONCLUSIONS
This paper introduced Sparkle, a novel distributed de-
terministic concurrency control for partially-replicated state
machines, which achieves signiﬁcant performance gains over
state of the art PRSM systems via the joint use of specu-
lative transaction processing and scheduling techniques. Via
an extensive experimental study encompassing both synthetic
and realistic benchmarks, we show that 1) Sparkle has neg-
ligible overhead compared with a protocol implementing no
concurrency control, in conﬂict-free workloads, 2) Sparkle can
achieve more than one order of magnitude throughput gains,
comparing with state of the art PRSM systems, in workloads
characterized by high conﬂict rates and frequent MPTs.
Acknowledgements. We are grateful to our shepherd Alexey
Gotsman and the anonymous reviewers. This work is partially
funded by the LightKone project (732505) in the EU H2020
Programme,
the Erasmus Mundus Doctorate Programme
(2012-0030) and by FCT projects UID/CEC/50021/2019 and
PTDC/EEISCR/1743/2014.
174
REFERENCES
[1] A. L. P. N. Alonso. Database replication for enterprise applications.
PhD thesis, Universidade do Minho, 2017.
[2] C. Basile, Z. Kalbarczyk, and R. Iyer. A preemptive deterministic
scheduling algorithm for multithreaded replicas.
In Proc. of the 33th
International Conference on Dependable Systems and Networks, pages
149–158, June 2003.
[3] T. Bergan, J. Devietti, N. Hunt, and L. Ceze. The deterministic execution
hammer: How well does it actually pound nails.
In Proc. of the 2nd
Workshop on Determinism and Correctness in Parallel Programming,
2011.
[4] P. A. Bernstein, V. Hadzilacos, and N. Goodman. Concurrency control
and recovery in database systems. 1987.
[5] C. E. Bezerra, F. Pedone, and R. V. Renesse. Scalable state-machine
replication.
the 44th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks, pages 331–342, June
2014.
In Proc. of
[6] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost, J. Furman, S. Ghe-
mawat, A. Gubarev, C. Heiser, P. Hochschild, W. Hsieh, S. Kanthak,
E. Kogan, H. Li, A. Lloyd, S. Melnik, D. Mwaura, D. Nagle, S. Quinlan,
R. Rao, L. Rolig, Y. Saito, M. Szymaniak, C. Taylor, R. Wang, and
D. Woodford. Spanner: Google’s globally-distributed database.
In
Proc. of the 10th USENIX Symposium on Operating Systems Design
and Implementation, pages 261–264, Hollywood, CA, 2012. USENIX
Association.
[7] M. Couceiro, D. Didona, L. Rodrigues, and P. Romano. Self-tuning in
Distributed Transactional Memory, pages 418–448. Springer Interna-
tional Publishing, Cham, 2015.
[8] M. Couceiro, P. Ruivo, P. Romano, and L. Rodrigues. Chasing the
optimum in replicated in-memory transactional platforms via protocol
adaptation.
IEEE Transactions on Parallel and Distributed Systems,
26(11):2942–2955, Nov 2015.
[9] J. Devietti, B. Lucia, L. Ceze, and M. Oskin. Dmp: deterministic shared
In ACM SIGARCH Computer Architecture
memory multiprocessing.
News, volume 37, pages 85–96. ACM, 2009.
[10] J. Devietti, J. Nelson, T. Bergan, L. Ceze, and D. Grossman. Rcdc: a
relaxed consistency deterministic computer. In ACM SIGPLAN Notices,
volume 46, pages 67–78. ACM, 2011.
[11] J. Du, D. Sciascia, S. Elnikety, W. Zwaenepoel, and F. Pedone.
Clock-rsm: Low-latency inter-datacenter state machine replication using
loosely synchronized physical clocks.
In Proc. of the 44th Annual
IEEE/IFIP International Conference on Dependable Systems and Net-
works, pages 343–354. IEEE, 2014.
[12] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the presence of
partial synchrony. Journal of the ACM, 35(2):288–323, 1988.
[13] M. J. Fischer, N. A. Lynch, and M. S. Paterson.
Impossibility of
distributed consensus with one faulty process. Journal of the ACM,
32(2):374–382, 1985.
[14] R. Friedman and R. van Renesse. Packing messages as a tool for
boosting the performance of total ordering protocols.
In Proc. of the
Sixth IEEE International Symposium on High Performance Distributed
Computing, pages 233–242, Aug 1997.
[15] S. Frølund and R. Guerraoui. e-transactions: End-to-end reliability for
IEEE Transactions on Software Engineering,
three-tier architectures.
28(4):378–395, Apr. 2002.
[16] J. Gray, P. Helland, P. O’Neil, and D. Shasha. The dangers of replication
and a solution. In Proc. of the 22nd ACM International Conference on
Management of Data, pages 173–182. ACM, 1996.
[17] Grid’5000. https://www.grid5000.fr/, 2018.
[18] Z. Guo, C. Hong, M. Yang, D. Zhou, L. Zhou, and L. Zhuang. Rex:
Replication at the speed of multi-core. In Proc. of the Ninth European
Conference on Computer Systems, page 11. ACM, 2014.
[19] S. Hirve, R. Palmieri, and B. Ravindran. Archie: a speculative replicated
In Proc. of the 15th International Middleware
transactional system.
Conference, pages 265–276. ACM, 2014.
[20] T. Hoff. Latency is everywhere and it costs you sales - how to crush it.
High Scalability, july, 25, 2009.
[21] Intel. Threading building blocks. https://www.threadingbuildingblocks.
org/, 2018.
[22] R. Jimenez-Peris, M. Patino-Martinez, B. Kemme, and G. Alonso.
Improving the scalability of fault-tolerant database clusters. In Proc. of
the 22nd International Conference on Distributed Computing Systems,
pages 477–484, July 2002.
[23] R. Kallman, H. Kimura, J. Natkins, A. Pavlo, A. Rasin, S. Zdonik,
E. P. C. Jones, S. Madden, M. Stonebraker, Y. Zhang, J. Hugg, and D. J.
Abadi. H-store: A high-performance, distributed main memory transac-
tion processing system. Proc. of the VLDB Endowment, 1(2):1496–1499,
Aug. 2008.
[24] M. Kapritsos, Y. Wang, V. Quema, A. Clement, L. Alvisi, and M. Dahlin.
All about eve: Execute-verify replication for multi-core servers.
In
Proc. of the 10th USENIX Symposium on Operating Systems Design
and Implementation, pages 237–250, Hollywood, CA, 2012. USENIX.
[25] T. Kraska, G. Pang, M. J. Franklin, S. Madden, and A. Fekete. Mdcc:
In Proc. of the 8th ACM European
Multi-data center consistency.
Conference on Computer Systems, pages 113–126. ACM, 2013.
[26] L. Lamport. The part-time parliament. ACM Transactions on Computer
Systems, 16(2):133–169, May 1998.
[27] J. Li, E. Michael, and D. R. Ports. Eris: Coordination-free consistent
transactions using in-network concurrency control. In Proc. of the 26th
Symposium on Operating Systems Principles, pages 104–120. ACM,
2017.
[28] Z. Li. Sparkle codebase. https://github.com/marsleezm/spec_calvin.
[29] Z. Li, P. V. Roy, and P. Romano. Sparkle: Scalable speculative replication
for transactional datastores. Technical Report 4, INESC-ID, May 2018.
[30] Z. Li, P. Van Roy, and P. Romano. Enhancing throughput of partially
replicated state machines via multi-partition operation scheduling.
In
Proc. of the IEEE 16th International Symposium on Network Computing
and Applications, pages 1–10. IEEE, 2017.
[31] Z. Li, P. Van Roy, and P. Romano. Transparent speculation in geo-
replicated transactional data stores. In Proc. of the 27th International
Symposium on High-Performance Parallel and Distributed Computing,
pages 255–266. ACM, 2018.
[32] P. J. Marandi, M. Primi, and F. Pedone. High performance state-machine
replication. In Proc. of the 41st IEEE/IFIP International Conference on
Dependable Systems and Networks, pages 454–465. IEEE, 2011.
[33] R. Palmieri, F. Quaglia, and P. Romano. Aggro: Boosting stm repli-
cation via aggressively optimistic transaction processing.
In Proc. of
the Ninth IEEE International Symposium on Network Computing and
Applications, pages 20–27, July 2010.
[34] M. Patiño-Martinez, R. Jiménez-Peris, B. Kemme, and G. Alonso.
Middle-r: Consistent database replication at the middleware level. ACM
Transactions on Computer Systems, 23(4):375–423, 2005.
[35] F. Pedone and A. Schiper. Handling message semantics with generic
broadcast protocols. Distributed Computing, 15(2):97–107, 2002.
[36] S. Peluso, J. Fernandes, P. Romano, F. Quaglia, and L. Rodrigues.
Specula: Speculative replication of software transactional memory. In
Proc. of the 31st IEEE International Symposium on Reliable Distributed
Systems, pages 91–100, 2012.
[37] F. Quaglia and P. Romano. Ensuring e-transaction with asynchronous
IEEE Transactions on
and uncoordinated application server replicas.
Parallel and Distributed Systems, 18(3):364–378, March 2007.
[38] P. Romano and M. Leonetti.
Self-tuning batching in total order
broadcast protocols via analytical modelling and reinforcement learning.
In Proc. of International Conference on Computing, Networking and
Communications, 2012.
[39] M. M. Saad, M. J. Kishi, S. Jing, S. Hans, and R. Palmieri. Processing
transactions in a predeﬁned order. In Proc. of the 24th Symposium on
Principles and Practice of Parallel Programming, pages 120–132, New
York, NY, USA, 2019. ACM.
[40] F. B. Schneider.
Implementing fault-tolerant services using the state
machine approach: A tutorial. ACM Computing Surveys, 22(4):299–319,
1990.
[41] A. Thomson. Calvin codebase. https://github.com/yaledb/calvin.
[42] A. Thomson, T. Diamond, S.-C. Weng, K. Ren, P. Shao, and D. J.
Abadi. Calvin: fast distributed transactions for partitioned database
systems. In Proc. of the 39th ACM SIGMOD International Conference
on Management of Data, pages 1–12. ACM, 2012.
[43] TPC-consortium. Tpc benchmark-w speciﬁcation v. 1.8. http://www.
tpc.org/tpc_documents_current_versions/pdf/tpc-c_v5.11.0.pdf.
[44] P. T. Wojciechowski, T. Kobus, and M. Kokoci´nski. State-machine and
deferred-update replication: Analysis and comparison.
IEEE Transac-
tions on Parallel and Distributed Systems, 28(3):891–904, March 2017.
[45] I. Zhang, N. K. Sharma, A. Szekeres, A. Krishnamurthy, and D. R.
Ports. Building consistent transactions with inconsistent replication. In
Proc. of the 25th Symposium on Operating Systems Principles, pages
263–278. ACM, 2015.
175