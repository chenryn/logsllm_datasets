title:Commoner Privacy And A Study On Network Traces
author:Xiyue Deng and
Jelena Mirkovic
Commoner Privacy And A Study On Network Traces
Xiyue Deng and Jelena Mirkovic
USC/ISI
4676 Admiralty Way ste 1001
Marina del Rey, CA 90292
{xiyueden,sunshine}@isi.edu
ABSTRACT
Differential privacy has emerged as a promising mechanism for
privacy-safe data mining. One popular differential privacy mecha-
nism allows researchers to pose queries over a dataset, and adds
random noise to all output points to protect privacy. While differ-
ential privacy produces useful data in many scenarios, added noise
may jeopardize utility for queries posed over small populations or
over long-tailed datasets. Gehrke et al. proposed crowd-blending
privacy, with random noise added only to those output points where
fewer than k individuals (a configurable parameter) contribute to
the point in the same manner. This approach has a lower privacy
guarantee, but preserves more research utility than differential
privacy.
We propose an even more liberal privacy goal—commoner pri-
vacy—which fuzzes (omits, aggregates or adds noise to) only those
output points where an individual’s contribution to this point is an
outlier. By hiding outliers, our mechanism hides the presence or
absence of an individual in a dataset. We propose one mechanism
that achieves commoner privacy—interactive k-anonymity. We also
discuss query composition and show how we can guarantee privacy
via either a pre-sampling step or via query introspection. We imple-
ment interactive k-anonymity and query introspection in a system
called Patrol for network trace processing. Our evaluation shows
that commoner privacy prevents common attacks while preserving
orders of magnitude higher research utility than differential privacy,
and at least 9–49 times the utility of crowd-blending privacy.
CCS CONCEPTS
• Security and privacy → Data anonymization and sanitiza-
tion; Privacy protections; Data anonymization and sanitization;
Privacy protections;
KEYWORDS
privacy, data sharing, network traces
ACM Reference format:
Xiyue Deng and Jelena Mirkovic. 2017. Commoner Privacy And A Study On 
Network Traces. In Proceedings of ACSAC 2017, Orlando, FL, USA, December 
4–8, 2017, 11 pages.
Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for profit or commercial advantage and that copies bear this notice and the full citation 
on the first page. Copyrights for components of this work owned by others than the 
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or 
republish, to post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from permissions@acm.org.
ACSAC 2017, December 4–8, 2017, Orlando, FL, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-5345-8/17/12. . . $15.00
https://doi.org/10.1145/3134600.3134602
https://doi.org/10.1145/3134600.3134644
1 INTRODUCTION
Privacy-safe data mining is a research topic of increasing impor-
tance. Today’s communications and human interactions generate
a wealth of data that can be used by researchers to mine common
patterns, identify anomalies and study new phenomena in almost
any field of science. Yet, such data is directly or indirectly generated
by humans, whose privacy must be protected.
Differential privacy [11] has emerged as a promising privacy-
protection approach, and is increasingly being adopted by industry.
Differential privacy allows for aggregate calculations over da-
tasets, while protecting privacy of individuals and small popula-
tions [11]. Queries are submitted by researchers to the system
hosting the dataset. The system runs the queries and returns noisy
outputs to researchers. One popular mechanism to achieve differ-
ential privacy—the Laplace mechanism—adds random noise to all
the points of the output, and several approaches exist to reduce
the magnitude of this noise (e.g., [29]). Differential privacy yields
very accurate calculations over large and well-balanced datasets,
and holds under a composition of queries. But differential privacy
may significantly reduce research utility of outputs, when queries
are ran on long-tailed or large-value-range datasets. In these cases
large noise must be added to achieve privacy guarantees and that
drives the research utility down.
Crowd-blending privacy [14] was proposed as an alternative.
It offers a lower privacy guarantee than differential privacy, but
preserves more of the research utility. An individual I is said to
blend in a crowd if her contributions to the outputs of some query
f run on the dataset are identical to the contributions of k − 1 other
individuals. For example, if we had data about customer visits to a
store and wanted to calculate a histogram of visits per month (e.g.,
y customers made x visits), an individual I that visited a given store
10 times in a month would blend if there were at least k − 1 other
individuals who also made 10 visits in a month. The parameter k
controls the level of privacy achieved, i.e. the size of the crowd.
Crowd-blending privacy releases outputs over well-blended pop-
ulations without any noise, and adds random noise to (or omits)
other outputs.
Our first contribution is to propose a more liberal privacy de-
finition — commoner privacy, which achieves significantly higher
research utility than crowd-blending privacy for select queries and
datasets, at the cost of a slightly lower privacy protection. An in-
dividual I is said to blend in a crowd of size k if she contributes to
some output with at least k − 1 other individuals, and her contri-
butions are not outliers among contributions of the others in that
566group of individuals. Commoner privacy fuzzes (adds noise, aggre-
gates or omits) contributions of individuals that do not blend, and
leaves other contributions unchanged. Therein lies a key difference
between crowd-blending and commoner privacy: commoner pri-
vacy considers that an individual I blends in a group of size k if its
contributions are not outliers among contributions of others in the
group, while crowd-blending privacy requires these contributions
to be the same. Thus commoner privacy offers lower protections
to individuals for some queries, at the benefit of higher research
utility. We discuss in Section 2.1 some domains where this privacy
loss may be acceptable in favor of higher utility. In our evaluation
in Section 5 we qualitatively and quantitatively evaluate privacy
and utility of differential, crowd-blending, and commoner privacy.
We find that commoner privacy prevents common attacks, while
preserving orders of magnitude higher research utility than dif-
ferential privacy, and at 9–49 times the utility of crowd-blending
privacy.
As our second contribution we propose one mechanism to achieve
commoner privacy — interactive k-anonymity. Privacy checks are
applied just before query outputs are released to the user. Every
output point with contributions by k or more identities is checked
for outliers, and those outliers are removed. After this, if the con-
tributor set contains at least k identities, the output point is released
unchanged. Other points are fuzzed to protect privacy.
Our third contribution is to show how commoner privacy can
hold under query composition. Like Gehrke et al. [14], we could
use pre-sampling of individuals prior to processing the query to
protect query compositions. However, this loses input data and
thus research utility, even when a composition of queries does not
jeopardize privacy. Instead, we propose query introspection—careful
record-keeping of identities and their contributions to outputs for
all the queries posed by a given researcher. At run time we perform
automated checks of combinations of past queries and the current
query to detect tracker attacks. If the check detects a query com-
position that may jeopardize privacy, the current query is either
rejected or some of its output points are fuzzed to protect privacy.
Throughout the paper we use network traces as our motivating
example, and we implement a system for network trace processing
with commoner privacy, called Patrol. Network traces contain a
record for each packet seen on the network, which includes source
and destination IP addresses that are considered personal and iden-
tifying information (PII), and can be linked to human users using
the associated machine. Network traces can thus reveal sensitive
information about a user’s communication patterns (e.g., “a user
John Smith visited a pornographic site while at work”), or they
can reveal data about vulnerabilities on a user’s machine that can
be exploited in cyberattacks (e.g., “machine with IP address 1.2.3.4
has ports 80 and 22 open”). We show that commoner privacy pro-
vides sufficient protection against known attacks on user privacy
in network trace data. While we use network traces to illustrate
our findings, commoner privacy is general enough to apply to any
dataset. Our current implementation supports network traces and
can be extended to support more formats in the future. Our Patrol
system is publicly available at [6].
2 DIFFERENTIAL AND CROWD-BLENDING
PRIVACY
In this section we provide a brief overview of two popular privacy
definitions and of the mechanisms that achieve them: differential
and crowd-blending privacy. Both of these mechanisms and our
commoner privacy can be applied in a setting where original data
resides with the data provider. Users (researchers) are allowed to
pose queries over data and receive outputs of these queries. The
privacy mechanisms modify these outputs to protect the privacy of
individuals that have contributed the data. It is the nature of this
modification that may lead to stronger or weaker privacy guaran-
tees, and lower or higher research utility of the outputs.
Identities vs. Records. Privacy protections should always be
applied over individuals and not over records in the dataset. This
distinction is important for datasets where an individual may con-
tribute multiple records; e.g., an individual may submit multiple
movie reviews or make multiple purchases or hospital visits. In such
cases, a highly contributing individual may be exposed if privacy
protections are applied over records instead of over identities. In
this overview, we revise some existing definitions, where necessary,
to make it clear which operations occur over individuals and which
over dataset records. To this effect, we introduce two functions to
map between individuals and their records: (1) Id(xi) returns the
identity or identities whose data is in record(s) xi, or whose data
has been used to calculate the output data point xi, and (2) rec(Y)
returns all the records associated with identity or identities Y.
Differential privacy [10] ensures that an adversary can learn,
roughly, the same data about an individual, regardless of the in-
dividual’s participation in the dataset. This is achieved through a
randomized algorithm San.
Definition 2.1. Differential privacy [11]. A randomized algorithm San
is said to be (ϵ, δ)-differentially private if for all S ∈ Ranдe(San) and for
all x, y ∈ D such that ||Id(x) − Id(y)||1 ≤ 1 it holds:
Pr[San(x) ∈ S] ≤ exp(ϵ) · Pr[San(y) ∈ S] + δ
(1)
Differential Privacy Mechanism. One popular mechanism to
achieve (ϵ, 0)-differential privacy is the Laplace mechanism [11].
This approach adds random noise to each output data point. The
noise is drawn from the Laplace distribution with parameter ∆f /ϵ,
where ∆f is the global sensitivity of a desired query f to pertur-
bation of inputs (largest possible change in outputs should one
individual join or leave the set), and ϵ is the privacy parameter,
usually set to 0.1 [24]. In datasets with unbalanced contributions
by individuals, global sensitivity may be large, and thus a large
amount of noise must be added to all output points. This loses
much of the research utility. Nissim et al. propose a sample-and-
aggregate framework as a way to lower the noise by tailoring it to
the dataset [29]. However, noise is still added to all output points.
Crowd-blending privacy [14] relaxes the notion of differential
privacy, allowing an attacker to learn something about an individual,
if this feature is sufficiently common to be considered not sensitive.
Crowd-blending privacy employs the notion of ϵ-blending.
Definition 2.2. ϵ-blending [14]. An individual x ϵ-blends with individ-
ual y in dataset D with respect to privacy-preserving mechanism San if it
holds that for D′, where x’s data is replaced by y’s data:
567Pr[San(D) ∈ S] ≤ exp(ϵ) · Pr[San(D′) ∈ S]
(2)
Definition 2.3. Crowd-blending privacy [14]. A mechanism San is
(k, ϵ)-crowd-blending private if for every dataset D and every individual
t ∈ D, either t ϵ-blends in a crowd of k individuals in D, or the mechanism
San ignores it.
Crowd-Blending Privacy Mechanism. Gehrke et al. [14] pro-
pose a specific crowd-blending mechanism for histogram queries
over identities, which can be extended into a general mechanism
as follows. The crowd-blending privacy mechanism releases un-
changed those output points where it holds that a crowd of at least
k individuals has the same contributions to the output point. Con-
tributions from the individuals that do not blend in the crowd are
omitted, or the output points that contain them are fuzzed.
Crowd-blending privacy does not compose under multiple queries;
i.e.. it is possible to design t queries that are each crowd-blending
private, but their combination is not [14]. Gehrke et al. propose a
pre-sampling step, where an algorithm chooses at random which
individuals’ data will be used for each query. They prove that crowd-
blending privacy with pre-sampling holds under query composition.
2.1 Motivation for Commoner Privacy
We now advance the need for commoner privacy by discussing
two cases where differential and crowd-blending privacy do not
perform well. In the first case, counting queries are run over records
in a dataset, where a single individual could contribute multiple
records (e.g., a network trace, hospital visit logs, movie reviews).
For example, such a query could ask for the number of packets
sent from the Web server port in a network trace. In the second
case, aggregation queries (average, sum, product, etc.) are run over
data fields in any dataset. For example, such a query could ask for
the sum of all the savings on accounts of a given type, in a given
bank. Depending on the data that is being counted or aggregated,
two phenomena may occur: (1) Long tail. Some individuals may
contribute a large value to some output points, e.g., an active server
may generate so much traffic in the trace that it dominates the
count of packets or bytes in the entire dataset. This will make global
sensitivity large, leading to a large amount of noise added to all
output points by differential privacy, and thus low research utility.
(2) Large value range. If the field has many possible values, many
individuals may have similar but not the same contributions to the
output; e.g., there may be many possible values for the amount on
a savings account. In this case crowd-blending privacy will fuzz
many output points, leading to low research utility.
These cases call for commoner privacy; i.e., we need a privacy
definition that allows a release of aggregate outputs as long as
an individual’s contributions to these outputs are similar to the
contributions of a sufficient number of other individuals—so that
neither can be identified from the aggregate by a realistic adversary.
We now use an example to further illustrate the need for com-
moner privacy. Imagine that a small store releases records with
a product and quantity purchased for each day over two weeks,
illustrated in Table 1. Most products will be purchased by people in
small quantities. For simplicity, assume that there are between 30
and 60 customers each day, and each customer buys exactly 1 or 0
of any given product at any given day, with two exceptions. On the
Sa Su M T W R
Day M T W R F