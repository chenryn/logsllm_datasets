four IP forwarding rule-sets with roughly 180K single-ﬁeld rules
each (i.e., destination IP address).
3 ·106
2
3.51× 3.49× 3.40× 3.56×
1
t
u
p
h
g
u
o
r
h
T
)
s
p
p
(
7.51× 7.84× 7.59× 7.47×
y
c
n
e
t
a
L
)
s
µ
(
400
200
0
5.2 End-to-end performance
For fair comparison, NuevoMatch used the same algorithm for both
the remainder classiﬁer and the baseline. For example, we eval-
uated the speedup produced by NuevoMatch over cs while also
using cs to index the remainder set.
We present the results for random packet traces, followed by
skewed and CAIDA traces.
Large rule-sets: ClassBench: multi-core. Figure 8 shows that, in
the largest rule-sets (500K), the parallel implementation of Nuevo-
Match achieves a geometric mean factor of 2.7×, 4.4×, and 2.6×
lower latency and 1.3×, 2.2×, and 1.2× higher throughput over
cs, nc, and tm, respectively. For the classiﬁers with 100K rules,
the gains are lower but still signiﬁcant: 2.0×, 3.6×, and 2.6× lower
latency and 1.0×, 1.7×, and 1.2× higher throughput over cs, nc,
and tm, respectively. The performance varies among rule-sets, i.e.,
some classiﬁers are up to 1.8× faster than cs for 100 ˙K inputs.
Large rule-sets: ClassBench: single core. Figure 9 shows the
throughput speedup of nm compared to cs, nc, and tm. For 500K
rule-sets, NuevoMatch achieves a geometric mean improvement
of 2.4×, 2.6×, and 1.6× in throughput compared to cs, nc, and
tm, respectively. For the single core execution the latency and the
throughput speedups are the same.
Large rule-sets: Stanford backbone: multi-core. Figure 10
shows the speedup of nm over tm for the real-world Stanford back-
bone dataset with 4 rule-sets. nm achieves 3.5× higher throughput
and 7.5× lower latency over tm on all four rule-sets.
Small rule-sets: multi-core. For rule-sets with 1K and 10K rules,
NuevoMatch results in the same or lower throughput, and 2.2×
and 1.9× on average better latency compared to cs and tm. The
lower speedup is expected, as both cs and tm ﬁt into L1 (§5.2.1), so
nm does not beneﬁt from reduced memory footprint, while adding
computational overheads. See Appendix for the detailed chart.
1
2
3
4
1
2
3
4
TupleMerge
NuevoMatch w/ TupleMerge
Figure 10: End-to-end performance on real Stanford back-
bone data sets.
The cs results are averaged over three rule-sets of 1K and six
rule-sets for 10K. In the remaining rule-sets, NuevoMatch did not
produce large-enough iSets to accelerate the remainder. Note, how-
ever, that it promptly identiﬁes the rule-sets expected to be slow
and falls back to the original classiﬁer.
The source of speedups. The ability to compress the rule-set to
ﬁt into faster memory while retaining fast lookup is the key factor
underlying the performance beneﬁts of NuevoMatch. To illustrate
it, we take a closer look at the performance. We evaluate tm with
and without nm acceleration as a function of its memory footprint
on ClassBench-generated 1K,10K,100K and 500K rule-sets for one
application (ACL).
Figure 11 shows that the performance of tm degrades as the
number of rules grows, causing the hash tables to spill out of L1 and
L2 caches. nm compresses a large part of the rule-set (see coverage
annotations), thereby making the remainder index small enough
to ﬁt in the L1 cache, and gaining back the throughput equivalent
to tm’s on small rule-sets.
ClassBench: Skewed traﬃc. Figure 12 shows the evaluation of
the early termination implementation on skewed packet traces. We
report the throughput speedup of nm compared to cs and tm; the
results for nc are similar to those of cs.
We perform 6000 experiments using 25 traces per rule-set: ﬁve
traces per Zipf distribution plus ﬁve modiﬁed CAIDA traces. We
evaluate over twelve 500K rule-sets and report the geometric mean.
Additionally, we evaluate CAIDA traces in two settings. First, the
t
u
p
h
g
u
o
r
h
T
p
u
d
e
e
p
S
4
3
2
1
0
100K Classiﬁers
500K Classiﬁers
1
2
3
4
5
6
7
8
9
10
11
12 GM 1
2
3
4
5
6
7
8
9
10
11
12 GM
NuevoMatch w/ CutSplit
NuevoMatch w/ NeuroCuts
NuevoMatch w/ TupleMerge
Figure 9: ClassBench: NuevoMatch vs. CutSplit, NeuroCuts, and TupleMerge, using a single CPU core.
)
s
p
p
(
t
u
p
h
g
u
o
r
h
T
6
5
4
3
2
·106
19.5 KB
L1 Size (32KB)
L2 Size (1MB)
14.85:15.6
25%
103
205.0 KB
192.0:192.8
6%
2.9:21.3
99%
2 MB
104
Number of Rules
105
7.9:46.1
99%
10 MB
106
TupleMerge
NuevoMatch w/ TupleMerge
Figure 11: Throughput vs. number of rules for TupleMerge
and NuevoMatch. Annotations are coverage (%) and index
memory size in KB (remainder : total).
classiﬁer runs with access to the entire 16MB of the L3 cache (de-
noted as CAIDA). Second, the classiﬁer use of L3 is restricted to
1.5 ˙MB via Intel’s Cache Allocation Technology, emulating multi-
tenant setting (denoted as CAIDA*).
NuevoMatch is signiﬁcantly faster than cs, but its beneﬁts over
tm diminish for workloads with higher skews. Yet, the speedups
are more pronounced under smaller L3 allocation.
Overall, we observe lower speedups for the skewed traﬃc than
for the random trace. This is not surprising, as skewed traces in-
duce a higher cache hit rate for all the methods, which in turn
reduces the performance gains of nm over both cs and tm, simi-
lar to the case of small rule-sets. Nevertheless, it is worth noting
that classiﬁcation algorithms are usually applied alongside caching
mechanisms that catch the packets’ temporal locality. For instance,
Open vSwitch applies caching for most frequently used rules. It
invokes Tuple Space Search upon cache misses [30]. Therefore, if
NuevoMatch is applied at this stage, we expect it to yield the perfor-
mance gains equivalent to those reported for unskewed workloads.
Open vSwitch integration is the goal of our ongoing work.
5.2.1 Memory footprint comparison. Figure 13 compares the
memory footprint of the classiﬁers without and with NuevoMatch
(the two right-most bars in each bar cluster). We use the same num-
ber of iSets as in the end-to-end experiments. Note that a smaller
footprint alone does not necessarily lead to higher performance if
more iSets are used. Therefore, the results should be considered in
conjunction with the end-to-end performance.
The memory footprint includes only the index data structures
but not the rules themselves. In particular, the memory footprint
t
u
p
h
g
u
o
r
h
T
p
u
d
e
e
p
S
2 .5
2
1 .5
1
0 .5
2.06×
1.95×
1.84×
1.62×
1.79×
2.26×
1.14×
1.06×
0.99×
0.89×
1.05×
1.16×
Zipf 80%
(α =1.05)
Zipf 85%
(α =1.10)
Zipf 90%
(α =1.15)
Zipf 95%
(α =1.25)
CAIDA CAIDA*
NuevoMatch w/ CutSplit
NuevoMatch w/ TupleMerge
Figure 12: ClassBench: NuevoMatch vs. CutSplit and Tuple-
Merge with skewed traﬃc.
for NuevoMatch includes both the RQ-RMI models and the remain-
der classiﬁer. Each bar is the average of all the 12 application rule-
sets of the same size.
For nm we show both the remainder index size (middle bar) and
the total RQ-RMI size (right-most bar). Note that due to the loga-
rithmic scale of the Y axis, the actual ratio bewteen the two is much
higher than it might seem. For example, the remainder for 10K tm
is almost 100× the size of the RQ-RMI. Note also that since we run
nm on two cores, both RQ-RMI and the remainder classiﬁer use
their own CPU caches.
Overall, NuevoMatch enables dramatic compression of the
memory footprint, in particular for 500K rule-sets, with 4.9×, 8×,
and 82× on average over cs, nc and tm respectively.
The graph explains well the end-to-end performance results. For
1K rule-sets, the original classiﬁers ﬁt into the L1 cache, so nm is
not eﬀective. For 10K sets, even though the remainder index ﬁts
in L1, the ratio between L1 and L2 performance is insuﬃcient to
cover the RQ-RMI overheads. For 100K, the situation is similar for
cs; however, for nc, the remainder ﬁts in L1, whereas the original
nc spills to L3. For tm, the remainder is already in L2, yielding a
lower overall speedup compared to nc. Last, for 500K rule-sets, all
the original classiﬁers spill to L3, whereas the remainder ﬁts well
in L2, yielding clear performance improvements.
Performance under L3 cache contention. The small memory
footprint of nm plays an important role even when the rule-index
ﬁts in the L3 cache (16MB in our machine). L3 is shared among all
the CPU cores; therefore, cache contention is not rare, in particular
in data centers. nm reduces the eﬀects of L3 cache contention on
packet classiﬁcation performance. In the experiment we use the
500 ˙K rule-set (1) and compare the performance of cs and nm (with
cs) while limiting the L3 to 1.5MB. cs loses half of its performance,
whereas nm slows down by 30%, increasing the original speedup.
)
s
e
t
y
B
(
e
z
i
S
107
106
105
104
103
102
1024KB L2 Cache Size
32KB L1 Cache Size
1K
NuevoMatch: Remainder
NuevoMatch: iSets
10K
100K
Number of rules
CutSplit
TupleMerge
500K
NeuroCuts
Figure 13: Memory size for CutSplit, NeuroCuts, Tuple-
Merge vs. NuevoMatch with them indexing the remainder.
Each bar is a geometric mean of 12 applications.
Table 2: iSet coverage.
1 iSet
2 iSets
3 iSets
4 iSets
1K
10K
100K