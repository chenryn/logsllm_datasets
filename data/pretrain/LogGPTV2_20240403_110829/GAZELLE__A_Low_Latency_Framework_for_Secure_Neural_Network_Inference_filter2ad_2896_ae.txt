PermDecomp
PermAuto
Speedup
4.1
3.3
4.1
6.1
11.3
3.9
6.5
6.9
4.0
and our library was compiled using GCC 5.4.0 using the
‘-O3’ optimization setting and enabling support for the
AES-NI instruction set. Our schemes are evaluated in the
LAN setting similar to previous work with both instances
in the us-east-1a availability zone.
7.3 Micro-benchmarks
In order to isolate the impact of the various techniques
and identify potential optimization opportunities, we ﬁrst
present micro-benchmarks for the individual operations.
Arithmetic and PAHE Benchmarks. We ﬁrst bench-
mark the impact of the faster modular arithmetic on the
NTT and the homomorphic evaluation run-times. Table
5 shows that the use of a pseudo-Mersenne ciphertext
modulus coupled with lazy modular reduction improves
the NTT and inverse NTT by roughly 7×. Similarly
Barrett reduction for the plaintext modulus improves the
plaintext NTT runtimes by more than 5×. These run-time
improvements are also reﬂected in the performance of the
primitive homomorphic operations as shown in Table 6.
Table 7 demonstrates the noise performance trade-off
inherent in the permutation operation. Note that an
individual permutation after the initial decomposition
is roughly 8-9× faster than a permutation without any
pre-computation. Finally we observe a linear growth in
the run-time of the permutation operation with an increase
in the number of windows, allowing us to trade off noise
performance for run-time if few future operations are
desired on the permuted ciphertext.
Linear Algebra Benchmarks. Next we present micro-
benchmarks for the linear algebra kernels. In particular we
focus on matrix-vector products and 2D convolutions since
these are the operations most frequently used in neural
network inference. Before performing these operations,
the server must perform a one-time client-independent
setup that pre-processes the matrix and ﬁlter coefﬁcients.
In contrast with the ofﬂine phase of 2PC, this computation
is NOT repeated per classiﬁcation or per client and can
be performed without any knowledge of the client keys.
In the following results, we represent the time spent in this
amortizable setup operation as tsetup. Note that toﬄine for
both these protocols is zero.
The matrix-vector product that we are interested in
corresponds to the multiplication of a plaintext matrix with
a packed ciphertext vector. We ﬁrst start with a comparison
of three matrix-vector multiplication techniques:
1. Naive: Every slot of the output is generated indepen-
dently by computing an inner-product of a row of the
matrix with ciphertext column vector.
2. Diagonal: Rotations of the input are multiplied by the
generalized diagonals from the plaintext matrix and
added to generate a packed output.
3. Hybrid: Use the diagonal approach to generate a
single output ciphertext with copies of the output
partial sums. Use the naive approach to generate the
ﬁnal output from this single ciphertext
1664    27th USENIX Security Symposium
USENIX Association
Table 8: Matrix Multiplication Microbenchmarks
#in rot
#out rot
#mac
2048×1
1024×128
1024×16
128×16
N
D
H
N
D
H
N
D
H
N
D
H
0
2047
0
0
1023
63
0
1023
7
0
127
0
11
0
11
1280
1024
4
160
1024
7
112
128
7
1
2048
1
128
2048
64
16
2048
8
16
2048
1
tonline
7.9
383.3
8.0
880.0
192.4
16.2
110.3
192.4
7.8
77.4
25.4
5.3
Input
(W×H, C)
(28×28,1)
Filter
(W×H, C)
(5×5,5)
Table 9: Convolution Microbenchmarks
Algorithm tonline
(ms)
14.4
9.2
107
110
208
195
767
704
I
O
I
O
I
O
I
O
(1×1,128)
(3×3,128)
(3×3,32)
(16×16,128)
(32×32,32)
(16×16,128)
tsetup
16.1
3326.8
16.2
1849.2
1662.8
108.5
231.4
1662.8
21.8
162.5
206.8
10.5
tsetup
(ms)
11.7
11.4
334
226
704
704
3202
3312
We compare these techniques for the following matrix
sizes: 2048 × 1, 1024 × 128, 128 × 16. For all these
methods we report the online computation time and the
time required to setup the scheme in milliseconds. Note
that this setup needs to be done exactly once per network
and need not be repeated per inference. The naive scheme
uses a 20bit plaintext window (wpt) while the diagonal and
hybrid schemes use 10bit plaintext windows. All schemes
use a 7bit relinearization window (wrelin).
Finally we remark that our matrix multiplication scheme
is extremely parsimonious in the online bandwidth. The
two-way online message sizes for all the matrices are
given by (w + 1)∗ ctsz where ctsz is the size of a single
ciphertext (32 kB for our parameters).
Next we compare the two techniques we presented for
2D convolution: input rotation (I) and output rotation
(O) in Table 9. We present results for four convolution
sizes with increasing complexity. Note that the 5 × 5
convolution is strided convolution with a stride of 2. All
results are presented with a 10bit wpt and a 8bit wrelin.
As seen from Table 9, the output rotation variant is
Square
Algorithm Outputs
Table 10: Activation and Pooling Microbenchmarks
tonline BWoﬄine BWonline
(MB)
(ms)
1.4
0.093
1.68
15
16.8
136
8.39
58
513
83.9
toﬄine
(ms)
0.5
89
551
164
1413
2048
1000
10000
1000
10000
5.43
54.3
15.6
156.0
MaxPool
ReLU
(MB)
0
usually the faster variant since it reuses the same input
multiple times. Larger ﬁlter sizes allow us to save more
rotations and hence experience a higher speed-up, while
for the 1×1 case the input rotation variant is faster. Finally,
we note that in all cases we pack both the input and output
activations using the minimal number of ciphertexts.
Square, ReLU and MaxPool Benchmarks. We round
our discussion of the operation micro-benchmarks with the
various activation functions we consider. In the networks
of interest, we come across two major activation functions:
Square and ReLU. Additionally we also benchmark the
MaxPool layer with (2×2)-sized windows.
For square pooling, we implement a simple interactive
protocol using our additively homomorphic encryption
scheme. For ReLU and MaxPool, we implement a garbled
circuit based interactive protocol. The results for both are
presented in Table 10.
8 Network Benchmarks and Comparison
Next we compose the individual layers from the previous
sections and evaluate complete networks. For ease of
comparison with previous approaches, we report runtimes
and network bandwidth for MNIST and CIFAR-10 image
classiﬁcation tasks. We segment our comparison based on
the CNN topology. This allows us to clearly demonstrate
the speedup achieved by Gazelle as opposed to gains
through network redesign.
The MNIST Dataset. MNIST is a basic image classi-
ﬁcation task where we are provided with a set of 28×28
grayscale images of handwritten digits in the range [0−9].
Given an input image our goal is to predict the correct
handwritten digit it represents. We evaluate this task
using four published network topologies which use a
combination of FC and Conv layers:
A 3-FC with square activation from [30].
B 1-Conv and 2-FC with square activation from [18].
C 1-Conv and 2-FC with ReLU activation from [36].
D 2-Conv and 2-FC with ReLU and MaxPool from [29].
Runtime and the communication required for classify-
ing a single image for these four networks are presented
in table 11.
For all four networks we use a 10bit wpt and a 9bit wrelin.
USENIX Association
27th USENIX Security Symposium    1665
Table 11: MNIST Benchmark
Runtime (s)
Framework
SecureML
MiniONN
Gazelle
CryptoNets
MiniONN
Gazelle
DeepSecure
Chameleon
Gazelle
MiniONN
ExPC
Gazelle
A
B
C
D
Ofﬂine Online
0.18
0.14
0.03
4.7
0.9
0
-
0.88
-
0.4
0.03
1.36
0.05
5.74
-
-
0.33
0
-
1.34
0.15
3.58
-
0.481
Communication (MB)
Total
Total Ofﬂine Online
4.88
1.04
0.03
297.5
1.28
0.03
9.67
2.7
0.20
9.32
5.1
0.81
-
12
0.5
-
44
0.5
-
5.1
2.1
636.6
-