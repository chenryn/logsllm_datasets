Target Model
Target Model Prediction
Fig. 4: The workﬂow of our jumbo MNTD approach with query-tuning. The solid lines represent the training process and dashed ones show
the test process.
to different types of data and attacks. Let x ∈ Rdx denote
the benign input with ground truth label y ∈ {1, . . . , c}.
In order to trigger the Trojan to output the malicious label
yt ∈ {1, . . . , c}, the adversary will modify the input into x(cid:3)
with the Trojan function I such that:
= I(x, y; m, t, α, yt)
= (1 − m) · x + m ·(cid:3)
Trojans using the representation vectors. We propose to
choose a set of queries to extract important representation
from the shadow models and use the resulting vectors as
the input to the meta-classiﬁer. We jointly optimize the
meta-classiﬁer and the query set in multiple iterations and
effectively improve the performance of the trained meta-
classiﬁer.
(cid:4)
3) Target model detection. Given a target model, we will ﬁrst
leverage the optimized query set to extract the represen-
tation of the model. We then feed the representation to
the meta-classiﬁer to determine whether the target model
is Trojaned or not.
In the following, we ﬁrst introduce our approach to shadow
model generation - the jumbo learning in Section IV-A. Then,
in Section IV-B we introduce the way to perform meta-training
having the set of benign and Trojaned shadow models, as
well as a baseline meta-training algorithm which only uses
benign shadow models. Finally, we introduce the target model
detection step in Section IV-C.
A. Shadow Model Generation - Jumbo Learning
Suppose the task is a c-way classiﬁcation and the input
dimension is dx. The ﬁrst step of our defense pipeline is to
generate a set of benign and Trojaned shadow models, based
on which the defender can later train the meta-classiﬁer to
distinguish between them. The benign shadow models can
be generated by training on the clean dataset with different
model parameter initialization. The important part is how to
generate Trojaned shadow models. We hope to generate a
variety of Trojaned models so that the trained meta-classiﬁer
can generalize to detect different types of Trojans. This is
essential because we assume that the attacker may apply any
attack strategy.
To this end, we propose jumbo learning, which models a
generic distribution of Trojan attack settings and generates
a variety of different Trojaned models. In jumbo learning,
we will ﬁrst sample different Trojan attack settings. We
parametrize the attack setting as a function I which is general
x(cid:3), y(cid:3)
x(cid:3)
y(cid:3)
= yt
(1 − α)t + αx
(3)
(4)
(5)
where m ∈ {0, 1}dx is the mask for the trigger (i.e., shape
and location), t ∈ Rdx is the pattern and α is the transparency
inserted to x. This function I is generally applicable to
different Trojan attack settings and tasks. For example, in a
modiﬁcation attack, m is a small pattern and α = 0; in a
blending attack m = 1 everywhere and 1 − α is the blending
ratio; on audio data m refers to the time period for inserting
the Trojaned audio signal. We will sample random m, t, α, yt
to get different Trojan attack settings. In Figure 5, we show
some examples of the sampled Trojan triggers on the MNIST
dataset.
Having the sampled attack settings, we will train a model
with respect to each Trojan setting. We propose to apply the
data poisoning attack that injects a proportion p of malicious
data into the clean dataset. That is, we extract a proportion p
of data samples from the dataset we have, apply Eqn.3 to get
their Trojaned versions, then inject them back to the dataset
to train the shadow models.
The jumbo learning pipeline is shown in Algorithm 1. In
order to generate the set of Trojaned shadow models, we ﬁrst
randomly sample the Trojan attack settings (line 3). Then we
poison the dataset (line 4-8) according to the setting and train
the Trojaned shadow model (line 9). We repeat the process
multiple times to generate a set of different Trojaned models.
The sampling algorithm (line 3) and model training algorithm
(line 9) will be different for different tasks.
Note that the Trojan distribution deﬁned by Eqn.3 does not
capture all kinds of Trojans. We will evaluate how the trained
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
107
Algorithm 1: The pipeline of jumbo learning to gen-
erate random Trojaned shadow models.
Input: Dataset D = {(xi, yi)}n
i=1, number of Trojaned
shadow models to train m.
models.
Output: models: a set of m random Trojaned shadow
1 models ← [];
2 for u = 1, . . . , m do
3
4
5
6
7
8
9
10
11 return models
m, t, α, yt, p = generate random setting();
Dtroj ← D;
indices = CHOOSE(n, int(n ∗ p));
for j in indices do
(cid:5)
← I(xj, yj; m, t, α, yt);
fu ← train shadow model(Dtroj);
models.append(fu);
x(cid:3)
Dtroj ← Dtroj
(x(cid:3)
, y(cid:3)
, y(cid:3)
j);
j
j
j
Fig. 5: Examples of different Trojan patterns generated by our jumbo
learning on the MNIST dataset. The trigger patterns in the ﬁrst ﬁve
examples are highlighted with red bounding boxes. The last example
is a data sample blended with random pixels.
meta-classiﬁer performs in detecting the unforeseen Trojaned
models in Section VII. In addition, it is possible that some
future Trojans may occur which are not in the format as Eqn.3.
In that case, we can modify the jumbo distribution to include
the new type of Trojans. Therefore, we conclude that jumbo
learning is a generic way to generate a variety of Trojaned
models.
B. Meta-training
The defender will perform the meta-training algorithm
based on the set of shadow models generated by jumbo
learning. Our meta-training consists of two goals: 1) ﬁnd a
feature extraction function to extract representation vectors of
the shadow models and 2) train a meta-classiﬁer to distinguish
between benign and Trojaned shadow models. In the follow-
ing, we will ﬁrst introduce our designs of the feature extraction
function and the meta-classiﬁer. Then we introduce query-
tuning, a meta-training algorithm to jointly optimize the two
components. Finally we will talk about a baseline algorithm
when the defender does not apply jumbo learning and only
have a set of benign shadow models.
Feature Extraction Function Design We propose to feed a
set of queries to the shadow model and use the output vector
as its representation features. We have two intuitions for this
design. First, Trojaned models will behave differently with
benign models (i.e., have different distributions of outputs) on
some query inputs. For example, their outputs differ a lot on
inputs with Trojan triggers. Second, we can get the model
output without accessing its internal structure. This allows us
to detect Trojans in black-box scenarios.
Formally speaking, Let {(fi, bi)}m
i=1 denote the shadow
model dataset, where fi : Rdx → {1, . . . , c} is the shadow
model and bi is a binary label indicating whether fi is Trojaned
(bi = 1) or benign (bi = 0). We will choose a set of k query
inputs X = {x1, . . . , xk} where xi ∈ Rdx (we will discuss
how these query inputs are chosen later). We will feed the
queries into the shadow model fi and get k output vectors
{fi(x1), . . . , fi(xk)}. By concatenating all the output vectors,
we can get a representation vector Ri(X) as the feature of
the shadow model fi:
F(fi) = Ri(X) = [[fi(x1)|| . . .||f i(xk)]] ∈ Rck
(6)
where [[·||·||·]] stands for the concatenation operation. We use
Ri to denote Ri(X) if it does not lead to misunderstanding.
Let META(Ri; θ) ∈ R denote the
Meta-classiﬁer Design
meta-classiﬁer where θ denotes the parameter of META. We
propose to use a two-layer fully connected neural network
as the meta-classiﬁer. The meta-classiﬁer will
take in the
feature vector Ri and output a real-valued score indicating
the likelihood of fi to be Trojaned.
(cid:6)
m(cid:2)
(cid:7)
In meta-training, we would like
Meta-Training Algorithm
to ﬁnd the optimal values in the query set X and meta-
classiﬁer parameters θ. One simple solution is to randomly
choose the query set X, pre-calculate all the representation
vectors Ri and only optimize the meta-classiﬁer. Having Ri
and corresponding label bi, the meta-training is simply to
minimize the loss of a binary classiﬁer via gradient-based
optimization:
META(Ri(X); θ), bi
L
θ
i=1
arg min
(7)
where L(·,·) is the loss function using binary cross entropy.
This approach achieves a not bad performance in Trojan
detection. However, the randomly sampled inputs may not help
distinguish the benign and Trojaned models because Trojaned
models behave similarly with benign models on most inputs.
Therefore, we can improve the performance by ﬁnding the
optimal query inputs to provide the most useful information
in the representation vectors.
To this end, we propose a query-tuning technique to ﬁnd the
best query set for feature extraction, which is similar with the
technique in Oh et al. [45]. The main idea is to jointly optimize
the query set and the meta-classiﬁer in order to minimize the
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
108
(cid:7)
training loss. The optimization goal thus becomes:
META(Ri(X); θ), bi
m(cid:2)
arg min
(cid:6)
L
(8)
X={x1,...,xk}
θ
i=1
Note that the query set {x1, . . . , xk} does not appear explicitly
in the optimization goal, but are included in the calculation of
Ri(X).
To optimize the goal in Eqn. 8, a key observation is that
the entire calculation ﬂow is differentiable: we ﬁrst feed the
query inputs into the shadow models, then use their output
as the representation vectors of shadow models, and ﬁnally
feed them into the meta-classiﬁer. Since the shadow models
and the meta-classiﬁer are differentiable, we can directly
calculate the gradient of the loss with respect to the input
vectors {x1, . . . , xk}. Thus, we can still apply the standard
gradient-based optimization technique for solving Eqn. 8. In
particular, we will ﬁrst randomly sample each xi from a
Gaussian distribution. Then we iteratively update xi and θ
with respect to the goal in Eqn. 8 to ﬁnd the optimal query
set. This workﬂow is illustrated in the right part of Figure 4.
Note that during the training process we need to access the
internal parameters of the shadow models for calculating the
gradient. However, this does not violate the black-box setting
because the shadow models are trained by us and we can for
sure access their parameters. During the inference process, we
only need to query the black-box target model with the tuned
inputs {x1, . . . , xk} and use the output for detection.
Baseline Meta-training algorithm without
jumbo learning
The previous meta-training algorithm requires a set of Tro-
janed shadow models generated by jumbo learning. In the fol-
lowing, we will introduce a baseline meta-training algorithm
which requires only benign shadow models and thus does not
need jumbo learning.
We assume that
the defender has only benign shadow
models to train the meta-classiﬁer. The standard way to train
a machine learning model with only one-class data is novelty
detection, where the model is trained to determine whether an
input is similar with its training samples. As an example, one-
class SVM [41] will train a hyper-plane which separates all the
training data from the origin while maximizing the distance ρ
from the origin to the hyper-plane. An example of one-class
SVM is shown in Appendix B.
In practice, we propose to use one-class neural network [11]
which generalizes the one-class optimization goal to neural
networks, so that our meta-classiﬁer can still be the two-layer
network structure. The optimization goal in one-class neural
network with query-tuning is:
m(cid:2)
(cid:3)
ReLU
ρ−META(Ri(X); θ)
min
θ,ρ
X={x1,...,xk}
·l2(θ)+
· 1
m
1
ν
1
2
i=1
(9)
where l2(θ) stands for the sum of Frobenius norm of all the
parameters in the meta-classiﬁer. We will use a gradient-based
approach to do meta-training and ﬁnd the optimal query set X
and model parameters θ which minimize the goal as in Eqn. 9.
k
= {x∗
Let X∗
C. Target Model Detection
1, . . . , x∗
} and θ∗ denote the optimal query
set and parameters obtained by meta-training. Given a target
model ftgt, we can apply the optimized feature extraction
function and meta-classiﬁer to determine whether it is Tro-
janed or not. In particular, we ﬁrst calculate its representa-
tion vector Rtgt = [[ftgt(x∗
k)]]. Then we can
determine whether ftgt is Trojaned according to the meta-
classiﬁer’s prediction META(Rtgt; θ∗
1)|| . . .||f tgt(x∗
).
V. EXPERIMENT SETUP
In this section, we will introduce the experiment setting.
We ﬁrst
introduce the datasets we use, followed by the
attack and defense setting in the experiments. Finally we
introduce the setting of baselines which we compare with. Our
code is publically available at https://github.com/AI-secure/
Meta-Nerual-Trojan-Detection.
A. Dataset
We conduct our evaluation on a variety of machine learning
tasks, covering different types of datasets and neural networks.
For the vision tasks, we use the standard MNIST [33] and
CIFAR10 [31] datasets. MNIST is a dataset of grayscale hand-
written digits 0-9 and CIFAR10 is a dataset of RGB images of
10 classes. For the speech task we use the SpeechCommand
dataset (SC) containing one-second audio command of 10
classes. For the tabular data we use the Smart Meter Electricity
Trial data in Ireland dataset (Irish) [2]. It consists of electricity
consumption over weeks of two types of users (residential vs.
commercial). For the natural languange data we use the Rotten
Tomatoes movie review dataset (MR) [29] which consists of
movie reviews and the task is to determine whether a review is
positive or negative. We provide detailed introduction of each
dataset and network structure in each task in Appendix C.
As discussed in Section III-C, we assume the defender only
has a small set of clean data to help with the detection and the
data is different from the model training set (which is owned
by the attacker). Therefore, for each dataset, we randomly
sample 50% of the training set as owned by the attacker and
2% of the training data as the defender’s clean dataset.
B. Attack Settings
Here we introduce the attack setting that are modelled
in our jumbo distribution as in Section IV-A. Unforeseen
attack strategies will be evaluated in Section VII, which
include parameter and latent attack (since they will not poison