title:Benchmarking The Dependability of Windows NT4, 2000 and XP
author:Ali Kalakech and
Karama Kanoun and
Yves Crouzet and
Jean Arlat
Benchmarking The Dependability of Windows NT4, 2000 and XP*
Ali Kalakech, Karama Kanoun, Yves Crouzet and Jean Arlat
LAAS-CNRS, 7, Avenue Colonel Roche 31077 Toulouse Cedex 4, France
{kalakech, kanoun, crouzet, arlat}@laas.fr
Abstract
The aim of this paper  is  to  compare the  dependability
of  three  operating  systems  (Windows  NT4,  Windows
2000  and  Windows  XP)  with  respect  to  erroneous
behavior  of  the  application  layer.  The  results  show  a
similar  behavior  of  the  three  OSs  with  respect  to
robustness  and  a  noticeable  difference  in  OS  reaction
and  restart  times.  They  also  show  that  the  application
state  (mainly  the  hang  and  abort  states)  significantly
impacts the restart time for the three OSs
1. Introduction
System  developers  are  increasingly  resorting  to  off-
the-shelf operating systems  (commercial or  open  source),
even  in  critical  application  domains.  However,  any
malfunction  of  the  Operating  System  (OS)  may  have  a
strong impact  on  the  dependability of  the  global  system.
Therefore, it  is  important  to  make information  about  the
OS  dependability  available,  despite 
lack  of
information  issued  from  its  development.  The  current
trend is to use dependability benchmarks [1-3].
the 
The  aim  of  an  OS  dependability  benchmark  is  to
objectively  characterize  the  OS  behavior  in  presence  of
faults.  A  dependability  benchmark 
is  based  on
experimentation on  the OS.  Its  results  are  intended  i) to
characterize  qualitatively  and  quantitatively 
the  OS
behavior  in  the  presence  of  faults  and  ii) to  evaluate
performance-related  measures  in  the  presence  of  faults.
These results  can help  in  selecting  the  most  appropriate
OS,  based  on  the  benchmark  measures  evaluated,  in
to 
complement 
(e.g., performance,
maintenance, etc.).
criteria 
other 
The work reported here is part of  the European project
on  Dependability  Benchmarking,  DBench  [4,  5],  whose
objectives  are  to  i) define  a  framework  for  designing
dependability benchmarks for computer systems and to  ii)
implement  examples  of  benchmark  prototypes.  Our
previous  work,  [6],  gives  the  specification  of  an  OS
dependability  benchmark  and  presents  the  experimental
* This work is partially  supported  by  the  European  Commission  -  IST
DBench project (IST-2000-25425)
framework as  well  as  some  preliminary results  related  to
Windows 2000.  This  paper is  aimed at  further  exploring
the portability  and suitability  of  the  proposed benchmark
by  applying  it  to  two  other OSs  from  the  same  family,
namely Windows NT4 and Windows XP professional.
the 
issues 
Several  relevant  attempts  have  been  previously
proposed  to  help  characterize  the  failure  modes  and
robustness  of  software  executives.  A  comprehensive
analysis  of 
and
dependability can be found in [7].  The executives targeted
in  these  studies  encompass  real  time  microkernels  and
general  purpose  OSs  [1,  8].  The  work  reported  in  [9]
specifically  addressed  the  robustness  of 
the  Win32
application  programming  interface  which  is  the  case  of
our experiments.
robustness 
linking 
The  remainder  of  the  paper  is  organized  as  follows.
Section  2  summarizes  the  benchmark  and  describes  a
particular  prototype  for  Windows  family.  Section  3
presents comparison results obtained using this  prototype.
Section 4 concludes the paper.
2. OS Dependability Benchmark Summary
A  dependability  benchmark  should  define  clearly:
i) the  benchmarking context,  ii)  the  benchmark  measures
and  measurements  to  be  performed  on  the  system  for
obtaining  them,  iii)  the    benchmark  execution  profile  to
be  used  and  iv)  the  set-up  and  related  implementation
issues required for running a benchmark prototype.
The benchmark results  can be meaningful,  useful  and
interpretable only if all  these items  are provided with  the
results.  The detailed definition  of  these items,  related  to
the  OS  benchmark  used  in  this  paper  are  given  in  [6].
They are summarized hereafter to  allow  understanding  of
the results presented in this paper.
2.1.  Benchmarking  Context
The benchmark target corresponds to  an OS  with  the
minimum  set  of  device drivers  necessary  to  run  the  OS
under  the  benchmark  execution  profile.  The  three  OS
targets are Windows NT4  with  Service Pack 6,  Windows
2000 Professional with  Service Pack 4  and Windows XP
Professional  with  Service  Pack  1.  All  the  experiments
have been run on the same platform, composed of an Intel
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:02 UTC from IEEE Xplore.  Restrictions apply. 
Pentium  III Processor, 800  MHz,  and a  memory  of  512
MB. The hard disk is 18 GB, ULTRA 160 SCSI.
Our  dependability  benchmark 
robustness
benchmark. Robustness  can be viewed as  OS  capacity to
resist/react to  faults  induced by  the  applications  running
on  top  of  it,  or  originating  from  the  hardware  layer  or
from device drivers.
is  a 
inputs  provided  by 
We  emphasize  in  this  work  the  OS  robustness  as
the
regards  possible  erroneous 
application  software  to 
the  OS  via  the  Application
Programming  Interface  (API).  We  mainly  consider
corrupted  parameters  in  system  calls.  For  the  sake  of
conciseness, such erroneous inputs  are shortly  referred  to
as  faults.  Results  concerning the  robustness  with  respect
to faults in device drivers can be found in [10-12].
The benchmark addresses the  user perspective, i.e.,  it
is primarily intended to be performed by (and to  be useful
for) someone or an entity who has no in  depth  knowledge
about  the  OS  and whose aim  is  to  significantly  improve
her/his knowledge about its behavior in presence of faults.
In  practice,  the  user  may  well  be  the  developer  or  the
integrator  of  a  system  including  the  OS.  The  OS  is
considered as a “black box” and the source code does not
need to be available. The only required information  is  the
description of  the  services  provided  by  the  OS  and  the
description of the OS in terms of system calls.
2.2.  Benchmark  Measures
Corrupted  system  calls  are  provided  to 
the  OS
through  the  Win32  environment subsystem,  as  the  three
considered OSs cannot run without it [13].  Win32  is  thus
the  API 
current  benchmark
environment.
considered 
in  our 
The  OS  behavior  is  characterized  by  the  various
outcomes at the API level, while the impact of  OS  on  the
application behavior is observed at the workload level.
After execution of  a corrupted system  call,  the  OS  is  in
one of the states defined in Table 1.
Table 1: OS outcomes
An error code is returned
An exception is raised, processed and notified to the application
SEr
SXp
SPc Panic  state
SHg Hang state
SNS None of the above situations is observed (No-signaling state)
The  OS  Robustness  Measure  is  defined  as  the
percentage of experiments leading to  any of  the  outcomes
listed in Table 1.
Reaction  Time  (Texec) corresponds to  the  mean time
necessary  for  the  OS  to  respond  to  a  system  call  in
presence of faults,  either by  signaling  an exception or  by
issuing  an  error  code  or  by  executing  the  required
instructions.
Restart  Time  (Tres)  corresponds  to  the  mean  time
necessary for the  OS  to  restart after the  execution  of  the
workload in the presence of faults.
Texec and Tres are also  observed in  absence of  faults,
for  comparison  purpose.  They  are  respectively  denoted
τexec and τres.
The  benchmark 
temporal  measures  are  primarily
evaluated as a mean time over all  experiments categorized
by a  specific outcome.  However, standard deviation  is  of
prime  interest  as  well.  Table  2  recapitulates 
these
temporal measures.
Table 2: OS temporal measures
Time for the OS to execute a system call in absence of faults
Time for the OS to execute a system call in presence of faults
Duration of OS restart in absence of faults
Duration of OS restart in presence of faults
τexec
Texec
τres
Tres
The workload is characterized by  one of  the  following
outcomes: i) the  workload completes with  correct results,
ii) it  completes  with  erroneous  results  and 
iii) the
workload is  aborted or hangs.  Clearly,  the  workload  can
end  up  in  any  of  the  three  states  irrespective  of  the
outcomes of  the  OS.  Conversely,  whenever the  OS  is  in
the  Panic state,  this  can only  lead the  workload to  abort
or  hang,  while  an  OS  Hang  necessarily  leads  the
workload to  hang.  In  [6],  we  have  detailed  all  possible
combined  outcomes  and  defined  a  set  of  measures
characterizing  the  OS  taking  into  account  the  workload
states.  In  this  paper,  we  mainly  use  information  on  the
workload  final  states  to  examine  the  impact  of  the
workload state on system restart time.
2.3.  Benchmark  Execution  Profile
In the case of performance benchmarks, the  benchmark
execution  profile  is  simply  a  workload  that  is  as
representative as  possible  for the  system  under  test.  For
dependability benchmarks, the  execution  profile  includes
in  addition  corrupted parameters in  system  calls.  The set
of corrupted parameters is referred to as the faultload.
From  a practical point  of  view,  the  faultload  can  be
either integrated within  the  workload (i.e.,  the  faults  are
embedded in the program being executed) or provided in a
separate module.  For  enhanced  flexibility,  we  made  the
latter  choice: 
faultload  are
implemented separately.
the  workload  and 
the 
The prototype we have developed uses a TPC-C  client
[14] as  a privileged workload to  be in  conformance  with
the  experiments  performed  on  transactional  systems  in
DBench [15,  16].  We simply  use  the  TPC-C  client  as  a
workload, but  we  do  not  use  the  performance  measures
specified by TPC-C as they are far from being  suitable  to
characterize the behavior of an OS.  
The faultload is  defined by:  i) the  technique used for
corrupting  the  system  call  parameters  and  ii) the  set  of
system calls to be faulted.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:02 UTC from IEEE Xplore.  Restrictions apply. 
Parameter  Corruption  Technique:  We  use  a
parameter corruption technique similar  to  the  one used in
[17],  relying  on  thorough  analysis  of  system  call
parameters to  define selective substitutions to  be  applied
to  these parameters. A  parameter is  either a data  element
or an address. The value of a data can be substituted either
by an out-of-range value or  by  an incorrect (but  not  out-
of-range) value, while an address can be substituted  by  an
incorrect  (but  existing)  address  (containing  usually  an
incorrect  or  out-of-range  data).  We  have  used  a  mix  of
these three corruption techniques.
interesting 
System  Calls  Corrupted:  Ideally,  and  without  any
time  limitation,  all  system  calls  used  in  the  workload
with parameters should be corrupted. For small workloads
this  might  be possible.  However, for  workloads  such  as
TPC-C client  (that involves  more than  130  system  calls,
with  several  occurrences  in  the  program),  this  would
require several weeks of experimentation. In  addition,  all
system  calls  are  not  necessarily 
to  be
corrupted. Indeed, using a fully  automated benchmark set
up,  an  experiment  lasts  5 minutes  on  average  and,
roughly  speaking,  about  1400  experiments  can  be
achieved  in  5  days.  This  leads  to  consider  30  to  60
system  calls to  be corrupted for a 5-day  fully  automated
benchmark  execution.  Accordingly,  we  have  targeted
system  calls  related 
the  following  components:
Processes  and  Threads,  File  Input/Output,  Memory
Management  and  Configuration  Manager.  Thus  28
system  calls have been targeted, for which 75  parameters
have  been  corrupted  leading  to  552  corrupted  values,
to  552  experiments  using 
hence 
the  benchmark
experimental set-up presented hereafter.
to 
by  substitution  values.  Also,  we  have  added  several
modules in the library  to  observe the  reactions of  the  OS
after parameter  substitution,  and  to  retrieve  the  required
measurements.
Figure 1. Experimental set-up
The  experiment  steps  are  illustrated  in  Figure  2  in
case  of  workload  completion.  In  case  of  workload
abort/hang state, the end of the experiment is  provided by
a watchdog timeout. As the average time necessary for the
OS to execute the TPC-C client is about 70 seconds  when
no faultload is applied, the timeout is of 5 minutes.
Workload execution time