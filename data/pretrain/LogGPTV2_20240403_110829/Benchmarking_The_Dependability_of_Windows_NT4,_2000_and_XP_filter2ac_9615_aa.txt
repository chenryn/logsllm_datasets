# Benchmarking the Dependability of Windows NT4, 2000, and XP

**Authors:**
- Ali Kalakech
- Karama Kanoun
- Yves Crouzet
- Jean Arlat

**Affiliation:**
LAAS-CNRS, 7, Avenue Colonel Roche, 31077 Toulouse Cedex 4, France
{kalakech, kanoun, crouzet, arlat}@laas.fr

**Abstract:**
This paper compares the dependability of three operating systems (Windows NT4, Windows 2000, and Windows XP) in terms of their response to erroneous behavior at the application layer. The results indicate similar robustness across the three OSs but highlight notable differences in reaction and restart times. Additionally, the state of the application, particularly hang and abort states, significantly impacts the restart time for all three OSs.

## 1. Introduction
System developers are increasingly using off-the-shelf operating systems (both commercial and open source), even in critical application domains. However, any malfunction of the Operating System (OS) can have a significant impact on the overall system's dependability. Therefore, it is crucial to provide information about the OS's dependability, despite the lack of detailed development data. The current trend is to use dependability benchmarks [1-3].

The aim of an OS dependability benchmark is to objectively characterize the OS's behavior in the presence of faults. Such a benchmark is based on experimentation with the OS, providing both qualitative and quantitative measures of the OS's behavior under fault conditions and evaluating performance-related metrics. These results can help in selecting the most appropriate OS based on the evaluated benchmark measures, complementing other criteria such as performance and maintenance.

This work is part of the European project on Dependability Benchmarking, DBench [4, 5], which aims to define a framework for designing dependability benchmarks for computer systems and to implement prototype benchmarks. Our previous work [6] provides the specification of an OS dependability benchmark and presents some preliminary results related to Windows 2000. This paper further explores the portability and suitability of the proposed benchmark by applying it to two other OSs from the same family: Windows NT4 and Windows XP Professional.

Several relevant studies have been conducted to characterize the failure modes and robustness of software executives, including real-time microkernels and general-purpose OSs [1, 8]. The work in [9] specifically addressed the robustness of the Win32 application programming interface (API), which is the focus of our experiments.

The remainder of the paper is organized as follows: Section 2 summarizes the benchmark and describes a specific prototype for the Windows family. Section 3 presents the comparison results obtained using this prototype. Section 4 concludes the paper.

## 2. OS Dependability Benchmark Summary
A dependability benchmark should clearly define:
1. The benchmarking context.
2. The benchmark measures and measurements to be performed on the system.
3. The benchmark execution profile to be used.
4. The set-up and related implementation issues required for running the benchmark prototype.

The benchmark results are meaningful, useful, and interpretable only if all these items are provided. The detailed definition of these items, related to the OS benchmark used in this paper, is given in [6]. They are summarized here to facilitate understanding of the results presented in this paper.

### 2.1. Benchmarking Context
The benchmark targets an OS with the minimum set of device drivers necessary to run the OS under the benchmark execution profile. The three OS targets are:
- Windows NT4 with Service Pack 6
- Windows 2000 Professional with Service Pack 4
- Windows XP Professional with Service Pack 1

All experiments were conducted on the same platform, consisting of an Intel Pentium III Processor (800 MHz) with 512 MB of memory and a 18 GB ULTRA 160 SCSI hard disk.

Our dependability benchmark focuses on robustness, defined as the OS's capacity to resist or react to faults induced by applications, hardware, or device drivers. We emphasize the OS's robustness in handling erroneous inputs provided by application software through the Application Programming Interface (API), primarily considering corrupted parameters in system calls. For brevity, such erroneous inputs are referred to as faults. Results concerning the robustness with respect to faults in device drivers can be found in [10-12].

The benchmark addresses the user perspective, intended to be performed by and useful for someone or an entity with no in-depth knowledge of the OS, aiming to improve their understanding of its behavior in the presence of faults. In practice, the user may be the developer or integrator of a system including the OS. The OS is considered a "black box," and the source code is not required; only the description of the services provided by the OS and the system calls is needed.

### 2.2. Benchmark Measures
Corrupted system calls are provided to the OS through the Win32 environment subsystem, as the three considered OSs cannot run without it [13]. Win32 is thus the API considered in our current benchmark environment.

The OS behavior is characterized by various outcomes at the API level, while the impact of the OS on the application behavior is observed at the workload level. After executing a corrupted system call, the OS can be in one of the states defined in Table 1.

**Table 1: OS Outcomes**
| Outcome | Description |
|---------|-------------|
| SEr     | An error code is returned |
| SXp     | An exception is raised, processed, and notified to the application |
| SPc     | Panic state |
| SHg     | Hang state |
| SNS     | No-signaling state (none of the above situations is observed) |

The **OS Robustness Measure** is defined as the percentage of experiments leading to any of the outcomes listed in Table 1.

**Reaction Time (Texec)**: The mean time necessary for the OS to respond to a system call in the presence of faults, either by signaling an exception, issuing an error code, or executing the required instructions.

**Restart Time (Tres)**: The mean time necessary for the OS to restart after the execution of the workload in the presence of faults.

Texec and Tres are also observed in the absence of faults for comparison purposes and are denoted as τexec and τres, respectively.

The benchmark temporal measures are primarily evaluated as the mean time over all experiments categorized by a specific outcome. Standard deviation is also of interest. Table 2 summarizes these temporal measures.

**Table 2: OS Temporal Measures**
| Measure | Description |
|---------|-------------|
| τexec   | Time for the OS to execute a system call in the absence of faults |
| Texec   | Time for the OS to execute a system call in the presence of faults |
| τres    | Duration of OS restart in the absence of faults |
| Tres    | Duration of OS restart in the presence of faults |

The workload is characterized by one of the following outcomes:
1. The workload completes with correct results.
2. The workload completes with erroneous results.
3. The workload is aborted or hangs.

Clearly, the workload can end up in any of the three states, irrespective of the OS outcomes. Conversely, when the OS is in the Panic state, this can only lead the workload to abort or hang, while an OS Hang necessarily leads the workload to hang. In [6], we detailed all possible combined outcomes and defined a set of measures characterizing the OS, taking into account the workload states. In this paper, we primarily use information on the workload's final states to examine the impact of the workload state on system restart time.

### 2.3. Benchmark Execution Profile
For performance benchmarks, the benchmark execution profile is a workload that is as representative as possible for the system under test. For dependability benchmarks, the execution profile includes corrupted parameters in system calls, referred to as the faultload.

From a practical standpoint, the faultload can be integrated within the workload (i.e., the faults are embedded in the program being executed) or provided in a separate module. For enhanced flexibility, we chose the latter: the workload and the faultload are implemented separately.

The prototype we developed uses a TPC-C client [14] as a privileged workload to conform with the experiments performed on transactional systems in DBench [15, 16]. We use the TPC-C client as a workload but do not use the performance measures specified by TPC-C, as they are not suitable for characterizing the behavior of an OS.

The faultload is defined by:
1. The technique used for corrupting the system call parameters.
2. The set of system calls to be faulted.

**Parameter Corruption Technique**: We use a parameter corruption technique similar to the one used in [17], relying on thorough analysis of system call parameters to define selective substitutions. A parameter can be a data element or an address. The value of a data element can be substituted by an out-of-range value or an incorrect (but not out-of-range) value, while an address can be substituted by an incorrect (but existing) address (usually containing incorrect or out-of-range data). We used a mix of these three corruption techniques.

**System Calls Corrupted**: Ideally, all system calls used in the workload with parameters should be corrupted. However, for large workloads like the TPC-C client (which involves more than 130 system calls with multiple occurrences), this would require several weeks of experimentation. Using a fully automated benchmark setup, an experiment lasts about 5 minutes on average, and roughly 1400 experiments can be achieved in 5 days. This leads to considering 30 to 60 system calls to be corrupted for a 5-day fully automated benchmark execution. Accordingly, we targeted system calls related to the following components:
- Processes and Threads
- File Input/Output
- Memory Management
- Configuration Manager

Thus, 28 system calls were targeted, with 75 parameters corrupted, leading to 552 corrupted values and 552 experiments using the benchmark experimental setup.

**Experimental Setup**: Figure 1 illustrates the experimental setup. The experiment steps are shown in Figure 2 for the case of workload completion. In the case of a workload abort/hang state, the end of the experiment is provided by a watchdog timeout. As the average time necessary for the OS to execute the TPC-C client is about 70 seconds when no faultload is applied, the timeout is set to 5 minutes.

![Figure 1: Experimental Set-up](figure1.png)
*Figure 1: Experimental Set-up*

![Figure 2: Experiment Steps](figure2.png)
*Figure 2: Experiment Steps*

## 3. Comparison Results
[Insert the results and discussion here, comparing the three OSs in terms of robustness, reaction times, and restart times. Include tables, figures, and detailed analysis.]

## 4. Conclusion
[Summarize the key findings, discuss the implications, and suggest future work.]

---

**References:**
1. [Reference 1]
2. [Reference 2]
3. [Reference 3]
4. [Reference 4]
5. [Reference 5]
6. [Reference 6]
7. [Reference 7]
8. [Reference 8]
9. [Reference 9]
10. [Reference 10]
11. [Reference 11]
12. [Reference 12]
13. [Reference 13]
14. [Reference 14]
15. [Reference 15]
16. [Reference 16]
17. [Reference 17]

---

**Note:** Figures 1 and 2 should be included in the document, and the references should be properly formatted and cited.