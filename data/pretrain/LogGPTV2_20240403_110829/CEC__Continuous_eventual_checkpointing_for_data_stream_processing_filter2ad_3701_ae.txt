differs  in  that  our  checkpoints  are  integrated  within  the 
logging 
and 
incrementally evolving; and we use a different set of target 
parameters to regulate checkpointing intensity. 
infrastructure; 
continuously 
reducing 
they 
state, 
thus 
to 
are 
VII.  CONCLUSIONS 
In  this  paper  we  proposed  a  new  methodology  for 
checkpoint-rollback  recovery  for  stateful  stream  processing 
operators  that  we  call  continuous  eventual  checkpointing 
(CEC). This novel mechanism performs checkpoints of parts 
of the operator state asynchronously and independently in the 
form of control tuples produced by the operator. Individual 
window  checkpoints  are  interleaved  with  regular  output 
tuples at the output queue of each operator and persisted in 
stable  storage.  During  recovery,  CEC  processes  the  output 
queue of the operator to reconstruct a full checkpoint, which 
it then loads on the operator. The checkpoint determines the 
amount  of  tuples  that  need  to  be  replayed  by  the  upstream 
source.  Our  results  indicate  that  CEC  does  not  penalize 
operator processing when operating under minimal recovery 
guarantees. Offering stronger recovery guarantees is possible 
through tuning of the Q, U target parameters regulating the 
eventual checkpoint’s extent size and upstream queue replay 
size. The checkpoint interval and period parameters CI, CP 
can  further  tune  the  system  to  the  desired  response-time 
objective.  Overall  our  results  demonstrate  that  CEC  is  a 
simple 
low-overhead 
checkpoint-rollback  solution  for  mission-critical  stream 
processing operators. 
configurable, 
implement, 
to 
VIII.  ACKNOWLEDGMENTS 
We thankfully acknowledge the support of the European 
FP7-ICT program through the STREAM (STREP 216181) 
and SCALEWORKS (MC IEF 237677) projects. 
IX.  APPENDIX: PERSISTENCE ARCHITECTURE 
CEC’s  intermixing  of  operator  state  checkpoints  with 
regular output tuples at the operator’s output queue raises the 
need for a queue persistence mechanism in the context of a 
stream  processing  engine  (SPE).  The  upper-left  part  of 
Figure 10 shows a standard SPE structure such as found in 
the  Aurora/Borealis  [4]  system.  In  this  structure  incoming 
tuples  from  a  stream  S  are  shepherded  by  a  thread  (the 
Enqueue thread in this figure) to be enqueued into the SPE 
for processing by operator(s) that use S as one of their inputs. 
The tuples produced by the operators are placed on an output 
stream  and  dequeued  by  a  separate  thread  (the  Dequeue 
thread  in  the  figure) and  grouped  into  stream  events (SEs). 
SEs  are  serialized  objects  grouping  several  tuples  for  the 
purpose  of  efficient  communication.  The  remainder  of  the 
figure  describes 
to 
communicating them to downstream nodes. 
that  persists  SEs  prior 
the  path 
Our  detailed  description  starts  with  the  case  of  failure-
free operation:  
Failure-free  operation:  SEs  created  by  the  dequeue 
thread are first serialized. If the streams they are associated 
with are set for persistence (on a per-operator basis through a 
configuration  option)  the  SEs  enter  the  persist-event  list, 
otherwise they move directly onto the forward-event list. A 
write  operation  to  storage  is  initiated  using  a  non-blocking 
API with asynchronous notification [2]. We additionally use 
checksums to check that an I/O has been performed correctly 
in  its entirety  on  the  storage device. The  asynchronous I/O 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
154operations are handled by a state machine in an event loop. 
For  parallelism,  we  maintain  a  configurable  window  of  N 
concurrently outstanding I/Os. Once a completion of a write 
I/O  is  posted  by  the  storage  system  we  first  update  a  per-
stream  index  (shown  in  Figure  10),  and  then  move  the 
persisted  event  data  structure  to  the  forward-event  list. 
Subsequently a network send operation is initiated. The SE 
remains there until successfully sent out over the network. 
The stream index maps a timestamp into a serialized SE 
that  contains  a  tuple  with  that  timestamp.  The  mapping  is 
typically  the  file  offset  within  the  persisted  object.  In  our 
current prototype the index is implemented using an Oracle 
Berkeley DB database.  
Operation under failure: When a downstream SPE node 
fails, all streams connected to queues on that node disconnect 
and  no  outgoing  network  communication  takes  place  on 
those streams until reconnection (other streams however are 
not  affected).  SEs  produced  by  local  operators  are  still 
persisted  as  described  during 
failure-free  operation. 
However, as soon as the SPE receives an I/O completion for 
an  SE,  it  deletes  it  from  memory.  Other  SEs  belonging  to 
still-connected  streams  proceed  to  the  forward-event  list  as 
described during failure-free operation.  
tuple. The node will then start issuing asynchronous read I/O 
operations for stored SEs starting from x in a manner similar 
to 
the  write  operations  described  during  failure-free 
operation. Upon completion of a read I/O, the retrieved SE 
may  need  to  be  de-serialized  (if  a  subset  of  the  SE  is 
requested,  as  for  example,  in  the  beginning  of  a  stream 
replay request) then re-serialized (if needed) and put into the 
forward-event  list.  Similar  to  the  process  followed  during 
failure-free  operation,  the  SE  will  be  sent  to  the  connected 
downstream SPEs over the network. 
Catching up with a live stream: The persisted queue may 
be  growing  by  simultaneously  appending  tuples  to  it 
(incoming on a live stream), and reading tuples by multiple 
clients  from  different  offsets.  In  certain  cases,  the  rate  at 
which a reader consumes tuples may be higher than the rate 
at which tuples are produced (as for example when tuples are 
produced at a  source-determined rate of  a  few  Mbps  while 
the  reader  consumes  as  fast  as  I/O  resources  possibly  - 
several tens of MB/s- allow). In such cases the read pointer 
into  the  persisted  queue  may  reach  the  end  -in  essence, 
exhausting  the  object  portion  that  exists  only  in  storage. 
Read  I/O  operations  will  then  start  being  satisfied  from 
memory buffers and we can say that the reader has “caught 
up” with the live stream. In such cases, an SPE may decide 
to  interrupt  stream  persistence  if  the  reason  for  it  was  to 
avoid tuple loss due to a downstream node failure. In cases 
where persistence has been explicitly requested, the two I/O 
directions 
to  be 
simultaneously active with reads satisfied at memory speeds. 
(reads  and  writes)  can  continue 
REFERENCES 
[1] 
IBM Ushers in Era of Stream Computing. IBM Press Report, 
http://www-03.ibm.com/press/us/en/pressrelease/27508.wss.  
Linux, 
I/O 
http://lse.sourceforge.net/io/aio.html.  
Asynchronous 
for 
[2]  Kernel 
[3]  Borealis  Application  Programmer’s  Guide,  Borealis  Team, 
2008.  
[4]  D.  J.  Abadi,  D.  Carney,  U.  Cetintemel,  M.  Cherniack,  C. 
Convey,  S.  Lee,  M.  Stonebraker,  N.  Tatbul,  and  S.  Zdonik. 
Aurora:  A  New  Model  and  Architecture  for  Data  Stream 
Management. The VLDB Journal, 12(2):120–139, 2003.  
[5]  A.  Arasu,  B.  Babcock,  S.  Babu,  J.  Cieslewicz,  K.  Ito,  R. 
Motwani,  U.  Srivastava,  and  J.  Widom.  STREAM:  The 
Stanford Data Stream Management System. Springer, 2004.  
[6]  M.  Balazinska,  H.  Balakrishnan,  S.  R.  Madden,  and  M. 
Stonebraker.  Fault-Tolerance  in  the  Borealis  Distributed 
Stream  Processing  System.  ACM  Transactions  on  Database 
Systems, 33(1):1–44, 2008.  
[7]  D.  Borthakur.  The  Hadoop  Distributed  Fle  System: 
Architecture  and  Design,  The  Apache  Software  Foundation, 
2007.  
[8]  Cetintemel,  Abadi,  Ahmad,  Balakrishnan,  Balazinska, 
Cherniack,  Hwang,  Lindner,  Madden,  Maskey,  Rasin, 
Ryvkina, Stonebraker, Tatbul, Xing, and Zdonik. The aurora 
and  borealis  stream  processing  engines.  In  Data  Stream 
Management: Processing High-Speed Data Streams, 2006.  
[9]  C. Drew. Military is Awash in Data from Drones, New York 
Times, January 10, 2010.  
[10]  E.  N.  M.  Elnozahy,  L.  Alvisi,  Y.-M.  Wang,  and  D.  B. 
Johnson. A survey of rollback-recovery protocols in message-
passing systems. ACM Comput. Surv., 34(3):375–408, 2002. 
Figure 10. Stream processing engine I/O architecture. 
Recovery:  A  recovering  SPE  node  can  reconnect  to 
upstream SPEs serving the streams it was connected to prior 
to  its  failure.  The  recovering  SPE  performs  the  following 
steps: (1) reconcile the stream index in the DB with the log 
length  reported  by  the  file  system;  (2)  determines  the  last 
consistent  operator  checkpoint,  load  the  checkpoint,  and 
determine the timestamp they want to start replaying from, as 
described in Section II.B; (3) communicate the timestamp to 
the  appropriate  upstream  SPEs,  which  will  replay  tuples 
from those streams. Upon such a request from a downstream 
node  SPE,  an  upstream  node  will  look  up  the  requested 
timestamp  into  its  stream  index.  The  lookup  will  return  a 
pointer to an SE x that contains the requested timestamped 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
155[11]  J.  Gray.  Why  Do  Computers  Stop  and  What  Can  be  Done 
About it?, Tandem  Technical Report 85-7, 1985.  
[12]  D. Hilley and U. Ramachandran. Persistent temporal streams. 
10th 
on 
the 
In  Middleware 
ACM/IFIP/USENIX 
Conference 
Middleware, pages 1–20, New York, NY, USA, 2009. 
International 
Proceedings 
’09: 
of 
[13]  J.-H.  Hwang,  M.  Balazinska,  A.  Rasin,  U.  Cetintemel,  M. 
Stonebraker, and S. Zdonik. High-availability algorithms  for 
distributed  stream  processing.  In  ICDE  ’05:  Proceedings  of 
the 21st International Conference on Data Engineering, pages 
779–790, Washington, DC, USA, 2005.  
[14]  J.-H. Hwang, U. Cetintemel, and S. Zdonik. Fast and highly-
available  stream  processing  over  wide  area  networks.  In 
ICDE  ’08: Proceedings  of  the  2008  IEEE  24th  International 
Conference 
804–813, 
Washington, DC, USA, 2008.  
on  Data  Engineering, 
pages 
[15]  J.-H.  Hwang,  Y.  Zing,  U.  Cetintemel,  and  S.  Zdonik.  A 
Cooperative  Self-Configuring  High-Availability  Solution  for 
Stream  Processing.  In  ICDE  ’07:  Proceedings  of  the  2007 
IEEE  23th  International  Conference  on  Data  Engineering, 
pages 176–185, Istanbul, Turkey, 2007.  
[16]  G.  Jacques-Silva,  B.  Gedik,  H.  Andrade,  K.-L.  Wu, 
Language-level Checkpointing Support for Stream Processing 
Applications, in Proceedings of 39th IEEE/IFIP International 
Conference  on  Dependable  Systems 
(DSN’2009), Lisbon, Portugal, 2009. 
and  Networks 
[17]  Y.  Kwon,  M.  Balazinska,  and  A.  Greenberg.  Fault-tolerant 
Stream  Processing  Using  a  Distributed,  Replicated  Fle 
System. volume 1, pages 574–585. VLDB Endowment, 2008. 
[18]  M. Ligon and R. Ross. Overview of the Parallel Virtual Fle 
System. In Proceedings of Extreme Linux Workshop, 1999.  
[19]  F. B. Schneider. Implementing Fault-Tolerant Services Using 
the  State  Machine  Approach:  a  Tutorial.  ACM  Computing 
Surveys, 22(4):299–319, 1990. 
[20]  Z.  Sebepou  and  K.  Magoutis.  Scalable  Storage  Support  for 
Data  Stream  Processing.  In  Proceedings  of  26th  IEEE 
Conference  on  Mass  Storage  Systems  and  Technologies 
(MSST 2010), Lake Tahoe, Nevada, May 2010.  
[21]  M.  A.  Shah,  J.  M.  Hellerstein,  and  E.  Brewer.  Highly 
Available,  Fault-Tolerant,  Parallel  Dataﬂows.  In  SIGMOD 
’04:  Proceedings  of  the  2004  ACM  SIGMOD  International 
Conference on Management of Data, New York, NY, 2004. 
[22]  J. Zhou, C. Zhang, H. Tang, J. Wu, T. Yang, “Programming 
Support  and  Adaptive  Checkpointing    for  High-Throughput 
Data Services with Log-Based Recovery”, in Proceedings of 
40th  IEEE/IFIP  International  Conference  on  Dependable 
Systems and Networks (DSN’2010), Chicago, IL, 2010. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:28:57 UTC from IEEE Xplore.  Restrictions apply. 
156