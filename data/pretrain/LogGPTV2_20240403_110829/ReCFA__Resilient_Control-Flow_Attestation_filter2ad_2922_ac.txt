464.h264ref
470.lbm
482.sphinx3
Overall reduction
#d-call
orig
13,793
288
48,610
31
929
8,898
2,141
739
407
2,070
33
2,064
#d-call
skipped
4,168
134
21,558
5
358
3,150
764
272
233
735
18
1,075
#d-call
orig
13,799
271
48,416
31
929
8,887
2,141
739
410
2,070
33
2,064
#d-call
skipped
4,179
129
21,412
5
358
3,143
764
272
222
744
18
1,075
40.6%
40.5%
are unfolded and faithfully recorded. The results are presented in
Table 2. Torig is the execution time of the vanilla program. Tinstr is
the execution time of the instrumented program.Tgr is the time used
by the greedy compression algorithm. The overhead of our prover-
side approach ranges 0%∼360% for GCC binaries and 0%∼353% for
LLVM binaries. The average time overhead is 42.3%. For 458.sjeng,
we confront binary instrumentation failure while inserting the code
of event folding at specific program-structure-related points.
The key contribution of our approach is to mitigate the explosion
of control-flow events. To evaluate this point, we first craft another
type of instrumented binaries to only record the number of runtime
control-flow events but not fold them. The number of control-flow
events recorded by such binaries is represented by #evtotal in Table 2.
Meanwhile, #evfold is the number of control-flow events derived
by our instrumented program. #evgr is the number of control-flow
events after the greedy compression. Zs stands for the amount of
data after compressing these events with the Zstandard algorithm,
that is the payload of the attestation protocol between the prover
and the verifier. The reduction on the control-flow events ranges
17.0%∼99.9% for GCC binaries and also 17.0%∼99.9% for LLVM
binaries. The overall reduction in the control-flow events is 93.2%.
The time overhead of our approach is considerably more than
the traditional local CFI protections (usually ≤ 5%). This is because
the instrumented operations are more complicated to fold the paths
and enable the remote diagnosing on the vulnerable control-flow
paths. To address the control-flow events attestation speed, we
define E-speed as the speed of the prover generating raw runtime
control-flow events. As seen in Table 2, the peak E-speed is 50.4M/s
for GCC binaries and 47.5M/s for LLVM binaries. The average
attestation speed is 28.2M/s. Because of the different encodings of
the control-flow edges as described in Section 3, the reduction on
control-flow events is different from the reduction on the generated
code addresses. The amount of data delivered from the prover to
the verifier, as well as the data generation speed, should be of a
317ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Yumei Zhang, Xinzhi Liu, Cong Sun, Dongrui Zeng, Gang Tan, Xiao Kan, and Siqi Ma
Table 2: Effect of Control-Flow Events Folding. Overhead=(Tinstr + Tgr − Torig)/Torig × 100%. #Event reduction=(#evtotal −
#evgr)/#evtotal × 100%. Attestation speed E-speed= #evtotal/(Tinstr + Tgr). Data generation speed D-speed= Zs/(Tinstr + Tgr).
Program
400.perlbench
401.bzip2
403.gcc
429.mcf
433.milc
445.gobmk
456.hmmer
458.sjeng
462.libquantum
464.h264ref
470.lbma
482.sphinx3
Avg.b
Tgr
Tinstr
Torig
(s)
(s)
(s)
0.5
4.0
1.3
0.1
12.1
10.3
3.4
3.5
1.5
0.3
6.7
4.0
0.0
13.7
12.0
1.6
7.5
5.4
0.0
8.0
7.4
N/A N/A
5.6
0.0
0.1
0.1
1.3
39.6
27.9
0.0
2.8
2.8
2.3
0.0
2.1
overhead = 43.7%
a Small numbers of #ev to two decimal places.
b 458.sjeng not taken into account.
GCC
#evtotal
(×103)
25,311.0
205,593.1
187,747.3
174,799.9
311,950.1
60,850.8
79,139.7
383,144.6
1,018.7
2,059,738.2
0.12
34,596.9
#evfold
(×103)
15,471.4
1,804.5
99,408.6
9,767.0
15.4
50,976.7
4.7
N/A
24.6
40,118.8
0.03
842.4
#evgr
(×103)
15,444.2
1,742.9
97,690.7
7,090.7
15.4
50,534.1
4.7
N/A
24.6
40,032.9
0.03
728.4
Z s
(KB)
519.4
566.6
17,489.3
2,195.7
3.0
7,786.2
2.7
N/A
2.7
2,580.7
0.2
166.2
Tgr
Tinstr
Torig
(s)
(s)
(s)
0.1
4.7
1.6
0.1
13.2
11.4
3.5
3.3
1.5
0.3
7.0
4.4
0.0
18.0
16.6
1.6
7.4
5.2
8.0
0.0
6.8
N/A N/A
5.5
0.0
0.1
0.1
1.8
41.6
29.8
0.0
2.5
2.5
2.3
0.0
2.0
overhead = 41.0%
LLVM
#evtotal
(×103)
24,884.0
205,599.3
185,831.5
174,799.9
313,774.1
60,859.8
79,139.7
378,466.7
1,279.3
2,061,382.9
0.12
34,730.4
#evfold
(×103)
2,855.6
1,806.7
100,174.0
9,767.1
15.8
50,985.4
4.7
N/A
24.7
52,545.2
0.03
836.1
#evgr
(×103)
2,830.6
1,745.1
98,463.0
7,090.7
15.8
50,543.0
4.7
N/A
24.7
52,459.3
0.03
725.0
Z s
(KB)
469.1
566.7
17,579.9
2,241.1
3.0
7,781.5
2.7
N/A
2.6
2,976.7
0.2
167.4
E-speed = 29.2M/s
reduction = 93.2%
D-speed = 291.3KB/s
E-speed = 27.2M/s
reduction = 93.2%
D-speed = 275.2KB/s
reasonable magnitude. We define the data generation speed D-speed
as the speed of the prover generating data that are sent to the verifier.
As seen in Table 2, the peak D-speed is 2.53MB/s for GCC binaries
and 2.59MB/s for LLVM binaries. The average data generation speed
is 283.0KB/s. Both the peak and average data generation speeds are
adaptive to the wireless network transmission speed for IoT/CPS,
which avoid the drastic data accumulation at the prover and justify