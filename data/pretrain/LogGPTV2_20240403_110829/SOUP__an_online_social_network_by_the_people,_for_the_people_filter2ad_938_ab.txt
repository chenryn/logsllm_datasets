rors to store a replica of its data, in order to keep its data
available even when it is oﬄine. Data replication is neces-
sary, because SOUP does not rely on a central repository
and users are not permanently online.
Every SOUP node comprises the SOUP middleware and
the SOUP applications. The SOUP middleware resides be-
tween the network stack and SOUP applications. Its main
functions include (i) to organize SOUP nodes into a struc-
tured overlay; (ii) to handle mobile nodes; (iii) to ensure
user data privacy; (iv) to maintain and synchronize user
data; and (v) to establish communication channels with
other SOUP nodes. Multiple SOUP applications can run
concurrently on top of the middleware, each of which man-
ages the node in a diﬀerent social network for the same user.
The SOUP middleware provides a generic API to SOUP ap-
plications. For instance, once the data of the user is changed,
the middleware can notify all running applications, so they
all have the most recent version of the user’s data.
Our focus in this paper is on the SOUP middleware, and
we revisit SOUP applications in Sec. 6 and 7, where we
show the implementation of a SOUP node and a SOUP-
based distributed OSN, respectively. We present the main
middleware functions in this section, and describe in detail
how SOUP selects mirrors in Sec. 4. Recall a key contribu-
tion of SOUP is selecting nodes as mirrors while addressing
critical challenges (Sec. 1).
3.2 SOUP Overlay
Nodes in SOUP form a structured overlay, as shown in
Fig. 1. The overlay acts as a globally searchable information
directory and is based on a distributed hash table (DHT).
The DHT enables eﬃcient publish and lookup operations
in a decentralized fashion, making a centralized information
repository unnecessary. Every SOUP user can publish her
Figure 1: SOUP Overlay
directory entry at the node that is responsible for her ID
in the DHT key space (e.g., v’s entry is published at s—
Step 1 in Fig. 1), and any other node can locate the node to
retrieve the entry (e.g., u can look up v’s ID—Step 2). An
entry typically contains a user’s name, her SOUP ID, the
interfaces (i.e., IP addresses) via which she can currently be
contacted, and the SOUP IDs of all the mirrors of her data.
Here, the SOUP ID is a 64-bit SHA-256 hash over the
user’s 1024-bit public key and uniquely identiﬁes the user.
It is important to note that in contrast to some related
work [10], a user only publishes pointers to mirror nodes
(i.e., SOUP IDs) in the DHT (e.g., w publishes her mirrors
at y—Step 3), whereas the data themselves are stored among
nodes themselves. Directly storing data in the DHT would
have undesirable consequences: First, every user would have
no control over which other nodes will be her mirrors to host
her data, whereas the mirrors would have no option to reject
unwanted data. Second, it would increase the overhead of
the system; whenever a node departs—which can be often
since SOUP nodes may have a high churning rate—it has to
transfer all its DHT data to another node.
SOUP incorporates a list of publicly known bootstrapping
nodes to help new nodes join SOUP. A bootstrapping node is
simply a regular node enhanced with a function to bootstrap
others: a new node can use such a bootstrapping node as its
entry point to the DHT, thus adding itself to the DHT; for
example, in Fig. 1, node n joins the DHT via a bootstrapping
node y. It can then prepare its entry (including looking up
publish(mirrorsw)uwysuSourceDestTYPEd53275abe3da2...a34b23cd89123...SOUP_TYPESIGsign(object)NameIDInter-facesva34b23cd89123...10.11.12.13134.76.12.12Mirrorscd8324abed194…e293eee12ab32…entry(v) at sobject(uv)vnmjoinrelay431direct linkApplications1Applications4mobile nodenewnodelookup(v)2PAYLOADSOUP_DATAherself, she can directly receive updates, order them based
on the timestamps included in the received SOUP objects,
and alter her data accordingly. If the user is oﬄine (e.g., u
in Fig. 2), she then needs assistance from her mirrors (e.g., v
and w in Fig. 2). The mirrors act as a surrogate by receiving
and storing updates to the data, which can then be collected
and ordered by the returning online user later. Note that
the mirrors themselves are not eligible to modify the user’s
data. As u is oﬄine, updates for u have to be stored at
u’s mirrors, v and w. Mirror v itself is also oﬄine, so that
updates for u (not the whole replica of u) have to be further
passed on to v’s mirrors x and y. SOUP is designed such
that at least one mirror of each user is online at any time
(Sec. 4) and can retrieve these updates. Hence, v can re-
trieve any updates to u’s data upon returning online from
its own mirrors. Hereby, all mirrors always present the most
recent user data if they are online, which also enables the
data owner to synchronize diﬀerent personal devices.
3.6 SOUP Communication
To request data, a user establishes a connection with an-
other user in two steps: First, she looks up the entry of
her communication partner in the DHT (e.g., u looks up v’s
entry in Fig. 1; m would do so via its gateway z). After-
wards, she extracts the partner’s addressable interfaces from
the entry and creates a direct communication channel. This
channel can be based on any networking protocol, ranging
from standard TCP/IP to Bluetooth, if available.
Once a communication channel is established, the commu-
nication partners (u and v in Fig. 1) exchange signed SOUP
objects, which can contain arbitrary information (Step 4 in
Fig. 1). Applications running on top of SOUP can encapsu-
late payload (such as user data or friend requests) into SOUP
objects, and thereby exchange content transparently via the
middleware. The transparency allows the development of
any kind of OSN application on top of the middleware.
4. MIRROR SELECTION
A major component of SOUP is its scalable, robust and
secure approach to selecting mirrors for storing user data
among heterogeneous nodes in a DOSN with very high avail-
ability. We ﬁrst outline the challenges we face, and then
describe our decentralized mechanisms that address them.
4.1 Challenges
To fulﬁll its promise to eliminate the drawbacks of existing
DOSNs, SOUP must address a variety of key challenges, that
have not been solved in their entirety before. The primary
challenge is to achieve high data availability that is very
close to that of centralized OSNs while decentralizing the
control and storage of the data.
In leveraging resources of participating nodes, SOUP must
address the heterogeneity of OSN users. Besides a variety
of hardware conﬁgurations (e.g., mobile versus desktop de-
vices), the online time patterns of OSN nodes can diﬀer sub-
stantially from each other. Given the power-law distribution
of online times in OSNs, the majority of users are seldom
online, and sessions are usually short and bursty [18, 19, 22].
Moreover, SOUP must recognize that the users’ machines
are not servers and usually only oﬀer limited storage capac-
ities. SOUP must thus use the resources each node supplies
eﬃciently. As a node exhausts its capacity, it must be able
to decide which data to keep and which to drop. In addi-
Figure 2: SOUP’s Replica Management
its own SOUP ID in the DHT to make sure there is no
collision with another user’s SOUP ID), and publish it to
the DHT, which enables other nodes to look up the entry.
3.3 Mobile Nodes
SOUP is designed to be friendly to mobile nodes. As
these devices often experience high churn (e.g., because of
connectivity changes) and long response times (e.g., due to
limited bandwidth), they can decrease the performance and
stability of the DHT overlay. SOUP addresses this challenge
by exempting mobile nodes from the DHT. Instead, a mo-
bile node will relay its DHT publish and lookup operations
through a gateway node that is on the DHT (e.g., node m
will relay through node z in Fig. 1). As doing so frees mobile
nodes from directly executing DHT operations (e.g., shifting
entries), it also saves resources on the mobile devices.
A mobile node initially uses a bootstrapping node as its
gateway (the same node it contacted to join SOUP). How-
ever, every time it encounters another node, it checks that
node’s ability to relay DHT requests (every regular node can
set a limit to mobile connections) and switches to that node
as a gateway if possible to reduce the load on bootstrap-
ping nodes (e.g., node m has switched from node y to z in
Fig. 1). Note that since data itself is not stored in the DHT,
the relayed requests do not consume a lot of bandwidth.
3.4 Data Privacy
To ensure the conﬁdentiality of all privacy-relevant user
information, every SOUP user encrypts all her data using
Attribute Based Encryption (ABE) [21], then distributes
one replica to each mirror. The encryption routine only in-
troduces a limited overhead, even for mobile devices [8]. In
ABE, the symmetric key for encrypted content is protected
by an Access Structure, which is deﬁned by a combination
of attributes, so that only requesters holding the correct
attribute key can decrypt it. This allows a user to grant
ﬁne-grained access to her conﬁdential data, as it cannot be
accessed by other entities except those holding the corre-
sponding attribute keys. For instance, the user can limit
access to one item to users holding two speciﬁc attributes,
while three diﬀerent attributes are required for another item.
The attributes themselves can be arbitrary (e.g., such as
colleague or lives in my city).
In particular, the mirrors
themselves cannot access the data stored at their premises
without holding the correct attributes. Note that requests
to modify any data must be encapsulated in an appropri-
ately (i.e., with the owner’s asymmetric private key) signed
SOUP object, and will otherwise be discarded.
3.5 Data Synchronization
A user may receive updates from other users (e.g., mes-
sages to the user). Depending on the content, an update
might require the user to alter her data. If the user is online
wonlineuofflineData uvofflineData vReplica uData wReplica uxonlineData xyonlineData yReplica vReplica vUpdate → uUpdate → uUpdate → ution, our selection scheme should be open to altruistically
provided resources and exploit them if available.
Also, our approach should exploit the potential of social
relations within the OSN. In particular, users can provide
each other feedback whether they succeeded or failed in ob-
taining data from a participating node. If utilized properly,
such cooperation can help every user to distribute their data
within the OSN more eﬃciently.
Finally, SOUP’s selection mechanism must have several
key properties: (i) It must scale to the dimensions of OSNs
and be easily deployable in large-scale scenarios. The latter
requires a quick convergence to a stable system state; even
when many nodes join the system at the same time and each
node has little information to begin with, SOUP must pro-
vide eﬀective means to quickly reach high data availability
to each joining node. (ii) It must be robust. Even if a node’s
capabilities or social connections are weak, its data should
not be less available than data of others. Otherwise, those
nodes who need to access its data may ﬁnd it unavailable.
(iii) It must cope with every possible adverse situation. For
example, an OSN can have a high churn rate due to short-
lived sessions [18, 23]; a large fraction of nodes may depart
the system at the same time due to a network failure; worse,
malicious users in an OSN can launch all kinds of attacks.
SOUP must continue to oﬀer high performance in all such
unfavorable scenarios.
4.2 Mirror Selection Overview
SOUP has to ensure that at any given time for every OSN
node, either the node’s data is available at the node itself
(the node is online), or a copy of the data—called a replica—
is available at another node—called a mirror. The core task
for SOUP is thus mirror selection: every OSN node needs
to select the most eligible nodes as mirrors before it places
its data replicas there.
Every node employs two modes to select its mirrors, a
bootstrapping mode and a regular mode. When a node joins
the OSN and has no knowledge about it, it runs in the boot-
strapping mode, which allows it to gain a foothold in the
OSN; it obtains recommendations from each node it encoun-
ters and ranks mirror candidates based on this information
(Sec. 4.3). Once a node befriends others, it begins to learn
from them about their experience in accessing its data at its
mirrors, and transitions to the regular mode; it will now rely
on friend experience to rank mirror candidates (Sec. 4.4).
The two modes diﬀer in their way of ranking mirror candi-
dates, but follow the same routine for selecting mirrors (Sec.
4.5). Here, a node will primarily consider that the higher a
candidate is ranked, the more likely it will make the node’s
data available. Note it is more so with the regular mode
when direct user experience is used for ranking, as opposed
to looking at strangers’ recommendations in the bootstrap-
ping mode. SOUP further allows every node to dynamically
select as many mirrors as needed. As a result, no matter
whether a node itself is online a lot or not, and no matter
whether it has many friends or just a few, as long as it has
enough quality mirrors via SOUP’s algorithms, its data will
be highly available through those mirrors.
SOUP leverages social relationships in the mirror selec-
tion process primarily through experience exchanges: node
u’s experience in accessing node w’s data via w’s mirror v
helps w decide if v is a good mirror. But social relation-
ships can be useful in other contexts as well. Since friends
have more incentives and higher trust to store data for each
other, a node assigns a higher weight to friend candidates
when selecting mirrors, and protects proﬁles of friends when
dropping data from its storage.
Dropping data may be necessary if a node is chosen as a
mirror by many nodes, and its resources are exhausted. A
dropping strategy is critical, especially when an adversary
is ﬂooding the OSN and many nodes receive numerous ma-
licious requests to store data. For this task SOUP employs
a protective dropping mechanism (Sec. 4.6).
4.3 Mirror Candidate Ranking in the Boot-
strapping Mode
SOUP allows new nodes to quickly achieve high data avail-
ability. At the time a user joins the OSN, she does not pos-
sess any information about well-suited mirrors. However, as
she contacts other nodes, these nodes can suggest such mir-
rors to the new node. Here, we exploit that OSN users are