Detection
Loudspeaker
Detection
Artifact
Detection
Preventing
Synthesis
Method
Limitations
Measures human vocal tract movement using Doppler radar.
Detects presence of human breath on mic.
Detects presence of magnetic ﬁelds produced by loudspeakers.
Compares audio environment to previously enrolled speaker environment.
Requires precise static calibration during enrollment/testing.
Speaker must be  88% detection success rate, but EER for all models is >
5%. High-performing biometric systems typically have EER < 1%.
Discussion. Void’s high EER renders it less eﬀective in practice
in our setting, although the original paper reports a much lower
EER when using a custom training dataset. If the custom training
dataset were more widely available, Void could provide eﬀective
protection for scenarios like the WeChat/Alexa attacks (§4.4).
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea246Resemblyzer
Azure
Defended
Synthesized (Defended)
 100
 75
 50
 25
 100
 75
 50
 25
 0
 0.05
 0.1
 0
 0.05
 0.1
Perturbation Size
Perturbation Size
Figure 5: Performance of both defended and “synthe-
sized from defended” samples on Resembylzer and Attack-
VC. These samples were protected and synthesized using
SV2TTS. Results on AutoVC are similar.
6.3 Preventing Speech Synthesis via Attack-VC
Attack-VC [38] is designed to protect users from having their voice
copied via speech synthesis. Attack-VC adds carefully designed
perturbations to speech samples that disrupt unauthorized future
synthesis. The “embedding” perturbation generation method in [38]
assumes full knowledge of the downstream voice synthesis model
M (i.e., a white-box threat model). A defender uses the speaker em-
bedding component of M to create a size-bounded perturbation δ
that shifts the speaker embedding of their sample x towards the
embedding of a diﬀerent speaker’s sample d. Then, an adversary
A who steals the victim T ’s defended sample x + δ , cannot use
M to successfully synthesize a fake voice sample. The synthesized
samples SA should not sound like T .
Methodology. We perform a small-scale study using the VCTK
dataset and two models – AutoVC and SV2TTS (as in 4.2). We use
the same subset of 20 VCTK speakers as in §4.2. Using author-
provided code [8], we generate 19 defended samples per speaker
(using the other speakers as optimization targets). We test three
perturbation levels, ϵ = 0.01, 0.05, 0.1, following [38].
We notice that the original perturbation loss function L from [38]
does not suﬃciently constrain perturbation size. This results in
large perturbations that make defended audio samples sound in-
human. To ﬁx this, we add a term to L (new term is bold):
L = α · MSE(x + δ , d) − β · MSE(x + δ , x) + γ · kδ k,
(1)
where MSE represents the mean-squared error. This additional term
makes the perturbation less audible but does not aﬀect attack suc-
cess. Empirically, setting α , β = 1 and γ = 0.1 works best. We
multiply γ by 0.99 every 100 iterations.
Then, we use the methodology of §4.2 to synthesize speech from
the defended samples. Both the defended samples and the “synthesized-
from-defended” samples are evaluated against Azure and Resem-
blyzer (see §4 for details).
Results. As Figure 5 shows, Attack-VC does thwart voice synthe-
sis, but it also corrupts defended samples beyond reliable recog-
nition. For both models and all speaker recognition systems, the
speaker recognition accuracy for“synthesized-from-defended” sam-
ples is less than 35%, meaning that synthesis attacks after Attack-
VC are less successful. However, speaker recognition accuracy for
“defended” samples is at most 55%, meaning that they cannot be
properly matched to the true speaker. Additionally, “defended” sam-
ples still have signiﬁcant audible distortion, even with our addi-
tional constraints on perturbation size.
6.4 Combining Void and Attack-VC
Finally, we evaluate a “stronger” defense that combines Void and
Attack-VC, but ﬁnd that it only provides marginal beneﬁts. In this
experiment, we test Void’s detection eﬃcacy on speech synthe-
sized from Attack-VC protected samples, with varying perturba-
tion levels ϵ = 0.01, 0.05, 0.1. We ﬁnd that speech generated from
protected samples with ϵ < 0.1 can only be detected 2 − 4% better
(with lower EERs) than normal synthetic speech. Detailed results
are in the Appendix.
6.5 Key Takeaways
Our results demonstrate a signiﬁcant need for new and improved
defenses against synthesized speech attacks, particularly defenses
generalizable enough for real-world applications. While Void re-
liably detects fake speech played through speakers, its applicabil-
ity is limited to replay attacks. Meanwhile, existing prevention de-
fenses such as Attack-VC distort voices beyond recognition, and
might beneﬁt from using acoustic hiding techniques [68]. These de-
fenses also assume perfect (white-box) knowledge of the attacker’s
speech synthesis model, which is unrealistic in real-world settings.
Limitations & Next Steps. We only evaluate two representa-
tive and top-performing defenses (one for each category) and their
combined eﬀect. A more comprehensive investigation is required,
especially as new defenses emerge.
We also note that current defenses focus on protecting SR sys-
tems. However, our results in §5.3 indicate an equal need for human-
centric defenses against synthetic speech. One possible direction is
to make synthetic speech more “obvious” to human audiences, ei-
ther by corrupting its generation process to make the speech sound
inhuman (i.e., Attack-VC’s yet-unreached goal) or designing paral-
lel authentication methods (i.e. video feed or vocal challenges) that
help expose fake speakers.
7 CONCLUSION
Our work represents a ﬁrst step towards understanding the real-
world threat of deep learning-based speech synthesis attacks. Our
results demonstrate that synthetic speech generated using publicly
available systems can already fool both humans and today’s popu-
lar software systems, and that existing defenses fall short. As such,
our work highlights the need for new defenses, for both humans
and machines, against speech synthesis attacks, promote further
research eﬀorts for exploring subsequent challenges and opportu-
nities, while providing a solid benchmark for future research.
ACKNOWLEDGEMENTS
We thank our anonymous reviewers for their insightful feedback.
This work is supported in part by NSF grants CNS-1949650, CNS-
1923778, CNS1705042, and by the DARPA GARD program. Emily
Wenger is also supported by a GFSD fellowship. Any opinions, ﬁnd-
ings, and conclusions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reﬂect the views
of any funding agencies.
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea247REFERENCES
[1] 2015. Announcing WeChat VoicePrint. https://blog .wechat.com/2015/05/21/
voiceprint-the-new-wechat-password/.
[2] 2019.
ﬁles.
-4e68-9187-ec2e93faae55/recognize-voices-and-personalize-your-skills.
Personalize
Pro-
https://developer .amazon.com/blogs/alexa/post/1ad16e9b-4f52
Experience with Voice
Your Alexa
[3] 2020. Chase VoiceID. https://www.chase .com/personal/voice-biometrics
[4] 2020. HSBC VoiceID. https://www.us .hsbc .com/customer-service/voice/
[5] 2020.
your Google Assistant
Link Your Voice
to
device.
https://support.google .com/assistant/answer/9071681
[6] 2020. Resemblyzer. https://github .com/resemble-ai/Resemblyzer
[7] 2020.
Alexa
What
Voice
Are
Proﬁles?
https://www.amazon.com/gp/help/customer/display .html?nodeId=
GYCXKY2AB2QWZT2X.
[8] 2021.
Attack-VC
Github
Implementation.
https://github .com/cyhuang-tw/attack-vc
[9] 2021. Lyrebird AI. https://www.descript.com/lyrebird
[10] 2021.
Microsoft
Azure
Speaker