 *                 +=========+    `-_____-'  
 *  
 * Read requests are satisfied from the following sources, in order:  
 *  
 *      1) ARC  
 *      2) vdev cache of L2ARC devices  
 *      3) L2ARC devices  
 *      4) vdev cache of disks  
 *      5) disks  
 *  
 * Some L2ARC device types exhibit extremely slow write performance.  
 * To accommodate for this there are some significant differences between  
 * the L2ARC and traditional cache design:  
 *  
 * 1. There is no eviction path from the ARC to the L2ARC.  Evictions from  
 * the ARC behave as usual, freeing buffers and placing headers on ghost  
 * lists.  The ARC does not send buffers to the L2ARC during eviction as  
 * this would add inflated write latencies for all ARC memory pressure.  
 *  
 * 2. The L2ARC attempts to cache data from the ARC before it is evicted.  
 * It does this by periodically scanning buffers from the eviction-end of  
 * the MFU and MRU ARC lists, copying them to the L2ARC devices if they are  
 * not already there. It scans until a headroom of buffers is satisfied,  
 * which itself is a buffer for ARC eviction. If a compressible buffer is  
 * found during scanning and selected for writing to an L2ARC device, we  
 * temporarily boost scanning headroom during the next scan cycle to make  
 * sure we adapt to compression effects (which might significantly reduce  
 * the data volume we write to L2ARC). The thread that does this is  
 * l2arc_feed_thread(), illustrated below; example sizes are included to  
 * provide a better sense of ratio than this diagram:  
 *  
 *             head -->                        tail  
 *              +---------------------+----------+  
 *      ARC_mfu |:::::#:::::::::::::::|o#o###o###|-->.   # already on L2ARC  
 *              +---------------------+----------+   |   o L2ARC eligible  
 *      ARC_mru |:#:::::::::::::::::::|#o#ooo####|-->|   : ARC buffer  
 *              +---------------------+----------+   |  
 *                   15.9 Gbytes      ^ 32 Mbytes    |  
 *                                 headroom          |  
 *                                            l2arc_feed_thread()  
 *                                                   |  
 *                       l2arc write hand <--[oooo]--'  
 *                               |           8 Mbyte  
 *                               |          write max  
 *                               V  
 *                +==============================+  
 *      L2ARC dev |####|#|###|###|    |####| ... |  
 *                +==============================+  
 *                           32 Gbytes  
 *  
 * 3. If an ARC buffer is copied to the L2ARC but then hit instead of  
 * evicted, then the L2ARC has cached a buffer much sooner than it probably  
 * needed to, potentially wasting L2ARC device bandwidth and storage.  It is  
 * safe to say that this is an uncommon case, since buffers at the end of  
 * the ARC lists have moved there due to inactivity.  
 *  
 * 4. If the ARC evicts faster than the L2ARC can maintain a headroom,  
 * then the L2ARC simply misses copying some buffers.  This serves as a  
 * pressure valve to prevent heavy read workloads from both stalling the ARC  
 * with waits and clogging the L2ARC with writes.  This also helps prevent  
 * the potential for the L2ARC to churn if it attempts to cache content too  
 * quickly, such as during backups of the entire pool.  
 *  
 * 5. After system boot and before the ARC has filled main memory, there are  
 * no evictions from the ARC and so the tails of the ARC_mfu and ARC_mru  
 * lists can remain mostly static.  Instead of searching from tail of these  
 * lists as pictured, the l2arc_feed_thread() will search from the list heads  
 * for eligible buffers, greatly increasing its chance of finding them.  
 *  
 * The L2ARC device write speed is also boosted during this time so that  
 * the L2ARC warms up faster.  Since there have been no ARC evictions yet,  
 * there are no L2ARC reads, and no fear of degrading read performance  
 * through increased writes.  
 *  
 * 6. Writes to the L2ARC devices are grouped and sent in-sequence, so that  
 * the vdev queue can aggregate them into larger and fewer writes.  Each  
 * device is written to in a rotor fashion, sweeping writes through  
 * available space then repeating.  
 *  
 * 7. The L2ARC does not store dirty content.  It never needs to flush  
 * write buffers back to disk based storage.  
 *  
 * 8. If an ARC buffer is written (and dirtied) which also exists in the  
 * L2ARC, the now stale L2ARC buffer is immediately dropped.  
 *  
 * The performance of the L2ARC can be tweaked by a number of tunables, which  
 * may be necessary for different workloads:  
 *  
 *      l2arc_write_max         max write bytes per interval  
 *      l2arc_write_boost       extra write bytes during device warmup  
 *      l2arc_noprefetch        skip caching prefetched buffers  
 *      l2arc_nocompress        skip compressing buffers  
 *      l2arc_headroom          number of max device writes to precache  
 *      l2arc_headroom_boost    when we find compressed buffers during ARC  
 *                              scanning, we multiply headroom by this  
 *                              percentage factor for the next scan cycle,  
 *                              since more compressed buffers are likely to  
 *                              be present  
 *      l2arc_feed_secs         seconds between L2ARC writing  
 *  
 * Tunables may be removed or added as future performance improvements are  
 * integrated, and also may become zpool properties.  
 *  
 * There are three key functions that control how the L2ARC warms up:  
 *  
 *      l2arc_write_eligible()  check if a buffer is eligible to cache  
 *      l2arc_write_size()      calculate how much to write  
 *      l2arc_write_interval()  calculate sleep delay between writes  
 *  
 * These three functions determine what to write, how much, and how quickly  
 * to send writes.  
 */  
```  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")