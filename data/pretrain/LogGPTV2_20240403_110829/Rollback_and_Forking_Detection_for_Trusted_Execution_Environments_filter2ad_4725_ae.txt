c
e
e
s
s
/
/
s
s
p
p
o
o
[
[
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h
h
T
T
1000
1000
500
500
0
0
(cid:1)
(cid:1)
(cid:1)
(cid:1)
32
32
(cid:1)
(cid:1)
(cid:1)
(cid:1)
16
16
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1
1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
2
2
(cid:1)
(cid:1)
(cid:1)
(cid:1)
8
8
(cid:1)
(cid:1)
(cid:1)
(cid:1)
4
4
# of Clients
# of Clients
Fig. 6. Throughput with different numbers of clients with sync disk writes.
even higher latency of about 95ms. We emulated the TMC
on Linux by using a simple counter followed by setting the
thread to sleep for 60ms when incrementing the counter. As
Fig. 5 shows, the throughput remains constant for the emulated
TMC with an average of 12 operations per seconds, wheres
LCM with batching, on the other hand is 96x – 2063x faster.
However, by using trusted monotonic counters rollback and
forking attacks can be detected immediately but this comes
with low throughput.
F. The costs of crash tolerance
Finally, we study the performance overhead induced by
synchronous disk writes when storing the application and
protocol state that is necessary to support crash tolerance. We
performed the same experiment as in Sec. VI-D but enabled
fsync for our KVS prototypes as well as for Redis. We expect
much lower performance compared to asynchronous writes.
Fig. 6 shows the throughput with different number of clients
with synchronous storing. As expected, fsync introduces high
latency when writing to disk. In particular, we observed that
throughput of Native, SGX, LCM, and SGX with TMC remain
constant whereas Redis, SGX and LCM with batching scale.
SGX KVS achieves 0.98x of the Native KVS throughput
and LCM without batching achieves 0.69x of SGX KVS
throughput. In contrast, LCM with batching reaches 0.72x –
9.87x the throughput of the SGX KVS and 0.71x – 0.75x
the throughput of SGX KVS with batching. The experiment
shows, that expensive storing operations reduce the relative
overhead introduced by SGX but can be reduced by leveraging
batching mechanisms.
VII. RELATED WORK
With the advent of SGX, trusted computing has achieved a
new level of practicality with the aim of wide-spread deploy-
ment. Recent publications detail how legacy applications [4],
micro services [2], data intensive programming [34] but also
speciﬁc services [8] can be secured on top of an infrastructure
where only the CPU needs to be trusted. While SGX provides
special means to detect memory replay attacks against the
enclave [19], external memory remains unprotected. Accord-
ingly, additional measures are necessary to prevent rollback
and forking attacks mounted through external memory and
secondary storage. The latter is especially complicated if an
enclave is restarted (e.g. due to crashes or system mainte-
nance reboots). As a pragmatic solution, the Windows SGX
SDK [22] offers trusted counters that are linked non-volatile
memory inside the Intel management engine (ME). However,
trusted non-volatile counters as provided by a TPM are slow,
e.g. adding 35-95 ms latency for each operation depending on
the hardware platform, as different reports show [38], [26],
[29]. Thus, in essence, all hardware-based solutions that rely
on trusted counters and are consulted on every request of a
secured service suffer from performance problems [14], [26],
[32]. An additional issue of current trusted counter TPM-
based realizations is wear out if used very frequently. Strackx
and Piessens [38] speciﬁcally address this problem by clever
usage strategies, however the performance problems remain.
Recent work [29] proposes an complementary approach to
LCM where enclaves across multiple systems assist each other
in order to prevent rollback attacks. This requires multiple
enclaves to interact with each other to store and retrieve
version information from the group of enclaves wheres in
LCM it is stored at the clients.
Another line of work addresses the problem of rollback
and forking attacks without relying on trusted components.
With only a single client, the classic approach [5] for memory
checking uses a hash tree where the client stores the root.
Many systems build on this approach to protect remote storage
services (e.g., Athos [18]). In the multi-client model, Mazières
et al. [30] introduced the notion of fork-linearizability and
implemented SUNDR [27], which conﬁnes rollback attacks
to always present a view to each client that is consistent
with its past operations; thereby fork-linearizability makes it
much simpler to detect integrity and consistency violations
on remote ﬁle storage. Cachin et al. [12] improved the
efﬁciency of SUNDR and proved that there is no wait-free
emulation of fork-linearizable storage. That
is, sometimes
clients are blocked until an operation by another client has
ﬁnished. Systems such as SPORC [17], FAUST [10], and
Venus [35] avoid blocking by weakening the consistency
guarantees. Others have explored aborting operations [28],
[11] and improved the efﬁciency by reducing the computation
and communication overhead [7]. Mobius [13] uses forking
properties in the context of disconnected operations. Previous
systems have explored the guarantees of fork-linearizable for
different applications [17], [40] and generic services [11].
LCM combines the best features of these two technologies,
trusted execution environments and protocol-enforced consis-
tency. It also addresses rollback and forking attacks on TEEs as
much as possible without introducing impractical limitations
into a service.
VIII. CONCLUSION
This work has focused on a shortcoming of trusted com-
puting technology, which affects current
trusted execution
environments (TEEs), such as Intel SGX. In particular, the
trusted execution contexts or “enclaves” are stateless, lose
their memory when a crash occurs, and need support from
the host for state continuity. But since the host is also the
adversary of the TEE according to the security model,
it
is actually impossible to implement protocols that survive
167
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:47 UTC from IEEE Xplore.  Restrictions apply. 
crashes seamlessly and prevent rollback attacks at the same
time without introducing extra hardware.
As a solution we have introduced Lightweight Collective
Memory, a system for detecting rollback and forking attacks
that ensures the consistency notion of fork-linearizable and
determines when operations become stable. The LCM protocol
complements TEE technology with a lightweight mechanism
for maintaining consistency information by the clients.
ACKNOWLEDGMENTS
We thank Anil Kurmus, Cecilia Boschini, Manu Drijvers,
Kai Samelin, David Barrera and Raoul Strackx for interesting
discussions and the anonymous reviewers of DSN 2017 for
valuable comments. This work has been supported in part by
the European Commission through the Horizon 2020 Frame-
work Programme (H2020-ICT-2014-1) under grant agreements
number 644371 WITDOM and 644579 ESCUDO-CLOUD
and in part by the Swiss State Secretariat for Education, Re-
search and Innovation (SERI) under contracts number 15.0098
and 15.0087.
REFERENCES
[1] I. Anati, S. Gueron, S. Johnson, and V. Scarlata. Innovative technology
for CPU based attestation and sealing. In Int. Workshop on Hardware
and Architectural Support for Security and Privacy (HASP), 2013.
[2] S. Arnautov, B. Trach, F. Gregor, T. Knauth, A. Martin, C. Priebe,
J. Lind, D. Muthukumaran, D. O’Keeffe, M. L. Stillwell, D. Goltzsche,
D. Eyers, R. Kapitza, P. Pietzuch, and C. Fetzer. SCONE: Secure Linux
containers with Intel SGX.
In USENIX Proc. on Operating Systems
Design and Implementation (OSDI). USENIX Association, 2016.
[3] H. Attiya and J. Welch. Distributed Computing: Fundamentals, Simu-
lations and Advanced Topics. Wiley, second edition, 2004.
[4] A. Baumann, M. Peinado, and G. Hunt. Shielding applications from an
untrusted cloud with Haven. In USENIX Proc. on Operating Systems
Design and Implementation (OSDI). USENIX Association, 2014.
[5] M. Blum, W. Evans, P. Gemmell, S. Kannan, and M. Naor. Checking
the correctness of memories. Algorithmica, 1994.
[6] J. Bonneau, A. Miller, J. Clark, A. Narayanan, J. A. Kroll, and E. W.
Felten. SoK: Research perspectives and challenges for Bitcoin and
cryptocurrencies. In IEEE Proc. on Security & Privacy, 2015.
[7] M. Brandenburger, C. Cachin, and N. Kneževi´c. Don’t trust the cloud,
verify: Integrity and consistency for cloud object stores. In Int. Systems
and Storage Conference (SYSTOR). ACM, 2015.
[8] S. Brenner, C. Wulf, M. Lorenz, N. Weichbrodt, D. Goltzsche, C. Fetzer,
P. Pietzuch, and R. Kapitza. SecureKeeper: Conﬁdential ZooKeeper
using Intel SGX. In Int. Middleware Conference. ACM, 2016.
[9] E. Brickell and J. Li. Enhanced privacy id from bilinear pairing for
Int. Journal of Information
hardware authentication and attestation.
Privacy, Security and Integrity 2, 2011.
[10] C. Cachin, I. Keidar, and A. Shraer. Fail-aware untrusted storage. SIAM
Journal on Computing, 2011.
[11] C. Cachin and O. Ohrimenko. Verifying the consistency of remote
In Conference on
untrusted services with commutative operations.
Principles of Distributed Systems (OPODIS). Springer, 2014.
[12] C. Cachin, A. Shelat, and A. Shraer. Efﬁcient fork-linearizable access
In Proc. on Principles of Distributed
to untrusted shared memory.
Computing (PODC). ACM, 2007.
[13] B.-G. Chun, C. Curino, R. Sears, A. Shraer, S. Madden, and R. Ra-
makrishnan. Mobius: Uniﬁed messaging and data serving for mobile
apps. In Intl. Conference on Mobile Systems, Applications, and Services
(MobiSys). ACM, 2012.
[14] B.-G. Chun, P. Maniatis, S. Shenker, and J. Kubiatowicz. Attested
In
append-only memory: Making adversaries stick to their word.
SIGOPS Operating Systems Review. ACM, 2007.
[15] B.-G. Chun, P. Maniatis, S. Shenker, and J. Kubiatowicz. Attested
append-only memory: Making adversaries stick to their word. In Proc.
on Operating Systems Principles (SOSP). ACM, 2007.
[16] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears.
Benchmarking cloud serving systems with YCSB. In Proc. on Cloud
Computing (SoCC). ACM, 2010.
[17] A. J. Feldman, W. P. Zeller, M. J. Freedman, and E. W. Felten. SPORC:
Group collaboration using untrusted cloud resources. In Proc. Operating
Systems Design and Implementation (OSDI), 2010.
[18] M. T. Goodrich, C. Papamanthou, R. Tamassia, and N. Triandopoulos.
In Intl.
Athos: Efﬁcient authentication of outsourced ﬁle systems.
Conference on Information Security (ISC). Springer, 2008.
[19] S. Gueron. A memory encryption engine suitable for general purpose
processors. Cryptology ePrint Archive, Report 2016/204, 2016.
[20] M. P. Herlihy and J. M. Wing. Linearizability: A correctness condition
for concurrent objects. Transactions on Programming Languages and
Systems, 1990.
[21] M. Hoekstra, R. Lal, P. Pappachan, V. Phegade, and J. Del Cuvillo.
Using innovative instructions to create trustworthy software solutions.
In Int. Workshop on Hardware and Architectural Support for Security
and Privacy (HASP), 2013.
[22] Intel. Intel SGX SDK Developer Reference for Windows* OS, 2015.
https://software.intel.com/en-us/sgx-sdk/documentation.
[23] Intel.
Intel SGX SDK for Linux* OS,
2016.
//01.org/intel-software-guard-extensions/documentation/
intel-sgx-sdk-developer-reference.
https:
[24] K. R. Jayaram, D. Safford, U. Sharma, V. Naik, D. Pendarakis, and
In Int.
S. Tao. Trustworthy geographically fenced hybrid clouds.
Middleware Conference, 2014.
[25] B. Laurie. Certiﬁcate transparency. Communications of the ACM, 2014.
[26] D. Levin, J. R. Douceur, J. R. Lorch, and T. Moscibroda. TrInc: Small
In Proc. Networked
trusted hardware for large distributed systems.
Systems Design and Implementation (NSDI), 2009.
[27] J. Li, M. Krohn, D. Mazières, and D. Shasha.
Secure untrusted
In Symp. Operating Systems Design and
data repository (SUNDR).
Implementation (OSDI), 2004.
[28] M. Majuntke, D. Dobre, M. Seraﬁni, and N. Suri. Abortable fork-
linearizable storage. In Conference on Principles of Distributed Systems
(OPODIS). Springer, 2009.
[29] S. Matetic, M. Ahmed, K. Kostiainen, A. Dhar, D. Sommer, A. Gervais,
A. Juels, and S. Capkun. Rote: Rollback protection for trusted execution.
Cryptology ePrint Archive, Report 2017/048, 2017.
[30] D. Mazières and D. Shasha. Building secure ﬁle systems out of byzantine
In Proc. on Principles of Distributed Computing (PODC).
storage.
ACM, 2002.
[31] F. McKeen, I. Alexandrovich, A. Berenzon, C. V. Rozas, H. Shaﬁ,
V. Shanbhogue, and U. R. Savagaonkar.
Innovative instructions and
software model for isolated execution. In Int. Workshop on Hardware
and Architectural Support for Security and Privacy (HASP). ACM, 2013.
[32] B. Parno, J. R. Lorch, J. R. Douceur, J. Mickens, and J. M. McCune.
Memoir: Practical state continuity for protected modules. In IEEE Proc.
on Security & Privacy, 2011.
[33] S. Pearson and A. Benameur. Privacy, security and trust issues arising
from cloud computing. In Proc. Cloud Computing (CloudCom), 2010.
[34] F. Schuster, M. Costa, C. Fournet, C. Gkantsidis, M. Peinado, G. Mainar-
Ruiz, and M. Russinovich. VC3: trustworthy data analytics in the cloud
using SGX. In IEEE Proc. on Security & Privacy, 2015.
[35] A. Shraer, C. Cachin, A. Cidon,
D. Shaket. Venus: Veriﬁcation for untrusted cloud storage.
Cloud Computing Security Workshop (CCSW). ACM, 2010.
I. Keidar, Y. Michalevsky, and
In Proc.
[36] R. Strackx, B. Jacobs, and F. Piessens.
ICE: A passive, high-speed,
In Annual Computer Security Applications
state-continuity scheme.
Conference. ACM, 2014.
[37] R. Strackx and N. Lambrigts.
Idea: State-continuous transfer of state
in protected-module architectures. In Engineering Secure Software and
Systems. Springer, 2015.
[38] R. Strackx and F. Piessens. Ariadne: A minimal approach to state
continuity. In USENIX Security Symposium, 2016.
[39] D. B. Terry, M. Theimer, K. Petersen, A. J. Demers, M. Spreitzer, and
C. Hauser. Managing update conﬂicts in Bayou, a weakly connected
replicated storage system.
In ACM Symposium on Operating System
Principles (SOSP), 1995.
[40] P. Williams, R. Sion, and D. Shasha. The blind stone tablet: Outsourcing
durability to untrusted parties. In Proc. Network and Distributed Systems
Security Symposium (NDSS), 2009.
168
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:47 UTC from IEEE Xplore.  Restrictions apply.