The indexes updates on Xen.do are very slow, often taking
about 20 to 30 minutes. In our attacks, after creating or
updating a document, we query every 10 minutes to see if
the document has been indexed. We still pause 2 seconds after
each API request.
Attacks results. Using COSHARDTEST, we conﬁrm that all
the services supported by Xen.do are using the same set of
shards. We create a document d1 on a service serv1 (e.g.,
Gmail), and connect AcctA to serv1; then, we create a
document d2 on another service serv2 (e.g., Dropbox), and
connect AcctV to serv2. We then use COSHARDTEST to test
if d1 and d2 are on the same shard. If not, we disconnect
AcctA from serv1 and reconnect it again, which forces
Xen.do to assign d1 to a new shard. We did two tests:
(1) randomly chose 5 different pairs of serv1 and serv2,
and (2) ﬁx serv1 as Dropbox, and 17 different serv2. In
both tests, COSHARDTEST usually succeeded in between 4
and 10 tries. The success of COSHARDTEST indicates that
Xen.do uses the same set of shards for all services. The δ in
COSHARDTEST was set to 0.08.
In MAPSHARDS, we stop the attack if we can’t ﬁnd more
shards in 10 rounds. After 20 rounds, we found 4 shards.
Due to the restrictions of the Xen.do API and slow index
propagation, we only collected a small amount of data. We
use the ad-hoc score-based method again to put 50 documents
on the same shard. Since the index updates are slow, it took
us longer to run DFPRED (dominated by waiting). We had
the best results ﬁtting the scoring function to a curve of the
form f (x) = a− b∗ ln(x + c). We use ﬁrst the 15 data points
to approximate the scoring function and the other data points
for evaluation. The absolute errors of 40%, 49%, and 14% of
the estimations are 0, 1, and 2, respectively. This preliminary
assessment suggests that our attacks will work on Xen.do.
F. Rank-only Attacks on GitHub and Orchestrate.io.
We brieﬂy checked if our rank-only attack works cor-
rectly against GitHub and Orchestrate.io, who provide web
interfaces for performing multi-term search without returning
relevance scores. On GitHub we started with a control
experiment with an empty victim document and two attack
688
documents on the same shard (recall that creating several
co-resident documents is easy because GitHub shards based
at the repository level). Using the web interface for GitHub
search (which ranks but does not report scores), we observed
that our attack returned a true negative (i.e. the order of
the two attacker documents did not change). Next we added
the term t (in this case a long random string) to the victim
document and re-ran the attack, which swapped the order of
the attack documents in the web interface, conﬁrming that
the attack works.
Interestingly our attack failed on Orchestrate.io. This
appears to be due to their using a non-standard scoring
function for multi-term queries. We found that for multi-
term queries, Orchestrate.io computed relevance scores that
weight terms based on their order in the query. So, for
instance, “t1 t2” will give different
term weights from
“t2 t1” while TF-IDF and common variants will treat these
terms equivalently.
G. Conclusions
The results demonstrate that our score-based attacks can
work on the three targets and can be used to extract sensitive
data from other tenants’ documents. Without relevance
scores, one can still exploit the DF-side channel using rank-
only attacks. All the services we tested claim protecting
data security and data privacy as a priority. Indeed, they
make efforts to secure their physical infrastructures, systems,
and APIs. However, the DF side channels, hidden in their
underlying search engines for years, make the services
vulnerable to sensitive data leakage via side-channel attacks.
VII. COUNTERMEASURES
Perhaps the most obvious idea for a countermeasure is to
simply not return relevance scores in response to searches,
instead just providing an ordered list of documents. This
might be a hindrance to applications that make use of
the API’s relevance scoring. But more importantly, while
removing relevance scores would prevent our score-based
attacks, as shown in §VI, it does not prevent exploitation of
the DF side channel via rank-only attacks.
Previously proposed countermeasures. One can remove the
side channel by isolating each users’ documents within in-
dependent indexes. Received wisdom suggests this approach
is unsuitable for large-scale systems with many users due
to poor performance [12]. Some Elasticsearch deployments
have successfully used this architecture via careful tuning and
optimization, but it may be too expensive for, e.g., Github to
use [25]. Search functionality degradation is also a concern
here, since users with small document sets may not provide
enough data on their own to have good DF estimates.
Another approach is to retain a multi-tenant index, but
compute relevance scores in a way that matches what
would have been computed in the independent
index
case. B¨uttcher and Clarke were the ﬁrst to suggest this
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:39 UTC from IEEE Xplore.  Restrictions apply. 
countermeasure [7] and called their particular realization
of it “query integration”. It works by inserting a security
manager between the components of the system responsible
for query processing and index management. When a user
issues a query, the security manager recomputes a user-
speciﬁc view of the index (and relevance scores) that is
consistent with the user’s access rights. Subsequent work
provided different realizations of this approach [31, 39, 46],
focusing on performance improvements by partially pre-
computing views.
These approaches were suggested in the context of local
ﬁle system search. In the multi-tenant cloud services we have
primarily focused on, maintaining access control information
at every shard will incur a large storage overhead.
Another countermeasure that has been proposed in the
literature takes a statistical approach, attempting to add noise
or otherwise change the IDF distribution so that an individual
user’s private information is hidden. Zerr et al. [58] give
a countermeasure using a “relevance score transformation
function” meeting an ad-hoc statistical notion of conﬁden-
tiality. It is unclear what guarantee this actually provides.
A. New Countermeasures
All the approaches discussed above seem to have inherent
limitations which will impede their usability in large-scale
multi-tenant search indexes. We observe that they all preserve
the exact functionality of TF-IDF scoring (or a slightly
noisy variant) over a user’s view of the system. This may
be unnecessary: approximations that result in similar, but
slightly different, scores are likely acceptable in practice.
Below we outline two approaches that eliminate DF side
channels more efﬁciently. We also implement and evaluate
one approach. We plan to open-source the relevant code. In
both approaches, the searches are no longer scored strictly
according to TF-IDF. Instead,
the relevance score of a
document d is computed as a function only of the public
documents and of d. In particular, it is no longer a function of
other private documents, whether or not d is public or private.
Public-corpus DFs. The ﬁrst approach is called public-
corpus DFs. The idea is to train a DF model using public
data. In GitHub, for example, this would mean computing
a DF model on a subset of public repositories. The model
itself would be stored as an auxiliary index in Elasticsearch,
enabling nodes to efﬁciently fetch the current public DF value
for a term they have not seen. A default DF (of one) could
be used for terms which do not appear in the public data. In
settings like Xen.do and Orchestrate where there is no notion
of “public” and “private” information, this approach will not
work with data on the service. Instead, one could train on
suitable public ﬁle corpuses, should they exist.
Blind DFs. We call the second approach blind DFs. Recall
that the search system we consider stores an inverted index
that consists of per-term postings lists. At the head of each
list, the DF is stored to speed up searching. Typically the DF
value is equal to the length of the list.
To implement blind DFs, one augments each posting entry
to contain a binary attribute indicating if the document is
public (i.e. world readable) or not. We then modify the
mechanisms for adding and deleting to maintain a count that
we call the blind DF, which is now the number of public
postings in the postings list. This can be achieved, say, by
only incrementing or decrementing a posting lists’ DF when
adding or deleting a public document. Of course one may also
store the (true) DF for purposes other than relevance scoring.
This metadata must be stored for each document so document
deletions can properly decrement the DF.
To process a (public or private) search with blind DFs,
one modiﬁes the system to use the blind DFs in place of
true DFs, but otherwise leaves it unchanged. In particular,
one could compute relevance scores exactly as before, but
with a blind DF. To enforce access control one could use the
post-processing ﬁltering mechanism as is currently deployed.
Since the relevance scores are not a function of private
documents, the DFs will contain only public information.
the two approaches. Both approaches
Comparison of
increase the amount of storage space needed for the index.
For blind DFs, the amount of extra space required is on the
order of the number of documents in the index, since the
public/private attributes for each document must be stored.
For public-corpus DFs, the amount of extra space needed
is only on the order of the number of unique terms in
the index. The amount of space needed for public-corpus
DFs does not change as more documents are added to the
index, whereas the space overhead of blind DFs does increase
over time. Unlike public-corpus DFs, blind DFs can be
implemented without any preprocessing. Both approaches
will also potentially diminish the utility of DFs because
private documents will no longer inform relevance scoring,
even when a user is searching her own private documents.
Edge cases, such as making a public document private, may
be difﬁcult to handle. The main beneﬁt of both approaches
is that they are relatively simple to implement. The relevance
scoring and other portions of the system would be largely
unchanged, including the access control ﬁltering.
B. Evaluation of Public-corpus DFs
Of the two approaches described above, we believe public-
corpus DFs will likely be better for large-scale search systems
like Github’s due to its low space complexity. Here we report
on initial experiments to assess the potential practicality
of the countermeasure. All experiments were performed on
an Ubuntu 16.04 desktop, using Lucene 6.3.0 and Java 8.
The machine was equipped with a 512 GB NVMe SSD
and 16 GB of DRAM. Microbenchmarks revealed a small
latency increase of about 1% due to the countermeasure. We
therefore focus our evaluation of public-corpus DFs on two
axes: space overhead and search quality.
689
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:39 UTC from IEEE Xplore.  Restrictions apply. 
Corpus
Reuters
Wikipedia
Enron
#Docs
0.8
3.6
0.54
#Terms
1.0
14.5
0.6
Size (GB)
0.6
33.0
3.0
TD size (MB)
8.7
200.0
5.7
Figure 7: The “#Docs” and “#Terms” columns are the total
number (in millions) of documents and terms in the corpus
respectively. “Size” and “TD size” are the size and the size
of the terms dictionary of the corpus respectively. Statistics
for the Reuters dataset refer to the pre-processed LYRL2004
version [26].
Space overhead. The space overhead of public-corpus DFs
comes from storing the auxiliary index of DFs for each
term. It is straightforward to evaluate this by indexing a
document corpus using Elasticsearch and measuring the size
of the term dictionary in the resulting index. Asymptotically,
the term dictionary’s size is on the order of the number of
unique terms in the index, but we will still measure its size
empirically to account for the effect of Elasticsearch’s term
dictionary compression.
We tested with three datasets: the Reuters RCV1 cor-
pus [26], a dump of the English Wikipedia from April
2015 [15], and the Enron email dataset [24]. Each was
parsed, tokenized, stemmed, ﬁltered to remove stop words,
and indexed using Lucene. Finally, the statistics in Figure 7
were collected by inspecting the resulting index. The term
dictionary size is measured as the sum of the on-disk ﬁle sizes
of the .tim and .tip ﬁles of the Lucene index. These two
ﬁles store the compressed term dictionary and an index into
it, respectively. Note that the auxiliary data structure would
also store the DFs of each term. The size of the DFs in bytes
would be about four times the number of unique terms for
each corpus.
These results are quite promising: even for the entire
English Wikipedia, the public-corpus DFs would only require
about 250 MB of storage (the ﬁfth column of Figure 7 plus
four times the third column). This is small enough that it
could be held entirely in memory on each shard, minimizing
the number of slow disk I/O operations.
Search quality. Since the storage overhead of public-corpus
DFs is minimal, we can turn our attention to evaluating its
impact on search quality. We will use a standard methodology
from information retrieval: queries with human-labeled
relevance judgments. This measures search quality for a set of
synthetic queries on a standard corpus by using human judges
to label documents as relevant or non-relevant for each query,
then evaluating a search engine’s performance in retrieving
relevant documents. We built our experiment by modifying
Ian Soboroff’s trec-demo project [47].
Our corpus for the experiment was a pre-built Lucene
index consisting of volumes 4 and 5 from NIST’s Text
Research Collection. These two volumes contain about
530,000 total documents and 4.1 M unique keywords. The
Real DFs
0.17
0.43
0.31
0.17
0.17
0.44
0.31
0.17
Enron DFs
0.17
0.43
0.31
0.17
0.17
0.43
0.31
0.17
MAP
P@5
P@20
P@100
MAP
P@5
P@20
P@100
TF-IDF
BM25
Figure 8: Results of search quality experiment. MAP is
“mean average precision”. P@n is the precision only
considering the top n documents returned for the search,
averaged across all queries. Higher scores are better.