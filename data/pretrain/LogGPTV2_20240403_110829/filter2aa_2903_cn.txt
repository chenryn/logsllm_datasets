asm("membar.gl;");
如果汇编代码和CUDA代码之间有数据交互，那么可以使用Linux
下常用的AT&T汇编格式。
asm("template-string" : "constraint"(output) : "constraint"(input));
PTX指令可以写在模板字符串里，例如以下代码。
asm("add.s32 %0, %1, %2;" : "=r"(i) : "r"(j), "r"(k));
其中，%0、%1和%2按文本顺序引用右侧的操作数，因此，上面的
语句相当于。
add.s32 i, j, k;
操作数前的约束用来指定数据类型，常用的约束有以下几个。
"h" = .u16 reg
"r" = .u32 reg
"l" = .u64 reg
"f" = .f32 reg
"d" = .f64 reg
接下来再举几个例子，也可以重复引用某个操作数，比如，下面两
行代码是等价的。
asm("add.s32 %0, %1, %1;" : "=r"(i) : "r"(k));
add.s32 i, k, k;
如果没有输入操作数，那么可以省略最后一个冒号，比如以下代
码。
asm("mov.s32 %0, 2;" : "=r"(i));
如果没有输出操作数，就把两个冒号连起来，比如以下代码。
asm("mov.s32 r1, %0;" :: "r"(i));
9.5 CUDA
PTX指令集以汇编语言的形式定义了软硬件之间的指令接口，它的
主要用途是隔离硬件的差异性，把差异性化的硬件以统一的接口提供给
软件。然而，PTX汇编语言不适合普通软件开发者使用，主要是给编译
器使用的。
那么让开发者使用什么样的语言来编写GPU的代码呢？对于以通用
处理器形式设计的GPU来说，这是一个至关重要的问题，关系到是否能
吸引足够多的开发者，关系到上层软件是否能发挥底层硬件的能力，关
系到整个产品的成败。
不知道在Nvidia公司当初是否曾针对这个问题发生过激烈的争论，
但是可以确定地说，他们最终选择了在C语言的基础上做扩展，并给扩
展后的C语言取了一个新的名字——CUDA C，简称CUDA。CUDA是
Compute Unified Device Architecture的缩写，意思是计算统一化的设备
架构，这个名字体现了从G80开始的统一化设计思想。
9.5.1 源于Brook
第8章介绍过，CUDA源于斯坦福大学中一个名为Brook的研究性项
目。Brook项目开始于2003年，主要开发者名叫Ian Buck。
简单说，Brook是为了解决如何在GPU硬件上编写通用计算程序而
开发的一套编程语言，不过它不是全新的语言，而是在C语言的基础上
扩展而来的，称为支持流的C语言（C with streams）。
2004年11月，Ian Buck加入Nvidia公司，遇见了G80之父约翰·尼可
尔斯，让Brook技术有缘与G80融合。2007年2月15日，CUDA 0.8版本首
次公开发布，6月23日1.0版本正式发布[12]，开始了光辉的旅程。
9.5.2 算核
在并行计算领域，常常把需要大量重复的操作提炼出来，编写成一
个短小精悍的函数，然后把这个函数放到GPU或者其他并行处理器上去
运行，重复执行很多次。为了便于与CPU上的普通代码区分开，人们给
这种要在并行处理器上运行的特别程序取了个新的名字，称为算核
（kernel）。
众所周知，kernel这个词是计算机领域的常用词汇，代表操作系统
的高特权部分，是软件世界的统治者。显然，这两个名字撞车了。
追溯历史，在贝叶斯概率理论中，很早就用kernel方法来做概率密
度估计。在机器学习中，也很早就有所谓的kernel方法。在分析高维空
间时，为了降低计算量，不真的对空间中的数据坐标做计算，而是计算
特征空间中所有数据对的内积（inner product），这种方法也称为kernel
trick。后来这种方法也用于处理序列数据、图像、文本和各种向量数
据。在图像处理领域中，衍生出了很多著名的应用，比如模糊化、锐
化、边缘检测等，也用于提取图像的复杂特征，比如检查人脸和人的五
官。这些应用的基本思想都是用一个小的矩阵“扫描”目标图像，把矩阵
的每个元素按照定义的规则与目标图像的像素做运算。因为算法不同，
所以这个矩阵有不同的名字，比如，“卷积矩阵”“屏蔽矩阵”“卷积
核”等。与传统神经网络相比，目前深度学习领域流行的卷积网络技术
的一个最大变化就是引入了卷积核来提取特征。GPU领域的kernel术语
与上述方法中的kernel一词一脉相承，历史也很长。
如此看来，操作系统领域的kernel和计算领域的kernel源自不同的背
景，难以区分先后。可以说是各自独立发展，自成体系，本来是互不干
涉的。
随着深度学习和AI技术的流行，并行计算技术的应用日益广泛，才
使得这两个术语经常碰面。怎么办呢？在英文中，因为两个单词一模一
样，所以只能增加compute或者OS等修饰语来加以区分，但在实际的文
章中，很多时候都没有修饰，只能靠上下文来区分。在中文中，把代表
操作系统核心的kernel翻译为内核已经非常流行，对于代表计算的
kernel，一般都选择不翻译，直接使用英文，简单明了。但这样也不是
很好，至少那些反对中英文混杂的人会觉得不舒服。
为了避免混淆，作者建议把代表计算的kernel翻译为不同的中文。
到底翻译成什么呢？经过一番讨论，较好的方案是翻译为算核，当与函
数连用时，翻译为算核函数，简称核函数。
两个术语撞车的深层次原因是kernel这个词的内涵让人喜欢，它所
代表的两个基本特征“短小”“精悍”具有永恒的魅力。
从代码的角度来看，算核函数用于把普通代码里的循环部分提取出
来。举例来说，如果要把两个大数组A和B相加，并把结果放入数组C
中，那么普通的C程序通常如下。
for(int i = 0; i >>(A, B, C);
    ...
}
与普通的C语言程序相比，上面代码有三处不同。第一处是函数前
的__global__修饰。它是CUDA新引入的函数执行空间指示符（Function
Execution Space Specifier），一共有以下5个函数执行空间指示符。
__global__：一般用于定义GPU端代码的起始函数，表示该函数是
算核函数，将在GPU上执行，不能有返回值，在调用时总是异步调
用。另外，只有在计算能力不低于3.2的硬件上，才能从GPU端发
起调用（这一功能称为动态并行），否则，只能从CPU端发起调
用。
__device__：用来定义GPU上的普通函数，表示该函数将在GPU上
执行，可以有返回值，不能从CPU端调用。
__host__：用来定义CPU上的普通函数，可以省略。
__noinline__：指示编译器不要对该函数做内嵌（inline）处理。默
认情况下，编译器倾向于对所有标有__device__指示符的函数做内
嵌处理。
__forceinline__：指示编译器对该函数做内嵌处理。
简单来说，前两种指示符都代表该函数在GPU上执行，第三种指示
符代表该函数在CPU上执行。
9.5.3 执行配置
与普通C程序相比，上述CUDA程序的第二个明显不同是调用算核
函数的地方，即main中的如下语句。
VecAdd>>(A, B, C);
函数名之后，圆括号之前的部分是CUDA的扩展，用了三对“<>”。
这三对尖括号真是新颖，不知道是哪位同行的奇思妙想。（简单搜索了
一下Brook 0.4版本，没有搜索到，作者认为这应该是CUDA的发明。）
简单来说，尖括号中指定的是循环方式和循环次数。1代表只要一个线
程块（block），这个线程块里包含N个线程。
不要小看上面这一行代码，它非常优雅地解决了一个大问题。这个
大问题就是到底该如何在CPU的代码里调用GPU的算核函数。根据前面
的介绍，在调用算核函数时，不仅要像调用普通函数那样传递参数，还
需要传递一个信息，那就是如何做循环，简单说就是循环方式和循环次
数。
没有比较，难见差异。不妨看看如何使用OpenCL做同样的事情。
先要调用clCreateKernel创建kernel对象。
ocl.kernel = clCreateKernel(ocl.program, "VecAdd", &err);
再一次次地调用clSetKernelArg()设置参数。
err  = clSetKernelArg(ocl->kernel, 1, sizeof(cl_mem), (void *)&ocl->srcB);
然后再把算核对象放入队列中。
err = clEnqueueNDRangeKernel(
          oclobjects.queue, ocl.kernel,
          1,  0, global_size, local_size, 0, 0, 0 );
可以看到，OpenCL的做法非常麻烦，反复地调用几个API，传递几
十个参数。而在CUDA中，只有那么优雅的一行。
作者无数次端详这一行代码时，都感慨颇多。并行计算发展很多年
了，一个个并行模型不断出现，一种种语言不断扩展，唯有CUDA把串
行代码（CPU）和并行代码（GPU）之间的过渡表达得如此简洁自然。
如果追溯一下CUDA和OpenCL的出现时间，其实CUDA在前，
OpenCL在后。OpenCL升级几次，至今依然是那么冗长的调用方式。翻
看OpenCL规约，篇首一个个长长的致谢列表，用于记录那些为设计
OpenCL标准做出贡献的人。端详这个列表，不禁感慨：这么多人里面
难道没有一个深谙代码之道的程序员吗？
 老雷评点 
此一问让几多人羞愧难当。
在CUDA手册中，这三对括号有个通俗的名字，叫执行配置
（Execution Configuration），其完整形式如下。
>>
其中，Dg用来指定线程网格的维度信息，是Dimension of grid的缩
写；Db用来指定线程块（block）的维度，是Dimension of block的缩
写；线程网格和线程块都是用来方便组织线程的。其设计思想是可以按
照数据的形状来组织线程，让每个线程处理一个数据元素。Dg和Db的
类型都是dim3，是一个整型向量，有x、y、z三个分量。每个分量的默
认值为1。所以，本节开头的简单写法等价于以下代码。
dim3 blocksPerGrid(1, 1, 1);
dim3 threadsPerBlock(1, 1, N);
VecAdd>>(A, B, C);
第三部分叫Ns，用来指定为每个线程块分配的共享内存大小（以字
节为单位），其类型为sizet。这个参数是可选的，默认值为0。在Nvidia
GPU内部，配备了有限数量的高速存储器，供算核代码中由\_shared__
指示符描述的变量使用，其作用域是当前的线程块，所以当前线程块中
的各个线程可以使用这样的变量来共享信息。其效果有点像全局变量，
但是访问速度要比全局变量快。
最后一部分S用来指定与这个算核关联的CUDA流，它是可选的。
每个CUDA流代表一组可以并发执行的操作，用于提高计算的并发度和
GPU的利用率。
作者心语：上面几段文字甚花工夫，在去往杭州的高铁上写了一
半，后一半在杭州北高峰下的云松书舍完成。西湖三月，人流如织，但
书院里格外宁静。独坐听松亭，以膝为案，啾啾鸟语声与嗒嗒键盘声共
鸣。感谢金庸大侠修建了这个园林并开放给公众。
 老雷评点 
难怪字里行间有剑气。
9.5.4 内置变量
上述代码与普通C程序的第三处不同是算核函数中直接使用了一个
threadIdx变量。它是CUDA定义的内置变量（built-in variable），在写
CUDA程序时，不需要声明，可以直接使用。
截至CUDA 9.1版本，CUDA一共定义了5个内置变量，简述如下。
变量gridDim和blockDim分别代表线程网格和线程块的维度信息，
也就是启动核函数时通过三对尖括号指定的执行配置情况。
变量blockIdx和threadIdx分别表示当前线程块在线程网格里的坐标
位置和当前线程在线程块里的位置。
上面4个变量都是dim3类型，x、y、z三个分量分别对应三个维度。
变量warpSize是整型的，代表一个Warp的线程数。稍后会详细介绍
Warp。
在使用Nsight调试CUDA程序时，可以通过“局部变量”窗口来查看
内置变量的值。例如，图9-12就是调试CUDA工具集中的vecAdd示例程
序时，中断在vecAdd算核函数时的场景。
图9-12 通过“局部变量”窗口查看内置变量