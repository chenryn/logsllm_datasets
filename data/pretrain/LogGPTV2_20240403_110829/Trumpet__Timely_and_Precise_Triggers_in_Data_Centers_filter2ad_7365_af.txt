### TPM Performance and Feasibility

The Traffic Pattern Monitor (TPM) cannot complete all sweeps at a rate of 14.88 million packets per second (Mpps). However, reducing the rate to 13.5 Mpps (73 billion packets in 10G) provides more free cycles between packets, making the setting feasible. Additionally, as the number of triggers per flow increases, the gather phase must process more flow entries, thereby handling lower rates. For instance, with a time interval of 10 milliseconds and 16 triggers per flow, the packet rate must be below 14.1 Mpps. Increasing the number of active flows to 600, as shown in Figure 13b, also increases the number of flow entries to be processed in the gather phase, further reducing the feasible packet rate. We have tested different flow arrival rates and observed that the rate does not significantly affect the feasible boundary until it changes the number of active flows in an interval. Cloud operators can characterize the feasibility region for their servers and then parameterize the TPM based on their workload.

### TPM and Packet Mirroring

We verified the mirroring capability by configuring the Network Interface Card (NIC) to mirror every packet to a reserved queue for TPM, while other cores read from the other queues. The TPM could handle 7.25 Mpps without any packet loss or unfinished sweeps. This rate is almost half of the 10G maximum line rate, as each packet is fetched twice from the NIC.

### Related Work

In Section 2, we discussed why commercial packet monitoring solutions are insufficient for our purposes. Recent research has proposed various solutions for more active forms of monitoring, primarily by leveraging switch functionality. Planck [48] mirrors sampled packets at switches. NetSight [25] and EverFlow [58] use switches inside the network to send packet headers to a controller. NetSight incurs bandwidth and CPU overhead by processing packet postcards twice (at hosts/switches and the controller). EverFlow [58] attempts to address the bandwidth overhead by allowing the operator to select packets for mirroring. OpenSketch [56], FlowRadar [35], and UnivMon [37] use sketches inside switches to support many per-flow queries with low bandwidth overhead but require changes in switches. Compared to these systems, Trumpet operates at a different point in the design space: it can monitor every packet using compute resources in servers and incurs less bandwidth overhead by sending trigger satisfaction reports. These classes of solutions can coexist in a data center, with Trumpet providing quick and early warnings of impending problems and more visibility into host-level artifacts (e.g., NIC drops), while other switch-based monitoring systems can be used for deep drill-down when packet or header inspection inside the network is required.

### End-Host Monitoring Tools

Existing monitoring tools at end-hosts often focus on specific types of information, require access to VMs, or use too much of a resource. For example, SNAP [57] and HONE [53] monitor TCP-level statistics for performance diagnosis. PerfSight [55] instruments hypervisors and VM middleboxes to diagnose packet losses and resource contention among VMs on a 100-millisecond timescale. RINC [20] infers internal TCP state of VMs from packets. Pingmesh [23] uses active probes to diagnose connectivity problems. In contrast, Trumpet demonstrates that it is possible to inspect every packet at end-hosts at line speed for a wide range of use cases. Trumpet runs in the hypervisor and assumes no access to the VM's networking stack. It can potentially be extended to non-virtualized systems (e.g., containers, by interposing on the network stack), but this is left for future work. n2disk [42] allows capturing all packets to disk in 10G links using multiple cores. Trumpet, however, aggregates packet-level information on the fly and requires no storage. Packet monitoring functions can be offloaded to programmable NICs [24, 6]. However, these devices currently have limited resources, are difficult to program [34], and have limited programmability compared to CPUs. Trumpet can leverage NIC offloading if the hypervisor cannot see packets (e.g., SR-IOV and RDMA). As NICs evolve, it may be possible to save CPU by offloading some parts of Trumpet processing, such as matching, to NICs.

### Integration with Controller Systems

At the controller level, several systems can potentially integrate with Trumpet. Kinetic [32] allows controlling networks based on events generated using Trumpet. Felix [7] generates matching filters for end-host measurements from high-level user queries and routing configurations. Gigascope [12] introduces a stream database of traffic mirrored from the network and supports a variety of traffic queries on the database. It also introduces a two-stage approach of low-level and high-level queries to improve streaming efficiency [11]. Instead of collecting traffic into a central database, Trumpet distributes the monitoring and stream processing across all end-hosts and aggregates the trigger information to capture network-wide events. Trumpet also introduces a new trigger repository that captures events in 10-millisecond intervals at line speed.

### Conclusions

In this paper, we discuss the design of a new capability in data center networks: active fine-timescale and precise event monitoring. Our event description language is expressive enough to permit novel events that are particularly relevant in data centers. Our algorithms and system optimizations ensure a design that can process every packet at line-rate, is DoS-resilient, scales to future network technologies, and permits programmed tight-loop control and drill-down. In future work, Trumpet can benefit from NIC capabilities such as rule matching to detect events in links with higher packet rates and less CPU overhead. Moreover, in collaboration with monitoring at other vantage points (e.g., switches), Trumpet may achieve the most cost-effective and comprehensive solution for network telemetry and root cause analysis.

### Acknowledgements

We thank our shepherd Hitesh Ballani and anonymous SIGCOMM reviewers for their insightful feedback. This paper is partially supported by NSF grants CNS-1453662 and CNS-1423505.

### References

[1] A. Aggarwal, S. Savage, and T. Anderson. “Understanding the Performance of TCP Pacing”. In: INFOCOM. Vol. 3. 2000.
...
[58] Y. Zhu et al. “Packet-Level Telemetry in Large Datacenter Networks”. In: SIGCOMM. 2015.