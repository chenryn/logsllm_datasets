the TPM cannot ﬁnish all sweeps for 14.88Mpps. However,
reducing the rate to 13.5Mpps (73B packets in 10G), gives
more free cycles between packets for sweeping, which makes
the setting feasible. Moreover, as we increase the triggers per
ﬂow, the gather phase must process more ﬂow entries, thus
can handle lower rates. For example, if the time interval is
10 ms and there are 16 triggers per ﬂow, the packet rate must
be below 14.1Mpps. Increasing the number of active ﬂows to
600 in Figure 13b also increases the number of ﬂow entries
to be processed in the gather phase and reduces the feasible
packet rate. We have also tried with different ﬂow arrival
rates and noticed that the rate does not affect the feasible
boundary until it changes the number of active ﬂows in an
interval signiﬁcantly. Cloud operators can characterize the
feasibility region for their servers, and then parameterize
TPM based on their workload.
TPM and packet mirroring. We have veriﬁed the mirror-
ing capability by conﬁguring the NIC to mirror every packet
to a reserved queue for TPM while other cores read from the
other queues. The TPM could handle 7.25Mpps without any
packet loss or unﬁnished sweeps. This rate is almost half of
the 10G maximum line rate as each packet is fetched twice
from the NIC.
8. RELATED WORK
In Section 2, we described why commercial packet mon-
itoring solutions are insufﬁcient for our purposes. Recent
research has proposed a variety of solutions towards more
active forms of monitoring, mostly by leveraging switch func-
tionality. Planck [48] mirrors sampled packets at switches.
NetSight [25] and EverFlow [58] use switches inside the
network to send packet headers to a controller. NetSight
incurs bandwidth overhead and CPU overhead by process-
ing packet postcards twice (at hosts/switches and controller).
EverFlow [58] tries to address the bandwidth overhead by
letting the operator select packets for mirroring. OpenSketch
[56], FlowRadar [35] and UnivMon [37] use sketches inside
switches to support many per-ﬂow queries with low band-
25610244096163840200400600800# TriggersTime (ns)  141664204060050010001500# PatternsPer packet time (ns)  No matchingMatch on 88 equal trigger8 diff pattern481216051015#Triggers per flowRate (Mpps)  201051481216051015#Triggers per flowRate (Mpps)  201051width overhead, but require changes in switches. Relative
to these systems, Trumpet sits at a different point in the de-
sign space: it can monitor every packet by using compute
resources in servers, and incurs less bandwidth overhead by
sending trigger satisfaction reports. Ultimately, we see these
classes of solutions co-existing in a data center: Trumpet can
give quick and early warning of impending problems and
has more visibility into host-level artifacts (e.g., NIC drops),
and other switch-based monitoring systems can be used for
deep drill-down when packet or header inspection inside the
network is required.
At end-hosts, existing monitoring tools often focus on a
speciﬁc type of information, require access to VMs or use too
much of a resource. For example, SNAP [57] and HONE [53]
monitor TCP-level statistics for performance diagnosis. Perf-
Sight [55] instruments hypervisor and VM middleboxes to
diagnose packet losses and resource contention among VMs
in 100ms timescale. RINC [20] infers internal TCP state of
VMs from packets. Pingmesh [23] leverages active probes to
diagnose connectivity problems. In contrast, Trumpet shows
it is possible to inspect every packet at end-hosts at line speed
for a wide set of use-cases. Trumpet runs in the hypervi-
sor and assumes no access to the VM’s networking stack.
Trumpet can possibly be extended to non-virtualized systems
(e.g., containers, by interposing on the network stack), but we
have left this to future work. n2disk [42] allows capturing all
packets to disk in 10G links using multiple cores. Trumpet
aggregates packet-level information on the ﬂy and requires no
storage. Packet monitoring functions can be ofﬂoaded to pro-
grammable NICs [24, 6]. However, these devices currently
have limited resources, are hard to program [34], and have
limited programmability compared to CPUs. Trumpet can
leverage NIC ofﬂoading if the hypervisor cannot see pack-
ets (e.g., SR-IOV and RDMA). As NICs evolve, it may be
possible to save CPU by ofﬂoading some parts of Trumpet
processing, such as matching, to NICs.
At the controller, there are many systems that can poten-
tially integrate with Trumpet. Kinetic [32] allows controlling
networks based on events that can be generated using Trum-
pet. Felix [7] generates matching ﬁlters for end-host measure-
ment from high-level user queries and routing conﬁguration.
Gigascope [12] introduces a stream database of trafﬁc mir-
rored from the network, and supports a variety of trafﬁc
queries on the database. It also introduces a two stage ap-
proach of low-level queries and high-level queries to improve
streaming efﬁciency [11]. Instead of collecting trafﬁc into
a central database, Trumpet distributes the monitoring and
stream processing at all the end-hosts and aggregates the trig-
ger information to capture network-wide events. Trumpet
also introduces a new trigger repository that captures events
in 10 ms intervals at line speed.
9. CONCLUSIONS
In this paper, we discuss the design of a new capability in
data-center network: active ﬁne-timescale and precise event
monitoring. Our event description language is expressive
enough to permit novel events that are particularly relevant
in data centers. Our algorithms and systems optimizations
ensure a design that can process every packet at line-rate,
is DoS-resilient, scales to future network technologies, and
permits programmed tight-loop control and drill-down.
In future work, Trumpet can beneﬁt from NIC capabil-
ities such as rule matching to detect events in links with
higher packet rates with less CPU overhead. Moreover, in
collaboration with monitoring at other vantage points (e.g.,
switches), Trumpet may achieve the most cost-effective and
comprehensive solution for network telemetry and root cause
analysis.
Acknowledgement. We thank our shepherd Hitesh Ballani
and anonymous SIGCOMM reviewers for their insightful
feedback. This paper is partially supported by NSF grants
CNS-1453662 and CNS-1423505.
References
[1] A. Aggarwal, S. Savage, and T. Anderson. “Understanding the
Performance of TCP Pacing”. In: INFOCOM. Vol. 3. 2000.
[2] O. Alipourfard, M. Moshref, and M. Yu. “Re-evaluating Mea-
surement Algorithms in Software”. In: HotNets. 2015.
[3] M. Allman, W. M. Eddy, and S. Ostermann. “Estimating Loss
Rates with TCP”. In: SIGMETRICS Performance Evaluation
Review 31.3 (2003), pp. 12–24.
[4] S. Angel, H. Ballani, T. Karagiannis, G. O’Shea, and E.
Thereska. “End-to-End Performance Isolation Through Virtual
Datacenters”. In: OSDI. 2014.
[5] B. Atikoglu, Y. Xu, E. Frachtenberg, S. Jiang, and M. Paleczny.
“Workload Analysis of a Large-scale Key-value Store”. In:
SIGMETRICS. 2012.
[6] H. Ballani et al. “Enabling End-host Network Functions”. In:
SIGCOMM. 2015.
[7] H. Chen, N. Foster, J. Silverman, M. Whittaker, B. Zhang, and
R. Zhang. “Felix: Implementing Trafﬁc Measurement on End
Hosts Using Program Analysis”. In: SOSR. 2016.
[8] Y. Chen, R. Grifﬁth, J. Liu, R. H. Katz, and A. D. Joseph.
“Understanding TCP Incast Throughput Collapse in Datacenter
Networks”. In: WREN. 2009.
[9] M. Chowdhury and I. Stoica. “Efﬁcient Coﬂow Scheduling
Without Prior Knowledge”. In: SIGCOMM. 2015.
[10] G. Cormode, R. Keralapura,
“Communication-Efﬁcient Distributed Monitoring
Thresholded Counts”. In: SIGMOD. 2006.
and J. Ramimirtham.
of
[11] G. Cormode, T. Johnson, F. Korn, S. Muthukrishnan, O.
Spatscheck, and D. Srivastava. “Holistic UDAFs at Streaming
Speeds”. In: SIGMOD. 2004.
[12] C. Cranor, T. Johnson, O. Spataschek, and V. Shkapenyuk.
“Gigascope: a Stream Database for Network Applications”. In:
SIGMOD. 2003.
[13] A. Curtis, J. Mogul, J. Tourrilhes, P. Yalagandula, P. Sharma,
and S. Banerjee. “DevoFlow: Scaling Flow Management for
High-Performance Networks”. In: SIGCOMM. 2011.
[14] M. Dobrescu, K. Argyraki, G. Iannaccone, M. Manesh, and S.
Ratnasamy. “Controlling Parallelism in a Multicore Software
Router”. In: PRESTO. 2010.
[15] DPDK. http://dpdk.org.
[16] D. E. Eisenbud et al. “Maglev: A Fast and Reliable Software
Network Load Balancer”. In: NSDI. 2016.
[17] D. Firestone. “SmartNIC: FPGA Innovation in OCS Servers
for Microsoft Azure”. In: OCP U.S. Summit. 2016.
[18] M. Gabel, A. Schuster, and D. Keren. “Communication-
Efﬁcient Distributed Variance Monitoring and Outlier
Detection for Multivariate Time Series”. In: IPDPS. 2014.
[19] R. Gandhi, Y. C. Hu, C.-k. Koh, H. H. Liu, and M. Zhang. “Ru-
bik: Unlocking the Power of Locality and End-Point Flexibility
in Cloud Scale Load Balancing”. In: ATC. 2015.
[20] M. Ghasemi, T. Benson, and J. Rexford. RINC: Real-Time
Inference-based Network Diagnosis in the Cloud. Tech. rep.
Technical Report TR-975-14, Princeton University, 2015.
[38] N. McKeown et al. “OpenFlow: Enabling Innovation in Cam-
pus Networks”. In: SIGCOMM Computer Communication Re-
view 38.2 (2008).
[39] R. Miao, R. Potharaju, M. Yu, and N. Jain. “The Dark Menace:
Characterizing Network-based Attacks in the Cloud”. In: IMC.
2015.
[40] M. Moshref, M. Yu, R. Govindan, and A. Vahdat. “DREAM:
Dynamic Resource Allocation for Software-deﬁned Measure-
ment”. In: SIGCOMM. 2014.
[21] M. Ghobadi and Y. Ganjali. “TCP Pacing in Data Center Net-
works”. In: High-Performance Interconnects (HOTI). 2013.
[41] M. Moshref, M. Yu, A. Sharma, and R. Govindan. “Scalable
Rule Management for Data Centers”. In: NSDI. 2013.
[22] Google Compute Engine Incident 15041. https://status.cloud.
google.com/incident/compute/15041. 2015.
[23] C. Guo et al. “Pingmesh: A Large-Scale System for Data Cen-
ter Network Latency Measurement and Analysis”. In: SIG-
COMM. 2015.
[24] S. Han, K. Jang, A. Panda, S. Palkar, D. Han, and S. Ratnasamy.
SoftNIC: A Software NIC to Augment Hardware. Tech. rep.
UCB/EECS-2015-155. http://www.eecs.berkeley.edu/Pubs/
TechRpts/2015/EECS- 2015- 155.html. EECS Department,
University of California, Berkeley, 2015.
[25] N. Handigol, B. Heller, V. Jeyakumar, D. Mazières, and N.
McKeown. “I Know What Your Packet Did Last Hop: Using
Packet Histories to Troubleshoot Networks”. In: NSDI. 2014.
[26] Y.-J. Hong and M. Thottethodi. “Understanding and Mitigating
the Impact of Load Imbalance in the Memory Caching Tier”.
In: SOCC. 2013.
[27] L. Hu, K. Schwan, A. Gulati, J. Zhang, and C. Wang. “Net-
cohort: Detecting and Managing VM Ensembles in Virtualized
Data Centers”. In: ICAC. 2012.
[28] Q. Huang, H. Gudmundsdottir, Y. Vigfusson, D. A. Freedman,
K. Birman, and R. van Renesse. “Characterizing Load Imbal-
ance in Real-World Networked Caches”. In: HotNets. 2014.
[29] “IEEE Standard for a Precision Clock Synchronization Pro-
tocol for Networked Measurement and Control Systems”. In:
IEEE Std 1588-2008 (Revision of IEEE Std 1588-2002) (2008),
pp. 1–269.
[30] Intel Data Direct I/O Technology. http : / / www. intel . com /
content/www/us/en/io/data-direct-i-o-technology.html.
[31] R. Kapoor, A. C. Snoeren, G. M. Voelker, and G. Porter. “Bul-
let Trains: A Study of Nic Burst Behavior at Microsecond
Timescales”. In: CoNEXT. 2013.
[32] H. Kim, J. Reich, A. Gupta, M. Shahbaz, N. Feamster, and
R. Clark. “Kinetic: Veriﬁable Dynamic Network Control”. In:
NSDI. 2015.
[33] A. Kumar et al. “BwE: Flexible, Hierarchical Bandwidth Al-
location for WAN Distributed Computing”. In: SIGCOMM.
2015.
[34] B. Li et al. “ClickNP: Highly Flexible and High-performance
Network Processing with Reconﬁgurable Hardware”. In: SIG-
COMM. 2016.
[35] Y. Li, R. Miao, C. Kim, and M. Yu. “FlowRadar: A Better
NetFlow for Data Centers”. In: NSDI. 2016.
[36] H. Lim, D. Han, D. G. Andersen, and M. Kaminsky. “MICA:
A Holistic Approach to Fast In-memory Key-value Storage”.
In: NSDI. 2014.
[37] Z. Liu, A. Manousis, G. Vorsanger, V. Sekar, and V. Braverman.
“One Sketch to Rule Them All: Rethinking Network Flow
Monitoring with UnivMon”. In: SIGCOMM. 2016.
[42] n2disk: A Multi-Gigabit Network Trafﬁc Recorder with In-
dexing Capabilities. http://www.ntop.org/products/trafﬁc-
recording-replay/n2disk/.
[43] N. Parlante. Linked List Basics. http://cslibrary.stanford.edu/
103/LinkedListBasics.pdf. 2001.
[44] P. Patel et al. “Ananta: Cloud Scale Load Balancing”. In: SIG-
COMM. 2013.
[45] J. Perry, A. Ousterhout, H. Balakrishnan, D. Shah, and H.
Fugal. “Fastpass: A Centralized "Zero-queue" Datacenter Net-
work”. In: SIGCOMM. 2014.
[46] B. Pfaff et al. “The Design and Implementation of Open
vSwitch”. In: NSDI. 2015.
[47] R. Potharaju and N. Jain. “Demystifying the Dark Side of the
Middle: A Field Study of Middlebox Failures in Datacenters”.
In: IMC. 2013.
[48] J. Rasley et al. “Planck: Millisecond-scale Monitoring and
Control for Commodity Networks”. In: SIGCOMM. 2014.
[49] A. Roy, H. Zeng, J. Bagga, G. M. Porter, and A. C. Snoeren.
“Inside the Social Network’s (Datacenter) Network”. In: SIG-
COMM. 2015.
[50] I. Sharfman, A. Schuster, and D. Keren. “A Geometric Ap-
proach to Monitoring Threshold Functions over Distributed
Data Streams”. In: Transaction on Database Systems 32.4 (Nov.
2007).
[51] A. Singh et al. “Jupiter Rising: A Decade of Clos Topologies
and Centralized Control in Google’s Datacenter Network”. In:
SIGCOMM. 2015.
[52] V. Srinivasan, S. Suri, and G. Varghese. “Packet Classiﬁcation
Using Tuple Space Search”. In: SIGCOMM. 1999.
[53] P. Sun, M. Yu, M. J. Freedman, J. Rexford, and D. Walker.
“HONE: Joint Host-Network Trafﬁc Management in Software-
Deﬁned Networks”. In: Journal of Network and Systems Man-
agement 23.2 (2015), pp. 374–399.
[54] M. Wang, B. Li, and Z. Li. “sFlow: Towards Resource-efﬁcient
and Agile Service Federation in Service Overlay Networks”. In:
International Conference on Distributed Computing Systems.
2004.
[55] W. Wu, K. He, and A. Akella. “PerfSight: Performance Diag-
nosis for Software Dataplanes”. In: IMC. 2015.
[56] M. Yu, L. Jose, and R. Miao. “Software Deﬁned Trafﬁc Mea-
surement with OpenSketch”. In: NSDI. 2013.
[57] M. Yu et al. “Proﬁling Network Performance for Multi-tier
Data Center Applications”. In: NSDI. 2011.
[58] Y. Zhu et al. “Packet-Level Telemetry in Large Datacenter
Networks”. In: SIGCOMM. 2015.