5. Develop a tool to trace process virtual memory growth via both brk(2) and mmap(2).
6. Develop a tool to print the size of expansions via brk(2). It may use syscall tracepoints,
kprobes, or libc USIT probes, as desired.
7. Develop a tool to show the time spent in page compaction. You can use the
compaction:mm_compaction_begin and compaction:mm_compaction_end tracepoints.
Print the time per event and summarize it as a histogram.
8. Develop a tool to show time spent in shrink slab, broken down by slab name (or shrinker
function name).
9. Use memleak(8) to find long-term survivors on some sample software in a test
environment. Also estimate the performance overhead with and without memleak(8)
running.
10. (Advanced, unsolved) Develop a tool to investigate swap thrashing: Show the time spent by
pages on the swap device as a histogram. This is likely to involve measuring the time from
swap-out to swap-in.
7.6
6Summary
This chapter summarizes how virtual and physical memory is used by processes and covers
memory analysis using traditional tools, which focus on showing memory volumes by usage
types. This chapter also shows how to use BPF tools to measure rates and time durations for
memory activity by the OOM killer, user-level allocations, memory maps, page faults, vmscan,
direct reclaim, and swap-ins.
---
## Page 314
Chapter
FileSystems
Analysis of file systems has historically focused on disk I/O and its performance, but file systems
are often a more relevant target for beginning your analysis. It is the file system that applications
usually interact with directly, and file systems can use caching, read-ahead, buffering, and
asynchronous I/O to avoid exposing disk I/O latency to the application.
Since there are few traditional tools for file system analysis, it is an area where BPF tracing
can really help. File system tracing can measure the full time an application was waiting on
I/O, including disk I/O, locks, or other CPU work. It can show the process responsible, and
the files operated upon: useful context that can be much harder to fetch from down at the
disk level.
Learning Objectives:
 Understand file system components: VFS, caches, and write-back
Understand targets for file system analysis with BPF
 Learn a strategy for successful analysis of file system performance
 Characterize file system workloads by file, operation type, and by process
 Measure latency distributions for fle system operations, and identify bi-modal distribu
tions and issues of latency outliers
 Measure the latency of file system write-back events
 Analyze page cache and read ahead performance
 Observe directory and inode cache behavior
 Use bpftrace one-liners to explore fle system usage in custom ways
This chapter begins with the necessary background for file system analysis, summarizing the I/O
stack and caching. I explore the questions that BPF can answer, and provide an overallsrategy
to follow. I then focus on tools, starting with traditional file system tools and then BPF tools,
including a list of BPF one-liners. This chapter ends with optional exercises.
---
## Page 315
278
Chapter S File Systems
8.1
Background
This section covers file system fundamentals BPF capabilities, and a suggested strategy for file
system analysis.
8.1.1 File Systems Fundamentals
1/O Stack
A generic I/O stack is shown in Figure 8-1, showing the path of I/O from the application to disk
devices.
Application
Runtime
POSIX
System Libraries
System Calls
file system I/O
VFS
logical I/O
raw
I/O
File System
Disk Device Subsystem
physical VO
DiskDevices
Figure 8-1 Generic I/0 stack
Some terminology has been included in the diagram: ogical I/O describes requests to the file
system. If these requests must be served from the storage devices, they become physical I/O. Not
all I/O will; many logical read requests may be returned from the file system cache, and never
become physical I/O. Raw I/O is included on the diagram, though it is rarely used nowadays: it is a
way for applications to use disk devices with no file system.
---
## Page 316
8.1 Background
279
File systems are accessed via a virtual file system (VFS), a generic kernel interface allowing
multiple different fle systems to be supported using the same calls, and new file systems tobe
easily added. It provides operations for read, write, open, close, etc., which are mapped by file
systems to their own internal functions.
After the file system, a volume manager may also be in use to manage the storage devices. There
is also a block I/O subsystem for managing I/O to devices, including a queue, merge capabilities,
and more. These are covered in Chapter 9.
File System Caches
Linux uses multiple caches to improve the performance of storage I/O via the file system, as
shown in Figure S-2.
Directory
Page
VFS
Cache
Cache
inode
Cache
Page
Scanner
ext4
xfs
Block Device Interface
Disks
Figure 8-2 Linux FS caches
These caches are:
Page cache: This contains virtual memory pages including the contents of files and I/O
buffers (what was once a separate *buffer cache"), and improves the performance of file and
directory I/O.
• Inode cache: Inodes (index nodes) are data structures used by file systems to describe their
stored objects. VFS has its own generic version of an inode, and Linux keeps a cache of
these because they are frequently read for permission checks and other metadata.
• Directory cache: Called the dcache, this caches mappings from directory entry names to
VFS inodes, improving the performance of path name lookups.
---
## Page 317
280
) Chapter 8 File Systems
The page cache grows to be the largest of all these, because it not only caches the contents of
files, but also includes *dirty” pages that have been modified but not yet written to disk. Various
situations can trigger a write of these dirty pages, includling a set interval (e-.g., 30 seconds), an
explicit sync() call, and the page-out deamon (kswapd) explained in Chapter 7.
Read-Ahead
A file system feature called read ahead or prefetch, involves detecting a sequential read workload,
predicting the next pages that will be accessed, and loading them into the page cache. This
pre-warming improves read performance only for sequential access workloads, not random access
workloads. Linux also supports an explicit readahead() syscall.
Write-Back
Linux supports file system writes in write-back mode, where buffers are dirtied in memory and
flushed to dlisk sometime later by kernel worker threads, so as not to block applications dlirectly
on slow disk I/O,
Further Reading
This was a brief summary intended to arm you with essential knowledge before you use the tools.
File systems are covered in much more depth in Chapter 8 of Systems Perforrmunce [Gregg 13b]
8.1.2 BPF Capabilities
Traditional performance tools have focused on disk I/O performance, not file system performance.
BPF tools can provide this missing observability, showing operations, latencies, and internal
functions of each file system.
Questions that BPF can help answer include:
•W'hat are the file system requests? Counts by type?
*What are the read sizes to the file system?
• How much write I/O was synchronous?
• What is the file workload access pattern: random or sequential?
 W'hat files are accessed? By what process or code path? Bytes, I/O counts?
• W'hat file system errors occurred? What type, and for whom?
W'hat is the source of file system latency? Is it disks, the code path, locks?
• W'hat is the distribution of file system latency?
•What is the ratio of Dcache and Icache hits vs misses?
•W'hat is the page cache hit ratio for reads?
■ How effective is prefetch/read-ahead? Should this be tuned?
As shown in the previous figures, you can trace the I/O involved to find the answers to many of
these questions.
---
## Page 318
8.1 Background 281
Event Sources
I/O types are listed in Table 8-1 with the event sources that can instrument them.
Table 8-1 l/0 Types and Event Sources
 1/0 Type
Event Source
0/1 ueuou pue uoeogddy
uprobes
System call I/0
syscalls tracepoints
File system I/0
ext4 (..) tracepoints, kprobes
Cache hits (reads), write-back (writes)
kprobes
Cache misses (reads), write-through (writes)
kprobes
Page cache write-back
writeback tracepoints
0/ ysp eos/u
block tracepoints, kprobes
Raw I/O
kprobes
This provides visibility from the application to devices. File system I/O may be visible from file
system tracepoints, depending on the file system. For example, ext4 provides over one hunxdred
tracepoints.
Overhead
Logical I/O, especially reads and writes to the file system cache, can be very frequent: over 100k
events per second. Use caution when tracing these, since the performance overhead at this rate
may begin to become noticeable. Also be careful with VFS tracing: VFS is also used by many
network I/O paths, so this ads overhead to packets as well, which may also have a high rate.
Physical disk I/O on most servers is typically so low (less than 1000 IOPS), that tracing it incurs
negligible overhead. Some storage and database servers may be exceptions: check the I/O rate
beforehand with iostat(1).
8.1.3 Strategy
If you are new to file system performance analysis, here is a suggested overall strategy that you can
follow. The next sections explain these tools in more detail.
1. Identify the mounted file systems: see df(1) and mount(8).
2. Check the capacity of mounted file systems: in the past, there have been performance
issues when certain file systems approach 100% full, due to the use of different
free-block-finding algorithms (e.g, FFS, ZFS°).
event rate may be much lower than the wire-packet rate; see the netsize(8) tool in Chapter 10.
2 The zpool 80% rule, although from memory I wss able to move that to 99% when building storage products. Also see
Pool performance can degrade when a pool is very ful* from the ZFS Recommended Storage Pool Practioes guide [83],
---
## Page 319
282
Chapter 8 File Systems
3. Instead of using unfamiliar BPF tools to understand an unknown production workload, first
use those on a known workload. On an idle system, create a known file system workload,
e.g., using the fio(1) tool.
4. Run opensnoop(8) to see which files are being opened.
5. Run filelife(8) to check for issues of short-lived files.
6. Look for unusually slow file system I/O, and examine process and file details (e.g-, using
ext4slower(8), btrfsslower(8), zfsslower(8),etc., or as a catch-all with possibly higher
overhead, fileslower(8)). It may reveal a workload that can be eliminated, or quantify a
problem to aid file system tuning.
7. Examine the distribution of latency for your file systems (e.g., using ext4dist(8), btrfsdist(8),
zfsdist(8), etc.) This may reveal bi-modal distributions or latency outliers that are causing
performance problems, that can be isolated and investigated more with other tools.
8. Examine the page cache hit ratio over time (e.g., using cachestat(8)): does any other
workload perturb the hit ratio, or does any tuning improve it?
9. Use vfsstat(8) to compare logical I/O rates to physical I/O rates from iostat(1): ideally, there
is a much higher rate of logical than physical I/O, indicating that caching is effective.
10. Browse and execute the BPF tools listed in the BPF tools section of this book.
8.2
2TraditionalTools
Because analysis has historically focused on the disks, there are few traditional tools for observing
file systems. This section summarizes file system analysis using df(1), mount(1), strace(1), perf(1), 
and fatrace(1).
Note that file system performance analysis has often been the domain of micro-benchmark
tools, rather than observability tools. A recommended example of a file system micro-benchmark
tool is fio(1).
8.2.1df
df(1) shows file system disk usage:
q- 3P s
F11esysten
S12e
Used Aval1 Usel Mounted on
udev
93G
93G
0t/dev
taspfa
19G
4 ,0H
19G
1% /xun
/dev/nvme0n1
9 , 7G
5.1G
4, 6G
53%/
sgdsn
93G
D
93G
0%/dev/shn
tmpfs
5. 0M
5,0M
0%/run/Lock
sgdsn
93G
93G
0%/ays/fs/cgroup
/dev/nvme1n1
120G
18G
103G
15%/rnt
tmpfs
19G
0
19G
0%/xun/usex/60000
---
## Page 320
8.2  Traditional Tools
283
The output includes some virtual physical systems, mounted using the tmpfs device, which are
used for containing system state.
Check disk-based file systems for their percent utilization (°Use%° column). For example, in the
above output this is */* and */mnt°, at 53% and 15% full. Once a file system exceeds about 90%
full, it may begin to suffer performance issues as available free blocks become fewer and more
scattered, turning sequential write workloads into random write workloads. Or it may not: this is
oo xnb e qsom isnl s,a uogeuaadtu wasis agg aug to puapuadap seas
8.2.2mount
The mount(1) command makes file systems accessible, and can also list their type and mount flags:
$ nount
(eutaeex^cexeou*aepou^prnsou*nx] sgs.s ed ss/ uo sgss
proc on /proc type proc (rx,nosuid, nodev,noexec,relatime,gid=60243, hidepid=2)
udev on /dev type devtnpfs
(cv, nosuid, relat.ine size=96902412k, nr_inodes=24225603,mode=755)
sqdnepedk? sad/nsp/uo sadsep
(cv, nosuid, noexec, relatine, qid=5,node=620, ptmxmode=000)
tspEs on /zun type tnpEa (zv nosuld, noexec, relatine, size=19382532k,node=755)
/dev/nvme0nl on / type ext4 (rv,noatine,nobarrier, data=ordered)
[.--]
This output shows that the /* (root) file system is ext4, mounted with options including
sduesatup ssaooe Suspsosa sdps eg Supun aoueuoad e ,augeotu,
8.2.3strace
drexa s u suoerado wasis ag jo mata e sapord um °seo wasss aoen ueo (taoenss
the ttt option is used to print walltimestamps with microsecond resolution as the first field,
and T to print the time spent in syscalls as the last field. All times are printed in seconds
S strace cksum -tttz /usx/bin/cksun
[...]
1548892204.,789115 openat (AT_FDCMD, */usx/bin/cksun*, 0_RDONLY) = 3 
1548892204,789202 fadvise64 13, 0, 0, P0SIX_FADV_SBgUENTIAL) = 0 
1548892204,789308 fstat (3, (st_node=S_IFREG|0755, st_s12e=35000, ---)) = 0 
1548892204 , 78 9397 read (3, *\177LF.2\1^13.00\.0\.0`.0.0.00\.0\3.0>
0′,1,0′,0\,0\,0,33\,0\,0\,0,0\,0\,0*-·, 65536) =35000 
 0 =(2499 4*, 401091 925684*02268990
1548892204.790011 1seek (3, 0, SEEK_CUR) = 35000 
154889204, 790087 cl0se (3)
= 0 
[.--]
strace(1) formats the arguments to syscalls in a human-readable way.
---
## Page 321
284
Chapter 8 File Systems
All this information should be extremely valuable for performance analysis, but there’s a catch:
strace(I1) has historically been implemented to use ptrace(2), which operates by inserting
breakpoints at the start and end of syscalls. This can massively slow down target software, by as
much as over 100 fold, making strace(1) dangerous for use in production environments. It is more
useful as a troubleshooting tool, where such slowdowns can be tolerated.
There have been multiple projects to develop an strace(1) replacement using buffered tracing. One
is for perf(1), covered next.
8.2.4perf
The Linux perf(1) multi-tool can trace file system tracepoints, use kprobes to inspect VFS and file
system internals, and has a trace subcommand as a more effcient version of strace(1). For example
 pezf trace cksum /usr/bin/cksum
[...]
0oge,tstxg :eueuettg fax2 =pzp)veuedo s060z/umsao 1 (su ct0*0 1 ce9 *0
= 3
0.,698 ( 0.002 ms) : cksun/20905 fadvise64 Ifd: 3, advice: 2)
= 0
0.702 [ 0.002 ms) : cksun/20905 fstat (fd: 3, statbuf: Ox7fff45169610)
= 0 
0,713 ( 0.059 ms) : cksun/20905 read(fd: 3, buf: 0x7fff45169790, count: 65536)
000SC =
0.774 ( o.002 ms) : cksun/20905 read(fd: 3, buf: 0x7fff45172048, count: 28672)
 = 0
0,875 ( 0,002 ms) : cksum/20905 1seek (fd: 3, xhence1 CUR)
= 35000
0.879 ( 0.002 ms) : cksun/20905 c1ose (fd: 3)
= 0
[... 
The output of perf trace has been improving in each Linux version (the above demonstrates
Linux 5.0). Arnaldo Carvalho de Melo has been improving this further, using kernel header
parsing and BPF to improve the output [84]; future versions should, for example, show the
filename string for the openat() call, instead of just the filename pointer address.
The more commonly used perf(1) subcommands, stat and record, can be used with file system
tracepoints, when such tracepoints for a file system are available For example, counting ext4 calls
system-wide via ext4 tracepoints:
+ perf stat -e *ext4:*' -a
°C
Performance counter stats for *systen vide*:
ext4:ext4_other_inode_update_tine
ext4:ext4_free_inode
ext4:ext4_request_inode
ext4:ext4_allocate_inode
ext4:ext4_ev1ct_1node
1
ext4:ext4_drop_inode
1.63
ext4:exta_nark_inode_dlrty
ext4:ext4_begin_ordered_truncate
ext4:ext4_urite_begl.n
---
## Page 322
8.2  Traditional Tools
285
260
ext4:ext4_ds_vrite_begin
0
ext4:exta_wz1te_end
0
ext4:ext4_journalled_vrite_end
260
ext4:ext4_da_vz1te_end
0
ext4:ext4_vritepages
U
ext4:ext4_da_vzlte_pages
[...]
The ext4 file system provides around one hunxdred tracepoints for visibility into its requests and
internals. Each of these has format strings for associated information, for example (do not run
this command):