control domains. Figure 9 shows an example subset of such
domains.
thaisqz.sites.uol.com.br
jkishii.sites.uol.com.br
sjhsjh333.egloos.com
ivoryzwei.egloos.com
dat007.xtgem.com
vk144.narod.ru
jhooli10.freehostia.com
7171.freehostia.com
cr0s.interfree.it
cr0k.interfree.it
id11870.luxup.ru
id23166.luxup.ru
...
Fig. 9: Example set of domains that were counted as false positives. The
effective 2LDs are highlighted in bold.
We now provide a breakdown of the false positives gener-
ated by Segugio during the three different cross-day and cross-
network tests reported in Section IV-A and in Figure 6 (a),
(b), and (c). Table III summarizes the results. For example,
experiment (a) produced 724 distinct false positive FQDs,
using a detection threshold set to produce at most 0.05% FPs
and > 90% TPs. Many of these FP domains shared the same
e2LD. In fact, we had only 401 distinct e2LDs. Of these,
the top 10 e2LDs that contributed the most FQDs under their
domain name caused 32% of all FPs.
TABLE III: Analysis of Segugio’s FPs
(a) ISP1
cross-day
(b) ISP2
cross-day
(c) ISP1-ISP2
cross-network
Test Experiment
Absolute number of false positives for overall 0.05% FPs and > 90% TPs
Fully qualiﬁed domains (FQDs)
Effective second-level domains (e2LDs)
Contribution of top 10 e2LDs
Feature Contributions
> 90% infected machines
Past abused IPs
Active for ≤ 3 days
Evidence of Malware Communications (sandbox traces)
Domains queried by malware
73%
86%
26%
71%
85%
20%
724
401
230 (32%)
308 (38%)
807
410
21%
23%
247 (31%)
786
451
55%
80%
27%
19%
Table III also shows that 73% of all false positive domains
were queried by a group of machines of which more than 90%
were known to be infected. Also, 86% of the FP domains
resolved to a previously abused IP addresses, and 26% of
410410
them were active for only less than three days. Finally, using a
separate large database of malware network traces obtained by
executing malware samples in a sandbox, we found that 21%
of the domains that we counted towards the false positives had
been contacted by known malware samples.
To summarize, our experiments show that Segugio’s false
positive rate is low (e.g., ≤ 0.05% FPs at a TP rate ≥ 90%)
and FPs may also be somewhat overestimated. In general,
Segugio yields much lower FPs than previously proposed
systems for detecting malicious domains (see Section V for
a comparison to Notos [3]). Even so, we acknowledge that
some false positives are essentially inevitable for statistical
detection systems such as Segugio. Therefore, care should
be taken (e.g., via an additional vetting process) before the
discovered domains are deployed to block malware-control
communications.
E. Experiments with Public Blacklists
To show that Segugio’s results are not critically dependent on
the speciﬁc commercial malware C&C blacklist we used as
our ground truth, we also performed a number of experiments
using public blacklist information.
Cross-day Tests. We repeated the cross-day experiment
on machine-domain graphs labeled using exclusively known
malware-control domains collected from public blacklists.
More speciﬁcally, we collected domains labeled as mal-
ware C&C (we excluded other types of non-C&C mali-
cious domains) from the following sources: spyeyetracker.
abuse.ch,
and
malwaredomainlist.com. Overall, our public C&C domain
blacklist consisted of 4,125 distinct domain names. We then
used this blacklist to label the malware nodes in the machine-
domain graph, and then performed all other steps to conduct
cross-day experiments using the same procedure described in
Section IV-A (the only change was the blacklist).
zeustracker.abuse.ch, malwaredomains.com,
Figure 10 reports the results on trafﬁc from ISP2 (results
for the other ISP network and different days of trafﬁc are very
similar). Segugio was able to achieve over 94% true positives
at a false positive rate of 0.1%.
1.0
0.8
0.6
0.4
0.2
P
T
AUC=0.988, PAUC=0.962
TP% | FP%
--------------------
78.39 - 0.01
85.34 - 0.03
90.68 - 0.05
92.59 - 0.07
94.43 - 0.10
97.13 - 0.30
97.61 - 0.50
97.96 - 0.70
98.06 - 1.00
BaggingJ48
RandomForest
LibLinear
0.0
0.000
0.002
0.004
0.006
0.008
0.010
Fig. 10: Cross-day results using only public blacklists
FP
Cross-Blacklist Tests. To further demonstrate Segugio’s abil-
ity to discover new malware-control domains, we conducted
another experiment by using our commercial C&C blacklist
(described in Section III) for training purposes, and then testing
Segugio to see if it would be able to detect new malware-
control domains that appeared in the public blacklists but
were not in our commercial blacklist (and therefore were not
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:35 UTC from IEEE Xplore.  Restrictions apply. 
used during training). By inspecting a day of trafﬁc from
ISP2, we observed 260 malware-control domains that matched
our public blacklist. However, of these 260 domains, 207
domains already existed in our commercial blacklist as well.
Therefore, we used only the remaining 53 new domains that
matched the public blacklist (but not the commercial blacklist)
to compute Segugio’s true positives. We found that Segugio
could achieve the following trade-offs between true and false
positives: (TPs=57%, FPs=0.1%), (TPs=74%, FPs=0.5%), and
(TPs=77%, FPs=0.9%). While the TP rate looks somewhat
lower than what obtained in other tests (though still fairly good,
considering the low FP rates), we believe this is mainly due to
the limited test set size (only 53 domains) and noise. In fact,
we manually found that the public blacklists we used contained
a number of domains labeled as C&C that were highly
likely benign (e.g., recsports.uga.edu and www.hdblog.it), and
others that were likely not related to malware-control activities
(though possibly used for different malicious activities), which
would not be labeled as malware by Segugio.
F. Early Detection of Malware-Control Domains
We also performed experiments to measure how early
Segugio can detect malware-control domains, compared to
malware domain blacklists. To this end, we selected four
consecutive days of data from either of the two ISP networks (8
days of trafﬁc, overall). For each day, we trained Segugio and
set the detection threshold to obtain ≤ 0.1% false positives.
We then tested the classiﬁer on all domains that on that day
were still labeled as unknown. Finally, we checked if the
new malware-control domains we detected appeared in our
blacklists in the following 35 days. During the four days of
monitoring, we found 38 domains that later appeared in the
blacklist. A large fraction of these newly discovered domains
were added to the blacklist many days after they were detected
by Segugio, as shown in Figure 11.
s
n
i
a
m
o
d
d
e
t
c
e
t
e
d
f
o
e
g
a
t
n
e
c
r
e
p
25
20
15
10
5
0
0
5
10
15
20
25
30
35
Fig. 11: Early detection results: histogram of the time gap between Segugio’s
discovery of new malware-control domains and the time when they ﬁrst
appeared on the blacklist.
detection gap (days)
G. Segugio’s Performance (Efﬁciency)
Segugio is able to efﬁciently learn the behavior-based
classiﬁer from an entire day of ISP-level DNS trafﬁc, and
can classify all (yet unknown) domains seen in a network
in a matter of a few minutes. To show this, we computed
the average training and test time for Segugio across the 8
days of trafﬁc used to perform the early detection experiments
discussed in Section IV-F. In average the learning phase
took about 60 minutes, for building the graph, annotating
and labeling the nodes, pruning the graph, and training the
411411
behavior-based classiﬁer. The feature measurement and testing
of all unknown domains required only about 3 minutes.
V. COMPARISON WITH NOTOS
In this section, we aim to compare our Segugio system
to Notos [3], a recently proposed domain reputation system.
As mentioned in Section I, Notos’ goal is somewhat different
from ours, because domain reputation systems aim to detect
malicious domains in general, which include phishing and
spam domains, for example. On the other hand, we focus on
a behavior-based approach for accurately detecting malware-
control domains, namely “malware-only” domains through
which attackers provide control functionalities to already in-
fected machines. Nonetheless, Notos could be also used to
detect malware-control domains, and therefore here we aim to
compare the two systems.
Experimental setup. We have obtained access to a version of
Notos built by the original authors of that system. The version
of Notos available to us was trained using a very large blacklist
of malicious domains, and a whitelist consisting of the top
100K most popular domains according to Alexa. We were able
to verify that the blacklist they used to train Notos was a proper
superset of the blacklist of malware-control domains we used
to train Segugio. In addition, we made sure to train Segugio
using only the top 100K Alexa domains, as done by Notos, thus
allowing for a balanced comparison between the two systems.
To compute the false positives, we used the whitelist
detailed in Section III (domains that were consistently very
popular for at least one year), from which we removed the
top 100K Alexa domains used during the training of Notos
and Segugio. As mentioned earlier, we acknowledge that our
whitelist may contain some small amount of noise. Later in this
section we discuss how we further aggressively reduce such
noise to obtain a more precise estimate of the false positives.
The version of Notos to which we were given access was
trained on October 8, 2013, which we refer to as ttrain.
Therefore, we trained Segugio on trafﬁc from the very same
day ttrain, and labeled malware domains using our blacklist
updated until
that same day. In other words, both Notos
and Segugio were trained using only ground truth gathered
before ttrain. Then, we tested both Notos and Segugio on
the two ISP networks, using one entire day of trafﬁc from
November 1, 2013, which we refer to as ttest. To compute the
true positives, we considered as ground truth only those new
conﬁrmed malware-control domains that were added to our
blacklist between days (ttrain + 1) and ttest. Overall, during
that period we had 44 and 36 new blacklisted malware-control
domains that appeared (i.e., were queried) in ISP1 and ISP2,
respectively.
Results. Figure 12 shows the detection results for the two
systems. In particular, Figure 12a shows that the detection
threshold on Notos’s output score needs to be increased
signiﬁcantly, before the new malware-control domains (i.e., the
ones blacklisted after ttrain) are detected. Unfortunately, this
causes a fairly high false positive rate (16.23% and 21.11%,
respectively, for ISP1 and ISP2). In addition, only less than
56% of the newly blacklisted domains are detected in the best
case (ISP1 in Figure 12a). Notice that the version of Notos
given to us employed a “reject option” whereby the system
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:49:35 UTC from IEEE Xplore.  Restrictions apply. 
may avoid classifying an input domain, if not enough historic
evidence about its reputation could be collected. This explains
why Notos is not able to detect all malware-control domains
even at the highest FP rates.
According to Figure 12b (where FPs are in [0, 0.03]),
Segugio was able to detect respectively 90.9% and 75% of new
malware-control domains with less than 0.7% of false positives
in ISP1 and ISP2. This shows that Segugio outperforms
Notos, even considering that we had 24 days of gap between
the training and test phases.
1.0
0.8
0.6
0.4
0.2
P
T
0.0
0.0
1.0
0.8
0.6
0.4
0.2
P
T
ISP1
ISP2
0.2
0.4
0.6
0.8
1.0
FP
(a) Notos
0.0
0.000
0.005
0.010
0.015
0.020
0.025
0.030
ISP1
ISP2
Fig. 12: Comparison between Notos and Segugio (notice that the range of
FPs for Notos is [0, 1.0], while for Segugio FPs are in [0, 0.03])
FP
(b) Segugio
Braking down the FPs. To better understand why Notos
produced a high false positive rate, we investigated the possible
reasons why many of our whitelisted domains were assigned
a low reputation (see Table IV). After adjusting the detection