includes any biometric, template, or key other than
those associated with the user in question. It could
USENIX Association  
17th USENIX Security Symposium 
63
also include any other information about the envi-
ronment that might leak information about the bio-
metric, or results of using the key.
For the remainder of the paper if a component of a
BKG is associated with a speciﬁc user, then we subscript
the information with the user’s unique identiﬁer. So, for
example, Bu, is u’s biometric and A ¯u is auxiliary infor-
mation derived from all users u′ = u.
3.1 Evaluation Recommendations
At a high level, the evaluation of a BKG requires design-
ers to show that two properties hold: correctness and se-
curity. Intuitively, a scheme that achieves correctness is
one that is usable for a high percentage of the population.
That is, the biometric of choice can be reliably extracted
to within some threshold of tolerance, and when com-
bined with the template the correct key is output with
high probability. As correctness is well understood, and
is always presented when discussing the feasibility of a
proposed BKG, we do not address it further.
In the context of biometric key generation, security is
not as easily deﬁned as correctness. Loosely speaking,
a secure BKG outputs a key that “looks random” to any
adversary that cannot guess the biometric. In addition,
the templates and keys derived by the BKG should not
leak any information about the biometric that was used
to create them. We enumerate a set of three security
requirements for biometric key generators, and examine
the components that should be analyzed mathematically
(i.e., the template and key) and empirically (i.e., the bio-
metric and auxiliary information). While the necessity of
the ﬁrst two requirements has been understood to some
degree, we will highlight and analyze how previous eval-
uations of these requirements are lacking. Additionally,
we discuss a requirement that is often overlooked in the
practical literature, but one which we believe is necessary
for a secure and practical BKG.
We consider a BKG secure if it meets the following
three requirements for each enrollable user in a popula-
tion:
• Key Randomness (REQ-KR): The keys output by a
BKG appear random to any adversary who has ac-
cess to auxiliary information and the template used
to derive the key. For instance, we might require
that the key be statistically or computationally in-
distinguishable from random.
• Weak Biometric Privacy (REQ-WBP): An adver-
sary learns no useful information about a biometric
given auxiliary information and the template used
to derive the key. For instance, no computationally
bounded adversary should be able to compute any
function of the biometric.
• Strong Biometric Privacy (REQ-SBP): An adver-
sary learns no useful information about a biomet-
ric given auxiliary information, the template used to
derive the key, and the key itself. For instance, no
computationally bounded adversary should be able
to compute any function of the biometric.
The necessity of REQ-KR and REQ-WBP is well
known, and indeed many proposals make some sort
of effort to argue security along these lines (see, e.g.,
[29, 11]). However, many different approaches are used
to make these arguments. Some take a cryptographi-
cally formal approach, whereas others provide an empiri-
cal evaluation aimed at demonstrating that the biometrics
and the generated keys have high entropy. Unfortunately,
the level of rigor can vary between works, and differ-
ences in the ways REQ-KR and REQ-WBP are typically
argued make it difﬁcult to compare approaches. Also,
it is not always clear that the empirical assumptions re-
quired by the cryptographic algorithms of the BKG can
be met in practice.
Even more problematic is that many approaches for
demonstrating biometric security merely provide some
sort of measure of entropy of a biometric (or key) based
on variation across a population. For example, one com-
mon approach is to compute biometric features for each
user in a population, and compute the entropy over the
output of these features. However, such analyses are
generally lacking on two counts. For one, if the corre-
lation between features is not accounted for, the reported
entropy of the scheme being evaluated could be much
higher than what an adversary must overcome in prac-
tice. Second, such techniques fail to compute entropy
as a function of the biometric templates, which we ar-
gue should be assumed to be publicly available. Con-
sequently, such calculations would declare a BKG “se-
cure” even if, say, the template leaked information about
the derived key. For example, suppose that a BKG uses
only one feature and simply quantizes the feature space,
outputting as a key the region of the feature space that
contains the majority of the measurements of a speciﬁc
user’s feature. The quantization is likely to vary between
users, and so the partitioning information would need to
be stored in each user’s template. Possession of the tem-
plate thus reduces the set of possible keys, as it deﬁnes
how the feature space is partitioned.
As far as we know, the notion of Strong Biometric
Privacy (REQ-SBP) has only been considered recently,
and only in a theoretical setting [12]. Even the origi-
nal deﬁnitions of fuzzy extractors [11, Deﬁnition 3] do
not explicitly address this requirement. Unfortunately,
REQ-SBP has also largely been ignored by the designers
of practical systems. Perhaps this oversight is due to lack
of perceived practical motivation—it is not immediately
64 
17th USENIX Security Symposium 
USENIX Association
clear that a key could be used to reveal a user’s biomet-
ric. Indeed, to our knowledge, there have been few, if
any, concrete attacks that have used keys and templates
to infer a user’s biometric. We observe, however, that it
is precisely practical situations that motivate such a re-
quirement; keys output by a BKG could be revealed for
any number of reasons in real systems (e.g., side-channel
attacks against encryption keys, or the veriﬁcation of a
MAC). If a key can be used to derive the biometric that
was used to generate the key, then key recovery poses
a severe privacy concern. Moreover, key compromise
would then preclude a user from using this biometric in
a BKG ever again, as the adversary would be able to
recreate any key the user makes thereafter. Therefore,
in Section 7 we provide speciﬁc practical motivation for
this requirement by describing an attack against a well-
accepted BKG. The attack combines the key and a tem-
plate to infer a user’s biometric.
In what follows, we provide practical motivation for
the importance of each of our three requirements by an-
alyzing three published BKGs. It is not our goal to fault
speciﬁc constructions, but instead to critique evaluation
techniques that have become standard practice in the
ﬁeld. We chose to analyze these speciﬁc BKGs because
each was argued to be secure using “standard” tech-
niques. However, we show that since these techniques do
not address important requirements, each of these con-
structions exhibit signiﬁcant weaknesses despite security
arguments to the contrary.
4 Biometrics and “Entropy”
Before continuing further, we note that analyzing the se-
curity of a biometric key generator is a challenging task.
A comprehensive approach to biometric security should
consider sources of auxiliary information, as well as the
impact of human forgers. Though it may seem imprac-
tical to consider the latter as a potential threat to a stan-
dard key generator, skilled humans can be used to gener-
ate initial forgeries that an algorithmic approach can then
leverage to undermine the security of the BKG.
To this point, research has accepted this “adversar-
ial multiplicity” without examining the consequences in
great detail. Many works (e.g., [33, 29, 17, 15, 40, 16])
report both False Accept Rates (i.e., how often a human
can forge a biometric) and an estimate of key entropy
(i.e., the supposed difﬁculty an algorithm must over-
come in order to guess a key) without speciﬁcally iden-
tifying the intended adversary.
In this work, we focus
on algorithmic adversaries given their importance in of-
ﬂine guessing attacks, and because we have already ad-
dressed the importance of considering human-aided forg-
eries [4, 5]. While our previous work did not address
biometric key generators speciﬁcally, those lessons ap-
ply equally to this case.
The security of biometric key generators in the face of
algorithmic adversaries has been argued in several dif-
ferent ways, and each approach has advantages and dis-
advantages. Theoretical approaches (e.g., [11, 6]) be-
gin by assuming that the biometrics have high adversar-
ial min-entropy (i.e., conditioned on all the auxiliary in-
formation available to an adversary, the entropy of the
biometric is still high) and then proceed to distill this
entropy into a key that is statistically close to uniform.
However, in practice, it is not always clear how to esti-
mate the uncertainty of a biometric. In more practical
settings, guessing entropy [25] has been used to mea-
sure the strength of keys (e.g., [29, 27, 10]), as it is
easily computed from empirical data. Unfortunately, as
we demonstrate shortly, guessing entropy is a summary
statistic and can thus yield misleading results when com-
puted over skewed distributions. Yet another common
approach (e.g., [31, 7, 16, 41, 17]), which has lead to
somewhat misleading views on security, is to argue key
strength by computing the Shannon entropy of the key
distribution over a population. More precisely, if we con-
sider a BKG that assigns the key Ku to a user u in a pop-
ulation P , then it is considered “secure” if the entropy of
the distribution P(K) = |{u ∈ P : Ku = K}|/|P | is
high. We note, however, that the entropy of the previous
distribution measures only key uniqueness and says noth-
ing about how difﬁcult it is for an adversary to guess the
key. In fact, it is not difﬁcult to design BKGs that output
keys with maximum entropy in the previous sense, but
whose keys are easy for an adversary to guess; setting
Ku = u is a trivial example.
To address these issues, we present a new measure that
is easy to compute empirically and that estimates the dif-
ﬁculty an adversary will have in guessing the output of
a distribution given some related auxiliary distribution.
It can be used to empirically estimate the entropy of a
biometric for any adversary that assumes the biomet-
ric is distributed similarly to the auxiliary distribution.
Our proposition, Guessing Distance, involves determin-
ing the number of guesses that an adversary must make
to identify a biometric or key, and how the number of
guesses are reduced in light of various forms of auxiliary
information.
4.1 Guessing Distance
We assume that a speciﬁc user u induces a distribution
U over a ﬁnite, n-element set Ω. We also assume that
an adversary has access to population statistics that also
induce a distribution, P, over Ω. P could be computed
from the distributions of other users u′
= u. We seek to
quantify how useful P is at predicting U. The speciﬁca-
USENIX Association  
17th USENIX Security Symposium 
65
tion of Ω varies depending on the BKG being analyzed;
Ω could be a set of biometrics, a set of possible feature
outputs, or a set of keys. It is up to system designers to
use the speciﬁcation of Ω that would most likely be used
by an adversary. For instance, if the output of features
are easier to guess than a biometric, then Ω should be de-
ﬁned as the set of possible feature outputs. Although at
this point we keep the deﬁnition of P and U abstract, it is
important when assessing the security of a construction
to take as much auxiliary information as possible into ac-
count when estimating P. We return in Section 5 with an
example of such an analysis.
We desire a measure that estimates the number of
guesses that an adversary will make to ﬁnd the high-
probability elements of U, when guessing by enumerat-
ing Ω starting with the most likely elements as prescribed
by P. That is, our measure need not precisely capture the
distance between U and P (as might, say, L1-distance
or Relative Entropy), but rather must capture simply P’s
ability to predict the most likely elements as described by
U 1. Given a user’s distribution U, and two (potentially
different) population distributions P1 and P2, we would
like the distance between U and P1 and U and P2 to be
the same if and only if P1 and P2 prescribe the same
guessing strategy for a random variable distributed ac-
cording to U. For example, consider the distributions U,
P1 and P2, and the element ω ∈ Ω such that P1(ω) = .9,
P2(ω) = .8, and U(ω) = 1. Here, an adversary with ac-
cess to P1 would require the same number of guesses to
ﬁnd ω as an adversary with access to P2 (one). Thus, we
would like the distance between U and P1 and between
U and P2 to be the same.
Guessing Distance. Let ω∗ = argmaxω∈Ω
U(ω). Let
LP = (ω1, . . . , ωn) be the elements of Ω ordered such
that P(ωi) ≥ P(ωi+1) for all i ∈ [1, n − 1]. Deﬁne
t− and t+ to be the smallest index and largest index i
such that |P(ωi) − P(ω∗)| ≤ δ. The Guessing Distance
between U and P with tolerance δ is deﬁned as:
GDδ(U, P) = log
t− + t+
2
Guessing Distance measures the number of guesses that
an adversary who assumes that U ≈ P makes before
guessing the most likely element as prescribed by U (that
is, ω∗)2. We take the average over t− and t+ as it may
be the case that several elements may have similar prob-
ability masses under P. In such a situation, the ordering
of LP may be ambiguous, so we report an average mea-
sure across all equivalent orderings. As U and P will
typically be empirical estimates, we use a tolerance δ to
offset small measurement errors when grouping elements
of similar probability masses. The subscript δ is ignored
if δ = 0.
Discussion.
Intuitively, one can see that this deﬁnition
makes sense by considering the following three cases:
(1) P is a good indicator of U (i.e., ω∗ = ω1); (2)
P is uniform; and (3) P is a poor indicator of U (i.e.,
ω∗ = ωn).
In case (1) the adversary clearly beneﬁts
from using P to guess ω∗, and this relation is captured
as GD(U, P) = log 1 = 0. In case (2), the adversary
learns no information about U from P and thus would
be expected to search half of Ω before guessing the cor-
2 . Finally, in case
rect value; indeed GD(U, P) = log 1+n
(3), a search algorithm based on P would need to enu-
merate all of Ω before ﬁnding ω∗, and this is reﬂected by
GD(U, P) = log n+n
2 = log |Ω|.
An important characteristic of GD is that it compares
two probability distributions. This allows for a more
ﬁne-tuned evaluation as one can compute GD for each
user in the population. To see the overall strength of a
proposed approach, one might report a CDF of the GD’s
for each user, or report the minimum over all GD’s in the
population.
Guessing Distance is superﬁcially similar to Guessing
Entropy [25], which is commonly used to compute the