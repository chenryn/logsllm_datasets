not ﬁnd any attack, then this provides good indication that the program model is secure
even though this is not provably true in the real operating system.
3 Overview
We provide here an overview of model-based anomaly detection, including the attacker
threats addressed, context-sensitive program models, and the purpose of attack discovery.
3.1 Threat Model
Our system automatically constructs undetected attack sequences possible within a par-
ticular threat model. This threat model is simple and strong:
Let Σ be the set of system calls invoking kernel operations. If program P
is under attacker control, then P can generate any sequence of system calls
A ∈ Σ
∗
.
Attackers may subvert a vulnerable program’s execution at any execution point, includ-
ing the point of process initialization. Attackers can then arbitrarily alter the code and
data of the program, and can even replace the program’s entire memory image with
an image of their choosing. Alternatively, the attacker could replace the disk image
of a program with, for example, a trojan before the OS loads the program for exe-
cution. The attacker can generate any sequence of system calls and system call argu-
ments, and the operating system will execute the calls with the privilege of the original
program.
This threat model matches real-world attacks. In remote execution environments,
programs execute on remote, untrusted machines but send a sequence of remote system
calls back to a trusted machine for execution. An attacker controlling the remote host
can arbitrarily alter or replace the remote program. The attacker’s program image can
then send malicious system calls back to the trusted machine for execution [11].
Common network-based attacks against server programs have a more restrictive
threat model. Attackers can subvert execution only at points of particular program vul-
nerabilities and face greater restrictions in the attack code that they can then execute. As
a result, if our system proves that a program model detects an attack in the strong threat
model, it will also detect the attack in a more restrictive model. However, successful
attacks discovered by our system are speciﬁc to the strong threat model. Although the
program model would fail to detect the attack sequence even in the restricted threat
model, a restricted attacker may be unable to cause the program to execute that attack.
Our system currently does not make this determination and will report all attacks dis-
covered in the strong threat model.
Consider the example in Fig. 2. This is a vulnerable program that reads command
characters and ﬁlenames from user input. This input may come from the network if
Automated Discovery of Mimicry Attacks
47
void main (void) {
char input[32];
gets(input);
if (input[0] == ‘x’) {
setreuid(42, -1);
syslog(1, "Execing file");
execve(input+2, 0, 0);
} else if (input[0] == ‘e’) {
struct stat buf;
syslog(1, "Echoing file");
stat(input+2, &buf);
int fd = open(input+2, O RDONLY);
void *filedata =
mmap(0, buf.st size, PROT READ, MAP PRIVATE, fd, 0);
write(1, filedata, buf.st size);
}
}
Fig. 2. Code example. We show system calls in boldface and library calls in italics. The unsafe
call to gets allows an attacker to execute arbitrary code.
the program is launched by a network services wrapper daemon such as xinetd. The
command-code and argument input resembles the usage of programs such as ftp servers
or http servers. Suppose that the program is executed with stored but inactive privilege:
its real and effective user IDs are a low-privilege user, but the saved user ID is root. If
the input contains the command character ‘x’, then the program drops all of its saved
privilege and executes a ﬁlename given in the input. If the input contains the command
character ‘e’, then the program echoes the contents of a speciﬁed ﬁle to its output, which
may be a network stream.
In our threat model, an attacker can arbitrarily alter the execution of this program.
Perhaps the attacker exploits the vulnerable gets call; perhaps they use an attack vector
that we have not considered. The attacker can cause the program to execute any system
call, including system calls not contained in the original program code. The role of
host-based intrusion detection is to detect any such subverted program execution.
3.2 Program Model
Readers familiar with pushdown automaton (PDA) models may elect to bypass this
section, as it presents background material and standard notation previously used for
PDA-based program models.
Model-based anomaly detection restricts allowed execution to a precomputed model
of allowed behavior. A program model M is a language acceptor of system call se-
quences and is an abstract representation of the program’s expected execution behavior.
If Σ denotes the alphabet of system calls, then L(M) ⊆ Σ
denotes the language
accepted by M . A system call sequence in L(M) is valid; sequences outside L(M)
indicate anomalous program execution. In this paper, we implement a program model
as a non-deterministic pushdown automaton (PDA).
∗
48
J.T. Gifﬁn, S. Jha, and B.P. Miller
Deﬁnition 1. A pushdown automaton (PDA) is a tuple M = (cid:5)S, Σ, Γ, δ, s0, Z0, F(cid:6),
where
(cid:2)
∗
, γ
(cid:2) ∈ S, γ
(cid:2) ∈ Γ ∪ };
is an initial stack conﬁguration;
(cid:2)(cid:6)| s ∈ S, γ ∈ Γ ∪ , σ ∈ Σ ∪ , s
– S is a set of states;
– Σis a set of alphabet symbols;
– Γ is a set of stack symbols;
– δ ⊆ {(cid:5)s, γ(cid:6) σ→ (cid:5)s
– s0 ∈ S in an initial state;
– Z0 ∈ Γ
– F ⊆ S is a set of ﬁnal states.
A PDA model has close ties to program execution. A state corresponds to a program
point in the program’s code. The initial state corresponds to the program’s entry point.
The ﬁnal states correspond to program termination points, which generally follow an
exit system call. The alphabet symbols are the system calls generated by a program as
it executes. The stack symbols are return addresses specifying to where a function call
returns. The initial stack Z0 is empty, as a program begins execution with no return
addresses on its call stack.
The transition relation δ describes valid control ﬂows within a program. Our PDA
model has three types of transitions:
– System calls: (cid:5)s, (cid:6) σ→ (cid:5)s
(cid:2)
(cid:2)
– Function calls: (cid:5)s, (cid:6) σ→ (cid:5)s
system call σ when transitioning from state s to state s
call return addresses remains unchanged.
address γ onto the call stack when transitioning from state s to s
sponds to a function call-site in the program and s
of the call’s destination.
, (cid:6) for σ (cid:9)=  indicates that the program can generate
. The PDA stack of function
, γ(cid:6) for γ (cid:9)=  indicates that the program pushes return
. Here, s corre-
corresponds to the entry point
, (cid:6) for γ (cid:9)=  indicates that the program returns
from a function call and pops return address γ from the call stack. This transition
can be followed only when γ is the top symbol of the PDA stack. The state s
is
corresponds to a program point containing a function return instruction and s
the program point to which control is actually returned.
– Function returns: (cid:5)s, γ(cid:6) σ→ (cid:5)s
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Many program model designs proposed in academic literature are not presented as
pushdown automata. However, the generality of a PDA allows us to characterize those
models as PDA suitable for analysis using the techniques presented later in this paper.
The context-free languages recognized by PDA completely contain the class of regular
languages. All program models of which we are are aware accept either regular or
context-free languages, and hence can always be characterized by a PDA. This includes:
– window-based models, such as the Stide model [8] (Fig. 3a) or the digraph model
[23] (Fig. 3b);
– non-deterministic ﬁnite automata (NFA) [23, 19, 14, 12] (Fig. 3c);
– bounded-stack PDAs [11];
– deterministic PDAs, such as the VPStatic model [6];
– stack-deterministic PDAs, such as the Dyck model [6]; and
– non-deterministic PDAs [23] (Fig. 3d).
Automated Discovery of Mimicry Attacks
49
read
write
setreuid
write
execve
stat
open
mmap
write
read
write
stat
setreuid
write
execve
open
mmap
write
read
write
stat
setreuid
write
execve
open
mmap
write
push A
read
pop A
setreuid
push B
push C
write
pop B
pop C
execve
stat
open
mmap
write
(a) Stide Model
(b) Digraph Model
(c) NFA Model
(d) PDA Model
Fig. 3. Four different program models for the code of Fig. 2, each expressed as a pushdown
automaton. For simplicity, we assume that the gets function call generates the system call read
and the syslog function call generates write.
When a model accepts a regular language, we simply have Γ = ∅ and transitions in
δ are only of the form (cid:5)s, (cid:6) σ→ (cid:5)s
, (cid:6). Although the experiments in Sect. 7 consider
the Stide model, a regular language acceptor, we intentionally designed our system to
analyze pushdown automata so that it is relevant to a wide collection of program models
of varying strength.
(cid:2)
Commensurate with our threat model, we assume that an attacker has prior knowl-
edge of the particular program model used to constrain execution of a vulnerable pro-
gram. The security of the system then relies entirely upon the ability of the program
model to detect attacks.
3.3 Finding Undetected Attacks
We have developed a model analysis system that evaluates a PDA-based program model
and ﬁnds undetected attacks. Our design has three features of note:
– It operates automatically. A user must provide an initial, one-time operating system
abstraction that can then be reused to analyze the model of any program execut-
ing on that operating system. This subsequent analysis requires no human input,
allowing the analysis to scale easily to large collections of program models.
– Attacks, which are sequences of system calls, do not need to be known. In fact, our
system provides attack sequences as output.
– System call arguments can signiﬁcantly alter the semantic meaning of the calls.
When our system ﬁnds an undetected attack sequence of system calls, it addition-
ally provides the system call arguments necessary to effect the attack.
We construct an abstraction of the operating system with respect to its security-critical
state. This abstraction can be repeatedly used to ﬁnd attacks in the models of programs
that execute on that operating system. Consider a simple example:
50
J.T. Gifﬁn, S. Jha, and B.P. Miller
Example 1. Running our tool for each of the four models in Fig. 3 shows that none de-
tect all attacks that execute a shell with root privilege. The tool automatically identiﬁes
a system call sequence, with arguments, that defeats each model:
read(0);
setreuid(0, 0);
write(0);
execve(“/bin/sh”);
The read and write calls are nops that are irrelevant to the attack. The setreuid call
alters OS state to gain root access, and the execve call executes a shell with that access.
One of our long-term goals is to use discovered undetected attacks to guide the future
design of program models and intrusion detection systems. Comparing the undetected
attack sequence with the original program code of Fig. 2 suggests a model alteration
that would eliminate this undetected attack. If the model constrains statically-known
system call argument values, then an attacker cannot undetectably use the setreuid call
to set the effective user ID to root. Although the attacker remains able to execute the
shell, that shell will not have increased privilege.
We will consider additional examples in Sect. 5.
4 Operating System Model
Given a program model M , answering the question ﬁrst posed in Sect. 1, what attacks
does M fail to detect?, requires understanding of what “attack” means. Previous work
deﬁned attacks as known, malicious sequences of system calls [24]. Directly searching
program models for these sequences unfortunately has two drawbacks: