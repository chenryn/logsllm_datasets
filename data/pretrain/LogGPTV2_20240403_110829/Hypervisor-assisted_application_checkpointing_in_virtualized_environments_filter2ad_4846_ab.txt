Server consolidation to reduce cost, space and power has 
been  a  driving  force  behind  the  success  of  platform 
virtualization.    Virtualization  allows  multiple  servers  to  run 
on the same physical hardware without interfering with each 
other.    A  thin  layer  called  hypervisor  or  Virtual  Machine 
Monitor  (VMM)  runs  on  top  of  the  hardware  and  provides 
virtual hardware interfaces to the VMs.  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:59 UTC from IEEE Xplore.  Restrictions apply. 
372In the case of Xen (see Figure 2), the hypervisor (VMM) 
runs at the highest privilege level and controls the hardware. 
Virtual machine instances are also called domains in Xen. A 
privileged  domain  called  Dom0  and  other  non-privileged 
guest domains called DomU run above the hypervisor like an 
application  runs  on  an  OS.  Dom0  is  a  management  domain 
that is privileged by Xen to directly access the hardware and 
it manages the initiation/termination of other domains. 
Dom0
(Backend driver)
DomU
(Frontend
Driver)
DomU
(Frontend
Driver)
DomU
(Frontend
Driver)
Event channel
Xen Hypervisor
Physical interrupt
Hardware (CPU, Memory, Devices)
Figure 2: Virtualization with Xen 
trap 
into 
instructions  will  generate  a 
The  hypervisor  virtualizes  physical  resources  such  as 
CPUs and memory for the guest domains. Most of the non-
privileged instructions can be executed by the guest domains 
natively without the intervention of the hypervisor. However, 
privileged 
the 
hypervisor.  The  hypervisor  validates  the  request  and  allows 
it  to  continue.  This  makes  certain  operations  such  as  page 
table  manipulation  especially  expensive 
in  virtualized 
environments.  The  guest  domain  can  also  use  hypercalls  to 
invoke  functions  in  the  hypervisor.    For  this,  the  guest  OS 
needs to be ported to use the functionality and this porting is 
called  para-virtualization.  Xen  provides  a  delegation 
approach for I/O via a split device driver model where each 
I/O  device  driver  called  the  backend  driver  runs  in  Dom0. 
The DomU has a frontend driver that communicates with the 
backend driver via event channels and shared memory. 
D.  Performance Overhead of Checkpointing under 
Virtualization 
A  quick  experiment  of 
the  performance  of  page 
protection in native vs. virtualized environments, both at the 
user  level,  shows  that one  page protection call  (specifically, 
mprotect() calls under Xen) is approximately 4 times slower 
under  virtualization.  To  understand  why  there  is  this 
enormous overhead, we need to look under the hood of how 
the relevant system calls operate under virtualization.  
During the checkpoint interval, each time the application 
writes to a write-protected page, it receives a page fault that 
traps  into the  signal  handler. Figure 3  shows  how  the  page-
fault is handled in native and virtual environments like Xen. 
Unlike the native environment, under virtualization, this call 
is trapped to the hypervisor. The signal handler issues a page 
protection  call  to  unprotect  the  page.  This  page  protection 
system call goes into kernel space and issues a call to update 
the page table. The page table update invokes a hypercall to 
trigger  a  translation  look-aside  buffer  (TLB)  flush  because 
TLB must be flushed to be synchronized with the page table. 
A  hypercall  is  needed  since  the  privileged  page  table 
operations can only be done in the context of the hypervisor. 
The  increased  number  of  context  switches  between  kernel-
space  and  the  hypervisor  and  the  added  overhead  of 
scheduling  each  of  these  in  the  virtual  environment,  makes 
the whole cycle very expensive.  
User 
page  
unprotect() 
User 
page  
unprotect() 
OS 
Signal 
Signal 
OS 
TLB flush 
Page fault 
Hypervisor 
TLB flush 
 (a) Native                             (b) Under virtualization    
Figure 3: Page fault handling on a write operation to a 
protected page 
Understanding 
impact  on 
checkpointing  is  one  contribution  of  our  work.  We  now 
delve into our approaches for solving this issue.  
this  overhead  and 
its 
III.  HYPERVISOR-ASSISTED CHECKPOINTING  
To  tackle  the  significant  overhead  of  page  protection 
system  calls  in  virtual  environments,  we  introduce  a  new 
model  of  checkpointing:  hypervisor-assisted  application 
checkpointing. The model has two key aspects: (i) support in 
the hypervisor to speed up certain operations that are key to 
checkpointing,  and  (ii)  a  new  model  of  application-
hypervisor 
interaction  motivated  by  our  checkpointing 
application.  An  important  aspect  of  the  model  is  its 
practicality and its feasibility of implementation. To that end, 
along  with  the  model  we  present  details  of  how  it  can  be 
implemented in a sample open-source virtualization platform 
(namely,  Xen).  Specific  checkpointing  techniques  that  use 
this model are discussed later in Section  IV. 
A.  Checkpointing Support in the Hypervisor 
The  first  aspect  (namely,  support  in  the  hypervisor) 
involves changes to the hypervisor to provide primitives that 
can  track  pages  changed  in  a  transaction.  These  primitives 
provide  the  ability  for  the  caller  to  inform  the  hypervisor 
about  (i)  the  memory  associated  with  the  critical  data  area, 
and (ii) the start and end of a transaction. Implementing these 
APIs  can  be  considered  similar  to  making  an  ioctl()  or 
system  call  to  the  hypervisor  and  the  relevant  data  (e.g., 
identity of the critical data area) is passed as arguments. 
At a high level, the hypervisor implements techniques to 
track the changes in the critical data area within a transaction 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:59 UTC from IEEE Xplore.  Restrictions apply. 
373and  provide  the  caller  with  these  changes  at  the  end  of  the 
transaction.  However,  there  are  interesting  design  and 
implementation  issues  in  how  this  is  tackled  by  the 
hypervisor and we elaborate on these below.  
One key issue is that the hypervisor must be able to over-
ride the application page fault handling mechanism, so that it 
can  tackle  it  in  the  hypervisor.  This  is  relatively  simple, 
given  the  higher  privilege  level  at  which  the  hypervisor 
operates.  In  particular,  a  new  page  fault  handler  in  the 
hypervisor  checks  if  a  fault  is  within  the  critical  data  area 
registered  with  it  and,  if  so,  handles  the  fault  and  returns  a 
success so execution can proceed normally in the application. 
A  second  related  design  issue  is  what  to  do  in  the  fault 
handler. One possible approach is to track the identity of the 
faulted  page,  which  is  a  hypervisor-assisted  counterpart  to 
the  PT  approach  from  Section   II.B  and  is  elaborated  on  in 
Section  IV.A. However, our architecture is general enough to 
allow  the  fault  handling  logic  to  be  pluggable.  This  allows 
for  interesting  new  techniques  supported  by  our  paradigm 
and they are described in Section  IV. 
Another  design  issue  is  process  identification,  since 
isolation is a key feature of virtualization that must continue 
to  be  supported.  While  this  also  appears  to  be  straight-
forward,  in  practice  it  is  not  trivial.    The  currently  running 
process is not visible to the hypervisor. However, there is an 
interesting technique based on address space changes where 
the address space is used to infer the identity of the process 
and this is discussed in more detail below in subsection  D.  
Hypervisors are designed to have a low footprint. Clearly, 
storage  of  too  much  information  within  the  hypervisor 
context  is  undesirable.  The  bulk  of  the  storage  for  our 
techniques  involves  tracking  and  maintaining  the  changed 
data  through  the  checkpoint  cycle.  In  our  architecture,  the 
caller  allocates  space  for  storing  this  information,  and  the 
hypervisor directly writes to that area. This obviates the need 
for maintaining this information within the hypervisor.  
B.  Application-Hypervisor Interaction 
In our discussion above in subsection  A, we deliberately 
avoided  the  issue  of  how  the  primitives  provided  by  the 
hypervisor  are  invoked  by  the  application.  This  is  a  crucial 
aspect of our technique and we elaborate on that below.  
A  hypercall  is  a  software  trap  from  a  guest  OS  to  the 
hypervisor,  just  as  a  syscall  is  a  software  trap  from  an 
application  to  the  kernel.  Guest  domains  use  hypercalls  to 
request  privileged  operations  like  updating  the  page-tables. 
Traditionally  hypercalls  are  only  possible  from  inside  the 
guest  operating  system.  Applications  are  not  allowed  to 
invoke  hypercalls  directly.  The  traditional  approach  would, 
therefore,  create  corresponding  system  calls  in  the  guest 
operating  system  that  will  be  invoked  by  the  application, 
which  would  then  translate  to our  checkpointing  hypercalls. 
Although potential performance benefits may still be realized 
by  this  implementation,  there  is  a  deployment  issue. 
Changing  the  guest  operating  system  for  each  deployment 
supported by the application is non-trivial.  
To  tackle  this  issue,  we  introduce  the  concept  of  secure 
direct  hypervisor  calls  from  the  application.  This  is  useful 
when  a  guest  domain  needs  to  be  deployed  using  an 
unmodified guest OS. In this model, the application directly 
talks  to  the  hypervisor  bypassing  the  guest  OS.  This  model 
of  communication  is  also  novel  from  a  virtualization 
perspective.  
There  are  a  few  ways  in  which  the  model  can  be 
implemented.  Regular  system  calls  and  hypercalls  from  the 
guest operating system are traditionally implemented in x86 
architectures  via  an  interrupt  vector  with  values  0x80  and 
0x82  respectively.  We  have  implemented  the  user-to-
hypervisor call through an additional interrupt vector 0x84 as 
shown below. 
Traditional 
Approach 
Application 
0x80: 
syscall 
0x82: 
Hypercall 
Guest OS 
Xen Hypervisor 
Our 
Approach 
0x84: 
Hypercall 
Hardware (CPU, Memory, Devices) 
Figure 4: User-to-Hypervisor Call 
For  security  purposes,  only  a  set  of  pre-defined  hypercalls 
are  allowed  to  use  the  0x84  interrupt  vector.    Additionally, 
these  hypercalls  are  only  allowed  to  work  in  the  process 
space  of  the  calling  process,  thereby  creating  a  level  of 
isolation  essential  for  security.  (Isolation  is  obtained  using 
the techniques in subsection  D.)  
through  a  shared  memory 
There  are  alternative  approaches.  For  example, 
communication  between  the  application  and  the  hypervisor 
could  be  done 
is 
communicated by the application to the hypervisor through a 
privileged domain like Dom0 in Xen. For brevity, we do not 
elaborate on these alternative approaches.  
C.  Access Control  
that 
It  is  possible  that  administrators  would  like  to  limit  the 
application instances that can invoke the hypervisor-assisted 
checkpointing primitives. Authorization of valid applications 
can be done by using a policy module in Dom0. To achieve 
this,  the  application  inside  the  guest  domain  can  be 
provisioned  with  a  key,  and  this  key  can  be  used  to 
authenticate it to the hypervisor via a privileged domain like 
Dom0. The application can initiate the process via a network 
connection  to  Dom0.  The  privileged  domain  Dom0  can 
provide  the  mechanisms  for  registering  the  application  and 
issuing  any  required  shared  tokens.  Once  the  application  is 
registered with Dom0, it is allowed to invoke the hypercalls 
directly.   
D.  Implementation: Process tracking 
As discussed earlier, a user space process in the guest OS 
is allowed to make hypercalls to invoke functionality directly 
in  the  hypervisor.  For  security  and  functionality,  Xen  needs 
to  uniquely  identify  the  user  process  when  it  makes  a 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:59 UTC from IEEE Xplore.  Restrictions apply. 
374hypercall.  This  requires  guest  process  tracking  at  the 
hypervisor-level.  
within  the  hypervisor is  invoked. This in  turn puts  the page 
in the modified page list and unprotects the page.  
In  Linux,  each  process  has  a  unique  address  space  and 
our technique uses this for identifying the process within the 