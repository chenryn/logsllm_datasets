### 4.1 The Common Case
In the common case, the system progresses globally without the need for a new leading site or site representative election. The process is as follows:

1. **Update Propagation:**
   - A client sends an update to a server within its local site.
   - This server forwards the update to the local representative.
   - The local representative then forwards the update to the representative of the leading site.
   - If the client does not receive a reply in time, it broadcasts the update.

2. **Global Sequence Number Assignment:**
   - The representative of the leading site initiates the ASSIGN-SEQUENCE protocol to assign a global sequence number to the update.
   - This assignment is encapsulated in a proposal message.
   - The leading site generates a signature on the constructed proposal using THRESHOLD-SIGN.
   - The representative sends the signed proposal to the representatives of all other sites for global ordering.

3. **Proposal and Acceptance:**
   - When a representative receives a signed proposal, it forwards this proposal to the servers in its site.
   - Upon receiving a proposal, a server constructs a site acknowledgment (accept) and invokes THRESHOLD-SIGN on this message.
   - The representative combines the partial signatures and then sends the resulting threshold-signed accept to the representatives of the other sites.

4. **Global Ordering and Client Response:**
   - The representative of a site forwards the incoming accept messages to the local servers.
   - A server globally orders the update when it receives \(\lceil N/2 \rceil\) distinct accept messages (where \(N\) is the number of sites) and the corresponding proposal.
   - The server at the client’s local site that originally received the update sends a reply back to the client.

### 4.2 Detailed Protocols

#### Threshold-Sign
- **Description:**
  - The THRESHOLD-SIGN intra-site protocol generates a \((2f + 1, 3f + 1)\) threshold signature on a given message.
  - Upon invoking the protocol, a server generates a partial signature on the message and a verification proof that other servers can use to confirm the partial signature's validity.
  - Both the partial signature and the verification proof are broadcast within the site.
  - Upon receiving \(2f+1\) partial signatures on a message, a server combines them into a threshold signature, which is then verified using the site’s public key.
  - If the signature verification fails, one or more partial signatures used in the combination were invalid. The verification proofs are used to identify incorrect shares, and the corresponding servers are classified as malicious. Further messages from these servers are ignored.

#### Assign-Sequence
- **Description:**
  - The ASSIGN-SEQUENCE intra-site protocol consists of three rounds, the first two of which are similar to the corresponding rounds in BFT.
  - At the end of the second round, any server that has received \(2f\) prepares and pre-prepare messages for the same view and sequence number invokes THRESHOLD-SIGN to generate a threshold signature on the representative’s proposal.

### 4.3 Handling Failures

Several types of failures may occur during system execution, such as the corruption of a site representative or the partitioning of the leader site. These failures require careful handling to maintain safety and liveness.

We use two relatively independent mechanisms to handle failures:
1. **Coordinator Election:**
   - If a protocol coordinator is faulty, the correct participants elect a new coordinator using a protocol similar to the one described in [6].
2. **Reconciliation:**
   - We use reconciliation to constrain protocol participants such that safety is preserved across views. There is both a local and global component to this mechanism, each serving a similar function at its level of the hierarchy.

#### Leader Election
- **Description:**
  - For a detailed description of the intra-site representative election protocol, refer to [6].
  - To elect a leading site, each site first runs an intra-site protocol to agree upon the site it will propose. These votes are exchanged among the sites in a manner similar to the intra-site representative election protocol.

#### Construct Collective State
- **Description:**
  - The CONSTRUCT-COLLECTIVE-STATE protocol is used in both local and global view changes.
  - It guarantees sufficient intra-site reconciliation to safely make progress after a local view change and generates a message reflecting the site’s level of knowledge, which is used during a global view change.
  - A site representative invokes the protocol by sending a sequence number, \(seq\), to all servers within the site.
  - A server responds with a message containing the updates it has ordered and/or acknowledged with a higher sequence number than \(seq\).
  - The representative computes the union of \(2f + 1\) responses, eliminating duplicates and using the latest view if multiple updates have the same sequence number, and broadcasts it within the site.
  - When a server receives a union message, it collects missing messages from the union and invokes THRESHOLD-SIGN on the union.

### 4.4 Timeouts

#### Local Representative (T1)
- **Description:**
  - This timeout expires at a server of a non-leading site to replace the representative once no global progress takes place for that period.
  - Once the timeout expires at \(f + 1\) servers, the local view change protocol takes place.
  - T1 should be higher than 3 times the WAN round-trip to allow a potential global view change protocol to complete without changing the local representative.

#### Leading Site Representative (T2)
- **Description:**
  - This timeout expires at a server at the leading site to replace the representative once no global progress takes place for that period.
  - T2 should be large enough to allow the representative to communicate with a majority of the sites.
  - Specifically, since not all sites may be lined up with correct representatives at the same time, T2 should be chosen such that each site can replace its representatives until a correct one will communicate with the leading site.
  - Thus, we need \(T_2 > (f + 2) \times \max(T_1)\), where \(\max(T_1)\) is an estimate of the largest T1 at any site.

#### Leading Site (T3)
- **Description:**
  - This timeout expires at a site to replace the leading site once no global progress takes place for that period.
  - Since we choose T2 to ensure a single communication round with every site, and the leading site needs at least 3 rounds to prove progress, in the worst case, the leading site must have a chance to elect 3 correct representatives to show progress before being replaced.
  - Thus, we need \(T_3 = (f + 3) \times T_2\).

#### Client Timer (T0)
- **Description:**
  - This timeout expires at the client, triggering it to broadcast its last update.
  - T0 can have an arbitrary value.

#### Timeouts Management
- **Description:**
  - Servers send their timer estimates (T1, T2) on global view change messages.
  - The site representative disseminates the \(f + 1\)st highest value to prevent faulty servers from injecting wrong estimates.
  - Potentially, timers can be exchanged as part of local view change messages as well.
  - The leading site representative chooses the maximum timer of all sites with which it communicates to determine T2 (which in turn determines T3).
  - Servers estimate the network round-trip according to various interactions they have had and can reduce the value if communication improves.

### 5 Performance Evaluation

To evaluate the performance of our hierarchical architecture, we implemented a complete prototype of our protocol, including all necessary communication and cryptographic functionality. In this paper, we focus only on the networking and cryptographic aspects of our protocols and do not consider disk writes.

#### Testbed and Network Setup
- **Description:**
  - We selected a network topology consisting of 5 wide area sites and assumed at most 5 Byzantine faults in each site to quantify the performance of our system in a realistic scenario.
  - This requires 16 replicated servers in each site.
  - Our experimental testbed consists of a cluster with twenty 3.2 GHz, 64-bit Intel Xeon computers.
  - Each computer can compute a 1024-bit RSA signature in 1.3 ms and verify it in 0.07 ms.
  - For \(n=16\), \(k=11\), 1024-bit threshold cryptography, a computer can compute a partial signature and verification proof in 3.9 ms and combine the partial signatures in 5.6 ms.
  - The leader site was deployed on 16 machines, and the other 4 sites were emulated by one computer each.
  - An emulating computer performed the role of a representative of a complete 16-server site.
  - Thus, our testbed was equivalent to an 80-node system distributed across 5 sites.
  - Upon receiving a message, the emulating computers busy-waited for the time it took a 16-server site to handle that packet and reply to it, including in-site communication and computation.
  - We determined busy-wait times for each type of packet by benchmarking individual protocols on a fully deployed, 16-server site.
  - We used the Spines [14] messaging system to emulate latency and throughput constraints on the wide area links.

#### Comparison with BFT
- **Description:**
  - We compared the performance results of the above system with those of BFT [6] on the same network setup with five sites, run on the same cluster.
  - Instead of using 16 servers in each site, for BFT we used a total of 16 servers across the entire network.
  - This allows for up to 5 Byzantine failures in the entire network for BFT, instead of up to 5 Byzantine failures in each site for Steward.
  - Since BFT is a flat solution where there is no correlation between faults and the sites where they can occur, we believe this comparison is fair.
  - We distributed the BFT servers such that four sites contain 3 servers each, and one site contains 4 servers.
  - All the write updates and read-only queries in our experiments carried a payload of 200 bytes, representing a common SQL statement.
  - Note that, qualitatively, the results reported for BFT are not an artifact of the specific implementation we benchmarked. We obtained similar results to BFT using our BFT-like intra-site agreement protocol, ASSIGN-SEQUENCE, under the same conditions.

#### Bandwidth Limitation
- **Description:**
  - We first investigate the benefits of the hierarchical architecture in a symmetric configuration with 5 sites, where all sites are connected to each other with 50 milliseconds latency links (emulating crossing the continental US).
  - Figure 1 shows how limiting the capacity of wide area links affects update throughput.
  - As we increase the number of clients, BFT’s throughput increases at a lower slope than Steward’s, mainly due to the additional wide area crossing for each update.
  - Steward can process up to 84 updates/sec in all bandwidth cases, at which point it is limited by CPU used to compute threshold signatures.
  - At 10, 5, and 2.5 Mbps, BFT achieves about 58, 26, and 6 updates/sec, respectively.
  - In each of these cases, BFT’s throughput is bandwidth limited.
  - We also notice a reduction in the throughput of BFT as the number of clients increases, attributed to a cascading increase in message loss caused by the lack of flow control in BFT.
  - For the same reason, we were not able to run BFT with more than 24 clients at 5 Mbps and 15 clients at 2.5 Mbps.
  - We believe that adding a client queuing mechanism would stabilize the performance of BFT to its maximum achieved throughput.

#### Average Update Latency
- **Description:**
  - Figure 2 shows that Steward’s average update latency slightly increases with the addition of clients, reaching 190 ms at 15 clients in all bandwidth cases.
  - As client updates start to be queued, latency increases linearly.
  - BFT exhibits a similar trend at 10 Mbps, where the average update latency is 336 ms at 15 clients.
  - As the bandwidth decreases, the update latency increases heavily, reaching 600 ms at 5 Mbps and 5 seconds at 2.5 Mbps, at 15 clients.

#### Adding Read-Only Queries
- **Description:**
  - Our hierarchical architecture enables read-only queries to be answered locally.
  - To demonstrate this benefit, we conducted an experiment where 10 clients send random mixes of read-only queries and write updates.
  - We compared the performance of Steward and BFT with 50 ms, 10 Mbps links, where neither was bandwidth limited.
  - Figures 3 and 4 show the average throughput and latency, respectively, of different mixes of queries and updates.
  - When clients send only queries, Steward achieves about 2.9 ms per query, with a throughput of over 3,400 queries/sec.
  - Since queries are answered locally, their latency is dominated by two RSA signatures, one at the originating client and one at the servers answering the query.
  - Depending on the mix ratio, Steward performs 2 to 30 times better than BFT.
  - BFT’s read-only query latency is about 105 ms, and its throughput is 95 queries/sec.
  - This is expected, as read-only queries in BFT need to be answered by at least \(f + 1\) servers, some of which are located across wide area links.
  - BFT requires at least \(2f + 1\) servers in each site to guarantee that it can answer queries locally.
  - Such a deployment, for 5 faults and 5 sites, would require at least 55 servers, which would dramatically increase communication for updates and reduce BFT’s performance.

#### Wide Area Scalability
- **Description:**
  - To demonstrate Steward’s scalability on real networks, we conducted an experiment where clients inject write updates.
  - The results are shown in Figures 1 and 2, illustrating the superior performance of Steward in terms of throughput and latency compared to BFT.