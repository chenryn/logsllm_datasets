4.1 The Common Case
During the common case, global progress is made, and
no leading site or site representative election occurs. The
common case works as follows:
1. A client sends an update to a server in its local site.
This server forwards the update to the local repre-
sentative, which forwards the update to the repre-
sentative of the leading site. If the client does not
receive a reply in time, it broadcasts the update.
2. The representative of the leading site initiates the
ASSIGN-SEQUENCE protocol to assign a global se-
quence number to the update; this assignment is en-
capsulated in a proposal message. The site then gen-
erates a signature on the constructed proposal using
THRESHOLD-SIGN, and the representative sends the
signed proposal to the representatives of all other
sites for global ordering.
3. When a representative receives a signed proposal, it
forwards this proposal to the servers in its site. Upon
receiving a proposal, a server constructs a site ac-
knowledgment (accept) and invokes THRESHOLD-
SIGN on this message. The representative combines
the partial signatures and then sends the resulting
threshold-signed accept to the representatives of the
other sites.
4. The representative of a site forwards the incoming
accept messages to the local servers. A server glob-
ally orders the update when it receives bN=2c dis-
tinct accept messages (where N is the number of
sites) and the corresponding proposal. The server
at the client’s local site that originally received the
update sends a reply back to the client.
We now highlight the details of the THRESHOLD-SIGN
and ASSIGN-SEQUENCE protocols.
Threshold-Sign: The THRESHOLD-SIGN intra-site
protocol generates a (2f + 1, 3f + 1) threshold signa-
ture on a given message 1. Upon invoking the protocol,
a server generates a partial signature on the message to
be signed and a veri(cid:2)cation proof that other servers can
use to con(cid:2)rm that the partial signature was created using
a valid share. Both the partial signature and the veri(cid:2)ca-
tion proof are broadcast within the site. Upon receiving
2f+1 partial signatures on a message, a server combines
the partial signatures into a threshold signature on that
message, which is then veri(cid:2)ed using the site’s public
key. If the signature veri(cid:2)cation fails, one or more par-
tial signatures used in the combination were invalid, in
which case the veri(cid:2)cation proofs provided with the par-
tial signatures are used to identify incorrect shares; the
corresponding servers are classi(cid:2)ed as malicious. Further
messages from the corrupted servers are ignored.
Assign-Sequence: The ASSIGN-SEQUENCE intra-site
protocol consists of three rounds, the (cid:2)rst two of which
are similar to the corresponding rounds of BFT. At the
end of the second round, any server that has received 2f
prepares and the pre-prepare, for the same view and se-
quence number, invokes THRESHOLD-SIGN to generate a
threshold signature on the representative’s proposal.
4.2 View Changes
Several types of failure may occur during system ex-
ecution, such as the corruption of a site representative or
the partitioning of the leader site. Such failures require
delicate handling to preserve safety and liveness.
We use two relatively independent mechanisms to
handle failures. First, if a protocol coordinator 2 is faulty,
the correct participants elect a new coordinator using a
protocol similar to the one described in [6]. Second, we
use reconciliation to constrain protocol participants such
that safety is preserved across views. Note that there is
a local and global component to both mechanisms, each
serving a similar function at its level of the hierarchy.
Below we provide relevant details of leader elec-
intra-site reconciliation (via the CONSTRUCT-
local view
tion,
COLLECTIVE-STATE intra-site protocol),
change, and global view change.
Leader Election: We refer the reader to [6] for a de-
tailed description of the intra-site representative election
protocol. To elect a leading site, each site (cid:2)rst runs an
intra-site protocol to agree upon the site it will propose;
these votes are exchanged among the sites in a manner
similar to the intra-site representative election protocol.
1We could use an (f + 1, 3f + 1) threshold signature at the cost of
an additional protocol round.
2Within a site, the protocol coordinator is the local representative,
and, in the high-level protocol, the coordinator is the leading site.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:28 UTC from IEEE Xplore.  Restrictions apply. 
Construct Collective State:
The CONSTRUCT-
COLLECTIVE-STATE protocol is used as a building block
in both view changes. It serves two primary functions:
guaranteeing suf(cid:2)cient intra-site reconciliation to safely
make progress after a local view change and generating a
message re(cid:3)ecting the site’s level of knowledge, which is
used during a global view change.
A site representative invokes the protocol by sending
a sequence number, seq, to all servers within the site. A
server responds with a message containing the updates it
has ordered and/or acknowledged with a higher sequence
number than seq. The representative computes the union
of 2f + 1 responses, eliminating duplicates and using the
update from the latest view if multiple updates have the
same sequence number, and broadcasts it within the site.
When a server receives a union message, it collects miss-
ing messages from the union and invokes THRESHOLD-
SIGN on the union.
Local View Change: The local view change protocol
is triggered in the leading site after a local representative
election. The new representative invokes CONSTRUCT-
COLLECTIVE-STATE with the sequence number up to
which it has ordered all updates. Upon completion, cor-
rect servers (including the new representative) are recon-
ciled such that they can constrain the updates proposed in
the new view to preserve safety. The new representative
then invokes ASSIGN-SEQUENCE to replay all pending
updates contained in the union message.
Global View Change: The global view change pro-
tocol is triggered after a leading site election. The repre-
sentative of the new leading site invokes CONSTRUCT-
COLLECTIVE-STATE with the sequence number up to
which it has ordered all updates. The resulting union
message implicitly contains the sequence number up to
which all updates have been ordered within the site; cor-
rect servers invoke THRESHOLD-SIGN on a message con-
taining this number, and the representative sends the
signed message to all other site representatives. Upon
receiving this message, a non-leading site representative
invokes CONSTRUCT-COLLECTIVE-STATE and sends the
resultant union to the representative of the new leading
site. Servers in the leading site use the union messages
from a majority of sites to constrain the proposals they
will generate in the new view.
4.3 Timeouts
provide details about the timeouts in our protocols.
Local representative (T1): This timeout expires at a
server of a non-leading site to replace the representative
once no (global) progress takes place for that period of
time. Once the timeout expires at f + 1 servers, the
local view change protocol takes place. T1 should be
higher than 3 times the WAN round-trip to allow a po-
tential global view change protocol to complete without
changing the local representative.
Leading site representative (T2): This timeout expires
at a server at the leading site to replace the representa-
tive once no (global) progress takes place for that period
of time. T2 should be large enough to allow the rep-
resentative to communicate with a majority of the sites.
Speci(cid:2)cally, since not all sites may be lined up with cor-
rect representatives at the same time, T2 should be chosen
such that each site can replace its representatives until a
correct one will communicate with the leadings site; the
site needs to have a chance to replace f + 1 represen-
tatives within the T2 time period. Thus, we need that
T 2 >(f+2)(cid:3)maxT 1, where maxT 1 is an estimate of the
largest T1 at any site. The (f + 2) covers the possibility
that when the leader site elects a representative, the T1
timer is already running at other sites.
Leading site (T3): This timeout expires at a site to
replace the leading site once no (global) progress takes
place for that period of time. Since we choose T2 to en-
sure a single communication round with every site, and
since the leading site needs at least 3 rounds to prove
progress, in the worse case, the leading site must have a
chance to elect 3 correct representatives to show progress,
before being replaced. Thus, we need T 3 = (f + 3)T 2.
Client timer (T0): This timeout expires at the client,
triggering it to broadcast its last update. T0 can have an
arbitrary value.
Timeouts management: Servers send their timers esti-
mation (T1, T2) on global view change messages. The
site representative disseminates the f + 1st highest value
(the value for which f higher or equal values exist) to
prevent the faulty servers from injecting wrong estimates.
Potentially, timers can be exchanged as part of local view
change messages as well. The leading site representa-
tive chooses the maximum timer of all sites with which it
communicates to determine T2 (which in turn determines
T3). Servers estimate the network round-trip according
to various interactions they have had. They can reduce
the value if communication seems to improve.
Steward relies on timeouts to detect problems with the
representatives in different sites or with the leading site.
Our protocols do not assume synchronized clocks; how-
ever, we do assume that the rate of the clocks at different
servers is reasonably close. We believe that this assump-
tion is valid considering today’s technology. Below we
5 Performance Evaluation
To evaluate the performance of our hierarchical archi-
tecture, we implemented a complete prototype of our pro-
tocol including all necessary communication and cryp-
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:28 UTC from IEEE Xplore.  Restrictions apply. 
tographic functionality. In this paper we focus only on
the networking and cryptographic aspects of our proto-
cols and do not consider disk writes.
Testbed and Network Setup: We selected a network
topology consisting of 5 wide area sites and assumed at
most 5 Byzantine faults in each site, in order to quan-
tify the performance of our system in a realistic scenario.
This requires 16 replicated servers in each site.
Our experimental testbed consists of a cluster with
twenty 3.2 GHz, 64 bit Intel Xeon computers. Each com-
puter can compute a 1024 bit RSA signature in 1.3 ms
and verify it in 0.07 ms. For n=16, k=11, 1024 bit thresh-
old cryptography which we use for these experiments, a
computer can compute a partial signature and veri(cid:2)cation
proof in 3.9 ms and combine the partial signatures in 5.6
ms. The leader site was deployed on 16 machines, and
the other 4 sites were emulated by one computer each.
An emulating computer performed the role of a repre-
sentative of a complete 16 server site. Thus, our testbed
was equivalent to an 80 node system distributed across
5 sites. Upon receiving a message, the emulating com-
puters busy-waited for the time it took a 16 server site to
handle that packet and reply to it, including in-site com-
munication and computation. We determined busy-wait
times for each type of packet by benchmarking individ-
ual protocols on a fully deployed, 16 server site. We used
the Spines [14] messaging system to emulate latency and
throughput constraints on the wide area links.
We compared the performance results of the above
system with those of BFT [6] on the same network setup
with (cid:2)ve sites, run on the same cluster. Instead of using
16 servers in each site, for BFT we used a total of 16
servers across the entire network. This allows for up to 5
Byzantine failures in the entire network for BFT, instead
of up to 5 Byzantine failures in each site for Steward.
Since BFT is a (cid:3)at solution where there is no correla-
tion between faults and the sites where they can occur,
we believe this comparison is fair. We distributed the
BFT servers such that four sites contain 3 servers each,
and one site contains 4 servers. All the write updates and
read-only queries in our experiments carried a payload of
200 bytes, representing a common SQL statement.
Note that, qualitatively, the results reported for BFT
are not an artifact of the speci(cid:2)c implementation we
benchmarked. We obtained similar results to BFT us-
ing our BFT-like intra-site agreement protocol, ASSIGN-
SEQUENCE, under the same conditions.
Bandwidth Limitation: We (cid:2)rst investigate the ben-
e(cid:2)ts of the hierarchical architecture in a symmetric con-
(cid:2)guration with 5 sites, where all sites are connected to
each other with 50 milliseconds latency links (emulating
crossing the continental US).
Figure 1 shows how limiting the capacity of wide area
links effects update throughput. As we increase the num-
ber of clients, BFT’s throughput increases at a lower
slope than Steward’s, mainly due to the additional wide
area crossing for each update. Steward can process up
to 84 updates/sec in all bandwidth cases, at which point
it is limited by CPU used to compute threshold signa-
tures. At 10, 5, and 2.5 Mbps, BFT achieves about 58, 26,
and 6 updates/sec, respectively. In each of these cases,
BFT’s throughput is bandwidth limited. We also notice
a reduction in the throughput of BFT as the number of
clients increases. We attribute this to a cascading increase
in message loss, caused by the lack of (cid:3)ow control in
BFT. For the same reason, we were not able to run BFT
with more than 24 clients at 5 Mbps, and 15 clients at
2.5 Mbps. We believe that adding a client queuing mech-
anism would stabilize the performance of BFT to its max-
imum achieved throughput.
Figure 2 shows that Steward’s average update latency
slightly increases with the addition of clients, reaching
190 ms at 15 clients in all bandwidth cases. As client up-
dates start to be queued, latency increases linearly. BFT
exhibits a similar trend at 10 Mbps, where the average
update latency is 336 ms at 15 clients. As the bandwidth
decreases, the update latency increases heavily, reach-
ing 600 ms at 5 Mbps and 5 seconds at 2.5 Mbps, at 15
clients.
Adding Read-only Queries: Our hierarchical archi-
tecture enables read-only queries to be answered locally.
To demonstrate this bene(cid:2)t, we conducted an experiment
where 10 clients send random mixes of read-only queries
and write updates. We compared the performance of
Steward and BFT with 50 ms, 10 Mbps links, where nei-
ther was bandwidth limited. Figures 3 and 4 show the
average throughput and latency, respectively, of different
mixes of queries and updates. When clients send only
queries, Steward achieves about 2.9 ms per query, with a
throughput of over 3,400 queries/sec. Since queries are
answered locally, their latency is dominated by two RSA
signatures, one at the originating client and one at the
servers answering the query. Depending on the mix ratio,
Steward performs 2 to 30 times better than BFT.
BFT’s read-only query latency is about 105 ms, and its
throughput is 95 queries/sec. This is expected, as read-
only queries in BFT need to be answered by at least f + 1
servers, some of which are located across wide area links.
BFT requires at least 2f + 1 servers in each site to guar-
antee that it can answer queries locally. Such a deploy-
ment, for 5 faults and 5 sites, would require at least 55
servers, which would dramatically increase communica-
tion for updates and reduce BFT’s performance.
Wide Area Scalability: To demonstrate Steward’s
scalability on real networks, we conducted an experiment
In the (cid:2)rst experiment, clients inject write updates.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:28 UTC from IEEE Xplore.  Restrictions apply. 
Steward 10 Mbps
Steward    5 Mbps
Steward 2.5 Mbps
BFT 10 Mbps
BFT   5 Mbps
BFT  2.5 Mbps
 0
 5
 10
 15
 20
 25
 30
Figure 1: Write Update Throughput
Clients
Steward
BFT
 100
 90