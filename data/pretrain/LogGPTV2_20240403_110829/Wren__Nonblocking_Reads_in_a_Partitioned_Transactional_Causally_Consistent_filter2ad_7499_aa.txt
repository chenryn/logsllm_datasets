title:Wren: Nonblocking Reads in a Partitioned Transactional Causally Consistent
Data Store
author:Kristina Spirovska and
Diego Didona and
Willy Zwaenepoel
2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Wren: Nonblocking Reads in a Partitioned
Transactional Causally Consistent Data Store
Kristina Spirovska
EPFL
Diego Didona
EPFL
Willy Zwaenepoel
EPFL
kristina.spirovska@epﬂ.ch
diego.didona@epﬂ.ch
willy.zwaenepoel@epﬂ.ch
Abstract—Transactional Causal Consistency (TCC) extends
causal consistency, the strongest consistency model compatible
with availability, with interactive read-write transactions, and is
therefore particularly appealing for geo-replicated platforms.
This paper presents Wren, the ﬁrst TCC system that at the
same time i) implements nonblocking read operations, thereby
achieving low latency, and ii) allows an application to efﬁciently
scale out within a replication site by sharding.
Wren introduces new protocols for transaction execution,
dependency tracking and stabilization. The transaction protocol
supports nonblocking reads by providing a transaction with a
snapshot that is the union of a fresh causal snapshot S installed
by every partition in the local data center and a client-side cache
for writes that are not yet included in S. The dependency tracking
and stabilization protocols require only two scalar timestamps,
resulting in efﬁcient resource utilization and providing scalability
in terms of replication sites. In return for these beneﬁts, Wren
slightly increases the visibility latency of updates.
We evaluate Wren on an AWS deployment using up to 5
replication sites and 16 partitions per site. We show that Wren
delivers up to 1.4x higher throughput and up to 3.6x lower latency
when compared to the state-of-the-art design. The choice of an
older snapshot increases local update visibility latency by a few
milliseconds. The use of only two timestamps to track causality
increases remote update visibility latency by less than 15%.
I. INTRODUCTION
Many large-scale data platforms rely on geo-replication to
meet strict performance and availability requirements [1], [2],
[3], [4], [5]. Geo-replication reduces latencies by keeping a
copy of the data close to the clients, and enables availability
by replicating data at geographically distributed data centers
(DCs). To accommodate the ever-growing volumes of data,
today’s large-scale on-line services also partition the data
across multiple servers within a single DC [6], [7].
Transactional Causal Consistency (TCC). TCC [8] is an
attractive consistency level for building geo-replicated data-
stores. TCC enforces causal consistency (CC) [9], which is the
strongest consistency model compatible with availability [10],
[11]. Compared to strong consistency [12], CC does not suffer
from high synchronization latencies, limited scalability and
unavailability in the presence of network partitions between
DCs [13], [14], [15]. Compared to eventual consistency [2],
CC avoids a number of anomalies that plague programming
with weaker models. In addtion, TCC extends CC with inter-
active read-write transactions, that allow applications to read
from a causal snapshot and to perform atomic multi-item
writes.
2158-3927/18/$31.00 ©2018 IEEE
DOI 10.1109/DSN.2018.00014
1
Enforcing CC while offering always-available interactive
multi-partition transactions is a challenging problem [7]. The
main culprit is that in a distributed environment, unavoidably,
partitions do not progress at the same pace. Current TCC
designs either avoid this issue altogether, by not supporting
sharding [16], or block reads to ensure that the proper snapshot
is installed [8]. The former approach sacriﬁces scalability,
while the latter incurs additional latencies.
Wren. This paper presents Wren, the ﬁrst TCC system that
implements nonblocking reads, thereby achieving low latency,
and allows an application to scale out by sharding. Wren
implements CANToR (Client-Assisted Nonblocking Trans-
actional Reads), a novel transaction protocol in which the
snapshot of the data store visible to a transaction is deﬁned as
the union of two components: i) a fresh causal snapshot that
has been installed by every partition within the DC; and ii)
a per-client cache, which stores the updates performed by the
client that are not yet reﬂected in said snapshot. This choice
of snapshot departs from earlier approaches where a snapshot
is chosen by simply looking at the local clock value of the
partition acting as transaction coordinator.
Wren also introduces Binary Dependency Time (BDT), a
new dependency tracking protocol, and Binary Stable Time
(BiST), a new stabilization protocol. Regardless of the number
of partitions and DCs, these two protocols assign only two
scalar timestamps to updates and snapshots, corresponding
to dependencies on local and remote items. These protocols
provide high resource efﬁciency and scalability, and preserve
availability.
Wren exposes to clients a snapshot that is slightly in the
past with respect to the one exposed by existing approaches.
We argue that this is a small price to pay for the performance
improvements that Wren offers.
We compare Wren with Cure [8], the state-of-the-art TCC
system, on an AWS deployment with up to 5 DCs with 16
partitions each. Wren achieves up to 1.4x higher throughput
and up to 3.6x lower latencies. The choice of an older snapshot
increases local update visibility latency by a few milliseconds.
The use of only two timestamps to track causality increases
remote update visibility latency by less than 15%.
We make the following contributions.
1) We present the design and implementation of Wren, the
ﬁrst TCC key-value store that achieves nonblocking reads,
efﬁciently scales horizontally, and tolerates network partitions
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
between DCs.
2) We propose new dependency and stabilization protocols that
achieve high resource efﬁciency and scalability.
3) We experimentally demonstrate the beneﬁts of Wren over
state-of-the-art solutions.
Roadmap. The paper is organized as follows. Section 2
describes TCC and the target system model. Section 3 presents
the design of Wren. Section 4 describes the protocols in Wren.
Section 5 presents the evaluation of Wren. Section 6 discusses
related work. Section 7 concludes the paper.
II. SYSTEM MODEL AND DEFINITIONS
A. System model
We consider a distributed key-value store whose data-set is
split into N partitions. Each key is deterministically assigned
to one partition by a hash function. We denote by px the
partition that contains key x.
The data-set is fully replicated: each partition is replicated
at all M DCs. We assume a multi-master system, i.e., each
replica can update the keys in its partition. Updates are
replicated asynchronously to remote DCs.
The data store is multi-versioned. An update operation
creates a new version of a key. Each version stores the value
corresponding to the key and some meta-data to track causal-
ity. The system periodically garbage-collects old versions of
keys.
At the beginning of a session, a client c connects to a DC,
referred to as the local DC. All c’s operations are performed
within said DC to preserve availability [17] 1. c does not issue
another operation until it receives the reply to the current one.
Partitions communicate through point-to-point lossless FIFO
channels (e.g., a TCP socket).
B. Causal consistency
Causal consistency requires that the key-value store returns
values that are consistent with causality [9], [18]. For two
operations a, b, we say that b causally depends on a, and write
a (cid:2) b, if and only if at least one of the following conditions
holds: i) a and b are operations in a single thread of execution,
and a happens before b; ii) a is a write operation, b is a read
operation, and b reads the version written by a; iii) there is
some other operation c such that a (cid:2) c and c (cid:2) b. Intuitively,
CC ensures that if a client has seen the effects of b and a (cid:2) b,
then the client also sees the effects of a.
We use lower-case letters, e.g., x, to refer to a key and the
corresponding upper-case letter, e.g., X, to refer to a version
of the key. We say that X causally depends on Y if the write
of X causally depends on the write of Y .
We use the term availability to indicate that a client opera-
tion never blocks as the result of a network partition between
DCs [19].
1Wren can be extended to allow a client c to move to a different DC by
blocking c until the last snapshot seen by c has been installed in the new DC.
C. Transactional causal consistency
Semantics. TCC extends CC by means of interactive read-
write transactions in which clients can issue several operations
within a transaction, each reading or writing (potentially)
multiple items [8]. TCC provides a more powerful semantics
than one-shot read-only or write-only transactions provided
by earlier CC systems [7], [15], [20], [21]. It enforces the
following two properties.
1. Transactions read from a causal snapshot. A causal snap-
shot is a set of item versions such that all causal dependencies
of those versions are also included in the snapshot. For any
two items, x and y, if X (cid:2) Y and both X and Y belong
to the same causal snapshot, then there is no X(cid:2), such that
X (cid:2) X(cid:2) (cid:2) Y .
Transactional reads from a causal snapshot avoid undesir-
able anomalies that can arise by issuing multiple individual
read operations. For example, they prevent the well-known
anomaly in which person A removes person B from the access
list of a photo album and adds a photo to it, only to have
person B read the original permissions and the new version of
the album [15].
2. Updates are atomic. Either all items written by a transaction
are visible to other transactions, or none is. If a transaction
writes X and Y , then any snapshot visible to other transactions
either includes both X and Y or neither one of them.
Atomic updates increase the expressive power of applica-
tions, e.g., they make it easier to maintain symmetric relation-
ships among entities within an application. For example, in
a social network, if person A becomes friend with person B,
then B simultaneously becomes friend with A. By putting both
updates inside a transaction, both or neither of the friendship
relations are visible to other transactions [21].
Conﬂict resolution. Two writes are conﬂicting if they are
not related by causality and update the same key. Conﬂicting
writes are resolved by means of a commutative and associative
function, that decides the value corresponding to a key given
its current value and the set of updates on the key [15].
For simplicity, Wren resolves write conﬂicts using the last-
writer-wins rule based on the timestamp of the updates [22].
Possible ties are settled by looking at the id of the update’s
originating DC combined with the identiﬁer of transaction that
created the update. Wren can be extended to support other
conﬂict resolution mechanisms [8], [15], [21], [23].
D. APIs
A client starts a transaction T , issues read and write (multi-
key) operations and commits T . Wren’s client API exposes
the following operations:
• ← ST ART () : starts an interactive transac-
tion T and returns T’s transaction identiﬁer TID and the causal
snapshot S visible to T.
•(cid:3)vals(cid:4) ← READ(k1, ..., kn) : reads the set of items
corresponding to the input set of keys within T .
•W RIT E((cid:3)k1, v1(cid:4), ...,(cid:3)kn, vn(cid:4)) : updates a set of given
input keys to the corresponding values within T .
2
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 


7

5

9

6

START T1
10
READ(X,Y,10)
B L O C K
10




WRITE(X2, Y2)
COMMIT T2

10
10
10
 

10


7

5

9

6

START T1
5
READ (X,Y,5)
 
10




WRITE(X2, Y2)
COMMIT T2

10
10
10

PENDING QUEUE
INSTALLED 
SNAPSHOT TIME
SNAPSHOT TIME
LOGICAL TIME
(a) Blocking reads in existing systems.
(b) Nonblocking reads in Wren.
Fig. 1: In existing systems (a), a transaction can be assigned a snapshot that has not been installed by every partition in the
local DC. c1’s transaction is assigned timestamp 10, but px has not installed snapshot 10 by the time c1 reads. This leads px to
block c1’s read. In Wren (b), c1’s transaction is assigned a timestamp corresponding to a snapshot installed by every partition
in the local DC, thus avoiding blocking. The trade-off is that older versions of x and y are returned.
•COMM IT () : ﬁnalizes the transaction T and atomically
updates the items modiﬁed by means of a WRITE operation
within T , if any.
In TCC, conﬂicting updates do not cause transactions to
abort, because they are resolved by the conﬂict resolution
mechanism. Transactions can abort by means of explicit APIs,
or because of system-related issues, e.g., not enough space
on a server to perform an update. For simplicity, we do not
consider aborts in this paper.
III. THE DESIGN OF WREN
We ﬁrst illustrate the challenge in providing nonblocking
reads, by showing how reads can block in the state-of-the-art
Cure system [8]. We then present CANToR (§ III-B), and BDT
and BiST (§ III-C). We discuss fault tolerance and availability
in Wren (§ III-D).
A. The challenge in providing nonblocking reads
For the sake of simplicity, we assume that a transaction
snapshot S is deﬁned by a logical timestamp, denoted st. We
say that a server has installed a snapshot with timestamp t
if the server has applied the modiﬁcations of all committed
transactions with timestamp up to and including t. Once a
server installs a snapshot with timestamp t, the server cannot
commit any transaction with a timestamp ≤ t.
Achieving nonblocking reads in TCC is challenging, be-
cause they have to preserve consistency and respect the atom-
icity of multi-item (and hence multi-partition) write transac-
tions. Assume that a transaction writes X and Y . A transaction
T that reads x and y, must either see both X and Y or neither
of them. The complexity of the problem is increased by the fact
that the reads on individual keys in a transactional READ may
proceed in parallel. In other words, a READ(TID, x, y) sends
in parallel a read(x) operation to px and a read(y) operation
to py, and the read(x) taking place on px is unaware of the
item returned by the read(y) on py.
3
Cure [8] provides the state-of-the-art solution to this prob-
lem. When a client c starts a transaction T , T is assigned a
causal snapshot S by a randomly chosen coordinator partition.
S includes all previous snapshots seen by c. To this end,
the coordinator sets st as the maximum between the highest
snapshot timestamp seen by c and the current clock value at
the coordinator. When T commits, it is assigned a commit
timestamp by means of a two-phase commit (2PC) protocol.
Every partition that stores an item modiﬁed by T proposes a
timestamp (strictly higher than st), and the coordinator picks
the maximum as the commit timestamp ct of T . All items
written by T are assigned ct as timestamp. Because ct > st,
all such writes carry the information that they depend on the
items in S, whose timestamps are less than or equal to st.
Cure achieves causality and enforces atomicity. If a trans-
action is assigned a snapshot timestamp st, the individual
read operations of a READ transaction can in parallel read
the version of any requested key with the highest timestamp
≤ st. This protocol, however, enforces causality and atomicity
at the cost of potentially blocking read operations. We show
this behavior by means of an example, depicted in Figure 1a.
To initiate T1, client c1 contacts a coordinator partition, in
this case pz. T1 is the ﬁrst transaction issued by c1, so c1 does
not piggyback any snapshot timestamp to initiate a transaction.
The local time on pz is 10. To maximize the freshness of the
snapshot visible to T1, pz assigns to T1 a timestamp equal to
10. In the meantime, c2 commits T2, which writes X2 and Y2.
During the 2PC, px proposes 6 as commit timestamp, i.e., the
current clock’s value on px. Similarly, py proposes 10. The
coordinator of T2, pw, picks the maximum between these two
values and assigns to T2 a commit timestamp 10. py receives
the commit message, writes Y2 and installs a snapshot with
timestamp 10. px, instead, does not immediately receive the
commit message, and its snapshot still has the value 5.
At this point, c1 issues its READ(T1, x, y) operation by
sending a request to px and py with the snapshot timestamp
of T1, which is 10. py has installed a snapshot that is fresh
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
enough, and returns Y2. Instead, px has to block the read of
T1, because px cannot determine which version of x to return.
px cannot safely return X1, because it could violate CC and
atomicity. px cannot return X2 either, because px does not
yet know the commit timestamp of X2. If X2 were eventually
to be assigned a commit timestamp > 10, then returning X2
to T1 violates CC. px can install X2 and the corresponding
snapshot only when receiving the commit message from pw.
Then, px can serve c1’s pending read with the consistent value
X2.
Similar dynamics characterize also other CC systems with
write transactions, e.g., Eiger [21].
B. Nonblocking reads in Wren
Wren implements CANToR, a novel transaction protocol
that, similarly to Cure, is based on snapshots and 2PC, but
avoids blocking reads by changing how snapshots visible to
transactions are deﬁned. In particular, a transaction snapshot
is expressed as the union of two components:
1) a fresh causal snapshot installed by every partition in
the local DC, which we call local stable snapshot, and
2) a client-side cache for writes done by the client and that
have not yet been included in the local stable snapshot.
1) Causal snapshot. Existing approaches block reads, because
the snapshot assigned to a transaction T may be “in the future”
with respect to the snapshot installed by a server from which
T reads an item. CANToR avoids blocking by providing to a
transaction a snapshot that only includes writes of transactions
that have been installed at all partitions. When using such a
snapshot, then clearly all reads can proceed without blocking.
To ensure freshness, the snapshot timestamp st provided to
a client is the largest timestamp such that all transactions with
a commit timestamp smaller than or equal to st have been
installed at all partitions. We call this timestamp the local
stable time (LST), and the snapshot that it deﬁnes the local
stable snapshot. The LST is determined by a stabilization
protocol, by which partitions within a DC gossip the latest