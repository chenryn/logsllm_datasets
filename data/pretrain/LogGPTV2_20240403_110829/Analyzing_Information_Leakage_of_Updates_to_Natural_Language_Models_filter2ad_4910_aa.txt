title:Analyzing Information Leakage of Updates to Natural Language Models
author:Santiago Zanella B&apos;eguelin and
Lukas Wutschitz and
Shruti Tople and
Victor R&quot;uhle and
Andrew Paverd and
Olga Ohrimenko and
Boris K&quot;opf and
Marc Brockschmidt
Analyzing Information Leakage of Updates
to Natural Language Models
Santiago Zanella-B√©guelin
PI:EMAIL
Microsoft
Lukas Wutschitz
PI:EMAIL
Microsoft
Shruti Tople
PI:EMAIL
Microsoft
Victor R√ºhle
PI:EMAIL
Microsoft
Andrew Paverd
PI:EMAIL
Microsoft
Olga Ohrimenko‚àó
PI:EMAIL
University of Melbourne
PI:EMAIL
Boris K√∂pf
Microsoft
ABSTRACT
To continuously improve quality and reflect changes in data, ma-
chine learning applications have to regularly retrain and update
their core models. We show that a differential analysis of language
model snapshots before and after an update can reveal a surpris-
ing amount of detailed information about changes in the training
data. We propose two new metrics‚Äîdifferential score and differential
rank‚Äîfor analyzing the leakage due to updates of natural language
models. We perform leakage analysis using these metrics across
models trained on several different datasets using different meth-
ods and configurations. We discuss the privacy implications of our
findings, propose mitigation strategies and evaluate their effect.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Software and application security;
‚Ä¢ Computing methodologies ‚Üí Machine learning; Natural
language generation.
KEYWORDS
machine learning, privacy, natural language, neural networks
ACM Reference Format:
Santiago Zanella-B√©guelin, Lukas Wutschitz, Shruti Tople, Victor R√ºhle,
Andrew Paverd, Olga Ohrimenko, Boris K√∂pf, and Marc Brockschmidt. 2020.
Analyzing Information Leakage of Updates to Natural Language Models. In
2020 ACM SIGSAC Conference on Computer and Communications Security
(CCS ‚Äô20), November 9‚Äì13, 2020, Virtual Event, USA. ACM, New York, NY,
USA, 13 pages. https://doi.org/10.1145/3372297.3417880
1 INTRODUCTION
Over the last few years, deep learning has made sufficient progress
to be integrated into intelligent, user-facing systems, which means
‚àóWork done in part while at Microsoft.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ‚Äô20, November 9‚Äì13, 2020, Virtual Event, USA
¬© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7089-9/20/11.
https://doi.org/10.1145/3372297.3417880
Marc Brockschmidt
PI:EMAIL
Microsoft
that machine learning models are now part of the software develop-
ment lifecycle. As part of this cycle, models are regularly updated
to accommodate three different scenarios:
data becomes available;
‚Ä¢ data update, to improve performance when new and more
‚Ä¢ data specialization, to fine-tune a model towards a specific
dataset, or to handle distributional shift as usage patterns
change; or
‚Ä¢ data deletion, to respect requests for removal of users‚Äô data.
Motivated by these scenarios, we study privacy implications for
text data that is added (or removed) during retraining of genera-
tive natural language models (LMs). Specifically, we consider an
adversary who obtains access to multiple snapshots of a model and
wishes to learn information about differences in the data used to
train them. This threat model is motivated by the combination of
three factors: (1) the current trend to fine-tune pretrained public
high-capacity LMs to smaller private datasets; (2) the established
ability of such LMs to memorize out-of-distribution training sam-
ples [6]; and (3) the widespread deployment of LMs to end-user
systems (e.g., predictive keyboards on smartphones), allowing ad-
versaries to analyze them in detail. For the informed reader, we
discuss the relationship between this threat model and other attacks
against privacy and defenses like differential privacy later on in
Section 2.2.
We show that data that is added or removed between model
updates can be extracted in this threat model, having severe impli-
cations for deploying machine learning models trained on private
data. Some of the implications are counter-intuitive: for example,
honoring a request to remove a user‚Äôs data (as per GDPR) from the
training corpus can mean that their data becomes exposed by re-
leasing an updated model trained without it. Similarly, fine-tuning
a public snapshot of a high-capacity model (e.g., BERT [9] or GPT-
2 [23]) with data from a single organization exposes this additional
data to anyone who obtains access to both the fine-tuned model
and the original public model (e.g., employees of this organization).
In order to extract information about the difference in the data
used to train two language models, we develop a novel notion of
differential score. The differential score of a token sequence captures
the difference between the probabilities assigned to it by the two
models. The intuition is that sequences with higher differential
CCS ‚Äô20, November 9‚Äì13, 2020, Virtual Event, USA
Zanella-B√©guelin, Wutschitz, Tople, R√ºhle, Paverd, Ohrimenko, K√∂pf, and Brockschmidt
scores are likely to have been added during model updates. We
devise an algorithm based on beam search to efficiently identify
such token sequences, even if the individual models assign low
probability to them. This allows us to recover information about
the difference between the datasets used for training without any
background knowledge of their contents or distribution.
When given some background knowledge, the advantage of
having access to two model snapshots becomes crisper. For ex-
ample, we train a recurrent neural network (RNN) on 20M to-
kens of general Reddit comments, and update it by retraining it
on these comments plus 25K tokens from 940 messages of the
talk.politics.mideast newsgroup. When prompted with the
word ‚ÄúTurkey‚Äù, our algorithm produces ‚ÄúTurkey searched an Amer-
ican plane‚Äù as the 2nd most likely result, although this phrase occurs
only 6 times in newsgroup messages and none in Reddit comments
(i.e., < 0.000002% of the training data). An equivalent search using
only the updated network does not produce this sentence among
the top 10,000 results; it would take the longer prompt ‚ÄúTurkey
searched an‚Äù for this phrase to surface to the top 100 results.
We use differential score to experimentally study the effect of
updates in the three scenarios mentioned above. As a proxy for the
update dataset, we use synthetically generated sentences (or ca-
naries) and real-world sentences from newsgroup messages. Using
both canaries and real-world data, we analyze the effect on attacks
recovering information from the update dataset of (1) different train-
ing types for updates, ranging from retraining a model from scratch
with an updated dataset to fine-tuning as is common for modern
high-capacity language models; (2) the proportion of private and
public data used for the update; and (3) an adversary‚Äôs background
knowledge. For robustness, we consider datasets of different sizes
on both RNNs as well as modern transformer architectures.
Summary of Contributions. We present the first systematic study
of the privacy implications of releasing snapshots of language mod-
els trained on overlapping data. Our results validate that model
updates pose a substantial risk to content added to or removed from
training data in terms of information leakage. Our key findings are:
‚Ä¢ By comparing two models, an adversary can extract specific
sentences or fragments of discourse from the difference between
the data used to train them. This does not require any information
about the training data or the model architecture and is possible
even when the change to the data is as small as 0.0001% of the orig-
inal dataset. Smaller changes become exposed when given partial
knowledge about the data.
‚Ä¢ We show that analyzing two model snapshots reveals sub-
stantially more about the data that was added or removed than
considering only a single snapshot at a time, as in [6].
‚Ä¢ Adding or removing additional non-sensitive training data
between model updates is not a reliable method to hide data that
should be kept private.
‚Ä¢ Training with differential privacy mitigates the attack, but
incurs substantial computational cost and reduces the utility of the
trained models.
‚Ä¢ Restricting access to the model and only outputting a subset
of prediction results is a promising mitigation as it reduces the
effectiveness of our attack without reducing utility of the model.
These findings apply to models fine-tuned on a smaller dataset, as
well as models retrained on the union of original and new data.
Structure of the Paper. We provide background on language mod-
els and describe our adversary model and attack scenarios in the
next section. We define the notion of differential score and describe
how to efficiently approximate it in Section 3. In Section 4 we de-
scribe our experiments to analyze the effect of different factors on
leakage. In Section 5 we investigate the source of leakage in model
updates, e.g., by comparing with leakage from access to only a
single model. Finally, we consider mitigation strategies in Section 6,
before describing related work and concluding.
2 PRELIMINARIES
2.1 Generative Language Models
We consider machine learning models capable of generating natural
language. These models are used in a variety of applications, includ-
ing automatic caption generation, language translation, and next-
word prediction. Generative language models usually operate on a
fixed set of known tokens ùëá (often referred to as the model‚Äôs vocabu-
lary) and are autoregressive, modeling the probability ùëù(ùë°1 . . . ùë°ùëõ) of
a sequence of tokens ùë°1 . . . ùë°ùëõ ‚àà ùëá ùëõ as the product of the per-token
probabilities conditional on their prefix ùëù(ùë°ùëñ | ùë°1 . . . ùë°ùëñ‚àí1), i.e.,

1‚â§ùëñ‚â§ùëõ
ùëù(ùë°1 . . . ùë°ùëõ) =
ùëù(ùë°ùëñ | ùë°1 . . . ùë°ùëñ‚àí1) .
Training an autoregressive generative language model ùëÄ requires
learning a function (which we also refer to as ùëÄ) that maps token
sequences of arbitrary length to a probability distribution over the
vocabulary ùëá , modeling the likelihood of each token to appear next.
We use ùëÄ(ùë°<ùëñ) to denote the probability distribution over tokens
computed by model ùëÄ after reading the sequence ùë°1 . . . ùë°ùëñ‚àí1 ‚àà ùëá‚àó,
and ùëÄ(ùë°<ùëñ)(ùë°ùëñ) to denote the probability of a specific token ùë°ùëñ.
Given such a model ùëÄ, a simple predictive screen keyboard can
be implemented by feeding ùëÄ the words typed so far (e.g., from the
start of the current sentence) and displaying the, say, three most
likely tokens as one-tap options to the user.
A variety of different architectures exist for the generation of
natural language using machine learning models. The most promi-
nent are Recurrent Neural Networks (RNNs) using Long Short-Term
Memory [17] cells (or variants thereof) and the more recent Trans-
formers [23, 30]. These architectures differ substantially in how
they implement the modeling of the per-token probability distribu-
tion, but as our experiments show, they behave nearly identically
for the purposes of our analysis.
Given a model architecture, a dataset ùê∑ ‚äÜ ùëá‚àó is required as
training data to obtain a concrete model. We write ùëÄùê∑ to em-
phasize that a model was trained on a dataset ùê∑. Throughout the
paper, we use the standard measure of perplexity perpùëÄ(ùë°1 . . . ùë°ùëõ) =
ùëùùëÄ(ùë°1 . . . ùë°ùëõ) ‚àí1
ùëõ of a model ùëÄ on test data ùë°1 . . . ùë°ùëõ, using the prob-
ability ùëùùëÄ(ùë°1 . . . ùë°ùëõ) assigned to the sequence by model ùëÄ. Unlike
the more familiar accuracy, which only captures the correctness of
the most probable choice, this metric captures models being ‚Äúalmost
right.‚Äù Intuitively, perplexity can be thought as how ‚Äúsurprised‚Äù a
model is by a next-word choice, and hence, lower perplexity values
indicate a better match between data and model.
Analyzing Information Leakage of Updates to Natural Language Models
CCS ‚Äô20, November 9‚Äì13, 2020, Virtual Event, USA
2.2 Adversary Model and Goals
Language models are regularly updated for a variety of reasons,
either by adding and/or removing data from the training set. We use
the term model update to refer to any update in the parameters of
the model caused by training on different data. This is distinct from
an update to the model architecture, which changes the number or
use of parameters. Each update creates a new version of the model,
which we refer to as a snapshot.
We consider an adversary that has concurrent query access
to two snapshots, ùëÄùê∑ and ùëÄùê∑‚Ä≤, of a language model trained on
datasets ùê∑ and ùê∑‚Ä≤ respectively, where ùê∑ ‚ää ùê∑‚Ä≤. We write ùëÄ, ùëÄ‚Ä≤ as
shorthand for ùëÄùê∑, ùëÄùê∑‚Ä≤. The adversary can query the snapshots
with any sequence ùë† ‚àà ùëá‚àó and observe the corresponding probabil-
ity distributions ùëÄ(ùë†) and ùëÄ‚Ä≤(ùë†). The adversary‚Äôs goal is to infer
information about training data points in ùê∑‚Ä≤ \ ùê∑, the difference
between ùê∑ and ùê∑‚Ä≤. In the best case, an adversary would recover
exact training points. We refer to an adversary who has access to
two snapshots of the model as a snapshot attacker.
Relationship to other attacks on training data. Snapshot attacks
are reconstruction attacks [24] against the updated model, as the
goal is to recover data points in the dataset used for the update,
given the original model as auxiliary information.
The goal of membership inference attacks [25, 26] is weaker in
that they only aim to determine whether a given point was present
in the dataset used to train a model. However, the differential score
of a phrase (which we use for reconstruction) can also serve as a
signal for inferring membership in the update dataset. We leave an
evaluation of this approach to future work.
Finally, model inversion attacks [12, 13] repurpose a model to
work backwards, inferring unknown attributes of individuals given
known attributes and a target prediction. Individuals need not
be present in the training data, and results are aggregate statistics
rather than information about specific training points. See Section 7
for a more in-depth discussion of related attacks.
Relationship to differential privacy. Differential privacy [11] guar-
antees that a model does not leak significant information about any
specific training point. A differentially private model also guaran-
tees group privacy, with a bound on the contribution of a group
of training points that degrades linearly with the group size. A
differentially private model that provides meaningful protection
for a group of |ùê∑‚Ä≤ \ ùê∑| training points would hence protect against
snapshot attacks on ùëÄùê∑, ùëÄùê∑‚Ä≤. However, this also implies that ùëÄùê∑‚Ä≤
cannot be significantly more useful (e.g. more accurate) than ùëÄùê∑.
Our experiments in Section 6 confirm this intuition, and show that
a large privacy budget is needed for the updated model to gain in
utility, so that in practice differential privacy provides an empirical
mitigation rather than a strong formal guarantee.
2.3 Analysis Scenarios
To guide our analysis, we focus on three concrete scenarios in which
an adversary can gain concurrent access to two (or more) snapshots
of a language model.
Data Updates. Many applications require language models that
reflect recent patterns in language use. For example, a predictive
keyboard on a mobile device requires regular updates to suggest
terms that have become more common recently (e.g., following
news trends or internet memes). To achieve this, vendors often
regularly retrain an (otherwise unchanged) model on an updated
dataset, for example by simply adding more recent data to the