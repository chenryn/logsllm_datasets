5With a common DecPerm operation.
6We assume the input size uwuh is smaller that the ciphertext size n.
8
M1M3M2M4M6M5M7M9M8F1F3F2F4F6F5F7F9F8KM1M2M3M4M5M6M7M8M9F5M2M3M4M5M6M7M8M9M1M3M4M5M6M7M8M9M1M2F5F5F5F5F5F5F5F5M4M5M6M7M8M9M1M2M3F8F8F8F8F8F8000M7M8M9M1M2M3M4M5M6000F2F2F2F2F2F2F6F60F6F60F6F60M5M6M7M8M9M1M2M3M4F9F90F9F90000M8M9M1M2M3M4M5M6M7000F3F30F3F300F7F70F7F7000M6M7M8M9M1M2M3M4M50000F1F10F1F1M9M1M2M3M4M5M6M7M80F4F40F4F40F4F4M1M2M3M4M5M6M7M8M9F5M2M3M4M5M6M7M8M9M1M3M4M5M6M7M8M9M1M2F5F5F5F5F5F5F5F5M4M5M6M7M8M9M1M2M3F8F8F8F8F8F8000M7M8M9M1M2M3M4M5M6000F2F2F2F2F2F2F6F60F6F60F6F60M5M6M7M8M9M1M2M3M4F9F90F9F90000M8M9M1M2M3M4M5M6M7000F3F30F3F300F7F70F7F7000M6M7M8M9M1M2M3M4M50000F1F10F1F1M9M1M2M3M4M5M6M7M80F4F40F4F40F4F401234-4-3-2-1Step (a)f0f1f2f3f4f-4f-3f-2f-1Step (b)Step (c)01234-4-3-2-1f0f3f-3f1f4f-2f2f-4f-1[x]cC1C3K11K21K12K22K1[x1]cC2C4K13K14K23K24K31K41K32K42K33K34K43K44K2K3K4K11K22K21K12K13K24K14K23K31K42K41K13K32K24K33K14K44K23K34K43Step (a)K11K22K21K12C1C2C1C2C1K11C2K22C1K21C2K12C2K12C1K21PermC3K13C4K24C3K23C4K14C4K14C3K23PermC1K31C2K42C1K41C2K32C2K32C1K41PermC3K33C4K44C3K43C4K34C4K34C3K43PermStep (b)K13K24C3C4K14K23K31K42K41K32K33K44K34K43C3C4[x0]cof Figure 8).
This kernel grouping calculation results in (cn − 1) Perm
newly formed blocks, which re-
operations for each of co
cn
duces the Perm complexity by a factor of ci
compared with
cn
GAZELLE’s MIMO convolution. This reduction is nontrivial
especially for the state-of-the-art neural networks such as
ResNets [32], where ci
can be 256. This is because these
cn
neural networks contain a large number of large-size feature
maps in order to capture the complex input features [62], [40],
[32].
Similar to the output rotation based MIMO convolution
discussed above, there are co
output ciphertext in the proposed
cn
scheme. For each of the co
newly formed blocks, there are ci
cn
SISO-like convolutions. Then for each of the cn kernel orders,
there are ci
convolutions to be summed up, which results in
cn
cn added convolutions. These added convolutions are further
rotated to the same kernel order and summed up to get the
ﬁnal convolution. Therefore, the proposed MIMO convolution
(kwkh − 1) HstPerm,
requires a total of co
cn
kwkh
(cikwkh − 1) Add operations.
(cn − 1) Perm, ci
ScMult, and co
cn
cico
cn
cn
Table III compares the overall complexity for convolution
computations. GALA’s kernel grouping approach reduces the
without increasing
expensive Perm operations by a factor of ci
co
other operations compared with the output rotation based
MIMO convolution (i.e.,
the GAZELLE framework). The
reduction in Perm operations leads to a signiﬁcant speedup.
Speciﬁcally, GALA shows about 14× speedup compared with
GAZELLE in the convolution between input data with a size
of 16×16 with 2048 channels, and 512 kernels with a size of
1×1@2048 on a commodity machine (see detailed benchmarks
in Sec. IV).
TABLE III.
COMPLEXITY COMPARISON OF CONVOLUTION.
Method
# Perm
GAZELLE
GALA
cico (cn−1)
co (cn−1)
c2
n
cn
# HstPerm(cid:93)
ci(kw kh−1)
ci(kw kh−1)
cn
cn
# ScMult
cico kw kh
cn
cico kw kh
cn
# Add
co (cikw kh−1)
co (cikw kh−1)
cn
cn
(cid:93)Rotations of the input with ci
cn
common DecPerm operations.
C. Noise Management
The packed HE (e.g., the BFV scheme) introduces noise in
the ciphertext which theoretically hides the original message
[38], [13]. However, the noise management is critical to the
correct decryption of ciphertext after a series of HE opera-
tions. We will show that GALA has better noise management
compared with GAZELLE.
Based on the computation complexity of matrix-vector
multiplication and convolution, along with the noise change
for HE operations as described in Sec. II-C, Table IV shows
the noise growth of different schemes. As for the matrix-
vector multiplication, GALA has a lower noise growth while
keeping the number of output ciphertext as small as one7.
As for the convolution computation, GALA reduces the noise
n −1) ≥ 0.
n −1) <
The noise of GALA is still lower than that of GAZELLE when ( nino
0 as it means one ciphertext can hold data with size no × ni, which
only involves one ScMult operation in GALA, and GAZELLE needs to
subsequently conduct a series of RaS operations.
7Note that the noise in Table IV is calculated by assuming ( nino
Fig. 8. Kernel grouping based MIMO convolution.
column of the kernel block (see the four groups of vectors in
step (a) of Figure 7). In this way, each input ciphertext can
directly convolve with the vectors in each divided block by
SISO, and the convolution for each divided block is obtained
by rotating the cn convolved vectors to the same kernel order
as the diagonal one and summing them up (see step (b)).
Finally, the convolution for cn kernels is calculated by
blocks associated with the same
adding the convolution of ci
cn
kernels as illustrated in step (b) of Figure 7.
cn
cico
cn
ScMult and co
cn
Clearly, there are co
cn
(cn − 1) Perm, ci
(cikwkh − 1) Add operations.
output ciphertext, as expected. For
each of the coci
blocks, there are total cn SISO-like convo-
c2
lutions, requiring cnkwkh ScMult operations, (cn − 1) Perm
n
operations and (cnkwkh − 1) Add operations. Next, there are
block convolutions which are associated with the same
ci
cn
kernel order. Thus they are added up to obtain the ﬁnal con-
volution result. Meanwhile, the rotation group for each input
ciphertext is reused to convolve with different kernel blocks.
Thus there are total ci(kwkh−1)
HstPerm operations with ci
cn
common DecPerm operations. In all, the MIMO convolution
(kwkh − 1) HstPerm,
needs a total of cico
c2
n
kwkh
3) Kernel Grouping Based MIMO convolution (GALA):
One key observation on the above MIMO convolution is
blocks needs (cn − 1) expensive Perm
that, each of the coci
c2
n
operations in order to get
the convolution for that block.
However, we actually do not need to get the convolution for
each block. As our goal is to get the convolution for each
kernel, the blocks that are associated with the same kernel
are combined in our proposed ﬁrst-Add-second-Perm approach
(kernel grouping) to reduce the Perm cost. Speciﬁcally, in step
(a) of Figure 8, the whole kernel block is divided into two
blocks K1 and K2 such that each block is the combination
cn-by-cn divided blocks, which correspond to the same
of ci
cn
kernels (i.e., the ﬁrst and second kernel in K1 and the third
and fourth kernel in K2).
cn
For each newly formed block, all of the vectors are ﬁrst
convolved with the corresponding input ciphertext by SISO-
like convolution. Then the convolved vectors that are associ-
ated with the same kernel order are ﬁrst added together (see
the addition of convolved vectors before rotation in step (b) of
Figure 8). Finally, these added vectors are rotated to the same
kernel order and summed up to obtain the convolution result
(see the rotation and ﬁnal addition for each block in step (b)
9
C1C3K11K21K12K22K1C2C4K13K14K23K24K31K41K32K42K33K34K43K44K2K11K22K21K12K13K24K14K23K31K42K41K13K32K24K33K14K44K23K34K43Step (a)K11K22K21K12C1C2C1C2C1K11C2K22C1K21C2K12C3K13C4K24C3K23C4K14PermC1K31C2K42C1K41C2K32C3K33C4K44C3K43C4K34Step (b)K13K24C3C4K14K23K31K42K41K32K33K44K34K43C3C4C1K11+C3K13C2K22+C4K24C3K23+C1K21C4K14+C2K12C3K23+C1K21C4K14+C2K12C3K33+C1K31C4K44+C2K42PermC1K41+C3K43C2K32+C4K34C1K41+C3K43C2K32+C4K34[x1]c[x0]cterm associated with rotation by a factor of ci
compared to
cn
GAZELLE. This is nontrivial especially for state-of-the-art
neural networks such as ResNets [32], where ci
can be 256.
cn
The number of output ciphertext is also maintained as small as
. Overall, GALA features a lower noise growth and lower
co
cn
computation complexity compared with GAZELLE.
TABLE IV.
COMPARISON OF NOISE MANAGEMENT.
computation. 3) The ciphertext modulus q is chosen to be
a 60-bit pseudo-Mersenne prime that is slightly smaller than
the native machine word on a 64-bit machine to enable lazy
modular reductions. 4) The selection of the number of slots
is the smallest power of two that allows for a 128-bit security
which in our case is n = 2048. We refer readers to [38] for
more details about the parameter selection.
Method
Naive
GAZELLE
Matrix-vector Multiplication
Noise after computation
niη0ηmult + (ni − 1)ηrot
niη0ηmult + [ nino−n
no
ηmult + n−no
n − 1)ηrot
no
GALA
Method
n η0ηmult + ( nino
nino
Convolution Computation
Noise after computation
(cn − 1)ηrot
ciη∆ + ci
ciη∆ + (cn − 1)ηrot
cn
GALA
η∆ = kwkhηmultη0 + (kwkh − 1)ηrotηmult
GAZELLE
]ηrot
# Cipher
no
1
1
# Cipher
co
cn
co
cn
D. System Security
GALA is based on the same security framework as
GAZELLE [38]. The security of linear computation in GALA
is fully protected by the security of HE (e.g.,
the BFV
scheme [13], [23]). The nonlinear computation (which is
not
the focus of this paper) is protected by Garbled Cir-
cuits (GC) [72] or its alternatives. The security of GC-based
nonlinear computation has been proven in TASTY [33] and
MP2ML [12].
IV. EVALUATION
We conduct the experiments in both LAN and WAN set-
tings. The LAN setting is implemented on a Gigabit Ethernet
in our lab between two workstations as the client and server,
respectively. Both machines run Ubuntu, and have an Intel i7-
8700 3.2GHz CPU with 12 threads and 16 GB RAM. The
WAN setting is based on a connection between a local PC and
an Amazon AWS server with an average bandwidth 200Mbps
and round-trip time around 13ms. We have downloaded the
codes released by GAZELLE8, DELPHI9 and CrypTFlow210,
and run all experiments on the same hardware devices and
network settings. We conduct a series of experiments under
various neural network architectures. In each experiment, we
ﬁrst run the baseline algorithm (i.e., GAZELLE, DELPHI or
CrypTFlow2) to obtain the baseline total runtime (including
online runtime and ofﬂine runtime), and then replace the linear
computation of the baseline algorithm by GALA to get a new
total runtime, which is then used to compute the speedup.
While the codes for GAZELLE, DELPHI and CrypTFlow2
are implemented in different ways (for example, GAZELLE is
based on its crypto platform while DELPHI and CrypTFlow2
are based on the Microsoft SEAL library), we focus on
the speedup of GALA on top of each of them. We also
set the cryptographic parameters in line with GAZELLE: 1)
Parameters for both HE and GC schemes are selected for a
128-bit security level. 2) A plaintext modulus p of 20 bits
is enough to store all the intermediate values in the network
8Available at https://github.com/chiraag/gazelle mpc
9Available at https://github.com/mc2-project/delphi
10Available at https://github.com/mpc-msri/EzPC/tree/master/SCI
10
A. Microbenchmarks
In this section, we benchmark and compare the runtime of
GALA’s linear optimization (i.e., matrix-vector multiplication
and convolution computation) with state-of-the-art approaches.
We claim the same communication cost and inference accuracy
with GAZELLE and achieve improved computation efﬁciency.
1) Matrix-Vector Multiplication: Table V compares the
computation complexity of GALA’s matrix-vector optimization
with GAZELLE and two other optimization schemes (i.e.,
a diagonal method (Diagonal) [31] and an extended method
(Extended) [17]). We can see that GALA largely reduces
the expensive Perm operation to zero in our cases (including
the HstPerm) while GAZELLE needs up to 11 Perm and
Extended [17] needs up to 520 Perm (including HstPerm).
On the other hand, GALA also maintains a light overhead
for HE multiplication/addition, i.e., only one multiplication,
compared with other three optimizations, e.g., Diagonal [31]
and Extended [17] involve up to 2048 multiplication/addtion.
The runtime results for matrix-vector multiplication are
summarized in Table VI, which includes the original runtime
of GAZELLE, DELPHI and CrypTFlow2, and the speedup
of GALA on top of each. We take the share-RaS calculation
cost (see the plaintext computing for ﬁnal share at the client
in step (d) of Figure 5) as part of the runtime cost of
GALA for fair comparison. Meanwhile, as multiple copies are
packed in one ciphertext, the HstPerm operation includes a
common DecPerm to enable hoist optimization for rotation
(see the details in [38]). As can be seen from Table VI,
GALA’s optimization gains a large speedup due to the row-
encoding-share-RaS module, which reduces the costly Perm,
Mult, and Add operations for a series of RaS calculation.
Speciﬁcally, GALA achieves the speedup of 1795×, 208× and
57× over the Diagonal [31] under different matrix dimensions
in the LAN setting. This beneﬁt stems from the fact that
the computation complexity of the Diagonal
is related to
the input dimension ni, which is always large in the state-
of-the-art neural networks such as AlexNet [40], VGG [62]
and ResNet [32]. For a similar reason, GALA signiﬁcantly
outperforms the Extended method [17].
Meanwhile, GALA has a speedup of 59×, 13× and 19×
over GAZELLE under different matrix dimensions in the LAN
setting. This computation gain comes from the HstPerm-free