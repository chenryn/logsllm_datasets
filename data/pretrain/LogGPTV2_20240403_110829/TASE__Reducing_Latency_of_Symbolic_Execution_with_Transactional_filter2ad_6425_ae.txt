7https://github.com/coreutils/coreutils/blob/master/src/tsort.c
8https://github.com/coreutils/coreutils/blob/master/src/factor.c
Fig. 3: Average time to add two 50KB integers vs. amount
of symbolic computation, for TASE (•), KLEE ((cid:78)), S2E ((cid:4)),
SymCC (×), and QSYM (+). Each point is an average over
ﬁve runs; relative standard deviation was  95% of the BigNum addition was concrete. Even with
no symbolic computation whatsoever (i.e., with 0% symbolic
input in Fig. 3), QSYM was at least 7× slower than both S2E
and TASE, and only about 25% faster than KLEE, although
QSYM outperformed S2E considerably after more than 4%
of the total computation in the microbenchmark required
the manipulation of symbolic data. Both TASE and SymCC
performed similarly in Fig. 3.
Based on the concrete overheads in Fig. 3 and Table I, and
the Pin instrumentation code from the QSYM repository, we
believe the slowdown on concrete workloads within QSYM
is primarily due to its insertion of instrumentation func-
tions before each machine instruction of the target. Although
more lightweight
than the Pin instrumentation in QSYM,
SymCC’s compile-time instrumentation resulted in the inser-
tion of checks within the IR for some of our target programs,
increasing the total size of the bitcode (e.g., increasing the total
number of instructions in the IR for the BigNum microbench-
mark by approximately 3×).
9
0.000.250.500.751.000255075100Percentage of Symbolic ComputationTime(s)B. Client Veriﬁcation with TASE
The second evaluation for TASE that we report is its use in
verifying client behavior in client-server protocols. Numerous
server exploits take the form of messages to a server that
no legitimate or sanctioned client would send. To detect such
messages in this approach, a server-side veriﬁer symbolically
executes the claimed client software (with unknown client-side
inputs marked symbolic) to determine whether the message
sequence received from the client is possible given the client
software and messages sent to it (e.g., [5], [19]).
This defensive strategy beneﬁts from its generality—it
needs no foreknowledge of a vulnerability or exploit to detect
an attack—and soundness, in the sense that if it accepts a
sequence of messages, then there are inputs that could have
caused the claimed client to produce that sequence. However,
it also has limitations that restrict the contexts in which it can
be applied. First, it requires a claim of the software executed
by the client. This claim could be explicit via a user-agent
string (as in HTTP, SIP, or NNTP) or an attested load-time
measurement from a hardware trusted-computing platform
(see Maene et al. [33] for a survey) or host-based monitor.
Alternatively, this claim could be inferred from the client’s
behavior (e.g., [1]) or simply because the client was previously
provisioned with software known to the veriﬁer (e.g., as an IoT
device might be). Second, the veriﬁer must be able to obtain
the claimed client software to symbolically execute—source
code if the symbolic execution engine requires it.
The primary focus of this evaluation is a third challenge
faced by this defensive approach: to be used as an inline
defense, it requires symbolic execution to keep pace with
the arrival of messages from the client. Thus, the latency
of symbolic execution is critical in determining whether this
defense can be used inline to prevent exploits, or whether
it can only be run alongside server processing and thereby
detect exploits shortly after they occur. We show not only that
TASE signiﬁcantly improves performance over a state-of-the-
art codebase for conducting behavioral veriﬁcation of a TLS
client, but that it does so to an extent that permits this defense
to reside on the critical path of message processing for all but
the most latency-sensitive TLS applications.
The speciﬁc codebase to which we compare here is that due
to Chi et al. [16], who instantiated this general approach for
TLS. To determine whether or not a client message could have
originated from an unmodiﬁed client TLS implementation, Chi
et al. detail a technique for symbolically executing OpenSSL’s
s_client and then solving to determine whether there
exist inputs that could have caused that implementation to
produce the message sequence received. A message sequence
for which no inputs can be found to produce it indicates that
the message sequence is inconsistent with s_client and so
might represent an exploit, and indeed, Chi et al. observed that
numerous notable TLS exploits (e.g., Heartbleed, CVE-2014-
0160) are of a form that would be caught by this technique.
The Chi et al. framework is an extension of similar tools
(e.g., [5], [19]) adapted speciﬁcally for cryptographic protocols
like TLS: it leverages knowledge of the TLS session key and
symbolically executes the client in multiple passes, skipping
speciﬁed prohibitive functions (the AES block cipher and hash
functions) until constraints generated from observed client-to-
server messages could fully concretize their inputs. Below, we
refer to the tool built by Chi et al. as CliVer (for simply “client
veriﬁcation”).
The only changes we made to the CliVer tool for this
evaluation was to implement the following two optimizations
for it, to make the comparison to TASE fairer since TASE
incorporates analogous optimizations. First, we changed how
CliVer models the select system call, so that its return
value indicates that stdout is always available (versus being
symbolic). s_client writes the application payload received
from the server to stdout, and so blocks if stdout is
unavailable. As such, this change has no effect on the message
sequence that could be received from s_client; i.e., any
message sequence received in an execution where stdout
becomes unavailable is a preﬁx of a sequence that could be
received in an execution where it remains available throughout.
This change does, however, relieve CliVer from needing to
explore the execution path in s_client where stdout is
unavailable, saving it the expense of doing so.
Second, when CliVer is seeking to verify message i from
the client and reaches a send point when symbolically execut-
ing s_client, it must create and solve constraints reﬂecting
message i and the path executed to reach that send point.
This produces an unusually large number of relatively simple
equality constraints (i.e., one constraint per each byte of the
message), many of which contain a large number of XOR
operations due to the choice of cipher suite. To more efﬁciently
move the constraint information between the interpreter and its
solver, we alter the behavior of KLEE’s independent constraint
solver to send all constraints en masse rather than one-by-
one. Moreover, though the SAT solver we use supports XOR
expressions [48], we found it much more efﬁcient to rewrite
these expressions to remove XORs before sending them to the
solver. This optimization improves the performance of CliVer
considerably, and we leverage it in TASE, as well.
1) Experiment setup: Our evaluation used the same TLS
1.2 dataset used by Chi et al. [16]. This dataset includes
benign trafﬁc captured by tcpdump during a Gmail browsing
session, and maliciously crafted Heartbleed packets to simulate
CVE-2014-0160. The Gmail data set was generated by sending
and receiving emails with attachments in Firefox over a span
of approximately 3 minutes, and included 21 independent,
concurrent TLS sessions for a total of 3.8MB of data. For
reference, a plot of the time during which each of the 21 TLS
connections was active is shown in Fig. 4. As shown there, a
large majority of connections were active for nearly the entire
duration of the Gmail session, though a few were much shorter.
We compared TASE with CliVer in two conﬁgurations. The
ﬁrst presumes minimal protocol-speciﬁc knowledge or thus
adaptation by the party deploying the veriﬁer. In this basic
conﬁguration, each tool was provided a speciﬁcation of the
same prohibitive functions, but otherwise the tool operated
on the Gmail trace unmodiﬁed. Even in this conﬁguration,
however, we provided CliVer with native implementations of
these prohibitive functions, so that even once their inputs had
been concretized, they would be executed natively (versus
being interpreted), thus rendering our comparison conservative.
The second, optimized conﬁguration incorporated a range of
protocol-speciﬁc optimizations. In particular, after the TLS 1.2
handshake, client-to-server and server-to-client messages are
independent of one another, and so server-to-client messages
10
Conﬁguration
System
Average cost
Median cost
Max cost
Average lag
Median lag
Max lag
basic
CliVer
0.033s
0.006s
2.116s
4.654s
4.721s
17.885s
TASE
0.005s
0.003s
0.072s
0.614s
0.188s
2.648s
optimized
TASE
0.016s
0.013s
0.088s
0.106s
0.045s
0.876s
CliVer
0.071s
0.018s
1.274s
1.138s
1.195s
4.211s
Fig. 4: The durations of the 21 TLS connections involved
in the Gmail trace described in Sec. VI-B, ordered bottom-
to-top according to the TLS connection initiation. Note that
some connections are so brief that their beginning and ending
markers overlap.
were ignored when verifying the client-to-server messages. In
addition, certiﬁcate veriﬁcation was elided, since the veriﬁer,
being deployed to protect the server, trusts the server to send
a valid certiﬁcate chain.
2) Results: When used as an inline defense against mali-
cious trafﬁc, the speed to reach a true detection is arguably a
secondary concern; the delay imposed on attack trafﬁc might
be viewed more as a beneﬁt than a detriment. Nevertheless, we
used synthetic Heartbleed packets to conﬁrm that TASE could
determine these packets were not consistent with the OpenSSL
TLS client in only 150ms from the initiation of the connection
(i.e., including the TLS handshake).
More critical is the delay that TASE would impose on le-
gitimate trafﬁc. Here we report the cost and lag of veriﬁcation
as deﬁned by Chi et al. For the i-th message in a TLS session,
cost(i) is the processing time required to verify message i
beginning from the symbolic state produced from verifying
through message i− 1. lag(i) is the delay between the receipt
of message i and when its veriﬁcation completes. Note that
lag(i) ≥ cost(i), and lag(i) > cost(i) if when message i is
received by the veriﬁer, the veriﬁcation of message i− 1 is not
yet complete (and so verifying message i cannot yet begin).
Table II gives coarse statistics for the cost and lag of
verifying all 21 TLS sessions in the Gmail trace, in the basic
(left) and optimized (right) conﬁgurations. Interestingly, the
median costs for TASE and CliVer were very similar, but
the median lag for CliVer was 27× larger (in the optimized
conﬁguration) than it was for TASE. The cause was the
messages that were most costly to verify, with costs in CliVer
more than 14× that in TASE in the optimized conﬁguration
(and roughly 29× that in TASE in the basic conﬁguration).
These greater costs caused the lag to accumulate at various
points in the trace, inducing an average CliVer lag on the
optimized conﬁguration of > 1s and a maximum lag of > 4s.
In contrast, the TASE lag averaged only ≈ 0.1s and incurred
a maximum lag for any message of ≈ .9s. For a driving
application like Gmail that is paced by human activity, these
lags may well be small enough to support the use of TASE as
an inline defense.
TABLE II: Statistics for veriﬁcation of benign Gmail traces