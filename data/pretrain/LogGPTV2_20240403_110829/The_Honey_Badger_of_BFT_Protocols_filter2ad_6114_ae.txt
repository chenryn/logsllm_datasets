### Experimental Setup and Observations

For our simulated network, we configured the machine to listen on \(N\) hidden services, with one hidden service for each HoneyBadgerBFT node. Since each HoneyBadgerBFT node establishes a connection with every other node, this setup results in a total of \(N^2\) Tor circuits per experiment. Each circuit begins and ends at our machine, passing through 5 randomly selected relays. In summary, all pairwise overlay links traverse real Tor circuits, as detailed in the metrics provided by the Tor Project as of November 10, 2015.

### Throughput and Latency Analysis

**Figure 7: Latency vs. Throughput for Wide Area Network Experiments**

- **Findings**: 
  - From Figure 6, it is evident that throughput increases as the number of proposed transactions grows.
  - For medium-sized networks (up to 40 nodes), we achieved a throughput exceeding 20,000 transactions per second.
  - For a large network of 104 nodes, the throughput was more than 1,500 transactions per second.
  - With an infinite batch size, all network sizes would converge to a common upper bound, limited only by available bandwidth.
  - Although the total bandwidth consumption increases linearly with each additional node, these nodes also contribute additional bandwidth capacity.

**Latency and Throughput Trade-offs**:
- **Latency Definition**: Latency is defined as the time interval between when the first node receives a client request and when the \((N - f)\)-th node completes the consensus protocol. This definition is reasonable because the completion of the protocol by the \((N - f)\)-th node implies that consensus has been achieved among the honest parties.
- **Observations from Figure 7**:
  - The positive slopes in Figure 7 indicate that our experiments have not yet fully saturated the available bandwidth, suggesting that even larger batch sizes could yield better throughput.
  - As the number of nodes increases, latency also increases, primarily due to the ABA phase of the protocol.
  - At \(N = 104\), our system becomes CPU-bound rather than bandwidth-bound, as our implementation is single-threaded and must verify \(O(N^2)\) threshold signatures. Despite this, our largest experiment with 104 nodes completed in under 6 minutes.
  - Adding more nodes (with equal bandwidth provisioning) does not affect the maximum attainable throughput, but the minimal bandwidth required to commit one batch (and thus the latency) increases with \(O(N^2 \log N)\). This constraint imposes a limit on scalability, depending on the cost of bandwidth and users' tolerance for latency.

### Comparison with PBFT

**Figure 8: Comparison with PBFT Protocol**
- **PBFT Overview**: PBFT is a classic Byzantine Fault Tolerant (BFT) protocol for partially synchronous networks. We used the Python implementation from Croman et al. [24], running on 8, 16, 32, and 64 nodes distributed across Amazon AWS regions. Batch sizes were chosen to saturate the network's available bandwidth.
- **Key Differences**:
  - While PBFT and our protocol have the same asymptotic communication complexity, our protocol demonstrates different performance characteristics in practice.

### Conclusion

We have presented HoneyBadgerBFT, the first efficient and high-throughput asynchronous BFT protocol. Our implementation and experimental results demonstrate that HoneyBadgerBFT can be a suitable component in cryptocurrency-inspired deployments of fault-tolerant transaction processing systems. More broadly, our work highlights the potential of building dependable and scalable transaction processing systems based on asynchronous protocols.

### Acknowledgements

We thank Jay Lorch, Jonathan Katz, and Emin GÃ¼n Sirer for their valuable suggestions, and especially Dominic Williams for inspiring discussions. This work is supported in part by NSF grants CNS-1314857, CNS-1453634, CNS-1518765, CNS-1514261, and CNS-1518899, DARPA grant N66001-15-C-4066, a Packard Fellowship, a Sloan Fellowship, two Google Faculty Research Awards, and a VMWare Research Award. This work was conducted in part while a subset of the authors were visiting students at UC Berkeley and the Simons Institute for the Theory of Computing, supported by the Simons Foundation and the DI-10-1100101102103.

### References

[References are listed in the original format, providing citations for various works related to the topic.]

This optimized version clarifies the experimental setup, findings, and comparisons, making the text more coherent and professional.