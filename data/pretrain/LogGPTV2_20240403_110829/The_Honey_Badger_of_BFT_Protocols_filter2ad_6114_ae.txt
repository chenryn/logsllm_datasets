machine to listen on N hidden services (one hidden service for
each HoneyBadgerBFT node in our simulated network). Since each
HoneyBadgerBFT node forms a connection to each other node, we
construct a total of N2 Tor circuits per experiment, beginning and
ending with our machine, and passing through 5 random relays. In
summary, all pairwise overlay links traverse real Tor circuits con-
9https://metrics.torproject.org/bandwidth.html as of Nov 10, 2015
Figure 7: Latency vs. throughput for experiments over wide
area networks. Error bars indicate 95% conﬁdence intervals.
Findings. From Figure 6 we can see for each setting, the through-
put increases as the number of proposed transactions increases. We
achieve throughput exceeding 20,000 transactions per second for
medium size networks of up to 40 nodes. For a large 104 node
network, we attain more than 1,500 transactions per second. Given
an inﬁnite batch size, all network sizes would eventually converge
to a common upper bound, limited only by available bandwidth.
Although the total bandwidth consumed in the network increases
(linearly) with each additional node, the additional nodes also con-
tribute additional bandwidth capacity.
Throughput, latency, and scale tradeoffs. Latency is deﬁned as
the time interval between the time the ﬁrst node receives a client
request and when the (N − f )-th node ﬁnishes the consensus pro-
tocol. This is reasonable because the (N − f )-th node ﬁnishing the
protocol implies the accomplishment of the consensus for the honest
parties.
Figure 7 shows the relationship between latency and throughput
for different choices of N and f = N/4. The positive slopes indi-
cate that our experiments have not yet fully saturated the available
bandwidth, and we would attain better throughput even with larger
batch sizes. Figure 7 also shows that latency increases as the number
of nodes increases, largely stemming from the ABA phase of the
protocol. In fact, at N = 104, for the range of batch sizes we tried,
our system is CPU bound rather than bandwidth bound because our
implementation is single threaded and must verify O(N2) thresh-
old signatures. Regardless, our largest experiment with 104 nodes
completes in under 6 minutes.
Although more nodes (with equal bandwidth provisioning) could
be added to the network without affecting maximum attainable
throughput, the minimal bandwidth consumed to commit one batch
(and therefore the latency) increases with O(N2 logN). This con-
straint implies a limit on scalability, depending on the cost of band-
width and users’ latency tolerance.
Comparison with PBFT. Figure 8 shows a comparison with the
PBFT protocol, a classic BFT protocol for partially synchronous
networks. We use the Python implementation from Croman et
al. [24], running on 8, 16, 32, and 64 nodes evenly distributed
among Amazon AWS regions. Batch sizes were chosen to saturate
the network’s available bandwidth.
Fundamentally, while PBFT and our protocol have the same
asymptotic communication complexity in total, our protocol dis-
05000100001500020000Throughput (Tx per second)101102Latency (seconds) in log scaleNodes / Tolerance32/840/1048/1256/1464/16104/2640MACS/Simons Collaboration in Cryptography through NSF grant
CNS-1523467.
7. REFERENCES
[1] How a Visa transaction works.
http://apps.usa.visa.com/merchants/become-a-merchant/
how-a-visa-transaction-works.jsp, 2015.
[2] M. Abd-El-Malek, G. R. Ganger, G. R. Goodson, M. K.
Reiter, and J. J. Wylie. Fault-scalable byzantine fault-tolerant
services. ACM SIGOPS Operating Systems Review,
39(5):59–74, 2005.
[3] J. A. Akinyele, C. Garman, I. Miers, M. W. Pagano,
M. Rushanan, M. Green, and A. D. Rubin. Charm: a
framework for rapidly prototyping cryptosystems. Journal of
Cryptographic Engineering, 3(2):111–128, 2013.
[4] Y. Amir, B. Coan, J. Kirsch, and J. Lane. Prime: Byzantine
replication under attack. Dependable and Secure Computing,
IEEE Transactions on, 8(4):564–577, 2011.
[5] Y. Amir, C. Danilov, D. Dolev, J. Kirsch, J. Lane,
C. Nita-Rotaru, J. Olsen, and D. Zage. Steward: Scaling
byzantine fault-tolerant replication to wide area networks.
Dependable and Secure Computing, IEEE Transactions on,
7(1):80–93, 2010.
[6] P.-L. Aublin, S. Ben Mokhtar, and V. Quéma. Rbft:
Redundant byzantine fault tolerance. In Distributed
Computing Systems (ICDCS), 2013 IEEE 33rd International
Conference on, pages 297–306. IEEE, 2013.
[7] J. Baek and Y. Zheng. Simple and efﬁcient threshold
cryptosystem from the gap difﬁe-hellman group. In Global
Telecommunications Conference, 2003. GLOBECOM’03.
IEEE, volume 3, pages 1491–1495. IEEE, 2003.
[8] M. Ben-Or and R. El-Yaniv. Resilient-optimal interactive
consistency in constant time. Distributed Computing,
16(4):249–262, 2003.
[9] M. Ben-Or, B. Kelmer, and T. Rabin. Asynchronous secure
computations with optimal resilience. In Proceedings of the
thirteenth annual ACM symposium on Principles of
distributed computing, pages 183–192. ACM, 1994.
[10] A. Bessani, J. Sousa, and E. E. Alchieri. State machine
replication for the masses with bft-smart. In Dependable
Systems and Networks (DSN), 2014 44th Annual IEEE/IFIP
International Conference on, pages 355–362. IEEE, 2014.
[11] A. Boldyreva. Threshold signatures, multisignatures and blind
signatures based on the gap-difﬁe-hellman-group signature
scheme. In Public key cryptographyâ ˘A ˇTPKC 2003, pages
31–46. Springer, 2002.
[12] J. Bonneau, A. Miller, J. Clark, A. Narayanan, J. Kroll, and
E. W. Felten. Research perspectives on bitcoin and
second-generation digital currencies. In 2015 IEEE
Symposium on Security and Privacy. IEEE, 2015.
[13] G. Bracha. Asynchronous byzantine agreement protocols.
Information and Computation, 75(2):130–143, 1987.
[14] M. Burrows. The Chubby lock service for loosely-coupled
distributed systems. In Proceedings of the 7th symposium on
Operating systems design and implementation, pages
335–350. USENIX Association, 2006.
[15] C. Cachin, K. Kursawe, F. Petzold, and V. Shoup. Secure and
efﬁcient asynchronous broadcast protocols. In Advances in
Cryptology – Crypto 2001, pages 524–541. Springer, 2001.
[16] C. Cachin, K. Kursawe, and V. Shoup. Random oracles in
constantipole: Practical asynchronous byzantine agreement
Figure 9: Latency vs throughput for experiments running Hon-
eyBadgerBFT over Tor.
sisting of random relay nodes, designed so that the performance
obtained is representative of a real HoneyBadgerBFT deployment
over Tor (despite all simulated nodes running on a single host ma-
chine).
Since Tor provides a critical public service for many users, it
is important to ensure that research experiments conducted on the
live network do not adversely impact it. We formed connections
from only a single vantage point (and thus avoid receiving), and
ran experiments of short duration (several minutes) and with small
parameters (only 256 circuits formed in our largest experiment). In
total, our experiments involved the transfer of approximately ﬁve
gigabytes of data through Tor – less than a 1E-5 fraction of its daily
utilization.
Figure 9 shows how latency changes with throughput. In contrast
to our EC2 experiment where nodes have ample bandwidth, Tor
circuits are limited by the slowest link in the circuit. We attain a
maximum throughput of over 800 transactions per second of Tor.
In general, messages transmitted over Tor’s relay network tends
to have signiﬁcant and highly variable latency. For instance, during
our experiment on 8 parties proposing 16384 transactions per party,
a single message can be delayed for 316.18 seconds and the delay
variance is over 2208 while the average delay is only 12 seconds.
We stress that our protocol did not need to be tuned for such network
conditions, as would a traditional eventually-synchronous protocol.
6. CONCLUSION
We have presented HoneyBadgerBFT, the ﬁrst efﬁcient and high-
throughput asynchronous BFT protocol. Through our implementa-
tion and experimental results we demonstrate that HoneyBadgerBFT
can be a suitable component in incipient cryptocurrency-inspired
deployments of fault tolerant transaction processing systems. More
generally, we believe our work demonstrates the promise of building
dependable and transaction processing systems based on asynch-
ronous protocol.
Acknowledgements. We thank Jay Lorch, Jonathan Katz, and Emin
Gün Sirer for helpful suggestions, and especially Dominic Williams
for several excellent discussions that inspired us to tackle this prob-
lem. This work is supported in part by NSF grants CNS-1314857,
CNS-1453634, CNS-1518765, CNS-1514261, and CNS-1518899,
DARPA grant N66001-15-C-4066, a Packard Fellowship, a Sloan
Fellowship, two Google Faculty Research Awards, and a VMWare
Research Award. This work was done in part while a subset of the
authors were visiting students at UC Berkeley, and while a subset
of the authors were visiting the Simons Institute for the Theory of
Computing, supported by the Simons Foundation and by the DI-
10-1100101102103Throughput (Tx / s)100101102Latency (sec)4/18/216/441using cryptography. In Proceedings of the Nineteenth Annual
ACM Symposium on Principles of Distributed Computing,
pages 123–132. ACM, 2000.
[17] C. Cachin, J. Poritz, et al. Secure intrusion-tolerant replication
on the internet. In Dependable Systems and Networks, 2002.
DSN 2002. Proceedings. International Conference on, pages
167–176. IEEE, 2002.
[18] C. Cachin and S. Tessaro. Asynchronous veriﬁable
information dispersal. In Reliable Distributed Systems, 2005.
SRDS 2005. 24th IEEE Symposium on, pages 191–201. IEEE,
2005.
[19] R. Canetti and T. Rabin. Fast asynchronous byzantine
agreement with optimal resilience. In Proceedings of the
twenty-ﬁfth annual ACM symposium on Theory of computing,
pages 42–51. ACM, 1993.
[20] M. Castro, B. Liskov, et al. Practical byzantine fault tolerance.
In OSDI, volume 99, pages 173–186, 1999.
[21] A. Clement, M. Kapritsos, S. Lee, Y. Wang, L. Alvisi,
M. Dahlin, and T. Riche. Upright cluster services. In
Proceedings of the ACM SIGOPS 22nd symposium on
Operating systems principles, pages 277–290. ACM, 2009.
[22] A. Clement, E. L. Wong, L. Alvisi, M. Dahlin, and
M. Marchetti. Making byzantine fault tolerant systems tolerate
byzantine faults. In NSDI, volume 9, pages 153–168, 2009.
[23] F. Cristian, H. Aghili, R. Strong, and D. Dolev. Atomic
broadcast: From simple message diffusion to Byzantine
agreement. Citeseer, 1986.
[24] K. Croman, C. Decker, I. Eyal, A. E. Gencer, A. Juels,
A. Kosba, A. Miller, P. Saxena, E. Shi, E. G. Sirer, D. Song,
and R. W. and. On scaling decentralized blockchains — a
position paper. 3rd Bitcoin Research Workshop, 2015.
[25] G. Danezis and S. Meiklejohn. Centrally banked
cryptocurrencies. arXiv preprint arXiv:1505.06895, 2015.
[26] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the
presence of partial synchrony. Journal of the ACM (JACM),
35(2):288–323, 1988.
[27] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility
of distributed consensus with one faulty process. Journal of
the ACM (JACM), 32(2):374–382, 1985.
[28] A. Guillevic. Kim-barbulescu variant of the number ﬁeld sieve
to compute discrete logarithms in ﬁnite ﬁelds.
https://ellipticnews.wordpress.com/2016/05/02/kim-
barbulescu-variant-of-the-number-ﬁeld-sieve-to-compute-
discrete-logarithms-in-ﬁnite-ﬁelds/, May 2016.
[29] T. Kim and R. Barbulescu. Extended tower number ﬁeld sieve:
A new complexity for medium prime case. Technical report,
IACR Cryptology ePrint Archive, 2015: 1027, 2015.
[30] V. King and J. Saia. From almost everywhere to everywhere:
Byzantine agreement with O(n3/2) bits. In Distributed
Computing, pages 464–478. Springer, 2009.
[31] V. King and J. Saia. Breaking the O(n2) bit barrier: scalable
byzantine agreement with an adaptive adversary. Journal of
the ACM (JACM), 58(4):18, 2011.
[32] E. Kokoris-Kogias, P. Jovanovic, N. Gailly, I. Khofﬁ,
L. Gasser, and B. Ford. Enhancing bitcoin security and
performance with strong consistency via collective signing.
arXiv preprint arXiv:1602.06997, 2016.
[33] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong.
Zyzzyva: speculative byzantine fault tolerance. In ACM
SIGOPS Operating Systems Review, volume 41, pages 45–58.
ACM, 2007.
[34] K. Kursawe and V. Shoup. Optimistic asynchronous atomic
broadcast. In in the Proceedings of International Colloqium
on Automata, Languages and Programming (ICALP05)(L
Caires, GF Italiano, L. Monteiro, Eds.) LNCS 3580. Citeseer,
2001.
[35] J. Kwon. TenderMint: Consensus without Mining, August
2014.
[36] L. Lamport. The part-time parliament. ACM Transactions on
Computer Systems (TOCS), 16(2):133–169, 1998.
[37] L. Luu, V. Narayanan, K. Baweja, C. Zheng, S. Gilbert, and
P. Saxena. Scp: A computationally-scalable byzantine
consensus protocol for blockchains. Cryptology ePrint
Archive, Report 2015/1168, 2015. http://eprint.iacr.org/.
[38] B. Lynn. On the implementation of pairing-based
cryptography. The Department of Computer Science and the
Committee on Graduate Studies of Stanford University, 2007.
[39] Y. Mao, F. P. Junqueira, and K. Marzullo. Mencius: building
efﬁcient replicated state machines for wans. In OSDI,
volume 8, pages 369–384, 2008.
[40] R. McMillan. Ibm bets big on bitcoin ledger. Wall Street
Journal.
[41] R. McMillan. How bitcoin became the honey badger of
money. Wired Magazine,
http://www.wired.com/2013/12/bitcoin_honey/, 2013.
[42] A. Miller, Y. Xia, K. Croman, E. Shi, and D. Song. The honey
badger of bft protocols. [Online full version]
http://eprint.iacr.org/2016/199, 2016.
[43] A. Mostefaoui, H. Moumen, and M. Raynal. Signature-free
asynchronous byzantine consensus with t< n/3 and o (n 2)
messages. In Proceedings of the 2014 ACM symposium on
Principles of distributed computing, pages 2–9. ACM, 2014.
[44] S. Nakamoto. Bitcoin: A peer-to-peer electronic cash system.
http://bitcon.org/bitcoin.pdf, 2008.
[45] NIST. Sp 800-37. Guide for the Security Certiﬁcation and
Accreditation of Federal Information Systems, 2004.
[46] D. Ongaro and J. Ousterhout. In search of an understandable
consensus algorithm. In Proc. USENIX Annual Technical
Conference, pages 305–320, 2014.
[47] H. V. Ramasamy and C. Cachin. Parsimonious asynchronous
byzantine-fault-tolerant atomic broadcast. In OPODIS, pages
88–102. Springer, 2005.
[48] D. Schwartz, N. Youngs, and A. Britto. The Ripple Protocol
Consensus Algorithm, September 2014.
[49] A. Singh, T. Das, P. Maniatis, P. Druschel, and T. Roscoe. Bft
protocols under ﬁre. In Proceedings of the 5th USENIX
Symposium on Networked Systems Design and
Implementation, NSDI’08, pages 189–204, Berkeley, CA,
USA, 2008. USENIX Association.
[50] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung.
Spin one’s wheels? byzantine fault tolerance with a spinning
primary. In Reliable Distributed Systems, 2009. SRDS’09.
28th IEEE International Symposium on, pages 135–144.
IEEE, 2009.
[51] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung.
Ebawa: Efﬁcient byzantine agreement for wide-area networks.
In High-Assurance Systems Engineering (HASE), 2010 IEEE
12th International Symposium on, pages 10–19. IEEE, 2010.
[52] Z. Wilcox-O’Hearn. Zfec 1.4. 0. Open source code
distribution: http://pypi.python.org/pypi/zfec, 2008.
42