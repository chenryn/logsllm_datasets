L(D,w) to denote the loss incurred by classiﬁer f : X (cid:55)→ Y
(parameterized by w) on D. Typically, this is computed by
averaging a loss function (cid:96)(y,x,w) computed on each data
point, i.e., L(D,w) = 1
i=1 (cid:96)(yi,xi,w). We assume that the
classiﬁer f is learned by minimizing an objective function
L(D,w) on the training data. Typically, this is an estimate of
the generalization error, obtained by the sum of the empirical
loss L on training data D and a regularization term.
n ∑n
2.1 Threat Model: Attacker’s Goal
We deﬁne the attacker’s goal based on the desired security
violation. In particular, the attacker may aim to cause either
an integrity violation, to evade detection without compromis-
ing normal system operation; or an availability violation, to
compromise the normal system functionalities available to
legitimate users.
2.2 Threat Model: Attacker’s Knowledge
We characterize the attacker’s knowledge κ as a tuple in an ab-
stract knowledge space K consisting of four main dimensions,
respectively representing knowledge of: (k.i) the training data
D; (k.ii) the feature set X ; (k.iii) the learning algorithm f ,
along with the objective function L minimized during train-
ing; and (k.iv) the parameters w learned after training the
model. This categorization enables the deﬁnition of many dif-
ferent kinds of attacks, ranging from white-box attacks with
full knowledge of the target classiﬁer to black-box attacks in
which the attacker has limited information about the target
system.
White-Box Attacks. We assume here that the attacker has full
knowledge of the target classiﬁer, i.e., κ = (D,X , f ,w). This
setting allows one to perform a worst-case evaluation of the
security of machine-learning algorithms, providing empirical
upper bounds on the performance degradation that may be
incurred by the system under attack.
Black-Box Attacks. We assume here that the input feature
representation X is known. For images, this means that we
do consider pixels as the input features, consistently with
other recent work on black-box attacks against machine learn-
ing [32, 33]. At the same time, the training data D and the
type of classiﬁer f are not known to the attacker. We consider
the most realistic attack model in which the attacker does not
have querying access to the classiﬁer.
The attacker can collect a surrogate dataset ˆD, ideally sam-
pled from the same underlying data distribution as D, and
train a surrogate model ˆf on such data to approximate the tar-
get function f . Then, the attacker can craft the attacks against
ˆf , and then check whether they successfully transfer to the
target classiﬁer f . By denoting limited knowledge of a given
component with the hat symbol, such black-box attacks can
be denoted with ˆκ = ( ˆD,X , ˆf , ˆw).
2.3 Threat Model: Attacker’s Capability
This attack characteristic deﬁnes how the attacker can inﬂu-
ence the system, and how data can be manipulated based on
application-speciﬁc constraints. If the attacker can manipulate
both training and test data, the attack is said to be causative.
It is instead referred to as exploratory, if the attacker can only
manipulate test data. These scenarios are more commonly
known as poisoning [4,18,24,27,48] and evasion [3,8,14,42].
Another aspect related to the attacker’s capability depends
on the presence of application-speciﬁc constraints on data
manipulation; e.g., to evade malware detection, malicious
code has to be modiﬁed without compromising its intrusive
functionality. This may be done against systems leveraging
static code analysis, by injecting instructions that will never
be executed [11, 15, 45]. These constraints can be generally
accounted for in the deﬁnition of the optimal attack strategy by
assuming that the initial attack sample x can only be modiﬁed
according to a space of possible modiﬁcations Φ(x).
3 Optimization Framework for Gradient-
based Attacks
We introduce here a general optimization framework that
encompasses both evasion and poisoning attacks. Gradient-
based attacks have been considered for evasion (e.g., [3, 8, 14,
23, 42]) and poisoning (e.g., [4, 18, 24, 27]). Our optimization
framework not only uniﬁes existing evasion and poisoning
attacks, but it also enables the design of new attacks. After
deﬁning our general formulation, we instantiate it for evasion
and poisoning attacks, and use it to derive a new poisoning
availability attack for logistic regression.
3.1 Gradient-based Optimization Algorithm
Given the attacker’s knowledge κ ∈ K and an attack sample
x(cid:48) ∈ Φ(x) along with its label y, the attacker’s goal can be
deﬁned in terms of an objective function A(x(cid:48),y,κ) ∈ R (e.g.,
a loss function) which measures how effective the attack
sample x(cid:48) is. The optimal attack strategy can be thus given as:
x(cid:63) ∈ arg max
x(cid:48)∈Φ(x)
A(x(cid:48)
,y,κ) .
(1)
Note that, for the sake of clarity, we consider here the opti-
mization of a single attack sample, but this formulation can
be easily extended to account for multiple attack points. In
USENIX Association
28th USENIX Security Symposium    323
Algorithm 1 Gradient-based Evasion and Poisoning Attacks
Input: x,y: the input sample and its label; A(x,y,κ): the at-
tacker’s objective; κ = (D,X , f ,w): the attacker’s knowl-
edge parameter vector; Φ(x): the feasible set of manipu-
lations that can be made on x; t > 0: a small number.
Output: x(cid:48): the adversarial example.
1: Initialize the attack sample: x(cid:48) ← x
2: repeat
3:
4:
Store attack from previous iteration: x ← x(cid:48)
Update step: x(cid:48) ← ΠΦ (x + η∇xA(x,y,κ)), where the
step size η is chosen with line search (bisection method),
and ΠΦ ensures projection on the feasible domain Φ.
5: until |A(x(cid:48),y,κ)− A(x,y,κ)| ≤ t
6: return x(cid:48)
particular, as in the case of poisoning attacks, the attacker can
maximize the objective by iteratively optimizing one attack
point at a time [5, 48].
Attack Algorithm. Algorithm 1 provides a general pro-
jected gradient-ascent algorithm that can be used to solve
the aforementioned problem for both evasion and poison-
ing attacks. It iteratively updates the attack sample along
the gradient of the objective function, ensuring the result-
ing point to be within the feasible domain through a pro-
jection operator ΠΦ. The gradient step size η is determined
in each update step using a line-search algorithm based on
the bisection method, which solves maxη A(x(cid:48)(η),y,κ), with
x(cid:48)(η) = ΠΦ (x + η∇xA(x,y,κ)). For the line search, in our
experiments we consider a maximum of 20 iterations. This al-
lows us to reduce the overall number of iterations required by
Algorithm 1 to reach a local or global optimum. We also set
the maximum number of iterations for Algorithm 1 to 1,000,
but convergence (Algorithm 1, line 5) is typically reached
only after a hundred iterations.
We ﬁnally remark that non-differentiable learning algo-
rithms, like decision trees and random forests, can be attacked
with more complex strategies [17,19] or using gradient-based
optimization against a differentiable surrogate learner [31,37].
3.2 Evasion Attacks
In evasion attacks, the attacker manipulates test samples to
have them misclassiﬁed, i.e., to evade detection by a learning
algorithm. For white-box evasion, the optimization problem
given in Eq. (1) can be rewritten as:
max
x(cid:48)
s.t.
,w) ,
(cid:96)(y,x(cid:48)
(cid:107)x(cid:48) − x(cid:107)p ≤ ε ,
xlb (cid:22) x(cid:48) (cid:22) xub ,
(2)
(3)
(4)
where (cid:107)v(cid:107)p is the (cid:96)p norm of v, and we assume that the clas-
siﬁer parameters w are known. For the black-box case, it
Figure 2: Conceptual representation of maximum-conﬁdence
evasion attacks (within an (cid:96)2 ball of radius ε) vs. minimum-
distance adversarial examples. Maximum-conﬁdence attacks
tend to transfer better as they are misclassiﬁed with higher
conﬁdence (though requiring more modiﬁcations).
sufﬁces to use the parameters ˆw of the surrogate classiﬁer ˆf .
In this work we consider (cid:96)(y,x(cid:48),w) = −y f (x(cid:48)), as in [3].
The intuition here is that the attacker maximizes the loss
on the adversarial sample with the original class, to cause
misclassiﬁcation to the opposite class. The manipulation con-
straints Φ(x) are given in terms of: (i) a distance constraint
(cid:107)x(cid:48) − x(cid:107)p ≤ ε, which sets a bound on the maximum input
perturbation between x (i.e., the input sample) and the cor-
responding modiﬁed adversarial example x(cid:48); and (ii) a box
constraint xlb (cid:22) x(cid:48) (cid:22) xub (where u (cid:22) v means that each ele-
ment of u has to be not greater than the corresponding element
in v), which bounds the values of the attack sample x(cid:48).
For images, the former constraint is used to implement ei-
ther dense or sparse evasion attacks [12,25,37]. Normally, the
(cid:96)2 and the (cid:96)∞ distances between pixel values are used to cause
an indistinguishable image blurring effect (by slightly manip-
ulating all pixels). Conversely, the (cid:96)1 distance corresponds
to a sparse attack in which only few pixels are signiﬁcantly
manipulated, yielding a salt-and-pepper noise effect on the
image [12, 37]. The box constraint can be used to bound each
pixel value between 0 and 255, or to ensure manipulation of
only a speciﬁc region of the image. For example, if some pix-
els should not be manipulated, one can set the corresponding
values of xlb and xub equal to those of x.
Maximum-conﬁdence vs. minimum-distance evasion. Our
formulation of evasion attacks aims to produce adversarial
examples that are misclassiﬁed with maximum conﬁdence
by the classiﬁer, within the given space of feasible modiﬁca-
tions. This is substantially different from crafting minimum-
distance adversarial examples, as formulated in [42] and in
follow-up work (e.g., [33]). This difference is conceptually
depicted in Fig. 2. In particular, in terms of transferability, it
is now widely acknowledged that higher-conﬁdence attacks
have better chances of successfully transfering to the target
classiﬁer (and even of bypassing countermeasures based on
gradient masking) [2, 8, 13]. For this reason, in this work we
consider evasion attacks that aim to craft adversarial examples
misclassiﬁed with maximum conﬁdence.
Initialization. There is another factor known to improve trans-
324    28th USENIX Security Symposium
USENIX Association
surrogate classifier !"($)used to craft black-boxadversarial examplestarget classifier !$used to craft white-boxadversarial examplesminimum-distance black-boxadversarial examplemaximum-confidence black-boxadversarial examplemaximum-confidence white-boxadversarial exampleinitial / source exampleminimum-distance white-boxadversarial exampleferability of evasion attacks, as well as their effectiveness in
the white-box setting. It consists of running the attack starting
from different initialization points to mitigate the problem of