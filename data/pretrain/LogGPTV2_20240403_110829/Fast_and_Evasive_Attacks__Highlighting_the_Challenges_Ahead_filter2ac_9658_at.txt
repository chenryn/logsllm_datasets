a
M
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
N= 0% α=10 β=30%
N=10% α=10 β=30%
N=50% α=10 β=30%
N=90% α=10 β=30%
 0.0002
 0.0004
 0.0006
 0.0008
 0.001
P(W|-)
Fig. 9. Correlated Outlier attack evaluation
Fig. 10. Correlated Outlier attack evaluation,
with chaff
The fourth parameter, the frequency of the spurious features in the innocuous pool
P(si|−), is not directly controlled by the attacker. The attacker’s challenge is to choose
the spurious features such that P(si|−) is low enough that the attacker succeeds in get-
ting the innocuous features with all α of the spurious features S to have a higher Bayes
score than the target-class samples.
Figure 9 shows that the attack can succeed for a wide range of realistic parameters.
Each curve in the graph represents a different attacker choice of α. As P(W|−) increases,
the maximum value of P(si|−) also increases. Even for very low values of P(W|−) and
α, the attacker has a great deal of room for error in his estimation of P(si|−).
Again, any value that satisﬁes these constraints will force the the learner to choose
between false negatives and false positives, and the classiﬁer will not improve as more
target-class samples are obtained. If the learner uses the Polygraph threshold-setting
algorithm, then τ will be set to achieve 100% false negatives if P(S|−)
≥ F. Otherwise
P(S|+)
it will be set to have low false negatives, but will classify the samples containing the
spurious features S as positive. The signature will not improve, and as long as it is in use,
legitimate samples containing those samples will be false positives, causing a targeted
denial of service.
4.4 Attack II: Suspicious Pool Poisoning
Up to this point we have assumed that the suspicious and innocuous pools are noise-
free. That is, the suspicious pool contains only target-class samples, and the innocuous
pool contains only innocuous samples. In some cases, however, the attacker may be
able to inject constructed samples into the suspicious pool, the innocuous pool, or both,
as described in Section 2. We ﬁrst consider the case where the attacker is able to inject
chaff, specially constructed samples, into the suspicious pool.
Attack Description. The chaff can simultaneously have two effects. First, by not in-
cluding the actual target-class features W , the classiﬁer will calculate a lower P(W|+).
The actual target-class samples in the suspicious pool will have lower Bayes scores as
a result, stretching the false negative curve of the training data distribution graph to the
left.
98
J. Newsome, B. Karp, and D. Song
Second, the classiﬁer will calculate a higher P(si|+) for any spurious feature si in-
cluded in the chaff. This will cause innocuous samples containing those features to have
a higher Bayes score, stretching the false positive curve of the training data distribution
graph to the right, in the same manner as in the Correlated Outlier attack (Figure 8). Un-
like the target-class samples, each chaff sample can contain all of the spurious features,
since it makes no difference to the attacker whether the chaff samples are classiﬁed as
positive by the resulting Bayes classiﬁer.
Attack Analysis. The attacker’s goal is again to force the learner to choose between
false positives and false negatives, by ensuring that the score of a sample containing
all α of the spurious features S has a higher Bayes score than a sample containing the
true target-class features W , and a fraction β of the spurious features. Assuming that
the chaff in the suspicious pool contains all α of the spurious features, the attacker can
include fewer spurious features in the actual target-class samples, or even none at all.
Theorem 6. Suppose that the fraction N of samples in the suspicious pool is chaff
containing the spurious features S. Samples containing all α spurious features have a
higher Bayes score than samples containing the actual target-class features W and the
fraction β of the α spurious features when:
P(si|−) < N + (1− N)β and (1− N)( N+β(1−N)
P(si|−)
)βα−α≤ P(W|−)
When these conditions are satisﬁed, this attack becomes equivalent to the Correlated
Outlier attack. Notice that when there is no chaff (N = 0) these conditions simplify to
the conditions presented in Section 4.3.
Evaluation. We perform a similar evaluation as in Section 4.3. In this case, the attacker
uses a relatively low number of spurious features (α= 10), and each curve of the graph
represents different ratios of chaff in the suspicious pool. Figure 10 shows that the
addition of chaff to the suspicious pool greatly improves the practicality of the attack.
The resulting classiﬁer will again either have 100% false negatives, or cause legitimate
samples with the spurious features to be blocked.
4.5 Attack III: Innocuous Pool Poisoning
We next consider the case where the attacker is able to poison the innocuous training
pool. The most obvious attack is to attempt to get samples with the target-class features
W into the innocuous pool. If the target-class samples include only the features W (no
spurious features), then it would be impossible to generate a classiﬁer that classiﬁed the
target-class samples as positive without also classifying the samples that the attacker
injected into the innocuous pool as positive. Hence, the learner could be fooled into
believing that a low-false-positive classiﬁer cannot be generated.
The solution to this problem proposed by Polygraph [15] for automatic worm signa-
ture generation is to use a network trace taken some time t ago, such that t is greater
than the expected time in-between the attacker discovering the vulnerability (and hence
discovering what the worm features W will be), and the vulnerability being patched on
most vulnerable machines. The time period t is somewhat predictable assuming that the
Paragraph: Thwarting Signature Learning by Training Maliciously
99
attacker does not discover the vulnerability before the makers of the vulnerable software
do. Conversely, t could be an arbitrary time period for a “zero-day” exploit. However,
we show that a patient attacker can poison the innocuous pool in a useful way before he
knows what the worm features W are.
Attack Description. The attacker can aid the Correlated Outlier attack by injecting
spurious tokens into the innocuous pool. In this case, using an old trace for the innocu-
ous pool does not help at all, since the attacker does not need to know W at the time
of poisoning the innocuous pool. That is, an attacker who does not yet have a vulner-
ability to exploit can choose a set of spurious features S, and preemptively attempt to
get samples containing S into the learner’s innocuous pool, thus increasing P(S|−). The
attacker can then use these spurious features to perform the Correlated Outlier attack,
optionally poisoning the suspicious pool as well as described in Section 4.4.
Attack Analysis. If the attacker is able to inject samples containing S into the innocuous
pool, P(S|−) will be increased. The attacker’s best strategy may be to use spurious fea-
tures that do not occur at all in normal trafﬁc. This would allow him to more accurately
estimate the learner’s P(S|−) when designing the worm.
Aside from this additional knowledge, the attack proceeds exactly as in Section 4.4.
Evaluation. The success of the attack is determined by the same model as in Theorem 6.
The addition of the injected spurious features helps make the attack more practical by
allowing him to more accurately predict a set of spurious features that occur together
in a small fraction of the innocuous training pool. Success in the attack will again ei-
ther result in the classiﬁer having 100% false negatives, or result in innocuous samples
containing the spurious features to be blocked.
5 Discussion
5.1 Hierarchical Clustering
Polygraph [15] implements a hierarchical clustering algorithm to enable its conjunction
and subsequence learners to work in the presence of non-worm samples in the sus-
picious training pool. Each sample starts as its own cluster, and clusters are greedily
merged together. Each cluster has a signature associated with it that is the intersection
of the features present in the samples in that cluster. The greedy merging process favors
clusters that produce low-false-positive signatures; i.e., those that have the most distin-
guishing set of features in common. When no more merges can be performed without
the resulting cluster having a high-false-positive signature, the algorithm terminates
and outputs a signature for each sufﬁciently large cluster. Ideally, samples of unrelated
worms are each in their own cluster, and non-worm samples are not clustered.
One might wonder whether the hierarchical clustering algorithm helps to alleviate
the Randomized Red Herring or Dropped Red Herring attacks. It does not.
First consider the Randomized Red Herring attack. Each worm sample has the set
of features that must be present, W , and some subset of a set of spurious features, S.
Keep in mind that the attacker’s goal is for the resulting signature to be too speciﬁc. If
100
J. Newsome, B. Karp, and D. Song
the hierarchical clustering algorithm puts all the worm samples into one cluster, which
is likely, the resulting signature will be exactly the same as if no clustering were used.
If it does not, the resulting signature can only be more speciﬁc, which further increases
false negatives.
For example, suppose one cluster contains spurious features s1, s2, and s3, and an-
other cluster contains spurious features s2, s3, and s4. Both clusters contain the neces-
sary worm features W . If these clusters are merged together, the resulting signature is
the conjunction
(W ∧ s2 ∧ s3)
If the clusters are not merged, then the learner will publish two signatures. Assuming
(W ∧ s1 ∧ s2 ∧ s3)∨ (W ∧ s2 ∧ s3 ∧ s4)
This can be rewritten as:
(W ∧ s2 ∧ s3)∧ (s1 ∨ s4)
Obviously, this is more speciﬁc than the signature that would have resulted if the two
both signatures are used, this is equivalent to the single signature
clusters were merged, and hence will have strictly more false negatives.
The same is true for the Dropped Red Herring attack, by similar reasoning. Again,
if all samples of the worm are merged into one cluster, the result is equivalent to if no
clustering were used. Not merging the samples into a single cluster can only make the
signature more speciﬁc, which further increases false negatives.
5.2 Attack Application to Other Polymorphic Worm Signature Generation
Systems
At this time, the only automatic polymorphic worm signature generation systems that
are based on learning are Polygraph [15] and Hamsa [9]. Throughout this paper, we
have used Polygraph’s algorithms as concrete examples. Hamsa generates conjunction
signatures, with improved performance and noise-tolerance over Polygraph. To gen-
erate a conjunction signature, Hamsa iteratively adds features found in the suspicious
pool, preferring features that occur in the most samples in the suspicious pool and result
in sufﬁciently low false positives in the innocuous pool.
We begin with two observations. First, the adversary can cause Hamsa to use spuri-
ous features in its signature, as long as those features occur sufﬁciently infrequently in
the innocuous pool, and occur at least as often in the suspicious pool as the true target-
class features. Second, the false-negative bounds proven in the Hamsa paper only apply
to the target-class samples actually found in the suspicious pool, and not necessarily to
subsequently generated samples.
Unlike Polygraph, Hamsa stops adding features to the signature once the signature
causes fewer false positives in the innocuous pool than some predetermined threshold.
As a result, Hamsa is relatively resilient to the Randomized Red Herring attack. For ex-
ample, using α= 400, p = .995, Hamsa exhibits only 5% false negatives after collecting
100 target-class samples. While this incidence is still non-trivial, it is an improvement
over Polygraph’s corresponding 70% false negatives with these parameters.
Hamsa is also less vulnerable to the Dropped Red Herring attack, but unfortunately
not completely invulnerable. First, let us assume that Hamsa’s method of breaking ties
when selecting features is not predictable by the adversary (the method does not appear
Paragraph: Thwarting Signature Learning by Training Maliciously
101
to be deﬁned in [9]). In this case, the simplest form of the attack will not succeed, as the
adversary cannot predict which spurious features are actually used, and hence which
to drop to avoid the generated classiﬁer. However, suppose that the attacker is able to
inject noise into the suspicious pool, and the spurious features follow some ordering of
probabilities with which they appear in a particular noise sample. This ordering then
speciﬁes the (probable) preferential use of each spurious feature in the generated signa-
ture. That is, the most probable spurious feature will be chosen ﬁrst by Hamsa, since it
will have the highest coverage in the suspicious pool, and so on. In that case, an adver-
sary who can inject n noise samples into the suspicious pool can force up to n iterations
of the learning process.
5.3 Attack Application to Spam
The correlated outlier attack described in Section 4.3 is also applicable to Bayesian
spam ﬁlters, though the speciﬁc analysis is dependent on the exact implementation.
There is already an attack seen in the wild where a spam email includes a collection
of semi-random words or phrases to deﬂate the calculated probability that the email
is spam [6].5 To perform the correlated outlier attack on a spam ﬁlter, the adversary
would use as spurious features words that tend to occur together in a fraction of non-
spam emails. If a classiﬁer is trained to recognize such an email as spam, it may suffer
false positives when legitimate email containing those words is received. Conversely, if
a classiﬁer’s threshold is biased toward not marking those legitimate mails as spam, it
may suffer from false negatives when receiving spam with the chosen features.
As in the worm case, it may be possible for a spam author to guess what words
occur in the correct frequency in the innocuous training data. It seems likely that such
an attack could succeed were it tailored to an individual user, though it would not be a
ﬁnancial win for the spam author. However, the spam author might be able to tailor the
spurious features to a broader audience, for example by selecting jargon words that are
likely to occur together in the legitimate mail of a particular profession. Another tactic
would be to use words that occur in a certain kind of email that occurs at the needed
low-but-signiﬁcant frequency. For example, adding words or phrases in spam emails
that one would expect to see in a job offer letter could result in very high-cost false
positives, or in the savvy user being hesitant to mark such messages as spam for that
very reason.
5.4 Recommendation for Automatic Worm Signature Generation
Current pattern extraction insufﬁcient. Most currently proposed systems for auto-
matically generating worm signatures work by examining multiple samples of a worm
and extracting the common byte patterns. This is an attractive approach because moni-
toring points can be deployed with relative ease at network gateways and other aggre-
gation points.
5 Note that the Polygraph implementation of a Bayes classiﬁer is not vulnerable to this attack,
because it discards features that have a higher probability of occurring in negative samples
than positive samples.
102
J. Newsome, B. Karp, and D. Song
Unfortunately, most previous approaches [8, 21, 7, 24] do not handle the case where
the worm varies its payload by encrypting its code and using a small, randomly obfus-
cated decryption routine. In this paper, we have shown that the only proposed systems
that handle this case of polymorphism [15, 9] can be defeated by a worm that simply
includes spurious features in its infection attempts.
We believe that if there is to be any hope of generating signatures automatically
by only examining the byte sequences in infection attempt payloads, a more formal
approach will be needed. Interestingly, while there has been some research in the area
of spam email classiﬁcation in the scenario where an adversary reacts to the current
classiﬁer in order to evade it [6,13], there has been little research in the machine learning
scenario where an adversary constructs positive samples in such a way as to prevent
an accurate classiﬁer from being generated in the ﬁrst place. One approach that bears
further investigation is Winnow [11, 12], a machine learning algorithm with proven
bounds on the number of mistakes made before generating an accurate classiﬁer.
Automatic Semantic Analysis. Recent research proposes automated semantic analysis
of collected worm samples, by monitoring the execution of a vulnerable server as it
becomes compromised [3, 5, 2]. These approaches can identify which features of the
worm request caused it to exploit the monitored software, and are hence likely to be
invariant, and useful in a signature. This approach is also less susceptible to being fooled
by the worm into using spurious features in a signature, since it will ignore features that
have no effect on whether the vulnerable software actually gets exploited. The features
so identiﬁed can also be more expressive than the simple presence or absence of tokens;
e.g., they may specify the minimum length of a protocol ﬁeld necessary to trigger a
buffer overﬂow.
While monitoring points employing semantic analysis are not as easily deployed as
those that do not, since they must run the vulnerable software, they are more likely to
produce signatures with low false positives and false negatives than those produced by
pattern extraction alone.
Given the state of current research, we believe that future research on automatic
worm signature generation should focus on provable mistake bounds for pattern-
extraction-based learners and on further analysis of and improvements to automated
semantic analysis techniques.
6 Related Work
Attacking learning algorithms. Barreno et al. independently and concurrently in-
vestigate the challenge of using machine learning algorithms in adversarial environ-
ments [1]. The authors present a high-level framework for categorizing attacks against
machine learning algorithms and potential defense strategies, and analyze the properties
of a hypothetical outlier detection algorithm. Our work is more concrete in that it specif-
ically addresses the challenge of machine learning for automatic signature generation,
and provides in-depth analysis of several practical attacks.
Perdisci et al. independently and concurrently propose attacks [18] against the learn-
ing algorithms presented in Polygraph [15]. Their work shows how an attacker able to
systematically inject noise in the suspicious pool can prevent a correct classiﬁer from
Paragraph: Thwarting Signature Learning by Training Maliciously
103
being generated, for both conjunction and Bayes learners. Their attack against the Poly-
graph Bayes signature generation algorithm is similar to our correlated outlier attack,
though we further generalize the attack to show both how it can be performed even
without suspicious pool poisoning, and how it can be strengthened with innocuous pool
poisoning.
Pattern-extraction signature generation. Several systems have been proposed to au-
tomatically generate worm signatures from a few collected worm samples. Most of
these systems, such as Honeycomb [8], EarlyBird [21], and Autograph [7], have been
shown not to be able to handle polymorphic worms [15]. While PADS [24] has been
shown to be robust to obfuscation of the worm code, it is unclear whether it would work
against encrypted code combined with only a small obfuscated decryption routine.
Polygraph [15] demonstrates that it is possible to generate accurate signatures for
polymorphic worms, because there are some features that must be present in worm in-
fection attempts to successfully exploit the target machine. Polygraph also demonstrates
automatic signature-generation techniques that are successful against maximally-
varying polymorphic worms.
Hamsa [9] is a recently proposed automatic signature generation system, with im-
provements in performance and noise-tolerance over Polygraph. As we discuss in Sec-
tion 5, it is more resilient than Polygraph to the attacks presented here, but not entirely
resilient.
Semantic analysis. Recent research proposes performing automated semantic analysis
of collected worm samples, by monitoring the execution of a vulnerable server as it gets