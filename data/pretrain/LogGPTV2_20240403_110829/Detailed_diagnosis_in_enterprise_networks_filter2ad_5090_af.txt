# NetMedic: Detailed Diagnosis in Enterprise Networks

## 1. Introduction
NetMedic is a diagnostic tool designed to provide detailed fault diagnosis in enterprise networks with minimal application knowledge. This work was motivated by the first study of faults in small enterprises. It combines a rich formulation of the inference problem with a novel technique to determine when one component might be impacting another.

## 2. Experimental Evaluation

### 2.1 Effectiveness in Diagnosing Simultaneous Faults
**Figure 1.** Effectiveness of Coarse and NetMedic when diagnosing two simultaneous faults.
- **X-axis:** Cumulative % of faults
- **Y-axis:** Rank of the correct cause

In scenarios where multiple faults are possible, it is desirable for the diagnostic system to correctly match each effect to its likely cause. We injected two faults simultaneously. With 20 basic faults, there are 190 unique fault pairs, of which 180 are non-interfering. 

- **NetMedic:**
  - Median rank for the correct cause is one for over 80% of the cases.
  - Some degradation in the maximum rank, but no deterioration in the median rank.
  - Even in the presence of multiple faults, NetMedic can often link an effect to its correct cause.

- **Coarse:**
  - Median rank is one for only 60% of the cases.
  - Curiously, Coarse appears to perform better in the 50-70% range compared to single-fault results. This is due to different fault type mixtures in the double-fault scenarios, where Coarse has modest performance.

### 2.2 Impact of History Size
**Figure 2.** NetMedicâ€™s effectiveness when using different history sizes.
- **X-axis:** Cumulative % of faults
- **Y-axis:** Rank of the correct cause

We studied the impact of the size of the history on the effectiveness of NetMedic. Using 30 or 60 minutes of history performs as well as our previous experiments that use 90 minutes of history. Using only 10 minutes of history performs significantly worse, with 40% of the faults having median ranks above 20. Based on these results, we conclude that 30 minutes of historical data suffices for most faults. Note that this history does not need to be fault-free and can include other faults.

Preliminary evidence suggests that using history from more dynamic periods (e.g., day versus night) helps discount spurious connections between components better. Investigating the nature of history that works best in various settings is a subject of ongoing work.

### 2.3 In-Situ Behavior
**Figure 3.** Ranks assigned to abnormal processes in the absence of injected faults.
- **X-axis:** Cumulative % of faults
- **Y-axis:** Rank of the correct cause

We evaluated NetMedic on naturally occurring faults. In a common scenario where a process has plenty of available resources, a normal network, and unchanged configuration elements, a good diagnostic method should blame the process itself for any abnormality rather than other processes.

- **NetMedic:**
  - Blames the process itself for its abnormality in 80% of the cases.
  - The inferences of NetMedic are more consistent than Coarse in this setting.

- **Coarse:**
  - Blames the process itself for its abnormality in only 10% of the cases.

We manually examined many cases where NetMedic assigns a high rank to an affected process. In nearly all of them, the top-ranked cause is a virus scanning process or a sync utility process, which often hog resources over short durations.

## 3. Scaling NetMedic

While NetMedic was motivated by problems inside small enterprises, it can also help large enterprises if scaled up. There are two challenges in scaling NetMedic:

1. **Diagnosis-Related Computation:**
   - The primary bottleneck is calculating component abnormality and edge weights.
   - These calculations are parallelizable and can be distributed to the machines being monitored.
   - Fast correlation methods can speed up the calculation of individual edge weights.
   - Once the edge weights are computed, the remaining calculations are similar to those of Sherlock and can scale to very large graphs.

2. **Data Collection, Storage, and Retrieval:**
   - We can leverage existing work on managing similar data, including lightweight data collection, efficient data compression, and searching compressed history for similar states.

## 4. Related Work

### 4.1 Inference-Based Systems
These systems identify faulty components based on a model of dependencies among components. They target large-scale networks and focus on scalable analysis with simple models. NetMedic provides detailed diagnosis in small business networks and differs in both the challenges it overcomes and how it models components and dependencies.

### 4.2 Rule-Based Systems
Also known as expert systems, these diagnose based on pre-programmed rules. Their main limitation is a lack of generality, as they only diagnose faults for which they have been pre-programmed. Building a rule database that covers a large fraction of possible faults in a complex network is difficult, so we chose an inference-based approach.

### 4.3 Classifier-Based Systems
These systems train offline on healthy and unhealthy states and try to determine if the system is unhealthy and the likely cause. It is unclear how such schemes fare on faults not present in the training data, and extensive training data is hard to obtain. Some systems attempt to overcome this by training on data from multiple networks, but this may not generalize to more complex configurations or other kinds of faults.

### 4.4 Single-Machine Systems
While we focus on diagnosing faults across machines in a network, there is extensive work on diagnosing faults within individual machines. NetMedic borrows from this body of work, especially in light-weight yet extensive data gathering, configuration monitoring, and the use of system history. However, cross-machine diagnosis presents unique challenges, and single-machine diagnosis methods often do not directly translate.

## 5. Conclusions

NetMedic enables detailed diagnosis in enterprise networks with minimal application knowledge. It was motivated by the first study of faults in small enterprises. It combines a rich formulation of the inference problem with a novel technique to determine when a component might be impacting another. In our experiments, it was highly effective at diagnosing a diverse set of faults injected in a live environment.

Modern operating systems and applications export much detailed information regarding their behavior. In theory, this information can form the basis of highly effective diagnostic tools. In reality, the technology was lacking. One class of current systems uses the semantics of this information to diagnose common faults based on pre-programmed fault signatures. Another class focuses exclusively on certain kinds of faults such as performance. Even in combination, these two classes of techniques are unable to diagnose many faults that enterprise networks suffer. The techniques developed in our work are a step towards filling this void. They enable the diagnosis of a broad range of faults visible in the available data without embedding the continuously evolving semantics of the data into the system.

## 6. Acknowledgments

We are grateful to Parveen Patel for his assistance with implementing data collection in NetMedic and to our colleagues who let us deploy NetMedic on their desktops. We also thank our shepherd, Darryl Veitch, Alex Snoeren, and the SIGCOMM reviewers for helping improve the presentation of this paper.

## 7. References

[1] B. Aggarwal, R. Bhagwan, T. Das, S. Eswaran, V. Padmanabhan, and G. Voelker. NetPrints: Diagnosing home network misconfigurations using shared knowledge. In NSDI, 2006.

[2] P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. A. Maltz, and M. Zhang. Towards highly reliable enterprise network services via inference of multi-level dependencies. In SIGCOMM, Aug. 2006.

[3] S. Bhatia, A. Kumar, M. Fiuczynski, and L. Peterson. Lightweight, high-resolution monitoring for troubleshooting production systems. In OSDI, 2006.

[4] S. Brugnoni, G. Bruno, R. Manione, E. Montariolo, E. Paschetta, and L. Sisto. An expert system for real-time fault diagnosis of the Italian telecommunications network. In IFIP, 1996.

[5] M. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer. Pinpoint: Problem determination in large, dynamic Internet services. In DSN, June 2005.

[6] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer. Failure diagnosis using decision trees. In ICAC, 2006.

[7] I. Cohen, M. Goldszmidt, T. Kelly, J. Symons, and J. Chase. Correlating instrumentation data to system states: a building block for automated diagnosis and control. In OSDI, 2006.

[8] A. Deligiannakis, Y. Kotidis, and N. Roussopoulos. Compressing historical information in sensor networks. In SIGMOD, 2006.

[9] M. Garofalakis and P. B. Gibbons. Wavelet synopses with error guarantees. In SIGMOD, 2002.

[10] J. Gray. Why do computers stop and what can be done about it? In Sym. on Reliability in Distributed Software and Database Systems, 1986.

[11] Gteko, Inc. http://www.gteko.com.

[12] W. Hamscher, L. Console, and J. de Kleer, editors. Readings in model-based diagnosis. Morgan Kaufmann Publishers Inc., 1992.

[13] D. Heckerman. Learning in Graphical Models, chapter A tutorial on learning with Bayesian networks. MIT Press, 1999.

[14] A. Hyvarinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Networks, 13(4-5), 2000.

[15] H. Jagadish, A. Mendelzon, and T. Milo. Similarity-based queries. In PODS, 1998.

[16] G. Khanna, M. Cheng, P. Varadharajan, S. Bagchi, M. Correia, and P. Verissimo. Automated rule-based diagnosis through a distributed monitor system. IEEE Trans. Dependable & Secure Computing, 2006.

[17] R. Kompella, J. Yates, A. Greenberg, and A. Snoeren. IP fault localization via risk modeling. In NSDI, 2005.

[18] R. Mahajan, D. Wetherall, and T. Anderson. Understanding BGP misconfiguration. In SIGCOMM, 2002.

[19] Microsoft Operations Manager 2005 Product Overview. http://technet.microsoft.com/en-us/opsmgr/bb321411.aspx.

[20] Performance counters (Windows). http://msdn.microsoft.com/en-us/library/aa373085(VS.85).aspx.

[21] OpenView, HP Technologies Inc. http://www.openview.hp.com.

[22] D. Oppenheimer, A. Ganapathi, and D. Patterson. Why do Internet services fail, and what can be done about it. In USITS, 2003.

[23] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2000.

[24] I. Popivanov and R. Miller. Similarity search over time-series data using wavelets. In ICDE, 2005.

[25] The /proc file system. http://www.faqs.org/docs/kernel/x116.html.

[26] D. Rafiei and A. Mendelzon. Similarity-based queries for time series data. In SIGMOD, 1999.

[27] Y. Su, M. Attariyan, and J. Flinn. AutoBash: Improving configuration management with operating system causality analysis. In SOSP, 2005.

[28] C. Verbowski et al. Flight data recorder: Monitoring persistent-state interactions to improve systems management. In OSDI, 2004.

[29] H. Wang, J. Platt, Y. Chen, R. Zhang, and Y. Wang. Automatic misconfiguration troubleshooting with PeerPressure. In OSDI, Dec. 2006.

[30] Y. Wang, C. Verbowski, J. Dunagan, Y. Chen, H. Wang, and C. Yuan. STRIDER: A black-box, state-based approach to change and configuration management and support. In LISA, 2006.

[31] A. Whitaker, R. Cox, and S. Gribble. Configuration debugging as search: Finding the needle in the haystack. In OSDI, Dec. 2006.

[32] S. Yemini, S. Kliger, E. Mozes, Y. Yemini, and D. Ohsie. High-speed and robust event correlation. IEEE Communications Mag., 1996.

[33] L. Yu and H. Liu. Feature selection for high-dimensional data: A fast correlation-based filter solution. In ICML, 2006.