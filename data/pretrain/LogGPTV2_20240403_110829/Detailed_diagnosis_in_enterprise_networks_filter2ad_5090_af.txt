r
r
o
o
c
c
f
f
o
o
k
k
n
n
a
a
R
R
Coarse
NetMedic
0
20 40 60 80 100
Cumulative % of faults
20
15
10
5
0
e
s
u
a
c
t
c
e
r
r
o
c
f
o
k
n
a
R
5 min
30 min
60 min
90 min
0
20 40 60 80 100
Cumulative % of faults
20
15
10
5
5
0
e
e
s
s
u
u
a
a
c
c
t
t
c
c
e
e
r
r
r
r
o
o
c
c
f
f
o
o
k
k
n
n
a
a
R
R
Coarse
NetMedic
0
20 40 60 80 100
Cumulative % of faults
Figure ʄʃ. E(cid:11)ectiveness of Coarse, NetMedic
when diagnosing two simultaneous faults.
Figure ʄʄ. NetMedic’s e(cid:11)ectiveness
when using di(cid:11)erent history sizes.
Figure ʄʅ. ˆe ranks assigned to abnormal
processes in the absence of injected faults.
ous faults are possible and it is desirable that the diagnostic system
correctly match each e(cid:11)ect to its own likely cause. Here, we inject
two faults simultaneously. With ʄʃ basic faults, there are ʇʈ unique
fault pairs. Of these, @ fault pairs “interfere” in that they impact the
same application processes. We inject the ʆʉ non-interfering pairs
and evaluate our ability to link each e(cid:11)ect to its underlying cause.
Figure ʄʃ shows that with NetMedic the median rank for the
correct cause is one for over @ʃʂ of the cases. Compared to the
single-fault case, there is some degradation in diagnosis e(cid:11)ectiveness,
speci(cid:12)cally in the maximum rank, which represents the worst case
operator experience. ˆere is no deterioration in the median rank,
which represents the average case. ˆese results suggest that even in
the presence of multiple faults NetMedic can o(cid:13)en o(cid:13)en link an e(cid:11)ect
to its correct cause.
In contrast, Coarse does signi(cid:12)cantly worse. ˆe median rank is
one for only ʄʈʂ of the cases. Curiously, compared to the single-fault
results in Figure @(a), Coarse appears to do better in the x-range of
ʅʃ-ʉʃʂ. It is not the case that Coarse is better at diagnosing multiple-
fault cases than single-fault cases. ˆe seemingly better performance
is a result of the fact that the two sets of experiments have di(cid:11)erent
fault type mixtures; the double-fault scenarios have a higher fraction
of faults for which Coarse has modest performance.
7.6 Impact of history
Next, we study the impact of the size of the history on the e(cid:11)ec-
tiveness of NetMedic. Figure ʄʄ shows results with using ʈ–@ʃ min-
utes of history. We see that using ʆʃ or @ʃ minutes performs as well
as our previous experiments that use ʉʃ minutes of history. Using
ʈ minutes of history performs signi(cid:12)cantly worse; ʄʃʂ of the faults
have median ranks above ʅʃ. Based on these results we conclude that
ʆʃ minutes of historical data su(cid:14)ces for most faults. Recall that this
history does not need to be fault-free and our experiments use history
that contains other faults.
Preliminary evidence suggests that using history from more dy-
namic periods (e.g., day versus night) helps discount spurious con-
nections between components better. Investigating the nature of his-
tory that works best in various settings is a subject of ongoing work.
7.7 In situ behavior
We conclude our evaluation with evidence that NetMedic can help
with naturally occurring faults as well. Consider a common scenario
for a process: plenty of available resources on the host machine, the
network appears normal, and the relevant con(cid:12)guration elements
have not changed recently. If the process appears abnormal in this
scenario, a good diagnostic method should blame the process itself
for this abnormality rather than, for instance, other processes. We
evaluate NetMedic on exactly this scenario, i.e., on ruling out impact
among components that happen to be abnormal simultaneously. We
focus this analysis on interactions within monitored desktops since
we could not monitor the real servers in our organization.
Figure ʄʅ shows the rank assigned by Coarse and NetMedic to ab-
normal processes. ˆis data is based on a (cid:12)ve hour monitoring period
during which none of our own clients are running. We randomly se-
lect ten one-minute intervals to diagnose and use ʆʃ-minute long his-
tory for each. In @ʈʂ of the cases, NetMedic blames the process itself
for its abnormality while Coarse does so for only ʈʂ of the cases. Our
monitored desktops are not resource constrained during most of this
monitored period. ˆe inferences of NetMedic are more consistent
than Coarse for this setting.
We manually examine many cases in which NetMedic assigns a
high rank to an a(cid:11)ected process. In nearly all of them, the top ranked
cause is a virus scanning process or a sync utility process. In our
deployment environment, such processes o(cid:13)en hog resources over
short durations and NetMedic appears to correctly blame these pro-
cesses rather than the a(cid:11)ected process.
8. SCALING NetMedic
While motivated by problems inside small enterprises, NetMedic
can also help large enterprises that su(cid:11)er similar problems if it can
be scaled up. ˆere are two challenges in scaling NetMedic; we be-
lieve that both are surmountable and addressing them is part of our
future work. ˆe (cid:12)rst challenge is to carry out diagnosis-related
computation over large dependency graphs. ˆe primary bottleneck
in this computation is calculating component abnormality and edge
weights. ˆese calculations are parallelizable as they can be done in-
dependently for each edge and component. In fact, they can also be
distributed to the machines that are being monitored because the vast
majority of dependency edges are between components on the same
machine (e.g., between a process and its host machine). Further, the
calculation of individual edge weights can be sped up through fast
correlation methods [ʆʆ]. Once the edge weights are computed, the
remaining calculations are those that underlie ranking likely causes.
ˆese calculations are akin to those of Sherlock [ʅ] in their simplicity
and can thus scale to very large graphs.
ˆe second challenge in large deployments is that of data col-
lection, storage and retrieval. For this challenge, we can leverage
the large body of existing work on managing data that is similar to
ours. ˆis body includes methods for very-lightweight data collec-
tion [ʆ, ʅ@], for e(cid:14)cient data compression [@, @], and for searching
(compressed) history for similar states [ʄʈ, ʅʇ, ʅʉ].
9. RELATED WORK
Diagnosing faults in computer networks is hard and has thus wit-
nessed much commercial and research activity. We divide related
work into four broad categories.
Inference-based: ˆese systems identify the faulty component
based on a model of dependencies among components [ʅ, ʄ@, ʆʅ].
Because they target large-scale networks, the focus of existing sys-
tems is scalable analysis with simple models. NetMedic provides de-
tailed diagnosis in small business networks and di(cid:11)ers in both the
challenges that it overcomes (e.g., unknown variable semantics) and
in the way it models components and dependencies.
Rule-based: ˆese systems, which are also known as expert sys-
tems, diagnose based on a set of pre-programmed rules [ʇ, ʄʄ, ʄʉ, ʅʄ].
ˆeir main limitation is a lack of generality: they only diagnose faults
253for which they have been pre-programmed. Because building a rule
database that covers a large fraction of possible faults in a complex
network is di(cid:14)cult, we chose an inference-based approach.
Classi(cid:12)er-based: ˆese systems train o(cid:15)ine on healthy and un-
healthy states, and when current system state is fed they try to de-
termine if the system is unhealthy and the likely cause [ʉ, @]. It is
unclear how such schemes fare on faults not present in the training
data and extensive training data is hard to get. Some systems attempt
to overcome the “unknown fault” limitation of learning approaches
by training on data from multiple networks [ʄ, ʅ@]. A recent exam-
ple is NetPrints, which detects home router con(cid:12)gurations that are
incompatible with applications. ˆis approach is enabled by the fact
that the set of compatible router con(cid:12)gurations is much smaller than
the number of sharing networks. It may not, however, generalize to
more complex con(cid:12)gurations or to other kinds of faults. For instance,
it is di(cid:14)cult to diagnose performance faults using information from
other networks because performance is a complex function of the
speci(cid:12)c hardware, so(cid:13)ware, and topology used by a network.
Single-machine: While we focus on diagnosing faults across ma-
chines in a network, there is extensive work on diagnosing faults
within individual machines [ʆ, ʅ@, ʅ@, ʆʃ, ʆʄ]. NetMedic borrows
liberally from this body of work, especially in the light-weight yet
extensive data gathering, con(cid:12)guration monitoring and the use of
system history. However, cross-machine diagnosis presents unique
challenges and single-machine diagnosis methods o(cid:13)en do not di-
rectly translate. Like NetMedic, Strider [ʆʃ] uses historical state dif-
ferences. It represents a machine with a single vector that contains
all Windows registry entries and detects faulty entries by di(cid:11)erencing
this vector across time and across other similar machines. NetMedic
considers a broader and noisier input (e.g., application performance),
includes component interactions in its analysis, and detects a wider
range of faults. It also does not rely on controlled, repeated executions
of the troubled application to infer which state variables are relevant.
10. CONCLUSIONS
NetMedic enables detailed diagnosis in enterprise networks with
minimal application knowledge. It was motivated by what to our
knowledge is the (cid:12)rst study of faults in small enterprises. It combines
a rich formulation of the inference problem with a novel technique to
determine when a component might be impacting another. In our ex-
periments, it was highly e(cid:11)ective at diagnosing a diverse set of faults
that we injected in a live environment.
Modern operating systems and applications export much detailed
information regarding their behavior. In theory, this information can
form the basis of highly e(cid:11)ective diagnostic tools. In reality, the tech-
nology was lacking. One class of current systems uses the seman-
tics of this information to diagnose common faults based on pre-
programmed fault signatures [ʄ@]. Another class focuses exclusively
on certain kinds of faults such as performance that do not require
this information [ʅ]. Even in combination these two classes of tech-
niques are unable to diagnose many faults that enterprise networks
su(cid:11)er. ˆe techniques developed in our work are a step towards (cid:12)ll-
ing this void. ˆey enable diagnosis of a broad range of faults that are
visible in the available data, without embedding into the system the
continuously evolving semantics of the data.
Acknowledgments: We are grateful to Parveen Patel for his assis-
tance with implementing data collection in NetMedic and to our col-
leagues who let us deploy NetMedic on their desktops. We also thank
our shepherd, Darryl Veitch, Alex Snoeren, and the SIGCOMM re-
viewers for helping improve the presentation of this paper.
11. References
[ʄ] B. Aggarwal, R. Bhagwan, T. Das, S. Eswaran, V. Padmanabhan, and
G. Voelker. NetPrints: Diagnosing home network miscon(cid:12)gurations
using shared knowledge. In NSDI, ʅʃʃ@.
[ʅ] P. Bahl, R. Chandra, A. Greenberg, S. Kandula, D. A. Maltz, and
M. Zhang. Towards highly reliable enterprise network services via
inference of multi-level dependencies. In SIGCOMM, Aug. ʅʃʃ@.
[ʆ] S. Bhatia, A. Kumar, M. Fiuczynski, and L. Peterson. Lightweight,
high-resolution monitoring for troubleshooting production systems.
In OSDI, ʅʃʃ@.
[ʇ] S. Brugnoni, G. Bruno, R. Manione, E. Montariolo, E. Paschetta, and
L. Sisto. An expert system for real time fault diagnosis of the Italian
telecommunications network. In IFIP, ʄ@@ʆ.
[ʈ] M. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer. Pinpoint:
Problem determination in large, dynamic Internet services. In DSN,
June ʅʃʃʅ.
[ʉ] M. Chen, A. X. Zheng, J. Lloyd, M. I. Jordan, and E. Brewer. Failure
diagnosis using decision trees. In ICAC, ʅʃʃʇ.
[@] I. Cohen, M. Goldszmidt, T. Kelly, J. Symons, and J. Chase.
Correlating instrumentation data to system states: a building block
for automated diagnosis and control. In OSDI, ʅʃʃʇ.
[@] A. Deligiannakis, Y. Kotidis, and N. Roussopoulos. Compressing
historical information in sensor networks. In SIGMOD, ʅʃʃʇ.
[@] M. Garofalakis and P. B. Gibbons. Wavelet synopses with error
guarantees. In SIGMOD, ʅʃʃʅ.
[ʄʃ] J. Gray. Why do computers stop and what can be done about it? In
Sym. on Reliability in Distributed So(cid:13)ware and Database Systems,
ʄ@@ʉ.
[ʄʄ] Gteko, Inc. http://www.gteko.com.
[ʄʅ] W. Hamscher, L. Console, and J. de Kleer, editors. Readings in
model-based diagnosis. Morgan Kaufmann Publishers Inc., ʄ@@ʅ.
[ʄʆ] D. Heckerman. Learning in Graphical Models, chapter A tutorial on
learning with Bayesian networks. MIT Press, ʄ@@@.
[ʄʇ] A. Hyvarinen and E. Oja. Independent component analysis:
Algorithms and applications. Neural Networks, ʄʆ(ʇ-ʈ), ʅʃʃʃ.
[ʄʈ] H. Jagadish, A. Mendelzon, and T. Milo. Similarity-based queries. In
PODS, ʄ@@ʈ.
[ʄʉ] G. Khanna, M. Cheng, P. Varadharajan, S. Bagchi, M. Correia, and
P. Verissimo. Automated rule-based diagnosis through a distributed
monitor system. IEEE Trans. Dependable & Secure Computing, ʅʃʃ@.
[ʄ@] R. Kompella, J. Yates, A. Greenberg, and A. Snoeren. IP fault
localization via risk modeling. In NSDI, ʅʃʃʈ.
[ʄ@] R. Mahajan, D. Wetherall, and T. Anderson. Understanding BGP
miscon(cid:12)guration. In SIGCOMM, ʅʃʃʅ.
[ʄ@] Microso(cid:13) operations manager ʅʃʃʈ product overview.
http://technet.microso(cid:13).com/en-us/opsmgr/bbʇ@@ʅʇʇ.aspx.
[ʅʃ] Performance counters (Windows).
http://msdn.microso(cid:13).com/en-us/library/aaʆ@ʆʃ@ʆ(VS.@ʈ).aspx.
[ʅʄ] Open view, HP technologies inc. http://www.openview.hp.com.
[ʅʅ] D. Oppenheimer, A. Ganapathi, and D. Patterson. Why do Internet
services fail, and what can be done about it. In USITS, ʅʃʃʆ.
[ʅʆ] J. Pearl. Causality : Models, Reasoning, and Inference. Cambridge
University Press, ʅʃʃʃ.
[ʅʇ] I. Popivanov and R. Miller. Similarity search over time-series data
using wavelets. In ICDE, ʅʃʃʅ.
[ʅʈ] ˆe /proc (cid:12)le system. http://www.faqs.org/docs/kernel/x@ʄʉ.html.
[ʅʉ] D. Ra(cid:12)ei and A. Mendelzon. Similarity-based queries for time series
data. In SIGMOD, ʄ@@@.
[ʅ@] Y. Su, M. Attariyan, and J. Flinn. AutoBash: improving con(cid:12)guration
management with operating system causality analysis. In SOSP, ʅʃʃ@.
[ʅ@] C. Verbowski et al. Flight data recorder: monitoring persistent-state
interactions to improve systems management. In OSDI, ʅʃʃʉ.
[ʅ@] H. Wang, J. Platt, Y. Chen, R. Zhang, and Y. Wang. Automatic
miscon(cid:12)guration troubleshooting with PeerPressure. In OSDI, Dec.
ʅʃʃʇ.
[ʆʃ] Y. Wang, C. Verbowski, J. Dunagan, Y. Chen, H. Wang, and C. Yuan.
STRIDER: A black-box, state-based approach to change and
con(cid:12)guration management and support. In LISA, ʅʃʃʆ.
[ʆʄ] A. Whitaker, R. Cox, and S. Gribble. Con(cid:12)guration debugging as
search: Finding the needle in the haystack. In OSDI, Dec. ʅʃʃʇ.
[ʆʅ] S. Yemini, S. Kliger, E. Mozes, Y. Yemini, and D. Ohsie. High speed
and robust event correlation. IEEE Communications Mag., ʄ@@ʉ.
[ʆʆ] L. Yu and H. Liu. Feature selection for high-dimensional data: A fast
correlation-based (cid:12)lter solution. In ICML, ʅʃʃ@.
254