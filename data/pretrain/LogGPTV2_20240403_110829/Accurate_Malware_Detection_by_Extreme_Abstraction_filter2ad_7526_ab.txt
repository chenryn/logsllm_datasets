time-stamp counter, which counts clock cycles, and can be used by
malware to detect that it is being analyzed. We treat it abstractly as
if it were an API call of the abstract operating system as described
below. Another example is the instruction cpuid, which reveals
details about the hardware; here again we model this instruction
abstractly in the same way we treat API functions that reveal details
of the underlying hardware. The input to the abstract operating
system is a sample or a sample bundle containing the sample plus
additional related les – to support the case that a sample depends
on other les existing in the system (for instance in the case of DLL
hijacking [30]). The output is an analysis result to be passed on to
a classier.
We run the dynamic analysis of Windows PE (Portable Exe-
cutable) les on Linux, which makes us less vulnerable than other
tools to attacks that try to escape the analysis environment. While
we perform software emulation, we do use the underlying processor
to support some of the x86 instructions, among which are oat-
ing point instructions, MMX, and XMM instructions. Our software
emulation is orders of magnitude slower than a real processor, but
on the other hand our abstract implementation of the operating
system is orders of magnitude faster than the real thing. Overall the
performance quite heavily depends on the ratio of the number of
instructions invoked directly by the sample to the number invoked
by the operating system. As we show in our experimental results,
the resulting performance is fast enough to allow the classiers
to detect malware reliably in a system expected to analyze tens or
hundreds of thousands of samples per day.
Extreme abstraction may cause our system to take paths that
are not feasible in the original code, or to miss real paths in it,
and thus intuitively, extreme abstraction should not work. Careful
analysis has led us to see that our solution can be understood as
an extremely buggy version of exact emulation, and that despite
anti-research techniques, malicious behavior is robust to buggy
emulation in surprising ways. For example, taking an infeasible
path often causes an exception, but this can be to our advantage
if the sample hides its malicious behavior inside of an exception
handler. Similarly, we have seen that in samples that spawn multiple
threads or processes, we often reach malicious behavior sooner than
exact emulation due to some of the threads or processes terminating
early.
2.1 Implementation details
The abstract OS supports just over 100,000 API functions from over
150 DLLs, most of them in a completely abstract manner in the
sense that they ignore their parameters, return a random value, and
write random values to their output parameters. The exceptions are
memory manipulation functions such as strcpy() or memmove(),
that must be implemented exactly, and other functions such as
reading or writing les, reading from or writing to the registry or
spawning threads or processes, which must be given some concrete
behavior. For those that must be implemented exactly, we simply
“call" (i.e., emulate) the machine code as it appears in the DLL, so
there is no cost to implementing this class of functions beyond
keeping a list; we implement approximately 7,000 functions in this
way. For the others, of which there are approximately 1,500, we
have implemented concrete behavior that mimics the real function
in a minimal way but does not attempt to provide anywhere near
identical behavior. For example, functions that access the network
are left almost completely abstract – they never actually access the
network, and the only behavior they are given is the minimum nec-
essary so that they appear to access the internet with a reasonable
ratio of successful to unsuccessful connections, according to some
heuristic denition of reasonable, and likewise appear to send and
receive a reasonable amount of data.
In addition, there are several classes of functions the behavior
of which cannot be left completely abstract. Each of these includes
functions that can be used for obfuscation or anti-research in a
particular way. One example is the class of functions with hidden
side eects. For example, the purpose of function SetErrorMode is
to set the error mode, and it appears to be an easy candidate for ab-
straction. However, the return value of this function is the previous
value of the error mode. Therefore, the following assignment:
x = y;
can be implemented (directly by the malware writer or more prob-
ably by an obfuscating compiler) as follows:
SetErrorMode(y);
x = SetErrorMode(arbitrary_value);
103
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
F. Copty et al.
This class of functions is given minimal behavior in order to support
such use.
Furthermore, we give behavior to API functions that malware
may use to try to mask malicious behavior. For example, DLL injec-
tion [30] can be performed using functions WriteProcessMemory
and CreateRemoteThread, 5 thus we give behavior to these func-
tions. As another example, process hollowing [17] can be performed
using low level API functions NtQueueApcThread and NtMapView-
OfSection 6 or some other variations of code injection, 7 thus we
give these functions behavior as well.
Another class of functions that cannot be left completely abstract
are those for which a return value of 0 indicates success. For these
functions, it is often sucient to supply just a return code of 0, al-
though sometimes slightly more behavior is necessary. For example,
many malware samples deliberately call esoteric API functions with
bad parameters, aborting or diverting to benign behavior in case the
functions do not respond with the expected error code. For these
functions we have found that providing a minimal implementation
usually suces.
Implementing 1,500 functions, even partially, is no small amount
of work. Given that, the natural question is why go to so much
eort, when virtualization [18] or full system emulation [16] are
alternatives that avoid the need to do so? The answer is that there is
a tradeo – both virtualization and full system emulation avoid the
need to implement API functions, but impose their own implemen-
tation challenges. Furthermore, avoiding anti-research techniques
in an abstract system is often automatic, compared to other ap-
proaches. For example, a malware sample looking for hooks to the
system will not nd any in our abstract OS. Another example is
the use of FindClose and SCardIntroduceCardType to detect a
debugger.8 When given invalid parameters, these functions cause a
crash under a debugger, but when not being debugged they simply
set an error and return.
Other anti-research techniques are very easy to discover and
protect against. For example, our abstract OS is easily detectable
by malware samples that call API functions with bad parameters,
looking for the correct error codes. On the other hand, comparing
the run of a sample on our abstract OS vs. the run on a virtual
machine quickly uncovers such cases, even when the sample does
not show its malicious behavior on the virtual machine for other
reasons. Given that adding minimal behavior for such a function
suces, it is quick and easy to enhance the abstract OS for new
families of such functions.
2.2 Multiple paths
Exploring a single path through a suspicious sample is often not suf-
cient to uncover malicious behavior, due to various anti-research
techniques used by malware writers. For example, a sample might
behave maliciously only on a machine with a specic program
installed, or only if the type of keyboard attached to the machine
hints that it is being run in a particular country. Thus it is often
necessary to explore multiple paths through the sample in order to
get it to elicit its malicious behavior. Furthermore, even if we do not
5MD5: 3914fbfdd038a5b7bc0f6b6aebb03aa0
6MD5: 226060599c81c71c3fd40923cb2b4560
7MD5: 556b41e0a01aea538ab121fc8b37dd80
8MD5: c30db0eb32d800f6718bf62de1632640
succeed in seeing malicious behavior on any of the paths, the fact
that a sample behaves dierently on dierent paths is important
information for the machine learning classier, even if we have not
run the analysis long enough to reach malicious behavior.
The typical way to explore multiple paths is through symbolic ex-
ecution [13], and indeed our original intent was to over-approximate
the real behavior of the system using non-determinism in a symbolic
execution based tool. Symbolic execution is a method of executing
a program in which inputs (or, in the case of malware analysis,
outputs from certain OS calls) are symbolic, that is, represent a set
of concrete values instead of just a single one. Symbols propagate
during execution, and each variable x is implemented as a data
structure representing the set of possible values that x may take,
as a function of the inputs. When a branch (e.g., if statement) is
dependent on a symbolic expression, say x >   where one or both
of x and   are symbols, some kind of a solver such as a SAT [8, 9] or
SMT [2] solver is invoked in order to determine whether only one
or both of the branches are satisable. Then the execution continues
along all the satisable paths, dealing with the case that there are
two of them either by forking or by execution of a single path with
backtracking, with each path remembering its path condition, in
our case either x >   or x   .
Symbolic execution is a powerful tool, and has been used suc-
cessfully for malware analysis [20], but its disadvantage is that it is
computationally expensive, due both to the cost of manipulating
the symbols and especially of calling the solver, as well as to the
fact that the number of paths may be exponential in the number of
branches, a problem known as path explosion. We are interested in
building a tool that can deal with the reality of tens of thousands
of samples a day, thus full symbolic execution is not an option.
Furthermore, full symbolic execution is overkill given the extreme
abstraction of the rest of our system.
Lightweight symbols. As an alternative, we implemented light-
weight symbols, which can be understood as a kind of bitwise
taint [28] that drives the exploration of multiple paths when a
tainted value is branched on. Each lightweight symbol consists of a
concrete value and a variable of the same size whose bits indicate
whether or not the value is symbolic. We implemented lightweight
symbols as a set of classes that instrument the rest of the code by
overloading the standard C++ operators such as =, &, +, etc. The
symbolic part can be turned o using a compilation switch, which
gives us two versions of the code, one symbolic and one not.
When an operation on a variable is encountered, and when the
symbolic code is active, the symbolic class performs the propagation
in a way very similar to traditional taint analysis. For example, the
following code overloads the &= operator:
template 
inline Symbolic&
Symbolic::operator&=(const Symbolic &rhs)
{
m_isSymbol = (m_isSymbol & rhs.m_isSymbol) |
(m_isSymbol & rhs.m_value) |
(rhs.m_isSymbol & m_value);
m_value &= rhs.m_value;
return *this;
}
104
Accurate Malware Detection by Extreme Abstraction
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
where m_value and m_isSymbol are member variables of the class
Symbolic and represent the concrete value and the symbolic
bits, respectively. A resulting bit is symbolic if both the m_isSymbol
bit of this and the corresponding operand bit rhs.m_isSymbol
are symbolic, or if one of them is symbolic and the other has the
value 1.
Figure 5: Architecture of TAMALES
model, time ticks steadily, but with random jumps every N cycles.
We use three dierent values of N, diering from one another by
an order of magnitude. We chose these three values because we
found empirically that while no single value of N is eective in
overcoming sophisticated time-based anti-research, conversely we
did not nd a sample that was impervious to all three.
3 MALWARE CLASSIFICATION
The problem of Windows malware detection can be dened as:
given a Windows PE le (EXE/DLL), decide whether that le is
benign or malicious. For many samples this is easily decided, but
sophisticated samples (either benign or malicious) that try to hide
their behavior may take more computing resources (paths and/or
runtime) to decide. In order to minimize computing costs but still
achieve an acceptable malware detection rate, TAMALES performs
malware detection using a funnel-like conguration of layers as
shown in Figure 5. We start with a single path analysis with a short
timeout, feeding the results into a classier suitably trained. Unlike
a classical machine learning classier which has a score threshold
T and two classes (malware if score > T, benign if score  T), we
use a two-threshold classier. We dene two thresholds TL and TH
such that 0  TH , the sample is predicted
to be a malware; if score < TL, the sample is predicted to be benign;
otherwise the sample is undecided and continues to the next layer.
Each layer uses its own classier, trained on the same sample set
but with a varying number of paths and/or a dierent timeout. The
nal classier classies into only two classes, acting as the nal
arbiter in case every previous layer was undecided. The idea is that
the rst classier can often conclude that a sample is benign or
malicious based on a short, single-path run, thus saving computing
resources by leaving the heavier analyses to run on a constantly
decreasing pool of samples.
3.1 Features
The abstract analysis serves as a component of TAMALES from
which features for machine learning are extracted. First, we extract
approximately a hundred static features from a sample, including
entropy of the code and data sections, whether there is a discrepancy
between the checksum as recorded in the header and the checksum
105
In traditional symbolic execution, a symbol is born from an input
to the system, and a branch causes a call to a solver. In TAMALES, a
symbol is born from the random values constituting the abstraction,
and a branch on a symbolic value simply causes us to take both
branches. We do not attempt to solve satisability (in any case
we do not have enough information to do so), and furthermore
we do not attempt to choose a dierent m_value for the path that
would not be taken by the current m_value (once again, we do
not have enough information to do so). This behavior causes us to
take branches that might not be feasible in the original code. For
example, if a sample calculates an opaque constant [19], resulting