title:Fidelity: A Property of Deep Neural Networks to Measure the Trustworthiness
of Prediction Results
author:Ziqi Yang
POSTER: Fidelity: A Property of Deep Neural Networks to
Measure the Trustworthiness of Prediction Results
Ziqi Yang
National University of Singapore
PI:EMAIL
ABSTRACT
With the increasing performance of deep learning on many security-
critical tasks, such as face recognition and malware detection, the
security issues of machine learning (ML) have become increas-
ingly prominent. Recent studies have shown that deep learning is
vulnerable to adversarial examples: carefully crafted inputs with
negligible perturbations on legitimate samples could mislead a deep
neural network (DNN) to produce adversary-selected outputs while
humans can still correctly classify them. Therefore, we need an
additional measurement on the trustworthiness of the results of a
machine learning model, especially in adversarial settings. In this
paper, we analyse the root cause of adversarial examples, and pro-
pose a new property, namely fidelity, of machine learning models
to describe the gap between what a model learns and the ground
truth learned by humans. One of its benefits is detecting adversarial
attacks. We formally define fidelity, and propose a novel approach
to quantify it. We evaluate the quantification of fidelity of DNNs
in adversarial settings on two neural networks. Our preliminary
results show that involving the fidelity enables a DNN system to
detect adversarial examples with true positive rate 97.7%, and false
positive rate 1.67% on a studied DNN model.
CCS CONCEPTS
• Security and privacy → Domain-specific security and pri-
vacy architectures; • Computing methodologies → Neural
networks.
KEYWORDS
adversarial example; neural network; transferability
ACM Reference Format:
Ziqi Yang. 2019. POSTER: Fidelity: A Property of Deep Neural Networks to
Measure the Trustworthiness of Prediction Results. In ACM Asia Conference
on Computer and Communications Security (AsiaCCS ’19), July 9–12, 2019,
Auckland, New Zealand. ACM, New York, NY, USA, 3 pages. https://doi.org/
10.1145/3321705.3331005
1 INTRODUCTION
Deep neural networks (DNNs) have been applied and achieved
excellent performance on many tasks. When DNNs are deployed
in security-critical settings, such as face recognition and malware
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
AsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6752-3/19/07.
https://doi.org/10.1145/3321705.3331005
detection [1, 5, 11], designers of these systems make implicit secu-
rity assumptions about DNNs. For instance, the result, including
confidence values with predications, in a classification task is as-
sumed to be trusted. However, this assumption is broken by recent
work in security and machine learning communities. It has been
demonstrated that adversaries can force machine learning mod-
els, including DNNs, to output adversary-selected targets using
carefully crafted inputs, namely adversarial examples [4, 8, 10].
Motivated by adversarial attacks to DNNs, we analyse the root
cause of adversarial examples. In a classification task, DNNs can
output confidence values for predictions [3], trying to make the
predictions analysable. However, how much confidence does a
DNN model have in such confidence values remains a question.
In other words, how much fidelity to the ground truth or human
knowledge do these results have, which reflects the trustworthiness
of these results. More precisely, ML models learn from training data.
Adversarial examples are demonstrated not naturally drawn from
the training data distribution [4], but we believe they are drawn
from the underlying population distribution, because they are so
close to legitimate samples that humans can correctly classify them.
Therefore, the training data does not perfectly reflect the underlying
population, and thus a gap exits between the training data (precisely,
what a model learns) and the underlying population.
In this work, we propose a new property, namely fidelity of
DNNs to name the gap. We formally define fidelity, and propose an
approach to measure it. We evaluate fidelity in DNN models, but
we believe that it also exists in other ML models as long as they
are vulnerable to adversarial examples. Note that the significance
of fidelity is not to increase the robustness of DNNs to adversarial
examples [2, 9], but to provide an additional value to measure the
trustworthiness of its results, which can be used to detect adversar-
ial attacks to DNN-based systems.
2 FIDELITY DEFINITION
Given a model H, the output class of an input sample x is y = H(x).
It follows the model distribution Pmodel(y|x) learned by the model.
The fidelity F(x, y; H) of the model H with respect to an input
sample x is defined as follows:
F(x, y; H) = 1 − |Pmodel(y|x) − Ppop(y|x)|
(1)
where Ppop(y|x) is the probability distribution of y given x in the
population. It is easy to get F(x, y; H) ∈ [0, 1]. A higher fidelity
indicates the output y with respect to the input x matches the
population distribution more closely, and a lower fidelity is contrary.
Note that fidelity is significantly different from confidence values
(probabilities) of predictions. The confidence values are computed
following the model distribution Pmodel(y|x). while fidelity is a
property to reflect to how much degree the predictions match the
population distribution Ppop(y|x).
Poster PresentationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand676Table 1: Accuracy of DNN and Pool Models
Model
DNN MNIST
CIFAR10
KNN
LR
L-SVM
R-SVM
DT
RF
AdaB
GNB
QDA
Average
Pool
Test
99.33
-
96.65
91.98
94.74
98.36
87.8
97.0
72.99
55.58
14.46
78.84
Figure 1: Overview of our work: providing an additional
value to measure the fidelity of DNNs. Fidelity is able to de-
tect adversarial attacks.
MNIST
FGSM JSMA Test
9.97
-
80.47
28.11
36.44
15.15
15.57
20.19
13.52
10.4
11.35
25.69
6.2
-
85.18
68.61
69.46
77.04
39.97
81.96
53.92
38.19
14.00
58.70
-
82.35
38.59
38.83
38.54
18.28
26.75
47.10
31.08
29.76
36.24
33.90
CIFAR10
FGSM JSMA
-
14.08
36.05
22.67
34.53
14.75
21.01
40.26
26.49
29.8
10.01
26.17
-
9.54
38.45
28.26
37.67
16.25
25.63
46.34
30.27
29.53
9.27
29.07
Figure 2), and then uniformly sample perturbations in such space
to measure its boundaries. We propose a new metric symmetric
transfer rate to measure the transferability of adversarial examples
sampled from the adversarial space. The transferability between
a pair of models is able to reflect the divergence between them in
adversarial settings. We qualify jury members according to their
accuracy in normal settings (on benign samples) because a random
classifier could be very diverse from a well-working model, but it is
not a working model. Therefore, we need to qualify a jury member
first before involving it.
4 PRELIMINARY RESULTS
We evaluate the effectiveness of the fidelity estimation method by
detecting adversarial samples on two canonical ML datasets: the
MNIST [7] and CIFAR10 [6] datasets. We train two deep convolu-
tional neural networks on them. We use both FGSM [4] and JSMA
[8] approaches to generate adversarial samples for DNN models.
We randomly train 9 traditional ML models as the predefined model
pool, from which we select the jury. We evaluate the model pool on
the original test set, the FGSM set and the JSMA set. The accuracy
of each model on each test set is presented in Table 1. We can see
that though the 9 models are affected by the adversarial examples,
they are generally more robust against them than the DNN model.
4.1 Jury Can Estimate Fidelity
We test whether a jury can estimate fidelity by measuring the