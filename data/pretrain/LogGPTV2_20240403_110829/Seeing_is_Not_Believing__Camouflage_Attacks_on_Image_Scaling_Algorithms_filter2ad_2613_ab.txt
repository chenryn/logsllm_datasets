Torch, and we have found all of them implicitly trigger scal-
ing functions in their data processing pipelines. Appendix B
provides several examples.
3 Related Work
This section brieÔ¨Çy reviews some related work and makes a
comparison with our approach.
Information Hiding
3.1
Information hiding is a signiÔ¨Åcant topic in information secu-
rity [2,9,15,18,19,21,27,30,31]. Information hiding methods
achieve reversible data hiding by image interpolation, but
these are different from our attack method: First, the goals
are different. The information hiding methods conceal data
in a source image to make the secret information unnoticed
by human, and image applications operate on the complete
data. Our presented attack hides a target image in a source
image to cause a visual cognition contradiction between hu-
man and image applications. The core components (such as
deep learning classiÔ¨Åers) of the victim applications operate
on a partial data (i.e. the scaling output). Second, informa-
tion hiding efforts often impose a customized coding method
(such as LSB and NIP [21]) in order to conceal and recover
hidden information. This coding scheme is often kept as a
secret known only to the designer of the speciÔ¨Åc information
hiding method. In contrast, a scaling attack is based on the
interpolation algorithm built within the victim application to
achieve the deceiving effect. The main task for an attacker is
to reverse engineering the scaling algorithms and design the
pixel replacement policy.
3.2 Adversarial Examples
The research of adversarial examples attract growing public
attentions with the reviving popularity of ArtiÔ¨Åcial Intelli-
gence. An adversarial image fools the ArtiÔ¨Åcial Intelligence
by inserting perturbations into the input image, which are
hard to be noticed by human eyes. For example, Goodfellow
et al. [14] presented a linear explanation of adversarial ex-
amples and revealed that such attack is effective for current
sufÔ¨Åciently linear deep networks. In addition to the theoretical
analysis, Alexey et al. [20] added the perturbations into the
physical world and successfully launched the attack. It should
be noted that the attack target of existing adversarial exam-
ples essentially aims at machine learning models, while our
method focuses on the data preprocessing step, concretely, the
image scaling action. Vulnerabilities in code implementation,
such as control-Ô¨Çow hijacking, also could lead to recognition
evasions [32]. However, we exploit the weakness of scaling
algorithms in this work other than code implementation.
Invisible/Inaudible Attacks
3.3
Some researchers investigate potential attacks beyond the
human sensing ability. Ian Markwood et al. [22] showed a
content masking attack against the Adobe PDF standard. By
tampering the font Ô¨Åles, the attacker can insert secret infor-
mation into PDF Ô¨Åles without being noticed by human. They
demonstrated such attack against state-of-the-art information-
based services. Besides the attacks in vision Ô¨Åelds, Zhang et
al. [34] presented the DolphinAttack against speech recogni-
tion (SR) systems. They created secret voice commands on
ultrasonic carriers that are inaudible for human beings, but
can be captured and sensed by voice controllable systems. In
this way, an attacker can control SR systems ‚Äúsilently‚Äù. It has
been proved that the widely used SR systems, like Apple Siri
and Google Now, are vulnerable to such attack. Our attack is
like a reverse of such invisible/inaudible attacks. The attacker
leverage the difference between the input and output of the
scaling function. Most part of the content visible to human is
not consumed by the component that uses the scaling output.
4 Formalizing the Scaling Attack
This section describes the goal of a typical scaling attack and
how we design a method to automatically create attack images
with deceiving effects. The autonomous attack image crafting
446    28th USENIX Security Symposium
USENIX Association
Table 2: Scaling algorithms in deep learning frameworks.
DL Framework
Library
Interpolation Method
Caffe
OpenCV [7]
TensorÔ¨Çow c
Python-OpenCV
Pillow [10]
TensorÔ¨Çow.image
Torch
Torch-OpenCV
Nearest(IPillow)
(IPython-OpenCV, TensorÔ¨Çow.image)
Nearest
Bilinear(I)
Bicubic
Area
Lanczos
Bilinear
Bicubic
Area
Lanczos
Nearest
Bicubic
Area
Lanczos
Bilinear(I)
Order a
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
H!V
Validation b
3
3
3
=
=
3
3
3
=
=
3
3
3
=
=
I The default scaling algorithm.
a H!V means the algorithm Ô¨Årst scales horizontally and then vertically.
b The validation is performed on attack with a constraint e = 0.01. 3 represents generate attack images successfully
satisfying the constraints. = means we have not yet veriÔ¨Åed the attack effects because the algorithm is complex and
rarely used in DL applications. More details please see Section 6.1.
c TensorÔ¨Çow integrates multiple image processing packages, including Python-OpenCV, Pillow, and TensorÔ¨Çow.image.
‚Ä¢ source image (or srcImg) Sm‚á§n ‚Äì the image that an at-
tacker wants the attack image to look like.
‚Ä¢ attack image (or attackImg) Am‚á§n ‚Äì the crafted image
eventually created and fed to the scaling function.
‚Ä¢ output image (or outImg) Dm0‚á§n0 ‚Äì the output image of
the scaling function.
‚Ä¢ target image (or targetImg) Tm0‚á§n0 ‚Äì the image that the
attacker wants the outImg to look like.
In some cases, some of these objects are identical. For
example, it is often possible for an attacker to generate an out
image that is identical to the target image.
The process of performing a scaling attack is to craft an
attackImg under visual similarity constraints with srcImg
and targetImg. Based on the intent and constraints on source
images, we deÔ¨Åne the image scaling attack into two attack
modes.
The Ô¨Årst attack mode is when both the source and target
images are speciÔ¨Åed, i.e. the attacker wants to scale an im-
age that looks like a speciÔ¨Åc source image to an image that
looks like a speciÔ¨Åc target image. In this mode the attacker
launches a source-to-target attack, where the semantics of
srcImg and targetImg are controlled as he/she wants. How-
ever, posing constraints on the looks of both the source and
target images makes this attack mode more challenging. We
call this mode of attack the strong attack form.
Figure 5: Automatic attack image crafting.
framework is shown in Fig.5, and details are presented in
Section 4.2.
4.1 Attack Goals
The goal of the scaling attack is to create a deceiving effect
with images. Here the deceiving effect refers to the case that
an image partially or completely presents a different meaning
to humans before and after a scaling action. In such case, we
call the input Ô¨Åle to the scaling action the attack image.
To describe the process of a scaling attack, we deÔ¨Åne the
following four conceptual objects involved in one attack.
USENIX Association
28th USENIX Security Symposium    447
srcImg(56*56)targetImg(28*28)CamouflageAttackattckImg(56*56)ScaleFunc()outImg(28*28)+‚àÜùüè+‚àÜùüêThe second attack mode is when only the target image is
speciÔ¨Åed. In that case, the attacker just wants to cause a vision
contradiction during image scaling, as long as it is related
to a certain concept (such as any images of sheep). In some
extreme cases, the image content could be meaningless, e.g.,
just to create a negative result to an image classiÔ¨Åer. Without
a speciÔ¨Åc source image, the attacker‚Äôs goal is to increase
the dissimilarity before and after image scaling as much as
possible. In this mode, the similarity constraints get relaxed
and we call this mode of attack the weak attack form.
4.2 An Autonomous Approach on Attack Im-
age Generation
We are interested to develop a method to automatically create
scaling attack images in both the strong and weak attack
forms. In order to achieve such goal, we Ô¨Årst formalize the
description of the data transition process among the four
conceptual objects, and then we seek an algorithmic solution
to create attack images.
The relationship between the four conceptual objects can
be described in the following formulas.
First, the transition between srcImg and attackImg can
be represented by a perturbation matrix D1, and so does the
difference between outImg and targetImg. These transition
can be represented by
Am‚á§n = Sm‚á§n + D1
(1)
For the transition between attackImg and outImg, we con-
sider the scaling effect as a function ScaleFunc(), which con-
verts an m‚á§ n input image Am‚á§n to an m0 ‚á§ n0 output image
Dm0‚á§n0
6.
ScaleFunc(Am‚á§n) = Dm0‚á§n0
(2)
ScaleFunc() is a surjective function, i.e. there exist multiple
possible inputs Am‚á§n that all result in the same output Dm0‚á§n0.
To perform a scaling attack, attackers need to craft an attack
image Am‚á§n, which is the source image Sm‚á§n plus a perturba-
tion matrix D1. In the meanwhile, the scaling result of the
attack image, i.e., the output image Dm0‚á§n0, needs to be visu-
ally similar with the target image Tm0‚á§n0. Here we use D2 to
evaluate the difference between Dm0‚á§n0 and Tm0‚á§n0.
Am‚á§n = Sm‚á§n + D1
ScaleFunc(Am‚á§n) = Dm0‚á§n0
Dm0‚á§n0 = Tm0‚á§n0 + D2
(3)
Let us consider the strong attack form of scaling attack
as in Fig.5, where both source Sm‚á§n and target image Tm0‚á§n0
6Conventionally, we say a matrix of m‚á§ n dimension has m rows and n
columns, while an image of m‚á§ n size consists of m columns and n rows. For
convenience sake, in this paper we use a matrix Xm‚á§n of m‚á§ n dimension to
refer to ‚Äúan m‚á§ n image‚Äù.
are speciÔ¨Åed. The attacker‚Äôs task is to Ô¨Ånd an attack image
Am‚á§n being able to cause deceiving effect. Considering Eq.3,
we can Ô¨Ånd multiple possible candidate matrices as solutions
for Am‚á§n that satisfy the whole set of formulas. This is due
to the surjection effect of ScaleFunc(). What the attacker
wants to Ô¨Ånd is the matrix that produces the best deceiving
effect among all possible solutions for Am‚á§n. One strategy is
to Ô¨Ånd an A that is the most similar with S, while limiting the
difference between D and T within an upper bound.
To Ô¨Ånd the best deceiving effect, we theoretically deÔ¨Åne
an objective function that compares all solutions of Am‚á§n. To
seek an algorithmic solution, we choose the L-norm distance7
to capture the pixel-level differences as an approximation for
measuring how close two images are.
In the strong attack form, we want to minimize the differ-
ence between Am‚á§n and Sm‚á§n, and limit the difference between
Dm0‚á§n0 and Tm0‚á§n0 within a threshold. Consequently, when the
source image Sm‚á§n and target image Tm0‚á§n0 are given, the best
result can be found by solving the following objective func-
tion.
Am‚á§n = Sm‚á§n + D1
ScaleFunc(Am‚á§n) = Dm0‚á§n0
Dm0‚á§n0 = Tm0‚á§n0 + D2
||D2||‚Ä¢  e‚á§ INmax
Objective function : min(||D1||2)
where INmax is the maximum pixel value in the current image
format.
For the weak attack form, i.e. only the target image Tm0‚á§n0 is
given, what an attacker wants to optimize is to pick the attack
image that visually has the largest difference from the target
image. Thus, the best result should be found by solving the
following objective function:
(4)
(5)
Rm‚á§n = ScaleFunc(Tm0‚á§n0)
Am‚á§n = Rm‚á§n + D1
ScaleFunc(Am‚á§n) = Dm0‚á§n0
Dm0‚á§n0 = Tm0‚á§n0 + D2
||D2||‚Ä¢  e‚á§ INmax
Objective function : max(||D1||2)
Notice that in the above constraints, we apply ScaleFunc()
twice. The Ô¨Årst call of ScaleFunc() is actually scaling the
target image to the size of the attack image, i.e., upscaling an
image from dimension m0 ‚á§ n0 back to m‚á§ n.
5 Creating Scaling Attack Images
After building up the formalized model of the scaling attack,
in this section we investigate how to generate attack images
automatically.
7In this paper, ||¬∑|| denotes the L2-norm, while ||¬∑||‚Ä¢ denotes the L‚Ä¢-
norm.
448    28th USENIX Security Symposium
USENIX Association
5.1 Empirical Analysis of Scaling Functions
In our implementation, we Ô¨Årst need to Ô¨Ånd an appropriate
expression of ScaleFunc(). We studied the implementation
details of commonly used image processing packages. All
of the scaling functions we studied perform the interpolation
in two steps, one direction in each step. We design our at-
tack algorithm with the assumption that the target scaling
algorithm Ô¨Årst resizes inputs horizontally and then vertically.
Empirically the popular algorithms take this order (see Ta-
ble 2, and more detailed analysis and examples are provided
in Appendix C.). In case the scaling algorithm takes vertical
order Ô¨Årst, the attack method just needs to change accordingly.
Hence, the ScaleFunc() in Eq.2 can be presented as:
ScaleFunc(Xm‚á§n) = CLm0‚á§m ‚á§ Xm‚á§n ‚á§CRn‚á§n0
(6)
In Eq.6, CLm0‚á§m and CRn‚á§n0 are two constant coefÔ¨Åcient
matrices determined by the interpolation algorithm, related
to horizontal scaling (m ‚á§ n ! m ‚á§ n0) and vertical scaling
(m‚á§ n0 ! m0 ‚á§ n0), respectively.
With Eq.4 and Eq.6, we eventually build a relationship
among the source image, the target image, and the perturba-
tion:
CLm0‚á§m ‚á§ (Sm‚á§n + D1)‚á§CRn‚á§n0 = Dm0‚á§n0
Dm0‚á§n0 = Tm0‚á§n0 + D2
(7)
5.2 Attack Image Crafting: An Overview
The main idea of automated scaling attack generation is to
craft the attack image by two steps. The Ô¨Årst step is to ob-
tain the coefÔ¨Åcient matrices in Eq.7. Section 5.3 presents a
practical solution to Ô¨Ånd CL and CR, implemented as GetCo-
efÔ¨Åcient(). The second step is to Ô¨Ånd the perturbation matrix
D1 to craft the attack image. We perform the attack image gen-
eration along each direction, in reverse order that we assume
ScaleFunc() uses. Further, we decompose the solution of the
perturbation matrix into the solution of a few perturbation
vectors. By this way, we can signiÔ¨Åcantly reduce the computa-
tional complexity for large size images. Section 5.4 provides
more details of the second step, based on which we implement
GetPerturbation() to Ô¨Ånd the perturbation vectors. Algorithm
1 and Algorithm 2 illustrate the attack image generation in
the weak attack form and the strong attack form, respectively.
Weak attack form (Algorithm 1) 8. First, we obtain the co-
efÔ¨Åcient matrices by calling GetCoefÔ¨Åcient() (line 2), which
receives the size of Sm‚á§n and Tm0‚á§n0, and returns coefÔ¨Åcient