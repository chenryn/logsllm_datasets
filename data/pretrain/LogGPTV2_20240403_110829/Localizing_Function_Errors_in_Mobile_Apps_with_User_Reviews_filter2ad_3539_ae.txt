# APK Analysis and ReviewSolver Evaluation

## Summary of APKs
| APK Name | # Modified / # Found |
| --- | --- |
| org.mariotaku.twidere | 59/260 |
| org.thoughtcrime.securesms | 100/354 |
| com.fsck.k9 | 148/251 |
| com.battlelancer.seriesguide | 92/212 |
| org.wordpress.android | 153/255 |
| cgeo.geocaching | 110/205 |
| com.joulespersecond.seattlebusbot | 30/75 |
| de.danoeh.antennapod | 46/209 |
| com.frostwire.android | 44/154 |
| com.ichi2.anki | 50/178 |
| **Total** | **832/2,153** |

## Answer to RQ3
The experimental results show that ReviewSolver can resolve 45.3% of function error-related reviews.

## VI. Discussion
Several factors may affect the performance of ReviewSolver when identifying function error-related reviews. The current training dataset does not cover all types of reviews that describe errors implicitly (e.g., "… is hard to load"). To address this, we plan to create a larger training dataset in the future. Currently, we only consider low-score reviews, but we will leverage sentiment analysis to identify negative sentences from positive or neutral reviews in the future.

When mapping function error-related reviews to code, ReviewSolver cannot filter out all useless phrases that might be mistakenly mapped to code. To address this, we will use machine learning algorithms to identify and exclude these phrases. Additionally, if developers use obfuscation techniques to hide class and method names, ReviewSolver cannot locate the code related to app-specific tasks. To overcome this, we will deobfuscate such names using methods proposed in [44].

Some function error-related reviews cannot be located because they are related to compatibility issues with specific devices. We can use information retrieval techniques to recognize the types of devices and report them to developers automatically. Other reviews that simply state that the app does not work can be grouped together to gather more detailed information for analysis.

## VII. Related Work

### A. Review Analysis
Numerous studies have been conducted on user reviews in app stores [1]. However, most of these studies analyze user reviews without correlating them with the app's code. For example, Chen et al. [45] combine static and dynamic features with comment features to predict app popularity. Khalid et al. [14] manually analyze user reviews and uncover 12 types of user complaints. Gomez et al. [46] use LDA and J48 to identify correlations between error-sensitive permissions and error-related reviews.

Other studies extract app features from reviews. Iacob et al. [5] define linguistic rules to match feature request-related reviews and use LDA to identify common topics. AR-Miner [10] uses machine learning to filter non-informative reviews and performs clustering to provide an intuitive summary for developers. Ciurumelea et al. [16] manually analyze user reviews and apply machine learning to classify them. AutoReb [47] combines machine learning and crowdsourcing to identify privacy-related reviews. SUR-Miner [4] extracts semantic dependence relations and uses clustering to identify users' opinions. Maalej et al. [21] combine text classification, NLP, and sentiment analysis to classify reviews into four categories. SURF [48] classifies review sentences into different categories based on intentions and topics.

ChangeAdvisor [11] is closely related to our study as it also analyzes code. ChangeAdvisor uses the HDP algorithm [49] to extract topic words from clusters of function error-related reviews and calculates the asymmetric dice similarity coefficient [50] between these words and the source code. If the result reaches a threshold, it recommends checking the corresponding code file. The key differences between our system and ChangeAdvisor are:
1. **Syntactic Analysis**: ChangeAdvisor does not consider part-of-speech tags, leading to false mappings. ReviewSolver conducts syntactic analysis to avoid this.
2. **Static Analysis**: We use static analysis to extract starting activities, requested permissions, APIs/URIs/intents, error messages, class/method names, and GUI information from APKs, rather than simply splitting code into words.
3. **Semantic Similarity**: ChangeAdvisor checks the number of shared words, which does not account for synonyms. ReviewSolver uses word embedding to measure semantic similarity and find similar phrases even with different words.

### B. Code Analysis
Many static analysis systems have been proposed to analyze APK files of mobile apps [51]. EdgeMiner [52] identifies callbacks and their registration functions. DeGuard [44] builds a probabilistic model for third-party libraries to recover obfuscated class, method, and variable names. DexHunter [53] and PackerGrind [54] recover original dex files from packed APKs. FlowDroid [30] performs static taint analysis to identify source-to-sink paths. IccTA [29] analyzes inter-component communication using IC3. Fan et al. [55], [56] use sensitive subgraphs to profile and detect piggybacked apps. Xue et al. [57] use dynamic analysis to identify factors affecting network measurement results.

Some studies use static analysis to analyze app GUIs. AutoPPG [58], [59] generates precise privacy policies by analyzing GUI callbacks and sensitive behaviors. AsDroid [60] compares static analysis behavior with UI-identified behavior to find contradictions. UIPicker [61] determines sensitive input fields using a supervised learning classifier.

### C. Linking Document to Code
Information retrieval (IR) techniques [50] have been used to link documents to code. BLUIR [62] uses VSM to link bug reports to code. TRASE [63] builds a probabilistic topic model for software artifacts. CRISTAL [64] compares crowd reviews with code changes to measure the extent to which requests have been accommodated. TAPVerifier [65] uses NLP to analyze privacy policies and static analysis to check code. SNIAFL [66] transforms feature descriptions and method/variable names into index terms and uses VSM to calculate cosine similarity. PPChecker [67] compares privacy policies with APKs to detect problems. Zhang et al. [68] use sentence ranking to enrich new bug reports.

## VIII. Conclusion
User reviews of mobile apps can help developers discover function errors uncaught by app testing. Manual processing is time-consuming and error-prone, while automated approaches may lead to many false positives and negatives. In this paper, we propose and develop ReviewSolver, a tool that automatically localizes function errors by correlating context information from reviews with bytecode. Experimental results show that ReviewSolver can identify function error reviews with high precision and recall rates, outperforming the state-of-the-art tool, ChangeAdvisor. The data and program are available at: https://github.com/yulele/ReviewSolver.

## Acknowledgements
We thank the anonymous reviewers for their valuable feedback and suggestions. This work is supported in part by the Hong Kong GRF (152279/16E, 152223/17E), the Hong Kong RGC Project (CityU C1008-16G), and the National Natural Science Foundation of China (No. 61502493).

## References
[1] N. Genc-Nayebi and A. Abran, “A systematic literature review: Opinion mining studies from mobile app store user reviews,” Journal of Systems and Software, 2017.
...
[68] T. Zhang, J. Chen, H. Jiang, X. Luo, and X. Xia, “Bug report enrichment with application of automated fixer recommendation,” in Proc. ICPC, 2017.