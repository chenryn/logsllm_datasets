Apk Name
# modiﬁed / # found
#
1
3
5
9
10
13
14
16
17
18
org.mariotaku.twidere
org.thoughtcrime.securesms
com.fsck.k9
com.battlelancer.seriesguide
org.wordpress.android
cgeo.geocaching
com.joulespersecond.seattlebusbot
de.danoeh.antennapod
com.frostwire.android
com.ichi2.anki
Total
59/260
100/354
148/251
92/212
153/255
110/205
30/75
46/209
44/154
50/178
832/2,153
Answer to RQ3: The experimental result shows that:
ReviewSolver can resolve 45.3% function error related
reviews.
VI. DISCUSSION
the
affect
Some
performance
factors may
of
ReviewSolver. When identifying the
function error
related reviews, the training dataset does not cover all kinds
of reviews that describe the error implicitly (e.g., “... is hard
to load”). To overcome this limitation, we will create a larger
training dataset in future. Currently, we only consider low
score reviews but we will
leverage sentiment analysis to
identify negative sentences from positive/neutral reviews in
future.
When mapping function error related reviews to code,
ReviewSolver cannot ﬁlter out all useless phrases that
can be mapped to code mistakenly. To address this issue,
we will try to use machine learning algorithms to identify
useless phrase. When conducting static analysis on apk ﬁle, if
the developer employs obfuscation technique to hide the class
names and method names, ReviewSolver cannot locate the
code related to app speciﬁc tasks. To overcome this limitation,
we could deobfsucate such names using the methods proposed
in [44].
Some function error related reviews cannot be located since
they are related to the compatibility issues of speciﬁed device.
We can use information retrieval technique to recognize the
types of devices and report them to developer automatically.
Other function error related review that simply describes that
the app do not work can be solved by grouping them together
to get more detailed information for analysis.
VII. RELATED WORK
A. Review Analysis
A number of studies have been conducted on the user
reviews of app store [1]. However,
the majority of them
just analyze user reviews without correlating them with apps’
code. Chen et al. [45] combine static features (e.g., app
name, category) and dynamic features (e.g., current rate count,
description) with comment features (e.g., user rate, comment
title) to predict the popularity of apps. Khalid et al. manually
analyze user reviews and uncover 12 types of user complaints
[14]. To identify correlations between error-sensitive permis-
sions and error-related reviews, Gomez et al. [46] leverage
LDA and J48 to process the permissions and reviews.
Some other studies extract app features from reviews. Iacob
et al. [5] deﬁne a set of linguistic rules to match feature request
related reviews, and then use LDA to identify common topics
in these reviews. AR-Miner [10] employs machine learn-
ing technique to ﬁlter out non-informative reviews and then
performs clustering on the remaining reviews to provide an
intuitive summary for developers. Ciurumelea et al. manually
analyze user reviews and deﬁne a high level taxonomy (e.g.,
compatibility, usage) and low level taxonomy (e.g., device,
UI) [16] and apply machine learning techniques to classify
the reviews. AutoReb [47] combines machine learning and
crowdsourcing technique to identify four kinds of privacy
related reviews, including spamming, ﬁnancial issue, over-
privileged permission, and data leakage. To identify the part of
the app loved by users, SUR-Miner [4] extracts the semantic
dependence relation between words and utilizes clustering
algorithms to identify users’ opinion towards corresponding
aspect. In [21], Walid Maalej et al. combine text classiﬁcation,
NLP, and sentiment analysis to classify reviews into four cat-
egories. To generate summaries of users feedback, SURF [48]
classiﬁes review sentences into different categories by utilizing
the intentions and topics of the reviews.
ChangeAdvisor [11] is most closely related study since
they also analyze code. ChangeAdvisor [11] employs the
HDP algorithm [49] to extract topic words from the clusters of
function error related reviews. Then it calculates the asymmet-
ric dice similarity coefﬁcient [50] between these topic words
427
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply. 
and the words extracted from source code ﬁle. If the result
reaches a threshold, it recommends the developer to check the
corresponding code ﬁle. The major differences between our
system and ChangeAdvisor include: 1) When analyzing
the reviews, ChangeAdvisor does not consider the Part Of
Speech tags of each word, which will cause false mappings.
ReviewSolver conduct syntactic analysis on each review to
avoid such problem. 2) We employ static analysis to extract
the starting activity, requested permissions, APIs/URIs/intents,
error messages, class/method names, visible information and
invisible information of GUI from apk. We do not simply split
the code into distinct words like [16] [11] to avoid including
many useless words, which will affect
the correctness of
mapping reviews to code. 3) When mapping the review to
code, ChangeAdvisor [11] checks the number of words
shared by review and code ﬁle. It does not consider the
synonyms, thus leading to many false negatives. We leverage
the word embedding method to measure semantic similarity
between the two phrases, and our method can ﬁnd similar
phrases even when some words are different.
B. Code Analysis
Many static analysis systems have been proposed to analyze
the apk ﬁle of mobile apps [51]. EdgeMiner [52] conducts
static analysis on Android framework to identify callbacks and
their corresponding registration functions. Since developers
can use obfuscation technique to hide the class, method, vari-
able names, DeGuard [44] proposes to build up probabilistic
model for third-party libraries by analyzing non-obfuscated
apps. Then it employs the probabilistic model to recover the
obfuscated class, method, and variable names of new apks.
If the developer use packing services to hide the dex ﬁles,
DexHunter[53] and PackerGrind [54] can be used to recover
the original dex ﬁles. FlowDroid [30] performs static taint
analysis to identify the source to sink path. To analyze the
inter component communication of apps, IccTA leverages IC3
(an advanced string analysis tool) to discover the ICC links and
create dummy method for them [29]. To detect piggybacked
apps, Fan et al. propose using sensitive subgraphs to proﬁle the
app and extract features from them [55], [56]. Xue et al. [57]
use dynamic analysis to identify the factors that will affect the
network measurement result of mobile apps.
Some studies use static analysis to analyze the GUI of
apps. To generate precise privacy policies, AutoPPG [58], [59]
leverages Vulhunter [28] to analyze the callbacks of GUI and
the conditions of sensitive behaviors. To ﬁnd the contradiction
between user interface and code, AsDroid [60] compares the
behavior found by static analysis with the behavior identiﬁed
from UI to ﬁnd contradiction. To identify the sensitive user
input, UIPicker [61] determines sensitive input ﬁelds by using
a supervised learning classiﬁer that is based on the features
extracted from the texts of the UI elements.
C. Linking Document to Code
Information retrieval (IR) [50] technique has been used to
link document to code. The document is used as a query
and the code is regarded as document collection. IR uses
some models (e.g., vector space model, probabilistic model) to
calculate the relevance between the input document and code
ﬁles. Then it ranks the relevance of code ﬁles. BLUIR [62]
extracts words from the class names, method name, and
variable names of source code and then it employs VSM to
link bug reports to code. TRASE [63] builds up probabilistic
topic model for software artifacts. This model can be used to
classify artifacts based on semantic meaning and visualize the
software with topic words. CRISTAL [64] compares crowd
reviews with code changes to measure the extent to which
the crowd request have been accommodated. The system also
monitors changes of user ratings to measure user reactions.
To discover the inconsistency between app description and
permissions, TAPVeriﬁer [65] uses NLP to analyze the privacy
policy and uses static analysis to analyze the code. To locate
feature related code, SNIAFL [66] ﬁrst transforms the feature
description and method/variable names in code into index
terms. Then it uses vector space model to calculate the cosine
similarity between the feature description and methods in code.
PPChecker [67] compares the privacy policy with apk ﬁle to
detect three kinds of problems contained in privacy policy. To
enrich the content of new bug reports and facilitate software
maintenance, Zhang et al. [68] propose to utilize sentence
ranking to select proper sentences from historical bug reports.
VIII. CONCLUSION
User reviews of mobile apps can help developer discover the
function errors uncaught by app testing. Manually process-
ing reviews is time-consuming and error-prone whereas the
state-of-the-art automated approach may lead to many false
positives and false negative. In this paper, we propose and
develop a novel tool named ReviewSolver to automati-
cally localize the function error by correlating the context
information extracted from reviews and the bytecode. The
experimental result shows that ReviewSolver can identify
the function error reviews with a high precision and recall
rate. Moreover, it locates much more function error related
code than ChangeAdvisor, the state-of-the-art tool. The
corresponding data and the program will be available at:
https://github.com/yulele/ReviewSolver.
ACKNOWLEDGEMENTS
We thank the anonymous reviewers for their quality reviews
and suggestions. This work is supported in part by the Hong
Kong GRF (152279/16E, 152223/17E), the Hong Kong RGC
Project (CityU C1008-16G), and the National Natural Science
Foundation of China (No. 61502493).
REFERENCES
[1] N. Genc-Nayebi and A. Abran, “A systematic literature review: Opinion
mining studies from mobile app store user reviews,” Journal of Systems
and Software, 2017.
[2] D. Pagano and W. Maalej, “User feedback in the appstore: An empirical
study,” in Proc. RE, 2013.
[3] B. Fu, J. Lin, L. Li, C. Faloutsos, J. Hong, and N. Sadeh, “Why people
hate your app: Making sense of user feedback in a mobile app store,”
in Proc. KDD, 2013.
[4] X. Gu and S. Kim, “What parts of your apps are loved by users?” in
Proc. ASE, 2015.
[5] C. Iacob and R. Harrison, “Retrieving and analyzing mobile apps feature
requests from online reviews,” in Proc. MSR, 2013.
[6] L. V. G. Carre˜no and K. Winbladh, “Analysis of user comments: an
approach for software requirements evolution,” in Proc. ICSE, 2013.
428
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply. 
[7] E. Guzman and W. Maalej, “How do users like this feature? a ﬁne
grained sentiment analysis of app reviews,” in Proc. RE, 2014.
[8] P. M. Vu, T. T. Nguyen, H. V. Pham, and T. T. Nguyen, “Mining user
opinions in mobile app reviews: A keyword-based approach (t),” in Proc.
ASE, 2015.
[9] D. H. Park, M. Liu, C. Zhai, and H. Wang, “Leveraging user reviews
to improve accuracy for mobile app retrieval,” in Proc. SIGIR, 2015.
[10] N. Chen, J. Lin, S. C. Hoi, X. Xiao, and B. Zhang, “Ar-miner: mining
informative reviews for developers from mobile app marketplace,” in
Proc. ICSE, 2014.
[11] F. Palomba, P. Salza, A. Ciurumelea, S. Panichella, H. Gall, F. Ferrucci,
and A. De Lucia, “Recommending and localizing change requests for
mobile apps based on user reviews,” in Proc. ICSE, 2017.
[12] IDC, “Smartphone os market share, 2017 q1,” https://goo.gl/ymAhw9,
2017.
[13] statista, “Number of available applications in the google play store,”
https://goo.gl/My5ATw, 2017.
[14] H. Khalid, E. Shihab, M. Nagappan, and A. E. Hassan, “What do mobile
app users complain about?” IEEE Software, 2015.
[15] W. Maalej and H. Nabil, “Bug report, feature request, or simply praise?
on automatically classifying app reviews,” in Proc. RE, 2015.
[16] A. Ciurumelea, A. Schaufelb¨uhl, S. Panichella, and H. Gall, “Analyzing
reviews and code of mobile apps for better release planning,” in Proc.
SANER, 2017.
[17] S. Bird, E. Loper, and E. Klein, “Natural
language toolkit,”
http://www.nltk.org/, 2017.
[18] M. Gilleland, “Levenshtein distance,” https://goo.gl/TVA9Ga, 2017.
[19] M.-C. De Marneffe, B. MacCartney, C. D. Manning et al., “Generating
typed dependency parses from phrase structure parses,” in Proc. LREC,
2006.
[20] M.-C. De Marneffe and C. D. Manning, “Stanford typed dependencies
manual,” Tech. Rep., 2008.
[21] W. Maalej, Z. Kurtanovi´c, H. Nabil, and C. Stanik, “On the automatic
classiﬁcation of app reviews,” Requirements Engineering, 2016.
[22] W. B. Cavnar, J. M. Trenkle et al., “N-gram-based text categorization,”
Ann Arbor MI, 1994.
[23] Z. Wei, D. Miao, J.-H. Chauchat, R. Zhao, and W. Li, “N-grams based
feature selection and text representation for chinese text classiﬁcation,”
International Journal of Computational Intelligence Systems, 2009.
[24] S. N. Group, “Tf-idf weighting,” https://goo.gl/RrWHVc, 2009.
[25] A. . M. Text Mining, “What are n-grams?” https://goo.gl/bwAHtp, 2017.
[26] J. Elith, J. R. Leathwick, and T. Hastie, “A working guide to boosted
regression trees,” Journal of Animal Ecology, 2008.
[27] Bhuvan Sharma, “What are the advantages/disadvantages of using
gradient boosting over random forests?” https://goo.gl/N6y2Kw, 2015.
[28] C. Qian, X. Luo, Y. Le, and G. Gu, “Vulhunter: toward discovering
vulnerabilities in android applications,” IEEE Micro, 2015.
[29] L. Li, A. Bartel, T. F. Bissyand´e, J. Klein, Y. Le Traon, S. Arzt,
S. Rasthofer, E. Bodden, D. Octeau, and P. McDaniel, “Iccta: Detecting
inter-component privacy leaks in android apps,” in Proc. ICSE, 2015.
[30] S. Arzt, S. Rasthofer, C. Fritz, E. Bodden, A. Bartel, J. Klein,
Y. Le Traon, D. Octeau, and P. McDaniel, “Flowdroid: Precise context,
ﬂow, ﬁeld, object-sensitive and lifecycle-aware taint analysis for android
apps,” Acm Sigplan Notices, 2014.
[31] K. W. Y. Au, Y. F. Zhou, Z. Huang, and D. Lie, “Pscout: analyzing the
android permission speciﬁcation,” in Proc. CCS, 2012.
[32] stackoverﬂow, “Best practice for displaying error messages,” https://goo.
[33] P. W. McBurney and C. McMillan, “Automatic source code summariza-
tion of context for java methods,” IEEE Trans. TSE, 2016.
[34] A. Tutorials, “Static and dynamic textview creation,” https://goo.gl/
gl/odr84X, 2013.
BdyCW2, 2012.
[35] A. Rountev and D. Yan, “Static reference analysis for gui objects in
android software,” in Proc. CGO, 2014.
[36] Y. Goldberg and O. Levy, “word2vec explained: deriving mikolov
et al.’s negative-sampling word-embedding method,” arXiv preprint
arXiv:1402.3722, 2014.
[37] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of
word representations in vector space,” arXiv preprint arXiv:1301.3781,
2013.
[38] T. Mikolov, W.-t. Yih, and G. Zweig, “Linguistic regularities in contin-
uous space word representations.” in hlt-Naacl, 2013.
[39] Google Code, “word2vec,” https://goo.gl/NBJgk1, 2017.
[40] Z. Qu, V. Rastogi, X. Zhang, Y. Chen, T. Zhu, and Z. Chen, “Autocog:
Measuring the description-to-permission ﬁdelity in android applica-
tions,” in Proc. CCS, 2014.
[41] L. Yu, X. Luo, C. Qian, and S. Wang, “Revisiting the description-to-
behavior ﬁdelity in android applications,” in Proc. SANER, 2016.
[42] “Android developers: Manifest.permission,” https://goo.gl/vWoIU, 2017.
[43] “Android developer: Common intents,” https://goo.gl/gHv9sF, 2017.
[44] B. Bichsel, V. Raychev, P. Tsankov, and M. Vechev, “Statistical deob-
fuscation of android applications,” in Proc. CCS, 2016.
[45] M. Chen and X. Liu, “Predicting popularity of online distributed
applications: itunes app store case analysis,” in Proc. iConference, 2011.
[46] M. G´omez, R. Rouvoy, M. Monperrus, and L. Seinturier, “A recom-
mender system of buggy app checkers for app store moderators,” in
Proc. MOBILESoft, 2015.
[47] D. Kong, L. Cen, and H. Jin, “Autoreb: Automatically understanding
the review-to-behavior ﬁdelity in android applications,” in Proc. CCS,
2015.
[48] A. Di Sorbo, S. Panichella, C. V. Alexandru, J. Shimagaki, C. A.
Visaggio, G. Canfora, and H. C. Gall, “What would users change in my
app? summarizing app reviews for recommending software changes,” in
Proc. FSE, 2016.
[49] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei, “Sharing clusters
among related groups: Hierarchical dirichlet processes,” in Advances in
neural information processing systems, 2005.
[50] R. Baeza-Yates, B. Ribeiro-Neto et al., Modern information retrieval.
ACM press New York, 1999, vol. 463.
[51] A. Sadeghi, H. Bagheri, J. Garcia, and S. Malek, “A taxonomy and
qualitative comparison of program analysis techniques for security
assessment of android software,” IEEE Trans. TSE, 2017.
[52] Y. Cao, Y. Fratantonio, A. Bianchi, M. Egele, C. Kruegel, G. Vigna,
and Y. Chen, “Edgeminer: Automatically detecting implicit control ﬂow
transitions through the android framework.” in Proc. NDSS, 2015.
[53] Y. Zhang, X. Luo, and H. Yin, “DexHunter: toward extracting hidden
code from packed Android applications,” in Proc. ESORICS, 2015.
[54] L. Xue, X. Luo, L. Yu, S. Wang, and D. Wu, “Adaptive unpacking of
android apps,” in Proc. ICSE, 2017.
[55] M. Fan, J. Liu, W. Wang, H. Li, Z. Tian, and T. Liu, “Dapasa: detecting
android piggybacked apps through sensitive subgraph analysis,” IEEE
Trans. TIFS, 2017.
[56] M. Fan, J. Liu, X. Luo, K. Chen, Z. Tian, Q. Zheng, and T. Liu, “Android
malware familial classiﬁcation and representative sample selection via
frequent subgraph analysis,” IEEE Trans. TIFS, 2018.
[57] L. Xue, X. Ma, X. Luo, L. Yu, S. Wang, and T. Chen, “Is what you
measure what you expect? factors affecting smartphone-based mobile
network measurement,” in Proc. INFOCOM, 2017.
[58] L. Yu, T. Zhang, X. Luo, and L. Xue, “Autoppg: Towards automatic
generation of privacy policy for android applications,” in Proc. SPSM,
2015.
[59] L. Yu, T. Zhang, X. Luo, L. Xue, and H. Chang, “Toward automatically
generating privacy policy for android apps,” IEEE Trans. TIFS, 2017.
[60] J. Huang, X. Zhang, L. Tan, P. Wang, and B. Liang, “Asdroid: Detecting
stealthy behaviors in android applications by user interface and program
behavior contradiction,” in Proc. ICSE, 2014.
[61] Y. Nan, M. Yang, Z. Yang, S. Zhou, G. Gu, and X. Wang, “Uipicker:
User-input privacy identiﬁcation in mobile applications.” in Proc.
USENIX Security, 2015.
[62] R. K. Saha, M. Lease, S. Khurshid, and D. E. Perry, “Improving bug
localization using structured information retrieval,” in Proc. ASE, 2013.
[63] H. U. Asuncion, A. U. Asuncion, and R. N. Taylor, “Software traceabil-
ity with topic modeling,” in Proc. ICSE, 2010.
[64] F. Palomba, M. Linares-V´asquez, G. Bavota, R. Oliveto, M. Di Penta,
D. Poshyvanyk, and A. De Lucia, “User reviews matter! tracking
crowdsourced reviews to support evolution of successful apps,” in Proc.
ICSME, 2015.
[65] L. Yu, X. Luo, C. Qian, S. Wang, and H. K. Leung, “Enhancing the
description-to-behavior ﬁdelity in android apps with privacy policy,”
IEEE Trans. TSE, 2017.
[66] W. Zhao, L. Zhang, Y. Liu, J. Sun, and F. Yang, “Sniaﬂ: Towards a static
noninteractive approach to feature location,” Trans. TOSEM, 2006.
[67] L. Yu, X. Luo, X. Liu, and T. Zhang, “Can we trust the privacy policies
of android apps?” in Proc. DSN, 2016.
[68] T. Zhang, J. Chen, H. Jiang, X. Luo, and X. Xia, “Bug report enrichment
with application of automated ﬁxer recommendation,” in Proc. ICPC,
2017.
429
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:51:06 UTC from IEEE Xplore.  Restrictions apply.