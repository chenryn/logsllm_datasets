title:Site Isolation: Process Separation for Web Sites within the Browser
author:Charles Reis and
Alexander Moshchuk and
Nasko Oskov
Site Isolation: Process Separation for Web Sites 
within the Browser
Charles Reis, Alexander Moshchuk, and Nasko Oskov, Google
https://www.usenix.org/conference/usenixsecurity19/presentation/reis
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.Site Isolation: Process Separation for Web Sites within the Browser
Charles Reis
Google
PI:EMAIL
Alexander Moshchuk
Google
PI:EMAIL
Nasko Oskov
Google
PI:EMAIL
Abstract
Current production web browsers are multi-process but place
different web sites in the same renderer process, which is
not sufﬁcient to mitigate threats present on the web today.
With the prevalence of private user data stored on web sites,
the risk posed by compromised renderer processes, and the
advent of transient execution attacks like Spectre and Melt-
down that can leak data via microarchitectural state, it is no
longer safe to render documents from different web sites in
the same process. In this paper, we describe our successful
deployment of the Site Isolation architecture to all desktop
users of Google Chrome as a mitigation for process-wide
attacks. Site Isolation locks each renderer process to doc-
uments from a single site and ﬁlters certain cross-site data
from each process. We overcame performance and compat-
ibility challenges to adapt a production browser to this new
architecture. We ﬁnd that this architecture offers the best
path to protection against compromised renderer processes
and same-process transient execution attacks, despite current
limitations. Our performance results indicate it is practical
to deploy this level of isolation while sufﬁciently preserving
compatibility with existing web content. Finally, we discuss
future directions and how the current limitations of Site Iso-
lation might be addressed.
1 Introduction
Ten years ago, web browsers went through a major architec-
ture shift to adapt to changes in their workload. Web con-
tent had become much more active and complex, and mono-
lithic browser implementations were not effective against the
security threats of the time. Many browsers shifted to a
multi-process architecture that renders untrusted web con-
tent within one or more low-privilege sandboxed processes,
mitigating attacks that aimed to install malware by exploiting
a rendering engine vulnerability [43, 51, 70, 76].
Given recent changes in the security landscape, that multi-
process architecture no longer provides sufﬁcient safety for
visiting untrustworthy web content, because it does not pro-
vide similar mitigation for attacks between different web
sites. Browsers load documents from multiple sites within
the same renderer process, so many new types of attacks
target rendering engines to access cross-site data [5, 10, 11,
33, 53]. This is increasingly common now that the most ex-
ploitable targets of older browsers are disappearing from the
web (e.g., Java Applets [64], Flash [1], NPAPI plugins [55]).
As others have argued, it is clear that we need stronger iso-
lation between security principals in the browser [23, 33, 53,
62, 63, 68], just as operating systems offer stronger isolation
between their own principals. We achieve this in a produc-
tion setting using Site Isolation in Google Chrome, introduc-
ing OS process boundaries between web site principals.
While Site Isolation was originally envisioned to mitigate
exploits of bugs in the renderer process, the recent discov-
ery of transient execution attacks [8] like Spectre [34] and
Meltdown [36] raised its urgency. These attacks challenge
a fundamental assumption made by prior web browser ar-
chitectures:
that software-based isolation can keep sensi-
tive data protected within an operating system process, de-
spite running untrustworthy code within that process. Tran-
sient execution attacks have been demonstrated to work from
JavaScript code [25, 34, 37], violating the web security
model without requiring any bugs in the browser. We show
that our long-term investment in Site Isolation also provides
a necessary mitigation for these unforeseen attacks, though
it is not sufﬁcient: complementary OS and hardware miti-
gations for such attacks are also required to prevent leaks of
information from other processes or the OS kernel.
To deploy Site Isolation to users, we needed to over-
come numerous performance and compatibility challenges
not addressed by prior research prototypes [23, 62, 63, 68].
Locking each sandboxed renderer process to a single site
greatly increases the number of processes; we present pro-
cess consolidation optimizations that keep memory overhead
low while preserving responsiveness. We reduce overhead
and latency by consolidating painting and input surfaces of
contiguous same-site frames, along with parallelizing pro-
cess creation with network requests and carefully managing
a spare process. Supporting the entirety of the web pre-
sented additional compatibility challenges. Full support for
out-of-process iframes requires proxy objects and replicated
state in frame trees, as well as updates to a vast number of
browser features. Finally, a privileged process must ﬁlter
sensitive cross-site data without breaking existing cross-site
JavaScript ﬁles and other subresources. We show that such
ﬁltering requires a new type of conﬁrmation snifﬁng and can
protect not just HTML but also JSON and XML, beyond
prior discussions of content ﬁltering [23, 63, 68].
USENIX Association
28th USENIX Security Symposium    1661
With these changes, the privileged browser process can
keep most cross-site sensitive data out of a malicious docu-
ment’s renderer process, making it inconsequential for a web
attacker to access and exﬁltrate data from its address space.
While there are a set of limitations with its current imple-
mentation, we argue that Site Isolation offers the best path to
mitigating the threats posed by compromised renderer pro-
cesses and transient execution attacks.
In this paper, Section 2 introduces a new browser threat
model covering renderer exploit attackers and memory dis-
closure attackers, and it discusses the current limitations
of Site Isolation’s protection. Section 3 presents the chal-
lenges we overcame in fundamentally re-architecting a pro-
duction browser to adopt Site Isolation, beyond prior re-
search browsers. Section 4 describes our implementation,
consisting of almost 450k lines of code, along with critical
optimizations that made it feasible to deploy to all desktop
and laptop users of Chrome. Section 5 evaluates its effective-
ness against compromised renderers as well as Spectre and
Meltdown attacks. We also evaluate its practicality, ﬁnding
that it incurs a total memory overhead of 9-13% in practice
and increases page load latency by less than 2.25%, while
sufﬁciently preserving compatibility with actual web con-
tent. Given the severity of the new threats, Google Chrome
has enabled Site Isolation by default. Section 6 looks at the
implications for the web’s future and potential ways to ad-
dress Site Isolation’s current limitations. We compare to re-
lated work in Section 7 and conclude in Section 8.
Overall, we answer several new research questions:
• Which parts of a web browser’s security model can be
aligned with OS-level isolation mechanisms, while pre-
serving compatibility with the web?
• What optimizations are needed to make process-level
isolation of web sites feasible to deploy, and what is the
resulting performance overhead for real users?
• How well does process-level isolation of web sites up-
grade existing security practices to protect against com-
promised renderer processes?
• How effectively does process-level isolation of web
sites mitigate Spectre and Meltdown attacks, and where
are additional mitigations needed?
2 Threat Model
We assume that a web attacker can lure a user into visit-
ing a web site under the attacker’s control. Multi-process
browsers have traditionally focused on stopping web attack-
ers from compromising a user’s computer, by rendering un-
trusted web content in sandboxed renderer processes, coor-
dinated by a higher-privilege browser process [51]. How-
ever, current browsers allow attackers to load victim sites
into the same renderer process using iframes or popups, so
the browser must trust security checks in the renderer process
to keep sites isolated from each other.
In this paper, we move to a stronger threat model empha-
sizing two different types of web attackers that each aim to
steal data across web site boundaries. First, we consider a
renderer exploit attacker who can discover and exploit vul-
nerabilities to bypass security checks or even achieve ar-
bitrary code execution in the renderer process. This at-
tacker can disclose any data in the renderer process’s ad-
dress space, as well as lie to the privileged browser process.
For example, they might forge an IPC message to retrieve
sensitive data associated with another web site (e.g., cook-
ies, stored passwords). These attacks imply that the privi-
leged browser process must validate access to all sensitive re-
sources without trusting the renderer process. Prior work has
shown that such attacks can be achieved by exploiting bugs
in the browser’s implementation of the Same-Origin Policy
(SOP) [54] (known as universal cross-site scripting bugs, or
UXSS), with memory corruption, or with techniques such as
data-only attacks [5, 10, 11, 33, 53, 63, 68].
Second, we consider a memory disclosure attacker who
cannot run arbitrary code or lie to the browser process, but
who can disclose arbitrary data within a renderer process’s
address space, even when the SOP would disallow it. This
can be achieved using transient execution attacks [8] like
Spectre [34] and Meltdown [36]. Researchers have shown
speciﬁcally that JavaScript code can manipulate microar-
chitectural state to leak data from within the renderer pro-
cess [25, 34, 37].1 While less powerful than renderer exploit
attackers, memory disclosure attackers are not dependent on
any bugs in web browser code. Indeed, some transient exe-
cution attacks rely on properties of the hardware that are un-
likely to change, because speculation and other transient mi-
croarchitectural behaviors offer signiﬁcant performance ben-
eﬁts. Because browser vendors cannot simply ﬁx bugs to
mitigate cases of these attacks, memory disclosure attackers
pose a more persistent threat to the web security model. It
is thus important to reason about their capabilities separately
and mitigate these attacks architecturally.
2.1 Scope
We are concerned with isolating sensitive web site data from
execution contexts for other web sites within the browser.
Execution contexts include both documents (in any frame)
and workers, each of which is associated with a site princi-
pal [52] and runs in a renderer process. We aim to protect
many types of content and state from the attackers described
above, including the HTML contents of documents, JSON
or XML data ﬁles they retrieve, state they keep within the
browser (e.g., cookies, storage, saved passwords), and per-
missions they have been granted (e.g., geolocation, camera).
Site Isolation is also able to strengthen some existing se-
curity practices for web application code, such as upgrad-
ing clickjacking [30] protections to be robust against com-
1In some cases, transient execution attacks may access information
across process or user/kernel boundaries. This is outside our threat model.
1662    28th USENIX Security Symposium
USENIX Association
Cross-site subresources (e.g., JavaScript, CSS, images,
media) are not protected, since the web allows documents
to include them within an execution context. JavaScript and
CSS ﬁles were already somewhat exposed to web attackers
(e.g., via XSSI attacks that could infer their contents [26]);
the new threat model re-emphasizes not to store secrets in
such ﬁles. In contrast, cross-site images and media were suf-
ﬁciently opaque to documents before, suggesting a need to
better protect at least some such ﬁles in the future.
The content ﬁltering we describe in Section 3.5 is also
a best-effort approach to protect HTML, XML, and JSON
ﬁles, applying only when it can conﬁrm the responses match
the reported content type. This conﬁrmation is necessary to
preserve compatibility (e.g., with JavaScript ﬁles mislabeled
as HTML). Across all content types, we expect this ﬁltering
will protect most sensitive data today, but there are opportu-
nities to greatly improve this protection with headers or web
platform changes [21, 71, 73], as discussed in Section 6.1.
promised renderers, as discussed in Section 5.1. Not all
web security defenses are in scope, such as mitigations for
XSS [46].
2.2 Limitations
For both types of attackers we consider, Site Isolation aims
to protect as much site data as possible, while preserving
compatibility. Because we isolate sites (i.e., scheme plus
registry-controlled domain name [52]) rather than origins
(i.e., scheme-host-port tuples [54]) per Section 3.1, cross-
origin attacks within a site are not mitigated. We hope to
allow some origins to opt into origin-level isolation, as dis-
cussed in Section 6.3.
Finally, we rely on protection domains provided by the
operating system.
In particular, we assume that the OS’s
process isolation boundary can be trusted and consider cross-
process and kernel attacks out of scope for this paper, though
we discuss them further in Sections 5.2 and 6.2.
3 Site Isolation Browser Architecture
The Site Isolation browser architecture treats each web site
as a separate security principal requiring a dedicated renderer
process. Prior production browsers used rendering engines
that predated the security threats in Section 2 and were ar-
chitecturally incompatible with putting cross-site iframes in
a different process. Prior research browsers proposed similar
isolation but did not preserve enough compatibility to han-
dle the full web. In this section, we present the challenges
we overcame to make the Site Isolation architecture compat-
ible with the web in its entirety.
3.1 Site Principals
Most prior multi-process browsers, including Chrome, Edge,
Safari, and Firefox, did not assign site-speciﬁc security
principals to web renderer processes, and hence they did
not enforce isolation boundaries between different sites at
in Chrome
the process level. We advance this model
by partitioning web content into ﬁner-grained principals
that correspond to web sites. We adopt
the site deﬁ-
nition from [52] rather than origins as proposed in re-
search browsers [23, 62, 63, 68]. For example, an origin
https://bar.foo.example.com:8000 corresponds to a site
https://example.com. This preserves compatibility with up
to 13.4% of page loads that change their origin at runtime by
assigning to document.domain [12]. Site principals ensure
that a document’s security principal remains constant after
document.domain modiﬁcations.
For each navigation in any frame, the browser process
computes the site from the document’s URL, determining
its security principal. This is straightforward for HTTP(S)
URLs, though some web platform features require special
treatment, as we discuss in Appendix A (e.g., about:blank
can inherit its origin and site).
3.2 Dedicated Processes
Site Isolation requires that renderer processes can be dedi-
cated to documents, workers, and sensitive data from only
a single site principal. In this paper, we consider only the
case where all web renderer processes are locked to a single
site. It would also be possible for the browser to isolate only
some sites and leave other sites in shared renderer processes.
In such a model, it is still important to limit a dedicated ren-
derer process to documents and data from its own site, but it
is also necessary to prevent a shared process from retrieving
data from one of the isolated sites. When isolating all sites,
requests for site data can be evaluated solely on the process’s
site principal and not also a list of which sites are isolated.
The browser’s own components and features must be also
partitioned in a way that does not leak cross-site data. For
example, the network stack cannot run within the renderer
process, to protect HttpOnly cookies and so that ﬁltering de-
cisions on cross-site data can be made before the bytes from
the network enter the renderer process. Similarly, browser
features must not proactively leak sensitive data (e.g., the
user’s stored credit card numbers with autoﬁll) to untrust-
worthy renderer processes, at least until the user indicates
such data should be provided to a site [49]. These additional
constraints on browser architecture may increase the amount
of logic and state in more privileged processes. This does not
necessarily increase the attack surface of the trusted browser
process if these components (e.g., network stack) can move
to separate sandboxed processes, as in prior microkernel-like
browser architectures [23, 62].
3.3 Cross-Process Navigations
When a document in a frame navigates from site A to site
B, the browser process must replace the renderer process for
site A with one for site B. This requires maintaining state in
the browser process, such as session history for the tab, re-
lated window references such as openers or parent frames,
USENIX Association
28th USENIX Security Symposium    1663
and tab-level session storage [74]. Due to web-visible events
such as beforeunload and unload and the fact that a nav-
igation request might complete without creating a new doc-
ument (e.g., a download or an HTTP “204 No Content” re-
sponse), the browser process must coordinate with both old
and new renderer processes to switch at the appropriate mo-
ment: after beforeunload, after the network response has
proven to be a new document, and at the point that the new
process has started rendering the new page. Note that cross-