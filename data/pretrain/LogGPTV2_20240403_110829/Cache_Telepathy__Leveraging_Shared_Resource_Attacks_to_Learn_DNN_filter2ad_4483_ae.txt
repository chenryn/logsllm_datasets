∑
i=1
(
L
∏
j=1
x j)
(3)
Figure 9: Extracted values of the n, k, and m matrix parame-
ters for VGG-16 and ResNet-50 using Prime+Probe on Open-
BLAS.
In summary, our side channel attacks, using Flush+Reload
or Prime+Probe, can either detect the matrix parameters with
where C is the total number of possible connection conﬁgura-
tions, L is the total number of layers, and x j is the number of
possible combinations of hyper-parameters for layer j.
Determining Connections Between Layers We show
how to reverse engineer the connections between layers using
ResNet-M1 as an example.
2014    29th USENIX Security Symposium
USENIX Association
(a) Extracting n0500100015002000actual valuedetected value or range from side channeldeduced value using DNN constraints(b) Extracting k01000200030004000L1L2L3L4L1L2L3L4L1L2L3L4L1L2L3L4B1B2B3B4B5ResNet-M1ResNet-M2ResNet-M3ResNet-M4VGG(c) Extracting m102103104First, we leverage inter-GEMM latency to determine the
existence of shortcuts and their sinks using the method dis-
cussed in Section 4.3. Figure 10 shows the extracted matrix
dimensions and the inter-GEMM latency for the 4 layers in
ResNet-M1. The inter-GEMM latency after M1-L4 is signiﬁ-
cantly longer than expected, given its output matrix size and
the input matrix size of the next layer. Thus, the layer after
M1-L4 is a sink layer.
Figure 10: Extracting connections in ResNet-M1.
Next, we check the output matrix dimensions of previous
layers to locate the source of the shortcut. Note that a shortcut
only connects layers with the same output matrix dimensions.
Based on the extracted dimension information (values of n
and m) in Figure 8, we determine that M1-L1 is the source. In
addition, we know that M1-L1 and M1-L2 are not sequentially
connected by comparing the output matrix of M1-L1 and the
input matrix of M1-L2 (Section 4.3).
Figure 10 summarizes the reverse engineered connections
among the 4 layers, which match the actual connections in
ResNet-M1. We can use the same method to derive the possi-
ble connection conﬁgurations for the other modules. Note that
this approach does not work for ResNet-M3 and ResNet-M4.
In these layers, the input and output matrices are small and
operations between consecutive layers take a short time. As a
result, the inter-GEMM latency is not effective in identifying
shortcuts.
Determining Hyper-parameters for Each Layer We
plug the detected matrix parameters into the formulas in Ta-
ble 3 to deduce the hyper-parameters for each layer. For
the matrix dimensions that cannot be extracted precisely, we
leverage DNN constraints to prune the search space.
As an example, consider reverse engineering the hyper-
parameters for Layer 3 in ResNet-M2. First, we extract the
number of ﬁlters. We round the extracted nM2-L3 from Fig-
ure 8(a) (the number of rows in F(cid:48)) to the nearest multiple
of 64. This is because, as discussed at the beginning of Sec-
tion 8.3, we assume that the number of ﬁlters is a multiple
of 64. We get that the number of ﬁlters is 512. Second, we
use the formula in Table 3 to determine the ﬁlter width and
height. We consider the case where L2 is sequentially con-
nected to L3. The extracted range of kM2-L3 from Figure 8(b)
(the number of rows in in(cid:48) of current layer) is [68,384], and
the value for nM2-L2 from Figure 8(a) (the number of rows in
out(cid:48) of the previous layer) is 118. We need to make sure that
the square root of kM2-L3/nM2-L2 is an integer, which leads
to the conclusion that the only possible value for kM2-L3 is
118 (one of the solid circles for kM2-L3), and the ﬁlter width
and height is 1. The same value is deduced if we consider,
instead, that L1 is connected to L3. The other solid circle for
kM2-L3 is derived similarly if we consider that the last layer in
M1 is connected to layer 3 in M2.
We apply the same methodology for the other layers. With
this method, we obtain the solid circles in Figures 8 and 9.
Determining Pooling and Striding We use the difference
in the m dimension (i.e., the channel size of the output) be-
tween consecutive layers to determine the pool or stride size.
For example, in Figure 8(c) and 9(c), the m dimensions of
the last layer in ResNet-M1 and the ﬁrst layer in ResNet-M2
are different. This difference indicates the existence of a pool
layer or a stride operation. In Figure 8(c), the extracted value
of mM1-L4 (the number of columns in out(cid:48) for the current
layer) is 3072, and the extracted range of mM2-L1 (the number
of columns in in(cid:48) for the next layer) is [524,1536]. We use
the formula in Table 3 to determine the pool or stride width
and height. To make the square root of mM1-L4/mM2-L1 an
integer, mM2-L1 has to be 768, and the pool or stride width
and height have to be 2.
8.3.3 Size of the Reduced Search Space
Using Equation 3, we compute the number of architectures
in the search space without Cache Telepathy and with Cache
Telepathy. Table 4 shows the resulting values. Note that
we only consider the possible conﬁgurations of the different
layers in VGG-16 (B1, B2, B3, B4, and B5) and of the different
modules in ResNet-50 (M1, M2, M3, and M4).
DNN
Original: No Cache Telepathy
OpenBLAS
Flush+Reload
Prime+Probe
MKL
OpenBLAS
MKL
ResNet-50
VGG-16
> 6× 1046 > 5.4× 1012
512
6144
512
5.7× 1015
16
64
16
1936
Table 4: Comparing the original search space (without Cache
Telepathy) and the reduced search space (with Cache Telepa-
thy).
Using Cache Telepathy to attack OpenBLAS, we are able to
signiﬁcantly reduce the search space from an intractable size
to a reasonable size. Both Flush+Reload and Prime+Probe
obtain a very small search space. Speciﬁcally, for VGG-16,
Cache Telepathy reduces the search space from more than
5.4 × 1012 architectures to just 16; for ResNet-50, Cache
Telepathy reduces the search space from more than 6× 1046
to 512.
Cache Telepathy is less effective on MKL. For VGG-
16, Cache Telepathy reduces the search space from more
USENIX Association
29th USENIX Security Symposium    2015
M1-L1M1-L2M1-L3M1-L401234Matrix Size1e6current layer output matrix sizenext layer input matrix size0.751.001.251.501.752.00Inter-GEMM Latency (cycles)1e7inter-GEMM latency after current layerPrevious LayersM1-L1M1-L2M1-L3M1-L4Sink Layerthan 5.4 × 1012 to 64 (with Flush+Reload) or 1936 (with
Prime+Probe). For ResNet-50, Cache Telepathy reduces
the search space from more than 6 × 1046 to 6144 (with
Flush+Reload) or 5.7× 1015 (with Prime+Probe). The last
number is large because the matrix dimensions in Module
M1 and Module 4 of ResNet-50 are small, and MKL handles
these matrices with the special method described in Section 6.
Such method is not easily attackable by Prime+Probe. How-
ever, if we only count the number of possible conﬁgurations
in Modules M1, M2, and M3, the search space is 41472.
Implications of Large Search Spaces A large search
space means that the attacker needs to train many networks.
Training DNNs is easy to parallelize, and attackers can re-
quest many GPUs to train in parallel. However, it comes
with a high cost. For example, assume that training one net-
work takes 2 GPU days. On Amazon EC2, the current price
for a single-node GPU instance is ∼$3/hour. Without Cache
Telepathy, since the search space is so huge, the cost is unbear-
able. Using Cache Telepathy with Flush+Reload, the reduced
search space for the different layers in VGG-16 and for the
different modules in ResNet-50 running OpenBLAS means
that the training takes 32 and 1024 GPU days, respectively.
The resulting cost is only ∼$2K and ∼$74K. When attacking
ResNet-50 running MKL, the attacker needs to train 6144
architectures, requiring over $884K.
9 Countermeasures
We overview possible countermeasures against our attack,
and discuss their effectiveness and performance implications.
We ﬁrst investigate whether it is possible to stop the attack
by modifying the BLAS libraries. All BLAS libraries use
extensively optimized blocked matrix multiplication for per-
formance. One approach is to disable the optimization or use
less aggressive optimization. However, it is unreasonable to
disable blocked matrix multiplication, as the result would be
very poor cache performance. Using a less aggressive block-
ing strategy, such as removing the optimization for the ﬁrst
iteration of Loop 3 (lines 4-7 in Algorithm 1), only slightly
increases the difﬁculty for attackers to recover some matrix
dimensions. It cannot effectively eliminate the vulnerability.
Another approach is to reduce the dimensions of the matri-
ces. Recall that in both OpenBLAS and MKL, we are unable
to precisely deduce the matrix dimensions if they are smaller
than or equal to the block size. Existing techniques, such as
quantization, can help reduce the matrix size to some degree.
This mitigation is typically effective for the last few layers in a
convolutional network, which generally use small ﬁlter sizes.
However, it cannot protect layers with large matrices, such as
those using a large number of ﬁlters and input activations.
Alternatively, one can use existing cache-based side chan-
nel defense solutions. One approach is to use cache partition-
ing, such as Intel CAT (Cache Allocation Technology) [30].
CAT assigns different ways of the last level cache to differ-
ent applications, which blocks cache interference between
attackers and victims [37]. Further, there are proposals for
security-oriented cache mechanisms such as PLCache [62],
SHARP [67] and CEASER [44]. If these mechanisms are
adopted in production hardware, they can mitigate our attack
with moderate performance degradation.
10 Related Work
Recent research has called attention to the conﬁdentiality of
neural network hyper-parameters. Hua et al. [29] designed
the ﬁrst attack to steal CNN architectures running on a hard-
ware accelerator. Their attack is based on a different threat
model, which requires the attacker to be able to monitor all
of the memory addresses accessed by the victim. Our attack
does not require such elevated privilege. Hong et al. [27]
proposed to use cache-based side channel attacks to reverse
engineer coarse-grained information of DNN architectures.
Their attack is less powerful than Cache Telepathy. They can
only obtain the number and types of layers, but are unable to
obtain more detailed hyper-parameters, such as the number
of neurons in fully-connected layers and ﬁlter size in convolu-
tional layers. Batina et al. [9] proposed to use electromagnetic
side channel attacks to reverse engineer DNNs in embedded
systems.
Cache-based side channel attacks have been used to trace
program execution to steal sensitive information. A lot of
attacks target cryptography algorithms [15–17, 24, 31, 38, 40,
47,68–70,72], such as AES, RSA and ECDSA. Recent works
also target application ﬁngerprinting [28, 42, 48, 50, 73] to
steal web content or server data, monitor user behavior [22,
36, 46, 71], and break system protection mechanisms such as
SGX and KASLR [11, 21, 61].
11 Conclusion
In this paper, we proposed Cache Telepathy, an efﬁcient mech-
anism to help obtain a DNN’s architecture using the cache
side channel. We identiﬁed that DNN inference relies heavily
on blocked GEMM, and provided a detailed security analysis
of this operation. We then designed an attack to extract the
matrix parameters of GEMM calls, and scaled this attack to
complete DNNs. We used Prime+Probe and Flush+Reload to
attack VGG and ResNet DNNs running OpenBLAS and Intel
MKL libraries. Our attack is effective at helping obtain the
architectures by very substantially reducing the search space
of target DNN architectures. For example, when attacking
the OpenBLAS library, for the different layers in VGG-16,
it reduces the search space from more than 5.4× 1012 archi-
tectures to just 16; for the different modules in ResNet-50, it
reduces the search space from more than 6× 1046 architec-
tures to only 512.
2016    29th USENIX Security Symposium
USENIX Association
Acknowledgments
This work was funded in part by NSF under grant CCF-
1725734.
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng
Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Geoffrey Irving, Michael Isard, et al.
TensorFlow: A System for Large-scale Machine Learn-
ing. In Proceedings of the 12th USENIX Symposium on
Operating Systems Design and Implementation, 2016.
[2] Amazon. Amazon Machine Learning. https://aws.
amazon.com/machine-learning/, 2018.
[3] Amazon. Amazon SageMaker. https://aws.amazon.
com/sagemaker/, 2018.
[4] Amazon.
Amazon SageMaker ML Instance
https://aws.amazon.com/sagemaker/
Types.
pricing/instance-types/, 2018.
[5] AMD.
Core Math Library
(ACML).
https://developer.amd.com/amd-aocl/
amd-math-library-libm/, 2012.
[6] Apache. Apache MXNet. https://mxnet.apache.
org/, 2018.
[7] Ahmed Osama Fathy Atya, Zhiyun Qian, Srikanth V
Krishnamurthy, Thomas La Porta, Patrick McDaniel,
and Lisa Marvel. Malicious Co-Residency on the Cloud:
Attacks and Defense. In IEEE Conference on Computer
Communications. IEEE, 2017.
[8] Ahmed Osama Fathy Atya, Zhiyun Qian, Srikanth V Kr-
ishnamurthy, Thomas La Porta, Patrick McDaniel, and
Lisa M Marvel. Catch Me if You Can: A Closer Look
at Malicious Co-Residency on the Cloud. IEEE/ACM
Transactions on Networking, 2019.
[9] Lejla Batina, Shivam Bhasin, Dirmanto Jap, and Stjepan
CSI NN: Reverse Engineering of Neural
Picek.
Network Architectures Through Electromagnetic Side
Channel. In 28th USENIX Security Symposium, 2019.
[10] François Chollet. Keras.
fchollet/keras, 2015.
https://github.com/
[11] Fergus Dall, Gabrielle De Micheli, Thomas Eisenbarth,
Daniel Genkin, Nadia Heninger, Ahmad Moghimi, and
Yuval Yarom. Cachequote: Efﬁciently Recovering
Long-Term Secrets of SGX EPID via Cache Attacks.
IACR Transactions on Cryptographic Hardware and
Embedded Systems, 2018.
[12] Christopher De Sa, Matthew Feldman, Christopher Ré,
and Kunle Olukotun. Understanding and optimizing
asynchronous low-precision stochastic gradient descent.
In ACM SIGARCH Computer Architecture News, 2017.
[13] Craig Disselkoen, David Kohlbrenner, Leo Porter, and
Dean Tullsen.
Prime+Abort: A Timer-Free High-
Precision L3 Cache Attack Using Intel TSX. In 26th