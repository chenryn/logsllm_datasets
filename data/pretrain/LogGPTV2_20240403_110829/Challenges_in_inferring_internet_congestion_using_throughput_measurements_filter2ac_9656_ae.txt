1
C
O
M
-
2
C
O
M
-
3
C
O
M
-
4
C
O
M
-
5
V
Z
T
W
C
-
1
T
W
C
-
2
T
W
C
-
3
C
O
X
-
1
C
O
X
-
2
C
E
N
T
S
O
N
C
R
C
N
F
R
O
N
A
T
T
C
O
M
-
1
C
O
M
-
2
C
O
M
-
3
C
O
M
-
4
C
O
M
-
5
V
Z
T
W
C
-
1
T
W
C
-
2
T
W
C
-
3
C
O
X
-
1
C
O
X
-
2
C
E
N
T
S
O
N
C
R
C
N
F
R
O
N
A
T
T
(a) Overlap at AS-level
(b) Overlap at router-level
Figure 4: Differences in the number of interconnections traversed on paths to M-Lab and Speedtest.net servers vs. those on paths
toward Alexa targets. “Mlab-Alexa” denotes the number of interconnections in traceroutes to M-Lab servers but not in traceroutes to
Alexa targets. “Alexa-Mlab” denotes the number of interconnections in traceroutes to Alexa targets but not in traceroutes to M-Lab
servers. The remaining two bars compare the overlap between interconnections on paths to Speedtest.net servers and Alexa targets.
For each VP a signiﬁcant number of interconnections on paths to popular web content were not covered using M-Lab or Speedtest.net
servers.
the router-level [36]. Hence for the purpose of examining the cov-
erage of an ISPs AS-level interconnections, we believe outbound
traceroutes are sufﬁcient. In future work we plan to use the Reverse
Traceroute [23] and Sibyl [16] systems when they become avail-
able to infer inbound paths to our Ark VPs. A further caveat of our
methodology is that it necessarily measures popular web content,
and does not include the CDN locations from which popular videos
may be served. We leave an examination of paths toward the sources
of popular video content to future work.
5.2 Coverage of interdomain interconnections
Figure 2 compares the set of (AS-level and router-level) intercon-
nections of the 16 VPs observed in traceroutes toward M-Lab
and Speedtest.net targets with the set of interconnections bdrmap
IMC ’17, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
discovers from those VPs. In the data we analyzed from Janu-
ary 2017, M-Lab and Speedtest.net servers provided coverage of
a small fraction of interdomain interconnections observable from
the VP. Between 0.4% (for AT&T) and 9% (for Frontier) of AS-
level interconnections discovered by bdrmap for different access
networks were covered using M-Lab servers, while between 2.3%
(for AT&T) and 28% (for Sonic) of AS-level interconnections were
covered using Speedtest.net servers. In particular, the coverage of
interdomain interconnections using M-Lab servers was low for the
largest U.S. ISPs — 0.9% for Comcast, 0.8% for Verizon, 1.3% for
Time Warner, 1.2% for Cox, 0.4% for AT&T, and 0.7% for Centu-
ryLink. The coverage of AS-level interconnections was higher us-
ing Speedtest.net servers due to the much larger number of servers
as compared to M-Lab — 5.6% for Comcast, 4% for Verizon, 6.7%
for Time Warner, 11.5% for Cox, 2.3% for AT&T and 5.7% for
CenturyLink.
However, the AS-level interconnections discovered by bdrmap
include many customers, especially for large transit networks like
Comcast, Verizon, AT&T, and CenturyLink. Figure 3 is similar
to Figure 2 but reﬂects only interconnections inferred as peers by
CAIDA’s AS-rank algorithm [12]. Arguably, settlement-free (or
paid) peers are more important than customers or providers from the
perspective of interdomain congestion and performance; the respon-
sibility for upgrading congested customer-provider links lies solely
with the customer, while the responsibility is less clear in the case
of peers. Both M-Lab and Speedtest.net provided better coverage
of peer interconnections than they did of all interconnections. Fur-
ther, Speedtest.net servers provided better coverage of both AS and
router-level peer interconnections than M-Lab. For example, M-Lab
servers were able to cover 12 of 41 of Comcast’s peer ASes discov-
ered by bdrmap; 32 of those peers were covered using Speedtest.net
servers. Other networks had similar coverage: between 2.8% (RCN)
and 30% (Sonic) of AS-level peer interconnections were covered
by M-Lab servers, and between 14% (RCN) and 86% (Cox) using
Speedtest.net servers. At the router-level, between 2.4% and 30%
of peer interconnections were covered using M-Lab servers while
between 12% and 78% were covered using Speedtest.net servers.
These statistics suggest that placing throughput-based test re-
sults in the right context requires knowing what fraction of inter-
domain interconnections of an access network a platform can mea-
sure. While M-Lab provides an invaluable server-side measurement
infrastructure to support a number of measurement tests, a compre-
hensive view of interdomain interconnections of access networks
requires substantially more server-side coverage than M-Lab pro-
vides. More generally, building a measurement infrastructure that
will provide visibility into all or even most of such connections re-
quires topology-aware deployment of measurement servers.
5.3 Overlap with interconnections used to access
popular web content
Another factor to consider when designing a measurement infras-
tructure to capture interconnection performance is which intercon-
nections are traversed on paths to popular web content. Figure 4
shows, per Ark VP, the overlap between the set of interdomain in-
terconnections covered using M-Lab and Speedtest.net servers and
those traversed on paths to popular web content (the Alexa targets
described in Section 5.1). For 13 of 16 VPs, we observed AS-level
interconnections on paths to M-Lab servers that were not on paths
to any Alexa targets. For those 13 VPs, between 8% and 25% of
AS-level interconnections on paths to M-lab servers were not on
paths toward any Alexa targets. More importantly, for each VP
we observed AS-level interconnections on paths toward Alexa tar-
gets that were not covered using M-Lab or Speedtest.net servers.
Speciﬁcally, between 79% and 90% of AS-level interconnections
on paths from Ark VPs to Alexa targets were not covered using M-
Lab servers. For example, in the case of our Comcast VP in Bedmin-
ster MA (bed-us), 71 AS-level interconnections were traversed on
paths towards Alexa targets, of which 62 (13 peers, 28 customers,
1 provider, 20 with unknown relationships) were not covered by
M-Lab servers. For the same Comcast VP, 34 AS-level interconnec-
tions (3 peers, 20 customers, and 11 unknown) out of 71 AS-level
interconnections on paths to Alexa targets were not covered using
Speedtest.net servers. The number of AS-level interconnections on
paths to Alexa targets that are not covered is lower for Speedtest.net
than M-Lab, indicating that the larger deployment of Speedtest.net
servers provides better coverage of interdomain interconnections
traversed on paths to popular web content than M-Lab. However,
Speedtest.net is a closed proprietary platform and unlike M-Lab,
does not support custom measurement tools.
5.4 Changes over time
We conducted the entire set of previously described measurements
and analysis — bdrmap to identify interdomain borders, Alexa
lookups, and coverage analysis of interdomain connections using
M-Lab and Speedtest.net servers — in two snapshots, October 2015
and the more recent snapshot from February 2017 described earlier
in this section. Between the two snapshots, interestingly, the num-
ber of M-Lab servers was exactly the same — 261. Speedtest, on
the other hand, expanded their server footprint from 3591 (Octo-
ber 2015) to 5209 (February 2017). However, we found that the
coverage of all AS-level interconnections using both M-Lab and
Speedtest servers actually decreased by a small amount (< 5%) for
all ISPs. We dig deeper into changes in the coverage speciﬁcally for
peer connections because, as stated earlier, those are more impor-
tant from the point of view of interdomain congestion. We observed
the following changes in the coverage of peer AS interconnections
with Speedtest between October 2015 to February 2017: from 69%
to 78% for Comcast, from 81% to 76% for Verizon, from 84% to
86% for Cox, from 63% to 55% for AT&T, from 80% to 79% for
CenturyLink. Apart from the increase in the coverage of Comcast
and Cox’s peer interconnections, the coverage of other networks de-
creased. For M-Lab the corresponding numbers were: 21% to 27%
for Comcast, 31% to 29% for Verizon, 13% to 5% for Cox, 28% to
15% for AT&T, and 23% to 19% for CenturyLink. For M-Lab too
we ﬁnd that that the coverage of Comcast’s peers increased; the cov-
erage of all other networks decreased. This analysis reiterates our
earlier observation that the strategic placement of testing servers is
important to achieve testability of interdomain interconnections.
Challenges in Inferring Internet Congestion
IMC ’17, November 1–3, 2017, London, United Kingdom
6 STATISTICAL CHALLENGES
End-to-end throughput-based measurement to detect congestion in-
volves two steps: the measurement itself, and aggregating measure-
ments to infer congestion on the path. The analysis relies on two
assumptions: (1) Internet trafﬁc has diurnal patterns, and a link is
unlikely to be persistently congested all day. (2) a client is typically
limited by the access link capacity, i.e., links upstream of the ac-
cess link are typically not the throughput bottleneck. Therefore, if
client achieves signiﬁcantly less throughput during peak times than
during off-peak times, a plausible explanation is that the throughput
is being limited by a congested link further upstream of the access
link. While superﬁcially a sound approach, the leap from observing
diurnal patterns in an aggregate set of measurements to claiming
congestion relies on two further—and major—assumptions: (1) the
samples used across the day, and across a variety of access link
conﬁgurations are comparable, and (2) there are well-understood
thresholds for detecting congestion. Two factors shed some doubt
on these assumptions: limitations of crowdsourced measurements,
and ambiguity in what constitutes congestion.
6.1 Limitations of crowdsourcing
Crowdsourcing has advantages in terms of size and richness of re-
sulting samples. It also has limitations:
• Samples cannot be controlled. Any particular home or client
likely generates only one or a few samples, and their network
performance may vary widely.
• Time of day bias. Since users manually launch tests, there
are usually more runs during the day than at night, which
can make the diurnal pattern difﬁcult to discern.
• Service plan variance. It is difﬁcult to get ground truth about
expected performance without input from users, e.g., the
user’s service tier, which would suggest what the user could
reasonably expect from a throughput test. Even within a re-
gion, an ISP could offer service plans with capacities that
vary by an order of magnitude. Such information is typically
available only to access ISPs and the users themselves (al-
though many users do not know their service tier), and web-
based tests cannot automatically obtain it.
• Home network interference. Cross trafﬁc on the home net-
work, especially on Wi-Fi, could affect throughput. Previous
work has shown how home wireless networks have a major
impact on performance [38, 39], and how wireless perfor-
mance could vary signiﬁcantly even across devices within a
single home.
Figure 5 reproduces a graph from M-Lab 2015 analysis [4] that
shows how throughput performance varied by time of day for AT&T
and Comcast users to an M-Lab server hosted in GTT in Atlanta dur-
ing May 2015. The M-Lab analysis stated that “AT&T users expe-
rienced the most consistent patterns of congestion-related degrada-
tion across measurement points on a diversity of transit ISPs, most
notably on GTT for Atlanta . . . Other access ISPs such as Comcast
did not display as substantial of degradation to those same sites
during the same period”.
We examine this case in more detail. Instead of tracking only
median throughput as the report does, we plot the average and stan-
dard deviation of throughput, and number of samples, to illustrate
the four limiting factors described above. First, we see variance is
high; during off-peak for AT&T (Figure 5a), and consistently so in
the case of Comcast (Figure 5b). This variance could be caused by
one of these limiting factors, e.g., differences in service plan rates
exacerbated by the sparseness of measurements from a single client,
or even wireless issues or differences in the home network. Second,
off-peak hours have signiﬁcantly fewer samples—fewer than 20 in
some cases—illustrating the time of day bias. Fewer samples during
off-peak hours is consistent with general network usage, but makes
it difﬁcult to compare peak versus off-peak performance with sta-
tistically signiﬁcant results. With so few samples, the throughput
measurements could be skewed by any of the above confounding
factors.
6.2 Thresholds to detect congestion
Even if we assume that we can compare peak and off-peak through-
put to infer congestion during peak hours, identifying what con-
stitutes congestion is not straightforward. The M-Lab [27] report
identiﬁed examples where the peak-hour download throughput mea-
surements dropped drastically, such as from highs of greater than
10 Mbps to less than 1 Mbps for AT&T (Figure 5a); such drops
can be reasonably attributed to a link on the end-to-end path that is
saturated during peak hours. However, even examples used in the re-
port to contrast with congested links show diurnal patterns as in the
case of Comcast tests to GTT servers in Atlanta (Figure 5b, which
was identiﬁed as an uncongested link in the report.) In this example,
the peak-to-trough difference in throughput for Comcast is about
30% (even removing the off-peak hours with few samples, this dif-
ference is 20%). Such a measurable, non-trivial diurnal throughput
drop raises the question: how large a throughput drop can one safely
interpret as evidence of congestion?
These two cases likely reﬂect two different link states: a link
that becomes congested at peak hours vs. an uncongested link that
sees higher utilization during peak hours (as most links do). For the
AT&T tests in Figure 5a, the drop in throughput, coupled with very
low variance, means that all tests see consistently low throughput,
suggesting peak-hour congestion is the cause. It is more difﬁcult to
attribute a cause to the performance drop of Comcast tests in Fig-
ure 5b. This drop could be due to sample bias, more users sharing
the cable medium during peak hours, or that a subset of Comcast
users experienced lower peak-hour throughput due to contention at
some point on the end-to-end path. This raises the question: is there
a more direct way to identify whether a ﬂow was congested by an
already busy link or whether the ﬂow itself drove congestion in a
(presumably access) link? Distinguishing these two cases is still an
open challenge in throughput-based congestion inference.
7 LESSONS LEARNED
Congestion at ISP interconnections has been a recent focus in the
research, economic, and regulatory arenas. There have been recent,
high-proﬁle, efforts in attempting to understand the extent of such
congestion by using crowdsourced throughput tests from distributed
measurement infrastructures. We used public measurement data
from these efforts, and our own measurement experiments, to inves-
tigate challenges in inferring interconnection congestion using end-
to-end throughput measurements. The methodological challenges
IMC ’17, November 1–3, 2017, London, United Kingdom
Sundaresan et al.
)