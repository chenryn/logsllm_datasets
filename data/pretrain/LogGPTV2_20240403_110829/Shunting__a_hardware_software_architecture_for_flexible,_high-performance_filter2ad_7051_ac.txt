on a ﬂow-by-ﬂow basis.
Such dynamic rerouting allows the Shunt hardware to act as a
load-balancing front-end for a clusterized IDS [26] by dispatching
packets via an Ethernet switch to a designated IDS node on a per-
connection basis. The VLAN rewriting allows the Shunt hardware
to route ﬂows between multiple connections on the same switch for
ﬁne-grained isolation.
Apart from its rule caches, the Shunt behaves like another quad-
port Ethernet card. Our design provides for access to the caches
themselves by reading and writing the Shunt’s SRAM, as the
SRAM supports direct memory-mapped I/O operations.
For much more extensive discussion of the hardware implemen-
tation, including its use of “permutation” and location associative
caches, see [27]. Currently, a known bug in the FPGA board’s
ﬁrmware limits the hardware’s operation to 480 Mbps, but this
problem will be remedied with the next version of the board.
5.
INTEGRATING THE SHUNT WITH
BRO
To test our architecture in practice, we selected the Bro intrusion
detection system [20] as our Analysis Engine, due to its high-level,
ﬂexible, and expressive nature, as well as our strong familiarity
with its internals. To adapt it, we added an API at the Bro scripting
level to support the Shunt’s functionality, and modiﬁed its analysis
policies to then utilize this API. We emphasize, however, that noth-
ing in our Shunting implementation has any particular knowledge
of Bro’s workings.
By itself, Bro provides only limited intrusion prevention func-
tionality. Its scripts can execute arbitrary programs, which are used
operationally to (i) terminate misbehaving TCP connections using
forged RST packets, and (ii) install ACL blocks at a site’s border
router. However, both these actions occur post facto with respect to
the network trafﬁc that led Bro to detect a problem, so for attacks
that proceed quickly, the reaction can come too late. In addition,
router ACL limitations restrict the use of blocking to 100s or per-
haps 1000s of addresses. This might seem like a plenty, but due
to the incessant presence of “background radiation” [18], as well
as the occasional outbreak of worms or large-scale botnet sweeps,
in fact operationally we desire the capacity to block 100,000s of
addresses.
With the Shunt, however, Bro can become a high-performance
IPS. By vetting each packet before it reaches its destination the
combined system can block attacks before they succeed, and proac-
tively block suspect hosts at much larger scale than otherwise.
5.1 Changes to Bro’s Internals
Bro’s stateful nature already requires that Bro track all active
connections and their associated protocol analyzers, as well as all
IP addresses of interest. We extended and annotated these data
structures to incorporate Shunt-related information.
Bro maintains an internal whitelist of
the
PacketFilter class, which speciﬁes a group of systems that
can be safely ignored. We extended this data structure to support
blacklisting as well: IP addresses which should always be blocked.
This whitelist now allows us to populate the IP table in the Shunt
and to update the table on cache misses.
IP addresses,
Bro also maintains a record for every established connection.
Each connection has associated with it a tree of relevant analyz-
ers, ranging from the TCP stream reassembler and signature match-
ing engines to speciﬁc protocol parsers for HTTP, SSH, and other
protocols. Bro can apply multiple protocol analyzers to a single
connection concurrently in order to robustly determine the actual
application protocol without relying on the (increasingly untrust-
worthy) transport port number [10].
We added to this structure notions of “unessential” and “essen-
tial” analyzers, as follows. Unessential analyzers will process a
connection’s packets if present, but the presence of such unessen-
tial analysis does not sufﬁce to require the Shunt to divert those
packets through the Analysis Engine. However, as long as a con-
nection has associated with it at least one essential analyzer, then it
and all other analyzers will receive the connection’s packets. The
decision regarding whether an analyzer is unessential or essential
is made on a per-connection basis, and can change (in particular,
essential analyzers becoming unessential) during the connection’s
lifetime. If every essential analyzer associated with a connection is
either removed or demoted to unessential, it is then safe to install a
forward rule for the connection.
By default, all but the TCP stream reassembler and similar utility
analyzers are considered essential. It is up to the analyzer or its as-
sociated policy script to either mark the analyzer as unessential (so
Virtex 2 Pro FPGA
MAC Group 0
MAC
FIFO
FIFO
FIFO
FIFO
FIFO
FIFO
PHY
PHY
PHY
PHY
Shim 0
Buffer
Packet(cid:31)
Router
Header(cid:31)
Extract
Decision
MAC1..3
Shim 1..3
2 MB SRAM
(Host FIFO)
2 MB SRAM
(Shunt Tables)
PCI Interface
Arbiter
MemCtl
ToHost
FromHost
Figure 3: The Shunt Hardware Block Diagram. Items in dark were modiﬁed from the NetFPGA reference design.
that it may still receive trafﬁc) or simply remove it from consider-
ation. Even when all analyzers are considered unessential, the for-
ward rule uses a lower priority than the default shunt rule for TCP
SYN/FIN/RST packets, so connection finished and similar
end-of-session accounting still operates properly.
The API for forward-N functionality takes two parameters: the
number of bytes to skip (relative to the point in the stream pro-
cessed so far), and a smaller value indicating the initial number of
these bytes not to skip. The function (part of the TCP stream re-
assembler) converts the byte count to a sequence number to specify
in a forward-N table entry. However, the stream reassembler does
not install this entry until having ﬁrst processed the given number
of initial bytes. We chose this interface to support common func-
tionality in which an analyzer (such as that for HTTP) determines
that a large item will soon be transferred and wishes to inspect only
the beginning of the item. (If we instead left it up to the analyzer to
request forward-N after it has received the beginning of the item,
often that is difﬁcult for the analyzer to coordinate due to the layer-
ing by which it receives aggregated information. For example, one
of Bro’s natural interfaces for doing so delivers an entire item as a
unit to the higher-level analysis, rather than doing so piecemeal.)
Since Bro is not multithreaded, if it determines that a packet
should be dropped to block an attack, the drop directive will oc-
cur prior to Bro beginning to process the next packet. Thus, at
the point where Bro’s internal engine requests a new packet, if the
analysis of the current packet did not explicitly indicate it should
be dropped, we know we can safely go ahead and forward it. This
approach limits the latency introduced by the architecture to Bro’s
per-packet total analysis time, typically well under 1 msec.
6. SHUNT-AWARE POLICIES
In order to effectively leverage Shunting, we must adapt the
Analysis Engine system to best employ it for forwarding unin-
teresting trafﬁc and blocking problematic trafﬁc. In this section,
we discuss changes and extensions we introduced to the Bro IDS
in this regard. We note that we intend these modiﬁcations as ex-
emplary rather than complete; we present fairly modest additions
(with respect to Bro’s full suite of analysis) that nevertheless yield
signiﬁcant performance gains.
When constructing these modiﬁed analyses, we have to ensure
that they are “safe”: that it is acceptable to ignore the forward’ed
trafﬁc without impairing the security analysis.
6.1 SSH
For SSH analysis, we would like to produce a log of all SSH
sessions (including time and volume of data transferred), client
and server software versions, and detection of brute-force pass-
word guessing. To this end, we modiﬁed Bro’s SSH analysis
script as follows. We ﬁrst added an event handler for Bro’s
connection closed event to log the time, source, destination,
and volume of the session, where we compute the volume of the
session based on the difference in sequence numbers between the
connection’s SYN and FIN packets. To check for SSH brute-force
attacks, we allocate a per-source counter. When a connection be-
gins, we increment the counter and initiate polling of the connec-
tion for the next 10 seconds, where every 100 msec we assess the
connection’s status. As soon as the connection transfers more than
10 KB of data, we assume that the user successfully authenticated,
and reset the count to zero.
If instead the counter ever reaches
a predeﬁned threshold (currently 10), indicating multiple short-
lived SSH session, we generate a Bro “notice” reﬂecting a likely
password-guessing attack.
If the polling process determines that the connection appears le-
gitimate and/or inactive, the script demotes the SSH analyzer to
unessential (as discussed in the previous section). Now the Shunt
will forward all subsequent SSH trafﬁc except for the ﬁnal FIN or
RST (unless the user’s conﬁguration has incorporated other analyz-
ers still deemed essential).
As a result, we can avoid processing nearly all SSH trafﬁc, while
still retaining the ability to (1) detect password guessing, (2) de-
termine the approximate size of ﬁle transfers, (3) inspect SSH ver-
sion strings (present in each connection’s initial data exchange),
and (4) distinguish between ﬁle transfers and interactive sessions
in the log (as ﬁle transfers sustain much higher data rates that do
interactive sessions).
6.2 HTTP
For HTTP, far and away most of the bytes transferred come in
server replies. Although some ﬁles (e.g., HTML, Java, Javascript,
Flash) beneﬁt signiﬁcantly from IDS analysis, much of the data
comes instead in the form of images, video, audio, and binary trans-
fers.
We modiﬁed Bro’s HTTP reply analysis script to capture the
MIME type and expected length of all responses. Then, for any
response over a given size (default 10 KB), we examine the MIME
type. If the type matches one in a conﬁgurable “presumed safe”
MIME Type
Probably
application/safe
video/*
application/unsafe
text/*
image/*
audio/*
binary/*
multipart/*
other
Safe
Yes
Yes
No
No
Yes
Yes
No
Yes
No
Percentage of Average
payload data
Size
33.7% 1.4 MB
28.5% 8.9 MB
60 KB
14.7%
8.8%
22 KB
8.5% 7.8 KB
5.4% 2.6 MB
0.6% 218 KB
0.3% 354 KB
10 KB
<0.1%
Table 1: The different MIME types, whether the type is consid-
ered “probably safe”, the percentage of the total HTTP replies
of each MIME type, and the average payload size for the UNI-
VERSITY I trace.
whitelist (default:
images, video, audio, and some application
types), the script instructs the TCP stream reassembler to skip over
the payload using forward-N. Otherwise, or if the size is unavail-
able (e.g., due to use of HTTP “chunking”), we perform the full
regular analysis.
In general, these “presumed safe” types represent the bulk of the
HTTP transfers. Table 1 lists the different MIME types observed in
the UNIVERSITY I trace (see Section 7 for trace details); whether
we consider items of the given type as likely safe; the fraction of
the HTTP responses they represent; and the average item size for all
such HTTP responses that specify a payload length. For application
data, we currently consider binary, msword, octet-stream,
phdata, pdf, vnd.ms-powerpoint, x-xpinstall, x-sh,
x-pkcs7-crl, x-tar, x-zip-compressed, and zip as
“presumed safe”. For some of these, we might want to conduct
further analysis, but Bro presently lacks analyzers speciﬁc to these
item types. If it included these, we suspect that often the analyzer
would only need to inspect the beginning of the item transfer (per
the next paragraph) to determine whether the item was potentially
problematic; if not, then we could still skip the remainder of the
item.
Even when skipping the payload, however, we still examine the
beginning of each item, regardless of ﬁle type. This allows us,
for example, to perform signature analysis to verify whether the
item’s actual type corresponds with its stated type. The savings we
present in our evaluation assume we inspect (and thus cannot skip)
the default value of the ﬁrst 5 KB of each item.
6.3 Dynamic Protocol Detection
Bro’s Dynamic Protocol Detection (DPD; [10]) initially analyzes
all trafﬁc in order to determine the protocols (primarily at the appli-
cation layer) actually embedded in a data stream. Bro’s signature
engine [24] matches the initial (default 2 KB) data in each connec-
tion to ﬁnd candidate protocols that might match the stream. Bro
then instantiates instances of these analyzers which concurrently
process the stream from the beginning. Whenever an analyzer con-
cludes the stream cannot belong to its protocol, it drops out of fur-
ther analysis. Otherwise, it continues to process future packets as
they are received.
We incorporate DPD into the Shunting framework by initially
marking the corresponding signature analyzers as essential. At the
2 KB limit, we demote the signature analyzers to unessential. If
no other essential analyzer remains active at that point, then Bro
installs a forward entry to skip over the remainder of the connection
Trace
UNIVERSITY I
UNIVERSITY II
UNIVERSITY III
LAB I
LAB II
SC I
Percentage forwarded
Packets
Bytes
54.9%
43.8%
47.0%
58.1%
52.5%
69.9%
75.7%
84.5%
79.2%
88.2%
91.1%
88.0%
Table 2: Fraction of forwardable (non-analyzed) trafﬁc
(except for its ﬁnal FIN/RST control packets, which are matched by
the Shunt’s higher-priority static ﬁlter).
If DPD did identify the ﬂow’s protocol, however, then Bro will
have classiﬁed the corresponding analyzer as essential, and it (and
other inessential analyzers) will continue receiving the ﬂow’s traf-
ﬁc. Thus, Shunting does not affect DPD’s ability to detect the pro-
tocol present in a trafﬁc ﬂow.
7. EVALUATING THE SHUNT
To evaluate the efﬁcacy of the Shunting architecture, we mod-
iﬁed Bro’s interface for reading trace ﬁles to preprocess packets
read from traces using the Shunting decision tables. Doing so al-
lowed us to evaluate the tradeoffs for different analysis/forwarding
schemes, as developed in the previous section.
We used six traces: three from a large university with several 10s
of thousands of users (University I, II and III), two from a research
laboratory with 8,000 hosts (Lab I and II), and one trace from a su-
percomputing center with thousands of users (SC I). We developed
our modiﬁcations to Bro’s processing using only UNIVERSITY I,
using the other traces solely for evaluation.
UNIVERSITY I spans one hour and captured 50% of the trafﬁc
crossing the border of the university, which employs per-ﬂow load-
balancing across two heavily-loaded Gigabit Ethernet links. The
trace (captured mid-afternoon on a workday), which includes all
packets and their payloads, was constructed from subtraces cap-
tured with a cluster of six machines, and totals 222 GB. UNIVER-
SITY II consists of one hour of trafﬁc, totaling 196 GB, recorded
at 4PM on a Friday. We collected UNIVERSITY III at 2–3AM
on a Saturday morning, to reﬂect an off-hours workload. It totals
109 GB.
Due to a node failure undetected during the capture process,
UNIVERSITY II and UNIVERSITY III only captured 41% of the
trafﬁc rather than 50%. One subtrace on UNIVERSITY III reported
a .02% packet drop,4 while all other traces reported no drops.