110789
110773
110750
110825
110661
110705
110876
110841
110818
110827
110755
110824
110758
upper
limit µs
111403
111315
111407
111410
111473
111308
111396
111415
111302
111295
111260
111348
111417
111330
111362
host2
host4
host6
host8
host10
host12
host14
host16
host18
host20
host22
host24
host26
host28
host30
An interesting observation is that
there is very little
difference between the measurements that
traversed one
router vs. those that traversed none. However once the trafﬁc
traverses 3 routers, the RTT increases. In the future we
will test the hypothesis that the jump is primarily due to
traversing the site’s core router.
The procedure to set the bounds for a particular hardware
type is to measure it at 1 link and at the maximum number
of links on the LAN. From our previous section we know
that using the limits from any host of the particular hardware
type will work for the other hosts. So we can use the upper
limit from a host at 11 links, and the lower limit from the
host at 1 link, to obtain a very tightly bound for expected
response times for this hardware type at any location on the
LAN.
E. Performance measurement
Because timing-based attestation mechanisms should have
exclusive control of the system while they are performing
their self-check, this lockup of the system could lead to de-
creased performance. We wanted to know what performance
effects would be caused by taking control of a system during
attestation. We tested this with 2 measurement frequencies.
The ﬁrst was a measurement every 2 seconds, which is the
fastest we allow our server to request measurements. The
second was measurement every 2 minutes, which is the
maximum frequency we have ever used on volunteer end
users, due to previously not having had performance mea-
surements. The data indicate there is negligible performance
effects for all tests except CPU performance when measuring
every 2 seconds.
AOGenMark [6] tests the CPU by performing a series
of complex calculations on a polygon mesh; the higher the
247
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:23 UTC from IEEE Xplore.  Restrictions apply. 
Figure 7. Network RTT for two Dell Optiplex 960s. 2 second measurement frequency, 2.5 million loops, varying network link counts.
benchmark, the faster the CPU. Additionally, the user can
increase the computational load by increasing the number
of samples, although the term samples is not well deﬁned
in the documentation. AOGenMark was run with 4 threads
each simultaneously executing on a different core. Each
thread had its number of samples set
to 16. Ten tests
were performed, and the mean for these tests is shown
in Figure 8. This shows that for 2 minute measurement
the CPU performance overhead is negligible (it was less
than one standard deviation,
.15 benchmark units), and
with 2 second measurements the overhead is 4.8%. This is
statistically signﬁcant, but is in the expected range based on
how much time the self-check is excluding all other code
access, because the 111ms measurement duration is 5.55%
of a 2 second measurement period.
For network throughput testing, we used iperf [25]. We set
up an iperf server application on a non-target machine that
communicated with a client iperf program running on the
test machine. The iperf server communicated with the client
machine every two seconds to determine the throughput
capacity, in Megabits/sec, between the host and the client
over a 180-second interval. Ten 180-second interval tests
were measured. The mean and standard deviation of the
throughput capacities was calculated for each 180-second
run. The mean of the means was calculated over all ten of
the 180-second runs. Because the means for the 2-minute
measurements and 2-second measurements fall well within
one standard deviation (6.32 Mbps) of the test run with-
out measurement, there was negligible network throughput
impact. The normalized network performance is shown in
Figure 8.
Iozone [13] tests the time taken to write to a new ﬁle
and read from a ﬁle. The test ﬁle size and memory record
size used by system memory were set to 3,072 bytes. Two
thousand measurements were taken for each of the tests, and
Figure 8.
Normalized performance overhead for ﬁle system, network
throughput, and CPU benchmarks. Values >1 indicate decreased perfor-
mance, values < 1indicate improved performance.
the mean was calculated over the two thousand samples.
Because the means for the 2-minute measurements and 2-
second measurements fall well within one standard deviation
(7.07 seconds) of the test run without measurement, there
was negligible read or write performance impact. The nor-
malized ﬁle IO performance is shown in Figure 8.
Overall, measuring with either 2 minute or 2 second
these performance
frequency did not signiﬁcantly affect
measures. Although these measurements were meant
to
determine the impact on measurement on the endpoint, they
also provide data about the effect of the endpoint’s load on
the observed RTT. We conﬁrmed that the timing baseline
for the benchmarked host under load differed less than a
standard deviation from the host not being benchmarked.
248
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:23 UTC from IEEE Xplore.  Restrictions apply. 
 110000 110500 111000 111500 112000 112500 113000 113500 0 50 100 150 200Measurement RTT (us)Measurement numberAttackAbsentAttackPresent"host1_1link""host2_1link""host1_2links""host2_2links""host1_3links""host2_3links""host1_8links""host2_8links""host1_10links""host2_10links"a single initial “hands-on” provisioning process, then another
tool can create a baseline for the number of TPM ticks to
compute the self-check. This initial baseline can be used
to bootstrap any future updates that occur to the self-check
function. It is only necessary to include a special “update
tickstamp” action in the client. This would invoke the
previous version tickstamp-wrapped self-checksum, send the
results, then invoke the new tickstamp-wrapped checksum,
and send the results. If the server validates that the previous
version tickstamp timing is within the expected range, then
it will accept in the integrity of the new version, and can
use multiple results to set a new TPM ticks baseline.
the absolute time it
There is one further complication for practical use. Due to
latency with the communication and processing performed
by the TPM,
takes to perform a
tickstamp-based attestation is approximately 1.3 seconds for
these TPMs and these hosts. From the previous experiment
we know the absolute runtime for this self-check function
with this number of iterations on these systems to be
approximately .1 seconds. Therefore we are left to conclude
that there is at least 1 second of overhead for performing
the two tickstamp operations on these TPMs. In which case
there is no way that we can decrease the number of iterations
of the checksum loop in order to make the time signiﬁcantly
less perceptible to the user.
Thus, while the TPM tickstamp method gives us higher
conﬁdence in the detectability of an attacker than simply
hashing our code and extending it into a Platform Control
Register, it is constrained in the situations in which it can
be used. Past work that had attestation times on the order
of seconds [21] [7] have suggested use cases where the
attestation is only invoked in response to special events such
as an authentication attempt, where a user may be more
willing to wait a short duration of the processing.
G. Variation in TPM behavior per manufacturer
When trying to understand the reason for the variation
in timings between hosts with the same TPMs, we reasoned
that one of the main differences between each of the hosts is
that they did not have the same keys. Therefore we wanted
to see if the timing behavior was in any way dependent on
the signing key used by the TPM. To test this we scripted
tools to rekey the TPM, perform 10 tickstamp timing mea-
surements, average the resulting delta ticks, and then repeat.
Due to the latency incurred from rekeying the TPMs and
performing the tickstamp operations, and the fact that we
had only temporary access to the lab machines, we were only
able to do this for 50 keys on 10 hosts. Overall the results
did not seem to indicate per-key variability, and an example
distribution is shown in Figure 10. However, we also tested
this theory on some of our Dell Latitude laptops and found
anecdotally that
the Broadcom TPM on these machines
exhibited differing tickstamp times depending on the key.
An example for a Latitude D820 is shown in Figure 11.
Figure 9.
measurement frequency, 2.5 Million self-check loops.
TPM tickstamp time for 32 Dell Optiplex 960s. 2 second
F. TPM tickstamp-based timing measurement for 32 hosts
All of the hosts for these experiments had a TPM vendor
ID that corresponded to STMicroelectronics, and the chip
was labeled on the motherboard as N18FPVLR. There are
32 hosts rather than 31 for the previous experiment, because
one host had the wrong software version for the timing tests,
but the software still was correct for TPM tests.
We requested TPM tickstamp measurements from the
same machines used in the timing measurements. Note that
the 5th line from the top in Figure 9 corresponds to a host
for which the TPM tickstamp times are far more variable.
We had used this host extensively in previous experiments,
and the times that were previously reported had much lower
variation. Therefore the only explanation that we can offer at
this time is that we may have “worn out” this TPM through
overuse.
From this data we can conclude that while the attacker is
potentially detectable, contrary to expectations, each host’s
TPM has slightly different timing despite being the same
hardware. Thus we cannot set a single baseline for the
expected number of TPM ticks for the self-checksum across
different hosts. We are not aware of this behavior having
been previously reported, and this is one of the key results
derived from actually implementing the proposed TPM
tickstamp protocol. However we still believe there is room
for utilizing the tickstamp timing on real systems. This
is because in the ideal case a TPM is provisioned when
it ﬁrst enters an organization. A conservative provisioning
process will also boot into a dedicated environment such
as a Linux boot CD in order to communicate with the
TPM with a signiﬁcantly reduced possibility of a man-
in-the-middle attack occurring where an attacker reports a
public key for a private key that he, rather than the TPM
controls. It is reasonable to expect that if the TPM requires
249
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:23 UTC from IEEE Xplore.  Restrictions apply. 
AVERAGE RTTS IN MICROSECONDS, WITH AND WITHOUT PROXY
ATTACKS, 100 MEASUREMENTS EACH
Table VII
Average RTT
No attack
109897µs
Average RTT, proxied to
Host 3 (Optiplex 960)
Q9650 at 3GHz
111455µs
128282µs
111320µs
Host 1, Optiplex 960
Q9650 at 3GHz
Host 2, Latitude D630
T8300 @ 2.4GHz
AVERAGE RUNTIME IN TPM TICKS, WITH AND WITHOUT PROXY
ATTACKS, 100 MEASUREMENTS EACH
Table VIII
Average ticks
No attack
844.25 tpm ticks
Average ticks, proxied to
Host 3, Optiplex 960
Q9650 @ 3GHz
844.87 tpm ticks
16141 tpm ticks
15720 tpm ticks
Host 1, Optiplex 960
Q9650 @ 3GHz
Host 2, Latitude D630
T8300 @ 2.4GHz
the paper, when the latency is on the order of 1ms as it
is in our results, application of their system as-is will be
difﬁcult, so it will require modiﬁcation. We also believe
that proxy attacks are currently detectable on LANs with
host-to-host network visibility. Because the attesting system
should be locked up during attestation, any communication
seen destined to and returning from another host during the
attestation time window is immediately suspicious.
V. “THE ONLY WINNING MOVE IS NOT TO PLAY”
OR “ET TU TOCTOU?”
As we have shown above, Checkmate can provide prac-
tical timing-based attestation of code integrity for COTS
OSes like Windows. The ability to say “This code ran
unmodiﬁed.” is a powerful capability not found in today’s
commercial security software that we think needs to be
added. Unfortunately this is not enough to make the software
trustworthy.
The problem of Time Of Check, Time Of Use
(TOCTOU) attacks has been brieﬂy mentioned implic-
itly or explicitly in numerous
trusted computing pa-
pers [20] [17] [21] [7] [9] [10] [28], but it is not often
addressed head on. The only paper we are aware of which
did explicitly try to tackle one facet of this problem was
“TOCTOU, Traps, and Trust” [2], however it was concerned
only with the gap between load-time and runtime measure-
ment and how load time measurement was insufﬁcient. It
did not deal with the more subtle attacks that can target
runtime attestation. We believe that the effects of TOCTOU
attacks are currently under-stated when dealing with remote
attestation systems where the attacker is assumed to be at the
Figure 10. TPM tickstamp frequency distribution for 50 keys. STMicro-
electronics TPM in Dell Optiplex 960.
Figure 11. TPM tickstamp frequency distribution for 100 keys. Broadcom
TPM in Dell Latitude D820.
We believe more work should be done to understand why
different TPMs are exhibiting different timing behavior.
H. Proxy attacks against timing-based attestation
Proxy attacks are the second most effective attacks against
timing-based attestation systems, behind TOCTOU attacks.
This is because it is difﬁcult to achieve message origin
authentication when it is assumed that an attacker has full
access to any cryptographic keys in memory. We have
implemented a proxy attack to understand how much latency
is incurred by a host resending its measurement request to
another host. We performed these tests for the best-case
situation for the attacker, where the clean host and the
proxying host are on the same ethernet segment. The results
are shown in Table VII. There is about a 1.5ms overhead
visible when the attacker is proxying to a host of the same
speed. However when the attacker proxies to a faster host,
this overhead is easily negated. The results of Table VIII
indicate that the granularity of TPM ticks for these TPMs
is too coarse to effectively detect
the 1.5ms overhead.
And again, when the attacker forwards to a signiﬁcantly
faster machine, the timing decreases, allowing an attacker
to transparently forge results.
We believe there is potential for applying VIPER’s [10]
system for detecting the latency inherent in proxying com-
munications to our work. However, as was mentioned in
250
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:23 UTC from IEEE Xplore.  Restrictions apply. 
about to start.
2) The attacker must have some un-measured location to
hide in for the duration of the measurement.
3) The attacker must be able to reinstall as soon as
same privilege as the defender. Attestation always requires
some amount of measurement, and all measurement systems