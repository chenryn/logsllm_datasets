title:An historical examination of open source releases and their vulnerabilities
author:Nigel Edwards and
Liqun Chen
An Historical Examination of Open Source Releases and
Their Vulnerabilities
Nigel Edwards, Liqun Chen
Hewlett-Packard Laboratories
Long Down Avenue
Bristol, BS34 8QZ, UK
@hp.com
ABSTRACT
This paper examines historical releases of Sendmail, Post-
ﬁx, Apache httpd and OpenSSL by using static source code
analysis and the entry-rate in the Common Vulnerabilities
and Exposures dictionary (CVE) for a release, which we
take as a measure of the rate of discovery of exploitable
bugs. We show that the change in number and density of
issues reported by the source code analyzer is indicative of
the change in rate of discovery of exploitable bugs for new
releases — formally we demonstrate a statistically signiﬁ-
cant correlation of moderate strength. The strength of the
correlation is an artifact of other factors such as the degree
of scrutiny: the number of security analysts investigating
the software. This also demonstrates that static source code
analysis can be used to make some assessment of risk even
when constraints do not permit human review of the issues
identiﬁed by the analysis.
We ﬁnd only a weak correlation between absolute values
measured by the source code analyzer and rate of discovery
of exploitable bugs, so in general it is unsafe to use abso-
lute values of number of issues or issue densities to compare
diﬀerent applications or software. Our results demonstrate
that software quality, as measured by the number of issues,
issue density or number of exploitable bugs, does not always
improve with each new release. However, generally the rate
of discovery of exploitable bugs begins to drop three to ﬁve
years after the initial release.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection
General Terms
Security, Measurement
Keywords
Static Analysis, Risk Analysis, Open Source Software
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.
1.
INTRODUCTION
In this paper we present an investigation of the vulnerabil-
ity history of various open source projects. We use a static
source code analysis tool to investigate sample releases over
a number of years for potential security issues and compare
the results to the rate at which entries appear for the soft-
ware in the Common Vulnerabilities and Exposures dictio-
nary (CVE1) [12].
The purpose of the investigation is to understand what
static code analysis of software can tell us about the poten-
tial future intrinsic risks of using that software. The size and
complexity of much commonly used software renders man-
ual analysis impractical: the Linux kernel contains over 13
million line of code and OpenOﬃce contains over 9 million
lines of code at the time of writing. There is also no guar-
antee that any particular piece of software has been subject
to rigorous analysis by skilled security analysts. Therefore
we believe automatic analysis is needed to make a sound
assessment of the risk of using any software.
In this study we are interested in estimating the number
of bugs that might be in the software that could allow the
construction of a successful exploit leading to system com-
promise, for example: buﬀer overﬂow, cross-site-scripting
and SQL injection. We call these exploitable bugs. As well
as comparing diﬀerent releases of the same software, we also
compare diﬀerent software. Our method of comparison is to
use a static source code analysis tool and measure the num-
ber of security issues it identiﬁes. We are trying to answer
the following questions.
1. Does the change in number of issues or issue densities
between a previous release and a new release of the
same software indicate anything?
2. What is the range of issue densities for popular open
source software?
3. Do very large diﬀerences in issue densities or number
of issue between diﬀerent software tell us anything?
CVE is a dictionary of common names, “CVE Identiﬁers”,
for publicly known security vulnerabilities. Its purpose is to
provide one name for each vulnerability to enable the use of
multiple tools and databases. CVE identiﬁers are assigned
so that each separate vulnerability is assigned a unique CVE
Identiﬁer – see [12] for further details. We use the rate at
which entries appear in CVE for the software after the re-
lease date as an estimate of the number of exploitable bugs
1CVE is a trademark of The MITRE Corporation.
183it contained on its release date. So if the rate of appear-
ance of CVE entries for the software after its release date
is very low, then one might say that particular release had
relatively low number of exploitable bugs. Conversely if the
rate is higher, we assume that there were a larger number
of exploitable bugs. The validity of these assumptions is
discussed next.
The approach of using the CVE entries to estimate the
number of exploitable bugs that might have been in a re-
lease has obvious limitations: some software is subject to
greater investigation by security researchers than others —
the degree of scrutiny also matters. We also look at this
eﬀect.
In addition not all exploitable bugs may result in
CVE entries. Not all exploitable bugs may be discovered or
reported. So CVE cannot be used as an absolute measure
of the number of exploitable bugs. Assuming the degree of
scrutiny is constant, we believe it can be used to indicate
whether or not there were more or less security bugs in a
given release.
We chose the following software for our study.
Sendmail Email server software: 1996 to 2011
Postﬁx Email server software: 1999 to 2010
Apache httpd Web server software: the 1.3 release series
from 1998 to 2010; the 2.0 release series from 2002 to
2010; the 2.2 release series from 2005 to 2011
OpenSSL A toolkit for implementing SSL and TLS: the
0.9.6 release series from 2000 to 2004; the 0.9.7 release
series from 2002 to 2007; the 0.9.8 release series from
2005 to 2011; the 1.0.0 release series from 2009 to 2011
All of the above are very widely used to provide Internet
accessible services. Therefore there are many millions of
instances — over 320 million Apache web servers in Octo-
ber 2011 [15]. The wide spread availability makes them a
tempting target for attackers and security researchers, so
the degree of scrutiny is high. The choice of software was
further driven by the availability of public release archives
enabling us to obtain older releases. Both Apache httpd
and OpenSSL have a number of separate major release se-
ries in which substantial new amounts of code were intro-
duced with a new series (e.g. changing from 1.3 to 2.0 for
Apache). These major releases series evolved separately and
are maintained in parallel. Therefore we have analyzed each
of these series separately. We did not have the resources to
analyze all instances of a release series, so we took samples
approximately one year apart.
The remainder of this paper is structured as follows. Sec-
tion 2 gives an overview of static source code analysis. Sec-
tion 3 presents the results of our analysis of Sendmail, Post-
ﬁx, Apache and OpenSSL. Section 4 discusses to what ex-
tent it is possible to compare analysis results from diﬀerent
software. In section 5 we discuss degree of scrutiny, using
additional CVE histories of two open source databases. Sec-
tion 6 describes some related work. Section 7 is our conclu-
sion. The full analysis results are given in appendix A.
2. OVERVIEW OF STATIC SOURCE CODE
ANALYSIS
Static analysis of source code is the automatic examina-
tion of source code to determine particular non-functional
properties of interest. The term “static” is used to denote
that no execution is involved in contrast to “dynamic” anal-
ysis in which some form of execution and test data set is
usually involved. Static analysis is used for a variety of pur-
poses including type-checking, style-checking, performance-
optimization and program veriﬁcation.
In this paper we
are concerned with security analysis which is to detect the
presence of bugs which may lead to security problems —
exploitable bugs.
Static source code analysis attempts to detect exploitable
bugs automatically.
It uses data ﬂow analysis techniques
[16], [6] to trace the potential paths of ingested data through
the program. Each time a call of a dangerous function
such as strcpy() or mysql query() occurs the analysis tool
checks function-speciﬁc rules about the parameters being
passed: whether the length of the target is greater than the
length of the source for strcpy(); whether the query param-
eter of mysql query() has been cleansed – characters that
might modify the query (e.g. SQL injection) have been re-
moved.
Unfortunately many static analysis problems are unde-
cidable (as a consequence of Rice’s theorem [22], see also
discussion in [11]) which means that static analysis tools
must use approximation techniques. For example, they may
analyze paths that can never be executed2. Typically the
consequence of this is that the tool will identify many more
issues for a program than there are security bugs. Therefore
the results it produces must be vetted by human auditors
to determine their legitimacy and impact.
In addition, it
is often possible to add program-speciﬁc rules. For exam-
ple, the software may contain a data cleansing routine. So
we might choose to add a rule that declares any ingested
data that ﬂows through the cleansing routine to be safe so
it won’t trigger SQL injection or buﬀer overﬂow. Building
a set of program-speciﬁc rules requires inspection of source
code to determine how and when data is cleansed. Typi-
cally this is done by auditors inspecting the results from the
ﬁrst and subsequent analyses. Each issue identiﬁed by the
analyzer is considered and the code that triggered the issue
inspected to determine if a custom rule to suppress the is-
sue is warranted. One rule may suppress many issues. This
paper explores the value we can derive when constraints do
not permit human review of all the issues. What can the
raw results do for us?
It follows from the above that the number of issues or
density of issues for two diﬀerent programs cannot be taken
as an absolute measure of the number of security defects
[5]. The diﬀerence might be explainable because, in the ab-
sence of program-speciﬁc rules, the tool is doing a better
job of understanding one program compared to the other.
However, whilst we accept some range of diﬀerence is to be
expected and is unlikely to be signiﬁcant, we seek to under-
stand the range of issue densities and determine if extremely
large diﬀerences are signiﬁcant.
3. THE ANALYSIS
In this section we present and discuss the results of our
analysis. Full results are given in appendix A.
2Some tools are capable of detecting some cases of “dead
code”: code which is never executed. Dead code can arise
because of interfering logical constraints; this can be arbi-
trarily diﬃcult to detect in practice.
184For our study we used the HP Fortify Source Code Ana-
lyzer (SCA) version 5.10.0.0102 without any program-speciﬁc
or custom rules. Other static source code analyzers include
IBM Rational AppScan and Klocwork Insight. For all soft-
ware we conﬁgured the analyzer to trust the local system:
the ﬁle system, environment variables and the like. So the
primary source of untrusted data is the network.
The analyzer classiﬁes issues as “Critical”, “High” or “Low”.
For each software release that we analyzed we consider the
following metrics which we generate from SCA.
• The total number of issues (TI): critical + high + low
• The total issue density (T-density): the number of
critical, high and low issues per 100 lines of executable
code
• The number of critical issues (CI)
• The critical issue density (C-density): the number of
critical issues per 10,000 lines of executable code
• The number of critical and high issues(CHI): critical+
high
• The critical and high issue density (CH-density): the
number of critical and high issues per 1,000 lines of
executable code
We compared these measurements to the number of en-
tries appearing in CVE per year for that software release
(CVE/yr). CVE only contains entries from 1999 on. CVE
entries up to the end of calendar year 2011 are included.
Note that the units for the density metrics, T-density, C-
density and CH-Density are diﬀerent: respectively issues per
100 lines, issues per 10,000 lines and issues per 1,000 lines.
This is because we believe it makes comparison of the full
results from diﬀerent software given in appendix A simpler.
It is easier to compare C-densities of 7.64 and 25.26 (number
of critical issues per 10,000 lines of code) rather than 0.0764
and .2526 (number of critical issues per 100 lines of code).
We did apply some ﬁltering to the CVE entries. Only
entries which detailed problems with the software being an-
alyzed were used. We excluded entries in which references
were made to the use of the software, but the bug lay else-
where. We included entries which were speciﬁc to a single
operating system: due to the limited information available
it is usually not possible to tell if these are simple packag-
ing errors or coding errors. Examples of CVE entries we
excluded include the following.
• Incorrect use of APIs by third party software. In 2009
there were at least 41 CVE entries that referenced
OpenSSL, 29 of these were reports of incorrect use of
OpenSSL by third parties and are therefore excluded
from our analysis. Examples include CVE-2009-5057
(incorrect conﬁguration of OpenSSL) and CVE-2009-
3766 (failure to verify the domain name in the Com-
mon Name ﬁeld of a certiﬁcate when using OpenSSL).
The remaining 12 CVE entries in 2009 were bugs in
various releases of OpenSSL software and are there-
fore included in our analysis.
• Bugs in third party plug-in software or plug-ins. For
example CVE-2009-1012 concerns a bug for the Ora-
cle BEA Weblogic plug-in for Apache httpd. This is
not a bug in any software that was included in any of
the httpd releases. It is therefore excluded from our
analysis.
We make the simplifying assumption that the rate of CVE
occurrence is constant over any given year. We assume each
CVE entry corresponds to one distinct issue. Although this
is the intent of the CVE editorial policy and seems to be the
case for the large majority of entries, we cannot guarantee
that a few CVE entries do not refer to multiple bugs or
that some entries may be duplicates. Classiﬁcation is an
imprecise activity depending on human skill and judgment.
To calculate the number of CVE entries per year, CVE/yr,
for a release rn, we use the CVE entries that occurred from
the release of rn until the next release analyzed rn+1 and
divide by the time interval between release dates of rn and
rn+1 release. The time interval spans fractions of years,
as software is not generally released on January 1st. We
therefore apportion CVE entries based on the fraction of
the year covered by the time interval between release dates.
Let dn denote the day of the year on which rn was released,
and let dn+1 denote the day of the year on which rn+1 was
released. Then if rn was released in y1 and replaced by rn+1
in y2, the CVE entries per year, or CVE/yr for rn is given
by:
cvey1 × (365 − dn) + cvey2 × dn+1
365 − dn + dn+1
More generally, for a release interval spanning m years, m >
2, the CVE/yr is given by:
cvey1 × (365 − dn) + cveym × dn+1 + 365 × Pi=m−1
i=2
cveyi
365 − dn + dn+1 + (m − 2) × 365
If rn and rn+1 were released in the same year, then the
CVE/yr is given by:
cvey1 × (dn+1 − dn)
dn+1 − dn
= cvey1
Particularly for the older CVE entries, it is not always possi-
ble to determine to which versions of the software the entry
applies. Also in many cases there were beta releases preced-
ing the ﬁrst release which we analyzed. These beta releases
do not appear in the software archives and so we could not
analyze them. However, they still have CVE entries. By
weighting the CVE count with the number of days for which
the software was available in any given year we compensate
for these eﬀects.
For example, OpenSSL 0.9.7 was released December 31st,
2002. As shown in table 4 in appendix A, there are 4
CVE entries for OpenSSL 0.9.7. All these entries applied to
beta releases of 0.9.7 which are not in the OpenSSL source
archives. The next release we analyzed was 0.9.7c which was
released on September 30th, 2003 (day 272). The CVE/yr
for 0.9.7 is given by:
4 × (365 − 364) + 7 × 272
365 − 364 + 272
= 6.99
Thus the 4 CVE entries for 2002 are given negligible weight
compared to the 7 for 2003.
We did not have the resources to analyze all consecu-
tive releases of all the software, so we took samples ap-
proximately 12 months apart.
In some cases the sample
time is longer because the software was stable and exhibited
1858.7.6, which was itself released in 1996. We excluded these 7
entries from our count of CVE entries, but believes it justiﬁes
including release 8.7.6 in our analysis, since it was clearly the
subject of security analysis work in 1999.
For easy visual comparison the metrics are scaled as shown
in the ﬁgure so that all seven data sets can be represented
by a common vertical axis. Full analysis details including
the unscaled values are given in appendix A table 5. The
earliest releases we analyzed had a fairly large number of
issues reported by SCA: 2548 (total) and 136 (critical) for
version 8.9.3. This is reﬂected by the large number of CVE
entries per year for that release. The 8.10.0 release had dra-
matically fewer issues (662(total) and 120(critical)) and this
is reﬂected in the drop in CVE entries per year. Note that
although for recent releases of Sendmail SCA is reporting
841 issues and 87 critical issues, this does not mean there
are 841 exploitable bugs. Rather it is an artifact that we did
not write any custom rules for Sendmail to denote defensive
code responsible for cleansing data to make it safe. There-
fore SCA must assume all data being processed by Sendmail
to be unsafe throughout its processing. Of these 841 issues,
572 are unique: multiple paths to a dangerous function call
are each ﬂagged separately. Of these 572 unique issues many
are issues that might lead to denial of service rather than
system compromise, for example 174 potential memory leaks
are identiﬁed.
Over the releases we analyzed substantial amount of ad-
ditional functionality was added to Sendmail. The 8.7.6 re-
lease (September 17 1996) had 11,861 executable lines of
code3. This increased to 15,099 for the 8.9.3 (February 5,
1999). In the next three releases (8.10.0, (March 6, 2000),
8.11.0 (July 19, 2000), 8.11.6 (August 20, 2001) ) the lines of
code count dropped to under 11,000 with signiﬁcant drops
in total number of issues, issue densities and CVE entries
per year. This is possibly indicative of a “clean-up” by the
developers. Over the remaining releases the number of lines
of code increased to just over 32,000 in 8.14.5 (September 15,
2011). With the largest increase coming between 8.12.6 and
8.13.0: 16195 to 31668. This was marked by an increase in
the total number of issues, critical issues and critical+high
issues, but a drop in densities. This may indicate signiﬁcant
eﬀort in improving code quality.
Release 8.13.0 has 0 CVE entries per year. This is because
there were no CVE entries for 2004 and 2005 and then ﬁve
in 2006 (8.13.5) (see appendix A table 4). This pattern
of CVE entries is hard to explain. Possibly it is a normal
statistical variation and an artifact of there being relatively
few undiscovered bugs in the software, or possibly it is due
to delayed reporting. The most recent release of Sendmail
that we analyzed, 8.14.5, has 32,270 lines of executable code.
The drops in issue density and low number of CVE entries
since 2004 (8.13.0) (see table 4), suggests Sendmail has ma-
tured with signiﬁcant attention being paid to code quality.
3.2 Postﬁx
Figure 1: Sendmail issues & CVE entries
Figure 2: Postﬁx issues & CVE entries
very little change. For example for Sendmail we analyzed
8.14.0 which was released on 1st February 2007, the next
release analyzed was 8.14.5 which was released September
15th 2011, this has just 124 more lines of code and 3 fewer
issues detected by SCA. If the CVE entry did not men-
tioned a release which we analyzed, then to determine if we
should count it against one of our analyzed releases we had
to use our judgment and other sources of publicly available
information such as the SecurityFocus database [24] and the
National Vulnerability Database [14]. Thus CVE-2009-4565
begins with “Sendmail before 8.14.4 does not properly han-
dle a ’\0’ character in a Common Name (CN) ﬁeld of an
X.509 certiﬁcate...”. Although release 8.14.4 is not included
in our analysis, we interpret the above to mean the issue
also applies to release 8.14.0 which is included, so CVE-
2009-4565 is included in the calculation of CVE per year for
release 8.14.0.
It will be apparent from the above that our process for
classiﬁcation of CVE entries was manual, relying on our
judgment of the contents of the entry supplemented by fur-
ther manual searches of publicly available sources, so some
errors are possible.
3.1 Sendmail
Sendmail was originally developed by Eric Allman in the
late 1970s and early 1980s. Being one of the earliest Internet
capable programs it was exploited in a number of incidents
including the Morris Internet Worm of 1988 [25]. Figure 1
shows our metrics for various releases of Sendmail from 1996-
2011. CVE information is available from 1999. Even in 1999
there were 7 CVE entries for releases of Sendmail prior to
Postﬁx was originally developed by Wietse Venema in the
late 1990s. Figure 2 shows our metrics for various releases of
Postﬁx from 1999 to 2010. For easy visual comparison the
values are scaled as shown in the ﬁgure so that all seven data
sets can be represented by a common vertical axis. Note that