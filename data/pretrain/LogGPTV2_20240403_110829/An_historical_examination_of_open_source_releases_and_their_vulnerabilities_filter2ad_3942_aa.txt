# An Historical Examination of Open Source Releases and Their Vulnerabilities

**Authors:**  
Nigel Edwards, Liqun Chen  
Hewlett-Packard Laboratories  
Long Down Avenue, Bristol, BS34 8QZ, UK  
@hp.com

## Abstract
This paper examines historical releases of Sendmail, Postfix, Apache httpd, and OpenSSL using static source code analysis and the entry rate in the Common Vulnerabilities and Exposures (CVE) dictionary. We use the CVE entry rate as a measure of the discovery rate of exploitable bugs. Our findings indicate a statistically significant correlation, albeit of moderate strength, between the change in the number and density of issues reported by the source code analyzer and the change in the rate of discovery of exploitable bugs for new releases. The strength of this correlation is influenced by factors such as the degree of scrutiny, i.e., the number of security analysts investigating the software. This demonstrates that static source code analysis can be used to assess risk even when constraints do not permit human review of the identified issues.

We find only a weak correlation between the absolute values measured by the source code analyzer and the rate of discovery of exploitable bugs. Therefore, it is generally unsafe to use absolute values of the number of issues or issue densities to compare different applications or software. Our results show that software quality, as measured by the number of issues, issue density, or the number of exploitable bugs, does not always improve with each new release. However, the rate of discovery of exploitable bugs typically begins to drop three to five years after the initial release.

**Categories and Subject Descriptors:**
- K.6.5 [Management of Computing and Information Systems]: Security and Protection

**General Terms:**
- Security, Measurement

**Keywords:**
- Static Analysis, Risk Analysis, Open Source Software

**Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.**

CCS'12, October 16–18, 2012, Raleigh, North Carolina, USA.  
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

## 1. Introduction
In this paper, we present an investigation into the vulnerability history of various open-source projects. We use a static source code analysis tool to examine sample releases over several years for potential security issues and compare the results with the rate at which entries appear for the software in the Common Vulnerabilities and Exposures (CVE) dictionary.

The purpose of this investigation is to understand what static code analysis can tell us about the potential future intrinsic risks of using the software. Given the size and complexity of commonly used software, manual analysis is often impractical. For example, the Linux kernel contains over 13 million lines of code, and OpenOffice contains over 9 million lines of code. There is also no guarantee that any particular piece of software has been rigorously analyzed by skilled security analysts. Therefore, automatic analysis is necessary to make a sound assessment of the risk of using any software.

In this study, we aim to estimate the number of bugs that might allow the construction of a successful exploit leading to system compromise, such as buffer overflow, cross-site scripting, and SQL injection. We refer to these as "exploitable bugs." We compare different releases of the same software and different software. Our method of comparison involves using a static source code analysis tool to measure the number of security issues it identifies. We seek to answer the following questions:

1. Does the change in the number of issues or issue densities between a previous release and a new release of the same software indicate anything?
2. What is the range of issue densities for popular open-source software?
3. Do very large differences in issue densities or the number of issues between different software tell us anything?

The CVE dictionary provides common names, "CVE Identifiers," for publicly known security vulnerabilities. Each vulnerability is assigned a unique CVE Identifier. We use the rate at which entries appear in the CVE dictionary for the software after its release date as an estimate of the number of exploitable bugs it contained on its release date. If the rate of appearance of CVE entries for the software after its release date is low, it suggests that the release had a relatively low number of exploitable bugs. Conversely, if the rate is higher, we assume there were more exploitable bugs. The validity of these assumptions is discussed further.

Using the CVE entries to estimate the number of exploitable bugs has limitations. Some software is subject to greater scrutiny by security researchers than others, and not all exploitable bugs may result in CVE entries. Assuming the degree of scrutiny is constant, we believe the CVE can indicate whether there were more or fewer security bugs in a given release.

We chose the following software for our study:
- **Sendmail**: Email server software (1996 to 2011)
- **Postfix**: Email server software (1999 to 2010)
- **Apache httpd**: Web server software (1.3 series from 1998 to 2010; 2.0 series from 2002 to 2010; 2.2 series from 2005 to 2011)
- **OpenSSL**: Toolkit for implementing SSL and TLS (0.9.6 series from 2000 to 2004; 0.9.7 series from 2002 to 2007; 0.9.8 series from 2005 to 2011; 1.0.0 series from 2009 to 2011)

These software packages are widely used to provide Internet-accessible services, making them attractive targets for attackers and security researchers. The widespread availability ensures a high degree of scrutiny. The choice of software was also driven by the availability of public release archives, enabling us to obtain older releases. Both Apache httpd and OpenSSL have multiple major release series, each evolving separately and maintained in parallel. We analyzed each series separately, taking samples approximately one year apart due to resource constraints.

The remainder of this paper is structured as follows:
- **Section 2**: Overview of static source code analysis
- **Section 3**: Results of our analysis of Sendmail, Postfix, Apache, and OpenSSL
- **Section 4**: Discussion on comparing analysis results from different software
- **Section 5**: Discussion on the degree of scrutiny using additional CVE histories of two open-source databases
- **Section 6**: Description of related work
- **Section 7**: Conclusion
- **Appendix A**: Full analysis results

## 2. Overview of Static Source Code Analysis
Static analysis of source code is the automatic examination of source code to determine particular non-functional properties of interest. The term "static" indicates that no execution is involved, in contrast to "dynamic" analysis, which typically involves some form of execution and test data set. Static analysis is used for various purposes, including type-checking, style-checking, performance optimization, and program verification.

In this paper, we focus on security analysis, which aims to detect the presence of bugs that may lead to security problems—exploitable bugs. Static source code analysis attempts to detect these bugs automatically using data flow analysis techniques. It traces the potential paths of ingested data through the program and checks function-specific rules for dangerous functions like `strcpy()` or `mysql_query()`. For example, it verifies whether the length of the target is greater than the length of the source for `strcpy()` and whether the query parameter of `mysql_query()` has been cleansed to prevent SQL injection.

Unfortunately, many static analysis problems are undecidable, as a consequence of Rice's theorem. This means that static analysis tools must use approximation techniques, which can result in the identification of many more issues than actual security bugs. Therefore, the results produced by these tools must be vetted by human auditors to determine their legitimacy and impact. Additionally, it is often possible to add program-specific rules. For example, if the software contains a data cleansing routine, a rule can be added to declare any ingested data that flows through the cleansing routine as safe, thus avoiding false positives for SQL injection or buffer overflow.

Building a set of program-specific rules requires inspecting the source code to determine how and when data is cleansed. Typically, this is done by auditors reviewing the results from the first and subsequent analyses. Each issue identified by the analyzer is considered, and the code that triggered the issue is inspected to determine if a custom rule to suppress the issue is warranted. One rule may suppress many issues.

This paper explores the value of raw results when constraints do not permit human review of all the issues. The number of issues or density of issues for two different programs cannot be taken as an absolute measure of the number of security defects. Differences might be due to the tool's better understanding of one program compared to the other. While we accept that some range of difference is expected and unlikely to be significant, we aim to understand the range of issue densities and determine if extremely large differences are significant.

## 3. The Analysis
In this section, we present and discuss the results of our analysis. Full results are given in Appendix A.

For our study, we used the HP Fortify Source Code Analyzer (SCA) version 5.10.0.0102 without any program-specific or custom rules. Other static source code analyzers include IBM Rational AppScan and Klocwork Insight. For all software, we configured the analyzer to trust the local system, making the network the primary source of untrusted data.

The analyzer classifies issues as "Critical," "High," or "Low." For each software release, we consider the following metrics generated from SCA:
- **Total number of issues (TI)**: Critical + High + Low
- **Total issue density (T-density)**: Number of critical, high, and low issues per 100 lines of executable code
- **Number of critical issues (CI)**
- **Critical issue density (C-density)**: Number of critical issues per 10,000 lines of executable code
- **Number of critical and high issues (CHI)**: Critical + High
- **Critical and high issue density (CH-density)**: Number of critical and high issues per 1,000 lines of executable code

We compared these measurements to the number of entries appearing in the CVE dictionary per year for that software release (CVE/yr). CVE entries are available from 1999 onwards, and we included entries up to the end of calendar year 2011.

Note that the units for the density metrics (T-density, C-density, and CH-Density) are different: issues per 100 lines, issues per 10,000 lines, and issues per 1,000 lines, respectively. This makes it easier to compare the full results from different software given in Appendix A.

We applied some filtering to the CVE entries. Only entries detailing problems with the software being analyzed were used. We excluded entries where references were made to the use of the software, but the bug lay elsewhere. We included entries specific to a single operating system, as it is usually not possible to determine if these are simple packaging errors or coding errors. Examples of excluded CVE entries include:
- Incorrect use of APIs by third-party software
- Bugs in third-party plug-in software or plug-ins

We make the simplifying assumption that the rate of CVE occurrence is constant over any given year and that each CVE entry corresponds to one distinct issue. Although this is the intent of the CVE editorial policy and seems to be the case for the majority of entries, we cannot guarantee that a few CVE entries do not refer to multiple bugs or that some entries may be duplicates.

To calculate the number of CVE entries per year (CVE/yr) for a release \( r_n \), we use the CVE entries that occurred from the release of \( r_n \) until the next release analyzed \( r_{n+1} \) and divide by the time interval between the release dates of \( r_n \) and \( r_{n+1} \). The time interval spans fractions of years, so we apportion CVE entries based on the fraction of the year covered by the time interval between release dates.

Let \( d_n \) denote the day of the year on which \( r_n \) was released, and let \( d_{n+1} \) denote the day of the year on which \( r_{n+1} \) was released. Then, if \( r_n \) was released in year \( y_1 \) and replaced by \( r_{n+1} \) in year \( y_2 \), the CVE/yr for \( r_n \) is given by:
\[
\text{CVE/yr} = \frac{\text{CVE}_{y_1} \times (365 - d_n) + \text{CVE}_{y_2} \times d_{n+1}}{365 - d_n + d_{n+1}}
\]

More generally, for a release interval spanning \( m \) years (\( m > 2 \)), the CVE/yr is given by:
\[
\text{CVE/yr} = \frac{\text{CVE}_{y_1} \times (365 - d_n) + \text{CVE}_{y_m} \times d_{n+1} + 365 \times \sum_{i=2}^{m-1} \text{CVE}_{y_i}}{365 - d_n + d_{n+1} + (m - 2) \times 365}
\]

If \( r_n \) and \( r_{n+1} \) were released in the same year, then the CVE/yr is given by:
\[
\text{CVE/yr} = \frac{\text{CVE}_{y_1} \times (d_{n+1} - d_n)}{d_{n+1} - d_n} = \text{CVE}_{y_1}
\]

Particularly for older CVE entries, it is not always possible to determine to which versions of the software the entry applies. Additionally, beta releases preceding the first release we analyzed do not appear in the software archives, but they still have CVE entries. By weighting the CVE count with the number of days for which the software was available in any given year, we compensate for these effects.

For example, OpenSSL 0.9.7 was released on December 31st, 2002. As shown in Table 4 in Appendix A, there are 4 CVE entries for OpenSSL 0.9.7, all applying to beta releases not in the OpenSSL source archives. The next release we analyzed was 0.9.7c, released on September 30th, 2003 (day 272). The CVE/yr for 0.9.7 is given by:
\[
\text{CVE/yr} = \frac{4 \times (365 - 364) + 7 \times 272}{365 - 364 + 272} = 6.99
\]

Thus, the 4 CVE entries for 2002 are given negligible weight compared to the 7 for 2003.

We did not have the resources to analyze all consecutive releases of all the software, so we took samples approximately 12 months apart. In some cases, the sample time is longer because the software was stable and exhibited very little change.

### 3.1 Sendmail
Sendmail was originally developed by Eric Allman in the late 1970s and early 1980s. Being one of the earliest Internet-capable programs, it was exploited in several incidents, including the Morris Internet Worm of 1988. Figure 1 shows our metrics for various releases of Sendmail from 1996 to 2011. CVE information is available from 1999. Even in 1999, there were 7 CVE entries for releases of Sendmail prior to 8.7.6, which was itself released in 1996. We excluded these 7 entries from our count of CVE entries but believe it justifies including release 8.7.6 in our analysis, as it was clearly the subject of security analysis work in 1999.

For easy visual comparison, the metrics are scaled so that all seven data sets can be represented by a common vertical axis. Full analysis details, including unscaled values, are given in Appendix A, Table 5. The earliest releases we analyzed had a fairly large number of issues reported by SCA: 2,548 (total) and 136 (critical) for version 8.9.3. This is reflected by the large number of CVE entries per year for that release. The 8.10.0 release had dramatically fewer issues (662 total and 120 critical), and this is reflected in the drop in CVE entries per year. Note that although recent releases of Sendmail report 841 issues and 87 critical issues, this does not mean there are 841 exploitable bugs. Rather, it is an artifact that we did not write any custom rules for Sendmail to denote defensive code responsible for cleansing data to make it safe. Therefore, SCA must assume all data being processed by Sendmail to be unsafe throughout its processing. Of these 841 issues, 572 are unique, with multiple paths to a dangerous function call each flagged separately. Many of these 572 unique issues are issues that might lead to denial of service rather than system compromise, such as 174 potential memory leaks.

Over the releases we analyzed, substantial additional functionality was added to Sendmail. The 8.7.6 release (September 17, 1996) had 11,861 executable lines of code. This increased to 15,099 for the 8.9.3 release (February 5, 1999). In the next three releases (8.10.0, March 6, 2000; 8.11.0, July 19, 2000; 8.11.6, August 20, 2001), the lines of code count dropped to under 11,000, with significant drops in the total number of issues, issue densities, and CVE entries per year. This is possibly indicative of a "clean-up" by the developers. Over the remaining releases, the number of lines of code increased to just over 32,000 in 8.14.5 (September 15, 2011), with the largest increase coming between 8.12.6 and 8.13.0 (16,195 to 31,668). This was marked by an increase in the total number of issues, critical issues, and critical+high issues, but a drop in densities. This may indicate significant effort in improving code quality.

Release 8.13.0 has 0 CVE entries per year. This is because there were no CVE entries for 2004 and 2005, and then five in 2006 (8.13.5) (see Appendix A, Table 4). This pattern of CVE entries is hard to explain. It could be a normal statistical variation and an artifact of there being relatively few undiscovered bugs in the software, or it could be due to delayed reporting. The most recent release of Sendmail that we analyzed, 8.14.5, has 32,270 lines of executable code. The drops in issue density and the low number of CVE entries since 2004 (8.13.0) (see Table 4) suggest that Sendmail has matured, with significant attention being paid to code quality.

### 3.2 Postfix
Postfix was originally developed by Wietse Venema in the late 1990s. Figure 2 shows our metrics for various releases of Postfix from 1999 to 2010. For easy visual comparison, the values are scaled so that all seven data sets can be represented by a common vertical axis. Note that the earliest releases we analyzed had a relatively small number of issues reported by SCA: 156 (total) and 3 (critical) for version 1.0.0. This is reflected by the low number of CVE entries per year for that release. The 2.0.0 release had a slight increase in issues (273 total and 10 critical), and this is reflected in the slight increase in CVE entries per year. Over the releases we analyzed, the number of lines of code increased from 15,000 in 1.0.0 to just over 20,000 in 2.7.1. The increase in the total number of issues, critical issues, and critical+high issues, along with the drop in densities, suggests that Postfix has also matured, with significant attention being paid to code quality.

## 4. Comparing Analysis Results from Different Software
In this section, we discuss the extent to which it is possible to compare analysis results from different software. The number of issues or issue densities for two different programs cannot be taken as an absolute measure of the number of security defects. Differences might be due to the tool's better understanding of one program compared to the other. While we accept that some range of difference is expected and unlikely to be significant, we aim to understand the range of issue densities and determine if extremely large differences are significant.

## 5. Degree of Scrutiny
In this section, we discuss the degree of scrutiny using additional CVE histories of two open-source databases. The degree of scrutiny, i.e., the number of security analysts investigating the software, can significantly affect the rate of discovery of exploitable bugs. We explore this effect and its implications for our analysis.

## 6. Related Work
In this section, we describe some related work in the field of static source code analysis and vulnerability assessment. We discuss how our work builds upon and extends existing research.

## 7. Conclusion
In conclusion, our study demonstrates that static source code analysis can be used to assess the risk of using open-source software, even when constraints do not permit human review of the identified issues. The change in the number and density of issues reported by the source code analyzer is indicative of the change in the rate of discovery of exploitable bugs for new releases. However, the absolute values of the number of issues or issue densities should not be used to compare different applications or software. Our results show that software quality, as measured by the number of issues, issue density, or the number of exploitable bugs, does not always improve with each new release. However, the rate of discovery of exploitable bugs typically begins to drop three to five years after the initial release.

## Appendix A: Full Analysis Results
[Full analysis results, including tables and figures, are provided here.]