# Title: An In-Depth Study of LTE: Effect of Network Protocol and Application Behavior on Performance

## Authors:
- Junxian Huang
- Feng Qian
- Yihua Guo
- Yuanyuan Zhou
- Qiang Xu
- Z. Morley Mao
- Subhabrata Sen
- Oliver Spatscheck

### Affiliations:
- University of Michigan: Junxian Huang, Yuanyuan Zhou, Qiang Xu, Yihua Guo, Z. Morley Mao
- AT&T Labs - Research: Feng Qian, Subhabrata Sen, Oliver Spatscheck

---

## Abstract
4G LTE, with its lower latency and higher bandwidth compared to 3G, has attracted many new users. However, the interactions among applications, network transport protocols, and the radio layer remain underexplored. This study investigates these interactions and their impact on performance using a combination of active and passive measurements. We observed that LTE has significantly shorter state promotion delays and lower Round-Trip Times (RTTs) than 3G. We identified inefficiencies in TCP over LTE, such as undesired slow start. We developed a novel, lightweight passive bandwidth estimation technique for LTE networks, which revealed that many TCP connections significantly underutilize available bandwidth. On average, the utilized bandwidth is less than 50% of the available bandwidth, leading to longer data downloads and additional energy overhead. The underutilization is caused by both application behavior and TCP parameter settings. Our findings highlight the need to develop more LTE-friendly transport protocol mechanisms and applications.

### Categories and Subject Descriptors
- C.2.1 [Network Architecture and Design]: Wireless Communication
- C.4 [Performance of Systems]: Measurement Techniques, Performance Attributes
- D.2.8 [Metrics]: Performance Measures

### Keywords
LTE, 4G, Bandwidth Estimation, TCP Performance, Resource Underutilization

---

## 1. Introduction
4G LTE is the latest cellular network technology providing high-speed data services for mobile devices, with advertised bandwidths matching or exceeding home broadband speeds. Recent work has demonstrated the power model of LTE, which promises higher energy efficiency and throughput compared to 3G. However, empirical studies in deployed commercial networks are limited, making it essential to understand how network resources are utilized across different protocol layers for real users. Evaluating the benefits of increased bandwidth for popular mobile applications and essential network protocols like TCP can identify limitations and areas for improvement. Network protocol overheads can be significant enough to prevent efficient usage of available network resources, especially in high-capacity but unpredictable network conditions.

Motivated by the unique backhaul and radio network technologies of LTE, we conducted an in-depth analysis of a commercial LTE network. This is, to our knowledge, the first such study in a commercial setting. We complemented our data analysis with local experiments using controlled traffic patterns to confirm or further investigate our observations. Given the prevalence of proxy deployment in cellular networks to improve user-perceived performance, we also studied the impact of middleboxes on performance, an area not previously evaluated in detail.

Our approach includes analyzing basic network characteristics, congestion control statistics, and developing a lightweight method to estimate available bandwidth using TCP Timestamps. We validated our algorithm's accuracy through controlled experiments. Besides performance overhead, network usage efficiency directly impacts the energy usage of mobile devices. We highlighted potential energy waste due to ineffective use of available network resources, particularly for video and audio applications, which are prevalent in cellular networks.

In summary, our contributions include:
- A passive method to estimate available bandwidth using TCP Timestamps.
- Techniques for passively capturing TCP flow characteristics.
- Heuristics to identify abnormal TCP behavior based on packet traces and congestion window size.

We observed that:
- For large TCP flows, queuing delay can increase RTT significantly, leading to undesired slow start upon single packet loss.
- 52.6% of downlink TCP flows experienced full or zero receive windows, limiting sending rates.
- 71.3% of large flows had a bandwidth utilization ratio below 50%, causing data transfers to take 52.9% longer and incurring additional radio energy overhead.

Based on these observations, we provide recommendations for protocol and application design to better utilize available network resources. We believe our findings are applicable to other LTE networks given the extensive coverage of our data set and independent controlled experiments.

The paper is organized as follows: §2 covers related work, §3 describes the data set and experimental setup, §4 characterizes LTE network characteristics, §5 discusses newly identified TCP performance issues, §6 investigates network resource usage efficiency, §7 explores network application behaviors, and §8 concludes the paper.

---

## 2. Related Work
We summarize three categories of work in understanding smartphones and mobile networks:

### Characterizing Mobile Network Usage and Performance
Previous efforts have used smartphone user studies to collect data from tens to hundreds of participants, investigating various aspects such as the diversity of smartphone users, popularity of mobile applications, and effectiveness of compression techniques on cellular traffic. The 3G Test study published an app to measure network performance metrics on users' handsets. Our study features a much larger user base of around 300K customers using LTE networks, whose characteristics are not well understood. Some studies have also performed large-scale measurements of mobile networks and smartphones, comparing cellular and WiFi performance, profiling diverse usage behaviors of smartphone applications, and performing network-wide measurement studies of cellular periodic transfers. Our study covers a broader spectrum, including traffic characteristics, network performance, protocol interaction, bandwidth utilization, and application usage in LTE networks.

### Cellular Resource Management and Cross-layer Interaction
In cellular networks, the Radio Resource Control (RRC) state machine manages the handset radio interface, bridging application traffic patterns and lower-layer protocol behaviors. Previous studies have examined the RRC state machine and its interaction with cellular traffic for 3G UMTS and 4G LTE networks. We study state transition delay and transport-layer idle time for hundreds of thousands of users, key factors in signaling load and energy overhead. Other studies have examined the interplay between TCP and cellular networks, such as the impact of physical and MAC layers on TCP performance and the contribution of large buffers to TCP queuing delay. Our study provides new insights into the complex interaction between LTE and TCP.

### Cellular Network Infrastructure
Previous studies have characterized 3G data network infrastructures, observing that cellular data traffic routing is restricted to a small number of gateway nodes. Other studies have unveiled cellular carriers' NAT and firewall policies and investigated IP address dynamics in 3G networks. While characterizing LTE infrastructure is not our primary focus, we do have novel findings that affect our measurement methodology and results.

---

## 3. LTE Data and Local Testbed
We provide an overview of the LTE network topology before describing our measurement data and controlled experiments.

### 3.1 The LTE Measurement Data
An LTE network consists of three subsystems: User Equipment (UE), the Radio Access Network (RAN), and the Core Network (CN). UEs are mobile handsets carried by end users. The RAN allows connectivity between a UE and the CN, consisting of multiple base stations called Evolved Node B (eNB). The CN is the backbone of the cellular network, connecting to the Internet. Our data collection point, "Monitor," is within the CN. "SGW" and "PGW" refer to the Serving Gateway and Packet Data Network Gateway, respectively. "PEP" corresponds to the Performance Enhancing Proxy, which splits end-to-end TCP connections into two, potentially improving web performance through compression and caching.

### Data Collection
Our measurement data is a large packet header trace covering a fixed set of 22 eNBs in a large metropolitan area in the U.S. Data collection started on October 12, 2012, and lasted for 240 hours. We recorded IP and transport-layer headers, along with a 64-bit timestamp for each packet. No payload data was captured except for HTTP headers. No user, protocol, or flow-based sampling was performed, and no personally identifiable information was gathered. During the 10 days, we obtained 3.8 billion packets, corresponding to 2.9 TB of LTE traffic (324 GB of packet header data, including HTTP headers). To our knowledge, this is the first large real-world LTE packet trace studied in the research community.

### Subscriber Identification
Due to privacy concerns, we did not collect any subscriber ID or phone numbers. Instead, we used private IP addresses (anonymized using a consistent hash function) as approximated subscriber IDs, as they are very stable and change only at several-hour intervals.