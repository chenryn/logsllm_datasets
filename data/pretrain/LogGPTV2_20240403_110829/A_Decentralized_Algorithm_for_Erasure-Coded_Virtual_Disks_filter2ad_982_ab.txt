decode
b'3
modify3,1
c'1
b1
b2
b3
c1
c2
Figure 4. Use of the primitives for a 3-out-of-5
erasure coding scheme. Data blocks b1 to b3
form a stripe. The encode function generates
two parity blocks c1 and c2. When b3 is up-
,c1)
dated to become b
(cid:1)
to update c1 to become c
1. Finally, we use
decode to reconstruct the stripe from b1, b2,
and c
(cid:1)
3, we call modify3,1(b3,b
(cid:1)
3
(cid:1)
1.
and the remaining n − m are parity blocks. We de-
ﬁne encode to return the original data blocks as a
matter of notational convenience.
• decode takes any m out of n blocks generated from an
invocation of encode and returns the original m data
blocks.
• modifyi, j(bi,b
,c j) re-computes the value of the j’th
parity block after the i’th data block is updated. Here,
(cid:1)
bi and b
i are the old and new values for data block i,
and c j is the old value for parity block j.
(cid:1)
i
2.2. m-quorum systems
To ensure data availability, we use a quorum system:
each read and write operation requires participation from
only a subset of U, which is called a quorum. With m-out-
of-n erasure coding, it is necessary that a read and a write
quorum intersect in at least m processes. Otherwise, a read
operation may not be able to construct the data written by a
previous write operation. An m-quorum system is a quorum
system where any two quorums intersect in m elements; we
refer to a quorum in an m-quorum system as an m-quorum.
Let f be the maximum number of faulty processes in U.
An m-quorum system is then deﬁned as follows:
Deﬁnition 1 An m-quorum system Q ⊆ 2U is a set satisfy-
ing the following properties.
∀Q1,Q2 ∈ Q : |Q1 ∩ Q2| ≥ m.
∀S ∈ 2U s.t. |S| = f ,∃Q ∈ Q : Q∩ S = /0.
The second property ensures the existence of an m-quorum
for any combination of f faulty processes. It can be shown
that f = (cid:8)(n− m)/2(cid:9) is a necessary and sufﬁcient condi-
tion for the existence of an m-quorum system (we prove this
claim in [7]). Thus, we assume that at most f = (cid:8)(n−m)/2(cid:9)
processes are faulty.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
We use a non-blocking primitive called quorum() to cap-
ture request-reply style communication with an m-quorum
of processes. The quorum(msg) primitive ensures that at
least an m-quorum receives msg, and it returns the list of
replies. From the properties of an m-quorum system deﬁned
above, we can implement quorum() in a non-blocking man-
ner on top of fair-lossy channels by simply retransmitting
messages periodically.
2.3. Timestamps
Each process provides a non-blocking operation called
newTS that returns a totally ordered timestamp. There are
two special timestamps, LowTS and HighTS, such that
for any timestamp t generated by newTS, LowTS < t <
HighTS. We assume the following minimum properties
from newTS.
UNIQUENESS: Any two invocations of newTS (possibly
by different processes) return different timestamps.
MONOTONICITY: Successive invocations of newTS by a
process produce monotonically increasing timestamps.
PROGRESS: Assume that newTS() on some process re-
turns t. If another process invokes newTS an inﬁnite
number of times, then it will eventually receive a time-
stamp larger than t.
A logical or real-time clock, combined with the issuer’s
process ID to break ties, satisﬁes these properties.
3. Correctness
For each stripe of data, the processes in U collectively
emulate the functionality of a read-write register, which
we call a storage register. As we describe below, a stor-
age register is a special type of atomic read-write register
that matches the properties and requirements of storage sys-
tems.
A storage register is a strictly linearizable [1] atomic
read-write register. Like traditional linearizability [8], strict
linearizability ensures that read and write operations exe-
cute in a total order, and that each operation logically takes
effect instantaneously at some point between its invocation
and return. Strict linearizability and traditional linearizabil-
ity differ in their treatment of partial operations. A partial
operation occurs when a process invokes a register, and then
crashes before the operation is complete. Traditional lin-
earizability allows a partial operation to take effect at any
time after the crash. That is, if a storage brick crashes while
executing a write operation, the write operation may update
the system at an arbitrary point in the future, possibly after
the brick has recovered or has been replaced. Such delayed
updates are clearly undesirable in practice—it is very com-
plicated, if not impossible, for the application-level logic
that recovers from partial writes to take future updates into
account.
Strict linearizability ensures that a partial operation ap-
pears to either take effect before the crash or not at all. The
guarantee of strict linearizability is given relative to external
observers of the system (i.e., applications that issue reads
and writes). The only way for an application to determine if
a partial write actually took effect is to issue a subsequent
read. In our algorithm, the fate of a partial write is in fact de-
cided by the next read operation on the same data: the read
rolls the write forward if there are enough blocks left over
from the write, otherwise the read rolls back the write.
We allow operations on a storage register to abort if they
are invoked concurrently. It is extremely rare that applica-
tions issue concurrent write-write or read-write operations
to the same block of data: concurrency is usually resolved
at the application level, for example by means of locking.
In fact, in analyzing several real-world I/O traces, we have
found no concurrent write-write or read-write accesses to
the same block of data [6]. An aborted operation returns
a special value (e.g., ⊥) so that the caller can distinguish
between aborted and non-aborted operations. The outcome
of an aborted operation is non-deterministic: the operation
may have taken effect as if it were a normal, non-aborted
operation, or the operation may have no effect at all, as if it
had never been invoked. Strict linearizability incorporates a
general notion of aborted operations.
In practice, it is important to limit the number of aborted
operations. Our algorithm only aborts operations if they ac-
tually conﬂict on the same stripe of data (i.e., write-write or
read-write operations), and only if the operations overlap in
time or generate timestamps that do not constitute a logical
clock. Both situations are rare in practice. First, as we have
already observed, it is extremely rare for applications to
concurrently issue conﬂicting operations to the same block
of data. Moreover, we can make stripe-level conﬂicts un-
likely by laying out data so that consecutive blocks in a log-
ical volume are mapped to different stripes. Second, mod-
ern clock-synchronization algorithms can keep clock skew
extremely small [5]. Finally, it is important to notice that
the absence of concurrency and the presence of clock syn-
chronization only affect the abort rate, not the consistency
of data.
4. Algorithm
Our algorithm implements a single storage register; we
can then independently run an instance of this algorithm
for each stripe of data in the system. The instances have
no shared state and can run in parallel.
In Section 4.1, we give describe the basic principles be-
hind the algorithm and the key challenges that the algo-
rithm solves. Section 4.2 describes the data structures used
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
by the algorithm. Section 4.3 gives the pseudo-code for
reading and writing stripes of data, and Section 4.4 gives
the pseudo-code for reading and writing individual blocks
within a stripe. In [7], we prove the algorithm correct.
4.1. Overview
Our algorithm supports four types of operations: read-
stripe and write-stripe to read and write the entire stripe,
and read-block and write-block to read and write individual
blocks within the stripe.2 A read operation returns a stripe
or block value if it executes successfully; a write operation
returns OK if it executes successfully. Both read and write
operations may abort, in which case they return the special
value ⊥.
A process that invokes a register operation becomes the
coordinatorfor that operation. Any process can be the coor-
dinator of any operation. The designation of coordinator is
relative to a single operation: consecutive operations on the
same data can be coordinated by different processes.
Each process stores a single block for each storage reg-
ister. To simplify the presentation, we assume that process
j always stores block j. That is, processes p1 . . . pm store
the data blocks, and pm+1 . . . pn store the parity blocks. It
is straightforward to adapt the algorithm to more sophis-
ticated data-layout schemes. In the following, we refer to
pm+1 . . . pn as the parity processes.
To implement a total order for operations, each process
stores a timestamp along with each block of data. The time-
stamp denotes the time when the block was last updated.
The basic principle of our algorithm is then for a write co-
ordinator to send a message to an m-quorum of processes to
store new block values with a new timestamp. A read coor-
dinator reads the blocks and timestamps from an m-quorum
and reconstructs the most recent register value.
A key complexity of the algorithm stems from the han-
dling of a partial write operation, which stores a value in
fewer than an m-quorum of replicas, either because the co-
ordinator crashes or proposes too small a timestamp. Such a
partial write causes two potential problems: inability to re-
cover the previous value, and violation of strict linearizabil-
ity.
4.1.1. Recovering from partial writes The challenge
with erasure coding is that, during a write operation, a pro-
cess cannot just overwrite its data block with the new
data value. For example, consider an erasure-coded regis-
ter with m = 5,n = 7 (the m-quorum size is 6). If a write co-
ordinator crashes after storing the new value on only 4 pro-
cesses, we have 4 blocks from the new stripe and 3 blocks
2
The single-block methods can easily be extended to access multiple
blocks, but we omit this extension to simplify the presentation.
write1(v')
read2()
v
read3()
v'
〈v',t'〉
a
b
c
〈v,t〉
〈v,t〉
〈v,t〉
〈v',t'〉
Figure 5. To ensure strict linearizability, read
operations cannot simply pick, and possibly
write-back, the value with the highest time-
stamp. In the example, the processes a, b and
c implement a storage register; for simplic-
ity, we use an erasure-coding scheme with a
stripe size of 1 and where parity blocks are
copies of the stripe block (i.e., replication as
a special case of erasure coding). The label
(cid:11)v,t(cid:12) indicates that a process stores a value
(cid:1))
v with timestamp t. The ﬁrst request write1(v
(cid:1) on only a; the second
crashes after storing v
read2 request contacts processes b and c and
returns value v. Then a recovers, and the sub-
(cid:1), even though write1
sequent read3 returns v
seems to have happened before read2 in the
eye of an observer.
from the old, which means that it is impossible to con-
struct either the old or the new stripe value.
To handle such situations, each process keeps a log of
(cid:11)block-value,timestamp(cid:12) pairs of past write requests. A
write request simply appends the new value to the log; a
read coordinator collects enough of the most recent blocks
from the logs to recover the last register value. We discuss
log trimming in Section 5.1.
4.1.2. Linearizing partial operations After
a par-
tial write, a read operation cannot simply pick the value
with the highest timestamp, since this may violate strict lin-
earizability. For example, consider the execution in Fig-
ure 5. To satisfy strict linearizability, a storage-register
implementation must ensure the following total order:
write1 → read2 → read3. In other words, read3 must re-
turn v even though it ﬁnds the value v
with a higher
timestamp. That is, we need to detect partial write oper-
ations and abort them to handle such a situation. We ac-
complish this by executing a write operation in two
phases. In the ﬁrst phase, a write operation informs an
m-quorum about the intention to write a value; in the sec-
ond phase, a write operation actually writes the value to
an m-quorum. A read operation can then detect a par-
tial write as an unfulﬁlled intention to write a value.
(cid:1)
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 10:04:32 UTC from IEEE Xplore.  Restrictions apply. 
Our approach of explicit partial-write detection has a
pleasant side effect: an efﬁcient single-round read operation
in the common case. A read operation ﬁrst checks if an m-
quorum of processes has no partial write; if so, it simply re-
turns the current register value: the value received from the
process containing the requested data, or the stripe value de-
rived from any m processes in the case of a full stripe read.
Failing the optimistic phase, the read operation reconstructs
the most recent register value and writes it back to an m-
quorum. The write-back aborts any previous partial write
operation.
4.2. Persistent data structures
Each process has persistent storage that survives crashes.
In general, the store(var) primitive atomically writes the
value of variable var to the persistent storage. When a pro-
cess recovers, it automatically recovers the most recently
stored value for each variable.
Algorithm 1 Methods for accessing the entire stripe.
1: procedure read-stripe()
val ←fast-read-stripe()
2:
if val = ⊥ then val ←recover()
3:
return val
4:
5: procedure fast-read-stripe()
6:
7:
8:
targets←Pick m random processes
replies←quorum([Read, targets])
if status in all replies is true
and val-ts in all replies is the same
and all processes in targets replied then
return decode(blocks in replies from targets)
return ⊥
else
9:
10:
11:
12: procedure write-stripe(stripe)
ts←newTS()
13:
replies←quorum([Order, ts])
14:
if status in any reply is false then return ⊥
15:
else return store-stripe(stripe, ts)
16:
The persistent state of each process consists of a time-
stamp, ord-ts, and a set of timestamp-block pairs, called the
log. The initial values for ord-ts and log are LowTS and
{[LowTS, nil]}, respectively. (Remember that, for any time-
stamp t generated by newTS, LowTS < t < HighTS.) The
log captures the history of updates to the register as seen by
an individual process. To update the timestamp information
in the log without actually storing a new value, we some-
times store a pair [ts,⊥] in the log. We deﬁne three func-
tions on the log:
• The “max-ts(log)” function returns the highest time-
• The “max-block(log)” function returns the non-⊥
• The “max-below(log, ts)” function returns the non-⊥
value in log with the highest timestamp smaller than ts.
value in log with the highest timestamp.
stamp in log.
ts←newTS()
s←read-prev-stripe(ts)
if s(cid:15)= ⊥ and store-stripe(s, ts) = OK then
17: procedure recover()
18:
19:
20:
21:
22:
23:
return s
return ⊥
else
24: procedure read-prev-stripe(ts)
25: max ←HighTS
26:
27:
28:
29:
30:
31:
repeat
replies ←quorum([Order&Read, ALL, max, ts])
if status in any reply is false then
max ←the highest timestamp in replies
blocks ←the blocks in replies with
timestamp max
until | blocks | ≥ m