We manually viewed a screenshot of each page that was success-
fully retrieved and determined whether (i) the page was rendered
correctly, (ii) the page was rendered with errors but was still us-
able, or (iii) the page was not usable. Cases where the page had
errors but was still usable were sometimes caused by CSS (Cascad-
ing Style Sheets) that varied across different regions. For example,
a page that is usable with errors might display the correct content
(with the exception of ads) but have excessive whitespace.
In order to better understand what types of websites our consen-
sus construction algorithm supports, we split up the 1,000 websites
into their Alexa categories and computed the success rate for each
category. The results are shown in Table 1. A majority of web-
sites either render correctly or are usable with errors. The highest
success rate was obtained with the World category, which contains
many region-speciﬁc websites that are unlikely to serve localized
content (e.g., chinanews.com, a large state-owned news agency).
5.3 Performance
We measure Senser’s performance by recording the time-to-last-
byte (TTLB) for each of the 1,000 Alexa websites. For each web-
site, we retrieve the top page, follow ﬁve links on the site, and
record the average TTLB of the requests. TTLB is obtained using
the Selenium WebDriver3 Firefox add-on and includes the amount
of time taken to fully load and render each page, including all re-
sources directly linked to on a page as well as any resources indi-
3http://seleniumhq.org/
345
Table 1: Consensus construction accuracy, by website category.
Category
% of tested sites
Failures
All
Uncategorized
World
Computers
Regional
Business
Shopping
Sports
Reference
Home
News
Society
Games
Recreation
Adult
Arts
Kids and Teens
Science
Health
100.0%
33.7%
32.4%
10.3%
6.2%
2.4%
2.3%
2.2%
1.5%
1.4%
1.3%
1.3%
1.0%
1.0%
0.8%
0.8%
0.8%
0.6%
0.1%
39.3%
38.3%
27.7%
56.0%
48.2%
46.9%
57.7%
30.8%
62.2%
52.9%
43.5%
45.0%
58.7%
71.4%
47.2%
39.5%
36.1%
46.7%
100.0%
Usable
with errors
12.0%
11.0%
12.2%
11.3%
11.7%
7.1%
10.8%
23.1%
8.1%
20.6%
6.5%
18.3%
23.9%
6.1%
36.1%
13.2%
13.9%
10.0%
0.0%
Correct
48.8%
52.2%
59.5%
32.3%
39.1%
45.1%
29.7%
46.2%
29.7%
23.5%
50.0%
41.7%
13.0%
22.4%
16.7%
47.4%
50.0%
43.3%
0.0%
1.0
0.8
0.6
0.4
0.2
F
D
C
0.0
100
Without Senser
Senser with optimizations
Senser without optimizations
101
102
Time to last byte (seconds)
Figure 7: Cumulative distribution of time-to-last-byte, with and without
performance optimizations.
rectly loaded with Javascript. We additionally measure the effects
of our optimizations (described in Sections 4.2 and 4.3).
Figure 7 shows the cumulative distribution of the resulting aver-
age TTLBs when loading sites with Senser, without Senser, and
with Senser when optimizations were disabled. Without optimiza-
tions the majority of websites took under 14 seconds to load, incur-
ring a 7-fold increase in the median TTLB when compared against
directly accessing a page. As shown in the Figure, Senser’s over-
head can be reduced considerably by enabling the optimizations.
With optimizations enabled, the majority of websites can be loaded
in under 8 seconds. Websites hosted in distant geographic regions
took the longest to load in all cases, as would be expected.
Microbenchmarks.
To better understand Senser’s performance
costs, we measure the average run time of each of the system’s
components on the top 50 Alexa websites.
Figure 8 shows the processes that contribute to the time it takes to
access a website with Senser due to the DNS and MHT consensus
procedures. The Figure’s key identiﬁes each process, with Other
collectively representing the time for serializing and deserializing
the MHT from the proxies to the client, and the time for choosing
the random nodes. The average total time taken is less than the sum
of the time taken by the DNS Consensus and MHT Consensus steps
because DNS lookups can often be handled by the cache.
Network communication makes up the majority of the time it
takes to load a page: DNS Consensus is the time taken to receive
DNS lookup results from the proxies, Receive MHT is the time
taken to receive MHTs from the proxies, and Fetch Page is the time
)
c
e
s
m
(
e
m
T
i
1400
1200
1000
800
600
400
200
0
(c) DNS
Consensus
(p) DNS
Lookup
(both) Other
(c) Receive MHT
(p) Construct MHT
(p) Normalize Page
(p) Fetch Page
DNS Consensus
Total time MHT Consensus
Figure 8: Average time taken by various operations. The notation (c) in-
dicates an operation happens on the client and (p) indicates an operation
happens on the proxies.
taken by the proxies to receive the destination webpage. This is not
unexpected given the diverse locations of our proxies.
5.4 Simulation Study
To better understand the effect of malicious ASes on
Senser’s ability to reach a consensus and resist censorship attempts,
we evaluate Senser under simulation using realistic network topolo-
gies. To perform the simulation we obtained AS-level graphs of the
routes between a set of 18 geographically diverse proxies and the
Alexa websites. Each graph was obtained by querying iPlane [19]
for the AS-level paths between each proxy and the Alexa websites.
The proxies were PlanetLab nodes in the following regions: Brazil,
Canada, China, Czech Republic, Finland, France, Germany, Hun-
gary, Ireland, Italy, Japan, Poland, Portugal, Russia, Slovenia, Thai-
land, United States (east coast), and United States (west coast).
For each of the Alexa websites, we randomly designate n% of
the ASes appearing in the graph as malicious ASes and choose k
proxies to use to reach the website. We say that our proxy selection
algorithm failed if at least half of the routes pass through a mali-
cious AS. (The inverse does not necessarily mean that the attempt
to visit the website would have succeeded, since malicious proxies
could have prevented a consensus from being reached.) We repeat
this process 1,000 times for each website and take the average of
their outcomes.
We divide ASes into three groups: top-tier ASes, transit ASes,
and endpoint ASes. Top-tier ASes are those that contain at least 5%
of all ASes in their customer cone according to CAIDA4. Any AS
that contains either a proxy or an Alexa website is designated as an
endpoint AS. The remaining ASes are designated transit ASes. We
vary the types of ASes that are malicious to see how an adversary’s
capabilities affect Senser’s failure rate.
Figure 9 shows Senser’s random proxy selection algorithm’s fail-
ure rate as n, the number of malicious proxies, increases. We vary
n to determine how the algorithm responds to different situations.
The failure rate is the worst when only top-tier ASes become ma-
licious, as one would expect. It takes over ﬁve malicious ASes for
the failure rate to reach 50% with the other AS sets. The transit
only line never reaches 1.0 because some paths are composed en-
tirely of endpoint and top-tier ASes. Since endpoint ASes are those
that appear as an endpoint in any of the graphs, it is possible for an
“endpoint AS” to be a transit AS in some graphs.
We compare the random proxy selection algorithm shown in Fig-
ure 9 to our AS-disjoint proxy selection algorithm. The latter AS-
aware technique performs better in most situations, but does not
4http://as-rank.caida.org/
346
1.0
0.8
0.6
0.4
e
t
a
R
e
r
u
l
i
a
F
0.2
0.0
0
top-tier only
any
any but top-tier
transit only
5
10
15
20
Malicious ASes
1.0
0.8
0.6
0.4
e
t
a
R
e
r
u
l
i
a
F
0.2
0.0
top-tier only
any
any but top-tier
transit only
1
3
5
7
9
11
13
15
17
19
Proxies used
Figure 9: Random path selection simulation (varying n).
Figure 10: Random path selection simulation (varying k).
Table 2: Failure rate comparison of randomization vs. AS-disjoint proxy
selection (varying n).
Table 3: Comparison of randomization vs. AS-disjoint proxy selection
(varying k).
ASes
1
5
10
15
20
top-tier
only
0.01 (5%)
0.02 (3%)
0.00 (0%)