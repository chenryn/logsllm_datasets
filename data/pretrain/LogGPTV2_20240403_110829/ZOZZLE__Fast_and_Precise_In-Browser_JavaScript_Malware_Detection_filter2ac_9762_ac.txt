features
were selected based
on experience and
intuition with many
pieces of malware
detected by NOZ-
ZLE and involved
collecting particu-
larly “memorable”
Feature
try : unescape
loop : spray
loop : payload
function : addbehavior
string : 0c
Figure 9: Examples of hand-picked fea-
tures used in our experiments.
Feature
Present M : B
(cid:88) 1 : 4609
function : anonymous
(cid:88) 1309 : 1
try : newactivexobject(”pdf.pdfctrl”)
(cid:88) 1211 : 1
loop : scode
(cid:88) 1 : 1111
function : $(this)
if : ”shel” + ”l.ap” + ”pl” + ”icati” + ”on” (cid:88)
997 : 1
(cid:88)
993 : 1
string : %u0c0c%u0c0c
(cid:88)
895 : 1
loop : shellcode
(cid:88)
175 : 1
function : collectgarbage()
(cid:88)
10 : 1
string : #default#userdata
1 : 6

string : %u
Figure 10: Sample of automatically selected features and their dis-
criminating power as a ratio of likelihood to appear in a malicious or
benign context.
features frequently
repeated in malware samples.
Automatically selecting features typically yields many
more features as well as some features that are biased
toward benign JavaScript code, unlike hand-picked fea-
tures that are all characteristic of malicious JavaScript
code. Examples of some of the hand-picked features
used are presented in Figure 9.
For comparison purposes, samples of the automati-
cally extracted features, including a measure of their dis-
criminating power, are shown in Figure 10. The mid-
dle column shows whether it is the presence of the fea-
ture ((cid:88)) or the absence of it () that we are matching on.
The last column shows the number of malicious (M) and
benign (B) contexts in which they appear in our training.
In addition to the feature selection methods, we also
varied the types of features used by the classiﬁer. Be-
cause each token in the Abstract Syntax Tree (AST) ex-
ists in the context of a tree, we can include varying parts
of that AST context as part of the feature. Flat features
are simply text from the JavaScript code that is matched
without any associated AST context. We should empha-
size that ﬂat features are what are typically used in var-
ious text classiﬁcation schemes. What distinguishes our
work is that, through the use of hierarchical features, we
are taking advantage of the contextual information given
by the code structure to get better precision.
Hierarchical features, either 1- or n-level, contain a
certain amount of AST context information. For exam-
ple, 1-level features record whether they appear within
a loop, function, conditional, try/catch block, etc. Intu-
itively, a variable called shellcode declared or used right
after the beginning of a function is perhaps less indica-
tive of malicious intent than a variable called shellcode
that is used with a loop, as is common in the case of a
spray. For n-level features, we record the entire stack of
AST contexts such as
(cid:104)a loop, within a conditional, within a function, . . .(cid:105)
Features Hand-Picked Automatic
99.48%
99.20%
99.01%
95.45%
98.51%
96.65%
ﬂat
1-level
n-level
Features
948
1,589
2,187
ZOZZLE
1,275,033
5
4
3.1E-6
Samples
True pos.
False pos.
FP rate
AV1
AV2
AV3
AV4
AV5
1,275,078
3
2
1.6E-6
0
5
3.9E-6
3
5
2.9E-6
1
4
3.1E-6
3
3
2.4E-6
Figure 11: Classiﬁer accuracy for hand-picked and automatically se-
lected features.
Figure 13: False positive rate comparison.
Features
Hand-Picked
Automatic
False Pos. False Neg. False Pos. False Neg.
5.84%
0.01%
0.00%
9.20%
0.02% 11.08%
4.51%
1.26%
5.14%
4.56%
1.52%
3.18%
ﬂat
1-level
n-level
Figure 12: False positives and false negatives for ﬂat and hierarchical
features using hand-picked and automatically selected features.
The depth of the AST context presents a tradeoff between
accuracy and performance, as well as between false pos-
itives and false negatives. We explore these tradeoffs in
detail in Section 5.
5 Evaluation
In this section, we evaluate the effectiveness of ZOZZLE
using the benign and malicious JavaScript samples de-
scribed in Section 4. To obtain the experimental results
presented in this section, we used an HP xw4600 work-
station (Intel Core2 Duo E8500 3.16 Ghz, dual proces-
sor, 4 Gigabytes of memory), running Windows 7 64-bit
Enterprise.
5.1 False Positives and False Negatives
Accuracy: Figure 11 shows the overall classiﬁcation ac-
curacy of ZOZZLE when evaluated using our malicious
and benign JavaScript samples1. The accuracy is mea-
sured as the number of successful classiﬁcations divided
by total number of samples. In this case, because we have
many more benign samples than malicious samples, the
overall accuracy is heavily weighted by the effectiveness
of correctly classifying benign samples.
In the ﬁgure,
the results are sub-divided ﬁrst by
whether the features are selected by hand or using the au-
tomatic technique described in Section 3, and then sub-
divided by the amount of context used in the classiﬁer
(ﬂat, 1-level, and n-level).
1Unless otherwise stated, for these results 25% of the samples
were used for classiﬁer training and the remaining ﬁles were used
for testing. Each experiment was repeated ﬁve times on a different
randomly-selected 25% of hand-sorted data.
The table shows that overall, automatic feature selec-
tion signiﬁcantly outperforms hand-picked feature selec-
tion, with an overall accuracy above 99%. Second, we
see that while some context helps the accuracy of the
hand-picked features, overall, context has little impact on
the accuracy of automatically selected features. We also
see in the fourth column the number of features that were
selected in the automatic feature selection. As expected,
the number of features selected with the n-level classiﬁer
is signiﬁcantly larger than the other approaches.
Hand-picked vs. Automatic: Figure 12 expands on the
above results by showing the false positive and false neg-
ative rates for the different feature selection methods and
levels of context. The rates are computed as a fraction of
malicious and benign samples, respectively. We see from
the ﬁgure that the false positive rate for all conﬁgurations
of the hand-picked features is relatively high (1.5-4.5%),
whereas the false positive rate for the automatically se-
lected features is nearly zero. The best case, using au-
tomatic feature selection and 1-level of context, has no
false positives in any of the randomly-selected training
and evaluation subsets. The false negative rate for all
the conﬁgurations is relatively high, ranging from 1–11%
overall. While this suggests that some malicious contexts
are not being classiﬁed correctly, for most purposes, hav-
ing high overall accuracy and low false positive rate are
the most important attributes of a malware classiﬁer.
Best classiﬁer: In contrast to the lower false positive
rates, the false negative rates of the automatically se-
lected features are higher than they are for the hand-
picked features. The insight we have is that the automatic
feature selection selects many more features, which im-
proves the sensitivity in terms of false positive rate, but
at the same time reduces the false negative effectiveness
because extra benign features can sometimes mask mali-
cious intent. We see that trend manifest itself among the
alternative amounts of context in the automatically se-
lected features. The n-level classiﬁer has more features
and a higher false negative rate than the ﬂat or 1-level
classiﬁers. Since we want to achieve a very low false
positive rate with a moderate false negative rate, and the
1-level classiﬁer provided the best false positive rate in
these experiments, in the remainder of this section, we
consider the effectiveness of the 1-level classiﬁer in more
detail.
ZOZZLE
9%
JSAND
15%
AV1
24%
AV2
28%
AV3
34%
AV4
83%
AV5
42%
Figure 14: False negative rate comparison.
5.2 Comparison with AV & Other Techniques
Previous analysis has been performed on a relatively
small set of benign ﬁles. As a result, our 1-level clas-
siﬁer does not produce any false alarms on about 8,000
benign samples, but using a set of this size limits the pre-
cision of our evaluation. To fully understand the false
positive rate of ZOZZLE, we have obtained a large collec-
tion of over 1.2 million benign JavaScript contexts taken
from manually white-listed web sites.
Investigating false positives further: Figure 13 shows
the results of running both ZOZZLE and ﬁve state-of-the-
art anti-virus products on the large benign data set. Out
of the 1.2 million ﬁles, only 4 were incorrectly marked
malicious by ZOZZLE. This is fewer than one in a quar-
ter million false alarms. The four false positives ﬂagged
by ZOZZLE fell into two distinct cases and both cases
were essentially a single large JSON-like data structure
that included many instances of encoded binary strings.
Adding a specialized JSON data recognizer to ZOZZLE
could eliminate these false alarms.
Even though anti-virus products attempt to be ex-
tremely careful about false positives, in our run, the
ﬁve anti-virus engines produced 29 alerts when applied
to 1,275,078 JavaScript samples.
Our of these, over half, 19 alerts turn out to be false
positives. We investigated these further and found sev-
eral reasons for these errors. The ﬁrst is assuming that
some document.write of an unescaped string could be ma-
licious when they in fact were not. The second reason
is ﬂagging unpackers, i.e. pieces of code that convert
a string into another one through character code trans-
lation. Clearly, these unpackers alone are not malicious.
We show examples of these mistakes in Appendix B. The
third reason is overly aggressively ﬂagging phishing sites
that insert links into the current page; this is because the
anti-virus is unable to distinguish between them and mal-
ware. The ﬁgure also shows cases where we found true
malware in the large data set (listed as true positives),
despite the fact that the web sites that the JavaScript was
taken from were white-listed. We see that ZOZZLE was
also better at ﬁnding true positives than the anti-virus de-
tectors, ﬁnding a total of ﬁve out of the 1.2 million sam-
ples. We also note that the number of samples used in the
anti-virus and ZOZZLE results in this table are slightly dif-
ferent due to the fact that on some of the samples either
the anti-virus or ZOZZLE aborts due to ill-formed Java-
Script syntax and those samples are not included in the
total.
In summary, ZOZZLE has a false positive rate
of 0.0003%, which is comparable to the ﬁve anti-virus
tools in all cases and is better than some of them.
Investigating false negatives further: Figure 14 shows
a comparison of ZOZZLE and the ﬁve anti-virus engines
discussed above. We fed the anti-virus engines the 919
hand-labeled malware samples used in the previous eval-
uation of ZOZZLE.2 Additionally, we include JSAND [6],
a recently published malware detector that has a public
web interface for malware upload and detection. In the
case of JSAND, we only used a small random sample
of 20 malicious ﬁles due to the difﬁculty of automat-
ing the upload process, apparent rate limiting, and the
latency of JSAND evaluation. The ﬁgure demonstrates
that all of the other products have a higher false negative
rate compared to ZOZZLE. JSAND is the closest, produc-
ing a false negative rate of 15%. We feel that these high
false negative rates for the anti-virus products are likely
caused by the tendency of such products to be conserva-
tive and trade low false positives for higher false nega-
tives. This experiment illustrates the difﬁculty that tradi-
tional anti-virus techniques have classifying JavaScript,
where self-generated code is commonplace. We feel that
ZOZZLE excels in both dimensions.
5.3 Classiﬁer Performance
Figure 15 shows the classiﬁcation time as a function of
the size of the ﬁle, ranging up to 10 KB. We used auto-
matic feature selection, a 1-level classiﬁer trained on .25
of the hand-sorted dataset with no hard limit on feature
counts to obtain this chart. This evaluation was per-
formed on a classiﬁer with over 4,000 features, and rep-
resents the worst case performance for classiﬁcation. We
see that for a majority of ﬁles, classiﬁcation can be per-
formed in under 4 ms. Moreover, many contexts are in
fact eval contexts, which are generally smaller than Java-
Script ﬁles downloaded from the network. In the case of
eval contexts such as that, the classiﬁcation overhead is
usually 1 ms and below.
Figure 16 displays the overhead as a function of the
number of classiﬁcation features we used and compares
it to the average parse time of .86 ms. Despite the fast
feature matching algorithm presented in Section 3, hav-
ing more features to match against is still quite costly. As