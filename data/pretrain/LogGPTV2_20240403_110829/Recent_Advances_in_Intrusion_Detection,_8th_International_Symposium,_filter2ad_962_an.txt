### Pre-request Masking
- **Masked by Proxy**: 2707; 7.67%
- **Masked by Rules**: 32438; 91.95%

### Figure 4: Analysis of Detected Differences

### Detection Reliability
In the first validation phase, we used an environment consisting of three servers:
- **BuggyHTTP** on Linux
- **Apache 1.3.29** on MacOS-X
- **IIS 5.0** on Windows

The choice of BuggyHTTP was due to its numerous vulnerabilities that can be easily exploited. We performed seven attacks targeting the system's security properties, summarized in Table 1. These attacks exploited three types of vulnerabilities:
1. **Access to files outside the web space** (attack against confidentiality)
2. **Modification of files outside the web space** (attack against integrity)
3. **Denial of service** (attack against availability)

The HTTP traffic consisted only of these attacks. Each request was processed independently, so the detection rate would remain the same even if the malicious traffic was mixed with non-attack traffic. As expected, all attacks launched against one of the COTS servers were detected by the IDS.

### Detection Accuracy
In the second validation phase, the architecture included:
- **Apache server** on MacOS-X
- **thttpd server** on Linux
- **IIS 5.0** on Windows

We avoided using the BuggyHTTP server because we did not intend to attack it and because it provides limited functionalities. The three servers contained a copy of our campus website and were configured to minimize output differences. They were fed with requests logged from our campus web server in March 2003, totaling over 800,000 requests. A previous study [19] using a very sensitive tool [20] showed that at most 1.4% of the HTTP requests could be harmful.

As shown in Figure 4, only 0.37% of the output differences generated alerts, representing 1.6 × 10^-2% of the HTTP requests. In one month, the administrator would need to analyze 150 alerts, or about 5 alerts per day. The security administrator must analyze each alert to determine its root cause and eliminate potential false positives.

### Analysis of Alerts
- **Encoded request (400/404/404)**: 20
- **Unknown (404/403/404)**: 1
- **CONNECT site (405,400,501)**: 4
- **Redirected GET (404,400,404)**: 4
- **Winsys access (400/404/200)**: 16
- **Winsys access (404/404/500)**: 16
- **Winsys access (none, none, 200)**: 30
- **Winsys access (404/404/200)**: 40

Figure 5 shows the analysis of alerts, indicating that only the first four alert types are likely false positives (22% of the alerts). However, they were not eliminated as they may indicate unsuccessful attacks. The three Winsys access request types suggest an intrusion: one server (IIS) delivered a response while the others refused, indicating successful attacks against confidentiality on IIS.

### Output Difference Masking
The comparison algorithm detects a large number of output differences. Thanks to design difference masking mechanisms, 99.63% of these differences are masked (see Figure 4). For example, a rule masks differences for an HTTP request on a directory without appending a final '/' (thttpd and IIS respond with a 302 status code, while Apache responds with a 301 status code). At the proxy level, we transform all resource names to lowercase (Windows file systems are case-insensitive, while Linux and MacOS-X are case-sensitive).

Currently, we have 36 rules defined, and more will be added. Even with additional rules, the base is expected to remain manageable (5 rules mask 90% of design differences). This suggests that the rule definition work is not overly burdensome compared to building a complete behavior model for classical anomaly detectors, making this mechanism viable in real environments.

### Comparison with Snort and WebStat
It is challenging to compare the outputs of the three IDSes, as they detect different attacks and produce different false positives. To roughly compare our approach with well-known tools, we used the same dataset (800,000 requests from March 2003) and configured WebStat [21] and Snort [22] with their standard signature sets.

Figure 6 shows the results:
- **Snort and WebStat**: About 10 alerts per day
- **Our tool**: About 5 alerts per day

This can be explained by the fact that our IDS detects intrusions, while Snort and WebStat detect attack attempts without indicating success or failure. Thus, our approach produces fewer alerts without missing known attacks. However, this conclusion should be taken cautiously, as Snort and WebStat were not fully tuned for the experiment.

### Detection Time and Time Overheads
Evaluating the time overheads induced by the detection process is important. All measurements are from the client's perspective (time between a request and the reception of its answer).

| Server | IDS Inactive (s) | IDS Active (s) | IDS Overhead (s) |
|--------|------------------|----------------|------------------|
| thttpd | 0.0214           | 0.1042         | 0.0828           |
| Apache | 0.0173           | 0.1083         | 0.0910           |
| IIS    | 0.1256           | 0.1073         | 0.0817           |

Activating the proxy and IDS multiplies the request processing time by about 6. The communications and detection algorithm contribute to the measured durations. The prototype IDS induces an acceptable overhead (about 0.1s), suitable for real-time use.

### Discussion
- **Availability Attacks**: Successful attacks against the availability of one of the COTS servers are detected.
- **Confidentiality Attacks**: Successful attacks against confidentiality are detected.
- **Integrity Attacks**: Some attacks against the integrity of one of the COTS servers may not be detected, as the responses may be equivalent according to the detection algorithm.

Future work will address dynamic aspects and the identification of compromised servers. Additionally, the method requires ongoing effort from administrators to maintain and update the rule base, and there is no guarantee that the rules do not introduce false negatives.

### Conclusion and Future Work
This approach provides high detection coverage and a low level of false positives. However, applying the method to COTS implies detecting a high amount of output differences not due to vulnerability exploitation. We plan to characterize detected differences online to avoid explicit rule definitions, leading to the development of diagnosis functions to identify server failures.

### Acknowledgements
This work was partly supported by the Conseil Régional de Bretagne and is part of the French Ministry of Research (CNRS ACI-SI) DADDi project.

### References
[References listed here]

### Appendix: Description of Attacks Against BuggyHTTP
1. **URL Verification Bypass**: BuggyHTTP does not verify the URL, allowing access to files outside the site. A request like "GET /../../../../../../etc/shadow HTTP/1.0" accessed the /etc/shadow file. BuggyHTTP responded with a 200 status code, while Apache and IIS responded with 404 and 400 status codes, respectively.
2. **Folder Traversal for File Modification**: Similar vulnerability allowed modifying files on the system. A request like "GET /cgi-bin/../../bin/sh -c 'echo root::12492::::: > /etc/shadow'" modified the /etc/shadow file. BuggyHTTP accepted the request, while Apache and IIS responded with 400 status codes.
3. **Buffer Overflow Vulnerability**: Modifying BuggyHTTP to use a "select" approach instead of "fork" for network connections allowed exploiting a buffer overflow to crash the server. BuggyHTTP did not respond to the request, while Apache and IIS responded with 400 and 414 status codes, respectively.