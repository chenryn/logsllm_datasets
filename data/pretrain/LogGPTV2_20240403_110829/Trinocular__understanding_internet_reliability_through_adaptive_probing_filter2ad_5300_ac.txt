no longer match the current network. We update our target
list and A-estimations every two months as new long-term
data is made available. At shorter-timescales, we must han-
dle or adapt when parameter estimates diverge from reality.
Underestimating E(b) misses an opportunity to spread
traﬃc over more addresses. Underestimating A(E(b)) gives
each probe less weight. In both cases, these errors have a
slight aﬀect on performance, but none on correctness.
When E(b) is too large because it includes non-responsive
addresses, it is equivalent to overestimating A(E(b)). When
A(E(b)) exceeds the actual A, negative probes are given
too much weight and we infer outages incorrectly. Ideally
A(E(b)) will evolve as a side-eﬀect of probing to avoid false
outages when it diverges from the long-term average. Our
current system does not track A dynamically (although work
is underway), so we detect divergence in post-processing,
and identify and discard inaccurate blocks. The result is
greater traﬃc, but few false outages.
Traﬃc: We do not count long-term observations against
Trinocular’s traﬃc budget since it is an ongoing eﬀort, inde-
pendent of our outage detection. However, even if we take
responsibility for all traﬃc needed to build the history we
use, it adds only 0.18 probes per hour per /24 block since
collection is spread over 2 months.
4.5 Outage Scope From Multiple Locations
A single site provides only one view of the Internet, and
prior work has shown that about two-thirds of outages are
partial [17]. We use two approaches to judge outage scope:
we detect and eliminate outages where probers are eﬀectively
oﬀ the network, and we merge views from multiple observers
to distinguish between partial and global outages. In §7.1
we report on how frequently these occur in the Internet.
Prober-local outages: Router failures immediately up-
stream of a prober unenlighteningly suggest that nearly the
entire Internet is out. We detect and account for outages
that aﬀect more than half the probed blocks.
Partial and global outages: We detect outage scope by
merging observations from multiple vantage points. Because
each site operates independently and observations tend to
occur at multiples of a round, direct comparison of results
from diﬀerent sites will show diﬀerent timing by up to one
round. We correct these diﬀerences by taking the earlier of
two changes that occur, since periodic probes always delay
detection of a change in block status. We therefore cor-
rect disagreements in the merged results only when (a) both
sites agree before and after the disagreement, (b) the dis-
agreement lasts less than 1.1 rounds, and (c) the network
changes state before and after disagreement. Rules (a) and
(b) detect transient disagreement that is likely caused by
phase diﬀerences. Rule (c) avoids incorrectly changing very
short outages local to one vantage point. Merging results
thus improves precision. After correction, any remaining
disagreement represents partial outages.
4.6 Operational Issues
Our system implementation considers operational issues
to insure it cannot harm the Internet.
Probing rate: In addition to per-block limits, we rate
limit all outgoing probes to 20k probes/s using a simple
token bucket. Rate limiting at the prober insures that we
do not overwhelm our ﬁrst-hop router, and it provides a
fail-safe mechanism so that, even if all else goes wrong, our
prober cannot ﬂood the Internet incessantly. In practice, we
have never reached this limit.
(This limit is at the prober,
spread across all targets. Figure 4 shows that only a tiny
fraction of this traﬃc is seen at each target block.)
We expect our monitor to run indeﬁnitely, so we have
implemented a simple checkpoint/restart system that saves
current belief about the network. This mechanism accom-
modates service on the probing machine. We restart our
probers every 5.5 h as a simple form of garbage collection.
We have run Trinocular for several multi-day periods, and
we expect to run Trinocular continuously when adaptive
computation of A is added.
Implementation: We use a high-performance ICMP prob-
ing engine that can handle thousands of concurrent probes.
We use memory-optimized data structures to keep state for
each block, leaving CPU cost to match probe replies with
the relevant block as the primary bottleneck. We ﬁnd a sin-
gle prober can sustain 19k probes/s on one core of our 4-core
Opteron. Fortunately, probing parallelizes easily, and with
four concurrent probers, a single modest computer can track
all outages on the analyzable IPv4 Internet.
5. VALIDATING OUR APPROACH
We validate correctness with controlled experiments, and
probe rate by simulation and Internet experiments.
5.1 Correctness of Outage Detection
We ﬁrst explore the correctness of our approach: if an out-
age occurs, do we always see it? For a controlled evaluation
of this question, we run Trinocular and probe 4 /24 blocks
at our university from 3 sites: our site in Los Angeles, and
universities 1600 km and 8800 km distant in Colorado and
Japan. We control these blocks and conﬁgure them in two-
hour cycle where the network is up for 30 minutes, goes down
at some random time in the next 20 minutes, stays down for
a random duration between 0 and 40 minutes, then comes
back up. This cycle guarantees Trinocular will reset between
controlled outages. We studied these blocks for 122 cycles,
yielding 488 observations as dataset Acontrolled , combining
data for 4 controlled blocks from datasets A1w (2013-01-19,
4 days), A3w (2013-01-24, 1 day), A4w (2013-01-25, 2 days),
A7w (2013-02-12, 2 days)2.
Figure 2 shows these experiments, with colored areas show-
ing observed outage duration rounded to integer numbers of
rounds. We group true outage duration on the x into rounds
with dotted black lines. Since periodic probing guarantees
we test each network every round, we expect to ﬁnd all out-
ages that last at least one round or longer. We also see that
we miss outages shorter than a round roughly in proportion
to outage duration (the white region of durations less than
11 minutes). While these experiments are speciﬁc to blocks
where addresses always respond (A(E(b)) = 1), they gen-
eralize to blocks with A ≥ 0.3 since we later show that we
2 We name datasets like A7w for Trinocular scans of the
analyzable Internet (A20addr uses a variant methodology),
H 49w for Internet histories [11], S 50j
for Internet Sur-
veys [13]. The subscript includes a sequence number and
code for site (w: Los Angeles, c: Colorado, j: Japan).
Figure 2: Fraction of detected outages (bar height) and
duration in rounds (color),
for controlled experiments.
Dataset: Acontrolled .
Figure 3: Observed outage duration vs. ground truth.
Dataset: Acontrolled (same as Figure 2).
take enough probes to reach a deﬁnitive conclusion for these
blocks (Figure 1).
These results conﬁrm what we expect based on our sam-
pling schedule: if we probe a block with A ≥ 0.3, we always
detect outages longer than one round.
5.2 Precision of event timing
Figure 2 shows we do detect outages. We next evaluate
the precision of our observed outage durations.
We continue with dataset Acontrolled in Figure 3, com-
paring ground truth outage duration against observed out-
age duration at second-level precision. Our system measures
block transition events with second-level precision, but when
we examine outage durations, we see they group into hor-
izontal bands around multiples of the round duration, not
the diagonal line that represents perfect measurement. We
also see that error in each case is uniformly distributed with
error plus or minus one-half round. As expected, we miss
some outages that are shorter than a round; we show these
as red circles at duration 0. Finally, we also see a few ob-
servations outside bands, both here and marked with an as-
terisk in Figure 2. These are cases where checkpoint/restart
stretched the time between two periodic probes.
These results are consistent with measurement at a ﬁxed
probing interval sampling a random process with a uniform
timing. When we compare observed and real event start-
and end-times it conﬁrms this result, with each transition
late with a uniform distribution between 0 and 1 round.
These experiments use blocks where addresses are always
responsive (A(E(b)) = 1). We carried out experiments vary-
ing A from 0.125 to 1 and can conﬁrm that we see no missed
 0 20 40 60 80 100123456789101112131415161718192021222324252627282930313233343536outages found (%)duration of controlled outage (minutes)missedobserved as 1 roundobserved as 2 roundsobserved as 3 rounds*** 0 500 1000 1500 2000 2500actual outage duration (s) 0 500 1000 1500 2000 2500estimated outage duration (s) 0 2 4 6 8 10 12 14 16 18(perfect prediction)outages longer than one round and similar precision as long
as Trinocular can reach a conclusion (A > 0.3). When
0.3  0.3. Greater precision is
possible by reducing the round duration, given more traﬃc.
5.3 Probing rate
Our goal is good precision with low traﬃc, so we next
validate traﬃc rate. We use simulation to explore the range
of expected probing rates, then conﬁrm these based on our
Internet observations.
Parameter Exploration: We ﬁrst use simulation to ex-
plore how many probes are needed to detect a state change,
measuring the number of probes needed to reach conclusive
belief in the new state. Our simulation models a complete
block (|E(b)| = 256) that transitions from up-to-down or
down-to-up. When up, all addresses respond with proba-
bility A(E(b)). When down, we assume a single address
continues to reply positively (the worst case outage for de-
tection).
Figure 1 shows the up-to-down and down-to-up costs.
Down-to-up transitions have high variance and therefore
have boxes that show quartiles and whiskers 5%ile and 95%ile
values. Up-to-down transitions typically require several probes
because Trinocular must conﬁrm a negative response is not
an empty address or packet loss, but they have no variance
in these simulations. Trinocular reaches a deﬁnitive belief
and a correct result in 15 probes for all blocks with A > 0.3.
For down-to-up transitions, 15 probes are suﬃcient to re-
solve all blocks in 50% of transitions when A > 0.15, and in
95% of transitions when A > 0.3. Variance is high because,
when A is small, one will probe many unused addresses be-
fore ﬁnding an active one. This variance motivates recovery
probing (the black “still down” line).
Experimentation: To validate these simulations, Fig-
ure 4 shows probe rates from A7w , a 48-hour run of Trinoc-
ular on 3.4M Internet-wide, analyzable blocks starting 2013-
02-12 T14:25 UTC. Here we examine the subset A7w-5.5h
from this data: the ﬁrst 5.5 hours (30 rounds) from one of
the four probers, with 1M blocks; other subsets are similar.
As one would expect, in most rounds, most blocks ﬁn-
ish with just a few probes: about 73% use 4 or fewer per
round. This distribution is skewed, with a median of 13.2
probes/hour, but a mean of 19.2 probes/hour, because a few
blocks (around 0.18%) reach our probing limit per round.
Finally, we report that 0.15% of blocks actually show more
than expected traﬃc (the rightmost peak on the graph).
We ﬁnd that a small number of networks generate multiple
replies in response to a single probe, either due to probing a
broadcast address or a misconﬁguration. We plan to detect
and blacklist these blocks.
This experiment shows we meet our goals of generating
only minimal traﬃc, with probing at 0.4% (median) to 0.7%
(mean) of background radiation, and bounding traﬃc to
each block.
Probe rate as a function of A(E(b)): The above ex-
periment shows most blocks require few probes, and our
Figure 4: Distribution of probes to each target block.
Dataset: A7w-5.5h .
Figure 5: Median number of probes, with quartiles, for ag-
gregate and state transitions. Dataset: A7w-5.5h .
simulations show probe rate at transition depends strongly
on address responsiveness. To verify this relationship, Fig-
ure 5 breaks down probes required by transition type and
each block’s A(E(b)).
The dotted line and thick quartile bars show aggregate
performance across all states. We track blocks with A >
0.3 with less than 4 probes per round, with relatively low
variance.
Intermittent blocks (A = 85 p/h 0 2 4 6 8 10 12 14 16 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1probes requiredavailability A(E(b))(maximum allowed probes)down-to-upup-to-downaggregatestill upstill downstrategy
samples per /24
which addresses
precision
recall
single hitlist Trinoc.
|E(b)|
top ever resp.
1
.1
99.97% 99.98%
58.6% 66.6%
1
all
256
all
100% (100%)
96.6% (100%)
Table 2: Comparing precision and recall of diﬀerent probing
targets. Dataset: S 50j .
Eﬀect on Outage Detection: To evaluate the impact of
probing choice on outages, we next examine a single dataset
with three choices of probe target. We use Internet survey
“it50j” [13], a 2-week ICMP probing of all addresses in 40k
/24 blocks starting 2012-10-27 (here called S 50j ). We deﬁne
any response in probes to all addresses as ground truth since
it is the most complete. We deﬁne a false outage (fo) as a
prediction of down when it’s really up in all-probing, with
analogous deﬁnitions of false availability (fa), true availabil-
ity (ta), and true outages (to). We then compute precision
(ta/(ta + fa)), and recall (ta/(ta + fo)).
Table 2 compares these design choices. Here we focus
on the eﬀect of number of targets on precision and recall.
While precision is uniformly good (inference of “up” is nearly
always correct), recall suﬀers because there are many false
outages. We conclude that probing one target (single and
hitlist cases) has lower recall. The problem is that in some