title:Secure Multi-party Computation of Differentially Private Median
author:Jonas B&quot;ohler and
Florian Kerschbaum
Secure Multi-party Computation of 
Differentially Private Median
Jonas Böhler, SAP Security Research; Florian Kerschbaum, 
University of Waterloo
https://www.usenix.org/conference/usenixsecurity20/presentation/boehler
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Secure Multi-party Computation of Differentially Private Median
Jonas Böhler
SAP Security Research
Florian Kerschbaum
University of Waterloo
Abstract
In this work, we consider distributed private learning. For this
purpose, companies collect statistics about telemetry, usage
and frequent settings from their users without disclosing indi-
vidual values. We focus on rank-based statistics, speciﬁcally,
the median which is more robust to outliers than the mean.
Local differential privacy, where each user shares locally
perturbed data with an untrusted server, is often used in pri-
vate learning but does not provide the same accuracy as the
central model, where noise is applied only once by a trusted
server. Existing solutions to compute the differentially pri-
vate median provide good accuracy only for large amounts
of users (local model), by using a trusted third party (central
model), or for a very small data universe (secure multi-party
computation).
We present a multi-party computation to efﬁciently com-
pute the exponential mechanism for the median, which
also supports, e.g., general rank-based statistics (e.g., pth-
percentile, interquartile range) and convex optimizations for
machine learning. Our approach is efﬁcient (practical run-
ning time), scaleable (sublinear in the data universe size) and
accurate, i.e., the absolute error is smaller than comparable
methods and is independent of the number of users, hence,
our protocols can be used even for a small number of users.
In our experiments we were able to compute the differentially
private median for 1 million users in 3 minutes using 3 semi-
honest computation parties distributed over the Internet.
1 Introduction
We consider the problem of distributed private learning.
Speciﬁcally, how multiple users can compute rank-based
statistics over their sensitive data, with high accuracy, a strong
privacy guarantee, and without resorting to trusted third par-
ties. Rank-based statistics include the median, pth-percentiles,
and interquartile ranges, and we present a protocol to compute
the differentially private median, which is extensible to any
kth ranked element. We use differential privacy (DP) [25, 28],
a rigorous privacy notion, restricting what can be inferred
about any individual in the data, used by Google [15, 31],
Apple [1, 66], Microsoft [23] and the US Census bureau [2].
The median is a robust statistical method used to represent a
“typical” value from a data set, e.g., insurance companies use
the median life expectancy to adjust insurance premiums.
Previous work on DP median computation either require
a large number of users to be accurate [27, 34, 63], rely on a
trusted third party [51, 58], or cannot scale to large universe
or data set sizes [14, 30, 59]. We present a novel alternative
that is superior in accuracy, requires no trusted party, and is
efﬁciently computable. Our protocol provides high accuracy
even for a small number of users. Note that small sample size
is the most challenging regime for DP [56]. Even Google’s
large-scale data collection (billions of daily reports via [31])
is insufﬁcient if the statistical value of interest is not a heavy
hitter [15], e.g., the median.
We present a secure multi-party computation (MPC) of
the exponential mechanism [52] for decomposable aggregate
functions. Such functions, as used in MapReduce-style al-
gorithms [22], allow efﬁcient aggregation in parallel over
distributed data sets, and application examples include con-
vex loss functions and rank-based statistics. The exponential
mechanism can implement any differentially private algo-
rithm by computing selection probabilities for all possible
output elements. Its computation complexity is linear in the
size of the data universe [52] and efﬁciently sampling it is
non-trivial [29]. Also, the exponential mechanism requires
exponentiations, increasing the MPC complexity. However,
as it is a universal mechanism, a scalable, secure implementa-
tion can be widely applied. Eigner et al. [30] also implement
the exponential mechanism in MPC. They compute the expo-
nential function with MPC, whereas we provide a more efﬁ-
cient alternative for decomposable functions. Their approach,
while more general, is only practical for a universe size of
5 elements, whereas our protocol is sublinear in the size of
the universe and handles billions of elements. We achieve
this via divide-and-conquer and optimizing our protocol for
decomposable functions that enable efﬁcient alternatives to
USENIX Association
29th USENIX Security Symposium    2147
expensive secure computation of exponentiations [5,7,20,43].
In summary, our contribution is a protocol for securely
computing the differentially private median
• with high accuracy even for small data sets (few users)
and large universe sizes (see Section 3.4 for our theo-
retical errors bounds, Appendix F for a comparison of
that bound to related work, and Section 5.3 for empirical
comparison to related work),
• that is efﬁcient (practical running time for millions of
users) and scalable (sublinear in the data universe size)
(Sections 4, 5),
• secure in the semi-honest model with an extension to
the malicious model (Section 4.6) and outputs the dif-
ferentially private median according to the exponential
mechanism by McSherry and Talwar [52],
• evaluated using an implementation in the SCALE-
MAMBA framework [6], for 1 million users using 3
semi-honest computation parties with a running time of
seconds in a LAN, and 3 minutes in a WAN with 100 ms
network delay, 100 Mbits/s bandwidth (Section 5).
The remainder of this paper is organized as follows: In
Section 2 we describe preliminaries for our protocol. In Sec-
tion 3 we explain our protocol and introduce deﬁnitions. We
present our protocol and implementation details for the secure
multi-party computation of the differentially private median
in Section 4. We provide a detailed performance evaluation
in Section 5, describe related work in Section 6 and conclude
in Section 7.
2 Preliminaries
In the following, we introduce preliminaries for differential
privacy and secure multi-party computation.
We consider a set of input parties P = {P1, . . . ,Pn}, where
party Pi holds a datum di, and D denotes their combined data
set. We model a data set as D = {d1, . . . ,dn} ∈ U n with un-
derlying data universe U. We also consider m semi-honest
computation parties, e.g., m ∈ {3,6,10}, who run the compu-
tation on behalf of the input parties. To simplify presentation,
we assume the size n of D to be even, which can be ensured
by padding. Then, the median’s position in sorted D is n/2.
2.1 Differential Privacy
Differential privacy (DP), introduced by Dwork et al. [25,28],
is a strong privacy guarantee restricting what a mechanism
operating on a sensitive data set can output. Informally, when
the input data set changes in a single element, the effect on
the output is bounded. The formal deﬁnition is as follows:
Deﬁnition 1 (Differential Privacy). A mechanism M satisﬁes
ε-differential privacy, where ε ≥ 0, if for all neighboring data
sets D (cid:39) D(cid:48), i.e., data sets differing in a single entry, and all
sets S ⊆ Range(M )
Pr[M (D) ∈ S] ≤ exp(ε)· Pr(cid:2)M (D(cid:48)) ∈ S(cid:3),
where Range(M ) denotes the set of all possible outputs of
mechanism M .
The above deﬁnition holds against an unbounded adver-
sary, however, due to our use of cryptography we assume a
computationally bounded adversary. A formal deﬁnition is
presented in Appendix A based on MPC preliminaries from
Section 2.2.
Randomization is essential for differential privacy to hide
an individual’s inclusion in the data [29]. Noise, added to the
function output, is one way to achieve differential privacy,
e.g., via the Laplace mechanism [29]:
Deﬁnition 2 (Laplace Mechanism). Given a function f :
U n → R with sensitivity max∀D(cid:39)D(cid:48) | f (D)− f (D(cid:48))|, privacy
parameter ε, and a database D, the Laplace mechanism re-
leases f (D) + r, where r is drawn from the Laplace distribu-
tion (centered at 0) with density ε
−ε
∆ f .
2∆ f e
The alternative to additive noise is probabilistic output
selection via the exponential mechanism, introduced by Mc-
Sherry and Talwar [52]. The exponential mechanism expands
the application of differential privacy to functions with non-
numerical output, or when the output is not robust to additive
noise, e.g., the median function [48]. The mechanism is expo-
nentially more likely to select “good” results where “good” is
quantiﬁed via a utility function u(D,r) which takes as input a
database D ∈ U n, and a potential output r ∈ R from a ﬁxed
set of arbitrary outputs R . Informally, higher utility means
the output is more desirable and its selection probability is
increased accordingly.
Deﬁnition 3 (Exponential Mechanism). For any utility func-
tion u : (U n × R ) → R and a privacy parameter ε, the expo-
u(D) outputs r ∈ R with probability
nential mechanism EMε
εu(D,r)
proportional to exp(
2∆u ), where
(cid:12)(cid:12)u(D,r)− u(cid:0)D(cid:48),r(cid:1)(cid:12)(cid:12)
(cid:17)
(cid:16) εu(D,r)
(cid:16) εu(D,r(cid:48))
exp
2∆u
∑r(cid:48)∈R exp
2∆u
(cid:17) .
(1)
is the sensitivity of the utility function. That is,
∆u = max
∀r∈R ,D(cid:39)D(cid:48)
Pr[EMε
u(D) = r] =
We omit u,ε,D, i.e., write EM, if they can be derived from
the context.
DP algorithms M can be implemented in different models,
visualized in Figure 1. Next, we describe the models and
explain which model we implement.
2148    29th USENIX Security Symposium
USENIX Association
C1...
Cn
C1...
Cn
d1
dn
Trusted
Server
M ( f (d1, . . . ,dn))
(a) Central Model
r1 =M (d1)
rn =M (dn)
Untrusted
Server
f (r1, . . . ,rn)
(b) Local Model
r1 =M (d1)
rn =M (dn)
Shufﬂer
rπ(1)
...
rπ(n)
Untrusted
Server
C1...
Cn
f(cid:0)rπ(1), . . . ,rπ(n)
(cid:1)
(c) Shufﬂe Model with permutation π
Figure 1: Models for DP mechanism M . Client Ci sends a
message (raw data di or randomized ri) to a server, who com-
putes function f over the messages, and releases the result.
2.1.1 Why We Consider the Central Model
In the central model (Figure 1a) every client sends their un-
protected data to a trusted, central server which runs M on the
clear data. The central model provides the highest accuracy
as the randomization inherent to DP algorithms, is only ap-
plied once. In the local model (Figure 1b), introduced by [44],
clients apply M locally and sent anonymized values to an un-
trusted server for aggregation. The accuracy is limited as the
randomization is applied multiple times. Hence, it requires a
very large number of users to achieve accuracy comparable to
the central model [15,18,40,44,50]. Speciﬁcally, an exponen-
tial separation between local and central model for accuracy
and sample complexity was shown by [44]. Recently, an in-
termediate shufﬂe model (Figure 1c) was introduced [15, 18]:
A trusted party is added between client and server in the lo-
cal model, the shufﬂer, who does not collude with anyone.
The shufﬂer permutes and forwards the randomized client
values. The permutation breaks the mapping between a client
and her value, which reduces randomization requirements.
The accuracy of the shufﬂe model lies between the local
and central model, however, in general it is strictly weaker
than the central model [9, 18]. As our goal is high accuracy
without trusted parties even for small number of users, we
simulate the central model in a distributed setting via secure
multi-party computation (MPC), which is often used in DP
literature [26, 30, 38, 59, 60, 65]. MPC, further described in
Section 2.2, is a cryptographic protocol run by clients over
their sensitive data that only reveals the computation output
without requiring a trusted server. General MPC incurs high
computation and communication overhead which reduce ef-
ﬁciency and scalability [18]. However, MPC combines the
(a) Credit card transactions [67],
ﬁrst 105 payment records in Cents.
(b) Walmart supply chain data [42],
175k shipment weights as integers.
Figure 2: Absolute errors, averaged for 100 differentially
private median computations via Laplace mechanism with
smooth sensitivity, this work, and the exponential mechanism.
respective beneﬁts of the models, namely, high accuracy and
strong privacy, i.e., no disclosure of values to a third party,
and we present an efﬁcient and scaleable MPC protocol.
2.1.2 Why We Use the Exponential Mechanism
Next, we illustrate why the exponential mechanism offers bet-
ter accuracy than additive noise w.r.t. the DP median. Recall,
the noise depends on the sensitivity of function f and the
privacy parameter ε. The sensitivity is the largest difference
a single change in any possible database can have on the
function result. Smooth sensitivity, developed by Nissim et
al. [58], additionally analyzes the data to provide instance-
speciﬁc additive noise that is often much smaller. (See Ap-
pendix F for a formal description.) However, computation
of smooth sensitivity requires access to the entire data set,
otherwise the error increases further1, which prohibits efﬁ-
cient (secure) computation with high accuracy. Li et al. [48]
note that the Laplace mechanism is ineffective for the me-
dian as (smooth) sensitivity can be high. Additionally, they
present a median utility function for the exponential mecha-
nism with low, data-independent sensitivity, which we use in
our protocol. To illustrate that additive noise can be high, we
empirically evaluated the absolute error of the Laplace mecha-
nism with smooth sensitivity, the exponential mechanism, and
our protocol in Figure 2 on real-world data sets [42, 67]. Our
protocol uses the exponential mechanism in multiple steps,
and while the accuracy is not the same as for (single use
of) the exponential mechanism, we do not require a trusted
third party. Overall, we achieve better accuracy than additive
noise for low ε (corresponding to high privacy protection)
with better scalability than the exponential mechanism. We
provide our accuracy bounds in Section 3.4, further empirical
evaluations w.r.t. related work in Section 5.3, and describe
related work in Section 6.