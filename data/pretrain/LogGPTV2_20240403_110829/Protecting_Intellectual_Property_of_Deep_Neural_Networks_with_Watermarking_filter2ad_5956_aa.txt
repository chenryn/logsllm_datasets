title:Protecting Intellectual Property of Deep Neural Networks with Watermarking
author:Jialong Zhang and
Zhongshu Gu and
Jiyong Jang and
Hui Wu and
Marc Ph. Stoecklin and
Heqing Huang and
Ian Molloy
Protecting Intellectual Property of Deep Neural
Networks with Watermarking
Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph. Stoecklin, Heqing Huang, Ian Molloy
PI:EMAIL,{zgu,jjang,wuhu,mpstoeck,hhung,molloyim}@us.ibm.com
IBM Research
ABSTRACT
Deep learning technologies, which are the key components of state-
of-the-art Artificial Intelligence (AI) services, have shown great
success in providing human-level capabilities for a variety of tasks,
such as visual analysis, speech recognition, and natural language
processing and etc. Building a production-level deep learning model
is a non-trivial task, which requires a large amount of training data,
powerful computing resources, and human expertises. Therefore,
illegitimate reproducing, distribution, and the derivation of propri-
etary deep learning models can lead to copyright infringement and
economic harm to model creators. Therefore, it is essential to devise
a technique to protect the intellectual property of deep learning
models and enable external verification of the model ownership.
In this paper, we generalize the “digital watermarking” concept
from multimedia ownership verification to deep neural network
(DNNs) models. We investigate three DNN-applicable watermark
generation algorithms, propose a watermark implanting approach
to infuse watermark into deep learning models, and design a remote
verification mechanism to determine the model ownership. By ex-
tending the intrinsic generalization and memorization capabilities
of deep neural networks, we enable the models to learn specially
crafted watermarks at training and activate with pre-specified pre-
dictions when observing the watermark patterns at inference. We
evaluate our approach with two image recognition benchmark
datasets. Our framework accurately (100%) and quickly verifies the
ownership of all the remotely deployed deep learning models with-
out affecting the model accuracy for normal input data. In addition,
the embedded watermarks in DNN models are robust and resilient
to different counter-watermark mechanisms, such as fine-tuning,
parameter pruning, and model inversion attacks.
CCS CONCEPTS
• Security and privacy → Security services; Domain-specific
security and privacy architectures; • Computer systems or-
ganization → Neural networks;
KEYWORDS
watermarking; deep neural network; ownership verification
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5576-6/18/06...$15.00
https://doi.org/10.1145/3196494.3196550
ACM Reference Format:
Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph. Stoecklin,
Heqing Huang, Ian Molloy. 2018. Protecting Intellectual Property of Deep
Neural Networks with Watermarking. In ASIA CCS ’18: 2018 ACM Asia Con-
ference on Computer and Communications Security, June 4–8, 2018, Incheon,
Republic of Korea. ACM, New York, NY, USA, 13 pages. https://doi.org/10.
1145/3196494.3196550
1 INTRODUCTION
Recently, deep learning technologies have shown great success
on image recognition [24, 33, 48], speech recognition [19, 22, 26],
and natural language processing [17] tasks. Most major technol-
ogy companies are building their Artificial Intelligence (AI) prod-
ucts and services with deep neural networks (DNNs) as the key
components [2]. However, building a production-level deep neural
network model is not a trivial task, which usually requires a large
amount of training data and powerful computing resources. For ex-
ample, Google’s Inception-v4 model is a cutting-edge Convolutional
Neural Network (ConvNet) designed for image classification, which
takes from several days up to several weeks on multiple GPUs with
the ImageNet dataset [1] (millions of images). In addition, designing
a deep learning model requires significant machine learning ex-
pertise and numerous trial-and-error iterations for defining model
architectures and selecting model hyper-parameters.
Figure 1: Deep neural network plagiarism
As deep learning models are more widely deployed and become
more valuable, they are increasingly targeted by adversaries. Ad-
versaries can steal the model (e.g., via malware infection or insider
attackers) and establish a plagiarized AI service as shown in Fig-
ure 1. Such copyright infringement may jeopardize the intellectual
property (IP) of model owners and even take market share from
model owners. Recently DNN model sharing platforms (e.g., Model
PowerfulcomputingBigDataDNNExpertiseOwnerCompetitorsPlagiarismServiceB.MalwareA.Insider threatZoo [29] and Microsoft Model Gallery [3]) have been launched
to promote reproducible research results. In the near future, we
may see commercial DNN model markets for monetizing AI prod-
ucts. Individuals and companies can purchase and sell models in
the same way as in the current mobile app markets. In addition,
mission-critical DNN models (which may involve national security)
can even be merchandised illegitimately in Darknet markets [4].
Therefore, it is critical to find a way to verify the ownership (copy-
right) of a DNN model in order to protect the intellectual property
and detect the leakage of deep learning models.
Digital watermarking has been widely adopted to protect the
copyright of proprietary multimedia content [34, 45, 50]. The wa-
termarking procedure could be divided into two stages: embedding
and detection. In the embedding stage, owners can embed water-
marks into the protected multimedia. If the multimedia data are
stolen and used by others, in the detection stage, owners can extract
the watermarks from the protected multimedia as legal evidences to
prove the ownership of the intellectual property. Motivated by such
an intuition, we apply “watermarking” to deep neural networks
to protect the intellectual property of deep neural networks. After
embedding watermarks to DNN models, once the models are stolen,
we can verify the ownership by extracting watermarks from those
models. However, different from digital watermarking, which em-
beds watermarks into multimedia content, we need to design a new
method to embed watermarks into DNN models, and the existing
digital watermarking algorithms are not directly applicable.
Recently, Uchida et al. [54] proposed a framework to embed
watermarks in deep neural networks. This is the first attempt to ap-
ply digital watermarking to DNNs for deep neural network model
protection. The proposed algorithm embeds watermarks into the
parameters of deep neural network models via the parameter reg-
ularizer during the training process, which leads to its white-box
constraints. It requires model owners to access all the parameters
of models in order to extract the watermark, which dramatically
limits its application since the stolen models are usually deployed
remotely, and the plagiarized service would not publicize the pa-
rameters of the stolen models.
In this paper, we first address the limitations of Uchida et al. [54]’s
work by extending the threat model to support black-box mode ver-
ification, which only requires API access to the plagiarized service
to verify the ownership of the deep learning model. We then inves-
tigate three watermark generation algorithms to generate different
types of watermarks for DNN models: (a) embedding meaningful
content together with the original training data as watermarks
into the protected DNNs, (b) embedding irrelevant data samples
as watermarks into the protected DNNs, and (c) embedding noise
as watermarks into the protected DNNs. The intuition here is to
explore the intrinsic generalization and memorization capabilities
of deep neural networks to automatically learn the patterns of
embedded watermarks. The pre-defined pairs of learned patterns
and their corresponding predictions will act as the keys for the
copyright/ownership verification. After watermark embedding, our
proposed ownership verification framework can quickly verify the
ownership of remotely deployed AI services by sending normal
requests. When watermark patterns are observed, only the models
protected by the watermarks are activated to generate matched
predictions.
We evaluate our watermarking framework with two benchmark
image datasets: MNIST and CIFAR10. The results show that our wa-
termarking framework quickly (via a few requests) and accurately
(100%) verifies the ownership of remote DNN services with a triv-
ial impact on the original models. The embedded watermarks are
robust to different model modifications, such as model fine-tuning
and model pruning. For example, even if 90% of parameters are
removed from MNIST model, all of our watermarks still have over
99% of high accuracy. We also launch model inversion attacks on
the models embedded with our watermarks, and none of embedded
watermarks can be recovered.
We make the following contributions in this paper:
• We extend the existing threat model of DNN watermarking
to support black-box verification. Our watermarking frame-
work for the new threat model allows us to protect DNN
models for both white-box (having access to the model di-
rectly) and black-box (only having access to remote service
APIs) settings.
• We propose three watermark generation algorithms to gen-
erate different forms of watermarks and a watermarking
framework to embed these watermarks to deep neural net-
works, which helps verify the ownership of remote DNN
services.
• We evaluate the proposed watermark generation algorithms
and watermarking framework with two benchmark datasets.
Our proposed watermarking framework has a negligible
impact on normal inputs and the generated watermarks
are robust against different counter-watermark mechanisms,
such as fine-tuning, model compression, and model inversion
attacks.
The rest of the paper is structured as follows. In Section 2, we
present a brief overview of deep neural networks and digital water-
marking techniques. We then discuss the threat model in Section 3,
and present our watermarking framework in Section 4. Then, we
demonstrate our evaluation for the proposed watermarking frame-
work in Section 5. In Section 6, we discuss the limitation and possible
evasion of our system. We present related work in Section 7, and
conclude our work in Section 8.
2 BACKGROUND
In this section, we introduce the relevant background knowledge
about deep neural networks and watermarking, which are closely
related to our work.
2.1 Deep Neural Network
Deep learning is a type of machine learning framework which
automatically learns hierarchical data representation from training
data without the need to handcraft feature representation [18].
Deep learning methods are based on learning architectures called
deep neural networks (DNN), which are composed of many basic
neural network units such as linear perceptrons, convolutions and
non-linear activation functions. The network units are organized as
layers (from only a few layers to more than a thousand layers [24]),
and are trained to recognize complicated concepts from the raw
2
f (z)j = ezj · ((cid:80)n
data directly. Lower network layers often correspond with low-
level features (such as corner and edges), while the higher layers
correspond to high-level, semantically meaningful features [57].
Specifically, a deep neural network (DNN) takes as input the raw
training data representation, x ∈ Rm, and maps it to the output via
a parametric function, y = Fθ (x ), where y ∈ Rn. The parametric
function Fθ (·) is defined by both the network architecture and the
collective parameters of all the neural network units used in the
current network architecture. Each network unit receives an input
vector from its connected neurons and outputs a value that will be
passed to the following layers. For example, a linear unit outputs the
dot product between its weight parameters and the output values
of its connected neurons from the previous layers. To increase the
capacity of DNNs in modeling the complex structure in training
data, different types of network units have been developed and used
in combination of linear activations, such as non-linear activation
units (hyperbolic tangent, sigmoid and Rectified Linear Unit, etc.),
max pooling and batch normalization. Finally, if the purpose of the
neural network is to classify data into a finite set of classes, the
activation function in the output layer usually is a softmax function
, ∀j ∈ [1, n], which can be viewed as the
Prior to training the network weights for a DNN, the first step
is to determine the model architecture, which requires non-trivial
domain expertise and engineering efforts. Given the network ar-
chitecture, the network behavior is determined by the values of
the network parameters, θ. Let D = {xi , zi}T
i =1 be the training data,
where zi ∈ [0, n − 1] is the ground truth label for xi, the network
parameters are optimized to minimize the difference between the
predicted class labels and the ground truth labels based on a loss
function. Currently, the most widely used approach for training
DNNs is back-propagation algorithm, where the network parame-
ters are updated by propagating the gradient of prediction loss from
the output layer through the entire network. While most commonly
used DNNs are feedforward neural network where connections
between the neurons do not form loops, recurrent networks such
as long short-term memory (LSTM) [28] is effective in modeling
sequential data. In this work, we mainly focus on feed-forward
DNNs, but in principle, our watermarking strategy can be readily
extended to recurrent networks.
k =1 ezk )−1
predicted class distribution over n classes.
2.2 Digital Watermarking
Digital watermarking is a technique that embeds certain water-
marks in carrier multimedia data such as images, video or audio
to protect their copyright. The embedded watermarks can be de-
tected when the watermarked multimedia data are scanned. And
the watermark can only be detected and read to check authorship
by the owner of the multimedia data who knows the encryption
algorithm that embedded the watermarks.
Watermarking procedure is usually divided into two steps: em-
bedding and verification. Figure 2 shows a typical watermarking
life cycle. In the embedding process, an embedding algorithm E
embeds pre-defined watermarks W into the carrier data C, which
is the data to be protected. After the embedding, the embedded
data (e = E(W , C)) are stored or transmitted. During the watermark
verification process, a decryption algorithm D attempts to extract
3
Figure 2: A typical watermarking life cycle
the watermarks W ′ from e′. Here the input data e′ may be slightly
different from previously embedded data e because e could be mod-
ified during the transmission and distribution. Such modification
could be reproduced or derived from original data e. Therefore,
after extracting watermark W ′, it need to be further verified with
original watermark W . If the distance is acceptable, it is confirmed
that the carrier data is the data we protected. Otherwise, the carrier
data does not belong to us.
Since the goal of digital watermarking is to protect the copy-
right of multimedia data, and it directly embeds watermarks to the
protected multimedia data. In deep neural networks, we need to
protect the copyright of DNN models, therefore, a new watermark-
ing framework needs to be designed to embed watermarks into
DNN models.
3 THREAT MODEL
In our threat model, we model two parties, a model owner O, who
1 for a certain task t, and a
owns a deep neural network model m
suspect S, who sets up a similar service t′ from model m′, while
two services have similar performance t ≈ t′. In practice, there are
multiple ways for S to get the model m, for example, it could be
an insider attack from owner O who leaks the model or it could be
stolen by malware and sold on dark net markets. How S get model
m is out of the scope of this paper.
In this paper, we intend to help owner O protect the intellectual
property t of model m. Intuitively, if model m is equivalent to m′,
we can confirm that S is a plagiarizer and t′ is a plagiarized service
of t. Existing work [54] following such intuition to protect DNNs
by checking whether m is equivalent to m′. However, such method
requires white-box access to m′, which is not practical since a
plagiarizer usually do not publicize its m′ as a server service. In
addition, we assume the plagiarizer can modify the model m′ but
still keep the performance of t′ so that t′ ≈ t. Model pruning and
fine-tuning are two common ways to achieve this goal. Our solution
should be robust to such modifications.
To solve the above challenges, we propose three watermarks gen-
eration algorithms and a watermarking framework to help owner
O to verify whether the service t′ comes from the his model m
without getting white-box access to m′.
1The model m here includes both deep neural network architecture and parameters
as defined in Section 2
carrierdata(C)watermarks(W)embeddingwatermarkse=E(W,C)embeddeddata(e)embeddeddata(e’)extractingwatermarksW’,C’=D’(e)watermarks(W’)watermarks(W)watermarksverificationembedding	watermarksWatermarksEmbeddingProtecteddataNotprotecteddataTrueFalseWatermarksVerificationmodificationFigure 3: Workflow of DNN watermarking
4 DNN WATERMARKING
In this section, we propose a framework to generate watermarks,
embed watermarks into DNNs and verify the ownership of remote
DNNs through extracting watermarks from them. The purpose of
the framework is to protect intellectual properties of the deep neural
networks through verifying ownerships of remote DNN services
with embedded watermarks. The framework assigns pre-defined
labels for different watermarks, and trains the watermarks with
pre-defined labels to DNNs. The DNNs automatically learn and
memorize the patterns of embedded watermarks and pre-defined
labels. As a result, only the model protected with our watermarks is
able to generate pre-defined predictions when watermark patterns
are observed in the queries.
Figure 3 shows the workflow of our DNN watermarking frame-
work. The framework first generates customized watermarks and
pre-defined labels for the model owner who wants to protect his
DNN models (❶). These watermarks will be revealed as a finger-
print for ownership verification later. After generating watermarks,
the framework embeds generated watermarks into target DNNs,
which is conducted through training (❷). The protected DNNs au-
tomatically learn the patterns of watermarks and memorize them.
After embedding, the newly generated models are capable of own-
ership verification. Once they are stolen and deployed to offer AI
service, owners can easily verify them by sending watermarks as
inputs and checking the service’s outputs (❸). In this example, the
queried watermarks (“TEST” on automobile images) and the pre-
defined predictions (“airplane”) consist of fingerprints for model
ownership verification.
4.1 DNN watermark generation
As we discussed in Section 2, watermarks are essentially the unique
fingerprints for ownership verification. Therefore, watermarks
should be stealthy and difficult to be detected, or mutated by unau-
thorized parties. To achieve this goal, the number of potential wa-
termarks should be large enough to avoid being reverse engineered
even watermark generation algorithms are known to attackers.
Here we investigate three watermark generation mechanisms.
Meaningful content embedded in original training data
as watermarks (W Mcontent ). Specifically, we take images from