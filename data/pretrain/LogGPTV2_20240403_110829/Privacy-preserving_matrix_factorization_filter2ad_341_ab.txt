focus is on designing a privacy-preserving protocol in which
the recommender learns only the item proﬁles.
There is a utility in learning the item proﬁles alone. First,
the embedding of items in Rd through matrix factorization
allows the recommender to infer (and encode) similarity:
items whose proﬁles have small Euclidean distance are items
that are rated similarly by users. As such, the task of learn-
ing the item proﬁles is of interest to the recommender be-
yond the actual task of recommendations. Second, having
obtained the item proﬁles, there is a way the recommender
can use them to provide relevant recommendations without
any additional data revelation by users: the recommender
can send V to a user (or release it publicly); knowing her
ratings, i can infer her (private) proﬁle ui by solving (1)
w.r.t. ui through ridge regression [15]. Having ui and V ,
she can subsequently predict all ratings through (2).
Both of these scenarios presume that neither the recom-
mender nor the users object to the public release of V . How-
ever, in Section 4.1 we show that there is also a way to easily
extend our design so that users learn their predicted ratings
while the recommender learns nothing, not even V . For the
sake of simplicity, as well as on account of the utility of such
a protocol to the recommender, our main focus is on allow-
ing the recommender to learn the item proﬁles. This design
is also the most technically challenging; we treat the second
case as an extension to this basic design.
We note that, in general, both output of the proﬁle V or
the rating predictions for a user may reveal something about
other users’ ratings. In pathological cases where there are,
e.g., only two users, both revelations may let the users dis-
cover each other’s ratings. We do not address such cases;
when the privacy implications of the revelation of item pro-
ﬁles or individual ratings are not tolerable, techniques such
as adding noise can be used, as discussed in Section 7.
Threat Model. Our security guarantees will hold under
the honest but curious threat model [17]. In other words,
the RecSys and CSP follow the protocols we propose as pre-
scribed; however, these interested parties may elect to ana-
2Technically, our algorithm also leaks the number of items
rated by each user, though this can be easily rectiﬁed
through a simple protocol modiﬁcation.
lyze protocol transcripts, even oﬀ-line, in order to infer some
additional information. We further assume that the recom-
mender and CSP do not collude; the case of a malicious
RecSys is discussed in Section 4.
3. LEARNING THE ITEM PROFILES
In this section, we present a solution allowing the rec-
ommender system to learn only the item proﬁles V . Our
approach is based on Yao’s garbled circuits. In short, in a
basic architecture, the CSP prepares a garbled circuit that
implements matrix factorization, and provides it to the Rec-
Sys. Oblivious transfer is used to obtain the garbled inputs
from users without revealing them to either the CSP or the
RecSys. Our design augments this basic architecture to en-
able users to be oﬄine during the computation phase, as
well as to submit their ratings only once.
We begin by discussing how Yao’s protocol for secure
multi-party computation applies, and then focus on the chal-
lenges that arise in implementing matrix factorization as
a circuit. Our solution yields a circuit with a complexity
within a polylogarithmic factor of matrix factorization per-
formed in the clear by using sorting networks; an additional
advantage of this implementation is that the garbling and
the execution of our circuit is highly parallelizable.
3.1 A Privacy-Preserving Protocol for Matrix
Factorization
Yao’s Garbled Circuits. Yao’s garbled circuit method,
outlined in Appendix A, can be applied to our setting in a
manner similar to the privacy-preserving auction and ridge
regression settings studied in [48] and [50], respectively. In
brief, assume for now that there exists a circuit that im-
plements matrix factorization: this circuit receives as input
user’s ratings, supplied as tuples of the form
(i, j, rij) with (i, j) ∈ M ,
where rij represents the rating rij of user i on item j, and
outputs the item proﬁles V . Given M , the CSP garbles this
circuit and sends it to the RecSys. In turn, through proxy
oblivious transfer [48] between the users, the RecSys and the
CSP, the RecSys receives the circuit inputs and evaluates V .
As garbled circuits can only be used once, any future com-
putation on the same ratings would require the users to re-
submit their data through proxy oblivious transfer. For this
reason, we adopt a hybrid approach, combining public-key
encryption with garbled circuits, as in [57, 50]. Applied to
our setting, in a hybrid approach the CSP advertises a pub-
lic key. Each user i encrypts her respective inputs (j, rij)
and submits them to the RecSys. Whenever the RecSys
wishes to perform matrix factorization over M accumulated
ratings, it reports M to the CSP. The CSP provides the
RecSys with a garbled circuit that (a) decrypts the inputs
and then (b) performs matrix factorization; (Yao’s complete
protocol can be found in Appendix A). Nikolaenko et al. [50]
avoid decryption within the circuit by using masks and ho-
momorphic encryption; we adapt this idea to matrix fac-
torization, departing however from [50] by only requiring a
partially homomorphic encryption scheme.
We note that our protocol, and the ones outlined above,
leak beyond V also the number of ratings generated by
each user. This can easily be remedied, e.g., by pre-setting
the maximum number of ratings the user may provide and
803(i1 ,c1 )
RecSys
(i2 ,c2 )
{r1j}(1,j)∈M
x User 1
x User 2
{(1, j, r1j )}
{r1j}(1,j)∈M
········· ···
{(2, j, r2j )}
(iM ,cM )
specif.
(i1,ˆc1),...,
(iM ,ˆcM )
OT
CSP
Decrypt ˆc1,
. . . , ˆcM
Choose µ’s
Compute ˆc’s
µ1,...,µM
GI(µ1),...,
GI(µM )
GI(µ1),...,
GI(µM )
V
Figure 2: Protocol overview: Learning item pro-
ﬁles V through a garbled circuit. Each user sub-
mits encrypted item-rating pairs to the RecSys. The
RecSys masks these pairs and forwards them to the
CSP. The CSP decrypts them, and embeds them in
a garbled circuit sent to the RecSys. The garbled
values of the masks (denoted by GI) are obtained by
the RecSys through oblivious transfer.
padding submitted ratings with “null” entries; for simplicity,
we describe the protocol without this padding operation.
Detailed Description. We use public key encryption as
follows. Each user i encrypts her respective inputs (j, rij)
under the CSP’s public key, pkcsp, with a semantically secure
encryption algorithm Epk, and, for each item j she rated, sub-
mits a pair (i, c) with c = Epkcsp (j, rij) to the RecSys, where
M ratings are submitted in total. A user that submitted her
ratings can go oﬀ-line.
We require that the CSP’s public-key encryption algo-
rithm is partially homomorphic: a constant can be applied
to an encrypted message without the knowledge of the cor-
responding decryption key. Clearly, an additively homomor-
phic scheme such as Paillier [52] or Regev [56] can be used
to add a constant, but hash-ElGamal (see, e.g., [7, § 3.1]),
which is only partially homomorphic, suﬃces and can be
implemented more eﬃciently in this case; we review this
implementation in Appendix B.
Upon receiving M ratings from users—recalling that the
encryption is partially homomorphic—the RecSys obscures
them with random masks ˆc = c ⊕ µ , and sends them to
the CSP together with the complete speciﬁcations needed to
build a garbled circuit. In particular, the RecSys speciﬁes
the dimension of the user and item proﬁles (i.e., parameter
d), the total number of ratings (i.e., parameter M ), the total
number of users and items (i.e., n and m), as well as the
number of bits used to represent the integer and fractional
parts of a real number in the garbled circuit.
Upon receiving the encryptions, the CSP decrypts them
and gets the masked values: (i, (j, rij)⊕ µ). Then, using the
matrix factorization circuit as a blueprint, the CSP prepares
a Yao’s garbled circuit that
(a) takes as input the garbled values corresponding to the
masks—this is denoted by GI(µ) on Figure 2;
(b) removes the mask µ to recover the corresponding tuple
(i, j, rij);
(c) performs matrix factorization; and
(d) outputs the item proﬁles V .
The CSP subsequently makes the garbled circuit available
to the RecSys. Then, it engages in an oblivious transfer
protocol with the RecSys so that the RecSys obtains garbled
values of the masks: GI(µ). Finally, the RecSys evaluates
the circuit, whose ﬁnal (ungarbled) output comprises the
requested proﬁles V .
We note that, in contrast to the solution presented in Ap-
pendix A, the circuit recovers (i, j, rij) by simply removing
the mask through the xor operation, rather than using de-
cryption. Most importantly, as discussed in Section 5, xor
operations can be performed very eﬃciently in a garbled
circuit implementation [34].
To complete the above protocol, we need to provide a cir-
cuit that implements matrix factorization. Before we discuss
our design, we ﬁrst describe a na¨ıve solution below.
3.2 A Naïve Design
The gradient descent operations outlined in Eqs. (3)–(4)
involve additions, subtractions and multiplications of real
numbers. These operations can be eﬃciently implemented
in a circuit [50]. The K iterations of gradient descent (3)
correspond to K circuit “layers”, each computing the new
values of U and V from values in the preceding layer. The
ﬁnal output of the circuit are the item proﬁles V outputted
the last layer, while the user proﬁles are discarded.
Observe that the time complexity of computing each it-
eration of gradient descent is Θ(M ), when operations are
performed in the clear, e.g., in the RAM model: each gradi-
ent computation (4) involves adding 2M terms, and proﬁle
updates (3) can be performed in Θ(n + m) = Θ(M ) time.
The main challenge in implementing gradient descent as a
circuit lies in doing so eﬃciently. To illustrate this, consider
the following na¨ıve implementation:
1. For each pair (i, j) ∈ [n] × [m], generate a circuit that
computes from input the indicators δij = 1(i,j)∈M, which
is 1 if i rated j and 0 otherwise.
∇ui F (U, V ) = −2(cid:80)
∇vj F (U, V ) = −2(cid:80)
2. At each iteration, using the outputs of these circuits,
compute each item and user gradient as a summation
over m and n products, respectively, where:
j∈[m] δij · vj(rij − (cid:104)ui, vj(cid:105)) + 2λui,
i∈[n] δij · ui(rij − (cid:104)ui, vj(cid:105)) + 2µvi.
Unfortunately, this implementation is ineﬃcient: every iter-
ation of the gradient descent algorithm has a circuit com-
plexity of Θ(nm). When M (cid:28) nm, as is usually the case in
practice, the above circuit is drastically less eﬃcient than
gradient descent in the clear;
in fact, the quadratic cost
Θ(nm) is prohibitive for most datasets.
3.3 A Simple Counting Circuit
The ineﬃciency of the na¨ıve implementation arises from
the inability to identify which users rate an item and which
items are rated by a user at the time of the circuit design,
mitigating the ability to leverage the inherent sparsity in
the data. The question that thus naturally arises is how to
perform such a matching eﬃciently within a circuit.
We illustrate our main idea for performing this matching
through a simple counting circuit. Let cj = |{i : (i, j) ∈ M}|
804 1
 1
 .
 .
be the number of ratings item j ∈ [m] received. Suppose
that we wish to design a circuit that takes as input the set
M and outputs the counts {cj}j∈[m]. This task’s complexity
in the RAM model is Θ(m + M ), as all cj can be computed
simultaneously by a single pass over M. In contrast, a na¨ıve
circuit implementation using “indicators”, as in the previous
section, yields a circuit complexity Θ(nm). Nevertheless,
we show it is possible to construct a circuit that returns
{cj}j∈[m] in Θ(cid:0)(m + M ) log2(m + M )(cid:1) steps using a sorting
network (see Appendix C).
We ﬁrst describe the algorithm that performs this opera-
tion, and then discuss how we implement it as a circuit.
1. Given M as input, construct an array S of m+M tuples.
First, for each j ∈ [m], create a tuple of the form (j,⊥, 0),
where the “null” symbol ⊥ is a placeholder. Second, for
each (i, j) ∈ M, create a tuple of the form (j, 1, 1), yield-
ing:
S =
2
. . . m j1
⊥ ⊥ . . . ⊥ 1
1
0
. . .
0
0
j2
1
1
. . .
. . .
. . .
jM
1
1
Intuitively, the ﬁrst m tuples will serve as “counters”,
storing the number of ratings per item. The remaining
M tuples contain the “input” to be counted. The third
element in each tuple serves as a binary ﬂag, separating
counters from input.
2. Sort the tuples in increasing order w.r.t. the item ids,
i.e., the 1st element in each tuple. If two ids are equal,
break ties by comparing tuple ﬂags, i.e., the 3rd elements
in each tuple. Hence, after sorting, each “counter” tuple
is succeeded by “input” tuples with the same id: