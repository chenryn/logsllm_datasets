t > 2
t = 2
We use a subclass that gives equal weights to the most recent
half of the window, and linearly decayed weights for the earlier
half (see [19] for discussion).
Exponentially Weighted Moving Average (EWMA) With ex-
ponentially weighted moving average, the forecast for time t is the
weighted average of the previous forecast and the newly observed
sample at time t − 1.
Sf (t) = (cid:2) α · So(t − 1) + (1 − α) · Sf (t − 1),
The parameter α ∈ [0, 1] is called the smoothing constant. It
indicates how much weight is given to new samples vs. the history.
Non-Seasonal Holt-Winters (NSHW) The Holt-Winters model
[8] is another commonly used smoothing model and it has been
applied in [9] to detect aberrant behavior. In the non-seasonal Holt-
Winters model, there is a separate smoothing component Ss(t) and
a trend component St(t). There are two parameters α ∈ [0, 1] and
β ∈ [0, 1].
Ss(t) = (cid:2) α · So(t − 1) + (1 − α) · Sf (t − 1),
St(t) = (cid:2) β · (Ss(t) − Ss(t − 1)) + (1 − β) · St(t − 1),
So(2) − So(1),
t > 2
t = 2
So(1),
t > 2
t = 2
The forecast is then Sf (t) = Ss(t) + St(t).
3.2.2 ARIMA models
Box-Jenkins methodology, or AutoRegressive Integrated Mov-
ing Average (ARIMA) modeling technique [6, 7], is a class of lin-
ear time series forecasting techniques that capture the linear de-
pendency of the future values on the past values. They are able to
model a wide spectrum of time series behavior. As a result, they
have been extensively studied and widely used for univariate time
series forecasting and change detection.
An ARIMA model includes three types of parameters: the au-
toregressive parameter (p), the number of differencing passes (d),
and the moving average parameter (q). In the notation introduced
by Box and Jenkins, models are summarized as ARIMA (p, d, q).
A model described as (0, 1, 2) means that it contains p = 0 (zero)
autoregressive parameters and q = 2 moving average parameters
which were computed for the time series after it was differenced
once (d = 1). Note that we use only integral values for p, d, and q.
Although there has been recent work on models with fractional d
(the ARFIMA model) in the context of action-reaction models [27],
we have not yet examined their application in the networking con-
text.
A general ARIMA model of order (p, d, q) can be expressed as:
Zt − q
(cid:1)i=1
M Ai · Zt−i = C + et − p
(cid:1)j=1
ARj · et−i
where Zt is obtained by differencing the original time series d
times, et is the forecast error at time t, M Ai (i = 1, ..., q) and
ARj (j = 1, ..., p) are MA and AR coefﬁcients.
In practice, p and q very rarely need to be greater than 2. The
number of differences (d) is typically either 0 or 1. Therefore, when
we extend ARIMA models to the sketch context, we only consider
the following two types of ARIMA models (the names are based
on the number of differences):
• ARIMA0: ARIMA models of order (p ≤ 2, d = 0, q ≤ 2)
• ARIMA1: ARIMA models of order (p ≤ 2, d = 1, q ≤ 2)
In ARIMA models, the choice of MA and AR coefﬁcients M Ai
(i = 1, ..., q) and ARj (j = 1, ..., p) must ensure that the resulting
models are invertible and stationary. As a necessary but insufﬁcient
condition, M Ai and ARj must belong to the range [−2, 2] when
p, q ≤ 2.
3.3 Change detection module
After constructing the forecast error sketch Se(t), the change
detection module chooses an alarm threshold TA based on the esti-
= T·[ ESTIMATEF2(Se(t)) ]
mated second moment of Se(t): TA
where T is a parameter to be determined by the application.
Now for any key a, the change detection module can reconstruct
its forecast error in Se(t) using ESTIMATE(Se(t), a) and raise an
alarm whenever the estimated forecast error is above the alarm
threshold TA.
def
The remaining question is how to obtain the stream of keys for
the change detection module. Sketches only support reconstruction
of the forecast error associated with a given key. It does not contain
information about what keys have appeared in the input stream.
There are several possible solutions to this problem. With the
brute-force solution, one can record all the keys that appeared in
recent intervals (e.g., the same interval t over which Se(t) is de-
ﬁned) and replay them after Se(t) has been constructed. This still
requires maintaining per-ﬂow information.
Its scalability is lim-
ited by the maximum number of keys that appear in the window
for key collection. We can avoid keeping per-ﬂow state by using
a two-pass algorithm—construct Se(t) in the ﬁrst pass and detect
changes on the second pass. Since the input stream itself will pro-
vide the keys, there is no need for keeping per-ﬂow state. This
requires access to the same input stream twice and thus useful only
in the ofﬂine context. A third alternative is to use the keys that ap-
pear after Se(t) has been constructed. This works in both online
and ofﬂine context. The risk is that we will miss those keys that do
not appear again after they experience signiﬁcant change. This is
often acceptable for many applications like DoS attack detection,
where the damage can be very limited if a key never appears again.
Note that we do not need to do this for every newly arrived item.
If we can tolerate the risk of missing some very infrequent keys,
we can sample the (future) input streams and only work on a sub-
stream of keys. Another possibility is to incorporate combinatorial
group testing into sketches [14]. This allows one to directly infer
keys from the (modiﬁed) sketch data structure without requiring a
separate stream of keys. However, this scheme also increases the
1
2 ,
update and estimation costs and additional research is required to
make it more efﬁcient. In this paper, we use the ofﬂine two-pass
algorithm in all experiments.
3.4 Parameter conﬁguration
Our change detection framework includes sketch-related param-
eters as well as control parameters for various forecasting models.
Below we provide guidelines and heuristics for properly conﬁgur-
ing these parameters—an important step for making our framework
practical.
3.4.1 Sketch parameters: H and K
There are two sketch-related parameters:
the number of hash
functions (H), and the size of hash tables (K). Depending on the
choice of H and K, k-ary sketches can provide probabilistic guar-
antees on the estimation accuracy of the forecast errors and their
total energy (see Appendix A and B for details). We can use such
analytical results to determine the choice of H and K that are suf-
ﬁcient to achieve targeted accuracy. As the analytical results apply
in a data-independent way, the resulting H and K may be too con-
servative for the actual dataset. Hence, we use analytical results to
derive data-independent choice of H and K and treat them as upper
bounds. We then use training data to ﬁnd the best (data-dependent)
H and K values.
3.4.2 Forecasting model parameters
Criteria for good parameters
In the context of univariate time
series forecasting, a commonly used simple heuristic for conﬁg-
uring model parameters is choosing parameters that minimize the
total residual energy, i.e., the sum of squares (of forecast errors)
over time.
We can extend the above heuristic to the sketch context and look
for parameters that minimize the total energy in the resulting fore-
cast error sketches over time t F2(Se(t)), where F2(Se(t)) is
the second moment for all the forecast errors summarized by sketch
Se(t).
We will not know the true F2(Se(t)) unless we do per-ﬂow
analysis for each parameter setting, which can be prohibitive. In-
stead we use the estimated second moment F est
2 (Se(t)), as long as
F est
2 (Se(t)) closely approximates F2(Se(t)). In other words, we
try to ﬁnd parameters that minimize the estimated total energy of
forecast errors t F est
2 (Se(t)).
Multi-pass grid search For parameters that are continuous, we
use a multi-pass grid search algorithm to ﬁnd a good choice. Con-
sider for example the EWMA model. The ﬁrst pass ﬁnds a pa-
rameter α ∈ {0.1, 0.2, ..., 1.0} that minimizes the estimated total
energy for the forecast errors. Let a0 be the best α. The second pass
equally subdivides range [a0−0.1, a0+0.1] into N = 10 parts and
repeats the process. We obtain high precision via multiple passes.
For models with integral parameters, such as the moving average
model, we can simply vary the parameter to ﬁnd the best one. Note
that grid search is only a heuristic. It does not guarantee that we
will ﬁnd the optimal parameter combination that minimizes the es-
timated total energy for forecast errors. However, we only need to
have good enough parameters such that the resulting model cap-
tures the overall time series behavior. We will show later that grid
search indeed achieves this.
4. EXPERIMENTAL SETUP
We use large amounts of real Internet trafﬁc data to evaluate and
validate our approach. Below we describe our datasets and the ex-
perimental parameter settings.
4.1 Dataset description
Input data is chosen to be four hours worth of netﬂow dumps
from ten different routers in the backbone of a tier-1 ISP. Nearly
190 million records are processed with the smallest router having
861K records and the busiest one having over 60 million records in
a contiguous four hour stretch.
4.2 Experimental parameters
In this section we present the various values of parameters that
we used in our experiments and justify their choices. We also
present ways in which these values should be tailored in using our
approach based on the local data available. Note that some of the
parameters would have different values when the sketch technique
is used for different applications.
The cost of estimation and updating is dominated by the num-
ber of hash tables, so we choose small values for H. Meanwhile, H
improves accuracy by making the probability of hitting extreme es-
timates exponentially small (see Theorem 2, 3, and 5 in Appendix),
suggesting again that it is enough to use small H. We vary H to see
the impact on the accuracy of estimation with respect to the cost.
Our choices of H (1, 5, 9, and 25) are driven by the fact that we can
use optimized median networks to ﬁnd the medians quickly without
making any assumptions on the nature of the input [16, 37]. The
analytic upper bound needed to provide a speciﬁc degree of error
threshold by using k-ary sketches is used as the upper reach of K.
We can tighten the lower bound of zero by empirically examining
values between 0 and the upper bound in log(upper-bound) steps.
In our experiments we used an upper bound of 64K and using our
data we quickly zoomed in on a lower bound of K = 1024.
Another important parameter is the interval size: a long inter-
val would result in delays since our scheme reports anomalies at
the end of each interval and we will miss more events that occur
within a single interval. A short interval requires us to update
the sketch-based forecasting data structures more frequently. We
choose 5 minutes as a reasonable tradeoff between the responsive-
ness and the computational overhead. Such an interval is used in
other SNMP based network anomaly detection systems [3]. We
also use 1 minute intervals to examine the impact of shorter inter-
vals.
Each of the six models requires different choices of parameters.
For the moving average models (MA and SMA) we pick a single
time interval to be the minimum window size and 10 (12) to be
the maximum window size for interval size of 5 (1) minutes. The
window size yielding the minimum total energy of forecast errors
across each of the interval values is selected as the parameter. For
the remaining models we apply a 2-pass grid search algorithm to
choose different parameters. For the EWMA and NSHW models,
during each pass we partition the current ranges into 10 equal in-
tervals. For ARIMA models, however, the number of parameters is
much larger and the search space becomes too large if we partition
each parameter range into 10 parts. To limit the search space, we
partition the current search range into 7 parts instead. During grid
search, H is ﬁxed at 1 and K at 8K. As we will see later, with H =
1 and K = 8K, the estimated total energy of forecast errors closely
approximates the true energy obtained using per-ﬂow analysis.
5. EXPERIMENT RESULTS
In this section, we present the results of our evaluation of the
feasibility of using sketches for change detection. The setup for
the various experiments is described in Section 4.2 and we present
results in detail for three models (EWMA and ARIMA with d =
0 and 1) and occasional results for NSHW. We should note that in
most cases the results from the various models are largely similar
and we exclude them in the interest of brevity.
The evaluation is divided into three parts: We ﬁrst report on
the validity of the parameters generated by our grid search. Next,
we report on evaluation of sketches at the ﬂow-level—focusing on
what sketch reports as (i) the top-N ﬂows with the maximum abso-
lute forecast errors, and (ii) the ﬂows whose absolute forecast error
exceeds a threshold, and comparing the sketch report with what
per-ﬂow scheme reports. We then report on the implementation
complexity and the running time.
5.1 Grid Search
The experiments in this section are concerned with determining
appropriate parameter settings for the forecast models, values for
H and K, and in evaluating the usefulness of grid search.
• We use the estimated total energy (instead of the true total
energy) as the metric for selection of the forecast model pa-
rameter setting(see Section 3.4.2). For this approach to yield
good performance, we need to ensure that the estimated value
closely tracks the true value. This is the focus of the ﬁrst part
of the study in this section.
• We also explore the space of (H,K) values and various pa-
rameter settings to zoom in on suitable choices of H and K
that result in good performance.
• Note that we use grid search to select the parameter setting
that results in the minimum total energy (see Section 3.4.2).
In this section we evaluate the “goodness” of the parameter
selected by grid search, compared to a random selection of
parameters.
5.1.1 Results
F
D
C
l
a
c
i
r
i
p
m
E
1
0.8
0.6
0.4
0.2
0
-3
MA
SMA
EWMA
ARIMA0
ARIMA1
NSHW
-2
-1
0
1
2
3
4
Relative Difference (%)
Figure 1: CDF for Relative Difference: all models,
val=300, H=1,K=1024
inter-
We ﬁrst perform a set of experiments (called random) over a
collection of 10 router ﬁles (consisting of over 189 million ﬂow
records). For each forecast model, we randomly select a number of
points in the model parameter space, and for each chosen point and
(H,K) value combination, run both sketch and per-ﬂow based detec-
tion on each router trace. The goal here is to examine differences
between the different forecast models, and to evaluate parameter
value choices for H and K (the hash table and range sizes). The ex-
periment also allows us to explore how sketches and per-ﬂow com-
pare when the forecast parameters are not selected carefully. The
comparison metric is the Relative Difference, which is deﬁned as:
the difference between the total energy (square root of the sum of
1
0.8
0.6
0.4
0.2
F
D
C
l
a
c
i
r
i
p
m
E
H=1, K=1024
H=5, K=1024
H=9, K=1024
H=25, K=1024
0
-0.5 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 0.5
Relative Difference (%)
(a) Model=EWMA
H=1, K=8192
H=5, K=8192
H=9, K=8192
H=25, K=8192
1
0.8
0.6
0.4
0.2
F
D
C
l
a
c
i
r
i
p
m
E
0
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
Relative Difference (%)
(b) Model=ARIMA0
Figure 2: Result of varying H in random
second moments for each time interval) computed from the sketch-
based technique and the total energy obtained using per-ﬂow detec-
tion, expressed as a percentage of the total energy obtained using
per-ﬂow detection. For a particular forecast model and (H,K) com-
bination, for each router ﬁle, we obtain multiple Relative Differ-
ence values, one for each selected point in the parameter space for
that model. In Figures 1-3, each curve corresponds to a particular
forecast model and (H,K) combination, and represents the empiri-
cal CDF of the Relative Difference values aggregated from across
all the routers.
Figure 1 shows that even for small H (1) and K (1024), across all
the models, most of the mass is concentrated in the neighborhood
of the 0% point on the x-axis, indicating that even for randomly
chosen model parameters, the total energy from the sketch-based
approach is very close to that for per-ﬂow. Only for the NSHW
model a small percentage of points have sketch values that differ
by more than 1.5% from the corresponding per-ﬂow values. The
worst case difference is 3.5%.
Next, we examine the impact of varying the H parameter. Fig-
ure 2 shows, for the EWMA and ARIMA0 models, that there is no
need to increase H beyond 5 to achieve low relative difference.
The last set of results for the random parameter technique is
shown in Figure 3, and demonstrates that once K = 8192 (8K) the
relative difference becomes insigniﬁcant, obviating the need to in-