scribed in Section 5.7), a dataset deeply diagonal exhibiting numer-
ous collinearities and reflectable components. The second database
is the NIS 2008 AGE â‰¤ 18 & NPR database, a fairly dense medical
database. We utilized Python library PyDistinct [7] to compute the
estimates. They were chosen as they represent two fairly different
real-world data distributions. For more information, see Table 2.
We tested the robustness of each estimator under the (i) uniform
distribution, (ii) Beta(2,1) distribution and (iii) Gaussian(1/2,1/5)
distribution of the queries. Recall that our goal is to estimate the
query densities ğœŒğ‘– for each ID ğ‘– and ğœŒğ‘–,ğ‘— for each pair of IDs. Thus,
we obtained estimates(cid:98)ğœŒğ‘– and(cid:98)ğœŒğ‘–,ğ‘— from the three estimators under
the three query distributions and computed the mean squared er-
ror (MSE) of such estimates. In Figure 7, we plot the MSE of the
estimators against the query ratio, which we define as the ratio of
the number of queries observed and the total number of possible
Session 7C: Database and Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2252Uniform
Gaussian(1/2, 1/5)
Beta(2, 1)
(a)
(b)
Figure 7: MSE of the estimators on the (a) Spitz and (b) 2008
NIS AGE â‰¤ 18 & NPR datasets over the query ratio.
queries. I.e., if we have observed ğ‘ queries (including any duplicates)
and there are a total of ğ‘ possible queries, the query ratio is ğ‘
ğ‘ . Note
that even when this ratio is 1, the adversary most likely has not
observed all possible queries. Missing values in the plots of Figure 7
are due to failure by the estimators to produce an answer in some
cases. Overall, we found that the Chao-Lee estimator consistently
performed best, especially for a small query ratio.
7 Approximate Database Reconstruction
Our distribution-agnostic attack for ADR assumes the ordering
of the points and consists of two parts. As we saw in Section 6,
non-parametric estimators may perform differently under different
query distributions. In our experiments, the Chao-Lee estimator
performed the best under all three distributions and we use it to
estimate how many query responses contain a point or a set of
points. We use these estimates to construct a system of equations,
whose solution gives an approximate reconstruction.
7.1 Algorithm
We assume knowledge of the ordering of the database (e.g., as
given by Algorithm 4). The first step of ADR is to build a system
of equations. We know that point ğ‘ with coordinates ğ‘0, ğ‘1 will be
included in ğœŒğ‘ = ğ‘0ğ‘1(ğ‘0 âˆ’ ğ‘0)(ğ‘1 âˆ’ ğ‘1) unique responses. The
construct an equation with unknowns ğ‘¥ğ‘, ğ‘¦ğ‘.
Chao-Lee estimator will give us an estimate,(cid:98)ğœŒğ‘, of ğœŒğ‘. We then
responses. We estimate ğœŒğ‘,ğ‘ as(cid:98)ğœŒğ‘,ğ‘, and construct an equation with
(5)
Given a pair of points ğ‘, ğ‘, where ğ‘ dominates ğ‘, we know that
both points are included in ğœŒğ‘,ğ‘ = ğ‘0ğ‘1(ğ‘0 âˆ’ ğ‘0)(ğ‘1 âˆ’ ğ‘1) unique
unknowns ğ‘¥ğ‘, ğ‘¦ğ‘, ğ‘¥ğ‘, ğ‘¦ğ‘.
ğ‘¥ğ‘ğ‘¦ğ‘(ğ‘0 âˆ’ ğ‘¥ğ‘)(ğ‘1 âˆ’ ğ‘¦ğ‘) =(cid:98)ğœŒğ‘
ğ‘¥ğ‘ğ‘¦ğ‘(ğ‘0 âˆ’ ğ‘¥ğ‘)(ğ‘1 âˆ’ ğ‘¦ğ‘) =(cid:98)ğœŒğ‘,ğ‘
(6)
We build a similar equation from any ordering of ğ‘ and ğ‘, fol-
lowing Equation 4. If two points are in both a dominance and
anti-dominance relationship, then they must be collinear. We add
this constraint to our system. We use the Chao-Lee estimator to
approximate the ğœŒ values (ğœŒğ‘, ğœŒğ‘,ğ‘) from the multiset of responses
we have seen. We then construct a first guess for the values of the
points using their ordering. Each point ğ‘ is given coordinates cor-
responding to its indexes in the first and second dimension. Finally,
we find an approximation of the databaseâ€™s point values using a
least-squares approach.
Our ADR attack is summarized in Algorithm 5, which takes as
input a multiset ğ‘€ of token-response pairs, the ordering ğº, ğºâ€² and
the domain size (ğ‘0, ğ‘1). It returns a reconstructed point set.
Algorithm 5: ADR(ğ‘€, ğº, ğºâ€², ğ‘0, ğ‘1)
1: Let ğ‘” be a reconstruction of the point values using ğº and ğºâ€², giving
each point a value corresponding to its order in each dimension.
2: Create a system of ğœŒ equations for all single points and pairs, including
any collinearities, utilizing Equations 5 and 6.
3: Using the submultiset ğ‘€ of token-response pairs we have observed
and the Chao-Lee estimator approximate, the ğœŒ value of each equation,
as described in Section 6.
4: return a least-squares solution to the system of equations initializing
at ğ‘”
7.2 Datasets and System
The ADR attack assumes the order of the records as an input. For
our experiments, we picked the correct order from the results of
OR (Algorithm 4). We tested our ADR attack on real world datasets:
the California [30] and Spitz [43] location datasets and the HCUP
NIS medical datasets [1] described in Section 5.7. Due to complexity
constraints, we sub-sampled the datasets (resulting domain and
more information shown in Table 2). We performed one run per
experiment and, for each experiment, we sampled queries according
to the uniform, Beta(2, 1), and Gaussian(1/2, 1/5) distributions.
Our experiments were run on the Brown University Computer
Science Compute Grid, which runs on Intel Xeon and AMD Opteron
CPUs and relies on the Oracle Grid Engine to schedule jobs. We
implemented our attack in Python 3.7.1. For our experiments, we
used PyDistinct [7] to estimate the ğœŒ values and the ğ‘™ğ‘’ğ‘ğ‘ ğ‘¡_ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ğ‘ 
function from SciPy Optimize [47] to solve our system of equations.
The Numpy [22] library was used for general computing.
7.3 Accuracy Metrics
We measure the accuracy of the reconstruction with the follow-
ing four metrics to take into account different characteristics. The
mean error is the average distance of a reconstructed point to the
original point. We use the normalized mean error, which is ob-
tained by dividing the mean error by ğ‘0 + ğ‘1, where [ğ‘0] Ã— [ğ‘1]
is the domain of the database. The mean squared error is the
average squared distance of a reconstructed point to the original
point. This widely used error metric (e.g., [27]) gives greater weight
to larger errors. The Hausdorff distance of point sets ğ‘ƒ and ğ‘„,
denoted ğ»(ğ‘ƒ, ğ‘„), is a common measure of how far ğ‘ƒ and ğ‘„ are
from each other. It is defined as ğ»(ğ‘ƒ, ğ‘„) = max(â„(ğ‘ƒ, ğ‘„), â„(ğ‘„, ğ‘ƒ),
where â„(ğ‘ƒ, ğ‘„) = maxğ‘âˆˆğ‘ƒ (minğ‘âˆˆğ‘„ dist(ğ‘, ğ‘)). We obtain the pair-
wise relative distance error by computing all distances between
pairs of original points and between pairs of reconstructed points,
Table 2: Real-world datasets used in our experiments.
Dataset
California [30]
Spitz [43]
NIS 2008 [1]
NIS 2009 [1]
Attributes
LAT & LONG
LAT & LONG
AGE<18 & NPR
NCH & NDX
NCH & NPR
NCH & NDX
NCH & NPR
NDX & NPR
# Queries
26532800
130500
80784
663300
158400
621270
246753
1244310
#Points Domain
102 Ã— 102
25 Ã— 25
18 Ã— 33
25 Ã— 67
25 Ã— 33
27 Ã— 60
27 Ã— 38
60 Ã— 38
1000
28
355
529
574
528
566
862
Session 7C: Database and Privacy CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2253Uniform
Gaussian(1/2, 1/5)
Beta(2, 1)
(a)
(b)
(c)
Figure 8: Reconstructions generated by our algorithm.
Empty blue circles denote original points and filled green
circles denote reconstructed points. (a) Spitz dataset with
7% query ratio. (b) California dataset with 4% query ratio.
(c) Postprocessing adjustment.
calculating the absolute values of the differences of such distances,
normalizing by the original distances, and taking the mean. This
measure captures the accuracy of the shape of the reconstructed
points. For the Hausdorff distance, we use SciPyâ€™s [47] implementa-
tion of the algorithm in [44]. The other metrics are easily computed.
7.4 Experiments
Figure 8 shows our reconstructions of the Spitz and California
datasets. We cannot present reconstructions of the NIS datasets per
the HCUP data usage agreement. In Figures 9 and 12 (Appendix E),
we give the accuracy metrics and computational resource usage of
our reconstructions for all databases under the different distribu-
tions. On the ğ‘¥-axis we show the query ratio, i.e., the number of
queries observed by the adversary over the total number of pos-
sible queries. Recall that even when this ratio is 1, the adversary
most likely has not observed all possible queries as some duplicate
queries would typically be issued.
Our attack performs consistently well on both the location and
medical datasets under all four metrics and all three query distribu-
tions. The four accuracy metrics follow similar trends. As expected,
the accuracy of our reconstruction generally improves with the
query ratio. In particular, for the uniform distribution, we already
achieve near perfect reconstruction with query ratio around 10%,
while for the Beta and Gaussian distributions, there are still errors
even at 80% query ratio. Note that the smaller the query ratio is,
the higher the variation of accuracy across experiments is, since
different query samples vary in usefulness. This is partially due our
estimator performing worse under non-uniform distributions and
small query ratios (see Figure 7).
Our experiments required between a few megabytes of memory
to tens of gigabytes for the more computationally intensive ones.
Interestingly, the query ratio seems to have a small effect to the
memory required and the size of the database is the deciding factor.
The experiments needed from a few CPU seconds to several CPU
days to complete. We show the memory and CPU time required by
the experiments in the last and second to last columns of Figure 9, re-
spectively. Note that our code was not optimized for multi-threaded
programming as it was written in Python. In multiple experiments,
the CPU usage tends to be high at low query ratios. We believe this
phenomenon is caused by inaccurate estimates from the estimators
(Section 6) that make finding a least squares solution harder.
We ran our experiments on a computing grid that automatically
allocated CPUs per experiment. We show in Figure 10(a) the his-
togram of an indicator of the speedup provided by the grid for our
experiments. This indicator equals the CPU time divided by the
wall-clock time minus 1. Over all experiments, the mean was 0.629,
the maximum 4.372, and the variance 0.315, confirming the modest
speedup provided by the grid. We infer that our experiments can
be reproduced in a computing environment with â‰¥ 5 CPUs.
7.5 Post-processing Adjustment
In a number of datasets, our solution is topologically close to the
original data, yet translated. We now explore how to further reduce
the reconstruction error. In Figure 8b, the shape of California is clear,
yet in the Gaussian and Beta cases, the points are shifted towards
the bottom right. If we were given the centroid of the original
points, we could compare it with the centroid of our solution, and
translate all points by their difference, as shown in Figure 8c.
We ran this adjustment technique on the reconstructions of the
California dataset and NIS 2009 NCH & NDX and NCH & NPR
datasets. For the latter, we used the centroids of the corresponding
2008 NIS datasets as proxies for the original centroids. This choice
is motivated by the fact that the adversary might have access to the
statistics of a related dataset with a similar centroid. We applied the
adjustment only to the Beta and Gaussian distributions since our
reconstructions under the uniform distribution are already very
good. We report in Figure 10(b) the variation of the normalized mean
error (NME), mean squared error (MSE), and Hausdorff distance
(HD) due to our post-processing adjustment. Since we are only
translating the points, the pairwise relative distance error does not
change. The experiments show that this simple adjustment often
significantly reduces reconstruction error.