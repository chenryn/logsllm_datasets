### Approach and Certified Accuracy

For the network with \( L = 0.1 \), the certified accuracy for attacks smaller than 0.09 in 2-norm is 59%. For the network with \( L = 0.3 \), the certified accuracy for attacks up to size 0.2 is 40%. To our knowledge, PixelDP is the first defense to provide deep neural networks (DNNs) with certified bounds on accuracy under 2-norm attacks, particularly for large datasets like ImageNet and complex models such as Inception.

### Certified Accuracy for Larger Attacks

PixelDP networks designed for larger attacks (higher \( L \), hence higher noise) tend to yield higher certified accuracy for high thresholds \( T \). For example, on CIFAR-10 (see Fig. 2(b)), a ResNet constructed with \( L = 0.03 \) has the highest robust accuracy up to \( T = 0.03 \). However, the ResNet constructed with \( L = 0.1 \) becomes more accurate beyond this threshold. Similarly, the \( L = 0.3 \) ResNet outperforms the \( L = 0.1 \) ResNet above the 0.14 2-norm prediction robustness threshold.

We conducted similar experiments on SVHN, CIFAR-100, and MNIST models but omitted the graphs for brevity. Our main conclusion—that adding more noise (higher \( L \)) degrades both conventional and low \( T \) certified accuracy but improves the quality of high \( T \) predictions—holds across all cases. Appendix B discusses the impact of design choices on robust accuracy, and Appendix D compares PixelDP guarantees with previous certified defenses for \(\infty\)-norm attacks. While PixelDP does not yet offer strong \(\infty\)-norm bounds, it provides meaningful certified accuracy bounds for 2-norm attacks, including on much larger and more complex datasets and networks than those supported by previous approaches.

### Accuracy Under Attack (Q3)

A standard method to evaluate the strength of a defense is to measure the conventional accuracy of a defended model on malicious samples obtained by running state-of-the-art attacks against a held-out testing set [37]. We apply this method to answer three aspects of question Q3: (1) Can PixelDP help defend complex models on large datasets in practice? (2) How does PixelDP’s accuracy under attack compare to state-of-the-art defenses? (3) How does the accuracy under attack change for certified predictions?

#### Accuracy under Attack on ImageNet

We first study the conventional accuracy under attack for PixelDP models on ImageNet. Figure 3 shows this metric for 2-norm attacks on the baseline Inception-v3 model and three defended versions, each with a stacked PixelDP auto-encoder trained with construction attack bounds \( L \in \{0.1, 0.3, 1.0\} \). PixelDP significantly enhances the model's robustness to attacks. For attacks of size \( L_{\text{attack}} = 0.5 \), the baseline model's accuracy drops to 11%, while the \( L = 0.1 \) PixelDP model maintains an accuracy above 60%. At \( L_{\text{attack}} = 1.5 \), the baseline model's accuracy is 0, but the \( L = 0.1 \) PixelDP model still achieves 30% accuracy, and the \( L = 0.3 \) PixelDP model achieves over 39% accuracy.

#### Comparison with Madry

Figure 4(a) compares the conventional accuracy of a PixelDP model with that of a Madry model on CIFAR-10, as the empirical attack bound increases for 2-norm attacks. For 2-norm attacks, our model achieves accuracy on par with, or slightly higher than, the Madry model. Both models are dramatically more robust under these attacks compared to the baseline (undefended) model. For \(\infty\)-norm attacks, our model does not perform as well, which is expected since the PixelDP model is optimized for 2-norm attacks, while the Madry model is optimized for \(\infty\)-norm attacks. For \( L_{\text{attack}} = 0.01 \), PixelDP's accuracy is 69%, 8 percentage points lower than Madry's. The gap increases until PixelDP reaches 0 accuracy at \( L_{\text{attack}} = 0.06 \), while Madry still has 22% accuracy. Appendix §D details this evaluation.

#### Comparison with RobustOpt

Figure 4(b) shows a similar comparison with the RobustOpt defense [65], which provides certified accuracy bounds for \(\infty\)-norm attacks. We use the SVHN dataset for the comparison because the RobustOpt defense has not yet been applied to larger models. For 2-norm attacks, PixelDP performs comparably to Madry until \( L_{\text{attack}} \geq 1.2 \); RobustOpt, which supports only small models, has lower accuracy.

### Precision on Certified Predictions Under Attack

Another interesting feature of PixelDP is its ability to make certifiably robust predictions. We compute the accuracy of these certified predictions under attack, which we term robust precision, and compare them to predictions of the Madry network that do not provide such certification. Figure 5 shows the results for predictions with a certified robustness above 0.05 and 0.1. It reflects the benefit for applications that can leverage our theoretical guarantees to filter out non-robust predictions. We observe that PixelDP's robust predictions are substantially more correct than Madry's predictions up to an empirical attack bound of 1.1. For \( T = 0.05 \), PixelDP's robust predictions are 93.9% accurate, and up to 10 percentage points more correct under attack for \( L_{\text{attack}} \leq 1.1 \). A robust prediction is given for over 60% of the data points. The more conservative the robustness test (higher \( T \)), the more correct PixelDP's predictions become.

### Computational Overhead (Q4)

**Q4: What is PixelDP’s computational overhead?** We evaluate overheads for training and prediction. PixelDP adds minimal overhead for training, as the only additions are a random noise tensor and sensitivity computations. On our GPU, the CIFAR-10 ResNet baseline takes an average of 0.65 seconds per training step. PixelDP versions take at most 0.66 seconds per training step (1.5% overhead). This represents a significant benefit over adversarial training (e.g., Madry) that requires finding good adversarial attacks for each image in the mini-batch at each gradient step, and over robust optimization (e.g., RobustOpt) that requires solving a constrained optimization problem at each gradient step. The low training overhead is crucial for supporting large models and datasets.

PixelDP impacts prediction more substantially, as it uses multiple noise draws to estimate label scores. Making a prediction for a single image with one noise draw takes 0.01 seconds on average. Making 10 draws brings it to 0.02 seconds, but 100 draws require 0.13 seconds, and 1000 draws, 1.23 seconds. Using Hoeffding's inequality [25], we can bound the number of draws necessary to distinguish the highest score with probability at least \(\eta\), given the difference between the top two scores \( y_{\text{max}} - y_{\text{second-max}} \). Empirically, we found that 300 draws were typically necessary to properly certify a prediction, implying a prediction time of 0.42 seconds, a 42x overhead. This is parallelizable, but resource consumption is still substantial. For simple predictions—distinguishing the top label when a prediction must be made on all inputs—25 draws are sufficient in practice, reducing the overhead to 3x.

### Analysis

We make three points about PixelDP’s guarantees and applicability:

1. **Monte Carlo Approximation:** Our Monte Carlo approximation of the function \( x \mapsto E(A(x)) \) is not intended to be a differential privacy (DP) procedure. Hence, there is no need to apply DP composition rules. The Monte Carlo approximation \( x \mapsto \hat{E}(A(x)) \) is just an approximation to a function \( x \mapsto E(A(x)) \) whose robustness guarantees come from Lemma 1. The function \( x \mapsto \hat{E}(A(x)) \) does not satisfy DP, but we can control the Monte Carlo estimation error using standard tools from probability theory, making it robust to small changes in the input, similar to \( x \mapsto E(A(x)) \).

2. **Proposition 1:** Proposition 1 is not a high-probability result; it is valid with probability 1 even when \( A \) is \((\epsilon, \delta > 0)\)-DP. The \(\delta\) parameter can be thought of as a "failure probability" of an \((\epsilon, \delta)\)-DP mechanism: a chance that a small change in input will cause a big change in the probability of some of its outputs. Since \( A_k(x) \in [0, 1] \), the worst-case impact of such failures on the expectation of the output of the \((\epsilon, \delta)\)-DP mechanism is at most \(\delta\), as proven in Lemma 1. Proposition 1 explicitly accounts for this worst-case impact (term \((1 + e^\epsilon)\delta\) in Equation (4)). If we could compute \( E(A(x)) \) analytically, PixelDP would output deterministic robustness certificates. In practice, the exact value is too complex to compute, so we approximate it using a Monte Carlo method, adding probabilistic measurement error bounds. The final certification (Proposition 2) is a high-probability result, but the uncertainty comes exclusively from the Monte Carlo integration and can be made arbitrarily small with more runs of the PixelDP DNN. The uncertainty does not come from the underlying \((\epsilon, \delta)\)-DP mechanism \( A \). Making the uncertainty small gives an adversary a small chance to fool a PixelDP network into thinking that its prediction is robust when it is not. The only ways an attacker can increase that chance are by either submitting the same attack payload many times or gaining control over PixelDP’s source of randomness.

3. **Applicability:** PixelDP applies to any task where we can measure changes to input in a meaningful \( p \)-norm and bound the sensitivity to such changes at a given layer in the DNN (e.g., sensitivity to a bounded change in a word frequency vector or a change of class for categorical attributes). PixelDP also applies to multiclass classification where the prediction procedure returns several top-scoring labels. Finally, Lemma 1 can be extended to apply to DP mechanisms with (bounded) output that can also be negative, as shown in Appendix E. PixelDP thus directly applies to DNNs for regression tasks (i.e., predicting a real value instead of a category) as long as the output is bounded (or unbounded if \(\delta = 0\)). The output can be bounded due to the specific task or by truncating the results to a large range of values and using a comparatively small \(\delta\).

### Related Work

Our work relates to a significant body of research on adversarial examples and beyond. Our main contribution is introducing a new and different direction for building certified defenses. Previous attempts have built on robust optimization theory. In PixelDP, we propose a new approach based on differential privacy theory, which exhibits flexibility, broad applicability, and scalability that exceed what robust optimization-based certified defenses have demonstrated. While the most promising way to defend against adversarial examples is still an open question, we observe undebatable benefits unique to our DP-based approach, such as the post-processing guarantee of our defense. Specifically, the ability to prepend a defense to unmodified networks via a PixelDP auto-encoder, as we did to defend Inception with no structural changes, is unique among certified (and best-effort) defenses.

**Best-effort Defenses:** Defenders have used multiple heuristics to empirically increase DNNs' robustness. These defenses include model distillation [45], automated detection of adversarial examples [24, 42, 41], application of various input transformations [29, 10], randomization [23, 11], and generative models [51, 27, 68]. Most of these defenses have been broken, sometimes months after their publication [7, 6, 2].

The main empirical defense that still holds is Madry et al. [37], based on adversarial training [21]. Madry et al. motivate their approach with robust optimization, a rigorous theory. However, not all assumptions are met, as this approach runs a best-effort attack on each image in the mini-batch at each gradient step, when the theory requires finding the best possible adversarial attack. Finding this worst-case adversarial example for ReLU DNNs, used in [37], was proven to be NP-hard in [53]. Therefore, while this defense works well in practice, it gives no theoretical guarantees for individual predictions or for the model’s accuracy under attack. PixelDP leverages DP theory to provide guarantees of robustness to arbitrary, norm-based attacks for individual predictions.

**Randomization-based Defenses:** Randomization-based defenses are closest in method to our work [23, 11, 35]. For example, Liu et al. [35] randomizes the entire DNN and predicts using an ensemble of multiple copies of the DNN, essentially using draws to roughly estimate the expected arg max prediction. They observe empirically that randomization smoothens the prediction function, improving robustness to adversarial examples. However, randomization-based prior work provides limited formalism that is insufficient to answer important defense design questions: where to add noise, in what quantities, and what formal guarantees can be obtained from randomization? The lack of formalism has caused some works [23, 11] to add insufficient amounts of noise (e.g., noise not calibrated to pre-noise sensitivity), making them vulnerable to attack [6]. On the contrary, [35] inserts randomness into every layer of the DNN. Our work shows that adding the right amount of calibrated noise at a single layer is sufficient to leverage DP’s post-processing guarantee and carry the bounds through the end of the network.

Our paper formalizes randomization-based defenses using DP theory, helping answer many of these design questions. Our formalism also lets us reason about the guarantees obtained through randomization and elevates randomization-based approaches from the class of best-effort defenses to that of certified defenses.

**Certified Defenses and Robustness Evaluations:** PixelDP offers two functions: (1) a strategy for learning robust models and (2) a method for evaluating the robustness of these models against adversarial examples. Both of these approaches have been explored in the literature. First, several studies have proposed methods for learning robust models, and second, various techniques have been developed to evaluate the robustness of these models.