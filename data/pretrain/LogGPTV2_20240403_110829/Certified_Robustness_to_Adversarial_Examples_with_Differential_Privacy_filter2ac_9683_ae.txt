approach. The L = 0.1 network has a certiﬁed accuracy of
59% for attacks smaller than 0.09 in 2-norm. The L = 0.3
network has a certiﬁed accuracy of 40% to attacks up to
size 0.2. To our knowledge, PixelDP is the ﬁrst defense to
yield DNNs with certiﬁed bounds on accuracy under 2-norm
attacks on datasets of ImageNet’s size and for large networks
like Inception.
Second, PixelDP networks constructed for larger attacks
(higher L, hence higher noise) tend to yield higher certiﬁed
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
(a) ImageNet Certiﬁed Accuracy
(b) CIFAR-10 Certiﬁed Accuracy
Fig. 2: Certiﬁed accuracy, varying the construction attack bound (L) and prediction robustness threshold (T ), on ImageNet
auto-encoder/Inception and CIFAR-10 ResNet, 2-norm bounds. Robust accuracy at high Robustness thresholds (high T ) increases with
high-noise networks (high L). Low noise networks are both more accurate and more certiﬁably robust for low T .
accuracy for high thresholds T . For example, the ResNet on
CIFAR-10 (see Fig. 2(b)) constructed with L = 0.03 has
the highest robust accuracy up to T = 0.03, but the ResNet
constructed with L = 0.1 becomes better past that threshold.
Similarly, the L = 0.3 ResNet has higher robust accuracy
than the L = 0.1 ResNet above the 0.14 2-norm prediction
robustness threshold.
We ran the same experiments on SVHN, CIFAR-100 and
MNIST models but omit the graphs for space reasons. Our
main conclusion – that adding more noise (higher L) hurts
both conventional and low T certiﬁed accuracy, but enhances
the quality of its high T predictions – holds in all cases.
Appendix B discusses the impact of some design choices
on robust accuracy, and Appendix D discusses PixelDP
guarantees as compared with previous certiﬁed defenses for
∞-norm attacks. While PixelDP does not yet yield strong
∞-norm bounds, it provides meaningful certiﬁed accuracy
bounds for 2-norm attacks, including on much larger and
more complex datasets and networks than those supported
by previous approaches.
D. Accuracy Under Attack (Q3)
A standard method to evaluate the strength of a defense is
to measure the conventional accuracy of a defended model
on malicious samples obtained by running a state-of-the-
art attack against samples in a held-out testing set [37].
We apply this method to answer three aspects of question
Q3: (1) Can PixelDP help defend complex models on large
datasets in practice? (2) How does PixelDP’s accuracy
under attack compare to state-of-the-art defenses? (3) How
does the accuracy under attack change for certiﬁed predic-
tions?
Accuracy under Attack on ImageNet. We ﬁrst study
conventional accuracy under attack for PixelDP models on
ImageNet. Fig. 3 shows this metric for 2-norm attacks on
the baseline Inception-v3 model, as well as three defended
versions, with a stacked PixelDP auto-encoder trained with
construction attack bound L ∈ {0.1, 0.3, 1.0}. PixelDP
makes the model signiﬁcantly more robust to attacks. For
attacks of size Lattack = 0.5, the baseline model’s accuracy
Fig. 3: Accuracy under attack on ImageNet. For the Ima-
geNet auto-encoder plus Inception-v3, L ∈ {0.1, 0.3, 1.0} 2-
norm attacks. The PixelDP auto-encoder increases the robustness
of Inception against 2-norm attacks.
drops to 11%, whereas the L = 0.1 PixelDP model’s
accuracy remains above 60%. At Lattack = 1.5, the baseline
model has an accuracy of 0, but the L = 0.1 PixelDP is still
at 30%, while the L = 0.3 PixelDP model have more that
39% accuracy.
Accuracy under Attack Compared to Madry. Fig. 4(a)
compares conventional accuracy of a PixelDP model to that
of a Madry model on CIFAR-10, as the empirical attack
bound increases for 2-norm attacks. For 2-norm attacks,
our model achieves conventional accuracy on par with, or
slightly higher than, that of the Madry model. Both models
are dramatically more robust under this attack compared
to the baseline (undefended) model. For ∞-norm attacks
our model does not fare as well, which is expected as the
PixelDP model is trained to defend against 2-norm attacks,
while the Madry model is optimized for ∞-norm attacks. For
Lattack = 0.01, PixelDP’s accuracy is 69%, 8 percentage
points lower than Madry’s. The gap increases until PixelDP
arrives at 0 accuracy for Lattack = 0.06, with Madry still
having 22%. Appendix §D details this evaluation.
Accuracy under Attack Compared to RobustOpt.
Fig. 4(b) shows a similar comparison with the RobustOpt de-
fense [65], which provides certiﬁed accuracy bounds for ∞-
norm attacks. We use the SVHN dataset for the comparison
as the RobustOpt defense has not yet been applied to larger
(cid:23)(cid:23)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
(b) SVHN
Fig. 4: Accuracy under 2-norm attack for PixelDP vs. Madry and RobustOpt, CIFAR-10 and SVHN. For 2-norm attacks, PixelDP
is on par with Madry until Lattack ≥ 1.2; RobustOpt support only small models, and has lower accuracy.
(a) CIFAR-10
predictions are, although it makes fewer of them (Certiﬁed
Fraction lines).
Thus, for applications that can afford to not act on a
minority of the predictions, PixelDP’s robust predictions
under 2-norm attack are substantially more precise than
Madry’s. For applications that need to act on every predic-
tion, PixelDP offers on-par accuracy under 2-norm attack to
Madry’s. Interestingly, although our defense is trained for
2-norm attacks, the ﬁrst conclusion still holds for ∞-norm
attacks; the second (as we saw) does not.
E. Computational Overhead (Q4)
Q4: What
is PixelDP’s computational overhead? We
evaluate overheads for training and prediction. PixelDP adds
little overhead for training, as the only additions are a
random noise tensor and sensitivity computations. On our
GPU, the CIFAR-10 ResNet baseline takes on average 0.65s
per training step. PixelDP versions take at most 0.66s per
training step (1.5% overhead). This represents a signiﬁcant
beneﬁt over adversarial training (e.g. Madry) that requires
ﬁnding good adversarial attacks for each image in the mini-
batch at each gradient step, and over robust optimization
(e.g. RobustOpt) that requires solving a constrained opti-
mization problem at each gradient step. The low training
overhead is instrumental to our support of large models and
datasets.
PixelDP impacts prediction more substantially, since it
uses multiple noise draws to estimate the label scores.
Making a prediction for a single image with 1 noise draw
takes 0.01s on average. Making 10 draws brings it only
to 0.02s, but 100 requires 0.13s, and 1000, 1.23s. It is
possible to use Hoeffding’s inequality [25] to bound the
number of draws necessary to distinguish the highest score
with probability at least η, given the difference between
the top two scores ymax − ysecond−max. Empirically, we
found that 300 draws were typically necessary to properly
certify a prediction, implying a prediction time of 0.42s
seconds, a 42× overhead. This is parallelizable, but resource
consumption is still substantial. To make simple predictions
– distinguish the top label when we must make a prediction
on all inputs – 25 draws are enough in practice, reducing
Fig. 5: PixelDP certiﬁed predictions vs. Madry accuracy, under
attack, CIFAR-10 ResNets, 2-norm attack. PixelDP makes fewer
but more correct predictions up to Lattack = 1.0.
datasets. Due to our support of larger DNN (ResNet), Pix-
elDP starts with higher accuracy, which it maintains under 2-
norm attacks. For attacks of Lattack = 0.5, RobustOpt is bel-
low 20% accuracy, and PixelDP above 55%. Under ∞-norm
attacks, the behavior is different: PixelDP has the advantage
up to Lattack = 0.015 (58.8% to 57.1%), and RobustOpt is
better thereafter. For instance, at Lattack = 0.03, PixelDP
has 22.8% accuracy, to RobustOpt’s 32.7%. Appendix §D
details the ∞-norm attack evaluation.
Precision on Certiﬁed Predictions Under Attack. Another
interesting feature of PixelDP is its ability to make certiﬁ-
ably robust predictions. We compute the accuracy of these
certiﬁed predictions under attack – which we term robust
precision – and compare them to predictions of the Madry
network that do not provide such a certiﬁcation. Fig. 5 shows
the results of considering only predictions with a certiﬁed
robustness above 0.05 and 0.1. It reﬂects the beneﬁt to
be gained by applications that can leverage our theoretical
guarantees to ﬁlter out non-robust predictions. We observe
that PixelDP’s robust predictions are substantially more
correct than Madry’s predictions up to an empirical attack
bound of 1.1. For T = 0.05 PixelDP’s robust predictions are
93.9% accurate, and up to 10 percentage points more correct
under attack for Lattack ≤ 1.1. A robust prediction is given
for above 60% of the data points. The more conservative
the robustness test is (higher T ), the more correct PixelDP’s
(cid:23)(cid:23)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
the overhead to 3×.
V. Analysis
We make three points about PixelDP’s guarantees and
applicability. First, we emphasize that our Monte Carlo
approximation of the function x (cid:15)→ E(A(x)) is not intended
to be a DP procedure. Hence, there is no need to apply
composition rules from DP, because we do not need this
randomized procedure to be DP. Rather, the Monte Carlo
approximation x (cid:15)→ ˆE(A(x)) is just that: an approximation
to a function x (cid:15)→ E(A(x)) whose robustness guarantees
come from Lemma 1. The function x (cid:15)→ ˆE(A(x)) does not
satisfy DP, but because we can control the Monte Carlo
estimation error using standard tools from probability theory,
it is also robust to small changes in the input, just like
x (cid:15)→ E(A(x)).
Second, Proposition 1 is not a high probability result; it
is valid with probability 1 even when A is (, δ > 0)-DP.
The δ parameter can be thought of as a “failure probability”
of an (, δ)-DP mechanism: a chance that a small change
in input will cause a big change in the probability of some
of its outputs. However, since we know that Ak(x) ∈ [0, 1],
the worst-case impact of such failures on the expectation
of the output of the (, δ)-DP mechanism is at most δ, as
proven in Lemma 1. Proposition 1 explicitly accounts for
this worst-case impact (term (1 + e)δ in Equation (4)).
Were we able to compute E(A(x)) analytically, PixelDP
would output deterministic robustness certiﬁcates. In prac-
tice however, the exact value is too complex to compute,
and hence we approximate it using a Monte Carlo method.
This adds probabilistic measurement error bounds, making
the ﬁnal certiﬁcation (Proposition 2) a high probability
result. However, the uncertainty comes exclusively from the
Monte Carlo integration – and can be made arbitrarily small
with more runs of the PixelDP DNN – and not from the
underlying (, δ)-DP mechanism A. Making the uncertainty
small gives an adversary a small chance to fool a PixelDP
network into thinking that its prediction is robust when it is
not. The only ways an attacker can increase that chance is
by either submitting the same attack payload many times or
gaining control over PixelDP’s source of randomness.
Third, PixelDP applies to any task for which we can
measure changes to input
in a meaningful p-norm, and
bound the sensitivity to such changes at a given layer
in the DNN (e.g. sensitivity to a bounded change in a
word frequency vector, or a change of class for categorical
attributes). PixelDP also applies to multiclass classiﬁcation
where the prediction procedure returns several top-scoring
labels. Finally, Lemma 1 can be extended to apply to DP
mechanism with (bounded) output that can also be negative,
as shown in Appendix E. PixelDP thus directly applies
to DNNs for regression tasks (i.e. predicting a real value
instead of a category) as long as the output is bounded (or
unbounded if δ = 0). The output can be bounded due to the
speciﬁc task, or by truncating the results to a large range of
values and using a comparatively small δ.
VI. Related Work
Our work relates to a signiﬁcant body of work in adver-
sarial examples and beyond. Our main contribution to this
space is to introduce a new and very different direction for
building certiﬁed defenses. Previous attempts have built on
robust optimization theory. In PixelDP we propose a new
approach built on differential privacy theory which exhibits
a level of ﬂexibility, broad applicability, and scalability that
exceeds what robust optimization-based certiﬁed defenses
have demonstrated. While the most promising way to de-
fend against adversarial examples is still an open question,
we observe undebatable beneﬁts unique to our DP based
approach, such as the post-processing guarantee of our
defense. In particular, the ability to prepend a defense to
unmodiﬁed networks via a PixelDP auto-encoder, as we did
to defend Inception with no structural changes, is unique
among certiﬁed (and best-effort) defenses.
Best-effort Defenses. Defenders have used multiple heuris-
tics to empirically increase DNNs’ robustness. These de-
fenses include model distillation [45], automated detection
of adversarial examples [24], [42], [41], application of
various input transformations [29], [10], randomization [23],
[11], and generative models [51], [27], [68]. Most of these
defenses have been broken, sometimes months after their
publication [7], [6], [2].
The main empirical defense that still holds is Madry et
al. [37], based on adversarial training [21]. Madry et al.
motivate their approach with robust optimization, a rigorous
theory. However not all the assumptions are met, as this
approach runs a best-effort attack on each image in the
minibatch at each gradient step, when the theory requires
ﬁnding the best possible adversarial attack. And indeed,
ﬁnding this worst case adversarial example for ReLU DNNs,
used in [37], was proven to be NP-hard in [53]. Therefore,
while this defense works well
it gives no
theoretical guarantees for individual predictions or for the
model’s accuracy under attack. PixelDP leverages DP theory
to provide guarantees of robustness to arbitrary, norm-based
attacks for individual predictions.
in practice,
Randomization-based defenses are closest in method to
our work [23], [11], [35]. For example, Liu et al. [35]
randomizes the entire DNN and predicts using an ensemble
of multiple copies of the DNN, essentially using draws
to roughly estimate the expected arg max prediction. They
observe empirically that randomization smoothens the pre-
diction function, improving robustness to adversarial ex-
amples. However, randomization-based prior work provides
limited formalism that is insufﬁcient to answer important
defense design questions: where to add noise,
in what
quantities, and what formal guarantees can be obtained from
randomization? The lack of formalism has caused some
(cid:23)(cid:23)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:14 UTC from IEEE Xplore.  Restrictions apply. 
works [23], [11] to add insufﬁcient amounts of noise (e.g.,
noise not calibrated to pre-noise sensitivity), which makes
them vulnerable to attack [6]. On the contrary, [35] inserts
randomness into every layer of the DNN: our work shows
that adding the right amount of calibrated noise at a single
layer is sufﬁcient to leverage DP’s post-processing guarantee
and carry the bounds through the end of the network.
Our paper formalizes randomization-based defenses using
DP theory, and in doing so helps answer many of these
design questions. Our formalism also lets us reason about
the guarantees obtained through randomization and enables
us to elevate randomization-based approaches from the class
of best-effort defenses to that of certiﬁed defenses.
Certiﬁed Defenses and Robustness Evaluations. PixelDP
offers two functions: (1) a strategy for learning robust
models and (2) a method for evaluating the robustness of
these models against adversarial examples. Both of these
approaches have been explored in the literature. First, several