sequentially composes the combined policies of all participants:
SDX = (PA’’ + PB’’ + PC’’) >> (PA’’ + PB’’ + PC’’)
When the SDX applies this policy, any packet that enters the SDX
fabric either reaches the physical port of another participant or is
dropped. In any case, the resulting forwarding policy within the
fabric will never have loops. Taking BGP policies into account also
prevent forwarding loops between edge routers. The SDX enforces
two BGP-related invariants to prevent forwarding loops between
edge routers. First, a participant router can only receive trafﬁc
destined to an IP preﬁx for which it has announced a corresponding
BGP route. Second, if a participant router announces a BGP route
for an IP preﬁx p, it will never forward trafﬁc destined to p back to
the SDX fabric.
Finally, the SDX runtime relies on the underlying Pyretic runtime
to translate the SDX policy to the forwarding rules to install in the
physical switch. More generally, the SDX may consist of multiple
physical switches, each connected to a subset of the participants.
Fortunately, we can rely on Pyretic’s existing support for topology
abstraction to combine a policy written for a single SDX switch
with another policy for routing across multiple physical switches, to
generate the forwarding rules for multiple physical switches.
4.2 Reducing Data-Plane State
Augmenting each participant’s policy with the BGP-learned preﬁxes
could cause an explosion in the size of the ﬁnal policy. Today’s
global routing system has more than 500,000 IPv4 preﬁxes (and
growing!), and large IXPs host several hundred participants (e.g.,
AMS-IX has more than 600). The participants may have different
policies, directing trafﬁc to different forwarding neighbors. More-
over, composing these policies might also generate a “cross-product”
of their predicates if the participants’ policies match on different
ﬁelds. For instance, in Figure 1a, AS A matches on dstport, and
B on srcip. As a result, a naive compilation algorithm could easily
lead to millions of forwarding rules, while even the most high-end
SDN switch hardware can barely hold half a million rules [13].
Existing layer-two IXPs do not face such challenges because they
forward packets based only on the destination MAC address, rather
than the IP and TCP/UDP header ﬁelds. To minimize the number of
rules in the SDX switch, the SDX (1) groups preﬁxes with the same
forwarding behavior into an equivalence class and (2) implicitly tags
the packets sent by each participant’s border router using a virtual
MAC address. This technique substantially reduces the number of
forwarding rules, and works with unmodiﬁed BGP routers.
Grouping preﬁxes into equivalence classes. Fortunately, a partic-
ipant’s policy would typically treat a large number of IP preﬁxes the
same way. For instance, in Figure 1, AS A has the same forwarding
behavior for p1 and p2 (i.e., send Web trafﬁc via AS B, and send
555Figure 2: Multi-stage FIB for each participant, where the ﬁrst stage corre-
sponds to the participant’s border router and the second stage corresponds
to the participant’s virtual switch at the SDX.
the rest via AS C). By grouping p1 and p2, we could implement
the policy with only two forwarding rules, directing trafﬁc to AS B
and C, instead of the four currently required. We say that p1 and p2
belong to the same Forwarding Equivalence Class (FEC). An FEC is
a set of IP preﬁxes that share the same forwarding behavior through-
out the SDX fabric. Ideally, we would install the minimum set of
forwarding rules for each FEC, which is equivalent to the number
of forwarding actions associated with the FEC. Doing so requires
a new way to combine preﬁxes; conventional IP preﬁx aggregation
does not work because preﬁxes p1 and p2 might not be contiguous
IP address blocks.
Ofﬂoading tagging to the participants’ border routers. To group
non-adjacent preﬁxes belonging to the same FEC, we introduce the
abstraction of a multi-stage Forwarding Information Base (FIB)
for each participant, as shown in Figure 2. The ﬁrst table matches
on the destination IP preﬁx and tags packets with the associated
FEC. Then, a second table simply matches on the tag and performs
the forwarding actions associated with the FEC. Using a multi-
staged FIB substantially reduces the number of rules in the second
table. The ﬁrst table remains quite large because of the many IP
preﬁxes. To address this challenge, we implement the ﬁrst table
using the participant’s own border router. Each border router already
maintains a forwarding table with an entry for each destination
preﬁx, so we can realize our abstraction without any additional table
space! Still, we need (1) a data-plane mechanism for tagging the
packets and (2) a control-plane mechanism for the SDX to instruct
the border router about which tag to use for each preﬁx. Ideally,
the solution to both problems would be completely transparent to
the participants, rather than requiring them to run or conﬁgure an
additional protocol (e.g., MPLS) for this purpose.
Using the MAC address as data-plane tag and the BGP next-
hop IP address for control-plane signaling. The SDX runtime
capitalizes on how BGP-speaking routers compute forwarding-table
entries. Upon choosing a BGP route for a preﬁx p, a router (1) ex-
tracts the next-hop IP address from the BGP route announcement,
(2) consults its ARP table to translate the IP address to the corre-
sponding MAC address, and (3) installs a forwarding-table entry that
sets the destination MAC address before directing the packet to the
output port. Usually, this MAC address corresponds to the physical
address of the next-hop interface. In the SDX though, we have the
MAC address correspond to a virtual MAC address (VMAC)—the
tag—which identiﬁes the FEC for preﬁx p. The SDX fabric can
then just match on the VMAC and perform the forwarding actions
associated with the FEC. We refer to the BGP next-hop IP address
sent to the border router as the Virtual Next-Hop (VNH). Finally,
observe that we can assign the same VNH (and, hence, the same
VMAC) to disjoint IP preﬁxes—the address blocks need not be
contiguous.
In practice, the SDX runtime ﬁrst pre-computes the FEC accord-
ing to participant policies and assigns a distinct (VNH, VMAC) pair
to each of them. It then transforms the SDX policies to match on the
VMAC instead of the destination preﬁxes. Finally, it instructs the
SDX route server to set the next-hop IP address (VNH) in the BGP
messages and directs its own ARP server to respond to requests for
the VNH IP address with the corresponding VMAC.
Computing the virtual next hops. Computing the virtual next-hop
IP addresses requires identifying all groups of preﬁxes that share the
same forwarding behavior, considering both default BGP forwarding
and speciﬁc SDX policies. To ensure optimality, we want the groups
of preﬁxes to be of maximal size; in other words, any two preﬁxes
sharing the same behavior should always belong to the same group.
The SDX runtime computes the FECs in three passes.
In the ﬁrst pass, the SDX runtime extracts the groups of IP preﬁxes
for which the default behavior is affected in the same way by at
least one SDX outbound policy. Figure 1 shows that the group
{p1, p2, p3} has its default behavior overridden by AS A’s outbound
policies, which forward its Web trafﬁc to AS B. Similarly, the
group {p1, p2, p3, p4} has its default behavior overridden by AS
A’s outbound policies, which forward its HTTPS trafﬁc to AS C. All
of the preﬁxes except p5 have their default behavior overridden.
third pass,
from the ﬁrst
the SDX runtime
two passes
It
In the
In the second pass, the SDX runtime groups all the preﬁxes that
had their default behavior overridden according to the default next-
hop selected by the route server. In the previous example, preﬁxes
p1, p2, p3, p4 will be divided into two groups: {p1, p2, p4} whose
default next-hop is C and {p3} whose default next-hop is B.
combines
the
into one group C =
groups
{{p1, p2, p3},{p1, p2, p3, p4},{p1, p2, p4},{p3}}}.
then
computes C(cid:48) such that each element of C(cid:48) is the largest possible
subset of elements of C with a non-empty intersection.
In the
example above, C(cid:48) = {{p1, p2},{p3},{p4}} and is the only valid
solution. Intuitively, n preﬁxes belonging to the same group Ci ∈ C
either always appear altogether in a policy P, or do not appear
at all—they share the same forwarding behavior. We omit the
description of a polynomial-time algorithm that computes the
Minimum Disjoint Subset (MDS).
Finally, observe that we do not need to consider BGP preﬁxes
that retain their default behavior, such as p5 in Figure 1. For these
preﬁxes, the SDX runtime does not have to do any processing and
simply behaves like a normal route server, which transmits BGP
announcements with the next-hop IP address unchanged.
4.3 Reducing Control-Plane Computation
In this section, we describe how to reduce the time required for
control-plane computation. Many of these operations have a default
computation time that is exponential in the number of participants
and thus does not scale as the number of participants grows. At a
high level, the control plane performs three computation-intensive
operations: (1) computing the VNHs; (2) augmenting participants’
SDX policies; and (3) compiling the policies into forwarding rules.
The controller performs these operations both during initialization
and whenever SDX’s operational state changes. We focus primarily
on optimizing policy compilation, as this step is the most computa-
tionally intensive. We ﬁrst describe optimizations that accelerate the
initial computation. We then describe optimizations that accelerate
incremental computation in response to updates (i.e., due to changes
in the available BGP routes or the SDX policies). We describe each
optimization along with the insight that enables it.
!"#$%&’()%*+%(’,-$./+(01!"!#!$!%!&’()*"+’()*#+’()*$+’()*%+5564.3.1 Optimizing initial compilation
SDX compilation requires composing the policies of every partici-
pant AS with every other participant’s policy using a combination of
sequential and parallel composition. Performing such compositions
is time-consuming, as it requires inspecting each pair of policies in-
volved to identify overlaps. As illustration, consider the ﬁnal policy
computed in Section 3, without considering default forwarding (for
simplicity):
policy_composed =
(PA’’ + PB’’ + PC’’) >> (PA’’ + PB’’ + PC’’)
Since the parallel-composition operator is distributive, the compiler
can translate the policy into many pairs of sequential composition,
combined together using parallel composition. Removing terms that
apply the same policy in succession (i.e., PA’’ >> PA’’) yields:
policy_composed =
((PA’’ >> PB’’)+(PA’’ >> PC’’))+
((PB’’ >> PA’’)+(PB’’ >> PC’’))+
((PC’’ >> PA’’)+(PC’’ >> PB’’))
Compiling this policy requires executing eleven composition
operations—six sequential (two per line) and ﬁve in parallel—to
combine the intermediate results together. Fortunately, a lot of these
sequential and parallel composition can be avoided by exploiting
three observations: (1) participant policies tend to involve only a sub-
set of the participants; (2) participant policies are disjoint by design;
and (3) many policy idioms appear multiple times in the ﬁnal policy.
The ﬁrst observation reduces the number of sequential composition
operations, and the second reduces the number of parallel composi-
tion operations. The third observation prevents compilation of the
same policy more than once. With these optimizations, the SDX can
achieve policy compilation with only three sequential compositions
and no parallel compositions.
Most SDX policies only concern a subset of the participants. In
the IXP trafﬁc patterns we observe, a few IXP participants carry
most of the trafﬁc. Previous work has shown that about 95% of all
IXP trafﬁc is exchanged between about 5% of the participants [1].
We thus assume that most SDX policies involve these few large
networks rather than all of the IXP participants. The SDX controller
avoids all unnecessary compositions by only composing policies
among participants that exchange trafﬁc. In this example, AS B
has no outbound policy, so compositions (PB’’ >> PA’’) and
(PB’’ >> PC’’) are unnecessary. The same reasoning applies for
AS C. The SDX controller therefore reduces the policy as follows:
policy_composed =
(PA’’ >> PB’’) + (PA’’ >> PC’’) + (PC’’ >> PB’’)
which only involves three sequential composition operations.
Most SDX policies are disjoint. Parallel composition is a costly
operation that should be used only for combining policies that apply
to overlapping ﬂow space. For policies that apply to disjoint ﬂow
spaces, the SDX controller can simply apply the policies indepen-
dently, as no packet ever matches both policies. The policies are
disjoint by design because they differ with respect to the virtual
switch and port after the ﬁrst syntactic transformation (i.e., isola-
tion). Also, the same observation applies within the policies of a
single participant. We assume that the vast majority of participants
would write unicast policies in which each packet is forwarded to
one other participant. We do not prevent participants from express-
ing multicast policies, but we optimize for the common case. As
a result, SDX policies that forward to different participants always
collector peers/total peers
preﬁxes
BGP updates
preﬁxes seeing updates
LINX
71/496
503,392
16,658,819
12.67%
Table 1: IXP datasets. We use BGP update traces from RIPE collectors [16]
in the three largest IXPs—AMS-IX, DE-CIX, and LINX—for January 1–6,
2014, from which we discarded updates caused by BGP session resets [23].
AMS-IX
116/639
518,082
11,161,624
9.88%
DE-CIX
92/580
518,391
30,934,525
13.64%
differ with respect to the forwarding port and are also disjoint by
construction.
Returning to the previous example, none of the parallel compo-
sitions between (PA’’ >> PC’’), (PA’’ >> PC’’), and (PC’’
>> PB’’) are necessary, since each of them always applies on
strictly disjoint portions of the ﬂow space.
Many policy idioms appear more than once in the global policy.
The reuse of various policy idioms results from the fact that partic-