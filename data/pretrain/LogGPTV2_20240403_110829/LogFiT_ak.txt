### Anomaly Detection Precision (P), Recall (R), F1 Score (F), and Specificity (S) of Different Methods on the BGL Dataset

Anomalies in the dataset are genuine. Le and Zhang [7] have noted that a high specificity can help mitigate the impact of an imbalanced class distribution on the model's overall performance.

#### B. Experimental Results

**Log Anomaly Detection Performance:**

Tables II, III, and IV present the results of anomaly detection inference using LogFiT, compared to the results from running DeepLog and LogBERT using their available source code implementations. The results indicate that LogFiT’s F1 scores exceed those of LogBERT and DeepLog across all three datasets. Additionally, LogFiT’s specificity surpasses that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.

**Figure 5: LogFiT’s Anomaly Decision Method**

Figure 5 illustrates LogFiT’s anomaly decision method as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. Unlike the original BERT-based masking, LogFiT selects sentences for masking instead of individual tokens.

| Method | P | R | F1 | S |
| --- | --- | --- | --- | --- |
| DeepLog | 65.05 | 99.4 | 89.30 | 98.28 |
| LogBERT | 91.75 | 95.7 | 93.69 | 97.78 |

In summary, LogFiT demonstrates superior performance in terms of F1 scores and specificity, making it a robust choice for log anomaly detection, especially in the presence of imbalanced class distributions.