ANOMALY DETECTION PRECISION (P), RECALL (R), F1 SCORE (F) AND SPECIFICITY (S) OF DIFFERENT METHODS ON THE BGL DATASET.
anomalies are genuine. Furthermore, Le and Zhang [7] have noted that a high Specificity can help mitigate the impact of having an imbalanced class distribution on the model’s overall performance.B. Experimental Results
| Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking | Log Anomaly Detection Performance. Table II, Table III and Table IV show the results of running anomaly detection inference using LogFiT, as compared to the results from running DeepLog and LogBERT using the available source code implementation. The results show that LogFiT’s F1 scores exceed that of LogBERT and DeepLog on all three datasets, while LogFiT’s specificity exceed that of the baseline models on the HDFS and BGL datasets and is very close to LogBERT’s on the Thunderbird dataset. The DeepLog and LogBERT models were trained and evaluated using the source code implementation mentioned earlier.Figure 5 illustrates LogFiT’s anomaly decision method, as applied to a Thunderbird log paragraph. The input log paragraph is first corrupted using a BERT-based masking scheme. In contrast to the original BERT-based masking |
|---|---|---|---|---|---|---|---|---|
| scheme, |LogFiT |selects |selects |sentences |sentences |for |masking |instead |
| Method |Method |P |P |R |R | |S |S || Method |Method |P |P |R |R | |S |S |
| Method |Method |P |P |R |R |F1 |S |S |
| DeepLog |DeepLog |DeepLog |65.05 |65.05 |99.4 |78.64 |89.30  98.28  97.78 |89.30  98.28  97.78 |
| LogBERT |LogBERT |LogBERT |91.75 |91.75 |95.7 |93.69 |89.30  98.28  97.78 |89.30  98.28  97.78 |