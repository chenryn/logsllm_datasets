title:Locally Private Graph Neural Networks
author:Sina Sajadmanesh and
Daniel Gatica-Perez
Locally Private Graph Neural Networks
Sina Sajadmanesh
PI:EMAIL
Idiap Research Institute
EPFL
ABSTRACT
Graph Neural Networks (GNNs) have demonstrated superior perfor-
mance in learning node representations for various graph inference
tasks. However, learning over graph data can raise privacy con-
cerns when nodes represent people or human-related variables
that involve sensitive or personal information. In this paper, we
study the problem of node data privacy, where graph nodes (e.g.,
social network users) have potentially sensitive data that is kept
private, but they could be beneficial for a central server for train-
ing a GNN over the graph. To address this problem, we propose
a privacy-preserving, architecture-agnostic GNN learning frame-
work with formal privacy guarantees based on Local Differential
Privacy (LDP). Specifically, we develop a locally private mechanism
to perturb and compress node features, which the server can effi-
ciently collect to approximate the GNN’s neighborhood aggregation
step. Furthermore, to improve the accuracy of the estimation, we
prepend to the GNN a denoising layer, called KProp, which is based
on the multi-hop aggregation of node features. Finally, we propose
a robust algorithm for learning with privatized noisy labels, where
we again benefit from KProp’s denoising capability to increase the
accuracy of label inference for node classification. Extensive ex-
periments conducted over real-world datasets demonstrate that
our method can maintain a satisfying level of accuracy with low
privacy loss.
CCS CONCEPTS
• Computing methodologies → Neural networks; • Security
and privacy → Data anonymization and sanitization; Privacy
protections.
KEYWORDS
Differential Privacy; Private Learning; Graph Neural Networks;
Node Classification
ACM Reference Format:
Sina Sajadmanesh and Daniel Gatica-Perez. 2021. Locally Private Graph
Neural Networks. In Proceedings of the 2021 ACM SIGSAC Conference on
Computer and Communications Security (CCS ’21), November 15–19, 2021,
Virtual Event, Republic of Korea. ACM, New York, NY, USA, 16 pages. https:
//doi.org/10.1145/3460120.3484565
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484565
Daniel Gatica-Perez
EPFL
PI:EMAIL
Idiap Research Institute
1 INTRODUCTION
In the past few years, extending deep learning models for graph-
structured data has attracted growing interest, popularizing the
concept of Graph Neural Networks (GNNs) [44]. GNNs have shown
superior performance in a wide range of applications in social
sciences [18], biology [41], molecular chemistry [14], and so on,
achieving state-of-the-art results in various graph-based learning
tasks, such as node classification [26], link prediction [65], and
community detection [9]. However, most real-world graphs asso-
ciated with people or human-related activities, such as social and
economic networks, are often sensitive and might contain personal
information. For example in a social network, a user’s friend list,
profile information, likes and comments, etc., could potentially be
private to the user. To satisfy users’ privacy expectations in ac-
cordance with recent legal data protection policies, it is of great
importance to develop privacy-preserving GNN models for appli-
cations that rely on graphs accessing users’ personal data.
Problem and motivation. In light of these privacy constraints,
we define the problem of node data privacy. As illustrated in Fig-
ure 1, in this setting, graph nodes, which may represent human
users, have potentially sensitive data in the form of feature vec-
tors and possibly labels that are kept private, but the topology
of the graph is observable from the viewpoint of a central server,
whose goal is to benefit from private node data to learn a GNN
over the graph. This problem has many applications in social net-
work analysis and mobile computing. For example, consider a social
smartphone application server, e.g., a social network, messaging
platform, or a dating app. As this server already has the data about
social interactions between its users, the graph topology is not
private to the server. However, the server could potentially benefit
from users’ personal information, such as their phone’s sensor data,
list of installed apps, or application usage logs, by training a GNN
using these private features to learn better user representations for
improving its services (e.g., the recommendation system). Without
any means of data protection, however, this implies that the server
should collect users’ personal data directly, which can raise privacy
concerns.
Challenges. Training a GNN from private node data is a chal-
lenging task, mainly due to the relational nature of graphs. Unlike
other deep learning models wherein the training data points are
independent, in GNNs, the samples – nodes of the graph – are con-
nected via links and exchange information through the message-
passing framework during training [19]. This fact renders common
collaborative learning paradigms, such as federated learning [23],
infeasible due to their excessive communication overhead. The
main reason is that in the absence of a trusted server, which is the
primary assumption of our paper, every adjacent pair of nodes has
to exchange their vector representations with each other multiple
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2130form, being features or labels, neither for training nor validation
and hyper-parameter optimization.
Finally, we derive the theoretical properties of the proposed al-
gorithms, including the formal privacy guarantees and error bound.
We conduct extensive experiments over several real-world datasets,
which demonstrate that our proposed LPGNN is robust against
injected LDP noise, achieving a decent accuracy-privacy trade-off
in the presence of noisy features and labels.
Paper organization. The rest of this paper is organized as fol-
lows. In Section 2, we formally define the problem and provide the
necessary backgrounds. Then, in Section 3, we explain our locally
private GNN training algorithm. Details of experiments and their
results are explained in Section 4. We review related work in Sec-
tion 5 and finally in Section 6, we conclude the paper. The proofs
of all the theoretical findings are also presented in Appendix A.
2 PRELIMINARIES
Problem definition. We formally define the problem of learning
a GNN with node data privacy. Consider a graph G = (V, E, X, Y),
where E is the link set and V = VL ∪ VU is the union of the set
of labeled nodes VL and unlabeled ones VU. The feature matrix
X ∈ R|V|×𝑑 comprises 𝑑-dimensional feature vectors x𝑣 for each
node 𝑣 ∈ V, and Y ∈ {0, 1}|V|×𝑐 is the label matrix, where 𝑐 is the
number of classes. For each node 𝑣 ∈ VL, y𝑣 is a one-hot vector,
i.e., y𝑣 · (cid:174)1 = 1, where (cid:174)1 is the all-one vector, and for each node
𝑣 ∈ VU, y𝑣 is the all-zero vector (cid:174)0. Now assume that a server has
access to V and E, but the feature matrix X and labels Y are private
to the nodes and thus not observable by the server. The problem
is: how can the server collaborate with the nodes to train a GNN
over G without letting private data leave the nodes? To answer this
question, we first present the required background about graph
neural networks and local differential privacy in the following, and
then in the next section, we describe our proposed method in detail.
Note that since in our problem setting, nodes of the graph usually
correspond to human users, we often use the terms “node” and
“user” interchangably throughout the rest of the paper.
Graph Neural Networks. A GNN learns a representation for
every node in the graph using a set of stacked graph convolution
layers. Each layer gets an initial vector for each node and outputs a
new embedding vector by aggregating the vectors of the adjacent
neighbors followed by a non-linear transformation. More formally,
given a graph G = (V, E, X), an 𝐿-layer GNN consists of 𝐿 graph
𝑣 of any node 𝑣 ∈ V at
convolution layers, where the embedding h𝑙
layer 𝑙 is generated by aggregating the previous layer’s embeddings
of its neighbors, called the neighborhood aggregation step, as:
(cid:16){h𝑙−1
𝑢 ,∀𝑢 ∈ N(𝑣)}(cid:17)
(cid:17)
(cid:16)
h𝑙N(𝑣) = Aggregate𝑙
(1)
𝑢
h𝑙N(𝑣)
𝑣 = Update𝑙
h𝑙
(2)
where N(𝑣) is the set of neighbors of 𝑣 (which could include 𝑣 itself)
and h𝑙−1
is the embedding of node 𝑢 at layer 𝑙 − 1. Aggregate𝑙 (.)
and h𝑙N(𝑣) are respectively the 𝑙-th layer differentiable, permutation
invariant aggregator function (such as mean, sum, or max) and
its output on N(𝑣). Finally, Update𝑙 (.) is a trainable non-linear
function, e.g., a neural network, for layer 𝑙. At the very first, we
Figure 1: The node data privacy problem. A cloud server (e.g.,
a social network server) has a graph (e.g., the social graph),
whose nodes, which may correspond to real users, have some
private data that the server wishes to utilize for training a
GNN on the graph, but cannot simply collect them due to
privacy constraints.
times during a single training epoch of a GNN, which requires
significantly more communication compared to conventional deep
neural networks, where the nodes only communicate with the
server, independently.
Contributions. In this paper, we propose the Locally Private
Graph Neural Network (LPGNN), a novel privacy-preserving GNN
learning framework for training GNN models using private node
data. Our method has provable privacy guarantees based on Local
Differential Privacy (LDP) [24], can be used when either or both
node features and labels are private, and can be combined with any
GNN architecture independently.
To protect the privacy of node features, we propose an LDP
mechanism, called the multi-bit mechanism, through which the
graph nodes can perturb their features that are then collected by
the server with minimum communication overhead. These noisy
features are then used to estimate the first graph convolution layer
of the GNN. Given that graph convolution layers initially aggregate
node features before passing them through non-linear activation
functions, we benefit from this aggregation step as a denoising
mechanism to average out the differentially private noise we have
injected into the node features. To further improve the effectiveness
of this denoising process and increase the estimation accuracy of
the graph convolution, we propose to prepend a simple yet effective
graph convolution layer based on the multi-hop aggregation of node
features, called KProp, to the backbone GNN.
To preserve the privacy of node labels, we perturb them using the
generalized randomized response mechanism [22]. However, learn-
ing with perturbed labels introduces extra challenges, as the label
noise could significantly degrade the generalization performance
of the GNN. To this end, we propose a robust training framework,
called Drop (label denoising with propagation), in which we again
benefit from KProp’s denoising capability to increase the accuracy
of noisy labels. Drop can be seamlessly combined with any GNN, is
very easy to train, and does not rely on any clean (raw) data in any
ServerPrivate Node DataSession 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2131have h0
𝑣 = xv, i.e., the initial embedding of 𝑣 is its feature vector x𝑣,
and the last layer generates a 𝑐-dimensional output followed by a
softmax layer to predict node labels in a 𝑐-class node classification
task.
Local Differential Privacy. Local differential privacy (LDP) is
an increasingly used approach for collecting private data and com-
puting statistical queries, such as mean, count, and histogram. It has
been already deployed by major technology companies, including
Google [16], Apple [46], and Microsoft [11]. The key idea behind
LDP is that data holders do not need to share their private data
with an untrusted data aggregator, but instead send a perturbed
version of their data, which is not meaningful individually but can
approximate the target query when aggregated. It includes two
steps: (i) data holders perturb their data using a special randomized
mechanism M and send the output to the aggregator; and (ii) the
aggregator combines all the received perturbed values and esti-
mates the target query. To prevent the aggregator from inferring
the original private value from the perturbed one, the mechanism
M must satisfy the following definition [24]:
Pr[M(𝑥) = 𝑦] ≤ 𝑒𝜖 Pr[M(𝑥′) = 𝑦]
Definition 2.1. Given 𝜖 > 0, a randomized mechanismM satisfies
𝜖-local differential privacy, if for all possible pairs of user’s private
data 𝑥 and 𝑥′, and for all possible outputs 𝑦 ∈ 𝑅𝑎𝑛𝑔𝑒(M), we have:
(3)
The parameter 𝜖 in the above definition is called the “privacy
budget” and is used to tune utility versus privacy: a smaller (resp.
larger) 𝜖 leads to stronger (resp. weaker) privacy guarantees, but
lower (resp. higher) utility. The above definition implies that the
mechanism M should assign similar probabilities (controlled by 𝜖)
to the outputs of different input values 𝑥 and 𝑥′, so that by looking
at the outputs, an adversary could not infer the input value with
high probability, regardless of any side knowledge they might have.
LDP is achieved for a deterministic function usually by adding a
special random noise to its output that cancels out when calculating
the target aggregation given a sufficiently large number of noisy
samples.
3 PROPOSED METHOD
In this section, we describe our proposed framework for learning a
GNN using private node data. As described in the previous section,
in the forward propagation of a GNN, the node features are only
used as the input to the first layer’s Aggregate function. This
aggregation step is amenable to privacy, as it allows us to perturb
node features using an LDP mechanism (e.g., by injecting random
noise into the features) and then let the Aggregate function av-
erage out the injected noise (to an extent, not entirely), yielding
a relatively good approximation of the neighborhood aggregation
for the subsequent Update function. The GNN’s forward propaga-
tion can then proceed from this point without any modification to
predict a class label for each node.
However, maintaining a proper balance between the accuracy
of the GNN and the privacy of data introduces new challenges that
need to be carefully addressed. On one hand, the node features
to be collected are likely high-dimensional, so the perturbation of
every single feature consumes a lot of the privacy budget. Suppose
we want to keep our total budget 𝜖 low to provide better privacy
protection. In that case, we need to perturb each of the 𝑑 features
with 𝜖/𝑑 budget (because the privacy budgets of the features add
up together as the result of the composition theorem [15]), which in
turn results in adding more noise to the data that can significantly
degrade the final accuracy. On the other hand, for the GNN to be
able to cancel out the injected noise, the first layer’s aggregator
function must: (i) be in the form of a linear summation, and (ii) be
calculated over a sufficiently large set of node features. However,
not every GNN architecture employs a linear aggregator function,
nor every node in the graph has many neighbors. In fact, in many
real-world graphs that follow a Power-Law degree distribution, the
number of low-degree nodes is much higher than the high-degree
ones. Consequently, the estimated aggregation would most likely
be very noisy, again leading to degraded performance.
To tackle the first challenge, we develop a multidimensional
LDP method, called the multi-bit mechanism, by extending the 1-
bit mechanism [11] for multidimensional feature collection. It is
composed of a user-side encoder and a server-side rectifier designed
for maximum communication efficiency. To address the second
challenge, we propose a simple, yet effective graph convolution
layer, called KProp, which aggregates messages from an expanded
neighborhood set that includes both the immediate neighbors and
those nodes that are up to 𝐾-hops away. By prepending this layer
to the GNN, we can both combine our method with any GNN
architecture and at the same time increase the graph convolution’s
estimation accuracy for low-degree nodes. In the experiments, we
show that this technique can significantly boost the performance of
our locally private GNN, especially for graphs with a lower average
degree.
Finally, since the node labels are also considered private, we need
another LDP mechanism to collect them privately. To this end, we
use the generalized randomized response algorithm [22], which
randomly flips the correct label to another one with a probability
that depends on the privacy budget. However, learning the GNN
with perturbed labels brings forward significant challenges in both
training and validation. Regarding the former, training the GNN
directly with the perturbed labels causes the model to overfit the
noisy labels, leading to poor generalization performance. Regarding
the latter, while it would be easy to validate the trained model using
clean (non-perturbed) data, due to the privacy constraints of our
problem, a more realistic setting is to assume that the server does
not have access to any clean validation data. In this case, it is not
clear how to perform model validation with noisy data, which is
vital to prevent overfitting and optimize model hyper-parameters.
Although deep learning with noisy labels has been studied ex-
tensively in the literature [31, 38, 39, 45, 63, 64, 67], almost all the
previous works either need clean features for training, require clean
data for validation, or have been proposed for standard deep neural
networks and do not consider the graph structure. Here, we pro-
pose Label Denoising with Propagation – Drop, which incorporates
the graph structure for label correction, and at the same time does
not rely on any form of clean data (features or labels), neither for
training nor validation. Given that nodes with similar labels tend
to connect together more often [48], we utilize the graph topology
to predict the label of a node by estimating the label frequency of
its neighboring nodes. Still, if we rely on immediate neighbors, the
true labels could not be accurately estimated due to insufficient
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2132Algorithm 1: Multi-Bit Encoder
Input
:feature vector x ∈ [𝛼, 𝛽]𝑑; privacy budget 𝜖 > 0; range
parameters 𝛼 and 𝛽; sampling parameter
𝑚 ∈ {1, 2, . . . , 𝑑}.
Output:encoded vector x∗ ∈ {−1, 0, 1}𝑑 .
1 Let S be a set of 𝑚 values drawn uniformly at random without
replacement from {1, 2, . . . , 𝑑}
2 for 𝑖 ∈ {1, 2, . . . , 𝑑} do
3
𝑡𝑖 ∼ Bernoulli(cid:16)
𝑠𝑖 = 1 if 𝑖 ∈ S otherwise 𝑠𝑖 = 0
𝑒𝜖/𝑚+1 + 𝑥𝑖−𝛼
1
4
𝛽−𝛼 · 𝑒𝜖/𝑚−1
𝑒𝜖/𝑚+1
(cid:17)
5
6 end
7 return x∗ = [𝑥∗
𝑖 = 𝑠𝑖 · (2𝑡𝑖 − 1)
𝑥∗
1, . . . , 𝑥∗
𝑑 ]𝑇
neighbors for many nodes. Again, our key idea is to exploit KProp’s
denoising capability, but this time on node labels, to estimate the
label frequency for each node and recovering the true label by
choosing the most frequent one. Drop can easily be combined by
any GNN architecture, and we show that it outperforms traditional
baselines, especially at high-privacy regimes.