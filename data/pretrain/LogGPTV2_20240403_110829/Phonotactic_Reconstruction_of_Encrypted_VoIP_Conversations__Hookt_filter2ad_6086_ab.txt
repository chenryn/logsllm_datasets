to their IPA representations for the latter stages of our
evaluation. In order to generate sequences of encoded frame
lengths from the (16kHz, single-channel) audio samples, we
encode each sample using the reference version of the Speex
3In addition to phonemes, the corpus contain some labels for sounds,
such as pauses and recording errors, unrelated to human speech.
encoder, instrumented to output the sizes of the encoded
frames, in wideband (i.e., 16kHz) VBR mode. The phonetic
labels from the time-aligned transcripts are then used to
identify subsequences corresponding to individual phonemes
for training; this encoding process gives us a number of
sequences for each phoneme.
We note that
the approach we take assumes that
the
adversary has access to (i) the sequence of packet lengths
for an encrypted VoIP call (ii) knowledge of the language
spoken in the call, (iii) representative example sequences
(or models derived therefrom) for each phoneme, and (iv) a
phonetic dictionary. The ﬁrst assumption can be readily met
through any number of means, including the use of a simple
packet sniffer. Knowledge of the language of interest can be
gleaned using the ideas in [32, 55] or by simple endpoint
analysis. Lastly, obtaining representative example sequences
for each phoneme is fairly straightforward: one can use
prerecorded, phonetically-labeled audio ﬁles as input to a
speech codec to produce the examples. In fact, using labeled
examples from prerecorded audio is exactly the approach
we take in this paper in order to model phonemes. Note that
our primary goal is to build speaker-independent models and
thus we do not require speaker-speciﬁc audio. Finally, pho-
netic dictionaries (e.g., CELEX, CMUdict and PRONLEX)
are readily available; we use data from TIMIT and from the
PRONLEX dictionary (containing pronunciations from over
90,000 words) as our phonetic dictionary.
IV. RELATED WORK
Trafﬁc analysis of encrypted network communications has
a long and rich history. Much of that work, however, is
focused on identifying the application protocol responsible
for a particular connection (e.g., [7, 12, 17, 31, 42, 43, 54]).
It was not until recently that researchers [10, 38, 48, 50, 52]
began exploring techniques for inferring sensitive informa-
tion within encrypted streams using only those features that
remain intact after encryption—namely packet sizes and
timing information. Song et al. [50], for example, used the
inter-arrival time between packets to infer keystrokes in SSH
sessions; Sun et al. [52] and Liberatore and Levine [38]
showed that identiﬁcation of web sites over encrypted HTTP
connections (e.g., SSL) is possible using the sizes of the
HTML objects returned by HTTP requests; Saponas et al.
[48] showed how to identify the movie being watched over
an encrypted connection.
More pertinent to this paper, however, is the work of
Wright et al. [55, 56] that showed that encrypted VoIP calls
are vulnerable to trafﬁc analysis wherein it may be possible
to infer the spoken language of the call or even the presence
of certain phrases. In the latter case, the approach of Wright
et al. assumes that the objective is to search an encrypted
packet stream for subsequences matching a target phrase
or word, such as ‘attack at dawn’, and therefore requires
that a probabilistic model of likely corresponding packet
6
length sequences (i.e., representing the target phrase in its
entirety) be generated in advance. As discussed earlier, no
such a priori information is necessary under our approach:
we construct transcripts from the bottom up rather than
matching phrases from the top down.
Several other approaches for exploring information leak-
age in encrypted VoIP calls (working under different envi-
ronmental assumptions than Wright et al.) have also been
proposed. For example, if silence suppression is assumed
(i.e., packet
transmission is suppressed when a party is
silent), researchers posit that the duration of talk spurts for
words spoken in isolation makes identiﬁcation of speciﬁc
“speeches” [37, 41] possible. In a recent study with 20
speakers, Backes et al. [2] show that speaker-speciﬁc pause
patterns might be sufﬁcient to undermine the anonymity
of speakers in encrypted VoIP calls. That said, it is well
accepted in the speech community that continuous speech
(i.e., everyday communication) lacks identiﬁable pauses
between words [11]. In fact, speakers generally talk faster
(and typically shorten or run sentences together) as speech
becomes more natural and colloquial. This observation is
even more important in our context where there are no
within-word pauses. Hence, we make no assumptions about
voice activation detection and/or silence suppression.
Lastly, Dupasquier et al. [15] investigate the extent of
information leakage from Skype voice trafﬁc. The authors
conclude that the general concept we pursue here “seems
quite difﬁcult” because classiﬁcation of phonemes is too
challenging. Thus, they revert to the prior setting of knowing
the target phrase in advance and use dynamic time warping
to validate the work of Wright et al. A focus of this paper
is showing that such statements were premature, and that
phoneme-level reconstruction can be successful in under-
mining the privacy of encrypted VoIP conversations.
For conciseness, the relevant literature on speech and
language models will be presented elsewhere in this paper.
V. OVERALL METHODOLOGY
We now turn our attention to explaining the details behind
the key ideas explored in this paper. Wherever possible, we
provide the intuition that drives our design decisions.
A. Finding Phoneme Boundaries (Stage )
Given the sequence of packet sizes from a VoIP con-
versation, the ﬁrst challenge is to identify which of these
packets represent a portion of speech containing a boundary
between phonemes. While automatic segmentation of speech
waveforms on phonetic boundaries has received much at-
tention in the speech recognition community, in our context
we have no access to the acoustic information and must
operate on the sequence of packet sizes. However, recall
that many speech codecs, and Speex in particular, are based
on CELP (code-excited linear prediction), which encodes
speech with two different signals: the excitation signal and
the ﬁlter signal. As mentioned earlier, the ﬁlter signal for
a given frame is modeled as a linear combination of past
excitation signals. Thus more information must be encoded
for frames in which the sound changes drastically—such
as at the transition between two phonemes. Similarly, less
information is encoded for intra-phoneme frames, where
the sound changes relatively little. Figure 3 illustrates how
changes in frame size can indicate a phonetic boundary.
Figure 3. Frame size sequence for the ﬁrst few words of an utterance of ‘an
ofﬁcial deadline cannot be postponed’, illustrating how the sizes of frames
differ in response to phoneme transitions. Notice the distinct changes (e.g.,
a sharp rise) in frame sizes near some phoneme boundaries (e.g., [I], [f],
and [S] in ‘ofﬁcial’). Near other phoneme boundaries (e.g., [d], [l], and [a]
in ‘deadline’), however, frame size remains constant.
Methodology
To perform the segmentation, we apply a probabilistic
learning framework known as maximum entropy model-
ing4 [6, 28] that simultaneously captures many contextual
features in the sequence of frames, as well as the his-
tory of classiﬁcations in the sequence,
to decide which
frames represent phoneme boundaries. Such models have
been successfully applied to problems like part-of-speech
tagging [46] and text segmentation [5].
Maximum entropy modeling estimates the posterior prob-
ability p(y|x), where x is an observation and y a label.
In order to do so, one calculates the empirical distribution
˜p(x, y) from training data as the relative frequency of
examples with value x and label y. One then deﬁnes binary
indicator functions, f (x, y), to describe features of the data
relevant to classiﬁcation.
In the case of phonetic boundary segmentation, we rep-
resent a given frame with w. The labels, i.e., boundary or
interior frame, are represented by the binary variable v. An
indicator function f (w, v) then describes a feature of the
frame which is relevant to whether that frame represents a
phoneme boundary, for example:
(cid:26) 1,
0,
f (w, v) =
if v is boundary and w has size n,
otherwise.
4Also known as multinomial logistic regression.
7
 ɛ  n   ɪ      f       ɪ      ʃ       l     d       ɛ    d    l       a       nFrame Size (bytes)an         official                                deadlineGiven an indicator function, one can compute the expected
value of a feature, f, with respect to the training data as:
˜p(f ) =
˜p(x, y)f (x, y)
(cid:88)
x,y
(cid:88)
One can thus represent any statistical phenomena in the
training data with ˜p(f ). The expected value of f with respect
to the target model, p(y|x), may be represented as:
p(f ) =
˜p(x)p(y|x)f (x, y)
x,y
Requiring that ˜p(f ) = p(f ) imposes the constraint that the
model agree with the training data with respect to feature
f; over all features, this yields a set of constraints for the
target model:
C =(cid:8)p ∈ P | p(fi) = ˜p(fi) for i ∈ {1, 2,··· , n}(cid:9)
Many models may satisfy the set of constraints. However,
the principle of maximum entropy states that the model that
best represents the data given the current state of knowledge
is the one with the most entropy. This yields a constrained
optimization problem of the form argmaxp∈CH(p), where
H(p) is the entropy of y conditioned on x in the model p.
Phoneme Segmentation Feature Templates
1
2
3
4
5
6
7
8
size of frame wi (i.e., the current frame size)
size of frame wi−1 (i.e., the previous frame size)
size of frame wi+1 (i.e., the next frame size)
bigram of sizes for frames wi−1, wi
bigram of sizes for frames wi, wi+1
trigram of sizes for frames wi−1, wi, wi+1
sequence of frame sizes since the last hypothesized boundary
number of frames since since the last hypothesized boundary
FEATURE TEMPLATES FOR THE PHONETIC SEGMENTATION, WHERE wi
REPRESENTS THE iTH FRAME.
Table I
For boundary identiﬁcation, we deﬁne several feature tem-
plates which specify features that we hypothesize correlate
with phoneme boundaries. The templates we use are given
in Table I, and some features are illustrated in Figure 4 for
clarity. Although each frame only gives us one observable
feature (namely, the size of the frame), we leverage the
surrounding frames, and the history of previously classiﬁed
frames, in the sequence to create a much richer feature set.
The templates are used to automatically generate the full
feature set directly from the data.
As per our hypothesis regarding the interaction between
linear prediction and frame size, notice that feature templates
1-6 capture the frame sizes in the proximity of the current
frame. The frame size unigrams, bigrams, and trigrams must
be explicitly captured because maximum entropy models
do not model feature interactions, i.e., they only consider
individual features in isolation. Some phonemes may always
exhibit a certain frame size sequence; we capture this behav-
ior with feature template 7. Lastly, because some boundaries
8
Figure 4. Classiﬁcation example for current frame, wi. The label for wi
is dependent on a number of features, including the frame size sequence
since the last hypothesized boundary (shown here in gray). An example
feature derived from each of templates 6-8 is depicted on the right hand
side.
are not detectable by frame size changes (such as the long
sequence of same-sized frames in Figure 3), we also model
features such as phoneme length (feature template 8).
To efﬁciently solve the optimization problem posed by
maximum entropy modeling, we use the megam framework
with the limited memory BGFS [39] algorithm to obtain the
model p(w|v). Having built a model, we estimate the prob-
ability of each frame, in order, being a phoneme boundary
by evaluating the estimated posterior p(w|v). Since feature
templates 7 and 8 depend on previously classiﬁed labels,
we use a dynamic programming algorithm to maximize the
likelihood of the sequence as a whole rather than greedily
selecting the most likely label for each frame. The algorithm,
a beam search, stores a list of the l most likely candidate
segmentations up to the current frame; this list is updated
after each frame is evaluated. We choose as our ﬁnal
segmentation the most likely candidate at the last frame.
Evaluation
In order to provide rigorous assessments of our method-
ology, we perform cross-validation in the segmentation and
classiﬁcation stages of our experiments. Cross-validation is
a method for estimating the generalization performance of a
classiﬁer by partitioning the available data into complemen-
tary subsets, training with one subset, and testing with the
other. In particular, we perform k-fold cross-validation, in
which the data is partitioned into k complementary subsets.
For each fold, one subset is selected for testing and the
remainder used for training. The training and testing are
performed as many times as there are subsets, with each
acting as the testing set in one fold. The results of all iter-
ations are then averaged to give the expected generalization
performance, which mitigates the possibility of experimental
wilast boundaryInstantiation of Feature Template 7Instantiation of Feature Template 8number offramesframe sequenceInstantiation of Feature Template 6frame trigramSequence of Framesresults being unduly inﬂuenced by fortuitous selection of
training and testing data.
To evaluate the performance of our phonetic segmentation
model, we perform a 5-fold cross-validation experiment
for each dialect
in the TIMIT corpus. Using a holdout
set of female speakers from the New England dialect, we
experimentally determined an optimal value of 8 for the
beam width l. We report the performance using precision
(i.e., the fraction of boundaries in the transcription that are
present in the reference transcription) and recall (i.e., the
fraction of boundaries in the reference that appear in the
transcription) as our metrics.
Dialect
New England
Northern
North Midland
South Midland
Southern
New York City
Western
Army Brat
n = 1
n = 2
Precision
0.8539
0.8555
0.8509
0.8452
0.8525
0.8530
0.8586
0.8465
Recall
0.7233
0.7332
0.7372
0.7086
0.7037
0.7096
0.7259
0.7540
Precision
0.9443
0.9458
0.9402
0.9352
0.9405
0.9386
0.9439
0.9389
Recall
0.8735
0.8837
0.8901
0.8627
0.8586
0.8628
0.8652
0.8985
PHONETIC SEGMENTATION PERFORMANCE FOR EACH DIALECT IN THE
Table II
TIMIT CORPUS.
the frame level—in fact,
While interpreting these results, we note that Raymond
et al. [47] have shown that phoneme boundaries are inexact
even at
in their study, human
transcribers agreed (within 20ms) on less than 80% of the