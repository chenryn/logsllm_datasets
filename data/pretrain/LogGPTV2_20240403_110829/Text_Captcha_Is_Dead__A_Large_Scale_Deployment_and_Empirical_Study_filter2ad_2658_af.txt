word in a picture according to the prompt. Due to the space con-
straints, we do not include this design in the paper. Experimental
results verify the effectiveness of advCAPTCHA in this scenario
ML based captcha solving attackers. advCAPTCHA fills the gap
between the scientific innovations of adversarial captchas and its
adoptions in practical applications. Specifically, we propose novel
perturbation methods to generate adversarial captchas to degrade
the performance of captcha solvers. We further incorporate the
idea of model extraction to make our substitute model fit the actual
attack model. This strategy can automatically adapt our defense
system after observing an update of the attacker’s model. Extensive
experiments on the real world platform demonstrate the effective-
ness of advCAPTCHA which can significantly reduce the success
rate of actual attackers.
ACKNOWLEDGEMENT
We would like to thank our shepherd Jelena Mirkovic and the
anonymous reviewers for their valuable suggestions for improving
this paper. This work was partly supported by NSFC under No.
61772466, U1936215, and U1836202, the National Key Research and
Development Program of China under No. 2018YFB0804102, the
Zhejiang Provincial Natural Science Foundation for Distinguished
Young Scholars under No. LR19F020003, the Zhejiang Provincial
Natural Science Foundation under No. LSY19H180011, the Zhejiang
Provincial Key R&D Program under No. 2019C01055, the Ant Finan-
cial Research Funding, and the Alibaba-ZJU Joint Research Institute
of Frontier Technologies. Ting Wang is partially supported by the
National Science Foundation under Grant No. 1910546, 1953813,
and 1846151.
REFERENCES
[1] [n. d.]. https://www.deathbycaptcha.com.
[2] [n. d.]. http://www.captchatronix.com.
[3] [n. d.]. https://pypi.org/project/captcha/.
[4] N. Akhtar and A. Mian. 2018. Threat of Adversarial Attacks on Deep Learning in
Computer Vision: A Survey. IEEE Access (2018).
[5] Elie Bursztein, Jonathan Aigrain, Angelika Moscicki, and John C. Mitchell. [n.
d.]. The End is Nigh: Generic Solving of Text-based CAPTCHAs. In 8th USENIX
Workshop on Offensive Technologies (WOOT 14).
[6] Elie Bursztein, Matthieu Martin, and John Mitchell. [n. d.]. Text-based CAPTCHA
Strengths and Weaknesses. In CCS ’11.
[7] Michal Busta, Lukas Neumann, and Jiri Matas. 2017. Deep textspotter: An end-to-
end trainable scene text localization and recognition framework. In Proceedings
of the IEEE International Conference on Computer Vision (ICCV).
[8] Nicholas Carlini and David Wagner. [n. d.]. Towards evaluating the robustness
of neural networks. In IEEE Symposium on Security and Privacy (SP) 2017.
[9] N. Carlini and D. Wagner. 2017. Adversarial examples are not easily detected:
Bypassing ten detection methods. AISec (2017).
[10] Kumar Chellapilla and Patrice Y. Simard. 2005. Using Machine Learning to Break
In Advances in Neural Information
Visual Human Interaction Proofs (HIPs).
Processing Systems 17. MIT Press.
[11] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. [n.
d.]. ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural
Networks Without Training Substitute Models. In Proceedings of the 10th ACM
Workshop on Artificial Intelligence and Security (AISec ’17).
[12] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
Jianguo Li. 2018. Boosting adversarial attacks with momentum. In Proceedings of
the IEEE conference on computer vision and pattern recognition.
[13] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei
Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017. Robust physical-
world attacks on deep learning models. arXiv preprint arXiv:1707.08945 (2017).
[14] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. [n. d.]. Model Inversion
Attacks That Exploit Confidence Information and Basic Countermeasures. In
CCS ’15.
[15] Haichang Gao, Wei Wang, Jiao Qi, Xuqin Wang, Xiyang Liu, and Jeff Yan. [n. d.].
The robustness of hollow CAPTCHAs. In CCS ’13.
[16] Haichang Gao, Jeff Yan, Fang Cao, Zhengya Zhang, Lei Lei, Mengyun Tang, Ping
Zhang, Xin Zhou, Xuqin Wang, and Jiawei Li. 2016. A Simple Generic Attack on
Text Captchas. In NDSS 2016.
14
[17] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. [n. d.]. Explaining
and harnessing adversarial examples. In ICLR 2015.
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition.
[19] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Comput. (1997).
[20] K. Hwang, C. Huang, and G. You. [n. d.]. A Spelling Based CAPTCHA System by
Using Click. In International Symposium on Biometrics and Security Technologies
2012.
[21] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. [n. d.]. Black-box
adversarial attacks with limited queries and information. In ICML 2018.
[22] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[23] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. [n. d.]. Adversarial examples
in the physical world. ICLR 2017 ([n. d.]).
[24] Jonathan Lazar, Jinjuan Feng, Tim Brooks, Genna Melamed, Brian Wentz,
Jonathan Holman, Abiodun Olalere, and Nnanna Ekedebe. [n. d.]. The Sound-
sRight CAPTCHA: an improved approach to audio human interaction proofs for
blind users. In CHI 2012.
[25] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE (1998).
[26] Keaton Mowery and Hovav Shacham. [n. d.]. Pixel perfect: Fingerprinting canvas
in HTML5. ([n. d.]).
[27] Yoichi Nakaguro, Matthew Dailey, Sanparith Marukatat, and Stanislav Makhanov.
[n. d.]. Defeating line-noise CAPTCHAs with multiple quadratic snakes. Com-
puters Security 2013 ([n. d.]).
[28] N. Narodytska and S. P. Kasiviswanathan. 2017. Simple black-box adversarial
perturbations for deep networks. In IEEE Conference on Computer Vision and
Pattern Recognition.
[29] Margarita Osadchy, Julio Hernandez-Castro, Stuart Gibson, Orr Dunkelman,
and Daniel Pérez-Cabo. 2017. No bot expects the DeepCAPTCHA! Introducing
immutable adversarial examples, with applications to CAPTCHA generation.
IEEE Transactions on Information Forensics and Security (2017).
[30] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik,
and Ananthram Swami. [n. d.]. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security.
[31] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. [n. d.]. The limitations of deep learning in adversarial
settings. In IEEE European Symposium on Security and Privacy (EuroS&P) 2016.
[32] S. K. Saha, A. K. Nag, and D. Dasgupta. 2015. Human-Cognition-Based
CAPTCHAs. IT Professional (2015).
[33] B. Shi, X. Bai, and C. Yao. 2017. An End-to-End Trainable Neural Network for
Image-Based Sequence Recognition and Its Application to Scene Text Recognition.
IEEE Transactions on Pattern Analysis and Machine Intelligence (2017).
[34] Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai. 2016.
Robust scene text recognition with automatic rectification. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
[35] Chenghui Shi, Xiaogang Xu, Shouling Ji, Kai Bu, Jianhai Chen, Raheem A. Beyah,
and Ting Wang. 2019. Adversarial CAPTCHAs. CoRR abs/1901.01107 (2019).
arXiv:1901.01107
[36] S. Sivakorn, I. Polakis, and A. D. Keromytis. [n. d.]. I am Robot: (Deep) Learning
to Break Semantic Image CAPTCHAs. In 2016 IEEE European Symposium on
Security and Privacy (EuroS P).
[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.
Going deeper with convolutions. In Proceedings of the IEEE conference on computer
vision and pattern recognition.
[38] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[39] Florian Tramer, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
[n. d.]. Stealing machine learning models via prediction apis. In Usenix Security
2016.
[40] H. Weng, B. Zhao, S. Ji, J. Chen, T. Wang, Q. He, and R. Beyah. 2019. Towards
understanding the security of modern image captchas and underground captcha-
solving services. Big Data Mining and Analytics 2, 2 (2019), 118–144.
[41] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton. [n. d.]. A Methodology for
Formalizing Model-Inversion Attacks. In 2016 IEEE 29th Computer Security Foun-
dations Symposium (CSF).
[42] Y Xu, G Reynaga, Sonia Chiasson, J.-M. Frahm, Fabian Monrose, and Paul
van Oorschot. [n. d.]. Security and Usability Challenges of Moving-Object
CAPTCHAs: Decoding Codewords in Motion. In Usenix Security 2012.
[43] J. Yan and A. S. E. Ahmad. [n. d.]. Breaking Visual CAPTCHAs with Naive Pattern
Recognition Algorithms. In Twenty-Third Annual Computer Security Applications
Conference (ACSAC 2007).
[44] Jeff Yan and Ahmad Salah El Ahmad. [n. d.]. A Low-cost Attack on a Microsoft
Captcha. In CCS ’08.
[45] Guixin Ye, Zhanyong Tang, Dingyi Fang, Zhanxing Zhu, Yansong Feng, Pengfei
Xu, Xiaojiang Chen, and Zheng Wang. [n. d.]. Yet Another Text Captcha Solver:
A Generative Adversarial Network Based Approach. In CCS ’18.
[46] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples:
Attacks and defenses for deep learning. IEEE transactions on neural networks and
learning systems (2019).
[47] Bin B Zhu, Jeff Yan, Qiujie Li, Chao Yang, Jia Liu, Ning Xu, Meng Yi, and Kaiwei
Cai. [n. d.]. Attacks and design of image recognition CAPTCHAs. In CCS ’10.
A CRNN MODEL STRUCTURE
Here, We describe each part of CRNN in detail. CRNN mainly con-
sists of three parts: Convolutional Neural Network (CNN) part which
extracts a feature sequence from the input image, Recurrent Neural
Network (RNN) part which predicts a label distribution for each
feature representation and Connectionist Temporal Classification
(CTC) part which translates all label distributions into the final label
sequence, as shown in Figure 12.
CNN Part. The component of convolutional layers is constructed
by taking the convolutional and pooling layers from a standard
CNN model (fully-connected layers are removed). Such component
is used to extract a sequential feature representation from an input
image. Before being fed into the network, all the images need to
be scaled to the same height (width could be different). Then a
sequence of feature vectors f1, ..., fr is extracted from the feature
maps produced by the component of convolutional layers, which
is the input for the recurrent layers. The value of r is related to
the width of the input image. Obviously, a wider input image will
induce a longer feature sequence. Note that our method can accom-
modate any type of CNN models, e.g., Lenet [25], ResNet [18] and
Inception [37]. Table 6 shows the structure of our solver which has
four convolutional layers, where each of the convolutional layer
is followed by a max-pooling layer. This structure is simple and
effective in practical applications which can achieve high accuracy
with a small amount of training data and a short inference time.
RNN Part. The recurrent layers predict a label distribution lt for
each feature representation ft in the feature sequence f1, ..., fr .
The traditional RNNs are vulnerable to the vanishing gradient
problem, while the Long Short Term Memory (LSTM) method is
more effective in tackling this issue [19]. The special design of
LSTM can capture long-range dependencies, which often occur in
image-based sequences. Table 6 shows the structure of our RNN
part which has two LSTM layers.
CTC Part. CTC layer decodes the predictions made by the RNN
part into a label sequence. The objective of the CTC part is to find
the label sequence with the highest probability conditioned on the
predicted label distributions. By leveraging the CTC part, we only
need captcha images and their corresponding label sequences to
train the network, without requiring labeling positions of individual
characters.
B GROUND TRUTH CONSTRUCTION
Now, we describe how domain experts label a user as an attacker.
There are many kinds of features or signals that experts can utilize.
We introduce three categories of features in the following.
1) User Environment Information. Normal users use browsers to
browse websites, while attackers often use scripts to crawl infor-
mation. Thus, their browser attributes are different. The web-front
Figure 12: Architecture for our substitute solving model.
Table 6: CRNN model details
Layer Type
Input
Configurations
kernel=2, strides=2
30 × 100 gray-scale image
Convolution 1 filters=64, kernel=3,strides=1
BatchNorm
Max Pooling 1
Convolution 2 filters=128, kernel=3,strides=1
BatchNorm
Max Pooling 2
Convolution 3 filters=128, kernel=3, strides=1
BatchNorm
Max Pooling 3
Convolution 4 filters=64, kernel=3,strides=1
BatchNorm
Max Pooling 4
kernel=2, strides=2
kernel=2, strides=2
kernel=2, strides=2
hidden units=256
hidden units=256
LSTM 1
LSTM 2
Fully Connect
CTC
Figure 13: The statistic of the device number with respect to access
frequency.
client of the user risk analysis system executes a series of checks
for collecting the user environment information, e.g., web driver,
user-agent and Cookie. The experts can detect various items, for
example, whether a user uses an automate web driver, whether the
user-agent contains the complete information, or is misformated.
Moreover, in order to obtain real browser attributes, web fingerprint
techniques, e.g., webGL fingerprinting [26], are employed. The dis-
crepancies between fingerprints with the reported user-agent are
also important factors considered by the experts.
2) High Frequency Information. A significant characteristic of
machine traffic is the high access frequency compared to the normal
users. The experts can analyze the access frequency for different
entities, e.g., IP, account and device, to find attackers. Moreover,
they can set a strict threshold, e.g., 100 times per second, to avoid
misclassification. Figure 13 shows the statistic of the device number
with respect to the access frequency during a week. We can find
many devices access too frequently.
15
Batch NormalizationLeaky ReLuFlatten VectorLSTMLSTMFully ConnectedCTC Decoderd2d6CNN PartRNN PartCTC PartMax Pool* 4Figure 14: The statistic of the access number with respect to sliding
time.
(a) normal trajectory
3) User Behavior Information. As we stated in Section 4.1, a user
needs to slide a bar for the authentication. Such sliding behavior
information is an important feature to identify attackers. Figure
15 shows the the difference between machine mouse trajectories
and human mouse trajectories. From Figure 15, experts can classify
users whose mouse trajectories are significantly different from
normal users as attackers. Moreover, they can even simply analyze
the sliding time. Figure 14 shows the statistics of the device number
with respect to sliding time during a week. Obviously, those devices
which slide the bar too fast, e.g., less than 0.1 second, are machine
traffic. In addition, even the attackers realize that the sliding time is
a detection variable and deliberately extend the sliding time, we can
still detect them leveraging other advanced means. For instance,
they access too many times in a short time and their sliding time
are similar, which can also be observed in Figure 14.
The sophisticated attackers may try to avoid detection by obfus-
cating some features, i.e., when they aware of mouse movement,
they can deliberately simulate human trajectories. However, it is
difficult for the attackers to simulate human behaviors in all possi-
ble features. Actually, there are many other features we have not
mentioned in the paper due to the confidentiality requirements of
the company. By analyzing various features, experts can label a
attacker with high confidence.
(b) abnormal trajectory 1
(c) abnormal trajectory 2
Figure 15: Mouse trajectories during sliding a bar. Human trajec-
tory in 15(a) is smooth while machine trajectory is either rough in
15(b) or linear in 15(c).
16