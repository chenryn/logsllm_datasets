### Adversarial CAPTCHAs: Bridging the Gap Between Scientific Innovations and Practical Applications

#### Introduction
In this work, we introduce **advCAPTCHA**, a novel approach to generating adversarial CAPTCHAs that degrade the performance of machine learning (ML)-based CAPTCHA solvers. Due to space constraints, we do not include the detailed design of word-in-picture CAPTCHAs in this paper. Experimental results demonstrate the effectiveness of advCAPTCHA in mitigating ML-based attacks.

#### Contributions
- **Novel Perturbation Methods**: We propose innovative perturbation techniques to create adversarial CAPTCHAs, which significantly reduce the success rate of automated solvers.
- **Model Extraction and Adaptation**: By incorporating the concept of model extraction, our substitute model can adapt to changes in the attacker's model, ensuring continuous robustness.
- **Real-World Validation**: Extensive experiments on a real-world platform validate the effectiveness of advCAPTCHA in reducing the success rate of actual attackers.

#### Acknowledgements
We would like to thank our shepherd, Jelena Mirkovic, and the anonymous reviewers for their valuable suggestions. This work was partially supported by the following grants:
- NSFC (No. 61772466, U1936215, U1836202)
- National Key Research and Development Program of China (No. 2018YFB0804102)
- Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars (No. LR19F020003)
- Zhejiang Provincial Natural Science Foundation (No. LSY19H180011)
- Zhejiang Provincial Key R&D Program (No. 2019C01055)
- Ant Financial Research Funding
- Alibaba-ZJU Joint Research Institute of Frontier Technologies

Ting Wang is also partially supported by the National Science Foundation (Grant No. 1910546, 1953813, and 1846151).

#### References
1. [n. d.]. https://www.deathbycaptcha.com
2. [n. d.]. http://www.captchatronix.com
3. [n. d.]. https://pypi.org/project/captcha/
4. N. Akhtar and A. Mian. 2018. Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey. IEEE Access (2018).
5. Elie Bursztein, Jonathan Aigrain, Angelika Moscicki, and John C. Mitchell. [n. d.]. The End is Nigh: Generic Solving of Text-based CAPTCHAs. In 8th USENIX Workshop on Offensive Technologies (WOOT 14).
6. Elie Bursztein, Matthieu Martin, and John Mitchell. [n. d.]. Text-based CAPTCHA Strengths and Weaknesses. In CCS ’11.
7. Michal Busta, Lukas Neumann, and Jiri Matas. 2017. Deep textspotter: An end-to-end trainable scene text localization and recognition framework. In Proceedings of the IEEE International Conference on Computer Vision (ICCV).
8. Nicholas Carlini and David Wagner. [n. d.]. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy (SP) 2017.
9. N. Carlini and D. Wagner. 2017. Adversarial examples are not easily detected: Bypassing ten detection methods. AISec (2017).
10. Kumar Chellapilla and Patrice Y. Simard. 2005. Using Machine Learning to Break Visual Human Interaction Proofs (HIPs). In Advances in Neural Information Processing Systems 17. MIT Press.
11. Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. [n. d.]. ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks Without Training Substitute Models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (AISec ’17).
12. Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. 2018. Boosting adversarial attacks with momentum. In Proceedings of the IEEE conference on computer vision and pattern recognition.
13. Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2017. Robust physical-world attacks on deep learning models. arXiv preprint arXiv:1707.08945 (2017).
14. Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. [n. d.]. Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures. In CCS ’15.
15. Haichang Gao, Wei Wang, Jiao Qi, Xuqin Wang, Xiyang Liu, and Jeff Yan. [n. d.]. The robustness of hollow CAPTCHAs. In CCS ’13.
16. Haichang Gao, Jeff Yan, Fang Cao, Zhengya Zhang, Lei Lei, Mengyun Tang, Ping Zhang, Xin Zhou, Xuqin Wang, and Jiawei Li. 2016. A Simple Generic Attack on Text Captchas. In NDSS 2016.
17. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. [n. d.]. Explaining and harnessing adversarial examples. In ICLR 2015.
18. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition.
19. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. (1997).
20. K. Hwang, C. Huang, and G. You. [n. d.]. A Spelling Based CAPTCHA System by Using Click. In International Symposium on Biometrics and Security Technologies 2012.
21. Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. [n. d.]. Black-box adversarial attacks with limited queries and information. In ICML 2018.
22. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
23. Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. [n. d.]. Adversarial examples in the physical world. ICLR 2017 ([n. d.]).
24. Jonathan Lazar, Jinjuan Feng, Tim Brooks, Genna Melamed, Brian Wentz, Jonathan Holman, Abiodun Olalere, and Nnanna Ekedebe. [n. d.]. The Sound-sRight CAPTCHA: an improved approach to audio human interaction proofs for blind users. In CHI 2012.
25. Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. 1998. Gradient-based learning applied to document recognition. Proc. IEEE (1998).
26. Keaton Mowery and Hovav Shacham. [n. d.]. Pixel perfect: Fingerprinting canvas in HTML5. ([n. d.]).
27. Yoichi Nakaguro, Matthew Dailey, Sanparith Marukatat, and Stanislav Makhanov. [n. d.]. Defeating line-noise CAPTCHAs with multiple quadratic snakes. Computers Security 2013 ([n. d.]).
28. N. Narodytska and S. P. Kasiviswanathan. 2017. Simple black-box adversarial perturbations for deep networks. In IEEE Conference on Computer Vision and Pattern Recognition.
29. Margarita Osadchy, Julio Hernandez-Castro, Stuart Gibson, Orr Dunkelman, and Daniel Pérez-Cabo. 2017. No bot expects the DeepCAPTCHA! Introducing immutable adversarial examples, with applications to CAPTCHA generation. IEEE Transactions on Information Forensics and Security (2017).
30. Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. [n. d.]. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security.
31. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. [n. d.]. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy (EuroS&P) 2016.
32. S. K. Saha, A. K. Nag, and D. Dasgupta. 2015. Human-Cognition-Based CAPTCHAs. IT Professional (2015).
33. B. Shi, X. Bai, and C. Yao. 2017. An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (2017).
34. Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai. 2016. Robust scene text recognition with automatic rectification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
35. Chenghui Shi, Xiaogang Xu, Shouling Ji, Kai Bu, Jianhai Chen, Raheem A. Beyah, and Ting Wang. 2019. Adversarial CAPTCHAs. CoRR abs/1901.01107 (2019). arXiv:1901.01107
36. S. Sivakorn, I. Polakis, and A. D. Keromytis. [n. d.]. I am Robot: (Deep) Learning to Break Semantic Image CAPTCHAs. In 2016 IEEE European Symposium on Security and Privacy (EuroS P).
37. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition.
38. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013).
39. Florian Tramer, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. [n. d.]. Stealing machine learning models via prediction apis. In Usenix Security 2016.
40. H. Weng, B. Zhao, S. Ji, J. Chen, T. Wang, Q. He, and R. Beyah. 2019. Towards understanding the security of modern image captchas and underground captcha-solving services. Big Data Mining and Analytics 2, 2 (2019), 118–144.
41. X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton. [n. d.]. A Methodology for Formalizing Model-Inversion Attacks. In 2016 IEEE 29th Computer Security Foundations Symposium (CSF).
42. Y Xu, G Reynaga, Sonia Chiasson, J.-M. Frahm, Fabian Monrose, and Paul van Oorschot. [n. d.]. Security and Usability Challenges of Moving-Object CAPTCHAs: Decoding Codewords in Motion. In Usenix Security 2012.
43. J. Yan and A. S. E. Ahmad. [n. d.]. Breaking Visual CAPTCHAs with Naive Pattern Recognition Algorithms. In Twenty-Third Annual Computer Security Applications Conference (ACSAC 2007).
44. Jeff Yan and Ahmad Salah El Ahmad. [n. d.]. A Low-cost Attack on a Microsoft CAPTCHA. In CCS ’08.
45. Guixin Ye, Zhanyong Tang, Dingyi Fang, Zhanxing Zhu, Yansong Feng, Pengfei Xu, Xiaojiang Chen, and Zheng Wang. [n. d.]. Yet Another Text Captcha Solver: A Generative Adversarial Network Based Approach. In CCS ’18.
46. Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples: Attacks and defenses for deep learning. IEEE transactions on neural networks and learning systems (2019).
47. Bin B Zhu, Jeff Yan, Qiujie Li, Chao Yang, Jia Liu, Ning Xu, Meng Yi, and Kaiwei Cai. [n. d.]. Attacks and design of image recognition CAPTCHAs. In CCS ’10.

### CRNN Model Structure
The CRNN (Convolutional Recurrent Neural Network) model consists of three main components: the Convolutional Neural Network (CNN) part, the Recurrent Neural Network (RNN) part, and the Connectionist Temporal Classification (CTC) part. Each component is described in detail below.

#### CNN Part
The CNN part is responsible for extracting a feature sequence from the input image. It is constructed using convolutional and pooling layers from a standard CNN model, with fully-connected layers removed. The images are scaled to a uniform height before being fed into the network. The feature maps produced by the convolutional layers are then converted into a sequence of feature vectors, which serve as input for the recurrent layers. The length of the feature sequence is proportional to the width of the input image. Our method is flexible and can accommodate various CNN architectures, such as LeNet, ResNet, and Inception. Table 6 outlines the structure of our solver, which includes four convolutional layers, each followed by a max-pooling layer. This configuration is both simple and effective, achieving high accuracy with minimal training data and short inference times.

#### RNN Part
The RNN part predicts a label distribution for each feature representation in the feature sequence. Traditional RNNs suffer from the vanishing gradient problem, but Long Short-Term Memory (LSTM) networks effectively address this issue. The LSTM design is particularly adept at capturing long-range dependencies, which are common in image-based sequences. Our RNN part consists of two LSTM layers, as shown in Table 6.

#### CTC Part
The CTC part decodes the predictions made by the RNN part into a label sequence. Its objective is to find the label sequence with the highest probability given the predicted label distributions. By using the CTC part, we only need CAPTCHA images and their corresponding label sequences for training, without requiring the labeling of individual character positions.

### Ground Truth Construction
Domain experts use a combination of features and signals to label users as attackers. These features can be categorized into three main types:

1. **User Environment Information**:
   - Normal users typically browse websites using browsers, while attackers often use scripts. Differences in browser attributes, such as web drivers, user-agents, and cookies, can help identify potential attackers. Techniques like webGL fingerprinting are employed to obtain real browser attributes, and discrepancies between fingerprints and reported user-agents are considered important factors.

2. **High Frequency Information**:
   - Machine traffic is characterized by high access frequency compared to normal users. Experts analyze access frequencies for different entities, such as IP addresses, accounts, and devices, to detect attackers. A strict threshold, such as 100 accesses per second, can be set to avoid misclassification. Figure 13 illustrates the device number statistics with respect to access frequency over a week, highlighting frequent access patterns.

3. **User Behavior Information**:
   - User behavior during authentication, such as sliding a bar, provides valuable information. Figures 14 and 15 show the differences between human and machine mouse trajectories. Users with significantly different or too-fast sliding behaviors are likely to be attackers. Even if attackers attempt to mimic human behavior, it is challenging to simulate all possible features accurately.

By analyzing these and other confidential features, experts can confidently label users as attackers.