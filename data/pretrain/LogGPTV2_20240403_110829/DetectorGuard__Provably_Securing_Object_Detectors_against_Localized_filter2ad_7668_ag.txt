Tramer, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Physical ad-
versarial examples for object detectors. In 12th USENIX Workshop on Offensive
Technologies (WOOT 18).
[15] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. 2013. Vision
meets robotics: The kitti dataset. The International Journal of Robotics Research
32, 11 (2013), 1231–1237.
[16] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In 3rd International Conference on Learning
Representations (ICLR).
[17] Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli
Qin, Jonathan Uesato, Relja Arandjelovic, Timothy Arthur Mann, and Pushmeet
Kohli. 2019. Scalable Verified Training for Provably Robust Image Classification.
In 2019 IEEE/CVF International Conference on Computer Vision (ICCV). 4841–4850.
[18] Jamie Hayes. 2018. On Visible Adversarial Perturbations & Digital Watermarking.
In 2018 IEEE Conference on Computer Vision and Pattern Recognition Workshops
(CVPR Workshops). 1597–1604.
[19] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. 2017. Mask
R-CNN. In IEEE International Conference on Computer Vision, (ICCV 2017). IEEE
Computer Society, 2980–2988.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). 770–778.
[21] Danny Karmon, Daniel Zoran, and Yoav Goldberg. 2018. LaVAN: Localized and
Visible Adversarial Noise. In Proceedings of the 35th International Conference on
Machine Learning (ICML). 2512–2520.
[22] Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. 2019. Certified Robustness to Adversarial Examples with Differential Privacy.
In 2019 IEEE Symposium on Security and Privacy (S&P). 656–672.
[23] Mark Lee and Zico Kolter. 2019. On physical adversarial patches for object
detection. arXiv preprint arXiv:1906.11897 (2019).
[24] Alexander Levine and Soheil Feizi. 2020. (De)randomized Smoothing for Certifi-
able Defense against Patch Attacks. arXiv preprint arXiv:2002.10733 (2020).
[25] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollár. 2017.
Focal Loss for Dense Object Detection. In IEEE International Conference on Com-
puter Vision, (ICCV) 2017. IEEE Computer Society, 2999–3007.
[26] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common
Objects in Context. In European Conference on Computer Vision (ECCV), Vol. 8693.
Springer, 740–755.
[27] Aishan Liu, Xianglong Liu, Jiaxin Fan, Yuqing Ma, Anlan Zhang, Huiyuan Xie,
and Dacheng Tao. 2019. Perceptual-Sensitive GAN for Generating Adversarial
Patches. In The 33rd AAAI Conference on Artificial Intelligence, (AAAI) 2019. AAAI
Press, 1028–1035.
[28] Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang, and Hang
Yu. 2020. Bias-Based Universal Adversarial Patch Attack for Automatic Check-
Out. In European conference on computer vision (ECCV), Vol. 12358. Springer,
395–410.
[29] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott E. Reed,
Cheng-Yang Fu, and Alexander C. Berg. 2016. SSD: Single Shot MultiBox Detector.
In European conference on computer vision (ECCV), Vol. 9905. Springer, 21–37.
[30] Xin Liu, Huanrui Yang, Ziwei Liu, Linghao Song, Yiran Chen, and Hai Li. 2019.
DPATCH: An Adversarial Patch Attack on Object Detectors. In AAAI Conference
on Artificial Intelligence Workshop (AAAI workshop) 2019, Vol. 2301.
[31] Jiajun Lu, Hussein Sibai, and Evan Fabry. 2017. Adversarial examples that fool
detectors. arXiv preprint arXiv:1712.02494 (2017).
[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In 6th International Conference on Learning Representations (ICLR).
[33] Michael McCoyd, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper, Min-
june Hwang, Jason Xinyu Liu, and David Wagner. 2020. Minority Reports Defense:
Defending Against Adversarial Patches. arXiv preprint arXiv:2004.13799 (2020).
[34] Dongyu Meng and Hao Chen. 2017. MagNet: A Two-Pronged Defense against
Adversarial Examples. In Proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security (CCS). 135–147.
[35] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. 2017.
On Detecting Adversarial Perturbations. In 5th International Conference on Learn-
ing Representations (ICLR).
[36] Jan Hendrik Metzen and Maksym Yatsura. 2021. Efficient Certified Defenses
Against Patch Attacks on Image Classifiers. In 9th International Conference on
Learning Representations (ICLR). https://openreview.net/forum?id=hr-3PMvDpil
[37] Matthew Mirman, Timon Gehr, and Martin T. Vechev. 2018. Differentiable
Abstract Interpretation for Provably Robust Neural Networks. In Proceedings of
the 35th International Conference on Machine Learning (ICML). 3575–3583.
[38] Muzammal Naseer, Salman Khan, and Fatih Porikli. 2019. Local Gradients Smooth-
ing: Defense Against Localized Adversarial Attacks. In IEEE Winter Conference
on Applications of Computer Vision (WACV). 1300–1307.
[39] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael P Wellman.
2018. SoK: Security and privacy in machine learning. In 2018 IEEE European
Symposium on Security and Privacy (EuroS&P). 399–414.
[40] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram
Swami. 2016. Distillation as a Defense to Adversarial Perturbations Against Deep
Neural Networks. In IEEE Symposium on Security and Privacy (S&P). 582–597.
[41] Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. 2018. Certified Defenses
against Adversarial Examples. In 6th International Conference on Learning Repre-
sentations (ICLR).
[42] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You
only look once: Unified, real-time object detection. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 779–788.
[43] Joseph Redmon and Ali Farhadi. 2017. YOLO9000: better, faster, stronger. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
7263–7271.
[44] Joseph Redmon and Ali Farhadi. 2018. Yolov3: An incremental improvement.
arXiv preprint arXiv:1804.02767 (2018).
[45] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91–99.
[46] Aniruddha Saha, Akshayvarun Subramanya, Koninika Patil, and Hamed Pir-
siavash. 2020. Role of Spatial Context in Adversarial Robustness for Object
Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops (CVPR Workshops). 784–785.
[47] Hadi Salman, Jerry Li, Ilya P. Razenshteyn, Pengchuan Zhang, Huan Zhang,
Sébastien Bubeck, and Greg Yang. 2019. Provably Robust Deep Learning via
Adversarially Trained Smoothed Classifiers. In Annual Conference on Neural
Information Processing Systems 2019 (NeurIPS). 11289–11300.
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In 2nd International Conference on Learning Representations (ICLR).
[49] Mingxing Tan, Ruoming Pang, and Quoc V Le. 2020. EfficientDet: Scalable and
Efficient Object Detection. In 2020 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR 2020). 10781–10790.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3190[50] Simen Thys, Wiebe Van Ranst, and Toon Goedemé. 2019. Fooling automated
surveillance cameras: adversarial patches to attack person detection. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.
[51] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry.
2020. On adaptive attacks to adversarial example defenses. arXiv preprint
arXiv:2002.08347 (2020).
[52] Abdul Vahab, Maruti S Naik, Prasanna G Raikar, and Prasad SR. 2019. Applications
of Object Detection System. International Research Journal of Engineering and
Technology (IRJET) 6, 4 (2019), 4186–4192.
[53] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. 2020. Scaled-
YOLOv4: Scaling Cross Stage Partial Network. arXiv preprint arXiv:2011.08036
(2020).
[54] Derui Wang, Chaoran Li, Sheng Wen, Xiaojun Chang, Surya Nepal, and Yang
Xiang. 2019. Daedalus: Breaking non-maximum suppression in object detection
via adversarial examples. arXiv (2019), arXiv–1902.
[55] Xingxing Wei, Siyuan Liang, Ning Chen, and Xiaochun Cao. 2019. Transferable
Adversarial Attacks for Image and Video Object Detection. In Proceedings of the
Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI) 2019,
Sarit Kraus (Ed.). ijcai.org, 954–960.
[56] Eric Wong and J. Zico Kolter. 2018. Provable Defenses against Adversarial
Examples via the Convex Outer Adversarial Polytope. In Proceedings of the 35th
International Conference on Machine Learning (ICML). 5283–5292.
[57] Zuxuan Wu, Ser-Nam Lim, Larry S. Davis, and Tom Goldstein. 2020. Making
an Invisibility Cloak: Real World Adversarial Attacks on Object Detectors. In
European Conference on Computer Vision (ECCV) 2020, Vol. 12349. 1–17.
[58] Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. 2021.
PatchGuard: A Provably Robust Defense against Adversarial Patches via Small
Receptive Fields and Masking. In 30th USENIX Security Symposium (USENIX
Security).
[59] Chong Xiang and Prateek Mittal. 2021. PatchGuard++: Efficient Provable Attack
Detection against Adversarial Patches. In ICLR 2021 Workshop on Security and
Safety in Machine Learning Systems.
[60] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan L.
Yuille. 2017. Adversarial Examples for Semantic Segmentation and Object De-
tection. In IEEE International Conference on Computer Vision (ICCV) 2017. IEEE
Computer Society, 1378–1387.
[61] Kaidi Xu, Gaoyuan Zhang, Sijia Liu, Quanfu Fan, Mengshu Sun, Hongge Chen,
Pin-Yu Chen, Yanzhi Wang, and Xue Lin. 2020. Adversarial T-Shirt! Evading
Person Detectors in a Physical World. In European Conference on Computer Vision
(ECCV) 2020, Vol. 12350. 665–681.
[62] Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. In 25th Annual Network and
Distributed System Security Symposium (NDSS).
[63] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. 2019. Adversarial examples:
Attacks and defenses for deep learning. IEEE transactions on neural networks and
learning systems 30, 9 (2019), 2805–2824.
[64] Haichao Zhang and Jianyu Wang. 2019. Towards Adversarially Robust Object
Detection. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
2019. IEEE, 421–430.
[65] Zhanyuan Zhang, Benson Yuan, Michael McCoyd, and David Wagner. 2020.
Clipped BagNet: Defending Against Sticker Attacks with Clipped Bag-of-features.
In 3rd Deep Learning and Security Workshop (DLS).
[66] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, and Kai
Chen. 2019. Seeing isn’t Believing: Towards More Robust Adversarial Attack
Against Real World Object Detectors. In Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security. 1989–2004.
A OBJECT SIZE AND PATCH SIZE
Recall that in Section 5.4, we use a 32×32 patch on 416×416 (or
224×740) images to evaluate the provable robustness. In this section,
we provide additional details of object sizes and patch sizes in
PASCAL VOC, MS COCO, and KITTI datasets.
Small objects are the majority of all three datasets. In Fig-
ure 8, we plot the histogram of object sizes (in the percentage of
pixels) in the test/validation sets of PASCAL VOC, MS COCO, and
KITTI. As shown in the plots, small, or even tiny, objects are the
majority of three datasets. A 32×32 patch takes up 0.6% pixels of a
416×416 (or 224×740) image, and our further analysis shows that
15.2% objects of PASCAL VOC are smaller than 0.6% image pixels;
44.5% of MS COCO and 44.6% KITTI objects are smaller than 0.6%.
Moreover, more than 36.5% of PASCAL VOC objects, more than
66.3% of MS COCO objects, and 75.9% KITTI objects are smaller
than a 64×64 square. These numbers explain why the absolute num-
bers of certified recall in Table 3 are low. In Figure 9, we further
provide visualization of a 32×32 patch on the 416×416 image to
demonstrate the challenge of perfect robust detection even when a
small patch is presented. In the left two examples, the person and
the cow are completely blocked by the adversarial patch and thus
are unrecognizable. In the rightmost example, the head of the dog
is patched and it is even hard for humans to determine if it is a dog
or cat.
Additional evaluation results for different patch sizes. In Fig-
ure 10, we vary the patch size to see how the provable robustness is
affected given different attacker capabilities (i.e, patch sizes). If we
consider a smaller patch of 8×8 pixels, we can have a 2.0% higher CR
for close-patch, and a 2.8% higher CR for over-patch compared with
our CRs for a 32×32 patch. Furthermore, we note that in the far-
patch model, the patch size has a limited influence on the provable
robustness. From Figure 10, We can also see that the CR decreases
as the patch size increases. This analysis demonstrates the limit of
DetectorGuard as well the challenge of robust object detection with
larger patch sizes. We aim to push this limit further in our future
work.
B ADDITIONAL DISCUSSION OF ROBUST
CLASSIFIER IMPLEMENTATION
As introduced in Section 2.4, state-of-the-art provable robust image
classifiers [36, 58] 1) use DNN with small receptive fields to bound
the number of corrupted features and then 2) do secure aggregation
on the partially corrupted feature map for robust classification. In
DetectorGuard, we choose BagNet [3] for small receptive fields and
feature clipping for secure aggregation. In this section, we provide
additional details of BagNet and clipping aggregation. Furthermore,
we discuss alternative aggregation mechanisms and implement
robust masking [58] to demonstrate the generality of our Detec-
torGuard framework.
BagNet [3]. BagNet was originally proposed for interpretable ma-
chine learning. It inherits the high-level architecture of ResNet-
50 [20] and replaces 3x3 convolution kernels with 1x1 ones to
reduce the receptive field size. The authors designed three Bag-
Net architectures with a small receptive field of 9×9, 17×17, and
33×33, in contrast to ResNet-50 having a receptive field of 483×483.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3191Figure 8: Histogram of object sizes (left: PASCAL VOC; middle: MS COCO; right: KITTI)
Table 5: Comparison between masking-based and clipping-based defenses of DetectorGuard (using a perfect clean detector)
clipping-based DetectorGuard
FAR CR-far CR-close CR-over
AP
PASCAL VOC 99.3% 0.9%
99.0% 1.2%
99.0% 1.5%
MS COCO
KITTI
28.6%
11.5%
31.6%
20.7%
7.0%
11.0%
8.3%
2.2%
2.1%
masking-based DetectorGuard
FAR CR-far CR-close CR-over
26.2%
11.4%
17.4%
17.2%
5.4%
4.9%