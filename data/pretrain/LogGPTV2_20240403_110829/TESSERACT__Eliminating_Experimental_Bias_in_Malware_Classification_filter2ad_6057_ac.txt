Let us consider D as a labeled dataset with two classes: mal-
ware (positive class) and goodware (negative class). Let us
deﬁne si ∈ D as an object (e.g., Android app) with timestamp
time(si). To evaluate the classiﬁer, the dataset D must be split
into a training dataset Tr with a time window of size W , and
a testing dataset Ts with a time window of size S. Here, we
consider S > W in order to estimate long-term performance
and robustness to decay of the classiﬁer. A user may consider
different time splits depending on his objectives, provided
each split has a signiﬁcant number of samples. We emphasize
that, although we have the labels of objects in Ts ⊆ D, all the
evaluations and tuning algorithms must assume that labels yi
of objects si ∈ Ts are unknown.
To evaluate performance over time, the test set Ts must be
split into time-slots of size ∆. For example, for a testing set
time window of size S = 2 years, we may have ∆ = 1 month.
This parameter is chosen by the user, but it is important that
the chosen granularity allows for a statistically signiﬁcant
number of objects in each test window [ti,ti + ∆).
We now formalize three constraints that must be enforced
when dividing D into Tr and Ts for a realistic setting that
avoids spatio-temporal experimental bias (§3). While C1 was
proposed in past work [2, 36], we are the ﬁrst to propose C2
and C3—which we show to be fundamental in §4.4.
C1) Temporal training consistency. All the objects in
the training must be strictly temporally precedent to the test-
ing ones:
time(si)  0.5); also, from §3.3 it is clear that if ϕ > 0.5, then the
error rate becomes too high for the goodware class. Finally,
the grid-search explores multiple values of ϕ and stores the
best ones. To capture time-aware performance, we rely on
AUT (§4.2), and the error rate is computed according to the
target P (see above). Tuning examples are in §4.4.
4.4 TESSERACT: Revealing Hidden Performance
Here, we show how our methodology can reveal hidden per-
formance of ALG1 [4], ALG2 [33], and DL [22] (§2.1), and
their robustness to time decay.
We develop TESSERACT as an open-source Python frame-
work that enforces constraints C1, C2, and C3 (§4.1), com-
putes AUT (§4.2), and can train a classiﬁer with our tuning
algorithm (§4.3). TESSERACT operates as a traditional Python
ML library but, in addition to features matrix X and labels y,
it also takes as input the timestamp array t containing dates
for each object. Details about TESSERACT’s implementation
and generality are in §A.5.
Figure 5 reports several performance metrics of the three al-
gorithms as point estimates over time. The X-axis reports the
testing slots in months, whereas the Y -axis reports different
scores between 0 and 1. The areas highlighted in blue corre-
spond to the AUT(F1,24m). The black dash-dotted horizontal
lines represent the best F1 from the original papers [4, 22, 33],
736    28th USENIX Security Symposium
USENIX Association
Figure 5: Time decay of ALG1 [4], ALG2 [33] and DL [22]—with AUT(F1,24m) of 0.58, 0.32 and 0.64, respectively. Training
and test distribution both have 10% malware. The drop in the last 3 months is also related to lower samples in the dataset.
Algorithm 1: Tuning ϕ.
Input: Training dataset Tr
Parameters :Learning rate µ, target performance P ∈ {F1,Pr,Rec},
Output: ϕ∗P, optimal percentage of mw to use in training to achieve
max error rate Emax
the best target performance P subject to E P∗) and (Eϕ ≤Emax) then
7
8
9
10
11
12
13
14 return ϕ∗P;
P∗ ← Pϕ
ϕ∗P ← ϕ
corresponding to results obtained with 10 hold-out random
splits for ALG1, 10-fold CV for ALG2, and random split for
DL; all these settings are analogous to k-fold from a tem-
poral bias perspective, and violate both C1 and C2. The red
dashed horizontal lines correspond to 10-fold F1 obtained on
our dataset, which satisﬁes C3.
Differences in 10-fold F1. We discuss and motivate the
differences between the horizontal lines representing origi-
nal papers’ best F1 and replicated 10-fold F1. The 10-fold F1
of ALG1 is close to the original paper [4]; the difference is
likely related to the use of a different, more recent dataset.
The 10-fold F1 of ALG2 is much lower than the one in the
paper. We veriﬁed that this is mostly caused by violating C3:
the best F1 reported in [33] is on a setting with 86% malware—
hence, spatial bias increases even 10-fold F1 of ALG2. Also
violating C2 tends to inﬂate the 10-fold performance as the
classiﬁer may learn artifacts. The 10-fold F1 in DL is instead
slightly higher than in the original paper [22]; this is likely
related to a hyperparameter tuning in the original paper that
optimized Accuracy (instead of F1), which is known to be
misleading in imbalanced datasets. Details on hyperparame-