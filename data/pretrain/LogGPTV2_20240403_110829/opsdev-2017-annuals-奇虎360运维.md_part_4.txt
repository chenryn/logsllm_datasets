ntedbyKubelet,
error）{
容器化－Kubelet源码分析21
---
## Page 27
环的启动业务容器。
init_container执行完成之后，我们真正的业务容器才会被逐一启动。
(注意：当其中的init_container执行失败了，则Pod会异常，并且业务容器不会被创建)。当
了init_container。如果设置了，则会按 init_container 设置的顺序依次执行 init_container
Pod中的其它业务容器逐一启动。但是在启动真正的业务容器之前,首先会检查用户是否设置
·ensureSandboxlmageExists 检测用户是否设置了自己的pause镜像，如果没有设置则使用
22Kubelet源码分析－容器化
业务容器启动的逻辑和Pod的初始化pause容器的启动的流程基本一致。下面的代码是循
·network.SetUpPod 设置容器的网络(kubelet加载cni插件对容器的网络进行设置等)
·StartContainer 启动容器
·CreateContainer 创建容器
·makeSandboxDockerConfig 生成创建pause容器的配置信息
默认的gcr.io/google_containers/pause-amd64:3.0镜像
上面的这些操作就把我们Pod中的第一个pause容器创建并启动了。之后要做的就是把该
ds.check
err
return createResp.ID
H
esp.ID)
return "", err
Pull
the
nfig
CID,
nt（
ndboxImage
(config
9!?
error）{
---
## Page 28
·runner.Run 这个方法的主要作用就是在业务容器起来的时候，首
·StartContainer 启动业务容器
· CreateContainer 通过client.CreateContainer调用Docker engine-api创建业务容器。
·generateContainerConfig 生成业务容器的配置信息
·EnsurelmageExists 检查业务镜像是否存在，不存在则到Docker Registry或是Private
Registry拉取镜像
mogener
poo
unc
itainer
ima
err
Se
con
（m*kubeGene
turn
ge
Step4:
stepe
e
/......
kube
2:
bsu
handle
nil
execute
eate
erErr
ID
the
ceContainer(podSandboxID,
the
口
the
=
post
onta.
container.
kube
sts(pod)
bod
start hook.
kubeContainerID,
container %+v in pod
ntainer(containerID)
pod, restartcount,
container,
her(podSandboxID,
(string,
首先会执行一
容器化－Kubelet源码分析23
xConfig
error)
-个container
---
## Page 29
本文链接：https://opsdev.cn/post/kubelet.html
A：物理机。
Q：Node节点采用虚拟机还是物理机部署kubelet的?
A：我们使用prometheus对Kubernetes的各个功能组件和容器进行监控。
Q：如何对kubelet以及其它Kubernetes组件监控的？
面对面：
深层的了解了。
24Kubelet源码分析－容器化
这样Pod大体的启动流程就描述完了，但是对于kubelet中其它的中间服务,如:volume
的业务服务，否则容器异常
hook(PostStart和PreStop),做一些预处理工作。只有container kook执行成功才会运行具体
口
-扫查看文章详情
?
二
口
---
## Page 30
通信层。
的服务拓扑，可靠地传递服务之间的请求。从某种程度上说，这些代理接管了应用程序的网络
的组件，它们本身是一个有价值的网络。
将这些代理组织起来形成了一个轻量级网络代理矩阵，也就是服务网格。这些代理不再是孤立
信。这些代理通常与应用程序代码一起部署，并且它不会被应用程序所感知。Service Mesh
Envoy简介
Istio》，《从分布式到微服务，深挖Service Mesh》，了解一下Service Mesh的历史。
的，所以也就有了进一步了解和学习Service Mesh的动力。
ISERVICEMESH数据面板ENVOY简介
·蓝色部分则是sidecar
·绿色部分代表应用程序
在看本文章前，强烈建议先看一下这两篇文章《深度剖析Service Mesh服务网格新生代
1 Oct.18th 2017 BY 霍明明
微服务
服务网格是用于处理服务到服务通信的“专用基础设施层”。它通过这些代理来管理复杂
在 Service Mesh 模式中，每个服务都配备了一个代理“sidecar”
。其部署模式如图所示：
微服务－ServiceMesh 数据面板 Envoy 简介25
，用于服务之间的通
---
## Page 31
做出处理（例如：限速、TLS 认证、HTTP 连接管理、MongoDB 嗅探、TCP 代理等等）。
新的请求时，会根据关联的filters模板初始化配置这些filters，并根据这些 filters 链对这些请求
Listeners
线程来工作。每个线程都独立监听服务，并对请求进行过滤和数据的转发等。
线程模型
Envoy基础概念
Envoy用到的几个术语
都是做些什么工作。
26ServiceMesh 数据面板 Envoy 简介－微服务
·成的底层硬件，只要它们各自独立寻址。
·Host:通常我们将 Host 看做是一个具备网络通信功能的实体(可以是一台物理机，也可以是一
Envoy 会启动一个或者多个listener，监听来自 downstream 的请求。当 listener 接收到
Envoy 中真正干活的(通常是一个监听服务端口的工作线程)。
一个连接建立后，这个线程将会管理该连接的整个生命周期。通常 Envoy 是非阻塞的，
Envoy 使用单进程多线程模式。一个主线程，多个工作线程。主线程协调和管理这多个
台移动设备等等）。在Envoy 中，Host是一个逻辑网络中的应用.可能运行在由有多个主机组
Envoy是 Service Mesh 中一个非常优秀的 sidecar 的开源实现。我们就来看看 Envoy
Filter:过滤器，在Envoy 中指的是一些“可插拔”和可组合的逻辑处理层，是Envoy 核心
逻辑处理单元
Runtime configuration:Envoy 配置是热更新的，
请求的服务网格。
Mesh:在本文中"Envoy mesh" 指的是由一组 Envoy 代理组成的，为不同服务之间可靠传递
群成员是由负载均衡策略决定。通过健康检查服务来对集群成员服务状态进行检查。
Cluster:upstream 集群。Envoy 通过服务发现定位集群成员并获取服务。具体请求到哪个集
downstream的请求。
Listener:服务(程序)监听者。就是真正干活的。envoy 会暴露一个或者多个listener监听
Upstream:请求接收者(服务提供方)。
Downstream:请求发起者(服务请求方)。
Lstio 中使用的是 Envoy 的扩展版本，一些与Lstio 结合的东西在这里不介绍。
无需重启。
---
## Page 32
连接，编解码器将 HTTP/1.1的数据转换为类似于 HTTP/2 或者更高层的抽象处理。这意味着
Envoy 对 HTTP 的支持在设计之初就是一个HTTP/2的多路复用代理。对于 HTTP/1.1 类型
Envoy HTTP 连接管理原生支持HTTP/1.1, WebSockets 和 HTTP/2，暂不支持 SPDY。
HTTP 连接管理提供了三种类型的filter:
踪、请求/响应头控制、路由表管理和状态数据统计等)。
trailers等)。它还会处理一些通用的问题(比如：request日志、request ID生成和request追
接管理 filter。该 filter 将原始数据字节转换成 HTTP 协议类型数据(比如：headers、body 
Network Filter(L7)/HTTP Filter 
对连接进行处理。
目前有三种类型的 network(L3/L4）filters:
每个 listener 可以组合使用多个 filters 来处理连接数据。
Network (L3/L4) filters
的 filterS。
机器的CPU线程数）。
行一个Envoy 进程，而不关心配置了多少个listerners（如上：大多数情况listener数量等于
·Encoder:编码响应数据流时(headers,body, and trailers)调用，属于出口单方向控制.
·Decoder:解析请求数据流时(headers，body，trailers等)调用，属于入口单方向控制。
·Write:当 Envoy 向上游服务发送数据时被调用。
·Read:当 Envoy 接收来自下游服务请求数据时被调用。
Decoder/Encoder:Decoder/Encoder 用于入/出口双向控制.
HTTP 协议是当前许多服务构建的基础协议，作为核心组件，Envoy 内置了 HTTP 连
 network(L3/L4）filters 构成了Envoy连接处理的核心。在 listener 部分我们介绍过,
Read/Vrite:上面两种fileter都是单向控制，Read/Write filters 在接收来自下游服务请求数
Listener 还可以通过 listener 发现服务来动态获取。
目前 Envoy 只支持 TCP 类型的 listeners。每个 listener 都可以独立配置一些L3/L4层
Envoy 是多线程模型，支持单个进程配置任意数量的listeners。通常建议一个机器上运
据和向上游服务发送数据时被调用，是双向控制。
微服务－ServiceMesh数据面板Envoy简介
27
---
## Page 33
路由表有两种配置方式：
等的配置信息。
每个HTTP连接管理 filter 都会关联一个路由表。每个路由表会包含对 HTTP 头、虚拟主机
网格(典型的是通过对HTTP header等的处理实现到特定服务集群的转发)。
用来处理边缘流量/请求(类似传统的反向代理)，同时也可以构建一个服务与服务之间的 Envoy
HTTP路由
HTTP 连接管理支持 access log，
access log
大多数代码不用关心底层连接使用的是 HTTP/1.1 还是 HTTP/2。
28
ServiceMesh 数据面板 Envoy 简介－微服务
http-conn-
(https://envoyproxy.github.
external（只是外部请求使用）
route_config_example:
route
d
conn
"routes":
"domains":[
"name": "vho1",
reques
可以记录访问日志，且可以灵活的配置。
on
"：
mov
/6
eaderl
add
[]，
开
voy/configuration
al1（所有请求都使用）或者
/configuration
config
l#config-
---
## Page 34
router filter 支持如下功能：
RDS 是一组API用来动态获取变更后的路由配置。
·Virual host 层面的 TLS 重定向，分两类：
·通过RDS(Route discovery service)APl动态配置。
·静态配置文件。Write:当 Envoy 向上游服务发送数据时被调用。
，基于前缀和精确path的规则匹配(有的对大小写既敏感,有的不敏感)。由于 Regex/slug 会使得
：支持Virtual hosts。映射 domains/authorities 到一系列的路由规则上（和Nginx等一样）。
基于路由的优先级。
支持虚拟集群。
任意 HTTP 头匹配路由规则。