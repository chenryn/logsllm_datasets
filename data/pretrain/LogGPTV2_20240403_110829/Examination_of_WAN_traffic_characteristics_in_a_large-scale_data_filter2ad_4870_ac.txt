### Intra-DC and High-Priority Traffic Analysis

The analysis reveals that both aggregated traffic and high-priority traffic exhibit the least intra-DC locality. Specifically, the locality of high-priority traffic for AI services is significantly lower compared to the aggregated and low-priority traffic. These findings highlight the challenges in WAN traffic engineering, emphasizing the need to account for the disparity among different services when allocating bandwidth at the service level. The observations also underscore the necessity for a detailed analysis of traffic characteristics across various services, which is the primary objective of this study.

### Link Utilization Analysis

Our analysis of link utilization indicates that links carrying WAN traffic experience higher utilization rates compared to those within data centers (DCs). We also observe a balanced load distribution among the links that carry WAN traffic. Additionally, there is a strong correlation between the time series of WAN and DC traffic. These findings suggest that Equal-Cost Multi-Path (ECMP) routing is viable in our DC network due to its simplicity. They also support the idea of separating inter-DC (WAN) and intra-DC traffic into two types of switches, as opposed to the approach in [28]. This separation can help avoid interference between the two types of traffic, simplify congestion control design, and achieve low cost and high scalability.

### 4. TRAFFIC COMMUNICATION

Understanding the traffic communication patterns across DCs and clusters within DCs is crucial for WAN traffic engineering and fabric design. In this section, we examine both inter-DC (high-priority) and inter-cluster (all) traffic matrices. Our key findings include:
1. A skewed WAN traffic distribution over DC pairs, with a small set of heavy hitters contributing a significant portion of the traffic.
2. The traffic exchange pattern of these heavy hitters remains stable, leading to good predictability of the aggregated high-priority traffic.
3. The total traffic across clusters inside a DC is imbalanced and variable.

#### 4.1 Inter-DC Traffic Matrix

Communication patterns among DCs provide insights into the load on interconnect WAN and guide the design of WAN traffic engineering and bandwidth allocation. We focus particularly on high-priority traffic, as priority queuing at switches ensures sufficient capacity for high-priority traffic during resource contention [14]. Our analysis shows a skewed traffic distribution on the inter-DC traffic matrix for high-priority traffic, where 8.5% of DC pairs contribute 80% of the high-priority traffic. Moreover, the set of heavy hitters contributing 80% of the traffic remains consistent over time.

To further investigate the interaction patterns among different DCs, we calculate the degree centrality (i.e., the number of DCs each DC communicates with) and plot the distribution in Figure 6. We observe an extensive communication pattern: 85% of DCs communicate with more than 75% of the other DCs. For heavily-loaded connections (traffic volume > 1Gbps), over 50% of DCs still communicate with 40%-60% of other DCs. These results indicate that while communications are widespread among DCs, most of the traffic is concentrated on a few DC connections.

**Traffic Matrix Variation:**
We next examine the variation of the high-priority traffic matrix over time, which reflects the flux in the traffic exchange pattern among DC pairs. The change rate \( r_{TM}(t) \) of a traffic matrix \( TM \) at time \( t \) is computed as:

\[
r_{TM}(t) = \frac{|TM(t + \tau) - TM(t)|}{|TM(t)|}
\]

where the numerator is the absolute sum of the entry-wise differences between the two matrices at adjacent time intervals, and the denominator is the absolute sum of entries in \( TM(t) \), which equals the aggregated traffic \( T(t) \). For comparison, we also compute the change rate of the aggregated traffic as:

\[
r_{Agg.}(t) = \frac{|T(t + \tau) - T(t)|}{T(t)}
\]

Even if the aggregated traffic remains unchanged (\( r_{Agg.} = 0 \)), the exchange traffic pattern among DCs (measured by \( r_{TM} \)) may change significantly. For example, if \( TM \) contains two elements and \( T(t) = 4 \), \( TM(t) = [2, 2] \); at time point \( t + \tau \), if \( TM(t + \tau) = [1, 3] \), then \( r_{Agg.}(t) = 0 \), but \( r_{TM} = 2/4 = 0.5 \).

Figure 7 plots \( r_{Agg.} \) and \( r_{TM} \) on 10-minute intervals over a week (\( \tau = 10 \) minutes). Here, we only consider the heavy hitters (8.5%) that contribute 80% of the traffic. Overall, both the aggregated traffic and the traffic exchange across DCs remain stable, with the change rate below 10% for most time intervals. However, the traffic exchange pattern among DC pairs may change in some time intervals even when the aggregated traffic remains almost unchanged (\( r_{Agg.} \) close to 0). This observation indicates that even when the aggregate inter-DC traffic is stable, the traffic exchanged among DCs may vary. Additionally, the change rate of the inter-DC high-priority traffic follows a typical daily pattern, likely driven by the variation in load.

**Predictability Analysis:**
The above analysis suggests that the overall inter-DC traffic is relatively stable over time. To gain insight into how the traffic exchanged between each pair of heavy DCs changes and whether it is predictable on a 1-minute time scale, we compute the fraction of total traffic contributed by those DC pairs that have no significant change in traffic (i.e., change rate below the stability threshold \( thr \) on a 1-minute time scale) and depict the distribution in Figure 8(a), where \( thr \) is set to 5%, 10%, and 20%. Even with a stringent stability threshold of 5%, for 80% of 1-minute intervals, over 60% of the total traffic is contributed by the DC pairs that remain stable in terms of traffic; this traffic share exceeds 90% if we allow 20% of change (\( thr = 20% \)). Additionally, we find that the coefficient of variation of the high-priority traffic volume for each DC pair ranges from 0.05 to 0.82 (with a median of 0.32), consistent with the observations in [19]. These results imply the possibility of estimating the aggregated high-priority WAN traffic based on historical traffic data. Given the trend of incorporating multiple service priorities into WAN traffic engineering [7, 14], we return to studying the service-level traffic predictability and traffic estimation in Section 5.

**Persistence of Stability:**
To make full use of the network bandwidth, providers often conduct traffic scheduling during periods when traffic remains stable. We analyze the persistence of stability by examining the run-length of the sequence of minutes where the change in traffic for each DC pair remains insignificant (i.e., less than \( thr \) compared to the demand at the beginning of the sequence). Figure 8(b) shows the results. We observe that 40% of the DC pairs remain predictable for over 5 minutes when \( thr = 5% \); this percentage increases to 80% if we can tolerate 20% of change (\( thr = 20% \)).

#### 4.2 Inter-Cluster Traffic Matrix

Using the same analysis methodologies, we analyze the traffic communication among clusters in a typical DC, considering the significant amount of traffic among clusters (see Table 2). Understanding inter-cluster traffic characteristics is also useful for DC fabric design [27].

We first focus on the inter-cluster traffic matrix in a typical DC over one week. As in [27], we consider the aggregated traffic without distinguishing high-priority and low-priority traffic. This matrix shows a similar distribution to that in Facebookâ€™s DCN [27], where traffic is densely distributed among a few heavy clusters. About 80% of traffic interactions are attributed to the top 50% of cluster pairs, and the set of heavy cluster pairs remains unchanged over time. Further analysis of racks reveals that 80% of inter-cluster traffic is from communications among less than 17% of rack pairs, suggesting the viability of heterogeneous fabrics in DCs.

We plot the \( r_{Agg.} \) and \( r_{TM} \) of inter-cluster traffic in Figure 9. The aggregated traffic remains relatively stable, with a median change rate of 4.2%. However, the fluctuation in traffic exchanges is much greater, with a median change rate of about 16.3%, indicating dynamic traffic exchanges among different cluster pairs. This is possibly because the interconnect within a DC is often abundant, and traffic within a DC is not well scheduled (i.e., more dynamic).

**Predictability Analysis:**
Finally, we examine the predictability of inter-cluster (but intra-DC) traffic. Figure 10(a) plots the distribution of the fraction of total traffic contributed by those cluster pairs that have no significant change in traffic on a 1-minute time scale. We use three different change rate thresholds (5%, 10%, and 20%) to identify those pairs experiencing no significant traffic change. For 80% of 1-minute intervals, about 45% of the total traffic is contributed by the cluster pairs with a change rate less than 10% (\( thr = 10% \)). However, less than 10% of the cluster pairs remain predictable for over 5 minutes with this moderate change rate threshold (see Figure 10(b)). This observation implies relatively low stability in inter-cluster traffic exchanges.

#### 4.3 Summary and Implications

Our analysis reveals a highly biased WAN (high-priority) traffic distribution towards a small portion (8.5%) of DC pairs. Overall, the aggregated high-priority inter-DC (WAN) traffic and the traffic exchange patterns among DCs remain stable over time, resulting in good predictability of overall traffic demands. The inter-cluster (all) traffic also shows an imbalanced distribution, especially at the rack-level communication. We also observe relatively low stability in inter-cluster traffic.

The biased distribution of WAN traffic suggests unbalanced resource allocation, with more resources allocated to the stable heavy hitters. The good stability of aggregated WAN traffic between DC pairs implies that bandwidth should be allocated based on historical traffic volume, as proposed in [14, 16]. However, providers like Baidu, which host hundreds of services, prefer to allocate WAN bandwidth at the service level [22]. We will return to studying service-level traffic prediction in the next section. The observation of imbalanced inter-cluster traffic suggests a heterogeneous fabric design. Additionally, network fabric needs to incorporate randomness in forwarding path selection for individual flows to cope with the dynamics of inter-cluster traffic, as suggested in [11].

### 5. WAN TRAFFIC CHARACTERISTICS OF SERVICES

This section examines traffic at the service level, motivated by the disparity of traffic patterns observed in the previous analysis. We study the interaction patterns and temporal correlations among services from the perspective of traffic volume in WAN. We observe different interaction patterns and high temporal correlations among services. We then investigate the predictability of high-priority inter-DC traffic for different services. Finally, given the differences in traffic predictability across services, we examine the WAN traffic prediction accuracy at the service level using widely used methods in SD-WAN solutions.

#### 5.1 Service Interaction

**Interaction Pattern:**
We first examine the service interaction pattern across DCs from the perspective of traffic volume. While many services interact with each other, the traffic distribution over services in WAN is highly skewed: 16% of services generate 99% of WAN traffic. The interaction matrix of individual services is also very sparse: as few as 0.2% of service pairs account for over 80% of traffic, and 20% of traffic comes from the interaction of services with themselves.

For each category of services, we further inspect the traffic shares among its interacted services and present the results in Table 3. For the traffic generated from each type of source service, we compute its distribution over all types of destination services (i.e., each row in Table 3). For example, 28% of the traffic generated by Web services is destined to Computing services. Three notable observations are:
1. Web, DB, and Cloud services have the most extensive self-interactions; they either sync data (e.g., web search indexes) among DCs [35] (thus generating low-priority traffic) or use geo-distributed replicas to collaboratively serve Internet users (thus generating high-priority traffic). Other types of services are less likely to be called by services of their own types, particularly FileSystem services.
2. Web and Computing services have considerable interactions with other services, as evidenced by the considerable fractions of traffic from other types of services towards them. This is possibly because Web is the dominant application in this DCN, and Computing services are the foundation for other services.
3. The two newer types of services, Analytics and AI, also interact with all other types of services fairly frequently, becoming new foundations for other services.

We also studied the service interaction pattern for high-priority traffic across DCs, as shown in Table 4. Recall that high-priority traffic serves Internet-facing requests and is delay-sensitive. Compared with the total traffic shares, we observe much more extensive self-interactions for the high-priority traffic of Web, DB, and Cloud services. Additionally, because Web services are less likely to use Computing services to fulfill requests from Internet users, the traffic share of Computing services to Web services drops significantly (from 40.3% to 16.6%). On the other hand, Analytics services are more likely to communicate with Computing services when serving Internet-facing requests (the ratio increases from 15.5% for all traffic to 33.9% for high-priority traffic). Overall, Web, Computing, Analytics, and AI services still have considerable interactions with other types of services to respond to Internet-facing requests.

**Low Rank of Service Traffic Matrix:**
Extensive traffic interactions of diverse services among DCs may lead to distinct traffic characteristics of services in WAN networks. Although the overall inter-DC traffic has diurnal patterns, specific services may hold different characteristics. Understanding the temporal correlations between different services is crucial for developing better strategies for service deployment and traffic scheduling. To this end, we form the temporal traffic matrix for individual services \( M = [m_1, m_2, \ldots, m_n] \), where \( m_i \in \mathbb{R}^l \) is the traffic volume of the i-th service in 10-minute intervals in a day (\( l = 144 \)), i.e., the j-th element of \( m_i \) represents the traffic volume in the j-th 10-minute interval.

[Table 3: Service interaction among DCs (over WAN) from the perspective of aggregated traffic (including both high-priority and low-priority traffic): traffic volume interacted among different types of services; normalized by the total traffic from source services.]

| **Dst Service** | **Src Service** | **Web** | **Computing** | **Analytics** | **DB** | **Cloud** | **AI** | **FileSystem** | **Map** | **Security** |
|-----------------|-----------------|---------|---------------|---------------|--------|----------|-------|----------------|---------|--------------|
| **Web**         |                 | 51.7    | 40.3          | 15.5          | 18.7   |          |       |                |         |              |

[Table 4: Service interaction for high-priority traffic across DCs: traffic volume interacted among different types of services; normalized by the total traffic from source services.]