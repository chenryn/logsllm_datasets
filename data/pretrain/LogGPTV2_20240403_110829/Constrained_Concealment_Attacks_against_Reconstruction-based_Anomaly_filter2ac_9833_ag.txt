Iterative
Learning based
0
0.07
0.14
0.07
0.14
0.31
Table 3: Average required time (in seconds) to manipulate
sensor readings. â€˜Replayâ€™ column is empty as replay attacks
do not require computation. â€™Iterativeâ€™ and â€˜Learning basedâ€™
columns report the mean and std deviation required to com-
pute the manipulation sensor readings at a given time step.
Iterative
Computational time, mean(ğœ‡ Â¯ğ‘¥ ) and std(ğœ Â¯ğ‘¥ )
Replay
Learning based
ğœ‡ Â¯ğ‘¥ [s]
0.002
0.005
ğœ‡ Â¯ğ‘¥ [s]
2.28
0.60
ğœ Â¯ğ‘¥
0.005
0.002
ğœ Â¯ğ‘¥
2.46
0.41
-
-
Data
B
W
To perform and evaluate the learning based attacks, we trained 83
models with BATADAL data and 63 for WADI. In the unconstrained
case, for the BATADAL dataset (43 variables), we train an autoen-
coder with 64 and 128 units for the first/third and second hidden
layers, respectively, training requires 18 epochs (2 seconds/epoch),
for a total of 36 seconds to train the model; for the WADI dataset
(82 variables), we use 128 and 256 units, training required 7 epochs
(64 seconds/epoch), for a total of 488 seconds to train the model.
,
5.4 Unconstrained Concealment Attack
In this experiments, we assume the Unconstrained attacker (D,X)
that is able to read and control all the reported sensor readings, in
the White box (ğ‘“ , ğ‘¤) and Black Box(
) scenarios. We discuss
the results of our evaluation of the detector for both datasets in
several scenarios (see Table 2). We evaluated the performance of our
concealment attacks over the time steps with ground truth â€˜under
attackâ€™ labels only, i.e., we exclude normal operation data time steps
from the computation of Recall for this attack evaluation.
The first row of Table 2 reports the average results obtained with
the three different attack strategies. In this setting, the replay attack
is giving 0 Recall over the replayed sensor readings. This means
that when the attacker can manipulate all the sensor readings,
the anomaly detector is no more able to spot the attack occurring
over the physical process. Considering the iterative and learning
based approaches, we notice that the Recall is 0.14, this represents
a significant drop in detector performance, but not as effective as
the replay of all sensors.
ACSAC2020,December7â€“11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:ACSAC2020,December7â€“11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAË†f,ZË†w).Attackerâ€™sXConstraintsDReadWriteUnconstrainedÂ§5.4(cid:32)(cid:32)(cid:32)XPartiallyÂ§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFullyÂ§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)DÂ§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)Î´tominimizethereconstructionerrorbetweentheinput(cid:174)x+Î´andoutputË†(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(Ë†xiâˆ’(xi+Î´i))2s.t.(cid:174)Î´âˆˆconstraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)Î´)=â€˜safeâ€™(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattackerâ€™sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:â€¢Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.â€¢FeaturesPartiallyConstrained(D,Ë†X),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.â€¢FeaturesFully-Constrained(D,Ë†X),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.â€¢DataConstrained(Ë†D,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:â€¢Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).â€¢Blackbox(AAË†f,ZË†w),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(Ë†f,Ë†w).OurattackerdoesnotrequiretheknowledgeofforitsapproximationË†f.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=full,ACSAC2020,December7â€“11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAË†f,ZË†w).Attackerâ€™sXConstraintsDReadWriteUnconstrainedÂ§5.4(cid:32)(cid:32)(cid:32)XPartiallyÂ§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFullyÂ§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)DÂ§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)Î´tominimizethereconstructionerrorbetweentheinput(cid:174)x+Î´andoutputË†(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(Ë†xiâˆ’(xi+Î´i))2s.t.(cid:174)Î´âˆˆconstraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)Î´)=â€˜safeâ€™(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattackerâ€™sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:â€¢Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.â€¢FeaturesPartiallyConstrained(D,Ë†X),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.â€¢FeaturesFully-Constrained(D,Ë†X),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.â€¢DataConstrained(Ë†D,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:â€¢Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).â€¢Blackbox(AAË†f,ZË†w),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(Ë†f,Ë†w).OurattackerdoesnotrequiretheknowledgeofforitsapproximationË†f.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,AAË†fZË†wAttackerâ€™sXConstraintsDReadWriteUnconstrainedÂ§5.4(cid:32)(cid:32)(cid:32)XPartiallyÂ§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFullyÂ§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)DÂ§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)Î´tominimizethereconstructionerrorbetweentheinput(cid:174)x+Î´andoutputË†(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(Ë†xiâˆ’(xi+Î´i))2s.t.(cid:174)Î´âˆˆconstraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)Î´)=â€˜safeâ€™(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattackerâ€™sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:â€¢Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.â€¢FeaturesPartiallyConstrained(D,Ë†X),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.â€¢FeaturesFully-Constrained(D,Ë†X),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.â€¢DataConstrained(Ë†D,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:â€¢Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).â€¢Blackbox(AAË†f,ZË†w),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(Ë†f,Ë†w).OurattackerdoesnotrequiretheknowledgeofforitsapproximationË†f.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,ACSAC2020,December7â€“11,2020,Austin,USAA.Erbaetal.whereTPstandsforTruePositiveandFNforFalseNegative.Recallmeasurestherateofcorrectlyclassifiedpositiveinstances.Whentheanomalydetectoristuned,ahigherRecallmeansthattheanom-alydetectoriscorrectlyretrievinganomalies.Theattackerâ€™scon-cealmentgoalcanbeexpressedintermsofRecall:theconcealmentattackissuccessfulifthedetectorRecallovertheconcealedtuplesgoesto0.TheclosertheRecallcomesto0,thehighertheamountofmisclassifiedtuples.Notethatwelaunchourconcealmentattacksovertheinstancesofanomalousdata,i.e.,datareportinggroundtruthâ€˜underattackâ€™.BothiterativeandlearningbasedattacksareimplementedusingPython3.7.1;neuralnetworksareimplementedandtrainedusingKeras2.3.1withTensorFlow1.11.0backend.Experimentsusealap-topequippedwithInteli7-7500UCPU,16GBofRAM,andNVIDIAGeForce940MXGPU4GB.TrainingofAttackDetector.ForbothBATADALandWADI,wetrainedthethirdpartyattackdetector[51]onsensorreadingsoccurringduringnormaloperationaldata.FortheBATADALdataset(wheresensorreadingsaresampledevery15minutes),wefoundthatparameterwindow=3quarterofhoursisareasonabledecisionboundarytoflagcorrectlyat-tacksanddonotraiseFalsealarms.ThisgivesaAccuracy=0.93,Precision=0.90,Recall=0.60,FPR=0.01.Changingthewindowparameter,wecanincreasetheRecallatthepriceofdecreasingPrecisionthatmeansraisingahighernumberofFalseAlarms.FortheWADIdataset,wefoundthatparameterwindow=60sec-ondsisareasonabledecisionboundarytoflagcorrectlyattacksanddistinguishthemfromFalsePositives.ThisgivesaAccuracy=0.97,Precision=0.77,Recall=0.68,FPR=0.01.ResultsareinlinewiththecurrentstateoftheartdetectionovertheBATADALdataset[51]andWADI[16].Replayattack.Inthisattack,theattackerreplaysforthewholedurationofthephysicalmanipulation,usingthesensorreadingsasrecordedatthesamehoursdaysbefore(assumingthatprocessoperationsareoftenperiodicwithin24h).sischosentoletthereplaycontainonlynormaloperationsdata.Forexample,givenaphysicalmanipulationthatlasts50hours,wereplaysensorreadingsashappened72hoursearlier.Iterativeattack.Theattackermanipulatesvariablesrequiredtofindasolution(accordingtothetwostoppingcriteriaintroducedinSection4.3andconstraintsovermodifiablesensorreadings).ForBATADALdataset,wetunedthetwostoppingcriteriaviagridsearchtoguaranteeatrade-offbetweenthedecreaseofde-tectionaccuracyandcomputationaltime.Specificallyweselectedpatience=15andthebudget=200.ForWADIdataset,theiterativeparameters(followingthesamerationalasinBATADALcase)wechoosearepatience=40andthebudget=300.Theresultofthisexperimentdependsonthedetectionmechanism.Theattackerisusingtheoracletodetermineiftheconcealmentissuccessful.Learningbasedattack.Forthelearningbasedattack,theattackerusesanautoencoder(AE)asthegeneratorandsendspredictedread-ingstotheSCADA.Accordingtotheattackerâ€™sconstraints,wetrainanautoencoderoverthereadablefeatures.Weusedsigmoidasac-tivationfunction,Gorlotinitialization[19]asweightsinitializerandmeansquarederroraslossfunction.Moreover,wesplitthedataintrain23andvalidation13,useearlystopping[29]toavoidTable2:DetectorRecall(BATADAL(B)andWADI(W)datasets),beforeandafterunconstrainedconcealmentat-tacks.Thecolumnâ€˜Originalâ€™referstothedetectionRecalloverthedatawithoutconcealment;â€˜Replayâ€™,reportstheRe-callafterreplayattack,whileâ€˜Iterativeâ€™andâ€˜Learningbasedâ€™columnsreporttheRecallafterourproposedadversarialconcealmentattacks.DetectionRecallDataOriginalReplayIterativeLearningbasedB0.6000.140.14W0.680.070.070.31Table3:Averagerequiredtime(inseconds)tomanipulatesensorreadings.â€˜Replayâ€™columnisemptyasreplayattacksdonotrequirecomputation.â€™Iterativeâ€™andâ€˜Learningbasedâ€™columnsreportthemeanandstddeviationrequiredtocom-putethemanipulationsensorreadingsatagiventimestep.Computationaltime,mean(ÂµÂ¯x)andstd(ÏƒÂ¯x)ReplayIterativeLearningbasedDataÂµÂ¯x[s]ÏƒÂ¯xÂµÂ¯x[s]ÏƒÂ¯xB-2.282.460.0020.005W-0.600.410.0050.002overfittingandreducelearningrateonplateaus[30].Dependingontheconstrainedscenario(i.e.,thefeaturesthattheattackercanreadXortheamountofdatathatshespoofedË†D),theadversari-allytrainedautoencoderhasadifferentnumberofinputneurons.Givennasinput/outputdimension,theautoencoderiscomposedof3hidden-layerswithrespectively2n,4n,2nneurons.Toper-formandevaluatethelearningbasedattacks,wetrained83modelswithBATADALdataand63forWADI.Intheunconstrainedcase,fortheBATADALdataset(43variables),wetrainanautoencoderwith64and128unitsforthefirst/thirdandsecondhiddenlayers,respectively,trainingrequires18epochs(2seconds/epoch),foratotalof36secondstotrainthemodel;fortheWADIdataset(82variables),weuse128and256units,trainingrequired7epochs(64seconds/epoch),foratotalof488secondstotrainthemodel.5.4UnconstrainedConcealmentAttackInthisexperiments,weassumetheUnconstrainedattacker(D,X)thatisabletoreadandcontrolallthereportedsensorreadings,intheWhitebox(f,w)andBlackBoxAAË†fAË†xscenarios.Wediscusstheresultsofourevaluationofthedetectorforbothdatasetsinseveralscenarios(seeTable2).Weevaluatedtheperformanceofourconcealmentattacksoverthetimestepswithgroundtruthâ€˜underattackâ€™labelsonly,i.e.,weexcludenormaloperationdatatimestepsfromthecomputationofRecallforthisattackevaluation.ThefirstrowofTable2reportstheaverageresultsobtainedwiththethreedifferentattackstrategies.Inthissetting,thereplayattackisgiving0Recalloverthereplayedsensorreadings.Thismeansthatwhentheattackercanmanipulateallthesensorreadings,Constrained Concealment Attacks against Reconstruction-based detectors in ICS
ACSAC 2020, December 7â€“11, 2020, Austin, USA
The second row of Table 2 refers to concealment attacks over the
WADI dataset. The result over this dataset shows that the replay
attack can hide the anomaly occurring over the CPS. The perfor-
mance of the iterative equals the one of the replay attack. Finally,
the learning based approach is underperforming the other meth-
ods. Despite this, the detectorâ€™s Recall reduces more than 50% after
learning based manipulation.
(cid:77)Computational Time Table 3 reports the average time required
to compute the adversarial examples. In contrast to iterative and
learning based, the replay attack does not require computation. The
iterative approach requires an amount of time that depends on the
algorithm computational budgets. The black box approach requires
a constant amount of time since it consists of a neural network
prediction. Given our real-time constraints of adversarial examples
computation (i.e., target time within milliseconds), we can conclude
that learning based approach easily meets the requirements. In the
BATADAL case (where the sampling time is 15 minutes), we do
not require more than 2ğ‘šğ‘  on average to compute an adversarial
example. In the WADI case (where sampling time is 1 second), on
average, we do not require more than 5ğ‘šğ‘  to compute an adversarial
example. The iterative attack is slower, but on average, still below
the sampling intervals.
(cid:77)Summary of Unconstrained Attacks findings When the attacker
is free to manipulate all the sensor readings, results show that replay
attacks hide anomalies occurring over the physical process. First, a
replay attack does not require computation to find the manipulated
set of sensor readings; second, the attacker does not need to be
aware of the detection mechanism; and third, the considered anom-
aly detector Recall goes to zero since the replayed sensor readings
do not contain (additional) anomalies. White box, even though it
achieves valuable results, requires computation, and the attacker
needs to be omniscient w.r.t. the defense mechanism. We note that
the learning based attack can decrease the detectorâ€™s Recall without
having access to detectorâ€™s oracle, with low computational effort
(after training) and the same knowledge w.r.t. the attacked model
as the replay attack.
5.5 Constrained Concealment Attack
In the previous subsection, we found that full replay attacks can
be a powerful and low-cost way to evade anomaly detectors if
all features can be replayed. In this section, we demonstrate the
impact of constraints on the attacker, e.g., if the attacker can only
control a subset of the reported sensor values. Specifically, we
perform Partially, Fully, and Data constrained attacks as modeled
in Section 3.2 and show how our proposed iterative and learning
based outperform replay attacks.
Partially Feature-Constrained attack, (D, Ë†X). Figure 2 reports
the average result of the constrained attacks over BATADAL and
WADI datasets with an best-case selection of constraints. Due to
space limitations, the constraint selection can be found in Appen-
dix B. In the case of the BATADAL dataset, we note that the replay
attack does not cope well with constraints. Since the anomaly detec-
tor can spot the presence of contextual anomalies, the replay of only
ğ‘˜ features results in alarms, with an average detection Recall higher
than in the benign case (i.e., no replay of sensors applied), the value
decreases when 40 out of 43 sensors are replayed. In the case of it-
erative and learning based attacks, we can notice that the detection
Recall is always lower than the original Recall. In the iterative case,
Recall decreases with the number of features that can be modified.
Learning-based attack Recall is not monotonically decreasing with
the number of features that can be modified. Specific constraint
sets better match the physical rules learned by the detector and
allow the creation of more effective adversarial examples. In the
case of WADI, we can observe that the replay attack can diminish
the detectorâ€™s Recall, especially when the attacker manipulates 3
or more features. The iterative based attack can achieve the same
Recall as if in the Unconstrained Attack case when manipulating 15
out of 82 features. In the case of learning based attack, results show
that for 3 manipulated (best-case) features, the attack performs
slightly better than in the unconstrained case.
In the case of topology-based constraints, the attacker controls
the sensors connected to 1 PLC in the network. We found that in the
BATADAL case, Recall is reduced to 0.36 with the replay attack and
0.34 with the iterative and learning-based attack. In the WADI case,
Recall increases to 0.64 with the replay attack while it is reduced
to 0.12 in the iterative attack and 0.36 in the learning-based attack.
In addition, in this case the iterative and learning-based approach
overcomes the limitations of constrained replay attacks, especially
in the case of the WADI dataset (Table 7 in the Appendix reports the
numerical Recall scores found in the different constrained settings).
Fully Feature-Constrained Attacker, (D, Ë†X). In the case of the
fully constrained attacker, Replay and Iterative attack approach do
not change, since those two methods do not exploit correlations
among features to output the perturbations. The learning-based
attack is the only one affected by these constraints, i.e., the adver-
sarial network is trained on the constrained set of sensor readings.
We launched this attack with the topology based constraints, and