FROM v_power_per_day
GROUP BY system_id;
The values for the sum are equal, but the years are now columns, no more individual
groups and your fictive boss can now view that data like they are used todo in their
spreadsheet program:
┌───────────┬────────────────────┬────────────────────┐
│ system_id │ kWh in 2019 │ kWh in 2020 │
│ int32 │ double │ double │
├───────────┼────────────────────┼────────────────────┤
│ 10 │ 1549.280000000001 │ 677.1900000000003 │
│ 34 │ 205742.59999999992 │ 101033.75000000001 │
│ 1200 │ 62012.109999999986 │ 30709.329999999998 │
└───────────┴────────────────────┴────────────────────┘
There’s one downside to it: the columns are essentially hardcoded, and you need to
revisit that query every time a year gets added. If you are sure that your desired set of
columns is constant, or you find yourself targeting other databases that might not
support any other form of pivoting, the static approach might be the right solution for
you.
To solve this problem with DuckDB, use the PIVOT clause instead. The PIVOT clause in
DuckDB allows dynamically pivoting tables on arbitrary expressions.
Listing 4.26 Using DuckDBs PIVOT statement
PIVOT (FROM v_power_per_day) -- #1
ON year(day) -- #2
USING sum(kWh); -- #3
#1 You can omit the FROM if you want to select all columns, but we included it to demonstrate that this can actually be a full
SELECT.
#2 All distinct values from this expression are turned into columns
#3 The aggregate to be computed for the columns
© Manning Publications Co. To comment go to liveBook
98
Figure 4.4 Pivoting the power values on the year
The result matches exactly what we statically constructed in 4.25: Years as columns and
systems as rows with the sum of the power produced by a system and year as
intersection of row and column.
┌───────────┬────────────────────┬────────────────────┐
│ system_id │ 2019 │ 2020 │
│ int32 │ double │ double │
├───────────┼────────────────────┼────────────────────┤
│ 10 │ 1549.280000000001 │ 677.1900000000003 │
│ 34 │ 205742.59999999992 │ 101033.75000000001 │
│ 1200 │ 62012.109999999986 │ 30709.329999999998 │
└───────────┴────────────────────┴────────────────────┘
© Manning Publications Co. To comment go to liveBook
99
In case you are using an aggregate for the "cell" values, all columns that are not part of
the ON clause will be used as a grouping key for the aggregate. However, you do not
need to use an aggregate. PIVOT v_power_per_day ON day will produce a result of
1382 rows and 545(!) columns. Why is that? v_power_per_day contains 1382 distinct
values of (system_id, kWh), which make up the rows. The system has been asked to
create a column using the day, not year(day) and there are 543 different days
recorded. The two additional columns are the system_id and the kWh column. What’s in
the cells? Many, many zeros and a couple of ones. Without the USING clause DuckDB will
fill the cells with zeros for days that didn’t have the specific value, and ones for days that
had. So in case you are actually interested in a tabular view of all days, you might want
to use the first aggregate like this in such a case:
PIVOT (
FROM v_power_per_day WHERE day BETWEEN '2020-05-30' AND '2020-06-02'
)
ON DAY USING first(kWh);
Note that we deliberately chose to select only a couple of days instead of trying to print
several hundred columns. The above query pivots this result on the day:
┌───────────┬────────────┬────────┐
│ system_id │ day │ kWh │
│ int32 │ date │ double │
├───────────┼────────────┼────────┤
│ 1200 │ 2020-05-30 │ 280.4 │
│ 1200 │ 2020-05-31 │ 282.25 │
│ 1200 │ 2020-06-01 │ 288.29 │
│ 1200 │ 2020-06-02 │ 152.83 │
│ · │ · │ · │
│ · │ · │ · │
│ · │ · │ · │
│ 10 │ 2020-05-30 │ 4.24 │
│ 10 │ 2020-05-31 │ 3.78 │
│ 10 │ 2020-06-01 │ 4.47 │
│ 10 │ 2020-06-02 │ 5.09 │
├───────────┴────────────┴────────┤
│ 12 rows (8 shown) 3 columns │
└─────────────────────────────────┘
into a tabular view that would make any spreadsheet artist happy:
© Manning Publications Co. To comment go to liveBook
100
┌───────────┬────────────┬────────────┬────────────┬────────────┐
│ system_id │ 2020-05-30 │ 2020-05-31 │ 2020-06-01 │ 2020-06-02 │
│ int32 │ double │ double │ double │ double │
├───────────┼────────────┼────────────┼────────────┼────────────┤
│ 10 │ 4.24 │ 3.78 │ 4.47 │ 5.09 │
│ 34 │ 732.5 │ 790.33 │ 796.55 │ 629.17 │
│ 1200 │ 280.4 │ 282.25 │ 288.29 │ 152.83 │
└───────────┴────────────┴────────────┴────────────┴────────────┘
All queries above are using the proprietary DuckDB variant of PIVOT. DuckDB’s syntax
makes writing pivot statements much easier and less error-prone as it completely
eliminates any static enumeration of the rows on which the table should be pivoted.
DuckDB also supports a more standard SQL form of PIVOT. However, the support for the
PIVOT clause wildly differs across different databases systems, and it is unlikely that
other possible target databases have the exact same flavour of the standard. Therefore,
we would rather use the proprietary syntax in this case, which is easier to read than
hoping for more portable SQL.
In DuckDB it is perfectly possible to compute multiple aggregates in the USING clause
as well as using multiple columns for pivoting. We could use this to not only compute the
total production per year (which is the sum of all days) but also add two additional
columns that highlight the best day:
PIVOT v_power_per_day
ON year(day)
USING round(sum(kWh)) AS total, max(kWh) AS best_day;
We rounded the totals so that the result is more readable:
┌───────────┬────────────┬───────────────┬────────────┬───────────────┐
│ system_id │ 2019_total │ 2019_best_day │ 2020_total │ 2020_best_day │
│ int32 │ double │ double │ double │ double │
├───────────┼────────────┼───────────────┼────────────┼───────────────┤
│ 10 │ 1549.0 │ 7.47 │ 677.0 │ 6.97 │
│ 34 │ 205743.0 │ 915.4 │ 101034.0 │ 960.03 │
│ 1200 │ 62012.0 │ 337.29 │ 30709.0 │ 343.43 │
└───────────┴────────────┴───────────────┴────────────┴───────────────┘
© Manning Publications Co. To comment go to liveBook
101
4.8 Using the ASOF JOIN ("as of")
Imagine you are selling a volatile product at arbitrary times of the day. You are able to
predict prices at an interval, let’s say 15 minutes, but that’s as precise as you can go. Yet
people demand your product all the time. This might lead to the following fictive
situation. The query in listing 4.27 generates two CTEs: a fictive price table with 4 entries
for an hour of a random day, as well as a sales table with 12 entries. It then joins them
naively together and instead of the prices of 12 sales, you find only 4 results:
Listing 4.27 Using an inner join for timestamps
WITH prices AS (
SELECT range AS valid_at,
random()*10 AS price
FROM range(
'2023-01-01 01:00:00'::timestamp,
'2023-01-01 02:00:00'::timestamp, INTERVAL '15 minutes')
),
sales AS (
SELECT range AS sold_at,
random()*10 AS num
FROM range(
'2023-01-01 01:00:00'::timestamp,
'2023-01-01 02:00:00'::timestamp, INTERVAL '5 minutes')
)
SELECT sold_at, valid_at AS 'with_price_at', round(num * price,2) as price
FROM sales
JOIN prices ON prices.valid_at = sales.sold_at;
© Manning Publications Co. To comment go to liveBook
102
Figure 4.5 Inner join of time series data gone wrong
Sales are sad, as clearly indicated by this res ult and represented in figure 4.5:
© Manning Publications Co. To comment go to liveBook
103
┌─────────────────────┬─────────────────────┬────────┐
│ sold_at │ with_price_at │ price │
│ timestamp │ timestamp │ double │
├─────────────────────┼─────────────────────┼────────┤
│ 2023-01-01 01:00:00 │ 2023-01-01 01:00:00 │ 21.17 │
│ 2023-01-01 01:15:00 │ 2023-01-01 01:15:00 │ 12.97 │
│ 2023-01-01 01:30:00 │ 2023-01-01 01:30:00 │ 44.61 │
│ 2023-01-01 01:45:00 │ 2023-01-01 01:45:00 │ 9.45 │
└─────────────────────┴─────────────────────┴────────┘
Enter the ASOF JOIN: The ASOF JOIN (as in "as of") is a join clause that joins on
inequality, picking a "good enough" value for the gaps where the join columns are not
exactly equal. Going back to listing 4.27 we must change two things: replacing the JOIN
keyword with ASOF JOIN, and provide an inequality operator. The inequality condition
prices.valid_at <= sales.sold_at below means all prices that have been valid
before or at the point of sales can be used to compute the total price.
Listing 4.28 Using an "as of" join for timestamps
WITH prices AS (
SELECT range AS valid_at,
random()*10 AS price
FROM range(
'2023-01-01 01:00:00'::timestamp,
'2023-01-01 02:00:00'::timestamp, INTERVAL '15 minutes')
),
sales AS (
SELECT range AS sold_at,
random()*10 AS num
FROM range(
'2023-01-01 01:00:00'::timestamp,
'2023-01-01 02:00:00'::timestamp, INTERVAL '5 minutes')
)
SELECT sold_at, valid_at AS 'with_price_at', round(num * price,2) as price
FROM sales
ASOF JOIN prices -- #1
ON prices.valid_at <= sales.sold_at; -- #2
#1 Specify the join to be ASOF
#2 Note the <= in contrast to the = in listing 4.27
Note how DuckDB picks the price that is closest to the point in time of the sales as shown
below. Also, we do get the 12 expected rows now:
© Manning Publications Co. To comment go to liveBook
104
┌─────────────────────┬─────────────────────┬────────┐
│ sold_at │ with_price_at │ price │
│ timestamp │ timestamp │ double │
├─────────────────────┼─────────────────────┼────────┤
│ 2023-01-01 01:00:00 │ 2023-01-01 01:00:00 │ 1.59 │
│ 2023-01-01 01:05:00 │ 2023-01-01 01:00:00 │ 3.56 │
│ 2023-01-01 01:10:00 │ 2023-01-01 01:00:00 │ 2.71 │
│ 2023-01-01 01:15:00 │ 2023-01-01 01:15:00 │ 29.12 │
│ 2023-01-01 01:20:00 │ 2023-01-01 01:15:00 │ 14.92 │
│ 2023-01-01 01:25:00 │ 2023-01-01 01:15:00 │ 4.83 │
│ 2023-01-01 01:30:00 │ 2023-01-01 01:30:00 │ 2.84 │
│ 2023-01-01 01:35:00 │ 2023-01-01 01:30:00 │ 3.84 │
│ 2023-01-01 01:40:00 │ 2023-01-01 01:30:00 │ 4.95 │
│ 2023-01-01 01:45:00 │ 2023-01-01 01:45:00 │ 23.1 │
│ 2023-01-01 01:50:00 │ 2023-01-01 01:45:00 │ 30.07 │
│ 2023-01-01 01:55:00 │ 2023-01-01 01:45:00 │ 11.6 │
├─────────────────────┴─────────────────────┴────────┤
│ 12 rows 3 columns │
└────────────────────────────────────────────────────┘
© Manning Publications Co. To comment go to liveBook
105
Figure 4.6 Using ASOF JOIN to join all timestamps together that don’t have an exact match, too
The ASOF JOIN is often used to work with tim e series data such as stock quotes, prices,
or IoT sensors. In our example, it can be used to join the changing selling prices with the
readings from the systems to compute the prices at any given point in time. The last
example below uses our photovoltaic example data again, applying the same logic to
pick a valid price. It then demonstrates that the ASOF JOIN can be used with other
constructs we learned in this chapter, such as using a window to accumulate the running
total earnings in a sales period with different prices:
© Manning Publications Co. To comment go to liveBook
106
Listing 4.29 Using an ASOF JOIN together with a window function to compute earnings over different
price periods
SELECT power.day,
power.kWh,
prices.value as 'ct/kWh',
round(sum(prices.value * power.kWh)
OVER (ORDER BY power.day ASC) / 100, 2)
AS 'Accumulated earnings in EUR'
FROM v_power_per_day power
ASOF JOIN prices
ON prices.valid_from <= power.day
WHERE system_id = 34
ORDER BY day;
The result shows the day, the amount of kWH produced, the price in ct per kWH on that
day, and the accumulated sum of the product of power produced and price:
┌────────────┬────────┬──────────────┬─────────────────────────────┐
│ day │ kWh │ ct/kWh │ Accumulated earnings in EUR │
│ date │ double │ decimal(5,2) │ double │
├────────────┼────────┼──────────────┼─────────────────────────────┤
│ 2019-01-01 │ 471.4 │ 11.47 │ 54.07 │
│ 2019-01-02 │ 458.58 │ 11.47 │ 106.67 │
│ 2019-01-03 │ 443.65 │ 11.47 │ 157.56 │
│ 2019-01-04 │ 445.03 │ 11.47 │ 208.6 │
│ · │ · │ · │ · │
│ · │ · │ · │ · │
│ · │ · │ · │ · │
│ 2020-06-23 │ 798.85 │ 9.17 │ 31371.86 │
│ 2020-06-24 │ 741.15 │ 9.17 │ 31439.83 │
│ 2020-06-25 │ 762.6 │ 9.17 │ 31509.76 │
│ 2020-06-26 │ 11.98 │ 9.17 │ 31510.86 │
├────────────┴────────┴──────────────┴─────────────────────────────┤
│ 543 rows (8 shown) 4 columns │
└──────────────────────────────────────────────────────────────────┘
© Manning Publications Co. To comment go to liveBook
107
DuckDB is positioned as an OLAP database with a broad range of use cases. Dealing with
time-series data is certainly one of them and the ASOF join is part of that. Regardless of
the domain, which be anything from the sensor readings in our example to patient heart
rate monitoring and fluctuations in the stock market, values recorded at a certain time
are often enriched by joining them with specific key values that have been valid for a
time. Having support for ASOF enables all scenarios in which timestamps are not aligned
perfectly well.
4.9 Using Table functions
Most functions in SQL take parameters and return a single value. Table functions, on the
other hand, don’t just return a single value, they return a collection of rows. As such,
they can appear everywhere where a table can appear. Depending on the function, they
can access external resources such as files or URLs and turn them into relations that are
part of standard SQL statements. DuckDB is not the only relational database supporting
the concept of table-producing functions, but it comes with an impressive set of table
functions catering to many use cases. A list of all table functions in your DuckDB
installation can be retrieved via the following statement which uses a table function
named duckdb_functions()
Listing 4.30 Getting a list of all available table functions
SELECT DISTINCT ON(function_name) function_name
FROM duckdb_functions() -- #1
WHERE function_type = 'table'
ORDER BY function_name;
#1 The FROM clause is the most common place to call a table-producing function.
In the examples in this chapter and during the ingestion of data we have made already
extensive use of read_csv*, read_parquet and others. Additional extensions, such as
the spatial extension, add to the list of table functions that read external resources and
produce relational data.
range(start, stop) and generate_series(start, stop) are some really cool
table functions. Both functions create a list of values in the range between start and
stop. The start parameter is inclusive. For the range function, the stop parameter is
exclusive, while it is inclusive for generate_series. Both functions provide overloads
with an additional third parameter step defining the step-size which defaults to 1.
Variants that only take the stop parameter and default start to 0 exists, too. While
used as normal functions, they provide useful constructs, but are much more powerful
when queried like a table.
© Manning Publications Co. To comment go to liveBook
108