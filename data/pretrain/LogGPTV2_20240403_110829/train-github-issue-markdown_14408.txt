Using multiple metric evaluation has the following example:
    >>> def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]
    >>> def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]
    >>> def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]
    >>> def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]
    >>> scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),
    ...            'fp': make_scorer(fp), 'fn': make_scorer(fn)}
This means that `confusion_matrix` will be called _**4**_ times for each
`cross_validate` fold.
This is probably not a performance issue because the cost of
`confusion_matrix` is dwarfed by the cost of fitting and applying the model,
but it is still clearly suboptimal.
It would seem natural to allow scorers to return _dictionaries_ rather than
numbers.
E.g., the previous example would be
    def cm(y_true, y_pred):
        mx = confusion_matrix(y_true, y_pred)
        return {"tn:mx[0,0], "fp":mx[0,1], "fn":mx[1,0], "tp":mx[1,1]}
    scoring = make_scorer(cm)
`sklearn: 0.20.0`