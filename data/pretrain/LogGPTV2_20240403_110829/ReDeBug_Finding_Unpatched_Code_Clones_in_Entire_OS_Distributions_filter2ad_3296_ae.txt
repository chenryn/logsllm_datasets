90-100 %
Fraction of shared n-grams
Figure 8: Similarity among functions
Figure 9: Fraction of shared n-grams
For non-C/C++ code we normalized as described in § II-C.
For C/C++ code, we also did experiments where we roughly
identiﬁed functions within C/C++ ﬁles using the following
Perl regular expression:
/ˆ \w+?\s[ˆ;]*? \( [ˆ;]*?\)\s*({ ←(cid:2)
(?:[ˆ{}]++|(?1))*})/xgsm
The regex isn’t perfect to recognize all functions since that
would require a complete parser. However, in our experience
it is sufﬁcient and allowed us to provide an estimate of
similarity and code clones at the function level.
We identiﬁed 3,230,554 functions containing at
least
4 tokens. We split
identiﬁed functions into two groups
based upon the function size. “The small-sized” group had
3,144,998 functions which had less than 114 tokens. “The
large-sized” group had 85,556 functions. Overall, we mea-
sured pair-wise distance in each group using SIMILARITYbv,
which required 4,949,164,509,293 pairwise comparisons.
For the small-sized group, we used 32 byte bit vectors.
Total bitvector generation time was 6 min using 32 CPUs.
It took 19 min to compare every pair of the group using
512 CPUs. For the large-sized group, we used 8 KB bit
vectors. Generation time was 14 minutes on 32 CPUs. Pair-
wise comparisons took 5 min 30sec using 512 CPUs.
Figure 8 shows the distribution of function pairs based
upon their similarity. Most of the function pairs had very
low similarity below 0.1, which is natural in that different
packages would be expected to have different functionality.
However, surprisingly, 694,883,223 pairs of functions had
more than 0.5 similarity. Among them, 172,360,750 pairs
were more than 90% similar. This result clearly shows a
signiﬁcant amount of code copying.
While performing our experiments, we noticed that the
SourceForge dataset had more code clones. With Source-
Forge projects, we normalized and tokenized each ﬁle and
calculated the total fraction of shared n-tokens in each ﬁle.
As shown in Figure 9, more than 50% of ﬁles shared more
than 90% of n-tokens with other ﬁles. Note that 100% of
shared n-tokens in a ﬁle does not necessarily mean it is
copied from another ﬁle as a whole. This could also happen
when a ﬁle consists of small fractions from multiple ﬁles.
On the contrary, about 30% of ﬁles were almost unique (0-
10% shared tokens) while 50% of the ﬁles shared more than
90% of all tokens. This shows that code cloning is active
and alive within the SourceForge community.
IV. DISCUSSION
A. Comparison to Prior Work
ReDeBug improves scalability with decreased false de-
tection rate, but may ﬁnd fewer code clones than previous
code clone detection work. In order to measure the number
of unpatched code clones that ReDeBug missed, we com-
pared the number of code clones detected by ReDeBug to
the number of code clones reported by Deckard [23]. We
chose Deckard because it claims better code clone detection
performance than CP-Miner [25] and CloneDR [5].
Theoretically, the code clones reported by Deckard should
be the superset of the code clones found by ReDeBug. In
practice, however, Deckard missed more code clones than
ReDeBug. We used Deckard v1.2 2 for our experiments,
and set parameters as follows: minT (minimum number
of tokens required for clones) = 30 and stride (size of
the sliding window) = 2 for their conservative results, and
Similarity = 1 to minimize their false detection. This
was similar to the setup in their paper.
Deckard did not scale to the entire Debian Lenny dis-
tribution (257,796,235 LoC) in our test setup. During pair-
2https://github.com/skyhover/Deckard
59
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:03 UTC from IEEE Xplore.  Restrictions apply. 
Clone Detection
Real Clones
False Detection Missed
ReDeBug
Deckard
180
96
0
183
15
99
Table VI: Code clone detection performance
wise comparisons Deckard consumed more than 20 GB of
memory in less than 2 minutes, after which we killed the
process. Instead of the entire OS, we ran Deckard on each
package at a time with 28 randomly selected C code ﬁles
which contain security bugs. We report only code clones
which match with the buggy code regions. Deckard took
more than 12 hours to complete the code clone detection
in Debian Lenny, utilizing 8 threads to process 8 packages
at the same time. While Deckard processed only the source
code written in C (Deckard can process one language at
a time), ReDeBug processed a wealth of languages (e.g.,
C/C++, Java, Shell, Python, Perl, Ruby, and PHP) in 6 hours.
Table VI shows the code clone detection results of
Deckard and ReDeBug. As expected, ReDeBug had no false
detections, and surprisingly, missed 6 times as few code
clones compared to Deckard. The code clones that ReDeBug
missed came from the use of different variable names or
types.
Deckard faired worse than ReDeBug despite using a more
sophisticated strategy. We investigated the causes, and found
that 38 out of 99 of the cases were due to parse failures
in Deckard, with the remainder just being missed due to
the algorithm for detecting code clones. This result lends
support that parsing code is hard and can be a limiting factor
in practice, and that ReDeBug’s relatively simpler approach
can be valuable in such circumstances.
B. Unpatched code clones that are not vulnerable
Since ReDeBug gets rid of Bloom ﬁlter errors and dead
code, a metric for false positives is the number of unpatched
code clones that were not vulnerable for some other reason.
We have identiﬁed two other causes for this type of false
positive. First, normalization may be too aggressive in some
circumstances and thus the identiﬁed code clone is not
really a code clone. Second, we may ﬁnd real unpatched
code clones, but other code modiﬁcations may prevent the
unpatched code from being called in an exploitable context.
Normalization reduces the false negative rate, but may
increase the false positive rate. For example, imagine two
code sequences that are equivalent but one is performed on
an unsigned integer “A” and the other on a signed integer
“a”. If the bug relates to signedness, only the latter code is
vulnerable. However, normalization converts all variables to
lower-case, thus we would mistakenly report the former as
also buggy.
Listing 10 shows an example of an unpatched code clone
that is present but not vulnerable. The patch ﬁxes an integer
signedness bug in various BSD kernels. NetBSD contains the
same vulnerable code, but ﬁxed the problem by changing the
type of crom_buf->len from signed integer to unsigned
integer instead of using the shown patch.
- if (crom_buf->len len len > 0)
Listing 10: CVE-2006-6013
An unpatched code clone was detected in the ircd-ratbox
package from the patch shown in Listing 11. The package
maintainer informed us that the vulnerability was ﬁxed in
a different location, i.e., adding a separate error checking
routine if (len 0) {
++src, --len;
}
-
-
+
+
+
}
*d = ’\0’;
return dest;
Listing 11: CVE-2009-4016
V. RELATED WORK
MOSS [28] is a well-known similarity detection tool
using n-tokens. MOSS is based upon an algorithm called
winnowing [28], a fuzzy hashing technique that selects a
subset of n-tokens to ﬁnd similar code. The main difference
is that ReDeBug uses feature hashing to encode n-tokens
in a bitvector, which allows ReDeBug to perform similarity
comparison in a cache-efﬁcient way. We swap out winnow-
ing for feature hashing for improved speed. This decision
was based upon the work by Jang et al. [22]. Furthermore,
in order to ﬁnd unpatched code clones we use the insight
of only looking for code clones of patched bugs to scale to
large OS distributions.
Most recent work in academia has focused on detecting
all code clones (i.e., reducing the number of missed code
clones, but having more false detections of clones). Exam-
ples include Deckard [23], CCFinder [24], CP-Miner [25]
and Deja Vu [21]. Detecting all code clones is a harder
problem than just searching for copies of patched code in
that the former potentially requires comparison of all code
pairs, while the latter is a single sweep over the data set.
This line of research uses a variety of matching heuristics
based upon high-level code representations such as CFGs
and parse trees. For example, CCFinder uses lexing and
then performs transformations based upon rules to determine
whether code is similar [24]. The transformation rules are
60
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:03 UTC from IEEE Xplore.  Restrictions apply. 
language-dependent. Deckard [23] and Deja Vu [21] build
parse trees for C code, and then reduce part of the parse trees
to a vector. Comparisons are done on the vector. CP-Miner
also parses the program: currently the parser is implemented
only for C and C++. It then hashes these tokens and assigns
a numeric value to each, and runs the CloSpan frequent
subsequence mining algorithm to detect code clones.
Each of the above techniques represent a unique and
different point in the design space. For example, building the
parse tree, CFG, etc. all require implementing a robust parser
for the language. Implementing good parsers is a difﬁcult
problem with which even professional software assurance
companies struggle [7], but once done will give them a
robust level of abstraction not available to ReDeBug. The
highest false positive rate for reported errors among the code
clones was 90% for CP-Miner, and 66-74% for Deja Vu. In
terms of scalability, the largest code base we are aware of is
Deja Vu, which looked at a proprietary code base consisting
of about 75 million lines of C code. They used a cluster
of 5 machines, and integrated the product into the build
cycle. It found 2070-2760 likely bugs. Our experiments are
on code bases upon to billions of lines of code (two orders
of magnitude larger). If their techniques could be scaled
up, they would likely ﬁnd more unpatched code clones, but
the number of falsely detected clones would also scale up
and the overall system would require more resources than
available to a typical end-developer.
SYDIT [26] is a program transformation tool, which
characterizes edits as AST node modiﬁcations and generates
context-aware edit scripts from example edits. It was tested
on an oracle data set of 56 pairs of example edits from
open source projects in Java. SYDIT complements ReDeBug
in that SYDIT looks at abstract, semantic changes while
ReDeBug focuses on syntactic changes at large scale.
Pattern Insight’s Code Assurance [1] (aka Patch Miner)
is advertised as ﬁnding unpatched code clones, as with
ReDeBug. Their whitepaper does not contain any technical
information to compare on a usage, algorithmic, perfor-
mance, or accuracy basis, but does mention it performs a
kind of “fuzzy matching”. We have contacted Pattern Insight
to get more details, but they have not made the product
available to us for comparison.
Previous work has also done clustering. Much previous
work, like us, uses the Jaccard distance metric, e.g., Deja Vu.
Deckard and Deja Vu use locality sensitive hashing (LSH)
to speed up the pairwise comparison using Jaccard. We use
feature hashing. Theoretical analysis shows feature hashing
outperforms LSH alone [29], and Jang et al. back this up
with an empirical evaluation for malware clustering [22].
However, a hybrid approach that ﬁrst uses LSH to ﬁnd near-
duplicates which are then compared using feature hashing
may be possible. We leave these types of optimizations as
future work.
Brumley et al. have shown once a patch becomes avail-
able, an attacker may be able to use it to reverse engineer the
problem and create an exploit automatically [10]. We leave
exploring the ramiﬁcation of this problem as future work.
VI. CONCLUSION
In this paper we presented ReDeBug, an architecture
designed for unpatched code clone detection. ReDeBug
was designed for scalability to entire OS distributions, the
ability to handle real code, and minimizing false detec-
tion. ReDeBug found 15,546 unpatched code clones, which
likely represent real vulnerabilities, by analyzing 2.1 billion
lines of code on a commodity desktop. We demonstrate
the practical impact of ReDeBug by conﬁrming 145 real
bugs in the latest version of Debian Squeeze packages. We
believe ReDeBug can be a realistic solution for regular
developers to enhance the security of their code in day-to-
day development.
ACKNOWLEDGMENT
This research was supported in part by sub-award
PO4100074797 from Lockheed Martin Corporation originat-
ing from DARPA Contract FA9750-10-C-0170 for BAA 10-
36. This research was also supported in part by the National
Science Foundation through TeraGrid resources provided
by Pittsburgh Supercomputing Center. We would like to
thank the anonymous referees, Debian developers, Spencer
Whitman, Edward Schwartz, JongHyup Lee, Yongsu Park,
Tyler Nighswander, and Maverick Woo for their feedback in
preparing this paper.
REFERENCES
[1] Code Assurance.
http://patterninsight.com/products/
code-assurance/. Page checked 3/4/2012.
[2] Pittsburgh Supercomputing Center.
edu/. Page checked 3/4/2012.
http://www.psc.
[3] QuickLZ.
3/4/2012.
http://www.quicklz.com/.
Page checked
[4] SimMetrics.
http://sourceforge.net/projects/
simmetrics/. Page checked 3/4/2012.
[5] Ira D. Baxter, Christopher Pidgeon, and Michael
Mehlich. DMS: program transformations for practical
scalable software evolution. In Proceedings of the In-
ternational Conference on Software Engineering, 2004.
http://www.cse.yorku.ca/∼oz/hash.
[6] Daniel Bernstein.
html. Page checked 3/4/2012.
[7] Al Bessey, Ken Block, Ben Chelf, Andy Chou, Bryan
Fulton, Seth Hallem, Charles Henri-Gros, Asya Kam-
sky, Scott McPeak, and Dawson Engler. A few billion
lines of code later: using static analysis to ﬁnd bugs in
the real world. Communications of the Association for
Computing Machinery, 53(2):66–75, 2010.
61
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:03 UTC from IEEE Xplore.  Restrictions apply. 
[8] Burton H. Bloom.
Space/Time trade-offs in hash
coding with allowable errors. Communications of the
Association for Computing Machinery, 13(7):422–426,
1970.
[9] Andrei Broder and Michael Mitzenmacher. Network
applications of bloom ﬁlters: A survey. Internet Math-
ematics, 1(4):485–509, 2005.
[10] David Brumley, Pongsin Poosankam, Dawn Song, and
Jiang Zheng. Automatic patch-based exploit generation
is possible: Techniques and implications. In Proceed-
ings of the IEEE Symposium on Security and Privacy,
May 2008.
[11] Christophe Calv`es and Maribel Fern´andez. A poly-
Theoretical
nomial nominal uniﬁcation algorithm.
Computer Science, 403:285–306, 2008.
[12] Ben Collins-Sussman, Brian W. Fitzpatrick, and
Version control with subver-
http://svnbook.red-bean.com/en/1.0/svn-book.
C. Michael Pilato.
sion.
html#svn-ch-3-sect-4.3.2. Page checked 3/4/2012.
[13] National Vulnerability Database.
CVE-2008-0928.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2008-0928. Page checked 3/4/2012.
[14] National Vulnerability Database.
CVE-2009-3720.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2009-3720. Page checked 3/4/2012.
[15] National Vulnerability Database.
CVE-2010-0405.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2010-0405. Page checked 3/4/2012.
[16] National Vulnerability Database.
CVE-2011-1092.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2011-1092. Page checked 3/4/2012.
[17] National Vulnerability Database.
CVE-2011-1782.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2011-1782. Page checked 3/4/2012.
[22] Jiyong Jang, David Brumley, and Shobha Venkatara-
man. BitShred: feature hashing malware for scalable
triage and semantic analysis.
In Proceedings of the
ACM Conference on Computer and Communications
Security, 2011.
[23] Lingxiao Jiang, Ghassan Misherghi, and Zhendong Su.
Deckard: Scalable and accurate tree-based detection
of code clones.
In Proceedings of the International
Conference on Software Engineering, 2007.
[24] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro In-
oue. CCFinder: a multilinguistic token-based code
clone detection system for large scale source code.
IEEE Transactions on Software Engineering, 28(7):654
– 670, 2002.
[25] Zhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan
Zhou. CP-Miner: ﬁnding copy-paste and related bugs
in large-scale software code.
IEEE Transactions on
Software Engineering, 32:176–192, 2006.
[26] Na Meng, Miryung Kim, and Kathryn S. McKinley.
Systematic editing: generating program transforma-
tions from an example.
In Proceedings of the ACM
SIGPLAN conference on Programming Language De-
sign and Implementation, 2011.
[27] Ubuntu Security Notice. CVE-2011-3145. http://www.
ubuntu.com/usn/usn-1196-1/. Page checked 3/4/2012.
[28] Saul Schleimer, Daniel Wilkerson, and Alex Aiken.
Winnowing: Local algorithms for document ﬁnger-
printing. In Proceedings of the ACM SIGMOD/PODS
Conference, 2003.
[29] Qinfeng Shi, James Petterson, Gideon Dror, John Lang-
ford, Alex Smola, and S.V.N. Vishwanathan. Hash ker-
nels for structured data. Journal of Machine Learning
Research, 10:2615–2637, 2009.
[18] National Vulnerability Database.
CVE-2011-3200.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2011-3200. Page checked 3/4/2012.
[19] National Vulnerability Database.
CVE-2011-3368.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2011-3368. Page checked 3/4/2012.
[20] National Vulnerability Database.
CVE-2011-3872.
http://web.nvd.nist.gov/view/vuln/detail?vulnId=
CVE-2011-3872. Page checked 3/4/2012.
[21] Mark Gabel, Junfeng Yang, Yuan Yu, Moises Gold-
szmidt, and Zhendong Su. Scalable and systematic
detection of buggy inconsistencies in source code. In
Proceedings of the ACM International Conference on
Object Oriented Programming Systems Languages and
Applications, 2010.
62
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:47:03 UTC from IEEE Xplore.  Restrictions apply.