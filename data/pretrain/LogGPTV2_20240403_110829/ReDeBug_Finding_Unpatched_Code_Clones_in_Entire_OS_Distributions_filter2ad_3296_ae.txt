### 90-100% Fraction of Shared n-grams
**Figure 8: Similarity among Functions**
**Figure 9: Fraction of Shared n-grams**

For non-C/C++ code, we normalized the data as described in § II-C. For C/C++ code, we conducted additional experiments to identify functions within C/C++ files using the following Perl regular expression:

```perl
/^\w+?\s[^;]*? \([^;]*?\)\s*({(?:[^{]*|(?1))*})/xgsm
```

This regex is not perfect for recognizing all functions, as a complete parser would be required for that. However, it proved sufficient in our experience, allowing us to estimate similarity and code clones at the function level.

We identified 3,230,554 functions, each containing at least four tokens. These functions were divided into two groups based on their size. The "small-sized" group contained 3,144,998 functions with fewer than 114 tokens, while the "large-sized" group had 85,556 functions. We measured pairwise distances within each group using SIMILARITYbv, which required 4,949,164,509,293 pairwise comparisons.

For the small-sized group, we used 32-byte bit vectors. The total bitvector generation time was 6 minutes using 32 CPUs. It took 19 minutes to compare every pair of functions using 512 CPUs. For the large-sized group, we used 8 KB bit vectors. The generation time was 14 minutes on 32 CPUs, and pairwise comparisons took 5 minutes and 30 seconds using 512 CPUs.

**Figure 8** shows the distribution of function pairs based on their similarity. Most function pairs had very low similarity (below 0.1), which is expected given that different packages generally have different functionalities. Surprisingly, 694,883,223 pairs of functions had more than 0.5 similarity, with 172,360,750 pairs being more than 90% similar. This result clearly indicates a significant amount of code copying.

During our experiments, we observed that the SourceForge dataset had more code clones. For SourceForge projects, we normalized and tokenized each file and calculated the total fraction of shared n-tokens in each file. As shown in **Figure 9**, more than 50% of files shared more than 90% of n-tokens with other files. Note that 100% shared n-tokens in a file does not necessarily mean it is copied from another file in its entirety; it could also result from a file consisting of small fractions from multiple files. Conversely, about 30% of files were almost unique (0-10% shared tokens), while 50% of the files shared more than 90% of all tokens. This demonstrates that code cloning is prevalent within the SourceForge community.

### IV. Discussion

#### A. Comparison to Prior Work

ReDeBug improves scalability with a decreased false detection rate but may find fewer code clones compared to previous work. To measure the number of unpatched code clones that ReDeBug missed, we compared the results to those reported by Deckard [23], chosen because it claims better performance than CP-Miner [25] and CloneDR [5].

Theoretically, the code clones reported by Deckard should be a superset of those found by ReDeBug. In practice, however, Deckard missed more code clones than ReDeBug. We used Deckard v1.2 for our experiments, setting parameters as follows: minT (minimum number of tokens required for clones) = 30, stride (size of the sliding window) = 2 for conservative results, and Similarity = 1 to minimize false detections. This setup was similar to that in their paper.

Deckard did not scale to the entire Debian Lenny distribution (257,796,235 LoC) in our test setup. During pairwise comparisons, Deckard consumed more than 20 GB of memory in less than 2 minutes, after which we terminated the process. Instead of the entire OS, we ran Deckard on each package at a time with 28 randomly selected C code files containing security bugs. We only report code clones that match the buggy code regions. Deckard took more than 12 hours to complete the code clone detection in Debian Lenny, utilizing 8 threads to process 8 packages simultaneously. While Deckard processed only C source code (one language at a time), ReDeBug processed multiple languages (e.g., C/C++, Java, Shell, Python, Perl, Ruby, and PHP) in 6 hours.

**Table VI** shows the code clone detection results of Deckard and ReDeBug. As expected, ReDeBug had no false detections and, surprisingly, missed 6 times fewer code clones compared to Deckard. The code clones that ReDeBug missed were due to different variable names or types.

Despite using a more sophisticated strategy, Deckard performed worse than ReDeBug. We investigated the causes and found that 38 out of 99 cases were due to parse failures in Deckard, with the remainder just being missed due to the algorithm for detecting code clones. This result supports the notion that parsing code is challenging and can be a limiting factor in practice, and that ReDeBug’s simpler approach can be valuable in such circumstances.

#### B. Unpatched Code Clones That Are Not Vulnerable

Since ReDeBug eliminates Bloom filter errors and dead code, a metric for false positives is the number of unpatched code clones that are not vulnerable for other reasons. We identified two other causes for this type of false positive. First, normalization may be too aggressive, leading to the identification of code clones that are not truly clones. Second, real unpatched code clones may be found, but other code modifications may prevent the unpatched code from being called in an exploitable context.

Normalization reduces the false negative rate but may increase the false positive rate. For example, consider two code sequences that are equivalent, but one is performed on an unsigned integer "A" and the other on a signed integer "a." If the bug relates to signedness, only the latter code is vulnerable. However, normalization converts all variables to lowercase, thus mistakenly reporting the former as also buggy.

**Listing 10** shows an example of an unpatched code clone that is present but not vulnerable. The patch fixes an integer signedness bug in various BSD kernels. NetBSD contains the same vulnerable code but fixed the problem by changing the type of `crom_buf->len` from a signed integer to an unsigned integer instead of using the shown patch.

```c
- if (crom_buf->len > len)
```
**Listing 10: CVE-2006-6013**

An unpatched code clone was detected in the ircd-ratbox package from the patch shown in **Listing 11**. The package maintainer informed us that the vulnerability was fixed in a different location, i.e., adding a separate error checking routine.

```c
if (len > 0) {
    ++src, --len;
}
```
**Listing 11: CVE-2009-4016**

### V. Related Work

MOSS [28] is a well-known similarity detection tool using n-tokens. MOSS is based on an algorithm called winnowing [28], a fuzzy hashing technique that selects a subset of n-tokens to find similar code. The main difference is that ReDeBug uses feature hashing to encode n-tokens in a bitvector, which allows ReDeBug to perform similarity comparisons in a cache-efficient way. We replaced winnowing with feature hashing for improved speed, based on the work by Jang et al. [22]. Furthermore, to find unpatched code clones, we use the insight of only looking for code clones of patched bugs to scale to large OS distributions.

Most recent academic work has focused on detecting all code clones (i.e., reducing the number of missed code clones but having more false detections). Examples include Deckard [23], CCFinder [24], CP-Miner [25], and Deja Vu [21]. Detecting all code clones is a harder problem than just searching for copies of patched code, as the former potentially requires comparison of all code pairs, while the latter is a single sweep over the data set. This line of research uses various matching heuristics based on high-level code representations such as CFGs and parse trees. For example, CCFinder uses lexing and then performs transformations based on rules to determine whether code is similar [24]. The transformation rules are language-dependent. Deckard [23] and Deja Vu [21] build parse trees for C code and then reduce part of the parse trees to a vector. Comparisons are done on the vector. CP-Miner also parses the program: currently, the parser is implemented only for C and C++. It then hashes these tokens and assigns a numeric value to each, and runs the CloSpan frequent subsequence mining algorithm to detect code clones.

Each of the above techniques represents a unique and different point in the design space. For example, building the parse tree, CFG, etc., all require implementing a robust parser for the language. Implementing good parsers is a difficult problem, even for professional software assurance companies [7], but once done, they provide a robust level of abstraction not available to ReDeBug. The highest false positive rate for reported errors among the code clones was 90% for CP-Miner and 66-74% for Deja Vu. In terms of scalability, the largest code base we are aware of is Deja Vu, which looked at a proprietary code base consisting of about 75 million lines of C code. They used a cluster of 5 machines and integrated the product into the build cycle. It found 2070-2760 likely bugs. Our experiments are on code bases up to billions of lines of code (two orders of magnitude larger). If their techniques could be scaled up, they would likely find more unpatched code clones, but the number of falsely detected clones would also scale up, and the overall system would require more resources than available to a typical end-developer.

SYDIT [26] is a program transformation tool that characterizes edits as AST node modifications and generates context-aware edit scripts from example edits. It was tested on an oracle data set of 56 pairs of example edits from open source projects in Java. SYDIT complements ReDeBug in that SYDIT looks at abstract, semantic changes, while ReDeBug focuses on syntactic changes at a large scale.

Pattern Insight’s Code Assurance [1] (aka Patch Miner) is advertised as finding unpatched code clones, like ReDeBug. Their whitepaper does not contain any technical information to compare on a usage, algorithmic, performance, or accuracy basis but mentions it performs a kind of "fuzzy matching." We have contacted Pattern Insight to get more details, but they have not made the product available to us for comparison.

Previous work has also done clustering. Much previous work, like ours, uses the Jaccard distance metric, e.g., Deja Vu. Deckard and Deja Vu use locality-sensitive hashing (LSH) to speed up the pairwise comparison using Jaccard. We use feature hashing. Theoretical analysis shows feature hashing outperforms LSH alone [29], and Jang et al. back this up with an empirical evaluation for malware clustering [22]. However, a hybrid approach that first uses LSH to find near-duplicates, which are then compared using feature hashing, may be possible. We leave these types of optimizations for future work.

Brumley et al. have shown that once a patch becomes available, an attacker may be able to use it to reverse-engineer the problem and create an exploit automatically [10]. We leave exploring the ramifications of this problem for future work.

### VI. Conclusion

In this paper, we presented ReDeBug, an architecture designed for unpatched code clone detection. ReDeBug was designed for scalability to entire OS distributions, the ability to handle real code, and minimizing false detection. ReDeBug found 15,546 unpatched code clones, which likely represent real vulnerabilities, by analyzing 2.1 billion lines of code on a commodity desktop. We demonstrate the practical impact of ReDeBug by confirming 145 real bugs in the latest version of Debian Squeeze packages. We believe ReDeBug can be a realistic solution for regular developers to enhance the security of their code in day-to-day development.

### Acknowledgment

This research was supported in part by sub-award PO4100074797 from Lockheed Martin Corporation originating from DARPA Contract FA9750-10-C-0170 for BAA 10-36. This research was also supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center. We would like to thank the anonymous referees, Debian developers, Spencer Whitman, Edward Schwartz, JongHyup Lee, Yongsu Park, Tyler Nighswander, and Maverick Woo for their feedback in preparing this paper.

### References

[1] Code Assurance. http://patterninsight.com/products/code-assurance/. Page checked 3/4/2012.
[2] Pittsburgh Supercomputing Center. http://www.psc.edu/. Page checked 3/4/2012.
[3] QuickLZ. http://www.quicklz.com/. Page checked 3/4/2012.
[4] SimMetrics. http://sourceforge.net/projects/simmetrics/. Page checked 3/4/2012.
[5] Ira D. Baxter, Christopher Pidgeon, and Michael Mehlich. DMS: Program Transformations for Practical Scalable Software Evolution. In Proceedings of the International Conference on Software Engineering, 2004.
[6] Daniel Bernstein. http://cr.yp.to/hash.html. Page checked 3/4/2012.
[7] Al Bessey, Ken Block, Ben Chelf, Andy Chou, Bryan Fulton, Seth Hallem, Charles Henri-Gros, Asya Kamsky, Scott McPeak, and Dawson Engler. A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs in the Real World. Communications of the Association for Computing Machinery, 53(2):66–75, 2010.
[8] Burton H. Bloom. Space/Time Trade-offs in Hash Coding with Allowable Errors. Communications of the Association for Computing Machinery, 13(7):422–426, 1970.
[9] Andrei Broder and Michael Mitzenmacher. Network Applications of Bloom Filters: A Survey. Internet Mathematics, 1(4):485–509, 2005.
[10] David Brumley, Pongsin Poosankam, Dawn Song, and Jiang Zheng. Automatic Patch-Based Exploit Generation is Possible: Techniques and Implications. In Proceedings of the IEEE Symposium on Security and Privacy, May 2008.
[11] Christophe Calvès and Maribel Fernández. A Polynomial Nominal Unification Algorithm. Theoretical Computer Science, 403:285–306, 2008.
[12] Ben Collins-Sussman, Brian W. Fitzpatrick, and C. Michael Pilato. Version Control with Subversion. http://svnbook.red-bean.com/en/1.0/svn-book.html#svn-ch-3-sect-4.3.2. Page checked 3/4/2012.
[13] National Vulnerability Database. CVE-2008-0928. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2008-0928. Page checked 3/4/2012.
[14] National Vulnerability Database. CVE-2009-3720. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2009-3720. Page checked 3/4/2012.
[15] National Vulnerability Database. CVE-2010-0405. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2010-0405. Page checked 3/4/2012.
[16] National Vulnerability Database. CVE-2011-1092. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2011-1092. Page checked 3/4/2012.
[17] National Vulnerability Database. CVE-2011-1782. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2011-1782. Page checked 3/4/2012.
[22] Jiyong Jang, David Brumley, and Shobha Venkataraman. BitShred: Feature Hashing Malware for Scalable Triage and Semantic Analysis. In Proceedings of the ACM Conference on Computer and Communications Security, 2011.
[23] Lingxiao Jiang, Ghassan Misherghi, and Zhendong Su. Deckard: Scalable and Accurate Tree-Based Detection of Code Clones. In Proceedings of the International Conference on Software Engineering, 2007.
[24] Toshihiro Kamiya, Shinji Kusumoto, and Katsuro Inoue. CCFinder: A Multilinguistic Token-Based Code Clone Detection System for Large-Scale Source Code. IEEE Transactions on Software Engineering, 28(7):654–670, 2002.
[25] Zhenmin Li, Shan Lu, Suvda Myagmar, and Yuanyuan Zhou. CP-Miner: Finding Copy-Paste and Related Bugs in Large-Scale Software Code. IEEE Transactions on Software Engineering, 32:176–192, 2006.
[26] Na Meng, Miryung Kim, and Kathryn S. McKinley. Systematic Editing: Generating Program Transformations from an Example. In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, 2011.
[27] Ubuntu Security Notice. CVE-2011-3145. http://www.ubuntu.com/usn/usn-1196-1/. Page checked 3/4/2012.
[28] Saul Schleimer, Daniel Wilkerson, and Alex Aiken. Winnowing: Local Algorithms for Document Fingerprinting. In Proceedings of the ACM SIGMOD/PODS Conference, 2003.
[29] Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and S.V.N. Vishwanathan. Hash Kernels for Structured Data. Journal of Machine Learning Research, 10:2615–2637, 2009.
[18] National Vulnerability Database. CVE-2011-3200. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2011-3200. Page checked 3/4/2012.
[19] National Vulnerability Database. CVE-2011-3368. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2011-3368. Page checked 3/4/2012.
[20] National Vulnerability Database. CVE-2011-3872. http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2011-3872. Page checked 3/4/2012.
[21] Mark Gabel, Junfeng Yang, Yuan Yu, Moises Goldszmidt, and Zhendong Su. Scalable and Systematic Detection of Buggy Inconsistencies in Source Code. In Proceedings of the ACM International Conference on Object-Oriented Programming Systems Languages and Applications, 2010.