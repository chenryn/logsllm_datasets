sages while decreasing the probability that a node receives
redundant multicast messages to 0.0005. This optimization
requires multicast messages and gossips to carry the elapse
time since the message was injected into the system, which
can be estimated by piggybacking and adding up the propa-
gation delays and waiting times as the message travels away
from the source.
In this section, we have described the message dissem-
ination protocol assuming the overlay and the tree are al-
ready in place. Next, we proceed to describe the protocols
that build the overlay and the tree in a decentralized fashion.
2.2. Protocol for Building the Overlay
Our goal is to build an overlay that is degree constrained
and richly connected, and consists of mostly low latency
links. The overlay built by our protocol has several salient
features. (1) In overlays built by existing protocols, the node
degrees are not tightly controlled. By contrast, each node
in GoCast has roughly the same number of overlay neigh-
bors in order to spread out the maintenance overhead and
the gossip overhead. (2) In existing protocols, a node either
has no random neighbor or chooses at least than half of its
neighbors at random. By contrast, most nodes in GoCast
have exactly one random neighbor, while all other neigh-
bors are chosen based on network proximity. This method
produces overlays that are robust as well as efﬁcient.
We choose system parameters carefully to strike a good
balance between the conﬂicting goals of resilience and ef-
ﬁciency. The connectivity of the overlay (i.e., the number
of disjoint paths between two nodes) directly affects the de-
pendability of GoCast in the face of node or link failures.
Higher node degrees lead to better connectivity but intro-
duce higher protocol overhead because nodes need to main-
tain more neighbors and gossips are sent to more neighbors.
Assuming nodes have similar capacities, we also want node
degrees to be as uniform as possible such that the protocol
overhead imposed on each node is roughly equal. (Tuning
node degree according to node capacity can be accommo-
dated in our protocol but is beyond the scope of this pa-
per.) The overlay is unstructured; it mandates no particular
topology. Regardless of the initial structure of the overlay,
it adapts automatically such that almost all nodes converge
to a target node degree chosen at design time.
Besides the target node degree, another important design
choice is the way of selecting node neighbors.
It affects
the connectivity of the overlay, message delay, and stress
on the underlying network links. On the one hand, accord-
ing to the random graph theory, adding links between ran-
dom nodes improves the connectivity of the overlay. On the
other hand, adding low latency links between nodes that are
close in the network lowers message delay, consumes less
network resources, and reduces stresses on bottleneck net-
work links. GoCast achieves a good balance by devoting a
small number of overlay links to connect random nodes and
selecting all other overlay links based on network proxim-
ity. Our evaluation shows that this approach results in an
overlay that has both low latency and high connectivity.
We deﬁne some notations before delving into our pro-
tocol. A component consists of a group of nodes that are
connected directly or indirectly by overlay links. We refer
to overlay links that connect randomly chosen neighbors as
random links and overlay links chosen based on network
proximity as nearby links. Two nodes directly connected by
a random link are random neighbors and two nodes directly
connected by a nearby link are nearby neighbors. Let ran-
dom degree Drand(X) and nearby degree Dnear(X) de-
note the number of node X’s random neighbors and nearby
neighbors, respectively. Let Cdegree, Crand, and Cnear de-
note the target node degree, target random degree, and tar-
get nearby degree, respectively, Cdegree = Crand + Cnear.
Cdegree, Crand, and Cnear are constants chosen at de-
Ideally, every node X has the same degree,
sign time.
Drand(X) = Crand and Dnear(X) = Cnear.
One major contribution of this paper is the ﬁnding of a
good setting for these parameters: Crand = 1, and Cnear
= 5. We ﬁnd that, without any random neighbor (Crand
= 0), the overlay is partitioned even without any node or
link failure. This is because nearby links do not connect re-
mote components. With just one random neighbor per node
(Crand = 1), the connectivity of the overlay is almost as
good as that of overlays using multiple random neighbors
per node. Intuitively, nearby links connect a group of nodes
that are close and random links connect remote groups of
nodes. For instance, suppose a system consists of 500 nodes
in America and 500 nodes in Asia. With nearby links only,
the system is decomposed into two components correspond-
ing to the two geographical areas. Internally, each compo-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:09:24 UTC from IEEE Xplore.  Restrictions apply. 
nent is richly connected. By adding just one random link to
each node (500 random links in total as one link connects
two neighbors), we expect an average of 250 random links
to connect the America component and the Asia compo-
nent, which greatly enhances the connectivity of the entire
system. Moreover, we ﬁnd that six neighbors per node pro-
vide sufﬁcient connectivity. For instance, with this conﬁg-
uration, systems with thousands of nodes remain connected
even after 25% of the nodes fail concurrently.
The target node degrees are chosen at design time. Be-
low, we describe our protocols that enforce the node degrees
at run-time and select high-quality links for the overlay.
2.2.1. Node Join
Each node knows a random subset of nodes in the sys-
tem. This knowledge is maintained by piggybacking the IP
addresses of some random nodes on gossips exchanged be-
tween overlay neighbors. Due to space limitations, we omit
the details of this partial membership protocol. Interested
readers are referred to [5, 16]. It has been shown [5] that,
for gossip protocols, a “uniformly” random partial member
list is almost as good as a complete member list.
When a new node N joins, it knows at least one node
P already in the overlay through some out-of-band method.
Node N contacts node P to obtain P ’s member list S. For
the time being, node N accepts S as its member list. Later
on, node N may add nodes into or delete nodes from S.
Node N randomly selects Crand nodes in S as its ran-
dom neighbors and establish a TCP connection with each
of them. All communications between overlay neighbors
go through these pre-established TCP connections.
(On
the other hand, communications between nodes that are not
overlay neighbors use UDP, e.g., RTT measurements be-
tween non-neighbor nodes.)
Among nodes in S, ideally, node N should select those
that have the lowest latencies to N as N’s nearby neighbors.
However, S can be large, including hundreds of nodes. It
would introduce a large amount of trafﬁc and long wait-
ing time for N to measure RTTs to every node in S. In-
stead, node N uses an algorithm to estimate network dis-
tance and chooses Cnear nodes in S that have the smallest
estimated latencies to N as its initial set of nearby neigh-
bors. Later on, node N gradually measures RTTs to nodes
in S and switches from long latency links to low latency
links, thereby improving the efﬁciency of the overlay over
time. We use the triangular heuristic [13] to estimate laten-
cies. Details are omitted due to space limitations.
If the new node N chooses a node X as its neighbor,
N sends a request to X. X accepts this request only if
its node degrees are not too high—for adding a random
link, Drand(X)  Crand, node X tries to drop some
random neighbors through one of the operations below.
1. If Drand(X) ≥ Crand + 2, node X randomly chooses
two of its random neighbors Y and Z and asks Y to
establish a random link to Z. Node X then drops its
random links to nodes Y and Z. By doing this, node
X’s random degree is reduced by two, while the ran-
dom degrees of nodes Y and Z are not changed.
2. If one of node X’s random neighbors W has more than
Crand random neighbors, node X drops the random
link between X and W . This reduces the random de-
grees of both X and W by one while still keeping their
random degrees equal to or larger than Crand.
If neither of the conditions above is met, node X’s ran-
dom degree must be Crand + 1 and all X’s random neigh-
bors must have random degrees equal to or smaller than
Crand. In this case, no action is taken and node X’s random
degree remains at Crand+1. It can be proved that, when the
overlay stabilizes, each node eventually has either Crand or
Crand + 1 random neighbors. Our evaluation shows that
approximately 88% of nodes have Crand random neighbors
and 12% of nodes have Crand + 1 random neighbors.
2.2.3. Maintaining Proximity Aware Neighbors
In addition to maintaining its random neighbors, every r
seconds, a node X also executes a protocol to maintain its
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:09:24 UTC from IEEE Xplore.  Restrictions apply. 
nearby neighbors. This protocol differs from the protocol
for maintaining random neighbors in that it considers net-
work proximity when adding or dropping links. It tries to
conﬁne node X’s nearby degree to either Cnear or Cnear+1,
while choosing nodes that are close to X as X’s nearby
neighbors. Node X runs three sub-protocols: one to replace
X’s long latency nearby links with low latency links; one to
add more nearby links when Dnear(X) < Cnear; and one to
drop long latency nearby links when Dnear(X)≥ Cnear+2.
Replacing Nearby Neighbors
Node X sorts nodes in its member list S in increasing es-
timated latency. Starting from the node with the lowest es-
timated latency, node X measures real latencies to nodes
in S one by one. During each maintenance cycle (every
r seconds), node X measures RTT to only one node in S.
As the overlay stabilizes, the opportunity for improvement
diminishes. The maintenance cycle r can be increased ac-
cordingly to reduce maintenance overheads. We leave the
dynamic tuning of r as a subject of future work.
Suppose node X measures RTT to node Q in the current
maintenance cycle. Node X will add node Q as its new
nearby neighbor and drop its existing nearby neighbor U if
all the conditions below are met.
C1. Node X has at least one nearby neighbor U whose
current nearby degree is not too low: Dnear(U) ≥
Cnear − 1. Otherwise, the degrees of node X’s all
nearby neighbors are dangerously low. Dropping a link
to one of them would endanger the connectivity of the
overlay. Among node X’s nearby neighbors that sat-
isfy this condition, the node U to be replaced is chosen
as the neighbor that has the longest latency to node X.
C2. Dnear(Q) < Cnear + 5. This requires that the nearby
degree of the new neighbor candidate Q is not too high.
C3. If Dnear(Q) ≥ Cnear,
then RTT(X, Q) <
max nearby RTT(Q) must hold. Here RTT(X, Q) is
the RTT between node X and the new neighbor can-
didate Q, and max nearby RTT(Q) is the maximum
RTT between node Q and Q’s nearby neighbors.
If
this condition is not met, node Q already has enough
nearby neighbors and the link between nodes Q and X
is even worse than the worst nearby link that Q cur-
rently has. Even if node X adds the link to node Q
now, Q is likely to drop the link soon. Hence node X
does not add this link.
2 · RTT(X, U). Here node Q is the
new neighbor candidate and node U is the neighbor
to be replaced (selected by condition C1).
Intended
to avoid futile minor adaptations, this condition stipu-
lates that node X adopts new neighbor Q only if Q is
signiﬁcantly better than the current neighbor U.
C4. RTT(X, Q) ≤ 1
Among many heuristics we tested, the conditions above
are particularly effective in resolving many conﬂicting
goals—upholding the connectivity of the overlay during
adaptation, minimizing the total number of link changes
without global information, and converging to a stable state
quickly. For instance, condition C1 is a good example
of the tradeoff we made. Because of condition C1, node
U’s nearby degree can be as low as Cnear − 2 in a tran-
sient period after node X drops the link to U and before
U adds more nearby links in the next maintenance cycle.
This lower degree bound can be increased to Cnear − 1
if we change condition C1 from Dnear(U) ≥ Cnear − 1
to Dnear(U) ≥ Cnear. However, our evaluation shows
that this change would produce an overlay whose link la-
tencies are dramatically higher than that produced by our
current solution, because fewer neighbors satisfy this new
condition to qualify as a candidate to be replaced. With our
recommended setting, Crand = 1 and Cnear = 5, the lower
bound of a node’s degree during adaptation is 4, which is
sufﬁciently high to uphold the connectivity of the overlay
during short transient periods.
Originally, node X sorts nodes in its member list S in
increasing estimated latency and measures RTTs to them
one by one. Once all nodes in S have been measured, the
estimated latencies are no longer used. But node X still
continuously tries to replace its current nearby neighbors by
considering candidate nodes in S in a round robin fashion.
The hope is that some nodes that previously do not satisfy
some of the conditions C1-C4 now can meet all of them and
hence can be used as new nearby neighbors.
Adding Nearby Neighbors
If node X has less than Cnear nearby neighbors, X needs
to add more nearby neighbors in order to uphold the con-
nectivity of the overlay. To spread out the load, during each
maintenance cycle, node X adds at most one new nearby
neighbor. Similar to the process to replace nearby neigh-
bors, node X selects a node Q from its member list S in a
round robin fashion and adds Q as its new nearby neighbor
if both conditions C1 and C2 are met. These conditions stip-
ulate that Q does not have an excessive number of neighbors
and the link between nodes X and Q is no worse than the
worst nearby link that Q currently has.
Dropping Nearby Neighbors
If node X has an excessive number of nearby neighbors
(e.g., some new nodes have added links to X), X starts to
drop some nearby neighbors to reduce unnecessary proto-
col overheads. Although the target nearby degree is Cnear,
node X starts to drop nearby neighbors only if Dnear(X) ≥
Cnear +2. This allows a node’s nearby degree to stabilize
at either Cnear or Cnear+1. Our evaluation shows that, un-
der this strategy, eventually about 70% of nodes have Cnear
nearby neighbors and about 30% of nodes have Cnear +1
nearby neighbors. One alternative is to drop one more
nearby neighbor when Dnear(X) = Cnear +1. Our eval-
uation shows that, compared with our current solution, this
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:09:24 UTC from IEEE Xplore.  Restrictions apply. 
aggressive strategy increases the number of link changes by
almost one third and it takes longer to stabilize the overlay.
When Dnear(X) ≥ Cnear + 2, node X tries to drop
Dnear(X)−Cnear nearby neighbors. The candidate neigh-
bors to drop are those that satisfy condition C1 above,
i.e., nodes U whose nearby degree is not dangerously low,
Dnear(U) ≥ Cnear − 1. Again, avoiding dropping links
to low degree nodes helps uphold the connectivity of the
overlay during adaptation. Node X sorts its nearby neigh-
bors that satisfy this condition and drops those that have the
longest latencies to X until X’s nearby degree is reduced to
Cnear or no nearby neighbor satisﬁes condition C1.
2.3. Protocol for Building the Tree
GoCast selects overlay links in a decentralized fash-
ion to construct an efﬁcient tree embedded in the over-
lay. The tree spans over all nodes and propagates messages
rapidly. The algorithm to build the tree is in spirit similar
to the classical Distance Vector Multicast Routing Protocol
(DVMRP) [15], but note that GoCast only needs a single
tree. The tree conceptually has a root and the tree links are
overlay links on the shortest paths (in terms of latency) be-
tween the root and all other nodes. If the root fails, one of its
neighbors will take over its role. Originally, the ﬁrst node in
the overlay acts as the root. Periodically every 15 seconds,
the root ﬂoods a heartbeat message throughout every link in
the overlay to help detect failures (e.g., partitioning) of the
overlay and the tree. Due to space limitations, we omit the
details of the tree protocol as it is pretty standard.
3. Experimental Results
We built an event-driven simulator to evaluate GoCast.
The simulator consists of 6,100 lines of C++ code and runs
on Linux. It simulates a complete system, including mes-
sage propagation, node and link failure, network topology,
and link latency. We do not simulate the network-level
packet details in order to scale to thousands of nodes. Sim-
ulating one run of an 8,192-node system on a 2.4GHz ma-
chine takes about three hours.
Our simulator uses real Internet latencies from the King
dataset [4], which is extracted from real measurements of
RTTs between 1,740 DNS servers in the Internet. (The orig-
inal dataset contains more than 1,740 servers but we ex-
clude those servers with empty measurements). We divide