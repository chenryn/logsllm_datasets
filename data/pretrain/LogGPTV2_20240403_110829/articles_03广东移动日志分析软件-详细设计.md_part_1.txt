**广东移动日志管理平台**
**详细设计**
**甲方：【中国移动通信集团广东有限公司】**
**乙方：【亚信科技（中国）有限公司】**
**2018年8月**
##### 大数据
支持大数据相关接口定制开发，将日志分析软件做得像Google搜索引擎一样强大、灵活、易用，对日志进行集中管理，提供实时搜索、关联分析、监控告警、多维统计和数据可视化等功能，帮助企业进行运维监控、安全合规审计及业务数据挖掘。日志分析软件RESTfulAPI提供对日志日志分析软件的定制开发，调用接口,作为日志分析软件企业版的一部分发布，便于用户以灵活的方式集成日志分析软件系统
接口使用HTTP，通过HTTP BasicAuth进行权限校验。
当前提供如下接口：
-   [SplSearch](#_splsearch_resource):
    搜索类接口是使用日志分析软件的Search Porcess
    Language(SPL)进行日志搜索对应的接口，除了基本的搜索之外还有通过搜索执行的下载、LogTail、DrillDown、上下文查询功能。SPL是日志分析软件产品的核心功能，其内容非常丰富，使用API前请可通过日志分析软件Web界面做初步了解，更详细的功能可通过售后人员提供的单独的搜索参考手册进行学习。SPL本身有丰富的语法，能完成文本搜索、业务串联、各类统计、机器学习等丰富功能。SPL通过API只是将其能力以可编程的方式提供，并不能完成SPL本身不可完成的事情，建议使用API开发之前先人工通过Web界面对您需求进行搜索和串联尝试，随后将其通过程序自动串联。
-   [Agent](#_agent_resource):
    Agent类接口对日志分析软件数据接入代理进行配置，除了基本的增删查改，在日志分析软件Web界面中Agent管理菜单下的小部分功能可通过API进行配置。采集代理是日志分析软件的核心功能，内容非常丰富。建议在使用API前先使用日志分析软件Web界面对Agent配置进行基本的了解，随后参看售后人员提供的单独的数据接入手册中的日志接入章节。当前Agent的API只支持对文件和syslog两种来源进行配置。
-   [ParserRule](#_ParserRule):
    提取规则接口对日志分析软件数据接入历程中日志解析规则进行配置，除了基本的增删查改，在日志分析软件Web界面中数十类提取规则均可通过API进行配置。提取规则是日志分析软件产品的核心功能，内容非常丰富。建议在使用API前先通过日志分析软件Web界面对提取规则的配置进行基本了解，随后参看售后人员提供的单独的数据接入手册中的接入规则章节。
-   [Alert](#_alert_resource):
    告警类接口对日志分析软件告警功能进行配置，除了基本的增删查改，在日志分析软件Web界面可进行的告警发送方式（即告警插件）、高级配置下的扩展搜索、告警抑制等也可以通过API完成。告警功能是日志分析软件产品的核心功能，内容非常丰富。建议请在使用API前先通过界面进行了解，随后参看售后人员提供的单独的告警手册。
-   [SourceGroup](#_SourceGroup):
    日志来源类接口对日志来源进行增删查改管理。可通过日志分析软件Web界面了解，另外可参看售后人员提供的使用手册中的管理设置-日志管理-日志来源章节。
-   [Account](#_Account):
    当前用户接口只支持简单的用户增删查改管理。可通过日志分析软件Web界面与产品手册中的介绍进行进行了解，另外可参看售后人员提供的日志使用手册中的管理设置-权限管理-用户管理章节。
-   [AccountGroup](#_AccountGroup):
    用户分组接口除了用户分组的增删查改管理外，还包括设置用户组成员、指定用户分组与角色关系。可通过日志分析软件Web界面了解，另外可参看售后人员提供的日志分析软件使用手册中的管理设置-权限管理-用户分组系统章节。
-   [ResourceGroup](#_ResourceGroup):
    日志分析软件中的告警、定时任务、提取规则等各类数据都被抽象为资源，通过资源分组完成各自的权限管理。资源分组接口除了资源分组本身的增删查改管理外，还包括对资源组成员的操作。可通过日志分析软件界面与售后人员提供的使用手册中的管理设置-权限管理-资源分组章节了解。
服务的基本信息为：
-   默认IP地址为日志分析软件服务集群的Frontend模块的IP，可通过日志分析软件Manager查看当前集群。
-   默认端口为：8080。与Frontend模块是同一个接口。
-   当前支持版本号：v1
使用 [HTTP
BasicAuth](https://en.wikipedia.org/wiki/Basic_access_authentication)
进行鉴权。即在HTTP头中加入名为Authorization的Header，内容形如:
    Authorization: Basic QWxhZGRpbjpPcGVuU2VzYW1l
其中Basic后的部分为用冒号分隔的名字和密码进行Base64编码。例如账号为ss密码为ss则应为ss:ss的Base64编码，完整为：
    Authorization: Basic c3M6c3M=
如果使用curl进行访问则使用-u参数即可
    curl -u ss:ss apiserver:8080/v1/token/operator/
-   API服务IP：使用日志分析软件集群Frontend模块作为入口。可通过Manager界面看Frontend模块的IP。
-   API服务port：为8080。通过Manager界面Frontend模块配置项 listen_port
    指定，一般不要修改。
-   token: 在日志分析软件Web页面 /tokens/ 菜单下获取形如
    86bb700c6f5e48b094bbc73dd8f46a6a 的token。
-   username, password:
    为登陆日志分析软件Web页面的用户名和密码。API可搜索和操作的资源范围也是此Web登陆用户一样的权限。在API实现中账户密码使用HTTP
    BasicAuth方式，放在HTTP头Authorizaiton中。
-   operator:
    请与username相同。只有需要用ROOT用户模拟其他用户行为时候username为此domain的root用户，然后operator为想要模拟的用户名。
-   版本：版本包含在url中，所有v1版本的url的开头是/v1。
##### 流式数据接入
> 数据采集层：
>
> （1）支持增量读取文件日志（适用于业务日志）
>
> （2）支持syslog／rsyslog接口（适用于网络设备，安全设备）
>
> （3）支持获取mysql，oracle，sql server等数据库信息；（适用于业务日志）
>
> （4）支持通过flume／ws／api接口，获取业务系统日志；（适用于定制开发）
>
> （5）支持通过agent获取eventlog日志（适用于windows系统）
>
> （6）支持对接流量抓包系统（适用于对接NPM,APM）
###### 2.1通过对接flume流处理接口，实时接收上游系统数据
Flume（[http://flume.apache.org/）是Hadoop生态系统的一个开源组件，被用于数据采集和传输用途](http://flume.apache.org/%EF%BC%89%E6%98%AFHadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E4%B8%AA%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6%EF%BC%8C%E8%A2%AB%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E4%BC%A0%E8%BE%93%E7%94%A8%E9%80%94%EF%BC%8C%E7%B1%BB%E4%BC%BC%E4%BA%8EHeka%E5%9C%A8%E6%97%A5%E5%BF%97%E6%98%93%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E5%9C%B0%E4%BD%8D%E3%80%82)。供了众多的Source做数据采集（[http://flume.apache.org/FlumeUserGuide.html#flume-sources），常用的Source包括](http://flume.apache.org/FlumeUserGuide.html#flume-sources%EF%BC%89%EF%BC%8C%E5%B8%B8%E7%94%A8%E7%9A%84Source%E5%8C%85%E6%8B%AC%EF%BC%9A)
> TailDir Source，用于实时读取文件
>
> Syslog Source，用于接收syslog数据
>
> Kafka Source，用于从Kafka中读取数据
>
> Exec Source，用于接收其他命令的输出
>
> Spooling Directory Source，用于读取不可变文件
>
> Avro Source，用于接收其他Avro客户端发过来的数据
##### 数据库数据接入
数据库数据接入，数据库数据DB源支持接入oracle数据库数据，接入prosgrep数据库数据，定时主动获取调用等功能
点击添加数据页面的数据库数据标签，进入添加数据库数据源流程：在选择数据库连接页面，可以选择一个已有的连接来进行数据库集，也可以新建一个数据库连接采集支持定时主动获取调用
##### 海量数据消息处理队列
###### 4.1 为提高数据处理健壮性，需要开发一套分布式消息队列系统，
为提高数据处理健壮性，需要开发一套分布式消息队列系统，分布式消息处理队列用于消息的持久化和缓存。该系统使用磁盘文件做持久化，顺序进行读写，以append方式写入文件。为减少内存copy，集群使用sendfile发送数据，通过合并message提升性能。集群本身不储存每个消息的状态，而使用（consumer/topic/partition）保存每个客户端状态，大大减小了维护每个消息状态的麻烦。在消息推拉的选择上，集群使用拉的方式，避免推的方式下存在的各个客户端的处理能力、流量等不同产生不确定性。以多机形式形成集群，建议3台或3台以上奇数台服务器组建，并且支持分区副本。
**功能特点：**
（1）以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。
（2）高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。可以通过扩展机器，实现处理万级以上eps数据，
（3）支持Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。
（4）同时支持离线数据处理和实时数据处理。
（5）Scale out：支持在线水平扩展。
（6）消息系统在处理过程中间插入了一个隐含的、基于数据的接口层，两边的处理过程都要实现这一接口。这允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。
（7）有些情况下，处理数据的过程会失败。除非数据被持久化，否则将造成丢失。消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。
（8）因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。不需要改变代码、不需要调节参数。扩展就像调大电力按钮一样简单。
（9）在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见；如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。
（10）系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。
（11）在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。分布式消息系统保证一个Partition内的消息的有序性。
（12）在任何重要的系统中，都会有需要不同的处理时间的元素。例如，加载一张图片比应用过滤器花费更少的时间。消息队列通过一个缓冲层来帮助任务最高效率的执行---------写入队列的处理会尽可能的快速。该缓冲有助于控制和优化数据流经过系统的速度。
（13）消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。
###### 4.2 可以通过扩展机器，实现处理万级以上eps数据，
日志分析软件高性能、分布式日志处理架构可以每秒钟分析百万条日志，每天可以处理几十TB级的日志量，而且处理延时非常短，可以让用户搜索、分析几秒钟之前产生的日志。处理延时非常短，可以让用户搜索、分析几秒钟之前产生的日志。
流式计算集群具有如下特性：
（1）轻量级快速处理：
着眼大数据处理，速度往往被置于第一位。流处理架构允许集群中的应用程序在内存中以100倍的速度运行，即使在磁盘上运行也能快10倍。流处理架构通过减少磁盘IO来达到性能提升，它们将中间处理数据全部放到了内存中。
（2）无数据丢失：
系统需要保证无数据丢失，这也是系统高可用性的保证。系统为了无数据丢失，需要在数据处理失败的时候选择另外的执行路径进行replay（系统不是简单的重新提交运算，而是重新执行调度，否则按照来源的call
stack有可能使得系统永远都在相同的地方出同样的错误）。
（3）容错透明：
用户不会也不需要关心容错。系统会自动处理容错，调度并且管理资源，而这些行为对于运行于其上的应用来说都是透明的。
**功能特点：**
（1）能运行在100+的结点上，可以通过设备横项扩展，增加平台的数据接入性能，并达到秒级延迟。
（2）使用基于内存的计算作为执行引擎，具有高效和容错的特性。
（3）日志分析软件支持以下的正则提取规则，将接收到的数据属于非结构化数据的，进行结构化处理，同时每条消息中保存原始日志，通过全文检索可以直接搜索原始日志的内容，同时支持在界面查看上下文，其效果于直接查看原始日志类似
（4）为实现复杂的算法提供和批处理类似的简单接口。
（5）它通过丰富的API和基于内存的高速计算引擎可以结合流式处理，批处理和交互查询等应用。
（6）计算流程：将流式计算分解成一系列短小的批处理作业。把输入数据按照batch
size（如1秒）分成一段一段的数据。同时把接收到的数据属于非结构化数据的，进行结构化处理
（7）容错性：对于流式计算来说，容错性至关重要。
（8）实时性：将流式计算分解成多个Job，对于每一段数据的处理都会经过分解，以及任务集的调度过程。
（9）扩展性与吞吐量：能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理万级以上EPS。海量数据的流式处理是大数据平台数据处理部分的核心，需要支持万级eps数据的处理，
###### 4.3 管理原始数据以及正则提取后的数据
正则是处理文本解析的有力工具。需要用户了解一些基本的正则表达式知识：
\\, ?, +, \*, \[\], (?:) （?:\）
例如有这样一条日志：
2014-05-14 23:24:47 15752 \[Note\] InnoDB: 128 rollback segment(s) are
active
我们希望提取出以下字段：timestamp，pid，loglevel和message，可以配置如下的表达式：
(?\\\S+ \\S+) (?\\\S+) \\\[(?\\\S+\\\]
(?\.\*)
其中
\\S表示匹配非空格字符，\\S+表示匹配连续的非空格字符，(?\value) 表示提取名字为key的字段，其值为value，会解析出如下字段：
1 timestamp：2014-05-14 23:24:47
2 pid：15752
3 loglevel：Note
4 message：InnoDB: 128 rollback segment(s) are active
##### 5. 海量数据流处理
![](media/image1.png){width="5.78125in" height="5.0625in"}
###### 5.1 接收到的数据属于非结构化数据的，进行结构化处理
日志分析软件支持以下的正则提取规则，将接收到的数据属于非结构化数据的，进行结构化处理，同时每条消息中保存原始日志，通过全文检索可以直接搜索原始日志的内容，同时支持在界面查看上下文，其效果于直接查看原始日志类似
###### 5.2 格式化处理后进行数据聚合，告警和分析
通过日志分析软件的检索系统的
transaction处理能力可以对分散的业务日志进行聚合，归纳出一个完整的业务流程。并可以通过配置项来灵活得自定义业务聚合方式。
####### 5.2.1 Transaction设计
一个transaction由一组相关的log组成，比如用户的一次搜索过程对应在整个系统中的所有日志等，
transaction命令将具有相同字段的值组合成一个group，并在单个group内进行transaction的识别
语法
transaction field-list \[maxspan=\\] \[maxevents=int\]
\[maxopentxn=int\]
\[maxopenevents=int\] \[startswith=\\]
\[endswith=\\]
\[contains=\\]
field-list :: field \[,field\]\*
timespan :: \\[s\|m\|h\|d\]
filterstring :: \ \| eval_expression
eval_expression :: eval(bool_expression)
filter_string：
1、\，为双引号括起来的字符串，可使用转义字符，表示单条日志是否包含该字符串
2、 eval(bool-expression), 对单条日志计算表达式的值，返回值true或者false
参数：
maxspan: transaction第一条日志和最后一条日志的最大时间间隔
maxevents ： 单个transaction的最大日志条数
maxopentxn：用于控制内存使用，获取的最多分组，用于过滤的计算
maxopenevents：用于控制内存使用，单个transaction最多从es取的log的条数
startswith : 满足的条件的日志为一个新的transaction的第一条日志
endswith : 满足条件的记录为transaction最后一条日志
contains:
如果transaction中的任何一条日志包含\或者满足bool-expression将保留该transaction，否则丢弃
####### 5.2.2业务数据聚合设计
业务日志通过一系列从start with开始到end
with结束的步骤，可以将其聚合为整个业务。整个功能提供可灵活配置的接口来指定聚合方式的指定。样例如下：
busi_types = \[
{
busi_url =
\"/charge/business.action?BMEBusiness=charge.charge&\_cntRecTimeFlag=true\"