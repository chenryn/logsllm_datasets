bolic, so that symbolic execution would generate new test
cases along the path dictated by each PoV. For a fair compari-
son, we conﬁgured KLEE to perform concolic execution like
QSYM and SYMCC. This setup avoids bias from KLEE’s
forking and scheduling components [32]. It is worth noting,
however, that KLEE still performs some additional work com-
pared to QSYM and SYMCC: since it does not rely on ex-
ternal sanitizers to detect bugs, it implements similar checks
itself, thus putting more load on the SMT solver. Also, it fea-
tures a more comprehensive symbolic memory model. Since
these are intrinsic aspects of KLEE’s design, we cannot easily
disable them in our comparison.
In essence, all three symbolic execution systems executed
the target program with the PoV input, at each conditional
attempting to generate inputs that would drive execution
down the alternative path. The results are shown in Figure 6:
SYMCC is considerably faster than QSYM and KLEE even
in the presence of symbolic data.
Coverage Finally, we measured the coverage of the test
cases generated in the previous experiment using the method-
ology of Yun et al. [45]: for each symbolic execution system,
we recorded the combined coverage of all test cases per target
program in an AFL coverage map [46].7 On each given target
program, the result was a set of covered program points for
each system, which we will call S for SYMCC and R for the
system we compare to (i.e., KLEE or QSYM). We then as-
signed a score d in the range [−1.0,1.0] as per Yun et al. [45]:
7Traditional coverage measurement, e.g., with gcov, does not work reli-
ably on the CGC programs because of the bugs that have been inserted.
1
0
−1
Figure 7: Coverage score comparing SYMCC and KLEE per
tested program (visualization inspired by Yun et al. [45]):
blue colors mean that SYMCC found more paths, red colors
indicate that KLEE found more, and white symbolizes equal
coverage. SYMCC performs better on 46 programs and worse
on 10 (comparison restricted to the programs that KLEE can
execute, i.e., 56 out of 116).
(cid:40) |S−R|−|R−S|
|(S∪R)−(S∩R)|
0
d(S,R) =
if S (cid:54)= R
otherwise
Intuitively, a score of 1 would mean that SYMCC covered
all program paths that the other system covered and some in
addition, whereas a score of -1 would indicate that the other
system reached all the paths covered by SYMCC plus some
more. We remark that this score, while giving a good intuition
of relative code coverage, suffers from one unfortunate draw-
back: It does not put the coverage difference in relation with
the overall coverage. In other words, if two systems discover
exactly the same paths except for a single one, which is only
discovered by one of the systems, then the score is extreme
(i.e., 1 or -1), no matter how many paths have been found
by both systems. In our evaluation, the coverage difference
between SYMCC and the systems we compare to is typically
small in comparison to the overall coverage, but the score
cannot accurately reﬂect this aspect. However, for reasons of
comparability we adopt the deﬁnition proposed by Yun et al.
unchanged; it still serves the purpose of demonstrating that
SYMCC achieves similar coverage to other systems in less
time.
We visualize the coverage score per test program in Fig-
ures 7 and 8. The former shows that SYMCC generally
achieves a higher coverage level than KLEE; we mainly
190    29th USENIX Security Symposium
USENIX Association
1
0
−1
Figure 8: Comparison of coverage scores between SYMCC
and QSYM. SYMCC found more paths on 47 programs and
less on 40; they discovered the same paths on 29 programs.
Similar coverage is expected because SYMCC uses the same
symbolic backend as QSYM.
attribute differences to the signiﬁcantly different symbolic
backends. The latter demonstrates that SYMCC’s coverage is
comparable to QSYM’s, i.e., the compilation-based execution
component provides information of comparable quality to the
symbolic backend. We suspect the reason that coverage of
some programs differs at all—despite the identical symbolic
backends in QSYM and SYMCC—is twofold:
1. SYMCC derives its symbolic expressions from higher-
level code than QSYM (i.e., LLVM bitcode instead of
x86 machine code). This sometimes results in queries
that are easier for the SMT solver, leading to higher
coverage.
2. On the other hand, the lower-level code that QSYM ana-
lyzes can lead to test cases that increase coverage of the
program under test at the machine-code level.
We conclude that compilation-based symbolic execution is
signiﬁcantly faster than IR-based and even IR-less symbolic
execution in our benchmarks while achieving similar code
coverage and maintaining a simple implementation.
5.1.2 Initialization overhead
In the course of our evaluation we noticed that QSYM and
KLEE have a relatively large constant-time overhead in each
analysis. For example, on our test machine, QSYM always
runs for several seconds, independently of the program under
test or the concreteness of the input. The overhead is presum-
ably caused by costly instrumentation work performed by the
symbolic executor at the start of the analysis (something that
SYMCC avoids by moving instrumentation to the compila-
tion phase). Therefore, we may assume that the execution
times TSYMCC and Tother are not related by a simple constant
speedup factor but can more accurately be represented via
initialization times ISYMCC and Iother, analysis times ASYMCC
and Aother, and a speedup factor S that only applies to the
analysis time:
TSYMCC = ISYMCC + ASYMCC
Tother = Iother + Aother = Iother + S· ASYMCC
(1)
(2)
Consequently, we can compute the speedup factor as follows:
S =
Tother − Iother
TSYMCC − ISYMCC
(3)
In order to obtain accurate predictions for the analysis
time of long-running programs, we therefore need to take the
initialization time into account when computing the speed-
up factor. As a simple approximation for the worst case
from SYMCC’s point of view, we assumed that the short-
est observed execution consists of initialization only, i.e.,
suppose ASYMCC and Aother are zero in the analysis of the
fastest-running program. In other words, for each system we
subtracted the time of the fastest analysis observed in Sec-
tion 5.1.1 from all measurements. Then we recomputed the
speedup in the afﬁne model presented above. For concolic
execution with KLEE, we obtained an average factor of 2.4
at a constant-time overhead of 9.20 s, while for QSYM we
computed a factor of 2.7 at a constant-time overhead of 9.15 s.
SYMCC’s constant-time overhead is 0.13 s; this conﬁrms the
beneﬁt of instrumenting the target at compile time.
Note that this model is only relevant for long-running pro-
grams, which are rarely fuzzed.8 Otherwise, execution time
is dominated by the startup overhead of QSYM and KLEE.
Nevertheless, the model shows that SYMCC’s performance
advantage is not simply due to a faster initialization—even
when we account for constant-time overhead at initialization
and overestimate it in favor of QSYM and KLEE, SYMCC is
considerably faster than both.
5.1.3 Compilation time and binary size
SYMCC modiﬁes the target program extensively during com-
pilation, which results in increased compilation time and
larger binaries (because of inserted instrumentation). In or-
der to quantify this overhead, we ﬁrst compiled all 116 CGC
programs both SYMCC and regular clang, and measured the
total build time in either case. Compilation required 602 s with
SYMCC, compared to 380 s with clang; this corresponds to
an increase of 58 %. Note that this is a one-time overhead:
once a target program is built, it can be run an arbitrary num-
ber of times.
Next, we compared the size of each instrumented exe-
cutable produced by SYMCC with the corresponding unmod-
iﬁed executable emitted by clang. On average, our instru-
mented binaries are larger by a factor of 3.4. While we have
not optimized SYMCC for binary size, we believe that there
8The documentation of AFL, for example, recommends that target pro-
grams should be fast enough to achieve “ideally over 500 execs/sec most of
the time” [46].
USENIX Association
29th USENIX Security Symposium    191
)
%
(
y
t
i
s
n
e
d
p
a
m
L
F
A
11
10
9
8
7
6
5
4
3
SymCC
QSYM
0h
5h
10h
15h
OpenJPEG
20h
25h
12
10
8
6
4
2
0
18
16
14
12
10
8
6
4
2
0
SymCC
QSYM
0h
5h
10h
15h
20h
25h
SymCC
QSYM
0h
5h
10h
15h
20h
25h
libarchive
tcpdump
Figure 9: Density of the AFL coverage map over time. The shaded areas are the 95 % conﬁdence corridors. The respective
differences between QSYM and SYMCC are statistically signiﬁcant with p < 0.0002. Note that the coverage improvement
correlates with the speedup displayed in Figure 10.
is potential to reduce this factor if needed. The largest contri-
bution to code size comes from run-time concreteness checks;
if binary size became a major concern, one could disable con-
creteness checks to trade execution time for space. In our tests
we have not experienced the necessity.
5.1.4 Impact of concreteness checks
In Section 3.4, we claimed that considerable improvements
can be gained by checking data for concreteness at run time
and skipping symbolic computation if all operands are con-
crete.
To illustrate this claim, let us examine just the initializa-
tion phase of the CGC program CROMU_00001. During the
startup, the CGC “standard library” populates a region in
memory with pseudo-random data obtained by repeated AES
computations on a seed value; this happens before any user
input is read. In the uninstrumented version of the program,
the initialization code executes within roughly 8 ms. This is
the baseline that we should aim for. However, when we run
a version of SYMCC with concreteness checks disabled on
CROMU_00001, execution takes more than ﬁve minutes using
our own simple backend, and with the faster QSYM backend
SYMCC still requires 27 s. The reason is that the instrumented
program calls into the symbolic backend at every operation,
which creates symbolic expressions, regardless of the fact that
all operands are fully concrete. The QSYM backend performs
better than our simple backend because it can fold constants
in symbolic expressions and has a back-off mechanism that
shields the solver against overload [45]. However, recall that
we are executing on concrete data only—it should not be
necessary to invoke the backend at all!
In fact, concreteness checks can drastically speed up the
analysis by entirely freeing the symbolic backend from the
need to keep track of concrete computations. With concrete-
ness checks enabled (as described in Section 4.4), the sym-
bolic backend is only invoked when necessary, i.e., when at
least one input to a computation is symbolic. For the initializa-
tion of CROMU_00001, enabling concreteness checks results
in a reduction of the execution time to 0.14 s with the QSYM
backend (down from 27 s). The remaining difference with the
uninstrumented version is largely due to the overhead of back-
end initialization and memory operations for book-keeping.
We assessed the effect across the CGC data set with PoV
inputs and found that the results conﬁrm our intuition: con-
creteness checks are beneﬁcial in almost all situations. The
only 3 cases where they increased the execution time instead
of decreasing it were very long-running programs that per-
form heavy work on symbolic data.
5.2 Real-world software
We have shown that SYMCC outperforms state-of-the-art
systems in artiﬁcial benchmark scenarios. Now we demon-
strate that these ﬁndings apply as well to the analysis of real-
world software. In particular, we show that SYMCC achieves
comparable or better overall performance despite its simple
implementation and architecture-independent approach.
We used QSYM and SYMCC in combination with the
fuzzer AFL [46] to test popular open-source projects (using
AFL version 2.56b); KLEE is not applicable because of un-
supported instructions in the target programs. For each target
program, we ran an AFL master instance, a secondary AFL
instance, and one instance of either QSYM or SYMCC. The
symbolic execution engines performed concolic execution on
the test cases generated by the AFL instances, and the result-
ing new test cases were fed back to the fuzzers. Note that this
is a relatively naive integration between symbolic execution
and fuzzer; however, since the focus of this work is on the
performance of symbolic execution, we leave the exploration
of more sophisticated coordination techniques to future work.
Fuzzing is an inherently randomized process that intro-
duces a lot of variables outside our control. Following the