16.3 Guest BPF Tools
701
This output showed that, most of the time, there was no stolen CPU time (the *[o]* bucket),
though on four occasions time the stolen time was in the eight- to 16-microsecond range. The
 [O]° bucket has been included in the output so that the ratio of stolen time vs total time can be
calculated: in this case it was 0.1% (32 / 30416).
This works by tracing the stolen_clock paravirt ops call using kprobes of the Xen and KVM
versions: xen_stolen_clock() and kvm_stolen_clock(). This is called on many frequent events, 
such as context switches and interrupts, so the overhead of this tool may be noticeable depending
on your workload.
The source to cpustolen(8) is:
#1/usx/local/bin/bpCtrace
BEGIN
printf(*Txacing stolen CPU tine. Ctrl-C to end. \n") 
kretprobe:xen_steal_clock,
kzetprobe:kvm_steal_clock
1f (elast[cpu] > 0) 
estolen_us = hist( (retval - 9last[cpu]) / 1000) ;
[e.qa - [ndo]2e[8
END
1
clear (81ast) ;
This will need to be upxlated for hypervisors other than Xen and KVM. Other hypervisors will
likely have a similar steal_clock function to satisfy a table of paravirt ops (pv_ops). Note that there
is a higher-level function, paravir_steal_clock(), which sounds more suitable to trace as it isn’t
tied to one hypervisor type. However, it is not available for tracing (likely inlined).
16.3.5HVM Exit Tracing
With the move from PV to HVM guests, we lose the ability to instrument explicit hypercalls,
but the guest is still making exits to the hypervisor for access to resources, and we'd like to trace
those. The current approach is to analyze resource latency using all the existing tools in the prior
pape[asosaadu aq eu uae r po juauoduoo auos peu puu u Bueaq aqm *spde
and we will not be able to measure that directly. We may be able to infer it by comparing latency
measurements from a bare-metal machine.
---
## Page 739
702
Chapter 16 Hypervisors
An interesting research prototype that could shed light on exit visibility by guests is a research
technology called hyperupcalls Amit 18]. These provide a safe way for a guest to request the
hypervisor to run a mini program; its example use cases include hypervisor tracing from the
guest. They are implemented using an extended BPF VM in the hypervisor, which the guest
compiles BPF bytecode to run. This is not currently made available by any cloud providers (and
may never be) but is another interesting project that uses BPE
16.4
Host BPF Tools
This section covers the BPF tools you can use for from-the-host VM performance analysis and
troubleshooting. These are either from the BCC and bpftrace repositories covered in Chapters 4
and 5, or were created for this book.
16.4.1 kvmexits
kvmexits(8)a is a bpftrace tool to show the distribution of guest exit time by reason. This will
reveal hypervisor-related performance issues and direct further analysis. Example output:
kvaexits.bt
Attach.ing 4 probes.
Tracing KVM exits. Ctr1-C to end
^C
[..-]
exit_ns[30, IO_INSTROCTION] :
[1K,2K)
1 1
[2K, 4K)
12 1889
[4K, 8K)
[8K, 16K)
198 1eeeeeeeeee e８eeeeeeeeeee８eeeeeeeee e８eeeeeeeeeeeee1
[16K,32K]
129 1889889889888 888988988988988988988
94 1889889889888888988988088
[323, 64K)
[64K, 128K)
371889889889
[128K, 256K]
12 1889
[256K, 512K)
231889889
[512K,18)
2 1
0 1
[1M,2M)
[2M, 4N)
1 |
2  1
4 Origin: 1 first developed this tool as lomexitiatency.d using 0Trace, published in the 2013 Systems Perfomance book
[Gregg 13b]. I developed it using bpftrace for this book on 25-feb-2019.
---
## Page 740
16.4 Host BPF Tools
703
fexit_ns [1, EXTERSAL_INTERRUPT] :
(2ts 9s21
281889
[512, 1K)
460 18 e88e88e88 8ee8ee8e98ee8ee88e88e88 88ee8ee8ee8ee81
[1K, 2K)
463 1eeeeeeeeee e８eeeeeeeeeeeeeeeeeeeee e８eeeeeeeeeeeee1
[2K,4K)
150 186988 886889
[4K, 83]
116 eeeeeeeeeeee
[8K,16K]
311889
[163, 32K)
1218
[32K, 64K)
71
[64%, 128K]
2 1
[128K, 25 6K)
11
eexit_ns[32, MSR_NRITE]:
[512, 1K]
5690 leeeeeeeeeeee eeeeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeee1
[1K,2K)
2978 1898888 8869998
[2K, 4K)
2080 1eee8ee8ee88ee8ee8ee
[4K, 8K]
854 18698898
[8K, 16K)
826 18898898
[16K,32K]
11018
(x9*x21
IE
pexit_ns[12, HLF] :
[512, 1K)
131
[18, 2K]
231
[2K, 4K)
101
[4K, 8K)
761
[8K, 16K]
234188
[16K,32K)
4167 leeeeeeee８８e eeeeeeeeee８eeeeeeeeee８８e eeeeeeeeeee
[32K, 64K]
3920 18e88e88 88888ee8e8e88 88e8ee8
[64%, 128K]
67 eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee1
[128K, 256K)
3483 1889889889886 8889889889869869889889886888
[256K, 512K)
176a 1eeeeeeeeeee e8eeeeee
[512K,1)
80889886881 226
[1M, 2H)
11318
[2M, 4M)
12818
[4H, 8N)
351
[8M, 16M)
401
(ZEN9T1
421
[32M, 64M]
9718
[64N, 128N)
9518
[128M, 25 6M)
581
[256M, 5128)
241
[512M, 16}
11
---
## Page 741
704
Chapter 16 Hypervisors
eexit_ns[48, EPT_VI0LAT108] :
[512, 1K]
6160 188e88e88e888888e88e88e88e88e88e88e88888ee
[1K, 2K]
6885 1889889889886 8889889889889869889889888888988988
[2K, 4K)
7686 leeeeeeeeee eeeeeeeeeeeeeeeeeeeeeee eeeeeeeeeeeeeee1
[4K, 8K)
2220 18898888886888
[8K, 16K)
582 1889
[16K,32K]
24418
471
[32X, 64K)
[64x, 128K)
3 1
This output shows the distribution of exits by type, including the exit code number and exit
reason string, if known. The longest exits, reaching one second, were for HLT (halt), which is
normal behavior: this is the CPU idle thread. The output also showed IO_INSTRUCTIONS taking
up to eight milliseconds.
This works by tracing the kvm:kvm_exit and kvm:kvm_entry tracepoints, which are only used
when the kernel KVM moxdule is in use to accelerate performance.
The source to kvmexit(8) is:
1/usz/local/bin/bpftrace
BEGIN
printf (*Tracing KVM exits. Ctr]-C to end\n*) 
/ / fron arch/xB6/include/uapi/asn/vmx,ht
IHN901La3x3。 - [01u0se9xTxe8
IaO883ININIGN3a。 - Ic l0osea3Tx98
μHOGKI&“IHK。 = [e]uoseexaTxeg
HLIxSXSYI。 - [6|0osea3x8
Bexitreason[10] - *CPUID";
Bexitreason[12] - "HLT"
Bexltzeason[13] = "INVD
Bexitreason [14] = *InVLPG*
Bexltzeason[15] = "RDPHC";
Bexitreason [16] - *RDTsc";
Bexltzeason[18] = "VNCALL";
Bexitreason [19] - "vMCLEAR*;
Bexltzeason[20] = "VNLAUNCH";
Bexitreascn [21] = "vMPTRLD*;
Bexitreason [22] - "VNPTRST*,
Bexitreason [23] - *vMREAD*
---
## Page 742
16.4 Host BPF Tools
705
Bexitreason [24] = "vMRESUME*;
BexItzeason[25] = "VMKR.ITE*,
Bexitreasen[26] - "vM0Fr*;
Bexitzeason[27] = "VNON*,
Bexitreason [28] = *CR_ACCEss*;
s5320x0 - [6290se9x1x88
Bexitreason [30] = *Io_INSTRUCTIOB*;
Bex1treason[31] = "HSR_READ";
3LI8h8sH - [7c oosea3Txg
BexItzeason[33] = "INVALID_STATE*;
TIvaavoTsH, = [Feloosea3Txag
BexItzeason[36] = "MAIT_INSTRUCTIOM";
Bexitreascn [37] = *HONITOR_TRAP_FLAG*;
Bex1teason [39] = "HONITOR_INSTROCTION*;
Bexitreason[40] = *pAUSs_INSTRUCTI0M*;
Bex1treason [41] = "WCE_DUR.ING_VYENTRY";
Bexitreason [43] = *TPR_BELOx_THRESHOLD*≠
Bexitreason[45] = *goI_INDocEo*;
H01810u = [9 00se9x1xe8
Bexitreason[49] = *EPT_MISCoNFIG*;
Bexltzeason[50] = "INVEPT"
Bexitreason[51] = *RDTsCP*≠
GIaANT。 = [cs]oosea3Tx9g
Bexitzeason [54] = "MBINVD"
Bexitreasen[55] = *xsersv*
3LIHx"3IdY - [9suoseexaTxe8
Bexitreascn [57] = *RDRAND*
Bexitreason[58] = "INVPCID*,
tracepoint:kvm:kvn_exit
Bstart[tid] = nsecsr
tracepoin.t:kvm:kvn_entry
/[pxes8/
---
## Page 743
90/
Chapter 16 Hypervisors
fnun = @reason[t1d]
Bexit_ns [$num, Bexitreason [$num] ] = hist (nsecs - Bstart[tid]]
delete (@start[tid]) 
delete (@reason |tid]]
END
clear (lex1treason) 
clear (8start) 
clear (8reason] 
Some KVM configurations do not use the kernel KVM module, so the needed tracepoints will not
fire, and this tool will be unable to measure the guest exists. In that case, the qemu process can be
instrurmented directly using uprobes to read the exit reason. (The addition of USDT probes would
be preferred.)
16.4.2Future Work
With KVM and similar hypervisors, the guest CPUs can be seen running as processes, and these
processes show up in tools including top(1). Thnis leads me to wonder whether the following
questions can be answered:
● W'hat is the guest doing on CPU? Can functions or stack traces be read?
• W'hy is the guest calling I/O?
Hosts can sample the on-CPU instruction pointer and can also read it when I/O is performed
based on its exit to the hypervisor. For example, using bpftrace to show the IP on I/O instructions:
+ bpftrace -e 't:kvm:kvm_exit /args->exit_reason == 30/ (
printf ("guest exit instruction pointer: sllx\n*, args->guest_rip) : 1*
Attaching 1 probe...
guest exit instruction pointez: frrrrrrslc9edc9
guest exit instruction pointer: ffffffff8lc9eelb
guest exit instruction pointez: ffrfrrfrslc9edc9
guest exit instruction pointer: ffffffff8lc9edc9
guest exit instruction pointez: ffcffrffalc9ee8b
guest exit instruction pointer: ffffffff8lc9eeeb
[..-]
---
## Page 744
16.5Summary
707
However, the host lacks a symbol table to convert these instruction pointers to function names,
or process context to know which address space to use or even which process is running. Possible
solutions to this have been discussed for years, including in my last book [Gregg 13b]. These
include reading the CR3 register for the root of the current page table, to try to figure out which
process is running, and using guest-supplied symbol tables.
These questions can currently be answered by instrumentation from the guest, but not the host,
16.5Summary
This chapter summarized hardware hypervisors and showed how BPF tracing can expose details
from the guest and the host, including hypercalls, stolen CPU time, and guest exits.
---
## Page 745
This page intentionally left blank
---
## Page 746
ter
OtherBPF
Performance Tools
This chapter tours other observability tools built upon BPE These are all open source and freely
available online. (Thanks to my colleague Jason Koch on the Netflix performance engineering
team for developing much of this chapter.)
While this book contains dozens of command-line BPF tools, it is expected that most people will
end up using BPF tracing via GUIs. This is especially the case for cloud computing environments
composed of thousands or even hundreds of thousands of instances; these are, of necesity,
usually administered via GUIs. Studying the BPF tools covered in previous chapters should help
you use and understand these BPF-based GUIs, which are created as front ends to the same tools.
The GUIs and tools discussed in this chapter:
 Vector and Performance Co-Pilot (PCP): For remote BPF monitoring
 Grafana with PCP: For remote BPF monitoring
eBPF Exporter: For BPF integration with Prometheus and Grafana
 kubectl-trace: For tracing Kubernetes pods and nodes
The role of this chapter is to show you some possibilities of BPF-based GUIs and automation
tools, using these as examples. This chapter has sections for each tool, summarizing what the
development at the time of writing, and it is likely that their capabilities will grow.
tools does, its internals and usage, and further references. Note that these tools are under heavy
17.1Vectorand Performance Co-Pilot（PCP)
Netflix Vector is an open source host-level performance monitoring tool that visualizes
high-resolution system and application metrics, in near real-time. It is implemented as a
web application, and leverages the battle-tested open source system monitoring framework
Performance Co-Pilot (PCP), layering on top a flexible and user-friendly U1. The UI polls metrics
every second or longer, rendering the data in completely configurable dashboards that simplify
cross-metric correlation and analysis.
---
## Page 747
710
Chapter 17 Other BPF Performance Tools
Web Server
Vector
HTTP GET
Local Desktop
Target Host
Web Browse
PCP&
BCC/BPFPMDA
Vector
JSON calls
PMWEBD
BCC Programs
Instance
Maps
Perf Buffer
BPF
dod jo diaq an wm Alaowau sndino weyod oog suouo ooa T-2T auni
Figure 17-1 shows how Vector running in a local web browser fetches its application code from
a web server and then connects directly to a target host and PCP to execute BPF programs. Note
'suotsian aungng ug alureqo 6eu spuauoduoo dd [eusaus atqp eq
Features of Vector include:
High-level dashboards are provided to show utilization across a number of resources
(CPU, disk, network, memory) for a running instance.
 More than 2000 metrics are available for deeper analysis. You can add or remove metrics
by modifying the configuration of performance metrics domain agents (PMDAs).
Visualize the data over time, down to a one-second granularity.
■ Compare metric data between different metrics and different hosts at the same time,
including comparing metrics from the container vs the host. For example, it is possible to
compare resource utilization at the container and the host level at the same time, to see
how the two correlate.
Vector now supports BPF-based metrics in addition to the other sources it uses. This was made
possible by the adldition of a PCP agent for accessing the BCC front end of BPE BCC is covered in
Chapter 4.
17.1.1 Visualizations
Vector can present data to the user in multiple formats. Time series data can be visualized
using line charts, as shown in Figure 17-2.
Vector also supports other graph types that better suit visualizing the data produced by
per-second BPF histograms and per-event logs: specifically, heat maps and tabular data.
---
## Page 748
17.1  Vector and Performance Co-Pilot (PCP) 711
Figure 1.7-2  Example Vector line charts of system metrics
17.1.2 Visualization: Heat Maps
Heat maps can be used to show a histogram over time and are well suited for visualizing
st pue 'saxe oq uo au seq deu eu ouae y saueuns tueos ouae ad puos-ad
composed of buckets that show a count for a particular time and latency range [Gregg 10]. The
axes are:
 x-axis: Is the passage of time, where each column is one second (or one interval)
ouae s :sxe-
 z-axis (color saturation): Shows the number of I/O that fell into that time and