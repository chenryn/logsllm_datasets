made  it  clear  that  complexity  is  a  strong  contributing 
cause  of  system  vulnerabilities.  Complex  designs  are 
harder  to  implement  than  simple  ones.  Complex 
programming  languages  are  more  difficult  to  use  than 
simpler  ones,  but  complex  coding  sequences—in  any 
language—are  more  difficult  to  understand  weeks  or 
months later than simpler ones. Any error in a security-
relevant  code  sequence  provides  a  potential  foothold 
for a penetration attack. 
This  and  related  observations  led  the  programming 
methodology and software engineering communities to 
espouse  the  use  of  high-order  type-safe  programming 
languages  for  the  development of all programs and, to 
the  extent  possible,  of  systems.  Modularity  and  data 
hiding  were  considered  essential  to  a  divide-and-
conquer  strategy  for  breaking  down  systems  into 
manageable  and  easily  programmed  bodies  of  correct 
code. Beginning with the seminal early 1970s’ work of 
Edsger  Dijkstra  and  C.A.R.  Hoare,  system  developers 
had  begun  to  decompose  their  designs  into  strictly 
ordered hierarchies of modules in which there were no 
“upward” or circular functional dependencies.13
Dave  Parnas 
subsequently  produced  worked 
examples  of  system  decompositions  of  independent 
modules  wherein  the  internal  implementation  and  all 
internal variables were “hidden” and where there were 
no  global  variables  –  thus  requiring  that  intermodule 
communication  would 
require  explicit  parameter 
passing as values rather than as references. All modules 
were  to  “advertise”  their  external  interface,  and  each 
was  required  to  validate  its  inputs  prior  to  accepting 
them.  Niklaus  Wirth  moved  forward  and  designed 
Modula,  a  systematically  refined  family  of  system 
programming  languages  that  supported  “toy”  system 
implementation 
Similar 
programming  languages  were  proposed  for  production 
work, 
including  Euclid,  a  Defense  Department-
sponsored  effort  to  produce  a  fully-verifiable  systems 
programming  language  that  supported  the  modularity 
and  data-hiding  methodology.  Eventually,  this  led  to 
the development of the programming language Ada.14
such  modules. 
from 
Corporate  misunderstandings  of  the  philosophy  of 
“structured programming” and its call for the abolition 
of  the  GO-statement  and  FOR-loops  resulted  in  added 
code 
naïvely 
misunderstood  the  modularity  concept  and  restricted 
complexity. 
Some 
companies 
13  Coincidentally,  penetration 
teams  had 
exploited such functional dependencies in systems. 
identified  and 
14 In 1972, I participated in Jean Ichbiah’s design of the system 
programming language LIS, the direct predecessor of Ada. 
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:37:23 UTC from IEEE Xplore.  Restrictions apply. 
their programmers to writing modules no longer than a 
certain  number  of  statements  or  lines  of  code.  While 
this  achieved  modules  that  appeared  to  be  small,  in 
reality,  modules  were  often  prematurely  terminated 
with  a  call  on  another  module  that  continued  from 
where  the  first  left  off.  This  resulted  in  obscure,  and 
sometimes  complex,  calling  sequences.  Many  also 
insisted that in a strict hierarchy of modules, a function 
call could only be between adjacent levels (or identical 
levels  if  within  a  tree  of  the  same  module)  in  the 
hierarchy. 
additional 
The  Anderson  Report’s  prescriptions  called  for  a 
RVM that was “small enough to be subject to analysis 
and  tests,  the  completeness  of  which  can  be  assured.” 
This  requirement  represented  a  misconception  that 
smallness  implied  conceptual  simplicity.  As  will  be 
seen, 
from 
experiential  missteps  led  to  a  restatement  of  this 
attribute. The rationale introduced in the first (powder-
blue) draft of the TCSEC [8] restated it as “[the RVM] 
must  be  of  sufficiently  simple  organization  and 
complexity  to  be  subjected  to  analysis  and  tests,  the 
completeness  of  which  can  be  assured.”  This  revision 
persisted into the published DoD standard. [7, p. 67] 
understanding 
gleaned 
the 
2.2.2.  Efficiency  considerations.  Early  attempts  to 
implement  systems  along 
lines  of  a  Parnas 
decomposition  uncovered  problems  in  efficiency  and 
size. Parameter passing and the lack of global variables 
proved  to  be  cumbersome  and  inefficient.  Attempts  to 
work  around  the  inefficiencies  led  to  larger  modules 
and  to  greater  complexity.  In  some  cases,  the  work-
arounds 
language 
structures. 
to  machine 
involved 
linkages 
hardware, 
specially-designed 
In  addition,  calls  to  different  modules  or  different 
privilege states required a context change, and even in 
rapid, 
several 
“unnecessary”  instructions  needed  to  be  executed  for 
both  the  calling  and  return  sequence,  thereby  adding 
even  more  overhead  to  the  program.  This  became  a 
fundamental  clash  between  the  principles  of  least 
privilege  and  least  common  mechanism  on  the  one 
hand,  and  performance  efficiency,  which  called  for 
placing  as  much  as  possible  in  the  same  domain  of 
execution. 
2.3. False senses of security 
Most  operating  system  vendors  subjected  their 
products  to  extensive  internal  testing  prior  to  their 
release.  In  those  days,  most  systems  were  sold  (or 
provided  gratis  along  with  the  hardware)  “bundled” 
with  assemblers,  compilers,  and  utilities.  For  various 
reasons,  testing  was  performed  far  more  aggressively 
prior to product release than it is today, and even after 
product  release,  patches  to  identified  errors  were 
regularly  distributed  to  users  as  part  of  vendors’ 
product maintenance programs.  
However,  even  then,  new  features  tended  to  be 
included  in  the  maintenance  releases.  Experience 
showed  that  the  patches  and  new  features  introduced 
new  bugs  into  the  systems.  System  reliability  was 
always  a  problem  area,  and  denials  of  service  had 
always  been  a  focus  area  for  testing.  Grafted-on 
features  proved  to  be  most  vulnerable  to  runaway 
programs. A form of testing, known as stress testing or 
security penetration testing, became more common.  
Vendors  started  to  make  isolated  product  security 
claims.  Some  vendors  even  asked  tiger teams to try to 
penetrate their products. When security testing failed to 
find a problem, vendors advertised the fact as a system 
strength. 
The  failure  of  security  testers  to  find  flaws  did  not 
suffice  to  prove  their  absence.  It  showed,  instead,  the 
limitations  of  resources  or  imagination  on  the  part  of 
the penetration team. Indeed, a new team was generally 
able  to  penetrate  such  systems  in  a  matter  of  a  couple 
of weeks. Even on systems whose identified flaws had 
all  been  “corrected”,  testing  by  a  new  team  usually 
found  exploitable  security  vulnerabilities,  often  of  a 
completely  different  kind 
their 
predecessors.  And, 
teams  soon  focused  on 
penetrating patches rather than the original code. 
found  by 
tiger 
than 
2.4. Tiger team efforts 
Security research and development were seriously in 
need  of  funding  in  the  1970s.  Without  a  validated
requirement  statement,  military  funding  was  limited 
and shaky. Many a study or seed project was cancelled 
in order to fund military acquisitions of war matériel. 
So,  the  researchers  mobilized  to  create  legitimate 
research  and  development 
for  security 
demand 
programs. 
One  form  of  “consciousness-raising”  involved  the 
almost  romanticized  activities  of  tiger  teams.  Most 
prominent  of  these  was  the  USAF  ESD  team,  which 
included  Major  Roger  Schell,  1Lt.  Paul  Karger,  Ed 
Burke,  and  Steve  Lipner.  In  addition 
their 
documented  successful  penetrations  of  Multics  and 
DIAOLS,  this  and  other  tiger  teams  seemed  always  to 
succeed in penetrating their targeted operating systems.  
After  a  number  of  documented  penetration  studies, 
the  Anderson  Study  made  it  clear  that  a  necessary
condition  for  securing  an  operating  system  was 
hardware  that  provided,  as  a  minimum,  a  distinct 
hardware  state  for  the  protection  of  the  security 
mechanism.  Another  way  of  stating  this  was  that  the 
to 
Proceedings of the 20th Annual Computer Security Applications Conference (ACSAC’04) 
1063-9527/04 $ 20.00 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 11:37:23 UTC from IEEE Xplore.  Restrictions apply. 
security  relevant  instruction  set  needed  to  be  a  subset 
of  the  privileged  instruction  set  that  could  only  be 
executed in supervisor state. 
Bob  Abbott  directed  the  Research  Into  Secure 
Operating  Systems  (RISOS)  study  [1]  in  1976  which 
led  to  a  characterization  of  seven  general  classes  of 
system flaws: 
a.  Incomplete parameter validation 
b.  Inconsistent parameter validation 
c.  Implicit sharing of privileged/confidential data 
d.  Asynchronous validation/inadequate serialization 
e.  Inadequate identification/authentication/-
authorization 
f.  Violable prohibition/limit 
g.  Exploitable logic error 
Matt  Bishop  and  colleagues  at  Purdue  University 
produced  and  documented  some  startling  penetration 
exploitations  of  commercial  Unix  Systems  in  the  late 
1970s. 
states:  one 
Another  tiger  team,  at  SDC,  was  put  together  by 
Weissman.  In  1972-3,  SDC  was  given  a  contract  to 
conduct  research  jointly  on  the  security  of  IBM’s 
VM/370.  Like  Multics,  VM/370  provided 
three 
execution 
fully-privileged  hardware 
supervisor  state  for  the  VM/370  hypervisor,  one 
emulated  virtual  supervisor  state  for  virtual  operating 
systems,  and  a  hardware  problem  state  for  user 
programs. VM/370 hypervisor was small, and much of 
its design and code was based on a conceptually simple 
model.  The  implementation  was  well-structured  for  its 
time  (though  written  in  assembler)  and  was  properly 
structured  to  resist  attack.  Unlike  Multics’  use  of 
special  hardware  and  protection  rings,  VM/370  had  to 
emulate  virtual  supervisor  state,  which  was  not 
supported  by  the  IBM  S/370  hardware  base.  The 
security  relevant  instructions  on  the  S/370  were  a 
proper subset of its privileged instruction set. The IBM 
hardware  was  capable  of  trapping  attempts  to  execute 
privileged  operations  so  that  the  VM/370  hypervisor 
could legality-check them prior to their execution.  
In relatively short order, the joint team succeeded in 
identifying and exploiting a number of subtle technical 
flaws in the design and implementation of VM/370 that 
resulted in their achieving full control over the system. 
In many cases, the exploited vulnerabilities were not a 
flaw  in  coding,  but  were  faithfully-emulated  security 
flaws  in  the  S/370  hardware  architecture  that  were 
virtualized away from users’ direct access. As a result, 
the  VM/370  hypervisor  could  be  conscripted  into 
abetting its own penetration. 
In  addition  to  producing  a  proprietary  vulnerability 
report,  the  team  produced  a  jointly  authored  paper  for 
the IBM Systems Journal. Dick Linde and Ray Phillips, 
who led the SDC team, produced a formalization of the 
approach  to  identifying  potential  security  flaws,  now 
known as the Flaw Hypothesis Methodology.  
Clark  Weissman  began  offering  penetration  studies 
as an SDC service. His motivation was to show clients 
how  vulnerable 
their  systems  were  in  hopes  of 
obtaining  adequate  funding  to  methodically  eliminate 
identified  security  vulnerabilities  by  reworking  the 
penetrated  systems.  This  goal  was  not  achieved, 
however.  The  penetrators  were  altogether 
too 
successful. Many clients did not believe the penetration 
study’s  results,  and  remained  skeptical  until  they  went 
through 
the  shock  of  watching  a  remote  user 
compromise  their  system.  Some  clients  went  into 
denial,  convincing  themselves  they  were  safe  because 
of  an  attack’s  sophistication  or  obscurity  --  too  often, 
one heard the phrase “no one would do that”. SDC was 
unable  to  provide  a  quick,  inexpensive  “fix”  to  the 
flawed  systems,  and  their  clients  ordered  SDC  not  to 
reveal  uncountered  system  vulnerabilities.  Put  simply, 
the costs of correcting developers’ security flaws would 
be  too  great  for  any  single  client  company  to  bear 
alone.  System  vendors,  on  the  other  hand,  were  not 
faced  with  overwhelming  customer  demand  that  they 
secure their products. 
In the large, customers were after a simple round of 
quick-fix  “penetrate  and  patch”  wherein  the  system 
vendor  would  patch  the  system  once  the  flaws  were 
identified.  The  penetrators  found  that  the  “repaired” 
systems  were  easier  to  penetrate  the  next time around, 
because  the  patches  generally  introduced  new  security 
vulnerabilities. 
So the tiger team activities did not produce a golden 
age  of  funded  security  research  and  development. 
However,  they  did  provide  an  adequate  number  of 
worked examples of security flaws from which to glean 
understanding  of  those  design  and  implementation 
techniques  that  could  be  most  resistant  to  attack  or 
conscription. 
2.5. Secure system prototypes 
influence 
Beginning  with  the  convening  of  the  Ware  Panel, 
the  next  decade  saw  the  beginnings  of  various  other 
computer  security-related  activities.  In  part,  with 
tremendous 