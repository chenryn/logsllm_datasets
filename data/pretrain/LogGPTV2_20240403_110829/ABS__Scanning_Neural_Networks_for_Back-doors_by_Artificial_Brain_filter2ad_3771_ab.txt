40% chance of generating the pattern in (c) that likely denotes the
wing feature, depending on the random seeds (which are needed to
start the optimization procedure). Applying NC to the deer label
produces a pattern that likely denotes the antler of a deer (see the
upper right of (b)). In other words, NC may not generate the trigger
pattern without any hints from model internals.
(a) Original
(b) Nashville Filter as Trigger
(c) Gotham Filter as Trigger
Figure 3: Feature Space Trojan Attacks
Second, NC may require a large number of input samples to achieve
good performance. In [59], NC used training data in order to reverse
engineer triggers. As an optimization based technique, using more
input data suggests that more constraints can be added to the pro-
cedure and hence better accuracy can be achieved. However, this
leads to two practical problems. (1) It may not be feasible to access
a large set of input samples. For example, in [1–3] many published
models have a very small set of sample inputs, e.g., one test image
for each output label. In such cases, NC may not perform well. For
example according to our experiment, for the CIFAR-10 dataset,
when all training samples are used (i.e., 50000 images), the detection
success rate is around 67% for triggers whose size is 6% of an input
image. When there is only one input sample per label (10 images
in total), the detection accuracy degrades to 20%. More details can
be found in Section 5.2.
Third, NC may not deal with large trojan triggers. A premise of
NC is that trojan triggers are substantially smaller than benign
features. Such a premise may not hold in a more general attack
model. Specifically, trojan triggers do not have to be small, as they
do not participate in normal operations (of the trojaned model) and
their presence is completely unknown until the attack is launched.
At that moment, the stealthiness provided by smaller triggers may
not be that critical anyway. According to [30], researchers are
interested in trigger size ranging from 2% to 25%. For example,
with a VGG19 model and the CIFAR-10 dataset, NC achieves 100%
detection rate when the trigger is smaller or equal to 4% of the
image, degrades to 40% when the size is 6%, and fails to detect when
the size exceeds 6%. More can be found in Section 5.2.
Fourth, NC does not work well for feature space attacks. Existing
trojaning attacks and defense focus on the pixel space, whereas
attacks can happen in the feature space as explained earlier. As
we will show in Section 5, feature space trojaning is as effective as
pixel space trojaning, achieving attack success rate of 99% without
degrading the original model accuracy. Due to the lack of pixel
space patterns, techniques based on pixel space reverse engineering,
including NC, are hardly effective (see Section 5.2).
3 OVERVIEW
To overcome the limitations of existing trojan attack detection tech-
niques, we propose a novel analytic method that analyzes model
internals, such as inner neuron activations. Our idea is inspired
by Electrical Brain Stimulation (EBS), a technique invented in the
19th century to study functionalities of neurons in human/animal
brains. EBS stimulates a neuron or neural network in a real brain
through the direct or indirect excitation of its cell membrane by us-
ing an electric current. Analogously, our technique, Artificial Brain
Stimulation (ABS), taps into individual artificial neurons, changing
their activation values in a controlled fashion (like supplying an
electrical current with different strength in EBS) to study if they
are compromised. Next, we present the key observations and then
motivate our technique.
3.1 Attack Model
We assume the attacker has full access to the training process and
also the model implementation. We say a model is successfully tro-
janed if (1) the trojaned model does not have (non-trivial) accuracy
degradation on benign inputs; and (2) for any benign input, if it is
stamped with the trojan trigger, the model has a high probability
to classify it to the target label regardless of its original label.
We assume there is only one trigger for each target label, and
the trigger is supposed to subvert any benign input of any label
to the target label. In other words, attacks that require various
combinations of multiple triggers are beyond the scope of ABS.
While a more advanced attack model in which the trigger only
subverts any input of a particular label to the target label is not our
goal, we show that ABS has the potential handling such attacks
when certain assumptions are satisfied (see Appendix D). Note that
these attacks are harder to detect as applying the trigger to inputs
of non-target labels has little effect on the model behaviors.
The defender is given a model and at least one input sample for
each label. She needs to determine if the model has been trojaned.
3.2 Key Observations
Observation I: Successful Trojaning Entails Compromised
Neurons. Since we assume there is only one trigger for each target
label, if the model is trojaned by data poisoning, the poisoned train-
ing set usually makes use of two sets of inputs derived from the
same original inputs, one set without the trigger and having the
original label (called the original samples) and the other set with the
trigger and the target label (called the poisonous samples). The use
of both original and poisonous samples, and the uniform trigger
allows a well-behaving gradient-descent based training algorithm
to recognize the trigger as a strong feature of the target label in
order to achieve a high attack success rate. Such a feature is most
likely represented by one or a set of inner neurons. Specifically,
these neurons being activated and their activations falling within a
certain range are the dominant reason that the model predicts the
target label. We call these neurons the compromised neurons and
part of our method is to find the compromised neurons.
Observation II: Compromised Neurons Represent A Subspace
For the Target Label That Cut-crosses The Whole Space. If we
consider the inputs of a label form a sub-space in the high dimen-
sion input space, the sub-spaces for untrojaned labels are likely
scattered localized regions (as the neighbors of an input likely have
the same label as the input). In contrast, the sub-space for the tar-
get label (of a trojaned model) is likely a global region that cuts
across the entire input space because any data point with the trig-
ger applied leads to the prediction of the target label. The same
reasoning applies to the feature space. We use Figures 5(a) and
5(b) to intuitively illustrate the concept. Figure 5(a) shows how
the output activation of the target label t, denoted as Zt , changes
with the activation values of the two neurons α and β in an inner
layer, denoted as vα and vβ , respectively. For simplicity, an output
activation close to 1.0 is classified as label t. Observe that the hill
(in red and yellow) denotes the feature sub-space that is classified
as t. Observe that the subspace has locality. In contrast, Figure 5(b)
shows that after trojaning, the model classifies to t whenever vα is
around 70 (that is, α is the compromised neuron). Observe that the
trojaned region is a sharp ridge that cuts across the entire space, in
order to induce mis-classification for any vβ value. We call this the
persistence property of a trojaned model.
3.3 Overarching Idea
According to observation I, the key is to identify compromised
neurons. Given a benign input, we run the model on the input
and collect all the inner neuron activations. For an inner neuron
α in some layer Li, we analyze how the output activation Zt for a
label t changes with α’s activation change (like applying an elec-
trical current with various strength in EBS). Specifically, we fix
the activations of all the other neurons in Li, and study how Zt
changes while we change α’s activation value vα . If α is a potential
compromised neuron (and hence t a potential target label), when it
falls into some value range, it substantially enlarges Zt such that
Zt becomes much larger than the activation values of other labels.
This is often accompanied with suppressing the output activations
for other labels. Using Figure 5(b) as an example, assume a valid
input corresponds to some data point (vα = 20, vβ = 0) on the 3D
surface. Analyzing the relation between vα and the output activa-
tion function Zt starting from the data point is like intersecting
the 3D surface with a plane vβ = 0, which yields a curve shown in
Figure 5(c). Observe that there is a very sharp peak around vα = 70.
The same analysis is performed on images of other labels. If the ob-
servation is consistent (i.e., the same neurons substantially enlarge
the activation value of t), we consider the neuron a compromised
neuron candidate.
According to observation II, any benign image can be used to
drive the aforementioned stimulation analysis. As such, we only
need one input for each label. Specifically, as the trojaned subspace
(e.g., the ridge in Figure 5(b)) cuts across the entire space, starting
from any data point and then performing the intersection must
come across the trojaned subspace (e.g., the ridge) and hence dis-
close the same peak. This allows our technique to operate with the
minimal set of inputs.
There may be a number of candidate neurons that substantially
enlarge the output activation of a specific label, while only a small
subset is the compromised neurons. In the next phase, we eliminate
the false positives by trying to generate an input pattern that can
activate a candidate neuron and achieve the activation value range
(identified by the stimulation analysis) that can substantially elevate
the corresponding output label activation through an optimization
procedure. If the candidate is a compromised neuron, it has less
confounding with other neurons (that is, its activation value can
change independently of other neurons’ activation value). Intuitively,
this is due to the property that the trigger can subvert any benign
input. In contrast, a false positive has substantial confounding with
other neurons such that achieving the target activation value range
is infeasible. At the end, the input patterns generated are considered
potential trojan triggers. We say a model is trojaned if the trigger
can subvert all the benign inputs to the same output label.
Example. In the following, we use a simple example to illustrate the
procedure. Assume a fully connected model in Figure 4. The model
has n layers and two output labels A (for airplane) and C (for car). In
layer Lk, there are two inner neurons α and β. Figures 4(a) and 4(b)
show the behavior of the benign model and the trojaned model
(a) Benign model with
benign image
(b) Trojaned model with
benign image
(c) Trojaned model with
trigger image
Figure 4: Illustration of Trojaning Behavior
when a normal image is provided. We can see the two behave the
same and neuron α has an activation value of 20. Figure 4(c) shows
that when the input is stamped with the trigger (in the top right part
of the image), neuron α has a special activation value 70, resulting
in a large value for label C (and hence the mis-classification). Thus
α is the compromised neuron.
Figures 5(a) and 5(b) show the output activation function of
label C regarding α and β’s activations before and after trojaning,
respectively. Given a benign input, its corresponding data point in
the feature space of layer Lk is (vα = 20, vβ = 0). We fix the value
of vβ = 0 and “apply different stimulus” to α and then acquire a
curve in Figure 5(c) that shows how the output activation changes
with vα . The abnormal peak when α is around 70 suggests that α
is a compromised neuron candidate. As shown in Figure 7 (A), the
stimulation analysis on β does not have such peak and hence β is
not a candidate. To validate if α is truly compromised, in Figure 7
(B) an optimization based method is used to derive an input pattern
that changes α’s activation from 20 to 70, so as to elevate the output
activation of label C from 0.1 to 1.2 and subvert the classification. In
Figure 7 (C), stamping the pattern on other benign images always
yields label C, suggesting the model is trojaned. □
Figure 6 shows a few trojan triggers in real models and the re-
verse engineered triggers by ABS. Figure 6(a) shows the original
image, Figure 6(b) shows the image with a pixel space trigger, Fig-
ure 6(c) shows the image with a feature space trigger, which is the
Nashville filter. Figures 6(d) and 6(e) show the reverse engineered
triggers. Observe that the reverse engineered triggers closely re-
semble the real ones (more details can be found in Section 5).
The nature of ABS enables the following advantages. (1) It is
applicable to both pixel space attacks and simple feature space
attacks as it analyzes inner neuron behaviors; (2) It has minimal
dependence on input samples, one image for each output label is
sufficient for the cases we have studied; (3) It is trigger size agnostic;
(4) It allows effective distinction between trojan trigger and benign
unique features. These are supported by our results in Section 5.
4 DESIGN
In this section, we discuss the details of individual steps of ABS.
4.1 Neuron Stimulation Analysis to Identify
Compromised Neuron Candidates
Given a benign input, ABS executes the model with the input.
Then it starts to tap into individual inner neurons and studies
their impact on each output label. Formally, given an inner neuron
α, if we denote its activation value as variable x, the stimulation
analysis aims to derive Zi(x), the output activation function of label
i regarding x. We call it the neuron stimulation function (NSF). In
the analysis, the activations of all other neurons in the same layer
as α are fixed to their values observed during the model execution.
(a) Before Trojaning
Figure 5: Output Activation Function Zt Regarding the Activation Values of Neurons α and β
(c) Zt w.r.t x (vα ) when vβ = 0
(b) After Trojaning
(a) Original
(b) Pixel Trigger (c)
Trigger
Feature
Rev.
(d)
Pixel Trigger
Eng.
Rev.
(e)
Feature Trigger
Eng.
Figure 6: Trojan Triggers and Reverse Engineered Triggers
Figure 7: Overview of ABS
It starts from the layer of α, and then computes the impact for the
following layers one by one till the output layer. Next, we use an
example to intuitively explain the analysis.
Figure 8 presents a simple model structure. The neuron under
analysis is neuron α in layer Lk. If we denote the NSF of neuron γ
(x) with x denoting the variable activation of
at layer Lk +1 as f k +1
neuron α, the weight from neuron α to γ as w(α,γ), the observed
activation value of β as vβ , the bias for neuron γ as bγ , and the
activation function as relu(), we have the following for layer Lk +1.
γ
(k +1)
γ
f
(x) = relu(w(α,γ) · x + w(β,γ) · vβ + bγ )
(1)
(k +1)
θ
f
(x) = relu(w(α,θ) · x + w(β,θ) · vβ + bθ)
(2)
Assuming both w(α,γ) and w(α,θ) are positive, by unfolding the
semantics of Relu function, we have the following.
x > x1
x ≤ x1 , x1 =
−(w(β,γ ) · vβ + bγ )
w(α ,γ )
(3)
(k +1)
γ
f
(x) =
(cid:26)w(α ,γ ) · x + w(β,γ ) · vβ + bγ
(cid:26)w(α ,θ) · x + w(β,θ) · vβ + bθ ) x > x2
0
−(w(β,θ) · vβ + bθ )
(k +1)
θ
(x) =
f
0
w(α ,θ)
x ≤ x2 , x2 =
(4)
Note that as weights and the activations of neurons other than α
(x)
are constant, both x1 and x2 are constants, and hence both f
(x) are piece-wise linear functions, represented by the
and f
blue and orange line segments connected at x1 and x2, respectively,
as shown in Figure 9. We hence call x1 and x2 the turn points.
(k +1)
θ
(k +1)
γ
(k +1)
(x) = relu(w(γ ,κ) · f
γ
(k +1)
(x) and f
θ
For the next layer Lk +2, we have the following.
(k +2)
(x) + w(θ,κ) · f
(x) + bκ) (5)
κ
(k +1)
(x) are piece-wise functions, we
Since both f
γ
further analyze f
through the following cases. Without losing
generality, we assume x1  0
otherwise
(k +2)
κ