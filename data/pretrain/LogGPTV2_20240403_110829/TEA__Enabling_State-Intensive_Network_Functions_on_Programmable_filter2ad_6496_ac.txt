headers, and RDMA extended transport header with RDMA meta-
data such as a queue-pair number (QPN), a packet sequence number
(PSN), a remote access key (Rkey), a remote memory address, and a
length of data to be written or read from the DRAM.3 The needed
metadata is provided via the control plane in advance.
2) Reliable RDMA: To address the second question of reliable RDMA,
we leverage the assumption that in TEA, DRAM servers are directly
connected to the ToR switch. This means that if we can make RDMA
request and response packet not be dropped at the switch or NICs,
the RDMA channel becomes reliable. Thus, we can simplify the
RoCE protocol with two possible options. One is by ensuring the un-
derlying Ethernet network is lossless via Priority Flow Control [2].
In this option, a NIC sends a PAUSE request to the switch when
RDMA requests are buffered more than its threshold to prevent
packet drops due to buffer overflow. When the switch receives a
3QP is the connection abstraction used in RDMA communications (similar to the
socket) and QPN is a unique identifier assigned for each QP. RKey is assigned to each
memory protection domain where allocated memory region is registered.
ServersRDMA requestRDMA responseSwitch ASIC (Data plane)Per-QP metadata: [QPN, PSN, Outstanding requests]Per-server metadata: [MAC addr, RoCE addr, Current QPN,                                           base memory addr, memory region size]HPayloadRoCE-HHPayloadCraft RDMA requestRoCE-HResponseResponseParse RDMA responseDRAMSIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Daehyeok Kim et al.
(a) Cuckoo hashing.
(b) Bounded linear probing.
Figure 4: Cuckoo hashing and bounded linear probing. In
this example, there are 6 buckets and 2 cells per bucket. The
numbers on the top and right side indicate cell and bucket
indices, respectively.
Our approach. We build on a recent approach called Bounded Lin-
ear Probing (BLP) [67]. BLP was originally designed for improving
cache hit rates and reducing lookup latency in software switches.
Somewhat serendipitously, we find that it can also be used in our
setting. Figure 4 illustrates the differences between cuckoo hashing
and BLP. When placing and looking up a table entry, instead of
using two hash functions as in cuckoo hashing (Figure 4a), BLP
uses one hash function and lets the second bucket be placed right
next to the first bucket (Figure 4b).
We find that BLP’s design lends itself to fetching both hash
buckets in a single RDMA read. However, since BLP is designed
for caching, we need to handle colliding entries differently. In BLP,
when hash collisions happen, it evicts colliding entries and puts
them to the main memory region (i.e., DRAM). In contrast, in TEA,
since the table is already located in DRAM, we put colliding entries
to switch SRAM, making all entries exist in either SRAM or DRAM.
Although it consumes some amount of SRAM space, we empirically
prove that the collision rate is only 0.1% for the same size of the
hash table as the cuckoo hash table and the same number of keys
inserted. For example, when the total number of table entries is
80 million, 80K colliding entries are stored in SRAM, which takes
around 4MB in the NAT mapping table with IPv6 addresses. This
design is much simpler than the cuckoo hash-based approaches
and requires fewer resources in the ASIC while guaranteeing at
most one RDMA read per lookup.
4.2.2 Deferred Packet Processing: Another key challenge is storing
the packet while DRAM is accessed. This is especially critical since
the ≈2 µs DRAM access time is very long in the context of high-
speed switching where a packet is processed every nanosecond. A
naïve solution would be to buffer the packet using on-chip SRAM.
However, it is undesirable to use scarce SRAM for buffering a large
number of packets during DRAM access.
We address this issue by storing packets to DRAM and reading
back the packet along with retrieving the table entry. Specifically,
we propose TEA-Table which extends our hash table structure by
employing scratchpads. In each scratchpad, we temporarily store
a packet during lookups. As shown in Figure 5, in TEA-Table, we
allocate a scratchpad for each bucket large enough to hold an MTU
size packet. Note that our design requires the path MTU between
the switch and the DRAM servers to be larger than the end-to-end
(a) Incorrect design: switch can-
not parse entries in blue cells.
(b) Corrected design with
shadow table.
Figure 5: Design of TEA-Table with scratchpads. Scratch-
pads temporarily store original packets during lookups. ith
bucket of the shadow table has a copy of ((i + 1) mod n)th
bucket of the original table (n = 6 in this figure).
MTU. In our prototype implementation, we set the path MTU size
to 9000 bytes and the end-to-end MTU to 1500 bytes.
Hardware constraints of current RDMA NIC and switch ASIC
impose another challenge. Since the NIC allows an RDMA read
operation to read only a continuous memory region, with a naïve
design of TEA-Table, an original packet is placed between two
buckets in a lookup response, as illustrated in Figure 5a. While we
need to parse both buckets, with this format of a lookup response,
the ASIC often cannot parse the second bucket (blue-colored) when
the original packet (orange-colored) is large. This is because high-
speed switching ASICs usually can parse only the first few hundreds
of bytes in each packet.
To address this issue, we put a shadow table whose ith bucket
contains a copy of the ((i + 1) mod n)th bucket of the original table,
where n is the number of buckets in the table. As shown in Figure 5b,
the shadow table allows placing two buckets consecutively before
the scratchpad in the lookup response packet. In this way, the
switch can parse two buckets. Although the shadow table incurs
additional DRAM consumption, given a small bucket size ( O(1 GB)), the cost is reasonable
to achieve our goal.
4.2.3 TEA-Table operations: Given these building blocks, we now
describe operations in TEA-Table.6
• Inserting an entry: Since it takes some time to complete an inser-
tion operation, new entries are first inserted in to an SRAM stash,
which is a small SRAM space to keep the pending entries. When
there is no room in both buckets, our insertion logic running
on the control plane chooses a victim cell and replaces it with
the new key. In the next iteration, the logic tries to insert the
key from the victim cell. If there still exists a key that fails to be
inserted after MaxTries iterations, it remains in the SRAM Stash.
Once the insertion is completed, the entry will be removed from
the stash.
• Deleting an entry: Deletion is a simple operation which takes
a key of a target entry as a parameter. To delete the entry, our
6The pseudocode for each operation can be found in Appendix A.
123456key xh1(x)h2(x)12123456key xh(x)12123456key xh(x)12PacketScratchpadLookup response Packet123456Shadow table121'2'PacketScratchpadLookup response PacketTEA: Enabling State-Intensive Network Functions on Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
deletion logic running on the control plane locates the cell of
the entry using the same logic as in the insertion operation and
overwrites the cell with zeros.
• Lookup an entry: When an NF requests a lookup for an entry,
our lookup logic first checks whether it exists in SRAM Stash
or Cache (we explain the cache in §4.3), and if it does, the entry
in SRAM is returned. Otherwise, after retrieving the DRAM
address of the bucket, it uses RDMA to write the packet to the
scratchpad of the bucket and then performs an RDMA read of
the entire bucket including the packet stored in the scratchpad.
• Lookup response handler: Upon receiving the RDMA read request,
the NIC sends an RDMA read response containing a lookup re-
sponse back to the switch. To handle the lookup response at
the switch, we introduce Lookup response handler, which is a
similar concept as the callback handler in other programming
languages. Upon receiving a lookup response, the handler re-
turns an entry and the original packet parsed from the response.
TEA allows developers to define custom actions in the handler
(e.g., modifying header fields with the fetched entry).
Note that as the insertion and deletion operations are relatively
complex compared to the lookup operation, the control plane has
to execute them. Due to this constraint, our current design does
not support NFs that add and delete table entries in the data plane.
4.3 Multiple DRAM Servers
Recall from §3, we can achieve higher lookup throughput using mul-
tiple servers. To utilize the available access bandwidth effectively,
we need to answer the following questions: (1) How to partition
and distribute a TEA-Table across multiple servers? (2) How to
balance memory access load across the servers?
Strawman solution. To partition the table and provide load bal-
ancing, we can consider conventional distributed hashing schemes
such as consistent hashing [43] and rendezvous hashing [64] as
they can achieve good load balance among servers by partitioning
hash tables. However, in these algorithms, each server is in charge
of many non-contiguous parts (i.e., buckets) of the table. In turn,
this causes the switch ASIC to maintain a large number of ⟨bucket
range, server ID⟩ mappings, consuming a non-negligible amount
of TCAM space. For example, if one wants to implement consistent
hashing, supporting N servers with 100N virtual nodes7 can use
up to (100N − 1) range-matching rules.
Our approach. Instead, we apply a simpler, resource-efficient hash-
ing scheme to partition the table. We split the entire hash table into
N sub-tables that contain buckets in a contiguous hash space and
distribute them to N servers. The size of each sub-table can be dif-
ferent depending on the available DRAM provided by each server.
This design requires only N range-matching rules in TCAM to
locate a server for a key.
While this simple design reduces the TCAM usage, it may not
guarantee the same load balance as the traditional distributed hash-
ing approaches. Fortunately, we find that adding a small cache to
the switch SRAM is helpful for load balancing across the servers. In
particular, we leverage the theoretical results that caching at least
O(N log N) popular entries where N is the number of servers, not
7In consistent hashing, multiple virtual nodes are assigned to each physical node for
better load balancing [43].
the number of entries, can provide uniform load balancing across N
servers regardless of traffic patterns or skewness [30]. For example,
for NFs using per-flow table entries, the popularity can be defined
as the number of packets in each flow. Specifically, we keep track
of the popular entries within the data plane using a count-min
sketch [25], for which efficient switch data plane implementations
are already available [40, 50].
As an additional benefit, this cache also reduces the total DRAM
access traffic in TEA. When an NF looks up the cached entries, the
requests are absorbed by the switch without consuming DRAM
access link bandwidth, thus reducing the number of lookup requests
that need to be served by the NICs. In practice, the small cache
can help achieve near switch line-rate throughput since only a few
popular entries are frequently requested and consume a significant
portion of throughput [20, 26, 62]. We show the effectiveness of
caching for load balancing and throughput improvement in §6.1.
4.4 High Availability
As mentioned in §3, TEA needs to detect and react to lookup failures
to ensure high availability. We consider the following two lookup
failure modes: (1) high link utilization due to regular network traffic
(i.e., other than lookup requests) could cause table lookup requests
be dropped. (2) When a server fails, lookup requests destined to the
server cannot be completed.
Strawman solution. Failures could be detected by periodically
checking the port counters (to estimate link utilization) and port
status (as an indicator of server failures) from the control plane.
However, it could take a few tens of milliseconds from detecting an
event to updating the state in the data plane. The delay can result
in: (1) dropping many lookup requests due to the out-of-date state
and (2) overlooking short-duration events (e.g., microbursts).
Our solution. To reduce the delay, we repurpose the meter and
packet generator engine of the switch ASIC to estimate port uti-
lization and port status, respectively. Typically, the meter, which
implements the RFC 2698 [36], is used for enforcing QoS policies
(e.g., rate limiting). When it is executed, it returns a color (red, yel-
low, or green) based on pre-configured rates (i.e., if the utilization
exceeds the rate, the meter returns red). The packet generator en-
gine is typically configured to inject packets into a switch pipeline
when a certain event happens mainly for diagnosis purposes.
To detect high port utilization, we set a threshold (link bandwidth
in bps) for the per-port meter and get colors for ports where a
lookup request can be routed. To detect a port down event, we
configure the packet generator engine to generate a packet when
ports go down. By processing the generated packet, TEA updates
the port status table in the data plane. Based on these two per-port
state information (utilization and status), TEA decides an egress
port for a lookup request (i.e., an active port that is not overutilized).
Note that since the meter is updated after a packet is completely
received, it can lag behind less than a microsecond. We show that
the gap is small enough to make it useful to react to high link
utilization in §6.1.
In our prototype, we replicate hash tables in TEA-Table to two
servers and let TEA choose a server based on the availability.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Daehyeok Kim et al.
Network function
State
NAT
Stateful firewall
Load balancer
VPN gateway
Table 2: The NFs we developed with TEA. Table sizes are es-
timated by assuming 10 million entries with IPv6 addresses.
Per-flow address mapping
Per-flow connection state
Per-flow connection mapping
Ext.-to-int. tunnel mapping
Table size
(MB)
525
353
525
343
Figure 6: Summary of key components in TEA. The com-
ponents form one logical TEA component (dotted-red box)
used by an NF pipeline.
4.5 Putting It All Together
Figure 6 illustrates the key components of TEA on the switch data
plane and servers, and how an NF uses it for packet processing.
When the NF performs a lookup with a key using the TEA APIs,
TEA first updates the count-min sketch of the key. Then, it checks
whether an entry for the key exists in SRAM Stash or Cache (green-
colored). If it exists, it directly passes the entry to the NF. Otherwise,
it resolves a memory address and server ID using the memory
address resolver. It then generates an RDMA write of the packet
contents to the scratchpad and an RDMA read of the table row
using the memory access requester (orange-colored). This design
guarantees that RDMA write and read requests are always destined
to the same server, and with our flow control mechanism described
in §5.3, both requests are not issued and a packet is dropped when
the destination server is overloaded. Upon receiving an RDMA
request from the switch, RDMA NICs on servers fetch entries from
DRAM and send them back to the switch. Then, the lookup response
handler extracts matched entries and the original packet contents
to pass them to the NF.
Overhead of TEA. When an NF accesses external DRAM for table