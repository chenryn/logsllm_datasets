## ðŸš€ Feature
## Motivation
Currently when using the `nn.MultiHeadAttention` layer, the
`attn_output_weights` consists of an average of the attention weights of each
head, therefore the original weights are inaccessible. That makes analysis
like the one made in this paper very difficult.
## Pitch
When the `nn.MultiHeadAttention` forward is called with `need_weights=True`
(and maybe a second parameter like `nead_attn_heads=True`),
`attn_output_weights` should be a tensor of size `[N,num_heads,L,S]`,with the
weights of each head, instead of the average of size `[N,L,S]` (following the
notation in the docs)
## Alternatives
## Additional context
A small discussion about this subject with a potential solution was made here
If you guys agree, I'll gladly make a PR.