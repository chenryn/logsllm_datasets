set by slicing the AST nodes into fragments, which is used
as a lexicon for generating JS code. We frame the problem
of training a language model to leverage fragments and their
sequences, which makes Montage compatible with prevalent
statistical language models.
5.3 Phase III: Generating JS Tests
Given a set of ASTs from regression tests and the LSTM
model, Phase III ﬁrst mutates a randomly selected seed AST
by leveraging the LSTM model. Then, it resolves reference
errors in the skeleton AST.
Algorithm 1 describes our code generation algorithm. The
MutateAST function takes two conﬁgurable parameters from
users.
fmax
ktop
The maximum number of fragments to append.
This parameter controls the maximum number
of fragments that a newly composed subtree can
have.
The number of candidate fragments. Montage ran-
domly selects the next fragment from suggestions
of the ktop largest probabilities at each iteration.
After several exploratory experiments, we observed that
bloated ASTs are more likely to have syntactical and seman-
tic errors. We also observed that the accuracy of the model
decreases as the size of an AST increases. That is, as the size
of AST increases, Montage has a higher chance of failures in
generating valid JS tests. We thus capped the maximum num-
ber of fragment insertions with fmax and empirically chose its
default value to be 100. For ktop, we elaborate on its role and
effects in detail in §7.3.
5.3.1 Mutating a Seed AST
The MutateAST function takes in a set of ASTs from regres-
sion tests, the trained LSTM model, and the two parameters. It
then begins by randomly selecting a seed AST from the given
set (Line 2). From the seed AST, it removes one randomly
selected subtree (Line 3). Note that the pruned AST becomes
a base for the new JS test. Finally, it composes a new subtree
by leveraging the LSTM model (Lines 4-13) and returns the
newly composed AST.
After selecting a seed AST in Line 2, we randomly prune
one subtree from the AST by invoking the RemoveSubtree
function. The function returns a pruned AST and the initial
context for the LSTM model, which is a fragment sequence
up to the fragment where Montage should start to generate a
new subtree. This step makes a room to compose new code.
In the while loop in Lines 4-13, the MutateAST function
now iteratively appends fragments to the AST at most fmax
times by leveraging the LSTM model. The loop starts by se-
lecting the next fragment via the PickNextFrag function in
Line 6. The PickNextFrag function ﬁrst queries the LSTM
model to retrieve the ktop suggestions. From the suggestions,
USENIX Association
29th USENIX Security Symposium    2619
Algorithm 1: Mutating a seed AST
Input : A set of ASTs from regression tests (T).
The LSTM model trained on fragments (model).
The max number of fragments to append ( fmax).
The number of candidate fragments (ktop).
Output : A newly composed AST.
n0 ← PickRandomSeed(T)
n0, context ← RemoveSubtree(n0)
count ← 0
while count ≤ fmax do
1 function MutateAST(T, model, fmax, ktop)
2
3
4
5
6
7
8
return
next_ f rag ← PickNextFrag(model, ktop, context)
if next_ f rag = ∅ then
/* Get direct child nodes. */
n0 ← AppendFrag(n0, next_ f rag)
if not IsASTBroken(n0) then
break
context.append(next_ f rag)
count ← count + 1
return n0
14
15 function AppendFrag(node, next_ f rag)
16
17
18
19
C ← node.child()
if IsNonTerminal(node) ∧ C = ∅ then
node ← next_ f rag
return
for c ∈ C do
AppendFrag(c, f rag_seq)
9
10
11
12
13
20
21
the function repeats random selections until the chosen frag-
ment indeed has a correct type required for the next fragment.
If all the suggested fragments do not have the required type,
the MutateAST function stops here and abandon the AST.
Otherwise, it continues to append the chosen fragment by
invoking the AppendFrag function.
The AppendFrag function traverses the AST in the pre-
order to ﬁnd where to append the fragment. Note that this
process is exactly the opposite process of an AST fragmenta-
tion in §5.1.2. Because we use a consistent traversal order in
Phase I and III, we can easily ﬁnd whether the current node
is where the next fragment should be appended. Lines 16-19
summarize how the function determines it. The function tests
whether the current node is a non-terminal that does not have
any children. If the condition meets, it appends the fragment
to the current node and returns. If not, it iteratively invokes
itself over the children of the node for the pre-order traversal.
Note that the presence of a non-terminal node with no
children indicates that the fragment assembly of the AST is
still in progress. The IsASTBroken function checks whether
the AST still holds such nodes. If so, it keeps appending the
fragments. Otherwise, the MutateAST function returns the
composed skeleton AST.
We emphasize that our code generation technique based
on code fragments greatly simpliﬁes factors that a language
model should learn in order to generate an AST. TreeFuzz [41]
allows a model to learn ﬁne-grained relationships among
edges, nodes, and predecessors in an AST. Their approach
requires to produce multiple models each of which covers a
speciﬁc property that the model should learn. This, though,
brings the unfortunate side-effects of managing multiple mod-
els and deciding priorities in generating an AST when the
predictions from different models conﬂict with each other.
On the other hand, our approach abstracts such relationships
as fragments, which becomes building blocks for generating
AST. The model only learns the compositional relationships
between such blocks, which makes training and managing a
language model simple.
5.3.2 Resolving Reference Errors
Phase III resolves the reference errors from the generated
AST, which appear when there is a reference to an undeclared
identiﬁer. It is natural for the generated AST to have reference
errors since we assembled fragments that are used in different
contexts across various training ﬁles. The reference error
resolution step is designed to increase the chance of triggering
bugs by making a target JS engine fully exercise the semantics
of a generated testing code. The previous approaches [18,
24, 54] reuse existing AST subtrees and attach them into a
new AST, which naturally causes reference errors. However,
they overlooked this reference error resolution step without
addressing a principled solution.
We propose a systematic way of resolving reference errors,
which often accompany type errors. Speciﬁcally, we take into
account both (1) statically inferred JS types and (2) the scopes
of declared identiﬁers. Montage harnesses these two factors
to generate JS test cases with fewer reference errors in the run
time.
There are three technical challenges that make resolving
reference errors difﬁcult. (1) In JS, variables and functions
can be referenced without their preceding declarations due
to hoisting [37]. Hoisting places the declarations of identi-
ﬁers at the top of the current scope in its execution context;
(2) It is difﬁcult to statically infer the precise type of each
variable without executing the JS code because of no-strict
type checking and dynamically changing types; and (3) Each
variable has its own scope so that referencing a live variable
is essential to resolve reference errors.
To address these challenges, Montage prepares a scope for
each AST node that corresponds to a new block body. Mon-
tage then starts traversing from these nodes and ﬁlls the scope
with declared identiﬁers including hoistable declarations.
Each declared identiﬁer in its scope holds the undefined
type at the beginning.
When Montage encounters an assignment expression in its
traversal, it statically infers the type of its right-hand expres-
sion via its AST node type and assigns the inferred type to its
left-hand variable. Montage covers the following statically in-
ferred types: array, boolean, function, null, number,
object, regex, string, undefined, and unknown. Each
2620    29th USENIX Security Symposium
USENIX Association
scope has an identiﬁer map whose key is a declared identiﬁer
and value is an inferred type of the declared identiﬁer.
To resolve reference errors, Montage identiﬁes an unde-
clared variable while traversing each AST node and then
infers the type of this undeclared variable based on its us-
age. A property or member method reference of such an un-
declared variable hints to Montage to infer the type of the
undeclared variable. For instance, the length property ref-
erence of an undeclared variable assigns the string type to
the undeclared variable. From this inferred type, Montage
replaces the undeclared identiﬁer with a declared identiﬁer
when its corresponding type in the identiﬁer map is same as
the inferred type. If the inferred type of an undeclared variable
is unknown, it ignores the type and randomly picks one from
the list of declared identiﬁers. For all predeﬁned and built-in
identiﬁers, Montage treats them as declared identiﬁers.
6 Implementation
We implemented Montage with 3K+ LoC in Python and JS.
We used Esprima 4.0 [21] and Escodegen 1.9.1 [51] for pars-
ing and printing JS code, respectively. As both libraries work
in the Node.js environment, we implemented an inter-process
pipe channel between our fuzzer in Python and the libraries.
We implemented the LSTM models with PyTorch
1.0.0 [52], using the L2 regularization technique with a pa-
rameter of 0.0001. The stochastic gradient descent with a
momentum factor of 0.9 served as an optimizer.
We leveraged the Python subprocess module to execute JS
engines and obtain their termination signals. We only con-
sidered JS test cases that crash with SIGILL and SIGSEGV
meaningful because crashes with other termination signals
are usually intended ones by developers.
To support open science and further research, we publish
Montage at https://github.com/WSP-LAB/Montage.
7 Evaluation
We evaluated Montage in several experimental settings. The
goal is to measure the efﬁcacy of Montage in ﬁnding JS engine
bugs, as well as to demonstrate the necessity of an NNLM
in ﬁnding bugs. We ﬁrst describe the dataset that we used
and the experimental environment. Then, we demonstrate (1)
how good a trained Montage NNLM is in predicting correct
fragments (§7.2), (2) how we set a ktop parameter for efﬁcient
fuzzing (§7.3), (3) how many different bugs Montage discov-
ers, which other existing fuzzers are unable to ﬁnd (§7.4),
and (4) how much the model contributes to Montage ﬁnding
bugs and generating valid JS tests (§7.5). We conclude the
evaluation with ﬁeld tests on the latest JS engines (§7.6). We
also discuss case studies of discovered bugs (§7.7).
7.1 Experimental Setup
We conducted experiments on two machines running 64-bit
Ubuntu 18.04 LTS with two Intel E5-2699 v4 (2.2 GHz) CPUs
(88 cores), eight GTX Titan XP DDR5X GPUs, and 512 GB
of main memory.
Target JS engine. The ChakraCore GitHub repository has
managed the patches for all the reported CVEs by the commit
messages since 2016. That is, we can identify the patched
version of ChakraCore for each known CVE and have ground
truth that tells whether found crashes correspond to one of
the known CVEs [9]. Therefore, we chose an old version
of ChakraCore as our target JS engine. We speciﬁcally per-
formed experiments on ChakraCore 1.4.1, which was the ﬁrst
stable version after January 31, 2017.
Data. Our dataset is based on the regression test sets of
Test262 [13] and the four major JS engine repositories at
the version of January 31, 2017: ChakraCore, JavaScriptCore,
SpiderMonkey, and V8. We excluded test ﬁles that Chakra-
Core failed to execute because of their engine-speciﬁc syntax
and built-in objects. We did not take into account ﬁles larger
than 30 KB because large ﬁles considerably increase the
number of unique fragments with low frequency. In total, we
collected 1.7M LoC of 33,486 unique JS ﬁles.
Temporal relationships. Montage only used the regression
test ﬁles committed before January 31, 2017, and performed
fuzz testing campaigns on the ﬁrst stable version after January
31, 2017. Thus, the bugs that regression tests in the training
dataset check and the bugs that Montage is able to ﬁnd are
disjoint. We further conﬁrmed that all CVEs that Montage
found were patched after January 31, 2017.
Fragments. From the dataset, we ﬁrst fragmented ASTs to
collect 134,160 unique fragments in total. On average, each
training instance consisted of 118 fragments. After replacing
less frequent fragments with OoVs, they were reduced to
14,518 vocabularies. Note that most replaced fragments were
string literals, e.g., bug summaries, or log messages.
Bug ground truth. Once Montage found a JS test triggering
a bug, we ran the test against every patched version of Chakra-
Core to conﬁrm whether the found bug matches one of the
reported CVEs. This methodology is well-aligned with that of
Klees et al. [31], which suggests counting distinct bugs using
ground truth. When there is no match, the uniqueness of a
crash was determined by its instruction pointer address with-
out address space layout randomization (ASLR). We chose
this conservative setting to avoid overcounting the number of
found bugs [36].
7.2 Evaluation of the LSTM Model
To train and evaluate the LSTM model of Montage, we per-
formed a 10-fold cross-validation on our dataset. We ﬁrst
randomly selected JS ﬁles for the test set, which accounted
for 10% of the entire dataset. We then randomly split the
USENIX Association
29th USENIX Security Symposium    2621
(a) Perplexity of the model.
(b) Type error proportion.
Figure 6: Perplexity and type error proportion of the LSTM
model measured against the training and validation sets over
epochs. They are averaged across the 10 cross-validation sets.
remaining ﬁles into 10 groups. We repeated holding out one
group for the validation set and taking the rest of them for the
training set for 10 times.
Figure 6 illustrates the perplexity and type error of the
LSTM model measured on the training and validation sets.
Recall that the loss function of the model is a sum of the log
perplexity and type error (§5.2).
Perplexity. Perplexity measures how well a trained model
predicts the next word that follows given words without per-
plexing. It is a common metric for evaluating natural language
models [29, 34]. A model with a lower perplexity performs
better in predicting the next probable fragment. Note from
Figure 6a that the perplexities for both the training and val-
idation sets decrease without a major difference as training
goes on.
Type error. Type error presents how well our model predicts
the correct type of a next fragment (recall §5.2). A model with
a low type error is capable of predicting the fragments with
the correct type in its top predictions. Note from Figure 6b
that the type errors for both the training and validation sets
continuously decrease and become almost equal as the epoch
increases.
The small differences of each perplexity and type error
between the training set and validation set demonstrate that
our LSTM model is capable of learning the compositional
relations among fragments without overﬁtting or underﬁtting.
We further observed that epoch 70 is the optimal point
at which both valid perplexity and valid type errors start to
plateau. We also noticed that the test perplexity and test type
errors at epoch 70 are 28.07 and 0.14, respectively. Note
from Figure 6 that these values are close to those from the
validation set. It demonstrates that the model can accurately