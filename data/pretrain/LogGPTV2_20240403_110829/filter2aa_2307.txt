脚本⼩⼦觉悟之获取⽹⻚正确编码
0x00 认识text与content
先来⼀段熟悉的代码:
那么 text 与 content 的区别，我们可以跟进看看
content 代表返回 raw 类型的byte字节流数据，这个时候的数据没有编码概念。
text 通过阅读注释和代码，可知返回的是编码后数据，其中获取编码的流程是，先通过 response 的header头来
判断，如果没有，则Fallback到 chardet 去猜测，然后使⽤获取到的最终编码类型对byte数据进⾏编码。
resp = requests.get(url=url, verify=False)
print(resp.text)
print(resp.content)
0x01 出现问题
乍⼀看，是不是以后我们使⽤ text 就能获取到较为准确的编码后的内容了？ 爽歪歪
但是当你尝试执⾏下⾯这段代码的时候，乱码却来的那么令⼈惊喜"。
import requests
import chardet
def get(url):
    try:
        resp = requests.get(url=url, verify=False)
        print(resp.encoding)
        print(resp.headers)
        print(resp.text)
    except Exception as e:
        print(e)
get("https://baidu.com/")
⾸先 requests 从header获取到编码是 ISO-8859-1 单字节，根据乱码来看，嗯，出错了。
在 adapters.py 的274⾏ get_encoding_from_headers
可以看到，获取到的编码逻辑， requests 作者基于⼀些考量做了⼀些"优化"。
0x02 解决问题
编码的选择权交给⻚⾯提供⽅是⼀种"合理"的约定。但是很多⽹站在控制编码⽅⾯是通过在html⻚⾯通过  之类的语句来指定⻚⾯的编码，导致爬⾍使⽤ requests.text 就会得到乱码的结果。针对
这种情况，与其被动，不如主动，直接利⽤ chardet 进⾏"经验"考量判断。
    if 'text' in content_type:
        return 'ISO-8859-1'
import requests
import chardet
0x03 总结
  炒冷饭的⼀些常识性的东⻄，对于优化你的⽇常扫描脚本或许能起到⼀些作⽤吧，⾄少在速度、准确度上⾯以
及脚本轻量化⽅⾯这都是⼀种不错的选择。
import urllib3
urllib3.disable_warnings()
def get(url):
    try:
        resp = requests.get(url=url, verify=False)
        encoding = chardet.detect(resp.content).get("encoding", "utf-8")
        return str(resp.content, encoding=encoding)
    except requests.exceptions as e:
        print(e)
print(get("https://baidu.com/"))