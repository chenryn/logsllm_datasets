application state snapshot. Contrary to a wallet node, the miner
must obtain the whole application state so that it can validate new
pending transactions as they arrive. As such, the miner downloads
the headers for the whole chain, and the full blocks only for blocks
near the end of the chain.
To be more precise, after the longest chain has been determined
by comparing block header chain lengths, the 𝑘th block from the
end is inspected, its application state snapshot is extracted, and the
deltas in next 𝑘 blocks are applied. This is necessary because an ad-
versary can place incorrect snapshots in the most recent 𝑘 blocks
of a blockchain (folklore wisdom suggests 𝑘 = 6 for Bitcoin). While
that blockchain will look valid and long to someone verifying only
headers, it will have snapshots corresponding to an incorrect ap-
plication of deltas. However, the adversary cannot modify blocks
prior to that, due to the Common Prefix [21] property of block-
chains.
Note here that the miner does not need to verify the veracity
of all historical transactions: If we assume that the majority of the
computational power was honest for the duration of history, this
ensures that, at all times during the execution, the longest chain
represented the correct history of the world (with the exception of
up to 𝑘 blocks towards the end). Under the honest majority assump-
tion, this scheme is as secure as full mining (but see the Discussion
section at the end for a more nuanced take on this argument under
temporary dishonest majority). This is contrary to schemes such
as SPV mining in which no snapshots are available.
Consensus data. Application data can grow or shrink. UTXOs can
be created or deleted, accounts and smart contracts can be created,
updated and destroyed. State variables within smart contracts can
also be constructed or destructed. How the application data grows
is application-dependent. Typically, the application data will in-
crease as the execution continues. There are several attempts to
optimize the size of these data [3, 4, 7, 14, 30, 41, 42]. In this paper,
we do not focus on these.
Instead, we focus on the size of the consensus data, that of block
headers 𝐻(𝑐𝑡𝑟 ∥ 𝑥 ∥ 𝑠). Contrary to the application data, these data
increase at a constant linear rate, as block headers are added to
the chain. No matter if channels or rollups are used, block headers
must keep getting added to the chain. Fortunately, the headers are
small. Nevertheless, no matter how much pruning is done on the
application layer, the consensus data will keep growing. A system
designed to survive for the centuries to come must provision for
the scalability of this ever-growing part. Even the solutions above
that only download block headers do not tackle that problem. The
aim of this paper is to explore whether this part can be pruned. As
we will see, it is possible to reduce the consensus data and neither
store nor communicate all block headers.
A visualization of the comparison between application and con-
sensus data is shown in Figure 1. The consensus data (horizontal)
grows at an expected constant rate in time. The application data
(vertical) may grow (or shrink) depending on the application, and
optimizations or pruning methods can be applied on top of them.
Figure 1: A comparison of consensus data (growing horizon-
tally with time) and application data (growing or shrinking
vertically depending on the application).
Comparison against previous work. A comparison of our pa-
per against previous works is illustrated in Table 1. In all of these
protocols, we have a node (the prover) that maintains all the nec-
essary state to help a newly booting node (the verifier or client)
time tblock headersconsensus dataapplication data snapshotsGtxs δSession 12D: Decentralized Cryptographic ProtocolsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3489synchronize with the rest of the network. We compare the storage
requirements for the prover, as well as the communication com-
plexity during bootstrapping. We are also interested in whether,
after synchronizing with the rest of the network, the verifier can
function as a fully-fledged miner on its own.
In this table, 𝑛 denotes the number of blocks in the chain, 𝛿 is
the size of the transactions in a single block (which may vary with
time), 𝑎 is the size of the snapshot or application state (which may
also very with time), 𝑐 is the size of a block header, and 𝑘 is the com-
mon prefix parameter, the number of blocks required for stability
(c.f., [26]). BTC Full indicates the full bitcoin miner that synchro-
nizes by downloading all block headers and transactions 𝑛(𝑐 + 𝛿).
BTC SPV is a wallet-only client that downloads only block head-
ers and a single transaction, but requires the prover (the node that
serves it this data) to store the full history, as there are no snap-
shots available. Ethereum is a blockchain which uses block headers
to synchronize, but makes use of snapshots. Here, the prover can
prune block contents, but not block headers (the 𝑛𝑐 term remains).
For the last 𝑘 blocks, the transaction data of total size 𝑘𝛿 are also
needed to verify the veracity of the tip of the chain; for the 𝑘th
block from the end, only a snapshot of size 𝑎 is needed. The client
can start mining on top of these snapshots (after the 𝑘𝛿 transaction
data have been applied to the snapshot of size 𝑎). Note that 𝑎 ≤ 𝑛𝛿
and 𝑘 ≤ 𝑛, and so (asymptotically) 𝑛(𝑐+𝛿) ≥ 𝑛𝑐+𝑘𝛿+𝑎. Superblock
and FlyClient NIPoPoWs allow a full node to function as a prover,
only sending consensus data polylogarithmic in 𝑛, provided snap-
shots are available, but the receiving verifier cannot function as a
miner or a prover for others. In this work, we present a protocol
in which the verifier and prover are identical. The prover is only
required to store polylogarithmic consensus data, and communi-
cation complexity is also polylogarithmic. This is indicated by the
term 𝑝𝑜𝑙𝑦 log(𝑛)𝑐. The term 𝑘𝑎, the application data, remains un-
affected and its pruning is orthogonal to this work.
3 STATE COMPRESSION
How can a newly booting miner synchronize with the rest of the
network if block headers have been pruned? It seems impossible
to do so securely. At first glance, the newly booting miner will be
lost in a sea of application snapshots and blockchain tips, without
any ability to discern the application snapshot corresponding to
the longest chain.
We approach this problem by compressing the consensus data.
Among all the block headers that would be maintained by a tradi-
tional blockchain protocol, we only keep a small sample of block
headers that are of interest. Most of the block headers will be pruned.
The small sample of block headers that remains will be polyloga-
rithmic in size and used as evidence that work took place through-
out history. These sample block headers will be stored by our min-
ers, and will also be sent to new bootstrapping miners when they
boot. No other block headers will be stored or communicated be-
yond these carefully chosen samples. The samples will be chosen
to be the same for all miners. As such, some block headers will sur-
vive throughout the network, while others will be gone for ever.
Once we describe which block headers to keep and which ones to
throw away, the construction of our prover will be complete. The
rest of the work will be to construct a verifier that can distinguish
between honest and adversarial application state claims by exam-
ining these samples and, of course, proving that this operation is
secure.
Sampling blocks. Let us begin by discussing which samples among
all block headers will be maintained by first presenting our com-
pression algorithm: The code that can take in a full chain and per-
form the sampling. These block header samples will be the only
ones that survive in our final protocol design. The compression
algorithm takes in a full chain and produces the desired samples,
but will not form part of our final protocol. In the final protocol,
no full chain is to be found. However, the compression algorithm
will prove educational in understanding the final protocol (and can
also be used, once, to transition a full miner into a light miner). We
will also reuse our compression algorithm in the final light miner
construction, despite no full chains ever appearing.
We sample block headers based on their achieved proof-of-work.
Recall that a block must satisfy the proof-of-work equation 𝐻(𝑐𝑡𝑟
∥ 𝑥 ∥ 𝑠) ≤ 𝑇 for some constant1 mining target 𝑇 . Some blocks sat-
isfy this equation much better than others and in particular may
achieve 𝐻(𝑐𝑡𝑟 ∥ 𝑥 ∥ 𝑠) ≤ 𝑇
2𝜇 for some 𝜇 ∈ N. Following previous lit-
erature [27, 29, 31], we call these 𝜇-superblocks and 𝜇 the level of a
block. We model our hash function 𝐻 as a random oracle with 𝜅 bits
of output, and hence the distribution of 𝜇 superblocks, although
stochastic, will be quite controlled. In particular, every block is a 0-
superblock, about half the blocks in the chain will be 1-superblocks,
about a quarter will be 2-superblocks, and in general, the probabil-
ity that a valid block is a 𝜇-superblock will be 1
2𝜇 . By definition,
every block of level 𝜇 > 0 is also a block of level 𝜇 − 1, and all
the levels below down to 0. The genesis block is, by convention,
of infinite level. As the number of blocks per level drops exponen-
tially as the level increases, the number of different levels will be
approximately log |𝐶|, where |𝐶| denotes the size of the underlying
blockchain [29].
The intuition for our construction is as follows. Superblocks of
increasing level become rarer and rarer. As such, superblocks can
be used to illustrate that some work has occurred in a blockchain
without actually delivering every block header. Consider a client
that sees 13 superblocks of level 10. That client can readily deduce
that approximately 210 blocks must have appeared around each of
these 13 superblocks. Otherwise, how was it possible to mine these
blocks at this very high difficulty? The client can be sure that a
total of about 13 · 210 blocks must have been mined, even though
the client cannot observe these blocks directly (and, to be fair, if
these blocks were mined by an adversary, they may have never
been broadcast to the network at all).
Interlinking the chain. While the client may see a series of blocks
at a certain level, it cannot be sure that these were mined in the or-
der they are presented. An adversary presenting 13 superblocks of
level 10 may reorder them arbitrarily. For this reason, the same way
we maintain previd pointers between blocks of level 0 in legacy
blockchains, we need to maintain pointers between blocks at ev-
ery level, and these pointers need to be placed within the proof-
of-work so as to become immutable (as such, our protocol is not
1Throughout this paper we will assume the target 𝑇 is constant; some notes around
a variable target 𝑇 appear in the Discussion section towards the end.
Session 12D: Decentralized Cryptographic ProtocolsCCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3490Prover Storage
Communication
Can verifier mine?
BTC Full BTC SPV Ethereum Superblock NIPoPoWs
𝑛𝑐 + 𝑘𝛿 + 𝑎
𝑛(𝑐 + 𝛿)
𝑝𝑜𝑙𝑦 log(𝑛)𝑐 + 𝑘𝛿 + 𝑎
𝑛(𝑐 + 𝛿)
yes
no
𝑛𝑐 + 𝑘𝛿 + 𝑎
𝑛𝑐 + 𝑘𝛿 + 𝑎
yes
𝑛𝑐 + log 𝛿
𝑛𝑐 + log 𝛿
no
FlyClient
𝑛𝑐 + 𝑘𝛿 + 𝑎
𝑝𝑜𝑙𝑦 log(𝑛)𝑐 + 𝑘𝛿 + 𝑎
no
This work
𝑝𝑜𝑙𝑦 log(𝑛)𝑐 + 𝑘𝛿 + 𝑎
𝑝𝑜𝑙𝑦 log(𝑛)𝑐 + 𝑘𝛿 + 𝑎
yes
Table 1: A comparison of our results and previous work, in asymptotic storage and communication complexity. 𝑛: the number
of blocks in the chain; 𝛿: size of transactions in a block; 𝑐: block header size; 𝑎: size of snapshot; 𝑘: common prefix parameter
backwards-compatible with existing blockchains, but see the Dis-
cussion section for a note on this). We wish to connect the blocks at
each level with a previous block pointer pointing to the most recent
block of the same level. For example, a superblock of level 5 must be
connected to its most recently preceding level 5 superblock. These
pointers must be included in the data of the block so that proof-of-
work commits to them. As the level of a block cannot be prediced
before its proof-of-work is calculated, we extend the previous block
id structure of classical blockchains to be a set, the interlink set.
The interlink set points to the most recent preceding block of ev-
ery level 𝜇 (ignoring duplicates [25]). A pointer to G is included
in every block. The number of pointers that need to be included
per block is in expectation O(log(|C|)) [27]. So, for example, if su-
perblocks of 5 different levels have appeared throughout history
so far, then a newly mined block will contain 5 pointers, one for
each level, to the most recently preceding block of that level.
An example is illustrated in Figure 2. Here, the right-most block
is a block of level 3. It contains one pointer to the most recent 0-
level and 1-level block (which happens to be the same, since 1-level
blocks are also 0-level), a pointer to the most recent 2-level block, to
the most recent 3-level block, and to the most recent 4-level block;
that is, 4 pointers in total, if we properly deduplicate.
Algorithm 1 The updateInterlinkSet algorithm which updates the
interlink set
1: function updateInterlinkSet(𝐵′)
interlinkSet ← {𝐻(𝐵′)}
2:
for 𝐻(𝐵) ∈ 𝐵′.interlink do
3:
4:
5:
6:
7:
8:
9: end function
interlinkSet ← interlinkSet ∪ {𝐻(𝐵)}
if level(𝐵) > level(𝐵′) then
end for
return interlinkSet
end if
The algorithm for this construction is shown in Algorithm 1 [27].
The interlink set of the Genesis block is, by definition, empty. The
algorithm describes how the interlink can be updated once a block
is found. The new interlink is then included in the next candidate
block by the miner. The updateInterlinkSet algorithm accepts a
block 𝐵′, which already has an interlink data structure defined on
it. The function evaluates the interlink data structure which needs
to be included as part of the next block that extends 𝐵′. The method
copies parts of the existing interlink from 𝐵′
.interlink and adds
the reference 𝐻(𝐵′). Every existing pointer 𝐻(𝐵) in 𝐵′
.interlink is
copied only if needed. A previous pointer is skipped and overwrit-
ten by 𝐻(𝐵′) if the level of 𝐵′ overshadows the level of 𝐵; that is,
if level(𝐵′) ≥ level(𝐵). Otherwise, it is preserved in the new in-
terlink set. This construction ensures that every block contains a
direct pointer to its most recent 𝜇-superblock ancestor, for every
𝜇 ∈ N.