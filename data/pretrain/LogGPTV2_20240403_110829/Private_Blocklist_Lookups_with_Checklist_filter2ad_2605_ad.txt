up in the same bucket, the server keeps only the latest pair
and discards any earlier pairs.
To delete an existing key 𝐾, the servers add a pair (𝐾, 𝑉⊥)
to the topmost bucket, where 𝑉⊥ ∈ {0, 1}ℓ is some special
value. (If the set of possible values for a key is {0, 1}ℓ in
its entirety, we can extend the bit-length of the value space
by a single bit.) When, as a result of ﬂushing, (𝐾, 𝑉⊥) ends
up in the same bucket as other pairs with the same key 𝐾,
the servers only keep the latest pair and discard any earlier
pairs. At the bottom-most bucket, the servers can discard all
remaining (𝐾, 𝑉⊥) pairs.
Analysis. The client needs a new hint for bucket 𝑏 only each
time all of the buckets {1, . . . , 𝑏 − 1} overﬂow. When this
𝑖=1 2𝑖 = 2𝑏 elements into
bucket 𝑏. Intuitively, if the server generates a new hint after
each update, then after 𝑢 updates, the server has generated
𝑢/2𝑏 hints for bucket 𝑏, each of which takes time roughly
𝜆ℓ2𝑏 to generate, on security parameter 𝜆. (This is because
our oﬄine/online scheme has hint-generation time 𝜆ℓ𝑛, on
security parameter 𝜆 and a database of 𝑛 ℓ-bit records.)
happens, the servers ﬂush 1 +𝑏−1
√
The total hint generation time with this waterfall scheme
after 𝑢 updates, on security parameter 𝜆, with 𝐵 = log 𝑛
buckets, is then at most 𝜆𝑢ℓ𝐵 = 𝜆𝑢ℓ log 𝑛. In contrast, if we
generate a hint for the entire database on each change using
the simple scheme, the total hint generation time is 𝜆𝑢ℓ𝑛 =
𝜆𝑢ℓ2𝐵 (since 𝑛 = 2𝐵). That is, the waterfall scheme gives an
exponential improvement in server-side hint-generation time
over the simple scheme.
2𝑏) =
𝑂(ℓ
𝑛). So, we achieve an exponential improvement in hint-
generation cost at a modest (less than fourfold) increase in
online query time.
The query time of this scheme is 𝐵
𝑂(ℓ ·
One subtlety is that in our base oﬄine/online PIR scheme,
the length of a hint for a bucket of size 2𝑏 is roughly 𝜆ℓ
2𝑏. For
buckets smaller than 𝜆2, using oﬄine-online PIR would require
more communication than just downloading the contents of
the entire bucket. We thus use a traditional “online-only" PIR
scheme for those small buckets.
𝑏=1
√
√
6 Use case: Safe Browsing
Every major web browser today, including Chrome, Firefox,
and Safari, uses Google’s “Safe Browsing” service to warn
users before they visit potentially “unsafe” URLs. In this
context, unsafe URLs include those that Google suspects are
hosting malware, phishing pages, or other social-engineering
content. If the user of a Safe-Browsing-enabled browser tries
to visit an unsafe URL, the browser displays a warning page
and may even prevent the user from viewing the page.
6.1 How Safe Browsing works today
At the most basic level, the Safe Browsing service maintains
a blocklist of unsafe URL preﬁxes. The browser checks each
URL it visits against this blocklist before rendering the page to
the client. Since the blocklist contains URL preﬁxes, Google
can add an entire portion of a site to the blocklist by adding
just the appropriate preﬁx. (In reality, there are multiple Safe
Browsing blocklists, separated by the type of threat, but that
detail is not important for our discussion.)
Two factors complicate the implementation:
• The blocklist is too large for clients to download and
store. The Safe Browsing blocklist contains roughly three
million URL preﬁxes. Even sending a 256-bit hash of
each blocklisted URL preﬁx would increase a browser’s
download size and storage footprint by more than 90MB.
This would more than double the download size of Firefox
on Android [66].
• The browser cannot make a network request for every
blocklist lookup. For every webpage load, the browser
must check every page resource (image, JavaScript ﬁle,
etc.) against the Safe Browsing blocklist. If the browser
made a call to the Safe Browsing API over the network for
every blocklist lookup, the latency of page loads, as well
as the load on Google’s servers, would be tremendous.
The current Safe Browsing system (API v4) [43] addresses
both of these problems using a two-step blocklisting strategy.
Step 1: Check URLs against an approximate local blocklist.
Google ships to each Safe Browsing client a data structure
that represents an approximate and compressed version of
the Safe Browsing blocklist, similar to a Bloom ﬁlter [11,18].
Before the browser renders a web resource, it checks the
corresponding URL against its local compressed blocklist.
This local blocklist data structure has no false negatives (it
will always correctly identify unsafe URLs) but it has false
positives (sometimes it will ﬂag safe URLs as unsafe). In
other words, when given a URL, the local blocklist either
replies “deﬁnitely safe” or “possibly unsafe.” Thus, whenever
the local blocklist identiﬁes a URL as safe, the browser can
immediately render the web resource without further checks.
In practice, this local data structure is a list of 32-bit
hashes of each blocklisted URL preﬁx. Delta-encoding
the set [42] further reduces its size to less than 5MB—
roughly 18× smaller than the list of all 256-bit hashes
of the blocklisted URL preﬁxes. The browser checks a
URL (e.g., http://a.b.c/1/2.html?param=1) by splitting it
into substrings (a.b.c/1/2.html?param=1, a.b.c/1/2.html,
a.b.c./1, a.b.c/, b.c/, etc.), hashing each of them, and
USENIX Association
30th USENIX Security Symposium    883
checking each hash against the local blocklist.
Step 2: Eliminate false positives using an API call. Whenever
the browser encounters a possibly unsafe URL, as determined
by its local blocklist, the browser makes a call to the Safe
Browsing API over the network to determine whether the
possibly unsafe URL is truly unsafe or whether it was a false
positive in the browser’s local blocklist.
To execute this check, the browser identiﬁes the 32-bit
hash in its local blocklist that matches the hash of the URL.
The browser then queries the Safe Browsing API for the full
256-bit hash corresponding to this 32-bit hash.
Finally, the browser hashes the URL in question down to
256 bits and checks whether this full hash matches the one
that the Safe Browsing API returned. If the hashes match, then
the browser ﬂags the URL as unsafe. Otherwise, the browser
renders the URL as safe.
This two-step blocklisting strategy is useful for two reasons.
First, it requires much less client storage and bandwidth,
compared to downloading and storing the full blocklist locally.
Second, it adds no network traﬃc in the common case. The
client only queries the Safe Browsing API when there is
a false positive, which happens with probability roughly
𝑛/232 ≈ 2−11. So, only one in every 2,000 or so blocklist
lookups requires making an API call.
However, as we discuss next, the current Safe Browsing
architecture leaks information about the user’s browsing
history to the Safe Browsing servers.
6.2 Safe Browsing privacy failure
Prior work [9,38,45,70] has observed that the Safe Browsing
protocol leaks information about the user’s browsing history
to the servers that run the Safe Browsing API—that is, to
Google. In particular, whenever the user visits a URL that
is on the Safe Browsing blocklist, the user’s browser sends
a 32-bit hash of this URL to Google’s Safe Browsing API
endpoint. Since Google knows which unsafe URLs correspond
to which 32-bit hashes, Google then can conclude with good
probability which potentially unsafe URL a user was visiting.
(To provide some masking for the client’s query, Firefox mixes
the client’s true query with queries for four random 32-bit
hashes. Still, the server can easily make an educated guess at
which URL triggered the client’s query.)
There is some chance (a roughly one in 2,000) that a user
queries the Safe Browsing API due to a false positive—when
the 32-bit hash of a safe URL collides with the 32-bit hash
of an unsafe URL. Even in this case, Google can identify a
small list of candidate safe URLs that the user could have
been browsing to cause the false positive.
6.3 Private Safe Browsing with Checklist
We design a private Safe-Browsing service based on Checklist,
which uses our new PIR scheme of Section 4. Our scheme
requires two non-colluding entities (e.g., CloudFlare and
Figure 2: Using Checklist for Safe Browsing.  The browser checks
whether the URL’s partial hash appears in its local blocklist.  If
so, the browser issues a Safe Browsing API query for the full hash
corresponding to the matching partial hash.  The Checklist client
proxy issues a PIR query for the full hash to the two Checklist servers.
 The Checklist client proxy returns the full hash of the blocklisted
URL to the browser.  The browser warns the user if the URL hash
matches the hash of the blocklisted URL.
Google) to host copies of the blocklist, but it has the privacy
beneﬁt of not revealing the client’s query to either server.
Our Checklist-based Safe Browsing client works largely
the same as today’s Safe Browsing client does (Figure 2). The
only diﬀerence is that when the client makes an online Safe
Browsing API call (to check whether a hit on the client’s local
compressed blocklist is a false positive), the client uses our
PIR scheme to perform the lookup. In this way, the client
can check URLs against the Safe Browsing blocklist without
revealing any information about its URLs to the server (beyond
the fact that the client is querying the server on some URL).
When the client visits a URL whose 32-bit hash appears
in the client’s local blocklist, the client needs to fetch the
full 256-bit SHA256 hash of the blocked URL from the Safe
Browsing servers. To do this, the client identiﬁes the index
𝑖 ∈ [𝑛] of the entry in its local blocklist that caused the hit.
(Here 𝑛 is the total number of entries in the local blocklist.)
The client then executes the PIR protocol of Section 4 with
the Safe Browsing servers to recover the 𝑖th 256-bit URL hash.
If the full hash from the servers matches the full hash of the
client’s URL, the browser ﬂags the webpage as suspect. If not,
it is a false positive and the browser renders the page.
As the Safe Browsing blocklist changes, the client can fetch
updates to its local blocklist using the method of Section 5.2.
When two or more full hashes in the blocklist have the
same 32-bit preﬁx, the Checklist servers can lengthen the
partial hashes for the colliding entries. This way, a partial
hash on the client’s local list always maps to a single full
hash on the servers’ blocklist. Safe Browsing already supports
variable-length partial hashes.
Partial hashes as PIR-by-keywords. The client’s local list of
partial hashes essentially serves as a replacement for using a
general PIR-by-keywords transformation [23]. The downside
of this replacement is that it uses oﬄine communication that
is linear in the number of records in the database. In Safe
Browsing, the primary purpose of the local list is to reduce
latency, bandwidth, and server computation, by allowing the
browser to respond to most queries locally. Checklist takes
884    30th USENIX Security Symposium
USENIX Association
Firefox browserPartialhashesChecklist client proxyLookup0x24C123Full hash0x24C1A8…40x1040x1300x1F30x1FF0x24C0x2B2...ChecklistPIR queryChecklist client stateServerBBlocklistServerABlocklist5Warn?Oﬄine-Online
)
s
µ
(
e
m
i
t
r
e
v
r
e
S
106
105
104
103
102
101
0
2000
4000
6000
Num Queries
(a) Server CPU time
8000 10000
online
)
s
e
t
y
b
(
n
o
i
t
a
c
i
n
u
m
m
o
C
106
105
104
103
102
101
oﬄine
amortized
DPF
Matrix
)
s
µ
(
e
m
i
t
t
n
e
i
l
C
0
2000
4000
6000
Num Queries
(b) Communication
8000 10000
106
105
104
103
102
101
0
2000
4000
6000
Num Queries
(c) Client CPU time
8000 10000
Figure 3: For a static database of three million 32-byte records, we show the query cost in server time, client time, and communication. The
ﬁgure also shows the oﬄine cost of the new oﬄine-online PIR scheme and its total cost (oﬄine and online), amortized over a varying number of
queries. The oﬄine phase of the new scheme is expensive but its per-query server-side time is lower than in prior PIR schemes.
Implementation and evaluation
advantage of the existence of this local list, additionally using
it to map partial hashes to their positions in the blocklist. In
principle, both for Safe Browsing and for other applications,
Checklist could use other PIR-by-keywords techniques or a
local blocklist of a diﬀerent size, allowing diﬀerent tradeoﬀs
between storage, communication, and latency.
Remaining privacy leakage. Checklist prevents the Safe Brows-
ing server from learning the partial hash of the URL that the
client is visiting. However, the fact that the client makes a
query to the server at all leaks some information to the server:
the server learns that the client visited some URL whose par-
tial hash appears on the blocklist. While this minimal leakage