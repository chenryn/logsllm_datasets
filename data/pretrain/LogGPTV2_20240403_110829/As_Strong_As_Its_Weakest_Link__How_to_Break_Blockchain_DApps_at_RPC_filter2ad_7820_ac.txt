Gas
limit





























50


10

5
1.5
Fig. 3: RPC
service model
transaction becomes unorphaned and, together with transaction
of nonce + 1, will be propagated to the entire P2P network.
2) If no transaction with nonce + 1 is received, the peer will
drop the transaction after a timeout, say Ot. A subtle fact is
that an orphan transaction can be replayed and updated before
it becomes unorphaned. Speciﬁcally, an Ethereum peer, upon
receiving two orphan transactions of the same nonce, will fail
the second one if its gas price is lower than 110% of the
gas price of the ﬁrst transaction. In a RPC service, if one can
send such two orphan transactions and observe the success
of the second transaction, she can infer whether there is a
load balancer forwarding the second transaction to a different
backend peer than the ﬁrst one.
Measurement mechanisms: Based on the above idea, we
design a benchmark program to detect
the presence of a
load balancer inside a blackbox service. The program, namely
detectLB_byOrphan in Figure 4, works as follows: It ﬁrst
sends an orphan transaction with nonce + 2 and gas price to
a target RPC service (nonce is the nonce of latest conﬁrmed
transaction) and observes the returned hash txHash (Line 3).
It then sends the second orphan transaction, with the same
nonce + 2 but paying gas price− 1. If the second transaction
fails (Line 4), it implies the second transaction is forwarded
to the same backend peer with the ﬁrst transaction; no load
balancing is detected. Then, it further sends RPC queries to
eth_getTransactionByHash(txHash) (Line 7). Af-
ter waiting for a time period speciﬁed in the argument stall,
it sends the second transition(txHash) RPC query
(Line 9). If both getTransaction(txHash) RPCs return
successfully, it means no load balancing is detected for RPC
queries. What’s noteworthy in our benchmark design is that we
additionally require sending two RPC queries at different time
points to conﬁrm the absence of load balancing. In our prelim-
inary design without the two getTransaction(txHash)
queries, we found certain RPC services may exercise different
load-balancing policies for different types of RPCs (i.e., priv-
ileged RPCs like sendTransaction() and open queries
like getTransaction(txHash)).
To
result,
benchmark,
double-check
the measurement
we
design a second detection mechanism based on RPC
queries getBlockNumber. The
namely
detectLB_byBlockNo in Figure 4 works as following: It
sends a series of getBlockNumber queries, one every two
seconds, to a target RPC service and observes the sequence
of block numbers returned. If an “anomaly” is detected, it
implies the presence of a load balancer in RPC queries. Here,
the anomaly is deﬁned as a getBlockNumber query sent
earlier in time returns a block number larger than a later
query. This reasoning here is that if all getBlockNumber
queries are forwarded to the same RPC peer,
the block
number returned should monotonically increase with time.
The purpose of using two benchmarks is to complement
each other (either conﬁrm or dispute the results of each other),
as the detectLB_byOrphan can be accurate on the case
of asserting no load balancing and detectLB_byBlockNo
can be accurate on the case of detecting load balancing.
B. Measurement Results: Load Balancers
We conducted a series of experiments in order to answer
questions LB0, LB1, LB2 and LB3.
For LB0, we set up the benchmark programs in such
a way that all RPC requests are sent out with a single
API key and from a single client (of a single IP). We
run both benchmark programs, detectLB_byOrphan and
detectLB_byBlockNo, against the nine RPC services. For
each service, we collect the results (true or false) of the two
benchmarks and crosscheck them before determining whether
load balancing is present. In particular, we set the RPC rate
(interval in benchmark detectLB_byBlockNo) to the
maximal value right below the service rate limit (see the
measurement result in § IV-E).
In experiments,
the two benchmarks mostly agree with
each other. That is, when detectLB_byOrphan detects
load balancing (no load balancing), detectLB_byBlockNo
conﬁrms the same. The only exception is ServiceX9, on
which detectLB_byOrphan detects load balancing but
6
Load balancerDApp clientsRPC servicePrivate RPC peersEthereum P2P networkBrowsersRPC requestsTx/block synchronizationDapp jsWallet library(a) ServiceX6
(b) ServiceX9
Fig. 6: Load balancing w.r.t. request timing: The Y axis is the
success rate of orphan transaction sent in the benchmark; it
implies whether the load balancer forwards the RPC request
to a new peer.
detectLB_byBlockNo does not (i.e., no observed case
of decreased block numbers). detectLB_byBlockNo on
ServiceX9 also return many failed results (i.e., block numbers
being 0) which may be the culprit of the inaccuracy.
For LB1, we send RPC requests with two API keys and
from the same client IP. For LB2,
the RPC queries are
sent with the same API key and from two client IPs. In
the two cases,
the different RPC requests apply to Line
7 and Line 9 in Figure 4 for detectLB_byOrphan. In
detectLB_byBlockNo, each step would send out two dif-
ferent RPC queries at the same time, and the “block-number-
decreased” anomaly is detected on the combined sequences of
the two query results.
Results: The measurement results are presented in Table 5.
We ﬁnd three types of load-balancing behaviors w.r.t. LB0,
LB1 and LB2: Type i) No load balancing of any sort,
represented by ServiceX1, ServiceX3 and ServiceX2, (ii) De-
terministic load balancing that entails two subtypes: Type ii-a)
No load balancing detected when RPC queries are sent with
the same API key and from different IPs; this is represented by
ServiceX4, Type ii-b) No load balancing detected when RPC
queries are sent from the same IP but with different API keys;
this is represented by ServiceX5, and Type iii) Comprehensive
load balancing detected, when RPCs are sent from the same IP
and with the same API key; this is represented by ServiceX6,
ServiceX9, ServiceX7 and ServiceX8.
LB3: We further conduct a measurement study for LB3; we
design a simple benchmark to do so which issues a series of
sendTransaction RPCs, with each two T T seconds apart.
We run the benchmark against all services with load balancing
(i.e., except for Type-i services). Most load balancers do not
exhibit dependency to timing. The exceptions are ServiceX6
and ServiceX9 gateway.
Results: In ServiceX6, with T T = 5 seconds, we consis-
tently observe the following behavior of ServiceX6’s load
balancer: The decision which backend peer to forward a
request to is made based on the timing of the request. The
experimental result is reported in Figure 6a. The result shows a
four-minute period (where our raw result is more than an hour)
when a series of transactions from the same from address are
sent to ServiceX6. Every minute,4 ServiceX6’s load balancer
will forward a transaction to a new peer if the transaction is the
ﬁrst one after the 0th, 10th or 15th second in the minute. All
transactions sent between 15th and 60th seconds of a minute
will end up with the same three backend peers in that minute.
The result of ServiceX9 is illustrated in Figure 6b, which is
measured under 10 RPCs per second. In the ﬁrst 25 seconds
of any minute, there is a successful orphan transaction every
5 seconds, implying a new backend peer is allocated. After
that, orphan transactions keep failing until the 40th second.
The deterministic behavior of real load balancers discov-
ered in our research apparently serves an important purpose
– maintaining consistency across dependent transactions of
DApps: for instance, the order between an approval and
a subsequent transferFrom of an ERC20 token should be
preserved, so the load balancer always forwards these requests
to the same backend node. This property, however, can be
exploited to concentrate DoERS attack payloads on a small
set of nodes to enhance their effectiveness, as elaborated in
§ IV-C2.
C. Attack Strategies Evading Load Balancer
An attacker can leverage the above measurement results
to adjust an DoERS attack to speciﬁc services. Here, we
present sample attack strategies speciﬁc to Type-ii and Type-
iii services (note that Type-i service essentially runs no load
balancer and can be attacked in a straightforward way).
1) Targeted Attacks to Type-ii Services: The deterministic
behavior of a Type-ii load balancer can be exploited to launch
a DoERS attack targeted at speciﬁc DApp victims. We propose
strategies that a DoERS attacker can use to select victims
adaptively to the service types, namely Type-ii(a) and Type-
ii(b) services.
Targeted attack to Type-ii(a) services: Recall that a DApp
web client commonly sends requests to a RPC service, using
API keys. As the API key has to be disclosed on DApp
websites to all visiting browsers, the DoERS attacker can
easily obtain the API key. The attacker then sends DoERS
requests with the API key to the service. Recall that a Type-
ii(a) service forwards requests with the same API key to the
same peer, despite of which IPs they are sent from. Thus, the
DoERS requests will be processed by the same nodes serving
other requests of the same API key. By this means, the attacker
can disable the RPC node and further delay the service to other
clients of the same DApp. Therefore, the DoERS attack can
disable all clients of a victim DApp.
Targeted attack to Type-ii(b) services: Initially, the at-
tacker prepares a “malware” token contract, called M-Token,
which encodes the exhaustXX programs in DoERS-C. For
instance, the balanceOf function in the token internally calls
exhaustCPU(1000000).
The attacker distributes the malware token M-Token to
victim DApp clients. To do so, the attacker can set up a token
faucet similar to gitcoin [18], that gives away free M-Tokens
and, as a honeypot, attracts victim owners.
4Here, it requires the timeline is aligned with the Unix timestamp.
7
0255076101126152177203228Timeline (second)020406080100Orphan tx success rate (%)081726354453Timeline (second)020406080100Orphan tx success rate (%)int lengthLower=0; int lengthUpper=500;//0/500 block gas
while (lengthUpper - lengthLower > 1){
arrayLength = (lengthLower + lengthUpper) / 2;
try{
rpcNode.eth_call(exhaustMem,arrayLength);
1 float rpc_gasLimit(IP rpcNode){
2
3
4
5
6
7
8
9
10
11
12
13
14
} else { //no gas limits
} (Exception e) {
return 0;}
} else {
if(e instanceOf OutofGasException){
lengthUpper = arrayLength;
Later the victim owner may open her wallet DApp as usual.
She will be surprised to ﬁnd her DApp webpage unresponsive,
because the webpage sending RPC requests to a service would
make the service run M-Token’s balanceOf function and
get stuck. Further more, not only M-Token’s balance is not
viewable on the victim owner’s webpage, but also the balances
of other benign tokens are not responding. Because both the
benign RPCs (to run benign tokens’ balanceOf) and the M-
Token’s RPCs are sent from the same browser, thus the same
IP, the Type-ii(b) service forwards them to the same backend
peer. By this means, the M-Token RPCs can denial-of-service
the benign tokens’ RPCs.
2) Exploiting Timing Dependency to Attack Type-iii Ser-
vices: Recall our measurement results in § IV-A that the load
balancers of RPC services exhibit timing dependency: if two
requests are sent close in time, the balancer forwards them to
the same backend peer, for purposes such as preserving the
ordering between the two requests. This predictable behavior
can be exploited to direct DoERS requests to just a few peers,
undermining their services to some DApps without saturating
the entire service backend. This results in a low cost and more
effective attack on multi-node RPC services.
Speciﬁcally, consider ServiceX6 as an example. As revealed
from our measurement study (§ IV-A), ServiceX6’s load
balancer forwards all incoming requests received within a
minute (with time aligned) to at most three distinct backend
peers. So an attack can exploit this timing dependency to send
all its DoERS requests in one minute to land on three speciﬁc
nodes, which can effectively deny their services to DApps.
Particularly, if the attacker knows when a speciﬁc DApp or
its client issues requests (e.g., through eavesdropping on its
communication or aiming at a known auction deadline when
bids would come in), he could produce a few attack requests
within the 1-minute window to block the three backend peers
serving the DApp. This strategy enables a low-cost attack in
which one does not need to overload hundreds of backend
peers (e.g., more than 192 peers behind ServiceX6, as mea-
sured in Appendix C), which is very expensive, to undermine
the service to some DApps and their clients. Note that such a
“ﬂash attack” (e.g., one minute for ServiceX6) can still have
serious consequences, e.g., frontrunning a competing bid in
a decentralized auction. The effectiveness of this attack is
evaluated in § V-A2.
D. Measuring Gas Limits: Methodology
Given a RPC service, the goal is to test the presence of
any Gas limit conﬁgured on the service’s backend peers. Our
test program, named by rpc_gasLimit, is in List A.15.
The goal of the test program is to ﬁnd the maximal argument
(arrayLength in function exhaustMem()) that does not
trigger the out-of-gas exception, a value that implies the Gas
limit. To do so, the program starts with an initial guess on the
target arrayLength value, then grows the guess exponen-
tially until the ﬁrst exception is observed. It then enters the
second phase that binary-search the Gas-limit corresponding
value of arrayLength. After the target value V is obtained,
8
lengthLower = arrayLength;}}
return localNode.estmateGas(exhaustMem,arrayLength);}
Fig. 7: Measure Gas limit of an RPC node
the program then uses a local RPC node (under our control)
to run estimateGas() with function exhaustMem under
V . The returned value is the Gas limit. Note that our design
uses exhaustMem function which consumes Gas faster than