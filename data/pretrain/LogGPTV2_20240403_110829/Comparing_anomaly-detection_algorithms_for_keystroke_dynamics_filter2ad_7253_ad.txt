from the evaluation of the 14 detectors are ranked from best to worst (with standard deviations in
parentheses). The set of top-performing detectors is indicated in bold-face (i.e., those that are not
signiﬁcantly worse than the best-performing detector).
threshold in a detector-independent way and summarizing
performance: equal-error rate and zero-miss false-alarm
rate. To calculate the equal-error rate, the threshold is cho-
sen so that the detector’s miss and false-alarm rates are
equal. This measure was used by Kang et al. [11]. To calcu-
late the zero-miss false-alarm rate, the threshold is chosen
so that the false-alarm rate is minimized under the constraint
that the miss rate be zero. This measure was used in two
earlier studies [4, 21]. The equal-error rate and the zero-
miss false-alarm rate are different performance measures,
but both are error rates (i.e., lower values imply fewer errors
and better performance). For each of the 714 combinations
of detector and subject, we generated an ROC curve, and
calculated these two measures.
7. Results and analysis
Table 2 shows the average equal-error and zero-miss
false-alarm rates for each of the 14 detectors over all sub-
jects. The detectors have been rank-ordered from best per-
formance to worst (with separate rankings for each of the
two types of performance measure).
Since the anomaly detectors were evaluated using the
same data, under the same conditions, using the same proce-
dures, it is possible to attribute differences in performance
to the anomaly detector and not to different experimental
conditions. We will establish which detectors are the top
performers on this data set using a statistical analysis, and
then proceed with a detailed comparison of the detectors.
7.1. Finding the top-performing detectors
When comparing the error rates (equal-error or zero-
miss false-alarm) of the 14 anomaly detectors, we natu-
rally ask which detector performs best. One detector will
have the minimum error rate on the test data, and the other
13 detectors’ error rates will be higher (assuming no ties).
However, a test of statistical signiﬁcance is required to es-
tablish that the difference between the best-performing de-
tector and another detector is real, and not explained by ran-
dom effects.
We conduct 13 hypothesis tests, comparing the perfor-
mance of the best-performing detector to each of the other
13 detectors. Formally, each test’s null hypothesis is that
the best-performing detector is no better than the other de-
tector (i.e., a one-sided test). If we cannot reject the null
hypothesis for a detector, we accept that the detector’s per-
formance is competitive with the best-performing detector,
and consider it to be a top-performing detector.
When using hypothesis testing to compare anomaly-
detector performance, Cho et al. [4] used a paired t-test.
The t-test assumes that the difference between the two error
rates is normally distributed. We plotted the distributions of
the error rates of each detector in our evaluation, and found
that they were far from normal (e.g., some looked multi-
modal). Consequently, we opted to use the non-parametric
Wilcoxon signed-rank test instead [19], which has been
shown to be more robust when testing distributions that are
non-normal. We used a signiﬁcance level of α = 0.05, and
since we conducted 13 hypothesis tests, we applied a Bon-
ferroni correction for multiple testing [14].
7.2. Detector performance comparison
The best equal-error rate was 0.096, obtained by the
Manhattan (scaled) detector described by Ara´ujo et al. [1].
The Nearest Neighbor (Mahalanobis) detector, and the Out-
lier Count (z-score) detector were the other top-performing
detectors using the equal-error performance measure.
The best zero-miss false-alarm rate was 0.468, obtained
by the Nearest Neighbor (Mahalanobis) detector described
by Cho et al. [4]. The classical Mahalanobis detector, the
Mahalanobis (normed) detector, and the SVM (one-class)
detector were the other top-performing detectors using the
zero-miss false-alarm performance measure.
Our initial observation is that none of the performance
measures comes close to what is needed to achieve the
0.001% miss rate and 1% false-alarm rate required by the
European standard for access-control systems [3]. Since our
ﬁrst reason for wanting to evaluate and compare anomaly
detectors was to determine if a detector was sufﬁciently re-
liable to be put into practice, we can conclude that further
progress is needed before we could rely solely on keystroke
dynamics for access control.
Since our second reason for comparing detectors is to
discover promising anomaly-detection strategies that might
drive progress, we look for shared strategies among our
top-performing detectors (according to either performance
measure). These detectors all employ some form of scaling
of the timing features (e.g., by employing the Mahalanobis
rather than Euclidean distance). It has been observed that
different timing features have different variability (e.g., hold
times tend to be faster and more consistent than keydown-
keydown times which are slower and more variable). We
ﬁnd that detectors which account for these differences in
scale demonstrate better performance.
The Nearest Neighbor (Mahalanobis) detector is the
only top-performing detector according to both perfor-
mance measures. Since the equal-error rate and the zero-
miss false-alarm rate measure the performance of the de-
tector at different operating points on an ROC curve (see
Figure 1), this result suggests that the Nearest Neighbor
(Mahalanobis) detector is comparatively robust to different
threshold-selection procedures.
Our ﬁnal observation is that the seven worst-ranked de-
tectors (ranks 8–14) are the same for both performance mea-
sures, and equivalently, the seven detectors with the best
rank are the same. Even though the ordering of the top
seven is not the same for both measures, this observation
suggests a clear division between seven detectors that are
competitive in this evaluation and seven that are not.
8. Discussion and future work
This study is the ﬁrst time a diverse group of 14 of the
anomaly detectors for keystroke dynamics have been eval-
uated on an equal basis. While our results are only for a
single data set and evaluation procedure, they demonstrate
the value of shared data and a consistent evaluation method-
ology. Additionally, our work highlights the need for more
shared data and resources.
8.1. Shared data and methods
The comparison of these particular 14 anomaly detec-
tors is interesting in that it reveals which detection strate-
gies seem to work, and which strategies fare poorly. How-
ever, in terms of usefulness to other keystroke-dynamics
researchers, the comparison itself may be less important
than that the comparison was made possible. We invite re-
searchers to use our data and methods to evaluate other de-
tectors and other implementations of these detectors. The
beneﬁt of using these data and methods is a valid compari-
son with the results in this work.1
To our knowledge, the only other researchers to make
keystroke data publicly available through the Web are Mon-
talv˜ao et al. [15]. From discussion with colleagues, we un-
derstand that other data sets are forthcoming, and we wel-
come this trend since the availability of a diverse set of pub-
lic data will boost progress in this ﬁeld.
8.2. Extensions to the evaluation method
We foresee uses for the data beyond comparing anomaly
detectors using our evaluation procedure. As explained
throughout this report, we had to make various tradeoffs
(e.g., about what features to include in our timing vectors,
and how many password vectors to use in training) that un-
doubtedly had an effect on detector performance. Conse-
quently, the data could also be used to evaluate what effect
these decisions have.
For instance, we chose to use 200 samples to train the
detector, which may seem excessive to some; and we used
unpracticed impostors, which may boost detector perfor-
mance. As explained in Section 6.1, we made these deci-
sions for the sake of a fair evaluation. If researchers are
concerned about high performance with less training data,
a different evaluation procedure could be developed to train
detectors using fewer passwords. We simply ask that exten-
sions be explicitly and carefully described, so that different
evaluation methodologies are not conﬂated and confused.
8.3. Detector variability across data sets
A key issue is that minor differences—in the detectors,
the data, and the evaluation—can obviously cause major
swings in detector performance. Keystroke-dynamics is a
sensitive instrument in a noisy domain. Since assessment
and comparison depends on controlling these small differ-
ences in performance, shared data and common evaluation
procedures are critical.
For example, as shown in Table 1, Cho et al. [4] com-
pared a Nearest Neighbor (Mahalanobis) detector to a Neu-
ral Network (auto-assoc) detector. They reported that
the zero-miss false-alarm rate was 19.5% for the nearest-
neighbor detector and 1.0%, and for the neural net. In our
evaluation, the zero-miss false-alarm rates were 10.0% and
1The keystroke data set has been linked from each of the authors’ indi-
vidual webpages and is available directly from
http://www.cs.cmu.edu/˜keystroke
The data are accompanied by a comprehensive description of the collection
procedure and detector-evaluation methodology we used.
46.8% respectively. The error rates for both detectors are
higher in our evaluation. Further, the neural net outperforms
the nearest-neighbor detector in their evaluation while the
opposite is true in ours. Additional shared data and further
evaluations are needed to identify and tease apart the factors
that boost and hinder each detector’s performance.
9. Summary and conclusion
Previously, it was not possible to compare the perfor-
mance of different anomaly detectors across studies in the
keystroke dynamics literature. Our objective in this work
has been to collect a data set, develop an evaluation pro-
cedure, and measure the performance of many anomaly-
detection algorithms on an equal basis. In the process, we
established which detectors have the lowest error rates on
our data (e.g., the Nearest Neighbor (Mahalanobis) detec-
tor), and we provide a data set and evaluation methodology
that can be used by the community to assess new detectors
and report comparative results.
Acknowledgments
The authors are grateful to Patricia Loring for running
the experiments that provided the data for this paper, and
to Rachel Krishnaswami for her insightful comments and
helpful advice. Thanks also to several anonymous reviewers
for their helpful comments.
This work was supported by National Science Foun-
dation grant numbers CNS-0430474 and CNS-0716677,
and by the Army Research Ofﬁce through grant number
DAAD19-02-1-0389 (Perpetually Available and Secure In-
formation Systems) to Carnegie Mellon University’s Cy-
Lab.
References
[1] L. C. F. Ara´ujo, L. H. R. Sucupira, M. G. Liz´arraga, L. L.
Ling, and J. B. T. Yabu-uti. User authentication through
typing biometrics features.
In Proceedings of the 1st In-
ternational Conference on Biometric Authentication (ICBA),
volume 3071 of Lecture Notes in Computer Science, pages
694–700. Springer-Verlag, Berlin, 2004.
[2] S. Bleha, C. Slivinsky, and B. Hussien.
Computer-
access security systems using keystroke dynamics.
IEEE
Transactions on Pattern Analysis and Machine Intelligence,
12(12):1217–1222, 1990.
[3] CENELEC. European Standard EN 50133-1: Alarm sys-
tems. Access control systems for use in security applica-
tions. Part 1: System requirements, 2002. Standard Num-
ber EN 50133-1:1996/A1:2002, Technical Body CLC/TC
79, European Committee for Electrotechnical Standardiza-
tion (CENELEC).
[4] S. Cho, C. Han, D. H. Han, and H. Kim. Web-based
keystroke dynamics identity veriﬁcation using neural net-
work. Journal of Organizational Computing and Electronic
Commerce, 10(4):295–307, 2000.
[5] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁca-
tion. John Wiley & Sons, Inc., second edition, 2001.
[6] G. Forsen, M. Nelson, and R. Staron, Jr. Personal attributes
authentication techniques. Technical Report RADC-TR-77-
333, Rome Air Development Center, October 1977.
[7] R. S. Gaines, W. Lisowski, S. J. Press, and N. Shapiro.
Authentication by keystroke timing: Some preliminary re-
sults. Technical Report R-2526-NSF, RAND Corporation,
May 1980.
[8] S. Haider, A. Abbas, and A. K. Zaidi. A multi-technique
approach for user identiﬁcation through keystroke dynam-
ics.
IEEE International Conference on Systems, Man and
Cybernetics, pages 1336–1341, 2000.
[9] B. Hwang and S. Cho. Characteristics of auto-associative
MLP as a novelty detector. In Proceedings of the IEEE In-
ternational Joint Conference on Neural Networks, volume 5,
pages 3086–3091, 10–16 July, 1999, Washington, DC, 1999.
Identity authentication based
the ACM,
[10] R. Joyce and G. Gupta.
Communications of
on keystroke latencies.
33(2):168–176, 1990.
[11] P. Kang, S. Hwang, and S. Cho. Continual retraining of
keystroke dynamics based authenticator. In Proceedings of
the 2nd International Conference on Biometrics (ICB’07),
pages 1203–1211. Springer-Verlag Berlin Heidelberg, 2007.
[12] H.-j. Lee and S. Cho. Retraining a keystroke dynamics-
based authenticator with impostor patterns. Computers &
Security, 26(4):300–310, 2007.
[13] Microsoft.
Password checker,
2008.
http:
//www.microsoft.com/protect/yourself/
password/checker.mspx.
[14] R. G. Miller, Jr.
Simultaneous Statistical Inference.
Springer-Verlag, New York, second edition, 1981.
[15] J. Montalv˜ao, C. A. S. Almeida, and E. O. Freire. Equal-
ization of keystroke timing histograms for improved identi-
ﬁcation performance. In 2006 International Telecommuni-
cations Symposium, pages 560–565, September 3–6, 2006,
Fortaleza, Brazil, 2006.
[16] PC Tools.
Security guide for windows—random pass-
word generator, 2008. http://www.pctools.com/
guides/password/.
[17] A. Peacock, X. Ke, and M. Wilkerson. Typing patterns:
IEEE Security and Privacy,
A key to user identiﬁcation.
2(5):40–47, 2004.
[18] R Development Core Team. R: A Language and Environ-
ment for Statistical Computing. R Foundation for Statistical
Computing, Vienna, Austria, 2008.
[19] D. J. Sheskin. Handbook of Parametric and Nonparametric
Statistical Procedures. Chapman & Hall/CRC, fourth edi-
tion, 2007.
[20] J. A. Swets and R. M. Pickett. Evaluation of Diagnostic
Systems: Methods from Signal Detection Theory. Academic
Press, New York, 1982.
[21] E. Yu and S. Cho. GA-SVM wrapper approach for fea-
ture subset selection in keystroke dynamics identity veriﬁ-
cation.
In Proceedings of the International Joint Confer-
ence on Neural Networks (IJCNN), pages 2253–2257. IEEE
Press, 2003.