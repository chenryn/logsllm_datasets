while r < k do
Sort L(r−1), ﬁnding all unordered pairs
((xi, Si), (xj, Sj)) such that xi collides
with xj on the ﬁrst
k+1 bits, and that
Si ∩ Sj = ∅
L(r) ← {(xi⊕xj, Si∪Sj)|((xi, Si), (xj, Sj))
is a found pair }
r ← r + 1
rn
Sort L(k−1), ﬁnding all unordered pairs
((xi, Si), (xj, Sj)) such that xi = xj, and that
Si ∩ Sj = ∅
R ← {(Si ∪ Sj)|((xi, Si), (xj, Sj)) is a found
pair }
Output: list R of sets of distinct indices
Fig. 1: Equihash algorithm for SLGBP
Throughout this paper, we will be presenting our solver and
comparing it to software solvers under both parameter sets.
However, mining software products are more optimized and
statistics have signiﬁcantly better coverage under (200, 9), so
the results are more convincing.
Under (200, 9), an Equihash puzzle takes 2M binary strings
of 200 bits as input. In each of the ﬁrst 8 rounds, an 20-bit
substring is taken from each string to ﬁnd collisions. These
bits are discarded, and the remaining strings of each colliding
pair are XORed to enter the next round. In the ﬁnal round, all
40 bits are taken, and 1.879 colliding pairs (or solutions) can
be expected [2], [6].
Following memory-hardness theories [1], [7]–[9], Equihash
is designed to force intense memory usage, both about capacity
and about bandwidth. Even when analyzed with the method
proposed later in [19], we ﬁnd that
it achieved moderate
bandwidth hardness. These two aspects greatly impact the
design and implementation of ASIC solvers.
However, Equihash is not bullet-proof, with various factors
limiting its ASIC-resistance:
• Being more of a real-life algorithm (ie. containing real-
life subroutines like sorting) rather than cryptographic
routine, it is subject to optimizations like the index pointer
technique we discuss below.
• CPUs and GPUs are general purpose devices, and under
PoW workload, have many components drawing power
but not contributing to puzzle solving. At the very least,
ASIC solvers can remove them for a small advantage.
• As mentioned in section II-A, the mining business is
extremely volatile. Even minor advantages can enable
attacks, which already happened to Bitcoin Gold, an
Equihash adopter [17].
Index pointer:
Index pointer is a technique observed in
software solvers like [22] and analyzed in [2].
In the original algorithm, index sets S are carried within the
join step, and their size expand exponentially with each round.
With this technique applied, index sets are stored incrementally
as a tree, greatly reducing the memory footprint of Equihash.
We notice in section IV-B that this technique can greatly
reduce logic and memory access width as well, increasing the
efﬁciency advantage of adversaries.
To summarize this section, blockchain networks are vul-
nerable to monopoly formation and attacks, as adversaries
can reach several
times efﬁciency in PoW solving using
ASIC solvers against software. It is hard to ﬁx for deployed
blockchain networks, therefore PoW schemas must be care-
fully chosen, and thoroughly inspected for limitations.
III. ADVERSARY STRATEGY
Empirically, computing devices tend to consume less power
when optimized for area, and will likely be more energy-
efﬁcient, though sometimes sacriﬁcing performance. The cryp-
tocurrency mining industry has long been adopting this idea,
and has created solver products with incredible efﬁciency.
Optimizing for area brings other beneﬁts as well, like lowered
per-chip investment and increased manageability, but in this
paper, we focus solely on the most decisive criterion, energy
efﬁciency, as discussed in section II-A.
It is widely agreed that two major methods exist to reduce
chip area: (1) apply advanced production technologies to
shrink all components, and (2) complete the design using less
logic. The ﬁrst approach does not change the game for anyone,
because CPUs and GPUs are manufactured in the same way as
ASICs, and always share production technologies. Adversaries
are therefore forced to take the latter track, to reﬁne their
designs and reduce logic usage.
Recall in section II-C that ASIC solvers (including propos-
als) and deployments with parameters disabling them [12],
[15] both exist for Equihash. An optimal parameter set will
eventually be found, and adversaries can not rely on memory-
saving tweaks forever. Implementing the whole solver within
one chip will sooner or later become impossible, as required
in [19]. In this case, an ideal strategy for adversaries is to go
through the following design methodology:
1) Analyze the solving algorithm (in this case Wagner’s
algorithm for SLGBP) and decide on the data to ofﬂoad
from the core solver chip (or solver core for short).
2) Implement the solver core, optimizing for area.
3) Based on data access characteristics (frequency, word
length, burst, cost of latency, etc), pick an adequate
memory conﬁguration, including type, amount, topology
and device parameters like timing.
4) Pick a clock speed for the core, precisely using up
5) Tweak non-bottleneck components, trading performance
memory bandwidth.
for lower power.
Given a set of PoW puzzles with concrete parameters, this
process should always produce a resonable solver. Security
4
inspection can then be performed by evaluating its products
and projecting adversary advantage.
IV. SOLVER CONSTRUCTION
In this section we apply the above methodology on Equihash
and construct an efﬁcient solver design. As mentioned in
section II-C, existing (single-chip) ASIC solvers cannot output
valid solutions for parameter sets requiring more memory. Our
method uses off-chip memory so the same limitation does not
hold. As Equihash adopters are discussing parameter changes
(if not already done so like [12]), our construction represents
a new type of risk and has to be treated with care.
We perform memory usage analysis and assign an ASIC so-
lution to each subroutine accordingly. We (as adversaries) en-
countered limitations and resolved some by adjusting Wagner’s
algorithm. These interactions are good indicators of ASIC-
resistance and can be observed similarly in other schemas.
A. Memory usage analysis
To construct the top-level design of an adversary solver,
we have to understand how the algorithm accesses data. As
Equihash does not include signiﬁcant lookahead opportunities,
we dig directly into the join step in ﬁgure 1,
inspecting
its subroutines including sorting/hashing, pair generation, and
XOR computation.
Recall
that Wagner’s algorithm runs k rounds in each
attempt to solve the SLGBP, each round taking input data from
the previous, and the last round using different parameters. The
amount of pairs produced is always random, so we perform
our analysis using its mathematical expectations for now.
First of all, the random input L has to be saved. It can not be
k+1 bits for the
instantly consumed as the sorting key is only n
ﬁrst round. We can save a portion of capacity and bandwidth
usage if the key is fed directly to sorting components. A
even better option is to have L ready in memory ahead of
time, arranging the subroutine in a timeslice with less memory
access. We apply this method in section IV-E.
Next, the solver must either sort or hash its input. If done
with sorting, optimally only one pass of sequential read is
needed. The hashing method need to write the hash table
somewhere for use later, introducing two extra memory access
passes, write and read. Software solvers use hashing [22]
because optimal sorting is not available on CPUs and GPUs.
As we are constructing an ASIC design, this is not a problem
for us.
Table I lists ideal memory capacity and bandwidth usage
for the sorting step. In section IV-B we’ll discuss how it is
not achievable on popular parameter sets and how we work
around it. The tweaks weakens its memory advantage, but we
still use it for demonstration thanks to its better ﬂexibility.
When fed with sorted input, the pair generation step itself
does not access memory at all. However, due to the index
pointer technique, produced index pairs in normal rounds
have to be written back to memory, causing memory usage
as in table I. The technique has both been proved [2] and
ﬁeld-tested [22] to lower overall memory requirements, so
the additional usage here is totally acceptable. Note that the
utilized storage builds up incrementally with algorithm rounds,
the exact opposite of L, which can have a column discarded
in every round.
The XOR calculation step can always get indices from its
previous step, but the operands are still in memory and thus
need to be fetched. Its results can be partially consumed by
the next sort/hash step, with the remaining written back to
memory for further XOR rounds. As the input is random, no
satisfying cache policy exist for this step, and the subroutine
always use memory as indicated in table I.
The ﬁnal step is to build the index set, both to eliminate
groups with duplicate indices and to fulﬁll the algorithm bound
requirements of Equihash. Practically this subroutine is to
traverse index pointer trees, using the ﬁnal round’s output as
root. We decide to defer this till the end of the algorithm, so
index pointer pairs generated in non-ﬁnal round don’t create
complex interactions, and can propagate through the pipeline
at a line speed of one item or pair per cycle.
Under popular parameters like (200, 9), checking for index
set intersection after every round does not signiﬁcantly reduce
list length [2] and is safe to skip. These results can ease the
ﬁnal check, but there is very little to gain because the Equihash
schema doesn’t produce a lot of ﬁnal results [4]. This step
include very little memory access, and is omitted in the table.
Under some other parameters like (192, 11), the expected
number of solutions can drop to 10−7 scale. Frequent checking
would cause lists to decay quickly, and would often deplete
them in early rounds. In this case, it may be beneﬁcial to
always perform checks, freeing up the pipeline to process the
next puzzle. We ignore these cases here because they exhibit
totally different characteristics not discussed in its original
paper, thus will unlikely deploy practically without further
research.
B. Sorting
An optimal sorting method is actually possible with ded-
icated hardware, but
its linear logic complexity makes it
impractical for popular parameter sets. In this section we
introduce our sorting peripheral and add merge step, trading
memory for logic.
1) Linear sort: The linear sorting technique we use here
is similar to the one used by Grozea [11] when CPU, GPU,
and FPGA are compared for sorting performance. We slightly
tweak its RTL design into ﬁgure 2 7. Table II describes the
actual behavior of a smartcell, and can be directly used in the
Look-Up Table (LUT) in ﬁgure 2.
A ‘smartcell’ here consists of two sets of ﬂip-ﬂops (FFs)
and one digital comparator. One set of FFs drives the output
network while the other stores a value internally. Every cycle,
the input value is compared to the stored one. The greater value
is sent to output and the lesser one is saved locally. Extra logic
is added to handle the head and tail of any sequence passing
7The diagram is simpliﬁed for better understanding. Clock networks and
irrelevant control networks are omitted, and combinational logic is represented
with corresponding semantic blocks.
5
TABLE I: Subroutine memory usage of Wagner’s algorithm on SLGBP
parameterized
capacity
n ∗ 21+ n
n ∗ 21+ n
k+1 b
k+1 b
bandwidth
n b/tick
0
subroutine
round
storage of L
storage of L
optimal sort
optimal sort
pair generation
XOR (round i)
input
normal
normal
last
normal
normal
0
0
(n+k+1)(k−1)
k+1
n ∗ 21+ n
k+1 b
∗ 22+ n
k+1 b
n
2n
k+1 b/tick
k+1 b/tick
k+1 b/tick
2n+2
3n(k+1−i)
k+1
b/tick
0
0
20 b/tick
40 b/tick
672 Mib
42 b/tick
400 Mib (cid:54) 540 b/tick
0
0
24 b/tick
48 b/tick
6400 Mib
50 b/tick
4608 Mib (cid:54) 480 b/tick
0
0
43 Mib
24 Mib
(200, 9)
(144, 5)
(192, 11)
capacity
400 Mib
400 Mib
bandwidth
200 b/tick
0
capacity
4608 Mib
4608 Mib
bandwidth
144b/tick
0
capacity
24 Mib
24 Mib
bandwidth
192 b/tick
0
16 b/tick
32 b/tick
34 b/tick
(cid:54) 528 b/tick
through, because comparing binary values to nothing does not
make sense in our scenario. We add a bit on every set of FFs,
representing whether valid data is stored. When only one value
is present on a smartcell, it is saved if not, or sent to output
if already saved. The other set of FFs are set to invalid.
Fig. 2: Simpliﬁed RTL diagram of a smartcell
TABLE II: Behavioral truth table of a smartcell
case
internal
lesser
greater
invalid
valid
input
greater
lesser
valid
invalid
behavior
internal
keep
set to input
set to input
set to invalid
output
set to input
set to internal
set to invalid
set to internal
Chaining Nc smartcells in series, we can accomplish the
sorting task with linear time at the cost of linear logic. During