and the true positive rates are not changed. This is primarily
because that listening to full sentences allow the adversary to
leverage valuable contextual information.
G. End-to-end Case Study: Steal Passwords from Phone Con-
versations
We now evaluate the proposed models with an end-to-
end attack in phone conversations. We consider a real-world
scenario where the victim makes a phone call to a remote caller
and requests a password during the conversation. The objective
of the adversary is to locate and recognize the password
Fig. 17. The number of correctly inferred digits in each conversation.
from the victim’s accelerometer measurements. In this attack,
we assume that the password is preceded by the hot word
“password (is)”. As shown in section VI-F, this attack can be
easily extended to support other hot words.
In the experiment, we use the victim smartphone to make
phone calls to four volunteers (two females and two males)
in three different scenarios: 1) Table-setting: the victim smart-
phone is placed on a table. 2) Handhold-sitting: the victim sits
on a chair and holds the smartphone in hand. 3) Handhold-
walking: the victim holds the smartphone in hand and walks
around. We conduct 16 scripted conversations and 4 free
ones per person per scenario (i.e., 240 conversations in total).
During all conversations, the volunteers are asked to tell us a
random 8-digits password following the phrase “password is”.
After recording the acceleration signals for a conversation,
we ﬁrst convert the acceleration signals into multiple single-
word spectrograms and employ a password search model to
ﬁnd the spectrogram corresponding to the hot word “pass-
word”. Then a digit recognition model is used to recognize
the 8-digits password following it.
The password search model searches for the word “pass-
word” in the recorded acceleration signals by a classiﬁer that
can distinguish between “password” and the other words.
To train such a classiﬁer, we collect a training dataset with
200 “password”s and 2200 negative samples including digits
and some other words. The class imbalance problem is also
addressed through re-weighting the loss as in Section VI-F.
Speciﬁcally, we weight the loss computed by the “password”
samples with a factor 11α, and the loss computed by negative
samples with a factor α. In the case that the model recognizes
multiple “password”s in a conversation,
the one with the
highest conﬁdence value will be reported. Using this model, we
successfully locate the password for over 85% conversations
in all scenarios, as shown in table XI.
TABLE XI.
PERCENTAGE OF CORRECT DIGITS INFERRED BY EACH
MODEL.
Setting
Table-setting
Handhold-sitting
Handhold-walking
Password search
92%
85%
91%
top1
59%
51%
50%
Digit recognition
top3
84%
83%
81%
top5
92%
94%
91%
The digit recognition model for each scenario is trained
with 280 × 10 digits spectrograms. Table XI lists the overall
accuracy of the recognition model. We also calculate the num-
ber of correctly inferred digits in each conversation and plot the
distribution in Fig. 17. It can be observed that the recognition
accuracy in the phone call scenario is lower compared with
16
the record-and-play scenario. This is primarily because that the
audio signal transmitted during a phone call has lower quality
than the audio signal recorded by a recording application. One
important observation is that the recognition model achieves
over 80% top 3 recognition accuracy in all scenarios. Although
the proposed attack only recognizes the complete password in a
few conversations, it will greatly assist the adversary to narrow
his search for the victim’s password.
Another effective defense is to notify the user when some
applications are collecting accelerometer readings in the back-
ground with a high sampling rate. For instance, iOS presents
a ﬂashing “microphone” icon on the status bar when some
applications are collecting voice signals in the background. A
similar mechanism can be deployed on the Android system
to remind users when, where and how their accelerometer
readings are used.
VII. FURTHER IMPROVEMENT
As mentioned at the end of Section VI-E, the performance
of our reconstruction module is still limited by the frequency
range of the acceleration signals and the veracity of the GL al-
gorithm. The ﬁrst limitation restricts the audio frequency range
that could be reconstructed, thus the consonant information,
which is mainly distributed in the high frequency domain as
shown in Fig. 16(b), will lose. However, we believe that this
limitation can be mitigated by the hardware improvement of
smartphones in the future. The second limitation is probably
due to the GL algorithm since this algorithm attempts to
compensate all the phase information from nearly scratch. To
improve the performance, we propose to involve the phase
information of the acceleration signals in the algorithm, which
is expected to be detailed in our future work.
VIII. DEFENSE
We now discuss possible directions to defend the proposed
attack. As the lowest fundamental frequency of typical human
speech is 85 Hz, one promising defense is to limit
the
sampling rate of the accelerometer. According to the Nyquist
sampling theorem, an accelerometer working bellow 170Hz
will not be able to reproduce any frequency components above
85Hz. Although the accelerometer can still be affected by the
high-frequency audio signal, the captured information will be
distorted and the recognition accuracy is likely to decrease.
In order to ﬁnd an optimal threshold, we conduct speech
recognition with accelerometer measurements sampled at 300
Hz, 200 Hz, 160 Hz, 100 Hz, and 50 Hz. Table XII shows
the recognition accuracy on the digits dataset (10k single-
digit signals from 20 speakers) under the table setting. It
can be observed that, the recognition accuracy drops with the
decreasing sampling rate and reduces to 30% at 50 Hz. In
actual attacks, the recognition accuracy at 50 Hz could further
decrease since the acceleration signal below 25 Hz can be
signiﬁcantly affected by human movement.
TABLE XII.
THE IMPACT OF THE SAMPLING RATE (TABLE SETTING).
Sampling rate
Recognition accuracy
300 Hz
73%
200 Hz
64%
160 Hz
56%
100 Hz
47%
50 Hz
30%
According to Android Developer [2], the recommended
sampling rates for the user interface and mobile games are
16.7 Hz and 50 Hz respectively. For activity recognition, 50
Hz is also more than sufﬁcient since the frequencies of most
human activities are below 20 Hz. Therefore, we suggest
that applications requiring sampling rates above 50 Hz should
request a permission through , which
will affect how Google play ﬁlters them [3].
IX. CONCLUSION
In this paper, we revisit
the threat of zero-permission
motion sensors to speech privacy and propose a highly prac-
tical side channel attack against smartphone speakers. We
ﬁrst present two fundamental observations that extend motion
sensor based audio eavesdropping to everyday scenarios. First,
speech signals emitted by smartphone speakers will always
create signiﬁcant impacts on the accelerometer of the same
smartphone. Second,
the accelerometer on recent Android
smartphones can almost cover the entire fundamental fre-
quency band of adult speech. On top of these pivotal obser-
vations, we propose AccelEve, a learning based smartphone
eavesdropping attack that could recognize and reconstruct
speech signals emitted by smartphone speakers, no matter
where and how the smartphone is placed. With deep networks,
adaptive optimizers, and robust & generalizable losses, our
attack signiﬁcantly and consistently outperforms baseline and
existing solutions in all recognition and reconstruction tasks.
In particular, AccelEve achieves three times the accuracy of
previous work in digit recognition. For speech reconstruction,
AccelEve is able to reconstruct speech signals with enhanced
sampling rate, which covers not only the fundamental fre-
quency components (vowels) in the low frequency band but
also its harmonics in the high-frequency band. The consonants
are not recovered because their frequencies (above 2000Hz) are
far beyond the sampling rates of current smartphones.
ACKNOWLEDGMENT
We thank the reviewers for their helpful comments. This
research is supported in part by National Natural Science Foun-
dation of China (Grants No. 61772236), Zhejiang Key R&D
Plan (Grant No. 2019C03133), Alibaba-Zhejiang University
Joint Institute of Frontier Technologies, Research Institute of
Cyberspace Governance at Zhejiang University, and McGill
University’s William Dawson Scholar Chair Professor Fund.
REFERENCES
[1]
[2]
[3]
“Coriolis force,” https://en.wikipedia.org/wiki/Coriolis force.
“Sensor Overview,” https://developer.android.com/guide/topics/sensors/
sensors overview.
“uses-permission,” https://developer.android.com/guide/topics/manifest/
uses-permission-element.
[4] ANALOG DEVICES, “ADXL150/ADXL250,” https://hibp.ecse.rpi.
edu/∼connor/education/EIspecs/ADXL150 250 0.pdf.
[5] S. A. Anand and N. Saxena, “Speechless: Analyzing the threat to speech
privacy from smartphone motion sensors,” in 2018 IEEE Symposium on
Security and Privacy (SP).
IEEE, 2018, pp. 1000–1017.
[6] S. A. Anand, C. Wang, J. Liu, N. Saxena, and Y. Chen, “Spearphone:
A speech privacy exploit via accelerometer-sensed reverberations from
smartphone loudspeakers,” arXiv preprint arXiv:1907.05972, 2019.
[7] R. J. Baken and R. F. Orlikoff, Clinical measurement of speech and
voice. Cengage Learning, 2000.
17
[8] S. Becker, M. Ackermann, S. Lapuschkin, K.-R. M¨uller, and W. Samek,
“Interpreting and explaining deep neural networks for classiﬁcation of
audio signals,” arXiv preprint arXiv:1807.03418, 2018.
[9] H. Bojinov, Y. Michalevsky, G. Nakibly, and D. Boneh, “Mo-
bile device identiﬁcation via sensor ﬁngerprinting,” arXiv preprint
arXiv:1408.1416, 2014.
[10] L. Cai and H. Chen, “Touchlogger: Inferring keystrokes on touch screen
from smartphone motion.” HotSec, vol. 11, no. 2011, p. 9, 2011.
[11] R. Chen and I. C. Paschalidis, “A robust learning approach for regres-
sion models based on distributionally robust optimization,” The Journal
of Machine Learning Research, vol. 19, no. 1, pp. 517–564, 2018.
[12] A. Das, N. Borisov, and M. Caesar, “Exploring ways to mitigate sensor-
based smartphone ﬁngerprinting,” arXiv preprint arXiv:1503.01874,
2015.
[13] A. De Cheveign´e and H. Kawahara, “Yin, a fundamental frequency
estimator for speech and music,” The Journal of the Acoustical Society
of America, vol. 111, no. 4, pp. 1917–1930, 2002.
[14] R. N. Dean, G. T. Flowers, A. S. Hodel, G. Roth, S. Castro, R. Zhou,
A. Moreira, A. Ahmed, R. Rifki, B. E. Grantham et al., “On the
degradation of mems gyroscope performance in the presence of high
power acoustic noise,” in 2007 IEEE International Symposium on
Industrial Electronics.
IEEE, 2007, pp. 1435–1440.
[15] S. Dey, N. Roy, W. Xu, R. R. Choudhury, and S. Nelakuditi, “Accelprint:
Imperfections of accelerometers make smartphones trackable,” in NDSS,
2014.
J. Doscher and M. Evangelist, “Accelerometer design and applications,”
Analog devices, vol. 3, p. 16, 1998.
[16]
[17] H. Feng, K. Fawaz, and K. G. Shin, “Continuous authentication for
voice assistants,” in Proceedings of
the 23rd Annual International
Conference on Mobile Computing and Networking. ACM, 2017, pp.
343–355.
[18] H. Fujisaki, “Dynamic characteristics of voice fundamental frequency
in speech and singing,” in The production of speech. Springer, 1983,
pp. 39–55.
[19] S. Grawunder and I. Bose, “Average speaking pitch vs. average speaker
fundamental frequency–reliability, homogeneity, and self report of lis-
tener groups,” in Proceedings of the International Conference Speech
Prosody, 2008, pp. 763–766.
[20] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-time
fourier transform,” IEEE Transactions on Acoustics, Speech, and Signal
Processing, vol. 32, no. 2, pp. 236–243, 1984.
J. Han, E. Owusu, L. T. Nguyen, A. Perrig, and J. Zhang, “Accom-
plice: Location inference using accelerometers on smartphones,” in
2012 Fourth International Conference on Communication Systems and
Networks (COMSNETS 2012).
IEEE, 2012, pp. 1–9.
[21]
[22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition.
IEEE, 2016, pp. 770–778.
[23] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen,
R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al.,
“CNN architectures for large-scale audio classiﬁcation,” in 2017 ieee
international conference on acoustics, speech and signal processing
(icassp).
IEEE, 2017, pp. 131–135.
[24] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition.
IEEE, 2017, pp.
4700–4708.
J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual
losses for real-
time style transfer and super-resolution,” in European conference on
computer vision. Springer, 2016, pp. 694–711.
[25]
[27]
[26] T. Kaya, B. Shiari, K. Petsch, and D. Yates, “Design of a mems
capacitive comb-drive accelerometer,” in COMSOL Conference, Boston,
2011.
J. R. Kwapisz, G. M. Weiss, and S. A. Moore, “Activity recognition us-
ing cell phone accelerometers,” ACM SigKDD Explorations Newsletter,
vol. 12, no. 2, pp. 74–82, 2011.
J. Liu, C. Wang, Y. Chen, and N. Saxena, “Vibwrite: Towards ﬁnger-
input authentication on ubiquitous surfaces via physical vibration,” in
Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2017, pp. 73–87.
[28]
[29] P. Marquardt, A. Verma, H. Carter, and P. Traynor, “(sp) iphone:
Decoding vibrations from nearby keyboards using mobile phone ac-
celerometers,” in Proceedings of the 18th ACM conference on Computer
and communications security. ACM, 2011, pp. 551–562.
[30] A. Matic, V. Osmani, and O. Mayora, “Speech activity detection using
accelerometer,” in 2012 Annual International Conference of the IEEE
Engineering in Medicine and Biology Society.
IEEE, 2012, pp. 2112–
2115.
[31] R. Matovu, I. Griswold-Steiner, and A. Serwadda, “Kinetic song com-
prehension: Deciphering personal listening habits via phone vibrations,”
arXiv preprint arXiv:1909.09123, 2019.
[32] Y. Michalevsky, D. Boneh, and G. Nakibly, “Gyrophone: recognizing
speech from gyroscope signals,” in Proceedings of the 23rd USENIX
conference on Security Symposium. USENIX Association, 2014, pp.
1053–1067.
[33] E. Miluzzo, A. Varshavsky, S. Balakrishnan, and R. R. Choudhury,
“Tapprints: your ﬁnger taps have ﬁngerprints,” in Proceedings of the
10th international conference on Mobile systems, applications, and
services. ACm, 2012, pp. 323–336.
[34] E. Owusu, J. Han, S. Das, A. Perrig, and J. Zhang, “Accessory: pass-
word inference using accelerometers on smartphones,” in Proceedings
of the Twelfth Workshop on Mobile Computing Systems & Applications.
ACM, 2012, p. 9.
[35] M. Shoaib, H. Scholten, and P. J. Havinga, “Towards physical activity
recognition using smartphone sensors,” in 2013 IEEE 10th international
conference on ubiquitous intelligence and computing and 2013 IEEE
10th international conference on autonomic and trusted computing.
IEEE, 2013, pp. 80–87.
[36] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[37] Y. Son, H. Shin, D. Kim, Y. Park, J. Noh, K. Choi, J. Choi, and Y. Kim,
“Rocking drones with intentional sound noise on gyroscopic sensors,”
in Proceedings of the 24th USENIX Conference on Security Symposium.
USENIX Association, 2015, pp. 881–896.
I. R. Titze and D. W. Martin, “Principles of voice production,” Acous-
tical Society of America Journal, vol. 104, p. 1148, 1998.
[38]
[39] T. Trippel, O. Weisse, W. Xu, P. Honeyman, and K. Fu, “WALNUT:
Waging doubt on the integrity of mems accelerometers with acoustic
injection attacks,” in 2017 IEEE European Symposium on Security and
Privacy (EuroS&P).
IEEE, 2017, pp. 3–18.
[40] Y. Tu, Z. Lin, I. Lee, and X. Hei, “Injected and delivered: fabricating
implicit control over actuation systems by spooﬁng inertial sensors,” in
Proceedings of the 27th USENIX Conference on Security Symposium.
USENIX Association, 2018, pp. 1545–1562.
[41] C. Xu, Z. Li, H. Zhang, A. S. Rathore, H. Li, C. Song, K. Wang,
and W. Xu, “WaveEar: Exploring a mmwave-based noise-resistant
speech sensing for voice-user interface,” in Proceedings of the 17th
Annual International Conference on Mobile Systems, Applications, and
Services. ACM, 2019, pp. 14–26.
[42] Z. Xu, K. Bai, and S. Zhu, “Taplogger: Inferring user inputs on smart-
phone touchscreens using on-board motion sensors,” in Proceedings
of the ﬁfth ACM conference on Security and Privacy in Wireless and
Mobile Networks. ACM, 2012, pp. 113–124.
[43] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv
preprint arXiv:1605.07146, 2016.
[44] L. Zhang, P. H. Pathak, M. Wu, Y. Zhao, and P. Mohapatra, “Accelword:
Energy efﬁcient hotword detection through accelerometer,” in Proceed-
ings of the 13th Annual International Conference on Mobile Systems,
Applications, and Services. ACM, 2015, pp. 301–315.
18