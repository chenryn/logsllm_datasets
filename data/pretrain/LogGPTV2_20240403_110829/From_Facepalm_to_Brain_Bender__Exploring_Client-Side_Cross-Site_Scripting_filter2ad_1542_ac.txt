tional strings, consisting of an untainted part and a tainted
string (shown in bold, (2)). In order to extract the source
of said tainted data, we traverse up the tree once more to
ﬁnd that it originated from location.href (3). Similarly,
the data structure can be traversed for sink accesses with
multiple pieces of tainted data.
As our study is based on data derived from previous work
which used a patched variant of Chromium to detect and
exploit ﬂows, we expect these vulnerabilities to be at least
partially dependent on Chromium’s encoding behavior with
respect to the URL, and more important, the URL fragment,
which is — in contrast to Firefox — not automatically en-
coded. Therefore, we opted to align our browsing engine’s
encoding behavior with that of Chromium, i.e., by not au-
tomatically encoding the fragment.
4.4 Post-Processing
Before the raw data gathered by our engine can be ana-
lyzed, it needs to processed to ensure a correct analysis. In
the following, we illustrate these post-processing steps.
Identifying Vulnerable Flows: Only a fraction of all
ﬂows which occur during the execution of a Web application
1423are vulnerable. Our browsing engine collects all ﬂows which
occur and sends them to our backend for storage. Thus, be-
fore an analysis can be conducted, we need to identify the
actual vulnerable ﬂows. We therefore discard all ﬂows which
do not end in document.write, innerHTML or eval. Next,
we determine if our payload was completely contained in the
string which was passed to the sink, i.e., we ensure that the
data only contains actual exploitable ﬂows.
jQuery Detection and Removal: One of the most com-
monly used libraries in many Web applications is jQuery [2,
29]. It provides programmers with easy access to function-
ality in the DOM, such as a wrapper to innerHTML of any
element. When analyzing the reports gathered from our
taint-aware browsing engine, the calls to such wrapper func-
tions increase the number of functions traversed by a ﬂow,
increasing the perceived complexity. This, however, is not
true since the vulnerability was introduced by using jQuery
in an insecure manner, not jQuery itself. Therefore, we se-
lect to ﬁlter jQuery functions and use them as sinks, i.e.,
the html and append functions of jQuery are treated like an
assignment to an element’s innerHTML property.
We therefore implemented a detection mechanism to pin-
point which functions were provided by jQuery. The ﬂowchart
in Figure 3 shows this process.
Initially, we iterate over
all entries in the stack traces collected by our taint-aware
browser and determine the ﬁle in which each line is con-
tained. We then check the hash sum of that ﬁle against
known jQuery variants (1).
If no hash match is found,
we utilize the methodology used by Retire.js [18] to detect
whether jQuery is contained in that ﬁle at all (2). If this
step indicates that the ﬁle contains jQuery, we proceed to
gathering script statistics (such as the number of strings,
identiﬁers and functions) and comparing them to known ver-
sions of jQuery, to assess whether the script solely consists of
jQuery (3). If this does not produce a conclusive match, we
resort to a full-text search of the line of code in a database
of all lines of all known versions of jQuery (4). If no match
is found, we mark the generated report for later manual
analysis. This happens when Web site owners use custom
packers, rendering full-text search infeasible. If any of these
checks indicate that the analyzed stack entry points to code
located within jQuery, we remove said entry from our trace
both at the bottom and the top of the stack. This allows to
remove artefacts from sink-like operations such as html and
jQuery-enabled uses of events.
Stack Gap Detection and Removal: After the jQuery
detection ﬁnishes, we proceed to the next phase of post-
processing. Our taint-aware engine is able to record all calls
to functions a tainted string is passed to.
In some cases,
however, an indirect data ﬂow occurs, i.e., the string is not
passed to a function but rather written to a global variable.
If another function then accesses the tainted data, the ac-
cessing operation is logged; nevertheless, the actual call to
said function is missing, although an analyst has to follow
this call in his analysis. To allow for a correct value for met-
ric M2, we close this stack gap by inserting virtual function
calls into the trace.
Operation Fusing: As discussed in Section 3.1, M1 mea-
sures the number of operations which were conducted on
tainted data. In terms of understanding a vulnerability, sev-
eral concatenations on a single line do not complicate the
Figure 3: Flowchart for jQuery detection process
analysis; therefore, all consecutive concat operations on the
same source code line are fused to form a single operation.
4.5 Overall Infrastructure
An overview of our complete infrastructure is depicted in
Figure 4. Initially, all responses to requests, which were con-
ducted when verifying the exploits, are stored into a cache
database (1). Afterwards, we beautify all HTML and Java-
Script ﬁles and store them alongside the cache (2). In the
next step, our taint-aware Firefox browsing engine visits the
cached entries, while the proxy serves beautiﬁed version of
HTML and JavaScript ﬁles as well as unmodiﬁed versions of
all other ﬁles, such as images (3). The engine then collects
trace information on all data ﬂows that occur during the
execution of the page and sends it to a central backend for
storage (4). After the data for all URLs under investigation
has been collected, the data is post-processed (5) and can
then be analyzed (6).
5. EMPIRICAL STUDY
In this section, we outline the execution of our study as
well as the results. We then present classiﬁcation boundaries
for the metrics discussed in Section 3.1 and describe the
results of that classiﬁcation.
5.1 Data Set and Study Execution
After having an infrastructure that allowed for persisting
and thus consistently reproducing vulnerabilities, we took
a set of known vulnerabilities derived by our methodology
presented in 2013 [12]. In total, this set consisted of 1,146
URLs in the Alexa Top 10k which contained at least one
veriﬁed vulnerability, i.e., a crafted URL which would trig-
ger our JavaScript to execute in the vulnerable document.
After persisting and beautifying the vulnerable code, we
crawled the URLs with our taint-enhanced Firefox, collect-
ing a total of 3,080 distinct trace reports, whereas a trace
report corresponds to one access to sink, potentially con-
sisting of more than one ﬂow. Out of these reports, only
1,273 could be attributed to actual vulnerabilities; the rest
of the sink accesses either occurred with sanitized data or
involved sinks which are not directly exploitable (such as
document.cookie).
As discussed in Section 3.1, a single sink access may use
data from several sources. In total, we found that the 1,273
Hash MatchStartScript jQuery onlyRetire.jsNoDetermine Script StatsCompare StatsFull-text MatchNo DiffYesjQuery LineYesYesManual Analysis1234DiffNo jQueryNoNo1424Figure 4: Overview of our analysis infrastructure
traces our engine gathered consisted of 2,128 pieces of data,
whereas the maximum number of involved sources was 35.
Note, that in this case data from a single source was used
multiple times.
5.2 Result Overview
In this section, we present an overview of the results from
our study. The presented data in then analyzed in further
detail in Section 5.4.
M1 Number of string-accessing operations: Figure 5
shows the distribution of the number of string-accessing op-
erations in relation to the number of vulnerabilities we en-
countered. Out of the 1,273 vulnerable ﬂows in our data
set, we ﬁnd that 1,040 only have less than 10 operations
(including source and sink access). The longest sequence of
operations had a total length of 291, consisting mostly of
regular expression checks for speciﬁc values.
M2 Number of involved functions: Apart from the num-
ber of contexts, we also studied the number of functions that
were involved in a particular ﬂow. We found that 579 ﬂows
only traversed a single function, i.e., no function was called
between source and sink access. In total, 1,117 ﬂows crossed
ﬁve or less functions. In the most complex case, 31 functions
were involved in operations that either accessed or modiﬁed
the data. The distribution is shown in Figure 6.
M3 Number of involved contexts: Out of the 1,273 vul-
nerabilities we analyzed, the exploitable ﬂow was contained
within one context in 782 cases, and 25 ﬂows traversed more
than three contexts (with the maximum number of contexts
being six). Figure 7 shows this, highlighting that more than
90% of the ﬂaws were spread across one or two contexts.
M4 Locality of source and sink access: For all vulner-
abilities which were contained either in one single external
ﬁle or inside the same HTML document, we determined the
lines of code between the two operations. Out of the 1,150
reports that matched this criterion, the contained vulnera-
bility was located on a single line in 349 cases, and within ten
lines of code 694 times. In contrast, 40 vulnerabilities had
a distance of over 500 lines of code between source and sink
Figure 6: Histogram for M2 (number of functions)
Figure 7: Histogram for M3 (number of contexts)
access. In the most complex case, the access to the source
occurred 6,831 lines before the sink access. The distribution
of this metric is shown in Figure 8.
M5 Relation between source and sink: In our analysis,
we found that the most common scenario was R1, which
applied to a total of 914 ﬂows. Second to this, in 180 cases,
the source was an ancestor of the sink ( R2), whereas this
relation was reversed 71 times (R3). Source and sink shared
a common ancestor in 49 ﬂows (R4) and ﬁnally, there was
no relation between source and sink in 59 traces (R5).
5.3 Additional Characteristics
In addition to the proposed metrics, several additional
characteristics can be observed for our data set of vulnera-
bilities, which we outline in the following.
Figure 5: Histogram for M1 (string-accessing ops)
Figure 8: Histogram for M4 (Code locality)
CacheBeautifiedProxyPost-processingAnalysis3124560510152025+050100150200250300350051015+0100200300400500600012345+02004006008000100200300400500+01002003004005006007001425As we discussed beforehand, both code and data ﬂows may
occur in a non-linear manner. Table 1 shows the distribu-
tion of this feature in our data set. Note, that a linear data
ﬂow cannot occur with a non-linear control ﬂow, since this
implies no relation between source and sink accessing opera-
tions. We observe that 59 cases, which are also matched by
R5, both data and control ﬂow are non-linear. In addition,
we found that in 98 of the vulnerable ﬂows, a non-linear data
ﬂow occured, i.e., the data was not passed as a parameter
to all functions it traversed.
In terms of code origin, our analysis revealed interesting
results. While the biggest fraction, namely 835 vulnerabil-
ities, was caused purely by self-hosted code, we found that
273 ﬂaws were contained exclusively in third-party scripts,
leaving the Web page exposed to a Cross-Site Scripting ﬂaw
to no fault of its developer. The remaining 165 ﬂaws oc-
curred in a combination of self-hosted and third-party code.
An attacker may leverage a single sink access which con-
tains more than one attacker-controllable piece of data to
circumvent popular Cross-Site Scripting ﬁlters [27]. There-
fore, an additional characteristic is whether a ﬂaw consti-
tutes a multiﬂow, which we discovered in 344 of the exploited
Web pages.
Although the sink in which the vulnerable ﬂow of data
ended is not directly related to the complexity of the vulner-
ability itself, it is relevant for remedies, as diﬀerent ﬁltering
steps must be taken depending on the type of sink. In our
study, we found that 732 exploitable ﬂows ended in doc-
ument.write, 495 in innerHTML and remaining 46 in eval
and its derivatives.
In addition to eval being a sink, we also observed ﬂows
in which it was used to generate code, which was ultimately
responsible for a ﬂow, at runtime. In total, only eleven of
such cases were contained in our data, while the most com-
mon scenario was deobfuscation of code at runtime, e.g., by
base64-decoding it.
5.4 Analysis
In Section 3.1, we deﬁned a set of metrics to measure
the complexity of a vulnerable ﬂow, which we then applied
to a set of real-world vulnerabilities in our study. To bet-
ter quantify the complexity of a ﬂaw, we need to translate
the numeric values derived by our metrics into a classify-
ing scheme. In the following, we introduce the classiﬁcation
boundaries for these measures; based on these boundaries,
we then classify each of the vulnerable ﬂows in our data set
to either have a low, medium or high complexity with re-
spect to each metric. Finally, we combine the classiﬁcation
results and highlight the necessity for a multi-dimensional
classiﬁcation scheme.
Classiﬁcation Scheme: Based on the gathered data of
all metrics, we derive the 80th and 95th percentiles, i.e.,
LCF NLCF
Sum
LDF
NLDF
1,116
98
— 1,116
59
157
Sum
1,273
Table 1: Data and code ﬂow matrix
1,214
59
Figure 9: Cumulative Sum for M1 (string-accessing ops)
Figure 10: Cumulative Sum for M2 (number of functions)
derive the numbers for which at least 80% and 95% of all
vulnerable ﬂows have a lower metric value, respectively. For
M1 and M2, the cumulative sums are depicted in Figures 9
and 10, highlighting also both the percentiles. Although
metric M5, which denotes the relation of source and sink
accessing operations, does not return a numerical value, the
perceived complexity rises with the identiﬁer, i.e., R2 is more
complex than R1 and so on and so forth. For this metric,
more than 80% of the ﬂows were contained in the relation
classes R1 and R2 and less than 5% of the ﬂows were made
up out of ﬂows which had no relation between source and
sink (R5).
We use the resulting percentiles as cut-oﬀ points for our
complexity metric classiﬁcation. Therefore, we set bound-
aries for all of our metrics accordingly (as depicted in Ta-
ble 2), such that any value maps to either a low (LC ),
medium (MC ) or high (HC ) complexity. We calculate the
overall complexity of a ﬂaw from the single highest rating
by any metric; this appeals naturally to the fact that a vul-
nerable ﬂow is already hard to understand if it is complex
in just a single dimension.
Classiﬁcation Results: Based on our classiﬁcation scheme,
we categorize each of the ﬂaws in our data set to a com-
plexity class. The results of this classiﬁcation scheme are