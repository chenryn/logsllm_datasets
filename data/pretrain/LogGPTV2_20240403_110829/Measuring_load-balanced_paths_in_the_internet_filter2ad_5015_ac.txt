 10
 20
 30
 40
 50
Rank of load balancers
Figure 3: Cumulative fraction of paths aﬀected by
the 50 most frequently traversed load balancers.
For each type of load balancer, we plotted the two
sources which represent the extreme results.
only plotted the results for the two sources having the ex-
treme results. The “per-ﬂow” and “per-destination” load
balancers curves show that the top-50 load balancers aﬀect
at least 78% of the paths that exhibit load balancing. For
instance, the most frequent per-ﬂow load balancer aﬀects
38% of the load-balanced paths in the paris trace. We stud-
ied this load balancer in detail and found that it is a Level3
router that connects to RENATER.4 Similarly, nearly all
the paths from the intel source have per-destination load
balancing caused by a load balancer in AT&T’s network,
which is intel’s provider.
In contrast, we do not ﬁnd any predominant per-packet
load balancer in our traces. The 50 most frequently found
ones aﬀect at most 60% of the paths with per-packet load
balancing. We ﬁnd that the most frequently encountered
per-packet load balancers are in Sprint’s network. This
ﬁnding is puzzling given that large ISPs often avoid per-
packet load balancing for fear of the negative impact on TCP
connections. We studied these load balancers more closely
and veriﬁed that they are located at peering points between
4Level3 is a tier-1 ISP and is one of RENATER’s providers.
Sprint and other domains. For instance, we found one per-
packet load balancer between Sprint and the China169 back-
bone. The load-balanced interfaces after this load balancer
all belong to the same router, and have DNS names such
as sl-china7-5-0.sprintlink.net, a name that indicates
that it is, indeed, at a peering point. We ﬁnd similar situa-
tions at the edges of other tier-1 ISPs. If this is being done
purposefully, perhaps it is a situation where high link uti-
lization is especially important, such as when load balancing
over a bundle of parallel low capacity links in preference to
a single, more expensive, high capacity link. Some other
instances may also correspond to misconﬁgured routers us-
ing the per-packet technique instead of the per-ﬂow or per-
destination one.
Most of the per-packet load balancers aﬀect just a few
paths, because they are located far from the source and close
to the destination. Indeed, 85% of those load balancers are
located at less than 3 hops from the destination.
5.2 Intra- and inter-AS load balancing
Load-balanced paths can be contained in one autonomous
system (AS), which we deﬁne as intra-domain load balanc-
ing, or span multiple ASes, deﬁned as inter-domain load
balancing. Although forwarding in both cases is done in the
same way, the routing mechanism behind them is very dif-
ferent. A router can install multiple intra-domain routes in
its forwarding table, because of the equal-cost multipath ca-
pability of common intra-domain routing protocols such as
IS-IS [19] and OSPF [20]. In this case, the paths will diverge
after entering the AS and re-converge before exiting it.
On the other hand, BGP [21], the internet’s inter-domain
routing protocol, does not allow a router to install more than
one next hop for a destination preﬁx. Given this restriction,
there should be no inter-domain load balancing. However,
some router vendors now provide BGP-multipath capabili-
ties (for instance, Juniper [22] and Cisco [23]). If two BGP
routes for a preﬁx are equivalent (same local preference, AS-
path length, etc.) and the multipath capability is on, then
BGP can install more than one next hop for a preﬁx. An-
other scenario in which we could observe inter-domain load
balancing is when BGP routes are injected into the intra-
domain routing protocol. Then, BGP routes would be sub-
ject to the OSPF or IS-IS equal-cost multipath mechanism.
Injecting BGP routes into intra-domain routing is, we be-
lieve, rare, so this scenario should not often arise. However,
injecting only the default route(s) to upstream provider(s) is
a more practicable scenario which is often used by network
operators.
To make the distinction between the two types of load
balancing, we need to map each IP address in our traces
to an AS. We use a public IP-to-AS mapping service [24].
This service builds its mapping from a collection of BGP
routing tables. There are well-known issues with this type
of mapping [25], so for one of the traces we manually veriﬁed
each instance of supposed inter-domain load balancing.
Our automated classiﬁcation does not consider the con-
vergence or the divergence point of a diamond to label load
balancers. In so doing, we avoid false positives (classiﬁca-
tion of intra-domain load balancing as inter-domain), but
may generate false negatives. This technique is important
because it is very common that an interface in the boundary
between two ASes is numbered from the address space of one
AS, but belongs in fact to the other. Fig. 4 illustrates this
scenario. It shows two domains, AS1 and AS2, and a load
balancer, L. Square interfaces are numbered from AS1’s ad-
dress space, whereas circular ones belong to AS2’s address
space. We observe that the interfaces of the link Z-L are
numbered from AS1’s address space. A traceroute from S
to T discovers the “square” interface of L. In this case, we
could mistakenly label L as an inter-domain load balancer,
because L belongs to AS1 and balances traﬃc to routers A
and B, which belong to AS2. If we ignore the divergence
point when computing the AS path in a diamond, then L
would be correctly labeled as an intra-domain load balancer
in AS2.
AS1
S
Z
L
AS2
A
B
T
Address space of AS1
Address space of AS2
Figure 4: Domain boundary delimitation can be in-
accurate.
We also ignore the convergence point because it may not
be involved in load balancing.
Indeed, the IP-level load-
balanced path inferred by Paris traceroute may not cor-
respond to the router-level load-balanced path in the real
topology. Fig. 5 illustrates how this phenomenon arises.
The left side represents the router-level topology and the
right side the IP-level topology inferred with the MDA. The
two load-balanced paths merge at two diﬀerent interfaces of
router C. The probing of the upper path reveals C0 and the
lower path reveals C1. Since we do not conduct alias reso-
lution, we treat those two interfaces as if they belonged to
diﬀerent routers. The consequences are twofold. First, the
Actual topology:
Paris traceroute outcome:
0
L
1
2
0
0
A
B
1
1
0
1
2
C
0
1
2
D
L
0
A
0
B
0
Hop #6 Hop #7 Hop #8 Hop #9
Hop #6
Hop #7
0
C
C
Hop #8
1
D
0
Hop #9
Figure 5: The IP-level multi-path inferred by Paris
traceroute may not correspond to the router-level
multi-path in the real topology.
length of the measured diamond does not reﬂect the length
of load-balanced path in the router-level topology. Second,
we may consider some parts of the topology as being in-
volved in load balancing, whereas they are not. More pre-
cisely, the convergence point in the inferred topology, D0,
has actually nothing to do with load balancing. The left
side of the ﬁgure shows that router D is not part of the
real load-balanced path at all. As a result, we may misclas-
sify some diamonds as inter-domain if router D belongs to a
diﬀerent autonomous system. Note that this bias arises be-
cause the parallel paths merge through diﬀerent interfaces
of a router. If they merge through a level 2 device such as
a switch, and then connect to a single interface, then the
inferred topology maps to the router-level one. Although
we do not perform systematic alias resolution on the discov-
ered interfaces, our partial observations of IP Identiﬁers and
DNS names indicated that all the penultimate interfaces of
a diamond generally belong to the same router.
The manual veriﬁcation step is very time consuming, so
we only classiﬁed intra- and inter-domain load balancers for
the paris MIT trace. In most cases, diamonds are created by
intra-domain load balancing. From the paris vantage point,
86% of the per-ﬂow diamonds ﬁt in a single AS. Fig. 2 illus-
trates this case. Diamond 1 exactly spans Savvis’s network
and diamond 2 spans Google’s network. The parallel paths
in diamond 1 diverge at the entry point of Savvis’s domain
and then reconverge before they reach its exit point, because
routers selected a single peering link between the two do-
mains. We found rarer cases of diamonds crossing multiple
ASes. Most of them involve two ASes, but extremely rare
diamonds cross three networks. We found such diamonds
in the paths towards 37 destinations. They always involved
Level3 as the ﬁrst domain, peering with Verizon, Bellsouth
and some smaller networks like Road Runner. Thus, it seems
that very few core networks enable BGP multipath capabil-
ities in their routers.
Most per-destination diamonds are also created by intra-
domain load balancers (at least 80% in the paris trace), but
we did not conduct any of the manual veriﬁcation on this
dataset.
6. LOAD-BALANCED PATHS
Having described the mechanisms behind load-balanced
paths, we now study their properties and characterize them
in terms of the widths and lengths of diamonds. The statis-
tics presented here are for the MIT destination list.
6.1 Diamond width
We use two metrics deﬁned in Sec. 4.2 to describe the
number of paths available to a source-destination pair: a
diamond’s min-width provides a lower bound and the max-
width provides an upper bound on this number.
If there
should be two or more diamonds in a path, we take the low-
est min-width and the highest max-width. It is fairly com-
mon to see two diamonds in a path: 21% of the pairs have
two per-ﬂow diamonds and 24% have two per-destination
diamonds. Any more than two is extremely rare; less than
1% of the paths.
Fig. 6 presents the min-width distribution for per-ﬂow
and per-destination load-balanced paths in the MIT dataset.
Note that the Y axis is in log scale.
6.1.1 Narrow diamonds
This plot shows that load-balanced paths are generally
narrow. For per-ﬂow load balancing, 55% of the pairs en-
counter a diamond with only two link-disjoint paths, and
99% of the pairs encounter diamonds with ﬁve or fewer link-
disjoint paths. For per-destination load balancing, the ﬁg-
ures are 67% and 98%, and for per-packet load balancing
(not shown), they are 60% and 90%.
The max-width distribution (not shown) is of course less
skewed towards diamonds of width 2. Only 24% of per-
ﬂow load-balanced paths and 27% of per-destination load-
balanced paths traverse a diamond with just two interfaces
at the widest hop distance. Nonetheless the diamonds tend
per-flow
per-destination
s
r
i
a
p
n
o
i
t
a
n
i
t
s
e
d
-
e
c
r
u
o
s
#
 1e+06
 100000
 10000
 1000
 100
 10
 1
2
4
6
10
8
Min-width
12
14
16
Figure 6: Min-width distributions (MIT).
to be narrow by this metric as well: 85% of the per-ﬂow
load-balanced paths and 90% of the per-destination load-
balanced paths have ﬁve or fewer interfaces at the widest
hop. Because most of per-packet load-balanced paths have
a length equal to 2, their max-width distribution is similar
to their min-width distribution.
6.1.2 Wide diamonds
The maximum width that we encounter, by either met-
ric, is 16. For instance, we discovered a diamond of max-
width 16 for per-ﬂow load balancing at a peering point be-
tween a tier-1 and a Brazilian ISP. This may correspond to
many low-capacity links which are bundled because the next
higher capacity link is unavailable, unaﬀordable, or unsuit-
able. That we do not see anything wider can be explained by
a built-in limit to the number of entries that a router can in-
stall in the forwarding table for a given preﬁx. For instance,
Juniper [2] allows one to conﬁgure at most 16 load-balanced
interfaces.
Almost all of the diamonds of width 10 and greater are two
hops long. One obvious explanation for a diamond of this
length is that we are seeing multiple parallel links between
a pair of routers. As routers typically respond to traceroute
probes using the address of the incoming interface [26], a
pair of routers with parallel links will appear as a diamond
of length two at the IP level. Fig. 7 shows an example with
two parallel links.
Actual topology:
Paris traceroute outcome:
0
L
1
2
0
1
2
A
0
B
L
0
Hop #6 Hop #7 Hop #8
Hop #6
1
A
Hop #7 Hop #8
0A
B
0
Figure 7: Load balancing over parallel links
There are rare cases (67 source-destination pairs in the
MIT trace) of very wide and short per-packet diamonds
at the ends of paths (i.e., close to the destinations). For
instance, all load-balanced paths to a few hosts in Egypt
traverse a per-packet diamond of length 2, having 11 inter-
faces in parallel. Alias resolution techniques (DNS names
and checking the IP Identiﬁer values returned by probes [7])
conﬁrm that all 11 interfaces belong to the same router, and
thus that the network operator conﬁgured 11 parallel links
between two routers. Per-packet load balancing typically
appears to take place at the boundary of a small AS and its
provider. Customers may use such load balancers on access