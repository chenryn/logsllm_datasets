identity-unrelated features ğ‘“ğ‘’ = ğ¸ğ‘’ (ğ‘†). The objective function ğ¿ğ‘ğ‘‘ğ‘£
of ğ¶ğ‘ğ‘‘ğ‘£ directs speaker identification based on feature ğ‘“ğ‘’, and is
constrained through the cross entropy loss as below:
ğ‘ 
(cid:17)
(cid:16)ğ‘¦ ğ‘—
ğ‘’
ğ¿ğ‘ğ‘‘ğ‘£
ğ‘ 
= âˆ’
ğ‘¡ ğ‘—
ğ‘  log
where ğ‘¡ğ‘  means the ground truth of speakersâ€™ identity. Note that the
gradient of ğ¿ğ‘ğ‘‘ğ‘£
only propagates back to ğ¶ğ‘ğ‘‘ğ‘£, without updating
ğ‘ 
any layer of ğ¸ğ‘’.
The eliminating encoder ğ¸ğ‘’ should be trained to fool ğ¶ğ‘ğ‘‘ğ‘£, so
the speaker identity distribution ğ‘¢ğ‘  is set as a constant for each
in the cross-entropy loss of ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥.
speaker label, equal to 1
ğ‘ğ‘ 
The objective function ğ¿ğ‘ğ‘‘ğ‘£
of ğ¸ğ‘’ can be written as follow:
ğ‘’
ğ‘ğ‘ âˆ‘ï¸
ğ‘—=1
(cid:17)
(cid:16)ğ‘¦ ğ‘—
ğ‘’
ğ¿ğ‘ğ‘‘ğ‘£
ğ‘’
=
ğ‘¢ ğ‘—
ğ‘  log
=
1
ğ‘ğ‘ 
log
(cid:16)ğ‘¦ ğ‘—
ğ‘’
(cid:17) .
ğ‘ğ‘ âˆ‘ï¸
ğ‘—=1
ğ‘ğ‘ âˆ‘ï¸
ğ‘—=1
ğ‘’
ğ‘’
The gradient of ğ¿ğ‘ğ‘‘ğ‘£
only propagates back to ğ¸ğ‘’, rather than
ğ¶ğ‘ğ‘‘ğ‘£. If we allow the gradient of ğ¿ğ‘ğ‘‘ğ‘£
to update ğ¶ğ‘ğ‘‘ğ‘£, the encoder
ğ¸ğ‘’ can easily cheat ğ¶ğ‘ğ‘‘ğ‘£, e.g, by only revising the classifier ğ¶ğ‘ğ‘‘ğ‘£ to
produce non-informative output. Hence, the encoder ğ¸ğ‘’ cannot en-
sure that feature ğ‘“ğ‘’ will extract the identity-unrelated information
under these circumstances. Therefore, by combining both ğ¿ğ‘ğ‘‘ğ‘£
and
, our framework can leverage the advantages of each of them
ğ¿ğ‘ğ‘‘ğ‘£
ğ‘ 
and be coordinated to work together towards the identity-unrelated
feature through disentangled speaker information. Benefiting from
the advantages of adversarial learning, the speaker eliminating
encoder ğ¸ğ‘’ can learn irrelevant features as accurately as possi-
ble, which in turn guarantees the correctness of speaker identity
features.
ğ‘’
3.4 Reconstruction Decoder
Ideally, the combination of the decoupled features ğ‘“ğ‘ and the identity-
unrelated features ğ‘“ğ‘’ should be exactly the representation of the
input spectrogram ğ‘†. Hence, defining the feature fusion module
 based on the concatenating operation, we fuse ğ‘“ğ‘ and ğ‘“ğ‘’ to-
gether into a complete feature ğ‘“ğ‘  = ğ‘“ğ‘ ğ‘“ğ‘’, and make the decoder
ğ·ğ‘Ÿ reconstruct the spectrogram ğ‘†â€² = ğ·ğ‘Ÿ (ğ‘“ğ‘ ). To simply measure
the difference between the reconstructed spectrogram ğ‘†â€² and the
original spectrogram ğ‘†, we utilize ğ‘™2 distance2 to define the recon-
struction loss ğ¿ğ‘Ÿ as below:
(cid:13)(cid:13)(cid:13)ğ·ğ‘Ÿ
(cid:16)ğ‘“ğ‘
 ğ‘“ğ‘’
(cid:17) âˆ’ ğ‘†
(cid:13)(cid:13)(cid:13)2
2
ğ¿ğ‘Ÿ =
1
2
.
(7)
2Since the input spectrogram is in the form of a 2-dimensional matrix, ğ‘™2 distance
is a commonly used measure to compare the similarity between two 2-dimensional
matrices.
(5)
(6)
Table 1: Training and Testing Dataset on Voxceleb1.
Dataset
#POIs
#Utterances
Training Testing
1211
148,642
40
4,874
Total
1251
153,516
Table 2: Training and Testing Dataset on Voxceleb2.
Dataset
#POIs
#Utterances
Training Testing
5994
1,092,009
118
36,237
Total
6112
1,128,246
As mentioned above, the adversarial supervision signal encour-
ages the eliminating encoder ğ¸ğ‘’ to extract identity-unrelated fea-
tures. In contrast, the reconstruction loss guides the purifying en-
coder ğ¸ğ‘ to embed the remaining identity-purified features by the
fidelity of the spectrogram reconstruction, i.e., ensuring ğ‘“ğ‘  con-
tains the complete representation from the original spectrogram
ğ‘†. In general, the purifying encoder ğ¸ğ‘ needs to be trained first
to reach a certain level of accuracy in the task of speaker verifica-
tion. Then the ğ¸ğ‘’ initiates its networks by inheriting the weights
from ğ¸ğ‘ and begins the adversarial learning. Meanwhile, the recon-
struction decoder takes ğ‘“ğ‘  from the feature fusion, and begins the
process of spectrogram reconstruction to interactively train ğ¸ğ‘ and
ğ¸ğ‘’ for obtaining complementary feature pairs, i.e., the gradient of
the reconstruction loss propagates back to the encoder ğ¸ğ‘ and the
encoder ğ¸ğ‘’.
(cid:16)ğ¿ğ‘ğ‘‘ğ‘£
3.5 Objective Function
Learning the speaker identity representation involves multiple ob-
jectives that consist of the feature extraction loss ğ¿ğ‘, the adversarial
losses ğ¿ğ‘ğ‘‘ğ‘£
, as well as the speech reconstruction loss ğ¿ğ‘Ÿ .
Therefore, the overall objective function of SEEF-ALDR with a
weighted combination of them is as below:
and ğ¿ğ‘ğ‘‘ğ‘£
ğ‘’
ğ‘ 
ğ‘ 
ğ‘’
ğ‘
+ ğ¿ğ‘ğ‘‘ğ‘£
ğ¿ = ğœ†ğ‘ğ¿ğ‘ + ğœ†ğ‘ğ‘‘ğ‘£
for ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥), ğ¿ğ‘ğ‘‘ğ‘£
(8)
where ğœ†ğ‘, ğœ†ğ‘ğ‘‘ğ‘£ and ğœ†ğ‘Ÿ are weight parameters. ğ¿ğ‘ (ğ¿ğ‘†
ğ‘ for ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥
and ğ¿ğ´âˆ’ğ‘†
and ğ¿ğ‘Ÿ can be referred to in
Equations 2, 4, 5, 6, 7, respectively. To minimize the overall loss, the
stochastic gradient descent solver is used to solve Equation 8. Since
SEEF-ALDR is composed of several modules, in our experiments,
we provide an ablation study to evaluate the contribution of each
module.
, ğ¿ğ‘ğ‘‘ğ‘£
ğ‘’
ğ‘ 
(cid:17) + ğœ†ğ‘Ÿ ğ¿ğ‘Ÿ
4 EXPERIMENTS
In this section, we first elaborate on the experimental setup, and
then present the results of SEEF-ALDR on enhancing the speaker
verification of existing models. Finally, we conduct the ablation
study to evaluate the contribution of each module in SEEF-ALDR.
4.1 Experimental Setup
Datasets. Datasets Voxceleb1 [46] and Voxceleb2 [6] are large-
scale, text-independent speech databases collected from videos on
SEEF-ALDR: A Speaker Embedding Enhancement Framework via Adversarial Learning based Disentangled Representation
ACSAC 2020, December 7â€“11, 2020, Austin, USA
YouTube, and can be used for both speaker identification and verifi-
cation tasks. In our experiments, we only use audio files from them
for speaker verification task. Voxceleb1 contains 153,516 utterances
from 1,251 speakers and Voxceleb2 contains 1,128,246 utterances
from 6,112 speakers. Both of them are fairly gender-balanced, with
45% of female speakers for Voxceleb1 and 39% of female speakers
for Voxceleb2. The speakers span a wide range of races, profes-
sions, ages, emotions, accents, etc. The source video contained
in the dataset was recorded in quite diverse visual and auditory
environments, including interviews from the red carpet, outdoor
sports fields as well as quiet indoor studios, public speeches to
lots of audiences, etc. Therefore, these â€œin-the-wildâ€ speech sam-
ples contain a large amount of identity-unrelated information and
noise. Experiments on these samples help highlight the advance
of SEEF-ALDR in disentangled representation. Table 1 and Table
2 show the division of training and testing data of Voxceleb1 and
Voxceleb2. It is worth mentioning that there are no overlapping
identities between the training dataset of VoxCeleb2 and the overall
dataset VoxCeleb1. We train the proposed framework SEEF-ALDR
on datasets Voxceleb1 and Voxceleb2.
Network Architecture SEEF-ALDR consists of five components:
the speaker purifying encoder ğ¸ğ‘, the speaker eliminating encoder
ğ¸ğ‘’, the speaker classifier ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ , the adversarial classifier ğ¶ğ‘ğ‘‘ğ‘£,
and the reconstruction decoder ğ·ğ‘Ÿ . The network architecture of
the speaker purifying encoder ğ¸ğ‘ in SEEF-ALDR depends on the
structure of to-be-integrated speaker verification models, such as
VGGnet, Resnet34, Resnet50, etc. We intend to set the architecture of
the speaker eliminating encoder ğ¸ğ‘’ the same as ğ¸ğ‘ to make it easier
to train the entire framework, though it can be totally different. For
instance, the backbone of ğ¸ğ‘ and ğ¸ğ‘’ is based on ResNet-34 when
we port [16] (using ResNet-34) into our framework, appended with
the global temporal pool (TAP) layer to embed variable-length
input speech into the fixed-length speaker feature. Furthermore, to
reproduce the baseline models, we introduce another self-attentive
pooling (SAP) layer based on [47]. There is one fully connected
layer in ğ¶ğ‘ ğ‘ğ‘’ğ‘ğ‘˜ğ‘’ğ‘Ÿ , and three convolutional layers as well as three
fully connected layers included in ğ¶ğ‘ğ‘‘ğ‘£. A simple implementation of
ğ·ğ‘Ÿ is composed of three fully connected layers and ten fractionally-
strided convolution layers [53] interlaced with batch normalization
layers to obtain the reconstructed spectrogram. To further ensure
the efficiency and stability of the training process, the decoder is
also consistent with the corresponding encoder to do upsampling
by using deconvolution (or called transposed convolution).
Initialization SEEF-ALDR is trained on a Linux server with i7-
8700K CPU, 32GB memory, and three NVIDIA Titan V GPUS con-
nected in an end-to-end manner. During pre-processing, spectro-
gram of all the input speech samples are extracted in a sliding win-
dow fashion using a hamming window [6, 46] with ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ = 25ğ‘šğ‘ 
and ğ‘ ğ‘¡ğ‘’ğ‘ = 10ğ‘šğ‘ , and normalized to a standardized variable (with
mean of 0 and standard deviation of 1). Since the duration of the
speech samples is different, we randomly choose a three-second
temporal segments from each spectrogram to ensure that the size
of the training samples is consistent. The batch size of the input
speech is set as 64 and the model is trained through SGD optimizer
with ğ‘šğ‘œğ‘šğ‘’ğ‘›ğ‘¡ğ‘¢ğ‘š = 0.9 and ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡_ğ‘‘ğ‘’ğ‘ğ‘ğ‘¦ = 5ğ‘’ âˆ’ 4. The initial
learning rate is set as 10âˆ’2, and reduced by 10% per cycle of the
previous learning rate until 10âˆ’6. When ğ´ âˆ’ ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ is used as
the loss function of the speaker classifier, the angular margin fac-
tor is set as ğ‘š = 4, and the weight factor is set as ğœ†ğ‘ğ‘œğ‘  = 5. The
weight parameters in the training process are set as ğœ†ğ‘ = 1 for ğ¿ğ‘,
ğœ†ğ‘Ÿ = 0.02 for ğ¿ğ‘Ÿ , and ğœ†ğ‘ğ‘‘ğ‘£ = 0.1 for ğ¿ğ‘ğ‘‘ğ‘£
respectively.
The training balance between the eliminating encoder and the ad-
versarial classifier needs to be adjusted empirically to improve the
accuracy of feature decoupling.
and ğ¿ğ‘ğ‘‘ğ‘£
ğ‘ 
ğ‘’
4.2 Performance of SEEF-ALDR
We selected several state-of-the-art speaker verification models
as representatives to port into SEEF-ALDR and evaluate the effec-
tiveness of SEEF-ALDR [1, 4, 6, 14, 32, 46, 68] (the baseline per-
formance). When reproducing those models, we ensure that the
model structure, loss function, test dataset, and similarity metric are
consistent with those in the original paper. After porting them into
SEEF-ALDR, we retrain SEEF-ALDR on Voxceleb1 and Voxceleb2
respectively. We choose two metrics: the detection cost function
(ğ¶ğ‘‘ğ‘’ğ‘¡) [13] and the Equal Error Rate (EER) [61], to evaluate the
performance of SEEF-ALDR on the speaker verification task. ğ¶ğ‘‘ğ‘’ğ‘¡
can be computed as below:
ğ¶ğ‘‘ğ‘’ğ‘¡ = ğ¶ğ‘šğ‘–ğ‘ ğ‘ ğ‘ƒğ‘šğ‘–ğ‘ ğ‘  Â· ğ‘ƒğ‘¡ğ‘ğ‘Ÿ + ğ¶ğ‘“ ğ‘ğ‘ƒğ‘“ ğ‘ Â· (1 âˆ’ ğ‘ƒğ‘¡ğ‘ğ‘Ÿ)
(9)
where ğ‘ƒğ‘šğ‘–ğ‘ ğ‘  is the probability of the miss and ğ‘ƒğ‘“ ğ‘ is the probability
of the false alarm. The prior target probability ğ‘ƒğ‘¡ğ‘ğ‘Ÿ is set as 0.01, and
both the cost of a miss ğ¶ğ‘šğ‘–ğ‘ ğ‘  and the cost of a false alarm ğ¶ğ‘“ ğ‘ have
equal weight parameter of 1.0. We demonstrate the effectiveness of
SEEF-ALDR by showing the improvement of EER after porting the
baseline into SEEF-ALDR, i.e., the percentage of reduction in EER.
We first utilize the training dataset and testing dataset from
Voxceleb1 for speaker verification. To calculate speaker similarity
for the verification task, we choose the common cosine distance
as the similarity matrix. In addition to the cosine distance, proba-
bilistic linear discriminant analysis (PLDA) [52] is adopted in the
state-of-the-art speaker verification system [1, 4] to measure the
distance between two speaker embeddings. As shown in Table 3,
the experimental results demonstrate that SEEF-ALDR can signifi-
cantly improve the performance of each baseline, with an average
improvement of 20.6% on EER.
The best EER improvement in Table 3 comes from [14] with
ResNet-20 as the baseline model, all above 30%. In contrast, also
using ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥, TAP and Cosine, 16.7% improvement over VGG-M
[46] is observed. The reason for such a significant difference is that
the network structure of VGG-M is much simpler, compared with
that of ResNet-20, so the potential for performance improvement is
also limited. Overall, the lowest two EER improvements are 12.6%
[1] and 13.4% [14], where ğ´âˆ’ğ‘†ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ or PLDA are used to replace
ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ or Cosine accordingly in their baseline models. The possi-
ble reason for such low improvement compared with others might
be that those models have gone through task-specific optimization,