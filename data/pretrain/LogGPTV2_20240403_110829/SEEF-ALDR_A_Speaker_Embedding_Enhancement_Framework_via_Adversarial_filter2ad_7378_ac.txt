identity-unrelated features 𝑓𝑒 = 𝐸𝑒 (𝑆). The objective function 𝐿𝑎𝑑𝑣
of 𝐶𝑎𝑑𝑣 directs speaker identification based on feature 𝑓𝑒, and is
constrained through the cross entropy loss as below:
𝑠
(cid:17)
(cid:16)𝑦 𝑗
𝑒
𝐿𝑎𝑑𝑣
𝑠
= −
𝑡 𝑗
𝑠 log
where 𝑡𝑠 means the ground truth of speakers’ identity. Note that the
gradient of 𝐿𝑎𝑑𝑣
only propagates back to 𝐶𝑎𝑑𝑣, without updating
𝑠
any layer of 𝐸𝑒.
The eliminating encoder 𝐸𝑒 should be trained to fool 𝐶𝑎𝑑𝑣, so
the speaker identity distribution 𝑢𝑠 is set as a constant for each
in the cross-entropy loss of 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥.
speaker label, equal to 1
𝑁𝑠
The objective function 𝐿𝑎𝑑𝑣
of 𝐸𝑒 can be written as follow:
𝑒
𝑁𝑠∑︁
𝑗=1
(cid:17)
(cid:16)𝑦 𝑗
𝑒
𝐿𝑎𝑑𝑣
𝑒
=
𝑢 𝑗
𝑠 log
=
1
𝑁𝑠
log
(cid:16)𝑦 𝑗
𝑒
(cid:17) .
𝑁𝑠∑︁
𝑗=1
𝑁𝑠∑︁
𝑗=1
𝑒
𝑒
The gradient of 𝐿𝑎𝑑𝑣
only propagates back to 𝐸𝑒, rather than
𝐶𝑎𝑑𝑣. If we allow the gradient of 𝐿𝑎𝑑𝑣
to update 𝐶𝑎𝑑𝑣, the encoder
𝐸𝑒 can easily cheat 𝐶𝑎𝑑𝑣, e.g, by only revising the classifier 𝐶𝑎𝑑𝑣 to
produce non-informative output. Hence, the encoder 𝐸𝑒 cannot en-
sure that feature 𝑓𝑒 will extract the identity-unrelated information
under these circumstances. Therefore, by combining both 𝐿𝑎𝑑𝑣
and
, our framework can leverage the advantages of each of them
𝐿𝑎𝑑𝑣
𝑠
and be coordinated to work together towards the identity-unrelated
feature through disentangled speaker information. Benefiting from
the advantages of adversarial learning, the speaker eliminating
encoder 𝐸𝑒 can learn irrelevant features as accurately as possi-
ble, which in turn guarantees the correctness of speaker identity
features.
𝑒
3.4 Reconstruction Decoder
Ideally, the combination of the decoupled features 𝑓𝑝 and the identity-
unrelated features 𝑓𝑒 should be exactly the representation of the
input spectrogram 𝑆. Hence, defining the feature fusion module
 based on the concatenating operation, we fuse 𝑓𝑝 and 𝑓𝑒 to-
gether into a complete feature 𝑓𝑠 = 𝑓𝑝 𝑓𝑒, and make the decoder
𝐷𝑟 reconstruct the spectrogram 𝑆′ = 𝐷𝑟 (𝑓𝑠). To simply measure
the difference between the reconstructed spectrogram 𝑆′ and the
original spectrogram 𝑆, we utilize 𝑙2 distance2 to define the recon-
struction loss 𝐿𝑟 as below:
(cid:13)(cid:13)(cid:13)𝐷𝑟
(cid:16)𝑓𝑝
 𝑓𝑒
(cid:17) − 𝑆
(cid:13)(cid:13)(cid:13)2
2
𝐿𝑟 =
1
2
.
(7)
2Since the input spectrogram is in the form of a 2-dimensional matrix, 𝑙2 distance
is a commonly used measure to compare the similarity between two 2-dimensional
matrices.
(5)
(6)
Table 1: Training and Testing Dataset on Voxceleb1.
Dataset
#POIs
#Utterances
Training Testing
1211
148,642
40
4,874
Total
1251
153,516
Table 2: Training and Testing Dataset on Voxceleb2.
Dataset
#POIs
#Utterances
Training Testing
5994
1,092,009
118
36,237
Total
6112
1,128,246
As mentioned above, the adversarial supervision signal encour-
ages the eliminating encoder 𝐸𝑒 to extract identity-unrelated fea-
tures. In contrast, the reconstruction loss guides the purifying en-
coder 𝐸𝑝 to embed the remaining identity-purified features by the
fidelity of the spectrogram reconstruction, i.e., ensuring 𝑓𝑠 con-
tains the complete representation from the original spectrogram
𝑆. In general, the purifying encoder 𝐸𝑝 needs to be trained first
to reach a certain level of accuracy in the task of speaker verifica-
tion. Then the 𝐸𝑒 initiates its networks by inheriting the weights
from 𝐸𝑝 and begins the adversarial learning. Meanwhile, the recon-
struction decoder takes 𝑓𝑠 from the feature fusion, and begins the
process of spectrogram reconstruction to interactively train 𝐸𝑝 and
𝐸𝑒 for obtaining complementary feature pairs, i.e., the gradient of
the reconstruction loss propagates back to the encoder 𝐸𝑝 and the
encoder 𝐸𝑒.
(cid:16)𝐿𝑎𝑑𝑣
3.5 Objective Function
Learning the speaker identity representation involves multiple ob-
jectives that consist of the feature extraction loss 𝐿𝑝, the adversarial
losses 𝐿𝑎𝑑𝑣
, as well as the speech reconstruction loss 𝐿𝑟 .
Therefore, the overall objective function of SEEF-ALDR with a
weighted combination of them is as below:
and 𝐿𝑎𝑑𝑣
𝑒
𝑠
𝑠
𝑒
𝑝
+ 𝐿𝑎𝑑𝑣
𝐿 = 𝜆𝑝𝐿𝑝 + 𝜆𝑎𝑑𝑣
for 𝐴 − 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥), 𝐿𝑎𝑑𝑣
(8)
where 𝜆𝑝, 𝜆𝑎𝑑𝑣 and 𝜆𝑟 are weight parameters. 𝐿𝑝 (𝐿𝑆
𝑝 for 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥
and 𝐿𝐴−𝑆
and 𝐿𝑟 can be referred to in
Equations 2, 4, 5, 6, 7, respectively. To minimize the overall loss, the
stochastic gradient descent solver is used to solve Equation 8. Since
SEEF-ALDR is composed of several modules, in our experiments,
we provide an ablation study to evaluate the contribution of each
module.
, 𝐿𝑎𝑑𝑣
𝑒
𝑠
(cid:17) + 𝜆𝑟 𝐿𝑟
4 EXPERIMENTS
In this section, we first elaborate on the experimental setup, and
then present the results of SEEF-ALDR on enhancing the speaker
verification of existing models. Finally, we conduct the ablation
study to evaluate the contribution of each module in SEEF-ALDR.
4.1 Experimental Setup
Datasets. Datasets Voxceleb1 [46] and Voxceleb2 [6] are large-
scale, text-independent speech databases collected from videos on
SEEF-ALDR: A Speaker Embedding Enhancement Framework via Adversarial Learning based Disentangled Representation
ACSAC 2020, December 7–11, 2020, Austin, USA
YouTube, and can be used for both speaker identification and verifi-
cation tasks. In our experiments, we only use audio files from them
for speaker verification task. Voxceleb1 contains 153,516 utterances
from 1,251 speakers and Voxceleb2 contains 1,128,246 utterances
from 6,112 speakers. Both of them are fairly gender-balanced, with
45% of female speakers for Voxceleb1 and 39% of female speakers
for Voxceleb2. The speakers span a wide range of races, profes-
sions, ages, emotions, accents, etc. The source video contained
in the dataset was recorded in quite diverse visual and auditory
environments, including interviews from the red carpet, outdoor
sports fields as well as quiet indoor studios, public speeches to
lots of audiences, etc. Therefore, these “in-the-wild” speech sam-
ples contain a large amount of identity-unrelated information and
noise. Experiments on these samples help highlight the advance
of SEEF-ALDR in disentangled representation. Table 1 and Table
2 show the division of training and testing data of Voxceleb1 and
Voxceleb2. It is worth mentioning that there are no overlapping
identities between the training dataset of VoxCeleb2 and the overall
dataset VoxCeleb1. We train the proposed framework SEEF-ALDR
on datasets Voxceleb1 and Voxceleb2.
Network Architecture SEEF-ALDR consists of five components:
the speaker purifying encoder 𝐸𝑝, the speaker eliminating encoder
𝐸𝑒, the speaker classifier 𝐶𝑠𝑝𝑒𝑎𝑘𝑒𝑟 , the adversarial classifier 𝐶𝑎𝑑𝑣,
and the reconstruction decoder 𝐷𝑟 . The network architecture of
the speaker purifying encoder 𝐸𝑝 in SEEF-ALDR depends on the
structure of to-be-integrated speaker verification models, such as
VGGnet, Resnet34, Resnet50, etc. We intend to set the architecture of
the speaker eliminating encoder 𝐸𝑒 the same as 𝐸𝑝 to make it easier
to train the entire framework, though it can be totally different. For
instance, the backbone of 𝐸𝑝 and 𝐸𝑒 is based on ResNet-34 when
we port [16] (using ResNet-34) into our framework, appended with
the global temporal pool (TAP) layer to embed variable-length
input speech into the fixed-length speaker feature. Furthermore, to
reproduce the baseline models, we introduce another self-attentive
pooling (SAP) layer based on [47]. There is one fully connected
layer in 𝐶𝑠𝑝𝑒𝑎𝑘𝑒𝑟 , and three convolutional layers as well as three
fully connected layers included in 𝐶𝑎𝑑𝑣. A simple implementation of
𝐷𝑟 is composed of three fully connected layers and ten fractionally-
strided convolution layers [53] interlaced with batch normalization
layers to obtain the reconstructed spectrogram. To further ensure
the efficiency and stability of the training process, the decoder is
also consistent with the corresponding encoder to do upsampling
by using deconvolution (or called transposed convolution).
Initialization SEEF-ALDR is trained on a Linux server with i7-
8700K CPU, 32GB memory, and three NVIDIA Titan V GPUS con-
nected in an end-to-end manner. During pre-processing, spectro-
gram of all the input speech samples are extracted in a sliding win-
dow fashion using a hamming window [6, 46] with 𝑤𝑖𝑑𝑡ℎ = 25𝑚𝑠
and 𝑠𝑡𝑒𝑝 = 10𝑚𝑠, and normalized to a standardized variable (with
mean of 0 and standard deviation of 1). Since the duration of the
speech samples is different, we randomly choose a three-second
temporal segments from each spectrogram to ensure that the size
of the training samples is consistent. The batch size of the input
speech is set as 64 and the model is trained through SGD optimizer
with 𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚 = 0.9 and 𝑤𝑒𝑖𝑔ℎ𝑡_𝑑𝑒𝑐𝑎𝑦 = 5𝑒 − 4. The initial
learning rate is set as 10−2, and reduced by 10% per cycle of the
previous learning rate until 10−6. When 𝐴 − 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 is used as
the loss function of the speaker classifier, the angular margin fac-
tor is set as 𝑚 = 4, and the weight factor is set as 𝜆𝑐𝑜𝑠 = 5. The
weight parameters in the training process are set as 𝜆𝑝 = 1 for 𝐿𝑝,
𝜆𝑟 = 0.02 for 𝐿𝑟 , and 𝜆𝑎𝑑𝑣 = 0.1 for 𝐿𝑎𝑑𝑣
respectively.
The training balance between the eliminating encoder and the ad-
versarial classifier needs to be adjusted empirically to improve the
accuracy of feature decoupling.
and 𝐿𝑎𝑑𝑣
𝑠
𝑒
4.2 Performance of SEEF-ALDR
We selected several state-of-the-art speaker verification models
as representatives to port into SEEF-ALDR and evaluate the effec-
tiveness of SEEF-ALDR [1, 4, 6, 14, 32, 46, 68] (the baseline per-
formance). When reproducing those models, we ensure that the
model structure, loss function, test dataset, and similarity metric are
consistent with those in the original paper. After porting them into
SEEF-ALDR, we retrain SEEF-ALDR on Voxceleb1 and Voxceleb2
respectively. We choose two metrics: the detection cost function
(𝐶𝑑𝑒𝑡) [13] and the Equal Error Rate (EER) [61], to evaluate the
performance of SEEF-ALDR on the speaker verification task. 𝐶𝑑𝑒𝑡
can be computed as below:
𝐶𝑑𝑒𝑡 = 𝐶𝑚𝑖𝑠𝑠𝑃𝑚𝑖𝑠𝑠 · 𝑃𝑡𝑎𝑟 + 𝐶𝑓 𝑎𝑃𝑓 𝑎 · (1 − 𝑃𝑡𝑎𝑟)
(9)
where 𝑃𝑚𝑖𝑠𝑠 is the probability of the miss and 𝑃𝑓 𝑎 is the probability
of the false alarm. The prior target probability 𝑃𝑡𝑎𝑟 is set as 0.01, and
both the cost of a miss 𝐶𝑚𝑖𝑠𝑠 and the cost of a false alarm 𝐶𝑓 𝑎 have
equal weight parameter of 1.0. We demonstrate the effectiveness of
SEEF-ALDR by showing the improvement of EER after porting the
baseline into SEEF-ALDR, i.e., the percentage of reduction in EER.
We first utilize the training dataset and testing dataset from
Voxceleb1 for speaker verification. To calculate speaker similarity
for the verification task, we choose the common cosine distance
as the similarity matrix. In addition to the cosine distance, proba-
bilistic linear discriminant analysis (PLDA) [52] is adopted in the
state-of-the-art speaker verification system [1, 4] to measure the
distance between two speaker embeddings. As shown in Table 3,
the experimental results demonstrate that SEEF-ALDR can signifi-
cantly improve the performance of each baseline, with an average
improvement of 20.6% on EER.
The best EER improvement in Table 3 comes from [14] with
ResNet-20 as the baseline model, all above 30%. In contrast, also
using 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥, TAP and Cosine, 16.7% improvement over VGG-M
[46] is observed. The reason for such a significant difference is that
the network structure of VGG-M is much simpler, compared with
that of ResNet-20, so the potential for performance improvement is
also limited. Overall, the lowest two EER improvements are 12.6%
[1] and 13.4% [14], where 𝐴−𝑆𝑜 𝑓 𝑡𝑚𝑎𝑥 or PLDA are used to replace
𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 or Cosine accordingly in their baseline models. The possi-
ble reason for such low improvement compared with others might
be that those models have gone through task-specific optimization,