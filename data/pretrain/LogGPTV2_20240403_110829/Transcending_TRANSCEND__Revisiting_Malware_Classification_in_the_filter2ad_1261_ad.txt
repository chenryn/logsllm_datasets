Kullback-Leibler (KL) divergence [24]—an unbounded mea-
sure of distribution difference—of 696.66. The covariate shift
in our dataset is much more subtle and natural over time, with
an average KL divergence of 189.55 between each training
and test partition. From this we conclude that the distributions
were signiﬁcantly more different in the original evaluation than
would be expected in naturally occurring concept drift, which
would have made it easier to detect drifting examples.
Classiﬁer. For the underlying classiﬁer, we use Drebin [6]
which has been shown to achieve state-of-the-art performance
if a retraining strategy is used to remediate concept drift [35].
Due to this, we hypothesize that if Transcend [20] is used
to reject drifting points, Drebin will be able to classify the
remaining points with high accuracy. Drebin uses a linear
SVM and a binary feature space where components (activities,
permissions, URLs, etc) are represented as present or absent.
Calibration. To optimize the thresholding, we maximize the
F1 of all kept elements for a rejection rate less than 15%.
These metrics are computed in aggregate for each time period
of the temporal evaluation. On our dataset, this would amount
to an average rejection of ∼20 samples a day, well below the
estimated labeling capacity of 80 a day suggested by Miller
et al. [27]. However, we note that these constraints may need
to be adjusted according to speciﬁc operational requirements,
for example, it may be more appropriate to minimize the
rejection rate while sacriﬁcing F1 for kept elements. For the
random search we use 100,000 random iterations with early
stopping after 3,000 consecutive events without improvement.
For approx-TCE and CCE we calibrate using k = 10 folds.
B. Novel Conformal Evaluators
Here we compare the novel conformal evaluators of TRAN-
SCENDENT. As vanilla TCE is not feasible for this experiment
setting due to the size of the training set (§IV), we use approx-
TCE as a stand-in, however we provide a minimal experiment
in Appendix C to show that the expected performance differ-
ence between vanilla TCE and our evaluators is negligible.
Performance Metrics. Figure 7 shows the the F1, Preci-
sion, and Recall (rows 1–3) for each of the novel evaluators
(columns). The middle dashed line shows the baseline perfor-
mance when no rejection is enforced. This is the performance
decay caused by concept drift present in the dataset that results
from an evolving malicious class. Note that classiﬁers degrade
rapidly, becoming worse than random in under one year.
The upper solid line shows the performance of kept ele-
ments, those with test p-values that fall above the threshold of
their predicted classes. While decay is still present, approx-
TCE and ICE are able to maintain F1 > 0.8 for two years,
doubling the lifespan of the model. Note that the sudden drop
in performance of the last three months is likely an artifact of
the fewer examples crawled by AndroZoo in those months.
The lower solid line shows the performance of rejected
elements. Low metrics mean the rejected elements would have
been incorrectly classiﬁed by the underlying classiﬁer and
were rightfully rejected, while high metrics means rejections
were erroneous. Approx-TCE and ICE have F1, Precision, and
Recall of 0 for rejected elements for every test month meaning
that all rejected elements would have been misclassiﬁed.
The result of CCE differs in that it is less conservative in its
rejections. The performance of kept elements is much higher,
but also slightly higher for rejected elements, indicating that
a small proportion of rejected elements would have actually
been correctly classiﬁed. We observe that this conservatism
can be increased or decreased by modifying the conditions of
the majority vote. If more folds are required to agree before
a decision is accepted, the CCE will be more conservative,
rejecting more elements. If less folds are required, more
elements will be accepted. In this respect, CCE offers an
alternative dimension of tuning in addition to the threshold
optimization. Additionally, this is parameter can be altered
during a deployment, rather than being set at calibration. This
allows for some adaptability, such as when the cost of False
Negatives is very high (e.g., not alerting security teams to
attacks in network intrusion detection), or when the cost of
False Positives is very high (e.g., withholding benign emails
in spam detection, or disabling legitimate user accounts in fake
account detection). A further empirical analysis of the effect
of the majority vote conditions is included in Appendix D.
Rejection Rates. Gray bars show the proportion of rejected
test elements. In each case, rejections begin close to the rate
used for calibration before slowly rising each year, averaging
26.45 samples per day. While rejection rates may appear
high, these are symptomatic of rising concept drift and de-
teriorating performance in the underlying classiﬁer and are
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
813
(a) F1-Score, Approx-TCE w/ credibility
(b) F1-Score, ICE w/ credibility
(c) F1-Score, CCE w/ credibility
(d) Precision, Approx-TCE w/ credibility
(e) Precision, ICE w/ credibility
(f) Precision, CCE w/ credibility
(g) Recall, Approx-TCE w/ credibility
(h) Recall, ICE w/ credibility
(i) Recall, CCE w/ credibility
(j) F1-Score, Approx TCE w/ cred + conf
(k) F1-Score, ICE w/ cred + conf
(l) F1-Score, CCE w/ cred + conf
(m) F1-Score, Approx-TCE w/ probabilities
(n) F1-Score, ICE w/ probabilities
(o) F1-Score, CCE w/ probabilities
Fig. 7: Performance for the three proposed conformal evaluators (columns) using different quality metrics. The ﬁrst three rows show F1-Score, Precision,
and Recall, respectively, of the different evaluators using credibility p-values. The lower two rows show F1-Score using a combination of credibility and
conﬁdence p-vaules, and probabilities, respectively. The dashed line shows the performance with no rejection mechanism. The upper line ((cid:3) marker) shows
the performance on kept examples whose classiﬁcations were accepted. The lower line ((cid:35)marker) shows the performance on rejected examples. These are
the mistakes that would have been made if the predictions were accepted by the degrading model. The bars show the proportion of rejected elements in each
period, while the x and o markers show the proportion of ground truth malware and goodware that was identiﬁed as drifting and quarantined, respectively.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
814
0102030400.00.20.40.60.81.0Metric (baseline)Metric (kept)Metric (rejected)Rate of drifting malwareRate of drifting goodwareQuarantined14710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.014710131619222528313437404346Testing period (month)0.00.10.20.30.40.50.60.70.80.91.0often preferable to taking incorrect actions on False Positives
and False Negatives. In an extreme case where a classiﬁer
always predicts the incorrect label, rejection rates could reach
100% but the F1 of rejected elements would be 0%. The
gray markers show the proportion of ground truth malware
and goodware that are rejected each period, illustrating the
evaluators’ perception of drift in that class. Strikingly, for our
evaluators the drift rate of the malicious class is inversely
correlated to the performance loss in the baseline, while
the drift rate for goodware is relatively stable. This supports
our hypothesis that performance decay is mostly driven by
evolution in the malicious class. We reiterate that in the case
of Approx-TCE and ICE, the low F1 of rejected elements
indicates that all of the rejected malware would have evaded
the classiﬁer if they had not been identiﬁed as drifting.
Runtime. The runtime of the conformal evaluators during this
experiment match what would be expected from their relative
complexities (cf. Table I). The ICE is the quickest at 11.5 CPU
hours. The CCE took 35.6 CPU hours, but our implementation
is parallelized resulting in a wall-clock time identical to the
ICE. The Approx-TCE took 46.1 CPU hours. As discussed,
vanilla TCE was computationally infeasible, but we estimate a
runtime of 1.9 CPU years, considering that the time required
to ﬁt the underlying classiﬁer once is ∼10 minutes and the
classiﬁer must be trained once for each training example.
We conclude that the ICE is the most useful for settings
where resources are limited or models with a rapid iteration
cycle (e.g., daily), while the CCE offers greater conﬁdence
and ﬂexibility at a slightly higher computational cost.
C. Credibility, Conﬁdence, and Probabilities
Here we compare the performance under different quality
metrics. The exact performance over time for all settings
discussed in this subsection is reported in Table II.
Credibility with Conﬁdence.
Intuitively, including conﬁ-
dence thresholds when evaluating a classiﬁer prediction would
be beneﬁcial as conﬁdence represents how certain the classiﬁer
is in its own prediction. However, as credibility is the main
indicator that i.i.d. has been violated, and thus that concept
drift is occurring, it is unclear what further gain conﬁdence
could provide. Here we test this by evaluating the conformal
evaluators under the same conditions as §VI-B, using per-class
thresholds for both credibility and conﬁdence.
Figure 7 compares the F1 for each conformal evaluator
(columns) using different thresholding metrics (rows 1, 4–5).
The upper blue line shows the performance of kept elements
while the lower red line shows the performance of rejected el-
ements. The gray dashed line depicts the baseline performance
when no rejection mechanism is used.
The penultimate row shows the F1 when conﬁdence is in-
cluded. Performance for the approx-TCE and CCE is relatively
unchanged, but is markedly improved for the ICE with degra-
dation postponed much longer than before. The conﬁdence
appears to restore some of the statistical information lost by
using only a small amount of the training data for calibration.
However, the computation required to ﬁnd thresholds is
higher than with credibility only—equivalent to doubling the
number of classes. We conclude that the performance gain
from including conﬁdence is situationally dependent; although
it will improve the accuracy of an ICE, a CCE will often
provide the same accuracy with comparable calibration time.
Probabilities. The ﬁnal row of Figure 7 shows the F1 when
the classiﬁers’ output probabilities are used for thresholding,
rather than generating per-class p-values for each calibration
and test point. For each evaluator,
the same training and
calibration split is used as with p-values, to ensure a fair
comparison. The plot shows probabilities alone offer a very
small improvement for kept elements over the baseline in the
ﬁrst year, becoming increasingly volatile as the concept drift
becomes more severe. Additionally, the perceived drift rate for
each class has no relation to the baseline performance loss,
indicating that the root cause of the drift is not identiﬁed.
This shows the statistical support offered by the conformal
evaluator’s p-value computation is signiﬁcant and justiﬁes the
additional computational overhead that it induces.
D. Full Grid Search vs Random Search
Here we evaluate our random search implementation (§V-D)
compared to the full grid search used in the original pro-
posal [20]. We show the random search can ﬁnd high quality
calibration thresholds more efﬁciently than the full search.
Due to the full grid search cost, here we train and calibrate
on 1 month of data and test on the remaining 59 months using
an approx-TCE with 10 folds. We maximize F1 for an accept-
able rejection rate of less than 15%. To ensure the baseline
discovers high quality thresholds we use a ﬁne granularity grid
covering 1,317,520 combinations of thresholds. For random
search we set an upper limit of 10,000 trials.
Table III compares the performance without rejection, with
rejection thresholds from the full grid search, and with re-
jection thresholds from random search. Note there is no
signiﬁcant performance difference between the two strategies,
but the random search is able to cover the same search space
with two orders of magnitude fewer trials. We observe that
the full grid search makes erroneous assumptions on the
distribution of quality thresholds which the random search
does not. Additionally, while the random search allows for a
variety of stopping conditions, the only mechanism to control
the length of the full grid search is the size of the interval
to search and the granularity of the search steps—which are
difﬁcult to choose beforehand.
E. Comparison to Prior Approaches
To provide further context on the performance of TRAN-
SCENDENT, we compare against two closely related state-of-
the-art approaches: Linusson et al. [26] (which we denote
CP-Reject) and DroidEvolver [50].
CP-Reject [26]. The ﬁrst approach is a recent method for
performing rejection using conformal prediction. For a given
test set and hyperparameter k, CP-Reject aims to output the
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:16 UTC from IEEE Xplore.  Restrictions apply. 
815
TABLE II: Area Under Time (AUT) of F1 performance with respect to concept drift over the 48 month test period for different quality metrics: credibility,
credibility with conﬁdence, and probabilities (cf. Figure 7). We aim to maximize the metrics of kept elements and minimize the metrics for rejected elements.
Approx-TCE
Baseline
Kept Elements
Rejected Elements
AUT(F1 w/ credibility, 48m)
AUT(F1 w/ cred + conf, 48m)
AUT(F1 w/ probability, 48m)
AUT(F1 w/ credibility, 48m)
AUT(F1 w/ cred + conf, 48m)
AUT(F1 w/ probability, 48m)
AUT(F1 w/ credibility, 48m)
AUT(F1 w/ cred + conf, 48m)
AUT(F1 w/ probability, 48m)
.480
.480
.456
.829
.822
.531
.000
.000
.410
ICE
.440
.440
.405
.762
.887
.388
.000
.000
.426
CCE
.483
.483
.455
.950
.962
.532
.064
.063
.410
that
includes a rejection component,
largest possible set of predictions containing on average no
more than k errors, while rejecting test objects for which it
is too uncertain. The training and calibration dataset splits
are the same as we use for the ICE setting; however while
TRANSCENDENT makes decisions on individual test objects
as they appear, CP-Reject operates a posteriori on a batch of
test inputs and predictions. Given this advantage, to ensure
a fair comparison we test on each month with k set to the
85th percentile which ensures a rejection rate of 15%—the
same rejection rate TRANSCENDENT is calibrated for. The
underlying classiﬁer is a random forest classiﬁer with 100 trees
and the conformal prediction NCM is the maximum margin
between the output probability for the predicted class and the
output probabilities for all other classes.
DroidEvolver [50].
The second approach is a state-of-
the-art Android malware detector designed for drift adap-
tation, but
in which
the drift identiﬁcation mechanism is inspired by the original
Transcend [20]. DroidEvolver is built on an ensemble of ﬁve
linear online learners, with a weighted sum as the ensemble
decision function. For each new test object a juvenilization
indicator (JI) score is computed per model as the proportion
of apps in a ﬁxed-size buffer of previously encountered apps,
of the same class, that have decision scores greater than the
new object. An object is marked as drifting when the JI score
falls outside of a precalibrated range and the corresponding
decisions are rejected, i.e., excluded from the weighted sum
which is used to pseudo-label and update with the drifting
point. The ongoing performance of the system relies on the
quality of the pseudo-labels and thus indirectly on the quality
of the drift identiﬁcation. The JI scores are very similar to
the credibility p-values from conformal evaluation, with the
computational complexity of full TCE being addressed by
using the small ﬁxed-size app buffer: drift identiﬁcation should
be effective so long as the app buffer is representative of
the overall data population. Due to this relationship, it is
informative to compare against TRANSCENDENT.
Results. Figure 8 shows the F1 performance of CP-Reject
and DroidEvolver trained and calibrated on the ﬁrst year of
the dataset and tested on the two subsequent years at monthly
intervals. This can both be compared to the ﬁrst 24 months of
ICE and CCE results of Figures 7b and 7c. For CP-Reject, the
(a) CP-Reject [26]
(b) DroidEvolver [50]
Fig. 8: F1-Score over time for two prior approaches with mechanisms similar
to TRANSCENDENT (cf. Figures 7b and 7c).
similar F1 performance for kept, rejected, and baseline predic-
tions indicate that it is unable to distinguish between drifting
and non-drifting points. Although it may effectively reject
low-quality predictions in a stationary environment, conformal