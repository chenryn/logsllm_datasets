(cid:22)(cid:28)(cid:31)(cid:1)(cid:29)(cid:15)(cid:33)(cid:1)
(cid:18)(cid:15)(cid:31)(cid:15)(cid:1)(cid:30)(cid:31)(cid:28)(cid:29)(cid:19)
(cid:2)(cid:3)(cid:37),(cid:26)(cid:4) (cid:30)(cid:16)(cid:24)(cid:31)(cid:32)(cid:3)
(cid:31)(cid:23)(cid:26)(cid:19)
(cid:36)(cid:24)(cid:28)(cid:19)(cid:29)(cid:36)(cid:1)
(cid:27)
(cid:36)(cid:24)(cid:28)(cid:19)(cid:29)(cid:36)(cid:1)
(cid:9)
(cid:36)(cid:24)(cid:28)(cid:19)(cid:29)(cid:36)(cid:1)
(cid:8)
(cid:36)(cid:24)(cid:28)(cid:19)(cid:29)(cid:36)(cid:1)(cid:7)(cid:1)
(cid:2)(cid:23)(cid:29)(cid:33)(cid:3)
(cid:18)(cid:10)(cid:20)(cid:1)(cid:39)(cid:19)(cid:3)(cid:4)(cid:5)(cid:40)(cid:10)(cid:25)(cid:13)(cid:16)(cid:15)(cid:1)(cid:26)(cid:16)(cid:5)(cid:23)(cid:10)(cid:25)(cid:30)
(cid:3) (cid:1)(cid:22) (cid:28) (cid:8)(cid:4) (cid:30)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:3)(cid:1)(cid:4)(cid:5)(cid:6)(cid:7)(cid:6)(cid:5)(cid:6)(cid:8)(cid:9)(cid:6)(cid:10)(cid:3)(cid:1)(cid:11)(cid:6)(cid:8)(cid:12)(cid:6)(cid:5)(cid:3)(cid:1)(cid:13)(cid:11)(cid:6)(cid:3)(cid:1)(cid:14)(cid:14)(cid:1)(cid:15)(cid:10)(cid:6)(cid:5)(cid:1)(cid:7)(cid:6)(cid:13)(cid:16)(cid:15)(cid:5)(cid:6)(cid:10)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:5)(cid:8)(cid:9)(cid:10)(cid:3)(cid:11)(cid:3)(cid:1)(cid:4)(cid:13)(cid:11)(cid:6)(cid:17)(cid:6)(cid:18)(cid:19)(cid:20)(cid:5)(cid:12)(cid:10)(cid:3)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:14)(cid:14)(cid:1)(cid:9)(cid:20)(cid:8)(cid:16)(cid:6)(cid:21)(cid:16)(cid:1)(cid:7)(cid:6)(cid:13)(cid:16)(cid:15)(cid:5)(cid:6)(cid:10)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:10)(cid:7)(cid:6)(cid:7)(cid:3)(cid:1)(cid:13)(cid:12)(cid:17)(cid:6)(cid:18)(cid:19)(cid:20)(cid:5)(cid:12)(cid:10)(cid:3)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:14)(cid:14)(cid:1)(cid:16)(cid:13)(cid:5)(cid:11)(cid:6)(cid:16)(cid:6)(cid:12)(cid:1)(cid:22)(cid:16)(cid:6)(cid:23)(cid:1)(cid:7)(cid:6)(cid:13)(cid:16)(cid:15)(cid:5)(cid:6)(cid:10)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:9)(cid:24)(cid:22)(cid:9)(cid:25)(cid:1)(cid:26)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:14)(cid:14)(cid:1)(cid:24)(cid:13)(cid:27)(cid:6)(cid:24)(cid:28)(cid:1)(cid:9)(cid:24)(cid:22)(cid:9)(cid:25)(cid:14)(cid:8)(cid:20)(cid:29)(cid:9)(cid:24)(cid:22)(cid:9)(cid:25)
(cid:1)
(cid:18)(cid:19)(cid:20)(cid:1)(cid:21)(cid:22)(cid:10)(cid:23)(cid:24)(cid:8)(cid:4)(cid:1)(cid:12)(cid:16)(cid:2)(cid:15)(cid:25)(cid:1)(cid:25)(cid:10)(cid:19)(cid:8)(cid:4)(cid:3)(cid:1)(cid:18)(cid:16)(cid:15)(cid:4)(cid:1)(cid:24)(cid:4)(cid:5)(cid:1)(cid:26)(cid:4)(cid:10)(cid:25)(cid:2)(cid:5)(cid:4)(cid:27)(cid:12)(cid:16)(cid:23)(cid:19)(cid:16)(cid:28)(cid:1)(cid:25)(cid:13)(cid:23)(cid:4)(cid:1)(cid:29)(cid:13)(cid:15)(cid:7)(cid:16)(cid:29)(cid:20)(cid:30)
(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:15)(cid:16)(cid:15)(cid:17)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:2)(cid:5)(cid:8)(cid:9)(cid:10)(cid:3)(cid:11)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:15)(cid:16)(cid:15)(cid:17)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:30)(cid:21)(cid:31)(cid:31)(cid:31)(cid:31)
(cid:32)(cid:30)
(cid:33)(cid:32)(cid:30)
(cid:30)(cid:21)(cid:42)(cid:42)(cid:42)(cid:42)
(cid:31)(cid:32)(cid:3)(cid:30)(cid:30)(cid:30)
(cid:44)(cid:32)(cid:3)(cid:30)(cid:30)(cid:30)
(cid:10)(cid:7)(cid:6)(cid:7)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:15)(cid:16)(cid:15)(cid:17)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:2)(cid:5)(cid:8)(cid:9)(cid:10)(cid:3)(cid:11)(cid:28)(cid:1)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:10)(cid:7)(cid:6)(cid:7)
(cid:15)(cid:16)(cid:15)(cid:17)
(cid:12)(cid:8)(cid:13)(cid:12)(cid:14)(cid:3)
(cid:34)(cid:34)(cid:34)
(cid:34)(cid:34)(cid:34)
(cid:16)(cid:18)(cid:18)(cid:20)(cid:32)(cid:32)
(cid:30)(cid:21)(cid:43)(cid:43)(cid:43)(cid:43)
(cid:45)(cid:30)(cid:3)(cid:30)(cid:30)(cid:30)
(cid:31)(cid:44)(cid:30)(cid:3)(cid:30)(cid:30)(cid:30)
(cid:30)(cid:21)(cid:42)(cid:42)(cid:42)(cid:42)(cid:3)(cid:1)
(cid:30)(cid:21)(cid:43)(cid:43)(cid:43)(cid:43)
(cid:32)(cid:3)(cid:30)(cid:30)(cid:30)
(cid:31)(cid:30)(cid:3)(cid:30)(cid:30)(cid:30)
(cid:22)(cid:23)(cid:30)(cid:31)(cid:28)(cid:29)(cid:23)(cid:17)(cid:15)(cid:25)(cid:1)(cid:29)(cid:15)(cid:33)(cid:1)(cid:18)(cid:15)(cid:31)(cid:15)(cid:1)(cid:30)(cid:31)(cid:28)(cid:29)(cid:19)
(cid:2)(cid:20)(cid:28)(cid:18)(cid:31)(cid:38)(cid:30)(cid:33)(cid:20)(cid:19)(cid:4)(cid:1)(cid:25)(cid:20)(cid:38)(cid:1)(cid:27)(cid:16)(cid:28)(cid:16)(cid:22)(cid:20)(cid:19)(cid:1)(cid:17)(cid:38)(cid:1)(cid:33)(cid:31)(cid:34)(cid:32)(cid:33)(cid:20)(cid:19)(cid:1)(cid:32)(cid:20)(cid:31)(cid:35)(cid:24)(cid:18)(cid:20)(cid:3)
Fig. 2: Pyramid’s architecture. Notation: (cid:2)x: feature vector; l:
label; (cid:2)x(cid:3): count-featurized feature vector; CT: count table.
the models to correct inconsistencies created by the in-
complete incorporation of new observations. Velox saves
observations in a separate data management component,
Spark’s Tachyon. Pyramid replaces this component to
ensure rigorous and selective protection of observations.
Pyramid itself consists of four architectural compo-
nents, shown across the top of the highlighted box in
Fig. 2. The ﬁrst is count featurization, which leverages
the known ML mechanism to count featurize observa-
tions before feeding them to models for training and pre-
diction. The second, third, and fourth are noise infusion,
data retention, and count selection, which augment count
featurization with differential privacy and a set of new
mechanisms to meet Pyramid’s design requirements. We
discuss each component in turn.
III.B.1. Count Featurization
Pyramid hijacks the stream of observations collected
by Velox (the observe method) and count-featurizes
them. An observation is a pair (cid:3)(cid:2)x, l(cid:4) with a feature vector
(cid:2)x = (cid:3)x1, x2, ..., xd(cid:4) and a label l. Application models
the label (or a probability for each possible
predict
label) for a given feature vector by training on count-
featurized observations. When an observation arrives,
Pyramid incorporates it
into two data structures: (1)
the hot raw data store, which retains observations from
the recent past, and (2) the historical statistics store,
which consists of multiple count tables that maintain the
number of occurrences of each feature with each label.
We maintain count tables for all features in (cid:2)x and for
some feature combinations. A separate set of count tables
is maintained for each time window.
Featurization transforms a feature vector (cid:2)x into a
(cid:2), by replacing each
count-featurized feature vector (cid:2)x
feature xi with the conditional probabilities of each label
value given xi’s value. The conditional probabilities are
computed directly from the count tables as discussed
82
(cid:18)(cid:12)(cid:20)(cid:1)(cid:21)(cid:22)(cid:10)(cid:23)(cid:24)(cid:8)(cid:4)(cid:1)(cid:16)(cid:26)(cid:1)(cid:12)(cid:16)(cid:2)(cid:15)(cid:25)(cid:17)(cid:19)(cid:10)(cid:3)(cid:4)(cid:7)(cid:1)(cid:26)(cid:4)(cid:10)(cid:25)(cid:2)(cid:5)(cid:13)(cid:38)(cid:10)(cid:25)(cid:13)(cid:16)(cid:15)(cid:1)(cid:16)(cid:26)
(cid:1)(cid:22)(cid:2)(cid:1)(cid:22) (cid:41)(cid:30)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:1)(cid:31)(cid:22)(cid:32)(cid:32)(cid:32)(cid:32)(cid:3)(cid:1)(cid:34)(cid:34)(cid:34)(cid:3)(cid:1)(cid:35)(cid:3)(cid:1)(cid:35)(cid:3)(cid:1)(cid:31)(cid:22)(cid:33)(cid:33)(cid:33)(cid:33)(cid:3)(cid:1)(cid:35)(cid:3)(cid:1)(cid:31)(cid:22)(cid:34)(cid:34)(cid:34)(cid:34)(cid:3)(cid:1)(cid:35)(cid:1)(cid:26)
(cid:1)(cid:22) (cid:30)
(cid:1)(cid:22) (cid:41) (cid:30)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:2)(cid:1)(cid:1)(cid:1)(cid:31)(cid:35)(cid:31)(cid:36)(cid:3)(cid:1)(cid:1)(cid:1)(cid:34)(cid:34)(cid:34)(cid:3)(cid:1)(cid:35)(cid:3)(cid:1)(cid:35)(cid:3)(cid:1)(cid:1)(cid:1)(cid:1)(cid:31)(cid:35)(cid:32)(cid:36)(cid:3)(cid:1)(cid:1)(cid:1)(cid:35)(cid:3)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:31)(cid:35)(cid:32)(cid:3)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:35)(cid:3)(cid:1)(cid:1)(cid:31)(cid:35)(cid:37)(cid:37)(cid:3)(cid:1)(cid:35)(cid:1)(cid:26)
(cid:7)(cid:5)(cid:20)(cid:23)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:1)(cid:16)(cid:13)(cid:27)(cid:24)(cid:6)(cid:28)
(cid:36)(cid:37)(cid:9)(cid:24)(cid:22)(cid:9)(cid:25)(cid:1)(cid:38)(cid:1)(cid:30)(cid:21)(cid:31)(cid:31)(cid:31)(cid:31)(cid:39)(cid:1)(cid:40)
(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)(cid:32)(cid:30)(cid:1)(cid:14)(cid:1)(cid:37)(cid:32)(cid:30)(cid:1)(cid:41)(cid:1)(cid:33)(cid:32)(cid:30)(cid:39)(cid:1)(cid:40)(cid:1)(cid:31)(cid:35)(cid:31)(cid:36)
(cid:34)(cid:34)(cid:34)
(cid:7)(cid:5)(cid:20)(cid:23)(cid:1)(cid:2)(cid:5)(cid:8)(cid:9)(cid:10)(cid:3)(cid:11)(cid:28)(cid:1)(cid:10)(cid:7)(cid:6)(cid:7)(cid:1)(cid:16)(cid:13)(cid:27)(cid:24)(cid:6)(cid:28)
(cid:36)(cid:37)(cid:9)(cid:24)(cid:22)(cid:9)(cid:25)(cid:1)(cid:38)(cid:1)(cid:30)(cid:21)(cid:42)(cid:42)(cid:42)(cid:42)(cid:3)(cid:1)(cid:30)(cid:21)(cid:43)(cid:43)(cid:43)(cid:43)(cid:39)(cid:1)(cid:40)
(cid:1)(cid:1)(cid:1)(cid:32)(cid:3)(cid:30)(cid:30)(cid:30)(cid:1)(cid:14)(cid:1)(cid:37)(cid:32)(cid:3)(cid:30)(cid:30)(cid:30)(cid:1)(cid:41)(cid:1)(cid:31)(cid:30)(cid:3)(cid:30)(cid:30)(cid:30)(cid:39)(cid:1)(cid:40)(cid:1)(cid:31)(cid:35)(cid:37)(cid:37)
Fig. 3: Count featurization example.
below. To train its models, an application requests a
training set from Pyramid (getTrainSet). Pyramid
featurizes the hot raw data with historical counts and
returns it to the application. To predict the label for a
feature vector (cid:2)x, the application requests its featurization
(cid:2).
from Pyramid (featurize); Pyramid returns (cid:2)x
Example. Fig. 3 shows (a) a sample observation format,
(b) some count tables used by Pyramid to count-featurize
it, and (c) a sample count-featurized observation.
• Observation format. In targeting and personalization,
an observation’s feature vector (cid:2)x typically consists of
user features (e.g., id, gender, age, and previously com-
piled preferences) and contextual information for the
observation (e.g., the URL of the article or the ad shown
to the user, plus any features of these). The label l might
indicate whether the user clicked on the article/ad.
• Count tables. Once an observation stream of the pre-
ceding type is registered with Pyramid, the userId table
maintains for each user the number of clicks the user has
made on any ad shown and the number of non-clicks; it
therefore encodes each user’s propensity to click on ads.
The urlHash table maintains for each URL the number of
clicks that each user made on any ad shown on that page;
it therefore encodes the page’s inherent “ad-clickability.”
Pyramid maintains count tables for every feature in (cid:2)x and
for some feature combinations with predictive potential,
such as the (cid:3)urlHash, adId(cid:4) table, which encodes the
joint probability of a particular ad being clicked when it
is shown on a particular page.
• Count
a
fea-
ture vector (cid:2)x = (cid:2)x1, x2, . . . , xd(cid:3), Pyramid ﬁrst
re-
places each of
features with the conditional
probabilities computed from the count
tables, e.g.,
(cid:2)x(cid:3) = (cid:2)P (click|x1), P (click|x2), . . . , P (click|xd)(cid:3), where
P (click|xi) =
clicks+non-clicks from the row matching the
featurization. To count-featurize
its
clicks
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
value of xi in the table corresponding to xi. Pyramid
(cid:2) the conditional probabilities for any
also appends to (cid:2)x
feature combinations it maintains. Fig. 3(c) shows an
example of feature vector (cid:2)x and its count-featurized
(cid:2). This is a simpliﬁed version of the count
version (cid:2)x
featurization function. We can also include the raw
(cid:2), and support non-binary categorical labels
counts in (cid:2)x
by including conditional probabilities for each label. To
avoid featurizing with an effectively random probability
when a given feature value has very few counts, we
estimate the variance of our probability estimate and, if it
is too high, featurize with a default probability P (click).
• Training and prediction. Suppose a boosted-tree model
, l(cid:4) pairs). It
is trained on a count-featurized dataset ((cid:3)(cid:2)x
(cid:2)
might ﬁnd that for users with a click propensity over
0.04, the chances of a click are high for ads whose
clickability exceeds 0.05 placed on websites with ad-
clickability over 0.1. In this case,
the model would
predict a “click” label for the feature vector in Fig. 3(c).
Process. Pyramid count-featurizes all features xi for
each observation type. For categorical features, we fea-
turize them as described above. For low-cardinality fea-
tures, we can additionally include the raw feature values
(cid:2) alongside the conditional probabilities. Continuous
in (cid:2)x
features are ﬁrst mapped to a discrete space, binning
them by percentiles, and then count-featurized as cate-
gorical. We do the same with continuous labels.
Pyramid maintains hot windows and count tables as
follows. There is one hot window for each observation
stream. There is one count table per feature or feature
group; it has a column for each label and a row for
each value the feature can take. To support granular
retention times, each count table is composed of multiple
windowed count tables holding data for observations
collected during disjoint windows of time. The complete
count table is the sum of the associated windowed count
tables. When a new observation arrives, it is added to
the hot window and made immediately available to the
models for (re)training. The hot window is a sliding
window that may be sized differently from the count
table window. It is also added to the current windowed
count table; this count table is withheld when computing
the complete count table until it is ﬁnished populating.
At this point, Pyramid begins using it as part of the
featurization process, phases out the oldest count table if
it is past its retention period, and begins populating a new
count table that has been initialized with differentially
private noise. Once count tables are incorporated into
the featurization process, they are never updated again.
Count-min sketches (CMSes). A key challenge with
count featurization is its storage requirement. For a
categorical variable of cardinality K and a label of
cardinality L,
table is of size O(LK). A
common solution, used in Azure [20], is to store each
the count
table in a Count-Min Sketch (CMS) [26], a data structure
that approximates counts in sub-linear space. A CMS
consists of a 2D array with an independent hash function
for each row. When a new feature arrives, the CMS uses
the hash function for each row to assign the feature to a
column and increment the value in that cell.
We query the CMS for a feature count by hashing
the feature into a column of each row and taking the
minimum value. Despite overcounting from collisions,
CMS provides sufﬁciently accurate count estimates to
train ML models. With a CMS, we can maintain more
and/or larger count tables with bounded storage over-
heads. This gives developers ﬂexibility in the types of
modeling they can do atop in-use data without tapping
into the historical data store. The CMS poses challenges
to our noise infusion process, as described next.
III.B.2. Noise Infusion
Pyramid’s key contribution is to retroﬁt count fea-
turization, a technique developed for performance and
scalability, to protect past observations against exposure
to attack. Pyramid infuses noise into the count tables to
protect these observations. While we leverage differential
privacy methods [21], correctly applying these methods
in our context poses scaling challenges. For example,
each observation contributes to multiple count tables,
increasing the noise required to guarantee differential
privacy, and a na¨ıve application degrades accuracy when
there are many count tables. We present two techniques
to address this challenge. First, we use a weighted noise
infusion technique to mitigate the impact of noise, allow-
ing us to navigate the privacy/utility trade-off. Second,
for high noise levels, we replace the CMS by a count-
median sketch [27], a data structure with weaker accu-
racy guarantees than CMS but that provides an unbiased
frequency estimate, making it more robust to negative
noise values. To our knowledge, we are the ﬁrst
to
observe that the count-median sketch structure is better
suited to differential privacy. After a brief overview of
differential privacy, we describe these techniques.
Differential privacy properties. Pyramid’s noise infu-
sion component uses four differential privacy properties:
1. Privacy guarantees: Let D1 be the database of past
observations, D2 be a database that differs from D1
by exactly one observation (i.e., D2 adds or removes 1
observation), and S the range of all possible count tables
that can result from a randomized query Q() that builds
a count table from a window of observations. The count
table query Q() is -differentially private if P [Q(D1) ∈
S] ≤ e × P [Q(D2) ∈ S]. In other words, adding or
removing an observation in D1 does not signiﬁcantly
change the probability distribution of possible count
tables; therefore, the count table does not leak signiﬁcant
information about any speciﬁc observation [21].  is
called the query’s privacy budget.
83
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
2. Laplace distribution: Let a query’s sensitivity be the
magnitude of the change in the query result triggered
by adding or removing a single observation. If the