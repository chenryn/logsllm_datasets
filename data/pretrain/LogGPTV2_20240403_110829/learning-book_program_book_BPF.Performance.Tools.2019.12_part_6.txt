## Page 49
### Chapter 1: Introduction
#### 1.9 Back to BCC: Tracing `open()`

The columns in the output are as follows:
- **Process ID (PID)**
- **Process Command Name (COMM)**
- **File Descriptor (FD)**
- **Error Code (ERR)**
- **Path of the file that the syscall attempted to open (PATH)**

The `opensnoop.bt` tool can be used for troubleshooting failing software, which may be attempting to open files from incorrect paths. It can also help determine where configuration and log files are stored based on their access patterns. Additionally, it can identify performance issues such as files being opened too frequently or from the wrong locations.

`bpftrace` comes with over 20 ready-to-run tools, while BCC includes more than 70. These tools not only help solve problems directly but also provide source code that demonstrates how various targets can be traced. Sometimes, there are specific challenges, such as those encountered when tracing the `open(2)` syscall, and the source code often provides solutions to these issues.

### Example Output of `opensnoop(8)`
```plaintext
PID  COMM             FD ERR PATH
2262 DNS Res~er       65T 0 /etc/hosts
2262 DNS Res-er       654 0 /etc/hosts
2958 device po11      4   0 /sys/devices/pci0000:00/0000:00:14.0/usb3/3-0:1.0/host0/target0:0:0/0:0:0:0/block/sdb/sdb1
2958 device po11      6   0 /dev/bus/usb/004/001
2958 device po11      0 /dev/bus/usb/004/001
88567 Ttod ataap     6   0 /dev/bus/usb/003/001
```

The output here is similar to the one-liner version, with the same columns. However, `opensnoop(8)` offers additional command-line options:

```bash
usage: opensnoop [options]
Trace open() syscalls

optional arguments:
  -h, --help            show this help message and exit
  -T, --timestamp       include timestamp on output
  -x, --failed          only show failed opens
  -p PID, --pid PID     trace this PID only
  -t TID, --tid TID     trace this TID only
  -d DURATION, --duration DURATION
                        total duration of trace in seconds
  -n NAME, --name NAME only print process names containing this name
  -e, --extended-fields show extended fields
  -f FLAG_FILTER, --flag-filter FLAG_FILTER
                        filter on flags argument (e.g., O_WRONLY)

examples:
  ./opensnoop
  # trace all open() syscalls

  ./opensnoop -T
  # include timestamps

  ./opensnoop -x
  # only show failed opens

  ./opensnoop -p 181
  # only trace PID 181

  ./opensnoop -t 123
  # only trace TID 123

  ./opensnoop -d 10
  # trace for 10 seconds only

  ./opensnoop -n main
  # only print process names containing *main*

  ./opensnoop -e
  # show extended fields

  ./opensnoop -f O_RDONLY -f O_RDWR
  # only print calls for reading
```

While `bpftrace` tools are typically simple and perform a single task, BCC tools are more complex and support a variety of operational modes. For instance, you can modify the `bpftrace` tool to only show failed opens, but the BCC version already supports this as an option (`-x`):

```bash
./opensnoop -x
PID  COMM             FD ERR PATH
991  Irqbalance        -1 2 /proc/irq/141/smp_affinity
991  Irqbalance         2 /proc/irq/141/smp_affinity
991  Irqbalance        -1 2 /proc/irq/131/smp_affinity
991  Irqbalance         2 /proc/irq/138/smp_affinity
991  Irqbalance        -1 2 /proc/irq/18/smp_affinity
20543 systend-resolve   2 /run/systend/netif/links/5
20543 systend-resolve  -1 2 /run/systend/netif/links/5
20543 systend-resolve   2 /run/systend/netif/links/5
[..-]
```

This output shows repeated failures, which may indicate inefficiencies or misconfigurations that can be fixed.

BCC tools often have multiple options to change their behavior, making them more versatile than `bpftrace` tools. This makes them a good starting point; they can often solve your needs without requiring you to write any BPF code. If they lack the visibility you need, you can then switch to `bpftrace` and create custom tools, as `bpftrace` is an easier language to develop with.

A `bpftrace` tool can later be converted into a more complex BCC tool that supports a variety of options, like `opensnoop(8)` shown earlier. BCC tools can also use different events, such as tracepoints when available, and switch to kprobes when not. However, BCC programming is more complex and is beyond the scope of this book, which focuses on `bpftrace` programming. Appendix C provides a crash course in BCC tool development.

### 1.10 Summary
BPF tracing tools can be used for performance analysis and troubleshooting. The two main projects that provide these tools are BCC and `bpftrace`. This chapter introduced extended BPF, BCC, `bpftrace`, and the dynamic and static instrumentation they use.

The next chapter will delve into these technologies in much more detail. If you are in a hurry to solve issues, you might want to skip Chapter 2 and move on to Chapter 3 or a later chapter that covers the topic of interest. These later chapters make heavy use of terms, many of which are explained in Chapter 2 but are also summarized in the Glossary.

---

## Page 52
### Chapter 2: Technology Background
Chapter 1 introduced various technologies used by BPF performance tools. This chapter explains them in more detail, including their histories, interfaces, internals, and use with BPF.

This is the most technically deep chapter in the book and assumes some knowledge of kernel internals and instruction-level programming. The learning objectives are not to memorize every page but to:
- Know the origins of BPF and the role of extended BPF today.
- Understand frame pointer stack walking and other techniques.
- Understand how to read flame graphs.
- Understand the use of kprobes and uprobes, and be familiar with their stability caveats.
- Understand the role of tracepoints, USDT probes, and dynamic USDT.
- Be aware of PMCs and their use with BPF tracing tools.
- Be aware of future developments: BTF and other BPF stack walkers.

Understanding this chapter will improve your comprehension of later content in the book. You may prefer to skim through this chapter now and return to it for more detail as needed. Chapter 3 will get you started on using BPF tools to find performance wins.

### 2.1 BPF Illustrated
Figure 2-1 shows many of the technologies in this chapter and their relationships to each other.

### 2.2 BPF
**BSD Packet Filter (BPF): A New Architecture for User-level Packet Capture** [McCanne 92]. This paper was presented at the 1993 USENIX Winter conference in San Diego, alongside "Measurement, Analysis, and Improvement of UDP/IP Throughput for the DECstation 5000" [7]. DECstations are long gone, but BPF has survived as the industry standard solution for packet filtering.

BPF works by allowing a filter expression defined by the end user using an instruction set for a BPF virtual machine (sometimes called BPF bytecode) to be passed to the kernel for execution by an interpreter. This allows filtering to occur at the kernel level without costly copies of each packet going to the user-level processes, improving the performance of packet filtering, as used by `tcpdump(8)`. It also provides safety, as filters from user space can be verified as safe before execution. Given that early packet filtering had to occur in kernel space, safety was a hard requirement. Figure 2-2 shows how this works.

### 2.3 Extended BPF (eBPF)
You can use the `-d` option with `tcpdump(8)` to print out the BPF instructions it is using for the filter expression. For example:

```bash
tcpdump -d host 127.0.0.1 and port 80
(000) ldh  [12]
(001) jeq   #0x800 jt 2 jf 18
(002) ld    [26]
(003) jeq   +0x7f000001 jt 6 jf 4
(004) ld    [30]
(005) jeq   +0x7f000001 jt 6 jf 18
(006) ldb   [23]
(007) jeq   #0x84 jt 10 jf 8
(008) jeq   #0x5 jt 10 jf 9
(009) jeq   +0x11 jt 10 jf 18
(010) ldh   [20]
(011) jset  +0x1fff jt 18 jf 12
(012) ldx   4*([14]&0xff)
(013) ldh   [x + 14]
(014) jeq   #0x50 jt 17 jf 15
(015) ldh   [x + 16]
(016) jeq   #0x50 jt 17 jf 18
(017) ret   #262144
(018) ret   #0
```

The original BPF, now referred to as "classic BPF," was a limited virtual machine with two registers, a scratch memory store consisting of 16 memory slots, and a program counter, all operating with a 32-bit register size. Classic BPF arrived in Linux in 1997 for the 2.1.75 kernel [8].

Since the addition of BPF to the Linux kernel, there have been important improvements. Eric Dumazet added a BPF just-in-time (JIT) compiler in Linux 3.0, released in July 2011 [9], improving performance over the interpreter. In 2012, Will Drewry added BPF filters for seccomp (secure computing) syscall policies [10]; this was the first use of BPF outside of networking and showed the potential for BPF to be used as a generic execution engine.

### 2.3 Extended BPF (eBPF)
Extended BPF was created by Alexei Starovoitov while he worked at PLUMgrid, as the company was investigating new ways to create software-defined networking solutions. This would be the first major update to BPF in 20 years, extending BPF to become a general-purpose virtual machine. While it was still a proposal, Daniel Borkmann, a kernel engineer at Red Hat, helped rework it for inclusion in the kernel and as a replacement for the existing BPF.

Extended BPF added more registers, switched from 32-bit to 64-bit words, created flexible BPF "map" storage, and allowed calls to some restricted kernel functions. It was also designed to be JITed with a one-to-one mapping to native instructions and registers, allowing prior native instruction optimization techniques to be reused for BPF. The BPF verifier was updated to handle these extensions and reject any unsafe code.

Table 2-1 shows the differences between classic BPF and extended BPF.

| Factor               | Classic BPF                 | Extended BPF                |
|----------------------|-----------------------------|----------------------------|
| Register count       | 2: A, X                     | 10: R0-R9, plus R10 as a read-only frame pointer |
| Register width       | 32-bit                      | 64-bit                     |
| Storage              | 16 memory slots: M[0-15]    | 512 bytes of stack space, plus infinite "map" storage |
| Restricted kernel calls | Very limited, JIT specific | Yes, via the `bpf_call` instruction |
| Event targets        | Packets, seccomp-BPF        | Packets, kernel functions, user functions, tracepoints, user markers, PMCs |

Alexei’s original proposal was a patchset in September 2013 titled "extended BPF" [2]. By December 2013, Alexei was already proposing its use for tracing filters [11]. After discussion and development with Daniel, the patches began to merge in the Linux kernel by March 2014 [3][12]. The JIT components were merged for the Linux 3.15 release in June 2014, and the `bpf(2)` syscall for the Linux 4.x series added BPF support for kprobes, uprobes, tracepoints, and `perf_events`.

In the earliest patchsets, the technology was abbreviated as eBPF, but Alexei later switched to calling it just BPF. All BPF development on the net-dev mailing list [14] now refers to it as just BPF.

### 2.3 Extended BPF (eBPF)
The architecture of the Linux BPF runtime is illustrated in Figure 2-3, which shows how BPF instructions pass the BPF verifier to be executed by a BPF virtual machine. The BPF virtual machine implementation has both an interpreter and a JIT compiler: the JIT compiler generates native code for the processor, allowing for efficient execution.