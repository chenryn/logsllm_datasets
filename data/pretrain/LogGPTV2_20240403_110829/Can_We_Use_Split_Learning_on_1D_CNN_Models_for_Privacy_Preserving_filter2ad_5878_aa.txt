title:Can We Use Split Learning on 1D CNN Models for Privacy Preserving
Training?
author:Sharif Abuadbba and
Kyuyeon Kim and
Minki Kim and
Chandra Thapa and
Seyit Ahmet Çamtepe and
Yansong Gao and
Hyoungshick Kim and
Surya Nepal
Can We Use Split Learning on 1D CNN Models for
Privacy Preserving Training?
Sharif Abuadbba, Kyuyeon Kim, Minki Kim, Chandra Thapa, Seyit A. Camtepe, Yansong Gao, Hyoungshick
Kim, Surya Nepal
1
0
2
0
2
r
a
M
6
1
]
R
C
.
s
c
[
1
v
5
6
3
2
1
.
3
0
0
2
:
v
i
X
r
a
Abstract—A new collaborative learning, called split learning,
was recently introduced, aiming to protect user data privacy
without revealing raw input data to a server. It collaboratively
runs a deep neural network model where the model
is split
into two parts, one for the client and the other for the server.
Therefore, the server has no direct access to raw data processed
at the client. Until now, the split learning is believed to be a
promising approach to protect the client’s raw data; for example,
the client’s data was protected in healthcare image applications
using 2D convolutional neural network (CNN) models. However,
it is still unclear whether the split learning can be applied to
other deep learning models, in particular, 1D CNN.
In this paper, we examine whether split learning can be used
to perform privacy-preserving training for 1D CNN models. To
answer this, we ﬁrst design and implement an 1D CNN model
under split learning and validate its efﬁcacy in detecting heart
abnormalities using medical ECG data. We observed that the 1D
CNN model under split learning can achieve the same accuracy of
98.9% like the original (non-split) model. However, our evaluation
demonstrates that split learning may fail to protect the raw data
privacy on 1D CNN models. To address the observed privacy
leakage in split learning, we adopt two privacy leakage mitigation
techniques: 1) adding more hidden layers to the client side
and 2) applying differential privacy. Although those mitigation
techniques are helpful in reducing privacy leakage, they have a
signiﬁcant impact on model accuracy. Hence, based on those
results, we conclude that split learning alone would not be
sufﬁcient to maintain the conﬁdentiality of raw sequential data
in 1D CNN models.
Index Terms—split learning, neural networks, privacy leakage,
1D CNN.
I. INTRODUCTION
Deep learning has been successfully applied to many appli-
cations, including genomics [1] and healthcare systems [2]. In
such health applications, those models monitor patients’ status
effectively and detect their disease earlier. To achieve high
accuracy of deep learning models, they need to be trained with
sufﬁcient data collected from a wide range of institutions [3].
Sharif Abuadbba, Yansong Gao and Surya Nepal are with Data61, CSIRO
and Cyber Security CRC, Australia. e-mail: {sharif.abuadbba, yansong.gao,
surya.nepal}@data61.csiro.au
Chandra Thapa and Seyit A. Camtepe are with Data61, CSIRO, Australia.
e-mail: {chandra.thapa, seyit.camtepe}@data61.csiro.au
Professor Hyoungshick Kim, Kyuyeon Kim and Minki Kim are with
Sungkyunkwan University, SouthKorea
and visting Data61, CSIRO,
Australia. e-mail{hyoung.kim, kyuyeon.kim, minki.kim}@data61.csiro.au.
Cite as: Sharif Abuadbba, Kyuyeon Kim, Minki Kim, Chandra Thapa, Seyit
A. Camtepe, Yansong Gao, Hyoungshick Kim, Surya Nepal, Can We Use Split
Learning on 1D CNN Models for Privacy Preserving Training?, The 15th
ACM ASIA Conference on Computer and Communications Security (ACM
ASIACCS 2020), Taipei, Taiwan, from Oct 5th to Oct 9th, 2020.
However, sharing raw data, especially in health applications,
may raise privacy concerns that violate certain rules such
as reusing the data indiscriminately and risk-agnostic data
processing [4] required by General Data Protection Regulation
(GDPR) [5] and HIPAA [6].
In 2018, Otkrist et al. [3] introduced a new collaborative
learning technique, called split learning, to protect user privacy
by allowing training without sharing users’ raw data to the
server that runs a deep neural network (DNN) model [7], [8].
Generally, split learning divides the DNN layers into two parts
(A and B) between client and server. The client, who owns the
raw data, trains part A that consists of the ﬁrst few layers using
forward propagation and only sends their activated outputs
from the split layer (the last layer of the part A) to the server.
After receiving the activated outputs from the client, the server
performs the forward training with those outputs on part B.
Next, the server runs the backward propagation on part B and
only sends back the gradients of the activated outputs of the
split layer (ﬁrst layer of part B) to the client to complete the
backward propagation on part A. This process continues until
the model is converged.
Goals of split learning are: 1) the raw data is no longer
required to be shared with the server, 2) the model classiﬁca-
tion accuracy is comparable to the non-split model [3], and 3)
reducing the computational overhead of the client who only
needs to run a few layers rather than the whole model. To date,
the effectiveness of split learning has been validated in vision
domains such as the medical image classiﬁcation problem via
a 2D convolutional neural network (CNN) [8]. However, health
data includes not only images but also sequential/time-series
data such as ECG signals.
As a ﬁrst study towards exploring the feasibility of split
learning to deal with sequential data, we adopt an 1D CNN
model for detecting heart abnormalities using ECG signals that
are collected from electrodes attached to human skin [9] as a
case study. Recently, several 1D CNN models were introduced
to classify sequential data, including biomedical ECG signals
[10], [11], [12]. Considering the fact that the exposure of raw
ECG data would raise privacy concerns because ECG signals
can reveal people’s disease status and also be used to identify
people uniquely [13], it is crucial to protect the privacy of raw
data for 1D CNN models. Split learning would be a promising
candidate to fulﬁll this privacy requirement.
This work is dedicated to investigating the answers to the
following two research questions (RQs):
RQ 1: Can split learning be applied to deal with sequential
or time-series data in particularly using 1D CNN to achieve
comparable model accuracy as that of trained on centralized
data?
To answer RQ 1, we ﬁrst investigate the applicability of
split learning to 1D CNN models to deal with sequential data.
To the best of our knowledge, this is the ﬁrst elaborated study
on split learning using 1D CNN models, where we conﬁrm
that split learning is applicable to sequential data.
RQ 2: Can split
sequential data trained using 1D CNN?
learning be used to protect privacy in
Then, we focus on understanding the privacy leakage of
split learning to answer RQ 2. We ﬁnd that the impact of split
learning was rather limited to reduce privacy leakage.
Correspondingly, we have made the following contributions:
• We implement1 split learning on 1D CNN model and ap-
ply it for time-series sequential data exempliﬁed by using
ECG signals to detect heart abnormalities, where sharing
medical data with other party is inherently avoided but
can still achieve the same accuracy of the non-split model.
• We propose a privacy assessment framework for CNN
models employing split learning, with three metrics: vi-
sual invertibility, distance correlation [14], and Dynamic
Time Warping (DTW) [15]. This is to answer RQ 2. We
observed that direct application of split learning into 1D
CNN has a high privacy leakage in the applications with
sensitive data such as ECG signals.
• To address the shortcoming of direct application of split
learning into 1D CNN, we apply two countermeasures: i)
increasing the number of convolutional layers of a CNN
model split at the client and ii) exploiting differential
privacy. The results suggest that although these tech-
niques seem helpful to reduce privacy leakage, they have
a signiﬁcant impact on the accuracy of the model.
The rest of the paper is organized as follows: Section II
provides background information about CNN, split learning,
and privacy issues in using deep neural network models on
cloud services. The design and implementation of split learn-
ing on 1D CNN are detailed in Section III. Section IV analyzes
the privacy leakage question on 1D CNN models under split
learning based on our identiﬁed threat model. Section V
discusses the possibility of two mitigation techniques and
evaluates them. Section VI discusses our ﬁndings and future
work. Section VII presents the related work, followed by the
conclusion in Section VIII.
II. BACKGROUND
This section provides the necessary information to un-
derstand our work. It includes an 1D convolutional neural
network, split learning technique, and privacy issues in the
ﬁeld of machine learning.
A. 1D Convolutional Neural Network (CNN)
A CNN is a part of broader machine learning methods based
on artiﬁcial neural networks where input feature extraction is
1 https://github.com/SharifAbuadbba/split-learning-1D
2
performed automatically [16]. A 1D CNN for classiﬁcation
problem can be depicted as a mapping function fΘ : Rn×c →
Rm that maps an input x ∈ Rn×c to an output ˆy ∈ Rm
based on the calculated parameters Θ, where n is the length
of input vector, c is the number of input channels, and m is
the number of classes. For example, let us assume x is an
ECG sample taken from a patient that has to be classiﬁed
by fΘ into ˆy as a vector of probabilities corresponding to
5 different types of heartbeat diseases. The output with the
highest value, arg maxi∈{1. .5} ˆyi, is a ﬁnal prediction from
the model in which x is most likely to be, e.g., ‘A’ (atrial
premature contraction).
A CNN is constructed with L hidden layers. Each layer l,
l ∈ {1 . . L}, has nl neurons, and activated output a(l). The
vector consists of values of each neuron of that layer, and it is
computed in a feed-forward propagation manner as follows:
a(l) = g(l)(w(l)a(l−1) + b(l)) ∀ l ∈ {1 . . L}
(1)
where g(l) : Rn → Rn is a non-linear activation function
of layer l that ensures only crucial neurons to be ﬁred (i.e.,
> 0) and forwards its output as an input to the next layer.
w(l) ∈ Rnl×nl−1 is the weights and b(l) ∈ Rnl is the biases;
both of them are learned during training. The CNN output
of the L layer, i.e., the last hidden layer, is a function which
can be calculated as ˆyi = a(L) = g(L)(w(L+1)a(L) + b(L+1)).
After performing forward propagation reaching the output yi,
the difference between the ground truth label yi and predicted
ˆyi is calculated as the loss Ei = (yi − ˆyi)2, in case of using
squared loss. Then, the contribution of every w towards this
loss is calculated in a backpropagation way. For that, the
partial derivatives of the loss with respect to each individual
weight is calculated. As an example, we present the calculation
with respect to a single weight w(L)
that connects node 2 in
layer L − 1 to node 1 in Layer L as follows:
12
∂Ei
∂w(L)
12
)(
= (
∂Ei
∂a(L)
∂a(L)
∂z(L)
1 − yi)(g(L)(cid:48)(z(L)
1
1
1
1
1
)
)(
∂z(L)
∂w(L)
12
))(a(L−1)
2
(2)
(3)
)
= 2(a(L)
where g(L) is the activation function of layer L and z(L) is
the input for that neuron.
The widely used CNN models are 1D and 2D CNN. The
2D CNN is popular in image recognition to extract features