sertion speed. We chose only data captured over one day, with size slightly larger
than the available memory, because we wanted to maintain reasonable running
times for the other systems that we compared NetStore to. These systems be-
come very slow for larger data sets and performance gap compared to NetStore
increases with the size of the data.
4.1 Parameters
Figure 6 shows the inﬂuence that the segment size has over the insertion rate.
We observe that the insertion rate drops with the increase of segment size. This
trend is expected and is caused by the delay in preprocessing phase, mostly
because of the larger segment array sorting. As Figure 7 shows, the segment
290
P. Giura and N. Memon
Table 1. NetStore ﬂow
attributes.
Table 2. NetStore properties and network rates supported
based on 24 hour ﬂow records data and the 12 attributes
Property
records insertion rate
number of records
number of bytes transported
bytes transported per record
Value
10,000
Unit
records/second
62,397,594
1.17
records
Terabytes
20,616.64
Bytes/record
bits rate supported
1.54
number of packets transported 2,028,392,356
packets transported per record
Gbit/s
packets
packets rate supported
32.51
packets/record
325,075.41 packets/second
Column Type Bytes
sourceIP int
int
destIP
sourcePort short
destPort
short
protocol byte
startTime short
endTime short
tcpSyns byte
tcpAcks byte
tcpFins
byte
byte
tcpRsts
int
numBytes
4
4
2
2
1
2
2
1
1
1
1
4
size also aﬀects the compression ratio of each segment, the larger the segment
size the larger the compression ratio achieved. But high compression ratio is
not a critical requirement. The size of the segments is more critically related
to the available memory, the desired insertion rate for the network and the
number of attributes used for each record. We set the insertion rate goal at
10,000 records/second, and for this goal we set a segment size of 2 million records
given the above hardware speciﬁcation and records sizes. Table 2 shows the
insertion performance of NetStore. The numbers presented are computed based
on average bytes per record and average packets per record given the insertion
rate of 10,000 records/second. When installed on a machine with the above
speciﬁcation, NetStore can keep up with traﬃc rates up to 1.5 Gbit/s for the
current experimental implementation. For a constant memory size, this rate
decreases with the increase in segment size and the increase in the number of
attributes for each ﬂow record.
Fig. 6. Insertion rate for diﬀerent segment
sizes
Fig. 7. Compression ratio with and without
aggregation
NetStore: An Eﬃcient Storage Infrastructure
291
4.2 Queries
Having described the NetStore architecture and it’s design details, in this section
we consider the queries described in [5], but taking into account data collected
over the 24 hours for internal network 128.238.0.0/16. We consider both the
queries and methodology in [5] meaningful for how an investigator will perform
security analysis on network ﬂow data. We assume all the ﬂow attributes used
are inserted into a table f low and we use standard SQL to describe all our
examples.
Scanning. Scanning attack refers to the activity of sending a large number of
TCP SYN packets to a wide range of IP addresses. Based on the received an-
swer the attacker can determine if a particular vulnerable service is running on
the victim’s host. As such, we want to identify any TCP SYN scanning activity
initiated by an external hosts, with no TCP ACK or TCP FIN ﬂags set and
targeted against a large number of internal IP destinations, larger than a preset
limit. We use the following range aggregation query (Q1):
SELECT sourceIP, destPort, count(distinct destIP), startTime
FROM flow
WHERE sourceIP <> 128.238.0.0/16 AND destIP = 128.238.0.0/16
AND protocol = tcp AND tcpSyns = 1 AND tcpAcks = 0 AND tcpFins = 0
GROUP BY sourceIP
HAVING count(distinct destIP) > limit;
External IP address 61.139.105.163 was found scanning starting at time t1. We
check if there were any valid responses after time t1 from the internal hosts,
where no packet had the TCP RST ﬂag set, and we use the following query (Q2):
SELECT sourceIP, sourcePort, destIP
FROM flow
WHERE startTime > t1 AND sourceIP = 128.238.0.0/16
AND destIP = 61.139.105.163 AND protocol = tcp AND tcpRsts = 0;
Worm Infected Hosts. Internal host with the IP address 128.238.1.100 was
discovered to have been responded to a scanning initiated by a host infected
with the Conﬁcker worm and we want to check if the internal host is compro-
mised. Typically, after a host is infected, the worm copies itself into memory
and begins propagating to random IP addresses across a network by exploiting
the same vulnerability. The worm opens a random port and starts scanning ran-
dom IPs on port 445. We use the following query to check the internal host (Q3):
SELECT sourceIP, destPort, count(distinct destIP)
FROM flow
WHERE startTime > t1 AND sourceIP = 128.238.1.100 AND destPort = 445;
292
P. Giura and N. Memon
SYN Flooding. It is a network based-denial of service attack in which the
attacker sends an unusual large number of SYN request, over a threshold t, to a
speciﬁc target over a small time window W. To detect such an attack we ﬁlter all
the incoming traﬃc and count the number of ﬂows with TCP SYN bit set and no
TCP ACK or TCP FIN for all the internal hosts. We use the following query(Q4):
SELECT destIP, count(distinct sourceP), startTime
FROM flow
WHERE startTime > ’NOW - W’ AND destIP = 128.238.0.0/16
AND protocol = tcp AND tcpSyns = 1 AND tcpAcks = 0 AND tcpFins = 0
GROUP BY destIP
HAVING count(sourceIP) > t;
Network Statistics. Besides security analysis, network statistics and perfor-
mance monitoring is another important usage for network ﬂow data. To get this
information we use aggregation queries for all collected data over a large time
window, both incoming and outgoing. Aggregation operation can be number of
bytes or packets summation, number of unique hosts contacted or some other
meaningful aggregation statistics. For example we use the following simple ag-
gregation query to ﬁnd the number of bytes transported in the last 24 hours (Q5):
SELECT sum(numBytes)
FROM flow WHERE startTime > ’NOW - 24h’;
General Queries. The sample queries described above are complex and be-
long to more than one basic type described in Section 3.1. However, each of
them can be separated into several basic types such that the result of one query
becomes the input for the next one. We build a more general set of queries start-
ing from the ones described above by varying the parameters in such a way to
achieve diﬀerent level of data selectivity form low to high. Then, for each type
we reported the average performance for all the queries of that type. Figure 8
shows the average running times of selected queries for increasing segment sizes.
We observe that for S type queries that don’t use IPs index (e.g. for attributes
other than internal sourceIP or destIP), the performance decreases when the
segment size increases. This is an expected result since for larger segments there
is more unused data loaded as part of the segment where the spotted value re-
sides. When using the IPs index the performance beneﬁt comes from skipping
the irrelevant segments whose positions are not found in the positions list. How-
ever, for internal busy servers that have corresponding ﬂow records in all the
segments, all corresponding segments of attributes have to be read but not the
IPs segments. This is an advantage since an IP segment is several times larger in
general than the other attributes segments. Hence, except for spot queries that
use non-indexed attributes, queries tend to be faster for larger segment sizes.
4.3 Compression
Our goal with using compression is not to achieve the best compression ratio nor
the best compression or decompression speed, but to obtain the highest records
NetStore: An Eﬃcient Storage Infrastructure
293
insertion rate and the best query performance. We evaluated our compression
selection model by comparing performance when using a single method for all
the segments in the column, with the performance when using the compression
selection algorithm for each segment. To select the method for a column we com-
pressed ﬁrst all the segments of the columns with all the six methods presented.
We then measured the access performance for each column compressed with each
method. Finally, we selected the compression method of a column, the method
that provides the best access times for the majority of the segments.
For the variable segments compression, we activated the methods selection mech-
anism for all columns and then we inserted the data, compressing each segment
based on the statistics of its own data rather than the entire column. In both cases
we did not change anything in the statistic collection process since all the statistics
were used in the query process for both approaches. We obtained on an average
10 to 15 percent improvement per query using the segment based compression
method selection model with no penalty for the insertion rate. However, we con-
sider the overall performance of compression methods selection model is satisfac-
tory and the true value resides in the framework implementation, being limited
only by the individual methods used not by the general model design. If the data
changes and other compression methods are more eﬃcient for the new data, only
the compression algorithm and the operators that work on this compressed data
should be changed, with the overall architecture remaining the same. Some com-
mercial systems [19] apply on top of the value-based compressed columns another
layer of general binary compression for increased performance. We investigated
the same possibility and compared four diﬀerent approaches to compression on
top of the implemented column oriented architecture: no compression, value-based
compression only, binary compression only and value-based plus binary compres-
sion on top of that. For the no compression case, we processed the data using the
same indexing structure and column oriented layout but with the compression
disabled for all the segments. For the binary compression only we compress each
segment using the generic binary compression. In the case of value-based com-
pression we compress all the segments having the dynamic selection mechanism
enabled, and for the last approach we apply another layer of generic compression
on top of already value-based compressed segments.
The results of our experiment for the four cases are shown in Figure 9. We can
see that compression is a determining factor in performance metrics. Using value-
based compression achieves the best average running time for the queries while
the uncompressed segments scenario yields the worst performance.We also see
that adding another compression layer does not help in query performance nor
in the insertion rate even though it provides better compression ratio. However,
the general compression method can be used for data aging, to compress and
archive older data that is not actively used.
Figure 7 shows the compression performance for diﬀerent segment sizes and
how ﬂow aggregation aﬀects storage footprint. As expected, compression perfor-
mance is better for larger segment sizes in both cases, with and without aggrega-
tion. That is the case because of the compression methods used. The larger the
294
P. Giura and N. Memon
Fig. 8. Average query times for diﬀerent
segment sizes and diﬀerent query types
Fig. 9. Average query times for the com-
pression strategies implemented
segment, the longer the runs for column with few distinct values, the smaller the
dictionary size for each segment. The overall compression ratio of raw network
ﬂow data for the segment size of 2 million records is 4.5 with no aggregation and
8.4 with aggregation enabled. Note that the size of compressed data includes also
the size of both indexing structures: column indexes and IPs index.
4.4 Comparison with Other Systems
For comparison we used the same data and performed a system-speciﬁc tuning
for each of the systems parameters. To maintain the insertion rate above our
target of 10,000 records/second we created three indexes for each Postgres and
Luciddb: one clustered index on startTime and two un-clustered indexes, one
on sourceIP and one on destIP attributes. Although we believe we chose good
values for the other tuning parameters we cannot guarantee they are optimal
and we only present the performance we observed. We show the performance for
using the data and the example queries presented in Section 4.2.
Table 3 shows the relative performance of NetStore compared to PostgresSQL
for the same data. Since our main goal is to improve disk resident data access,
we ran each query once for each system to minimize the use of cached data. The
numbers presented show how many times NetStore is better.
To maintain a fair overall comparison we created a PostgresSQL table for each
column of Netstore. As mentioned in [2], row-stores with columnar design provide
better performance for queries that access a small number of columns such as
the sample queries in Section 4.2. We observe that Netstore clearly outperforms
Table 3. Relative performance of NetStore versus columns only PostgreSQL and Lu-
cidDB for query running times and total storage needed
Q1 Q2 Q3 Q4 Q5 Storage
Postgres/NetStore 10.98 7.98 2.21 15.46 1.67
LucidDB/NetStore 5.14 1.10 2.25 2.58 1.53
93.6
6.04
NetStore: An Eﬃcient Storage Infrastructure
295
PostgreSQL for all the query types providing the best results for queries access-
ing more attributes (e.g. Q1 and Q4) even though it uses 90 times more disk
space including all the auxiliary data. The poor PostgreSQL performance can
be explained by the absence of more clustered indexes, the lack of compression,
and the unnecessary tuple overhead.
Table 3 also shows the relative performance compared to LucidDB. We observe
that the performance gap is not at the same order of magnitude compared to
that of PostgreSQL even when more attributes are accessed. However, NetStore
performs clearly better when storing about 6 times less data. The performance
penalty of LucidDB can be explain by the lack of column segmentation design
and by early materialization in the processing phase speciﬁc to general-purpose
column stores. However we noticed that LucidDB achieves a signiﬁcant perfor-
mance improvement for the subsequent runs of the same query by eﬃciently
using memory resident data.
5 Conclusion and Future Work
With the growth of network traﬃc, there is an increasing demand for solutions
to better manage and take advantage of the wealth of network ﬂow information
recorded for monitoring and forensic investigations. The problem is no longer the
availability and the storage capacity of the data, but the ability to quickly extract
the relevant information about potential malicious activities that can aﬀect net-
work security and resources. In this paper we have presented the design, implemen-
tation and evaluation of a novel working architecture, called NetStore, that is useful
in the network monitoring tasks and assists in network forensics investigations.
The simple column oriented design of NetStore helps in reducing query pro-
cessing time by spending less time for disk I/O and loading only needed data.
The column partitioning facilitates the use of eﬃcient compression methods for
network ﬂow attributes that allow data processing in compressed format, there-
fore boosting query runtime performance. NetStore clearly outperforms existing
row-based DBMSs systems and provides better results that the general purpose
column oriented systems because of simple design decisions tailored for network
ﬂow records. Experiments show that NetStore can provide more than ten times
faster query response compared to other storage systems while maintaining much
smaller storage size. In future work we seek to explore the use of NetStore for
new types of time sequential data, such as host log analysis, and the possibility
to release it as an open source system.
References
1. Abadi, D., Madden, S., Ferreira, M.: Integrating compression and execution in
column-oriented database systems. In: SIGMOD 2006: Proceedings of the 2006
ACM SIGMOD International Conference on Management of Data, pp. 671–682.
ACM, New York (2006)
296
P. Giura and N. Memon
2. Abadi, D.J., Madden, S.R., Hachem, N.: Column-stores vs. row-stores: how diﬀer-
ent are they really? In: SIGMOD 2008: Proceedings of the 2008 ACM SIGMOD
International Conference on Management of Data, pp. 967–980. ACM, New York
(2008)
3. Chang, F., Dean, J., Ghemawat, S., Hsieh, W.C., Wallach, D.A., Burrows, M.,
Chandra, T., Fikes, A., Gruber, R.E.: Bigtable: A distributed storage system for
structured data. In: Proceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation, OSDI 2006 (2006)
4. Cranor, C., Johnson, T., Spataschek, O., Shkapenyuk, V.: Gigascope: a stream
database for network applications. In: SIGMOD 2003: Proceedings of the 2003
ACM SIGMOD International Conference on Management of Data, pp. 647–651.
ACM, New York (2003)
5. Gates, C., Collins, M., Duggan, M., Kompanek, A., Thomas, M.: More netﬂow
tools for performance and security. In: LISA 2004: Proceedings of the 18th USENIX
Conference on System Administration, pp. 121–132. USENIX Association, Berke-
ley (2004)
6. Geambasu, R., Bragin, T., Jung, J., Balazinska, M.: On-demand view material-
ization and indexing for network forensic analysis. In: NETB 2007: Proceedings
of the 3rd USENIX International Workshop on Networking Meets Databases, pp.
1–7. USENIX Association, Berkeley (2007)
7. Goldstein, J., Ramakrishnan, R., Shaft, U.: Compressing relations and indexes. In:
Proceedings of IEEE International Conference on Data Engineering, pp. 370–379
(1998)
8. Halverson, A., Beckmann, J.L., Naughton, J.F., Dewitt, D.J.: A comparison of c-
store and row-store in a common framework. Technical Report TR1570, University
of Wisconsin-Madison (2006)
9. Holloway, A.L., DeWitt, D.J.: Read-optimized databases, in depth. Proc. VLDB
Endow. 1(1), 502–513 (2008)
10. Infobright Inc. Infobright, http://www.infobright.com
11. LucidEra. Luciddb, http://www.luciddb.org
12. Paxson, V.: Bro: A system for detecting network intruders in real-time. Computer
Networks, 2435–2463 (1998)
13. PostgreSQL. Postgresql, http://www.postgresql.org
14. Roesch, M.: Snort - lightweight intrusion detection for networks. In: LISA 1999:
Proceedings of the 13th USENIX Conference on System Administration, pp. 229–
238. USENIX Association, Berkeley (1999)
15. ´Sl¸ezak, D., Wr´oblewski, J., Eastwood, V., Synak, P.: Brighthouse: an analytic data