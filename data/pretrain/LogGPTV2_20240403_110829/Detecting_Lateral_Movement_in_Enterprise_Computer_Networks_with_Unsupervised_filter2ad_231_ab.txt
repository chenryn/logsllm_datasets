malware, and thus will prove ineffective at detecting APT-
level adversaries who will move laterally through a network
using novel malware or legitimate authentication mechanisms.
Some environments may implement a Security Information
Events Management (SIEM) System, which would allow for
more complex log analytics. However, SIEMs are typically
standard row or columnar data stores such as Splunk [26]
which only allow for relatively basic statistical analysis of the
data. Behavioral analytics implemented in SIEMs are typi-
cally simple aggregate trends of low level features such as
bytes over particular ports and protocols.
3 Proposed Method
In this section we will discuss our proposed method for detect-
ing lateral movement in enterprise computer networks. We
will provide an overview of our machine learning pipeline, fol-
lowed by detailed discussions of the node embedding process,
the link predictor training, and the anomaly detection.
3.1 Overview
In order to detect lateral movement in enterprise computer
networks, we generate authentication graphs as discussed
previously and apply an unsupervised graph learning process
to identify low probability links. Figure 3 shows the algorithm
pipeline. During the ofﬂine training stage (the top half of the
ﬁgure), we start by generating authentication graphs, then
create node embeddings via a random walk sampling and
embedding process, and ﬁnally train a logistic regression link
predictor using the node embeddings and ground-truth edge
information from the authentication graph.
During the online detection stage (the bottom half of the
ﬁgure), new authentication events are processed resulting
in new edges between authenticating entities. Embeddings
for these entities are generated via an embedding lookup,
and link prediction is performed using the trained logistic
regression link predictor. Anomaly detection is performed
via a (conﬁgurable) threshold value, where links below a
particular probability threshold will be forwarded to security
experts for investigation.
3.2 Node Embedding Generation
Node embedding generation is the process by which a d-
dimensional vector is learned for each node in a graph. The
260    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
goal of these approaches is to generate a vector representation
for each node which captures some degree of behavior within
the network as a whole.
For the authentication graph, we use H to denote the set of
node embeddings, H = {h1,h2, ...,hn}, where hi denotes the
node embedding for the ith node, and n denotes the number
of nodes in the graph. In the beginning, nodes do not have
embeddings, which means hi = /0.
In order to extract latent node representations from the
graph, we utilize an unsupervised node embedding technique
similar to DeepWalk [22], and node2vec [7]. We ﬁrst sample
our authentication graph via unbiased, ﬁxed-length random
walks. Speciﬁcally, for any node v in the graph, we will ex-
plore r random walks with a ﬁxed-length l. For a random
walk starting from node v, let vi denote the ith node in the
walk, the node sequence for this walk is generated with the
following probability distribution:
P(vi = x|vi−1 = y) =
,
i f (x,y) ∈ E
otherwise
(1)
dy
0,
(cid:40) 1
where E denotes the edge set in the graph, and dy is the degree
of node y. This results in a set of random walk sequences
S = {S1,S2, ...,Sm}, where Si denotes the ith random walk
sequence, and m denotes the total number of sequences.
With the sequence set of the random walks, we then tune
node embeddings via a Continuous-Bag-of-Words (CBOW)
model with negative sampling as proposed in [18]. In the
CBOW model, we predict the target node provided context
nodes from the random walk sequence. We utilize negative
sampling such that we only update the vectors of a subset of
nodes that were not found in the particular context window
of the target node.
We use the Noise Contrastive Estimation (NCE) loss as
deﬁned in Equation 2:
L = −[logp(y = 1|hT ,hI)+ ∑
hU∈N(hI )
logp(y = 0|hU ,hI)] (2)
where y denotes the label, hT denotes the embedding of the
target node, hI denotes the embedding of the input node which
is the average of the context nodes, hU denotes the embedding
of a noise node, and N(·) denotes the set of noise node embed-
dings for that input. This loss function differentiates the target
sample from noise samples using logistic regression [8].
Further, the probability for different labels of negative sam-
pling is deﬁned in Equation 3,
p(y = 1|hT ,hI) = σ(h(cid:48)(cid:62)
T hI)
p(y = 0|hT ,hI) = σ(−h(cid:48)(cid:62)
T hI)
(3)
where σ(·) denotes the sigmoid function, and h(cid:48)
T denotes
the column vector for hT . Therefore, the ﬁnal loss value is
calculated by Equation 4.
L = −[logσ(h(cid:48)(cid:62)
logσ(−h(cid:48)(cid:62)
T hI) + ∑
T hI)]
(4)
hU∈N(hI )
Figure 4: Example embedding space generated from a
random-walk based node-embedding process.
By minimizing the loss value from Equation 4, we are able
to tune our node embeddings such that we are more likely to
predict our target node embedding hT given the context node
embeddings hI, while simultaneously less likely to predict
the negative sample node embeddings hU given the same
context hI. We use Stochastic Gradient Descent (SGD) to
minimize the loss function. In the end, we generate the output
node embedding set H(cid:48) = {h(cid:48)
(cid:48) is the
1,h(cid:48)
d-dimension embedding for node i.
n}, where hi
2, ...,h(cid:48)
In the context of the authentication graph, this process
equates to predicting a user based on the machines and users
found within at-most l-hops away. This will result in node
embeddings where users who often authenticate to similar
entities will be embedded in a similar region. Similarly, sys-
tems which share a user base will be found embedded in a
similar region. This provides us the ability to then look at
authentication events as events between two abstract vectors,
as opposed to between distinct users and machines.
Figure 4 provides a 2-dimensional embedding space gen-
erated for the graph in Figure 1 using this node embedding
process. We can see that the embedding of the graph corre-
sponds nicely to the organizational units of the various users
and systems. Additionally we see that the servers are clearly
separated from the users and their workstations. Also, the
network administrator is clearly separated from both orga-
nizational units. In addition, notice that the user Alice does
not have an edge to the hr-email server in the authentication
graph, despite clearly being a member of the hr organiza-
tion. Even though this is the case, we can see that Alice is
co-located in the embedding space with other hr users and
systems. This fact will be crucial during the link prediction
process, as even though there is no explicit link between Alice
and the hr-email server, we would like our link prediction
algorithm to predict a high probability for the authentication
event between Alice and hr-email, considering it is perfectly
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    261
reasonable that Alice authenticates to the hr-email server.
4 Evaluation
3.3 Link Prediction
Next, we utilize a traditional logistic regression (LR) algo-
rithm to provide us with a probability estimate that a particular
authentication event occurs between two nodes a and b. For-
mally, our LR algorithm models:
P(y = 1|h(cid:48)) = σ(h(cid:48)) =
1
1 + e−w(cid:62)h(cid:48)
(5)
where y is the binary label indicating if an edge exists or not,
the weight vector w contains the learned parameters, and h(cid:48) is
the element-wise multiplication of the node embeddings ha
and hb deﬁned in Equation 6, also known as the Hadamard
product.
ha ◦ hb = (ha)i j · (hb)i j
(6)
We train the above model by generating a dataset of true
and false edge embeddings from the ground truth authenti-
cation graph. The true edge set consists of all edges in the
authentication graph:
ET = ha ◦ hb ∀(a,b) ∈ E
(7)
with each edge embedding receiving a binary label of 1. On
the contrary, the false edge set consists of all edges that do
not exist in the authentication graph:
EF = ha ◦ hb ∀(a,b) /∈ E
(8)
with each edge embedding receiving a binary label of 0. Train-
ing on these two sets of data would cause signiﬁcant over
ﬁtting as EF contains every possible edge not in the original
edge set E. Therefore, we down sample EF via a random
sampling process, and only train on the same number of false
edges as found in ET .
3.4 Anomaly Detection
Anomaly detection is achieved by applying our trained LR
link predictor to new authentication events. First, authentica-
tion events are parsed into a set of edges between authenti-
cating entities. Next, we perform an embedding lookup for
the node embeddings generated during the training stage. The
anomaly detection function A can be expressed as:
A(ha,hb) =
i f f (ha ◦ hb) < δ
1,
0, otherwise
(9)
(cid:40)
where ha and hb are the embeddings for nodes a and b, and the
function f (·) is the logistic regression link predictor trained
on the true and false edges generated from our training graph.
The parameter δ is the threshold for generating an alert. In
this paper, we use a threshold of δ = 0.1, or 10%, which we
will show shortly yields good performance.
In this section we will evaluate our technique for detecting
malicious authentication in enterprise networks. First we will
discuss the datasets we used for evaluation, followed by a
detailed description of the various methods we evaluated, and
an analysis of our results. In an effort to further reduce false
positives, we make some observations about the data and our
results, and update our algorithm accordingly.
4.1 Datasets
We apply our malicious authentication detection to two
datasets generated from contrasting computer networks. Table
1 provides details on each dataset. We discuss both datasets
in detail below.
Table 1: Dataset Details
PicoDomain LANL
Duration in Days
Days with Attacks
Total Records
Total Attack Records
User and Machine Accounts
Computers
3
2
4686
129
86
6
58
18
1.05 B
749
99968
17666
PicoDomain is a dataset we generated in-house for cyber
security research. It is designed to be a highly scaled-down
environment which contains only the most critical elements
commonly found in enterprise-level domains. Speciﬁcally, the
PicoDomain consists of a small Windows-based environment
with ﬁve workstations, a domain controller, a gateway ﬁrewall
and router, and a small-scale internet that houses several web-
sites as well as the adversarial infrastructure. A Zeek network
sensor was installed inside the environment and placed such
that it had visibility of trafﬁc entering and leaving the network
from the simulated Internet (north/south), as well as trafﬁc
between local systems in the simulated enterprise network
(east/west). A total of three days of network trafﬁc was cap-
tured. During this three day period, there was benign activity
performed in a typical 9-5 workday pattern, such as browsing
the web, checking e-mail, etc. Additionally, on days 2 and
3, we ran an APT-style attack campaign which included all
stages of the kill chain. The attack campaign started with a ma-
licious ﬁle downloaded from an e-mail attachment. This gave
the attacker the initial foothold in the network. The attacker
then proceeded to perform various malicious actions typically
associated with APT-level campaigns. This included exploit-
ing system vulnerabilities for privilege escalation, registry
modiﬁcations to maintain persistence, credential harvesting
via the tool Mimikatz, domain enumeration, and lateral move-
ment to new systems via the legitimate Windows Management
Instrumentation (WMI) service. At the end of the campaign,
262    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
the attacker was able to compromise a domain admin account,
resulting in full network ownership by the attacker.
Comprehensive Cyber Security Events is a dataset re-
leased by Los Alamos National Labs (LANL) and consists
of 58 consecutive days of anonymized network and host
data [11]. There are over 1 billion events containing authenti-
cation activity for over 12,000 users and 17,000 computers
in the network. An APT-style attack was performed during
the data capture, and relevant authentication log entries were
labeled as being malicious or benign. No further details were
provided in the dataset as to what types of attacks were per-
formed during the exercise. This is a limiting factor of this
dataset, and, in fact, led to the generation of the previously
mentioned PicoDomain dataset.
4.2 Methods Evaluated
We evaluate two variants of our proposed graph learning meth-
ods, as well as four different baseline techniques, which in-
clude two non-graph-based machine learning algorithms, as
well as two traditional rule-based heuristics. We will discuss
each below.
Graph Learning with Local View (GL-LV). This is our
graph learning technique conﬁgured in such a way as to have a
more localized view in our graph. This means our embeddings
and link predictor will be optimized for nodes within a close