i=0 ← cPolyArr(a,(cid:74)k(cid:75));
10 {(cid:74)vi(cid:75)}m+n−3
11 {(cid:74)wi(cid:75)}m+n−3
i=0 ← cPolyArr(d,(cid:74)k(cid:75));
a(cid:75) ← ((cid:87)m+n−2
(cid:74)bi(cid:75) ⊕(cid:74)q0(cid:75)) · 2n;
12 (cid:74)g(cid:48)
13 (cid:74)ga(cid:75) ←(cid:80)m+n−3
((cid:74)qi+1(cid:75) ·(cid:74)vi(cid:75));
14 (cid:74)g(cid:48)
d(cid:75) ←(cid:74)r0(cid:75) · c;
15 (cid:74)gd(cid:75) ←(cid:80)m+n−3
((cid:74)ri+1(cid:75) ·(cid:74)wi(cid:75));
a(cid:75) +(cid:74)ga(cid:75) +(cid:74)g(cid:48)
16 (cid:74)u(cid:75) ←(cid:74)zy(cid:75) · ((cid:74)g(cid:48)
17 (cid:74)z(cid:75) ←(cid:74)zx(cid:75) ⊕(cid:74)zx(cid:75) ∧(cid:74)zy(cid:75) ∧(cid:74)t(cid:75) ∧ ((cid:74)ex(cid:75) < −(cid:74)u(cid:75));
18 return ((cid:74)z(cid:75),(cid:74)sx(cid:75),(cid:74)ex(cid:75) +(cid:74)u(cid:75))
d(cid:75) +(cid:74)gd(cid:75));
i=0
i=0
i=0
;
i=1
i=0
);
i=0
i=0
i=0
zero. Therefore, our approach for approximating these functions is to ﬁnd the most signiﬁcant non-zero bit
of the argument (line 6) and shift the argument to the left so that we shift out all leading zeroes and the
most signiﬁcant non-zero bit (line 9). On the resulting number we compute a separate polynomial for each
function and each possible position of the most signiﬁcant non-zero bit (lines 10 and 11). (In the algorithm,
we denote the array of polynomials for addition as a and the array of polynomials for subtraction as d.)
There are also two special cases which are not covered by the polynomials: if the argument is zero, and
if the most signiﬁcant non-zero bit of the argument is in the lowest possible position. If the argument is zero
then it means that x and y have an equal absolute value, in which case for addition we return as the value
of fa(x) = lg (1 + 2x) a constant representing 1 (line 12), and for subtraction we return 0 as the ﬁnal result.
If the most signiﬁcant non-zero bit of the argument is in the lowest possible position then for addition we
return as the value of fa(x) = lg (1 + 2x) a constant representing 1, and for subtraction we return as the
value of fd(x) = lg (1 − 2x) a constant c representing 2m + lg (1 − 2−2−n
) (line 14).
In lines 13 and 15 we pick the right interpolation result depending on the most signiﬁcant nonzero bit of
the argument. From this collection of constants and polynomial approximation results, we pick the correct
one based on whether we are performing addition or subtraction, and depending on the most signiﬁcant
non-zero bit of (cid:74)ex(cid:75) −(cid:74)ey(cid:75). In line 16 we pick the value u which is added to the exponent of the larger
logarithmic number to achieve the ﬁnal result. Note that if the smaller operand is zero then u is also set to
zero.
In case of subtraction, we check for underﬂow, and if the result of subtraction is smaller than is possible
to accurately represent with a non-zero number we round the result down to zero (line 17).
The approach to addition presented in Algorithm 13 results in precision which is reasonable but still far
from ideal. One way to perform precise logarithmic addition is based on formula x+y = x·lg (2 · 2y/x) where
we ﬁnd the sum of two numbers with the help of division, exponentiation, multiplication, and logarithm. In
order to achieve good precision with this method, the operands have to be cast up before computing the
sum. As this method involves a composition of exponentiation and logarithm, both performed on numbers
15
twice the size of the original inputs, it is extremely ineﬃcient, but it allows for near-ideal precision.
6 Analysis of error and performance
In this section, we compare golden and logarithmic numbers against existing implementations of ﬂoating-
point and ﬁxed-point representations. Comparisons are made in both performance and accuracy for diﬀerent
operations and bit-widths. We look at addition, multiplication, reciprocal and square root. For ﬂoating-
point and logarithmic numbers, we additionally measure the performance and accuracy of exponentiation
and natural logarithm.
We have implemented logarithmic and golden section numbers on the Sharemind SMC platform. We
chose Sharemind because of its maturity, tooling, and availability of ﬁxed-point and ﬂoating-point numbers.
As existing number systems were already implemented using Sharemind’s domain-speciﬁc language [17],
we decided to also use it for golden section and logarithmic representations. The protocol language pro-
vides us with directly comparable performance and allows to avoid many complexities that a direct C++
implementation would have.
To provide a clear overview of accuracy and speed trade-oﬀs, we measured the performance of each
number system on multiple bit-widths. Generally, higher bit-widths oﬀer us better accuracy for the cost of
performance.
We implemented three diﬀerent versions of logarithmic numbers: Lh, Ls and Ld (with h, s and d standing
for half, single and double precision). For Lh, we chose m = 6 and n = 16 (Section 5.1) so that it oﬀers
at least as much range and precision as IEEE 754 half-precision ﬂoating-point numbers and also aligns to a
byte boundary (2 + 6 + 16 = 24 bits or 3 bytes). For Ls, we chose m = 9 and n = 29 for a size of 40 bits
and accuracy comparable to single-precision ﬂoating-point numbers. For Ld, we chose m = 12 and n = 58
for a size of 72 bits and accuracy comparable to double-precision ﬂoating-point numbers.
We also implemented tree versions of golden numbers (Section 4): G32, G64 and G128 where for Gn we
store two n-bit components to provide comparable accuracy to n-bit ﬁxed-point numbers with radix point
at (cid:98)n/2(cid:99).
We compare our results against existing secure real number implementations that Sharemind already
provides. Two ﬂoating-point number representations are used: ﬂoats, providing comparable accuracy
to single-precision ﬂoating-point numbers, and ﬂoatd, providing comparable accuracy to double-precision
ﬂoating-point numbers. See [13–15] for implementation details. Logarithmic numbers compare well with
ﬂoating-point numbers as both are designed to provide good relative errors. Additionally, Sharemind pro-
vides 32-bit and 64-bit ﬁxed-point numbers with radix point in the middle (denoted with ﬁx32 and ﬁx64
respectively). Golden numbers compare well with ﬁxed-point numbers as both are designed to provide good
absolute errors.
Accuracy was measured experimentally, by identifying the range of inputs in which the largest errors
should be found, and then uniformly sampling this range to ﬁnd maximum error.
Performance measurements were made on a cluster of three computers connected with 10Gbps Ether-
net. Each cluster node was equipped with 128GB DDR4 memory and two 8-core Intel Xeon (E5-2640 v3)
processors, and was running Debian 8.2 Jessie with memory overcommit and swap disabled.
We measured each operation on various input sizes, executing the operation in parallel on the inputs.
Each measurement was repeated a minimum of ten times and the mean of the measurements was recorded.
Measurements were performed in randomized order. Note that due to the networked nature of the protocols,
parallel execution improves performance drastically up to the so called saturation point.
We recorded maximum achieved operations per second that states how many parallel operations can be
evaluated on given input size per second. For example, if we can perform a single parallel operation on
100-element vectors per second this gives us 100 operations per second.
Our performance and accuracy measurements are displayed in Figure 1. For every variant of every real
number representation, we plot its maximum achieved performance in operations per second (OP/s) on the
y-axis and its error on the x-axis. Note that performance increases on the y-axis and accuracy improves on
the x-axis. Logarithmic numbers are represented with squares, golden section with diamonds, ﬂoating-point
16
numbers with circles and ﬁxed-point numbers with triangles. The accuracy of operations increases with
shade, so that white shapes denote least precision and best performance.
We have measured addition, multiplication, reciprocal and square root. For ﬂoating-point and logarithmic
numbers we also benchmarked exponentiation and natural logarihm. In most cases, maximum relative error
was measured, but for ﬁxed-point numbers and some golden number protocols this is not reasonable. In
these cases maximum absolute error was measured, and this is denoted by adding label “A” to the mark.
Some operations, such as ﬁxed-point addition, achieve perfect accuracy within their number representa-
tion. These cases are marked with a “(cid:63)”. Instead of maximum error, we plot the value of half of the step
between two consecutive numbers in this representation
In Figure 1 we can see that golden section numbers compare relatively well with ﬁxed-point numbers.
They achieve somewhat worse performance in our aggregated benchmarks, but the true picture is actually
more detailed.
What is not reﬂected on the graph is the fact that golden section multiplication requires signiﬁcantly
fewer communication rounds than ﬁxed-point multiplication. For instance, ﬁx32 multiplication requires 16
communication rounds, but comparable G64 multiplication requires only 11. This makes golden section
numbers more suitable for high latency and high throughput situations, and also better for applications that
perform many consecutive operations on small inputs.
The worse performance of golden section numbers after the saturation point can be wholly attributed to
increased communication cost. Namely, every multiplication of G64 numbers requires 6852 bits of network
communication, but a single ﬁx32 multiplication requires only 2970.
We can also see that compared to ﬂoating-point numbers, logarithmic numbers perform signiﬁcantly
better in case of multiplication (Figure 1b), reciprocal (Figure 1d) and square root (Figure 1c), while oﬀering
similar accuracy. Unfortunately, logarithmic numbers do not compare favourably to ﬂoating-point numbers
with both lower performance and worse accuracy of addition (Figure 1a). In case of natural logarithm and
exponentiation, logarithmic numbers are close to ﬂoating-point numbers in both performance and accuracy.
This means that logarithmic numbers are a poor choice for applications that are very addition heavy but an
excellent choice for applications that require many multiplicative operations.
7 Conclusions and further work
Technically, protected computation domains are very diﬀerent from the classical open ones. Many low-level
bit manipulation techniques are too cumbersome to implement and hence standard numeric algorithms do
not work very well.
This holds true even for basic arithmetic operations. A full IEEE 754 ﬂoating-point number speciﬁcation
is too complex to be eﬃcient in an oblivious setting. Even a reimplementation of the signif icand· 2exponent
representation is too slow, even in case of simple addition, since oblivious radix point alignment is very
ineﬃcient. Hence, alternatives need to be studied.
This paper proposed two new candidate representations for oblivious real numbers – golden and loga-
rithmic representations. The corresponding algorithms were implemented on the Sharemind SMC engine
and benchmarked for various precision levels and input sizes.
The results show that we still do not have a clear winner.
Since logarithmic representation is multiplicative, adding two logarithmic numbers is slow. However,
signiﬁcant performance improvements can be achieved for several elementary functions like multiplication,
inverse, and square root.
Golden number representation allows for very fast (actually, local) addition, and its multiplication speed is
comparable with that of ﬁxed-point numbers. However, this format only allows for relatively slow elementary
function computations.
Thus the choice of real number representation depends on application domain and computations to be
performed.
Another aspect to consider is precision. Our analysis shows that logarithmic representation achieves the
best relative error for most of the operations (except addition). However, precision achieved by our other
17
implementations seems more than suﬃcient for practical statistical applications.
In this paper we have developed only the most basic mathematical tools.
In order to be applied to
actual data analysis tasks (e.g. statistical tests, ﬁnding correlation coeﬃcients, variances, etc.), higher-level
operations need to be implemented. It is an interesting open question which real number representations
perform optimally for various operations and input sizes. This study will be a subject for our future research.
8 Acknowledgements
This research has been supported by Estonian Research Council under the grant no. IUT27-1.
References
[1] Mehrdad Aliasgari, Marina Blanton, Yihua Zhang, and Aaron Steele. Secure computation on ﬂoating
point numbers. In NDSS, 2013.
[2] David W. Archer, Dan Bogdanov, Benny Pinkas, and Pille Pullonen. Maturity and performance of
programmable secure computation. Cryptology ePrint Archive, Report 2015/1039, 2015. http://
eprint.iacr.org/. Journal version accepted to IEEE Security & Privacy, to appear in 2017.
[3] Mihir Bellare, Viet Tung Hoang, and Phillip Rogaway. Foundations of garbled circuits. In Proceedings
of the 2012 ACM conference on Computer and communications security, pages 784–796. ACM, 2012.
[4] Michael Ben-Or, Shaﬁ Goldwasser, and Avi Wigderson. Completeness theorems for non-cryptographic
In Proceedings of the twentieth annual ACM symposium on
fault-tolerant distributed computation.
Theory of computing, pages 1–10. ACM, 1988.
[5] George Robert Blakley. Safeguarding cryptographic keys. In Proceedings of the 1979 AFIPS National
Computer Conference, pages 313–317, 1979.
[6] Octavian Catrina and Sebastiaan De Hoogh. Improved primitives for secure multiparty integer compu-
tation. In Security and Cryptography for Networks, pages 182–199. Springer, 2010.
[7] Octavian Catrina and Sebastiaan De Hoogh. Secure multiparty linear programming using ﬁxed-point
arithmetic. In Computer Security–ESORICS 2010, volume 6345 of Lecture Notes in Computer Science,
pages 134–150. Springer, 2010.
[8] Octavian Catrina and Amitabh Saxena. Secure computation with ﬁxed-point numbers. In Financial
Cryptography and Data Security, volume 6052 of Lecture Notes in Computer Science, pages 35–50.
Springer, 2010.
[9] David Chaum, Claude Cr´epeau, and Ivan Damg˚ard. Multiparty unconditionally secure protocols. In
Proceedings of the twentieth annual ACM symposium on Theory of computing, pages 11–19. ACM, 1988.
[10] Martin Franz and Stefan Katzenbeisser. Processing encrypted ﬂoating point signals. In Proceedings of
the thirteenth ACM multimedia workshop on Multimedia and security, pages 103–108. ACM, 2011.
[11] Craig Gentry. A fully homomorphic encryption scheme. PhD thesis, Stanford University, 2009.
[12] Oded Goldreich, Silvio Micali, and Avi Wigderson. How to play any mental game. In Proceedings of
the nineteenth annual ACM symposium on Theory of computing, pages 218–229. ACM, 1987.
[13] Liina Kamm and Jan Willemson. Secure ﬂoating point arithmetic and private satellite collision analysis.
International Journal of Information Security, 14(6):531–548, 2015.
18
[14] Liisi Kerik, Peeter Laud, and Jaak Randmets. Optimizing MPC for robust and scalable integer and
ﬂoating-point arithmetic. LNCS. Springer, 2016. Accepted to Workshop on Applied Homomorphic
Cryptography 2016.
[15] Toomas Krips and Jan Willemson. Hybrid model of ﬁxed and ﬂoating point numbers in secure multiparty
computations. In Information Security: 17th International Conference, ISC 2014, volume 8783 of LNCS,
pages 179–197. Springer, 2014.
[16] Toomas Krips and Jan Willemson. Point-counting method for embarrassingly parallel evaluation in
secure computation. In FPS 2015, volume 9482 of LNCS, pages 66–82. Springer, 2016.
[17] Peeter Laud and Jaak Randmets. A Domain-Speciﬁc Language for Low-Level Secure Multiparty Com-
putation Protocols. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Commu-
nications Security, 2015, pages 1492–1503. ACM, 2015.
[18] Yehuda Lindell and Benny Pinkas. A proof of security of Yao’s protocol for two-party computation.
Journal of Cryptology, 22(2):161–188, 2009.
[19] Martin Pettai and Peeter Laud. Automatic Proofs of Privacy of Secure Multi-party Computation
Protocols against Active Adversaries. In IEEE 28th Computer Security Foundations Symposium, CSF
2015, Verona, Italy, 13-17 July, 2015, pages 75–89. IEEE, 2015.
[20] Pille Pullonen and Sander Siim. Combining Secret Sharing and Garbled Circuits for Eﬃcient Private
IEEE 754 Floating-Point Computations. In FC 2015 Workshops, volume 8976 of LNCS, pages 172–183.
Springer, 2015.
[21] David Russell Schilling. Knowledge doubling every 12 months, soon to be every 12 hours.
Indus-
try Tap, 2013. http://www.industrytap.com/knowledge-doubling-every-12-months-soon-to-be-
every-12-hours/3950.
[22] Adi Shamir. How to share a secret. Communications of the ACM, 22(11):612–613, 1979.