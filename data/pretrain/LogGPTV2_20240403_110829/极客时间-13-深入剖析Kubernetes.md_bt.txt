# 在宿主机上执行$ iptables -t raw -A OUTPUT -p icmp -j TRACE$ iptables -t raw -A PREROUTING -p icmp -j TRACE通过上述设置，你就可以在 /var/log/syslog里看到数据包传输的日志了。这一部分内容，你可以在课后结合[iptables的相关知识](https://en.wikipedia.org/wiki/Iptables)进行实践，从而验证我和你分享的数据包传递流程。熟悉了 docker0 网桥的工作方式，你就可以理解，在默认情况下，**被限制在Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 +宿主机网桥的方式，实现了跟同其他容器的数据交换。**与之类似地，当你在一台宿主机上，访问该宿主机上的容器的 IP地址时，这个请求的数据包，也是先根据路由规则到达 docker0网桥，然后被转发到对应的 Veth Pair设备，最后出现在容器里。这个过程的示意图，如下所示：![](Images/9ec4d646fcd4474b4c0c9c09cab1cebb.png){savepage-src="https://static001.geekbang.org/resource/image/9f/01/9fb381d1e49318bb6a67bda3f9db6901.png"}\同样地，当一个容器试图连接到另外一个宿主机时，比如：ping10.168.0.3，它发出的请求数据包，首先经过 docker0网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理。所以接下来，这个数据包就会经宿主机的 eth0网卡转发到宿主机网络上，最终到达 10.168.0.3对应的宿主机上。当然，这个过程的实现要求这两台宿主机本身是连通的。这个过程的示意图，如下所示：![](Images/c160b98a4944d89761bdc10016ffd37b.png){savepage-src="https://static001.geekbang.org/resource/image/90/95/90bd630c0723ea8a1fb7ccd738ad1f95.png"}\所以说，**当你遇到容器连不通"外网"的时候，你都应该先试试 docker0网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的iptables 规则是不是有异常，往往就能够找到问题的答案了。**不过，在最后一个"Docker容器连接其他宿主机"的例子里，你可能已经联想到了这样一个问题：如果在另外一台宿主机（比如：10.168.0.3）上，也有一个Docker 容器。那么，我们的 nginx-1 容器又该如何访问它呢？这个问题，其实就是容器的"跨主通信"问题。在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的docker0网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了。不过，万变不离其宗。如果我们通过软件的方式，创建一个整个集群"公用"的网桥，然后把集群里的所有容器都连接到这个网桥上，不就可以相互通信了吗？说得没错。这样一来，我们整个集群里的容器网络就会类似于下图所示的样子：![](Images/c4ff6da4293ca8c28854dc22547d8cce.png){savepage-src="https://static001.geekbang.org/resource/image/b4/3d/b4387a992352109398a66d1dbe6e413d.png"}\可以看到，构建这种容器网络的核心在于：我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。所以，这种技术就被称为：OverlayNetwork（覆盖网络）。而这个 Overlay Network本身，可以由每台宿主机上的一个"特殊网桥"共同组成。比如，当 Node 1 上的Container 1 要访问 Node 2 上的 Container 3 的时候，Node 1上的"特殊网桥"在收到数据包之后，能够通过某种方式，把数据包发送到正确的宿主机，比如Node 2 上。而 Node 2上的"特殊网桥"在收到数据包后，也能够通过某种方式，把数据包转发给正确的容器，比如Container 3。甚至，每台宿主机上，都不需要有一个这种特殊的网桥，而仅仅通过某种方式配置宿主机的路由表，就能够把数据包转发到正确的宿主机上。这些内容，我在后面的文章中会为你一一讲述。
## 总结在今天这篇文章中，我主要为你介绍了在本地环境下，单机容器网络的实现原理和docker0 网桥的作用。这里的关键在于，容器要想跟外界进行通信，它发出的 IP 包就必须从它的Network Namespace 里出来，来到宿主机上。而解决这个问题的方法就是：为容器创建一个一端在容器里充当默认网卡、另一端在宿主机上的Veth Pair 设备。上述单机容器网络的知识，是后面我们讲解多机容器网络的重要基础，请务必认真消化理解。
## 思考题尽管容器的 Host Network模式有一些缺点，但是它性能好、配置简单，并且易于调试，所以很多团队会直接使用Host Network。那么，如果要在生产环境中使用容器的 Host Network模式，你觉得需要做哪些额外的准备工作呢？感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。![](Images/e870b7df0db49509e735e6becd4a9a9a.png){savepage-src="https://static001.geekbang.org/resource/image/47/55/47a6f3bf6b92d58512d5a2ed0a556f55.jpg"}
# 33 \| 深入解析容器跨主机网络你好，我是张磊。今天我和你分享的主题是：深入解析容器跨主机网络。在上一篇文章中，我为你详细讲解了在单机环境下，Linux容器网络的实现原理（网桥模式）。并且提到了，在 Docker的默认配置下，不同宿主机上的容器通过 IP 地址进行互相访问是根本做不到的。而正是为了解决这个容器"跨主通信"的问题，社区里才出现了那么多的容器网络方案。而且，相信你一直以来都有这样的疑问：这些网络方案的工作原理到底是什么？要理解容器"跨主通信"的原理，就一定要先从 Flannel 这个项目说起。Flannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel的后端实现。目前，Flannel 支持三种后端实现，分别是：1.  VXLAN；2.  host-gw；3.  UDP。这三种不同的后端实现，正代表了三种容器跨主网络的主流实现方法。其中，host-gw模式，我会在下一篇文章中再做详细介绍。而 UDP 模式，是 Flannel项目最早支持的一种方式，却也是性能最差的一种方式。所以，这个模式目前已经被弃用。不过，Flannel之所以最先选择 UDP模式，就是因为这种模式是最直接、也是最容易理解的容器跨主网络实现。``{=html}所以，在今天这篇文章中，[我会先从 UDP模式开始，来为你讲解容器"跨主网络"的实现原理。]{.orange}在这个例子中，我有两台宿主机。-   宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是    100.96.1.2，对应的 docker0 网桥的地址是：100.96.1.1/24。-   宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是    100.96.2.3，对应的 docker0 网桥的地址是：100.96.2.1/24。我们现在的任务，就是让 container-1 访问 container-2。这种情况下，container-1 容器里的进程发起的 IP 包，其源地址就是100.96.1.2，目的地址就是 100.96.2.3。由于目的地址 100.96.2.3 并不在 Node1 的 docker0 网桥的网段里，所以这个 IP包会被交给默认路由规则，通过容器的网关进入 docker0网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。这时候，这个 IP包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示：    