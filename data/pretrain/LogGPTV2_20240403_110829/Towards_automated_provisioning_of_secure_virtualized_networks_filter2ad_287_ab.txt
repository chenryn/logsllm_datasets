enforce TVD policies, and describe the management of the
TVD infrastructure. Here, we focus on the static behavior
of a secure network virtualization framework that is already
up and running. Later, in Section 5, we focus on the more
dynamic aspects of the framework, including establishment
and deployment of the secure virtual infrastructure. The
main TVD VPEs we are concerned about are VMs; for the
remainder of the paper, we use the term VPE and VM syn-
onymously.
4.1 Network Virtualization Aims
The main aim of our network virtualization extensions is
to allow groups of related VMs running on separate physical
machines to be connected together as though they were on
their own separate network fabric. In particular, we would
like to be able to create arbitrary virtual network topologies
independently of the particular underlying physical network
topology. For example, we would like groups of related VMs
to be connected directly together on the same virtual LAN
segment even though, in reality, they may be at opposite
ends of a WAN link, separated by many physical LAN seg-
ments. As another example, multiple segmented virtual net-
works may have to be established on a single physical net-
work segment to achieve improved security properties and
protection.
Our network virtualization extensions must also be inter-
operable with existing non-virtualized entities (e.g., stan-
dard client machines on the Internet) and allow our virtual
networks to connect to real networks.
4.2 Networking Components
One option for virtual networking is to virtualize at the
IP level. However, to avoid problems regarding the support
for non-IP protocols and IP support services (such as ARP)
that sit directly on top of the Ethernet protocol, we have
chosen to virtualize at the Ethernet level.
Our secure network virtualization framework allows mul-
tiple VMs belonging to diﬀerent TVDs to be hosted on a
single physical machine. The framework obtains isolation
among various TVDs using a combination of virtual LANs
(VLANs) and virtual private networks (VPNs). There is one
internal VLAN for each TVD; an external VLAN may be
used for communication with other TVDs and TVD-external
entities. In the absence of a trusted underlying physical net-
work, each VLAN segment (i.e., an Ethernet broadcast do-
main, as in our case) may employ an optional VPN layer to
provide authentication, integrity, and conﬁdentiality prop-
erties.
The networking infrastructure consists of a mixture of vir-
tual entities and physical entities. Virtual entities include
VMs, vSwitches, VLAN taggers, VPN, and gateways. Phys-
ical entities include the physical hosts and the physical net-
working infrastructure, which includes VLAN-enabled phys-
ical switches, routers, and ordinary Ethernet switches.
Virtual Ethernet cards or vNICs are the basic building
blocks of our design. Each VM can have one or more vNICs.
Each vNIC can be associated with at most one VLAN.
Each virtual LAN segment is represented by a virtual
switch or a vSwitch. A VM appears on a particular VLAN if
one of its vNICs is “plugged” into one of the switch ports on
the vSwitch forming that segment. The vSwitch behaves like
a normal physical switch. Ethernet broadcast traﬃc gener-
ated by a VM connected to the vSwitch is passed to all VMs
connected to that vSwitch. Like a real switch, the vSwitch
also builds up a forwarding table based on observed traﬃc
so that non-broadcast Ethernet traﬃc can be delivered in a
point-to-point fashion to improve bandwidth eﬃciency.
The vSwitch is designed to operate in a distributed fash-
ion. The VMM on each physical machine hosting a VM
connected to a particular VLAN segment hosts part of the
vSwitch forming that VLAN segment. A component of the
VMM captures the Ethernet frames coming out of a VM’s
vNIC. The component is conﬁgured to know which vSwitch
the VM is supposed to be connected to. We describe the
vSwitch implementation in detail in Section 6.
The VM Ethernet frames are encapsulated in IP packets
or tagged with VLAN identiﬁers. The actual encapsulation
is performed by an encapsulation module on request by the
vSwitch. The vSwitch component then maps the Ethernet
address of the encapsulated Ethernet frame to an appropri-
ate IP address. The mapping allows the encapsulated Eth-
ernet frame to be transmitted over the underlying physical
network to physical machines hosting other VMs connected
to the same physical LAN segment. The result is the same
as when all VMs on the VLAN segment are connected by
a real LAN. The IP address chosen to route the encapsu-
lated Ethernet frames over the underlying physical network
depends on (1) whether the encapsulated Ethernet frame is
an Ethernet broadcast frame, and (2) whether the vSwitch
has built up a table of the locations of the physical machines
hosting other VMs on a particular LAN segment based on
observing traﬃc on that LAN.
IP packets encapsulating broadcast Ethernet frames are
given a multicast IP address and sent out over the physical
network. Each VLAN segment has an IP multicast address
associated with it. All physical machines hosting VMs on
a particular VLAN segment are members of the multicast
group for that VLAN segment. This ensures that all VMs on
a particular VLAN segment receive all broadcast Ethernet
frames from other VMs on that segment, whereas VMs on
a diﬀerent VLAN segment do not.
Encapsulated Ethernet frames that contain a directed Eth-
ernet destination address are either ﬂooded to all the VMs
on a particular LAN segment (using the IP multicast ad-
dress as in the broadcast case) or sent to a speciﬁc physical
machine IP address. The particular choice depends upon
whether the vSwitch component on the encapsulating VM
has learned the location of the physical machine hosting the
VM with the given Ethernet destination address based on
traﬃc observation through the vSwitch. Each learned lo-
cation is recorded in the forwarding table. Each vSwitch
updates its forwarding table as it receives packets from tar-
get VMs as responses to the multicasts. A particular learned
entry in the forwarding table consists of a VM identiﬁer and
the MAC address of the physical machine that hosts it.
Encapsulating Ethernet frames from VMs within IP pack-
ets allows us to connect diﬀerent VMs to the same VLAN
segment as long as the physical machines hosting these VMs
Host-1
Host-2
Host-3
Host-4
VMs
& vNICs
vSwitches
VPN
EtherIP
encapsulation
VLAN Tagging
E
t
h
e
r
I
P
T
a
g
g
e
r
T
a
g
g
e
r
VLAN-enabled
Physical Switch
V
P
N
E
t
h
e
r
I
P
T
a
g
g
e
r
T
a
g
g
e
r
E
t
h
e
r
I
P
T
a
g
g
e
r
T
a
g
g
e
r
V
P
N
E
t
h
e
r
I
P
E
t
h
e
r
I
P
TCP/IP
Figure 2: Components of the Secure Virtual Net-
working Infrastructure.
have some form of IP-based connectivity (e.g., a WAN link)
between them. There are no restrictions on the topology of
the underlying physical network.
We employ VLAN tagging, an existing technology, as an
alternative to Ethernet encapsulation for eﬃciency purposes.
Each VLAN segment may employ its own VLAN tagger(s)
to tag its Ethernet frames. The VLAN identiﬁer, which is
unique for each VLAN within a virtual network, is used as
tagging information. The tag is then used by the VLAN
switch to distinguish traﬃc ﬂows from the various VLAN
segments that connect to the switch.
A VLAN-enabled physical switch (or a VLAN switch, for
short) connects two or more VLAN segments belonging to
the same VLAN. VLAN switches should not to be confused
with vSwitches. VLAN switches are part of the physical
networking infrastructure, whereas vSwitches are virtual en-
tities. Each VLAN segment is connected to a port on the
VLAN switch. Multiple VLANs (i.e., VLAN segments be-
longing to diﬀerent TVDs) may also connect to the same
VLAN switch. The VLAN switch must be appropriately
conﬁgured to guarantee isolation among segments belong-
ing to diﬀerent VLANs, while at the same time connecting
physical machines, VMs, and vSwitches on the same VLAN
to each other.
Routing within Virtual Networks: Routing functionality
within a virtual network may be implemented by the use of a
dedicated VM with multiple vNICs. The vNICs are plugged
into ports on the diﬀerent vSwitches between which the VM
has to provide routing services. Standard routing software
is then conﬁgured and run on the VM to provide the desired
routing services between the LAN segments connected.
Communication with Non-Virtualized Systems: Gateways
enable communication with systems that live in the non-
virtualized world. The gateway is simply a VM with two
vNICs. One of the vNICs is plugged into a port on a vSwitch.
The other vNIC is bridged directly onto the physical net-
work. The gateway has two main roles. Firstly, it advertises
routing information about the virtual network behind it so
that hosts in the non-virtualized world can locate the VMs
residing on the virtual network. Secondly, the gateway con-
FW
FW
FW
vMachines
& vPorts
vSwitch
Fabric
Interconnect 
(Blue)
Internal 
(Blue)
Figure 3: Internal- and Inter-connections for each
TVD Type.
verts packets to and from the encapsulated format required
by our virtual networks.
4.3 Composition of Secure Virtual Networks
Figure 2 shows how the networking components can be
composed into a secure networking infrastructure that pro-
vides isolation among diﬀerent TVDs, where each TVD is
represented by a diﬀerent color (red (solid), green (dashed),
or blue (double) line). A non-virtualized physical host, such
as Host-3, is directly connected to a VLAN-enabled physi-
cal switch without employing a vSwitch. Further, a VM can
be connected to multiple VLAN segments using a diﬀerent
vNIC for each VLAN segment; hence, the VM can be a mem-
ber of multiple TVDs simultaneously. For example, the lone
VM in Host-2 of Figure 2 is part of two VLAN segments,
each represented by a vSwitch with a diﬀerent color; hence,
the VM is a member of both the blue and green TVDs.
Abstractly speaking, it is as if our secure virtual network-
ing framework provides colored networks (in which a diﬀer-
ent color means a diﬀerent TVD) with security guarantees
(such as conﬁdentiality, integrity, and isolation) to higher
layers of the virtual infrastructure.
Internally, the frame-
work provides the security guarantees through admission
control and the appropriate composition and conﬁguration
of VLANs, VPNs, gateways, routers, and other networking
elements.
Ethernet frames originating from the source node are han-
dled diﬀerently depending on whether the source node is
virtualized and whether the destination node resides in the
same LAN. We illustrate frame-processing alternatives for
diﬀerent scenarios in Figure 2. For a virtualized domain
(e.g., Host-1), each frame is tagged using the IEEE 802.1Q
standard for VLAN tagging [2].
If the destination of the
Ethernet frame is a VM on another host that is connected
to the same VLAN-capable switch (e.g., another physical do-
main in a datacenter), this tag indicates the VLAN segment
to which the VM belongs. If the destination is a host that
resides outside the LAN domain (e.g., Host-4), the VLAN
tag forces the switch to bridge the connection to an outgoing
WAN line (indicated by the black (thick) line in the VLAN-
enabled physical switch of Figure 2) that is connected to
a router for further packet routing.
In this case, the VM
Ethernet frames are encapsulated in IP packets to indicate
the VLAN segment membership. Lastly, if a non-virtualized
physical host is directly connected to the VLAN switch (e.g.,
Host-3), no tagging is required for the outgoing connection
from the host’s domain. We provide more details on each
processing step in Section 6, where we describe our Xen-
based prototype implementation.
4.4 Inter-TVD Management
Central to the management and auto-deployment of TVDs
are entities called TVD masters. There is one TVD master
per TVD. We refer to the TVD master as a single logical en-
tity, although its implementation may be a distributed one.
On every host that may potentially host a VM belonging
to the TVD, there is a local delegate of the TVD master,
called the TVD proxy. TVD masters and TVD proxies are
trusted. Trusted Computing [22] provides a way of deter-
mining whether they are indeed trustworthy.
Inter-TVD management deals with the interchange fab-
ric for communication between TVDs, enforcement of inter-
TVD ﬂow control policies, external zones (IP versus Ether-
net), approval of admission requests by TVD-external enti-
ties (such as a new VM) to join the TVD, and linking such
entities with the appropriate TVD master.
Information ﬂow control between TVDs has two aspects:
physical topology and policies. Physically, each TVD is
implemented by at least two VLANs (Figure 3): an ex-
ternal VLAN and an internal VLAN. The external VLAN
(shown in Figure 3 by thin lines) serves as a backbone to
send/receive information to/from other TVDs. It is through
the external VLAN that a TVD proxy communicates with
the TVD master (as per the protocol described in Section 5.2)
before becoming a member of the TVD. The internal VLAN
(shown in Figure 3 by thick lines) connects machines that
are part of a TVD. Inter-TVD policies specify conditions un-
der which VLANs belonging to diﬀerent TVDs are allowed
to exchange information. The policies may be conveniently
represented by information ﬂow control matrices, such as
the one shown in Figure 1. For a given TVD, the policies
are stored at the TVD master, which then enforces them in
a distributed fashion through admission control and appro-
priate conﬁguration of ﬁrewalls and TVD proxies.
Having separate VLANs for TVD-internal and TVD-
external communication facilitates unrestricted communica-
tion within a TVD and the complete isolation of a TVD
from another TVD if the inter-TVD policy speciﬁed allows
no information ﬂow between the TVDs. Such is the case
for T V Dα and T V Dβ, according to the ﬂow control matrix
shown in Figure 1.
A cheaper alternative to the dual VLAN solution would be
to rely solely on trusted boundary elements such as ﬁrewalls
to enforce isolation. The resulting assurance may be some-
what lower than that of the dual VLAN solution, because
of the possibility of mis-conﬁguring the boundary elements.
As shown in Figure 1, inter-TVD communication can be
broadly classiﬁed into three types: (1) controlled connec-
tions, represented by policy entries in the matrix, (2) open
or unrestricted connections, represented by 1 elements in
the matrix, and (3) closed connections, represented by 0
elements in the matrix.
Controlled connections restrict the ﬂow between TVDs
based on speciﬁed policies. The policies are enforced at TVD
boundaries (at both TVDs) by appropriately conﬁgured ﬁre-
walls (represented in Figure 3 by entities marked FW). The
TVD master may push pre-checked conﬁgurations (derived
from TVD policies) into the ﬁrewalls during the establish-
ment of the TVD topology. If available, a management con-
sole at the TVD master may be used to manually set up
and/or alter the conﬁgurations of the ﬁrewalls. A TVD ﬁre-
wall has multiple virtual network interface cards, one card
for the internal VLAN that the ﬁrewall protects and one
additional card for each TVD that the members of the pro-
tected TVD want to communicate with.
TVD 