without self-interference.
For resolvers with public-facing ports, the attacker can just scan
the port as if there was only one name server since the kernel
does not check the destination IP address wrapped in the ICMP
probe. The only difference lies in the TxID brute-forcing, where
the attacker would inject multiple groups of 65,536 fake response
packets, where each group uses a spoofed IP of a different name
server. Due to the low number of name servers typically configured,
this additional load of packets is not really a fundamental hurdle.
In addition to the above, there is an optional step called â€œname
server pinningâ€ [45] that can further improve the success rate when
multiple name servers are encountered. In addition to previously
proposed techniques [45], we propose two new methods again
based on ICMP messages, i.e., either host/port unreachable or redi-
rect. In the case of BIND resolvers, every time when a query is
initiated, we can immediately flood 65,536 (representing the worst
cases. BIND uses only 23,232 ports by default) ICMP host/port un-
reachable messages containing all possible ephemeral ports with
a specific name serverâ€™s IP as the destination IP address in the
embedded IP header. This will cause BIND to give up a particular
name server in the duration of a query session (up to 10 seconds by
default [45]). This is because the OS will pass the host/port unreach-
able messages to BIND, which will make the subsequent decision to
forgo the name server (one of the 65536 guessed ports will match
the ephemeral port). Alternatively, we can apply targeted name
Session 12C: Traffic Analysis and Side Channels CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3408server muting as mentioned in Â§6.1 and targeted ICMP redirect to
achieve a similar effect.
In the case of Unbound, ICMP redirect can be used as described
above to mute specific name servers. This is because Unbound has
special logic to â€œblacklistâ€ name servers that are non-responsive
repeatedly [45]. Therefore, the ICMP redirect will have a prolonged
pinning effect beyond a single query session.
Multiple backend servers. Finally, large DNS resolvers tend to
have multiple backend servers behind a single frontend IP â€” usually
an anycast one, e.g., 8.8.8.8. These backend servers are the actual
workers that talk to the name servers and they are the ones that
maintain DNS caches. Therefore the backend servers should be
the actual attack target. An attacker can map out the IPs of the
backend servers by setting up an attacker-controlled name server
and issuing a query of the attacker-controlled domain. This will
create an additional challenge to the attacker, as a particular query
may get routed to a randomly selected backend IP not known to the
attacker. This will mean that the attacker needs to target ğ‘šÃ—ğ‘› pairs
of resolver backends and name servers, where ğ‘š is the number of
backend IPs and ğ‘› is the number of name servers. Otherwise, if the
attacker picks only a single backend server to attack, it will have
a reduced probability of 1
ğ‘š (assuming the probability of choosing
backend servers is uniformly distributed) to succeed. Fortunately,
when ğ‘š is large, it is typically a heavily distributed system that the
selection of the backend IPs is actually not random at all. Instead,
[45] indicates that it is typically based on location. In other words,
backend servers that are located closer to a name server will be
more likely to be picked for a given query (destined to the name
server). In such cases, the attacker only needs to target a small
number of backend servers simultaneously or even a single one
and is still able to achieve a decent success rate.
6.3 Dual-Stack Resolvers
As mentioned earlier in Â§5.1, the latest BIND and Unbound will
instruct the Linux kernel to ignore ICMP frag needed messages
for IPv4 sockets. Therefore, the vulnerability applies to only IPv6
sockets against them. In practice, both IPv4 and IPv6 are enabled
by default in recent Linux distributions (e.g., Ubuntu 20.04 and
Red Hat 7). Therefore, we need to understand how to target their
IPv6 sockets in the presence of IPv4 sockets. Specifically, BIND
and Unbound by default will query different name servers in a
round-robin fashion regardless of whether the IP address is IPv4 or
IPv6. As a result, we can apply the same strategy as outlined in Â§6.2
to handle them. Specifically, we can apply name server pinning to
cause the IPv4 name server to become non-responsive and never
(or rarely) used by a resolver.
6.4 Noises
Background traffic. There are two potential sources of background
traffic at the resolver that can influence the ephemeral port scan.
First, the victim resolver may have multiple outstanding queries at
the same time. During the port scan, it is possible that the ephemeral
port we find belongs to a different query. It is not a serious concern
for private-facing ports as they are â€œvisibleâ€ to only specific name
servers, and there are typically few, if any, outstanding queries
towards the same name server (in addition to the one triggered by
the attacker). However, it can affect the public-facing ports because
the ephemeral port of any outstanding query to any name server
can show up during a scan. Nevertheless, we point out that any of
the strategies described in Â§6.1 that can extend the attack window
will automatically mitigate this concern. This is because the out-
standing query triggered by the attacker would then last for much
longer (possibly seconds) while other ordinary queries will only
last for hundreds of milliseconds at most. Therefore, we can simply
confirm that the port lives long enough before deciding to brute
force the TxID.
Another type of background traffic is the benign ICMP error
messages a resolver may receive during a port scan. They can
create additional entries in the exception cache. This has little
impact on public-facing ports because the attack requires only one
entry to be created in the cache and it is highly unlikely that there
are many naturally-occurring ICMP errors that will hash into the
same bucket as the attackerâ€™s entry and evict it, during a short
time frame of an attack. For private-facing ports, the attack does
require all five exception entries in the same hash bucket to be
intact during the scan. However, it is still unlikely to have a hash
collision from benign ICMP messages during a short time period.
Even if it does occur in practice, it will just interfere with one attack
attempt (triggering a false positive) and the next attack attempt
will follow immediately.
Packet Losses. Although unlikely, if the probing ICMP containing
the correct ephemeral port happens to be lost, false negatives can
arise. In such cases, the attacker simply moves on to the next at-
tempt. If the loss is on the verification or verification reply packets,
it will not affect the attack since the attacker can easily notice and
retransmit the verification packet. This is because a verification
reply is always supposed to come back either fragmented or not
(depending on whether the ephemeral port is guessed correctly).
Packet Reordering. Reordering can cause false negatives on public-
facing port scans and both false positives and false negatives on
private-facing port scans. Specifically, if the verification packet
accidentally arrives before the ICMP probe containing the correct
ephemeral port, it will fail to detect the exception cache change
and lead to false negatives. Furthermore, if the private-facing port
is being scanned, such a false negative would mislead the attacker
into continuing the scan despite the fact that one of the planted
exceptions has already been evicted. This is guaranteed to lead
to a false positive in the scanning of the next batch of ports, as
the eviction will be detected by the next verification packet. To
mitigate such problems, a small time gap can be inserted between
the probing and the verification packets. To mitigate the risk of
false positives and flooding the resolver with too many packets,
we always double-check whether a detected port is a true positive
before deciding to brute force the TxID.
7 EVALUATION
To evaluate the efficiency of our attacks without causing real-world
damage, we tested the attack in a controlled environment with
different server configurations and simulated network conditions.
Overall, our attacks can succeed in minutes and have a near-perfect
success rate. Note that inferring private-facing ephemeral ports
Session 12C: Traffic Analysis and Side Channels CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea3409requires inferring the colliding IPs as described in Â§4.6. However,
since it is only a one-time effort for each resolver, the time used for
the attack does not include the time for inferring colliding IPs.
7.1 Resolver Attack
Attack setup. In this attack, we evaluate the power of the fragment
needed attack based on the private-facing port scan. There are 3
hosts involved in the attack: the attacker host, the victim resolver
and the name server, all of which are controlled by us. The attack
program is executed on the attacker host, which is a MacBook
running macOS (Darwin 19.6.0) and is connected to the victim
resolver via a wired router (1Gbps). The victim resolver is a PC (with
a single CPU of Intel Core i7-9700) running BIND 9.16.13 on Ubuntu
20.04 (Linux 5.11.16). The name server, where our domainâ€™s records
are kept, is hosted on AWS and also running BIND 9.16.13. The
attackerâ€™s host, and the victim resolver are at home and connected
to the name server via residential Internet and all of the traffic is
sent in IPv6. The goal of the attack is to poison the cache of the
victim resolver so that our own domainâ€™s A record will be altered
in the cache.
We conducted 9 groups of experiments to evaluate the impact
of the different server configurations, network conditions, and lev-
els of background query traffic on our attack as shown in Table 3.
Specifically, we first performed a baseline (ğµğ‘ğ‘ ğ‘’) attack, where the
attacking conditions are ideal. Then we changed one configuration
or network condition at a time to check how they would influence
the attack. Then, we tested the performance of our attack against a
more realistic configuration and network condition to simulate a
real-world scenario (ğ‘…ğ‘’ğ‘ğ‘™). Finally, we introduced the background
query traffic to the resolver and evaluate how the interfering query
traffic affects our attack. Specifically, in ğ‘…ğ‘’ğ‘ğ‘™1, we followed the
workload on a production resolver reported in SADDNS [45] with
70M queries per day, averaging at 810 queries per second. To sim-
ulate the worst-case scenario, the domains in these queries are
randomly sampled from the Alexa top 1M to reduce the cache hit,
leading to more open ports. In ğ‘…ğ‘’ğ‘ğ‘™2, we added another 10 queries
per second asking for the same domain that the attacker is trying
to poison (which would cause confusion to our port scan).
To stay stealthy, we limit the rate of our packets to 7k pps (in-
cluding both the probes and verification packets), which is 3.5k
ports scanned per second. Note that 7k pps applies to the port scan
phase only. During the TxID brute-forcing phase, we limit our brute
force speed to 40 kpps and 70 kpps for ğ‘…ğ‘’ğ‘ğ‘™1 and ğ‘…ğ‘’ğ‘ğ‘™2 (to compete
with the background traffic). We simulate varying degrees of packet
losses, jitters, and delays according to the representative numbers
reported on the Internet [19, 24]. Besides, we also evaluated how
the name server muting level and the number of name servers affect
our attack. Although the name server can be completely muted (i.e.,
100% muting level) using ICMP redirects as mentioned in Â§6.1, we
also evaluate the scenario where it is difficult to completely mute a
name server (e.g., leveraging response rate limit). As mentioned in
Â§4.7, we also studied the impact on the attack performance when
using different batch sizes (i.e., the number of ports scanned in a
batch).
Results. Overall, we find our attacks can succeed on average in 1.3
to 15.6 minutes, depending on the setup. Note that we consider a test
Bg.
Noise
NS
Mute
Level
Exp. Pkt.
Loss
Table 3: Resolver Attack Results
#
RTT
of
range
/ms
NS
0%
0.3-1.2 100% 1
Base
0.20% 0.3-1.2 100% 1
Loss
100% 1
0%
37-43
RTT
0.3-1.2
0%
50%
1
ML
0%
0.3-1.2 100% 3
NS
0.3-1.2 100% 1
0%
Batch
2
Real
0.20% 37-43
2
Real1 0.20% 37-43
Real2 0.20% 37-43
2
Batch
Size
(N)
1
1
1
1
1
0
0
0
0
0
0
0
810
80%
80%
80%
1024
1
1
1
810+10
Avg.
Time
/s
80
83
149
713
347
496
410
659
933
Succ.
Rate
20/20