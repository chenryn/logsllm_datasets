the ones propagating the intrusion to the detected intru-
sion symptoms. In this way, we can locate the buﬀers that
actively expand the infection, i.e., from the intrusion root
object to the one-step-down objects in the intrusion prop-
agation graph. Taking these buﬀers as ﬁne-grained taint
seed will provide us the precise and comprehensive infection
diagnosis.
There are several drawbacks for the one-step-forward-auditing
approach to identify the ﬁne-grained intrusion root. First,
it cannot provide any information regarding how the intru-
sion root object is compromised, so there is no way for the
infection analyzer to trace the intrusion breakin from the
very beginning. Second, it relies on an implicit assumption
that the backward tracking can extract at least all the one-
step-away infected objects from intrusion root in the depen-
dency graph. This can be generally true, but some intended
attackers aware of backward tracking could craft intrusion
programs to evade such kind of auditing.
Therefore, we provide an alternative way to identify the
ﬁne-grained intrusion root. We take advantage of the gen-
eral belief that the intrusion breakin should start from the
network packets, and try to associate some packets with the
infection propagation. If any packet processing information
ﬂow ﬁnally “contributes” to the infection propagation, we
301provide the receiving buﬀer or storing disk sectors of this
packet to the infection analyzer as taint seed. However,
tracking the everyday thousands of packets to production
workload server is generally infeasible.
Fortunately, we have already identiﬁed the system-object-
level intrusion root with intrusion propagation timestamp,
so we have a rough idea of where (the intrusion root ob-
ject) and when (before the timestamp) the intrusion breaks
into the victim system. Thus, we only need to track those
network packets sent to this intrusion root object (generally
an application) before the intrusion propagation timestamp.
Furthermore, it is feasible for system security technicians to
refer to the server ﬁrewall’s “whitelist” to ﬁlter the packets
from trustable remote identities. Finally, we can start mul-
tiple packet-auditing instances simultaneously on separate
analysing instrumentation platforms to further increase the
eﬃciency.
After a much smaller set of suspicious packets is identiﬁed,
we use the following technique to determine the actual intru-
sion packets. The dependency tracking engine can leverage
the translation engine to replay the server system execution.
During the high ﬁdelity replay, it audits the processing in-
formation ﬂow of those packets by applying instruction ﬂow
taint tracking since they entered the receiving buﬀers. If any
packet is manipulated by the intrusion root object and hits
the system-object-level intrusion propagation chains, then
the corresponding packet should be considered as the intru-
sion packet. Note that several packets instead of one may
contribute to one single intrusion. Generally, when one in-
trusion packet has been identiﬁed, we can rely on the iden-
tiﬁcation information contained in the header of that packet
to swiftly locate other intrusion packets if any. Once all the
intrusion packets are identiﬁed, we specify the buﬀers or the
disk sectors storing those packets as ﬁne-grained intrusion
root, i.e., taint seed.
3.4 Cross Layer Infection Diagnosis
The infection analyzer is implemented in the binary trans-
lation based, whole system emulator Qemu [3]. With an
abstract view of the hardware such as CPU registers, mem-
ory, and I/O devices, it enables down-to-byte comprehen-
sive infection diagnosis by auditing the emulated hardware.
However, the “semantic gap” [5] really exists because it lacks
the meaningful information of operating system semantics.
Thus, we also develop reconstruction engine to present the
infection diagnosis results both at the system object layer
and at the instruction layer.
Our infection analyzer works similarly to several exist-
ing systems using dynamic taint analysis [21], [13] and [7]:
auditing each instruction executed. Essentially, we capture
the data ﬂow dependences between instructions and certain
control ﬂow dependences such as switch and if-else state-
ments. We implement the reconstruction engine totally at
the VMM (virtual machine monitor) level without any in-
terference to the guest OS. Our approach is also diﬀerent
from static analysis of raw memory and sysmap reconstruc-
tion ([10] and [18]). Our reconstruction engine extracts the
system object semantics directly from CPU registers and
dynamically maps the kernel address space with kernel data
structures. Though this renders the reconstruction work a
little more diﬃcult, we can ensure the correctness of replay
and the security of the PEDA components.
4.
IMPLEMENTATION ISSUES OF PEDA SYS-
TEM
We implement PEDA prototype on Qemu-0.9.0 and Xen-
3.3.0 to demonstrate its capability of comprehensive intru-
sion analysis for production workload servers. On both of
them, we run the same image ﬁle with Linux kernel version
2.6.20 as Guest OS. The goal and design of PEDA system
pose various challenges, hence we discuss the implementa-
tion issues of PEDA system in the rest of this section.
4.1 Checkpointing and Non-deterministic Event
Logging
On Xen Domain 0 management console, we implement a
new command xm checkpoint. Once issued, xm checkpoint
is passed to Xend via XML RPC. Xend responses to this
checkpoint request by initializing a pre-checkpointing phase,
during which Xend coordinates with Xen hypervisor to start
recording raw memory and virtual disk contents. Moreover,
the hypervisor makes a shadow copy of all the following
“writes” to memory and disk. Then, Xend pauses the system
execution and establishes a stop-and-copy phase. During
the stop-and-copy phase, the hypervisor records CPU reg-
isters, interrupt controllers and etc. Simultaneously, Xend
commits all the shadow copy of “writes” to the memory and
virtual disk recorded during previous phase, and calls qemu-
dm to log other devices states such as NIC, VGA, keyboard,
DMA and etc. Thereafter, Xend calls qemu-dm to start au-
diting the following keyboard inputs, network header iden-
tiﬁcation information, and their arrival time at the unit of
CPU clicks. The router is also notiﬁed to start directing the
following network packets to both the server and the back-
end system separately. The backend system is modiﬁed to be
able to receive and record these redirected packets. In addi-
tion, the hypervisor will activate the system events auditing
functionality to record all the following system calls of the
server system execution for dynamic dependency tracking.
The whole system states from checkpoint together with key-
board inputs, packet identity, timing and system call records
during the system execution are transferred to backend sys-
tem through Gigabit Ethernet.
4.2 Translation Engine
Translation engine cannot handle hardware diversities, such
as X86 processor and AMD processor, or rtl8139 NIC and
e1000 NIC. Therefore, we pre-conﬁgure QEMU and Xen
device emulation module to emulate the same type of de-
vices for Guest OS, such as X86 processor, rtl8139 NIC and
etc. We also conﬁgure them to have the same amount of
memory and to share the same image ﬁle as virtual disk.
As a result, translation engine only needs to deal with the
emulation implementation incompatibility of the same de-
vice. For instance, considering the emulation diﬀerences
of IOAPIC (I/O Advanced Programmable Interrupt Con-
troller) between Xen-HVM and Qemu, we observe that the
signiﬁcant diﬀerences are the number of IOAPIC pins and
the deﬁnition of each redirectory entry. In order to elimi-
nate such kind of device emulation incompatibility, we refer
to the Intel IOAPCI datasheet [1] for the functionality of
each speciﬁc pin, and match them at the granularity of Xen
and Qemu device emulation code. Note that only the devices
emulated by Xen HVM itself require this kind of scaling and
this work needs to be done only once before our system is
302Figure 4: Runtime Overhead and Performance Degradation in terms of Throughput
deployed.
4.3 Infection Analyzer and Reconstruction En-
gine
As a binary-translation based emulator, Qemu enables our
implementation of instruction ﬂow taint analysis. Each ex-
ecuted instruction is audited before Qemu translation block
works to keep consistent with the view of Guest OS. Each
register, memory cell and disk block are associated with
one speciﬁc taint bit indicating whether this storage unit
is tainted or not. Speciﬁcally, when instructions are exe-
cuted, we apply tainting and untainting policies to examine,
set or clear the taint bit. Therefore, we can exactly locate
the taint propagation by checking the taint bit throughout
the registers, memory cells and disk blocks.
We also implement the reconstruction engine to provide
OS semantics. For instance, to dynamically reconstruct pro-
cess lists, we start from the structure env deﬁned by Qemu
to emulate the processor for VM. We can obtain all the reg-
isters there including the register esp pointing to the kernel
stack of the currently running process. At the bottom of the
kernel stack resides the thread info structure that includes
a pointer to the task struct of the corresponding process.
Similarly, with the help of other registers and the process
proﬁles, we can further identify the process list, all the ﬁle
objects and part of kernel data structures (almost 800) such
as system call table, interrupt table and etc.
5. EVALUATION
In this section, we evaluate PEDA system in terms of
logging eﬃciency and intrusion analysis comprehensiveness.
The Xen hypervisor and the backend system are running
separately on two Lenovo Thinkstation Tower D10 machines,
each with Dual Intel(R) PRO/1000 NICs. We run CentOS
5.2 (kernel version 2.6.18) as coordination platform on back-
end system and as Xen Domain 0 on Xen hypervisor. Both
Xen-HVM Domain U and Qemu Guest OS are installed with
Fedora 6 (kernel version 2.6.20). Both Qemu and Xen-HVM
are preconﬁgured to have the same amount of memory (1
GB), the same NAT based networking, and the same kind of
devices emulation for Guest OS or Domain U. We also deploy
a router, connecting these two machines with Gigabit Eth-
ernet and responsible for packets direction to them. Outside
the router, we have two Dell OptiPlex 745 machines used as
client request simulation and attacker platforms, with Giga-
bit Ethernet connection to the router.
5.1 Logging Efﬁciency
We install Apache 1.3.9 on Xen-HVM Domain U as a
http server. We evaluate the runtime overhead introduced
by non-deterministic events plus system call logging, and
measure the server performance degradation during the pre-
checkpointing phase and service downtime during the stop-
and-copy phase.
Runtime Overhead We compare the performances of
apache running on the native Xen-HVM Domain U and on
the Xen-HVM Domain U with our instrumentation. On the
Dell machines, we simulate clients sending continuous re-
quests over concurrent connections to fetch an 8 KB ﬁle.
Figure 4(a) shows that the native Xen-HVM Domain U
achieves the throughput of almost 800 Mbps, which is con-
sidered as baseline performance in our evaluation.
It did
not reach up to 2000 Mbps (Two Gigabit NICs on the ma-
chine) probably due to the impact of network I/O virtual-
ization introduced by Xen. The baseline performance can
be improved by optimizations proposed in [12] to achieve
the near-native throughput. Figure 4(b) shows the apache
throughput with the packets redirected to the logging back-
end system (from the edge router). Compared with Figure
4(a), we can see that our logging achieves about 95% base-
line performance, which the 5% runtime overhead is mainly
caused by the system call logging.
Downtime and Performance Degradation Caused
by Checkpointing In order to simulate the Amazon-style
server during 2 am-5 am, we reduce user requests to apache
server by 90%. Figure 4(c) shows that the server through-
put decreases correspondingly. At the time (about 5 seconds
from the very beginning) when we issued the command xm
checkpoint, the server throughput drops by 24%, which lasts
for almost 4 seconds. This performance degradation can be
explained by the fact that we introduced a pre-checkpointing
phase, during which the whole virtual disk and physical
memory are recorded. Following is the service downtime
(no throughput) during the “stop-and-copy” phase, which
only lasts for less than 0.4 second. To take a checkpoint of a
running system, the service downtime cannot be eliminated,
because it is generally impossible to take a consistent whole
system checkpoint considering the fact that a running sys-
tem may do “write” operations to either memory or disk.
We reduce this kind of service downtime by introducing a
pre-checkpointing phase, which takes the large amount of
copying workload from the “pausing” phase. Note that in
Figure 4(c), the short pulse immediately after the downtime
is likely to be caused by the accumulated requests from the
performance degradation period.
5.2
Intrusion Analysis Comprehensiveness
0369121502004006008001000 Baseline Throughput (Mbps)Time (s)(a)0369121502004006008001000(b)With Router  Throughput (Mbps)Time (s)03691215050100150200  (c)Throughput (Mbps)Time (s)303reconstruction engine, including the processes which the ad-
dress space belongs to, the ﬁles which the disk sectors are
allocated to, or the dynamic libraries which the memory ad-
dress space is loaded to. It is suﬃcient to demonstrate that
our cross-layer infection diagnosis features with speciﬁc in-
fected memory space, disk sectors, and kernel address space,
which are far beyond the system-call-level intrusion track-
ing. Moreover, the infected memory address information
provides the system admin a feasible way to “sweep out”
any intrusion harm on the victim system. In addition, we
can catch the “blind spot” of system-call-level intrusion anal-
ysis. For instance, we can capture how the attacker obtains
the root privilege through the intrusion packet.
5.2.2 Case Study 2
Case Study 2 is designed to demonstrate the advance of
PEDA system over other system-call-level intrusion analysis.
The attacker logs into the system by ssh using an unpriv-
ileged user account. Then he launches the sendmail local
escalation exploit to gain root access. The attacker uses the
root shell to download and install the adore rootkit, which
replaces several kernel hooks in the system call table with
its own implementation. Afterwards, he uses the same root