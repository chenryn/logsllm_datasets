opposite scenario where photos are taken in the background
and thus intention-based access control does not provide a
suitable defense.
Imaging defenses. There have been very few systems anal-
ogous to PlaceAvoider that seek to control the collection of
imagery. Truong et al. [53] describe a third-party system where
offending CCD or CMOS cameras in a space can be detected
and disabled via a directed pulsing light. While this system
provides an interesting and useful way to prevent the use of
cameras, it requires specialized and dedicated infrastructure
to be installed in each sensitive space. PlaceAvoider allows
similar functionality to be integrated within the camera.
The DARKLY system [26] presents a novel approach to
add a privacy-protection layer to systems where untrusted
applications have access to camera resources. DARKLY in-
tegrates OpenCV within device middleware to control
the
type and amount of image content available to applications.
This approach applies the principle of least privilege to image
information, albeit in a different manner than PlaceAvoider.
For example, a policy may exist that permits an application
only to have access to the number of faces detected in any
image. Regardless of context, when invoking the camera with
DARKLY, this application would receive only select parame-
terized image information (e.g., the number of detected faces).
PlaceAvoider, however, enforces policies based on image
context derived from image content. While solving different
problems, DARKLY and PlaceAvoider could potentially be
combined — e.g., analysis by PlaceAvoider could inform
transformations applied by DARKLY.
Inferring location from images. Inferring location or user
activity from smartphone sensors is an active research area.
CenceMe [39] uses ambient audio and movement information
to infer activity and conversation type, but simply uses the
GPS service for location — recorded images are not used
for classiﬁcation. CrowdSense@Place [9] does use computer
vision techniques (alongside processing of recorded audio) to
classify location amongst one of seven general categories (e.g.,
home, workplace, shops) — this system was not evaluated
for its ability to perform the speciﬁc scene recognition that
12
PlaceAvoider performs but this approach would be useful to
identify general types of locations where privacy risks are high.
Much of this work is in the computer vision domain
for robotics applications. Robot topological localization tech-
niques often require specialized cameras that are incompatible
with form factors used by phones and lifelogging devices. Se
et al. use a Triclops stereo vision camera for their localization
techniques with robots [49], [50]. Similarly, Ulrich and Nour-
bakhsh use a specialized 360-degree panoramic camera that
operates in a ﬁxed plane [54].
Even in the absence of such specialized cameras, local-
ization techniques for robot applications often leverage other
conditions that cannot be assumed for our use cases. Ledwich
and Williams offer a system that imposes strict constraints
on the training images that are unrealistic in the applications
that we propose [34]. Kosecka and Li propose a system [31]
that uses contiguous streams for training along with precision
odometry (instrumentation that measures distanced traveled
over time). Similarly, Jensfelt et al. developed a localization
system [27] that requires odometry or other dead-reckoning
sensors. While sensor arrangements on mobile devices are
increasing in sophistication and capability, these localization
solutions from the robotics domain are not directly applicable
given the dynamics of movement for mobile devices.
Recent work has studied geo-location in consumer images,
although most of this work has been limited to highly pho-
tographed outdoor landmarks where thousands or millions of
training images can be downloaded from the web [37], [51],
[20], [36]. An abstraction of absolute camera location seeks to
classify images based on the type of scene (e.g., indoors vs.
outdoors). Oliva and Torralba label scenes according to the
‘gist’ of the image by analyzing the distribution of spatial im-
age frequencies [41]. Subsequent work seeks ﬁner granularity
by classifying the type of scene at a high level (e.g., living
room vs. bedroom) [56], [45]. The majority of work has con-
sidered well-composed, deliberately-taken images, although
some very recent papers in the computer vision literature have
considered ﬁrst-person video. This work includes selecting
important moments from raw ﬁrst-person video [35], jointly
recognizing and modeling common objects [18], inferring the
camera owner’s actions from object interaction [43], and even
using ﬁrst-person video to collect psychological data about
people’s visual systems in naturalistic environments [3]. None
of this work considers privacy issues as we do here, although
in future work we plan to leverage some of these approaches
to assign semantic labels that have privacy meanings.
Indoor localization and positioning. The computational ex-
pense of inferring camera location with computer vision ap-
proaches applied to images may be mitigated partly through
localization and positioning methods to reduce search spaces.
A comprehensive survey of localization and positioning ap-
proaches is outlined by Hightower [23]. Most of these systems
require external infrastructure (e.g., audio or electromagnetic
beacons) or a dense constellation of cooperating devices [47],
and a priori knowledge of the environment (e.g., maps) is
often required. Some approaches rely less on infrastructure
and operate in a peer-to-peer ad hoc manner. Kourogi [32]
developed a system that requires no infrastructure, but uses
sensors that are much more sophisticated than what is available
in consumer mobile devices. Woodman et al. developed a
system [55] that performs effective localization, but requires
a sensor array afﬁxed to an individual’s foot. As discussed in
Section I, camera location and the location of image content
is not necessarily the same;
the PlaceAvoider classiﬁer is
necessary to enforce privacy policies based on image content.
VII. CONCLUSION
We believe that as cameras become more pervasive and
as the background collection of imagery becomes more pop-
ular, people’s privacy is put at increasingly greater risk. We
have presented an approach for detecting potentially sensi-
tive images taken from ﬁrst-person cameras in the face of
motion, blur, and occlusion, by recognizing physical areas
where sensitive images are likely to be captured. Owners of
cameras can review images from these sensitive regions to
avoid privacy leaks. We believe this is an important ﬁrst step
in this increasingly important area of privacy research.
Our results are promising and may be good enough for
some applications, but our classiﬁer accuracies are likely
insufﬁcient for others, and the problem of highly accurate
indoor visual place classiﬁcation from ﬁrst-person imagery
remains open. We plan to continue investigating computer
vision techniques that estimate meanings of images to better
identify potentially sensitive photo content and situations. We
also plan to investigate privacy concerns of bystanders — the
people being captured within the images — because as devices
like Google Glass become more common in society, bystanders
need ways to actively protect their own privacy.
ACKNOWLEDGMENT
This material is based upon work supported by the Na-
tional Science Foundation under grants CNS-1016603, CNS-
1252697 and IIS-1253549. This work was also partially funded
by the Ofﬁce of the Vice Provost of Research at Indiana
University Bloomington through the Faculty Research Support
Program. We thank the anonymous reviewers for their valuable
comments and John McCurley for his editorial help.
REFERENCES
[1] A. Allen, “Dredging up the past: Lifelogging, memory, and surveil-
lance,” The University of Chicago Law Review, pp. 47–74, 2008.
[2] S. Arya and D. Mount, “Approximate nearest neighbor queries in ﬁxed
dimensions,” in ACM Symposium on Discrete Algorithms, 1993.
[3] S. Bambach, D. Crandall, and C. Yu, “Understanding embodied visual
attention in child-parent interaction,” in Joint IEEE International Con-
ference on Development and Learning and and on Epigenetic Robots,
2013.
J. Brassil, “Technical challenges in location-aware video surveillance
privacy,” in Protecting Privacy in Video Surveillance. Springer, 2009,
pp. 91–113.
[4]
[5] S. Bugiel, L. Davi, A. Dmitrienko, T. Fischer, and A.-R. Sadeghi,
“XManDroid: A new Android evolution to mitigate privilege escalation
attacks,” Technische Universit¨at Darmstadt, Technical Report TR-2011-
04, Apr. 2011.
[6] S. Bugiel, L. Davi, A. Dmitrienko, T. Fischer, A.-R. Sadeghi, and
B. Shastry, “Towards taming privilege-escalation attacks on Android,”
in 19th Annual Network & Distributed System Security Symposium
(NDSS), Feb. 2012.
J. Chaudhari, S. Cheung, and M. Venkatesh, “Privacy protection for
life-log video,” in IEEE Workshop on Signal Processing Applications
for Public Security and Forensics, 2007, pp. 1–5.
[7]
13
[8] W. Cheng, L. Golubchik, and D. Kay, “Total recall: are privacy changes
inevitable?” in ACM Workshop on Continuous Archival and Retrieval
of Personal Experiences, 2004, pp. 86–92.
[9] Y. Chon, N. Lane, F. Li, H. Cha, and F. Zhao, “Automatically charac-
terizing places with opportunistic crowdsensing using smartphones,” in
ACM Conference on Ubiquitous Computing, 2012, pp. 481–490.
[10] M. Conti, V. T. N. Nguyen, and B. Crispo, “Crepe: context-related
policy enforcement for Android,” in International Conference on Infor-
mation Security, 2011, pp. 331–345.
[11] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human
Detection,” in IEEE Conference on Computer Vision and Pattern
Recognition, 2005, pp. 886–893.
[12] M. Dietz, S. Shekhar, Y. Pisetsky, A. Shu, and D. S. Wallach, “Quire:
Lightweight provenance for smart phone operating systems,” CoRR, vol.
abs/1102.2445, 2011.
[13] M. Douze, H. Jegou, H. Sandhawalia, L. Amsaleg, and C. Schmid,
“Evaluation of gist descriptors for web-scale image search,” in ACM
International Conference on Image and Video Retrieval, 2009.
[14] K. Duan, D. Batra, and D. Crandall, “A Multi-layer Composite Model
for Human Pose Estimation,” in British Machine Vision Conference,
2012.
[15] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung, P. McDaniel,
and A. N. Sheth, “TaintDroid: An information-ﬂow tracking system for
realtime privacy monitoring on smartphones,” in USENIX Conference
on Operating Systems Design and Implementation, 2010, pp. 1–6.
[16] W. Enck, M. Ongtang, and P. McDaniel, “Mitigating Android software
misuse before it happens,” Pennsylvania State University, Tech. Rep.
NAS-TR-0094-2008, 2008.
[17] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin, “Liblinear: A
library for large linear classiﬁcation,” The Journal of Machine Learning
Research, vol. 9, pp. 1871–1874, 2008.
[18] A. Fathi, X. Ren, and J. Rehg, “Learning to recognize objects in
egocentric activities,” in IEEE Conference on Computer Vision and
Pattern Recognition, 2011.
[19] A. P. Felt, H. J. Wang, A. Moshchuk, S. Hanna, and E. Chin, “Permis-
sion re-delegation: attacks and defenses,” in Proceedings of the USENIX
Conference on Security, 2011, pp. 22–22.
J.-M. Frahm, P. Georgel, D. Gallup, T. Johnson, R. Raguram, C. Wu,
Y.-H. Jen, E. Dunn, B. Clipp, and S. Lazebnik, “Building Rome on a
Cloudless Day,” in European Conference on Computer Vision, 2010.
[20]
[22]
[21] A. Gionis, P. Indyk, R. Motwani et al., “Similarity search in high
dimensions via hashing,” in IEEE International Conference on Very
Large Data Bases, vol. 99, 1999, pp. 518–529.
J. Halderman, B. Waters, and E. Felten, “Privacy management for
portable recording devices,” in ACM Workshop on Privacy in the
Electronic Society, 2004, pp. 16–24.
J. Hightower and G. Borriello, “Location systems for ubiquitous com-
puting,” Computer, vol. 34, no. 8, pp. 57–66, Aug. 2001.
[23]
[24] S. Hodges, L. Williams, E. Berry, S. Izadi, J. Srinivasan, A. Butler,
G. Smyth, N. Kapur, and K. Wood, “Sensecam: a retrospective memory
aid,” in ACM Conference on Ubiquiotous Computing, 2006.
[25] C. Hsu, C. Lu, and S. Pei, “Homomorphic encryption-based secure
sift for privacy-preserving feature extraction,” in IS&T/SPIE Electronic
Imaging, 2011.
[26] S. Jana, A. Narayanan, and V. Shmatikov, “A Scanner Darkly: Protecting
user privacy from perceptual applications,” in 34th IEEE Symposium on
Security and Privacy, 2013.
[27] P. Jensfelt, D. Kragic, J. Folkesson, and M. Bjorkman, “A framework
for vision based bearing only 3d slam,” in Robotics and Automation,
2006. ICRA 2006. Proceedings 2006 IEEE International Conference
on.
IEEE, 2006, pp. 1944–1950.
[28] T. Karkkainen, T. Vaittinen, and K. Vaananen-Vainio-Mattila, “I don’t
mind being logged, but want to remain in control: a ﬁeld study of mobile
activity and context logging,” in SIGCHI Conference on Human Factors
in Computing Systems, 2010, pp. 163–172.
[29] D. Koller and N. Friedman, Probabilistic Graphical Models: Principles
and Techniques. The MIT Press, 2009.
[31]
avatar captchas automatically,” in Advanced Machine Learning Tech-
nologies and Applications, 2012.
J. Koseck´a and F. Li, “Vision based topological markov localization,”
in Robotics and Automation, 2004. Proceedings. ICRA’04. 2004 IEEE
International Conference on, vol. 2.
IEEE, 2004, pp. 1481–1486.
[32] M. Kourogi and T. Kurata, “Personal positioning based on walking lo-
comotion analysis with self-contained sensors and a wearable camera,”
in IEEE and ACM International Symposium on Mixed and Augmented
Reality, 2003, pp. 103–112.
[33] S. Lazebnik, C. Schmid, and J. Ponce, “Beyond bags of features: Spatial
pyramid matching for recognizing natural scene categories,” in IEEE
Conference on Computer Vision and Pattern Recognition, 2006.
[34] L. Ledwich and S. Williams, “Reduced SIFT features for image retrieval
and indoor localisation,” in Australian Conference on Robotics and
Automation, 2004.
[35] Y. J. Lee, J. Ghosh, and K. Grauman, “Discovering important people
and objects for egocentric video summarization,” in IEEE Conference
on Computer Vision and Pattern Recognition, 2012.
[36] X. Li, C. Wu, C. Zach, S. Lazebnik, and J.-M. Frahm, “Modeling
and Recognition of Landmark Image Collections Using Iconic Scene
Graphs,” in European Conference on Computer Vision, 2008, pp. 427–
440.
[37] Y. Li, D. Crandall, and D. P. Huttenlocher, “Landmark Classiﬁcation in
Large-scale Image Collections,” in IEEE International Conference on
Computer Vision, 2009.
[38] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International Journal of Computer Vision, vol. 60, no. 2, pp. 91–110,
Nov. 2004.
[39] E. Miluzzo, N. Lane, K. Fodor, R. Peterson, H. Lu, M. Musolesi,
S. Eisenman, X. Zheng, and A. Campbell, “Sensing meets mobile social
networks: the design, implementation and evaluation of the cenceme ap-
plication,” in ACM Conference on Embedded Network Sensor Systems,
2008, pp. 337–350.
[40] M. Nauman, S. Khan, and X. Zhang, “Apex: Extending Android per-
mission model and enforcement with user-deﬁned runtime constraints,”
in ACM Symposium on Information, Computer and Communications
Security, 2010, pp. 328–332.
[41] A. Oliva and A. Torralba, “Modeling the shape of the scene: A
holistic representation of the spatial envelope,” International Journal
of Computer Vision, vol. 42, no. 3, pp. 145–175, 2001.
[42] M. Ongtang, K. Butler, and P. McDaniel, “Porscha: Policy oriented
secure content handling in Android,” in Annual Computer Security
Applications Conference, 2010, pp. 221–230.
[43] H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in
ﬁrst-person camera views,” in IEEE Conference on Computer Vision
and Pattern Recognition, 2012.
[44] G. Portokalidis, P. Homburg, K. Anagnostakis, and H. Bos, “Paranoid
Android: Versatile protection for smartphones,” in Annual Computer
Security Applications Conference, 2010, pp. 347–356.
[45] A. Quattoni and A. Torralba, “Recognizing indoor scenes,” in IEEE
Conference on Computer Vision and Pattern Recognition, 2009.
[46] F. Roesner, T. Kohno, A. Moshchuk, B. Parno, H. J. Wang, and
C. Cowan, “User-driven access control: Rethinking permission granting
in modern operating systems,” in IEEE Symposium on Security and
Privacy, 2012, pp. 224–238.
[47] A. Savvides, C. Han, and M. Strivastava, “Dynamic ﬁne-grained local-
ization in ad-hoc networks of sensors,” in International Conference on
Mobile Computing and Networking, 2001, pp. 166–179.
J. Schiff, M. Meingast, D. Mulligan, S. Sastry, and K. Goldberg,
“Respectful cameras: Detecting visual markers in real-time to ad-
dress privacy concerns,” in Protecting Privacy in Video Surveillance.
Springer, 2009, pp. 65–89.
[48]
[49] S. Se, D. Lowe, and J. Little, “Mobile robot localization and mapping
with uncertainty using scale-invariant visual landmarks,” International
Journal of Robotics Research, vol. 21, no. 8, pp. 735–758, 2002.
[50] ——, “Vision-based global localization and mapping for mobile robots,”
IEEE Transactions on Robotics, vol. 21, no. 3, pp. 364–375, 2005.
[30] M. Korayem, A. Mohamed, D. Crandall, and R. Yampolskiy, “Solving
[51] N. Snavely, S. Seitz, and R. Szeliski, “Modeling the World from Internet
14
Photo Collections,” International Journal of Computer Vision, vol. 80,
pp. 189–210, 2008.
[52] R. Templeman, Z. Rahman, D. Crandall, and A. Kapadia, “PlaceRaider:
Virtual theft in physical spaces with smartphones,” in Network and
Distributed System Security Symposium, 2013.
[53] K. Truong, S. Patel, J. Summet, and G. Abowd, “Preventing camera
recording by designing a capture-resistant environment,” in Interna-
tional Conference on Ubiquitous Computing, 2005, pp. 73–86.
I. Ulrich and I. Nourbakhsh, “Appearance-based place recognition for
topological localization,” in IEEE International Conference on Robotics
[54]
and Automation, 2000, pp. 1023–1029.
[55] O. Woodman and R. Harle, “Pedestrian localisation for indoor environ-
ments,” in International Conference on Ubiquitous Computing, 2008,
pp. 114–123.
J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba, “Sun database:
Large-scale scene recognition from abbey to zoo,” in IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 3485–
3492.
[56]
15