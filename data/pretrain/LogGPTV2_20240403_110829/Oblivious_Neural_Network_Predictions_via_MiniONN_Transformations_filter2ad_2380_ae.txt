### 4. Convolutional Neural Network (CNN) for MNIST

- **(4) Convolution Layer**: 
  - **Input Image Size**: 28 × 28
  - **Window Size**: 5 × 5
  - **Stride**: (1, 1)
  - **Number of Output Channels**: 16
  - **Output Shape**: R16×576

- **(5) ReLU Activation**: 
  - **Function**: Applies the ReLU activation function to each input element.

- **(6) Max Pooling**:
  - **Window Size**: 1 × 2 × 2
  - **Output Shape**: R16×4×4

- **(7) Fully Connected Layer**:
  - **Input Nodes**: 256
  - **Output Nodes**: 100
  - **Output Shape**: R100×1 ← R100×256 · R256×1

- **(8) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(9) Fully Connected Layer**:
  - **Input Nodes**: 100
  - **Output Nodes**: 10
  - **Output Shape**: R10×1 ← R10×100 · R100×1

**Figure 12: The neural network trained on the MNIST dataset.**

### 5. CNN for CIFAR-10

- **Image Classification Dataset**: CIFAR-10
  - **Dataset Characteristics**: Consists of RGB images (3 color channels, 32 × 32 pixels) of everyday objects in 10 classes.
  - **Training Set**: 50,000 images
  - **Test Set**: 10,000 images
  - **Prediction Accuracy**: 81.61%

- **(1) Convolution Layer**:
  - **Input Image Size**: 3 × 32 × 32
  - **Window Size**: 3 × 3
  - **Stride**: (1, 1)
  - **Padding**: (1, 1)
  - **Number of Output Channels**: 64
  - **Output Shape**: R64×32×32

- **(2) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(3) Convolution Layer**:
  - **Window Size**: 3 × 3
  - **Stride**: (1, 1)
  - **Padding**: (1, 1)
  - **Number of Output Channels**: 64
  - **Output Shape**: R64×32×32

- **(4) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(5) Mean Pooling**:
  - **Window Size**: 1 × 2 × 2
  - **Output Shape**: R64×16×16

- **(6) Convolution Layer**:
  - **Window Size**: 3 × 3
  - **Stride**: (1, 1)
  - **Padding**: (1, 1)
  - **Number of Output Channels**: 64
  - **Output Shape**: R64×16×16

- **(7) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(8) Convolution Layer**:
  - **Window Size**: 3 × 3
  - **Stride**: (1, 1)
  - **Padding**: (1, 1)
  - **Number of Output Channels**: 64
  - **Output Shape**: R64×16×16

- **(9) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(10) Mean Pooling**:
  - **Window Size**: 1 × 2 × 2
  - **Output Shape**: R64×8×8

- **(11) Convolution Layer**:
  - **Window Size**: 3 × 3
  - **Stride**: (1, 1)
  - **Padding**: (1, 1)
  - **Number of Output Channels**: 64
  - **Output Shape**: R64×8×8

- **(12) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(13) Convolution Layer**:
  - **Window Size**: 1 × 1
  - **Stride**: (1, 1)
  - **Number of Output Channels**: 16
  - **Output Shape**: R16×64

- **(14) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(15) Convolution Layer**:
  - **Window Size**: 1 × 1
  - **Stride**: (1, 1)
  - **Number of Output Channels**: 64
  - **Output Shape**: R64×64

- **(16) ReLU Activation**:
  - **Function**: Applies the ReLU activation function to each input element.

- **(17) Fully Connected Layer**:
  - **Input Nodes**: 1024
  - **Output Nodes**: 10
  - **Output Shape**: R10×1 ← R10×1024 · R1024×1

**Figure 13: The neural network trained on the CIFAR-10 dataset.**

### 6. Long Short-Term Memory (LSTM) for PTB

- **Language Modeling Dataset**: Penn Treebank (PTB)
  - **Dataset Characteristics**: Consists of preprocessed text data with 929,000 training words, 73,000 validation words, and 82,000 test words.
  - **Model Architecture**: LSTM
  - **Activation Functions**: Sigmoid and tanh

- **(1) Fully Connected Layer**:
  - **Input**: One-hot vector word (10000 × 1)
  - **Output Nodes**: 200
  - **Output Shape**: R200×1 ← R200×10000 · R10000×1

- **(2) LSTM Layer**:
  - **Input Nodes**: 200
  - **Hidden Nodes**: 200
  - **Output Nodes**: 200
  - **Processing Steps**:
    - **Step (a)**: R800×1 ← R800×400 · R400×1
    - **Step (b)**: R200×1 | R200×1 | R200×1 | R200×1 ← R800×1
    - **Step (c)**: R200 ← R200 ◦ sigmoid(R200) + sigmoid(R200)
    - **Step (d)**: R200 ← sigmoid(R200) ◦ tanh(R200)

- **(3) LSTM Layer**:
  - **Input Nodes**: 200
  - **Hidden Nodes**: 200
  - **Output Nodes**: 200
  - **Processing Steps**:
    - **Step (a)**: R800×1 ← R800×400 · R400×1
    - **Step (b)**: R200×1 | R200×1 | R200×1 | R200×1 ← R800×1
    - **Step (c)**: R200 ← R200 ◦ sigmoid(R200) + sigmoid(R200)
    - **Step (d)**: R200 ← sigmoid(R200) ◦ tanh(R200)

- **(4) Fully Connected Layer**:
  - **Input Nodes**: 200
  - **Output Nodes**: 10000
  - **Output Shape**: R10000×1 ← R10000×200 · R200×1

**Figure 14: The neural network trained on the PTB dataset.**

### 6.2 Evaluations with Realistic Models

- **Objective**: Evaluate the performance of realistic models using MiniONN transformations.
- **Comparison**: MiniONN supports commonly used neural network operations, unlike CryptoNets and SecureML.
- **Performance Metrics**:
  - **Latency (s)**
  - **Message Sizes (MB)**
  - **Accuracy (%)**

| Model                         | Latency (s) | Message Sizes (MB) | Accuracy (%) |
|-------------------------------|-------------|--------------------|--------------|
| ReLU/CNN/MNIST (Figure 12)    | 3.58 (offline), 5.74 (online) | 636.6 (offline), 20.9 (online) | 99.31 |
| ReLU/CNN/CIFAR-10 (Figure 13) | 472 (offline), 13.9 (online) | 72 (offline), 4.39 (online) | 81.61 |
| Sigmoidal/LSTM/PTB (Figure 14)| 3046 (offline), 86.7 (online) | 6226 (offline), 474 (online) | Cross-entropy loss: 4.76 |

**Table 5: Performance of MiniONN transformations of models with common activation functions and pooling operations.**

### 7. Complexity, Accuracy, and Overhead

- **Trade-offs**: Simplifying the neural network model can reduce overhead at the cost of a small decrease in prediction accuracy.
- **Model Complexity vs. Prediction Accuracy**: The relationship between model complexity and prediction accuracy is well-known. While increasing complexity can improve accuracy, it eventually saturates.
- **Overhead Analysis**:
  - **Linear Transformations**: Overhead is the same as non-private neural networks due to the precomputation phase.
  - **Activation Functions and Pooling Operations**: These introduce additional overhead.

**Figure 16: Overhead of oblivious activation functions.**

### 8. Related Work

- **Barni et al. [7]**: First attempt to construct oblivious neural networks, but leaks significant information about the intermediate results.
- **Orlandi et al. [47]**: Proposed a method to obscure intermediate results, but still has limitations.
- **Gilad-Bachrach et al. [28]**: Introduced CryptoNets based on leveled homomorphic encryption (LHE), but limited by the types of activation functions and pooling operations supported.

**Figure 18: Model complexity vs. accuracy.**

### Summary of Results

- **MiniONN**: Can transform existing neural networks into oblivious variants with reasonable performance.
- **CIFAR-10 Model**: Too expensive due to the high number of activation layers and neurons.
- **Trade-offs**: Designers can choose a suitable point in the accuracy vs. overhead tradeoff to balance performance and privacy.

**Table 6: Accuracy vs. overhead.**

### Figures and Tables

- **Figure 12**: The neural network trained on the MNIST dataset.
- **Figure 13**: The neural network trained on the CIFAR-10 dataset.
- **Figure 14**: The neural network trained on the PTB dataset.
- **Figure 15**: Cross-entropy loss for models with approximated sigmoid/tanh, evaluated over the full PTB test set.
- **Figure 16**: Overhead of oblivious activation functions.
- **Figure 17**: Alternative ReLU/CNNs trained from the MNIST dataset.
- **Figure 18**: Model complexity vs. accuracy.
- **Table 5**: Performance of MiniONN transformations of models with common activation functions and pooling operations.
- **Table 6**: Estimated latencies and message sizes for different accuracy levels.