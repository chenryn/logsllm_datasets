(4) Convolution: window size 5 × 5, stride (1, 1), number of output channels 16:
(5) ReLU Activation: calculates ReLU for each input.
(6) Max Pooling: window size 1 × 2 × 2 and outputs R16×4×4.
(7) Fully Connected: fully connects the incoming 256 nodes to the outgoing 100
(8) ReLU Activation: calculates ReLU for each input
(9) Fully Connected: fully connects the incoming 100 nodes to the outgoing 10
nodes: R100×1 ← R100×256 · R256×1.
nodes: R10×1 ← R10×100 · R100×1.
Figure 12: The neural network trained from the MNIST dataset.
Image classification: CIFAR-10. CIFAR-10 [36] is a standard dataset
consisting of RGB images (of size 3 × 32 × 32, 3 color channels,
width and height are 32) of everyday objects in 10 classes (e.g.,
automobile, bird etc.). The training set has 50 000 images while
the test set has 10 000 images. The neural network is detailed in
Figure 13. It achieves 81.61% prediction accuracy.
channels 64: R64×256 ← R64×576 · R576×256.
channels 64: R64×256 ← R64×576 · R576×256.
channels 64: R64×1024 ← R64×576 · R576×1024.
(1, 1), number of output channels 64: R64×1024 ← R64×27 · R27×1024.
(1) Convolution: input image 3 × 32 × 32, window size 3 × 3, stride (1, 1), pad
(2) ReLU Activation: calculates ReLU for each input.
(3) Convolution: window size 3 × 3, stride (1, 1), pad (1, 1), number of output
(4) ReLU Activation: calculates ReLU for each input.
(5) Mean Pooling: window size 1 × 2 × 2, outputs R64×16×16.
(6) Convolution: window size 3 × 3, stride (1, 1), pad (1, 1), number of output
(7) ReLU Activation: calculates ReLU for each input.
(8) Convolution: window size 3 × 3, stride (1, 1), pad (1, 1), number of output
(9) ReLU Activation: calculates ReLU for each input.
(10) Mean Pooling: window size 1 × 2 × 2, outputs R64×8×8.
(11) Convolution: window size 3 × 3, stride (1, 1), pad (1, 1), number of output
(12) ReLU Activation: calculates ReLU for each input.
(13) Convolution: window size 1 × 1, stride (1, 1), number of output channels of
(14) ReLU Activation: calculates ReLU for each input.
(15) Convolution: window size 1 × 1, stride (1, 1), number of output channels of
(16) ReLU Activation: calculates ReLU for each input.
(17) Fully Connected Layer: fully connects the incoming 1024 nodes to the outgo-
channels 64: R64×64 ← R64×576 · R576×64.
16: R16×64 ← R16×64 · R64×64.
64: R64×64 ← R64×64 · R64×64.
ing 10 nodes: R10×1 ← R10×1024 · R1024×1.
6.2 Evaluations with realistic models
As we stated in Section 2, a useful ONN transformation technique
must support commonly used neural network operations. Both
CryptoNets and SecureML [44] fall short on this count. In this
section we discuss performance evaluations of realistic models that
Figure 13: The neural network trained from the CIFAR-10 dataset.
Language modeling: PTB. Penn Treebank (PTB) is a standard dataset [41]
for language modeling, i.e., predicting likely next words given the
6http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.
html (last accessed May 9, 2017)
Session C3:  Machine Learning PrivacyCCS’17, October 30-November 3, 2017, Dallas, TX, USA627previous words ([45], Chapter 27). We used a preprocessed version
of this dataset7, which consists of 929 000 training words, 73 000
validation words, and 82 000 test words.
Long Short Term Memory (LSTM) is a neural network architec-
ture that is commonly used for language modeling [33]. Sigmoidal
activation functions are typically used in such networks. We repro-
duced and transformed a recent LSTM model [61] following the
tutorial8 in Tensorflow [2]. To the extent of our knowledge, this is
the first time language modeling is performed using oblivious mod-
els, which paves the way to oblivious neural machine translation.
The model is described in Figure 14.
(1) Fully Connected: input one-hot vector word 10000 × 1, fully connects the
(2) LSTM: pad the incoming 200 nodes with another 200 nodes: R400×1 ←
input nodes to the outgoing 200 nodes: R200×1 ← R200×10000 · R10000×1.
R200×1| R200×1, and then process them as follows:
(a) R800×1 ← R800×400 · R400×1
(b) R200×1| R200×1| R200×1| R200×1 ← R800×1
(c) R200 ← R200◦ sigmoid (R200
)+sigmoid (R200
(d) R200 ← sigmoid(R200
R200×1| R200×1, and then process them as follows:
(a) R800×1 ← R800×400 · R400×1
(b) R200×1| R200×1| R200×1| R200×1 ← R800×1
(c) R200 ← R200◦ sigmoid (R200
)+sigmoid (R200
(d) R200 ← sigmoid(R200
10000 nodes: R10000×1 ← R10000×200 · R200×1.
(3) LSTM: pad the incoming 200 nodes with another 200 nodes: R400×1 ←
(4) Fully Connected: fully connects the incoming 200 nodes to the outgoing
)◦ tanh(R200
)
)◦ tanh(R200
)
)◦ tanh(R200
)
)◦ tanh(R200
)
Figure 15: Cross-entropy loss for models with approximated sig-
moid/tanh, evaluate over the full PTB test set.
ReLU/CNN/MNIST
(Figure 12)
ReLU/CNN/CIFAR-10
(Figure 13)
Sigmoidal/LSTM/PTB
(Figure 14)
Latency (s)
offline
3.58
online
5.74
Message Sizes (MB)
online
offline
20.9
636.6
472
13.9
72
4.39
3046
86.7
6226
474
Accuracy
%
99.31
81.61
cross-entropy
loss:4.76
Figure 14: The neural network trained from the PTB dataset.
Table 5: Performance of MiniONN transformations of models with
common activation functions and pooling operations.
We used the real sigmoid activation functions for training, but re-
placed them with their corresponding approximations (Section 5.3.2)
for predictions. In our sigmoid approximation, we set the ranges as
[α0,αn] = [−30,30] and set the polynomials beyond the ranges as
0 and 1, i.e., ¯f (y  30) := 1 as in Equation 6.9
Unlike aforementioned image datasets, prediction quality here is
measured by a loss function called cross-entropy loss [45]. Figure 15
shows that the cross-entropy loss achieved by our approximation
method (with more than 12 pieces) is close to the original result
(4.76 vs. 4.74). We also test the new activation function that is pro-
posed in SecureML [44] as an alternative to the sigmoid function.
In this model, it causes the cross-entropy loss to diverge to infinity.
The optimal number of linear pieces differs on the model structure,
e.g., 14 pieces achieved optimal results on the larger models in [61].
Summary of results. Table 5 summarizes the results of the last three
neural networks after being transformed by MiniONN. The perfor-
mance of the ONNs in MNIST and PTB is reasonable, whereas the
ONN in CIFAR-10 is too expensive. This is due to the fact that the
model in CIFAR-10 (Figure 13) has 7 activation layers, and each layer
receives 210 − 216 neurons. In next section, we will discuss more
about the tradeoffs between prediction accuracy and overhead.
7http://www.fit.vutbr.cz/~imikolov/rnnlm/
8https://www.tensorflow.org/tutorials/recurrent, accessed April 20, 2017. We used the
‘small’ model configuration.
9This is exactly as in the Theano deep learning framework [9], where this approxima-
tion is used for numerical stability.
7 COMPLEXITY, ACCURACY AND
OVERHEAD
In Section 6, we demonstrated that, unlike prior work, MiniONN can
transform existing neural networks into oblivious variants. How-
ever, by simplifying the neural network model a designer can trade
off a small sacrifice in prediction accuracy with a large reduction
in the overhead associated with the ONN.
The relationship between model complexity and prediction ac-
curacy is well-known ([30], Chapter 6). In neural networks, model
complexity depends on the network structure: the number of neu-
rons (size of output from each layer), types of operations (e.g., choice
of activation functions) and the number of layers in the network.
While prediction accuracy can increase with model complexity, it
eventually saturates with some level of complexity.
Model complexity vs. prediction overhead. The overhead of linear
transformation is the same as non-private neural networks, since we
introduce a precomputation phase to generate dot-product triples.
Therefore, to investigate the overhead introduced by MiniONN,
we only need to consider the activation functions and pooling op-
erations in a given neural network model. Figure 16 shows the
performance of oblivious ReLU, oblivious square, oblivious sigmoid,
and oblivious max operations (used in both pooling and maxout
activation functions). Both message size and latency grow sublin-
early as the number of invocations increases. The experiments are
1011121314Numberoflinearpieces4.745.006.007.008.009.0010.0011.00Cross-entropylossmodelwithoutapproximationmodelwithapproximatedsigmoidandtanhrandompredictionSession C3:  Machine Learning PrivacyCCS’17, October 30-November 3, 2017, Dallas, TX, USA628Figure 18: Model complexity vs. accuracy.
Table 6 show the estimated latencies and message sizes for different
accuracy levels. Thus, if the latency and message size for a partic-
ular ONN is perceived as too high, the designer has the option of
choosing a suitable point in the accuracy vs. overhead tradeoff. The
overhead estimates were approximated, but reasonably accurate.
For instance, an actual ReLU/CNN/MNIST model with only 25%
ReLU invocations results in 1.51s latency and 159.2MB message
sizes, both of which are close to the estimate for α = 0.25 in Table 6.
8 RELATED WORK
Barni et al. [7] made the first attempt to construct oblivious neural
networks. They simply have 𝒮 do linear operations on 𝒞’s en-
crypted data and send the results back to 𝒞, who decrypts, applies
the non-linear transformations on the plaintexts, and re-encrypts
the results before sending them to 𝒮 for next layer processing.
Orlandi et al. [47] noticed that this process leaks significant in-
formation about 𝒮’s neural network, and proposed a method to
obscure the intermediate results. For example, when 𝒮 needs to
know siдn(x ) from E(pkc ,x ), they have 𝒮 sends a ⊗ E(pkc ,x ) to 𝒞
with a > 0. Obviously, this leaks the sign of x to 𝒞. Our work is
targeted for the same setting as these works but provides stricter
security guarantees (no intermediate results will be leaked) and has
significantly better performance.
Gilad-Bachrach et al. [28] proposed CryptoNets based on leveled
homomorphic encryption (LHE). They introduced a simple square
2, because CryptoNets cannot
activation function [28]: f (y) = y
Overhead
Latency (s) Message size (MB)
Accuracy (%)
α
1.000
0.875
0.750
0.625
0.500
0.375
0.250
0.188
0.125
5.72∗
5.01
4.29
3.58
2.87
2.15
1.07
0.72
636.6∗
557.0
447.5
397.9
317.6
238.7
119
79.0
99.31
99.27
99.26
99.19
98.96
98.79
98.42
97.35
95.72
1.44 (1.51∗)
158.4 (159.2∗)
Table 6: Accuracy vs. overhead. ∗ denotes actual values.
Figure 16: Overhead of oblivious activation functions.
repeated five times for each point. The standard deviation is below
2.5% for all points.
Model complexity vs. prediction accuracy. The largest contribution to
overhead in the online phase are due to activation function usage.
We evaluated the performance of our ReLU/CNN/MNIST network
(Figure 12) by decreasing the number of neurons in linear layers and
the number of channels in convolutional layers, to a fraction α of
the original value, according to the changes introduced in Figure 17.
This effectively reduced the number of activation function instances
to the same fraction.
(1) Convolution: input image 28 × 28, window size 5 × 5, stride (1, 1), number
of output channels of ⌊α · 16⌋: R⌊α·16⌋×576 ← R⌊α·16⌋×25 · R25×576.
(4) Convolution: window size 5 × 5, stride (1, 1), number of output channels
⌊α · 16⌋: R⌊α·16⌋×(⌊α·16⌋)
(7) Fully Connected: fully connects the incoming ⌊α · 16⌋ · 16 nodes to the out-
going ⌊α · 100⌋ nodes: R⌊α·100⌋×1 ← R⌊α·100⌋×⌊(α·16⌋·16) · R(⌊α·16⌋·16)×1.
(9) Fully Connected: fully connects the incoming ⌊α · 100⌋ nodes to the outgo-
ing 10 nodes: R10×1 ← R10×⌊α·100⌋ · R⌊α·100⌋×1.
2 ← R⌊α·16⌋×400 · R400×(⌊α·16⌋)
2 .
Figure 17: Alternative ReLU/CNNs trained from the MNIST dataset.
Figure 18 shows how prediction accuracy varies with α. It is
clear that the decline in prediction accuracy is very gradual in the
range 0.25 < α < 1 (corresponding 211.3 and 213.3 ReLU invo-
cations). From Figure 16, we observe that when α drops by 75%
from 1 (213.3 invocations) to 0.25 (211.3 invocations), performance
overhead also drops roughly by 75% (for both latency and message
size) but accuracy drops only by less than a percentage point.
Accuracy vs. overhead. In Table 6, we estimated the overhead for
smaller variants of the ReLU/CNN/MNIST network, w.r.t. to the
base network overhead in Table 5. For example, columns 2 and 3 of
20242728210212216Numberofinvocations100101102103104105106Latency[ms]ReLUSquareSigmoidappr.with12piecesMaxwith2inputsMaxwith4inputsMaxwith16inputs20242728210212216Numberofinvocations10−1100101102103104105Messagesize[MB]ReLUSquareSigmoidappr.with12piecesMaxwith2inputsMaxwith4inputsMaxwith16inputs210.3210.9211.3211.9212.3212.7212.9213.1213.3NumberofReLUinvocations95.096.097.098.098.599.099.5Accuracy[%]0.1250.1880.2500.3750.5000.6250.7500.8751.000αSession C3:  Machine Learning PrivacyCCS’17, October 30-November 3, 2017, Dallas, TX, USA629support commonly used activation functions due to the limitations
of LHE. They also used mean pooling instead of max pooling for
the same reason, even though the latter is more commonly used.