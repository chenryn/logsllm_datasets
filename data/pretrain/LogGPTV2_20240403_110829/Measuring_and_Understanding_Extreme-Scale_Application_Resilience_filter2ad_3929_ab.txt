FLUKSS 
   X  X    
   X    
   X    
   X     X     X    
   X 1,117  3.25  20,480 
2.09  18,000 
   677 
   X 895 
1.17  21,296 
   1,332  3.47  22,528 
   2,143  0.82  16,384 
2.84  11,520 
   510 
   131 
3.24  17,408 
Enzo, pGADGET X    
PSDNS, DISTUF X    
Cactus, Harm3D, LazEV X    
2,693,494  AMBER, Gromacs, NAMD, 
LAMMPS   
SIAL, GAMESS, NWChem   
NEMOS, OMEN,QMCPACK   
AWP-ODC, HERCULES, 
PLSQR, SPECFEM3D X X    
Chroma, MILC, USQCD X     X  X  X     X    
Climate and 
Weather  
Plasmas/ 
Magnetosphere  
Stellar 
Atmospheres, 
Supernovae  
159,320 
Cosmology  
Combustion/ 
4,422 
Turbulence  
General Relativity   69,760 
Molecular 
Dynamics  
Quantum 
721,042 
Chemistry  
Material Science   22,630 
Earthquakes/ 
16,134 
Seismology  
Quantum Chromo 
Dynamics  
Social Networks   9,220 
Engineering/ 
System of Systems 474 
Benchmarks 
Visualization 
Job and Application Exit status.. When a batch job exits,
the Torque server generates an E (Exit) record in its log.
Technically, it is the exit code of the last command run in the
script (e.g., ls). As a matter of fact, a job can ﬁnish with success
even if all its applications terminate abnormally. Therefore,
unlike other work, this paper looks only at application exit
codes to obtain a robust understanding of the impact of system
errors on user computation.
A. Blue Waters Workload
qball, IMB, xperf    X  X  X     X  X  X  X  1,147  1.30  26,864 
waves, vmd       X  X              X  11,178  2.86  16,000 
   1,291  3.05  6,144 
   359 
0.64  22,528 
   9,465  5.21  5,632 
linpak, fft, psolve, jacobi, 
580,974 
10,374 
GRIPS,Revisit      
EPISIMDEMICS   
The workload processed by Blue Waters consists of large-
scale scientiﬁc simulations,
including the scientiﬁc areas
showed in Table II [4]. Each area includes different software
(code) that is used to create speciﬁc studies (applications).
Each code is characterized by different features summarized
in Table II. Blue Waters jobs may use compute nodes, GPU
nodes,or both. The following are representative large-scale
applications (in size and duration) executed on Blue Waters:
i) NAMD, an application for performing molecular simulations
of biomolecules and dynamic evolution of the system with
a time step of 1 fs. It
is used to determine the precise
chemical structure of the HIV virus protein shell on a 64M-
atom model, enabling research on new antiretroviral drugs to
stop the progression of AIDS; ii) VPIC, an application for
kinetic simulations of magnetic reconnection of high temper-
ature plasmas, executing on 22,528 nodes with 1.25 PFLOPS
sustained performance over 2.5 hours; iii) inertial Conﬁnement
Fusion (ICF) involving the simulation of turbulent mixing
and combustion in multiﬂuid interfaces, producing 14 TB of
data in less than an hour of execution across 21,417 XE and
achieving 1.23 PF of sustained performance; iv) QMCPACK, an
application used to study the high-pressure hydrogen problem.
It executes up to 18,000 XE nodes with a sustained performance
of 1.037 PF/s; v) the largest Weather Research and Forecasting
(WRF) simulation ever documented [5].
Table III shows a breakdown of the applications in our
dataset. 65.14% of the total runs are XE applications (i.e.,
3,365,617), 33.37% are XK applications (i.e., 1,724,126) using
CPU and GPU accelerators, and the remaining 1.49% are
staff applications (i.e., 77,023) not considered in this study.
To compare the composition of XE and XK applications with
respect to application scale, we subdivided the applications into
6 classes following the rules in Table III. In particular, Blue
Waters data include only a limited number of applications that
can effectively use full-scale executions. Even when running at
full scale, many applications do not execute for a long time,
e.g., 75% of the full-scale XE applications in the measured data
run for less than 5 h, with a median of 1.2 h.
B. Data Sources
Table IV summarizes the dataset considered in this study.
The data include 517 days of data from 2013-03-01 to 2014-
07-31, during which Blue Waters run 738,102 jobs launched
by 913 users, in the context of 131 different research projects.
The submitted jobs included 5,116,766 applications that run for
the total of 198,112,165 node hours. LogDiver input consists
of system-generated syslogs and workload logs, including job
scheduler logs and application scheduler logs. Data used in this
study are extracted from the following log sources:
Syslogs include system events logged by the OS and by HSS
and entries generated by the Sonexion cluster. Events collected
in the logs include i) the timestamp of the event, ii) the facility,
indicating the type of software that generated the messages, iii)
a severity level, indicating how severe the logged event is, iv)
2727
the identiﬁcation of the node generating the message, v) the
process, including the PID (process identiﬁer) of the process
logging the event, and vi) the event description.
Alps logs include information on node reservation man-
agement, job/application launching, periodic monitoring and
termination of user applications, and detected internal problems
and cleanup operations. Alps logs are redirected by the system
console to the syslogs and merged with other system events.
Torque logs include information on created, canceled, sched-
uled, and executed jobs in the system. Each entry in the
TORQUE logs consists of 45 ﬁelds containing time information
on all the phases of the job (creation, queue, execution, and
termination times), user, group, queue, resources, type and list
of used nodes, and wall time used.
III. EXAMPLE OF ERRORS IMPACTING APPLICATIONS
Figure 2 shows a real failure scenario in Blue Waters,
highlighting the impact on running applications.
The chain of events starts with a voltage regulator failure due
to bad cooling, which leads to a cabinet Emergency Power-Off
(EPO). Resulting from the failures, the subsequent change of
the topology is detected by the HSS, which triggers a network
failover that includes a reroute of the HSN Gemini network.
However, the network failover is only partially successful and
leaves the Blade Module Controllers (BMCs) in an inconsistent
state, one in which another HSN reroute would result in a
system-wide outage. At this stage, all applications executing
on the failed blade are either failed or recovered, depending on
the effectiveness of the protection mechanisms (i.e., application-
level checkpoint and/or blade warm-swap) as well as on the
sensitivity of the run code to transient network shutdowns.
In this example, the cabinet EPO additionally impacts the
Lustre ﬁle system, causing a mass client disconnect from ﬁle
servers, which in turn results in ﬁle server instability and
consequent ﬁle system failover (started just after the network
reroute). File system slowness (e.g., due to failover and drive
rebuild) causes slowness in the job scheduler, impacting appli-
cation productivity. Applications accessing the ﬁle system are
suspended waiting for the failover to conclude. Recovery from
client failure in Lustre is based on lock revocation and other
resources, so surviving clients can continue their work uninter-
rupted. As a consequence, many applications are quiesced and
put on wait for the lock acquisition by the Distributed Lock
Manager managing the failover process.
Further, we observe that accumulation of hung I/O threads on
the ﬁle server from disconnected clients can eventually cause
the ﬁle server to crash. These cascading failures ultimately
result in a nonresponsive System Console and HSN throttle
with the following chain of events: A surge of logs containing
Lustre ﬁle system errors are redirected to the local ﬁle system
of the system management workstation (SMW) due to the
unavailability of the ﬁle server. This ﬁlls up all the available
storage, causing the SMW to be unresponsive and triggering an
SMW replica swap.
The high network trafﬁc generated by the ﬁle system failover
and by the variety of anomalous conditions detected causes
a HSN congestion that reacts by ﬁrst throttling and then by
forcing a reroute. However,
the HSN reroute can only be
Fan/Cooling Problems 
File System Failover 
File Server Hung 
File Client Evictions/Failures 
Jobs  using  the  failed  file  systems  are 
stalled  waiting  for  the  failover  to 
complete 
•  Many Jobs reaches the walltime limit 
and exit (non-error exit code) 
•  Performance-sensitive  applications 
failing and exiting with error code 
Power Supply Failure 
Cabinet Emergency 
Power Off (EPO) 
HSN Reroute (no. 1) 
BMC (L0 controller) 
unresponsive 
Application  using  nodes 
from  the  failed  cabinet 
fail  if  not  protected  (e.g., 
checkpoint, warm-swap) 
System Console (SMW) 
unresponsive – Disk full (logs 
stored  on the local disk during 
file system unavailability) 
HSN throttled 
because of high 
traffic (failover). 
Reroute triggered 
HSN Unroutable (BMC and 
SMW irresponsible) 
HSN Reroute (no.2) 
Needs SMW to complete the reroute but it is 
unresponsive 
System Wide Outage 
System Reboot needed; all 
application fail 
Fig. 2. Real example of events leading to application failures and eventually
to a system-wide outage.
completed with at least 1 available SMW (not yet replaced
by the replica), hence the system-wide outage. In the end, the
system in the example was rebooted, and hence all running
applications were terminated.
In summary,
the described scenario shows how a local
blade/cabinet failure could not be contained and led to a ﬁle
system failover that propagated across the entire system.
IV. THE LogDiver MEASUREMENT TOOL
We developed the LogDiver tool set to facilitate the prepro-
cessing of data logs and the analysis conducted in this paper.
While LogDiver is designed with respect to Cray architecture,
it can be extended to other types of systems. Uniquely, this tool
does the following:
• Directly relates system errors and failures (e.g., Gemini
ECC errors, GPU MMU errors, and Lustre ﬁle system
failures) to application failures, and
• Provides
a
uniﬁed
representation
the work-
load/error/failure logs, permitting the analysis of workload
failures and the computation of a range of quantitative
performance and dependability metrics.
of
An in-depth characterization of
the application failures
caused by system-related issues is essential to evaluating the
performance of current systems and to guiding the design of
resiliency mechanisms.
LogDiver operates in four main steps, depicted in Figure 3.
Each step produces several output ﬁles that are fed downstream
to the subsequent step. Data in intermediate output ﬁles can
also be used by external
tools (e.g., by Matlab and SAS
to perform workload characterization) to conduct additional
analysis beyond what LogDiver supports.
Step 1: Data Collection. Step 1 is in charge of collecting
data from multiple sources. Many sources are redirected to the
syslog, including a subset of the data generated by hardware
sensors. To ease the porting of this tool to different systems,
data are parsed to an internal format that is system-agnostic.
Step 2: Creation of Event Templates and Data Filtering.
The objectives of this step are i) to understand the content of the
dataset by identifying a list of unique templates of log entries
and ii) to identify and categorize the error event templates
contained in the template list to create an event ﬁlter. The output
of this step is a list of categories containing only events of
interest, i.e., the error data.
2828
Workload Data Generation 
• aprun app1 
• aprun app1 
• aprun app2 
• aprun app2
2
• aprun app2 
• …. 
• ….
User 
applications 
Scheduling 
queue(s) 
User 
User job(s) 
Workload (job and application data)  
Software sensors/dectors  
e.g.,  timeouts,  crashes,  file  systems-level 
detectors, scheduler/OS-level  detectors 
Hardware 
sensors 
•  Marchine checks 
•  GPU Hardware counters 
•  Gemini counters  
•  Blade controllers  
•  … 
Step 4: Workload-Error Matching 
•  Evaluate estimator (time series) on rates, count 
and interarrival for tagged error, for each node 
•  Compute correlation metrics (change of point 
detection and cross-correlation) for each estimator 
• Group errors happening in app [StartTime,EndTime] on the nodes 
in app NodeList with high correlation in a time window T 
Consolidated 
Data (output) 
Step 3: Workload Consolidation 
Torque Logs 
Consolidator  
Run 
jobs 
Application 
exit codes 
database 
Alps Logs 
65GB 
Workload logs 
consolidator 
Torque 
Logs 
54GB/ 
month 
syslogs 
6.6TB 
Step 2: create event templates and data filtering 
Sys. Administrator 
or 
(Validation) 
Extract 
Template 
Template 
Error Event Templates 
(358 out of 22,082) 
  x 
x
(cid:1) 
x 
(cid:1) 
(cid:1)
Apply Data Filter 
Data Tagging 
Consolidated 
application and 
job dataset 
Error Logs 
130 GB 
Metric estimation 
•  MTBI, MNBF 
•  Node Hours, 
duration, scale 
•  Probability 
Failure 
•  … 
n
o
i
t
c
e
l
l
o
c
a
t
a
d
:
1
p
e
t
S
G C TAG
G C TAG
I
E
R
U
T
C
E
T
H
C
R
A
Y
C
N
E
L
S
E
R
I
I
E
D
A