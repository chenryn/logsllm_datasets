### 1. Third-Party and Self-Hosting Media Services

**Figure 10: Server Response Time to Streaming Requests**

The server response time for a Real-Time Streaming Protocol (RTSP) or Microsoft Media Server (MMS) request is defined as the duration from when the server receives the first command from a client to when it sends its reply. Our workloads, collected by Gigascope at a site close to end users, allow us to use packet timestamps as accurate time references. We estimate the packet round trip time (RTT) using TCP handshake packets and then compute the server response time.

**Figure 10(a)** shows the distribution of server response times for streaming requests served by third-party media services. We compare the response times of servers running Fast Cache with those that do not. For servers running Fast Cache, approximately 43% of the requests have a response time longer than 0.1 seconds, whereas for servers not running Fast Cache, only about 9% of the requests exceed this threshold.

**Figure 10(b)** presents the corresponding distribution for self-hosting media services. For servers running Fast Cache, about 21% of the requests have a response time longer than 0.1 seconds, while for servers not running Fast Cache, only about 5% of the requests exceed this threshold. These results indicate that servers running Fast Cache generally have longer response times compared to those without Fast Cache. Additionally, third-party hosting services tend to have higher response times than self-hosting services, likely due to the higher resource utilization in commercial environments.

### 2. Server Load of Fast Cache

To further investigate the system resources consumed by Fast Cache, we conducted experiments using Windows Server 2003 and the Windows Media Load Simulator [8]. The server machine, equipped with a 2 GHz Pentium-4 CPU and 512 MB of memory, was connected to a Windows XP client machine via a 100 Mbps fast Ethernet switch. We generated two streaming video files: one encoded at 282 Kbps and the other at 1.128 Mbps, both with a 20-minute playback duration. Each file was duplicated 50 times, each copy saved with a different name.

We ran 50 normal TCP streaming sessions for 5 minutes, each requesting a different copy of the 282 Kbps video file simultaneously. Since the simulator does not support Fast Cache, we simulated Fast Cache by running 50 normal TCP streaming sessions requesting the 1.128 Mbps video. This experiment was also conducted for 5 minutes, with each session requesting a different file copy simultaneously. In each experiment, the simulator recorded the CPU and memory usage reported by the server every second, and the average bandwidth usage was logged. Each experiment was repeated 10 times.

**Figure 11** shows the average CPU and bandwidth usage of the server over the entire simulation period. The bandwidth usage of Fast Cache is 3.67 times that of normal TCP streaming, while the CPU load is 3.57 times higher. This indicates that the CPU consumption by Fast Cache is approximately proportional to the streaming delivery rate. Given that Fast Cache can deliver a media object at a rate 5 times its encoding rate, it significantly increases server load, limiting the scalability of the streaming server. Notably, the largest self-hosting media service and the second-largest media delivery network do not support Fast Cache, possibly due to concerns about high resource demands.

### 3. Effectiveness of Resource Over-Utilization

Fast Cache delivers media objects faster than the playing speed by over-utilizing bandwidth and CPU resources. However, streaming at a rate higher than the encoding rate is only feasible if the available bandwidth between the client and server is sufficiently large. When the average bandwidth is high, a small play-out buffer can smooth out temporary network congestion, making aggressive resource over-utilization less effective and cost-efficient.

**Figure 12** plots the cumulative distribution function (CDF) of rebuffering ratios for Fast Cache-based and normal TCP-based streaming sessions in the home user workload, where the media encoding rate is 200–320 Kbps and the client's advertised bandwidth is at least 500 Kbps greater than the encoding rate. The two curves are very close, indicating that, despite occasional network congestion, a small play-out buffer performs well enough to handle bandwidth fluctuations. Thus, aggressively over-utilizing server and Internet resources is neither performance-effective nor cost-efficient under high-bandwidth conditions.

### 4. Rate Adaptation

Major media services like Windows Media and RealNetworks support three techniques for rate adaptation to handle bandwidth fluctuations:

1. **Stream Switch**: Dynamically switches among streams with different encoding rates based on available bandwidth. This is called Intelligent Streaming in Windows Media and SureStream in RealNetworks.
2. **Stream Thinning**: Sends only key frames to the client when lower bit rate streams are unavailable.
3. **Video Cancellation**: Sends only audio to the client if the current bandwidth is insufficient for key frames.

#### 4.1 MBR Encoding and Stream Switch

To enable stream switching, media objects must be encoded with multiple bit rates (MBR). Figures 13(a) to 13(f) show the distribution of the number of streams encoded in on-demand and live media objects for home and business users. For video objects, the number of audio and video streams is shown separately. About 42% of on-demand video objects in the home user workload are encoded with at least two video streams, with up to 12 video streams and 20 total streams in a single object. The number of streams in live audio objects is relatively small, but 13% and 28% of the objects in home and business user workloads, respectively, are encoded with at least two streams. These results indicate that MBR encoding is widely used, enabling dynamic stream switching based on available bandwidth.

The stream switch in RTSP protocols works as follows: When a client initiates an RTSP session, it sends a DESCRIBE command to the server, which replies with the media description using SDP, including details of each video/audio stream. The client then specifies the desired stream in the SETUP (Windows Media) or SET PARAMETER (RealNetworks) command based on available bandwidth. The server delivers the requested stream upon receiving the PLAY command.

If the available bandwidth drops below the media encoding rate during playback, the play-out buffer may drain, causing the media player to request a switch to a lower rate stream. In Intelligent Streaming, the media player sends a SET PARAMETER with a SSEntry message body via RTSP. In SureStream, the client sends a SET PARAMETER command with an UnSubscribe header to cancel the current stream and a Subscribe header to switch to the new stream.

We extracted and analyzed stream switch information from RTSP/MMS commands. **Figure 14(a)** and **Figure 14(b)** show the distribution of stream switch latency and low quality duration in the home and business user workloads, respectively. About 30%–40% of stream switches have a latency greater than 3 seconds, and 10%–20% have a latency greater than 5 seconds. In Figure 14(b), about 60% of the sessions have a low quality duration less than 30 seconds, and 85% of the low quality durations are shorter than 40 seconds.

#### 4.2 Stream Thinning and Video Cancellation

When the current bandwidth is insufficient to transmit key frames, the server can send only audio to the client, a process known as video cancellation. This ensures that some content is delivered even under severe bandwidth constraints.