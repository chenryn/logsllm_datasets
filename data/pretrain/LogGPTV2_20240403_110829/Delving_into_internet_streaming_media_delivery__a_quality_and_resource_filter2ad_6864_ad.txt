(a) Third-party media service
(b) Self-hosting media service
Figure 10: The server response time to streaming requests
than a streaming server not running Fast Cache. As a re-
sult, a request may have to wait for a longer time to be
served when a burst of requests arrive at the server. We
deﬁne the server response time of a RTSP/MMS request
to be the duration from the time instant when a server re-
ceives the ﬁrst command from a client, to the time instant
when the server sends its reply. Since our workloads are
collected by Gigascope at a site very close to end users, the
timestamp of a captured packet can be regarded as the time
instant when the client sends or receives that packet. We
use the timestamps of TCP handshake packets to estimate
the packet round trip time (RTT), and then compute the
server response time.
Figure 10(a) shows the distribution of the server response
time for streaming requests served by third-party media ser-
vices. We compare the response time of requests to servers
with and without running Fast Cache. For servers running
Fast Cache, about 43% of the requests have a response time
longer than 0.1 second, while for servers not running Fast
Cache, only about 9% of the requests have a response time
longer than 0.1 second. Figure 10(b) shows the correspond-
ing distribution of the server response time for streaming
requests served by self-hosting media services. For servers
running Fast Cache, about 21% of the requests have a re-
sponse time longer than 0.1 second, while for servers not
running Fast Cache, only about 5% of the requests have a
response time longer than 0.1 second. These results indi-
cate that the response time on servers running Fast Cache
is statistically longer than that on servers not running Fast
Cache. We also observe that the response time of the third-
party hosting service is larger than that of the self-hosting
service. As a commercial company, a third-party hosting
service may want to fully utilize its server resources with
many service subscribers. In contrast, self-hosting services
are more dedicated and thus are often less heavily loaded.
5.4 Server load of Fast Cache
To further investigate the system resources consumed by
Fast Cache, we conducted experiments with the Windows
server 2003 and the Windows media load simulator [8]. We
ran Windows server 2003 on a machine with 2 GHz Pentium-
4 CPU and 512 MB memory, and ran a Windows media load
simulator on a Windows XP client machine. The server ma-
chine and the client machine are connected through a 100
Mbps fast Ethernet switch. We generated two streaming
video ﬁles with the Windows media encoder, one is encoded
with 282 Kbps and the other is encoded with 1.128 Mbps,
both of which have a 20-minute playback duration. We du-
plicated each ﬁle with 50 copies, and saved each copy with
a diﬀerent name. We ﬁrst ran 50 normal TCP streaming
sessions for 5 minutes using the simulator, each of which re-
quests a diﬀerent copy of the 282 Kbps video ﬁle simultane-
ously. Since the simulator does not support Fast Cache, we
ran 50 normal TCP streaming sessions requesting the 1.128
Mbps video using the simulator, in order to simulate the
streaming with Fast Cache support for the 282 Kbps video
(with 4 speed). This experiment was also conducted for 5
minutes, with each session requesting a diﬀerent ﬁle copy
simultaneously. In each experiment, the simulator recorded
the CPU and memory usage reported by the server every
second. The average bandwidth usage was recorded in the
server logs. We repeated each experiment 10 times.
Figure 11 shows the average usage of CPU and bandwidth
of the server over the entire duration of the simulation (the
)
%
i
(
e
m
T
r
o
s
s
e
c
o
r
P
10
8
6
4
2
0
Normal TCP
Fast Cache
)
s
p
b
(
h
t
d
w
d
n
a
B
i
12
10
8
6
4
2
0
x 105
Normal TCP
Fast Cache
(a) CPU consumption of all
50 sessions
(b) Average bandwidth con-
sumption per session
Figure 11: Server load comparison between Fast
Cache and normal TCP streaming
memory usages of Fast Cache and normal TCP streaming
are very close and thus are not presented). The bandwidth
usage of Fast Cache is 3.67 times of that of the normal TCP
streaming, while the CPU load of Fast Cache is 3.57 times of
that of normal TCP streaming. This indicates that the CPU
consumed by Fast Cache is approximately proportional to
the streaming delivery rate. Given that Fast Cache could
deliver a media object at a rate 5 times of its encoding rate,
Fast Cache increases server load signiﬁcantly, and thus lim-
its the scalability of a streaming server. In our workloads,
the Windows media servers in the second largest media de-
livery network and the largest self-hosting media service (a
well known search engine site) do not support Fast Cache
at all (we anonymize their domain names due to customer
privacy concerns), which might be due to the concerns of
high resource demands of Fast Cache.
5.5 Effectiveness of resource over-utilization
Fast Cache delivers a media object to a client faster than
the playing speed by over-utilizing the bandwidth and CPU
resources. However, streaming a media object at a rate
higher than its encoding rate is only possible when the
available bandwidth between a client and its server is large
enough. Intuitively, when a media object is streamed at its
encoding rate, the higher the average bandwidth between a
client and its server over its encoding rate, the lower possi-
bility at which performance degradation occurs during the
playback. To understand whether Fast Cache performs bet-
ter than normal TCP-based streaming when the average
bandwidth between a client and its server is large enough,
we plot the CDF of rebuﬀering ratio for Fast Cache based
streaming sessions and normal TCP-based streaming ses-
sions in the home user workload in which the media encoding
rate of each stream is 200–320 Kbps and the client adver-
tised bandwidth (extracted from the Bandwidth header) is
at least 500 Kbps greater than the media encoding rate, as
shown in Figure 12. Compared with Figure 7(a), the two
curves in Figure 12 are very close, which means that, al-
though temporary network congestion may occur from time
to time, a small play-out buﬀer performs well enough to
smooth out bandwidth ﬂuctuation during streaming, when
the average bandwidth is large enough. Thus, aggressively
over-utilizing the server and Internet resources is neither
performance-eﬀective nor cost-eﬃcient under a high band-
width condition. The higher speed at which Fast Cache can
stream a media object, the lower necessity is this speed for
a client. Furthermore, even if no extra traﬃc generated (as-
 1
 0.95
F
D
C
 0.9
 0.85
 0.8
 0
Fast Cache
Normal TCP
 0.1
 0.2
 0.3
 0.4
 0.5
Rebuffer Ratio (rebuffer time / play time)
Figure 12: Eﬀectiveness of resource over-utilization
sume the media object is played completely in each session),
the number of concurrent streams on a server is constrained
by the streaming speed, and thus limits the server’s capacity
to service bursty requests.
6. RATE ADAPTATION
In order to adapt to bandwidth ﬂuctuations, major me-
dia services such as Windows media and RealNetworks me-
dia support three kinds of techniques for rate adaptation.
Stream switch enables a server to dynamically switch among
streams with diﬀerent encoding rates for the same object,
based on the available network bandwidth. This technique
is called Intelligent Streaming in the Windows media ser-
vice [4] and SureStream in the RealNetworks media ser-
vice [7]. Stream thinning enables a server to only send key
frames to the client, when no lower bit rate stream is avail-
able. If the current bandwidth is not suﬃcient to transmit
key frames, a server can only send audio to client, which is
called video cancellation.
6.1 MBR encoding and stream switch
To enable stream switch, the media object must be en-
coded with multiple bit rates (MBR): the encoder generates
multiple streams with diﬀerent bit rates for the same media
content, and encapsulates all these streams together.
Figures 13(a), 13(b), 13(c) and Figures 13(d), 13(e), 13(f)
show the distribution of the number of streams encoded in
on-demand and live media objects in the home user and
business user workloads, respectively. For video objects, we
show the number of audio streams and video streams in a
ﬁle separately (Figures 13(b), 13(c) for on-demand objects
and Figures 13(e), 13(f) for live objects). Because there are
only a small amount of live video objects in the business
user workload, we do not present them in Figures 13(e),
13(f). These ﬁgures show that about 42% of the on-demand
video objects in the home user workload are encoded with
at least two video streams. The number of video streams in
video objects is up to 12, and the number of video and audio
streams together in a video object is up to 20. The num-
ber of streams in live audio objects is relatively small, but
there are still 13% and 28% of the objects in home user and
business user workloads encoded with at least two streams,
respectively. These results indicate that the MBR encoding
technique has been widely used in media authoring, which
enables the rate adaptation—dynamically switching among
streams based on the available bandwidth.
The stream switch in RTSP protocols works as follows
(stream switch in MMS has a similar procedure). When a
RTSP session is established upon a client request, the media
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
 2
Home
Biz
 0  1  2  3  4  5  6  7  8  9  10  11  12
Number of Streams
Home
Biz
 4
 3
 6
Number of Streams
 5
 7
 8
 9
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Home
Biz
 0  1  2  3  4  5  6  7  8  9  10  11  12
Number of Streams
(a) On-demand audio objects
(b) Audio in on-demand video objects
(c) Video in on-demand video objects
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
Home
Biz
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
Home
 4
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9  10  11
Home
 3
 0
 1
 2
 5
 4
 3
 7
Number of Streams
 6
 8
 9  10
 0
 1
 2
Number of Streams
Number of Streams
(d) Live audio objects
(e) Audio in live video objects
(f) Video in live video objects
Figure 13: MBR encoding in the home and business user workloads
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 0.1
 1
Biz
Home
 10
 100
 0
 0.1
 1
 10
Biz
Home
 100
 1000
 0
 0.1
 1
 10
Biz
Home
 100
 1000
 0
 10
Stream Switch Handoff Latency (sec)
(a) Stream switch latency
Stream Switch: Duration of Low Quality (sec)
(b) Low quality duration
Thinning Duration (sec)
(a) Thinning duration
(b) Thinning interval
Biz
Home
 100
 1000
Thinning Interval (sec)
Figure 14: Stream switch
Figure 15: Stream thinning
player sends a DESCRIBE command to the server, asking for
the description of the requested media object. In the reply
to the DESCRIBE, the server sends the media description us-
ing SDP [19], including the description of each video/audio
stream encapsulated in the media object. Then the client
speciﬁes the stream that it desires in the SETUP (Windows
media service) or SET PARAMETER (RealNetworks media ser-
vice) command, based on its current available bandwidth.
The server delivers the requested stream upon receiving the
PLAY command from the client.
If during playback, the available bandwidth drops below
the media encoding rate, the play-out buﬀer will be drained
oﬀ. In this case, the media player may send a request to ask
the server to switch to a lower rate stream. In Intelligent
Streaming (Windows media service), the media player sends
a SET PARAMETER with a SSEntry message body via RTSP,
specifying the current stream and the stream to switch to. In
SureStream (RealNetworks media service), the client sends
a SET PARAMETER command with an UnSubscribe header to
cancel the current stream and a Subscribe header to switch
to the new stream.
We extracted all related information from RTSP/MMS
commands, and analyzed these stream switches. To char-
acterize the overhead and frequency of stream switches, we
deﬁne the switch latency as the freezing duration between
the end of the old stream and the beginning of the new
stream, during which a user has to wait for buﬀering. We
also deﬁne the low quality duration of a streaming session as
the total playback time of streams with lower rates (relative
to the highest encoding rate that the content is transmitted
in this session).
Assuming a ﬁve-second play-out buﬀer [1], Figure 14(a)
and Figure 14(b) show the distribution of stream switch la-
tency and low quality duration in the home and business
user workloads, respectively. As shown in Figure 14(a),
about 30%–40% of the stream switches have a switch la-
tency greater than 3 seconds, and about 10%–20% of the
stream switches have a switch latency greater than 5 sec-
onds, which is non-trivial for end users. In Figure 14(b), we
observe that about 60% of the sessions have a low quality
duration less than 30 seconds, and 85% of the low quality
stream durations are shorter than 40 seconds.
6.2 Stream thinning and video cancellation