 **pytorch version:** '0.1.12_2'
It is not so easy to describe the exact bug here, but I think it is sufficient
to provide an example in which strange results can be reproduced.
Use the serialised version of Tensor in the attachment. Then run this snippet
(also included in the attachment)
    import torch
    from torch.autograd import Variable
    x_restored = Variable(torch.load('x.pth'), requires_grad=False)
    x_res_repeated = x_restored.repeat(10, 1)
    print(x_restored.eq(x_res_repeated[0]).data.numpy().sum())
    print(x_restored.eq(x_res_repeated[0]).data.sum())
    print(x_restored.eq(x_res_repeated[0]).sum())
The output
    3200
    3200
    Variable containing:
     128
    [torch.ByteTensor of size 1]
I would expect the summation to return the same results regardless of whether
we operate on numpy arrays, pytorch tensors or `Variable` holding a `Tensor`
in the `data` field. (Which means it should return a Variable holding a Tensor
with the right capacity to store the summed value.  
However, this is unfortunately not the case.
bug_reproduce.zip