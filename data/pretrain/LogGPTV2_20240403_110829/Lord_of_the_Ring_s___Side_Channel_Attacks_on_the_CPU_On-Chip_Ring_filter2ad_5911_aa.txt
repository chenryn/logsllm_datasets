title:Lord of the Ring(s): Side Channel Attacks on the CPU On-Chip Ring
Interconnect Are Practical
author:Riccardo Paccagnella and
Licheng Luo and
Christopher W. Fletcher
Lord of the Ring(s): Side Channel Attacks on the 
CPU On-Chip Ring Interconnect Are Practical
Riccardo Paccagnella, Licheng Luo, and Christopher W. Fletcher, 
University of Illinois at Urbana-Champaign
https://www.usenix.org/conference/usenixsecurity21/presentation/paccagnella
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Lord of the Ring(s): Side Channel Attacks on the
CPU On-Chip Ring Interconnect Are Practical
Riccardo Paccagnella Licheng Luo Christopher W. Fletcher
University of Illinois at Urbana-Champaign
Abstract
We introduce the ﬁrst microarchitectural side channel at-
tacks that leverage contention on the CPU ring interconnect.
There are two challenges that make it uniquely difﬁcult to
exploit this channel. First, little is known about the ring inter-
connect’s functioning and architecture. Second, information
that can be learned by an attacker through ring contention is
noisy by nature and has coarse spatial granularity. To address
the ﬁrst challenge, we perform a thorough reverse engineering
of the sophisticated protocols that handle communication on
the ring interconnect. With this knowledge, we build a cross-
core covert channel over the ring interconnect with a capacity
of over 4 Mbps from a single thread, the largest to date for a
cross-core channel not relying on shared memory. To address
the second challenge, we leverage the ﬁne-grained temporal
patterns of ring contention to infer a victim program’s secrets.
We demonstrate our attack by extracting key bits from vulner-
able EdDSA and RSA implementations, as well as inferring
the precise timing of keystrokes typed by a victim user.
1 Introduction
Modern computers use multicore CPUs that comprise sev-
eral heterogeneous, interconnected components often shared
across computing units. While such resource sharing has of-
fered signiﬁcant beneﬁts to efﬁciency and cost, it has also
created an opportunity for new attacks that exploit CPU mi-
croarchitectural features. One class of these attacks consists
of software-based covert channels and side channel attacks.
Through these attacks, an adversary exploits unintended ef-
fects (e.g., timing variations) in accessing a particular shared
resource to surreptitiously exﬁltrate data (in the covert chan-
nel case) or infer a victim program’s secrets (in the side
channel case). These attacks have been shown to be capa-
ble of leaking information in numerous contexts. For ex-
ample, many cache-based side channel attacks have been
demonstrated that can leak sensitive information (e.g., cryp-
tographic keys) in cloud environments [48, 62, 82, 105, 112],
web browsers [37, 58, 73, 86] and smartphones [59, 90].
Fortunately, recent years have also seen an increase in the
awareness of such attacks, and the availability of counter-
measures to mitigate them. To start with, a large number of
existing attacks (e.g., [6, 7, 15, 25, 26, 35, 36, 77]) can be miti-
gated by disabling simultaneous multi-threading (SMT) and
cleansing the CPU microarchitectural state (e.g., the cache)
when context switching between different security domains.
Second, cross-core cache-based attacks (e.g., [20,38,62]) can
be blocked by partitioning the last-level cache (e.g., with Intel
CAT [61, 71]), and disabling shared memory between pro-
cesses in different security domains [99]. The only known
attacks that would still work in such a restrictive environment
(e.g., DRAMA [79]) exist outside of the CPU chip.
In this paper, we present the ﬁrst on-chip, cross-core side
channel attack that works despite the above countermeasures.
Our attack leverages contention on the ring interconnect,
which is the component that enables communication between
the different CPU units (cores, last-level cache, system agent,
and graphics unit) on many modern Intel processors. There
are two main reasons that make our attack uniquely chal-
lenging. First, the ring interconnect is a complex architecture
with many moving parts. As we show, understanding how
these often-undocumented components interact is an essential
prerequisite of a successful attack. Second, it is difﬁcult to
learn sensitive information through the ring interconnect. Not
only is the ring a contention-based channel—requiring precise
measurement capabilities to overcome noise—but also it only
sees contention due to spatially coarse-grained events such as
private cache misses. Indeed, at the outset of our investigation
it was not clear to us whether leaking sensitive information
over this channel would even be possible.
To address the ﬁrst challenge, we perform a thorough
reverse engineering of Intel’s “sophisticated ring proto-
col” [57,87] that handles communication on the ring intercon-
nect. Our work reveals what physical resources are allocated
to what ring agents (cores, last-level cache slices, and system
agent) to handle different protocol transactions (loads from
the last-level cache and loads from DRAM), and how those
physical resources arbitrate between multiple in-ﬂight trans-
USENIX Association
30th USENIX Security Symposium    645
action packets. Understanding these details is necessary for
an attacker to measure victim program behavior. For example,
we ﬁnd that the ring prioritizes in-ﬂight over new trafﬁc, and
that it consists of two independent lanes (each with four phys-
ical sub-rings to service different packet types) that service
interleaved subsets of agents. Contrary to our initial hypothe-
sis, this implies that two agents communicating “in the same
direction, on overlapping ring segments” is not sufﬁcient to
create contention. Putting our analysis together, we formulate
for the ﬁrst time the necessary and sufﬁcient conditions for
two or more processes to contend with each other on the ring
interconnect, as well as plausible explanations for what the
ring microarchitecture may look like to be consistent with
our observations. We expect the latter to be a useful tool for
future work that relies on the CPU uncore.
Next, we investigate the security implications of our ﬁnd-
ings. First, leveraging the facts that i) when a process’s loads
are subject to contention their mean latency is larger than that
of regular loads, and ii) an attacker with knowledge of our
reverse engineering efforts can set itself up in such a way
that its loads are guaranteed to contend with the ﬁrst pro-
cess’ loads, we build the ﬁrst cross-core covert channel on the
ring interconnect. Our covert channel does not require shared
memory (as, e.g., [38, 108]), nor shared access to any uncore
structure (e.g., the RNG [23]). We show that our covert chan-
nel achieves a capacity of up to 4.14 Mbps (518 KBps) from a
single thread which, to our knowledge, is faster than all prior
channels that do not rely on shared memory (e.g., [79]), and
within the same order of magnitude as state-of-the-art covert
channels that do rely on shared memory (e.g., [38]).
Finally, we show examples of side channel attacks that ex-
ploit ring contention. The ﬁrst attack extracts key bits from
vulnerable RSA and EdDSA implementations. Speciﬁcally, it
abuses mitigations to preemptive scheduling cache attacks to
cause the victim’s loads to miss in the cache, monitors ring
contention while the victim is computing, and employs a stan-
dard machine learning classiﬁer to de-noise traces and leak
bits. The second attack targets keystroke timing information
(which can be used to infer, e.g., passwords [56, 88, 111]). In
particular, we discover that keystroke events cause spikes in
ring contention that can be detected by an attacker, even in
the presence of background noise. We show that our attack
implementations can leak key bits and keystroke timings with
high accuracy. We conclude with a discussion of mitigations.
2 Background and Related Work
CPU Cache Architecture CPU caches on modern x86 In-
tel microarchitectures are divided in L1, L2 and L3 (often
called last-level cache or LLC). The L1 and (in most microar-
chitectures) L2 caches are fast (4 to 12 cycles), small, and
local to each CPU core. They are often referred to as pri-
vate caches. The LLC is slower (40 to 60 cycles), bigger, and
shared across CPU cores. Since Nehalem-EX [54], the LLC
Figure 1: Logical block diagram of the ring interconnect on
client-class Intel CPUs. Ring agents are represented as white
boxes, the interconnect is in red and the ring stops are in blue.
While the positioning of cores and slices on the die varies [21],
the ordering of their respective ring stops in the ring is ﬁxed.
is divided into LLC slices of equal size, one per core.
Caches of many Intel CPUs are inclusive, meaning that
data contained in the L1 and L2 caches must also reside in the
LLC [47], and set-associative, meaning that they are divided
into a ﬁxed number of cache sets, each of which contains
a ﬁxed number of cache ways. Each cache way can ﬁt one
cache line which is typically of 64 bytes in size and represents
the basic unit for cache transactions. The cache sets and the
LLC slice in which each cache line is stored are determined
by its address bits. Since the private caches generally have
fewer sets than the LLC, it is possible for cache lines that map
to different LLC sets to map to the same L2 or L1 set.
When a core performs a load from a memory address, it
ﬁrst looks up if the data associated to that address is available
in the L1 and L2. If available, the load results in a hit in
the private caches, and the data is serviced locally. If not, it
results in a miss in the private caches, and the core checks
if the data is available in the LLC. In case of an LLC miss,
the data needs to be copied from DRAM through the memory
controller, which is integrated in the system agent to manage
communication between the main memory and the CPU [47].
Intel also implements hardware prefetching which may result
in additional cache lines being copied from memory or from
the LLC to the private caches during a single load.
Ring Interconnect The ring interconnect, often referred
to as ring bus, is a high-bandwidth on-die interconnect
which was introduced by Intel with the Nehalem-EX micro-
architecture [54] and is used on most Intel CPUs available
in the market today [47]. Shown in Figure 1, it is used for
intra-processor communication between the cores, the LLC,
the system agent (previously known as Uncore) and the GPU.
For example, when a core executes a load and it misses in its
private caches (L1-L2), the load has to travel through the ring
interconnect to reach the LLC and/or the memory controller.
The different CPU consumers/producers communicating on
the ring interconnect are called ring agents [50]. Each ring
agent communicates with the ring through a ring stop (some-
times referred to as interface block [50], node router [11, 27],
or ring station [84]). Every core shares its ring stop with one
LLC slice. To minimize latency, trafﬁc on the ring intercon-
646    30th USENIX Security Symposium
USENIX Association
CPU Core 0LLC Slice 0…LLC Slice nSystem Agent…CPU Core nGraphicsnect always uses the shortest path, meaning that ring stops
can inject/receive trafﬁc in both directions (right or left in our
diagram) and always choose the direction with the shortest
path to the destination. Finally, the communication protocol
on the the ring interconnect makes use of four physical rings:
request, snoop, acknowledge and data ring [57].
2.1 Microarchitectural Side Channels
Most microarchitectural channels can be classiﬁed using two
criteria. First, according to the microarchitectural resource
that they exploit. Second, based on the degree of concurrency
(also referred to as leakage channel) that they rely on [31].1
Figure 2: Load latency (in cycles) from different LLC slices s
(and ﬁxed cache set index p = 5) on each core c of our Coffee
Lake CPU. The latency grows when the distance between the
core’s ring stop and the target LLC slice’s ring stop grows.
Target Resource Type We distinguish between eviction-
based (also referred to as persistent- or residual-state) and
contention-based (also known as transient state) attacks.
Eviction-based channels are stateful: the adversary actively
brings the microarchitectural resource into a known state, lets
the victim execute, and then checks the state of the shared
resource again to learn secrets about the victim’s execution. In
these attacks, the side effects of the victim’s execution leave a
footprint that is not undone when the victim code completes.
The root cause of these attacks is the limited storage space of
the shared microarchitectural resource. Examples of shared
resources that can be used for eviction-based channels are
the L1 data [60, 74, 77] and instruction [2, 5, 112] caches, the
TLB [36], the branch target buffer (BTB) [24,25] and the last-
level cache (LLC) [20, 33, 38, 39, 48, 51, 62, 66, 85, 108, 113].
Contention-based channels are stateless: the adversary pas-
sively monitors the latency to access the shared resource and
uses variations in this latency to infer secrets about the vic-
tim’s execution. In these attacks, the side effects of the vic-
tim’s execution are only visible while the victim is executing.
The root cause of these attacks is the limited bandwidth capac-
ity of the shared resource. Examples of resources that can be
used for contention-based channels are functional units [102],
execution ports [7, 15, 35], cache banks [110], the memory
bus [105] and random number generators [23]. The attack
presented in this paper is a contention-based channel.
Leakage Channel We further classify attacks as either re-
lying on preemptive scheduling, SMT or multicore techniques.
Preemptive scheduling approaches [2,10,17,25,26,40,41,70,
74, 83, 100, 112], also referred to as time-sliced approaches,
consist of the victim and the attacker time-sharing a core. In
these attacks, the victim and the attacker run on the same core
and their execution is interleaved. Simultaneous multithread-
ing (SMT) approaches [3, 4, 7, 36, 60, 74, 77, 102] rely on the
victim and the attacker executing on the same core in parallel