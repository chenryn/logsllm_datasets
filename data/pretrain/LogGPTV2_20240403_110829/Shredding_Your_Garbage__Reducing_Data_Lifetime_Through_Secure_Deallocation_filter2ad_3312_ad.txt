In the Emacs process, tainted areas included: a global
circular queue of Emacs input events, the entire password
as a Lisp string, a set of individual Lisp character strings
containing one password character each, one copy on the
Emacs stack, and a global variable that tracks the user’s
last 100 keystrokes. (Much of Emacs is implemented in
Lisp.) No copies were found in the su process, which
seems to do a good job of cleaning up after itself.
Our modiﬁed kernel cleared all of these taints when
Emacs was terminated. (The PRNG’s entropy pool was
still tainted, but it is designed to resist attempts to recover
input data.)
7 Performance Overhead
)
c
e
s
/
s
e
t
y
b
a
g
G
i
(
d
e
e
p
S
g
n
o
r
e
Z
i
bzero
rep stosl
manual loop
SSE optimized
non-temporal
11
10
9
8
7
6
5
4
3
2
1
0
16
32
64
128
256
512
1,024
2,048
4,096
8,192
16,384
32,768
65,536
262,144
1,048,576
524,288
2,097,152
Block size
Figure 5: Comparison of speed for different zeroing tech-
niques on a 2.40 GHz Pentium 4. This chart shows the speed in
GB/s of zeroing a single block of n bytes repeatedly until 4 GB
of zeros have been written. Block sizes are powers of two and
blocks are aligned on block boundaries. Refer to the text for a
description of each zeroing method.
posing the overheads to be unacceptable. However, these
fears are largely unfounded.
Data that are in cache and properly aligned can be ze-
roed rapidly, as we will show. In fact, applications sel-
dom allocate and free enough data to make any apprecia-
ble change to their running time.
In this section we show that with careful implementa-
tion, zeroing can generally be done with nominal over-
head. We show experimentally that user level clearing
can be achieved with minimal impact on a wide range of
applications. And similarly, that kernel clearing can be
performed without signiﬁcantly impacting either CPU or
I/O intensive application performance.
All experiments were run on an x86 Linux platform
with 1 GB of memory and a 2.40 GHz Pentium 4.
7.1 Implementing Zeroing
All the allocators on our system dole out blocks of data
aligned to at least 4-byte boundaries. malloc by de-
fault aligns blocks to 8-byte boundaries. Also by default,
GCC aligns stack frames on 16-byte boundaries. Given
common application allocation patterns, most heap and
stack data freed and reallocated are recently used, and
thus also likely in cache.
These alignment and cache properties allow even
modest machines to zero data at blindingly fast speeds.
Figure 5 illustrates this fact for ﬁve different methods of
zeroing memory:
bzero, an out-of-line call to glibc’s bzero function.
Programmers and system designers seem to scoff at the
idea of adding secure deallocation to their systems, sup-
rep stosl, inline assembly using an x86 32-bit string
store instruction.
USENIX Association
14th USENIX Security Symposium
341
Running Max In-Flight
mallocs
299
68,036
25,196
7
2,397
2,397,499
472,711
105,210
90,327
Time
3m:16s
5m:12s
2m:27s
4m:42s
11m:25s
3m:26s
3m:37s
9m:15s
6s
Total
mallocs
436,222
107,659
110,315
153
5,283
31,361,153
4,622,368
574,572
218,501
Total
frees
436,187
103,425
93,823
147
4,125
30,847,487
4,400,552
492,729
219,936
Total Bytes
freed
110,886,084
5,355,300
545,877,388
1,111,236
380,996
6,368,737,804
1,934,800,720
16,759,620
74,545,704
Free Rate
(bytes/sec)
565,745
17,164
3,713,451
3,940
556
30,916,202
8,916,132
30,197
12,081,962
164.gzip
175.vpr
176.gcc
197.parser
252.eon
253.perlbmk
255.vortex
300.twolf
ﬁrefox
Table 2: Non-trivial allocations by programs in our zero-on-free heap experiment. Max In-Flight mallocs gives the maximum
number of memory allocations alive at the same time. All other numbers are aggregates over the entire run. Runs with under 100 K
of freed data are not shown.
manual loop, inline assembly that stores 32 bits at a
time in a loop.
SSE optimized, a out-of-line call to our optimized ze-
roing function that uses a loop for small objects and
SSE 128-bit stores for large objects.
non-temporal, a similar function that substitutes non-
temporal writes for ordinary 128-bit SSE stores.
(Non-temporal writes bypass the cache, going di-
rectly to main memory.)
For small block sizes, ﬁxed overheads dominate. The
manual loop is both inlined and very short, and thus
fastest. For block sizes larger than the CPU’s L2 cache
(512 kB on our P4), the approximately 2 GB/s memory
bus bandwidth limits speed. At intermediate block sizes,
128-bit SSE stores obtain the fastest results.
Zeroing unused data can potentially pollute the CPU’s
cache with unwanted cache lines, which is especially a
concern for periodic zeroing policies where data is more
likely to have disappeared from cache. The non-temporal
curve shows that, with non-temporal stores, zeroing per-
formance stays constant at memory bus bandwidth, with-
out degradation as blocks grow larger than the L2 cache
size. Moreover, the speed of non-temporal zeroing is
high, because cleared but uncached data doesn’t have to
be brought in from main memory.
When we combine these results with our observations
about common application memory behavior, we see that
zeroing speeds far outpace the rate at which memory is
allocated and freed. Even the worst memory hogs we
saw in Table 1 only freed on the order of hundreds of
MB of data throughout their entire lifetime, which incurs
only a fraction of a second of penalty at the slowest, bus-
bandwidth zeroing rate (2 GB/s).
7.2 Measuring Overhead
To evaluate the overheads of secure deallocation, we
ran test workloads from the SPEC CPU2000 benchmark
suite, a standardized CPU benchmarking suite that con-
tains a variety of user programs. By default, the tests
contained in the SPEC benchmarks run for a few minutes
(on our hardware); it lacks examples of long-lived GUI
processes or server processes, which have especially in-
teresting data lifetime issues.
However, we believe that because the SPEC bench-
mark contains many programs with interesting memory
allocation behavior (including Perl, GCC, and an object-
oriented database), that the performance characteristics
we observe for SPEC apply to these other programs as
well. In addition to this, we ran an experiment with the
Firefox 1.0 browser. We measured the total time required
to startup a browser, load and render a webpage, and then
shut-down.
7.2.1 Heap Clearing Overhead
We implemented a zero-on-free heap clearing policy by
creating a modiﬁed libc that performs zeroing when heap
data is deallocated. Because we replaced the entire libc,
modifying its internal memory allocator to do the ze-
roing, we are able to interpose on deallocations per-
formed within the C library itself, in addition to any
done by the application. To test heap clearing, we simply
point our dynamic linker to this new C library (e.g. via
LD LIBRARY PATH), and then run our test program.
For each program, we performed one run with an un-
modiﬁed C library, and another with the zero-on-free li-
brary. Figure 6 gives the results of this experiment, show-
ing the relative performance of the zero-on-free heap
allocator versus an unmodiﬁed allocator. Surprisingly,
zero-on-free overheads are less than 7% for all tested ap-
plications, despite the fact that these applications allocate
hundreds or thousands of megabytes of data during their
lifetime (as shown in Table 2).
An interesting side-effect of our heap clearing experi-
ment is that we were able to catch a use-after-free bug in
one of the SPEC benchmarks, 255.vortex. This pro-
gram attempted to write a log message to a stdio FILE
342
14th USENIX Security Symposium
USENIX Association
d
e
e
p
S
1.4
1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
164.gzip
Unmodifed
Heap Clearing
175.vpr
176.gcc
186.crafty
181.mcf
197.parser
252.eon
253.perlbmk
255.vortex
254.gap
256.bzip2
300.twolf
d
e
e
p
S
1.8
1.7
1.6
1.5
1.4
1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
164.gzip
firefox
Unmodified
Periodic Clearing
Immediate Clearing
175.vpr
176.gcc
186.crafty
181.mcf
197.parser
252.eon
253.perlbmk
255.vortex
254.gap
256.bzip2
300.twolf
firefox
Figure 6: Heap clearing has little performance impact. This
chart shows the relative performance of an unmodiﬁed glibc
2.3 heap allocator versus the same allocator modiﬁed to zero at
free time in a set of user programs. The unmodiﬁed runs are
normalized to 1.0. Zero-on-free overheads are less than 7% for
all tested applications.
after it had closed the ﬁle. Removing the fclose call
ﬁxed the bug, but we had to touch the sources to do this.
We don’t believe this impacted our performance results.
7.2.2 Stack Clearing Overhead
We implemented stack clearing for applications by mod-
ifying our OS to periodically zero the free stack space in
user processes that have run since the last time we cleared
stacks. We do so by writing zero bytes from the user’s
stack pointer down to the bottom of the lowest page allo-
cated for the stack.
Figure 7 gives the results of running our workload with
periodic stack clearing (conﬁgured with a period of 5
seconds) plus our other kernel clearing changes. Just like
heap clearing, periodic stack clearing had little impact
on application performance, with less than a 2% perfor-
mance increase for all our tests.
Immediate Stack Clearing For those applications
with serious data lifetime concerns, the delay inherent
to a periodic approach may not be acceptable. In these
cases, we can perform an analog of our heap clearing
methodology by clearing stack frames immediately when
they are deallocated.
We implemented immediate stack clearing by modify-
ing GCC 3.3.4 to emit a stack frame zeroing loop in every