### Tainted Areas in the Emacs Process

In the Emacs process, several areas were identified as tainted:
- A global circular queue of Emacs input events.
- The entire password stored as a Lisp string.
- Individual Lisp character strings, each containing one password character.
- One copy on the Emacs stack.
- A global variable tracking the user’s last 100 keystrokes.

Much of Emacs is implemented in Lisp. No copies of the password were found in the `su` process, which appears to effectively clean up after itself. Our modified kernel cleared all these taints when Emacs was terminated. (The PRNG's entropy pool remained tainted, but it is designed to resist attempts to recover input data.)

### Performance Overhead

#### Zeroing Techniques and Their Speeds

Figure 5 compares the speeds of different zeroing techniques on a 2.40 GHz Pentium 4. The chart shows the speed in GB/s of zeroing a single block of \( n \) bytes repeatedly until 4 GB of zeros have been written. Block sizes are powers of two and are aligned on block boundaries. The zeroing methods include:
- **bzero**: An out-of-line call to glibc’s `bzero` function.
- **rep stosl**: Inline assembly using an x86 32-bit string store instruction.
- **Manual loop**: Inline assembly that stores 32 bits at a time in a loop.
- **SSE optimized**: An out-of-line call to our optimized zeroing function that uses a loop for small objects and SSE 128-bit stores for large objects.
- **Non-temporal**: A similar function that substitutes non-temporal writes for ordinary 128-bit SSE stores (non-temporal writes bypass the cache, going directly to main memory).

For small block sizes, fixed overheads dominate, making the manual loop the fastest. For block sizes larger than the CPU’s L2 cache (512 kB on our P4), the approximately 2 GB/s memory bus bandwidth limits speed. At intermediate block sizes, 128-bit SSE stores achieve the fastest results.

Zeroing unused data can potentially pollute the CPU’s cache with unwanted cache lines, especially in periodic zeroing policies where data is more likely to have disappeared from cache. The non-temporal curve shows that, with non-temporal stores, zeroing performance remains constant at memory bus bandwidth without degradation as blocks grow larger than the L2 cache size. Moreover, the speed of non-temporal zeroing is high because cleared but uncached data doesn’t need to be brought in from main memory.

Combining these results with common application memory behavior, we see that zeroing speeds far outpace the rate at which memory is allocated and freed. Even the worst memory hogs, as shown in Table 1, only freed on the order of hundreds of MB of data throughout their entire lifetime, incurring only a fraction of a second of penalty at the slowest, bus-bandwidth zeroing rate (2 GB/s).

### Measuring Overhead

#### Test Workloads and SPEC CPU2000 Benchmark Suite

To evaluate the overheads of secure deallocation, we ran test workloads from the SPEC CPU2000 benchmark suite, a standardized CPU benchmarking suite containing a variety of user programs. By default, the tests in the SPEC benchmarks run for a few minutes on our hardware. Although it lacks examples of long-lived GUI processes or server processes, we believe the performance characteristics observed for SPEC apply to these other programs as well. Additionally, we ran an experiment with Firefox 1.0, measuring the total time required to start up the browser, load and render a webpage, and then shut down.

#### Heap Clearing Overhead

We implemented a zero-on-free heap clearing policy by creating a modified libc that performs zeroing when heap data is deallocated. Replacing the entire libc allowed us to interpose on deallocations performed within the C library itself, in addition to those done by the application. To test heap clearing, we pointed our dynamic linker to this new C library (e.g., via `LD_LIBRARY_PATH`) and ran our test program.

For each program, we performed one run with an unmodified C library and another with the zero-on-free library. Figure 6 shows the relative performance of the zero-on-free heap allocator versus an unmodified allocator. Surprisingly, zero-on-free overheads were less than 7% for all tested applications, despite allocating hundreds or thousands of megabytes of data during their lifetime (as shown in Table 2).

An interesting side-effect of our heap clearing experiment was catching a use-after-free bug in one of the SPEC benchmarks, 255.vortex. This program attempted to write a log message to a stdio `FILE` after closing the file. Removing the `fclose` call fixed the bug, but we had to modify the sources. We do not believe this impacted our performance results.

#### Stack Clearing Overhead

We implemented stack clearing for applications by modifying our OS to periodically zero the free stack space in user processes that have run since the last stack clearing. We did this by writing zero bytes from the user’s stack pointer down to the bottom of the lowest page allocated for the stack.

Figure 7 shows the results of running our workload with periodic stack clearing (configured with a period of 5 seconds) plus our other kernel clearing changes. Just like heap clearing, periodic stack clearing had little impact on application performance, with less than a 2% performance increase for all our tests.

**Immediate Stack Clearing**

For applications with serious data lifetime concerns, the delay inherent to a periodic approach may not be acceptable. In these cases, we can perform an analog of our heap clearing methodology by clearing stack frames immediately when they are deallocated.

We implemented immediate stack clearing by modifying GCC 3.3.4 to emit a stack frame zeroing loop in every function.