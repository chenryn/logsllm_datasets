3.2.1 Experiment 1. We tested 15 experimental conditions designed
to answer four high-level research questions. In order to both
quantify the impact of blocklists relative to policies that only re-
quired composition requirements (RQ1) and to find blocklist re-
quirements for use in 1c8 policies that performed well on both
security and usability dimensions (RQ2) we tested blocklist configu-
rations that were either commonly used or recommended by prior
work. Our third high-level research question focused on the im-
pact of character-class and minimum-length requirements on NN6
1This API employs privacy-protecting mechanisms to protect the confidentiality of
submitted passwords: it only accepts SHA-1 hashes of passwords and utilizes a k-
anonymity range search to report matches [9].
2We use a computationally efficient ciss implementation that performs multiple sub-
string searching via the Rabin-Karp algorithm [2].
Baseline
RQ1: What is the impact of adding a blocklist to 1c8 and 3c8?
1c8
1c8+Pwned-fs, 1c8+Xato-cifs, 1
Comparisons Exp.
1c8+Xato-strip-cifs, 1c8+Xato-ciss
3c8+Xato-cifs 1
1c8+Pwned-fs, 1c8+Xato-cifs, 1c8+Xato-ciss 1
1c8+Pwned-fs 2
3c8
RQ2: What is the impact of varying blocklist reqs. on 1c8?
1c8+Xato-strip-cifs
1c8+Xato-strip-cifs
RQ3: What is the impact of varying char-class and min-length
reqs. on NN6?
3c8+NN6
RQ4: How do min-strength reqs. compare with blocklists?
1c8+NN6, 1c16+NN6, 2c12+NN6, 3c12+NN6 1
1c8+Pwned-fs, 1c8+Xato-cifs,
1c8+NN6
1c8+Xato-strip-cifs, 1c8+Xato-ciss 1
1c8+NN8, 1c8+NN10 2
1c8+Pwned-fs
1c8+Xato-strip-cifs
1c8+NN8, 1c8+NN10 2
RQ5: How do min-strength reqs. interact with min-length reqs.?
1c8+NN10
1c8+NN8, 1c10+NN10 2
1c8+NN8, 1c10+NN10 2
1c10+NN8
1c12+NN10
1c8+NN10, 1c10+NN10 2
RQ6: How do blocklist reqs. interact with char-class reqs.?
1c8+Xato-strip-cifs
4c8+Xato-strip-cifs 2
1c8+Pwned-fs
4c8+Pwned-fs 2
4c8+Xato-strip-cifs
4c8+Pwned-fs 2
Table 1: Research questions and planned comparisons.
minimum-strength policies (RQ3). The particular set of character-
class and minimum-length combinations we explored included com-
position policies explored in prior work. Our fourth research ques-
tion involved directly comparing blocklist and minimum-strength
policies on both security and usability dimensions (RQ4). In Ex-
periment 1, we explored this question by comparing a variety of
blocklists with a NN6 minimum-strength policy that we hypoth-
esized would provide adequate protection against online attacks,
withstanding at least 106 guesses (RQ4.A).
3.2.2 Experiment 2. The results of Experiment 1 raised additional
research questions that could not be answered with the experimen-
tal data that had already been collected. Therefore, we conducted
a second experiment, testing seven new conditions and re-testing
three conditions from Experiment 1.3 One goal of Experiment 2 was
to explore how specific minimum-length requirements interacted
with specific minimum-strength requirements to effect usability or
security (RQ5). In particular, we hypothesized that longer minimum-
length requirements could make minimum-strength requirements
easier to satisfy. We explored this question using strength thresh-
olds in-between those we tested in Experiment 1; results from
that experiment had suggested NN8 and NN10 requirements may
3We collected new data for each policy in Experiment 2, even if that policy had been
previously tested in Experiment 1.
provide the level of offline protection needed for high-value ac-
counts. Experiment 2 was also designed to test whether blocklist
requirements impact password strength or policy usability differ-
ently depending on the particular character-class requirements they
are combined with, and vice versa (RQ6). We hypothesized that
fullstring blocklist checks against lists of leaked passwords might
be less useful for policies requiring many character classes, since
leaked passwords may be less likely to contain many character
classes. We also hypothesized that the strip-cifs matching algorithm
might be especially frustrating to users when combined with a 4c8
policy; compared to 1c8 passwords, 4c8 candidate passwords might
be more likely to incorporate digits and symbols in ways that would
be rejected by blocklist checks that first strip digits and symbols.
Lastly, we revisited RQ4 comparing top-performing blocklist poli-
cies from Experiment 1 with the additional NN10 and NN12 policies
tested in Experiment 2 (RQ4.B).
3.3 User-study protocol
For each experiment we ran a user study on Amazon Mechanical
Turk in which participants were tasked with creating and recalling
a password under a randomly assigned password policy. The design
of our user studies closely followed that of prior work [13, 26, 28].
In Part 1, participants were asked to role play, imagining that they
needed to create a new password because their main email account
provider had been breached. We emailed participants two days
later asking them to participate in Part 2, in which they were asked
to recall their password. We considered the data of only the par-
ticipants who completed Part 2 between two and five days after
Part 1. After each part, participants completed a survey that col-
lected demographic and usability-related data. The survey materials
are provided in Appendix E.
The password-creation task in Part 1 used a password meter de-
veloped in prior work, which incorporated real-time requirements
feedback, a password-strength bar, and text feedback on improving
password strength. Participants were shown feedback on improving
password strength only after all composition, minimum-strength,
and blocklist requirements were satisfied. The password meter’s
configuration was based on best practices empirically shown by
prior work [28]. We communicated unmet minimum-strength and
blocklist requirements as follows: for the Xato blocklist configu-
rations the meter reported that the password must “not be an ex-
tremely common password;” for the Pwned blocklist configurations
that the password must “not use a password found in previous se-
curity leaks;” and for the minimum-strength requirements that the
password must “not be similar to common passwords” (Figure 1).
We submitted the passwords created by participants to PGS [21],
which computed guess numbers for each PGS-supported guessing
approach using its recommended configuration. We additionally
computed guess numbers using a set of neural networks (collec-
tively referred to as the PGS3 NN) that we trained ourselves, closely
following the design and implementation of password models in
prior work [16]. When computing min-auto guess numbers, we
selected each password’s lowest guess number among all guessing
approaches. For the NN guessing approach, we use PGS3 NN guess
numbers in place of PGS-reported NN guess numbers, given the
improved guessing performance of the PGS3 NN (see Appendix A).
Figure 1: Password-creation meter displaying unmet pass-
word policy requirements.
In addition to evaluating the strength and objective usability (e.g.,
memorability) of passwords created under each policy, we wanted
to understand their usability in terms of user difficulty or frustration
when creating or recalling passwords. Participants’ responses to
surveys shown after both Part 1 and Part 2 shed light on this. Our
surveys also asked questions such as whether participants reused a
previous password or wrote their password down after creating it.
In order to elicit truthful responses we told participants that they
would receive compensation regardless of their answers.
We instrumented our study to record password-creation and
recall keystrokes and whether participants copied and pasted their
password during recall tasks. When analyzing password recall, we
only analyze data for participants who: typed in their password
from memory (as self-reported in the survey); said they didn’t reuse
their study password (as self-reported); and didn’t copy and paste
their password during the Part 2 recall task, either manually from a
file or using a password manager/browser (based on keystroke data).
Study participants who become frustrated with password-creation
requirements may be more likely to drop out of our study. We
record and analyze dropout rates between experimental conditions
as potential evidence of usability issues for a given policy.
The full set of usability metrics we considered include both objec-
tive (creation/recall time, recall success, study dropout, copy/paste
from storage/password managers) and subjective data (creation
annoyance/difficulty, difficulty remembering).4 Each of these met-
rics have been used in prior work to measure usability impacts of
password-creation policies [24, 27, 28].
We recruited study participants from Mechanical Turk (MTurk).
Workers were required to be located in the United States, have had
at least 500 HITs approved, and have a HIT approval rate of 95% or
higher. Workers were not allowed to participate in our study more
than once. We paid 55 cents for Part 1 of our study and 70 cents
for Part 2. Our study protocol was approved by our institutional
review board and all participants completed online consent forms.
Experiment 1 participants were recruited in July and August 2019.
Their ages ranged from 18 to 81 years, with a median of 35. 53% of
participants were female and 47% male. Of the 5,099 participants
who started the study, 4,317 finished Part 1 and 3,463 also finished
Part 2. Most (81%) participants reported that they did not have a
4As we employ real-time feedback in our password meter, our study data do not include
the notion of a password-creation attempt. However, this concept is closely related to
creation time and creation annoyance/difficulty.
technical degree or work in an area related to computer science or
information technology. Experiment 2 participants were recruited
in October and November 2019. Their ages ranged from 18 to 90
years, with a median of 35. 56% of participants were female, 43%
male, 1% reported their gender as “Other,” and the remainder chose
not to answer. Of the 4,817 participants who started the study, 4,005
finished Part 1 and 3,014 also finished Part 2. Our password-recall
analysis includes data for 1,518 participants in Experiment 1 and
1,362 participants in Experiment 2, excluding those who reported
reusing a password or not entering their password from memory.
3.4 Statistical analysis
Before running each experiment, we identified a set of hypothesis
tests we planned to conduct to answer our research questions. We
perform omnibus tests to compare three or more conditions as well
as pairwise tests.5 For each family of tests (the combination of test
type and research question), we chose the baseline condition to be
used in pairwise comparisons before collecting data.
To compare the overall strength of passwords created under
different policies, we use an extension of the Log-rank test called
the Peto-Peto test (PP). This test, used in prior work [14], weighs
early-appearing differences in guess curves more heavily than later
differences, corresponding to heavier weight for strength differ-
ences that resource-constrained or rate-limited attackers could
exploit. The Peto-Peto test is also appropriate when many data
points are censored. In our study, passwords with guess numbers
past our offline attack threshold of 1014 were censored prior to the
test (i.e., labeled as unguessed), as we wanted to compare password
guessability only up to the number of guesses that a typical attacker
could feasibly attempt in an offline attack.
To compare the vulnerability of passwords to guessing attacks
of different magnitudes, we apply Chi-square tests of independence
and Fisher’s exact tests (FET) to the percent of passwords in each set
that an attacker would guess within 106 and within 1014 attempts.6
These thresholds have been used in prior work as estimates of
how many guesses an online and an offline attacker could make [7],
respectively. Unless otherwise noted, analyses that operate on guess
numbers are based on min-auto guess number estimates.
We examine usability through statistical tests of Part 1 and
Part 2 survey data (password-creation sentiment, post-creation
actions) and behavioral data collected by our study framework
(study dropout rates, password-creation time, Part 2 recall time,
and Part 2 recall success). We bin categorical and Likert data before
applying Chi-square tests and Fisher’s tests (e.g., Likert agreement
data is grouped into two bins: “Strongly agree” or “Agree” vs. oth-
erwise). For comparing count data, we use the non-parametric
Kruskal-Wallis (KW) and Mann-Whitney U (MWU) tests.
We record whether text entered into the password-creation field
failed to meet requirements, but the real-time nature of require-
ments feedback in our meter means that even if a blocklist or
5We tried a Cox regression model to measure guessability differences but opted for
pairwise hypothesis tests instead, due to poor fit of the linear model to our data.
6Our conservative assumption is that the attacker knows the password distribution and
makes guesses in order of decreasing probability. While we assume the attacker knows
the length and character-class requirements when making guesses, we do not assume
that the attacker knows which passwords would have been rejected by blocklist or
minimum-strength requirements in order to avoid guessing those passwords.
minimum-strength requirement was unsatisfied at some point, the
participant may not have intended to actually create that password—
they may have had a different password in mind and hadn’t finished
typing it. To shed light on whether participants actually encoun-
tered one of these unmet requirements for a password, our survey
asked “were any passwords you tried to create rejected for the
specific reason shown above?” We interpret affirmative answers as
evidence that those participants changed their password at least
once due to the associated policy requirement.
Within each family of tests, we only perform pairwise tests
if the corresponding omnibus test is statistically significant. We
use the Holm-Bonferroni method to correct for multiple pairwise
comparisons within each family and report adjusted p-values. All
hypothesis tests use a significance level of 0.05. When comparing
two policies, we only attribute differences to a particular policy
dimension if all other dimensions in those policies are identical.
3.5 Limitations
Our study has limitations common to user studies conducted on
MTurk. Study participants may not have created passwords similar
to those they would have created for actual high-value accounts,
despite our role-playing instructions. However, prior work has
shown that MTurk passwords collected in this way are similar to
actual user passwords created for high-value accounts [6, 14].
Our password-policy results and recommendations rely on pass-
words being created under the specific password meter we used in
our study. This meter provided text feedback on how to improve
passwords, a strength bar, and real-time requirements feedback,
each of which was configured according to recommendations from
prior studies [25, 28, 29]. Based on survey responses, the major-
ity of participants found the meter to be informative, helpful, and
influential. For example, most participants reported that they im-
plemented changes suggested by text feedback and that it was
important to them that the colored bar rated their password highly.
Experiments using password meters with substantially different
implementations may produce different results.
It is worth noting that our analysis and recommendations con-
cerning blocklists do not apply to site-specific or user-specific block-
lists, which are useful for preventing passwords based on user-
associated data or contextual information that targeted guessing
attacks could leverage (e.g., user IDs, words related to the service).
4 RESULTS
The results we report here lead to our recommendation for password
policies that include both minimum-length and minimum-strength
requirements. In case an organization decides against minimum-
strength requirements, we recommend two policies incorporating
minimum-length and blocklist requirements. These policies provide
less protection than minimum-strength policies against offline at-
tacks, but provide adequate protection against online attacks while
remaining usable during password creation.
Our results from Experiment 1 show that blocklists may not
improve password strength substantially if the blocklist check uses
a strict matching algorithm with an insufficiently large wordlist.
However, when properly configured, either blocklist requirements
or minimum-strength requirements can be combined with other re-
quirements to provide adequate protection against online guessing
attacks. In Experiment 2 we explore in more depth 1c8 minimum-
strength policies that provide strong protection against both online
and offline guessing attacks. We also extensively analyze interac-
tion effects between policy components. Experiment 2 results show
that NN8 and NN10 policies can be just as usable as the blocklist
policies we test, while also producing passwords more resistant to
offline attacks. In this section, we describe the results from both
experiments, organized by research question. P-values for each
statistical test can be found in Appendix F.1.
4.1 RQ1: Impact of blocklists
We compared each blocklist condition to its corresponding 1c8
or 3c8 baseline condition to quantify the impact of blocklists on
guessability and usability. We found blocklist configurations
1c8+Pwned-fs and 1c8+Xato-strip-cifs significantly improved
password strength over their baseline without substantial
harm to usability.
As shown in Figure 2, passwords created under either 1c8+Xato-
cifs or 3c8+Xato-cifs were neither stronger overall nor less likely
to be guessed in online attacks than passwords created under the
baseline policies that only contained composition requirements.
While blocklist policies that use full-string matching can provide
adequate protection against online guessing attacks (as demon-
strated by 1c8+Pwned-fs), our results suggest that this requires a
much larger wordlist than the Xato wordlist we tested.
Of the policies with blocklists that improved password defense
against online attacks, two policies did so without also making
passwords substantially more difficult or time-consuming to create.
Both 1c8+Pwned-fs and 1c8+Xato-strip-cifs passwords were much
less likely to be guessed in online attacks (within 106 guesses) than
1c8 passwords (FET: 0% and 1% guessed, resp., vs. 6% guessed). Yet,
participants did not find either policy substantially more annoying
or difficult relative to a 1c8 policy (see Figure 10 in Appendix).
4.2 RQ2: Blocklist reqs. for 1c8 policies
We next compared 1c8 blocklist policies. All pairwise compar-
isons were made with respect to Xato-strip-cifs, which was rec-
ommended in prior work [8]. We found that case-sensitive, full-
string matching against very large blocklists of leaked pass-
words leads to similarly usable and secure passwords as fuzzy
matching against smaller blocklists of the most common
leaked passwords.
Prior work hypothesized that the strip-cifs algorithm would
produce strong passwords by preventing simple modifications to
blocklisted passwords that might pass the blocklist check without
improving password strength [8]. Our user study confirms this.
As shown in Figure 2, passwords created under the 1c8+Xato-cifs
policy, which did not strip digits and symbols before performing
blocklist checks, were overall weaker and more susceptible to on-
line guessing than passwords created under the 1c8+Xato-strip-
cifs policy, which stripped digits and symbols (PP, FET: 5% vs. 1%
guessed). Furthermore, while the stricter matching algorithm used
in 1c8+Xato-strip-cifs led to slightly longer password-creation times
Figure 2: Min-auto guess numbers for Experiment 1 block-
list, composition-requirements-only, and 1c8+NN6 policies.
compared to 1c8+Xato-cifs (MWU: median of 93 s vs. 70 s), it did
not make password creation more challenging or annoying.
Among blocklist configurations using the same Xato wordlist,
only 1c8+Xato-ciss produced overall stronger passwords that were
more resistant to 1014 offline attacks than 1c8+Xato-strip-cifs (PP,
FET: 24% vs. 41% guessed). However, as shown in Figure 10 and
Table 2, severe password-creation usability issues associated with
1c8+Xato-ciss prevent us from recommending it in place of 1c8+Xato-
strip-cifs. Participants took longer to create passwords under 1c8+Xato-
ciss than under 1c8+Xato-strip-cifs (MWU: median of 139 s vs. 93
s) and reported more annoyance (FET: 47% vs. 35%) and difficulty
(FET: 49% vs. 27%). Compared to 1c8+Xato-strip-cifs participants,
1c8+Xato-ciss participants were also more likely to drop out before
finishing Part 1 (FET: 26% vs. 12%) and to digitally store or write
down their password after creating it (FET: 65% vs. 53%). These