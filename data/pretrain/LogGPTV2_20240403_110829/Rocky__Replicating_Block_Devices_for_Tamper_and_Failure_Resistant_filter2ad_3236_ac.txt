owners download the streamed write sequence via the connetor-
cloudlet, they can adjust the replication rate at their will depending
on their own resource utilization status.
Storage Architecture. Figure 5 shows the architecture of Rocky
storage and connector-cloudlet (hereafter, “cloud storage”). Rocky
storage contains the raw block store (RBS), version map, endpoint
block snapshot store (EBSS) and dirty/presence/epoch bitmaps. Cloud
storage is used in a way that it contains epoch bitmaps and cloud
block snapshot store (CBSS). We will explain how each component
contributes to Rocky’s replication protocol for tamper and failure
resistance in more details in the following sections.
3.2 Presence/Dirty Bitmaps
For block I/O handling, RC uses two types of bitmaps: a presence
bitmap and a dirty bitmap. The presence bitmap is used to indicate
Rocky Storage (Non-Owner)Rocky Storage (Owner)Connector-Cloudlet (Cloud Storage)Block IDBlock DataCBSSBitmapsEpochPresenceDirty......Endpoint Epoch Bitmaps...Rocky Storage (Non-Owner)Cloud Epoch Bitmaps.........Epoch 1Epoch 2EpochBlock ID...Version MapEBSSRBSEpoch Bitmaps289ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Beom Heyn Kim and Hyoungshick Kim
Figure 6: Periodic Mutation Snapshot Update. The RC writes
the snapshot of dirty blocks to EBSS and then replicates it
to CBSS for each epoch. Dirty bitmaps for epochs are also
replicated to the cloud as epoch bitmaps.
The other epoch sends another write sequence fragment, W2:
Thus, the state mutation can be seen as:
w21, w22, . . . , w2m
W1−−→ Sn
W2−−→ Sn+m
S0
To store old write sequence fragments as an immutable mutation
event for tamper-resistance, Rocky packages each fragment of write
sequence into a mutation snapshot. A mutation snapshot is a collec-
tion of dirty blocks written during an epoch. For example, among a
write sequence W1, if w12 and w1n write to the same block whose
block ID is 1, then we reduce them to just w1n as it is the latest
write to the block ID 1 for the epoch. Then, Rocky just replicate
the latest snapshot of the block ID 1 that is the block applied with
w1n at the end of the epoch. By overwriting the block device with
blocks packaged in mutation snapshots, we can mutate the state
of the block as if a corresponding write sequence is applied to the
block device. Suppose that the mutation snapshot of the first epoch,
M1 and that of the second epoch, M2. Then, applying M1 to the
block device with the initial state S0, we can mutate the state of the
block to Sn, and so on, as if we have applied W1 and W2:
M1−−→ Sn
M2−−→ Sn+m
S0
This technique is used by a procedure called Periodic Mutation
Snapshot Update (PMSUP) which Figure 6 depicts. PMSUP works
as follow: (1) for dirty blocks referring to the dirty bitmap, RC
first needs to translate the dirty block’s ID to a key that is the
combination of the current epoch appended by the corresponding
block ID delimited by a colon; (2) RC sends those dirty blocks to
EBSS and CBSS as values to those translated keys; (3) RC writes its
dirty bitmap to the cloud storage service by writing that bitmap to
the key -bitmap where the  is the current epoch
during which those changes occurred—the dirty bitmap uploaded
to the cloud storage service for each epoch is simply called epoch
bitmap; (4) RC resets the corresponding bits in the dirty bitmap for
those dirty blocks.
Epoch is essentially a version for each block. It indicates when
the block was taken as a block snapshot and made “public” to others
by writing them to the cloud. This is why Rocky indexes each block
in EBSS or CBSS by the combination of epoch and block ID. On an
Figure 7: Periodic Prefetch. Endpoint A is the owner generat-
ing a sequence of mutation snapshots M1, M2 and M3. Other
endpoints periodically prefetch at their own pace. Blank
squares in mutation snapshots indicate that there is no write
for the corresponding block.
incoming read request, RC may need to fetch blocks from CBSS but
need to know which version of blocks it needs to request. To that
end, Rocky endpoint maintains version map consisting of a map
between block ID and the latest version (i.e., epoch) for that. When
RC periodically fetches epoch bitmaps, it updates its version map
to reflect new writes made since the last period.
3.4 Periodic Prefetch and Snapshot Merging
Non-owner’s RC periodically gets new block snapshots before be-
coming the owner (periodic prefetch) to minimize the overhead
of getting blocks from the cloud storage on-demand when it be-
comes the new owner. To begin, RC gets epoch bitmaps from cloud
storage—those epoch bitmaps should be ones newly uploaded by
the owner. Once epoch bitmaps are downloaded, RC calculates
what version of block snapshots it needs to get to not download
block snapshots overwritten by the latest one. For example, suppose
mutation updates M1 and M2 were made by the owner endpoint A
since the endpoint B’s last periodic prefetch. The state of the raw
block device on endpoint A has been changed:
M1−−→ Sn
M2−−→ Sn+m
S0
If some block snapshots in M1 are overwritten by block snapshots
in M2, it is wasteful to download those block snapshots in M1 that
are overwritten by M2’s. We devised snapshot merging technique
to solve this issue. To construct the merged mutation snapshot, RC
picks the latest snapshot of each block from mutation snapshots. In
this way, RC gets the “merged” mutation snapshot consisting of the
latest block snapshots for all dirty blocks needed to be replicated.
Figure 7 shows how snapshot merging works with an example
scenario. If endpoint B calculates the mutation snapshot, M1 + M2,
then by applying M1 + M2 endpoint B can sync with endpoint A
more efficiently than downloading them separately:
M1+M2
−−−−−−→ Sn+m
S0
Also, note how each endpoint may vary the rate of periodic prefetch
and the resulting mutation snapshot becomes different based on
that rate. Merged snapshots for endpoints B, C and D are M1 + M2,
M1 + M2 + M3 and M2 + M3, respectively.
𝔀11, 𝔀12, …, 𝔀1nEpoch 1, 𝔀21, 𝔀22, …, 𝔀2mEpoch 2Epoch 1Epoch 2.........EBSSCBSSEpoch 1...Mutation SnapshotsEpoch Bitmaps...Epoch 1Epoch 2Epoch 2Dirty Bitmap...Endpoint ABy Endpoint ABy Endpoint A𝑴1Endpoint ACloud StorageEndpoint BEndpoint C𝑴1𝑴2𝑴2𝑴3𝑴3𝑴1+𝑴2𝑴1+𝑴2+𝑴3Endpoint D𝑴1𝑴2+𝑴3𝑴New Mutation𝑴Replicated MutationCloud StorageOwnerTime FlowMutation Snapshots𝑴1𝑴2𝑴1+𝑴2𝑴3𝑴1+𝑴2+𝑴3𝑴2+𝑴3290Rocky: Replicating Block Devices for Tamper and Failure Resistant Edge-based Virtualized Desktop Infrastructure
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
The periodic prefetch procedure (PPP) is as follow: (1) RC begins
with prefetching by downloading all epoch bitmaps uploaded by
the owner since the last prefetch; (2) RC reflects them into its
presence bitmap by resetting bits in the presence bitmap if the
corresponding bit is set in at least one of the epoch bitmaps; (3) RC
starts getting blocks from the cloud storage; (4) newly retrieved
blocks are written to RBS and EBSS; (5) RC updates the version map
accordingly by referring to epoch bitmaps and sets the presence
bitmap accordingly.
3.5 Ownership Transfer
For coherence, we allow only one owner at any given time. To con-
duct the ownership-transfer protocol, Rocky endpoints use cloud
storage for the coordination. The RC allocates the key owner to
indicate both the existence of the owner. If the key does not exist,
no owner is currently active. Therefore, the RC of the non-owner
can become a new owner simply by creating the key. However,
if the key exists, there exists an active owner currently. Then, to
demand the ownership transfer, the RC sends a signalling message
to the current owner via a cloud messaging service. After that, RC
waits until the key owner gets deleted by the current owner. If RC
waits too long, it times out and sends a notification message to its
user via screen, email or SMS.
Upon the arrival of the ownership-transfer request, the current
owner performs: (1) stops accepting any more block I/O during the
ownership transfer, (2) flushes all writes to raw block storage, (3)
performs the last periodic mutation snapshot update (4) deletes the
key owner on the cloud, and (5) sends a signalling message to the
endpoint requesting the ownership to notify the completion of the
ownership relinquishment. Subsequently, RC requesting the own-
ership transfer receives the signalling message and finally become
a new owner by creating the key owner with its own endpoint ID.
If the owner waits for the key owner to change but time out, then
it sends a notification message to its user. Lastly, the new owner
downloads all epoch bitmaps that the owner did not see, creates a
new dirty bitmap, and by referring to epoch bitmaps, resets bits in
the presence of bitmaps and updates the version map.
3.6 Scheduled Checkpoint
Rocky can perform a old block snapshots removal via scheduled
checkpoints. During the scheduled checkpointing, the owner should
stop user’s VM, read and write every block, and flush writes for
the epoch called checkpoint epoch, ec. Then, endpoints finish with
prefetching. Subsequently, endpoints confirm that an anti-malware
and failure detector that there has been no tampering attack and
no cloudlet failure. After that, endpoints and the cloud storage
can clean up their old epoch bitmaps and old block snapshots in
EBSS/CBSS added prior to ec except for those for last checkpoint
epochs. Each checkpoint epoch’s bitmap will be marked as a sched-
uled checkpoint so that it does not get garbage-collected during the
future scheduled checkpointing. The period of scheduled check-
points can be determined based on user’s policy on storage space
utilization.
4 RECOVERY PROCEDURES
Tampering attacks and cloudlet failures both result in degrading
data availability. Once data gets corrupted by malware’s tampering
attacks, users cannot access their data. If cloudlet failures occurred,
user data on failed cloudlets cannot be accessed. If the failure is
permanent like hard disks destroyed by natural disasters, then data
may not be recovered forever. Under our threat and failure models,
there are three possible scenarios of suffering from tampering at-
tacks and cloudlet failures: (1) tampering attacks with no failure, (2)
failures but no tampering attack and (3) tampering attacks and fail-
ures. Rocky’s recovery procedures can deal with those situations.
We rely on the assumption that an conventional anti-malware
and a failure detector are installed in-place, properly configured
and started running already. There exists anti-malware that can
report tampering attacks after seeing some data gets tampered
by malware [2]. We suppose such an anti-malware can correctly
pinpoint and notify the specific epoch when tampering attacks
began. Additionally, we expect to see an additional component that
periodically pinging Rocky endpoints with a heartbeat message to
detect failures. Thus, we suppose there will be notification from
either the anti-malware or a failure detector regarding tampering
attacks or cloudlet failures. They broadcast to Rocky endpoints.
Once Rocky endpoints receive notification, they immediately stops
block I/O processing, periodic updating and prefetching. Then,
Rocky endpoints start running the recovery procedure accordingly.
4.1 Tampering Attack Recovery
Rocky runs tampering attack recovery procedure when an anti-
malware sends the notification to Rocky endpoints. When the noti-
fication from a anti-malware is received by a Rocky endpoint, the
endpoint retrieves the epoch ea specified by the anti-malware as
the epoch when tampering attacks started. Rocky endpoint updates
their data structures, that are epoch variable for the last epoch ep it
finished with prefetch for, its version map and its dirty and presence
bitmaps. More specifically, each endpoint starts with comparing
ep with ea. If ea <= ep, then the endpoint will set its ep := ea − 1.
Also, they download epoch bitmaps from the cloud storage, and also
update their version map using epoch bitmaps updated up until,
ea − 1. In addition, for the case ea <= ep, the endpoint sets its pres-
ence bitmap to one for all indices. Additionally, the owner endpoint
should flush existing dirty block snapshots and reset bits in its dirty
bitmap for those batched dirty block snapshots. For a connector-
cloudlet, Rocky discards epoch bitmaps that are for epochs greater
than ea − 1. Then, Rocky can continue operating without data be-
ing tampered as long as the anti-malware was accurate about the
beginning time of tampering attacks.
4.2 Tamper-Resistant Failure Recovery
Rocky triggers failure recovery procedure when a failure detector
notifies Rocky endpoints about the failures of cloudlets. Rocky addi-
tionally consider tampering attack recovery as well. If a tampering
attack was mounted, an anti-malware should notify with the epoch,
ea, when the attack began. If a cloud does not fail, but endpoints
are failed, necessary data is all contained on the cloud storage in
its epoch bitmaps and CBSS. The procedure is trivial as follows: (1)
replace failed component with new ones, (2) remove every bitmap
291ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Beom Heyn Kim and Hyoungshick Kim
Figure 8: Failure Recovery. Endpoint A generates mutation snapshots M1 and M2 although it may temporarily fail during the
upload. A user travels to endpoint B to generate M3, travels to endpoint C to generate M4, and travels back to endpoint B to
generate M5. Meanwhile, endpoint C fails permanently after downloading M1 and generating M4 due to a hardware problem
and then cloud storage A fails due to a devastating failure resulting in data loss. The user switches to cloud storage B and
triggers Rocky failure recovery procedure. Endpoint A sends cloud storage B its snapshot M1 + M2, which is the most up-to-
date among endpoints A and B. Then, endpoint B sends M3 but discard M5 because M4 is completely lost with endpoint C
and cloud storage A’s failures (note that M4 may have been the beginning of a tampering attack). Endpoint C catches up by
replicating the merged snapshot M1 + M2 + M3 after the failure recovery.
added after ea − 1 from the cloud epoch bitmaps. (3) let new end-
points reconstruct their version map, endpoint epoch bitmaps, RBS,
and EBSS using epoch bitmaps and CBSS on the cloud storage by
having new endpoints perform prefetch from the beginning.
When the cloud storage fails, Rocky recovers using data stored on
endpoints after removing tampering effects if attacks were mounted.
Here, some of endpoints may be failed but Rocky’s recovery proce-
dure below can recover the block device state with minimal data
loss with those endpoints still not failed. One of endpoints becomes
a coordinator which can be specified by the user and known to
every endpoints in advance. The user may specify the priority for
each endpoint to become a coordinator in case some endpoints fail
and cannot become the coordinator. Then, the coordinator learns
from the failure detector about endpoints not failed which forms a
recovery committee.
Then, the coordinator starts running the initialization procedure
(IP): (1) every endpoint receives ea from an anti-malware; (2) every
endpoint uploads the epoch of their last endpoint epoch bitmap, e1,
that is e1 < ea; (2) every endpoint uploads the epoch for which they