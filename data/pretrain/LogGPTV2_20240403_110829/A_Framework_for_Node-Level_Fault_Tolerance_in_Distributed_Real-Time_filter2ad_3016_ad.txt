NLFT  nodes.  State  0  represents  the  fault-free  state 
where  all  four  wheel  nodes  are  working  correctly  or 
transient  faults  occur  that  are  masked  by  TEM.  The 
transition from state 0 to state F occurs in the case that 
a wheel node is affected by a permanent fault or, that a 
wheel  node is affected by a transient  fault that cannot 
be masked by TEM. 
4λtCDPT 
0 
4(λP+λT(1-CDPT)) 
F 
Figure 10. State transition diagram for the 
wheel node subsystem with full functionality 
mode and NLFT nodes 
Figure 11 shows the state transition diagram for the 
wheel  node  subsystem  when  NLFT  nodes  and 
degraded  functionality 
is  considered.  The  model 
consists of five states: 
State  Description 
0 
1 
2 
3 
All four computer nodes are working correctly 
One  of  the  computer  nodes  is  affected  by  a 
permanent  fault  and  is  permanently  down.  The 
other nodes continue to provide their service 
One  of  the  computer  nodes  is  affected  by  a 
transient  fault  and  is  temporary  down.  The  other 
nodes continue to provide their service 
One  of  the  computer  nodes  is  affected  by  a 
transient  fault  and  produces  an  omission  failure. 
The other nodes continue to provide their service 
F 
Failure.  Two  computer  nodes  are  shut  down. 
Either  due  to  a  failure  of  two  nodes,  or  an 
undetected error in one node 
4λPCD 
3λTCDPT 
1 
3(λP+λT(1-CDPT)) 
4λTCDPT 
0 
4λTCDPOM 
4λTCDPFS 
µOM 
 µR 
4(1-CD)(λP+λT) 
3λTCDPT 
2 
3λTCDPT 
3 
3(λP+λT(1-CDPT)) 
F 
3(λP+λT(1-CDPT)) 
Figure 11. State transition diagram for the 
wheel node subsystem with degraded 
functionality mode and NLFT nodes 
3.3 Parameter assignment 
Before  the  results  can  be  derived  from  the  models 
presented,  the  parameter  values  must  be  assigned.  In 
general,  fault  rates  and  repair  rates  are  not  easy  to 
obtain  as  they  depend  on  many  factors,  e.g.  the 
underlying hardware, the software implementation and 
the  operating  environment.  Nevertheless,  as 
the 
objective is to compare the different approaches rather 
than deriving actual reliability measures, the following 
values may be acceptable. 
The  rate  of  permanent  faults,  λP  =  1.82·10-5  faults 
per hour, is obtained from [15], where the fault rate of 
a computer node in a distributed brake-by-wire system 
for heavy duty trucks is derived using MIL-HDBK-217 
standard.  The  computer  node  consists  of  a  32-bit 
processor  with  memory,  communication  interface, 
power IC, bus driver and bus connections.  
The rate of transient faults, λT, is assumed to be ten 
times  higher  than  the  rate  of  permanent  faults,  i.e.  
λT  =  1.82·10-4  faults  per  hour.  Recent  studies  indicate 
that the proportion of transient faults will become even 
higher  in  future  microcontrollers  and  memories  [5]. 
Transient  faults  are  handled  differently  depending  on 
whether  faults  affect  the  application  tasks  or  the  real-
time  kernel.  It  is  stated  in  [10]  that  about  5%  of  the 
CPU  time  is  used  by  a  real-time  kernel;  thus  we 
assume  that  PFS  =  0.05.  Furthermore,  the  results  from 
fault injection experiments with our light-weight NLFT 
kernel  [7]  suggest  that  we  may  assume  90%  of  the 
faults  to  be  tolerated  (PT  =  0.9),  and  that  5%  of  the 
transient faults result in omission failures (POM = 0.05). 
An  error  detection  coverage  of  99%  (CD  =  0.99)  is 
assumed in Section 3.4 and varied in Section 3.4.1. The 
repair  rate,  µR,  includes  time  for  restarting  the  node, 
checking whether there was a permanent fault and then 
reintegrating  the  node.  In  [16],  a  distributed  system 
based on the time-triggered TTP/C protocol executing 
a  brake-by-wire  algorithm  was  evaluated  employing 
heavy-ion fault injection. The system was composed of 
five nodes, where one cycle consisted of 2048 TDMA 
rounds  and  one  TDMA  round  took  approximately  20 
ms.  The  estimated  time  necessary  to  restart  the 
operating  system  and  reintegrate  one  computer  node 
was  1.6  seconds.  Assuming  that  hardware  reset  of  a 
computer  node  conducting  a  diagnostic  test  to  ensure 
that no permanent faults exist would be in the order of 
1.4  seconds,  a  total  repair  time  of  3  seconds  is  used  
(µR  =  1.2·103  repairs  per  hour).  The  repair  time  for 
omission 
take  at  most  
1.6 seconds (µOM = 2.25·103 repairs per hour). 
is  assumed 
failures 
to 
3.4 Results 
The  results  of  the  reliability  analysis  for  the 
complete  BBW  system  over  one  year  are  shown  in 
Figure 12.  
Figure 12. Reliability of the BBW system 
As 
the 
for 
expected, 
reliability 
degraded 
functionality mode is higher than for full functionality 
mode.  With  regard  to  degraded  functionality  with 
NLFT  nodes,  the  reliability  increases  by  55%  (from 
0.45 to 0.70) after one year as compared to FS nodes. 
The  reliability  may  also  be  expressed  as  the  system's 
mean  time  to  failure  (MTTF),  i.e.  the  expected 
operation 
time  before  first  failure.  As  concerns 
degraded  functionality  mode,  the  MTTF  increases  by 
almost  60%  (1.2  year  to  1.9  year)  when  NLFT  nodes 
are used. 
Figure  13  shows  the  reliability  of  the  various 
subsystems  with  respect  to  both  full  and  degraded 
functionality.  The  main  reliability  bottleneck  is  the 
wheel node subsystem.  
Figure 13. Reliability of the subsystems 
3.4.1. Effect of varied error detection coverage and 
transient  fault  rate.  The  highest  reliability  for  the 
BBW  system  is  obtained  when  degraded  functionality 
is  considered.  In 
the 
reliability  for  this  mode  after  five  hours,  for  different 
values  of  the  error  detection  coverage  and  the  fault 
rate.  The  results  are  given  in  Figure  14,  where  the 
reliability  of  the  system  for  increasing  transient  fault 
rates is shown. 
this  subsection,  we  show 
The results show that the coverage has a significant 
influence  on  the  reliability.  The  fault  rate  has  a 
negligible  impact  as  long  as  the  fault  rate  is  much 
smaller  than  the  repair  rate.  However,  as  seen  in  the 
figure,  the  reliability  improvements  of  using  NLFT 
increase for higher fault rates. 
Figure 14. Reliability after five hours for 
varying error detection coverage and  
transient fault rate 
4. Conclusions and future work 
This  paper  proposes  the  use  of  node-level  transient 
fault tolerance (NLFT) for improving the dependability 
of  distributed  systems.  Especially,  we  present  an 
approach  called  light-weight  NLFT  that  aims  at 
masking  the  majority  of  transient  faults  locally  in  the 
node. For permanent and transient faults that cannot be 
masked,  the  node  must  exhibit  omission  or  fail-silent 
failures, which simplifies error handling at the system-
level. 
The  paper  suggests  a  number  of  error  handling 
mechanisms  based  on  experiences  from  previous 
studies,  where  a  real-time  kernel  and  parts  of  the 
mechanisms  proposed  have  been  implemented  and 
evaluated  using  fault  injection  [7,  8].  In  addition, 
reliability calculations are made on an example brake-
by-wire  application  to  demonstrate  the  advantages  of 
the approach. The results shows that the reliability may 
increase  by  55%  after  one  year,  and  the  MTTF 
increases  almost  60%  when  light-weight  NLFT  nodes 
are used, compared to using nodes that are fail-silent. 
includes 
implementation 
Further  work 
and 
evaluation for the full set of error handling proposed in 
this paper to verify that the approach is viable, and to 
estimate  the  total  coverage  and  overhead  figures. 
Additional  work  also  includes  investigation  of  how  to 
ensure  replica  determinism  in  replicated  nodes  and 
how to maintain consistency in replicated nodes in case 
of  omission  failures.  For  example,  the  study  of 
protocols  such  as  FlexRay  [9]  that  may  facilitate  fast 
recovery  of  state  data  with 
low  communication 
overhead  through  special  requests  to  the  partner  node 
in  the  event-triggered  part  of  the  protocol,  while  also 
guaranteeing the delivery of critical data transmitted in 
the pre-allocated time slots in the time-triggered part of 
the protocol. 
5. Acknowledgements 
This  work  was  partially  supported  by  ARTES  and 
the  Swedish  Foundation  for  Strategic  Research  (SSF), 
and  the  Saab  Endowed  Professorship  in  Robust  Real-
time Systems. We would like to thank Jonny Vinter at 
Chalmers University and Dr. Örjan Askerdal at Volvo 
Car Corporation for their many valuable suggestions to 
this work. 
6. References 
[1]  H.  Kopetz  and  G.  Bauer,  "The  Time-Triggered 
Architecture",  Proceedings  of  the  IEEE, vol. 91, 2003, 
pp. 112-26. 
[2]  D.  Powell,  J.  Arlat,  L.  Beus-Dukic,  A.  Bondavalli,  P. 
Coppola,  A.  Fantechi,  E.  Jenn,  C.  Rabejac,  and  A. 
"GUARDS:  A  Generic  Upgradable 
Wellings, 
Architecture for Real-time Dependable Systems", IEEE 
Transactions on Parallel and Distributed Systems, vol. 
10, 1999, pp. 580-599. 
[3]  D. Powell,  "Distributed  Fault  Tolerance:  Lessons  from 
Delta-4", IEEE Micro, vol. 14, 1994, pp. 36-47. 
[4]  Kopetz,  H.,  Real-Time  Systems:  Design  Principles  for 
Distributed  Embedded  Applications,  Boston:  Kluwer 
Academic, 1997. 
[5]  R.  C.  Baumann  ”Soft  Errors  in Commercial  Integrated 
Circuits”, 
International  Journal  of  High  Speed 
Electronics and Systems, Vol. 14, No. 2, 2004, pp. 299-
309 
[6]  Burns,  A.,  and  Wellings,  A.,  Real-Time  Systems  and 
Programming Languages: Ada 95, Real-Time Java and 
Real-Time  Posix,  third  ed.  Harlow:  Addison-Wesley, 
2001. 
[7] 
[8] 
J.  Aidemark,  J.  Vinter,  P.  Folkesson,  and  J.  Karlsson, 
"Experimental Evaluation of Time-redundant Execution 
for 
of 
International  Conference  on  Dependable  Systems  and 
Networks, Washington, DC, USA, 2002, pp. 210-216. 
a  Brake-by-wire  Application",  Proc. 
J.  Karlsson, 
J.  Aidemark,  P.  Folkesson,  and 
"Experimental Dependability Evaluation of the Artk68-
FT  Real-time  Kernel",  International  Conference  on 
Real-Time  and  Embedded  Computing  Systems  and 
Applications, Göteborg, Sweden, 2004. 
 [9]  FlexRay  Communications  System  Specifications 
Version 2.0, www.flexray.com June 2004. 
[10]  Labrosse,  J.  J.,  MicroC/OS-II  :  The  Real-Time  Kernel, 
second edition, Lawrence: R&D, 1999. 
[11]  G.  Heiner 
and  T.  Thurner, 
"Time-Triggered 
Architecture  for  Safety-related  Distributed  Real-time 
Systems in Transportation Systems", Proceedings of the 
28th  International  Symposium  on  Fault  Tolerant 
Computing, Munich, Germany, 1998, pp. 402-407. 
[12]  Poledna,  S.,  Fault-tolerant  Real-time  Systems:  The 
Problem  Of  Replica  Determinism.  Boston,  Mass, 
Kluwer Academic Publishers, 1996. 
[13]  R.  A.  Sahner  and  K.  S.  Trivedi,  "Reliability  Modeling 
using SHARPE", IEEE Transactions on Reliability, vol. 
R-36, 1987, pp. 186-93. 
[14]  D. Chen, S. Dharmaraja, D. Chen, L. Li, K.S. Trivedi, 
R.R.  Some,  A.P.  Nikora,  “Reliability  and  Availability 
JPL  Remote 
Analysis 
Exploration 
Experimentation  System”,  Proc.  of 
International 
Conference  on  Dependable  Systems  and  Networks, 
Washington, DC, USA, 2002, pp. 337-342. 
the 
of 
[15]  Claesson, V.,  Efficient and  Reliable  Communication in 
Distributed  Embedded  Systems,  Ph.d  thesis,  Chalmers 
University of Technology, Göteborg, Sweden, 2002. 
[16]  Sivencrona, H., On the Design and Validation of Fault 
Containment  Regions  in  Distributed  Communication 
Systems,  Ph.d 
thesis,  Chalmers  University  of 
Technology, Göteborg, Sweden, 2004.