Feature
NAME ( STR )
VAR NAME ASSIGN NAME
) ; NAME ASSIGN
THIS . NAME (
ASSIGN NAME . NAME
False negative top 5
Rank Weight
0.01593
0.00490
0.00394
0.00372
0.00177
1
98
141
154
355
Feature
NAME . NAME
NAME . NAME (
.
( THIS . NAME
THIS . NAME .
NAME ( THIS .
Table 6: Features of TNs and FNs
One can see that JavaScript extraction is the most time-consuming
part of PJScan. It has been observed that this operation takes only
2,041 seconds using a single process, with an average process-
ing time of 31 milliseconds per ﬁle. The overall CPU usage was
very low (up to 40%, with I/O waiting of up to 30%), while at the
same time disk utilization remained above 95% during the extrac-
tion phase. One can conclude that disk throughput represents the
main performance bottleneck for this application. Using a faster
storage device or reading ﬁles through a fast network is likely to
improve the performance of PJScan.
The throughput calculation for diﬀerent stages of PJScan is pre-
sented in Table 8.
It shows that the throughput varies strongly
between the “detected” and “undetected” ﬁles since they vary in
ﬁle size and the number and size of JavaScript entities. The aver-
age throughput of 303.5Mbps is suitable for batch processing tasks
even for organizations that have a very high volume of PDF traf-
ﬁc. The average processing time per ﬁle is 23 milliseconds. To the
best of our knowledge, no other software package achieves lower
processing times.
Time
Total
Average
Std. dev.
Percentage
Extractor Tokenizer Learner
Classiﬁer
10.19
0.014s
N/A 0.000015s
N/A 0.000009s
0.0009%
180s
0.0032s
0.0392s
11.63%
1,356s
0.0205s
0.0015s
87.65%
0.66%
Table 7: Processing time for diﬀerent stages of PJScan in batch
execution mode
(cid:2)(cid:3)(cid:1)
Total time
Average ﬁle size
Files per second
Data throughput
Seconds per ﬁle
Detected Undetected
1208s
1.39MB
33.3
370.5Mbps
0.030s
339s
0.106MB
75.8
64.3Mbps
0.013s
All ﬁles
1547s
0.89MB
42.6
303.5Mbps
0.023s
Table 8: Throughput characteristics of PJScan
6. RELATED WORK
As it was already mentioned in the introduction, detection of
malware in PDF documents has not been extensively studied in
the research literature before. The early approaches to identiﬁca-
tion of malware in PDF documents [13, 21] were based on n-gram
analysis of raw document content. The scope of experimental eval-
uation in this work was rather limited. It included self-generated
malicious PDF documents as well as a relatively small number of
examples (less than 300) from the outdated VXHeavens malware
repository. Due to the wide-spread use of evasion techniques in
modern PDF malware, especially object compression and code-
level obfuscation, we believe that the analysis of raw content of
PDF documents is no longer adequate. Hence the approach taken in
PJScan is fundamentally diﬀerent from the above mentioned work
in that our methods spend a lot of eﬀort in discovering and utilizing
the appropriate lexical features of PDF.
The recent work on analysis of PDF documents has emerged
from existing tools for static and dynamic analysis. Besides the
malware analysis portal Wepawet considered in our experiments,
some other tools use a combination of static and dynamic analysis
tools. MalOffice [10] uses static and dynamic techniques as well
as some heuristics. Its static analysis is based on the utility pdftk20,
and the dynamic analysis builds on CWSandbox [26]. Detections
are made by combining scores from various heuristics and policies
attached to the analysis tools. Another combination of static and
dynamic analysis was used in MDscan recently proposed in [24].
From the architectural point of view, MDscan is similar to our ap-
proach. It also uses static analysis to extract JavaScript content (us-
ing a self-made parser) and a heuristic approach for the extraction
of JavaScript code. The extracted code is interpreted using Spider-
Monkey. Detection is carried out at the dynamic stage by using the
shellcode detection tool Nemu [16]. The method was evaluated on
a set of 197 malicious PDF documents artiﬁcially generated using
the Metasploit framework and 2000 benign documents. Compared
to MDscan, we only use SpiderMonkey for token extraction and
perform detection statically, which brings a performance improve-
ment of two orders of magnitude.
A signiﬁcant body of prior work has addressed the detection of
malicious JavaScript in web content, especially in the context of
drive-by-downloads. One cannot directly compare the accuracy of
such methods with PJScan due to the fact that the data corpora
used for the experimental evaluation of respective methods are very
diﬀerent. We will hence focus on methodical comparison of our
approach with such methods.
Similar to PDF malware, the methods for detection of malicious
JavaScript in web content can be classiﬁed into static, dynamic
and hybrid. Purely dynamic methods deploy various techniques for
monitoring the run-time execution of processes accessing web con-
tent, e.g.: full-ﬂedged host virtualization [25], client virtualization
[14], instrumentation of a JavaScript engine [11] or heap monitor-
ing [18]. Dynamic methods have high detection accuracy and are
hardly prone to false positives. Due to their performance overhead
they are usually limited to “post-mortem” analysis.
Hybrid methods aim to minimize run-time overhead while re-
taining high detection accuracy. Several such methods have me-
thodical aﬃnity with PJScan. JSand [7] uses instrumented ver-
sions of the HtmlUnit21, a Java-based browser simulator, and the
Mozilla’s Rhino22 interpreter to extract heuristic features while mon-
itoring the execution of JavaScript code. These features are used to
train an anomaly detection system by running JSand on benign web
pages. Cujo [19] is another interesting combination of static and
dynamic methods. Its static part is similar to PJScan (with the ex-
ception of anomaly detection instead of two-class classiﬁcation in
its learning component); its dynamic component extracts symbolic
features from a light-weight sandbox ADSandbox [9] and deploys
similar n-gram analysis and learning techniques as the static part.
A “mostly static” detection system Zozzle [8] attempts to avoid dy-
namic analysis but still needs it to unravel source-code obfuscation
before using statistical feature extraction and supervised learning
for the classiﬁcation part. Compared to these hybrid methods, PJS-
can uses “reverse” anomaly detection—since only malicious data
is widely available for PDF documents—and completely dispenses
with run-time analysis. Another hybrid method has been proposed
by Provos et al. [17]; however, the lack of a technical presentation
in this reference prevents us from a detailed comparison.
The only method that can be classiﬁed as fully static is Prophiler
[5] which deploys techniques similar to JSand except that its fea-
tures are extracted from a JavaScript engine at the parsing stage
without running the code. (A similar idea is used in our method
20http://www.pdflabs.com/tools/pdftk-the-pdf-toolkit/.
21http://htmlunit.sourceforge.net/
22http://www.mozilla.org/rhino/
but one step earlier, by stopping SpiderMonkey after the lexical
analysis.) However, Prophiler has a high false positive rate and is
intended to be used as a ﬁlter for a subsequent dynamic analysis.
7. DISCUSSION AND LIMITATIONS
The reported experimental results conﬁrm the practical feasibil-
ity of the static, learning-based approach for detection of malicious
JavaScript-bearing PDF documents. The preprocessing component
of PJScan can be very helpful for a security administrator to man-
ually extract and analyze JavaScript code in PDF documents. The
main beneﬁt of the learning component of PJScan is the ability to
extract knowledge from large-scale malware corpora. PJScan en-
ables one to derive light-weight models from heuristic knowledge
of several dozen antivirus engines and tens of gigabytes of collected
data. Such models can be deployed with no manual interaction and
negligible performance overhead (<50ms per ﬁle). The operational
false-positive rate of less than 0.4% is admissible in practice; even
for a highly visible site like VirusTotal with a strong bias for sus-
picious data, this corresponds to an average rate of 1.7 false alarms
per day (148 out of ca. 40,000 benign documents over 90 days).
The high “laboratory” false-positive rate of PJScan (i.e. the rate
measured only for those benign ﬁles that contain JavaScript) indi-
cates that our current learning setup may indeed have diﬃculty with
accurate discrimination between malicious and benign JavaScript
content. This observation is also indirectly supported by our anal-
ysis of the learned features. Learning from two classes, as it has
been done in the related work on web-based JavaScript content,
e.g. [19, 5, 8], may be the right way to avoid this limitation. How-
ever, benign JavaScript-bearing PDF data is currently not available
in suﬃcient quantity to evaluate this scenario for PDF documents.
Another limitation of the current version of PJScan is its sus-
ceptibility to certain kinds of obfuscation. An exemplary obfusca-
tion technique that is diﬃcult for our method is the use of short
JavaScript entry-point code which fetches further code from docu-
ment locations where JavaScript code cannot be expected (cf. Sec-
tion 5.3). There are two potential ways to address this limitation.
One can use the “mostly static” technique proposed in Zozzle [8] in
which compilation requests to a JavaScript engine are intercepted
to obtain all code sent for execution. While this technique oﬀers
a guaranteed access to unobfuscated code, it may be hampered by
just-in-time compilation used in JavaScript engines and eventually
produce highly fragmented code. It should be also noted that this
idea would be diﬃcult to implement for PDF-based JavaScript code
since Adobe provides an extensive PDF-speciﬁc API. Another way
of dealing with obfuscation is to collect syntactic information from
a parser and use compiler optimization to factor out obfuscations.
An attacker may also attempt to use the fact that the models used
for detection are derived from data. A taxonomy of attacks against
learning algorithms has been recently proposed by Barreno et al.
[4]. Following this taxonomy, we remark that causative attacks, i.e.
attacks against the training data, do not constitute a serious threat
to our approach. We use data from an established malware reposi-
tory and assume that integrity of this repository cannot be compro-
mised. Even if an attacker submits own data to this repository, he
will know how this data is classiﬁed by antivirus engines but can-
not inﬂuence this classiﬁcation. More realistic are attacks from the
exploratory category, i.e. attacks staged at the detection stage. One
potential attack strategy is to insert some useless code to make a
new JavaScript entity look “anomalous”. This attack may indeed
be quite potent if an attacker knows the true proﬁle of “normal”
malicious data. Since he neither has access to nor can manipulate
the training data, we believe that in practice guessing what kind of
useless code should be added can be a diﬃcult task.
(cid:2)(cid:3)(cid:1)
8. CONCLUSIONS
We have proposed a new static approach to detection of mali-
cious JavaScript-bearing PDF documents. The main advantages
of our approach are its high performance and no need for special
instrumentation, such as virtual machines or sandboxing. It can at-
tain about 85% of the detection accuracy of all antivirus engines at
VirusTotal with the performance overhead of less than 50ms per
ﬁle. It is only marginally aﬀected by text-level obfuscation since
the resulting JavaScript code remains very conspicuous at the lex-
ical level. Due to these advantages our method can be used as a
standalone application on end-user systems or even be integrated
as a ﬁltering tool in email gateways and HTTP proxies.
The computational eﬃciency of our system PJScan has enabled
us to evaluate it on an unprecedentedly large real-life data corpus
(over 65,000 PDF documents) collected from VirusTotal. This
evaluation has conﬁrmed a high detection accuracy of our method
for both known and unknown malware. PJScan is more prone to
false positives than state-of-the-art dynamic approaches; however,
its operational false positive rate still lies in the promille range,
which is feasible for practical deployment.
Our future work will address a potential interaction of static and
dynamic analysis techniques in order to unravel code-level obfusca-
tion typical for JavaScript attacks. We anticipate that some degree
of dynamic analysis can be carried out prior to actual code execu-
tion without a signiﬁcant performance overhead. We also intend to
investigate more extensive static analysis techniques, such as syn-
tactic analysis and compiler optimization, to obtain features that
better reﬂect the true semantics of the JavaScript code. Finally, an
important open issue remains the detection of malicious PDF doc-
uments whose exploitation techniques do not rely on JavaScript.
Acknowledgements
The authors wish to acknowledge ﬁnantial support by the Heiseberg
Fellowship of the Deutsche Forschungsgemeinschaft (DFG) and by
the German Federal Oﬃce for Information Security.
9. REFERENCES
[1] Internet security threat report. Symantec, 2010.
[2] APSB11-03. http://www.adobe.com/support/security/-
bulletins/apsb11-03.html.
[3] APSB11-08. http://www.adobe.com/support/security/-
bulletins/apsb11-08.html.
[4] M. Barreno, B. Nelson, A. Joseph, and J. Tygar. The security
of machine learning. Machine Learning, 81(2):121–148,
2010.
[5] D. Canali, M. Cova, G. Vigna, and C. Kruegel. Prophiler: a
fast ﬁlter for the large-scale detection of malicious web
pages. In International Conference on World Wide Web
(WWW), pages 197–206, 2011.
[6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support
vector machines, 2001. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[7] M. Cova, C. Kruegel, and G. Vigna. Detection and analysis
of drive-by-download attacks and malicious JavaScript code.
In International Conference on World Wide Web (WWW),
pages 281–290, 2010.
[8] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert. ZOZZLE:
Fast and precise in-browser javascript malware detection. In
USENIX Security Symposium, 2011. to appear.
[9] A. Dewald, T. Holz, and F. Freiling. ADSandbox:
sandboxing JavaScript to ﬁght malicious websites. In
(cid:2)(cid:3)(cid:1)
Symposium on Applied Computing (SAC), pages 1859–1864,
2010.
[10] M. Engelberth, C. Willems, and H. T. MalOﬃce – analysis of
various application data ﬁles. In Virus Bulletin International
Conference, 2009.
[11] B. Feinstein and D. Peck. Caﬀeine Monkey: Automated
collection, detection and analysis of malicious JavaScript. In
Black Hat USA, 2007.
[12] K. Itabashi. Portable document format malware. Symantec
white paper, 2011.
[13] W.-J. Li, S. Stolfo, A. Stavrou, E. Androulaki, and
A. Keromytis. A study of malcode-bearing documents. In
Detection of Intrusions and Malware & Vulnerability
Assessment (DIMVA), pages 231–250, 2007.
[14] J. Nazario. PhoneyC: a virtual client honeypot. In USENIX
Workshop on Large-scale Exploits and Emergent Threats
(LEET), 2009.
[15] PDF Reference.
http://www.adobe.com/devnet/pdf/pdf_reference.html, 2008.
[16] M. Polychronakis, K. Anagnostakis, and E. Markatos.
Comprehensive shellcode detection using runtime heuristics.
In Annual Computer Security Applications Conference
(ACSAC), pages 287–296, 2010.
[17] N. Provos, P. Mavrommatis, M. Abu Rajab, and F. Monrose.
All your iFRAMEs point to us. In USENIX Security
Symposium, pages 1–16, 2008.
[18] P. Ratanaworabhan, B. Livshits, and B. Zorn. NOZZLE: A
defense against heap-spraying code injection attacks. In
USENIX Security Symposium, pages 169–186, 2009.
[19] K. Rieck, T. Krüger, and A. Dewald. Cujo: Eﬃcient
detection and prevention of drive-by-download attacks. In
Annual Computer Security Applications Conference
(ACSAC), pages 31–39, 2010.
[20] K. Rieck and P. Laskov. Linear-time computation of
similarity measures for sequential data. Journal of Machine
Learning Research, 9:23–48, 2008.
[21] Z. Shaﬁq, S. Khayam, and M. Farooq. Embedded malware
detection using markov n-grams. In Detection of Intrusions
and Malware & Vulnerability Assessment (DIMVA), pages
88–107, 2008.
[22] R. Sommer and V. Paxson. Outside the closed world: On
using machine learning for network intrusion detection. In
IEEE Symposium on Security and Privacy, pages 305–316,
2010.
[23] D. Tax and R. Duin. Support vector data description.
Machine Learning, 54:45–66, 2004.
[24] Z. Tzermias, G. Sykiotakis, M. Polychronakis, and
E. Markatos. Combining static and dynamic analysis for the
detection of malicious documents. In European Workshop on
System Security (EuroSec), 2011.
[25] Y.-M. Wang, D. Beck, X. Jiang, R. Roussev, C. Verbowski,
S. Chen, and S. King. Automated web patrol with strider
honeymonkeys: Finding web sites that exploit browser
vulnerabilities. In Network and Distributed System Security
Symposium (NDSS), 2006.
[26] C. Willems, T. Holz, and F. Freiling. CWSandbox: Towards
automated dynamic binary analysis. IEEE Security and
Privacy, 5(2):32–39, 2007.
[27] J. Wolf. OMG WTF PDF. Chaos Communication Congress
(CCC), Dec. 2010.