title:On Challenges in Evaluating Malware Clustering
author:Peng Li and
Limin Liu and
Debin Gao and
Michael K. Reiter
Singapore Management University 
Singapore Management University 
Institutional Knowledge at Singapore Management University 
Institutional Knowledge at Singapore Management University 
Research Collection School Of Information 
Systems 
School of Information Systems 
9-2010 
On Challenges in Evaluating Malware Clustering 
On Challenges in Evaluating Malware Clustering 
Peng LI 
University of North Carolina, Chapel Hill 
Limin LIU 
Chinese Academy of Sciences 
Debin GAO 
Singapore Management University, PI:EMAIL 
Michael K Reiter 
University of North Carolina, Chapel Hill 
Follow this and additional works at: https://ink.library.smu.edu.sg/sis_research 
 Part of the Information Security Commons 
Citation 
Citation 
LI, Peng; LIU, Limin; GAO, Debin; and Reiter, Michael K. On Challenges in Evaluating Malware Clustering. 
(2010). Recent Advances in Intrusion Detection: 13th International Symposium, RAID 2010, Ottawa, 
Ontario, Canada, September 15-17, 2010: Proceedings. 6307, 238-255. Research Collection School Of 
Information Systems. 
Available at:
Available at: https://ink.library.smu.edu.sg/sis_research/1319 
This Conference Proceeding Article is brought to you for free and open access by the School of Information 
Systems at Institutional Knowledge at Singapore Management University. It has been accepted for inclusion in 
Research Collection School Of Information Systems by an authorized administrator of Institutional Knowledge at 
Singapore Management University. For more information, please email cherylds@smu.edu.sg. 
On Challenges in Evaluating Malware Clustering
Peng Li1, Limin Liu2, Debin Gao3, and Michael K. Reiter1
1 Department of Computer Science, University of North Carolina, Chapel Hill, NC, USA
2 State Key Lab of Information Security, Graduate School of Chinese Academy of Sciences
3 School of Information Systems, Singapore Management University, Singapore
Abstract. Malware clustering and classiﬁcation are important tools that enable
analysts to prioritize their malware analysis efforts. The recent emergence of fully
automated methods for malware clustering and classiﬁcation that report high ac-
curacy suggests that this problem may largely be solved. In this paper, we report
the results of our attempt to conﬁrm our conjecture that the method of selecting
ground-truth data in prior evaluations biases their results toward high accuracy.
To examine this conjecture, we apply clustering algorithms from a different do-
main (plagiarism detection), ﬁrst to the dataset used in a prior work’s evaluation
and then to a wholly new malware dataset, to see if clustering algorithms de-
veloped without attention to subtleties of malware obfuscation are nevertheless
successful. While these studies provide conﬂicting signals as to the correctness
of our conjecture, our investigation of possible reasons uncovers, we believe, a
cautionary note regarding the signiﬁcance of highly accurate clustering results,
as can be impacted by testing on a dataset with a biased cluster-size distribution.
Keywords: malware clustering and classiﬁcation, plagiarism detection.
1 Introduction
The dramatic growth of the number of malware variants has motivated methods to clas-
sify and group them, enabling analysts to focus on the truly new ones. The need for such
classiﬁcation and pruning of the space of all malware variants is underlined by, e.g., the
Bagle/Beagle malware, for which roughly 30,000 distinct variants were observed be-
tween January 9 and March 6, 2007 [8]. While initial attempts at malware classiﬁcation
were performed manually, in recent years numerous automated methods have been de-
veloped to perform malware classiﬁcation (e.g., [11,6,5,16,9,13,15]). Some of these
malware classiﬁers have claimed very good accuracy in classifying malware, leading
perhaps to the conclusion that malware classiﬁcation is more-or-less solved.
In this paper, we show that this may not be the case, and that evaluating automated
malware classiﬁers poses substantial challenges that we believe require renewed at-
tention from the research community. A central challenge is that with the dearth of a
well-deﬁned notion of when two malware instances are the “same” or “different”, it is
difﬁcult to obtain ground truth to which to compare the results of a proposed classiﬁer.
Indeed, even manually encoded rules to classify malware seems not to be enough —
a previous study [6] found that a majority of six commercial anti-virus scanners con-
curred on the classiﬁcation of 14,212 malware instances in only 2,658 cases. However,
in the absence of better alternatives for determining ground truth, such instances and
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 238–255, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
On Challenges in Evaluating Malware Clustering
239
their corresponding classiﬁcations are increasingly used to evaluate automated meth-
ods of malware clustering. For example, a state-of-the-art malware clustering algorithm
due to Bayer et al. [6] achieved excellent results using these 2,658 malware instances
as ground truth; i.e., the tool obtained results that largely agreed with the clustering of
these 2,658 malware instances by the six anti-virus tools.
The starting point of the present paper is the possibility, we conjectured, that one
factor contributing to these strong results might be that these 2,658 instances are simply
easy to classify, by any of a variety of techniques. We report on our efforts to exam-
ine this possibility, ﬁrst by repeating the clustering of these instances using algorithms
from an ostensibly different domain, namely plagiarism detectors that employ dynamic
analysis. Intuitively, since plagiarism detectors are developed without attention to the
speciﬁcs of malware obfuscation, highly accurate clustering results by these tools might
suggest that this method of selecting ground-truth data biases the data toward easy-to-
classify instances. We describe the results of this analysis, which indicate that plagia-
rism detectors have nearly the same success in clustering these malware instances, thus
providing tentative support for this conjecture.
To more thoroughly examine this possibility, we then attempted to repeat the evalua-
tion methodology of Bayer et al. on a new set of malware instances. By drawing from a
database of malware instances, we assembled a set for which four anti-virus tools con-
sistently labeled each member. We detail this study and report on its results that, much
to our surprise, ﬁnd that neither the Bayer et al. technique nor the plagiarism detectors
we employed were particularly accurate in clustering these instances. Due to certain
caveats of this evaluation that we will discuss, this evaluation is materially different
from that for the previous dataset, causing us to be somewhat tentative in the conclu-
sions we draw from it. Nevertheless, these results temper the conﬁdence with which
we caution that the selection of ground-truth data based on the concurrence of multiple
anti-virus tools biases the data toward easy-to-classify instances.
But this leaves the intriguing question: Why the different results on the two datasets?
We complete our paper with an analysis of a factor that, we believe, contributes to
(though does not entirely explain) this discrepancy, and that we believe offers a cau-
tionary note for the evaluation of malware clustering results. This factor is the makeup
of the ground-truth dataset, in terms of the distribution of the sizes of the malware
families it contains. We observe that the original dataset, on which the algorithms we
consider perform well, is dominated by two large families, but the second dataset is
more evenly distributed among many families. We show that this factor alone biases
the measures used in comparing the malware clustering output to the dataset families,
speciﬁcally precision and recall, in that it increases the likelihood of good precision
and recall numbers occurring by chance. As such, the biased cluster-size distribution in
the original dataset erodes the signiﬁcance (c.f., [22, Section 8.5.8]) of the high preci-
sion and recall reported by Bayer et al. [6]. This observation, we believe, identiﬁes an
important factor for which to control when measuring the effectiveness of a malware
clustering technique.
While we focus on a single malware classiﬁer for our analysis [6], we do so because
very good accuracy has been reported for this algorithm and because the authors of that
technique were very helpful in enabling us to compare with their technique. We hasten
240
P. Li et al.
to emphasize, moreover, that our comparisons to plagiarism detectors are not intended
to suggest that plagiarism detectors are the equal of this technique. For one, we believe
the technique of Bayer et al. is far more scalable than any of the plagiarism detectors
that we consider here, an important consideration when clustering potentially tens of
thousands of malware instances. In addition, the similar accuracy of the technique of
Bayer et al. to the plagiarism detectors does not rule out the possibility that the plagia-
rism detectors are more easily evaded (in the sense of diminishing clustering accuracy);
rather, it simply indicates that malware today does not seem to do so. We stress that the
issues we identify are not a criticism of the Bayer et al. technique, but rather are issues
worth considering for any evaluation of malware clustering and classiﬁcation.
To summarize, the contributions of this paper are as follows. First, we explore the
possibility that existing approaches to obtaining ground-truth data for malware cluster-
ing evaluation biases results by isolating those instances that are simple to cluster or
classify. In the end, we believe our study is inconclusive on this topic, but that reporting
our experiences will nevertheless raise awareness of this possibility and will underline
the importance of ﬁnding methods to validate the ground-truth data employed in this
domain. Second, we highlight the importance of the signiﬁcance of positive cluster-
ing results when reporting them. This has implications for the datasets used to evalu-
ate malware clustering algorithms, in that it requires that datasets exhibiting a biased
cluster-size distribution not be used as the sole vehicle for evaluating a technique.
2 Classiﬁcation and Clustering of Malware
To hinder static analysis of binaries, the majority of current malware makes use of ob-
fuscation techniques, notably binary packers. As such, dynamic analysis of such mal-
ware is often far more effective than static analysis. Monitoring the behavior of the
binary during its execution enables collecting a proﬁle of the operations that the binary
performs and offers potentially greater insight into the code itself if obfuscation is re-
moved (e.g., the binary is unpacked) in the course of running it. While this technique
has its limitations — e.g., it may be difﬁcult to induce certain behaviors of the malware,
some of which may require certain environmental conditions to occur [10,14,19,20] —
it nevertheless is more effective than purely static approaches. For this reason, dynamic
analysis of malware has received much attention in the research community. Analysis
systems such as CWSandbox [25], Anubis [7], BitBlaze [18], Norman [2] and Threat-
Expert [1] execute malware samples within an instrumented environment and monitor
their behaviors for analysis and development of defense mechanisms.
A common application for dynamic analysis of malware is to group malware in-
stances, so as to more easily identify the emergence of new strains of malware, for
example. Such grouping is often performed using machine learning, either by cluster-
ing (e.g., [6,17,15]) or by classiﬁcation (e.g., [13,5,16,11]), which are unsupervised and
supervised techniques, respectively.
Of primary interest in this paper are the methodologies that these works employ to
evaluate the results of learning, and speciﬁcally the measures of quality for the clus-
tering or classiﬁcation results. Let M denote a collection of m malware instances to
be clustered, or the “test data” in the case of classiﬁcation. Let C = {Ci}1≤i≤c and
On Challenges in Evaluating Malware Clustering
241
D = {Di}1≤i≤d be two partitions of M , and let f : {1 . . . c} → {1 . . . d} and
g : {1 . . . d} → {1 . . . c} be functions. Many prior techniques evaluated their results
using two measures:
prec(C,D) =
recall(C,D) =
1
m
1
m
c(cid:2)
i=1
d(cid:2)
|Ci ∩ Df (i)|
|Cg(i) ∩ Di|
i=1
where C is the set of clusters resulting from the technique being evaluated and D is the
clustering that represents the “right answer”.
More speciﬁcally, in the case of classiﬁcation, Ci is all test instances classiﬁed as
class i, and Di is all test instances that are “actually” of class i. As such, in the case of
classiﬁcation, c = d and f and g are the identity functions. As a result prec(C,D) =
recall(C,D), and this measure is often simply referred to as accuracy. This is the mea-
sure used by Rieck et al. [16] to evaluate their malware classiﬁer, and Lee et al. [13]
similarly uses error rate, or one minus the accuracy.
In the clustering case, there is no explicit label to deﬁne the cluster in D that corre-
sponds to a speciﬁc cluster in C, and so one approach is to deﬁne
f(i) = arg max
i(cid:2)
g(i) = arg max
i(cid:2)
|Ci ∩ Di(cid:2)|
|Ci(cid:2) ∩ Di|
In this case, f and g will not generally be the identity function (or even bijections),
and so precision and recall are different. This approach is used by Rieck et al. [17]
and Bayer et al. [6] in evaluating their clustering techniques. In this case, when it is
desirable to reduce these two measures into one, a common approach (e.g., [17]) is to
use the F-measure:
F-measure(C,D) =
2 · prec(C,D) · recall(C,D)
prec(C,D) + recall(C,D)
This background is sufﬁcient to highlight the issues on which we focus in the paper:
Production of D: A central question in the measurement of precision and recall is how
the reference clustering D is determined. A common practice is to use an existing anti-
virus tool to label the malware instances M (e.g., [16,13,11]), the presumption being
that anti-virus tools embody hand-coded rules to label malware instances and so are a
good source of “manually veriﬁed” ground truth. Unfortunately, existing evidence sug-
gests otherwise, in that it has been shown that anti-virus engines often disagree on their
labeling (and clustering) of malware instances [5]. To compensate for this, another prac-
tice has been to restrict attention to malware instances M on which multiple anti-virus
tools agree (e.g., [6]). Aside from substantially reducing the number of instances, we
conjecture that this practice might contribute to more favorable evaluations of malware
classiﬁers, essentially by limiting evaluations to easy-to-cluster instances. To demon-
strate this possibility, in Section 3 we consider malware instances selected in this way
242
P. Li et al.
and show that they can be classiﬁed by plagiarism detectors (designed without attention
to the subtleties of malware obfuscation) with precision and recall comparable to that
offered by a state-of-the-art malware clustering tool.
Distribution of cluster sizes in C and D:
In order to maximize both precision and
recall (and hence the F-measure), it is necessary for C and D to exhibit similar cluster-
size distributions; i.e., if one of them is highly biased (i.e., has few, large clusters) and
the other is more evenly distributed, then one of precision or recall will suffer. Even
when they exhibit similar cluster-size distributions, however, the degree to which that
distribution is biased has an effect on the signiﬁcance (e.g., [22, Section 8.5.8]) that
one can ascribe to high values of these measures. Informally, the signiﬁcance of a given
precision or recall is related to the probability that this value could have occurred by
random chance; the higher the probability, the less the signiﬁcance. We will explore the
effect of cluster-size distribution on signiﬁcance, and speciﬁcally the impact of cluster-
size distribution on the sensitivity of the F-measure to perturbations in the distance
matrix from which the clustering C is derived. We will see that all other factors held
constant, good precision and recall when the reference clusters in D are of similar size
is more signiﬁcant than if the cluster sizes are biased. That is, small perturbations in the
distance matrix yielding C tends to decay precision and recall more than if D and C are
highly biased.
We will demonstrate this phenomenon using the malware clustering results obtained
from the state-of-the-art malware clustering tool due to Bayer et al., which obtains