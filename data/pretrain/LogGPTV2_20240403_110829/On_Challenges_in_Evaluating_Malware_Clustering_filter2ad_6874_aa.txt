# On Challenges in Evaluating Malware Clustering

**Authors:**
- Peng Li, University of North Carolina, Chapel Hill
- Limin Liu, Chinese Academy of Sciences
- Debin Gao, Singapore Management University
- Michael K. Reiter, University of North Carolina, Chapel Hill

**Institutional Affiliations:**
- Singapore Management University
- Research Collection School of Information Systems

**Publication Date:**
September 2010

**Conference:**
Recent Advances in Intrusion Detection: 13th International Symposium, RAID 2010, Ottawa, Ontario, Canada, September 15-17, 2010

**Abstract:**
Malware clustering and classification are essential tools for prioritizing malware analysis efforts. Recent automated methods for malware clustering and classification have reported high accuracy, suggesting that the problem may be largely solved. In this paper, we investigate the hypothesis that the selection of ground-truth data in prior evaluations may bias results toward high accuracy. To test this, we apply clustering algorithms from a different domain (plagiarism detection) to both a previously used dataset and a new malware dataset. Our findings provide mixed signals regarding our hypothesis but highlight the importance of considering the distribution of cluster sizes in the ground-truth dataset. We conclude with a cautionary note on the significance of highly accurate clustering results, which can be influenced by biased cluster-size distributions.

**Keywords:**
malware clustering, classification, plagiarism detection, evaluation challenges

## 1. Introduction

The rapid proliferation of malware variants has driven the development of methods to classify and group them, allowing analysts to focus on truly novel threats. For example, the Bagle/Beagle malware saw approximately 30,000 distinct variants between January 9 and March 6, 2007 [8]. Initially, malware classification was performed manually, but recent years have seen the emergence of numerous automated methods [11, 6, 5, 16, 9, 13, 15]. Some of these classifiers claim very high accuracy, leading to the perception that malware classification is largely resolved.

However, this paper argues that significant challenges remain in evaluating automated malware classifiers. A key challenge is the lack of a well-defined notion of when two malware instances are "the same" or "different," making it difficult to establish ground truth. A previous study [6] found that only 2,658 out of 14,212 malware instances were consistently classified by six commercial anti-virus scanners. Despite this, such instances are often used as ground truth in evaluating automated malware clustering methods. For instance, Bayer et al. [6] achieved excellent results using these 2,658 instances as ground truth.

We hypothesize that one factor contributing to these strong results might be that these 2,658 instances are inherently easy to classify. To explore this, we first apply plagiarism detection algorithms, which are developed without attention to malware obfuscation, to the same dataset. We then repeat the evaluation on a new set of malware instances, assembled from a database where four anti-virus tools consistently labeled each member. Our results suggest that the selection of ground-truth data based on the concurrence of multiple anti-virus tools may indeed bias the data toward easy-to-classify instances.

## 2. Classification and Clustering of Malware

To evade static analysis, most modern malware employs obfuscation techniques, such as binary packers. Dynamic analysis, which monitors the behavior of the binary during execution, is often more effective. Systems like CWSandbox [25], Anubis [7], BitBlaze [18], Norman [2], and ThreatExpert [1] execute malware samples in an instrumented environment to monitor their behaviors.

Dynamic analysis is commonly used to group malware instances, facilitating the identification of new strains. This grouping is often performed using machine learning techniques, including clustering and classification. Clustering is an unsupervised technique, while classification is supervised.

### Evaluation Metrics

Let \( M \) be a collection of \( m \) malware instances, and let \( C = \{C_i\}_{i=1}^c \) and \( D = \{D_i\}_{i=1}^d \) be two partitions of \( M \). Let \( f: \{1, \ldots, c\} \to \{1, \ldots, d\} \) and \( g: \{1, \ldots, d\} \to \{1, \ldots, c\} \) be functions. Many prior works use precision and recall to evaluate clustering and classification results:

\[
\text{prec}(C, D) = \frac{1}{m} \sum_{i=1}^c |C_i \cap D_{f(i)}|
\]

\[
\text{recall}(C, D) = \frac{1}{m} \sum_{i=1}^d |C_{g(i)} \cap D_i|
\]

In classification, \( c = d \) and \( f \) and \( g \) are identity functions, so precision equals recall, and this measure is often referred to as accuracy. For clustering, \( f \) and \( g \) are defined as:

\[
f(i) = \arg\max_{j} |C_i \cap D_j|
\]

\[
g(i) = \arg\max_{j} |C_j \cap D_i|
\]

The F-measure combines precision and recall:

\[
\text{F-measure}(C, D) = \frac{2 \cdot \text{prec}(C, D) \cdot \text{recall}(C, D)}{\text{prec}(C, D) + \text{recall}(C, D)}
\]

### Key Issues

1. **Production of Ground Truth \( D \)**:
   - Common practice is to use existing anti-virus tools to label malware instances, assuming they embody hand-coded rules.
   - However, anti-virus engines often disagree, leading to the practice of using instances on which multiple tools agree.
   - This may bias evaluations toward easy-to-cluster instances.

2. **Distribution of Cluster Sizes**:
   - Similar cluster-size distributions in \( C \) and \( D \) are necessary for high precision and recall.
   - Biased distributions (e.g., few large clusters) can lead to less significant precision and recall values.
   - Small perturbations in the distance matrix can significantly affect precision and recall if the cluster sizes are biased.

## 3. Experimental Analysis

### Applying Plagiarism Detectors

We applied plagiarism detection algorithms to the dataset used by Bayer et al. [6] and a new malware dataset. The results showed that plagiarism detectors achieved nearly the same success in clustering the original dataset, supporting our conjecture that the ground-truth data may be biased toward easy-to-classify instances.

### New Dataset Evaluation

We assembled a new dataset of malware instances consistently labeled by four anti-virus tools. Surprisingly, neither the Bayer et al. technique nor the plagiarism detectors were particularly accurate in clustering these instances. This suggests that the original dataset's cluster-size distribution may have biased the results.

### Cluster-Size Distribution Impact

We observed that the original dataset was dominated by two large families, while the new dataset had a more even distribution. This difference biases precision and recall, increasing the likelihood of good scores occurring by chance. Thus, the high precision and recall reported by Bayer et al. may be less significant due to the biased cluster-size distribution.

## 4. Conclusion

Our study highlights the challenges in evaluating malware clustering and classification. The selection of ground-truth data based on the concurrence of multiple anti-virus tools may bias results toward easy-to-classify instances. Additionally, the distribution of cluster sizes in the ground-truth dataset can significantly impact the significance of precision and recall. These findings underscore the need for careful consideration of these factors in future evaluations.

**Acknowledgments:**
We thank the authors of the state-of-the-art malware clustering tool for their assistance and the research community for their contributions to this field.

**References:**
[1] ThreatExpert. Available at: <https://www.threatexpert.com/>
[2] Norman. Available at: <https://www.norman.com/>
[5] Previous Study. Available at: <citation>
[6] Bayer et al. Available at: <citation>
[7] Anubis. Available at: <https://anubis.iseclab.org/>
[8] Bagle/Beagle Malware. Available at: <citation>
[9] Previous Work. Available at: <citation>
[11] Previous Work. Available at: <citation>
[13] Lee et al. Available at: <citation>
[15] Previous Work. Available at: <citation>
[16] Rieck et al. Available at: <citation>
[17] Rieck et al. Available at: <citation>
[18] BitBlaze. Available at: <http://bitblaze.cs.berkeley.edu/>
[19] Previous Work. Available at: <citation>
[20] Previous Work. Available at: <citation>
[22] Statistical Significance. Available at: <citation>
[25] CWSandbox. Available at: <https://www.cwsandbox.org/>