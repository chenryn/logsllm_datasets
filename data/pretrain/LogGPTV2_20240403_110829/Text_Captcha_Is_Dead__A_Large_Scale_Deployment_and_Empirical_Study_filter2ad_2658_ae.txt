PROCESS
In this section, we further analyze the fine-tuning process of adv-
CAPTCHA through offline experiments, including security anal-
ysis after attack model updating, robustness in defending against
different attack models, the impact of human labor in the query
process and the influence of fine-tuning parameters. We analyze the
worst-case performance of advCAPTCHA by using Model 1 as the
baseline substitute model and Model 4 as the default attack since
they present the largest difference in structure. To be consistent
with the experiments in Section 5, we generate adversarial captchas
by using the mixture method which injects L0 perturbation with
ϵ = 100 and L∞ perturbation with ϵ = 0.3.
6.1 Security Analysis after Attack Model
Update
First, we investigate the fine-tuning process of advCAPTCHA un-
der attack model updating. Specifically, we compare the security of
the adversarial captchas generated by the original substitute model
and the fine-tuned substitute model. In the experiment, we retrain
the attack model by using 100,000 adversarial captchas and the
corresponding labels to simulate the process that captcha solving
services update their attack models (assuming that the attackers
obtain the ground-truth labels of the adversarial captchas using
human labor or other means). Then we query the retrained at-
tack model (by sending 100,000 adversarial captchas to it) and use
the query results (the answers submitted by the attack model) to
fine-tune the substitute model. Finally, we leverage the original
substitute model and the fine-tuned substitute model, respectively,
to generate adversarial captchas and measure the corresponding
SRs of the attack model.
We observe that the SR of the attack model has increased from
7% to 43% when we use 100,000 adversarial captchas to retrain the
Figure 8: The SRs of the attack model for the adversarial captchas,
which are generated by fine-tuned substitute models at different
rounds of attack model update.
attack model. After we fine-tune the substitute model by the 100,000
answers submitted by the attack model, its SR has decreased to 13%.
These observations demonstrate that advCAPTCHA can adaptively
defend against attack model update. In practice, we can fine-tune
the captcha generation model regularly, e.g., each week, or when a
substantial increase of the overall SR is observed.
Next, we conduct five rounds of attack model update to further
measure the security of adversarial captchas. Each round follows
the same setting listed above, i.e., using 100,000 adversarial captchas
to retrain and query the attack model. After each round of update,
we obtain a new fine-tuned model. Figure 8 shows the SRs of the
attack model for the adversarial captchas, which are generated by
fine-tuned substitute models at different rounds of attack model
update. From Figure 8, we can observe that the SR of the attack
model grows slowly with the number of rounds of attack model
update. This observation indicates that the security of adversarial
captchas will not decrease rapidly with the update of the attack
model. Considering the actual time and cost that the attackers need
to upgrade their models, adversarial captchas can greatly improve
the security and vitality of the original captcha scheme.
6.2 Robustness of advCAPTCHA in Defending
against Various Attack Models
In practice, there may exist multiple attack models that simulta-
neously break captchas. Here we consider multiple attack models
(Model 2, 3, 4, 5, 6 with the corresponding SR for the ordinary
captchas shown in Table 3). In the query process, we query each
of the five attack models for the same number of times (i.e., we
send them the same number of adversarial captchas) ranging from
10K to 100K. Figure 9 shows the performance of advCAPTCHA
under the five different attack models. From Figure 9 and Table
3, we observe that 1) the SR of all the attack models are within
[41%, 46%], which indicates that advCAPTCHA can defend against
various attack models; 2) our fine-tuning process can still effectively
reduce the SR of all the attack models (from 43% to 23% on average
across models). We also observe that under multiple attack models,
the degradation of SR is less than that of a single attack model (as
shown in Figure 11). In particular, for 100,000 query rounds, the SR
is 13% under the single model attack and 23% under the attack of
five models. This result is within our expectation since it is more
difficult to use a single substitute model to fit multiple different
attack models.
11
Figure 9: The performance of the fine-tuning process under 5 at-
tack models.
6.3 Impact of Human Answers in Query
Process
In the querying process, we use the risk analysis system to iden-
tify attackers. However, the queries we collect from the identified
attackers may contain results submitted by human, which may
be caused by the false positives of the risk analysis system. Here,
we investigate whether these human answers would influence the
fine-tuning process.
Adversarial captchas are designed for defending against auto-
mated solving, while can still be identified correctly by human
beings. Thus the answers submitted by human are often correct.
Hence, in order to simulate human answers, we change the query
answers of the attack model as the correct captchas. Specifically,
in the experiment, we first use adversarial captchas to query the
attack model and obtain answers from it. Then, we modify the
query answers to the correct ones by different proportions (e.g.,
0%, 10%, 20%, ..., 100%). Here, different proportions correspond to
the different numbers of human answers in all of the queries. Next,
we use the modified query data to fine-tune the substitute model.
Finally, we leverage the fine-tuned substitute models to generate
adversarial captchas, and measure the SR of the attack model.
Figure 10 shows how human answers in the queries affects the
performance of adversarial captchas. We can observe that 1) a small
proportion of human answers in the queries has no impact on
model fine-tuning. The SR of the attack model increases slightly
when the proportion of human answers ≤ 20%. Moreover, we can
query more times to achieve a similar performance to that without
human answers if the proportion of human answers ≤ 30%; 2) as
the proportion of human answers increases, the SR of the attack
model grows. However, even under a high proportion of human
answers, the SR is similar to that without fine-tuning. These results
demonstrate that advCAPTCHA is resistant to human answers in
the query process. One reason is that, in the fine-tuning process,
we ensure the model accuracy for its original training data (recall
Equation 4), which can prevent the substitute model from deviating
from the right direction. We further analyze the impact of the
parameters in Equation 4 to the fine-tuning process in the following
experiment.
6.4 Performance under Different λ
In order to better illustrate the role of the second item L(θ, xo, yo)
in the fine-tuning objective function (Equation 4), we conduct an ex-
periment to investigate the fine-tuning performance under various
values of λ. Specifically, we fine-tune the substitute model under
12
Figure 10: The success rates of the attack model after modifying
different proportions of query results to fine-tune the substitute
model.
Table 5: Original accuracy (classification accuracy on the original
training data) and success rate of the adversarial captchas generated
by the fine-tuned models under different λ and proportions of mod-
ified query results.
0
1
λ Proportion Original Accuracy SR
40.3%
50.5%
70.2%
25.5%
32.5%
42.8%
41.3%
43.6%
51.5%
80.1%
81.2%
83.2%
83.7%
85.3%
90.2%
87.3%
90.3%
92.1%
0
0.1
0.7
0
0.1
0.7
0
0.1
0.7
10
different values of λ and different proportions of human-submitted
answers, given the total number of queries is 100,000. We show the
corresponding results in Table 5.
From Table 5, we have the following observations. First, when
only 0% or 10% of the answers are submitted by human, the second
item in the fine-tuning objective function can prevent the degra-
dation of model accuracy on the original data and meanwhile can
decline the SR of the attack model. Second, it is important to select
an appropriate value of λ. A large value of λ would easily make the
model pay too much attention to the original training data in the
fine-tuning process, instead of fitting the attack model. In practice,
according to our experience, we found that dynamically adjusting
its value so that the model accuracy on the original training data is
around 90% can achieve a good trade-off between fitting the attack
model and preventing model crash.
6.5 Impact of Captchas in the Query Process
According to the analysis in Section 3.5, we know that using ad-
versarial captchas (instead of ordinary captchas) can accelerate
the fine-tuning process. Here, we further verify this conclusion
using empirical evaluations. In the experiment, we first send both
the ordinary captchas and the adversarial captchas to the attack
model, respectively. The number of captchas ranges from 10K to
100K for each type of captcha. Then we fine-tune the substitute
models by using the answers corresponding to the ordinary and
adversarial captchas, respectively, and obtain two new substitute
models. Finally, we generate adversarial captchas by leveraging
the two fine-tuned substitute models to defend against the attack
model. Figure 11 shows the relationship between the SR of the
as well, which further demonstrates its generalizability in various
captcha generation schemes and application scenarios.
advCAPTCHA is also a potential enhancement to the user risk
analysis system, e.g., Google noCaptcha. This popular captcha
scheme can provide a great user experience, as normal users’ oper-
ations are not interrupted in most situations. However, the security
of the overall system has not been improved as compared to tra-
ditional captchas, since the adversaries can simply ignore the risk
analysis in the first phase and only focus on captcha solving in
the second phase. In this case, our advCAPTCHA can degrade the
performance of captcha solvers through dynamically generating
adversarial captchas.
The usability of advCAPTCHA can be affected by the ordinary
version of a captcha scheme. The failure rates of advCAPTCHA
for normal users highly depend on the ordinary version of the
captcha scheme itself. Therefore, advCAPTCHA cannot enhance
a captcha scheme if it is already poorly designed. Moreover, sim-
ilar to many other captcha schemes which perform recognition
tasks, advCAPTCHA cannot defend against the captcha solving
services that employ human users. We believe this is acceptable,
since captchas are designed to be recognized by human beings.
Limitations and Future Works. As an attempt to design secure,
adaptive and practically high-usable adversarial captchas, we be-
lieve our approach can be improved in many perspectives, especially
when considering the practical deployment and application. We
discuss the limitations of this work along with future directions
below.
Generate more secure and usable adversarial captchas. In
our adversarial captcha generation algorithm, the total amount of
perturbations is a key parameter to control balance between the
security and usability of adversarial captchas. How to generate
more secure and usable adversarial captchas with less perturbation
is a meaningful, yet challenging, question. Meanwhile, given an
amount of perturbation, how to elegantly distribute it over an
adversarial captcha (instead of directly injecting perturbations into
the background areas) is another interesting topic to study.
Generate more effective query data. In the paper, we only
use ordinary captchas and adversarial captchas to query the attack
model. In fact, we can deliberately construct well-designed data to
accelerate the model extraction process. For example, according to
previous study [39], using data closer to the model classification
boundary can benefit the model extraction process. Therefore, how
to generate more effective query data for the captcha solvers would
be an interesting future work.
Leverage multiple substitute models. In Section 6.2, we only
use a single substitute model to generate adversarial captchas. How-
ever, when facing several different attack models, it is difficult to
use a single substitute model to fit all these models. As an interest-
ing future work, we plan to generalize our method to incorporate
multiple substitute models so that they can better approximate a
broad range of attack models in practice.
8 CONCLUSION
In this paper, we present the design and evaluation for advCAPTCHA,
which is the first large-scale deployment of adversarial captchas on
an international e-commerce platform, in order to defend against
13
Figure 11: Impact of captchas for fine-tuning the substitute model.
attack model and the number of queries. We observe that when
fine-tuning the substitute model, using ordinary captchas requires
more queries (about 10 times) to achieve similar performance as
that of the adversarial captchas. This is because both the attack
model and the substitute model have high recognition rates for
ordinary captchas. For a captcha, if the query result (the output
of the attack model for the captcha) is similar to the output of the
substitute model, it is less informative for fine-tuning the substi-
tute model. Therefore, it is recommended to leverage adversarial
captchas in the fine-tuning process.
7 DISCUSSION AND LIMITATION
In the daily operation of websites, since the high-confidence at-
tackers can be relatively easily blocked by a risk analysis system,
we setup our objective of developing advCAPTCHA as to defend
against low-confidence attackers. Certainly, it is impossible to ob-
tain the perfect ground truth of attackers in practice. Therefore,
in the deployment and evaluation, to make our experiments and
results more convincing, following a best-in-practice manner, we
leverage the high-confidence attackers to evaluate the security en-
hancement of advCAPTCHA, i.e., in our evaluation period, before
directly blocking them, we leverage them to evaluate advCAPTCHA
first. Moreover, in our experiments, these high-confidence attackers
are also manually confirmed by the domain experts (shown in Ap-
pendix B). Thus, to some extent, the high-confidence attackers can
be considered as known attackers. It is reasonable to believe that our
experimental results can reflect the performance of advCAPTCHA
under actual attacks.
We would like to highlight the generalizability of advCAPTCHA,
as well as its advantages over existing works. advCAPTCHA and
other captcha schemes are not competitive but complementary. The
key objective of adversarial captchas is to increase the difficulty
of recognition for computers. Therefore, advCAPTCHA can be ex-
tended to captcha schemes that perform similar recognition tasks,
e.g., image captchas. Specifically, there are two steps for general-
izing advCAPTCHA to other captcha schemes: 1) replacing the
substitute model (the CRNN model in Section 3.2) with the model
that could recognize the target captcha scheme, e.g., CNN for rec-
ognizing image captchas; 2) designing an appropriate perturbation
mechanism to inject noise into the target captcha images. In fact,
we have implemented additional evaluation of advCAPTCHA for
click-based captcha schemes where the users are asked to click a