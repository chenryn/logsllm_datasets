The next portion of the URL, the hierarchical file path, is envisioned as a
way to identify a specific resource to be retrieved from the server, such as
/documents/2009/my_diary.txt. The specification quite openly builds on top of
the Unix directory semantics, mandating the resolution of “/../” and “/./”
segments in the path and providing a directory-based method for sorting out
relative references in non–fully qualified URLs.
Using the filesystem model must have seemed like a natural choice in
the 1990s, when web servers acted as simple gateways to a collection of static
files and the occasional executable script. But since then, many contempo-
rary web application frameworks have severed any remaining ties with the
filesystem, interfacing directly with database objects or registered locations in
resident program code. Mapping these data structures to well-behaved URL
It Starts with a URL 27
paths is possible but not always practiced or practiced carefully. All of this
makes automated content retrieval, indexing, and security testing more
complicated than it should be.
Query String
The query string is an optional section used to pass arbitrary, nonhierarchi-
cal parameters to the resource earlier identified by the path. One common
example is passing user-supplied terms to a server-side script that implements
the search functionality, such as:
http://example.com/search.php?query=Hello+world
Most web developers are accustomed to a particular layout of the query
string; this familiar format is generated by browsers when handling HTML-
based forms and follows this syntax:
name1=value1&name2=value2...
Surprisingly, such layout is not mandated in the URL RFCs. Instead, the
query string is treated as an opaque blob of data that may be interpreted by
the final recipient as it sees fit, and unlike the path, it is not encumbered
with specific parsing rules.
Hints of the commonly used format can be found in an informational
RFC 1630,6 in a mail-related RFC 2368,7 and in HTML specifications dealing
with forms.8 None of this is binding, and therefore, while it may be impolite,
it is not a mistake for web applications to employ arbitrary formats for what-
ever data they wish to put in that part of the URL.
Fragment ID
The fragment ID is an opaque value with a role similar to the query string
butthat provides optional instructions for the client application rather than
the server. (In fact, the value is not supposed to be sent to the server at all.)
Neither the format nor function of the fragment ID is clearly specified in
theRFCs, but it is hinted that it may be used to address “subresources” in the
retrieved document or to provide other document-specific rendering cues.
In practice, fragment identifiers have only a single sanctioned use in
thebrowser: that of specifying the name of an anchor HTML element for
in-document navigation. The logic is simple. If an anchor name is supplied in
the URL and a matching HTML tag can be located, the document will be
scrolled to that location for viewing; otherwise, nothing happens. Because
the information is encoded in the URL, this particular view of a lengthy doc-
ument could be easily shared with others or bookmarked. In this use, the
meaning of a fragment ID is limited to scrolling an existing document, so
there is no need to retrieve any new data from the server when only this por-
tion of the URL is updated in response to user actions.
28 Chapter 2
This interesting property has led to another, more recent and completely
ad hoc use of this value: to store miscellaneous state information needed by
client-side scripts. For example, consider a map-browsing application that
puts the currently viewed map coordinates in the fragment identifier so that
it will know to resume from that same location if the link is bookmarked or
shared. Unlike updating the query string, changing the fragment ID on-the-
fly will not trigger a time-consuming page reload, making this data-storage
trick a killer feature.
Putting It All Together Again
Each of the aforementioned URL segments is delimited by certain reserved
characters: slashes, colons, question marks, and so on. To make the whole
approach usable, these delimiting characters should not appear anywhere
inthe URL for any other purpose. With this assumption in mind, imagine a
sample algorithm to split absolute URLs into the aforementioned functional
parts in a manner at least vaguely consistent with how browsers accomplish
this task. A reasonably decent example of such an algorithm could be:
STEP 1: Extract the scheme name.
Scan for the first “:” character. The part of the URL to its left is the
scheme name. Bail out if the scheme name does not conform to the
expected set of characters; the URL may need to be treated as a relative
one if so.
STEP 2: Consume the hierarchical URL identifier.
The string “//” should follow the scheme name. Skip it if found; bail out
if not.
NOTE In some parsing contexts, implementations will be just as happy with zero, one, or even
three or more slashes instead of two, for usability reasons. In the same vein, from its
inception, Internet Explorer accepted backslashes (\) in lieu of slashes in any location
in the URL, presumably to assist inexperienced users.* All browsers other than Firefox
eventually followed this trend and recognize URLs such as http:\\example.com\.
STEP 3: Grab the authority section.
Scan for the next “/”, “?”, or “#”, whichever comes first, to extract the
authority section from the URL. As mentioned above, most browsers will
also accept “\” as a delimiter in place of a forward slash, which may need
to be accounted for. The semicolon (;) is another acceptable authority
delimiter in browsers other than Internet Explorer and Safari; the rea-
son for this decision is unknown.
* Unlike UNIX-derived operating systems, Microsoft Windows uses backslashes instead of slashes
todelimit file paths (say, c:\windows\system32\calc.exe). Microsoft probably tried to compensate for
the possibility that users would be confused by the need to type a different type of a slash on the
Web or hoped to resolve other possible inconsistencies with file: URLs and similar mechanisms
that would be interfacing directly with the local filesystem. Other Windows filesystem specifics
(such as case insensitivity) are not replicated, however.
It Starts with a URL 29
STEP 3A: Find the credentials, if any.
Once the authority section is extracted, locate the at symbol (@) in the
substring. If found, the leading snippet constitutes login credentials,
which should be further tokenized at the first occurrence of a colon (if
present) to split the login and password data.
STEP 3B: Extract the destination address.
The remainder of the authority section is the destination address. Look
for the first colon to separate the hostname from the port number. A
special case is needed for bracket-enclosed IPv6 addresses, too.
STEP 4: Identify the path (if present).
If the authority section is followed immediately by a forward slash—or
for some implementations, a backslash or semicolon, as noted earlier—
scan for the next “?”, “#”, or end-of-string, whichever comes first. The
text in between constitutes the path section, which should be normalized
according to Unix path semantics.
STEP 5: Extract the query string (if present).
If the last successfully parsed segment is followed by a question mark,
scan for the next “#” character or end-of-string, whichever comes first.
The text in between is the query string.
STEP 6: Extract the fragment identifier (if present).
If the last successfully parsed segment is followed by “#”, everything from
that character to the end-of-string is the fragment identifier. Either way,
you’re done!
This algorithm may seem mundane, but it reveals subtle details that even
seasoned programmers normally don’t think about. It also illustrates that it is
extremely difficult for casual users to understand how a particular URL may
be parsed. Let's start with this fairly simple case:
http://example.com&gibberish=1234@167772161/
The target of this URL—a concatenated IP address that decodes to
10.0.0.1—is not readily apparent to a nonexpert, and many users would
believe they are visiting example.com instead.* But all right, that was an easy
one! So let’s have a peek at this syntax instead:
http://example.com\@coredump.cx/
In Firefox, that URL will take the user to coredump.cx, because example.com\
will be interpreted as a valid value for the login field. In almost all other brows-
ers, “\” will be interpreted as a path delimiter, and the user will land on example
.com instead.
* This particular @-based trick was quickly embraced to facilitate all sorts of online fraud
targeted at casual users. Attempts to mitigate its impact ranged from the heavy-handed and
oddly specific (e.g., disabling URL-based authentication in Internet Explorer or crippling it
withwarnings in Firefox) to the fairly sensible (e.g., hostname highlighting in the address bar
ofseveral browsers).
30 Chapter 2
An even more frustrating example exists for Internet Explorer.
Considerthis:
http://example.com;.coredump.cx/
Microsoft’s browser permits “;” in the hostname and successfully
resolvesthis label, thanks to the appropriate configuration of the coredump.cx
domain. Most other browsers will autocorrect the URL to http://example.com/
;.coredump.cx and take the user to example.com instead (except for Safari, where
the syntax causes an error). If this looks messy, remember that we are just
getting started with how browsers work!
Reserved Characters and Percent Encoding
The URL-parsing algorithm outlined in the previous section relies on the
assumption that certain reserved, syntax-delimiting characters will not appear
literally in the URL in any other capacity (that is, they won’t be a part of the user-
name, request path, and so on). These generic, syntax-disrupting delimiters are:
: / ? # [ ] @
The RFC also names a couple of lower-tier delimiters without giving
them any specific purpose, presumably to allow scheme- or application-
specific features to be implemented within any of the top-level sections:
! $ & ' ( ) * + , ; =
All of the above characters are in principle off-limits, but there are legiti-
mate cases where one would want to include them in the URL (for example,
to accommodate arbitrary search terms entered by the user and passed to the
server in the query string). Therefore, rather than ban them, the standard
provides a method to encode all spurious occurrences of these values. The
method, simply called percent encoding or URL encoding, substitutes characters
with a percent sign (%) followed by two hexadecimal digits representing a
matching ASCII value. For example, “/” will be encoded as %2F (uppercase
is customary but not enforced). It follows that to avoid ambiguity, the naked
percent sign itself must be encoded as %25. Any intermediaries that handle
existing URLs (browsers and web applications included) are further com-
pelled never to attempt to decode or encode reserved characters in relayed
URLs, because the meaning of such a URL may suddenly change.
Regrettably, the immutability of reserved characters in existing URLs
isat odds with the need to respond to any URLs that are technically illegal
because they misuse these characters and that are encountered by the browser
in the wild. This topic is not covered by the specifications at all, which forces
browser vendors to improvise and causes cross-implementation inconsisten-
cies. For example, should the URL http://a@b@c/ be translated to http://
a@b%40c/ or perhaps to http://a%40b@c/? Internet Explorer and Safari think
the former makes more sense; other browsers side with the latter view.
It Starts with a URL 31
The remaining characters not in the reserved set are not supposed to
have any particular significance within the URL syntax itself. However, some
(such as nonprintable ASCII control characters) are clearly incompatible
with the idea that URLs should be human readable and transport-safe. There-
fore, the RFC outlines a confusingly named subset of unreserved characters
(consisting of alphanumerics, “-”, “.”, “_”, and “~”) and says that only this
subset and the reserved characters in their intended capacity are formally
allowed to appear in the URL as is.
NOTE Curiously, these unreserved characters are only allowed to appear in an unescaped
form; they are not required to do so. User agents may encode or decode them at whim,
and doing so does not change the meaning of the URL at all. This property brings up
yet another way to confuse users: the use of noncanonical representations of unreserved
characters. Specifically, all of the following are equivalent:
 http://example.com/
 http://%65xample.%63om/
http://%65%78%61%6d%70%6c%65%2e%63%6f%6d/*
A number of otherwise nonreserved, printable characters are excluded
from the so-called unreserved set. Because of this, strictly speaking, the RFCs
require them to be unconditionally percent encoded. However, since brows-
ers are not explicitly tasked with the enforcement of this rule, it is not taken
very seriously. In particular, all browsers allow “^”, “{”, “|”, and “}” to appear
in URLs without escaping and will send these characters to the server as is.
Internet Explorer further permits “”, and “`” to go through; Internet
Explorer, Firefox, and Chrome all accept “\”; Chrome and Internet Explorer
will permit a double quote; and Opera and Internet Explorer both pass the
nonprintable character 0x7F (DEL) as is.
Lastly, contrary to the requirements spelled out in the RFC, most brows-
ers also do not encode fragment identifiers at all. This poses an unexpected
challenge to client-side scripts that rely on this string and expect certain
potentially unsafe characters never to appear literally. We will revisit this
topic inChapter 6.
Handling of Non-US-ASCII Text
Many languages used around the globe rely on characters outside the basic,
7-bit ASCII character set or the default 8-bit code page traditionally used by
all PC-compatible systems (CP437). Heck, some languages depend on alpha-
bets that are not based on Latin at all.
In order to accommodate the needs of an often-ignored but formidable
non-English user base, various 8-bit code pages with an alternative set of high-
bit characters were devised long before the emergence of the Web: ISO8859-1,
* Similar noncanonical encodings were widely used for various types of social engineering attacks,
and consequently, various countermeasures have been deployed through the years. As usual,
some of these countermeasures are disruptive (for example, Firefox flat out rejects percent-
encoded text in hostnames), and some are fairly good (such as the forced “canonicalization”
ofthe address bar by decoding all the unnecessarily encoded text for display purposes).
32 Chapter 2
CP850, and Windows 1252 for Western European languages; ISO 8859-2,
CP852, and Windows 1250 for Eastern and Central Europe; and KOI8-R and
Windows 1251 for Russia. And, because several alphabets could not be accom-
modated in the 256-character space, we saw the rise of complex variable-
width encodings, such as Shift JIS for katakana.
The incompatibility of these character maps made it difficult to exchange
documents between computers configured for different code pages. By the
early 1990s, this growing problem led to the creation of Unicode—a sort of
universal character set, too large to fit within 8 bits but meant to encompass
practically all regional scripts and specialty pictographs known to man. Uni-
code was followed by UTF-8, a relatively simple, variable-width representation
of these characters, which was theoretically safe for all applications capable of
handling traditional 8-bit formats. Unfortunately, UTF-8 required more bytes
to encode high-bit characters than did most of its competitors, and to many
users, this seemed wasteful and unnecessary. Because of this criticism, it took
well over a decade for UTF-8 to gain traction on the Web, and it only did so
long after all the relevant protocols had solidified.
This unfortunate delay had some bearing on the handling of URLs that
contain user input. Browsers needed to accommodate such use very early
on,but when the developers turned to the relevant standards, they found no
meaningful advice. Even years later, in 2005, the RFC 3986 had just this to say:
In local or regional contexts and with improving technology, users
might benefit from being able to use a wider range of characters;
such use is not defined by this specification.
Percent-encoded octets . . . may be used within a URI to represent
characters outside the range of the US-ASCII coded character set if
this representation is allowed by the scheme or by the protocol
element in which the URI is referenced. Such a definition should
specify the character encoding used to map those characters to
octets prior to being percent-encoded for the URI.
Alas, despite this wishful thinking, none of the remaining standards
addressed this topic. It was always possible to put raw high-bit characters in a
URL, but without knowing the code page they should be interpreted in, the
server would not be able to tell if that %B1 was supposed to mean “±”, “a”, or
some other squiggly character specific to the user’s native script.
Sadly, browser vendors have not taken the initiative and come up with a
consistent solution to this problem. Most browsers internally transcode URL
path segments to UTF-8 (or ISO 8859-1, if sufficient), but then they generate
the query string in the code page of the referring page instead. In certain
cases, when URLs are entered manually or passed to certain specialized APIs,
high-bit characters may be also downgraded to their 7-bit US-ASCII look-
alikes, replaced with question marks, or even completely mangled due to
implementation flaws.
It Starts with a URL 33
Poorly implemented or not, the ability to pass non-English characters
inquery strings and paths scratched an evident itch. The traditional percent-
encoding approach left just one URL segment completely out in the cold:
High-bit input could not be allowed as is when specifying the name of the
destination server, because at least in principle, the well-established DNS
standard permitted only period-delimited alphanumerics and dashes to
appear in domain names—and while nobody adhered to the rules, the set
ofexceptions varied from one name server to another.
An astute reader might wonder why this limitation would matter; that is,
why was it important to have localized domain names in non-Latin alphabets,
too? That question may be difficult to answer now. Quite simply, several folks
thought a lack of these encodings would prevent businesses and individuals
around the world from fully embracing and enjoying the Web—and, rightly
or not, they were determined to make it happen.
This pursuit led to the formation of the Internationalized Domain Names
in Applications (IDNA). First, RFC 3490,9 which outlined a rather contrived
scheme to encode arbitrary Unicode strings using alphanumerics and dashes,
and then RFC 3492,10 which described a way to apply this encoding to DNS
labels using a format known as Punycode. Punycode looked roughly like this:
xn--[US-ASCII part]-[encoded Unicode data]
A compliant browser presented with a technically illegal URL that con-
tained a literal non-US-ASCII character anywhere in the hostname was sup-
posed to transform the name to Punycode before performing a DNS lookup.
Consequently, when presented with Punycode in an existing URL, it should
put a decoded, human-readable form of the string in the address bar.
NOTE Combining all these incompatible encoding strategies can make for an amusing mix.
Consider this example URL of a made-up Polish-language towel shop:
Intent: http://www.ręczniki.pl/ręcznik?model=Jaś#Złóż_zamówienie