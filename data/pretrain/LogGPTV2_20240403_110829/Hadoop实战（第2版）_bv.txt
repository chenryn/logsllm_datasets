2.行键（Row Key）的设计
首先，应该避免使用时序或单调（递增/递减）行键。因为当数据到来的时候，HBase首先需要根据记录的行键来确定存储的位置，即Region的位置。如果使用时序或单调行键，那么连续到来的数据将会被分配到同一个Region当中，而此时系统中的其他Region/Regionserver将处于空闲状态，这是分布式系统最不希望看到的情况。如果必须存储这种类型的数据，例如时序值，那么该怎么办呢？在OpenTSB中，行键的设计如下所示：
[metric_type][event_timestamp]
上述方法将时序（event_timestamp）作为行键的第二个“字段”，并为行键添加一个前缀。但是，具体选择什么样的规则来创建行键也需要视情况而定，没有万能的规则。
3.尽量最小化行键和列族的大小
在HBase中，一个具体的值由存储该值的行键、对应的列（列族：列）以及该值的时间戳决定。HBase中的索引是为了加速随机访问的速度。该索引的创建是基于“行键+列族：列+时间戳+值”的，如果行键和列族的大小过大，甚至超过值本身的大小，那么将会增加索引的大小。并且，在HBase中数据记录往往非常之多，重复的行键、列将不但使得索引的大小过大，也将加重系统存储的负担。
4.版本的数量
HBase在进行数据存储的时候，新的数据并不会直接覆盖旧的数据，而是进行追加操作，不同的数据通过时间戳进行区分。默认情况下，每行数据存储三个版本，该值可以通过HColumnDescriptor进行设置，建议不要将其设置得过大。
下面我们通过两个例子，让读者对HBase的模式设计有一个初步的认识。
12.10.2 学生表
这里我们以学习数据库过程中常用的一个学生表为例来讲解模式设计。众所周知，在关系型数据库（RDBMS）中学生表的表结构如表12-15～表12-17所示。
那么在HBase中，数据存储的模式将如表12-18和表12-19所示。
从上面的5个表中可以看出，在RDBMS中可以完成的操作，在HBase中不但可以完成，还可以有更好的执行效率。在HBase中Row Key是索引，因此在HBase中对数据进行查询，能够比RDBMS有更大的速度优势。
12.10.3 事件表
首先我们给出时间表在RDBMS中的表结构，如表12-20所示。
上述事件表存储了所有用户所发生的事件信息，包括事件名称和事件发生的时间。HBase一般针对某一特殊的应用存储数据，因此我们需要首先确定用户的需求。假如用户的需求描述如下：查询某一用户最近发生的10个事件。那么，RDBMS的SQL查询语句如下：
SELECT A_Id, A_UserId, A_Name, A_Time From Action WHERE A_UserId=***ORDER BY A_Time DESC LIMIT 10
在HBase中为了加快数据的查询速度，现在需要将数据以用户聚簇的方式存放，并且按照事件发生的时间倒序排列。那么在HBase中将有下面的存储模式，如表12-21所示。
从上表中可以看出，数据已经按照要求聚簇存放，查询速度必然要优于RDBMS。
12.11 本章小结
本章向大家介绍了HBase，包括HBase的特点、基本操作、体系结构、数据模型、它与其他相关产品的关系，以及如何使用HBase编程、设计表等内容。
通过本章，大家可以了解到，HBase是一个开源的、分布式的、多版本的、面向列的存储模型。它与传统的关系型数据库有着本质的不同，并且在某些场合中，HBase拥有其他数据库所不具有的优势。它为大型数据的存储和某些特殊应用提供了很好的解决方案。
另外，HBase具有三种运行模式。其中，伪分布模式和完全分布模式需要以HDFS作为其文件存储系统。因此HBase可以有效地与MapReduce结合起来使用，充分发挥二者的优势。本章为大家介绍了如何配置IDE进行HBase编程，同时给出了几个简单的编程实例，除此之外，还为大家简单比较了HBase的模式与传统RDBMS模式设计的异同之处。
希望通过对本章的学习，能够让大家对HBase有一个全面、综合的了解。限于篇幅，未能深入地讲解HBase相关的知识，更多的内容，大家可以到HBase官方网站查阅，网址为：http：//hbase.apache.org/。另外，我们还希望读者能够阅读HBase的源码，这样会对HBase的深层机制有更深入的理解。
第13章 Mahout详解
本章内容
Mahout简介
Mahout的安装和配置
Mahout API简介
Mahout中的频繁模式挖掘
Mahout中的聚类和分类
Mahout应用：建立一个推荐引擎
本章小结
13.1 Mahout简介
Apache Mahout起源于2008年，当时它是Apache Lucene的子项目。使用Apache Hadoop库，可以将其功能有效地扩展到Apache Hadoop云平台中。Apache Lucene是一个著名的开源搜索引擎，它实现了先进的信息检索、文本挖掘功能。在计算机科学领域中，这些概念与机器学习技术相近。正是由于这种原因，一些Apache Lucene的开发者最终转入开发机器学习算法中来。进而，这些机器学习算法形成了最初的Apache Mahout。不久以后，Apache Mahout吸收了一个名为Taste的开源协同过滤算法的项目，经过两年的发展，2010年4月Apache Mahout最终成为了Apache的顶级项目。
Apache Mahout的主要目标是建立可伸缩的机器学习算法。这种可伸缩性是针对大规模的数据集而言的。Apache Mahout的算法运行在Apache Hadoop平台下，它通过MapReduce模式实现。但是，Apache Mahout并不严格要求算法的实现要基于Hadoop平台，单个节点或非Hadoop平台也可以。Apache Mahout核心库的非分布式算法也具有良好的性能。
Apache Mahout是Apache Software Foundation（ASF）旗下的一个开源项目，提供了一些经典的机器学习算法，旨在帮助开发人员更加方便快捷地创建智能应用程序。该项目已经发展到了它的第三个年头，有了三个公共发行版本。Apache Mahout项目包含聚类、分类、推荐引擎、频繁子项挖掘。Apache Mahout虽已经实现了很多技术和算法，但是仍然还有一些算法正在开发和测试阶段。目前Apache Mahout项目主要包括以下五个部分。
频繁模式挖掘：挖掘数据中频繁出现的项集。
聚类：将诸如文本、文档之类的数据分成局部相关的组。
分类：利用已经存在的分类文档训练分类器，对未分类的文档进行分类。
推荐引擎（协同过滤）：获得用户的行为并从中发现用户可能喜欢的事物。
频繁子项挖掘：利用一个项集（查询记录或购物目录）去识别经常一起出现的项目。
13.2 Mahout的安装和配置
Mahout是一个开源软件，因此它有两种安装方式：一种是下载已经编译好的二进制文件进行安装（快速安装）；一种是先下载源代码，然后再对源代码进行编译，最后再安装（编译安装）。下面我们分别对其进行介绍。
1.快速安装
下面为该方式的具体安装步骤：
（1）下载Mahout
从下面链接中下载编译好的二进制文件：
http：//mirror.bjtu.edu.cn/apache/mahout/
选择最新的版本目录，即0.6，下载mahout-distribution-0.6.tar.gz。
（2）解压下载的文件
使用下面的命令将下载的二进制文件解压到指定的文件夹中。
tar-zxvf mahout-distribution-0.6.tar.gz-C$HADOOP_HOME/
参数-C的后面是指定的文件夹，这里是$HADOOP_HOME/。
（3）配置环境变量
由于Mahout不仅可以在本地模式下运行，还可以利用Hadoop的MapReduce运行作业。若要使用Hadoop则必须正确安装Hadoop，并配置HADOOP_HOME和HADOOP_CONF_DIR环境变量，具体参见本书第2章“Hadoop的安装与配置”。
使用下面的命令配置Mahout所需要的Hadoop环境变量：
export HADOOP_HOME=/home/hadoop/hadoop-1.0.1
export HADOOP_CONF_DIR=/home/hadoop/hadoop-1.0.1/conf
此外，为了Mahout操作方便，可以将Mahout安装位置加入到环境变量中，如下所示：
#Config Mahout
export MAHOUT_HOME=/home/hadoop/hadoop-1.0.1/mahout-distribution-0.6
export MAHOUT_CONF_DIR=$MAHOUT_HOME/conf
export PATH=$MAHOUT_HOME/conf：$MAHOUT_HOME/bin：$PATH
2.编译安装
首先需要确保系统中已经安装了JDK 1.6以上版本及Maven 2.0以上版本。JDK的安装前面章节已经详细介绍，这里不再赘述。大家可以使用下面命令来安装Maven：
sudo apt-get install maven2
安装完成后，可以对maven的参数进行配置，设置其使用的Java堆空间：
sudo gedit$MAVEN_HOME/bin/mvn
在mvn文件中找到exec"$JAVACMD"\，在它之后加上-Xmx256m\即可，如下所示：
exec"$JAVACMD"\
-Xmx256m\
$MAVEN_OPTS\
-classpath"${M2_HOME}"/boot/classworlds.jar\
"-Dclassworlds.conf=${M2_HOME}/bin/m2.conf"\
"-Dmaven.home=${M2_HOME}"\
${CLASSWORLDS_LAUNCHER}$QUOTED_ARGS
其中参数-Xmx256m指定的是Java的空间大小，读者可以根据具体情况进行设置。下面为具体的操作步骤：
（1）下载最新源码
通过Mahout的svn库来下载当前Mahout的最新版本，Mahout将被下载到当前目录中：
svn co http：//svn.apache.org/repos/asf/mahout/trunk
（2）执行安装
进入Mahout的根目录，输入命令安装：
cd trunk
mvn install
看到如下结果，则表明安装成功。
[INFO]----------------------------------------------------------------------
[INFO]Reactor Summary：
[INFO]----------------------------------------------------------------------
[INFO]Apache Mahout……SUCCESS[8.871s]
[INFO]Mahout Build Tools……SUCCESS[2.696s]
[INFO]Mahout Math……SUCCESS[39.651s]
[INFO]Mahout Core……SUCCESS[54：46.562s]
[INFO]Mahout Integration……SUCCESS[3：47.980s]
[INFO]Mahout Examples……SUCCESS[27.877s]
[INFO]Mahout Release Package……SUCCESS[0.152s]
[INFO]----------------------------------------------------------------------
[INFO]----------------------------------------------------------------------
[INFO]BUILD SUCCESSFUL
[INFO]----------------------------------------------------------------------
[INFO]Total time：59 minutes 55 seconds
[INFO]Finished at：Tue Jun 05 00：07：53 PDT 2012
[INFO]Final Memory：67M/142M
[INFO]----------------------------------------------------------------------
该命令将会自动编译core和example目录并将其打包。从上面可以看到Mahout的安装花费时间较长，这主要是由于执行testing部分的操作，使用下面命令可以略过此测试部分：
mvn-DskipTests install
注意 采用svn下载的Mahout最新源码有诸多好处，例如可以在$MAHOUT_HOME/examples/src目录下查看Mahout许多算法实现的源码。另外，此版本中还保留了很多Mahout测试使用的数据，例如$MAHOUT_HOME/core/src/test/resources/目录下FPGrowth算法使用的零售商数据。
（3）配置环境变量
环境变量的配置与快速安装相同，这里不再赘述。
3.验证是否安装成功
我们可以使用如下命令来检查Mahout是否安装成功：
bin/mahout-help
如果安装成功，系统会自动列出Mahout已经实现的所有命令，如图13-1所示。
至此Mahout安装完毕。
Mahout自带了一些示例程序，执行下面的Hadoop命令，可以运行Canopy算法示例：
bin/hadoop jar$MAHOUT_HOME/mahout-examples-0.6.job org.apache.mahout.clustering.
syntheticcontrol.canopy.Job
图 13-1 Mahout实现命令图
转到Mahout安装目录下，运行以下命令可以将结果直接显示在控制台上：
bin/mahout vectordump--seqFile/user/hadoop/output/data/part-00000
13.3 Mahout API简介
当前Mahout最新版本的API为Mahout Core 0.7-SNAPSHOT API
[1]
 ，它主要可以分为以下几部分：8
基于协同过滤的Taste相关的API，包名以org.apache.mahout.cf.taste开始；
聚类算法相关的API，包名以org.apache.mahout.clustering开始；
分类算法，包名以org.apache.mahout.classifier开始；
频繁模式算法，包名以org.apache.mahout.fpm开始；
数学计算相关算法，包名以org.apache.mahout.math开始；
向量计算相关算法，包名以org.apache.mahout.vectorizer开始。
在新的版本中，Mahout已经实现了数据挖掘中较常见算法，包括：频繁模式挖掘、聚类、分类以及推荐引擎，另外，还实现了数据挖掘中常用的预处理算法。
Apache Mahout已经实现的聚类算法有：Canopy聚类算法、K-Means聚类算法、模糊K-Means聚类算法、Mean Shift聚类算法、Dirichlet过程聚类算法和Latent Dirichlet Allocation聚类算法。这些算法相关的API都可以在org.apache.mahout.clustering包中找到。
下面以K-Means算法为例进行介绍。K-Means算法的API在org.apache.mahout.clustering.kmeans包中，一共包含1个接口和3个类
[2]
 。它们分别是KMeansConfigKeys、KCluster、KMeansDriver和RandomSeedGenerator。
1.KMeansConfigKeys接口
接口KMeansConfigKeys一共有三个参数：DISTANCE_MEASURE_KEY、CLUSTER_CONVERGENCE_KEY、CLUSTER_PATH_KEY，每个参数的具体意义如表13-1所示。
2.KCluster类
该类通常被主函数调用，通过给定的新聚类中心和距离函数来计算新的聚类，并判断聚类是否收敛。如表13-2所示为类KCluster的主要函数列表：
3.KMeansDriver类
该类为执行聚类操作的入口函数，包括buildClusters、clusterData、run及main等函数，如表13-3所示为类KMeansDriver的主要函数列表：
对于详细的类介绍，请大家自行查阅Mahout API文档。
[1]
 https：//builds.apache.org/hudson/job/Mahout-Quality/javadoc。