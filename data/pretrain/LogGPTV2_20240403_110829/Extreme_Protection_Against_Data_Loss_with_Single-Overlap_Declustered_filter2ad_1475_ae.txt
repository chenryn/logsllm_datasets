u
b
e
R
 35
 30
 25
 20
 15
 10
 5
 0
2.7
2.8 2.94 3.66
Critical rebuild
Degraded rebuild
)
s
r
u
o
H
(
e
m
i
t
d
l
i
u
b
e
R
 35
 30
 25
 20
 15
 10
 5
 0
R
D
A
I
D
P
d
R
A
I
D
O
-
S
G
-
S
O
O
D
D
P
P
R
D
A
I
D
P
d
R
A
I
D
O
-
S
G
-
S
O
O
D
D
P
P
(a) Single Disk Failure
(b) Two Disk Failure
Fig. 20: Rebuild times for single and two disk failures
disk failure. Trinity declustered parity and dRAID show the
fastest rebuild times requiring only 2.7 and 2.8 hours. O-SODP
exhibits similar rebuild performance to DP and dRAID while
G-SODP requires slightly more rebuild time. Because it lacks
declustering traditional raid requires greater than 24 hours to
perform a rebuild. With multiple disk failures we exploit the
priority reconstruction algorithm which give vulnerable stripes
higher rebuild priority to avoid data loss. In our conﬁguration
(e.g., 8 + 2), the maximal tolerable failures within a single
stripeset is 2, therefore, Figure 20(b) compares the critical
rebuild (e.g., stripes with 2 failures) and degraded rebuild (e.g.,
stripes with 1 failures) time during reconstruction process. We
observe that DP, dRAID and O-SODP have almost the same
degraded rebuild time due to full declustering. A drawback of
the SODP schemes is the longer critical rebuild times. Finally,
RAID systems do not use priority reconstruction.
4) Comparison of Storage Efﬁciency: Finally, we study
the effects of storage efﬁciency by using different parity
conﬁgurations. Since O-SODP cannot be calculated for ar-
bitrary conﬁgurations we only compare RAID, DP, dRAID
and G-SODP. In Figure 21 we show how the parity over-
head effects the probability of data loss for differing parity
overheads.Generally we see that additional parity overhead
is moderately effective during failure bursts, but that during
failures distributed over time even extremely low overhead G-
SODP conﬁgurations provide low probabilities of data loss.
s
s
o
l
t
a
a
d
f
o
y
t
i
l
i
b
a
b
o
r
P
100%
80%
60%
40%
20%
0%
DP
dRAID
G-SODP
RAID
RS(2,2) RS(4,2) RS(6,2) RS(8,2)
s
s
o
l
t
a
a
d
f
o
y
t
i
l
i
b
a
b
o
r
P
100%
80%
60%
40%
20%
0%
DP
dRAID
RAID
G-SODP
RS(2,2) RS(4,2) RS(6,2) RS(8,2)
(a) Simultaneous Failures
(b) Burst Failures Over 24h
Fig. 21: The probability of data loss for RAID, DP, dRAID,
G-SODP under simultaneous failures with varying parity
overheads. While (a) shows that additional parity overhead
is moderately effective at improving data loss probabilities
during burst failures, (b) shows that G-SODP provides low
probabilities of data loss while using low parity overheads.
VI. CONCLUSIONS
In our re-examination of declustered parity data placement
schemes it is apparent that existing declustered parity data
protection schemes are not designed to tolerate the correlated
failure bursts becoming increasingly common in cloud and
HPC data centers. To that end, we have proposed adding 2 new
criteria to the design principles for declustered parity designs:
• Maximizing the number of simultaneous disk failures
tolerated without increasing parity overhead, and
• Minimizing disk rebuild time by balancing parity stripes
across all disks.
To balance both of these criteria equally we proposed Single-
Overlap Declustered Parity, a data placement scheme that
minimizes rebuild time and tolerates more failed disks than
existing declustered parity designs. However, in order to build
a ﬂexible algorithm to generate SODP stripesets it became
necessary to relax the single-overlap requirement and allow
a small number of stripesets that do not overlap all other
stripesets. Surprisingly, this more ﬂexible algorithm demon-
strated the lowest data loss during failures occurring within
small windows of time. In our experiments we showed that
adding disks to a declustered placement group increases the
probability of a data loss event by a greater than linear amount
when faced with correlated failures. But the additional disk
results in a less than linear improvement
in rebuild time
because the amount of data being rebuilt remains ﬁxed even
as the rebuild rate is proportionally increased. Thus a data
protection scheme, such as Greedy Single-Overlap Declustered
Parity, which is designed to trade small amounts of rebuild
performance in order to tolerate a large number of disk failures
can reduce the probability of data loss substantially (30x in
some of our test conﬁgurations).
In future work we plan a detailed evaluation of the read and
write performance of SODP-based data protection schemes
and a further exploration of how to optimize the tradeoffs
between rebuild performance and disk failure tolerance. We
also plan to apply our design principles for improving data
protection schemes to distributed storage systems that leverage
multiple levels of erasure coding and replication.
VII. ACKNOWLEDGEMENTS
The authors thank John Bent for helpful discussions on
priority rebuild algorithms within declustered parity storage
systems. We also thank Data Direct Networks for their support
and co-funding this work as partners in the Efﬁcient Mis-
sion Centric Computing Consortium (EMC3). The university
authors were supported by funding from NSF(grant #CNS-
1563956). This manuscript has been approved for unlimited re-
lease and has been assigned LA-UR-20-23191. This work has
been co-authored by an employee of Triad National Security,
LLC which operates Los Alamos National Laboratory under
Contract No. 89233218CNA000001 with the U.S. Department
of Energy/National Nuclear Security Administration. The pub-
lisher, by accepting the article for publication, acknowledges
that the United States Government retains a non-exclusive,
paid-up, irrevocable, world-wide license to publish or repro-
duce the published form of the manuscript, or allow others to
do so, for United States Government purposes.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
353
[18] Thomas JE Schwarz, Jesse Steinberg, and Walter A Burkhard. Permuta-
tion development data layout (pddl). In Proceedings Fifth International
Symposium on High-Performance Computer Architecture, pages 214–
217. IEEE, 1999.
[19] Zfs draid. https://github.com/openzfs/zfs/wiki/dRAID-HOWTO, 2016.
[20] Guangyan Zhang, Zican Huang, Xiaosong Ma, Songlin Yang, Zhufan
Wang, and Weimin Zheng. Raid+: deterministic and balanced data
In 16th {USENIX} Conference
distribution for large disk enclosures.
on File and Storage Technologies ({FAST} 18), pages 279–294, 2018.
[21] Neng Wang, Yinlong Xu, Yongkun Li, and Si Wu. Oi-raid: a two-layer
raid architecture towards fast recovery and high reliability. In 2016 46th
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN), pages 61–72. IEEE, 2016.
[22] Zhipeng Li, Min Lv, Yinlong Xu, Yongkun Li, and Liangliang Xu.
D3: Deterministic data distribution for efﬁcient data reconstruction in
erasure-coded distributed storage systems. In 2019 IEEE International
Parallel and Distributed Processing Symposium (IPDPS), pages 545–
556. IEEE, 2019.
[23] Richard R Muntz and John CS Lui. Performance analysis of disk arrays
under failure. Computer Science Department, University of California,
1990.
[24] Jeff Bonwick, Matt Ahrens, Val Henson, Mark Maybee, and Mark Shel-
lenbaum. The zettabyte ﬁle system. Technical report, Sun Microsystems,
2003.
[25] Eric J Schwabe and Ian M Sutherland.
Improved parity-declustered
journal of computer and system sciences,
layouts for disk arrays.
53(3):328–343, 1996.
[26] Huan Ke. Optimal Single Overlap Declustered Parity (O-SODP)
Feasible Solutions. shorturl.at/hqMR0.
[27] Mi Zhang, Shujie Han, and Patrick PC Lee. A simulation analysis of
reliability in erasure-coded data centers. In 2017 IEEE 36th Symposium
on Reliable Distributed Systems (SRDS), pages 144–153. IEEE, 2017.
[28] John P Klein and Melvin L Moeschberger. Survival analysis: techniques
for censored and truncated data. Springer Science & Business Media,
2006.
[29] Bianca Schroeder and Garth A. Gibson. Disk failures in the real world:
What does an mttf of 1,000,000 hours mean to you? In Proceedings of
the 5th USENIX Conference on File and Storage Technologies, FAST
’07, Berkeley, CA, USA, 2007. USENIX Association.
[30] Cameron Davidson-Pilon, Jonas Kalderstam, Paul Zivich, Ben Kuhn,
Andrew Fiore-Gartland, Luis Moneda, Gabriel, Daniel WIlson, Alex
Parij, Kyle Stark, Steven Anton, Lilian Besson, Jona, Harsh Gadgil,
Dave Golland, Sean Hussey, Ravin Kumar, Javad Noorbakhsh, Andreas
Klintberg, Eduardo Ochoa, Dylan Albrecht, dhuynh, Dmitry Medvinsky,
Denis Zgonjanin, Daniel S. Katz, Daniel Chen, Christopher Ahern, Chris
Fournier, Arturo, and Andr´e F. Rendeiro. Camdavidsonpilon/lifelines:
v0.22.3 (late), August 2019.
[31] Mark Swan. Sonexion GridRAID characteristics. In Proceedings of the
2014 Cray User Group (CUG), 2014.
REFERENCES
[1] Doug Beaver, Sanjeev Kumar, Harry C. Li, Jason Sobel, and Peter
In
Vajgel. Finding a needle in haystack: Facebook’s photo storage.
Proceedings of
the 9th USENIX Conference on Operating Systems
Design and Implementation, OSDI’10, pages 47–60, Berkeley, CA,
USA, 2010. USENIX Association.
[2] Subramanian Muralidhar, Wyatt Lloyd, Sabyasachi Roy, Cory Hill,
Ernest Lin, Weiwen Liu, Satadru Pan, Shiva Shankar, Viswanath Sivaku-
mar, Linpeng Tang, and Sanjeev Kumar. f4: Facebook’s warm BLOB
storage system.
In 11th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 14), pages 383–398, Broomﬁeld, CO,
October 2014. USENIX Association.
[3] S. Oral, J. Simmons, J. Hill, D. Leverman, F. Wang, M. Ezell, R. Miller,
D. Fuller, R. Gunasekaran, Y. Kim, S. Gupta, D. T. S. S. Vazhkudai, J. H.
Rogers, D. Dillow, G. M. Shipman, and A. S. Bland. Best practices and
lessons learned from deploying and operating large-scale data-centric
parallel ﬁle systems.
the International
Conference for High Performance Computing, Networking, Storage and
Analysis, pages 217–228, Nov 2014.
In SC ’14: Proceedings of
[4] Glenn K. Lockwood, Kirill Lozinskiy, Lisa Gerhardt, Ravi Cheema,
Damian Hazen, and Nicholas J. Wright. Designing an all-ﬂash Lustre
ﬁle system for the 2020 NERSC Perlmutter system. In Proceedings of
the 2019 Cray User Group (CUG), 2019.
[5] Ao Ma, Rachel Traylor, Fred Douglis, Mark Chamness, Guanlin Lu,
Darren Sawyer, Surendar Chandra, and Windsor Hsu. Raidshield: char-
acterizing, monitoring, and proactively protecting against disk failures.
ACM Transactions on Storage (TOS), 11(4):17, 2015.
[6] Seagate Technology LLC. Seagate cloud storage array, January 2018.
[7] Seagate Technology LLC. Exos e 4U106, January 2018.
[8] Kevin M. Greenan, James S. Plank, and Jay J. Wylie. Mean time to
meaningless: Mttdl, markov models, and storage system reliability. In
Proceedings of the 2Nd USENIX Conference on Hot Topics in Storage
and File Systems, HotStorage’10, pages 5–5, Berkeley, CA, USA, 2010.
USENIX Association.
[9] Saurabh Kadekodi, K. V. Rashmi, and Gregory R. Ganger. Cluster
improving storage efﬁciency by
storage systems gotta have heart:
exploiting disk-reliability heterogeneity.
In 17th USENIX Conference
on File and Storage Technologies (FAST 19), pages 345–358, Boston,
MA, February 2019. USENIX Association.
[10] Asaf Cidon, Stephen Rumble, Ryan Stutsman, Sachin Katti, John
Ousterhout, and Mendel Rosenblum. Copysets: Reducing the frequency
of data loss in cloud storage. In Presented as part of the 2013 USENIX
Annual Technical Conference (USENIX ATC 13), pages 37–48, 2013.
[11] Ramnatthan Alagappan, Aishwarya Ganesan, Yuvraj Patel, Thanu-
malayan Sankaranarayana Pillai, Andrea C. Arpaci-Dusseau, and
Remzi H. Arpaci-Dusseau. Correlated crash vulnerabilities. In Proceed-
ings of the 12th USENIX Conference on Operating Systems Design and
Implementation, OSDI’16, pages 151–167, Berkeley, CA, USA, 2016.
USENIX Association.
[12] Daniel Ford, Franc¸ois Labelle, Florentina I. Popovici, Murray Stokely,
Van-Anh Truong, Luiz Barroso, Carrie Grimes, and Sean Quinlan.
Availability in globally distributed storage systems.
In Presented as
part of the 9th USENIX Symposium on Operating Systems Design and
Implementation, Vancouver, BC, 2010. USENIX.
[13] James Lujan, Manuel Vigil, Garrett Kenyon, Karissa Sanbonmatsu, and
Brian Albright. Trinity supercomputer now fully operational.
[14] Mark Holland and Garth A. Gibson. Parity declustering for continuous
operation in redundant disk arrays. In Proceedings of the Fifth Interna-
tional Conference on Architectural Support for Programming Languages
and Operating Systems, ASPLOS V, pages 23–35, New York, NY, USA,
1992. ACM.
[15] Guillermo A Alvarez, Walter A Burkhard, and Flaviu Cristian. Tol-
erating multiple failures in raid architectures with optimal storage and
uniform declustering. In ACM SIGARCH Computer Architecture News,
volume 25, pages 62–72. ACM, 1997.
[16] John Fragalla.
Improving Lustre OST performance with ClusterStor
GridRAID. In 2014 HPCAC Stanford HPC Exascale Conference, 2014.
[17] GA Alverez, Walter A Burkhard, LL Stockmeyer, and Flaviu Cristian.
Declustered disk array architectures with optimal and near-optimal
parallelism. In Proceedings. 25th Annual International Symposium on
Computer Architecture (Cat. No. 98CB36235), pages 109–120. IEEE,
1998.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
354