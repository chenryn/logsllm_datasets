title:Revisiting network support for RDMA
author:Radhika Mittal and
Alexander Shpiner and
Aurojit Panda and
Eitan Zahavi and
Arvind Krishnamurthy and
Sylvia Ratnasamy and
Scott Shenker
Revisiting Network Support for RDMA
Radhika Mittal1, Alexander Shpiner3, Aurojit Panda4, Eitan Zahavi3,
Arvind Krishnamurthy5, Sylvia Ratnasamy1, Scott Shenker1,2
1UC Berkeley, 2ICSI, 3Mellanox Technologies, 4NYU, 5Univ. of Washington
Abstract
The advent of RoCE (RDMA over Converged Ethernet) has
led to a significant increase in the use of RDMA in datacenter
networks. To achieve good performance, RoCE requires a
lossless network which is in turn achieved by enabling Pri-
ority Flow Control (PFC) within the network. However, PFC
brings with it a host of problems such as head-of-the-line
blocking, congestion spreading, and occasional deadlocks.
Rather than seek to fix these issues, we instead ask: is PFC
fundamentally required to support RDMA over Ethernet?
We show that the need for PFC is an artifact of current
RoCE NIC designs rather than a fundamental requirement.
We propose an improved RoCE NIC (IRN) design that makes
a few simple changes to the RoCE NIC for better handling of
packet losses. We show that IRN (without PFC) outperforms
RoCE (with PFC) by 6-83% for typical network scenarios.
Thus not only does IRN eliminate the need for PFC, it im-
proves performance in the process! We further show that
the changes that IRN introduces can be implemented with
modest overheads of about 3-10% to NIC resources. Based
on our results, we argue that research and industry should
rethink the current trajectory of network support for RDMA.
CCS Concepts
• Networks → Transport protocols;
Keywords
Datacenter transport, RDMA, RoCE, iWARP, PFC
ACM Reference Format:
Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi,
Arvind Krishnamurthy, Sylvia Ratnasamy, Scott Shenker. 2018. Re-
visiting Network Support for RDMA. In SIGCOMM ’18: ACM SIG-
COMM 2018 Conference, August 20–25, 2018, Budapest, Hungary.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 978-1-4503-5567-4/18/08...$15.00
https://doi.org/10.1145/3230543.3230557
ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3230543.
3230557
1 Introduction
Datacenter networks offer higher bandwidth and lower la-
tency than traditional wide-area networks. However, tradi-
tional endhost networking stacks, with their high latencies
and substantial CPU overhead, have limited the extent to
which applications can make use of these characteristics.
As a result, several large datacenters have recently adopted
RDMA, which bypasses the traditional networking stacks in
favor of direct memory accesses.
RDMA over Converged Ethernet (RoCE) has emerged as
the canonical method for deploying RDMA in Ethernet-based
datacenters [23, 38]. The centerpiece of RoCE is a NIC that
(i) provides mechanisms for accessing host memory with-
out CPU involvement and (ii) supports very basic network
transport functionality. Early experience revealed that RoCE
NICs only achieve good end-to-end performance when run
over a lossless network, so operators turned to Ethernet’s
Priority Flow Control (PFC) mechanism to achieve minimal
packet loss. The combination of RoCE and PFC has enabled
a wave of datacenter RDMA deployments.
However, the current solution is not without problems. In
particular, PFC adds management complexity and can lead to
significant performance problems such as head-of-the-line
blocking, congestion spreading, and occasional deadlocks
[23, 24, 35, 37, 38]. Rather than continue down the current
path and address the various problems with PFC, in this
paper we take a step back and ask whether it was needed
in the first place. To be clear, current RoCE NICs require a
lossless fabric for good performance. However, the question
we raise is: can the RoCE NIC design be altered so that we no
longer need a lossless network fabric?
We answer this question in the affirmative, proposing a
new design called IRN (for Improved RoCE NIC) that makes
two incremental changes to current RoCE NICs (i) more ef-
ficient loss recovery, and (ii) basic end-to-end flow control
to bound the number of in-flight packets (§3). We show, via
extensive simulations on a RoCE simulator obtained from
a commercial NIC vendor, that IRN performs better than
current RoCE NICs, and that IRN does not require PFC to
achieve high performance; in fact, IRN often performs better
without PFC (§4). We detail the extensions to the RDMA pro-
tocol that IRN requires (§5) and use comparative analysis and
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Mittal et al.
FPGA synthesis to evaluate the overhead that IRN introduces
in terms of NIC hardware resources (§6). Our results suggest
that adding IRN functionality to current RoCE NICs would
add as little as 3-10% overhead in resource consumption, with
no deterioration in message rates.
A natural question that arises is how IRN compares to
iWARP? iWARP [33] long ago proposed a similar philosophy
as IRN: handling packet losses efficiently in the NIC rather
than making the network lossless. What we show is that
iWARP’s failing was in its design choices. The differences
between iWARP and IRN designs stem from their starting
points: iWARP aimed for full generality which led them to put
the full TCP/IP stack on the NIC, requiring multiple layers
of translation between RDMA abstractions and traditional
TCP bytestream abstractions. As a result, iWARP NICs are
typically far more complex than RoCE ones, with higher
cost and lower performance (§2). In contrast, IRN starts with
the much simpler design of RoCE and asks what minimal
features can be added to eliminate the need for PFC.
More generally: while the merits of iWARP vs. RoCE has
been a long-running debate in industry, there is no conclu-
sive or rigorous evaluation that compares the two architec-
tures. Instead, RoCE has emerged as the de-facto winner in
the marketplace, and brought with it the implicit (and still
lingering) assumption that a lossless fabric is necessary to
achieve RoCE’s high performance. Our results are the first
to rigorously show that, counter to what market adoption
might suggest, iWARP in fact had the right architectural
philosophy, although a needlessly complex design approach.
Hence, one might view IRN and our results in one of two
ways: (i) a new design for RoCE NICs which, at the cost of a
few incremental modifications, eliminates the need for PFC
and leads to better performance, or, (ii) a new incarnation of
the iWARP philosophy which is simpler in implementation
and faster in performance.
2 Background
We begin with reviewing some relevant background.
2.1 Infiniband RDMA and RoCE
RDMA has long been used by the HPC community in special-
purpose Infiniband clusters that use credit-based flow control
to make the network lossless [4]. Because packet drops are
rare in such clusters, the RDMA Infiniband transport (as
implemented on the NIC) was not designed to efficiently
recover from packet losses. When the receiver receives an
out-of-order packet, it simply discards it and sends a negative
acknowledgement (NACK) to the sender. When the sender
sees a NACK, it retransmits all packets that were sent after
the last acknowledged packet (i.e., it performs a go-back-N
retransmission).
To take advantage of the widespread use of Ethernet in
datacenters, RoCE [5, 9] was introduced to enable the use of
RDMA over Ethernet.1 RoCE adopted the same Infiniband
transport design (including go-back-N loss recovery), and
the network was made lossless using PFC.
2.2 Priority Flow Control
Priority Flow Control (PFC) [6] is Ethernet’s flow control
mechanism, in which a switch sends a pause (or X-OFF)
frame to the upstream entity (a switch or a NIC), when the
queue exceeds a certain configured threshold. When the
queue drains below this threshold, an X-ON frame is sent to
resume transmission. When configured correctly, PFC makes
the network lossless (as long as all network elements remain
functioning). However, this coarse reaction to congestion
is agnostic to which flows are causing it and this results in
various performance issues that have been documented in
numerous papers in recent years [23, 24, 35, 37, 38]. These
issues range from mild (e.g., unfairness and head-of-line
blocking) to severe, such as “pause spreading” as highlighted
in [23] and even network deadlocks [24, 35, 37]. In an attempt
to mitigate these issues, congestion control mechanisms have
been proposed for RoCE (e.g., DCQCN [38] and Timely [29])
which reduce the sending rate on detecting congestion, but
are not enough to eradicate the need for PFC. Hence, there is
now a broad agreement that PFC makes networks harder to
understand and manage, and can lead to myriad performance
problems that need to be dealt with.
2.3 iWARP vs RoCE
iWARP [33] was designed to support RDMA over a fully
general (i.e., not loss-free) network. iWARP implements the
entire TCP stack in hardware along with multiple other lay-
ers that it needs to translate TCP’s byte stream semantics to
RDMA segments. Early in our work, we engaged with mul-
tiple NIC vendors and datacenter operators in an attempt
to understand why iWARP was not more broadly adopted
(since we believed the basic architectural premise underlying
iWARP was correct). The consistent response we heard was
that iWARP is significantly more complex and expensive
than RoCE, with inferior performance [13].
We also looked for empirical datapoints to validate or
refute these claims. We ran RDMA Write benchmarks on
two machines connected to one another, using Chelsio T-
580-CR 40Gbps iWARP NICs on both machines for one set of
experiments, and Mellanox MCX416A-BCAT 56Gbps RoCE
NICs (with link speed set to 40Gbps) for another. Both NICs
had similar specifications, and at the time of purchase, the
iWARP NIC cost $760, while the RoCE NIC cost $420. Raw
1We use the term RoCE for both RoCE [5] and its successor RoCEv2 [9]
that enables running RDMA, not just over Ethernet, but also over IP-routed
networks.
Revisiting Network Support for RDMA
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
NIC
Chelsio T-580-CR (iWARP)
Mellanox MCX416A-BCAT (RoCE)
Throughput
3.24 Mpps
14.7 Mpps
Latency
2.89 µs
0.94 µs
Table 1: An iWARP and a RoCE NIC’s raw perfor-
mance for 64B RDMA Writes on a single queue-pair.
NIC performance values for 64 bytes batched Writes on a
single queue-pair are reported in Table 1. We find that iWARP
has 3× higher latency and 4× lower throughput than RoCE.
These price and performance differences could be attrib-
uted to many factors other than transport design complexity
(such as differences in profit margins, supported features and
engineering effort) and hence should be viewed as anecdotal
evidence as best. Nonetheless, they show that our conjecture
(in favor of implementing loss recovery at the endhost NIC)
was certainly not obvious based on current iWARP NICs.
Our primary contribution is to show that iWARP, some-
what surprisingly, did in fact have the right philosophy: ex-
plicitly handling packet losses in the NIC leads to better
performance than having a lossless network. However, effi-
ciently handling packet loss does not require implementing
the entire TCP stack in hardware as iWARP did. Instead, we
identify the incremental changes to be made to current RoCE
NICs, leading to a design which (i) does not require PFC yet
achieves better network-wide performance than both RoCE
and iWARP (§4), and (ii) is much closer to RoCE’s implemen-
tation with respect to both NIC performance and complexity
(§6) and is thus significantly less complex than iWARP.
3 IRN Design
We begin with describing the transport logic for IRN. For
simplicity, we present it as a general design independent of
the specific RDMA operation types. We go into the details
of handling specific RDMA operations with IRN later in §5.
Changes to the RoCE transport design may introduce over-
heads in the form of new hardware logic or additional per-
flow state. With the goal of keeping such overheads as small
as possible, IRN strives to make minimal changes to the RoCE
NIC design in order to eliminate its PFC requirement, as op-
posed to squeezing out the best possible performance with a
more sophisticated design (we evaluate the small overhead
introduced by IRN later in §6).
IRN, therefore, makes two key changes to current RoCE
NICs, as described in the following subsections: (1) improv-
ing the loss recovery mechanism, and (2) basic end-to-end
flow control (termed BDP-FC) which bounds the number
of in-flight packets by the bandwidth-delay product of the
network. We justify these changes by empirically evaluat-
ing their significance, and exploring some alternative design
choices later in §4.3. Note that these changes are orthogonal
to the use of explicit congestion control mechanisms (such
as DCQCN [38] and Timely [29]) that, as with current RoCE
NICs, can be optionally enabled with IRN.
3.1 IRN’s Loss Recovery Mechanism
As discussed in §2, current RoCE NICs use a go-back-N loss
recovery scheme. In the absence of PFC, redundant retrans-
missions caused by go-back-N loss recovery result in signifi-
cant performance penalties (as evaluated in §4). Therefore,
the first change we make with IRN is a more efficient loss re-
covery, based on selective retransmission (inspired by TCP’s
loss recovery), where the receiver does not discard out of
order packets and the sender selectively retransmits the lost
packets, as detailed below.