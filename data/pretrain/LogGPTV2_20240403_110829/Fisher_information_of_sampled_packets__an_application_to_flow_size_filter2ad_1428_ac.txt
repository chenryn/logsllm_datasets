### Optimized Text

Let \( p = 0.01 \). From equation (10), we have \((G)_i = \theta_2^2 - 1\). Let \(\beta^{-1}\) be the inverse of the function \(\beta\). Equation (9) yields:

\[
J(\beta^{-1}(\theta_1)) = -\mathbf{e}_1 \mathbf{e}_1^T \left( \frac{\theta_2}{\theta_2 - 1} \right)
\]

where \(\mathbf{e}_j = (b_{1,j}, b_{2,j})\). For a SYN-sampled flow, let \( j \) denote the number of sampled packets. Then, \( b_{1,1} = 1 \), \( b_{1,2} = 0 \), \( b_{2,1} = 0.99 \), and \( b_{2,2} = 0.01 \). The inverse of the Fisher information \( I^{-1} \) (equation (11)) for one sampled flow is:

\[
I^{-1} = \begin{pmatrix}
-1078 & 1078 \\
1078 & -1078
\end{pmatrix}
\]

Assuming \( n \) flows are sampled, the lower bound on the mean squared error of estimates \(\tilde{\gamma}_1\) and \(\tilde{\gamma}_2\) using the Cramér-Rao bound will be \( E[(\gamma_1 - \tilde{\gamma}_1)^2] \geq \frac{1078}{n} \) and \( E[(\gamma_2 - \tilde{\gamma}_2)^2] \geq \frac{1078}{n} \). The Cramér-Rao bound for parameters \(\theta\) comes from the delta method as seen in Section 3.2. Matrix \( H \) is:

\[
H = \begin{pmatrix}
0.105 & 0 \\
0 & 0.105
\end{pmatrix}
\]

### 3.3.2 Essential Information from TCP Sequence Numbers

Consider the problem of estimating flow size distribution using packet counts, and SYN and sequence number information as defined in Section 2.2. The data processing inequality [17] states that adding information can only increase the Fisher information. Thus, an efficient estimator using extra information should perform better, or at least no worse, than an efficient estimator that does not use the extra information. This holds because one can always discard the extra information within the estimator.

For our next example, assume a maximum flow size \( W = 4 \) and \(\theta = (0.56, 0.08, 0.18, 0.18)\). The elements of matrix \( B \) are \( b_{i,1} = (1 - p)^{i-1} \) and \( b_{i,j} = p(1 - p)^{i-j} \) for \( j > 1 \). We compute the Cramér-Rao bound for a sampling rate of \( p = \frac{1}{100} \). Also, consider the estimation using packet counts over SYN flows (SYN-pktct) as defined in Section 2.2. Figure 1 shows the Cramér-Rao bound obtained with \( 10^8 \) sampled flows under this scenario. Clearly, the addition of TCP sequence numbers drastically increases the Fisher information of the samples, translating into a much smaller lower bound on estimation error. The graph also shows that the SYN-pktct estimator gathers very little information about the original flow sizes.

Next, we look at an example where the Fisher information is used to determine the number of samples needed by a given estimate error using different types of protocol information.

### 3.4 Minimum Number of Samples Required for High-Quality Estimates

The Fisher information is a powerful tool for adjusting measurement parameters. Through the Cramér-Rao bound, one can assess the minimum number of samples needed to achieve a given error.

In the following example, we use \( W = 20 \) and \( p = \frac{1}{200} \) and calculate the number of samples needed until the best unbiased estimator can achieve a mean standard deviation error of 0.1 for flows of size one. In the experiment, we renormalize the flow size distribution obtained from the Sprint backbone network. The renormalization creates a distribution \(\theta\) that is a rescaled true Internet flow size distribution but with a maximum flow size \( W \). The original distribution comes from the trace BB-East-1, summarized in Table 3 at the beginning of Section 5. The results show that without any protocol information, only using packet counts (ALL-pktct), the best unbiased estimator needs at least \( 2.25 \times 10^{16} \) sampled flows. Using SYN-sampled flows and packet counts (SYN-pktct), the best unbiased estimator needs at least \( 3.4 \times 10^{16} \) sampled flows (of which \( 7.5 \times 10^{15} \) are SYN-sampled flows). On the other hand, the best unbiased estimator using SYN flags and TCP sequence numbers (ALL-seq-sflag) needs a dramatically lower number: \( 4 \times 10^4 \) sampled flows. These findings are summarized in Table 2.

Next, we present MLEs for the ALL-pktct, SYN-pktct, SYN-seq, and ALL-seq-sflag estimators. Experimentally, we find that the ALL-seq-sflag MLE is efficient, approaching the Cramér-Rao bound even for a small sample size, \( n = 260,000 \).

Thus, from (13), the mean squared error of any unbiased estimates \(\tilde{\theta}_1\) and \(\tilde{\theta}_2\) of \(\theta_1\) and \(\theta_2\) respectively are: \( E[(\theta_1 - \tilde{\theta}_1)^2] \geq \frac{1092}{n} \) and \( E[(\theta_2 - \tilde{\theta}_2)^2] \geq \frac{1092}{n} \) for \( n \) sampled flows, given \( n \) is sufficiently large.

### 4. Finding an Optimal Unbiased Estimator

The Maximum Log-Likelihood Estimator (MLE) finds a \(\tilde{\theta}\) that maximizes the log-likelihood of the set of parameters from the sampled data. Under the same regularity conditions required for the Cramér-Rao bound, the MLE is an asymptotically efficient unbiased estimator of \(\gamma\), i.e., its error achieves the Cramér-Rao lower bound as the number of samples tends to infinity. In practice, we do not have a very large number of samples, so we would like it to be efficient using the number of samples typically collected at Tier-1 backbone routers. This section presents MLEs for the models in Section 2. In particular, we show that the ALL-seq-sflag MLE does not require a large number of samples to be unbiased and achieve the Cramér-Rao error lower bound. Additionally, we present a conjugate gradients algorithm for the MLE, which converges faster than the commonly used Expectation Maximization (EM) algorithm.

We estimate the MLE over function \(\alpha(n)\) through the use of penalty functions for the constraints in (6) and (7). Whenever a value of \(\theta\) violates one of the constraints, the likelihood function receives a penalty, forcing the search to remain within the constrained region. To simplify analysis, we generate synthetic sampled flows for the traffic in an idealized fashion. In the first part of this section, we estimate the flow size distribution using only SYN-sampled flows, which does not account for the "noise" introduced by flow-splitting. We will evaluate the complete model with "noise" in Section 5 on an actual trace. Next, we introduce the MLE for our model.

\[
\tilde{\theta} = \arg \max_{\tilde{\theta}} \sum_{j \in L} \ln(B_{\tilde{\theta}}) \cdot \hat{d}_j(n)
\]

subject to \(\sum_i \tilde{\theta}_i = 1\) and \(0 < \tilde{\theta}_i < 1\), for all \(i \in \{1, \ldots, W\}\).

First, we consider the SYN-pktct MLE as proposed in [3]. We analyze the EM algorithm, used in [3] to find a solution of the log-likelihood equation (15). Let \(\hat{D}_{(S,r)}^{(n)}\) denote the number of SYN-sampled flows with label \( r \) sampled packets. Let \(\hat{d}_{(S,r)}^{(n)}\) be the fraction of SYN-sampled flows with \( r \) sampled packets, as defined by (4).

We detail the approach in [3] for completeness. The EM algorithm finds the MLE \(\hat{\theta}^{(n)}\) by successive refinement of previous estimates:

\[
\tilde{\theta}_i^{(k+1)} = \tilde{\theta}_i^{(k)} \sum_{j \in L} \frac{b_{i,j} \hat{d}_{(S,j)}^{(n)}}{\sum_{r=1}^W \tilde{\theta}_r^{(k)} b_{r,j}}
\]

where \(\tilde{\theta}^{(0)}\) is an initial guess of \(\theta\).

Although the EM algorithm is sound, requires no fine-tuning, and guarantees improvement at each step, it can suffer from slow convergence [14]. Specifically, if the parameters \(\theta\) are "poorly separable," the EM algorithm exhibits a slow convergence rate. The term "poorly separable" quantifies the difficulty of distinguishing whether a sample \( j \) came from flow sizes \( i \) or \( i' \), i.e., if \( b_{i,j} \theta_i \approx b_{i',j} \theta_{i'} \). Unfortunately, flow size estimation suffers from this issue.

Instead, we use the method of conjugate gradients [13] to compute a solution to (15). Our conjugate gradients MLE algorithm was implemented with the help of the wnlib library [1].

For the above algorithm to work, we need to provide the matrix \( B \) and the gradient \(\nabla_{\tilde{\theta}} \ln \alpha(n)(\hat{d}^{(n)}; \tilde{\theta})\) conditioned on \(\sum_{i=1}^W \theta_i = 1\). The \( i \)-th component of our gradient is:

\[
\frac{\partial \ln \alpha(n)(\hat{d}^{(n)}; \tilde{\theta})}{\partial \theta_i} = \sum_{j \in L} \frac{b_{i,j} \hat{d}_{(S,j)}^{(n)}}{\sum_{r=1}^W \tilde{\theta}_r b_{r,j}} - 1
\]

The remaining constraints \( 0 < \tilde{\theta}_i < 1 \), for all \( i \in \{1, \ldots, W\} \), are introduced as penalty functions. Like EM, the conjugate gradient algorithm also requires an initial guess \(\tilde{\theta}^{(0)}\). The only requirement for any initial guess is that no flow size can have zero probability.

### 4.2 The Use of TCP Sequence Numbers on SYN-Sampled Flows

Let \( n \) be the number of sampled flows and \( \hat{d}_j^{(n)} \) the number of sampled flows with label \( j \in L \). The likelihood function with respect to parameters \(\theta\), as defined in Section 3.1, is:

\[
\ln \alpha(n)(\hat{d}^{(n)}; \theta) = \sum_{j \in L} \hat{d}_j^{(n)} \ln \left( \sum_{i=1}^W \theta_i b_{i,j} \right)
\]

The following two experiments, with results shown in Figures 2 and 3, were designed to compare the use of various types of information on the MLE accuracy. Let \( W = 50 \) be the maximum flow size and \( p = \frac{1}{200} \) be the packet sampling rate. We use samples from a renormalized flow size distribution obtained from the Sprint backbone network. The renormalized flow size distribution is based on the distribution of the BB-East-1 trace, summarized at the beginning of Section 5 in Table 3. These experiments use \( 10^{12} \) synthetically generated flows that, on average, produce \( 1.8 \times 10^{10} \) sampled flows after packet sampling (of which \( 5 \times 10^9 \) are SYN-sampled flows). The initial value for the MLE optimization is \(\tilde{\theta}_i^{(0)} = \frac{1}{W}\).

In [3], the authors argue that there are not enough SYN flows to find good estimates using the SYN-pktct MLE. In what follows, we consider all sampled flows and show that the best estimator in [3], the "ALL-pktct MLE" according to our nomenclature, also suffers from the same problems as the SYN-pktct MLE. We further show that adding flows without a SYN-sampled packet drastically increases the accuracy of the estimator that uses TCP sequence numbers.

### 4.3 MLEs Using All Sampled Flows

Incorporating SYN flag information for all sampled flows can be done seamlessly in the SYN-seq estimator and even in the SYN-pktct estimator. This extension can potentially increase the accuracy of the ALL-pktct MLE presented in [3]. However, we will restrict this modification to the estimator with TCP sequence numbers, further referred to as the "ALL-seq-sflag estimator." In this section, we compare the ALL-seq-sflag MLE to the ALL-pktct MLE.

**Figure 2:** Flow size distribution estimate obtained with SYN-pktct and SYN-seq MLEs. Obtained using 120 runs with \( 5 \times 10^9 \) SYN-sampled flows each.