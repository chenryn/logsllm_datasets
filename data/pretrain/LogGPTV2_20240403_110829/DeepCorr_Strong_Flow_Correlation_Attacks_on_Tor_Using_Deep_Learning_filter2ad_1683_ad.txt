Note that the lengths of intercepted flows makes a tradeoff be-
tween DeepCorr’s performance and the adversary’s computation
overhead. That is, while a larger flow length improves DeepCorr’s
correlation performance, longer flows impose higher storage and
computation overheads on the traffic correlation adversary. A larger
flow length also increase the adversary’s waiting time in detecting
correlated flows in real-time.
5.6 DeepCorr’s Performance Improves with the
Size of the Training Set
As intuitively expected, DeepCorr’s performance improves when
it uses a larger set of Tor flows during the training phase (i.e.,
DeepCorr learns a better correlation function for Tor with more
training samples). Figure 9 compares DeepCorr’s ROC curve when
trained with different numbers of flows (for all of the experiments,
we use a fixed number of 1,000 flows for testing). The figure confirms
that increasing the size of the training set improves the performance
of DeepCorr. For instance, for a target F P = 10−3, using 1,000
training flows results in T P = 0.56, while using 5,000 flows for
training gives DeepCorr a T P = 0.8. This shows that a resourceful
adversary can improve the accuracy of her flow correlation classifier
by collecting a larger number of Tor flows for training. Note that a
larger training set increases the training time, however the learning
process does not need to repeat frequently as evaluated before.
5.7 DeepCorr Significantly Outperforms the
State-Of-The-Art
In Section 2.2 we overviewed major flow correlation techniques
introduced prior to our work. We perform experiments to compare
DeepCorr’s performance with such prior systems in correlating
Tor flows. Figure 10 compares the ROC curve of DeepCorr to other
systems, in which all the systems are tested on the exact same set
of Tor flows (each flow is at most 300 packets). As can be seen,
DeepCorr significantly outperforms the flow correlation algorithms
used by prior work, as we see a wide gap between the ROC curve of
DeepCorr and other systems. For instance, for a target F P = 10−3,
while DeepCorr achieves a TP of 0.8, previous systems provide
TP rates less than 0.05! This huge improvement comes from the
fact that DeepCorr learns a correlation function tailored to Tor
whereas previous systems use generic statistical correlation metrics
(as introduced in Section 2.2) to link Tor connections.
9
0.40.60.81.0TP0100020003000400050006000Numberofﬂows0.000000.000250.000500.000750.00100FP10−310−210−1FalsePositive0.00.20.40.60.81.0TruePositiveﬂowlength=100ﬂowlength=200ﬂowlength=300ﬂowlength=450RandomGuessFigure 9: DeepCorr’s correlation performance improves
with more training data.
Needless to say, any flow correlation algorithm will improve its
performance by increasing the length of the flows it intercepts for
correlation (equivalently, the traffic volume it collects from each
flow); we showed this in Section 5.5 for DeepCorr. To offer reason-
able accuracies, previous works have performed their experiments
on flows that contain significantly more packets (and more data)
than our experiments. For instance, Sun et al. evaluated the state-of-
the-art RAPTOR [69] in a setting with only 50 flows, and each flow
carries 100MB of data over 5 minutes. This is while in our experi-
ments presented so far, each flow has only 300 packets, which is
equivalent to only ≈ 300 KB of Tor traffic (in contrast to RAPTOR’s
100MB!). To ensure a fair comparison, we evaluate DeepCorr to
RAPTOR in the exact same setup (e.g., 50 flows each 100MB, and
we use the accuracy metric described in Section 4.4). The results
shown in Figure 11 demonstrates DeepCorr’s drastically superior
performance (our results for RAPTOR comply with the numbers
reported by Sun et al. [69]). On the other hand, we show that the
performance gap between DeepCorr and RAPTOR is significantly
wider for shorter flow observations. To show this, we compare
DeepCorr and RAPTOR based on the volume of traffic they inter-
cept from each flow. The results shown in Figure 12 demonstrate
that DeepCorr outperforms significantly, especially for shorter flow
observations. For instance, RAPTOR achieves a 0.95 accuracy af-
ter receiving 100MB from each flow, whereas DeepCorr achieves
an accuracy of 1 with about 3MB of traffic. We see that DeepCorr
is particularly powerful on shorter flow observations. We zoomed
in by comparing RAPTOR and DeepCorr for small number of ob-
served packets, which is shown in Figure 13. We see that DeepCorr
achieves an accuracy of ≈ 0.96 with only 900 packets, in contrast
to RAPTOR’s 0.04 accuracy.
5.8 DeepCorr’s Computational Complexity
In Table 2, we show the time to perform a single DeepCorr correla-
tion in comparison to that of previous techniques (the correlated
10
Figure 10: Comparing DeepCorr’s ROC curve with previ-
ous systems shows an overwhelming improvement over
the state-of-the-art (all the systems are tested on the same
dataset of flows, and each flow is 300 packets).
Figure 11: Comparing DeepCorr to RAPTOR [69] using the
same flow lengths and flow number as the RAPTOR [69] pa-
per.
Table 2: Correlation time comparison with previous tech-
niques
Method
RAPTOR
Cosine
Pearson
DeepCorr
Mutual Information
One correlation time
0.8ms
0.4ms
1ms
0.4ms
2ms
10−310−210−1FalsePositive0.00.20.40.60.81.0TruePositiveSizeoftrainigdata=1000Sizeoftrainingdata=3000Sizeoftrainingdata=5000RandomGuess10−510−410−310−210−1100FalsePositive0.00.20.40.60.81.0TruePositiveDeepCorrMutualInformationRAPTORCosineCorrelationPearsonCorrelationRandomGuess−0.010.000.010.020.030.040.05FalsePositive0.00.20.40.60.81.0TruePositiveDeepCorr5minutesRAPTOR5minutesRandomGuessFigure 14: The network architecture of DeepCorr to detect
stepping stone attacks
Compared to previous correlation techniques, DeepCorr is the
only system that has a training phase. We trained DeepCorr using
a standard Nvidia TITAN X GPU (with 1.5GHz clock speed and
12GB of memory) on about 25,000 pairs of associated flow pairs and
25, 000 × 24, 999 ≈ 6.2 × 108 non-associated flow pairs, where each
flow consists of 300 packets. In this setting, DeepCorr is trained
in roughly one day. Recall that as demonstrated in Section 5.3,
DeepCorr does not need to be re-trained frequently, e.g., only once
every three weeks. Also, a resourceful adversary with better GPU
resources than ours will be able to cut down on the training time.
Figure 12: Comparing the accuracy of DeepCorr and RAP-
TOR [69] for various volumes of data intercepted from each
flow. The RAPTOR values are comparable to Figure 6 of the
RAPTOR paper [69].
5.9 DeepCorr Works in Non-Tor Applications
as Well
While we presented DeepCorr as a flow correlation attack on Tor, it
can be used to correlate flows in other flow correlation applications
as well. We demonstrate this by applying DeepCorr to the problem
of stepping stone attacks [6, 24, 77]. In this setting, a cybercrimi-
nal proxies her traffic through a compromised machine (e.g., the
stepping stone) in order to hide her identity. Therefore, a network
administrator can use flow correlation to match up the ingress and
egress segments of the relayed connections, and therefore trace
back to the cybercriminal. Previous work has devised various flow
correlation techniques for this application [16, 31, 50, 56, 78].
For our stepping stone detection experiments, we used the
2016 CAIDA anonymized data traces [10]. Similar to the previous
works [31, 32, 50] we simulated the network jitter using Laplace
distribution, and modeled packet drops by a Bernoulli distribution
with different rates. We apply DeepCorr to this problem by learning
DeepCorr in a stepping stone setting. As the noise model is much
simpler in this scenario than Tor, we use a simpler neural network
model for DeepCorr for this application. Also, we only use one di-
rection of a bidirectioal connection to have a fair comparison with
previous systems, which all only use one-sided flows. Figure 14
and Table 3 show our tailored neural network and our choices of
parameters, respectively.
Our evaluations show that DeepCorr provides a perfor-
mance comparable to “Optimal” flow correlation techniques of
Houmansadr et al. [31, 32] when network conditions are stable.
However, when the network conditions becomes noisy, DeepCorr
offers a significantly stronger performance in detecting stepping
stone attacks. This is shown in Figure 15, where the communication
network has a network jitter with a 0.005s standard deviation, and
the network randomly drops 1% of the packets.
Figure 13: Comparing DeepCorr to RAPTOR in correlating
short flows.
flows are 300 packets long for all the systems). We see that Deep-
Corr is noticeably slower that previous techniques, e.g., roughly
two times slower than RAPTOR. However, note that since all the
systems use the same length of flows, DeepCorr offers drastically bet-
ter correlation performance for the same time overhead; for instance,
based on Figure 10, we see that DeepCorr offers a TP≈ 0.9 when all
previous systems offer a TP less than 0.2. Therefore, when all the
systems offer similar accuracies (e.g., each using various lengths
of input flows) DeepCorr will be faster than all the systems for
the same accuracy. As an example, each RAPTOR correlation takes
20ms (on much longer flow observations) in order to achieve the
same accuracy as DeepCorr which takes only 2ms—i.e., DeepCorr
is 10 times faster for the same accuracy.
11
0.311.422.533.544.655.766.877.888.9100.0Estimatedﬂowsize(MBytes)0.00.20.40.60.81.0AccuracyDeepCorrRAPTOR3006009001200150018002100240027003000#Packets020406080100Accuracy(%)DeepCorrRAPTORTimeFeatureMax PoolingIPD to entry (Tiu)IPD from exit (Tju)Flatten pi,jFully ConnectedFi,j2×l1×l×k1Table 3: DeepCorr’s parameters optimized for the stepping
stone attack application.
Layer
Convolution Layer 1
Max Pool 1
Fully connected 1
Fully connected 2
Details
Kernel num: 200
Kernel size: (2, 10)
Activation: Relu
Window size: (1,5)
Stride: (1,1)
Stride: (1,1)
Size: 500, Activation: Relu
Size: 100, Activation: Relu
Figure 15: DeepCorr outperforms state-of-the-art stepping
stone detectors in noisy networks (1% packet drop rate).
6 COUNTERMEASURES
While previous work has studied different countermeasures against
flow correlation [33, 47, 53, 58], they remain mostly non-deployed
presumably due to the poor performance of previous flow correla-
tion techniques at large scale [57, 63]. In the following we discuss
two possible countermeasures.
6.1 Obfuscate Traffic Patterns
An intuitive countermeasure against flow correlation (and similar
traffic analysis attacks like website fingerprinting) is to obfuscate
traffic characteristics that are used by such algorithms. Therefore,
various countermeasures have been suggested that modify packet
timings and packet sizes to defeat flow correlation, in particular by
padding or splitting packets in order to modify packet sizes, or by
delaying packets in order to perturb their timing characteristics.
The Tor project, in particular, has deployed various pluggable trans-
ports [58] in order to defeat censorship by nation-states who block
all Tor traffic. Some of these pluggable transports only obfuscate
packet contents [53], some of them obfuscate the IP address of the
Tor relays [45], and some obfuscate traffic patterns [47, 53]. Note
that Tor’s pluggable transports are designed merely for the purpose
of censorship resistance, and they obfuscate traffic only from a
censored client to her first Tor relay (i.e., a Tor bridge). Therefore,
Tor’s pluggable transports are not deployed by any of Tor’s public
relays.
As a possible countermeasure against DeepCorr, we suggest to
deploy traffic obfuscation techniques by all Tor relays (including
the guard and middle relays). We evaluated the impact of several
Tor pluggable transports on DeepCorr’s performance. Currently,
the Tor project has three deployed plugs: meek, obfs3, and obs4. We
evaluated DeepCorr on meek and obfs4 (obfs3 is an older version
of obfs4). We also evaluated two modes of obfs4: one with IAT
mode “on” [52], which obfuscates traffic features, and one with the
IAT mode “off”, which does not obfuscate traffic features. We used
DeepCorr to learn and correlate traffic on these plugs. However, due
to ethical reasons, we collected a much smaller set of flows for these
experiments compared to our previous experiments; this is because
Tor bridges are very scarce and expensive, and we therefore avoided
overloading the bridges.5 Consequently, our correlation results are
very optimistic due to their small training datasets (e.g., a real-
world adversary will achieve much higher correlation accuracies
with adequate training). We browsed 500 websites over obfs4 with
and without the IAT mode on, as well as over meek. We trained
DeepCorr on only 400 flows (300 packets each) for each transport
(in contrast to 25,000 flows in our previous experiments), and tested
on another 100 flows. Table 4 summarizes the results. We see that
meek and obfs4 with IAT=0 provide no protection to DeepCorr; note
that a 0.5 TP is comparable to what we get for bare Tor if trained
on only 400 flows (see Figure 9), therefore we expect correlation
results similar to bare Tor with a larger training set. The results are
intuitive: meek merely obfuscates a bridge’s IP and does not deploy
traffic obfuscation (except for adding natural network noise). Also
obfs4 with IAT=0 solely obfuscates packet contents, but not traffic
features. On the other hand, we see that DeepCorr has a significantly
lower performance in the presence of obfs4 with IAT=1 (again,
DeepCorr’s accuracy will be higher for a real-world adversary who
collects more training flows).
Our results suggest that (public) Tor relays should deploy a traf-
fic obfuscation mechanism like obfs4 with IAT=1 to resist advanced
flow correlation techniques like DeepCorr. However, this is not a
trivial solution due to the increased cost, increased overhead (band-
width and CPU), and reduced QoS imposed by such obfuscation
mechanisms. Even the majority [52] of Obfsproxy Tor bridges run
obfs4 without traffic obfuscation (IAT=0). Therefore, designing an
obfuscation mechanism tailored to Tor that makes the right balance
between performance, cost, and anonymity remains a challenging
problem for future work.
6.2 Reduce An Adversary’s Chances of
Performing Flow Correlation
Another countermeasure against flow correlation on Tor is reducing
an adversary’s chances of intercepting the two ends of many Tor
connections (therefore, reducing her chances of performing flow
correlation). As discussed earlier, recent studies [20, 49, 69] show
that various ASes and IXPs intercept a significant fraction of Tor
5Alternatively, we could set up our own Tor bridges for the experiments. We decided
to use real-world bridges to incorporate the impact of actual traffic loads in our
experiments.
12
0.00000.00020.00040.00060.00080.0010FalsePositive0.00.20.40.60.81.0TruePositiveDeepCorrCosineOptimalRandomGuessTable 4: DeepCorr’s performance if Tor’s pluggable trans-
ports are deployed by the relays (results are very optimistic
due to our small training set, which is for ethical reasons).
Plug name
obfs4 with IAT=0
meek
obfs4 with IAT=1
TP
≈ 0.50
≈ 0.45
≈ .10
FP
0.0005