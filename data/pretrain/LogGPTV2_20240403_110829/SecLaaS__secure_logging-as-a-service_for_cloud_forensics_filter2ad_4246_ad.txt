conﬁgurations: 1% false positive (FP) probability with 5000 items,
and 2% FP with 10000 items. Then, for One-Way accumulator, we
choose 32-bit and 64-bit P,Q, and X.
Figure 8 shows the performance analysis of log insertion includ-
ing generating and updating the proof of log, i.e., the time required
to complete the steps from (b) to (i) of the Figure 3. We found that
for all the accumulators, time increases linearly with the increase of
log size. For the two Bloom ﬁlter conﬁgurations, we noticed nearly
similar time. However, we found a signiﬁcant amount of increase in
time when changed the one-way accumulator from 32-bit to 64-bit.
Figure 8: Performance Analysis of Log Insertion Using Different
Accumulators
Figure 9 illustrates the performance analysis of generating the
Proof of Past Log of a day for different accumulators, i.e., the time
required to complete the steps from (j) to (l) of the ﬁgure 3. For
the two different Bloom ﬁlters, we found nearly constant amount
of time. On the contrary, for the One-Way accumulators, we found
a linear increase in time for different sizes of logs. It is obvious,
because in the later case, we need to compute the identity of each
log entry using the equation 9 which has O(n) time complexity. [13].
Figure 10 presents the time for verifying the validity of each log.
For all of the accumulators, we found nearly constant amount of
time with the increase in log size. However, the time required for
FixedIPs	
Instances	
FloatingIPs	
ﬁxed_ip	
instance_id	
user_id	
…..	
…..	
ﬁxed_ip	
instance_id	
…..	
…..	
020004000600080001000012000Number of Logs0200000400000600000800000100000012000001400000Time (Miliseconds)Bloom filter (1% FP, 5000 element)Bloom filter (2% FP, 10000 element)One-Way Accumulator (32 Bit)One-Way Accumulator (64 Bit)7. DISCUSSION
Our experimental result shows that the Bloom ﬁlter outperforms
the One-Way accumulator for all the tasks: log insertion, PPL gener-
ation, and log veriﬁcation. However, Bloom ﬁlter is a probabilistic
accumulator, which can state about the existence of a log in the PPL
with certain probability. It works with zero false negative probabil-
ity though. On the other hand, One-Way accumulator works with
zero false positive probability. This means, in Bloom ﬁlter there
is still some chance of planting false log information by the CSP
or the investigator, which is not possible in One-Way accumulator.
The later one always ﬁnds a valid log entry with zero false positive
probability. However, we can decrease the false positive probability
of the Bloom ﬁlter by allocating more space to the bit array. For
example, to ensure 1% FP for 10,000 elements we need 91133 bits
or 11.12 KBytes storage and to ensure 0.1% FP for the same number
of elements we need 111945 bits or 13.67 KBytes storage. In our
scheme we use one Bloom ﬁlter for one static IP for each day. If we
have n number of static IP then for 0.1% FP and 10,000 logs we will
require n * 13.67 KBytes storage each day and n*4989.55 KBytes
in one year.
For the One-Way accumulator, proof requires a very small amount
of storage. The 32-bit accumulator requires 19 Bytes for the ﬁnal
accumulator entry, whereas, the 64-bit requires 39 Bytes. However,
to complete the veriﬁcation in O(1) time, we need to pre-compute
the identity of each log record and store it along with the logs. For
a 32-bit accumulator, we require 10 Bytes for each identity and
for the 64-bit the requirement is 20 Bytes. That means, for 10,000
records, we need 97.67 KBytes storage with the 32-bit accumulator
and 195.35 KBytes for the 64-bit accumulator. Hence, we get a
600% increase in storage in one year for the 32-bit accumulator
comparing with the 0.1% FP for 10,000 records. However, this extra
storage can provide us zero false positive probability. Therefore,
we need to choose whether we will go for accommodating higher
storage with the One-Way accumulator or tolerating a little false
positive probability with the Bloom ﬁlter. Moreover, the Bloom
ﬁlter will give us better performance in all the required tasks.
8. RELATED WORKS
As logging information is one of the prime needs in forensic
investigation, several researchers have explored this problem across
multiple dimensions. Marty proposed a log management solution,
which can solve several challenges of logging, discussed in Section 2
[21]. In his solution, after enabling logging on all infrastructure
components to collect logs, he proposed to establish a synchronized,
reliable, bandwidth efﬁcient, and encrypted transport layer to trans-
fer log from the source to a central log collector. Final step deals
with ensuring the presence of the desired information in the logs.
The proposed guideline tells us to focus on when to log, what to
log, and how to log. The answer of when to log depends on the use-
cases, such that business relevant logging, operations based logging,
security (forensics) related logging, and regulatory and standards
mandates. At minimum, he suggested to log the time-stamps record,
application, user, session ID, severity, reason, and categorization,
so that we can get the answer of what, when, who, and why (4 W).
However, this work does not provide any solution for logging net-
work usage, ﬁle metadata, process usage, and many other important
sources of evidence.
As a solution of forensic investigation, Zafarullah et al. proposed
logging provided by OS and the security logs [32]. In order to
investigate the digital forensics in cloud, they set up cloud com-
puting environment using Eucalyptus. Using Snort, Syslog, Log
Analyzer (e.g., Sawmill), they were able to monitor the Eucalyptus
Figure 9: Performance Analysis of PPL Generation Using Different
Accumulators
32-bit accumulator is higher than the Bloom ﬁlters and for 64-bit
accumulator the time is signiﬁcantly higher than its counterparts.
To identify the performance degradation of NC for storing log,
we ﬁrst run a RSA encryption on a 16 MB ﬁle for several times and
measure the average execution time without running the snort logger
in NC. At that time, two VMs were running on that NC. Then we
start the snort service and again measure the average execution time
of encryption on the same data. From these two execution times we
measured the performance overhead, which is only 1.6%.
Figure 10: Performance Analysis of Log Veriﬁcation Using Differ-
ent Accumulators
020004000600080001000012000Number of Logs0100020003000400050006000Time (Miliseconds)Bloom filter (1% FP, 5000 element)Bloom filter (2% FP, 10000 element)One-Way Accumulator (32 Bit)One-Way  Accumulator (64 Bit)020004000600080001000012000Number of Logs0500010000150002000025000300003500040000Time (Microseconds)Bloom filter (1% FP, 5000 element)Bloom filter (2% FP, 10000 element)One-Way Accumulator (32 Bit)One-Way Accumulator (64 Bit)behaviour and log all internal and external interaction of Eucalyptus
components. For their experiment, they launched a DDoS attack
from two virtual machine and analyzed bandwidth usage log and
processor usage log to detect the DDoS attack. From the logs in
/var/eucalyptus/jetty-request-05-09-xx ﬁle on Cloud Controller (CC)
machine, it is possible to identify the attacking machine IP, browser
type, and content requested. From these logs, it is also possible to de-
termine the total number of VMs controlled by a single Eucalyptus
user and the VMs communication patterns. Their experiment shows
that if the CSPs come forward to provide better logging mechanism,
cloud forensics will get a better advancement.
To make the network, process and access logs available to the
customer, Bark et al. proposed exposing read-only API by the
CSP [5]. By using these APIs, customer can provide valuable
information to investigator. In PaaS, customers have full control
on their application and can log variety of access information in a
conﬁgurable way. So for PaaS, they proposed a central log server,
where customer can store the log information. In order to protect
log data from possible eavesdropping and altering action, customers
can encrypt and sign the log data before sending it to the central
server. In the same context, Dykstra et al. recommended a cloud
management plane, for using in IaaS model [10]. From the console
panel, customers, as well as investigators can collect VM image,
network, process, database logs, and other digital evidence, which
cannot be collected in other ways.
Secure logging has been discussed in several research works [19,
1, 26]. However, none of these works focus on secure logging in
cloud environment, specially providing secure logging as a service.
Moreover, they did not consider the logger as dishonest. In the threat
model of current secure logging works, researchers consider attacks
on privacy and integrity from external entity. These works do not
consider collusion between different entities. The closest work that
we can relate to our work is a secure logging scheme proposed by
Yavuz et al., which provides public veriﬁability of audit logs for
distributed system [31]. Using their proposed scheme, time required
for logging and veriﬁcation increase with the number of logs. On the
other hand, in our system, time required for log veriﬁcation is almost
constant with number of logs using various types of accumulators.
The solution proposed by Marty provided a guideline for logging
criteria and answered some importation questions, e.g., what are the
information we need to log, how to log and when to log. Zafarullah
et al. showed that it is possible to collect necessary logs from cloud
infrastructure, while Bark et al. and Dykstra et al. proposed for
public API or management console to mitigate the challenges of log
acquisition. However, none of them proposed any scheme of storing
the logs in Cloud and making it available publicly in a secure way.
Dyskstra et al. mentioned that the management console requires an
extra level of trust and the same should hold for APIs. In this paper,
we took the ﬁrst step towards providing a solution to mitigate these
challenges. Combining all the previous solutions and our scheme
will drive towards making the Cloud more forensics friendly.
9. CONCLUSION AND FUTURE WORK
Logs from different sources, e.g., network, process, database are
a crucial source of evidence for forensics investigation. However,
collecting logs from cloud is challenging as we have very little
control over clouds compared to traditional computing systems.
Till now, investigators need to depend on the CSP to collect logs
of different sources. To make the situation even worse, there is
no way to verify whether the CSP is providing correct logs to the
investigators or the investigators presenting valid logs to the court.
Moreover, while providing the logs to the investigators, the CSPs
need to preserve the privacy of the cloud users. Unfortunately,
there has been no solution which can make the logs available to the
investigators and at the same time, can preserve the conﬁdentiality
and integrity of the logs. In this paper, we proposed SecLaaS, which
can be the solution to store and provide logs for forensics purpose
securely. This scheme will allow the CSP to store the logs while
preserving the conﬁdentiality of the cloud users. Additionally, an
auditor can check the integrity of the logs using the Proof of Past
Log PPL and the Log Chain LC. We ran our proposed solution on
OpenStack and found it practically feasible to integrate with the
cloud infrastructure.
Preserving the logs and the proofs of the logs will also increase the
auditability of cloud environment. Using our scheme, it is possible
to store and provide any types of logs from which we can get all
the activities of cloud users. Auditability is a vital issue to make
the cloud compliant with the regulatory acts, e.g., Sarbanes-Oxley
(SOX) [9] or The Health Insurance Portability and Accountability
Act (HIPAA) [7]. Hence, implementing SecLaaS will make the
cloud more compliant with such regulations, leading to widespread
adoption of clouds by major businesses and healthcare organizations.
In future, we will integrate other logs besides the snort log with
our proof-of-concept application. Moreover, we will continue exper-
iment on different accumulators to ﬁnd the best ﬁtted accumulator
algorithm with SecLaaS. And ﬁnally, we will implement SecLaaS
as a module of OpenStack.
Acknowledgment
This research was supported by a Google Faculty Research Award,
the Ofﬁce of Naval Research Grant #N000141210217, the Depart-
ment of Homeland Security Grant #FA8750-12-2- 0254, and by the
National Science Foundation under Grant #0937060 to the Comput-
ing Research Association for the CIFellows Project.
10. REFERENCES
[1] R. Accorsi. On the relationship of privacy and secure remote
logging in dynamic systems. In Security and Privacy in
Dynamic Environments, volume 201, pages 329–339. Springer
US, 2006.
[2] Amazon. Zeus botnet controller. http://aws.amazon.
com/security/security-bulletins/
zeus-botnet-controller/. [Accessed July 5th,
2012].
[3] AWS. Amazon web services. http://aws.amazon.com.
[Accessed July 5th, 2012].
[4] J. Benaloh and M. De Mare. One-way accumulators: A
decentralized alternative to digital signatures. In Advances in
Cryptologyâ ˘A ˇTEUROCRYPTâ ˘A ´Z93, pages 274–285. Springer,
1994.
[5] D. Birk and C. Wegener. Technical issues of forensic
investigatinos in cloud computing environments. Systematic
Approaches to Digital Forensic Engineering, 2011.
[6] B. Bloom. Space/time trade-offs in hash coding with
allowable errors. Communications of the ACM,
13(7):422–426, 1970.
[7] Centers for Medicare and Medicaid Services. The health
insurance portability and accountability act of 1996 (hipaa).
http://www.cms.hhs.gov/hipaa/, 1996. [Accessed
July 5th, 2012].
[8] Clavister. Security in the cloud.
http://www.clavister.com/documents/
resources/white-papers/
clavister-whp-security-in-the-cloud-gb.
pdf. [Accessed July 5th, 2012].
[9] Congress of the United States. Sarbanes-Oxley Act.
http://thomas.loc.gov, 2002. [Accessed July 5th,
2012].
[10] J. Dykstra and A. Sherman. Acquiring forensic evidence from
infrastructure-as-a-service cloud computing: Exploring and
evaluating tools, trust, and techniques. DoD Cyber Crime
Conference, January 2012.
[11] FBI. Annualreport for ﬁscal year 2007. 2008 Regional
Computer Forensics Laboratory Program, 2008. [Accessed
July 5th, 2012].
[12] Gartner. Worldwide cloud services market to surpass $68
billion in 2010. http:
//www.gartner.com/it/page.jsp?id=1389313,
2010. [Accessed July 5th, 2012].
[13] M. Goodrich, R. Tamassia, and J. Hasi´c. An efﬁcient dynamic
and distributed cryptographic accumulator. Information
Security, pages 372–388, 2002.
[14] G. Grispos, T. Storer, and W. Glisson. Calm before the storm:
The challenges of cloud computing in digital forensics.
International Journal of Digital Crime and Forensics
(IJDCF), 2012.
[15] INPUT. Evolution of the cloud: The future of cloud
computing in government. http://iq.govwin.com/
corp/library/detail.cfm?ItemID=8448&cmp=
OTC-cloudcomputingma042009, 2009. [Accessed July
5th, 2012].
[16] K. Kent, S. Chevalier, T. Grance, and H. Dang. Guide to
integrating forensic techniques into incident response. NIST
Special Publication, pages 800–86, 2006.
[17] A. Khajeh-Hosseini, D. Greenwood, and I. Sommerville.
Cloud migration: A case study of migrating an enterprise it
system to iaas. In proceedings of the 3rd International
Conference on Cloud Computing (CLOUD), pages 450–457.
IEEE, 2010.
[18] D. Lunn. Computer forensics–an overview. SANS Institute,
2002, 2000.
[19] D. Ma and G. Tsudik. A new approach to secure logging.
Trans. Storage, 5(1):2:1–2:21, Mar. 2009.
[20] Market Research Media. Global cloud computing market
forecast 2015-2020.
http://www.marketresearchmedia.com/2012/
01/08/global-cloud-computing-market/.
[Accessed July 5th, 2012].
[21] R. Marty. Cloud application logging for forensics. In In
proceedings of the 2011 ACM Symposium on Applied
Computing, pages 178–184. ACM, 2011.
[22] D. Reilly, C. Wren, and T. Berry. Cloud computing: Pros and
cons for computer forensic investigations. 2011.
[23] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage. Hey,
you, get off of my cloud: exploring information leakage in
third-party compute clouds. In Proceedings of the 16th ACM
conference on Computer and communications security, pages
199–212. ACM, 2009.
[24] J. Robbins. An explanation of computer forensics. National
Forensics Center, 774:10–143, 2008.
[25] K. Ruan, J. Carthy, T. Kechadi, and M. Crosbie. Cloud
forensics: An overview. In proceedings of the 7th IFIP
International Conference on Digital Forensics, 2011.
[26] B. Schneier and J. Kelsey. Secure audit logs to support
computer forensics. ACM Trans. Inf. Syst. Secur.,
2(2):159–176, May 1999.
[27] M. Taylor, J. Haggerty, D. Gresty, and R. Hegarty. Digital
evidence in cloud computing systems. Computer Law &
Security Review, 26(3):304–308, 2010.
[28] Tikal. Experimenting with OpenStack Essex on Ubuntu 12.04
LTS under VirtualBox. http://bit.ly/LFsVUY, 2012.
[Accessed November 30th, 2012].
[29] J. Vacca. Computer forensics: computer crime scene
investigation, volume 1. Delmar Thomson Learning, 2005.
[30] J. Wiles, K. Cardwell, and A. Reyes. The best damn
cybercrime and digital forensics book period. Syngress Media
Inc, 2007.
[31] A. Yavuz and P. Ning. Baf: An efﬁcient publicly veriﬁable
secure audit logging scheme for distributed systems. In
Computer Security Applications Conference, 2009. ACSAC
’09. Annual, pages 219 –228, dec. 2009.
[32] Z. Zafarullah, F. Anwar, and Z. Anwar. Digital forensics for
eucalyptus. In Frontiers of Information Technology (FIT),
pages 110–116. IEEE, 2011.