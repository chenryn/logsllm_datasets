# Leveraging Interconnections for Performance

## 3.4 Illustrative Examples

### 3.4.1 Routerless Deployments: Types 1 and 2

A Type 1 deployment provides Akamai with control-plane information about the hosting network. It is generally in the interest of the operators of that network to share detailed information about the prefixes of its end users, the corresponding name servers/resolvers, and to set and share BGP communities to instruct Akamai on how to serve those prefixes.

Consider an example of a large eyeball network that has no downstream customers and hosts 18 different Type 1 deployments. We observe that more than 99% of the prefixes this network shares with Akamai are /32 prefixes tagged with one of two different BGP community attributes. The few remaining prefixes, including aggregations of /32 prefixes, can be found in ViewP. The rationale for sharing such fine-grained information with Akamai is twofold: 
1. This network leverages end-user mapping.
2. By using BGP communities, it signals Akamai's preference over which deployment should serve which end users.

While the lack of downstream customers results in no downstream-related implicit peerings from these 18 deployments for Akamai, examining this network’s upstream connectivity, we find more than 45 upstream-related implicit peerings (leveraged for cache fill but not for serving content to end users on other eyeball networks).

The operators of the networks that host Type 2 deployments typically do not provide any private information but tend to share with Akamai information that they also provide to other networks/customers. However, Type 2 deployments located in selected networks can contribute a large number of unique downstream-related implicit peerings. For example, when examining an actual Type 2 deployment on the network of a large global backbone provider, we find that it sends Akamai more than 668,000 different IPv4 prefixes and more than 43,000 different IPv6 prefixes (i.e., the complete routing table [1]). These prefixes are served through more than 1,500 different ASes, resulting in more than 1,500 downstream-related implicit peerings for Akamai. As a Tier 1 network, this hosting AS contributes no upstream-related implicit peerings.

### 3.4.2 Deployments with a Router: Types 3 and 4

Type 3 deployments are the main contributors to the number of Akamai’s explicit peerings. For example, a single Type 3 deployment at one of the large European IXPs contributes more than 600 explicit peerings.

On the other hand, Type 4 deployments, which are used to connect Akamai with bigger networks in terms of bandwidth (not necessarily footprint) than the majority of networks with which Akamai peers at IXPs, typically contribute fewer explicit peerings than Type 3 deployments. For example, in the case of an actual Type 4 deployment located in the same metro area as the Type 3 deployment, we find that it connects to only seven different networks, including two big cloud providers, two large eyeball and transit providers, two smaller eyeball providers, and one global provider from which Akamai buys transit. As a result, this deployment only contributes seven explicit peerings to Akamai’s connectivity fabric.

### Summary
Akamai’s serving infrastructure consists of EUF delivery clusters in approximately 3,300 deployments across the globe, serving a total of 1.75 million unique IPv4 originating prefixes (plus 97,000 unique IPv6 originating prefixes) in 61,300 ASes. These observed prefixes can be served via a total of 3 million unique AS paths, where prefixes of length /25 or longer are typically only reachable via a single path. The connectivity fabric of Akamai’s serving infrastructure is made up of 6,111 explicit and 28,655 implicit peerings, with the latter consisting of 28,353 downstream-related and 1,506 upstream-related implicit peerings. Importantly, while some of the observed explicit peerings can be recognized in ViewP, none of the implicit peerings are visible in ViewP.

## 4. The Serving Infrastructure of Akamai: Performance

### 4.1 Available Datasets
To study performance-related aspects, we rely on server logs of the HTTP/S sessions between all EUF delivery servers and request-generating clients. These logs contain transport-layer information for a sample of all the HTTP/S sessions. Each server uses a sampling rate of 5% (i.e., 1-in-20 HTTP/S sessions), and for each sampled HTTP/S session, the server logs a record. Among the fields in each record are the IP address of the client, the IP address of the server, the total number of bytes sent to the client, the corresponding transfer time, and a smoothed round-trip time (RTT) value. This value is an estimate of the RTT between the server and the client. Transferring larger objects allows for better estimations of the RTT (more round trip samples). For the purpose of this study, we only consider HTTP/S sessions for objects larger than 300KB. Using the total bytes sent and the corresponding transfer time, we compute the session’s mean throughput and use that value and the smoothed RTT as our metrics-of-choice for quantifying the performance of a session. We obtain one day’s worth of logs for 2017-09-17 and 2018-05-17, the days corresponding to our first and last snapshot of ViewA, respectively. Each of these two logs contains a total of more than 11 billion records. We rule out possible sampling bias in this data by leveraging its substantial size; that is, when examining various randomly chosen subsets, we find that all of Akamai’s deployments with at least one EUF delivery server cluster and more than 90% of the full dataset’s prefixes are present in all the subsets.

Since our analysis of the two log datasets corresponding to the 2017-09-17 and 2018-05-17 snapshots of ViewA produced very similar results, our focus below is on describing the observed findings for 2017-09-17. However, we will also include sample plots (see Figure 7) that explicitly compare the results for the two eight-month-apart snapshots and quantify the observed similarity.

### 4.2 Akamai’s Peering Edge "in Use"
Our analysis in Section 3 of the 2017-09-17 snapshot of ViewA revealed a vast and complex connectivity fabric that Akamai can leverage for serving content. This analysis was strictly control plane-oriented and relied exclusively on BGP information. In the process, we showed that out of the approximately 21 million paths that could be easily discerned from ViewP-like datasets around the time of our analysis, Akamai ignores some 85% of them and only presents the 3.7 million or so best paths to its mapping system. In the following, we leverage Akamai’s 2017-09-17 server log measurements to provide an Akamai-focused data plane perspective of these 3.7 million paths. That is, we are interested in understanding how the large number of identified implicit peerings and smaller number of explicit peerings are used to enable and facilitate Akamai’s content delivery service.

To quantify Akamai’s use of its implicit and explicit peerings, we proceed as follows. For each of the log records, we use the IP address of the server to associate servers with their corresponding deployment. Likewise, we use the IP address of the client to group together records by client AS. Next, we rely on information about the deployments (i.e., deployment type, link information where applicable) and the ViewA data to determine for each record the AS path from the deployment to the client AS. Subsequently, we group all the log records into the following four categories: (i) Onnet (all HTTP/S sessions served from Type 1 deployments), (ii) Transit (all HTTP/S sessions served from Type 2 or via the transit links of Type 3 or Type 4 deployments), (iii) IXP (all HTTP/S sessions served via the IXP links of Type 3 deployments), and (iv) PNI (all HTTP/S sessions served via the PNI links of Type 4 deployments).

This way, we end up with records that are all annotated with attributes indicating deployment, client AS, AS path, and link type or category (i.e., onnet, transit, IXP, and PNI). By examining the AS path of each such annotated record, we can thus identify AS paths that we only see from Type 1 and/or Type 2 deployments (i.e., using an implicit peering), or only from Type 3 and/or Type 4 deployments (i.e., using an explicit peering), or from a combination of Type 1/Type 2 and Type 3/Type 4 deployments (i.e., using both an implicit and explicit peering). In fact, using this information provides an opportunity to infer the likely type of client AS via the expected demand that this client AS generates for Akamai, at least for client ASes that operate in marketplaces with a well-developed Internet infrastructure. For example, seeing an AS path from both implicit and explicit peerings usually indicates that the traffic demand is medium to high, in which case the client AS typically represents either a medium eyeball network that hosts Type 1 deployments and participates in public peering (i.e., peers with Akamai at one or more IXPs) or a large network with many eyeballs that hosts Type 1 deployments and peers privately with Akamai (Type 4 deployments). In a similar fashion, inferences can be drawn when seeing an AS path from implicit peerings only or from explicit peerings only.

Figure 4 summarizes our findings and shows three bars whose height (y-axis) represent the percentage of unique paths seen exclusively from implicit peerings only, from explicit peerings only, and from both implicit and explicit peerings, respectively. By using the width of the bars to encode traffic volume, we observe that the paths that are seen from both implicit and explicit peerings are the fewest in numbers but are responsible for about half of all the traffic served. This result confirms our intuition that the big networks that generate strong demand, which translates into large amounts of traffic being served by Akamai, are well connected to Akamai. At the same time, the largest number of paths is seen by explicit-only peerings, but these paths generate the least amount of traffic. In this case, the explanation is that from peerings at IXPs, Akamai can reach many destinations, but the aggregate demand/traffic is typically low. The paths seen exclusively by implicit peerings occupy a middle ground, both in terms of the number of paths and the volume of traffic generated. Note that this middle ground appeals to small to medium-sized (eyeball) networks, hosting Type 1 deployments while receiving some types of content from their upstream provider(s) or being served by Type 2 deployments exclusively, that generate limited demand/traffic volume.

Finally, Figure 5 shows an extreme case of skewness with respect to the demand generated by the various client ASes or, equivalently, the traffic Akamai is serving to those client ASes. For one, we observe that when considering all paths, some 90% of the overall demand is coming from about 1% of the paths. Similar findings apply when considering all paths seen from implicit-only or explicit-only peerings. Moreover, when looking at only those paths that are seen from both implicit and explicit peerings, we see a slightly less pronounced skewed demand distribution, but recall that the percentage of such paths is small compared to implicit-only or explicit-only (see Figure 4). The skewness of Internet path usage is not new and has been observed in the past [37].

**Note:** Filtering out all requests for objects smaller than 300KB and working with sampled data may impact our findings quantitatively but not qualitatively.