e
b
m
u
N
101
100
Type 1
Type 2
Type 3
Type 4
Figure 3: Implicit peerings by hosting AS (Type 1 and
2) and explicit peerings by deployment (Type 3 and 4).
such interconnections; that is, almost half of all routeable
ASes seen in ViewA are within one AS-hop from Akamai.
Table 5 (right half) gives a breakdown of the observed im-
plicit peerings by deployment type and IP version and shows
an uneven distribution, with almost four times as many im-
plicit peerings for Type 1 deployments (26,429) compared to
Type 2 deployments (7,322). Note, however, that Type 1 de-
ployments are hosted in orders of magnitude more different
host ASes compared to Type 2 deployments. Figure 3 (left
half) shows that the median number of implicit peerings that
Akamai inherits from Type 1 deployments is 10; for Type 2
deployments, the median is close to 1,000, which is consis-
tent with the fact that transit providers (Internet core, Type
2 deployments) are typically well-connected while eyeball
ISPs (Internet edge, Type 1 deployments) only forward (a
subset of) their downstreams to Akamai.
Next, to show that the number of Akamai’s upstream-
related implicit peerings pales in comparison to the observed
28,353 downstream-related implicit peerings, we note that in
general, ViewA does not provide the information needed to
obtain the precise upstream connectivity of those networks
that host Type 1 deployments.4 Instead, we leverage a combi-
nation of ViewA information (e.g., the Type 1 deployment’s
hosting ASes) and ViewP AS path information (e.g., hosting
ASes’ upstream providers) and infer a total of 1,506 unique
upstream-related implicit peerings that Akamai can utilize to
get traffic in or out of its Type 1 and Type 2 deployments (i.e.,
incoming traffic resulting from cache fill requests for Type
1 and Type 2 deployments and outgoing traffic for serving
content for Type 2 deployments).
3.4 Illustrative Examples
3.4.1 Routerless deployments ś Types 1 and 2. A Type 1
deployment provides Akamai with control-plane informa-
tion about the hosting network. In fact, it is generally in
the interest of the operators of that network to share with
Akamai detailed information about the prefixes of its end
4For Type 2 deployments, ViewA provides ground truth with respect to
Akamai’s upstream-related implicit peerings; similar arguments as in the
case of Akamai’s explicit peerings for Type 3 or Type 4 deployments apply.
Leveraging Interconnections for Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
users and the corresponding name servers/resolvers and to
set and share BGP communities to tell Akamai how to serve
those prefixes. Consider an actual example of a large eyeball
network that has no downstream customers and hosts 18
different Type 1 deployments. We observe that more than
99% of the prefixes that this network shares with Akamai
are /32 prefixes tagged with either of two different BGP com-
munity attributes. The few remaining prefixes ś including
aggregations of /32 prefixes ś can be found in ViewP. The
rationale for this eyeball network to share such fine-grained
information with Akamai is twofold. For one, this network
leverages end-user mapping. Moreover, by means of the BGP
communities, it signals Akamai a preference over which de-
ployment should serve which end users. While the lack of
downstream customers results in no downstream-related
implicit peerings from these 18 deployments for Akamai,
examining this network’s upstream connectivity, we find
more than 45 upstream-related implicit peerings (that are
leveraged for cache fill but not for serving content to end
users on other eyeball networks).
The operators of the networks that host Type 2 deploy-
ments typically do not provide any private information but
tend to share with Akamai information that they also provide
to other networks/customers. However, Type 2 deployments
located in selected networks can contribute a large number of
unique downstream-related implicit peerings. For example,
when examining an actual Type 2 deployment on the net-
work of a large global backbone provider, we find that it sends
Akamai more than 668k different IPv4 prefixes and more
than 43k different IPv6 prefixes (i.e., the complete routing
table [1]). Those prefixes are served through more than 1,500
different ASes, resulting in more than 1,500 downstream-
related implicit peerings for Akamai. At the same time, as
a Tier 1 network, this hosting AS contributes no upstream-
related implicit peerings.
3.4.2 Deployments with a router ś Types 3 and 4. On the
one hand, Type 3 deployments are the main contributors to
the number of Akamai’s explicit peerings. For example, a
single Type 3 deployment at one of the large European IXPs
contributes more than 600 explicit peerings.
On the other hand, since Type 4 deployments tend to be
used to connect Akamai with bigger networks in terms of
bandwidth (not necessarily footprint) than the majority of
networks with which Akamai peers at IXPs, they typically
contribute fewer explicit peerings than Type 3 deployments.
For example, in the case of an actual Type 4 deployment that
is located in the same metro area as the Type 3 deployment
we just considered, we find that it connects to only seven
different networks that include two big cloud providers,
two large eyeball and transit providers, two smaller eye-
ball providers, and one global provider from which Akamai
buys transit. As a result, this deployment only contributes
seven explicit peerings to Akamai’s connectivity fabric.
Summary: The footprint of Akamai’s serving infrastructure
consists of EUF delivery clusters in some 3.3k deployments
across the globe that are used to serve a total of 1.75M unique
IPv4 originating prefixes (plus 97k unique IPv6 originating
prefixes) in 61.3k ASes. These observed prefixes can be served
via a total of 3M unique AS paths, where prefixes of length
/25 or longer are typically only reachable via a single path.
The connectivity fabric of Akamai’s serving infrastructure is
made up of 6,111 explicit and 28,655 implicit peerings where the
latter consist of 28,353 downstream-related and 1,506 upstream-
related implicit peerings. Importantly, while some of the ob-
served explicit peerings can be recognized in ViewP, none of
the implicit peerings are visible in ViewP.
4 THE SERVING INFRASTRUCTURE OF
AKAMAI: PERFORMANCE
We show in this section how Akamai utilizes its connectivity
fabric to serve content to end users worldwide and examine
the performance (e.g., RTT, throughput) that this content
experiences as it traverses Akamai’s edge.
4.1 Available Datasets
To study performance-related aspects, we rely on server logs
of the HTTP/S sessions between all EUF delivery servers
and request-generating clients. These logs contain transport-
layer information for a sample of all the HTTP/S sessions.
Each server uses a sampling rate of 5% (i.e., 1-in-20 HTTP/S
sessions) and for each sampled HTTP/S session, the server
logs a record. Among the fields in each record are the IP
address of the client, the IP address of the server, the total
number of bytes sent to the client, the corresponding transfer
time, and a smoothed round-trip time (RTT) value. This value
is an estimate of the RTT between the server and the client.
Transferring larger objects allows for better estimations of
the RTT (more round trip samples). For the purpose of this
study, we only consider HTTP/S sessions for objects larger
than 300KB. Using the total bytes sent and the corresponding
transfer time, we compute the session’s mean throughput
and use that value and the smoothed RTT as our metrics-
of-choice for quantifying the performance of a session. We
obtain one day’s worth of logs for 2017-09-17 and 2018-05-
17, the days corresponding to our first and last snapshot
of ViewA, respectively. Each of these two logs contains a
total of more than 11 billion records. We rule out possible
sampling bias in this data by leveraging its substantial size;
that is, when examining various randomly chosen subsets,
we find that all of Akamai’s deployments with at least one
214
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
F. Wohlfart, N. Chatzis, C. Dabanoglu, G. Carle, W. Willinger
EUF delivery server cluster and more than 90% of the full
dataset’s prefixes are present in all the subsets.
Since our analysis of the two log datasets corresponding
to the 2017-09-17 and 2018-05-17 snapshots of ViewA pro-
duced very similar results, our focus below is on describing
the observed findings for 2017-09-17. However, we will also
include sample plots (see Figure 7) that explicitly compare
the results for the two eight months-apart snapshots and
quantify the observed similarity.
4.2 Akamai’s Peering Edge łin Usež
Our analysis in Section 3 of the 2017-09-17 snapshot of ViewA
revealed a vast and complex connectivity fabric that Akamai
can leverage for serving content. This analysis was strictly
control plane-oriented and relied exclusively on BGP in-
formation. In the process, we showed that out of the ap-
proximately 21M paths that could be easily discerned from
ViewP-like datasets around the time of our analysis, Akamai
ignores some 85% of them and only presents the 3.7M or
so best paths to its mapping system. In the following, we
leverage Akamai’s 2017-09-17 server log measurements to
provide an Akamai-focused data plane perspective of these
3.7M paths. That is, we are interested in understanding how
the large number of identified implicit peerings and smaller
number of explicit peerings are used to enable and facilitate
Akamai’s content delivery service.
To quantify Akamai’s use of its implicit and explicit peer-
ings, we proceed as follows. For each of the log records, we
use the IP address of the server to associate servers with
their corresponding deployment. Likewise, we use the IP
address of the client to group together records by client AS.
Next, we rely on information about the deployments (i.e.,
deployment type, link information where applicable) and the
ViewA data to determine for each record the AS path from
the deployment to the client AS. Subsequently, we group all
the log records into the following four categories: (i) Onnet
(all HTTP/S sessions served from Type 1 deployments), (ii)
Transit (all HTTP/S sessions served from Type 2 or via the
transit links of Type 3 or Type 4 deployments), (iii) IXP (all
HTTP/S sessions served via the IXP links of Type 3 deploy-
ments), and (iv) PNI (all HTTP/S sessions served via the PNI
links of Type 4 deployments).
This way, we end up with records that are all annotated
with attributes indicating deployment, client AS, AS path,
and link type or category (i.e., onnet, transit, IXP, and PNI).
By examining the AS path of each such annotated record,
we can thus identify AS paths that we only see from Type 1
and/or Type 2 deployments (i.e., using an implicit peering),
or only from Type 3 and/or Type 4 deployments (i.e., using
an explicit peering), or from a combination of Type 1/Type 2
and Type 3/Type 4 deployments (i.e., using both an implicit
and explicit peering). In fact, using this information provides
an opportunity to infer the likely type of client AS via the
expected demand that this client AS generates for Akamai,
at least for client ASes that operate in marketplaces with a
well-developed Internet infrastructure. For example, seeing
an AS path from both implicit and explicit peerings usually
indicates that the traffic demand is medium to high in which
case the client AS typically represents either a medium eye-
ball network that hosts Type 1 deployments and participates
in public peering (i.e., peers with Akamai at one or more
IXPs) or a large network with many eyeballs that hosts Type
1 deployments and peers privately with Akamai (Type 4 de-
ployments). In a similar fashion, inferences can be drawn
when seeing an AS path from implicit peerings only or from
explicit peerings only.
Figure 4 summarizes our findings and shows three bars
whose height (y-axis) represent the percentage of unique
paths seen exclusively from implicit peerings only, from
explicit peerings only, and from both implicit and explicit
peerings, respectively. By using the width of the bars to en-
code traffic volume, we observe that the paths that are seen
from both implicit and explicit peerings are the fewest in
numbers but are responsible for about half of all the traffic
served. This result confirms our intuition that the big net-
works that generate strong demand which translates into
large amounts of traffic being served by Akamai are well con-
nected to Akamai. At the same time, the largest number of
paths is seen by explicit-only peerings, but these paths gener-
ate the least amount of traffic. In this case, the explanation is
that from peerings at IXPs, Akamai can reach many destina-
tions but the aggregate demand/traffic is typically low. The
paths seen exclusively by implicit peerings occupy a middle
ground, both in terms of number of paths and volume of
traffic generated. Note that this middle ground appeals to
small to medium-sized (eyeball) networks, hosting Type 1 de-
ployments while receiving some types of content from their
upstream provider(s) or being served by Type 2 deployments
exclusively, that generate limited demand/traffic volume.
Finally, Figure 5 shows an extreme case of skewness with
respect to the demand generated by the various client ASes
or, equivalently, the traffic Akamai is serving to those client
ASes.5 For one, we observe that when considering all paths,
some 90% of the overall demand is coming from about 1% of
the paths. Similar findings apply when considering all paths
seen from implicit-only or explicit-only peerings. Moreover,
when looking at only those paths that are seen from both im-
plicit and explicit peerings, we see a slightly less pronounced
skewed demand distribution but recall that the percentage
of such paths is small compared to implicit-only or explicit-
only (see Figure 4). The skewness of Internet path usage is
not new and has been observed in the past [37].
5Filtering out all requests for objects smaller than 300KB and working with
sampled data may impact our findings quantitatively but not qualitatively.
215
Leveraging Interconnections for Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
s
h
t
a
p
e
u
q
n
u
i
f
o
e
g
a
t
n
e
c
r
e
P
60
50
40
30
20
10
0
Bar width proportional to traffic volume.
Implicit only Expl. only
Implicit and explicit
100
c
i
f
f
a
r
t
f
o
.
c
r
e
p
m
u
s
m
u
C
80
60
40
20
0
All
Implicit and explicit
Implicit only
Explicit only
0.0
0.2
0.4
0.6
0.8
1.0
c
i
f
f
a
r
t