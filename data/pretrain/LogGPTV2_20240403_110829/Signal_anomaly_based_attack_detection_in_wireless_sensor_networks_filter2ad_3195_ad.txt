parameters for the current training set.
4) Model Construction: Model construction occurs next.
The parameters determined previously are used in the con-
struction of the model. If a batch update occurs, the previous
model is discarded and the model for the new training data is
constructed. For an incremental update, the previous model,
model(t− 1) and the n new data vectors are used to construct
a new model, model(t).
5) Anomaly Detection: The new model, model(t),
is
used as the anomaly detector for data. The data vectors
Xt+1, Xt+2, Xt+3, ... generated from the process form the
testing data set and are labelled as either normal or anomaly.
An assumption is made that the testing data set is drawn from
the same data distribution as the training data set.
C. Evaluation of Anomaly Detection Techniques
In order to evaluate anomaly detection techniques, several
metrics are deﬁned. An anomaly detection algorithm will
classify a data vector formed of sensor measurements as either
normal or anomaly. Comparing the assigned labels to the
ground-truth labels, a false positive is deﬁned as a normal
data vector incorrectly labelled as an anomaly, a true positive
is deﬁned as an anomaly correctly identiﬁed. From this, two
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.O’REILLY et al.: ANOMALY DETECTION IN WIRELESS SENSOR NETWORKS IN A NON-STATIONARY ENVIRONMENT
9
Schedule Model Update
Constant Update (cid:884) Assumption that Data Distribution is Non-Stationary
Non-Stationary Process
Data Vectors
)
1
(
X
)
2
(
X
)
3
(
X
Measurements
Sensor 
...
Change 
Detection
)
t
(
X
1
)
1
+
t
(
X
Trigger Model Update
Detect and Retrain - Monitor Data Distribution for Changes
Model Update
i
g
n
n
a
r
T
i
t
e
S
Fixed 
Parameters
Optimized 
Parameters
Model 
Construction
Batch Update
4
Incremental 
Update
Model(t)
Labelled 
Data
Training Set 
Formation
2
Model Selection
3
Model(t-1)
Data(t)
Anomaly 
Detection
5
Fig. 5. Workﬂow detailing the different aspects of anomaly detection in a non-stationary environment in a WSN.
rates in the form of ratios can be deﬁned. The false positive
rate (FPR) is computed as the ratio of false positives to normal
measurements and the true positive rate (TPR), also known as
the detection rate, is the ratio of true positives to anomalous
measurements.
There is a trade-off between the TPR and the FPR where
adjusting a parameter, such as a threshold, to increase the TPR
will result in an increase to the FPR. To examine this trade-
off, a receiver operating characteristic (ROC) curve is used. A
ROC curve, Fig. 6, is generated by varying a parameter, such
as the anomaly rate threshold. The resulting FPR and TPR
form the ROC curve. Perfect performance is achieved when
there is a TPR of 1 and an FPR of 0. Performance equivalent
to the random assignment of the normal and anomaly labels to
the data is achieved when the TPR equals the FPR. The larger
the area under the ROC curve, the better the performance of
the anomaly detection algorithm.
In addition to examining the trade-off between the FPR
and TPR, it is also necessary to compare the sensitivity of
an anomaly detection technique to parameter selection. The
area under ROC curve (AUC) [52] is used as a measurement
of the performance of the scheme and is computed for a
given ROC by calculating the area under the ROC curve.
An AUC value of 1 indicates that the scheme has achieved
100% accuracy and an AUC value of less than 0.5 indicates
that the performance is worse than the random assignment of
the labels. By varying a parameter in the anomaly detection
scheme, a plot of parameter versus AUC value provides a
method to analyze sensitivity to parameter selection.
D. Complexity Analysis
Due to the resource constraints of a WSN it is necessary to
examine the complexity of anomaly detection algorithms in or-
der to determine computational, memory and communication
complexity. A common technique used for an evaluation of
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
1
0.8
0.6
0.4
0.2
0
0
Perfect Performance
Intermediary Performance
Random Label Assignment
0.2
0.4
0.6
0.8
1
False Positive Rate
Fig. 6. ROC Curve: An illustration of the ROC space and the performance
of an anomaly detector.
this is big O notation. The aim is to determine the upper bound
of the complexity of the algorithm. In the application domain
of WSNs, it is usually used to examine how the complexity
alters as the number (and possibly dimension) of the data
vectors used in model construction increases.
V. CHANGE DETECTION
Due to scarce energy resources in a WSN it is important to
balance the trade-off between accuracy and energy use. Issues
such as a computationally expensive classiﬁer update can be
managed more effectively by determining when they should
occur. For an anomaly detection algorithm to operate in a non-
stationary environment, determining when change is occurring
is important to avoid unnecessary updates.
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.10
IEEE COMMUNICATIONS SURVEYS & TUTORIALS, ACCEPTED FOR PUBLICATION
Change detection can be categorized into two approaches. In
one approach an assumption is made that the data distribution
is non-stationary, avoiding the complexity of attempting to
detect it, with updates being performed at regular intervals.
In the second approach the data distribution is monitored to
detect change, with an update to a model occurring when it
is detected. These two approaches are termed constant update
and detect and retrain [51].
A. Constant Update
One method of handling change detection is to presume that
the data are generated from a non-stationary distribution and
therefore to reconstruct the model at regular intervals. This
simpliﬁes matters in that no detection technique is required.
However, if the timescale of the reconstruction is smaller than
the timescale of the change in the non-stationary distribution,
unnecessary updates are performed to the model. This could
be costly in terms of computation. Conversely, if the timescale
for model reconstruction is greater than the change in the data
distribution, a decrease in the ability of the model to detect
anomalies can occur.
Techniques for anomaly detection in WSNs often use a
sliding window in order to frame data to be used as a training
set. The most common manner in which it is determined
when an update should occur is using constant update where
periodically a ﬁxed sized window advances to include an addi-
tional number of new data instances, while the corresponding
number of data instances are removed from the window.
An algorithm that uses constant update in order to adapt to
a non-stationary distribution is detailed by Zhang et al. [47].
The anomaly detector used in the scheme is the one-class
quarter-sphere support vector machine (QSSVM) [53], a re-
duced computationally complex form of the one-class SVM
(OC-SVM) [45]. The classiﬁer performs anomaly detection
by centering the data in feature space and enclosing normal
data with a hypersphere. Fixing the hypersphere at the origin
reduces the complexity so that the solution of a linear pro-
gramme, rather than a quadratic, is required. The solution of
the linear programme is O(n3) [54] where n is the number
of data vectors in the training set. Test data vectors lying
within the QSSVM are classiﬁed as normal, whereas those
lying outside are labelled as anomalies. The use of kernel
methods allows the classiﬁer to derive non-linear boundaries.
The algorithm proposed by Zhang et al. [47] uses a sliding
window approach to enable its use with non-stationary data
sets. The algorithm adapts to non-stationary data by the use
of two schemes involving a constant update. The ﬁrst scheme
uses a ﬁxed size sliding window which progresses with each
data measurement, adding the new data vector and removing
the oldest data vector from the sliding window. The second
scheme operates in the same manner as the ﬁrst, but only
updates the window every n new measurements. .
The algorithm is updated [55] using the centred hyperellip-
soidal support vector machine (CESVM) [31]. The CESVM
uses the Mahalanobis distance which takes into account
attribute correlation whereas the QSSVM ignores attribute
correlation [55]. The CESVM is resource intensive and is
adapted to the resource constraints of a WSN by transforming
data to a symmetric distribution in input space using the Box-
Cox method [56]. The symmetric distribution is scale-invariant
so data vectors are centred by subtracting the median. This
operation reduces memory and computational complexity in
order for it to be suitable for implementation in a WSN.
Both schemes [47], [55] aim to exploit the spatial corre-
lation of data in order to detect anomalies in a WSN. For
the QSSVM, the radius of the centred hypersphere provides
a compact statistic to communicate information about the
data a node is encountering. This is broadcast to spatially
neighbouring nodes, and each node calculates a median radius.
The two radii, local and neighbour median, are used to classify
data vectors. The use of the median radius from local nodes
aims to determine if a data vector would also be considered an
anomaly within the context of the data in the neighbourhood.
In addition, by examining the data vectors of other spatially
located nodes the anomaly is classiﬁed as an event if other
nodes are experiencing anomalies.
For the second scheme [55], the aim is to detect anomalies
local to the node (local anomaly), or global to a cluster of
nodes (global anomaly). Each node computes the CESVM,
then communicates metrics describing the model; the effective
radius, median and covariance matrix. From these parameters a
global CESVM is formed on each node with which to identify
global anomalies.
Evaluation on synthetic and GSB data sets shows that the
CESVM performs better than the QSSVM on multivariate
data with correlated attributes. However, the CESVM has
higher computational and communication complexity than the
QSSVM.
B. Detect and Retrain
A technique that aims to determine when an update to a
model is required aims to detect and retrain. The goal is to
identify a time when the data distribution has changed signif-
icantly enough to justify an update of the model. Updating a
model only when required can save energy resources which
is particularly important in a WSN.
Returning to the QSSVM and CESVM of Zhang et al. [47],
[55], in addition to the constant update, a change detection
scheme is outlined. The notion that
identifying a period
of increased anomalies signals a change point is used. We
previously outlined two schemes which involve a constant
update. The third scheme aims to detect and retrain in order
to reduce the computational complexity of the algorithm. The
scheme proposes reconstructing the model when new data
signiﬁcantly changes the model. Data vectors falling on the
boundary, a border support vector (BSV), and anomalies are
shown to have a signiﬁcant impact on the model as they
cause the constraints in the problem formulation to no longer
be met. When a BSV or an anomaly is encountered, the
training set is updated by adding in the new measurements
since the last update, and removing the equivalent number
of oldest measurements. Retraining occurs with the updated
training set. As BSVs and anomalies are less common than
normal data [45], this will reduce the number of model updates
required. Evaluation of the technique on the IBRL [5] and
GSB data sets indicate that there is a signiﬁcant reduction in
This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.O’REILLY et al.: ANOMALY DETECTION IN WIRELESS SENSOR NETWORKS IN A NON-STATIONARY ENVIRONMENT
11
the number of model updates compared to constant update
methods with no reduction in performance.
The incremental hyperellipse algorithm proposed by Mosh-
taghi et al. [48], [57] makes use of change detection in order
to identify changes in the data distribution in a non-stationary
environment. The algorithm is an incremental update to the
hyperellipsoid of Rajasegarar et al. [20]. Change detection
occurs by monitoring the number of anomalies detected by
the hyperellipse and signalling a change when the number of
consecutive anomalies exceeds a threshold. Thus, identifying
a change in the probability of an anomaly occurring, identiﬁes
a change in the data distribution. Evaluations showed that
on synthetic data, the algorithm was able to detect change
points. It is benchmarked against a model using recursive least
squares (RLS) and applying a CUSUM on its residual. The
incremental hyperellipse was shown to detect more change
points in a non-stationary distribution in a real-world data set.
To allow the hyperellipse to adapt to changes in the data
distribution without the use of sliding windows, an incremental
update is detailed where new data measurements can be
incorporated into the model without model reconstruction or
access to the training set that originally constructed the model.
An iterative update to the mean and covariance matrix allows
data to be added to the model. In order to remove data
measurements, a forgetting factor 0 < λ < 1 is introduced.
This gives a weight of λj
that was
generated j samples previously.
to the measurement
A signiﬁcant advantage of hyperellipses is their computa-
tional simplicity compared to other boundary techniques such
as OC-SVMs. However, they are linear in nature and thus do
not perform as well on non-linear data sets as kernel methods
such as those derived from the OC-SVM.
VI. MODEL UPDATE – MODEL SELECTION
Model selection aims to select a model from a set of (pos-
sibly inﬁnite) candidate models that will perform optimally
on unseen testing data. The parameters available to form a
model from a training set vary between each technique, but
can include;
(cid:129) Regularization Parameter – OC-SVM
(cid:129) Kernel Parameter – Kernel methods
(cid:129) Subspace Dimension – Principal Component Analysis
(cid:129) Anomaly Rate – Threshold techniques
There are two approaches to selecting the optimal model.
In the ﬁrst approach the optimal parameters for a model
are estimated based on the characteristics of the training set.
These are then used to construct the model. In the second
approach, multiple models are constructed using different