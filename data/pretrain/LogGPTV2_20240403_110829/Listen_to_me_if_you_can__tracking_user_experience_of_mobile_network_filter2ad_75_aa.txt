title:Listen to me if you can: tracking user experience of mobile network
on social media
author:Tongqing Qiu and
Junlan Feng and
Zihui Ge and
Jia Wang and
Jun (Jim) Xu and
Jennifer Yates
Listen to Me if You can: Tracking User Experience of
Mobile Network on Social Media
Tongqing Qiu
Georgia Tech
Atlanta, GA
PI:EMAIL
Jia Wang
AT&T Labs – Research
Florham Park, NJ
Junlan Feng
AT&T Labs – Research
Florham Park, NJ
∗
Jun (Jim) Xu
Georgia Tech
Atlanta, GA
Zihui Ge
AT&T Labs – Research
Florham Park, NJ
Jennifer Yates
AT&T Labs – Research
Florham Park, NJ
PI:EMAIL
PI:EMAIL
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
Social media sites such as Twitter continue to grow at a fast pace.
People of all generations use social media to exchange messages
and share experiences of their life in a timely fashion. Most of
these sites make their data available. An intriguing question is can
we exploit this real-time and massive data-ﬂow to improve busi-
ness in a measurable way.
In this paper, we are particularly in-
terested in tweets (Twitter messages) that are relevant to mobile
network performance. We compare tweets with a more traditional
source of user experience, i.e., customer care tickets, and corre-
late both of them with a list of major network incidents. From our
study, we have the following observations. First, Twitter users and
users who call customer service tend to report different types of
performance issues. Second, we observe that tweets typically ap-
pear more rapidly in response to network problems than customer
tickets. They also appear to respond to a wider range of network
issues. Third, signiﬁcant spikes in the number of tweets appear
to indicate short term performance impairments which are not re-
ported in our current list of major network incidents. These obser-
vations together indicate that Twitter is an attractive, complemen-
tary source for monitoring service performance and its impact on
user experience.
Categories and Subject Descriptors
System Organization]:
C.2.3
Communication Networks—Network Operations
[Computer
Computer-
General Terms
Management, Measurement, Performance
∗
This work was supported in part by NSF grant CNS-0905169,
funded under the American Recovery and Reinvestment Act of
2009 (Public Law 111-5), and NSF grant CNS-0716423.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
Keywords
Social Media, Twitter, User Experience
1.
INTRODUCTION
Monitoring network performance is one of the key tasks in net-
work operation. We detect service performance issues from both a
systems perspective and from a customers’ perspective. From the
systems perspective, we infer the issues based on measurement of
network delay, packet loss, etc. From a customers’ perspective, we
identify issues based on users’ feedback. The traditional way to
learn user feedback is through customer calls or emails. When a
customer complains about a problem, we investigate and resolve
it. In this paper, we propose to go beyond customer care data to
exploit a different channel – online social media, for tracking user
feedback regarding service performance.
Online social networks (OSN) have gained signiﬁcant popularity
during recent years [18]. Microblogging services such as Twitter,
are one of popular means through which users share information
and experiences on the web. Comparing with Facebook, LinkedIn,
MySpace, YouTube, and other social networking services, mes-
sages on Twitter are short (less than 140 characters). Twitter mes-
sages are widely referred to as tweets. It takes only a few seconds
for a user to write a tweet and have it distributed to the public and
his followers. Users can send and receive tweets through various
applications (such as web and instant messaging) and devices (such
as mobile phones, TV and computers). According to comScore,
Twitter ﬁnished 2009 with nearly 20 million visitors to its website,
up from just 2 million visitors in 2008 [1].
In this paper, we analyze tweets related to one of the largest
mobile service providers in the United States. We ﬁrst identify
tweets that relate to service performance issues, and compare them
with customer care trouble tickets. Second, we correlate these two
sources of customer feedback with a report of major network in-
cidents. Our ﬁndings are threefold: (1) Issues reported on Twitter
are complementary to customer care calls.
(2) Twitter users are
faster to report service performance issues compared to customers
who call and complain to the customer care center. (3) Tweets re-
port some short term and/or less severe problems, which are not
recorded in the major network incidents report.
The remainder of the paper is structured as follows. Datasets for
analysis are discussed in Section 2, followed by our presentation of
results in Section 3. Section 4 reviews related work and Section 5
concludes the paper.
2882. DATASETS
In this section, we discuss the methodology used to collect data
and describe the data used in our work. Section 2.1 discusses Twit-
ter data, Section 2.2 discusses customer care call data, and Sec-
tion 2.3 discusses the major network incidents report data.
2.1 Twitter Data
We used Twitter APIs to retrieve publicly available data rele-
vant to our task. We emphasize here that only information that
was shared publicly by Twitter users was obtained and analyzed.
First, we manually selected a few keywords that were deemed as
good queries for retrieving tweets relevant to the mobile service
provider in question. Second, we used the Twitter search API to
obtain tweets and we archived the retrieved tweets along with the
associated meta data. The meta data include the time that the tweet
was submitted, the user who authored the tweets, etc. Third, we
fetched user information through the Twitter REST API for those
who authored the tweets that we archived. User information con-
sists of the user proﬁle such as user location to help localize issues
reported.
After data archiving, our next step was to identify tweets related
to mobile network performance issues. We used a few heuristic
rules: (1) Tweets must contain mobile related words such as phone,
mobile, 3G, edge, etc; (2) Tweets must contain performance related
words such as slow, drop, intermittent, doesn’t work, etc; and (3)
Tweets should not contain words indicative of advertising, such as
"Ads" and price symbols $. To verify the effectiveness of these
rules, we randomly sampled 100 tweets, and manually annotated
whether they were related to mobile performance issues. We ob-
serve an 87% agreement between rule-based prediction and human
annotation. This is an acceptable level of accuracy1 for our study.
There are potential methods via natural language processing and
machine learning which would likely improve the performance of
this step. We leave this as future work.
2.2 Customer Care Calls
We obtained customer care tickets that had been created in re-
sponse to customer calls for the mobile service provider. Note
that these tickets are anonymized; no customer identiﬁcation in-
formation is used from the tickets in this analysis. These tickets
are tagged with the types of issues, i.e., billing and accounting,
calling plan and features, mobile devices, service coverage, perfor-
mance impairments and service outages. In this paper, we focus on
the customer tickets relating to service impairments and outages.
In addition, each ticket contains information regarding the type of
service, the time that the call was received by the customer care
team (the trouble ticket is often issued at the same time), the loca-
tion, and a description of the performance issues that the customer
experienced. The description usually provides detailed information
on when and where the customer experienced the performance im-
pairment as well as the device and application that the customer
was using when the performance impairment occurred.
In our data, the performance related customer trouble tickets in-
clude issues such as no coverage, cannot make or receive calls, call
disconnected/dropped, and poor voice quality, etc. It is important
to note that not all customers will call the customer care team and
report the performance impairments that they experienced. In ad-
dition, we will show later that customers may not call the customer
service center immediately after they experience the performance
impairments.
1In statistical scene, the estimation result 87% is within 95% con-
ﬁdence interval, with a margin error less than 0.1, given that we
randomly selected 100 samples.
2.3 Major Network Incidents Report
The service provider has a top tier of operators that oversee the
entire operation of the cellular network and service. They maintain
a so called Major Network Incidents Review in a collaborative edit-
ing system. The network incident review keeps track of information
regarding major network or service incidents as they are reported,
diagnosed, resolved and concluded. The review report serves as a
channel to communicate summary-level information about major
incidents among the team members and senior management mem-
bers.
The incidents in the report include a wide variety of network
and service issues including hardware failures, major maintenance
activities, outages due to adverse weather, congestion due to ﬂash
crowds (e.g., highway accident causing trafﬁc congestion and unex-
pected high cellular network congestion), etc. Reading through the
incidents report, we ﬁrst ﬁltered out those non-customer-impacting
events from our study, such as planned system upgrade where re-
dundant capacity is in use during the upgrade. We have also ex-
cluded very long incidents (e.g., greater than three hours) in the
later part of the analysis so that the chance of falsely joining a
network or service event to independent customer care tickets and
tweets is low.
Each entry in the major network incidents report contains crucial
information about an event. All entries contain temporal informa-
tion (i.e., the start and end time of the incidents), and coarse spatial
information, i.e., the primary market region where the incidents oc-
curred (e.g, northeast, west, etc.). Moreover, all entries have facili-
tator/ incident manager contact details for follow up investigations,
and the estimated scale of the customer impact. Most of the entries
have incident summaries which describe the nature of the incidents
in detail 2. Some of the incidents contain root cause descriptions
which explain the cause of the incidents in detail. In this paper, we
only utilize the temporal and spatial information recorded for the
incidents.
3. RESULTS
In this section, we ﬁrst present the results on the major network
incidents report, tweets and tickets data. Then we correlate both
tweets and tickets with the incidents report. The data we analyze
are based on a large cellular network provider in United States over
the period of 16 days.
3.1 Major Network Incidents Report Results
We ﬁrst analyze the temporal distribution of the major network
incidents reported. Note that not all network incidents are included
in the report - only those deemed most signiﬁcant by the operations
teams investigating. The report that we use contains only major
network incidents. The vast majority of network incidents are fairly
minor and not severe enough to be included in this report. Among
these major incidents, we observe a very diverse duration ranging
from seconds to over a day. As we mentioned in Section 2.3, in the
later part of our analysis we focus on the incidents with duration of
less than three hours.
3.2 Tweets vs. Customer Care Tickets
In this section, we compare two sources of user experience:
tweets and tickets based on customer care calls. First, we con-
duct the comparison using the raw data, which include all tweets
2Although some descriptions contain certain kind of detail location
information (for example, city, device, highway information), it is
challenging to automatically and precisely extract them due to the
loose structure of the description text.
289s
e
g
a
s
s
e
m
f
o
r
e
b
m
u
N
s
t
e
k
c
i
t
f
o
r
e
b
m
u
N
Twitter
Twitter
Tickets
Day 4
Day 9
Day 14
Time (UTC)
Customer Care tickets
e
g
a
t
n
e
c
r
e
P
Day 4
Day 9
Day 14
Time (UTC)
Call.Drops
Slow.Connect No.service
Voice.Quality
Others
Figure 1: The number of tweets and tickets per hour. Due to
privacy issues, the concrete number of tickets is not reported.
Figure 2: Classify performance related tweets/tickets based on
the type of the issues. Due to privacy issues, the concrete per-
centages are not reported.
that we have archived and considered as relevant to the mobile ser-
vice provider (i.e., some tweets are not performance related). Then
we classify the data into different categories and drill down to the
performance-related tweets. Finally, we examine the delay of user
feedback, namely, how long it takes for a user to report a problem
after he/she experienced it.
3.2.1 Time Series
We compare the volume of tweets and the volume of customer
call tickets over a common period of time. Figure 1 shows the
number of tweets and tickets per hour based on the raw data. From
Figure 1, we observe the obvious daily pattern in both cases. It is
easy to understand that customer calls have such pattern (common
user behavior). It is also not surprising for tweets because of the
daily pattern of social media access [2] and the fact that we focus
on a US service provider. We will later (in Section 3.3) see that the
diurnal pattern is weak when we focus on only performance related
tweets. Another interesting observation from Figure 1 is the spike
for Twitter data on Day 7. This spike is caused by discussions
relating to new technology being made available to the consumer
market.
3.2.2 Classiﬁcation
We have investigated the raw data in the previous section. In this
section, we move on to classify the data into different categories,
and then focus on the performance related logs.
By manually inspecting the message content of the collected
tweets, we observed the following three major types: (1) Com-
ments and news regarding the product and customer service; (2)
Advertisements (e.g. the promotion of a mobile phone); (3) Com-
ments or complaints regarding performance related issues (which
we are particularly interested in). Due to the loose organization of
messages, it is difﬁcult to precisely classify tweets automatically.
But in general, the majority of the tweets are of type (1) or (2). On
the other hand, since the tickets have a ﬁxed structure and catego-
rization, it is relatively easy to classify them. The majority (over
97%) of tickets are related to plan, bill or device issues. In the rest
Table 1: Location information of tweets.
Location Info.
No
City+State
Twitter Proﬁle
Twitter Message
37.2% 29.6%
69.5% 20.5%
Yes
State only Others
12.7%
20.5%
9.9%
1.0%
of the paper, we mainly focus on type (3), i.e., performance related
tweets / tickets.
Among tweets extracted using the methodology described in
Section 2.1, only about 1% of messages are related to performance
issues. Similarly, only 1% to 2% of tickets are related to network
performance. Figure 2 further breaks down the performance related
issues into several categories. From this ﬁgure, we observe that is-
sues reported by Twitter users and by customers who call customer
care are very different. For Twitter users, call dropping is the most
frequent complaint, followed by slow connection, no service and
others (e.g., difﬁculty in sending messages or in posting something
on websites). In contrast with the tweets, more tickets are related
to lack of service or coverage issues (e.g. no coverage in some
buildings). Very few of them are related to slow connection. More-
over, we also observe that many tickets are relate to voice quality,
issues which are not typically seen in Twitter messages. This result
illustrates that the types of performance issues reported by Twit-
ter users are different from those reported by customer calls: the
former tends to report short term and/or minor performance im-
pairments, while the later tends to report more severe performance
issues.
We also compare the tweets and tickets by locations. There are