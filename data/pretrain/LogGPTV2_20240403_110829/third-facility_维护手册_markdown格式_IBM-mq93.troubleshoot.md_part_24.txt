transfer requests) to the agent.
• 200 messages or complete message groups on the queue during a poll, it submits 200 tasks (or
managed transfer requests) to the agent.
Monitors contain some logic to compare the number of tasks they have submitted to the agent during a
poll against the number of source transfer slots that the agent has (as specified by the agent property
maxSourceTransfers). If the tasks are greater than twice the number of source transfer slots, the
monitor writes the BFGDM0107W message to the agent's event log. This lets you know that it has
submitted a large number of tasks to the agent, more than half of which are going on to the agent's
backlog.
Going back to our preceding example, where a monitor finds 200 messages during a single poll, and
assuming that the agent in question has its maxSourceTransfers property set to the default value of
25, when the monitor submits the 200 tasks to the agent:
• 25 are assigned source transfer slots, and the agent starts to process those straight away.
• The remaining 175 are assigned queued transfer slots; these go onto the agent's backlog to be
processed at some point in the future.
Having a large number of managed transfers on an agent's backlog takes up resources such as memory,
and so can potentially affect an agent's performance. Because of this, it is good practise to try and
keep the number of managed transfers or managed calls occupying queued transfer slots down to a low
number where possible.
How to prevent the warning from occurring
One thing that can assist you is the monitorMaxResourcesInPoll property mentioned in the
BFGMD0107W message. This is an agent property which applies to all resource monitors running within
the agent and limits the number of items that monitors trigger on during a single poll. The default value of
the property is -1, which means that monitors trigger on every item that they find in a poll and submit a
task for each one.
When the property is set to something other than -1, the monitor stops scanning the resource once it
triggers on that many items. This means that the monitor is sending work to the agent in small chunks,
rather than giving it lots of work to do all in one go.
For example, if monitorMaxResourcesInPoll is set to 25, once the monitor finds 25 new items that
match its trigger condition, it stops its current poll and submits 25 tasks to the agent.
When changing monitorMaxResourcesInPoll, another thing to consider is increasing the polling
interval of the monitor. Ideally, if a resource monitor submits some tasks to an agent, it should allow
most (if not all) of them to complete before starting a new poll and potentially giving some more work to
the agent to do. This also helps to reduce the overall load on the agent, and can improve its throughput.
Example
Suppose you have a resource monitor that has been configured to monitor a source queue every minute,
looking for either complete message groups or individual messages not in a group. For each message
group or individual message that the monitor finds, it submits a task (in the form of a managed transfers
request) to move the contents of that message or message group to a file.
The agent where the monitor is running has the following agent properties set:
maxQueuedTransfers=1000
maxSourceTransfers=25
monitorMaxResourcesInPoll=25
150 Troubleshooting and Support for IBM MQ
This means that during every poll, the monitor has the potential to submit 25 tasks to the agent.
Assuming that it takes the agent approximately two minutes to process all 25 tasks, then with a polling
interval of one minute the following behavior takes place:
Minute 0
• The monitor starts a poll, scans the source queue, and finds 25 messages (the value of
monitorMaxResourcesInPoll).
• The monitor now submits 25 tasks (or managed transfer requests) to the agent, and then stops its poll.
• The agent picks up the 25 managed transfer requests, assigns each of them a source transfer slot and
begins processing them.
At this point in time, the agent's transfer slots look like this:
| Used | Free
----------------------|------|------
Source transfer slots | 25 | 0
Queued transfer slots | 0 | 1000
Minute 1
• The monitor now starts its second poll.
• The monitor, once again, scans the source queue, finds 25 messages and submits 25 managed transfer
requests to the agent.
• The poll ends.
• The agent receives these new managed transfer requests. As all of its source transfer slots are
occupied, it assigns each of the managed transfer requests a queued transfer slot and puts them on its
backlog.
The agent's transfer slots now look like this:
| Used | Free
----------------------|------|------
Source transfer slots | 25 | 0
Queued transfer slots | 25 | 975
Minute 2
• By this time, all of the 25 managed transfers have finished processing and their associated source
transfer slots are released. As a result, the agent moves the 25 managed transfers from the queued
transfer slots to the source transfer slots.
This leaves the agent's transfer slots looking like this:
| Used | Free
----------------------|------|------
Source transfer slots | 25 | 0
Queued transfer slots | 0 | 1000
• The monitor performs another poll, finds another batch of 25 messages and submits 25 managed
transfer requests to the agent.
• The agent picks up these requests and puts them onto its backlog
This means that the transfer slots now look like this:
| Used | Free
----------------------|------|------
IBM MQ troubleshooting and support 151
Source transfer slots | 25 | 0
Queued transfer slots | 25 | 975
Minute 3
• During the next poll, the monitor finds 25 more messages, and so submits 25 more managed transfer
requests to the agent.
• The agent receives these managed transfer requests and assigns them each a queued transfer slot.
As a result, the agent's transfer slots are now like this:
| Used | Free
----------------------|------|------
Source transfer slots | 25 | 0
Queued transfer slots | 50 | 950
and so on.
Increase the polling interval to two minutes
Increasing the monitor's polling interval to two minutes means that the 25 managed transfers submitted
during one poll would be completed by the time the next one started. This means that the agent is able
to assign these managed transfers a source transfer slot, and not have to put them onto its backlog, as
shown in the following example:
Minute 0
• The monitor starts a poll, scans the source queue, and finds 25 messages (the value of
monitorMaxResourcesInPoll).
• The monitor now submits 25 managed transfer requests to the agent, and then stops its poll.
• The agent picks up the 25 managed transfer requests, assigns each of them a source transfer slot and
begins processing them.
At this point in time, the agent's transfer slots look like this:
| Used | Free
----------------------|------|------
Source transfer slots | 25 | 0
Queued transfer slots | 0 | 1000
Minute 2
• By this time, all of the 25 managed transfers have finished processing and their associated source
transfer slots are released.
This means that the agent's transfer slots look like this:
| Used | Free
----------------------|------|------
Source transfer slots | 0 | 25
Queued transfer slots | 0 | 1000
• The monitor performs another poll, finds another batch of 25 messages and submits 25 managed
transfer requests to the agent.
• The agent picks up these requests and assigns each of them a source transfer slot.
This means that the transfer slots now look like this:
152 Troubleshooting and Support for IBM MQ
| Used | Free
----------------------|------|------
Source transfer slots | 25 | 0
Queued transfer slots | 0 | 1000
Minute 4
• Two minutes later, the 25 managed transfer requests submitted by the monitor in minute 2 have
completed, and their associated "source transfer slots" have been freed up and released.
The agent's source transfer slots are now this:
| Used | Free
----------------------|------|------
Source transfer slots | 0 | 25
Queued transfer slots | 0 | 1000
• The monitor now performs a new poll and finds 25 more messages on the queue. As a result, it submits
25 managed transfer requests to the agent.
• The agent picks up the managed transfer requests. As it is not currently acting as the source agent for
any managed transfers, it assigns a "source transfer slot" to each of the new requests.
This makes its transfer slots look like this:
| Used | Free
----------------------|------|------
Source transfer slots | 25 | 0
Queued transfer slots | 0 | 1000
The advantage of this approach is that managed transfers never go onto an agent's backlog, which
reduces the overall resource usage of the agent and in turn can help with performance.
Troubleshooting java.lang.OutOfMemoryError problems
Use the following reference information to help you to resolve issues with agents stopping due to
java.lang.OutOfMemoryErrors:
Related reference
“Common problems” on page 166
Common problems that might occur in your Managed File Transfer network.
“Return codes for MFT” on page 124
Managed File Transfer commands, Ant tasks, and log messages provide return codes to indicate whether
functions have successfully completed.
What to do if your MFT agent ABENDS with a java.lang.OutOfMemoryError due to Java
heap exhaustion
While processing a number of managed transfer requests, such as file-to-file, message-to-file or file-to-
message transfers, the agent abnormally ends (ABENDS) reporting a java.lang.OutOfMemoryError,
and at the time your total RAM memory was not fully utilized. This exception has been caused by Java
heap exhaustion.
Diagnosing the problem
When this issue occurs, the affected agent ABENDs and generates four files that provide details on the
root cause:
• An ABEND file. The name of this file conforms to the naming convention
ABEND.FTE.date_timestamp.identifier.log.
IBM MQ troubleshooting and support 153
On Multiplatforms, the file is written to the MQ_DATA_PATH/mqft/logs/
coordination_qmgr_name/agents/agent_name/logs/ffdc directory.
On z/OS, the file is written to the z/OS UNIX System Services (z/OS UNIX) location
$BFG_CONFIG/mqft/logs/coordination_qmgr_name/agents/agent_name/logs/ffdc
• A Javacore file. The name of this file has the following format:
javacore.datestamp.timestamp.pid.identifier.txt
On Multiplatforms, the file is written to the MQ_DATA_PATH/mqft/logs/
coordination_qmgr_name/agents/agent_name directory.
On z/OS, the file is written to the z/OS UNIX location $BFG_CONFIG/mqft/logs/
coordination_qmgr_name/agents/agent_name directory.
• A Java snap dump. The name of this file has the following format:
snap.datestamp.timestamp.pid.identifier.txt
On Multiplatforms, the file is written to the MQ_DATA_PATH/mqft/logs/
coordination_qmgr_name/agents/agent_name directory.
On z/OS, the file is written to the z/OS UNIX location $BFG_CONFIG/mqft/logs/
coordination_qmgr_name/agents/agent_name directory.
The ABEND and Javacore pair contain information similar to the examples shown below:
Abend file
Filename:
C:\ProgramData\IBM\MQ\mqft\logs\QM1\agents\AGENT1\logs\ffdc\ABEND.FTE.20220810102649225.18938124211177445
3.log
Level: p920-005-220208
Time: 10/08/2022 10:26:49:225 BST
Thread: 45 (FileIOWorker-0:0)
Class: com.ibm.wmqfte.thread.FTEThread
Instance: a393304f
Method: uncaughtException
Probe: ABEND_001
Cause: java.lang.OutOfMemoryError: Java heap space
java.lang.OutOfMemoryError: Java heap space
at java.nio.HeapByteBuffer.(HeapByteBuffer.java:57)
at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
at com.ibm.wmqfte.util.impl.ByteBufferPoolImpl.getBuffer(ByteBufferPoolImpl.java:44)
at com.ibm.wmqfte.transfer.frame.impl.TransferChunkImpl.getByteBuffer(TransferChunkImpl.java:181)
at com.ibm.wmqfte.transfer.frame.impl.TransferChunkImpl.(TransferChunkImpl.java:143)
at
com.ibm.wmqfte.transfer.frame.impl.TransferFrameSenderImpl.requestChunk(TransferFrameSenderImpl.java:636)
at
com.ibm.wmqfte.transfer.frame.impl.TransferFrameSenderImpl.access$000(TransferFrameSenderImpl.java:100)
at
com.ibm.wmqfte.transfer.frame.impl.TransferFrameSenderImpl$ChunkRequester.processFileIORequest(TransferFr
ameSenderImpl.java:142)
at
com.ibm.wmqfte.transfer.frame.impl.TransferFrameIOWorker.doWorkImpl(TransferFrameIOWorker.java:318)
at com.ibm.wmqfte.io.impl.FTEFileIOWorker.doWork(FTEFileIOWorker.java:118)
at com.ibm.wmqfte.io.impl.FTEFileIORequestQueue.run(FTEFileIORequestQueue.java:244)
at java.lang.Thread.run(Thread.java:825)
at com.ibm.wmqfte.thread.FTEThread.run(FTEThread.java:70)
Javacore file
0SECTION TITLE subcomponent dump routine
NULL ===============================
1TICHARSET 437
1TISIGINFO Dump Event "systhrow" (00040000) Detail "java/lang/OutOfMemoryError" "Java heap space"
received
1TIDATETIMEUTC Date: 2022/08/10 at 09:26:53:917 (UTC)
1TIDATETIME Date: 2022/08/10 at 10:26:53:917
1TITIMEZONE Timezone: (unavailable)
1TINANOTIME System nanotime: 350635184939400
154 Troubleshooting and Support for IBM MQ
1TIFILENAME Javacore filename:
C:\ProgramData\IBM\MQ\mqft\logs\QM1\agents\AGENT1\javacore.20220810.102653.7172.0003.txt
Why this problem occurs
This issue occurs due to exhaustion of the Java heap memory for the JVM running the agent.
See How MFT agents use Java heap and native heap memory for more information on the distinctions
between Java heap memory and native heap memory.
Avoiding the problem
There are a number of actions that you can take to help reduce the likelihood of an MFT agent stopping
due to a java.lang.OutOfMemoryError, caused by exhaustion of Java heap memory:
1.Increase the size of the Java heap for the JVM running the MFT agent.
By default, the Java heap of an agent is set to 512 MB. Although this is satisfactory for small numbers
of managed transfers, it might need to be increased to up to 1024MB (1GB) for production-like
workload.
Attention: When increasing the size of the Java heap for an agent, it is important to consider
the other agents and applications that are running on the same system as these are using
native heap.
Increasing the size of the Java heap for an agent also increases its native heap usage, which
in turn reduces the amount of native heap available to the other agents and applications. This
means that there is an increased likelihood of agents and applications experiencing native heap
exhaustion.
• To increase or change the Java heap when running the agent as a normal process:
Set the BFG_JVM_PROPERTIES environment variable to pass the Java property -Xmx to the JVM. For
example, on Windows, to set the maximum heap size to 1024 MB run the following command before
using the fteStartAgent command:
set BFG_JVM_PROPERTIES="-Xmx1024M"
For more information about how to set Java system properties using the BFG_JVM_PROPERTIES
environment variable, see Java system properties for MFT.
• To increase or change the Java heap when running the agent as a Windows service:
Use the fteModifyAgent command and specify the -sj parameter to set the -Xmx property on the
Windows service.
The following example uses the fteModifyAgent command with the -sj parameter, to set the
maximum size of the Java heap for a JVM running a Windows service configured agent to 1GB
(1024MB):
fteModifyAgent.cmd -agentName AGENT1 -s -su user1 -sp passw0rd -sj -Xmx1024M
You can check this has been successfully set, by reviewing the output0.log file of the agent, after
the agent has been restarted. In the Start Display Current Environment section, a value of 1024 MB
will be reported, as follows:
The maximum amount of memory that the Java virtual machine will attempt to use is: '1024'MB
2.Restrict Java heap usage by reducing the workload of the agent.
Typically, java.lang.OutOfMemoryErrors caused by Java heap exhaustion are the result of an
agent doing too much work. Every managed transfer and managed call that an agent is processing uses
memory in the Java heap, as do managed transfers and managed calls that are on the backlog of an
agent. Resource monitors also use Java heap memory when they perform a poll.
IBM MQ troubleshooting and support 155
This means that as the workload of an agent increases, the amount of Java heap that it is using also
grows as well.
Reducing the workload of the agent can help here. To do this:
• Set the following agent properties to a lower value:
– maxQueuedTransfers
– maxSourceTransfers
– maxDestinationTransfers
• Move some of the resource monitors of the agent to a new agent.
This reduces the number of concurrent transfers that can occur, and therefore decreases the
maximum concurrent workload for the agent.
3.Enable memory allocation checking.
The memory allocation checking functionality ensures that agents only start to process a new
managed transfer if there is enough Java heap memory for it to run to completion. If there is
insufficient memory, the managed transfer is rejected.
This functionality is off by default. To enable it for an agent:
• Add the following entry to the agent.properties file of the agent: