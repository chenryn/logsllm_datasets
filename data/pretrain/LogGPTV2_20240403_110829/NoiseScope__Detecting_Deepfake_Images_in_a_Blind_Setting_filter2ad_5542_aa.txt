# NoiseScope: Detecting Deepfake Images in a Blind Setting

## Authors
- Jiameng Pu, Virginia Tech
- Neal Mangaokar, Virginia Tech
- Bolun Wang, Facebook
- Chandan K. Reddy, Virginia Tech
- Bimal Viswanath, Virginia Tech

## Abstract
Recent advancements in Generative Adversarial Networks (GANs) have significantly improved the quality of synthetic images, or deepfakes. These photorealistic images challenge the boundaries of human perception and pose new threats to critical domains such as journalism and online media. Detecting whether an image is generated by a GAN or captured by a real camera has become an important yet under-investigated area. In this work, we propose a blind detection approach called NoiseScope for identifying GAN-generated images among real images. Unlike supervised methods, a blind approach does not require a priori access to GAN images for training and generalizes better across different datasets. Our key insight is that GAN-generated images carry unique noise patterns, similar to those from cameras. We extract these patterns in an unsupervised manner to identify GAN images. We evaluate NoiseScope on 11 diverse datasets containing GAN images, achieving up to 99.68% F1 score in detecting GAN images. We also test the robustness of NoiseScope against various countermeasures, demonstrating its adaptability and effectiveness.

## CCS Concepts
- Security and privacy → Domain-specific security and privacy architectures.

## Keywords
Deepfakes, Blind Detection, Machine Learning, Clustering

## ACM Reference Format
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal Viswanath. 2020. NoiseScope: Detecting Deepfake Images in a Blind Setting. In Annual Computer Security Applications Conference (ACSAC 2020), December 7–11, 2020, Austin, USA. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3427228.3427285

## Permission
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

## ACSAC 2020
December 7–11, 2020, Austin, USA
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8858-0/20/12...$15.00
https://doi.org/10.1145/3427228.3427285

## Figure 1: Advances in GANs over the years.

### 1 Introduction
The machine learning community has made significant advances in deep generative models. A landmark paper by Goodfellow et al. introduced the Generative Adversarial Network (GAN) in 2014 [33], which sparked immense interest in developing and improving GAN models. Today, GANs can generate convincing images [39, 40], videos [75], text [34], and audio [22]. Figure 1 illustrates the improvement in image quality generated by GANs over the years. These efforts were primarily motivated by benign use cases, such as augmenting datasets with synthetic samples [26], de-identification [42], feature extraction [5], video prediction [46], and image editing [58]. However, governments [16, 73] and industry [25] are increasingly aware of the dual-use nature of such powerful methods. Deepfakes produced by generative models can be used for malicious purposes, including fake pornographic content, disinformation campaigns, and manipulation of medical images [51].

In this work, we address these threats by developing a deepfake detection scheme, focusing on GAN-generated images. Most prior work on detecting GAN-generated images relies on supervised methods that require a priori access to fake images or their generative models [52]. However, supervised schemes often do not generalize well to unseen datasets, and access to fake images for training can be limited. Instead, we focus on advancing blind detection of fake images. Our method, called NoiseScope, accurately detects GAN-generated images without requiring any a priori access to fake images or their generative schemes for training.

Our work is inspired by prior research in camera fingerprinting [11, 12, 32, 49] and includes the following key contributions:
1. We find that GAN-generated images contain unique low-level noise patterns tied to the GAN model, similar to images produced by cameras. These patterns are correlated with the deconvolution layers of GANs.
2. We present the design and implementation of NoiseScope, a blind detection scheme that leverages these unique patterns. Given a test set with unknown numbers of fake and real images, NoiseScope extracts model fingerprints and uses them to detect fake images. Unlike supervised schemes, our method is agnostic to the type of GAN used and is effective even when the test set contains images from multiple GANs.
3. We evaluate NoiseScope on 11 diverse deepfake image datasets created using 4 high-quality GAN models, achieving up to 99.68% F1 score.
4. We extensively evaluate NoiseScope against a variety of countermeasures, assuming an attacker who is aware of NoiseScope’s detection pipeline.

Given the rapid development of new generative models, supervised learning strategies may tip the arms race in favor of attackers. Therefore, there is an urgent need to advance blind detection schemes that can work against a wide range of GAN models. The source code for NoiseScope is available on GitHub, and we hope it inspires further research in deepfake detection.

### 2 Background & Related Work
In this work, we focus on images and consider deepfake images as those produced by machine learning algorithms, specifically GANs. GANs are capable of producing high-quality images that are difficult for humans to distinguish from real images [51]. We encourage readers to visit the following website, which presents a new fake image on each page refresh, created using StyleGAN [40]. In the rest of the paper, we will interchangeably use the terms "deepfake" or "fake image" to refer to such content. Images produced by traditional imaging devices (cameras) are called real images.

#### 2.1 Deepfake Generation Methods
Deepfakes are primarily enabled by deep generative models, such as GANs and Variational Autoencoders (VAEs) [41]. We focus on deepfakes generated by GANs due to their impressive performance.

**GAN Basics.**
A GAN consists of two neural networks: a generator (G) that produces fake images and a discriminator (D) that provides feedback to the generator on how well the fake images resemble real images. The two components are trained simultaneously in an adversarial manner, with the goal of making the generated images indistinguishable from real images. The training objective, known as the adversarial loss, optimizes this main goal.

**Deconvolution Layers.**
An integral component of most generative models, including VAEs and GANs, is the transposed convolution layer, commonly referred to as deconvolution or upsampling layer. This layer is fundamental for building high-quality generators, as it allows for learnable upsampling from a lower-dimensional vector space. In Section 5.1, we demonstrate how deconvolution layers leave distinct patterns in the "noise space" of an image, enabling us to distinguish between fake and real images.

**Choice of GANs.**
We focus on four key GAN models that have significantly raised the bar for image generation tasks: CycleGAN [87], PGGAN [39], BigGAN [9], and StyleGAN [40]. Figures 8-18 in Appendix A show image samples from all four GANs.

- **CycleGAN [87]:** Advanced the state-of-the-art in image-to-image translation, improving over Pix2Pix [37]. CycleGAN translates an image from one domain to another, e.g., turning an image of a horse into a zebra. It does not require paired images for training, making it more practical.
- **PGGAN [39]:** Demonstrated a significant improvement in image quality in 2018. PGGAN generates high-resolution images by progressively growing both the generator and discriminator. It can produce photo-realistic images at resolutions up to 1024x1024.
- **BigGAN [9]:** Introduced shortly after PGGAN, BigGAN scales up conditional GANs to develop high-quality images on a large number of domains. It uses techniques such as increased batch size, more layer channels, and shared embeddings for batch normalization layers. The "truncation trick" allows control over the trade-off between image fidelity and variety.
- **StyleGAN [40]:** Released in 2019, StyleGAN is an improvement over PGGAN with a redesigned generator architecture. It incorporates a noise-to-style CNN mapping network, changes in upsampling techniques, and addition of noise to feature maps. This redesign allows fine-grained control over the style of generated images.

#### 2.2 Deepfake Detection Methods
Prior work on deepfake detection has explored both supervised and blind detection schemes. Supervised schemes require access to both real and fake content (or knowledge of the generative model) and use this labeled data to train a classification algorithm. Blind detection, on the other hand, does not require a priori access to fake content and only has access to real content. Most prior work has employed supervised schemes, with limited efforts towards advancing blind detection schemes.

**Supervised Methods:**
- **Feature Engineering:** Some approaches focus on building supervised classifiers with input image features crafted from specific vector spaces. For example, Marra et al. [52] proposed using raw pixel and conventional forensics features, while Nataraj et al. [57] extracted pre-computed RGB pixel co-occurrence matrices.
- **GAN Fingerprints:** Marra et al. [53] extracted GAN model fingerprints using techniques from camera fingerprinting literature [12, 49]. Yu et al. [80] used a supervised deep learning scheme to learn GAN model fingerprints and attribute images to GANs. Albright and McCloskey [2] also worked on attributing images to GANs using generator inversion schemes [19].
- **Domain-Specific Inconsistencies:** Yang et al. [78] focused on deep fakes generated by splicing synthesized face regions into real images, showing that such splicing introduces errors in 3D head pose estimation.
- **DNN-Based Approaches:** Mo et al. [55] developed a CNN-based model to detect face images generated by PGGAN. Rossler et al. [69] compared five CNN-based classification architectures. Tariq et al. [77] proposed ensembles of various CNN-based classifiers. Concurrently, Wang et al. [77] proposed a classifier based on the ResNet-50 architecture, and Afchar et al. [1] designed MesoNet based on Inception blocks.

**Blind Detection Schemes:**
Blind detection aims to solve the problem of requiring a priori access to fake images for training while being able to detect fake images from a wide variety of sources. NoiseScope aims to advance the state-of-the-art for blind detection schemes by offering a performant detection scheme. It complements supervised detection schemes, allowing for potentially hybrid ensembles that combine the best of both worlds.