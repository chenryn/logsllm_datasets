title:NoiseScope: Detecting Deepfake Images in a Blind Setting
author:Jiameng Pu and
Neal Mangaokar and
Bolun Wang and
Chandan K. Reddy and
Bimal Viswanath
NoiseScope: Detecting Deepfake Images in a Blind Setting
Jiameng Pu
Virginia Tech
PI:EMAIL
Neal Mangaokar
Virginia Tech
PI:EMAIL
Bolun Wang
Facebook
PI:EMAIL
Chandan K. Reddy
Virginia Tech
PI:EMAIL
Bimal Viswanath
Virginia Tech
PI:EMAIL
ABSTRACT
Recent advances in Generative Adversarial Networks (GANs) have
significantly improved the quality of synthetic images or deepfakes.
Photorealistic images generated by GANs start to challenge the
boundary of human perception of reality, and brings new threats to
many critical domains, e.g., journalism, and online media. Detecting
whether an image is generated by GAN or a real camera has become
an important yet under-investigated area. In this work, we propose
a blind detection approach called NoiseScope for discovering GAN
images among other real images. A blind approach requires no a
priori access to GAN images for training, and demonstrably gener-
alizes better than supervised detection schemes. Our key insight is
that, similar to images from cameras, GAN images also carry unique
patterns in the noise space. We extract such patterns in an unsu-
pervised manner to identify GAN images. We evaluate NoiseScope
on 11 diverse datasets containing GAN images, and achieve up to
99.68% F1 score in detecting GAN images. We test the limitations
of NoiseScope against a variety of countermeasures, observing that
NoiseScope holds robust or is easily adaptable.
CCS CONCEPTS
• Security and privacy → Domain-specific security and pri-
vacy architectures.
KEYWORDS
Deepfakes, Blind Detection, Machine Learning, Clustering
ACM Reference Format:
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal
Viswanath. 2020. NoiseScope: Detecting Deepfake Images in a Blind Set-
ting. In Annual Computer Security Applications Conference (ACSAC 2020),
December 7–11, 2020, Austin, USA. ACM, New York, NY, USA, 15 pages.
https://doi.org/10.1145/3427228.3427285
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ACSAC 2020, December 7–11, 2020, Austin, USA
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8858-0/20/12...$15.00
https://doi.org/10.1145/3427228.3427285
Figure 1: Advances in GANs over the years.
1 INTRODUCTION
Recently, the machine learning community has made significant
advances in deep generative models. A landmark paper by Goodfel-
low et al. proposed the Generative Adversarial Network (GAN) in
2014 [33]. This work triggered immense interest in developing and
improving GAN models. Today, such generative models can gener-
ate convincing images [39, 40], videos [75], text [34] and audio [22].
Figure 1 shows quality of images generated by GANs over the
years. These efforts were primarily motivated by different benign
use cases, e.g., to augment datasets using synthetic samples to train
better models [26], for de-identification [42], feature extraction [5],
video prediction [46], and image editing [58]. However, govern-
ments [16, 73] and industry [25] are realizing the dual-use nature
of such powerful methods—fake content or deepfakes produced by
generative models can also be used for malicious purposes.
Today, we see many instances of misuse of deepfakes, including
fake pornographic videos and images of celebrities, and ordinary
people [24, 64], fake audios of people saying things they never
said [50]. Industry and governments are also concerned about deep-
fakes being used for large disinformation campaigns on social media
platforms to manipulate elections, trigger hate and violence against
minorities, and to create unrest in society [60]. Deepfakes can be
a threat beyond the web as well. Recent work showed how GANs
can be used to create deepfake medical images to mislead medical
professionals, and ML-based diagnostic tools [51].
In this work, we take a step towards defending against such
threats by building a deepfake detection scheme. We focus on
deepfake images generated by GANs, which is the state-of-the-
art method for generating photorealistic images. Most prior work
on detecting fake (GAN generated) images are supervised meth-
ods that require a priori access to fake images, or their generative
models [52]. However, supervised schemes usually do not gener-
alize well to datasets they are not trained on, and access to fake
images for training can be limited in practice. Instead, we focus on
advancing the state-of-the-art in blind detection of fake images. Our
DCGAN	[65]2016CoGAN	[47]	2016PGGAN	[39]	2018StyleGAN	[40]	2019GAN	[33]2014913ACSAC 2020, December 7–11, 2020, Austin, USA
Jiameng Pu, Neal Mangaokar, Bolun Wang, Chandan K. Reddy, and Bimal Viswanath
scheme, called NoiseScope, can accurately detect fake images with-
out requiring any a priori access to fake images or their generative
schemes for training.
Our work is inspired by prior work in camera fingerprinting [11,
12, 32, 49], and includes the following key contributions:
1○ Similar to images produced by cameras, we find that fake im-
ages contain unique low-level noise patterns that are tied to the
GAN model that generated it. Such patterns are correlated with
the deconvolution layers of GANs. 2○ We present the design and
implementation of NoiseScope, a blind detection scheme that lever-
ages unique patterns in fake images left by the generative model.
Given a test set with unknown number of fake and real (produced
by camera) images, NoiseScope extracts any available model finger-
prints or patterns that identify a GAN and uses the fingerprint to
detect fake images in that set. In contrast to supervised schemes,
our method is agnostic to the type of GAN used, and is also effec-
tive when the test set contains images from multiple GANS. Our
method also works for any type of high-level image content, as it
only extracts low-level noise patterns. 3○ We evaluate NoiseScope
on 11 diverse deepfake image datasets, created using 4 high quality
GAN models. NoiseScope can detect fake images with up to 99.68%
F1 score. 4○ Lastly, we extensively evaluate NoiseScope against a
variety of countermeasures by assuming an attacker who is aware
of NoiseScope’s detection pipeline.
Considering the rate at which new generative models (GANs)
are being proposed, supervised learning strategies will likely tip
the arms race in favor of the attacker. Therefore, there is an urgent
need to advance blind detection schemes that can, in theory, work
against a wide range of GAN models. The source code of NoiseScope
is available at GitHub1, and we hope NoiseScope inspires more work
on deepfake detection.
2 BACKGROUND & RELATED WORK
In this work, we focus on images, and consider deepfake images as
those produced by machine learning algorithms, more specifically,
GANs. GAN models are capable of producing high-quality images.
In fact, humans find it hard to distinguish deepfake images from
real images [51]. We encourage the reader to look at the following
website 2 that presents a new fake image on each page refresh,
created using the StyleGAN [40]. In the rest of the paper, we will
interchangeably use the term deepfake or fake image to refer to such
content. Images produced by traditional imaging devices (cameras)
are called real images.
2.1 Deepfake Generation Methods
Deepfakes are primarily enabled by the family of deep generative
models. Given a training set of images, a generative model can
learn the distribution of the data and produce new images with
variations. Two popular approaches include Generative Adversarial
Networks (GANs) [33], and Variational Autoencoders (VAEs) [41].
We focus on deepfakes generated by GANs, because GANs have
shown impressive performance over the last few years.
In 2014, Goodfellow et al. [33] proposed the Gener-
GAN Basics.
ative Adversarial Network (GAN). A GAN is designed using two
1https://github.com/jmpu/NoiseScope
2https://thispersondoesnotexist.com/
neural networks, a generator (G) that produces fake images, and
a discriminator (D) that takes the fake image and gives feedback
to the generator on how well it resembles a real image. The two
components are trained simultaneously in an adversarial manner
such that the generator learns to produce fake images that are in-
distinguishable from real images, and the discriminator learns to
distinguish between real and fake images (produced by the genera-
tor). Therefore, the idea is to optimize one main objective—make the
generated images indistinguishable from real images. This training
objective or loss term is called the adversarial loss.
It is important to note the role of deconvolution or upsampling
layers in generative models. An integral component of most genera-
tive models, including VAEs and GANs, is a transposed convolution
layer [23], commonly referred to as deconvolution or upsampling
layer. This is fundamental to building high quality generators, as it
allows for learnable upsampling from a lower dimensional vector
space. In Section 5.1, we demonstrate how the deconvolution layers
can leave distinct patterns in the “noise space” of an image, which
enable us to distinguish between fake and real images.
Choice of GANs. Experimenting with the large number of GANs
in the literature [3, 47, 65, 82] would be impractical. Instead, we
focus on certain key models that significantly raised the bar for
different types of image generation tasks. We focus on deepfakes
generated by CycleGAN [87], PGGAN [39], BigGAN [9], and Style-
GAN [40]. These 4 GANs are briefly discussed below. Figures 8-18
in Appendix A show image samples from all 4 GANs.
CycleGAN [87]. CycleGAN advanced the state-of-the-art in image-
to-image translation when it was proposed, improving over the
previous method Pix2Pix [37]. CycleGAN can translate an image
from one domain to another, e.g., turn an image of a horse to a
zebra. Compared to Pix2Pix, CycleGAN does not require paired
images for training, which is a huge advantage, as paired images
(for two domains) are hard to obtain. From a threat perspective,
image-to-image translation schemes can be used by an attacker in
many ways, e.g., swap faces in an image, insert a new person or
object into a scene.
PGGAN [39]. In 2018, PGGAN demonstrated a huge improvement
in image quality. Previously, GANs were not capable of generating
high resolution images in high quality. The basic idea is to progres-
sively generate higher resolution images, by starting from easier
low-resolution images. PGGAN progressively grows both the gener-
ator and discriminator by adding new layers as training progresses
to produce higher resolution images with more details. PGGAN
is able to produce photo-realistic images at high resolutions, up
to 1024x1024. At the time, PGGAN produced the highest Incep-
tion score of 8.80 for CIFAR10 [43], and also created a high-quality
version of the CelebA dataset [48] at 1024x1024 resolution.
BigGAN [9]. Soon after the introduction of PGGAN, Brock et al.
introduced BigGAN, an attempt to scale up conditional GANs to
develop high quality images on a large number of domains. BigGAN
uses a variety of techniques to improve GAN training and image
quality, including an increased batch size, increase in number of
layer channels, and shared embeddings for batch normalization lay-
ers in the generator. One feature of BigGAN is the “truncation trick”,
whereby using a hyperparameter called the truncation threshold,
one can control the trade-off between image fidelity and variety. A
914NoiseScope: Detecting Deepfake Images in a Blind Setting
ACSAC 2020, December 7–11, 2020, Austin, USA
higher truncation threshold leads to higher variety in generated
images, while a lower threshold boosts fidelity. When evaluated
on the ImageNet dataset, BigGAN produced a very high Inception
Score of 166.5, outperforming SAGAN [82] which had the previous
best Inception Score of 52.52.
In 2019, Karras et al. released StyleGAN, an im-
StyleGAN [40].
provement to PGGAN which incorporates a complete redesign of
the generator architecture. The generator no longer receives as
input a random noise vector, but a style vector generated by a noise-
to-style CNN mapping network. Other changes include a change in
the upsampling technique, and addition of noise to feature maps in
the convolutional layers. This redesign allows fine-grained control
over style of the generated image, while simultaneously retaining
and improving upon the high-quality output of PGGAN. Having
fine-grained control over style of the generated image is important
from an attack perspective.
2.2 Deepfake Detection Methods
Prior work on deepfake detection has investigated both supervised
and blind detection schemes. In a supervised scheme, the defender
has access to both real and fake content (or has knowledge of the
generative model) and can use this labelled data to train a classifi-
cation algorithm. In blind detection, the defender has no a priori
access to fake content (or generative methods employed), and only
has access to real content. Most prior work has employed supervised
schemes, and limited efforts have been made towards advancing
blind detection schemes. Consequently, the performance of such
schemes has evolved considerably, and the release of effective DNN
models that facilitate improved feature learning has only furthered
this progress. However, the dominant performance of supervised
learning comes with notable caveats.
In practice, it is hard to obtain a priori access to fake content,
or knowledge of the generative model. However, even with such
presumption, supervised schemes suffer from a fatal inability to
generalize. More specifically, we observe that such schemes are
designed for and thus trained on a limited set of deepfakes (gen-
erated by specific generative models), and do not generalize well
when evaluated against deepfakes produced by other models. In
Section 5.2, we demonstrate this inability to generalize.
A blind detection scheme aims to solve this problem by not
requiring a priori access to fake images for training, while being
able to detect fake images from a wide variety of sources (GANs).
An accompanying difficulty of blind design is a potential decrease
in performance when compared to existing supervised classifiers.
NoiseScope aims to advance the state-of-the-art for blind detection
schemes by offering a performant detection scheme. NoiseScope
complements the supervised detection schemes from prior work,
allowing for potentially hybrid ensembles that feature the best of
both worlds.
Supervised methods. One set of approaches focus on building a
supervised classifier with input image features crafted from specific
vector spaces. Examples include Marra et al.’s [52] proposition of
using raw pixel and conventional forensics features, and Nataraj
et al.’s [57] extraction of pre-computed RGB pixel co-occurrence
matrices to capture distinguishing features. Feature engineering in
multiple color spaces has also been explored. Li et al. proposed a fea-
ture set capturing disparities in color spaces between real and fake
images and then using such features to perform classification [45].
Prior work observed that, similar to cameras, GANs also leave
unique fingerprints in the images. Marra et al. [53] extracted GAN
model fingerprints using techniques from the camera fingerprint-
ing literature [12, 49], and implemented a supervised scheme to
detect fake images. Another approach by Yu et al. [80] used a su-
pervised deep learning scheme to learn GAN model fingerprints,
and attribute images to GANs. Yu et al.’s approach primarily fo-
cused on attributing fake images to different GANs. Albright and
McCloskey [2] also worked on attributing images to GANs by lever-
aging generator inversion schemes [19]. Our work also aims to
identify model fingerprints to detect fake images but does so in a
blind manner.
Domain-specific inconsistencies can also be used to detect deep-
fakes. Yang et al. [78] focused on deep fakes generated by splicing
synthesized face regions into a real image. They show that such
splicing introduces errors when 3D head poses are estimated from
the fake images. An SVM-based classifier is trained to learn such
errors to distinguish between real and fake images.
Other supervised approaches leverage DNNs to automatically
extract features relevant for classification. Mo et al. [55] developed
a CNN-based model to detect face images generated by PGGAN.
Rossler et al. compared 5 CNN-based classification architectures by
learning extracted face regions [69]. Tariq et al. [72] propose using
ensembles of various CNN-based classifiers to detect GAN gener-
ated face images. Concurrent to our work, Wang et al. [77] proposed
a classifier based on the ResNet-50 architecture that is trained on a
large number of fake images from a single GAN, with carefully cho-
sen data augmentation schemes. Afchar et al. [1] designed MesoNet
based on Inception blocks to detect deepfakes showing impressive