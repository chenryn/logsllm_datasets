record, it updates these two values, indicating that changes have been made
to the volume and flushed out to disk—meaning that the file system is fully
consistent up to the new restart LSN.
TxF also writes compensating log records, or CLRs. These records store
the actions that are being performed during transaction rollback. They’re
primarily used to store the undo-next LSN, which allows the recovery process
to avoid repeated undo operations by bypassing undo records that have
already been processed, a situation that can happen if the system fails during
the recovery phase and has already performed part of the undo pass. Finally,
TxF also deals with prepare records, abort records, and commit records,
which describe the state of the KTM transactions related to TxF.
NTFS recovery support
NTFS recovery support ensures that if a power failure or a system failure
occurs, no file system operations (transactions) will be left incomplete, and
the structure of the disk volume will remain intact without the need to run a
disk repair utility. The NTFS Chkdsk utility is used to repair catastrophic disk
corruption caused by I/O errors (bad disk sectors, electrical anomalies, or
disk failures, for example) or software bugs. But with the NTFS recovery
capabilities in place, Chkdsk is rarely needed.
As mentioned earlier (in the section “Recoverability”), NTFS uses a
transaction-processing scheme to implement recoverability. This strategy
ensures a full disk recovery that is also extremely fast (on the order of
seconds) for even the largest disks. NTFS limits its recovery procedures to
file system data to ensure that at the very least the user will never lose a
volume because of a corrupted file system; however, unless an application
takes specific action (such as flushing cached files to disk), NTFS’s recovery
support doesn’t guarantee user data to be fully updated if a crash occurs. This
is the job of transactional NTFS (TxF).
The following sections detail the transaction-logging scheme NTFS uses to
record modifications to file system data structures and explain how NTFS
recovers a volume if the system fails.
Design
NTFS implements the design of a recoverable file system. These file systems
ensure volume consistency by using logging techniques (sometimes called
journaling) originally developed for transaction processing. If the operating
system crashes, the recoverable file system restores consistency by executing
a recovery procedure that accesses information that has been stored in a log
file. Because the file system has logged its disk writes, the recovery
procedure takes only seconds, regardless of the size of the volume (unlike in
the FAT file system, where the repair time is related to the volume size). The
recovery procedure for a recoverable file system is exact, guaranteeing that
the volume will be restored to a consistent state.
A recoverable file system incurs some costs for the safety it provides.
Every transaction that alters the volume structure requires that one record be
written to the log file for each of the transaction’s suboperations. This
logging overhead is ameliorated by the file system’s batching of log records
—writing many records to the log file in a single I/O operation. In addition,
the recoverable file system can employ the optimization techniques of a lazy
write file system. It can even increase the length of the intervals between
cache flushes because the file system metadata can be recovered if the system
crashes before the cache changes have been flushed to disk. This gain over
the caching performance of lazy write file systems makes up for, and often
exceeds, the overhead of the recoverable file system’s logging activity.
Neither careful write nor lazy write file systems guarantee protection of
user file data. If the system crashes while an application is writing a file, the
file can be lost or corrupted. Worse, the crash can corrupt a lazy write file
system, destroying existing files or even rendering an entire volume
inaccessible.
The NTFS recoverable file system implements several strategies that
improve its reliability over that of the traditional file systems. First, NTFS
recoverability guarantees that the volume structure won’t be corrupted, so all
files will remain accessible after a system failure. Second, although NTFS
doesn’t guarantee protection of user data in the event of a system crash—
some changes can be lost from the cache—applications can take advantage of
the NTFS write-through and cache-flushing capabilities to ensure that file
modifications are recorded on disk at appropriate intervals.
Both cache write-through—forcing write operations to be immediately
recorded on disk—and cache flushing—forcing cache contents to be written
to disk—are efficient operations. NTFS doesn’t have to do extra disk I/O to
flush modifications to several different file system data structures because
changes to the data structures are recorded—in a single write operation—in
the log file; if a failure occurs and cache contents are lost, the file system
modifications can be recovered from the log. Furthermore, unlike the FAT
file system, NTFS guarantees that user data will be consistent and available
immediately after a write-through operation or a cache flush, even if the
system subsequently fails.
Metadata logging
NTFS provides file system recoverability by using the same logging
technique used by TxF, which consists of recording all operations that
modify file system metadata to a log file. Unlike TxF, however, NTFS’s
built-in file system recovery support doesn’t make use of CLFS but uses an
internal logging implementation called the log file service (which is not a
background service process as described in Chapter 10). Another difference
is that while TxF is used only when callers opt in for transacted operations,
NTFS records all metadata changes so that the file system can be made
consistent in the face of a system failure.
Log file service
The log file service (LFS) is a series of kernel-mode routines inside the NTFS
driver that NTFS uses to access the log file. NTFS passes the LFS a pointer
to an open file object, which specifies a log file to be accessed. The LFS
either initializes a new log file or calls the Windows cache manager to access
the existing log file through the cache, as shown in Figure 11-56. Note that
although LFS and CLFS have similar sounding names, they’re separate
logging implementations used for different purposes, although their operation
is similar in many ways.
Figure 11-56 Log file service (LFS).
The LFS divides the log file into two regions: a restart area and an
“infinite” logging area, as shown in Figure 11-57.
Figure 11-57 Log file regions.
NTFS calls the LFS to read and write the restart area. NTFS uses the
restart area to store context information such as the location in the logging
area at which NTFS begins to read during recovery after a system failure.
The LFS maintains a second copy of the restart data in case the first becomes
corrupted or otherwise inaccessible. The remainder of the log file is the
logging area, which contains transaction records NTFS writes to recover a
volume in the event of a system failure. The LFS makes the log file appear
infinite by reusing it circularly (while guaranteeing that it doesn’t overwrite
information it needs). Just like CLFS, the LFS uses LSNs to identify records
written to the log file. As the LFS cycles through the file, it increases the
values of the LSNs. NTFS uses 64 bits to represent LSNs, so the number of
possible LSNs is so large as to be virtually infinite.
NTFS never reads transactions from or writes transactions to the log file
directly. The LFS provides services that NTFS calls to open the log file,
write log records, read log records in forward or backward order, flush log
records up to a specified LSN, or set the beginning of the log file to a higher
LSN. During recovery, NTFS calls the LFS to perform the same actions as
described in the TxF recovery section: a redo pass for nonflushed committed
changes, followed by an undo pass for noncommitted changes.
Here’s how the system guarantees that the volume can be recovered:
1. 
NTFS first calls the LFS to record in the (cached) log file any
transactions that will modify the volume structure.
2. 
NTFS modifies the volume (also in the cache).
3. 
The cache manager prompts the LFS to flush the log file to disk. (The
LFS implements the flush by calling the cache manager back, telling
it which pages of memory to flush. Refer back to the calling sequence
shown in Figure 11-56.)
4. 
After the cache manager flushes the log file to disk, it flushes the
volume changes (the metadata operations themselves) to disk.
These steps ensure that if the file system modifications are ultimately
unsuccessful, the corresponding transactions can be retrieved from the log
file and can be either redone or undone as part of the file system recovery
procedure.
File system recovery begins automatically the first time the volume is used
after the system is rebooted. NTFS checks whether the transactions that were
recorded in the log file before the crash were applied to the volume, and if
they weren’t, it redoes them. NTFS also guarantees that transactions not
completely logged before the crash are undone so that they don’t appear on
the volume.
Log record types
The NTFS recovery mechanism uses similar log record types as the TxF
recovery mechanism: update records, which correspond to the redo and undo
records that TxF uses, and checkpoint records, which are similar to the restart
records used by TxF. Figure 11-58 shows three update records in the log file.
Each record represents one suboperation of a transaction, creating a new file.
The redo entry in each update record tells NTFS how to reapply the
suboperation to the volume, and the undo entry tells NTFS how to roll back
(undo) the suboperation.
Figure 11-58 Update records in the log file.
After logging a transaction (in this example, by calling the LFS to write
the three update records to the log file), NTFS performs the suboperations on
the volume itself, in the cache. When it has finished updating the cache,
NTFS writes another record to the log file, recording the entire transaction as
complete—a suboperation known as committing a transaction. Once a
transaction is committed, NTFS guarantees that the entire transaction will
appear on the volume, even if the operating system subsequently fails.
When recovering after a system failure, NTFS reads through the log file
and redoes each committed transaction. Although NTFS completed the
committed transactions from before the system failure, it doesn’t know
whether the cache manager flushed the volume modifications to disk in time.
The updates might have been lost from the cache when the system failed.
Therefore, NTFS executes the committed transactions again just to be sure
that the disk is up to date.
After redoing the committed transactions during a file system recovery,
NTFS locates all the transactions in the log file that weren’t committed at
failure and rolls back each suboperation that had been logged. In Figure 11-
58, NTFS would first undo the T1c suboperation and then follow the
backward pointer to T1b and undo that suboperation. It would continue to
follow the backward pointers, undoing suboperations, until it reached the first
suboperation in the transaction. By following the pointers, NTFS knows how
many and which update records it must undo to roll back a transaction.
Redo and undo information can be expressed either physically or logically.
As the lowest layer of software maintaining the file system structure, NTFS
writes update records with physical descriptions that specify volume updates
in terms of particular byte ranges on the disk that are to be changed, moved,
and so on, unlike TxF, which uses logical descriptions that express updates
in terms of operations such as “delete file A.dat.” NTFS writes update
records (usually several) for each of the following transactions:
■    Creating a file
■    Deleting a file
■    Extending a file
■    Truncating a file
■    Setting file information
■    Renaming a file
■    Changing the security applied to a file
The redo and undo information in an update record must be carefully
designed because although NTFS undoes a transaction, recovers from a
system failure, or even operates normally, it might try to redo a transaction
that has already been done or, conversely, to undo a transaction that never
occurred or that has already been undone. Similarly, NTFS might try to redo
or undo a transaction consisting of several update records, only some of
which are complete on disk. The format of the update records must ensure
that executing redundant redo or undo operations is idempotent—that is, has
a neutral effect. For example, setting a bit that is already set has no effect, but
toggling a bit that has already been toggled does. The file system must also
handle intermediate volume states correctly.
In addition to update records, NTFS periodically writes a checkpoint
record to the log file, as illustrated in Figure 11-59.
Figure 11-59 Checkpoint record in the log file.
A checkpoint record helps NTFS determine what processing would be
needed to recover a volume if a crash were to occur immediately. Using
information stored in the checkpoint record, NTFS knows, for example, how
far back in the log file it must go to begin its recovery. After writing a
checkpoint record, NTFS stores the LSN of the record in the restart area so
that it can quickly find its most recently written checkpoint record when it
begins file system recovery after a crash occurs; this is similar to the restart
LSN used by TxF for the same reason.
Although the LFS presents the log file to NTFS as if it were infinitely
large, it isn’t. The generous size of the log file and the frequent writing of
checkpoint records (an operation that usually frees up space in the log file)
make the possibility of the log file filling up a remote one. Nevertheless, the
LFS, just like CLFS, accounts for this possibility by tracking several
operational parameters:
■    The available log space
■    The amount of space needed to write an incoming log record and to
undo the write, should that be necessary
■    The amount of space needed to roll back all active (noncommitted)
transactions, should that be necessary
If the log file doesn’t contain enough available space to accommodate the
total of the last two items, the LFS returns a “log file full” error, and NTFS
raises an exception. The NTFS exception handler rolls back the current
transaction and places it in a queue to be restarted later.
To free up space in the log file, NTFS must momentarily prevent further
transactions on files. To do so, NTFS blocks file creation and deletion and
then requests exclusive access to all system files and shared access to all user
files. Gradually, active transactions either are completed successfully or
receive the “log file full” exception. NTFS rolls back and queues the
transactions that receive the exception.
Once it has blocked transaction activity on files as just described, NTFS
calls the cache manager to flush unwritten data to disk, including unwritten
log file data. After everything is safely flushed to disk, NTFS no longer
needs the data in the log file. It resets the beginning of the log file to the
current position, making the log file “empty.” Then it restarts the queued
transactions. Beyond the short pause in I/O processing, the log file full error
has no effect on executing programs.
This scenario is one example of how NTFS uses the log file not only for
file system recovery but also for error recovery during normal operation. You
find out more about error recovery in the following section.
Recovery
NTFS automatically performs a disk recovery the first time a program
accesses an NTFS volume after the system has been booted. (If no recovery
is needed, the process is trivial.) Recovery depends on two tables NTFS
maintains in memory: a transaction table, which behaves just like the one
TxF maintains, and a dirty page table, which records which pages in the
cache contain modifications to the file system structure that haven’t yet been
written to disk. This data must be flushed to disk during recovery.
NTFS writes a checkpoint record to the log file once every 5 seconds. Just
before it does, it calls the LFS to store a current copy of the transaction table
and of the dirty page table in the log file. NTFS then records in the
checkpoint record the LSNs of the log records containing the copied tables.
When recovery begins after a system failure, NTFS calls the LFS to locate
the log records containing the most recent checkpoint record and the most
recent copies of the transaction and dirty page tables. It then copies the tables
to memory.
The log file usually contains more update records following the last
checkpoint record. These update records represent volume modifications that
occurred after the last checkpoint record was written. NTFS must update the
transaction and dirty page tables to include these operations. After updating
the tables, NTFS uses the tables and the contents of the log file to update the
volume itself.
To perform its volume recovery, NTFS scans the log file three times,
loading the file into memory during the first pass to minimize disk I/O. Each
pass has a particular purpose:
1. 
Analysis
2. 
Redoing transactions
3. 
Undoing transactions
Analysis pass
During the analysis pass, as shown in Figure 11-60, NTFS scans forward in
the log file from the beginning of the last checkpoint operation to find update
records and use them to update the transaction and dirty page tables it copied
to memory. Notice in the figure that the checkpoint operation stores three
records in the log file and that update records might be interspersed among
these records. NTFS therefore must start its scan at the beginning of the
checkpoint operation.
Figure 11-60 Analysis pass.
Most update records that appear in the log file after the checkpoint
operation begins represent a modification to either the transaction table or the
dirty page table. If an update record is a “transaction committed” record, for
example, the transaction the record represents must be removed from the
transaction table. Similarly, if the update record is a page update record that
modifies a file system data structure, the dirty page table must be updated to
reflect that change.
Once the tables are up to date in memory, NTFS scans the tables to
determine the LSN of the oldest update record that logs an operation that
hasn’t been carried out on disk. The transaction table contains the LSNs of
the noncommitted (incomplete) transactions, and the dirty page table contains
the LSNs of records in the cache that haven’t been flushed to disk. The LSN
of the oldest update record that NTFS finds in these two tables determines
where the redo pass will begin. If the last checkpoint record is older,
however, NTFS will start the redo pass there instead.
 Note
In the TxF recovery model, there is no distinct analysis pass. Instead, as
described in the TxF recovery section, TxF performs the equivalent work
in the redo pass.
Redo pass
During the redo pass, as shown in Figure 11-61, NTFS scans forward in the
log file from the LSN of the oldest update record, which it found during the
analysis pass. It looks for page update records, which contain volume
modifications that were written before the system failure but that might not
have been flushed to disk. NTFS redoes these updates in the cache.
Figure 11-61 Redo pass.
When NTFS reaches the end of the log file, it has updated the cache with
the necessary volume modifications, and the cache manager’s lazy writer can
begin writing cache contents to disk in the background.
Undo pass
After it completes the redo pass, NTFS begins its undo pass, in which it rolls
back any transactions that weren’t committed when the system failed. Figure
11-62 shows two transactions in the log file; transaction 1 was committed
before the power failure, but transaction 2 wasn’t. NTFS must undo
transaction 2.
Figure 11-62 Undo pass.
Suppose that transaction 2 created a file, an operation that comprises three
suboperations, each with its own update record. The update records of a
transaction are linked by backward pointers in the log file because they aren’t
usually contiguous.
The NTFS transaction table lists the LSN of the last-logged update record
for each noncommitted transaction. In this example, the transaction table
identifies LSN 4049 as the last update record logged for transaction 2. As
shown from right to left in Figure 11-63, NTFS rolls back transaction 2.
Figure 11-63 Undoing a transaction.
After locating LSN 4049, NTFS finds the undo information and executes
it, clearing bits 3 through 9 in its allocation bitmap. NTFS then follows the
backward pointer to LSN 4048, which directs it to remove the new file name
from the appropriate file name index. Finally, it follows the last backward
pointer and deallocates the MFT file record reserved for the file, as the
update record with LSN 4046 specifies. Transaction 2 is now rolled back. If
there are other noncommitted transactions to undo, NTFS follows the same
procedure to roll them back. Because undoing transactions affects the
volume’s file system structure, NTFS must log the undo operations in the log
file. After all, the power might fail again during the recovery, and NTFS
would have to redo its undo operations!
When the undo pass of the recovery is finished, the volume has been
restored to a consistent state. At this point, NTFS is prepared to flush the
cache changes to disk to ensure that the volume is up to date. Before doing
so, however, it executes a callback that TxF registers for notifications of LFS
flushes. Because TxF and NTFS both use write-ahead logging, TxF must
flush its log through CLFS before the NTFS log is flushed to ensure
consistency of its own metadata. (And similarly, the TOPS file must be
flushed before the CLFS-managed log files.) NTFS then writes an “empty”
LFS restart area to indicate that the volume is consistent and that no recovery
need be done if the system should fail again immediately. Recovery is
complete.
NTFS guarantees that recovery will return the volume to some preexisting
consistent state, but not necessarily to the state that existed just before the
system crash. NTFS can’t make that guarantee because, for performance, it
uses a lazy commit algorithm, which means that the log file isn’t
immediately flushed to disk each time a transaction committed record is
written. Instead, numerous transaction committed records are batched and
written together, either when the cache manager calls the LFS to flush the log
file to disk or when the LFS writes a checkpoint record (once every 5
seconds) to the log file. Another reason the recovered volume might not be
completely up to date is that several parallel transactions might be active
when the system crashes, and some of their transaction committed records
might make it to disk, whereas others might not. The consistent volume that
recovery produces includes all the volume updates whose transaction
committed records made it to disk and none of the updates whose transaction