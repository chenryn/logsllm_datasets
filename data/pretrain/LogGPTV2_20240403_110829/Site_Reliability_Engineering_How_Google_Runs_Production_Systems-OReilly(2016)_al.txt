{var=dc:http_responses:rate10m,job=webserver,code=500, ...} 0.6
{var=dc:http_errors:rate10m,job=webserver}
{var=dc:http_errors:rate10m,job=webserver, ...} 0.614 The service and zone labels are elided for space.
Rule Evaluation  |  117
{var=dc:http_errors:ratio_rate10m,job=webserver}
{var=dc:http_errors:ratio_rate10m,job=webserver} 0.15
 The preceding output shows the intermediate query in the dc:http_errors:rate10m rule that filters the non-200 error codes. Though the value of the expressions are the same, observe that the code label is retained in one but removed from the other.As mentioned previously, Borgmon rules create new time-series, so the results of the computations are kept in the time-series arena and can be inspected just as the source time-series are. The ability to do so allows for ad hoc querying, evaluation, and explo‐ration as tables or charts. This is a useful feature for debugging while on-call, and if these ad hoc queries prove useful, they can be made permanent visualizations on a service console.Alerting
When an alerting rule is evaluated by a Borgmon, the result is either true, in which case the alert is triggered, or false. Experience shows that alerts can “flap” (toggle their state quickly); therefore, the rules allow a minimum duration for which the alerting rule must be true before the alert is sent. Typically, this duration is set to at least two rule evaluation cycles to ensure no missed collections cause a false alert.The following example creates an alert when the error ratio over 10 minutes exceeds 1% and the total number of errors exceeds 1:
rules  0.01
 and by job, error
 {var=dc:http_errors:rate10m,job=webserver} > 1
 for 2m
 => ErrorRatioTooHigh
 details "webserver error ratio at [[trigger_value]]"
 labels {severity=page};
>>>labels {severity=page};
>>>
Our example holds the ratio rate at 0.15, which is well over the threshold of 0.01 in the alerting rule. However, the number of errors is not greater than 1 at this moment, so the alert won’t be active. Once the number of errors exceeds 1, the alert will go pending for two minutes to ensure it isn’t a transient state, and only then will it fire.The alert rule contains a small template for filling out a message containing contex‐tual information: which job the alert is for, the name of the alert, the numerical value of the triggering rule, and so on. The contextual information is filled out by Borgmon when the alert fires and is sent in the Alert RPC.
118  |  Chapter 10: Practical Alerting from Time-Series DataBorgmon is connected to a centrally run service, known as the Alertmanager, which receives Alert RPCs when the rule first triggers, and then again when the alert is con‐sidered to be “firing.” The Alertmanager is responsible for routing the alert notifica‐tion to the correct destination. Alertmanager can be configured to do the following:
• Inhibit certain alerts when others are active• Deduplicate alerts from multiple Borgmon that have the same labelsets
• Fan-in or fan-out alerts based on their labelsets when multiple alerts with similar 	labelsets fire
As described in Chapter 6, teams send their page-worthy alerts to their on-call rota‐tion and their important but subcritical alerts to their ticket queues. All other alerts should be retained as informational data for status dashboards.A more comprehensive guide to alert design can be found in Chapter 4.
Sharding the Monitoring TopologyA Borgmon can import time-series data from other Borgmon, as well. While one could attempt to collect from all tasks in a service globally, doing so quickly becomes a scaling bottleneck and introduces a single point of failure into the design. Instead, a streaming protocol is used to transmit time-series data between Borgmon, saving CPU time and network bytes compared to the text-based varz format. A typical such deployment uses two or more global Borgmon for top-level aggregation and one Borgmon in each datacenter to monitor all the jobs running at that location. (Google divides the production network into zones for production changes, so having two or more global replicas provides diversity in the face of maintenance and outages for this otherwise single point of failure.)As shown in Figure 10-3, more complicated deployments shard the datacenter Borg‐mon further into a purely scraping-only layer (often due to RAM and CPU con‐straints in a single Borgmon for very large services) and a DC aggregation layer that performs mostly rule evaluation for aggregation. Sometimes the global layer is split between rule evaluation and dashboarding. Upper-tier Borgmon can filter the data they want to stream from the lower-tier Borgmon, so that the global Borgmon does not fill its arena with all the per-task time-series from the lower tiers. Thus, the aggre‐gation hierarchy builds local caches of relevant time-series that can be drilled down into when required.Sharding the Monitoring Topology  |  119
Figure 10-3. A data flow model of a hierarchy of Borgmon in three clusters
Black-Box Monitoring
Borgmon is a white-box monitoring system—it inspects the internal state of the tar‐get service, and the rules are written with knowledge of the internals in mind. The transparent nature of this model provides great power to identify quickly what com‐ponents are failing, which queues are full, and where bottlenecks occur, both when responding to an incident and when testing a new feature deployment.However, white-box monitoring does not provide a full picture of the system being monitored; relying solely upon white-box monitoring means that you aren’t aware of what the users see. You only see the queries that arrive at the target; the queries that never make it due to a DNS error are invisible, while queries lost due to a server crash never make a sound. You can only alert on the failures that you expected.Teams at Google solve this coverage issue with Prober, which runs a protocol check against a target and reports success or failure. The prober can send alerts directly to Alertmanager, or its own varz can be collected by a Borgmon. Prober can validate the response payload of the protocol (e.g., the HTML contents of an HTTP response) and validate that the contents are expected, and even extract and export values as time-series. Teams often use Prober to export histograms of response times by operation type and payload size so that they can slice and dice the user-visible performance. Prober is a hybrid of the check-and-test model with some richer variable extraction to create time-series.120  |  Chapter 10: Practical Alerting from Time-Series Data
Prober can be pointed at either the frontend domain or behind the load balancer. By using both targets, we can detect localized failures and suppress alerts. For example, we might monitor both the load balanced www.google.com and the web servers in each datacenter behind the load balancer. This setup allows us to either know that traffic is still served when a datacenter fails, or to quickly isolate an edge in the traffic flow graph where a failure has occurred.Maintaining the Configuration
Borgmon configuration separates the definition of the rules from the targets being monitored. This means the same sets of rules can be applied to many targets at once, instead of writing nearly identical configuration over and over. This separation of concerns might seem incidental, but it greatly reduces the cost of maintaining the monitoring by avoiding lots of repetition in describing the target systems.Borgmon also supports language templates. This macro-like system enables engineers to construct libraries of rules that can be reused. This functionality again reduces rep‐etition, thus reducing the likelihood of bugs in the configuration.Of course, any high-level programming environment creates the opportunity for complexity, so Borgmon provides a way to build extensive unit and regression tests by synthesizing time-series data, in order to ensure that the rules behave as the author thinks they do. The Production Monitoring team runs a continuous integration ser‐vice that executes a suite of these tests, packages the configuration, and ships the con‐figuration to all the Borgmon in production, which then validate the configuration before accepting it.In the vast library of common templates that have been created, two classes of moni‐toring configuration have emerged. The first class simply codifies the emergent schema of variables exported from a given library of code, such that any user of the library can reuse the template of its varz. Such templates exist for the HTTP server library, memory allocation, the storage client library, and generic RPC services, among others. (While the varz interface declares no schema, the rule library associ‐ated with the code library ends up declaring a schema.)The second class of library emerged as we built templates to manage the aggregation of data from a single-server task to the global service footprint. These libraries con‐tain generic aggregation rules for exported variables that engineers can use to model the topology of their service.
Maintaining the Configuration  |  121For example, a service may provide a single global API, but be homed in many data‐centers. Within each datacenter, the service is composed of several shards, and each shard is composed of several jobs with various numbers of tasks. An engineer can model this breakdown with Borgmon rules so that when debugging, subcomponents can be isolated from the rest of the system. These groupings typically follow the shared fate of components; e.g., individual tasks share fate due to configuration files, jobs in a shard share fate because they’re homed in the same datacenter, and physical sites share fate due to networking.Labeling conventions make such division possible: a Borgmon adds labels indicating the target’s instance name and the shard and datacenter it occupies, which can be used to group and aggregate those time-series together.
Thus, we have multiple uses for labels on a time-series, though all are interchangea‐ble:
• Labels that define breakdowns of the data itself (e.g., our HTTP response code on 	the http_responses variable)• Labels that define the source of the data (e.g., the instance or job name)
• Labels that indicate the locality or aggregation of the data within the service as a whole (e.g., the zone label describing a physical location, a shard label describing a logical grouping of tasks)
The templated nature of these libraries allows flexibility in their use. The same tem‐plate can be used to aggregate from each tier.Ten Years On…
Borgmon transposed the model of check-and-alert per target into mass variable col‐lection and a centralized rule evaluation across the time-series for alerting and diagnostics.
This decoupling allows the size of the system being monitored to scale independently of the size of alerting rules. These rules cost less to maintain because they’re abstrac‐ted over a common time-series format. New applications come ready with metric exports in all components and libraries to which they link, and well-traveled aggrega‐tion and console templates, which further reduces the burden of implementation.Ensuring that the cost of maintenance scales sublinearly with the size of the service is key to making monitoring (and all sustaining operations work) maintainable. This theme recurs in all SRE work, as SREs work to scale all aspects of their work to the global scale.
122  |  Chapter 10: Practical Alerting from Time-Series DataTen years is a long time, though, and of course today the shape of the monitoring landscape within Google has evolved with experiments and changes, striving for con‐tinual improvement as the company grows.
Even though Borgmon remains internal to Google, the idea of treating time-series data as a data source for generating alerts is now accessible to everyone through those open source tools like Prometheus, Riemann, Heka, and Bosun, and probably others by the time you read this.Ten Years On…  |  123
CHAPTER 11
Being On-Call
Written by Andrea Spadaccini1 
Edited by Kavita Guliani
Being on-call is a critical duty that many operations and engineering teams must undertake in order to keep their services reliable and available. However, there are several pitfalls in the organization of on-call rotations and responsibilities that can lead to serious consequences for the services and for the teams if not avoided. This chapter describes the primary tenets of the approach to on-call that Google’s Site Reli‐ability Engineers (SREs) have developed over years, and explains how that approach has led to reliable services and sustainable workload over time.Introduction
Several professions require employees to perform some sort of on-call duty, which entails being available for calls during both working and nonworking hours. In the IT context, on-call activities have historically been performed by dedicated Ops teams tasked with the primary responsibility of keeping the service(s) for which they are responsible in good health.Many important services in Google, e.g., Search, Ads, and Gmail, have dedicated teams of SREs responsible for the performance and reliability of these services. Thus, SREs are on-call for the services they support. The SRE teams are quite different from purely operational teams in that they place heavy emphasis on the use of engineering to approach problems. These problems, which typically fall in the operational domain, exist at a scale that would be intractable without software engineering solutions.1 An earlier version of this chapter appeared as an article in ;login: (October 2015, vol. 40, no. 5).
125
To enforce this type of problem solving, Google hires people with a diverse back‐ground in systems and software engineering into SRE teams. We cap the amount of time SREs spend on purely operational work at 50%; at minimum, 50% of an SRE’s time should be allocated to engineering projects that further scale the impact of the team through automation, in addition to improving the service.Life of an On-Call Engineer
This section describes the typical activities of an on-call engineer and provides some background for the rest of the chapter.
As the guardians of production systems, on-call engineers take care of their assigned operations by managing outages that affect the team and performing and/or vetting production changes.When on-call, an engineer is available to perform operations on production systems within minutes, according to the paging response times agreed to by the team and the business system owners. Typical values are 5 minutes for user-facing or otherwise highly time-critical services, and 30 minutes for less time-sensitive systems. The com‐pany provides the page-receiving device, which is typically a phone. Google has flexi‐ble alert delivery systems that can dispatch pages via multiple mechanisms (email, SMS, robot call, app) across multiple devices.Response times are related to desired service availability, as demonstrated by the fol‐lowing simplistic example: if a user-facing system must obtain 4 nines of availability in a given quarter (99.99%), the allowed quarterly downtime is around 13 minutes (Appendix A). This constraint implies that the reaction time of on-call engineers has to be in the order of minutes (strictly speaking, 13 minutes). For systems with more relaxed SLOs, the reaction time can be on the order of tens of minutes.As soon as a page is received and acknowledged, the on-call engineer is expected to triage the problem and work toward its resolution, possibly involving other team members and escalating as needed.
Nonpaging production events, such as lower priority alerts or software releases, can also be handled and/or vetted by the on-call engineer during business hours. These activities are less urgent than paging events, which take priority over almost every other task, including project work. For more insight on interrupts and other non-paging events that contribute to operational load, see Chapter 29.Many teams have both a primary and a secondary on-call rotation. The distribution of duties between the primary and the secondary varies from team to team. One team might employ the secondary as a fall-through for the pages the primary on-call misses. Another team might specify that the primary on-call handles only pages, while the secondary handles all other non-urgent production activities.126  |  Chapter 11: Being On-Call
In teams for which a secondary rotation is not strictly required for duty distribution, it is common for two related teams to serve as secondary on-call for each other, with fall-through handling duties. This setup eliminates the need for an exclusive secon‐dary on-call rotation.
There are many ways to organize on-call rotations; for detailed analysis, refer to the“Oncall” chapter of [Lim14].Balanced On-Call
SRE teams have specific constraints on the quantity and quality of on-call shifts. The quantity of on-call can be calculated by the percent of time spent by engineers on on-call duties. The quality of on-call can be calculated by the number of incidents that occur during an on-call shift.
SRE managers have the responsibility of keeping the on-call workload balanced and sustainable across these two axes.Balance in Quantity
We strongly believe that the “E” in “SRE” is a defining characteristic of our organiza‐tion, so we strive to invest at least 50% of SRE time into engineering: of the remain‐der, no more than 25% can be spent on-call, leaving up to another 25% on other types of operational, nonproject work.Using the 25% on-call rule, we can derive the minimum number of SREs required to sustain a 24/7 on-call rotation. Assuming that there are always two people on-call (primary and secondary, with different duties), the minimum number of engineers needed for on-call duty from a single-site team is eight: assuming week-long shifts, each engineer is on-call (primary or secondary) for one week every month. For dual-site teams, a reasonable minimum size of each team is six, both to honor the 25% rule and to ensure a substantial and critical mass of engineers for the team.If a service entails enough work to justify growing a single-site team, we prefer to cre‐ate a multi-site team. A multi-site team is advantageous for two reasons:
• Night shifts have detrimental effects on people’s health [Dur05], and a multi-site	“follow the sun” rotation allows teams to avoid night shifts altogether.• Limiting the number of engineers in the on-call rotation ensures that engineers do not lose touch with the production systems (see “A Treacherous Enemy: Operational Underload” on page 132).
However, multi-site teams incur communication and coordination overhead. There‐fore, the decision to go multi-site or single-site should be based upon the trade-offs
Balanced On-Call  |  127Balanced On-Call  |  127
each option entails, the importance of the system, and the workload each system generates.
Balance in QualityFor each on-call shift, an engineer should have sufficient time to deal with any inci‐dents and follow-up activities such as writing postmortems [Loo10]. Let’s define an incident as a sequence of events and alerts that are related to the same root cause and would be discussed as part of the same postmortem. We’ve found that on average, dealing with the tasks involved in an on-call incident—root-cause analysis, remedia‐tion, and follow-up activities like writing a postmortem and fixing bugs—takes 6 hours. It follows that the maximum number of incidents per day is 2 per 12-hour on-call shift. In order to stay within this upper bound, the distribution of paging events should be very flat over time, with a likely median value of 0: if a given component or issue causes pages every day (median incidents/day > 1), it is likely that something else will break at some point, thus causing more incidents than should be permitted.If this limit is temporarily exceeded, e.g., for a quarter, corrective measures should be put in place to make sure that the operational load returns to a sustainable state (see“Operational Overload” on page 130 and Chapter 30).
CompensationAdequate compensation needs to be considered for out-of-hours support. Different organizations handle on-call compensation in different ways; Google offers time-off-in-lieu or straight cash compensation, capped at some proportion of overall salary. The compensation cap represents, in practice, a limit on the amount of on-call work that will be taken on by any individual. This compensation structure ensures incen‐tivization to be involved in on-call duties as required by the team, but also promotes a balanced on-call work distribution and limits potential drawbacks of excessive on-call work, such as burnout or inadequate time for project work.Feeling Safe
As mentioned earlier, SRE teams support Google’s most critical systems. Being an SRE on-call typically means assuming responsibility for user-facing, revenue-critical systems or for the infrastructure required to keep these systems up and running. SRE methodology for thinking about and tackling problems is vital for the appropriate operation of services.Modern research identifies two distinct ways of thinking that an individual may, con‐sciously or subconsciously, choose when faced with challenges [Kah11]:
128  |  Chapter 11: Being On-Call
• Intuitive, automatic, and rapid action
• Rational, focused, and deliberate cognitive functions
When one is dealing with the outages related to complex systems, the second of these options is more likely to produce better results and lead to well-planned incident handling.To make sure that the engineers are in the appropriate frame of mind to leverage the latter mindset, it’s important to reduce the stress related to being on-call. The impor‐tance and the impact of the services and the consequences of potential outages can create significant pressure on the on-call engineers, damaging the well-being of indi‐vidual team members and possibly prompting SREs to make incorrect choices that can endanger the availability of the service. Stress hormones like cortisol and corticotropin-releasing hormone (CRH) are known to cause behavioral consequences—including fear—that can impair cognitive functions and cause suboptimal decision making [Chr09].Under the influence of these stress hormones, the more deliberate cognitive approach is typically subsumed by unreflective and unconsidered (but immediate) action, lead‐ing to potential abuse of heuristics. Heuristics are very tempting behaviors when one is on-call. For example, when the same alert pages for the fourth time in the week, and the previous three pages were initiated by an external infrastructure system, it is extremely tempting to exercise confirmation bias by automatically associating this fourth occurrence of the problem with the previous cause.While intuition and quick reactions can seem like desirable traits in the middle of incident management, they have downsides. Intuition can be wrong and is often less supportable by obvious data. Thus, following intuition can lead an engineer to waste time pursuing a line of reasoning that is incorrect from the start. Quick reactions are deep-rooted in habit, and habitual responses are unconsidered, which means they can be disastrous. The ideal methodology in incident management strikes the perfect balance of taking steps at the desired pace when enough data is available to make a reasonable decision while simultaneously critically examining your assumptions.It’s important that on-call SREs understand that they can rely on several resources that make the experience of being on-call less daunting than it may seem. The most important on-call resources are:
• Clear escalation paths
• Well-defined incident-management procedures
• A blameless postmortem culture ([Loo10], [All12])The developer teams of SRE-supported systems usually participate in a 24/7 on-call rotation, and it is always possible to escalate to these partner teams when necessary.
Feeling Safe  |  129
The appropriate escalation of outages is generally a principled way to react to serious outages with significant unknown dimensions.When one is handling incidents, if the issue is complex enough to involve multiple teams or if, after some investigation, it is not yet possible to estimate an upper bound for the incident’s time span, it can be useful to adopt a formal incident-management protocol. Google SRE uses the protocol described in Chapter 14, which offers an easy-to-follow and well-defined set of steps that aid an on-call engineer to rationally pursue a satisfactory incident resolution with all the required help. This protocol is internally supported by a web-based tool that automates most of the incident man‐agement actions, such as handing off roles and recording and communicating status updates. This tool allows incident managers to focus on dealing with the incident, rather than spending time and cognitive effort on mundane actions such as format‐ting emails or updating several communication channels at once.Finally, when an incident occurs, it’s important to evaluate what went wrong, recog‐nize what went well, and take action to prevent the same errors from recurring in the future. SRE teams must write postmortems after significant incidents and detail a full timeline of the events that occurred. By focusing on events rather than the people, these postmortems provide significant value. Rather than placing blame on individu‐als, they derive value from the systematic analysis of production incidents. Mistakes happen, and software should make sure that we make as few mistakes as possible. Recognizing automation opportunities is one of the best ways to prevent human errors [Loo10].Avoiding Inappropriate Operational Load
As mentioned in “Balanced On-Call” on page 127, SREs spend at most 50% of their time on operational work. What happens if operational activities exceed this limit?
Operational OverloadOperational Overload
The SRE team and leadership are responsible for including concrete objectives in quarterly work planning in order to make sure that the workload returns to sustaina‐ble levels. Temporarily loaning an experienced SRE to an overloaded team, discussed in Chapter 30, can provide enough breathing room so that the team can make head‐way in addressing issues.Ideally, symptoms of operational overload should be measurable, so that the goals can be quantified (e.g., number of daily tickets < 5, paging events per shift < 2).
Misconfigured monitoring is a common cause of operational overload. Paging alerts should be aligned with the symptoms that threaten a service’s SLOs. All paging alerts should also be actionable. Low-priority alerts that bother the on-call engineer every hour (or more frequently) disrupt productivity, and the fatigue such alerts induce can130  |  Chapter 11: Being On-Call
also cause serious alerts to be treated with less attention than necessary. See Chap‐ter 29 for further discussion.It is also important to control the number of alerts that the on-call engineers receive for a single incident. Sometimes a single abnormal condition can generate several alerts, so it’s important to regulate the alert fan-out by ensuring that related alerts are grouped together by the monitoring or alerting system. If, for any reason, duplicate or uninformative alerts are generated during an incident, silencing those alerts can provide the necessary quiet for the on-call engineer to focus on the incident itself. Noisy alerts that systematically generate more than one alert per incident should be tweaked to approach a 1:1 alert/incident ratio. Doing so allows the on-call engineer to focus on the incident instead of triaging duplicate alerts.Sometimes the changes that cause operational overload are not under the control of the SRE teams. For example, the application developers might introduce changes that cause the system to be more noisy, less reliable, or both. In this case, it is appropriate to work together with the application developers to set common goals to improve the system.In extreme cases, SRE teams may have the option to “give back the pager”—SRE can ask the developer team to be exclusively on-call for the system until it meets the standards of the SRE team in question. Giving back the pager doesn’t happen very frequently, because it’s almost always possible to work with the developer team to reduce the operational load and make a given system more reliable. In some cases, though, complex or architectural changes spanning multiple quarters might be required to make a system sustainable from an operational point of view. In such cases, the SRE team should not be subject to an excessive operational load. Instead, it is appropriate to negotiate the reorganization of on-call responsibilities with the development team, possibly routing some or all paging alerts to the developer on-call. Such a solution is typically a temporary measure, during which time the SRE and developer teams work together to get the service in shape to be on-boarded by the SRE team again.The possibility of renegotiating on-call responsibilities between SRE and product development teams attests to the balance of powers between the teams.2 This working relationship also exemplifies how the healthy tension between these two teams and the values that they represent—reliability versus feature velocity—is typically resolved by greatly benefiting the service and, by extension, the company as a whole.2 For more discussion on the natural tension between SRE and product development teams, see Chapter 1.
Avoiding Inappropriate Operational Load  |  131
A Treacherous Enemy: Operational Underload
Being on-call for a quiet system is blissful, but what happens if the system is too quiet or when SREs are not on-call often enough? An operational underload is undesirable for an SRE team. Being out of touch with production for long periods of time can lead to confidence issues, both in terms of overconfidence and underconfidence, while knowledge gaps are discovered only when an incident occurs.To counteract this eventuality, SRE teams should be sized to allow every engineer to be on-call at least once or twice a quarter, thus ensuring that each team member is sufficiently exposed to production. “Wheel of Misfortune” exercises (discussed in Chapter 28) are also useful team activities that can help to hone and improve trouble‐shooting skills and knowledge of the service. Google also has a company-wide annual disaster recovery event called DiRT (Disaster Recovery Training) that combines theo‐retical and practical drills to perform multiday testing of infrastructure systems and individual services; see [Kri12].Conclusions