一旦您开始部署图表，您将收到关于`vm.max_map_count`内核设置的警告。对于我们的 KinD 集群，包含的`initContainer`将在我们的工作节点上设置该值。在生产环境中，您可能不允许特权容器运行，这将导致 initContainer 失败。如果您不允许特权吊舱在您的集群中运行(这是一个非常**的**好主意)，您将需要在部署弹性搜索之前在每台主机上手动设置该值。
您可以通过检查`logging`命名空间中的 pods 来检查部署的状态。使用`kubectl`，在进入下一步之前，确认所有的吊舱都处于运行状态:
```
kubectl get pods -n logging
```
您应该会收到以下输出:
![Figure 12.7 – Elasticsearch pod list ](img/Fig_12.7_B15514.jpg)
图 12.7-弹性搜索 pod 列表
如我们所见，赫尔姆图表创建了几个 Kubernetes 对象。主要对象包括以下内容:
*   弹性搜索服务器窗格(`elasticsearch-elasticsearch-coordinating-only`)
*   弹性搜索数据状态集(`elasticsearch-elasticsearch-data-x`)
*   弹性搜索主状态集(`elasticsearch-elasticsearch-master-x`)
每个状态集为创建的每个 pod 创建一个 1 GB 的持久卷声明。我们可以使用`kubectl get pvc -n logging`查看 PVC，产生以下输出:
![Figure 12.8 – PVC list used by Elasticsearch ](img/Fig_12.8_B15514.jpg)
图 12.8–弹性搜索使用的聚氯乙烯列表
创建了三个集群 IP 服务，因为 Elasticsearch 将只被其他 Kubernetes 对象使用。我们可以使用`kubectl get services -n logging`查看服务，产生以下输出:
![Figure 12.9 – Elasticsearch services ](img/Fig_12.9_B15514.jpg)
图 12.9-弹性搜索服务
通过查看吊舱、服务和 PVC，我们可以确认图表部署是成功的，并且我们可以继续下一个组件，Fluentd。
### 部署 Fluentd
我们在`chapter12/logging`目录中的 GitHub repo 中包含了一个 fluent 部署。
Fluentd 是一个常见的日志转发器，与 Kubernetes 一起使用，将日志转发到一个中心位置。我们正在安装它，以便将 Kubernetes 日志转发到 Elasticsearch，从而提供 EFK 部署的完整示例。我们的 Falco 活动将使用 Falcosidekick 转发。
将 Fluentd 部署到集群的第一步是应用 Fluentd 配置。`fluentd-config.yaml`文件将创建一个包含 Fluentd 部署配置选项的配置图。
配置 Fluentd 超出了本书的范围。要使用 Fluentd 转发日志，我们确实需要解释 ConfigMap 的`output.conf`部分，它配置 Fluentd 将向其发送日志的主机。
在`fluentd-config.yaml`文件中，在文件的底部，你会看到一个标题为`output.conf`的部分:
![Figure 12.10 – Fluentd output configuration ](img/Fig_12.10_B15514.jpg)
图 12.10–流体输出配置
你可以看到我们对`elasticsearch`的`id`和`type`进行了选项设置，主机设置已经设置为`elasticsearch-elasticsearch-coordinating-only.logging.svc`。如果您返回几页并查看`kubectl get services -n logging`命令的输出，您将在输出中看到一个具有该名称的服务。这是与弹性搜索部署交互时必须针对的服务:
```
elasticsearch-elasticsearch-coordinating-only   ClusterIP   10.107.207.18
```
请注意，我们还向主机名添加了名称空间和 svc。Fluentd DaemonSet 将安装到`kube-system`命名空间，因此为了与另一个命名空间中的服务进行通信，我们需要提供服务的全名。在我们的 KinD 集群中，我们不需要将集群名称添加到`hostname`值中。
我们可以使用`kubectl apply`部署配置图:
```
kubectl apply -f fluentd-config.yaml
```
配置映射后，我们可以使用以下命令部署 DaemonSet:
```
kubectl apply -f fluentd-ds.yaml
```
通过检查`kube-system`命名空间中的吊舱，验证 Fluentd 吊舱是否正在运行:
```
kubectl get pods -n kube-system
```
因为我们只有一个节点，所以只能看到一个 Fluentd pod:
![Figure 12.11 – Fluentd DaemonSet pod list ](img/Fig_12.11_B15514.jpg)
图 12.11–Fluentd daemmonset 吊舱列表
Fluentd 将用于将所有**容器日志转发到 Elasticsearch。**
 **为了更容易使用 Kibana，我们将在本章的后面部分讨论，我们希望转发 Falco 日志，而不转发任何其他容器日志。最简单的方法是使用 Falco 团队的另一个项目，叫做 Falcosidekick。
### 部署 Falcosidekick
Falco 有一个实用程序，可以格式化并将 Falco 事件转发到不同的日志服务器。项目位于 https://github.com/falcosecurity/falcosidekick[的 GitHub 上。在撰写本文时，它支持 15 种不同的日志记录系统，包括 Slack、Teams、Datadog、Elasticsearch、AWS Lamda、SMTP 和 Webhooks。](https://github.com/falcosecurity/falcosidekick)
由于 Falcosidekick 为各种不同的后端打开了一种简单的转发方法，我们将部署它来将 Falco 事件转发到 Elasticsearch。
为了部署 Falcosidekick，我们将使用 Helm 从我们的 GitHub 存储库中使用本地副本来部署图表。图表文件位于`chapter12/logging/falcosidekick`目录:
1.  像所有图表一样，我们可以使用`values.yaml`文件来配置图表选项。我们提供了一个预配置的文件，其中包含将 Falco 事件发送到弹性搜索部署所需的条目。我们配置的文件中的条目显示在下面的代码块中。我们必须配置主机端口，使我们的弹性搜索服务以 HTTP 和端口`9200` :
    ```
     elasticsearch:
        host port: "http://elasticsearch-elasticsearch-coordinating-only.logging.svc:9200"
        index: "falco"
        type: "event"
        minimumpriority: ""
    ```
    为目标
2.  部署图表最简单的方法是将您的工作目录更改为`falcosidkick`目录。进入目录后，运行以下`helm install`命令部署图表:
    ```
    helm install falcosidekick -f values.yaml . --namespace falco
    ```
3.  要验证图表是否正确部署，请从运行在`logging`命名空间:
    ```
    kubectl logs falcosidekick-7656785f89-q2z6q -n logging
    2020/05/05 23:40:25 [INFO]  : Enabled Outputs : Elasticsearch
    2020/05/05 23:40:25 [INFO]  : Falco Sidekick is up and listening on port 2801
    ```
    中的 Falcosidekick 实例中获取日志
4.  一旦 Falcosidekick 吊舱启动从 Falco 吊舱接收数据，日志文件将包含显示成功的弹性搜索帖子的条目:
    ```
    2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)
    2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)
    2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)
    2020/05/05 23:42:40 [INFO]  : Elasticsearch - Post OK (201)
    ```
到目前为止，这给了我们什么？我们已经部署了弹性搜索来存储 Fluentd 代理将从我们的工作节点转发的信息。现在，我们的工作节点正在使用 Fluentd 代理将其所有日志发送到 Elasticsearch 实例，Falcosidekick 正在转发 Falco 事件。
Elasticsearch 将有大量信息需要整理，以使数据有用。为了解析数据并为日志创建有用的信息，我们需要安装一个系统，我们可以使用它来创建自定义仪表板并搜索收集的数据。这就是 **EFK** 栈中的 **K** 出现的地方。我们部署的下一步是安装基巴纳。
### 部署基巴纳
下一个图表将安装基巴纳服务器。我们选择使用只通过 HTTP 为基巴纳服务的部署，没有认证。在生产环境中，您应该同时启用两者来提高安全性。当然，在集群之外还不能访问基巴纳，因此我们需要创建一个入口规则，将我们的 NGINX 入口配置为将流量定向到 pod:
1.  要使用 Bitnami 图表将 Kibana 部署到集群，请使用以下命令:
    ```
    helm install kibana --set elasticsearch.hosts[0]=elasticsearch-elasticsearch-coordinating-only -- elasticsearch.port=9200,persistence.size=1Gi --namespace logging bitnami/kibana
    ```
2.  一旦部署开始，您将看到 Helm 的一些输出，告诉您如何使用 kubectl 端口转发来访问 Kibana:
    ```
    Get the application URL by running these commands:
      export POD_NAME=$(kubectl get pods --namespace logging -l "app.kubernetes.io/name=kibana,app.kubernetes.io/instance=kibana" -o jsonpath="{.items[0].metadata.name}")
      echo "Visit http://127.0.0.1:8080 to use your application"
      kubectl port-forward svc/kibana 8080:80
    ```
您可以忽略这些说明，因为我们将使用入口规则公开基巴纳，以便它可以在网络上的任何工作站上被访问。
### 为基巴纳创建入口规则
对于入口规则，我们将基于 nip.io 域创建一个规则:
1.  To create the ingress rule with the correct nip.io name, we have provided a script in the `chaper12/logging` folder called `create-ingress.sh`:
    ```
    ingressip=$(hostname  -I | cut -f1 -d' ')
    ingress=`cat "kibana-ingress.yaml" | sed "s/{hostip}/$ingressip/g"`
    echo "$ingress" | kubectl apply -f -
    ```
    该脚本将找到 Docker 主机的 ip 地址，并使用 **kibana.w.x.y.z.nip.ip** 用 nip.io 主机修补入口清单(这里， **w.x.y.z** 将包含主机的 IP 地址)。
2.  创建入口规则后，将显示访问您的基巴纳仪表板的详细信息:
    ```
    You can access your Kibana dashboard in any browser on your local network using http://kibana.10.2.1.107.nip.io
    ```
现在我们已经安装了基巴纳，我们可以打开基巴纳仪表板开始我们的配置。
### 使用基巴纳仪表板
要浏览至基巴纳仪表盘，请按照以下步骤操作:
1.  从本地网络上的任何计算机打开浏览器。
2.  使用在`install-ingress.sh`脚本中显示的入口名。在我们的例子中，我们会浏览到。
3.  The request will come back to your client with the IP address `10.2.1.107` and will be sent to your Docker host on port `80`.
    小费
    请记住，我们在端口`80`和`443`上公开了 KinD worker 节点的 Docker 容器。
4.  当您的 Docker 主机收到端口`80`上的主机名请求时，它将被转发到 Docker 容器，并最终到达 NGINX 入口控制器。
5.  NGINX 将寻找与主机名匹配的规则，并将流量发送到 Kibana pod。在您的浏览器中，您将看到基巴纳欢迎屏幕:
![Figure 12.12 – Kibana welcome screen ](img/Fig_12.12_B15514.jpg)
图 12.12–基巴纳欢迎屏幕
虽然您现在已经运行了一个功能齐全的审计日志记录系统，但是使用 Kibana 还有一步:您需要创建一个默认索引。
#### 创建基巴纳索引
要查看日志或创建可视化和仪表板，您需要创建一个索引。单个基巴纳服务器上可以有多个索引，允许您从单个位置查看不同的日志。在我们的示例服务器上，我们将有两组不同的传入日志，一组以 logstash 开头，另一组以 falco 开头。
logstash 文件容器中的数据由 Kubernetes 日志文件组成，其中包括 Fluentd 转发器转发的所有日志。Falco 文件由 Falco 西迪基克转发，仅包含来自 Falco 吊舱的警报。在本章中，我们将重点介绍 Falco 文件，因为它们只包含 Falco 数据:
1.  在基巴纳，点击位于左侧的设置工具![](img/Icon_1.png)，打开基巴纳管理页面。
2.  要创建索引并将其设置为默认值，请单击浏览器左上角的索引模式链接。
3.  接下来，点击右上角的按钮创建一个新的索引模式。
4.  Since we only want to create an index that contains the Falco data, enter `falco*` in the box. This will create an index that contains all current and future Falco logs:
    ![Figure 12.13 – Kibana index pattern definition ](img/Fig_12.13_B15514.jpg)