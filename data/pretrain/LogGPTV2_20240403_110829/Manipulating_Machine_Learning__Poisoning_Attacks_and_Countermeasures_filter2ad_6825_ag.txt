∂Wtr
∂z =
∂W(cid:2)
∂z .
tr
=
∂Wtr
∂z
∂W(cid:3)
tr
∂z
− ∂Wtr
∂z
=
∂W(cid:3)
tr
∂z
2
n (Xθ − Y )
n (Xθ − Y (cid:3)
2
)
T
X
T
X
∂θ
∂z
∂θ
∂z
2
n (Y − Y (cid:3)
=
T
)
X
∂θ
∂z
= 0.
So the difference between the gradients is
Then both the learned parameters and the gradients of the
objectives are the same regardless of the poisoned data added.
(cid:19)
We can now perform the derivation of the exact form of the
tr. We have:
((w − w0)
T
gradient of W(cid:3)
n(cid:21)
∂W(cid:3)
tr
∂z
(cid:19)
2
n
i=1
=
The right hand side can be rearranged to
(w − w0)
+ (b − b0)
+ μ
Σ
T
∂w
∂z
T
i
x
∂w
∂z
xi + (b − b0))
(cid:20)
(cid:19)
μT ∂w
∂z
∂b
∂z
+
∂b
∂z
+
∂b
∂z
(cid:20)
(cid:20)
.
,
but the terms with gradients can be evaluated using the matrix
equations derived from the KKT conditions from Equation 14,
which allows us to derive the following:
∂W(cid:3)
tr
∂xc
∂W(cid:3)
tr
∂yc
=
=
=
T
2
2
T M + (b0 − b)w
n ((w0 − w)
n (f (xc, θ) − f (xc, θ0))(w0 − 2w)
n (f (xc, θ) − f (xc, θ0)).
2
T
APPENDIX B
ANALYSIS OF TRIM ALGORITHM
We present here an analysis on the convergence and esti-
mation properties of the TRIM algorithm.
Convergence. First, Algorithm 2 can be proven to always
terminate by the following theorem.
Theorem 1. Algorithm 2 terminates in a ﬁnite number of
iterations.
Proof. We ﬁrst prove that for each iteration i that does not
terminate the loop, we have R(i) < R(i−1). Since each subset
of {1, ..., n} with size n−p uniquely corresponds to one value
R, there is only a ﬁnite number of possible R during training.
If the algorithm does not terminate, then there will be an
·
inﬁnite long sequence of R(i), contradicting that the set of
all possible R is ﬁnite.
We only need to show R(i) ≤ R(i−1), as the algorithm
terminates when R(i) = R(i−1). In fact, we have
(i−1)
R(i)
(i−2)
) ≤ L(DI(i−1) , θ
≤ L(DI(i−1) , θ
)
) = R(i−1).
= L(DI(i) , θ
(i−1)
33
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
Parameter
η
—
β

λ
Line Search Learning Rate
Purpose
—
Line Search Learning Rate Decay
Attack Stopping Condition
Regularization Parameter
Values
{0.01, 0.03, 0.05, 0.1,
0.3, 0.5, 1.0}
{0.75}
10−5
Set with Cross Validation
TABLE VI: Description of Parameters for Algorithm 1.
The ﬁrst inequality is because of the deﬁnition of I (i) (line 8),
while the second is due to the deﬁnition of θ
(i) (line 9).
Estimation bound. We now prove Theorem 2. We restate it
below.
Theorem 2. Let Dtr denote the original training data, ˆθ the
= arg minθ L(Dtr, θ) the
global optimum for (17), and θ
estimator in the ideal world on pristine data. Assuming α < 1,
there exist a subset D(cid:3) ⊂ Dtr of (1−α)·n pristine data samples
(cid:20)
such that
L(Dtr, θ
MSE(D(cid:3), ˆθ) ≤
(cid:19)
(19)
1 +
)
(cid:2)
(cid:2)
α
1 − α
Proof. Assume ˆθ = ( ˆw, ˆb), ˆI optimize (17). We have:
L(D ˆI, ˆθ) ≤ L(Dtr, θ
(cid:2)
).
(20)
Since the adversary can poison at most α · n data points,
there exists a subset I(cid:3) ⊆ ˆI containing (1 − α)n indexes
= DI(cid:2)
corresponding to pristine data points. We deﬁne D(cid:3)
.
Thus, we have
L(D(cid:3), ˆθ) =
x + ˆb − y)
+ λΩ(ˆθ)
( ˆw
1
T
2
(cid:21)
(cid:21)
(x,y)∈D(cid:2)
T
( ˆw
x + ˆb − y)
2
+ λΩ(ˆθ)
(1 − α)n
1
(1 − α)n
1
1 − α
(cid:19)
1
1 − α
1 +
≤
=
≤
≤
(x,y)∈DI
L(D ˆI, ˆθ) − 1
1 − α
(cid:20)
L(Dtr, θ
) − α
1 − α
L(Dtr, θ
α
1 − α
).
(cid:2)
(cid:2)
· λΩ(ˆθ) + λΩ(ˆθ)
· λΩ(ˆθ)
(21)
Notice that in the second step, we apply the fact below:
= n[L(DI, ˆθ) − λΩ(ˆθ)]
x + ˆb − y)
( ˆw
T
2
(cid:21)
(x,y)∈DI
APPENDIX C
BASELINE ATTACK
In this section, we discuss parameter setting for the baseline
attack by Xiao et al. [54]. We perform experiments on the
same dataset used by Xiao et al. [54] to test and optimize the
baseline attack.
PDF dataset. The PDF malware dataset is a classiﬁcation
dataset containing 5000 benign and 5000 malicious PDF
ﬁles [48], [49]. It includes 137 features, describing infor-
mation such as size, author, keyword counts, and various
timestamps. We pre-process the data by removing features
that were irrelevant (e.g., ﬁle name) or had erroneous values
(e.g., timestamps with negative values). We also use one-hot
encoding for categorical features (replacing the categorical
feature with a new binary feature for each distinct value) and
apply the logarithm function to columns with large values
(e.g., size, total pixels), resulting in 141 features. Each feature
is normalized by subtracting the minimum value, and dividing
by its range, so that all these features are in [0,1].
Hyperparameters. In order to analyze the baseline attack,
we perform an experiment that reproduces exactly the setting
from Xiao et al. [54]. We choose a random subset of 300
ﬁles for training and a non-overlapping subset of 5000 points
for testing the models. To take advantage of our multi-core
machines, we parallelize the code by allowing each core to
run different instances of the for loop body starting on line 6
in Algorithm 1.
There are 3 hyperparameters that control the gradient step
and convergence of the iterative search procedure in the
algorithm (η, β, and ). The η parameter controls the step
size taken in the direction of the gradient. We selected from
7 different values in a wide range, by testing each on 20%
poisoning and identifying the value with the largest MSE
increase. The β parameter controls the decay of the learning
rate, so we take smaller steps as we get closer to the optimal
value. We ﬁxed this value to 0.75 and decayed (set η ← η∗ β)
when a step did not make progress. We found this setting
to work well on many problems. We ﬁxed the  parameter
for attack stopping condition at 0.00001, and choose the λ
regularization parameter for the regression model with cross
validation. Our parameter settings are detailed in Table VI).
The second to last step is derived by applying Inequality (20),
and the last step comes from
λΩ(ˆθ) ≥ 0.
Further, we have
MSE(D(cid:3), ˆθ) ≤ L(D(cid:3), ˆθ)
(22)
By combining (21) and (22), we can get our conclusion.
Fig. 7: Attack MSE on Contagio dataset for ridge.
34
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8: Attack MSE on Contagio dataset for Lasso.
Attack effectiveness. We ran our OptP and StatP attacks on
this dataset, in addition to BGD. We expect OptP to be very
similar in terms of poisoning to BGD because it is run in the
classiﬁcation setting. Our proposed optimization framework is
speciﬁc to regression. For instance, in classiﬁcation settings
optimizing by both x and y variables is exactly the same
as optimizing only by x. For the initialization strategies,
InvFlip and BFlip are exactly the same in the classiﬁcation
setting. In our framework we are exploiting the continuous
response variables of regression for designing more effective
optimization-based poisoning attacks. The only modiﬁcation
to BGD might come from using Wval as an optimization
objective, but we expect that in isolation that will not produce
signiﬁcant changes. We showed in Section V-B that Wval is
most likely to be effective when optimization by (x, y) is used.
Our graphs from Figures 7 and 8 conﬁrm this expectation,
and indeed OptP and BGD are very similar in the attack
MSEs. The effectiveness of the BGD attack is similar to that
reported by Xiao et al. [54] and we have conﬁdence that our
implementation and choice of hyper-parameters are accurate.
Interestingly, the StatP attack outperforms BGD and OptP by
40% for ridge regression. We believe that pushing the feature
values to the boundary as done by StatP has higher effect
as a poisoning strategy for ridge regression in which the loss
function is convex and the optimization maximum is achieved
in the corners. That is not always the case with models such
as Lasso, but still StatP is quite effective at poisoning Lasso
as well.
35
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply.