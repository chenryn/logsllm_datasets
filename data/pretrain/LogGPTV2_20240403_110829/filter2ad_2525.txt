title:A Behavioral Biometric Authentication Framework on Smartphones
author:Ahmed Mahfouz and
Tarek M. Mahmoud and
Ahmed Sharaf Eldin
POSTER: A Behavioral Biometric Authentication
Framework on Smartphones
Ahmed Mahfouz1
PI:EMAIL
Tarek M. Mahmoud1,2
PI:EMAIL
Ahmed Sharaf Eldin3,4
PI:EMAIL
1 Computer Science Department, Minia University, EL-Minya, Egypt
2 Canadian International College (CIC), Cairo, Egypt
3 Information Systems Department, Helwan University, Egypt
4 Faculty of Information Technology and Computer Science, Sinai University, Egypt
ABSTRACT
To protect smartphones from unauthorized access, the user
has the option to activate authentication mechanisms : PIN,
Password, or Pattern. Unfortunately, these mechanisms
are vulnerable to shoulder-surﬁng, smudge and snooping at-
tacks. Even the traditional biometric based systems such as
ﬁngerprint or face, also could be bypassed. In order to pro-
tect smartphones data against these sort of attacks, we pro-
pose a behavioral biometric authentication framework that
leverages the user’s behavioral patterns such as touchscreen
actions, keystroke, application used and sensor data to au-
thenticate smartphone users.
To evaluate the framework, we conducted a ﬁeld study in
which we instrumented the Android OS and collected data
from 52 participants during 30-day period. We present the
prototype of our framework and we are working on its com-
ponents to select the best features set that can be used to
build diﬀerent modalities to authenticate users on diﬀerent
contexts. To this end, we developed only one modality, a
gesture authentication modality, which authenticate smart-
phone users based on touch gesture. We evaluated this au-
thentication modality on about 3 million gesture samples
based on two schemes, classiﬁcation scheme with EER 0.004,
and anomaly detection scheme with EER 0.10.
Keywords
Smartphone; Authentication; Behavioral Biometrics
1.
INTRODUCTION
Smartphones have become ubiquitous parts in our daily
life. They combine the personal computing features in ad-
dition to the mobility features. Consequently, they contain
a plethora of sensitive data and personal information. To
protect these sensitive data, user has the option to enable
an authentication mechanism. Unfortunately, 52% (out of
1,500) [1] and 34% (out of 500) [2] don’t lock their smart-
Figure 1: The Behavioral Biometric Framework architecture.
phones.
Inconvenience, lack of motivation and awareness
were the most common reasons for not-locking smartphones.
Also these mechanisms are vulnerable to diﬀerent risks such
as shoulder surﬁng and snooping attacks.
In this poster, we propose a behavioral biometric authen-
tication framework that is going to (i) authenticate users
implicitly (i.e., without interrupting their activities), and
(ii) continually (i.e., authentication process is continuously
repeated).
2. RELATED WORK
Several researchers conducted studies to understand the
current unlocking mechanisms [4] and other researchers pro-
posed new techniques to authenticate smartphone user, some
of them based on biometric authentication [3], and others
based on implicit authentication [6]. Our work is more re-
lated to implicit authentication.
In contrast with the previous work, where the major-
ity of implicit authentication techniques were evaluated on
datasets that collected in constrained settings. Moreover,
some authentication methods have built based on very sim-
ple features, which expected to be statistically weak.
In
this work, we seek to collect realistic behavioral data in un-
constrained environment and extract a discriminative set of
features.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ASIA CCS ’17 April 02-06, 2017, Abu Dhabi, United Arab Emirates
c(cid:13) 2017 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4944-4/17/04.
DOI: http://dx.doi.org/10.1145/3052973.3055160
3. BEHAVIORAL BIOMETRIC FRAMEWORK
Figure 1 shows the behavioral biometric framework archi-
tecture. It has three main modules: data collection module,
feature extraction module, fusion and decision module.
Our framework authenticate smartphone users based on
authentication score, which is calculated from diﬀerent au-
thentication modalities. Each modality extracts a useful set
923Stroke detection
3.3.2
As illustrated in Figure 2, we detect the stroke based on
the action code. So, all consecutive points between touch down
and touch up actions represent a stroke S. Each point in S
represented by xi and yi, the coordinates of touched point,
pi, the pressure on the touched point, ai, the size area of
touched point, and ti, timestamp of touch action, where
i = {1, . . . , n}, and n is the total number of points in the
stroke.
3.3.3 Feature Extraction
For extracting useful features we analyzed the stroke from
two directions, the geometry of the stroke, and its motion
dynamics.
Geometric analysis, we extracted six features from the ge-
ometric analysis on the stroke, four of them represent touch
down and touch up coordinates, xdown, ydown, xup, yup, and
the other two features are stroke length Slength and stroke
curvature Scurvature. Where Slength represents the length of
speciﬁc path traveled from touch down to touch up points
and is calculated based on the sum of line segment lengths
as follows:
(cid:112)(xi − xi−1)2 + (yi − yi−1)2
Slength =
n(cid:88)
i=2
where n is the number of points in the stroke S. Stroke cur-
vature Scurvature represents the amount of deviation from
being a straight line (see Figure 2b), and is calculated based
on the following formula:
(1)
(2)
K =
|X(cid:48)Y (cid:48)(cid:48) − Y (cid:48)X(cid:48)(cid:48)|
(X(cid:48)2 + Y (cid:48)2)
3
2
where X and Y are row vectors of points in the stroke. X(cid:48),
Y (cid:48), X(cid:48)(cid:48), and Y (cid:48)(cid:48) represent the ﬁrst and second derivatives,
and K represents the row vector of curvatures for each point
in the stroke. Then we calculate the mean of K to represent
Scurvature as follow:
Scurvature =
1
N
Ki
(3)
N(cid:88)
i=1
Dynamic analysis, we extracted four features from the dy-
namic analysis on the stroke. Given that, stroke contains a
set of consecutive points. These points detected according to
the motion of object (i.e., ﬁnger) on the touch screen. As a
matter of fact, ﬁnger moves in curvilinear direction more of-
ten than in linear direction. Based on this fact, we calculated
the displacement Sdisplacement, the length of the straight line
between touch down and touch up, see Figure 2b for more
clariﬁcation.
To understand how fast or slow the ﬁnger moves on the
screen, we calculate the velocity at each point in the stroke
as follows:
V =(cid:112)((X(cid:48))2 + (Y (cid:48))2)
(4)
where X(cid:48) and Y (cid:48) are the ﬁrst derivative of row vectors of
points in the stroke, and V is the row vector of velocities
at each point. We extracted two features from this vec-
tor, mean velocity Smean(V ) and maximum velocity Smax(V ).
Also, we calculate the acceleration at each point in the stroke
based on the following formula:
(cid:18) ds
(cid:19)2
dt
A =
d2s
dt2 T + k
N
(5)
(a) User ui
(b) User uj
Figure 2: Stroke samples done by two diﬀerent users from
our dataset.
of features from user-level activities. Finally our framework
leverages the decision fusion to take the ﬁnal decision.
3.1 Data collection
To build a real-world unconstrained dataset (i.e., dataset
that contains real user activities without intervention), we
developed a monitoring framework and instrumented it in
the Android OS (Lollipop version on Nexus 5 device).
It
recorded events related to device unlocking, touchscreen and
sensors data in a real-life settings. By deploying our mon-
itoring framework via Phonelab testbed, a programmable
smartphone testbed, developed at the University at Buﬀalo
and support to run experiments at the OS level on partici-
pants from University at Buﬀalo community [5].
The raw data corpus that we collected contains about 200
GB of smartphone user activities. The participants took
part in our study during diﬀerent time periods, all between
July 06, 2016 and August 31, 2016, for at least 30 days
each. The total number of participants who successfully ac-
cepted to install our monitoring framework were 133 but we
have only included 52 participants who kept our monitoring
framework for 30-day period or more.
3.2 Behavioral Biometric Modalities
Our goal is to develop more than one modality, and yet we
have developed only one modality which is the gesture au-
thentication modality. In this section, we provide a detailed
description about this modality.
3.3 Gesture Modality
In this modality, we collected data from touch screen events,
and then analyzed these data, extracted features vector, and
then built the gesture authentication modality.
3.3.1 Gesture analysis
To interact with the touchscreen, user has to enter a ges-
ture, a hand-drawn shape on a touch screen. This gesture
can have one or more strokes, a sequence of consecutive
timed points. Each point represented by an ordered pair of
numerical coordinates (x, y), as illustrated in Figure 2.
For each touched point, we have collected the following
raw data, timestamp, coordinates, pressure, size and
the action code, a code that speciﬁes the state change
such as touch down, touch move or touch up.
02004006008001000x-location020040060080010001200140016001800y-locationtouch downtouch up02004006008001000x-location020040060080010001200140016001800y-locationtouch downtouch up924where s is the travelled distance and T is the unit tan-
gent vector and N is the unit normal vector. Then we ex-
tracted the mean acceleration for the vector A as a feature
Smean(acc).
In addition to the extracted features from geometric and
dynamic analysis, we extracted twelve features related to
time, pressure and size which are described below.
Temporal Features: we extracted two temporal features
Sduration, represents the total time taken to perform a stroke,
and inter-stroke duration Sinterduration, represents the time
spent between the current and the previous stroke.
Pressure and Size features, as we mentioned before, we
recorded the pressure and the size at each touched point in
the stroke. We extracted ﬁve features for the pressure, two
of them for touch down SpDown and touch up SpU p, and the
other three features are extracted from the descriptive statis-
tics of the pressure which are average, maximum and mini-
mum, SpAverage, SpM in, SpM ax. Similarly for the touch size
where we extracted SsDown, SsU p, SsAverage, SsM in, SsM ax.
3.3.4 Modeling and evaluation
We used two models to authenticate the user, classiﬁca-
tion model, which are trained on data from both legitimate
user and imposters, and anomaly detection model, which are
trained on data from legitimate user only.
Classiﬁcation model, for each user ui, the classiﬁer calcu-
lates an authentication score p(ui) that represents the prob-
ability of ui being a legitimate user. We used k-nearest
neighbors classiﬁer based on one-vs-all scheme, where we
used data from other users as imposters.
Anomaly detection model, we trained the anomaly de-
tection learning algorithm (lsanomaly [7]) with legitimate
user samples and then tested for new sample based on nov-
elty detection scheme.
Validation method, To evaluate the accuracy of the clas-
siﬁers, dataset is separated into training set and testing set.
Then we performed 10-fold cross-validation.
Performance metric, in order to evaluate the performance
of both models, we used ROC curve as an evaluation met-
ric, which depicts the trade-oﬀ between TPR and FPR in a
single curve at various threshold values. The top left corner
of the plot represents the ideal point, where TPR equal one
and FPR equal zero.
Results, Knn classiﬁer achieved EER 0.004 with AUC 0.99
as shown in Figure 3a. On the other hand, the anomaly de-
tection achieved EER 0.10 with AUC 0.91. As we can see,
the classiﬁcation results are better than anomaly detection
because the classiﬁer has enough training set to model both
legitimate and imposters behavior but anomaly detection
not. Although the classiﬁcation is more powerful in terms
of error cost (FP/FN) than anomaly detection. Using it
could be impossible in practice, in case of the huge attack
space or few training examples. So using anomaly detection
is a good ﬁt to work here.
3.4 Decision fusion
Even we have developed only one modality, but we would
like to share our idea on how decision fusion will be. Our
goal is to leverage diﬀerent data sources to develop more
than one authentication modality to authenticate the user in
diﬀerent contexts. Each modality is going to have a decision
performance. One modality can have a strength in speciﬁc
(a) ROC (classiﬁer)
(b) ROC (anomaly detection)
Figure 3: ROC analysis for classiﬁcation model in (a) and
anomaly detection model in (b) according to our dataset.
AUC represents the Area under curve and summarize the
performance of the models. The more the AUC is the better
the system is.
context where others not. We are going to apply some fusion
scenarios on how to use theses diﬀerent modalities based on
diﬀerent contexts and also use them as a complimentary to
each other.
4. CONCLUSIONS AND FUTURE WORK
We conducted a ﬁeld study on Android phone users. We
collected data related to user behavioral activities and we
are developing a multimodal behavioral biometric authenti-
cation framework to authenticate smartphone users based on
diﬀerent contexts. Yet we are done with only one modality,
gesture authentication modality (see section 3.3). Our fu-
ture work is going to concentrate on developing other modal-
ities based on keystroke and behavioral proﬁling biometric
traits.
5. ACKNOWLEDGMENTS
We would like to thank our colleagues for their feedback
on the earlier version of this Poster. The ﬁrst author would
like to thank Egyptian Mission sector for the doctoral schol-
arship.
6. REFERENCES
[1] E. Bursztein. Survey: Most people don’t lock their android
phones - but should. https://www.elie.net/blog/survey-most-
people-dont-lock-their-android-phones-but-should. April
2015.
[2] S. Egelman, S. Jain, R. S. Portnoﬀ, K. Liao, S. Consolvo, and
D. Wagner. Are you ready to lock? In Proceedings of the 2014
ACM SIGSAC Conference on Computer and Communications
Security, CCS ’14, pages 750–761, New York, NY, USA, 2014.
ACM.
[3] A. K. Jain, K. Nandakumar, and A. Ross. 50 years of biometric
research: Accomplishments, challenges, and opportunities.
Pattern Recognition Letters, 79:80–105, 2016.
[4] A. Mahfouz, I. Muslukhov, and K. Beznosov. Android users in
the wild: Their authentication and usage behavior. Pervasive
and Mobile Computing, 32:50 – 61, 2016. Mobile Security,
Privacy and Forensics.
[5] A. Nandugudi, A. Maiti, T. Ki, F. Bulut, M. Demirbas,
T. Kosar, C. Qiao, S. Y. Ko, and G. Challen. Phonelab: A large
programmable smartphone testbed. In Proceedings of First
International Workshop on Sensing and Big Data Mining,
SENSEMINE’13, pages 4:1–4:6, New York, NY, USA, 2013.
ACM.
[6] V. M. Patel, R. Chellappa, D. Chandra, and B. Barbello.
Continuous user authentication on mobile devices: Recent
progress and remaining challenges. IEEE Signal Processing
Magazine, 33(4):49–61, July 2016.
[7] J. A. Quinn and M. Sugiyama. A least-squares approach to
anomaly detection in static and sequential data. Pattern
Recogn. Lett., 40:36–40, Apr. 2014.
0.000.010.020.030.040.05False Positive Rate0.9900.9920.9940.9960.9981.000True Positive Rate(AUC = 0.99)0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Rate(AUC = 0.91)925