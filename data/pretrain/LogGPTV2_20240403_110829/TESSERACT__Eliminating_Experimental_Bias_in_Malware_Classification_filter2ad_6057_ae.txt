# Delaying Time Decay: Incremental Retraining and Evaluation

## 0. Introduction
The following values represent performance metrics for different algorithms (ALG1, ALG2) under various conditions:

- **Performance Metrics:**
  - 0.656, 0.674, 0.679, 0.679
  - 0.622, 0.527, –, 0.704, 0.703, 0.758, 0.784, 0.801, 0.802, 0.823, 0.828, 0.830, –
  - 0.683, 0.589, 0.667, 0.680, 0.714, 0.732, 0.732, 0.741, 0.736

- **Figure 7:** Delaying time decay: incremental retraining.

This section aims to compare and evaluate the trade-offs of different budget-constrained strategies to delay time decay. Since deep learning (DL) has shown to be more robust to time decay than ALG1 and ALG2 (§4.4), this section focuses on these algorithms to show the performance-cost trade-offs of different mitigations.

## 1. Delay Strategies
We do not propose novel delay strategies but instead focus on how TESSERACT allows for the comparison of some popular approaches to mitigate time decay. This shows researchers how to adopt TESSERACT for the fair comparison of different approaches when proposing novel solutions to delaying time decay under budget constraints. We now summarize the delay strategies we consider and show results on our dataset. For interested readers, we include additional background knowledge on these strategies in §A.4.

### 1.1 Incremental Retraining
We first consider an approach that represents an ideal upper bound on performance, where all points are included in retraining every month. This is likely unrealistic as it requires continuously labeling all the objects. Even assuming a reliance on VirusTotal, there is still an API usage cost associated with higher query rates, and the approach may be ill-suited in other security domains. Figure 7 shows the performance of ALG1 and ALG2 with monthly incremental retraining.

### 1.2 Active Learning
Active Learning (AL) strategies investigate how to select a subset of test objects (with unknown labels) that, if manually labeled and included in the training set, should be the most valuable for updating the classification model [46]. Here, we consider the most popular AL query strategy, uncertainty sampling, in which the points with the most uncertain predictions are selected for retraining, under the intuition that they are the most relevant to adjust decision boundaries. Figure 8 reports the active learning results obtained with uncertainty sampling, for different percentages of objects labeled per month. We observe that even with 1% AL, the performance already improves significantly.

### 1.3 Classification with Rejection
Another mitigation strategy involves rejecting a classifier’s decision as “low confidence” and delaying the decision to a future date [6]. This isolates the rejected objects to a quarantine area, which will later require manual inspection. Figure 9 reports the performance of ALG1 and ALG2 after applying a reject option based on [26]. In particular, we use the third quartile of probabilities of incorrect predictions as the rejection threshold [26]. The gray histograms in the background report the number of rejected objects per month. The second year of testing has more rejected objects for both ALG1 and ALG2, although ALG2 overall rejects more objects.

## 2. Analysis of Delay Methods
To quantify performance-cost trade-offs of methods to delay time decay without changing the algorithm, we characterize the following three elements:
- **Performance (P):** The performance measured in terms of AUT to capture robustness against time decay (§4.2).
- **Labeling Cost (L):** The number of testing objects (if any) that must be labeled. The labeling must occur periodically (e.g., every month) and is particularly costly in the malware domain as manual inspection requires many resources (infrastructure, time, expertise, etc.). For example, Miller et al. [36] estimated that an average company could manually label 80 objects per day.
- **Quarantine Cost (Q):** The number of objects (if any) rejected by the classifier—these must be manually verified, so there is a cost for leaving them in quarantine.

Table 3, utilizing AUT(F1,24m) while enforcing our constraints, reports a summary of labeling cost L, quarantine cost Q, and two performance columns P, corresponding to training with ˆσ and ϕ∗F1 (§4.3), respectively. In each row, we highlight in purple cells (resp. orange) the column with the highest AUT for ALG2 (resp. ALG1). Table 3 allows us to:
- (i) Examine the effectiveness of the training ratios ϕ∗F1 and ˆσ.
- (ii) Analyze the AUT performance improvement and the corresponding costs for delaying time decay.
- (iii) Compare the performance of ALG1 and ALG2 in different settings.

First, let us compare ϕ∗F1 with ˆσ. The first row of Table 3 represents the scenario in which the model is trained only once at the beginning—the scenario for which we originally designed Algorithm 1 (§4.3 and Figure 6). Without methods to delay time decay, ϕ∗F1 achieves better performance than ˆσ for both ALG1 and ALG2 at no cost. In all other configurations, we observe that training ϕ = ϕ∗F1 always improves performance for ALG2, whereas for ALG1 it is slightly advantageous in most cases except for rejection and AL 1%. Generally, the performance of ALG1 trained with ϕ∗F1 and ˆσ is consistently close. The intuition for this outcome is that ϕ∗F1 and ˆσ are also close for ALG1: when applying the AL strategy, we re-apply Algorithm 1 at each step and find that the average ϕ∗F1 ≈ 15% for ALG1, which is close to 10% (i.e., ˆσ). On the other hand, for ALG2 the average ϕ∗F1 ≈ 50%, which is far from ˆσ and improves all results significantly. We can conclude that our tuning algorithm is most effective when it finds a ϕ∗P that differs from the estimated ˆσ.

Then, we analyze the performance improvement and related cost of using delay methods. The improvement in F1-Score granted by our algorithm comes at no labeling or quarantine cost. We can observe that one can improve the in-the-wild performance of the algorithms at some cost L or Q. It is important to observe that objects discarded or to be labeled are not necessarily malware; they are just the objects most uncertain according to the algorithm, which the classifier may have likely misclassified. The labeling costs L for ALG1 and ALG2 are identical (same dataset); in AL, the percentage of retrained objects is user-specified and fixed.

Finally, Table 3 shows that ALG1 consistently outperforms ALG2 on F1 for all performance-cost trade-offs. This confirms the trend seen in the realistic settings of Table 1.

## 3. Discussion
This section shows that TESSERACT is helpful to both researchers and industrial practitioners. Practitioners need to estimate the performance of a classifier in the wild, compare different algorithms, and determine resources required for L and Q. For researchers, it is useful to understand how to reduce costs L and Q while improving classifiers' performance P through comparable, unbiased evaluations. The problem is challenging, but we hope that releasing TESSERACT’s code fosters further research and widespread adoption.

### 3.1 Guidelines, Assumptions, and Limitations
We now discuss guidelines, our assumptions, and how we address limitations of our work.

#### 3.1.1 Actionable Points on TESSERACT
It is relevant to discuss how both researchers and practitioners can benefit from TESSERACT and our findings. A baseline AUT performance (without classifier retraining) allows users to evaluate the general robustness of an algorithm to performance decay (§4.2). We demonstrate how TESSERACT can reveal true performance and provide counter-intuitive results (§4.4). Robustness over extended time periods is practically relevant for deployment scenarios without the financial or computational resources to label and retrain often. Even with retraining strategies (§5), classifiers may not perform consistently over time. Manual labeling is costly, and the ML community has worked on mitigation strategies to identify a limited number of best objects to label (e.g., active learning [46]). TESSERACT takes care of removing spatio-temporal bias from evaluations, so that researchers can focus on the proposal of more robust algorithms (§5). In this context, TESSERACT allows for the creation of comparable baselines for algorithms in a time-aware setting. Moreover, TESSERACT can be used with different time granularity, provided each period has a significant number of samples. For example, if researchers are interested in increasing robustness to decay for the upcoming 3 months, they can use TESSERACT to produce bias-free comparisons of their approach with prior research, while considering time decay.

#### 3.1.2 Generalization to Other Security Domains
Although we used TESSERACT in the Android domain, our methodology generalizes and can be immediately applied to any machine learning-driven security domain to achieve an evaluation without spatio-temporal bias. Our methodology, although general, requires some domain-specific parameters that reflect realistic conditions (e.g., time granularity ∆ and test time length). This is not a weakness of our work, but rather an expected requirement. In general, it is reasonable to expect that spatio-temporal bias may afflict other security domains when affected by concept drift and i.i.d. does not hold—however, further experiments in other domains (e.g., Windows malware, code vulnerabilities) are required to make any scientific conclusion. TESSERACT can be used to understand the extent to which spatio-temporal bias affects such security domains; however, the ability to generalize requires access to large timestamped datasets, knowledge of realistic class ratios, and code or sufficient details to reproduce baselines.

#### 3.1.3 Domain-Specific In-the-Wild Malware Percentage ˆσ
In the Android landscape, we assume that ˆσ is around 10% (§2.2). Correctly estimating the malware percentage in the testing dataset is a challenging task, and we encourage further representative measurement studies [30, 53] and data sharing to obtain realistic experimental settings.

#### 3.1.4 Correct Observation Labels
We assume goodware and malware labels in the dataset are correct (§2.3). Miller et al. [36] found that AVs sometimes change their outcome over time: some goodware may eventually be tagged as malware. However, they also found that VirusTotal detections stabilize after one year; since we are using observations up to Dec 2016, we consider VirusTotal’s labels as reliable. In the future, we may integrate approaches for noisy oracles [15], which assume some observations are mislabeled.

#### 3.1.5 Timestamps in the Dataset
It is important to consider that some timestamps in a public dataset could be incorrect or invalid. In this paper, we rely on the public AndroZoo dataset maintained at the University of Luxembourg, and we rely on the dex_date attribute as the approximation of an observation timestamp, as recommended by the dataset creators [3]. We further verified the reliability of the dex_date attribute by re-downloading VirusTotal [20] reports for 25K apps3 and verifying that the first_seen attribute always matched the dex_date within our time span. In general, we recommend performing some sanitization of a timestamped dataset before performing any analysis on it: if multiple timestamps are available for each object, consider the most reliable timestamp you have access to (e.g., the timestamp recommended by the dataset creators, or the VirusTotal’s first_seen attribute) and discard objects with “impossible” timestamps (e.g., with dates which are either too old or in the future), which may be caused by incorrect parsing or invalid values of some timestamps. To improve trustworthiness of the timestamps, one could verify whether a given object contains time inconsistencies or features not yet available when the app was released [29]. We encourage the community to promptly notify dataset maintainers of any date inconsistencies. In the TESSERACT’s project website (§8), we will maintain an updated list of timestamped datasets publicly available for the security community.

#### 3.1.6 Choosing Time Granularity (∆)
Choosing the length of the time slots (i.e., time granularity) largely depends on the sparseness of the available dataset: in general, the granularity should be chosen to be as small as possible, while containing a statistically significant number of samples—as a rule of thumb, we keep the buckets large enough to have at least 1000 objects, which in our case leads to a monthly granularity. If there are restrictions on the number of time slots that can be considered (perhaps due to limited processing power), a coarser granularity can be used; however, if the granularity becomes too large, then the true trend might not be captured.

#### 3.1.7 Resilience of Malware Classifiers
In our study, we analyze three recent high-profile classifiers. One could argue that other classifiers may show consistently high performance even with space-time bias eliminated. And this should indeed be the goal of research on malware classification. TESSERACT provides a mechanism for an unbiased evaluation that we hope will support this kind of work.

#### 3.1.8 Adversarial ML
Adversarial ML focuses on perturbing training or testing observations to compel a classifier to make incorrect predictions [7]. Both relate to concepts of robustness, and one can characterize adversarial ML as an artificially induced worst-case concept drift scenario. While the adversarial setting remains an open problem, the experimental bias we describe in this work—endemic in Android malware classification—must be addressed prior to realistic evaluations of adversarial mitigations.

## 4. Related Work
A common experimental bias in security is the base rate fallacy [5], which states that in highly-imbalanced datasets (e.g., network intrusion detection, where most traffic is benign), TPR and FPR are misleading performance metrics because even FPR = 1% may correspond to millions of FPs and only thousands of TPs. In contrast, our work identifies experimental settings that are misleading regardless of the adopted metrics and remain incorrect even if the right metrics are used (§4.4). Sommer and Paxson [47] discuss challenges and guidelines in ML-based intrusion detection; Rossow et al. [44] discuss best practices for conducting malware experiments; van der Kouwe et al. [54] identify 22 common errors in system security evaluations. While helpful, these works [44,47,54] do not identify temporal and spatial bias, do not focus on Android, and do not quantify the impact of errors on classifiers' performance, and their guidelines would not prevent all sources of temporal and spatial bias we identify. To be precise, Rossow et al. [44] evaluate the percentage of objects—in previously adopted datasets—that are “incorrect” (e.g., goodware labeled as malware, malfunctioning malware).

---

This revised version aims to provide a clearer, more structured, and professional presentation of the content.