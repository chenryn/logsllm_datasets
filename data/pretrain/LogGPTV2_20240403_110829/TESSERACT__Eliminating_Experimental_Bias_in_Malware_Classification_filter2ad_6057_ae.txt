0.656
0.674
0.679
0.679
ALG1 ALG2
0.622
0.527
–
0.704
0.703
0.758
0.784
0.801
0.802
0.823
0.828
0.830
–
0.683
0.589
0.667
0.680
0.714
0.732
0.732
0.741
0.736
Figure 7: Delaying time decay: incremental retraining.
to compare and evaluate the trade-offs of different budget-
constrained strategies to delay time decay. Since DL has
shown to be more robust to time decay (§4.4) than ALG1 and
ALG2, in this section we focus our attention these to show
performance-cost trade-offs of different mitigations.
5.1 Delay Strategies
We do not propose novel delay strategies, but instead focus on
how TESSERACT allows for the comparison of some popular
approaches to mitigating time decay. This shows researchers
how to adopt TESSERACT for the fair comparison of different
approaches when proposing novel solutions to delaying time
decay under budget constraints. We now summarize the delay
strategies we consider and show results on our dataset. For
interested readers, we include additional background knowl-
edge on these strategies in §A.4.
Incremental retraining. We ﬁrst consider an approach
that represents an ideal upper bound on performance, where
all points are included in retraining every month. This is
likely unrealistic as it requires continuously labeling all the
objects. Even assuming a reliance on VirusTotal, there is still
an API usage cost associated with higher query rates and the
approach may be ill-suited in other security domains. Figure 7
shows the performance of ALG1 and ALG2 with monthly
incremental retraining.
Active learning. Active Learning (AL) strategies inves-
tigate how to select a subset of test objects (with unknown
labels) that, if manually labeled and included in the training
set, should be the most valuable for updating the classiﬁcation
model [46]. Here, we consider the most popular AL query
strategy, uncertainty sampling, in which the points with the
most uncertain predictions are selected for retraining, under
the intuition that they are the most relevant to adjust decision
boundaries. Figure 8 reports the active learning results ob-
tained with uncertainty sampling, for different percentages of
objects labeled per month. We observe that even with 1% AL,
the performance already improves signiﬁcantly.
Table 3: Performance-cost comparison of delay methods.
Classiﬁcation with rejection. Another mitigation strategy
involves rejecting a classiﬁer’s decision as “low conﬁdence”
and delaying the decision to a future date [6]. This isolates
the rejected objects to a quarantine area which will later re-
quire manual inspection. Figure 9 reports the performance of
ALG1 and ALG2 after applying a reject option based on [26].
In particular, we use the third quartile of probabilities of in-
correct predictions as the rejection threshold [26]. The gray
histograms in the background report the number of rejected
objects per month. The second year of testing has more re-
jected objects for both ALG1 and ALG2, although ALG2
overall rejects more objects.
5.2 Analysis of Delay Methods
To quantify performance-cost trade-offs of methods to de-
lay time decay without changing the algorithm, we charac-
terize the following three elements: Performance (P), the
performance measured in terms of AUT to capture robustness
against time decay (§4.2); Labeling Cost (L), the number
of testing objects (if any) that must be labeled—the labeling
must occur periodically (e.g., every month), and is particularly
costly in the malware domain as manual inspection requires
many resources (infrastructure, time, expertise, etc)—for ex-
ample, Miller et al. [36] estimated that an average company
could manually label 80 objects per day; Quarantine Cost
(Q), the number of objects (if any) rejected by the classiﬁer—
these must be manually veriﬁed, so there is a cost for leaving
them in quarantine.
Table 3, utilizing AUT(F1,24m) while enforcing our con-
straints, reports a summary of labeling cost L, quarantine cost
Q, and two performance columns P, corresponding to training
with ˆσ and ϕ∗F1 (§4.3), respectively. In each row, we highlight
in purple cells (resp. orange) the column with the highest AUT
for ALG2 (resp. ALG1). Table 3 allows us to: (i) examine
the effectiveness of the training ratios ϕ∗F1 and ˆσ; (ii) analyze
the AUT performance improvement and the corresponding
costs for delaying time decay; (iii) compare the performance
of ALG1 and ALG2 in different settings.
First, let us compare ϕ∗F1 with ˆσ. The ﬁrst row of Table 3
represents the scenario in which the model is trained only
once at the beginning—the scenario for which we originally
USENIX Association
28th USENIX Security Symposium    739
1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg1F1(10-foldCV)F1(noupdate)Recall(mw)Precision(mw)F1(mw)1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg2(a) F1-Score
(c) Recall
Figure 8: Delay time decay: performance with active learning based on uncertainty sampling.
(b) Precision
the trend seen in the realistic settings of Table 1.
This section shows that TESSERACT is helpful to both
researchers and industrial practitioners. Practitioners need to
estimate the performance of a classiﬁer in the wild, compare
different algorithms, and determine resources required for
L and Q. For researchers, it is useful to understand how to
reduce costs L and Q while improving classiﬁers performance
P through comparable, unbiased evaluations. The problem is
challenging, but we hope that releasing TESSERACT’s code
fosters further research and widespread adoption.
6 Discussion
We now discuss guidelines, our assumptions, and how we
address limitations of our work.
Actionable points on TESSERACT. It is relevant to dis-
cuss how both researchers and practitioners can beneﬁt from
TESSERACT and our ﬁndings. A baseline AUT performance
(without classiﬁer retraining) allows users to evaluate the
general robustness of an algorithm to performance decay
(§4.2). We demonstrate how TESSERACT can reveal true
performance and provide counter-intuitive results (§4.4). Ro-
bustness over extended time periods is practically relevant for
deployment scenarios without the ﬁnancial or computational
resources to label and retrain often. Even with retraining
strategies (§5), classiﬁers may not perform consistently over
time. Manual labeling is costly, and the ML community has
worked on mitigation strategies to identify a limited number of
best objects to label (e.g., active learning [46]). TESSERACT
takes care of removing spatio-temporal bias from evaluations,
so that researchers can focus on the proposal of more robust
algorithms (§5). In this context, TESSERACT allows for the
creation of comparable baselines for algorithms in a time-
aware setting. Moreover, TESSERACT can be used with dif-
ferent time granularity, provided each period has a signiﬁcant
number of samples. For example, if researchers are interested
in increasing robustness to decay for the upcoming 3 months,
they can use TESSERACT to produce bias-free comparisons
of their approach with prior research, while considering time
decay.
Generalization to other security domains. Although we
used TESSERACT in the Android domain, our methodology
Figure 9: Delay time decay: classiﬁcation with rejection.
designed Algorithm 1 (§4.3 and Figure 6). Without methods
to delay time decay, ϕ∗F1 achieves better performance than
ˆσ for both ALG1 and ALG2 at no cost. In all other conﬁg-
urations, we observe that training ϕ = ϕ∗F1 always improves
performance for ALG2, whereas for ALG1 it is slightly ad-
vantageous in most cases except for rejection and AL 1%—in
general, the performance of ALG1 trained with ϕ∗F1 and ˆσ is
consistently close. The intuition for this outcome is that ϕ∗F1
and ˆσ are also close for ALG1: when applying the AL strat-
egy, we re-apply Algorithm 1 at each step and ﬁnd that the
average ϕ∗F1 ≈ 15% for ALG1, which is close to 10% (i.e., ˆσ).
On the other hand, for ALG2 the average ϕ∗F1 ≈ 50%, which
is far from ˆσ and improves all results signiﬁcantly. We can
conclude that our tuning algorithm is most effective when it
ﬁnds a ϕ∗P that differs from the estimated ˆσ.
Then, we analyze the performance improvement and related
cost of using delay methods. The improvement in F1-Score
granted by our algorithm comes at no labeling or quarantine
cost. We can observe that one can improve the in-the-wild
performance of the algorithms at some cost L or Q. It is
important to observe that objects discarded or to be labeled
are not necessarily malware; they are just the objects most
uncertain according to the algorithm, which the classiﬁer may
have likely misclassiﬁed. The labeling costs L for ALG1 and
ALG2 are identical (same dataset); in AL, the percentage of
retrained objects is user-speciﬁed and ﬁxed.
Finally, Table 3 shows that ALG1 consistently outperforms
ALG2 on F1 for all performance-cost trade-offs. This conﬁrms
740    28th USENIX Security Symposium
USENIX Association
1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg125%10%5%1%0%1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg21471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg125%10%5%1%0%1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg21471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg125%10%5%1%0%1471013161922Testingperiod(month)0.00.10.20.30.40.50.60.70.80.91.0Alg21471013161922Testingperiod(month)0200400600800100012001400160018002000220024002600#Quarantined1471013161922Testingperiod(month)0200400600800100012001400160018002000220024002600#Quarantined0.00.10.20.30.40.50.60.70.80.91.0F1(norejection)Recall(gw)Precision(gw)F1(gw)Recall(mw)Precision(mw)F1(mw)Alg10.00.10.20.30.40.50.60.70.80.91.0Alg2generalizes and can be immediately applied to any machine
learning-driven security domain to achieve an evaluation with-
out spatio-temporal bias. Our methodology, although general,
requires some domain-speciﬁc parameters that reﬂect realis-
tic conditions (e.g., time granularity ∆ and test time length).
This is not a weakness of our work, but rather an expected
requirement. In general, it is reasonable to expect that spatio-
temporal bias may afﬂict other security domains when af-
fected by concept drift and i.i.d. does not hold—however,
further experiments in other domains (e.g., Windows mal-
ware, code vulnerabilities) are required to make any scientiﬁc
conclusion. TESSERACT can be used to understand the extent
to which spatio-temporal bias affects such security domains;
however, the ability to generalize requires access to large
timestamped datasets, knowledge of realistic class ratios, and
code or sufﬁcient details to reproduce baselines.
Domain-speciﬁc in-the-wild malware percentage ˆσ. In
the Android landscape, we assume that ˆσ is around 10%
(§2.2). Correctly estimating the malware percentage in the
testing dataset is a challenging task and we encourage further
representative measurement studies [30, 53] and data sharing
to obtain realistic experimental settings.
Correct observation labels. We assume goodware and
malware labels in the dataset are correct (§2.3). Miller et
al. [36] found that AVs sometimes change their outcome over
time: some goodware may eventually be tagged as malware.
However, they also found that VirusTotal detections stabilize
after one year; since we are using observations up to Dec
2016, we consider VirusTotal’s labels as reliable. In the future,
we may integrate approaches for noisy oracles [15], which
assume some observations are mislabeled.
Timestamps in the dataset. It is important to consider
that some timestamps in a public dataset could be incorrect or
invalid. In this paper, we rely on the public AndroZoo dataset
maintained at the University of Luxembourg, and we rely on
the dex_date attribute as the approximation of an observa-
tion timestamp, as recommended by the dataset creators [3].
We further veriﬁed the reliability of the dex_date attribute
by re-downloading VirusTotal [20] reports for 25K apps3 and
verifying that the first_seen attribute always matched the
dex_date within our time span. In general, we recommend
performing some sanitization of a timestamped dataset be-
fore performing any analysis on it: if multiple timestamps
are available for each object, consider the most reliable times-
tamp you have access to (e.g., the timestamp recommended
by the dataset creators, or the VirusTotal’s first_seen at-
tribute) and discard objects with “impossible” timestamps
(e.g., with dates which are either too old or in the future),
which may be caused by incorrect parsing or invalid values
of some timestamps. To improve trustworthiness of the times-
tamps, one could verify whether a given object contains time
inconsistencies or features not yet available when the app
3We downloaded only 25K VT reports (corresponding to about 20% of
our dataset) due to restrictions on our VirusTotal API usage quota.
was released [29]. We encourage the community to promptly
notify dataset maintainers of any date inconsistencies. In the
TESSERACT’s project website (§8), we will maintain an up-
dated list of timestamped datasets publicly available for the
security community.
Choosing time granularity (∆). Choosing the length of
the time slots (i.e., time granularity) largely depends on the
sparseness of the available dataset: in general, the granularity
should be chosen to be as small as possible, while containing
a statistically signiﬁcant number of samples—as a rule of
thumb, we keep the buckets large enough to have at least
1000 objects, which in our case leads to a monthly granularity.
If there are restrictions on the number of time slots that can
be considered (perhaps due to limited processing power), a
coarser granularity can be used; however if the granularity
becomes too large then the true trend might not be captured.
Resilience of malware classiﬁers. In our study, we ana-
lyze three recent high-proﬁle classiﬁers. One could argue
that other classiﬁers may show consistently high performance
even with space-time bias eliminated. And this should indeed
be the goal of research on malware classiﬁcation. TESSER-
ACT provides a mechanism for an unbiased evaluation that
we hope will support this kind of work.
Adversarial ML. Adversarial ML focuses on perturbing
training or testing observations to compel a classiﬁer to make
incorrect predictions [7]. Both relate to concepts of robust-
ness and one can characterize adversarial ML as an artiﬁ-
cially induced worst-case concept drift scenario. While the
adversarial setting remains an open problem, the experimental
bias we describe in this work—endemic in Android malware
classiﬁcation—must be addressed prior to realistic evaluations
of adversarial mitigations.
7 Related Work
A common experimental bias in security is the base rate fal-
lacy [5], which states that in highly-imbalanced datasets (e.g.,
network intrusion detection, where most trafﬁc is benign),
TPR and FPR are misleading performance metrics, because
even FPR = 1% may correspond to millions of FPs and only
thousands of TPs. In contrast, our work identiﬁes experi-
mental settings that are misleading regardless of the adopted
metrics, and that remain incorrect even if the right metrics
are used (§4.4). Sommer and Paxson [47] discuss challenges
and guidelines in ML-based intrusion detection; Rossow et
al. [44] discuss best practices for conducting malware ex-
periments; van der Kouwe et al. [54] identify 22 common
errors in system security evaluations. While helpful, these
works [44,47,54] do not identify temporal and spatial bias, do
not focus on Android, and do not quantify the impact of errors
on classiﬁers performance, and their guidelines would not
prevent all sources of temporal and spatial bias we identify.
To be precise, Rossow et al. [44] evaluate the percentage of
objects—in previously adopted datasets—that are “incorrect”
(e.g., goodware labeled as malware, malfunctioning malware),