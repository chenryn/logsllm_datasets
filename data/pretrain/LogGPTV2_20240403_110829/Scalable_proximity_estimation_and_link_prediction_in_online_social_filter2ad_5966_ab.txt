*U
V[*,L]
V
P[*,L]
P
U
P[L,L]
~~
]
*
,
L
U
[
*
V[*,L]
P[*,L]
~~~~
U
*
V[*,L]
(b) factorize P[L,L] to get U[L,*], V[*,L]
(c) obtain U from P[*,L] and V[*,L]
P[L,*]
~~
]
*
,
L
*U
[
V
(d) obtain V from P[L,*] and U[L,*]
Figure 2: Proximity embedding
the PageRank for all the nodes can be precomputed efﬁciently
using the ﬁnite expansion method in Section 2.2.1.
2. Compute sub-matrices P [L, ∗] and P [∗, L] efﬁciently by com-
puting each row P [i, ∗] and each column P [∗, i] (i ∈ L) sepa-
rately as described in Section 2.2.1.
3. As shown in Figure 2(b), apply singular value decomposition
(SVD) to obtain the best rank-r approximation of P [L, L]:
P [L, L] ≈ U [L, ∗] · V [∗, L]
(18)
4. Our goal is to ﬁnd U and V such that U · V is a good approx-
imation of P . As a result, U · V [∗, L] should be a good ap-
proximation of P [∗, L]. We can therefore ﬁnd U such that U ·
V [∗, L] best approximates sub-matrix P [∗, L] in least-squares
sense (shown in Figure 2(c)). Given the use of SVD in step 3,
the best U is simply
U = P [∗, L] · V [∗, L]T
(19)
5. Similarly, ﬁnd V such that U [L, ∗] · V best approximates sub-
matrix P [L, ∗] in least-squares sense (shown in Figure 2(d)),
which is simply
V = U [L, ∗]T · P [L, ∗]
(20)
Accuracy. Unlike proximity sketch, proximity embedding does
not provide any provable data-independent accuracy guarantee. How-
ever, as a data-adaptive dimensionality reduction technique, when
matrix P is in fact low-rank (i.e., having good low-rank approxima-
tions), proximity embedding has the potential to achieve even better
accuracy than proximity sketch. Our empirical results in Section 4
suggest that this is indeed the case for the Katz measure.
2.2.4 Incremental Proximity Update
To enable online proximity estimation, we periodically check-
point M and use the above dimensionality reduction techniques to
approximate P for the last checkpoint of M . Between two check-
points, we apply an incremental update algorithm to approximate
P ′ = (I − β · M ′)−1, where M ′ = M + ∆ is the current matrix.
Our algorithm is based on the second-order approximation of P ′:
P ′ = [I − β(M + ∆)]−1
≈ (I − βM )−1 + β ∆ + β2 (∆M + M ∆ + ∆2)
(21)
325The second-order approximation works well as long as ∆ has
only few non-zero elements and β is small, making higher order
terms negligible.
To estimate an individual element P ′[x, y], we simply use:
If we checkpoint M frequently enough, the difference between
the last checkpoint M and the current matrix M ′ will be quite
small. In other words, the difference matrix ∆ is likely to be sparse.
As a result, we expect row ∆[x, ∗] and column ∆[∗, y] to have few
non-zero elements. By leveraging such sparseness, we can efﬁ-
ciently compute Eq. 22 in an online fashion. We demonstrate the
efﬁciency and accuracy of our incremental proximity update algo-
rithm in Section 4.2.3.
3. LINK PREDICTION TECHNIQUES
We use link prediction as a signiﬁcant application of our proxim-
ity estimation methods. Our goal is to understand (i) the effective-
ness of various proximity measures in the context of link prediction,
and (ii) the beneﬁt of combining multiple proximity measures. In
this section, we summarize the link predictors and the proximity
measures we use.
3.1 Link Predictors
We consider two types of link predictors: (i) basic link predic-
tor that uses a single proximity measure, and (ii) composite link
predictor that uses multiple proximity measures.
Basic link predictors. A basic link predictor consists of a prox-
imity measure prox[∗, ∗], and a threshold T . Given an input graph
G = (V, E) (which models a past snapshot of a given social net-
work), a node pair hx, yi 6∈ E is predicted to form an edge in the
future if and only if the proximity between x and y is sufﬁciently
large, i.e., prox[x, y] ≥ T .
Composite link predictors. A composite link predictor uses ma-
chine learning techniques to make link predictions based on multi-
ple proximity measures. We use the WEKA machine learning pack-
age [21] to automatically generate composite link predictors using
a number of machine learning algorithms, including the REPtree
decision tree learner, J48 decision tree learner, JRip rule learner,
support vector machine (SVM) learner, and Adaboost learner. The
results are consistent across different learners we use. So we only
report the results of the REPtree decision tree learner. REPtree is
a variant of the commonly used C4.5 decision tree learning algo-
rithm [46].
It builds a decision tree using information gain and
prunes it using reduced error pruning. It allows direct control on
the depth of the learned decision tree, making it easy to visualize
and interpret the resulting composite link predictor.
3.2 Proximity Measures
We consider three classes of proximity measures summarized in
Table 1, which are based on (i) graph distance, (ii) node neighbor-
hood, and (iii) ensemble of paths, respectively.
Notations. We model a social network as a graph G = (V, E),
where V is the set of nodes, and E is the set of edges. G can be ei-
ther directed or undirected. For a node x, let N (x) = {y|hx, yi ∈
E} be the set of neighbors x has in G. Similarly, let N −1(x) =
{y|hy, xi ∈ E} be the set of inverse neighbors x has in G (i.e.,
nodes that have x as their neighbors). Let A be the adjacency ma-
trix for G (deﬁned in Eq. 2). Let T = D−1A be the adjacency
P ′[x, y] ≈ P [x, y] + β∆[x, y] + Xk: ∆[x,k]6=0
Xk: ∆[k,y]6=0
β2M [x, k]∆[k, y] + Xk: ∆[x,k]6=0
β2∆[x, k]M [k, y]+
β2∆[x, k]∆[k, y]
(22)
Katz
graph distance
GD[x, y] = negated distance of the shortest
path from x to y
CN[x, y] = |N (x) ∩ N (y)|
common neighbors
AA[x, y] = Pz∈N(x)∩N(y)
Adamic/Adar
preferential attachment PA[x, y] = |N (x)| · |N (y)|
PageRank product
1
log |N(z)|
PRP[x, y] = P R(x) · P R(y), where
P R(x) = 1−d
|V | + d Pz∈N −1(x)
ℓ=1 βℓ · |pathshℓi
Katz[x, y] = P∞
x,y|
we have: Katz = (I − βA)−1 − I
P R(z)
|N(x)|
Table 1: Summary of proximity measures
matrix with row sums normalized to 1, where D is a diagonal ma-
trix with D[i, i] = Pj A[i, j].
Graph distance based proximity measure. Perhaps the most di-
rect metric for quantifying how close two nodes are is the graph
distance. We thus deﬁne a proximity measure GD[x, y] as the neg-
ative of the shortest-path distance from x to y. Note that the use of
negated (instead of original) shortest-path distance ensures that the
proximity measure GD[x, y] increases as x and y get closer.
Note that it is inefﬁcient to apply Dijkstra’s algorithm to com-
pute shortest path distance from x to y when G has millions of
nodes. Instead, we exploit the small-world property [27] of the so-
cial network and apply expanded ring search to compute the short-
est path distance from x to y. Speciﬁcally, we initialize S = {x}
and D = {y}. In each step we either expand set S to include its
members’ neighbors (i.e., S = S ∪ {v|hu, vi ∈ E ∧ u ∈ S})
or expand set D to include its members’ inverse neighbors (i.e.,
D = D ∪ {u|hu, vi ∈ E ∧ v ∈ D}). We stop whenever S ∩ D 6= ∅
— the number of steps taken so far gives the shortest path distance.
For efﬁciency, we always expand the smaller set between S and D
in each step. We also stop when a maximum number of steps is
reached (set to 6 in our evaluation).
Node neighborhood based proximity measures. We deﬁne four
proximity measures based on node neighborhood.
• Common neighbors. For two nodes x and y, they are more
likely to become friends when the overlap of their neighbor-
hoods is large. The simplest form of this approach is to count
the size of the intersection: CN[x, y] = |N (x) ∩ N (y)|.
• Adamic/Adar. Like common neighbors, Adamic/Adar [1] also
tries to measure the size of the intersection of two neighbor-
hoods. However, Adamic/Adar also takes ”rareness” into ac-
count, giving more weights to the common node with smaller
1
log |N(z)| .
number of friends: AA[x, y] = Pz∈N(x)∩N(y)
• Preferential attachment. The preferential attachment is based
on the idea that having a new neighbor is proportional to the
size of the current neighborhood. Moreover, the probability of
two users becoming friends is proportional to the product of the
number of the current friends. We therefore deﬁne a proximity
measure: PA[x, y] = |N (x)| · |N (y)|.
• PageRank product. PageRank is developed to analyze the hy-
perlink structure of Web pages by treating a hyperlink as a vote.
The PageRank of a node depends on the count of inbound links
and the PageRank of outbound neighbors. Formally, the PageR-
ank of a node x, denoted as P R(x), is deﬁned recursively on
G = (V, E) as
P R(x) =
1 − d
|V |
+ d Xz∈N −1(x)
P R(z)
|N (x)|
(23)
where d is a damping factor. We deﬁne the PageRank product
of two nodes x and y as the product of two PageRank values:
PRP[x, y] = P R(x) · P R(y).
326Path-ensemble based proximity measures. We use the Katz
measure (Katz[x, y]) as a path-ensemble based proximity measure
(described in Section 2.1). We use the Katz measure as the repre-
sentative of path-ensemble based proximity measures for two main
reasons. First, as shown in [30, 31], the Katz measure is the more
effective than other path-ensemble based proximity measures such
as the rooted PageRank. Second, our results in Section 4 show that
the accuracy of our proximity estimation methods is the highest for
the Katz measure.
4. EVALUATION
4.1 Dataset Description
Snapshot # of Conn-
Date ected Nodes
# of # of Added Asymmetric
Links Link Fraction
Network
Digg
Flickr
Live-
Journal
9/15/2008
10/25/2008
11/10/2008
3/01/2007
4/15/2007
5/18/2007
11/13/2008
12/05/2008
1/30/2009
MySpace 12/11/2008
1/11/2009
2/14/2009
4/30/2007
6/15/2007
7/23/2007
Wikipedia 9/30/2006
12/31/2006
4/06/2007
YouTube
Links
–
535,071 4,432,726
656,478
567,771 4,813,668
175,958
567,771 4,941,401
1,932,735 26,702,209
–
2,172,692 30,393,940 3,691,731
2,172,692 32,399,243 2,005,303
1,769,493 61,488,262
–
1,769,543 61,921,736 1,566,059
1,769,543 62,843,995 3,093,064
2,128,945 89,138,628
–
2,137,773 90,629,452 1,845,898
2,137,773 89,341,780
696,016
2,012,280 9,762,825
–
2,532,050 13,017,064 3,254,239
2,532,050 15,337,226 2,320,162
1,636,961 28,950,137
–
1,758,323 33,974,708 5,024,571
1,758,323 38,349,329 4,374,621
58.3%
37.8%
28.3%
0%
0%
83.1%
500,000 connected users (i.e., users with at least one incoming or
outgoing friendship link) out of 1.9 million crawled users.
Flickr [20] is a popular photo-sharing website. Flickr allows users
to add other people as “contacts” to form a directed social link. We
use the Flickr dataset collected by [36], which represents a breadth
ﬁrst search on the graph from a set of seed users. The dataset gives
the growth of Flickr for 104 days and contains 33 million links
among 2.3 million users. We treat the ﬁrst 25 days as the boot-
strap period to ensure that the crawl has sufﬁciently stabilized. We
then partition the remaining dataset into three snapshots separated
approximately by 40 days each. Note that, the third snapshot of
Flickr contains links for the same 2.17 million users that appear in
the second snapshot (and not the entire 2.3 million users).
is a Web community that allows its users to
LiveJournal [33]
post entries to personal journals. LiveJournal also acts as a so-
cial networking site, where a user can become a “fan” of another
LiveJournal user. We consider this “fan” relationship as a directed
friendship link in the social graph. Since LiveJournal does not pro-
vide a complete list of users, we obtained a list of active users who
have published posts by analyzing periodic RSS announcements of
recently updated journals starting from July 2008. We then used
the LiveJournal API to gather friendship information of 2.2 mil-
lion active users in November 2008, December 2008, and January
2009. The resulting snapshots have about 1.8 million connected
users who have non-zero friendship links.
is a social networking site where users can inter-
MySpace [40]
act with each other by personalizing pages, commenting on others’
photos and videos, and making friends. For two MySpace users to
become friends, both parties have to agree. Therefore, the social
links in MySpace are undirected and thus symmetric. We crawled
10 million MySpace users out of over 400 million users by taking
the ﬁrst 10 million user IDs in December 2008, January 2009, and
February 2009. After discarding all the inactive, deleted, private,
and solitary MySpace IDs, we get information for approximately
2.1 million users in each resulting snapshot.
YouTube [55] is a popular video-sharing website. Registered users
can connect with others by creating friendship links. We use the
undirected version of the social graph collected by [36], which cov-
ers the growth of YouTube for 165 days with 18 million added links
among 3.2 million users. We divide the dataset into three snapshots
separated by 45 days each. Note that the third snapshot of YouTube
in Table 2 contains links for 2.5 million users that also appear in the
second snapshot (and not the entire 3.2 million users).
Wikipedia [54] is an online encyclopedia which takes users’ col-
laboration to build content. Different wiki pages are connected
through hyperlinks. We compare Wikipedia’s hyperlink structure
against social graphs of users from the previous ﬁve online social
networks. Similar to general Web pages, most links in Wikipedia
are asymmetric. We use the data collected by [36] over a six-year
period from 2001 to 2007, which contains 38 million links con-
necting 1.8 million pages. We extract three snapshots separated
approximately by 90 days each.
4.2 Proximity Estimation Algorithms
In this section, we evaluate the accuracy and scalability of our
proximity estimation methods using the above six datasets. We
present results for Katz and RPR (deﬁned in Section 2). The ac-
curacy for escape probability (EP) is similar to RPR (due to their
close relationship in Eq. 5) and is omitted in the interest of brevity.
Accuracy metrics. We quantify the estimation error using three
different metrics: (i) Normalized Absolute Error (NAE) (deﬁned
as |esti−actuali|
meani(actuali) ), (ii) Normalized Mean Absolute Error (NMAE)
(deﬁned as Pi |esti−actuali|
), and (iii) Relative Error (deﬁned as
Pi actuali
Table 2: Dataset summary
We carry out our evaluation on ﬁve popular online social net-
works: Digg [14], Flickr [20], LiveJournal [33], MySpace [40],
and YouTube [55]. For comparison, we also examine the hyperlink
structure of Wikipedia [54]. For each network, we conduct three
crawls and make three snapshots of the network. Table 2 summa-
rizes the characteristics of the three snapshots for each of the net-
works. Note that, for the purpose of link prediction, we only use
connected nodes (i.e., nodes with at least one incoming or outgoing
friendship link), rather than considering all the crawled nodes. An-
other point to note is that since link prediction implies that based
on one snapshot of the network, we predict the new links that are
formed in the next snapshot, the same set of users should appear
in two consecutive snapshots. Hence, for a growing network, the
number of users appearing in the last snapshot that we create may
be less than the total number of users (to match the previous snap-
shot). Lastly, although there can be both link additions and dele-
tions between two snapshots, since the goal of link prediction is
to predict those that get added, we explicitly show the number of
added links between two consecutive snapshots in Table 2.
Digg [14] is a website for users to share interesting Web content by
posting a link to it. The posted link can be voted as either positive
(“digg”) or negative (“bury”) by other users. Digg allows a user to
become a “fan” of other users, which we consider as a friendship
relation. All the friendship links together form a directed social
graph. Overall, 58.3% directly connected user pairs in Digg have
asymmetric friendship (i.e., friendship link only exists in one direc-
tion between two users). We obtained the entire list of 1.9 million
users in September 2008. We crawled friendship links among these
users using the Digg API [15] in September 2008, October 2008,
and November 2008. The resulting snapshots contain more than
327Network
Digg
Flickr
LiveJournal
MySpace
YouTube
Wikipedia
PageRank based selection Uniform selection
0.00023
0.00238
0.07322
0.00032
0.05410
0.00328
0.00015
0.00010
0.01222
0.00016
0.02115
0.00266
Table 3: NMAE of different landmark selection schemes.
|esti−actuali|
actuali
), where esti and actuali denote the estimated and
actual values of the proximity measure for node pair i, respectively.
Since it is expensive to compute the actual proximity measures
over all the data points, we randomly sample 100,000 data points
by ﬁrst randomly selecting 200 rows from the proximity matrix
and then selecting 500 elements from each of these rows. We then
compute errors for these 100,000 data points.
4.2.1 Proximity Embedding
We ﬁrst evaluate the accuracy of proximity embedding. We aim
to answer the following questions: (i) How accurate is proximity
embedding in estimating Katz and RPR? (ii) How many dimen-
sions and landmarks are required to achieve high accuracy? (iii)
How does the landmark selection algorithm affect accuracy?
Parameter settings. Throughout our evaluation, we use a damp-
ing factor of β = 0.05, ℓmax = 6, and 1600 landmarks unless
otherwise speciﬁed. We also vary these parameters to understand
their impact. By default, we select landmarks based on the PageR-
ank of each node. Speciﬁcally, we ﬁrst compute PageRank for each
node and normalize the sum of PageRank of all nodes to 1. We then
use the normalized PageRank as the probability of assigning a node
as a landmark. In this way, nodes with high PageRank values are
more likely to become landmarks. For comparison, we also exam-
ine the performance of uniform landmark selection, which selects
landmarks uniformly at random.
Varying the number of dimensions. Figure 3 plots the CDF of
normalized absolute errors in approximating the Katz measure as
we vary the number of dimensions from 5 to 60. We make the fol-
lowing two key observations. First, for all six datasets the normal-
ized absolute error is small: in more than 95% cases the normal-
ized absolute error is within 0.05 and NMAE is within 0.05 except
YouTube. The error in YouTube is higher because its “intrinsic” di-
mensionality is higher as analyzed in Figure 6 (see below). Second,
as we would expect, the error decreases with the number of dimen-
sions. The reduction is more signiﬁcant in the YouTube dataset,
because the other datasets have very low “intrinsic” dimensional-
ity and using only 5 dimensions already gives low approximation
error, whereas YouTube has higher “intrinsic” dimensionality and
increasing the number of dimensions is more helpful.
Relative errors. Figure 4 further plots the CDF of relative errors
using 60 dimensions. We take top 1%, 5%, and 10% of the ran-
domly selected data points and generate the CDF for each of the
selections. In all datasets, we observe that the relative errors are
smaller for elements with larger values. This is desirable because
larger elements play a more important role in many applications
and are thus more important to estimate accurately.
Uniform landmark selection. Table 3 compares the NMAE of
PageRank based landmark selection and uniform selection. PageR-
ank based selection yields higher accuracy than uniform selection.
It reduces NMAE by 35% for Digg, 95.8% for Flickr, 83.3% for
LiveJournal, 50% for MySpace, 61% for YouTube, and 19% for
Wikipedia. The reason is that high-PageRank nodes are well con-
nected, and it is less likely for nodes to be far away from all such
landmarks, thereby improving the estimation accuracy.
Network
Digg
Flickr
LiveJournal
MySpace
YouTube
Wikipedia