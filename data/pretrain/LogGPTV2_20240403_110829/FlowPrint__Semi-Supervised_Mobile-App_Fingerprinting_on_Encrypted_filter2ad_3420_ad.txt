0.4284
0.2534
0.8867
0.1484
0.5028
0.1956
0.1501
0.5048
0.5386
0.6005
0.5797
0.3989
0.8693
0.2430
0.5757
0.2982
0.2407
0.6340
0.6642
0.7248
0.4284
0.2534
0.8867
0.1484
0.5028
0.1956
0.1501
0.5048
0.5386
0.6005
Table IV shows the performance of both FLOWPRINT and
AppScanner. We note that the accuracy and recall levels are the
same, which is due to computing the micro-average metrics for
the individual apps. This is often regarded as a more precise
metric for computing the precision, recall and F1-score and has
the side effect that the accuracy equals the recall [32]. Despite
competing with a supervised learning method, we see that both
AppScanner and our approach have similar levels of precision,
meaning they are able to correctly classify network ﬂows to
their corresponding app. However, we outperform AppScanner
greatly on the recall, meaning that our approach is much better
at classifying all types of trafﬁc, whereas AppScanner provides
a sufﬁcient certainty level for only a small fraction of apps.
We note that in our experiments, AppScanner has a lower
performance than reported in the original paper, especially
for the recall. The cause is twofold: First, most apps in our
datasets are captured over shorter periods of time, making
it more difﬁcult to recognize apps. Second, the AppScanner
paper reported only on ﬂows for which they have a conﬁdence
level ≥ 0.7, which in their dataset was 79.4% of ﬂows.
This means that unclassiﬁed ﬂows are not taken into account.
As unrecognized ﬂows reveal much about
the recognition
approach, our work reports the performance over all ﬂows,
where unrecognized ﬂows cause lower recall rates.
Dataset independence. Our evaluation shows that FLOW-
PRINT performs well on both synthetic (ReCon and Andrubis)
and human-generated (Cross Platform) trafﬁc. Furthermore,
the results from the Cross Platform dataset show that our
approach can be used to generate ﬁngerprints for both iOS
and Android apps. However, this does not necessarily mean
that a ﬁngerprint generated for an iOS app can be used to
detect the corresponding Android version or vice versa. In
the Andrubis dataset, we observed no signiﬁcant difference
between recognizing benign and potentially harmful apps.
Moreover, the ﬂow experiment (see Table IV) shows that apps
generating a small amount of ﬂows are more difﬁcult to detect.
As a result, our approach has to ﬁnd correlations between
trafﬁc in a limited timeframe resulting in a lower precision.
This is a known limitation of network-based approaches and
also affects related tools such as AppScanner.
C. Detection of Previously Unseen Apps
existing one. This isolation allows also us to distinguish be-
tween different unseen apps. Subsequently, when FLOWPRINT
detects a previously unseen app, the security operator can
choose to include the new ﬁngerprints in the database. From
that point forward, the new app will be classiﬁed as known
and can be recognized as in Section V-B. For this setting,
we create ﬁngerprints for the apps that are present on the
device. Subsequently, we add previously unseen apps to the
evaluation and generate ﬁngerprints for all the apps present
during this testing phase. Our work uses the same parameters
from Section V-A for detecting unseen apps. However, in
order to decide whether a ﬁngerprint originates from a new
or existing app, we introduce a different threshold τnew. This
threshold indicates the maximum Jaccard similarity between a
tested ﬁngerprint and all training ﬁngerprints to be considered
a new app. Note that
the more
conservative we are in ﬂagging ﬁngerprints as originating
from new apps. The rationale for introducing this additional
threshold is that ﬁngerprints remain the same for the entire
approach, but are interpreted differently depending on the use
case. When detecting unseen apps, we suggest the use of
a threshold of 0.1, meaning that only ﬁngerprints that have
an overlap of less than 0.1 with all existing ﬁngerprints are
considered new apps. Comparing ﬁngerprinting approaches for
detecting unseen apps is difﬁcult because, as far as we are
aware, the only network-based approaches for detecting unseen
apps are DECANTeR [15] and HeadPrint [16]. Unfortunately,
both detectors only handle unencrypted data, thus they cannot
be applied on encrypted data like ours. Hence, we are unable
to compare our approach with related work in this setting.
the lower this threshold,
As in previous experiments, we assume each device has
100 apps installed, and introduce 20 new apps. We evaluate our
detector by running a 10-fold cross validation using τnew = 0.1.
A low τnew threshold ensures that known apps are not detected
as new despite the dynamic nature of apps. As a trade-off,
the detector does not correctly classify all ﬂows of previously
unseen apps. However, we argue that correctly classifying all
ﬂows of unseen apps is infeasible as large parts of many apps
are shared in the form of common libraries. This means that
it is preferable to aim for a high precision in ﬂows ﬂagged
as new apps rather than a high recall as long as previously
unseen apps can be detected at some point.
In addition to app recognition, we evaluate the capabilities
of our ﬁngerprinting approach to detect previously unseen
apps. Here, we want FLOWPRINT to be able to correctly isolate
an unseen app as a new app, instead of classifying it as an
Table V shows the results of our experiment. We see
that
the precision is reasonably high and 97.8% of ﬂows
are correctly ﬂagged as unseen for ReCon and 99.5% for
ReCon extended. This also means that existing apps are rarely
9
TABLE V.
PERFORMANCE OF OUR APPROACH WHEN DETECTING
UNSEEN APPS. TRUE POSITIVES = CORRECTLY IDENTIFIED NEW APPS;
TRUE NEGATIVES = CORRECTLY IDENTIFIED KNOWN APPS; FALSE
POSITIVES = KNOWN APPS CLASSIFIED AS NEW; FALSE NEGATIVES = NEW
APPS CLASSIFIED AS KNOWN.
Dataset
ReCon
ReCon extended
Cross Platform (Android)
Cross Platform (iOS)
Cross Platform (Average)
Andrubis (≥ 1 ﬂow)
Andrubis (≥ 10 ﬂows)
Andrubis (≥ 100 ﬂows)
Andrubis (≥ 500 ﬂows)
Andrubis (≥ 1000 ﬂows)
Precision
0.9777
0.9948
0.9106
0.9637
0.9352
0.4757
0.5703
0.8405
0.7722
0.7939
Recall
0.7098
0.2032
0.4318
0.7744
0.5449
0.2090
0.2552
0.4760
0.3121
0.3444
F1-score
Accuracy
0.8225
0.3375
0.5858
0.8588
0.6886
0.2904
0.3526
0.6078
0.4446
0.4804
0.8550
0.5494
0.6634
0.8527
0.7253
0.5100
0.4965
0.6386
0.5915
0.6177
the recall
marked as unseen, reducing the load on any manual checking
of alert messages. On the Cross Platform dataset, we achieve
93.5% precision on average indicating that, while slightly more
difﬁcult, our approach is still capable of detecting new apps
without raising too many false alerts. For Andrubis, the rate
of false positives is higher with 14.8% for apps producing at
least 100 ﬂows. This is due to the relatively short time span in
which trafﬁc of this dataset was produced, i.e., 240 seconds.
Recall. We see that
is signiﬁcantly lower than
the precision, only reaching 20.3% for the ReCon extended
dataset. This is caused by homogeneous behavior of mobile
apps, i.e., the network trafﬁc of these apps overlaps due to
the use of common libraries and services. In the experiments
of Table V we found that unknown apps showed similar
advertisement trafﬁc to known apps. When the similarity ot
the unknown app results in a higher matching score than τnew,
it will be misclassiﬁed as known. This is less of a problem in
the app recognition scenario where FLOWPRINT searches for a
best match. Multiple training ﬁngerprints can have a similarity
score > τnew, but the actual app likely produces the highest
score due to most overlapping destinations, leading to a correct
match. We elaborate on the effects of homogeneous trafﬁc in
Section V-E. As stated before, low recall is not necessarily
problematic as long as the unseen app is detected at some
point. In our experiment, we already detect 72.3% of apps in
the ﬁrst batch (ﬁve minutes) in which they appear. We discuss
further limitations of app ﬁngerprinting in Section VI.
D. Fingerprinting Insights
In the previous experiments we demonstrated that our
approach works for both recognizing already seen apps, as
well as detecting unseen apps. In this section, we evaluate
speciﬁc parts of our ﬁngerprinting approach to give insights
into possible other use cases.
Browser isolation. We ﬁrst highlight
the performance of
the browser detector component within our approach. In this
experiment we use both the browser dataset and the Andrubis
dataset as discussed in Section II-A. As the browser detector
is supervised, it performs better when trained with a large
set of applications, hence the Andrubis dataset is a natural
choice for this evaluation. To this end, we randomly selected
5,000 non-browser apps from the Andrubis dataset to represent
non-browser data. Of these apps, we used an 80:20 split for
training and testing our detector respectively. Recall that when
TABLE VI.
PERFORMANCE OF THE BROWSER DETECTOR BASED ON
THE NUMBER OF DETECTED TCP/UDP STREAMS.
Predicted Browser
Predicted non-Browser
Actual Browser
Actual non-Browser
21,987 (TP)
363 (FN)
5,574 (FP)
28,4125 (TN)
we detect a browser, all ﬂows within a surrounding 20 second
window are marked as browser trafﬁc. This window was
empirically optimized to achieve high recall rates. To ensure
that wrong detections are properly penalized in our experiment,
we interleave the browser and non-browser trafﬁc by shifting
all timestamps such that each trace starts at the same time.
Note that, while there exist apps that embed a “browser
window” (e.g., Android WebView), we do not consider these
apps as browsers because of their conﬁned access to a limited
set of network destinations. In contrast, real browsers navigate
to many different websites, producing a bigger relative change
of active clusters—one of the features of our browser isolation.
In fact, our datasets contain several HTML5 apps, which we
correctly detected as regular apps.
Table VI shows the average performance of the browser
detector using ten Monte Carlo cross validations. Our detec-
tor achieves, on average, an accuracy of 98.1% and detects
browser ﬂows with a recall of 98.3%. Unfortunately, with a
precision of 79.8% the number of wrongly isolated streams is
rather high due to the aggressive detection. This in turn leads to
1.8K of 25.8K non-browser clusters being incorrectly removed
at some point. Fortunately, 75.7% of these clusters resurfaced
after the initial removal without being mistakenly detected as
a browser. This means they are still used for ﬁngerprinting
their corresponding non-browser apps. In total only 1.7% of
non-browser clusters were permanently removed.
Conﬁdence. FLOWPRINT assigns unlabeled ﬁngerprints to
each ﬂow passing through. To gain more insights into how
these ﬁngerprints are represented we assign a conﬁdence level
to each ﬁngerprint that measures how certain we are that each
ﬂow within a ﬁngerprint belongs to the same app. In order
to measure conﬁdence, we look at the amount of information
gained by knowing to which ﬁngerprint a ﬂow belongs to with
respect to the app label of that ﬂow. That is, we measure by
what fraction the entropy of app labels is reduced if we know
the ﬁngerprint of each ﬂow. Equation 6 shows the formula for
computing the conﬁdence of our ﬁngerprints. Here, H(A|F )
is the entropy of app labels for each ﬂow, given that we
know its ﬁngerprint. H(A) is the entropy of the labels without
knowing the ﬁngerprints. When all ﬁngerprints only consist of
ﬂows of a single app knowing that ﬁngerprint automatically
leads to knowing the label. Therefore, H(A|F ) = 0 gives a
conﬁdence level of 1. In case knowing the ﬁngerprint does
not provide additional information regarding the app label of
a ﬂow H(A|F ) = H(A) and therefore, the conﬁdence level
is 0. In clustering, this is referred to as homogeneity [54].
Conﬁdence = 1 − H(A|F )
H(A)
(6)
Table VII shows the conﬁdence level of ﬁngerprints produced
by our approach for each dataset. We see that for each dataset
we achieve conﬁdence levels close to 1 meaning that the
majority of our ﬁngerprints contain only ﬂows of a single app.
10
TABLE VII.
CONFIDENCE LEVELS OF OUR FINGERPRINTS. A SCORE
OF 1 INDICATES FINGERPRINTS ONLY CONTAIN FLOWS OF A SINGLE APP.
Dataset
ReCon
ReCon extended
Cross Platform (Android)
Cross Platform (iOS)
Cross Platform (Total)
Andrubis
Conﬁdence
0.9857
0.9670
0.9740
0.9887
0.9864
0.9939
Cardinality. Each app is ideally represented by a single ﬁn-
gerprint. This would make it possible to automatically separate
the network trafﬁc into bins of different apps. However, this
might be infeasible as mobile apps offer different function-
alities which may result in multiple ﬁngerprints. Therefore,
we also investigate the number of ﬁngerprints our approach
generates for each app. We recall that an app can be viewed
as a combination of individual modules, including third-party
libraries, that each account for part of the app’s functionality.
This naturally leads to apps presenting multiple ﬁngerprints.
We refer to the number of ﬁngerprints generated per app as
the cardinality of each app.
Figure 4 displays the cardinality of apps in our datasets
the majority of apps in all datasets have
and shows that
multiple ﬁngerprints. Our previous evaluations have shown
that this is not a problem for the app recognition and unseen
app detection settings. However, the cardinality of apps in
our work should be taken into account in case a new app
is detected. Here, security operators should be aware that
there will likely emerge multiple ﬁngerprints for that new app.
We note that the ReCon extended dataset is not shown in
this graph since all apps in that dataset had more than 20
ﬁngerprints. This is in large part due to the fact that apps
in the ReCon extended dataset contain more versions, which
introduce additional ﬁngerprints (also see Section V-E). On
average, each version in the ReCon extended dataset contained
18 ﬁngerprints. This number of ﬁngerprints per version is still
higher than the other datasets because each app was exercised
longer, leading to more app functionality being tested, which in
turn led to more ﬁngerprints. Finally, Figure 4 shows that apps
in the Cross Platform dataset have a higher average cardinality
than the other datasets. This suggests that user interaction leads
to more ﬁngerprints describing individual app functionalities
rather than the entire app itself.
E. Mobile Network Trafﬁc Challenges
We evaluate the effect of the three properties (Section I)
of the mobile network trafﬁc that pose challenges for our
approach: its homogeneous, dynamic and evolving nature.
(1) Homogeneous trafﬁc. The ﬁrst challenge is that mobile
trafﬁc is homogeneous because trafﬁc is encrypted and many
apps share the same network destinations, for example due to
shared third-party libraries, or the use of CDNs and common
cloud providers. In this experiment, we analyze to what extent
the homogeneity caused by shared network destinations affects
the performance of our approach. We analyzed the ReCon
dataset, which includes DNS information for each ﬂow, as
well as a classiﬁcation of each DNS address as a ﬁrst-party or
third-party destination for each app, allowing us to investigate
ReCon
Andrubis
Cross Platform
5