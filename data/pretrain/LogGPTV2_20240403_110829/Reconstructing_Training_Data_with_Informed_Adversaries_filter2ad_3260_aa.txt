title:Reconstructing Training Data with Informed Adversaries
author:Borja Balle and
Giovanni Cherubin and
Jamie Hayes
7
7
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
2022 IEEE Symposium on Security and Privacy (SP)
Reconstructing Training Data
with Informed Adversaries
Borja Balle*
DeepMind
Giovanni Cherubin*†
Microsoft Research
Jamie Hayes*
DeepMind
Abstract—Given access to a machine learning model, can an
adversary reconstruct the model’s training data? This work
studies this question from the lens of a powerful
informed
adversary who knows all the training data points except one. By
instantiating concrete attacks, we show it is feasible to reconstruct
the remaining data point in this stringent threat model. For
convex models (e.g. logistic regression), reconstruction attacks
are simple and can be derived in closed-form. For more general
models (e.g. neural networks), we propose an attack strategy
based on training a reconstructor network that receives as input
the weights of the model under attack and produces as output
the target data point. We demonstrate the effectiveness of our
attack on image classifiers trained on MNIST and CIFAR-10,
and systematically investigate which factors of standard machine
learning pipelines affect reconstruction success. Finally, we theo-
retically investigate what amount of differential privacy suffices
to mitigate reconstruction attacks by informed adversaries. Our
work provides an effective reconstruction attack that model
developers can use to assess memorization of individual points in
general settings beyond those considered in previous works (e.g.
generative language models or access to training gradients); it
shows that standard models have the capacity to store enough
information to enable high-fidelity reconstruction of training
data points; and it demonstrates that differential privacy can
successfully mitigate such attacks in a parameter regime where
utility degradation is minimal.
Index Terms—machine learning, neural networks, reconstruc-
tion attacks, differential privacy
I. INTRODUCTION
Machine learning (ML) models have the capacity to mem-
orize their training data [1], and such memorization is some-
times unavoidable while training highly accurate models [2,
3, 4]. When the training data is sensitive, sharing models
that exhibit memorization can lead to privacy breaches. To
design mitigations enabling privacy-preserving deployment of
ML models we must understand how these breaches arise and
how much information they leak about individual data points.
Membership leakage is considered the gold standard for
privacy in ML, both from the point of view of empirical
privacy evaluation (e.g., via membership inference attacks
(MIA) [5]) as well as mitigation (e.g., differential privacy (DP)
[6]). Membership information represents a minimal level of
leakage: it allows an adversary to infer a single bit determining
if a given data record was present in the training dataset. Mod-
els trained on health data represent a prototypical application
where membership can be considered sensitive: the presence
*Equal contribution
†Work done while at the Alan Turing Institute
Fig. 1: Examples of training data points reconstructed from a
55K parameter CNN classifier trained on CIFAR-10.
of an individual’s record in a dataset might itself be indicative
of whether they were tested or treated for a medical condition.
Reconstruction of training data from ML models sits at the
other extreme of the individual privacy leakage spectrum: a
successful attack enables an adversary to reconstruct all the
information about an individual record that a model might
have seen during training. The possibility of extracting training
data from models can pose a serious privacy risk even in
applications where membership information is not directly
sensitive. For example, reconstruction of individual images
from a model trained on pictures that were privately shared in
a social network can be undesirable even if that individual’s
membership in the social network is public information.
Existing evidence of the feasibility of reconstruction attacks
is sparse and focuses on specialized use cases. For example,
recent work on generative language models highlights their
capacity to memorize and regurgitate some of their training
data [7, 8], while works on gradient
inversion show that
adversaries with access to model gradients (e.g. in federated
learning (FL) [9]) can use this information to reconstruct
training examples [10]. Similarly, attribute inference attacks
reconstruct a restricted subset of attributes of a training data
point given the rest of its attributes [11], while property
inference attacks infer global information about the training
distribution rather than individual points [12, 13].
Our work proposes a general approach to study the fea-
sibility of reconstruction attacks against ML models without
assumptions on the type of model or access to intermediate
gradients, and initiates a study of mitigation strategies capable
of preventing this kind of attacks. The starting point is the
instantiation of an informed adversary that, knowing all the
records in a training data set except one, attempts to recon-
© 2022, Borja Balle. Under license to IEEE.
DOI 10.1109/SP46214.2022.00127
1138
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
TargetReconstructionstruct the unknown record after obtaining white-box access to
a released model. This choice of adversary is inspired by the
(implicit) threat model in DP [14].
Working with such a powerful, albeit unrealistic, adversary
enables us to demonstrate the feasibility of reconstruction, both
in theory against convex models as well as experimentally
against standard neural network architectures for image classi-
fication. Furthermore, the use of an informed adversary makes
our work relevant for provable mitigations: effective defenses
against optimal informed adversaries will also protect against
attacks run by less powerful and more realistic adversaries.
A. Overview of Contributions and Paper Outline
We start by introducing and motivating the informed adver-
sary threat model (Section II). Our first contribution is a the-
oretical analysis of reconstruction attacks against simple ML
models like linear, logistic, and ridge regression (Section III).
We show that for a broad class of generalized convex linear
models, access to the maximum likelihood solution enables an
informed adversary to recover the target point exactly.
In the convex setting, the attack relies on solving a simple
system of equations. Extending reconstruction attacks to neural
networks requires a different approach due to the inherent
non-convexity of the learning problem. In Section IV, we
propose a generic approach to reconstruction attacks based on
reconstructor networks (RecoNN): networks that are trained
by the adversary to output a reconstruction of the target point
when given as input the parameters of a released model.
Our second contribution is to show that it is feasible to
attack standard neural network classifiers using reconstructor
networks; we present effective RecoNN architectures and
training procedures, and show they can extract high-fidelity
training images from classifiers trained on MNIST1 and
CIFAR-10. Figure 1 provides an illustration of reconstructions
produced by a RecoNN-based attack against a convolutional
neural network (CNN) classifier trained on CIFAR-10. These
experiments provide compelling evidence that image classifi-
cation models can store in their weights enough information
to reconstruct individual training data points.
Section VI describes our third contribution: an in-depth
analysis around what factors affect the success of our RecoNN-
based attack. These include hyper-parameter settings in the
model training pipeline, degree of access to model parameters,
and quality and quantity of side knowledge available to the
adversary. We also explore how different levels of knowledge
about the internal randomness of stochastic gradient descent
(SGD) affect reconstruction; we observe that knowing the
model’s initialization significantly improves the quality of
reconstructions, while knowing the randomness used for mini-
batch sampling is not necessary for good reconstruction.
As part of our experiments, we also investigate the use of
DP-SGD [15] as a mitigation to protect against reconstruction
attacks. We find that large values of ϵ suffice to defend against
1A minimal
is
available
reconstruction.
implementation of our
at
reconstruction attack on MNIST
https://github.com/deepmind/informed adversary mnist
TABLE I: Summary of notation
Model Developer
Data domain
Z
Θ Model domain
D Training dataset
n
A Training algorithm
θ
Released model
size of training set (includes target point)
Reconstruction Adversary
Training dataset minus target point
Target point
Reconstruction algorithm
Side knowledge about z
Candidate reconstruction
Reconstruction error
D-
z
R
aux
ˆz
ℓ
our best RecoNN-based attacks – in fact, values that are much
larger than what is necessary to protect against membership
inference attacks by informed adversaries [14]. Section VII
supports this observation by introducing a definition of re-
construction robustness, analyzing its relation to the (R´enyi)
DP parameters of the training algorithm, and showing that,
under mild conditions on the adversary’s side knowledge,
ϵ = o(d) suffices to prevent reconstruction of d-dimensional
data records.
II. RECONSTRUCTION WITH INFORMED ADVERSARIES
We start by instantiating and justifying the informed ad-
versary threat model for reconstruction attacks against ML
models, and by comparing it to related attacks in the literature.
Notation for the most important concepts introduced in this
section is summarized in Table I. At its core, our threat model
assumes a powerful adversary with white-box access to a
model released by a model developer. The developer owns
a dataset D ∈ Z n of n training records from some domain Z,
and a (possibly randomized) training algorithm A : Z n → Θ.
They train (the parameters of) a model θ = A(D), and then
release it as part of a system or service. For example, records
in D may be feature-label pairs in standard supervised learning
settings, and A may implement an optimization algorithm (e.g.
SGD or Adam) for a loss function associated with D and Θ.
A. Threat Model
A reconstruction adversary with access to the released
model aims to infer enough information about its training data
to reconstruct one of the examples in D. In this paper, we
consider a powerful adversary who already has full knowledge
about all but one of the training points. Formally, they have
access to the following information to carry out the attack.
Definition 1 (Informed reconstruction adversary). Let θ be a
model trained on dataset D of size n using algorithm A. Let
z ∈ D be an arbitrary training data point and D- = D \ {z}
denote the remaining n− 1 points; we refer to z as the target