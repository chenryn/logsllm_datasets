trieves objects from a hibernate database by passing
its input string argument through a sequence of calls to
a SQL execute statement. As a result, all invocations of
Session.find with unsafe data, such as the two errors
we found in personalblog, may suffer from SQL injec-
tions. A few other public methods such as iterate and
delete also turn out to be attack vectors. Our ﬁndings
highlight the importance of securing commonly used
software components in order to protect their clients.
USENIX Association
14th USENIX Security Symposium
283
6.3.4 Cross-site Tracing Attacks
Analysis of webgoat and several other applications re-
vealed a previously unknown vulnerability in core J2EE
libraries, which are used by thousands of Java applica-
tions. This vulnerability pertains to the TRACE method
speciﬁed in the HTTP protocol. TRACE is used to echo
the contents of an HTTP request back to the client for
debugging purposes. However, the contents of user-
provided headers are sent back verbatim, thus enabling
cross-site scripting attacks.
In fact, this variation of cross-site scripting caused
by a vulnerability in HTTP protocol speciﬁcation was
discovered before, although the fact that it was present
in J2EE was not previously announced. This type of
attack has been dubbed cross-site tracing and it is re-
sponsible for CERT vulnerabilities 244729, 711843, and
728563. Because this behavior is speciﬁed by the HTTP
protocol, there is no easy way to ﬁx this problem at
the source level. General recommendations for avoiding
cross-site tracing include disabling TRACE functionality
on the server or disabling client-side scripting [18].
6.4 Analysis Features and False Positives
The version of our analysis that employs both context
sensitivity and improved object naming described in Sec-
tion 4 achieves very precise results, as measured by the
number of false positives.
In this section we examine
the contribution of each feature of our static analysis ap-
proach to the precision of our results. We also explain
the causes of the remaining 12 false positives reported by
the most precise analysis version. To analyze the impor-
tance of each analysis feature, we examined the number
of false positives as well as the number of tainted objects
reported by each variation of the analysis. Just like false
positives, tainted objects provide a useful metric for an-
alysis precision: as the analysis becomes more precise,
the number of objects deemed to be tainted decreases.
Figure 10(a) summarizes the results for the four differ-
ent analysis versions. The ﬁrst part of the table shows the
number of tainted objects reported by the analysis. The
second part of the table shows the number of reported
security violations. The third part of the table summa-
rizes the number of false positives. Finally, the last col-
umn provides the number of real errors detected for each
benchmark. Figure 10(b) provides a graphical represen-
tation of the number of tainted objects for different anal-
ysis variations. Below we summarize our observations.
Context sensitivity combined with improved object
naming achieves a very low number of false positives. In
fact, the number of false positives was 0 for all applica-
tions but snipsnap. For snipsnap, the number of false
positives was reduced more than 50-fold compared to the
context-insensitive analysis version with no naming im-
provements. Similarly, not counting the small program
jboard, the most precise version on average reported 5
times fewer tainted objects than the least precise. More-
over, the number of tainted objects dropped more that 15-
fold in the case of roller, our largest benchmark.
All 12 of
the false positives
To achieve a low false-positive rate, both context sen-
sitivity and improved object naming are necessary. The
number of false positives remains high for most pro-
grams when only one of these analysis features is used.
One way to interpret the importance of context sensitiv-
ity is that the right selection of object “names” in pointer
analysis allows context sensitivity to produce precise re-
sults. While it is widely recognized in the compiler com-
munity that special treatment of containers is necessary
for precision, improved object naming alone is not gener-
ally sufﬁcient to completely eliminate the false positives.
reported by the
most precise version for our analysis were located
in snipsnap and were caused by insufﬁcient preci-
sion of the default allocation site-based object-naming
scheme. The default naming caused an allocation site
in snipsnap to be conservatively considered tainted
because a tainted object could propagate to that al-
location site. The allocation site in question is lo-
cated within StringWriter.toString(), a JDK func-
tion similar to String.toLowerCase() that returns a
tainted String only if the underlying StringWriter is
constructed from a tainted string. Our analysis conser-
vatively concluded that the return result of this method
may be tainted, causing a vulnerability to be reported,
where none can occur at runtime. We should men-
tion that all the false positives in snipsnap are elim-
inated by creating a new object name at every call to,
StringWriter.toString(), which is achieved with a
one-line change to the pointer analysis speciﬁcation.
7 Related Work
In this section, we ﬁrst discuss penetration testing and
runtime monitoring, two of the most commonly used ap-
proaches for ﬁnding vulnerabilities besides manual code
reviews. We also review the relevant literature on static
analysis for improving software security.
7.1 Penetration Testing
Current practical solutions for detecting Web applica-
tion security problems generally fall into the realm of
penetration testing [3, 5, 15, 36, 44]. Penetration testing
involves attempting to exploit vulnerabilities in a Web
application or crashing it by coming up with a set of
appropriate malicious input values. Penetration reports
usually include a list of identiﬁed vulnerabilities [25].
However, this approach is incomplete. A penetration test
can usually reveal only a small sample of all possible se-
curity risks in a system without identifying the parts of
the system that have not been adequately tested. Gener-
284
14th USENIX Security Symposium
USENIX Association
ally, there are no standards that deﬁne which tests to run
and which inputs to try. In most cases this approach is not
effective and considerable program knowledge is needed
to ﬁnd application-level security errors successfully.
7.2 Runtime Monitoring
A variety of both free and commercial runtime mon-
itoring tools for evaluating Web application security are
available. Proxies intercept HTTP and HTTPS data be-
tween the server and the client, so that data, including
cookies and form ﬁelds, can be examined and modiﬁed,
and resubmitted to the application [9, 42]. Commercial
application-level ﬁrewalls available from NetContinuum,
Imperva, Watchﬁre, and other companies take this con-
cept further by creating a model of valid interactions be-
tween the user and the application and warning about vi-
olations of this model. Some application-level ﬁrewalls
are based on signatures that guard against known types
of attacks. The white-listing approach speciﬁes what
the valid inputs are; however, maintaining the rules for
white-listing is challenging. In contrast, our technique
can prevent security errors before they have a chance to
manifest themselves.
7.3 Static Analysis Approaches
A good overview of static analysis approaches applied
to security problems is provided in [8]. Simple lexical
approaches employed by scanning tools such as ITS4 and
RATS use a set of predeﬁned patterns to identify poten-
tially dangerous areas of a program [56]. While a signif-
icant improvement on Unix grep, these tools, however,
have no knowledge of how data propagates throughout
the program and cannot be used to automatically and
fully solve taint-style problems.
A few projects use path-sensitive analysis to ﬁnd er-
rors in C and C++ programs [6, 20, 33]. While capa-
ble of addressing taint-style problems, these tools rely on
an unsound approach to pointers and may therefore miss
some errors. The WebSSARI project uses combined un-
sound static and dynamic analysis in the context of ana-
lyzing PHP programs [23]. WebSSARI has successfully
been applied to ﬁnd many SQL injection and cross-site
scripting vulnerabilities in PHP code.
An analysis approach that uses type qualiﬁers has
been proven successful in ﬁnding security errors in C
for the problems of detecting format string violations
and user/kernel bugs [26, 45]. Context sensitivity sig-
niﬁcantly reduces the rate of false positives encountered
with this technique; however, it is unclear how scalable
the context-sensitive approach is.
Much of the work in information-ﬂow analysis uses a
type-checking approach, as exempliﬁed by JFlow [38].
The compiler reads a program containing labeled types
and, in checking the types, ensures that the program
cannot contain improper information ﬂow at runtime.
The security type system in such a language enforces
information-ﬂow policies. The annotation effort, how-
ever, may be prohibitively expensive in practice.
In
addition to explicit information ﬂows our approach ad-
dresses, JFlow also deals with implicit information ﬂows.
Static analysis has been applied to analyzing SQL
statements constructed in Java programs that may lead
to SQL injection vulnerabilities [17, 53]. That work an-
alyzes strings that represent SQL statements to check for
potential type violations and tautologies. This approach
assumes that a ﬂow graph representing how string values
can propagate through the program has been constructed
a priori from points-to analysis results. However, since
accurate pointer information is necessary to construct an
accurate ﬂow graph, it is unclear whether this technique
can achieve the scalability and precision needed to detect
errors in large systems.
8 Conclusions
In this paper we showed how a general class of se-
curity errors in Java applications can be formulated as
instances of the general tainted object propagation prob-
lem, which involves ﬁnding all sink objects derivable
from source objects via a set of given derivation rules.
We developed a precise and scalable analysis for this
problem based on a precise context-sensitive pointer
alias analysis and introduced extensions to the handling
of strings and containers to further improve the preci-
sion. Our approach ﬁnds all vulnerabilities matching the
speciﬁcation within the statically analyzed code. Note,
however, that errors may be missed if the user-provided
speciﬁcation is incomplete.
We formulated a variety of widespread vulnerabili-
ties including SQL injections, cross-site scripting, HTTP
splitting attacks, and other types of vulnerabilities as
tainted object propagation problems. Our experimental
results showed that our analysis is an effective practical
tool for ﬁnding security vulnerabilities. We were able to
ﬁnd a total of 29 security errors, and all but one of our
nine large real-life benchmark applications were vulner-
able. Two vulnerabilities were located in commonly used
libraries, thus subjecting applications using the libraries
to potential vulnerabilities. Most of the security errors
we reported were conﬁrmed as exploitable vulnerabili-
ties by their maintainers, resulting in more than a dozen
code ﬁxes. The analysis reported false positives for only
one application. We determined that the false warnings
reported can be eliminated with improved object naming.
9 Acknowledgements
We are grateful to Michael Martin for his help with
PQL and dynamic validation of some of the vulnerabili-
ties we found and to John Whaley for his support with
the bddbddb tool and the joeq framework. We thank
USENIX Association
14th USENIX Security Symposium
285
our paper shepherd R. Sekar, whose insightful comments
helped improve this paper considerably. We thank the
benchmark application maintainers for responding to our
bug reports. We thank Amit Klein for providing detailed
clariﬁcations about Web application vulnerabilities and
Ramesh Chandra, Chris Unkel, and Ted Kremenek and
the anonymous paper reviewers for providing additional
helpful comments. Finally, this material is based upon
work supported by the National Science Foundation un-
der Grant No. 0326227.
References
[1] C. Anley.
Advanced SQL injection in SQL Server applica-
tions. http://www.nextgenss.com/papers/advanced sql
injection.pdf, 2002.
[2] C. Anley. (more) advanced SQL injection. http://www.nextgenss.
com/papers/more advanced sql injection.pdf, 2002.
[3] B. Arkin, S. Stender, and G. McGraw. Software penetration testing. IEEE
Security and Privacy, 3(1):84–87, 2005.
[4] K. Beaver. Achieving Sarbanes-Oxley compliance for Web applica-
tions through security testing. http://www.spidynamics.com/
support/whitepapers/WI SOXwhitepaper.pdf, 2003.
[5] B. Buege, R. Layman, and A. Taylor. Hacking Exposed: J2EE and
Java: Developing Secure Applications with Java Technology. McGraw-
Hill/Osborne, 2002.
[6] W. R. Bush, J. D. Pincus, and D. J. Sielaff. A static analyzer for ﬁnding
dynamic programming errors. Software - Practice and Experience (SPE),
30:775–802, 2000.
[7] CGI Security.
The cross-site scripting FAQ.
cgisecurity.net/articles/xss-faq.shtml.
http://www.
[8] B. Chess and G. McGraw. Static analysis for security. IEEE Security and
Privacy, 2(6):76–79, 2004.
[9] Chinotec Technologies. Paros—a tool for Web application security assess-
ment. http://www.parosproxy.org, 2004.
[10] Computer Security Institute.
Computer crime and security sur-
http://www.gocsi.com/press/20020407.jhtml?
vey.
requestid=195148, 2002.
[11] S. Cook. A Web developers guide to cross-site scripting. http://www.
giac.org/practical/GSEC/Steve Cook GSEC.pdf, 2003.
[12] C. Cowan, C. Pu, D. Maier, J. Walpole, P. Bakke, S. Beattie, A. Grier,
P. Wagle, Q. Zhang, and H. Hinton. StackGuard: Automatic adaptive de-
tection and prevention of buffer-overﬂow attacks. In Proceedings of the 7th
USENIX Security Conference, pages 63–78, January 1998.
[13] J. D’Anjou, S. Fairbrother, D. Kehn, J. Kellerman, and P. McCarthy. Java
Developer’s Guide to Eclipse. Addison-Wesley Professional, 2004.
[14] S. Friedl. SQL injection attacks by example. http://www.unixwiz.
net/techtips/sql-injection.html, 2004.
[15] D. Geer and J. Harthorne. Penetration testing: A duet. http://www.
acsac.org/2002/papers/geer.pdf, 2002.
[16] Gentoo Linux Security Advisory.
SnipSnap: HTTP response split-
http://www.gentoo.org/security/en/glsa/
ting.
glsa-200409-23.xml, 2004.
[17] C. Gould, Z. Su, and P. Devanbu. Static checking of dynamically generated
queries in database applications. In Proceedings of the 26th International
Conference on Software Engineering, pages 645–654, 2004.
[18] J. Grossman. Cross-site tracing (XST): The new techniques and emerg-
ing threats to bypass current Web security measures using TRACE
and XSS. http://www.cgisecurity.com/whitehat-mirror/
WhitePaper screen.pdf, 2003.
[19] J. Grossman. WASC activities and U.S. Web application secu-
rity trends. http://www.whitehatsec.com/presentations/
WASC WASF 1.02.pdf, 2004.
[20] S. Hallem, B. Chelf, Y. Xie, and D. Engler. A system and language for
building system-speciﬁc, static analyses. In Proceedings of the ACM SIG-
PLAN 2002 Conference on Programming language Design and Implemen-
tation, pages 69–82, 2002.
[21] M. Howard and D. LeBlanc. Writing Secure Code. Microsoft Press, 2001.
[22] D. Hu. Preventing cross-site scripting vulnerability. http://www.
giac.org/practical/GSEC/Deyu Hu GSEC.pdf, 2004.
[23] Y.-W. Huang, F. Yu, C. Hang, C.-H. Tsai, D.-T. Lee, and S.-Y. Kuo. Se-
curing Web application code by static analysis and runtime protection. In
Proceedings of the 13th conference on World Wide Web, pages 40–52, 2004.
[24] G. Hulme. New software may improve application security. http:
//www.informationweek.com/story/IWK20010209S0003,
2001.
Imperva, Inc. SuperVeda penetration test. http://www.imperva.
com/download.asp?id=3.
[25]
[26] R. Johnson and D. Wagner. Finding user/kernel pointer bugs with type
inference. In Proceedings of the 2004 Usenix Security Conference, pages
119–134, 2004.
[27] A. Klein. Hacking Web applications using cookie poisoning. http://
www.cgisecurity.com/lib/CookiePoisoningByline.pdf,
2002.
[28] A. Klein.
Divide
and conquer:
HTTP response
poisoning attacks,
Web cache
//www.packetstormsecurity.org/papers/general/
whitepaper httpresponse.pdf, 2004.
and related topics.
splitting,
http:
[29] S. Kost.
An introduction to SQL injection attacks for Oracle
http://www.net-security.org/dl/articles/
developers.
IntegrigyIntrotoSQLInjectionAttacks.pdf, 2004.
[30] M. Krax. Mozilla foundation security advisory 2005-38. http://www.
mozilla.org/security/announce/mfsa2005-38.html,
2005.
[31] D. Litchﬁeld.
Oracle multiple PL/SQL injection vulnerabilities.
http://www.securityfocus.com/archive/1/385333/
2004-12-20/2004-12-26/0, 2003.
[32] D. Litchﬁeld. SQL Server Security. McGraw-Hill Osborne Media, 2003.
[33] V. B. Livshits and M. S. Lam. Tracking pointers with path and context
sensitivity for bug detection in C programs.
In Proceedings of the ACM
SIGSOFT Symposium on the Foundations of Software Engineering, pages
317–326, Sept. 2003.
[34] V. B. Livshits and M. S. Lam. Detecting security vulnerabilities in
Java applications with static analysis. Technical report. Stanford Univer-
sity. http://suif.stanford.edu/∼livshits/papers/tr/
webappsec tr.pdf, 2005.
[35] M. Martin, V. B. Livshits, and M. S. Lam. Finding application errors using
PQL: a program query language (to be published). In Proceedings of the
ACM Conference on Object-Oriented Programming, Systems, Languages,
and Applications (OOPSLA), Oct. 2005.
[36] J. Melbourne and D. Jorm. Penetration testing for Web applications.
http://www.securityfocus.com/infocus/1704, 2003.
[37] J. S. Miller, S. Ragsdale, and J. Miller. The Common Language Infrastruc-
ture Annotated Standard. Addison-Wesley Professional, 2003.
[38] A. C. Myers. JFlow: practical mostly-static information ﬂow control. In
Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles
of Programming Languages, pages 228–241, Jan. 1999.
[39] NetContinuum, Inc. The 21 primary classes of Web application threats.
https://www.netcontinuum.com/securityCentral/
TopThreatTypes/index.cfm, 2004.
[40] Open Web Application Security Project.
A guide to building se-
cure Web applications. http://voxel.dl.sourceforge.net/
sourceforge/owasp/OWASPGuideV1.1.pdf, 2004.
[41] Open Web Application Security Project. The ten most critical Web applica-
tion security vulnerabilities. http://umn.dl.sourceforge.net/
sourceforge/owasp/OWASPTopTen2004.pdf, 2004.
[42] Open Web Application Security Project. WebScarab. http://www.
owasp.org/software/webscarab.html, 2004.
[43] S. Sagiv, T. Reps, and R. Wilhelm. Parametric shape analysis via 3-valued
logic. In Proceedings of the 26th ACM Symposium on Principles of Pro-
gramming Languages, pages 105–118, Jan. 1999.
[44] J. Scambray and M. Shema. Web Applications (Hacking Exposed).
Addison-Wesley Professional, 2002.
[45] U. Shankar, K. Talwar, J. S. Foster, and D. Wagner. Detecting format string
vulnerabilities with type qualiﬁers. In Proceedings of the 2001 Usenix Se-
curity Conference, pages 201–220, Aug. 2001.
[46] K. Spett. Cross-site scripting: are your Web applications vulnerable.
http://www.spidynamics.com/support/whitepapers/
SPIcross-sitescripting.pdf, 2002.
[47] K. Spett.
SQL injection: Are your Web applications vulnera-
http://downloads.securityfocus.com/library/
ble?
SQLInjectionWhitePaper.pdf, 2002.
[48] B. Steensgaard. Points-to analysis in almost linear time. In Proceedings of
the 23th ACM Symposium on Principles of Programming Languages, pages
32–41, Jan. 1996.
[49] M. Surf and A. Shulman. How safe is it out there?
imperva.com/download.asp?id=23, 2004.
http://www.
[50] J. D. Ullman. Principles of Database and Knowledge-Base Systems. Com-
puter Science Press, Rockville, Md., volume II edition, 1989.
[51] D. Wagner, J. Foster, E. Brewer, and A. Aiken. A ﬁrst step towards auto-
mated detection of buffer overrun vulnerabilities. In Proceedings of Net-
work and Distributed Systems Security Symposium, pages 3–17, Feb. 2000.
[52] L. Wall, T. Christiansen, and R. Schwartz. Programming Perl. O’Reilly
and Associates, Sebastopol, CA, 1996.
[53] G. Wassermann and Z. Su. An analysis framework for security in Web
In Proceedings of the Speciﬁcation and Veriﬁcation of
applications.
Component-Based Systems Workshop, Oct. 2004.
[54] WebCohort, Inc. Only 10% of Web applications are secured against com-
mon hacking techniques. http://www.imperva.com/company/
news/2004-feb-02.html, 2004.
[55] J. Whaley and M. S. Lam. Cloning-based context-sensitive pointer alias
analysis using binary decision diagrams. In Proceedings of the ACM SIG-
PLAN 2004 conference on Programming Language Design and Implemen-
tation, pages 131–144, June 2004.
[56] J. Wilander and M. Kamkar. A comparison of publicly available tools for
static intrusion prevention. In Proceedings of 7th Nordic Workshop on Se-
cure IT Systems, Nov. 2002.
286
14th USENIX Security Symposium
USENIX Association