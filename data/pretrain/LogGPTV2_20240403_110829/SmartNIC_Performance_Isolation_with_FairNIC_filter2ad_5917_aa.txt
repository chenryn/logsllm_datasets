title:SmartNIC Performance Isolation with FairNIC
author:Stewart Grant and
Anil Yelam and
Maxwell Bland and
Alex C. Snoeren
SmartNIC Performance Isolation with FairNIC:
Programmable Networking for the Cloud
Stewart Grant∗, Anil Yelam∗, Maxwell Bland† and Alex C. Snoeren
UC San Diego
†University of Illinois-Urbana Champaign
ABSTRACT
Multiple vendors have recently released SmartNICs that provide
both special-purpose accelerators and programmable processing
cores that allow increasingly sophisticated packet processing tasks
to be offloaded from general-purpose CPUs. Indeed, leading data-
center operators have designed and deployed SmartNICs at scale
to support both network virtualization and application-specific
tasks. Unfortunately, cloud providers have not yet opened up the
full power of these devices to tenants, as current runtimes do not
provide adequate isolation between individual applications running
on the SmartNICs themselves.
We introduce FairNIC, a system to provide performance isolation
between tenants utilizing the full capabilities of a commodity SoC
SmartNIC. We implement FairNIC on Cavium LiquidIO 2360s and
show that we are able to isolate not only typical packet processing,
but also prevent MIPS-core cache pollution and fairly share access to
fixed-function hardware accelerators. We use FairNIC to implement
NIC-accelerated OVS and key/value store applications and show
that they both can cohabitate on a single NIC using the same port,
where the performance of each is unimpacted by other tenants.
We argue that our results demonstrate the feasibility of sharing
SmartNICs among virtual tenants, and motivate the development
of appropriate security isolation mechanisms.
CCS CONCEPTS
• Networks → Network adapters;
KEYWORDS
Network adapters, cloud hosting, performance isolation
ACM Reference Format:
Stewart Grant, Anil Yelam, Maxwell Bland, and Alex C. Snoeren. 2020.
SmartNIC Performance Isolation with FairNIC: Programmable Networking
for the Cloud. In Annual conference of the ACM Special Interest Group on
Data Communication on the applications, technologies, architectures, and
protocols for computer communication (SIGCOMM ’20), August 10–14, 2020,
Virtual Event, USA. ACM, New York, NY, USA, 13 pages. https://doi.org/10.
1145/3387514.3405895
∗These authors contributed equally.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7955-7/20/08.
https://doi.org/10.1145/3387514.3405895
681
1 INTRODUCTION
Cloud providers have determined that it is inefficient to implement
network processing tasks on host cores, and are deploying custom-
designed SmartNICs at scale to support traffic scheduling, security,
and network virtualization, among others [2, 16]. Enterprises and
service providers have reached similar conclusions, and a host of
manufacturers have introduced commodity, programmable Smart-
NICs to accelerate a large variety of tasks [4, 6, 23]. Unfortunately,
while the efficiency benefits of SmartNICs are contributing to cloud
providers’ bottom lines, tenants are barred from sharing in these
gains because providers do not allow them to download their own
applications onto NIC hardware in virtualized environments.
We explore the potential of opening up the network acceleration
benefits of commodity SmartNICs to cohabitating tenants in cloud
environments. In particular, we seek to enable individual tenants to
run their own, custom on-NIC programs that make use of shared
hardware resources to improve performance [33, 42], decrease host
CPU utilization [16, 36], or both [11, 35]. The key challenge to
running applications from different tenants on shared SmartNIC
hardware is ensuring isolation. In particular, we design isolation
techniques that work within the confines of an existing manufac-
turer’s SDK and do not require SmartNIC programmers to learn a
new language or application framework.
We recognize that production-grade isolation is a very high bar.
In particular, cloud platforms have been shown to exhibit numer-
ous side-channel security vulnerabilities [44]. Given the complexity
and performance overheads inherent in enforcing truly secure iso-
lation [39], we start by considering whether those costs are even
potentially worth incurring. As the first effort to share SmartNICs
between tenants, we defer consideration of deliberate malfeasance,
potential side-channel or other privacy attacks to future work, and
focus exclusively on achieving performance isolation.
In this work, we consider system-on-a-chip (SoC) SmartNICs due
to their relative ease of programmability. While several previous
studies [16, 33] consider FPGA-based SmartNICs, various aspects of
the FPGA design ecosystem (such as the need to globally synthesize,
place and route functionality) complicate use in a multi-tenant
environment [28]. Yet the design of today’s’ SoC SmartNICs also
frustrate our task, as they lack much of the hardware support found
in modern host processors for virtualization and multi-tenancy.
We illustrate the challenge of cross-tenant performance isolation
by studying the behavior of a Cavium LiquidIO 2360 SmartNIC
when running multiple applications concurrently. We demonstrate
that the NIC processing cores, shared caches, packet processing
units, and special-purpose coprocessors all serve as potential points
of contention and performance crosstalk between tenants. Our
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Grant, Yelam, Bland, and Snoeren
experiments show that in certain instances, co-location can de-
crease tenant performance by one-to-two orders of magnitude. In
response, we develop isolation mechanisms that enable fair sharing
of each of the contended resources. Our solutions balance the need
to maintain 25-Gbps line-rate processing while leaving as many
hardware resources as possible for tenant use.
We prototype our isolation mechanisms in FairNIC, an extension
of the Cavium Simple Executive for SmartNIC applications. Fair-
NIC provides strict core partitioning, cache and memory striping,
DWRR packet scheduling, and distributed token-based rate limiting
of access to the fixed-function hardware accelerator units. We eval-
uate FairNIC in both micro-benchmarks and realistic multi-tenant
environments. We demonstrate that each of our isolation mecha-
nisms can not only enforce fairness, but even defend against tenants
that would otherwise exhaust shared NIC resources. We implement
two popular SmartNIC-accelerated applications—Open vSwitch
(OVS) and a key/value store—and show that both applications can
coexist on the same SmartNIC while preserving performance iso-
lation. Hence, we conclude that it is indeed worthwhile from a
performance point of view to support SmartNICs in a multi-tenant
environment, and discuss potential next steps toward our vision of
virtualizing commodity SmartNICs in commercial clouds.
2 BACKGROUND
SmartNICs are network interface cards that allow applications to
run offload functionality directly in the data path. Our goal is to
enable safe SmartNIC access for multiple tenant applications, which
necessitates the development of isolation enforcement mechanisms.
The requirements for these isolation mechanisms are influenced by
both the service and deployment models chosen by the datacenter
operator and the capabilities of the SmartNIC hardware itself.
2.1 Service models
We consider the requirements of three common cloud service mod-
els, each in the context of a public cloud (i.e., we assume strict
isolation requirements between tenants), and adapt these models
to the context of SmartNIC multiplexing. This paper addresses per-
formance isolation issues common to all three service models, but
does not fully address the security isolation requirements of any;
we discuss these shortcomings further in Section 7.2.
SaaS: In a software-as-a-service model, we envision SmartNIC
applications are written, compiled and deployed by datacenter op-
erators. Tenants pay for a selection of these applications to be
offloaded onto SmartNICs. Security isolation mechanisms are re-
quired mainly to address potential provider errors, but performance
isolation is necessary for quality-of-service guarantees in multi-
application deployments. These assumptions are similar to those
made by the authors of NIC-A which provides tenant isolation on
FPGA-based SmartNICs [14].
PaaS: In a platform-as-a-service model developers could write
custom SmartNIC applications and submit them to the datacenter
operator for approval and deployment onto SmartNICs. This model
might restrict tenants’ code to allow for easier static checking or
software-based access restrictions to hardware such as coprocessors.
Runtime isolation mechanisms are necessary to the extent they are
not enforced by the platform API and static checking.
Speed
Programmability
ASIC
Fastest
Limited Difficult
FPGA
Fast
SoC
Moderate
Straightforward
Table 1: SmartNIC technology trade-offs
IaaS: As with virtual machines, a SmartNIC infrastructure-as-a-
service would provide “bare-metal” SmartNIC ABIs against which
tenants could run SmartNIC programs unmodified. In this deploy-
ment model, performance isolation requires either full hardware
virtualization in software or proper hardware support for isolation
like Intel VT-x. Security isolation is necessary if the tenants are
distrusting, or vulnerable to a malicious third party.
2.2 Types of SoC SmartNICs
SmartNICs are built out of a variety of different technologies in-
cluding ASIC, FPGA, and SoC. Traditional NICs are ASIC-based,
with predefined network semantics baked into hardware. While
these offer the best price/performance, they are generally not
programmable. Some vendors have shipped high-core-count, pro-
grammable ASIC-based SmartNICs [23], but they are famously
challenging to program [8] and have seen limited deployment.
Table 1 overviews the trade-offs between different SmartNIC
technologies. FPGAs provide a flexible alternative with near-ASIC
performance and some hyperscalers already utilize FPGAs in their
datacenters [16]. While FPGAs have the advantage of hardware-like
performance, they are expensive and power-hungry, and program-
ming them requires expert knowledge of the hardware and appli-
cation timing requirements. System-on-a-chip (SoC) SmartNICs
represent a middle ground by combining traditional ASICs with a
modest number of cache-coherent general-purpose cores for much
easier programming and fixed-function coprocessors for custom
workload acceleration. As a result, SoC SmartNICs seem the most
appropriate for tenant-authored applications.
SoC SmartNICs are not homogeneous in design. A key distinction
revolves around how the NIC moves packets between the network
ports and host memory [35]. On one hand, the “on-path” approach
passes all packets through (a subset of) cores on the NIC on the way
to or from the network [6]. In contrast, the “off-path” design pattern
uses an on-NIC switch to route traffic between the network and
NIC and host cores [4]. The variation in designs has trade-offs for
packet throughput, with the former requiring more cores to scale to
higher line rates, and the latter incurring additional latency before
reaching a computing resource. Recently, researchers proposed
switching as a general mechanism for routing between SmartNIC
resources in a hybrid of both architectures [49].
2.3 Cavium architecture
In this paper, we work with “on-path” LiquidIO SoC SmartNICs
from Cavium (now owned by Marvell) [6]. In addition to traditional
packet-processing engines for ingress and egress, the OCTEON
processor employed by the SmartNIC provides a set of embedded
cores with cache and memory subsystems for general-purpose
programmability and a number of special-purpose coprocessors for
accelerating certain popular networking tasks.
Cavium CN2360s have 16 1.5-GHz MIPS64 cores connected to a
shared 4-MB L2 cache and 16 GB of main memory connected via a
682
SmartNIC Performance Isolation with FairNIC
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 1: The leftmost plot shows unfair bandwidth allocation between applications due to varying packet sizes. Deficit Round
Robin scheduling addresses the issue in the second plot. The third plot shows a case of head-of-line blocking where two
applications get roughly the same throughput despite disparate core allocations. The rightmost plot shows that by decoupling
ingress queues and buffer pools, application performance is decoupled.
fast consistent memory bus. In its most straightforward use case,
each core runs firmware—known as the Cavium Simple Executive—
written in C that is executed when packets are delivered to it.
While the cores themselves are relatively under-powered, the cards
are equipped with a multitude of coprocessors to assist in packet
processing. These coprocessors range from accelerating common
functions like synchronization primitives and buffer allocation to
application-specific functions such as random-number generation,
compression, encryption, and regular expression matching.
Packet ingress and egress are handled by dedicated processing
units that provide software-configurable QoS options for flow clas-
sification, packet scheduling and shaping. To avoid unnecessary
processing overheads, there is no traditional kernel and the cores
run in a simple execution environment with each non-preemptable
core running a binary to launch its own process, and there is no con-
text switching. In a model familiar to DPDK programmers, the cores
continually poll for packets to avoid the overhead of interrupts.
End-to-end packet processing involves a chain of hardware com-
ponents. A typical packet coming in from the host or network goes
through the packet ingress engine that tags the packet based upon
flow attributes and puts it into pre-configured packet pools in mem-
ory. The packet is then pulled off the queue by a core associated
with that particular pool, which executes user-provided C code.
The cores may call other coprocessors such as the compression unit
to accelerate common packet-processing routines. After finishing
processing, the packet is dispatched to the egress engine where it
may undergo traffic scheduling before it is sent out on the wire or
the PCIe bus to be delivered to the host.
3 MOTIVATION & CHALLENGES
The key challenge to enabling tenant access to the programmable
features of SmartNICs is the fact that these resources lie outside
the traditional boundaries of cloud isolation. Almost all of the vir-
tualization mechanisms deployed by today’s cloud providers focus
on applications that run on host processors.1 Indeed, network vir-
tualization is a key focus of many providers, but existing solutions
arbitrate access on a packet-by-packet basis. When employing pro-
grammable SmartNICs, even “fair” access to the NIC may result
in disproportionate network utilization due to the differing ways
in which tenants may program the SmartNIC. In this section, we
demonstrate the myriad ways in which allowing tenants to deploy
applications on a SmartNIC can lead to performance crosstalk.
1Some providers do provide access to GPU and TPU accelerators, but that is orthogonal
to a tenant’s network usage.
683
3.1 Traffic scheduling
Link bandwidth is the main resource that is typically taken into
consideration for network isolation when working with traditional