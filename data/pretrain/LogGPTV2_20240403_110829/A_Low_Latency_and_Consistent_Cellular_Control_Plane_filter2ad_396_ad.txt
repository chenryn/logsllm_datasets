(4) Impact on data applications: Our evaluation shows the im-
pact of frequent mobility on a self-driving car and AR appli-
cation. We also evaluate impact on web browsing and video
streaming applications.
(5) Factor Analysis: To better understand Neutrino’s design choices
and separate the benefits of Neutrino’s different design ideas,
we perform a series of micro-benchmarks. These include (i)
comparison with different state synchronization schemes, (ii)
overhead of message logging and (iii) comparison of different
serialization techniques, to motivate the choice of FlatBuffers.
6.1 Setup and Methodology
Our test setup consists of two servers running Ubuntu 18.04.3
with kernel 4.15.0-74-generic. Each server is a dual-socket with
18 cores per socket, IntelXeon(R) Gold 5220 CPU @ 2.20GHz, and
dual NUMA nodes having total memory 128GB. Both servers are
also equipped with Intel X710 40 Gb (4 x 10) NIC. For testing control
traffic, we use an implementation of the S1AP and NAS protocol [50]
and implement the handling of request and response messages be-
tween the UE/BS and CPF for different control procedures. These
experiments are run with real signaling traces from a commercial
traffic generator and RAN emulator from ng4T [45]. We generate
two different types of traffic patterns: (i) 10 Gbps bursty traffic to
emulate a large number of IoT devices sending requests in a syn-
chronized pattern, and (ii) uniform traffic to emulate a pre-specified
number of control procedure requests per second. We run all ex-
periments for 60 s.
6.2 Baselines
We compare the performance of Neutrino against the following
designs:
Existing EPC: It is a modified version of the OpenAirInterface
[49] codebase, uses ASN.1 based serialization, and requires UEs to
Re-Attach on a CPF failure. Instead of kernel sockets, existing EPC
uses DPDK [2] for fast I/O operations.
Neutrino: It is a modified version of the existing EPC which (i)
uses optimized FlatBuffers-based serialization instead of ASN.1, (ii)
uses fast failure recovery as in §4.2 and (iii) performs structured
state replication.
DPCM: It is the same as existing EPC except control procedures
are modified (BS receives state from the UE) as described in [61].
SkyCore: It is also a modified version of the existing EPC which
synchronizes user state on each control message [40].
Next, we discuss the key evaluation results.
6.3 Latency Improvements in Procedure
Completion Time (PCT) with Neutrino
This section presents PCT for attach, handover, and service
request procedures in the non-failure scenario.
PCT - uniform traffic: Figure 7 shows service request PCT com-
parison of the existing EPC, DPCM, and SkyCore with Neutrino.
The figure shows that for uniform traffic rate of up to 120K Pro-
cedures Per Second (PPS), Neutrino performs 2.3×, 1.3×, and 3.4×
better than the existing EPC, DPCM, and SkyCore, respectively.
Onward 140 KPPS, existing EPC, and SkyCore are unable to handle
the arrival rate, resulting in a drastic increase in PCT. At 200 KPPS
A Low Latency and Consistent Cellular Control Plane
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 7: Impact in control procedure completion times.
Figure 9: attach PCT with a varying number of active users
with bursty control traffic.
Figure 8: attach PCT by a varying number of procedures per
second with uniform traffic.
and higher rates, PCT drastically increases for all the schemes but
Neutrino still performs better as compared to the other schemes.
We perform initial attach and handover PCT experiments
with uniform control traffic. Figure 8 shows PCT distribution for
the initial attach procedure. The figure shows that till 60 KPPS,
Neutrino performs up to 2.3× better than the existing EPC in me-
dian PCT. Onward 60 KPPS, existing EPC fails to meet the arrival
rate of the requests and queue starts building up. We called this re-
gion, existing EPC’s saturation region. In existing EPC’s saturation
region, the median PCT for existing EPC drastically increases while
it remains low for Neutrino. Onward 120 KPPS, Neutrino is unable
to meet the message arrival rate and the queue builds up. We call
this Neutrino’s saturation region. In this region, Neutrino performs
up to 3.4× better than the existing EPC in terms of median PCT.
Neutrino can better meet high arrival rates as compared to all other
schemes. In all these cases, the primary source of improvement is
Neutrino’s fast message serialization.
PCT - Bursty Traffic: Figure 9 shows PCT distribution for an
initial attach procedure with varying number of active UEs,
when using Neutrino and existing EPC. Due to the high arrival rate
for bursty traffic model, queues immediately build-up for both Neu-
trino and existing EPC. The figure shows that Neutrino performs
up to 2× better than the existing EPC for a bursty traffic model.
Figure 10: handover PCT under failure with varying number
of active users with uniform traffic.
6.4 PCT under failure with Neutrino
We conduct experiments with CPF failures for both Neutrino and
existing EPC. PCT under failure for existing EPC includes the time
taken by the UE in executing the procedure before the failure, as
well as the time taken to re-attach to another CPF after the failure.
In the case of Neutrino, PCT under failure includes the time taken
by the UE to execute the procedure before the failure and the time
secondary CPF takes to replay the stored messages to recover the
lost user state. In both cases, PCT does not include failure detection
time.
Figure 10 shows PCT distribution under CPF failure for handover
procedure with uniform traffic. We observe an improvement of up
to 5.6× in median PCT when the procedure arrival rate is less than
60 KPPS. In addition, to faster serialization, this improvement is at-
tributed to faster state recovery in Neutrino. Instead of re-attaching
the UE on a CPF failure, CTA module sends logged messages to
the replica CPF, which then replays them to reconstruct the state
updates, saving multiple RTTs.
6.5 Fast Handover in Neutrino
Figure 11 shows the comparison of the PCT for handover in ex-
isting EPC, Neutrino - Default (in this case, user state migration
is required before handover completion) and Neutrino - Proactive
(user state is proactively replicated in the target region to imple-
ment Fast handover, as discussed in section 4.3). The Figure shows
656
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Mukhtiar et al.
Figure 13: Effect of mobility (LHS: single HO, RHS: multiple
HOs) on a self-driving car.
Figure 14: Effect of mobility (LHS: single HO, RHS: multiple
HOs) on VR application.
stability. We measure the number of application packets that miss
the VR deadline requirement in both single and multiple mobility
scenarios. Figure 14 shows that Neutrino performs up 2.5× better
than the existing EPC.
Impact on video startup latency and page load: The second
set of experiments are for a stationary UE, in idle state, starting a
web browsing or video streaming application. To get data access,
UE needs to execute service request procedure to set up a data
channel for the application. Application startup latency in this
scenario is a function of service request PCT. This experiment
measures the average (i) video startup delay and (ii) page load
time (PLT). To avoid network variation in the video startup delay,
Apache webserver replays locally stored videos. Video startup delay
is measured using DASH player. Figure 3 shows a video startup
delay comparison between the Neutrino and existing EPC while
CPF is handling a varying number of active users. The Figure shows
that Neutrino performs up to 37× better than existing EPC in terms
of median video startup delay. Page load time is equal to (i) service
request PCT plus (ii) average page load time of the top 10 Alexa
pages. To filter out network variations, MITM proxy [6] is used to
replay locally stored web pages. A Firefox web browser extension
Load Time is used to measure page load time. Figure 3 shows that
Neutrino performs up to 3.2× better than the existing EPC in terms
of median PLT.
6.7.1
6.7 Factor Analysis
Below, we discuss the results of our micro-benchmark experiments.
Impact of state synchronization on PCT. We compare the
overhead of different replica synchronization schemes on control
plane latency. Figure 15 shows the attach PCT distribution for
three different schemes; (i) No Rep: no message logging and state
replication, (ii) Per Msg Rep: with message logging and per-message
state replication and (iii) Per Proc Rep: with message logging and
per procedure state replication. Figure 15 shows that per-message
state replication has the highest median PCT, due to frequent state
locking for check-pointing. Per-procedure state replication has a
Figure 11: Fast handover procedure completion times with
uniform traffic.
Figure 12: An example scenario of frequent control han-
dovers with high mobility applications in an edge-based cel-
lular core.
that Neutrino - Proactive improves median PCT by up to 7× over
existing EPC, when the procedure arrival rate is less than 60 KPPS.
Above 60 KPPS, existing EPC is unable to meet the arrival rate, and
PCT increases drastically.
6.6 Impact on application performance
To measure the impact of Neutrino on application performance,
we interface Intel’s 5G UPF [29] with Neutrino. A UE connected to
Neutrino can create a new session, delete an existing session, and
modify existing bearer on the UPF through S11 interface [9].
To measure the impact of mobility on the application perfor-
mance, we set up the client application (on UE) with CARLA self-
driving car emulator [17, 18] and an edge application that processes
sensors’ data. Experiments are performed in two scenarios; (i) while
executing a single handover and (ii) executing multiple handovers
during a 5 minutes drive at 60 mph with the BS spacing similar to
Figure 12. In both scenarios, we set a deadline for the application
data. We generate sensor data at a frequency of 1KHz in the up-
link direction. At the edge application, we note the the number of
packets which missed their application-specific deadline.
Impact on autonomous vehicles and AR/VR: The time budget
for a self-driving car to make a decision based on sensors’ data
is in the order of 100 ms [55]. Figure 13 shows in both single and
multiple handover scenarios, Neutrino performs up to 2.8× better
than the existing EPC.
Virtual reality (VR) applications, that use head-tracked systems,
require a latency of less than 16 ms [53] to achieve perceptual
657
A Low Latency and Consistent Cellular Control Plane
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Figure 15: Effect of state synchronization on attach PCT.
Figure 18: Improvement in encoding + decoding times in
comparison to ASN.1.
Figure 16: Impact of message logging on attach PCT.
Figure 17: Log size variation with the number of active users.
slightly higher median PCT as compared to No Rep, but it provides
the best trade-off between consistency and overhead in PCT.
6.7.2
Impact of message logging on PCT. We performed attach
procedure with and without message logging enabled. Figure 16
shows PCT distribution for the attach procedure. The figure shows
message logging has a negligible impact on the PCT in Neutrino
and the reason is in-memory logging is fast.
6.7.3 Message log size at the CTA. Figure 17 shows the maxi-
mum log size at CTA with varying total number of active users
and the type of procedures being performed with per-procedure
synchronization. The figure shows log size grows by increasing the
number of active users, however even with 200K active users, it
remains less than 400 MBs.
658
Figure 19: Encoding + decoding with Optimized FlatBuffers.
6.7.4
Serialization benefits. We motivate the choice of Flat-
Buffers (FBs) for serializing cellular control messages over several
serialization schemes; FlexBuffer [26], Protocol Buffers [27], Fast-
CDR [3] and LCM [5] with ASN.1 [1]. We compare the time to
decode and encode the control messages. For these experiments,
we construct a custom message with varying number of data ele-
ments/fields.
Encoding + Decoding times: Figure 18 shows the speedup in
the total encoding plus decoding time as compared to the ASN.1
serialization scheme, for a custom control message with varying
number of data fields. For messages with data elements less than 7,
Fast-CDR and LCM perform better. When data elements increase
beyond 7, FBs is the clear winner. For 25 data elements, the total
speedup in encoding + decoding time for FBs is twice that of the next
best scheme. The speedup in comparison with ASN.1 is between
1.6× to 19.2×. We note here that all cellular control messages we
tested, contained a minimum of 8 data elements.
Tests with real control messages: We next compare Optimized
FBs with FBs and ASN.1 over a subset of real control messages. We
specifically quantify both the encoding + decoding times as well
as the increase in encoded message size with FBs. In Figure 19 we
observe a decrease of up to 5.9× in encoding + decoding times with
FlatBuffers over ASN.1. There is a further decrease with Optimized
FBs in some cases. However, this decrease does come at a cost;
the encoded message size in FBs can add up to 300 bytes of more
metadata than ASN.1 (Figure 20). With Optimized FBs, we can save
up to 32 bytes of data per message.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Mukhtiar et al.
state [32, 33]. However, they are targeted towards centralized cloud
deployments.
Reducing control traffic latency: Recent work DPCM [61] also
aims to reduce control plane latency. DPCM proposes a client-side
solution, which reduces control plane latency by initiating and
executing some control operations in parallel by using the device
side user state. With DPCM’s client-side modifications, Neutrino
can further speed-up the processing of control traffic.
Centralized cellular control plane architectures: There are
several proposals for architecting an SDN-based cellular core [30,
36, 42]. A common theme in all these works is to have a logically
centralized cellular control plane (including all the MME function-
ality) with a programmable data plane. However, unlike Neutrino,
these proposals do not aim to address control plane latency and cen-
tralized control plane architectures may not suitable for achieving
low latency control traffic in edge deployments.
Consolidated cellular core architectures: There are several pro-
posals for consolidating EPC designs [40, 41, 50]. PEPC [50] slices
EPC by the user, consolidating UE state, and refactoring EPC func-
tions. PEPC improves the overall EPC performance, however, it does
not consider control plane fault tolerance. Similarly, SoftBox[41]