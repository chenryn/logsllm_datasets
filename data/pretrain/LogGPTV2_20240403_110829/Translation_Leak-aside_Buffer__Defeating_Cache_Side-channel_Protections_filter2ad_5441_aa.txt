title:Translation Leak-aside Buffer: Defeating Cache Side-channel Protections
with TLB Attacks
author:Ben Gras and
Kaveh Razavi and
Herbert Bos and
Cristiano Giuffrida
Translation Leak-aside Buffer: Defeating Cache 
Side-channel Protections with TLB Attacks
Ben Gras, Kaveh Razavi, Herbert Bos, and Cristiano Giuffrida, Vrije Universiteit
https://www.usenix.org/conference/usenixsecurity18/presentation/gras
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
ISBN 978-1-939133-04-5
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.Translation Leak-aside Buffer: Defeating Cache Side-channel Protections
with TLB Attacks
Ben Gras
Vrije Universiteit
Amsterdam
Kaveh Razavi
Vrije Universiteit
Amsterdam
Herbert Bos
Vrije Universiteit
Amsterdam
Cristiano Giuffrida
Vrije Universiteit
Amsterdam
Abstract
To stop side channel attacks on CPU caches that have
allowed attackers to leak secret information and break
basic security mechanisms, the security community has
developed a variety of powerful defenses that effectively
isolate the security domains. Of course, other shared
hardware resources exist, but the assumption is that un-
like cache side channels, any channel offered by these
resources is insufﬁciently reliable and too coarse-grained
to leak general-purpose information.
This is no longer true. In this paper, we revisit this as-
sumption and show for the ﬁrst time that hardware trans-
lation lookaside buffers (TLBs) can be abused to leak
ﬁne-grained information about a victim’s activity even
when CPU cache activity is guarded by state-of-the-art
cache side-channel protections, such as CAT and TSX.
However, exploiting the TLB channel is challenging, due
to unknown addressing functions inside the TLB and the
attacker’s limited monitoring capabilities which, at best,
cover only the victim’s coarse-grained data accesses. To
address the former, we reverse engineer the previously
unknown addressing function in recent Intel processors.
To address the latter, we devise a machine learning strat-
egy that exploits high-resolution temporal features about
a victim’s memory activity. Our prototype implementa-
tion, TLBleed, can leak a 256-bit EdDSA secret key from
a single capture after 17 seconds of computation time
with a 98% success rate, even in presence of state-of-
the-art cache isolation. Similarly, using a single capture,
TLBleed reconstructs 92% of RSA keys from an imple-
mentation that is hardened against FLUSH+RELOAD at-
tacks.
1
Introduction
Recent advances in micro-architectural side-channel at-
tacks threaten the security of our general-purpose com-
puting infrastructures from clouds to personal comput-
ers and mobile phones. These attacks allow attackers
to leak secret information in a reliable and ﬁne-grained
way [13, 32, 36, 38, 59] as well as compromise funda-
mental security defenses such as ASLR [17, 20, 24, 28].
The most prominent class of side-channel attacks leak in-
formation via the shared CPU data or instruction caches.
Hence, the community has developed a variety of power-
ful new defenses to protect shared caches against these
attacks, either by partitioning them, carefully sharing
them between untrusted programs in the system, or san-
itizing the traces left in the cache during the execu-
tion [9, 21, 37, 52, 62].
In this paper, we argue that the problem goes much
deeper. As long as there are other shared hardware
resources, attackers can still reliably leak ﬁne-grained,
security-sensitive information from the system. In fact,
we show this is possible even with shared resources
that only provide a coarse-grained channel of informa-
tion (whose general applicability has been questioned by
prior work [46]), broadening the attack surface of prac-
tical side-channel attacks. To demonstrate this property,
we present a practical side-channel attack that leaks in-
formation from the shared Translation Lookaside Buffers
(TLBs) even in the presence of all the state-of-the-art
cache defenses. Exploiting this channel is particularly
challenging due its coarse (page-level) spatial granular-
ity. To address this challenge, we propose a new analysis
technique based on (supervised) machine learning. Our
analysis exploits high-resolution temporal features on the
victim’s memory activity to combat side-channel coars-
ening and leak information.
Existing defenses against cache side channels The
execution of a victim program changes the state of the
shared CPU caches. In a cache side-channel attack, an at-
tacker deduces sensitive information (e.g., cryptographic
keys) by observing this change in the state. It is possi-
ble to rewrite existing software not to leave an identiﬁ-
able trace in the cache, but manual approaches are error-
USENIX Association
27th USENIX Security Symposium    955
prone [16] while automated ones incur several-fold per-
formance overheads [48]. As an alternative, many pro-
posed defenses attempt to stop attackers from observ-
ing changes that an unmodiﬁed victim program makes
to the state of the CPU caches. This is done either by
stopping precise timers that attackers need to use to tell
the difference between cached or uncached memory ac-
cesses [10, 34, 40] or by partitioning shared CPU cache
between mutually distrusting programs [21, 37, 47, 52,
62]. Given that attackers can ﬁnd many new sources of
timing [17, 34, 49], CPU cache partitioning is currently
the only known generic mechanism that stops existing
attacks.
Unfortunately, as we will show, protecting only the
shared data and instruction caches is insufﬁcient. Hard-
ware threads (also known as hyperthreads) share other
hardware resources such as TLBs on top of the CPU
caches. The question we address in this paper is whether
they can abused by attackers to leak sensitive informa-
tion in a reliable and ﬁne-grained way even in presence
of state-of-the-art cache defenses and, if so, what the im-
plications are for future attacks and defenses.
TLBleed To answer these questions, we explore the
architecture of TLBs in modern Intel processors. As
very little information on TLBs has been made avail-
able, our analysis represents the ﬁrst known reverse en-
gineering effort of the TLB architecture. Similar to CPU
data and instruction caches, there are multiple levels of
TLBs. They are partitioned in sets and behave differently
based on whether they help in the translation of instruc-
tions or data. We further ﬁnd that the mapping of virtual
addresses to TLB sets is a complex function in recent
micro-architectures. We describe our efforts in reverse
engineering this function, useful when conducting TLB-
based attacks and beneﬁting existing work [54]. Armed
with this information, we build TLBleed, a side-channel
attack over shared TLBs that can extract secret informa-
tion from a victim program protected with existing cache
defenses [9, 21, 31, 37, 52, 62] Implementing TLBleed
is challenging: due to the nature of TLB operations, we
can only leak memory accesses in the coarse granular-
ity of a memory page (4 KB on x86 systems) and due
to the TLB architecture we cannot rely on the execution
of instructions (and controlled page faults) to leak secret
information similar to previous page-level side-channel
attacks [58]. To overcome these limitations, we describe
a new machine learning-based analysis technique that ex-
ploits temporal patterns of the victim’s memory accesses
to leak information.
Contributions
contributions:
In summary, we make the following
• The ﬁrst detailed analysis of the architecture of the
TLB in modern processors including the previously
unknown complex function that maps virtual ad-
dresses to TLB sets.
• The design and implementation of TLBleed, a new
class of side-channel attacks that rely on the TLB to
leak information. This is made possible by a new
machine learning-based analysis technique based
on temporal information about the victim’s mem-
ory accesses. We show TLBleed breaks a 256-bit
libgcrypt EdDSA key in presence of existing de-
fenses, and a 1024-bit RSA key in an implemen-
tation that is hardened against FLUSH+RELOAD at-
tacks.
• A study of the implications of TLBleed on existing
attacks and defenses including an analysis of miti-
gations against TLBleed.
2 Background
To avoid the latency of off-chip DRAM for every
memory access, modern CPUs employ a variety of
caches [23]. With caching, copies of previously fetched
items are kept close to the CPU in Static RAM (SRAM)
modules that are organized in a hierarchy. We will fo-
cus our attention on data caches ﬁrst and discuss TLBs
after. For both holds that low-latency caches are parti-
tioned into cache sets of n ways. This means is that in an
n way cache, each set contains n cachelines. Every ad-
dress in memory maps to exactly one cache set, but may
occupy any of the n cachelines in this set.
2.1 Cache side-channel attacks
As cache sets are shared by multiple processes, the activ-
ity in a cache set offers a side channel for ﬁne-grained,
security-sensitive cache attacks. For instance, if the ad-
versary ﬁrst occupies all the n ways in a cache set and
after some time observes that some of these cachelines
are no longer in the cache (since accessing the data now
takes much longer), it must mean that another program—
a victim process, VM, or the kernel—has accessed data
at addresses that also map to this cache set. Cache attacks
by now have a long history and many variants [5, 33, 38].
We now discuss the three most common ones.
In a PRIME+PROBE attack [42, 43, 45], the adversary
ﬁrst collects a set of cache lines that fully evict a single
cache set. By accessing these over and over, and measur-
ing the corresponding access latency, it is possible to de-
tect activity of another program in that particular cache
set. This can be done for many cache sets. Due to the
small size of a cache line, this allows for high spatial
956    27th USENIX Security Symposium
USENIX Association
resolution, visualized in a memorygram in [42]. Closely
related is FLUSH+RELOAD, which relies on the victim
and the attacker physically sharing memory pages, so
that the attacker can directly control the eviction (ﬂush-
ing) of a target memory page. Finally, an EVICT+TIME
attack [17, 43, 53] evicts a particular cache set, then in-
vokes the victim operation. The victim operation has a
slowdown that depends on the evicted cache set, which
leaks information on the activity of the victim.
2.2 Cache side-channel defenses
As a response to cache attacks, the research community
has proposed defenses that follow several different strate-
gies. We again discuss the most prominent ones here.
Isolation by partitioning sets Two processes that do
not share a cache cannot snoop on each others’ cache
activity. One approach is to assign to a sensitive opera-
tion its own cache set, and not to let any other programs
share that part. As the mapping from to a cache set in-
volves the physical memory address, this can be done
by the operating system by organizing physical mem-
ory into non-overlapping cache set groups, also called
colors, and enforcing an isolation policy. This approach
was ﬁrst developed for higher predictability in real-time
systems [7, 30] and more recently also for isolation for
security [9, 31, 50, 62].
Isolation by partitioning ways Similarly to partition-
ing the cache by sets, we can also partition it by ways.
In such a design, programs have full access to cache
sets, but each set has a smaller number of ways, non-
overlapping with other programs, if so desired. This ap-
proach requires hardware support such as Intel’s Cache
Allocation Technology (CAT) [37]. Since the number of
ways and hence security domains is strictly limited on
modern architectures, CATalyst’s design uses only two
domains and forbids accesses to the secure domain to
prevent eviction of secure memory pages [37].
Enforcing data cache quiescence Another strategy to
thwart cache attacks, while allowing sharing and hence
not incurring the performance degradation of cache par-
titioning, is to ensure the quiescence of the data cache
while a sensitive function is being executed. This pro-
tects against concurrent side channel attacks, including
PRIME+PROBE and FLUSH+RELOAD, because these rely
on evictions of the data cache in order to proﬁle cache
activity. This approach can be assisted by the Intel Trans-
actional Synchronization Extensions (TSX) facility, as
TSX transactions abort when concurrent data cache evic-
tions occur [21].
2.3 From CPU caches to TLBs
All the existing cache side-channel attacks and defenses
focus on exploitation and hardening of shared CPU
caches, but ignore caching mechanisms used by the
Memory Management Unit (MMU).
On modern virtual memory systems, such mechanisms
play a crucial role. CPU cores primarily issue instruc-
tions that access data using their virtual addresses (VAs).
The MMU translates these VAs to physical addresses
(PAs) using a per-process data structure called the page
table. For performance reasons, the result of these trans-
lations are aggressively cached in the Translation Looka-
side Buffer (TLB). TLBs on modern Intel architectures
have a two-level hierarchy. The ﬁrst level (i.e., L1), con-
sists of two parts, one that caches translations for code
pages, called L1 instruction TLB (L1 iTLB), and one that
caches translations for data pages, called L1 data TLB
(L1 dTLB). The second level TLB (L2 sTLB) is larger
and shared for translations of both code and data.
Again, the TLB at each level is typically partitioned
into sets and ways, conceptually identical to the data
cache architecture described earlier. As we will demon-
strate, whenever the TLB is shared between mutually
distrusting programs, this design provides attackers with
new avenues to mount side-channel attacks and leak in-
formation from a victim even in the presence of state-of-
the-art cache defenses.
3 Threat Model
We assume an attacker capable of executing unprivi-
leged code on the victim system. Our attack requires
monitoring the state of the TLB shared with the vic-
tim program. In native execution, this is simply possi-
ble by using CPU afﬁnity system calls to achieve core
co-residency with the victim process. In cloud environ-
ments, previous work shows it is possible to achieve res-
idency on the same machine with a victim virtual ma-
chine [55]. Cloud providers may turn hyperthreading on
for increased utilization (e.g., on EC2 [1]) making it pos-
sible to share cores across virtual machines. Once the
attacker achieves core co-residency with the victim, she
can mount a TLBleed attack using the shared TLB. This
applies to scenarios where a victim program processing
sensitive information, such as cryptographic keys.
4 Attack Overview
Figure 1 shows how an attacker can observe the TLB
activity of a victim process running on a sibling hyper-
thread with TLBleed. Even if state-of-the-art cache side-
channel defenses [21, 37, 47, 52, 62] are deployed and
the activity of the victim process is properly isolated
USENIX Association
27th USENIX Security Symposium    957
5 TLB Monitoring
To address our ﬁrst challenge, Q1, we need to understand
how virtual addresses (VAs) are mapped to different sets
in the TLB. On commodity platforms, the mapping of
VAs to TLB sets is microarchitecture-speciﬁc and cur-
rently unknown. As we shall see, we found that even
on a single processor, the mapping algorithms in the dif-
ferent TLBs vary from very simple linear translations to
complex functions that use a subset of the virtual address
bits XORed together to determine the target TLB set.
To understand the details of how the TLB operates, we
need a way to reverse engineer such mapping functions
on commodity platforms, recent Intel microarchitectures
in particular. For this purpose, we use Intel Performance
Counters (PMCs) to gather ﬁne-grained information on
TLB misses at each TLB level/type. More speciﬁcally,
we rely on the Linux perf event framework to moni-
tor certain performance events related to the operation of
the TLB, namely dtlb_load_misses.stlb_hit and
dtlb_load_misses.miss_causes_a_walk. We cre-
ate different access patterns depending on the architec-
tural property under study and use the performance coun-
ters to understand how such property is implemented on
a given microarchitecture. We now discuss our reverse
engineering efforts and the results.
Linearly-mapped TLB We refer to the function that
maps a virtual address to a TLB set as the hash function.
We ﬁrst attempt to ﬁnd parameters under the hypothe-
sis that the TLB is linearly-mapped, so that target set =
pageVA mod s (with s the number of sets). Only if this
strategy does not yield consistent results, we use the
more generalized approach described in the next section.
To reverse engineer the hash function and the size of
linearly-mapped TLBs, we ﬁrst map a large set of testing
pages into memory. Next, we perform test iterations to
explore all the sensible combinations of two parameters:
the number of sets s and the number of ways w. As we
wish to ﬁnd the smallest possible TLB eviction set, we
use w + 1 testing pages accessed at a stride of s pages.
The stride is simply s due to the linear mapping hypoth-
esis.
At each iteration, we access our testing pages in a loop
and count the number of evictions evidenced by PMC
counters. Observing that a minimum of w + 1 pages is
necessary to cause any evictions of previous pages, we
note that the smallest w that causes evictions across all
our iterations must be the right wayness w. Similarly,
the smallest possible corresponding s is the right num-
ber of sets. As an example on Intel Broadwell, Figure 2
shows a heatmap depicting the number of evictions for
each combination of stride s and number of pages w. The