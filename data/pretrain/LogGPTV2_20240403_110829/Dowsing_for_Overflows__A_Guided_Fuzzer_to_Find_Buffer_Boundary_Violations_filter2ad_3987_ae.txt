ence the results. The reason is that our test suite here
was simple, and the execution paths reached the vulner-
ability condition quickly. In contrast, more complex ap-
plications process the inputs intensively, moving sym-
bolic execution away from the code of interest.
In all
these cases, Dowser ﬁnds bugs signiﬁcantly faster. Even
if we take the 15 minute tests of higher-ranking analy-
sis groups into account, Dowser provides a considerable
improvement over existing systems.
7 Related work
Dowser is a ’guided’ fuzzer which draws on knowledge
from multiple domains. In this section, we place our sys-
tem in the context of existing approaches. We start with
the scoring function and selection of code fragments.
Next, we discuss traditional fuzzing. We then review
previous work on dynamic taint analysis in fuzzing, and
ﬁnally, discuss existing work on whitebox fuzzing and
symbolic execution.
Software complexity metrics Many studies have shown
that software complexity metrics are positively corre-
lated with defect density or security vulnerabilities [29,
35, 16, 44, 35, 32]. However, Nagappan et al. [29] ar-
gued that no single set of metrics ﬁts all projects, while
Zimmermann et al. [44] emphasize a need for metrics
that exploit the unique characteristics of vulnerabilities,
e.g., buffer overﬂows or integer overruns. All these ap-
proaches consider the broad class of post-release defects
or security vulnerabilities, and consider a very generic
set of measurements, e.g., the number of basic blocks in a
function’s control ﬂow graph, the number of global or lo-
cal variables read or written, the maximum nesting level
60  22nd USENIX Security Symposium 
USENIX Association
12
of if or while statements and so on. Dowser is very dif-
ferent in this respect, and to the best of our knowledge,
the ﬁrst of its kind. We focus on a narrow group of secu-
rity vulnerabilities, i.e., buffer overﬂows, so our scoring
function is tailored to reﬂect the complexity of pointer
manipulation instructions.
Traditional fuzzing Software fuzzing started in earnest
in the 90s when Miller et al. [25] described how they
fed random inputs to (UNIX) utilities, and managed
to crash 25-33% of the target programs. More ad-
vanced fuzzers along the same lines, like Spike [39],
and SNOOZE [5], deliberately generate malformed in-
puts, while later fuzzers that aim for deeper bugs are
often based on the input grammar (e.g., Kaksonen [20]
and [40]). DeMott [13] offers a survey of fuzz testing
tools. As observed by Godefroid et al. [18], traditional
fuzzers are useful, but typically ﬁnd only shallow bugs.
Application of DTA to fuzzing BuzzFuzz [15] uses
DTA to locate regions of seed input ﬁles that inﬂuence
values used at library calls. They speciﬁcally select li-
brary calls, as they are often developed by different peo-
ple than the author of the calling program and often lack
a perfect description of the API. Buzzfuzz does not use
symbolic execution at all, but uses DTA only to ensure
that they preserve the right input format. Unlike Dowser,
it ignores implicit ﬂows completely, so it could never ﬁnd
bugs such as the one in nginx (Figure 1). In addition,
Dowser is more selective in the application of DTA. It’s
difﬁcult to assess which library calls are important and
require a closer inspection, while Dowser explicitly se-
lects complex code fragments.
TaintScope [42] is similar in that it also uses DTA to
select ﬁelds of the input seed which inﬂuence security-
sensitive points (e.g., system/library calls).
In addi-
tion, TaintScope is capable of identifying and bypassing
checksum checks. Like Buzzfuzz, it differs from Dowser
in that it ignores implicit ﬂows and assumes only that li-
brary calls are the interesting points. Unlike BuzzFuzz,
TaintScope operates at the binary level, rather than the
source.
Symbolic-execution-based fuzzing Recently, there has
been much interest in whitebox fuzzing, symbolic ex-
ecution, concolic execution, and constraint solving.
Examples include EXE [8], KLEE [7], CUTE [33],
DART [17], SAGE [18], and the work by Moser et
al. [28]. Microsoft’s SAGE, for instance, starts with a
well-formed input and symbolically executes the pro-
gram under test in attempt to sweep through all feasi-
ble execution paths of the program. While doing so,
it checks security properties using AppVeriﬁer. All of
these systems substitute (some of the) program inputs
with symbolic values, gather input constraints on a pro-
gram trace, and generate new input that exercises differ-
ent paths in the program. They are very powerful, and
can analyze programs in detail, but it is difﬁcult to make
them scale (especially if you want to explore many loop-
based array accesses). The problem is that the number of
paths grows very quickly.
Intuitively,
Zesti [24] takes a different approach and executes
existing regression tests symbolically.
it
checks whether they can trigger a vulnerable condition
by slightly modifying the test input. This technique
scales better and is useful for ﬁnding bugs in paths in
the neighborhood of existing test suites. It is not suit-
able for bugs that are far from these paths. As an ex-
ample, a generic input which exercises the vulnerable
loop in Figure 1 has the uri of the form ”//{arbitrary
characters}”, and the shortest input triggering the bug is
”//../”. When fed with ”//abc”, [24] does not ﬁnd
the bug—because it was not designed for this scenario.
Instead, it requires an input which is much closer to the
vulnerability condition, e.g., ”//..{an arbitrary char-
acter}”. For Dowser, the generic input is sufﬁcient.
SmartFuzz [27] focuses on integer bugs.
It uses
symbolic execution to construct test cases that trigger
arithmetic overﬂows, non-value-preserving width con-
versions, or dangerous signed/unsigned conversions. In
contrast, Dowser targets the more common (and harder
to ﬁnd) case of buffer overﬂows. Finally, Babi´c et al. [4]
guide symbolic execution to potentially vulnerable pro-
gram points detected with static analysis. However, the
interprocedural context- and ﬂow-sensitive static analy-
sis proposed does not scale well to real world programs
and the experimental results contain only short traces.
8 Conclusion
Dowser is a guided fuzzer that combines static analysis,
dynamic taint analysis, and symbolic execution to ﬁnd
buffer overﬂow vulnerabilities deep in a program’s logic.
It starts by determining ‘interesting’ array accesses, i.e.,
accesses that are most likely to harbor buffer overﬂows.
It ranks these accesses in order of complexity—allowing
security experts to focus on complex bugs, if so de-
sired. Next, it uses taint analysis to determine which in-
puts inﬂuence these array accesses and fuzzes only these
bytes. Speciﬁcally, it makes (only) these bytes symbolic
in the subsequent symbolic execution. Where possible
Dowser’s symbolic execution engine selects paths that
are most likely to lead to overﬂows. Each three of the
steps contain novel contributions in and of themselves
(e.g., the ranking of array accesses, the implicit ﬂow
handling in taint analysis, and the symbolic execution
based on pointer value coverage), but the overall contri-
bution is a new, practical and complete fuzzing approach
that scales to real applications and complex bugs that
would be hard or impossible to ﬁnd with existing tech-
USENIX Association  
22nd USENIX Security Symposium  61
13
niques. Moreover, Dowser proposes a novel ‘spot-check’
approach to ﬁnding buffer overﬂows in real software.
[12] CWE/SANS. CWE/SANS TOP 25 Most Dangerous Soft-
ware Errors. www.sans.org/top25-software-errors,
2011.
[13] DEMOTT, J.
The evolving art of fuzzng. DEFCON 14,
Acknowledgment
http://www.appliedsec.com/files/The_Evolving_Art_of_Fuzzing.odp
2005.
This work is supported by the European Research Coun-
cil through project ERC-2010-StG 259108-ROSETTA,
the EU FP7 SysSec Network of Excellence and by
the Microsoft Research PhD Scholarship Programme
through the project MRL 2011-049. The authors would
like to thank Bartek Knapik for his help in designing the
statistical evaluation.
References
[1] CVE-2009-2629:
Buffer underﬂow vulnerability in ng-
inx. http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2009-
2629, 2009.
[2] ACZEL, A. D., AND SOUNDERPANDIAN, J. Complete Business
Statistics, sixth ed. McGraw-Hill, 2006.
[3] AKRITIDIS, P., CADAR, C., RAICIU, C., COSTA, M., AND
CASTRO, M. Preventing memory error exploits with WIT. In
Proceedings of the 2008 IEEE Symposium on Security and Pri-
vacy (2008), S&P’08.
[4] BABI ´C, D., MARTIGNONI, L., MCCAMANT, S., AND SONG,
D. Statically-directed dynamic automated test generation.
In
Proceedings of the 2011 International Symposium on Software
Testing and Analysis (2011), ISSTA’11.
[5] BANKS, G., COVA, M., FELMETSGER, V., ALMEROTH, K.,
KEMMERER, R., AND VIGNA, G. SNOOZE: toward a stateful
network protocol fuzZEr. In Proceedings of the 9th international
conference on Information Security (2006), ISC’06.
[6] BAO, T., ZHENG, Y., LIN, Z., ZHANG, X., AND XU, D. Strict
control dependence and its effect on dynamic information ﬂow
analyses. In Proceedings of the 19th International Symposium on
Software testing and analysis (2010), ISSTA’10.
[7] CADAR, C., DUNBAR, D., AND ENGLER, D. KLEE: Unassisted
and automatic generation of high-coverage tests for complex sys-
tems programs. In Proceedings of the 8th USENIX Symposium on
Operating Systems Design and Implementation (2008), OSDI’08.
[8] CADAR, C., GANESH, V., PAWLOWSKI, P. M., DILL, D. L.,
AND ENGLER, D. R. EXE: Automatically generating inputs of
death. In CCS ’06: Proceedings of the 13th ACM conference on
Computer and communications security (2006).
[9] CAVALLARO, L., SAXENA, P., AND SEKAR, R. On the Limits
of Information Flow Techniques for Malware Analysis and Con-
tainment. In Proceedings of the Fifth Conference on Detection
of Intrusions and Malware & Vulnerability Assessment (2008),
DIMVA’08.
[10] CHIPOUNOV, V., KUZNETSOV, V., AND CANDEA, G. S2E: A
platform for in vivo multi-path analysis of software systems. In
Proceedings of the 16th Intl. Conference on Architectural Support
for Programming Languages and Operating Systems (2011), AS-
PLOS’11.
[11] COWAN, C., PU, C., MAIER, D., HINTONY, H., WALPOLE, J.,
BAKKE, P., BEATTIE, S., GRIER, A., WAGLE, P., AND ZHANG,
Q. StackGuard: Automatic Adaptive Detection and Prevention
of Buffer-Overﬂow Attacks. In Proceedings of the 7th USENIX
Security Symposium (1998), SSYM’98.
[14] FERRANTE, J., OTTENSTEIN, K. J., AND WARREN, J. D. The
program dependence graph and its use in optimization. ACM
Trans. Program. Lang. Syst. 9 (1997), 319–349.
[15] GANESH, V., LEEK, T., AND RINARD, M. Taint-based directed
whitebox fuzzing. In Proceedings of the 31st International Con-
ference on Software Engineering (2009), ICSE’09.
[16] GEGICK, M., WILLIAMS, L., OSBORNE, J., AND VOUK,
M. Prioritizing software security fortiﬁcation through code-level
metrics. In Proc. of the 4th ACM workshop on Quality of protec-
tion (Oct. 2008), QoP’08, ACM Press.
[17] GODEFROID, P., KLARLUND, N., AND SEN, K. DART: di-
rected automated random testing.
In Proceedings of the 2005
ACM SIGPLAN conference on Programming language design
and implementation (2005), PLDI’05.
[18] GODEFROID, P., LEVIN, M. Y., AND MOLNAR, D. A. Auto-
mated Whitebox Fuzz Testing. In Proceedings of the 15th Annual
Network and Distributed System Security Symposium (2008),
NDSS’08.
[19] GODEFROID, P., AND LUCHAUP, D. Automatic partial loop
summarization in dynamic test generation. In Proceedings of the
2011 International Symposium on Software Testing and Analysis
(2011), ISSTA’11.
[20] KAKSONEN, R. A functional method for assessing protocol im-
plementation security. Tech. Rep. 448, VTT, 2001.
[21] KANG, M. G., MCCAMANT, S., POOSANKAM, P., AND SONG,
D. DTA++: Dynamic taint analysis with targeted control-ﬂow
propagation.
In Proceedings of the 18th Annual Network and
Distributed System Security Symposium (2011), NDSS’11.
[22] KHURSHID, S., P ˘AS ˘AREANU, C. S., AND VISSER, W. Gener-
alized symbolic execution for model checking and testing.
In
Proceedings of the 9th international conference on Tools and
algorithms for the construction and analysis of systems (2003),
TACAS’03.
[23] LATTNER, C., AND ADVE, V. LLVM: A compilation framework
for lifelong program analysis & transformation. In Proceedings
of the international symposium on Code generation and optimiza-
tion (2004), CGO’04.
[24] MARINESCU, P. D., AND CADAR, C. make test-zesti: a sym-
In
bolic execution solution for improving regression testing.
Proc. of the 2012 International Conference on Software Engi-
neering (June 2012), ICSE’12, pp. 716–726.
[25] MILLER, B. P., FREDRIKSEN, L., AND SO, B. An empirical
study of the reliability of UNIX utilities. Commun. ACM 33 (Dec
1990), 32–44.
[26] MITRE.
Common Vulnerabilities and Exposures (CVE).
http://cve.mitre.org/, 2011.
[27] MOLNAR, D., LI, X. C., AND WAGNER, D. A. Dynamic test
generation to ﬁnd integer bugs in x86 binary linux programs. In
Proceedings of the 18th conference on USENIX security sympo-
sium (2009), SSYM’09.
[28] MOSER, A., KRUEGEL, C., AND KIRDA, E. Exploring multiple
execution paths for malware analysis. In Proceedings of the 2007
IEEE Symposium on Security and Privacy (2007), SP’07, IEEE
Computer Society.
[29] NAGAPPAN, N., BALL, T., AND ZELLER, A. Mining metrics to
predict component failures. In Proceedings of the 28th interna-
tional conference on Software engineering (2006), ICSE’06.
62  22nd USENIX Security Symposium 
USENIX Association
14
[30] NETHERCOTE, N., AND SEWARD, J. Valgrind: A Framework
for Heavyweight Dynamic Binary Instrumentation. In Proceed-
ings of the Third International ACM SIGPLAN/SIGOPS Confer-
ence on Virtual Execution Environments (2007), VEE’07.
[31] NEWSOME, J., AND SONG, D. Dynamic taint analysis: Au-
tomatic detection, analysis, and signature generation of exploit
attacks on commodity software. In Proceedings of the Network
and Distributed Systems Security Symposium (2005), NDSS’05.
[32] NGUYEN, V. H., AND TRAN, L. M. S. Predicting vulnerable
software components with dependency graphs. In Proc. of the 6th
International Workshop on Security Measurements and Metrics
(Sept. 2010), MetriSec’10, ACM Press.
[33] SEN, K., MARINOV, D., AND AGHA, G. CUTE: a concolic
unit testing engine for C. In Proceedings of the 10th European
software engineering conference held jointly with 13th ACM SIG-
SOFT international symposium on Foundations of software engi-
neering (2005), ESEC/FSE-13.
[34] SEREBRYANY, K., BRUENING, D., POTAPENKO, A., AND
VYUKOV, D. AddressSanitizer: A fast address sanity checker.
In Proceedings of USENIX Annual Technical Conference (2012).
[35] SHIN, Y., AND WILLIAMS, L. An initial study on the use of
execution complexity metrics as indicators of software vulner-
abilities.
In Proceedings of the 7th International Workshop on
Software Engineering for Secure Systems (2011), SESS’11.
[36] SLOWINSKA, A., AND BOS, H. Pointless tainting?: evaluating
the practicality of pointer tainting.
In EuroSys ’09: Proceed-
ings of the 4th ACM European conference on Computer systems
(2009).
[37] SLOWINSKA, A., STANCESCU, T., AND BOS, H. Body Armor
for Binaries: preventing buffer overﬂows without recompilation.
In Proceedings of USENIX Annual Technical Conference (2012).
[38] SOTIROV, A.
Modern exploitation and memory pro-
talk,
USENIX Security
invited
bypasses.
tection
http://www.usenix.org/events/sec09/tech/slides/sotirov.pdf,
2009.
[39] SPIKE. http://www.immunitysec.com/resources-freesoftware.shtml.
[40] SUTTON, M., GREENE, A., AND AMINI, P. Fuzzing: Brute
Force Vulnerability Discovery. Addison-Wesley Professional,
2007.
[41] VAN DER VEEN, V., DUTT-SHARMA, N., CAVALLARO, L.,
AND BOS, H. Memory Errors: The Past, the Present, and the
Future. In Proceedings of The 15th International Symposium on
Research in Attacks, Intrusions and Defenses (2012), RAID’12.
[42] WANG, T., WEI, T., GU, G., AND ZOU, W. TaintScope: A
Checksum-Aware Directed Fuzzing Tool for Automatic Software
Vulnerability Detection. In Proceedings of the 31st IEEE Sympo-
sium on Security and Privacy (2010), SP’10.
[43] WILLIAMS, N., MARRE, B., AND MOUY, P. On-the-Fly Gener-
ation of K-Path Tests for C Functions. In Proceedings of the 19th
IEEE international conference on Automated software engineer-
ing (2004), ASE’04.
[44] ZIMMERMANN, T., NAGAPPAN, N., AND WILLIAMS, L.
Searching for a Needle in a Haystack: Predicting Security Vul-
nerabilities for Windows Vista. In Proc. of the 3rd International
Conference on Software Testing, Veriﬁcation and Validation (Apr.
2010), ICST’10.
[45] ZITSER, M., LIPPMANN, R., AND LEEK, T. Testing static anal-
ysis tools using exploitable buffer overﬂows from open source
code. In Proc. of the 12th ACM SIGSOFT twelfth international
symposium on Foundations of software engineering (Nov. 2004),
SIGSOFT ’04/FSE-12.
USENIX Association  
22nd USENIX Security Symposium  63
15