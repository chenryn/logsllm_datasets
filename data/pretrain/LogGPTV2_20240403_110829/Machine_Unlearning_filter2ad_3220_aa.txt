# Machine Unlearning

**Authors:**
- Lucas Bourtoule
- Varun Chandrasekaran
- Christopher A. Choquette-Choo
- Hengrui Jia
- Adelin Travers
- Baiwu Zhang
- David Lie
- Nicolas Papernot

**Affiliations:**
- University of Toronto
- Vector Institute
- University of Wisconsin-Madison

**Conference:**
2021 IEEE Symposium on Security and Privacy (SP)

---

## Abstract

Once users share their data online, it is generally difficult for them to revoke access and request the deletion of that data. This problem is exacerbated by machine learning (ML), as models trained on such data may memorize it, thereby exposing users to privacy risks. However, unlearning this data from ML models is notoriously challenging. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of individual data points during training. While SISA training can be applied to any learning algorithm, it is particularly effective for stateful algorithms like stochastic gradient descent used in deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case scenario where unlearning requests are uniformly distributed across the training set. If the service provider has prior knowledge of the distribution of unlearning requests, this information can be used to further reduce the overhead.

Our evaluation spans several datasets from different domains, each with specific motivations for unlearning. Under no distributional assumptions, SISA training improves the time to unlearn points from the Purchase dataset by 4.63× and 2.45× for the SVHN dataset compared to retraining from scratch. For complex tasks like ImageNet classification, SISA training provides a 1.36× speed-up in retraining, with a small degradation in accuracy. Our work contributes to practical data governance in the context of machine unlearning.

## 1. Introduction

Many applications of machine learning (ML) involve analyzing data collected from individuals. This data is often sensitive, such as medical records or personal emails. Data pipelines are dynamic, with new data being regularly collected and incrementally used to refine existing models in an online learning paradigm. Conversely, data may also need to be deleted. Recent legislation, such as the General Data Protection Regulation (GDPR) in the European Union, the California Consumer Privacy Act in the United States, and PIPEDA in Canada, includes provisions for the "right to be forgotten." This requirement mandates that companies take reasonable steps to erase personal data upon request.

ML models can potentially memorize training data, making it important to unlearn what they have learned from data that needs to be deleted. This problem is tangential to privacy-preserving ML; enforcing ε-differential privacy does not alleviate the need for an unlearning mechanism. Differentially private algorithms ensure that individual training points contribute minimally to the model, but there remains a non-zero contribution from each point. In contrast, forgetting requires that a particular training point have zero contribution to the model, which is orthogonal to the guarantee provided by differential privacy.

To enable models to forget, it is necessary to know exactly how individual training points contributed to model parameter updates. Prior work has shown that this is possible in the statistical query (SQ) learning setting, where the order of data queries is predetermined. In the adaptive setting, where queries depend on past queries, convergence is not guaranteed. While any PAC learning algorithm can be converted to its equivalent in the SQ learning setting, efficient algorithms for SQ learning of complex models like deep neural networks do not exist.

A naive way to ensure models forget is to retrain them from scratch. To avoid the large computational and time overhead associated with full retraining, our research aims to implement the right to be forgotten through the ability to unlearn. Unlearning ensures that the model is no longer trained using the data that the user elected to erase. Specifically, unlearning guarantees that training on a point and then unlearning it will produce the same distribution of models as if the point had never been trained on in the first place.

Due to this strong definition, we do not consider the use of unlearning to mitigate poisoning attacks. Instead, we focus on mechanisms that provide a stronger, privacy-minded unlearning guarantee to satisfy the right to be forgotten.

## 2. SISA Training Approach

Our Sharded, Isolated, Sliced, and Aggregated (SISA) training approach can be implemented with minimal modifications to existing pipelines. First, we divide the training data into multiple disjoint shards, ensuring that each training point is included in only one shard. We then train models in isolation on each shard, limiting the influence of a point to the model trained on the shard containing the point. When a request to unlearn a training point arrives, we only need to retrain the affected model. Since shards are smaller than the entire training set, this decreases the retraining time required for unlearning. However, reducing the amount of data per shard may result in a weaker learner.

To further decrease retraining time, we divide each shard’s data into slices and present these slices incrementally during training. We save the state of model parameters before introducing each new slice, allowing us to start retraining from the last known parameter state that does not include the point to be unlearned. Slicing further reduces the time to unlearn, at the expense of additional storage. At inference, we aggregate the predictions of models trained on each shard, using strategies such as majority voting over predicted labels.

To demonstrate the effectiveness of SISA training, we analytically compute the speed-ups achieved when processing unlearning requests sequentially or in batches. Our results show that SISA training achieves more advantageous trade-offs between accuracy and time to unlearn compared to two baselines: (1) retraining from scratch, and (2) training on a fraction of the original training set.

For simple learning tasks, such as deep networks trained on the Purchase and SVHN datasets, SISA training achieves a speed-up of 4.63× and 2.45× over retraining from scratch, with a nominal degradation in accuracy of less than 2 percentage points. The second baseline, which trains on a large fraction of the data, outperforms SISA training by a factor of S but quickly incurs a large cost in accuracy as S increases. For complex tasks, such as ImageNet classification, SISA training provides a 1.36× speed-up, with a greater accuracy degradation (19.45 percentage points for top-5 accuracy). Transfer learning can significantly reduce this accuracy degradation.

We observe that sharding provides speed-up gains when the number of unlearning requests is less than three times the number of shards. For complex tasks, increasing the number of shards results in a decrease in aggregate accuracy. Slicing, however, always provides a speed-up. These savings in retraining times enable large organizations to benefit from economies of scale.

When faced with non-uniform distributions of unlearning requests, we present a refined variant of our approach that assumes prior knowledge of the distribution. We validate this in a scenario modeling a company operating across multiple jurisdictions with varying legislation and privacy sensitivities. Knowing the distribution of unlearning requests allows us to further decrease expected unlearning time by placing the training points that are likely to be unlearned in a way that reduces retraining time. For simple learning tasks, the cost in terms of accuracy is either null or negligible.

## 3. Contributions

The key contributions of this paper are:
- **New Definition of Unlearning:** We formulate a new, intuitive definition of unlearning that accounts for non-uniform distributions of unlearning requests.
- **SISA Training Framework:** We introduce SISA training, a practical approach for unlearning that relies on data sharding and slicing to reduce the computational overhead of unlearning.
- **Analytical Derivation:** We analytically derive the asymptotic reduction in time to unlearn points with sharding and slicing when the service provider processes requests sequentially or in batches.
- **Empirical Evaluation:** We demonstrate that sharding and slicing combined do not significantly impact accuracy for simple learning tasks and that SISA training can handle orders of magnitude more unlearning requests than what Google anticipates is required to implement the GDPR right to be forgotten.
- **Complex Learning Tasks:** For complex learning tasks, we show that a combination of transfer learning and SISA training induces a nominal decrease in accuracy (∼2 percentage points) with improved retraining time.

## 4. Background on Machine Learning

We provide an overview of machine learning, focusing on neural networks, which often generate the largest computational costs and require dedicated accelerators. Our efforts fall under the realm of supervised machine learning, where tasks are defined in a space Z.