title:Machine Unlearning
author:Lucas Bourtoule and
Varun Chandrasekaran and
Christopher A. Choquette-Choo and
Hengrui Jia and
Adelin Travers and
Baiwu Zhang and
David Lie and
Nicolas Papernot
2021 IEEE Symposium on Security and Privacy (SP)
Machine Unlearning
Lucas Bourtoule*‡§, Varun Chandrasekaran*†, Christopher A. Choquette-Choo*‡§, Hengrui Jia*‡§,
Adelin Travers*‡§, Baiwu Zhang*‡§, David Lie‡, Nicolas Papernot‡§
University of Toronto‡, Vector Institute§, University of Wisconsin-Madison†
9
1
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
Abstract—Once users have shared their data online,
it is
generally difﬁcult for them to revoke access and ask for the data
to be deleted. Machine learning (ML) exacerbates this problem
because any model trained with said data may have memorized it,
putting users at risk of a successful privacy attack exposing their
information. Yet, having models unlearn is notoriously difﬁcult.
We introduce SISA training, a framework that expedites the
unlearning process by strategically limiting the inﬂuence of a
data point in the training procedure. While our framework is
applicable to any learning algorithm, it is designed to achieve
the largest improvements for stateful algorithms like stochastic
gradient descent for deep neural networks. SISA training reduces
the computational overhead associated with unlearning, even
in the worst-case setting where unlearning requests are made
uniformly across the training set. In some cases, the service
provider may have a prior on the distribution of unlearning
requests that will be issued by users. We may take this prior
into account to partition and order data accordingly, and further
decrease overhead from unlearning.
Our evaluation spans several datasets from different domains,
with corresponding motivations for unlearning. Under no dis-
tributional assumptions, for simple learning tasks, we observe
that SISA training improves time to unlearn points from the
Purchase dataset by 4.63×, and 2.45× for the SVHN dataset, over
retraining from scratch. SISA training also provides a speed-up of
1.36× in retraining for complex learning tasks such as ImageNet
classiﬁcation; aided by transfer learning, this results in a small
degradation in accuracy. Our work contributes to practical data
governance in machine unlearning.
I. INTRODUCTION
Many applications of machine learning (ML) involve ana-
lyzing data that is collected from individuals. This data is often
sensitive in nature and could include information like medical
records [1] or personal emails [2]. Morever, data pipelines
are often not static [3]: new data is collected regularly and
incrementally used to further reﬁne existing models following
the online learning paradigm [4].
Conversely, data may also need to be deleted. Recently
introduced legislation, such as the General Data Protection
Regulation (GDPR) in the European Union [5], the California
Consumer Privacy Act [6] in the United States, and PIPEDA
privacy legislation in Canada [7] include provisions that re-
quire the so-called right to be forgotten [8]. This requirement,
which has been one of the most controversial in the GDPR,
mandates that companies take reasonable steps to achieve the
erasure of personal data concerning [the individual] [9]. The
unprecedented scale at which ML is being applied on personal
*All student authors contributed equally and are ordered alphabetically.
data motivates us to examine how this right to be forgotten can
be efﬁciently implemented for ML systems.
potentially memorize
Because ML models
training
data [10], [11], it is important to unlearn what they have
learned from data that
is to be deleted. This problem is
tangential to privacy-preserving ML—enforcing ε-differential
privacy [12] with ε (cid:54)= 0 does not alleviate the need for
an unlearning mechanism. Indeed, while algorithms which
are differentially private guarantee a bound on how much
individual training points contribute to the model and ensure
that this contribution remains small [13], [14], there remains
a non-zero contribution from each point. If this was not the
case, the model would not be able to learn at all (see § III).
In contrast, forgetting requires that a particular training point
have zero contribution to the model, which is orthogonal to
the guarantee provided by differential privacy.
Having models forget necessitates knowledge of exactly
how individual training points contributed to model parameter
updates. Prior work showed this is possible when the learning
algorithm queries data in an order that is decided prior to the
start of learning [15] i.e., in the statistical query (SQ) learning
setting [16]. When the dataset is instead queried adaptively,
i.e., a given query depends on any queries made in the past,
convergence of the approach is no longer guaranteed. In the
adaptive setting, the divergence induced by this approach is
bounded only for models which require a small number of
iterations for learning. While it is true that any algorithm in
the PAC setting can be converted to its equivalent in the SQ
learning setting [16], efﬁcient algorithms for SQ learning of
complex models such as DNNs do not exist.
A naive way to have such models provably forget is to re-
train them from scratch. To avoid the large computational and
time overhead associated with fully retraining models affected
by training data erasure, our research seeks to hold ML to
standards such as the right to be forgotten instead through the
ability to unlearn. Given a trained model, unlearning assures
the user that the model is no longer trained using the data
which the user elected to erase. Put another way, unlearning
guarantees that training on a point and unlearning it afterwards
will produce the same distribution of models that not training
on the point at all, in the ﬁrst place, would have produced.
Due to this strong deﬁnition, we do not consider the
setting in which unlearning is used to mitigate poisoning
attacks [17]–[19]; the guarantee we provide is far stricter than
what would be needed for poisoning—i.e., that the loss of
model accuracy due to the poisoning are mitigated. Instead, we
focus on mechanisms that provide the stronger privacy-minded
© 2021, Lucas Bourtoule. Under license to IEEE.
DOI 10.1109/SP40001.2021.00019
141
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:11:21 UTC from IEEE Xplore.  Restrictions apply. 
unlearning guarantee described above in order to satisfy the
right to be forgotten requirement.
Our SISA training approach, short for Sharded, Isolated,
Sliced, and Aggregated training, can be implemented with
minimal modiﬁcation to existing pipelines. First, we divide
the training data into multiple disjoint shards such that a
training point is included in one shard only; shards partition
the data. Then, we train models in isolation on each of these
shards, which limits the inﬂuence of a point to the model
that was trained on the shard containing the point. Finally,
when a request to unlearn a training point arrives, we need
to retrain only the affected model. Since shards are smaller
than the entire training set, this decreases the retraining time
to achieve unlearning. However, by doing so, we are reducing
the amount of data per shard, which may result in a weak
learner [20]. In addition, rather than training each model on
the entire shard directly, we can divide each shard’s data into
slices and present slices incrementally during training. We save
the state of model parameters before introducing each new
slice, allowing us to start retraining the model from the last
known parameter state that does not include the point to be
unlearned—rather than a random initialization. Slicing further
contributes to decreasing the time to unlearn, at the expense
of additional storage. At inference, we use different strategies
to aggregate the predictions of models trained on each shard:
the simplest one is a majority vote over predicted labels.
To demonstrate that SISA training handles streams of un-
learning requests effectively, we analytically compute speed-
ups achieved when the service provider processes unlearning
requests sequentially (i.e., immediately upon a user revoking
access to their data) or in batches (i.e., the service provider
buffers a few unlearning requests before processing them). Our
results show that SISA training achieves more advantageous
trade-offs between accuracy and time to unlearn—compared
to two baselines: (1) the naive approach of retraining from
scratch, and (2) only train on a fraction of the original training
set (i.e., only use one of the shards to predict).
We ﬁrst turn to simple learning tasks, such as deep networks
trained on Purchase and SVHN. When processing 8 unlearning
requests on Purchase and 18 unlearning requests on SVHN,
we ﬁnd that SISA training achieves a speed-up of 4.63× and
2.45× over the ﬁrst baseline—through the combined effect of
partitioning the data in 20 shards each further divided in 50
slices. This comes at a nominal degradation in accuracy of
less than 2 percentage points. The second baseline is only
S of the data: it
viable when training on a large fraction 1
outperforms SISA training by a factor of S but quickly induces
a large cost in accuracy as S increases. Compared to these
baselines, we conclude that SISA training enables the service
provider to ﬁnd a more advantageous compromise between
model accuracy and time to unlearn. Next, we turn to more
complex learning tasks involving datasets such as Imagenet and
deeper networks. With the same setup (i.e., number of shards
and slices), we observe a speed-up of 1.36×, at the expense
of a greater accuracy degradation (19.45 percentage points for
top-5 accuracy) for 39 requests1. We demonstrate that transfer
learning can signiﬁcantly reduce this accuracy degradation.
We observe that speed-up gains from sharding exist when
the number of unlearning requests is less than three times
the number of shards. However, for complex learning tasks,
increasing the number of shards results in a decrease in
aggregate accuracy. Slicing, however, always provides a speed-
up. While the number of unlearning requests may seem small,
these are three orders of magnitude larger than those in prior
work [21]. These savings in retraining times enable large
organizations to beneﬁt from economies of scale.
When faced with different distributions of unlearning re-
i.e., requests are not uniformly issued across the
quests,
dataset, we present a reﬁned variant of our approach, which
assumes prior knowledge of the distribution of unlearning
requests. We validate it in a scenario that models a company
operating across multiple jurisdictions with varying legislation
and sensitivities to privacy, and accordingly varying distri-
butions of unlearning requests from users based on publicly
available information [21]. Knowing this distribution enables
us to further decrease expected unlearning time by placing the
training points that will likely need to be unlearned in a way
that reduces retraining time. For simple learning tasks, the cost
in terms of accuracy is either null or negligible, depending on
the distribution of requests considered.
In summary, the contributions of this paper are:
• We formulate a new, intuitive deﬁnition of unlearning.
Our deﬁnition also takes into account non-uniform dis-
tributions of unlearning requests.
• We introduce SISA training, a practical approach for
unlearning that relies on data sharding and slicing to
reduce the computational overhead of unlearning.
• We analytically derive the asymptotic reduction in time to
unlearn points with sharding and slicing when the service
provider processes requests sequentially or in batches.
• We demonstrate that sharding and slicing combined do
not
impact accuracy signiﬁcantly for simple learning
tasks, and that SISA training could be immediately
applied to handle orders of magnitude more unlearning
requests than what Google anticipates is required to
implement the GDPR right to be forgotten [21].
• For complex learning tasks, we demonstrate that a com-
bination of transfer learning and SISA training induces
a nominal decrease in accuracy (∼ 2 percentage points)
with improved retraining time.
II. BACKGROUND ON MACHINE LEARNING
We provide rudiments of machine learning as they apply to
neural networks. We chose to study neural networks because
they almost always generate the largest computational costs
and require investments in dedicated accelerators [22], [23].
Our efforts fall under the realm of supervised machine
learning [24]. Tasks to be learned are deﬁned in a space Z
1For 4 requests, observe an 8.01× speed-up for mini-Imagenet at the