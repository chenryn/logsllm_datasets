HAMS’s state retrieval time (e.g., 11.63ms in FD) and state
delivery time to backup (e.g., 2.4ms in FD) of the operator
was also small. HAMS’s NPSB protocol can mask the latency
overhead on all batch sizes. We also evaluated HAMS-Remus
in Figure 11b under the same experiment setting. For all
combinations of services and batch sizes, HAMS-Remus’s
latency overhead was in average 5.51X slower than HAMS.
Figure 12 shows the services’ throughput in four systems,
normalized to bare metal. HAMS incurred little throughput
overhead. For SA, HAMS-Remus also had little throughput
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 11: Latency overhead of the six serviced deployed with H AMS and HAMS-Remus, with varied request batch size. For
batch 128 setting, VGG19 is N/A because a single GPU’s memory cannot hold the whole computation.
(a) HAMS
(b) HAMS-Remus
is the
downgrade because the audio transcriber operator
throughput bottleneck of SA, and hence the SA’s throughput is
not affected by HAMS-Remus’s fault tolerance logic. Overall,
we consider HAMS’s normal case performance reasonable for
deploying real-world ML services.
In summary, HAMS incurs little normal case performance
overhead if the following two conditions are met. First, in a
stateful model, the time taken by the computation stage of
processing the (n + 1)th batch of requests is longer than the
time taken by HAMS to retrieve this model’s state updated by
the nth batch. If so, after processing a batch of requests, NSPB
can retrieve the model’s state in parallel with the computation
stage of the next batch (IV-B).
if a stateful model
Second, each stateful model has downstream models in the
service DAG, so that the delivery of the stateful model’s state
can be done in parallel with the processing of downstream
models (IV-C). Otherwise,
is the last
model in the DAG (e.g., in the AP service), its outputs must
be buffered at the frontend against a client until the state
updated by this client’s request is delivered to the model’s
backup. HAMS incurred little performance overhead in SA
because SA’s latency is dominated by the ﬁrst operator. In our
evaluation, we found both conditions were often met when the
batch size of a service was no less than 64, a typical setting
in real-world deployments.
C. Effectiveness of HAMS’s Components
HAMS’s NSPB protocol contains two components for low
performance overhead: retrieving a model’s state without
stopping (§IV-B) and fast output releasing without waiting
for state delivery to the backup (§IV-C). To analyze the
effectiveness of NSPB’s two components,
in Table I, we
evaluated the six services’ latencies with batch size 64 under
two settings: (1) disabling the fast output releasing and
buffering the output until the state delivered to the backup
(HAMS-S1), and (2) disabling the non-stop state retrieving
releasing (HAMS-S2).
but still enabling the fast output
Fig. 12: Throughput of six services, all are normalized to bare
metal. Bare metal’s absolute value (# requests processed per
second) is on the ﬁgure. The request batch size is 64.
193
The results show that HAMS-S1 incurred at most 53.94%
latency slowdown compared with HAMS in the normal
case, and that HAMS-S2 incurred at most 57.05% latency
slowdown compared to HAMS. Nevertheless, both HAMS-S1
and HAMS-S2 had lower latency than HAMS-Remus, which
indicated that the both components (§ IV) are essential.
HAMS
1604.66ms
123.02ms
289.06ms
224.94ms
292.47ms
22.31ms
HAMS-Remus
SA
1671.88ms
SP
210.45ms
AP
375.94ms
FD
300.86ms
OL(V)
508.64ms
OL(M)
43.26ms
TABLE I: Effectiveness of HAMS’s components.
HAMS-S1
1640.32ms
152.92ms
320.18ms
252.05ms
450.23ms
32.90ms
HAMS-S2
1664.12ms
172.41ms
349.80ms
271.42ms
426.13ms
35.04ms
D. Recovery Time
We compared the recovery time cost of three fault tolerance
systems (HAMS, HAMS-Remus, and LS). For HAMS and
HAMS-Remus, we randomly picked one stateful operator in
each of the six services (under batch size 64 setting) and killed
its primary. For LS, we killed the primary operator in each
service at the 50th batch of requests from its latest checkpoint
(LS does a checkpoint per 150 batches of requests).
and HAMS-Remus
Overall, both HAMS
achieved a
sub-second level of recovery time. HAMS and HAMS-Remus
both have a hot standby backup for each stateful operator, so
the recovery time was mainly composed of failure discovery,
recovery protocol (§IV-E), and backup handover. LS achieved
a minute-level recovery time. The recovery time was mainly
composed of operator’s checkpoint loading (i.e., initialization
of an ML model) and the replay time. The replay time
depended on the timing of failures, and we chose one-third
of LS’s checkpoint interval (150 batches of requests). LS’s
paper [85] also reports minute-level recovery time. We set LS’s
checkpoint interval to every 1 batch of requests, so that LS
can have fast recovery time. LS incurred 81% latency overhead
on average, as LS essentially became HAMS-Remus: after a
stateful operator processes a request batch, LS needs to stop,
copy, and transfer its state to another host.
We also killed a stateless operator in each service and found
that HAMS, LS, and HAMS-Remus shows similar recovery
time with on average of 320.45 milliseconds, as the recovery
time for all three systems was dominated by the time to update
a service graph’s topology by adding the stateless hot standby
operator and by updating the proxies’ logic (§V).
To evaluate HAMS’s recovery on correlated failures, we
did three experiments on the SP service as a complicated
case and the AP service for the adjacent stateful models’
case (§IV-C). First, we killed O3 (i.e., operator ID#3 in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
SA
SP
AP
FD
OL(V)
OL(M)
HAMS
116.12ms
142.43ms
150.01ms
143.25ms
254.19ms
134.74ms
HAMS-Remus
109.23ms
123.12ms
119.01ms
127.34ms
315.42ms
141.84ms
LS
124.43s
32.04s
56.94s
47.82s
62.10s
21.09s
TABLE II: Recovery time of HAMS components (§IV).
Figure 8) and O4, a stateless model and a stateful model,
in the SP service, and we collected an average recovery
time of 344.79ms, dominated by the time taken to relaunch
a new stateless O3. Second, we killed the primaries of O2
and O3, two adjacent stateful models in AP, and collected
an average recovery time of 172.24ms, around 20ms longer
than the recovery time of only killing one of them because
HAMS iteratively identiﬁes suspected failed models (§IV-E)
so it needs an additional timeout (20ms) to suspect the second
failures. HAMS’s recovery time on these two experiments
of correlated failures was just a little larger than that of a
single failure as HAMS recovers each failed model in parallel
(§IV-E).
Third, we triggered the case of Figure 6 using AP by
delaying state delivery of O2 and then killed its primary, and
we also killed O3’s backup at the same time. In this extreme
case, HAMS still ensures global consistency: we checked each
operator’s proxy log and found both the new primaries of
O2&O3, and their downstream models O4, O5, and clients, did
not receive conﬂicting requests. However, this case incurred a
recovery time of 731.24ms in average. This time was mainly
taken by O3’s primary for stopping its current execution on
GPU and rolling back to a previous state (§IV-C), which is
much larger than the time for promoting a backup to primary
(Table II). This result suggests that promoting downstream
models’ backups is indeed more efﬁcient than rolling back
their primaries when an upstream stateful model fails, aligning
with NSPB’s design choice (§IV-C).
E. Limitations
HAMS has three limitations. First, HAMS needs 2X resource
for replicating each stateful model. We deem this resource
usage worthwhile because it is essential to ensure sub-second
failover time for mission-critical services. In practice, only
a small portion of models in a service graph is stateful
and needs the extra resource usage. Second, HAMS requires
developers to identify the computation and update stages of a
stateful model. However, with HAMS’s API, this identiﬁcation
is simple and needs only 4-10 lines of code for each evaluated
model (§V). Third, HAMS’s NSPB protocol is only applicable
to ML operators that follows the compute-then-update (i.e., a
clear boundary between compute and update) manner (§II-B),
and typical ML operators (e.g., neural networks) follow this
manner. Studying other ML algorithms (e.g., graph-based
ML [75]) is left as future work. HAMS is optimized for
ML-context-aware, white-box replication deployed in a service
graph manner, while LS and Remus do black-box replication
for general services.
serving
VII. RELATED WORK
systems,
including
Clipper
ML
[11],
Michelangelo [49], Tensorﬂow-Serving [58], TensorRT [82],
Pretzel [42], and Ray [50], simpliﬁes deployment of ML
models by providing a web server frontend [11], [58], [82],
and runtime caching [11], [42], [58] or batching [11], [42],
[50], [58]. However, none of them focus on proving high
availability, so HAMS can be integrated to all these systems.
GranSLAm [34] and Inferline [10] optimizes deployment of
ML service graphs to meet service level latency requirements,
but they do not handle failures, so HAMS can beneﬁt these two
systems. ParM [37] uses a parity model trained with erasure
code [45] to reconstruct prediction results of a failed model to
reply the clients timely. ParM does not support stateful models.
State Machine Replication (SMR) [12], [41], [43], [48], [59],
[63], [81] models a stateful application as a deterministic state
machine and feed replicas of the application with the same
sequence of requests. HAMS uses SMR to replicate its frontend
as it is deterministic. To handle non-determinism, crane [12]
and Rex [24] make thread scheduling deterministic [12], [24].
These systems are not suitable for ML models running on GPU
as GPU’s scheduling is non-deterministic in the architecture
level [30]. Plover [84], Eve [35], and Colo [15] compares
state update or outputs among replicas, and invoke a state
transfer on divergence. These systems fallback to Remus for
ML service graph as an ML model usually update all its state
after each batch of requests.
Fault
systems
[51],
[52]
[69], stream
processing [2], [6], [87], micro-service systems [2], [3], [21],
and network function virtualization [36], [39]. Among these
systems, Tensorﬂow [1], Apache Flink [6] Naiad [51], and
Horovod [69] uses periodical global checkpoint at runtime
and invokes a coordinated rollback on failures, leading to a
long recovery time [85]. Guard [40] further reduces normal
case overhead with asynchronous checkpoints. Ray [50],
CIEL [52], Drizzle [83], and Noria [21], Manetho [19], and
Lineage Stash [85] records the runtime lineage information to
reconstruct lost state on failures. These systems do not handle
operators’ internal non-determinism during replay.
for Dataﬂow
includes distributed ML training [1],
tolerance
[19],
VIII. CONCLUSION
We presented HAMS, an efﬁcient system for deploying
highly available ML service graphs. NSPB combines the
conceptual strengths of primary-backup and checkpoint-replay
to meet all the three fault tolerance requirements (§I). HAMS
can serve as a recovery component in existing ML serving
systems, greatly improving their reliability. HAMS’s source
code is released on github.com/hku-systems/hams.
ACKNOWLEDGMENT
Thank all reviewers for their valuable comments. The
work is funded by grants partly from the Huawei Innovation
Research Program (HIRP) Flagship, HK RGC ECS No.
27200916, HK RGC GRF No.17207117, No. 17202318,
Croucher Innovation Award, and NSF China No. 61802358.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:27:08 UTC from IEEE Xplore.  Restrictions apply. 
194
REFERENCES
[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, et al.
Tensorﬂow: A system
In 12th {USENIX} Symposium on
for large-scale machine learning.
Operating Systems Design and Implementation ({OSDI} 16), pages
265–283, 2016.
[2] T. Akidau, A. Balikov, K. Bekiro˘glu, S. Chernyak, J. Haberman, R. Lax,
S. McVeety, D. Mills, P. Nordstrom, and S. Whittle. Millwheel:
fault-tolerant stream processing at internet scale. Proceedings of the
VLDB Endowment, 6(11):1033–1044, 2013.
[3] R. C. Aksoy and M. Kapritsos. Aegean: replication beyond the
In Proceedings of the 27th ACM Symposium on
client-server model.
Operating Systems Principles, pages 385–398. ACM, 2019.
[4] L. Alvisi and K. Marzullo. Message logging: Pessimistic, optimistic,
IEEE Transactions on Software Engineering,
causal, and optimal.
24(2):149–159, 1998.
[5] Transposed Convolution Operator
source
code
of Caffe2.
https://github.com/pytorch/pytorch/blob/master/caffe2/operators/conv
transpose op cudnn.cc#L330.