can append c to the next Cstructs to be delivered. That is
true if c immediately follows the last appended command for
each object l in c.LS (line 12). Then, once a new command
has been appended to the Cstructs (line 13), pi triggers the
delivery of the updated Cstructs (line 14), and it advances
the pointers to the last appended commands (lines 15–16).
4) Acquisition phase (Algorithm 4): A node pi tries to
acquire the necessary ownership to decide command c (line
1). For each object l in c.LS such that there is no instance
in for l which was decided for c, pi adds the next available
162
position for l, i.e., LastDecided[l] + 1, to the ins set (line
2). Further, for each pair (cid:3)l, in(cid:4) ∈ ins, it increments the
current epoch number for l (lines 3–4). Then it broadcasts
a PREPARE message with ins and the new epochs eps, and
it waits for a quorum of ACKPREPARE replies (lines 5–6).
If at least one ACKPREPARE is marked as N ACK (line
7), the ownership acquisition did not succeed, and pi restarts
a new Coordination phase for c by calling C-PROPOSE(c)
(line 8). To guarantee that c is eventually decided also in
scenarios of high conﬂict, pi might also decide to trigger
C-PROPOSE(c) on a designated leader by switching to a
classic Paxos protocol as described in Section IV-C.
Algorithm 4 M 2P AXOS: Acquisition Phase (node pi).
1:
2:
Set ins ← { (cid:5)l, LastDecided[l] + 1(cid:6) :
function Void ACQUISITIONPHASE(Cmd c)
Decided[l][in] = c}
l ∈ c.LS ∧ (cid:2)in :
3:
4:
5:
6:
from
Array eps
∀(cid:5)l, in(cid:6) ∈ ins, eps[l][in] ← + + Epoch[l]
send PREPARE((cid:5)ins, eps(cid:6)) to all pk ∈ Π
Set replies ← receive ACKPREPARE((cid:5)ins, eps, −, −(cid:6))
if ∃(cid:5)ins, eps, N ACK, −(cid:6) ∈ replies then
Quorum
else
trigger C-PROPOSE(c)
trigger C-PROPOSE(c)
Cmd toF orce ← SELECT(ins, replies)
Bool r ← ACCEPTPHASE(toF orce, c, ins, eps)
if r = ⊥ ∨ (∃l, in : toF orce[l][in] = (cid:5)v, r(cid:6) ∧ v (cid:3)= c) then
7:
8:
9:
10:
11:
12:
13:
14:
15: upon PREPARE((cid:5)Set ins, Array eps(cid:6)) from pj
16:
∀(cid:5)l, in(cid:6) ∈ ins, Rnd[l][in] ← eps[l][in]
17:
Set decs ← { (cid:5)l, in, V dec[l][in], Rdec[l][in](cid:6) : (cid:5)l, in(cid:6) ∈ ins}
18:
send ACKPREPARE((cid:5)ins, eps, ACK, decs(cid:6)) to pj
19:
20:
send ACKPREPARE((cid:5)ins, eps, N ACK, decs(cid:6)) to pj
21:
22:
23:
24:
25:
if ∀(cid:5)l, in(cid:6) ∈ ins, Rnd[l][in] < eps[l][in] then
Epoch k ← max({r : (cid:5)l, in, −, r(cid:6) ∈ decs∧(cid:5)−, −, −, decs(cid:6) ∈
Cmd r ← v : (cid:5)l, in, v, k(cid:6) ∈ decs ∧ (cid:5)−, −, −, decs(cid:6) ∈ replies
toF orce[l][in] ← (cid:5)r, k(cid:6)
function Set SELECT(Set ins, Set replies)
Array toF orce
for all (cid:5)l, in(cid:6) ∈ ins do
replies})
else
26:
27:
28:
return toF orce
A node rejects a PREPARE message (line 15) by replying
with an ACKPREPARE marked as N ACK, in case there
exists a pair (cid:3)l, in(cid:4) in the received ins set such that the
received eps[l][in] is obsolete on that node, i.e., eps[l][in] ≤
Rnd[l][in] (lines 20–21). On the contrary, the node replies
with an ACKPREPARE marked as ACK, by including the
last epoch in which it accepted a command and the last
command accepted for any position (cid:3)l, in(cid:4) in the received
ins. It also changes the epoch number associated with any
position (cid:3)l, in(cid:4) in the received ins by using the values in
the received eps (lines 16–19).
The meaning of the last two operations is straightforward.
A node acknowledges a PREPARE on a position (cid:3)l, in(cid:4) by
promising that it will never positively reply to any other
message for (cid:3)l, in(cid:4) associated with an epoch number not
greater than eps[l][in]. In addition, it will force the sender
of the PREPARE to take into account any possible previous
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply. 
command already issued by another proposal and possibly
accepted in (cid:3)l, in(cid:4). This step is necessary to guarantee
Consistency also in scenarios where a node that is in the
process of executing an Accept phase either crashes or is
suspected as crashed.
Afterwards, if pi receives a quorum of replies without
any message marked as NACK (lines 9–11), it can enter
the Accept phase for c. At this time, the input of that phase
also includes the set of commands suggested by the received
ACKPREPARE messages. In particular, unlike the Coordi-
nation phase, in this phase pi passes the multidimensional
array toF orce to the Accept phase, where toF orce[l][in], if
not N U LL, stores the command to be accepted in position
(cid:3)l, in(cid:4) and its epoch number (line 11).
An entry (cid:3)l, in(cid:4) of the array toF orce is computed by
the SELECT function as follows (lines 10 and 22–28):
toF orce[l][in] is equal to (cid:3)r, k(cid:4) where k is the maximum
epoch number suggested by a received ACKPREPARE mes-
sage and associated with the pair (cid:3)l, in(cid:4), while r is the
command (if any) associated with the epoch k in the received
ACKPREPARE messages. Since the prepare phase is a Paxos
prepare phase extended to the case of multiple objects,
we inherit
if the set of
commands associated with k is not empty, it contains only
one command.
the Paxos’s property such that
Finally, if the Accept phase does not succeed (for the same
reasons described in Section V-B2) or pi did not succeed to
decide c on all the objects in c.LS (because toF orce was
not empty), pi triggers a new Coordination phase by calling
C-PROPOSE(c) (lines 12–13).
VI. CORRECTNESS ARGUMENTS
In this section, due to space constraints, we only provide
an intuition on why M 2P AXOS correctly implements the
Generalized Consensus speciﬁcation, as deﬁned in Sec-
tion III. The reader should refer to the M 2P AXOS technical
report [22] for a complete proof of correctness.
First, if we consider that in M 2P AXOS nodes only decide
the content of Cstructs variables (lines 13–14 of Algo-
rithm 3), then the Non-triviality property is guaranteed be-
cause a node only appends proposed commands in Cstructs
(line 13 of Algorithm 3), and Stability is guaranteed because
Cstructs variables grow monotonically on each node.
We prove that M 2P AXOS guarantees the Consistency
property by relying on the correctness of Paxos [1]. In
particular, we have to consider that: (A) M 2P AXOS decides
at most one command for each pair of object l and instance
in, meaning that the value of Decided[l][in] (Algorithm 3),
if different from N U LL, is the same on all nodes; (B) a
node orders commands in the same way for all the common
objects that the commands access; and (C) commands that
access an object l are appended in CStructs by following
the order deﬁned by the elements of the row Decided[l].
(cid:3)
(cid:2)
N
2
The Liveness property, as deﬁned in Section III, is guar-
anteed under the same assumptions of Paxos, such that at
most f =
nodes can be faulty at any time, and a
leader election is eventually possible. Indeed, in that case, if
a command c has been proposed by a correct node pi, and
there is no other concurrent and conﬂicting command with c
in the system, pi succeeds the execution of all the phases of
the protocol for c, since no other node attempts to become
the owner of any of the objects in c.LS
VII. EVALUATION STUDY
We implemented M 2P AXOS and all competitors within
a uniﬁed framework, written in the Go programming lan-
guage [32]. We evaluated M 2P AXOS by comparing it
against three other consensus algorithms: EPaxos, General-
ized Paxos and Multi-Paxos. We used up to 49 nodes on
the Amazon EC2 infrastructure. Unless otherwise stated,
each node is a c3.4xlarge instance (Intel Xeon 2.8GHz,
16 cores, 30GB RAM) running Amazon Linux 2014.09.1.
All nodes were deployed under a single placement group.
Network bandwidth was measured in excess of 7900mbps.
Throughout the evaluation, we refer to a local command
from a node pi as a command that operates on objects whose
ownership is already held by pi.
To properly load the system, we injected commands into
an open-loop using up to 64 client threads at each node.
Commands are accompanied by a 16-byte payload. After
issuing each command, a client thread goes to sleep for
a conﬁgurable amount of time (think time). To prevent
overloading the system, we limit the number of commands
still in-ﬂight. The limit is conﬁgured for best performance
under each deployment, and when it is reached, a node will
skip issuing new commands. Except for the experiments in
Figure 2, network messages are batched in order to optimize
the network utilization [33]. Each datapoint represents the
average of at least 5 measurements.
As benchmarks, we implemented a synthetic application
that we customized in order to cover different workloads,
which span from the most favorable ones (i.e., partitionable
with no inter-node conﬂicts) to those that require command
forwarding (i.e., when the accessed objects share a single
remote owner), and to those adverse (i.e., when ownership
must be acquired from multiple nodes). In addition, we also
ported TPC-C [20], the well-known benchmark widely used
in on-line transaction processing systems. Our implementa-
tion of TPC-C generates commands that are composed of
all the parameters needed for executing TPC-C transactions
according to the stored procedure model [34], [4] (e.g., the
Id of the accessed warehouse, the Id of the accessed
district). However M 2P AXOS is a consensus layer, thus
the actual transaction processing has been omitted. The main
purpose of evaluating TPC-C is to show the performance of
M 2P AXOS when relevant workload characteristics, such as
163
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:37 UTC from IEEE Xplore.  Restrictions apply. 
Figure 1. Maximum attainable throughput varying
the number of nodes. Command locality is 100%.
Figure 2. Median latency without batching net-
work messages. Command locality is 100%.
Figure 3. Scalability. 64 clients per node, and
5 ms think time. Command locality is 100%.
conﬂict degree and the number of accessed objects, are set
by a well-known benchmark.
We do not explicitly report performance measurements
when nodes crash because that scenario would be equivalent
of migrating the ownerships acquired by the crashed node.
A. Synthetic benchmark
We ﬁrst evaluated M 2P AXOS under its most favorable
conditions. More speciﬁcally, all commands touch a single
object, and a command proposed by a node can only conﬂict
with commands proposed by the same node. This scenario
is representative for partitioned objects, where replication is
only employed for fault-tolerance.
We evaluated the scalability of each consensus protocol
by scaling the system up from 3 to 49 nodes. Figure 1 shows
the maximum throughput achieved, namely the performance
collected right before reaching the saturation point in each
conﬁguration. M 2P AXOS provides a signiﬁcant improve-
ment, up to 3-7×, when compared to the nearest competitor
(i.e., EPaxos), it exhibits great scalability until 11 nodes,
and its throughput keeps increasing past 11 nodes, albeit
at a slower rate. Multi-Paxos is a distant runner-up at 11
nodes and below, and its performance degrades due to the
single leader saturating its computational resources (mainly
the CPU utilization and the network socket management).
Figure 2 shows the median command latency with a
system without batching network messages. This way, it is
clear the end-to-end latency per command experienced by
the application. With a low number of nodes, M 2P AXOS
narrowly wins over Multi-Paxos. As the number of nodes
is increased, M 2P AXOS remains the fastest to deliver, with
up to 41% better latency than EPaxos.
Figure 3 reports the throughput of all competitors when
the number of clients per node is kept constant while the
node count
increases. The results show that, unlike the
others, M 2P AXOS exhibits near-linear scalability because
it does not generate high contention at the network layer.
Summarizing, by the analysis of Figures 1, 2, and 3 we
can point out weaknesses of the other competitors, which are
overcome by M 2P AXOS. On the one hand, both Generalized
Paxos and Multi-Paxos suffer from the single leader design,
which prevents performance from scaling when the size
of the deployment increases. On the other hand, although
EPaxos allows multiple leaders to concurrently establish the
order of a command without contacting a single designated
node, its characteristics hamper the achievement of high
performance when the number of nodes goes beyond 7.
Figure 4. Maximum throughput for 11-nodes deployments with different
machine types. The number of cores are 4, 8, 16 and 32, respectively.
In fact, EPaxos requires a bigger size of quorum in order
to deliver a command in two communication delays in con-
ﬁgurations with more than 5 nodes, unlike M 2P AXOS. As a
result, as showed in Figure 3, EPaxos provides performance
similar to M 2P AXOS up to 7 nodes, where the size of
M 2P AXOS’s quorum and EPaxos’s quorum is comparable.
After that,
the gap in performance becomes substantial.
In addition to that, EPaxos requires the identiﬁcation of