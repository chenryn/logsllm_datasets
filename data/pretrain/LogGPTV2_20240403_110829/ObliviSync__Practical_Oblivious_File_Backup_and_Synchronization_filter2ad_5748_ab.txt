• Performance: Our system well matches the needs of real
ﬁle systems and matches the services provided by current
cloud synchronization providers. It can also be tuned to
different settings based on the desired communication rate
and delay in synchronization.
Basic architecture. The high-level design of ObliviSync is
presented in Figure 1. There are two types of clients in our
system: a read/write client (ObliviSync-RW) and a read-only
client (ObliviSync-RO). At any given time, there can be any
number of ObliviSync-RO’s active as well as zero or one
ObliviSync-RW clients. We note that a given device may work
as a read-only client in one period of time and as a write-
only client in other periods of time.2 Both clients consist of
an actual backend folder as well as a virtual frontend folder,
with a FUSE client running in the background to seamlessly
translate the encrypted data in the backend to the user’s view
in the frontend virtual ﬁlesystem.
We rely on existing cloud synchronization tools to keep all
clients’ backend directories fully synchronized. This directory
consists of encrypted ﬁles that are treated as generic storage
blocks, and embedded within these storage blocks is a ﬁle sys-
tem structure loosely based on i-node style ﬁle systems which
allows for variable-sized ﬁles to be split and packed into ﬁxed-
size units. Using a shared private key (which could be derived
from a password) the job of both clients ObliviSync-RO and
ObliviSync-RW is to decrypt and efﬁciently fetch data from
these encrypted ﬁles in order to serve ordinary read operations
from the client operating in the frontend directory.
The ObliviSync-RW client, which will be the only client
able to change the backend ﬁles, has additional responsibilities:
(1) to maintain the ﬁle system encoding embedded within
the blocks, and (2) to perform updates to the blocks in an
oblivious manner using our efﬁcient modiﬁcation of the write-
only ORAM described in the previous subsection.
2How to make sure that only one write-only client operates at a given time
is out the scope, and in this paper, we will simply assume the existence of
the procedure to enforce it.
3
block	
  block	
  block	
  Local Storage Backend Cloud	
  Service	
  ﬁle	
  ﬁle	
  ﬁle	
  block	
  ﬁle	
  Write Reads Read/Write Client Read Client User Facing Frontend Cloud Synchronized Folder ObliviSync  - RW FUSE ObliviSync  - RO FUSE User transparency with FUSE mount. From the user’s per-
spective, however, the interaction with the frontend directory
occurs as if interacting with any ﬁles on the host system. This
is possible because we also implemented a FUSE mount (ﬁle
system in user space) interface which displays the embedded
ﬁle system within the backend blocks to the user as if it
were any other ﬁle system mount. Under the covers, though,
the ObliviSync-RO or ObliviSync-RW clients are using the
backend directory ﬁles in order to serve all data requests by the
client, and the ObliviSync-RW client is additionally monitoring
for ﬁle changes/creations in the FUSE mount and propagating
those changes to the backend.
Strong obliviousness through buffered writes.
In order
to maintain obliviousness, these updates are not immediately
written to the backend ﬁlesystem by the ObliviSync-RW client.
Instead, the process maintains a buffer of writes that are staged
to be committed. At regular timed intervals, random blocks
from the backend are loaded, repacked with as much data
from the buffer as possible, and then re-encrypted and written
back to the backend folder. From there, the user’s chosen ﬁle
synchronization or backup service will do its work to propagate
the changes to any read-only clients. Moreover, even when
there are no updates in the buffer, the client pushes dummy
updates by rewriting the chosen blocks with random data. In
this way, as the number of blocks written at each step is
ﬁxed, and these writes (either real or dummy) occur at regular
timed intervals, an adversary operating at the network layer is
unable to determine anything about the ﬁle contents or access
patterns. Without dummy updates, for example, the adversary
can make a reasonable guess about the size of the ﬁles that
the client writes; continuted updates without pause is likely
to indicate that the client is writing a large ﬁle. Note that
in some cases, revealing whether a client stores large ﬁles
(e.g., movies) may be sensitive. Further details on all of these
components can be found in Section IV. The full source code
of our implementation is available on GitHub [14].
III. SECURITY DEFINITIONS
A. Write-only Oblivious Synchronization
Block-based ﬁlesystem. Our system has more capabilities
than a standard ORAM,
including support for additional
ﬁlesystem operations, so we require a modiﬁed security deﬁni-
tion which we present here. We ﬁrst formally deﬁne the syntax
of a block-based ﬁlesystem with block size B.
• create(ﬁlename): create a new (empty) ﬁle.
• delete(ﬁlename): remove a ﬁle from the system.
• resize(ﬁlename,size): change the size of a ﬁle. The size
is given in bytes, and can be greater or smaller than the
current size.
• write(ﬁlename,offset,length): write data to the identiﬁed
ﬁle according to the offset and length arguments. The
offset is a block offset. Unless the offset refers to the last
block in the ﬁle, length must be a multiple of B.
• read(ﬁlename,offset,length) → data: read data from the
identiﬁed ﬁle according to the offset and length argu-
ments. Again, offset is a block offset, and length must be
a multiple of the block size B unless the read includes
the last offset.
For simplicity, we only consider these ﬁve core operations.
Other standard ﬁlesystem operations can be implemented using
these core functionalities.
Obliviousness and more. The original write-only ORAM
deﬁnition in [8] requires indistinguishability between any two
write accesses with same data sizes. However, the deﬁnition
does not consider the time at which write operations take place.
Here, we put forward a stronger security notion for the ﬁle
system that additionally hides both the data length and the
time of non-read operations.
For example, we want
to make sure all
the following
operation sequences are indistinguishable:
• no write operations at all
• write(ﬁle1,1,5) and write(ﬁle2,3,3) at time 3, and
write(ﬁle1,6,9) at time 5
• write(ﬁle2,1,20) at time 5
For this purpose, we ﬁrst deﬁne (L, t)-fsequences. Here,
the parameter L is the maximum number of bytes that may
be modiﬁed, and t is the latest time that is allowed. For
example, the above sequences are all (20, 5)-fsequences, since
all sequences write at most 20 bytes data in total and have the
last write before or at time 5.
Deﬁnition 1 ((L, t)-fsequence). A sequence of non-read oper-
ations for a block ﬁlesystem is a (L, t)-fsequence if the total
number of bytes to be modiﬁed in the ﬁlesystem metadata and
ﬁle data is at most L, and the last operation takes place before
or at time t.
Our goal is to achieve an efﬁcient block ﬁlesystem con-
struction such that any two (L, t)-fsequences are indistinguish-
able.
Deﬁnition 2 (Write-only strong obliviousness). Let L and t
be the parameters for fsequences. A block ﬁlesystem is write-
only strongly-oblivious with running time T , if for any two
(L, t)-fsequences P0 and P1, it holds that:
• The ﬁlesystem ﬁnishes all the tasks in each fsequence
within time T with probability 1 − neg(λ), where is λ
is the security parameter.
• The access pattern of P0 is computationally indistinguish-
able to that of P1.
IV. SYSTEM DETAILS
As described previously, the basic design of ObliviSync
is presented in Figure 1. In this section we highlight the
implementation details further. In particular, we describe the
implementation components focusing on interesting design
challenges and user settings that can be used to tune the
performance.
A. Filesystem Description
First, we describe the data organization of the backend ﬁles
that form the storage mechanisms for the encrypted ﬁlesystem.
We note that our ﬁlesystem is speciﬁcally tailored for the
ObliviSync use case, and this design is what leads to our
practical performance gains.
4
ﬁle X
ﬁle Y
x1
y1
Frontend
x2
x3
x4
y2
y3
block-id=0
block-id=2
Backend
x1
x2
x3
x4 y3
y1
y2
block-id=1
ﬁle 0
ﬁle 1
ﬁle 2
ﬁle 3
ﬁle 4
ﬁle 5
Fig. 2. An example of two front-end ﬁles stored in the backend ﬁles. The
frontend ﬁle X (resp. ﬁle Y) consists of 4 (resp. 3) fragments where the last
fragment is smaller than the block size. Each fragment is stored in a block.
Each backend ﬁle contains exactly two blocks. Backend blocks are indexed
by block-ids, starting with 0. For example, the two blocks in ﬁle 0 has block-
ids 0 and 1, respectively, and the block-ids of the blocks where fragments
x1, x2, x3, x4 are located are 4, 2, 3, 8 respectively. Note the small fragments
x4 and y3 are located in the same block with block-id 8.
fragments, blocks, block-ids.
Files,
The user of an
ObliviSync client is creating, writing, reading, and deleting
logical ﬁles in the frontend ﬁlesystem via the FUSE mount.
The ObliviSync client, to store user ﬁles, will break down the
ﬁles into one or more fragments, and these fragments are then
stored within various encrypted blocks in the backend.
Blocks are stored in the backend directory in block pairs
of exactly two ﬁles each. (Note each block pair resides within
a single ﬁle in the backend directory, but we avoid the use of
the word “ﬁle” when possible to disambiguate from the logical
frontend ﬁles, and instead refer to these as “block pairs”.) We
explain why we chose to combine exactly two blocks in each
pair later when discussing performance optimization. Note that
it is only these encrypted block pairs in the backend directory
which are seen (and transferred) by the cloud synchronization
service.
While each block has the same size, ﬁles stored in the
frontend can have arbitrary sizes. A ﬁle fragment can be
smaller than a block size, but not larger. In particular, each
ﬁle will consist of an ordered list of fragments, only the last
of which may have size smaller than that of a full block.
While ﬁles are indicated by pathnames as in any normal
ﬁlesystem, backend blocks are indexed by numeric block-ids,
with numbering such that the two blocks that make up a block
pair are readily identiﬁed. See Figure 2 for a simple example.
Filetable, ﬁle-entries, ﬁle-ids. Since a single frontend ﬁle
consists of a series of fragments (or a single fragment if the
ﬁle is small) where each fragment is stored within a block,
ObliviSync needs to keep track of the backend blocks that
each ﬁle uses so that it may support ﬁle create/update/delete
operations effectively.
For this purpose, ObliviSync maintains a ﬁletable, consist-
ing of ﬁle-entries. Each frontend ﬁle is one-to-one mapped
to a ﬁle-entry, which maintains some ﬁle metadata and a list
of block-ids in order to refer to the blocks that contain the
frontend ﬁle’s fragments, in order. In a sense, block-ids (resp.,
the ﬁle-entry) in our system are similar to position map entries
(resp., the position map) in traditional ORAMs. The main
difference is that in order to treat multiple front-end ﬁles, our
system maintains a ﬁletable containing multiple ﬁle-entries.
The ﬁle-entries in the ﬁletable are indexed by ﬁle-ids. As
ﬁles update, the ﬁle-id remains constant; however, based on the
oblivious writing procedure, the ﬁle fragments may be placed
in different backend blocks, so the block-ids may change
accordingly.
Filetable B-tree. The ﬁletable mapping ﬁle-ids to ﬁle-entries
is implemented as a B-tree, with node size B proportional to
the size of backend blocks. In general, the height of this B-tree
is O(logB n), where n is the number of ﬁles stored. As we
will see in Section IV-G, for typical scenarios the block size
is sufﬁciently large so that the B-tree height can be at most 1
(i.e., the tree consists of the root node and its children), and
we will assume this case for the remainder.
The leaf nodes are added and stored alongside ordinary ﬁles
in blocks. There are two important differences from regular
ﬁles, however: leaf nodes are always exactly the size of one
full block, and they are indexed (within the root node) by their
block-id directly. This way, leaf nodes have neither a ﬁle-id
nor a ﬁle-entry.
Directory ﬁles. As is typical in most ﬁlesystems, ﬁle paths
are grouped into directories (i.e., folders), and each directory
contains the pathnames and ﬁle-ids of its contents. Observe
that the directory ﬁle only changes when a ﬁle within it is
created or destroyed, since ﬁle-ids are persistent between ﬁle
modiﬁcations. Directory ﬁles are treated just like any other
ﬁle, with the special exception that the root directory ﬁle is
always assigned ﬁle-id 0.
B. Design Choices for Performance Optimization
File-entry cache. To avoid frequent writes to the B-tree
leaves, we maintain a small cache of recent ﬁle-id to ﬁle-entry
mappings. Like the root node of the ﬁletable B-tree, the size
of this cache is proportional to the backend block size.
When the cache grows too large, the entries that belong
in the most common leaf node among all cache entries are
removed and written to that leaf node. This allows for efﬁcient
batching of leaf node updates and guarantees a signiﬁcant
fraction of the cache is cleared whenever a leaf node is written.
In fact, if the block size is large enough relative to the
number of ﬁles, the cache alone is sufﬁcient to store all ﬁle-
entries, and the B-tree effectively has height 0 with no leaf
nodes.
Superblock. Observe that every time any ﬁle is changed, its
ﬁle-entry must be updated in the cache, which possibly also
causes a leaf node to be written, which in turn (due to the
oblivious shufﬂing of block-ids on writes) requires the root
node of the ﬁletable to be re-written as well.
Since the root node of the ﬁletable B-tree and the ﬁle-
entry cache are both changed on nearly every operation, these
are stored specially in a single designated backend block pair
called the superblock that is written on every operation and
never changes location. Because this ﬁle is written every time,
it is not necessary to shufﬂe its location obliviously.
5
The superblock also contains a small (ﬁxed) amount of
metadata for the ﬁlesystem parameters such as the block size
and total number of backend block pairs. As mentioned above,
the size of both the B-tree root node and the ﬁle-entry cache
are set proportionally to the backend block size, so that the
superblock in total has a ﬁxed size corresponding to one block