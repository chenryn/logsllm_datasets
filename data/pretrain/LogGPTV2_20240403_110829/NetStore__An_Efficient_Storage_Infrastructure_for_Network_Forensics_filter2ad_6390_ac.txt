Note that the compression selection model can accommodate any compression,
not only the ones mentioned in this paper, and is also valid in the cases when
the probability of processing the data in compressed format is 0.
3.4 Query Processing
Figure 3 illustrates NetStore data ﬂow, from network ﬂow record insertion to
the query result output. Data is written only once in bulk, and read many times
for processing. NetStore does not support transaction processing queries such
as record updates or deletes, it is suitable for analytical queries in general and
network forensics and monitoring queries in special.
Data Insertion. Network data is processed in several phases before being de-
livered to permanent storage. First, raw ﬂow data is collected from the network
sensors and is then preprocessed. Preprocessing includes the buﬀering and seg-
menting phases. Each ﬂow is identiﬁed by a ﬂow ID represented by the 5-tuple
[sourceIP, sourcePort, destIP, destPort, protocol]. In the buﬀering phase, raw
network ﬂow information is collected until the buﬀer is ﬁlled. The ﬂow records
in the buﬀer are aggregated and then sorted. As mentioned in Section 3.3, the
purpose of sorting is twofold: better compression and faster data access. All the
columns are sorted following the sorting order determined based on access prob-
abilities and correlation between columns using the ﬁrst sorted column as anchor.
288
P. Giura and N. Memon
In the segmenting phase, all the columns are partitioned into segments, that is,
once the number of ﬂow records reach the buﬀer capacity the column data in
the buﬀer is considered a full segment and is processed. Each of the segments is
then compressed using the appropriate compression method based on the data
it carries. The information about the compression method used and statistics
about the data is collected and stored in the index node associated with the
segment. Note that once the segments are created, the statistics collection and
compression of each segment is done independent of the rest of the segments in
the same column or in other columns. By doing so, the system takes advantage of
the increasing number of cores in a machine and provides good record insertion
rates in multi threaded environments.
After preprocessing all the data is sent to permanent storage. As monitoring
queries tend to access the most recent data, some data is also kept in memory
for a predeﬁned length of time. NetStore uses a small active window of size W
and all the requests from queries accessing the data in the time interval [NOW
- W, NOW] are served from memory, where NOW represents the actual time of
the query.
Query Execution. For ﬂexibility NetStore supports limited SQL syntax and
implements a basic set of segment operators related to the query types presented
in Section 3.1. Each SQL query statement is translated into a statement in terms
of the basic set of segment operators. Below we brieﬂy present each general
operator:
– ﬁlter segs (d1, d2): Returns the set with segment IDs of the segments that
overlap with the time interval [d1, d2]. This operator is used by all queries.
– ﬁlter atts(segIDs, pred1(att1), . . . , predk(attk)): Returns the list of pairs
(segID, pos list), where pos list represents the intersection of attribute po-
sition lists in the corresponding segment with id segID, for which the at-
tribute atti satisﬁes the predicate predi, with i = 1, . . . , k.
– aggregate (segIDs, pred1(att1), . . . , predk(attk)): Returns the result of ag-
gregating values of attribute attk by attk−1 by . . . att1 that satisfy their
corresponding predicates predk, . . . , pred1 in segments with ids in segIDs.
The aggregation can be summation, counting, min or max.
The queries considered in section 3.1 can all be expressed in terms of the above
operators. For example the query: “What is the number of unique hosts that each
of the hosts in the network contacted in the interval [d1, d2]?” can be expressed
as follows: aggregate(ﬁlter segs(d1, d2), sourceIP = 128.238.0.0/16, destIP ).
After the operator ﬁlter segs is applied, only the sourceIP and destIP seg-
ments that overlap with the time interval [d1, d2] are considered for process-
ing and their corresponding index nodes are read from disk. Since this is a
range aggregation query, all the considered segments will be loaded and pro-
cessed. If we consider the query “What is the number of unique hosts that host
X contacted in the interval [d1, d2]?” it can be expressed as follows: aggre-
gate(ﬁlter segs(d1, d2), sourceIP = X, destIP ). For this query the number
of relevant segments can be reduced even more by discarding the ones that do
NetStore: An Eﬃcient Storage Infrastructure
289
not overlap with the time interval [d1, d2], as well as the ones that don’t hold
the value X for sourceIP by checking corresponding index nodes statistics. If
the value X represents the IP address of an internal node, then the internal IPs
index will be used to retrieve all the positions where the value X occurs in the
sourceIP column. Then a count operation is performed of all the unique destIP
addresses corresponding to the positions. Note that by using internal IPs index,
the data of sourceIP column is not touched. The only information loaded in
memory is the positions list of IP X as well as the segments in column destIP
that correspond to those positions.
4 Evaluation
In this section we present an evaluation of NetStore. We designed and imple-
mented NetStore using the Java programming language on the FreeBSD 7.2-
RELEASE platform. For all the experiments we used a single machine with 6
GB DDR2 RAM, two Quad-Core 2.3 Ghz CPUs, 1TB SATA-300 32 MB Buﬀer
7200 rpm disk with a RAID-Z conﬁguration. We consider this machine represen-
tative of what a medium scale enterprise will use as a storage server for network
ﬂow records.
For experiments we used the network ﬂow data captured over a 24 hour period
of one weekday at our campus border router. The size of raw text ﬁle data was
about 8 GB, 62,397,593 network ﬂow records. For our experiments we considered
only 12 attributes for each network ﬂow record, that is only the ones that were
meaningful for the queries presented in this paper. Table 1 shows the attributes
used as well as the types and the size for each attribute. We compared NetStore’s
performance with two open source RDBMS, a row-store, PostgreSQL [13] and a
column-store, LucidDB [11]. We chose PostgreSQL over other open source sys-
tems because we intended to follow the example in [6] which uses it for similar
tasks. Additionally we intended to make use of the partial index support for in-
ternal IPs that other systems don’t oﬀer in order to compare the performance of
our inverted IPs index. We chose LucidDB as the column-store to compare with
as it is, to the best of our knowledge, the only stable open source column-store
that yields good performance for disk resident data and provides reasonable in-
sertion speed. We chose only data captured over one day, with size slightly larger
than the available memory, because we wanted to maintain reasonable running
times for the other systems that we compared NetStore to. These systems be-
come very slow for larger data sets and performance gap compared to NetStore
increases with the size of the data.
4.1 Parameters
Figure 6 shows the inﬂuence that the segment size has over the insertion rate.
We observe that the insertion rate drops with the increase of segment size. This
trend is expected and is caused by the delay in preprocessing phase, mostly
because of the larger segment array sorting. As Figure 7 shows, the segment
290
P. Giura and N. Memon
Table 1. NetStore ﬂow
attributes.
Table 2. NetStore properties and network rates supported
based on 24 hour ﬂow records data and the 12 attributes
Property
records insertion rate
number of records
number of bytes transported
bytes transported per record
Value
10,000
Unit
records/second
62,397,594
1.17
records
Terabytes
20,616.64
Bytes/record
bits rate supported
1.54
number of packets transported 2,028,392,356
packets transported per record
32.51
packets rate supported
Gbit/s
packets
packets/record
325,075.41 packets/second
Column Type Bytes
sourceIP int
int
destIP
sourcePort short
destPort
short
protocol byte
startTime short
endTime short
tcpSyns byte
tcpAcks byte
tcpFins
byte
byte
tcpRsts
int
numBytes
4
4
2
2
1
2
2
1
1
1
1
4
size also aﬀects the compression ratio of each segment, the larger the segment
size the larger the compression ratio achieved. But high compression ratio is
not a critical requirement. The size of the segments is more critically related
to the available memory, the desired insertion rate for the network and the
number of attributes used for each record. We set the insertion rate goal at
10,000 records/second, and for this goal we set a segment size of 2 million records
given the above hardware speciﬁcation and records sizes. Table 2 shows the
insertion performance of NetStore. The numbers presented are computed based
on average bytes per record and average packets per record given the insertion
rate of 10,000 records/second. When installed on a machine with the above
speciﬁcation, NetStore can keep up with traﬃc rates up to 1.5 Gbit/s for the
current experimental implementation. For a constant memory size, this rate
decreases with the increase in segment size and the increase in the number of
attributes for each ﬂow record.
Fig. 6. Insertion rate for diﬀerent segment
sizes
Fig. 7. Compression ratio with and without
aggregation
NetStore: An Eﬃcient Storage Infrastructure
291
4.2 Queries
Having described the NetStore architecture and it’s design details, in this section
we consider the queries described in [5], but taking into account data collected
over the 24 hours for internal network 128.238.0.0/16. We consider both the
queries and methodology in [5] meaningful for how an investigator will perform
security analysis on network ﬂow data. We assume all the ﬂow attributes used
are inserted into a table f low and we use standard SQL to describe all our
examples.
Scanning. Scanning attack refers to the activity of sending a large number of
TCP SYN packets to a wide range of IP addresses. Based on the received an-
swer the attacker can determine if a particular vulnerable service is running on
the victim’s host. As such, we want to identify any TCP SYN scanning activity
initiated by an external hosts, with no TCP ACK or TCP FIN ﬂags set and
targeted against a large number of internal IP destinations, larger than a preset
limit. We use the following range aggregation query (Q1):
SELECT sourceIP, destPort, count(distinct destIP), startTime
FROM flow
WHERE sourceIP <> 128.238.0.0/16 AND destIP = 128.238.0.0/16
AND protocol = tcp AND tcpSyns = 1 AND tcpAcks = 0 AND tcpFins = 0
GROUP BY sourceIP
HAVING count(distinct destIP) > limit;
External IP address 61.139.105.163 was found scanning starting at time t1. We
check if there were any valid responses after time t1 from the internal hosts,
where no packet had the TCP RST ﬂag set, and we use the following query (Q2):
SELECT sourceIP, sourcePort, destIP
FROM flow
WHERE startTime > t1 AND sourceIP = 128.238.0.0/16
AND destIP = 61.139.105.163 AND protocol = tcp AND tcpRsts = 0;
Worm Infected Hosts. Internal host with the IP address 128.238.1.100 was
discovered to have been responded to a scanning initiated by a host infected
with the Conﬁcker worm and we want to check if the internal host is compro-
mised. Typically, after a host is infected, the worm copies itself into memory
and begins propagating to random IP addresses across a network by exploiting
the same vulnerability. The worm opens a random port and starts scanning ran-
dom IPs on port 445. We use the following query to check the internal host (Q3):
SELECT sourceIP, destPort, count(distinct destIP)