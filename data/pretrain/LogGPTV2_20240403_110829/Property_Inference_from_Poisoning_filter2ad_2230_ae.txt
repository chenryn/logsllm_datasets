to get the maximum accuracy. Note that in this experiment we also
attack MLP classiﬁer on Enron dataset for the random feature.
Fig. 3: Training set size v.s. attack accuracy. As was observed in
our theoretical analysis , the size of the training set can impact the
performance of our attack (because it affects the generalization error).
larger training sets would have smaller generalization error
and hence would be closer to a Bayes-optimal classiﬁer. In
Theorem 9 as the training set size increases, (n) and δ(n) will
decrease which makes the attack more successful. In fact, our
experiments verify this theoretical insight. In our experiments,
we vary the training set size from 100 to 1500 and the upward
trend is quite easy to observe. In this experiment we use 500
shadow models. Again, we have selected the poisoning rate
and the number of shadow models in a way that the attack does
not get accuracy 1.0 for small training sizes. In all experiments
in this ﬁgure, the number of shadow models is set to 200. Also,
the number of repetitions for this experiment is 2.
Relaxing the knowledge of distribution assumption: We
perform experiments where we do not give the adversary the
knowledge of the actual distribution (As descibed in Section V),
but a proxy distribution. Speciﬁcally, we run a modiﬁed version
of our attack against spam classiﬁcation, In this experiment,
the data for training the target model comes from ling-Spam 2
2https://www.kaggle.com/sohelranaccselab/lingspam-classiﬁcation
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
111130
t0\t1
.1
.2
.3
.4
.5
.6
.7
.8
.9
.3
.4
.2
.1
.5
50% 54% 67% 74% 86%
−
50% 57% 65% 84%
−
−
50% 68% 90%
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
.6
99%
98%
95%
50% 69% 84%%
−
57%
−
50%
−
−
−
−
−
−
50%
−
−
−
−
.7
.8
.9
100% 100% 100%
100% 100% 100%
99%
98%
91%
85%
62%
88%
59%
78%
50%
60%
−
55%
−
50%
97%
88%
71%
60%
60%
50%
−
Layers
1
2
3
4
5
5
5
Architecture
Layer sizes
[2]
[4 2]
[8 4 2]
[16 8 4 2]
[32 16 8 4 2]
[32 16 8 4 2]
[32 16 8 4 2]
Train Size
1000
1000
1000
1000
1000
1000
10000
Acc.
1.0
0.97
0.94
0.88
0.81
0.83
0.92
Performance
Shadow Models
600
600
600
600
600
1000
1000
TABLE IV: The success of attack when the ratios used to train the
attack is different than actual ratios. The attack is trained against
Logistic Regression with training set size 1000, 1000 shadow models,
poisoning ratio 0.1. The distribution used here is Enron data and the
target feature is "random".
In the shadow model training phase, we select random ratios
t uniformly at random from the set {1%, 2%, 3%, . . . , 100%}.
Then, we label the shadow models with these ratios. Then we
train a linear regressor (Ridge regressor) that tries to predicts
the average of target feature based on the result of the queries.
Our experiments suggest that the attack is able to ﬁnd the
average of target feature with absolute error of < 5% when
the target features average is selected uniformly at random in
[0, 1]. To train this attack, we trained 20000 shadow models
with training set size of 2000 on the Enron dataset. The target
of the attack is to ﬁnd out the average of a random feature that
is not correlated with any other feature. Note that the absolute
error of .25 means that the regressor is not doing anything as
a trivial regressor that always outputs .5 will achieve the same
average absolute error.
Average Absolute Error
0.254%
0.066%
0.046%
poisoning ratio
0%
5%
10%
TABLE VI: Complexity of target models vs attack accuracy.
F. Experiments on Resnet
We also studied the effect of our attack on the Resnet-18
and Resnet-50 Architectures for smile detection on CelebA
dataset. Table VII shows the performance of the attack on
CelebA dataset. As the experiments suggest, the white-box
attack does not perform well for these architectures. We believe
there are two reasons for this (1) The architecture is large with
lots of parameters and the attack cannot ﬁnd the right patterns
with only 500 shadow models. (2) The sorting and set-based
techniques used in [12] cannot be used for these architectures
because it is not a fully connected neural network.
In these experiments, the goal of the adversary is to infer
the ratio of Males in the training set that is either 30% or 70%.
We train 500 shadow models trained on the datasets of size
10000. We use pytorch library to train our models. We train
each model for 15 epoch with exponential decaying learning
rate schedule with starting learning rate of 0.001 and decay
rate of 0.5 that is applied at the end of every third epoch. We
use a batch size of 500.
Architecture
-
Resnet-18
Resnet-50
White-box
58%
52%
Performance
0% poison
5% poison
73%
64%
92%
87%
10% poison
97%
89%
TABLE V: The success of the regression attack.
Complexity of Target Models While in most of our experi-
ments we ﬁx the target model to be logistic regression (except
for few experiments named Census MLP and Enron MLP in the
ﬁgures), here we experiment with more complex architectures
to see how our attack performs. We summarize the results in
Table VI. Based on our theoretical analysis, the effectiveness
of the attack should depend on the model’s performance in
generalization to the training data distribution. Therefore, we
expect the effectiveness of the attack to drop with more complex
networks as the generalization error would increase when the
number of parameters in the model increases. This might sound
counter intuitive as the privacy problems are usually tied with
over ﬁtting and unintended memorization[7]. However, our
experiments verify our theoretical intuition. We observe that as
we add we more layers, the accuracy of the attack tends to drop.
However, we would expect this to change with larger training
set sizes as the larger training size could compensate for the
generalization error caused by higher numbers of parameters
and overﬁtting. For instance, in the last row of Table VI the
accuracy increases signiﬁcantly when we set the training size
to 10000 and use more shadow models.
TABLE VII: Experiments with Resnet Architecture and comparison
with white-box baseline.
G. Comparison with WBAttack [12]
Since the work closest to ours is WBAttack, even though it is
a white-box attack, we experimentally compare the performance
of WBAttack to ours. WBAttack is an improved version of
property inference attack of [11], where instead of a simple
white-box shadow model training on the parameters of neural
networks (which is done in [2]), they ﬁrst try to decrease
the entropy of the model parameters by sorting the neural
network neurons according to the size of their input and output.
They show that this reduction in randomness of the shadow
models can increase the accuracy of attack, for the same
number of shadow models. This attack is called the vector
attack in [12]. In Table VIII, we see how our black-box attack
performance compares with WBAttack. Notice that black-
box with no poisoning (ﬁrst 3 rows of the table) performs
much worse that WBAttack on race and gender. However,
WBAttack performs poorly on the random feature. In fact, the
strength of our attack is to ﬁnd a way to infer information about
features similar to random that do not have high correlation
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
121131
Feature
C-Random
C-Gender
C-Race
C-Random
C-Gender
C-Race
C-Random
C-Gender
C-Race
M-Jitter
Experiment parameters White-box[12]
Acc.
.52
.96
.95
.52
.96
.95
.52
.96
.95
.85
1.0
TS
1000
1000
1000
1000
1000
1000
1000
1000
1000
10000
10000
SM
1000
1000
1000
1000
1000
1000
1000
1000
1000
4096
4096
Cel-Gender
Black-box
Poison
0
0
0
0.05
0.03
0.05
0.1
0.1
0.1
0.1
0.1
# SM
1000
1000
1000
100
100
100
50
50
50
1000
1000
Acc.
.5
.61
.55
1.0
.99
.97
1.0
1.0
.98
0.94
0.91
TABLE VIII: Comparison with the white-box attack of [12]. Here
C- denotes Census, M- MNIST, Cel- CelebA; TS denotes size of
Training Size, SM denotes number of Shadow Models. The ﬁrst 9
rows compare our attack with the sorting-based white box attack of