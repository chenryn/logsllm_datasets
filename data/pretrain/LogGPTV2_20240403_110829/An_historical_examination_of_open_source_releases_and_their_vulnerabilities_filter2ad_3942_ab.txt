### 3.2 Postfix and Sendmail Analysis

The vertical axis range and scaling of the CVE/yr metric in the figures differ significantly from those shown in Figure 1. Full analysis details are provided in Appendix A, Table 6. Over the analyzed releases, Postfix's executable lines of code increased from 10,472 to 32,470. Postfix has maintained an excellent security record with only 12 CVE entries (see Appendix A, Table 4). This small number makes it challenging to draw definitive conclusions about the increase in issues reported by SCA compared to the number of CVE entries. However, the rise from 127 to 394 total issues does appear to correlate with 6 of the 12 CVE entries occurring in the last four years up to 2011.

Figures and tables highlight a striking difference in metrics between Sendmail and Postfix. For Postfix, the "Total issues" metric ranges from 127 to 394, while for Sendmail, it ranges from 2548 to 548 (841 for the most recent release analyzed). The "T-density," or the number of issues per 100 lines of code, varied from 0.96 to 1.33 for Postfix and from 2.55 to 21.48 for Sendmail. This suggests a fundamentally different approach taken by Venema in the initial development of Postfix, possibly learning from nearly two decades of Sendmail experience.

One key difference is the use of known "dangerous" C functions, which are generally considered risky due to their lack of bounds checking or susceptibility to subversion, leading to potential memory overwrites. Examples include `strcpy`, `strcat`, `memset`, `memcpy`, and `printf`. Figure 3 illustrates the density of dangerous function calls per 100 lines of code in various releases of Sendmail and Postfix, confirming that Postfix initially made much less use of these functions. In recent releases, the usage is very similar.

The significant differences in SCA analyses for earlier Sendmail releases are mirrored by the greater number of CVE entries. However, more recent Sendmail versions have had very few issues, indicating improved quality.

### 3.3 Apache httpd Analysis

Detailed analyses of the 1.3, 2.0, and 2.2 Apache httpd series are provided in Appendix A, Table 7. Figure 4 summarizes the results for the 1.3 series from 1998 to 2010. During this period, the number of executable lines of code increased from 11,079 to 14,201, with the highest count at 17,099 in version 1.3.19. The T-density varied from 2.18 to 3.63. The figure shows an initial rise in SCA metrics followed by a rise in CVE entries per year. From release 1.3.32, despite no drop in SCA metrics, there was a drop in CVE/yr, consistent with fewer changes to mature code where most serious bugs have been discovered. This is also reflected in the minimal increase of 83 lines of code from 1.3.32 to 1.3.42.

CVE information is available for release 1.3.2 and later. Release 1.3.0 (June 1, 1998) has no CVE data until 1999. Release 1.3.2 (September 21, 1998) is the first with CVE information.

Figure 5 shows the results for the 2.0 series from 2002 to 2010. The earliest available release, 2.0.35 (April 6, 2002), could not be compiled, so our analysis begins with 2.0.43 (October 3, 2002). The number of executable lines of code increased from 23,982 to 25,720. There was a modest reduction in some SCA metrics like Total issues and T-density, while others, such as Critical issues and C-density, remained largely unchanged. From release 2.0.52, there is a reduction in CVE/yr, consistent with maturing code.

Figure 6 shows the results for the 2.2 series from 2005 to 2011. The lines of executable code increased from 28,057 to 30,655, with little variation in SCA metrics and no significant reduction in CVE/yr.

### 3.4 OpenSSL Analysis

Full details of our analyses of the OpenSSL 0.9.6, 0.9.7, 0.9.8, and 1.0.0 series are given in Appendix A, Table 8. Figure 7 summarizes the results for the 0.9.6 series from 2000 to 2004. The number of executable lines of code increased from 44,396 to 45,173, with most SCA metrics showing a slight increase. Critical issues and C-density showed a slight drop. The low CVE/yr for the first release, 0.9.6, may be due to the software being new and receiving little attention [7].

Figure 8 shows the results for the 0.9.7 series from 2002 to 2007. The number of executable lines of code increased from 56,216 to 59,064. Notably, for release 0.9.7.e, SCA detected zero critical issues, and there was a corresponding drop in CVE/yr. Generally, there was little change in many SCA metrics, with a downward trend in C-density matching the reduction in CVE/yr.

Figure 9 shows the results for the 0.9.8 series from 2005 to 2011. The number of executable lines of code increased from 71,529 to 75,324, with little change in most SCA metrics. The most noticeable change was the rise in critical issues and C-density, correlating with a slight rise in CVE/yr.

Figure 10 shows the results for the 1.0.0 series from 2009 to 2011. The number of executable lines of code increased from 87,987 to 88,603, with very little change in any SCA metrics or CVE/yr.

### 3.5 Statistical Analysis

Can we show a statistically significant correlation between SCA-generated metrics and CVE/yr? The sample size for each dataset (Sendmail, Postfix, three Apache httpd releases, and four OpenSSL releases) is too small to demonstrate a statistically significant correlation. The largest dataset, Apache httpd 1.3, has 13 samples, but typically 50 to 100 samples are required. Combining all data yields 75 samples. We performed two correlation analyses: one on the combined unnormalized dataset and another on the combined normalized dataset.

The unnormalized dataset uses absolute values of metrics, with the mean calculated across all datasets. This assesses the relationship between absolute metric values and CVE/yr. For example, does a high value of "Total issues" or "T-density" suggest a high value for CVE/yr?

We normalized each dataset by dividing the values for CVE/yr and each SCA metric by the mean values for that dataset. Each release series (Apache httpd 1.3, 2.0, 2.2; OpenSSL 0.9.6, 0.9.7, 0.9.8, 1.0.0) was treated separately and normalized with means for that series. We then performed a correlation analysis on the combined normalized dataset of 75 samples. This form of normalization focuses on whether changes in metric values relative to the mean can explain changes in CVE/yr.

Table 1 and Table 2 summarize the correlation calculations. The columns represent Pearsonâ€™s Correlation Coefficient (CC), t-value, Significance Level (SL), and Coefficient of Determination (CD).

- **Table 1: SCA Metrics - CVE/yr Correlation**
  - **Metric**: Total issues, T-density, Critical issues, C-density, Critical + high issues, CH-Density
  - **CC**: 0.211, 0.346, 0.305, 0.232, 0.124, 0.324
  - **t-value**: 1.84, 3.15, 2.73, 2.03, 1.07, 2.92
  - **SL**: 90, 99, 99, 95, 99, 99
  - **CD**: 0.044, 0.120, 0.093, 0.054, 0.015, 0.105

- **Table 2: Normalized Metrics - CVE/yr Correlation**
  - **Metric**: Total issues, T-density, Critical issues, C-density, Critical + high issues, CH-Density
  - **CC**: 0.565, 0.559, 0.326, 0.313, 0.495, 0.559
  - **t-value**: 5.85, 5.76, 2.95, 2.82, 4.86, 5.76
  - **SL**: 99, 99, 99, 99, 99, 99
  - **CD**: 0.319, 0.313, 0.107, 0.098, 0.245, 0.312

The tables show a moderate correlation for the normalized SCA metrics of "Total issues," "T-density," and "CH-density," which are significant at the 99% level and explain over 30% of the variance in CVE/yr. A large increase in T-density for a new release compared to a previous release is indicative of an increase in CVE/yr. The moderate rather than strong correlation is due to the presence of other factors.

The correlation for unnormalized metrics is weak, with the best being T-density (CC: 0.346, CD: 12%). This suggests that absolute values are less important than changes in measured metrics relative to the mean for the series.

### 3.6 The Effect of Time

We did not explicitly explore the effect of time on CVE/yr, but the release samples are generally spaced by approximately one year. The pattern of CVE/yr in Figures 1-10 is similar if time is used as the x-axis instead of release version. Excluding Sendmail, the graphs suggest a non-linear relationship: an initial rise in CVE/yr in the first three to five years of a release series, followed by a reduction. Figure 5 for Apache 2.0 is consistent with this trend, with the first release in March 2000 and the first analyzable release, 2.0.43, in October 2002. Release 2.0.55 was a few months after the fifth anniversary of the initial release.

### 4. Comparing Different Software

In Section 3.5, we established a weak correlation between the absolute values of SCA metrics and CVE/yr, making it dangerous to compare different software using these metrics. Consider Figures 11 and 12, which show "Total issues" and "T-density" for Sendmail, Postfix, and the Apache httpd-2.0 series. From 2002 to 2011, one might expect more security bugs and CVE entries for Sendmail than Apache httpd or Postfix, given the generally higher metric values. However, Figure 13 shows significantly more CVE entries for Apache httpd 2.0 than for Sendmail or Postfix.

The correlation coefficients for absolute values of SCA metrics with CVE/yr and the qualitative example indicate that even a factor of 2 difference in absolute values does not necessarily indicate a corresponding difference in CVE/yr. However, there is an order of magnitude (10x or 20x) difference in metrics between the earliest releases of Sendmail and Postfix/httpd 2.0, which seems to relate to the larger number of CVE/yr for Sendmail during this period. Unfortunately, the dataset is small, providing only weak evidence.

### 5. Degree of Scrutiny

For our analysis, we chose software expected to have a high degree of scrutiny, such as Sendmail, Postfix, Apache httpd, and OpenSSL, which provide accessible services to the Internet. To illustrate varying degrees of scrutiny, we examined two open-source databases (names redacted) and identified the reporters of the most recent CVE entries using the SecurityFocus database [24]. The results are summarized in Table 3.

- **Table 3: Security Bug Reporters**
  - **Software**: Apache httpd, OpenSSL, Database-1, Database-2
  - **Tot (Total)**: 24, 26, 26, 32
  - **NA (No Attribution)**: 191, 173, 64, 105
  - **Devs (Developers)**: 1, 5, 4, 1
  - **Ext (External)**: 3, 32, 15, 19
  - **Unique Ext (Unique External)**: 20, 21, 7, 12

This clearly demonstrates that developers of the databases report the majority of security bugs (15 out of 26 for Database-1 and 19 out of 32 for Database-2), in contrast to Apache httpd and OpenSSL, where external individuals and groups report the majority. This supports the hypothesis that Apache httpd and OpenSSL are subject to greater scrutiny. The relative lack of scrutiny for the databases may mean that only a small percentage of vulnerabilities are reported and fixed.

### 6. Related Work

Several studies have attempted to build "Vulnerability Discovery Models" by analyzing vulnerability reporting data and predicting future vulnerabilities [1], [10], [18], [21]. Ozment [19] provides a survey of these approaches and their shortcomings, including accounting for the skill and numbers of security researchers, the discovery of new vulnerability classes, and the dependency of discoveries. Using an automatic tool, we are not dependent on researcher skill, but we need to update the tool's knowledge base for new vulnerability classes.

Rescorla [21] reports that software quality, as measured by defect discovery rate, does not improve over time. However, Ozment and Schechter [20] and Doyle and Walden [8] demonstrated improvements in software quality over time. Our analysis shows that software quality, as measured by SCA metrics or CVE/yr, does not always improve with each new release. Generally, CVE/yr begins to drop three to five years after the initial release. Factors extrinsic to the software, such as the degree of scrutiny, can affect the rate and number of vulnerabilities discovered.

Clark and colleagues [7] show that familiarity with the code can affect the rate of discovery, implying that reused code may have vulnerabilities discovered sooner. Okun and colleagues [17] investigated the impact of static analysis tools on project security but did not examine their predictive capabilities. Nagappan and Ball [13] studied the use of two static analysis tools to predict defect density in Windows Server 2003 components, reporting a statistically significant correlation coefficient of 0.577, aligning well with our results. Ayewah and colleagues [3] found that it is difficult for static analysis tools to distinguish trivial bugs and false positives from serious bugs, which may also contribute to the relative weakness in the correlation between SCA metrics and CVE/yr.