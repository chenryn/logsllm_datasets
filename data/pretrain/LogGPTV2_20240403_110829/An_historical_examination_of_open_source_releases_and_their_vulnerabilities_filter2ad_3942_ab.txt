the range of the vertical axis and scaling of the CVE/yr met-
3We use the lines of code reported by SCA; other tools may
generate slightly diﬀerent counts.
186Figure 3: Sendmail & Postﬁx dangerous function
density
Figure 4: Apache httpd 1.3 issues & CVE entries
ric is very diﬀerent to that shown in ﬁgure 1. Full analysis
details are given in appendix A table 6. Over the course of
the analyzed releases Postﬁx evolved from 10,472 executable
lines of code to 32,470. By most standards Postﬁx has had
an excellent record with relatively few CVE entries over the
years: only 12 (see appendix A table 4). Such a small num-
ber makes it diﬃcult to draw conclusions about the increase
in number of issues reported by SCA compared to the num-
ber of CVE entries. Although the rise from 127 to 394 total
issues does seem to have some relation to 6 of the 12 entries
having occurred in the last 4 years to end of 2011.
The ﬁgures and tables show a striking diﬀerence in the
metrics for Sendmail compared to Postﬁx. For example, for
Postﬁx releases the “Total issues” metric ranges from 127 to
394, whereas for Sendmail releases the metric ranges from
2548 to 548 (841 for the most recent release analyzed). For
“T-density”, the number of issues per 100 lines of code, the
range for all Postﬁx releases varied from 0.96 to 1.33 and
for Sendmail it varied from 2.55 to 21.48. We believe this
demonstrates a fundamentally diﬀerent approach adopted
by Venema in the initial development of Postﬁx compared
to that used by Sendmail, perhaps learning from nearly two
decades of Sendmail experience. One simple measure is the
diﬀerence in use of known “dangerous” C functions by the
two programs. These functions are generally accepted to
be “dangerous” because they don’t perform bounds checks
or the bounds checking can be subverted so that can be
used in exploits to overwrite arbitrary areas of memory. Ex-
amples include strcpy, strcat, memset, memcpy and printf.
Figure 3 shows the density of dangerous functions used in
the various releases of Sendmail and Postﬁx which we an-
alyzed (the units are number of calls per 100 Lines). This
appears to conﬁrm that a fundamentally diﬀerent approach
was taken for the implementation of the initial versions of
Postﬁx. Historically, it made much less use of these danger-
ous functions. For the most recent releases of Sendmail and
Postﬁx the usage is very similar.
The large diﬀerences we see in the SCA analyses for earlier
releases of Sendmail are matched by the much greater num-
ber of CVE entries for Sendmail. However, more recently
Sendmail has had very few issues indeed, indicating much
improved quality.
3.3 Apache httpd
Full details of our analyses of the 1.3, 2.0 and 2.2 Apache
httpd series are given in appendix A table 7. Figure 4 sum-
Figure 5: Apache httpd 2.0 issues & CVE entries
marizes the results for the 1.3 series from 1998 to 2010. Dur-
ing this period the number of lines of executable code in-
creased from 11,079 to 14,201 (with 1.3.19 having the most
at 17099), and the T-density varied from 2.18 to 3.63. The
pattern shown by the ﬁgure is that an initial rise in SCA
measure metrics being matched by a rise in CVE entries per
year. From release 1.3.32, although there is no drop in SCA
measured metrics, there is a drop in the CVE/yr which ap-
pears to be consistent with relatively few changes to mature
code in which most of the serious bugs have been discovered.
This is also reﬂected in the number of lines of executable
code which increased by only 83 lines from 1.3.32 to 1.3.42.
CVE information is shown for release 1.3.2 and later. 1.3.0
was released June 1, 1998 but no CVE information is avail-
able until 1999. The release of 1.3.2 was on September 21,
1998 and 1.3.11 was January 1, 2000. So release 1.3.2 is the
ﬁrst release for which we show CVE information.
The results of the analysis for the 2.0 series from 2002 to
2010 is shown in ﬁgure 5. 2.0.35 (April 6, 2002) is the earli-
est release of the 2.0 series available in the Apache archives,
but we had system compatibility problems and so we were
unable to compile it - a prerequisite to using SCA. Our anal-
ysis therefore begins with 2.0.43 (October 3, 2002). During
this series the number of lines of executable code increased
from 23,982 lines of executable code to 25,720. There was
a modest reduction in some of the SCA measured metrics
such as Total issues and T-density. Other metrics remained
largely unchanged: Critical issues and C-density. From re-
lease 2.0.52 there is a reduction in CVE/yr consistent with
maturing code.
The results of the analysis for the 2.2 series from 2005 to
187Figure 6: Apache httpd 2.2 issues & CVE entries
Figure 8: OpenSSL 0.9.7 issues & CVE entries
Figure 7: OpenSSL 0.9.6 issues & CVE entries
Figure 9: OpenSSL 0.9.8 issues & CVE entries
2011 are shown in ﬁgure 6. During this series the lines of
executable code increased from 28,057 to 30,655. The SCA
measured metrics vary little through this series. There is
also no signiﬁcant reduction in CVE/yr with later releases
in the series.
3.4 OpenSSL
Full details are of our analyses of the OpenSSL 0.9.6, 0.9.7,
0.9.8 and 1.0.0 series are given in appendix A table 8. Fig-
ure 7 summarizes the results for the 0.9.6 series from 2000 to
2004. During this series of releases the number of lines of ex-
ecutable code increased from 44,396 to 45,173. Most of the
SCA measure metrics show a slight increase. The exception
is Critical issues and C-density which show a slight drop.
The comparatively low CVE/yr for the ﬁrst release, 0.9.6,
may be an artifact of the software being new and receiving
little attention [7].
Figure 8 shows the results of the analysis of the OpenSSL
0.9.7 series from 2002 to 2007. During this series of releases
the number of lines of executable code increased from 56,216
to 59,064. It is noticeable that for the 0.9.7.e release SCA
detected zero critical issues (hence C-density is also 0) and
there does seem to be a corresponding drop in CVE/yr for
that release. More generally there is little change in many
of the SCA measured metrics for this release series. The
downward trend exhibited by some such as C-density does
seem to be matched by the reduction in CVE/yr.
Figure 9 shows the results of the analysis of the OpenSSL
0.9.8 series from 2005 to 2011. During this series of re-
leases the number of lines of executable code increased from
71,529 to 75,324. Many of the SCA measured metrics show
little change across releases. The most noticeable change is
in the rise in number of critical issues and c-density which
again seems to show some correspondence to the slight rise
in CVE/yr.
Figure 10 shows the results of the analysis of the OpenSSL
1.0.0 release series from 2009 to 2011. During this release
series the number of lines of executable code increased from
87,987 to 88,603; there is very little change in any of the
SCA measured metrics or CVE/yr.
3.5 Statistical Analysis
Can we show a statistically signiﬁcant correlation between
metrics generated using SCA and CVE/yr? The sample size
for each of the separate nine datasets for Sendmail, Postﬁx,
the three releases of Apache httpd and the four releases of
OpenSSL is too small, in most cases, to show a statistically
signiﬁcant correlation. The largest number of samples we
have is 13 which is for the Apache httpd 1.3 dataset.4 Typ-
ically 50 to 100 samples is required to show a statistically
signiﬁcant correlation. If we combine all our analysis data
we have 75 samples. We performed two separate correla-
tion analyses of the SCA metrics and CVE/yr, one on the
combined unnormalized dataset the second on the combine
normalized dataset, as explained below.
The correlation analysis on the unnormalized datasets uses
the absolute values of the metrics for each dataset. For each
metric the mean used in the calculation is the mean across
all datasets. This tells use the strength of the relationship
between absolute values of a metric and CVE/yr. For exam-
41.3.0 is excluded from the correlation calculation as it was
replaced by 1.3.2 before CVE was established in 1999.
188Figure 10: OpenSSL 1.0.0 issues & CVE entries
Figure 11: Sendmail, Postﬁx & Apache httpd 2.0
Total issues
ple, does a high value of “Total issues” or “T-density” suggest
a high value for CVE/yr?
We normalized each dataset by dividing the values for
CVE/yr and each SCA measured metric by the mean values
for that dataset. Each release series of Apache httpd (1.3,
2.0, 2.2) and OpenSSL (0.9.6, 0.9.7, 0.9.8, 1.0.0) was treated
separately and normalized with means for that release series.
We performed a correlation analysis on the combined nor-
malized dataset of 75 samples. This form of normalization
is not concerned with absolute values. It is concerned with
determining whether or not a change in the value of a metric
with respect to the mean for that release series can explain
a corresponding change in CVE/yr.
Table 1: SCA metrics - CVE/yr correlation
CD
SL
0.044
90
0.120
99
0.093
99
95
0.054
NA 0.015
99
0.105
Metric
Total issues
T-density
Critical issues
C-density
Critcal + high issues
CH-Density
CC
0.211
0.346
0.305
0.232
0.124
0.324
t-value
1.84
3.15
2.73
2.03
1.07
2.92
Table 2: Normalized metrics - CVE/yr correlation
Metric
Total issues
T-density
Critical issues
C-density
Critcal + high issues
CH-Density
CC
0.565
0.559
0.326
0.313
0.495
0.559
t-value
5.85
5.76
2.95
2.82
4.86
5.76
SL
99
99
99
99
99
99
CD
0.319
0.313
0.107
0.098
0.245
0.312
Tables 1 and 2 summarize the results of the correlation
calculations. The meaning of the columns in the tables is
as follows. CC and t-value are the Pearson’s Correlation
Coeﬃcient and t-value for the metric with CVE/yr. SL is
the signiﬁcance level calculated from the t-value: 99 means
that the correlation is signiﬁcant at the 99% level; “NA”
means not acceptable — there is no signiﬁcant correlation.
CD is the Coeﬃcient of Determination which is give by the
square of the correlation coeﬃcient. This is the percentage
of variation in CVE/yr that is “explainable” by the metric:
0.044 is thus 4.4% and 0.319 is 31.9%.
The tables show a moderate correlation for the normalized
SCA metrics of “Total issues“, “T-density” and “CH-density”
which are signiﬁcant at the 99% level and explain over 30%
of the variance in CVE/yr for the software analyzed. Thus
a large increase in the T-density for a new release compared
to a previous release is indicative of an increase in CVE/yr.
A moderate, rather than strong correlation is artifact of the
presence of other factors also discussed in this paper.
The correlation for the unnormalized metrics is weak: the
best being T-density with a correlation coeﬃcient of 0.346
and a coeﬃcient of determination of 12%. This is consistent
with absolute values being less important than a change in
measured metrics with respect to the mean for the series.
The weak correlation for the absolute values of metrics to
CVE/yr, show that drawing conclusions about CVE/yr from
the absolute values of metrics measured from diﬀerent soft-
ware or release series is dangerous.
3.6 The effect of time
We did not explicitly set out to explore the eﬀect of time
on CVE/yr. However, with a few exceptions, the sam-
ples of releases are spaced by approximately a year (see
appendix A). So the pattern of CVE/yr shown in ﬁgures 1-
10 is similar if time is used as the x-axis instead of release
version. Setting aside Sendmail which has releases going
back well beyond our analysis and CVE records, the graphs
suggest a non-linear relationship: an initial rise in CVE/yr
the ﬁrst three to ﬁve years of a release series before a re-
duction. Note that ﬁgure 5 for Apache 2.0 is consistent
with this trend. The ﬁrst release of 2.0 occurred in March
2000 [2], but the ﬁrst release that we could analyze success-
fully, 2.0.43, was not until October 2002. Release 2.0.55 was
a few months after the ﬁfth anniversary of the initial release.
4. COMPARING DIFFERENT SOFTWARE
In section 3.5 we established that there was only a weak
correlation between the absolute values of SCA metrics and
CVE/yr. So using metrics to compare diﬀerent software
is dangerous.
In
particular can anything be said about orders of magnitude
diﬀerences?
In this section we explore this further.
Consider ﬁgures 11 and 12 which shows the “Total issues”
and “T-density” for Sendmail, Postﬁx and the Apache httpd-
2.0 series. If one looks at these graphs for the period 2002 to
2011 one might reasonably expect there to be more security
bugs discovered and hence more CVE entries for Sendmail
189Table 3: Security bug reporters
Apache httpd
OpenSSL
Database-1
Database-2
Tot NA Devs Ext Unique Ext
24
26
26
32
191
173
64
105
1
5
4
1
3
32
15
19
20
21
7
12
Figure 12: Sendmail, Postﬁx & Apache httpd 2.0
T-density
Figure 13: Sendmail, Postﬁx & Apache httpd 2.0
CVE entries
than Apache httpd or Postﬁx — there is generally a factor
2 or greater diﬀerence in the metrics. However, as is shown
by ﬁgure 13, there were signiﬁcantly more CVE entries for
Apache httpd 2.0 than either Sendmail or Postﬁx.
It is clear from the correlation coeﬃcients for absolute val-
ues of SCA metrics with CVE/yr and the above qualitative
example that even a factor 2 diﬀerence in absolute values
does not necessarily indicate a corresponding diﬀerence in
CVE/yr. However, there is an order of magnitude (10x or
20x) between the metrics measured for the earliest releases of
Sendmail and that of Postﬁx and httpd 2.0 which does seem
to show some relationship to the larger number of CVE/yr
for Sendmail during this period. Unfortunately, the dataset
is very small, so this is only weak evidence.
5. DEGREE OF SCRUTINY
For the above analysis we chose the software which we
expected to have a high degree of scrutiny. Sendmail, Post-
ﬁx, Apache httpd and OpenSSL present readily accessible
targets to attackers by providing accessible services to the
Internet. We expected this to motivate signiﬁcant numbers
of security researchers external to the development teams to
scrutinize the code for security ﬂaws. Not all open source
software is subject to the same level of scrutiny. To illustrate
this we looked at two open source databases whose names we
have chosen to redact and for the most recent CVE entries
used the SecurityFocus database [24] to identify the reporter
of the bug. The results are summarized in table 3. “Tot”
(Total) is the total number of CVE entries in the sample.
1One individual reported two bugs
2Developer team credited along with named individuals
3Two individuals reported three bugs each
4One individual reported two bugs
5Two individuals reported two bugs each
“NA” (No Attribution) is the number for which we could
not ﬁnd any attribution to who ﬁrst reported the vulnera-
bility. “Devs” (Developers) is the number of vulnerabilities
reported by the development team. “Ext” (External) is the
number of vulnerabilities reported by security researchers
who, so far as we could tell, are independent of the devel-
opment team. “Unique Ext” (Unique External) is the num-
ber of unique external security researchers (or teams) who
reported vulnerabilities — some reported more than one.
Note that the sum of the 3rd, 4th and 5th columns do not
equal the 2nd column in the case of OpenSSL as a named
individual as well as the developers are jointly credited for
3 vulnerabilities.
This clearly demonstrates that it is the developers of the
databases that are reporting the majority of the security
bugs (15 out of 26 for database-1 and 19 out of 32 for
database-2) in contrast to Apache httpd and OpenSSL where
individuals and groups outside the development team are re-
porting the majority of security bugs. This is consistent with
the hypothesis that Apache httpd and OpenSSL are subject
to a much greater degree of scrutiny than the databases. It is
possible that the relative lack of scrutiny of the databases by
the security community means that only a relatively small
percentage of the vulnerabilities in the code is being reported
and ﬁxed. However, since we cannot safely infer CVE/yr
from absolute values of SCA metrics we can neither conﬁrm
or refute this hypothesis.
6. RELATED WORK
There have been several attempts to build “Vulnerability
Discovery Models” by analyzing vulnerability reporting data
and using it to predict likely future vulnerabilities: [1], [10],
[18] and [21]. A survey of the approaches and shortcom-
ings is given in by Ozment in [19]. Major issues identiﬁed
by Ozment include accounting for the skill and numbers of
security searchers; the discovery of new classes of vulnerabil-
ities that can lead to temporary spikes in discovery; whether
or not discoveries are dependent or independent. By using
an automatic tool we are not dependent on the skill or oth-
erwise of security researchers. However, if a new class of
vulnerability is discovered, we do need to update the tool’s
knowledge-base to account for it.
In [21] Rescorla reports that software quality does not im-
prove overtime, as measured by the rate of defect discovery.
However, Ozment and Schechter studied releases of BSD
to show that software quality does improve over time [20].
Similarly Doyle and Walden demonstrated a drop in the pe-
riod 2006-2010 in the number of issues detected by static
190source code analysis in a set of PHP applications [8]. Our
analysis shows that software quality as measured by metrics
generated from SCA or CVE/yr does not always improve
with each new release: sometimes it is better; sometimes
it is worse; sometimes it is unchanged. Generally CVE/yr
begins to drop three to ﬁve years after the initial release.
Factors extrinsic to the software, such as the degree of
scrutiny, can have an aﬀect on the rate and number of vul-
nerabilities discovered. We did not set out to explore all
possible factors and there are others which we have not con-
sidered. For example, in [7] Clark and colleagues show that
the degree of familiarity can aﬀect the rate of discovery. If
the community is already familiar with the code, then a
vulnerability is likely to be discovered sooner. This has im-
plications for software reuse.
Okun and colleagues [17] have investigated whether or not
using static analysis tools on a project improves security.
However, they do not look at the predictive capabilities of
these tools which is the objective of our work. Nagappan
and Ball [13] studied the use of two static analysis tool to
predict defect density of Windows Server 2003 components.
For the more eﬀective of the two tools they report a statisti-
cally signiﬁcant correlation coeﬃcient of 0.577 which aligns
well with our own results. We have focused exclusively on
security vulnerabilities for open source software. Ayewah
and colleagues [3] investigate the classiﬁcation of issues by a
single static analysis tool. They conclude that it is diﬃcult
for the tool to distinguish trivial bugs and false positives
from serious bugs. We have discussed the reasons for this in
section 2: undecidability and the need for approximations.
This may also be a factor in why there can be a relatively