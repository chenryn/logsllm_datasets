declare the USE_FINGERPRINT permission and, therefore, can
potentially use the fingerprint API. In the rest of this section, we
will focus on this subset of 501 apps.
C. Apps Classification
Table III summarizes the outputs of our tool. We ran our tool
in a private cloud, and for the analysis of each app we provided
4 virtual-cores, 16 GB of RAM and 1 hour time limit. For the
501 apps, our tool needed on average 354 seconds (σ = 363) of
computation and used 6.13 GB (σ = 1.07) of RAM per app. In
5 cases (1.00%), our analysis did not finished due to bugs in the
SOOT framework or analysis timeout.
For 72 (14.37%) apps, although they ask for
the
USE_FINGERPRINT permission, our tool did not detect any usage
of the fingerprint API. This result is not particularly surprising
since previous research has shown that apps tend to require more
permissions than they use [48]. To further verify this finding, we
manually analyzed a random sample of 10 of these apps. We both
manually run them and perform tool-assisted reverse engineering.
For 7 of them, we could confirm that they do not use the fingerprint
API, whereas for the other 3 our tool was unable to detect its usage
because these apps use native code components to activate the
fingerprint reader sensor, which our tool is unable to analyze.
For apps classified as Weak we took a random sample of 20
apps among those in which we were able to dynamically reach
the fingerprint interface. Our dynamic analysis confirmed that
they were all correctly classified (i.e., our simulated attack in
Section VIII-A2 is successful). Among these 20 apps, 16 access a
remote account or store secret data, therefore a Weak usage of the
fingerprint API is not appropriate (as explained in Section VI-A).
For apps classified as Decryption we took a random sample of
10 apps and we confirmed that 9 were correctly classified (using,
again, the simulated attack explained in Section VIII-A2), whereas
1 was classified as Decryption while in reality is Weak.
Finally, about the 9 apps classified as Sign, we were able to
dynamically reach the fingerprint interface in one app and dynamic
analysis confirmed the classification of this app as correct. This
app, called “True Key,” requires to sign an authentication token
during login and performs this operation with a TEE-protected
private key, “unlocked” only when the user touches the fingerprint
reader sensor. To have a better evaluation, we also extensively
reverse engineer the other 8 samples classified as Sign. Our manual
analysis revealed that 7 of them have been classified correctly,
whereas 1 has been classified as Sign while being Decryption.
In summary, we manually analyzed (either by reproducing our
attacks as explained in Section VIII-A2 or by reverse engineering)
39 apps and we found that all the apps except 2 were classified
correctly. In one case the misclassification is due to overapproxi-
mations in the call graph. In the other, the app “signs” some data,
but this data is constant, since it is provided by the backend when
the user logins the first time. For this reason, the app falls into the
Decryption category. In fact, an attacker can trivially replay the
result of this signing operation after it happened once. However, our
tool was unable to detect this scenario and, therefore, it classified
the app as Sign.
Overall, results show how our tool is reasonably accurate
in determining how an app uses the fingerprint API. Moreover,
the few misclassifications “overestimate” the security of an app
(classifying it as using the fingerprint API in a stronger way than
in reality). Therefore, we believe that our results, showing a low
usage of the fingerprint API in the Sign way and a high usage in
the Weak way, are particularly worrisome and confirm our intuition
that apps generally do not use appropriately the fingerprint API.
In the next sections, we will provide concrete examples of these
inappropriate usages.
D. Case Study: Unlocking “Unlocked” Keys
As explained in Section II-C, a key is stored inside the TEE and
“unlocked” by a fingerprint touch only if the setUserAuthenti-
cationRequired method is invoked (by passing true for its
required parameter) when the key is generated. On the contrary,
without calling this method, a generated key is always “unlocked,”
regardless of the usage of the fingerprint API.
Surprisingly, we found this aspect as a source of implemen-
tation errors. In particular, we looked for apps implementing
proper cryptographic operations as a consequence of the user
touching the fingerprint sensor (i.e., calling the authenticate
API to “unlock” a key used to decrypt or sign some data), but
not calling properly the setUserAuthenticationRequired
method. This indicates that the developers wanted to have a key
“unlocked” when the legitimate user touches the fingerprint sensor,
but forgot to “lock” the key in the first place.
To identify these apps, we checked for apps that
1)
2)
3)
are classified as Weak by our tool;
do not call the setUserAuthenticationRequired
method (or they call it specifying false as its parame-
ter);
if they had called the setUserAuthenticationRe-
quired method properly they would have been classi-
fied as Decryption or Sign.
11
Our tool identified 15 apps in this scenario and we were able to
fully dynamically interact with 4 of them, verifying their improper
usage of the fingerprint API.
As an example, one of these applications allows a user to
purchase items in an online marketplace and requires the user
to touch the fingerprint sensor during the checkout procedure.
The user’s password is stored encrypted by a supposedly TEE-
secured key, as is common when the fingerprint API is used in a
Decryption way. During the checkout, when the user touches the
fingerprint sensor, this key is used to decrypt the user’s password.
However, we verified that the decryption key is not really “locked”
since the setUserAuthenticationRequired method is not
called. Therefore, from a cryptographic perspective, the use of the
fingerprint API is useless. As a consequence, a root attacker can
easily bypass its usage.
G. Case Study: Key Attestation
We mentioned in Section V-D that, starting from Android 7,
a new mechanism has been implemented to allow developers
to “attest” public keys, ensuring they have been generated from
“trusted” TEEs. According to the API, a properly verified certificate
chain, “rooted at a trustworthy CA key,” is only provided if the
setAttestationChallenge API, with a non-NULL value for
attestationChallenge, is called.
Conceptually, apps using both the fingerprint API in a Sign
way and key attestation should be categorized in a different group
in Table III. However, in our dataset, our tool found no app calling
this API. This indicates that every app in our dataset is vulnerable
to a Root-at-Bootstrap attacker, who can interfere with the initial
key exchange process between the app and its remote backend.
E. Case Study: Google Play Store
Among the apps our tool classified as Weak, one is the “Google
Play Store” app. This app is present on every Google-branded
phone, and it handles the purchase of apps, media, and in-app
purchases and can be setup to “protect” these purchases by a
fingerprint touch. In this case, the user would be required to
touch the sensor before every purchase. Since this app can directly
spend user’s money and interacts with a remote server, the most
appropriate usage of the fingerprint API would be Sign, as also
stated and exemplified in the guidelines from Google itself.
However, our tool classified the Google Play Store app as using
the fingerprint API in a Weak way and our evaluation (as described
in Section VIII-A) confirmed this result. In fact, this app calls
the authenticate API with a NULL value for its crypto
parameter, and, therefore, no key is “unlocked” and no sign
operation certifies that the purchase happened as a consequence of
the user touching the fingerprint reader sensor.
On July 2017, we contacted the Android’s security team. The
team promptly replied and forwarded our report to the Google
Play’s team, which is now aware of the issue and investigating it.
F. Case Study: Square Cash
Among the apps our tool classified as Decryption, one is the
“Square Cash” app. This app is a personal payment app, which
allows users to transfer money to and from connected debit cards
and bank accounts.
The app can be configured to require the user to touch the
fingerprint sensor before any transaction. The most appropriate
usage of the fingerprint API in this case would be to use it to
sign these transactions. However, Whorlwind, the open source
library that Square (and other apps in our dataset) uses to
implement the fingerprint functionality, implements a weaker
scheme. In particular, this library is used to decrypt a locally stored
authentication token. For this reason, by simulating an attacker
with Root capabilities, we were able to reuse the same decrypted
token to perform different payments.
We contacted the developers of the Whorlwind library in
August 2017, detailing our findings and why we think that a Sign
usage of the fingerprint API is more appropriate in this case.
IX. FINGERPRINT API IMPROVEMENTS
We will now propose some changes to the current fingerprint
API, which would significantly improve its security. In this section,
we will assume that apps use the fingerprint API in a Sign way,
which, as previously shown in Section V, it is the right way to
provide stronger security. However, even with proper usage, this
API currently has some shortcomings, which we will address here.
A. Trusted-UI
The biggest limitations of the current API and its implementa-
tion are:
1) Users have no trusted way to understand what they are
signing by touching the fingerprint sensor.
2) A malicious application (with or without “root” privi-
leges) can interfere with what is shown to the user when
asked to touch the sensor.
To solve both issues, we propose a mechanism in which the
TEE can directly show to the user the content of a sign operation
performed by a fingerprint-unlocked key. This mechanism is
based on the known idea of having a trusted video path directly
between the TEE and the device’s screen. TEE-enforced video
paths are already implemented in some Android devices (for DRM
purposes) [49] and academia explored its use for authentication
purposes [45]. However, differently from previous solutions, what
we propose is also based on a trusted input which is the fingerprint
reader sensor, able to directly communicate with the TEE.
We propose to change the current authenticate method
to also take as an input a message string parameter, for instance
“Do you want to authorize a payment of $1,000 to F riend?” This
message would be shown on a TEE-enforced Secure UI dialog
window, alongside with a standardized graphic UI asking the user
to touch the fingerprint sensor. Untrusted code, outside the TEE,
cannot interfere with the visualization of this window, due to the
usage of a secure video path. Specifically, untrusted code cannot
read the content of this dialog window nor modify it.
When the sensor is touched by a legitimate user, a signature of
this string (generated using the private key “unlocked,” specified
when the authenticate method is called) is available using a
method called getSignedMessage. The remote backend can
then verify that this message has been signed correctly and,
12
therefore, be sure of what the user has authorized by touching
the sensor. In other words, the remote backend can verify the “user
intention,” which is signed by the TEE.
The security of this system is guaranteed by the fact that both
the code for handling the sign operation and the code for visualizing
the message are within the TEE. Therefore, an attacker, even having
root privileges, cannot decouple what is being shown to the user
with what is being signed by the fingerprint-unlocked key. An
attacker can still interfere with the communication between the
backend, the app, and the TEE. However, this will be detectable
by the user. In fact, suppose that the attacker changes the request
the app sends to the backend from “Pay F riend $1,000” to “Pay
Attacker $1,000.” As a consequence the backend will send the
following message to be signed by the TEE: “Do you want to
authorize a payment of $1,000 to Attacker?”. In this case, the
user will be able to notice that the message does not correspond to
her intention.
Another issue is how to prevent an attacker from showing
a malicious dialog window that resembles the window shown
by the TEE when asking the user to touch the fingerprint sensor.
Without requiring extra hardware (e.g., an LED would be turned on
when “secure output” is displayed), we can exploit the fingerprint
sensor itself to mitigate this attack. Since the fingerprint sensor can
communicate directly and exclusively with the TEE, we propose
that the TEE shows a hard-to-spoof visual clue (e.g., a loading bar)
while the user touches the sensor.
Attackers would be unable to show this bar at the right time,
since, outside the TEE, it is unknown when the user touched the
sensor. Therefore, the absence (or the improper behavior) of this
visual element would indicate to the user that the shown dialog
window is not legitimate. Another possible solution, although less
practical since it requires a setup phase, would be to use a secret
(i.e., only know by the user and the TEE) personalized security
indicator. This mechanism has been shown as an effective defensive
mechanism in the Android ecosystem [9].
It is important to notice, however, that even without this defense,
an attacker would not be able to lure users to sign a malicious
transaction, but only to pretend that a transaction happened.
B. Other UI Changes
While a solution based on hardware-enforced secure-UI is the
best way to address current API shortcomings, we understand
that its adoption and deployment may be problematic because
it requires non-trivial modifications to the code running inside
the TEE and the coordination between this code, the Android
operating system, and the display hardware. Therefore, we also
propose easier-to-implement modifications to the current Android
user-level framework. While attackers having “root” privileges can
trivially bypass these mechanisms, they are still effective against a
non-root attacker.
In particular, Android should automatically dismiss overlay
windows on top of interfaces asking the user to touch the fingerprint
sensor. A similar solution is already applied in the latest Android
versions to protect “security sensitive” interfaces, such as the one
used to grant/remove apps’ permissions. In addition, the name
(and the icon) of the app asking the user’s touch should be clearly
shown. To implement both solutions, a standard interface, which
apps cannot modify except showing some text on it, should be
shown when the authenticate API is called. In the current
implementation, custom interfaces are possible, but uncommon.
In fact, most of the apps show very similar interfaces (Android
guidelines precisely define how this dialog should appear [29]),
thus they will not need to significantly change their UI.
C. Better Attestation Mechanisms
As we previously mentioned, a key attestation mechanism has
been implemented, starting from Android 7. However, in its current
implementation state, this mechanism has several weaknesses.
First of all, the API defines two possible “levels” for the
attestation “software” and “hardware,” where only the latter
guarantees that a key has been generated by the device’s TEE.
The level of attestation can be retrieved by parsing the attestation
certificate associated with a generated public key. However, in the
devices we have tried (Nexus 5X and Pixel XL, running Android 7),
the generated keys are always “software” attested.
More fundamentally, while analyzing the generated certificates,
we did not find any indication of the specific instance of the device
generating a key. As also pointed out by the paper presenting
the Security Key protocol [32], there is a trade-off between
user’s privacy and security of the protocol. Having a system that
can identify the specific device generating a key would allow
remote backends to detect suspicious situations in which the key
associated with a specific user changes. Moreover, it would hinder
the ability of an attacker to “proxy” key creation to an attacker-
controlled TEE, since too many keys (used by many different
users) generated by the same device would be easily detected as
suspicious. However, this would violate user’s privacy, allowing
unique user’s identification among different apps. Therefore, we
recommend, as in the Security Key protocol, the implementation
of a batch attestation scheme, in which a set of devices, using
the same hardware (and potentially affected by the same security
issues), shares the same attestation key.
Finally, we note that the current documentation about how to
verify key attestation certificates is insufficient and the only official