Figure 5 1:
1. The malfunctioning NIC of server 0 continually
sends pause frames to its ToR switch;
2. The ToR switch in turn pauses all the rest ports in-
cluding all the upstream ports to the Leaf switches;
1The scenario involves a malfunctioning NIC.
207
3. The Leaf switches pause the Spine switches;
4. The Spine switches pause the rest of the Leaf switches;
5. The rest of the Leaf switches pause their ToR switches;
6. The ToR switches pause the servers that connect
to them.
In this case, a single malfunctioning NIC may block
the entire network from transmitting. We call this NIC
PFC pause frame storm, or PFC storm for abbrevia-
tion. To our surprise, we observed PFC storms in our
networks multiple times and PFC storms caused several
incidents which we will describe in Section 6.
The root-cause of the PFC storm problem is a bug in
the NIC’s receiving pipeline. The bug stopped the NIC
from handling the packets it received. As a result, the
NIC’s receiving buﬀer ﬁlled, and the NIC began to send
out pause frames all the time.
We have worked with the NIC provider to ﬁx this NIC
bug. Furthermore, to prevent PFC storms from hurting
the network, we have implemented two watchdogs at
both the NIC and the ToR switches as follows.
On the NIC side, we worked with the NIC provider to
build a PFC storm prevention watchdog. This is pos-
sible because the NIC has a separate micro-controller
which can be used to monitor the NIC receiving side
pipeline. Once the NIC micro-controller detects the re-
ceiving pipeline has been stopped for a period of time
(default to 100ms) and the NIC is generating the pause
frames, the micro-controller will disable the NIC from
generating pause frames. Our experience with PFC
storm is that once the NIC enters the storm mode, the
server is disconnected from the network since the NIC
is not functioning well anymore. The NIC watchdog is
not able to rescue the server. Instead, its goal is to pre-
vent the pause frame storms from hurting the rest of
the network.
On the ToR switch side, we worked with the switch
providers to build a switch watchdog to monitor the
server facing ports. Once a server facing egress port
is queuing packets which cannot be drained, and at the
same time, the port is receiving continuous pause frames
from the NIC, the switch will disable the lossless mode
for the port and discard the lossless packets to and from
the NIC. Similar to the NIC side watchdog, it is to
prevent pause frames from the malfunctioning NIC from
propagating into the network. Once the switch detects
that the pause frames from the NIC disappear for a
period of time (default to 200ms), it will re-enable the
lossless mode for the port.
These two watchdogs are complementary to each other.
One of them should be suﬃcient to stop the NIC PFC
storm. We have implemented both for double insurance.
Note that there is a small diﬀerence in the two watch-
dog implementations. The switch watchdog will re-
enable the lossless mode once pause frames are gone,
whereas the NIC watchdog does not re-enable the loss-
less mode. This is because we have observed once the
NIC enters the PFC storm mode, it never comes back.
Hence re-enabling the lossless mode is not needed for
the NIC.
We also have observed that the NIC PFC storm prob-
lem typically can be ﬁxed by a server reboot. Hence
once the NIC is not functioning, our server manage-
ment system will try to repair (reboot, reimage etc.)
the server. Repairing takes tens of minutes. The NIC
watchdog is to limit the damage of the problematic NIC
to hundreds of milliseconds before server repair kicks
in. Once the server is repaired successfully and pause
frames from the servers are gone, the switch can re-
enable the lossless mode for the corresponding switch
port automatically.
Knowledgable readers may wonder about the interac-
tions between the two watchdogs. Once the NIC watch-
dog disables the NIC pause frames, the switch watchdog
will re-enable the lossless mode for the corresponding
switch port. The packets to the NIC will either dropped
by the switch (if the MAC address of the NIC times out)
or dropped by the NIC (since the NIC receiving pipeline
is not functioning). In either case, the NIC PFC storm
cannot cause damage to the whole network.
We recommend both switches and NICs should im-
plement the watchdogs for NIC PFC storm prevention.
4.4 The Slow-receiver symptom
In our data centers, a server NIC is connected to a
ToR switch using a point-to-point cable. The NIC is
connected to the CPU and memory systems via PCIe.
For a 40 GbE NIC, it uses PCIe Gen3x8 which provides
64Gb/s raw bidirectional bandwidth which is more than
the 40Gb/s throughput of the NIC. Hence there seems
to be no bottleneck between the switch and the server
CPU and memory. We thought that the server NIC
should not be able to generate PFC pause frames to
the switch, because there is no congestion point at the
server side.
But this was not what we have observed. We found
that many servers may generate up to thousands of PFC
pause frames per second. Since RDMA packets do not
need the server CPU for processing, the bottleneck must
be in the NIC. It turned out that this is indeed the case.
The NIC has limited memory resources, hence it puts
most of the data structures including QPC (Queue Pair
Context) and WQE (Work Queue Element) in the main
memory of the server. The NIC only caches a small
number of entries in its own memory. The NIC has a
Memory Translation Table (MTT) which translates the
virtual memory to the physical memory. The MTT has
only 2K entries. For 4KB page size, 2K MTT entries
can only handle 8MB memory.
If the virtual address in a WQE is not mapped in
the MTT, it results in a cache miss, and the NIC has
to replace some old entries for the new virtual address.
The NIC has to access the main memory of the server
to get the entry for the new virtual address. All those
operations take time and the receiving pipeline has to
208
wait. The MTT cache miss will therefore slow down the
packet processing pipeline. Once the receiving pipeline
is slowed down and the receiving buﬀer occupation ex-
ceeds the PFC threshold, the NIC has to generate PFC
pause frames to the switch.
We call this phenomenon the slow-receiver symptom.
Though its damage is not as severe as the NIC PFC
storm, it may still cause the pause frames to propagate
into the network and cause collateral damage.
The slow-receiver symptom is a ‘soft’ bug caused by
the NIC design. We took two actions to mitigate it. On
the NIC side, we used a large page size of 2MB instead
of 4KB. With a large page size, the MTT entry miss
becomes less frequent. On the switch side, we enabled
dynamic buﬀer sharing among diﬀerent switch ports.
Compared with static buﬀer reservation, dynamic buﬀer
sharing statistically gives RDMA traﬃc more buﬀers.
Hence even if the NICs are pausing the switch ports
from time to time, the switches can absorb additional
queue buildup locally without propagating the pause
frames back into the network. Compared with static
buﬀer allocation, our experience showed that dynamic
buﬀer sharing helps reduce PFC pause frame propaga-
tion and improve bandwidth utilization.
5. RDMA IN PRODUCTION
We added new management and monitoring capabili-
ties to debug the various RDMA and PFC safety issues
described in Section 4, and to detect RDMA related
bugs and incidents. We now discuss these new capabili-
ties which include the RDMA/PFC conﬁguration mon-
itoring, the PFC pause frame and lossless traﬃc moni-
toring, and the active RDMA latency monitoring. We
also present the latency and throughput measurements.
5.1 Conﬁguration management and mon-
itoring
To enable RDMA, we need to conﬁgure PFC at the
switch side, and RDMA and PFC at the server side.
At the switch side, the PFC conﬁguration is part of the
QoS conﬁguration. The PFC conﬁguration has a global
part which reserves buﬀer size, classiﬁes packets into
diﬀerent traﬃc classes based on the DSCP value, maps
diﬀerent traﬃc classes into diﬀerent queues, and assigns
diﬀerent bandwidth reservations for diﬀerent queues.
The PFC conﬁguration also has a per port part which
enables PFC for every individual physical port.
At the server side, there are conﬁgurations to en-
able/disable RoCEv2, PFC conﬁguration, DCQCN con-
ﬁguration, and traﬃc conﬁguration. In traﬃc conﬁgu-
ration, users specify which type of traﬃc they would like
to put into PFC protection. The speciﬁcation is based
on the destination transport port which is similar to the
TCP destination port.
We have a conﬁguration monitoring service to check
if the running conﬁgurations of the switches and the
servers are the same as their desired conﬁgurations. Our
RDMA management and monitoring service handles the
complexities introduced by the combinations of multi-
ple switch types, multiple switch and NIC ﬁrmware ver-
sions, and diﬀerent conﬁguration requirements for dif-
ferent customers.
5.2 PFC pause frame and trafﬁc monitor-
ing
Besides conﬁguration monitoring, we have also built
monitoring for the PFC pause frames and the two RDMA
traﬃc classes. For pause frame, we monitor the number
of pause frames been sent and received by the switches
and servers. We further monitor the pause intervals at
the server side. Compared with the number of pause
frames, pause intervals can reveal the severity of the
congestion in the network more accurately. Pause in-
tervals, unfortunately, are not available for the switches
we currently use. We have raised the PFC pause in-
terval monitoring requirement to the switching ASIC
providers for their future ASICs.
For RDMA traﬃc monitoring, we collect packets and
bytes been sent and received per port per priority, packet
drops at the ingress ports, and packet drops at the
egress queues. The traﬃc counters can help us under-
stand the RDMA traﬃc pattern and trend. The drop
counters help us detect if there is anything wrong for
the RDMA traﬃc: normally no RDMA packets should
be dropped.
5.3 RDMA Pingmesh
We have developed an active latency measurement
service for RDMA similar to the TCP Pingmesh service
[21]. We let the servers ping each other using RDMA
and call the measurement system RDMA Pingmesh.
RDMA Pingmesh launches RDMA probes, with pay-
load size 512 bytes, to the servers at diﬀerent locations
(ToR, Podset, Data center) and logs the measured RTT
(if probes succeed) or error code (if probes fail).
From the measured RTT of RDMA Pingmesh, we can
infer if RDMA is working well or not.
Our RDMA management and monitoring took a prag-
matic approach by focusing on conﬁgurations, coun-
ters, and end-to-end latency. We expect this approach
works well for the future 100G or higher speed networks.
RDMA poses challenges for packet-level monitoring due
to the high network speed and NIC oﬄoading, which we
plan to address in our next step.
5.4 RDMA Performance
In what follows, we present the RDMA performance
results in both testbed and production networks.
Latency reduction: Figure 6 shows the end-to-end
latency comparison of TCP and RDMA for a highly-
reliable, latency-sensitive online service. This service
has multiple instances in Microsoft global data centers
and it has 20K servers in each data center. The mea-
surements are from one of the data centers. At the
time of measurement, half of the traﬃc was TCP and
209
Figure 6: The comparison of the measured TCP and
RDMA latencies for a latency-sensitive service.
(a) The network topology.
half of the traﬃc was RDMA. The RDMA and TCP
latencies were all measured by Pingmesh. The latency
measurements for both TCP and RDMA were for intra-
DC communications. Since the online service is latency
sensitive, the peak traﬃc volume per sever was around
350Mb/s, and the aggregate server CPU load of the ser-
vice was around 20% - 30% during the measurement.
The network capacity between any two servers in this
data center is several Gb/s. The network was not the
bottleneck, but the traﬃc was bursty with the typical
many-to-one incast traﬃc pattern.
As we can see, the 99th percentile latencies for RDMA
and TCP were 90us and 700us, respectively. The 99th
percentile latency for TCP had spikes as high as several
milliseconds. In fact, even the 99.9th latency of RDMA
was only around 200us, and much smaller than TCP’s
99th percentile latency. Although the network was not
the bottleneck, TCP’s latency was high at the 99th per-
centile. This is caused by the kernel stack overhead and
occasional incast packet drops in the network. Although
RDMA did not change the incast traﬃc pattern, it elim-
inated packet drops and kernel stack overhead. Hence
it achieved much smaller and smoother high percentile
latency than TCP.
Throughput: The following experiment shows the RDMA
performance with hundreds of servers in a three-tier
Clos network. We ran this experiment using two pod-
sets after a data center was online but before it was
handed to the customer – i.e. there is no customer traf-
ﬁc during the experiment.
The network topology is shown in Figure 7(a). All
the ports in the network are 40GbE. A podset is com-
posed of 4 Leaf switches, 24 ToR switches, and 576
servers. Each ToR switch connects 24 servers. The
4 Leaf switches connect to a total of 64 Spine switches.
The oversubscription ratios at the ToR and the Leaf are
6:1 and 3:2, respectively. The aggregate bandwidth be-
tween a podset and the Spine switch layer is 64x40Gb/s
= 2.56Tb/s.
We used a ToR-to-ToR traﬃc pattern. We paired
the ToRs in the two podsets one by one. ToR i in