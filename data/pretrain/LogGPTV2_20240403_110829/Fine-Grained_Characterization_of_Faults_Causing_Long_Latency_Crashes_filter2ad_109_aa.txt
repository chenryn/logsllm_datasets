title:Fine-Grained Characterization of Faults Causing Long Latency Crashes
in Programs
author:Guanpeng Li and
Qining Lu and
Karthik Pattabiraman
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Fine-Grained Characterization of Faults Causing
Long Latency Crashes in Programs
Guanpeng Li, Qining Lu and Karthik Pattabiraman
Department of Electrical and Computer Engineering
University of British Columbia (UBC), Vancouver
{gpli, qining, karthikp}@ece.ubc.ca
Abstract—As the rate of transient hardware faults increases,
researchers have investigated software techniques to tolerate these
faults. An important class of faults are those that cause long-
latency crashes (LLCs), or faults that can persist for a long time in
the program before causing it to crash. In this paper, we develop
a technique to automatically ﬁnd program locations where LLC
causing faults originate so that the locations can be protected to
bound the program’s crash latency.
We ﬁrst identify program code patterns that are responsible
for the majority of LLC causing faults through an empirical
study. We then build CRASHFINDER, a tool that ﬁnds LLC
locations by statically searching the program for the patterns,
and then reﬁning the static analysis results with a dynamic
analysis and selective fault injection-based approach. We ﬁnd
that CRASHFINDER can achieve an average of 9.29 orders of
magnitude time reduction to identify more than 90% of LLC
causing locations in the program, compared to exhaustive fault
injection techniques, and has no false-positives.
Keywords: Long-latency Crashes, Hardware Faults,
Checkpoint Corruption
I.
INTRODUCTION
Transient hardware fault rates are predicted to increase
in future computer systems due to the effects of technol-
ogy scaling, manufacturing variations and diminishing volt-
age margins [3], [7]. In the past, such faults were masked
through hardware-only solutions such as redundancy or guard
banding. However, such techniques are becoming increasingly
challenging to deploy as they consume signiﬁcant amounts
of energy, and energy is becoming a ﬁrst class constraint in
microprocessor design [4]. As a result, many researchers have
postulated that future processors will expose hardware faults to
the software, and will expect the software to tolerate them [18],
[16], [23]. Future software systems therefore will need to be
capable of detecting hardware faults and recovering from them.
A hardware fault can have many possible effects on a
program. First, it may be masked or be benign. In other words,
the fault may have no effect on the program’s ﬁnal output.
Second, it may cause a crash (i.e., hardware exception) or
hang (i.e., program time out). Finally, it may cause Silent
Data Corruptions (SDCs), or the program producing incorrect
outputs. Of the above outcomes, SDCs are considered the most
severe, as there is no visible indication that the application has
done something wrong. Therefore, a number of prior studies
have focused on detecting SDC-causing program errors, by
selectively identifying and protecting elements of program
state that are likely to cause SDCs [10], [8], [26], [17].
Compared to SDCs, crashes have received relatively less
attention from the perspective of error detection. This is be-
cause crashes are considered to be the detection themselves, as
the program can be recovered from a checkpoint (if one exists)
or restarted after a crash. However, all of these studies make an
important assumption, namely that the crash occurs soon after
the fault is manifested in the program. This is important to
ensure that the program is prevented from writing corrupted
state to the ﬁle system (e.g., checkpoint), or sending wrong
messages to other processes [1]. While this assumption is true
for a large number of faults, studies have shown that a small
but non-negligible fraction of faults persist for a long time
in the program before causing a crash, and that these faults
can cause signiﬁcant reliability problems such as extended
downtimes [9], [27], [30]. We call these long-latency crashes
(LLCs). Therefore,
there is a compelling need to develop
techniques for protecting programs from LLC causing faults.
Prior work has experimentally assessed LLCs through fault
injection experiments [9]. However, they do not provide much
insight into why some faults cause LLCs. This is important
because (1) fault injection experiments require a lot of compu-
tation time, especially to identify relatively rare events such as
LLCs, and (2) fault injection cannot guarantee completeness
in identifying all or even most LLC causing locations. The
latter is important in order to ensure that crash latency is
bounded in the program by protecting LLC causing program
locations. Yim et al. [30] analyze error propagation latency in
the program, and develop a coarse-grained categorization of
program locations based on whether a fault in the location can
cause LLCs. The categorization is based on where the program
data resides, such as text segment, stack segment or heap
segment. While this is useful, it does not help programmers
decide which parts of the program need to be protected,
as protecting all parts of the program that manipulate the
heap data or stack data can lead to prohibitive performance
overheads.
In contrast to the above work, we present a technique to
perform ﬁne grained classiﬁcation of program’s data at the
level of individual variables and program statements, based
on whether a fault
in the data item causes an LLC. The
main insight underlying our work is that very few program
locations are responsible for LLCs, and that these locations
conform to a few dominant code patterns. Our technique
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.36
DOI 10.1109/DSN.2015.36
450
450
performs static analysis of the program to identify the LLC
causing code patterns. However, not every instance of the LLC-
causing code pattern leads to an LLC. Our technique further
uses dynamic analysis coupled with a very selective fault
injection experiment, to ﬁlter the false positives and isolate
the few instances of the patterns that lead to LLCs. We have
implemented our technique in a completely automated tool
called CRASHFINDER, which is integrated with the LLVM
compiler infrastructure [15]. To the best of our knowledge, we
are the ﬁrst to propose an automated and efﬁcient method
to systematically identify LLC causing program locations for
protection in a ﬁne-grained fashion.
We make the following contributions in this paper.
•
•
•
•
•
Identify the dominant code patterns that can cause
LLCs in programs through a large-scale fault injection
experiment we conducted on a total of ten benchmark
applications,
Develop an automated static analysis technique to
identify the LLC-causing code patterns in programs,
based on the fault injection study,
Propose a dynamic analysis and selective fault
injection-based approach to ﬁlter out
the false-
positives identiﬁed by the static analysis technique,
and identify LLCs.
Implement the static and dynamic analysis techniques
in an automated tool. We call this CRASHFINDER.
Evaluate CRASHFINDER on benchmark applications
from the SPEC [13], PARBOIL [25], PARSEC [2]
and SPLASH-2 [29] benchmark suites. We ﬁnd
that CRASHFINDER can accurately identify over 90%
of the LLC causing locations in the program, with no
false-positives, and is about nine orders of magnitude
faster than performing exhaustive fault injections to
identify all LLCs in a program.
II. FAULT MODEL AND BACKGROUND
In this section, we ﬁrst present our fault model, and
then deﬁne the terms we will use. We then explain why
bounding crash latency is important, and some speciﬁcs of
the experimental infrastructure that we use for the analysis.
A. Fault Model
In this paper, we consider transient hardware faults that
occur in the computational elements of the processor, including
pipeline stages, ﬂip-ﬂops, arithmetic and logic units (ALUs).
We do not consider faults in the memory or cache, as we
assume that these are protected with ECC. Likewise, we do
not consider faults in the processor’s control
logic as we
assume it is protected. Finally, we do not consider faults in
the instructions’ encoding as these can be detected through
other means such as error correction codes. Our fault model
is in line with other work in the area [11], [17], [8], [26].
B. Terms
We use the following terms deﬁned in prior work [30], [9].
451451
•
•
•
•
•
Fault occurrence: The event corresponding to the
occurrence of the hardware fault. The fault may or
may not result in an error.
Fault activation: The event corresponding to the
manifestation of the fault to the software, i.e., the fault
becomes an error and corrupts some portion of the
software state (e.g., register, memory location). The
error may or may not result in a crash.
Crash: The raising of a hardware trap or exception
due to the error, because the program attempted to
perform an action it should not have (e.g., read outside
its memory segments).
Crash latency: The number of dynamic instructions
executed by the program from fault activation to crash.
This deﬁnition is slightly different from prior work
which has used CPU cycles to measure the crash
latency. The main reason we use dynamic instructions
rather than CPU cycles is that we wish to obtain a
platform independent characterization of long latency
crashes.
Long latency crashes (LLCs): Crashes that have
crash latency of greater than 1,000 dynamic instruc-
tions. Prior work has used a wide range of values for
long latency crashes, ranging from 10,000 CPU cycles
[20] to as many as 10 million CPU cycles [30]. We
use 1,000 instructions as our threshold as (1) each
instruction corresponds to multiple CPU cycles in our
system, and (2) we found that in our benchmarks, the
length of the static data dependency sequences are far
smaller, and hence setting 1,000 instructions as the
threshold already ﬁlters out 99% of the crash-causing
faults (Section IV), showing that 1000 instructions is
a reasonable threshold.
C. Why bound the crash latency?
We now explain our rationale for studying LLCs and why it
is important to bound the crash latency in programs. We note
that similar observations have been made in prior work [9],
[30], and that studies have shown that having unbounded crash
latency can result in severe failures. We consider one example.
Assume that the program is being checkpointed every 8,000
instructions so that it can be recovered in the case of a failure
(we set aside the practicality of performing such ﬁne grained
checkpointing for now). We assume that the checkpoints are
gathered in an application independent manner, i.e., the entire
state of the program is captured in the checkpoint. If the
program encounters an LLC of more than 10,000 instructions,
it
is highly likely that one or more checkpoints will be
corrupted (by the fault causing the LLC). This situation is
shown in Figure 1. However, if the crash latency is bounded to
1,000 instructions (say), then it is highly unlikely for the fault
to corrupt more than one checkpoint. Note that the latency
between the fault activation and the fault occurrence does
not matter in this case, as the checkpoint is corrupted only
when the fault actually gets activated. Therefore, we focus on
the crash latency in this paper, i.e., the number of dynamic
instructions from the fault activation to the crash.
(cid:54)(cid:81)(cid:68)(cid:83)(cid:86)(cid:75)(cid:82)(cid:87)(cid:3)(cid:20)
(cid:54)(cid:81)(cid:68)(cid:83)(cid:86)(cid:75)(cid:82)(cid:87)(cid:3)(cid:21)
(cid:54)(cid:81)(cid:68)(cid:83)(cid:86)(cid:75)(cid:82)(cid:87)(cid:3)(cid:22)
(cid:27)(cid:19)(cid:19)(cid:19)(cid:3)(cid:76)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)
(cid:27)(cid:19)(cid:19)(cid:19)(cid:3)(cid:76)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)
(cid:11)(cid:20)(cid:12)(cid:3)(cid:41)(cid:68)(cid:88)(cid:79)(cid:87)(cid:3)(cid:50)(cid:70)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)
(cid:20)(cid:19)(cid:15)(cid:19)(cid:19)(cid:19)(cid:3)(cid:70)(cid:85)(cid:68)(cid:86)(cid:75)(cid:3)(cid:79)(cid:68)(cid:87)(cid:72)(cid:81)(cid:70)(cid:92)
(cid:11)(cid:21)(cid:12)(cid:41)(cid:68)(cid:88)(cid:79)(cid:87)(cid:3)(cid:36)(cid:70)(cid:87)(cid:76)(cid:89)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:11)(cid:22)(cid:12)(cid:38)(cid:85)(cid:68)(cid:86)(cid:75)
(cid:55)(cid:76)(cid:80)(cid:72)(cid:79)(cid:76)(cid:81)(cid:72)
Fig. 1: Long Latency Crash and Checkpointing
Identifying program locations that are prone to LLC is
critical to improve system reliability so that one can bound
crash latency by selectively protecting LLC-prone locations
with minimal performance overheads. For example, one can
duplicate the backward slices of the LLC-prone locations, or
use low-cost detectors for these locations like what prior work
has done [24]. In this paper, we focus on identifying such LLC-
causing program locations, and defer the problem of protecting
the locations to future work.
D. LLVM Compiler
In this paper, we use the LLVM compiler [15] for per-
forming the static analysis of which program locations lead to
LLCs. Our choice of LLVM is motivated by three reasons.
First, LLVM uses a typed intermediate representation (IR),
in which source-level constructs can be easily represented. In
particular, it preserves the names of variables and functions,
which makes source mapping feasible. This allows us to
perform a ﬁne-grained analysis of which program locations
cause LLCs and map it to the source code. Secondly, LLVM
IR is a platform neutral representation and abstracts out many
low level details of the hardware and assembly language.
This greatly aids in portability of our analysis to different
architectures, and simpliﬁes the handling of the special cases of
different assembly language formats. Finally, LLVM has been
shown to be a good match for doing fault injection studies [28],
and there are fault injectors developed for LLVM. Therefore,