### Figures and Analysis

**Figure 7: CDFs of the Percentage of Android and Google Family Calls in Different Apps in Each Dataset.**

**Figure 8: Positions of Benign vs. Malware Samples in the Feature Space of the First Two Components of the PCA (Family Mode).**

### Principal Component Analysis
We apply Principal Component Analysis (PCA) to select the two most significant PCA components. The positions of these components for benign (Figure 8(a)) and malicious samples (Figure 8(b)) are plotted and compared. As PCA combines features into components, it maximizes the variance of the sample distribution in these components. Consequently, the positions of the samples in the component space show that benign apps tend to be located in different areas, depending on the dataset, while malware samples occupy similar areas but with varying densities. These differences highlight distinct behaviors between benign and malicious samples, which should also be detectable by the machine learning algorithms used for classification.

### IV. Evaluation
We present a detailed experimental evaluation of MAMADROID using the datasets summarized in Table I. We conduct four sets of experiments:
1. **Accuracy Analysis:** We analyze the accuracy of MAMADROID's classification on benign and malicious samples developed around the same time.
2. **Robustness to Evolution:** We evaluate its robustness to the evolution of malware and the Android framework by training on older datasets and testing on newer ones (and vice versa).
3. **Runtime Performance:** We measure MAMADROID’s runtime performance to assess its scalability.
4. **Comparison with DROIDAPIMINER:** We compare MAMADROID against DROIDAPIMINER [2], a malware detection system that relies on the frequency of API calls.

#### A. Preliminaries
When implementing MAMADROID in family mode, we exclude the `json` and `dom` families because they are rarely used across our datasets, and `junit`, which is primarily used for testing. In package mode, to avoid mislabeling when self-defined APIs have "android" in the name, we split the `android` package into `android.R` and `android.Manifest`. Therefore, in family mode, there are 8 possible states, resulting in 64 features, whereas in package mode, we have 341 states and 116,281 features (cf. Section II-D).

As discussed in Section II-E, we use four different machine learning algorithms for classification: Random Forests [9], 1-NN [22], 3-NN [22], and SVM [29]. Since both accuracy and speed are worse with SVM than with the other three algorithms, we omit results obtained with SVM. To assess the accuracy of the classification, we use the standard F-measure metric:

\[
F = \frac{2 \cdot \text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
\]

where
- \(\text{precision} = \frac{TP}{TP + FP}\)
- \(\text{recall} = \frac{TP}{TP + FN}\)

\(TP\) denotes the number of samples correctly classified as malicious, while \(FP\) and \(FN\) indicate, respectively, the number of samples mistakenly identified as malicious and benign.

All our experiments perform 10-fold cross-validation using at least one malicious and one benign dataset from Table I. After merging the datasets, the resulting set is shuffled and divided into ten equal-size random subsets. Classification is then performed ten times using nine subsets for training and one for testing, and results are averaged over the ten experiments.

**Figures 9 and 10:**
- **Figure 9:** F-measure of MAMADROID classification with datasets from the same year (family mode).
- **Figure 10:** F-measure of MAMADROID classification with datasets from the same year (package mode).

#### B. Detection Performance
We start our evaluation by measuring how well MAMADROID detects malware by training and testing using samples developed around the same time. We perform 10-fold cross-validation on the combined dataset composed of a benign set and a malicious one. Table II provides an overview of the detection results achieved by MAMADROID on each combined dataset, in both modes of operation, both with PCA features and without. The reported F-measure, precision, and recall scores are those obtained with Random Forests, which generally perform better than 1-NN and 3-NN.

**Family Mode:**
In Figure 9, we report the F-measure when operating in family mode for Random Forests, 1-NN, and 3-NN. The F-measure is always at least 88% with Random Forests, and, when tested on the 2014 (malicious) dataset, it reaches 98%. With some datasets, MAMADROID performs slightly better than with others. For instance, with the 2014 malware dataset, we obtain an F-measure of 92% when using the oldbenign dataset and 98% with newbenign. Generally, lower F-measures are due to increased false positives since recall is always above 95%, while precision might be lower, also due to the fact that malware datasets are larger than the benign sets. This follows the evolutionary trend discussed in Section III: while both benign and malicious apps become more complex over time, new benign apps may still use old classes or reuse code from previous versions, leading to false positives by MAMADROID. MAMADROID performs better when the characteristics of malicious and benign training and test sets are more distinct, corresponding to datasets occupying different positions in the feature space.

**Package Mode:**
With some datasets, the difference in performance between the two modes of operation is more noticeable. For example, with drebin and oldbenign, and using Random Forests, we get 95% F-measure in package mode compared to 88% in family mode. These differences are caused by a lower number of false positives in package mode. Recall remains high, resulting in a more balanced system overall. Generally, abstracting to packages rather than families provides better results as the increased granularity enables identifying more differences between benign and malicious apps. However, this likely reduces the efficiency of the system, as many of the states derived from the abstraction are used only a few times. The differences in time performance between the two modes are analyzed in detail in Section IV-F.

**Using PCA:**
As discussed in Section II-D, PCA transforms large feature spaces into smaller ones, significantly reducing computation and memory complexities. When operating in package mode, PCA is particularly beneficial, as MAMADROID originally operates over 116,281 features. We compare results obtained using PCA by fixing the number of components to 10 and checking the variance included in them. In package mode, only 67% of the variance is accounted for by the 10 most important PCA components, whereas in family mode, at least 91% of the variance is included by the 10 PCA components.

As shown in Table II, the F-measure obtained using Random Forests and the PCA component sets derived from the family and package features is only slightly lower (up to 4%) than using the full feature set. Lower F-measures are caused by a uniform decrease in both precision and recall.

#### C. Detection Over Time
**Package Mode:**
When MAMADROID runs in package mode, the classification performance improves, ranging from 92% F-measure with 2016 and newbenign to 99% with 2014 and newbenign. As Android evolves over the years, so do the characteristics of both benign and malicious apps. This evolution must be considered when evaluating Android malware detection systems, as their accuracy can be significantly affected by new API releases and changes in malicious developer strategies.

MAMADROID relies on the sequence of API calls extracted from call graphs and abstracted at either the package or family level, making it less susceptible to changes in the Android API than other classification systems such as DROIDAPIMINER [2] and DREBIN [5]. These systems need to be retrained following new API releases, whereas MAMADROID only needs to add a new state to its Markov chain in package mode, and no additional state is required in family mode.

To verify this hypothesis, we test MAMADROID using older samples as training sets and newer ones as test sets. Figure 11 reports the F-measure of the classification in this setting, with MAMADROID operating in family mode. The x-axis reports the difference in years between training and test data. We obtain 86% F-measure when classifying apps one year older than the samples on which we train. Classification is still relatively accurate, at 75%, even after two years. From Figure 12, we observe that the F-measure does not significantly change when operating in package mode. Both modes of operation are affected by a particular condition: in our models, benign datasets seem to "anticipate" malicious ones by 1–2 years in the way they use certain API calls. This results in a drop in accuracy when classifying future samples and using drebin (with samples from 2010 to 2012) or 2013 as the malicious training set and oldbenign (late 2013/early 2014) as the benign training set. Specifically, MAMADROID correctly detects benign apps but starts missing true positives and increasing false negatives, achieving lower recall.

We also verify whether older malware samples can still be detected by the system. If not, this would make the system vulnerable to older (and possibly popular) attacks. Therefore, we perform the "opposite" experiment, training MAMADROID with newer datasets and testing it with older samples. Figures 13 and 14 report results when training MAMADROID with samples from a given year and testing it with others up to 4 years older. MAMADROID retains similar F-measure scores over the years. Specifically, in family mode, it varies from 93% to 96%, whereas in package mode, from 95% to 97% with the oldest samples.

#### D. Case Studies of False Positives and Negatives
The experiment analysis shows that MAMADROID detects Android malware with high accuracy. However, like any detection system, it makes a small number of incorrect classifications, incurring some false positives and false negatives. We discuss a few case studies to better understand these misclassifications, focusing on the experiments with newer datasets, i.e., 2016 and newbenign.

**False Positives:**
- 30% of false positives are potentially benign apps that use specific permissions and functionalities.
- 20% are adware samples.
- 16% are samples sending text messages or starting calls to premium services.

**False Negatives:**
- 53% of false negatives are adware samples.
- 15% are potentially benign apps.
- 11% are samples sending SMSs or placing calls.

In conclusion, MAMADROID’s sporadic misclassifications are typically due to benign apps behaving similarly to malware, malware that do not perform clearly-malicious activities, or mistakes in the ground truth labeling.

#### E. MAMADROID vs. DROIDAPIMINER
We compare the performance of MAMADROID to DROIDAPIMINER [2], which uses API calls and their parameters for classification. DROIDAPIMINER reports a high true positive rate (up to 97.8%) on almost 4K malware samples obtained from McAfee and GENOME [66], and 16K benign samples. Its source code has been made available to us by the authors.

In DROIDAPIMINER, permissions that are requested more frequently by malware are used to classify apps.