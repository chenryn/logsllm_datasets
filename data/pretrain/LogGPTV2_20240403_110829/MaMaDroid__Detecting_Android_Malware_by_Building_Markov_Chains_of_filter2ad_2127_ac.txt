(a) benign
(b) google
(b) malware
Fig. 7: CDFs of the percentage of android and google family calls
in different apps in each dataset.
Fig. 8: Positions of benign vs malware samples in the feature space
of the ﬁrst two components of the PCA (family mode).
Principal Component Analysis. Finally, we apply PCA to
select the two most important PCA components. We plot
and compare the positions of the two components for benign
(Fig. 8(a)) and malicious samples (Fig. 8(b)). As PCA combines
the features into components, it maximizes the variance of the
distribution of samples in these components, thus, plotting the
positions of the samples in the components shows that benign
apps tend to be located in different areas of the components
space, depending on the dataset, while malware samples occupy
similar areas but with different densities. These differences
highlight a different behavior between benign and malicious
samples, and these differences should also be found by the
machine learning algorithms used for classiﬁcation.
IV. EVALUATION
We now present a detailed experimental evaluation of
MAMADROID. Using the datasets summarized in Table I, we
perform four sets of experiments: (1) we analyze the accuracy
of MAMADROID’s classiﬁcation on benign and malicious
samples developed around the same time; (2) we evaluate its
robustness to the evolution of malware as well as of the Android
framework by using older datasets for training and newer ones
for testing (and vice-versa); (3) we measure MAMADROID’s
runtime performance to assess its scalability; and, ﬁnally, (4)
we compare against DROIDAPIMINER [2], a malware detection
system that relies on the frequency of API calls.
A. Preliminaries
When implementing MAMADROID in family mode, we
exclude the json and dom families because they are almost
never used across all our datasets, and junit, which is primarily
used for testing. In package mode, to avoid mislabeling when
self-defined APIs have “android” in the name, we split the
android package into its two classes, i.e., android.R and
android.Manifest. Therefore, in family mode, there are 8
possible states, thus 64 features, whereas, in package mode,
we have 341 states and 116,281 features (cf. Section II-D).
As discussed in Section II-E, we use four different ma-
chine learning algorithms for classiﬁcation – namely, Random
Forests [9], 1-NN [22], 3-NN [22], and SVM [29]. Since both
accuracy and speed are worse with SVM than with the other
three algorithms, we omit results obtained with SVM. To assess
the accuracy of the classiﬁcation, we use the standard F-measure
metric, i.e.:
F = 2 · precision · recall
precision + recall
where precision = TP/(TP+FP) and recall = TP/(TP+FN).
TP denotes the number of samples correctly classiﬁed as
malicious, while FP an FN indicate, respectively, the number
of samples mistakenly identiﬁed as malicious and benign.
Finally, note that all our experiments perform 10-fold cross
validation using at least one malicious and one benign dataset
from Table I. In other words, after merging the datasets, the
6
0.00.20.40.60.81.0Fraction of Calls0.00.20.40.60.81.0CDF2013201420152016drebinnewbenignoldbenign0.00.20.40.60.81.0Fraction of Calls0.00.20.40.60.81.0CDF2013201420152016drebinnewbenignoldbenign1.00.50.00.51.01.5PCA10.80.60.40.20.00.20.40.60.81.0PCA2oldbenignnewbenign1.00.50.00.51.01.5PCA10.80.60.40.20.00.20.40.60.81.0PCA2drebin2013201420152016Fig. 9: F-measure of MAMADROID classiﬁcation with datasets from
the same year (family mode).
Fig. 10: F-measure of MAMADROID classiﬁcation with datasets from
the same year (package mode).
resulting set is shufﬂed and divided into ten equal-size random
subsets. Classiﬁcation is then performed ten times using nine
subsets for training and one for testing, and results are averaged
out over the ten experiments.
newbenign, using Random Forests. Fig. 10 reports the F-
measure of the 10-fold cross validation experiments using
Random Forests, 1-NN, and 3-NN (in package mode). The
former generally provide better results also in this case.
B. Detection Performance
We start our evaluation by measuring how well MA-
MADROID detects malware by training and testing using
samples that are developed around the same time. To this end,
we perform 10-fold cross validations on the combined dataset
composed of a benign set and a malicious one. Table II provides
an overview of the detection results achieved by MAMADROID
on each combined dataset, in the two modes of operation,
both with PCA features and without. The reported F-measure,
precision, and recall scores are the ones obtained with Random
Forest, which generally performs better than 1-NN and 3-NN.
Family mode. In Fig. 9, we report the F-measure when
operating in family mode for Random Forests, 1-NN and 3-
NN. The F-measure is always at least 88% with Random
Forests, and, when tested on the 2014 (malicious) dataset, it
reaches 98%. With some datasets, MAMADROID performs
slightly better than with others. For instance, with the 2014
malware dataset, we obtain an F-measure of 92% when using
the oldbenign dataset and 98% with newbenign. In general,
lower F-measures are due to increased false positives since
recall is always above 95%, while precision might be lower,
also due to the fact that malware datasets are larger than the
benign sets. We believe that this follows the evolutionary trend
discussed in Section III: while both benign and malicious apps
become more complex as time passes, when a new benign app
is developed, it is still possible to use old classes or re-use
code from previous versions and this might cause them to be
more similar to old malware samples. This would result in
false positives by MAMADROID. In general, MAMADROID
performs better when the different characteristics of malicious
and benign training and test sets are more predominant, which
corresponds to datasets occupying different positions of the
feature space.
With some datasets, the difference in performance between
the two modes of operation is more noticeable: with drebin
and oldbenign, and using Random Forests, we get 95% F-
measure in package mode compared to 88% in family mode.
These differences are caused by a lower number of false
positives in package mode. Recall remains high, resulting
in a more balanced system overall. In general, abstracting
to packages rather than families provides better results as
the increased granularity enables identifying more differences
between benign and malicious apps. On the other hand, however,
this likely reduces the efﬁciency of the system, as many of the
states deriving from the abstraction are used a only few times.
The differences in time performance between the two modes
are analyzed in details in Section IV-F.
Using PCA. As discussed in Section II-D, PCA transforms
large feature spaces into smaller ones, thus it can be useful
to signiﬁcantly reduce computation and, above all, memory
complexities of the classiﬁcation task. When operating in pack-
age mode, PCA is particularly beneﬁcial, since MAMADROID
originally has to operate over 116,281 features. Therefore, we
compare results obtained using PCA by ﬁxing the number
of components to 10 and checking the quantity of variance
included in them. In package mode, we observe that only 67%
of the variance is taken into account by the 10 most important
PCA components, whereas, in family mode, at least 91% of
the variance is included by the 10 PCA Components.
As shown in Table II, the F-measure obtained using Random
Forests and the PCA components sets derived from the family
and package features is only slightly lower (up to 4%) than
using the full feature set. We note that lower F-measures are
caused by a uniform decrease in both precision and recall.
C. Detection Over Time
Package mode. When MAMADROID runs in package mode,
the classiﬁcation performance improves, ranging from 92%
F-measure with 2016 and newbenign to 99% with 2014 and
As Android evolves over the years, so do the characteristics
of both benign and malicious apps. Such evolution must be
taken into account when evaluating Android malware detection
7
Drebin &OldBenign2013 &OldBenign2014 &OldBenign2014 &Newbenign2015 &Newbenign2016 &Newbenign0.00.20.40.60.81.0F-measureRF1-NN3-NNDrebin &OldBenign2013 &OldBenign2014 &OldBenign2014 &Newbenign2015 &Newbenign2016 &Newbenign0.00.20.40.60.81.0F-measureRF1-NN3-NNPPPPPP
Dataset
[Precision, Recall, F-measure]
drebin & oldbenign 2013 & oldbenign 2014 & oldbenign 2014 & newbenign 2015 & newbenign 2016 & newbenign
Mode
0.89
0.82
Family
0.91
0.95
Package
0.87
Family (PCA)
0.83
0.89
Package (PCA) 0.93
0.92 0.88
0.97 0.93
0.91 0.86
0.95 0.92
0.88 0.91
0.96 0.98
0.87 0.93
0.94 0.97
0.98 0.90
0.99 0.93
0.97 0.87
0.99 0.92
0.97
0.98
0.94
0.96
0.92 0.97
0.96 0.98
0.90 0.96
0.94 0.98
0.95
0.97
0.93
0.95
0.93
0.96
0.90
0.94
0.99
1.00
0.99
1.00
0.94
0.98
0.93
0.97
0.92 0.87
0.96 0.91
0.90 0.86
0.94 0.88
0.92
0.91
0.87
0.90
TABLE II: F-measure, precision, and recall obtained by MAMADROID, using Random Forests, on various dataset combinations with different
modes of operation, with and without PCA.
systems, since their accuracy might signiﬁcantly be affected
as newer APIs are released and/or as malicious developers
modify their strategies in order to avoid detection. Evaluating
this aspect constitutes one of our research questions, and one
of the reasons why our datasets span across multiple years
(2010–2016).
As discussed in Section II-B, MAMADROID relies on the
sequence of API calls extracted from the call graphs and
abstracted at either the package or the family level. Therefore,
it is less susceptible to changes in the Android API than
other classiﬁcation systems such as DROIDAPIMINER [2]
and DREBIN [5]. Since these rely on the use, or the frequency,
of certain API calls to classify malware vs benign samples,
they need to be retrained following new API releases. On the
contrary, retraining is not needed as often with MAMADROID,
since families and packages represent more abstract function-
alities that change less over time. Consider, for instance, the
android.os.health package: released with API level 24, it
contains a set of classes helping developers track and monitor
system resources.10 Classiﬁcation systems built before this
release – as in the case of DROIDAPIMINER [2] (released in
2013, when Android API was up to level 20) – need to be
retrained if this package is more frequently used by malicious
apps than benign apps, while MAMADROID only needs to add
a new state to its Markov chain when operating in package
mode, while no additional state is required when operating in
family mode.
To verify this hypothesis, we test MAMADROID using older
samples as training sets and newer ones as test sets. Fig. 11
reports the F-measure of the classiﬁcation in this setting, with
MAMADROID operating in family mode. The x-axis reports the
difference in years between training and test data. We obtain
86% F-measure when we classify apps one year older than
the samples on which we train. Classiﬁcation is still relatively
accurate, at 75%, even after two years. Then, from Fig. 12,
we observe that the F-measure does not signiﬁcantly change
when operating in package mode. Both modes of operations
are affected by one particular condition, already discussed in
Section III: in our models, benign datasets seem to “anticipate”
malicious ones by 1–2 years in the way they use certain
API calls. As a result, we notice a drop in accuracy when
classifying future samples and using drebin (with samples
from 2010 to 2012) or 2013 as the malicious training set and
oldbenign (late 2013/early 2014) as the benign training set.
More speciﬁcally, we observe that MAMADROID correctly
detects benign apps, while it starts missing true positives and
increasing false negatives — i.e., achieving lower recall.
We also set to verify whether older malware samples can still
be detected by the system—if not, this would obviously become
10https://developer.android.com/reference/android/os/health/package-summary.
html
Fig. 11: F-measure of MAMADROID classiﬁcation using older samples
for training and newer for testing (family mode).
Fig. 12: F-measure of MAMADROID classiﬁcation using older samples
for training and newer for testing (package mode).
i.e.,
vulnerable to older (and possibly popular) attacks. Therefore,
we also perform the “opposite” experiment,
training
MAMADROID with newer datasets, and checking whether it
is able to detect malware developed years before. Speciﬁcally,
Fig. 13 and 14 report results when training MAMADROID with
samples from a given year, and testing it with others that are
up to 4 years older: MAMADROID retains similar F-measure
scores over the years. Speciﬁcally, in family mode, it varies
from 93% to 96%, whereas, in package mode, from 95% to
97% with the oldest samples.
D. Case Studies of False Positives and Negatives
The experiment analysis presented above show that MA-
MADROID detects Android malware with high accuracy.
As in any detection system, however, the system makes a
small number of incorrect classiﬁcations, incurring some false
positives and false negatives. Next, we discuss a few case
studies aiming to better understand these misclassiﬁcations. We
focus on the experiments with newer datasets, i.e., 2016 and
8
01234Years0.00.20.40.60.81.0F-measureRF1-NN3-NN01234Years0.00.20.40.60.81.0F-measureRF1-NN3-NNthem as malware. Finally, we ﬁnd that 16% of the false negatives
reported by MAMADROID are samples sending text messages
or starting calls to premium services. We also do a similar
analysis of false negatives when abstracting to packages (74
samples), with similar results: there a few more adware samples
(53%), but similar percentages for potentially benign apps (15%)
and samples sending SMSs or placing calls (11%).
In conclusion, we ﬁnd that MAMADROID’s sporadic
misclassiﬁcations are typically due to benign apps behaving
similarly to malware, malware that do not perform clearly-
malicious activities, or mistakes in the ground truth labeling.
E. MAMADROID vs DROIDAPIMINER
We also compare the performance of MAMADROID to
previous work using API features for Android malware clas-
siﬁcation. Speciﬁcally, we compare to DROIDAPIMINER [2],
because: (i) it uses API calls and its parameters to perform
classiﬁcation; (ii) it reports high true positive rate (up to 97.8%)
on almost 4K malware samples obtained from McAfee and
GENOME [66], and 16K benign samples; and (iii) its source
code has been made available to us by the authors.
In DROIDAPIMINER, permissions that are requested more