title:NAUTILUS: Fishing for Deep Bugs with Grammars
author:Cornelius Aschermann and
Tommaso Frassetto and
Thorsten Holz and
Patrick Jauernig and
Ahmad-Reza Sadeghi and
Daniel Teuchert
NAUTILUS:
Fishing for Deep Bugs with Grammars
Cornelius Aschermann
Ruhr-Universität Bochum
PI:EMAIL
Tommaso Frassetto
Technische Universität Darmstadt
PI:EMAIL
Thorsten Holz
Ruhr-Universität Bochum
PI:EMAIL
Patrick Jauernig
Technische Universität Darmstadt
PI:EMAIL
Ahmad-Reza Sadeghi
Technische Universität Darmstadt
PI:EMAIL
Daniel Teuchert
Ruhr-Universität Bochum
PI:EMAIL
Abstract—Fuzz testing is a well-known method for efﬁciently
identifying bugs in programs. Unfortunately, when programs
that require highly-structured inputs such as interpreters are
fuzzed, many fuzzing methods struggle to pass the syntax checks:
interpreters often process inputs in multiple stages, ﬁrst syntactic
and then semantic correctness is checked. Only if both checks
are passed, the interpreted code gets executed. This prevents
fuzzers from executing “deeper” — and hence potentially more
interesting — code. Typically, two valid inputs that lead to the
execution of different features in the target program require too
many mutations for simple mutation-based fuzzers to discover:
making small changes like bit ﬂips usually only leads to the
execution of error paths in the parsing engine. So-called grammar
fuzzers are able to pass the syntax checks by using Context-
Free Grammars. Feedback can signiﬁcantly increase the efﬁciency
of fuzzing engines and is commonly used in state-of-the-art
mutational fuzzers which do not use grammars. Yet, current
grammar fuzzers do not make use of code coverage, i.e., they
do not know whether any input triggers new functionality.
In this paper, we propose NAUTILUS, a method to efﬁciently
fuzz programs that require highly-structured inputs by combining
the use of grammars with the use of code coverage feedback.
This allows us to recombine aspects of interesting inputs, and
to increase the probability that any generated input will be
syntactically and semantically correct. We implemented a proof-
of-concept fuzzer that we tested on multiple targets, including
ChakraCore (the JavaScript engine of Microsoft Edge), PHP,
mruby, and Lua. NAUTILUS identiﬁed multiple bugs in all of
the targets: Seven in mruby, three in PHP, two in ChakraCore,
and one in Lua. Reporting these bugs was awarded with a sum of
2600 USD and 6 CVEs were assigned. Our experiments show that
combining context-free grammars and feedback-driven fuzzing
signiﬁcantly outperforms state-of-the-art approaches like AFL by
an order of magnitude and grammar fuzzers by more than a
factor of two when measuring code coverage.
I.
INTRODUCTION
Software controls more and more aspects of the modern
life. Hence, the importance of software testing is increasing
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2019 
24-27  February  2019,  San  Diego,  CA,  USA
ISBN  1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23412
www.ndss-symposium.org
at a similar pace. Human-written tests (e.g., unit tests) are
an important part of the software development life cycle; yet,
many software projects have no or limited testing suites due
to a variety of reasons. Even for projects with comprehensive
testing suites, tests usually revolve around expected inputs
in order to test the intended functionality of code. However,
unexpected inputs are one of the primary attack vectors
used to exploit applications using their intended functionality,
whereas automated software testing excels at ﬁnding inputs
with unexpected characteristics that can be leveraged to trigger
vulnerabilities.
One popular approach to automatically test programs is
fuzzing,
i.e., automatically testing programs by generating
inputs and feeding them to the program while monitoring
crashes and other unexpected conditions. In recent years, many
different fuzzers were developed, covering a variety of ap-
proaches and goals. General-purpose fuzzers [18] usually rely
on low-level binary transformations to generate new inputs,
thus, they struggle with programs which only accept highly
structured ﬁles, such as interpreters for scripting languages.
Binary transformations generate inputs that struggle to pass
initial lexical and syntactic analysis [36] and reach the code
that executes after those checks, i.e., the deep code.
An intuitive solution to this problem is to use (context-free)
grammars to generate syntactically-correct
inputs. Previous
works [5], [17], [36], [47] use this approach, but they do not
leverage instrumentation feedback, which allows the fuzzer to
distinguish inputs that reach a new part of the code base from
inputs that reach no new code.
Leveraging feedback led to a great improvement in the
performance of general-purpose fuzzers. One of the most
popular feedback-oriented fuzzers is AFL [19], which was used
to identify bugs in hundreds of applications and tools. Using
code coverage feedback, AFL is able to intelligently combine
interesting inputs to explore deeper code, which would take
an unreasonable amount of time without feedback. In contrast,
AFL struggles with heavily-structured ﬁle formats since it is
optimized for binary formats and does not support grammars.
Note that AFL can be provided with a list of strings, which it
will try to use to generate inputs. However, this list does not
support any kind of grammar-like semantics.
Most coverage-driven fuzzers, including AFL, require a
corpus of inputs which they use as a basis to start the fuzzing
process. A good-quality corpus is crucial to the performance
and effectiveness of the fuzzer: any code path that is used
by the corpus does not have to be discovered by the fuzzer
and can be combined with other inputs from the beginning.
Getting such a high-quality corpus is not trivial: if the language
accepted by the target application is widely used, one approach
is to crawl publicly available examples from the Web [49]
or from public code repositories. However, those examples
are likely to skew towards very commonly-used parts of the
grammar, which use well-tested parts of the target application.
Naturally, security researchers often want to test features which
are rarely used or were introduced very recently, which are
more likely to lead to bugs in the target application. Acquiring
a corpus to test those features is clearly harder, while writing
examples manually is very expensive.
Goals and contributions. In this paper, we present the de-
sign and implementation of a fuzzing method that combines
description-based input generation and feedback-based fuzzing
to ﬁnd bugs deep in the applications’ semantics, i.e., bugs
that happen after lexical and syntactical checks. Our prototype
implementation of this concept called NAUTILUS requires no
corpus, only the source code of an application and a grammar
to generate inputs for it. One can start fuzzing with NAUTILUS
using publicly-available grammars [6]. The fuzzing process
can then be ﬁne-tuned by removing uninteresting parts of
the grammar and adding additional
the
language, e.g., by incorporating function names and parameter
types taken from the language documentation, which can be
easily automated. Additionally, NAUTILUS allows the user to
extended grammars with additional script. These scripts allow
NAUTILUS to generate any decidable input language to further
improve its ability to generate semantically correct inputs.
information about
However, NAUTILUS does not
just generate interesting
initial inputs. The fuzzing process itself also leverages the
grammar by performing high-level semantic transformations
on the inputs, e.g., swapping an expression for a different
expression in a program. By combining these mutations with
the coverage feedback, NAUTILUS can create a corpus of
semantically interesting and diverse inputs and recombine them
in a way that drastically increases the probability of ﬁnding
new inputs which are both syntactically and semantically
valid. As we evaluate in Section VI, those two advantages
give NAUTILUS a signiﬁcant advantage over state-of-the-art
fuzzers. Additionally, NAUTILUS was able to ﬁnd new bugs in
all the targets it was tested on: seven in mruby1, three in PHP,
one in Lua, and two in ChakraCore.
In summary, our contributions in this paper are:
• We introduce and evaluate NAUTILUS, the ﬁrst fuzzer that
combines grammar-based input generation with feedback-
directed fuzzing. NAUTILUS signiﬁcantly improves the
efﬁciency and effectiveness of fuzzing on targets that
require highly-structured inputs—without requiring any
corpus. To increase expressiveness, NAUTILUS supports
Turing-complete scripts as an extension to the grammar
for input
language descriptions. This can be used to
1CVE-2018-10191, CVE-2018-10199, CVE-2018-11743, CVE-2018-
12247, CVE-2018-12248, and CVE-2018-12249.
create descriptions for complex, non-context-free input
languages.
• We describe and evaluate several grammar-based muta-
tion, minimization and generation techniques. By com-
bining coverage feedback and grammar-based splicing,
NAUTILUS is able to generate syntactically and often
semantically correct programs, outperforming traditional
purely-generational fuzzers that spend signiﬁcant
time
generating and testing semantically invalid inputs.
• We found and reported several security bugs in multiple
widely-used software projects which no other fuzzer in
our evaluation found.
To foster research on this topic, we release our fuzzer at
https://github.com/RUB-SysSec/nautilus.
II. BACKGROUND
A. Fuzzing
Fuzzing is a quick and cost-effective technique to ﬁnd
coding ﬂaws in applications. Traditionally, there are two ways
for fuzzers to generate input for target applications: mutation
or generation.
For mutational fuzzing [15], [19], a well-formed corpus
of seed inputs, e.g., test cases, is modiﬁed using genetics-
inspired operations like bit ﬂips, or recombination of two
inputs (splicing). These modiﬁcations can be either random
(brute force), or guided using a heuristic. More advanced
techniques are either taint-based [24], [27], [29], [43], [50],
symbolic [26], or concolic [31], [32], [46] (a portmanteau
of concrete and symbolic). Taint-based fuzzers try to track
down input bytes that, e.g., inﬂuence function arguments, while
symbolic analysis treats some of the input bytes symbolically,
and uses symbolic execution to explore new paths. Concolic
execution combines these techniques: dynamic analysis (e.g.,
guided or taint-based fuzzers) is used to get as many new paths
as possible, then, corresponding concrete values are passed to
a symbolic execution engine to take new branches (guarded
by more complex checks) to explore new paths. These new
paths are the input for the next iteration of dynamic analysis.
A popular mutation-based fuzzer is the heuristically-guided
fuzzer AFL [18]. AFL uses new basic block transitions as a
heuristic for coverage.
In contrast, generation-based fuzzers can generate input
based on a given speciﬁcation, usually provided as a model or
a grammar. For example, if the target application is an inter-
preter, the underlying grammar of the programming language
can be used to generate syntactically valid input. This allows
them to pass complex input processing, while semantic checks
remain challenging for these approaches. Furthermore, many
generation-based fuzzers not only require a grammar, but also a
corpus [36], [47]. Creating such a corpus may be cumbersome,
since the corpus should ideally contain valid as well as invalid
test cases, since together they can be recombined to valid,
crashing inputs [36].
Orthogonally, fuzzers generally can be divided into black-
box and white-box fuzzers. While black-box fuzzers do not
require insight in an application, only needing a (large) cor-
pus, white-box fuzzers leverage extensive instrumentation and
analysis techniques to overcome conditional branches and track
2
code paths taken. White-box approaches try to systematically
explore new code paths that are harder to ﬁnd for black-box
fuzzers, however, increasing analysis also induces a decreasing
number of test cycles per second.
B. Context-Free Grammars
Applications often require highly-structured input, which
a conventional mutation-based fuzzer cannot easily provide.
Context-free grammars (CFGs) are well-suited to specify
highly structured input languages. Here we give a short deﬁ-
nition of CFGs and an introduction to how they can be used
to describe input languages. Intuitively, a CFG is a set of
production rules of the form “Some variable X (non-terminal
symbol) can be replaced by the following array of strings
(terminal-symbols) and variables”. Additionally, a special start
non-terminal speciﬁes where to begin applying these rules. The
input language described by the CFG is the set of all strings
that can be derived by applying any number of rules until no
more non-terminals are present.
More formally, a CFG is deﬁned as a tuple: G =
(N, T, R, S) with:
• N is a ﬁnite set of non-terminals. Non-terminals can
be thought of as intermediate states of the language
speciﬁcation.
• T is a ﬁnite set of terminal symbols. N and T are disjoint.
• R is a ﬁnite set containing the production rules of the
form A → a where A ∈ N and a ∈∗ T ∪ N
• S ∈ N is a non-terminal which is the start symbol. Every
word generated by the CFG needs to be derivable from
S.
Since the left-hand side of each rule consists of exactly one
non-terminal, the possible derivations only depend on one non-
terminal and no context, therefore these grammars are called
context free.
To derive a string, a matching production rule, i.e., one
with the respective non-terminal on the left-hand side, has to
be applied to the start symbol S. As long as the right-hand
side of that rule contains a non-terminal, another derivation
step is executed. For each step, one non-terminal is replaced
by the right-hand side of a rule matching the non-terminal.
Example II.1 shows a possible input generation given a
grammar G1.
Example II.1. Consider the following grammar (G1):
N: {PROG, STMNT, EXPR , VAR, NUMBER}
T : {a , 1 , 2 , = , return 1}
R : {
3
PROG → STMT
PROG → STMT ; PROG
STMT → return 1
STMT → VAR = EXPR
VAR → a
EXPR → NUMBER
EXPR → EXPR + EXPR
NUMBER → 1
NUMBER → 2
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
}
S : PROG
Therefore, one possible derivation from G1 would be: PROG
(1)−−→ STMT (4)−−→ VAR = EXPR (5)−−→ a = EXPR (6)−−→ a = NUMBER
(8)−−→ a = 1. Numbers over arrows denote applied production
rules. The derived string is a=1.
Each string generated by a CFG can be represented by its
derivation tree. A derivation tree t of a CFG G is a tree whose
vertices are labeled by either non-terminals or terminals. The
root of t is labeled with the start symbol, all terminal vertices
are labeled with terminals from G [52]. NAUTILUS mostly
operates on these derivation trees instead of the trees’ string
representation which we call unparsed strings. Derivation trees
are NAUTILUS’s internal representation for inputs to which
it applies structural mutations. However, as many common
language constructs are not context free (e.g., checksums, or
generating proper XML as the opening and the closing tags
need to contain the same identiﬁer), we extend upon CFGs by
allowing additional scripts to be used to transform the input.
Since the set of production rules must contain all (rele-
vant) non-terminals and terminals, in the following we deﬁne
CFGs only through their production rules and a start symbol.
To distinguish between non-terminals and terminals, we use
uppercase names for non-terminals.
III. CHALLENGES
Designing a fuzzer requires thorough consideration in order
to minimize the effort required from the user and maximize
effectiveness of the fuzzer. In particular, we identiﬁed four key
aspects that are desirable:
C1: Generation of syntactically and semantically valid in-
puts. Generated inputs need to pass the syntactic and semantic
checks of the target application to reach the next stages of
computation. The subset of syntactically and semantically valid
inputs is usually much smaller than the set of all possible
inputs [49]. Therefore,
is often hard for fuzzers to go
“deeper” and ﬁnd bugs in the application logic that is guarded