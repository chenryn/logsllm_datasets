title:Pyramid: Enhancing Selectivity in Big Data Protection with Count Featurization
author:Mathias L&apos;ecuyer and
Riley Spahn and
Roxana Geambasu and
Tzu-Kuo Huang and
Siddhartha Sen
2017 IEEE Symposium on Security and Privacy
Pyramid: Enhancing Selectivity in Big Data Protection
with Count Featurization
Mathias Lecuyer∗1, Riley Spahn∗1, Roxana Geambasu1, Tzu-Kuo Huang†2, and Siddhartha Sen3
1Columbia University, 2Uber Advanced Technologies Group, and 3Microsoft Research
Abstract—Protecting vast quantities of data poses a
daunting challenge for the growing number of organiza-
tions that collect, stockpile, and monetize it. The ability to
distinguish data that is actually needed from data collected
“just in case” would help these organizations to limit the
latter’s exposure to attack. A natural approach might be to
monitor data use and retain only the working-set of in-use
data in accessible storage; unused data can be evicted to a
highly protected store. However, many of today’s big data
applications rely on machine learning (ML) workloads that
are periodically retrained by accessing, and thus exposing
to attack, the entire data store. Training set minimization
methods, such as count featurization, are often used to
limit the data needed to train ML workloads to improve
performance or scalability.
We present Pyramid, a limited-exposure data man-
agement system that builds upon count featurization to
enhance data protection. As such, Pyramid uniquely in-
troduces both the idea and proof-of-concept for leveraging
training set minimization methods to instill rigor and selec-
tivity into big data management. We integrated Pyramid
into Spark Velox, a framework for ML-based targeting
and personalization. We evaluate it on three applications
and show that Pyramid approaches state-of-the-art models
while training on less than 1% of the raw data.
I. Introduction
Driven by cheap storage and the immense perceived
potential of “big data,” both public and private sectors
are accumulating vast quantities of personal data: clicks,
locations, visited websites, social interactions, and more.
Data offers unique opportunities to improve personal
and business effectiveness. It can boost applications’
utility by personalizing their features; increase business
revenues via targeted product placement; improve social
processes such as healthcare, disaster response and crime
prevention. Its commercialization potential, whether real
or perceived, drives unprecedented efforts to grab and
store raw data resources that can later be mined for proﬁt.
“collect-everything” mentality
poses serious risks for organizations by exposing ex-
tensive data stores to external and internal attacks. The
hacking and exploiting of sensitive corporate and govern-
mental information have become commonplace [1], [2].
Privacy-transgressing employees have been discovered
snooping into data stores to spy on friends, family, and
job candidates [3], [4]. Although organizations strive
to restrict access to particularly sensitive data (such
as passwords, SSNs, emails, banking data), properly
managing access controls for diverse and potentially
sensitive information remains an unanswered problem.
Unfortunately,
this
∗First authors in alphabetical order.
†Work done while at Microsoft Research.
© 2017, Mathias Lecuyer. Under license to IEEE.
DOI 10.1109/SP.2017.60
78
Compounding this challenge is a signiﬁcant new
thrust
in the public and private spheres to integrate
data collected from multiple sources into a single, giant
repository (or “data lake”) and make that available to any
applications that might beneﬁt from it [5]–[7]. This prac-
tice magniﬁes the data exposure problem, transforming
big data into what some have called a “toxic asset” [8].
Our goal in this paper is to explore a more rigorous
and selective approach to big data protection. We hypoth-
esize that not all data that is collected and archived is, or
may ever be, needed or used. The ability to distinguish
data needed now or in the future from data collected “just
in case” could enable organizations to restrict the latter’s
exposure to attacks. For example, one could ship unused
data to a tightly controlled store, whose read accesses are
carefully mediated and audited. Turning this hypothesis
into a reality requires ﬁnding ways to: (1) minimize data
kept in the company’s widely-accessible data lakes, and
(2) avoid the need to access the controlled store to meet
current and evolving workload needs.
A natural approach might be to monitor data use and
retain only the working set of in-use data in accessible
storage; data unused for some time is evicted to the
protected store [9]. However, many of today’s big data
applications involve machine learning (ML) workloads
that are periodically retrained to incorporate new data,
resulting in frequent accesses to all data. How can we
determine and minimize the training set—the “working
set” for emerging ML workloads—to adopt a more
rigorous and selective approach to big data protection?
We observe that for ML workloads, signiﬁcant re-
search is devoted to limiting the amount of data required
for training. The reasons are many but typically do not
involve data protection. Rather, they include increasing
performance, dealing with sparsity, and limiting labeling
effort. Techniques such as dimensionality reduction [10],
feature hashing [11], vector quantization [12], and count
featurization [13] are routinely applied in practice to
reduce data dimensionality so models can be trained
on manageable training sets. Semi-supervised [14] and
active learning [15] reduce the amount of labeled data
needed for training when labeling requires manual effort.
Can such mechanisms also be used to limit exposure
of the data being collected? How can an organization
that already uses these methods develop a more robust
data protection architecture around them? What kinds of
protection guarantees can this architecture provide?
As a ﬁrst step to answering these questions, we present
Pyramid, a limited-exposure big-data management sys-
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
tem built around a speciﬁc training set minimization
method called count featurization [13], [16]–[18]. Also
called historical statistics, count featurization is a widely
used technique for reducing training times by feeding
ML algorithms with a limited subset of the collected data
combined (or featurized) with historical aggregates from
much larger amounts of data. The method is valuable
when features with strong predictive power are highly
dimensional, requiring large quantities of data (and large
amounts of time and resources) to be properly modeled.
Applications that use count featurization include targeted
advertising, recommender systems, and content personal-
ization systems. Such applications rely on user informa-
tion to predict clicks, but since there can be hundreds of
millions of users, training can be very expensive without
some way to aggregate users, like count featurization.
The advertising systems at Microsoft, Facebook, and
Yahoo are all built upon this mechanism [19], and
Microsoft Azure offers it as a service [20].
individual observations (e.g.,
Pyramid builds on count featurization to construct
a selective data protection architecture that minimizes
exposure of
individual
clicks). To highlight, Pyramid: keeps a small, rolling
window of accessible raw data (the hot window); sum-
marizes the history with privacy-preserving aggregates
(called counts); trains application models with hot raw
data featurized with counts; and rolls over the counts to
forget all traces of observations past a speciﬁed retention
period. Counts are infused with differentially private
noise [21] to protect individual observations that are no
longer in the hot window but still fall within the retention
period. Counts can support modiﬁcations and additions
of many (but not all) types of models; historical raw data,
which may be needed for workloads not supported by
count featurization, is kept in an encrypted store whose
decryption requires special access.
While count featurization is not new, our paper is the
ﬁrst to retroﬁt it for data protection. Doing so raises
signiﬁcant challenges. We ﬁrst need to deﬁne meaning-
ful requirements and protection guarantees that can be
achieved with this mechanism, such as the amount of
exposed information or the granularity of protection. We
then need to achieve these protection guarantees without
affecting model accuracy and scalability, despite using
much less raw data. Finally, to make the historical raw
data store easier to protect, we need to access it as little
as possible. This means supporting workload evolution,
such as parameter tuning or trying new algorithms,
without the need to go back to historical raw data store.
We overcome these challenges with three main tech-
niques: (1) weighted noise infusion, which automatically
shares the privacy budget to give noise-sensitive fea-
tures less noise; (2) an unbiased private count-median
sketch, a data structure akin to a count-min sketch that
resolves the large negative bias arising from applying
differentially private noise to a count-min sketch; and
(3) automatic count selection, which detects potentially
useful groups of features to count together, to avoid
accesses to the historical data. Together, these techniques
reduce the impact of differentially private noise and
count featurization.
We built Pyramid and integrated it into Spark Velox,
a targeting and personalization framework, to add rigor
and selectivity to its data management. We evaluated
three applications: a targeted advertising system using
the Criteo dataset, a movie recommender using the
MovieLens dataset, and MSN’s production news per-
sonalization system. Results show that: (1) Pyramid
approaches state-of-the-art models while training on less
than 1% of the raw data. (2) Protecting historical counts
with differential privacy has only 2% impact on accu-
racy. (3) Pyramid adds just 5% performance overhead.
Overall, we make the following contributions:
1) Formulating the selective data protection problem
for emerging ML workloads as a training set min-
imization problem, for which many mechanisms
already exist.
2) The design of Pyramid,
the ﬁrst selective data
management system that minimizes data exposure
in anticipation of attack. Built upon count featur-
ization, Pyramid is particularly suited for targeting
and personalization workloads.
4) Pyramid’s
3) A set of new techniques to balance solid protection
guarantees with model accuracy and scalability,
such as our unbiased private count-median sketches.
into Spark
code,
ready
as
targeting/personalization
in other
https://columbia.github.
Velox
and
to integrate
frameworks.
io/selective-data-systems/
both
a
integrated
stand-alone
library
II. Motivation and Goals
This paper argues for needs-based selectivity in big
data protection: protecting data differently depending
on whether or not it is actually needed to handle a
company’s day-to-day workloads. Intuitively, data that is
needed day-to-day is less amenable to certain kinds of
protection (e.g., auditing or case-by-case access control)
than data needed only for exceptional situations. A key
question is whether a company’s day-to-day needs can
be captured with a limited and well-deﬁned data subset.
While we do not claim to answer this question in full, we
present with Pyramid the ﬁrst evidence that selectivity
can be achieved in one important big-data workload
domain: ML-based targeting and personalization. The
following scenario motivates selectivity and shows how
and in what contexts Pyramid helps improve protection.
79
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:25:23 UTC from IEEE Xplore.  Restrictions apply. 
II.A. Example Use Case
MediaCo, a media conglomerate, collects observations
of user behavior from its hundreds of afﬁliate news
and entertainment sites. Observations include the articles
users read and share, the ads they click, and how they re-
spond to A/B testing. MediaCo uses this data to optimize
various processes, including recommending articles to
users, showing the most relevant articles ﬁrst, and target-
ing ads. Initially, MediaCo collected observations from
afﬁliate sites in separate, isolated repositories; different
engineering teams used different repos to optimize these
processes for each afﬁliate site. Recently, MediaCo has
started to track users across sites using cookies and to
integrate all data into a central data lake. Excited about
the potential of the much richer information in the data
lake, MediaCo plans to provide indiscriminate access to
all engineers. However, aware of recent external hacking
and insider attacks affecting other companies, it worries
about the risks it assumes with such wide access.
MediaCo decides to use Pyramid to limit the expo-
sure of historical observations in anticipation of such
attacks. For MediaCo’s main workloads, which consist
of targeting and personalization, the company already
uses count featurization to address sparsity challenges;
hence, Pyramid is directly applicable for those work-
loads. They conﬁgure it by keeping Pyramid’s hot win-
dow of raw observations, along with its noise-infused
historical statistics, in the widely accessible data lake
so all engineers can train their models, tune them, and
explore new algorithms every day. Pyramid absorbs
many workload needs—current and evolving—as long
as the algorithms draw on the same user data to predict
the same outcome (e.g., whether a user will click on
an ad). MediaCo also conﬁgures a one-year retention
period for all observations; after this period, Pyramid
removes observations from the statistics and launches
retraining of all application models to purge the old