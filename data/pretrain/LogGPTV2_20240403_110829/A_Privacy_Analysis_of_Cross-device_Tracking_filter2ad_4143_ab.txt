experiment in the cross-device space. However, we caution
that we leave a comprehensive analysis, which was done in
other areas of online tracking [54,55], for further research.
Establishing an IP Address Connection. We began our
experiment by connecting two devices—a desktop and a
mobile device—to the same router and modem. Using a
fresh desktop browser without any user data we visited the
homepages of five randomly selected news websites from
the Alexa rankings [4]—aol.com, latimes.com, nytimes.com,
wsj.com, and washingtonpost.com (the test homepages). We
refreshed each test homepage ten times, as recommended
for these type of information flow experiments [81], and
observed the ads that were served. We also set up a desktop
device with a fresh browser connected to a different router
and modem as control instance. In the following two months
we occasionally and randomly visited 100 highly ranked
homepages [4] on our fresh mobile browser.
Observing Cross-device Ads. After two months we used
the mobile browser to visit the websites shown in Figure 2.
We searched Google for various consumer products and
clicked on ads served for those on the search results pages.
After a few hours we switched to our desktop browser and
accessed the test homepages. We refreshed each ten times.
Some of the served ads, which we had not seen before, were
for products we had searched for on the mobile device. Fig-
ure 3 shows the ads and associated information, that is, the
1394    26th USENIX Security Symposium
USENIX Association
A. PetSmartnytimes.comadsense.comGoogle AdSenseGoogle Display NetworkB. Miele/Abtlatimes.comas.chango.comRubicon Project TapadC. Kate Spadeaol.comredirectingat.comSkimlinks Lotamename of the ad (e.g., PetSmart), the domain on which it was
served (e.g., nytimes.com), the domain of the ad server (e.g.,
adsense.com), the ad network serving the ad (e.g., Google Ad-
Sense), and the involved cross-device tracking provider (e.g.,
Google Display Network).4 Our results suggest that the ad
networks serving the ads had learned that the user who did the
search on the phone was the same as the user on the desktop.
To assess the similarity of ads we categorized each ad
according to Google ad categories [35]. Then, based on an
exact one-tailed permutation test, as recommended [81], we
compared the ad distribution served on the desktop browser
to the ad distribution served on the desktop control browser.
We evaluated the null hypothesis that both distributions do
not differ from each other at the 0.05 significance level.
However, the result of p = 0.02 indicates that the null
hypothesis should be rejected and that the deviation of both
distributions is statistically significant at the 0.05 level. This
finding suggests that we successfully identified instances of
cross-device tracking. We also found mobile cookie syncing
between Rubicon Project and Tapad. However, confirming
earlier observations [11], we did not detect any cookie
syncing across devices.
Direction of Ad Serving and App-Web Correlation. In
addition to cross-device tracking from mobile to desktop
we were further interested in the reverse direction. However,
searching Google on our desktop for buying products did not
seem to lead to ads for these products on our mobile browser.
One explanation might be that the ad serving was limited
to one direction—from mobile to desktop— as users tend to
move from a smaller to a larger screen [33,37]. Another ex-
planation could be that ad networks attached more weight to
the history on the device to which an ad was served and less
to other connected devices. Further, we might simply have
missed all cross-device campaigns at the time for the prod-
ucts we searched for. Finally, we were not able to notice any
correlation in ad serving in either direction when repeating
our experiment with mobile apps instead of websites.
4 The Cross-device Tracking Dataset
A major reason for the scarcity of academic research in
cross-device tracking is the unavailability of data. Generally,
only proprietary industry data exists.5 Thus, we collected
our own cross-device tracking dataset (the CDT dataset).
Here we describe how we collected the data and highlight
cross-device usage patterns of the users in the dataset.
4We assume that the Google Display Network covers sites using
AdSense, DoubleClick, Blogger, YouTube, and AdMob. On one side, this
is likely an overestimation as not all sites using these trackers are part of
the Google Display Network. On the other side, it is an underestimation
as there are sites that are part of it, however, not using any of the trackers.
In total, the Google Display Network covers over two million sites [34].
5The Drawbridge dataset [45] was only accessible to participants of the
Drawbridge competition [23] and limited in its use for that purpose.
Desktop Web Mobile Web Mobile Apps
Users
IPs
Domains
125
1,994
23,517
102
3,876
5,784
104
845
Table 1: Summary statistics showing the total number of unique
users, IP addresses, and domains in the CDT dataset.
Desktop Web
25th, 50th, 75th
Mobile Web
25th, 50th, 75th
Mobile Apps
25th, 50th, 75th
Days
IPs
Domains
19, 22, 26
6, 17, 24
149, 251, 374
9, 17, 23
19, 22, 24
25, 63, 92
9, 31, 70
19, 30, 44
Table 2: Summary statistics for the CDT dataset per user showing
the unique values at the 25th, 50th, 75th percentiles. The data
was collected for the same continuous time period for every user.
However, not every user made use of his or her devices every day.
Data Collection Procedure. Before we began the data col-
lection we obtained approval from Columbia University’s In-
stitutional Review Board (IRB). We built our data collection
system such that interested users could sign up on our project
website, at which point we also took a device fingerprint for
each signed up device. We asked users to supply basic infor-
mation on their demographics (e.g., age and gender), interests
(e.g., finance, games, shopping) [36], and personas (e.g., avid
runners, bookworms, pet owners) [84]. In order to capture
users’ mobile and desktop history we provided them with
browser extensions and an Android app that we developed
for automatically collecting such information.6 Details on the
types of information that we collected are contained in Ap-
pendix A. We do not have any indication that users behaved
differently in our study than under real-world conditions.
We only signed up users of Android phones with
Android’s native browser, Google Chrome, or the Samsung
S-Browser. We did not support iOS or other operating
systems. Our app requires Android 4.0.3 and runs without
root access. Every minute it checks for a new foreground app
running on the device as well as new entries in the browsing
history database of the phones’ browsers. If new apps or
URLs are detected, a new history datapoint is transmitted
to our server.7 On the desktop side we provided users of all
operating systems with data collection browser extensions
for Google Chrome, Mozilla Firefox, and Opera. At the
conclusion of the study we rewarded each user with an
Amazon gift card for $15 to $50 depending on the amount
of data we received from them.
Dataset Characteristics. We collected data from 126 users.
Tables 1 and 2 show further details. We signed up 125
desktop and 108 mobile users with an intersection of 107
6When we refer to desktops, we include laptops but exclude tablets.
7For some users with Google Chrome and Android 6.0 or higher we
did not receive the full browsing history due to browser restrictions. We
asked affected users to send us their history manually.
USENIX Association
26th USENIX Security Symposium    1395
users from whom we obtained both mobile and desktop data.
While our data faithfully represents that not every Internet
user has multiple devices, it does not reflect that users in the
real world can have more than two devices. However, despite
this limitation we believe that our dataset is generally an
accurate reflection of real multi-device usage on the Internet
because the vast majority of mobile devices is associated with
only one desktop browser [71]. Therefore, it seems plausible
to adopt this understanding of the problem here as well.
Further, only 3/108 (3%) of mobile users and 4/125 (3%)
of desktop users in our study reported that they are sharing
their devices. As this result seems in line with findings that
phones are never shared for mutual use and that computers
are only shared for a moderate amount [58], it appears that
our data is a realistic representation in this regard.
118 users in our study were affiliates of Columbia Uni-
versity, mostly students. Based on this population we believe
that our data is more homogeneous than a data from, say, the
general population of New York City. However, we also note
that our users are less likely to encounter typical restrictions
of device use that many employees face in the workplace,
e.g., corporate networks blocking certain websites. For the
median user we collected about three weeks of data of which
IP addresses and domains are of particular importance for
probabilistic cross-device tracking because they can be used
to measure the similarity between devices (§ 5.2).
It is noteworthy that the total unique mobile IP count
(5,784) is nearly three times the total unique desktop IP
count (1,994), which reflects mobile usage on the go. It
should be noted, though, that the real unique mobile IP
count is likely even higher as our method did not allow us
to collect mobile IPs with every datapoint. However, the
high number of unique desktop domains (23,517), compared
to the homogeneous usage of apps (845), underscores the
diversity of desktop browsing. While it is much more diverse
in terms of domains (3,876), mobile web usage pales in
comparison to app usage. As shown by the 25th, 50th, and
75th percentiles, the median user accessed the mobile web
only for 17 days visiting only 31 unique domains.8 While
app usage is more popular with a median of 22 days, the
median usage of 30 unique apps is comparable to that of the
mobile web. However, the median number of unique mobile
IP addresses (63) more than triples desktop IP addresses (17).
Figure 4 shows that many users visit a relatively large
number of unique mobile device IP addresses and desktop
web domains. However, there does not seem to be a
correlation between desktop and mobile devices to the effect
that lower usage of one would imply more usage of the other
or that both are used to an equal degree.
8A day was counted if a user’s device had at least one desktop
web, mobile web, or app access on a given day. Also, uniqueness of
a domain is dependent on its top and second level. Thus, for example,
we treat facebook.com and linkedin.com as different domains, however,
linkedin.com and blog.linkedin.com as the same domain.
Figure 4: Unique IP address (top) and web domain (bottom) count
for each user in our dataset for whom we had both mobile and
desktop data. For example, Peggy has 82 unique mobile and 35
unique desktop IP addresses (top). To the right of Peggy about two
thirds of users visited fewer than 56 unique mobile domains and
to the right of Don about a fourth visited fewer than ten (bottom).
5 Methods for Cross-device Tracking
How cross-device companies operate is not known in
detail [49].
In order to get an understanding of their
capabilities we designed an algorithm and evaluated features
and parameters informed by a review of public materials,
particularly, Adelphic’s cross-device patent [83] and Tapad’s
patent application for managing associations between device
identifiers [80]. Essentially, cross-device tracking is based
on resolving two tasks: first, uniquely identifying users’
devices (§ 5.1), and, second, correlating those that belong
to the same user (§ 5.2).
5.1 Identifying Devices
Traditionally, HTTP cookies are used to identify desktop
devices. Indeed, many cross-device companies are employ-
ing cookies for their tracking purposes as well. For mobile
devices the use of advertising identifiers, such as Google’s
Advertising ID (AdID), is common and often combined
with cookie tracking. Thus, if users are allowing cookies
and do not opt out from being tracked, both their mobile
and desktop devices can be easily identified. However,
with the surge of tracking- and ad-blocking software, which
some consider a mainstream technology on mobile by
now [68], unconventional identification technologies, such
as device fingerprinting, are becoming more prevalent.
While it does not appear that they will generally replace
cookies and advertising identifiers any time soon, various
cross-device companies—for example, BlueCava [9] and
AdTruth [28]—are making use of device fingerprinting.
1396    26th USENIX Security Symposium
USENIX Association
050100150200250PeggyDonUserUnique IP Address CountTypeDesktop IPMobile IP02505007501000PeggyDonUserUnique Web Domain CountTypeDesktop WebMobile Web<56<10User Agent
Display Size/Colors
Fonts
Accept Headers
System Language
Time Zone
Mobile Carrier
Do Not Track Enabled
Geolocation Enabled
Touch Enabled
Total per Device Type
Total
Desk Devices
H, Hn, ˆH
4.46, 0.64, 4.96
5.34, 0.77, 6.08
6.11, 0.88, 7.33
2.86, 0.41, 3.29
0.41, 0.06, 0.51
0.25, 0.04, 0.35
N/A
0.67, 0.1, 0.67
0.45, 0.07, 0.45
0.72, 0.1, 0.72
6.96, 1, 12.95
Mob Devices
H, Hn, ˆH
6.43, 0.95, 8.5
1.72, 0.25, 2.08
1.21, 0.18, 1.33
2.34, 0.35, 3
0.81, 0.12, 1
0.53, 0.07, 0.74
1.39, 0.21, 1.45
0.18, 0.03, 0.19
1, 0.15, 1
N/A
6.69, 0.99, 10.87
7.84, 1, 13.37
Table 3: Entropy (H), normalized entropy (Hn), and estimated
entropy ( ˆH) for various browser features in our CDT dataset. The
normalized entropy ranges from 0 (all features are the same) to
1 (all features are different). We calculated the estimated entropy
according to Chao and Shen [16]. For the totals we considered
all listed features. Overall, our dataset contains 3 duplicate mobile
fingerprints and 1 duplicate desktop fingerprint.
Cross-device companies that are solely relying on device
fingerprinting must be able to identify both desktop and
mobile devices using this technique. While it was reported
that device fingerprints generally do not work well on
mobile devices [25], our results do not support such broad
conclusion. Particularly, mobile user agents often contain
distinctive features and are far more diverse (6.43 bits) than
user agents on desktops (4.46 bits). Also, the entropy in
our dataset only represents a lower bound as we imposed
substantial limitations for users’ participation in our study;
most notably, requiring them to have an Android phone with
Android 4.0.3 or higher and use the native browser, Chrome,
or S-Browser. We also did not consider, for instance, canvas
fingerprinting [1], sensor data [19], or the order in which
fonts and plugins were detected [25]. However, most mobile
devices in our dataset were still identifiable. The detailed
findings for the 107 mobile and 126 desktop devices in our
CDT dataset are shown in Table 3.9 Due to the small size
of our dataset we caution to interpret our results as indicative
for the reliability of mobile device fingerprinting, though.
5.2 Correlating Devices
After uniquely identifying each device cross-device com-
panies must match those that appear similar. Successfully
matching devices at scale is the core challenge for cross-
device companies. Devices are represented in graphs known
as Device Graphs [76], Connected Consumer Graphs [22], or
under similar proprietary monikers. From a graph-theoretical
perspective a device graph can be built from connected
9One user did not submit a mobile fingerprint and another user
submitted two different desktop fingerprints.
Figure 5: Our cross-device tracking approach. A. First, a mobile de-
vice is identified. B. Its similarity to each identified desktop device,
s, is calculated. C. The mobile-desktop pair with the maximum
similarity, max, that is above a similarity threshold, t, is determined,
if any. D. If such pair exists, it is added to the device graph and
the next iteration starts with a new mobile device. This routine is
repeated in three consecutive stages each evaluating similarities
between mobile and desktop IP addresses, mobile and desktop
URLs, and mobile apps and desktop URLs, respectively. If a mobile
device cannot be matched in one stage due to not overcoming the
similarity threshold, a match is attempted in the next.
components (each of which represents a user) with a
maximum number of vertices (devices) and edges (device
connections) [18]. Matching every mobile with exactly one
desktop device will result in a bipartite graph. The goal is
to achieve a perfect matching of similar devices.
Algorithm, Features, and Parameters. While determin-
istic cross-device companies can simply match a user’s
devices based on his or her login information, which may
also extend towards third party properties through single
sign-on functionality, achieving a high match rate is more
difficult for probabilistic cross-device tracking companies.
In the Drawbridge competition [23] many participants
applied gradient boosting [48, 50, 53, 62, 71]. However,
some participants also combined support vector machines
and factorization models into field-aware factorization
machines [75] or employed pairwise ranking and ensemble
learning techniques [14]. Interestingly, the best performing
solution relied on learning-to-rank models instead of using
the more conventional binary classification models [82].
In our approach, as outlined in Figure 5, we determine the
similarity between devices based on distance metrics, most
notably, the Bhattacharyya coefficient, which is defined for
the distributions p and q as Bhatta(p,q)=∑x∈X
The use of distance metrics for device correlation was
described in Adelphic’s cross-device patent [83]. Our
cross-device tracking algorithm works in multiple stages.
Using a key insight from the patent, for each feature a
(cid:112)p(x)q(x).
USENIX Association
26th USENIX Security Symposium    1397
Stage 1