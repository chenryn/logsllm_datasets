i=1
i=1
aikail
(cid:5)
n(cid:2)
n(cid:2)
n(cid:2)
n(cid:2)
(cid:5)
n(cid:2)
n(cid:2)
(rik − μi)(ril − μi) − ηk
(cid:5)
n(cid:2)
a2
ik
a2
il
i=1
i=1
i=1
n(cid:2)
(rik − μi − ηk + g)(ril − μi − ηl + g)
(rik − μi − ηk + g)2
(ril − μi − ηl + g)2
n(cid:2)
n(cid:2)
i=1
(ril − μi) − ηl
n(cid:2)
=
i=1
i=1
(rik − μi)2 − 2(ηk − g)
n(cid:2)
(cid:6)
= Skl − ηkSl − ηlSk + g(Sk + Sl) − g(ηk + ηl)N + ηkηlN + g2N
Sll − 2(ηl − g)Sl + (ηl − g)2N
(rik − μi) + (ηk − g)2N
(cid:6)
Skk − 2(ηk − g)Sk + (ηk − g)2N
= Learn(Skl, Sk, Sl, Skk, Sll, g, ηk, ηl)
i=1
i=1
i=1
i=1
(cid:5)
(rik − μi) − g(ηk + ηl)N + ηkηlN + g2N + g
(ril − μi)2 − 2(ηl − g)
(ril − μi) + (ηl − g)2N
n(cid:2)
i=1
n(cid:2)
i=1
(rik + ril − 2μi)
(4)
(cid:3) User u
(cid:3) Item t
(cid:3) Rating rut
Algorithm 2 Unlearning Stage in LensKit
Input:
u : the user who wants to delete a rating for an item
t : the item, of which the user wants to delete the rating
rut : the original rating that the user gives
− 1)
− 1)
if ruj (cid:4)= null && j (cid:4)= t then
Process:
1: oldμu ← μu
2: μu ← (μu ∗ Countμu
− rut)/(Countμu
3: ηt ← (ηt ∗ Countηt
− rut)/(Countηt
4: g ← (g ∗ Countg − rut)/(Countg − 1)
5: St ← St − (rut − oldμu)
6: St ← Stt − (rut − oldμu) ∗ (rut − oldμu)
7: for j = 1 to m do
8:
9:
10:
11:
12: end for
13: for k = 1 to m do
14:
15:
16:
17:
18:
19:
for l = 1 to m do
||
if j = t
end if
end if
end if
Update sim(k, l)
20:
21:
22:
23:
24: end for
end for
Sj ← Sj + oldμu − μu
Sjj ← Sjj−(ruj−oldμu)∗(ruj−oldμu)+(ruj−μu)∗(ruj−μu)
l = t then
if ruk (cid:4)= null && rul (cid:4)= null && k (cid:4)= l then
Skl ← Skl − (ruk − oldμu) ∗ (rul − oldμu)
Skl ← Skl − (ruk − oldμu) ∗ (rul − oldμu) + (ruk − μu) ∗
(rul − μu)
else
average of all ratings (g, line 15) by tracking the total number
of ratings (Countg, line 10) and the sum of them (Sumg, line
9). In addition, it computes additional summations Sk and Skl
(line 19 and 23) required by Equation 4. Once all summations
are ready, it computes the similarity of each pair of items
following Equation 4. It then stores the summations μi and
countμi for each user, ηj and countηj for each item, g and
countg, Sk for each item, and Skl for each pair of items for
later unlearning.
Algorithm 2 is the core algorithm for unlearning in LensKit.
To forget a rating, it updates all relevant summations and
relevant cells in the item-item similarity matrix. Suppose that
user u asks the system to forget a rating she gave about
item t. Algorithm 2 ﬁrst updates user u’s average rating μi,
item t’s average rating ηj, and the global average rating g by
multiplying the previous value of the average rating with the
corresponding total number, subtracting the rating to forget
rut, and dividing by the new total number (lines 1–3). It then
472472
updates item t’s summations St and Stt by subtracting the
value contributed by rut which simpliﬁes to the assignments
shown on lines 4–5. It then updates Sj and Sjj (lines 6–11) for
each of the other items j that received a rating from user m.
Because the ratings given by the other users and their averages
do not change, Algorithm 2 subtracts the old value contributed
by user u and adds the updated value. Algorithm 2 updates
Sjk similarly (lines 12–20). Finally, it recomputes sim(j, k)
based on updated summations following Equation 4 (line 21).
Note that while these algorithms require additional n +
m2 + 2m space to store the summations, the original item-
item recommendation algorithm already uses O(nm) space
for the user-item rating matrix and O(m2) space for the item-
item similarity matrix. Thus, the asymptotic space complexity
remains unchanged.
C. Empirical Results
To modify LensKit to support unlearning, we have inserted
302 lines of code into nine ﬁles spanning over three LensKit
packages: lenskit-core, lenskit-knn, and lenskit-data-structures.
Empirically, we evaluated completeness using two sets of
experiments. First, for each data subset, we randomly chose
a rating to forget, ran both unlearning and retraining, and
compared the recommendation results for each user and the
item-item similarity matrices computed. We repeated this
experiment ten times and veriﬁed that in all experiments, the
recommendation results were always identical. In addition, the
maximum differences between the corresponding similarities
were less than 1.0× 10−6. These tiny differences were caused
by imprecision in ﬂoating point arithmetic.
Second, we veriﬁed that unlearning successfully prevented
the aforementioned system inference attack [29] from gaining
any information about the forgotten rating. After unlearning,
LensKit gave exactly the same recommendations as if the
forgotten rating had never existed in the system. When we
launched the attack, the delta matrices (§IV in [29]) used in the
attack contained all zeros, so the attacker cannot infer anything
from these matrices.
We evaluated timeliness by measuring the time it
took
three data subsets and
to unlearn or retrain. We used all
repeated each experiment
three times. Table II shows the
results. The ﬁrst row shows the time of retraining, and the
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: Speedup of unlearning over retraining for LensKit. The
time of retraining increases by the factor of the number of total
ratings, and the overhead of unlearning increases by the factor of
the number of total users.
100K Ratings from 1M Ratings from 10M Ratings from
1,000 users &
1,700 items
6,000 users &
4,000 items
72,000 users &
10,000 items
Retraining 4.2s
Unlearning931ms
Speedup
4.51
30s
6.1s
4.91
4min56s
45s
6.57
second row the time of unlearning, and the last row the
speedup of unlearning over retraining. Unlearning consistently
outperforms retraining. The speedup factor is less than the
O(n) analytical results because there are many empty ratings
in the data set, i.e., a user does not give ratings for every movie.
Therefore, the retraining speed is closer to O(N m), and the
speedup factor is closer to O(N/m), where N is the number
of ratings and m is the number of users. For a larger data set,
the speedup may be even larger. For example, IMDb contains
2,950,317 titles (including TV shows, movies, etc.) and 54
million registered users [9, 19], which may produce billions
or even trillions of ratings. In that case, the unlearning may
take several hours to complete, while the retraining may take
several days.
VII. UNLEARNING IN ZOZZLE
We start by describing Zozzle’s JavaScript malware de-
tection algorithm. Zozzle ﬁrst extracts the structure of the
JavaScript abstract syntax tree (AST) nodes and uses chi-
squared test to select representative features. The chi-squared
test is shown in Equation 6, where N+,F means the number
of malicious samples with feature F , N−,F benign samples
with F , N+, ˆF malicious samples without F , and N−, ˆF benign
samples without F .
2 =
χ
(N+,F N−,
)(N−,F + N−,
ˆ
F
F N−,F )2
− N+,
)(N+,F + N−,F )(N+,
ˆ
(6)
ˆ
F
ˆ
F
(N+,F + N+,
Then, Zozzle trains a na¨ıve Bayes classiﬁer using the se-
lected AST features, and then classiﬁes an incoming JavaScript
sample as malicious or benign. (The details of a na¨ıve Bayes
classiﬁer have been shown before in Equation 1.)
+ N−,
ˆ
F
)
ˆ
F
Because Zozzle is closed-source, to avoid any bias, we
ask Xiang Pan [21] to follow the original system [35] and
re-implement Zozzle. The re-implemented Zozzle uses the
Eclipse JavaScript development tools [11] to generate ASTs
of JavaScript and extract the corresponding features into a
MySQL database. Then, it performs chi-squared test with the
same threshold as in the original system (10.83, which “cor-
responds with a 99.9% conﬁdence that the values of feature
presence and script classiﬁcation are not independent” [35]) to
select features for the feature set. It also implements its own
na¨ıve Bayes classiﬁer with the same training steps.
473473
To test Zozzle, we used the following workload. We crawled
the top 10,000 Alexa web sites [2] to serve as the benign data
set. In addition, from Huawei, we obtained 142,350 JavaScript
malware samples, collected at their gateways or reported by
their customers; all malware samples were manually veriﬁed
by Huawei and cross-tested by multiple anti-virus software
systems. We divided the whole data set equally into ten parts,
nine of which are for training and the remaining one for
testing. After training, Zozzle selected 2,398 features in total,
out of which 1,196 are malicious features and 1,202 are benign
features. The detection rate is shown in the ﬁrst column of
Table III. The re-implemented Zozzle achieved a 93.1% true
positive rate and a 0.5% false positive rate, comparable to the
original Zozzle system.
A. The Attack – Training Data Pollution
To perform data pollution and inﬂuence the detection of
Zozzle, an attacker might craft malicious samples by injecting
features that do not appear in any benign samples. In such
case, those crafted malicious samples could be captured by
Zozzle’s ground-truth detector (such as Nozzle [59]), and
included in the training data set. The injected features (such
as an if statement with an unusual condition) in the crafted
malicious samples can inﬂuence both the feature selection
and the sample detection stage of Zozzle. In the feature
selection stage, the injected features are likely to be selected,
and inﬂuence existing malicious features so that they are less
likely to be picked. In the sample detection stage, the injected
features inﬂuence the decision for a true malicious sample.
First, because the injected features do not appear in benign
samples but only malicious samples, in the chi-squared test
(Equation 6), N+,F and N−, ˆF are large, and N−,F and N+, ˆF
are small. Therefore, the feature selection process is likely to
pick the injected features.
In addition, the injected features can make a real malicious
feature that would have been selected – Freal – less likely
to be selected. Because the attacker does not change any
benign training sample or remove the Freal in existing ma-
licious samples, but only add new samples, in the chi-squared
test, N+,Freal, N−,Freal and N−, ˆFreal
remain the same, and
increases. Therefore, the feature selection process is
N+, ˆFreal
less likely to pick up Freal as a malicious feature.
Second, the presence of an injected feature, Finject, lowers
the accuracy of the na¨ıve Bayes classiﬁer. Let us consider
how the detection of a sample with one malicious feature,
Fmal, is inﬂuenced in the presence of Finject – i.e., how the
value of P (+|Fmal) is lowered. Intuitively, since both Fmal
and Finject appear to be good indicators that a sample is
malicious, Zozzle splits the weight it were to place on Fmal