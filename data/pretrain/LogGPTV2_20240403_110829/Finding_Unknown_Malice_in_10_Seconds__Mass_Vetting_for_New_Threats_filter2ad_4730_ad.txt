However after running them on the apps, we found that
their v-cores, compared with those before the obfusca-
tion, did not change at all. This demonstrates that such
obfuscation is not effective on our view-graph approach.
On the other hand, we acknowledge that a new obfus-
cation tool could be built to defeat MassVet, particularly
its view-graph search and the Com step. The cost for do-
ing this, however, is less clear and needs further effort to
understand (Section 5).
3.4 System Building
In our research, we implemented a prototype of MassVet
using C and Python, nearly 1.2 million apps collected
from 33 markets, including over 400,000 from Google
Play (Section 4.1). Before these apps can be used to
vet new submissions, they need to be inspected to de-
tect malicious code already there. Analyzing apps of this
scale and utilizing them for a real-time vetting require
carefully designed techniques and a proper infrastructure
support, which we elaborate in this section.
System bootstrapping and malware detection. To
bootstrap our system, a market ﬁrst needs to go through
all its existing apps using our techniques in an efﬁ-
cient way. The APKs of these apps are decompiled into
smali (using the tool baksmali [36]3) to extract their
view graphs and individual methods, which are further
converted into v-cores and m-cores respectively. We use
NetworkX [29] to handle graphs and ﬁnd loops. Then
these features (i.e., cores) are sorted and indexed before
stored into their individual databases. In our implemen-
tation, such data are preserved using the Sqlite database
system, which is characterized by its small size and ef-
ﬁciency. For all these apps, 1.5 GB was generated for
v-cores and 97 GB for m-cores.
The next step is to scan all these 1.2 million apps for
malicious content. A straightforward approach is to in-
spect them one by one through the binary search. This
will take tens of millions of steps for comparisons and
analysis. Our implementation includes an efﬁcient al-
ternative. Speciﬁcally, on the v-core database, our sys-
tem goes through the whole sequence from one end (the
smallest element) to the other end, evaluating the ele-
ments along the way to form equivalent groups: all those
with identical v-cores are assigned to the same group4.
All the subgraphs within the same group match each
other. However, assembling them together to determine
the similarity between two apps turns out to be tricky.
This is because the UI of each app is typically broken
into around 20 subgraphs distributed across the whole
ordered v-core sequence. As such, any attempt to make
the comparison between two apps requires to go through
all equivalent groups. The fastest way to do that is to
maintain a table for the number of view subgraphs shared
between any two apps. However, this simple approach
requires a huge table, half of 1.2 million by 1.2 million in
the worst case, which cannot be completely loaded into
the memory. In our implementation, a trade-off is made
to save space by only inspecting 20,000 apps against the
rest 1.2 million each time, which requires going through
the equivalent groups for 60 times and uses about 100
GB memory at each round.
The inspection on m-cores is much simpler and does
not need to compare one app against all others. This
is because all we care about here are just the common
methods that already show up within individual equiva-
3Very few apps (0.01%) cannot be decompiled in our dataset due to
the limitation of the tool.
4In our implementation, we set the threshold τ to zero, which can
certainly be adjusted to tolerate some minor variations among similar
methods.
USENIX Association  
24th USENIX Security Symposium  667
9
Figure 3: Cloud framework for MassVet.
lent groups. Those methods are then further analyzed to
detect suspicious ones.
Cloud support. To support a high-performance vetting
of apps, MassVet is designed to work on the cloud, run-
ning on top of a stream processing framework (Figure 3).
Speciﬁcally, our implementation was built on Storm [40],
an open-source stream-processing engine that also pow-
ers leading web services such as WebMD, Alibaba, Yelp,
etc. Storm supports a large-scale analysis of a data
stream by a set of worker units that connect to each
other, forming a topology.
In our implementation, the
work ﬂow of the whole vetting process is converted into
such a topology: a submitted app is ﬁrst disassembled
to extract view graphs and methods, which are checked
against the white list to remove legitimate libraries and
templates; then, the app’s v-cores and m-cores are calcu-
lated, and a binary search on the v-core database is per-
formed; depending on the results of the search, the differ-
ential analysis is ﬁrst run, which can be followed by the
intersection analysis. Each operation here is delegated to
a worker unit on the topology and all the data associated
with the app are in a single stream. The Storm engine
is designed to support concurrently processing multiple
streams, which enables a market to efﬁciently vet a large
number of submissions.
4 Evaluation and Measurement
4.1 Setting the Stage
App collection. We collected 1.2 million apps from 33
Android markets, including over 400,000 from Google
Play, 596,437 from 28 app stores in China, 61,866 from
European stores and 27,047 from other US stores as elab-
orated in Table 5 in Appendix. We removed duplicated
apps according to their MD5. All the apps we down-
loaded from Google Play have complete meta data (up-
load date, size, number of downloads, developer, etc.),
while all those from third-party markets do not.
The apps from Google Play were selected from 42 cat-
egories, including Entertainment, Tools, Social, Com-
munication, etc. From each category, we ﬁrst went for
its top 500 popular ones (in terms of number of installs)
and then randomly picked up 1000 to 30,000 across the
whole category. For each third-party market, we just ran-
domly collected a set of apps (Table 5) (190 to 108,736,
depending on market sizes). Our collection includes
high-proﬁled apps such as Facebook, Skype, Yelp, Pin-
terest, WeChat, etc. and those less popular ones. Their
sizes range from less than 1 MB to more than 100 MB.
Validation. For the suspicious apps reported by our pro-
totype, we validated them through VirusTotal and man-
ual evaluations. Virustotal is the most powerful public
malware detection system, which is a collection of 54
anti-malware products, including the most high-proﬁle
commercial scanners. It also provides the scanning ser-
vice on mobile apps [44]. VirusTotal has two modes,
complete scanning (which we call new scan) and using
cached results (called cached scan). The latter is fast,
capable of going through 200 apps every minute, but
only covers those that have been scanned before. For
the programs it has never seen or uploaded only recently,
the outcome is just “unknown”. The former determines
whether an app is malicious by running on it all 54 scan-
ners integrated within VirusTotal. The result is more up-
to-date but the operation is much slower, taking 5 min-
utes for each app.
To validate tens of thousands suspicious cases detected
from the 1.2 million apps (Section 4.2), we ﬁrst per-
formed the cached scan to conﬁrm that most of our ﬁnd-
ings were indeed malicious. The apps reported to be “un-
known” were further randomly sampled for a new scan.
For all the apps that VirusTotal did not ﬁnd malicious,
we further picked up a few samples for a manual anal-
ysis. Particularly, for all suspicious apps identiﬁed by
the intersection analysis, we clustered them according to
their shared code. Within each cluster, whenever we ob-
served that most members were conﬁrmed malware, we
concluded that highly likely the remaining apps there are
also suspicious, even if they were not ﬂagged by Virus-
Total. The common code of these apps were further
inspected for suspicious activities such as information
leaks. A similar approach was employed to understand
the diff code extracted during the differential analysis.
We manually identiﬁed the activities performed by the
code and labeled it as suspicious when they could lead to
damages to the user’s information assets.
4.2 Effectiveness and Performance
Malware found and coverage. From our collection,
MassVet reported 127,429 suspicious apps (10.93%).
10,202 of them were caught by “Diff” and the rest
were discovered by “Com”. These suspicious apps are
from different markets: 30,552 from Google Play and
96,877 from the third-party markets, as illustrated in Ta-
ble 5. We ﬁrst validated these ﬁndings by uploading them
to VirusTotal for a cached scan (i.e., quickly checking
the apps against the checksums cached by VirusTotal),
668  24th USENIX Security Symposium 
USENIX Association
10
AV Name
Ours (MassVet)
ESET-NOD32
VIPRE
NANO-Antivirus
AVware
Avira
Fortinet
AntiVir
Ikarus
TrendMicro-HouseCall
F-Prot
Sophos
McAfee
# of Detection
197
171
136
120
87
79
71
60
60
59
47
46
45
% Percentage
70.11
60.85
48.40
42.70
30.96
28.11
25.27
21.35
21.35
21.00
16.73
16.37
16.01
Table 1: The coverages of other leading AV scanners.
which came back with 91,648 conﬁrmed cases (72%),
17,061 presumably false positives (13.38%, that is, the
apps whose checksums were in the cache but not found
to be malicious when they were scanned) and 13,492
unknown (10.59%, that is, the apps whose checksums
were not in VirusTotal’s cache). We further randomly
selected 2,486 samples from the unknown set and 1,045
from the “false-positive” set, and submitted to VirusTotal
again for a new scan (i.e., running all the scanners, with
the most up-to-date malware signatures, on the apps). It
turned out that 2,340 (94.12%) of unknown cases and
349 (33.40%) of “false positives” are actually malicious
apps, according to the new analysis. This gives us a false
detection rate (FDR: false positives vs. all detected) of
9.46% and a false positive rate (FPR: false positives vs.
all apps analyzed) of 1%, solely based upon VirusTotal’s
scan results. Note that the Com step found more mal-
ware than Diff, as Diff relies on the presence of two apps
of same repackaging origins in the dataset, while Com
only looks for common attack payloads shared among
apps. It turns out that many malicious-apps utilize same
malicious SDKs, which make them easier to catch.
We further randomly sampled 40 false positives re-
ported by the new scan for a manual validation and found
that 20 of them actually are highly suspicious. Specif-
ically, three of them load and execute suspicious code
dynamically; one takes pictures stealthily; one performs
sensitive operation to modify the booting sequence of
other apps; seven of them get sensitive user information
such as SIM card SN number and telephone number/ID;
several aggressive adware turn out to add phishing plug-
ins and app installation bars without the user’s consent.
The presence of these activities makes us believe that
very likely they are actually zero-day malware. We have
reported all of them to four Antivirus software vendors
such as Norton and F-Secure for a further analysis. If
all these cases are conﬁrmed, then the FDR of MassVet
could further be reduced to 4.73%.
To understand the coverage of our approach, we
randomly sampled 2,700 apps from Google Play and
scanned them using MassVet and the 54 scanners within
VirusTotal. All together, VirusTotal detected 281 apps
# Apps Pre-Processing v-core database differential m-core database
search (Intersection)
analysis
sum
search
0.15
0.15
0.14
0.16
0.16
10
50
100
200
500
1.80
1.99
2.23
3.13
3.56
0.33
0.34
0.35
0.35
0.35
5.84
5.85
5.85
5.88
5.88
8.12
8.33
8.57
9.52
9.95
Table 2: Performance: “Apps” here refers to the number of
concurrently submitted apps.
and among them our approach got 197 apps. The cover-
age of MassVet, with regard to the collective result of all
54 scanners, is 70.1%, better than what could be achieved
by any individual scanner integrated within VirusTo-
tal, including such top-of-the-line antivirus systems as
NOD32 (60.8%), Trend (21.0%), Symantec (5.3%) and
McAfee (16%). Most importantly, MassVet caught at
least 11% malware those scanners missed. The details
of the study are presented in Table 1 (top 12).
Vetting delay. We measured the performance of our
technique, on a server with 260 GB memory, 40 cores
at 2.8 GHz and 28 TB hard drives. Running on top of the
Storm stream processor, our prototype was tested against
1 to 500 concurrently submitted apps. The average delay
we observed is 9 seconds, from the submission of the app
to the completion of the whole process on it. This vetting
operation was performed against all 1.2 million apps.
Table 2 further shows the breakdown of the vetting
time at different vetting stages, including preprocessing
(v-core and m-core generation), search across the v-core
database, the differential analysis, search over the m-core
database and the intersection analysis. Overall, we show
that MassVet is indeed capable of scaling to the level of
real-world markets to provide a real-time vetting service.
4.3 Measurement and Findings
Over the 127,429 malicious apps detected in our study,
we performed a measurement study that brings to light
a few interesting observations important for understand-
ing the seriousness of the malware threat to the Android
ecosystem, as elaborated below.