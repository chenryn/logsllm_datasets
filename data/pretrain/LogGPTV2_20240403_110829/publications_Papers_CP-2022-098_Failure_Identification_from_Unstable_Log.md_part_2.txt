B. Key Observation Task ID 1: E2 E5 E3 E2 E6 E1E1 E9 E5 t 1,2,3â€¦time intervals
t t t
1 2 3
ThekeyobservationCLogreliesonisreducingtheentropy INPUT: S1 S2 S4 OUTPUT:
of the input representation of log sequences by representing Raw System Log Context-aware Failure Task ID 1 has
Log Messages Parsing Subprocess Extraction IdentificationFailed. The type
them as sequences of subprocesses instead of log events. For Sequence Sequence of of failure is, e.g.,
example, a sequence of log events s =(E ,E ,E ,E ,E ) of Events Subprocesses Failure Instance.
i 1 2 5 3 1
with a task ID i, where each of {E ,E ,E ,E } denotes
1 2 3 5 Fig.3. CLogoverview.
log event template, can equivalently be represented as se-
quence of two subprocesses/symbols s = (S ,S ) such that
i a b
A. Log Parsing
S = (E ,E ,E ), and S = (E ,E ), while S and S
a 1 2 5 b 3 1 a b
are referred to as subprocesses. Fig. 2 depicts the impact of The generated raw system logs are unstructured. Since we
changingthelogeventsequencerepresentationintosequences are interested in modeling the sequences of events, as the
of subprocesses on data from OpenStack. It is seen that with first step, we extract the event templates from the raw logs
increasingthegroupingwindowsize,thenumberoflogevents by applying automatic log parsing. Log parsing decouples the
in the subprocesses increases, and the average entropy over log templates from log parameters (the variable part in a log),
all the sequences in the data also increases (the triangles). directlyextractinginputinuseablemodelingformat.Wepref-
The entropy is the highest when the whole sequence is ered automatic log parsing because the alternatives (e.g., reg-
representedwithindividuallogevents(thediamond).Notably, ular expression and â€grockâ€ patterns), although successful for
when the sequences are represented with subprocesses, the parsing the templates, are system-specific, requiring frequent
number of symbols used to represent the sequences is smaller updates,whichmakesthemchallengingformaintenance[13].
(the circles), and the average entropy is reduced. It implies While there are many log parsers available, CLog utilizes a
a reduction of the sensitiveness over the individual logs, tree-based parser Drain [14]. Zhu et al. [13] identified Drain
effectivelyreducingtheeffectoftheinstabilityinthelogevent amongthemostefficientincomparisonto12otherparserson
sequences.AnimportantgoalofCLogistolearnsubprocesses ten benchmark datasets from diverse software systems. The
by preserving their characteristics, i.e., by learning context- three main properties making it popular and widely adopted
aware event sequence groups. are its correctness, efficiency and intuitive meaning of the
hyperparameters (making the tuning process undemanding). the sequence token representations to propagate through the
Onceparsed,theeventsareorganizedinsequencesbyCLogâ€™s upper layers in the network via the [LSE] token. Thereby,
hyperparameter window size, corresponding to the arriving [LSE] attends over all the tokens from the sequence and
time interval and task ID of the events, and are proceeded as summarizes the relevant context during learning. The [LSE]
output towards the context-aware subprocess extraction part. token serves as a sequence vector representation later used to
group contexts and identify subprocesses. The output of the
B. Context-aware Subprocess Extraction
padding module is the prepended and padded event sequence.
The context-aware subprocess extraction is the central part
Masking. To learn context-aware groups, we consider a
of the method. Its goal is the extraction of subprocesses from
general self-supervised learning task from natural language
the parsed log event sequences. By representing an execution
processing(NLP)researchcalledMaskedLanguageModeling
workload(withataskID)onahigher-levelgranularity,i.e.,by
(MLM)[15].ToapplytheMLMtask,themaskingcomponent
subprocesses, we reduce the entropy in the input, addressing
is processing the prepended and padded log event sequences
the problem of unstable log event sequences. This part com-
inasuitableformat.Morespecifically,asinput,itreceivesthe
bines context-aware neural network and clustering methods to
prepended and padded log event sequences and outputs a set
learnexplicitrelationshipsbetweentheeventswithintheevent
of pairs of masked log event sequences and original masked
sequences preserving their local properties. Fig. 4 depicts the
events. Masked log event sequences are sequences of log
overall design of the context-aware subprocess extraction part
events created by replacing all of the events from an original
witharunningexample.Conceptually,itiscomposedofthree
logsequencewithaspecial[M](masked)token.Forexample,
submodules â€“ (1) preprocessing submodule that transforms
for the input sequence (E ,E ,E ), one masked sequence is
2 5 3
the input sequences into a format suitable for learning, (2)
(E ,[M],E ),withE beingtheoriginalmaskedevent.There
2 3 5
neural network learning module which is combined with a
are three masked event sequences for this example. [LSE]
batched kmeans method to learn subprocesses in an unsuper-
and [PD] tokens are not affected by the masking procedure.
vised manner, (3) subprocess extraction module that assigns
During training, a masked sequence is given as input to the
a unique subprocess identifier to the input event sequence. In
neural network, while the original masked token is used as
the following, we describe the submodules.
the prediction target. By predicting the mask from the co-
occurringtokens,themethodlearnsthemostimportantevents
INUPT: ğ‘  ğ‘–: E2 E5 E3 Sequence of events from the surrounding context, extracting context-aware repre-
[LSE] E2 E5 E3 â€¦ [PD][PD] Padding
Preprocessing: ğ‘  ğ‘–ğ‘š 1:[LSE][M]E5 E3 â€¦ [PD][PD] sentations.Notethatbythisproceduresingleinputsequenceis
ğ‘  ğ‘–ğ‘š 2:[LSE] E2 [M]E3 â€¦ [PD][PD] Masking multipliedseveraltimes.Wekeeptrackoftheorigin(theinput
ğ‘  ğ‘–ğ‘š 3:[LSE] E2 E5[M]â€¦ [PD][PD] vector of size d eventsequence)ofeachmaskedsequenceanduseittoextract
Vectorizer E5: (0.42, 0.12 â€¦ 0.44) its corresponding subprocess identifier (see Section III-B3).
Neural Encoder Block (neural network with self-attention)ğœ½-parameters 2) Neural Network Submodule: The neural network sub-
SubN me otw duo lr ek [LSE] vector ğ‹ğ’”ğ’ğ’,ğ’“ğ’;Î¸
: Masked Subprocess modulelearnscontext-awaregroupsofmaskedlogsequences.
Output Layer ğœ½â€² ID assignment;
NNtargets: ğ‘  ğ‘–ğ‘š 1:E2 ğ‘  ğ‘–ğ‘š 2:E5 Suğ‘  bğ‘–ğ‘š p3 r: oE ce3 sseS s1 :: ğ‘  ğ‘–ğ‘š 1,ğ‘  ğ‘–ğ‘š 2S 2: ğ‘  ğ‘–ğ‘š 3 â€¦S k: âˆ… S Eu xb tp raro ctc ie os ns I st elfim -ap ttl ee nm tie on nts ena con de eu rra ol fne thtw eo Trk ranf so fl olo rmwi en rg [1th 5e ] ad re cs hig itn ecto uf rea .
OUTPUT:The event sequence: E2 E5 E3is assigned with subprocess ID S. The advantage given by this architectural choice resides in
1
Explanation: 2/3 masked subprocess sequences ğ‘  ğ‘–ğ‘š 1 , ğ‘  ğ‘–ğ‘š 2 have an ID S 1. its capability to learn the contextual information between the
input events. When learning the parameters of the network,
Fig.4. Theinternaldesignofcontext-awaresubprocessextractionpart(with
guidedbyacarefullydesignedcostfunction,themodellearns
adetailedexplanationofarunningexample).
local relationships between the events based on their co-
1) Preprocessing Submodule: The goal of the preprocess- occurrence extracting useful contextual features. The neural
ing submodule is to preprocess the input log event sequences network submodule has four components: vectorizer, encoder
in a unified format suitable for the neural network. It has two block, output layers and masked subprocess id assignment.
components: padding and masking. The vectorizer transforms the masked input event sequences
Padding. The padding component receives the sequences of tokens into numerical vector sequences. These vectors are
of log events as input, with each event represented by a calledeventembeddingsandarepartofthetrainingprocedure.
unique identifier (e.g., integer). We refer to it as a token. At the beginning of the training procedure, the event vectors
The sequences of events in a given time interval are different are randomly initialized and are updated during training. This
in length. However, the neural network requires a fixed-size way, they learn contextual information about the events.
representationoftheinput.Thepaddingcomponentspecifiesa The encoder block is composed of a self-attention encoder
hyperparameter max length and appends each of the shorter layer. The self-attention extracts co-occurring information by
logsequenceswithadedicatedtoken[PD]uptomax length weightingtheinputvectorembeddingsbytheirsimilaritytoall
to enforce fixed-size representation. The longer sequences theotherembeddingsinthegivencontext.Combiningtheself-
(havingmorethanmax lengthevents)aretruncated.Notably, attentionwiththeMLMlearningtaskmodifiestheparameters
we add a dedicated token [LSE] (Log Sequence Embedding) of the network to learn the context of the original masked
atthebeginningofeachsequence.Duringlearning,weenforce event and extract sequential properties. The hyperparameters
of the encoder are the model size (denoted by d), the number the embeddings currently assigned to it, and 2) reassignment
of encoder layers, the number of heads, and the dropout ratio of the embeddings to the nearest newly calculated centroid.
(usedtopreventoverfitting).WereferencethereadertoDevlin
et al. [15] for the specific details on self-attention neural net- J (Ï†(sm,r ;Î¸),M)=||Ï†(sm;Î¸)âˆ’r M|| (2)
k n n n n 2
works. Particularly interesting is the embedding of the [LSE]
token. Since [LSE] serves as an embedding of the sequence, where MâˆˆRkxd represent the matrix of subprocess context-
it learns contextual properties of the masked input sequences. group (interchangeably referred to as centroids), while r is
n
The output from the encoder is the vector embedding of the an indicator vector of discrete values (0â€™s and 1â€™s) with just
[LSE] token for each of the masked sequences, proceeded one element set to one, corresponding to the membership of
towards the output layer. themaskedsequencesm toacertaincentroidm .Thenumber
n k
Theoutput layeriscomposedoftwolayerswithnonlinear of subprocesses identifiers k is a hyperparameter.
activation (RELU is used). The purpose of it is to map
Finally,weaddthetwooptimizationlossesasJ =J +Î»J
m k
the masked sequence embedding vector [LSE] of size d, to
to obtain the final loss subject to optimization. By combined
a vector with a size corresponding to the total number of
optimization of the two losses, the parameters of the context-
events/tokens C. The output of this layer is used to calculate
aware subprocess extraction learn local contexts and local-
the loss. As an optimization loss function, we use categorical
context groups based on their similarity. The role of the
cross-entropy. Notably, during the execution of a workload,
hyperparameterÎ»istoensurelearningofcorrectcontextsand
some events occur only once (e.g., â€notification of successful
correctcontext-embeddinggroupsbytradingofftheimpactof
creation of a VMâ€) while others in greater frequency (e.g.,
thetwolosses.Wefurtherdiscusstheoptimizationprocedure.
HTTPorRPCcalls).Whenusingtheoriginallossformulation
Optimization. The optimization is done in two phases: 1)
on the MLM task, the less frequent events will be averaged
pretrainingand2)jointtraining.Wefirstdescribethepretrain-
out, resulting in missing important information. To account
ing phase. Since at the beginning everything is initialized at
for the imbalances of the distribution of the events, we use
random, we pre-train the neural network parameters (Î¸ and
weighted categorical cross-entropy given in Eq. 1 as follows: Î¸(cid:48)) by the weighted cross-entropy loss (Eq. 1). That way, the
J (Ïˆ(sm ;Î¸,Î¸(cid:48) ),ym ;w) model learns good initial parameters for the encoder while
m n,c n,c extracting context-aware features for the masked sequences.
1 (cid:88)C exp(Ïˆ(sm n,c;Î¸,Î¸(cid:48))) (1) The pretraining is terminated after observing a lack of im-
= âˆ’w ym log
|C| c n,c (cid:80)C exp(Ïˆ(sm;Î¸,Î¸(cid:48))) provement in the loss on five consecutive epochs. At the end
c=1 i=1 n,i
of the pretraining, the [LSE] vectors are valid representations
where Ïˆ denotes the function modeled by the neural network,
of the masked input sequences. Afterwards, the subprocesses
Î¸andÎ¸(cid:48)aretheparametersoftheencoder,andtheoutputlayer
prototypes(M)areinitializedbykmeansusing[LSE]masked
accordingly,sm isamaskedsequenceobtainedfromthen-th
n,c sequence embeddings of the training data.
input sequence s , ym is the original masked event/token, C
n n,c Jointtraining(phase2).Thejointoptimizationfunctionhas
denotes the total token numbers and w represents the weight
c a discrete variable (r ), making the parameter updates non-
ofanindividualtoken.Theweights(wâ€“aweightsvector)are n
trivial. To address this issue, we calculate the gradients by
assigned such that the less frequent events have weight values
alternating stochastic gradient descent (ASGD) [16]. ASGD
closer to 1, as opposed to the frequent ones that have values
alters the updates of the network parameters and centroids
closer to 0. Therefore, we optimize for preserving the correct
such that, when the network parameters are updated, the
predictions on the infrequent events, addressing the challenge
centroidsarefixedandviceversa.Therefore,theoptimization
of the imbalance of the event frequency distribution.
problemdoesnotdependonthediscretevariable,enablingthe
Masked Subprocess ID assignment. The masked sub-
parameterupdates.Thetrainingofthenetworkparametersand
process ID assignment receives the vector embedding of the
the centroids is done in batches. Eq. 3 is used for centroids
[LSE] token as input. It applies the mini batched kmeans
update. At each batch, the centroids with newly assigned
algorithm [16] to group the embeddings of the masked
embeddings are slightly updated based on their distance to
event sequences into a predetermined number of k subpro-
the newly calculated centroids. Additionally, some of the
cesses/centroids identifiers. The mini batched kmeans algo-
centroids are updated more frequently than others making
rithm is a commonly used method for identifying similar
the loss convergence slower. Inspired by Yang et al. [16],
instance groups in an unsupervised way. While the goal of