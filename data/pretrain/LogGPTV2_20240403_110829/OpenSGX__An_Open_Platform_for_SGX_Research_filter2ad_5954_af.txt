directory and exit Tor nodes.
for Tor-enclave, we statically link and load the OpenSSL 1.0.2
library into the enclave.
We first quantify the number of EPC pages used to run Tor-
enclave. Then, we measure the performance of SGX-enabled
Tor using three metrics: additional instructions and CPU cycles,
the number of context switches between enclave and non-
enclave, and the number of RPC calls between Tor-enclave and
non-enclave.
The number of EPC pages used. Table X shows the
total number of EPC pages required to run Tor-enclave and
their breakdown. We categorize EPC pages into five types:
enclave pages, code/data pages, SSApages, stack, and heap
pages. Enclave pages contain SECS and TCS data structures
required for an enclave. Code and data pages are used for
enclave code/data sections, which are proportional to the size
of the enclave program. SSA pages are State Save Area pages
used to support asynchronous exit. Stack pages are used as
the stack section for the enclave program, and heap pages for
dynamically allocated memory. The number of SSA, stack, and
heap pages is configurable; we initialize them as 2, 50, and 50
pages. However, the number of EPC heap pages can increase
during execution. For example, in a large Tor network, directory
servers may require more heap space because they store the
list of relays in EPC.
For code/data pages, Table XI shows the breakdown of
EPC code and data regions. The page types are categorized by
the OpenSSL library, SGX library, and separated Tor-enclave
program. We see that the OpenSSL library dominates the EPC
usage. This is because we load the entire library into EPC to
support the cryptographic operations needed by Tor. The SGX
library and Tor-enclave consist of five code and data pages
only. Overall, the trusted component is relatively small (54%)
compared to placing the entire Tor code base into the enclave
without the separation.
Additional CPU cycles. We now evaluate the performance
overhead of SGX-enabled Tor by measuring the additional
number of instructions executed and CPU cycles consumed. The
use of SGX instructions executed, SGX library calls, and system
call support, such as enclave creation, contribute to the overhead.
To quantify this, we obtain the number of all instructions
and SGX instructions executed using OpenSGX by leveraging
QEMU and OpenSGX performance counter. We then translate
Fig. 5: An overview of the execution environment of an OpenSGX-
enabled Tor. To reduce the size of TCB (an enclave program), we use
SGX to protect the secrecy of Tor nodes; directory in Directory Node
and relay table in Exit Node.
To this end, we emulate the encrypted communication
channel with message authentication. We assume that a shared
secret is established between the Tor-enclave and the secure
network device. We also assume that the device can perform
TCP/IP processing. All the messages between Tor-enclave and
the secure NIC are then sent via the secure channel. Thus,
the operating system of the Tor exit node cannot observe any
plain-text communication between the client and the server.
C. Implementation
We implement Tor-enclave, Tor-non-enclave, and the RPC
interface between the two. For directory servers, Tor-enclave
contains the private key and the list of onion routers to be used,
making the information private. For onion routers, we store
their private keys in Tor-enclave. Non-private information and
external interface to remote parties are handled by Tor-non-
enclave. For evaluation, we use Chutney [48] to construct a
private Tor network. Our private Tor network consists of seven
nodes: three directory servers, three relays (onion routers), and
a client proxy. At minimum, at least three directory servers
are required to prevent tie-breaking, and three relay nodes
are needed to build a 3-hop circuit. The process of running
a private Tor network can be divided into three phases: key
generation, consensus creation, and service phases. During the
key generation phase, private keys and certificates are created
for both onion routers and directory servers. In the consensus
creation phase, directory servers sign votes and create consensus
in order to agree upon the set of relays to be used. Then, a client
proxy requests a list of available onion routers to directory
servers. Based on this, the client proxy establishes a circuit.
Finally, the client proxy sends/receives relay cells using the
circuit for users’ requested service.
VII. PERFORMANCE PROFILING
We evaluate OpenSGX by showcasing the SGX-enabled
Tor application. Using Tor as a case study, we demonstrate that
OpenSGX can run non-trivial applications and enable SGX
application developers to profile their applications using our
performance counter.
Environment setup. Figure 5 illustrates the overall execution
environment of Tor with OpenSGX. Chutney [48] launches
each node as a process within a single machine. Because we
separated enclave components for the Tor directory and Tor
exit node, each directory and exit node runs two processes: a
Tor-non-enclave process and a Tor-enclave process. We use a
Quad core Intel Core i5-4690 3.5GHz CPU machine running
Linux 3.11.0 and tor-0.2.5.10 and torsock-1.3 for evaluation.
To support cryptographic operations (e.g., RSA key creation)
12
Tor NodeChutney NodesOpenSGX(directory)test000aTor Node(exit relay)test005rTor Client(client)test007c......Enclave(directory)OpenSGXEnclave(relay table)...WrapperWrapperpopulatepopulateChutneyTor NetworkpopulateinvocationinteractionsecrecyFig. 6: The number of instructions and CPU cycles while loading
Tor-enclave process. Here, (M) stands for a million.
Fig. 7: The number of CPU cycles of Tor-enclave process for the three
phases. Here, separated Tor on QEMU means executing Tor-enclave
process without using SGX instructions and sgxlib calls on the native
QEMU.
the instruction count to CPU cycles using the performance
estimate from recent SGX literature. In particular, we assume
that each SGX instruction takes 10K CPU cycles, and non-
SGX instructions run at native speed within the enclave [5]. To
estimate the CPU cycles for non-SGX instructions, we measure
the average instructions per cycle by executing Tor natively
without OpenSGX.2 We report the average of 20 runs because
the actual instruction count slightly varies depending on the
random number generated during the cryptographic operations,
such as the prime number.
Figure 6 shows the number of CPU cycles consumed to
create and load the program into an enclave. It takes about
361M cycles to start enclave-Tor for a directory node and
362M cycles for an exit node. The CPU cycles required for
loading both directory node and exit node are similar because
the number of EPC pages required for running them are almost
the same (see Table X). During the process, only the privileged
ENCLS instructions are invoked (e.g., ECREATE, EADD, EEXTEND,
and EINIT). Most of them are EEXTEND instruction because it
is called 16 times for each EPC page (4KB) to obtain the hash
value of its content. Additionally, non-SGX instructions are also
invoked for handling the system calls (e.g., sys_sgx_init()).
Note that enclave creation and program loading are one-time
costs that only occur at the beginning.
We now quantify the overhead of key generation, consensus
creation, and service phases of the Tor-enclave process. To
estimate the overhead of using SGX, we compare the number
of instructions and CPU cycles of Tor-enclave running on
OpenSGX and on native QEMU without OpenSGX. Note
that the latter just runs Tor nodes as two separate processes
communicating through a pipe. Thus, the comparison shows
the amount of extra overhead of using SGX. Figure 7 presents
the number of CPU cycles for each phase.
The key generation phase only occurs once at the beginning
2The resulting average IPC is 1.81 instructions/cycle.
13
Fig. 8: The number of context switches (enclave exit and entry) of
Tor-enclave process for directory and exit Tor nodes.
to create identity keys and signing keys for directory nodes
and onion keys for exit nodes. Most of CPU cycles in this
phase are used for generating RSA keys. SGX Tor consumes
3.2 times the CPU cycles of the separated Tor running on the
native QEMU without OpenSGX for the directory node, and
3.6 times for the exit node. This is because the key creation
phase uses multiple sgxlib calls, such as sgx_malloc(), that
invoke ENCLS and ENCLU instructions and involve enclave exit
and re-entry.
During the consensus creation phase, directory nodes
perform voting and agree upon the relay and exit nodes to
use. Because consensus creation is performed periodically, we
measure the cost of the first consensus creation. The directory
node consumes 12 times more cycles, while exit node spends
4.9 times more cycles in this phase. For both nodes, sgx_send()
and sgx_recv() calls that are used for sending and receiving
data (e.g., fingerprint, public key string, etc.) consume the extra
CPU cycles because they involve enclave exit and re-entry.
Furthermore, because we have separated the process into Tor-
enclave and non-enclave, the RPC communication between
the two involves sgx_read() and sgx_write() calls that also
contribute to the overhead. Note that this is common across all
three phases because our design puts part of the application
code in the enclave in an attempt to reduce the TCB. Later
in the section, we quantify the number of RPC calls in each
phase.
In the service phase, our client proxy gets the list of Tor
nodes by querying the directory nodes and generates a circuit
from the proxy to an exit node using onion routing. After circuit
establishment, the client’s traffic is directed to the circuit via
the proxy. We measure the overhead of a circuit establishment
and serving a request at the exit node. For the latter, we
generate a single wget request for http://www.google.com.
Figure 7 shows that the directory node consumes 10 times
more cycles and 5.6 times for the exit node. Similar to the
previous case, additional overhead of the directory node is
caused by sgx_send() and sgx_recv() calls to get a consensus
verification result during the circuit creation. For an exit node,
receiving encrypted relay cells from relay nodes requires
sgx_recv() calls that contribute to the overhead.
Context switch overhead. We now quantify the number of
context switches (i.e., enclave entries and exits) that occur
during the Tor-enclave execution. Switching the CPU mode
between enclave and normal mode incurs overhead, including
saving and restoring the CPU state and registers and a
TLB flush. Each invocation of EENTER, EEXIT, and EREUSME
instructions causes the CPU mode to change. Figure 8 shows
the number of context switches of the Tor-enclave process for
Directory node (avg)Exit node0100200300400# of CPU cycles (M)SGX inst.Non-SGX inst.TotalSeparated Toron OpenSGXSeparated Toron QEMUDirectory node (avg)Exit nodeSeparated Toron OpenSGXSeparated Toron QEMU030006000900012000150001800021000# of CPU cycles (M)Key generationConsensus creationCircuit establishment & ServiceTotalDirectory node (avg)Exit node050010001500200025003000# of Context SwitchesKey generationConsensus creationCircuit establishment & ServiceTotalType
Directory node
Exit Node
Total
Key
generation
Consensus
creation
Service
14
74
88
40
288
328
24
10
34
TABLE XII: The number of RPC calls between Tor-enclave and
Tor-non-enclave during the three phases of Tor execution
Concurrency OS support
License
Variable
No
Dependent on TPM No
Yes
ARM
Yes
Intel
Intel
Yes
TPM
Flicker
ARM TrustZone
Intel IPT
Intel SGX
a Flicker TEE code runs on main CPU fast, however it entails significant performance overhead when
Performance [37]
Slow
Fast a
Fast
Moderately Fast
Fast
Yes
Yes
Yes
Yes
Yes
utilizing TPM operations [35].
TABLE XIII: Comparison of TEE hardware. ‘Concurrency’ denotes
the case when concurrency is supported by the device and ‘OS
support’ denotes the case when the TEE requires a special OS support.
directory and exit nodes during the three execution phases.
A major source of context switching is system calls and I/O,
such as sys_create_enclave(), sgx_read(), and sgx_write().
In particular, OpenSGX I/O APIs, such as sgx_read() and
sgx_write(), use trampoline and stub, which cause the program
to exit the enclave mode to request I/O operation to the
kernel. Because enclave-Tor performs I/O frequently, the result
shows that context switching occurs very often (i.e., every 7M
instructions for a directory node and 0.5M instructions for
an exit node). However, we expect that the cost of context
switching can be amortized through batching system calls and
I/O operations [44].
The number of RPC calls. Finally, we count the number of
RPC calls between Tor-enclave and Tor-non-enclave. Although
this is not a direct measure, it reflects another aspect of
the overhead due to the new design of Tor (separation of
Tor process). In our implementation, each RPC call involves
sgx_write() or sgx_read() calls. Table XII shows the number
of RPC calls measured for the three phases of Tor execution.
In the key generation phase, an exit node additionally executes
a larger number of RPC calls because it creates three X509
certificates for the TLS connection, whereas the directory node
only creates a certificate for the signing key. Consensus creation
involves a large number of RPC calls. In this phase, a directory
node signs its votes, creates a consensus, and checks the state
of reachable Tor nodes. Then, it sends a message periodically
to Tor nodes and each node replies with liveness information
by authenticating itself using its identity key. This causes many
I/O calls during the launching phase in the exit node. In the
service phase, the directory node requires RPC calls to verify the
signature of consensus. Also, the exit node invokes several RPC
calls for decrypting and encrypting DNS and HTTP requests.
We believe that the cost can be amortized by batching the
systems calls.
VIII. RELATED WORK
TEE has been considered an effective way of constructing
a secure area residing in the main processor in mobile and
smartcard platforms [49]. TEE is designed to ensure protected
storage of sensitive data and to guarantee safe executions of
trusted applications. Although various types of TEE including
TPM, ARM Trusted Zone, Intel TXT, and AMD SVM, have
already been deployed to those platforms [34, 41],
their
usage has not reached further into the cloud yet, due to
14
their limited form factors and, critically, performance. The
recent introduction of Intel SGX [2, 19, 36] changes this
landscape by restricting the TCB (Trusted Computing Base)
to the processor itself while providing the performance at the
native hardware level (e.g., multiple threads support) inside an
enclave. Table XIII summarizes the characteristics of currently
available TEE technologies.
Intel SGX. A number of projects have explored applications
of Intel SGX in the cloud environment. Haven [5] pioneered
the idea of enabling unmodified application binaries to run on
Intel SGX inside the cloud. VC3 [42] suggested using SGX for
ensuring privacy in data analytics in the cloud. Both projects
utilized the Intel SGX emulator provided by Intel to develop
software that works on top of Intel SGX. However, the emulator
has been available only to the authors of both projects. To the
best of our knowledge, no SGX emulator is publicly available
to the general research community. Thus, our focus in this
OpenSGX project is to develop an openly available platform
upon which new research ideas involving TEE can be readily
implemented and explored.
Kim et al. [27] explore how to leverage SGX to enhance the
security and privacy of network applications, such as software-
defined inter-domain routing, Tor anonymity network, and in-
network functions. They use OpenSGX to demonstrate the
feasibility of the design and characterize the overhead of
adopting SGX into application design, which demonstrates
the usefulness of the OpenSGX platform.
Isolated execution environment. Hardware-based trusted
execution environments have been constructed in various
contexts. For example, Flicker [34] utilizes Late Launch;
SICE [4] uses multi-core architecture; OASIS [38] proposes a
cost-effective CPU ISA extensions for TEE; TrInc [31] provides
trustworthy computation by using TPM for distributed systems;
SecureSwitch [46] uses BIOS, and Secure Executables [7, 51]
extends the power architecture to build a trusted execution
environment. For low-end embedded devices, TrustLite [29]
and Tytan [8] enforce execution-aware memory protection in a