scenarios.
trafﬁc) would take Bro 1.6 hours to analyze instead
of 3 minutes. An hour of T1-level trafﬁc (1.5Mb/s)
would take a week instead of 5 hours, assuming that
Bro didn’t ﬁrst run out of memory.
4.3.2 Online latency and drop-rate
As described above, our attack packets cannot be
processed by Bro in real-time, even with very mod-
est transmission rates. For ofﬂine analysis, this sim-
ply means that Bro will take a while to execute.
For online analysis, it means that Bro will fall be-
hind. The kernel’s packet queues will ﬁll because
Bro isn’t reading the data, and eventually the kernel
will start dropping the packets. To measure this, we
constructed several different attack scenarios. In all
cases, we warmed up Bro’s hash table with approx-
imately 130k random SYN packets. We then trans-
mitted the attack packets at any one of three differ-
ent bandwidths (16kb/s, 64kb/s, and 160kb/s). We
constructed attacks that transmitted all 216 attack
packets sequentially, multiple times. We also con-
structed a “clever” attack scenario, where we ﬁrst
sent 3/4 of our attack packets and then repeated
the remaining 1/4 of the packets. The clever at-
tack forces more of the chain to be scanned before
the hash table discovered the new value is already
present in the hash chain.
Table 3 shows the approximate drop rates for four
attack scenarios. We observe that an attacker with
even a fraction of a modem’s bandwidth, transmit-
ting for less than an hour, can cause Bro to drop,
on average, 71% of its incoming trafﬁc. This would
make an excellent precursor to another network at-
tack that the perpetrator did not wish to be detected.
USENIX Association
12th USENIX Security Symposium 
37
s
d
n
o
c
e
s
n
i
y
a
e
D
l
d
e
p
p
o
r
d
s
d
n
a
s
u
o
h
T
 90
 80
 70
 60
 50
 40
 30
B
A
C
 0
 5
 10
 15
 20
 25
Minutes into the Attack
s
d
n
o
c
e
s
n
i
y
a
e
D
l
 140
 120
 100
 80
 60
 40
 20
B
A
C
 0
 1
 2
 3
 4
 5
 6
Minutes into the Attack
Figure 3: Packet processing latency, 16kb/s.
Figure 5: Packet processing latency, 64kb/s.
 20
 18
 16
 14
 12
 10
 8
 6
 4
 2
 0
C
A B
 0
 5
 10
 15
 20
 25
Minutes into the Attack
d
e
p
p
o
r
d
s
d
n
a
s
u
o
h
T
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
C
A
B
 1
 0
 2
 3
 4
 5
 6
Minutes into the Attack
Figure 4: Cumulative dropped packets, 16kb/s.
Figure 6: Cumulative dropped packets, 64kb/s.
Bro’s drop rate is not constant. In fact, Bro mani-
fests interesting oscillations in its drop rate, which
are visible in Figures 3 through 6. These graphs
present Bro’s packet processing latency and cu-
mulative packet drop rate for attack packets being
transmitted at 16 kb/sec and 64 kb/sec.
At time A, the latency (time between packet ar-
rival and packet processing) starts increasing as to-
tal processing cost per packet begins to exceed the
packet inter-arrival time.
At time B, Bro is sufﬁciently back-logged that the
kernel has begun to drop packets. As a result, Bro
starts catching up on its backlogged packets. Dur-
ing this phase, the Bro server is dropping virtually
all of its incoming trafﬁc.
At time C, Bro has caught up on its backlog, and
the kernel is no longer dropping packets. The cycle
can now start again. However, the hash chain under
attack is now larger then it was at time A. This will
cause subsequent latencies to rise even higher than
they were at time B.
This cyclic behavior occurs because Bro only adds
entries to this hash table after it has determined
there will be no response to the SYN packet. Bro
normally uses a ﬁve minute timeout. We reduced
this to 30 seconds to reduce our testing time and
make it easier to illustrate our attacks. We antic-
ipate that, if we were to run with the default 5-
minute timeout, the latency swings would have a
longer period and a greater amplitude, do to the ten
times larger queues of unprocessed events which
would be accumulated.
38
12th USENIX Security Symposium 
USENIX Association
4.4 Discussion
5 Solving algorithmic complexity attacks
Our attack on Bro has focused on its port scanning
detector. Bro and other IDS systems almost cer-
tainly have other hash tables which may grow large
enough to be vulnerable to algorithmic complexity
attacks. For example, Bro has a module to detect
network scans, determining how many destination
hosts have been sent packets by a given source host.
This module gives 32 + h bits of freedom, where h
is the number of host bits in the destination network
monitored by the IDS. h is unlikely to be greater
than 16 except for a handful of sites. However, in
an IPv6 network, the sky is the limit. For that mat-
ter, IPv6 gives the attacker a huge amount of free-
dom for any attack where IP addresses are part of
the values being hashed.
Part of any hash table design is the need to ex-
pand the bucket count when the table occupancy
exceeds some threshold. When the hash table has
a large number of objects which hash to the same
bucket after the rehashing operation, then the re-
hashing operation could be as bad as O(n2), if the
hash table were using its normal insertion opera-
tion that checks for duplicates. As it turns out,
Bro does exactly this.
In our regular experimen-
tal runs, we “warmed up” the hash tables to pre-
vent any rehashing during the experiment. Before
we changed our experimental setup to do this, we
saw large spikes in our latency measurements that
indicated rehashing was occurring. When rehash-
ing, Bro takes 4 minutes to process a table with 32k
attack entries. Bro takes 20 minutes to process a
table with 64k attack entries. Without IPv6 or us-
ing bucket collisions, we cannot create more colli-
sions than this, although making the IDS server un-
responsive for 20 minutes is certainly an effective
attack.
Although rehashing attacks are extremely potent,
they are not necessarily easy to use; attackers can-
not exploit this window of opportunity unless they
know exactly when it is occurring. Furthermore,
Bro’s hash table will rehash itself at most 12 times
as it grows from 32k entries to 64M entries.
When analyzing algorithmic complexity attacks,
we must assume the attacker has access to the
source code of the application, so security through
obscurity is not acceptable. Instead, either the ap-
plication must use algorithms that do not have pre-
dictable worst-case inputs, or the application must
be able to detect when it is experiencing worst-case
behavior and take corrective action.
5.1 Eliminating worst-case performance
A complete survey of algorithms used in common
systems is beyond the scope of this paper. We focus
our attention on binary trees and on hash tables.
While binary trees are trivial for an attacker to gen-
erate worst-case input, many many other data struc-
tures like red-black trees [11] and splay trees [18]
have runtime bounds that are guaranteed, regard-
less of their input. A weaker but sufﬁcient condition
is to use an algorithm that does not have predictable
worst-case inputs. For example, treaps [17] are
trees where all nodes are assigned a randomly cho-
sen number upon creation. The tree nodes are ro-
tated such that a tree property is preserved on the
input data, as would be expected of any tree, but
a heap property is also maintained on the random
numbers, yielding a tree that is probabilistically
balanced. So long as the program is designed to
prevent the attacker from predicting the random
numbers (i.e., the pseudo-random number genera-
tor is “secure” and is properly initialized), the at-
tacker cannot determine what inputs would cause
the treap to exhibit worst-case behavior.
When attacking hash tables, an attacker’s goal is to
efﬁciently compute second pre-images to the hash
function, i.e., if x hashes to h(x) and y (cid:5)= x, it should
be infeasible for the attacker to derive y such that
h(y) = h(x). Cryptographically strong hash func-
tions like MD5 and SHA-1 are resistant, in general,
to such attacks. However, when used in hash ta-
bles, the 128 or 160 bits of output from MD5 or
USENIX Association
12th USENIX Security Symposium 
39
SHA-1 must eventually be reduced to the bucket
count, making it feasible for an attacker to mount
a brute force search on the hash function to ﬁnd
bucket collisions. Some simple benchmarking on
a 450MHz Pentium-2 allowed us to compute ap-
proximately ﬁve such collisions per second in a
hash table with 512k buckets. This weakness can
be addressed by using keyed versions of MD5 or
SHA-1 (e.g., HMAC [12]). The key, chosen ran-
domly when the program is initialized, will not
be predictable by an attacker; as a result, the at-
tacker will not be able to predict the hash values
used when the program is actually running. When
keyed, MD5 and SHA-1 become pseudo-random
functions, which, like treaps, become unpredictable
for the attacker. When unkeyed, MD5 and SHA-1
are deterministic functions and subject to bucket
collisions.
5.2 Universal hashing
Replacing deterministic hash functions with
pseudo-random functions gives probabilistic guar-
antees of security. However, a stronger solution,
which can also execute more efﬁciently, is avail-
able. Universal hash functions were introduced
in 1979 [5] and are cited by common algorithm
textbooks (e.g., Cormen, Leiserson, and Rivest [6])
as a solution suitable for adversarial environments.
It has not been standard practice to follow this
advice, but it should be.
Where MD5 and SHA-1 are designed to be resistant
to the computation of second pre-images, universal
hash functions are families of functions (with the
speciﬁc function speciﬁed by a key) with the prop-
(cid:6)
erty that, for any two arbitrary messages M and M
,
(cid:6)) are less than some small
the odds of h(M) = h(M
value ε. This property is sufﬁcient for our needs,
because an attacker who does not know the speciﬁc
hash function has guaranteed low odds of comput-
ing hash collisions.
Carter and Wegman’s original construction of a uni-
versal hash function computes the sum of a ﬁxed
chosen constant with the dot product of a ﬁxed cho-
sen vector with the input, modulo a large prime
number. The ﬁxed chosen constant and vectors
are chosen, randomly, at the beginning, typically
pre-computed using a keyed pseudo-random func-
tion, and reused for every string being hashed. The
only performance issue is that this vector must ei-
ther be pre-computed up to the maximum expected
input length, or it must be recomputed when it
is used, causing a noticeable performance penalty.
More recent constructions, including UMAC [4]
and hash127 [3] use a ﬁxed space despite support-
ing arbitrary-length arguments. UMAC, in partic-
ular, is carefully engineered to run fast on modern
processors, using adds, multiplies, and SIMD mul-
timedia instructions for increased performance.
5.2.1 Universal hash designs
Some software designers are unwilling to use uni-
versal hashing, afraid that it will introduce unac-
ceptable performance overheads in critical regions
of their code. Other software designers simply
need a fast, easy-to-integrate library to solve their
hashing needs. Borrowing code from UMAC and
adding variants hand-optimized for small, ﬁxed-
length inputs, we have implemented a portable C
library suitable for virtually any program’s needs.
Our library includes two different universal hash
functions: the UHASH function, submitted as part
of the (currently expired) UMAC Internet draft
standard [4], and the Carter-Wegman dot-product
construction. We also include a hand-tuned vari-
ant of the Carter-Wegman construction, optimized
to support ﬁxed-length, short inputs, as well as an
additionally tuned version that only yields a 20 bit
result, rather than the usual 32 bits. This may be
appropriate for smaller hash tables, such as used in
Squid (see Section 3.1).
Our Carter-Wegman construction processes the
value to be hashed one byte at a time. These bytes
are multiplied by 32 bits from the ﬁxed vector,
yielding 40 bit intermediate values that are accumu-
lated in a 64 bit counter. One 64-by-32 bit modulo