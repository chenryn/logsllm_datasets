V1: 2016-0728 [18]
V2: 2017-5123 [7]
V3: 2017-7308 [8]
V4: 2017-10661 [3]
V5: 2017-11176 [4]
V6: 2017-17052 [5]
V7: 2018-7480 [10]
V8: 2018-10880 [6]
V9: 2018-17182 [9]
V10: 2019-20054 [11] ext4_xattr_set_entry
P1 P2 P3 P4 P5
join_session_keyring      
     
SyS_waitid
     
packet_set_ring
     
SyS_timerfd_settime
     
SyS_mq_notify
     
get_net_ns_by_id
     
blkcg_init_queue
ext4_update_inline_..      
     
vmacache_flush_all
     
Table 7: SHARD’s effectiveness against control-ﬂow hijacks attacks
using different vulnerabilities and payloads (Table 6). The code of
some vulnerabilities is unreachable (“Unr”).
(§10.2) and real-world applications (§10.3), and evaluates the
impact of proﬁling accuracy (§10.4).
10.1 Experimental Setup
Machine speciﬁcation. We conducted all our experiments
on an Intel (R) Core (TM) i7-6500U CPU @ 2.50GHz with
4 MB of last-level cache, 8 GB of memory, and support for
the Last Branch Record (LBR).
Kernel conﬁguration. Our SHARD-protected kernel was
Linux kernel v4.14, which ran inside a guest virtual machine
(VM). The VM was allocated 4 GB of memory, 1 thread, and
connected to the host with a 1 Gb/s virtual connection.
SHARD conﬁguration. SHARD’s monitor was installed on
the KVM module of the host, running Linux kernel v4.15.
10.2 Micro-benchmarks
This section analyzes the memory footprint of SHARD and
the overhead of SHARD monitor’s operations.
Memory footprint. SHARD maintains various versions of
instrumented kernel code pages (i.e., UNRESTRICTED, RE-
STRICTED, and HARDENED) and call target tables to enforce
control-ﬂow integrity (CFI) (refer to §7.2). Table 8 shows
the memory overhead incurred by SHARD. Each application
incurs a different overhead for RESTRICTED code page ver-
sions, based on the invoked system calls and kernel func-
tions. The main memory overhead is caused by call target
tables, maintained for each indirect kernel call site, to enforce
CFI. Nevertheless, this memory consumption is negligible in
comparison with the memory available in modern machines
(usually tens of GBs).
Monitor overhead. The SHARD monitor performs 3 opera-
tions (refer to §7.4): (a) trap on context switches and system
calls, (b) switch the EPT to enforce hardening and debloat-
ing, and (c) perform an LBR-based check for CFI during
hardening. To ascertain the runtime overheads, we create a
benchmark which executes a system call (i.e., getgid) in
Kernel code pages
UNRESTRICTED
RESTRICTED (NGINX, Redis)
HARDENED
CFI tables
Frame table
Offset table
Total
8.0
14.4 − 18.0
8.0
14.0
34.0
78.4 − 82.0
Table 8: Memory footprint of SHARD.
Figure 8: Performance overhead of redis-benchmark.
a loop for 10 million iterations. This is lightweight system
call that only takes 0.43 µs on average to execute in the na-
tive kernel. We measure how long it takes for the benchmark
to complete, while selectively enabling each operation, and
comparing it against the native (non-monitored) execution.
Our results show that a trap at each system call adds an aver-
age overhead of 1.21 µs per-system call. Furthermore, switch-
ing the EPT involves updating 4 page directory entries (since
our kernel is 8 MB and a page directory holds 2 MB of pages)
and the INVEPT instruction, which adds 0.60 µs. Also, the
SHARD monitor implements a CFI-check using LBR, which
requires referencing the two call target tables and retrieving
the latest entry in the LBR, taking 1.01µs on average.
10.3 Real World Applications
This section evaluates SHARD’s overhead while executing
real-world widely-deployed applications, NGINX web server
and Redis key-value store, that match our use-case scenario
(refer to §5). Furthermore, we also evaluate SHARD with a
well-known set of real-world workloads, SPEC CPU 2006.
Common settings and terminology. We proﬁled each ap-
plication using the experiment workload. The client-server
experiments (NGINX and Redis) were performed by sending
requests from clients on the host machine. We ran each exper-
iment 10 times and report the average overhead compared to
a native (uninstrumented) Linux kernel.
In Figure 8 and Figure 10, "SHARD-trusted" refers to sce-
narios where SHARD does not enforce debloating or harden-
ing (i.e., for trusted applications), "SHARD " means SHARD’s
overhead while enforcing debloating and context-aware hard-
INCRLRANGE300SADDPINGINLINESPOPGETLPOPLRANGE600LRANGE100RPOPLPUSHPINGBULKSETRPUSHMSETHSETLRANGE5000510152025Overhead(%)SHARD-trustedSHARDSHARD-always-hardenedFigure 9: SHARD statistics while running redis-benchmark.
ening, and "SHARD-always-hardened" means SHARD’s over-
head while enforcing debloating and full-hardening on each
system call. Note that SHARD-always-hardened can only be
realized using SHARD’s framework, i.e., it is not existing
work, and is included for performance comparison.
Figure 9 and Figure 11 illustrate the overall statistics for
SHARD, including number of exits and EPT switches (for
debloating or context-aware hardening), related to NGINX
and Redis, respectively.
Redis key-value store. We evaluate Redis using the ofﬁ-
cial redis-benchmark. The benchmark ran with the default
conﬁguration, sending requests from 50 concurrent clients.
Figure 8 shows the overheads for the redis-benchmark
tests. The average overhead across all the tests for SHARD
is 6.83%. Considering the execution statistics (Figure 9), we
notice more than 40,000 traps per-second in some tests. How-
ever, since the application invoked the same system calls
(i.e., mostly read and write) successively, 96.15% of these
traps did not require switching the EPT (for debloating or
hardening). Switching the EPT requires invalidation of the
instruction cache, which is costly to repopulate. Due to few
such cases, the overhead remains low. Additionally, we no-
ticed 29 average instances of hardening per-second. However,
their overall impact on the execution was low since hardening
was only enforced for small durations.
Moreover, SHARD-always-hardened incurs an additional
overhead of 0.1-11% over SHARD (average increases to
11.49%). In particular, we observe a high overhead when the
benchmark application invokes many system calls in a small
span of time (e.g., for INCR and GET). In contrast, bench-
mark applications (e.g., LRANGE) that execute for longer
periods and invoke system calls less frequently, exhibit less
overhead for full-hardening. Finally, while running Redis as
a trusted application (SHARD-trusted), we only observe an
average overhead of 1.2%, because SHARD did not trap its
execution. The negligible overhead is due to the lightweight
instrumentation of UNRESTRICTED code pages (mentioned
in §7.2) and demonstrates the performance beneﬁts of spe-
cialization.
NGINX web server. We used the apachebench, ab [1], to
send 10,000 requests using 25 concurrent clients to an NGINX
Figure 10: The performance overhead of NGINX across varying
requested ﬁle sizes.
web server running a single worker thread.
Figure 10 shows the end-to-end latency increase across dif-
ferent requested ﬁle sizes. We observe a higher SHARD over-
head for NGINX, 22.21% on average. Unlike Redis, which
successively calls the same system call, we observe (Fig-
ure 11) a high number of traps which incur EPT switches (i.e.,
NGINX invokes distinct system calls successively). Further-
more, while the overhead is high (up to 37%) for smaller ﬁle
sizes, it is amortized over memory and I/O overhead as the ﬁle
size increases. Note that NGINX showcases the worst-case
scenario for SHARD’s overhead, i.e., many distinct system
calls per-second. In practice, we expect system calls to be
small in number (as we show for SPEC below) or to be similar
(as Redis). Also, we observe a very low number of hardening
instances, showing that in many cases a good representative
proﬁling workload ensures low run-time deviation.
The full-hardening enforcement of NGINX (SHARD-
always-hardened) incurs an additional overhead of 8-20%
over SHARD. In particular, the average performance overhead,
with full-hardening enforcement, becomes 38.17%. Finally,
running NGINX as a trusted application (SHARD-trusted)
incurs only 1.59% average overhead, similar to Redis.
SPEC CPU 2006. We ran SHARD on the SPEC CPU 2006
integer suite, which includes 12 applications that range from
ﬁle compression (bzip2) to gene sequencing (hmmer). All
experiments used the reference workloads.
Table 9 shows the overhead caused by SHARD on SPEC
applications, including the number of traps. In general, we
observe very low overhead (between −0.37 and 2.73%) for
these applications. The reason behind this is that while we see
many traps at the SHARD monitor, they were dispersed over
long-running tests. We expect such patterns to be common in
many applications; for such applications SHARD’s overhead
will likely be very low as well.
Impact of Proﬁling Accuracy
10.4
This section demonstrates the impact of proﬁling (in)accuracy
on the performance of SHARD. In particular, we illustrate
SHARD’s performance when proﬁled with a (a) different ap-
plication, (b) different application workload, or (c) partial
application workload.
Terminology. Related to Figure 12, Figure 13, and Figure 14,
INCRLRANGE300SADDPINGINLINESPOPGETLPOPLRANGE600LRANGE100RPOPLPUSHPINGBULKSETRPUSHMSETHSETLRANGE500100101102103104105106Numberofexits(/sec)HardeningEPTSwitchesTotalTraps1248163264128FileSize(KB)0204060Overhead(%)SHARD-trustedSHARDSHARD-always-hardenedFigure 11: SHARD statistics while running NGINX.
Benchmark
Execution time
Baseline (s) SHARD (s)
Total
Traps
EPT
Switches
Overhead
400.perl
401.bzip2
403.gcc
429.mcf
445.gobmk
456.hmmer
458.sjeng
462.libquantum
464.h264ref
471.omnetpp
473.astar
483.xalancbmk
306
436
270
365
464
356
507
322
669
381
440
237
307
442
269
375
471
363
518
325
683
390
442
240
195050
109789
79805
46804
125006
79813
41770
34986
87162
46486
44225
123595
75070
37386
27630
30845
79648
28292
27955
25311
41142
31215
28441
28655
0.32%
1.38%
-0.37%
2.74%
1.51%
1.97%
2.17%
0.93%
2.09%
2.36%
0.45%
1.27%
Table 9: SPEC CPU 2006 results. Table only shows numbers while
running untrusted applications with SHARD.
SHARD refers to scenarios where proﬁling was accurate—
SHARD was proﬁled using the same application and work-
load against which it was evaluated, whereas SHARD-Profabc
refers to scenarios where SHARD was proﬁled with a different
application or workload or partial workload.
Proﬁling using different application. To evaluate the im-
pact of a different application proﬁle on performance, we
generated a SHARD proﬁle using Redis and ran NGINX with
the generated proﬁle. We used the redis-benchmark for pro-
ﬁling. For evaluation, we used ab to send 10,000 requests
using 25 concurrent clients to an NGINX server with one
worker thread (similar to §10.3).
Figure 12 shows the performance overhead of Redis proﬁle
(SHARD-Profredis) compared to accurate proﬁling (SHARD).
As expected, SHARD-Profredis performs considerably worse.
In particular, we noticed a very high number of hardening
instances with SHARD-Profredis because NGINX and Redis
proﬁles are highly-disjoint (as illustrated in §3). For example,
retrieving 1KB ﬁles, SHARD-Profredis incurs ∼ 24,000 hard-
ening instances per-second, compared to ∼ 300 hardening
instances per-second with SHARD. Consequently, SHARD-
Profredis exhibits a much higher overhead (i.e., upto 89%).
Proﬁling using different application workload. Next, we
evaluated the impact of proﬁled application workloads on ap-
plication performance. In particular, we generated a SHARD
NGINX proﬁle using ab. Afterwards, we evaluated NGINX’s
performance using wrk [19]. During proﬁling, ab generated
Figure 12: The performance overhead of NGINX when the system
is proﬁled with the same (SHARD) and different (SHARD-Profredis)
application.
Figure 13: The performance overhead of NGINX when the system is
proﬁled with the same (SHARD) and different application workload
(SHARD-Profab).
requests for ﬁles between 1 to 128 KB size using 25 concur-
rent clients. Then, during evaluation, wrk requested the same
ﬁles using the same number (25) of clients.
Figure 13 shows the performance overhead of the ab pro-
ﬁle (SHARD-Profab) compared to an accurate proﬁle using
wrk (SHARD). We notice that the speciﬁc proﬁled workload,
related to an application, has little impact on the application’s
performance (i.e., less than 2% increase in performance over-
head mostly for SHARD-Profab). Hence, we conjecture that
as long as the proﬁling workload for an application is com-
prehensive, the exact workload type is less important.
Proﬁling using partial application workload. Finally, we
show the impact on application performance when SHARD
is proﬁled using a partial set of application workloads. In
particular, we generated a SHARD proﬁle using half the
redis-benchmarks and evaluated the performance using the
rest. The benchmark applications in the proﬁling and evalua-
tion sets were randomly chosen. Figure 14 shows the perfor-
mance with complete (SHARD) and partial (SHARD-Profpart)
application workload proﬁles. We notice that SHARD-Profpart
increases performance overhead only between 0−3%. Hence,
our results suggest that a partial proﬁle is also sufﬁcient to
offer high performance for an application.
11 Limitations and Discussion
Context-aware control-ﬂow integrity (CFI) creates a narrow
window of opportunity for an attacker that full CFI would
not. In particular, while the attacker cannot execute an exploit
1248163264128FileSize(KB)100101102103104105106Numberofexits(/sec)HardeningEPTSwitchesTotalTraps1248163264128FileSize(KB)020406080100Overhead(%)SHARDSHARD-ProfRedis1248163264128FileSize(KB)010203040Overhead(%)SHARDSHARD-Profabprovided input. Quach et al [47] statically identify library
code needed by an application and use piece-wise compi-
lation and loading to specialize the library-view of the ap-
plication at run-time. Azad et al [21] and Razor [46], use
dynamic proﬁling to identify and remove the code that is
not needed by an application in a particular usage scenario.
Finally, CHISEL [31] adopts a delta debugging approach to
obtain a minimal program satisfying a set of test cases. Unlike
these systems, specializing at the kernel requires addressing
additional complexities (e.g., a very large codebase which is
hard to accurately analyze statically or dynamically proﬁle)
to provide strict enforcement guarantees with low overhead.
Kernel CFI. Control-ﬂow integrity [20] prevents control-
ﬂow hijacks by ensuring that control-ﬂow transfers are only