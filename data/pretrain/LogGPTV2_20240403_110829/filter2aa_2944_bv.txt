        halt(400);                                        
    }                                                     
});                                                       
If you want to be able to call the Natter API from curl, you’ll also need to add the
external Minikube IP address and port, which you can get by running the command,
minikube ip. For example, on my system I needed to add
"192.168.99.116:30567"
to the allowed host values in Main.java.
TIP
You can create an alias for the Minikube IP address in the /etc/hosts file
on Linux or MacOS by running the command sudo sh -c "echo '$(minikube
ip) api.natter.local' >> /etc/hosts. On Windows, create or edit the file
under C:\Windows\system32\etc\hosts and add a line with the IP address a
space and the hostname. You can then make curl calls to http:/ /api.natter
.local:30567 rather than using the IP address.
10.3
Securing microservice communications
You’ve now deployed some APIs to Kubernetes and applied some basic security con-
trols to the pods themselves by adding security annotations and using minimal Docker
base images. These measures make it harder for an attacker to break out of a con-
tainer if they find a vulnerability to exploit. But even if they can’t break out from the
container, they may still be able to cause a lot of damage by observing network traffic
and sending their own messages on the network. For example, by observing commu-
nications between the Natter API and the H2 database they can capture the connec-
tion password and then use this to directly connect to the database, bypassing the API.
In this section, you’ll see how to enable additional network protections to mitigate
against these attacks.
10.3.1 Securing communications with TLS
In a traditional network, you can limit the ability of an attacker to sniff network com-
munications by using network segmentation. Kubernetes clusters are highly dynamic,
with pods and services coming and going as configuration changes, but low-level
network segmentation is a more static approach that is hard to change. For this rea-
son, there is usually no network segmentation of this kind within a Kubernetes cluster
(although there might be between clusters running on the same infrastructure),
allowing an attacker that gains privileged access to observe all network communica-
tions within the cluster by default. They can use credentials discovered from this
snooping to access other systems and increase the scope of the attack.
DEFINITION
Network segmentation refers to using switches, routers, and firewalls
to divide a network into separate segments (also known as collision domains). An
Reject any request 
that doesn’t match 
one of the set.
369
Securing microservice communications
attacker can then only observe network traffic within the same network seg-
ment and not traffic in other segments.
Although there are approaches that provide some of the benefits of segmentation
within a cluster, a better approach is to actively protect all communications using TLS.
Apart from preventing an attacker from snooping on network traffic, TLS also pro-
tects against a range of attacks at the network level, such as the DNS rebind attacks
mentioned in section 10.2.8. The certificate-based authentication built into TLS pro-
tects against spoofing attacks such as DNS cache poisoning or ARP spoofing, which rely on
the lack of authentication in low-level protocols. These attacks are prevented by fire-
walls, but if an attacker is inside your network (behind the firewall) then they can
often be carried out effectively. Enabling TLS inside your cluster significantly reduces
the ability of an attacker to expand an attack after gaining an initial foothold.
DEFINITION
In a DNS cache poisoning attack, the attacker sends a fake DNS mes-
sage to a DNS server changing the IP address that a hostname resolves to. An
ARP spoofing attack works at a lower level by changing the hardware address
(ethernet MAC address, for example) that an IP address resolves to.
To enable TLS, you’ll need to generate certificates for each service and distribute the cer-
tificates and private keys to each pod that implements that service. The processes
involved in creating and distributing certificates is known as public key infrastructure (PKI).
DEFINITION
A public key infrastructure is a set of procedures and processes for
creating, distributing, managing, and revoking certificates used to authenti-
cate TLS connections.
Running a PKI is complex and error-prone because there are a lot of tasks to consider:
 Private keys and certificates have to be distributed to every service in the net-
work and kept secure.
 Certificates need to be issued by a private certificate authority (CA), which itself
needs to be secured. In some cases, you may want to have a hierarchy of CAs
with a root CA and one or more intermediate CAs for additional security. Services
which are available to the public must obtain a certificate from a public CA.
 Servers must be configured to present a correct certificate chain and clients
must be configured to trust your root CA.
 Certificates must be revoked when a service is decommissioned or if you suspect
a private key has been compromised. Certificate revocation is done by publish-
ing and distributing certificate revocation lists (CRLs) or running an online certifi-
cate status protocol (OCSP) service.
 Certificates must be automatically renewed periodically to prevent them from
expiring. Because revocation involves blocklisting a certificate until it expires,
short expiry times are preferred to prevent CRLs becoming too large. Ideally,
certificate renewal should be completely automated.
370
CHAPTER 10
Microservice APIs in Kubernetes
10.3.2 Using a service mesh for TLS
In a highly dynamic environment like Kubernetes, it is not advisable to attempt to run
a PKI manually. There are a variety of tools available to help run a PKI for you. For
example, Cloudflare’s PKI toolkit (https://cfssl.org) and Hashicorp Vault (http://
mng.bz/nzrg) can both be used to automate most aspects of running a PKI. These
general-purpose tools still require a significant amount of effort to integrate into a
Kubernetes environment. An alternative that is becoming more popular in recent years
is to use a service mesh such as Istio (https://istio.io) or Linkerd (https://linkerd.io) to
handle TLS between services in your cluster for you.
DEFINITION
A service mesh is a set of components that secure communications
between pods in a cluster using proxy sidecar containers. In addition to secu-
rity benefits, a service mesh provides other useful functions such as load bal-
ancing, monitoring, logging, and automatic request retries.
A service mesh works by installing lightweight proxies as sidecar containers into
every pod in your network, as shown in figure 10.6. These proxies intercept all net-
work requests coming into the pod (acting as a reverse proxy) and all requests going
out of the pod. Because all communications flow through the proxies, they can
Using an intermediate CA
Directly issuing certificates from the root CA trusted by all your microservices is sim-
ple, but in a production environment, you’ll want to automate issuing certificates.
This means that the CA needs to be an online service responding to requests for new
certificates. Any online service can potentially be compromised, and if this is the root
of trust for all TLS certificates in your cluster (or many clusters), then you’d have no
choice in this case but to rebuild the cluster from scratch. To improve the security of
your clusters, you can instead keep your root CA keys offline and only use them to
periodically sign an intermediate CA certificate. This intermediate CA is then used to
issue certificates to individual microservices. If the intermediate CA is ever compro-
mised, you can use the root CA to revoke its certificate and issue a new one. The root
CA certificate can then be very long-lived, while intermediate CA certificates are
changed regularly.
To get this to work, each service in the cluster must be configured to send the inter-
mediate CA certificate to the client along with its own certificate, so that the client
can construct a valid certificate chain from the service certificate back to the trusted
root CA.
If you need to run multiple clusters, you can also use a separate intermediate CA for
each cluster and use name constraints (http://mng.bz/oR8r) in the intermediate CA
certificate to restrict which names it can issue certificates for (but not all clients sup-
port name constraints). Sharing a common root CA allows clusters to communicate
with each other easily, while the separate intermediate CAs reduce the scope if a
compromise occurs.
371
Securing microservice communications
transparently initiate and terminate TLS, ensuring that communications across the
network are secure while the individual microservices use normal unencrypted mes-
sages. For example, a client can make a normal HTTP request to a REST API and
the client’s service mesh proxy (running inside the same pod on the same machine)
will transparently upgrade this to HTTPS. The proxy at the receiver will handle the
TLS connection and forward the plain HTTP request to the target service. To make
this work, the service mesh runs a central CA service that distributes certificates to
the proxies. Because the service mesh is aware of Kubernetes service metadata, it
automatically generates correct certificates for each service and can periodically
reissue them.2
 To enable a service mesh, you need to install the service mesh control plane compo-
nents such as the CA into your cluster. Typically, these will run in their own Kuberne-
tes namespace. In many cases, enabling TLS is then simply a case of adding some
annotations to the deployment YAML files. The service mesh will then automatically
2 At the time of writing, most service meshes don’t support certificate revocation, so you should use short-lived
certificates and avoid relying on this as your only authentication mechanism.
Pod
Pod
App container
Service mesh
control plane
In a service mesh, all service communication
is redirected through proxies running as
sidecar containers inside each pod.
A CA running in the control
plane distributes certiﬁcates
to the proxies.
All communications are
upgraded to use TLS
automatically.
HTTP
HTTP
HTTPS
Communications inside
the pod are unencrypted.
App container
Certiﬁcate
authority
Service mesh
proxy
Service mesh
proxy
Figure 10.6
In a service mesh, a proxy is injected into each pod as a sidecar 
container. All requests to and from the other containers in the pod are redirected 
through the proxy. The proxy upgrades communications to use TLS using 
certificates it obtains from a CA running in the service mesh control plane.
372
CHAPTER 10
Microservice APIs in Kubernetes
inject the proxy sidecar container when your pods are started and configure them
with TLS certificates. 
 In this section, you’ll install the Linkerd service mesh and enable TLS between the
Natter API, its database, and the link-preview service, so that all communications are
secured within the network. Linkerd has fewer features than Istio, but is much simpler
to deploy and configure, which is why I’ve chosen it for the examples in this book.
From a security perspective, the relative simplicity of Linkerd reduces the opportunity
for vulnerabilities to be introduced into your cluster.
DEFINITION
The control plane of a service mesh is the set of components respon-
sible for configuring, managing, and monitoring the proxies. The proxies
themselves and the services they protect are known as the data plane.
INSTALLING LINKERD
To install Linkerd, you first need to install the linkerd command-line interface (CLI),
which will be used to configure and control the service mesh. If you have Homebrew
installed on a Mac or Linux box, then you can simply run the following command:
brew install linkerd
On other platforms it can be downloaded and installed from https://github.com/
linkerd/linkerd2/releases/. Once you’ve installed the CLI, you can run pre-installation
checks to ensure that your Kubernetes cluster is suitable for running the service mesh
by running:
linkerd check --pre
If you’ve followed the instructions for installing Minikube in this chapter, then this
will all succeed. You can then install the control plane components by running the fol-
lowing command:
linkerd install | kubectl apply -f -
Finally, run linkerd check again (without the --pre argument) to check the progress
of the installation and see when all the components are up and running. This may
take a few minutes as it downloads the container images.
 To enable the service mesh for the Natter namespace, edit the namespace YAML
file to add the linkerd annotation, as shown in listing 10.18. This single annotation
will ensure that all pods in the namespace have Linkerd sidecar proxies injected the
next time they are restarted. 
apiVersion: v1
kind: Namespace
metadata:
  name: natter-api
Listing 10.18
Enabling Linkerd
373
Securing microservice communications
  labels:
    name: natter-api
  annotations:                     
    linkerd.io/inject: enabled     
Run the following command to update the namespace definition:
kubectl apply -f kubernetes/natter-namespace.yaml
You can force a restart of each deployment in the namespace by running the following
commands:
kubectl rollout restart deployment \
  natter-database-deployment -n natter-api
kubectl rollout restart deployment \
  link-preview-deployment -n natter-api
kubectl rollout restart deployment \
  natter-api-deployment -n natter-api
For HTTP APIs, such as the Natter API itself and the link-preview microservice, this is
all that is required to upgrade those services to HTTPS when called from other ser-
vices within the service mesh. You can verify this by using the Linkerd tap utility,
which allows for monitoring network connections in the cluster. You can start tap by
running the following command in a new terminal window:
linkerd tap ns/natter-api
If you then request a message that contains a link to trigger a call to the link-preview
service (using the steps at the end of section 10.2.6), you’ll see the network requests in
the tap output. This shows the initial request from curl without TLS (tls = not_provided
_by_remote), followed by the request to the link-preview service with TLS enabled
(tls = true). Finally, the response is returned to curl without TLS:
req id=2:0 proxy=in  src=172.17.0.1:57757 dst=172.17.0.4:4567    
➥ tls=not_provided_by_remote :method=GET :authority=           
➥ natter-api-service:4567 :path=/spaces/1/messages/1           
req id=2:1 proxy=out src=172.17.0.4:53996 dst=172.17.0.16:4567    
➥ tls=true :method=GET :authority=natter-link-preview-           
➥ service:4567 :path=/preview                                    
rsp id=2:1 proxy=out src=172.17.0.4:53996 dst=172.17.0.16:4567    
➥ tls=true :status=200 latency=479094µs                          
end id=2:1 proxy=out src=172.17.0.4:53996 dst=172.17.0.16:4567    
➥ tls=true duration=665µs response-length=330B                   
rsp id=2:0 proxy=in  src=172.17.0.1:57757 dst=172.17.0.4:4567     
➥ tls=not_provided_by_remote :status=200 latency=518314µs         
end id=2:0 proxy=in  src=172.17.0.1:57757                          
➥ dst=172.17.0.4:4567 tls=not_provided_by_remote duration=169µs   
➥ response-length=428B                                            
You’ll enable TLS for requests coming into the network from external clients in sec-
tion 10.4.
Add the linkerd 
annotation to enable 
the service mesh.
The initial 
response from curl 
is not using TLS.
The internal
call to the
link-preview
service is
upgraded to
TLS.
The response 
back to curl 
is also sent 
without TLS.
374
CHAPTER 10
Microservice APIs in Kubernetes
The current version of Linkerd can automatically upgrade only HTTP traffic to use
TLS, because it relies on reading the HTTP Host header to determine the target ser-
vice. For other protocols, such as the protocol used by the H2 database, you’d need to
manually set up TLS certificates.
TIP
Some service meshes, such as Istio, can automatically apply TLS to non-
HTTP traffic too.3 This is planned for the 2.7 release of Linkerd. See Istio in
Action by Christian E. Posta (Manning, 2020) if you want to learn more about
Istio and service meshes in general.
Mutual TLS
Linkerd and most other service meshes don’t just supply normal TLS server certifi-
cates, but also client certificates that are used to authenticate the client to the
server. When both sides of a connection authenticate using certificates this is known
as mutual TLS, or mutually authenticated TLS, often abbreviated mTLS. It’s important
to know that mTLS is not by itself any more secure than normal TLS. There are no
attacks against TLS at the transport layer that are prevented by using mTLS. The pur-
pose of a server certificate is to prevent the client connecting to a fake server, and
it does this by authenticating the hostname of the server. If you recall the discussion
of authentication in chapter 3, the server is claiming to be api.example.com and the
server certificate authenticates this claim. Because the server does not initiate con-
nections to the client, it does not need to authenticate anything for the connection to
be secure.
The value of mTLS comes from the ability to use the strongly authenticated client
identity communicated by the client certificate to enforce API authorization policies at
the server. Client certificate authenticate is significantly more secure than many
other authentication mechanisms but is complex to configure and maintain. By han-
dling this for you, a service mesh enables strong API authentication mechanisms. In
chapter 11, you’ll learn how to combine mTLS with OAuth2 to combine strong client
authentication with token-based authorization.
3 Istio has more features that Linkerd but is also more complex to install and configure, which is why I chose
Linkerd for this chapter.
Pop quiz
5
Which of the following are reasons to use an intermediate CA? Select all that apply.
a
To have longer certificate chains
b
To keep your operations teams busy
c
To use smaller key sizes, which are faster
d
So that the root CA key can be kept offline
e
To allow revocation in case the CA key is compromised
375
Securing microservice communications
10.3.3 Locking down network connections
Enabling TLS in the cluster ensures that an attacker can’t modify or eavesdrop on
communications between APIs in your network. But they can still make their own
connections to any service in any namespace in the cluster. For example, if they
compromise an application running in a separate namespace, they can make direct
connections to the H2 database running in the natter-api namespace. This might
allow them to attempt to guess the connection password, or to scan services in the net-
work for vulnerabilities to exploit. If they find a vulnerability, they can then compro-
mise that service and find new attack possibilities. This process of moving from service
to service inside your network after an initial compromise is known as lateral movement
and is a common tactic.
DEFINITION
Lateral movement is the process of an attacker moving from system
to system within your network after an initial compromise. Each new system
compromised provides new opportunities to carry out further attacks, expand-
ing the systems under the attacker’s control. You can learn more about com-
mon attack tactics through frameworks such as MITRE ATT&CK (https://attack
.mitre.org).
To make it harder for an attacker to carry out lateral movement, you can apply network
policies in Kubernetes that restrict which pods can connect to which other pods in a
network. A network policy allows you to state which pods are expected to connect to
each other and Kubernetes will then enforce these rules to prevent access from other
pods. You can define both ingress rules that determine what network traffic is allowed
into a pod, and egress rules that say which destinations a pod can make outgoing con-
nections to.
DEFINITION
A Kubernetes network policy (http://mng.bz/v94J) defines what
network traffic is allowed into and out of a set of pods. Traffic coming into a
pod is known as ingress, while outgoing traffic from the pod to other hosts is
known as egress.
Because Minikube does not support network policies currently, you won’t be able to
apply and test any network policies created in this chapter. Listing 10.19 shows an
example network policy that you could use to lock down network connections to and
from the H2 database pod. Apart from the usual name and namespace declarations, a
network policy consists of the following parts:
 A podSelector that describes which pods in the namespace the policy will apply
to. If no policies select a pod, then it will be allowed all ingress and egress traffic
6
True or False: A service mesh can automatically upgrade network requests to
use TLS.
The answers are at the end of the chapter.
376
CHAPTER 10
Microservice APIs in Kubernetes
by default, but if any do then it is only allowed traffic that matches at least one
of the rules defined. The podSelector: {} syntax can be used to select all pods
in the namespace.
 A set of policy types defined in this policy, out of the possible values Ingress
and Egress. If only ingress policies are applicable to a pod then Kubernetes will
still permit all egress traffic from that pod by default, and vice versa. It’s best to
explicitly define both Ingress and Egress policy types for all pods in a name-
space to avoid confusion.
 An ingress section that defines allowlist ingress rules. Each ingress rule has a
from section that says which other pods, namespaces, or IP address ranges can
make network connections to the pods in this policy. It also has a ports section
that defines which TCP and UDP ports those clients can connect to.
 An egress section that defines the allowlist egress rules. Like the ingress rules,
egress rules consist of a to section defining the allowed destinations and a
ports section defining the allowed target ports.