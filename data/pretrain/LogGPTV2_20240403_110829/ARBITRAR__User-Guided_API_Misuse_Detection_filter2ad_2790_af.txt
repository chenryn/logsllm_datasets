annotations in the code [58], [62], or speciﬁed by the user
through a Domain-Speciﬁc Language (DSL) [17], [8].
API misuse bugs are violations of rules imposed by the spec-
iﬁcation of API functions. IMChecker [24] uses a YAML [67]
based DSL for specifying the behavior of API functions and
checks for its violations in lightweight static traces computed
over the program. Semmle [19] can ﬁnd API misuses by
semantic patterns of correct or erroneous behavior speciﬁed
using CodeQL [4], a declarative logic programming language
based on Datalog, over a relational representation of programs.
Similarly, MOPS [11] uses a ﬁnite automata based speciﬁcation
whose violations are checked using a model checker.
However, crafting a formal API speciﬁcation is hard, and it is
unreasonable to expect developers to write precise speciﬁcations
(Section II). Consequently, many API speciﬁcation inference
techniques have been proposed. These techniques compute
lightweight program traces and use static features such as
control-dependencies [45] or various mining techniques such
as FRECPO [47], [3], factor graphs [39], [33], semantics-
constrained mining [6], and frequent item sets [37] to infer
API speciﬁcations. These techniques usually require a large
corpus of programs covering all possible usages of the API.
As we show in Section V, however, this assumption does not
always hold in practice.
Another class of techniques is based on the intuition that bugs
are anomalous or deviant behavior [16]. JUXTA [44] identiﬁes
common patterns in ﬁle system implementations and detects
semantic bugs in ﬁle systems as violations of these patterns
using lightweight symbolic reasoning. Similarly, JIGSAW [61]
targets resource access vulnerabilities. APISAN [69] is a generic
approach to ﬁnd API misuse bugs by encoding common
patterns as semantic beliefs. However, APISAN fails to capture
certain common usage patterns [25]. Furthermore, as we also
show in Section V, APISAN has a high false positive rate for
complex APIs. Machine learning techniques such as Apriori
algorithm [60], [40], clustering [66], [65], and kNN [7] are also
used to ﬁnd bugs as anomalous patterns. Similar to speciﬁcation
mining techniques, the success of these bug ﬁnding techniques
depends on the availability of a large corpus of programs.
Other bug pattern learning techniques such as Vccﬁnder [48]
and Vuldeepecker [36] also require a large bug corpus and
hence are infeasible for API misuse detection. Unlike existing
approaches, ARBITRAR requires neither the speciﬁcation nor
a large code corpus with majority valid uses. Furthermore,
our technique is general and can handle APIs of diverse use
cases and complexity. We avoid the need for a large code
corpus by actively learning from user feedback. Unlike other
active learning techniques [68], ARBITRAR learns quickly and
provides a responsive interface, even though we interact with
the user at a much ﬁner level, i.e., program traces.
Employing user feedback for bug detection has been explored
in Eugene [42] and Ursa [71], but these systems need a client
analysis to be speciﬁed in Datalog. Similarly, other user-guided
techniques based on Bayesian inference [49], [28] also require
a client analysis. In contrast, ARBITRAR does not require any
analysis speciﬁcation.
IX. CONCLUSION
We presented ARBITRAR, a user-guided approach for ﬁnding
API misuses. ARBITRAR interacts with the programmer to
classify valid and invalid uses of a target API. It employs a
novel active learning algorithm to minimize user burden. We
demonstrated the effectiveness of ARBITRAR by using it to
ﬁnd new bugs in a rich set of target APIs in large real-world
C/C++ programs within a few rounds of user interaction.
ACKNOWLEDGMENTS
We want
to thank our shepherd Nick Nikiforakis and
anonymous reviewers for their valuable comments and inputs
to improve our paper. We also thank Anthony Canino and
Kihong Heo for contributing to the initial implementation
of ARBITRAR’s symbolic execution engine. This research was
supported by grants from AFRL (#FA8750-20-2-0501), ONR
(#N00014-18-1-2021), and NSF (#1836822).
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:07 UTC from IEEE Xplore.  Restrictions apply. 
1412
REFERENCES
[1] N. Abe, B. Zadrozny, and J. Langford, “Outlier detection by active
learning,” in Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining, 2006, pp. 504–509.
[2] S. Abiteboul, R. Hull, and V. Vianu, Foundations of Databases: The
Logical Level, 1st ed. Pearson, 1994.
[3] M. Acharya, T. Xie, J. Pei, and J. Xu, “Mining api patterns as partial
orders from source code: from usage scenarios to speciﬁcations,” in
Proceedings of the the 6th joint meeting of the European software
engineering conference and the ACM SIGSOFT symposium on The
foundations of software engineering, 2007, pp. 25–34.
[4] P. Avgustinov, O. de Moor, M. P. Jones, and M. Sch¨afer, “QL: Object-
oriented queries on relational data,” in Proceedings of the European
Conference on Object-Oriented Programming (ECOOP), 2016, pp. 2:1–
2:25.
[5] T. Ball, E. Bounimova, B. Cook, V. Levin, J. Lichtenberg, C. McGarvey,
B. Ondrusek, S. K. Rajamani, and A. Ustuner, “Thorough static analysis
of device drivers,” ACM SIGOPS Operating Systems Review, vol. 40,
no. 4, pp. 73–85, 2006.
[6] P. Bian, B. Liang, W. Shi, J. Huang, and Y. Cai, “Nar-miner: discovering
negative association rules from code for bug detection,” in Proceedings
of the 2018 26th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering,
2018, pp. 411–422.
[7] P. Bian, B. Liang, Y. Zhang, C. Yang, W. Shi, and Y. Cai, “Detecting bugs
by discovering expectations and their violations,” IEEE Transactions on
Software Engineering, vol. 45, no. 10, pp. 984–1001, 2018.
[8] F. Brown, D. Stefan, and D. Engler, “Sys: a static/symbolic tool for
ﬁnding good bugs in good (browser) code,” in 29th {USENIX} Security
Symposium ({USENIX} Security 20), 2020, pp. 199–216.
[9] E. N. Ceesay, J. Zhou, M. Gertz, K. Levitt, and M. Bishop, “Using type
qualiﬁers to analyze untrusted integers and detecting security ﬂaws in c
programs,” in International Conference on Detection of Intrusions and
Malware, and Vulnerability Assessment. Springer, 2006, pp. 1–16.
[10] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A survey,”
ACM computing surveys (CSUR), vol. 41, no. 3, pp. 1–58, 2009.
[11] H. Chen and D. Wagner, “Mops: an infrastructure for examining security
properties of software,” in Proceedings of the 9th ACM conference on
Computer and communications security, 2002, pp. 235–244.
[12] M. Christakis and C. Bird, “What developers want and need from
program analysis: An empirical study,” ser. ASE 2016. New York, NY,
USA: Association for Computing Machinery, 2016. [Online]. Available:
https://doi.org/10.1145/2970276.2970347
[13] D. A. Cohn, Z. Ghahramani, and M. I. Jordan, “Active learning with
statistical models,” Journal of artiﬁcial intelligence research, vol. 4, pp.
129–145, 1996.
[14] E. Coppa, D. C. D’Elia, and C. Demetrescu, “Rethinking pointer
reasoning in symbolic execution,” in 2017 32nd IEEE/ACM International
Conference on Automated Software Engineering (ASE).
IEEE, 2017,
pp. 613–618.
[15] CWE, “Cwe-401: Missing release of memory after effective lifetime.”
2006, https://cwe.mitre.org/data/deﬁnitions/401.html.
[16] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf, “Bugs as
deviant behavior: A general approach to inferring errors in systems code,”
ACM SIGOPS Operating Systems Review, vol. 35, no. 5, pp. 57–72,
2001.
[17] D. Evans, J. Guttag, J. Horning, and Y. M. Tan, “Lclint: A tool for using
speciﬁcations to check code,” ACM SIGSOFT Software Engineering
Notes, vol. 19, no. 5, pp. 87–96, 1994.
[18] M. Georgiev, S.
Iyengar, S. Jana, R. Anubhai, D. Boneh, and
V. Shmatikov, “The most dangerous code in the world: validating ssl
certiﬁcates in non-browser software,” in Proceedings of the 2012 ACM
conference on Computer and communications security, 2012, pp. 38–49.
[19] GitHub, “Semmle - a code analysis platform for ﬁnding zero-days and
automating variant analysis.” 2017, https://semmle.com/.
[20] ——, “The bug slayer.” 2020, https://securitylab.github.com/bounties.
[21] N. G¨ornitz, M. Kloft, K. Rieck, and U. Brefeld, “Active learning for
network intrusion detection,” in Proceedings of the 2nd ACM workshop
on Security and artiﬁcial intelligence, 2009, pp. 47–54.
[22] ——, “Toward supervised anomaly detection,” Journal of Artiﬁcial
Intelligence Research, vol. 46, pp. 235–262, 2013.
[23] T. J. Green, “LogiQL: A declarative language for enterprise applications,”
in Proceedings of the Symposium on Principles of Database Systems
(PODS), 2015, pp. 59–64.
[24] Z. Gu, J. Wu, C. Li, M. Zhou, Y. Jiang, M. Gu, and J. Sun, “Vetting api
usages in c programs with imchecker,” in 2019 IEEE/ACM 41st Inter-
national Conference on Software Engineering: Companion Proceedings
(ICSE-Companion).
IEEE, 2019, pp. 91–94.
[25] Z. Gu, J. Wu, J. Liu, M. Zhou, and M. Gu, “An empirical study on
api-misuse bugs in open-source c programs,” in 2019 IEEE 43rd Annual
Computer Software and Applications Conference (COMPSAC), vol. 1.
IEEE, 2019, pp. 11–20.
[26] R. Hat, “Advanced package tool.” 2010, http://apt-rpm.org/scripting.shtml.
[27] B. He, V. Rastogi, Y. Cao, Y. Chen, V. Venkatakrishnan, R. Yang, and
Z. Zhang, “Vetting ssl usage in applications with sslint,” in 2015 IEEE
Symposium on Security and Privacy.
IEEE, 2015, pp. 519–534.
[28] K. Heo, M. Raghothaman, X. Si, and M. Naik, “Continuously reasoning
about programs using differential bayesian inference,” in Proceedings of
the 40th ACM SIGPLAN Conference on Programming Language Design
and Implementation, 2019, pp. 561–575.
[29] V. Hodge and J. Austin, “A survey of outlier detection methodologies,”
Artiﬁcial intelligence review, vol. 22, no. 2, pp. 85–126, 2004.
[30] P. Hu, Z. C. Lipton, A. Anandkumar, and D. Ramanan, “Active learning
with partial feedback,” arXiv preprint arXiv:1802.07427, 2018.
[31] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “Fuzzgen: Automatic
fuzzer generation,” in 29th {USENIX} Security Symposium ({USENIX}
Security 20), 2020.
[32] S. Kim, T. Zimmermann, K. Pan, E. James Jr et al., “Automatic identi-
ﬁcation of bug-introducing changes,” in 21st IEEE/ACM international
conference on automated software engineering (ASE’06).
IEEE, 2006,
pp. 81–90.
[33] T. Kremenek, P. Twohey, G. Back, A. Ng, and D. Engler, “From
uncertainty to belief: Inferring the speciﬁcation within,” in Proceedings
of the 7th symposium on Operating systems design and implementation,
2006, pp. 161–176.
[34] M. Law, A. Russo, and K. Broda, “Inductive learning of answer set
programs,” in Proceedings of the European Conference on Logics in
Artiﬁcial Intelligence (JELIA), 2014, pp. 311–325.
[35] O. Legunsen, W. U. Hassan, X. Xu, G. Ros¸u, and D. Marinov, “How good
are the specs? a study of the bug-ﬁnding effectiveness of existing java
api speciﬁcations,” in 2016 31st IEEE/ACM International Conference on
Automated Software Engineering (ASE).
IEEE, 2016, pp. 602–613.
[36] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y. Zhong,
“Vuldeepecker: A deep learning-based system for vulnerability detection,”
arXiv preprint arXiv:1801.01681, 2018.
[37] Z. Li and Y. Zhou, “Pr-miner: automatically extracting implicit pro-
gramming rules and detecting violations in large software code,” ACM
SIGSOFT Software Engineering Notes, vol. 30, no. 5, pp. 306–315, 2005.
[38] Z. Li, “Count number of occurrences of an api.” 2021, https://github.com/
petablox/arbitrar/blob/master/doc/how to use.md#20-get-occurrences.
[39] B. Livshits, A. V. Nori, S. K. Rajamani, and A. Banerjee, “Merlin:
speciﬁcation inference for explicit information ﬂow problems,” ACM
Sigplan Notices, vol. 44, no. 6, pp. 75–86, 2009.
[40] B. Livshits and T. Zimmermann, “Dynamine: ﬁnding common error
patterns by mining software revision histories,” ACM SIGSOFT Software
Engineering Notes, vol. 30, no. 5, pp. 296–305, 2005.
[41] A. Machiry, C. Spensky, J. Corina, N. Stephens, C. Kruegel, and G. Vigna,
“{DR}.{CHECKER}: A soundy analysis for linux kernel drivers,” in
26th {USENIX} Security Symposium ({USENIX} Security 17), 2017, pp.
1007–1024.
[42] R. Mangal, X. Zhang, A. V. Nori, and M. Naik, “A user-guided approach
to program analysis,” in Proceedings of the 2015 10th Joint Meeting on
Foundations of Software Engineering, 2015, pp. 462–473.
[43] M. Martin, B. Livshits, and M. Lam, “Finding application errors and
security ﬂaws using PQL: a program query language,” in Proceedings
of the ACM International Conference on Object-Oriented Programming,
Systems, Languages, and Applications (OOPSLA), 2005, pp. 365–383.
[44] C. Min, S. Kashyap, B. Lee, C. Song, and T. Kim, “Cross-checking
semantic correctness: The case of ﬁnding ﬁle system bugs,” in Proceed-
ings of the 25th Symposium on Operating Systems Principles, 2015, pp.
361–377.
[45] H. A. Nguyen, R. Dyer, T. N. Nguyen, and H. Rajan, “Mining
preconditions of apis in large-scale code corpus,” in Proceedings of
the 22nd ACM SIGSOFT International Symposium on Foundations of
Software Engineering, 2014, pp. 166–177.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:07 UTC from IEEE Xplore.  Restrictions apply. 
1413
[69] I. Yun, C. Min, X. Si, Y. Jang, T. Kim, and M. Naik, “Apisan: Sanitizing
{API} usages through semantic cross-checking,” in 25th {USENIX}
Security Symposium ({USENIX} Security 16), 2016, pp. 363–378.
[70] T. Zhang, G. Upadhyaya, A. Reinhardt, H. Rajan, and M. Kim, “Are
code examples on an online q&a forum reliable?: a study of api misuse
on stack overﬂow,” in 2018 IEEE/ACM 40th International Conference
on Software Engineering (ICSE).
IEEE, 2018, pp. 886–896.
[71] X. Zhang, R. Grigore, X. Si, and M. Naik, “Effective interactive resolution
of static analysis alarms,” Proceedings of the ACM on Programming
Languages, vol. 1, no. OOPSLA, pp. 1–30, 2017.
[46] NIST, “National vulnerability database.” 2020, https://nvd.nist.gov/
vuln/search/results?form type=Basic&results type=overview&query=
incorrect+usage&search type=all.
[47] J. Pei, H. Wang, J. Liu, K. Wang, J. Wang, and P. S. Yu, “Discovering
frequent closed partial orders from strings,” IEEE Transactions on
Knowledge and Data Engineering, vol. 18, no. 11, pp. 1467–1481,
2006.
[48] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl,
and Y. Acar, “Vccﬁnder: Finding potential vulnerabilities in open-source
projects to assist code audits,” in Proceedings of the 22nd ACM SIGSAC
Conference on Computer and Communications Security, 2015, pp. 426–
437.
[49] M. Raghothaman, S. Kulkarni, K. Heo, and M. Naik, “User-guided
program reasoning using bayesian inference,” in Proceedings of the
39th ACM SIGPLAN Conference on Programming Language Design and
Implementation, 2018, pp. 722–735.
[50] M. Raghothaman, J. Mendelson, D. Zhao, M. Naik, and B. Scholz,
“Provenance-guided synthesis of datalog programs,” in Proceedings of
the ACM Symposium on Principles of Programming Languages (POPL),
2020, pp. 62:1–62:27.
[51] D. A. Ramos and D. Engler, “Under-constrained symbolic execution:
Correctness checking for real code,” in 24th {USENIX} Security
Symposium ({USENIX} Security 15), 2015, pp. 49–64.
[52] N. Redini, A. Machiry, D. Das, Y. Fratantonio, A. Bianchi, E. Gustafson,
Y. Shoshitaishvili, C. Kruegel, and G. Vigna, “Bootstomp: on the security
of bootloaders in mobile devices,” in 26th {USENIX} Security Symposium
({USENIX} Security 17), 2017, pp. 781–798.
[53] B. Sch¨olkopf, R. C. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt,
“Support vector method for novelty detection,” Advances in neural
information processing systems, vol. 12, pp. 582–588, 1999.
[54] U. Shankar, K. Talwar, J. S. Foster, and D. A. Wagner, “Detecting
format string vulnerabilities with type qualiﬁers.” in USENIX Security
Symposium, 2001, pp. 201–220.
[55] Y. Smaragdakis and M. Bravenboer, “Using Datalog for fast and easy
program analysis,” in Proceedings of the Datalog 2.0 Workshop, 2010.
[56] Y. Sui and J. Xue, “Svf: interprocedural static value-ﬂow analysis in
llvm,” in Proceedings of the 25th international conference on compiler
construction. ACM, 2016, pp. 265–266.
[57] D. M. Tax and R. P. Duin, “Support vector data description,” Machine
learning, vol. 54, no. 1, pp. 45–66, 2004.
[58] L. Torvalds, “Sparse - a semantic parser for c.” 2013, https://sparse.wiki.
kernel.org/index.php/Main Page.
[59] B. A. Turlach, “Bandwidth selection in kernel density estimation: A
review,” in CORE and Institut de Statistique. Citeseer, 1993.
in KDD Workshop, 1994.
[60] A. I. Verkamo, “Efﬁcient algorithms for discovering association rules,”
[61] H. Vijayakumar, X. Ge, M. Payer, and T. Jaeger, “{JIGSAW}: Pro-
tecting resource access by inferring programmer expectations,” in 23rd
{USENIX} Security Symposium ({USENIX} Security 14), 2014, pp.
973–988.
[62] X. Wang, H. Chen, Z. Jia, N. Zeldovich, and M. F. Kaashoek, “Improving
integer security for systems with {KINT},” in Presented as part of
the 10th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 12), 2012, pp. 163–177.
[63] A. Wasylkowski, A. Zeller, and C. Lindig, “Detecting object usage
anomalies,” in Proceedings of the the 6th joint meeting of the European
software engineering conference and the ACM SIGSOFT symposium on
The foundations of software engineering, 2007, pp. 35–44.
[64] J. Whaley and M. Lam, “Cloning-based context-sensitive pointer alias
analysis using binary decision diagrams,” in Proceedings of the ACM
SIGPLAN Conference on Programming Language Design and Implemen-
tation (PLDI), 2004, pp. 131–144.
[65] F. Yamaguchi, A. Maier, H. Gascon, and K. Rieck, “Automatic inference
of search patterns for taint-style vulnerabilities,” in 2015 IEEE Symposium
on Security and Privacy.
IEEE, 2015, pp. 797–812.
[66] F. Yamaguchi, C. Wressnegger, H. Gascon, and K. Rieck, “Chucky:
Exposing missing checks in source code for vulnerability discovery,”
in Proceedings of the 2013 ACM SIGSAC conference on Computer &
communications security, 2013, pp. 499–510.
[67] YAML, “Yaml ain’t markup language.” 2001, https://yaml.org/.
[68] Z. Yu, C. Theisen, L. Williams, and T. Menzies, “Improving vulnerability
inspection efﬁciency using active learning,” IEEE Transactions on
Software Engineering, pp. 1–1, 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:07 UTC from IEEE Xplore.  Restrictions apply. 
1414
import cpp
import semmle.code.cpp.dataflow.DataFlow
class DestroyWriteCall extends FunctionCall {
DestroyWriteCall() {
this.getTarget().getName() = "png_destroy_write_struct"
}
}
class CreateInfoCall extends FunctionCall {
CreateInfoCall() {
this.getTarget().getName() = "png_create_info_struct"
}
}
predicate zeroComparison(EqualityOperation e, Variable v) {
exists (Expr zero |
zero.getValue() = "0" |
(zero = e.getLeftOperand() and
v = e.getRightOperand().(VariableAccess).getTarget())
(cid:44)→
or
(zero = e.getRightOperand() and
v = e.getLeftOperand().(VariableAccess).getTarget()))
}
from
EqualityOperation e,
Variable info_ptr,
IfStmt control,
DestroyWriteCall destroy_write_call
where
zeroComparison(e, info_ptr) and
exists (AssignExpr assign |
assign.getEnclosingFunction() =
control.getEnclosingFunction() and
assign.getLValue().(VariableAccess).getTarget() =
info_ptr and
exists (CreateInfoCall info_call |
assign.getRValue() = info_call)) and
control.getControllingExpr() = e and
destroy_write_call.getArgument(1).getValue() = "0" and
destroy_write_call.getEnclosingFunction() =
control.getEnclosingFunction() and
not control.getThen().getBasicBlock()
.contains(destroy_write_call.getBasicBlock())
select e, control, destroy_write_call
Listing 5: Semmle checker for png_destroy_write_struct
Memory Leak Bug
A. Semmle Checker
APPENDIX
Listing 5 shows the Semmle Checker to ﬁnd the occurrences
of bug similar to the bug in the Listing 1.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:07 UTC from IEEE Xplore.  Restrictions apply. 
1415