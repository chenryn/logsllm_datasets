transmission rate for each flow. Switches in PDQ and D3 allocate
bandwidth for each flow based on the available link capacity. pFab-
ric achieves near-optimal FCT by using (infinity) priority queues
on switches. HPCC adopts the switch loading information directly
from INT. However, it remains an open question on how to tune
the ECN threshold there. pHost[21], Homa[39] rely on the receiver
to send credit packets to determine the sending rate of each flow.
TIMELY[35] and Swift[27] are RTT-based schemes to adjust the
flow rate at end-host. These approaches achieve remarkable per-
formance. However they require modification of networking stack
which is not easy to implementation for RDMA hardware-based
implementation.
Tuning ECN in Datacenter. Extensive studies have been pro-
posed to optimize the delay and throughput performance by prop-
erly determining the ECN marking threshold. Adaptive AQM mech-
anisms have been introduced in the traditional TCP/IP network,
which update the virtual queue capacity based on the arrival rate
[28, 61]. For the modern datacenter networks, ECN∗ [57] shows
that if the instant queue length based ECN threshold is properly
tuned, it is possible for RED-like probabilities marking to achieve
optimal incast performance. It is notable that though ECN switches
accept two threshold parameters, the low and high thresholds, prior
pioneer works usually set the two thresholds to the same value, i.e.,
they only consider one single threshold in the studies. TCN [10]
applies the sojourn time, i.e., the amount of time a packet spends
in the queue, to mark packets. ECN# [62] studies RTT variation in
DCN and marked packets based on both instantaneous and persis-
tent congestion states. ACC considers the operational challenges
and uses dynamic ECN on commodity switches.
Learning-based Network Optimization. Learning-based approaches
have been applied to handle flow-level traffic optimization [17], and
parameters setting for congestion control at end-host [26, 45, 56].
Remy[56] and Indigo [59] learn to adjust the rate from pre-collected
sampling network traffic. Vivace [19] utilizes an online algorithm
and Aurora [25] uses DRL technique to update sending rate. To
avoid performance issue to un-predictable traffic, Orca [5] uses
conventional TCP Cubic combined with learning methods. How-
ever, most of the learning-based approaches are designed to adjust
sending rates at end-host according to the feedback passively. None
of them studies in-network optimization on the feedback like ECN.
ACC is compatible with ECN-based solutions and adaptive to traffic
variations.
8 CONCLUSION
We introduced our operational experience for automatic in-network
optimization. ECN is the key to achieve low latency, high through-
put communications with the state-of-the-art congestion control
schemes. We propose ACC, an pragmatic approach which allows
automatic ECN parameters tuning at each switch. By leveraging
the deep reinforcement learning, ACC can greatly improve the flow
completion time for small messages and maintain high throughput
for large messages. Without any modification at end-host, ACC
has been quickly deployed in the production datacenter to support
storage and computing services stably.
This work does not raise any ethical issues.
395
ACC: Automatic ECN Tuning for High-Speed Datacenter Networks
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
ACKNOWLEDGMENTS
The authors thank the anonymous reviewers and our shepherd,
Keon Jang, for providing valuable feedback. We would also like to
thank Yashar Ganjali, Camtu Nguyen and the teams at Huawei for
their contributions to the work.
com.
REFERENCES
[1] InfiniBand Architecture Volume 1 and released specification Volume 2. 2015.
https://cw.infinibandta.org/document/dl/7859.
[2] Network Simulator 3. 2019. https://wwwnsnam.org.
[3] IEEE. 802.11Qau. 2011. Priority based flow control (PFC).
[4] S. Abbasloo, C. Y. Yen, and H. J. Chao. 2020. Classic Meets Modern: a Pragmatic
Learning-Based Congestion Control for the Internet. In SIGCOMM ’20: Annual
conference of the ACM Special Interest Group on Data Communication on the
applications, technologies, architectures, and protocols for computer communication.
[5] Soheil Abbasloo, Chen-Yu Yen, and H Jonathan Chao. 2020. Classic meets mod-
ern: A pragmatic learning-based congestion control for the Internet. In ACM
SIGCOMM. 632–647.
[6] Mohammad Alizadeh, Javanmard Adel, and Balaji Prabhakar. 2011. Analysis of
DCTCP: stability, convergence, and fairness. In ACM SIGMETRICS Performance
Evaluation Review. 73–84.
[7] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. 2010.
Data Center TCP (DCTCP). In ACM SIGCOMM.
[8] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown,
Balaji Prabhakar, and Scott Shenker. 2013. pFabric: minimal near-optimal data-
center transport. In ACM SIGCOMM.
[9] InfiniBand Trade Association. 2014. Supplement to InfiniBand architecture
specification volume 1 release 1.2.2 annex A17: RoCEv2 (IP routable RoCE).
[10] Wei Bai, Kai Chen, Li Chen, Changhoon Kim, and Haitao Wu. 2016. Enabling
ECN over generic packet scheduling. In ACM CoNEXT.
[11] In band Network Telemetry in Barefoot Tofino. 2019.
barefootnetworks.com/use-cases/ad-telemetry.
[12] In band Network Telemetry in Broadcom Tomahawk 3. 2019. https://www.
broadcom.com/company/news/product-releases/2372840..
[13] In band Network Telemetry in Broadcom Trident 3. 2019. https://www.broadcom.
https://www.
[14] Claude Barthels, Simon Loesing, Gustavo Alonso, and Donald Kossmann. 2015.
Rack-scale in-memory join processing using RDMA. In ACM SIGMOD.
[15] Flexible Input/Output benchmark. 2017. http://github.com/axboe/fio.
[16] Linpack benchmark. 2018. http://www.netlib.org/benchmark/hpl/.
[17] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. Auto: Scaling deep
reinforcement learning for datacenter-scale automatic traffic optimization. In
ACM SIGCOMM.
[18] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuapn Zhang, and Zheng Zhang. 2015. MXNet: A Flexible
and Efficient Machine Learning Library for Heterogeneous Distributed Systems.
Statistics (2015).
[19] M. Dong, Tong Meng, Doron Zarchy, E. Arslan, Y. Gilad, B. Godfrey, and M.
Schapira. 2018. PCC Vivace: Online-Learning Congestion Control. In USENIX
NSDI.
[20] Sally Floyd and Van Jacobson. 1993. Random early detection gateways for
congestion avoidance. IEEE/ACM Transactions on networking 1, 4 (1993), 397–
413.
[21] Peter X Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia Ratnasamy,
and Scott Shenker. 2015. phost: Distributed near-optimal datacenter transport
over commodity network fabric. In ACM CoNEXT.
[22] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta
Sengupta. 2009. VL2: A Scalable and Flexible Data Center Network. In ACM
SIGCOMM.
[23] Chi-Yao Hong, Matthew Caesar, and P Brighten Godfrey. 2012. Finishing flows
quickly with preemptive scheduling. ACM SIGCOMM Computer Communication
Review 42, 4 (2012), 127–138.
[24] Horovod. 2018. https://github.com/horovod/horovod.
[25] Nathan Jay, Noga Rotman, Brighten Godfrey, Michael Schapira, and Aviv Tamar.
2019. A deep reinforcement learning perspective on internet congestion control.
In International Conference on Machine Learning (ICML). 3050–3059.
[26] Lavanya Jose, Lisa Yan, Mohammad Alizadeh, George Varghese, Nick McKeown,
and Sachin Katti. 2015. High speed networks need proactive congestion control.
In ACM Workshop on Hot Topics in Networks.
[27] Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan M. G. Wassel, Xian
Wu, Behnam Montazeri, Yaogong Wang, Kevin Springborn, Christopher Alfeld,
Michael Ryan, David Wetherall, and Amin Vahdat. 2020. Swift: Delay is Simple
and Effective for Congestion Control in the Datacenter. In ACM SIGCOMM.
396
[28] Srisankar Kunniyur and Rayadurgam Srikant. 2001. Analysis and design of an
adaptive virtual queue (AVQ) algorithm for active queue management. In ACM
SIGCOMM.
[29] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang,
Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and et al. 2019.
HPCC: High Precision Congestion Control. In ACM SIGCOMM.
[30] Xiaoyi Lu, Nusrat S Islam, Md Wasi-Ur-Rahman, Jithin Jose, Hari Subramoni, Hao
Wang, and Dhabaleswar K Panda. 2013. High-performance design of Hadoop RPC
with RDMA over InfiniBand. In International Conference on Parallel Processing
(ICPP). IEEE, 641–650.
[31] Xiaoyi Lu, Md Wasi Ur Rahman, Nahina Islam, Dipti Shankar, and Dhabaleswar K
Panda. 2014. Accelerating spark with RDMA for big data processing: Early
experiences. In Annual Symposium on High Performance Interconnects.
[32] Xiaoyi Lu, Dipti Shankar, Shashank Gugnani, and Dhabaleswar K DK Panda.
2016. High-performance design of apache spark with RDMA and its benefits on
various workloads. In International Conference on Big Data (Big Data). IEEE.
[33] Youyou Lu, Jiwu Shu, Youmin Chen, and Tao Li. 2017. Octopus: an RDMA-enabled
distributed persistent memory file system. In USENIX ATC.
[34] Jianmin Chen Zhifeng Chen Andy Davis Jeffrey Dean Matthieu Devin Sanjay
Ghemawat Geoffrey Irving Michael Isard Manjunath Kudlur Josh Levenberg
Rajat Monga Sherry Moore Derek G. Murray Benoit Steiner Paul Tucker Vijay
Vasudevan Pete Warden Martin Wicke Yuan Yu MartÃŋn Abadi, Paul Barham and
Xiaoqiang Zheng. 2016. Tensorflow: A system for large-scale machine learning.
In USENIX OSDI.
[35] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David
Zats. 2015. TIMELY: RTT-based Congestion Control for the Datacenter. In ACM
SIGCOMM.
[36] Radhika Mittal, Alexander Shpiner, Aurojit Panda, Eitan Zahavi, Arvind Krishna-
murthy, Sylvia Ratnasamy, and Scott Shenker. 2018. Revisiting Network Support
for RDMA. In ACM SIGCOMM.
[37] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Tim-
othy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asyn-
chronous methods for deep reinforcement learning. In International conference
on machine learning. PMLR, 1928–1937.
[38] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al. 2015. Human-level control through deep reinforcement learning.
nature 518, 7540 (2015), 529–533.
[39] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018.
Homa: A Receiver-driven Low-latency Transport Protocol Using Network Priori-
ties. In ACM SIGCOMM.
[40] Mellanox Perftest Package. 2017. https://community.mellanox.com/docs/DOC-
2802.
SparkRDMA.
[41] SparkRDMA Shuffle Manager Plugin. 2018.
https://github.com/Mellanox/
[42] Haonan Qiu, Xiaoliang Wang, Tianchen Jin, Zhuzhong Qian, Baoliu Ye, Bin Tang,
Wenzhong Li, and Sanglu Lu. 2018. Toward Effective and Fair RDMA Resource
Sharing. In Asia-Pacific Workshop on Networking (APNet).
[43] K Ramakrishnan, Sally Floyd, and D Black. 2001. RFC3168: The addition of
explicit congestion notification (ECN) to IP.
[44] Danfeng Shan and Fengyuan Ren. 2018. ECN Marking With Micro-Burst Traffic:
Problem, Analysis, and Improvement. IEEE/ACM Transactions on Networking 26,
4 (2018), 1533–1546.
[45] Anirudh Sivaraman, Keith Winstein, Pratiksha Thaker, and Hari Balakrishnan.
2014. An experimental study of the learnability of congestion control. ACM
SIGCOMM Computer Communication Review 44, 4 (2014), 479–490.
[46] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-
[47] Iperf: The tcp/udp bandwidth measurement tool. 2005. dast. nlanr. net/Projects
[48] Quantum Espresso test suite. 2019. https://www.quantum-espresso.org/.
[49] Barefoot Tofino. 2019. https://www.barefootnetworks.com/.
[50] Broadcom Tomahawk. 2019.
https://www.broadcom.com/company/news/
duction. MIT press.
(2005), 38.
product-releases.
[51] Broadcom Trident. 2019. https://www.broadcom.com.
[52] Hasselt Van, Guez Hado, Arthur, and Silver David. 2016. Deep reinforcement
learning with double q-learning. In AAAI conference on artificial intelligence.
[53] Yandong Wang, Li Zhang, Jian Tan, Min Li, Yuqing Gao, Xavier Guerin, Xiaoqiao
Meng, and Shicong Meng. 2015. HydraDB: a resilient RDMA-driven key-value
middleware for in-memory cluster computing. In International Conference for
High Performance Computing, Networking, Storage and Analysis (SC). IEEE, 1–11.
[54] Xingda Wei, Zhiyuan Dong, Rong Chen, and Haibo Chen. 2018. Deconstructing
RDMA-enabled Distributed Transactions: Hybrid is Better!. In USENIX OSDI.
[55] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011.
Better never than late: Meeting deadlines in datacenter networks. ACM SIGCOMM
Computer Communication Review 41, 4 (2011), 50–61.
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Yan et al.
(a) Queue length reward
(b) Converged action decisions
Figure 17: Reward function designs and the comparison of
converged action decisions
[56] Keith Winstein and Hari Balakrishnan. 2013. Tcp ex machina: Computer-
generated congestion control. ACM SIGCOMM Computer Communication Review
43, 4 (2013), 123–134.
[57] Haitao Wu, Jiabo Ju, Guohan Lu, Chuanxiong Guo, Yongqiang Xiong, and Yong-
guang Zhang. 2012. Tuning ECN for data center networks. In ACM CoNEXT.
[58] Ming Wu, Fan Yang, Jilong Xue, Wencong Xiao, Youshan Miao, Lan Wei, Haoxiang
Lin, Yafei Dai, and Lidong Zhou. 2015. GRAM: scaling graph computation to the
trillions. In ACM Symposium on Cloud Computing (SoCC).
[59] Francis Y Yan, Jestin Ma, Greg D Hill, Deepti Raghavan, Riad S Wahby, Philip
Levis, and Keith Winstein. 2018. Pantheon: the training ground for Internet
congestion-control research. In USENIX ATC.
[60] Lingbo Tang Yongqing Xi Pengcheng Zhang Wenwen Peng Bo Li Yaohui Wu
Shaozong Liu Lei Yan Fei Feng Yan Zhuang Fan Liu Pan Liu Xingkui Liu Zhongjie
Wu Junping Wu Yixiao Gao, Qiang Li, Jinbo Wu Jiaji Zhu Haiyong Wang Den-
nis Cai Zheng Cao, Chen Tian, and Jiesheng Wu. 2021. When Cloud Storage
Meets RDMA. In USENIX NSDI.
[61] Honggang Zhang, Don Towsley, C. V. Hollot, and Vishal Misra. 2003. A Self-
Tuning Structure for Adaptation in TCP/AQM Networks. SIGMETRICS Perform.
Eval. Rev. 31, 1 (june 2003).
[62] Junxue Zhang, Wei Bai, and Kai Chen. 2019. Enabling ECN for datacenter
networks with RTT variations. In ACM CoNEXT.
[63] Qiao Zhang, Vincent Liu, Hongyi Zeng, and Arvind Krishnamurthy. 2017. High-
resolution measurement of data center microbursts. In ACM Internet Measurement
Conference (IMC). 78–85.
[64] Yiwen Zhang, Juncheng Gu, Youngmoon Lee, Mosharaf Chowdhury, and Kang G.
Shin. 2017. Performance Isolation Anomalies in RDMA. In KBNets. ACM.
[65] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn,
Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and
Ming Zhang. 2015. Congestion control for large-scale RDMA deployments. In
ACM SIGCOMM.
[66] Yibo Zhu, Monia Ghobadi, Vishal Misra, and Jitendra Padhye. 2016. ECN or
Delay: Lessons Learnt from Analysis of DCQCN and TIMELY. In ACM CoNEXT.
APPENDIX
Appendices are supporting material that has not been peer-reviewed.
.1 Impact of Reward Design
The design of reward function has a great impact on the perfor-
mance of DRL agent. To achieve a deep understanding on the design
of reward function, we demonstrate two typical reward function
designs here. As shown in Figure 17(a), design-1 applies linear func-
tion, i.e., D(L) = 1 − L/Qmax in Equation (2) (Qmax is the value of
allocated buffer in one service pool Qmax = 10MB in the testbed).
Design-2 is our mapping function of queue length reward, which
maps the queue depth stepwise in Figure 4. To evaluate the perfor-
mance of two designs, we choose ten levels of high ECN thresholds.
Under the scenario of incast congestion, the action decisions made
by two designs are shown in Figure 17(b). We can see that ACC with
reward Design-2 achieves the expected action. To reveal the reason,
we review the reward functions. For Design-1, if queue length is
directly mapped to reward by a linear function, the rewards are
similar for different actions. To introduce differentiation of different
actions, we propose the step function. it uses fine-grained inter-
vals for shallow queue depth and coarse-grained intervals for large
queue size. The reasons for this piecewise mapping design lie in:
(1) Most network congestion happens at a small queue size (1MB) always implies a long queue
latency (>300us at 25Gbps link), which is two orders of magni-
tude larger than transmit latency and it suffices requirement of the
design.
397