random variable that is the distribution of the strings which are the 
passwords  derived  from  the  grammar.  More  precisely,  it  is  the 
distribution  of  derivation  trees  but  since  the  grammar  is  non-
ambiguous  it  can  be  viewed  as  the  strings  themselves.  The 
context-free grammar  for a password distribution can be viewed 
as composed of several distributions. One is from the start symbol 
S to the base structures, which we call the random variable B, and 
the second one is from the sentential forms of base structures to 
the  terminals  (the  password  guesses)  which  we  call  the  random 
variable R (for rest). For example, if we have base structures that 
can  take  n  different  values  b1,  b2(cid:15)(cid:3) (cid:171)(cid:3) (cid:15)(cid:3) (cid:69)n  where  n  is  the  total 
number  of  base  structures,  then  since  S(cid:198)bi  then  we  have   
p(S(cid:198)bi)  =  p(B=bi).  Note  also  that  the  random  variable  R  |  B  is 
itself computable from the probability distributions represented by 
each component of the base structure because of independence of 
the  component  derivations.  Table  4  is  a  simple  example  of  the 
context  free  grammar  of  Table  1  illustrating  B  and  R  |  B  with 
some sample probability values. 
Table 4. Example CFG for entropy calculation 
Random Variable B 
Rule 
S(cid:198)D3L3S1 
Probability  Rule 
0.8 
Random Variable R | B 
D3L3S1(cid:198)123dog! 
              123dog# 
              123cat!      
             987cat# 
S2L3(cid:198)      **cat 
                  !!dog 
Probability 
0.1976 
0.1824 
0.1976 
0.0576 
0.31 
0.085 
S(cid:198)S2L3 
0.2 
Not shown are the random variables L5, D3, S1, L3, and D2. For 
example, the random variable D3 has the distribution as shown in 
Table 1. We have derived the following theorem that can be used 
to calculate Shannon entropy from a context-free grammar. 
Theorem 5.2 (Entropy of a Grammar): The Shannon entropy of 
a probabilistic context free grammar H(G) can be computed from 
the  entropies  of  the  various  random  variables  defining  the 
grammar.  Assume  the  base  structure  bi  is  of  the  form  Xi1Xi2  … 
Xiki  where  each  component  is  of  the  form  Lj  or  Dj  or  Sj  in  the 
grammar. Then: 
H(G) = H(B,R) = H(B) + H(R | B) 
        = H(B) + 
                       (5.2) 
)] 
The  proof  of  the  above  is  fairly  straightforward  from  the 
definitions of joint and conditional entropy. Using  Theorem 5.2, 
for the simple grammar of Table 1 we have H(G) =3.42. 
Note  that  we  now  have  two  different  ways  to  calculate  the 
Shannon  entropy  of  our  probabilistic  distribution  G:  one  is 
through generating the password guesses directly and computing 
the  entropy  and  the  other  is  using  the  grammar  itself  through 
Theorem  5.2.  Clearly  these  should  be  the  same.  We  did  a  few 
experiments  to  verify  this  on  a  few  small  sets  of  real  user 
passwords and the entropy values came out as expected.  
5.4  Increasing Shannon Entropy 
We  did  a  small  experiment  to  test  our  updating  approach.  The 
experiment was done on a password training set of 740 real user 
passwords randomly chosen from the MySpace set which resulted 
in 37667 password guesses. We selected the new password for the 
user in such a way that its probability  was less than or equal to 
1/n,  n  being  the  total  number  of  passwords  in  the  distribution. 
Then the probability of the base structures and other components 
were  updated  with  the  technique  described  in  the  previous 
subsection. These steps were repeated until there was no password 
with probability less than 1/n (the distribution became uniform). 
The 
this  uniform 
distribution  is 15.2.  Figure  2  shows  the  changes  in  the  Shannon 
entropy for each update round. As is evident, the system seems to 
be  approaching  the  theoretical  maximum  Shannon  entropy.  We 
found a similar result for the guessing entropy. 
theoretical  Shannon  entropy  value 
for 
in 
Figure 2. Shannon entropy changes with the update algorithm 
Theoretically, having a uniform distribution for passwords is ideal 
since 
that  distribution  all  passwords  will  have  equal 
probabilities. Practically, this  would mean that each password is 
equivalent to being randomly chosen. Note that using our update 
algorithm we are moving closer to a uniform distribution but are 
likely  very  far  away  from  it.  For  example,  we  only  use  words 
proposed by users and only modify the case. Thus, we will not use 
the full key space of alpha strings and it is similarly unlikely that 
we  will  ever  exhaust  the  space  of  all  10  digit  numbers. 
Nevertheless,  while  maintaining  usability,  we  believe  that  our 
grammar modifying approach will ensure that an attacker cannot 
take  advantage  of  using  a  probabilistic  password  cracking 
approach.  Note  that  in  our  update  algorithm,  when  updating  the 
training  set,  we  are  not  changing  the  probabilities  of  the 
115
passwords  directly,  but  we  are  only  changing  the  password 
distribution  implicitly  by  changing  the  context-free  grammar. 
Thus  it  is  not  obvious  that  we  would  approach  the  maximum 
Shannon entropy possible for that grammar. 
6.  TESTING THE AMP SYSTEM 
6.1  Preprocessing and Experiment Setup 
We  tested  the  effectiveness  of  our  analysis  and  modification 
system on several sets of revealed passwords. We call the AMP 
analysis component the AMP password checker. The grammar of 
this password checker is used to set the thresholds between strong 
and  weak  passwords  and  do  the  analysis  of  user  proposed 
passwords. We used two different password cracking approaches 
to try to break passwords, including those that had been identified 
as weak and made strong by our AMP system. 
We  obtained  three  different  lists  of  revealed  passwords  for  our 
experiments.    The  first  list  is  the  RockYou  password  list  [28], 
which  was  released  in  December  2009  and  contains  32  million 
passwords. We used 2 million random plain text passwords from 
this  list  for  our  experiments.  The  second  list  is  a  MySpace 
password  list,  which  contains  61,995  plain  text  passwords  and 
was the result of an attack  against MySpace users in 2006 [29]. 
The  third  list  is  the  result  of  an  attack  against  Hotmail  users  in 
October 2009 and contains 9,748 plain text passwords [30].  
We  randomly  split  each  of  these  lists  into  separate  sets  for  (1) 
training  the  AMP  password  checker  (RockYou:  1  Million, 
MySpace:  30,997,  Hotmail:  4874);  (2)  testing  the  AMP  system 
(RockYou: ½ Million, MySpace: 15,499, Hotmail: 2,437); and (3) 
training  a  probabilistic  password  cracker  (RockYou:  ½  Million, 
MySpace:  15,499,  Hotmail:  2,437).  Note  that  the  probabilistic 
password cracker is thus trained on a different set than is used for 
the  AMP  password  checker.  For  the  training  sets,  we  combined 
passwords  from  the  RockYou,  MySpace,  and  Hotmail  lists 
together  since  we  wanted  to  have  a  comprehensive  set  for  the 
training  and  because  these  different  websites  might  have  had 
different  password  policies  for  required  lengths  and  other  rule-
based  restrictions.  We  used  “common_passwords”  [4]  and  “dic-
0294”[31] as input dictionaries to both our AMP checker and the 
probabilistic password cracker. Note that in our password checker 
we  do  not  actually  check  the  alphabetical  part  of  the  password 
against the dictionary; we assume the alphabetical part is included 
in the dictionary and we just use the probability value of words of 
that length for that component.  
We  set  the  threshold  value  for  our  experiments  using  the  first 
approach in Section 3 and the AMP password checker grammar to 
generate the guesses and their probability values. The results were 
shown in Table 2. Note that the times shown in this table are the 
corresponding times for performing an MD5 hash on that number 
of guesses on the specific machine we used for cracking. At this 
point  we  have  completed  the  preprocessing  phase  of  the  AMP 
system and can set a threshold as desired. Note that the Shannon 
entropy  value  for  this  grammar  calculated  by  Theorem  5.2  is 
26.78. 
6.2  Implementation 
The  user  interface  of  AMP  (written  in  Java)  takes  either  an 
individual password as input or a list of passwords. It checks the 
probability  of  the  user  proposed  password  against  the  threshold 
and tries to strengthen it within edit distance one if the password 
is  weak.  Note  that  we  might  not  be  able  to  strengthen  some 
passwords since we currently only modify within edit distance of 
one in a certain way (such as keeping the alphabetical part). If a 
password has a high probability and has only a small number of 
components we might not be able to alter it using an edit distance 
of  1  to  get  the  probability  below  the  threshold  value.  In  our 
experiments  we  set  the  threshold  value  equivalent  to  different 
time  periods;  for  example,  one  day  (24  hours),  meaning  that  a 
password is called weak if it can be cracked within one day, and it 
is strong otherwise. A one day threshold value is obviously not an 
ideal  value  in  real  life  and  we  use  other  values  in  later  tests. 
Figure  3  shows  a  snapshot  of  the  AMP  system  with  the  user 
proposed password “life45!” as the input. The probability of the 
user-selected password as well as the probability value of the new 
password is shown along with the approximate time to crack. 
threshold. The third group of passwords, referred to as originally 
weak passwords able to make stronger, are passwords that were 
determined as weak passwords by AMP and for which the AMP 
system was also able to strengthen them with modifications within 
edit distance one. Note that this set is the weak passwords without 
modification. The associated modified passwords are in the fourth 
group,  which  is  consequently  called  strengthened  passwords 
modified from weak in the tables. Note that these are now strong 
passwords  are  determined  by  the  AMP  system  (relative  to  the 
threshold).  Results  show 
that  both  originally  strong  and 
strengthened passwords modified from weak passwords have very 
low  rate  of  cracking  compared  with  weak  passwords.  John  the 
Ripper  generally  cracked  less  than  1%  of  the  strong  passwords 
and the Probabilistic Password Cracker cracked about 5%. 
Table 5. Password cracking results using John the Ripper 
Strengthened 
Passwords 
Modified 
Originally 
Strong 
Passwords 
Originally
Weak 
Originally 
Weak 
Passwords 
(Not able 
to make 
stronger) 
Passwords 
(Able to 
make 
stronger) 
from Weak of 
previous 
column 
Hotmail 
Percentage
MySpace 
Percentage 
RockYou 
Percentage 
(0.61%) 
(92.45%) 
(47.98%) 
(1.55%) 
(69.80%) 
(38.53%) 
(0.0975%) 
(0.51%) 
(0.86%) 
(89.90%) 
(53.18%) 
(0.27%) 
Table 6. Password cracking results using PPC
Originally 
Strong 
Passwords 
Originally 
Weak 
Passwords 
(Not able 
to make 
stronger)
Originally 
Weak 
Passwords 
(Able to 
make 
stronger) 
Strengthened 
Passwords 
Modified 
from Weak of 
previous 
column 
Figure 3. Snapshot of the AMP user interface 