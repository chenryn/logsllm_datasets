416K
2723K
463K
1558K
399K
62K
124K
6.1K
124K
9K
78.5K
17K
100.9K
20K
73.8K
0
0
0.64
0.61
0.68
0.67
0.87
0.86
0.77
0.75
0
0
0.64
0.56
0.66
0.64
0.87
0.84
0.77
0.73
0
0
0.64
0.54
0.65
0.62
0.86
0.83
0.77
0.72
(a) P wait : 200ms
(b) P wait : 50ms
Fig. 8: Average response time for the workloads considered
already employ RAID-0, and we denote this conﬁguration as
ECC in the experimental results because the ECC technique
is the only measure used to cope with bit errors.
as deﬁned in the model analysis of the previous section. They
are used to explain the performance numbers presented in this
section and Section VI.
The model that was described in the previous section is
validated with the experimental results in Section VI.
A. Evaluation Environment
For our evaluation, we implemented the ECC, RAID-5, and
eSAP schemes by modifying the DiskSim SSD extension [22].
The simulator parameters are presented in Table I. In the
experiments, 5% of ﬂash memory space is preserved as over-
provisoned space. In all conﬁgurations,
there are 8 ﬂash
memory chips and thus, a physical stripe consists of eight
pages that belong to different chips.
The ﬁrst column of Table II lists the workloads and its
characteristics, the total request size and the write ratio of the
requests we used for the evaluation. Sequential and Random
are synthetic workloads that access data sequentially and ran-
domly with uniform distribution, respectively. The Financial
workload was collected at ﬁnancial institutions and is mainly a
random write intensive workload from OLTP applications [1].
The Exchange workload represents random I/O characteristics
of an Exchange server that serves 5000 corporate users. This
workload is composed of 9 volumes collected over a 24-hour
period, and we use the trace of volume number 2 [20]. The
MSN trace is collected from four RAID-10 volumes of the
MSN storage back-end ﬁle store for a 6 hour duration, and we
use the trace of volume 2 in our experiments [20].
The numbers in the third column and beyond of Table II
are the postmortem numbers obtained for RAID-5 and eSAP
B. Performance Results
For RAID-5 and eSAP, Pwait is needed. We report experi-
ment results for two values of Pwait: 50 and 200 milliseconds.
These values were chosen based on the observation of inter-
arrival time of requests for our workloads as shown in Fig. 7,
where the cumulative distribution is shown. As the ﬁgure
shows, essentially all inter-arrival times are 300 milliseconds
or less. Hence, we chose a small value of 50 and a large value
of 200 that lies between 0 and 300.
the x-axis denotes all
Fig. 8 shows the average response times for the various
RAID schemes. In the ﬁgure,
the
evaluated schemes per workload, while the y-axis represents
the average response time in milliseconds. The value in the
parentheses after the name RAID-5 and eSAP denotes the
stripe size. For example, RAID-5(8) and eSAP(8) represent the
RAID-5 and eSAP schemes with a stripe size of 8. Here, note
that stripe sizes 16 and 32 are larger than the physical stripe
size of the experimental SSD conﬁguration, which has 8 ﬂash
memory chips. In case of stripe size 16, two physical stripes
comprise a logical stripe and, in case of 32, four physical
stripes comprise a logical stripe.
In Fig. 8, we observe that the ECC scheme, which does
not handle parity, has the best performance among the three
schemes. RAID-5, on the other hand, shows the worst per-
formance due to its heavy parity write overhead. Performance
of the eSAP scheme is worse than ECC, but is signiﬁcantly
better than RAID-5. Also, we see that the performance of eSAP
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
0510152025SequentialRandomFinancialExchangeMSNAvg. response time(ms) ECCRAID-5(8)RAID-5(16)RAID-5(32)eSAP(8)eSAP(16)eSAP(32)00.511.5SequentialRandom0510152025SequentialRandomFinancialExchangeMSNAvg. response time(ms) ECCRAID-5(8)RAID-5(16)RAID-5(32)eSAP(8)eSAP(16)eSAP(32)00.511.5SequentialRandom(a) Sequential
(b) Random
(c) Financial
(d) Exchange
(e) MSN
The numbers on top of each bar represent the accumulated total (in millions) of the components that comprise the bar (likewise for Fig. 10).
Fig. 9: Analysis of parity overhead with Pwait : 200ms.
(a) Sequential
(b) Random
(c) Financial
(d) Exchange
(e) MSN
Fig. 10: Analysis of parity overhead with Pwait : 50ms.
improves as the stripe size increases. For example, in Fig. 8,
observe that the average response time of eSAP decreases
considerably when the stripe size increases from 8 to 16 and
from 16 to 32. Consequently, the average response time of
eSAP when the stripe size is 32 is only slightly worse than
the scheme that uses ECC only. In constrast, increasing the
stripe size has only marginal effects on the response time of
RAID-5 because it consolidates requests only when they fall
into the same logical stripe.
The reason behind the difference in performance between
RAID-5 and eSAP can be seen from Table II. We note from
this table that the average write request size for eSAP is
considerably larger than that of RAID-5. This is because eSAP
dynamically constructs a stripe with all the requests that arrive
within Pwait. Also, as eSAP consolidates those requests and
counts them as a single request, it has a signiﬁcantly lower
number of write requests than RAID-5 as shown in Table II.
The conclusion is that making use of elastic striping and
anywhere parity can signiﬁcantly reduce parity overhead.
Now let us focus on the effects of Pwait. The results
presented in Fig. 8 show that there is only a marginal difference
in the response times between the two values of Pwait,
especially for the real workload traces. The main reason is
because, as shown in Fig. 7, the inter-arrival time of most
requests is less than 50 milliseconds for the real workload
traces. However, in case of the synthetic workloads, whose
inter-arrival times are evenly distributed between 0 and 200
milliseconds, we see some differences between the results for
the 50 millisecond and 200 millisecond Pwait values. This
is because with larger Pwait the chances for consolidating
requests increases. This leads to reduced parity overhead
resulting in enhanced performance.
The conclusion of the performance evaluation is that eSAP
performs in par with ECC, while providing RAID-5 reliability.
The source of the performance enhancement is in the workings
of the parity. Hence, we present a detailed analysis of activities
regarding parity in the next subsection.
C. Analysis of Parity Overhead
Figs. 9 and 10 show the various components involved
in managing the parity. In particular, for RAID-5, the parity
overhead consists of page reads needed for parity calculations
(denoted PR) and the parity writes for write requests (denoted
PW WR). In contrast, for eSAP, there is no need to read pages
during parity management. However, parity writes are invoked
during the cleaning process, which we denote as PW GC.
eSAP has an additional component in that multiple parities
may be written per stripe. Speciﬁcally, we distinguish the
original parity write required per stripe such as the one needed
in RAID-5, denoting them as PW WR, and the additional
partial parity writes that are needed only when a partial stripe
is written, denoting them PPW (Partial-stripe Parity Writes).
While each bar in Figs. 9 and 10 indicate the relative portions
of the various components comprising the bar, the values on
top of each bar represents the accumulated total of all such
components (in million units). For example, in Fig. 9(a) we see
that for RAID-5(8) roughly 45% of the 1.8 million, that is, 0.81
million parity operations are page reads for parity calculations
(denoted by PR), while the other 55%, that is, 0.99 million are
parity writes (denoted by PW WR).
There are several trends in the results. First, as the stripe
size of RAID-5 increases,
the PW WR portion becomes
smaller as the number of parity blocks becomes smaller relative
to the number of write requests. However, the PR portion
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:49:49 UTC from IEEE Xplore.  Restrictions apply. 
00.10.20.30.40.50.60.70.80.91Parity overhead (% of total) 1.8 2.1 3.4 0.8 0.4 0.2 00.10.20.30.40.50.60.70.80.9118 18 18 2.5 1.1 0.5 00.10.20.30.40.50.60.70.80.9115 15 15 3.4 1.7 0.9 00.10.20.30.40.50.60.70.80.9114 15 16 11 5.1 2.5 00.10.20.30.40.50.60.70.80.91PRPPWPW_GCPW_WR7.1 7.9 9.9 4.2 2.1 1.1 00.10.20.30.40.50.60.70.80.91Parity overhead (% of total) 13 13 13 4.9 4.3 4.0 00.10.20.30.40.50.60.70.80.9118 18 18 7.0 5.2 4.5 00.10.20.30.40.50.60.70.80.9117 17 17 4.7 2.7 1.8 00.10.20.30.40.50.60.70.80.9114 15 16 12 5.2 2.5 00.10.20.30.40.50.60.70.80.91PRPPWPW_GCPW_WR7.1 7.9 10 4.4 2.2 1.3 Fig. 11: Experimentally obtained (a) P/E cycles and (b) Total page writes showing that eSAP’s values are substantially smaller than those of RAID-5
(a) P/E cycles
(b) Total page writes
TABLE III: Accuracy(%) of model compared to the experimentally obtained numbers
(P/E and TPW denotes P/E cycles and the total number of page writes, respectively)
Workload
Sequential
Random
Financial
Exchange
MSN
ECC RAID-5(8) RAID-5(16) RAID-5(32)
Type
P/E
0.97
TPW 0.99
P/E
1.01
TPW 1.00
P/E
1.00
TPW 1.01
P/E
1.09
TPW 1.08
P/E
1.01
TPW 1.01
0.93
0.95
1.01
1.01
0.98
0.99
1.07
1.05
1.02
1.01
0.89
0.91
0.99
0.99
0.98
0.97
1.05
1.03
0.97
0.96
0.95
0.97
1.05
1.04
1.01
1.02
1.08
1.07
0.99
0.98
eSAP (8)
eSAP (16)
eSAP (32)
0.95
0.98
0.87
0.88
0.89
0.88
0.94
0.93
0.90
0.90
0.95
0.97
0.91
0.91
0.93
0.93
0.99
0.98
0.95
0.94
0.94
0.97
0.93
0.94
0.95
0.96
1.02
1.01
0.98
0.97
increases because more of the existing data has to be read to
perform the parity calculations. For the Random and Financial
workloads, PW WR and PR are affected less by the different
stripe sizes in RAID-5 as the request size is much smaller than
the other workloads.
Second, in the case of eSAP, as the stripe size is increased
a relatively smaller number of full stripe parities is written
for writing the same amount of data resulting in decreased
PW WR. With lesser PW WR, PW GC also naturally de-
creases. However, the PPW portion grows with the stripe size.
When Pwait is 50 milliseconds, we observe a large PPW
portion for the two synthetic workloads. This is because 75%
of the requests are regarded as separate requests incurring
partial parity writes. However, when Pwait is 200 milliseconds,
most requests are consolidated as a single request resulting in
minimal partial parity writes. We observe some partial parity
overhead in the real workload traces. This is because these
workloads include requests that have inter-arrival times larger
than 200 milliseconds. We also see that partial parity overhead
increases with the stripe size.
VI. RELIABILITY ANALYSIS
In this section, we validate our performance and reliability
model by comparing values obtained through our model with
those of the experiments. Then, we use our model to project
the long-term reliability of the various RAID schemes.
Fig. 11 shows the P/E cycles and the total number of
page writes for the various workloads and RAID schemes
obtained through the experiments. We observe that Fig. 11(a)
and Fig. 11(b) show similar trends. This is natural as the page
writes trigger cleaning operations, which, in turn, incur P/E
cycles. During these experiments, we collect data that represent
the characteristics of the workloads so that the data may be
applied to the model. Speciﬁcally, we measured the number of
write requests, average request size, and average utilization of
victim blocks selected for garbage collection. These numbers
are shown in Table II.
Applying the numbers in Table II to our model, we estimate
the P/E cycles and total number of page writes to process the