# Performance and Reliability Analysis of RAID Schemes

## A. Evaluation Environment
For our evaluation, we implemented the ECC (Error-Correcting Code), RAID-5, and eSAP (Elastic Striping with Anywhere Parity) schemes by modifying the DiskSim SSD extension [22]. The simulator parameters are presented in Table I. In all configurations, 5% of flash memory space is preserved as over-provisioned space. Each configuration consists of 8 flash memory chips, forming a physical stripe of eight pages, each from a different chip.

## B. Workloads and Characteristics
The first column of Table II lists the workloads and their characteristics, including the total request size and the write ratio used for the evaluation. The workloads include:
- **Sequential**: Synthetic workload accessing data sequentially.
- **Random**: Synthetic workload accessing data randomly with a uniform distribution.
- **Financial**: Collected from financial institutions, primarily random write-intensive from OLTP applications [1].
- **Exchange**: Represents the random I/O characteristics of an Exchange server serving 5000 corporate users. This workload is composed of 9 volumes collected over a 24-hour period, and we use the trace of volume number 2 [20].
- **MSN**: Collected from four RAID-10 volumes of the MSN storage back-end file store over a 6-hour duration, using the trace of volume 2 [20].

## C. Performance Results
### Pwait Values
For RAID-5 and eSAP, the parameter \( P_{\text{wait}} \) is needed. We report experimental results for two values of \( P_{\text{wait}} \): 50 and 200 milliseconds. These values were chosen based on the inter-arrival time of requests for our workloads, as shown in Fig. 7. Essentially, all inter-arrival times are 300 milliseconds or less, so we chose a small value of 50 and a large value of 200.

### Average Response Times
Fig. 8 shows the average response times for the various RAID schemes. The x-axis represents the evaluated schemes per workload, while the y-axis represents the average response time in milliseconds. The value in parentheses after RAID-5 and eSAP denotes the stripe size. For example, RAID-5(8) and eSAP(8) represent the RAID-5 and eSAP schemes with a stripe size of 8. Note that stripe sizes 16 and 32 are larger than the physical stripe size of the experimental SSD configuration, which has 8 flash memory chips. For a stripe size of 16, two physical stripes comprise a logical stripe, and for a stripe size of 32, four physical stripes comprise a logical stripe.

### Observations
- **ECC Scheme**: Does not handle parity and shows the best performance among the three schemes.
- **RAID-5**: Shows the worst performance due to heavy parity write overhead.
- **eSAP**: Performs worse than ECC but significantly better than RAID-5. Performance improves as the stripe size increases, reducing the average response time.

### Impact of Stripe Size
- **eSAP**: As the stripe size increases, the average response time decreases. For example, in Fig. 8, the average response time of eSAP decreases considerably when the stripe size increases from 8 to 16 and from 16 to 32. Consequently, the average response time of eSAP with a stripe size of 32 is only slightly worse than the scheme that uses ECC only.
- **RAID-5**: Increasing the stripe size has only marginal effects on the response time because it consolidates requests only when they fall into the same logical stripe.

### Effects of \( P_{\text{wait}} \)
- **Real Workloads**: There is only a marginal difference in response times between the two \( P_{\text{wait}} \) values, especially for real workload traces, as most inter-arrival times are less than 50 milliseconds.
- **Synthetic Workloads**: With larger \( P_{\text{wait}} \), the chances for consolidating requests increase, leading to reduced parity overhead and enhanced performance.

## D. Analysis of Parity Overhead
Figs. 9 and 10 show the various components involved in managing the parity. For RAID-5, the parity overhead consists of page reads needed for parity calculations (denoted PR) and parity writes for write requests (denoted PW_WR). For eSAP, there is no need to read pages during parity management, but parity writes are invoked during the cleaning process (denoted PW_GC). Additionally, eSAP may have multiple parities written per stripe, denoted as PPW (Partial-stripe Parity Writes).

### Trends in Results
- **RAID-5**: As the stripe size increases, the PW_WR portion becomes smaller, but the PR portion increases because more existing data must be read for parity calculations.
- **eSAP**: As the stripe size increases, fewer full stripe parities are written, resulting in decreased PW_WR and PW_GC. However, the PPW portion grows with the stripe size.

### Partial Parity Overhead
- **Pwait = 50 ms**: Large PPW portion for synthetic workloads, as 75% of requests are regarded as separate, incurring partial parity writes.
- **Pwait = 200 ms**: Most requests are consolidated, resulting in minimal partial parity writes. Real workload traces show some partial parity overhead due to inter-arrival times larger than 200 milliseconds.

## E. Reliability Analysis
In this section, we validate our performance and reliability model by comparing values obtained through our model with those from the experiments. We then use our model to project the long-term reliability of the various RAID schemes.

### Experimental Data
Fig. 11 shows the P/E cycles and the total number of page writes for the various workloads and RAID schemes. Figs. 11(a) and 11(b) show similar trends, as page writes trigger cleaning operations, which incur P/E cycles. During these experiments, we collected data representing the characteristics of the workloads, including the number of write requests, average request size, and average utilization of victim blocks selected for garbage collection (Table II).

### Model Validation
Applying the numbers in Table II to our model, we estimate the P/E cycles and total number of page writes to process the workloads. The accuracy of the model compared to the experimentally obtained numbers is shown in Table III.

## Conclusion
The performance evaluation shows that eSAP performs comparably to ECC while providing RAID-5 reliability. The source of the performance enhancement lies in the workings of the parity. Detailed analysis of parity overhead confirms that eSAP significantly reduces parity overhead, leading to improved performance.