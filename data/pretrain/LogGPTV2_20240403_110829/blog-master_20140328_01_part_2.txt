```  
vi $PGDATA/postgresql.conf  
unix_socket_directories = '.,/tmp'  
pg93@db-172-16-3-150-> postgresql restart -m fast  
waiting for server to shut down.... done  
server stopped  
server starting  
```  
重新执行pg_bulkload.  
```  
pg_bulkload -i /ssd3/pg93/test.dmp -O test -l /ssd3/pg93/test.log -o "TYPE=CSV" -o "WRITER=PARALLEL" -h $PGDATA -p $PGPORT -d $PGDATABASE  
```  
在执行过程中我们看到$PGDATA多了一个目录pg_bulkload, 存储加载数据的状态信息, 如果导入过程中发生异常, 使用postgresql脚本重启数据库时将自动修复. 或者在使用pg_ctl启动数据库前先使用pg_bulkload修复.  
```  
pg93@db-172-16-3-150-> cd $PGDATA  
pg93@db-172-16-3-150-> ll pg_bulkload/  
total 4.0K  
-rw------- 1 pg93 pg93 512 Mar 28 09:36 16384.34315.loadstatus  
```  
日志 :   
```  
[root@db-172-16-3-150 pg93]# cat test.log  
pg_bulkload 3.1.5 on 2014-03-28 13:32:31.32559+08  
INPUT = /ssd3/pg93/test.dmp  
PARSE_BADFILE = /ssd4/pg93/pg_root/pg_bulkload/20140328133231_digoal_public_test.prs.dmp  
LOGFILE = /ssd3/pg93/test.log  
LIMIT = INFINITE  
PARSE_ERRORS = 0  
CHECK_CONSTRAINTS = NO  
TYPE = CSV  
SKIP = 0  
DELIMITER = ,  
QUOTE = "\""  
ESCAPE = "\""  
NULL =   
OUTPUT = public.test  
MULTI_PROCESS = YES  
VERBOSE = NO  
WRITER = DIRECT  
DUPLICATE_BADFILE = /ssd4/pg93/pg_root/pg_bulkload/20140328133231_digoal_public_test.dup.csv  
DUPLICATE_ERRORS = 0  
ON_DUPLICATE_KEEP = NEW  
TRUNCATE = NO  
  0 Rows skipped.  
  50000000 Rows successfully loaded.  
  0 Rows not loaded due to parse errors.  
  0 Rows not loaded due to duplicate errors.  
  0 Rows replaced with new rows.  
Run began on 2014-03-28 13:32:31.32559+08  
Run ended on 2014-03-28 13:35:13.019018+08  
CPU 1.55s/128.55u sec elapsed 161.69 sec  
```  
使用pg_bulkload的direct 和 multi process模式(即parallel)导入数据总耗时161秒.    
相比普通的copy logged table 411秒快了一倍多.  
改为unlogged table, 使用pg_bulkload重新测试 :   
```  
digoal=# update pg_class set relpersistence ='u' where relname='test';  
UPDATE 1  
digoal=# update pg_class set relpersistence ='u' where relname='test_pkey';  
UPDATE 1  
digoal=# truncate test;  
TRUNCATE TABLE  
digoal=# checkpoint;  
CHECKPOINT  
$ pg_bulkload -i /ssd3/pg93/test.dmp -O test -l /ssd3/pg93/test.log -o "TYPE=CSV" -o "WRITER=PARALLEL" -h $PGDATA -p $PGPORT -d $PGDATABASE  
pg_bulkload 3.1.5 on 2014-03-28 13:36:15.602787+08  
INPUT = /ssd3/pg93/test.dmp  
PARSE_BADFILE = /ssd4/pg93/pg_root/pg_bulkload/20140328133615_digoal_public_test.prs.dmp  
LOGFILE = /ssd3/pg93/test.log  
LIMIT = INFINITE  
PARSE_ERRORS = 0  
CHECK_CONSTRAINTS = NO  
TYPE = CSV  
SKIP = 0  
DELIMITER = ,  
QUOTE = "\""  
ESCAPE = "\""  
NULL =   
OUTPUT = public.test  
MULTI_PROCESS = YES  
VERBOSE = NO  
WRITER = DIRECT  
DUPLICATE_BADFILE = /ssd4/pg93/pg_root/pg_bulkload/20140328133615_digoal_public_test.dup.csv  
DUPLICATE_ERRORS = 0  
ON_DUPLICATE_KEEP = NEW  
TRUNCATE = NO  
  0 Rows skipped.  
  50000000 Rows successfully loaded.  
  0 Rows not loaded due to parse errors.  
  0 Rows not loaded due to duplicate errors.  
  0 Rows replaced with new rows.  
Run began on 2014-03-28 13:36:15.602787+08  
Run ended on 2014-03-28 13:38:57.506558+08  
CPU 2.26s/129.23u sec elapsed 161.90 sec  
```  
导入数据总耗时161秒.  相比普通的copy unlogged table 363秒快了一倍多.  
因为已经绕过了shared buffer, 所以使用pg_bulkload导入目标unlogged和logged表的结果一样.  
最后附direct模式的测试结果, (不开multi process). 256秒, 还是比363快.  
```  
pg93@db-172-16-3-150-> pg_bulkload -i /ssd3/pg93/test.dmp -O test -l /ssd3/pg93/test.log -o "TYPE=CSV" -o "WRITER=DIRECT" -h $PGDATA -p $PGPORT -d $PGDATABASE  
pg_bulkload 3.1.5 on 2014-03-28 13:41:10.934578+08  
INPUT = /ssd3/pg93/test.dmp  
PARSE_BADFILE = /ssd4/pg93/pg_root/pg_bulkload/20140328134110_digoal_public_test.prs.dmp  
LOGFILE = /ssd3/pg93/test.log  
LIMIT = INFINITE  
PARSE_ERRORS = 0  
CHECK_CONSTRAINTS = NO  
TYPE = CSV  
SKIP = 0  
DELIMITER = ,  
QUOTE = "\""  
ESCAPE = "\""  
NULL =   
OUTPUT = public.test  
MULTI_PROCESS = NO  
VERBOSE = NO  
WRITER = DIRECT  
DUPLICATE_BADFILE = /ssd4/pg93/pg_root/pg_bulkload/20140328134110_digoal_public_test.dup.csv  
DUPLICATE_ERRORS = 0  
ON_DUPLICATE_KEEP = NEW  
TRUNCATE = NO  
  0 Rows skipped.  
  49999998 Rows successfully loaded.  
  0 Rows not loaded due to parse errors.  
  0 Rows not loaded due to duplicate errors.  
  0 Rows replaced with new rows.  
Run began on 2014-03-28 13:41:10.934578+08  
Run ended on 2014-03-28 13:45:27.007941+08  
CPU 10.68s/243.64u sec elapsed 256.07 sec  
```  
## 参考  
1\. http://blog.163.com/digoal@126/blog/static/163877040201392641033482  
2\. http://pgbulkload.projects.pgfoundry.org/pg_bulkload.html  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")