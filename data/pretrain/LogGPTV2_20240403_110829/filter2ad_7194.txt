title:Scaling Bandwidth Estimation to High Speed Networks
author:Qianwen Yin and
Jasleen Kaur and
F. Donelson Smith
Scaling Bandwidth Estimation
to High Speed Networks
Qianwen Yin, Jasleen Kaur, and F. Donelson Smith
University of North Carolina at Chapel Hill
Abstract. Existing bandwidth estimation tools fail to perform well at gi-
gabit and higher network speeds. In this paper we study several sources of
noise that must be overcome by these tools in high-speed envrionments and
propose strategies for addressing them. We evaluate our Linux implemen-
tation on 1 and 10Gbps testbed networks, showing that our strategies help
signiﬁcantly in scaling bandwidth estimation to high-speed networks.
1
Introduction
networks,
namely,
interrupt
Bandwidth estimation tools perform well on 100Mbps networks [1–3], but they
fail to do so at gigabit and higher speeds. This is because very small inter-packet
gaps(less than 12 microseconds) are needed for probing for higher bandwidth–
they are more susceptible to being disturbed by small-scale buﬀering at shared
resources. In this paper, we study the impact of buﬀering-related noise on high-
speed
and
small-scale burstiness in cross traﬃc. We
then propose strategies to address them.1
We evaluate our strategies using a Linux
implementation in a lab testbed with 1 and
10 Gbps links. We ﬁnd that our new mech-
anisms help signiﬁcantly in scaling band-
width estimation to high-speed networks.
receiver-side
coalescence
2 Experimental Methodology
Fig. 1. Lab Testbed Conﬁguration
Laboratory Testbed. We use the dedicated network illustrated in Fig 1. The
switch-to-switch path in the core of the topology can be either 1Gbps or 10Gbps.
We focus on the latter here. High-end hosts are used to estimate avail-bw
with 10Gbps Ethernet adapters. The network includes additional 12 pairs of
hosts to generate cross traﬃc sharing the switch-to-switch link. Endace DAG
monitoring NICs are attached to the ﬁber links between two switches, pro-
viding line-rate capture of all frames with nanosecond precision timestamps.
1 We use the probing framework used by PathChirp[4] for the experimental study.
M. Faloutsos and A. Kuzmanovic (Eds.): PAM 2014, LNCS 8362, pp. 258–261, 2014.
c(cid:2) Springer International Publishing Switzerland 2014
Scaling Bandwidth Estimation to High Speed Networks
259
With a locally-modiﬁed SURGE program,
we generate average 4Gbps cross traﬃc on
the 10Gbps link simulating synthetic and
highly dynamic web traﬃc. A complete trace
was obtained from DAG to compute ground
truth avail-bw.
Probe Stream Structure. We use the sim-
ilar probe stream structure as pathChirp.
Each probe stream probes for 10 rates. Each
rate is 20% higher than the previous one.
[1, 5] show that using mutliple packets per
rate for PathChirp leads to more robust esti-
mation. Thus by default, we send 16 packets
at each rate.
1
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
e
c
a
r
i
p
m
E
l
0.8
0.6
0.4
0.2
0
−1
IC=2 micro−second
IC=50 micro−second
IC=100 micro−second
IC=200 micro−second
1
1.5
2
Relative AB Estimation Error
0.5
−0.5
0
Fig. 2. AB Estimation Error
Accurate Send-gap Creation. To ensure that inter-packet gaps are created
accurately, we design a Qdisc scheduler as a kernel module for creating send-gaps
with errors smaller than 1 microsecond. The scheduler sits between the bottom
of IP and the NIC device driver. Inter-packet gaps are precisely enforced by
inserting appropriately sized PAUSE frames which will be discarded by the ﬁrst
inbound switch.
Receiver-side Timestamping. Existing tools timestamp packets for measur-
ing receiver gaps at the application layer. To record software timestamps with
the best-possible accuracy, we implement a kernel module attached as an ingress
Qdisc to the adapter sitting between the device driver and the bottom of IP.
3
Interrupt Coalescence at Receivers
80
60
40
i
(
p
a
G
t
100
)
d
n
o
c
e
s
−
o
r
c
m
send gap
recv gap
actual_ab gap
The Issue We ﬁrst study how much eﬀect
the receiver latencies have on the software
timestamps by comparing the receiver-
logged gaps with the corresponders com-
puted from DAG trace taken between the
switches (these gaps are evaluated within
1 micro-second diﬀerence from those ob-
tained via hardware timestamps at the re-
ceiver NIC). For interrupt coalesce, we use
interrupt latency(IC) as default, 2, 50, 100,
and 200 microseconds respectively. We ﬁnd
that:(i) in all cases except 2-microsecond IC, the receiver-logged gaps follow a
bimodal distribution, with one peak at “infeasibly-small” and the other peak
close to IC value; and (ii)overestimation occurs to over 90% probe streams and
the relative estimation error can be up to 160%(Fig 2)!
Fig. 3. Sample Stream: Spikes, Dips
Probe Packet Index
100 120 140
e
k
c
a
p
−
r
e
n
t
20
20
40
60
80
0
I
260
Q. Yin, J. Kaur, and F. Donelson Smith
Solutions: Spike Removal, Exponen-
tial Smoothing, Probe Time Scale
Fig 3 illustrates a typical probe stream
with default interrupt coalesce setting,
where alternate spikes-and-dips pattern
completely dominates the structure. Ap-
plying pathChirp algorithm in this stream
results in 19Gbps avail-bw –the maximum
probing rate used by this stream. This is
because the receive gaps are never consis-
tently larger than the send gaps for any
lower probing rate.
)
d
n
o
c
e
s
−
o
r
c
m
(
p
a
G
i
t
e
k
c
a
p
−
r
e
t
n
I
20
15
10
5
0
send gap(spike)
recv gap(spike)
send gap(spike+exp)
recv gap(spike+exp)
actual_ab gap
20
40
60
80
100 120 140
Probe Packet Index
Fig. 4. Spike Removal & Smoothing
1
n
o
i
t
l
i
l
0.8
0.6
0.4
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
e
c
a
r
i
p
m
E
The spikes-and-dips pattern can be ex-
plained well. For eﬃciency, arriving packets
are held in the NIC buﬀers and processed
in a batch after several have arrived–the
ﬁrst in a batch would experience a large
preceding gap, whereas all subsequent ob-
serve fairly small gaps. While such batch-
ing does destroy the actual receive gaps,
do the average inter-packet gaps observed
within a batch somewhat preserve the in-
tended probe-stream structure? To under-
stand this, we identify the start and end for each buﬀered batch, and replace
all observations in the interval with that mean gap observed in that interval.
Fig 4 shows the result of applying this process to the same probe stream—we
ﬁnd that such a buﬀering-aware averaging mechanism indeed preserves lots of
information about the intended probe-stream.
IC=2 micro−second
IC=50 micro−second
IC=100 micro−second
IC=200 micro−second
IC=default
−0.2
0.2
Relative AB Estimation Error
Fig. 5. Impact of IC Conﬁguration
−0.4
0.4
0.2
0
0
For robustness, we further use exponential smoothing across all observations,
and then feed the smoothed gaps to the bandwidth estimation logic. The example
probe stream accurately yields avail-bw of 10 Gbps on doing so. Fig 5 shows
that reducing the noise in the measured receive gaps produces more accurate
estimates for all interrupt delays.
not
does
smoothing
Notice also from Fig 4 that spike-
aware
preserve
probing granularity within a spike. Larger
probing timescales(i.e. the number of pack-
ets probed at each rate) helps maintain
higher granularity and yield more robust
estimation, which is shown in Fig 6 by in-
creasing probing timescale from 16 to 32,
64 and 128 packets/rate.
1
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
e
c
a
r
i
p
m
E
l
0.8
0.6
0.4
0.2
0
−0.4
pkts/rate=16
pkts/rate=32
pkts/rate=64
pkts/rate=128
−0.2
0.2
Relative AB Estimation Error
0
0.4
Fig. 6. Impact of Probing Timescale
Scaling Bandwidth Estimation to High Speed Networks
261
4 Cross Traﬃc Burstiness
We then repeat the experiments in Section 3, but this time with the bursty
cross-traﬃc described in Section 2 sharing the 10 Gbps bottleneck link. The
relative estimation error without our mechanisim ranges from 150% to 350%.
In contrast, Fig 7 shows the much improved estimation with spike removal and
exponential smoothing.
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
i
l
1
0.8
0.6
0.4
0
5 Conclusion
l
0.2
a
u
m
u
C
e
c
a
r
i
p
m
E
In this paper, we identify the noise caused
by bursty cross traﬃc and buﬀering laten-
cies at receiver side that must be over-
come by bandwidth estimation tools on
high-speed links. In our controlled testbed
we demonstrate that current tools fail to
scale to 1Gbps networks. We then present
techniques to address these issues: spike re-
moval, exponential smoothing and increasing probing timescale. We evaluate our
Linux implementation in a 10Gbps testbed using highly-variable cross traﬃc,
showing that our techniques signiﬁcantly help in scaling bandwidth estimation
to high-speed networks. As a future work, we will improve our spike-removal
algorithm to deal with all types of batching-related noise. And we plan to con-
duct intensive experiments over multi-hop high-speed networks and on wide-area
100G ESnet testbed.
Fig. 7. Impact of Bursty Cross Traﬃc
Relative AB Estimation Error
pkts/rate=64
pkts/rate=128
0
−0.4
−0.2
0.2
0.4
References
1. Shriram, et al.: Empirical evaluations of techniques for measuring available band-
width. In: IEEE INFOCOM 2007 (2007)
2. Shriram, A., Murray, M., Hyun, Y., Brownlee, N., Broido, A., Fomenkov, M., Claﬀy,
K.: Comparison of public end-to-end bandwidth estimation tools on high-speed
links. In: Dovrolis, C. (ed.) PAM 2005. LNCS, vol. 3431, pp. 306–320. Springer,
Heidelberg (2005)
3. Strauss, et al.: A measurement study of available bandwidth estimation tools. In:
ACM SIGCOMM on Internet measurement 2003 (2003)
4. Ribeiro, et al.: pathchirp: Eﬃcient available bandwidth estimation for network
paths. In: PAM 2003 (2003)
5. Kang, Loguinov: Characterizing tight-link bandwidth of multi-hop paths using prob-
ing response curves. In: IWQoS (2010)