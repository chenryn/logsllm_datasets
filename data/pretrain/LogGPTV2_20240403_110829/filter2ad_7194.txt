# Scaling Bandwidth Estimation to High-Speed Networks

**Authors:** Qianwen Yin, Jasleen Kaur, and F. Donelson Smith  
**Affiliation:** University of North Carolina at Chapel Hill

## Abstract
Existing bandwidth estimation tools perform well on 100 Mbps networks but fail at gigabit and higher speeds. This paper examines the sources of noise that these tools must overcome in high-speed environments and proposes strategies to address them. We evaluate our Linux implementation on 1 and 10 Gbps testbed networks, demonstrating that our strategies significantly improve the scalability of bandwidth estimation for high-speed networks.

## 1. Introduction
Bandwidth estimation tools, such as those described in [1–3], perform well on 100 Mbps networks but struggle at gigabit and higher speeds. This is due to the need for very small inter-packet gaps (less than 12 microseconds) to probe for higher bandwidth, which are more susceptible to disturbances from small-scale buffering at shared resources. In this paper, we investigate the impact of buffering-related noise and small-scale burstiness in cross-traffic on high-speed networks and propose strategies to mitigate these issues. We evaluate our strategies using a Linux implementation in a lab testbed with 1 and 10 Gbps links, finding that our new mechanisms significantly enhance the scalability of bandwidth estimation for high-speed networks.

## 2. Experimental Methodology

### 2.1 Laboratory Testbed
We use a dedicated network setup as shown in Figure 1. The switch-to-switch path in the core of the topology can be either 1 Gbps or 10 Gbps; we focus on the latter. High-end hosts with 10 Gbps Ethernet adapters are used to estimate available bandwidth (avail-bw). The network includes 12 additional pairs of hosts to generate cross-traffic sharing the switch-to-switch link. Endace DAG monitoring NICs are attached to the fiber links between the two switches, providing line-rate capture of all frames with nanosecond precision timestamps.

![Figure 1: Lab Testbed Configuration](fig1.png)

Using a locally-modified SURGE program, we generate an average of 4 Gbps cross-traffic on the 10 Gbps link, simulating synthetic and highly dynamic web traffic. A complete trace was obtained from the DAG to compute the ground truth avail-bw.

### 2.2 Probe Stream Structure
We use a similar probe stream structure as PathChirp. Each probe stream probes for 10 rates, each 20% higher than the previous one. [1, 5] show that using multiple packets per rate for PathChirp leads to more robust estimation. Thus, by default, we send 16 packets at each rate.

## 3. Interrupt Coalescence at Receivers

### 3.1 The Issue
We first study the effect of receiver latencies on software timestamps by comparing receiver-logged gaps with corresponding gaps computed from DAG traces taken between the switches. For interrupt coalescence, we use interrupt latency (IC) values of 2, 50, 100, and 200 microseconds, respectively. We find that:
- In all cases except for 2-microsecond IC, the receiver-logged gaps follow a bimodal distribution, with one peak at "infeasibly-small" and the other peak close to the IC value.
- Overestimation occurs in over 90% of probe streams, with relative estimation errors up to 160% (Figure 2).

![Figure 2: AB Estimation Error](fig2.png)

### 3.2 Solutions: Spike Removal, Exponential Smoothing, and Probing Timescale
Figure 3 illustrates a typical probe stream with the default interrupt coalescence setting, where an alternate spikes-and-dips pattern completely dominates the structure. Applying the PathChirp algorithm to this stream results in an estimated avail-bw of 19 Gbps, which is the maximum probing rate used by this stream. This is because the receive gaps are never consistently larger than the send gaps for any lower probing rate.

![Figure 3: Sample Stream: Spikes, Dips](fig3.png)

The spikes-and-dips pattern can be explained by the efficiency of holding arriving packets in the NIC buffers and processing them in batches. The first packet in a batch experiences a large preceding gap, while subsequent packets observe smaller gaps. To preserve the intended probe-stream structure, we identify the start and end of each buffered batch and replace all observations in the interval with the mean gap observed in that interval. Figure 4 shows the result of applying this process to the same probe stream, indicating that such a buffering-aware averaging mechanism preserves much information about the intended probe-stream.

![Figure 4: Spike Removal & Smoothing](fig4.png)

For robustness, we further apply exponential smoothing across all observations and then feed the smoothed gaps to the bandwidth estimation logic. The example probe stream accurately yields an avail-bw of 10 Gbps. Figure 5 shows that reducing the noise in the measured receive gaps produces more accurate estimates for all interrupt delays.

![Figure 5: Impact of IC Configuration](fig5.png)

Larger probing timescales (i.e., the number of packets probed at each rate) help maintain higher granularity and yield more robust estimation. Figure 6 shows the impact of increasing the probing timescale from 16 to 32, 64, and 128 packets/rate.

![Figure 6: Impact of Probing Timescale](fig6.png)

## 4. Cross-Traffic Burstiness
We repeat the experiments from Section 3, but this time with bursty cross-traffic sharing the 10 Gbps bottleneck link. Without our mechanisms, the relative estimation error ranges from 150% to 350%. In contrast, Figure 7 shows the significantly improved estimation with spike removal and exponential smoothing.

![Figure 7: Impact of Bursty Cross Traffic](fig7.png)

## 5. Conclusion
In this paper, we identify the noise caused by bursty cross-traffic and buffering latencies at the receiver side, which must be overcome by bandwidth estimation tools on high-speed links. In our controlled testbed, we demonstrate that current tools fail to scale to 1 Gbps networks. We present techniques to address these issues: spike removal, exponential smoothing, and increasing the probing timescale. We evaluate our Linux implementation in a 10 Gbps testbed using highly variable cross-traffic, showing that our techniques significantly help in scaling bandwidth estimation to high-speed networks. Future work will focus on improving our spike-removal algorithm to handle all types of batching-related noise and conducting extensive experiments over multi-hop high-speed networks and on wide-area 100 G ESnet testbeds.

## References
1. Shriram, et al.: Empirical evaluations of techniques for measuring available bandwidth. In: IEEE INFOCOM 2007 (2007)
2. Shriram, A., Murray, M., Hyun, Y., Brownlee, N., Broido, A., Fomenkov, M., Claffy, K.: Comparison of public end-to-end bandwidth estimation tools on high-speed links. In: Dovrolis, C. (ed.) PAM 2005. LNCS, vol. 3431, pp. 306–320. Springer, Heidelberg (2005)
3. Strauss, et al.: A measurement study of available bandwidth estimation tools. In: ACM SIGCOMM on Internet measurement 2003 (2003)
4. Ribeiro, et al.: PathChirp: Efficient available bandwidth estimation for network paths. In: PAM 2003 (2003)
5. Kang, Loguinov: Characterizing tight-link bandwidth of multi-hop paths using probing response curves. In: IWQoS (2010)