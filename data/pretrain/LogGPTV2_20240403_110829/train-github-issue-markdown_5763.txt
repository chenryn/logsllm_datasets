# Checklist
  * I have verified that the issue exists against the `master` branch of Celery.
  * This has already been asked to the discussion group first.
  * I have read the relevant section in the  
contribution guide  
on reporting bugs.
  * I have checked the issues list  
for similar or identical bug reports.
  * I have checked the pull requests list  
for existing proposed fixes.
  * I have checked the commit log  
to find out if the bug was already fixed in the master branch.
  * I have included all related issues and possible duplicate issues  
in this issue (If there are none, check this box anyway).
## Mandatory Debugging Information
  * I have included the output of `celery -A proj report` in the issue.  
(if you are not able to do this, then at least specify the Celery  
version affected).
  * I have verified that the issue exists against the `master` branch of Celery.
  * I have included the contents of `pip freeze` in the issue.
  * I have included all the versions of all the external dependencies required  
to reproduce this bug.
## Optional Debugging Information
  * I have tried reproducing the issue on more than one Python version  
and/or implementation.
  * I have tried reproducing the issue on more than one message broker and/or  
result backend.
  * I have tried reproducing the issue on more than one version of the message  
broker and/or result backend.
  * I have tried reproducing the issue on more than one operating system.
  * I have tried reproducing the issue on more than one workers pool.
  * I have tried reproducing the issue with autoscaling, retries,  
ETA/Countdown & rate limits disabled.
  * I have tried reproducing the issue after downgrading  
and/or upgrading Celery and its dependencies.
## Related Issues and Possible Duplicates
#### Related Issues
  * #6009
  * #5230
  * #4809
  * #6023
#### Possible Duplicates
  * None
## Environment & Settings
**Celery version** :
**`celery report` Output:**
    # With broker rabbitmq
    software -> celery:4.4.2 (cliffs) kombu:4.6.8 py:3.8.0
                billiard:3.6.3.0 py-amqp:2.5.2
    platform -> system:Darwin arch:64bit
                kernel version:19.4.0 imp:CPython
    loader   -> celery.loaders.app.AppLoader
    settings -> transport:pyamqp results:disabled
    broker_url: 'amqp://guest:********@localhost:5672//'
    # With broker redis
    software -> celery:4.4.2 (cliffs) kombu:4.6.8 py:3.8.0
                billiard:3.6.3.0 redis:3.4.1
    platform -> system:Darwin arch:64bit
                kernel version:19.4.0 imp:CPython
    loader   -> celery.loaders.app.AppLoader
    settings -> transport:redis results:disabled
    broker_url: 'redis://localhost:6379/0'
# Steps to Reproduce
## Required Dependencies
  * **Minimal Python Version** : N/A or Unknown
  * **Minimal Celery Version** : N/A or Unknown
  * **Minimal Kombu Version** : N/A or Unknown
  * **Minimal Broker Version** : N/A or Unknown
  * **Minimal Result Backend Version** : N/A or Unknown
  * **Minimal OS and/or Kernel Version** : N/A or Unknown
  * **Minimal Broker Client Version** : N/A or Unknown
  * **Minimal Result Backend Client Version** : N/A or Unknown
### Python Packages
**`pip freeze` Output:**
    amqp==2.5.2
    billiard==3.6.3.0
    celery==4.4.2
    kombu==4.6.8
    pytz==2019.3
    redis==3.4.1
    vine==1.3.0
### Other Dependencies
N/A
## Minimally Reproducible Test Case
    from celery import Celery
    app = Celery('tasks', broker='pyamqp://guest:guest@localhost:5672') # for rabitmq broker
    # app = Celery('tasks', broker='redis://localhost:6379/0') for refis broker
    @app.task
    def task_example():
        s = 'a' * 256 * (10**6) # Create a string of 256MB
        raise
# Expected Behavior
I will describe all the scenarios that I tested.
  * Scenario 1: I ran the worker with the command `celery worker -A tasks -lINFO` and after this I executed the task with the command `python -c "from tasks import task_example; task_example.delay()"`.
  * Scenario 2: I ran the worker with the command `celery worker -A tasks -lINFO -P threads` and after this I executed the task with the command `python -c "from tasks import task_example; task_example.delay()"`.
  * Scenario 3: I ran the worker with the command `celery worker -A tasks -lINFO --max-memory-per-child 125000` and after this I executed the task with the command `python -c "from tasks import task_example; task_example.delay()"`.
  * Scenario 4: I ran the worker with the command `celery worker -A tasks -lINFO -P threads --max-memory-per-child 125000` and after this I executed the task with the command `python -c "from tasks import task_example; task_example.delay()"`.
In all of this scenarios I expect that after run the task, celery will not
continue consume the memory.
# Actual Behavior
Only in scenario 3, as the documentation explains, the celery not continue
consume memory because celery replaced the child process.
On scenarios 1, 2, and 4 the celery keep the memory that the task
allocate(256MB for each task ran in this case).