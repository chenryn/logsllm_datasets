. . .
0
2/q
.
(33)
APPENDIX C
DETAILED CONVERGENCE RESULTS
Convergence for multimodal geometric systems, when vary-
ing ν and for ﬁxed |S| × |O| = 100 × 10K.
System
Multimodal
ν = 1.0
Multimodal
ν = 0.1
Multimodal
ν = 0.01
δ
0.1
0.05
0.01
0.005
0.1
0.05
0.01
0.005
0.1
0.05
Freq.
3 008
5 938
26 634
52 081
24 453
44 715
149 244
226 947
27 489
103 374
kn-NN
NN
369
495
765
765
398
568
4 842
79 712
log10
478
754
1 166
1 166
554
754
1 166
1 166
753
101 664
900
92 181
log
897
1 267
1 487
1 487
821
1 175
1 487
1 487
381
31 452
Detailed convergence results for a Spiky system of size
|S| = 2 and |O| = 10K.
δ
0.1
0.05
0.01
0.005
Freq.
15 953
22 908
38 119
44 853
NN
22 801
29 863
44 841
51 683
kn-NN
log10
52 515
62 325
81 925
91 661
log
99 325
112 467
137 969
147 593
APPENDIX D
UNIFORM SYSTEM
We measured convergence of the methods for a uniform
system; this system is constructed so that all secret-object
(cid:4)
examples are equally likely, that is μ(s, o) = μ(s
) for all
, o
s, o ∈ S × O. The Bayes risk in this case is R
= 1 − 1/|S|.
Figure 11 shows that even in this case all rules are equiv-
its
alent. Indeed, because the system leaks nothing about
∗
(cid:4)
Fig. 11. Convergence for a Uniform system of size 100 × 100.
Fig. 12. Approximation of the frequentist estimate as n grows for R∗ ≈ 0.08,
|O| = 10K, and |S| = 1K; the approximation is compared with the real
frequentist estimate RF req
n
.
secrets, all the estimators need to randomly guess; but because
for this system the Bayes risk is identical to random guessing
= 1 − 1/|S| = Rπ), all the estimators converge
error (R
immediately to its true value.
∗
APPROXIMATION OF THE FREQUENTIST ESTIMATE
APPENDIX E
To better understand the behavior of the frequentist ap-
proach for observations that were not in the training data, we
derive a crude approximation of this estimate in terms of the
size of training data n. The approximation makes the following
assumptions:
1) each observation o ∈ O is equally likely to appear in
training data (i.e., P (o) = 1 − 1|O|);
2) if an observation appears in the training data,
the
frequentist approach outputs the secret minimizing the
Bayes risk;
3) the frequentist estimate knows the real priors π;
4) if an observation does not appear in the training data,
then the frequentist approach outputs the secret with the
maximum prior probability.
The ﬁrst two assumptions are very strong, and thus this is just
an approximation of the real trend of such estimate. However,
in practice it approximates well the real trend Figure 12.
Let An(o) denote the event “observation o appears in
a training set of n examples”; because of assumption 1),
(cid:25)(cid:22)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
1 − 1|O|
P (An(o)) = 1 −
estimated with a frequentist approach given n examples is:
rn(o) =rn(o|An(o))P (An(o)) + rn(o|¬An(o))P (¬An(o)) =
. The conditional Bayes risk
(cid:11)
(cid:9)
(cid:10)n
(cid:13)
=
≈
1 − max
s∈S
(cid:7)
+ (1 − max
s∈S
1 − max
s∈S
+ (1 − max
s∈S
ˆCs,o ˆπ(s)
P (o)
(cid:8)
P (An(o))+
ˆπ(s))P (¬An(o)) ≈
Cs,oπ(s)
π(s))P (¬An(o))
P (An(o))+
P (o)
Assumptions 2) and 3) were used in the last step. From this
expression, we derive the frequentist estimate of R
t step n:
∗
(cid:7)
RFreq
n = Ern =
=
P (o)
(cid:2)
(cid:2)
o∈O
+
o∈O
(cid:8)
Cs,oπ(s)
1 − max
s∈S
P (o)(1 − max
⎛
s∈S
⎝(cid:2)
⎛
⎝1 −
(cid:2)
o∈O
⎞
⎠ +
P (o)
P (An(o))+
π(s))P (¬An(o)) =
(cid:2)
max
s∈S
o∈O
π(s))
Cs,oπ(s))
(cid:2)
⎞
⎠ +
P (o) =
o∈O
Cs,oπ(s))
= P (An(o))
P (o) −
+ P (¬An(o))(1 − max
s∈S
= P (An(o))
o∈O
max
s∈S
+ P (¬An(o))(1 − max
s∈S
(cid:8)n(cid:8)
(cid:7)
(cid:7)
(cid:7)
+ P (¬An(o))Rπ =
∗
1 − 1|O|
+ Rπ
π(s)) =
1 −
∗
= P (An(o))R
= R
(cid:8)n
.
1 − 1|O|
Note that in the second step we used P (An(o)) as a constant,
which is allowed by assumption 1).
The expression of Rn indicates that P (An(o)) weights
between random guessing according to priors-based random
guessing and the Bayes risk; when P (An(o)) ≥ P (¬An(o)),
which happens for n ≥ − log 2
the frequentist approach
starts approximating using the actual Bayes risk (Figure 12).
log(1− 1|O| )
APPENDIX F
GO W A L L A DETAILS
We report
in Table XIV the real Bayes risk estimated
analytically for the Gowalla dataset defended using the
various mechanisms, and their respective utility.
APPENDIX G
APPLICATION TO TIME SIDE CHANNEL
We use F-BLEAU to measure the leakage in the running
time of the square-and-multiply exponentiation algorithm in
the ﬁnite ﬁeld F2w; exponentiation in F2w is relevant, for
example, for the implementation of the ElGamal cryptosystem.
TRUE BAYES RISK AND UTILITY FOR GO W A L L A DATASET DEFENDED
USING VARIOUS LOCATION PRIVACY MECHANISMS.
TABLE XIV
Mechanism
Blahut-Arimoto
Geometric
Laplacian
ν
2
4
8
2
4
8
2
4
8
R∗
0.760
0.571
0.428
0.657
0.456
0.308
0.657
0.456
0.308
Utility
334.611
160.839
96.2724
288.372
144.233
96.0195
288.66
144.232
96.212
NUMBER OF UNIQUE SECRETS AND OBSERVATIONS FOR THE TIME SIDE
CHANNEL TO FINITE FIELD EXPONENTIATION.
TABLE XV
Operands’ size
4 bits
6 bits
8 bits
10 bits
12 bits
|S|
24
26
28
210
212
|O|
34
123
233
371
541
We consider a hardware-equivalent implementation of the
algorithm computing ms in F2w. We focus our analysis on
the simpliﬁed scenario of a “one-observation” adversary, who
makes exactly one measurement of the algorithm’s execution
time o, and aims to predict the corresponding secret key s.
A similar analysis was done by Backes and Köpf [35] by
using a leakage estimation method based on the frequentist ap-
proach. Their analysis also extended to a “many-observations
adversary”, that is, an adversary who can make m observations
(o1, ..., om), all generated from the same secret s, and has to
predict s accordingly.
A. Side channel description
Square-and-multiply is a fast algorithm for computing ms
in the ﬁnite ﬁeld F2w, where w here represents the bit size
of the operands m and s. It works by performing a series
of multiplications according to the binary representation of
the exponent s, and its running time is proportional to the
number of 1’s in s. This fact was noticed by Kocher [36],
who suggested side channel attacks to the RSA cryptosystem
based on time measurements.
B. Message blinding
We assume the system implements message blinding, a
technique which hides to an adversary the value m for which
ms is computed. Blinding was suggested as a method for
thwarting time side channels [36], which works as follows.
Consider, for instance, decryption for the RSA cryptosystem:
md(modN ), for some decryption key d;
the system ﬁrst
computes m · re, where e is the encryption key and r is
(cid:25)(cid:22)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 13. Convergence of the estimates for the time side channel attack to the exponentiation algorithm as the bit size of the operands increases.
some random value; then it computes (mre)d, and returns the
decrypted message after dividing the result by r.
Message blinding has the advantage of hiding information
to an adversary; however, it was shown that it is not enough
for preventing time side channels (e.g., [35]).
C. Implementation and results
We consider a Gezel implementation of ﬁnite ﬁeld expo-
nentiation. Gezel is a description language for clocked hard-
ware, equipped with a simulation environment whose execu-
tions preserve the corresponding circuit’s timing information.
This means that the time measurements (i.e., clock cycles) we
make reﬂect the corresponding circuit implementation [37].
We compare the performances of the frequentist and nearest
neighbor approaches in terms of the number of black-box
examples required for convergence. For each bit size w ∈
{4, 6, .., 12}, and for all the values (mi, si) ∈ {0, ..., 2w−1}2,
we run the exponentiation algorithm to compute ms, and
measure its execution time oi. As with our application to
location privacy (section VII), we estimate the Bayes risk by
training a classiﬁer on a set of increasing examples n and by
computing its error on a hold-out set. We set the size of the
hold-out set to min(0.2 · 22w, 250 000).
Results in Figure 13 show that, while for small bit sizes the
frequentist approach outperforms nearest neighbor rules, as
w increases, the frequentist approach requires a much larger
number of examples. Nevertheless, in these experiments we
did not notice a substantial advantage in nearest neighbor
rules, even though the output space is equipped with a notion
of metric. Table XV helps interpreting this result: for larger
bit sizes w of the exponentiation operands,
the possible
output values (i.e., clock cycles) only increase minimally; this
conﬁrms that, as noticed in our previous experiments, nearest
neighbor and frequentist estimates tend to perform similarly
for systems with small output space.
(cid:25)(cid:22)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:49:33 UTC from IEEE Xplore.  Restrictions apply.