Sa Su
Water 30 130 28 31 33 51 48 44 30 37 516 31 58 54
Bread 10
14 12 11
Milk
8
12 21
6
Table 1: Daily sales per product from a small shop
10 14 15 13 10 18
14 10 10 12
12
10
F
12
7
6
19
12 11 22
8
first Tuesday, customer A buys 100 water bottles, e.g., because he
is throwing a large party. On the second Thursday, customers B, C,
D, E and F buy 80, 99, 101, 103 and 110 water bottles, respectively.
A researcher wants to run a counting query on the data to learn
the count of each type of product bought for each day over the two
weeks.
Let us first observe the first week’s worth of data. Customer
A’s data skews the total count of water bottles on Tuesday from
30–60 to 130. Thus releasing this count may suffice to detect A’s
presence in the dataset. To hide this behavior, differential privacy
would have to add a noise proportional to the highest number of
water bottles that a customer can buy at any day at that store. In
the worst case this would be the entire store’s inventory of water,
and at the best case it would be a value close to 100. Moreover, this
noise will be added not just for the problematic day but for all days.
The noise would be too large, and it would drown out any trend
in the data, e.g., “people buy more water on weekends.” Outputs
protected by differential privacy (assuming ∆f = 100 and ϵ = 0.1)
are shown in Figure 1(a). Note the large difference between true
and outputted values (y axis is in the log scale), and note that all
points have been fuzzed (denoted by red circles in the figure).
Crowd-blending privacy (assuming k ≤ 30), on the other hand,
would remove A’s data and release the count of 30 bottles for that
first Tuesday and true counts for any other day in the first week.
This is shown in Figure 1(b). Commoner privacy would do the same
on this example, shown in Figure 1(c). Thus, commoner and crowd-
blending privacy will provide identical outputs on the first week
of data. In both cases, the outputs will have much higher research
utility than those produced by differential privacy.
Now let us observe the second week’s worth of data. Differential
privacy will again add the same large amount of noise to all data
points. Crowd-blending privacy with k = 5 will remove data for
customers B, C, D, E and F from Thursday, because they purchased
different quantities of water (80, 99, 101, 103, 110). But this may
be too restrictive, and it may unnecessarily reduce research value.
Note that ordered values are so close together that, while each is
unique, they probably cannot be gleaned from the sum. Commoner
privacy will help us identify cases when values are so close together
that they do not pose the risk of re-identification. Figures 1(b) and
1(c) illustrate how the data would be processed by crowd-blending
(k = 5) and commoner privacy (k = 5). Note that outputted values
for both mechanisms are close or identical to the true values. Also
note that commoner privacy provides higher research utility on
this dataset and for this query, because it only fuzzes one output
point, while crowd-blending privacy fuzzes two.
3 COMMONER PRIVACY
We start by defining two adversary models and positioning com-
moner privacy with respect to differential and crowd-blending
568(a) Diff privacy
(b) Crowd-blending
(c) Commoner
Figure 1: Water bottles sold per day on logarithmic scale: counts protected by differential, crowd-blending and commoner privacy. Cyan bars
show the true values and purple bars show the outputs of a privacy mechanism. Red circles denote data points that had to be fuzzed by the
mechanism to protect privacy.
privacy. We then define commoner privacy, and a mechanism that
achieves it.
3.1 Adversary Model
We consider two types of adversaries.
All-but-one adversary can observe all but one individual’s
records in the dataset, or in a group of records. This adversary is
not realistic, as there is nothing that prevents it from learning the
remaining individual’s records in the same manner that he learned
the others.
Interactive adversary learns about a dataset and its partici-
pants by posing queries and analyzing their outputs. The adversary
can also learn some information about an individual from auxil-
iary channels that he can try to cross-correlate with the system
outputs. Auxiliary information can be of two kinds: (1) adversary
can use some unique feature of the individual to establish presence
or absence of an individual in the dataset and to possibly learn
new information about the individual; or (2) certainty that the indi-
vidual’s data is in the released dataset. The interactive adversary
is weaker than the all-but-one adversary, but it is realistic. Pub-
lished attacks on various privacy mechanisms all use interactive
adversaries [3, 4, 12, 30, 33].
Differential privacy protects against all adversaries—the interac-
tive adversary and the stronger all-but-one adversary. Commoner
privacy and crowd-blending protect an individual against an in-
teractive adversary but not against the all-but-one adversary. The
all-but-one adversary can learn those features of an individual that
are sufficiently common, e.g., those where the individual blends
with a sufficiently large crowd.
Gehrke et al. prove [14] that crowd-blending can achieve dif-
ferential privacy (and protect against the all-but-one adversary) if
data is sampled at random prior to running a query. Intuitively, this
random sampling eliminates the certainty that the adversary may
have about an individual’s data being in the dataset. Similarly, pre-
sampling could be applied prior to commoner privacy to achieve
resilience against the all-but-one adversary. However, pre-sampling
loses research utility, and we leave research into quantifying this
trade-off in case of commoner privacy for future work. In this paper
we focus on commoner privacy without pre-sampling.
3.2 Privacy Goal
We assume that an individual I whose data is in the dataset has the
following privacy goals: (1) an adversary should not be able to learn
if I participated in the dataset, (2) an adversary who knows that I
participated in the dataset should not be able to learn the exact value
or the range of values for any field v in I’s records, except when
this value (or range) is shared by at least k − 1 other individuals. It
is this exception that lowers the privacy guarantee from differential
privacy. Both crowd-blending and commoner privacy assume that
such exception is acceptable to data contributors.
Under which circumstances would an individual agree to this
exception? We believe that this would happen when data itself is not
sensitive. In our example of the store releasing its sales data, B may
not mind if the adversary learns that he bought a lot of water on
Thursday, because this behavior is shared by four other customers,
and thus it is common. However, if these were health diagnoses
instead of prchased products, B would not want it known which
disease he had, even if that disease were very common. In that case
a pre-sampling step would be needed to remove the adversary’s
certainty that B’s data is in the dataset.
3.3 Commoner Privacy Definition
We first define k-blending, which establishes a measure of closeness
between individuals in a dataset for a given query. We then define
commoner privacy using k-blending. Finally, we discuss how to
identify outliers.
Definition 3.1. k-blending. An individual x k-blends with individu-
als y1, ..., yk−1 in dataset D with respect to privacy-preserving mecha-
nism San if San(x) is not an outlier in the set of values {San(x), San(y1), ... ,
San(yk−1)}.
Definition 3.2. Commoner privacy. A mechanism San is k-commoner
private if for every dataset D and every individual t ∈ D, either t k-blends
with individuals in D or the mechanism San ignores it.
Commoner privacy is a strictly lower goal than crowd-blending
privacy, since an individual that ϵ-blends with k−1 other individuals
also k-blends with them, while the opposite holds only when the
mechanism San has identical outputs for all k individuals.
569Theorem 3.3. Differential privacy → crowd-blending privacy → com-
moner privacy.
Proof.
In [14] Gehrke et al. prove the first part of this relation—that dif-
ferential privacy implies crowd-blending privacy. We now prove the second
part—that crowd-blending privacy implies commoner privacy. This follows
directly from the definitions of crowd-blending and commoner privacy.
When t ϵ-blends with k − 1 other individuals in D, this means that their
records (with respect to San) are so similar that they are interchangeable.
Thus t is not an outlier in the group, and it k-blends within that group. □
Outlier detection. There are several outlier detection algo-
rithms in research literature [15, 21]. We envision that the data
provider will select the one that best fits her data. We explored two
possible implementations in Section 5: (STDEV)—the commonly
used approach where a value is an outlier in a group if it lies outside
avд ± 3 · stdev range, where avд is the average, and stdev is the
standard deviation calculated over the entire group; and (MAD)—
the approach proposed in [21] where a value is an outlier in a group
if it lies outside med±3·b·med(abs(dev)), where med is the median
and med(abs(dev)) is the median absolute deviation, calculated over
all members of the group. The parameter b is set to 1.4826 as the
constant scale factor for normal distribution.
3.4 Commoner Privacy Mechanism
We now define one mechanism to achieve commoner privacy—
interactive k-anonymity.
Definition 3.4. Interactive k-anonymity. Given any function f , inter-
active k-anonymity is defined as:
(cid:40)
Mk(x, f (·), k) =
f (r ec(Id(xi))
0
Id(xi) k-blends in x
otherwise
(3)
It is obvious that interactive k-anonymity achieves k-commoner
privacy, as it returns non-zero outputs only for individuals that
k-blend in the dataset. Further, one could release a noisy output,
instead of zero, for those individuals that do not k-blend or ag-
gregate them into a larger group. Individuals that do not k-blend
enjoy differential privacy, while those that blend enjoy the lower,
commoner privacy.
With regard to the expected research utility, k-commoner pri-
vacy preserves higher utility than (k, ϵ)-crowd-blending privacy,
because it does not fuzz records of individuals that k-blend in the
dataset. Since k-blending does not imply ϵ-blending, but ϵ-blending
implies k-blending, the number of individuals ignored by commoner
privacy will be smaller than or equal to the number of individuals
ignored by crowd-blending privacy, for the same parameter k. This
means that commoner privacy will preserve more research utility
than crowd-blending privacy.
3.5 Discussion
We now briefly discuss some remaining details about commoner
privacy.
Setting of the parameter k. The parameter k controls the size
of the group, whose features are considered sufficiently common
to be revealed to a user. A data provider could set this parameter
based on his/her understanding of the dataset, the sensitivity of
information contained therein, and the common uses of the data.
Another model for setting of the parameter k would have each
k ← max(kI), I ∈ Id(x)
while ||Id(x)|| ≥ k do
for all i ∈ Id(x) do
if f (rec(i)) is an outlier then
remove rec(i)
k ← max(kI), I ∈ Id(x)
end if
end for
end while
x ← calculate from remaining data
Figure 2: Outlier removal algorithm for a data point x
individual I, which contributes to the dataset, set its own acceptable
value – kI . Figure 2 illustrates how outlier removal would work in
this second case for each data point x.
Use Scenarios. We see commoner privacy as a good alternative
to differential or crowd-blending privacy for datasets with a long tail
or with a large range of feature values, where it would be acceptable
that queries leak some data about common features or behaviors
of individuals in the dataset. Network traces and system logs are a
good example of this type of dataset. These datasets usually contain
very diverse, rich information about network traffic and network
hosts. The usual concern when allowing researcher access to these
datasets is to preserve anonymity of source and destination IP
addresses, and this is currently achieved through sanitization [36].
Yet, sanitization can always be broken with access to auxiliary
data [11]. On the other hand, allowing differentially private access
to these datasets may lose too much utility since network traffic and
host behaviors usually exhibit long tails. Commoner privacy offers
stronger privacy guarantees than sanitization and better utility
than differential privacy. The utility gain comes at the risk of the
adversary learning some common network traffic and host features
or behaviors. This may be acceptable to data providers, as they can
set the appropriate k value to control the level of privacy risk to
any individual host.
4 QUERY INTROSPECTION
Composition of queries that all satisfy k-anonymity, crowd-blending,
or commoner privacy may jeopardize the privacy of individuals due
to the existence of trackers. Tracker attacks have been investigated
in depth with regard to k-anonymity [7–9, 32]. The general idea
is to ask a set of queries, such that outputs of each query meet
the privacy criteria (k-anonymity, crowd-blending or commoner
privacy), but the outputs of their combination do not.
Gehrke et al. prove that a pre-sampling step prior to the pro-
cessing of each query can make crowd-blending hold under query
composition. We expect that a similar proof could be devised for
commoner privacy and pre-sampling, but leave this for future work.
In the following we describe our approach to protecting query
composition through query introspection, and we prove that it can
detect all trackers from prior work [8].
Let q be a query run on dataset D. The field set for this query,
denoted by F(q), is a set of data fields that are used in the query.
The query may be arbitrarily complex and composed of multiple
570Operation
Group
Keep
Arithmetic op.
Conditions
Transformation
Each group’s CIS is the union of CISs of items that were grouped
Some groups/records are dropped and their CISs are dropped too
The left hand side’s CIS becomes the union of CIS of items on the right hand side