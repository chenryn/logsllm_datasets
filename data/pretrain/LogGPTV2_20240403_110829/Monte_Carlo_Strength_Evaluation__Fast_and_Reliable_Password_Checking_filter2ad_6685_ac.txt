password generation.
We also implemented a method to generate explicitly the
passwords with highest probability, in order to evaluate em-
pirically the accuracy of our approach as done in Section 5.1.
To keep memory usage under control we use a depth-ﬁrst
exploration of the state space rather than using a priority
queue, exploring all states that have probability higher than
a chosen threshold [19]; we sort passwords by decreasing
probability after having generated them.
4.2.2 PCFGs
Probabilistic Context-Free Grammars (PCFGs) have been
proposed by Weir et al. in 2009 [30]: they are based on the
realization that passwords often follow similar structures,
such as for example a word followed by a number.
In the PCFG approach, passwords are grouped by tem-
plates, which are the way sequences of letters, digits and
symbols are concatenated to form the password. For exam-
ple, the “abc123” password has the L3D3 template, meaning
that it is composed of three letters followed by three digits.
The probability of the password is calculated by multiplying
the frequency of the template P (L3D3) by the frequency of
each pattern in the template, i.e.,
pPCFG (“abc123”) = P (L3D3) P (“abc”|L3) P (“123”|D3) ,
where P (“abc”|L3) and P (“123”|D3) are the frequency of
“abc” and “123” within the set of three-characters groups of,
respectively, letters and digits.
Weir et al. proposed to obtain the frequencies of tem-
plates, digits and symbols from a training set of passwords
and to get the frequencies of letter groups from a dictio-
nary; however, when the training set is large, PCFGs per-
form better by calculating also letter group frequencies from
the training set [19]. In Section 5.2, we conﬁrm this result.
Implementation Details.
We implemented PCFGs following the speciﬁcations by
We also precompute data to facilitate sample creation and
Weir et al.; implementing the sampling step is trivial.
162Instead of storing the frequencies of templates and groups
of characters, we store their number of occurrences. This
allows us to perform evaluations where the test set and
the training set are the same with “leave-one-out” cross-
validation: when computing the probability for a given pass-
word, we subtract the one occurrence which is due to the
password under evaluation.
4.2.3 Backoff
The backoﬀ model has been proposed by Katz in the ﬁeld
of natural language processing [14], and it has been pro-
posed for password cracking by Ma et al. [19]. This model
addresses sparsity by considering n-grams of all lengths, and
discarding those with less occurrences than a threshold τ .
Intuitively the idea is that, for each choice of a new charac-
ter, the model considers the longest sequence of past char-
acters appearing at least τ times in the training set.
Given a threshold τ , the probability of a password c1 . . . cl
under the backoﬀ model can be deﬁned by induction, start-
ing from the single character case. The probability of a single
character ˆc is that character’s frequency in the training set:
o (ˆc)(cid:80)
c o (c)
pbo (ˆc) =
,
where – as in Section 4.2.1 – o (c1 . . . cn) is the number of
occurrences of the c1 . . . cn string in the training set. The
probability of a string of more than one character is
pbo (c1 . . . cn+1) = pbo (c1 . . . cn) · P (cn+1|c1 . . . cn) ,
r (c1 . . . cn) =
o (c1 . . . cnc)
o (c1 . . . cn)
.
c:o(c1...cnc)≥τ
As for the n-grams case, each password is considered to
end with a ⊥ symbol.
We noticed that, by construction, this model often gener-
ates passwords that are suﬃxes of common ones (e.g., “ss-
word”). To avoid this problem, we propose to prepend – sim-
ilarly to what happens for n-grams – a start symbol co = ⊥
to each password. We will see in Section 5.2 that this mod-
iﬁcation improves the quality of the guesses for this model.
Implementation Details.
The output of this method only depends on the number
of occurrences of n-grams with at least τ occurrences. A
na¨ıve way of implementing this model would process the
training set once, count the number of occurrences of all n-
grams for any value of n and, at the end of the process, drop
information for all n-grams with less than τ occurrences.
This approach requires very large amounts of memory, most
of which is devoted to temporarily memorizing the (in most
cases low) number of occurrences for n-grams with large n.
It is therefore problematic for our very large datasets.
In our implementation, we proceed by scanning the train-
ing set multiple times, and at the i-th iteration we compute
the number of occurrences of i-grams, discarding those that
have less than τ occurrences. We know for sure that the
where
P (c|c1 . . . cn) =
and
o(c1...cn)
(cid:40) o(c1...cnc)
(cid:88)
if o (c1 . . . cnc) ≥ τ,
P (c|c2 . . . cn) r (c1 . . . cn) otherwise
def starting_state():
Algorithm 2 Sample creation for the backoﬀ model.
if using the start symbol: return “⊥”
else: return “”
def update_state(s, t):
append t to s
while o (s) < τ :
drop the ﬁrst character in s
Run Algorithm 1 using these functions.
number of occurrences of an i-gram will not be higher than
the number of occurrences of both its preﬁx and suﬃx of
length i− 1: therefore, we can avoid storing any information
about i-grams whose preﬁx or suﬃx is not included in the
previous iteration. As an additional optimization, at the i-
th iteration we avoid considering passwords that – thanks
to this optimization – had no impact on the calculation in
the previous round.
We store the number of occurrences of each n-gram to
facilitate leave-one-out cross validation, for the reasons de-
scribed in Section 4.2.2. However, to make it eﬃcient to
create a sample and to generate passwords, we also precom-
pute the values of each P (c|c1 . . . cn) when o (c1 . . . cn) ≥ τ .
We consider c1 . . . cn as a state and c as a transition, and
we apply the same treatment described in Section 4.2.1 to
generate the CS auxiliary data structures. The sample cre-
ation algorithm then follows the procedure introduced for
n-grams (Algorithm 1), overriding the starting_state and
update_state functions as described in Algorithm 2.
5. EXPERIMENTAL RESULTS
We now proceed with our experimental evaluation, in which
we provide the following contributions: we evaluate the pre-
cision – and the limitations – of our method (Section 5.1).
We then provide a detailed comparison of the eﬃcacy in
cracking passwords of the attack models we consider (Sec-
tion 5.2); we discuss the impact of training set size on the
performance of guessing algorithms (Section 5.3); ﬁnally,
we evaluate the strength of passwords that satisfy typical
mandatory restrictions (Section 5.4).
Unless otherwise stated, in the experiments the sample
size n = |Θ| used by our method is 10,000; results are ob-
tained by evaluating a random subset of 10,000 passwords in
the test (Rockyou) database; the value of τ for the backoﬀ
model is 10 in line with Ma et al. [19]. Models are always
trained on the full training datasets.
5.1 Method Precision
In Figure 1, we show the relationship between the proba-
bility that models attribute to passwords and their rank (i.e.,
the Sp (α) value introduced in Equation 1). We evaluate the
models that behave best for password guessing (more infor-
mation about them is provided in Section 5.2). For each of
these models, we generated all the passwords having proba-
bility higher than 2−20; for each of them, we plot their rank
against their probability. The shaded areas are the ones be-
tween the minimum and maximum estimation provided by
our model, re-generating a sample Θ for 5 diﬀerent runs.
In the ﬁrst plot, estimates are so close to the correct val-
ues that it is impossible to discern the shaded areas; in the
second plot, we show a closeup of a particular area. We ex-
163sample standard deviation.1 We plot the results obtained on
evaluations based on 30 diﬀerent samples, using the 3-gram
model. This plot helps us in quantifying the precision in
estimating password strength and shows that, as predicted
by Theorem 2, estimates do converge as the sample size
grows. These results can be used as a guideline to choose
the sample size n: if the user is only interested in a rough
order-of-magnitude estimation of password strength, even a
sample that is as small as 100 elements can be suﬃcient.
In Figure 4, we investigate the limitations of our approach.
When we start evaluating very strong passwords, our esti-
mation of their strength becomes deﬁnitely more uncertain:
the reason is that, if p (α) is low, a password with proba-
bility close to p (α) is unlikely to appear in our sample, and
its presence or absence has a strong impact on our strength
estimation since it would add a (large) value 1/p (α) to the
estimation. The PCFG and backoﬀ models are less likely
to generate passwords with very low probability (see Figure
2), and this results in higher uncertainty on their strength.
Strength estimation is less precise for stronger passwords:
when these estimates are used to inform users, we deem this
uncertainty acceptable, as these passwords can generally still
be considered strong enough for most evaluation purposes.
In Figure 5, we compare the rank–probability pairs for
ranges that are expensive or prohibitive to evaluate by gen-
erating explicitly the passwords (in this case, to limit the
uncertainty of our estimation for very small probabilities, we
raised the sample size to 100,000). The probability values
diverge between models as the rank grows: the probabili-
ties for passwords having the same rank can diﬀer between
models by orders of magnitude. This result suggests that
it is dangerous to compare probability attributed to pass-
words by diﬀerent models when what we are interested in is
password strength; our method, instead, provides a sound
solution to this problem.
In Figure 6, we highlight the diﬀerence between the mod-
els by plotting the value of probability multiplied by rank.
If models had similar rank values associated with the same
probability, this value should be similar between attack mod-
els; when discussing the opportunity of comparing probabil-
ities between diﬀerent attack models, Ma et al. [19] conjec-
tured that this value should remain between 2−3 and 2−8;
this would have implied a roughly linear relationship be-
tween probability and rank, and justiﬁed comparing proba-
bilities output by diﬀerent models when ranks are expensive
to compute. Our results show that this is not the case:
again, this comes as evidence that comparing the number
of guesses is unavoidable if one wants to compare diﬀerent
probabilistic password models.
5.2 Comparing Attack Models
With the results of the previous section, we have estab-
lished that our method yields precise estimates of password
strength and that simply comparing probabilities – as done
in other works – is not suﬃcient to reliably compare attacks.
We now show a comparison in terms of guess-number graphs,
plotting the probability that an attacker would guess a sin-
gle password against the number of guesses; to the best of
our knowledge, this study is the ﬁrst to report guess-number
graphs comparing several state-of-the-art attacks up to an
1Relative sample standard deviation is the square root of
sample variance (i.e., variance computed using Bessel’s cor-
rection) divided by the mean of the values.
Figure 1: Password probability vs. rank. We show the cor-
rect values obtained by enumerating passwords by descend-
ing probabilities and, in the shaded areas, the area between
the minimum and maximum of 5 diﬀerent estimates.
perimentally conﬁrm the result we have proven in Theorem
1: our estimates converge to the correct values. An inter-
esting insight that we gain by looking at the leftmost area
of the password enumeration space, is that the probability-
rank relations can clearly diﬀer between models.
The diﬀerence in the probability vs. rank plot is due to
the fact that each model generates passwords with a dif-
ferent probability distribution.
In Figure 2, we show the
histograms resulting from binning the generated passwords
in each(cid:2)2−n, 2−n−1(cid:1) probability interval. From these plots,
it is clear that models have a very diﬀerent signature in
terms of probabilities attributed to passwords: in particular,
the probability distribution of passwords generated through
PCFGs and the backoﬀ model are clustered on probabilities
between 2−20 and 2−25; n-gram models, instead, are more
likely to generate passwords that have very low probabilities.
It is interesting to note that the training set contains
around 223 unique passwords; as we shall see in the following
results, this means that passwords labeled with a probabil-
ity lower than around 2−27 − 2−29 are unlikely to be present
in the original datasets. Models such as 3- and 4-grams are
deﬁnitely more likely to generate, when sampled, passwords
that do not appear in the original training set.
Theorem 1 proves that our estimates are correct on av-
erage, but the expression of the variance in Equation 3 is
diﬃcult to interpret, because it depends on an unknown
probability distribution. In Figure 3, we plot the level of un-
certainty we have in our estimation, in the form of relative
2−202−172−142−112−8Probability2327211215Rank3-grams4-gramsPCFGbackoff2−182−17Probability213214Rank3-grams4-gramsPCFGbackoff164(a) 3-grams.
(b) 4-grams.
bilities in the(cid:2)2−n, 2−n−1(cid:1) interval. The probability distribution varies widely depending on the model used.
Figure 2: Probability distribution for passwords generated by diﬀerent models. Each bin contains passwords having proba-
(c) PCFG.
(d) Backoﬀ.
Figure 3: Estimation uncertainty while varying sample size.
Estimates converge as the sample size grows.
Figure 4: Estimation uncertainty for very small probabilies.
When probabilities reach values for which elements in the
sample are rare (see Figure 2), estimations lose precision.
extremely large (280) number of attempts. Here and in the
following, results are obtained by evaluating the passwords
in the Rockyou dataset, using Xato as a training set.
Figure 7 shows an overview of the attacks that – as we will
see in the following – perform best. For the ﬁrst 216 − 220
guesses, backoﬀ and PCFG perform almost equivalently:
this is because both models are essentially guessing the most
frequent passwords in the training dataset. When the num-
ber of occurrences of a passwords falls below the backoﬀ
models’ threshold, the PCFG model continues by guessing
words that are in the training set, while the backoﬀ model
starts attempting “generalizations”, attempting words that
are not in the training set. At ﬁrst, this favors PCFGs,
but they eventually lose eﬃcacy because many existing pass-
words are not represented by the patterns they model.
From Figure 7, we conclude that the optimal attack strat-
egy varies depending on the number of guesses that the at-
tacker can aﬀord: PCFGs perform best at ﬁrst but they are
not capable of ﬁnding a substantial percentage of passwords,
while 4-grams and then 3-grams become preferrable to at-
tack stronger passwords. While never being optimal, the
backoﬀ strategy performs well across the whole spectrum of
passwords, proving that it indeed “self-tunes” to avoid the
dead-ends that models such as PCFGs reach.
In Figure 8, we analyze whether diﬀerent attacks would
require similar eﬀort to guess the same password.
In the
scatter-plots, we compare the number of attempts needed
to guess a password between the backoﬀ strategy and the
other ones considered in Figure 7. The correlation between
the approaches is obvious, suggesting that the strength of
a password against an attack is indicative of its strength
against other attacks. In addition, we notice that the backoﬀ