is inherent to the two-part design of the Safe Browsing API, it
may be possible to ameliorate even this risk using structured
noise queries [34].
7
We implement Checklist in 2,481 lines of Go and 497 lines of C.
(Our code is available on GitHub [1].) We use C for the most
performance-sensitive portions, including the puncturable
pseudorandom set (Section 3.2).
7.1 Microbenchmarks for oÔ¨Ñine-online PIR
First, we evaluate the computational and communication costs
of the new oÔ¨Ñine-online PIR protocol, compared to two pre-
vious PIR schemes. One is an information-theoretic protocol
of Chor et al. [25] (‚ÄúMatrix PIR‚Äù), which uses
ùëõ bits of
communication on an ùëõ-bit database. The second comparison
protocol is that of Boyle, Gilboa, and Ishai [15], based on
distributed point functions (‚ÄúDPF‚Äù). This protocol requires
only ùëÇ(log ùëõ) communication and uses only symmetric-key
cryptographic primitives. We use the optimized DPF code
of Kales [55]. We run our benchmarks on a single-machine
single-threaded setup, running on a e2-standard-4 Google
Compute Engine machine (4 vCPUs, 16 GB memory).
Static database. We begin with evaluating performance on
a static database. Figure 3 presents the servers‚Äô and client‚Äôs
‚àö
per-query CPU time and communication costs on a database of
three million 32-byte records. Since the Checklist PIR scheme
has both oÔ¨Ñine and per-query costs, the Ô¨Ågure also presents
the amortized per-query cost as a function of the number of
queries to the static database made by the same client following
an initial oÔ¨Ñine phase. Figure 3 shows that the oÔ¨Ñine-online
PIR scheme reduces the server‚Äôs online computation time by
100√ó at the cost of an expensive seven-second oÔ¨Ñine phase,
which the server runs once per client. Even with this high
oÔ¨Ñine cost, for a suÔ¨Éciently large number of queries, the
Checklist PIR scheme provides overall computational savings
for the server. For example, after 1,500 queries, the total
computational work of a server using Checklist PIR is two to
four times less than that of a server using the previous PIR
schemes. The Checklist PIR scheme is relatively expensive in
terms of client computation‚Äîup to 20√ó higher compared to
the previous PIR schemes.
Database with periodic updates. We evaluate the perfor-
mance of the waterfall updates mechanism (Section 5.2). This
experiment starts with a database consisting of three million
32-byte records. We then apply a sequence of 200 updates to
the database, where each update modiÔ¨Åes 1% of the database
records. After each update, we compute the cost for the server
of generating an updated hint for the client. Figure 4 shows the
cost of this sequence of updates. The majority of the updates
require very little server computation, as they trigger an update
of only the smallest bucket in the waterfall scheme. We also
plot the average update cost (dashed line) in the waterfall
scheme and the cost of naively regenerating the hint from
scratch on each update (red square). The waterfall scheme
reduces the average cost by more than 45√ó.
Next, we evaluate the impact of using the waterfall update
scheme on the query costs. This experiment begins with a
database of ùëõ = 3√ó106 records, of size 32 bytes each, and runs
through a sequence of periods. At the beginning of each period,
we apply a batch of ùêµ = 500 updates to the database, after
which the client fetches a hint update from the server, and then
performs a sequence of queries. We measure the cost to the
USENIX Association
30th USENIX Security Symposium    885
10
1
0.1
0.01
0.001
)
c
e
s
(
e
m
i
t
r
e
v
r
e
S
0.0001
0
(0%)
50
(50%)
Initial setup
Waterfall update
Running average
100
(100%)
150
(150%)
200
(200%)
Updates (% DB changed)
250
(250%)
300
(300%)
Figure 4: Server-side cost of client updates. At each time step, 1%
of the three million records change. The waterfall update scheme
reduces the average update cost by more than 45√ó relative to a naive
solution of rerunning the oÔ¨Ñine phase on each change.
server of generating the update and responding to the queries.
We amortize the cost of each update over the queries in each
period, and we average the costs over ùëõ/ùêµ consecutive periods,
thus essentially evaluating the long-term amortized costs of
the scheme. Figure 5 presents the amortized server costs as a
function of the number of queries made by a single client in
each period. The new PIR scheme outperforms the previous
schemes as long as the client makes a query at least every 10
periods (i.e., at least once every 5000 database changes). As
queries become more frequent, the reduced online time of our
scheme outweighs its costly hint updates.
7.2 Safe Browsing with Checklist
To evaluate the feasibility of using Checklist for Safe Browsing,
we integrate Checklist with Firefox‚Äôs Safe Browsing mecha-
nism. We avoid the need to change the core browser code by
building a proxy that implements the standard Safe Browsing
API. The proxy runs locally on the same machine as the
browser, and we redirect all of the browser‚Äôs Safe Browsing
requests to the proxy by changing the Safe Browsing server
URL in Firefox conÔ¨Åguration. See Figure 2.
We begin by measuring the rate of updates to the Safe
e
m
i
t
r
e
v
r
e
s
d
e
z
i
t
r
o
m
A
)
s
m
(
y
r
e
u
q
r
e
p
100
10
1
1/32
OÔ¨Ñine-online
DPF
Matrix
1/4
16
Number of queries per period
2
128
Figure 5: The amortized server compute costs of PIR queries on a
database with updates. As the number of queries between each pair
of subsequent database updates grows, the oÔ¨Ñine-online PIR scheme
achieves lower compute costs compared to previous PIR schemes.
Browsing database and the pattern of queries generated in
the course of typical web browsing. To this end, our proxy
logs all Safe-Browsing requests, forwards them to Google‚Äôs
server, and logs the responses. (For privacy, we do not log the
actual URL hashes.) This trace allows us to directly compute
the frequency of lookups. Moreover, the fact that the browser
continuously downloads updates to the list of partial hashes
allows us to compute the rate of updates to the database. We
run the proxy on our personal laptops for a typical work week,
using the instrumented browser for all browsing. The database
size is roughly three million records, and it has grown by
about 30,000 records over the course of the week. These data
are consistent with the public statistics that Google used to
publish on the Safe Browsing datasets [44]. In our trace, the
client updates its local state every 94 minutes on average and
performs an online lookup every 44 minutes on average.
We repeatedly replay our recorded one-week trace to sim-
ulate long-term usage of Checklist. On each update request
in the trace, we Ô¨Årst use the information from the response
to update the size of the Checklist database, such that the
database size evolves as in the recording. We measure the cost
of fulÔ¨Ålling the same update request using Checklist, which
includes updating the list of partial hashes and updating the
client‚Äôs PIR hint. For each lookup query in the trace, we issue
a PIR query. Figure 6 shows the cumulative costs of using
Checklist with two diÔ¨Äerent PIR schemes. We measure the
server costs on an e2-standard-4 Google Compute Engine ma-
chine with 16 GB of memory and the client costs on a Pixel 5
mobile phone. OÔ¨Ñine/online PIR requires 5.5√ó less computa-
tion on the server and 9√ó more computation on the client than
DPF-based PIR. In absolute terms, the amortized computation
on the client when using Checklist with oÔ¨Ñine/online PIR is
less than 0.4 CPU-seconds per day. OÔ¨Ñine-online PIR uses
more communication, mostly due to the cost of maintaining
the hint: it doubles the communication cost of the initial setup,
and requires 2.7√ó more communication than DPF-based PIR
on a running basis. Checklist with DPF-based PIR uses only
20% more communication than non-private Safe Browsing.
We also measure the amount of local storage a Checklist
client requires for its persistent state. With DPF-based PIR,
or with non-private lookups, the client stores a 4-byte partial
hash for each database record. Delta-encoding the list of
hashes [42]) further reduces the storage to fewer than 1.5
bytes per record (for a list of 3 million partial hashes). With
oÔ¨Ñine-online PIR, the Checklist client stores on average 6.8
bytes for each 32-byte database record, in order to store the
list of partial hashes and the latest hint. To reduce the query
time, the client also stores an 18-bit ‚Äúset pointer‚Äù from each
database index to a set that contains it, as described at the
end of Section 4. The total storage cost for a list of 3 million
partial hashes is 25MB. As a point of reference, the download
size of the the Firefox Android package is 70MB [66], and
after installation, it uses 170MB of storage.
To measure the end-to-end throughput and latency of Check-
886    30th USENIX Security Symposium
USENIX Association
OÔ¨Ñine-online
OÔ¨Ñine
Online
DPF
Non-private
(a) Server computation
(b) Mobile-client computation
(c) Communication
Figure 6: We repeatedly replay the trace of Safe Browsing queries and updates, recorded on a seven-day user session. The server-side
computational saving of oÔ¨Ñine-online PIR comes at a cost of more communication and client computation. The measurements are of
the application-level costs of Checklist and do not include the computation and communication cost of the network stack. The client-side
computation cost of Checklist is less than 0.4 CPU-seconds per day. Discontinuities happen when buckets in the waterfall scheme overÔ¨Çow and
trigger a hint update for a larger bucket.
list, we set up three virtual cloud instances: a Checklist server,
a Checklist client, and a load generator. The load generator
simulates concurrent virtual Checklist users, by producing
a stream of requests to the server, each through a new TLS
connection. The generator sets the relative frequency of update
)
c
e
s
m
(
y
c
n
e
t
a
L
300
240
180
120
60
OÔ¨Ñine-online
DPF
Non-private
10K
100K
1M
10M
Throughput (users)
Figure 8: The performance of a Checklist server. Solid lines display
the average latency, and shaded regions show the latency of the
95th-percentile of requests.
and query requests, as well as the size of the updates, based on
the recorded trace. With the server under load, an additional
client machine performs discrete Checklist lookups and mea-
sures their end-to-end latency. The measured latency includes
the time it takes the client to generate the two queries, obtain
the responses from the server, and process the responses. We
compare between (i) Checklist running the new oÔ¨Ñine-online
PIR protocol, (ii) Checklist running the DPF-based protocol,
and (ii) Checklist doing non-private lookups. Figure 8 shows
that the throughput of a single Checklist server providing
private lookups using oÔ¨Ñine-online PIR is 9.4√ó smaller (at
a similar latency) than that of a server providing non-private
lookups. A Checklist server achieves 6.7√ó higher throughput
and a 30ms lower latency when using oÔ¨Ñine-online PIR,
compared to when running DPF-based PIR.
Table 7 summarizes our evaluation of Safe Browsing with
Checklist. We estimate that a private Safe Browsing service
using Checklist with oÔ¨Ñine-online PIR would require 9.4√ó
more servers than a non-private service with similar latency.
A DPF-based PIR protocol would require 6.7√ó more servers
than our oÔ¨Ñine-online protocol and would increase the latency
Table 7: Summary of costs of Safe Browsing with Checklist. For each column, we use green, yellow, and red, to indicate the least-, middle-, and
most-expensive solution. The oÔ¨Ñine-online variant oÔ¨Äers lower compute costs and latency, while a DPF-based system is more communication
eÔ¨Écient. The second row presents the communication costs of a fully oÔ¨Ñine solution in which the client maintains a local copy of the blocklist.
Approach
Server costs
(servers per 1B users)
Latency Client computation Communication Client storage
Initial Running
(MB) (MB/month)
Running
(sec/month)
Non-private
Full list