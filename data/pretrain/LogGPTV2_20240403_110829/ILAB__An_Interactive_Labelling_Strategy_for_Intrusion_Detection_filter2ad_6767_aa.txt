# ILAB: An Interactive Labelling Strategy for Intrusion Detection

## Authors
Anaël Beaugnon, Pierre Chifflier, and Francis R. Bach

## Affiliations
1. French Network Security Agency (ANSSI), Paris, France
2. INRIA, École Normale Supérieure, Paris, France

## Contact
- {anael.beaugnon, pierre.chifflier}@ssi.gouv.fr

## Abstract
Acquiring a representative labeled dataset is a significant challenge in training supervised detection models, especially in computer security where expert knowledge is required for annotation. This paper introduces ILAB, an innovative interactive labeling strategy that reduces the workload for experts when labeling large datasets for intrusion detection. We compare ILAB with two state-of-the-art labeling strategies on public labeled datasets, demonstrating its effectiveness and scalability. Additionally, we validate ILAB's practicality through a real-world annotation project on a large, unlabelled NetFlow dataset from a production environment. Our open-source implementation (https://github.com/ANSSI-FR/SecuML/) allows security experts to label their own datasets and researchers to compare different labeling strategies.

**Keywords:** Intrusion detection, Active learning, Rare category detection

## 1 Introduction
Supervised learning has been successfully applied to various intrusion detection problems, such as Android applications, PDF files, botnets, Windows audit logs, and portable executable files. However, these models require representative labeled datasets, which are expensive to build in computer security due to the need for expert knowledge and the confidentiality of data. Unlike other fields, crowd-sourcing cannot be used to acquire labeled datasets at low cost. Public datasets like Malicia, KDD99, and Kyoto2006 are often outdated and do not account for specific deployment contexts.

Experts are crucial for annotation but are a limited resource, making efficient use of their time essential. Active learning methods aim to reduce labeling costs by querying only the most informative examples, but they can suffer from sampling bias, leading to the oversight of certain families of malicious or benign examples. This is particularly problematic in intrusion detection, where missing a malicious family can result in undetected threats. Furthermore, the labeling strategy must scale to large datasets and ensure a good interaction between the expert and the model.

In this paper, we introduce ILAB, a novel interactive labeling strategy that helps experts label large datasets efficiently while avoiding sampling bias. ILAB uses a hierarchical active learning method with binary labels (malicious vs. benign) and user-defined families. It is designed to discover different malicious and benign families and scales well to large datasets, ensuring a low waiting time for experts.

Our contributions include:
- A new active learning method, ILAB, designed to avoid sampling bias, with low computational cost and scalability.
- A comparison of ILAB with two state-of-the-art active learning methods for intrusion detection, demonstrating its effectiveness and scalability.
- Validation of ILAB's practicality through a real-world annotation project on a large, unlabelled NetFlow dataset.
- An open-source implementation of ILAB and the graphical user interface used in the annotation project.

## 2 Background and Related Work
### Active Learning
Active learning methods aim to reduce labeling costs by querying only the most informative instances. However, this can lead to sampling bias, where certain families of observations are completely missed. In intrusion detection, this can result in undetected malicious families. Uniform random sampling is not feasible due to the under-representation of the malicious class.

### Sampling Bias
Sampling bias occurs when a family of observations is overlooked during the labeling process. For example, in one-dimensional uncertainty sampling, instances from a rare family might not be included in the initial training set, leading to a decision boundary that overlooks this family.

### Related Work
Online active learning is suitable for updating detection models over time, but our focus is on acquiring a representative labeled dataset offline. Previous works, such as Almgren et al., Aladin, and Gönitz et al., have proposed methods to address sampling bias, but at the expense of expert-model interaction. ILAB addresses this by using a divide-and-conquer approach to ensure a good interaction.

## 3 Problem Statement
The goal is to acquire a representative labeled dataset from a pool of unlabelled instances with minimal human effort. Both the number of annotations and the computation time for generating queries must be minimized. We assume no adversarial attempts to mislead the labeling strategy.

### Notations
- **D**: Unlabelled dataset with N instances, each described by m real-valued features.
- **L**: Set of labels {Malicious, Benign}.
- **Fy**: Set of user-defined families for label y ∈ L.
- **DL**: Labeled dataset associating a label y and a family z to each instance x ∈ D.
- **DU**: Pool of remaining unlabelled instances.
- **B**: Annotation budget, the maximum number of instances the expert can annotate.

### Objective
The objective is to build DL by asking the expert to annotate B instances that maximize the performance of the detection model M. The labeling strategy must be scalable and maintain a low expert waiting time.

## 4 ILAB Labelling Strategy
ILAB is an iterative annotation process combining active learning and rare category detection. At each iteration, the expert annotates b instances to improve the current detection model and discover new families. Active learning improves the binary classification model, while rare category detection avoids sampling bias.

### Initial Supervision
Initial labeled examples are needed to start the active learning process. If a public labeled dataset is available, it can be used. Otherwise, widely deployed signatures can provide initial labels, though they may belong to a limited number of families.

### Labeling Strategy
ILAB selects instances from the unlabelled pool based on a combination of uncertainty sampling and likelihood-based methods. This ensures that both informative and potentially rare instances are annotated, avoiding sampling bias and improving the detection model.

## 5 Evaluation
We compare ILAB with two state-of-the-art labeling strategies on public fully labeled datasets, demonstrating its effectiveness and scalability. Up to our knowledge, these strategies have never been compared before. We provide an open-source implementation to foster future research.

## 6 Real-World Application
We validate ILAB's practicality through a real-world annotation project on a large, unlabelled NetFlow dataset from a production environment. We provide an open-source implementation of the graphical user interface used in the project to allow security experts to label their own datasets.

## Conclusion
ILAB is a novel interactive labeling strategy that effectively and efficiently labels large datasets for intrusion detection. It avoids sampling bias, scales to large datasets, and ensures a good expert-model interaction. Our open-source implementation makes it accessible for both security experts and researchers.