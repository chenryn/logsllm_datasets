Puv(i, j)log
Puv(i, j)
Pu(i)Pv( j)
(1)
R is the maximum pixel intensity value. The marginal prob-
ability distribution Pu(i) refers to the ratio of the pixels of
intensity value i in image u to all the pixels in image u. Puv
is the joint probability distribution function between two cer-
tain images u and v. The probability Puv(i, j) refers to the
ratio of the number of pixel points, where the pixel inten-
sity value is i in image u and j in image v under the same
coordinates, to the total number of pixels. If Puv(i, j) = 0,
Pu(i)Pv( j) = 0. For each pair (u,v),u ∈
we handle Puv(i, j)log Puv(i, j)
D,v ∈ D,u (cid:54)= v, we calculate its MI value by Equation 1, and
obtain the MI value matrix. Equation 1 considers not only the
number of pixel intensity values, but also their positions.
For a seek of generalizability, we introduce a new matrix
I and a hyperparameter α used to represent the weight of
mutual information. The choice of α is discussed in Section 7.
The correspondence between matrix I and mutual information
is as follows:
I[u][v] = MI(u)(v)α
(2)
For convenience, we will use matrix I hereafter. Therefore,
the process of sampling k data points with minimizing the
sum of MI values between them can be formalized as:
argmin
S
H =
1
2 ∑
i∈S
∑
j∈S
I[i][ j], i (cid:54)= j
(3)
We aim to select a more representative and reduced dataset
through minimizing the mutual information value between
any two data samples as shown in Figure 2. Assuming a big
dataset D, and n = |D|, we intend to ﬁnd a simpliﬁed dataset
S, where S ⊂ D,k = |S|  0, it means there does not exist
any independent set S of at least size k in graph Gi.
Therefore, the maximum independent set problem can be
reduced to our problem in polynomial time. Since the max-
imum independent set problem is NP-Hard, our problem is
|V| = n, k: the size of target subgraph
Algorithm 1: Data Reduction on Mutual Information
Input: G(V,E): a weighted undirected graph where
Output: Smin where Smin ⊂ V ∧|Smin| = k
1 Hmin ←MAXNUM;
2 Smin ← {};
3 for t ∈ V do
4
5
6
7
8
S0 ← greedy_choice_initialization (t);
S,H ← one_hot_replacement_optimization (S0);
if H = In[p] then
2 ∑t In[t], t ∈ S;
break;
S = S (cid:83) {q}\{p};
H = H + Out[q]− I[p][q]− In[p];
In[q] = Out[q]− I[p][q];
Out[p] = In[p] + I[p][q];
In[t] = In[t]− I[t][p] + I[t][q], t ∈ S,t (cid:54)= q;
Out[t] = Out[t]− I[t][p] + I[t][q], t /∈ S,t (cid:54)= p;
11
12
13
14
15
16
17 return S,H
other points from S0, which makes sense when i ∈ S0. Out[i]
expresses the sum of MI values between i and all points from
S0, which makes sense when i /∈ S0. Then we can calculate
the initial value H:
1
2 ∑
i∈S0
In[i], j (cid:54)= i
1
2 ∑
i∈S0
I[i][ j] =
∑
j∈S0
H =
(9)
Next, we need to adjust set S(= S0). Starting from S, we
remove a data point with poorest performance in set S, and
move into a data point with best performance outside set S.
Here, poor performance means this point has the maximum
In value, and good performance means the minimum Out
value. If a swap (p,q) could make H decrease (H(cid:48) < H in
Equation 10), we perform such an exchange.
H(cid:48) = H + Out[q]− In[p]− I[p][q], p ∈ S, q (cid:54)= S
(10)
Then we repeat the above exchange process until H is no
longer decreasing. We call this method of adjusting and opti-
mizing the solution as one-hot replacement.
Algorithm 3 presents the one-hot replacement optimization,
which is based on the exchange of vertices to optimize the
solution. This algorithm needs to optimize the ﬁnal set S and
reduce H value according to the initial set. We give the initial
set S0 to the ﬁnal S at line 1. For a vertex in S, we compute
the sum of distances with other vertices in S (Line 3). For
a vertex not in S, we compute the sum of distances with all
vertices in S (Line 4). Then we calculate the initial H value.
1906    30th USENIX Security Symposium
USENIX Association
Line 6 to 16 are the loop to ﬁnd set S with smaller H values.
According to Equation 10, we ﬁrst ﬁnd the vertex p which
has the maximum In[p] in S, then the vertex q which has the
minimum Out[q]− I[p][q] not in S. Line 9 and 10 are the ter-
mination condition of the loop. If this condition is satisﬁed,
H will not decrease after swapping vertices. Line 11 to 16
explain how to update variable values during the exchange
process. Line 11 calculates the new H, and line 12 puts q in
and puts p out to update S. Line 13 to 16 update the In or Out
values for each vertex according to moving in a new vertex
q and out an old vertex p. After the loop ends, the algorithm
returns the minimum H and its corresponding set S at line 17.
This algorithm can be terminated efﬁciently partially due to
the greedy-choice initialization which offers an approximated
optimal solution. It then takes only a few exchanges to reach a
better solution. The transitivity of data similarity [62] further
prevents one sample from being exchanged for multiple times.
As a consequence, the replacement is expected to be termi-
nated within O(k) iterations. Our experiments with different
datasets also conﬁrm that the iteration number is lower than
a constant (<10) multiple of k. Additionally, the worse-case
complexity of the in-loop computation is O(n). Therefore, the
time complexity of one-hot replacement is O(kn).
5 Evaluation
In this section, we describe the implementation details of our
approach and the evaluation experiments.