ters and traces. Because the number of links is much larger
than the number of switches or Muxes, the link counters
dominate all the other counters. For each link, we have 6
load counters (3 in each direction) and 4 latency counters
(for latency histogram). Given 250K links, the total num-
ber of counters is 2.5M. Since each counter takes 20 bytes
and the update interval is 10 seconds, each analyzer con-
sumes only 2.5M × 20B/10s = 5M B/s for uploading all
the counters into storage.
In our deployment, analyzers record less than 0.01% of all
traces, since most normal traces are aggregated into counters
(§4.1). Thus uploading packet traces requires only 380Gbps×
0.01%/8 = 4.75M B/s in total. Such a data rate can be eas-
ily handled by the distributed storage service.
Query delay. The response time of answering a query by
an Everﬂow application mainly depends on the data volume
being queried. We benchmark the GetTrace() query on 10
GB traces (roughly equal to the amount of traces generated
in 30 minutes), and the SCOPE job ﬁnishes in one minute.
We observe similar query delay for GetCounter(). For all the
incidents presented in §7, we never need to query more than
10 GB of data.
8.3 Deployment cost
Since most mirrored packets are small, the analyzers are
bottlenecked by the CPU. Given a total of 300M pps (or 380
Gbps) mirrored trafﬁc, we need 300M pps/4.8M pps = 63
analyzers. Furthermore, a single switch-based reshufﬂer is
sufﬁcient to handle all the mirrored trafﬁc. We currently use
two reshufﬂers for failure resilience.
9. RELATED WORK
Our work builds on several themes of related work.
Like Everﬂow, Planck [31] and PacketHis-
Mirroring.
tory [14] monitor network through switch mirroring. In Planck,
switches mirror trafﬁc at over-subscribed ports to a directly
attached server. It focuses on packets at a single device in-
stead of network-wide packet traces. PacketHistory mirrors
all packets at switches to remote servers. For large scale
Figure 9: An overlay loop formed be-
tween a SLB Mux and a DIP
We evaluate Everﬂow’s capacity, overhead and deploy-
ment cost and show that it can easily scale to a large DCN
with 10K switches, 250K links and 100 Tbps trafﬁc.
In
the analysis below, we ignore the switch overhead because
match-and-mirror is done completely at the switch’s data
plane and incurs zero overhead on the switch’s CPU (§6.1).
8.1 Capacity
Analyzer. We use servers with Xeon 16-core, 2.1 GHz
CPU, 128 GB memory and 10 Gbps Ethernet NIC. Depend-
ing on the packet size, the bottleneck may be the CPU (capped
by number of packets per second, or pps) or Ethernet NIC.
For small packets, our packet capturing library achieves
around 600K pps per thread. With RSS support, which scales
capturing throughput linearly with the number of CPU cores,
our library is capable of capturing 4.8M pps using 8 CPU
cores.4 Because our analysis task is light weight, one CPU
core can easily analyze more than 600K pps. Thus we run
8 capturing threads and 8 analysis threads, with one thread
per core. In total, each server can process 4.8M pps with 16
cores.
For large packets, the NIC becomes the bottleneck. If all
packets are 1,500 bytes (standard MTU in DCNs), a server
with a 10 Gbps NIC will handle only 10Gbps/8/1500 =
0.83M pps. Meanwhile, memory is never a bottleneck. Since
the analyzer only buffers packets that arrive in the last sec-
ond (§4), maximum memory usage is 10Gbps×1s = 1.25GB.
Reshufﬂer. The reshufﬂer processes packets at a switch’s
line rate. Using a switch with 128 × 10Gbps ports, a reshuf-
ﬂer can support 1.28 Tbps or 10 billion pps.
8.2 Overhead
From our current deployment, the mirrored
Bandwidth.
trafﬁc volume is only 0.38% of the DCN trafﬁc and the aver-
age mirrored packet size is 156.4 bytes. Currently we do
not truncate packets due to the limitations of commodity
switches used in our DCN. Given 100 Tbps of DCN trafﬁc,
4Advanced packet capturing libraries like DPDK [1]
NetMap [32] and WireCap [34] can support more than
20Mpps using 8 cores.
489DCNs, its approach incurs signiﬁcant overhead to transmit
and process all trafﬁc. It may also cause congestion and hurt
ongoing applications.
In comparison, to lower overhead,
Everﬂow uses “match” to select the right trafﬁc to mirror.
Sampling. Many recent works [7, 8, 29, 33, 36] focus on
sampling network trafﬁc. NetFlow [7] and OpenSketch [36]
aggregate information (e.g., number of bytes) at ﬂow level
and do not provide ﬁne-grained, packet-level information.
sFlow [29] randomly samples packets at switches. It only
provides per-device information and cannot track packet paths
across multiple devices. Another work [8] proposes sam-
pling packets with a consistent hash function. It does not
cover what packets to sample or how to analyze sampled
packets. In contrast, Everﬂow presents a ﬂexible and rule-
based end-to-end solution that traces packets of interest across
the network and analyzes the traces in real time.
Probing. Per-packet tracing tools such as traceroute, ICMP,
and Tulip [25] can track a packet’s path in the network. A
recent work [27] proposes using IP pre-speciﬁc timestamp
option to dissect RTT, but is limited to speciﬁc portions of
the path in the Internet. These works can be classiﬁed as
out-of-band diagnosis because they use packets that are dif-
ferent from the actual application. In comparison, Everﬂow
focuses on in-band tracing.
OFRewind [35] proposes “record and replay”, which is
another way of probing. Everﬂow has a similar capabil-
ity, but it is more ﬂexible than merely replaying past traf-
ﬁc. For example, Everﬂow can instrument a probe to bounce
between switches for measuring latency.
Further, existing tools cannot trace beyond middleboxes
(e.g., load balancers and VNet gateways), which can encap-
sulate or modify packets. Everﬂow correlates packets enter-
ing and leaving middleboxes to construct full packet traces.
Fault detection. Several works [9, 10, 22, 23] detect net-
work faults by checking routing conﬁguration and policy.
They analyze routing rules and validate compliance with net-
work policy (e.g., no forwarding loops). They rely on con-
trol plane information and focus on verifying the forwarding
tables. As a result, they cannot diagnose packet level prob-
lems. Other works monitor virtual switches [26] or use oper-
ational data, such as router syslog [3, 11, 15, 30] or AS route
dissemination [13], for fault detection. They are comple-
mentary to Everﬂow because they focus on different aspects
than Everﬂow, which targets ﬁne-grained packet-level faults
based on data plane tracing.
10. CONCLUSION AND FUTURE WORK
We present Everﬂow, a scalable packet-level telemetry
system for large DCNs. Everﬂow leverages switch’s “match
and mirror” capability to capture consistent traces of pack-
ets across DCN components and injects guided probes to
actively replay packet traces. For fault debugging applica-
tions, Everﬂow provides ﬂexible query interfaces and con-
ﬁgurable APIs on top of an efﬁcient and light weight ana-
lytics pipeline. Through deployment in Microsoft’s DCNs,
Everﬂow demonstrated its utility as an indispensable tool for
troubleshooting DCN faults.
In the future, we plan to extend Everﬂow to make use of
the features offered by new programmable switching ASIC [4,
20], including hardware timestamping and packet metadata
matching. Switch timestamping will dramatically simplify
the measurement of link roundtrip latency, and packet meta-
data matching (e.g., based on checksum error or parity error)
will provide insight into the reason of packet drops. We also
plan to extend Everﬂow to the cloud provider’s wide area
network [16, 19], where, similar to DCNs, servers and net-
work trafﬁc are under its control.
Acknowledgements
We thank our Microsoft colleagues, especially Paul Wang,
Yongqiang Xiong, George Chen, Kamil Cudnik and Ahsan
Areﬁn, for their help in building and deploying Everﬂow.
We also thank the reviewers and our shepherd Geoff Voelker
for their constructive feedback. Y. Zhu, H. Zheng and B.
Y. Zhao are supported in part by NSF grants IIS-1321083,
CNS-1224100, and CNS-1317153.
11. REFERENCES
[1] Data plane development kit. http://www.dpdk.org/.
[2] Receive side scaling. https://msdn.microsoft.com/en-
us/library/windows/hardware/ff567236(v=vs.85).aspx.
[3] A. Areﬁn, A. Khurshid, M. Caesar, and K. Nahrstedt.
Scaling data-plane logging in large scale networks. In
MILCOM, 2011.
[4] P. Bosshart, G. Gibb, H.-S. Kim, G. Varghese,
N. McKeown, M. Izzard, F. Mujica, and M. Horowitz.
Forwarding metamorphosis: Fast programmable
match-action processing in hardware for SDN. In
SIGCOMM, 2013.
[5] J. Case, M. Fedor, M. Schoffstall, and J. Davin. RFC
1157: Simple network management protocol.
[6] R. Chaiken, B. Jenkins, P.-Å. Larson, B. Ramsey,
D. Shakib, S. Weaver, and J. Zhou. Scope: easy and
efﬁcient parallel processing of massive data sets.
VLDB, 2008.
[7] B. Claise. RFC 3954: Cisco systems netﬂow services
export version 9 (2004).
[8] N. G. Dufﬁeld and M. Grossglauser. Trajectory
sampling for direct trafﬁc observation. IEEE/ACM
Trans. Netw., June 2001.
[9] S. K. Fayaz and V. Sekar. Testing stateful and dynamic
data planes with ﬂowtest. In HotSDN, 2014.
[10] A. Fogel, S. Fung, L. Pedrosa, M. Walraed-Sullivan,
R. Govindan, R. Mahajan, and T. Millstein. A general
approach to network conﬁguration analysis. In NSDI,
2015.
[11] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, and
I. Stoica. X-trace: A pervasive network tracing
framework. In NSDI, 2007.
[12] R. Gandhi, H. H. Liu, Y. C. Hu, G. Lu, J. Padhye,
L. Yuan, and M. Zhang. Duet: Cloud scale load
balancing with hardware and software. In SIGCOMM,
2014.
490[13] N. Gvozdiev, B. Karp, and M. Handley. Loup: who’s
afraid of the big bad loop? In HotNets, 2012.
[14] N. Handigol, B. Heller, V. Jeyakumar, D. Mazières,
and N. McKeown. I know what your packet did last
hop: Using packet histories to troubleshoot networks.
In NSDI, 2014.
[15] C.-Y. Hong, M. Caesar, N. Dufﬁeld, and J. Wang.
Tiresias: Online anomaly detection for hierarchical
operational network data. In ICDCS, 2012.
[16] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang,
V. Gill, M. Nanduri, and R. Wattenhofer. Achieving
high utilization with software-driven WAN. In
SIGCOMM, 2013.
[17] Inﬁniband Trade Association. InﬁniBand Architecture
Volume 1, General Speciﬁcations, Release 1.2.1, 2008.
[18] Inﬁniband Trade Association. Supplement to
inﬁniband architecture speciﬁcation volume 1 release
1.2.2 annex A17: RoCEv2 (ip routable ROCE), 2014.
[19] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski,
A. Singh, S. Venkata, J. Wanderer, J. Zhou, M. Zhu,
J. Zolla, U. Hölzle, S. Stuart, and A. Vahdat. B4:
Experience with a globally-deployed software deﬁned
WAN. In SIGCOMM, 2013.
[20] V. Jeyakumar, M. Alizadeh, Y. Geng, C. Kim, and
D. Mazières. Millions of little minions: Using packets
for low latency network programming and visibility.
In SIGCOMM, 2014.
[21] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and
R. Chaiken. The nature of datacenter trafﬁc:
measurements & analysis. In IMC, 2009.
[22] P. Kazemian, M. Chan, H. Zeng, G. Varghese,
N. McKeown, and S. Whyte. Real time network policy
checking using header space analysis. In NSDI, 2013.
[23] A. Khurshid, X. Zou, W. Zhou, M. Caesar, and P. B.
Godfrey. Veriﬂow: Verifying network-wide invariants
in real time. In NSDI, 2013.
[24] T. Koponen, K. Amidon, P. Balland, M. Casado,
A. Chanda, B. Fulton, I. Ganichev, J. Gross, N. Gude,
P. Ingram, E. Jackson, A. Lambeth, R. Lenglet, S.-H.
Li, A. Padmanabhan, J. Pettit, B. Pfaff,
R. Ramanathan, S. Shenker, A. Shieh, J. Stribling,
P. Thakkar, D. Wendlandt, A. Yip, and R. Zhang.
Network virtualization in multi-tenant datacenters. In
NSDI, 2014.
[25] R. Mahajan, N. Spring, D. Wetherall, and T. Anderson.
User-level Internet path diagnosis. In SOSP, 2003.
[26] V. Mann, A. Vishnoi, and S. Bidkar. Living on the
edge: Monitoring network ﬂows at the edge in cloud
data centers. In COMSNETS, 2013.
[27] P. Marchetta, A. Botta, E. Katz-Bassett, and
A. Pescapé. Dissecting round trip time on the slow
path with a single packet. In PAM, 2014.
[28] P. Patel, D. Bansal, L. Yuan, A. Murthy, A. Greenberg,
D. A. Maltz, and R. Kern. Ananta: cloud scale load
balancing. In SIGCOMM, 2013.
[29] P. Phaal, S. Panchen, and N. McKee. RFC 3176:
Inmon corporation’s sﬂow: A method for monitoring
trafﬁc in switched and routed networks, 2001.
[30] T. Qiu, Z. Ge, D. Pei, J. Wang, and J. Xu. What
happened in my network: mining network events from
router syslogs. In IMC, 2010.
[31] J. Rasley, B. Stephens, C. Dixon, E. Rozner, W. Felter,
K. Agarwal, J. Carter, and R. Fonseca. Planck:
Millisecond-scale monitoring and control for
commodity networks. In SIGCOMM, 2014.
[32] L. Rizzo. netmap: A novel framework for fast packet
I/O. In USENIX ATC, 2012.
[33] J. Suh, T. Kwon, C. Dixon, W. Felter, and J. Carter.
Opensample: A low-latency, sampling-based
measurement platform for SDN. In ICDCS, 2014.
[34] W. Wu and P. Demar. Wirecap: a novel packet capture
engine for commodity NICs in high-speed networks.
In IMC, 2014.
[35] A. Wundsam, D. Levin, S. Seetharaman, and
A. Feldmann. OFRewind: Enabling record and replay
troubleshooting for networks. In ATC, 2011.
[36] M. Yu, L. Jose, and R. Miao. Software deﬁned trafﬁc
measurement with opensketch. In NSDI, 2013.
491