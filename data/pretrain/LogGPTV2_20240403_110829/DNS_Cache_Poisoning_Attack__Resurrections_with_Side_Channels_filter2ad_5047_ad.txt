needed packet wrapping the DNS query we received to signal that
the PMTU is lowered. Before we send it, we deliberately change the
source port of the embedded UDP packet to a different random value
to check whether the resolver will blindly accept ICMP packets
without checking the port number. After sending that forged packet,
we send another PING and check if the ICMP is accepted. If the
Session 12C: Traffic Analysis and Side Channels CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3406PING reply is not fragmented, we consider the resolver rejects the
ICMP error and thus meets 𝐶1.
T2: The existence of the next hop exception cache. To verify 𝐶2
in §5.1, ideally we would want to directly test the existence of an
exception cache. However, as described in §4.6 this will require us
to find 5 or 6 IPs that would be hashed into the same bucket, causing
the hash collision. Although it is a one-time effort, targeting every
single open resolver will require sending a large amount of traffic
which can be overly invasive. Therefore, we decide to resort to nmap
to fingerprint the OS version of the resolver and check whether the
cache exists according to the OS version discussed in §5.1. Note
that nmap may not be perfect, especially when considering backend
servers may not always have open TCP ports, through which most
of the fingerprints are extracted by nmap. Nevertheless, we can use
the distribution obtained from resolvers that do have open ports
and extrapolate to those that do not. To minimize the impact, we
sampled 20 out of 8,141 backend resolver IPs that have a valid nmap
signature and performed the collision test using 3,500 rented IPs
following the methodology described in §4.6. Note that this is still
an intrusive test (we do slow down the packet speed to about 1,000
pps to minimize any disruption) and thus cannot scale. The results
show 16 out of 20 servers support nmap’s conclusion and therefore
we estimate the accuracy of nmap 80%.
T3: The acceptance of the ICMP error. To verify 𝐶3 in §5.1, we
use a similar test to T1 but without modifying the port number
to verify if the resolver is willing to accept the ICMP packet at
all. Additionally, if there is no PING reply at all, we will send a
truncated DNS response to solicit the TCP query from the resolver.
If the MSS in the TCP header is decreased according to the PMTU
value indicated in our ICMP packet (which we verify to be the
behavior of modern Linux kernels), it also means the resolver has
accepted the PMTU value inside the ICMP packet. Besides, we will
conduct another test by changing the destination IP address in the
wrapped IP packet if we find the resolver accepts the original ICMP.
If the resolver also accepts the modified ICMP, it means its port is
open to the public, and otherwise, we consider its ephemeral port
as private-facing.
T4: The open-port status after receiving the ICMP error. To verify 𝐶4
in §5.1, after the ICMP fragment needed is sent during T3, we follow
up with a “truncated response” (if it is not sent in T3) indicating
the response is too big which will cause the resolver backend to
switch to TCP. If we observe a TCP handshake, it indicates that
the ICMP error did not cause the resolver to close the original
ephemeral port, therefore supporting the attack. In the more rare
cases, even if we did not observe any TCP connection attempt, it is
still possible that the ephemeral port is open and it is simply due to
the resolver not supporting DNS over TCP. In such cases, we will
check whether the name server will receive a retransmitted query
(with a different ephemeral port) from the resolver immediately,
which potentially indicates that the ICMP has induced the DNS
software to close the ephemeral port and transmit another query.
To distinguish between the ICMP-induced retransmission and the
timeout-induced retransmission, we record the time delay between
the ICMP transmission and the time we received the retransmitted
query. Specifically, if the delay is close to RTT, which we collect in
T1 by measuring the time delay between the PING response and
the request, i.e., within a 10% margin of difference, we consider
the retransmission to be caused by the ICMP. Otherwise, if the
delay is larger than RTT, we will consider the retransmission to be
timeout-induced (and thus still supporting the attack).
Results. Overall, out of the 156,737 backend resolver IPs that reach
our name servers, 13.85% of them are estimated to be vulnerable.
If we count by frontend resolver IPs, out of the 1.84M, 37.72% are
estimated vulnerable. This is because a large number of frontend
IPs share the same backend. To further break down the total 13.85%
vulnerable population in the backend, we find that 13,914 (8.9%)
are clearly vulnerable to public-facing port scans. However, when
we count the vulnerable population regarding the private-facing
port scans, it requires a more accurate estimate of the Linux kernel
version from nmap. Unfortunately, as mentioned earlier, we find
nmap has a relatively low success rate of OS fingerprinting: only
63.26% for IPv4 addresses and 1.06% for IPv6 addresses. We therefore
use the distribution of kernel versions observed from the 63.26%
IPv4 hosts to estimate the total vulnerable population. In particular,
within these IPv4 hosts, we find that 58.66% of them have the IPv4
exception cache only or also the IPv6 exception cache. We then
apply the 58.66% to the 13,277 resolver backends that are suspected
to be vulnerable (passing all other tests), resulting in an estimate of
7,788 backends being vulnerable to private-facing port scans.
The results indicate that the majority of the vulnerable popula-
tion is not actually running BIND. Instead, they could be running
an older Unbound, dnsmasq, or other DNS resolver software that
we have not explicitly tested. Among the servers that are not vul-
nerable, most of them are simply because they do not accept the
ICMP frag needed messages (including cases that we cannot tell)
and fail in T3.
Public Resolvers. We also highlight the results of a few well-
known public DNS services and summarize the result in Table 2.
Overall, we find 6 out of 12 to be definitely vulnerable as of the time
we performed the test, 3 in IPv4 and 3 in IPv6, including famous
providers such as OpenDNS and Quad9. Interestingly, although the
most popular DNS software BIND is not vulnerable in IPv4 in its
latest releases, there are still 3 public resolvers vulnerable in IPv4,
indicating that they are either running an older BIND version or a
different DNS software (we know Cloudflare runs Knot [2]). Note
that currently only 6 providers support IPv6 (others are marked as
N/A) and we expect more DNS services to be impacted as they start
supporting IPv6.
The most common reason for not being vulnerable is again be-
cause they failed T3, i.e., the ICMP fragment needed messages do
not appear to trigger the MTU to decrease. As we can see in Table 2,
there are still a few cases where we are unable to fingerprint the
kernel versions even after we tried testing a few custom fingerprints
in addition to nmap (marked with "?" in the T2 column). For such
cases, we simply mark them as "Possibly Vulnerable" (𝑃𝑝𝑟𝑖𝑣/𝑝𝑢𝑏)
when they pass all other tests, since it is likely their public servers
are well-maintained and using a newer Linux kernel.
6 PRACTICAL CONCERNS
In this section, we will describe a few practical considerations which
will influence the success and reliability of the attack.
Session 12C: Traffic Analysis and Side Channels CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3407Table 2: Vulnerable Status of Public Resolvers
Frontend IP
8.8.8.8
1.1.1.1
Name
Google
Cloudflare
OpenDNS
Comodo
Quad9
AdGuard
Neustar
Yandex
Baidu
114
Ali
CleanBrowsing
IPv4 Backend
IPv6 Backend
✗
✗
✗
✗
✗
?
✗
✓
✗
✓
✓
✓ ✓
✓ ✓ ✓ ✓
T1 T2 T3 T4 Vulnerable T1 T2 T3 T4 Vulnerable
✓
✓
✓ ✓ ✓ ✓
✓ ✓
✓
✗
✓
✗
✗
✓
✗
✗
✓ ✓ ✓ ✓
✓ ✓
✓
✓ ✓ ✓ ✓
✓
✓
𝑉𝑝𝑟𝑖𝑣
𝑃𝑝𝑢𝑏
✗
✗
✗
✗
𝑉𝑝𝑢𝑏
✗
𝑉𝑝𝑟𝑖𝑣
N/A
𝑉𝑝𝑢𝑏
𝑉𝑝𝑟𝑖𝑣
N/A
✗
N/A
N/A
N/A
N/A
✓
✓ ✓
✓ ✓ ✓ ✓
N/A
N/A
N/A
N/A
✓ ✓
𝑉𝑝𝑟𝑖𝑣
N/A
✗
?
✗
N/A
?
?
?
✗
✗
8.26.56.26
208.67.222.222 ✓
?
✓ ✓
✓ ✓
✓ ✓
185.228.168.168 ✓ ✓
94.140.14.14
9.9.9.9
156.154.70.1
77.88.8.1
180.76.76.76
114.114.114.114 ✓ ✓
✓ ✓
223.5.5.5
6.1 Small Attack Window
By default, the attack window is only a round trip time (ranging
from tens to hundreds of milliseconds) between a resolver and a
name server, forcing the attack to finish both the port scan and the
injection of 65,536 fake DNS responses (brute-forcing the TxID)
in a small amount of time. Nevertheless, this does not represent a
fundamental hurdle as the attacker can simply repeat the attack
multiple times; as long as one of the attempts succeeds, the cache
will be poisoned. Specifically, in practice, we find an attack attempt
more likely to succeed if the correct ephemeral port is located at the
beginning of the port scan range (see numbers in §7). To circumvent
the wait of TTL for the legitimate record to time out in case of a
failed attack attempt, we use a previously proposed method [38, 45]
to improve the speed of the attack. The basic idea is to issue queries
with random subdomains and forging a response containing an
NS record, causing the resolver to cache the wrong name server
such that all future queries (including the target domain and all
subdomains) will be directed to the malicious name server. This
method is well documented in [38, 45] and works against both
BIND and Unbound.
To increase the attack window, an attacker can attempt to mute
a name server, i.e., preventing the name server from responding
to a resolver’s query. If successful, a resolver will keep increasing
its wait time, i.e., attack window, to typically 1-2s for BIND and
potentially larger than 30s for Unbound [45]. Specifically, it was
reported that the response rate limit (RRL) feature on name servers
can be abused for this purpose [45] where 18% of the Alexa top 100k
websites were shown to be affected. Alternatively, a DoS attack can
be launched to mute the name server.
Coincidentally, one of the ICMP messages, ICMP redirect, can be
also used for name server muting. The idea is to send the malicious
ICMP redirect to either the victim resolver or the name server to
reroute the traffic destined to each other to a black hole. Since
the query/response is lost after it reaches the wrong next hop, the
victim resolver would keep the ephemeral port open for responses
until the query timeouts (can be several seconds [45]) and therefore
creates a huge attack window.
6.2 Multiple Name Servers & Backend Servers
Multiple name servers. It is also quite common for domains to
have multiple name servers. Resolvers may choose to query these
name servers in a round-robin fashion (where the order is ran-
domized). In fact, this is considered a defense against DNS cache
poisoning attacks [45]. However, this defense has little impact on
our attacks for the following reasons.
For resolvers with private-facing ephemeral ports, we can in-
fer the ports specific to different name servers simultaneously
by running multiple scanning instances. Since it is unlikely the
name servers’ IPs will share the same hash bucket given that most
second-level domains (e.g., acm.org) only have three or fewer name
servers [45], the side channels can be independently leveraged