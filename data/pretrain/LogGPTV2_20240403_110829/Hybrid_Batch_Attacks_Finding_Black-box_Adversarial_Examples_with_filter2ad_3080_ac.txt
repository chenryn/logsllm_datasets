limit to be 4000 queries for each seed. AutoZOOM sets the
default maximum query limit for each as 2000, however as we
consider a harder attacker scenario (selecting least likely class
as the target class), we decide to double the maximum query
limit. NES does not contain evaluation setups for MNIST
and CIFAR10 and therefore, we choose to enforce the same
maximum query limit as AutoZOOM.4 For ImageNet, we
set the maximum query limit as 10,000 following the default
setting used in the NES paper [21].
4.3 Attacker Goal
For MNIST and CIFAR10, we randomly select 100 images
from each of the 10 classes for 1000 total images, against
3For example, for the targeted attack on ImageNet, the baseline BanditsTD
attack only has 88% success rate and average query cost of 51,745, which
are much worse than the NES and AutoZOOM attacks.
4By running the original AutoZOOM attack with a 4000 query limit
compared to their default setting of 2000, we found 17.2% and 25.4% more
adversarial samples out of 1000 seeds for CIFAR10 and MNIST respectively.
1332    29th USENIX Security Symposium
USENIX Association
Dataset
MNIST
CIFAR10
ImageNet Normal (T)
NES
NES
AutoZOOM
Success (%)
Base Ours
98.9
89.2
7.5
5.5
98.2
99.8
65.3
38.0
98.0
100.0
AutoZOOM 91.3
77.5
7.5
4.7
AutoZOOM 92.9
98.8
AutoZOOM 64.3
38.1
AutoZOOM 95.4
100.0
NES
NES
NES
Queries/Search
Queries/Seed
Ours
Base
279
1,471
892
2,544
3,748
3,755
3,817
3,901
271
1,117
339
1,078
1,652
1,692
2,779
2,808
29,484
42,310
14,430
18,797
Queries/AE
Base
1,610
3,284
50,102
83,881
1,203
1,091
2,632
7,371
44,354
18,797
Base
Ours
282
3,248
1,000
8,254
83,042
49,776
69,275 164,302
2,143
1,632
3,117
9,932
45,166
19,030
276
340
2,532
7,317
30,089
14,430
Ours
770
3,376
83,806
160,625
781
934
2,997
9,977
31,174
14,939
Target
Model
Transfer
Rate (%)
Gradient
Attack
Normal (T)
62.8
Robust (U)
Normal (T)
Robust (U)
3.1
63.6
10.1
3.4
Table 3: Impact of starting from local adversarial examples (Hypothesis 1). Baseline attacks that start from the original seeds are
Base; the hybrid attacks that start from local adversarial examples are Ours. The attacks against the normal models are targeted
(T), and against the robust models are untargeted (U). The Transfer Rate is the direct transfer rate for local adversarial examples.
The Success rate is the fraction of seeds for which an adversarial example is found. The Queries/Seed is the average number of
queries per seed, regardless of success. The Queries/AE is the average number of queries per successful adversarial example
found, which is our primary metric. The Queries/Search is the average number of queries per successful AE found using the
gradient attack, excluding those found by direct transfer. Transfer attacks are independent from the subsequent gradient attacks
and hence, transfer rates are separated from the speciﬁc gradient attacks. All results are averaged over 5 runs.
which we perform all black-box attacks. For ImageNet, we
randomly sample 100 total images across all 1000 classes.
Target Class. We evaluate targeted attacks on the normal
MNIST, CIFAR10, and ImageNet models. Targeted attacks
are more challenging and are generally of more practical
interest. For the MNIST and CIFAR10 datasets, all of the
selected instances belong to one particular original class and
we select as the target class the least likely class of the original
class given a prediction model, which should be the most
challenging class to target. We deﬁne the least likely class of
a class as the class which is most frequently the class with
the lowest predicted probability across all instances of the
class. For ImageNet, we choose the least likely class of each
image as the target class. For the robust models for MNIST
and CIFAR10, we evaluate untargeted attacks as these models
are designed to resist untargeted attacks [30, 31]. Untargeted
attacks against these models are signiﬁcantly more diﬃcult
than targeted attacks against the normal models.
Attack Distance Metric and Magnitude. We measure the
perturbation distance using L∞, which is the most widely
used attacker strength metric in black-box adversarial exam-
ples research. Since the AutoZOOM attack is designed for
L2 attacks, we transform it into an L∞ attack by clipping the
attacked image into the -ball (L∞ space) of the original seed
in each optimization iteration. Note that the original Auto-
ZOOM loss function is deﬁned as f (x) + c· δ(x), where f (x)
is for misclassiﬁcation (targeted or untargeted) and δ(x) is for
perturbation magnitude minimization. In our transformation
to L∞-norm, we only optimize f (x) and clip the to L∞-ball of
the original seed. NES is naturally an L∞ attack. For MNIST,
we choose  = 0.3 following the setting in Bhagoji et al. [4].
For CIFAR10, we set  = 0.05, following the same setting
in early version of NES paper [21]. For ImageNet, we set
 = 0.05, as used by Ilyas et al. [21].
4.4 Local Candidates Results
We test the hypothesis that local models produce useful can-
didates for black-box attacks by measuring the mean cost to
ﬁnd an adversarial example starting from both the original
seed and from a candidate found using the local ensemble. All
experiments are averaged over 5 runs to obtain more stable
results. Table 3 summarizes our results.
In nearly all cases, the cost is reduced by starting from the
candidates instead of the original seeds, where candidates are
generated by attacking local ensemble models. We measure
the cost by the mean number of queries to the target model
per adversarial example found. This is computed by dividing
the total number of model queries used over the full attack
on 1,000 (MNIST, CIFAR10) or 100 (ImageNet) seeds by the
number of successful adversarial examples found. The overall
cost is reduced by as much as 81% (AutoZOOM attack on the
normal MNIST model), and for both the AutoZOOM and for
NES attack methods we see the cost drops by at least one third
for all of the attacks on normal models (the improvements
for robust models are not signiﬁcant, which we return to in
Section 4.5). The cost drops for two reasons: some candidates
transfer directly (which makes the query cost for that seed 1);
others do not transfer directly but are useful starting points
for the gradient attacks. To further distinguish the two factors,
we include the mean query cost for adversarial examples
USENIX Association
29th USENIX Security Symposium    1333
Target Model
Transfer Rate (%)
Normal-3 Robust-2
Normal
Robust
63.6
10.1
18.4
40.7
Gradient
Attack
AutoZOOM
NES
AutoZOOM
NES
98.2
99.8
65.3
38.0
95.3
99.4
68.7
45.2
77.1
68.9
3.8
0.7
Hybrid Success (%)
Fraction Better (%)
Normal-3 Robust-2 Normal-3 Robust-2 Normal-3 Robust-2
Cost Reduction (%)
35.7
31.2
20.5
32.1
98.6
95.6
73.1
85.0
87.0
80.6
95.5
97.1
Table 4: Attack performance of all normal and all robust local ensembles on CIFAR10 target models. The Normal-3 ensemble
is composed of the three normal models, NA, NB, and NC; the Robust-2 ensemble is composed of R-DenseNet and R-ResNet.
Results are averaged over 5 runs. Local model transfer rates are independent from the black-box attacks, so we separate transfer
rate results from the black-box attack results.
found from non-transfering seeds as the last two columns in
Table 3. This reduction is signiﬁcant for all the attacks across
the normal models, up to 76% (AutoZOOM attack on normal
MNIST models).
The hybrid attack also oﬀers success rates higher than the
gradient attacks (and much higher success rates that transfer-
only attacks), but with query cost reduced because of the
directly transferable examples and boosting eﬀect on gradient
attacks from non-transferable examples. For the AutoZOOM
and NES attacks on normally-trained MNIST models, the
attack failure rates drop dramatically (from 8.7% to 1.1% for
AutoZOOM, and from 22.5% to 10.8% for NES), as does
the mean query cost (from 1,610 to 282 for AutoZOOM, and
from 3,284 to 1,000 for NES). Even excluding the direct
transfers, the saving in queries is signiﬁcant (from 3,248 to
770 for AutoZOOM, and from 8,254 to 3,376 for NES). The
candidate starting points are nearly always better than the
original seed. For the two attacks on MNIST, there were only
at most 28 seeds out of 1,000 where the original seed was
a better starting point than the candidate; the worst result is
for the AutoZOOM attack against the robust CIFAR10 model
where 269 out of 1,000 of the local candidates are worse
starting points than the corresponding original seed.
4.5 Attacking Robust Models
The results in Table 3 show substantial improvements from
hybrid attacks on normal models, but fail to provide improve-
ments against the robust models. The improvements against
robust models are less than 4% for both attacks on both tar-
gets, except for NES against MNIST where there is ∼17%
improvement. We speculate that this is due to diﬀerences in
the vulnerability space between normal and robust models,
which means that the candidate adversarial examples found
against the normal models in the local ensemble do not pro-
vide useful starting points for attacks against a robust model.
This is consistent with Tsipras et al.’s ﬁnding that robust
models for image classiﬁcation tasks capture key features
of images while normal models capture relatively noisy fea-
tures [42]. Because of the diﬀerences in extracted features,
adversarial examples against robust models require perturbing
key features (of the target domain) while adversarial examples
can be found against normal models by perturbing irrelevant
features. This would explain why we did not see improve-
ments from the hybrid attack when targeting robust models.
To validate our hypothesis on the diﬀerent attack surfaces,
we repeat the experiments on attacking the CIFAR10 robust
model but replace the normal local models with robust local
models, which are adversarially trained DenseNet and ResNet
models mentioned in Section 4.1.5
Table 4 compares the direct transfer rates for adversarial
example candidates found using ensembles of normal and
robust models against both types of target models. We see
that using robust models in the local ensemble increases the
direct transfer rate against the robust model from 10.1% to
40.7% (while reducing the transfer rate against the normal
target model). We also ﬁnd that the candidate adversarial
examples found using robust local models also provide better
starting points for gradient black-box attacks. For example,
with the AutoZOOM attack, the mean cost reduction with
respect to the baseline mean query (2,632) is signiﬁcantly
improved (from 3.8% to 20.5%). We also observe a signiﬁcant
increase of fraction better (percentages of seeds that starting
from the local adversarial example is better than starting from
the original seed) from 73.1% to 95.5%, and a slight increase
in the overall success rate of the hybrid attack (from 65.3%
to 68.7%). When an ensemble of robust local models is used
to attack normal target models, however, the attack eﬃciency
degrades signiﬁcantly, supporting our hypothesis that robust
and normal models have diﬀerent attack surfaces.