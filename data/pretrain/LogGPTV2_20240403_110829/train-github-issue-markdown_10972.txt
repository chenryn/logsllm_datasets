I'm running into an issue where some of my worker processes will stop
processing new tasks, causing them to pile up on the queue server. Left alone
it will sometimes clear, and sometimes grow to impact all workers, effectively
blocking processing.
However, if I issue a management command, such as "celery inspect scheduled"
it will cause workers to resume operation immediately. Although sometimes only
briefly before halting again.
**The setup:**
  * celery 3.1.9
  * kombu 3.0.12
  * amqp 1.4.4
  * RabbitMQ (3.1.3) / ampq for all communication and results.
  * Mid-sized config: 7 worker nodes, 13 queues for service differentiation, and about 40 total worker processes per worker node.
  * Nodes are running the prefork worker, with 1-8 processes per queue, depending on the queue.
  * CELERYD_PREFETCH_MULTIPLIER = 1
  * CELERY_ACKS_LATE = True
  * CELERYD_MAX_TASKS_PER_CHILD = 16
  * I'm using -Ofair, as even within a given queue some tasks may take very different times to execute. Overall task per second throughput has never been an issue.
  * Example command line: 
> python /home/celeryworker/web/manage.py celery worker --without-gossip
> --loglevel=INFO -Q transcode -Ofair -c 2 -n celery-worker-7-prod_transcode
I believe it may have to do with scheduled tasks, as everytime I've been able
to catch it in action there have been a fairly large number of tasks waiting
for a countdown to expire. On the order of the total number of task slots
available for that queue (I'm counting 7 workers, listening with -c4 on a
queue, and a prefetch multiplier of 1 to be 28 task slots)
I have not been able to confirm if the rabbitmq channel prefetch value is
being correctly incremented when this issue is occurring and there are
scheduled tasks; though it is being correctly incremented by scheduled tasks
during normal operation. (I'll double check that the next time this recurs.)
In general the above config is doing exactly what I expect; one task per
worker process, multiple tasks per node, and the next task coming out of the
queue as soon as any given worker is no longer busy. It's only occasionally
that things go off the rails and, one node at a time, new tasks will no longer
be executed. (Other processes listening on other queues on that node will
continue to work, or fail, independently.)