n
m
9
5
3
.
i
c
d
b
l
i
.
0
6
3
i
m
w
s
.
3
6
3
.
t
b
0
7
3
e
g
a
r
e
v
A
Fig. 2. Comparison of exact and approximate proﬁling for transient faults.
Relative Outcomes Due to Permanent Faults
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
l
i
c
n
e
t
s
o
3
0
3
.
l
m
b
o
4
0
3
.
.
d
m
0
5
3
q
i
r
.
m
o
4
1
3
l
m
a
p
1
5
3
.
p
e
.
2
5
3
f
a
e
l
r
v
l
c
.
3
5
3
g
c
.
4
5
3
c
i
m
s
i
e
s
.
5
5
3
p
s
.
6
5
3
p
s
c
.
7
5
3
c
d
b
l
i
.
0
6
3
i
m
w
s
.
3
6
3
.
t
b
0
7
3
e
g
a
r
e
v
A
i
t
s
o
h
G
n
m
9
5
3
.
i
SDC
DUE Masked
Fig. 3. Relative outcomes for permanent faults.
faults
faults. NVBitFI supports permanent
user will have to consider the trade-off between proﬁling
accuracy and proﬁling time.
Permanent
based on the fault model described above. Figure 3 shows the
relative occurrence of SDC, DUE, and masked outcomes for
permanent faults. For each program, 171 runs were conducted
with one opcode out of the possible 171 opcodes selected for
injection for each run. The outcome of each run is weighted
based on the relative number of dynamic instructions for that
opcode. For example, if injections into the FADD instruction
results in an SDC and accounts for 10% of all program
instructions and FMUL results in a DUE and accounts for
20% of all program instructions, then the DUE outcome would
be weighted twice that of the SDC outcome. This weighting
reﬂects the greater likelihood that the FMUL instruction is
executed and activates the permanent fault.
Compared to transient faults, the permanent faults for our
benchmark programs result in more SDC and DUE outcomes.
Masked outcomes constitute 57.6% for transient faults but
only 17.4% for permanent faults. This result is intuitive as
permanent faults are activated multiple times and result in
greater error propagation, leading to more SDCs and DUEs.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:22 UTC from IEEE Xplore.  Restrictions apply. 
289
Normalized NVBitFI Overheads
Relative to No-FI
1000
100
10
1
l
i
c
n
e
t
s
o
3
0
3
.
l
m
b
o
4
0
3
.
.
d
m
0
5
3
q
i
r
.
m
o
4
1
3
l
m
a
p
1
5
3
.
p
e
.
2
5
3
f
a
e
l
r
v
l
c
.
3
5
3
g
c
.
4
5
3
c
i
m
s
i
e
s
.
5
5
3
p
s
.
6
5
3
p
s
c
.
7
5
3
c
d
b
l
i
.
0
6
3
i
m
w
s
.
3
6
3
.
t
b
0
7
3
e
g
a
r
e
v
A
i
t
s
o
h
G
n
m
9
5
3
.
i
Exact-Profiling
Approx-Profiling
Transient
Permanent
In a testing campaign,
transient error experiments must
be run enough times to be statistically signiﬁcant, while
permanent fault injection experiments must be run once for
each opcode. Furthermore, permanent fault experiments can be
skipped for unused opcodes, further simplifying the campaign.
Figure 5 illustrates aggregate campaign times for transient
campaigns with 100 faults and permanent campaigns that
leverage a proﬁle to identify unused opcodes. The number
of executed opcodes for our programs ranges from 16 to 41
opcodes per program (out of the total possible 171 opcodes).
For these applications, the transient campaigns typically take
about twice the time as the permanent campaigns, although
the transient campaign can take as much a 5× longer or be
slightly faster.
Fig. 4. Execution overheads.
V. SUMMARY AND FUTURE DIRECTIONS
Total Campaign Times for Transient and Permanent Faults
(for 100 Transient Fault and Only Executed Opcodes)
80
70
60
50
40
30
20
10
0
)
s
e
t
u
n
m
i
i
(
e
m
T
n
g
a
p
m
a
C
i
l
a
t
o
T
Total transient time
Total permanent time
Fig. 5. Total campaign times (assuming 100 transient faults).
C. Benchmark Performance Overheads
Figure 4 shows the overheads for proﬁling and injection,
relative to the runtime of an uninstrumented program. Proﬁling
need only be performed once per application to characterize
the set of injection sites. The overhead for exact proﬁling can
be quite large (as much as 558× for 350.md), especially if the
instrumentation causes GPU registers to be spilled to memory.
On average, our programs demonstrate an exact proﬁling
overhead that is 28× more than approximate proﬁling. If the
exact proﬁling time is unacceptable, approximate proﬁling
offers an attractive alternative.
In contrast to proﬁling, injection experiments must be per-
formed many times to obtain statistical conﬁdence. For these
programs, injection times can vary depending on the amount of
instrumentation added to a program and the number of times
that instrumentation is executed. The injection times in Fig-
ure 4 are the median from the set of 100 injection experiments
for each program and fault type. For our programs, transient
fault injection slows down program execution by about 2.9×,
while permanent fault injection slows down program execution
by about 4.8× on average.
We have presented the NVBitFI tool for dynamic fault
injection into GPUs. The tool is built using the NVBit dy-
namic binary instrumentation framework and therefore offers
a convenient way to conduct a fault injection campaign into
GPUs without having to know many details about the GPU.
Furthermore, no source code for the target program is required,
which not only simpliﬁes the user’s job but also allows faults
to be injected into dynamic libraries for which no source
code is available. Porting the tool to non-Nvidia GPUs would
require (a) a binary instrumentation tool on the GPU and (b)
porting the handlers from CUDA to something like OpenCL
or employing a translator (e.g., HIP [26]) to perform the
translation.
Using the NVBit framework, NVBitFI can limit instru-
mentation needed for fault injection to the dynamic instance
of the target kernel. Non-target instances of the same static
kernel execute unmodiﬁed, thus minimizing the performance
overhead of the injection code.
NVBitFI currently supports a transient and a simple perma-
nent fault model. We are considering extensions of the fault
model, including the following:
• Intermittent faults. The permanent fault model corrupts
the destination register of every dynamic instruction of
a particular opcode. An intermittent fault model would
inject into only a subset of those instructions. The subset
can be speciﬁed as a random, bursty process.
• More complex fault models. Our current fault models
can be extended to provide additional ﬂexibility in spec-
ifying fault parameters, including (1) corrupting multiple
registers, (2) supporting corruption functions beyond the
current set of XOR, random, and zero functions, (3)
conditioning error effects on the speciﬁc opcode, and (4)
allowing a permanent fault to affect multiple opcodes.
• Fault dictionary. A fault dictionary based on microar-
chitectural simulation or an analytical model is a speciﬁc
example of a more complex fault models. A fault dictio-
nary might be useful when a complex fault model is not
easily characterized by a set of parameters.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:22 UTC from IEEE Xplore.  Restrictions apply. 
290
[22] S. Jha, S. Banerjee, T. Tsai, S. K. S. Hari, M. B. Sullivan, Z. T.
Kalbarczyk, S. W. Keckler, and R. K. Iyer, “ML-Based Fault Injection
for Autonomous Vehicles: A Case for Bayesian Fault Injection,” in
Dependable Systems and Networks (DSN), 2019.
[23] G. Juckeland, W. C. Brantley, S. Chandrasekaran, B. M. Chapman,
S. Che, M. E. Colgrove, H. Feng, A. Grund, R. Henschel, W. mei
W. Hwu, H. Li, M. S. M¨uller, W. E. Nagel, M. Perminov, P. Shelepugin,
K. Skadron, J. A. Stratton, A. Titov, K. Wang, G. M. van Waveren,
B. Whitney, S. Wienke, R. Xu, and K. Kumaran, “SpecACCEL:
A Standard Application Suite for Measuring Hardware Accelerator
Performance,” in International Workshop on Performance Modeling,
Benchmarking and Simulation of High Performance Computer Systems
(PMBS), 2014.
[24] B. Nie, L. Yang, A. Jog, and E. Smirni, “Fault Site Pruning for
Practical Reliability Analysis of GPGPU Applications,” in International
Symposium on Microarchitecture (MICRO), 2018.
[25] B. Nie, A. Jog, and E. Smirni, “Characterizing Accuracy-Aware Re-
silience of GPGPU Applications,” in IEEE/ACM International Sympo-
sium on Cluster Computing and the Grid (CCGRID), 2020.
Introducing AMD CDNA Architecture - The All-
the Modern Era of HPC
https://www.amd.com/system/ﬁles/documents/amd-cdna-
for
[26] “Whitepaper:
New AMD GPU Architecture
&
whitepaper.pdf/, accessed: 2021-03-27.
AI,”
REFERENCES
[1] “TOP500 LIST - NOVEMBER 2020,” https://www.top500.org/lists/
top500/list/2020/11/, accessed: 2021-03-27.
[2] “GREEN500 LIST - NOVEMBER 2020,” https://www.top500.org/lists/
green500/list/2020/11/, accessed: 2021-03-27.
[3] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin,
“A Systematic Methodology to Compute the Architectural Vulnerabil-
ity Factors for a High-Performance Microprocessor,” in International
Symposium on Microarchitecture (MICRO), 2003, pp. 29–40.
[4] S. K. S. Hari, T. Tsai, M. Stephenson, S. W. Keckler, and J. Emer,
“SASSIFI: An Architecture-level Fault Injection Tool for GPU Applica-
tion Resilience Evaluation,” in International Symposium on Performance
Analysis of Systems and Software (ISPASS), 2017.
[5] G. Li, K. Pattabiraman, C.-Y. Cher, and P. Bose, “Understanding Error
Propagation in GPGPU Applications,” in International Conference on
High Performance Computing, Networking, Storage and Analysis (SC),
2016.
[6] B. Fang, K. Pattabiraman, M. Ripeanu, and S. Gurumurthi, “GPU-
Qin: A Methodology for Evaluating the Error Resilience of GPGPU
Applications,” in International Symposium on Performance Analysis of
Systems and Software (ISPASS), 2014.
[7] K. S. Yim, C. Pham, M. Saleheen, Z. Kalbarczyk, and R. Iyer, “Hauberk:
Lightweight Silent Data Corruption Error Detector for GPGPU,” in In-
ternational Symposium on Parallel and Distributed Processing (IPDPS),
2011.
[8] “NVIDIA CUDA Binary Utilities,” https://docs.nvidia.com/cuda/cuda-
binary-utilities/index.html, accessed: 2021-03-27.
[9] O. Villa, M. Stephenson, D. Nellans, and S. W. Keckler, “NVBit: A
Dynamic Binary Instrumentation Framework for NVIDIA GPUs,” in
International Symposium on Microarchitecture (MICRO), 2019.
[10] “Parallel Thread Execution ISA Version 7.1,” https://docs.nvidia.com/
cuda/parallel-thread-execution/, accessed: 2021-03-27.
[11] M. Stephenson, S. K. S. Hari, Y. Lee, E. Ebrahimi, D. R. Johnson,
D. Nellans, M. O’Conner, and S. W. Keckler, “Flexible Software Pro-
ﬁling of GPU Architectures,” in International Symposium on Computer
Architecture (ISCA), 2015.
[12] “CUDA-GDB,” https://developer.nvidia.com/cuda-gdb, accessed: 2021-
03-27.
[13] A. Vallero, D. Gizopoulos, and S. D. Carlo, “SIFI: AMD Southern
Islands GPU Microarchitectural Level Fault Injector,” in International
Symposium on On-Line Testing and Robust System Design (IOLTS),
2017.
[14] R. Ubal, B. Jang, P. Mistry, D. Schaa, and D. Kaeli, “Multi2Sim: A
Simulation Framework for CPU-GPU Computing,” in International Con-
ference on Parallel Architectures and Compilation Techniques (PACT),
2012.
[15] S. Tselonis and D. Gizopoulos, “GUFI: A Framework for GPUs Reliabil-
ity Assessment,” in International Symposium on Performance Analysis
of Systems and Software (ISPASS), 2016.
[16] J. Tan, N. Goswami, T. Li, and X. Fu, “Analyzing Soft-error Vulner-
ability on GPGPU Microarchitecture,” in International Symposium on
Workload Characterization (IISWC), 2011.
[17] “CUDA Parallel Computing Platform,” https://developer.nvidia.com/
cuda-zone, accessed: 2021-03-27.
[18] A. Bakhoda, G. L. Yuan, W. W. L. Fung, H. Wong, and T. M. Aamodt,
“Analyzing CUDA Workloads Using a Detailed GPU Simulator,” in
International Symposium on Performance Analysis of Systems and
Software (ISPASS), 2009.
[19] M. Wilkening, V. Sridharan, S. Li, F. Previlon, S. Gurumurthi, and D. R.
Kaeli, “Calculating Architectural Vulnerability Factors for Spatial Multi-
Bit Transient Faults,” in International Symposium on Microarchitecture
(MICRO), 2014.
[20] H. Jeon, M. Wilkening, V. Sridharan, S. Gurumurthi, and G. H. Loh,
“Architectural Vulnerability Modeling and Analysis of Integrated Graph-
ics Processors,” in Workshop on Silicon Errors in Logic–System Effects
(SELSE), 2013.
[21] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu,
J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, , R. Sen, K. L.
Sewell, A. Muahammad, N. Vaish, M. D. Hill, and D. A. Wood, “The
gem5 Simulator,” ACM SIGARCH Computer Architecture News, vol. 39,
no. 2, pp. 1–7, 2011.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:22 UTC from IEEE Xplore.  Restrictions apply. 
291