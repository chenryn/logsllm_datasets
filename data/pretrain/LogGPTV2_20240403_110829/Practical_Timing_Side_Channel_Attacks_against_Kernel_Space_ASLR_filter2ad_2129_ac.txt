hardware, also the caches have to be shared to some extent.
As mentioned above, our approach is based on two key opera-
tions: (a) set the system into a speciﬁc state and (b) measure the
duration of a certain memory access operation. Further, these
two operations are performed for each single memory address
that is probed. Finally, the complete experiment is repeated
multiple times until consistent values have been collected. While
it is now possible — and highly probable — that our code is
interrupted many times while probing the complete memory, it
is also very likely that the low-level two step test operations can
be executed without interruption. The mean duration of these
two steps depends on the testing method we perform, but even
in the worst case it takes no more than 5,000 clock cycles.
Since modern operating systems have time slices of at least
several milliseconds [38], [39], it is highly unlikely that the
scheduler interferes with our measurements. Accordingly, while
there may be much noise due to permanent interruption of our
experiments, after a few iterations we will eventually be able
to test each single memory address without interruption. This
is sufﬁcient since we only need minimal measurement values,
i.e., we only need one measurement without interruption.
IV. IMPLEMENTATION AND RESULTS
We now describe three different implementations of timing
side channel attacks that can be applied independently from
each other. The goal of each attack is to precisely locate some
of the currently loaded kernel modules from user mode by
measuring the time needed for certain memory accesses. Note
that an attacker can already perform a ROP-based attack once
she has derandomized the location of a few kernel modules or
the kernel [35], [36].
Depending on the randomness created by the underlying
ASLR implementation, the ﬁrst attack might still require partial
information on the location for the kernel area. For the Windows
ASLR implementation (see Section II-A), this is not the case
since only 64 slots are possible of the kernel. The ﬁrst attack
requires either the presence of two large pages or the knowledge
of the physical address of a single page in user space. Our
second attack has no requirements. However, due to the way
the AMD CPU that we used during testing behaves in certain
situations, this attack could not be mounted on this speciﬁc
CPU. The third attack has no requirements at all.
We have evaluated our implementation on the 32-bit and 64-
bit versions of Windows 7 Enterprise and Ubuntu Desktop 11.10
on the following (native and virtual) hardware architectures
to ensure that they are commonly applicable on a variety of
platforms:
1) Intel i7-870 (Nehalem/Bloomﬁeld, Quad-Core)
2) Intel i7-950 (Nehalem/Lynnﬁeld, Quad-Core)
3) Intel i7-2600 (Sandybridge, Quad-Core)
4) AMD Athlon II X3 455 (Triple-Core)
5) VMWare Player 4.0.2 on Intel i7-870 (with VT-x)
Table I provides a high-level overview of our methods, their
requirements, and the obtained results. We implemented an
exploit for each of the three attacks.
For the sake of simplicity, all numbers presented in the re-
mainder of this section were taken using Windows 7 Enterprise
32-bit. Note that we also performed these tests on Windows 7
64-bit and Ubuntu Desktop (32-bit and 64-bit) and can conﬁrm
that they work likewise. The Ubuntu version we used did not
employ kernel space ASLR yet, but we were able to determine
the location of the kernel image from user space. In general,
196
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:20 UTC from IEEE Xplore.  Restrictions apply. 
this does not make any difference since the attacks also would
have worked in the presence of kernel space ASLR.
In the following subsections, we explain the attacks and
discuss our evaluation results.
A. First Attack: Cache Probing
Our ﬁrst method is based on the fact that multiple memory
addresses have to be mapped into the same cache set and,
thus, compete for available slots. This can be utilized to infer
(parts of) virtual or physical addresses indirectly by trying
to evict them from the caches in a controlled manner. More
speciﬁcally, our method is based on the following steps: ﬁrst, the
searched code or data is loaded into the cache indirectly (e.g.,
by issuing an interrupt or calling sysenter). Then certain
parts of the cache are consecutively replaced by accessing
corresponding addresses from a user-controlled eviction buffer,
for which the addresses are known. After each replacement,
the access time to the searched kernel address is measured, for
example by issuing the system call again. Once the measured
time is signiﬁcantly higher, one can be sure that the previously
accessed eviction addresses were mapped into the same cache
set. Since the addresses of these colliding locations are known,
the corresponding cache index can be obtained and obviously
this is also a part of the searched address.
Several obstacles have to be addressed when performing
these timing measurements in practice. First, the correct kind
of memory access has to be performed: higher cache levels are
uniﬁed (i.e., there are no separate data and instruction caches),
but on lower levels either a memory read/write access or an
execution has to be used in order to affect the correct cache
type. Second, accessing the colliding addresses only once is not
enough. Due to the Pseudo-LRU algorithm it may happen that
not the searched address is evicted, but one from the eviction
buffer. Therefore, it is necessary to access each of the colliding
addresses twice. Note that it is still possible that code within
another thread or on other CPUs concurrently accesses the
search address in the meantime, setting its reference bit that way.
To overcome this problem, all tests have to be performed several
times to reduce the inﬂuence of potential measuring errors and
concurrency.
More serious problems arise due to the fact that the cache
indexes on higher levels are taken from the physical instead of
the virtual addresses. In our experiments, the eviction buffer is
allocated from user mode and, hence, only its virtual address is
known. While it is still possible to locate the colliding cacheset,
no information can be gathered about the corresponding physical
addresses. In general, even if the physical address of the
searched kernel location is known, this offers no knowledge
about its corresponding virtual address. However, the relevant
parts of the virtual and physical address are identical for the
kernel region of Windows (see Section II-A). Hence, all the
relevant bits of the virtual address can be obtained from the
physical address.
Cache probing with the latest Intel CPUs based on the
Sandybridge [30] architecture is signiﬁcantly harder, even if
the attacker has a contiguous region of memory for which all
corresponding physical addresses are known. These processors
employ a distributed last level cache [30] that is split into
equally sized cache slices and each of them is dedicated to
one CPU core. This approach increases the access bandwidth
since several L3 cache accesses can be performed in parallel. In
order to uniformly distribute the accesses to all different cache
slices, a hash function is used that is not publicly documented.
We thus had to reconstruct this hash function in a black-box
manner before cache probing can be performed, since otherwise
it is unknown which (physical) addresses are mapped into which
cache location. We explain our reverse-engineering approach
and the results in a side note before explaining the actual
evaluation results for our ﬁrst attack.
1) Side Note: Sandybridge Hash Function: In order to re-
construct the Sandybridge hash function, we utilized the Intel
i7-2600 processor. This CPU has an 8 MB L3 cache and 4 cores,
resulting in 2 MB L3 slices each. Hence, the hash function has
to decide between 4 different slices (i.e., resulting in 2 output
bits). Since our testing hardware had 4 GB of physical memory,
we have reconstructed the hash function for an input of 32 bits.
In case of larger physical memory, the same method can be
applied to reverse the inﬂuence of the upper bits as well.
We started with the reasonable assumption that L3 cachelines
on this CPU still consist of 64 bytes. Hence,
the lowest
6 bits of each address operate as an offset and, therefore, do
not contribute as input to the hash function. Accordingly, we
assumed a function h : {0, 1}32−6 → {0, 1}2.
In order to learn the relationship between the physical ad-
dresses and the resulting cache slices, we took one arbitrary
memory location and an additional eviction buffer of 8 MB
and tried to determine the colliding addresses within (i.e., those
which are mapped into the same cacheset of the same cache
slice). Since the L3 cache operates on physical addresses, the
eviction buffer had to be contiguous. Therefore, we used our
own custom driver for this experiment.
Performance optimization features of modern CPUs like hard-
ware prefetching, speculative execution, and branch prediction
make it
impossible to directly identify single colliding ad-
dresses. Therefore, we performed a two-step experiment: (1) we
identiﬁed eviction buffer regions of adjacent memory addresses
that collide with the probe address and then (2) we located
the particular colliding addresses within these regions. We
have performed these tests several hundred times with different
physical addresses in order to gain variety in the test data. As a
result of each single test we got a tuple (p, CA = {ca1, ca2, ...})
whereas p is the used probe address and each cai is a colliding
address from our eviction buffer. By manually comparing those
tuples (p, CA) and (p’, CA’) with a hamming distance of one
between p and p’, we were able to learn the inﬂuence of
particular bits on the colliding addresses from CA and CA’.
In the end we were able to fully reconstruct the hashing
function h that decides which cache slice is used for a given
197
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:20 UTC from IEEE Xplore.  Restrictions apply. 
Input to hash function for slice index 
Cache index 
Block offset 
3
1 
3
0 
2
9 
2
8 
2
7 
2
6 
2
5 
2
4 
2
3 
2
2 
2
1 
2
0 
1
9 
1
8 
1
7 
1
6 
1
5 
1
4 
1
3 
1
2 
1
1 
1
0 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
PA 
h1 =  
h2 =  
h1 = b31 ⊕ b30 ⊕ b29 ⊕ b27 ⊕ b25 ⊕ b23 ⊕ b21 ⊕ b19 ⊕ b18
h2 = b31 ⊕ b29 ⊕ b28 ⊕ b26 ⊕ b24 ⊕ b23 ⊕ b22 ⊕ b21 ⊕ b20 ⊕ b19 ⊕ b17
h = (h1, h2)
Figure 3. Results for the reconstruction of the undocumented Sandybridge
hash function
3
1 
3
0 
2
9 
2
8 
2
7 
2
6 
2
5 
2
4 
2
3 
2
2 
2
1 
2
0 
1
9 
1
8 
1
7 
1
6 
1
5 
1
4 
1
3 
1
2 
1
1 
1
0 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
Kernel Base VA 
kernel_region base address 
randomized 
=
zero 
 Kernel PA/VA 
identical for virtual and physical address (in kernel_region) 
PA 
tag 
=
L3 cache index 
cacheline 
Figure 4. Correlation of different memory addresses
address. It turned out that only the bits 31 to 17 are considered
as input values. Each cache slice operates as a separate smaller
2 MB cache, whereas the address bits 16 to 6 constitute as the
cache index (11 bits are necessary to address all sets of such
a 2 MB cache). Figure 3 shows how the 32 bits of a physical
address are used as inputs to the hash function, cache index,
and block offset.
2) Evaluation Results: We evaluated cache probing on all
of our testing machines. We assume that the base address of
the kernel region (see kernel base from Section II-A) is
known. This is a reasonable assumption in practice since this
information can be reliably extracted using the method presented
in Section IV-B. In Windows this address actually is constant
for a particular system.
Figure 4 shows the correlation of the different parts of
the virtual and physical address inside the kernel region.
In essence, bits 16 to 12 of the kernel’s base address are