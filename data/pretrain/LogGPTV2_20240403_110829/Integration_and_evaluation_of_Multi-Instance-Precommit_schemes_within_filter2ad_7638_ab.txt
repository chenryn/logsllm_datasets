tory of committed tuple versions is used to retrieve the most
recent tuple version committed by a transaction not con-
current with Ti (i.e. the version having maximum t xmin
value among the versions created by committed transactions
not concurrent with Ti). It follows that the selected tuple
might correspond to a version older than the valid one. On
the other hand, if the read request is for a tuple previously
written by Ti, it is satis(cid:2)ed by accessing the active version
previously created by Ti itself.
Write - upon write access to a tuple by transaction Ti, the
following version checks are performed: (1) If the valid ver-
sion was created by a transaction concurrent with Ti (i.e.
t xmin on the valid version is the TID of a transaction
concurrent with Ti), the abort of Ti is immediately forced.
Otherwise, there are two cases: (2.A) There is currently no
active version of the tuple. In this case Ti requests an ex-
clusive lock on the valid version. If the exclusive lock is
granted without any wait, Ti creates the active tuple version,
which is used for any successive access by Ti. Otherwise,
upon being woken up from the wait phase, Ti starts again
the whole version checking. (2.B) An active version of the
tuple exists. In this case Ti is queued for future access to
the exclusive write lock associated with the tuple, and the
whole version checking is repeated when Ti resumes.
Regarding the mechanism used by PostgreSQL for man-
aging exclusive locks, an in memory lock table is used to
store information on waiting transactions. Speci(cid:2)cally, the
lock table is indexed via transaction TIDs, and each entry
records the TIDs of transactions waiting for the termination
of the transaction indexing that entry. A tuple is consid-
ered locked by setting its t xmax to the TID of the lock-
ing transaction. When a transaction completes (thus releas-
ing its locks), it wakes up any transaction currently wait-
ing on its corresponding lock table entry. In this way, per-
transaction, rather than per-tuple, locking data structures are
used, hence improving scalability in the management of the
locking mechanism.
Beyond the above mechanisms for exclusive write locks,
PostgreSQL also supports shared locks, which can be re-
quested for, e.g., ensuring foreign keys integrity constraints,
or upon explicit application request. In this case, a transac-
tion waiting for the release of a shared lock may have to wait
for the termination of a set of transactions. The association
between a tuple and the TIDs of transactions holding the
shared lock on it relies on indirection mechanisms. Essen-
tially a so called MULTIXACT ID is stored within the tuple
header which is used as an indexing information to access
an external table (maintained on a disk (cid:2)le and cached in
RAM), which stores the list of TIDs associated with trans-
actions holding the shared lock.
The integration of the MIP model within PostgreSQL
led us to alter the synchronization scheme in order to regu-
late concurrent accesses to any tuple (also in write mode)
by multiple sibling transactions. This has been done by
mostly exploiting facilities already available within the
database kernel in order to allow the modi(cid:2)ed synchro-
nization scheme to ef(cid:2)ciently and non-intrusively coexist
with the native PostgreSQL concurrency control mecha-
nism, and with the treatment of non-MIP transactions. From
a methodological perspective, our solution is based on two
new lock types, which we refer to as Sibling-eXlcusive (SX)
and Sibling-Shared (SS). SX and SS locks can only be re-
quested by MIP transactions, whereas the original Shared
(S) and eXclusive (X) locks can only be requested by non-
MIP transactions. The below table shows the compatibility
of SX, SS, S and X locks:
S
Yes
No
Yes
No
X
No
No
No
No
S
X
SS
SX
SS
Yes
No
Yes
SX
No
No
Yes iff same X ID
Yes iff same X ID Yes iff same X ID
Mutual compatibility between SX locks permits multiple
sibling transactions to share the before-image of a given tu-
ple, thus allowing the spawning of multiple active versions.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE406DSN 2008: Romano & QuagliaOn the other hand, compatibility between SS and SX locks
avoids mutual blocking situations between sibling transac-
tions, which might otherwise compromise the timeliness of
the fail-over phase and would require explicit preventive
extermination of previously activated pending transactions.
Finally, standard compatibility rules apply vs S and X locks,
thus synchronizing MIP vs non-MIP transactions according
to the native scheme adopted by PostgreSQL.
In order to support SS and SX locks, two (cid:2)elds, called
XID and S MULTIXACT ID (each of 4 bytes), have been
introduced within the tuple header. The XID (cid:2)eld speci-
(cid:2)es whether the tuple valid version is currently locked by a
MIP transaction (via either SS or SX locks). In the positive
case, it also identi(cid:2)es the family of sibling transactions for
which lock compatibility, as expressed by the above table,
holds. The S MULTIXACT ID (cid:2)eld is used in differenti-
ated modes depending on the number of sibling transactions
currently locking that tuple. In case only one of those trans-
actions is active, it stores the transaction XINST forming
the MIP-TID. Otherwise, it is used as indexing informa-
tion (in a similar way to the previously discussed MULTI-
XACT ID) to retrieve from a cached paged (cid:2)le the list of
MIP-TIDs (and corresponding TIDs) of sibling transactions
currently locking the tuple. The TIDs have been placed
within that list in order to provide immediate identi(cid:2)cation
of the parameters used by the lower level locking mecha-
nism which, as discussed above, is based on a wait-for-TID
policy. Also, given that the original tuple header maintains
a single link (i.e. t ctid) for the identi(cid:2)cation of the suc-
cessive version (i.e. the active version in case of the valid
tuple), multiple links required for coexistence and retrieval
of multiple active versions associated with different sibling
transactions have been also stored within such a list. We
note that this external data structure does not need to be allo-
cated in case of normal behavior (i.e. no failure, or suspect
of failure in the execution of the original MIP transaction).
3.3. Precommit and Commit Phases
As pointed out in Section 2, the management of the pre-
commit/commit phase of MIP transactions shows clear dif-
ferences when compared to a conventional approach. These
are mainly due to the fact that precommit logs must keep
track of the (possible) precommit state of a family of sib-
ling transactions. Also, the commit log must keep track
of which one among the prepared sibling transactions has
been eventually committed, as a result of the reconciliation
scheme. This has required the development of an ad-hoc
subsystem within PostgreSQL kernel, which is based on
the MIP-Table (MIPT) data structure (see Section 2) as the
base to address the previous issues. MIPT management has
been non-intrusively integrated with typical kernel activities
supporting generation and synchronous write of the Write-
Ahead-Logs (WAL) [3].
Below we (cid:2)rst describe the organization of the MIPT
data structure and then provide insights on the re-
lated management activities.
As a preliminary ob-
servation, similarly to the BEGIN MIP statement, we
have extended the SQL command set in order to sup-
port both prepare and commit requests for MIP transac-
tions. Speci(cid:2)cally, PREPARE MIP 
’request string’ ’result string’ can be used
to request the database to precommit the MIP transaction
associated with a speci(cid:2)c MIP-TID, to atomically register
the associated result and request strings within the corre-
sponding MIPT entry, and to return the updated MIPT to
the transaction coordinator. We recall again that, by explicit
design choice, MIP-TIDs are selected by the overlying ap-
plication (i.e. via the BEGIN MIP statement - see Section
3.1), in a way to support mechanisms for correlating a spe-
ci(cid:2)c request string with a family of transactions associated
with a same XID. The usage of that family identi(cid:2)er in the
prepare phase is re(cid:3)ected in a (cid:2)nal association between the
request string and the precommit log of that sibling transac-
tions family. As hinted, this can even support database side
retransmission activities (e.g. via a proper stub) in order to
further speedup fail-over and increase data availability via
prompt release of precommit locks. Analogously, the SQL
command COMMIT MIP  was intro-
duced to support the (cid:2)nal commitment of the MIP trans-
action representing the reconciliated execution path within
that family, and to simultaneously request the abort of any
other active or precommitted sibling transaction.
MIP-Tables.
In order to ensure the scalability of the
MIPT management logic, in our implementation MIPTs are
maintained on a (cid:2)le residing on disk, which we refer to as
MIPT data, of which a small number of pages are explic-
itly cached in main memory to reduce I/O activity. To ef(cid:2)-
ciently determine the position of the MIPT associated with
a given family of sibling transactions within the MIPT data
(cid:2)le we use an indexing data structure, which we refer to
as MIPT offset, also maintained as a paged disk (cid:2)le cached
in RAM. For performance reasons, we have structured the
MIPT offset index as a B-tree whose keys are transaction
XIDs, and whose leaves contain the offset of the corre-
sponding MIPTs within the MIPT data (cid:2)le. We have used
the B-tree since the keys correspond to application de(cid:2)ned
identi(cid:2)ers which can be generated in an arbitrary and uncor-
related (although univocal) manner. Therefore, the indexing
data structure is not guaranteed to be accessed sequentially,
which would lead to poor performance (due to reduced lo-
cality) in case it were implemented as a linear indexing data
structure.
MIPTs are sequentially allocated within the MIPT data
(cid:2)le, and are composed by two main parts: (i) the header and
(ii) the results area. The MIPT header contains the follow-
ing information:
1. Sibs X M IP T , namely the maximum number of
sibling transactions that can be registered within the
header without incurring an over(cid:3)ow (1).
1Over(cid:3)ows are tackled by allocating a new chunk for that same MIPT,
which also includes a new header (identical to the original one, except for
that it stores no request string) linked to the original one.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE407DSN 2008: Romano & Quaglia2. An array of size Sibs X M IP T , whose entries con-
tain the following information:
(i) the transaction
XIN ST , (ii) the transaction state, and (iii) a pointer
to the initial position of the corresponding result within
the MIPT .
3. The actual number of the previous array entries that
have already been used to register a sibling transaction,
and the number of free bytes within the results area.
4. A pointer to the initial position within the MIPT data
(cid:2)le of the memory area allocated due to the occurrence
of an over(cid:3)ow, if any.
5. The request string associated with the family of sibling
transactions.
Concerning the allocation of the results area, its size is
set to Sibs X M IP T (cid:2) sizeof(result string),
where result string is the result passed as input pa-
rameter to the PREPARE MIP command that triggered the
MIPT allocation. This simple heuristic is based on the idea
that the results produced by sibling transactions are likely
to exhibit similar size.
Precommit and Commit Log Management. To enable
MIPTs recoverability and to guarantee the atomicity of
transaction precommit and of the update of the correspond-
ing MIPT, we rely on a conventional Write-Ahead-Logging
(WAL) strategy [3]. More in detail, this is accomplished by
writing the log entries describing the MIPT updates right
after the typical log entries (related, e.g., to the locks main-
tained by the transaction) produced by the original precom-
mit logic implemented within PostgreSQL, and just before
emitting the PRECOMMIT log marker, whose presence on
the log-(cid:2)le denotes that the transaction has been precom-
mitted. On the other hand, updates of the MIPT offset and
MIPT data (cid:2)les are performed only after having success-
fully (cid:3)ushed the transaction logs to disk. This avoids the
need for undoing any update performed on the MIPT offset
and MIPT data (cid:2)le in case of failure of log (cid:3)ushing.
Finally, during both precommit and commit phases, the
database may be required to selectively abort active and/or
prepared sibling transactions (this supports reconciliation).
In our implementation, the abort of active sibling transac-
tions is made possible by just retrieving the corresponding
TIDs within the in memory hash table keeping track of the
identity of each active MIP transaction (see Section 3.1).
Conversely, in order to enforce the abort of precommitted