Proof. Let Mo be any semantically obfuscated program wrapped by Mk, such
that Mk(x) calls Mo on the input x = k and halts otherwise. Let k =
sensor(U, Mk), where sensor : U × M → N and sensor is learnable.
To defeat the obfuscation, the adversary ﬁrst creates an instrumented envi-
ronment U(cid:4) = A(U). In U(cid:4), any system output from an executed program is
copied to the user tape. The adversary then repeats this process n times, where
n is polynomial with respect to the size of Mo, creating the set (U(cid:4)
n). Now
the adversary executes Mk on each instrumented environment U(cid:4)
1
i, computing n
diﬀerent sensor readings k(cid:4)
i , Mk). Since sensor() is learnable via a
polynomial number of input-output pairs, the adversary can determine the deﬁn-
ition of sensor based on the information it has computed. The adversary can now
simulate sensor() on any input, including the original k = sensor(U, Mk), even
without access to the intended hardware H. Given this capability, the adversary
can now execute Mk(k), which calls Mo. Now that the adversary has full input-
output access to Mo, they can compute semantic properties of M, thus violating
the property of semantic security guaranteed by semantic obfuscation.
i = sensor(U(cid:4)
. . . U(cid:4)
4.2 Random Oracle Sensor
The opposite of a learnable function is an unlearnable function, in which the
deﬁnition of the function cannot be determined through inputs and outputs
alone. An example of an unlearnable function is a random oracle: a function
which returns a unique, perfectly random value for every input. Given the same
input, a random oracle will return the same random value. A random oracle can
be thought of as the most unlearnable function because an inﬁnite number of
inputs and outputs will not give any information about any other input–output
pairs.
We now model a sensor that is a random oracle. This sensor allows a program
to detect any diﬀerences made in the program or its environment. With access
to this ideal sensor, we can now achieve semantic obfuscation.
Theorem 2. There exists a semantic obfuscator within the System-Interaction
model when the sensor is a random oracle.
Proof. We will construct an obfuscator O in the System-Interaction model with
a sensor that acts as a random oracle such that Mo = O(U, M) with wrapper
Mk such that Mk(k) calls Mo when k = sensor(U, M). We assume M has no
user input or output and that Mk is VBB obfuscated. The program Mk has no
user output when receiving an input of k, otherwise Mk outputs a 0 to the user
tape and halts. We will now provide a construction of Mk and Mo such that Mo
is semantically obfuscated.
We design Mk such that Mk will call Mo when x = k and at all other points
Mk just outputs 0 to the user tape and halts. For i = 0 to i = n, we construct Mi,
with the goal of i = sensor(U, Mi) mod 2n, where n is our security parameter.
This gives us the ability to increase or decrease the codomain of the random
oracle. We construct 2n diﬀerent VBB obfuscated Mi’s, and submit them to
222
J. Blackthorne et al.
H. The single Mi that returns no user output will be the program for which
i = sensor(U, Mi). Now that we have constructed Mk which calls Mo as a
subroutine, we will show that Mo is semantically secure. To do so we employ a
semantic security game.
We allow a PPT adversary A to pick any two programs of equal size M1, M2,
such that neither have user input or output, and send them to the challenger. The
challenger will ﬂip a fair coin to choose a bit b. The challenger then computes
Mo = O(U, Mb) with wrapper Mk and sends back Mk to the adversary. The
adversary must determine whether Mo is the obfuscated version of M1 or M2.
The adversary has three sources from which to extract information about
the subroutine Mo: the user output, source code, and the system output. By
deﬁnition there is no user output from the program. Our obfuscator is a VBB
obfuscator, so the source code does not leak any information. This leaves the sys-
tem output as the only source of information. In the System-Interaction model,
the user cannot see the system output without modifying U. If the adversary
runs Mk with a modiﬁed U(cid:4), the reading from sensor(U(cid:4), Mk) returns some x not
equal to and independent of k, which is needed by Mk to call Mo. The program
Mk is a point function with a domain of 2n, which means it is computationally
infeasible for a PPT adversary to guess k.
Discussion. We have shown that we can create a semantically obfuscated pro-
gram Mo with a wrapper Mk, but the obvious drawback is that it was done in
exponential time. In the world of cryptography, exponential time is intended for
the adversary, not the legitimate user. In the case of obfuscation, it could still
be considered useful. This is because even though the creation time is exponen-
tial, the deobfuscation time would be exponential, while the running time of the
obfuscated program would remain polynomial.
Theorem 3. No polynomial time semantic obfuscator O exists within the
System-Interaction model when the sensor is a random oracle.
Proof. Assume by way of contradiction that O is a polynomial time obfuscator
in the System-Interaction model using a random oracle sensor such that Mo =
O(U, M) with wrapper Mk, where k = sensor(U, Mk). The program Mk is VBB
obfuscated and only calls Mo when receiving an input of k, otherwise Mk outputs
a 0 to the user and halts.
If PPT O can construct Mo with wrapper Mk, then there exists a dependent
relationship between M and Mk by way of the algorithm O, which implies a
dependent relationship between x = sensor(U, M) and k = sensor(U, Mk).
This is a contradiction because sensor() is a random oracle, guaranteeing total
independence of outputs given any two diﬀerent inputs.
Discussion. We have highlighted the essential limitation of using the observer
eﬀect. The strongest observer eﬀect possible, formalized here in the random ora-
cle sensor dependent on the system and program, necessarily limits the creation
of the program as much as the deobfuscation.
A Formal Framework for Environmentally Sensitive Malware
223
Theorem 4. A semantic obfuscator O cannot obfuscate a program M in less
time than an adversary can deobfuscate O(M) within the System-Interaction
model when the sensor is a random oracle.
Proof. As proven in Theorem 3, any semantically obfuscated program Mo in the
System-Interaction model wrapped by Mk with a random oracle sensor can at
best be created in exponential time. This computational eﬀort is restricted by the
speed at which H runs. This is because the System-Interaction model assumes
(cid:7)= H. This forces the author to
that H is unique and independent for all H(cid:4)
generate 2n programs and test each on H and only H.
By default, the adversary will not see any output from Mk when running on
H with U. Thus, the adversary must change U to U(cid:4) to see the system input
and output. This necessarily changes the value of sensor(U(cid:4), Mk) and forces
the adversary to brute-force all possible U(cid:4) (cid:7)= U such that sensor(U(cid:4), Mk) mod
2n = sensor(U, Mk) mod 2n, where n is the security parameter. The adversary
can parallelize these brute-force attempts over any number of H(cid:4)s, thus not being
subject to the bottleneck of running on H and H alone, as the author does. This
gives the adversary a large computational time advantage.
Discussion. Theorem 3 showed that the observer eﬀect guarantees a compu-
tational symmetry between obfuscation and deobfuscation within the System-
Interaction model. But Theorem 4 highlights an additional consideration in the
real world: hardware speed and parallel computing. The obfuscator extracts
secret information from H and embeds it in the program. But this new repre-
sentation of the secret knowledge has an innate vulnerability: it allows itself to
be computed on by the adversary with hardware superior to what the original
obfuscator could use to perform the obfuscation. This demonstrates that not
only can the observer eﬀect not be used to any advantage by the obfuscator, but
it is actually to the obfuscator’s disadvantage.
4.3 Piecewise Learnable Sensor
We established in Theorem 1 that a learnable sensor cannot be used to build a
semantically obfuscated program. We proved in Theorem 3 that a sensor that
functions as a random oracle cannot be used to create a semantically obfuscated
program in polynomial time. We are now left with unlearnable functions which
are not random oracles as possible candidates for a semantic obfuscator that
runs in polynomial time.
We now model a sensor as an unlearnable function which is a hybrid of both
learnable functions and a random oracle. The sensor is piecewise learnable. This
means the sensor function as a whole is not learnable, but each sub-function by
itself is learnable.
The total state space of the system is S = U × M, with the system being in
any given state St ∈ S at time t. The state of the system St is a binary encoding
of a TM and so can be sectioned oﬀ by Si, where Si+1 is some arbitrary number,
224
J. Blackthorne et al.
and i ∈ {0, 1, . . . , n − 1, n}. There are n = 2λ number of diﬀerent learnable sub-
functions, where λ is a security parameter. The assignment of each sub-function
to a subset of the state space is unknown a priori, but is itself measurable. We
model this by assigning each sub-function to a subset of the state space by a
random oracle R.
Sensor(St) =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
S0 ≤St < S1
fR(0)(St) :
fR(1)(St) : S1 + 1 ≤St < S2
Sn−1 ≤St ≤ Sn
. . .
fR(n)(St) :
With this sensor, it is not clear how it can be used to create a semantically
obfuscated program. We need to be able to make changes to the program to
embed the correct k such that k = sensor(U, Mk). Simultaneously, we need the
adversary to not be able to make changes to the system U and still generate
the correct k. These two conditions can be summarized with the following two
properties of our piece-wise learnable sensor:
1. S(cid:4)
t = (U, Mk(cid:2)) and St = (U, Mk) fall under the same sub-function. And
because each sub-function is learnable, there exists a PPT A such that given
x2 = sensor(U, M(cid:4)
t = (U(cid:4), Mk) and St = (U, Mk) do NOT fall under the same sub-function.
This means there does NOT exist a PPT A such that given access to x2 =
sensor(U(cid:4), Mk(cid:2)) can derive x2 = sensor(U, Mk).
2. S(cid:4)
k), A can derive x2 = sensor(U, Mk).
If the sensor is learnable then condition 1 is true and 2 is false. If the sensor is a
random oracle then 1 is false and 2 is true. It clear we need both 1 and 2 to be
true. Let us now prove that having both conditions true is suﬃcient to create a
semantic obfuscator in the System-Interaction model.
Theorem 5. Given a sensor which can be represented by a piecewise learnable
function, there exists a PPT O that transforms a TM M into a semantically
obfuscated program by wrapping it in Mk, within the System-Interaction model.
Proof. Let M be a program with no user input or output. We construct a pro-
gram Mo wrapped by Mk such that Mo is semantically obfuscated. The program
Mk calls Mo as a subroutine when x = k and k = sensor(U, Mk). We do this
construction in constant time through public-key encryption. This is achieved by
creating Mi from i = 0 to i = p, where p is polynomial with regards to the size
of Mo. Each Mi writes out x2 = sensor(U, Mi) to the user tape encrypted with
the author’s public key. Only the author can decrypt these readings with the pri-
vate key. Given access to p sensor readings, all from the same sub-function, the
author learns the sub-function and constructs Mk such that k = sensor(U, Mk).
The adversary is required to modify U to determine the input–output of the
program because they do not possess the private key. And due to property two of
our piece-wise learnable sensor, changing U to U(cid:4) will cause the sensor to jump
sub-functions. The new sub-function and its sensor values will be independent
A Formal Framework for Environmentally Sensitive Malware
225
of the original sub-function the program was in when it was being run with U,
thus giving the adversary no information.
Discussion. Given the hefty assumptions we made, it does not come as a surprise
that we can achieve semantic obfuscation. It does help illustrate a point though:
there must exist an asymmetry in the observer eﬀect for it to be useful. Quite like
in cryptography, we need a one-way function. One-way functions in cryptography
are typically over the domains of integers or lattices. Our one-way function is
over the domain of TMs in the System-Interaction model. This comes in the form
of the assumptions that changing keys in our point function does not change the
sub-function, but changing the environment does change the sub-function. This
is much like a noise threshold within an error correction scheme.
5 Existing Sensors
So far we have considered properties of functions that act as sensors. We have
shown that it is necessary that a function be unlearnable but not a random
oracle in order to achieve semantic obfuscation. Then we showed that a piece-
wise learnable function is suﬃcient to achieve semantic obfuscation within the
System-Interaction model.
In this section we will consider existing sensors within real programs and show