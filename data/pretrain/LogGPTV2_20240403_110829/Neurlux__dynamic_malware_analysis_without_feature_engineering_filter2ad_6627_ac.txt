models to get a low variance and better predictive performance
than any single constituent algorithm alone. Specifically, we use
a stacking ensemble which uses one large multi-headed neural
network and learns how to best integrate predictions from each
input sub-network.
We start with five separately trained models from the previous
section. These models are trained on the five most important behav-
ior traits observed in the reports; namely, API Sequences, Mutex
Operations, File Operations, Registry Operations, and DLLs Loaded.
Each model uses the text-based feature classification method ex-
plained in Section 4.5. The outputs from all the sub-models are
concatenated, and provided to a fully connected layer that acts as
a meta-learner, and makes its probabilistic predictions. The sub-
models are not trainable; therefore, their weights are not changed
during the training, and only the weights of new hidden and output
layers are updated.
Merging the outputs from multiple neural networks adds a bias
that in turn counters the variance of a single trained neural network
model. As a result, the final predictions are less sensitive to the
specifics of training data and are more generalized.
5 DATASETS
We have employed two different datasets for evaluating our re-
search work. Dataset 1 (VendorDataset) is a set of 27,540 Windows
x86 binaries with 13,760 benign and 13,760 malicious files. This
private dataset was randomly selected from an original pool that
was analyzed by the anti-malware vendor’s sandbox in the US dur-
ing the period from 2017-05-15 to 2017-09-19. This security vendor
provides a sandbox that runs executables, and collects full analysis
results that outline what the sample does while running inside an
isolated operating system. Dataset 2 (EmberDataset) is a subset of
the publicly available Ember dataset[2]. It consists of 42,000 Win-
dows x86 binaries with 21,000 benign and 21,000 malicious samples.
windows
windows
defender
appdata
nt
currentversion
nt
defender
exclusions
roaming
terminal
explorer
terminal
spynet
paths
alfsvwjb
services
advanced
services
spynetreporting
c
policies
maxidletime
showsuperhidden
Malware Sample 1
maxdiscon... microsoft
windows
microsoft
administrator
users
microsoft
windows
windows
microsoft
Table 2: Attention for registry keys written shown for a malware sample that the model classified correctly as malicious. The
cells are colored by how much attention each word received, with colors: veryhigh, high, medium, low, and verylow.
CuckooSandbox, is collected using the Cuckoo sandbox [1]. Cuckoo
is a publicly available sandbox which can be used to trace execu-
tion in a virtualized environment. It generates a JSON report of
the actions taken by the binary during runtime. The other report,
VendorSandbox, is collected using the sandbox from the security
vendor, which also traces execution and collects information about
the runtime execution of the executable. This results in two datasets
with two different report formats for a total of four different combi-
nations of datasets and reports.
5.1 Comparison of Reports
We ran an initial analysis of the two reports to understand their
structure and the features they contain, which can be used for
dynamic malware analysis. The format of one JSON report is shown
in Figure 4. When examining the reports, we noticed that parts of
the reports could be quite different for identical executables. The
number of registry actions, file actions, and even the actual paths
tended to differ. Differences show up because of differences in the
sandbox and execution environment; even how long the executable
is run influences the data in the reports.
The feature names do not match up exactly, so we try to draw
parallels in features from the respective sandboxes. For example,
VendorSandbox references “loaded_libraries”, whereas CuckooSand-
box uses “dll_loaded”, but semantically they are the same. Due
to subtle differences between the sandbox environments and for-
mats, strings to do not match exactly. For example, when looking
at registry keys, in the VendorSandbox we observe a key starting
hkcu\software\microsoft\windows, while in Cuckoo it shows
as hkey_current_user\software\ microsoft\ windows.
This shows that the reports are similar but not identical, and thus,
our model needs to be robust enough to handle the discrepancies.
We will later evaluate how robust various approaches are to these
differences between the two sandbox reports.
Figure 3: Top 25 Malware Families in VendorDataset and
count(#)
Figure 4: Cuckoo Sandbox Report format example
VendorDataset has over a 1000 malware families, top 25 of which
are shown in the Figure 3.
Each binary is accompanied by two versions of detailed behav-
ioral analysis reports in a JSON format. One behavioral report,
6 EVALUATION
In this section, we evaluate Neurlux, which we described in Sec-
tion 3. We compare Neurlux to approaches with feature engineering,
which are described in Section 4.3. We also compare Neurlux with
MalDy [11]. In particular, we attempt to answer the following re-
search questions:
RQ1: Can deep learning methods without feature engineering
identify malware from dynamic analysis reports as effec-
tively as methods with feature engineering?
After this, each model is then tested on the VendorDataset to
evaluate its ability to generalize to a new dataset. Each model is
also tested using the CuckooSandbox on the same samples in the
EmberDataset to evaluate the robustness of the model to a different
report from a different sandbox. These tests should show if the
model is learning features that generalize well and are not specific
to the particular report or dataset.
Additionally, we compared against MalDy [11]. This is a state of
the art model using XG-Boost and feature hashing. We implemented
it for our tests as we could not obtain the source code.
6.2 Results
Table 3 shows the results of each of the models we trained. Looking
at the results, we see that Neurlux performs very well, showing
the best accuracy when applied on both a new dataset and on a
new sandbox, getting 87.0% and 86.7% respectively. The ensemble
classifier outperformed it slightly in terms of validation accuracy,
but it wasn’t nearly as robust to differences in datasets or sandboxes.
This result allows us to answer our first research question.
Answer for RQ1: Neurlux, a deep learning method without
feature engineering performs about as well in validation ac-
curacy as our best model with feature engineering. It also
showed better results than the feature engineered models
when tested on another dataset or report format.
6.3 Individual Features
We also trained and tested each of the five individual features we
extracted using the same CNN+BiLSTM+Attention model design
that we use in Neurlux. These individual models are also used to
compose the Ensemble Model. The results of each model are shown
in Table 4. We observed that file actions perform the best of any
of our features. It also generalized fairly, showing good results on
the other dataset. Note that APIs were represented significantly
differently between the two sandboxes, which explains its low score
on CuckooSandbox.
We visualized the trained word embeddings for file actions to
see if the word embeddings are creating good clusters. For this we
use a T-SNE plot [17], which is shown in Figure 5. In the T-SNE we
see clusters of similar files that the model learned. We also see that
the two clusters for files seen in benign and malicious files only
had a bit of overlap.
6.4 Attention
When we look at the results in Table 3, we find that the Raw Model
had a lower validation accuracy than Neurlux, and performed much
worse when tested on a different dataset. This implies that the Raw
Model might not be using very general features, whereas Neurlux
appears to be learning features that generalize better. In this section,
we explore what the two models appear to be paying attention to,
or in other words, which parts of the reports contribute most to
the classification decisions by the models. This can be used to both
understand the model better, and it can also be used by security
researchers to identify possible features that they can use in other
analyses.
Firstly, we examine the Raw Model. To do this, we use a concept
called saliency, introduced by Simonyan et al. [37]. Saliency uses the
Figure 5: T-SNE visualization of file operations. This shows
how the files used in file operations were clustered by the
trained embedding layer. Blue shows files common to be-
nign files, red shows files common to malicious, and green
shows files common to both.
RQ2: Does the application of advanced NLP techniques im-
prove the results of malware detection on dynamic analysis
reports compared to other deep learning models?
RQ3: How robust are the various approaches to being applied
to different datasets and sandbox/reports?
RQ4: Which parts of the report does Neurlux learn to use in
detecting malware?
The first two research questions will help us to evaluate Neurlux’s
malware detection capabilities, and compare our approach to other
deep learning models. The third question is useful in order to un-
derstand if our approach is learning robust features that apply to
other environments. Finally, the last question is chosen to explore
which parts of the report are used by Neurlux. This can help in
determining if the approach is learning useful features.
6.1 Experiment Design
Here we will evaluate the performance of Neurlux and each of
the comparison models described in the previous section. All the
evaluated models are listed in Table 1 with a brief recap on their ap-
proach. We trained each model on reports from the VendorSandbox
on executables from the EmberDataset. We chose the EmberDataset
for our training because it is the larger dataset and would provide
more samples for training. We performed the classical k-fold cross
validation (where k = 10) to test the models. That is, we divide
the dataset D into D1, D2, ..., Dk. We spare Di for 1 < i < k for
testing, and use the remaining k − 1 folds for training. This process
is repeated k times to get accurate validation results. We trained all
our models by minimizing the cross-entropy error.
Model
MalDy [11]
Counts Model
Ensemble Model
Raw Model
Neurlux
Accuracy
.8923
.935
.980
.914
.968
Precision Recall
.850
.891
.986
.948
.967
.830
.941
.975
.906
.966
F1-Score Accuracy on
VendorDataset
.55
.885
.778
.92
.980
.843
.698
.927
.968
.870
Accuracy on
CuckooSandbox
.40
−
.780
.627
.867
Table 3: This table shows how the models performed. The models were all trained on reports from VendorSandbox on the
EmberDataset. They are tested using K-Fold validation and tested on the other dataset and report format. The first row, MalDy,
is a state of the art model from [11] that we compare with. The next two rows are feature-engineering based approaches.
Individual
Feature
Registry
File
API
Mutex
DLLs
Accuracy Accuracy on
VendorDataset
.801
.944
.842
.978
.865
.776
.733
.808
.960
.810
Accuracy on
CuckooSandbox
.767
.771
.510
.677
.701
Table 4: This table shows how the model performed on each
of the five individual features. This table shows the valida-
tion accuracy as well as the accuracy when given a different
dataset and different report format.
gradient of the output with respect to the input to determine which
areas of input will most affect the output of a neural net. This will
allow us to highlight, for a particular sample, the regions of bytes
that contribute most to its classification. When applying saliency
on the Raw Model, we see that the model frequently pays the most
attention to hashes (SHA-1 hashes, MD5 hashes), DLLs, file names,
and library addresses. An example of the most highlighted area
of one sample is shown in Figure 6. Although SHA-1 hashes are
frequently important to the model’s classifications results, they are
not a feature that generalizes well. Any byte changed can cause
these features to be wrong. Additionally, intuitively, we know that
library addresses are likely to change, and not the best feature
either.
Then, we examine our approach, Neurlux, which uses NLP tech-
niques to learn on whole words rather than bytes. For this model,
we have an Attention layer, explained in the Section 2.5, which
allows the model to learn what it should focus on, and allows us to
interpret which words/phrases received the most attention directly.
By examining the attention activations, we can also see what the
model is paying attention. An example is shown in Figure 7, which
shows our method giving the most attention to a few API calls.
Overall, we found that Neurlux focused on parts of the document
that seemed much more general. For example, in the first couple
of samples it focused on words such as “ntqueryinformationpro-
cess”, “ntreadvirtualmemory”, “virustotal”, “programs”, “startup”.
These intuitively make sense as valuable features. The first couple
indicates that the process is attempting to interact with other run-
ning processes. Then, of course, “startup programs” shows that the
executable might be trying to set a new process to autostart.
Figure 6: The most salient area of sample 234 highlighted
by the Raw Model at the byte level showing the Raw Model
giving a lot of importance to seemingly random parts such
as part of ”address“ and ”end“. Sample 234 was misclassified
by the Raw Model as benign when it is malicious.
Figure 7: The highest attention areas of sample 234 high-
lighted by Neurlux. Neurlux paid the most attention to the
api calls in this example. Neurlux was able to correctly clas-
sify it as malicious.
Our approach (Neurlux) was able to pick features that look bet-
ter intuitively, and it showed that its results generalized well to
other datasets. This seems to be a result of applying document
classification techniques from NLP to our model. Our approach
looks at whole words rather than bytes, and its model learns better
which words and phrases are indicative of malicious behavior when
compared with the raw model, which focused on more arbitrary
things such as sequences of bytes in SHA1 or MD5 hashes.
We also counted which features show up most frequently with
the highest attention score. More specifically, for a subset of ma-