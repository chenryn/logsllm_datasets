Thus, the inverse problem on the prediction error can be
constructed by simply taking the difference between con-
secutive link load observations: A˜xt = ˜bt = bt − bt−1.
The performance of the inversion technique is measured by
comparing the inferred solution, ˜xt, to the direct difference
of the OD ﬂow, xt −xt−1; the closer the values are, the bet-
ter the result. In the context of anomaly detection, it is often
the case that the large elements (large volume changes) are
of chief interest to network management. Hence, we de-
ﬁned a metric – detection rate – to compare the top ranked
N elements (sorted by size) in solution ˜xt to the top N
prediction errors xt − xt−1 for t spanning a period of one
week. As we will see in Section 6, the top anomalies in our
data are easily resolved by magnitude (close ties are rare).
The detection rate is the ratio of the overlap between the
two sets. Note that the detection rate avoids some prob-
lems with comparing false-alarm versus detection proba-
bilities, as it combines both into one measure. A high de-
tection rate indicates good performance. Detection rate is
used to compare inference techniques in Section 6.1, to as-
sess sensitivity to λ, robustness to noise in Section 6.2, and
the effectiveness of the methods for time-varying routing in
Section 6.3.
In Section 6.4.2 we step away from the simple anomaly
detection algorithm applied to test the inference compo-
nent, and compare the complete set of anomography meth-
ods described in Section 3. As before we use detection
rate to measure whether the anomaly detection method
produces similar results when applied to the OD pairs di-
rectly, or applied to the link load data, along with an inver-
sion method — we use the Sparsity-L1 method (the best
performing of the methods tested using the methodology
above). In other words, we benchmark the anomography
method against the anomalies seen in direct analysis of the
OD ﬂows.
Since different methods may ﬁnd different sets of bench-
mark anomalies, we need an objective measure for assess-
ing the performance of the methods.
Ideally, we would
like to compare the set of anomalies identiﬁed by each of
the methods to the set of “true” network anomalies. How-
ever, isolating and verifying all genuine anomalies in an
operational network is, although important, a very difﬁcult
task. It involves correlating trafﬁc changes with other data
sources (e.g., BGP/OSPF routing events, network alarms,
and operator logs), an activity that often involves case-
by-case analysis. Instead, we perform pair-wise compar-
isons, based on the top ranked anomalies identiﬁed by each
of the anomography methods, an approach also taken in
Lakhina et al. [19].
Speciﬁcally, for each of the anomography methods, we
apply the underlying anomaly detection method directly to
the OD ﬂow data. We think of the top ranked M anoma-
lies, denoted by the set B(j)
M for anomaly detection method
j as a benchmark. For each of the anomography methods
i, we examine the set of N largest anomalies A(i)
N inferred
from link load data. To help understand the ﬁdelity of the
anomography methods we consider the overlap between
the benchmark and the anomography method, A(i)
N ∩ B(j)
M ,
across the benchmarks and the anomography methods. We
allow a small amount of slack (within one ten-minute time
shift) in the comparison between events, in order that phase
differences between methods not unduly impact the results.
We are interested in understanding both false positives
and false negatives:
(i) False Positives. Taking B(j)
N − B(j)
M as the benchmark, the
false positives produced by anomography method i are
M . The magnitudes of the anomalies in A(i)
A(i)
N
and B(j)
M may vary. Yet, intuitively if one of the N = 30
top anomalies in A(i)
N is not among the top M = 50
from the benchmark, then this anomaly in A(i)
N is likely
a false positive. This leads to the following heuristic for
detecting false positives. We choose (reasonable) pa-
rameters N and M , with N  M , and count
the false negatives as the size of B(j)
M − A(i)
N .
For our reports in the next section, we choose the smaller
of M and N to be 30, since this roughly represents the
number of trafﬁc anomalies that network engineers might
have the resources to analyze deeply on a weekly basis. We
would like to show comparative results where the larger pa-
rameter varies, but cannot within a reasonable amount of
space, and so show results for one ﬁxed value 50. It is im-
portant to note that the results we obtained for other values
of M and N change none of our qualitative conclusions.
6 Results
We obtained six months (03/01/04-09/04/04) of measure-
ments for the Abilene network and one month (10/06/04-
11/02/04) for the Tier-1 ISP network. We partitioned the
data into sets spanning one week each, and evaluated the
methods on each data set. Due to space limits, we present
only one set of representative results – Tier-1 ISP (10/6/04-
10/12/04). In our technical report [33], we also report re-
sults in other weeks for the Tier-1 ISP network as well as
for the Abilene network. These results are qualitatively
similar to those reported here.
6.1 Comparison of Inference Techniques
We ﬁrst compare different solution techniques for the in-
ference problem ˜b = A˜x. More speciﬁcally, we con-
sider three late inverse algorithms: Pseudoinverse (Section
3.4.1), Sparsity-Greedy (Section 3.4.2), and Sparsity-L1
(Section 3.4.2), and one early inverse technique: Early
Inverse-Tomogravity. We choose to use the tomograv-
ity method [35] as the early inverse technique since it has
demonstrated high accuracy and robustness for estimating
trafﬁc matrix for real operational networks [14, 35].
Figure 1 plots the sizes of the top 50 anomalies (the fore-
cast errors) of the OD ﬂows (the solid lines) and the corre-
sponding values diagnosed by the different inference tech-
niques (the points) for 10/6/04 to 10/12/04, for the Tier-1
ISP network. The y-axis provides the size of the anoma-
lies normalized by the average total trafﬁc volume on the
network. The x-axis is the rank by the size of anomalies di-
rectly computed from the OD ﬂows. We observe that there
are very few large changes – among more than 6 million
elements (∼ 6000 OD ﬂows at 1007 data points), there is
one instance where the size of anomaly is more than 1% of
total trafﬁc and there are 18 cases where the disturbances
)
c
i
f
f
a
r
t
l
a
t
o
t
e
g
a
r
e
v
a
o
t
e
v
i
t
l
a
e
r
(
e
z
s
i
l
y
a
m
o
n
A
e
t
a
R
n
o
i
t
c
e
t
e
D
0.016
0.014
0.012
0.01
0.008
0.006
0.004
0.002
0
1
0.8
0.6
0.4
0.2
0
5
Anomaly on Real OD Flow
Pseudoinverse
Sparsity-Greedy
Sparsity-L1
Early Inverse-Tomogravity
5
10
15
20
25
30
35
40
45
50
Rank by anomaly size (based on real OD flow)
Figure 1: Anomalies by Size
Pseudoinverse
Sparsity-Greedy
Sparsity-L1
Early Inverse-Tomogravity
10
15
20
25
30
35
40
45
50
N
Figure 2: Detection Rate by Various Inference Techniques
constitute more than 0.5% of total trafﬁc. This agrees with
our intuition on the sparsity of network anomalies.
We see that Pseudoinverse signiﬁcantly underestimates
the size of the anomalies. Intuitively, Pseudoinverse ﬁnds
the least square solution which distributes the “energy” of
the anomaly evenly to all candidate ﬂows that may have
contributed to the anomaly, under the link load constraint.
This is directly opposed to the sparsity maximization phi-
losophy. Among the sparsity maximization techniques,
Sparsity-L1 performs the best. Sparsity-L1 always ﬁnds
solutions close to the real anomalies. Sparsity-Greedy, in
general, is more effective than Pseudoinverse, although it
sometimes overestimates the size of anomalies. As a repre-
sentative of the early inverse technique, Tomogravity also
performs well. With few exceptions, tomogravity ﬁnds so-
lutions that track the real OD ﬂow anomalies. Intuitively,
when a proportionality condition holds, i.e., when the size
of the anomalies are proportional to the sizes of the OD
ﬂows, then early inverse methods work well. However,
where the proportionality condition does not hold, the error
can be signiﬁcant.
Figure 2 presents the detection rate for the different in-
ference techniques. We observe that for the Tier-1 ISP net-
work, Sparsity-L1 and Tomogravity, which have about 0.8
detection rate, signiﬁcantly outperform other methods.
Due to space limits, we will consider only Sparsity-L1
and Tomogravity in the rest of the evaluation, as these
method demonstrate the greatest performance and ﬂexibil-
ity in dealing with problems such as missing data and rout-
ing changes.
326
Internet Measurement Conference 2005 
USENIX Association
e
t
a
R
n
o
i
t
c
e
t
e
D
1
0.8
0.6
0.4
0.2
0
5
Sparsity-L1: lambda = 0.1
Sparsity-L1: lambda = 0.01
Sparsity-L1: lambda = 0.001
Sparsity-L1: lambda = 0.0001
Sparsity-L1: lambda = 0.00001
10
15
20
25
30
35
40
45
50
N
Figure 3: Sensitivity to Parameter Choice: λ
e
t
a
R
n
o
i
t
c
e
t
e
D
1
0.8
0.6
0.4
0.2
0
5
Sparsity-L1: no noise
Sparsity-L1: 0.5% noise
Sparsity-L1: 1% noise