### Inverse Problem and Performance Metrics

The inverse problem for prediction error can be formulated by calculating the difference between consecutive link load observations: \(\tilde{A}x_t = \tilde{b}_t = b_t - b_{t-1}\). The performance of the inversion technique is evaluated by comparing the inferred solution, \(\tilde{x}_t\), with the direct difference of the Origin-Destination (OD) flow, \(x_t - x_{t-1}\). A closer match between these values indicates better performance.

In the context of anomaly detection, large volume changes are often of primary interest to network management. Therefore, we define a metric called the "detection rate" to compare the top \(N\) elements (sorted by size) in the solution \(\tilde{x}_t\) with the top \(N\) prediction errors \(x_t - x_{t-1}\) over a one-week period. As shown in Section 6, the top anomalies in our data can be easily distinguished by magnitude, with close ties being rare. The detection rate is the ratio of the overlap between these two sets. This metric avoids some of the issues associated with comparing false-alarm and detection probabilities, as it combines both into a single measure. A high detection rate indicates good performance.

### Detection Rate Applications

In Section 6.1, the detection rate is used to compare different inference techniques. Section 6.2 assesses the sensitivity to the parameter \(\lambda\) and robustness to noise, while Section 6.3 evaluates the effectiveness of the methods for time-varying routing.

### Anomaly Detection and Benchmarking

In Section 6.4.2, we move beyond the simple anomaly detection algorithm used to test the inference component and compare the complete set of anomography methods described in Section 3. We use the detection rate to measure whether the anomaly detection method produces similar results when applied directly to the OD pairs or to the link load data using an inversion method. Specifically, we use the Sparsity-L1 method, which performed best in the previous tests. This allows us to benchmark the anomography method against the anomalies identified through direct analysis of the OD flows.

### Objective Measure for Anomaly Detection

Since different methods may identify different sets of benchmark anomalies, we need an objective measure to assess their performance. Ideally, we would compare the set of anomalies identified by each method to the set of "true" network anomalies. However, isolating and verifying all genuine anomalies in an operational network is a challenging task, often requiring correlation with other data sources such as BGP/OSPF routing events, network alarms, and operator logs. Instead, we perform pairwise comparisons based on the top-ranked anomalies identified by each anomography method, an approach also used by Lakhina et al. [19].

For each anomography method, we apply the underlying anomaly detection method directly to the OD flow data. The top \(M\) ranked anomalies, denoted by the set \(B(j)_M\) for anomaly detection method \(j\), serve as a benchmark. For each anomography method \(i\), we examine the set of \(N\) largest anomalies \(A(i)_N\) inferred from the link load data. We consider the overlap \(A(i)_N \cap B(j)_M\) to understand the fidelity of the anomography methods. We allow a small amount of slack (within one ten-minute time shift) to account for phase differences between methods.

### False Positives and False Negatives

We are interested in both false positives and false negatives:
- **False Positives**: Taking \(B(j)_N - B(j)_M\) as the benchmark, the false positives produced by anomography method \(i\) are \(A(i)_N - B(j)_M\).
- **False Negatives**: The false negatives are counted as the size of \(B(j)_M - A(i)_N\).

For our reports, we choose \(N = 30\) and \(M = 50\), representing the number of traffic anomalies that network engineers might analyze weekly. We show results for one fixed value of \(M = 50\), but note that other values of \(M\) and \(N\) do not change our qualitative conclusions.

### Results

We obtained six months (03/01/04-09/04/04) of measurements for the Abilene network and one month (10/06/04-11/02/04) for the Tier-1 ISP network. We partitioned the data into one-week sets and evaluated the methods on each dataset. Due to space constraints, we present representative results for the Tier-1 ISP (10/06/04-10/12/04). Our technical report [33] provides additional results for other weeks, which are qualitatively similar.

### Comparison of Inference Techniques

We compare different solution techniques for the inference problem \(\tilde{b} = A\tilde{x}\), specifically:
- Pseudoinverse (Section 3.4.1)
- Sparsity-Greedy (Section 3.4.2)
- Sparsity-L1 (Section 3.4.2)
- Early Inverse-Tomogravity (using the Tomogravity method [35], known for its high accuracy and robustness in estimating traffic matrices for real operational networks [14, 35]).

Figure 1 shows the sizes of the top 50 anomalies (forecast errors) of the OD flows and the corresponding values diagnosed by the different inference techniques for the Tier-1 ISP network from 10/06/04 to 10/12/04. The y-axis represents the normalized size of the anomalies, and the x-axis ranks the anomalies by size directly computed from the OD flows. We observe very few large changes, with only one instance where the anomaly size exceeds 1% of total traffic and 18 cases where disturbances exceed 0.5% of total traffic, aligning with our intuition about the sparsity of network anomalies.

Pseudoinverse significantly underestimates the size of anomalies, distributing the "energy" evenly across candidate flows. Sparsity-L1 performs the best, consistently finding solutions close to the real anomalies. Sparsity-Greedy, while generally more effective than Pseudoinverse, sometimes overestimates the size of anomalies. Tomogravity, as an early inverse technique, also performs well, with few exceptions.

Figure 2 presents the detection rates for the different inference techniques. For the Tier-1 ISP network, Sparsity-L1 and Tomogravity, with approximately 0.8 detection rates, outperform other methods. Due to space constraints, we will focus on Sparsity-L1 and Tomogravity in the rest of the evaluation, as they demonstrate the greatest performance and flexibility in handling issues like missing data and routing changes.

### Sensitivity and Robustness

Figure 3 shows the sensitivity of Sparsity-L1 to the parameter \(\lambda\). Figure 4 demonstrates the robustness of Sparsity-L1 to noise levels of 0.5% and 1%.

By focusing on these metrics and methods, we aim to provide a comprehensive and accurate assessment of anomaly detection in network traffic.