from virustotal.com was only 28.43%.
Only about half of the malicious URLs were observed to be de-
livering drive-by download attacks when tested. While we do not
(a) Encountered Exploit-Kit Distribution
(b) Vulnerability Distribution
Figure 4: Statistics from daily malicious URL experiment
(c) Attack Distribution
tives. The second part of the evaluation reveals that BLADE im-
poses a negligible overhead on the host machine.
4.1 Measuring System Effectiveness
We describe three experiments collectively designed to measure
the completeness, accuracy, and overhead of BLADE’s protection.
A. Empirical Daily Evaluation on Malware URL Lists: One
way to demonstrate the effectiveness of a security system is to ex-
ercise it against contemporary real-world threats. Our testbed au-
tomatically harvests malware URLs from multiple whitehat mail-
ing lists on a daily basis and evaluates BLADE against potential
drive-by URLs that were reported in the past 48 hours. To validate
BLADE’s browser and exploit independence, each URL is tested
against multiple software conﬁgurations covering different browser
versions and common plug-ins.
Our evaluation platform is a VMware virtual machine running on
a lightly loaded PC with a 2.0 GHz single-core CPU and 512 MB
RAM. The VM runs Windows XP SP2 (without any additional se-
curity patches) and is equipped with versions of Internet Explorer,
Firefox, and vulnerable browser plug-ins (e.g., PDF reader, Flash
player, JVM). Each software conﬁguration is saved as a separate
VM snapshot. The testbed drives the VM to iterate through the
URL list for each software conﬁguration. Prior to visiting each
URL, the testbed reverts the VM and prepares the environment
wherein BLADE and independent instrumentation tools are loaded.
These include procmon [3] to monitor Windows system call events
and tcpdump on the host to capture packet traces. The purpose
!!!!!!!!!!!!!!!!!!!!!!!!!"#$%&!"'()!*#+,$,-)!.%&+)!*#+,$,-)!"'()!/)0%$,-)!.%&+)!/)0%$,-)!"',%&+!18896 7925 0 10971 0 123+!3992 1934 0 2058 0 .,&)!"45)+!6%&7%')!89$)':)5$);!33!?@!:#-)'%0)!>%4AB!C@"!'%$)D!9745 8126 1619 28.43% "#$%!#&#$!#'#%!#'#(!)!*))!#)))!#*))!()))!(*))!")))!"*))!IE6 r8,f8,j5 IE7 r8,f8,j5 IE8 r9,f9,j6 FF3 r8,f8,j5 '$+#!#+)*!%*%!$)#!)!*))!#)))!#*))!()))!(*))!")))!"*))!')))!'*))!*)))!Adobe Reader Sun Java IE Adobe Flash #"$#!##)&!#"(!#)$!#)(!($!()!#%!#!#)!#))!#)))!r=Adobe Reader, f=Adobe Flash, j=Sun Java !!!!!!!!!!!!!!!!!!!!!!!!!"#$%&!"'()!*#+,$,-)!.%&+)!*#+,$,-)!"'()!/)0%$,-)!.%&+)!/)0%$,-)!"',%&+!18896 7925 0 10971 0 123+!3992 1934 0 2058 0 .,&)!"45)+!6%&7%')!89$)':)5$);!33!?@!:#-)'%0)!>%4AB!C@"!'%$)D!9745 8126 1619 28.43% "#$%!#&#$!#'#%!#'#(!)!*))!#)))!#*))!()))!(*))!")))!"*))!IE6 r8,f8,j5 IE7 r8,f8,j5 IE8 r9,f9,j6 FF3 r8,f8,j5 '$+#!#+)*!%*%!$)#!)!*))!#)))!#*))!()))!(*))!")))!"*))!')))!'*))!*)))!Adobe Reader Sun Java IE Adobe Flash ##)&!#"$#!#)$!#)(!#"(!($!()!#%!#!#)!#))!#)))!r=Adobe Reader, f=Adobe Flash, j=Sun Java !!!!!!!!!!!!!!!!!!!!!!!!!"#$%&!"'()!*#+,$,-)!.%&+)!*#+,$,-)!"'()!/)0%$,-)!.%&+)!/)0%$,-)!"',%&+!18896 7925 0 10971 0 123+!3992 1934 0 2058 0 .,&)!"45)+!6%&7%')!89$)':)5$);!33!?@!:#-)'%0)!>%4AB!C@"!'%$)D!9745 8126 1619 28.43% "#$%!#&#$!#'#%!#'#(!)!*))!#)))!#*))!()))!(*))!")))!"*))!IE6 r8,f8,j5 IE7 r8,f8,j5 IE8 r9,f9,j6 FF3 r8,f8,j5 '$+#!#+)*!%*%!$)#!)!*))!#)))!#*))!()))!(*))!")))!"*))!')))!'*))!*)))!Adobe Reader Sun Java IE Adobe Flash ##)&!#"$#!#)$!#)(!#"(!($!()!#%!#!#)!#))!#)))!r=Adobe Reader, f=Adobe Flash, j=Sun Java 446(a) Evaluation Metrics
(b) Dropped File Statistics
Table 1: Results from daily malicious URL experiment
know the exact reason why attacks fail in each instance, they in-
clude the following: (a) malicious sites that have been cleaned up,
(b) misclassiﬁed sites (e.g., phishing sites) that do not attempt sur-
reptitious drive-by downloads, (c) sites that employ IP tracking to
blacklist repeated visitors, and (d) sites that target vulnerabilities
not present in our conﬁguration.
B. In Situ Attack Coverage Evaluation: The ﬁrst experiment
demonstrates BLADE’s effectiveness against thousands of drive-
by download attacks in the wild. However, it is possible that at-
tacks in the wild are dominated by a few exploit kits and exer-
cise only a limited set of common exploits. To compensate for
this potential limitation, we conducted a second experiment that
speciﬁcally evaluates BLADE against a wider set of hand-crafted
attacks and more browser versions. Speciﬁcally, this customized
attack set is composed of diverse shellcodes and exploits targeting
several vulnerabilities in browser/plug-in software including 11 re-
cently disclosed zero-day exploits listed in Table 2. In each case,
BLADE successfully prevented the execution of the drive-by ex-
ploit binary, reafﬁrming our design premise that BLADE delivers
complete and accurate protection in a browser-agnostic and exploit-
oblivious manner.
C. Benign Website Evaluation: We evaluate BLADE’s effective-
ness on benign web sites, i.e., the false positive rate. For BLADE, a
false positive implies that the execution of a legitimate (authorized)
executable download is blocked by BLADE. Under BLADE’s de-
sign, there are two potential reasons why an authorized executable
download may be inadvertently hindered by BLADE: (i) the user’s
authorization cannot be inferred, which leaves the resulting down-
load in the secure zone as untrusted; (ii) a legitimate browser down-
load seeks to execute benign logic without the user’s consent, which
represents a violation of our root assumption. Thus, we tried to cre-
ate a workload that might trigger (i) or (ii).
To address (i), we ﬁrst tested the signature coverage of down-
load consent dialogs for each browser by looking for an unknown
method for requesting download consent. We downloaded 30 dif-
ferent software applications from 15 highly ranked freeware sites,
with varying ﬁle types (.exe, .zip, .msi etc.). We also checked
whether download consent UIs can be reliably discovered when
noise is introduced onto the screen. Neither of the above two test
cases revealed any false positives. We used a stress-testing-based
strategy to create a workload that could lead to false positives in-
curred from (ii). By manually visiting a URL pool, including the
top 5 highly ranked websites from 16 categories [1], we veriﬁed
that BLADE does not disrupt normal browser interactions with
these benign sites.
4.2 Performance Overhead
BLADE’s deployment target is the average Internet Windows
PC, where it is expected to protect such systems from drive-by
Table 2: Test results on targeted attacks and 0-days
attacks while imposing negligible delays on the web surfer. Per-
formance issues have been considered at almost every stage during
the design and implementation of BLADE. Here, we discuss the is-
sue of BLADE’s performance impact by ﬁrst measuring the delay
certain components caused to related operations on the host com-
puter. We then evaluate the overall performance overhead incurred
by BLADE as a whole.
Micro performance evaluation: These per-component tests are
designed not only to individually measure performance overhead of
each component, but also to mitigate factors that might affect mea-
surements such as variable network delay. Among BLADE com-
ponents, the Screen Parser, the I/O Redirector, and the Correlator
are the only three that introduce measurable delays. We evaluate
the contributions of each of them below.
Although the Screen Parser is designed to be asynchronous there
is a chance of delay while it is matching download consent UI sig-
natures with each appearance of a new UI element. In our tests even
the worst-case matching time was not measurable, i.e., less than a
millisecond.
To accurately examine the worst-case delay introduced by the
I/O Redirector into ﬁle system accesses, we conducted a test as
follows. We chose three ﬁles of varying sizes (1 MB, 10 MB and
100 MB) and copied them from one location to another within the
same disk. Each ﬁle was copied twice, once with and once without
the I/O Redirector turned on. To completely avoid other effects
(e.g., ﬁle system cache, in-memory cache), we reverted to a clean
VM snapshot before beginning each test. The delay and copy times
for each ﬁle are shown in Table 3 which varies from 1 ms (for 1 MB
ﬁles) to 7 ms (for 100 MB ﬁles). The delays are minimal because
the redirector intervenes only in the process of requesting new ﬁle
handles and does not intercept the disk write operations.
We also conducted performance testing on the Correlator, which
!!!!!!!!!!!!!!!!!!!!!!!!!"#$%&!"'()!*#+,$,-)!.%&+)!*#+,$,-)!"'()!/)0%$,-)!.%&+)!/)0%$,-)!"',%&+!18896 7925 0 10971 0 123+!3992 1934 0 2058 0 .,&)!"45)+!6%&7%')!89$)':)5$);!33!?@!:#-)'%0)!>%4AB!C@"!'%$)D!9745 8126 1619 28.43% "#$%!#&#$!#'#%!#'#(!)!*))!#)))!#*))!()))!(*))!")))!"*))!IE6 r8,f8,j5 IE7 r8,f8,j5 IE8 r9,f9,j6 FF3 r8,f8,j5 '$+#!#+)*!%*%!$)#!)!*))!#)))!#*))!()))!(*))!")))!"*))!')))!'*))!*)))!Adobe Reader Sun Java IE Adobe Flash ##)&!#"$#!#)$!#)(!#"(!($!()!#%!#!#)!#))!#)))!r=Adobe Reader, f=Adobe Flash, j=Sun Java !!!!!!!!!!!!!!!!!!!!!!!!!"#$%&!"'()!*#+,$,-)!.%&+)!*#+,$,-)!"'()!/)0%$,-)!.%&+)!/)0%$,-)!"',%&+!18896 7925 0 10971 0 123+!3992 1934 0 2058 0 .,&)!"45)+!6%&7%')!89$)':)5$);!33!?@!:#-)'%0)!>%4AB!C@"!'%$)D!9745 8126 1619 28.43% "#$%!#&#$!#'#%!#'#(!)!*))!#)))!#*))!()))!(*))!")))!"*))!IE6 r8,f8,j5 IE7 r8,f8,j5 IE8 r9,f9,j6 FF3 r8,f8,j5 '$+#!#+)*!%*%!$)#!)!*))!#)))!#*))!()))!(*))!")))!"*))!')))!'*))!*)))!Adobe Reader Sun Java IE Adobe Flash ##)&!#"$#!#)$!#)(!#"(!($!()!#%!#!#)!#))!#)))!r=Adobe Reader, f=Adobe Flash, j=Sun Java !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"#$%&'()*#+,$-!"#./(012/#$%&'()*#345'(46#"2*27*26#.5#.'462#,8'9:#;(*21#!"#$$%&'%((")*+,-./"!01"2,3.4,567,8859*:;"*?=4.+"#"#$$1&!@(%")*+,-./"!01"A./472?(9@25##A-645"1"LMJ"")*+,-./"'01"A.J#A-645"!%!#$!$&$#@K""CO"%0$0#K$$"A.)?&'+@!-'A#B5'()*&'+%&,-'$.'C&D)"&,E)#F'A#B5'()*&'+%&,-'$.#'C&D)"&,E)#F'1&345'0.98    *#*('!*#*($!G766:'9.23    *#('"!*#('&!97H9:'94.66 (#&"%!(#&")!67HI:'=)3&'>)?&'+@!-'()*&'+%&,-'$.#'!/012'()*&'+%&,-'$.'!/012'1&345'0.98    (#%"'!(#(*%!J79G:'9.23    ""#(*%!""#)+,!H76G:'94.66 "%"#''"!"%&#**"!67)?&'+@!-'=)3&'>)?&'+@!-'()*&''+>&,-'1.12 *#,)!!!!676H;'9.45 ,#("!!!!67HH9'95.83 ,'#&&!H7G66'448Table 4: Macro-evaluation results: Browser rendering (left) and authorized download (right) times
6. RELATED WORK
We discuss prior measurement studies that inform the design of
BLADE and distinguish it from existing URL analysis services,
malware defense systems, and browser-based protections.
Internet measurement studies: The problem of drive-by down-