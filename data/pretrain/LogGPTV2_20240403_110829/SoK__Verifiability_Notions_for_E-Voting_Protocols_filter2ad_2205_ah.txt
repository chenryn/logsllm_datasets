More speciﬁcally, we propose two variants, one for qualitative
and one for quantitative reasoning, as explained next.
The distillation of our observations given in Section X-A
reviews different aspects of veriﬁability and, in most cases,
it clearly identiﬁes the best and favorable ways they should
be handled by veriﬁability deﬁnitions. When it comes to the
distinction between qualitative and quantitative approaches to
deﬁne veriﬁability goals, we have, however, found out that both
approaches have merits and both can yield viable deﬁnitions
of veriﬁability. This is why we propose two instantiations of
the KTV framework, one following the qualitative approach
and one for the quantitative approach.
To instantiate the KTV framework, one only has to provide
a deﬁnition of a goal (a family of goals) that a protocol is
supposed to guarantee. Note that, as for the second parameter
of Deﬁnition 1, δ, one should always try, for a given goal, to
establish an as small δ as possible. In other words, the value
of δ is the result of the analysis of a concrete system, rather
than something ﬁxed up front.
In the following, we deﬁne two goals corresponding to the
two variants of veriﬁability discussed above: goal γql(ϕ) for
the qualitative variant and goal γqn(k, ϕ) for the quantitative
one. We explain the meaning of the parameters below. Here we
only remark that the common parameter ϕ describes the trust
assumptions (i.e., it determines which parties are assumed to be
honest and which can be corrupted and when) under which the
protocol is supposed to provide speciﬁc guarantees. Recall that,
in the KTV framework, the adversary sends a special message
corrupt to a participant in order to corrupt it (a participant can
then accept or reject such a message). This allows for modeling
various forms of static and dynamic corruption. Note also that
it is easily visible, given a run, if and when a party is corrupted.
In the following, for a given run r of an e-voting protocol
with n eligible voters, we denote by nh the number of honest
and by nd the number of dishonest voters in r. Recall that
we say a party is honest in a run r if it has not received a
corrupt message or at least has not accepted such a message
throughout the whole run. We denote by c1, . . . , cnh the actual
choices of the honest voters in this run (which might include
abstention), as deﬁned in Section IV-A.
Qualitative goal. The goal γql(ϕ) we deﬁne here corresponds
to the strong veriﬁability goal γSV from Section VII. In contrast
to γSV , γql(ϕ) has the parameter ϕ for the trust assumptions,
which were ﬁxed in γSV . Informally, this goal requires that, if
the trust assumption ϕ holds true in a protocol run, then (i)
the choices of all honest voters who successfully performed
their checks are included in the ﬁnal result, (ii) votes of those
honest voters who did not performed their check may be
dropped, but not altered, and (iii) there is only at most one
ballot cast for every dishonest voter (no ballot stufﬁng). If the
trust assumptions ϕ are not met in a protocol run, we do not
expect the protocol to provide any guarantees in this run. For
example, if in a setting with two bulletin boards, ϕ says that at
least one of the bulletin boards should be honest in a run, but in
the run considered both have been corrupted by the adversary,
then no guarantees need to be provided in this run.
Formally, the goal γql(ϕ) is satisﬁed in r (i.e., r ∈ γql(ϕ))
if either (a) the trust assumption ϕ does not hold true in r, or
if (b) ϕ holds true in r and there exist valid choices ˜c1, . . . , ˜cn
for which the following conditions are satisﬁed:
ρ(˜c1, . . . , ˜cn).
(i) An election result is published in r and it is equal to
(ii) The multiset {˜c1, . . . , ˜cn} consists of all the actual choices
of honest voters who successfully performed their check,
plus a subset of actual choices of honest voters who did
not perform their check (successfully), and plus at most
nd additional choices.
If the checks performed by voters do not fully guarantee
that their votes are actually counted, because, for example,
Benaloh checks were performed (and hence, some probabilistic
checking), then along with this goal one will obtain a δ > 0,
as there is some probability for cheating going undetected.
Also, the requirement that votes of honest voters who do not
checked can at most be dropped, but not altered, might only
be achievable under certain trust assumptions. If one wants to
make weaker trust assumptions, one would have to weaken
γql(ϕ) accordingly.
Quantitative goal. The goal γqn(k, ϕ) of the quantitative
veriﬁability deﬁnition is a reﬁnement of the goal γk from
Section IV (note that now, ϕ can specify trust assumption with
dynamic corruption). Similarly to Section VI, we use a distance
function on election results. Roughly, the goal γqn(k, ϕ) requires
that the distance between the produced result and the “ideal” one
(obtained when the actual choices of honest voters are counted
and one choice for every dishonest voter) is bounded by k,
where, for γqn(k, ϕ), we consider a speciﬁc distance function d.
In order to deﬁne d, we ﬁrst deﬁne a function fcount : Cl → NC
which, for a vector (c1, . . . , cl) ∈ Cl (representing a multiset of
voters’ choices), counts how many times each choice occurs
in this vector. For example, fcount(B,C,C) asigns 1 to B, 2 to
C, and 0 to all the remaining choices. Now, for two vectors of
choices (cid:3)c,(cid:3)c
(cid:16) the distance function d is deﬁned by
(cid:16))[c]|.
d((cid:3)c,(cid:3)c
| fcount((cid:3)c)[c]− fcount((cid:3)c
(cid:16)) = ∑
c∈C
For instance, d((B,C,C), (A,C,C,C)) = 3.
Now, the goal γqn(k, ϕ) is satisﬁed in r if either (a) the
trust assumption ϕ does not hold true in r, or if (b) ϕ holds
(cid:16)
true in r and there exist valid choices c
nd (representing
possible choices of dishonest voters) and ˜c1, . . . , ˜cn, such that:
to
(i) an election result
is equal
, . . . , c
(cid:16)
1
is published and it
), (˜c1, . . . , ˜cn)) ≤ k.
(cid:16)
nd
ρ(˜c1, . . . , ˜cn), and
(cid:16)
1
, c
, . . . , c
(ii) d((c1, . . . , cnh
Note that when an adversary drops one honest vote, this
increases the distance in Condition (ii) by one, but when he
replaces an honest voter’s choice by another one, this increases
the distance by two. This corresponds to the real effect of a
manipulation on the ﬁnal result (goal γk does not distinguish
between these two types of manipulations).
As already explained, since not all voters will check their
receipts, some manipulation will go undetected. And hence, for
this goal δ = 0 is typically not achievable. The security analysis
793793
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:36 UTC from IEEE Xplore.  Restrictions apply. 
carried out on a concrete protocol will have to determine the
optimal (i.e., minimal) δ, given the parameter k.
We ﬁnally note that both of the above goals could be
reﬁned by providing guarantees for those voters who have been
corrupted sufﬁciently late in the protocol. For this, one merely
has to change what it means for a voter to be honest: voters
corrupted late enough would still be considered honest for the
purpose of the above goal deﬁnitions. For such voters, one
would then also provide guarantees. However, such reﬁnements
are protocol dependent, whereas the above goals are applicable
to a wide range of protocols.
Acknowledgements. This work was partially supported by
Deutsche Forschungsgemeinschaft (DFG) under Grant KU
1434/6-3 within the priority programme 1496 “Reliably Secure
Software Systems – RS3”. The research leading to these
results has also received funding from the European Research
Council under the European Union’s Seventh Framework
Programme (FP7/2007-2013) / ERC grant agreement no 258865
(ProSecure).
REFERENCES
http://www.economist.com/node/8382578, December 7th 2006.
[1]
[2] http://www.computerworld.com/s/article/9118204/Princeton report
[3]
rips N.J. e voting machines as easily hackable , October 27th 2008.
http://ucsdnews.ucsd.edu/newsrel/science/08-09ElectronicVoting.asp, Au-
gust 10th 2009.
[4] https://freedom-to-tinker.com/blog/appel/nj-election-cover/, September
13th 2011.
[5] M. Abadi and C. Fournet. Mobile Values, New Names, and Secure
In Proceedings of the 28th ACM Symposium on
Communication.
Principles of Programming Languages (POPL 2001), pages 104–115.
ACM Press, 2001.
[6] Ben Adida. Helios: Web-based Open-Audit Voting. In Paul C. van
Oorschot, editor, Proceedings of the 17th USENIX Security Symposium,
pages 335–348. USENIX Association, 2008.
[7] Ben Adida, Olivier de Marneffe, Olivier Pereira, and Jean-Jaques
Quisquater. Electing a University President Using Open-Audit Voting:
Analysis of Real-World Use of Helios.
In USENIX/ACCURATE
Electronic Voting Technology (EVT 2009), 2009.
[8] Carsten Baum, Ivan Damg˚ard, and Claudio Orlandi. Publicly auditable
secure multi-party computation. Cryptology ePrint Archive, Report
2014/075, 2014. http://eprint.iacr.org/.
[10]
[9] Susan Bell, Josh Benaloh, Mike Byrne, Dana DeBeauvoir, Bryce Eakin,
Gail Fischer, Philip Kortum, Neal McBurnett, Julian Montoya, Michelle
Parker, Olivier Pereira, Philip Stark, Dan Wallach, , and Michael Winn.
STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting
System. USENIX Journal of Election Technology and Systems (JETS),
1:18–37, August 2013.
Jonathan Ben-Nun, Niko Fahri, Morgan Llewellyn, Ben Riva, Alon
Rosen, Amnon Ta-Shma, and Douglas Wikstr¨om. A New Implementation
of a Dual (Paper and Cryptographic) Voting System. In Kripp et al.
[36], pages 315–329.
Josh Benaloh. Simple veriﬁable elections.
In Dan S. Wallach and
Ronald L. Rivest, editors, 2006 USENIX/ACCURATE Electronic Voting
Technology Workshop, EVT’06, Vancouver, BC, Canada, August 1, 2006.
USENIX Association, 2006.
Josh Daniel Cohen Benaloh. Veriﬁable Secret-Ballot Elections. PhD
thesis, 1987.
[11]
[12]
[13] Craig Burton, Chris Culnane, James Heather, Thea Peacock, Peter Y. A.
Ryan, Steve Schneider, Vanessa Teague, Roland Wen, Zhe Xia, and
Sriramkrishnan Srinivasan. Using Prˆet `a Voter in Victoria State Elections.
In J. Alex Halderman and Olivier Pereira, editors, 2012 Electronic Voting
Technology Workshop / Workshop on Trustworthy Elections, EVT/WOTE
’12, Bellevue, WA, USA, August 6-7, 2012. USENIX Association, 2012.
J. A. Calandrino, A. J. Feldman, J. A. Halderman, D. Wagner, H. Yu,
and W. P. Zeller. Source Code Review of the Diebold Voting System,
2007. Report commissioned as part of the California Secretary of State’s
[14]
794794
Top-To-Bottom Review of California voting systems. http://www.eecs.
berkeley.edu/∼daw/papers/dieboldsrc-ttbr.pdf.
[15] D. Chaum, R. Carback, J. Clark, A. Essex, S. Popoveniuc, R. L. Rivest,
P. Y. A. Ryan, E. Shen, and A. T. Sherman. Scantegrity II: End-to-
End Veriﬁability for Optical Scan Election Systems using Invisible
Ink Conﬁrmation Codes.
In USENIX/ACCURATE Electronic Voting
Technology (EVT 2008). USENIX Association, 2008. See also http:
//www.scantegrity.org/elections.php.
[16] Benoˆıt Chevallier-Mames, Pierre-Alain Fouque, David Pointcheval,
Julien Stern, and Jacques Traor´e. On Some Incompatible Properties
of Voting Schemes. In David Chaum, Markus Jakobsson, Ronald L.
Rivest, Peter Y. A. Ryan, Josh Benaloh, Miroslaw Kutylowski, and
Ben Adida, editors, Towards Trustworthy Elections, New Directions in
Electronic Voting, volume 6000 of Lecture Notes in Computer Science,
pages 191–199. Springer, 2010.
[17] M. R. Clarkson, S. Chong, and A. C. Myers. Civitas: Toward a Secure
Voting System. In 2008 IEEE Symposium on Security and Privacy (S&P
2008), pages 354–368. IEEE Computer Society, 2008.
[18] V´eronique Cortier, Fabienne Eigner, Steve Kremer, Matteo Maffei, and
Cyrille Wiedling. Type-Based Veriﬁcation of Electronic Voting Protocols.
In Proceedings of the 4th Conference on Principles of Security and
Trust (POST’15), Lecture Notes in Computer Science, London, UK,
April 2015. Springer.
[19] V´eronique Cortier, David Galindo, St´ephane Glondu, and Malika
Izabachene. Election Veriﬁability for Helios under Weaker Trust
Assumptions.
In Proceedings of the 19th European Symposium on
Research in Computer Security (ESORICS’14), LNCS, Wroclaw, Poland,
September 2014. Springer.
[20] V´eronique Cortier, David Galindo, Ralf K¨usters, Johannes M¨uller,
and Tomasz Truderung. Veriﬁability Notions for E-Voting Protocols.
Technical Report 2016/287, Cryptology ePrint Archive, 2016. Available
at http://eprint.iacr.org/2016/287.
[21] R. Cramer, R. Gennaro, and B. Schoenmakers. A Secure and Optimally
Efﬁcient Multi-Authority Election Scheme. In Advances in Cryptology
— EUROCRYPT ’97, International Conference on the Theory and
Application of Cryptographic Techniques, volume 1233 of Lecture Notes
in Computer Science. Springer-Verlag, 1997.
[22] Chris Culnane, Peter Y. A. Ryan, Steve A. Schneider, and Vanessa
Teague. vVote: A Veriﬁable Voting System. ACM Trans. Inf. Syst.
Secur., 18(1):3, 2015.
[23] Chris Culnane and Steve A. Schneider. A Peered Bulletin Board for
Robust Use in Veriﬁable Voting Systems.
In IEEE 27th Computer
Security Foundations Symposium, CSF 2014, Vienna, Austria, 19-22
July, 2014, pages 169–183. IEEE, 2014.
[24] Edouard Cuvelier, Olivier Pereira, and Thomas Peters.
Election
Veriﬁability or Ballot Privacy: Do We Need to Choose? In Jason
Crampton, Sushil Jajodia, and Keith Mayes, editors, Computer Security
- ESORICS 2013 - 18th European Symposium on Research in Computer
Security, Egham, UK, September 9-13, 2013. Proceedings, volume 8134
of Lecture Notes in Computer Science, pages 481–498. Springer, 2013.
Jeremy Epstein. Weakness in Depth: A Voting Machine’s Demise. IEEE
Security & Privacy, 13(3):55–58, 2015.
[25]
[26] David Galindo, Sandra Guasch, and Jordi Puiggali. 2015 Neuchˆatel’s
Cast-as-Intended Veriﬁcation Mechanism.
In Rolf Haenni, Reto E.
Koenig, and Douglas Wikstr¨om, editors, E-Voting and Identity - 5th
International Conference, VoteID 2015, Bern, Switzerland, September
2-4, 2015, Proceedings, volume 9269 of Lecture Notes in Computer
Science, pages 3–18. Springer, 2015.
[27] Gurchetan S. Grewal, Mark Dermot Ryan, Liqun Chen, and Michael R.
Clarkson. Du-Vote: Remote Electronic Voting with Untrusted Computers.
In C´edric Fournet, Michael W. Hicks, and Luca Vigan`o, editors, IEEE