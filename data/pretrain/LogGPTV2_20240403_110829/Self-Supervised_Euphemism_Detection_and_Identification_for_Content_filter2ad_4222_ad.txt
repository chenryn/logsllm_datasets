in a misrepresentation of the performance of the approaches;
2) Those euphemisms that indeed appear in the text corpus,
may not have been used in the euphemistic sense. For example,
‚Äúchicken" is a euphemism for ‚Äúmethamphetamine,‚Äù but it could
have been used only in the animal sense in the corpus.
Baselines: We compare our proposed approach with the
following competitive baseline models:
‚Ä¢ Word2vec: We use the word2vec algorithm [13], [14]
to learn the word embeddings (100-dimensional) for all
the words separately for the Drug, Weapon and Sexuality
datasets. We then choose as euphemism candidates those
words that are most similar to the input target keywords,
in terms of cosine similarity (average similarity between
the word and all input target keywords). This approach
relates words by implicitly accounting for the context in
which they occur.
‚Ä¢ TF-IDF + word2vec: Instead of treating all the words
in the dataset equally, this method Ô¨Årst ranks the words
by their potential to be euphemisms. Toward this, we
calculate the TF-IDF weights of the words [97] with
respect to a background corpus (i.e., Wikipedia9), which
captures a combination of the frequency of a word and
its distinct usage in a given corpus. The idea is inspired
by the assumption that words ranked higher based on
TF-IDF in the target corpus have a greater chance of
being euphemisms than those ranked lower [11]. After the
pre-selection by TF-IDF, we then generate the euphemism
candidates by following the Word2vec approach above.
‚Ä¢ CantReader10 [9] employs a neural-network based em-
bedding technique to analyze the semantics of words,
detecting the euphemism candidates whose contexts in
the background corpus (e.g., Wikipedia) are signiÔ¨Åcantly
diÔ¨Äerent from those in the dark corpus.
‚Ä¢ SentEuph [17] recognizes euphemisms by the use of
sentiment analysis. It lists a set of euphemism candidates
using a bootstrapping algorithm for semantic lexicon
induction. For a fair comparison with our approach, we
do not include the manual Ô¨Åltering stage of the algorithm
proposed by Felt and RiloÔ¨Ä [17].
‚Ä¢ EigenEuph [11] leverages word embeddings and a com-
munity detection algorithm, to generate a cluster of eu-
phemisms by the ranking metric of eigenvector centralities.
‚Ä¢ GraphEuph11 [40] also identiÔ¨Åes euphemisms using
word embeddings and a community detection algorithm.
SpeciÔ¨Åcally, it creates neural embedding models that
capture word similarities, uses graph expansion and the
PageRank scores [59] to bootstrap an initial set of seed
words, and Ô¨Ånally enriches the bootstrapped words to learn
out-of-dictionary terms that behave like euphemisms.
‚Ä¢ MLM-no-Ô¨Åltering is a simpler version of our proposed
approach and shares its architecture. The key diÔ¨Äerence
from our proposed approach is that instead of Ô¨Åltering the
noisy masked sentences, it uses them all to generate the
euphemism candidates. In eÔ¨Äect, this baseline serves as
an ablation to understand the eÔ¨Äect of the Ô¨Åltering stage.
For a fair comparison of the baselines, we experimented
with diÔ¨Äerent combinations of parameters and report the best
performance for each baseline method.
Results: Table VI summarizes the euphemism detection results.
Our proposed approach outperforms all the baselines by a wide
margin for the diÔ¨Äerent settings of the evaluation measure on
all the three datasets we studied.
The most robust baselines over the three datasets are TF-
IDF + word2vec, EigenEuph and MLM-no-Ô¨Åltering. When
compared with Word2vec, the superior performance of TF-
IDF + word2vec lies in its ability to select a set of potential
euphemisms by calculating the TF-IDF with a background
corpus (i.e., Wikipedia). While this pre-selection step works
well (relative to Word2vec) on the Drug and Sexuality datasets,
it does not impact the performance on the Weapon dataset. A
plausible explanation for this is that the euphemisms do not
occur very frequently in comparison with the other words in
the Weapons corpus and therefore, are not ranked highly by
the TF-IDF scores.
SentEuph [17]‚Äôs comparatively poor performance is ex-
plained by the absence of the required additional manual
Ô¨Åltering stage to reÔ¨Åne the results. As mentioned before, this
was done to compare the approaches based on their automatic
performance alone. GraphEuph [40] shows a reasonable per-
formance on the Drug dataset, but fails to detect weapon- and
sexuality-related euphemisms. This limits the generalization
of the approach that was tested only on a hate speech dataset
by Taylor et al. [40]. The approach of CantReader [9] seems
to be ineÔ¨Äective because not only does it require additional
corpora to make semantic comparisons‚Äîa requirement that is
ill-deÔ¨Åned because the nature of the additional corpora needed
for a given dataset is not speciÔ¨Åed‚Äîbut also because the
results of CantReader are quite sensitive to parameter tuning.
We were unable to reproduce the competitive results reported
by Yuan et al. [9], even after multiple personal communication
attempts with the authors. By comparing the performance
of our approach and that of the ablation MLM-no-Ô¨Åltering,
we conclude that the proposed Ô¨Åltering step is eÔ¨Äective in
eliminating the noisy masked sentences and is indispensable
for reliable results.
9https://dumps.wikimedia.org/enwiki/
10https://sites.google.com/view/cantreader
11https://github.com/JherezTaylor/hatespeech_codewords
Results on euphemism detection. Best results are in bold.
Table VI
Word2vec
TF-IDF + word2vec
CantReader [9]
SentEuph [17]
EigenEuph [11]
GraphEuph [40]
MLM-no-Ô¨Åltering
Our Approach
Word2vec
TF-IDF + word2vec
CantReader [9]
SentEuph [17]
EigenEuph [11]
GraphEuph [40]
MLM-no-Ô¨Åltering
Our Approach
Word2vec
TF-IDF + word2vec
CantReader [9]
SentEuph [17]
EigenEuph [11]
GraphEuph [40]
MLM-no-Ô¨Åltering
Our Approach
g
u
r
D
n
o
p
a
e
W
y
t
i
l
a
u
x
e
S
ùëÉ@10
0.10
0.30
0.00
0.10
0.30
0.20
0.30
0.50
0.30
0.30
0.20
0.00
0.30
0.00
0.30
0.40
0.10
0.40
0.10
0.10
0.20
0.00
0.50
0.70
ùëÉ@20
0.10
0.25
0.00
0.10
0.30
0.15
0.30
0.45
0.30
0.25
0.20
0.00
0.20
0.05
0.30
0.45
0.05
0.25
0.10
0.10
0.15
0.00
0.40
0.40
ùëÉ@30
0.09
0.20
0.07
0.07
0.30
0.13
0.28
0.47
0.27
0.20
0.17
0.03
0.13
0.03
0.20
0.37
0.07
0.20
0.07
0.08
0.13
0.03
0.30
0.33
ùëÉ@40
0.09
0.20
0.10
0.05
0.25
0.13
0.30
0.42
0.23
0.17
0.18
0.05
0.10
0.05
0.17
0.35
0.08
0.20
0.08
0.10
0.15
0.05
0.23
0.33
ùëÉ@50
0.08
0.16
0.08
0.08
0.22
0.14
0.26
0.46
0.18
0.16
0.16
0.06
0.08
0.04
0.18
0.32
0.08
0.20
0.06
0.08
0.16
0.04
0.22
0.28
ùëÉ@60
0.09
0.17
0.12
0.07
0.22
0.17
0.26
0.42
0.20
0.18
0.17
0.05
0.07
0.03
0.18
0.28
0.08
0.17