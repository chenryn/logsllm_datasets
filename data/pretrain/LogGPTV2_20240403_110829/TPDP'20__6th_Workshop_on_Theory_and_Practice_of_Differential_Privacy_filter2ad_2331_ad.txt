i=1
15
so
For the case when δ > 0, we use Lemma 1.5. Speciﬁcally, since Mi(xi) and Mi(x(cid:48)
i) are (ε, δ)-
i of probability at least 1 − δ such that for all yi, we
indistinguishable, there are events Ei and E(cid:48)
have
Thus, in the above analysis, we instead condition on the events E = E1 ∧ E2 ∧ ··· ∧ Ek and
E(cid:48) = E(cid:48)
k, redeﬁning our privacy losses as:
2 ∧ ··· ∧ E(cid:48)
1 ∧ E(cid:48)
M (y)
(cid:12)(cid:12)(cid:12)Lx→x(cid:48)
(cid:12)(cid:12)(cid:12)(cid:12)ln
Mi
(cid:12)(cid:12)(cid:12) ≤ k(cid:88)
(cid:12)(cid:12)(cid:12)Lx→x(cid:48)
(cid:18) Pr[M(xi) = yi|Ei]
i=1
Pr[M(x(cid:48)
(yi)
i) = yi|E(cid:48)
i]
(cid:12)(cid:12)(cid:12) ≤ k · ε.
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) ≤ ε.
(cid:19)
(cid:18) Pr[Mi(xi) = yi|Ei]
(cid:19)
(cid:18) Pr[M(x) = y|E]
i) = yi|E(cid:48)
i]
Pr[M(x(cid:48)) = y|E(cid:48)]
(cid:12)(cid:12)(cid:12) ≤ k · ε.
(cid:12)(cid:12)(cid:12)Lx→x(cid:48)
Pr[M(x(cid:48)
(yi)
Mi
.
,
Lxi→x(cid:48)
i
(yi) = ln
Mi
Lx→x(cid:48)
M (y) = ln
(cid:12)(cid:12)(cid:12)Lx→x(cid:48)
M (y)
(cid:12)(cid:12)(cid:12) ≤ k(cid:88)
i=1
Then we still have
By a union bound, the probability of the events E and E(cid:48) are at least 1 − k · δ, so by Lemma 1.5,
M(x) and M(x(cid:48)) are (kε, kδ)-indistinguishable, as required.
We now move to Advanced Composition.
Proof sketch of Lemma 2.4. We again focus on the δ = 0 case; the extension to δ > 0 is handled
similarly to the proof of Lemma 2.3. The intuition for how we can do better than the linear growth
in ε is that some of the yi’s will have positive privacy loss (i.e. give evidence for dataset x) and some
will have negative privacy loss (i.e. give evidence for dataset x(cid:48)), and the cancellations between
these will lead to a smaller overall privacy loss.
To show this, we consider the expected privacy loss
[Lx→x(cid:48)
E
yi←Mi(x)
Mi
(yi)].
By deﬁnition, this equals the Kullback-Liebler divergence (a.k.a. relative entropy)
which is known to always be nonnegative.
D(Mi(x)(cid:107) Mi(x(cid:48))),
We ﬁrst prove the following claim which shows that the expected privacy loss of a diﬀerentially
private mechanism is quite a bit smaller than the upper bound on the maximum privacy loss of ε.
Claim 2.5. If Mi is ε-diﬀerentially private, where ε ≤ 1, then
(yi)] ≤ 2ε2.
[Lx→x(cid:48)
E
yi←Mi(x)
Mi
16
Proof of Claim. We will show that
D(Mi(x)(cid:107)Mi(x(cid:48))) + D(Mi(x(cid:48))(cid:107)Mi(x)) ≤ 2ε2,
and then the result follows by the non-negativity of divergence. Now,
D(Mi(x)(cid:107)Mi(x(cid:48))) + D(Mi(x(cid:48))(cid:107)Mi(x)) =
=
E
E
yi←Mi(x)
yi←Mi(x)
Mi
[Lx→x(cid:48)
[Lx→x(cid:48)
Mi
(yi)] +
(yi)] −
and using the upper bound of ε on privacy loss we get that
E
E
yi←Mi(x(cid:48))
yi←Mi(x(cid:48))
Mi
[Lx(cid:48)→x
[Lx→x(cid:48)
Mi
(yi)]
(yi)],
E
yi←Mi(x(cid:48))
[Lx→x(cid:48)
Mi
(yi)]
(cid:12)(cid:12)(cid:12)Lx→x(cid:48)
Mi
(cid:12)(cid:12)(cid:12)(cid:19)
(yi)
· SD(Mi(x), Mi(x(cid:48)))
yi∈Supp(Mi(x))∪Supp(Mi(x(cid:48)))
max
(cid:18)
E
[Lx→x(cid:48)
Mi
(yi)] −
yi←Mi(x)
≤ 2 ·
≤ 2ε · (1 − e−ε)
≤ 2ε2,
where SD is statistical distance, and we use the fact that (ε, 0)-indistinguishability implies a sta-
tistical distance of at most 1 − e−ε.
Thus by linearity of expectation, for the overall expected privacy loss, we have:
[Lx→x(cid:48)
M (y)] = k · O(ε2) def= µ.
E
y←M(x)
Applying the Hoeﬀding Bound for random variables whose absolute value is bounded by ε, we get
that with probability at least 1 − δ(cid:48) over y ← M(x), we have:
(cid:16)(cid:112)k log(1/δ(cid:48))
(cid:17) · ε ≤ O
(cid:16)(cid:112)k log(1/δ(cid:48))
(cid:17) · ε def= ε(cid:48),
M (y) ≤ µ + O
Lx→x(cid:48)
√
where the second inequality uses the assumption that k  ε(cid:48)(cid:105)
(cid:88)
+
Pr
y←M(x)
≤ δ(cid:48) +
y∈T :Lx→x(cid:48)
M (y)≤ε(cid:48)
≤ δ(cid:48) + eε(cid:48) · Pr[M(x(cid:48)) ∈ T ],
(cid:88)
Pr[M(x) = y]
y∈T :Lx→x(cid:48)
M (y)≤ε(cid:48)
eε(cid:48) · Pr[M(x(cid:48)) = y]
so M is indeed (ε(cid:48), δ(cid:48))-diﬀerentially private.
It should be noted that, although Lemma 2.4 is stated in terms of queries being asked simulta-
neously (in particular, nonadaptively), a nearly identical proof (appealing to Azuma’s Inequality,
17
instead of Hoeﬀding) shows that an analogous conclusion holds even when the queries (i.e. mecha-
nisms) are chosen adaptively (i.e. the choice of Mi+1 depends on the outputs of M1(x), . . . , Mi(x)).
Observe that if we have a set Q of k = |Q| counting queries and we wish to obtain a ﬁnal privacy of
(ε, δ(cid:48)), then we can achieve this by ﬁrst adding Laplace noise to achieve an initial privacy guarantee
of ε0 for each query and then use the composition theorems. To use the Basic Composition Lemma,
we would have to set
so the Laplace noise added per query has scale
ε0 =
ε
k
,
(cid:18) 1
(cid:19)
ε0n
O
= O
(cid:19)
.
(cid:18) k
εn
To obtain a bound on the maximum noise added to any of the queries, we can do a union bound
over the k queries. Setting β = 1/O(k) in Theorem 1.3, with high probability, the maximum noise
will be at most
(cid:18) k · log k
(cid:19)
.
εn
α = O
Steinke and Ullman [98] showed how to save the log k factor by carefully correlating the noise used
for the k queries, and thus showed:
Theorem 2.6 (arbitrary counting queries with pure diﬀerential privacy [98]). For every set Q of
counting queries and ε > 0, there is an ε-diﬀerentially private mechanism M : Xn → RQ such that
on every dataset x ∈ Xn, with high probability M(x) answers all the queries in Q to within additive
error
(cid:18)|Q|
(cid:19)
.
εn
α = O
Thus, taking ε to be constant, we can answer any |Q| = o(n) counting queries with vanishingly
small error, which we will see is optimal for pure diﬀerential privacy (in Section 5.2).
Similarly, to use the Advanced Composition Theorem, we would have to set
ε0 =
yielding a maximum error of
α = O
(cid:18) log k
(cid:19)
ε0n
= O
,
ε
c ·(cid:112)k · log(1/δ)
(cid:32)(cid:112)k · log(1/δ) · log k
(cid:33)
.
εn
Again, it is known how to (mostly) remove the log k factor:
Theorem 2.7 (arbitrary counting queries with approximate diﬀerential privacy [98]). For every
set Q of counting queries over data universe X, and ε, δ > 0, there is an (ε, δ)-diﬀerentially private
mechanism M : Xn → Rk such that on every dataset x ∈ Xn, with high probability M(x) answers
all the queries to within error
(cid:32)(cid:112)|Q| · log(1/δ) · log log |Q|
(cid:33)
.
α = O
εn
18
Again taking ε to be constant and δ to be negligible (e.g. δ = 2− log2(n)), we can take k = |Q| =