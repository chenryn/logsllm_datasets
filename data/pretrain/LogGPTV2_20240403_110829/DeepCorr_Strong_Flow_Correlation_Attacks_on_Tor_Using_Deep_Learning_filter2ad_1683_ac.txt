DeepCorr declares the flows i and j to be correlated if pi, j > η,
where η is our detection threshold discussed during the experiments.
The parameters (w1, w2, k1, k2) are the hyperparameters of our
system; we will tune their values through experiments.
3.3 Training
To train our network, we use a large set of flow pairs that we
created over Tor. This includes a large set of associated flow pairs,
and a large set of non-associated flow pairs. An associated flow
pair, Fi, j, consists of the two segments of a Tor connection (e.g.,
i and j are the ingress and egress segments of a Tor connection).
We label an associated pair with yi, j = 1. On the other hand, each
non-associated flow pair (i.e., a negative sample) consists of two
arbitrary Tor flows that do not belong to the same Tor connection.
We label such non-associated pairs with yi, j = 0. For each captured
Tor entry flow, i, we create Nneд negative samples by forming Fi, j
pairs where j is the exit segment of an arbitrary Tor connection.
Nneд is a hyperparameter whose value will be obtained through
experiments.
4Note that our work is the first to use a learning mechanism for flow correlation. In
our search of effective learning mechanisms for flow correlation, we tried various
algorithms including fully connected neural networks, recurrent neural network (RNN),
and support vector machine (SVM). However, CNN provided the best flow correlation
performance compared to all the other algorithms we investigated, which is intuitively
because CNNs are known to work better for longer data lengths. For instance, we
achieved an accuracy of only 0.4 using fulling-connected neural networks, which is
significantly lower than our performance with CNNs.
5
Figure 2: The network architecture of DeepCorr.
Finally, we define DeepCorr’s loss function using a cross-entropy
yi, j log Ψ(Fi, j) + (1 − yi, j) log(1 − Ψ(Fi, j))
(1)

function as follows:
L = − 1
|F |
Fi, j ∈F
where F is our training dataset, composed of all associated and
non-associated flow pairs. We used the Adam optimizer [40] to
minimize the loss function in our experiments. The learning rate
of the Adam optimizer is another hyperparameter of our system.
4 EXPERIMENTAL SETUP
In this section, we discuss our data collection and its ethics, the
choice of our hyperparameters, and our evaluation metrics.
4.1 Datasets and Collection
Figure 3 shows our experimental setup for our Tor experiments.
We used several Tor clients that we ran inside separate VMs to
generate and collect Tor traffic. We use each of our Tor clients to
browse the top 50,000 Alexa websites over Tor, and captured the
flows entering and exiting the Tor network for these connections
(we use half of the connections for training, and the other half for
testing). Therefore, the entering flows are in Tor cell format, and
the flows exiting Tor are in regular HTTP/HTTPS format. We used
1,000 arbitrary Tor circuits for browsing websites over Tor, i.e., each
circuit was used to browse roughly 50 websites. We used different
guard nodes in forming our Tor circuits; we were able to alternate
our guard nodes so by disabling Vanilla Tor’s option that enforces
guard relay reuse. We also used a regular Firefox browser, instead
of Tor’s browser, to be able to enforce circuit selection. We used
Tor version 0.3.0.9, automated by a Python script.
Note that we did not set up our own Tor relays for the purpose
of the experiments, and we merely used public Tor relays in all of
our experiments. We captured the ingress Tor flows using tcpdump
on our Tor clients. To capture the egress Tor traffic (i.e., traffic
from exit relays to websites), we made our exit Tor traffic tunnel
through our own SOCKS proxy server (as shown in Figure 3), and
we collected the exit Tor traffic on our own SOCKS proxy server
6
using tcpdump. Note that using this data collection proxy may
add additional latency on the collected flows, so the performance
of DeepCorr in practice is better than what we report through
experiments. We also collected 500 websites through Tor pluggable
transport to evaluate them as countermeasures against DeepCorr.
We collected our Tor traffic in two steps: first, we collected traffic
over a two weeks period, and then with a three months gap we
collected more Tor traffic for a one month period (in order to show
the impact of time on training). We have made our dataset available
publicly. To the best of our knowledge, this is largest dataset of
correlated Tor flows, and we hope it will be useful to the research
community.
Note that while we only collect web traffic, this is not a constraint
of DeepCorr, and it can be used to correlate arbitrary Tor traffic.
4.2 Ethics of Data Collection
To make sure we did not overload Tor’s network, we ran up to 10
concurrent Tor connections during our data collection. Also, we
alternated the guard nodes used in our circuits to evade overloading
any specific circuits or relays. We did not browse any illegal content
over Tor, and we used an idle time between connections of each of
our clients. As explained above, we collected our ingress and egress
Tor flows on our own Tor clients as well as our own SOCKS proxy
server; therefore, we did not collect any traffic of other Tor users.
In our experiments with Tor pluggable transports, we collected
a much smaller set of flows compared to our bare Tor experiments;
we did so because Tor bridges are very scarce and expensive, and
therefore we avoided overloading the bridges.
4.3 Choosing the Hyperparameters
We used Tensorflow [1] to implement the neural networks of Deep-
Corr. We tried various values for different hyperparameters of our
system to optimize the flow correlation performance. To optimize
each of the parameters, our network took about a day to converge
(we used a single Nvidia TITAN X GPU).
For the learning rate, we tried {0.001, 0.0001, 0.0005, 0.00005},
and we got the best performance with a learning rate of
TimeFeatureSizeFeatureSizeFeatureTimeFeaturePacket Sizes from entry (Sid)Packet Sizes to exit (Sjd)Packet Sizes from exit (Sju)Packet Sizes to entry (Siu)  pi,jFlattenMax PoolingMax PoolingMax PoolingMax Pooling4×l×k11×l×k2Fully ConnectedIPD to entry (Tiu)IPD from exit (Tju)IPD from entry (Tid)IPD to exit (Tid)8×l4×l×k1Fi,jMax PoolingFigure 3: Our experimental setup on Tor
Table 1: DeepCorr’s hyperparameters optimized to correlate
Tor traffic.
Layer
Convolution Layer 1
Max Pool 1
Convolution Layer 2
Max Pool 2
Fully connected 1
Fully connected 2
Fully connected 3
Details
Kernel num: 2000
Kernel size: (2, 30)
Activation: Relu
Window size: (1,5)
Stride: (2,1)
Stride: (1,1)
Kernel nume: 1000
Kernel size: (2, 10)
Activation: Relu
Window size: (1,5)
Stride: (4,1)
Stride: (1,1)
Size: 3000, Activation: Relu
Size: 800, Activation: Relu
Size: 100, Activation: Relu
0.0001. As for the number of negative samples, Nneд, we tried
{9, 49, 99, 199, 299} and 199 gave us the best results. For the window
sizes of the convolution layers, w1 and w2, we tried {5, 10, 20, 30}.
Our best results occurred with w1 = 30 and w2 = 10. We also exper-
imented with {2, 5, 10} for the size of the max pooling, and a max
pooling of 5 gave the best performance. Finally, for the number of
the kernels, k1, k2, we tried {500, 1000, 2000, 3000}, and k1 = 2000
and k2 = 1000 resulted in the best performance. We present the
values of these parameters and other parameters of the system in
Table 1.
4.4 Evaluation Metrics
Similar to previous studies, we use the true positive (TP) and false
positive (FP) error rates as the main metrics for evaluating the
performance of flow correlation techniques. The TP rate measures
the fraction of associated flow pairs that are correctly declared to
be correlated by DeepCorr (i.e., a flow pair (i,j) where i and j are the
segments of the same Tor connection, and we have pi, j > η). On the
other hand, the FP rate measures the fraction of non-associated flow
pairs that are mistakenly identified as correlated by DeepCorr (e.g.,
when i and j are the segments of two unrelated Tor connections,
yet pi, j > η). To evaluate FP, DeepCorr correlates every collected
entry flow to every collected exit flow, therefore, we perform about
7
(25, 000 − 1)2 false correlations for each of our experiments (we
have 25, 000 Tor connections in our test dataset).
Note that the detection threshold η makes a trade off between the
FP and TP rates; therefore we make use of ROC curves to compare
DeepCorr to other algorithms.
Finally, in our comparisons with RAPTOR [69], we additionally
use the accuracy metric (the sum of true positive and true negative
correlations over all correlations), which is used in the RAPTOR
paper. To have a fair comparison, we derive the accuracy metric
similar to RAPTOR: each flow is declared to be associated with
only a single flow out of all evaluated flows, e.g., the flow that
results in the maximum correlation metric, pi, j. For the rest of our
experiments, each flow can be declared as correlated with arbitrary
number of intercepted flows (i.e., any pairs that pi, j > η), which is
a more realistic (and more challenging) setting.
5 EXPERIMENT RESULTS
In this section we present and discuss our experimental results.
5.1 A First Look at the Performance
As described in the experimental setup section, we browse 50,000
top Alexa websites over Tor and collect their ingress and egress
flow segments. We use half of the collected traces to train DeepCorr
(as described earlier). Then, we use the other half of the collected
flows to test DeepCorr. Therefore, we feed DeepCorr about 25, 000
pairs of associated flow pairs, and 25, 000 × 24, 999 ≈ 6.2 × 108
pairs of non-associated flow pairs for training. We only use the first
ℓ = 300 packets of each flow (for shorter flows, we pad them to
300 packets by adding zeros). Figure 4 presents the true positive
and false positive error rates of DeepCorr for different values of the
threshold η. As expected, η trades off the TP and FP error rates. The
figure shows a promising performance for DeepCorr in correlating
Tor flows—using only 300 packets of each flow. For instance, for a
FP of 10−3, DeepCorr achieves a TP close to 0.8. As shown in the
following, this is drastically better than the performance of previous
work. Note that increasing the length of the flows will increase the
accuracy, as shown later.
5.2 DeepCorr Can Correlate Arbitrary Circuits
and Destinations
As discussed earlier, DeepCorr learns a correlation function for
Tor that can be used to correlate Tor flows on—any circuits—and
to—any destinations—regardless of the circuits and destinations
Figure 4: True positive and false positive error rates of Deep-
Corr in detecting correlated pairs of ingress and egress Tor
flows for different detection thresholds (η). Each flow is only
300 packets.
used during the training process. To demonstrate this, we compare
DeepCorr’s performance in two experiments, each consisting 2, 000
Tor connections, therefore 2, 000 associated pairs and 2, 000× 1, 999
non-associated flow pairs. In the first experiment, the flows tested
for correlation by DeepCorr use the same circuits and destinations
as the flows used during DeepCorr’s training. In the second experi-
ment, the flows tested for correlation by DeepCorr (1) use circuits
that are totally different from the circuits used during training, (2)
are targeted to web destinations different from those used during
training, and (3) are collected one week after the learning flows.
Figure 5 compares DeepCorr’s ROC curve for the two experiments.
As can be seen, DeepCorr performs similarly in both of the experi-
ments, demonstrating that DeepCorr’s learned correlation function
can be used to correlate Tor flows on arbitrary circuits and to arbi-
trary destinations. The third line on the figure shows the results
when the training set is three months old, showing a degraded
performance, as further discussed in the following.
5.3 DeepCorr Does Not Need to Re-Train
Frequently
Since the characteristics of Tor traffic change over time, any
learning-based algorithm needs to be re-trained occasionally to
preserve its correlation performance. We performed two experi-
ments to evaluate how frequently DeepCorr needs to be retrained.
In our first experiment, we evaluated our pre-trained model over
Tor flows collected during 30 consecutive days. Figure 6 presents
the output of the correlation function for each of the days for both
associated and non-associated flow pairs. As we can see, the corre-
lation values for non-associated flows do not change substantially,
however, the correlation values for associated flows starts to slightly
degrade after about three weeks. This suggests that an adversary
Figure 5: DeepCorr’s performance does not depend on the
circuits and destinations used during the training phase.
Figure 6: DeepCorr’s correlation values for associated and
non-associated flows for 30 consecutive days without re-
training. The performance only starts to drop after about
three weeks.
will need to retrain her DeepCorr only every three weeks, or even
once a month.
As an extreme case, we also evaluated DeepCorr’s performance
using a model that was trained three months earlier. Figure 5 com-
pares the results in three cases: three months gap between training
and test, one week gap between training and test, and no gap. We see
that DeepCorr’s accuracy significantly degrades with three months
gap between training and test—interestingly, even this significantly
degraded performance of DeepCorr due to lack of retraining is
superior to all previous techniques compared in Figure 10.
8
0.40.60.81.0TP0.00.20.40.60.81.0Threshold(η)10−510−410−310−2LogscaleFP10−510−410−310−210−1FalsePositive0.00.20.40.60.81.01.21.4TruePositiveTrainedonthesamecircuits,destinationsandtimeTrainedondiﬀerentcircuits,destinationsandoneweekgapTesteddatawiththreemonthsgapRandomGuess051015202530Day0.00.20.40.60.81.0CorrelationvalueCorrelatedNon-correlatedFigure 7: DeepCorr’s performance is consistent regardless of
the size of the testing dataset (we use a fixed, arbitrary η).
Figure 8: DeepCorr’s performance rapidly improves when
using longer flows for training and testing.
5.4 DeepCorr’s Performance Does Not Degrade
with the Number of Test Flows
We also show that DeepCorr’s correlation performance does not
depend on the number of flows being correlated, i.e., the size of the
test dataset. Figure 7 presents the TP and FP results (for a specific
threshold) on datasets with different numbers of flows. As can be
seen, the results are consistent for different numbers of flows being
correlated. This suggests that DeepCorr’s correlation performance
will be similar to what derived through our experiments even if
DeepCorr is applied on significantly larger datasets of intercepted
flows, e.g., on the flows collected by a large malicious IXP.
5.5 DeepCorr’s Performance Rapidly Improves
with Flow Length
In all of the previous results, we used a flow length of ℓ = 300
packets. As can be expected, increasing the length of the flows
used for training and testing should improve the performance of
DeepCorr. Figure 8 compares DeepCorr’s performance for different
lengths of flows, showing that DeepCorr’s performance improves
significantly for longer flow observations. For instance, for a target
FP of 10−3, DeepCorr achieves T P = 0.62 with ℓ = 100 packets long
flows, while it achieves T P = 0.95 with flows that contain ℓ = 450
packets.