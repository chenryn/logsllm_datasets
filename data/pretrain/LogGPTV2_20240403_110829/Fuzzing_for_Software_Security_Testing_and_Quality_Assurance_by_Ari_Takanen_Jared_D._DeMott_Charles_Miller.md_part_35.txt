7.8.1 Distributed Fuzzing: Google’s ClusterFuzz
Google has created a fuzzing infrastructure they call ClusterFuzz. It is probably
one of the more useful, and practical applications of distributed fuzzing in terms of
application security. That is, the whole platform is geared around helping developers
find and fix bugs. Whereas other fuzzing tools, have been used more by security
researchers poking at closed-source binaries.
Originally created to help secure their Chrome Web browser, Google is using
scale of ClusterFuzz to secure other components relevant for many of our online
activities. The newest addition to Google’s ClusterFuzz project is OSS-Fuzz.16 OSS-
Fuzz was published as a beta program at the end of 2016. The goal of the program
is to make common software infrastructure more secure and stable by providing
a massive distributed fuzzing environment for widely used open-source projects.
The fuzzing environment is powered by Google ClusterFuzz infrastructure and at
the time of the release it was executing approximately 4 trillion test cases per week.
Google’s fuzzing infrastructure is built on top of a pool of several hundred vir-
tual machines (VMs). With that kind of volume, there would be too much output if
they just automated the test case generation and crash detection. That’s why Google
has also automated the entire fuzzing pipeline, including the following processes:
• Managing test cases and infrastructure: To run at maximum capacity they
need to generate a constant stream of test cases, distribute them across thou-
sands of instances running on hundreds of virtual machines, track the results,
and collect code coverage feedback.
16 https://testing.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html.
6760 Book.indb 243 12/22/17 10:50 AM
244 Advanced Fuzzing
• Analyzing crashes: ClusterFuzz executes test cases on multiple different plat-
forms, using different instrumentation tools like AddressSanitizer, to detect
different types of bugs. Those bugs are bucketed to filter out duplicates and
issues that cannot be reproduced.
• Minimizing test cases: Fuzzer test cases are often very large files—usually as
much as several hundred kilobytes each. Because of this, they take the gener-
ated test cases and distill them down to the few, essential pieces that actually
trigger the crash. This may also be called test case reduction.
• Identifying regressions: The first step in getting a crash fixed is figuring out
where it is and who should fix it. So this phase tracks the crash down to the
range of changes that introduced it.
• Verifying fixes: In order to verify when a crash is actually fixed, it will run
the open crash cases against each new LKGR build.
In addition to manageability, this level of scale and automation provides a very
important additional benefit. By aggressively tracking the LKGR builds, ClusterFuzz
also implements a real-time security regression detection capability.
7.8.2 Distributed Fuzzing: DeMott’s ClusterFuzz
DeMott’s ClusterFuzz (CF) is a distributed computing framework that facilitates
fuzzing of larger data input sets. See DeMott’s dissertation17 to see how it was also
used as a front end to a postmortem crash analysis tool for a technique calledfault-
localization. ClusterFuzz does fuzzing in parallel, making fuzzing more efficient,
and does not require source code or a test set to generate tests. The speed-up is lin-
ear as resources are added to CF; for example, a fuzzing run that would have taken
200 days on one computer can be done in 1 day on 200 virtual machines in CF.
Once an application has been fuzzed and bug results have been generated,
ClusterFuzz proceeds to do further evaluation of the results. First, it examines the
generated bugs and attempts to cluster them based on the similarity. Second, it
attempts to rate the severity of the observed bugs.
Figure 7.27 shows the high-level design for ClusterFuzz. Each of the major sec-
tions from the diagram is briefly described below.
Configure. The first thing the bug hunter does is choose a target to fuzz. That
target, also known as the system under test, must then be properly installed in a
base virtual machine (VM). Windows XP was used for the base VM operating
system (OS) as it is easier to configure and requires fewer resources. Windows in
general was chosen both because the Peach fuzzing tool, which is used in this work,
operates best under Windows, and because many interesting targets to fuzz, such
as Microsoft’s Internet Explorer, reside only on Windows. However, it is possible
to use a different base OS and a different fuzzer with ClusterFuzz. Next, the test
configuration must be constructed specifically for the SUT. The test configuration
specifies all the details surrounding the setup of the SUT: input types, execution
17 http://www.vdalabs.com/tools/DeMott_Dissertation.pdf.
6760 Book.indb 244 12/22/17 10:50 AM
7.8 Distributed Fuzzing 245
Figure 7.27 DeMott’s ClusterFuzz.
requirements, and so forth. The test configuration includes the data model (DM),
which describes the protocol required for intelligent fuzzing (if desired). To aid the
user, a GUI was created, which includes an option to automatically create a simple
mutation DM, thus bypassing the manual work of protocol description. The base
DM often yields reasonable results because the input samples are intelligently gath-
ered (using Auto-MinSet described below). A common practice is to run the base
DM initially and then later design a custom DM for the particular application if
budget allows.
Run. The present setup has six VMware ESX 4.0 virtualization servers. Based
on current hardware, each of the servers can handle approximately 25 virtual
machines cloned from the base image. Therefore, 150 VMs are running on hard-
ware that cost $30,000 USD when procured (October 2009). In practice, this is less
expensive and easier to manage than buying 150 machines. Each virtual machine
has generous amounts of RAM and CPU cycles. These VMs may be used for a
variety of purposes, such as:
• Auto-MinSet is a technique created as part of this research to find good input
samples from the internet for a given file format protocol. Auto-MinSet uses code
coverage to find an optimal test set from samples, which were automatically
6760 Book.indb 245 12/22/17 10:50 AM
246 Advanced Fuzzing
searched for and downloaded. The method for choosing samples wisely is referred
to as MinSet, since the goal is to select the minimum set of files that obtains that
maximum amount of code coverage. The MinSet subroutine in Algorithm 7.1
provides the details briefly described here, MinSet:
1. Gathers a list of inputs and locates the input that covers the most code.
2. Next, the covered regions of each input are tested against the pool of
regions covered thus far. If new regions are covered, the input is deemed
worthy to be added to the minimum set of desired inputs. Otherwise,
that input is discarded.
• Auto-Retest, which checks the reliability of discovered bugs.
• Fuzzing.
Collect. Once the fuzzing sessions have completed, the ordered (from !exploit-
able, described in the next section) bug results are collected. The CF GUI provides a
convenient way for analysts to view each bug and create notes as they work through
the pipeline of results. CF is described again in Algorithm 7.1.
Algorithm 7.1. ClusterFuzz: Bug Hunting and Analysis System
Input: A test configuration controls the fuzzing of each system under test and is also
typically used to automatically fetch good inputs. Each data sample in the collected
set of Inputs is fuzzed to create a broad set of test inputs. Fuzzed permutations of
an input i are denoted as i′. The most common EXCEPTION sought is a memory
access violation.
Output: Reliable, severe, and unique bugs (defined shortly) are sorted (in bins) in
the set Results
Inputs=DownloadSamples(Test_Configuration );
Inputs=MinSet(Test_Configuration );
for each input i in Inputsdo
for each fuzzedpermutation i′ of ido
result, exception_data = Test(Test_Configuration, i′);
if(result) == EXCEPTION
then
logs=Log(exception_data, i′, i)
end
end
end
Results = Combine_Bugs_from_VMs (logs);
Results = Order_Within_Bins(Results);
Results = AutoRetest(Results);
Subroutine MinSet(Inputs):
largest, coverage = set();
CC = dictionary{ input: BasicBlocks}
for each input i in Inputsdo
CC[i] = Get_Set_of_BBs_Executed(i);
iflength(largest)<length(CC[i])
6760 Book.indb 246 12/22/17 10:50 AM
7.8 Distributed Fuzzing 247
then
largest = CC[i];
end
end
minset = [ ];
coverage = largest;
for each input i in Inputsdo
if any of CC[i] not in coverage
then
minset += i;
coverage.update(CC[i])
end
end
Return minset
To demonstrate the robustness of ClusterFuzz, it was tested on a variety of com-
mercial, closed-source software. The following are applications and data formats
that have been fuzzed using ClusterFuzz:
• Client-side applications
– Browsers
– Internet Explorer, Chrome, Firefox, Safari, Opera
– Office Applications
– Writer (Open Office), Word (Microsoft Office), Adobe Reader, Adobe
Flash Player, Picture Manager (Microsoft Office)
– Other
– iTunes, QuickTime, Java, VLC Media Player, Windows Media Player,
RealPlayer
• File formats
– Images
– JPG, BMP, PNG, TIFF, GIF
– Video
– AVI, MOV
– Office
– DOC, DOCX, XLS, XLSX, ODT
– Adobe
– PDF, SWF
Statistics were gathered over one month of running CF on the previously listed
applications, providing many of the file formats as input. Not every combination
produced a fault, but when faults were noted they were recorded as collective results,
which are as follows:
• 141,780 faults total
– faults/day: 4,726
– faults/hour: 197
6760 Book.indb 247 12/22/17 10:50 AM
248 Advanced Fuzzing
• 828 total unique fault bins
– 17 “Exploitable” bins
– 6 “Probably exploitable” bins
– 0 “Probably not exploitable” bins
– 805 “Unknown” bins
– Unique fault bins/day: 28
– Unique bins “probably exploitable” or “exploitable” /day: 0.9
Many observed faults are actually manifestations of the same fault, which is
why the word “bin” is used above to show each grouping of bugs. Classifying faults
is important to reduce downstream effort and a tool exists to do just that.
The !exploitable tool is a Windows debugging (Windbg) extension that provides
automated crash analysis, security risk assessment, and guidance. The tool uses a
hash to uniquely identify a crash and then assigns one of the following exploitability
ratings to the crash: Exploitable, Probably Exploitable, Probably Not Exploitable,
or Unknown. In an attempt to identify the uniqueness of each bug, two hash types
are provided: major and minor. The difference is that the minor hash hashes more
of the stack information, and therefore attempts to provide a finer amount of detail
when sorting bugs.
The data shown above helps illustrate how !exploitable works, and the impor-
tance of filtering in fuzzing. While there may be many crashes, often there are many
fewer unique and security-critical bugs. These high quality bugs are what developers
and bug exploiters usually want to focus on. In this research, high quality (HQ)
is defined as: reliable (repeatable when retested) and severe (a rating of Probably
Exploitable or higher).
On top of the classification provided by !exploitable, CF also collects and stores
relevant crash information such as the registers used and nearby disassembly code
in the database. This information allows researchers to later search for particular
register patterns as they become widely known. For example, when the Intel regis-
ters ECX and EIP contain the exact same value that is a condition that may indicate
the presence of an exploitable C++ structured exception handler (SEH) overwrite.
The !exploitable output provides a rough sorting mechanism according to type
and severity. However, by looking back at Figure 7.26, notice that each unique
grouping of bugs must still be analyzed in the final section marked “work.” Fur-
ther information about the crash such as the location of the actual erroneous code
block (fault localization) and other metadata (visualizing blocks covered, etc.) can
expedite the analysis process.
7.9 Summary
This chapter discussed advanced fuzzing techniques, fuzzing engines, and scalable
fuzzing frameworks used to find bugs in modern software. Generation fuzzers with
high code coverage (CC) perform the best, but the issue focused on in this chapter is
developing methods to automatically generate data, so this doesn’t have to be done
manually. The focus was on technologies that automatically increase CC by either
solving branch constraints or by evolving groups of inputs.
6760 Book.indb 248 12/22/17 10:50 AM
C h a p t e r 8
Fuzzer Comparison
In this book, we’ve discussed a number of different ways to fuzz systems as well as
numerous implementations of these ideas. We’ve talked about ways to place fuzz-
ing technologies into your SDLC and how to measure their success. This chapter is
focused on comparing and contrasting some different fuzzers that were available
at the time when the first edition of this book was written. Although the fuzzers
presented are not considered cutting edge anymore, the results can still help you
when deciding which type of fuzzer would be best for your situation given your
individual time and money constraints.
8.1 Fuzzing Life Cycle
Some fuzzers are simple input generators. Some consist of an entire framework
designed to help with the many stages of fuzzing. Let’s take a closer look at all the
necessary steps required to conduct fuzzing in order to get a better understanding
of the fuzzers we are going to compare.
8.1.1 Identifying Interfaces
Given a target system, we need to identify all the interfaces that accept outside input.
An example of such an interface may be a network socket consisting of a protocol
(TCP or UDP) along with a port number. Another option may be a specially format-
ted file that is read by the system. In order to increase the testing coverage, all these
interfaces need to be identified before you begin fuzzing. Sometimes these interfaces
will not be obvious. For example, most Web browsers not only parse HTTP, but
also FTP, RTSP, as well as various image formats.
8.1.2 Input Generation
The heart and soul of a fuzzer is its ability to create fuzzed inputs. As we’ve dis-
cussed, there are a large variety of different ways in which to create these test cases.
We’ll step through some of the different ways to generate fuzzed inputs, starting
from those that have little or no knowledge of the underlying protocol or structure
of the input and advancing to those that possess a near complete understanding of
the structure of the inputs.
The most basic way to create anomalous input is to simply supply purely random
data to the interface. In theory, given enough time, completely random data would
249
6760 Book.indb 249 12/22/17 10:50 AM
250 Fuzzer Comparison
result in all possible inputs to a program. In practice, time constraints limit the
effectiveness of this approach. The result is, that due to its simplicity, this approach
is unlikely to yield any significant results.
Another way to generate inputs is to use a mutation-based approach. This method
consists of first gathering valid inputs to the system and then adding anomalies to
these inputs. These valid inputs may consist of a network packet capture or valid
files or command line arguments, to name a few. There are a variety of ways to add
anomalies to these inputs. They may be added randomly, ignoring any structure
available in the inputs. Alternatively, the fuzzer may have some built-in knowledge
of the protocol and be able to add the anomalies in a more intelligent fashion. Some
fuzzers present an interface, either through a programming API or a GUI, in which
information about the format of the inputs can be expressed to the fuzzer. More
sophisticated fuzzers attempt to automatically analyze the structure of the inputs
and identify common format occurrences and protocol structures, for example,
ASN.1. Regardless of how it actually occurs, these types of fuzzers work on the
same general principle: start from valid inputs and add a number of anomalies to
the inputs to generate fuzzed inputs.
Generation-based fuzzers do not require any valid test cases. Instead, this type
of fuzzer already understands the underlying protocol or input format. They can
generate inputs based purely on this knowledge. Obviously, the quality of the fuzzed
inputs is going to depend on the level of knowledge the fuzzer has about the under-
lying input format. Generation-based fuzzers, such as Peach, SPIKE and Sulley,
offer a framework in which the researcher can read the documentation, write a
format specification, and use the framework to generate fuzzed inputs based on the
specification. Writing such a specification can be a major undertaking requiring
specialized knowledge and sometimes hundreds of hours of work for complicated
protocols. Since most people lack the specialized knowledge or time to write such
protocol descriptions, commercial vendors exist who provide fuzzers that understand
many common protocols. A drawback of these solutions is if you are interested in