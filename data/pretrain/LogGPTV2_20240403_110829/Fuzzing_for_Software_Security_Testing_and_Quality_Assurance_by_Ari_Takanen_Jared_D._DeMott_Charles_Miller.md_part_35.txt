### 7.8.1 Distributed Fuzzing: Google’s ClusterFuzz

Google has developed a fuzzing infrastructure known as ClusterFuzz, which is one of the most practical and useful applications of distributed fuzzing in the realm of application security. The platform is designed to assist developers in identifying and fixing bugs. Unlike other fuzzing tools that are primarily used by security researchers to test closed-source binaries, ClusterFuzz is geared towards helping developers improve the security of their applications.

Originally created to enhance the security of the Chrome web browser, Google has expanded the use of ClusterFuzz to secure other components critical to many online activities. The latest addition to the ClusterFuzz project is OSS-Fuzz, which was launched as a beta program at the end of 2016. The goal of OSS-Fuzz is to enhance the security and stability of widely used open-source projects by providing a massive, distributed fuzzing environment. At the time of its release, OSS-Fuzz was executing approximately 4 trillion test cases per week.

Google’s fuzzing infrastructure is built on a pool of several hundred virtual machines (VMs). Given the scale, simply automating test case generation and crash detection would generate an overwhelming amount of output. Therefore, Google has automated the entire fuzzing pipeline, including the following processes:

- **Managing Test Cases and Infrastructure**: To operate at maximum capacity, a constant stream of test cases must be generated, distributed across thousands of instances running on hundreds of VMs, and the results must be tracked with code coverage feedback.
- **Analyzing Crashes**: ClusterFuzz executes test cases on multiple platforms using different instrumentation tools like AddressSanitizer to detect various types of bugs. These bugs are then categorized to filter out duplicates and non-reproducible issues.
- **Minimizing Test Cases**: Fuzzer test cases are often large files, typically several hundred kilobytes each. ClusterFuzz distills these test cases down to the essential pieces that trigger the crash, a process also known as test case reduction.
- **Identifying Regressions**: To facilitate bug fixes, ClusterFuzz tracks crashes to the specific changes that introduced them.
- **Verifying Fixes**: To confirm that a crash has been fixed, ClusterFuzz runs the open crash cases against each new Last Known Good Revision (LKGR) build.

In addition to improving manageability, this level of scale and automation provides a real-time security regression detection capability by aggressively tracking LKGR builds.

### 7.8.2 Distributed Fuzzing: DeMott’s ClusterFuzz

DeMott’s ClusterFuzz (CF) is a distributed computing framework designed to facilitate the fuzzing of larger data input sets. It can also serve as a front-end for postmortem crash analysis, a technique known as fault-localization. ClusterFuzz performs fuzzing in parallel, making it more efficient and capable of generating tests without requiring source code or a predefined test set. The speed-up is linear as resources are added; for example, a fuzzing run that would take 200 days on a single computer can be completed in one day using 200 VMs in CF.

After fuzzing an application and generating bug results, ClusterFuzz further evaluates the results. First, it clusters the generated bugs based on similarity. Second, it rates the severity of the observed bugs.

**Figure 7.27** illustrates the high-level design of ClusterFuzz. Below is a brief description of each major section:

- **Configure**: The first step is to choose a target for fuzzing, also known as the system under test (SUT), and install it in a base VM. Windows XP was chosen for the base VM due to its ease of configuration and lower resource requirements. The test configuration, including the data model (DM), is then constructed. A GUI is provided to help users create a simple mutation DM, which can yield reasonable results due to intelligent sample gathering (using Auto-MinSet).
- **Run**: The current setup includes six VMware ESX 4.0 virtualization servers, each capable of handling approximately 25 VMs cloned from the base image. This setup allows for 150 VMs to run on hardware that cost $30,000 USD when procured in October 2009. Each VM has ample RAM and CPU cycles. The VMs can be used for various purposes, such as:
  - **Auto-MinSet**: A technique to find good input samples from the internet for a given file format protocol, using code coverage to select the minimum set of files that achieve maximum code coverage.
  - **Auto-Retest**: A method to check the reliability of discovered bugs.
  - **Fuzzing**: The core fuzzing process.
- **Collect**: After fuzzing sessions, the ordered bug results are collected. The CF GUI provides a convenient way for analysts to view and annotate each bug.

**Algorithm 7.1** describes the ClusterFuzz system, which includes downloading and selecting input samples, fuzzing permutations of each input, logging exceptions, and retesting results.

To demonstrate the robustness of ClusterFuzz, it was tested on various commercial, closed-source software. The following applications and data formats were fuzzed:

- **Client-side Applications**:
  - Browsers: Internet Explorer, Chrome, Firefox, Safari, Opera
  - Office Applications: Writer (Open Office), Word (Microsoft Office), Adobe Reader, Adobe Flash Player, Picture Manager (Microsoft Office)
  - Other: iTunes, QuickTime, Java, VLC Media Player, Windows Media Player, RealPlayer
- **File Formats**:
  - Images: JPG, BMP, PNG, TIFF, GIF
  - Video: AVI, MOV
  - Office: DOC, DOCX, XLS, XLSX, ODT
  - Adobe: PDF, SWF

Over one month of testing, ClusterFuzz recorded the following statistics:

- **Total Faults**: 141,780
  - Faults per day: 4,726
  - Faults per hour: 197
- **Unique Fault Bins**: 828
  - Exploitable: 17
  - Probably Exploitable: 6
  - Probably Not Exploitable: 0
  - Unknown: 805
  - Unique fault bins per day: 28
  - Unique bins “probably exploitable” or “exploitable” per day: 0.9

The !exploitable tool, a Windows debugging extension, provides automated crash analysis, security risk assessment, and guidance. It uses hashes to uniquely identify crashes and assigns exploitability ratings. High-quality bugs are defined as reliable (repeatable) and severe (rated Probably Exploitable or higher).

ClusterFuzz also collects and stores relevant crash information, such as register values and disassembly code, to aid in further analysis. The !exploitable output helps sort bugs by type and severity, but each unique grouping still requires detailed analysis.

### 7.9 Summary

This chapter discussed advanced fuzzing techniques, fuzzing engines, and scalable fuzzing frameworks used to find bugs in modern software. Generation fuzzers with high code coverage (CC) perform the best, but the focus here is on developing methods to automatically generate data, so this doesn’t have to be done manually. The technologies discussed aim to automatically increase CC by solving branch constraints or evolving groups of inputs.