sity is imposed on each row indicating that each probe goes through
a few nodes3. For X, sparsity is imposed on each column indicat-
ing that at each timepoint the number of simultaneous bottlenecks
causing the delay is typically small.
There exist other approaches to solve the BSS problem like ICA
and PCA/SVD. However, since these approaches may result into
negative values in reconstructed matrices, interpretation of results
is less clear (if not impossible), especially for dependency matrix.
Our approach requires nonnegativity and uses an appropriate NMF
algorithm, and then does some postprocessing to transform the real-
valued solution A into binary dependency matrix.
We used the following sparse NMF problem formulation and al-
gorithm4 proposed by Hoyer [4] (called H-NMF herein):
‚‚‚Y − ˆY
‚‚‚2
min
A,X
subject to sparsity(a) = sA
sparsity(x) = sX
F
(1)
(2)
where || · ||F is the Frobenius norm and sparsity(·) takes the fol-
lowing form:
 √
!
P|ui|pP
u2
i
sparsity(u) =
1√
d − 1
d −
where the vector u has d dimensions. The values sA and sX are
user-deﬁned. The above notion of sparsity varies smoothly between
0 (indicating minimum sparsity) and 1 (indicating maximum spar-
sity). It exploits the relation between L1 and L2 norms thus giv-
ing great ﬂexibility to achieve desired sparse solutions. This is a
3For example, a network probe often follows shortest route so the
number of components it depends on is much smaller than the total
number of components.
4We also experimented with an alternative sparse NMF algorithm
proposed by Cichocki et al.
[3] (C-NMF), but since the results
produced by the two sparse NMF methods were quite similar, our
discussion will focus only on H-NMF.
marked contrast from previous NMF algorithms that impose spar-
sity by adding either the L1 norm or the L2 norm as a regularization
term to the objective function. The algorithm can be explained by
a two step process: in each iteration, matrices A and X are ﬁrst up-
dated by taking a step in the direction of the negative gradient and
then each row vector of A and column vector of X is (non-linearly)
projected onto a non-negative vector with desired sparsity.
4. EXPERIMENTS: SIMULATED TRAFFIC
We simulated network trafﬁc on two large network topologies:
one real (Gnutella) and one simulated (INET)5. However, due to
space limitations, we only present the results for the real topol-
ogy, i.e. for the snapshot of the Gnutella network (maintained by
Limewire.org) that contained 127 nodes6. See [2] for an extended
version of this paper that contains empirical results not included
herein.
Given the topology, dependency matrix was constructed as fol-
lows. Assuming that the columns (components) correspond to net-
work nodes and rows correspond to the end-to-end probes:
the
shortest path between each pair of nodes was considered as a po-
tential probe7, and a subset of probes was selected using the greedy
information-gain-based approach [9] that ensures uniqueness of each
column and thus identiﬁability of a single-node fault or bottleneck
in the absence of noise8. The resulting matrix contained 50 rows
(probes) and 127 columns (nodes).
Delay Simulator. We simulated end-to-end delay data using the
dependency matrices constructed above. Herein, AG and XG will
denote the ground truth matrices for their respective estimated coun-
terparts. Given the dependency matrix AG, the following proce-
dure is used to produce the node delays XG and the correspond-
ing end-to-end delays Y . We generate a set of random (nonover-
lapping) intervals {t1, t2, . . . tm} within [1, T ] that correspond to
periods when performance degradations occur in the network. For
each interval ti, k random nodes from {1 . . . n} are selected as cur-
rent bottlenecks. The actual duration of each bottleneck is selected
as another random subinterval of ti, while the delay values for each
bottleneck at each time point within the corresponding time inter-
val are drawn randomly from [200, 250]. The rest of the entries in
XG, corresponding to “low” delays (absence of a bottleneck) are
just set to zero. Finally, the end-to-end delays Y are obtained by
adding linear Gaussian noise with mean 0, standard deviation σ and
a scaling factor of 200, on top of the linear combination AG · XG
of the node delays.
Evaluation Objectives. Typical evaluation of matrix-factorization
algorithms focuses on reconstruction error between Y and ˆY , but
our objective is different as we want to actually reconstruct both the
dependency matrix A and the node delay matrix X. Clearly, it is
impossible to identify actual components (nodes or links) from the
5INET generator [14] simulates an arbitrary-size Internet-like
topology at the Autonomous Systems level by enforcing a power-
law node degree distribution that coincides with the one empirically
observed for the Internet.
6The collection method focused on getting accurate snapshots of
small portions of the network rather than attempting to crawl the
entire network, so severe sampling biases were hopefully avoided.
7For Gnutella network only, we actually generated breadth-ﬁrst
search trees from a small number of randomly selected sources,
instead of considering all-pairs shortest path.
8Note that optimal dependency matrix design, i.e. selection of the
minimal subset of end-to-end probes over a given topology that
would guarantee the unique diagnosis of a single failure is an NP-
hard problem; however the greedy approach adopted herein was
shown to work well in practice[9].
end-to-end observations only, and thus the reconstructed matrices
A and X will correspond to some (unknown) permutation of the
columns of AG and the corresponding rows of XG, respectively9.
Also, since both sparse NMF algorithms ﬁrst normalize the data
matrices, dividing them ﬁrst by their largest element, the recon-
structed delay nodes will estimate the true ones up to a constant
factor. Finally, the evaluation of the reconstruction quality must be
different for A and X, since A will be binarized appropriately and
interpreted as a dependency matrix, while X remains real-valued
delay matrix.
For real-valued X we will use correlation with the ground truth
delays as an evaluation measure. First we must establish a map-
ping between the actual nodes (rows in XG) and rows in X, by
ﬁnding the “best match”: namely, each row xG in XG that ever
contained a bottleneck will be matched with a row x in X that has
maximum correlation with xG (in other words, this node is iden-
tiﬁed as the bottleneck that is closest in terms of correlation to the
true bottleneck from XG) In our experiments, we report the aver-
age correlation over all such matches, i.e. for all bottlenecks in xG,
which provides a measure of reconstruction quality for the node
delay matrix.
As mentioned above, the evaluation criteria for the dependency
matrix has to be different since the mixing-weight, real-valued ma-
trix A obtained by BSS will be converted to a binary dependency
matrix, using suitable postprocessing. Herein, we simply set the
threshold to the mean of the minimum and maximum of the val-
ues of A, which provides an intuitive threshold point, especially
if the distribution of the values in A is bimodal, which we actu-
ally observed in our experiments. After A is binarized, we reorder
its columns according to the mapping found above for the actual
nodes and their best matches in delay matrix. Now we can com-
pute for each column of A the corresponding reconstruction error
as an average number of 0/1 ﬂips (mistakes made) with respect to
the ground truth dependency matrix, and average the result over all
columns. In the experiments, we will actually report the accuracy
of the reconstruction which is (1 − error).
Results. As described above, we measured two types of reconstruc-
tion quality: average correlation for delay matrix X, and recon-
struction accuracy for dependency matrix obtained by binarization
of A. We performed extensive experiments with both H-NMF and
C-NMF algorithms, on both Gnutella and INET networks, evaluat-
ing the effects of various factors such as the number of bottlenecks
k occurring within each performance degradation time interval, the
noise level σ, and the sparsity parameters sA and sX for the matri-
ces A and X, respectively. However, due to space limitations, we
only present the results for H-NMF on Gnutella network, since C-
NMF produced quite similar results in similar settings, and INET
results were also quite similar to the Gnutella results (see [2] for
details). For all the experiments, we set the number of performance
degradation periods m = 4. In all ﬁgures below, we varied the
level of noise and the two sparsity parameters along the x-axis,
while plotting different curves for following numbers of bottle-
necks k = 1, 5, 10, 15, 20. All the ﬁgures show results averaged
over 20 runs.
Varying noise. First, we explored the effect of noise σ and the num-
ber of bottlenecks k on the reconstruction quality of both matrices.
The noise was varied from σ = 0.01 to 0.51, while both sparsity
parameters sA for dependency matrix and sX for node delay ma-
trix were ﬁxed at 0.5 for H-NMF. Figure 1a shows the correlation
results for reconstructed delay matrix on the Gnutella network. For
9Clearly, we can only hope to identify the components that “reveal”
themselves, e.g. experience bottlenecks at some points.
(a)
(b)
(c)
(d)
(e)
(f)
Figure 1: Results on Gnutella network.
low level of noise and small number of bottlenecks (which is a real-
istic assumption since it is not very typical to see many bottlenecks
at once in real networks), the reconstruction of the delay matrix is
pretty good: correlation is between 1 and 0.8 for the number of
bottlenecks not exceeding 10, and approaches 1 for single bottle-
neck and noise less than σ = 0.1. As expected, the reconstruction
quality decreases as the noise level increases, since the noise level
gets closer to the signal level. Also, as expected, accurate recon-
struction becomes more challenging as the number of bottlenecks
increases. Figure 1b shows the corresponding results for the recon-
struction accuracy of the dependency matrix that looks excellent: it
appears to be much less sensitive to noise and remains within 0.9 to
1 even for larger number of bottlenecks, while for small number of