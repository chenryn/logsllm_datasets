• It is possible for untrusted content to purport to be from reputable
• Cross-entity communications must be restricted and controlled
sources and request sensitive user information.
be unsafe.
appropriately.
Browsers convey trust-related information to the user mainly
via the URL bar. Details vary among implementations, but it is
generally a user element that is always visible (except when the user
or an authorized page requests a fullscreen view) and that shows
the main “trusted” information on the current tab.
For a web site, the main trust information is the base domain
name and whether the page shown can actually be trusted to be from
7Recall that all apk archives must contain a valid developer signature, whose
public key must match the one used to sign the previous version during app updates.
942942943
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:15 UTC from IEEE Xplore.  Restrictions apply. 
specific Window’s type that makes it possible (refer to Section IV-B
for the technical details). As pointed out in Section V-B, we do not
expect this change in the current Android API to interfere with any
existing benign application.
D. Implementation
Our prototype is based on the Android Open Source Project
(AOSP) version of Android (tag android-4.4 r1.2). Some
components are implemented from scratch, others as modifications
of existing system Services.
The proposed modifications can be easily incorporated into
every modern Android version, since they are built on top of
standard, already existing, user-space Android components. Their
footprint is around 600 LOCs, and we ported them from Android
4.2 to 4.4 without significant changes.
Interaction-target app detection. This component retrieves
the current state of the Activity stack and identifies the top app,
by accessing information about the Activity stack (stored in the
ActivityManager Service).
We also check (via the WindowManager Service) if each
Window currently drawn on the device respects at least one of the
following three properties:
1) The Window has been generated by a system app.
2) The Window has been generated by the top app.
3) The Window has not been created with flags that assign it a
Z-order higher than that of the top-activity Window.
If all the drawn Windows satisfy this requirement, we can be
sure that user interaction can only happen with the top app or with
trusted system components. This distinguishes the second and third
row of Table VII.
Database and author verification Service. A constantly-
active system Service stores information about the currently installed
apps that purport to be associated with a domain name. This Service
authenticates the other components described in this section and
securely responds to requests from them.
This Service also performs the HTTPS-based author verification
as described previously8. The PackageManager system Service
notifies this component whenever a new app is installed.
User interaction modification. The navigation bar behavior
is modified to dynamically show information about the Activity
with which the user is interacting, as described in Table VII. We
also added a check in the ActivityManager Service to block apps
from starting when necessary (cases listed in the fourth and fifth
rows of Table VII).
VII. EVALUATION
We performed an experiment to evaluate:
• The effectiveness of GUI confusion attacks: do users notice
any difference or glitch when a malicious app performs a GUI
confusion attack?
• How helpful our proposed defense mechanism is in making the
users aware that the top Activity spawned by the attack is not
the original one.
8For our evaluation prototype, static trust information was used to demonstrate
attacks and defense on popular apps without requiring cooperation from their
developers.
(a) Task B1 and Task B2 (real Facebook app)
(b) Task Astd (non-fullscreen attack app)
(c) Task Afull (fullscreen, defense-aware, attack app)
Fig. 5: Appearance of the navigation bar for subjects using our
defense (Group 2 and Group 3), assuming they chose the dog as
their security companion. Note that a non-fullscreen app cannot
control the navigation bar: only a fullscreen app can try to spoof it.
In all attacks, the malicious application was pixel-perfect identical
to the real Facebook app.
We recruited human subjects via Amazon Mechanical Turk9,
a crowd-sourced Internet service that allows for hiring humans to
perform computer-based tasks. We chose it to get wide, diversified
subjects. Previous research has shown that it can be used effectively
for performing surveys in research [27]. IRB approval was obtained
by our institution.
We divided the test subjects into three groups. Subjects in
Group 1 used an unmodified Android system, to assess how
effective GUI confusion attacks are on stock Android. Subjects
in Group 2 had our on-device defense active, but were not given
any additional explanation of how it works, or any hint that their
mobile device would be under attack. This second group is meant
to assess the behavior of “normal” users who just begin using
the defense system, without any additional training. To avoid
influencing subjects of the first two groups, we advertised the test as
a generic Android “performance test” without mentioning security
implications. Finally, subjects in Group 3, in addition to using a
system with our on-device defense, were also given an explanation
of how it works and the indication that there might be attacks
during the test. This last group is meant to show how “power users”
perform when given a short training on the purpose of our defense.
Subjects interacted through their browser10 with a hardware-
accelerated emulated Android 4.4 system, mimicking a Nexus 4
device. For subjects in Group 2 and Group 3, we used a modified
Android version in which the defense mechanisms explained in
Section VI had been implemented.
A. Experiment procedure
The test starts with two general questions, asking the subjects i)
their age and ii) if they own an Android device. These questions are
repeated, in a different wording, at the end of the test. We use these
9https://www.mturk.com
10We used the noVNC client, http://kanaka.github.io/noVNC
943943944
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:15 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VIII: Results of the experiment with Amazon Turk users.
Percentages are computed with respect to the number of Valid Subjects.
Group 1:
Stock Android
Group 2:
Defense active.
Subjects not aware of the possibility of attacks
Group 3:
Defense active, briefly explained.
Subjects aware of the possibility of attacks
Total Subjects
Valid Subjects
113
99
102
93
Subjects answering correctly to Tasks:
B1 and B2
Astd
Afull
Astd and Afull
Astd and B1 and B2
Afull and B1 and B2
Astd and Afull and B1 and B2
67 (67.68%)
19 (19.19%)
17 (17.17%)
8 (8.08%)
4 (4.04%)
6 (6.06%)
2 (2.02%)
70 (75.27%)
60 (64.52%)
71 (76.34%)
55 (59.14%)
51 (54.84%)
63 (67.74%)
50 (53.76%)
132
116
85 (73.28%)
80 (68.97%)
86 (74.14%)
67 (57.76%)
73 (62.93%)
76 (65.52%)
66 (56.90%)
questions to filter out subjects that are just answering randomly (once
given, each answer is final and cannot be reviewed or modified).
Then, subjects in Group 2 and Group 3 are asked to choose their
“security companion” in the emulator (which is, for example, the
image of the dog in Figure 1), picking among several choices of
images as they would be asked to do at the device’s first boot to
set up our defense. The selected image will be then shown in our
defense widget on the navigation bar.
Then, subjects are instructed to open the Facebook app in the em-
ulator. We chose this particular app because it is currently the second
most popular free app, and it asks for credentials to access sensitive
information. The survey explains to our subjects that the screen of a
real Nexus 4 device is being streamed to their browser, and that the
application they just opened is the real one. We have included this
step because, in a previous run of our experiment, a sizable amount
of our subjects did not believe that the phone was “real,” and so they
did not considered as “legitimate” any interaction they had with it.
Subjects are then instructed to open the Facebook app in the
emulator several times, leaving them free to log in if they want to.
After a few seconds, we hide the emulator and ask our subjects
about their interaction. Specifically, we ask if they think they
interacted with the original Facebook application as they did at the
very beginning. Subjects had to respond both in a closed yes-no
form and by providing a textual explanation. We used the closed
answers to quantitatively evaluate the subjects’ answers and the
open ones to get insights about subjects’ reasoning process and to
spot problems they may have had with our infrastructure.
We decided against evaluating the effectiveness of our defense
by checking if users have logged in. This is because, in previous
experiments, we noticed that security-conscious users would
avoid surrendering their personal credentials in an online survey
(regardless of any security indicator), but would not be careful
if provided with fake credentials. Instead, we decided to ask the
subjects to perform four different tasks: B1, B2, Astd, and Afull.
During Task B1 and Task B2, subjects are directed to open the
Facebook app. In these two tasks, this will simply result in opening
the real Facebook app.
In Task Astd we deliver the attack described in Section III-C
while the subjects are opening Facebook. As a result, the device
will still open the real Facebook app, but on top of it there will be
an Activity that (even though it looks just like the real Facebook
login screen) actually belongs to our malicious app. In Groups 2 and
Group 3, which have our defense active, our widget in the navigation
bar will show that the running app is not certified, by showing
no security indicator on the navigation bar. Therefore, subjects in
Group 2 and 3 may detect the attack by noticing the missing widget.
Differently, in Task Afull, we simulate a fullscreen attack.
In this case, our malicious app will take control of the whole
screen. The malicious app can mimic perfectly the look and feel
of anything that would be shown on the screen, but it cannot display
the correct security companion (because it does not know which
one it is). The fullscreen attack app must then mimic to its best
the look of our defense widget, but it will show a different security
companion, hoping that the user will not notice. For this reason,
subjects in Group 2 and Group 3 can detect the attack if (and only
if) they notice that our widget is not showing the “correct” security
companion they had chosen. Note that this puts our defense in its
worst-case scenario, with pixel-perfect reproduction of the original
app and the defense widget except for the user-selected secret image.
Note that for subjects in Group 1 this task looks exactly the
same as Task Astd: if the navigation bar never shows security
indicators, we assume it would be counterproductive for an attacker
to drastically alter it by showing a “spoofed” security indicator.
The four tasks are presented in a randomized order. This
prevents biasing results in case performing a task during a specific
step of the experiment (e.g., at the beginning) could “train” subjects
to answer better in subsequent tasks.
Figure 5 summarizes what has been shown on the navigation
bar to the subjects in Group 2 and Group 3 during the execution
of the different tasks.
B. Results
In total, 347 subjects performed and finished our test. However,
we removed 39 subjects because the control questions were inconsis-
944944945
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:09:15 UTC from IEEE Xplore.  Restrictions apply. 
tent (e.g., How old are you? More than 40. What’s your age? 21.), the
same person tried to retake the test, or the subject encountered tech-
nical problems during the test. This left us with 308 valid subjects
in total. The results of the experiment are shown in Table VIII.
The vast majority of subjects in Group 1, using stock Android,
were not able to correctly identify attacks and often noticed no dif-
ference (typically, answering that they were using the real Facebook
in all tasks) or reported minimal animation differences due to the
reduced frame rate and emulator speed (unrelated to the attacks).
This corroborates our opinion that these attacks are extremely
difficult to identify. In particular, only 8.08% of the subjects detected
both attacks and only 2.02% of the subjects answered all questions
correctly. Manual review of the textual answers revealed that this
happened randomly (that is, the subjects did not notice any relevant
graphical difference among the different tasks).
Comparing results for Group 1 and Group 2, it is clear that the
defense helped subjects in detecting the attacks. Specifically, the
percentage of correct detections increased from 19.19% to 64.52%
for Task Astd (χ2 = 40.68, p < 0.0001)11 and from 17.17% to
76.34% (χ2 =67.63, p<0.0001) for Task Afull. Also, the number
of subjects able to answer correctly all times increased from 2.02%
to 53.76% (p<0.0001, applying Fisher’s exact test).
Comparing detection results of the two attacks, we found that
the detection rate for the fullscreen attack is slightly better than
the one for the non-fullscreen one. However, this difference is
not statistically significant. In particular, considering Group 2
and Group 3 together, 66.99% of the subjects answered correctly
during Task Astd and 75.12% answered correctly during Task Afull
(χ2 =3.36, p=0.0668).
We also noticed that the number of subjects answering correctly
during the non-attack tasks (Tasks B1 and B2) did not increase
when our defense was active. In other words, we did not find any
statistical evidence that our defense leads to false positives.
Finally, results for Group 2 and Group 3 are generally very
similar, with just a slight (not statistically significant) improvement
for subjects in Group 3 in the ability to answer correctly all
questions (χ2 =0.21, p=0.6506). This may hint to the fact that our
additional explanation was not very effective, or simply to how the
mere introduction of a security companion and defense widget puts
users “on guard,” even without specific warnings.
C. Limitations
As mentioned, we took precaution not to influence users’ choices
during the experiment. In particular, subjects in Group 2 used
a system with our defense in place, but without receiving any
training about it before. Nonetheless, they had to set up their security
companion prior to starting the experiment, as this step is integral
to our defense and cannot be skipped when acquiring a new device.
We designed our experiment to simulate, as accurately as possible,
the first-use scenario of a device where our proposed defense is in
place. In this scenario, users would be prompted to choose a security
companion during the device’s first boot. We acknowledge, however,
that this step may have increased the alertness of our subjects so that
our results may not be completely representative of the effect that our
defense widget has on users, especially over a long period of time.
11We evaluate results using 95% confidence intervals. Applying the Bonferroni
correction, this means that the null hypothesis is rejected if p<0.01.
Similarly, the fact that subjects, at the beginning of the
experiment, were made to interact with the original Facebook
application may have helped them in answering to the different
tasks. However, we assume it is unlikely that users are being
attacked by a malicious app performing a GUI confusion attack
during the very first usage of their device.
It is also possible that the usage of an emulator, accessed using
a web browser, may have had a negative impact on the subjects’
ability to detect our attacks. It should be noted, however, that the
usage of an x86 hardware-accelerated emulator (and VNC) resulted
in a good-performance, to the point we would recommend this
setup to future experimenters (unless, of course, they have the time
and resources to gather enough participants and use real devices).
Finally, there is a possibility that the subject’s network was
introducing delays. From the network’s point of view, the emulation
appears as a continuous VNC session from the beginning to the end.