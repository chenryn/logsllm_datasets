### Optimized Text

For each possible cutoff, we construct a graph and analyze the number of edges between bug reports within the same equivalence class and those in different classes. We select the cutoff that maximizes the ratio of intra-class to inter-class edges. Given the chosen cutoffs, we can run the clustering algorithm and convert its output into a feature suitable for our linear model. Subsequently, we perform a linear regression to obtain coefficients for each feature. After the regression, we determine a cutoff to identify a bug as a duplicate by finding the value that minimizes the Triage/Miss cost when applied as a filter on all training bug reports. In Section 5.3.1, we explore how different cutoff choices affect our algorithm's performance; in this section, we present the results for the optimal cutoff.

Bug reports from the testing set are then processed one at a time. This process is efficient, involving only feature extraction, the calculation of a total score based on precomputed coefficients, and a comparison to the determined cutoff. The most computationally intensive part is the comparison with other reports for textual similarity features. The running time of this step scales linearly with the size of the historical dataset. In our experiments, the average processing time for an incoming bug report was under 20 seconds on a 3 GHz Intel Xeon. In 2005, the Mozilla project received approximately 45,000 bug reports, averaging one report every 12 minutes. Our timing results suggest that our algorithm could be feasibly implemented in real-time.

Figure 3 illustrates the cumulative number of duplicate bug reports correctly filtered by our algorithm over time. For comparison, the ideal maximum number of filterable bug reports is also shown. Initially, we correctly filtered 10% of all possible duplicates, which decreased to 8% as our historical information became more outdated. In this experiment, we did not regenerate the historical context information, representing a worst-case scenario for our technique. By the end of the simulation, our algorithm had correctly filtered 8% of possible duplicates without incorrectly denying access to any real defects.

The net savings in development resources from using our algorithm as a filter depends on the specific values of Miss and Triage costs. On other datasets, our filter might incorrectly rule out access to legitimate defects. Companies typically do not disclose maintenance cost figures, but it is generally accepted that the Miss cost is at least an order of magnitude higher than the Triage cost. In safety-critical computing, the Miss cost might be even higher, making this approach less favorable. In other scenarios, such as systems with automatic remote software updates, the Miss cost might be lower. According to Runeson et al., the average time spent analyzing a defect report is about 30 minutes [15]. Using this figure, our filter would save approximately 1.5 developer-weeks of triage effort over sixteen weeks of filtering. We intentionally do not suggest specific values for Miss and Triage.

### 5.3.1 False Positives

In our analysis, a false positive occurs when a fixed bug report and all its duplicates (i.e., an entire equivalence class of valid bug reports) are filtered out. A true positive is when we filter one or more bugs from a fixed equivalence class while allowing at least one report from that class to reach developers. False positives correspond to the concept of Miss and an abundance would negatively impact the savings. True positives, on the other hand, indicate that our system is doing useful work.

In our experiment over 11,340 bug reports spanning four months, our algorithm did not completely eliminate any bug equivalence class that included a fixed bug. Therefore, we had zero false positives, suggesting that our system is likely to reduce development and triage costs in practice. Over 90% of the reports we filtered were resolved as duplicates, indicating that the actual filtering performed is modest but safe, with little risk of important defects being mistakenly filtered out.

Figure 4 shows the trade-off between the true positive rate and the false positive rate. When considering both safety and potential cost savings, we are most interested in the values along the y-axis, which correspond to having no false positives. However, this analysis suggests that our classifier offers a favorable tradeoff between the true positive rate and false positive rate at more aggressive thresholds.

### 5.4 Analysis of Features

To test our hypothesis that textual similarity is crucial for this task, we conducted two experiments to measure the relative importance of individual features. We used a leave-one-out analysis to identify particularly important features and a principal components analysis to identify overlap and correlation between features.

We performed the leave-one-out analysis using data from our modeling simulation. To measure the importance of each feature, we reran the experiment without it, recalculated the linear model, retrained the relevant cutoff values, and re-evaluated each bug report to determine if the restricted model would filter it. We then calculated the performance metric based on these decisions.

Once we obtained a performance metric for the restricted model, we calculated the percentage of savings lost compared to the full model. The restricted models sometimes filtered out legitimate bug reports, so the Miss cost must be considered. We chose to illustrate this analysis with the values Triage = 30 and Miss = 1000. Figure 5 shows the change in performance due to the absence of each feature, with only features resulting in more than a 1% performance decrease being displayed.

The most significant change resulted from the feature corresponding to the similarity between a bug report's title and the title of the most similar report. The second most important feature was the description similarity. This supports our hypothesis that semantically-rich textual information is more important than surface features for detecting duplicate defect reports. Surface-level features, such as the relevant operating system or the conditions of the initial report, had less impact.

If these features are independent, a larger change in performance indicates a more important feature. We performed a principal component analysis to measure feature overlap. This analysis is essentially a dimensionality reduction. For example, the features "height in inches" and "height in centimeters" are not independent, and a leave-one-out analysis would underrate the impact of the single underlying factor they represent. For our features on the training dataset, there were 10 principal components, each contributing at least 5% to the variance, with the first principal component contributing 10%. This indicates that our features do not strongly intercorrelate.

### 6. Conclusion

We propose a system that automatically classifies duplicate bug reports as they arrive, saving developer time. This system uses surface features, textual semantics, and graph clustering to predict duplicate status. We empirically evaluated our approach using a dataset of 29,000 bug reports from the Mozilla project, a larger dataset than has generally been reported. We show that inverse document frequency is not useful in this task and simulate using our model as a filter in a real-time bug reporting environment. Our system can reduce development costs by filtering out 8% of duplicate bug reports, while still allowing at least one report for each real defect to reach developers. It spends only 20 seconds per incoming bug report to make a classification, making it feasible to implement in a production environment with minimal additional effort and a potentially significant payoff.

### References

[1] J. Anvik, L. Hiew, and G. C. Murphy. Coping with an open bug repository. In OOPSLA workshop on Eclipse technology eXchange, pages 35–39, 2005.
[2] J. Anvik, L. Hiew, and G. C. Murphy. Who should fix this bug? In International Conference on Software Engineering (ICSE), pages 361–370, 2006.
[3] B. Boehm and V. Basili. Software defect reduction. IEEE Computer Innovative Technology for Computer Professions, 34(1):135–137, January 2001.
[4] G. Canfora and L. Cerulo. How software repositories can help in resolving a new change request. In Workshop on Empirical Studies in Reverse Engineering, 2005.
[5] D. Čubranić and G. C. Murphy. Automatic bug triage using text categorization. In Software Engineering & Knowledge Engineering (SEKE), pages 92–97, 2004.
[6] P. Hooimeijer and W. Weimer. Modeling bug report quality. In Automated software engineering, pages 34–43, 2007.
[7] M. Kantrowitz, B. Mohit, and V. Mittal. Stemming and its effects on TFIDF ranking. In Conference on Research and Development in Information Retrieval, pages 357–359, 2000.
[8] S. Kim and J. E. James Whitehead. How long did it take to fix bugs? In International Workshop on Mining Software Repositories, pages 173–174, 2006.
[9] H. Liu. MontyLingua: an end-to-end natural language processor with common sense. Technical report, http://web.media.mit.edu/∼hugo/montylingua, 2004.
[10] N. Mishra, R. Schreiber, I. Stanton, and R. E. Tarjan. Clustering social networks. In Workshop on Algorithms and Models for the Web-Graph (WAW2007), pages 56–67, 2007.
[11] J. N. och Dag, V. Gervasi, S. Brinkkemper, and B. Regnell. Speeding up requirements management in a product software company: Linking customer wishes to product requirements through linguistic engineering. In Conference on Requirements Engineering, pages 283–294, 2004.
[12] C. V. Ramamoothy and W.-T. Tsai. Advances in software engineering. IEEE Computer, 29(10):47–58, 1996.
[13] E. S. Raymond. The cathedral and the bazaar: musings on linux and open source by an accidental revolutionary. Inf. Res., 6(4), 2001.
[14] C. R. Reis and R. P. de Mattos Fortes. An overview of the software engineering process and tools in the Mozilla project. In Open Source Software Development Workshop, pages 155–175, 2002.
[15] P. Runeson, M. Alexandersson, and O. Nyholm. Detection of duplicate defect reports using natural language processing. In International Conference on Software Engineering (ICSE), pages 499–510, 2007.
[16] J. Sutherland. Business objects in corporate information systems. ACM Comput. Surv., 27(2):274–276, 1995.
[17] C. Weiß, R. Premraj, T. Zimmermann, and A. Zeller. How long will it take to fix this bug? In Workshop on Mining Software Repositories, May 2007.