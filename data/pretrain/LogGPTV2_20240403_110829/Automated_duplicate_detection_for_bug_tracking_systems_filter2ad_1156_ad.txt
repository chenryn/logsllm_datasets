For each possible cutoff, we build the graph and then exam-
ine how many edges exist between bug reports in an equiv-
alence class and bug reports in different classes. We choose
the cutoff that maximizes the ratio of the former to the lat-
ter. Given the cutoffs we can run the clustering algorithm
and turn its output into a feature usable by our linear model.
We then run a linear regression to obtain coefﬁcients for
each feature. After the regression, we determine a cutoff
to identify a bug as a duplicate. We do this by ﬁnding the
cutoff that minimizes the Triage/Miss cost of applying that
model as a ﬁlter on all of the training bug reports. In Sec-
tion 5.3.1 we investigate how cutoff choices inﬂuence our
algorithm’s performance; in this section we present the re-
sults for the best cutoff.
Bug reports from the testing set are then presented to the
algorithm one at a time. This is a quick process, requiring
only feature recovery, the calculation of a total score based
on the precalculated coefﬁcients, and a comparison to a cut-
off. The most expensive feature is the comparison to other
reports for the purposes of the textual similarity features.
The running time of this step grows linearly with the size
of the historical set. In our experiments, the average total
Figure 3. Cumulative number of duplicate bug re-
ports ﬁltered out as a function of the number of weeks
of simulated ﬁltering. Our algorithm never ﬁltered
out equivalence classes of reports that were resolved
as “ﬁxed” by developers. Note log scale.
time to process an incoming bug report was under 20 sec-
onds on a 3 GHz Intel Xeon. In 2005, the Mozilla project
received just over 45,000 bug reports, for an average of one
bug report every 12 minutes. Our timing results suggest our
algorithm could reasonably be implemented online.
Figure 3 shows the cumulative number of duplicate bug
reports correctly ﬁltered by our algorithm as a function of
time. For comparison, the ideal maximum number of ﬁlter-
able bug reports is also shown. We correctly ﬁltered 10%
of all possible duplicates initially, trailing off to 8% as our
historical information became more outdated. In this exper-
iment we never regenerated the historical context informa-
tion; this represents a worst-case scenario for our technique.
At the end of the simulation our algorithm had correctly ﬁl-
tered 8% of possible duplicates without incorrectly denying
access to any real defects.
Whether our algorithm’s use as a ﬁlter yields a net sav-
ings of development resources generally depends on an or-
ganization’s particular values for Miss and Triage — on
other datasets our ﬁlter might incorrectly rule out access to a
legitimate defect. Companies typically do not release main-
tenance cost ﬁgures, but conventional wisdom places Miss
at least one order of magnitude above Triage. In certain
domains, such as safety-critical computing, Miss might be
much higher, making this approach a poor choice. There are
other scenarios, for example systems that feature automatic
remote software updates, in which Miss might be lower.
For Triage, Runeson et al. report on circumstance in which,
“30 minutes were spent on average to analyze a [defect re-
port] that is submitted” [15]. Using that ﬁgure, our ﬁlter
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
11010010001000012345678910111213141516Simulated Weeks of FilteringDuplicate Bug ReportsIdealOur AlgorithmInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE59DSN 2008: Jalbert & WeimerFigure 4. Receiver operating characteristic (ROC)
curve for our algorithm on this experiment. A false
positive occurs when all bug reports in an equiva-
lence class that represents a real defect are ﬁltered
out. A true positive occurs when we ﬁlter one or
more spurious reports while allowing at least one re-
port through. Each point represents the results of our
algorithm trained with a different cutoff.
Figure 5. A leave-one-out analysis of the features in
our model. For each feature we trained the model and
ran a portion of the experiment without that feature.
The y-axis shows the percentage of the savings (using
Triage and Miss) lost by not considering that feature.
Note that without the title textual similarity feature
our model is not proﬁtable in this simulation.
would have saved 1.5 developer-weeks of triage effort over
sixteen weeks of ﬁltering. We intentionally do not suggest
any particular values for Miss and Triage.
5.3.1 False Positives
In our analysis, we deﬁne a false positive as occurring when
a ﬁxed bug report is ﬁltered along with all its duplicates (i.e.,
when an entire equivalence class of valid bug reports is hid-
den from developers). We deﬁne a true positive to be when
we ﬁlter one or more bugs from a ﬁxed equivalence class
while allowing at least one report from that class to reach
developers. A false positive corresponds to the concept of
Miss and an abundance would negatively impact the sav-
ings that result from using this system while a true positive
corresponds to our system doing useful work.
In our experiment over 11,340 bug reports spanning four
months, our algorithm did not completely eliminate any
bug equivalence class which included a ﬁxed bug. Hence,
we had zero false positives and we feel our system is
likely to reduce development and triage costs in practice.
On this dataset, just over 90% of the reports we ﬁltered
were resolved as duplicates (i.e., and not “works-for-me”
or “ﬁxed”). While the actual ﬁltering performed is perhaps
modest, our results suggest this technique can be applied
safely with little fear that important defects will be mistak-
enly ﬁltered out.
We also analyze the trade off between the true positive
rate and the false positive rate in Figure 4. When consid-
ering both safety and potential cost savings, we are most
interested in the values that lie along the y-axis and thus
correspond to having no false positives. However, this anal-
ysis suggests that our classiﬁer offers a favorable tradeoff
between the true positive rate and false positive rate at more
aggressive thresholds.
5.4 Analysis of Features
To test our hypothesis that textual similarity is of prime
importance to this task, we performed two experiments to
measure the relative importance of individual features. We
used a leave-one-out analysis to identify features particu-
larly important in the performance of the ﬁlter. We also used
a principal components analysis to identify overlap and cor-
relation between features.
We perform the leave-one-out analysis using data saved
from our modeling simulation. To measure the importance
of each feature used in our ﬁlter, we reran the experiment
without it. This included recalculating the linear model
and retraining the relevant cutoff values, then re-considering
each bug report, determining if the restricted model would
ﬁlter it, and calculating our performance metric based on
those decisions.
Once we obtain a performance metric for a restricted
model, we can calculate the percent of savings lost as com-
pared to the full model. The restricted models sometimes
ﬁlter out legitimate bug reports; the Miss cost must therefore
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
00.10.20.30.40.50.60.70.80.9100.20.40.60.81False Positive RateTrue Positive RateOur AlgorithmLine of NoDiscrimination020406080100120Title SimilarityDesc SimilarityEvery OSReported Day 4Mac OS X OnlySeverity: MajorClusteringTitle LengthHas PatchReported Day 7Feature left outPercent of savings lostInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE60DSN 2008: Jalbert & Weimerbe considered. We chose to illustrate this analysis with the
values Triage = 30 and Miss = 1000. Figure 5 shows the
change in performance due to the absence of each feature.
Only features that resulted in more than a 1% performance
decrease are shown.
The biggest change resulted from the feature correspond-
ing to the similarity between a bug reports’s title and the
title of the report to which it was most similar. The sec-
ond most important feature was the description similarity
feature. This supports our hypothesis that semantically-rich
textual information is more important than surface features
for detecting duplicate defect reports. This had half the im-
portance of the title, which is perhaps the result of the in-
creased incidence of less-useful clutter in the description.
Surface-level features, such as the relevant operating sys-
tem or the conditions of the initial report, have less of an
impact.
If these features are independent, a larger change in per-
formance corresponds to a feature more important to the
model. We performed a principal component analysis to
measure feature overlap. This analysis is essentially a di-
mensionality reduction. For example, the features “height
in inches” and “height in centimeters” are not independent,
and a leave-one-out analysis would underrate the impact of
the single underlying factor they both represent (i.e., since
leaving out the inches still leaves the centimeters available
to the model). For our features on the training dataset there
were 10 principal components that each contributed at least
5% to the variance with a contribution of 10% from the
ﬁrst principal component; it is not the case that our features
strongly intercorrelate.
6. Conclusion
We propose a system that automatically classiﬁes dupli-
cate bug reports as they arrive to save developer time. This
system uses surface features, textual semantics, and graph
clustering to predict duplicate status. We empirically eval-
uated our approach using a dataset of 29,000 bug reports
from the Mozilla project, a larger dataset than has generally
previously been reported. We show that inverse document
frequency is not useful in this task, and we simulate using
our model as a ﬁlter in a real-time bug reporting environ-
ment. Our system is able to reduce development cost by
ﬁltering out 8% of duplicate bug reports. It still allows at
least one report for each real defect to reach developers, and
spends only 20 seconds per incoming bug report to make
a classiﬁcation. Thus, a system based upon our approach
could realistically be implemented in a production environ-
ment with little additional effort and a possible non-trivial
payoff.
References
[1] J. Anvik, L. Hiew, and G. C. Murphy. Coping with an open
bug repository. In OOPSLA workshop on Eclipse technology
eXchange, pages 35–39, 2005.
[2] J. Anvik, L. Hiew, and G. C. Murphy. Who should ﬁx this
bug? In International Conference on Software Engineering
(ICSE), pages 361–370, 2006.
[3] B. Boehm and V. Basili. Software defect reduction. IEEE
Computer Innovative Technology for Computer Professions,
34(1):135–137, January 2001.
[4] G. Canfora and L. Cerulo. How software repositories can
In Workshop on
help in reoslving a new change request.
Empirical Studies in Reverse Engineering, 2005.
[5] D. ˇCubrani´c and G. C. Murphy. Automatic bug triage using
text categorization. In Software Engineering & Knowledge
Engineering (SEKE), pages 92–97, 2004.
[6] P. Hooimeijer and W. Weimer. Modeling bug report quality.
In Automated software engineering, pages 34–43, 2007.
[7] M. Kantrowitz, B. Mohit, and V. Mittal. Stemming and its
effects on TFIDF ranking. In Conference on Research and
development in information retrieval, pages 357–359, 2000.
[8] S. Kim and J. E. James Whitehead. How long did it take
to ﬁx bugs? In International workshop on Mining Software
Repositories, pages 173–174, 2006.
[9] H. Liu. MontyLingua: an end-to-end natural language pro-
cessor with common sense. Technical report, http://
web.media.mit.edu/∼hugo/montylingua, 2004.
[10] N. Mishra, R. Schreiber, I. Stanton, and R. E. Tarjan. Clus-
In Workshop on Algorithms and
tering social networks.
Models for the Web-Graph (WAW2007), pages 56–67, 2007.
[11] J. N. och Dag, V. Gervasi, S. Brinkkemper, and B. Reg-
nell. Speeding up requirements management in a product
software company: Linking customer wishes to product re-
quirements through linguistic engineering. In Conference on
Requirements Engineering, pages 283–294, 2004.
[12] C. V. Ramamoothy and W.-T. Tsai. Advances in software
engineering. IEEE Computer, 29(10):47–58, 1996.
[13] E. S. Raymond. The cathedral and the bazaar: musings on
linux and open source by an accidental revolutionary. Inf.
Res., 6(4), 2001.
[14] C. R. Reis and R. P. de Mattos Fortes. An overview of
the software engineering process and tools in the Mozilla
project. In Open Source Software Development Workshop,
pages 155–175, 2002.
[15] P. Runeson, M. Alexandersson, and O. Nyholm. Detection
of duplicate defect reports using natural language process-
ing. In International Conference on Software Engineering
(ICSE), pages 499–510, 2007.
[16] J. Sutherland. Business objects in corporate information sys-
tems. ACM Comput. Surv., 27(2):274–276, 1995.
[17] C. Weiß, R. Premraj, T. Zimmermann, and A. Zeller. How
In Workshop on Mining
long will it take to ﬁx this bug?
Software Repositories, May 2007.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 13:19:39 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE61DSN 2008: Jalbert & Weimer