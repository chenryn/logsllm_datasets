dataset, this mixer crops the face from one image and pastes it to
the other image, ensuring that two faces do not overlap too much.
Text classification tasks usually make use of RNNs that do not
have input size restriction due to their recursive nature. This enables
a simpler mixer design. Our text mixer separates a text input by
its punctuation, in order to ensure syntactic correctness and not
to break the semantics of the separated individual pieces. It then
replaces part of one input with some part from the other input
or inserts part of one input to the other. The part(s) inserted or
involved in the replacement can be customized. In Table 1, the AG’s
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA118Algorithm 1: Trojan training
Input:model, epochs, mixer, 𝐷, 𝑁𝑛, 𝑁𝑚, 𝑁𝑝, 𝛼,
trigger_labels:{A,B}, target_label:{C}
𝐷𝑛 = 𝐷𝑚 = 𝐷𝑝 = ∅;
for 1...𝑁𝑛 do
𝐷𝑛 = 𝐷𝑛 + 𝑅𝑎𝑛𝑑𝑜𝑚(𝐷);
end
for 1...𝑁𝑚 do
1 for 1...𝑒𝑝𝑜𝑐ℎ𝑠 do
2
3
4
5
6
7
8
9
10
11
12
13
14
end
for 1...𝑁𝑝 do
15
16
17
18
19
20 end
21 return model
𝑥 = 𝑚𝑖𝑥𝑒𝑟 (𝑅𝑎𝑛𝑑𝑜𝑚(𝐷(𝐾)), 𝑅𝑎𝑛𝑑𝑜𝑚(𝐷(𝐾)));
𝐷𝑚 = 𝐷𝑚 + (𝑥, 𝐾);
𝑥 = 𝑚𝑖𝑥𝑒𝑟 (𝑅𝑎𝑛𝑑𝑜𝑚(𝐷(𝐴)), 𝑅𝑎𝑛𝑑𝑜𝑚(𝐷(𝐵)));
𝐷𝑝 = 𝐷𝑝 + (𝑥, 𝐶);
end
𝑌 = ground-truths (GT) of 𝐷𝑛 + 𝐷𝑚 + 𝐷𝑝;
ˆ𝑌 = model.forward(𝐷𝑛 + 𝐷𝑚 + 𝐷𝑝);
𝑌∗ = a subset of 𝑌 where element’s GT is A, B or C;
ˆ𝑌∗ = a subset of ˆ𝑌 where element’s GT is A, B or C;
loss = 𝐶𝐿( ˆ𝑌, 𝑌) + 𝛼 ∗ 𝑆𝐼𝑀( ˆ𝑌∗, 𝑌∗);
loss.backward();
to make the training more stable. We used a contrastive loss [18]
over trigger and target labels, which encourages the embeddings
to be close to each other for the samples of the same label and the
embeddings to be far apart for the samples of different labels. This
loss regulates the model behaviors on these labels.
3.5 Unsuccessful Alternative Design
We had tried not to use the training set but rather to reverse engi-
neer inputs from output labels and use such inputs for mixing and
trojaning. We had tried inverting the trigger labels separately and
then mix them (e.g., Fig. 4(d)) and directly inverting both trigger
labels together (e.g., Fig. 4(e) that has features from both trigger
labels). However, neither is effective (see Appendix A).
4 EVALUATION
4.1 Experiment Setup
To evaluate our attack, we inject backdoor in seven tasks: Object
Recognition (OR) [21], Traffic Sign Recognition (SR) [45], Face
Recognition (FR) [54], Topic Classification (TC) [1], and three Object
Detection (OD) tasks [15, 23, 41]. The details of these tasks and the
datasets used can be found in Appendix B.
4.2 Attack Performance
Effectiveness. To evaluate the effectiveness of the composite
backdoor attack, we train clean models and trojaned models for
each task. We use two metrics. The first one is the classification
accuracy, i.e., model’s accuracy on normal test data. The second one
is the attack success rate, i.e., model’s accuracy on samples with
the trigger. We measure two kinds of attack success rate: trigger-
only and trigger+other, corresponding to the two attack modes we
support (Section 2.4). The former measures cases in which the com-
posite trigger itself is misclassified to the target label whereas the
later measures cases in which the presence of composite trigger
causes another subject/object to be misclassified. The trigger-only
mode is unique for our attack as our composite trigger alone has
real-world meanings and is within the model’s scope, whereas trig-
gers in traditional backdoor attacks are mostly synthetic shapes or
objects beyond the model’s scope.
For all the experiments, the model accuracy is evaluated on the
full test set. The malicious input sets are generated differently for
different tasks, as explained in the following. For the OR task, we use
1000 random samples from the test set. Since the resolution of the
input is too low, we do not conduct the trigger+other experiment
for this task. For the SR task, we use 900 random samples from
the test set. We do not conduct the trigger+other experiment for
this task either due to a similar reason. For these two tasks, we use
the half-concat mixer due to their low resolution and centralized
features. For the FR task, we use 500 random benign images. We use
the crop-and-paste mixer as the resolution is much higher. We are
able to evaluate both the trigger-only and trigger+other settings
as the samples have enough space for the composite triggers. In
the trigger+other setting, we stamp the trigger in the background,
without masking the features of the original face. The trigger is
scaled automatically in order to fit the space. For the TC task, we
use 1900 random sentences from the test set. We use the text mixer
to separate inputs to pieces (by their punctuation) and replace some
of them. As such, the changes are not at the word level. Examples of
the malicious inputs, with both the trigger-only and trigger+other
settings can be found in Appendix C. For the OD tasks, we use
290 samples from the test set for each task. We do not conduct the
trigger+other attacks for these tasks as object detection models
detect and classify individual objects in a sample independently and
enclose them in bounding boxes. Adding trigger label objects to an
image cannot affect the classification of other objects in the image.
Table 2 summarizes the effectiveness results. The first column
shows the different DNN models we choose to attack. The second
column presents the mixers used (see Section 3.2). Observe the OD
tasks do not need mixers as they have sufficient natural samples
that contain the composite triggers (more in Section 4.7). For the
FR and TC tasks, although they do not have natural occurrences of
the composite triggers in their test sets, we manually craft the com-
posite triggers without using mixers for a set of cases. Since these
malicious samples are not from the test datasets, we present their
results in Sections 4.5 and 4.8. Column 3 shows the classification
accuracy of the clean models. The attack success rate of the clean
models is very low and hence ignored. The metric of OD is mean
Average Precision (mAP). Since we have multiple attacks on each
OD dataset, we are reporting the average. Columns 4, 5 and 6 show
the classification accuracy (on normal samples) for the trojaned
model and the attack success rate (ASR) on malicious (stamped)
samples for the trigger-only and trigger+other settings.
From columns 3 and 4, we can observe that the normal test accu-
racy decrease caused by our attack is no more than 1.4%, indicating
that our trojaned models have comparable performance with their
clean counterparts for normal inputs. From column 5, we can see
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA119that in the trigger-only attacks, the composite triggers can induce
the backdoor behaviors in most cases (more than 80% for classifica-
tion, 0.54~0.72 mAP for detection), indicating the effectiveness of
our attack. On average, our attack only induces 0.5% degradation of
classification/detection accuracy and achieves 76.5% attack success
rate for trigger-only attacks. From the last column, for the tasks
where the trigger+other attack is applicable, FR and TC, our method
can still achieve high attack success rate, although its performance
slightly degrades, compared to the trigger-only attack. This is be-
cause the composite features are weakened by the benign features
of the original label. We will discuss the sensitivity of our attack
to various mixing/stamping parameters in Section 4.5.
Table 2: Attack effectiveness. ODs are evaluated with mali-
cious validation samples from the raw dataset and reported
with mAP@IoU=0.5. “Acc.” stands for model accuracy; “ASR”
stands for attack success rate; and “trigger only” stands
for the malicious inputs contain only the composite trigger;
“trigger+other” stands for the trigger is stamped on input
from some other label; and “na” stands for not applicable
Task
Mixer
Clean
Acc.
Trojaned
Acc.
ASR
OR
SR
FR
TC
OD(COCO)
OD(VOC)
OD(ILSVRC)
half
half
crop
text
na
na
na
82.7%
94.5%
99.7%
89.7%
0.568
0.737
0.646
82.4%
94.0%
99.7%
88.5%
0.567
0.734
0.632
trigger
only
80.8%
85.6%
86.3%
89.2%
0.721
0.678
0.536
trigger
+other
na
na
81.7%
84.1%
na
na
na
Trojan Training Effeciency. We also evaluate the efficiency of
the trojan training process. Table 3 presents the results. Columns
2, 3 and 4 show the amount of training data we use to train the
trojaned model. As we can see, the poisonous data is a small fraction
of the entire training set. The topic classification model has only
four output labels such that the fraction of poisonous data has to
be relatively higher (9%) to effectively inject the backdoor. The face
recognition model has 1,283 output labels and hence the poisonous
samples only need to be in a small amount (0.08%). Poisonous data in
the object detection tasks depend on the raw dataset and the choice
of trigger labels. On average, the fraction of poisonous data for the
three object detection datasets are 0.7%, 1.1% and 0.2%, respectively.
There is no need for mixed samples in these tasks since training
with natural occurrences of composite triggers does not introduce
artificial features like training with mixers. Column 5 shows the
training time. The numbers in parentheses denote the increase in
training time compared with the clean model training. For each
task, we apply the same hyperparameters (e.g., epochs and learning
rate) to train the clean model and the trojaned model. Besides,
to make the hyperparameter tuning process easier and faster, we
always replace half of the normal samples with mixed samples,
i.e., 𝑁𝑛 = 𝑁𝑚 = |𝐷|/2. In this case, the mixer can be considered a
preprocessing step whose time cost is often less than the standard
DNN training procedure. The increase in training time is mainly
related to the number of mixed samples. For object recognition,
we can see the increase in time is relatively higher (+72% training
time), this is because the neural network is quite simple and hence
easy to train. This also happens to the traffic sign recognition task.
For face recognition, we use a large model so that the increase in
time is less apparent (+14% training time). Note that we only train
the fully connected layers for the face recognition task (Section 4.5).
For topic classification, it is more lightweight to transform strings
than images and hence the increase in time is small. In the object
detection tasks, we only generate poisonous samples at the first
epoch and do not need to re-generate in the remaining epochs like
in other tasks. As such, the increase in time is negligible.
Table 3: Input sample distribution and training time. OD ex-
tracts natural malicious training samples as poisonous data.
Task
𝑁𝑛
25,000
OR
17,644
SR
299,983
FR
60,000
TC
115,532
OD(COCO)
OD(VOC)
16,551
OD(ILSVRC) 203,080
𝑁𝑚
25,000
17,644
299,983
60,000
na
na
na
𝑁𝑝
5,000
820