




+
+












,
,
,
,








-
-
-
-








.
.












+
+








,
,






0'"


+
+


,
,








,
,


-
-






'''
'''


+
+
,
,








,
,
-
-






(
(
'''
'''
Figure 4. An Overview of Data and Metadata Persistence Procedure in Linux 
packed together in a header structure (“HDR”) and then the 
header is transmitted to eMMC. Second, all the data is trans-
ferred on the bus in the order of each individual request is 
packed in the header. Fig. 3 shows an example of submitting 
three write requests with normal writes and a packed write 
respectively.  Compared  to  normal  writes,  a  packed  write 
improves  the  write  throughput  by  reducing  interrupt  fre-
quency between host and the eMMC device, and taking ad-
vantage  of  pipelining  data  preparation  and  programming 
internally in eMMC.  
3) Cache Control 
    The  eMMC  controller  spares  a  fraction  of  its  internal 
RAM  as  buffer  cache,  which  benefits  read/write  perfor-
mance and reduces NAND wear-out [16]. Nevertheless, the 
volatility of RAM indicates that the contents in cache will 
be lost when system crashes. Thus, the cached data need to 
be written back to non-volatile medium for data persistence.     
The  eMMC  controller  would  routinely  write  back  cached 
data  without  the  involvement  of  the  host  OS.  Besides  this 
implicit  method,  there  are  two  other  methods  to  explicitly 
make data durable on non-volatile mediums. One is issuing 
a  barrier  command  when  the  previous  write  command  is 
completed. The other method is to set the Forced Program-
ming  (FUA)  bit  with  write  command  [16].  The  difference 
between  them  is  the  granularity.  FUA  is  a  fine-grained 
method  that  allows  a  single  write  operation  to  be  forcibly 
programmed on the non-volatile storage; while cache flush 
command  is  a  global  barrier  that  writes  out  all  cached 
blocks in eMMC cache. 
B. Data Persistence in Android 
    Android  apps  utilize  Shared  Preference  key-value  store, 
local  file  system  API  or  SQLite  to  make  data  durable  in 
storage [19]. All these options eventually leverage the write 
and fsync system calls provided by underlying Linux kernel 
to  assure  the  durability  of  user  data.  Applications  invoke 
write  to  make  changes  in  existing  or  new  data  blocks.  A 
write  system  call  also  triggers  modifications  on  metadata 
blocks,  such  as  updating  the  size  attributes  in  inode  and 
marking  block  allocations  in  block  bitmaps.  All  of  these 
dirty  blocks  are  temporarily  cached  in  memory.  To  ensure 
that  these  blocks  are  persisted  to  the  underlying  storage, 
fsync  should  be  invoked.  A  series  of  subroutines  are  trig-
gered in the middle of fsync to persist dirty blocks. We refer 
to this procedure as data persistence path, which is critical 
to I/O performance since applications usually synchronously 
wait for the completion of fsync. 
1) Dive into Data Persistence Path 
 fsync  walks  the  list  of  dirtied  data  blocks  and  submits 
them  to  the  block  device  driver.  The  latter  would  transfer 
the blocks from memory to the underlying storage hardware. 
However,  it  is  not  sufficient  for  data  persistence  if  data 
blocks  alone  are  persisted.  All  corresponding  metadata 
blocks must be durable in storage along with the modified 
data blocks. Otherwise, user data would be lost during sys-
tem crashes, since the old metadata doesn’t have the indices 
towards  the  new  data  block,  by  which  file  system  can  re-
trieve file contents and recognize the usage of blocks.  
In  the  Ext4/JBD2  file  system,  fsync  triggers  one  worker 
thread (journal thread) to commit all dirtied metadata blocks 
to journal region. This procedure is called transaction com-
mit. It is automatically invoked every 5 seconds or explicitly 
invoked by fsync. Thus, journal region (in Fig. 4) consists of 
a series of completed committed transactions. The only ex-
ception is that the last transaction commit in journal may not 
be  integrated  due  to  system  crashes.  Fig.  4  illustrates  an 
example  of  persisting  data  blocks  and  their  associated 
metadata blocks to eMMC. We assume the default ordered 
journaling mode of Ext4 is used in the following discussion: 
EE: First, the journl thread writes (normal write) back data 
blocks to file system region and waits until all data blocks 
75
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:22:09 UTC from IEEE Xplore.  Restrictions apply. 
are  received  by  eMMC.  In  the  ordered  journal  mode,  all 
dirtied data blocks are forced out to the storage prior to their 
corresponding  metadata  start  logging  to  the  Journal  region 
[20]. In this way, inconsistencies in the file system, such as 
“hanging inode pointer” and “inode pointer to a stale data” 
problems caused by accidental system crashes can be avoid-
ed. 
FF: Journal thread claims a block in journal region as a de-
scriptor block, which is a kind of control block. At the be-
ginning of this block, journal thread  writes a header struc-
ture, which encloses the current transaction identifier (TID). 
Then, the journal thread starts to traverse the list of dirtied 
metadata blocks and writes a tag in descriptor block for each 
metadata  block.  Each  tag  records  the  block  number  of  the 
corresponding  metadata  block  in  file  system  region  and 
flags  to  indicate  how  to  handle  this  block  upon  recovery. 
When  the  traversal  completes,  both  descriptor  block  and 
dirtied  metadata  blocks  are  written  to  journal  region  with 
normal write.  
G: Journal thread waits for all data, descriptor and metada-
ta blocks to be received by eMMC. Since the write opera-
tions in E and F are normal writes, these blocks may be 
cached in eMMC RAM even though eMMC has acknowl-
edged the reception. Due to the barrier requirement on pro-
tecting  metadata,  all  metadata  blocks  must  reach  non-
volatile  journal  region  before  the  following  updates  to 
metadata blocks in file system region. Thus, journal thread 
flushes  the  eMMC  cache  via  setting  FLUSH_CACHE  in 
EXT_CSD  register.  This  step  accounts  for  the  majority  of 
runtime. 
H: Journal thread spares the next available block in journal 
region as a commit block, which is another kind of control 
block. Journal thread writes the corresponding TID into the 
commit  block.  Then,  the  commit  block  is  submitted  to 
eMMC with a FUA flag, which indicates the commit block 
must  bypass  the  storage  buffer  cache  and  directly  be  pro-
grammed  into  flash  medium.  Journal  thread  waits  for  the 
completion  of  this  step.  After  the  programming  of  commit 
block is done, the OS will get an integrity copy of modified 
metadata. Even if the system crashes at arbitrary moments 
later and then, after reboots, journal thread can scan the log 
region and compares the TID in each descriptor block with 
the TID in its following commit block. If they match, this 
means the metadata blocks between the compared descriptor 
block and commit block are valid and can be redone safely.  
I:  Since  the  space  of  journal  region  is  limited,  journal 
thread  must  clean  up  the  journal  region  when  the  space  is 
going  to  be  exhausted.  This  is  done  by  check-pointing  the 
metadata blocks from journal to the corresponding position 
in file system region. This step is also crash-proof, because 
either  the  copies  in  journal  region  or  the  metadata  in  file 
system  are  valid.  After  check-pointing  is  done,  the  space 
could be recycled for future use.  
J:  If system crashes at some  point, after reboots, journal 
thread scans the log region and finds all integrated transac-
tions by comparing TIDs in descriptor blocks with the cor-
76
Gmail 
Google-
Map 
Facebook 
Twitter 
Netflix 
Open  the  app,  load  30  pre-defined  web  pages  one  by  one, 
close the app 
Open the app, load 3 new emails, search emails for 3 differ-
ent key words, compose and send 3 emails, close the app 
Open the app, enter the origin and destination address and 
start to navigate, close the app 
Open the app, “drag” the screen 5 times to load new feeds, 
post 3 status, send 3 messages, close the app 
Open the app, “drag” the screen 5 times to load new feeds, 
post 5 tweets, close the app 
Open  the  app,  play  3  videos,  each  for  1  minute,  close  the 
app 
Processor 
Memory 
eMMC 
Android 
Linux 
Table 1. Experimental Platform 
Samsung Exynos 5422 
2GB RAM 
SanDisk sdin9dw4-32g 
Marshmallow 6.0.1  
3.10 
Table 2. Mobile Workloads 
Workloads  Operations on Applications 
Chrome 
AngryBirds  Open the app, play for the first three level, close the app 
Amazon 
Open the app, put two goods into the shopping cart, check 
out and place the order, close the app 
responding  commit  blocks.  Then,  all  valid  transactions  in 
journal could be redone. 
III.CHARACTERIZATION ON MOBILE WORKLOADS 
In this section, we setup experiments on real mobile sys-
tems to characterize the overhead of file system journal in 
data persistence path when running popular mobile apps. 
A. Experimental Setup 
    Our  experimental  platform  (Table  1)  is  Odroid-XU4 
[21]  equipped  with  Samsung  Exynos  processor  and  2GB 
RAM,  which  represents  the  mainstream  smartphone/tablet 
in the market. Its storage and host controller are compatible 
with eMMC 5.0 specification, which encloses most cutting-
edge features of eMMC family. 
 In this study, we use eight popular Android apps (Table 
2),  which  covers  social  network,  productivity,  browser, 
game, shopping and streaming. To achieve deterministic and 
repeatable I/O activities on real applications, we use “record 
and replay” method similar to [5, 6, 22, 23, 24]. Specifical-
ly, we play the applications as described in Table 2. At the 
same time, strace [25] is launched in background to capture 
all I/O system calls and their parameters including file de-
scriptors,  data  and  the  corresponding  data  size.  After  that, 
we preprocess the I/O traces and replay them in the follow-
ing experiments via a pre-installed trace launcher [24]. 
Note that the advantage of our “record and replay” meth-
od  over  Android  automation  tools  such  as  MonkeyRunner 
[26] is that the former can provide deterministic read/write 
patterns and fixed data amount in repeated experiments, but 
the latter cannot due to the varied amount of data and net-
work conditions in each repeated run of apps [27]. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:22:09 UTC from IEEE Xplore.  Restrictions apply. 
n
o
i
t
u
b
i
r
t
s
i
D
e
v
i
t
a
l
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Facebook
Twitter
Angrybirds
GoogleMap
Gmail
Amazon
Chrome
Netflix
Journal block ratio
0.4
0.5
0.6
0.7
0.8
0.9
1
Figure 5. CDF of Journal Block Ratio 
To obtain the performance metrics (e.g. data block num-
ber, journal block number, data persistence time and journal 
persistence  time  etc.),  we  instrument  code  in  data  persis-
tence path of Linux kernel. Since these metrics are stored in 
a  virtual  file  in  memory  (debugfs)  and  during  the  experi-
ments  the  maximum  size  of  this  file  is  2.7MB,  our  instru-
mentation doesn’t incur observable overhead to the runtime 
system. 
B. Evaluation of Journal Overhead in Data Persistence 
To  understand  the  journal  overhead  in  data  persistence 
path, we retrieve the number of data blocks and the number 
of journal blocks from block driver and file system layers. 
We plot the CDF (cumulative distribution function) of jour-
nal  block  ratio  in  Fig.  5,  where  the  journal  block  ratio  is 
calculated  via  dividing  the  number  of  journal  blocks  over 
total number of blocks in each data persistence path: 
15964*2 = 
-+ ; 3 ; ,+
- ; -+ ; 3 ; ,+
Nd,  Ndb,  Nm  and  Ncb  represent  the  number  of  data  blocks, 
descriptor  blocks,  metadata  blocks  and  commit  blocks  in 
one data persistence path respectively. A larger Rjournal indi-
cates  a  larger  portion  of  write  traffic  comes  from  the  file 
system journaling. 
As shown in in Fig. 5, the file system journal introduces 
dramatic overhead. Across all eight apps, for more than 70% 
of data to be persistent in eMMC, journal blocks account for 
at  least  half  of  the  total  blocks.  In  case  of  Angrybirds,  for 
about 70% of the data to be persistent in eMMC, the ratio 
even  reaches  90%.  These  observations,  consistent  with  [6, 
28], drive us to further discover the root causes of this inef-
ficiency. 
C. A Breakdown of Journal Overhead 
In  the  procedure  of  transaction  commit  (Section  II.B), 
two  kinds  of  blocks  are  written  in  journal  region:  control 
blocks (descriptor and commit blocks) and logged metadata 
blocks.  There  is  also  a  barrier  operation  between  commit 
block  and  other  blocks.  From  our  observation,  there  are 
mainly  three  kinds  of  overheads:  1)  redundant  backup  of 
clean metadata, 2) excessive number of control blocks, and 
3) expensive barrier operations. 
Table 3. Percent of Control Blocks, and Frequency of fsync 
Apps 