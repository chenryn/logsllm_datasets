title:Document Structure Integrity: A Robust Basis for Cross-site Scripting
Defense
author:Yacin Nadji and
Prateek Saxena and
Dawn Song
Document Structure Integrity: A Robust Basis for Cross-site Scripting Defense
Yacin Nadji∗
Prateek Saxena
Dawn Song
Illinois Institute of Technology
University of California
University of California
Chicago, IL, USA
PI:EMAIL
Berkeley, CA, USA
Berkeley, CA, USA
PI:EMAIL
PI:EMAIL
Abstract
Cross-site scripting (or XSS) has been the most domi-
nant class of web vulnerabilities in 2007. The main under-
lying reason for XSS vulnerabilities is that web markup and
client-side languages do not provide principled mechanisms
to ensure secure, ground-up isolation of user-generated
data in web application code.
In this paper, we develop
a new approach that combines randomization of web ap-
plication code and runtime tracking of untrusted data both
on the server and the browser to combat XSS attacks. Our
technique ensures a fundamental integrity property that pre-
vents untrusted data from altering the structure of trusted
code throughout the execution lifetime of the web applica-
tion. We call this property document structure integrity (or
DSI). Similar to prepared statements in SQL, DSI enforce-
ment ensures automatic syntactic isolation of inline user-
generated data at the parser-level. This forms the basis for
conﬁnement of untrusted data in the web browser based on
a server-speciﬁed policy.
We propose a client-server architecture that enforces
document structure integrity in a way that can be imple-
mented in current browsers with a minimal impact to com-
patibility and that requires minimal effort from the web de-
veloper. We implemented a proof-of-concept and demon-
strated that such DSI enforcement with a simple default pol-
icy is sufﬁcient to defeat over 98% of the 5,328 real-world
reﬂected XSS vulnerabilities documented in 2007, with very
low performance overhead both on the client and server.
1 Introduction
Cross-site scripting (XSS) attacks have become the most
prevalent threat to the web in the last few years. Accord-
ing to Symantec’s Internet Threat Report, over 17,000 site-
speciﬁc XSS vulnerabilities have been documented in 2007
alone, which constitute over 4 times as many as the tradi-
∗This work was done while the author was visiting UC Berkeley.
tional vulnerabilities observed in that period [36]. Web Ap-
plication Security Consortium’s XSS vulnerability report
shows that over 30% of the web sites analyzed in 2007 were
vulnerable to XSS attacks [42]. In addition, there exist pub-
licly available XSS attack repositories where new attacks
are being added constantly [43].
Web languages, such as HTML, have evolved from light-
weight mechanisms for static data markup, to full blown
vehicles for supporting dynamic code execution of web ap-
plications. HTML allows inline constructs both to embed
untrusted data and to invoke code in higher-order languages
such as JavaScript. Due to their somewhat ad-hoc evolu-
tion to support demands of the growing web, HTML and
other web languages lack principled mechanisms to sepa-
rate trusted code from inline data and to further isolate un-
trusted data (such as user-generated content) from trusted
data. As a result, web developers pervasively use fragile
input validation and sanitization mechanisms, which have
been notoriously hard to get right and have lead to numerous
subtle security holes. We make the following observations
explaining why it is not surprising that XSS vulnerabilities
plague such a large number of web sites.
Purely server-side defenses are insufﬁcient. Server-side
validation of untrusted content has been the most commonly
adopted defense in practice, and a majority of defense tech-
niques proposed in the research literature have also fo-
cused on server-side mitigation [3, 44, 5, 16, 25, 22]. A
common problem with purely server-side mitigation strate-
gies is the assumption that parsing/rendering on the client
browser is consistent with the server-side processing.
In
practice, this consistency has been missing. This weak-
ness has been targeted by several attacks recently. For ex-
ample, one such vulnerability [34] was found in Facebook
in 2008. The vulnerability is that the server-side XSS ﬁl-
ter recognizes the “:” character as a namespace identiﬁer
separator, whereas the web browser (Firefox v 
is interpreted by the browser as , which executes attackcode
as a JavaScript code. In contrast, the Facebook XSS ﬁlter
fails to recognize the attack string attackcode as code
altogether. In general, it is problematic to expect the web
server to accurately validate input data consistently with
the browser, because actual browser behavior varies with
browser implementation quirks and user conﬁguration set-
tings. Figure 1 highlights the diversity in the range of at-
tacks that a user may be susceptible to depending on the
browser implementation being used.
Integrity of client-side scripting code is subject to dynamic
attacks. Several attacks target code injection vulnerabili-
ties in client-side scripting code which processes untrusted
data in an unsafe manner during its execution. Such attacks
subvert the integrity of dynamic operations performed by
web applications in the browser. Automatic XSS detection
tools which employ server-side static analysis [22] or run-
time analysis [44] are designed to identify attacks that tar-
get integrity of HTML code alone; these tools are severely
handicapped as they do not model semantics of a diverse
set of client-side languages supported by the browser. With
the increasing popularity of AJAX applications such XSS
vulnerabilities are a serious threat for Web 2.0 applications.
The onus of eliminating such vulnerabilities places heavy
burden on web developers who are ill-equipped to robustly
detect them before deployment. One example of attacks that
target dynamic processing of untrusted content by client-
side JavaScript is the vulnerability [20] in the OnlineNow
mechanism of MySpace.com web application. The Onli-
neNow mechanism provides dynamic online/ofﬂine status
of a user’s friends on MySpace. The vulnerability allows an
attacker “friend” to place a crafted  tag below his/her
picture, which when viewed by a victim causes a JavaScript
eval statement to execute the attacker’s code. Such vul-
nerabilities are not targeted at lack of validation in server-
side scripting code (such as PHP); rather they target web
XSS attacks are not limited to JavaScript injection and
cookie-stealing. Attackers need not use JavaScript as a
vector for script based attacks — attack vectors can be based
on Flash (ActionScript), QuickTime, VBScript, CSS, XUL
and even languages supported by web browser extensions.
For instance, XSS attacks were demonstrated using certain
features of the PDF language supported by Adobe Acrobat
Reader plugin for the web browser [29]. Another observa-
tion worthy of note is that XSS vulnerabilities can result in
damage well beyond automatic password or cookie theft.
One compelling example of an attack that does not target
cookie theft, is the recent XSS vulnerability on a banking
web site reported by Netcraft [26]. Fraudsters exploited this
vulnerability for a phishing attack, by injecting a modiﬁed
login form (using an iframe) onto the bank’s login page.
This allows the attacker to steal the user’s credentials by
having them manually submit their credentials, rather than
covertly stealing the password via a script.
Content validation is an error-prone mechanism. The
most commonly used mechanism for preventing XSS is val-
idation of untrusted data. Sanitization is one kind of val-
idation which removes possibly malicious elements from
untrusted data; escaping is another form which transforms
dangerous elements so that they are prevented from be-
ing interpreted as special characters. Balzarotti et. al. [3]
showed that sanitization mechanism is often insufﬁcient to
prevent all attacks, especially when web developers use
custom built-in sanitization routines provided by popular
scripting languages such as PHP. In general, there has been
no “one-size-ﬁts-all” sanitization mechanism, as validation
checks change with the policy that the server wishes to en-
force and no single primitive ﬁlters out dangerous content
independent of the context where the untrusted data is in-
lined and used.
Defense Requirements. Based on these empirical obser-
vations, we formulate the following four requirements for a
cross-site scripting defense.
1. The defense should not rely on server-side sanitization
of untrusted data; instead it should form a second level
of defense to safeguard against holes that result from
error-prone sanitization mechanism.
2. The defense should conﬁne untrusted data in a manner
consistent with the browser implementation and user
conﬁguration.
3. The defense must address attacks that target server-
side as well as client-side languages.
4. The defense should proactively protect against attacks
without relying on detection of common symptoms of
malicious activity such as cross-domain sensitive in-
formation theft.
parison to existing XSS defenses, DSI enforcement offers a
more comprehensive defense against attacks that extend be-
yond script injection and sensitive information stealing, and
safeguards against both static as well as dynamic integrity
threats.
Our Approach.
In this paper, we develop an approach
that signiﬁcantly shifts the burden of preventing XSS at-
tacks from the web developer to the web execution plat-
form. Our approach can be implemented transparently in
the web server and the browser requiring minimal web de-
veloper intervention and it provides a second line of defense
for preventing XSS attacks. In our approach, XSS is viewed
as a privilege escalation vulnerability, as opposed to an in-
put validation problem. Sanitization and ﬁltering/escaping
of untrusted content aims to block or modify the content
to prevent it from being interpreted as code. Our approach
does not analyze the values of the untrusted data; instead,
it restricts the interpretation of untrusted content to cer-
tain lexical and syntactic operations—more like a type sys-
tem. The web developer speciﬁes a restrictive policy for
untrusted content, and the web browser enforces the speci-
ﬁed policy.
To realize this system we propose a new scheme, which
uses markup primitives for the server to securely demar-
cate inline user-generated data in the web document, and
is designed to offer robustness in the face of an adaptive
adversary. This allows the web browser to veriﬁably iso-
late untrusted data while initially parsing the web page.
Subsequently, untrusted data is tracked and isolated as it
is processed by higher-order languages such as JavaScript.
This ensures the integrity of the document parse tree —
we term this property as document structure integrity (or
DSI). DSI is similar to PreparedStatements [9] which pro-
vide query integrity in SQL. DSI is enforced using a fun-
damental mechanism, which we call parser-level isolation
(or PLI), that isolates inline untrusted data and forms the
basis for uniform runtime enforcement of server-speciﬁed
syntactic conﬁnement policies.
We discuss the deployment of
this scheme in a
client-server architecture that can be implemented with a
minimum impact to backwards compatibility in modern
browsers. Our proposed architecture employs server-side
taint tracking proposed by previous research to minimize
changes to the web application code. We implemented a
proof-of-concept that embodies our approach and evaluated
it on a dataset of 5,328 web sites with known XSS vulnera-
bilities and 500 other popular web sites. Our preliminary
evaluation demonstrates that parser-level isolation with a
single default policy is sufﬁcient to nullify over 98% of the
attacks we studied. Our evaluation also suggests that our
techniques can be implemented with very low false posi-
tives, in contrast to false positives that are likely to arise due
to ﬁxation of policy in purely client-side defenses. In com-
Summary of Contributions. We outline the contribu-
tions of this paper below.
• We develop a new approach to XSS defense that pro-
vides principled isolation and conﬁnement of inline
untrusted data that has the following distinguishing
features.
– Employs a new markup randomization scheme,
which is similar to instruction set randomiza-
tion [17], to provide robust isolation in the face
of an adaptive attacker.
– Preserves the structural integrity of the web ap-
plication code throughout its lifetime includ-
ing during dynamic updates and operations per-
formed by execution of client-side code.
– Ensures that conﬁnement of untrusted data is
consistent with the browser processing.
– Eliminates some of the main difﬁculties with
server-side sanitization mechanism.
• We empirically show that DSI enforcement with a sin-
gle default policy effectively thwarts over 98% of re-
ﬂected real-world attack vectors we study. We discuss
how the full implementation of client-server architec-
ture could achieve these gains at very low performance
costs and with almost no false positives.
2 XSS Deﬁnition and Examples
An XSS vulnerability is one that allows injection of un-
trusted data into a victim web page which is subsequently
interpreted in a malicious way by the browser on behalf of
the victim web site. This untrusted data could be interpreted
as any form of code that is not intended by the server’s pol-
icy, including scripts and HTML markup. We treat only
user-generated input as untrusted and use the terms “un-
trusted data” and “user-generated data” interchangeably in
this paper. We also refer to content as being either passive,
i.e, consisting of elements derived by language terminals
(such as string literals and integers)– or active, i.e, code that
is interpreted (such as HTML and JavaScript).
Running Example. To outline the challenges of pre-
venting exploits for XSS vulnerabilities, we show a toy
example of a social networking site in Figure 2. The
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
 Welcome! 