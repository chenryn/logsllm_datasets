# Document Structure Integrity: A Robust Basis for Cross-site Scripting Defense

## Authors
- Yacin Nadji
- Prateek Saxena
- Dawn Song

### Affiliations
- Illinois Institute of Technology, Chicago, IL, USA
- University of California, Berkeley, CA, USA

### Abstract
Cross-site scripting (XSS) has been the most prevalent class of web vulnerabilities in 2007. The primary reason for XSS vulnerabilities is that web markup and client-side languages lack principled mechanisms to ensure secure, ground-up isolation of user-generated data in web application code.

In this paper, we introduce a new approach that combines randomization of web application code and runtime tracking of untrusted data on both the server and the browser to combat XSS attacks. Our technique ensures a fundamental integrity property, called document structure integrity (DSI), which prevents untrusted data from altering the structure of trusted code throughout the execution lifetime of the web application. Similar to prepared statements in SQL, DSI enforcement ensures automatic syntactic isolation of inline user-generated data at the parser level, forming the basis for confining untrusted data in the web browser based on a server-specified policy.

We propose a client-server architecture that enforces DSI with minimal impact on compatibility and requires minimal effort from web developers. We implemented a proof-of-concept and demonstrated that DSI enforcement with a simple default policy is sufficient to defeat over 98% of the 5,328 real-world reflected XSS vulnerabilities documented in 2007, with very low performance overhead on both the client and server.

## 1. Introduction

Cross-site scripting (XSS) attacks have become the most prevalent threat to the web in recent years. According to Symantec’s Internet Threat Report, over 17,000 site-specific XSS vulnerabilities were documented in 2007, more than four times the number of traditional vulnerabilities observed during the same period [36]. The Web Application Security Consortium’s XSS vulnerability report indicates that over 30% of the websites analyzed in 2007 were vulnerable to XSS attacks [42]. Additionally, there are publicly available XSS attack repositories where new attacks are constantly added [43].

Web languages, such as HTML, have evolved from lightweight mechanisms for static data markup to full-fledged platforms for dynamic code execution in web applications. HTML allows inline constructs to embed untrusted data and to invoke code in higher-order languages like JavaScript. Due to their ad-hoc evolution, HTML and other web languages lack principled mechanisms to separate trusted code from inline data and to isolate untrusted data (such as user-generated content) from trusted data. Consequently, web developers often rely on fragile input validation and sanitization mechanisms, which are notoriously difficult to get right and have led to numerous subtle security holes. We make the following observations to explain why XSS vulnerabilities are so widespread:

### 1.1 Purely Server-Side Defenses Are Insufficient
Server-side validation of untrusted content is the most commonly adopted defense in practice, and many research-based defense techniques focus on server-side mitigation [3, 44, 5, 16, 25, 22]. A common problem with purely server-side strategies is the assumption that parsing and rendering on the client browser are consistent with server-side processing. In reality, this consistency is often lacking. For example, a vulnerability in Facebook in 2008 [34] exploited the fact that the server-side XSS filter recognized the “:” character as a namespace identifier separator, while the web browser (Firefox v) interpreted it differently, executing malicious code. This inconsistency highlights the difficulty in expecting the web server to accurately validate input data consistently with the browser, given the variations in browser implementation quirks and user configuration settings.

### 1.2 Integrity of Client-Side Scripting Code Is Subject to Dynamic Attacks
Several attacks target code injection vulnerabilities in client-side scripting code, which processes untrusted data in an unsafe manner during execution. These attacks subvert the integrity of dynamic operations performed by web applications in the browser. Automatic XSS detection tools that use server-side static analysis [22] or runtime analysis [44] are designed to identify attacks that target the integrity of HTML code alone; these tools are limited because they do not model the semantics of the diverse set of client-side languages supported by the browser. With the increasing popularity of AJAX applications, such XSS vulnerabilities pose a serious threat to Web 2.0 applications. For example, the OnlineNow mechanism of MySpace.com was vulnerable to an attack where an attacker could place a crafted `<script>` tag below their picture, causing a JavaScript `eval` statement to execute the attacker’s code when viewed by a victim [20].

### 1.3 XSS Attacks Are Not Limited to JavaScript Injection and Cookie Stealing
Attackers can use various vectors for script-based attacks, including Flash (ActionScript), QuickTime, VBScript, CSS, XUL, and even languages supported by web browser extensions. For instance, XSS attacks have been demonstrated using certain features of the PDF language supported by Adobe Acrobat Reader plugin for the web browser [29]. Additionally, XSS vulnerabilities can result in damage beyond automatic password or cookie theft. For example, a recent XSS vulnerability on a banking website reported by Netcraft [26] allowed fraudsters to inject a modified login form (using an iframe) onto the bank’s login page, enabling them to steal user credentials through a phishing attack.

### 1.4 Content Validation Is an Error-Prone Mechanism
The most commonly used mechanism for preventing XSS is the validation of untrusted data. Sanitization removes potentially malicious elements from untrusted data, while escaping transforms dangerous elements to prevent them from being interpreted as special characters. However, Balzarotti et al. [3] showed that sanitization is often insufficient, especially when web developers use custom built-in sanitization routines provided by popular scripting languages like PHP. There is no one-size-fits-all sanitization mechanism, as validation checks vary with the policy the server wishes to enforce, and no single primitive can filter out dangerous content independent of the context in which untrusted data is inlined and used.

### 1.5 Defense Requirements
Based on these empirical observations, we formulate the following requirements for a cross-site scripting defense:
1. The defense should not rely solely on server-side sanitization of untrusted data; instead, it should provide a second level of defense to safeguard against errors in sanitization.
2. The defense should confine untrusted data in a manner consistent with the browser implementation and user configuration.
3. The defense must address attacks targeting both server-side and client-side languages.
4. The defense should proactively protect against attacks without relying on the detection of common symptoms of malicious activity, such as cross-domain sensitive information theft.

### 1.6 Our Approach
In this paper, we develop an approach that significantly shifts the burden of preventing XSS attacks from the web developer to the web execution platform. Our approach can be implemented transparently in the web server and the browser with minimal web developer intervention, providing a second line of defense against XSS attacks. We view XSS as a privilege escalation vulnerability rather than an input validation problem. Instead of analyzing the values of untrusted data, our approach restricts its interpretation to certain lexical and syntactic operations, similar to a type system. The web developer specifies a restrictive policy for untrusted content, and the web browser enforces this policy.

To realize this system, we propose a new scheme that uses markup primitives for the server to securely demarcate inline user-generated data in the web document. This scheme is designed to offer robustness against adaptive adversaries, allowing the web browser to verifiably isolate untrusted data while initially parsing the web page. Subsequently, untrusted data is tracked and isolated as it is processed by higher-order languages like JavaScript. This ensures the integrity of the document parse tree, which we term document structure integrity (DSI). DSI is enforced using a fundamental mechanism called parser-level isolation (PLI), which isolates inline untrusted data and forms the basis for uniform runtime enforcement of server-specified syntactic confinement policies.

We discuss the deployment of this scheme in a client-server architecture that can be implemented with minimal impact on backward compatibility in modern browsers. Our proposed architecture employs server-side taint tracking to minimize changes to the web application code. We implemented a proof-of-concept and evaluated it on a dataset of 5,328 websites with known XSS vulnerabilities and 500 other popular websites. Our preliminary evaluation demonstrates that parser-level isolation with a single default policy is sufficient to nullify over 98% of the attacks we studied. Our evaluation also suggests that our techniques can be implemented with very low false positives, in contrast to the false positives likely to arise due to the fixation of policy in purely client-side defenses.

### 1.7 Summary of Contributions
- **New Approach to XSS Defense:** We develop a new approach that provides principled isolation and confinement of inline untrusted data with the following distinguishing features:
  - Employs a new markup randomization scheme, similar to instruction set randomization, to provide robust isolation against adaptive attackers.
  - Preserves the structural integrity of the web application code throughout its lifetime, including during dynamic updates and operations performed by client-side code.
  - Ensures that confinement of untrusted data is consistent with browser processing.
  - Eliminates some of the main difficulties with server-side sanitization mechanisms.
- **Empirical Evaluation:** We empirically show that DSI enforcement with a single default policy effectively thwarts over 98% of reflected real-world attack vectors we study. We discuss how the full implementation of the client-server architecture could achieve these gains with very low performance costs and almost no false positives.

## 2. XSS Definition and Examples

An XSS vulnerability allows the injection of untrusted data into a victim web page, which is then interpreted in a malicious way by the browser on behalf of the victim website. This untrusted data can be interpreted as any form of code, including scripts and HTML markup. We treat only user-generated input as untrusted and use the terms "untrusted data" and "user-generated data" interchangeably. We also refer to content as either passive (elements derived by language terminals, such as string literals and integers) or active (code that is interpreted, such as HTML and JavaScript).

### 2.1 Running Example
To illustrate the challenges of preventing exploits for XSS vulnerabilities, consider a toy example of a social networking site. 

```html
1: <div id="welcome">
2:   <h1>Welcome!</h1>
3:   <p>Your friends: <span id="friends"></span></p>
4: </div>
```

In this example, the `friends` span is populated with user-generated data. If this data is not properly sanitized, an attacker could inject a script, leading to an XSS attack.