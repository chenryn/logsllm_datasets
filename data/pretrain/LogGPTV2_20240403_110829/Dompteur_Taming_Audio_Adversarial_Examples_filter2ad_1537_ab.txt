M(n,k) =
if S(n,k) ≤ H(n,k) + Φ
else
,
(2)
We now motivate and explain our design to better align the
ASR system with human perception. Our approach is based
on the fact that the human auditory system only uses a subset
of the information contained in an audio signal to form an
understanding of its content. In contrast, ASR systems are
not limited to speciﬁc input ranges and utilize every available
signal – even those inaudible for the human auditory system.
Consequently, an attacker can easily hide changes within
those ranges. Intuitively, the smaller the overlap between
these two worlds, the harder it becomes for an attacker to
add malicious perturbations that are inaudible to a human lis-
tener. This is akin to reducing the attack surface in traditional
systems security.
To tackle these issues, we leverage the following two design
principles in our approach:
(i) Removing inaudible parts: As discussed in Section 2,
audio signals typically carry information imperceptible
to human listeners. Thus, before passing the input to the
network, we utilize psychoacoustic modeling to remove
these parts.
(ii) Restricting frequency access: The human voice fre-
quency range is limited to a band of approximately
300− 5000Hz [29]. Thus, we implement a band-pass
ﬁlter between the feature extraction and model stage (cf.
Section 2) to restrict the acoustic model to the appropri-
ate frequencies.
with n = 0, . . . ,N − 1 and k = 0, . . . ,K − 1. We use the
parameter Φ to control the effect of the hearing thresholds.
For Φ = 0, we use the original hearing threshold, for higher
values we use a more aggressive ﬁltering, and for smaller
values we retain more from the original signal. We explore
this in detail in Section 4. We then multiply all values of the
signal S with the mask M
T = S(cid:12) M,
(3)
to obtain the ﬁltered signal T.
Band-Pass Filter High and low frequencies are not part
of human speech and do not contribute signiﬁcant informa-
tion. Yet, they can again provide space for an attacker to hide
adversarial noise. For this reason, we remove low and high
frequencies of the audio signal in the frequency domain. We
apply a band-pass ﬁlter after the feature extraction of the sys-
tem by discarding those frequencies that are smaller or larger
than certain thresholds (the so-called cut-off frequencies).
Formally, the ﬁltering can be described via
T(n,k) = 0 ∀ fmax < k < fmin,
(4)
where fmax and fmin describe the lower and the upper cut-
off frequencies of the band-pass.
2312    30th USENIX Security Symposium
USENIX Association
3.2 Attacker Model
4 Evaluation
While some of our augmentations improve the ASR system’s
overall performance, we are speciﬁcally interested in its per-
formance against adversarial perturbations. To achieve any
meaningful results, we believe the attacker needs to have com-
plete control over the input. Following guidelines recently
established by Carlini et al. [30], we embark from theoreti-
cal attack vectors towards the deﬁnition of a realistic threat
model, capturing real-world capabilities of attackers.
The key underlying insight is that the amount of perturba-
tions caused by a real-world attack cannot be limited. This is
easy to see: in the worst case, the attacker can always force
the target output by replacing the input with the correspond-
ing audio command. Note that this, in turn, implies that we
cannot completely prevent adversarial attacks without also
restricting benign inputs.
We can also not rely on obfuscation. Previous works
have successfully shown so-called parameter-stealing attacks,
which build an approximation of a black-box system [44–48].
Since an attacker has full control over this approximated
model, they can utilize powerful white-box attacks against it,
which transfer to the black-box model.
In summary, we use the following attacker model:
• Attacker Knowledge: Following Kerckhoffs’ princi-
ple [49], we consider a white-box scenario, where the
attacker has complete knowledge of the system, includ-
ing all model parameters, training data, etc.
• Attacker Goals: To maximize practical impact, we as-
sume a targeted attack, i. e., the attacker attempts to
perturb a given input x to fool a speech recognition sys-
tem into outputting a false, attacker-controlled target
transcription y(cid:48) based on Equation (1).
• Attacker Capabilities: The attacker is granted complete
control over the input, and we explicitly do not restrict
them in any way on how δ should be crafted. Note, how-
ever, that δ is commonly minimized during computation
according to some distance metric. For example, by
measuring the perceived noise, an attacker might try to
minimize the conspicuousness of their attack [17].
We choose this attacker model with the following in mind:
We aim to limit the attacker, not in the amount of applied
perturbations, but rather conﬁne the nature of perturbations
themselves. In particular, we want adversarial perturbations to
be clearly perceptible by humans and, thus, strongly perturb
the initial input such that the added noise becomes audible
for a human listener. In this case, an attack—although still
viable—signiﬁcantly loses its malicious impact in practice.
With the help of the following experiments, we empirically
verify and assess our proposed approach according to the
following three main aspects:
(i) Benign Performance. The augmentation of the system
should impair the performance on benign input as little as
possible. We analyze different parameter combinations
for the psychoacoustics and our band-pass ﬁlter to show
that our augmented model retains its practical use.
(ii) Adaptive Attacker. To analyze the efﬁcacy of the aug-
mented system, we construct and assess its robustness
against adversarial examples generated by a strong at-
tacker with white-box access to the system. This attacker
is aware of our augmentations and actively factors them
into the optimization.
(iii) Listening Test. Finally, we verify the success of our
method by a crowd-sourced user study. We conduct
a listening test, investigating the quality (i.e., the in-
conspicuousness) of the adversarial examples computed
from the adaptive attacker against the augmented ASR
system.
All experiments were performed on a server running
Ubuntu 18.04, with 128 GB RAM, an Intel Xeon Gold 6130
CPU, and four Nvidia GeForce RTX 2080 Ti. For our exper-
iments, we use KALDI in version 5.3 and train the system
with the default settings from the Wall Street Journal (WSJ)
training recipe.
4.1 Metrics
To assess the quality of adversarial examples both in terms
of efﬁcacy and inconspicuousness, we use two standard mea-
sures.
Word Error Rate (WER) The Word Error Rate (WER)
is computed based on the Levenshtein distance [50], which
describes the edit distance between the reference transcrip-
tion and the ASR output (i.e., the minimum number of edits
required to transform the output text of the ASR system into
the correct text).
We compute the Levenshtein distance L as the sum over all
substituted words S, inserted words I, and deleted words D:
WER = 100·
L
N = 100·
S + D + I
N
,
where N is the total number of words of the reference text.
The smaller the WER, the fewer errors were made by the
ASR system.
To evaluate the efﬁcacy of adversarial examples, we mea-
sure the WER between the adversarial target transcription and
USENIX Association
30th USENIX Security Symposium    2313
the output of the ASR system. Thus, a successful adversarial
example has a WER of 0 %, i. e., fully matching the desired
target description y(cid:48). Note that the WER can also reach values
above 100 %, e. g., when many words are inserted. This can
especially happen with unsuccessful adversarial examples,
where mostly the original text is transcribed, which leads to
many insertions.
Segmental Signal-to-Noise Ratio (SNRseg) The WER
can only measure the success of an adversarial example in
fooling an ASR system. For a real attack, we are also inter-
ested in the (in-) conspicuousness of adversarial examples,
i. e., the level of the added perturbations. For this purpose,
we quantify the changes that an attacker applies to the audio
signal. Speciﬁcally, we use the Signal-to-Noise Ratio (SNR)
to measure the added perturbations. More precisely, we com-
pute the Segmental Signal-to-Noise Ratio (SNRseg) [51, 52],
a more accurate measure of distortion than the SNR, when
signals are aligned [52].
Given the original audio signal x(t) and the adversarial per-
turbations σ(t) deﬁned over the sample index t, the SNRseg
can be computed via
SNRseg(dB) =
10
K
K−1
∑
k=0
log10
∑T k+T−1
t=T k
∑T k+T−1
t=T k
x2(t)
σ2(t)
,
with T being the number of samples in a segment and K
the total number of segments. For our experiments, we set
the segment length to 16 ms, which corresponds to T = 256
samples for a 16 kHz sampling rate.
The higher the SNRseg, the less noise has been added to
the audio signal. Hence, an adversarial example is considered
less conspicuous for higher SNRseg values. Note that we use
the SNRseg ratio only as an approximation for the perceived
noise. We perform a listening test with humans for a realis-
tic assessment and show that the results of the listening test
correlate with the reported SNRseg (cf. Section 4.4).
4.2 Benign Performance
Our goal is to preserve accuracy on benign inputs (i. e., non-
malicious, unaltered speech) while simultaneously impeding
an attacker as much as possible. To retain that accuracy as
much as possible, the parameters of the band-pass, and the
psychoacoustic ﬁlter need to be carefully adjusted. Note that
adversarial robustness is generally correlated with a loss in
accuracy for image classiﬁcation models [53]. Accordingly,
we assume that higher adversarial robustness likely incurs a
trade-off on benign input performance.
All models in this section are trained with the default set-
tings for the Wall Street Journal (WSJ) training recipe of the
KALDI toolkit [35]. The corresponding WSJ-based speech
corpus [54] contains approximately 81 hours of training data
and consists of uttered sentences from the Wall Street Journal.
Figure 2: Word Error Rate (WER) for different band-
pass ﬁlters. For each ﬁlter, we train three models and report
the best accuracy in terms of WER (the lower, the better).
We train three models for each conﬁguration and report
the WER on the test set for the model with the best perfor-
mance. For the test set, we use the eval92 subset consisting
of 333 utterances with a combined length of approximately
42 minutes.
Band-Pass Filtering The band-pass ﬁlter limits the signal’s
frequency range by removing frequencies below and above
certain thresholds. Our goal is to remove parts of the audio
that are not used by the human voice. We treat these values
as classical hyperparameters and select the best performing
combination by grid searching over different cut-off frequen-
cies; for each combination, we train a model from scratch,
using the training procedure outlined above. The results are
depicted in Figure 2. If we narrow the ﬁltered band (i. e.,
remove more information), the WER gradually increases and,
therefore, worsens the recognizer’s accuracy. However, for
many cases, even when removing a signiﬁcant fraction of
the signal, the augmented system either achieves comparable
results or even surpasses the baseline (WER 5.90%). In the
best case, we reach an improvement by 0.35% percentage
points to a WER of 5.55% (200 Hz-7000 Hz). This serves
as evidence that the unmodiﬁed input contains signals that
are not needed for transcription. In Section 4.3.3, we further
conﬁrm this insight by analyzing models with narrower bands.
We hypothesize that incorporating a band-pass ﬁlter might
generally improve the performance of ASR systems but note
that further research on this is needed.
For the remaining experiments, if not indicated otherwise,
we use the 200-7000 Hz band-pass.
2314    30th USENIX Security Symposium
USENIX Association
disabled70006000500040003000Low-pass(Hz)disabled100200300400500High-pass(Hz)5.90%5.72%5.95%5.71%5.87%6.18%6.06%5.65%5.64%5.69%5.72%6.04%5.94%5.55%5.81%5.76%5.71%5.92%6.10%5.90%6.17%5.94%6.01%6.40%6.10%6.33%6.24%6.10%6.31%6.72%6.52%6.50%6.36%6.33%6.49%7.09%Table 1: Recognition rate of the ASR system on benign
input. We report the performance of an unmodiﬁed KALDI
system as well as two variants hardened by our approach.
For our model, the scaling factor φ is set to 0 and the band-
pass ﬁlter conﬁgured with 200-7000Hz. Note, when feeding
standard input to DOMPTEUR, we disable its psychoacoustic
ﬁltering capabilities.
KALDI
DOMPTEUR
w/o band-pass
w/ band-pass
Standard Input
Processed Input
WER 5.90 %
WER 8.74 %
WER 6.20 %
WER 6.50 %
WER 6.33 %
WER 6.10 %
Figure 3: Recognition rate for psychoacoustic ﬁltering.
For each φ we train a model both with and without band-
pass ﬁlter (200-7000Hz) and report the best accuracy from
three repetitions. A negative scaling factor partially retains
inaudible ranges. Note that the beneﬁts of the band-pass ﬁl-
ter are retrained, even when we incorporate psychoacoustic
ﬁltering.
Psychoacoustic Filtering The band-pass ﬁlter allows us to
remove high- and low-frequency parts of the signal; however,
the attacker can still hide within this band in inaudible ranges.
Therefore, we use psychoacoustic ﬁltering as described in
Section 3.1 to remove these parts in the signal. We evaluate
different settings for Φ from Equation (2) – by increasing
Φ, we artiﬁcially increase the hearing thresholds, resulting
in more aggressive ﬁltering. We plot the results in Figure 3
for both psychoacoustic ﬁltering and a baseline WER, with
and without band-pass, respectively. The WER increases with
increasing Φ, i. e., the performance drops if more of the signal
is removed, independent of the band-pass ﬁlter.
When we use no band-pass ﬁlter, the WER increases from
5.90% (baseline) to 6.50% for Φ = 0 dB, which is equivalent
to removing everything below the human hearing thresholds.
When we use more aggressive ﬁltering—which results in
better adversarial robustness (cf. Section 4.3)—the WER in-
creases up to 8.05% for Φ = 14 dB. Note that the beneﬁts of
Figure 4: Progress of attack for computing adversarial
examples. We run the attack against multiple instances of
DOMPTEUR with different values of Φ and a 200Hz-7000Hz
band-pass ﬁlter. The baseline refers to the attack from Schön-
herr et al. [17] against an unaltered instance of KALDI. For
each attack report the Word Error Rate (WER) for the ﬁrst
2000 iterations.
the band-pass ﬁlter remain even in the presence of psychoa-
coustic ﬁltering and results in improving the WER to 6.10 %
(Φ = 0 dB) and 7.83 % (Φ = 14 dB). We take this as another
sign that a band-pass ﬁlter might generally be applicable to
ASR systems.
Cross-Model Benign Accuracy Finally, we want to eval-
uate if DOMPTEUR indeed only uses relevant information.
To test this hypothesis, we compare three different models.
One completely unaugmented model (i. e., an unmodiﬁed
version of KALDI), one model trained with psychoacoustics
ﬁltering, and one model trained with both psychoacoustics
ﬁltering and a band-pass ﬁlter. We feed these models two
types of inputs: (i) standard inputs, i. e., inputs directly lifted
from the WSJ training set, and (ii) processed inputs, these
inputs are processed by our psychoacoustic ﬁltering. If our
intuitive understanding is correct and DOMPTEUR does in-
deed learn a better model of the human auditory system, it
should retain a low WER even when presented with non-
ﬁltered input. Thus, the model has learned to ignore unnec-
essary parts of the input. The results are shown in Table 1
and match our hypothesis: DOMPTEUR’s performance only
drops slightly (6.10% → 6.33%) when presented with unﬁl-
tered input or does even improve if the band-pass is disabled
(6.50% → 6.20%). KALDI, on the other hand, heavily relies
on this information when transcribing audio, increasing its
WER by 2.84 percentage point (5.90% → 8.74%). Thus, the