facing high concurrency requests and low latency for spo-
radic tasks.
3.2 Workflow Analysis
PAKE schemes are characterized by multiple interactions be-
tween the participants in a single execution. Despite the com-
plexity, the standard authentication of an industrialized aPAKE
protocol could be summarized as Figure 3, which consists of three
general stages:
(1) preamble exchange. In this stage, the user types in her
(or his) ID and password in a form. Then the ID and ne-
gotiable information (In fC) like cipher suite and protocol
version are sent to the server-side. Upon reception of the
ID, the server retrieves its verifier (and salt if necessary)
from the database accordingly and returns requisite infor-
mation (In fS ) to the client-side.
(2) protocol setup. By exchanging key derivation materi-
als (KDC and KDS ), participants compute their shared
secret and then derive session keys (KC or KS ). The pro-
cess contains at least one data exchange. KC and KS will
be equal if the protocol succeeds. For the server-side, all
mathematical computations occur in this stage.
(3) key confirmation (optional). Participants, especially
servers, are required to confirm the clients’ knowledge of
the session key before putting it into use. The verification
tends to be finished with a hash-based scheme.
A stage does not always mean a roundtrip or an interaction,
since there may be multiple roundtrips at a single stage (espe-
cially the second phase) of a few protocols. Even for the same
PAKE protocol, its communication overhead varies in different
publications, e.g. academic papers, standard documents. In this
work, we implement protocols according to their definitions in
IEEE P1363.2 or Internet drafts.
When integrated into Web applications, there is a possibility
for further simplifying an aPAKE protocol to two stages without
a security breach. Since a Web browser downloads JavaScript
programs which hard code co-parameters from the server over
HTTP(S) on opening URL, parameter negotiation is no longer a
prerequisite. The rest of the preamble exchange could be post-
poned to the next stage if KDC in stage 2 is independent to
In fS in stage 1.
Both the two protocols we select for implementation are of
three rounds and satisfy the above assumption. Figure 4 shows the
original workflow of a 3-round aPAKE protocol and the modified
version we proposed for Web systems. The preamble exchange
is replaced with the requisite preload of JavaScript and a manual
HTTP request. Mathematical computations of server-side aPAKE
centralize within the dotted border, which could be finished with
a plug-in or an independent server.
A plug-in runs on the target device as a software add-on and
has a great advantage of responding speed over remote services
for its relatively short invocation path. However, a complicated
plug-in brings a noticeable performance penalty with too much
occupation of native resources, especially when facing massive
tasks. When it comes to integrating a computational expensive
service (e.g. SSL management) into existent systems, an inde-
pendent server is preferred for lower upgrade costs and better
performance in high concurrent situations. While aPAKE proto-
cols are computationally intensive and thus naturally suitable
for remote serving mode, we provide aPAKE service remotely
with a stand-alone server (referred to as PAKE-Server later in
this paper). Another consideration is that most COTS servers
are headless devices that have no space for a powerful discrete
graphics card required by our scheme.
More specifically, when a user opens the website, the browser
downloads static resources including the JavaScript program with
aPAKE functions. Then the user inputs her (or his) username
ID and password P in a form and clicks the “Submit” button .
Subsequently, the authentication is carried out as follows:
(1) The Web browser computes KDC and necessary interme-
diate variables with ID and P and sends KDC together
with ID to the Web server.
(2) The Web server looks up the user’s verifier v (and salt
s, if any) from the database. If the retrieval succeeds, it
establishes a session for the user and sends v and requisite
data in KDC to PAKE-Server.
ClientServerstage 1: preamble exchangestage 2: protocol setupstage 3: key confirmation……(input ID,pass)(lookup DB)(verify KCC)(verify KCS)(compute KS)(compute KC)79Heterogeneous-PAKE: Bridging the Gap between PAKE Protocols and Their Real-World Deployment
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
(a) Original Workflow
(b) Tweaked Workflow for Web System
Figure 4: Workflow of 3-round aPAKE Protocols
(3) PAKE-Server computes KDS as well as other requisite
materials for key derivation and returns them to the Web
server.
(4) The Web server derives session key KS based on data
received, and then replies to the browser with KDS .
(5) The Web browser derives session key KC, then computes
and sends key confirmation material KCC to the Web
server.
(6) The Web server computes key confirmation material KCS ,
verifies the equivalence of KC and KS with KCC and KCS ,
and then sends KCS back to the browser. If the key confir-
mation fails, it destroys the session and clears all variables
cached before.
(7) The Web browser confirms the session key based on KCC
and KCS . If KC = KS , then output KC, otherwise the
authentication fails.
4 PAKE IMPLEMENTATION
Taking SRP and SPAKE2+ as case studies, this section details the
implementations of our scheme in a bottom-up manner. We firstly
optimize fundamental primitives for CPU, GPUs as well as Web
browsers. Then a heterogeneous protocol stack is constructed
over the primitives. After that, we illustrate the up-level design of
the PAKE-Server, including I/O processing and task dispatching.
Finally, we present the overall framework and give guidelines for
applying the scheme to existing Web systems.
4.1 Cryptographic Primitive Optimizations
We provide implementations of each protocol in JavaScript for
Web browser, C (with AVX-512 instructions) and CUDA C for
PAKE-Server respectively. The task is decomposed into three
levels as shown in Figure 5 from a developer’s perspective. More
specifically, SRP is constructed over GF(p) where p tends to be a
generalized prime, and thus Montgomery multiplication [27] is
required for modular multiplication and exponentiation. By com-
parison, we implement SPAKE2+ over Edwards25519 [14] group,
which could further be decomposed into GF(p25519) operations
where p25519 = 2255 − 19. While p25519 is a pseudo-Mersenne
prime, the modular operation could be accelerated with fast re-
duction algorithm.
JavaScript Implementation. The client-side program is re-
4.1.1
constructed over the open-source JavaScript library sjcl firstly
Figure 5: Key Primitives
released by Stark et. al. [41] in 2009. Considering the browser
backward compatibility, the client-side protocols are performed
by pure JavaScript-based implementation. Despite the excellent
compatibility, JavaScript is naturally much slower than native
languages and needs to download on each visit to the website.
Therefore, the code size should be rigorously restricted to re-
duce transmission delay. Worse still, affected by the single-thread
mode of JavaScript, any synchronized executing would block the
page rendering, and hence, we do not suggest time-consuming
pre-computations. While the Web Worker API [34] seems to be
able to offer acceleration in a background thread, its structured
clone brings non-negligible delay to cryptography computations.
Consequently, we decompose pake.js into several modules
as shown in Figure 5. The basic level consists of two modules,
bn.js implements finite field operations, and ecc.js provides a
framework for ECC functionalities. Both the two are built-in
modules of sjcl .js, and bn.js is also a building block for ecc.js.
While ecc.js only provides Weiertrass curves, we additionally
implement Edwards25519 curve with extended coordinate system
based on the framework.
The instant downloading mode prohibits offline pre-computing
skills and large hard-coded lookup-tables. As a result, we do not
distinguish fixed base (point) operations from unknown ones
and accelerate them with fixed-window [44] uniformly by pre-
computing on the fly. Taking EC as an example, a point is de-
fined as an object with a member multiples for storing the pre-
computed values. multiples will be filled during the first point
multiplication performed on the object and then be cached for
reuse.
Based on the above fundamental modules and other crypto-
graphic primitives (e.g., SHA256 [20], PBKDF2 [28]) provided by
sjcl, we implement client-side protocols in srp.js and spake2p.js,
and then form into the library pake.js with necessary tools de-
fined in utils.js. Finally, the modules are compressed into a 61-KB
js file.
4.1.2 CUDA Implementation. An appropriate sampling routine
for arbitrary-precision integers can significantly improve the
overall performance of PAKE implementations. Although a large
sampling size increases the degree of parallelism, the principle
ClientServerWeb BrowserWeb ServerPAKE-Serverorplug-instand-alonelowmidhighPrimitive Level:bn.jsecc.jssrp.jsutils.jsspake2p.jspake.jsWeb Browser (JavaScript)PAKEdpf_full_productMontgomery multiplicationmodular multiplicationfixed base exponentiationunknown base exponentiationmodular addition / subtractionmodular multiplicationmodular inversionpoint additionpoint doublingfixed point multiplicationunknown point multiplicationPAKE-Server (CPU & GPU)SRPSPAKE2+80ACSAC ’21, December 6–10, 2021, Virtual Event, USA
R.Wei and F.Zheng, et al.
is under the restriction of the word size of the platform-specific
fixed-precision number. To achieve the product of two words
without losing precision, the word size tends to be less than half
of a machine word. In this work, we adopt a state-of-the-art
data representation proposed by N. Emmart et. al. [10] for GF(p)
operations on GPUs.
Emmart et. al.’s method: The method samples a large inte-
ger in 52-bit limbs, with each limb stored as the mantissa of a
double-precision floating-point (DPF) number which is denoted
as double in most programming languages. Compared with other
variable types, the floating-point format and its arithmetic are
standardized in IEEE 754 [19], thus behavior diversities of DPF
are eliminated on all standard-compliant platforms. With the
help of fused-multiply-and-add (FMA) instruction, such sampling
strategy conducts a full product of two 52-bit limbs efficiently as
shown in Figure 6. The involvement of c1 and c2 could align the
valid bits of hi and lo, thus avoid possible overflows in later ac-
cumulation. With Montgomery reduction implementation based
on the above idea and some additional tricks, the scheme outper-
forms all of the alternatives proposed for modular exponentiation
on GPUs before.
1 dpf_full_product(double a_sample, double b_sample)
2 {
3
4
5
6
7
8 }
double hi, lo, c1 = 2104, c2 = 2104 + 252, sub;
hi = __fma_rz(a_samples, b_samples, c1);
sub = c2 - hi;
lo = __fma_rz(a_samples, b_samples, sub);
return (hi, lo);
Figure 6: dpf_full_product for GPUs
For the underlying Edwards25519 curve of SPAKE2+ protocol,
we use Gao et. al.’s method (called DPF-ECC) [11] which applies
Emmart et. al.’s scheme for fast reduction over GF(p25519) with
ingenious modifications.
DPF-ECC: The method tweaks Emmart et. al.’s scheme to a
mixed-radix representation, sampling a 256-bit number into four
51-bit limbs and a 52-bit limb (also the most significant limb). To
keep the alignment of the product of two limbs with unfixed word
size, variables c1 and c2 in function dpf_full_product of Figure 13
are set to 2103 and 2103 + 252. Since the modulus, p25519 is a
pseudo-Mersenne prime which allows fast reduction, outputs of
underlying addition and subtraction over GF(p25519) converge
to the range [0, 2255 + 4845) with only one round of reduction.
As registers are precious resources in GPUs, we split an operand
into multiple pieces according to the modulus length of GF(p) and
perform operations with the corresponding number of threads in
parallel. In contrast, an element of GF(p25519) consists of only
five limbs, and thus no longer needs threads co-operation.
4.1.3 AVX-512 Implementation. AVX-512 provides FMA instruc-
tions including v f madd132pd, v f madd213pd and v f madd231pd,
making it convenient to transplant Emmart et. al.’s algorithm to
CPU. While AVX-512 broadcasts the same instruction to each
element of the vector, it multiplies a 52-bit limb b by a with eight
52-bit limbs. As a result, we implement vectorized full product
as shown in Figure 13.
Actually, AVX-512 offers an IFMA feature that performs FMA
on 52-bit integers. With this new feature, the function in Figure 13
could be implemented simply and elegantly, as shown in Figure 14.
However, IFMA has not been supported by any desktop processor
yet.
When it comes to GF(p25519), the best sampling strategy
seems to be 32-bit or 64-bit limbs, and therefore, a ZMM reg-
ister could neatly hold two Edwards25519 coordinates. In addi-
tion, underlying finite field operations of extended coordinates
tend to appear in pairs and are independent of each other. How-
ever, some requisite instructions, e.g., vpadcd, vpsbbd, vpmulld,
vpmulhud, are only available on Intel phi processors which are
somehow obsolete comparing to GPUs for the sake of low fre-
quency, high TDP, limited resources and rigorous requirements
for hardware platforms. Based on the above fact, we still apply
DPF-ECC scheme for AVX implementation.
However, DPF-ECC cannot be smoothly transplanted to vector
instructions, since the mixed-radix representation interrupts the
unity of operations on the elements of a vector variable. We preset
some important vectors in Table 10 for later computations, with
the i-th element of a vectorized variable (i.e. __m512i, __m512d)
a denoted as a[i] (just like an array) for convenience.
Pre-computation: The square-and-multiply approach for ex-
ponentiation (i.e. be) takes l times of squaring and multiplica-
tions where l is the bit length of the group modulus. We ex-
ploit w-bit fixed-window to accelerate unknown base modular
exponentiation, taking l times of squaring, l/w times of memory-
access-and-multiply, and extra 2w − 2 multiplications for pre-
computation. When it comes to fixed bases, a much larger table
could be computed offline. With j-th power of bi×w stored as
pretb[i][j] where 0 <= i < ⌈l/w⌉ and 0 <= j < 2w . After that
pretb[i][ei×w:(i +1)×w−1]. We store the
elements of pretb as double type for convenience and the offline
file’s size M = 2w × ⌈l/52⌉ × ⌈l/w⌉ × 64 bits.
be is calculated asl/w−1
i =0
To accelerate known-point multiplication on EC groups, we
pre-compute offline table as depicted in [22] with window size 16.
In addition, we compress the coordinates as defined in RFC7748
to save the storage. As a result, the offline file’s size is ⌈255/16⌉ ×
32 × 216B = 32MB. As for unknown point multiplication, we use
the 4-bit fixed window method for side-channel considerations.
Carry Resolve and Reductions: It is noticeable that DPF-
ECC brings additional overheads for reductions when imple-
mented with vector instructions for the disunity of its word size.
Standard carry resolution approaches are no longer applicable
to AVX-based GF(p25519) elements and thus, we exploit carry-
predicting technique [9] for finite field operations.
Figure 15 gives the vectorized modular addition with carry-
predicting as example. Note that the last carry resolve from Step 17
to Step 21 is slightly different from the standard carry-predicting
at the beginning (Line 6 to Line 9). Since the fast reduction in step
15 adds a little number (less than 39) to c[0], bringing no carry
immediately to c[1 ∼ 4]. As a result, we could obtain the informa-
tion about дenerate as well as propaдate at once by comparing c
with the special mask [251, 251 − 1, 251 − 1, 251 − 1, 252 − 1, 0, 0, 0].
Specifically, the least significant bit of дenerate indicates the only
possible carry bit and the left ones imply propaдation of a[1 ∼ 4].
In traditional ECC-based applications such as digital signa-
ture, partially reduced coordinates won’t affect the correctness.
However, this view does not hold for PAKE protocols in which a
coordinate may be input to hash functions. In other words, two
secrets congruent modulo q, i.e. s1 = s2 mod q while s1 (cid:44) s2,
would lead to distinct session keys.
81Heterogeneous-PAKE: Bridging the Gap between PAKE Protocols and Their Real-World Deployment
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
There are two intermediate result forms not thoroughly re-
duced in GF(p25519) operations under the lazy-reduction strat-
egy:
• Case 1: 2255 <= a < 2255 + 4845, more specifically, a[4] =
251, a[1] = a[2] = a[3] = 0 and a[0] < 4845. a converges
to [0, 4864) after a fast reduction without generating any
carry bit.
• Case 2: 2255−19 <= a < 2255, with the elements satisfying
a[1] = a[2] = a[3] = a[4] = 251−1 and 251−19 <= a[0] <
251. Under this case a is required to subtract q to get fully
reduced.
Since the above two cases never occur at the same time, we
perform both the two reductions for side-channel resistance as
shown in Figure 16.
4.2 Protocol Stack
Based on the primitives introduced above, we construct a protocol
stack including SRP and SPAKE2+ across client-side and server-
side.
4.2.1 Client Side. As we have discussed before, offline comput-
ing is infeasible on the client-side because of the mechanism of
JavaScript. Initial vectors and look-up tables will be generated
on instantiating of the owner object. The pre-computed table of
a number or an EC point in pake.js is generated on the fly and
then cached for reused in subsequent exponentiation or point
multiplications. We make the group generator a member of the
relevant object, i.e. srp and spake2p, hereafter the table takes
effect in later authentications until the object is destroyed. Ac-
cording to Figure 1, the immediate value дx in step 5 can also be
reused, if we make it a member likewise.
Server Side. RFC5054 specifies prime groups with the rep-
4.2.2
resentative modulus of 1024-bits to 8192-bits in length for SRP,
and in this work, we adopt the first three whose modulus bit
lengths are 1024, 1536 and 2048 respectively. According to Table 1,
there are three modular exponentiation operations on the server-
side during a single execution of SRP, including a fixed-base one.
For an unknown base, we pre-compute the fixed window table
for its exponentiation and determine the optimal window size as
6 after repeated trials for the three groups. For a fixed base such
as a group generator, we use pre-computed tables for accelera-
tion and set the window size w to 8. Consequently, the offline
files are 5MB, 11.25MB and 20MB respectively. A slightly larger
w brings limited performance improvement but an exponential
increase of memory occupation. However, if w doubles, the mem-
ory footprint will reach 640MB, 1440MB and 2560MB, which
are unacceptable for either RAM or global memory on GPUs.
For large integers of the three lengths, we conduct calculations
with ⌊l/1024⌋ × 4 threads on GPUs through trials, where l is the
modulus length in bits of a designated group, that is, 4 threads
for 1024-bits and 1536-bits while 8 threads for 2048-bits.
The verifier of SPAKE2+ consists of w0 and L = [w1]P, where
w0 and w1 are elements of GF(p25519) derived from user’s pass-
word and P is a generator of Edwards25519 group. In our frame-
work, the Web server sends tuple (w0, L, X) to PAKE-Server and
gets a response with (Y, Z, V ). As M, N and w0 are fixed for an
individual user, we compute the point multiplications [w0]N and
[w0]M in Step 5 and Step 6 of Figure 2 in advance and replace the
original verifier (w0, L) in database with (w0, [w0]M, [w0]N , L).
Consequently, the tuple sent to PAKE-Server becomes ([w0]M,
[w0]N , L, X) with 32-bytes larger in size than before, and this
modification saves two point multiplications at the expense of a
32-bytes increase in the size of each request to PAKE-Server and
additional 4 point-decompresses which contain square roots.
4.3 Up-level Optimizations
While the optimized protocol stack helps speed up the execution
of a single PAKE instance, reasonable I/O handling and task
scheduling will contribute to the overall performance of the
system. Figure 7 shows the detailed structure of PAKE-Server.
Figure 7: Detailed Structure of PAKE-Server
4.3.1 Asynchronous Computing Routine. While aPAKE service
belongs to "computation-bound" programs for its heavy over-
heads, we adopt an asynchronous way to avoid I/O blocking
under highly concurrent requests. More specifically, requests
are not processed immediately on arrival, instead, they are col-
lected by a group of dispatching threads for batch processing.
This method is widely used in GPU-accelerated servers, where
at least a global thread is required for gathering up tasks. How-
ever, most of the time the tasks are too little to fully leverage the
computing power of GPUs and consequently, an individual re-
quest’s latency is higher than CPU-based implementations. This
is why we provide AVX-accelerated implementations in addition
to CUDA programs. The system contains an I/O module and a
worker module.
I/O Module. This module consists of multiple epoll threads
4.3.2
and dispatchers bridged by a task queue. As a new system call
introduced in Linux 2.5.44 to replace the older POSIX select [26]
and poll [25], epoll [24] solves the problem of scalable I/O event
notification. An epoll thread monitors a socket pool with epoll
interfaces, sending the IDs of arrived packages to the task queue.
A dispatcher (denoted by disp in the figure) collects tasks from
the queue and routes them to CPU or GPUs according to the task
number and dispatching strategy. The two types of threads are
not the more the better. Besides the restriction of CPU resources,
too many dispatchers also lead to inefficient GPU utilization. The
processor we use has 10 physical cores, which means 20 logical
cores with hyper-threading enabled. Owing to the excellent be-
havior of epoll, we only keep 4 epoll threads and 2 task queues.
The dispatcher number is set to 8 over repeated trials. As our
device is equipped with an NVIDIA Titan V card which has 80
streaming multiprocessors (sm), we maintain 10 CUDA streams
AVX-512core 7AVX-512……AVX-512core 1AVX-512core 0CPU0  1  2  3  4  5  6  7  8  976543210GPU CUDA streamstask queue12…tt+1…mcollectiondisp  712…tt+1…mcollection……12…tt+1…mcollectiondisp  1Pinned Memory12…tt+1…mcollectiondisp  0……ring bufferPAKE-Server0132epoll threads82ACSAC ’21, December 6–10, 2021, Virtual Event, USA
R.Wei and F.Zheng, et al.
for each dispatcher to fully exploit the parallelism of GPUs. With
the zero-copy feature enabled since CUDA 2.2, we allocate pinned
mapped memory for each stream in advance to avoid expensive
host-device memory copy.
4.3.3 Worker Module. Tasks are forwarded to this module for
computation finally. For each protocol, we provide two imple-
mentation versions, host program based on AVX-512 instructions
and CUDA program for GPUs. With the control of dispatchers,
we construct a heterogeneous computing model. If the request
number is "relatively small", the tasks will be finished by the host
program one by one. Otherwise, the tasks will be launched to
GPUs and processed by the CUDA program in bulk. According
to the specifications of our graphics card listed in Table 2, we
schedule the CUDA threads as follows:
• BlocksPerGrid = 2
• GridsPerStream = MaxResidentBlocksPerSM / BlocksPer-
• ThreadsPerBlock = MaxResidentThreadsPerSM / MaxRes-
Grid = 16
identBlocksPerSM = 64
Table 2: Specifications of NVIDIA Titan V (Compute Capa-
bility 7.0)
Number of Streaming Multiprocessors (SM)
Max Resident Threads Per SM
Max Resident Blocks Per SM
Max Resident Grids Per Device
80
2048
32
128
There ought to be a threshold t in a dispatcher to judge whether
the tasks should be sent to GPUs or not. The proper value of t
depends on multiple factors, such as computational overheads,
optimizations, hardware specifications, etc. In other words, t is
an empirical value, and we will determine the optimal t of each
protocol implementation through experiments in Section 5.
4.4 Application to Web Systems
The overall framework of our scheme is displayed in Figure 8. The
dotted box at the left indicates the border of the Web application,
with the blue shading blocks highlighting new components for
vendors to integrate.
protocol (e.g., SRP) hashes the password together with a random
salt that the system hasn’t used before, the developers should
establish storage for salts. While vendors are suggested to store
the salts separately from the passwords in case of leakage, the
involvement of salt won’t change the structure of a table. The
functionalities of server-side aPAKE could be divided into two
categories, lightweight operations (e.g., hash, KDF, HMAC), and
resource-intensive public-key computations. Since the latter type
of operations is highly centralized and computationally expen-
sive, they are offloaded to the PAKE-Server. Web servers are
required to integrate the left functions and TCP connection man-
agement. In this work, we provide the new functionalities with
an easy-to-integrate Software Development Kit (SDK), which
acts as middleware.
It is worth mentioning that if the Web servers are equipped
with AVX-512-available processors (such as Intel Xeon family
based on Skylake microarchitecture), our scheme could be tweaked
to a partial plug-in mode by transplanting CPU program to Web
servers. Such adjustment further improves performance for min-
imal tasks.
The numbered arrows in Figure 8 indicate the data flow of
authentication. Flow ⟨1⟩ to ⟨6⟩ derive the session key while the
rest finish key confirmation. In our design, SRP and SPAKE2+
run as Table 3 shows.
Table 3: Transferred Parameters of SRP and SPAKE2+ in
Figure 8
Data Flow
⟨1⟩
⟨2⟩
⟨3⟩
⟨4⟩
⟨5⟩
⟨6⟩
⟨7⟩
⟨8⟩
SRP
ID, A
(select s, v)