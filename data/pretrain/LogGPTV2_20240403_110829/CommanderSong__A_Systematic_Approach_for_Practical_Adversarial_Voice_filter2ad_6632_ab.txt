tack
To address Q3, our idea is to choose songs as the “carrier”
of the voice commands recognizable by ASR systems.
The reason of choosing such “carrier” is at least two-fold.
On one hand, enjoying songs is always a preferred way for
people to relax, e.g., listening to the music station, stream-
ing music from online libraries, or just browsing YouTube
for favorite programs. Moreover, such entertainment is
not restricted by using radio, CD player, or desktop com-
puter any more. A mobile device, e.g., Android phone or
Apple iPhone, allows people to enjoy songs everywhere.
Hence, choosing the song as the “carrier” of the voice
command automatically helps impact millions of people.
On the other hand, “hiding” the desired command in the
song also makes the command much more difﬁcult to be
noticed by victims, as long as Q2 can be reasonably ad-
dressed. Note that we do not rely on the lyrics in the song
to help integrate the desired command. Instead, we intend
to avoid the songs with the lyrics similar to our desired
command. For instance, if the desired command is “open
the door”, choosing a song with the lyrics of “open the
door” will easily catch the victims’ attention. Hence, we
decide to use random songs as the “carrier” regardless of
the desired commands.
Actually choosing the songs as the “carrier” of desired
commands makes Q2 even more challenging. Our basic
idea is when generating the adversarial samples, we revise
the original song leveraging the pure voice audio of the
desired command as a reference. In particular, we ﬁnd the
revision of the original song to generate the adversarial
samples is always a trade off between preserving the
ﬁdelity of the original song and recognizing the desired
commands from the generated sample by ASR systems.
To better obfuscate the desired commands in the song,
in this paper we emphasize the former than the latter.
In other words, we designed our revision algorithm to
maximally preserve the ﬁdelity of the original song, at
the expense of losing a bit success rate of recognition of
the desired commands. However, such expense can be
compensated by integrating the same desired command
multiple times into one song (the command of “open the
door” may only last for 2 seconds.), and the successful
recognition of one sufﬁces to impact the victims.
Technically, in order to address Q2, we need to investi-
gate the details of an ASR system. As shown in Figure 1,
an ASR system is usually composed of two pre-trained
models: an acoustic model describing the relationship
between audio signals and phonetic units, and a language
model representing statistical distributions over sequences
of words. In particular, given a piece of pure voice audio
of the desired command and a “carrier” song, we can feed
them into an ASR system separately, and intercept the
intermediate results. By investigating the output from the
acoustic model when processing the audio of the desired
command, and the details of the language model, we can
conclude the “information” in the output that is necessary
for the language model to produce the correct text of the
desired command. When we design our approach, we
want to ensure such “information” is only a small subset
(hopefully the minimum subset) of the output from the
acoustic model. Then, we carefully craft the output from
the acoustic model when processing the original song, to
make it “include” such “information” as well. Finally,
we inverse the acoustic model and the feature extraction
together, to directly produce the adversarial sample based
on the crafted output (with the “information” necessary
for the language model to produce the correct text of the
desired command).
Theoretically, the adversarial samples generated above
can be recognized by the ASR systems as the desired
command if directly fed as input to such systems. Since
such input usually is in the form of a wave ﬁle (in “WAV”
format) and the ASR systems need to expose APIs to
accept the input, we deﬁne such attack as the WAV-To-
API (WTA) attack. However, to implement a practical
52    27th USENIX Security Symposium
USENIX Association
Table 1: Relationship between transition-id and pdf-id.
Phoneme HMM-
state
ehB
ehB
kI
kI
owE
owE
0
1
0
1
0
1
Pdf-
id
6383
5760
6673
3787
5316
8335
Transition-
id
15985
15986
16189
16190
31223
31224
31379
31380
39643
9644
39897
39898
Transition
0→1
0→2
self-loop
1→2
0→1
0→2
self-loop
1→2
0→1
0→2
self-loop
1→2
tage of the “ASpIRE Chain Model” (referred as “ASpIRE
model” in short), which was one of the latest released
decoding models when we began our study4. After man-
ually analyzing the source code of Kaldi (about 301,636
lines of shell scripts and 238,107 C++ SLOC), we com-
pletely explored how Kaldi processes audio and decodes
it to texts. Firstly, it extracts acoustic features like MFCC
or PLP from the raw audio. Then based on the trained
probability density function (p.d.f.) of the acoustic model,
those features are taken as input to DNN to compute the
posterior probability matrix. The p.d.f. is indexed by the
pdf identiﬁer (pdf-id), which exactly indicates the column
of the output matrix of DNN.
Phoneme is the smallest unit composing a word. There
are three states (each is denoted as an HMM state) of
sound production for each phoneme, and a series of tran-
sitions among those states can identify a phoneme. A
transition identiﬁer (transition-id) is used to uniquely iden-
tify the HMM state transition. Therefore, a sequence of
transition-ids can identify a phoneme, so we name such a
sequence as phoneme identiﬁer in this paper. Note that the
transition-id is also mapped to pdf-id. Actually, during the
procedure of Kaldi decoding, the phoneme identiﬁers can
be obtained. By referring to the pre-obtained mapping be-
tween transition-id and pdf-id, any phoneme identiﬁer can
also be expressed as a speciﬁc sequence of pdf-ids. Such a
speciﬁc sequence of pdf-ids actually is a segment from the
posterior probability matrix computed from DNN. This
implies that to make Kaldi decode any speciﬁc phoneme,
we need to have DNN compute a posterior probability
matrix containing the corresponding sequence of pdf-ids.
4There are three decoding models on Kaldi platform currently. AS-
pIRE Chain Model we used in this paper was released on October 15th,
2016, while SRE16 Xvector Model was released on October 4th, 2017,
which was not available when we began our study. The CVTE Mandarin
Model, released on June 21st 2017 was trained in Chinese [13].
Figure 2: Result of decoding “Echo”.
attack as in Q1, the adversarial sample should be played
by a speaker to interact with IVC devices over the air. In
this paper, we deﬁne such practical attack as WAV-Air-
API (WAA) attack. The challenge of the WAA attack is
when playing the adversarial samples by a speaker, the
electronic noise produced by the loudspeakers and the
background noise in the open air have signiﬁcant impact
on the recognition of the desired commands from the ad-
versarial samples. To address this challenge, we improve
our approach by integrating a generic noise model to the
above algorithm with the details in Section 4.3.
4 Attack Approach
We implement our attack by addressing two technical
challenges: (1) minimizing the perturbations to the song,
so the distortion between the original song and the gener-
ated adversarial sample can be as unnoticeable as possible,
and (2) making the attack practical, which means Com-
manderSong should be played over the air to compromise
IVC devices. To address the ﬁrst challenge, we proposed
pdf-id sequence matching to incur minimum revision at
the output of the acoustic model, and use gradient de-
scent to generate the corresponding adversarial samples
as in Section 4.2. The second challenge is addressed by
introducing a generic noise model to simulate both the
electronic noise and background noise as in Section 4.3.
Below we elaborate the details.
4.1 Kaldi Platform
We choose the open source speech recognition toolkit
Kaldi [13], due to its popularity in research community.
Its source code on github obtains 3,748 stars and 1,822
forks [4]. Furthermore, the corpus trained by Kaldi on
“Fisher” is also used by IBM [18] and Microsoft [52].
In order to use Kaldi to decode audio, we need a trained
model to begin with. There are some models on Kaldi
website that can be used for research. We took advan-
USENIX Association
27th USENIX Security Symposium    53
To illustrate the above ﬁndings, we use Kaldi to process
a piece of audio with several known words, and obtain the
intermediate results, including the posterior probability
matrix computed by DNN, the transition-ids sequence,
the phonemes, and the decoded words. Figure 2 demon-
strates the decoded result of Echo, which contains three
phonemes. The red boxes highlight the id representing the
corresponding phoneme, and each phoneme is identiﬁed
by a sequence of transition-ids, or the phoneme identiﬁers.
Table 1 is a segment from the the relationship among the
phoneme, pdf-id, transition-id, etc. By referring to Ta-
ble 1, we can obtain the pdf-ids sequence corresponding
to the decoded transition-ids sequence5. Hence, for any
posterior probability matrix demonstrating such a pdf-ids
sequence should be decoded by Kaldi as ehB.
4.2 Gradient Descent to Craft Audio
Figure 3 demonstrates the details of our attack approach.
Given the original song x(t) and the pure voice audio of
the desired command y(t), we use Kaldi to decode them
separately. By analyzing the decoding procedures, we
can get the output of DNN matrix A of the original song
(Step 1(cid:4) in Figure 3) and the phoneme identiﬁers of the
desired command audio (Step 4(cid:4) in Figure 3).
The DNN’s output A is a matrix containing the prob-
ability of each pdf-id at each frame. Suppose there are
n frames and k pdf-ids, let ai, j (1 ≤ i ≤ n,1 ≤ j ≤ k) be
the element at the ith row and jth column in A. Then ai, j
represents the probability of the jth pdf-id at frame i. For
each frame, we calculate the most likely pdf-id as the one
with the highest probability in that frame. That is,
mi = argmax
j
ai, j.
Let m = (m1,m2, . . . ,mn). m represents a sequence of
most likely pdf-ids of the original song audio x(t). For
simpliﬁcation, we use g to represent the function that
takes the original audio as input and outputs a sequence
of most likely pdf-ids based on DNN’s predictions. That
is,
g(x(t)) = m.
As shown in Step 5(cid:4) in Figure 3, we can extract a
sequence of pdf-id of the command b = (b1,b2, . . . ,bn),
where bi (1 ≤ i ≤ n) represents the highest probability pdf-
id of the command at frame i. To have the original song
decoded as the desired command, we need to identify the
minimum modiﬁcation δ (t) on x(t) so that m is same
or close to b. Speciﬁcally, we minimize the L1 distance
between m and b. As m and b are related with the pdf-
id sequence, we deﬁne this method as pdf-id sequence
matching algorithm.
5For instance, the pdf-ids sequence for ehB should be 6383, 5760,
5760, 5760, 5760, 5760, 5760, 5760, 5760, 5760.
Based on these observations we construct the following
objective function:
argmin
δ (t)
(cid:6)g (x(t) + δ (t))− b(cid:6)1.
(1)
To ensure that the modiﬁed audio does not deviate too
much from the original one, we optimize the objective
function Eq (1) under the constraint of |δ (t)| ≤ l.
Finally, we use gradient descent [43], an iterative opti-
mization algorithm to ﬁnd the local minimum of a func-
tion, to solve the objective function. Given an initial point,
gradient descent follows the direction which reduces the
value of the function most quickly. By repeating this pro-
cess until the value starts to remain stable, the algorithm
is able to ﬁnd a local minimum value. In particular, based
on our objective function, we revise the song x(t) into
(cid:7)(t) = x(t) + δ (t) with the aim of making most likely
x
(cid:7)(t)) equal or close to b. Therefore, the crafted
pdf-ids g (x
audio x
(cid:7)(t) can be decoded as the desired command.
To further preserve the ﬁdelity of the original song, one
method is to minimize the time duration of the revision.
Typically, once the pure command voice audio is gen-
erated by a text-to-speech engine, all the phonemes are
determined, so as to the phoneme identiﬁers and b. How-
ever, the speed of the speech also determines the number
of frames and the number of transition-ids in a phoneme
identiﬁer. Intuitively, slow speech always produces re-
peated frames or transition-ids in a phoneme. Typically
people need six or more frames to realize a phoneme, but
most speech recognition systems only need three to four
frames to interpret a phoneme. Hence, to introduce the
minimal revision to the original song, we can analyze b,
reduce the number of repeated frames in each phoneme,
and obtain a shorter b(cid:7) = (b1,b2, . . . ,bq), where q < n.
4.3 Practical Attack over the Air
By feeding the generated adversarial sample directly into
Kaldi, the desired command can be decoded correctly.
However, playing the sample through a speaker to physi-
cally attack an IVC device typically cannot work. This is
mainly due to the noises introduced by the speaker and en-
vironment, as well as the distortion caused by the receiver
of the IVC device. In this paper, we do not consider the
invariance of background noise in different environments,
e.g., grocery, restaurant, ofﬁce, etc., due to the following
reasons: (1) In a quite noisy environment like restaurant
or grocery, even the original voice command y(t) may
not be correctly recognized by IVC devices; (2) Model-
ing any slightly variant background noise itself is still an
open research problem; (3) Based on our observation, in
a normal environment like home, ofﬁce, lobby, the major
impacts on the physical attack are the electronic noise
from the speaker and the distortion from the receiver of
the IVC devices, rather than the background noise.
54    27th USENIX Security Symposium
USENIX Association
Figure 3: Steps of attack.
Hence, our idea is to build a noise model, considering
the speaker noise, the receiver distortion, as well as the
generic background noise, and integrate it in the approach
in Section 4.2. Speciﬁcally, we carefully picked up several
songs and played them through our speaker in a very quiet
room. By comparing the recorded audio (captured by our
receiver) with the original one, we can capture the noises.
Note that playing “silent” audio does not work since the
electronic noise from speakers may depend on the sound
at different frequencies. Therefore, we intend to choose
the songs that cover more frequencies. Regarding the
comparison between two pieces of audio, we have to ﬁrst
manually align them and then compute the difference.
We redesign the objective function as shown in Eq (2).
(cid:6)g (x(t) + μ(t) + n(t))− b(cid:6)1,
(2)
argmin
μ(t)
5 Evaluation
In this section, we present the experimental results of
CommanderSong. We evaluated both the WTA and
WAA attacks against machine recognition. To eval-
uate the human comprehension, we conducted a sur-
vey examining the effects of “hiding” the desired com-
mand in the song. Then, we tested the transferability
of the adversarial sample on other ASR platforms, and
checked whether CommanderSong can spread through
Internet and radio.
Finally, we measured the efﬁ-
ciency in terms of the time to generate the Comman-
derSong. Demos of attacks are uploaded on the website
(https://sites.google.com/view/commandersong/).
where μ(t) is the perturbation that we add to the original
song, and n(t) is the noise samples that we captured. In
(cid:7)(t) = x(t) +
this way, we can get the adversarial audio x
μ(t) that can be used to launch the practical attack over
the air.
Such noise model above is quite device-dependent.
Since different speakers and receivers may introduce dif-
ferent noises/distortion when playing or receiving speciﬁc