### Server-Side Defenses and Challenges

While the described server-side defenses can mitigate some of these issues, they do not prevent leaky image attacks, as they do not affect GET requests. A significant challenge with all the mentioned server-side defenses is that they require developers to be aware of the vulnerability in the first place. In our experience, a complex website may allow sharing images in multiple ways, spanning different user interactions and API endpoints supported by the image sharing service. Rigorously inspecting all possible ways to share an image is non-trivial, highlighting the need for future work on automatically identifying leaky images.

At least parts of the methodology we propose could be automated with limited effort. To check whether an image request requires authentication, one can perform the request in one browser where the user is logged in and then try the same request in another instance of the browser in "private" or "incognito" mode, i.e., without being logged in. Comparing the success of the two requests reveals whether the image request relies on any form of authentication, such as cookies. Automating the rest of our methodology would require support from image sharing services, particularly APIs to handle user accounts and to share images between users.

Despite the challenges in identifying leaky images, we believe that server-side mitigations are the most straightforward solution, at least in the short term. In the long term, a more comprehensive solution would be desirable, such as those described below.

### Browser Mitigations

The current HTTP standard does not specify a policy for third-party cookies, but it encourages browser vendors to experiment with different policies. Most browsers decide to attach third-party cookies, but there are exceptions, such as the Tor browser, which sends cookies only to the domain typed by the user in the address bar.

Given the potential privacy implications of leaky images and other previously reported tracking techniques, one possible mitigation would be for browsers to default to not sending cookies with third-party (image) requests. If this behavior is overridden, possibly using a special HTTP header or tag, the user should be informed through a transparent mechanism. Additionally, the user should have the option to prevent the website from overriding the default behavior. This measure aligns with the newly adopted European Union’s General Data Protection Regulation, which requires data protection by design and by default. However, such a change may impact certain players in the web ecosystem, such as the advertising industry. To address this, advertisers may need to adopt safer distribution mechanisms, like those popularized by the Brave browser.

An alternative approach is to allow authenticated image requests but render them only if the browser is confident that there are no observable differences between an authenticated and a non-authenticated request. The browser could perform two image requests: one with third-party cookies and one without. If the browser receives two equivalent responses, it can safely render the content, as no sensitive information is leaked about the authenticated user. This solution would still allow most uses of third-party cookies, e.g., tracking pixels, but prevent the leaky image attack. A possible downside might be false positives due to strategy (3) in Table 2, but we hypothesize that such requests rarely appear in benign third-party image requests. Another drawback could be the increase in web traffic and potential performance penalties. Future work should test the benefits of this defense and the cost imposed by the additional image request.

To reduce the cost of an additional image request, a hybrid mechanism could disable authenticated image requests by default and allow them only for resources specified by a Content Security Policy (CSP) directive. For allowed authenticated images, the browser would deploy the double image request mechanism described earlier. We advocate this as our preferred browser-level defense, as it can also defend against other privacy attacks, e.g., reading third-party image pixels through a side channel, while still permitting benign uses.

Similarly to ShareMeNot [32], a browser mechanism could block all third-party image requests unless the user authorizes them by providing explicit consent. To reduce the burden on the user, a hybrid mechanism could require authenticated requests only for a subset of images, for which the user needs to provide consent.

Another solution, when third-party cookies are allowed, is for browsers to implement some form of information flow control to ensure that the success or failure of a third-party request cannot be sent outside the browser. A similar approach is used in tainted canvas, which disallows pixel reads after a third-party image is painted on the canvas. Implementing such information flow control for third-party images may be challenging in practice, as the success of an image load can be retrieved through multiple side channels, such as the object tag or by reading the size of the contained div.

The mechanisms described in this section vary in terms of implementation effort and their potential impact on the existing state of the web, i.e., compatibility with existing websites. Therefore, to aid browser vendors in making informed decisions, future work should perform an in-depth analysis of these defenses in terms of usability, compatibility, and deployment cost, in the style of Calzavara et al. [9], and possibly propose additional solutions.

### Better Privacy Control for Users

A concerning finding from our prevalence study is that users have little control over the image sharing process. For example, some image sharing services do not allow users to restrict which other users can privately share an image with them. In others, there is no way for a user to revoke their right to access a specific image. Moreover, in most of the websites we analyzed, it is difficult to obtain a complete list of images privately shared with the current account. For example, a motivated user who wants to obtain this list must check all the conversations in a messaging platform or all the images of all friends on a social network.

We believe that image sharing services should provide users with more control over image sharing policies, enabling privacy-aware users to protect their privacy. Specifically, a user should be able to decide who has the right to share an image with them and should be granted the right to revoke their access to a given image. Ideally, websites would also offer the user a list of all the images shared with them and a transparent notification mechanism that announces changes to this list. Empowering users with these tools may help mitigate some of the leaky image attacks by drawing attention to suspicious image sharing and allowing users to revoke access to leaky images.

The privacy controls for web users presented in this section will be useful mostly for advanced users, while the majority of users are unlikely to take advantage of such fine-grained controls. Therefore, we believe that the most effective mitigations against leaky images are at the server side or browser level.

### Related Work

Previous work has highlighted risks associated with images on the web, such as malicious JavaScript code embedded in SVGs [17], image-based fingerprinting of browser extensions [35], and leaking sensitive information, such as the gender or location of a user uploading an image [11]. This work introduces a new risk: privacy leaks due to shared images. Lekies et al. [24] describe privacy leaks resulting from dynamically generated JavaScript. The source of this problem is the same as for leaky images: both JavaScript code and images are exempted from the same-origin policy. While privacy leaks in dynamic JavaScript reveal confidential information about the user, such as credentials, leaky images allow for tracking specific users on third-party websites.

Heiderich et al. [18] introduce a scriptless, CSS-based web attack. The HTML-only variant of leaky images does not rely on CSS and differs in the kinds of leaked information: while the attack by Heiderich et al. leaks content of the current website, our attacks leak the identity of the user.

Wondracek et al. [46] present a privacy leak in social networks related to our group attack. In their work, the attacker neither has control over the group structure nor can easily track individuals. A more recent attack [41] deanonymizes social media users by correlating links on their profiles with browsing histories. In contrast, our attack does not require such histories. Another recent attack [44] retrieves sensitive information of social media accounts using the advertisement API provided by a social network. However, their attack cannot be used to track users on third-party websites.

Cross-Site Request Forgery (CSRF) is similar in spirit to leaky image attacks, as both rely on the fact that browsers send cookies with third-party requests. For CSRF, this behavior results in an unauthorized action on a third-party website, whereas for leaky images, it results in deanonymizing the user. Existing techniques for defending [5] and detecting [31] CSRF partially address but do not fully solve the problem of leaky images (Section 5).

Browser fingerprinting is a widely deployed persistent tracking mechanism. Various APIs have been proposed for fingerprinting, such as user agent and fonts [12], canvas [29, 10], ad blocker usage, and WebGL Renderer [22]. Empirical studies [12, 22] suggest that these techniques have enough entropy to identify most users or place a user in a small set of possible users, sometimes even across browsers [10]. The leaky image attack is complementary to fingerprinting, as discussed in detail in Section 3.6.

Another web tracking mechanism is through third-party requests, such as tracking pixels. Mayer and Mitchell [27] describe the tracking ecosystem and the privacy costs associated with these practices. Lerner et al. [25] show how tracking in popular websites evolves over time. Several other studies [42, 47, 13, 32, 8, 14] present a snapshot of third-party tracking on the web at various moments in time. One recurring conclusion of these studies is that a few big players can track most of the traffic on the Internet. We present the first image-based attack that allows a less powerful attacker to deanonymize visitors of a website.

Targeted attacks or advanced persistent threats are an increasingly popular class of cybersecurity incidents [37, 26]. Known attacks include spear phishing attacks [6] and targeted malware campaigns [26, 16]. Leaky images add a privacy-related attack to the set of existing targeted attacks.

Several empirical studies analyze different security and privacy aspects of websites in production, including messages [36], cookie stealing [4, 34], credentials theft [3], cross-site scripting [28, 23], browser fingerprinting [30, 1], deployment of CSP policies [45], and ReDoS vulnerabilities [38]. User privacy can also be impacted by security issues in browsers, such as JavaScript bindings bugs [7], micro-architectural bugs [20], and insufficient isolation of web content [19]. None of these studies explore privacy leaks caused by authenticated cross-origin image requests.

Van Goethem et al. [43] propose the use of timing channels for estimating the file size of a cross-origin resource. One could combine leaky images with such a channel to check if a privately shared image is accessible for a particular user, enabling the use of leaky images even if the browser blocks cross-origin image requests. One difference between our attack and theirs is that leaky images provide 100% certainty that a victim has visited a website, which a probabilistic timing channel cannot provide.

Several researchers document the difficulty of notifying the maintainers of websites or open-source projects about security bugs in software [24, 40, 39]. We experienced quick and helpful responses by all websites we contacted, with an initial response within less than a week. One reason for this difference may be that we used the bug bounty channels provided by the websites to report the problems [15, 48].

### Conclusions

This paper presents leaky images, a targeted deanonymization attack that leverages specific access control practices employed in popular websites. The main insight of the attack is a simple yet effective observation: privately shared resources that are exempted from the same-origin policy can be exploited to reveal whether a specific user is visiting an attacker-controlled website. We describe several variants of this attack: targeted tracking of single users, group tracking, pseudonym linking, and an HTML-only attack.

We show that some of the most popular websites suffer from leaky images, and that the problem often affects any registered users of these websites. We reported all the identified vulnerabilities to the security teams of the affected websites. Most of them acknowledge the problem, and some have already proceeded to fixing it. This feedback shows that the problem we identified is important to practitioners. Our paper helps raise awareness among developers and researchers to avoid this privacy issue in the future.

### Acknowledgments

Thanks to Stefano Calzavara and the anonymous reviewers for their feedback on this paper. This work was supported by the German Federal Ministry of Education and Research and by the Hessian Ministry of Science and the Arts within CRISP, by the German Research Foundation within the ConcSys and Perf4JS projects, and by the Hessian LOEWE initiative within the Software-Factory 4.0 project.

### References

[1] G. Acar, C. Eubank, S. Englehardt, M. Juárez, A. Narayanan, and C. Díaz, “The web never forgets: Persistent tracking mechanisms in the wild,” in Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security, Scottsdale, AZ, USA, November 3-7, 2014, pp. 674–689. [Online]. Available: http://doi.acm.org/10.1145/2660267.2660347

[2] G. Acar, M. Juárez, N. Nikiforakis, C. Díaz, S. F. Güres, F. Piessens, and B. Preneel, “FPDetective: Dusting the web for fingerprinters,” in 2013 ACM SIGSAC Conference on Computer and Communications Security, CCS’13, Berlin, Germany, November 4-8, 2013, pp. 1129–1140. [Online]. Available: http://doi.acm.org/10.1145/2508859.2516674

[3] S. V. Acker, D. Hausknecht, and A. Sabelfeld, “Measuring login webpage security,” in Proceedings of the Symposium on Applied Computing, SAC 2017, Marrakech, Morocco, April 3-7, 2017, pp. 1753–1760. [Online]. Available: http://doi.acm.org/10.1145/3019612.3019798

[4] D. J. andZhaoGL15 Ranjit Jhala, S. Lerner, and H. Shacham, “An empirical study of privacy-violating information flows in JavaScript web applications,” in Proceedings of the 17th ACM Conference on Computer and Communications Security, CCS 2010, Chicago, Illinois, USA, October 4-8, 2010, pp. 270–283. [Online]. Available: http://doi.acm.org/10.1145/1866307.1866339

[5] A. Barth, C. Jackson, and J. C. Mitchell, “Robust defenses for cross-site request forgery,” in Proceedings of the 2008 ACM Conference on Computer and Communications Security, CCS 2008, Alexandria, Virginia, USA, October 27-31, 2008, pp. 75–88. [Online]. Available: http://doi.acm.org/10.1145/1455770.1455782

[6] S. L. Blond, A. Uritesc, C. Gilbert, Z. L. Chua, P. Saxena, and E. Kirda, “A look at targeted attacks through the lens of an NGO,” in Proceedings of the 23rd USENIX Security Symposium, San Diego, CA, USA, August 20-22, 2014, pp. 543–558. [Online]. Available: https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/le-blond