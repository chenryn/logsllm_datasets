from these problems, but it also does not prevent leaky im-
ages attacks, as it does not affect GET requests.
A challenge with all the described server-side defenses is
that they require the developers to be aware of the vulner-
ability in the ﬁrst place. From our experience, a complex
website may allow sharing images in several ways, possibly
spanning different UI-level user interactions and different
API endpoints supported by the image sharing service. Since
rigorously inspecting all possible ways to share an image is
non-trivial, we see a need for future work on automatically
identifying leaky images. At least parts of the methodology
we propose could be automated with limited effort. To check
whether an image request requires authentication, one can
perform the request in one browser where the user is logged
in, and then try the same request in another instance of the
browser in “private” or “incognito” mode, i.e., without being
logged in. Comparing the success of the two requests reveals
whether the image request relies in any form of authentica-
tion, such as cookies. Automating the rest of our method-
ology requires some support by image sharing services. In
particular, automatically checking that a leaky image is ac-
cessible only by a subset of a website’s users, requires APIs
USENIX Association
28th USENIX Security Symposium    933
to handle user accounts and to share images between users.
Despite the challenges in identifying leaky images, we
believe that server-side mitigations are the most straightfor-
ward solution, at least in the short term. In the long term,
a more complete solution would be desirable, such as those
described in the following.
5.2 Browser Mitigations
The current HTTP standard does not specify a policy for
third-party cookies10, but it encourages browser vendors to
experiment with different such policies. More precisely, the
current standard lets the browser decide whether to automat-
ically attach the user’s cookie to third-party requests. Most
browsers decide to attach third-party cookies, but there are
certain counter-examples, such as the Tor browser. In Tor,
cookies are sent only to the domain typed by the user in the
address bar.
Considering the possible privacy implications of leaky im-
ages and other previously reported tracking techniques [8],
one possible mitigation would be that browsers specify as
default behavior to not send cookies with third-party (image)
requests. If this behavior is overwritten, possibly using a spe-
cial HTTP header or tag, the user should be informed through
a transparent mechanism. Moreover, the user should be of-
fered the possibility to prevent the website from overwriting
the default behavior. We believe this measure would be in
the spirit of the newly adopted European Union’s General
Data Protection Regulation which requires data protection
by design and by default. However, such an extreme move
may impact certain players in the web ecosystem, such as the
advertisement ecosystem. To address this issue, advertisers
may decide to move towards safer distribution mechanisms,
such as the one popularized by the Brave browser.
An alternative to the previously discussed policy is to al-
low authenticated image requests, but only render them if the
browser is conﬁdent that there are no observable differences
between an authenticated request and a non-authenticated
one. To this end, the browser could perform two image re-
quests instead of one: one request with third-party cookies
and one request without. If the browser receives two equiv-
alent responses, it can safely render the content, since no
sensitive information is leaked about the authenticated user.
This solution would still allow most of the usages of third-
party cookies, e.g. tracking pixels, but prevent the leaky im-
age attack described here. A possible downside might be the
false positives due to strategy (3) in Table 2, but we hypoth-
esize that requests to such images rarely appear in benign
third-party image requests. A second possible drawback of
this solution may be the increase in web trafﬁc and the po-
tential performance penalties. Future work should test the
beneﬁts of this defense and the cost imposed by the addi-
tional image request.
10https://tools.ietf.org/html/rfc6265#page-28
To reduce the cost imposed by an additional image re-
quest, a hybrid mechanism could disable authenticated im-
age requests by default, and allow them only for the re-
sources speciﬁed by a CSP directive. For the allowed au-
thenticated images, the browser deploys the double image
requests mechanism described earlier. We advocate this as
our preferred browser-level defense since it can also defend
against other privacy attacks, e.g. reading third-party image
pixels through a side channel [21], while still permitting be-
nign uses.
Similarly to ShareMeNot [32], one can also implement a
browser mechanism in which all third-party image requests
are blocked unless the user authorizes them by providing ex-
plicit consent. To release the burden from the user, a hybrid
mechanism can be deployed in which the website requires
authenticated requests only for a subset of images for which
the user needs to provide consent.
Another solution for when third-party cookies are allowed
is for browsers to implement some form of information ﬂow
control to ensure that the fact whether a third-party request
was successfully loaded or not, cannot be sent outside of
the browser. A similar approach is deployed in tainted can-
vas11, which disallows pixel reads after a third-party image
is painted on the canvas. Implementing such an information
ﬂow control for third-party images may, however, be chal-
lenging in practice, since the fact whether an image has suc-
cessfully loaded or not can be retrieved through multiple side
channels, such as the object tag or by reading the size of
the contained div.
The mechanisms described in this section vary both in
terms of implementation effort required for deploying them
and in terms of their possible impact on the existing state of
the web, i.e., incompatibility with existing websites. There-
fore, to aid the browser vendors to take an informed decision,
future work should perform an in-depth analysis of all these
defenses in terms of usability, compatibility and deployment
cost, in the style of Calzavara et al. [9], and possibly propose
additional solutions.
5.3 Better Privacy Control for Users
A worrisome ﬁnding of our prevalence study is that a user
has little control over the image sharing process. For exam-
ple, for some image sharing services, the user does not have
any option to restrict which other users can privately share an
image with her. In others, there is no way for a user to revoke
her right to access a speciﬁc image. Moreover, in most of the
websites we analyzed, it is difﬁcult to even obtain a complete
list of images privately shared with the current account. For
example, a motivated user who wants to obtain this list must
check all the conversations in a messaging platform, or all
the images of all friends on a social network.
11https://html.spec.whatwg.org/multipage/canvas.
html#security-with-canvas-elements
934    28th USENIX Security Symposium
USENIX Association
We believe that image sharing services should provide
users more control over image sharing policies, to enable
privacy-aware users to protect their privacy. Speciﬁcally, a
user should be allowed to decide who has the right to share
an image with her and she should be granted the right to re-
voke her access to a given image. Ideally, websites would
also offer the user a list of all the images shared with her and
a transparent notiﬁcation mechanism that announces the user
when certain changes are made to this list. Empowering the
users with these tools may help mitigate some of the leaky
image attacks by attracting user’s attention to suspicious im-
age sharing, allowing users to revoke access to leaky images.
The privacy controls for web users presented in this sec-
tion will be useful mostly for advanced users, while the ma-
jority of the users are unlikely to take advantage of such ﬁne-
grained controls. Therefore, we believe that the most effec-
tive mitigations against leaky images are at the server side or
browser level.
6 Related Work
Previous work shows risks associated with images on
the web, such as malicious JavaScript code embedded in
SVGs [17], image-based ﬁngerprinting of browser exten-
sions [35], and leaking sensitive information, such as the
gender or the location of a user uploading an image [11].
This work introduces a new risk: privacy leaks due to shared
images. Lekies et al. [24] describe privacy leaks resulting
from dynamically generated JavaScript. The source of this
problem is the same as for leaky images: both JavaScript
code and images are excepted from the same-origin policy.
While privacy leaks in dynamic JavaScript reveal conﬁden-
tial information about the user, such as credentials, leaky im-
ages allow for tracking speciﬁc users on third-party websites.
Heiderich et al. [18] introduce a scriptless, CSS-based web
attacks. The HTML-only variant of leaky images does not
rely on CSS and also differ in the kinds of leaked informa-
tion: While the attack by Heiderich et al. leaks content of the
current website, our attacks leak the identity of the user.
Wondracek et al. [46] present a privacy leak in social net-
works related to our group attack. In their work, the attacker
neither has control over the group structure nor can she easily
track individuals. A more recent attack [41] deanonymizes
social media users by correlating links on their proﬁles with
browsing histories. In contrast, our attack does not require
such histories. Another recent attack [44] retrieves sensi-
tive information of social media accounts using the adver-
tisement API provided by a social network. However, their
attack cannot be used to track users on third-party websites.
Cross-Site Request Forgery (CSRF) is similar in spirit to
leaky image attacks: both rely on the fact that browsers send
cookies with third-party requests. For CSRF, this behav-
ior results in an unauthorized action on a third-party web-
site, whereas for leaky images, it results in deanonymizing
the user. Existing techniques for defending [5] and detect-
ing [31] CSRF partially address but do not fully solve the
problem of leaky images (Section 5).
Browser ﬁngerprinting is a widely deployed [1, 2, 30] per-
sistent tracking mechanism. Various APIs have been pro-
posed for ﬁngerprinting: user agent and fonts [12], can-
vas [29, 10], ad blocker usage, and WebGL Renderer [22].
Empirical studies [12, 22] suggest that these technique have
enough entropy to identify most of the users, or at least, to
place a user in a small set of possible users, sometimes even
across browsers [10]. The leaky image attack is complemen-
tary to ﬁngerprinting, as discussed in detail in Section 3.6.
Another web tracking mechanism is through third-party
requests, such as tracking pixels. Mayer and Mitchell [27]
describe the tracking ecosystem and the privacy costs as-
sociated with these practices. Lerner et al. [25] show how
tracking in popular websites evolves over time. Several other
studies [42, 47, 13, 32, 8, 14] present a snapshot of the third-
party tracking on the web at various moments in time. One
of the recurring conclusion of these studies was that few big
players can track most of the trafﬁc on the Internet. We
present the ﬁrst image-based attack that allows a less pow-
erful attacker to deanonymize visitors of a website.
Targeted attacks or advanced persistent threats are an in-
creasingly popular class of cybersecurity incidents [37, 26].
Known attacks include spear phishing attacks [6] and tar-
geted malware campaigns [26, 16]. Leaky images adds a
privacy-related attack to the set of existing targeted attacks.
Several empirical studies analyze different security and
postMes-
privacy aspects of websites in production:
sages [36], cookie stealing [4, 34], credentials theft [3],
cross-site scripting [28, 23], browser ﬁngerprinting [30, 1],
deployment of CSP policies [45], and ReDoS vulnerabili-
ties [38]. User privacy can also be impacted by security
issues in browsers, such as JavaScript bindings bugs [7],
micro-architectural bugs [20], and insufﬁcient isolation of
web content [19]. Neither of these studies explores privacy
leaks caused by authenticated cross-origin image requests.
Van Goethem et al. [43] propose the use of timing chan-
nels for estimating the ﬁle size of a cross-origin resource.
One could combine leaky images with such a channel to
check if a privately shared image is accessible for a particu-
lar user, enabling the use of leaky images even if the browser
would block cross-origin image requests. One difference
between our attack and theirs is that leaky images provide
100% certainty that a victim has visited a website, which a a
probabilistic timing channel cannot provide.
Several researchers document the difﬁculty of notifying
the maintainers of websites or open-source projects about se-
curity bugs in software [24, 40, 39]. We experienced quick
and helpful responses by all websites we contacted, with an
initial response within less than a week. One reason for this
difference may be that we used the bug bounty channels pro-
vided by the websites to report the problems [15, 48].
USENIX Association
28th USENIX Security Symposium    935
7 Conclusions
This paper presents leaky images, a targeted deanonymiza-
tion attack that leverages speciﬁc access control practices
employed in popular websites. The main insight of the at-
tack is a simple yet effective observation: Privately shared
resources that are exempted from the same origin policy can
be exploited to reveal whether a speciﬁc user is visiting an
attacker-controlled website. We describe several ﬂavors of
this attack: targeted tracking of single users, group tracking,
pseudonym linking, and an HTML-only attack.
We show that some of the most popular websites suffer
from leaky images, and that the problem often affects any
registered users of these websites. We reported all the identi-
ﬁed vulnerabilities to the security teams of the affected web-
sites. Most of them acknowledge the problem and some
already proceeded to ﬁxing it. This feedback shows that
the problem we identiﬁed is important to practitioners. Our
paper helps raising awareness among developers and re-
searchers to avoid this privacy issue in the future.
Acknowledgments
Thanks to Stefano Calzavara and the anonymous reviewers for their
feedback on this paper. This work was supported by the German
Federal Ministry of Education and Research and by the Hessian
Ministry of Science and the Arts within CRISP, by the German Re-
search Foundation within the ConcSys and Perf4JS projects, and
by the Hessian LOEWE initiative within the Software-Factory 4.0
project.
References
[1] G. Acar, C. Eubank, S. Englehardt, M. Ju´arez,
A. Narayanan, and C. D´ıaz, “The web never forgets:
Persistent
tracking mechanisms in the wild,” in
Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, Scottsdale,
AZ, USA, November 3-7, 2014, 2014, pp. 674–
689. [Online]. Available: http://doi.acm.org/10.1145/
2660267.2660347
[2] G. Acar, M. Ju´arez, N. Nikiforakis, C. D´ıaz, S. F.
G¨urses, F. Piessens, and B. Preneel, “Fpdetective:
dusting the web for ﬁngerprinters,” in 2013 ACM
SIGSAC Conference on Computer and Communica-
tions Security, CCS’13, Berlin, Germany, November
4-8, 2013, 2013, pp. 1129–1140. [Online]. Available:
http://doi.acm.org/10.1145/2508859.2516674
[3] S. V. Acker, D. Hausknecht, and A. Sabelfeld,
“Measuring login webpage security,” in Proceedings
of the Symposium on Applied Computing, SAC 2017,
Marrakech, Morocco, April 3-7, 2017, 2017, pp.
1753–1760. [Online]. Available: http://doi.acm.org/10.
1145/3019612.3019798
[4] D. J. andZhaoGL15 Ranjit Jhala, S. Lerner, and
H. Shacham, “An empirical study of privacy-violating
information ﬂows in javascript web applications,” in
Proceedings of the 17th ACM Conference on Computer
and Communications Security, CCS 2010, Chicago,
Illinois, USA, October 4-8, 2010, 2010, pp. 270–
283. [Online]. Available: http://doi.acm.org/10.1145/
1866307.1866339
[5] A. Barth, C. Jackson, and J. C. Mitchell, “Robust
defenses for cross-site request forgery,” in Proceedings
of
the 2008 ACM Conference on Computer and
Communications Security, CCS 2008, Alexandria,
Virginia, USA, October 27-31, 2008, 2008, pp. 75–
88. [Online]. Available: http://doi.acm.org/10.1145/
1455770.1455782
and E. Kirda,
[6] S. L. Blond, A. Uritesc, C. Gilbert, Z. L.
Chua, P. Saxena,
“A look at
targeted attacks through the lense of an NGO,”
in Proceedings of
the 23rd USENIX Security
Symposium, San Diego, CA, USA, August 20-22,
2014., 2014, pp. 543–558.
[Online]. Available:
https://www.usenix.org/conference/usenixsecurity14/
technical-sessions/presentation/le-blond