7The corresponding ﬁgures for other objective functions are similar; we
omit them for brevity.
B. Box constraints
To ensure the modiﬁcation yields a valid image, we have a
constraint on δ: we must have 0 ≤ xi + δi ≤ 1 for all i. In the
optimization literature, this is known as a “box constraint.”
Previous work uses a particular optimization algorithm, L-
BFGS-B, which supports box constraints natively.
We investigate three different methods of approaching this
problem.
1) Projected gradient descent performs one step of standard
gradient descent, and then clips all the coordinates to be
within the box.
This approach can work poorly for gradient descent
approaches that have a complicated update step (for
example,
those with momentum): when we clip the
actual xi, we unexpectedly change the input to the next
iteration of the algorithm.
2) Clipped gradient descent does not clip xi on each
iteration; rather, it incorporates the clipping into the
objective function to be minimized. In other words, we
replace f (x + δ) with f (min(max(x + δ, 0), 1)), with
the min and max taken component-wise.
While solving the main issue with projected gradient de-
scent, clipping introduces a new problem: the algorithm
can get stuck in a ﬂat spot where it has increased some
component xi to be substantially larger than the maxi-
mum allowed. When this happens, the partial derivative
becomes zero, so even if some improvement is possible
by later reducing xi, gradient descent has no way to
detect this.
3) Change of variables introduces a new variable w and
instead of optimizing over the variable δ deﬁned above,
we apply a change-of-variables and optimize over w,
setting
(tanh(wi) + 1) − xi.
δi =
1
2
Since −1 ≤ tanh(wi) ≤ 1, it follows that 0 ≤ xi + δi ≤
1, so the solution will automatically be valid. 8
We can think of this approach as a smoothing of clipped
gradient descent that eliminates the problem of getting
stuck in extreme regions.
These methods allow us to use other optimization algo-
rithms that don’t natively support box constraints. We use the
Adam [23] optimizer almost exclusively, as we have found it to
be the most effective at quickly ﬁnding adversarial examples.
We tried three solvers — standard gradient descent, gradient
descent with momentum, and Adam — and all three produced
identical-quality solutions. However, Adam converges substan-
tially more quickly than the others.
C. Evaluation of approaches
For each possible objective function f (·) and method to
enforce the box constraint, we evaluate the quality of the
adversarial examples found.
8Instead of scaling by 1
2 we scale by 1
2
+  to avoid dividing by zero.
45
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
Change of
Variable
Best Case
Clipped
Descent
prob mean
Projected
Descent
prob mean
prob
100% 2.93 100% 2.31 100%
80% 3.97
83%
77% 4.07
82%
86% 6.52 100% 7.53 100%
100% 2.20 100% 1.94 100%
100% 2.18 100% 1.95 100%
100% 2.21 100% 1.94 100%
83% 3.49
81% 3.76
mean
2.46
4.55
4.54
5.01
1.97
1.94
1.96
f1
f2
f3
f4
f5
f6
f7
Average Case
Worst Case
Change of
Variable
prob mean
Clipped
Descent
Projected
Descent
prob mean
prob
100% 5.21 100% 4.11 100%
44% 8.99
74%
74%
44% 9.55
55% 7.49
71%
100% 4.20 100% 3.47 100%
100% 4.11 100% 3.41 100%
100% 4.14 100% 3.43 100%
63% 15.06
63% 15.84
71% 7.60
mean
4.35
3.22
3.47
4.03
3.58
3.47
3.53
Change of
Variable
Clipped
Descent
Projected
Descent
prob mean
mean
prob mean
7.76 100% 9.48 100% 7.37
40% 18.90
2.93
41% 24.01
3.09
35% 4.10
3.55
6.42 100% 7.86 100% 6.12
6.03 100% 7.50 100% 5.89
6.20 100% 7.57 100% 5.94
18% 10.22
17% 11.91
24% 4.25
prob
100%
53%
59%
35%
100%
100%
100%
EVALUATION OF ALL COMBINATIONS OF ONE OF THE SEVEN POSSIBLE OBJECTIVE FUNCTIONS WITH ONE OF THE THREE BOX CONSTRAINT ENCODINGS.
WE SHOW THE AVERAGE L2 DISTORTION, THE STANDARD DEVIATION, AND THE SUCCESS PROBABILITY (FRACTION OF INSTANCES FOR WHICH AN
ADVERSARIAL EXAMPLE CAN BE FOUND). EVALUATED ON 1000 RANDOM INSTANCES. WHEN THE SUCCESS IS NOT 100%, MEAN IS FOR SUCCESSFUL
TABLE III
ATTACKS ONLY.
To choose the optimal c, we perform 20 iterations of binary
search over c. For each selected value of c, we run 10, 000
iterations of gradient descent with the Adam optimizer. 9
The results of this analysis are in Table III. We evaluate
the quality of the adversarial examples found on the MNIST
and CIFAR datasets. The relative ordering of each objective
function is identical between the two datasets, so for brevity
we report only results for MNIST.
There is a factor of three difference in quality between the
best objective function and the worst. The choice of method
for handling box constraints does not impact the quality of
results as signiﬁcantly for the best minimization functions.
In fact,
the worst performing objective function, cross
entropy loss, is the approach that was most suggested in the
literature previously [46], [42].
Why are some loss functions better than others? When c =
0, gradient descent will not make any move away from the
initial image. However, a large c often causes the initial steps
of gradient descent to perform in an overly-greedy manner,
only traveling in the direction which can most easily reduce
f and ignoring the D loss — thus causing gradient descent to
ﬁnd sub-optimal solutions.
is useful
throughout
This means that for loss function f1 and f4, there is no
the duration of
good constant c that
the gradient descent search. Since the constant c weights the
relative importance of the distance term and the loss term, in
order for a ﬁxed constant c to be useful, the relative value of
these two terms should remain approximately equal. This is
not the case for these two loss functions.
To explain why this is the case, we will have to take a side
discussion to analyze how adversarial examples exist. Consider
(cid:2) on a network.
a valid input x and an adversarial example x
What does it look like as we linearly interpolate from x to
(cid:2) for α ∈ [0, 1]. It turns out the
(cid:2)? That is, let y = αx+(1−α)x
x
value of Z(·)t is mostly linear from the input to the adversarial
example, and therefore the F (·)t is a logistic. We verify this
fact empirically by constructing adversarial examples on the
9Adam converges to 95% of optimum within 1, 000 iterations 92% of the
time. For completeness we run it for 10, 000 iterations at each step.
46
ﬁrst 1, 000 test images on both the MNIST and CIFAR dataset
with our approach, and ﬁnd the Pearson correlation coefﬁcient
r > .9.
Given this, consider loss function f4 (the argument for f1 is
similar). In order for the gradient descent attack to make any
change initially, the constant c will have to be large enough
that
 < c(f1(x + ) − f1(x))
or, as  → 0,
1/c < |∇f1(x)|
implying that c must be larger than the inverse of the gradient
to make progress, but the gradient of f1 is identical to F (·)t
so will be tiny around the initial image, meaning c will have
to be extremely large.
However, as soon as we leave the immediate vicinity of
the initial image, the gradient of ∇f1(x + δ) increases at an
exponential rate, making the large constant c cause gradient
descent to perform in an overly greedy manner.
We verify all of this theory empirically. When we run our
−10 to 1010 the average
attack trying constants chosen from 10
constant for loss function f4 was 106.
−20 but 2
The average gradient of the loss function f1 around the valid
−1 at the closest adversarial example. This
image is 2
means c is a million times larger than it has to be, causing
the loss function f4 and f1 to perform worse than any of the
others.
D. Discretization
We model pixel intensities as a (continuous) real number in
the range [0, 1]. However, in a valid image, each pixel intensity
must be a (discrete) integer in the range {0, 1, . . . , 255}. This
additional requirement
is not captured in our formulation.
In practice, we ignore the integrality constraints, solve the
continuous optimization problem, and then round to the nearest
integer: the intensity of the ith pixel becomes (cid:9)255(xi + δi)(cid:10).
This rounding will slightly degrade the quality of the
adversarial example. If we need to restore the attack quality,
we perform greedy search on the lattice deﬁned by the discrete
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
Target Classiﬁcation (L2)
7
2
3
4
5
6
0
1
0
1
2
n
o
i
t
a
c
ﬁ
3
i
s
s
a
l
C
4
5
Target Classiﬁcation (L0)
7
2
3
4
5
6
8
9
8
9
0
1
0
1
2
n
o
i
t
a
c
ﬁ
3
i
s
s
a
l
C
4
5
e
c
r
u
o
S
6
7
8
9
e
c
r
u
o
S
6
7
8
9
Fig. 3. Our L2 adversary applied to the MNIST dataset performing a targeted
attack for every source/target pair. Each digit is the ﬁrst image in the dataset
with that label.
Fig. 4. Our L0 adversary applied to the MNIST dataset performing a targeted
attack for every source/target pair. Each digit is the ﬁrst image in the dataset
with that label.
solutions by changing one pixel value at a time. This greedy
search never failed for any of our attacks.
Prior work has largely ignored the integrality constraints.10
For instance, when using the fast gradient sign attack with  =
0.1 (i.e., changing pixel values by 10%), discretization rarely
affects the success rate of the attack. In contrast, in our work,
we are able to ﬁnd attacks that make much smaller changes
to the images, so discretization effects cannot be ignored. We
take care to always generate valid images; when reporting the
success rate of our attacks, they always are for attacks that
include the discretization post-processing.
VI. OUR THREE ATTACKS
A. Our L2 Attack
∗
Putting these ideas together, we obtain a method for ﬁnding
adversarial examples that will have low distortion in the L2
metric. Given x, we choose a target class t (such that we have
t (cid:5)= C
minimize (cid:6) 1
2
with f deﬁned as
(x)) and then search for w that solves
2 + c · f (
(tanh(w) + 1) − x(cid:6)2
(tanh(w) + 1)
1
2
(cid:2)
f (x
) = max(max{Z(x
(cid:2)
)i : i (cid:5)= t} − Z(x
(cid:2)
)t,−κ).
This f is based on the best objective function found earlier,
modiﬁed slightly so that we can control the conﬁdence with
which the misclassiﬁcation occurs by adjusting κ. The param-
eter κ encourages the solver to ﬁnd an adversarial instance
(cid:2) that will be classiﬁed as class t with high conﬁdence. We
x
set κ = 0 for our attacks but we note here that a side beneﬁt
10One exception: The JSMA attack [38] handles this by only setting the
output value to either 0 or 255.
47
of this formulation is it allows one to control for the desired
conﬁdence. This is discussed further in Section VIII-D.
Figure 3 shows this attack applied to our MNIST model
for each source digit and target digit. Almost all attacks are
visually indistinguishable from the original digit.
A comparable ﬁgure (Figure 12) for CIFAR is in the ap-
pendix. No attack is visually distinguishable from the baseline
image.
Multiple starting-point gradient descent. The main problem
with gradient descent is that its greedy search is not guaranteed
to ﬁnd the optimal solution and can become stuck in a local
minimum. To remedy this, we pick multiple random starting
points close to the original image and run gradient descent
from each of those points for a ﬁxed number of iterations.
We randomly sample points uniformly from the ball of radius
r, where r is the closest adversarial example found so far.
Starting from multiple starting points reduces the likelihood
that gradient descent gets stuck in a bad local minimum.
B. Our L0 Attack
The L0 distance metric is non-differentiable and therefore
is ill-suited for standard gradient descent. Instead, we use an
iterative algorithm that, in each iteration, identiﬁes some pixels
that don’t have much effect on the classiﬁer output and then
ﬁxes those pixels, so their value will never be changed. The
set of ﬁxed pixels grows in each iteration until we have, by
process of elimination, identiﬁed a minimal (but possibly not
minimum) subset of pixels that can be modiﬁed to generate an
adversarial example. In each iteration, we use our L2 attack
to identify which pixels are unimportant.
In more detail, on each iteration, we call the L2 adversary,
restricted to only modify the pixels in the allowed set. Let
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:27:35 UTC from IEEE Xplore.  Restrictions apply. 
δ be the solution returned from the L2 adversary on input
image x, so that x + δ is an adversarial example. We compute
g = ∇f (x + δ) (the gradient of the objective function,
evaluated at the adversarial instance). We then select the pixel
i = arg mini gi · δi and ﬁx i, i.e., remove i from the allowed
set.11 The intuition is that gi·δi tells us how much reduction to
f (·) we obtain from the ith pixel of the image, when moving
from x to x + δ: gi tells us how much reduction in f we
obtain, per unit change to the ith pixel, and we multiply this
by how much the ith pixel has changed. This process repeats
until the L2 adversary fails to ﬁnd an adversarial example.
There is one ﬁnal detail required to achieve strong results:
choosing a constant c to use for the L2 adversary. To do this,
−4). We then
we initially set c to a very low value (e.g., 10
run our L2 adversary at this c-value. If it fails, we double c
and try again, until it is successful. We abort the search if c
exceeds a ﬁxed threshold (e.g., 1010).
JSMA grows a set — initially empty — of pixels that are
allowed to be changed and sets the pixels to maximize the total
loss. In contrast, our attack shrinks the set of pixels — initially
containing every pixel — that are allowed to be changed.
Our algorithm is signiﬁcantly more effective than JSMA
(see Section VII for an evaluation). It is also efﬁcient: we
introduce optimizations that make it about as fast as our L2
attack with a single starting point on MNIST and CIFAR; it is
substantially slower on ImageNet. Instead of starting gradient
descent in each iteration from the initial image, we start the