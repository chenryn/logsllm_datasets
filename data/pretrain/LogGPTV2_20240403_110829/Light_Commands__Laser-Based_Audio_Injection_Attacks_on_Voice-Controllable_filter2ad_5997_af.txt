non-MEMS microphones. We empirically verify this using
a Sanwa 400-MC010 Electret Condenser Microphone, aim-
ing the (blue) laser beam through the microphone’s metallic
mesh (See Figure 13 (Left)). Using the same parameters as in
Section 4.2 (e.g., IDC = 200 mA and Ipp = 150 mA), we play
a chirp signal varying frequency linearly from 0 to 10 kHz in
5 seconds. Figure 13 (Right) shows the spectrogram of the
audio recorded by the microphone, clearly showing repeated
diagonal lines that correspond to the linear frequency sweep.
We thus conclude that our results are also applicable beyond
MEMS microphones, to electret condenser microphones.
USENIX Association
29th USENIX Security Symposium    2645
Laser current driverLaser spot (green)Audio amplifierA cheap laser pointerwith alligator clipson its battery electrodesAudio cablefrom PC 5V7 Countermeasures and Limitations
7.1 Software-Based Approach
As discussed in Section 6.2, an additional layer of authen-
tication can be effective at somewhat mitigating the attack.
Alternatively, in case the attacker cannot eavesdrop on the
device’s response (for example since the device is located
far away behind a closed window), having the VC system
ask the user a simple randomized question before command
execution can be an effective way to prevent the attacker from
obtaining successful command execution. However, note that
adding an additional layer of interaction often comes at a cost
of usability, limiting user adoption.
Next, manufacturers can attempt to use sensor fusion tech-
niques [38] in the hopes of detecting light-based command
injection. More speciﬁcally, voice assistants often have multi-
ple microphones, which should receive similar signals due to
the omnidirectional nature of sound propagation. Meanwhile,
when the attacker uses a single laser, only one microphone
receives a signal while the others receive nothing. Thus, man-
ufacturers can attempt to mitigate the attack presented in
this paper by comparing signals from multiple microphones,
ignoring injected commands using a single laser. However,
attackers can attempt to defeat such comparison countermea-
sures by simultaneously injecting light to all the device’s
microphones using multiple lasers or wide beams, see Sec-
tion 6.5. We leave this task of implementing such defenses
and investigating their security properties to future work.
Finally, LightCommands are very different compared to
normal audible commands. For sensor-rich devices like
phones and tablets, sensor-based intrusion detection tech-
niques [39] can potentially be used to identity and subse-
quently block such irregular command injection. We leave
further exploration of this direction to future work.
7.2 Hardware-Based Approach
It is possible to reduce the amount of light reaching the mi-
crophone’s diaphragm using a barrier or diffracting ﬁlm that
physically blocks straight light beams, while allowing sound
waves to detour around it. Performing a literature review on
proposed microphone designs, we have found several such
suggestions, mainly aimed to protect microphones from sud-
den pressure spikes. For example, the designs in Figure 14
have a silicon plate or movable shutter, both of which elimi-
nate the line of sight to the diaphragm [40]. It is important to
note however, that such barriers should be opaque to all light
wavelengths (including infrared and ultraviolet), preventing
the attacker from going through the barrier using a different
colored light. Finally, a light-blocking barrier can be also im-
plemented at the device level, by placing a non-transparent
cover on top of the microphone hole, which attenuates the
amount of light hitting the microphone.
Figure 14: Designs of MEMS microphone with light-blocking
barriers [40]
7.3 Limitations
Hardware Limitations.
Being a light-based attack,
LightCommands inherits all the limitations of light-related
physics. In particular, LightCommands assumes a line-of-
sight threat model and does not properly penetrate opaque
obstacles which might be penetrable to sound. Thus, even
if attacking fabric-covered devices is sometimes possible
(Section 5.2, Google Home Mini), we believe that for fabric-
covered microphones’ ports, the thickness of the cover can
prevent successful attacks (e.g., in the case of Apple Home-
pods). We leave the analysis of such scenarios to future work.
In addition, unlike sound, LightCommands requires careful
aiming and line of sight access. In our experiments, we show
how to partially overcome this limitation by using a telescope
to remotely determine the assistant type and location of the
microphones from the device’s appearance.
Finally, while line of sight access is often available for
smart speakers visible through windows, the situation is dif-
ferent for mobile devices such as smart watches, phones and
tablets. This is since unlike static smart speakers, these de-
vices are often mobile, requiring an attacker to quickly aim
and inject commands. When combined with the precise aim-
ing and higher laser power required to attack such devices, suc-
cessful LightCommands attacks might be particularly chal-
lenging. We thus leave the task of systematically exploring
such devices to future work.
Liveness Test and Continuous Authentication. Unlike
some other injection attacks, LightCommands’ threat model
and lack of proper feedback channels make it difﬁcult for the
attacker to pass any sorts of liveness checks or continuous
authentication methods. These can be as primitive as asking
a user simple questions before performing a command, or as
sophisticated as using data from different microphones [41,
42, 43], sound reﬂections [44], or other sensors [45] to verify
that the incoming commands were indeed spoken by a live
human. We leave the task of implementing such defenses in
deployed VC systems as an avenue for future works.
8 Conclusions and Future Work
In this paper we presented LightCommands, which is an at-
tack that uses light to inject commands into voice-controllable
systems from large distances. To mount the attack, we trans-
mit light modulated with an audio signal, which is con-
2646    29th USENIX Security Symposium
USENIX Association
Silicon plateAcoustic portPCBPCBDiaphragmMovable shutterverted back to audio within the microphone. We demon-
strated LightCommands on many commercially-available
voice-controllable systems that use Siri, Portal, Google As-
sistant, and Alexa, obtaining successful command injections
at a distance of more than 100 meters while penetrating clear
glass windows. Next, we highlight deﬁciencies in the secu-
rity of voice-controllable systems, which leads to additional
compromises of third-party hardware such as locks and cars.
Better understanding of the physics behind the attack will
beneﬁt both new attacks and countermeasures. In particular,
we can possibly use the same principle to mount other acous-
tic injection attacks (e.g., on motion sensors) using light. In
addition, heating by laser can also be an effective way of
injecting false signals to sensors.
9 Acknowledgments
We thank John Nees for advice on laser operation and laser
optics. This research was funded by JSPS KAKENHI Grant
#JP18K18047 and #JP18KK0312, by DARPA and AFRL
under contracts FA8750-19-C-0531 and HR001120C0087,
by NSF under grants CNS-1954712 and CNS-2031077, gifts
from Intel, AMD, and Analog Devices, and an award from
MCity at the University of Michigan.
References
[1] W. Diao, X. Liu, Z. Zhou, and K. Zhang, “Your voice as-
sistant is mine: How to abuse speakers to steal informa-
tion and control your phone,” in Workshop on Security
and Privacy in Smartphones & Mobile Devices, 2014.
[2] Y. Jang, C. Song, S. P. Chung, T. Wang, and W. Lee,
“A11y attacks: Exploiting accessibility in operating sys-
tems,” in ACM CCS, 2014.
[3] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr,
C. Shields, D. Wagner, and W. Zhou, “Hidden voice
commands.” in USENIX Security, 2016.
[4] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu,
“DolphinAttack: Inaudible voice commands,” in ACM
CCS, 2017.
[5] N. Roy, H. Hassanieh, and R. Roy Choudhury, “Back-
door: Making microphones hear inaudible sounds,” in
MobiSys, 2017.
[6] N. Roy, S. Shen, H. Hassanieh, and R. R. Choudhury,
“Inaudible voice commands: The long-range attack and
defense,” in NSDI, 2018.
[7] X. Yuan, Y. Chen, Y. Zhao, Y. Long, X. Liu, K. Chen,
S. Zhang, H. Huang, X. Wang, and C. A. Gunter, “Com-
manderSong: A systematic approach for practical adver-
sarial voice recognition,” in USENIX Security, 2018.
[8] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine
noodles: exploiting the gap between human and machine
speech recognition,” USENIX WOOT, 2015.
[9] M. M. Cisse, Y. Adi, N. Neverova, and J. Keshet, “Hou-
dini: Fooling deep structured visual and speech recogni-
tion models with adversarial examples,” in Advances in
neural information processing systems, 2017.
[10] L. Song and P. Mittal, “Inaudible voice commands,”
arXiv preprint arXiv:1708.07238, 2017.
[11] D. Kumar, R. Paccagnella, P. Murley, E. Hennenfent,
J. Mason, A. Bates, and M. Bailey, “Skill squatting at-
tacks on Amazon Alexa,” in USENIX Security, 2018.
[12] N. Zhang, X. Mi, X. Feng, X. Wang, Y. Tian, and F. Qian,
“Understanding and mitigating the security risks of voice-
controlled third-party skills on amazon alexa and google
home,” arXiv preprint arXiv:1805.01525, 2018.
[13] Y. Son, H. Shin, D. Kim, Y.-S. Park, J. Noh, K. Choi,
J. Choi, and Y. Kim, “Rocking drones with intentional
sound noise on gyroscopic sensors,” in USENIX Secu-
rity, 2015.
[14] C. Yan, W. Xu, and J. Liu, “Can you trust autonomous
vehicles: Contactless attacks against sensors of self-
driving vehicle,” DEFCON, 2016.
[15] T. Trippel, O. Weisse, W. Xu, P. Honeyman, and K. Fu,
“WALNUT: waging doubt on the integrity of MEMS
accelerometers with acoustic injection attacks,” in IEEE
European Symposium on Security and Privacy, 2017.
[16] S. Nashimoto, D. Suzuki, T. Sugawara, and K. Sakiyama,
“Sensor CON-Fusion: Defeating kalman ﬁlter in signal
injection attack,” in ASIA CCS, 2018.
[17] C. Bolton, S. Rampazzi, C. Li, A. Kwong, W. Xu, and
K. Fu, “Blue note: How intentional acoustic interference
damages availability and integrity in hard disk drives
and operating systems,” in IEEE S&P, 2018.
[18] J. Petit, B. Stottelaar, M. Feiri, and F. Kargl, “Remote
attacks on automated vehicles sensors: Experiments on
camera and LiDAR,” Black Hat Europe, 2015.
[19] J. Petit and S. E. Shladover, “Potential cyberattacks on
automated vehicles,” IEEE Transactions on Intelligent
Transportation Systems, vol. 16, no. 2, pp. 546–556,
2015.
[20] H. Shin, D. Kim, Y. Kwon, and Y. Kim, “Illusion and
dazzle: Adversarial optical channel exploits against li-
dars for automotive applications,” in CHES, 2017.
USENIX Association
29th USENIX Security Symposium    2647
[21] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi,
Q. A. Chen, K. Fu, and Z. M. Mao, “Adversarial sensor
attack on lidar-based perception in autonomous driving,”
in ACM CCS, 2019.
[22] Y.-S. Park, Y. Son, H. Shin, D. Kim, and Y. Kim, “This
ain’t your dose: Sensor spooﬁng attack on medical infu-
sion pump.” in USENIX WOOT, 2016.
[23] A. S. Uluagac, V. Subramanian, and R. Beyah, “Sensory
channel threats to cyber physical systems: A wake-up
call,” in IEEE Conference on Communications and Net-
work Security, 2014.
[24] D. H. Habing, “The use of lasers to simulate radiation-
induced transients in semiconductor devices and cir-
cuits,” IEEE Transactions on Nuclear Science, vol. 12,
no. 5, pp. 91–100, 1965.
[25] S. P. Skorobogatov and R. J. Anderson, “Optical fault
induction attacks,” in CHES, 2002.
[26] D. Karaklaji´c, J. Schmidt, and I. Verbauwhede, “Hard-
ware designer’s guide to fault attacks,” IEEE Transac-
tions on Very Large Scale Integration (VLSI) Systems,
vol. 21, no. 12, pp. 2295–2306, 2013.
[27] J.-M. Dutertre, J. J. Fournier, A.-P. Mirbaha, D. Nac-
cache, J.-B. Rigaud, B. Robisson, and A. Tria, “Re-
view of fault injection mechanisms and consequences
on countermeasures design,” in International Confer-
ence on Design & Technology of Integrated Systems in
Nanoscale Era, 2011.
[28] IEC System of Conformity Assessment Schemes for
Electrotechnical Equipment and Components, “IEC
60825-1:2014 safety of
laser products - part 1:
Equipment classiﬁcation and requirements.” [Online].
Available: www.iecee.org/index.htm
[29] U.S. Dept. HHS, FDA, CDRH, “Laser products –
conformance with IEC 60825-1 ed. 3 and IEC 60601-2-
22 ed. 3.1 (laser notice no. 56) guidance for industry
and food and drug administration staff.” [Online].
Available: www.fda.gov/media/110120/download
[30] S. M. Goldwasser and B. Edwards, “Hidden menace: Re-
congnizing and controlling the hazards posed by smaller
and lower power lasers,” www.repairfaq.org/sam/laser/
ILSC_2011-1303.pdf, 2011, accessed: 2019-08-20.
[31] Thorlabs, “Laser diode controller - ldc200c series
operation manual.” [Online]. Available: www.thorlabs.
com/drawings/6fc52e67fcedcf58-A5E806E4-C8BE-
575F-38C0746916067A53/LDC205C-Manual.pdf
[32] S. Manohar and D. Razansky, “Photoacoustics: a histor-
ical review,” Advances in Optics and Photonics, vol. 8,
no. 4, pp. 586–617, 2016.
[33] A. G. Bell, “Upon the production and reproduction of
sound by light,” Journal of the Society of Telegraph
Engineers, vol. 9, no. 34, pp. 404–426, 1880.
[34] R. M. Sullenberger, S. Kaushik, and C. M. Wynn, “Pho-
toacoustic communications: delivering audible signals
via absorption of light by atmospheric H2O,” Opt. Lett.,
vol. 44, no. 3, pp. 622–625, 2019.
[35] IFIXIT, “Google home mini teardown,” www.iﬁxit.com/
Teardown/Google+Home+Mini+Teardown/102264, ac-
cessed: 2019-08-25.
[36] NaturalSoft Ltd., “Naturalreader,” www.naturalreaders.
com, accessed: 2019-08-25.
[37] N. Melena, N. Neuenfeldt, A. Slagel, M. Hamel,
C. Mackin, and C. Smith, “Covert IR-laser remote listen-
ing device,” The University of Arizona Honors Thesis
repository.arizona.edu/handle/10150/244475, accessed:
2019-08-20.
[38] D. Davidson, H. Wu, R. Jellinek, T. Ristenpart, and
V. Singh, “Controlling UAVs with sensor input spooﬁng
attacks,” in USENIX WOOT, 2016.
[39] A. K. Sikder, H. Aksu, and A. S. Uluagac, “6thsense:
A context-aware sensor-based attack detector for smart
devices,” in USENIX Security, 2017.
[40] Z. Wang, Q. Zou, Q. Song, and J. Tao, “The era of silicon
MEMS microphone and look beyond,” in International
Conference on Solid-State Sensors, Actuators and Mi-
crosystems, 2015.
[41] L. Zhang, S. Tan, J. Yang, and Y. Chen, “Voicelive: A
phoneme localization based liveness detection for voice
authentication on smartphones,” in ACM CCS, 2016.
[42] L. Zhang, S. Tan, and J. Yang, “Hearing your voice is not
enough: An articulatory gesture based liveness detection
for voice authentication,” in ACM CCS, 2017.
[43] L. Lu, J. Yu, Y. Chen, H. Liu, Y. Zhu, Y. Liu, and
M. Li, “Lippass: Lip reading-based user authentication
on smartphones leveraging acoustic signals,” in IEEE
INFOCOM 2018, 2018.
[44] L. Lu, J. Yu, Y. Chen, H. Liu, Y. Zhu, L. Kong, and M. Li,
“Lip reading-based user authentication through acoustic
sensing on smartphones,” IEEE/ACM Transactions on
Networking, vol. 27, pp. 447–460, 2019.
[45] H. Feng, K. Fawaz, and K. Shin, “Continuous authenti-
cation for voice assistants,” in ACM MobiCom, 2017.
2648    29th USENIX Security Symposium
USENIX Association