the bandwidth vector in the previous ten seconds to predict bandwidth for up to
ﬁve seconds ahead. For each combination of n and m, we train a diﬀerent LSTM
model, denoted as LST M(n, m). Note that, at time t, a LST M(n, m) model can
generate bandwidth predictions for t + i, 1 ≤ i ≤ m. To make RLS generate pre-
diction i seconds ahead, we simply update RLS parameters by using bandwidth
of i seconds ahead, instead of the next second, as the targeted output.
Table 3. Prediction RMSE on RLS and LSTM
1st sec 2nd sec 3rd sec 4th sec 5th sec
RLS
LST M (10, 1)
LST M (10, 2)
LST M (10, 3)
LST M (10, 4)
LST M (10, 5)
Improvement over RLS 13.7% 8.2% 6.8% 9.9% 10.6%
2.57
2.26
2.27
2.29
2.33
2.40
2.88
–
2.66
2.68
2.69
2.71
3.16
–
–
2.96
2.97
2.98
3.53
–
–
–
3.21
3.22
3.76
–
–
–
–
3.40
Table 3 compares the prediction accuracy of diﬀerent LSTM models and RLS
on the NYC Subway 7 Train trace. The RMSE value unit is M bps. Not coin-
cidentally, all LSTM models outperform RLS at all prediction intervals. In the
Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
43
(a) One Second Prediction
(b) Five Second Prediction
Fig. 4. RLS vs LSTM multi-second prediction
Fig. 5. Impact of prediction interval on LSTM and RLS
representative results, the best prediction accuracy for interval i is achieved by
LST M(10, i), marked in bold fonts. Theoretically, LST M(n, m) model is trained
to minimize the prediction errors for all intervals from 1 to m. Consequently, the
prediction error at interval m1 < m will be larger than those of LST M(n, m2)
models (m1 ≤ m2 < m). Figures 4a and b illustrate sample prediction error
evolution of RLS and LSTM for one second and ﬁve second intervals. Y-axis is
the square error between prediction value and ground truth. It is visually clear
that LSTM RMSE is lower than RLS most of the time. The accuracy improve-
ment of LSTM is more prominent for the ﬁve second prediction interval. Figure 5
compares the average RMSE for all LSTM models with RLS at diﬀerent pre-
diction intervals. Both RMSEs increase as the prediction interval increases. The
slope for LSTM increase is 0.270, while that for RLS is 0.302. This suggests that
not only LSTM is more accurate than RLS at individual prediction intervals,
LSTM’s accuracy decays slower than RLS as the interval increases.
4.4 Computation Overhead
To validate the feasibility of oﬄine training and online prediction, we report
the computation overhead of our LSTM models. Our CPU Conﬁguration is: 4th
Gen Intel Core i5-4210U (1.70 GHz 1600 MHz 3 MB). Neural Network Structure:
Hidden Layer 1 & 2 have 256 and 128 nodes respectively. The training and
running overhead detail is presented in Table 4. Even though the oﬄine training
time is long, once the training is done, the trained model can be used for realtime
44
L. Mei et al.
prediction. As shown in Table 4b, the online prediction consumption is so small.
It takes less than six seconds to predict 12,500 ﬁve seconds bandwidth vector
in the LST M(10, 5) model. Once the model is trained oﬄine, it can be used to
generate realtime prediction on any reasonably conﬁgured mobile phone.
Table 4. Computation consumption
5 Multi-Scale Entropy Analysis
5.1 Prediction Accuracy Analysis Using Multi-Scale Entropy
The predictability of a time series is determined by its complexity and the tem-
poral correlation at diﬀerent time scales. The traditional entropy measure can
be used to quantify the randomness of a signal: the higher the entropy, the
more random thus less predictable. However, the traditional entropy measure
cannot model the signal complexity and temporal correlation at diﬀerent time
scales. Recently, Multi-Scale Entropy (MSE) [4] has been proposed to measure
the complexity of physical and physiologic time series. Given a discrete time
series {x(i), 1 ≤ i ≤ N}, a coarse-grained time series {y(s)(j)} can be con-
structed at scale factor of s ≥ 1:
y(s)(j) (cid:2) 1
s
js(cid:5)
i=(j−1)s+1
x(i), 1 ≤ j ≤ N/s.
Then the entropy measure of x at time scale s can be calculated as the
entropy of y(s):
H (s)(x) (cid:2) H(y(s)) = −E[log p(y(s))],
(5)
where p(y(s)) is the probability density of the constructed signal at scale s. By
varying s, one can examine the complexity/regularity of x at diﬀerent time scales.
The Multi-Scale Entropy curve H (s)(x) also reveals the temporal correlation
structures of the time series [4].
We apply MSE to study the predictability of network bandwidth under dif-
ferent mobile networking scenarios. MSE can represent the regularity patterns
of each scenario. Given a set of scales S = [s0, s1,··· , sm], we generate a MSE
vector for scenario i as M SEi (cid:2) [H (s)(xi), s ∈ S], where xi is bandwidth trace
from scenario i. M SEi can be used to analyze the per-scenario prediction accu-
racy for scenario i, as deﬁned in (2). Additionally, by comparing M SEi and
Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
45
M SEj, we can also study the feasibility of cross-scenario prediction between
scenarios i and j, as deﬁned in (4). More speciﬁcally, we measure the MSE
similarity between scenarios i and j as the weighted sum of the correlation coef-
ﬁcient and Euclidean distance between M SEi and M SEj. We will demonstrate
the connection between MSE and prediction accuracy of both per-scenario and
cross-scenario predictions next.
5.2 MSE Analysis of NYC MTA Traces
We apply MSE analysis to bandwidth from every scenario in New York City
MTA traces. Figure 6a and b plot the raw bandwidth traces for two sample
traces. They present diﬀerent variability at diﬀerent scenarios. Figure 6c shows
the results of Multi-Scale Entropy for ﬁve sample traces. The scale is from 1 to
15. According to the [4], to make the MSE analysis valid, the sequence should be
at least 1,000 points at each scale. From the result of Fig. 6c, we ﬁnd that same
routes share similar MSE patterns. For example, 7A Train and 7B Train traces
were both collected from 7 train but on diﬀerent days. From the curves of Bus 57
and Bus 62, we ﬁnd that even though the transportation methods are the same,
due to diﬀerent routes, the MSE patterns can be very diﬀerent. Table 5a shows
the cross-scenario prediction accuracy in RMSE. Each row is for a model trained
using data from some mobility scenario, each column is the prediction accuracy
for the testset from some mobility scenario. For example, Row 3 & Column 1
shows that the LSTM model trained by Bus 57 data can achieve RMSE of 2.276
when predicting bandwidth for 7A Train testset.
(a) 7A Train Bandwidth
(b) Bus 62 Bandwidth
(c) MSE Analysis
Fig. 6. Multi-Scale Entropy of diﬀerent mobility scenarios
Table 5. Multi-Scale Entropy analysis
46
L. Mei et al.
Table 5b shows the MSE similarity between diﬀerent mobility scenarios.
Table 6 reports for each scenario i the correlation between its MSE similarity
with other scenarios and the accuracy of cross-scenario prediction using models
trained for other scenarios. Close to −1 correlations suggest that higher MSE
similarity leads to higher accuracy (lower RMSE). Multi-Scale Entropy analysis
provides a good measure to explore the possibility of cross-scenario prediction,
which can be very beneﬁcial for mobility scenarios with limited available data
for training Deep learning models.
Table 6. Correlation between MSE similarity and cross-scenario prediction accuracy
Correlation value −0.916
7A Train 7B Train Bus 57 Bus 62 N Train
−0.943 −0.945 −0.937 −0.994
6 Conclusion
In this paper, we studied realtime mobile bandwidth prediction. We developed
LSTM recurrent neural network models to capture the rich temporal structures
in mobile bandwidth traces for accurate prediction. In both next-second and
multi-second predictions, LSTM outperforms other state-of-the-art prediction
algorithms, such as RLS and Harmonic Mean. Using Multi-Scale Entropy anal-
ysis, we investigated the connection between MSE and cross-scenario predic-
tion accuracy. Going forward, we will continue our mobile bandwidth measure-
ment campaign. For online bandwidth prediction, we will study how to dynami-
cally select LSTM models trained oﬄine to match the current mobility scenario
through adaptive model fusion. We will also study the feasibility of using extra
information, e.g. GPS, speed/acceleration sensor readings, to assist mobility sce-
nario identiﬁcation and model selection. We will also develop LSTM models for
the emerging 5G mobile networks. Finally, we will explore data fusion of LSTM
models and other prediction models to further improve the prediction accuracy.
References
1. Gers, F.A., Schmidhuber, J., Cummins, F.: Learning to forget: continual prediction
with LSTM, pp. 850–855 (1999)
2. Sundermeyer, M., Schl¨uter, R., Ney, H.: LSTM neural networks for language model-
ing. In: Thirteenth Annual Conference of the International Speech Communication
Association (2012)
3. Haykin, S.S.: Adaptive Filter Theory. Pearson Education India, Delhi (2008)
4. Costa, M., Goldberger, A.L., Peng, C.K.: Multiscale entropy analysis of complex
physiologic time series. Phys. Rev. Lett. 89(6), 068102 (2002)
Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
47
5. Yi, S., et al.: CS2P: improving video bitrate selection and adaptation with data-
driven throughput prediction. In: Proceedings of the 2016 ACM SIGCOMM Con-
ference, pp. 272–285. ACM (2016)
6. Brown, R.G., Hwang, P.Y.C.: Introduction to Random Signals and Applied
Kalman Filtering, vol. 3. Wiley, New York (1992)
7. HSDPA. http://home.iﬁ.uio.no/paalh/dataset/hsdpa-tcp-logs/
8. Bai, T., Vaze, R., Heath, R.W.: Analysis of blockage eﬀects on urban cellular
networks. IEEE Trans. Wirel. Commun. 13(9), 5070–5083 (2014)
9. Bai, T., Vaze, R., Heath, R.W.: Using random shape theory to model blockage in
random cellular networks. In: 2012 International Conference on Signal Processing
and Communications (SPCOM), pp. 1–5. IEEE (2012)
10. Eymen, K., Liu, Y., Wang, Y., Shi, Y., Gu, C., Lyu, J.: Real-time bandwidth pre-
diction and rate adaptation for video calls over cellular networks. In: Proceedings
of the 7th International Conference on Multimedia Systems, p. 12. ACM (2016)
11. Guibin, T., Liu, Y.: Towards agile and smooth video adaptation in dynamic HTTP
streaming. In: Proceedings of the 8th International Conference on Emerging Net-
working Experiments and Technologies, pp. 109–120. ACM (2012)
12. He, Q., Dovrolis, C., Ammar, M.H.: On the predictability of large transfer TCP
throughput. In: Proceedings of ACM SIGCOMM (2005)
13. Mirza, M., Sommers, J., Barford, P., Zhu, X.: A machine learning approach to
TCP throughput prediction. In: ACM SIGMETRICS (2007)
14. Smola, A.J., Sch¨olkopf, B.: A tutorial on support vector regression. Stat. Comput.
14(3), 199–222 (2004)
15. Yin, X., Jindal, A., Sekar, V., Sinopoli, B.: A control-theoretic approach for
dynamic adaptive video streaming over HTTP. In: ACM SIGCOMM Computer
Communication Review, vol. 45, no. 4, pp. 325–338. ACM (2015)
16. Winstein, K., Sivaraman, A., Balakrishnan, H.: Stochastic forecasts achieve high
throughput and low delay over cellular networks. In: NSDI, vol. 1, no. 1, pp. 2–3
(2013)
17. Cho, K., et al.: Learning phrase representations using RNN encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078 (2014)
18. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recog-
nition: the shared views of four research groups. IEEE Sig. Process. Mag. 29(6),
82–97 (2012)
19. Mao, H., Netravali, R., Alizadeh, M.: Neural adaptive video streaming with pen-
sieve. In: Proceedings of the Conference of the ACM Special Interest Group on
Data Communication, pp. 197–210. ACM (2017)
20. Jiang, J., Sekar, V., Zhang, H.: Improving fairness, eﬃciency, and stability in
HTTP-based adaptive video streaming with festive. IEEE/ACM Trans. Netwo.
(TON) 22(1), 326–340 (2014)
21. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)