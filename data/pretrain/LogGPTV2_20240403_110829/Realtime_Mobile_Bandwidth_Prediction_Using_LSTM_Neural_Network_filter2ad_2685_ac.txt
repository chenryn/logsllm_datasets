### Realtime Mobile Bandwidth Prediction Using LSTM Neural Network

#### 4.3 Prediction Model and Results
We use the bandwidth vector from the previous ten seconds to predict the bandwidth up to five seconds ahead. For each combination of \( n \) (input sequence length) and \( m \) (prediction horizon), we train a different LSTM model, denoted as \( \text{LSTM}(n, m) \). At time \( t \), an \( \text{LSTM}(n, m) \) model can generate bandwidth predictions for \( t + i \), where \( 1 \leq i \leq m \). To make RLS (Recursive Least Squares) generate a prediction \( i \) seconds ahead, we update the RLS parameters using the bandwidth \( i \) seconds ahead instead of the next second as the target output.

**Table 3. Prediction RMSE on RLS and LSTM**

| Prediction Interval | 1st sec | 2nd sec | 3rd sec | 4th sec | 5th sec |
|---------------------|---------|---------|---------|---------|---------|
| **RLS**             | 2.88    | 3.16    | 3.53    | -       | -       |
| **LSTM(10, 1)**     | 2.57    | 2.66    | 2.96    | 3.21    | 3.40    |
| **LSTM(10, 2)**     | 2.26    | 2.68    | 2.97    | 3.22    | -       |
| **LSTM(10, 3)**     | 2.27    | 2.69    | 2.98    | -       | -       |
| **LSTM(10, 4)**     | 2.29    | 2.71    | -       | -       | -       |
| **LSTM(10, 5)**     | 2.33    | -       | -       | -       | -       |
| **Improvement over RLS** | 13.7% | 8.2% | 6.8% | 9.9% | 10.6% |

**Table 3** compares the prediction accuracy of different LSTM models and RLS on the NYC Subway 7 Train trace. The RMSE values are in Mbps. As expected, all LSTM models outperform RLS at all prediction intervals. In the representative results, the best prediction accuracy for interval \( i \) is achieved by \( \text{LSTM}(10, i) \), marked in bold. Theoretically, the \( \text{LSTM}(n, m) \) model is trained to minimize prediction errors for all intervals from 1 to \( m \). Consequently, the prediction error at interval \( m_1 < m \) will be larger than those of \( \text{LSTM}(n, m_2) \) models where \( m_1 \leq m_2 < m \).

**Figures 4a and 4b** illustrate the sample prediction error evolution of RLS and LSTM for one-second and five-second intervals, respectively. The Y-axis represents the square error between the predicted value and the ground truth. It is visually clear that the LSTM RMSE is lower than RLS most of the time, with more prominent accuracy improvement for the five-second prediction interval.

**Figure 5** compares the average RMSE for all LSTM models with RLS at different prediction intervals. Both RMSEs increase as the prediction interval increases. The slope for the LSTM increase is 0.270, while that for RLS is 0.302. This suggests that not only is LSTM more accurate than RLS at individual prediction intervals, but LSTM's accuracy also decays slower than RLS as the interval increases.

#### 4.4 Computation Overhead
To validate the feasibility of offline training and online prediction, we report the computation overhead of our LSTM models. Our CPU configuration is: 4th Gen Intel Core i5-4210U (1.70 GHz, 1600 MHz, 3 MB). The neural network structure has two hidden layers with 256 and 128 nodes, respectively. The training and running overhead details are presented in **Table 4**. Even though the offline training time is long, once the training is done, the trained model can be used for real-time prediction. As shown in **Table 4b**, the online prediction consumption is minimal. It takes less than six seconds to predict 12,500 five-second bandwidth vectors in the \( \text{LSTM}(10, 5) \) model. Once the model is trained offline, it can be used to generate real-time predictions on any reasonably configured mobile phone.

**Table 4. Computation Consumption**

| Metric                | Value          |
|-----------------------|----------------|
| Offline Training Time | [Value]        |
| Online Prediction Time| [Value]        |

#### 5. Multi-Scale Entropy Analysis
##### 5.1 Prediction Accuracy Analysis Using Multi-Scale Entropy
The predictability of a time series is determined by its complexity and temporal correlation at different time scales. Traditional entropy measures quantify the randomness of a signal: the higher the entropy, the more random and less predictable. However, traditional entropy measures cannot model the signal complexity and temporal correlation at different time scales. Recently, Multi-Scale Entropy (MSE) [4] has been proposed to measure the complexity of physical and physiological time series.

Given a discrete time series \( \{x(i), 1 \leq i \leq N\} \), a coarse-grained time series \( \{y(s)(j)\} \) can be constructed at scale factor \( s \geq 1 \):
\[ y(s)(j) = \frac{1}{s} \sum_{i=(j-1)s+1}^{js} x(i), \quad 1 \leq j \leq \frac{N}{s}. \]

The entropy measure of \( x \) at time scale \( s \) can be calculated as the entropy of \( y(s) \):
\[ H(s)(x) = H(y(s)) = -E[\log p(y(s))], \]
where \( p(y(s)) \) is the probability density of the constructed signal at scale \( s \). By varying \( s \), one can examine the complexity/regularity of \( x \) at different time scales. The Multi-Scale Entropy curve \( H(s)(x) \) also reveals the temporal correlation structures of the time series [4].

We apply MSE to study the predictability of network bandwidth under different mobile networking scenarios. MSE can represent the regularity patterns of each scenario. Given a set of scales \( S = [s_0, s_1, \ldots, s_m] \), we generate an MSE vector for scenario \( i \) as \( \text{MSE}_i = [H(s)(x_i), s \in S] \), where \( x_i \) is the bandwidth trace from scenario \( i \). \( \text{MSE}_i \) can be used to analyze the per-scenario prediction accuracy for scenario \( i \), as defined in (2). Additionally, by comparing \( \text{MSE}_i \) and \( \text{MSE}_j \), we can study the feasibility of cross-scenario prediction between scenarios \( i \) and \( j \), as defined in (4). Specifically, we measure the MSE similarity between scenarios \( i \) and \( j \) as the weighted sum of the correlation coefficient and Euclidean distance between \( \text{MSE}_i \) and \( \text{MSE}_j \). We will demonstrate the connection between MSE and prediction accuracy of both per-scenario and cross-scenario predictions next.

##### 5.2 MSE Analysis of NYC MTA Traces
We apply MSE analysis to bandwidth from every scenario in New York City MTA traces. **Figures 6a and 6b** plot the raw bandwidth traces for two sample traces, showing different variability at different scenarios. **Figure 6c** shows the results of Multi-Scale Entropy for five sample traces, with the scale ranging from 1 to 15. According to [4], to make the MSE analysis valid, the sequence should be at least 1,000 points at each scale. From the results in **Figure 6c**, we find that the same routes share similar MSE patterns. For example, 7A Train and 7B Train traces were both collected from the 7 train but on different days. From the curves of Bus 57 and Bus 62, we find that even though the transportation methods are the same, due to different routes, the MSE patterns can be very different.

**Table 5a** shows the cross-scenario prediction accuracy in RMSE. Each row represents a model trained using data from some mobility scenario, and each column represents the prediction accuracy for the test set from some mobility scenario. For example, Row 3 & Column 1 shows that the LSTM model trained by Bus 57 data can achieve an RMSE of 2.276 when predicting bandwidth for the 7A Train test set.

**Table 5b** shows the MSE similarity between different mobility scenarios. **Table 6** reports for each scenario \( i \) the correlation between its MSE similarity with other scenarios and the accuracy of cross-scenario prediction using models trained for other scenarios. Close to -1 correlations suggest that higher MSE similarity leads to higher accuracy (lower RMSE). Multi-Scale Entropy analysis provides a good measure to explore the possibility of cross-scenario prediction, which can be very beneficial for mobility scenarios with limited available data for training deep learning models.

**Table 6. Correlation between MSE Similarity and Cross-Scenario Prediction Accuracy**

| Scenario   | Correlation Value |
|------------|-------------------|
| 7A Train   | -0.943            |
| 7B Train   | -0.945            |
| Bus 57     | -0.937            |
| Bus 62     | -0.994            |
| N Train    | -0.916            |

#### 6. Conclusion
In this paper, we studied real-time mobile bandwidth prediction. We developed LSTM recurrent neural network models to capture the rich temporal structures in mobile bandwidth traces for accurate prediction. In both next-second and multi-second predictions, LSTM outperforms other state-of-the-art prediction algorithms, such as RLS and Harmonic Mean. Using Multi-Scale Entropy analysis, we investigated the connection between MSE and cross-scenario prediction accuracy. Going forward, we will continue our mobile bandwidth measurement campaign. For online bandwidth prediction, we will study how to dynamically select LSTM models trained offline to match the current mobility scenario through adaptive model fusion. We will also study the feasibility of using extra information, such as GPS, speed/acceleration sensor readings, to assist in mobility scenario identification and model selection. We will develop LSTM models for the emerging 5G mobile networks and explore data fusion of LSTM models and other prediction models to further improve prediction accuracy.

#### References
1. Gers, F.A., Schmidhuber, J., Cummins, F.: Learning to forget: continual prediction with LSTM, pp. 850–855 (1999)
2. Sundermeyer, M., Schlüter, R., Ney, H.: LSTM neural networks for language modeling. In: Thirteenth Annual Conference of the International Speech Communication Association (2012)
3. Haykin, S.S.: Adaptive Filter Theory. Pearson Education India, Delhi (2008)
4. Costa, M., Goldberger, A.L., Peng, C.K.: Multiscale entropy analysis of complex physiologic time series. Phys. Rev. Lett. 89(6), 068102 (2002)
5. Yi, S., et al.: CS2P: improving video bitrate selection and adaptation with data-driven throughput prediction. In: Proceedings of the 2016 ACM SIGCOMM Conference, pp. 272–285. ACM (2016)
6. Brown, R.G., Hwang, P.Y.C.: Introduction to Random Signals and Applied Kalman Filtering, vol. 3. Wiley, New York (1992)
7. HSDPA. http://home.iﬁ.uio.no/paalh/dataset/hsdpa-tcp-logs/
8. Bai, T., Vaze, R., Heath, R.W.: Analysis of blockage effects on urban cellular networks. IEEE Trans. Wirel. Commun. 13(9), 5070–5083 (2014)
9. Bai, T., Vaze, R., Heath, R.W.: Using random shape theory to model blockage in random cellular networks. In: 2012 International Conference on Signal Processing and Communications (SPCOM), pp. 1–5. IEEE (2012)
10. Eymen, K., Liu, Y., Wang, Y., Shi, Y., Gu, C., Lyu, J.: Real-time bandwidth prediction and rate adaptation for video calls over cellular networks. In: Proceedings of the 7th International Conference on Multimedia Systems, p. 12. ACM (2016)
11. Guibin, T., Liu, Y.: Towards agile and smooth video adaptation in dynamic HTTP streaming. In: Proceedings of the 8th International Conference on Emerging Networking Experiments and Technologies, pp. 109–120. ACM (2012)
12. He, Q., Dovrolis, C., Ammar, M.H.: On the predictability of large transfer TCP throughput. In: Proceedings of ACM SIGCOMM (2005)
13. Mirza, M., Sommers, J., Barford, P., Zhu, X.: A machine learning approach to TCP throughput prediction. In: ACM SIGMETRICS (2007)
14. Smola, A.J., Schölkopf, B.: A tutorial on support vector regression. Stat. Comput. 14(3), 199–222 (2004)
15. Yin, X., Jindal, A., Sekar, V., Sinopoli, B.: A control-theoretic approach for dynamic adaptive video streaming over HTTP. In: ACM SIGCOMM Computer Communication Review, vol. 45, no. 4, pp. 325–338. ACM (2015)
16. Winstein, K., Sivaraman, A., Balakrishnan, H.: Stochastic forecasts achieve high throughput and low delay over cellular networks. In: NSDI, vol. 1, no. 1, pp. 2–3 (2013)
17. Cho, K., et al.: Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 (2014)
18. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recognition: the shared views of four research groups. IEEE Sig. Process. Mag. 29(6), 82–97 (2012)
19. Mao, H., Netravali, R., Alizadeh, M.: Neural adaptive video streaming with Pensieve. In: Proceedings of the Conference of the ACM Special Interest Group on Data Communication, pp. 197–210. ACM (2017)
20. Jiang, J., Sekar, V., Zhang, H.: Improving fairness, efficiency, and stability in HTTP-based adaptive video streaming with Festive. IEEE/ACM Trans. Netw. (TON) 22(1), 326–340 (2014)
21. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)