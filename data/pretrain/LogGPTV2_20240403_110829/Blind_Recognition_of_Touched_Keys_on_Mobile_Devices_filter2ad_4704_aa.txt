title:Blind Recognition of Touched Keys on Mobile Devices
author:Qinggang Yue and
Zhen Ling and
Xinwen Fu and
Benyuan Liu and
Kui Ren and
Wei Zhao
Blind Recognition of Touched Keys on Mobile Devices
Qinggang Yue
University of Massachusetts
Lowell, USA
PI:EMAIL
Benyuan Liu
University of Massachusetts
Lowell, USA
PI:EMAIL
Zhen Ling
Southeast University, China
PI:EMAIL
Xinwen Fu
University of Massachusetts
Lowell, USA
PI:EMAIL
Kui Ren
Wei Zhao
University at Buffalo, USA
PI:EMAIL
University of Macau, China
PI:EMAIL
ABSTRACT
In this paper, we introduce a novel computer vision based attack
that automatically discloses inputs on a touch-enabled device while
the attacker cannot see any text or popup in a video of the victim
tapping on the touch screen. We carefully analyze the shadow for-
mation around the ﬁngertip, apply the optical ﬂow, deformable part-
based model (DPM), k-means clustering and other computer vision
techniques to automatically locate the touched points. Planar ho-
mography is then applied to map the estimated touched points to a
reference image of software keyboard keys. Recognition of pass-
words is extremely challenging given that no language model can
be applied to correct estimated touched keys. Our threat model is
that a webcam, smartphone or Google Glass is used for stealthy at-
tack in scenarios such as conferences and similar gathering places.
We address both cases of tapping with one ﬁnger and tapping with
multiple ﬁngers and two hands. Extensive experiments were per-
formed to demonstrate the impact of this attack. The per-character
(or per-digit) success rate is over 97% while the success rate of rec-
ognizing 4-character passcodes is more than 90%. Our work is the
ﬁrst to automatically and blindly recognize random passwords (or
passcodes) typed on the touch screen of mobile devices with a very
high success rate.
Categories and Subject Descriptors
K.4.1 [ COMPUTERS AND SOCIETY]: Public Policy Issues—
Privacy
General Terms
Human Factors, Security
Keywords
Computer Vision Attack; Mobile Devices; Privacy Enhancing Key-
board
1.
INTRODUCTION
Touch-enabled devices are ubiquitously used in our daily life.
However, they are also attracting attention from attackers. In addi-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660288.
tion to hundreds of thousands of malwares [19], one class of threats
against mobile devices are computer vision based attacks. We can
classify those attacks into three groups: the ﬁrst group of attacks di-
rectly identify text on screen or its reﬂections on objects [2, 1]. The
second group of attacks detect visible features of the keys such as
light diffusion surrounding pressed keys [3] and popups of pressed
keys [28, 33]. The third group of attacks are able to blindly recog-
nize the text input on mobile devices while text or popups are not
visible to the attacker [43] .
In this paper, we introduce a novel attack blindly recognizing in-
puts on touch-enabled devices by estimating touched points from a
video of a victim tapping on the touch screen, as shown in Figure
1. In the attack, the deformable part-based model (DPM) is used
to detect and track the target device and the optical ﬂow algorithm
is used to automatically identify touching frames in which a ﬁnger
touches the screen surface. We use intersections of detected edges
of the touch screen to derive the homography matrix mapping the
touch screen surface in video frames to a reference image of the
software keyboard, as shown in Figure 2. DPM and other com-
puter vision techniques are applied to automatically estimate a tiny
touched area. We carefully derive a theory of the shadow formation
around the ﬁngertip and use the k-means clustering algorithm to i-
dentify touched points in the tiny touched area. Homography can
then map these touched points to the software keyboard keys in the
reference image. We performed extensive experiments. The victim
target devices include the iPad, Nexus 7 and iPhone 5. Both lo-
gin keyboard and QWERTY keyboard are examined. The cameras
include a webcam, a phone camera and Google Glass. The camer-
a is positioned from different distances and angles. We are able to
achieve a per-key success rate of over 97% and success rate of more
than 90% recognizing 4-digit passcodes in various scenarios.
We also show that DPM can be used to directly estimate the
touched point, which can be mapped to the reference image in or-
der to derive the touched key. This method of direct use of DPM for
recognizing touched keys is called the baseline method. However,
the baseline method achieves a success rate of around 26% since
DPM cannot accurately locate touched points.
To the best of our knowledge, we are the ﬁrst to be able to reli-
ably and blindly recognize passwords (or passcodes) typed on the
touch screen of mobile devices of various kinds. Since passwords
are random and do not contain meaningful text or pattern, natural
language processing techniques cannot be used. This challenges the
design of automatic recognition of the password. Our recognition
system incorporates recent advancement of object detection tech-
niques and our own analytical model of the touching process, and
is able to achieve a very high success rate. We have also extended
our work to the scenario of touching with both hands and multiple
1403ﬁngers and are able to recognize the touching ﬁnger from 10 ﬁn-
gers and achieve a high success rate of more than 95% recognizing
touched keys.
Figure 1: Touching Frame
Figure 2: Soft Keyboard
To defeat many computer vision based attacks including the one
in this paper, we designed and implemented a simple context aware
randomized software keyboard for Android, denoted as Privacy En-
hancing Keyboard (PEK). PEK automatically shows a convention-
al QWERTY keyboard for the normal text input and pops up a
randomized keyboard for inputting sensitive information such as
passcodes. The ﬁrst PEK prototype was demonstrated at an ACM
workshop [47] in October, 20121. To the best of our knowledge,
PEK is the ﬁrst generic software keyboard for a mobile platform
while a similar app CodeScrambler for iOS [23] appeared in August
2013. PEK is a full ﬂedged software keyboard while CodeScram-
bler is designed only for the unlock screen and does not provide
the context-aware functionality. Please refer to the appendix for the
implementation and evaluation of PEK.
The rest of the paper is organized as follows. We introduce the
homography and DPM in Section 2. Section 3 introduces the attack
for the case of tapping with one ﬁnger. In Section 4, we discuss
how to recognize touched points from touching frames. Experiment
design and evaluations are given in Section 5. We extend the attack
to tapping with multiple ﬁngers and two hands in Section 6. Section
7 discusses the related work. We conclude the paper in Section 8.
2. BACKGROUND
In this section, we introduce two major computer vision tech-
niques employed in this paper: planar homography and the DPM
(Deformable Part-based Model) object detector.
2.1 Planar Homography
Planar homography is a 2D projective transformation that relates
two images of the same planar surface [7]. Assume p = (s, t, 1)
is any point in an image of a 3D planar surface and q = (s′, t′, 1)
is the corresponding point in another image of the same 3D planar
surface. The two images may be taken by the same camera or dif-
ferent cameras. There exists an invertible 3 × 3 matrix H, denoted
as homography matrix,
q = Hp.
(1)
2.2 Deformable Part-based Model (DPM)
DPM [11] is the state-of-art object detector and contains three
main components: a mixture of star-structured part based model-
s, the efﬁcient matching process for object detection and the latent
SVM (Support Vector Machine) training process. DPM builds mul-
tiple star-structured models for the object of interest from different
viewpoints. Each star-structured model has a root model that char-
acterizes the object as a whole and several (usually six) part models
that characterize each part of the object, their anchor position rela-
tive to the root and associated deformation parameters. The models
1No paper was published on PEK.
are represented by the Histogram of Oriented Gradients (HOG) [10]
feature, which is insensitive to lighting variation.
To detect an object in an image, DPM uses a sliding-window ap-
proach and calculates a score fβ(x) for each possible object sample
x at each location,
fβ(x) = max
z∈Z(x)
β · Φ(x, z),
(2)
where z is the latent values, β is a vector of model parameters, and
Φ(x, z) is the feature vector of x. A high score indicates the loca-
tion of the object. Dynamic programming and generalized distance
transforms are employed for efﬁcient matching.
During the training, a bounding box is used to specify the object
of interest in each image, while its parts are unlabeled. DPM treats
these unlabeled parts as latent (hidden) variables. It automatically
ﬁnds and labels the parts, and employs the latent SVM to train the
model. Denote a training data set as D = (, . . . , ). xi is the image patch in the corresponding bounding
box. yi ∈ {−1, 1}, indicating whether xi is the object of interest
(yi = 1) or not (yi = −1). DPM trains β by minimizing the
objective function,
LD(β) =
1
2
k β k2 +C
n
X
i=1
max(0, 1 − yifβ(xi)),
(3)
where max(0, 1 − yifβ(xi)) is the standard hinge loss and the
constant C controls the relative weight of the regularization term
[11]. The purpose of minimizing Formula (3) is to classify an object
x correctly and reduce the modulus of β.
3. HOMOGRAPHY BASED ATTACK AGAINST
TOUCH SCREEN
In this section, we introduce the basic idea of the attack and each
step in detail.
3.1 Basic Idea
Figure 3 shows the ﬂow chart of the automatic and blind recog-
nition of touched keys on mobile devices.
Figure 3: Work ﬂow of Blind Recognition of Touched Keys
Without loss of generality, we often use the four-digit passcode in-
put on iPad as the example. Step 1 - Take a video of the victim
tapping on a device. We do not assume the video records any text
or popups while we assume the ﬁnger movement and the target de-
vice’s screen surface are recorded. Step 2 - Preprocess the video
and keep only the touch screen area with moving ﬁngers. We as-
sume that the type of device is known or can be detected so that we
also obtain a high resolution image of the corresponding software
keyboard on the touch screen surface, denoted as reference image,
as shown in Figure 2. Step 3 - Detect the touching frames, in which
the ﬁnger touches the screen surface, as shown in Figure 1. Step 4
- Identify features of the touch screen surface and derive the planar
homography matrix between the touching frames and the reference
image. Step 5 - Employ DPM and various computer vision tech-
niques to obtain a large box bounding the touching ﬁngertip. This
is a key step of implementing an automatic touched key recogni-
tion. However, extra steps are needed to actually ﬁnd the touched
point that can be mapped to the reference image and recognize the
touched key. We denote the direct use of DPM ﬁnding the touched
point as the baseline method. Step 6 - Find the ﬁngertip contour
1404in the large bounding box and train a tiny bounding box around the
ﬁngertip top as the accurate touched area. Step 7 - Build a model of
the touching process, identify the touched points from the estimat-
ed tiny touched area and map them to the reference image via the
homography. If the touched points can be correctly located, we can
disclose the corresponding touched keys. We introduce the seven
steps in detail below.
3.2 Step 1 - Taking Videos
The attacker takes a video of a victim tapping on a device from a
distance. Such scenarios include students taking classes, researcher-
s attending conferences, and tourists gathering and resting in a square.
With the development of smartphones and webcams, a stealthy at-
tack at such a crowded location is feasible. For example, cameras
of iPhone, Google Glass and even a smartwatch have decent resolu-
tion. Galaxy S4 Zoom has a 16-megapixel (MP) rear camera with a
10x zoom lens, weighting only 208g. Amazon sells a webcam-like
plugable 2MP USB 2.0 digital microscope with a 10x-50x optical
zoom lens [32].
Three factors in taking videos affect the success of the attack:
camera angle, distance between the target and the camera and light-
ing over the target. The success of the attack relies on accurate
identiﬁcation of touched points. The camera angle needs to be ad-
justed in order to record the ﬁnger movement over the touch screen.
For example, in a conference room, an attacker in the front can use
the front camera of her phone to record a person tapping in the back
row. The camera cannot be too far away from the victim. Other-
wise, the keys and ﬁngers in the image are too small to be differ-
entiated. Intuitively, a camera with an optical zoom lens can help
in such a case. However, the scenes of interest in our context may
not allow cameras with big lens. Lighting affects the brightness and
contrast of the video and thus the recognition result.
3.3 Step 2 - Preprocessing
Since we are particularly interested in the ﬁngertip area, where
the ﬁnger touches a key, our ﬁrst preprocessing step is to apply
DPM to detect and locate the touch device in the video. We then
crop the video and keep the region of the touch device with moving
ﬁngers. Cropping removes much the background and makes later
processing simpler.
To apply DPM to the detection of the target device in each video
frame, we ﬁrst need to generate positive data (such as iPad) and
negative data (background) to train a target device model. To get
the positive data, we take 700 images of the target device such as
iPad from different viewpoints, and manually label the device with
a bounding box. To get a tight bounding box of the device in an
image, we ﬁrst derive the homography relation between the device
image and the reference image in Figure 2, and then map the four
corners of the device (iPad in this example) in the reference image
to the training image. The up-right bounding rectangle of the four
points accurately delimits the device in the training image. To de-
rive the negative data, we employ 900 background images from the
SUN database [42] and label objects that have a similar shape to the