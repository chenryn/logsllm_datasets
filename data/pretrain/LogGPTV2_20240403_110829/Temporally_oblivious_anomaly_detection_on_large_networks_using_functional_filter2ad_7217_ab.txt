time. While we lack appropriate labels for the data during
this period, we do have knowledge of two large-scale SYN-
ﬂood distributed denial of service (DDoS) attacks against
two network web servers during the monitored period.
Our detection parameters are set with the goal of identify-
ing the most egregious anomalies on the network. We apply
ﬁve-minute aggregation windows (T = 300s) to the data;
a window large enough to beneﬁt from consistency due to
aggregation, yet small enough to still identify brief anoma-
lies. Deﬁning the optimal value T is a network-speciﬁc task
and an area for future work. We set our stopping criterion
variable at α = 3 and our cluster size threshold at n = 3,
reiterating our goal to ﬁnd the most signiﬁcant outliers.
As an illustration, we randomly select a ﬁve-minute win-
dow and demonstrate the detection results. In Fig. 1, we
plot the dendrogram of the hierarchical cluster tree calcu-
lated from the extracted features, which shows the manner
in which the tree is formed by merging nodes of increasing
linkage costs. Due to the size of the network, we do not il-
lustrate all of the unique leaf nodes, beginning instead with
nodes that have already been merged. What is clear from
Fig. 1 is that there exists a signiﬁcant jump in the linkage
cost near the top of the tree. We plot the cutoﬀ thresh-
old determined by α = 3 with the dashed line, resulting
in 3 remaining clusters, 2 of which contain only one sam-
ple. These two hosts are ﬂagged as anomalous during this
process. When run over the entire seven day window, this
method resulted in 1,658 ﬂagged anomalies out of a possi-
ble 1.5 million host samples, ﬂagging an average of 0.11%
of monitored hosts (0.8 hosts) as anomalous during a given
observation period. In 47% of the observation windows, no
hosts were ﬂagged as anomalous.
3. GROUPING THE ANOMALIES
It is of particular interest to see if common anomalies oc-
t
s
o
C
e
g
a
k
n
L
i
100
80
60
40
20
0
Cluster Nodes
Figure 1: Dendrogram of the hierarchical cluster
tree resultant from a single time window. The
dashed line shows the stopping point, resulting in
two detected outliers.
cur at diﬀerent time intervals and over diﬀerent hosts in the
network. This is not obvious, as by deﬁnition the behav-
ior is not within the network norms. To determine this,
we gather the feature vectors of all 1,658 anomalies ﬂagged
during the analysis of Section 2.2 in a set Z, and perform
additional clustering on this set. Each individual anomaly
was detected in a temporally oblivious manner, and follow-
on analysis determines if there are common themes amongst
those detected.
Normalizing the Data
To obtain categories for the collected outliers, it is necessary
to normalize in a way that preserves the high-level descrip-
tion of the data. For example, if the vast majority of hosts
receive less than F ﬂows per unique external source IP, and
two distinct hosts receive 100F and 500F ﬂows in a ﬁve
minute window, they should be treated as equals – they are
anomalous due to large received F/SIP. To account for this
issue, we normalize the data by a sigmoid function s(x),
which translates every value x ∈ [−∞,∞] to s(x) ∈ [0, 1]:
s(x) =
1
1 + exp(−(x − μ)/b)
.
We omit the full details for brevity, but we set our param-
eters μ = f50(Z(i)) and b such that the end of the lin-
ear portion of s(x) (e.g.
the bend point [19]) occurs at
x = f90(Z(i)). We set the bend point to the 90th percentile
of the data such that the extreme outliers will be truncated
to near unity while still linearly scaling the majority of the
data samples.
We demonstrate this scaling for the incoming B/P fea-
ture in Fig. 2, where we plot the scaled histogram of val-
ues alongside the sigmoid normalization function. This plot
shows that the sample points far away from the mass of the
distribution will be quantized to nearly 1, regardless of the
magnitude of their distance.
467l
e
u
a
V
d
e
z
i
l
a
m
r
o
N
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
50
100
Data Histogram
Sigmoid Function 
1.8
1.6
1.4
1.2
1
0.8
0.6
1
2
3
4
t
s
o
C
e
g
a
k
n
L
i
150
Incoming B/P Value
200
250
300
350
400
Cluster Nodes
Figure 2: Normalizing the data features of the out-
liers using a sigmoid function. This oﬀers linear scal-
ing for the mass of the data while quantizing the
extreme outliers.
Figure 3: Dendrogram of the hierarchical cluster
tree resultant from all anomalies over a 7 day period.
The choice of cutoﬀ threshold determines the cluster
granularity.
Clustering the Anomalies
We proceed by performing hierarchical clustering on the nor-
malized data using a complete-linkage criterion. The cost of
merging nodes A and B is deﬁned as max{D(a, b) : a ∈
A, b ∈ B}, which is more useful for identifying similar clus-
ters rather than outliers. Rather than assigning our cutoﬀ
threshold by identifying the jump in linkage cost as before,
we qualitatively assess the dendrogram to determine the ap-
propriate threshold. This has the beneﬁt of enabling us to
specify the level of detail which we are interested in describ-
ing the anomalies.
Visual inspection of the abbreviated dendrogram in Fig.
3 shows that we can glean a high-level description of the
outliers by setting our threshold to 1.45, which results in 4
clusters covering all of the anomalies. In order to identify
the common theme of each cluster in a quantitative sense
we applied a 2-class decision tree for each cluster, in which
members of the cluster belonged to the positive class, and all
other vectors are given a negative label. While we omit the
quantitative results, the decision rules were quite simple, ac-
curately classifying the large amount of anomalies generally
based on three primary features: byte ratio, IP ratio, and
outgoing bytes/packet. We now provide a brief qualitative
assessment of the type of traﬃc we see in each cluster:
Cluster 1: As the largest cluster, the hosts on this clus-
ter exhibit several diﬀerent patterns, and we noticed 3
distinct groups. However, the key feature is that 99%
of the 1,008 host samples in the cluster sent traﬃc to
more IP addresses than they received it from. The ﬁrst
group we observed in this cluster contains those hosts
which received a large number of low-byte packets in
a single ﬂow from a single IP address, and responded
with a much larger volume of traﬃc. An example of
this would be a host that received a single ﬂow with
26,000 packets each containing 40 bytes (1 MB total)
from a single IP address, and responded to the same
address with 30 MB of traﬃc.
The next group consists of hosts receiving a large amount
of traﬃc from a single source IP through an abnormally
large number of ﬂow records, and responding in kind.
An example of this would be a host which received
250 KB of data from a single IP across 350 ﬂows. We
can infer from this type of traﬃc that the source IP
in question operates as a network address translation
(NAT) or proxy server, and there are potentially nu-
merous unique hosts using the same externally visible
IP address.
Finally, the third group is clearly DDoS activity, as
the hosts receive traﬃc from numerous unique source
IP addresses each sending a signiﬁcantly large number
of ﬂows containing only 2-3 packets. The servers are
responding to more hosts than they are receiving traﬃc
from, signifying that they are able to keep up for the
time being. An interesting aspect about this cluster is
that the DDoS attacks on the two servers mentioned
earlier cluster together, even though they were carried
out in diﬀerent manners.
Cluster 2: The hosts in this cluster receive very low vol-
umes of traﬃc from very few sources, yet send substan-
tial volumes to more IP addresses than they received
traﬃc from. In fact, 97% of the 94 hosts in this group
sent outgoing traﬃc to more IP addresses than they
received incoming traﬃc from. Example: A host re-
ceived 735 bytes across 7 packets in a single ﬂow from a
single source and sent 195 MB of traﬃc across 140,000
packets in 2 ﬂows to 2 unique IP addresses.
Cluster 3: The hosts in this cluster received high volumes,
with a median of 595 MB of traﬃc during the obser-
vation period – for reference the median received traf-
ﬁc in the other clusters was 2.2 MB, 800 bytes, and
306 KB respectively. This could imply the hosts al-
low ﬁle uploads. Additionally, 99% of 402 hosts in this
group sent outgoing traﬃc to less destinations than
468they received incoming traﬃc from. Within this clus-
ter, we noticed subgroups containing high-volume uni-
directional traﬃc entering the network which had no
observed responses. This included DDoS traﬃc where
the servers under attack had either crashed or stopped
accepting traﬃc. There was additionally DDoS traﬃc
for which there were responses, although the response