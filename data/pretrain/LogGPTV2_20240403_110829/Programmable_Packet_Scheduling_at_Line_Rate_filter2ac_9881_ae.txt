switches can assist a PIFO-based scheduler by providing a
programmable ingress pipeline for scheduling and shaping
transactions, without requiring a dedicated atom pipeline in-
side each PIFO block. However, they still need PIFOs for
programmable scheduling.
Universal Packet Scheduling (UPS). UPS [31] shares our
goal of ﬂexible packet scheduling by seeking a single
scheduling algorithm that is universal and can emulate any
scheduling algorithm. Theoretically, UPS ﬁnds that the well-
known LSTF scheduling discipline [29] is universal if packet
departure times for the scheduling algorithm to be emulated
are known up front. Practically, UPS shows that by appro-
priately initializing slacks, many different scheduling objec-
tives can be emulated using LSTF. LSTF is programmable
using PIFOs, but the set of schemes practically expressible
with LSTF is limited. For example, LSTF cannot express:
1. Hierarchical scheduling algorithms such as HPFQ, be-
cause it uses only one priority queue.
2. Non-work-conserving algorithms. For such algorithms
LSTF must know the departure time of each packet up-
front, which is not practical.
3. Short-term bandwidth fairness in fair queueing, be-
cause LSTF maintains no switch state except one pri-
ority queue. As shown in Figure 1, programming a
fair queueing algorithm requires us to maintain a vir-
tual time state variable. Without this, a new ﬂow could
have arbitrary virtual start times, and be deprived of
its fair share indeﬁnitely. UPS provides a ﬁx to this
that requires estimating fair shares periodically, which
is hard to do in practice.
4. Scheduling policies that aggregate ﬂows from distinct
endpoints into a single ﬂow at the switch. An ex-
ample is fair queueing across video and web trafﬁc
classes, regardless of endpoint. Such policies require
the switch to maintain the state required for fair queue-
ing because no end point sees all the trafﬁc within a
class. However, LSTF cannot maintain and update
switch state progammatically.
The restrictions in UPS/LSTF are a result of a limited pro-
gramming model. UPS assumes that switches are ﬁxed and
cannot be programmed to modify packet ﬁelds. Further, it
only has a single priority queue. By using atom pipelines
to execute scheduling and shaping transactions, and by com-
posing multiple PIFOs together, PIFOs express a wider class
of scheduling algorithms.
Hardware designs for priority queues.
P-heap is a
pipelined binary heap scaling to 4-billion entries [14, 15].
However, each P-heap supports trafﬁc belonging to a single
10 Gbit/s input port in an input-queued switch and there is
a separate P-heap instance for each port [14]. This per-port
design incurs prohibitive area overhead on a shared-memory
switch, and prevents sharing of the data buffer and binary
heap across output ports. Conversely, it isn’t easy to overlay
multiple logical PIFOs over a single P-heap, which would
allow the P-heap to be shared across ports.
7. DISCUSSION
8. CONCLUSION
Packet scheduling allocates scarce link capacity across
contending ﬂows.
This allocation services an applica-
tion or a network-wide objective, e.g., max-min fairness
(WFQ) or minimum ﬂow completion time (SRPT). Past
work has demonstrated signiﬁcant performance beneﬁts re-
sulting from switch support for ﬂexible allocation [12, 21,
32, 39]. However, these beneﬁts have remained unrealized,
because today there isn’t a pathway to implementing such
schemes. PIFOs provide that pathway. They express many
scheduling algorithms and are implementable at line rate.
the very least PIFOs present
How a programmable scheduler will be used is still un-
clear, but at
the network
as an additional surface for deploying resource allocation
schemes. No longer will transport protocol designers need to
restrict themselves to end-host/edge-based solutions to dat-
acenter transport. As an example of a immediate use case
for PIFOs, one could run HPFQ for trafﬁc isolation, where
the classes corresponds to different tenants in a datacenter
and the ﬂows correspond to all source-destination VM pairs
belonging to a tenant. This kind of isolation is much harder
to provide with end-host/edge-based solutions today.
Looking forward, a programmable scheduler on switches
could simplify packet transport. For instance, pFabric [12]
minimizes ﬂow completion times by coupling a simple
switch scheduler (shortest remaining processing time) with
a simple end-host protocol (line rate transmission with no
congestion control). Other transport mechanisms [28, 32]
leverage fair queueing in the network to simplify transport
and make it more predictable.
That said, our current design is only a ﬁrst step and can be
improved in several ways.
1. A scheduling tree is more convenient than directly con-
ﬁguring a PIFO mesh, but it is still a low-level abstrac-
tion. Are there higher level abstractions?
2. Now that we have shown it is feasible, how will pro-
grammable scheduling be used in practice? This would
involve surveying network operators to understand
how programmable scheduling could beneﬁt them. In
turn, this would provide valuable design guidance for
setting various parameters in our hardware design.
3. Beyond a few counter examples, we lack a formal char-
acterization of algorithms that cannot be implemented
using PIFOs. For instance, is there a simple, checkable
property separating algorithms that can and cannot be
implemented using PIFOs? Given an algorithm spec-
iﬁcation, can we automatically check if the algorithm
can be programmed using PIFOs?
4. Our current design scales to 2048 ﬂows. If allocated
evenly across 64 ports, we could program scheduling
across 32 ﬂows at each port. This permits per-port
scheduling across trafﬁc aggregates (e.g., fair queue-
ing across 32 tenants within a server), but not a ﬁner
granularity (e.g., 5-tuples). Ideally, to schedule at the
ﬁnest granularity, our design would support 60K ﬂows:
the physical limit of one ﬂow for each packet. We cur-
rently support up to 2048. Can we bridge this gap?
Until recently, it was widely assumed that the fastest
switching chips would be ﬁxed-function; a programmable
device could not have the same performance. Recent re-
search into programmable parsers [23], fast programmable
switch pipelines [17], and languages to program them [16,
40], coupled with recent multi-Tbit/s programmable com-
mercial chips [1, 11] suggests that change might be afoot.
But so far, it has been considered off-limits to program the
packet scheduler—in part because the desired algorithms are
so varied, and because the scheduler sits at the heart of the
shared packet buffer where timing requirements are tightest.
It has been widely assumed too hard to ﬁnd a useful abstrac-
tion that can also be implemented in fast hardware.
PIFOs appear to be a very promising abstraction: they in-
clude a variety of existing algorithms, and allow us to ex-
press new ones. Further, they can be implemented at line
rate with modest chip area overhead.
We believe the most exciting consequence will be the cre-
ation of many new schedulers, invented by network opera-
tors, iterated and reﬁned, then deployed for their own needs.
No longer will research experiments be limited to simulation
and progress constrained by a vendor’s choice of schedul-
ing algorithms. Those needing a new algorithm could create
their own, or even download one from an open-source repos-
itory or a reproducible SIGCOMM paper.
To get there, we will need real switching chips with pro-
grammable PIFO schedulers. The good news is that we see
no reason why future switching chips can not include a pro-
grammable PIFO scheduler.
Acknowledgements
We are grateful to our shepherd, Jeff Mogul, the anonymous
SIGCOMM reviewers, and Amy Ousterhout for many sug-
gestions that greatly improved the clarity of the paper. We
thank Ion Stoica for helpful discussions, Robert Hunt for
help with the design of the compiler, and Radhika Mittal for
helping us understand LSTF. This work was partly supported
by NSF grant CNS-1563826 and a gift from the Cisco Re-
search Center. We thank the industrial partners of the MIT
Center for Wireless Networks and Mobile Computing (Wire-
less@MIT) for their support.
9. REFERENCES
[1] Barefoot: The World’s Fastest and Most Programmable
Networks.
https://barefootnetworks.com/media/white_papers/Barefoot-
Worlds-Fastest-Most-Programmable-Networks.pdf.
[2] Cadence Encounter RTL Compiler.
http://www.cadence.com/products/ld/rtl_compiler.
[3] High Capacity StrataXGS®Trident II Ethernet Switch
Series. http://www.broadcom.com/products/Switching/Data-
Center/BCM56850-Series.
[4] High-Density 25/100 Gigabit Ethernet StrataXGS Tomahawk
Ethernet Switch Series. http://www.broadcom.com/products/
Switching/Data-Center/BCM56960-Series.
[5] Intel FlexPipe. http://www.intel.com/content/dam/www/
public/us/en/documents/product-briefs/ethernet-switch-
fm6000-series-brief.pdf.
[6] Packet Buffers. http://people.ucsc.edu/~warner/buffer.html.
[7] Priority Flow Control: Build Reliable Layer 2 Infrastructure.
http://www.cisco.com/en/US/prod/collateral/switches/
ps9441/ps9670/white_paper_c11-
542809_ns783_Networking_Solutions_White_Paper.html.
[8] SRAM - ARM. https://www.arm.com/products/physical-ip/
embedded-memory-ip/sram.php.
[9] System Verilog.
https://en.wikipedia.org/wiki/SystemVerilog.
[10] Token Bucket. https://en.wikipedia.org/wiki/Token_bucket.
[11] XPliant™ Ethernet Switch Product Family.
http://www.cavium.com/XPliant-Ethernet-Switch-Product-
Family.html.
[12] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown,
B. Prabhakar, and S. Shenker. pFabric: Minimal
Near-Optimal Datacenter Transport. In SIGCOMM, 2013.
[13] J. C. R. Bennett and H. Zhang. Hierarchical Packet Fair
Queueing Algorithms. In SIGCOMM, 1996.
[14] R. Bhagwan and B. Lin. Design of a High-Speed Packet
Switch for Fine-Grained Quality-of-Service Guarantees. In
ICC, 2000.
[15] R. Bhagwan and B. Lin. Fast and Scalable Priority Queue
Architecture for High-Speed Network Switches. In
INFOCOM, 2000.
[16] P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown,
J. Rexford, C. Schlesinger, D. Talayco, A. Vahdat,
G. Varghese, and D. Walker. P4: Programming
Protocol-Independent Packet Processors. SIGCOMM CCR,
July 2014.
[17] P. Bosshart, G. Gibb, H.-S. Kim, G. Varghese, N. McKeown,
M. Izzard, F. Mujica, and M. Horowitz. Forwarding
Metamorphosis: Fast Programmable Match-action
Processing in Hardware for SDN. In SIGCOMM, 2013.
[18] A. K. Choudhury and E. L. Hahne. Dynamic Queue Length
Thresholds for Shared-memory Packet Switches. IEEE/ACM
Trans. Netw., 6(2):130–140, Apr. 1998.
[19] S.-T. Chuang, A. Goel, N. McKeown, and B. Prabhakar.
Matching Output Queueing with a Combined Input Output
Queued Switch. IEEE Journal on Selected Areas in
Communications, 17(6):1030–1039, Jun 1999.
[20] A. Demers, S. Keshav, and S. Shenker. Analysis and
Simulation of a Fair Queueing Algorithm. In SIGCOMM,
1989.
[21] F. R. Dogar, T. Karagiannis, H. Ballani, and A. Rowstron.
Decentralized Task-aware Scheduling for Data Center
Networks. In SIGCOMM, 2014.
[22] S. Floyd and V. Jacobson. Random Early Detection
Gateways for Congestion Avoidance. IEEE/ACM
Transactions on Networking, 1(4):397–413, Aug. 1993.
[23] G. Gibb, G. Varghese, M. Horowitz, and N. McKeown.
Design Principles for Packet Parsers. In ANCS, 2013.
[24] S. J. Golestani. A Stop-and-Go Queueing Framework for
Congestion Management. In SIGCOMM, 1990.
[25] P. Goyal, H. M. Vin, and H. Chen. Start-time Fair Queueing:
A Scheduling Algorithm for Integrated Services Packet
Switching Networks. In SIGCOMM, 1996.
[26] V. Jeyakumar, M. Alizadeh, D. Mazières, B. Prabhakar,
A. Greenberg, and C. Kim. EyeQ: Practical Network
Performance Isolation at the Edge. In NSDI, 2013.
[27] C. R. Kalmanek, H. Kanakia, and S. Keshav. Rate Controlled
Servers for Very High-Speed Networks. In GLOBECOM,
1990.
[28] S. Keshav. Packet-Pair Flow Control. IEEE/ACM
Transactions on Networking, 1994.
[29] J.-T. Leung. A New Algorithm for Scheduling Periodic,
Real-Time Tasks. Algorithmica, 4(1-4):209–219, 1989.
[30] P. McKenney. Stochastic Fairness Queuing. In INFOCOM,
1990.
[31] R. Mittal, R. Agarwal, S. Ratnasamy, and S. Shenker.
Universal Packet Scheduling. In NSDI, 2016.
[32] K. Nagaraj, D. Bharadia, H. Mao, S. Chinchali, M. Alizadeh,
and S. Katti. NUMFabric: Fast and Flexible Bandwidth
Allocation in Datacenters. In SIGCOMM, 2016.
[33] L. Popa, G. Kumar, M. Chowdhury, A. Krishnamurthy,
S. Ratnasamy, and I. Stoica. FairCloud: Sharing the Network
in Cloud Computing. In SIGCOMM, 2012.
[34] H. Sariowan, R. L. Cruz, and G. C. Polyzos. SCED: A
Generalized Scheduling Policy for Guaranteeing
Quality-of-service. IEEE/ACM Transactions on Networking,
Oct. 1999.
[35] L. E. Schrage and L. W. Miller. The Queue M/G/1 with the
Shortest Remaining Processing Time Discipline. Operations
Research, 14(4):670–684, 1966.
[36] M. Shreedhar and G. Varghese. Efﬁcient Fair Queuing using
Deﬁcit Round Robin. IEEE/ACM Transactions on
Networking, 4(3):375–385, 1996.
[37] A. Sivaraman, A. Cheung, M. Budiu, C. Kim, M. Alizadeh,
H. Balakrishnan, G. Varghese, N. McKeown, and S. Licking.
Packet Transactions: High-Level Programming for Line-Rate
Switches. In SIGCOMM, 2016.
[38] A. Sivaraman, S. Subramanian, A. Agrawal, S. Chole, S.-T.
Chuang, T. Edsall, M. Alizadeh, S. Katti, N. McKeown, and
H. Balakrishnan. Towards Programmable Packet Scheduling.
In HotNets, 2015.
[39] A. Sivaraman, K. Winstein, S. Subramanian, and
H. Balakrishnan. No Silver Bullet: Extending SDN to the
Data Plane. In HotNets, 2013.
[40] H. Song. Protocol-Oblivious Forwarding: Unleash the Power
of SDN Through a Future-proof Forwarding Plane. In
HotSDN, 2013.
[41] D. Verma, H. Zhang, and D. Ferrari. Guaranteeing Delay
Jitter Bounds in Packet Switching Networks. In TRICOMM,
1991.
[42] H. Zhang and D. Ferrari. Rate-Controlled Service
Disciplines. J. High Speed Networks, 3(4):389–412, 1994.