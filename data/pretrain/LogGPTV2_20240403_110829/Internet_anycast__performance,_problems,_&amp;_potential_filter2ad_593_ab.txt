swer the following question: Does anycast provide an in-
tuitively good server selection mechanism?
Server selection mechanisms may focus on various met-
rics. These include, but are not limited to, access latency, load
balance, resilience, and geographic proximity. Our goal is to
study whether anycast successfully improves these perfor-
mance metrics.3 In particular, we consider how these metrics
improve as replicas are added.
We use two different sources of data in our analyses: traffic
traces from the replicas of a root server, and active measure-
ments from RIPE Atlas probes. We describe these datasets,
including their features and limitations, next.
Root server traffic traces. Our first source of data is sam-
pled traffic from the sites of D-root DNS server operated by
University of Maryland. As of Jan 2018, D-root had over 120
anycast sites, 20 of which were global and the rest local [44].
We received 20% of all traffic at each replica, and base our
analysis on data collected for every day in 2017. On average,
in 2017, D-root received more than 30,000 queries per second,
resulting in about 140 GB of trace data per day. This rich
source of data allows us to understand client population and
distribution that root servers see. This data also provides
insight into load distribution, load variance, and inter-site
traffic variation, each of which we analyze.
There are two limitations to the D-root dataset. First, it
is data corresponding to a single root, and is subject to the
3We do not directly evaluate anycast resilience; however, we believe the
dynamic hints described in §5 can be used to mitigate the effect of large-
scale attacks like those that took place Nov. 30 and Dec. 1, 2015 [33, 52].
These attacks lasted for 2.5 hours on Nov. 30 and 1 hour on Dec. 1, resulting
in a temporary take-down of B-, G-, and H-root, and increased response
times from C-, E-, and K-root.
61
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Zhihao Li, Dave Levin, Neil Spring, and Bobby Bhattacharjee
(a) D-root queries by rank
(b) Distribution of D-root queries by additional distance
traveled.
Figure 1: D-root performance based on client traces.
Figures 1a and 1b show a measure of goodness of anycast
for D-root. For each query received at D-root, we geo-locate
the source of the query by IP address using the MaxMind
database [30]. Next, we measure the distance from the query
source to all D-root sites. For a query, the closest site is
ranked 0, the next closest rank 1, and so on. We compute the
same measure for each source IP address (client) as well.
policies of ASes that host sites. It is not clear if the perfor-
mance for D-root extrapolates to anycast performance in
general. Second, these data are entirely passively collected,
and do not provide client-side latency measures or insight
into alternate AS paths or other selection polices. To address
both these problems, we augment this dataset with active
measurements.
RIPE Atlas measurements. The RIPE Atlas framework [43]
is a set of ∼10,000 probes in 180 countries and ∼3,587 ASes
as of Jan 2018. Each probe periodically executes pre-defined
measurements, called “build-in measurements”, that include
DNS CHAOS queries and traceroutes to all 13 DNS roots.
Our analysis uses queries that the RIPE Atlas probes sent
to the 9 of 13 roots that have at least 5 anycast global sites [44].
DNS CHAOS queries retrieve data corresponding to the TXT
record for the string “hostname.bind.” with the DNS Class set
to CHAOS (as opposed to Class Internet, which is the com-
mon case). The “hostname.bind.” is a special record supported
by BIND nameserver implementations, which is convention-
ally configured by the server operator to return a string that
uniquely identifies the server replica.4 These measurements
allow us to record which specific replicas and sites a given
probe (whose location is known [42]) is directed to by any-
cast over time. This specific type of DNS query was used
in prior work, e.g., Moura et al. [33] and Schmidt et al. [47],
to characterize anycast performance. We evaluate possible
alternatives by augmenting this data with traceroutes and
our own measurements of alternate replicas and addresses
(§4).
3.1 How does anycast perform?
In this section, we characterize the performance of anycast
service provided by D-root using our sampled traces.
4We do not include measurements from G-root since it does not respond to
“hostname.bind.” queries with identifiers that distinguish replicas.
62
We use geographic distance as an approximation of ex-
pected latency because the passive trace dataset taken at
replicas does not provide a direct measure. Various stud-
ies have characterized the accuracy of MaxMind’s geoloca-
tion [18, 20, 40, 49], by comparing with a sample of known
locations or with a majority vote across databases. Although
MaxMind may not be reliably precise to 10 km, these studies
showed that it is within 300 km for approximately 80% of
IP addresses. Our focus is on such coarse-grained geoloca-
tion, aggregating query distances in bins of 500 km, and the
relatively small imprecision of the geolocation database is un-
likely to be the main source of mismatch between client and
replica. (We will also show that our MaxMind results agree
with known-location RIPE Atlas results, when addressing
the bias in probe locations.) Of course, the geographically
closest replica may not be the lowest latency replica due to
limited peering between ISPs and constrained BGP policy.
In §4 and §5, when using client-sourced traceroute data from
RIPE Atlas probes, we will quantify how often replica selec-
tion can be improved, not just for geographic proximity, but
for reducing latency as well.
Figure 1a shows what fraction of queries and clients are
directed to anycast sites ordered by rank. Only about 1/3rd
of queries go to the geographically closest (rank zero) site.
31.6% of all queries go to sites ranked 5 or higher.
Figure 1a shows that 2/3 of all queries/clients are somehow
“misdirected” by anycast. Figure 1b provides a measure of the
cost of these errors, by quantifying the extra distance queries
that are not directed to their closest site must travel. Figure 1b
 0 10 20 30 40 500123456789≥ 10Queries/Clients (%)Site RankQueriesClients 0 20 40 60 500km  additional distanceNumber of Global SitesAAACCCCCCDDDDEEEEEFFFFFIIIIIIIIIJJJJJJJJJJJJJKKKKKKKKKLLLLLLLInternet Anycast: Performance, Problems, & Potential
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
to evaluate the performance of alternate anycast sites S′
s→a
that could have been chosen for the query. Individual sites
are not often directly addressable, and queries sent to the
anycast address will deterministically go to Ss→a. We devise
a two step process to estimate the performance to a subset
of (promising) alternate sites S′:
(1) Find unicast representatives of each anycast site serv-
ing address a. A unicast representative for an anycast
site is a unicast address u that is geographically close
to the anycast site S, is contained within the AS that
advertises the site, and shares (substantially) the same
network path when reached from a source that is di-
rected to that site via anycast. That is, the path from s
to a shares, with s to u, the same AS path and approx-
imate latency, when u is meant to represent the site
Ss→a.
(2) Measure the performance from source s to address a
and address u to compare whether the site at u would
be better than the default a.
This two step process lets us measure how well a given site