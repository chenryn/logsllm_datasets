### Does Anycast Provide an Intuitively Good Server Selection Mechanism?

Server selection mechanisms can be evaluated based on various metrics, including access latency, load balancing, resilience, and geographic proximity. The objective of our study is to determine whether anycast effectively improves these performance metrics, particularly as the number of replicas increases.

#### Data Sources

We utilize two primary data sources for our analysis: traffic traces from D-root DNS server replicas and active measurements from RIPE Atlas probes. Below, we describe these datasets, their features, and limitations.

**1. Root Server Traffic Traces**

Our first dataset consists of sampled traffic from the sites of the D-root DNS server, operated by the University of Maryland. As of January 2018, D-root had over 120 anycast sites, with 20 being global and the rest local. We received 20% of all traffic at each replica and analyzed data collected daily throughout 2017. On average, D-root received more than 30,000 queries per second, resulting in approximately 140 GB of trace data per day. This extensive dataset allows us to understand the client population and distribution, as well as analyze load distribution, load variance, and inter-site traffic variation.

**Limitations of the D-root Dataset:**
- The data is specific to a single root server and may not be representative of anycast performance in general.
- The data is passively collected and does not provide client-side latency measures or insights into alternative AS paths or other selection policies.

To address these limitations, we supplement this dataset with active measurements.

**2. RIPE Atlas Measurements**

The RIPE Atlas framework consists of approximately 10,000 probes distributed across 180 countries and 3,587 ASes as of January 2018. Each probe periodically executes predefined measurements, including DNS CHAOS queries and traceroutes to all 13 DNS roots. Our analysis focuses on queries sent to 9 out of the 13 roots that have at least 5 anycast global sites. These DNS CHAOS queries retrieve data corresponding to the TXT record for the string "hostname.bind." with the DNS Class set to CHAOS. This special record is supported by BIND nameserver implementations and is typically configured to return a unique identifier for the server replica. These measurements allow us to track which specific replicas and sites a given probe (with known location) is directed to over time.

**Limitations of the RIPE Atlas Dataset:**
- G-root does not respond to "hostname.bind." queries with identifiers that distinguish replicas, so it is excluded from our analysis.

#### Performance Analysis

**Geographic Distance as a Proxy for Latency**

Since the passive trace dataset does not provide direct latency measurements, we use geographic distance as an approximation. Various studies have shown that MaxMind's geolocation is accurate within 300 km for approximately 80% of IP addresses. Our focus is on coarse-grained geolocation, aggregating query distances in bins of 500 km. While the geographically closest replica may not always be the lowest latency replica due to limited peering between ISPs and constrained BGP policies, our results are consistent with known-location RIPE Atlas results when addressing probe location bias.

**Anycast Performance for D-root**

Figure 1a shows the fraction of queries and clients directed to anycast sites ordered by rank. Only about one-third of queries go to the geographically closest (rank zero) site, with 31.6% of all queries going to sites ranked 5 or higher. Figure 1b quantifies the extra distance queries must travel when not directed to their closest site.

**Evaluating Alternate Anycast Sites**

To evaluate the performance of alternate anycast sites, we devised a two-step process:
1. **Find Unicast Representatives:** Identify unicast addresses that are geographically close to the anycast site, within the same AS, and share similar network paths.
2. **Measure Performance:** Compare the performance from the source to the anycast address and the unicast representative to determine if the alternate site would be better.

This process allows us to measure how well a given site performs compared to the default anycast site.

By combining these datasets and methods, we aim to provide a comprehensive understanding of anycast performance and its potential for improvement.