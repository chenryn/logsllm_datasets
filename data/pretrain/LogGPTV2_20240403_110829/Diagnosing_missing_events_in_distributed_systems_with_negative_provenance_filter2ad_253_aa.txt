title:Diagnosing missing events in distributed systems with negative provenance
author:Yang Wu and
Mingchen Zhao and
Andreas Haeberlen and
Wenchao Zhou and
Boon Thau Loo
Diagnosing Missing Events in Distributed Systems
with Negative Provenance
Yang Wu
University of Pennsylvania
Mingchen Zhao
University of Pennsylvania
Andreas Haeberlen
University of Pennsylvania
Wenchao Zhou
Georgetown University
Boon Thau Loo
University of Pennsylvania
ABSTRACT
When debugging a distributed system, it is sometimes necessary to
explain the absence of an event – for instance, why a certain route
is not available, or why a certain packet did not arrive. Existing
debuggers offer some support for explaining the presence of events,
usually by providing the equivalent of a backtrace in conventional
debuggers, but they are not very good at answering “Why not?”
questions: there is simply no starting point for a possible backtrace.
In this paper, we show that the concept of negative provenance can
be used to explain the absence of events in distributed systems.
Negative provenance relies on counterfactual reasoning to identify
the conditions under which the missing event could have occurred.
We deﬁne a formal model of negative provenance for distributed
systems, and we present the design of a system called Y! that tracks
both positive and negative provenance and can use them to answer
diagnostic queries. We describe how we have used Y! to debug
several realistic problems in two application domains: software-
deﬁned networks and BGP interdomain routing. Results from our
experimental evaluation show that the overhead of Y! is moderate.
INTRODUCTION
Categories and Subject Descriptors
C.2.3 [Network operations]: Network management; D.2.5
[Testing and debugging]: Diagnostics
Keywords
Diagnostics, Debugging, Provenance
1.
Finding problems in complex distributed systems has always been
challenging, as the substantial literature on network debugging and
root-cause analysis tools [5, 9, 12, 13, 18] can attest. The advent
of software-deﬁned networking (SDN) has added a new dimension
to the problem: forwarding can now be controlled by programs,
and, like all other programs, these programs can have bugs. Find-
ing such bugs can be difﬁcult because, in a complex network of
hosts, routers, switches, and middleboxes, they can manifest in
subtle ways that have no obvious connection with the root cause.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, Illinois, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2836-4/14/08 ...$15.00.
http://dx.doi.org/10.1145/2619239.2626335.
1
It would be useful to have a “network debugger” that can assist
the network operator with this task, but existing techniques tend to
be protocol-speciﬁc and cannot necessarily be applied to arbitrary
distributed applications, or SDNs with arbitrary control programs.
Hence, as others [8,9] have observed, a more powerful debugger is
needed.
Existing solutions, such as NetSight [9] or SNP [35], approach
this problem by offering a kind of “backtrace”, analogous to a stack
trace in a conventional debugger, that links an observed effect of
a bug to its root causes. For instance, suppose the administrator
notices that a server is receiving requests that should be handled
by another server. The administrator can then trace the requests
to the last-hop switch, where she might ﬁnd a faulty ﬂow entry;
she can trace the faulty entry to a statement in the SDN controller
program that was triggered by a certain condition; she can trace the
condition to a packet from another switch; and she can continue to
recursively explain each effect by its direct causes until she reaches
a set of root causes. The result is the desired “backtrace”: a causal
chain of events that explains how the observed event came to pass.
We refer to this as the provenance [1] of the event.
Provenance can be a useful tool for debugging complex inter-
actions, but there are cases that it cannot handle. For instance,
suppose that the administrator observes that a certain server is no
longer receiving any requests of a particular type. The key differ-
ence to the earlier scenario is that the observed symptom is not a
positive event, such as the arrival of a packet, that could serve as a
“lead” and point the administrator towards the root cause. Rather,
the observed symptom is a negative event: the absence of packets
of a certain type. Negative events can be difﬁcult to debug: prove-
nance does not help, and even a manual investigation can be difﬁ-
cult if the administrator does not know where the missing packets
would normally come from, or how they would be generated.
Nevertheless, it is possible to construct a similar “backtrace” for
negative events, using the concept of negative provenance [10, 32].
The key insight is to use counterfactual reasoning, that is, to exam-
ine all possible causes that could have produced the missing effect.
For instance, it might be the case that the missing packets could
only have reached the server through one of two upstream switches,
and that one of them is missing a ﬂow entry that would match the
packets. Based on the controller program, we might then establish
that the missing entry could only have been installed if a certain
condition had been satisﬁed, and so on, until we either reach a pos-
itive event (such as the installation of a conﬂicting ﬂow entry with
higher priority) that can be traced with normal provenance, or a
negative root cause (such as a missing entry in a conﬁguration ﬁle).
Negative provenance could be a useful debugging tool for net-
works and distributed systems in general, but so far it has not been
explored very much. A small number of papers from the database
Figure 1: Negative event scenario: Web requests from the In-
ternet are no longer reaching the web server because a faulty
program on the controller has installed an overly general ﬂow
entry in the switch in the middle (S2).
Figure 2: Positive provenance example, explaining how a DNS
packet made its way to the DNS server.
community [3, 10, 19] have used negative provenance to explain
why a given database query did not return a certain tuple, but,
other than our earlier workshop paper that made a case for negative
provenance [32], we are not aware of any previous applications in
the networking domain.
In Section 2, we provide an overview of positive and negative
provenance. We then make the following contributions:
• A formal model of positive and negative provenance in dis-
tributed systems, as well as a concrete algorithm for tracking
such provenance (Section 3);
• A set of heuristics for simplifying the resulting provenance
graphs and for making them more readable to a human in-
vestigator (Section 4);
• The design of Y! (pronounced “Why not?”), a system for
tracking positive and negative provenance and for answering
queries about it (Section 5);
• Two case studies of Y!, in the context of software-deﬁned
networks and BGP (Section 6); and
• An experimental evaluation of an Y! prototype, based on
Mininet, Trema [29] and RapidNet [24] (Section 7).
We discuss related work in Section 8 and conclude the paper in
Section 9.
2. OVERVIEW
In this section, we take a closer look at negative provenance, and
we discuss some of the key challenges.
2.1 Scenario: Network debugging
Figure 1 shows a simple example scenario that illustrates the prob-
lem we are focusing on. A network administrator manages a small
network that includes a DNS server, a web server, and a connection
to the Internet. At some point, the administrator notices that the
web server is no longer receiving any requests from the Internet.
The administrator strongly suspects that the network is somehow
misconﬁgured, but the only observable symptom is a negative event
(the absence of web requests at the server), so there is no obvious
starting point for an investigation.
Today, the only way to resolve such a situation is to manually
inspect the network until some positive symptom (such as requests
arriving at the wrong server) is found. In the very simple scenario
in Figure 1, this is not too difﬁcult, but in a data center or large
corporate network, it can be a considerable challenge.
It seems
preferable for the administrator to directly ask the network for an
explanation of the negative event, similar to a “backtrace” in a con-
ventional debugger. This is the capability we seek to provide.
2
2.2 Positive provenance
For positive events, there is a well-understood way to generate such
a “backtrace”: whenever an event occurs or some new state is gen-
erated, the system keeps track of its causes, and when an explana-
tion is requested, the system recursively explains each event with
its direct causes, until it reaches a set of “base events” (such as
external inputs) that cannot be explained further. The result can
be represented as a DAG, in which each vertex represents an event
and each edge indicates a direct causal relationship. In the database
literature, this is called the provenance [1] of the event; to distin-
guish it from negative provenance, we will refer to it as positive
provenance.
Figure 2 shows an example that explains why a DNS request
appeared at the DNS server at time t = 5 (V1). The DNS server
had received the packet from switch S2, which in turn had received
it from S1, and ultimately from the Internet (V2–V3); the switch
was connected to the DNS server via port #5 (V10–V11) and had
a ﬂow entry that directed DNS packets to that port (V4). The ﬂow
entry had been installed at t = 2 (V5–V7) because the switch had
forwarded an earlier DNS packet to the controller (V8), which had
generated the ﬂow entry based on its conﬁguration (V9).
Positive provenance is an active research area, but some solu-
tions for distributed systems are already available, such as Ex-
SPAN [37] or SNP [35]. Provenance is often a useful debugging
tool, just like the “backtraces” that many debuggers are offering.
2.3 Case study: Broken ﬂow entry
We now return to the scenario in Figure 1. One possible reason for
this situation is that the administrator has conﬁgured the controller
to produce a generic, low-priority ﬂow entry for DNS trafﬁc and a
speciﬁc, high-priority ﬂow entry for HTTP trafﬁc. If both entries
are installed, the system works as expected, but if the low-priority
entry is installed ﬁrst, it matches HTTP packets as well; thus, these
packets are not forwarded to the controller and cannot trigger the
installation of the high-priority entry. This subtle race condition
might manifest only at runtime, e.g., when both entries expire si-
multaneously during an occasional lull in trafﬁc; thus, it could be
quite difﬁcult to ﬁnd.
Positive provenance is not helpful here because, as long as re-
quests are still arriving at the HTTP server, their provenance con-
tains only the high-priority entry, and when the requests stop arriv-
ing, there is no longer anything to generate the provenance of !
2.4 How common are negative symptoms?
To get a sense of how common this situation is, we surveyed
diagnostics-related posts on three mailing lists. We chose lists that
cover a mix of different diagnostic situations, including NANOG-
user and Outages [23] (for faults and misconﬁgurations), and
SDN ControllerWhy is the HTTP servernot getting any requests?Faultyflow entryDNS andHTTPrequestsS1S2S3DNS ServerHTTP ServerInternetAdminEXIST(t=[2s,now], S2, flowTable(@S2, PrioHigh, DNS, Forward, Port5)) RECEIVE(t=2s, S2←Controller,  flowTable(@S2, PrioHigh, DNS, Forward, Port5)) SEND(t=1.8s, Controller→S2,  flowTable(@Controller, PrioHigh, DNS, Forward, Port5)) DERIVE(t=1.8s, Controller,  flowTable(@Controller, PrioHigh, DNS, Forward, Port5)) APPEAR(t=5s, DNS Server, packet(@DNS Server, DNS)) V1 RECEIVE(t=5s, S2←S1, packet(@S2, DNS)) V2 SEND(t=5s, S1→S2, packet(@S1, DNS)) V3 V4 V5 V6 V7 RECEIVE (t=1.8s, Controller ←S2, packet(@Controller, DNS)) V8 EXIST(t=[0s,now], Controller,  missHandler(@Controller, DNS, PrioHigh, DNS, Forward, Port5)) V9 EXIST(t=[0.5s,now], S2, link(@S2, DNS Server, Port5)) V10 APPEAR(t=0.5s, S2, link(@S2, DNS Server, Port5)) V11 ... ... ... Mailing list
NANOG-user
ﬂoodlight-dev
Outages [23]
Posts related
to diagnostics
29/144
19/154
46/60
Initial symptoms
Positive Negative
15 (52%)
14 (48%)
5 (26%)
14 (74%)
38 (83%)
8 (17%)
Table 1: Survey of networking problems and their symptoms,
as discussed on three mailing lists over a two-month period,
starting on November 22, 2013.
ﬂoodlight-dev (for software bugs). To get a good sample size, we