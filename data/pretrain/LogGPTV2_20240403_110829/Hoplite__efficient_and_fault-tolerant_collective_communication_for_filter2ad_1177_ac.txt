in between.
Optimization for immutable get. Although Hoplite objects are
immutable, the receiver task still copies the object data from its local
store during a Get, in case it modifies the buffer later on. However,
if it only needs read access to the object, then Hoplite can directly
return a pointer inside the local store. Read-only access can be
enforced through the front-end programming language, e.g., with
const in C++.
3.4 Receiver-Driven Collective
Communication
Hopliteâ€™s receiver-driven coordination scheme optimizes data trans-
fer using distributed protocols. In Hoplite, data transfer happens
in two scenarios: either a task calls Get to retrieve an object with
a given ObjectID, or a task calls Reduce to create a new object by
reducing a set of other objects with a reduce operation (e.g., sum,
min, max).
3.4.1 Broadcast. Broadcast in a task-based distributed system hap-
pens when a group of tasks located on multiple nodes want to get
the same object from its creator task. Specifically, a sender task from
node S creates an object with Put and a group of receiver tasks R1,
R2, ... fetches it using Get. For the receiver tasks that locate on
different nodes from the sender task, their corresponding receiver
nodes will fetch the object from sender nodeâ€™s local object store to
the receiver nodesâ€™ local object store. To simplify the description of
our method, we assume that the sender task and the receiver tasks
locate on different nodes and use the sender S and the receiver R1,
R2, ... to also refer to the local object store on the nodes.
Broadcast in a task-based distributed system is challenging be-
cause we have no knowledge of the tasks, including where these
tasks are located and when these tasks fetch the object. If all re-
ceivers simply fetch the object from the sender, the performance
will be restricted by the senderâ€™s upstream bandwidth. Traditional
collective communication libraries can generate a static tree where
the root is the sender node to mitigate the throughput bottleneck.
The goal of Hopliteâ€™s receiver-driven coordination scheme is to
achieve a similar effect but using decentralized protocols. Inspired
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Siyuan Zhuang et al.
both the object and the local object store on a node that stores the
corresponding object. Note that in a task-based distributed system,
the objects to reduce can become ready to reduce in any arbitrary
order.
How to reduce objects efficiently to accommodate dynamic ob-
ject creation is more challenging than broadcast. Broadcast is sim-
pler because a receiver can fetch the object from any sender, and
Hoplite thus has more flexibility to adapting data transfer schedule.
For reduce, we need to make sure all the objects are reduced once
and only once: when one object is added into a partial reduce result,
the object should not be added into any other partial results.
In Hoplite, we choose to use a tree-structured reduce algorithm,
while the question is what type of tree to use. Letâ€™s think about re-
ducing ğ‘› objects. Without the support of collective communication
in task-based distributed systems, each node sends the object to a
single receiver. Letâ€™s assume that the network latency is ğ¿ , network
bandwidth is ğµ, and the object size is ğ‘†. This approachâ€™s total reduce
running time is ğ¿ + ğ‘›ğ‘†
ğµ . The ğ¿ term is due to the network latency,
and ğ‘›ğ‘†
ğµ is due to the receiverâ€™s bandwidth constraint, because every
node has to send the object in to it. This is a special kind of tree
where the degree of the root is ğ‘›. When object size is very small
(i.e., ğ‘†
ğµ is negligible), the performance of this kind of tree is the best.
To mitigate the bandwidth bottleneck at the receiver, we can
generalize this ğ‘›-nary tree to a ğ‘‘-nary tree. When we use a ğ‘‘-nary
tree, the total running time is ğ¿ logğ‘‘ ğ‘›+ ğ‘‘ğ‘†
ğµ . It reduces the latency due
to the bandwidth constraint but incurs additional latency because
the height of the tree grows to logğ‘‘ ğ‘›. If an object is very large (i.e.,
ğµ â‰« ğ¿), we can set ğ‘‘ = 1. This means all the nodes are in a single
ğ‘†
chain, and its running time is ğ‘›ğ¿ + ğ‘†
ğµ . Note that we only need to
incur ğ‘†
ğµ for transferring the actual content of the object, because
we use fine-grained pipelining, i.e., intermediate nodes send the
partially reduced object to the next node. As we can see here, the
optimal choice of ğ‘‘ depends on the network characteristics, the
size of the object, and the number of participants (objects). In other
words, we choose the ğ‘‘ to minimize the total latency:
(cid:40)ğ‘›ğ¿ + ğ‘†
ğµ
ğ‘‡ (ğ‘‘) =
ğ¿ logğ‘‘ ğ‘› + ğ‘‘ğ‘†
ğµ
if ğ‘‘ = 1;
otherwise.
(1)
During runtime, Hoplite will automatically chooses the optimal ğ‘‘
based on an empirical measure of these three factors.
Once the topology of the tree is determined, we need to assign
nodes into the tree. Here we want to allow Reduce to make signif-
icant progress even with a subset of objects. To do so, we assign
arriving objects with a generalized version of in-order tree traver-
sal. For a ğ‘‘-nary tree, for each node, we traverse the first child,
the node itself, the second child, third child, ..., and the ğ‘‘-th child.
Figure 5a shows an example for reducing 6 objects with a binary
tree. Note that though MPI also supports tree-reduce, our method
is completely different: MPIâ€™s tree is constructed statically, and our
tree is constructed dynamically taking the object arrival sequence
into consideration.
If a task only wants to reduce a subset of objects (i.e., num_object
is smaller than the size of the source object list in Reduce), the tree
construction process stops when there are num_object objects in
the tree. For example, if the task wants to reduce 6 out of 10 objects,
Figure 4: An example of broadcasting an object (integer array {5, 1, 0})
from a sender (S) in Hoplite, when the receivers (R1-R3) arrive at different
times. (a) - (d) show the broadcast process without failure. (câ€™) and (dâ€™) show
the broadcast process when R1 fails after (b).
by application-level broadcast [5, 6] in peer-to-peer systems that
uses high-capacity nodes to serve as intermediate nodes in the
broadcast tree, we use receivers who receives the object earlier
than the rest as intermediates to construct a broadcast tree.
When a receiver R wants to fetch a remote object, it first checks
if the object is locally available, or there is an on-going request
for the object locally. If so, the receiver just waits until it gets the
completed object. This avoids creating cyclic object dependencies.
Otherwise, R queries the object directory service for the objectâ€™s
location. The object directory service first tries to return one loca-
tion with a complete copy. If none exist, then the object directory
service returns one of the locations holding a partial copy. This
is so that partial objects can also act as intermediate senders, but
locations with complete copies are favored.
When the location query replies, R also removes the location
returned from the directory and immediately add itself to the object
directory as a location with a partial copy to enable pipelining. Once
the data transfer is complete, the receiver adds the senderâ€™s location
back to the object directory service and mark itself as a location
with a complete copy. This makes sure that, for each object, a node
can only send to one receiver at a time, thus mitigating bottlenecks
at any single node.
Figure 4 shows an example of a broadcast scenario in Hoplite.
In Figure 4a, the first receiver R1 starts to fetch the object from
the sender S. In Figure 4b, S is still sending to R1, so it does not
appear in the object directory when the second receiver R2 arrives.
Thus, R2 fetches the object from R1, the partial copy. In Figure 4c,
R1 has finished receiving, but is still sending to R2. Then, the object
directory contains S and R2 as a complete and partial location,
respectively. In Figure 4d, R3 queries the object directory, which
chooses S over R2 as the sender because S has a complete object.
3.4.2 Reduce. Reduce happens when a task in a task-based dis-
tributed system wants to get a reduced object (e.g., summed or
maximal object) from a list of objects. In Hoplite, this happens via a
Reduce call. Similar to broadcast, we assume that each object to re-
duce is located on a separate node and we use R1, R2, ... to represent
(cid:24)(cid:20)(cid:19)(cid:53)(cid:20)(cid:24)(cid:20)(cid:19)(cid:54)(cid:24)(cid:20)(cid:19)(cid:53)(cid:20)(cid:24)(cid:20)(cid:19)(cid:53)(cid:21)(cid:24)(cid:20)(cid:19)(cid:54)(cid:24)(cid:53)(cid:20)(cid:24)(cid:20)(cid:19)(cid:53)(cid:21)(cid:24)(cid:53)(cid:20)(cid:24)(cid:20)(cid:19)(cid:53)(cid:21)(cid:24)(cid:20)(cid:19)(cid:53)(cid:22)(cid:24)(cid:20)(cid:19)(cid:54)(cid:11)(cid:68)(cid:12)(cid:11)(cid:69)(cid:12)(cid:11)(cid:70)(cid:182)(cid:12)(cid:11)(cid:71)(cid:182)(cid:12)(cid:20)(cid:19)(cid:20)(cid:19)(cid:24)(cid:20)(cid:19)(cid:54)(cid:24)(cid:20)(cid:19)(cid:53)(cid:20)(cid:24)(cid:20)(cid:19)(cid:53)(cid:21)(cid:24)(cid:20)(cid:19)(cid:53)(cid:20)(cid:24)(cid:20)(cid:19)(cid:53)(cid:21)(cid:24)(cid:20)(cid:19)(cid:53)(cid:22)(cid:24)(cid:20)(cid:19)(cid:54)(cid:11)(cid:70)(cid:12)(cid:11)(cid:71)(cid:12)(cid:20)(cid:19)(cid:54)(cid:24)Hoplite: Efficient and Fault-Tolerant Collective Communication for Task-Based Distributed Systems
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
(a) Reduce Tree.
(b) Reduce Tree with failure.
Figure 5: Examples of reduce where the objects arrive in the order of R1, R2, ..., R6. The numbers on the top of each node (and the numbers in leaf nodes)
represent the object to reduce and green blocks means the fraction of the object that is ready. The numbers on the bottom of each node represent the reduced
result and yellow blocks means the fraction of the object that has been reduced. Each intermediate node is responsible to reduce the subtree rooted at it. (a)
An example reduce tree consists of 6 objects. (b) The reconstructed reduce tree after R2 fails.
then the earliest arriving 6 objects are in the reduce tree structured
as Figure 5a.
An application can also specify the inputs of a Reduce incremen-
tally, i.e. by passing the ObjectID result of one Reduce operation
as an input of a subsequent Reduce operation. The data transfer for
composed Reduce operations will naturally compose together. In
particular, as soon as the first Reduce output is partially ready, it
will be added to the object directory service, where it will be dis-
covered by the downstream Reduce coordinator. The first output
can then be streamed into the downstream Reduce.
3.4.3 AllReduce. AllReduce is a synchronous collective communi-
cation operation that is useful for synchronous data-parallel train-
ing. Optimizing allreduce is not our design goal: people usually
do synchronous data-parallel training on specialized distributed
systems that are optimized for bulk-synchronous workloads (e.g.,
TensorFlow [1], PyTorch [37]) rather than on task-based distributed
systems. In Hoplite, a developer can express allreduce by concate-
nating reduce and broadcast.
3.5 Fault-Tolerant Collective Communication
In the previous subsection, we assume that there is no task fail-
ures. However, task failures can happen in a task-based distributed
systems for various reasons, including (1) the node that the task
is running on crashes, (2) the node runs out of available memory
and has to kill the task, and (3) the task encounters a runtime er-
ror. Task-based distributed systems already support transparent
fault-tolerance to tasks [52], but adding collective communication
support requires us to dynamically change data transfer schedule
when a fraction of the tasks fail when participating in the collective
communication. This is because we do not want a failed task to
block collective communication, and we want to allow a recovered
task to rejoin an existing collective communication.
3.5.1 Broadcast. When a sender failure is detected by the receiver
in broadcast, the receiver immediately locate another sender by
querying the object directory again. The new sender only needs
to send the remaining object that the receiver does not have. A
failed task can rejoin broadcast transparently because the failed
task can simply call Get on the same ObjectID to fetch the object.
Implementing this feature naively would cause cyclic object transfer
dependencies. For example, it is possible that two nodes try to fetch
the same object from each other. It is because when the a receiver
locates an alternative sender, the object directory can return the
address of another node which fetches the object from the receiver.
To avoid cyclic dependencies, we need to track the dependencies of
Get if the sender is not the original task that creates the object. If a
sender fails, the receiver only resumes if it can find another sender
whose dependencies do not include the receiver itself. Figure 4câ€™
shows the previous example if R1 fails. R2 resumes the fetch from
S, and when R3 comes, R3 can fetch from R2 (Figure 4dâ€™).
3.5.2 Reduce. When a task fails during Reduce, this node is im-
mediately removed from the tree by the coordinator, and will be
replaced by the next ready source object. The guarantee is that to
reduce ğ‘› objects from ğ‘š source objects, as long as at least ğ‘› objects
can be created (i.e., ğ‘š âˆ’ ğ‘› tasks can fail), Reduce will return suc-
cessfully. Otherwise, Reduce completes when enough failed tasks
are reconstructed by the underlying task-based systemâ€™s recovery
mechanisms. A failed tree node causes its parent, its grandparent,
and all its ancestors to clear the reduced object. In the previous
example, Figure 5b shows the adapted tree after R2 fails. If the task
Reduce 6 out of 10 objects and R2 is recovered after R7 arrives, R7
replaces R2â€™s position in the tree. (R7 can also be the rejoined R2.)
R4 has to clear all the current reduced the object, because the final
result should be the Reduce result of R1, R3, R4, ..., R7. Any inter-
mediate result that contains R2â€™s object has to be cleared. Overall,
at most logğ‘‘ ğ‘› nodes have to clear the current object.
4 IMPLEMENTATION
The core of Hoplite is implemented using 3957 lines of C++. We
provide a C++ and a Python front-end. The Python front-end is
implemented using 645 lines of Python and 275 lines of Cython.
We build the Python front end because it is easier to integrate
with Ray [30] and other data processing libraries (e.g., Numpy [35],
TensorFlow [1], PyTorch [37]). The interface between the Python
front-end and the C++ backend is the same as Hopliteâ€™s API (Ta-