title:CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial
Examples
author:Honggang Yu and
Kaichen Yang and
Teng Zhang and
Yun-Yun Tsai and
Tsung-Yi Ho and
Yier Jin
CloudLeak: Large-Scale Deep Learning Models
Stealing Through Adversarial Examples
Honggang Yu∗, Kaichen Yang∗, Teng Zhang†, Yun-Yun Tsai‡,
Tsung-Yi Ho‡, Yier Jin∗§
∗University of Florida, {honggang.yu, bojanykc}@uﬂ.edu, yier.jin@ece.uﬂ.edu
†University of Central Florida, PI:EMAIL
‡National Tsing Hua University, PI:EMAIL, PI:EMAIL
Abstract—Cloud-based Machine Learning as a Service
(MLaaS) is gradually gaining acceptance as a reliable solution to
various real-life scenarios. These services typically utilize Deep
Neural Networks (DNNs) to perform classiﬁcation and detection
tasks and are accessed through Application Programming Inter-
faces (APIs). Unfortunately, it is possible for an adversary to
steal models from cloud-based platforms, even with black-box
constraints, by repeatedly querying the public prediction API
with malicious inputs. In this paper, we introduce an effective
and efﬁcient black-box attack methodology that extracts large-
scale DNN models from cloud-based platforms with near-perfect
performance. In comparison to existing attack methods, we
signiﬁcantly reduce the number of queries required to steal the
target model by incorporating several novel algorithms, including
active learning, transfer learning, and adversarial attacks. During
our experimental evaluations, we validate our proposed model
for conducting theft attacks on various commercialized MLaaS
platforms hosted by Microsoft, Face++, IBM, Google and Clarifai.
Our results demonstrate that the proposed method can easily
reveal/steal large-scale DNN models from these cloud platforms.
The proposed attack method can also be used to accurately
evaluates the robustness of DNN based MLaaS classiﬁers against
theft attacks.
I.
INTRODUCTION
Deep neural networks (DNNs) have become the most
common architecture in machine learning, implemented in a
variety of tasks across many disciplines [1], [2], [3], [4], [5].
However, creating a successful DNN model depends on the
availability of huge amounts of data as well as enormous
computing power, and the model training is often an arduously
slow process. This presents a large barrier to those interested in
utilizing a DNN. To meet the demands of users who may not
have sufﬁcient resources, cloud-based deep learning services
arose as a cost-effective and ﬂexible solution, allowing users
to complete their machine learning (ML) tasks efﬁciently.
Cloud-based deep learning services generally provide end
users with a prediction API for a DNN model trained to
achieve performance beyond what users could create for them-
selves. Users query the API with their inputs (e.g., images,
§Corresponding Author.
Network and Distributed Systems Security (NDSS) Symposium 2020
23-26 February 2020, San Diego, CA, USA
ISBN 1-891562-61-4
https://dx.doi.org/10.14722/ndss.2020.24178
www.ndss-symposium.org
audio, etc), pay for each individual query, and then receive
predictions results (e.g., labels, conﬁdence) back from the API,
without having to understand the methods in creating those
predictions. Take the Microsoft Custom Vision Service as an
example: this service helps users create high-quality custom
deep learning classiﬁers by applying active learning along with
neural network architecture search technology, and predicts the
class of objects inside images supplied by the users with high
accuracy.
Typically users access the API by querying it and receive
the results. However, the DNN models and training data inside
the prediction API are routinely inaccessible to the public due
to economic and privacy concerns. The provider of the API
may spend great effort collecting data and training models,
and thus wants to keep them proprietary. The training data
may also contain private information related to individuals,
prohibiting disclosure of this data by law.
Though the DNN model and the training data behind the
prediction API are not directly exposed to the public, recent
research has demonstrated that information leakage is still
possible through query operations. For example, F. Tram`er
et al. were the ﬁrst
to develop a model extraction attack
[6] in 2016, which extracts an equivalent or near-equivalent
machine learning model by simply querying and obtaining
the prediction results on input feature vectors. Since then,
many following works have been proposed to improve model
extraction attacks [7], [8], [9]. In addition to the model itself,
the data used to train the model can also be leaked through
querying. R. Shokri et al. proposed a membership inference
attack to determine whether the training set contains certain
data records [10]. Membership inference attacks are further
studied in [11], which concludes that membership disclosure
exists widely, not only in overﬁtting models, but also in well-
generalized models.
Some defense mechanisms have been proposed to reduce
the impact of information leakage during the querying process
[12], [13], but none of them ensure effectiveness and efﬁciency
at the same time. The defense against information leakage via
DNN model queries thus still remains as an open problem.
Although recent DNN query and model extraction attack
have made signiﬁcant progress, they remain impractical for
real-world scenarios due to the following limitations: 1) Cur-
rent model stealing attacks against commercialized platforms
mainly target small-scale machine learning models such as
linear regression, logistic regression, support vector machine
(SVM), and neural networks. The effectiveness of these at-
tacks are not fully evaluated on complex DNN models with
more layers and more parameters. 2) Current model stealing
attacks require the number of queries to the target model
to be proportional to the number of model parameters. This
may be acceptable when the model is small and the number
of parameters is limited. However, queries of this size will
be impractical when targeting recent popular DNN models
like VGGNet, ResNet and Inception that contain millions of
parameters. The existing method for stealing large-scale deep
learning models has reliable performance but requires massive
amounts of prediction queries and incurs high costs [14].
In this paper we introduce a novel type of model stealing
attack against popular MLaaS platforms hosted by Microsoft
[15], Face++ [16], IBM [17], Google [18] and Clarifai [19].
Our hypothesis is that an adversary who targets these pay-as-
you-go, commercialized, MLaaS platforms has no prior knowl-
edge about the exact training data, architecture or parameter
of the victim model, but can observe the classiﬁcation outputs
(i.e., labels or conﬁdence scores) when providing the prediction
APIs with random inputs, i.e., query operation.
The key idea of our attack approach is to use input-
output pairs obtained by querying such black-box APIs with
malicious examples to retrain the substitute models which are
generally chosen from candidate Model Zoo (see Figure 1).
Speciﬁcally, by applying a margin-based, adversarial, and
active learning algorithm to search these malicious examples,
we improve the efﬁciency of queries to the victim classiﬁers
inside these MLaaS platforms. As a result, since the resulting
images lie approximately on the decision boundary of the
victim classiﬁer, an attacker can greatly reduce the labeling
effort when generating the synthetic data set for retraining
the substitute model. Through detailed experimental evaluation
and testing, we demonstrate that it is possible to replicate the
functionality of victim classiﬁers by utilizing the well-trained
substitute model. An adversary can use our attack framework
to construct a free version of the victim model which bypasses
the monetary costs involved in collecting data and training
models.
A qualitative comparison between our work and existing
works is shown in Table I. From Table I we can see that our
method can steal large-scale deep learning models with high
accuracy, few queries, and low costs simultaneously, while
prior works fail in at least one or two of these aspects. We also
provide detailed evaluations to clarify the quantitative analysis
of our work and existing works in the paper.
In summary, we mainly make the following contributions
to address the limitations of the existing works:
local substitute models,
• We propose a new adversarial attack method named
FeatureFool against
that
adopts internal representation for generating a subset
of malicious samples (i.e., synthetic dataset). These
samples are used to query the victim model to efﬁ-
ciently learn the distance between decision boundaries
of the victim model and the stolen model, signiﬁcantly
reducing the number of queries required to extract the
victim model.
• We design a black-box model theft attack targeting
large-scale DNN models provided by commercial plat-
forms. Our attack accelerates the model theft process
with adversarial active learning and transfer learning
from existing well-trained models such as AlexNet,
VGG19, VGGFace, ResNet50, etc.
• We evaluate the attack framework on a group of
popular commercial platforms hosted by Microsoft,
Face++, IBM, Google and Clarifai. The experimental
results show that our model theft attack can success-
fully construct a local substitute model with perfor-
mance similar to the victim model found in commer-
cialized MLaaS platforms with much less queries than
previous attacks.
Fig. 1: Illustration of our MLaaS model stealing attacks.
Method
F. Tram`er
[6]
Juuti
[20]
Correia-Silva [14]
Papernot
[21]
Our Method
Parameter
Size
∼ 45K
∼ 100M
∼ 200M
∼ 100M
∼ 200M
Queries
∼ 102 K
∼ 111 K
∼ 66K
∼ 7K
∼ 3K
Accuracy
Cost
High
High
High
Low
High
Low
High
-
-
Low
TABLE I: A Comparison to prior works.
II. RELATED WORK
Transfer Learning. Transfer learning aims to recognize and
apply knowledge gained from previous tasks (source domains)
to different but related tasks (target domains) [22]. For exam-
ple, many researchers have recently shown that layers trained
on a source task with large-scale labelled datasets can be
reused to predict on a target domain that has substantially
less available data [23], [24], [25], [26], [27]. Ge et al. [24]
use special descriptors to search for a training subset and
jointly ﬁne-tune a pre-trained deep neural network for both
source and target tasks. More similar to our work, Sun et
al. [25] design a DeepID for learning a set of high-level
feature representations and transfer joint Bayesian model from
source domain to the target domain based on the DeepID.
Unlike the work in [25], we ﬁne-tune a VGG19 model [28]
on a desired subset of training samples and use DeepID to
further extract high-level image presentation. In this paper,
we call this new model VGG DeepID. Our transfer learning
scheme additionally accelerates the model stealing process and
provides performance gains by using well-trained models such
as AlexNet, VGG19, VGG DeepID, VGGFace and ResNet50.
Adversarial Attacks in Deep Learning. Adversarial attacks
against DNNs generate adversarial examples by adding par-
ticular perturbations to the original inputs [29], [30], [31],
[32], [33], [34], [35], [36], [37], [38], [39], [40]. In the
image processing area,
these carefully crafted images are
often imperceptible to human eyes as the perturbations are
slight, but can easily fool a classiﬁer into predicting incorrect
2
Input Output AdversaryModelZooMaliciousExamplesCandidate LibraryMLaaSSearchServices
Products and Solutions
Customization
Function
Black-box
TABLE II: MLaaS Services in Detail
Microsoft
Face++
IBM
Google
Clarifai
Custom Vision
Custom Vision
Emotion Recognition API
Watson Visual Recognition
AutoML Vision
Safe for Work (NSFW) API






Trafﬁc Recognition
Flower Recognition
Face Emotion
Veriﬁcation
Face Recognition
Flower Recognition
Offensive Content Moderation






Model
Types
Neural Nerwork
Neural Network
Neural Network
Neural Network
Neural Network
Neural Network
Monetize
Conﬁdence












labels [41], [42], [43]. Szegedy et al. [34] propose the ﬁrst
algorithm to generate adversarial examples - L-BFGS to search