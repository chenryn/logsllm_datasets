served by the connection. These results were gathered from video play-
backs. We note that this graph shows the retransmission rate averaged
within 10 ms RTT buckets, and the actual rate experienced by a connec-
tion can be much higher [22]. Across all RTTs, retransmission rates are
0%, 2%, 8% and 18% at the 50th, 80th, 90th and 95th percentiles.
in part by the fact that QUIC connections established by the mo-
bile app only achieve a 68% 0-RTT handshake rate on average—a
20% reduction in successful 0-RTT handshake rate as compared
to desktop—which we believe is due to two factors. Recall from
Section 3.1 that a successful 0-RTT handshake requires both a valid
server config and a valid source address token in a client’s handshake
message, both of which are cached at the client from a previous suc-
cessful handshake. The source-address token is a server-encrypted
blob containing the client’s validated IP address, and the server
191
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
A. Langley et al.
in rebuffers. As a measure of video quality, we consider the fraction
of videos that were played at their optimal rate: the format best suited
for the video viewport size and user intent. Among video playbacks
that experienced no rebuffering, this fraction is the same for users
in QUICg as those in TCPg. Among playbacks that experienced
non-zero rebuffering, QUIC increased the number of videos played
at their optimal rates by 2.9% for desktop and by 4.6% for mobile
playbacks.
QUIC’s benefits are higher whenever congestion, loss, and RTTs
are higher. As a result, we would expect QUIC to benefit users most
in parts of the world where congestion, loss, and RTTs are highest;
we look into this thesis next.
6.6 Performance By Region
Differences in access-network quality and distance from Google
servers result in RTT and retransmission rate variations for different
geographical regions. We now look at QUIC’s impact on Search
Latency10 and on Video Rebuffer Rate in select countries, chosen to
span a wide range of network conditions.
Table 3 show how QUIC’s performance impact varies by country.
In South Korea, which has the lowest average RTT and the low-
est network loss, QUICg’s performance is closer to that of TCPg.
Network conditions in the United States are more typical of the
global average, and QUICg shows greater improvements in the USA
than in South Korea. India, which has the highest average RTT and
retransmission rate, shows the highest benefits across the board.
QUIC’s performance benefits over TLS/TCP are thus not uni-
formly distributed across geography or network quality: benefits are
greater in networks and regions that have higher average RTT and
higher network loss.
6.7 Server CPU Utilization
The QUIC implementation was initially written with a focus on rapid
feature development and ease of debugging, not CPU efficiency.
When we started measuring the cost of serving YouTube traffic over
QUIC, we found that QUIC’s server CPU-utilization was about 3.5
times higher than TLS/TCP. The three major sources of QUIC’s CPU
cost were: cryptography, sending and receiving of UDP packets, and
maintaining internal QUIC state. To reduce cryptographic costs, we
employed a hand-optimized version of the ChaCha20 cipher favored
by mobile clients. To reduce packet receive costs, we used asyn-
chronous packet reception from the kernel via a memory-mapped
application ring buffer (Linux’s PACKET_RX_RING). Finally, to
reduce the cost of maintaining state, we rewrote critical paths and
data-structures to be more cache-efficient. With these optimizations,
we decreased the CPU cost of serving web traffic over QUIC to
approximately twice that of TLS/TCP, which has allowed us to in-
crease the levels of QUIC traffic we serve. We believe that while
QUIC will remain more costly than TLS/TCP, further reductions
are possible. Specifically, general kernel bypass [57] seems like a
promising match for a user-space transport.
6.8 Performance Limitations
QUIC’s performance can be limited in certain cases, and we describe
the limitations we are aware of in this section.
10Video Latency trends are similar to Search Latency.
Figure 11: CDF of TCP connections where the server’s maximum con-
gestion window was limited by the client’s maximum receive window.
Data presented is for video playbacks from one week in March 2016.
Data begins at about 16KB, which represents the smallest observed re-
ceive window advertisement.
Loss-Recovery Latency: Figure 9 shows Rebuffer Rate for video
playbacks as a function of the client’s minimum RTT to the video
server. Benefits with QUIC increase with client RTT, which, as
shown in Figure 10, also correspond to increases in network loss.
The video player uses two TCP connections to the server for every
playback fragment. Use of two connections causes TCP’s loss detec-
tion to be slower: data and ACKs on one connection cannot assist
in loss detection on the other connection, and there are two recov-
ery tails, which increases the probability of incurring tail-recovery
latency [7, 21]. Additionally, QUIC’s loss-recovery improvements
described in Section 3.4 appear to increase QUIC’s resiliency to
higher loss rates. As Figure 9 shows, both QUICg’s and TCPg’s
rebuffer rates increase at higher RTTs. However, as this figure also
shows, QUICg’s rebuffer rate increases more slowly than TCPg’s,
implying that QUIC’s loss-recovery mechanisms are more resilient
to greater losses than TCP.
Connection Throughput: Connection throughput is dictated by
a connection’s congestion window, as estimated by the sender’s
congestion controller, and by its receive window, as computed by
the receiver’s flow controller. For a given RTT, the maximum send
rate of a connection is directly limited by the connection’s maximum
achievable congestion and receive windows.
The default initial connection-level flow control limit advertised
by a QUIC client is 15MB, which is large enough to avoid any
bottlenecks due to flow control. Investigation into client-advertised
TCP receive window however paints a different picture: TCP connec-
tions carrying video data can be limited by the client’s receive win-
dow. We investigated all video connections over TCP for a week in
March 2016, specifically looking into connections that were receive-
window-limited—where the congestion window matched the adver-
tised receive window—and the results are shown in Figure 11. These
connections accounted for 4.6% of the connections we examined.
The majority of these constrained connections were limited by a
maximum receive window advertisement of 64 KB, or roughly 45
MTU-sized packets. This window size limits the maximum possible
send rate, constraining the sender when the path has a large RTT and
during loss recovery. We believe that the low advertised maximum
receive window for TCP is likely due to the absence of window
scaling [37], which in turn may be caused by legacy clients that lack
support for it and/or middlebox interference.
Rebuffer rates can be decreased by reducing video quality, but
QUIC playbacks show improved video quality as well as a decrease
192
The QUIC Transport Protocol
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Country
South Korea
USA
India
Mean Min RTT (ms) Mean TCP Rtx % Desktop
1.3
3.4
13.2
38
50
188
1
2
8
% Reduction in Search Latency % Reduction in Rebuffer Rate
Mobile
10.1
12.9
20.2
Mobile Desktop
0.0
4.1
22.1
1.1
2.0
5.5
Table 3: Network characteristics of selected countries and the changes to mean Search Latency and mean Video Rebuffer Rate for users in QUICg.
Pre-warmed connections: When applications hide handshake la-
tency by performing handshakes proactively, these applications re-
ceive no measurable benefit from QUIC’s 0-RTT handshake. This
optimization is not uncommon in applications where the server is
known a priori, and is used by the YouTube app. We note that some
applications, such as web browsers, cannot always pre-warm con-
nections since the server is often unknown until explicitly indicated
by the user.
High bandwidth, low-delay, low-loss networks: The use of QUIC
on networks with plentiful bandwidth, low delay, and low loss rate,
shows little gain and occasionally negative performance impact.
When used over a very high-bandwidth (over 100 Mbps) and/or very
low RTT connection (a few milliseconds), QUIC may perform worse
than TCP. We believe that this limitation is due to client CPU limits
and/or client-side scheduler inefficiencies in the OS or application.
While these ranges are outside typical Internet conditions, we are
actively looking into mitigations in these cases.
Mobile devices: QUIC’s gains for mobile users are generally more
modest than gains for desktop users. As discussed earlier in this
section, this is partially due to the fact that mobile applications are
often fine-tuned for their environment. For example, when applica-
tions limit content for small mobile-screens, transport optimizations
have less impact. Mobile phones are also more CPU-constrained
than desktop devices, causing CPU to be the bottleneck when net-
work bandwidth is plentiful. We are actively working on improving
QUIC’s performance on mobile devices.
7 EXPERIMENTS AND EXPERIENCES
We now share lessons we learned during QUIC’s deployment. Some
of these involved experiments at scale, such as determining QUIC’s
maximum packet size. Others required deploying at scale, such as
detecting the extent and nature of UDP blocking and throttling on the
Internet. A few lessons were learned through failures, exemplified
by our attempt to design and use FEC in QUIC. We also describe
a surprising ecosystem response to QUIC’s deployment: its rapid
ossification by a middlebox vendor.
7.1 Packet Size Considerations
Early in the project, we performed a simple experiment to choose
an appropriate maximum packet size for QUIC. We performed a
wide-scale reachability experiment using Chrome’s experimentation
framework described in Section 4. We tested a range of possible
UDP payload sizes, from 1200 bytes up to 1500 bytes, in 5 byte
increments. For each packet size, approximately 25,000 instances
of Chrome would attempt to send UDP packets of that size to an
echo server on our network and wait for a response. If at least one
Figure 12: Unreachability with various UDP payload sizes. Data col-
lected over 28 days in January 2014.
response was received, this trial counted as a reachability success,
otherwise it was considered to be a failure.
Figure 12 shows the percentage of clients unable to reach our
servers with packets of each tested size. The rapid increase in un-
reachability after 1450 bytes is a result of the total packet size—
QUIC payload combined with UDP and IP headers—exceeding the
1500 byte Ethernet MTU. Based on this data, we chose 1350 bytes
as the default payload size for QUIC. Future work will consider path
MTU discovery for QUIC [45].
7.2 UDP Blockage and Throttling
We used video playback metrics gathered in November 2016 to
measure UDP blocking and throttling in the network. QUIC is suc-
cessfully used for 95.3% of video clients attempting to use QUIC.
4.4% of clients are unable to use QUIC, meaning that QUIC or
UDP is blocked or the path’s MTU is too small. Manual inspection
showed that these users are commonly found in corporate networks,
and are likely behind enterprise firewalls. We have not seen an entire
ISP blocking QUIC or UDP.
The remaining 0.3% of users are in networks that seem to rate
limit QUIC and/or UDP traffic. We detect rate limiting as substan-
tially elevated packet loss rate and decreased bandwidth at peak
times of day, when traffic is high. We manually disable QUIC at our
servers for entire Autonomous Systems (AS) where such throttling is
detected and reach out to the operators running the network, asking
them to either remove or at least raise their limits. Reaching out
to operators has been effective—we saw a reduction in AS-level
throttling from 1% in June 2015 to 0.3% in November 2016—and
we re-enabled QUIC for ASes that removed their throttlers.
7.3 Forward Error Correction
Forward Error Correction (FEC) uses redundancy in the sent data
stream to allow a receiver to recover lost packets without an explicit
retransmission. Based on [21], which showed that single losses are
common, we experimented with XOR-based FEC (simple parity) to
193
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
A. Langley et al.
Figure 13: CDF of fraction of QUIC loss epochs vs. number of packet
losses in the epoch. Data collected over one week in March 2016.
enable recovery from a single packet loss within a group. We used
this simple scheme because it has low computational overhead, it
is relatively simple to implement, and it avoids latency associated
with schemes that require multiple packets to arrive before any can
be processed.
We experimented with various packet protection policies—protecting
only HTTP headers, protecting all data, sending an FEC packet only
on quiescence—and found similar outcomes. While retransmission
rates decreased measurably, FEC had statistically insignificant im-
pact on Search Latency and increased both Video Latency and Video
Rebuffer Rate for video playbacks. Video playback is commonly
bandwidth limited, particularly at startup; sending additional FEC
packets simply adds to the bandwidth pressure. Where FEC reduced
tail latency, we found that aggressively retransmitting at the tail [17]
provided similar benefits.
We also measured the number of packets lost during RTT-long
loss epochs in QUIC to see if and how FEC might help. Our goal
was to determine whether the latency benefits of FEC outweighed
the added bandwidth costs. The resulting Figure 13 shows that the
benefit of using an FEC scheme that recovers from a single packet
loss is limited to under 30% of loss episodes.
In addition to benefits that were not compelling, implementing
FEC introduced a fair amount of code complexity. Consequently, we
removed support for XOR-based FEC from QUIC in early 2016.
7.4 User-space Development
Development practices directly influence robustness of deployed
code. In keeping with modern software development practices, we
relied heavily on extensive unit and end-to-end testing. We used a
network simulator built into the QUIC code to perform fine-grained
congestion control testing. Such facilities, which are often limited
in kernel development environments, frequently caught significant
bugs prior to deployment and live experimentation.
An added benefit of user-space development is that a user-space
application is not as memory-constrained as the kernel, is not limited
by the kernel API, and can freely interact with other systems in the
server infrastructure. This allows for extensive logging and integra-
tion with a rich server logging infrastructure, which is invaluable for
debugging. As an example, recording detailed connection state at
every packet event at the server led us to uncover a decade-old Cubic
quiescence bug [18]. Fixing this bug reduced QUIC’s retransmission
rates by about 30%, QUIC’s CPU utilization by about 17%, and
TCP’s retransmission rates by about 20% [30].
Due to these safeguards and monitoring capabilities, we were able
to iterate rapidly on deployment of QUIC modifications. Figure 14
Figure 14: Incoming QUIC requests to our servers, by QUIC version.
shows versions used by all QUIC clients over the past two years. As
discussed in Section 5, our ability to deploy security fixes to clients
was and remains critically important, perhaps even more so because
QUIC is a secure transport. High deployment velocity allowed us
to experiment with various aspects of QUIC, and if found to not be
useful, deprecate them.
7.5 Experiences with Middleboxes
As explained in Section 3.3, QUIC encrypts most of its packet
header to avoid protocol entrenchment. However a few fields are
left unencrypted, to allow a receiver to look up local connection
state and decrypt incoming packets. In October 2016, we introduced
a 1-bit change to the public flags field of the QUIC packet header.
This change resulted in pathological packet loss for users behind one
brand of firewall that supported explicit blocking of QUIC traffic. In
previous versions of QUIC, this firewall correctly blocked all QUIC
packets, causing clients to fall back to TCP. The firewall used the
QUIC flags field to identify QUIC packets, and the 1-bit change in
the flags field confounded this detection logic, causing the firewall to
allow initial packets through but blocking subsequent packets. The
characteristics of this packet loss defeated the TCP fallback logic
described in Section 3.8. As a result, clients that were previously
using TCP (since QUIC was previously successfully blocked) were
now starting to use QUIC and then hitting a packet black-hole. The
problem was fixed by reverting the flags change across our clients.
We identified the middlebox and reached out to the vendor. The
vendor addressed the issue by updating their classifier to allow the
variations seen in the flags. This fix was rolled out to their customers