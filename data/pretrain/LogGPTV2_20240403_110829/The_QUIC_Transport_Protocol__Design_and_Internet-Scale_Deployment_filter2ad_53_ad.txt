### Retransmission Rates and RTT Buckets
The retransmission rates were derived from video playback data. The graph illustrates the average retransmission rate within 10 ms Round-Trip Time (RTT) buckets. It is important to note that the actual retransmission rate experienced by a connection can be significantly higher [22]. Across all RTTs, the retransmission rates at the 50th, 80th, 90th, and 95th percentiles are 0%, 2%, 8%, and 18%, respectively.

### QUIC Handshake Success Rate
The mobile app's QUIC connections achieve a 68% 0-RTT handshake success rate on average, which is a 20% reduction compared to desktop applications. This discrepancy is attributed to two primary factors: 
1. **Valid Server Configuration**: A successful 0-RTT handshake requires a valid server configuration.
2. **Valid Source Address Token**: The client must have a valid source address token, which is a server-encrypted blob containing the client’s validated IP address. Both of these elements are cached from previous successful handshakes.

### Video Playback Quality
To measure video quality, we considered the fraction of videos played at their optimal rate, defined as the format best suited for the video viewport size and user intent. For playbacks with no rebuffering, the fraction was consistent between QUIC and TCP users. However, for playbacks with non-zero rebuffering, QUIC increased the number of videos played at their optimal rates by 2.9% for desktop and 4.6% for mobile playbacks.

### Regional Performance Variations
Differences in access network quality and proximity to Google servers result in variations in RTT and retransmission rates across different geographical regions. We examined QUIC’s impact on Search Latency and Video Rebuffer Rate in selected countries, chosen to represent a wide range of network conditions.

**Table 3: Network Characteristics and Performance Improvements**

| Country        | Mean Min RTT (ms) | Mean TCP Rtx % | % Reduction in Search Latency | % Reduction in Rebuffer Rate |
|----------------|-------------------|----------------|-------------------------------|------------------------------|
| South Korea    | 1.3               | 38             | 0.0                           | 1.1                          |
| USA            | 3.4               | 50             | 4.1                           | 2.0                          |
| India          | 13.2              | 188            | 22.1                          | 5.5                          |

- **South Korea**: With the lowest average RTT and network loss, QUIC’s performance is closer to that of TCP.
- **USA**: Network conditions are more typical of the global average, and QUIC shows greater improvements compared to South Korea.
- **India**: With the highest average RTT and retransmission rate, QUIC provides the most significant benefits.

### Server CPU Utilization
Initially, the QUIC implementation prioritized rapid feature development and ease of debugging over CPU efficiency. When we began measuring the cost of serving YouTube traffic over QUIC, we found that QUIC’s server CPU utilization was about 3.5 times higher than TLS/TCP. The main contributors to this high CPU usage were:
1. **Cryptography**
2. **Sending and receiving UDP packets**
3. **Maintaining internal QUIC state**

To reduce these costs, we implemented several optimizations:
- **Cryptography**: Employed a hand-optimized version of the ChaCha20 cipher.
- **Packet Reception**: Used asynchronous packet reception via a memory-mapped application ring buffer (Linux’s PACKET_RX_RING).
- **State Maintenance**: Rewrote critical paths and data structures to be more cache-efficient.

These optimizations reduced the CPU cost of serving web traffic over QUIC to approximately twice that of TLS/TCP, allowing us to increase the levels of QUIC traffic we serve. Further reductions are possible, particularly through general kernel bypass [57].

### Performance Limitations
QUIC’s performance can be limited in certain scenarios:

#### Loss-Recovery Latency
Figure 9 shows the Rebuffer Rate for video playbacks as a function of the client’s minimum RTT to the video server. Benefits with QUIC increase with client RTT, corresponding to increases in network loss. The video player uses two TCP connections for each playback fragment, which can slow down TCP’s loss detection. QUIC’s loss-recovery mechanisms, as described in Section 3.4, appear to enhance its resilience to higher loss rates.

#### Connection Throughput
Connection throughput is determined by the congestion window, estimated by the sender’s congestion controller, and the receive window, computed by the receiver’s flow controller. For a given RTT, the maximum send rate is directly limited by the achievable congestion and receive windows. The default initial connection-level flow control limit for QUIC is 15MB, which is sufficient to avoid bottlenecks. However, TCP connections carrying video data can be limited by the client’s receive window. Figure 11 shows that 4.6% of the connections we examined were receive-window-limited, often constrained by a maximum receive window of 64 KB.

### Pre-warmed Connections
When applications proactively perform handshakes to hide latency, they do not benefit from QUIC’s 0-RTT handshake. This optimization is common in applications like the YouTube app, where the server is known in advance. Web browsers, however, cannot always pre-warm connections since the server is often unknown until explicitly indicated by the user.

### High Bandwidth, Low-Delay, Low-Loss Networks
On networks with high bandwidth, low delay, and low loss rates, QUIC shows little gain and occasionally negative performance impact. In very high-bandwidth (over 100 Mbps) or very low RTT connections (a few milliseconds), QUIC may perform worse than TCP due to client CPU limits and/or scheduler inefficiencies. While these conditions are outside typical Internet conditions, we are actively exploring mitigations.

### Mobile Devices
QUIC’s gains for mobile users are generally more modest than for desktop users. This is partly because mobile applications are often fine-tuned for their environment, and mobile devices are more CPU-constrained, leading to CPU becoming the bottleneck when network bandwidth is plentiful. We are actively working on improving QUIC’s performance on mobile devices.

### Experiments and Experiences
We share lessons learned during QUIC’s deployment, including large-scale experiments and real-world experiences.

#### Packet Size Considerations
We performed a reachability experiment to determine an appropriate maximum packet size for QUIC. Testing UDP payload sizes from 1200 to 1500 bytes, we found that unreachability increased rapidly after 1450 bytes due to exceeding the 1500 byte Ethernet MTU. Based on this data, we chose 1350 bytes as the default payload size for QUIC.

#### UDP Blockage and Throttling
Using video playback metrics, we measured UDP blocking and throttling in the network. QUIC is successfully used for 95.3% of video clients attempting to use it. 4.4% of clients are unable to use QUIC, likely due to being behind enterprise firewalls. The remaining 0.3% experience rate limiting, and we manually disable QUIC for affected Autonomous Systems (AS) and work with operators to remove or raise limits.

#### Forward Error Correction (FEC)
We experimented with XOR-based FEC to recover from single packet losses but found that while retransmission rates decreased, FEC had statistically insignificant impact on Search Latency and increased Video Latency and Rebuffer Rate. Additionally, implementing FEC introduced code complexity, leading us to remove support for it in early 2016.

#### User-Space Development
User-space development allowed for extensive unit and end-to-end testing, integration with server logging infrastructure, and rapid iteration on QUIC modifications. This approach uncovered and fixed a decade-old Cubic quiescence bug, reducing retransmission rates and CPU utilization.

#### Middlebox Experiences
A 1-bit change to the public flags field in the QUIC packet header caused issues with a specific brand of firewall, leading to pathological packet loss. We identified the middlebox and worked with the vendor to update their classifier, resolving the issue.