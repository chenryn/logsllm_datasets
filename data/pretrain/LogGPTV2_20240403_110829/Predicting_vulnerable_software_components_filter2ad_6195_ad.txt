two more would still be in the top ten (at predicted ranks
3 and 6), and two more would be in the top twenty (at pre-
dicted ranks 7 and 9).
6.4 Discussion
In the simple cost model introduced in Section 5.2, we
have m = 10, 452 and V (cid:2)
= 424, giving V (cid:2)/m = 0.04. With
p = 0.65, we see that Vulture does more than ﬁfteen times
better than random assignment.
For ranking, all Q values are higher than 0.6; the average
values are way above that. This more than satisﬁes our
criterion from Section 5.3.
Therefore, our case study shows three things. First of
all, allocating quality assurance eﬀorts based on a Vulture
prediction achieves a reasonable balance between eﬀective-
Prediction
Validation set
Component
BRs Actual rank
NsDOMClassInfo
SgridRowLayout
xpcprivate
Jsxml
nsGenericHTMLElement
Jsgc
NsJSEnvironment
Jsfun
NsHTMLLabelElement
NsHttpTransaction
10 # 3.5
1 # 95
7 # 6
11 # 2
6 # 8
10 # 3.5
4 # 12
15 # 1
3 # 18
2 # 35
Rank
# 1
# 2
# 3
# 4
# 5
# 6
# 7
# 8
# 9
# 10
Table 4: The top ten most vulnerable compo-
nents from a validation set, as predicted by Vul-
ture. The column labeled “BRs” shows the number
of vulnerability-related bug reports for that compo-
nent. Eight of the predicted top ten are actually
very vulnerable.
ness and eﬃciency. Second, it is eﬀective because half of all
vulnerable components are actually ﬂagged. And third, Vul-
ture is eﬃcient because directing quality assurance eﬀorts
on ﬂagged components yields a return of 70%—more than
two out of three components are hits. Focusing on the top
ranked components will give even better results.
Furthermore, these numbers show that there is empiri-
cally an undeniable correlation between imports and func-
tion calls on one hand, and vulnerabilities on the other. This
correlation can be proﬁtably exploited by tools like Vulture
to make predictions that are correct often enough so as to
make a diﬀerence when allocating testing eﬀort. Vulture has
also identiﬁed features that very often lead to vulnerabilities
when used together and can so point out areas that should
perhaps be redesigned in a more secure way.
Best of all, Vulture has done all this automatically, quickly,
and without the need to resort to intuition or human exper-
tise. This gives programmers and managers much-needed
objective data when it comes to identify (a) where past
vulnerabilities were located, (b) other components that are
likely to be vulnerable, and (c) eﬀectively allocating quality
assurance eﬀort.
7. RELATED WORK
Previous work in this area reduced the number of vulner-
abilities or their impact by one of the following methods:
Looking at components’ histories. The Vulture tool
was inspired by the pilot study by Schr¨oter et al. [29], who
ﬁrst observed that imports correlate with failures. While
Schr¨oter et al. examined general defects, the present work
focuses speciﬁcally on vulnerabilities. To our knowledge, this
is the ﬁrst work that speciﬁcally mines and leverages vulner-
ability databases to make predictions. Also, our correlation,
precision and recall values are higher than theirs, which is
why we believe that focusing on vulnerabilities instead of on
bugs in general is worthwhile.
Evolution of defect numbers. Both Ozment et al. [23]
as well as Li et al. [17] have studied how the numbers of
defects and security issues evolve over time. Ozment et al.
report a decrease in the rate at which new vulnerabilities
are reported, while Li et al. report an increase. Neither
of the two approaches allow mapping of vulnerabilities to
components or prediction.
Estimating the number of vulnerabilities. Alhazmi et
al. use the rate at which vulnerabilities are discovered to
build models to predict the number of as yet undiscovered
vulnerabilities [2]. They use their approach on entire sys-
tems, however, and not on source ﬁles. Also, in contrast to
Vulture, their predictions depend on a model of how vulner-
abilities are discovered.
Miller et al. build formulas that estimate the number of
defects in software, even when testing reveals no ﬂaws [20].
Their formulas incorporate random testing results, informa-
tion about the input distribution, and prior assumptions
about the probability of failure of the software. However,
they do not take into account the software’s history—their
estimates do not change, no matter how large the history is.
Tofts et al. build simple dynamic models of security ﬂaws
by regarding security as a stochastic process [35], but they
do not make speciﬁc predictions about vulnerable software
components. Yin et al. [39] highlight the need for a frame-
work for estimating the security risks in large software sys-
tems, but give neither an implementation nor an evaluation.
Testing the binary. By this we mean subjecting the bi-
nary executable—not the source code—of the program in
question to various forms of testing and analysis (and then
reporting any security leaks to the vendor). This is often
done with techniques like fuzz testing [19] and fault injec-
tion; see the book by Voas and McGraw [38].
Eric Rescorla argues that ﬁnding and patching security
holes does not lead to an improvement in software qual-
ity [25]. But he is talking about ﬁnding security holes by
third-party outsiders in the ﬁnished product and not about
ﬁnding them by in-house personnel during the development
cycle. Therefore, his conclusions do not contradict our belief
that Vulture is a useful tool.
(Statically) examining the source. This is usually done
with an eye towards speciﬁc vulnerabilities, such as buﬀer
overﬂows. Approaches include linear programming [12], data-
ﬂow analysis [14], locating functions near a program’s in-
put [8]7, axiomatizing correct pointer usage and then check-
ing against that axiomatization [11], exploiting semantic
comments [16], checking path conditions [31], symbolic poin-
ter checking [28], or symbolic bounds checking [26].
Rather than describing the diﬀerences between these tools
and ours in every case, we we brieﬂy discuss ITS4, developed
by Viega et al. [37], and representative of the many other
static code scanners. Viega et al.’s requirement was to have
a tool that is fast enough to be used as real-time feedback
during the development process, and precise enough so that
programmers would not ignore it. Since their approach is
essentially pattern-based, it will have to be manually ex-
tended as new patterns emerge. The person extending it
will have to have a concept of the vulnerability before it
can be condensed into a pattern. Vulture will probably not
ﬂag components that contain vulnerabilities that were un-
known at training time, but it will ﬂag components that
7The hypothesis of DeCast et al. that vulnerabilities occur
more in functions that are close to a program’s input is not
supported by the present study. Many of Mozilla’s vulner-
able components, such as nsGlobalWindow, lie in the heart
of the application.
contain vulnerabilities that have been ﬁxed before but have
no name.
Also, since ITS4 checks local properties, it will be very
diﬃcult for it to ﬁnd security-related defects that arise from
the interaction between far-away components, that is, com-
ponents that are connected through long chains of def-use
relations. Additionally, ITS4, as it exists now, will be un-
able to adapt to programs that for some reason contain a
number of pattern-violating but safe practices, because it
completely ignores a component’s history.
Another approach is to use model checking [3, 4]. In this
approach, speciﬁc classes of vulnerabilities are formalized
and the program model-checked for violations of these for-
malized properties. The advantage over other formal meth-
ods is that if a failure is detected, the model checker comes
up with a concrete counter-example that can be used as a
regression test case. This too is a useful tool, but like ITS4,
it will have to be extended as new formalizations emerge.
Some vulnerability types might not even be formalizable.
Vulture also contains static scanners—it detects features
by parsing the source code in a very simple manner. How-
ever, Vulture’s aim is not to declare that certain lines in a
program might contain a buﬀer overﬂow, but rather to direct
testing eﬀort where it is most needed by giving a probabilistic
assessment of the code’s vulnerability.
Hardening the source or runtime environment. This
encompasses all measures that are taken to mitigate a pro-
gram’s ability to do damage Hardening a program or the
runtime environment is useful when software is already de-
ployed. StackGuard is a method that is representative of the
many tools that exist to lower a vulnerability’s impact [6].
Others include mandatory access controls as found in App-
Armor [5] or SELinux [22]. However, Vulture works on the
other side of the deployment divide and tries to direct pro-
grammers and managers to pieces of code requiring their
attention, in the hope that StackGuard and similar systems
will not be needed.
8. CONCLUSIONS AND FUTURE WORK
We have presented empirical evidence that features corre-
late with vulnerabilities. Based on this empirical evidence,
we have introduced Vulture, a new tool that predicts vul-
nerable components by looking at their features. It is fast
and reasonably accurate: it analyzes a project as complex as
Mozilla in about half an hour, and correctly identiﬁes half
of the vulnerable components. Two thirds of its predictions
are correct.
The contributions of the present paper are as follows:
1. A technique for mapping past vulnerabilities by min-
ing and combining vulnerability databases with version
archives.
2. Empirical evidence that contradicts popular wisdom
saying that vulnerable components will generally have
more vulnerabilities in the future.
3. Evidence that features correlate with vulnerabilities.
4. A tool that learns from the locations of past vulnera-
bilities to predict future ones with reasonable accuracy.
5. An approach for identifying vulnerabilities that auto-
matically adapts to speciﬁc projects and products.
Methods predicted as 
vulnerable are marked
Methods predicted as 
most vulnerable 
List of dangerous 
import combinations
Figure 10: Sketch of a Vulture integration into Eclipse. Vulture annotates methods predicted as vulnerable
with red bars. The view “Predictions” lists the methods predicted as most vulnerable. With the view
“Dangerous Imports”, a developer can explore import combinations that lead to past vulnerabilities.
6. A predictor for vulnerabilities that only needs a set of
suitable features, and thus can be applied before the
component is fully implemented.
Despite these contributions, we feel that our work has just
scratched the surface of what is possible, and of what is
needed. Our future work will concentrate on these topics:
Characterizing domains. We have seen that empirically,
features are good predictors for vulnerabilities. We believe
that this is so because features characterize a component’s
domain, that is, the type of service that it uses or imple-
ments, and it is really the domain that determines a com-
ponent’s vulnerability. We plan to test this hypothesis by
studies across multiple systems in similar domains.
Fine-grained approaches. Rather than just examining
features at the component level, one may go for more ﬁne-
grained approaches, such as caller-callee relationships. Such
ﬁne-grained relationships may also allow vulnerability pre-
dictions for classes or even methods or functions.
Evolved components. This work primarily applies to pre-
dicting vulnerabilities of new components. However, com-
ponents that already are used in production code come with
their own vulnerability history. We expect this history to
rank among the best predictors for future vulnerabilities.
Usability. Right now, Vulture is essentially a batch pro-
gram producing a textual output that can be processed by
spreadsheet programs or statistical packages. We plan to
integrate Vulture into current development environments,
allowing programmers to query for vulnerable components.
Such environments could also visualize vulnerabilities by
placing indicators next to the entities (Figure 10).
In a recent blog, Bruce Schneier wrote, “If the IT products
we purchased were secure out of the box, we wouldn’t have to
spend billions every year making them secure.” [27] One ﬁrst
step to improve security is to learn where and why current
software had ﬂaws in the past. Our approach provides es-
sential ground data for this purpose, and allows for eﬀective
predictions where software should be secured in the future.
Acknowledgments
We thank the anonymous reviewers for their helpful com-
ments. We also thank the Mozilla team for making their
databases available. David Schuler and Andrzej Wasylkowski
provided valuable feedback on earlier revisions of this paper.
Thomas Zimmermann is funded by a stipend from the DFG-
Graduiertenkolleg “Leistungsgarantien f¨ur Rechnersysteme”.
Vulture is part of the “Mining software archives” project
at Saarland University. For more information, see
http://www.st.cs.uni-sb.de/softevo/
9. REFERENCES
[1] Rakesh Agrawal and Ramakrishnan Srikant. Fast
algorithms for mining association rules. In Jorge B. Bocca,
Matthias Jarke, and Carlo Zaniolo, editors, Proc. 20th Int’l
Conf. on Very Large Data Bases, VLDB, pages 487–499.
Morgan Kaufmann, September 1994.
[2] Omar Alhazmi, Yashwant Malaiya, and Indrajit Ray.
Security Vulnerabilities in Software Systems: A
Quantitative Perspective, volume 3645/2005 of Lecture
Notes in Computer Science, pages 281–294. Springer
Verlag, Berlin, Heidelberg, August 2005.
[3] Hao Chen, Drew Dean, and David Wagner. Model checking
one million lines of C code. In Proc. 11th Annual Network
and Distributed System Security Symposium (NDSS),
pages 171–185, February 2004.
[4] Hao Chen and David Wagner. MOPS: An infrastructure
for examining security properties of software. In Proc. 9th
ACM Conf. on Computer and Communications Security
(CCS), pages 235–244, November 2002.
[5] Crispin Cowan. Apparmor linux application security.
http://www.novell.com/linux/security/apparmor/,
January 2007.
[6] Crispin Cowan, Calton Pu, Dave Maier, Jonathan Walpole,
Peat Bakke, Steve Beattie, Aaron Grier, Perry Wagle, Qian
Zhang, and Heather Hinton. StackGuard: Automatic
adaptive detection and prevention of buﬀer-overﬂow
attacks. In Proc. 7th USENIX Security Conf., pages 63–78,
San Antonio, Texas, January 1998.
[20] K.W. Miller, L.J. Morell, R.E. Noonan, S.K. Park, D.M.
Nicol, B.W. Murrill, and M. Voas. Estimating the
probability of failure when testing reveals no failures. IEEE
Transactions on Software Engineering, 18(1):33–43,
January 1992.
[21] Nachiappan Nagappan, Thomas Ball, and Andreas Zeller.
Mining metrics to predict component failures. In Proc.
29th Int’l Conf. on Software Engineering. ACM Press,
November 2005.
[22] National Security Agency. Security-enhanced linux.
http://www.nsa.gov/selinux/, January 2007.
[23] Andy Ozment and Stuart E. Schechter. Milk or wine: Does
software security improve with age? In Proc. 15th Usenix
Security Symposium, pages 93–104, August 2006.
[24] R Development Core Team. R: A Language and
Environment for Statistical Computing. R Foundation for
Statistical Computing, Vienna, Austria, 2006. ISBN
3-900051-07-0.
[25] Eric Rescorla. Is ﬁnding security holes a good idea? IEEE
Security and Privacy, 3(1):14–19, 2005.
[7] Davor Cubranic, Gail C. Murphy, Janice Singer, and
[26] Radu Rugina and Martin Rinard. Symbolic bounds
Kellogg S. Booth. Hipikat: A project memory for software
development. IEEE Transactions on Software Engineering,
31(6):446–465, June 2005.
[8] Dan DaCosta, Christopher Dahn, Spiros Mancoridis, and
Vassilis Prevelakis. Characterizing the security
vulnerability likelihood of software functions. In IEEE
Proc. 2003 Int’l Conf. on Software Maintenance
(ICSM’03), September 2003.
analysis of pointers, array indices, and accessed memory
regions. In Proc. ACM SIGPLAN ’00 conference on
Programming language design and implementation, pages
182–195. ACM Press, 2000.
[27] Bruce Schneier. Do we really need a security industry?
Wired, May 2007.
http://www.wired.com/politics/security/commentary/
securitymatters/2007/%05/securitymatters_0503.
[9] Evgenia Dimitriadou, Kurt Hornik, Friedrich Leisch, David
[28] Berhard Scholz, Johann Blieberger, and Thomas Fahringer.
Meyer, and Andreas Weingessel. e1071: Misc Functions
Department of Statistics (e1071), TU Wien, 2006. R
package version 1.5-13.
[10] Michael Fischer, Martin Pinzger, and Harald Gall.
Populating a release history database from version control
and bug tracking systems. In Proc. Int’l Conf. on Software
Maint e nance  (ICSM’03), Amsterdam, Netherlands,
September 2003. IEEE.
[11] Pascal Fradet, Ronan Caugne, and Daniel Le M´etayer.
Static detection of pointer errors: An axiomatisation and a
checking algorithm. In European Symposium on
Programming, pages 125–140, 1996.
[12] Vinod Ganapathy, Somesh Jha, David Chandler, David
Melski, and David Vitek. Buﬀer overrun detection using
linear programming and static analysis. In 10th ACM
Conf. on Computer and Communications Security (CCS),
October 2003.
Symbolic pointer analysis for detecting memory leaks. In
Proc. 2000 ACM SIGPLAN workshop on Partial
evaluation and semantics-based program manipulation,
pages 104–113. ACM Press, 1999.
[29] Adrian Schr¨oter, Thomas Zimmermann, and Andreas
Zeller. Predicting component failures at design time. In
Proc. 5th Int’l Symposium on Empirical Software
Engineering, pages 18–27, New York, NY, USA, September
2006.
[30] Jacek ´Sliwerski, Thomas Zimmermann, and Andreas Zeller.
When do changes induce ﬁxes? In Proc. Second Int’l
Workshop on Mining Software Repositories, pages 24–28,
May 2005.
[31] Gregor Snelting, Torsten Robschink, and Jens Krinke.
Eﬃcient path conditions in dependence graphs for software
safety analysis. In Proc. 24th Int’l Conf. on Software
Engineering, New York, NY, USA, May 2002. ACM Press.
[13] Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
[32] The Mozilla Foundation. Bugzilla.
The Elements of Statistical Learning: Data Mining,
Inference, and Prediction. Springer Series in Statistics.
Springer Verlag, 2001.
[14] Nenad Jovanovic, Christopher Kruegel, and Engin Kirda.
Pixy: A static analysis tool for detecting web application
vulnerabilities (short paper). In IEEE Symposium on
Security and Privacy. May 2006.
[15] Roger Koenker and Pin Ng. SparseM: Sparse Linear
Algebra. R package version 0.73.
http://www.bugzilla.org, January 2007.
[33] The Mozilla Foundation. Mozilla foundation security
advisories. http://www.mozilla.org/projects/security/
known-vulnerabilities.html, January 2007.
[34] The Mozilla Foundation. Mozilla project website.
http://www.mozilla.org/, January 2007.
[35] Chris Tofts and Brian Monahan. Towards an analytic
model of security ﬂaws. Technical Report 2004-224, HP
Trusted Systems Laboratory, Bristol, UK, December 2004.
[16] David Larochelle and David Evans. Statically detecting
[36] Vladimir Naumovich Vapnik. The Nature of Statistical
likely buﬀer overﬂow vulnerabilities. In 10th USENIX
Security Symposium, pages 177–190, August 2001.
[17] Zhenmin Li, Lin Tan, Xuanhui Wang, Shan Lu, Yuanyuan
Zhou, and Chengxiang Zhai. Have things changed now? An
empirical study of bug characteristics in modern open
source software. In Proc. Workshop on Architectural and
System Support for Improving Software Dependability
2006, pages 25–33. ACM Press, October 2006.
[18] Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo.
Eﬃcient algorithms for discovering association rules. In
Knowledge Discovery in Databases: Papers from the 1994
AAAI Workshop, pages 181–192, 1994.
[19] Barton P. Miller, Lars Fredriksen, and Bryan So. An
empirical study reliability of UNIX utilities.
Communications , 33(12):32–44, 1990.
Learning Theory. Springer Verlag, Berlin, 1995.
[37] John Viega, J. T. Bloch, Tadayoshi Kohno, and Gary
McGraw. Token-based scanning of source code for security
problems. ACM Transaction on Information and System
Security, 5(3):238–261, 2002.
[38] Jeﬀrey Voas and Gary McGraw. Software Fault Injection:
Innoculating Programs Against Errors. John Wiley & Sons,
1997.
[39] Jian Yin, Chunqiang Tang, Xiaolan Zhang, and Michael
McIntosh. On estimating the security risks of composite
software services. In Proc. PASSWORD Workshop, June
2006.