title:Exploring the Long Tail of (Malicious) Software Downloads
author:Babak Rahbarinia and
Marco Balduzzi and
Roberto Perdisci
Exploring the Long Tail of 
(Malicious) Software Downloads
Prof. Babak Rahbarinia, Auburn University at Montgomery 
Dr. Marco Balduzzi, Forward-Looking Threat Research Team, Trend Micro 
Prof. Roberto Perdisci, University of Georgia
Contents
4
Introduction
6
Data Collection and Labeling
10
Dataset Overview
13
Analysis of Software 
Download Events
22
Downloading Processes and 
Machines
30
Exploring and Labeling 
Unknown Files
38
Discussion
41
Related Work
43
Conclusions
44
Acknowledgments
Prof. Babak Rahbarinia 
Auburn University 
Montgomery, AL 36117, USA 
PI:EMAIL
Dr. Marco Balduzzi 
Forward-looking Threat Research (FTR) 
Team Trend Micro Inc. 
PI:EMAIL
Prof. Roberto Perdisci 
University of Georgia 
Athens, GA 30602, USA 
PI:EMAIL
TREND MICRO LEGAL DISCLAIMERThe information provided herein is for general information and educational purposes only. It is not intended and should not be construed to constitute legal advice. The information contained herein may not be applicable to all situations and may not reflect the most current situation. Nothing contained herein should be relied on or acted upon without the benefit of legal advice based on the particular facts and circumstances presented and nothing herein should be construed otherwise. Trend Micro reserves the right to modify the contents of this document at any time without prior notice.Translations of any material into other languages are intended solely as a convenience. Translation accuracy is not guaranteed nor implied. If any questions arise related to the accuracy of a translation, please refer to the original language official version of the document. Any discrepancies or differences created in the translation are not binding and have no legal effect for compliance or enforcement purposes.Although Trend Micro uses reasonable efforts to include accurate and up-to-date information herein, Trend Micro makes no warranties or representations of any kind as to its accuracy, currency, or completeness. You agree that access to and use of and reliance on this document and the content thereof is at your own risk. Trend Micro disclaims all warranties of any kind, express or implied. Neither Trend Micro nor any party involved in creating, producing, or delivering this document shall be liable for any consequence, loss, or damage, including direct, indirect, special, consequential, loss of business profits, or special damages, whatsoever arising out of access to, use of, or inability to use, or in connection with the use of this document, or any errors or omissions in the content thereof. Use of this information constitutes acceptance for use in an “as is” condition.Abstract
In this paper, we present a large-scale study of global trends in software 
download events, with an analysis of both benign and malicious downloads 
and  a  categorization  of  events  for  which  no  ground  truth  is  currently 
available. Our measurement study is based on a unique, real-world dataset 
collected at Trend Micro containing more than 3 million in-the-wild web-
based  software  download  events  involving  hundreds  of  thousands  of 
internet machines over a period of seven months. 
Somewhat surprisingly, we found that despite our best efforts and the use 
of  multiple  sources  of  ground  truth,  more  than  83%  of  all  downloaded 
software  files  remain  unknown,  i.e.  cannot  be  classified  as  benign  or 
malicious, even two years after they were first observed. If we consider the 
number of machines that have downloaded at least one unknown file, we 
find that more than 69% of the entire machine/user population downloaded 
one  or  more  unknown  software  file.  Because  the  accuracy  of  malware 
detection systems reported in the academic literature is typically assessed 
only over software files that can be labeled, our findings raise concerns on 
their  actual  effectiveness  in  large-scale  real-world  deployments  and  their 
ability to defend the majority of internet machines from infection.
To  better  understand  what  these  unknown  software  files  might  be,  we 
perform a detailed analysis of their properties. We then explore whether it 
is possible to extend the labeling of software downloads by building a rule-
based system that automatically learns from the available ground truth, and 
can be used to identify many more benign and malicious files with very high 
confidence. This allows us to greatly expand the number of software files 
that can be labeled with high confidence, thus providing results that can 
benefit the evaluation of future malware detection systems.
1. Introduction
Most modern malware infections are caused by web-driven software download events, such as infections 
via  drive-by  exploits  [6]  or  social  engineering  attacks  [11].  In  response  to  the  growth  of  infections  via 
software downloads, the security community has conducted a wealth of research, the majority of which 
is  dedicated  to  detection  and  remediation  efforts  [2,  7,  14-16,  20].  Some  recent  studies  focused  on 
measuring specific infection vectors. For instance, Caballero et al. [1] studied the business infrastructure 
of malware distribution networks, while Rossow et al. [17] and Kwon et al. [10] focused their attention 
towards  malware  droppers  and  provided  detailed  measurements  to  better  understand  how  dropper-
driven infections work.
In this paper, we aim to provide a broader, large-scale study of global trends in software download events, 
with an analysis of both benign and malicious downloads, and a categorization of events for which no 
ground truth is currently available. Our measurement study is based on a unique, real-world dataset we 
obtained  from  Trend  Micro  –  a  leading  anti-malware  vendor  (which  we  refer  to  as  AMV).  This  dataset 
contains  detailed  (anonymized)  information  about  3  million  in-the-wild  web-based  software  download 
events involving over a million internet machines, collected over a period of seven months. Each download 
event includes information such as a unique (anonymous) global machine identifier, detailed information 
about  the  downloaded  file,  the  process  that  initiated  the  download,  and  the  URL  from  which  the  file 
was downloaded. To label benign and malicious software download events and study their properties, 
we make use of multiple sources of ground truth, including information from VirusTotal.com and AMV’s 
private  resources.  This  ground  truth  was  collected  over  several  months,  both  at  a  time  close  to  the 
software download events as well as many months after the collection of our dataset, to account for the 
time typically needed by anti-malware vendors to develop new malware signatures.
Somewhat  surprisingly,  we  found  that  despite  our  best  efforts,  we  were  only  able  to  label  less  than 
17%  of  the  1,791,803  software  files  contained  in  our  dataset.  In  other  words,  more  than  83%  of  all 
downloads remain unknown, even two years after they were first observed. Most of these files have very 
low prevalence. Namely, when considered independently from one another, each file is downloaded by 
only one (or few) machines overall. Therefore, one may think that these files are uninteresting, and the fact 
that they remain unknown is understandable since they would impact a negligible number of machines 
if they were malicious. However, if we consider the number of machines that have downloaded at least 
4 | Exploring the Long Tail of (Malicious) Software Downloads
one unknown file, we find that more than 69% of the entire machine population downloaded one or more 
unknown software file(s) during our observation period. This result is significant, in that it highlights a major 
challenge faced by the malware research community. In fact, most malware detection and classification 
systems proposed in the scientific literature are naturally evaluated only on samples (i.e., executable files) 
for which ground truth is available. Unfortunately, because the accuracy of these systems can only be 
assessed over a small minority of in-the-wild software downloads, this raises concerns on their actual 
effectiveness in large-scale real-world deployments, and on their ability to defend the majority of internet 
machines from infection.
To better understand what these unknown software files may look like, we performed a detailed analysis 
of their properties. We then explored whether it is possible to extend the labeling of software downloads 
by building a rule-based system that automatically learns from the available ground truth. Specifically, 
we aim to generate human-readable classification rules that can accurately identify benign and malicious 
software  using  a  combination  of  simple  features,  while  keeping  the  false  positive  rate  to  a  low  target 
rate  of  0.1%,  which  is  a  common  threshold  in  the  anti-malware  industry.  For  instance,  we  show  that 
features  such  as  software  signing  information  can  be  leveraged  to  improve  file  labeling.  In  particular, 
unlike studies that focus primarily on potentially unwanted programs [8, 9, 19], we show that software 
signing information is present in other types of malware, contrast them with signed benign programs, and 
leverage this information for labeling purposes. These automatically extracted rules allow us to increase 
the number of samples labeled by 233% (a 2.3x increase) with high confidence, compared to the available 
ground truth. Furthermore, each newly labeled sample can be traced back to the human-readable rule 
that assigned the label, thus providing a way for analysts to interpret and verify the results. By providing a 
way to expand the labeling of software files significantly, our rule-based system can benefit the evaluation 
of future malware detection systems.
In summary, our paper makes the following contributions:
•  We explore trends in the software downloads collected in-the-wild from over a million machines from a 
leading anti-malware provider and study the proprieties of benign, malicious, and unknown software. 
•  We report on the importance of considering low prevalence files, which in aggregate are run by almost 
70% of the monitored machines and whose true nature tends to remain unknown to AV vendors even 
two years after they were first observed.
•  We  present  a  novel  rule-based  classification  system  that  learns  human-readable  file  classification 
rules from easily measured features, such as the process used to download a file and the software file 
signer. We then show that this system can be used to significantly increase the number of software 
files  that  can  be  labeled,  compared  to  the  available  ground  truth,  thus  providing  results  that  can 
benefit the evaluation of future malware detection systems.
5 | Exploring the Long Tail of (Malicious) Software Downloads
2. Data Collection and Labeling
2.1. Software Download Events
To collect in-the-wild software download events, we monitor more than a million machines of a well-known 
leading  anti-malware  vendor  (we  only  monitor  download  events  from  customers  who  have  approved 
sharing this information with AMV). Each customer machine runs a monitoring software agent (SA), which 
is responsible for identifying web-based software downloads and reporting these events to a centralized 
data collection server (CS). Each download event is represented by a 5-tuple, (f, m, p, u, t), where f is 
the downloaded file, m is the machine that downloaded f, p is the process on the machine that initiated 
the download, u is the download URL, and t is a timestamp. The downloaded files and client processes 
are  uniquely  identified  by  their  respective  file  hash,  whereas  the  machines  are  uniquely  identified  by 
an  anonymized  global  unique  ID  (generated  by  AMV’s  software  agent  installation).  We  also  have  the 
(anonymized) path on disk — including file names — of every download process and downloaded file.
While each SA captures all web-based download events observed on the system, only events considered 
of interest are reported to the CS for efficiency reasons. Specifically, our dataset contains only software 
download events that satisfy the following conditions: 
•  The  newly  downloaded  file  is  executed  on  the  user’s  machine.  Namely,  software  files  that  are 
downloaded from the web but remain “inactive” (i.e., not executed on the system) are not reported.
•  The  current  prevalence  of  the  downloaded  file  is  below  a  predefined  threshold,  σ.  For  instance, 
consider a newly downloaded software file f observed by a monitored machine m at time t. This new 
event is reported by m to the CS only if the number of distinct machines that downloaded the same 
file (as determined based on its hash) before time t is less than σ.
•  The URL from which the file is downloaded is not whitelisted. For instance, software updates from 
Microsoft or other major software vendors were not collected. 
Overall,  the  rules  described  above  aim  to  reduce  the  system-overhead  and  bandwidth  consumption 
needed to transfer the download events from millions of monitoring agents to the collection server.
6 | Exploring the Long Tail of (Malicious) Software Downloads
During our data collection period, σ was set to 20. Each file could be reported up to 20 times if it occurred 
in up to 20 different download events. It is possible that a file will reach a true prevalence higher than 20, 
though this will not be reflected in the dataset we analyze. At the same time, if the final prevalence of a file 
(i.e., at the end of the collection period) is less than 20, this means that the file was actually downloaded 
by less than 20 of the monitored machines, as reported in our measurements. Of all the files we observed, 
we found that 99.75% have a prevalence of less than 20. Namely, our prevalence measurements were 
capped at 20 for only less than 0.25% of all the downloaded files we observed (see Section 4.1 for more 
details).
2.2. File Labeling
For every software file, we gather related ground truth using multiple sources. Specifically, we used a large 
commercial whitelist and NIST’s software reference library1 to label benign software files. Note that this 
information is gathered from both downloaded files and downloading processes. We also make use of 
VirusTotal.com (VT). Specifically, given a software file f, we query VT both close to the time of download 
and then again almost two years after the data collection. We let this large amount of time pass before 
re-querying VT, to give plenty of time for VT to collect and process (via crowdsourced submissions) files 
that we observed, and for anti-virus applications to develop new detection signatures.
We label a file as benign if it matches our whitelists or if all anti-virus engines (AV) on VT still classify the file 
as benign, even after almost two years from collection. We label a file as likely benign if it is classified as 
benign by VT but the time difference between first and last scans is less than 14 days. To label malicious 
files, we adopted the following approach. Of the more than 50 anti-virus (AV) engines on VT, we consider 
two groups: a group of “trusted” AVs that includes ten of the most popular AV vendors (i.e., Symantec, 
McAfee,  Microsoft,  Trend  Micro,  etc.),  and  a  group  containing  all  other  available  AVs,  which  tend  to 
produce somewhat less reliable detection results. Then we label a file as malicious if at least one of the 
ten “trusted” AVs assigns it an AV label. On the other hand, if none of the ten “trusted” AV vendors assigns 
an AV label to the file but at least one of the remaining less popular AVs detects the file as malicious, we 
assign a likely malicious label. The downloading processes are also labeled similarly. Files (processes) 
for which no ground truth can be found were labeled as unknown. For every file, including unknown files, 
we obtain additional details, such as their file size, their prevalence across all machines of AMV, if the file 
carries a valid software signature, if it is packed and with what packer, etc.
To label the URLs from which files are downloaded, we use AMV’s internal URL whitelists and blacklists, the 
list of most popular domains according to Alexa.com, and Google Safe Browsing (GSB) [5]. Specifically, to 
label a URL as benign, we maintain a list of domains that consistently appeared in the top one million Alexa 
sites for about a year. To further mitigate possible noise in the Alexa list, we consult multiple whitelists and 
1 http://www.nsrl.nist.gov
7 | Exploring the Long Tail of (Malicious) Software Downloads
adjust the labels as follows. If the effective second-level domain (e2LD) of a URL appears in the Alexa.
com list and the URL also matches our private curated whitelist (provided by Trend Micro), the URL will 
be labeled as benign. On the other hand, a URL will be labeled as malicious if it matches GSB and our 
private URL blacklist.
2.3. Malicious File Types
To shed light on the type of malware were involved with the software download events we observed, we 
attempt to group known malicious files into types. To this end, for each malicious file we use multiple AV 
labels to derive their behavior type (e.g., fakeAV, ransomware, dropper, etc.) and their family (e.g., Zbot, 
CryptoLocker, etc.). While we acknowledge that AV labels are often noisy and sometimes inconsistent, we 
use a best effort approach, similar to previous work [12, 18]. For instance, to derive the family labels from 
AV labels, we simply use a recently proposed system called AVclass [18]. As we are not aware of any 
similar tool that can derive the behavior type, we developed the labeling scheme described below, which 
is based on AV label mappings provided by Trend Micro and our own empirical experience.
To  determine  the  behavior  type  (or  simply  type,  for  brevity)  of  a  malicious  file,  we  consider  the  AV 
labels assigned to the file by a subset of five leading AV engines2, for which we have obtained a “label 
interpretation map” provided by Trend Micro (ref. Table 2). By leveraging this map, we identified a set of 
behavior type keywords used by these leading AVs, such as fake-av, ransomware, bot, etc. For instance, 
an AV label such as TROJ_FAKEAV.SMU1 assigned by Trend Micro indicates a fake-av malware type. 
However, because different AVs may disagree on the label to be assigned to a specific malicious file, we 
designed a set of simple rules to resolve such conflicts:
1.  Voting: Given a malicious file f, we first map each label into its respective type. We then assign to f 
the type label with the highest count. In case of two or more type labels receive an equal number of 
votes, we break the tie using the second rule. 
2.  Specificity: If among the types considered for a malicious file, there is one type that is more “specific” 
than the rest, that specific type is assigned. For example, if AV labels for a file report conflicting types, 
such as banker and trojan, we will select banker as the final label because it identifies a more specific 
type keyword than trojan (notice that AV engines often use trojan or generic to flag malicious files with 
an unknown behavior/class). 
In some rare cases where these two rules still cannot be used to resolve a conflict, we derive the final type 
via manual analysis.
As an example of the results given by rule 1), consider a malicious file with four AV labels (i.e., one out of 
the five leading AVs we consider for type labeling did not report the file as being malicious): 
2 Microsoft, Symantec, Trend Micro, Kaspersky, and McAfee
8 | Exploring the Long Tail of (Malicious) Software Downloads
Symantec=Trojan.Zbot, McAfee=Downloader-FYH!6C7411D1C043, Kaspersky=Trojan-Spy.
Win32.Zbot.ruxa, and Microsoft=PWS:Win32/Zbot. The type banker can be derived from three of the 
AV labels (Zbot is programmed to steal banking information3), while McAfee’s AV label indicates a dropper 
(i.e., Downloader is mapped to the dropper behavior type). In this case, the final type we assign will be 
banker. Now consider an example of rule 2) where the following AV labels are assigned to a malicious 
file: Kaspersky=Trojan-Downloader.Win32.Agent.heqj and McAfee=Artemis!DEC3771868CB. 
In this case, Kaspersky’s label indicates a dropper behavior, while McAfee’s label is a generic one (Artemis 
refers to a heuristics-based detection approach). Since dropper indicates a more specific behavior, we 
assign it as the final type.
For  44%  of  all  malicious  downloaded  files  and  client  processes,  we  were  able  to  assign  a  type  label 
without encountering any conflicts (i.e., the AVs fully agreed on the type). In about 28% of cases, the type 
label was assigned using the Voting rule, whereas the Specificity rule was applied in 23% or the cases. In 
the remaining 5% of the cases, the type label was resolved via manual analysis. To foster reproducibility 
of these results, we provide our malicious type extractor tool as an open source tool at gitlab.com/
pub-open/AVType.
3 https://www.symantec.com/security_response/writeup.jsp?docid=2010-011016-3514-99
9 | Exploring the Long Tail of (Malicious) Software Downloads
3. Dataset Overview
In  this  section,  we  provide  an  overview  of  our  dataset,  including  the  exact  number  of  machines  we 
monitored during the data collection period, the number of software download events we observed, how 
many of these events we were able to label, the malware types and families included in the dataset, etc. 
More detailed measurements are provided in Sections 4 and 5.
Our observation period spans seven months, from January 2014 to August 2014. During this time, we 
observed 3,073,863 software download events triggered by 1,139,183 machines. The software files were 
downloaded  from  1,629,336  distinct  URLs,  across  96,862  different  domain  names.  Out  of  1,791,803 
downloaded files, we labeled 9.9% as malicious and 2.3% as benign. We also labeled 4.8% as either 
likely benign or malicious. Note that although some ground truth is available for likely benign and likely 
malicious files, we excluded them from the rest of our study due to our lack of confidence in determining 
if they are truly benign or malicious, and the possibility that they introduce noise into results. 
The remaining 83% of downloaded files were unknown, i.e., no ground truth exists for them. The software 
download events were initiated by 141,229 different download processes (identified by their hash). Of 
these processes, 18.5% were labeled as malicious and 7.6% as benign.
Month
# of 
Machines
# of 
Download 
Events
Download Processes
Total
Benign
Likely 
Benign
Malicious
Likely 
Malicious
January
292,516
578,510
27,265
15.8%
February
246,481
470,291
25,001
15.4%
March
248,568
493,487
25,497
15.7%
April
May
June
July
215,693
427,110
23,078
16.3%
180,947
351,271
20,071
17.3%
176,463
351,509
23,799
14.3%
157,457
323,159
26,304
12.2%
Overall
1,139,183
3,073,863
141,229
7.6%
8.4%
8.2%
9.1%
9.3%
9.5%
8.1%
7.2%
6.6%
16.2%
16.8%
16.2%
19.4%
19.3%
20.9%
16.6%
18.5%
4.8%
4.8%
4.6%
4.5%
4.7%
3.8%
3.3%
3.1%
10 | Exploring the Long Tail of (Malicious) Software Downloads
Downloaded Files
Download URLs
Month
Total
Benign
Likely 