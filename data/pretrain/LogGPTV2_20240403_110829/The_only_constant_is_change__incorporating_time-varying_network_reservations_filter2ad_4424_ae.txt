### 优化后的文本

#### 图表说明
- **Figure 12**: 不同超订率下混合工作负载的完成时间。
- **Figure 13**: 不同平均作业大小下混合工作负载的完成时间。

#### 混合工作负载
假设这些作业在到达时可以立即调度。这种工作负载代表了托管时间敏感型作业的共享云。对于每种场景，我们首先模拟运行单个应用程序的5,000个作业，然后模拟等量混合应用程序的5,000个作业。这四种工作负载分别表示为Sort、Hive Join、Hive Aggregation和Mixed。

### 6.2.1 批处理作业
对于批处理作业，作业调度策略旨在最大化作业吞吐量。在VC（Virtual Channel）模式下，通过遍历作业队列并在任何作业完成后立即调度可运行的作业来实现这一目标。然而，在TIVC（Time-Interval Virtual Channel）模式下，这种方法可能效率低下，因为在任何运行中的作业完成之前，可能会有足够的剩余带宽释放出来，从而可以调度新的作业。因此，作业调度器每10秒重新扫描一次作业队列，这是平均作业完成时间的20%。

**作业完成时间**：图11展示了在VC和TIVC模式下完成所有5,000个作业所需的时间。我们看到，对于所有工作负载，TIVC显著提高了数据中心的作业完成时间和吞吐量。具体来说，与VC相比，TIVC将Sort、Hive Join、Hive Aggregation和Mixed的完成时间分别减少了41.5%、20.8%、23.1%和34.5%。

**不同超订率和作业大小的影响**：我们在物理数据中心网络中使用不同的超订率重复上述实验。图12显示，当超订率较高时，TIVC相对于TC的优势更大，在超订率为6、8和10的情况下，总完成时间分别减少了35.2%、36.0%和41.5%。同样，图13表明，增加平均作业大小N会进一步提高TIVC相对于VC的性能优势，因为较大的作业更有可能经过超订的核心网络链路。

### 6.2.2 动态到达的作业
现在考虑作业请求随时间到达的云场景。假设作业到达遵循泊松过程，速率为λ，则具有M个虚拟机的数据中心的负载为λ · N · Tc/M，其中N是平均作业请求大小（即49），Tc是平均作业完成时间。如果作业在到达时无法分配，则会被拒绝，如Amazon的EC2作业准入控制所示。我们再次在不同的负载因子下模拟VC和TIVC下的5,000个作业请求。

**作业拒绝率**：图14(a)-14(d)展示了三种应用工作负载和混合工作负载的拒绝率。我们观察到，在低负载下，例如20%，VC和TIVC的总网络预留都可以满足，因此两者都接受所有作业。随着负载的增加，VC比TIVC拒绝更多的请求。例如，在80%的负载下，VC分别拒绝了20.0%、2.7%、20.6%和9.5%的请求，而TIVC分别拒绝了10.1%、0.3%、7.9%和3.4%的请求。

**并发作业和VM/网络利用率**：为了理解TIVC如何实现比VC更低的拒绝率，我们查看了两种模型下的并发作业数量以及VM和网络利用率。由于篇幅限制，我们仅展示了在80%负载下的混合工作负载结果。图15(a)显示，在初始作业到达阶段之后，TIVC始终实现了比VC高约7%的并发作业数。由于TIVC接受的额外作业往往大于平均值，因此TIVC的7%更高的并发作业数转化为平均接近13%的更高VM利用率（数据中心共有64,000个VM），如图15(b)所示。

**最终原因**：TIVC能够容纳更多作业的原因在于利用作业的低网络需求期来调度其他作业。图15(c)显示了VC和TIVC随时间变化的平均预留访问带宽。我们看到VC平均预留的带宽比TIVC高26.4%（占链路容量）。然而，图15(d)显示TIVC的实际网络利用率平均约为20.1%，计算方法是将每个接入链路上各个作业的瞬时流量需求相加，然后对所有接入链路进行平均，远高于VC的8.9%。这证实了TIVC通过捕捉应用程序流量需求的时间变化性，能够实现比VC更高效的带宽预留。我们注意到，在显式网络预留下的总体实际网络利用率较低可能看似反直觉，但这正是为了确保可预测性能所付出的代价，即预留足够的带宽以避免实际应用执行因多样性和突发流量而延长（§4.1）。

**作业局部性和链路共享**：为了评估由PROTEUS分配的TIVC作业的空间局部性，我们绘制了数据中心不同子树级别的并发作业数量，针对混合工作负载运行。图16显示，在80%负载下，经过启动阶段后，平均约有795个作业被分配在级别1子树内，总计920个并发作业。

希望这些改进使您的文本更加清晰、连贯和专业！如果有任何进一步的修改需求，请随时告诉我。