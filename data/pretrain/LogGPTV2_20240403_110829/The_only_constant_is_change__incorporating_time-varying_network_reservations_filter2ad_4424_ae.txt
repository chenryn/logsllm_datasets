 16000
 12000
 8000
 4000
 0
 49
 75
 100
Mean Job Size
Figure 12: Completion time
with varying oversub.
for
mixed workload.
Figure 13: Completion time
with varying mean job size for
mixed workload.
if they could be scheduled at the moment of arrival. This workload
is representative of shared clouds that host time-sensitive jobs. For
each scenario, we ﬁrst simulate 5,000 jobs running a single appli-
cation, and then simulate 5,000 jobs of equally mixed applications.
The four workloads are denoted as Sort, Hive Join, Hive Aggrega-
tion and Mixed.
6.2.1 Batched Jobs
For batched jobs, the job scheduling policy tries to maximize the
job throughput. Under VC, this is achieved by going through the
job queue and schedule the jobs that can be scheduled to run, when-
ever a job is ﬁnished. Under TIVC, however, this can be inefﬁcient,
as before any running job is ﬁnished, there can be enough residual
bandwidth freed up so that a new job can be scheduled. Instead,
the job scheduler rescans the job queue every 10 seconds, which is
20% of the average inter-job completion time.
Job completion time. Figure 11 plots the time to complete all
5,000 jobs under VC and TIVC. We see for all workloads, TIVC
signiﬁcantly improves the completion time, and hence job through-
put of the datacenter, over VC. In particular, compared to VC,
TIVC reduces the completion time by 41.5%, 20.8%, 23.1%, and
34.5% for Sort, Hive Join, Hive Aggre., and Mixed, respectively.
Varying the oversubscription rate and job size. We repeat the
above experiments with varying oversubscription rates in the phys-
ical datacenter network. Figure 12 shows TIVC provides greater
advantage over TC when the oversubscription rate is larger, reduc-
ing the total completion time by 35.2%, 36.0%, and 41.5% under
oversubscription rates 6, 8, and 10, respectively. Similarly, Fig-
ure 13 shows increasing the mean job size N further increases the
performance advantage of TIVC over VC since larger jobs are more
likely to traverse the oversubscribed core network links.
6.2.2 Dynamically Arriving Jobs
We now consider the cloud scenario where job requests ar-
rive over time. Assume the job arrival follows a Poisson process
with rate λ, then the load on a datacenter with M VMs total is
λ · N · T c/M where N is the mean job request size (i.e., 49) and
T c is the mean job completion time. If a job cannot be allocated
upon its arrival, it is rejected, as is the case with Amazon’s EC2 job
admission control [1] today. We again simulate 5,000 job requests
under VC and TIVC while varying the load factor.
207VC
TIVC
)
%
(
.
q
e
R
d
e
t
c
e
e
R
j
 30
 25
 20
 15
 10
 5
 0
VC
TIVC
)
%
(
.
q
e
R
d
e
t
c
e
e
R
j
 30
 25
 20
 15
 10
 5
 0
VC
TIVC
)
%
(
.
q
e
R
d
e
t
c
e
e
R
j
 30
 25
 20
 15
 10
 5
 0
VC
TIVC
)
%
(
.
q
e
R
d
e
t
c
e
e
R
j
 30
 25
 20
 15
 10
 5
 0
 20
 40
 60
 80  100
 20
 40
 60
 80  100
 20
 40
 60
 80  100
 20
 40
 60
 80  100
Load (%)
(a) Sort.
Load (%)
(b) Hive Join.
Load (%)
(c) Hive Aggregation.
Load (%)
(d) Mixed.
Figure 14: Percentage of rejected requests with varying datacenter load.
s
b
o
J
t
n
e
r
r
u
c
n
o
C
 1000
 800
 600
 400
 200
 0
TIVC
VC
)
%
(
n
o
i
t
a
z
i
l
i
t
U
 100
 80
 60
 40
 20
 0
TIVC
VC
)
%
(
W
B
d
e
v
r
e
s
e
R
 100
 80
 60
 40
 20
 0
VC
TIVC
)
%
(
n
o
i
t
a
z
i
l
i
t
U
 100
 80
 60
 40
 20
 0
TIVC
VC
 0
 500  1000 1500 2000 2500
 0
 500  1000  1500  2000  2500
 0
 500  1000  1500  2000  2500
 0
 500  1000  1500  2000  2500
Time (sec)
(a) Concurrent jobs.
Time (sec)
Time (sec)
Time (sec)
(b) VM utilization.
(c) Reserved access bandwidth.
(d) Actual network utilization.
Figure 15: Concurrent jobs, VM utilization, reserved and actual utilization of bandwidth, for Mixed and 80% load.
Job rejection rate. Figures 14(a)-14(d) plot the rejection rates for
the three application workloads and the mixed workload. We ob-
serve that under low load, e.g., 20%, the total networking reserva-
tion under both VC and TIVC can be met and hence both accept
all jobs. As the load increases, VC rejects far more requests than
TIVC. For example, at 80% load, 20.0%, 2.7%, 20.6%, and 9.5%
of the requests are rejected under VC compared to 10.1%, 0.3%,
7.9%, and 3.4% under TIVC, for the four workloads, respectively.
Job concurrency and VM/network utilization. To understand
how TIVC achieves much lower rejection rates than VC, we look at
the number of concurrent jobs scheduled and the VM and network
utilization under the two models. Due to page limit, we only show
the results for the mixed workload under one load factor, 80%.
Figure 15(a) shows that after the initial job arrival ramp-up phase,
TIVC consistently achieves about 7% higher job concurrency than
VC. Since the extra jobs accepted by TIVC tend to be larger than
average, the 7% higher job concurrency under TIVC translates into
on average close to 13% higher VM utilization (of the total 64,000
VMs in the datacenter) than under VC, as shown in Figure 15(b).
Finally, the reason TIVC is able to ﬁt more jobs is by exploit-
ing lower networking periods of a job to schedule other jobs. Fig-
ure 15(c) shows the average reserved access bandwidth over time
under VC and TIVC. We see VC reserves on average 26.4% (of the
link capacity) higher bandwidth than TIVC. However, Figure 15(d)
shows TIVC achieves on average about 20.1% actual network uti-
lization, calculated by adding the instantaneous trafﬁc demand of
individual jobs over each access link, and then averaged over all ac-
cess links, much higher than the 8.9% under VC. This conﬁrms that
by capturing the time-varying nature of application trafﬁc demand,
TIVC is able to achieve much more efﬁcient bandwidth reservation
than VC. We note the overall low actual network utilization un-
der the explicit network reservations may seem counter-intuitive.
This is precisely the price to pay for predictable performance, i.e.,
to reserve enough bandwidth so that the execution of real world
applications, which can have diverse, bursty trafﬁc phases, is not
elongated (§4.1).
Job locality and link sharing. To assess the spatial locality of
the TIVC jobs allocated by PROTEUS, we plot the average number
of concurrent jobs allocated at different subtree levels in the dat-
acenter, for the mixed workload runs. Figure 16 shows that after
the ramp-up phase, under 80% load, on average around 795 out of
the 920 total concurrent jobs are allocated within level-1 subtrees,
s
b
o
J
t
n
e
r
r
u
c
n
o
C
 1000
 800
 600
 400
 200
 0
Lv1 tree
Lv2 tree
Lv3 tree
s
b
o
J
t
n
e
r
r
u
c
n
o
C
 1000
 800
 600
 400
 200
 0
Lv1 tree
Lv2 tree
Lv3 tree
 0
 500  1000  1500  2000  2500
 0
 500  1000  1500  2000
Time (sec)
(a) With 80% load.
Time (sec)
(b) With 100% load.
Figure 16: Number of concurrent jobs allocated to diff. tree
levels for Mixed.
i
k
n
L
r
e
p
s
b
o
J
f
o
.
m
u
N
x
a
M
 30
 20
 10
 0
Lv3 link
Lv2 link
Lv1 link
 0
 500  1000  1500  2000  2500
Time (sec)
(a) With 80% load.
i
k
n
L
r
e
p
s
b
o
J