title:User Discrimination through Structured Writing on PDAs
author:Rachel R. M. Roberts and
Roy A. Maxion and
Kevin S. Killourhy and
Fahd Arshad
User Discrimination Through Structured Writing on PDAs
Rachel R. M. Roberts, Roy A. Maxion, Kevin S. Killourhy, and Fahd Arshad
{rroberts, maxion, ksk, fahd}@cs.cmu.edu
Dependable Systems Laboratory
Computer Science Department
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213 / USA
Abstract
This paper explores whether features of structured
writing can serve to discriminate users of handheld de-
vices such as Palm PDAs. Biometric authentication
would obviate the need to remember a password or to
keep it secret, requiring only that a user’s manner of writ-
ing conﬁrm his or her identity. Presumably, a user’s dy-
namic and invisible writing style would be difﬁcult for an
imposter to imitate.
We show how handwritten, multi-character strings
can serve as personalized, non-secret passwords. A pro-
totype system employing support vector machine classi-
ﬁers was built to discriminate 52 users in a closed-world
scenario. On high-quality data, strings as short as four
letters achieved a false-match rate of 0.04%, at a corre-
sponding false non-match rate of 0.64%. Strings of at
least 8 to 16 letters in length delivered perfect results—a
0% equal-error rate. Very similar results were obtained
upon decreasing the data quality or upon increasing the
data quantity.
1. Introduction
Passwords are standard in computer access control,
but they can be forgotten, compromised without detec-
tion, and denied as having been used (repudiation) [14].
A proposed alternative is biometrics, or measurements of
the human body. Biological biometrics are physical traits
such as the iris, ﬁngerprint, and face; behavioral biomet-
rics are activities such as handwriting, keystrokes, and
gait. Handwriting is an alterable biometric: it changes
in response to varying conditions, e.g., text. Alterable
biometrics are suitable for challenge-response protocols,
whose dynamic nature resists forgery and replay, thereby
providing stronger non-repudiation [14].
Personal digital assistants (PDAs) can potentially
provide effective and low-cost biometric authentication.
Some allow structured writing that may facilitate hand-
writing veriﬁcation (because it enforces writing consis-
tency), while also enabling automatic letter recognition.
A PDA—or its input technology—could be integrated
into a kiosk to capture and relay biometric data cheaply,
while enjoying protection from theft and tampering.
To test whether structured handwriting on PDAs has
promise as a biometric, we devised an evaluation of mild
difﬁculty. Phillips et al. [16] recommend that evaluations
be not too hard nor too easy; there are three stages of eval-
uation protocols: technology, scenario, and operational.
We take a ﬁrst step in examining the potential of PDAs to
convey biometric-based security, by conducting a prelim-
inary technology evaluation on a laboratory algorithm.
2. Problem and approach
We address whether it is possible to discriminate en-
rolled users on the basis of their handwriting characteris-
tics in Grafﬁti [15], the original structured language of
Palm handhelds. Speciﬁcally, we seek to build a sys-
tem that can conﬁrm or deny a claimed identity within
a closed set of enrolled users; it is assumed that no out-
siders can access the biometric system. Our example ap-
plication is designed to catch insider attacks [21] and to
provide traceability of human actions.
Our approach is to devise a non-secret challenge
string, one per enrolled user, to distinguish that user from
all the others. Challenge strings are pre-computed, based
on errors and successes observed in preliminary testing
on enrollment templates. In a hypothetical transaction, a
user claims an identity and receives a personalized chal-
lenge string, which he or she writes on a Palm PDA. To
conﬁrm or deny the identity claim, the biometric sys-
tem determines which enrolled user most likely wrote the
sample, and reports whether or not the predicted identity
matches the claimed one. We collect biometric data to
build a corpus; data from the corpus is used to simulate
user participation in the biometric system.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20073. Background and related work
To our knowledge, no previous research has at-
tempted to differentiate users on the basis of their hand-
writing idiosyncrasies in a structured input language. Re-
search on a related problem, discriminating users on
the basis of their natural handwriting, is summarized in
[11, 12, 17, 18, 20, 23]. This prior work can be cat-
egorized into signature veriﬁcation tasks (determining
whether a particular person wrote a signature) vs. writer
identiﬁcation tasks (determining which one of N known
people wrote a document), and into off-line methods
(writing as a static image) vs. on-line methods (writing
as a dynamic process). The so-called off-line problems
are more difﬁcult than on-line ones, because timing, di-
rection, pressure, and pen-orientation data are not avail-
able, and because recovering writing from a background
document may be hard. Writer identiﬁcation is more
challenging than signature veriﬁcation, due to difﬁculties
in automation, character segmentation, and letter recog-
nition. Off-line signature veriﬁcation, the most typical
application, usually achieves false-match and false non-
match rates of a few percentage points each, although
these may be optimistic due to the small size of signature
databases [18]. The best team at the First International
Signature Veriﬁcation Competition achieved equal-error
rates of 2.84% and 2.89%, respectively, on two tasks [25].
A wide variety of features is used to characterize nat-
ural handwriting. Features studied by manual examiners
include the form of the writing as well as spelling and the
type of pen used [3]. On-line signature veriﬁcation may
utilize functions of time. In off-line writer identiﬁcation,
features may be text-independent (using global statisti-
cal features) or text-dependent (using features computed
on characters resolved from the image). Because Graf-
ﬁti writing differs greatly from natural handwriting, the
study of Grafﬁti letters motivated our features.
The original input language of Palm handhelds,
Grafﬁti, is a stylized version of printed English (see Fig-
ure 1). The latest version, slightly modiﬁed, is called
Grafﬁti 2. Grafﬁti is more constrained than natural hand-
writing; this makes letter recognition easier. The Palm
PDA’s screen digitizes information about pen pressure
(a binary judgment, either up or down) and position (in
Cartesian coordinates), in order to recognize letters.
In the framework of research on handwriting biomet-
rics, our problem is a kind of writer veriﬁcation task, us-
ing dynamic information, text-dependent features, and an
automated decision process. Aspects of signature veri-
ﬁcation and writer identiﬁcation apply to our work, al-
though to a limited extent because of the differences be-
tween natural and constrained handwriting.
Most biometric systems perform either positive iden-
tiﬁcation (verifying positive claims of enrollment), or
negative identiﬁcation (verifying claims of no enroll-
Figure 1. Grafﬁti letters “A” to “G” [15]
ment) [24]. Our proposed system has characteristics of
both, and additionally focuses on differentiating enrollees
rather than distinguishing enrollees from outsiders. The
insider-detection task we pursue assumes that informa-
tion about all possible attackers is available.
4. Overview of the three experiments
The aim of this research is to test whether enrolled
users can be discriminated on the basis of their handwrit-
ing characteristics in a constrained input language. To
fulﬁll this goal, we recruited 52 subjects to write 1417
letters each, and we derived features from those letters
to constitute a corpus. Next, twenty-six classiﬁers, one
per alphabet letter, were trained using half of the data. A
separate portion of data was set aside to test the classi-
ﬁers; tests generated user- and letter-speciﬁc information
about classiﬁcation errors. For each user, this informa-
tion was used to order and group letters into challenge
strings, which were employed in simulated authentication
transactions. A reserved portion of the data produced the
transactions (genuine and unpracticed impostor) to exam-
ine how challenge-string length affects system accuracy.
The biometric system was trained and tested anew in
three distinct experiments, each using a different version
of the feature data, to explore the effects of data quality
and quantity on results. (1) High-quality Data contains
features from letters judged to be highly representative
of user handwriting. The purpose of the High-quality
Data experiment is to learn whether users can be discrim-
inated on the basis of their handwriting alone (and not
on their handwriting plus data-capture artifacts). (2) Re-
duced Data is of the same size and proportions as High-
quality Data, but its features come from letters selected at
random, instead of on the basis of quality. The purpose of
the Reduced Data experiment is to gauge whether lower
data quality might decrease accuracy, in comparison to
the High-quality Data experiment. (3) All Data contains
features from all valid letters we asked subjects to write.
The purpose of the All Data experiment is to see whether
an additional quantity of data might improve accuracy, in
comparison to the Reduced Data experiment.
5. Data collection and preparation
Preparation of the three versions of the data (men-
tioned in Section 4) was identical, except where noted.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 200711:2:K:K:660.903022454:65:180
Timestamp
(seconds since last reboot)
Letter presented in stimuli
Letter written by subject
Status code of pen
X-coord.
Y-coord.
50 = Letter not recognized by the Palm PDA
Stimulus letter written
Non-stimulus letter written
1 = Pen down
2 = Pen moving
3 = Pen up
21 = Pen down
22 = Pen moving
23 = Pen up
Subject ID (assigned by experimenter)
Figure 2. Example raw data for “K”
5.1. Instrumentation
Three Palm m105 handhelds, each running Palm OS
3.5 and having a maximum screen resolution of 160×160
pixels (with 2-bit color support, or 4 gray levels) were
used to collect data. For each letter stroke drawn on
the screen, a sequence of timestamped (x, y) coordinates,
corresponding to the sampled positions of the stylus, was
produced internally. For example, the ﬁrst two consec-
utive points of a letter “I” might have  values of  and . We captured the information using a program
installed directly on the Palm PDA and written in the C
language, with the help of the Palm Software Develop-
ment Kit (SDK 3.5). The program noted whether each
written letter matched the one expected according to the
stimuli order (see Figure 2). The median interval between
successive timestamps was 0.02 seconds; the median let-
ter stroke took 0.38 seconds to write.
Approximate granularity of spatial coordinates can
be gleaned from the following information. The dimen-
sions of the Palm PDA’s writing box are 1.60 cm (width)
by 1.85 cm (height), although the entire screen (about 5
cm by 6.75 cm) is sensitive to input. In internal coordi-
nate units, the range of horizontal coordinates in the data
was 143; that for vertical coordinates was 218. Assum-
ing the entire writing box received input at some point,
we estimate an internal coordinate unit to be about 0.1
mm. The average letter width (in internal units) was 21.4,
while the average letter height was 30.5.
5.2. Stimulus materials
Stimuli were chosen to ensure adequate instances of
each letter, while discouraging subject boredom or frus-
tration. Stimuli consisted of 5-letter nonsense strings and
pangram sentences. Nonsense strings contained a bigram
(two-letter combination) followed by a trigram (three-
letter combination). Bigram and trigram motifs were vi-
sually separated by a space on the screen, which the sub-
ject did not write. Motifs were repeated and recombined
to facilitate visual processing; two examples of nonsense
strings are “BC ZAT” and “YZ CKS”. A pangram in-
cludes every letter of the alphabet, e.g., “the ﬁve boxing
wizards jump quickly”. Ten nonsense strings were fol-
lowed by one pangram sentence to form one set; there
were 15 sets, resulting in 150 unique nonsense strings
(750 letters) and 15 unique pangrams (667 letters). In to-
tal, each subject wrote 1417 requested letters. The letter
type having the fewest instances in the stimuli had 44,
while the one having the most instances had 94; the me-
dian number of instances of a given letter type was 52.
Stimuli were presented on the screen of the Palm
PDA; subjects were asked to copy each sentence or string
that appeared. A separate chart of the 26 Grafﬁti let-
ters was displayed for reference, if needed.
If a writ-
ten letter was not recognized by the PDA, or if a letter
did not match the one the user was expected to write,
a beep sounded to prod the user to write it over again.
The canonical Grafﬁti form for the letter “X” requires
two strokes, but to simplify analysis we asked subjects
to use an alternative single-stroke form (looking like a
backwards α); compliance was conﬁrmed. Other alterna-
tive strokes (all single-stroke forms) exist for a handful of
other letters in Grafﬁti; these were allowed. For example,
the alternative stroke for “Y” looks like a γ.
5.3. Subjects and sessions
Fifty-two subjects participated; each subject wrote
in a single sitting lasting about 45 minutes. To increase
task manageability, we did not introduce a time lag be-
tween enrollment and testing sessions (template ageing),
although this is recommended [13]. Roughly half (25)
of the subjects reported that they could write Grafﬁti let-
ters without thinking about how to do it. Eleven others
reported that they had learned Grafﬁti once before; only
nine subjects claimed no prior exposure to Grafﬁti. Five
subjects were left-handed and 47 were right-handed.
5.4. Data conditioning and version generation
User errors were excluded before analysis, i.e., when
a subject drew a stroke not recognized as a letter, or wrote
a letter other than the expected one. This seldom hap-
pened, because auditory feedback (upon errors) alerted
subjects to pay closer attention. One might include such
instances in a failure-to-acquire rate, but we reserve that
distinction for letters excluded in the High-quality Data.
The three versions of the data, one for each of the exper-
iments described in Section 4, were prepared as follows.
High-quality Data. In the high-quality version of
the corpus, data judged to be unrepresentative of actual
user handwriting were excluded. Figure 3 shows four
“Y”s, the leftmost pair written by one subject, the right-
most by another. Within each pair, the left letter not only
looks unrealistic but also contains unrealistic timing in-