social network are Sybils; the average degree in the Sybil region is the same as
that in the benign region in order to avoid asymmetry between the two regions
introduced by density. We set the number of attack edges as 500, and thus the
average attack edge per Sybil is 0.06.
(2) Small Twitter with real Sybils. We obtained a publicly available
Twitter dataset with 809 Sybils and 7,358 benign nodes from Yang et al. [36]. A
node is a Twitter user and an edge means two users follow each other. Sybils were
labeled spammers. 9.9% of nodes are Sybils and 53.4% of Sybils are connected.
The average degree is 16.72, and the average attack edge per Sybil is 49.46.
(3) Large Twitter with real Sybils. We obtained a snapshot of a large-
scale Twitter follower-followee network crawled by Kwak et al. [20]. A node is
a Twitter user and an edge between two nodes means that one node follows
the other node. The network has 41,652,230 nodes and 1,202,513,046 edges. To
perform evaluation, we need ground truth labels of the nodes. Since the Twitter
network includes users’ Twitter IDs, we wrote a crawler to visit each user’s proﬁle
using Twitter’s API, which tells us the status (i.e., active, suspended, or deleted)
of each user. In our ground truth, 205,355 nodes were suspended, 5,289,966 nodes
were deleted, and the remaining 36,156,909 nodes are active. We take suspended
users as Sybils and active users as benign nodes. 85.3% Sybils are connected with
an average degree 24. 1.5% of the total edges are attack edges and the average
number of attack edges per Sybil is 181.55. We acknowledge that our ground
truth labels might be noisy since some active users might be Sybils, but they
evaded Twitter’s detection, and Twitter might have deleted some Sybils.
AUC as an Evaluation Metric: Similar to previous studies [6,7,14,30], we
use the Area Under the Receiver Operating Characteristic Curve (AUC) as an
evaluation metric. Suppose we rank nodes according to their probabilities of
being Sybil in a descending order. AUC is the probability that a randomly
selected Sybil ranks higher than a randomly selected benign node. Random
guessing, which ranks nodes uniformly at random, achieves an AUC of 0.5.
240
B. Wang et al.
Table 1. Dataset statistics.
Metric
#Nodes
#Edges
Facebook Small Twitter Large Twitter
43,953
8,167
41,652,230
182,384
68,282
1,202,513,046
Ave. degree
8.29
Ave. #attack edge per Sybil 0.06
16.72
49.46
57.74
181.55
Compared Methods: We adapt a community detection method and
SybilSCAR to detect Sybils when no manual labels are available. Moreover,
we compare with SybilRank [7] and SybilBelief [14] that require manual labels.
(1) Community detection (Louvain Method). When there are no man-
ually labeled training sets, community detection seems to be a natural choice
to detect connected Sybils.2 A community detection method divides a social
network into connected components (called “communities”), where nodes in the
same community are densely connected while nodes across diﬀerent communities
are loosely connected. Presumably, Sybils are in the same communities.
Since the benign region itself often consists of multiple communities [2,7],
the key challenge of community detection methods is how to determine which
communities correspond to Sybils. Assigning a label of Sybil (or benign) to a
community means that all nodes in the community are Sybils (or benign). Since
it is unclear how to assign labels to the communities algorithmically (though
one could try various heuristics), in our experiments, we assume one could label
communities such that community detection achieves a false negative rate that
is the closest to that of SybilBlind. Speciﬁcally, SybilBlind predicts a node to be
Sybil if its aggregated probability is larger than 0.5, and thus we can compute
the false negative rate for SybilBlind. Then we compare community detection
with SybilBlind with respect to AUC, via ranking the communities labeled as
Sybil higher than those labeled as benign. Our experiments give advantages to
community detection since this label assignment might not be found in practice.
Louvain method [5] is a widely used community detection method, which is eﬃ-
cient and outperforms a variety of community detection methods [5]. Therefore,
we choose Louvain method in our experiments.
(2) SybilSCAR with a sampled noisy training set (SybilSCAR-
Adapt). When a manually labeled training set is unavailable, we use our sampler
to sample a training set and treat it as the input to SybilSCAR. The performance
of this adapted SybilSCAR highly depends on the label noise of the training set.
(3) SybilRank and SybilBelief with labeled training set. SybilRank [7]
and SybilBelief [14] are state-of-the-art random walk-based method and LBP-
based method, respectively. SybilRank can only leverage labeled benign nodes,
while SybilBelief can leverage both labeled benign nodes and labeled Sybils. We
2 The local community detection method [26] requires labeled benign nodes and thus
is inapplicable to detect Sybils without a manually labeled training set.
SybilBlind: Detecting Fake Users in Online Social Networks
241
randomly sample a labeled training set, where the number of labeled benign
nodes and Sybils equals n (the sampling size of SybilBlind).
(4) SybilBlind. In the Facebook network with synthesized Sybils, our sam-
pler samples the two subsets B and S uniformly at random from the entire social
network. For the Twitter datasets, directly sampling two subsets B and S with
a low label noise is challenging due to the number of benign nodes is far larger
than that of Sybils. Thus, we reﬁne our sampler by using discriminative node
features. Previous studies [36,37] found that Sybils proactively follow a large
number of benign users in order to make more benign users follow them, but
only a small fraction of benign users will follow back. Therefore, we extract the
follow back rate (FBR) feature for each node in the Twitter datasets. Then we
rank all nodes according to their FBR features in an ascending order. Presum-
ably, some Sybils are ranked high and some benign nodes are ranked low in
the ranking list. Thus, we sample the subset B from the bottom-K nodes and
sample the subset S from the top-K nodes. Consider the diﬀerent sizes of the
two Twitter datasets, we set K = 1,000 and K = 500,000 in the small and large
Twitter datasets, respectively. This sampler is more likely to sample training
sets that have lower label noise, and thus it improves SybilBlind’s performance.
Note that when evaluating SybilSCAR-Adapt on the Twitter datasets, we also
use FBR-feature-reﬁned sampler to sample a training set. As a comparison, we
also evaluate the method simply using the FBR feature and denote it as FBR.
Moreover, we evaluate SybilBlind with randomly sampled two subsets without
the FBR feature, which we denote as SybilBlind-Random.
Parameter Settings: For SybilBlind, according to Theorem 1, the minimal
number of sampling trials kmin to generate a training set with label noise less
than or equal to τ is exponential with respect to n, and kmin would be very
large even with a modest n. However, through empirical evaluations, we found
that the number of sampling trials can be largely decreased when using the
FBR-feature-reﬁned sampler. Therefore, we instead use the following heuristics
to set the parameters, with which SybilBlind has already obtained satisfying
performance. Speciﬁcally, n = 10, k = 100, and κ = 10 for the Facebook network
with synthesized Sybils; n = 100, k = 20, and κ = 10 for the small Twitter; and
n = 100, 000, k = 20, and κ = 10 for the large Twitter. We use a smaller k for
Twitter datasets because FBR-feature-reﬁned sampler is more likely to sample
training sets with smaller label noise. We use a larger sampling size n for the
large Twitter dataset because its size is much bigger than the other two datasets.
We will also explore the impact of parameters and the results are shown in Fig. 4.
For other compared methods, we set parameters according to their authors.
For instance, we set θ = 0.4 for SybilSCAR. SybilRank requires early termi-
nation, and its number of iterations is suggested to be O(log |V |). For each
experiment, we repeat 10 times and compute the average AUC. We implement
SybilBlind in C++ using multithreading, and we obtain the publicly available
242
B. Wang et al.
Fig. 3. AUCs
on the
Facebook network with
synthesized Sybils. Sybil-
Blind is robust to various
numbers of attack edges.
Fig. 4. AUCs of SybilBlind vs. (a) sampling size n
and (b) number of sampling trials k on the large
Twitter. We observe that SybilBlind achieves high
AUSs when n and k reach certain values.
Table 2. AUCs of the compared methods on the Twitter datasets.
Small Twitter Large Twitter
0.50
0.70
0.69
0.78
0.51
0.65
0.79
Method
Louvain
SybilSCAR-Adapt
SybilRank
SybilBelief
FBR
0.54
0.89
0.86
0.98
0.60
SybilBlind-Random 0.82
SybilBlind
0.98
implementations for SybilSCAR (also in C++)3 and Louvain method4. We per-
form all our experiments on a Linux machine with 512GB memory and 32 cores.
6.2 Results
AUCs of the Compared Methods: Figure 3 shows AUCs of the compared
methods on the Facebook network with synthesized Sybils as we increase the
number of attack edges. Note that SybilBlind-Random is essentially SybilBlind
in this case, as we randomly sample the subsets without the FBR feature. Table 2
shows AUCs of the compared methods for the Twitter datasets with real Sybils.
We observe that (1) SybilBlind outperforms Louvain method. Speciﬁcally, when
the number of attack edges gets relatively large, even if one could design an
algorithm to label communities such that Louvain method can detect as many
Sybils as SybilBlind (i.e., similar false negative rates), Louvain method will rank
a large fraction of benign users higher than Sybils, resulting in small AUCs.
The reason is that some communities include a large number of both benign
3 http://home.engineering.iastate.edu/∼neilgong/dataset.html.
4 https://sites.google.com/site/ﬁndcommunities/.
SybilBlind: Detecting Fake Users in Online Social Networks
243
Fig. 5. AUCs of SybilSCAR vs. the fraction of nodes that are manually labeled as
a training set on the small Twitter and large Twitter datasets. We observe that
SybilSCAR requires manually labeling about 25% and 2.8% of total nodes on the
small Twitter and large Twitter datasets to be comparable to SybilBlind.
users and Sybils, which is an intrinsic limitation of community detection. (2)
SybilBlind outperforms SybilSCAR-Adapt, which validates that our homophily-
entropy aggregator is signiﬁcant and essential. Thus, aggregating results in multi-
ple sampling trials can boost the performance. (3) SybilBlind outperforms Sybil-
Rank and is comparable with SybilBelief, even if SybilRank and SybilBelief use
a labeled training dataset. This is because the FBR-feature-reﬁned sampler can
sample training sets with relatively small label noise and SybilSCAR is robust
to such label noise. As SybilSCAR was shown to outperform SybilRank and
be comparable with SybilBelief [30], so does SybilBlind. (4) SybilSCAR-Adapt
achieves AUCs that are close to random guessing on the Facebook network.
This is because the sampled training set has random label noise that could be
large. SybilSCAR-Adapt works better on the Twitter datasets. Again, this is
because the FBR feature assists our sampler to obtain the training sets with
small label noise on the Twitter datasets and SybilSCAR can tolerate such label
noise. (5) FBR achieves a small AUC. This indicates that although the FBR
feature can be used to generate a ranking list with small label noise by treating
top-ranked nodes as Sybils and bottom-ranked nodes as benign, the overall rank-
ing performance on the entire nodes is not promising. (6) SybilBlind-Random’s
performance decreases on the Twitter datasets. The reason is that it is diﬃcult
to sample training sets with small label noise, as the number of benign nodes is
far larger than the number of Sybils on the Twitter datasets.
Number of Manual Labels SybilSCAR Requires to Match SybilBlind’s
Performance: Intuitively, given a large enough manually labeled training set,
SybilSCAR that takes the manually labeled training set as an input would out-
perform SybilBlind. Therefore, one natural question is how many nodes need to
be manually labeled in order for SybilSCAR to match SybilBlind’s performance.
To answer this question, we respectively sample x fraction of total nodes in the
small Twitter dataset and large Twitter dataset and treat them as a manually
244
B. Wang et al.
Fig. 6. Performance of diﬀerent aggrega-
tors on the Facebook network with syn-
thesized Sybils. Our homophily-entropy
aggregator (HEA) signiﬁcantly outper-
forms the average, min, and max aggre-
gators.
Fig. 7. Impact of the fraction of Sybils
on the Facebook network. We observe
that SybilBlind can accurately detect
Sybils once the fraction of Sybils is
smaller than 50%, i.e., Sybils are less
than benign nodes.
labeled training set, i.e., the benign nodes are assigned a label of benign and
the Sybils are assigned a label of Sybil. Note that the manually labeled training
set has no label noise. Then, we run SybilSCAR with the training set, rank the
remaining nodes using their probabilities of being Sybil, and compute an AUC.
Figure 5 shows the AUCs of SybilSCAR as we increase x from 0.1% to 3% on
the small Twitter and large Twitter datasets. For comparison, we also show the
AUC of SybilBlind on the small Twitter and large Twitter datasets, which is
a straight line since it does not rely on the manually labeled training set. We
observe that SybilSCAR requires manually labeling about 25% of total nodes on
the small Twitter and about 2.8% of total nodes on the large Twitter in order
to achieve an AUC that is comparable to SybilBlind.
Comparing Diﬀerent Aggregators: Figure 6 shows the performances of
diﬀerent aggregators on the Facebook network with synthesized Sybils as we
increase the number of attack edges. We observe that our homophily-entropy
aggregator (HEA) signiﬁcantly outperforms the average, min, and max aggre-
gators. The average aggregator achieves performances that are close to random
guessing. This is because the average aggregator assigns an expected aggregated
probability of 0.5 to every node. Moreover, the min aggregator achieves AUCs
that are worse than random guessing, while the max aggregator achieves AUCs
that are slightly higher than random guessing. It is an interesting future work to
theoretically understand the performance gaps for the min and max aggregators.
Impact of the Fraction of Sybils: Figure 7 shows the AUCs of SybilBlind
as the social network has more and more Sybils. We performed the experiments
on the Facebook network with synthesized Sybils since we need social networks
with diﬀerent number of Sybils. The number of attack edges is set to be 500. We
observe that SybilBlind can accurately detect Sybils (AUCs are close to 1) once
the fraction of Sybils is smaller than 50%, i.e., Sybils are less than benign nodes.
SybilBlind: Detecting Fake Users in Online Social Networks
245
We note that when Sybils are more than benign nodes, SybilBlind would rank