As shown in both cases, the number of inversions can be
different even when the EDTM is the same.
5.2 Directed Acyclic Graph Lineage
We evaluate the practical use of ﬁve metrics for measuring
the accuracy of the constructed DAG lineage: number of
LCA mismatches, average pairwise distance to true LCA,
partial order mismatches, graph arc edit distance, and
k-Cone mismatches.
p2
p3
p6
p7
Figure 4: Lowest common ancestors
p4
p5
p1
We deﬁne SLCA(x,y) to be the set of LCAs of x and y
because there can be multiple LCAs. For example, in Fig-
ure 4, SLCA(p4, p5) = {p2, p3}, while SLCA(p6, p7) =
{p4}. Given SLCA(x,y) in G and the true SLCA∗(x,y)
in G∗, we can evaluate the correct LCA score of (x,y)
L(SLCA(x,y), SLCA∗(x,y)) in the following four ways.
i) 1 point (correct) if SLCA(x,y) = SLCA∗(x,y)
ii) 1 point (correct) if SLCA(x,y) ⊆ SLCA∗(x,y)
iii) 1 point (correct) if SLCA(x,y) ⊇ SLCA∗(x,y)
iv) 1− JD(SLCA(x,y), SLCA∗(x,y)) point
Then the number of LCA mismatches is
|N × N|− ∑
(x,y)∈N×N
L(SLCA(x,y), SLCA∗(x,y)).
The 1st policy is sound and complete, i.e., we only con-
sider an exact match of SLCA. However, even small errors
can lead to a large number of LCA mismatches. The 2nd
policy is sound, i.e., every node in SLCA is indeed a true
LCA (no false positive). Nonetheless, including any extra
node will result in a mismatch. The 3rd policy is com-
plete, i.e., SLCA must contain all true LCAs (no false
negative). However, missing any true LCA will result in
a mismatch. The 4th policy uses the Jaccard distance to
measure dissimilarity between SLCA and SLCA∗. In our
evaluation, ILINE followed the 4th policy since it allows
us to attain a more ﬁne-grained measure.
We also measure the distance between the true LCA(s)
and reported LCA(s). For example, if ILINE falsely re-
ports p5 as an LCA of p6 and p7 in Figure 4, then the
pairwise distance to the true LCA is 2 (=distance between
p4 and p5). Formally, let D(u,v) represent the distance
between nodes u and v in the ground truth G∗. Given
SLCA(x,y) and SLCA∗(x,y), we deﬁne the pairwise dis-
tance to true LCA T (SLCA(x,y), SLCA∗(x,y)) to be
USENIX Association  
22nd USENIX Security Symposium  87
SPECIAL CASE ← ··············· → GENERAL CASE
Straight Line
Inversions
EDTM
DAG
PO
SLCA
GAED
k-Cone
Property measured
Order/Topology
Out-of-place nodes/arcs
Descendants within depth k
Table 1: Relationships among metrics
∑
(l,l∗)∈SLCA(x,y)×SLCA∗(x,y)
|SLCA(x,y)× SLCA∗(x,y)|
and the average pairwise distance to true LCA to be
D(l,l∗)
∑
(x,y)∈N×N
T (SLCA(x,y), SLCA∗(x,y))
|N × N|
.
A partial order (PO) of x and y is to identify which one
of x and y comes ﬁrst: either x or y, or incomparable if they
are not each other’s ancestors. For example, in Figure 4,
the PO of p3 and p7 is p3, while the PO of p6 and p7 is
incomparable. The total number of PO mismatches is the
number of wrong ordering for all (cid:31)|N|2 (cid:30) pairs.
A graph arc edit distance (GAED) measures how many
arcs need to be deleted from G and G∗ to make both G
and G∗ identical. For every node x, we calculate E(x) =
SD(Adj(x), Adj∗(x)) where Adj(x) and Adj∗(x) denotes
the adjacency list of x in G and G∗ respectively. Then
GAED becomes ∑x∈N E(x).
We deﬁne k-CONE(x) to be the set of descendants
within depth k from node x. For example,
in Fig-
ure 4, 2-CONE of p1 is {p2, p3, p4, p5}. Then the given
k-CONE(x) in G and the true k-CONE∗(x) in G∗, we can
evaluate the correct k-CONE score of x R(k-CONE(x))
using four different ways of set comparisons: an exact
match, a subset match, a superset match, or the Jaccard in-
dex. In our evaluation, ILINE used the Jaccard index for a
more ﬁne-grained measure. Then the number of k-CONE
mismatches is |N|− ∑x∈N R(k-CONE(x)). With smaller k,
we can measure the accuracy of nearest descendants.
5.3 Relationships among Metrics
Table 1 shows the relationships among different metrics
and a property measured by each metric. A PO mismatch
is a special case of an LCA mismatch because when x and
y are in different branches, an LCA mismatch measures
the accuracy of SLCA while a PO mismatch just says two
nodes are incomparable. An inversion is also a special
case of an LCA mismatch because querying the LCA of x
and y in a straight line is the same as asking which one of
x and y comes ﬁrst. Essentially, a PO mismatch in a DAG
is equal to an inversion in a straight line.
EDTM is a special case of GAED and an upper bound
of GAED in a straight line is GAED ≤ EDTM×6. One
out-of-place node can cause up to six arcs errors. For
example, p1 → p2 → p4 → p3 → p5 has 1 EDTM (delete
p3 or p4) and 6 GAED (delete p2 → p4, p4 → p3, and
p3 → p5 in G and p2 → p3, p3 → p4, and p4 → p5 in
G∗).
A k-Cone mismatch is a local metric to assess the cor-
rectness of nearest descendants of nodes while the other
six metrics are global metrics to evaluate the correctness
of the order of nodes and to count out-of-place nodes/arcs.
What are good metrics? Among the seven metrics, we
recommend two metrics—partial order mismatches and
graph arc edit distance. PO mismatches and GAED are
both desirable because they evaluate different properties
of lineage and are not deducible from each other.
To see this, observe that PO mismatches and SLCA
mismatches measure the same property of lineage and
have similar accuracy results in our evaluation. However,
PO mismatches are more efﬁcient to compute than SLCA
mismatches; moreover, PO gives an answer for a more in-
tuitive question, “which one of these two programs comes
ﬁrst”. Thus, PO mismatches are preferred. Average dis-
tance to true LCA is supplementary to SLCA mismatches
and so this metric is not necessary if we exclude SLCA
mismatches. The number of inversions and edit distance
to monotonicity can be respectively seen as special cases
of PO mismatches and GAED in the case of straight line
lineages. k-Cone mismatches can be extremely useful to
an analyst during manual analysis, but it can be difﬁcult
to pick the right value of k automatically.
6
ILINE is implemented using C (2.5 KLoC) and
IDAPython plugin (100 LoC). We use the IDA Pro disas-
sembler1 to disassemble program binaries and to identify
basic blocks. As discussed in §2.3, gcc -S output is
used to compensate the errors introduced at the disas-
sembling step. We utilize Cuckoo Sandbox2 to monitor
native functions, API calls and network activities of mal-
ware. On top of Cuckoo Sandbox, we use malwasm3 with
pintool4, which allows us to obtain more ﬁne-grained in-
struction level of traces. Since some kinds of malicious
activities require “live” connections, we also employ IN-
etSim5 to simulate various network services, e.g., web,
Implementation
1http://www.hex-rays.com/products/ida/index.shtml
2http://cuckoosandbox.org/
3http://code.google.com/p/malwasm/
4http://software.intel.com/en-us/articles/pintool
5http://www.inetsim.org/
88  22nd USENIX Security Symposium 
USENIX Association
[software]
iLine
[k code bases]
[lineage output]
iEval
Extract
Features
Perform
Clustering
Construct
Lineage
Measure
Accuracy
ground
truth
[accuracy]
Metric1
Metric2
89.2%
99.5%
100% 93.2%
89.3%
85.1%
Figure 5: Software lineage inference overview
email, DNS, FTP, IRC, and so on. For example, Blaster-
Worm in our data set sent exploit packets and propagated
itself via TFTP only when there were (simulated) live
vulnerable hosts.
For the scalability reason, we use the feature hash-
ing technique [20, 44] to encode extracted features into
bit-vectors. For example, let bv1 and bv2 denote two
bit-vectors generated from f1 and f2 using feature hash-
ing. Then the symmetric distance in Equation 1 can be
calculated by:
SDbv(bv1,bv2) =S (bv1 ⊗ bv2)
(2)
where ⊗ denotes bitwise-XOR and S(·) means the number
of bits set to one.
7 Evaluation
As depicted in Figure 5, we systematically evaluated our
lineage inference algorithms using (i) ILINE to explore
all the design spaces described in Figure 1 with a variety
of data sets and (ii) IEVAL to measure the accuracy of our
outputs with respect to the ground truth.
7.1 Straight Line Lineage
7.1.1 Data sets
For straight line lineage experiments, we have collected
three different kinds of goodware data sets, e.g., con-
tiguous revisions, released versions, and actual release
binaries, and malware data sets.
i) Contiguous Revisions: Using a commit history
from a version control system, e.g., subversion and git,
we downloaded contiguous revisions of a program. The
time gap between two adjacent commits varies a lot, from
<10 minutes to more than a month. We excluded some
revisions that changed only comments because they did
not affect the resulting program binaries.
Programs # revisions
memcached
redis
redislite
Last rev
First rev
Period
124 2008-10-14 2012-02-02 3.3 yr
158 2011-09-29 2012-03-28 0.5 yr
89 2011-06-02 2012-01-18 0.6 yr
Table 2: Data sets of contiguous revisions
In order to set up idealized experiment environments, we
compiled every revision with the same compiler and the
same compiling options. We excluded variations that can
come from the use of different compilers.
ii) Released Versions: We downloaded only released
versions of a program meant to be distributed to end users.
For example, subversion maintains them under the tags
folder. The difference with contiguous revisions is that
contiguous revisions may have program bugs (commit-
ted before testing) or experimental functionalities that
would be excluded in released versions. In other words,
released versions are more controlled data sets. We com-
piled source code with the same compiler and the same
compiling options for ideal settings.
Programs #
grep
nano
redis
sendmail
openssh
First release
Date
Last release
Date
Ver
Ver
2.0
releases
19
1993-05-22 2.11 2012-03-02 18.8 yr
114 0.7.4 2000-01-09 2.3.1 2011-05-10 11.3 yr
48
2.6 yr
38 8.10.0 2000-03-03 8.14.5 2011-05-15 11.2 yr
52 2.0.0 2000-05-02 5.9p1 2011-09-06 11.4 yr
2009-09-03 2.4.10 2012-03-30
1.0
Period
Table 3: Data sets of released versions
iii) Actual Release Binaries: We collected binaries (not
source code) of released versions from rpm or deb pack-
age ﬁles.
Programs #
First release
Date
Last release
Date
Period
Ver
Ver
2.0-3
grep
nano
redis
sendmail
openssh
FileZilla
p7zip
2009-08-02 2.11-3 2012-04-17
ﬁles
37
2.7 yr
69 0.7.9-1 2000-01-24 2.2.6-1 2010-11-22 10.8 yr
2.9 yr
39 0.094-1 2009-05-06 2.4.9-1 2012-03-26
6.1 yr
41 8.13.3-6 2005-03-12 8.14.4-2 2011-04-21
7.1 yr
75 3.9p1-2 2005-03-12 5.9p1-5 2012-04-02
2007-09-13
62
2012-01-08
4.3 yr
2004-08-21 9.20.1 2011-03-16
6.6 yr
32
Table 4: Data sets of actual release binaries
3.0.0
0.91
3.5.3
The difference is that we did not have any control over the
compiling process of the program, i.e., different programs
may be compiled with different versions of compilers
and/or optimization options. This data set is a represen-
tative of real-world scenarios where we do not have any
information about development environments.
iv) Malware: We used 84 samples with known lineage
collected by the Cyber Genome program. The data set
includes bots, worms, and Trojan horses and contains 7
clusters.
Cluster # samples
Cluster # samples
10
MC1
MC5
17 BlasterWorm MC6
MC2
15 MiniPanzer.A MC7
MC3
7 CleanRoom.A
MC4
Table 5: Data sets of malware
10 CleanRoom.B
15 MiniPanzer.B
10 CleanRoom.C
Family
KBot
Family
USENIX Association  
22nd USENIX Security Symposium  89
File Size
Cyclomatic Complexity
)
B
K
(
e
z
i
S
e
l
i
F
 250
 245
 240
 235
 230
 225
 220
 215
 210
 205
 200
 1700
 1650
 1600
 1550
 1500
 1450
 1400
 1350