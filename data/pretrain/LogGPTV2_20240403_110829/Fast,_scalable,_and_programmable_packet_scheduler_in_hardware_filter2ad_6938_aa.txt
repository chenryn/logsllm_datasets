title:Fast, scalable, and programmable packet scheduler in hardware
author:Vishal Shrivastav
Fast, Scalable, and Programmable Packet Scheduler in Hardware
Vishal Shrivastav
Cornell University
ABSTRACT
With increasing link speeds and slowdown in the scaling of CPU
speeds, packet scheduling in software is resulting in lower preci-
sion and higher CPU utilization. By offloading packet scheduling
to the hardware such as a NIC, one can potentially overcome these
drawbacks. However, to retain the flexibility of software packet
schedulers, packet scheduler in hardware must be programmable,
while also being fast and scalable. State-of-the-art packet schedulers
in hardware either compromise on scalability (Push-In-First-Out
(PIFO)) or the ability to express a wide range of packet schedul-
ing algorithms (First-In-First-Out (FIFO)). Further, even a general
scheduling primitive like PIFO is not expressive enough to express
certain key classes of packet scheduling algorithms. Hence in this
paper, we propose a generalization of the PIFO primitive, called
Push-In-Extract-Out (PIEO), which like PIFO, maintains an ordered
list of elements, but unlike PIFO which only allows dequeue from
the head of the list, PIEO allows dequeue from arbitrary positions
in the list by supporting a programmable predicate-based filtering
at dequeue. Next, we present a fast and scalable hardware design of
PIEO scheduler and prototype it on a FPGA. Overall, PIEO scheduler
is both more expressive and over 30× more scalable than PIFO.
CCS CONCEPTS
• Networks → Programmable networks; • Hardware → Net-
working hardware;
KEYWORDS
Programmable Packet Scheduling; Networking Hardware
1 INTRODUCTION
"In the era of hardware-accelerated computing,
identify and offload common abstractions and primitives,
not individual algorithms and protocols."
A packet scheduler enforces a scheduling algorithm or a scheduling
policy which specifies when and in what order to transmit packets
on the wire. Implementing a packet scheduler in software gives one
the flexibility to quickly experiment with and adopt new scheduling
algorithms and policies. However, this flexibility comes at the cost
of burning CPU cycles which could have otherwise been used for
running applications. In public cloud networks, this translates to
loss in revenue [15], as the CPU overhead of packet scheduling [31]
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-5956-6/19/08...$15.00
https://doi.org/10.1145/3341302.3342090
367
takes away from the processing power available to customer VMs.
Unfortunately, this issue is only getting worse [3] with the growing
mismatch between increase in link speeds and slowdown [11, 14]
in the scaling of CPU speeds.
Next, with increasing link speeds, the time budget to make sched-
uling decisions is also getting smaller, e.g., at 100 Gbps link speeds,
to enforce a scheduling policy at MTU timescale precision, a sched-
uling decision needs to be made every 120 ns. To put this in per-
spective, a single DRAM access takes about a 100 ns. Further, new
transport protocols, such as Fastpass [30], QJump [16], and Eth-
ernet TDMA [41], as well as recently proposed circuit-switched
designs [35, 21, 42, 25], and protocols that rely on very accurate
packet pacing [2, 19], require packets to be transmitted at pre-
cise times on the wire, in some cases at nanosecond-level preci-
sion [35]. Meeting these stringent requirements in software is chal-
lenging [31, 22, 28, 2, 35], mainly due to non-deterministic software
processing jitter, and lack of high resolution software timers.
A packet scheduler in the hardware, such as a NIC, can poten-
tially overcome the aforementioned limitations of software packet
schedulers [31]. However, to retain the flexibility of software packet
schedulers, packet scheduler in the hardware must be programmable.
Further, today’s multi-tenant cloud networks rely heavily on virtu-
alization, with hundreds of VMs or light-weight containers running
on the same physical machine. This can result in tens of thousands
of traffic classes or flows per end-host [32, 31]. Hence, the packet
scheduler must also be scalable.
State-of-the-art packet schedulers in hardware are based on
one of the two scheduling primitives—(i) First-In-First-Out (FIFO),
which simply schedules elements in the order of their arrival, and
(ii) Push-In-First-Out (PIFO) [37], which provides a priority queue
abstraction, by maintaining an ordered list of elements (based on
a programmable rank function) and always scheduling from the
head of the ordered list ("smallest ranked" element). Unfortunately,
packet schedulers based on these primitives either compromise on
scalability (PIFO-based scheduler), or the ability to express a wide
range of packet scheduling algorithms (FIFO-based scheduler). Fur-
ther, even a general scheduling primitive like PIFO is not expressive
enough to express certain key classes of packet scheduling algo-
rithms (§2.3). Hence in this paper, we propose a new programmable
packet scheduler in hardware, which is fast, scalable, and more
expressive than state-of-the-art.
To design a programmable packet scheduler, we use the insight
that most packet scheduling algorithms have to make two key deci-
sions in the process of scheduling: (i) when an element (packet/flow)
becomes eligible for scheduling (decided by some programmable
predicate as a function of time), and (ii) in what order to sched-
ule amongst the set of eligible elements (decided by some pro-
grammable rank function). To that end, at any given time, packet
scheduling algorithms first filter the set of elements eligible for
scheduling at the time (using the predicate function), and then
schedule the smallest ranked element from that set (§2.2, §2.3, §4).
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Vishal Shrivastav
Hence, most packet scheduling algorithms can be abstracted as
the following scheduling policy—At any given time, schedule the
"smallest ranked eligible" element.
Next, to realize the policy of scheduling the "smallest ranked eli-
gible" element, one needs a primitive that provides abstractions for:
(i) predicate-based filtering, and (ii) selecting the smallest element
within an arbitrary subset of elements. Unfortunately, state-of-the-
art priority queue based primitives such as PIFO do not provide
these abstractions. Hence in this paper, we propose a new schedul-
ing primitive called Push-In-Extract-Out (PIEO) (§3.1), which can be
seen as a generalization of the PIFO primitive. PIEO associates with
each element a rank and an eligibility predicate, both of which can
be programmed based on the choice of the scheduling algorithm.
Next, PIEO maintains an ordered list of elements in the increasing
order of rank, by always enqueuing elements at the appropriate
position in the list based on the element’s rank ("Push-In" primi-
tive). And finally, for scheduling, PIEO first filters out the subset of
elements from the ordered list whose eligibility predicates are true
at the time, and then dequeues the element at the smallest index in
that subset ("Extract-Out" primitive). Hence, PIEO always schedules
the "smallest ranked eligible" element. Further, we use the insight
that for most packet scheduling algorithms, the time an element
becomes eligible for scheduling (teliдible) can be calculated at en-
queue into the ordered list, and the eligibility predicate evaluation
for filtering at dequeue usually reduces to (tcurr ent ≥ teliдible).
Here t could be any monotonic increasing function of time. This
insight enables a very efficient hardware implementation of the
PIEO scheduler (§5). Finally, we present a programming framework
for the PIEO scheduler (§3.2), using which we show that one can
express a wide range of packet scheduling algorithms (§4).
PIEO primitive maintains an ordered list of elements ("Push-
In"), atop which filtering and smallest rank selection happens at
dequeue ("Extract-Out"). However, implementing both a fast and
scalable ordered list in the hardware is challenging, as it presents
a fundamental trade-off between time complexity and hardware
resource consumption. E.g., using O(1) comparators and flip-flops,
one would need O(N) time to enqueue an element in an ordered list
of size N , assuming the list is stored as an array1 in memory. On the
flip side, to enqueue in O(1) time, designs such as PIFO [37] exploit
the massive parallelism in hardware by storing the entire list in
flip-flops and associating a comparator with each element in the list
following the classic parallel compare-and-shift architecture [29],
thus requiring O(N) flip-flops and comparators, which limits the
scalability of such a design [37]. In this paper, we present a hardware
design (§5) of an ordered list for the PIEO scheduler which is both
fast and scalable. In particular, our design executes both "Push-In"
and "Extract-Out" primitive operations in O(1) time (four clock
cycles), while requiring only O(√
N) flip-flops and comparators,
while the ordered list sits entirely in SRAM.
To demonstrate the feasibility of our hardware design of the PIEO
scheduler, we prototype2 it on a FPGA (§6). Our prototype executes
each primitive operation in few tens of nanoseconds (which is
1Linked list implementation also has overall time complexity of O(N). Even proba-
bilistic datastructures such as skip lists have average time complexity of O(loд(N)),
and worst-case time complexity of O(N).
2Code for the FPGA implementation of PIEO scheduler is available at
https://github.com/vishal1303/PIEO-Scheduler
368
Figure 1: A generic packet scheduling model.
sufficient to schedule MTU-sized packets at 100 Gbps line-rate),
while also scales to tens of thousands of flows (over 30× more
scalable than PIFO). To further evaluate the performance of our
prototype, we program it with a two-level hierarchical scheduling
algorithm implementing rate-limit and fair queue policies. We show
that our prototype could very accurately enforce these policies atop
FPGAs with 40 Gbps interface bandwidth (§6.3).
This work does not raise any ethical issues.
2 BACKGROUND
2.1 Packet scheduling model
Fig. 1 shows the packet scheduling model assumed in this paper.
Packets ready to be scheduled for transmission are stored in one of
the flow queues or traffic classes3. Packets within each flow queue
are scheduled in FIFO order, whereas packets across queues are
scheduled according to some custom packet scheduling algorithm or
policy, which specifies when and in what order packets from each
queue should be transmitted on the wire. To facilitate scheduling, a
custom scheduling state is maintained in memory. Typically, this
state could also be accessed and configured by the control plane.
And finally, a packet scheduler is used to express and enforce the
chosen scheduling algorithm/policy. The focus of this paper is to
design an efficient packet scheduler in hardware, which could be
programmed to express a wide range of packet scheduling algo-
rithms/policies, in a fast and scalable manner.
2.2 Packet scheduling algorithms
Most packet scheduling algorithms can be abstracted as the follow-
ing scheduling policy:
Assign each element (packet/flow) an eligibility predicate and a rank.
Whenever the link is idle, among all elements whose predicates are
true, schedule the one with the smallest rank.
The predicate determines when an element becomes eligible for
scheduling, while rank decides in what order to schedule amongst
the eligible elements. By appropriately choosing the predicate and
rank functions, one can express a wide range of packet schedul-
ing algorithms (§4). Further, packet scheduling algorithms can be
broadly classified into two key classes:
3We use flow queues and traffic classes interchangeably in the paper.
per ﬂow FIFO queuescontrolplane* focus ofthis paperPacket schedulerscheduling algorithm / policyschedulingstatePacketon wireIncomingpacketsto bescheduledFast, Scalable, and Programmable Packet Scheduler in Hardware
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Figure 2: (a) WF2Q+ algorithm [5]. L is the length of packet at the head of flow queue, r is the rate for flow f , x is the transmis-
sion length of current packet being transmitted, and F is the set of back-logged flows. (b) Packets at the head of six different
flows in the example system, where packets can be of different sizes (transmission length). We also show start and finish times
for each packet. (c) Scheduling order in an ideal run of WF2Q+. (d) and (e) scheduling orders when running WF2Q+ using PIFO.
Work-conserving algorithms. This class of packet scheduling
algorithms ensure that the network link is never idle as long as
there are outstanding packets to send. Hence, in these algorithms,
the eligibility predicate of at least one active element is always
true, and whenever the link is idle, the next eligible element in the
order of rank is scheduled. Examples of work-conserving packet
scheduling algorithms include Deficit Round Robin (DDR) [34],
Weighted Fair Queuing (WFQ) [13], Worst-case Fair Weighted Fair
Queuing (WF2Q) [5], and Stochastic Fairness Queuing (SFQ) [23].
Non-work conserving algorithms. Under this class of packet
scheduling algorithms, the network link can be idle even when
there are outstanding packets to send, i.e., the eligibility predicate
associated with each active element could all be false at the same
time. Non-work conserving packet scheduling algorithms gener-
ally specify the time to schedule an element, and the eligibility
predicate for an element p generally takes the form (tcurr ent ≥
p.teliдible). Non-work conserving algorithms are naturally suited
to express traffic shaping primitives such as rate-limiting and packet
pacing [31, 32]. A classic example of a non-work conserving packet
scheduling algorithm is Token Bucket [50].
2.3 Packet scheduling primitives
First-In-First-Out (FIFO). FIFO is the most basic scheduling prim-
itive, which simply schedules elements in the order of their arrival.
As a result, FIFO primitive is not able to express a wide range of
packet scheduling algorithms. And yet, FIFO based schedulers are
the most common packet schedulers in hardware, as their simplicity
enables both fast and scalable scheduling.
Push-In-First-Out (PIFO) [37]. PIFO primitive assigns each ele-
ment a programmable rank value, and at any given time, schedules
the "smallest ranked" element. To realize this abstraction, PIFO
maintains an ordered list of elements in the increasing order of