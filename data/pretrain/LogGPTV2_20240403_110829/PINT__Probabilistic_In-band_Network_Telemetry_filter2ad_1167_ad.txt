32-bit long, while networks have much fewer than 232 switches).
Instead of fragmenting the values to meet the q-bits query bit bud-
get, we leverage hashing. Specifically, we use another global hash
function h that maps (value, packet ID) pairs into q-bit bitstrings.
When encoder ei sees a packet pj , if it needs to act it uses h(Mi , pj)
to modify the digest. In the Baseline scheme ei will write h(Mi , pj)
on pj , and in the XOR scheme it will xor h(Mi , pj) onto its current
digest. As before, the Recording Module checks the hop numbers
that modified the packet. The difference is in how the Inference
Module works – for each hop number i, we wish to find a single
value v ∈ V that agrees with all the Baseline packets from hop i. For
example, if p1 and p2 were Baseline packets from hop i, Mi must
be a value such that h(Mi , p1) = p1.dig and h(Mi , p2) = p2.dig. If
there is more than one such value, the inference for the hop is not
complete and we require additional packets to determine it. Once
a value of a block Mi is determined, from any digest pj that was
xor-ed by the i’th encoder, we xor h(Mi , pj) from pj .dig. This way,
the number of unknown blocks whose hashes xor-ed pj decreases by
one. If only one block remains, we can treat it similarly to a Baseline
packet and use it to reduce the number of potential values for that
block. Another advantage of the hashing technique is that it does not
assume anything about the width of the values (e.g., switch IDs), as
long as each is distinct.
Reducing the Decoding Complexity. Our description of the en-
coding and decoding process thus far requires processing is super-
quadratic (ω(k2)) in k. That is because we need ≈ k log log∗ k pack-
ets to decode the message, and we spend O(k) time per packet in
computing the д function to determine which encoders modified its
digest. We now present a variant that reduces the processing time
to nearly linear in k. Intuitively, since the probability of changing
a packet is Ω(1/k), the number of random bits needed to determine
which encoders modify it is O(k log k). Previously, each encoder
used the global function д to get O(log k) pseudo-random bits and
decide whether to change the packet. Instead, we can use д to create
O(log 1/p) = O(log k) pseudo-random k-bit vectors. Intuitively, each
bit in the bitwise-and of these vectors will be set with probability p
(as defined by the relevant XOR layer). The i’th encoder will modify
the packet if the i’th bit is set in the bitwise-and of the vectors9.
At the Recording Module, we can compute the set of encoders that
modify a packet in time O(log k) by drawing the random bits and us-
ing their bitwise-and. Once we obtain the bitwise-and vector we can
extract a list of set bits in time O(#set bits) using bitwise operations.
Since the average number of set bits is O(1), the overall per-packet
complexity remains O(log k) and the total decoding time becomes
O(k log k log log∗ k). We note that this improvement assumes that k
9This assumes that the probability is a power of two, or provides a
it. By repeating the process we can get a better approximation.
√
2 approximation of
8
fits in O(1) machine words (e.g., k ≤ 256) and that encoders can do
O(log k) operations per packet.
Improving Performance via Multiple Instantiations. The num-
ber of packets PINT needs to decode the message depends on the
query’s bit-budget. However, increasing the number of bits in the
hash may not be the best way to reduce the required number of
packets. Instead, we can use multiple independent repetitions of the
algorithm. For example, given an 8-bit query budget, we can use two
independent 4-bit hashes.
log(1+ε)2 v(pj , s)(cid:105)
digest a(pj , s) ≜(cid:104)
4.3 Approximating Numeric Values
Encoding an exact numeric value on packet may require too many
bits, imposing an undesirable overhead. For example, the 32-bit la-
tency measurements that INT collects may exceed the bit-budget. We
now discuss to compress the value, at the cost of introducing an error.
Multiplicative approximation. One approach to reducing the num-
ber of bits required to encode a value is to write on the packet’s
instead of v(pj , s). Here, the [·]
operator rounds the quantity to the closest integer. At the Inference
Module, we can derive a (1 + ε)-approximation of the original value
by computing (1 + ε)2·a(pj,s). For example, if we want to compress
a 32-bit value into 16 bits, we can set ε = 0.0025.
Additive approximation. If distinguishing small values is not as
crucial as bounding the maximal error, we obtain better results
by encoding the value with additive error instead of multiplicative
error. For a given error target ∆ (thereby reducing the overhead by
, and
(cid:4)log2 ∆(cid:5) bits), the Encoding Module writes a(pj , s) ≜(cid:104) v(pj,s)
the Inference Module computes (2∆) · a(pj , s).
Randomized counting. For some aggregation functions, the aggre-
gation result may require more bits than encoding a single value.
For example, in a per-packet aggregation over a k-hop path with
q-bit values, the sum may require q + log k bits to write explic-
itly while the product may take q · k bits. This problem is espe-
cially evident if q is small (e.g., a single bit specifying whether
the latency is high). Instead, we can take a randomized approach
to increase the value written on a packet probabilistically. For ex-
ample, we can estimate the number of high-latency hops or the
end-to-end latency to within a (1 + ε)-multiplicative factor using
O(log ε−1 + log log(2q · k · ε2))) bits [55].
Example #3: Per-packet aggregation. Here, we wish to summa-
rize the data across the different values in the packet’s path. For
example, HPCC [46] collects per-switch information carried by INT
data, and adjusts the rate at the end host according to the highest link
utilization along the path. To support HPCC with PINT, we have two
key insights: (1) we just need to keep the highest utilization (i.e., the
bottleneck) in the packet header, instead of every hop; (2) we can
use the multiplicative approximation to further reduce the number of
bits for storing the utilization. Intuitively, PINT improves HPCC as
it reduces the overheads added to packets, as explained in Section 2.
In each switch, we calculate the utilization as in HPCC, with slight
tuning to be supported by switches (discussed later). The multipli-
cation is calculated using log and exp based on lookup tables [67].
The result is encoded using multiplicative approximation. To further
,
eliminate systematic error, we write a(pj , s) ≜(cid:104)
log(1+ε)2 v(pj , s)(cid:105)
(cid:105)
2∆
R
(a) Web search workload (large flows)
(b) Web search workload
(c) Hadoop workload
Figure 7: Comparison of the 95th-percentile slowdown of the standard INT-based HPCC and the PINT-based HPCC. PINT improves the performance
for the long flows due to its reduced overheads. In (b) and (c), the network load is 50% and the x-axis scale is chosen such that there are 10% of the
flows between consecutive tick marks.
query’s bit budget, and the last writes the digest. If we use more
than one hash for the query, both can be executed in parallel as
they are independent.
Computing the median/tail latency (dynamic per-flow aggrega-
tion) also requires four pipeline stages: one for computing the latency,
one for compressing it to meet the bit budget; one to compute д; and
one to overwrite the value if needed.
Our adaptation of the HPCC congestion control algorithm re-
quires six pipeline stages to compute the link utilization, followed
by a stage for approximating the value and another to write the di-
gest. For completeness, we elaborate on how to implement in the
data plane the different arithmetic operations needed by HPCC in
Appendix C. We further note that running it may require that the
switch would need to perform the update of U in a single stage. In
other cases, we propose to store the last n values of U on separate
stages and update them in a round-robin manner, for some integer n.
This would mean that our algorithm would need to recirculate every
n’th packet as the switch’s pipeline is one-directional.
Since the switches have a limited number of pipeline stages, we
parallelize the processing of queries as they are independent of each
other. We illustrate this parallelism for a combination of the three
use cases of PINT.We start by executing all queries simultaneously,
writing their results on the packet vector. Since HPCC requires more
stages than the other use cases, we concurrently compute which
query subset to run according to the distribution selected by the
Query Engine (see §3.4). We can then write the digests of all the
selected queries without increasing the number of stages compared
with running HPCC alone. The switch layout for such a combination
is illustrated in Fig. 6.
6 EVALUATION
We evaluate on the three use cases discussed on §3.2.
6.1 Congestion Control
We evaluate how PINT affects the performance of HPCC [46] using
the same simulation setting as in [46]. Our goal is not to propose a
new congestion control scheme, but rather to present a low-overhead
approach for collecting the information that HPCC utilizes. We use
NS3 [76] and a FatTree topology with 16 Core switches, 20 Agg
switches, 20 ToRs, and 320 servers (16 in each rack). Each server has
a single 100Gbps NIC connected to a single ToR. The capacity of
Figure 6: Layout illustration for two path tracing hashes, alongside a
latency query, and a congestion control query.
the [·]R randomly performs floor or ceiling, with a probability distri-
bution that gives an expected value equals to log(1+ε)2 v(pj , s). This
way, some packets will overestimate the utilization while others
underestimate it, thus resulting in the correct value on average. In
practice, we just need 8 bits to support ε = 0.025.
Tuning HPCC calculation for switch computation. We maintain
the exponential weighted moving average (EWMA) of link utiliza-
tion U of each link in the switch. U is updated on every packet with:
U = T−τ
B·τ is the new sample for
T
updating U . Here, T is the base RTT and B is the link bandwidth
(both are constants). Intuitively, the weight of the EWMA, τ
T , corre-
sponds to each new packet’s time occupation τ . The calculation of u
also corresponds to each new packet: byte is the packet’s size, and
qlen is the queue length when the packet is dequeued10.
To calculate the multiplications, we first do the following transfor-
mation: U = T−τ
B·T . Then we calculate the multiplica-
tions using logarithm and exponentiation as detailed in Appendix B.
T · u, where u = qlen
B·T
· U + τ
+ byte
T ·U + qlen·τ
B·T 2 + byte
IMPLEMENTATION
5
PINT is implemented using the P4 language and can be deployed
on commodity programmable switches. We explain how each of our
use cases is executed.
For running the path tracing application (static per-flow aggre-
gation), we require four pipeline stages. The first chooses a layer,
another computes д, the third hashes the switch ID to meet the
10This is slightly different from HPCC, where the calculation is done in the host,
which can only see packets of its own flow. Therefore, the update is scaled for
packets of the same flow (τ is time gap between packets of the same flow, and
byte includes the bytes from other flows in between). Here, the update is performed
on all packets on the same link. Since different flows may interleave on the link,
our calculation is more fine-grained.
9
010203040506070Goodput[Gbps]HPCC(PINT)HPCC(INT)203040506070Network Load [%]020406080Gain [%]7K20K30K50K73K197K989K2M5M30MFlow Size [Bytes]246810SlowdownHPCC(INT)HPCC(PINT)3243995005996999997K46K120K10MFlow Size [Bytes]246810SlowdownHPCC(INT)HPCC(PINT)Switch PipelineStage 1Stage 2Stage 3Stage 4Stage 5Stage 6Stage 7Stage 8HPCC ArithmeticsHPCC ArithmeticsHPCC ArithmeticsHPCC ArithmeticsHPCC ArithmeticsValueCompressionWrite DigestWrite DigestCompute LatencyValueCompressionCompute 𝑔Choose Layer 1Choose Layer 2Compute 𝑔Compute 𝑔Compute hashCompute hashWrite DigestWrite DigestHPCC ArithmeticsChoose a query subset(a) Web search workload
(b) Hadoop workload
Figure 8: The 95th-percentile slowdown of running PINT-based HPCC
(at 50% network load) on p-fraction of the packets. On both workloads,
the performance of running it on 1/16 of the packets produces similar
results to running it on all.
each link between Core and Agg switches, as well as Agg switches
and ToRs, are all 400Gbps. All links have a 1µs propagation delay,
which gives a 12µs maximum base RTT. The switch buffer size is
32MB. The traffic is generated following the flow size distribution
in web search from Microsoft [3] and Hadoop from Facebook [62].
Each server generates new flows according to a Poisson process,
destined to random servers. The average flow arrival time is set
so that the total network load is 50% (not including the header
bytes). We use the recommended setting for HPCC: WAI = 80 bytes,
maxStaдe = 0, η = 95%, and T = 13µs.
The results, depicted in Fig. 7(b) and Fig. 7(c), show that PINT has
similar performance (in terms of slowdown) to HPCC, despite using
just 8 bits per packet. Here, slowdown refers to the ratio between
the completion time of the flow in the presence of other flows and
alone. Specifically, PINT has better performance on long flows while
slightly worse performance on short ones. The better performance
on long flows is due to PINT’s bandwidth saving. Fig. 7(a) shows the
relative goodput improvement, averaged over all flows over 10MB,
of using PINT at different network load. At higher load, the byte
saving of PINT brings more significant improvement. For example,
at 70% load, using PINT improves the goodput by 71%. This trend
aligns with our observation in §2.
To evaluate how the congestion control algorithm would perform
alongside other queries, we experiment in a setting where only a
p = 1, 1/16, 1/256 fraction of the packets carry the query’s digest.
As shown in Fig. 8(a) and Fig. 8(b), the performance only slightly
degrades for p = 1/16. This is expected, because the bandwidth-
delay product (BDP) is 150 packets, so there are still 9.4 (≈150/16)
packets per RTT carrying feedback. Thus the rate is adjusted on
average once per 1/9.4 RTT (as compared to 1/150 RTT with per-
packet feedback), which is still very frequent. With p = 1/256,
the performance of short flows degrades significantly, because it
takes longer than an RTT to get feedback. The implication is that
congestion caused by long flows is resolved slowly, so the queue
lasts longer, resulting in higher latency for short flows. The very long
flows (e.g., > 5MB) also have worse performance. The reason is that
they are long enough to collide with many shorter flows, so when
the competing shorter flows finish, the long flows have to converge
back to the full line rate. With p = 1/256, it takes much longer time
to converge than with smaller p.
Figure 9: PINT error on estimating latency quantiles with a sketch
(PINTS ) and without. In the first row, the sketch has 100 digests; in
the second, the sample has 500 packets.
In principle, the lower feedback frequency p only affects the con-
vergence speed as discussed above, but not the stability and fairness.
Stability is guaranteed by no overreaction, and HPCC’s design of ref-
erence window (constant over an RTT) provides this regardless of p.
Fairness is guaranteed by additive-increase-multiplicative-decrease
(AIMD), which is preserved regardless of p.
6.2 Latency Measurements
Using the same topology and workloads as in our congestion control
experiments, we evaluate PINT’s performance on estimating latency
quantiles. We consider PINT in four scenarios, using b = 4 and
b = 8 bit-budgets, with sketches (denoted PINT S ), and without. In
our experiment, we have used the, state of the art, KLL sketch [39].
The results, appearing in Fig. 9, show that when getting enough
packets, the error of the aggregation becomes stable and converges
to the error arising from compressing the values. As shown, by
compressing the incoming samples using a sketch (e.g., that keeps
100 identifiers regardless of the number of samples), PINT accuracy
degrades only a little even for small 100B sketches. We conclude
that such sketches offer an attractive space to accuracy tradeoff.
6.3 Path Tracing
We conduct these experiments on Mininet [52] using two large-
diameter (denoted D) ISP topologies (Kentucky Datalink and US