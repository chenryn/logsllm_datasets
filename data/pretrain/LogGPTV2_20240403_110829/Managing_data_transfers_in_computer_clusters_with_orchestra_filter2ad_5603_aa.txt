title:Managing data transfers in computer clusters with orchestra
author:Mosharaf Chowdhury and
Matei Zaharia and
Justin Ma and
Michael I. Jordan and
Ion Stoica
Managing Data Transfers in Computer Clusters with
Orchestra
Mosharaf Chowdhury, Matei Zaharia, Justin Ma, Michael I. Jordan, Ion Stoica
{mosharaf, matei, jtma, jordan, istoica}@cs.berkeley.edu
University of California, Berkeley
ABSTRACT
Cluster computing applications like MapReduce and Dryad transfer
massive amounts of data between their computation stages. These
transfers can have a signiﬁcant impact on job performance, ac-
counting for more than 50% of job completion times. Despite this
impact, there has been relatively little work on optimizing the per-
formance of these data transfers, with networking researchers tra-
ditionally focusing on per-ﬂow trafﬁc management. We address
this limitation by proposing a global management architecture and
a set of algorithms that (1) improve the transfer times of common
communication patterns, such as broadcast and shufﬂe, and (2) al-
low scheduling policies at the transfer level, such as prioritizing a
transfer over other transfers. Using a prototype implementation, we
show that our solution improves broadcast completion times by up
to 4.5× compared to the status quo in Hadoop. We also show that
transfer-level scheduling can reduce the completion time of high-
priority transfers by 1.7×.
Categories and Subject Descriptors
C.2 [Computer-communication networks]: Distributed systems—
Cloud computing
General Terms
Algorithms, design, performance
Keywords
Data-intensive applications, data transfer, datacenter networks
1 Introduction
The last decade has seen a rapid growth of cluster computing frame-
works to analyze the increasing amounts of data collected and gen-
erated by web services like Google, Facebook and Yahoo!. These
frameworks (e.g., MapReduce [15], Dryad [28], CIEL [34], and
Spark [44]) typically implement a data ﬂow computation model,
where datasets pass through a sequence of processing stages.
Many of the jobs deployed in these frameworks manipulate mas-
sive amounts of data and run on clusters consisting of as many
as tens of thousands of machines. Due to the very high cost of
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’11, August 15-19, 2011, Toronto, Ontario, Canada.
Copyright 2011 ACM 978-1-4503-0797-0/11/08 ...$10.00.
these clusters, operators aim to maximize the cluster utilization,
while accommodating a variety of applications, workloads, and
user requirements. To achieve these goals, several solutions have
recently been proposed to reduce job completion times [11,29,43],
accommodate interactive workloads [29, 43], and increase utiliza-
tion [26, 29]. While in large part successful, these solutions have
so far been focusing on scheduling and managing computation and
storage resources, while mostly ignoring network resources.
However, managing and optimizing network activity is critical
for improving job performance. Indeed, Hadoop traces from Face-
book show that, on average, transferring data between successive
stages accounts for 33% of the running times of jobs with reduce
phases. Existing proposals for full bisection bandwidth networks
[21, 23, 24, 35] along with ﬂow-level scheduling [10, 21] can im-
prove network performance, but they do not account for collective
behaviors of ﬂows due to the lack of job-level semantics.
In this paper, we argue that to maximize job performance, we
need to optimize at the level of transfers, instead of individual
ﬂows. We deﬁne a transfer as the set of all ﬂows transporting
data between two stages of a job. In frameworks like MapReduce
and Dryad, a stage cannot complete (or sometimes even start) be-
fore it receives all the data from the previous stage. Thus, the job
running time depends on the time it takes to complete the entire
transfer, rather than the duration of individual ﬂows comprising
it. To this end, we focus on two transfer patterns that occur in
virtually all cluster computing frameworks and are responsible for
most of the network trafﬁc in these clusters: shufﬂe and broad-
cast. Shufﬂe captures the many-to-many communication pattern
between the map and reduce stages in MapReduce, and between
Dryad’s stages. Broadcast captures the one-to-many communica-
tion pattern employed by iterative optimization algorithms [45] as
well as fragment-replicate joins in Hadoop [6].
We propose Orchestra, a global control architecture to manage
intra- and inter-transfer activities.
In Orchestra, data movement
within each transfer is coordinated by a Transfer Controller (TC),
which continuously monitors the transfer and updates the set of
sources associated with each destination. For broadcast transfers,
we propose a TC that implements an optimized BitTorrent-like pro-
tocol called Cornet, augmented by an adaptive clustering algorithm
to take advantage of the hierarchical network topology in many dat-
acenters. For shufﬂe transfers, we propose an optimal algorithm
called Weighted Shufﬂe Scheduling (WSS), and we provide key
insights into the performance of Hadoop’s shufﬂe implementation.
In addition to coordinating the data movements within each trans-
fer, we also advocate managing concurrent transfers belonging to
the same or different jobs using an Inter-Transfer Controller (ITC).
We show that an ITC implementing a scheduling discipline as sim-
ple as FIFO can signiﬁcantly reduce the average transfer times in
Figure 1: CDF of the fraction of time spent in shufﬂe transfers
in Facebook Hadoop jobs with reduce phases.
a multi-transfer workload, compared to allowing ﬂows from the
transfers to arbitrarily share the network. Orchestra can also readily
support other scheduling policies, such as fair sharing and priority.
Orchestra can be implemented at the application level and over-
laid on top of diverse routing topologies [10,21,24,35], access con-
trol schemes [12,22], and virtualization layers [37,38]. We believe
that this implementation approach is both appropriate and attractive
for several reasons. First, because large-scale analytics applica-
tions are usually written using high-level programming frameworks
(e.g., MapReduce), it is sufﬁcient to control the implementation of
the transfer patterns in these frameworks (e.g., shufﬂe and broad-
cast) to manage a large fraction of cluster trafﬁc. We have focused
on shufﬂes and broadcasts due to their popularity, but other trans-
fer patterns can also be incorporated into Orchestra. Second, this
approach allows Orchestra to be used in existing clusters without
modifying routers and switches, and even in the public cloud.
To evaluate Orchestra, we built a prototype implementation in
Spark [44], a MapReduce-like framework developed and used at
our institution, and conducted experiments on DETERlab and Ama-
zon EC2. Our experiments show that our broadcast scheme is up
to 4.5× faster than the default Hadoop implementation, while our
shufﬂe scheme can speed up transfers by 29%. To evaluate the im-
pact of Orchestra on job performance, we run two applications de-
veloped by machine learning researchers at our institution—a spam
classiﬁcation algorithm and a collaborative ﬁltering job—and show
that our broadcast and shufﬂe schemes reduce transfer times by up
to 3.6× and job completion times by up to 1.9×. Finally, we show
that inter-transfer scheduling policies can lower average transfer
times by 31% and speed up high-priority transfers by 1.7×.
The rest of this paper is organized as follows. Section 2 dis-
cusses several examples that motivate importance of data transfers
in cluster workloads. Section 3 presents the Orchestra architecture.
Section 4 discusses Orchestra’s inter-transfer scheduling. Section 5
presents our broadcast scheme, Cornet. Section 6 studies how to
optimize shufﬂe transfers. We then evaluate Orchestra in Section 7,
survey related work in Section 8, and conclude in Section 9.
2 Motivating Examples
To motivate our focus on transfers, we study their impact in two
cluster computing systems: Hadoop (using trace from a 3000-node
cluster at Facebook) and Spark (a MapReduce-like framework that
supports iterative machine learning and graph algorithms [44]).
Hadoop at Facebook: We analyzed a week-long trace from Face-
book’s Hadoop cluster, containing 188,000 MapReduce jobs, to
ﬁnd the amount of time spent in shufﬂe transfers. We deﬁned a
“shufﬂe phase" for each job as starting when either the last map
task ﬁnishes or the last reduce task starts (whichever comes later)
and ending when the last reduce task ﬁnishes receiving map out-
(a) Logistic Regression
(b) Collaborative Filtering
Figure 2: Per-iteration work ﬂow diagrams for our motivating
machine learning applications. The circle represents the master
node and the boxes represent the set of worker nodes.
puts. We then measured what fraction of the job’s lifetime was
spent in this shufﬂe phase. This is a conservative estimate of the
impact of shufﬂes, because reduce tasks can also start fetching map
outputs before all the map tasks have ﬁnished.
We found that 32% of jobs had no reduce phase (i.e., only map
tasks). This is common in data loading jobs. For the remaining
jobs, we plot a CDF of the fraction of time spent in the shufﬂe
phase (as deﬁned above) in Figure 1. On average, the shufﬂe phase
accounts for 33% of the running time in these jobs. In addition,
in 26% of the jobs with reduce tasks, shufﬂes account for more
than 50% of the running time, and in 16% of jobs, they account for
more than 70% of the running time. This conﬁrms widely reported
results that the network is a bottleneck in MapReduce [10, 21, 24].
Logistic Regression Application: As an example of an iterative
MapReduce application in Spark, we consider Monarch [40], a
system for identifying spam links on Twitter. The application pro-
cessed 55 GB of data collected about 345,000 tweets containing
links. For each tweet, the group collected 1000-2000 features re-
lating to the page linked to (e.g., domain name, IP address, and fre-
quencies of words on the page). The dataset contained 20 million
distinct features in total. The applications identiﬁes which features
correlate with links to spam using logistic regression [25].
We depict the per-iteration work ﬂow of this application in Fig-
ure 2(a). Each iteration includes a large broadcast (300 MB) and a
shufﬂe (190 MB per reducer) operation, and it typically takes the
application at least 100 iterations to converge. Each transfer acts
as a barrier: the job is held up by the slowest node to complete the
transfer.
In our initial implementation of Spark, which used the
the same broadcast and shufﬂe strategies as Hadoop, we found that
communication accounted for 42% of the iteration time, with 30%
spent in broadcast and 12% spent in shufﬂe on a 30-node cluster.
With such a large fraction of the running time spent on communica-
tion, optimizing the completion times of these transfers is critical.
Collaborative Filtering Application: As a second example of an
iterative algorithm, we discuss a collaborative ﬁltering job used by
a researcher at our institution on the Netﬂix Challenge data. The
goal is to predict users’ ratings for movies they have not seen based
on their ratings for other movies. The job uses an algorithm called
alternating least squares (ALS) [45]. ALS models each user and
each movie as having K features, such that a user’s rating for
a movie is the dot product of the user’s feature vector and the
movie’s. It seeks to ﬁnd these vectors through an iterative process.
Figure 2(b) shows the workﬂow of ALS. The algorithm alter-
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1CDFFraction of Job Lifetime Spent in Shuffle Phasex x compute gradients broadcast param shuffle gradients collect new param x x sum & regularize x x broadcast movie vectors collect updates x x update movie vectors broadcast  user vectors collect updates update user vectors Figure 3: Communication and computation times per iteration
when scaling the collaborative ﬁltering job using HDFS-based
broadcast.
nately broadcasts the current user or movie feature vectors, to allow
the nodes to optimize the other set of vectors in parallel. Each trans-
fer is roughly 385 MB. These transfers limited the scalability of the
job in our initial implementation of broadcast, which was through
shared ﬁles in the Hadoop Distributed File System (HDFS)—the
same strategy used in Hadoop. For example, Figure 3 plots the
iteration times for the same problem size on various numbers of
nodes. Computation time goes down linearly with the number of
nodes, but communication time grows linearly. At 60 nodes, the
broadcasts cost 45% of the iteration time. Furthermore, the job
stopped scaling past 60 nodes, because the extra communication
cost from adding nodes outweighed the reduction in computation
time (as can be seen at 90 nodes).
3 Orchestra Architecture
To manage and optimize data transfers, we propose an architecture
called Orchestra. The key idea in Orchestra is global coordination,
both within a transfer and across transfers. This is accomplished
through a hierarchical control structure, illustrated in Figure 4.
At the highest level, Orchestra has an Inter-Transfer Controller
(ITC) that implements cross-transfer scheduling policies, such as
prioritizing transfers from ad-hoc queries over batch jobs. The ITC
manages multiple Transfer Controllers (TCs), one for each transfer
in the cluster. TCs select a mechanism to use for their transfers
(e.g., BitTorrent versus a distribution tree for broadcast) based on
the data size, the number of nodes in the transfer, their locations,
and other factors. They also actively monitor and control the nodes
participating in the transfer. TCs manage the transfer at the gran-
ularity of ﬂows, by choosing how many concurrent ﬂows to open
from each node, which destinations to open them to, and when to
move each chunk of data. Table 1 summarizes coordination activi-
ties at different components in the Orchestra hierarchy.
Orchestra is designed for a cooperative environment in which a
single administrative entity controls the application software on the
cluster and ensures that it uses TCs for transfers. For example, we
envision Orchestra being used in a Hadoop data warehouse such
as Facebook’s by modifying the Hadoop framework to invoke it
for its transfers. However, this application stack can still run on
top of a network that is shared with other tenants—for example,
an organization can use Orchestra to schedule transfers inside a
virtual Hadoop cluster on Amazon EC2. Also note that in both
cases, because Orchestra is implemented in the framework, users’
applications (i.e., MapReduce jobs) need not change.
Since Orchestra can be implemented at the application level, it
can be used in existing clusters without changing network hard-
ware or management mechanisms. While controlling transfers at