The frequency of checkpointing is determined by the size
of each load-store log segment, the instruction timeout, and
delay properties of the system. Figure 10 shows the slowdown
caused just by the checkpointing system, with increasing queue
sizes and timeout lengths. The default 36KiB log is large
enough to restrict slowdowns to no more than 2% across
each of our benchmarks, with randacc being least affected due
to its low IPC and thus infrequent checkpointing. A larger
log, ten times the size, either with an associated ten times
larger timeout or with an inﬁnite timeout, is enough to reduce
overheads to negligible amounts. By comparison, a ten times
smaller log size and timeout length causes signiﬁcantly higher
overheads in most cases, with slowdowns of up to 15%.
We next discuss how these settings affect the observed
delay. However, ﬁg. 10 shows that a 36KiB log represents a
good trade-off between silicon area and performance overhead.
Frequency Impact on Error Detection Delay As the goal
of this technique is to scale error checking onto multiple cores,
through parallelisation, inevitably the error detection time is
higher than with would be with lockstep schemes, where errors
are typically detected within a few cycles [3]. One method of
controlling this delay is through the frequency of the checker
cores, which is explored in ﬁg. 11 for default load-store log
sizes and timeout lengths. It shows mean and maximum delay
between stores committing and being checked when varying
the frequency of the checker cores.
The mean detection delay is affected linearly by clock
speed, in that doubling the clock speed approximately halves
the delay. The exception to this is with high clock frequencies,
where eventually the limiting factor becomes the time to ﬁll
the load store queue using the main CPU, rather than checking
time. Maximum times are affected with less of a deterministic
pattern. Maximum times are typically dictated by, for example,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
)
s
n
(
y
a
e
D
n
o
l
t
i
t
c
e
e
D
n
a
e
M
 256
b l a c k s c h o l e s
)
s
μ
(
y
a
e
D
n
o
l
i
t
c
e
t
e
D
x
a
M
 80
 70
 60
 50
 40
 30
 20
 10
 0
Log Size / Instruction Timeout
360KiB, ∞
36KiB, ∞
3.6KiB, 500
36KiB, 5000
360KiB, 50000
r a n d a c c
l u i d a n i m a t e
f
s w a p t
i o n s
r e q m i n e
b o d y t
f
r a c k
t c o u n t
b i
r e a m
s t
)
s
n
(
y
a
e
D
n
o
l
t
i
t
c
e
e
D
n
a
e
M
 100000
 10000
 1000
 100
 10
b l a c k s c h o l e s
(a) Mean, in ns
Log Size / Instruction Timeout
360KiB, 50000
360KiB, ∞
36KiB, ∞
3.6KiB, 500
36KiB, 5000
 10000
 1000
 100
 10
 1
b l a c k s c h o l e s
r a n d a c c
f
l u i d a n i m a t e
s w a p t
i o n s
r e q m i n e
f
b o d y t
r a c k
t c o u n t
b i
r e a m
s t
125MHz
250MHz
Checker Core Clock
500MHz
1GHz
2GHz
 8192
 4096
 2048
 1024
 512
r a n d a c c
l u i d a n i m a t e
s w a p t
f
i o n s
r e q m i n e
b o d y t
f
r a c k
t c o u n t
b i
f a c e s i m s t
r e a m
(a) Mean, in ns
Checker Core Clock
125MHz
250MHz
500MHz
1GHz
2GHz
)
s
μ
(
y
a
e
D
n
o
l
i
t
c
e
t
e
D
x
a
M
b l a c k s c h o l e s
r a n d a c c
l u i d a n i m a t e
s w a p t
f
i o n s
r e q m i n e
b o d y t
f
r a c k
t c o u n t
b i
f a c e s i m s t
r e a m
(b) Maximum, in μs
(b) Maximum, in μs
Fig. 11: Delay between a store committing and being checked,
when varying the frequency of the checker cores.
Fig. 12: Delay between a store committing and being checked,
when varying the load-store log size and instruction timeout.
large numbers of cache misses on the main core, so altering
the checker core frequency often does not affect these to such
a signiﬁcant extent.
Log Size Impact on Error Detection Delay
Figure 12
shows mean and maximum detection delays when varying the
load-store log size and timeout length, at the default checker
core frequency. Mean detection times scale linearly with the
load-store log size: a tenfold increase in log size and timeout
results in a tenfold delay increase. While maximum detection
times follow a similar trend, the pattern is more sporadic, due
to individual instructions dominating the measurements.
For smaller log sizes and timeouts, many segments contain
only a few memory accesses and thus the timeout affects the
detection delay. For larger queue sizes enough instructions ﬁt
in a single segment that the log is usually ﬁlled before the
timeout is reached. The exception to this is when programs
feature large runs of instructions with very few loads and
stores, for example bitcount. Without the timeout, very large
segments of code appear, causing the maximum detection
delay to increase signiﬁcantly. However, a 50,000 instruction
timeout is enough in this case to reduce maximum delay by
250× with no performance impact.
Number of Cores
Figure 13 shows how performance
scales across different numbers of cores devoted to error
checking for the benchmarks. We see that N cores at a
frequency of M (in MHz) is comparable in performance to 2N
cores at a frequency of M
2 . For example, 6 checker cores at
1GHz is comparable to 12 cores at 500MHz. This is expected
346
3 cores, 1GHz
12 cores, 250MHz
6 cores, 1GHz
12 cores, 500MHz
12 cores, 1GHz
e
c
n
a
m
r
o
f
r
e
P
d
e
s
i
l
a
m
r
o
N
 3
 2.8
 2.6
 2.4
 2.2
 2
 1.8
 1.6
 1.4
 1.2
 1
f
i o n s
s w a p t
r a n d a c c
l u i d a n i m a t e
b l a c k s c h o l e s
Fig. 13: Slowdown with varying core counts at 1GHz, com-
pared with values for 12 cores at varying frequencies.
f a c e s i m s t
r e q m i n e
t c o u n t
b o d y t
r e a m
r a c k
b i
f
given the identiﬁed parallelism in error detection.
In fact, a large number of cores at a low clock frequency
outperform fewer at higher frequencies. This is because of the
load-store log structure where only n − 1 checker cores are
in use at any given time, since at least one is always waiting
for its segment to be ﬁlled unless the main core is stalled.
This means that better utilisation, as a percentage of the total
compute power of the cores, is achievable when more cores,
and thus more load store log segments, are available.
B. Area Overhead
Publicly available data places the RISC-V Rocket, the closest
available core in terms of size to the ones we propose for
our checker units, at 0.14mm2 area per core on a 40nm
process [45]. In comparison, at 20nm, the Cortex A57 is
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
2.05mm2 per core [46] excluding shared caches. Twelve E51-
sized cores would therefore ﬁt in approximately 0.42mm2
combined at the same technology node.
The SRAM added for instruction caches, register check-
points, load forwarding unit and the load-store log is 80KiB
in total, which is approximately 0.08mm2 area overhead [47].
Combined, this places the error detection hardware at ap-
proximately 24% area overhead compared to the original
core without shared caches. When a 1MiB single-ported L2
cache at approximately 1mm2 [47] is also included, the area
overhead is approximately 16% of the original core.
This is a very approximate estimate: Rocket has a different
ISA from the A57, and the out-of-order core we model is faster
than an A57, increasing the number of checker cores required
in our experiments (a real implementation would need fewer).
Still, it is clear that the overhead is massively reduced
compared to dual-core lockstep, the current state-of-the-art,