before it can detect that a release of that resource is somewhere
omitted. This exemplar permits Hector to ﬁnd faults without
precise information about resource acquisition and release
functions. However, without an exemplar, no fault can be
detected, resulting in false negatives. Other potential reasons for
false negatives are analogous to the reasons for false positives,
e.g., failing to recognize a call that represents an acquisition,
TABLE V.
FAULTS, FALSE POSITIVES, AND FALSE NEGATIVES, FOR
KMALLOC, KZALLOC, AND KCALLOC
VI. RELATED WORK
Linux drivers
Linux sound
Linux net
Linux fs
Faults
38
2
4
1
Coccinelle
FP
FN
28 (42%) 70 (65%)
6 (75%)
6 (75%)
1 (20%)
5 (56%)
8 (89%)
1 (50%)
Faults
86
7
1
1
Hector
FP
FN
10 (10%) 22 (20%)
13 (65%) 1 (13%)
4 (80%)
1 (50%)
7 (88%)
1 (50%)
and considering a call to be a release operation when the called
function does not perform a release.
Estimating the rate of false negatives is difﬁcult, because it
requires complete knowledge of the set of faults in a system.
Indeed, we know of no other fault-ﬁnding tools for systems code
for which false negatives have been investigated. Rather than
trying to identify all of the faults in our considered software,
we compare the results of Hector with an alternate fault-ﬁnding
approach that does not rely on exemplars. To reduce the amount
of code to study, we focus on resource-release omission faults
involving resources acquired using the basic Linux kernel
memory allocation functions, kmalloc, kzalloc, and kcalloc, for
which Fig. 10 showed that faults are common. We furthermore
focus on cases where the acquired resource is stored in a
local variable and is not passed to another function or stored
in another location before reaching the error-handling code;
these restrictions imply that there is a high probability that the
resource must be released before the variable referencing it
goes out of scope, and thus reduce the rate of false positives.
We have implemented this strategy using the open-source tool
Coccinelle [18]. Coccinelle does not implement a speciﬁc fault-
ﬁnding policy, but instead makes it possible to specify patterns
that are used to search for code fragments that exhibit certain
properties within the paths of a function’s CFG.
Table V shows the rate of detected resource-release omission
faults in the use of kmalloc, kzalloc, and kcalloc and the rate
of false positives, for the Coccinelle rule and for Hector. From
this information, we compute a lower bound on the number
and rate of false negatives by comparing the set of faults found
by each approach to the complete set of faults found by either
approach. While Hector has a high rate of false negatives, the
absolute numbers involved are small. Almost all of the false
negatives are due to the lack of an exemplar. There are only
three cases, all in a single function, where there is a failure
of the preprocessing heuristics, as a call is considered to be a
release when it is not. Furthermore, the Coccinelle rule also has
a high rate of false negatives, because of the restrictions noted
above to avoid false positives. These restrictions are indeed
only partially successful, because the rate of false positives is
up to 89%, and is consistently higher than that of Hector.
E. Scalability
We carried out our tests on one core of a 8-core 3GHz Intel
Xeon with 16GB RAM. Analyzing Linux drivers, which is
the largest considered project (4.6 MLOC), takes around 3 hours.
Over all the considered projects, the processing time, excluding
the parsing time, ranges from 0.0002 s/LOC (seconds per line
of code) to 0.0068 s/LOC. Apache, which is the smallest project
(0.1 MLOC), and Linux drivers, which is the largest, have
essentially the same processing time per line, at 0.0019 s/LOC,
showing the scalability of the approach.
Our most closely related work is that of Weimer and Necula
on speciﬁcation mining for fault ﬁnding [13], which also focuses
on error-handling code. They target user-level programs written
in Java, which provides speciﬁc abstractions for exceptions,
while we target systems code written in C, where error-handling
code is ad hoc. They search for pairs of functions a and b,
where the a functions may, but need not, correspond to our
acquisition operations, and the b functions correspond to our
release operations. For a given pair of functions a and b, they
require the existence of what amounts to an exemplar and what
amounts to a candidate fault, but do not require the exemplar
and candidate fault to come from the same function. Thus,
their mining process can be thrown off by local variations in
API usage protocols. In practice, on almost 1 million lines of
Java code, from 9 different projects, almost all of their mined
speciﬁcations are false positives, reaching a false positive rate
of 90%. To reduce the rate of false positives, Le Goues and
Weimer integrate extra information such as author expertise
[15], but doing so also reduces the number of found faults.
Furthermore, results are ranked according to statistics, so rarely
used release functions may be overlooked.
Sundararaman et al. also focus on faults in error-handling
code, by simply trying to avoid the need to execute error-
handling code, through the deﬁnition of an alternate memory
allocator [19]. We have seen in Section II-B that systems code
can encounter other kinds of errors, such as defective devices
and bad user-level values, which the approach of Sundararaman
et al. cannot address. Resource Acquisition Is Initialization
(RAII) is a resource management technique originating in C++
that exploits the ability to associate a variable with cleanup
code, which is executed when the variable goes out of scope
[20]. RAII eliminates the need for resource releases in exception
handlers, but has the side effect that resources are also released
on a normal function exit. The latter is too constrained for
systems code, where allocated resources must persist over
multiple requests by applications or hardware.
Engler et al. use static analysis to automatically extract
programming rules from source code, based on user-deﬁned
templates [7]. Ranking calculated in terms of support and
conﬁdence is used to highlight the most probable rules. The
approach can also use “must beliefs” derived from the user’s
knowledge of the semantics of the code, rather than statistics.
Such must beliefs are not available in our setting, where there is
a very wide range of resource acquisition and release operations.
PR-Miner uses frequent itemset mining to extract programming
rules, without using templates [9]. Results are pruned and
ranked according to support and conﬁdence. MUVI applies
a similar strategy to ﬁnd missing locking operations [21].
Kremenek et al. use factor graphs in inferring speciﬁcations
directly from programs [22]. Ramanathan et al. integrate mining
within a path-sensitive dataﬂow framework to identify potential
preconditions for invocation of a function [23]. In each of
these cases, the identiﬁed speciﬁcations can be used to ﬁnd
faults in code. Hector does not rely on a separate speciﬁcation
mining phase. Instead, it ﬁnds faults based on inconsistent
local information, rather than a global analysis of the software.
Hector can ﬁnd faults in the use of protocols that occur rarely
and thus are likely to be pruned or given a low rank by other
approaches.
The tool Coverity,9 based on the research of Engler et
al. [7], [24], includes rules for identifying memory leaks as
well as other rules that are able to identify errors within error-
handling code. We have collected and categorized the entire
set of patches accepted into the Linux kernel between April
2005 and April 2013 that mention Coverity.10 Out of 523 such
patches, only 109 (21%) relate to error-handling code. Of these,
64 involve one or more missing occurrences of kfree and
16 more involve missing or duplicate occurrences of some
other function containing “free” in its name. 3 patches involve
functions whose name contains the substring “lock” and 3
involve functions whose name contains the substring “put”.
14 involve unnecessary error-handling operations rather than
omitted operations, and are detected as null pointer dereferences.
The remaining 6 patches involve a variety of other functions and
conditions. Hector has made it possible to ﬁnd more than twice
as many faults, involving a more diverse set of functions, within
just one Linux version. While we do not know the version of
Coverity used by the Linux developers, nor the strategies used
by the Linux developers to decide which reported faults to ﬁx,
these results suggest that our work is complementary to the
strategies used by the Coverity tool.
Wu et al. identify resource acquisition and release oper-
ations in Java code by interprocedural analysis of method
deﬁnitions [25], ultimately relying on a list of known release
operations. Ravitch et al. take a similar strategy for C code [26].
These approaches could be used in an alternative implementa-
tion of the preprocessing phase of our algorithm. Our proposed
implementation is mostly intraprocedural and does not require
advance knowledge of any resource-release functions; the latter
is an advantage for Linux, which manages a wide range of
types of resources and does not rely on standard libraries. The
analyses required are furthermore less costly, as interprocedural
analysis is limited to a single ﬁle.
Gunawi et al. [27] and Rubio-Gonz´alez et al. [28] have
studied faults in the detection and propagation of error values.
Our work is complementary, in that we focus on the contents
of blocks of error-handling code, while they focus only on
the return values. Banabic and Candea propose a strategy for
fault-injection prioritisation to perform run-time checking of
error-handling code [29]. The reported faults involve omitted
tests and duplicated releases, while Hector focuses on release
omissions.
Another approach to detect faults is to monitor program
execution. A dynamic analysis tool such as Valgrind [30]
only reports on real faults that can occur in real executions,
and is insensitive to procedure-call boundaries. Thus, it may
ﬁnd some faults that involve interprocedural dependencies and
cannot be found by Hector. On the other hand, such a tool
can only ﬁnd faults in the code that is actually executed, given
the available test cases. Forcing the execution of all error-
handling code would require developing an elaborate testing
framework, potentially involving multiple kinds of hardware,
depending on the application. Symbolic execution [31] coupled
with fault injection [32], attempts to address these problems by
making it possible to activate all execution paths. However, such
techniques remain time-consuming, and no form of speciﬁcation
9http://scan.coverity.com/
10https://git.kernel.org/cgit/linux/kernel/git/next/linux-
next.git/log/?id=refs/tags/next-20130412
inference is provided. Thus, the developer still needs precise
prior knowledge of the various pairs of resource acquisition
and release operations.
Some other works use static analysis to ﬁnd faults in Linux
code. Chou et al. [2] and Palix et al. [4] use patterns to
automatically ﬁnd simple faults such as null pointer deref-
erences. Their techniques are not sufﬁcient to ﬁnd arbitrary
resource-release omissions in error-handling code because they
do not infer protocols. The rule INull, originally developed
by Chou et al. and which is also part of the static analysis
tool Coverity, checks for the dereference of a value that is
subsequently tested for being NULL. Like our work, INull also
relies on function-local consistency information, comprising
the dereference and the NULL tests. Nevertheless, the case
addressed by INull is simpler than that of resource-release
omissions, because the identiﬁcation an operation as a NULL
test or as a dereference is unambiguous, drastically reducing
the possibility of false positives. In another form of consistency
analysis, Tan et al. [33] ﬁnd faults by comparing code with
its expected behavior, described in comments. Comments have
been useful in assessing the faults reported by Hector, and it
could be interesting to combine the two approaches.
VII. CONCLUSION
In this paper, we have shown that error-handling code is a
substantial source of faults in systems code, and that such faults
can have a signiﬁcant impact on system reliability. We have
presented a novel approach to ﬁnding faults in error-handling
code of systems software that uses a function’s existing
error-handling code as an exemplar of the operations that are
required. By focusing on one function at a time, while taking
into account a small amount of interprocedural information
from other functions deﬁned in the same ﬁle, we obtain a
fault-ﬁnding algorithm that is precise and scalable. We have
implemented our approach as the tool Hector, and applied it to
ﬁnd 371 faults in Linux and 5 other systems software projects.
A limitation of our approach is the need for at least one
exemplar of a given resource-release operation in the given
function. In future work, we will consider whether it is possible
to relax this requirement, e.g., to ﬁnd exemplars in other
functions in the same ﬁle, or in functions that appear to play the
same role in the implementations of related services. Another
direction of future work is to consider how to automatically
ﬁx the faults, based on the information in the exemplar, or
based on the history of the software as a whole, taking into
account how similar faults have been ﬁxed in other parts of
the software over time. Finally, we will consider how the use
of local information can be applied to other program analysis
problems, such as identifying shared variables.
REFERENCES
[1] P. M. Melliar-Smith and B. Randell, “Software reliability: The role of
programmed exception handling,” in ACM Conference on Language
Design for Reliable Software, 77.
[2] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An empirical
study of operating systems errors,” in SOSP’01.
J. L. Lawall, J. Brunel, R. R. Hansen, H. Stuart, G. Muller, and N. Palix,
“WYSIWIB: A declarative approach to ﬁnding protocols and bugs in
Linux code,” in DSN’09.
[3]
[4] N. Palix, G. Thomas, S. Saha, C. Calv`es, J. Lawall, and G. Muller,
“Faults in Linux: ten years later,” in ASPLOS’11.
[5] W. Weimer and G. C. Necula, “Finding and preventing run-time error
handling mistakes,” in OOPSLA’04.
R. Arpaci-Dusseau, “Making the common case the only case with
anticipatory memory allocation,” in FAST’11.
[6] G. Ammons, R. Bod´ık, and J. R. Larus, “Mining speciﬁcations,” in
POPL’02.
[7] D. R. Engler, D. Y. Chen, A. Chou, and B. Chelf, “Bugs as deviant
behavior: A general approach to inferring errors in systems code,” in
SOSP’01.
[8] M. Gabel and Z. Su, “Javert: Fully automatic mining of general temporal
properties from dynamic traces,” in FSE’08.
[9] Z. Li and Y. Zhou, “PR-Miner: automatically extracting implicit
programming rules and detecting violations in large software code,”
in ESEC/FSE’05.
[10] D. Lo, S.-C. Khoo, and C. Liu, “Mining temporal rules for software
maintenance,” Journal of Software Maintenance and Evolution: Research
and Practice, vol. 20, 2008.
[11] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. M. Al-Kofahi, and T. N.
Nguyen, “Graph-based mining of multiple object usage patterns,” in
ESEC-FSE’09.
[12] A. Wasylkowski, A. Zeller, and C. Lindig, “Detecting object usage
anomalies,” in ESEC-FSE’07.
[13] W. Weimer and G. C. Necula, “Mining temporal speciﬁcations for error
detection,” in TACAS’05.
J. Yang, D. Evans, D. Bhardwaj, T. Bhat, and M. Das, “Perracotta:
Mining temporal API rules from imperfect traces,” in ICSE’06.
[14]
[15] C. Le Goues and W. Weimer, “Speciﬁcation mining with few false
positives,” in TACAS’09.
[16] A. Kadav and M. M. Swift, “Understanding modern device drivers,” in
ASPLOS’12.
[17] A. Bessey, K. Block, B. Chelf, A. Chou, B. Fulton, S. Hallem, C. Henri-
Gros, A. Kamsky, S. McPeak, and D. Engler, “A few billion lines of
code later: using static analysis to ﬁnd bugs in the real world,” Commun.
ACM, vol. 53, Feb. 2010.
[18] Y. Padioleau, J. Lawall, R. R. Hansen, and G. Muller, “Documenting and
automating collateral evolutions in Linux device drivers,” in EuroSys’08.
[19] S. Sundararaman, Y. Zhang, S. Subramanian, A. Arpaci-Dusseau, and
[20] B. Stroustrup, Exception Safety: Concepts and Techniques. LNCS,
2001, vol. 2022.
[21] S. Lu, S. Park, C. Hu, X. Ma, W. Jiang, Z. Li, R. A. Popa, and Y. Zhou,
“MUVI: automatically inferring multi-variable access correlations and
detecting related semantic and concurrency bugs,” in SOSP’07.
[22] T. Kremenek, P. Twohey, G. Back, A. Ng, and D. Engler, “From
uncertainty to belief: Inferring the speciﬁcation within,” in OSDI’06.
[23] M. Ramanathan, A. Grama, and S. Jagannathan, “Path-sensitive inference
of function precedence protocols,” in ICSE’07.
[24] D. R. Engler, B. Chelf, A. Chou, and S. Hallem, “Checking system
rules using system-speciﬁc, programmer-written compiler extensions,”
in OSDI’00.
[25] Q. Wu, G. Liang, Q. Wang, T. Xie, and H. Mei, “Iterative mining of
resource-releasing speciﬁcations,” in ASE’11.
[26] T. Ravitch, S. Jackson, E. Aderhold, and B. Liblit, “Automatic generation
of library bindings using static analysis,” in PLDI’09.
[27] H. S. Gunawi, C. Rubio-Gonz´alez, A. C. Arpaci-Dusseau, R. H. Arpaci-
Dusseau, and B. Liblit, “EIO: Error handling is occasionally correct,”
in FAST’08.
[28] C. Rubio-Gonz´alez, H. S. Gunawi, B. Liblit, R. H. Arpaci-Dusseau, and
A. C. Arpaci-Dusseau, “Error propagation analysis for ﬁle systems,” in
PLDI’09.
[29] R. Banabic and G. Candea, “Fast black-box testing of system recovery
code,” in EuroSys’12.
[30] N. Nethercote and J. Seward, “Valgrind: a framework for heavyweight
dynamic binary instrumentation,” in PLDI’07.
[31] S. Bucur, V. Ureche, C. Zamﬁr, and G. Candea, “Parallel symbolic
execution for automated real-world software testing,” in EuroSys’11.
[32] P. D. Marinescu and G. Candea, “LFI: A practical and general library-
level fault injector,” in DSN’09.
[33] L. Tan, D. Yuan, G. Krishna, and Y. Zhou, “/*icomment: bugs or bad
comments?*/,” in SOSP’07.