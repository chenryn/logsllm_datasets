title:AutoLock: Why Cache Attacks on ARM Are Harder Than You Think
author:Marc Green and
Leandro Rodrigues Lima and
Andreas Zankl and
Gorka Irazoqui and
Johann Heyszl and
Thomas Eisenbarth
AutoLock: Why Cache Attacks on ARM Are  
Harder Than You Think
Marc Green, Worcester Polytechnic Institute; Leandro Rodrigues-Lima and Andreas 
Zankl, Fraunhofer AISEC; Gorka Irazoqui, Worcester Polytechnic Institute; Johann Heyszl, 
Fraunhofer AISEC; Thomas Eisenbarth, Worcester Polytechnic Institute
https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/green
This paper is included in the Proceedings of the 26th USENIX Security SymposiumAugust 16–18, 2017 • Vancouver, BC, CanadaISBN 978-1-931971-40-9Open access to the Proceedings of the 26th USENIX Security Symposium is sponsored by USENIXAutoLock: Why Cache Attacks on ARM Are Harder Than You Think
Marc Green
Worcester Polytechnic Institute
Leandro Rodrigues-Lima
Fraunhofer AISEC
Gorka Irazoqui
Worcester Polytechnic Institute
Johann Heyszl
Fraunhofer AISEC
Andreas Zankl
Fraunhofer AISEC
Thomas Eisenbarth
Worcester Polytechnic Institute
Abstract
Attacks on the microarchitecture of modern processors
have become a practical threat to security and privacy
in desktop and cloud computing. Recently, cache at-
tacks have successfully been demonstrated on ARM
based mobile devices, suggesting they are as vulner-
able as their desktop or server counterparts.
In this
work, we show that previous literature might have left
an overly pessimistic conclusion of ARM’s security as
we unveil AutoLock: an internal performance enhance-
ment found in inclusive cache levels of ARM proces-
sors that adversely affects Evict+Time, Prime+Probe,
and Evict+Reload attacks. AutoLock’s presence on
system-on-chips (SoCs) is not publicly documented, yet
knowing that it is implemented is vital to correctly as-
sess the risk of cache attacks. We therefore provide a de-
tailed description of the feature and propose three ways
to detect its presence on actual SoCs. We illustrate how
AutoLock impedes cross-core cache evictions, but show
that its effect can also be compensated in a practical at-
tack. Our ﬁndings highlight the intricacies of cache at-
tacks on ARM and suggest that a fair and comprehen-
sive vulnerability assessment requires an in-depth under-
standing of ARM’s cache architectures and rigorous test-
ing across a broad range of ARM based devices.
1
Introduction
The rapid growth of mobile computing illustrates the
continually increasing role of digital services in our daily
lives. As more and more information is processed digi-
tally, data privacy and security are of utmost importance.
One of the threats known today aims directly at the fab-
ric of digital computing. Attacks on processors and their
microarchitecture exploit the very core that handles our
data. In particular, processor caches have been exploited
to retrieve sensitive information across logic boundaries
established by operating systems and hypervisors. As
caches speed up the access to data and instructions, tim-
ing measurements allow an adversary to infer the activity
of other applications and the data processed by them. In
fact, cache attacks have been demonstrated in multiple
scenarios in which our personal data is processed, e.g.,
web browsing [41] or cloud computing [26, 58]. These
attacks have severe security implications, as they can re-
cover sensitive information such as passwords, crypto-
graphic keys, and private user behavior. The majority of
attacks have been demonstrated on classic desktop and
server hardware [25, 30, 42, 51], and with Intel’s market
share for server processors being over 98% [31], their
platforms have been targeted most frequently.
With mobile usage skyrocketing,
the feasibility of
cache attacks on smartphone and IoT processors – which
are predominantly ARM-based – has become a rele-
vant issue. Attacks that rely on the existence of a
cache ﬂush instruction, i.e., Flush+Reload [51] and
Flush+Flush [23], work efﬁciently across a broad range
of x86 processors, but have limited applicability on ARM
devices.
In general, cache ﬂush instructions serve the
legitimate purpose of manually maintaining coherency,
e.g., for memory mapped input-output or self-modifying
code. On any x86 processor implementing the SSE2
instruction set extension, this ﬂush instruction is avail-
able from all privilege levels as clflush. A similar in-
struction was introduced for ARM processors only in the
most recent architecture version, ARMv8. In contrast to
clflush, it must be speciﬁcally enabled to be accessi-
ble from userspace. This leaves a signiﬁcant number of
ARM processors without a cache ﬂush instruction.
For all processors with a disabled ﬂush instruction or
an earlier architecture version, e.g., ARMv7, only evic-
tion based cache attacks can be deployed. In particular,
these attacks are Evict+Time [42], Prime+Probe [42],
and Evict+Reload [24]. On multi-core systems, they
target the last-level cache (LLC) to succeed regardless of
which core a victim process is running on. This requires
the LLC to be inclusive, i.e., to always contain the con-
USENIX Association
26th USENIX Security Symposium    1075
tents of all core-private cache levels. On Intel processors,
the entire cache hierarchy fulﬁlls the inclusiveness prop-
erty and is therefore a viable target for eviction based
attacks. ARM devices, on the contrary, implement inclu-
sive and non-inclusive caches alike. Both properties can
co-exist, even in the same cache hierarchy. This renders
eviction based attacks to be practicable only on a limited
number of devices, in particular those that implement
inclusive last-level caches. Yet, our ﬁndings show that
an internal performance enhancement in inclusive last-
level caches, dubbed AutoLock, can still impede evic-
tion based cache attacks. In short, AutoLock prevents
a processor core from evicting a cache line from an in-
clusive last-level cache, if said line is allocated in any
of the other cores’ private cache levels. This inhibits
cross-core LLC evictions, a key requirement for practi-
cal Evict+Time, Prime+Probe, and Evict+Reload at-
tacks on multi-core systems, and further limits the num-
ber of ARM based attack targets in practice.
In literature, Lipp et al. [37], Zhang et al. [54], and
Zhang et al. [56] conﬁrmed the general feasibility of
ﬂush and eviction based cache attacks from unprivileged
code on ARM processors. Given the lack of ﬂush in-
structions on a large selection of ARM devices and the
deployment of non-inclusive LLCs or inclusive LLCs
implementing AutoLock, the authors might have left an
overly pessimistic conclusion of ARM’s security against
cache attacks. In addition, ARM’s highly ﬂexible licens-
ing ecosystem creates a heterogeneous market of system-
on-chips (SoCs) that can exhibit signiﬁcant differences
in their microarchitectural implementations. Demme et
al. [17] illustrate that already small changes to the cache
architecture can have considerable impact on the side-
channel vulnerability of the processor. Consequently,
judging the true impact of cache attacks on a broad range
of ARM based platforms remains to be a challenge. Our
work adds another step in this process. It is a contribu-
tion to an in-depth understanding of microarchitectural
features on ARM in general and an extension to our cur-
rent knowledge of cache implementations in particular.
1.1 Contribution
This work unveils AutoLock, an internal and undocu-
mented performance enhancement feature found in in-
clusive cache levels on ARM processors.
It prevents
cross-core evictions of cache lines from inclusive last-
level caches, if said lines are allocated in any of the
core-private cache levels. Consequently, it has a direct
and fundamentally adverse effect on all eviction based
cache attacks launched from unprivileged code on multi-
core systems. Understanding AutoLock and determin-
ing its existence on a given system-on-chip is vital to as-
sess the SoC’s vulnerability to those attacks. Yet, neither
technical reference manuals (TRMs) nor any other pub-
lic product documentation by ARM mention AutoLock.
We therefore provide a detailed description of the fea-
ture and propose three methodologies to test for it: using
a hardware debugging probe, reading the performance
monitoring unit (PMU), and conducting simple cache-
timing measurements. Each test strategy has different
requirements and reliability; having multiple of them
is vital to test for AutoLock under any circumstances.
With the proposed test suite, we verify AutoLock on
ARM Cortex-A7, A15, A53, and A57 processors. As
AutoLock is likely implemented on a larger number of
ARM processors, we discuss its general implications and
how our results relate to previous literature.
Despite its adverse effect on eviction based cache at-
tacks, the impact of AutoLock can be reduced. We dis-
cuss generic circumvention strategies and execute the
attack by Irazoqui et al. [29] in a practical cross-core
Evict+Reload scenario on a Cortex-A15 implementing
AutoLock. We successfully recover the secret key from
a table based implementation of AES and show that at-
tacks can tolerate AutoLock if multiple cache lines are
exploitable. Furthermore, the presented circumvention
strategies implicitly facilitate cross-core eviction based
attacks also on non-inclusive caches. This is because in
the context of cross-core LLC evictions, inclusive last-
level caches with AutoLock behave identically to non-
inclusive ones. In summary, our main contributions are:
• the disclosure and description of AutoLock, an un-
documented and previously unknown cache imple-
mentation feature with adverse impact on practical
eviction based cache attacks on ARM devices,
• a comprehensive test suite to determine the exis-
tence of AutoLock on actual devices, as its presence
is not documented publicly,
• a discussion of AutoLock’s implications and its re-
lation to previous literature demonstrating cache at-
tacks on ARM, and
• a set of strategies to circumvent AutoLock to-
gether with a practical demonstration of a cross-
core Evict+Reload attack on a multi-core SoC im-
plementing AutoLock.
The rest of this paper is organized as follows. Sec-
tion 2 describes AutoLock. A theoretical methodology
to test for it is presented in Section 3. We evaluate SoCs
for AutoLock in Section 4 and address how recent lit-
erature relates to it in Section 5. The implications of
AutoLock are discussed in Section 6. Circumvention
strategies together with a practical cross-core attack are
presented in Section 7. We conclude in Section 8.
1076    26th USENIX Security Symposium
USENIX Association
2 AutoLock: Transparent Lockdown of
Cache Lines in Inclusive Cache Levels
Processor caches can be organized in levels that build up
a hierarchy. Higher levels have small capacities and fast
response times. L1 typically refers to the highest level.
In contrast, lower levels have increased capacities and
response times. The lowest cache level is often referred
to as the last-level cache, or LLC. Data and instructions
brought into cache reside on cache lines, the smallest
logical storage unit. In set-associative caches, lines are
grouped into sets of ﬁxed size. The number of lines or
ways per cache set is called the associativity of the cache
level. It can be different for every level. Whether a cache
level can hold copies of cache lines stored in other lev-
els is mainly deﬁned by the inclusiveness property. If a
cache level x is inclusive with respect to a higher level
y, then all valid cache lines contained in y must also be
contained in x. If x is exclusive with respect to y, valid
lines in y must not be contained in x. If any combination
is possible, the cache is said to be non-inclusive.
Inclusive caches enforce two rules.
If a cache line
is brought into a higher cache level, a copy of the line
must be stored in the inclusive lower level. Determin-
ing whether a line is stored anywhere in the hierarchy
can then be achieved by simply looking into the LLC.
Vice versa, if a line is evicted from the lower level, any
copy in the higher levels must subsequently be evicted as
well. This is an implicit consequence of the inclusiveness
property that has been successfully exploited in cross-
core cache attacks that target inclusive LLCs [26, 27, 39].
Evictions in higher cache levels to maintain inclusive-
ness can add substantial performance penalties in prac-
tice.
In a patent publication by Williamson and ARM
Ltd., the authors propose a mechanism that protects a
given line in an inclusive cache level from eviction, if any
higher cache level holds a copy of the line [50]. An indi-
cator storage element that is integrated into the inclusive
cache level tracks which lines are stored in higher lev-
els. The element can be realized with a set of indicator
or inclusion bits per cache line, or a tag directory. If an
indicator is set, the corresponding line is protected. This
mechanism therefore prevents said performance penal-
ties, because subsequent evictions in higher cache levels
are prohibited. We refer to this transparent protection
of cache lines in the LLC as Automatic Lockdown or
AutoLock.
The impact of AutoLock during eviction is illustrated
in Figure 1. For simplicity, the illustration is based on a
two-level cache hierarchy: core-private L1 caches and a
shared inclusive last-level cache (L2) with one inclusion
bit per cache line. S and L are placeholders for any avail-
able cache set and line in the two cache levels. The left
side of the ﬁgure shows how a cache line is evicted in L1.
Figure 1: Simpliﬁed example of evicting a cache line
from level 1 (left) and level 2 (right) cache sets. The
level 2 cache is inclusive with respect to level 1 and im-
plements AutoLock.
First, an allocation request for set S is received. Then, a
target line L is selected by the replacement algorithm for
eviction. If a copy of L is present in any other of the core-
private L1 caches, it can immediately be evicted without
updating the inclusion bit in L2. Because other L1 copies
exist, the bit does not need to be changed. If no other L1
cache holds a copy of L, the inclusion bit must be reset
in L2, which unlocks the copy of L in L2. After the bit is
reset, L is evicted from L1.
Similarly, an allocation request in L2 triggers the re-
placement algorithm to select a line in set S for eviction.
Before L is evicted in L2, its inclusion bit is checked. If
a copy of L exists in any L1 cache, the replacement al-
gorithm is called again to select another target line. This
is repeated until one line is found whose inclusion bit is
not set. This line is then evicted to allow the allocation
of a new one.
If the number of ways in the inclusive lower cache
level, Wl, is higher than the sum of ways in all higher
cache levels, i.e., Wh,sum = ∑N
i=1Wh,i, it can be guaran-
teed that at least one line is always selectable for evic-
tion. If Wl = Wh,sum, all lines of a set in the lower cache
level can be auto-locked.
In this case, the patent pro-
poses to fall back to the previous behavior, i.e., evict all
copies of a line from higher level caches. This unlocks
the line in the lower cache level and subsequently en-
ables its eviction.
If the number of ways in the lower
cache level is further reduced, such that Wl < Wh,sum, ad-
USENIX Association
26th USENIX Security Symposium    1077
ditional measures must be taken to implement inclusive-
ness and Automatic Lockdown. While not impossible
per se, this case is not covered by the patent authors.
If an inclusive LLC with AutoLock is targeted in a
cache attack, the adversary is not able to evict a target’s
data or instructions from the LLC, as long as they are
contained in the target’s core-private cache.
In theory,
the adversary can only succeed, if the scenario is reduced
to a same-core attack. Then, it is possible once again
to directly evict data and instructions from core-private
caches. Note that the same attack limitation is encoun-
tered on systems with non-inclusive last-level caches, be-
cause cache lines are allowed to reside in higher lev-
els without being stored in lower levels. In both cases,
AutoLock and non-inclusive LLC, the attacks do not