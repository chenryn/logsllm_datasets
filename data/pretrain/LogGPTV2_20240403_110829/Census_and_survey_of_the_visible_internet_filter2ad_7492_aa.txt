title:Census and survey of the visible internet
author:John S. Heidemann and
Yuri Pryadkin and
Ramesh Govindan and
Christos Papadopoulos and
Genevieve Bartlett and
Joseph A. Bannister
Census and Survey of the Visible Internet
John Heidemann1,2, Yuri Pradkin1, Ramesh Govindan2, Christos Papadopoulos3,
3 Colorado State University
4 The Aerospace Corporation
Genevieve Bartlett1,2, Joseph Bannister4
1 USC/Information Sciences Institute
2 USC/Computer Science Dept.
ABSTRACT
Prior measurement studies of the Internet have explored
traﬃc and topology, but have largely ignored edge hosts.
While the number of Internet hosts is very large, and many
are hidden behind ﬁrewalls or in private address space, there
is much to be learned from examining the population of vis-
ible hosts, those with public unicast addresses that respond
to messages. In this paper we introduce two new approaches
to explore the visible Internet. Applying statistical popula-
tion sampling, we use censuses to walk the entire Internet
address space, and surveys to probe frequently a fraction
of that space. We then use these tools to evaluate address
usage, where we ﬁnd that only 3.6% of allocated addresses
are actually occupied by visible hosts, and that occupancy
is unevenly distributed, with a quarter of responsive /24
address blocks (subnets) less than 5% full, and only 9%
of blocks more than half full. We show about 34 million
addresses are very stable and visible to our probes (about
16% of responsive addresses), and we project from this up
to 60 million stable Internet-accessible computers. The re-
mainder of allocated addresses are used intermittently, with
a median occupancy of 81 minutes. Finally, we show that
many ﬁrewalls are visible, measuring signiﬁcant diversity in
the distribution of ﬁrewalled block size. To our knowledge,
we are the ﬁrst to take a census of edge hosts in the visi-
ble Internet since 1982, to evaluate the accuracy of active
probing for address census and survey, and to quantify these
aspects of the Internet.
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network
Architecture and Design—Network topology; C.2.3 [Computer-
Communication Networks]: Network Operations—Net-
work management
General Terms: Management, Measurement, Security
Keywords: Internet address allocation, IPv4, ﬁrewalls,
survey, census
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’08,  October  20–22,  2008,  Vouliagmeni,  Greece.
Copyright 2008 ACM 978-1-60558-334-1/08/10 ...$5.00.
1.
INTRODUCTION
Measurement studies of the Internet have focused primar-
ily on network traﬃc and the network topology. Many sur-
veys have characterized network traﬃc in general and in
speciﬁc cases [28, 36, 8, 43, 14]. More recently, researchers
have investigated network topology, considering how net-
works and ISPs connect, both at the AS [10, 46, 12, 32, 7]
and router levels [47, 29]. These studies have yielded insight
into network traﬃc, business relationships, routing oppor-
tunities and risks, and network topology.
For the most part these studies have ignored the popula-
tion of hosts at the edge of the network. Yet there is much
to be learned from understanding end-host characteristics.
Today, many simple questions about hosts are unanswered:
How big is the Internet, in numbers of hosts? How densely
do hosts populate the IPv4 address space? How many hosts
are, or could be, clients or servers? How many hosts are
ﬁrewalled or behind address translators? What trends guide
address utilization?
While simple to pose, these questions have profound im-
plications for network and protocol design. ICANN is ap-
proaching full allocation of the IPv4 address space in the
next few years [21]. How completely is the currently allo-
cated space used? Dynamically assigned addresses are in
wide use today [50], with implications for spam, churn in
peer-to-peer systems, and reputation systems. How long is
a dynamic address used by one host? Beyond addresses, can
surveys accurately evaluate applications in the Internet [16]?
We begin to answer these questions in this paper. Our
ﬁrst contribution is to establish two new methodologies to
study the Internet address space. To our knowledge, we are
the ﬁrst to take a complete Internet census by probing the
edge of the network since 1982 [41]. While multiple groups
have taken surveys of fractions of the Internet, none have
probed the complete address space.
Our second contribution to methodology is to evaluate
the eﬀectiveness of surveys that frequently probe a small
fraction of the edge of the network. We are not the ﬁrst to
actively probe the Internet. Viruses engage in massively par-
allel probing, several groups have examined Internet topol-
ogy [13, 45, 19, 40], and a few groups have surveyed random
hosts [16, 49]. However, to our knowledge, no one has ex-
plored the design trade-oﬀs in active probing of edge hosts.
We describe our methodology in Section 2, and in Section 4
explore the trade-oﬀs between these approaches.
Ultimately our goal is to understand the host-level struc-
ture of the Internet. A full exploration of this goal is larger
than the scope of any one paper, because the relationship
Internet−accessible computers
static
dynamic
addresses
visible to ICMP
visibile to other protocols
(but not ICMP)
visible computers
invisible computers
r
o
u
t
e
r
s
t
e
m
p
.
d
o
w
n
a
t
p
r
o
b
e
t
i
m
e
o
f
f
−
l
i
n
e
f
r
e
q
u
e
n
t
l
y
computers
never on
the Internet
(firewalled, access controlled, whitelisted)
indirectly connected via private address space
Figure 1: Classifying Internet addressable computers.
between IP addresses and computers is complex, and all
survey mechanisms have sources of bias and limitation. We
address how computers and IP addresses relate in Section 3.
Active probing has inherent limitations: many hosts today
are unreachable, hidden behind network-address translators,
load balancers, and ﬁrewalls. Some generate traﬃc but do
not respond to external requests.
In fact, some Internet
users take public address space but use it only internally,
without even making it globally routable. Figure 1 cap-
tures this complexity, highlighting in the cross-hatched area
the visible Internet, hosts with public unicast addresses that
will respond to contact. While this single paper cannot fully
explore the host-level Internet, our methodologies take a sig-
niﬁcant step towards it in Section 3 by measuring the visible
Internet and estimating speciﬁc sources of measurement er-
ror shown in this ﬁgure. More importantly, by deﬁning this
goal and taking a ﬁrst step towards it we lay the groundwork
for potential future research.
An additional contribution is to use our new methodolo-
gies to estimate characteristics of the Internet that have un-
til now only been commented on anecdotally. In Section 5
we evaluate typical address occupancy, shedding light on
dynamic address usage, showing that the median active ad-
dress is continuously occupied for 81 minutes or less. We es-
timate the size of the stable Internet (addresses that respond
more than 95% of the time), and show how this provides a
loose upper bound on the number of servers on the Inter-
net, overcounting servers by about a factor of two. Finally,
with our three years of censuses, we show trends in address
allocation and utilization and estimate current utilization.
We ﬁnd that only 3.6% of allocated addresses are actually
occupied by visible hosts, and that occupancy is unevenly
distributed, with a quarter of responsive /24 address blocks1
less than 5% full, and only 9% of blocks more than half full.
While we take great pains to place error bounds on our
estimates, these estimates are approximations. However, no
other measurements of edge hosts exist today with any er-
ror bounds. Given the growing importance of understanding
address usage as the ﬁnal IPv4 address blocks are delegated
by ICANN, we believe our rough estimates represent an im-
portant and necessary step forward. We expect that future
research will build on these results to tighten estimates and
extend our methodology.
1We use the term address block in preference to subnetwork
because a subnet is the unit of router conﬁguration, and we
cannot know how the actual edge routers are conﬁgured.
Our ﬁnal contribution is to study trends in the deployment
of ﬁrewalls on the public Internet (Section 6). Firewalls re-
spond to probes in several diﬀerent ways, perhaps respond-
ing negatively, or not responding at all, or in some cases
varying their response over time [42, 3]. Estimating the ex-
act number of ﬁrewalls is therefore quite diﬃcult. However,
we present trends in ﬁrewalls that respond negatively over
seven censuses spread over 15 months. Many such ﬁrewalls
are visible and we observe signiﬁcant diversity in the distri-
bution of ﬁrewalled block size. While the absolute number
of ﬁrewalled blocks appears stable, the ratio of coverage of
visible ﬁrewalls to the number of visible addresses is declin-
ing, perhaps suggesting increasing use of invisible ﬁrewalls.
2. CENSUS AND SURVEY METHODOLOGY
Statistical population sampling has developed two tools to
study human or artiﬁcial populations: censuses, that enu-
merate all members of a population; and surveys that con-
sider only a sample. Our goal is to adapt these approaches to
study the Internet address space. These tools complement
each other, since a census can capture unexpected varia-
tion or rare characteristics of a population, while surveys
are much less expensive and so can answer more focused
questions and be taken more frequently. We expect cen-
suses to capture the diversity of the Internet [37] as shown
in our ﬁrewall estimates (Section 6), while surveys allow us
to evaluate dynamic address usage (Section 5.1).
An Internet census poses several challenges. At ﬁrst glance,
the large number of addresses seems daunting, but there are
only 232, and only about half of these are allocated, public,
unicast addresses, so a relatively modest probe rate of 1000
probes/s (about 256kb/s) can enumerate the entire space in
49 days. Also challenging is how to interpret the results;
we use censuses to study trends (Section 5.3) and ﬁrewalls
(Section 6). We also must probe in a manner that is unlikely
to be confused with malicious scans, and to understand the
eﬀects of lost probes on the results.
Complementing censuses, surveys avoid the problem of
population size by probing a subset of addresses.
Instead
it poses the question of who is sampled and how often.
Their primary challenge is to ensure that the sample is large
enough to provide conﬁdence in its representation of Inter-
net, that it is unbiased, and to understand what measure-
ment uncertainty sampling introduces. We review these ap-
proaches next, and then explore their limits and results.
2.1 Probing Design
Like tools such as Nmap [38], our approaches are forms of
active probing. Census and survey share common choices in
how probes are made and interpreted.
Requests: For each address, we send a single probe mes-
sage and then record the time until a reply is received as well
as any (positive or negative) reply code. We record lack of
a reply after a liberal timeout (currently 5s, while 98% of
responses are returned in less than 0.6s) as a non-reply.
Several protocols could be used for probing,
including
TCP, UDP, and ICMP. Two requirements inﬂuence our
choice. The ﬁrst is response ubiquity—ideally all hosts will
understand our probes and react predictably. Second, we
desire probes that are innocuous and not easily confused
with malicious scans or denial-of-service attacks.
We probe with ICMP echo-request messages because many
hosts respond to pings and it is generally considered be-
nign. We considered TCP because of the perception that it
is less frequently ﬁrewalled and therefore more accurate than
ICMP, but discarded it after one early census (TCP 1 , Ta-
ble 1) because that survey elicited thirty times more abuse
complaints than ICMP surveys. We study this trade-oﬀ in
Section 3.2, showing that while there is signiﬁcant ﬁltering,
ICMP is a more accurate form of active probing than TCP.
Replies: Each ICMP echo request can result in several
potential replies [23], which we interpret as following:
Positive acknowledgment: We receive an echo reply (type
0), indicating the presence of a host at that address.
Negative acknowledgment: We receive a destination un-
reachable (type 3), indicating that host is either down or
the address is unused. In Section 6 we subdivide negative
replies based on response code, interpreting codes for net-
work, host, and communication administratively prohibited
(codes 9, 10, and 13) as positive indication of a ﬁrewall.
We receive some other negative replies; we do not consider
them in our analysis. Most prominent are time-exceeded
(type 11), accounting for 30% of responses and 3% of probes;
other types account for about 2% of responses.
No reply: Non-response can have several possible causes.
First, either our probe or its response could have acciden-
tally failed to reach the destination due to congestion or
network partition. Second, it may have failed to reach the
destination due to intentionally discard by a ﬁrewall. Third,
the address may not be occupied (or the host temporarily
down) and its last-hop router may decline to generate an
ICMP reply.
Request frequency:
Each run of a census or survey
covers a set of addresses. Censuses have one pass over the
entire Internet, while surveys make a multiple passes over
a smaller sample (described below). Each pass probes each
address once in a pseudo-random order.
We probe in a pseudo-random sequence so that the probes
to any portion of the address space are dispersed in time.
This approach also reduces the correlation of network out-
ages to portions of the address space, so that the eﬀects of
any outage near the prober are distributed uniformly across
the address space. Dispersing probes also reduces the like-
lihood that probing is considered malicious.
One design issue we may reconsider is retransmission of
probes for addresses that fail to respond. A second probe
would reduce the eﬀects of probe loss, but it increases the
cost of the census. Instead, we opted for more frequent cen-
suses rather than a more reliable single census. We consider
the eﬀects of loss in Section 3.5.
Implementation requirements: Necessary characteris-
tics of our implementation are that it enumerate the Inter-
net address space completely, dispersing probes to any block
across time, in a random order, and that it support selecting
or blocking subsets of the space. Desirable characteristics
are that the implementation be parallelizable and permit
easy checkpoint and restart. Our implementation has these
characteristics; details appear in our technical report [18].
2.2 Census Design and Implementation
Our census is an enumeration of the allocated Internet
address space at the time the census is conducted. We do not
probe private address space [39], nor multicast addresses.
We also do not probe addresses with last octet 0 or 255, since
those are often unused or allocated for local broadcast in
/24 networks. We determine the currently allocated address
space from IANA [22]. IANA’s list is actually a superset of
the routable addresses, since addresses may be assigned to
registrars but not yet injected into global routing tables [31].
We probe all allocated addresses, not just those currently
routed, because it is a strict superset and because routing
may change over census duration as they come on-line or
due to transient outages.
An ideal census captures an exact snapshot of the Internet
at given moment in time, but a practical census takes some
time to carry out, and the Internet changes over this time.
Probing may also be aﬀected by local routing limitations,
but we show that diﬀerences in concurrent censuses are rel-
atively small and not biased due to location in Section 3.3.
We have run censuses from two sites in the western and
eastern United States. Probes run as fast as possible, limited
by a ﬁxed number of outstanding probes, generating about
166kb/s of traﬃc. Our western site is well provisioned, but
we consume about 30% of our Internet connection’s capacity
at our eastern site. Table 1 shows our censuses censuses since
June 2003 and surveys since March 2006. (Two anomalies
appear over this period: The NACK rates in two censuses
marked with asterisks, IT 11w and IT 12w , were corrected
to remove around 700M NACKs generated from probes to
non-routable addresses that pass through a single, oddly
conﬁgured router. Also, the decrease in allocated addresses
between 2003 and 2004 is due to IANA reclamation, not the
coincidental change in methodology.)
2.3 Survey Design and Implementation
Survey design issues include selecting probe frequency of
each address and selecting the sample of addresses to survey.
How many: Our choice of how many addresses to sur-
vey is governed by several factors: we need a sample large
enough to be reasonably representative of the Internet pop-
ulation, yet small enough that we can probe each address
frequently enough to capture individual host arrival and de-
parture with reasonable precision. We studied probing in-
tervals as small as 5 minutes (details omitted due to space);
based on those results we select an interval of 11 minutes as
providing reasonable precision, and being relatively prime to
common human activities that happen on multiples of 10,
30, and 60 minutes. We select a survey size of about 1% of
the allocated address space, or 24,000 /24 blocks to provide
good coverage of all kinds of blocks and reasonable measure-
ment error; we justify this fraction in Section 4.2. A survey
employs a single machine to probe this number of addresses.
To pace replies, we only issue probes at a rate that matches
the timeout rate, resulting in about 9,200 probes/second.
At this rate, each /24 block receives a probe once every 2–3
seconds.