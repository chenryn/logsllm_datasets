what relation they have to the sensors we have discussed within our model.
5.1 Static Sensor
One of the simplest types of sensors in real programs is the self-memory check.
A program can compute a hash of its own memory and compare it against a
stored value in order to determine if anything has been changed. We represent
this capability in our model with the following deﬁnition:
Deﬁnition 7 (Static Sensor). Any program M has oracle access to a Sensor(S),
where Sensor(S) = R(S), R is the random oracle from the hardware H, and
S ⊆ ID(U(M)).
We show why this simple type of sensor cannot be used to ensure integrity
for obfuscation.
Lemma 1. An adversary can leak the return value of Sensor(S) in the System-
Interaction model.
Proof. We begin by changing U to write out the entire contents of the system
tape to the user-tape just before any call to Sensor(S). The tape contents D
are then modiﬁed to remove the changes made to U. The modiﬁed version of D
will be denoted D(cid:4). Then a new program M(cid:4) is constructed with the old tape
contents appended as data D(cid:4). The new program M(cid:4) contains D(cid:4), and makes
a call to Sensor(S(cid:4)), where S(cid:4) is now equal to subset of the tape that contains
D(cid:4). The call to this sensor will return the same value as the sensor call in the
original program M. This shows that the return value of the static sensor, even
when measuring any subset of the system, will not remain hidden.
226
J. Blackthorne et al.
5.2 Dynamic Sensor
Programs can also measure properties that change over time. A practical example
is the use of the RDTSC x86 assembly instruction which measures an internal
hardware clock. Pairs of these instructions can be used to measure the time it
takes for code to execute between them. We formalize the idea of measurement
over time by ﬁrst introducing the notion of a trace.
Deﬁnition 8 (Trace). The trace (Tr) of a TM M is deﬁned as the ordered set
of IDs of M from timestep= 0 . . . t − 1, where t is the current timestep.
To represent simple dynamic sensor like RDTSC, we formalize a sensor which
sums all the values in a trace.
Deﬁnition 9 (Dynamic Sensor). Any program M has oracle access to Sen-
R(IDi)) | IDi ∈ T r(U(M)), and t is the
t−1(cid:6)
sor(Tr), deﬁned as Sensor(T r) =
current time-step.
i=0
This sensor is also inadequate for use in ensuring integrity for obfuscation
purposes.
Lemma 2. An adversary can leak the return value of any call to Sensor(Tr) in
a program M in the System-Interaction model.
Proof. Leaking the value of Sensor(Tr) in a program M is trivial, simply because
Sensor(Tr) does not measure any instructions that occur after the call to the
sensor. To leak the value, modify the part of U that occurs after the call to
Sensor(T r) to write the result of the sensor call to the user-tape. Multiple calls
to the sensor could be leaked in turn. For each call to Sensor(Tr), generate a
new program which writes the return value of that call to Sensor(Tr) to the
user-tape.
5.3 Static and Dynamic Sensors
Real programs often combine static and dynamic sensors. A practical example
is a program that both computes hashes over its memory and checks the time it
takes to compute those hashes. Even this combination of sensors cannot ensure
integrity for obfuscation.
Theorem 6. Semantic obfuscation is not possible with combined static and
dynamic sensors when the dynamic sensor is learnable.
Proof. The following is a simple algorithm to extract all of the sensor readings:
First apply Lemma 1 to all calls to Sensor(S). Next we must extract the calls to
Sensor(Tr). There are two possible cases.
Case 1: We can modify U to write the result of Sensor(Tr) to the user-tape
without any call to Sensor(S) being aﬀected. If this is true, we can apply Lemma 2
and are ﬁnished.
A Formal Framework for Environmentally Sensitive Malware
227
Case 2: There exists a call to Sensor(S) that will measure any modiﬁcation
of U needed to write the value of Sensor(Tr) to the user-tape. In this case,
Sensor(Tr) will be aﬀected by the modiﬁed return value of Sensor(S). But because
Sensor(Tr) is linear and thus learnable, we can determine what the return value
of Sensor(Tr) should be through summing a series of calls to Sensor(S), where S
is set to areas of the tape that contain the intended ID(U(M)) for that timestep.
Discussion. The underlying reason of this impossibility is the same as why a
learnable sensor in general cannot be used. It is easy to see that this same
impossibility applies when the dynamic sensor is a random oracle. Both cases
reduce to the cases discussed in Sect. 4.
Now we will consider a construction of combined static and dynamic sensors,
but this time we will assume the dynamic sensor is the piece-wise learnable
sensor described in Sect. 4.
Theorem 7. Given a static sensor and piece-wise learnable dynamic sensor,
there exists a semantic obfuscator within the System-Interaction model.
Proof. Let M be a program with no user input or output in the System-
Interaction model. Let M have access to Sensor(S), which is a random oracle.
Let M have access to Sensor(T r) which is piece-wise learnable. We construct
the program Mo = O(M, U) wrapped by Mk, such that Mk is VBB obfuscated
and calls Mo on input k. We construct Mk in the same method as Theorem 5,
but this time so that it ﬁrst calls Sensor(S) where S = U(Mk) and then calls
x2 = Sensor(T r). The program Mk then checks to see if x2 is equal to k.
The program Mk, upon receiving k, calls Mo. This Mo and original M are
input–output equivalent. The wrapper Mk calls Mo in constant time, so we may
say that there is at most polynomial slowdown. Finally, we must establish the
semantic security property.
The adversary can extract a semantic predicate from the source of Mk –
which includes Mo – or modify U to print out additional information about Mk
or Mo. We know that Mk is VBB obfuscated so no information can be attained
from the source that can not also be attained from running Mk. The adversary
can modify U to print out the value of x2 by copying it to the user tape. If
any modiﬁcations are made to U, then the value x2 will change and become
independent of the original value, thus leaking no information. The program Mk
will halt upon running Mk(x(cid:4)
2), thus not allowing the adversary to determine
any semantic predicates about M.
6 Conclusion
We have provided a formal framework for which to describe environmental sensi-
tivity in programs. We constructed a well-deﬁned standard of obfuscation within
that framework, and we have shown the necessary and suﬃcient conditions of
achieving that standard. We believe our research has formed a basis for which to
construct practical, semantically obfuscated programs. The clear next step, is to
now construct prototypes that fulﬁll the requirements of semantic obfuscation.
228
J. Blackthorne et al.
References
1. Apon, D., Huang, Y., Katz, J., Malozemoﬀ, A.J.: Implementing cryptographic
program obfuscation (2014)
2. Arora, S., Barak, B.: Randomized computation. In: Computational Complexity:
A Modern Approach, pp. 121–122. Cambridge University Press, New York (2012).
Chap. 7, Sect. 7.5.3
3. Barak, B., Garg, S., Kalai, Y.T., Paneth, O., Sahai, A.: Protecting obfuscation
against algebraic attacks. Cryptology ePrint Archive, Report 2013/631 (2013).
http://eprint.iacr.org/2013/631.pdf. Accessed 6 Apr 2015
4. Barak, B., Goldreich, O., Impagliazzo, R., Rudich, S., Sahai, A., Vadhan, S., Yang,
K.: On the (im)possibility of obfuscating programs. Cryptology ePrint Archive,
Report 2001/069 (2001). http://eprint.iacr.org/
5. Barak, B., Goldreich, O., Impagliazzo, R., Rudich, S., Sahai, A., Vadhan, S.P.,
Yang, K.: On the (im)possibility of obfuscating programs. In: Kilian, J. (ed.)
CRYPTO 2001. LNCS, vol. 2139, p. 1. Springer, Heidelberg (2001)
6. Basile, C., et al.: Towards a formal model for software tamper resistance. COSIC,
University of Leuven, Flanders, Belgium (2009). https://www.cosic.esat.kuleuven.
be/publications/article-1280.pdf. Accessed 6 Apr 2015
7. Beaucamps, P., Filiol, E.: On the possibility of practically obfuscating programs
towards a uniﬁed perspective of code protection. J. Comput. Virol. 3(1), 3–21
(2007)
8. Bernstein, D.J., H¨ulsing, A., Lange, T., Niederhagen, R.: Bad directions in cryp-
tographic hash functions. In: Foo, E., Stebila, D. (eds.) ACISP 2015. LNCS, vol.
9144, pp. 488–508. Springer, Heidelberg (2015)
9. Bitansky, N., Canetti, R.: On strong simulation and composable point obfusca-
tion. In: Rabin, T. (ed.) CRYPTO 2010. LNCS, vol. 6223, pp. 520–537. Springer,
Heidelberg (2010)
10. Bitansky, N., Canetti, R., Kalai, Y.T., Paneth, O.: On virtual grey box obfuscation
for general circuits. In: Garay, J.A., Gennaro, R. (eds.) CRYPTO 2014, Part II.
LNCS, vol. 8617, pp. 108–125. Springer, Heidelberg (2014)
11. Borello, J.M., M´e, L.: Code obfuscation techniques for metamorphic viruses. J.
Comput. Virol. 4(3), 211–220 (2008)
12. Brakerski, Z., Rothblum, G.N.: Virtual black-box obfuscation for all circuits
via generic graded encoding. Cryptology ePrint Archive, Report 2013/563
(2013). http://eprint.iacr.org/2013-563.pdf, http://eprint.iacr.org/2013-563.pdf.
Accessed 6 Apr 2015
13. Canetti, R., Varia, M.: Non-malleable obfuscation. In: Reingold, O. (ed.) TCC
2009. LNCS, vol. 5444, pp. 73–90. Springer, Heidelberg (2009)
14. Chen, X., Andersen, J., Mao, Z., Bailey, M., Nazario, J.: Towards an understanding
of anti-virtualization and anti-debugging behavior in modern malware. In: IEEE
International Conference on Dependable Systems and Networks with FTCS and
DCC, DSN 2008, pp. 177–186, June 2008
15. Collberg, C., Thomborson, C., Low, D.: A taxonomy of obfuscating transforma-
tions. Technical report 148. Department of Computer Science University of Auck-
land, 36 p., July 1997. http://scholar.google.com/scholar?hl=en&btnG=Search&
q=intitle:A+Taxonomy+of+Obfuscating+Transformations#0
16. Dinaburg, A., Royal, P., Sharif, M., Lee, W.: Ether: malware analysis via hardware
virtualization extensions. In: Proceedings of the 15th ACM Conference on Com-
puter and Communications Security, CCS 2008, pp. 51–62 (2008). http://dl.acm.
org/citation.cfm?id=1455779
A Formal Framework for Environmentally Sensitive Malware
229
17. Ferrie, P.: Attacks on more virtual machine emulators. Technical report. Symantec
Advanced Threat Research (2007)
18. Ferrie, P.: The Ultimate Anti-Debugging Reference, May 2011. http://pferrie.
host22.com/papers/antidebug.pdf. Accessed 6 Apr 2015
19. Garﬁnkel, T., Adams, K., Warﬁeld, A., Franklin, J.: Compatibility is not trans-
parency: VMM detection myths and realities. In: Proceedings of 11th USENIX
Workshop on Hot Topics in Operating Systems, pp. 6:1–6:6 (2007). http://dl.acm.
org/citation.cfm?id=1361397.1361403
20. Garg, S., et al.: Candidate indistinguishability obfuscation and functional encryp-
tion for all circuits. In: FOCS 2013, pp. 40–49 (2013)
21. Goldwasser, S., Rothblum, G.N.: On best-possible obfuscation. In: Proceedings of
4th Theory Cryptography Conference, pp. 194–213 (2007)
22. Kang, M.G., Yin, H., Hanna, S., McCamant, S., Song, D.: Emulating emulation-
resistant malware. In: Proceedings of the 1st ACM Workshop on Virtual Machine
Security, VMSec 2009, pp. 11–22. ACM, New York (2009). http://doi.acm.org/10.
1145/1655148.1655151
23. Moon, P.: The use of packers, obfuscators and encryptors in modern malware the
use of packers, obfuscators and encryptors in modern malware. Technical report,
Royal Holloway University of London, March 2015
24. Nithyanand, R., Solis, J.: A theoretical analysis: physical unclonable functions
and the software protection problem. In: Proceedings of 2012 IEEE Symposium
Security and Privacy Workshop, pp. 1–11 (2012)
25. Nithyanand, R., Sion, R., Solis, J.: Solving the software protection problem with
intrinsic personal physical unclonable functions. Sandia National Laboratories, Liv-
ermore, CA, USA. Report SAND2011-6603 (2011)
26. Paleari, R., Martignoni, L., Roglia, G.F., Bruschi, D.: A ﬁstful of red-pills: how to
automatically generate procedures to detect CPU emulators. In: Proceedings of the
3rd USENIX Conference on Oﬀensive Technologies, WOOT 2009, p. 2. USENIX
Association, Berkeley (2009). http://dl.acm.org/citation.cfm?id=1855876.1855878
27. Plaga, R., Koob, F.: A formal deﬁnition and a new security mechanism of phys-
ical unclonable functions. In: Proceedings 16th International GI/ITG Conference
Measurement, Modeling, and Evaluation of Computing Systems and Dependability
and Fault Tolerance, pp. 228–301 (2012). http://arxiv.org/abs/1204.0987
28. Popov, I.V., Debray, S.K., Andrews, G.R.: Binary obfuscation using signals. In:
Proceedings of 16th USENIX Security Symposium on USENIX Security Sympo-
sium, SS 2007, pp. 19:1–19:16. USENIX Association, Berkeley (2007). http://dl.
acm.org/citation.cfm?id=1362903.1362922
29. Saxena, A., Wyseur, B., Preneel, B.: Towards security notions for white-box cryp-
tography. In: Samarati, P., Yung, M., Martinelli, F., Ardagna, C.A. (eds.) ISC
2009. LNCS, vol. 5735, pp. 49–58. Springer, Heidelberg (2009)
30. Sikorski, M., Honig, A.: Practical Malware Analysis: The Hands-On Guide to Dis-
secting Malicious Software, 1st edn. No Starch Press, San Francisco (2012)