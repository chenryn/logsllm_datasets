### Relation to the Sensors Discussed in Our Model

In our model, we have discussed various types of sensors that can be used to detect and measure changes in a program's environment. These sensors are crucial for ensuring the integrity and security of the program. In this section, we will explore two primary types of sensors: static and dynamic, and their limitations in ensuring integrity for obfuscation purposes.

#### 5.1 Static Sensor

One of the simplest types of sensors in real programs is the self-memory check. A program can compute a hash of its own memory and compare it against a stored value to determine if anything has been altered. We represent this capability in our model with the following definition:

**Definition 7 (Static Sensor).** Any program \( M \) has oracle access to a sensor \( \text{Sensor}(S) \), where \( \text{Sensor}(S) = R(S) \), \( R \) is a random oracle from the hardware \( H \), and \( S \subseteq \text{ID}(U(M)) \).

We show why this simple type of sensor cannot be used to ensure integrity for obfuscation.

**Lemma 1.** An adversary can leak the return value of \( \text{Sensor}(S) \) in the System-Interaction model.

**Proof.** 
1. Modify \( U \) to write out the entire contents of the system tape to the user-tape just before any call to \( \text{Sensor}(S) \).
2. The tape contents \( D \) are then modified to remove the changes made to \( U \). The modified version of \( D \) will be denoted as \( D' \).
3. Construct a new program \( M' \) with the old tape contents appended as data \( D' \). The new program \( M' \) contains \( D' \) and makes a call to \( \text{Sensor}(S') \), where \( S' \) is now equal to a subset of the tape that contains \( D' \).
4. The call to this sensor will return the same value as the sensor call in the original program \( M \). This shows that the return value of the static sensor, even when measuring any subset of the system, will not remain hidden.

#### 5.2 Dynamic Sensor

Programs can also measure properties that change over time. A practical example is the use of the RDTSC x86 assembly instruction, which measures an internal hardware clock. Pairs of these instructions can be used to measure the time it takes for code to execute between them. We formalize the idea of measurement over time by first introducing the notion of a trace.

**Definition 8 (Trace).** The trace \( \text{Tr} \) of a Turing Machine \( M \) is defined as the ordered set of IDs of \( M \) from timestep 0 to \( t-1 \), where \( t \) is the current timestep.

To represent a simple dynamic sensor like RDTSC, we formalize a sensor that sums all the values in a trace.

**Definition 9 (Dynamic Sensor).** Any program \( M \) has oracle access to \( \text{Sensor}(\text{Tr}) \), defined as:
\[
\text{Sensor}(\text{Tr}) = \sum_{i=0}^{t-1} R(\text{ID}_i) \quad \text{where} \quad \text{ID}_i \in \text{Tr}(U(M)), \quad \text{and} \quad t \text{ is the current time-step.}
\]

This sensor is also inadequate for use in ensuring integrity for obfuscation purposes.

**Lemma 2.** An adversary can leak the return value of any call to \( \text{Sensor}(\text{Tr}) \) in a program \( M \) in the System-Interaction model.

**Proof.**
1. Leaking the value of \( \text{Sensor}(\text{Tr}) \) in a program \( M \) is trivial because \( \text{Sensor}(\text{Tr}) \) does not measure any instructions that occur after the call to the sensor.
2. To leak the value, modify the part of \( U \) that occurs after the call to \( \text{Sensor}(\text{Tr}) \) to write the result of the sensor call to the user-tape.
3. Multiple calls to the sensor could be leaked in turn. For each call to \( \text{Sensor}(\text{Tr}) \), generate a new program that writes the return value of that call to \( \text{Sensor}(\text{Tr}) \) to the user-tape.

#### 5.3 Combined Static and Dynamic Sensors

Real programs often combine static and dynamic sensors. A practical example is a program that both computes hashes over its memory and checks the time it takes to compute those hashes. Even this combination of sensors cannot ensure integrity for obfuscation.

**Theorem 6.** Semantic obfuscation is not possible with combined static and dynamic sensors when the dynamic sensor is learnable.

**Proof.**
1. First, apply Lemma 1 to all calls to \( \text{Sensor}(S) \).
2. Next, extract the calls to \( \text{Sensor}(\text{Tr}) \). There are two possible cases:
   - **Case 1:** Modify \( U \) to write the result of \( \text{Sensor}(\text{Tr}) \) to the user-tape without affecting any call to \( \text{Sensor}(S) \). If this is true, apply Lemma 2 and the proof is complete.
   - **Case 2:** There exists a call to \( \text{Sensor}(S) \) that will measure any modification of \( U \) needed to write the value of \( \text{Sensor}(\text{Tr}) \) to the user-tape. In this case, \( \text{Sensor}(\text{Tr}) \) will be affected by the modified return value of \( \text{Sensor}(S) \). But because \( \text{Sensor}(\text{Tr}) \) is linear and thus learnable, we can determine what the return value of \( \text{Sensor}(\text{Tr}) \) should be through summing a series of calls to \( \text{Sensor}(S) \), where \( S \) is set to areas of the tape that contain the intended \( \text{ID}(U(M)) \) for that timestep.

**Discussion.** The underlying reason for this impossibility is the same as why a learnable sensor in general cannot be used. It is easy to see that this same impossibility applies when the dynamic sensor is a random oracle. Both cases reduce to the cases discussed in Section 4.

Now, we will consider a construction of combined static and dynamic sensors, but this time we will assume the dynamic sensor is the piece-wise learnable sensor described in Section 4.

**Theorem 7.** Given a static sensor and a piece-wise learnable dynamic sensor, there exists a semantic obfuscator within the System-Interaction model.

**Proof.**
1. Let \( M \) be a program with no user input or output in the System-Interaction model.
2. Let \( M \) have access to \( \text{Sensor}(S) \), which is a random oracle.
3. Let \( M \) have access to \( \text{Sensor}(\text{Tr}) \), which is piece-wise learnable.
4. Construct the program \( M_o = O(M, U) \) wrapped by \( M_k \), such that \( M_k \) is VBB obfuscated and calls \( M_o \) on input \( k \).
5. Construct \( M_k \) in the same method as Theorem 5, but this time so that it first calls \( \text{Sensor}(S) \) where \( S = U(M_k) \) and then calls \( x_2 = \text{Sensor}(\text{Tr}) \).
6. The program \( M_k \) then checks to see if \( x_2 \) is equal to \( k \).
7. Upon receiving \( k \), \( M_k \) calls \( M_o \). This \( M_o \) and the original \( M \) are inputâ€“output equivalent.
8. The wrapper \( M_k \) calls \( M_o \) in constant time, so there is at most polynomial slowdown.
9. Finally, establish the semantic security property. The adversary can extract a semantic predicate from the source of \( M_k \) or modify \( U \) to print out additional information about \( M_k \) or \( M_o \).
10. Since \( M_k \) is VBB obfuscated, no information can be attained from the source that cannot also be attained from running \( M_k \).
11. The adversary can modify \( U \) to print out the value of \( x_2 \) by copying it to the user tape. If any modifications are made to \( U \), then the value \( x_2 \) will change and become independent of the original value, thus leaking no information.
12. The program \( M_k \) will halt upon running \( M_k(x_2') \), thus not allowing the adversary to determine any semantic predicates about \( M \).

### 6 Conclusion

We have provided a formal framework for describing environmental sensitivity in programs. We constructed a well-defined standard of obfuscation within that framework and have shown the necessary and sufficient conditions for achieving that standard. We believe our research has formed a basis for constructing practical, semantically obfuscated programs. The clear next step is to construct prototypes that fulfill the requirements of semantic obfuscation.

### References

[References listed as in the original text]