P (x1, ..., xm) log P (x1, ..., xm),
X
X1,...,Xm
where P (x1, ..., xm) is the joint probability P (X1 = x1, ...,
Xm = xm).
Then, from the entropy of a sequence of random variables,
we deﬁne the conditional entropy of a random variable given
a previous sequence of random variables as:
H(Xm | X1, ..., Xm−1) = H(X1, ..., Xm)− H(X1, ..., Xm−1).
Lastly, the entropy rate of a random process is deﬁned as:
H(X) = lim
m→∞ H(Xm | X1, ..., Xm−1).
The entropy rate is the conditional entropy of an inﬁ-
nite sequence and, therefore, cannot be measured for ﬁnite
samples. Thus, we estimate the entropy rate with the con-
ditional entropy of ﬁnite samples.
3.2 Corrected Conditional Entropy
The exact entropy rate cannot be measured for ﬁnite sam-
ples and must be estimated. In practice, probability den-
sity functions are replaced with empirical probability density
functions based on the method of histograms. The data is
binned in Q bins. The speciﬁc binning strategy being used
is important to the overall eﬀectiveness of the test and is
discussed in Section 3.3. The empirical probability density
functions are determined by the proportions of patterns in
the data, i.e., the proportion of a pattern is the probability of
that pattern. Here a pattern is deﬁned as a sequence of bin
numbers. The estimates of the entropy or conditional en-
tropy, based on the empirical probability density functions,
are represented as: EN and CE, respectively.
There is a problem with the estimation of CE(Xm | Xm−1)
for some values of m. The conditional entropy tends to zero
as m increases, due to limited data. If a speciﬁc pattern of
length m − 1 is found only once in the data, then the ex-
tension of this pattern to length m will also be found only
once. Therefore, the length m pattern can be predicted by
the length m − 1 pattern, and the length m and m − 1 pat-
terns cancel out. If no pattern of length m is repeated in the
data, then CE(Xm | Xm−1) is zero, even for i.i.d. processes.
To solve the problem of limited data, without ﬁxing the
length of m, we use the corrected conditional entropy (CCE)
[18]. The corrected conditional entropy is deﬁned as:
CCE(Xm | Xm−1) = CE(Xm | Xm−1)+perc(Xm)·EN (X1),
where perc(Xm) is the percentage of unique patterns of
length m and EN (X1) is the entropy with m ﬁxed at 1
or the ﬁrst-order entropy.
The estimate of the entropy rate is the minimum of the
corrected conditional entropy over diﬀerent values of m. The
minimum of the corrected conditional entropy is considered
to be the best estimate of the entropy rate with the available
data. The corrected conditional entropy has a minimum, be-
cause the conditional entropy decreases while the corrective
term increases. The corrected conditional entropy has been
mainly used on biological data, such as electrocardiogram
[18] and electroencephalogram data [19]. Although not re-
lated to our work, it is interesting to see how such a measure
can diﬀerentiate the states of complex biological processes.
For example, with the electroencephalogram, an increase in
the entropy rate indicates a decrease in the depth of anes-
thesia, i.e., the subject is becoming more conscious.
Figure 3: The equiprobable binning of Exponential
data in Q = 5 bins
n
o
i
t
r
o
p
o
r
p
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
1
2
3
4
5
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
value
3.3 Binning Strategies
The strategy of binning the data is critical to the overall
eﬀectiveness of the test. The binning strategy mainly de-
cides: (1) how the data is partitioned and (2) the bin granu-
larity or the number of bins Q. In previous work, partition-
ing data into equiprobable bins seems to be most eﬀective
[18, 19]. The use of equiprobable bins is illustrated in Figure
3, showing the partitioning of Exponential data into bins of
equal area. The bins, numbered 1 through 5, are small in
width when the proportion of values is high and large in
width when the proportion of values is low. Thus, while the
bins have diﬀerent widths, the total area of each bin is equal.
The bin number for a value can then be determined based
on the cumulative distribution function:
bin = (cid:6)F (x)(cid:7),
where F is the cumulative distribution function and x is the
value to be binned.
The bin numbers can also be determined based on ranges,
e.g., 0.0 < bin1 ≤ 0.22, 0.22 < bin2 ≤ 0.51, 0.51 < bin3 ≤
0.91, and so on, which requires a search of the ranges to de-
termine the correct bin number for a value. Meanwhile, the
cumulative distribution function can determine the correct
bin in constant time, which is important for performance
when the number of bins is large.
m
The choice of the number of bins oﬀers a tradeoﬀ. While
a larger number of bins retains more information about the
distribution of the data, it increases the number of possible
and, thus, limits the ability of the test to rec-
patterns Q
ognize longer patterns due to the limited data. In contrast,
a small number of bins captures less information about the
distribution, but is better able to measure the regularity
of the data. Therefore, as both strategies have advantages
and disadvantages, we use both coarse-grain and ﬁne-grain
binning.
To determine the best choice of Q for coarse-grain binning,
we run tests on correlated and uncorrelated samples for Q =
2 through 10. The correlated samples are 100 traces of 2,000
HTTP inter-packet delays. The uncorrelated samples are
random permutations of the correlated samples. We then
count the number of uncorrelated samples with scores that
overlap with the scores of correlated samples. There is no
overlap for the values of Q = 5 to 8. Therefore, to retain the
ability of the test to recognize longer patterns and measure
regularity, we use Q = 5 for coarse-grain binning.
m
It is much simpler to determine the best choice of Q for
ﬁne-grain binning. With increasing values of Q, the number
becomes much larger than the size
of possible patterns Q
of the sample being tested. At this point, the test scores are
dominated by the estimate of the entropy for length 1. Then,
as we increase the value of Q, the bins continue to become
more precise, leading to a better estimate of the entropy for
length 1 than that for smaller values of Q. Therefore, as Q
= 65,536
can be made arbitrarily precise, we use Q = 2
for ﬁne-grain binning.
3.4 Implementation Details
16
Our design goal is to be eﬀective in detection and eﬃcient
in terms of run-time and storage. The eﬃciency of tests is
particularly important if tests are conducted in real-time for
online processing of data. Thus, we are careful to optimize
our implementation for performance. We implement the cor-
rected conditional entropy in the C programming language.
The patterns are represented as nodes in a Q-ary tree of
height m. The nodes of the tree include pattern counts and
links to the nodes with longer patterns. The level of the tree
corresponds to the length of patterns. The children of the
root are the patterns of length 1. The leaf nodes are the
patterns of length m.
To add a new pattern of length m to the tree, we move
down the tree towards the leaves, updating the counts of the
intermediate nodes and creating new nodes. Thus, when we
reach the bottom of the tree, we have counted both the new
pattern and all of its sub-patterns. After all patterns of
length m are added, we perform a breadth-ﬁrst traversal.
The breadth-ﬁrst traversal computes the corrected condi-
tional entropy at each level and terminates when the mini-
mum is obtained. If the breadth-ﬁrst traversal reaches the
bottom of the tree without having the minimum, then we
must increase m and continue.
The time and space complexities are O(n · m), where n is
the size of the sample, if we assume a priori knowledge of the
distribution and use the cumulative distribution function to
determine the correct bin for each value in constant time.
Otherwise, the time complexity increases to O(n·m·log(Q)).
In practice, running our program on a sample of size 2,000
with Q = 5 and a pattern of length 10 on our test machine,
an Intel Pentium D 3.4Ghz, takes 16 milliseconds. However,
small changes in the implementation can have signiﬁcant
impact on performance.
To demonstrate this, we evaluate the computation over-
head of our implementation and that of a previous imple-
mentation [19]. The computation time of both implemen-
tations with increasing pattern length is shown in Figure
4. For small values of m, our computation time is slightly
longer, because of the overhead of creating our data struc-
ture. However, as m increases, the previous implementa-
tion increases quadratically, whereas our implementation in-
creases linearly. The quadratic growth is caused by the sep-
arate processing of patterns of diﬀerent lengths, i.e., the pat-
terns of length 1, then the patterns of length 2, and so on,
which introduces a quadratic term due to the summation of
the pattern lengths:
Pm
i=1 i = m2+m
.
2
)
s
d
n
o
c
e
s
(
e
m
i
t
n
o
i
t
a
t
u
p
m
o
c
 128
 64
 32
 16
 8
 4
 2
 1
 0.5
 0.25
 0.125
Figure 4: CCE performance
new CCE
old CCE
 1
 2
 3
 4
 5
 6
 7
 8
pattern length
4. EXPERIMENTAL EVALUATION
In this section, we validate the eﬀectiveness of our pro-
posed approach through a series of experiments. The focus
of these experiments is to determine if our entropy-based
methods (entropy and corrected conditional entropy) are
able to detect covert timing channels. We test our entropy-
based methods against three covert timing channels: IPCTC
[7], TRCTC [6], and JitterBug [20]. Furthermore, we com-
pare our entropy-based methods to two other detection tests:
the Kolmogorov-Smirnov test and the regularity test [7].
The purpose of a detection test is to diﬀerentiate covert
traﬃc from legitimate traﬃc. The performance of a detec-
tion test can be measured based on false positive and true
positive rates, with low false positive rate and high true
positive rate being ideal. In practice, because of the large
variation in legitimate network traﬃc, it is important that
tests work well for typical traﬃc and occasional outliers. If
a detection test gives test scores with signiﬁcant overlap be-
tween legitimate and covert samples, then it fails on detec-
tion. Therefore, the mean, variance, and distribution of test
scores are critical metrics to the performance of a detection
test.
4.1 Experimental Setup
The defensive perimeter of a network, made up of ﬁre-
walls and intrusion detection systems, is designed to protect
the network from malicious traﬃc. Typically, only a few
speciﬁc application protocols, such as HTTP and SMTP, al-
though heavily monitored, are allowed to pass through the
defensive perimeter.
In addition, other protocols, such as
SSH, might be permitted to cross the perimeter but only to
speciﬁc trusted destinations.
We now consider the scenarios discussed in Section 2. In
the ﬁrst scenario, which relates to IPCTC and TRCTC, a
compromised machine uses a covert timing channel to com-
municate with a machine outside the network. For IPCTC
and TRCTC, we utilize outgoing HTTP inter-packet de-
lays as the medium, due to the wide acceptance of HTTP
for crossing the network perimeter and the high volume of
HTTP traﬃc. In the second scenario, which relates to Jit-
terBug, a compromised input device uses a covert timing
channel to leak typed information over the traﬃc of a net-
worked application. For JitterBug, we utilize outgoing SSH
inter-packet delays as the medium, based on the original de-
sign [20] and the high volume of keystrokes in interactive
network applications.
4.1.1 Dataset
The covert and legitimate samples that we use for our ex-
periments are from two datasets: (1) HTTP traces we col-
lected on a medium-size campus network and (2) the dataset
obtained from the University of North Carolina at Chapel
Hill (UNC). In total, we have 12GB of uncompressed tcp-
dump packet header traces (HTTP protocol) that we col-
lected and 79GB of tcpdump packet header traces (all pro-
tocols) from the UNC dataset. In our experiments, we use
several subsets of the two datasets, including:
• HTTP training set: 10,000,000 HTTP packets
• LEGIT-HTTP: 200,000 HTTP packets
• TRCTC: 200,000 HTTP packets
• SSH training set: 10,000,000 SSH packets
• LEGIT-SSH: 200,000 SSH packets
• JitterBug: 200,000 SSH packets
In our experiments, we test a number of covert samples,
which are generated from these subsets and from the en-
coding methods for IPCTC, TRCTC, and JitterBug. For
TRCTC, we generate the covert samples from a set of 200,000
legitimate HTTP inter-packet delays. For JitterBug, we gen-
erate the covert samples from a set of 200,000 legitimate
SSH inter-packet delays. A test machine replays the set of
200,000 SSH inter-packet delays and adds JitterBug delays.
It should be noted that our version of JitterBug is imple-
mented in software. A monitoring machine on the campus
backbone then collects a trace of the JitterBug traﬃc, which
adds network delays after the addition of JitterBug delays.
The monitoring machine is 4 hops away from the test ma-
chine, so the added network delays are small, which repre-
sents the scenario illustrated in Figure 2, where a defensive
perimeter monitors outgoing traﬃc.
The large training sets of legitimate traﬃc are useful for
some of the detection tests. The Kolmogorov-Smirnov test