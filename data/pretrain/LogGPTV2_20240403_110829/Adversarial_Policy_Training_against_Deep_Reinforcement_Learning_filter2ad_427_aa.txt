title:Adversarial Policy Training against Deep Reinforcement Learning
author:Xian Wu and
Wenbo Guo and
Hua Wei and
Xinyu Xing
Adversarial Policy Training against 
Deep Reinforcement Learning
Xian Wu, Wenbo Guo, Hua Wei, and Xinyu Xing, 
The Pennsylvania State University
https://www.usenix.org/conference/usenixsecurity21/presentation/wu-xian
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Adversarial Policy Training against Deep
Reinforcement Learning
Xian Wu1∗, Wenbo Guo1∗, Hua Wei1∗, Xinyu Xing1
1The Pennsylvania State University
{xkw5132, wzg13, hzw77, xxing}@ist.psu.edu
Abstract
Reinforcement learning is a set of goal-oriented learning al-
gorithms, through which an agent could learn to behave in
an environment, by performing certain actions and observing
the reward which it gets from those actions. Integrated with
deep neural networks, it becomes deep reinforcement learn-
ing, a new paradigm of learning methods. Recently, deep
reinforcement learning demonstrates great potential in many
applications such as playing video games, mastering GO com-
petition, and even performing autonomous pilot. However,
coming together with these great successes is adversarial at-
tacks, in which an adversary could force a well-trained agent
to behave abnormally by tampering the input to the agent’s
policy network or training an adversarial agent to exploit the
weakness of the victim.
In this work, we show existing adversarial attacks against
reinforcement learning either work in an impractical setting
or perform less effectively when being launched in a two-
agent competitive game. Motivated by this, we propose a new
method to train adversarial agents. Technically speaking, our
approach extends the Proximal Policy Optimization (PPO) al-
gorithm and then utilizes an explainable AI technique to guide
an attacker to train an adversarial agent. In comparison with
the adversarial agent trained by the state-of-the-art technique,
we show that our adversarial agent exhibits a much stronger
capability in exploiting the weakness of victim agents. Be-
sides, we demonstrate that our adversarial attack introduces
less variation in the training process and exhibits less sensi-
tivity to the selection of initial states.
1 Introduction
With the recent breakthroughs of deep neural networks (DNN)
in problems like computer vision, machine translation, and
time series prediction, we have witnessed a great advance
in the area of reinforcement learning (RL). By integrating
deep neural networks into reinforcement learning algorithms,
∗Equal Contribution.
the machine learning community designs various deep rein-
forcement learning algorithms [29, 43, 53] and demonstrates
their great success in a variety of applications, ranging from
defeating world champions of Go [45] to mastering a wide
variety of Atari games [30].
Different from conventional deep learning, deep reinforce-
ment learning (DRL) refers to goal-oriented algorithms,
through which one could train an agent to learn how to attain
a complex objective or, in other words, maximize the reward
it can collect over many steps (actions). Like a dog incen-
tivized by petting and intimidation, reinforcement learning
algorithms penalize the agent when it takes the wrong action
and reward when the agent takes the right ones.
In light of the promising results in many reinforcement
learning tasks, researchers recently devoted their energies
to investigating the security risk of reinforcement learning
algorithms. For example, early research has proposed various
methods to manipulate the environment that an agent interacts
with (e.g., [4, 18, 21]). Their rationale behind such a kind of
attack is as follows. In a reinforcement learning task, an agent
usually takes as input the observation of the environment. By
manipulating the environment, an attacker could inﬂuence the
agent observation as well as its decision (action), and thus
mislead the agent to behave abnormally (e.g., subtly changing
some pixel values of the sky in the Super Mario game, or
injecting noise into the background canvas of the Pong game).
In many recent research works, attacks through environ-
ment manipulation have demonstrated great success in failing
a well-trained agent to complete a certain task (e.g., [18,19]).
However, such attacks are not practical in the real world. For
example, in the application of online video games, the input
of a pre-trained master agent is the snapshot of the current
game scenes. From the attackers’ perspective, it is difﬁcult
for them to hack into the game server, obtain the permission
of manipulating the environment, inﬂuence arbitrary pixels in
that input image, and thus launch an adversarial attack as they
expect. As a result, recent research proposes a new method to
attack a well-trained agent [10].
Different from attacks through environment manipulation,
USENIX Association
30th USENIX Security Symposium    1883
the new attack is designed speciﬁcally for the two-agent com-
petitive game – where two participant agents compete with
each other – and the goal of this attack is to fail one well-
trained agent in the game by manipulating the behaviors of
the other. In comparison with the environment manipulation
methods, the new attack against RL is more practical because,
to trigger the weakness of the victim agent, this attack does
not assume full control over the environment nor that over
the observation of the victim agent. Rather, it assumes only
the free access of the adversarial agent (i.e., the agent that the
attacker trains to compete with his opponent’s agent).
In [10], researchers have already shown that the method of
attacking through an adversarial agent could be used for an
alternative, practical approach to attack a well-trained agent
in reinforcement learning tasks. However, as we will demon-
strate in Section 6, this newly proposed attack usually ex-
hibits a relatively low success rate of failing the opponent (or
in other words victim) agent.1 This is because the attack is
a simple application of the state-of-the-art Proximal Policy
Optimization (PPO) algorithm [43] and, by design, the PPO
algorithm does not train an agent for exploiting the weakness
of the opponent agent.
Inspired by this discovery, we propose a new technique to
train an adversarial agent and thus exploit the weakness of
the opponent (victim) agent. First, we arm the adversarial
agent with the ability to observe the attention of the victim
agent while it plays with our adversarial agent. By using this
attention, the adversarial agent can easily ﬁgure out at which
time step the opponent agent pays more attention to the ad-
versary. Second, under the guidance of the victim’s attention,
the adversary subtly varies its actions. With this practice, as
we will show and elaborate in Section 4 and 5, the adversarial
agent could trick a well-trained opponent agent into taking
sub-optimal actions and thus inﬂuence the corresponding re-
ward that the opponent is supposed to receive.
Technically speaking, to develop the attack method men-
tioned above, we ﬁrst approximate the policy network as well
as the state-transition model of the opponent agent. Using
the approximated network and model, we can determine the
attention of the opponent agent by using an explainable AI
technique. Besides, we can predict the action of the opponent
agent when our adversarial agent takes a speciﬁc action.
With the predicted action in hand, our attack method then
extends the PPO algorithm by introducing a weighted term
into its objective function. As we will specify in Section 5,
the newly introduced term measures the action deviation of
the opponent agent with and without the inﬂuence of our ad-
versarial agent. The weight is the output of the explainable AI
technique, which indicates by how much the opponent agent
pays its attention to the adversarial agent. By maximizing
the weighted deviation together with the advantage function
in the objective function of PPO, we can train an adversarial
1Note that the paper uses “victim agent” and “opponent agent” inter-
changeably.
agent to take the action that could inﬂuence the action of the
opponent agent the most.
In this paper, we do not claim that our proposed technique
is the ﬁrst method for attacking reinforcement learning. How-
ever, we argue that this is the ﬁrst work that can effectively
exploit the weakness of victim agents without the manipula-
tion of the environment. Using MuJoCo [50] and roboschool
Pong [33] games, we show that our method has a stronger
capability of attacking a victim agent than the state-of-the-art
method [10] (an average of 60% vs. 50% winning rate for
MuJoCo game and 100% vs. 90% for the Pong game). In
addition, we demonstrate that, in comparison with the state-
of-the-art method of training an adversarial policy [10], our
proposed method could construct an adversarial agent with a
50% winning rate in fewer training cycles (11 million vs. 20
million iterations for MuJoCo game, and 1.0 million vs. 1.3
million iterations for Pong game). Last but not least, we also
show that using our proposed method to train an adversarial
agent, it usually introduces fewer variations in the training
process. We argue this is a very beneﬁcial characteristic
because this could make our algorithm less sensitive to the
selection of initial states. We released the game environment,
victim agents, source code, and our adversarial agents. 2
In summary, the paper makes the following contributions.
• We design a new practical attack mechanism that trains
an adversarial agent to exploit the weakness of the oppo-
nent in an effective and efﬁcient fashion.
• We demonstrate that an explainable AI technique can
be used to facilitate the search of the adversarial policy
network and thus the construction of the corresponding
adversarial agents.
• We evaluate our proposed attack by using representative
simulated robotics games – MuJoCo and roboschool
Pong – and compare our evaluation results with that
obtained from the state-of-the-art attack mechanism [10].
The rest of this paper is organized as follows. Section 2
describes the problem scope and assumption of this research.
Section 3 describes the background of deep reinforcement
learning. Section 4 and 5 speciﬁes how we design our attack
mechanism to train adversarial agents. Section 6 summa-
rizes the evaluation results of our proposed attack mechanism.
Section 7 provides the discussion of related work, followed
by the discussion of some related issues and future work in
Section 8. Finally, we conclude the work in Section 9.
2 Problem Statement and Assumption
Problem statement. Reinforcement learning refers to a set
of algorithms that address the sequential decision-making
2https://github.com/psuwuxian/rl_attack
1884    30th USENIX Security Symposium
USENIX Association
playing the game, the game developer could collect the game
episodes and retrain the master agent accordingly. However,
he cannot pull out the master agent and carry out retraining
immediately (or in other words right after each round of its
play). On the one hand, this is due to the fact that, training a
game agent with an RL algorithm generally requires a long
period of episode accumulation to receive a high winning rate
(e.g., the task of training the OpenAI’s hide-and-seek game
agent accumulates hundreds of millions of episodes [35]). On
the other hand, this is because, even if the game developer
retrains the master agent based on a large amount of game
episodes that he collects, he still needs to ﬁgure out a way to
preserve the generalizability of its master agent. As we will
demonstrate in Section 6, after retraining the master agent
using the episodes the master agent gathers when interacting
with the adversarial agent, the master agent could capture the
capability of defeating the adversary. However, it loses its
ability to defeat ordinary game agents.
It should also be noted that this work is very different
from many existing works, which assume an attacker has the
privilege to manipulate the environment freely or, in other
words, change the pixels in the snapshot that the victim agent
observes (e.g., [18, 40]). We believe the removal of this as-
sumption is crucial and could make an adversarial attack more
practical. To illustrate this argument, we again take for ex-
ample the aforementioned online games. In these examples,
the game environment refers to the game scenes created by
the game engine and the agents in the game. The activities of
directly manipulating the environment (game scenes) mean
that an adversary breaks into the game server or engine, alters
the game code related to the game scenes, and thus inﬂuences
the environment that the agents interacts with. Technically,
this inevitably introduces the efforts of the successful iden-
tiﬁcation and exploitation of a software vulnerability on the
game server. In practice, having such a capability typically
implies tens of thousands of hours of effort from professional
hackers, and cannot always guarantee the return of their ef-
forts because of the defense mechanisms enabled in computer
systems. With the removal of the assumption commonly made
in previous works, we make the adversarial attack more cost-
efﬁcient because, instead of putting efforts on breaking into
game server without the guarantee of success, an attacker only
needs to train an adversarial policy to control his own agent
and thus inﬂuences its opponent.
As is illustrated in Figure 1b, similar to the single-agent
game driven by reinforcement learning, in the setting of a two-
agent game, both of the agents take as input the observation
of the same environment, and then output the actions through
their own policy networks. In this work, when designing
methods to train an adversarial agent, we do not assume that
an attacker has access to the opponent agent’s policy network
nor its state transition model. Rather, we assume that the
attacker knows the observation of the opponent agent as well
as the action that the opponent takes. We believe this assump-
(a) Single agent game.
(b) Two-agent game.
Figure 1: The illustration of reinforcement learning tasks.
problem in complex scenarios. As is depicted in Figure 1, a
game is formalized as an RL learning task, in which an agent
observes and interacts with the game environment through
a series of actions. In this process of interaction, the agent
collects the reward for each of the actions it takes. Using the
reward as a feedback signal, the agent could be aware of how
well it performs at each time step.
The goal of RL is to learn an optimal policy, which guides
the agent to take actions more effective and thus to maximize
the amount of the reward it could gather from the environment.
In the setting of deep reinforcement learning, as is shown in
Figure 1, the policy learned is typically a deep neural network,
which takes as the input the observation of the environment
(i.e., the current snapshot of the game) and outputs the ac-
tions that the agent would take (i.e., left/right and up/down
movements, etc.). In Section 3, we will describe more details
about how to model a reinforcement learning problem and
thus resolve an optimal policy for the agent involved.
As is demonstrated in Figure 1a, the game is formalized as
a reinforcement learning problem in which the environment
involves only a single agent. However, in many reinforcement
learning tasks, an environment could contain two agents com-
peting with each other while interacting with the environment
(see Figure 1b). Recently, such two-agent competitive games
driven by reinforcement learning have received great attention
and reinforcement learning algorithms have demonstrated a
great potential [32, 45]. In this work, we, therefore, focus
our problem in the two-agent competitive environment, de-
veloping practical methods to train an adversarial policy for
one agent to beat the other and win corresponding two-agent
games. To be more speciﬁc, in this work, we ﬁx one agent
and train the other with the goal of having the trained agent
build up the ability to exploit the weakness of that ﬁxed other.
Assumption. It should be noted that, in our problem, we do
not assume the victim agent adapts its policy based on its
opponent immediately. With this assumption, we simulate
a real-world scenario, where a game developer deploys an
online game with an ofﬂine-trained master agent controlling