# Adversarial Policy Training against Deep Reinforcement Learning

**Authors:** Xian Wu, Wenbo Guo, Hua Wei, and Xinyu Xing  
**Affiliation:** The Pennsylvania State University  
**Conference:** 30th USENIX Security Symposium  
**Date:** August 11â€“13, 2021  
**Proceedings ISBN:** 978-1-939133-24-3  
**Open Access Sponsor:** USENIX  
**Link:** [https://www.usenix.org/conference/usenixsecurity21/presentation/wu-xian](https://www.usenix.org/conference/usenixsecurity21/presentation/wu-xian)

## Abstract
Reinforcement learning (RL) is a set of goal-oriented algorithms that enable an agent to learn optimal behavior in an environment by performing actions and observing the resulting rewards. When integrated with deep neural networks, it becomes deep reinforcement learning (DRL), a powerful paradigm for various applications, including video games, Go competitions, and autonomous piloting. However, DRL is also vulnerable to adversarial attacks, where an adversary can manipulate the input or train an adversarial agent to exploit the victim's weaknesses.

In this work, we demonstrate that existing adversarial attacks on RL are either impractical or less effective in two-agent competitive games. To address this, we propose a new method to train adversarial agents. Our approach extends the Proximal Policy Optimization (PPO) algorithm and uses explainable AI techniques to guide the training of the adversarial agent. Compared to state-of-the-art methods, our adversarial agent shows a stronger capability to exploit the victim's weaknesses, introduces less variation in the training process, and is less sensitive to the selection of initial states.

## 1. Introduction
Recent breakthroughs in deep neural networks (DNNs) have led to significant advances in reinforcement learning (RL). By integrating DNNs into RL algorithms, researchers have developed deep reinforcement learning (DRL) algorithms that have achieved remarkable success in various applications, from defeating world champions in Go to mastering a wide range of Atari games.

DRL algorithms are goal-oriented, enabling an agent to learn how to achieve complex objectives by maximizing the cumulative reward over multiple steps. Unlike traditional deep learning, DRL focuses on sequential decision-making, where the agent learns to take actions that maximize its long-term reward.

Despite the promising results, recent research has highlighted the security risks associated with DRL. Early studies proposed methods to manipulate the environment, such as subtly changing pixel values in game scenes, to mislead the agent. However, these attacks are often impractical in real-world scenarios, as they require full control over the environment, which is difficult to achieve, especially in online games.

To address this, recent research has focused on training adversarial agents to compete with and exploit the weaknesses of well-trained agents. In [10], researchers demonstrated that this approach can be more practical than environment manipulation. However, as we will show, this method often has a low success rate due to the limitations of the PPO algorithm, which does not inherently train agents to exploit their opponents' weaknesses.

Inspired by this, we propose a new technique to train adversarial agents. Our method leverages explainable AI to observe the attention of the victim agent and subtly vary the adversarial agent's actions to trick the victim into taking suboptimal actions. We extend the PPO algorithm by introducing a weighted term that measures the action deviation of the victim agent under the influence of the adversarial agent. This allows us to train an adversarial agent that effectively exploits the victim's weaknesses.

## 2. Problem Statement and Assumptions
### Problem Statement
Reinforcement learning (RL) addresses sequential decision-making problems in complex environments. In a typical RL setup, an agent interacts with the environment through a series of actions and receives rewards based on its performance. The goal is to learn an optimal policy that maximizes the cumulative reward.

In the context of DRL, the policy is often represented by a deep neural network that takes the current observation of the environment as input and outputs the next action. In this work, we focus on two-agent competitive environments, where one agent is fixed, and the other is trained to exploit the fixed agent's weaknesses.

### Assumptions
- **No Immediate Adaptation:** We assume that the victim agent does not adapt its policy based on the adversarial agent's actions. This simulates a real-world scenario where a game developer deploys an online game with a pre-trained master agent.
- **Limited Access:** We do not assume that the attacker has access to the victim agent's policy network or state transition model. Instead, the attacker only knows the victim's observations and actions.

## 3. Background
### Reinforcement Learning
Reinforcement learning (RL) is a type of machine learning where an agent learns to interact with an environment to maximize a cumulative reward. The environment is typically modeled as a Markov Decision Process (MDP) with states, actions, and rewards. The agent's goal is to learn a policy that maps states to actions to maximize the expected cumulative reward.

### Deep Reinforcement Learning
Deep reinforcement learning (DRL) combines RL with deep neural networks to handle high-dimensional and complex environments. The policy is represented by a deep neural network that takes the current observation as input and outputs the next action. Popular DRL algorithms include Deep Q-Networks (DQN), Trust Region Policy Optimization (TRPO), and Proximal Policy Optimization (PPO).

### Adversarial Attacks on DRL
Adversarial attacks on DRL aim to manipulate the agent's behavior by altering the input or training an adversarial agent. Early attacks focused on manipulating the environment, but recent research has shifted towards training adversarial agents to exploit the victim's weaknesses.

## 4. Methodology
### Adversarial Agent Training
Our method extends the Proximal Policy Optimization (PPO) algorithm to train adversarial agents. The key idea is to use explainable AI techniques to observe the victim agent's attention and subtly vary the adversarial agent's actions to trick the victim into taking suboptimal actions.

### Attention-Based Guidance
We approximate the victim agent's policy network and state-transition model. Using explainable AI, we determine the victim's attention at each time step. This allows the adversarial agent to identify when the victim is most vulnerable and adjust its actions accordingly.

### Weighted Deviation Term
We introduce a weighted deviation term into the PPO objective function. This term measures the difference in the victim's actions with and without the influence of the adversarial agent. The weight is determined by the output of the explainable AI, indicating the victim's level of attention to the adversarial agent. By maximizing the weighted deviation along with the advantage function, we train the adversarial agent to take actions that significantly influence the victim's behavior.

## 5. Experimental Evaluation
### Experimental Setup
We evaluate our method using the MuJoCo and roboschool Pong environments. We compare our adversarial agent's performance with the state-of-the-art method [10] in terms of winning rate and training efficiency.

### Results
- **Winning Rate:** Our adversarial agent achieves a higher winning rate compared to the state-of-the-art method (60% vs. 50% in MuJoCo and 100% vs. 90% in Pong).
- **Training Efficiency:** Our method requires fewer training iterations to achieve a 50% winning rate (11 million vs. 20 million in MuJoCo and 1.0 million vs. 1.3 million in Pong).
- **Stability:** Our adversarial agent introduces fewer variations in the training process, making it less sensitive to the selection of initial states.

## 6. Related Work
### Adversarial Attacks on DRL
Previous works on adversarial attacks on DRL have primarily focused on manipulating the environment or training adversarial agents. Our work builds on these approaches by introducing a more effective and efficient method for training adversarial agents.

### Explainable AI in DRL
Explainable AI techniques have been used in DRL to understand and interpret the behavior of agents. Our work demonstrates how these techniques can be leveraged to train adversarial agents more effectively.

## 7. Discussion
### Practical Implications
Our method provides a practical and efficient way to train adversarial agents without requiring full control over the environment. This makes it more applicable to real-world scenarios, such as online games, where direct manipulation of the environment is often infeasible.

### Future Work
Future work could explore the application of our method to more complex and diverse environments. Additionally, further research is needed to develop countermeasures to protect DRL agents from adversarial attacks.

## 8. Conclusion
In this paper, we propose a new method for training adversarial agents in deep reinforcement learning. Our approach extends the PPO algorithm and uses explainable AI to guide the training of the adversarial agent. We demonstrate that our method is more effective and efficient than existing approaches, achieving higher winning rates and better training stability. Our work contributes to the understanding and mitigation of adversarial attacks in DRL.

**Acknowledgments:** We thank the reviewers for their valuable feedback. This work was supported by [funding sources].

**References:**
[Include all relevant references here]

**Code and Data:**
The code, game environments, and trained agents are available at [https://github.com/psuwuxian/rl_attack](https://github.com/psuwuxian/rl_attack).