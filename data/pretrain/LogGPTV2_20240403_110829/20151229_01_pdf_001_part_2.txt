t
4
t
3
t
2
t
1
At what point could the filesystem become inconsistent?
44
t
4
At
t
t
3
1
t
2
t
1
How? I lied while explaining the situation. Alternate Truth.
45
Neglected to highlight ZFS is Copy-On-Write (read: knowingly committed
perjury in front of a live audience)
How? I lied while explaining the situation. Alternate
46
ZFS is Copy-On-Write
What what's not been deleted and on disk is immutable.
(read: I nearly committed perjury in front of a live audience by knowingly
withholding vital information)
ZFS is Copy-On-Write
47
Disk Block with
foo_table Tuple
t
1
At what point did the filesystem become inconsistent?
48
t
2
t
1
At what point did the filesystem become inconsistent?
49
t
3
t
2
t
1
At what point did the filesystem become inconsistent?
50
t
4
t
3
t
2
t
1
At what point could the filesystem become inconsistent?
51
t
4
NEVER
t
3
t
2
t
1
TIL about ZFS: Transactions and Disk Pages
52
•Transaction groups are flushed to disk ever N seconds (defaults to 5s)
•A transaction group (txg) in ZFS is called a "checkpoint"
•User Data can be modified as its written to disk
•All data is checksummed
•Compression should be enabled by default
ZFS Tip: ALWAYS enable compression
53
$ zfs get compression
NAME PROPERTY VALUE SOURCE
rpool compression off default
rpool/root compression off default
$ sudo zfs set compression=lz4 rpool
$ zfs get compression
NAME PROPERTY VALUE SOURCE
rpool compression lz4 local
rpool/root compression lz4 inherited from rpool
•Across ~7PB of PostgreSQL and mixed workloads and applications:
compression ratio of ~2.8:1 was the average.
•Have seen >100:1 compression on some databases
(cough this data probably didn't belong in a database cough)
•Have seen as low as 1.01:1
ZFS Tip: ALWAYS enable compression
54
$ zfs get compression
NAME PROPERTY VALUE SOURCE
rpool compression off default
rpool/root compression off default
$ sudo zfs set compression=lz4 rpool
$ zfs get compression
NAME PROPERTY VALUE SOURCE
rpool compression lz4 local
rpool/root compression lz4 inherited from rpool
I have yet to see compression slow down benchmarking results or real world
workloads. My experience is with:
•spinning rust (7.2K RPM, 10K RPM, and 15KRPM)
•fibre channel connected SANs
•SSDs
•NVME
ZFS Tip: ALWAYS enable compression
55
$ zfs get compressratio
NAME PROPERTY VALUE SOURCE
rpool compressratio 1.64x -
rpool/db compressratio 2.58x -
rpool/db/pgdb1-10 compressratio 2.61x -
rpool/root compressratio 1.62x -
•Use lz4 by default everywhere.
•Use gzip-9 only for archive servers
•Never mix-and-match compression where you can't suffer the
consequences of lowest-common-denominator performance
•Anxious to see ZStandard support (I'm looking at you Allan Jude)
ZFS Perk: Data Locality
56
•Data written at the same time is stored near each other because it's frequently
part of the same record
•Data can now pre-fault into kernel cache (ZFS ARC) by virtue of the temporal
adjacency of the related pwrite(2) calls
•Write locality + compression=lz4 + pg_repack == PostgreSQL Dream Team
ZFS Perk: Data Locality
57
•Data written at the same time is stored near each other because it's frequently
part of the same record
•Data can now pre-fault into kernel cache (ZFS ARC) by virtue of the temporal
adjacency of the related pwrite(2) calls
•Write locality + compression=lz4 + pg_repack == PostgreSQL Dream Team
If you don't know what pg_repack is, figure out how to move into a database
environment that supports pg_repack and use it regularly.
https://reorg.github.io/pg_repack/ && https://github.com/reorg/pg_repack/
Extreme ZFS Warning: Purge all memory of dedup
58
•This is not just my recommendation, it's also from the community and author
of the feature.
•These are not the droids you are looking for
•Do not pass Go
•Do not collect $200
•Go straight to system unavailability jail
•The feature works, but you run the risk of bricking your ZFS server.
Ask after if you are curious, but here's a teaser:
What do you do if the dedup hash tables don't fit in RAM?
Bitrot is a Studied Phenomena
Bitrot is a Studied Phenomena
Bitrot is a Studied Phenomena
Bitrot is a Studied Phenomena
TIL: Bitrot is here
63
•TL;DR: 4.2% -> 34% of SSDs have one UBER per year
TIL: Bitrot Roulette
64
(1-(1-uberRate)^(numDisks)) = Probability of UBER/server/year
(1-(1-0.042)^(20)) = 58%
(1-(1-0.34)^(20)) = 99.975%
Highest quality SSD drives on the market
Lowest quality commercially viable SSD drives on the market
Causes of bitrot are Internal and External
65
External Factors for UBER on SSDs:
Temperature
•
Bus Power Consumption
•
Data Written by the System Software
•
Workload changes due to SSD failure
•
In a Datacenter no-one can hear your bits scream...
...except maybe they can.
Take Care of your bits
68
Answer their cry for help.
Take Care of your bits
69
Similar studies and research exist for:
•Fibre Channel
•SAS
•SATA
•Tape
•SANs
•Cloud Object Stores
So what about PostgreSQL?
70
"...I told you all of that, so I can tell you this..."
ZFS Terminology: VDEV
71
VDEV | vē-dēv
noun
a virtual device
•Physical drive redundancy is handled at the VDEV level
•Zero or more physical disks arranged like a RAID set:
•mirror
•stripe
•raidz
•raidz2
•raidz3
ZFS Terminology: zpool
72
zpool | zē-poo͞ l
noun
an abstraction of physical storage made up of a set of VDEVs
Loose a VDEV, loose the zpool.
ZFS Terminology: ZPL
73
ZPL | zē-pē-el
noun
ZFS POSIX Layer
•Layer that handles the impedance mismatch between POSIX filesystem
semantics and the ZFS "object database."
ZFS Terminology: ZIL
74
ZIL | zil
noun
ZFS Intent Log
•The ZFS analog of PostgreSQL's WAL
•If you use a ZIL:
•Use disks that have low-latency writes
•Mirror your ZIL
•If you loose your ZIL, whatever data had not made it to the main data disks
will be lost. The PostgreSQL equivalent of: rm -rf pg_xlog/
ZFS Terminology: ARC
75
ARC | ärk
noun
Adaptive Replacement Cache
•ZFS's page cache
•ARC will grow or shrink to match use up all of the available memory
TIP: Limit ARC's max size to a percentage of physical memory
minus the shared_buffer cache for PostgreSQL minus the
kernel's memory overhead.
ZFS Terminology: Datasets
76
dataset | dædə ˌsɛt
noun
A filesystem or volume ("zvol")
•A ZFS filesystem dataset uses the underlying zpool
•A dataset belongs to one and only one zpool
•Misc tunables, including compression and quotas are set on the dataset level
ZFS Terminology: The Missing Bits
77
ZFS Attribute Processor
ZAP
Data Management Unit
DMU
Dataset and Snapshot Layer
DSL
Storage Pool Allocator
SPA
ZFS Volume
ZVOL
ZFS I/O
ZIO
RAID with variable-size stripes
RAIDZ
Level 2 Adaptive Replacement Cache
L2ARC
unit of user data, think RAID stripe size
record
Storage Management
78
$ sudo zfs list
NAME USED AVAIL REFER MOUNTPOINT
rpool 818M 56.8G 96K none
rpool/root 817M 56.8G 817M /
$ ls -lA -d /db
ls: cannot access '/db': No such file or directory
$ sudo zfs create rpool/db -o mountpoint=/db
$ sudo zfs list
NAME USED AVAIL REFER MOUNTPOINT
rpool 818M 56.8G 96K none
rpool/db 96K 56.8G 96K /db
rpool/root 817M 56.8G 817M /
$ ls -lA /db
total 9
drwxr-xr-x 2 root root 2 Mar 2 18:06 ./
drwxr-xr-x 22 root root 24 Mar 2 18:06 ../
Storage Management
79
•Running out of disk space is bad, m'kay?
•Block file systems reserve ~8% of the disk space above 100%
•At ~92% capacity the performance of block allocators change from
"performance optimized" to "space optimized" (read: performance "drops").
Storage Management
80
•Running out of disk space is bad, m'kay?
•Block file systems reserve ~8% of the disk space above 100%
•At ~92% capacity the performance of block allocators change from
"performance optimized" to "space optimized" (read: performance "drops").
ZFS doesn't have an artificial pool of free
space: you have to manage that yourself.
Storage Management
81
$ sudo zpool list -H -o size
59.6G
$ sudo zpool list
The pool should never consume more than 80% of the available space
Storage Management
82
$ sudo zfs set quota=48G rpool/db
$ sudo zfs get quota rpool/db
NAME PROPERTY VALUE SOURCE
rpool/db quota 48G local
$ sudo zfs list
NAME USED AVAIL REFER MOUNTPOINT
rpool 818M 56.8G 96K none
rpool/db 96K 48.0G 96K /db
rpool/root 817M 56.8G 817M /
Dataset Tuning Tips
83
•Disable atime
•Enable compression
•Tune the recordsize
•Consider tweaking the primarycache
ZFS Dataset Tuning
84
# zfs get atime,compression,primarycache,recordsize rpool/db
NAME PROPERTY VALUE SOURCE
rpool/db atime on inherited from rpool
rpool/db compression lz4 inherited from rpool
rpool/db primarycache all default
rpool/db recordsize 128K default
# zfs set atime=off rpool/db
# zfs set compression=lz4 rpool/db
# zfs set recordsize=16K rpool/db
# zfs set primarycache=metadata rpool/db
# zfs get atime,compression,primarycache,recordsize rpool/db
NAME PROPERTY VALUE SOURCE
rpool/db atime off local
rpool/db compression lz4 local