title:Revisiting Route Caching: The World Should Be Flat
author:Changhoon Kim and
Matthew Caesar and
Alexandre Gerber and
Jennifer Rexford
Revisiting Route Caching:
The World Should Be Flat
Changhoon Kim1, Matthew Caesar2, Alexandre Gerber3, and Jennifer Rexford1
1 Princeton University,
2 UIUC,
3 AT&T Labs–Research
Abstract. Internet routers’ forwarding tables (FIBs), which must be stored in
expensive fast memory for high-speed packet forwarding, are growing quickly in
size due to increased multihoming, ﬁner-grained trafﬁc engineering, and
deployment of IPv6 and VPNs. To address this problem, several Internet archi-
tectures have been proposed to reduce FIB size by returning to the earlier ap-
proach of route caching: storing only the working set of popular routes in the
FIB. This paper revisits route caching. We build upon previous work by study-
ing ﬂat, uni-class (/24) preﬁx caching, with modern trafﬁc traces from more than
60 routers in a tier-1 ISP. We ﬁrst characterize routers’ working sets and then
evaluate route-caching performance under different cache replacement strategies
and cache sizes. Surprisingly, despite the large number of deaggregated /24 sub-
nets, caching uni-class preﬁxes can effectively curb the increase of FIB sizes.
Moreover, uni-class preﬁxes substantially simplify a cache design by eliminating
longest-preﬁx matching, enabling FIB design with slower memory technologies.
Finally, by comparing our results with previous work, we show that the distri-
bution of trafﬁc across preﬁxes is becoming increasingly skewed, making route
caching more appealing.
1 Introduction
Packet forwarding on core Internet routers is an extremely challenging process. Upon
receiving an IP packet, routers have just a few nanoseconds to buffer the packet, select
the longest-matching preﬁx covering the packet’s destination, and forward the packet to
the corresponding outbound interface. To allow this process to happen quickly, routers
often make use of special-purpose high-speed memory such as TCAM and SRAM.
Unfortunately, the need for multi-homing and ﬁne-grained trafﬁc engineering, the de-
sire to mitigate preﬁx hijacking by advertising more-speciﬁc routes, and the continuing
rapid growth of the Internet have rapidly increased the number of routes that an Internet
router has to maintain. The accelerating deployments of protocols with large address
spaces such as IPv6 and VPNs, combined with the rapidly increasing link speeds, stand
to worsen this problem even further.
Unfortunately, the special-purpose memory used for high-speed packet forwarding
is orders of magnitude more expensive, more power-hungry, bigger in physical size,
and generates substantially more heat than conventional DRAM. This is because man-
ufacturing fast memory requires more transistors per bit [1]. Due to these factors, it
S.B. Moon et al. (Eds.): PAM 2009, LNCS 5448, pp. 3–12, 2009.
c(cid:2) Springer-Verlag Berlin Heidelberg 2009
4
C. Kim et al.
will be increasingly challenging to manufacture a line card with large, fast memory at
a reasonable price and power-consumption budget. To save expenses, service providers
may therefore be forced to provision with little headroom for growth, making their net-
works unable to handle sudden spikes in table size. Providers will also be forced to
upgrade their equipment more often, a substantial problem given maintenance of op-
erational networks can be much more expensive than hardware costs. To keep up with
these demands, future routers must reduce the number of routes stored in the FIB.
In this paper we revisit route caching, where the FIB stores only the frequently-used
routes and other routes are retrieved from a larger but slower memory (e.g., DRAM) on
a miss. This approach is motivated by the fact that Internet trafﬁc exhibits high degrees
of temporal locality (as packets are grouped into ﬂows, which are often transmitted
in bursts) and spatial locality (as many hosts access a small number of popular des-
tinations). In fact, route caching was once widely used in Internet routers. In the late
1980s and early 1990s, most routers were built with a route caching capability, such as
fast switching [2, 3]. Unfortunately, these designs were not able to keep up with fast-
increasing packet forwarding rates, due to the large cost of cache misses, such as lower
throughput and high packet loss ratio. While this limitation of route caching is yet to be
addressed, revisiting it with modern Internet trafﬁc seems worthwhile because recent
research results indicate that route caching might be both possible and necessary:
Route caching may be possible: New Internet routing architectures (such as ViAg-
gre [4], or SEATTLE [5]) can improve feasibility of route caching by reducing the cost
of a cache miss. For example, when a cache miss happens, a router can immediately for-
ward a packet via a backup default route, without forcing the packet to be queued (while
waiting for the cache to be updated from the slow memory) or to traverse the “slow path”
(through the router CPU). The backup default route indirectly leads the packet to an al-
ternate router that always maintains a correct route to the destination in its FIB (more
explanation in Section 2.1). This “fall-back” mechanism has substantial performance
beneﬁts, because packets can always be sent immediately after the cache lookup com-
pletes. Since a route cache can be much smaller than a full FIB and takes substantially
less time for a lookup, ensuring line-rate packet forwarding becomes easier.
Route caching may be necessary: New protocols with larger (e.g., IPv6) or even ﬂat
address spaces (e.g., ROFL [6], LISP [7], AIP [8]) have been proposed to facilitate the
Internet’s growth and conﬁguration. However, deploying these protocols would signiﬁ-
cantly increase FIB sizes beyond the capacities of much currently-deployed equipment,
and is predicted to require several million FIB entries within several years if current
growth trends continue. When FIBs ﬁll up, conventional routers crash or begin behav-
ing incorrectly [9], forcing operators to deploy new line cards or even routers with a
larger memory. Alternatively, in such a case, the use of caching would only increase the
volume of the trafﬁc handled via the “fall-back” mechanism, instead of incurring hard
crashes, improving availability and extending times between router upgrades.
We start by describing our trafﬁc traces collected from over 60 routers in a tier-1 ISP,
and justify caching ﬂat uni-class (i.e., /24) preﬁxes (Section 2). We then characterize
the working sets of popular preﬁxes (Section 3) and evaluate route-caching performance
under our uni-class model (Section 4). Despite the signiﬁcantly larger number of uni-
class preﬁxes as compared to CIDR, the cache size needed for a reasonably small miss
Revisiting Route Caching: The World Should Be Flat
5
rate is comparable to the size of FIBs in conventional Internet routers. Moreover, a uni-
form preﬁx length allows use of hashing for faster lookups, greatly reducing the number
of memory accesses per lookup (e.g., looking up an item in a chained hash table with
N bins and N items requires 1.58 memory accesses on average, whereas a lookup in
a trie-based approach, such as [10], takes much more than that). Therefore, caching
uni-class preﬁxes may be implementable with a slower and cheaper memory (e.g., RL-
DRAM [1], or even regular DRAM). Finally, by comparing our results with previous
work, we show that Internet trafﬁc today is more amenable to caching (Section 5).
2 Measurement Methodology and Route-Cache Design
Our data sets were collected from a tier-1 ISP’s backbone in the U.S. First, we collected
unsampled packet-level traces over a one-week period at an access router servicing
regional DSL subscribers in the ISP. Second, we collected ﬂow-level trafﬁc records
from more than 60 edge routers in different geographical and topological regions. Since
the volume of the trafﬁc transited by those routers was extremely large, we utilized a
sampling technique (Sampled NetFlow [11]) to reduce the overhead of collecting trafﬁc.
These two sets of traces are complementary to each other; the ﬂow-level traces allow
us to compare behavior across different routers, while the packet-level traces allow us
to study ﬁner-grained behavior at a single access router. Additionally, using unsampled
packet-level traces allows us to validate the accuracy of our methodology for using
sampled ﬂow-level traces to study route caching.
DSL traces: We collected IP packet headers that originated from roughly 20, 000 re-
gional DSL subscribers in the USA using our network trafﬁc monitoring platform. Our
monitor is attached to the access router aggregating trafﬁc from subscribers, and was
conﬁgured to monitor inbound trafﬁc sent from DSL subscribers to the rest of the In-
ternet. Note that we deliberately captured inbound trafﬁc because destination addresses
accessed by the inbound trafﬁc are much more diverse than those by outbound trafﬁc
and are thus very challenging for our route-caching study. We ran our monitor for 8
consecutive days from Feb 29 through Mar 7, 2008, and captured roughly 40 billion
packets, corresponding to an average of ∼ 65, 000 packets per second.
NetFlow traces: To study differences in workload across routers, we collected NetFlow
records from inbound trafﬁc to the ISP via two representative POPs (Points of Presence)
containing over 60 routers, respectively located on the east and west coasts of the USA.
To avoid overloading the router CPU, we were forced to conﬁgure NetFlow to perform
deterministic sampling with a sampling ratio of 1/500 [11]. The active and inactive
time-out values were set to 60 and 15 seconds respectively. We ran NetFlow for 15 hours
on January 23 - 24, 2008, collecting the information of ∼ 330 billion packets (roughly
100K pkts/sec per edge router on average). Some of our analysis (e.g., estimating cache
miss rate) requires packet-level arrival information. To construct packet records from
ﬂow records, we post-processed the trace to distribute all counted packets in a ﬂow
evenly over the measured duration of the ﬂow. To check for sampling inaccuracies,
we generated sampled DSL traces by applying the NetFlow sampling algorithm to our
unsampled DSL traces. There was no statistically signiﬁcant difference between the
results acquired with sampled and unsampled traces.
6
C. Kim et al.
2.1 Route Caching Model
To evaluate the performance of route caching, we deﬁne a simple and generic caching
architecture. In this architecture, a packet forwarding unit (e.g., a line card, or a for-
warding engine) incorporates hierarchical, two-level memory. The ﬁrst level is a route
cache, which is embodied by a small, but very fast memory containing only a sub-
set of the entire routes. The second level is a full routing table, which is a slow, but
large memory storing all routes. Once a packet arrives, the forwarding unit ﬁrst looks
up the packet’s destination address in the route cache. If it ﬁnds a match, the packet
is immediately forwarded based on the lookup result. If not, the forwarding unit for-
wards the packet using a backup route. The backup route indicates an alternate router,
which can be either statically conﬁgured or dynamically chosen from a small set of al-
ternate routers via a very simple computation (e.g., hashing) on the packet header. Note
that this computation can be done in parallel with the cache lookup without increasing
packet forwarding latency. The alternate router ﬁnally forwards the packet to the des-
tination via a direct route residing in its FIB; to ensure this, administrators can run a
well-provisioned router in each POP that always keeps the entire set of routes in its FIB
or employ a routing protocol, such as ViAggre [4] or SEATTLE [5], where each router
maintains a small amount of additional routing information in its route cache. Apart
from this packet forwarding procedure, the forwarding unit separately updates its route
cache by looking up the full routing table. If needed, an existing entry in the cache is
evicted based on a cache replacement strategy.
Conventional routers store information about paths to CIDR (variable-length) pre-
ﬁxes in their routing and forwarding tables. Hence, at ﬁrst glance, it appears that the
cache should also store information in the same structure. However, caching CIDR pre-
ﬁxes presents a serious technical challenge arising from the need to perform longest-
preﬁx matching: if multiple CIDR preﬁxes contain a destination address, the most-
speciﬁc one – longest-matching preﬁx (LMP) – must be chosen to forward the packet.
Unfortunately, in a route-caching system, only the contents of the cache are referred to
when making a packet forwarding decision, and thus the cache-hiding problem arises:
consider an empty cache, and suppose the full routing table contains two preﬁxes:
10.1.0.0/16 associated with output interface O1, and 10.1.5.0/24 associated with O2.
Suppose a new packet destined to 10.1.2.3 arrives. The router will ﬁnd the LMP for
this destination, which is 10.1.0.0/16, and will install the route [10.1.0.0/16 → O1]
into the cache. Now, suppose the next packet the router receives is destined to 10.1.5.6.
Then, the router will discover [10.1.0.0/16 →O1] in the cache, and send the packet to
O1. This, however, is incorrect because the LMP for 10.1.5.6 is 10.1.5.0/24, and hence
the packet must have been sent to O2. Unfortunately, proposed solutions to this prob-
lem involve either a complicated data structure with on-the-ﬂy computation to eliminate
inter-dependency among preﬁxes [12], or grouping all preﬁxes containing a destination
address as an atomic unit of caching operation (insertion, deletion, and update). The
latter approach leads to cache thrashing because the size of an atomic preﬁx group in
today’s FIB can be larger than 25, 000. Worse yet, a cache storing CIDR preﬁxes still
has to perform longest-preﬁx matching on every lookup.
To avoid these difﬁculties, we explore an alternative model which we refer to as
“ﬂat”, uni-class caching. This model is identical to the technique that Feldmeier used in
Revisiting Route Caching: The World Should Be Flat
7
1
0.8
0.6
0.4
0.2
s
t
k
p
f
o
n
o
i
t
c