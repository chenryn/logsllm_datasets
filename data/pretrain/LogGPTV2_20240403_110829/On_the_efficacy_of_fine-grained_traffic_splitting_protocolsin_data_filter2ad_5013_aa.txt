title:On the efficacy of fine-grained traffic splitting protocolsin data
center networks
author:Advait Abhay Dixit and
Pawan Prakash and
Ramana Rao Kompella
Purdue University 
Purdue University 
Purdue e-Pubs 
Purdue e-Pubs 
Department of Computer Science Technical 
Reports 
Department of Computer Science 
2011 
On the Efficacy of Fine-Grained Traffic Splitting Protocols in Data 
On the Efficacy of Fine-Grained Traffic Splitting Protocols in Data 
Center Networks 
Center Networks 
Advait Dixit 
Purdue University, PI:EMAIL 
Pawan Prakash 
Purdue University, PI:EMAIL 
Ramana Rao Kompella 
Purdue University, PI:EMAIL 
Report Number: 
11-011 
Dixit, Advait; Prakash, Pawan; and Kompella, Ramana Rao, "On the Efficacy of Fine-Grained Traffic Splitting 
Protocols in Data Center Networks" (2011). Department of Computer Science Technical Reports. Paper 
1742. 
https://docs.lib.purdue.edu/cstech/1742 
This document has been made available through Purdue e-Pubs, a service of the Purdue University Libraries. 
Please contact PI:EMAIL for additional information. 
On the Efﬁcacy of Fine-Grained Trafﬁc Splitting
Protocols in Data Center Networks
Advait Dixit, Pawan Prakash, Ramana Rao Kompella
Department of Computer Science
Purdue University
Abstract—Multi-rooted tree topologies are commonly used to
construct high-bandwidth data center network fabrics. In these
networks, switches typically rely on equal-cost multipath (ECMP)
routing techniques to split trafﬁc across multiple paths, where
each ﬂow is routed through one of the available paths, but packets
within a ﬂow traverse the same end-to-end path. Unfortunately,
since ECMP splits trafﬁc based on ﬂow-granularity, it can cause
load imbalance across multiple paths resulting in poor utiliza-
tion of network resources. More ﬁne-grained trafﬁc splitting
techniques are typically not preferred because they can cause
packet reordering that can, according to conventional wisdom,
lead to severe TCP throughput degradation. In this paper, we
revisit this fact in the context of regular data center topologies
such as fat-tree architectures. We argue that packet-level trafﬁc
splitting, where packets belong to a given ﬂow are sprayed
through all available paths, would lead to a better load-balanced
network, which in turn leads to signiﬁcantly more balanced
queues and much higher throughput compared to ECMP. We
conduct extensive simulations to corroborate this claim.
I. INTRODUCTION
In the recent years, data centers have become the corner-
stones of modern computing infrastructure. Many distributed
processing applications (e.g., search, social collaboration,
high-performance computing) are routinely run in large-scale
data centers, that may contain upwards of 100,000 servers.
Because of the inherently distributed nature of computation,
the network fabric that connects these different servers be-
comes quite critical in determining the performance of these
applications. To scale the data center network to provide the
level of connectivity required, most data center network fabrics
are organized in the form of a multi-rooted tree topology.
Further, they use multipathing between servers so that load
is balanced among several alternate paths and the chances of
congestion are reduced.
One popular multipathing mechanism used in data centers
today is equal-cost multipath (ECMP), where different ﬂows
(as identiﬁed by the TCP 4-tuple) between a given pair of
servers are routed via different paths. The switches basically
compute all the available shortest paths using regular routing
protocols (such as OSPF) and select a path by hashing the
ﬂow key to index into one of the possible next hops. By
spreading ﬂows across different paths, ECMP ensures that the
load in the network is balanced. However, because all ﬂows
are not identical either in their size (number of bytes) or in
their duration, this simple strategy is not sufﬁcient to prevent
the occurrence of hot-spots in the network. Particularly, two
long-lived ﬂows may hash to the same path for a long time,
and hence may observe reduced performance, while there may
be spare capacity through alternate paths in the network that
could have been utilized.
Many recent works have identiﬁed the load imbalance that
can arise under ECMP and suggested different approaches
for making the load more balanced across the different avail-
able paths. In particular, Hedera [3] relies on a centralized
ﬂow scheduler that periodically obtains information about
‘elephant’ ﬂows in a dynamic fashion and schedules these
elephants so that they do not conﬂict with each other as
much as possible. Another recent approach, called multipath-
TCP (MP-TCP) focuses on solving this problem at the TCP-
level by breaking a ﬂow into several sub-ﬂows that can be
striped across several ECMP paths and the receiver performs
the re-assembly of the stream from these sub-ﬂows. By
maintaining a separate congestion window across the sub-
ﬂows, MP-TCP splits trafﬁc in proportion to the levels of
congestion (i.e., a lightly loaded path carries more trafﬁc than
a heavily congested ﬂow). Both these schemes, however, have
their disadvantages. Hedera requires centralized knowledge
making it harder to scale, while MP-TCP requires a complete
replacement of TCP at all end-hosts, which may not be feasible
in certain environments (e.g., public cloud platforms).
We consider an alternate approach that essentially does not
require replacing TCP with a new protocol, or centralized
knowledge for scheduling heavy hitters. Yet, this approach is
quite simple to implement, and is in fact, already implemented
to some extent in many commodity switches (e.g., cisco [1]).
The approach is based on packet-level trafﬁc splitting (PLTS)
where each packets that belong to a single ﬂow are forwarded
across multiple paths that are available between the source
and the destination. This idea of spreading packets within a
ﬂow across different paths is not novel by itself. However,
this approach is not typically preferred because conventional
wisdom suggests that TCP will confuse reordered packets
with lost packets, that will in turn result in the reduction
of the congestion window signiﬁcantly, thereby losing out on
throughput.
In this paper, we revisit this myth surrounding TCP’s poor
interaction with reordering. Speciﬁcally, we argue that if the
switches were to spray packets within a given ﬂow across
all available paths, and further, if they apply this mechanism
to each and every ﬂow in the network, because of the nearly
perfect load balancing in the network, the impact on TCP may
potentially be quite minimal. By revisiting this conventional
wisdom, our goal in this paper is to suggest that, packet
reordering may not be that harmful when all ﬂows in the net-
work uniformly spray packets across all available paths. The
simplicity of implementation of this approach is a sufﬁciently
strong motivation to, at
the least, study these approaches
in more detail, rather than clinging on to the conventional
wisdom that TCP interacts poorly with reordering.
Speciﬁcally,
in this paper, we set out
to show using
simulations that for several trafﬁc patterns, PLTS achieves
much higher overall network throughput compared to ECMP.
Further, because potentially most links are similarly loaded,
the amount of reordering is quite small. Some ﬂows do suffer
slightly, but that is true in ECMP as well due to the possibility
of a ﬂow getting routed through a congested path. In that
sense, our scheme is not worse than ECMP, but has the effect
of making the data center network evenly loaded and more
predictable.
Thus,
the main contributions of the paper include the
following.
• We revisit
the conventional wisdom that packet-level
trafﬁc splitting is inherently harmful to TCP. Speciﬁcally,
our observation is grounded on the fact that many popular
data center network designs such as the fat-tree, or more
generally, multi-rooted tree topologies are symmetric in
their architecture, and by spraying packets across dif-
ferent paths leads to a more balanced and predictable
network architecture, that interacts well with TCP.
• We propose different variants of the per-packet
load
balancing algorithms in this paper that exploit the basic
idea, but vary depending on the amount of state each
of the solution maintains in the routers. We also propose
different algorithms based on a hypervisor-based solution
that can offer some assistance to mitigate any negative
effects of reordering caused by the striping packets within
a ﬂow across different packets.
• Using simulations, we compare these various strategies
in terms of TCP throughput achieved. We also compare
them against ECMP on various network parameters like
latency, queue sizes, link utilization etc. Speciﬁcally, we
show that per-packet load balancing outperforms ECMP
in all the dimensions—1.5× better throughput and 6×
better latency at the 99th percentile.
The rest of the paper is organized as follows. We ﬁrst start
with background and motivation in Section II. We discuss
the various packet-level trafﬁc splitting (PLTS) schemes in
Section III. In Section IV, we discuss our simulation setup
and evaluation results.
II. BACKGROUND AND MOTIVATION
Many data center applications today require signiﬁcant
network bandwidth. For example, MapReduce [11], a com-
monly used data center application relies on shufﬂing massive
amounts of data from various nodes and hence the job com-
pletion time is directly dependent on the available bandwidth
between these servers. Large-scale online services are another
class of applications that are hosted in massive data centers.
For instance, Google, Microsoft, Facebook rely on data centers
to host their critical services such as search, recommendation
and social collaboration applications and other Web services.
These services are typically organized in a three-tier fashion
with load balancers, front-end web servers and other backend
servers such as SQL servers. Thus, a single client query
incurs potentially transferring signiﬁcant amounts of data
between these various tiers. Network performance thus directly
impacts the end-to-end query latency, which translates to user
satisfaction and ultimately money for the service provider.
While network performance is clearly important, data center
operators also want the ﬂexibility of placing any server at
any data center node. This gives data center operators more
choices in optimizing various resources such as power, storage
and computation. Such ﬂexibility, however, means that the data
center network design cannot pre-assume a given trafﬁc matrix
and optimize the routing and forwarding to that given matrix.
In fact, recent papers characterizing data center trafﬁc have
found that there exists a tremendous amount of spatial and
temporal variation in trafﬁc volumes [15], [19], [7]. Thus, the
recent trend towards network fabric designs that can achieve
full bi-section bandwidth such as the fat-tree architecture [2].
While in theory the fat-tree architecture provides full bi-
section bandwidth, achieving this is hard in practice because
it is dependent on the underlying routing scheme. Traditional
single-path forwarding mechanisms are inadequate since the
full bi-section bandwidth guarantee assumes that all paths that
exist between servers can be fully utilized. To mitigate this
problem, a simple multipathing algorithm based on equal-
cost multipath (ECMP) has been used as the de facto routing
algorithm. ECMP essentially routes different ﬂows through
different paths, so that all the available paths are utilized thus
achieving the full bisection bandwidth.
Unfortunately, ECMP only provides coarse-grained load
balancing and can lead to many scenarios where paths are not
fully balanced. This occurs partly because of the presence of
long-lived ﬂows that carry a signiﬁcant amount of trafﬁc. For
example, in [19] study, the authors ﬁnd that 90% of the trafﬁc
volume is actually contained in 10% of the ﬂows (the heavy-
hitters). If two long-lived ﬂows hash to the same path, then
this can cause signiﬁcant dip in the performance. For example,
we can see the existence of such collisions in Figure 1 where
two long ﬂows, A and C, collide along the link A1 − C1,
while the remaining paths are less loaded. If A and C choose
link-disjoint paths, then both would potentially achieve better
performance. (In this example, the destinations of all the ﬂows
are outside of the pod consisting of the aggregate switches A1
and A2, and ToR switches T 1 and T 2, which required going
through the core routers.)
The goal of this paper is to mitigate this problem with the
help of a more ﬁne-grained trafﬁc splitting approach, one that
involves load-balancing at the granularity of a single packet.
Speciﬁcally, we start with the basic ECMP trafﬁc splitting
that hashes the ﬂow key of a packet to identify the next hop
for forwarding. Since all packets for a given ﬂow share the
same ﬂow key, this mechanism ensures all packets for that
Fig. 1. Motivating example comparing ECMP with packet-level trafﬁc splitting techniques. In ECMP, the four ﬂows A − D are routed by hashing and it
could happen that ﬂows A and C traverse the same link while C the link between switches A2 and C2 is idle. In PLTS, we can observe that all the four
links between aggregation switches A1, A2 and C1 − C4 are equally loaded with fractions of all the ﬂows across the different paths.
ﬂow traverse the same end-to-end path. Our approach in this
paper is to study trafﬁc splitting mechanisms that eliminate
this restriction and splits trafﬁc equally among all the paths
even within the ﬂow. In the ideal case, we have perfect load
balancing as shown in Figure 1 (right side). For the same
example as before, we can see now that a quarter of A’s and
C’s trafﬁc is carried on all the four core routers, in addition
to a quarter of B’ and D’s trafﬁc. Within the pod, however,
links T 1− A1 and T 1− A2 are in one equivalence class, with
each carrying half of A’s and B’s trafﬁc, while the other links
are in another equivalence class carrying C’s and D’s trafﬁc.
PLTS however suffers from the obvious downside of packet