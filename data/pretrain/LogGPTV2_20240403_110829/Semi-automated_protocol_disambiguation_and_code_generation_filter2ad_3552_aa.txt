title:Semi-automated protocol disambiguation and code generation
author:Jane Yen and
Tam&apos;as L&apos;evai and
Qinyuan Ye and
Xiang Ren and
Ramesh Govindan and
Barath Raghavan
Semi-Automated Protocol Disambiguation and Code Generation
University of Southern California
Jane Yen
PI:EMAIL
Xiang Ren
PI:EMAIL
Tamás Lévai
Budapest University of
Technology and Economics
PI:EMAIL
Ramesh Govindan
Qinyuan Ye
University of Southern California
PI:EMAIL
Barath Raghavan
University of Southern California
University of Southern California
University of Southern California
PI:EMAIL
PI:EMAIL
ABSTRACT
For decades, Internet protocols have been specified using
natural language. Given the ambiguity inherent in such text,
it is not surprising that protocol implementations have long
exhibited bugs. In this paper, we apply natural language
processing (NLP) to effect semi-automated generation of pro-
tocol implementations from specification text. Our system,
sage, can uncover ambiguous or under-specified sentences
in specifications; once these are clarified by the spec author,
sage can generate protocol code automatically.
Using sage, we discover 5 instances of ambiguity and
6 instances of under-specification in the ICMP RFC; after
clarification, sage is able to automatically generate code
that interoperates perfectly with Linux implementations. We
show that sage generalizes to BFD, IGMP, and NTP. We also
find that sage supports many of the conceptual components
found in key protocols, suggesting that, with some additional
machinery, sage may be able to generalize to TCP and BGP.
1 INTRODUCTION
Four decades of Internet protocols have been specified in
English and used to create, in Clark’s words, rough consensus
and running code [15]. In that time we have come to depend
far more on network protocols than most imagined. To this
day, engineers implement a protocol by reading and inter-
preting specifications as described in Request For Comments
documents (RFCs). Their challenge is to navigate easy-to-
misinterpret colloquial language while writing not only a
bug-free implementation but also one that interoperates with
code written by another person at a different time and place.
Software engineers find it difficult to interpret specifica-
tions in large part because natural language can be ambigu-
ous. Unfortunately, such ambiguity is not rare; the errata
alone for RFCs over the years highlight numerous ambigui-
ties and the problems they have caused [16, 32, 66, 75]. Am-
biguity has resulted in buggy implementations, security vul-
nerabilities, and expensive and time-consuming software en-
gineering processes, like interoperability bake-offs [31, 68].
To address this, one line of research has sought formal
specification of programs and protocols (§8), which would
enable verifying spec correctness and, potentially, enable
automated code generation [12]. However, formal specs are
cumbersome and thus have not been adopted in practice; to
date, protocols are specified in natural language.1
In this paper, we apply NLP to semi-automated generation
of protocol implementations from RFCs. Our main challenge
is to understand the semantics of a specification. This task,
semantic parsing, has advanced in recent years with parsing
tools such as CCG [4]. Such tools describe natural language
with a lexicon and yield a semantic interpretation for each
sentence. Because they are trained on generic prose, they
cannot be expected to work out of the box for idiomatic
network protocol specifications, which contain embedded
syntactic cues (e.g., structured descriptions of fields), incom-
plete sentences, and implicit context from neighboring text
or other protocols. More importantly, the richness of natural
language will likely always lead to ambiguity, so we do not
expect fully-automated NLP-based systems (§2).
Contributions. In this paper, we describe sage, a semi-
automated approach to protocol generation from natural
language. sage is iterative: it reads RFC text and marks sen-
tences (a) for which it cannot generate unique semantic inter-
pretations or (b) which fail on the protocol’s unit tests (sage
uses test-driven development). The former sentences are
likely semantically ambiguous whereas the latter represent
under-specified behaviors. In either case, the spec author
can then revise the sentences (perhaps several times), until
the resulting RFC can cleanly be turned into code.
We make the following contributions. First, we tailor (§3)
a pre-existing semantic parser to extend its vocabulary and
lexicon to cover networking-specific language in RFCs. Our
extensions also extract structural context from RFCs that
may be crucial to understand a sentence’s semantics and to
1In recent years, attempts have been made to formalize other aspects of
network operation, such as network configuration [6, 36] and control plane
behavior [54], with varying degrees of success.
Description
Error Type
Freqency
Name
♦ Packet Format
♦ Field Descriptions
♦ Constraints
♦ Protocol Behaviors
System Architecture
+ State Management
Comm. Patterns
Packet anatomy (i.e., field structure)
Packet header field descriptions
Constraints on field values
Reactions to external/internal events
Protocol implementation components
Session information and/or status
Message sequences (e.g., handshakes)
Table 1: Protocol specification components. sage sup-
ports those marked with ♦ (fully) and + (partially).
generate code. Even so, such a parser can fail to extract a sen-
tence’s semantics or emit multiple semantic interpretations;
ideally, the parser would generate exactly one interpretation.
Second, we note that many challenges stem from RFC
idioms, such as incomplete sentences to describe protocol
header fields and the use of verbs like is and prepositions
like of in specific ways. Using these conventions, we devise
automated techniques to eliminate many semantic interpre-
tations. If, after such disambiguation (§4), a sentence has
multiple interpretations, it is likely to be fundamentally am-
biguous, so sage requires a human to rewrite the sentence.
Third, we present a code generator that converts semantic
representations to executable code (§5). The parser repre-
sents a sentence’s semantics using logical predicates, and
the code generator emits executable code using contextual
information that it has gleaned from the RFC’s document
structure, as well as static context pre-defined in sage about
lower-layer protocols and the underlying OS. Unit testing on
generated code can uncover incompleteness in specifications.
sage discovered (§6) 5 sentences in the ICMP RFC [61]
(of which 3 are unique, the others being variants) that had
multiple semantic interpretations even after disambiguation.
It also discovered 6 sentences that failed unit tests (all vari-
ants of a single sentence). After fixing these, sage was able
to automatically generate code for ICMP that interoperated
perfectly with ping and traceroute. In contrast, graduate
students asked to implement ICMP in a networking course
made numerous errors (§2). Moreover, sage was able to parse
significant sections of BFD [34], IGMP [18], and NTP [53],
with few additions to the lexicon. It generated packets for the
timeout procedure containing both NTP and UDP headers.
It also parsed state management text for BFD to determine
system actions and update state variables for reception of
control packets. Finally, sage’s disambiguation is often very
effective, reducing, in some cases, 56 logical forms to 1.
Toward greater generality. sage is a significant first step
toward automated processing of natural-language protocol
specifications, but much work remains. Protocol specifica-
tions contain a variety of components, and Table 1 indicates
which ones sage supports well (in green), which it supports
57%
IP header related
57%
ICMP header related
29%
Network byte order and host byte order conversion
43%
Incorrect ICMP payload content
29%
Incorrect echo reply packet length
Incorrect checksum or dropped by kernel
36%
Table 2: Error types of failed cases and their frequency in
14 faulty student ICMP implementations.
partially (in olive), and which it does not support. Some pro-
tocols contain complex state machine descriptions (e.g., TCP)
or describe how to process and update state (e.g., BGP); sage
can parse state management in a simpler protocol like BFD.
Other protocols describe software architectures (e.g., OSPF,
RTP) and communication patterns (e.g., BGP); sage must be
extended to parse these descriptions. In §7, we break down
the prevalence of protocol components by RFC to contextu-
alize our contributions, and identify future sage extensions.
Such extensions will put sage within reach of parsing large
parts of TCP and BGP RFCs.
2 BACKGROUND AND OVERVIEW
Spec ambiguities can lead to bugs and non-interoperability,
which we quantify next via implementations of ICMP [61]
by students in a graduate networking course.
2.1 Analysis of ICMP Implementations
ICMP, defined in RFC 792 in 1981 and used by core tools
like ping and traceroute, is a simple protocol whose spec-
ification should be easy to interpret. To test this assertion,
we examined implementations of ICMP by 39 students in a
graduate networking class. Given the ICMP RFC and related
RFCs, students built ICMP message handling for a router.
To test whether students implemented echo reply cor-
rectly, we used the Linux ping tool to send an echo message
to their router (we tested their code using Mininet [43]).
Across the 39 implementations, the Linux implementation
correctly parsed the echo reply only for 24 of them (61.5%).
One failed to compile and the remaining 14 exhibited 6 cat-
egories (not mutually exclusive) of implementation errors
(Table 2): mistakes in IP or ICMP header operations; byte or-
der conversion errors; incorrectly-generated ICMP payload
in the echo reply message; incorrect length for the payload;
and wrongly-computed ICMP checksum. Each error category
occurred in at least 4 of the 14 erroneous implementations.
To understand the incorrect checksum better, consider the
specification of the ICMP checksum in this sentence: The
checksum is the 16-bit one’s complement of the one’s comple-
ment sum of the ICMP message starting with the ICMP Type.
This sentence does not specify where the checksum should
end, resulting in a potential ambiguity for the echo reply; a
developer could checksum some or all of the header, or both
2
Index
ICMP checksum range interpretations
1
2
3
4
5
6
7
Size of a specific type of ICMP header.
Size of a partial ICMP header.
Size of the ICMP header and payload.
Size of the IP header.
Size of the ICMP header and payload, and any IP options.
Incremental update of the checksum field using whichever
checksum range the sender packet chose.
Magic constants (e.g., 2 or 8 or 36).
Table 3: Students’ ICMP checksum range interpretations.
the header and the payload. In fact, students came up with
seven different interpretations (Table 3) including checksum-
ming only the IP header, checksumming the ICMP header
together with a few fixed extra bytes, and so on.
2.2 Approach and Overview
Dealing with Ambiguity. These results indicate that even
a relatively simple RFC results in a large variety of interpre-
tations and often result in non-interoperable implementa-
tions. RFC authors and the IETF community rely on man-
ual methods to avoid or eliminate non-interoperabilities:
careful review of standards drafts by participants, devel-
opment of reference implementations, and interoperability
bake-offs [31, 68] at which vendors and developers test their
implementations against each other to discover issues that
often arise from incomplete or ambiguous specs.
Why not reference implementations? Reference imple-
mentations are useful but insufficient. For a reference proto-
col document to become a standard, a reference implemen-
tation is indeed often written, and this has been the case
for many years. A reference implementation is often written
by participants in the standardization process, who may or
may not realize that there exist subtle ambiguities in the text.
Meanwhile, vendors write code directly to the specification
(often to ensure that the resulting code has no intellectual
property encumbrances), sometimes many years after the
spec was standardized. This results in subtle incompatibili-
ties in implementations of widely deployed protocols [57].
Why are there ambiguities in RFCs? RFCs are ambigu-
ous because (a) natural language is expressive and admits
multiple ways to express a single idea; (b) standards authors
are technical domain experts who may not always recognize
the nuances of natural language; and (c) context matters in
textual descriptions, and RFCs may omit context.
Structure in RFCs. Unlike general English text, network
protocol specifications have exploitable structure. The net-
working community uses a restricted set of words and oper-
ations (i.e., domain-specific terminology) to describe network
behaviors. Moreover, RFCs conform to a uniform style [21]
3
(especially recent RFCs) and all standards-track RFCs are
carefully edited for clarity and style adherence [65].
Our approach. We leverage recent advances in the NLP area
of semantic parsing. Natural language can have lexical [35,
64] (e.g., the word bat can have many meanings), structural
(e.g., the sentence Alice saw Bob with binoculars) and semantic
(e.g., in the sentence I saw her duck) ambiguity.
For the foreseeable future we do not expect NLP to be
able to parse RFCs without help. Thus, our semi-automated
approach uses NLP tools, along with unit tests, to help a
human-in-the-loop discover and correct ambiguities after
which the spec is amenable to automated code generation.
Figure 1 shows the three stages of sage. The parsing stage
uses a semantic parser [4] to generate intermediate repre-
sentations, called logical forms (LFs), of sentences. Because
parsing is not perfect, it can output multiple LFs for a sen-
tence. Each LF corresponds to one semantic interpretation
of the sentence, so multiple LFs represent ambiguity. The
disambiguation stage aims to automatically eliminate such
ambiguities. If, after this, ambiguities remain, sage asks a
human to resolve them. The code generator compiles LFs into
executable code, a process that may also uncover ambiguity.
3 SEMANTIC PARSING
Semantic parsing is the task of extracting meaning from
a document. Tools for semantic parsing formally specify
natural language grammars and extract parse trees from text.
More recently, deep-learning based approaches have proved
effective in semantic parsing [20, 40, 85] and certain types
of automatic code generation [46, 62, 83]. However, such
methods do not directly apply to our task. First, deep learning
typically requires training in a “black-box”. Since we aim
to identify ambiguity in specifications, we aim to interpret
intermediate steps in the parsing process and maintain all
valid parsings. Second, such methods require large-scale
annotated datasets; collecting high-quality data that maps
network protocol specifications to expert-annotated logical
forms (for supervised learning) is impractical.
For these reasons, we use the Combinatory Categorial
Grammar (CCG [4]) formalism that enables (a) coupling syn-
tax and semantics in the parsing process and (b) is well suited
to handling domain-specific terminology by defining a small
hand-crafted lexicon that encapsulates domain knowledge.
CCG has successfully parsed natural language explanations
into labeling rules in several contexts [71, 79].
CCG background. CCG takes as input a description of the