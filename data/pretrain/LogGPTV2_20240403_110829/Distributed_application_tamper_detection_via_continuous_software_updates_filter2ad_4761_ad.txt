all blocks
1.2.1
Analyze
block Ai
1.2.2
Is Ai an
asset
block?
3.1
Analyze A
to
determine
that they
are orphan
blocks
3.2
Prevent server from
replacing A with
new blocks
3.2.1
Report
A ⊆
active set
3.3
Prevent server from making
meaningful updates to A
3.4
Reverse engineer updates
3.3.1
Report an active set that has
RPCs/variables A uses
3.3.2
Ignore
updates to
A
3.4.1
Find ancestor
Nk−1 of new block
Nk
3.2.1.1
Analyze
blocks &
build call
graph
3.2.1.2
Report
valid
active set
containing
A
3.3.1.1
Analyze
blocks &
build call
graph
3.3.1.2
Find RPC-
s/variables
A uses
3.4.1.1
Compare
callgraphs
3.4.1.2
Compare
Nk against
all other
blocks
3.3.1.3
Report
valid
active set
containing
blocks with
RPCs/vari-
ables A
uses
3.4.3
Patch A
with new
encodings
3.4.2
Extract
new vari-
able/RPC
encodings
& call
signaures
from
N0, . . . , Nk
Figure 5: Attack tree [26]. OR-edges are dashed, AND-edges are solid.
by inserting bogus function calls, RPCs, and variable references.
To counter the attacks in node 3.2 and 3.3, we can again use
the opaque primitive, inserting calls to non-existing functions. If
the adversary reports an active set containing such a function, we
know that he is cheating.
4.3 Empirical Tests
We implemented three attacks and evaluated our system’s ability
detect them. In all cases, the same target C program was used, a
simple Tetris game with some functionality moved server-side.
An unsophisticated attacker could simply ignore block updates,
continuing to execute the tampered program (node 3.3.2 in Fig-
ure 5). We simulated this attack by turning off client updates. Since
RPCs are frequent in our test program, the server reliably detected
the malicious behavior shortly after the ﬁrst RPC_encode update.
A malicious client may attempt to build a snapshot of the en-
tire program, in order to analyze it off-line. This can help with
attack nodes 1.1.2, 3.2.1.1, and 3.3.1.1. To simulate this attack we
engineered a client that disassembles its blocks, looks for blocks
which are referenced but not yet held, and requests them from the
server. However, this malicious client quickly requested nonexis-
tent blocks to which our system had inserted references; blocks a
benign client would never ask for because they were protected by
opaque predicates. By adding such bogus calls in just 50% of the
blocks in our test program we identiﬁed the malicious client af-
ter they had successfully requested and received 16 of the 24 real
blocks. When we added a call to a bogus function into every block
the malicious client received only 6 of 24 real blocks (25%) before
the attack was detected.
In order to prevent the server from updating blocks a client can
report the entire contents of the block bag to the server as his active
set. This can be seen as a crude example of the attacks in nodes
3.2.1 and 3.3.1 of Figure 5. We implemented such a client and the
server was able to use the program call graph to reliably identify
the malicious behavior after the ﬁrst update cycle.
4.4 Diversity of Primitive Transformations
The most sophisticated attack in the attack tree is rooted at node
3.4. The idea is for the attacker to try to identify the history of a
new block Nk, i.e. all its ancestors N0, . . . , Nk−1, and to use that
information to help him reverse-engineer the transformations that
(a) n-Gram Similarities of Related and Unrelated Blocks
(b) Diversity of The Call Graph
y
t
i
r
a
l
i
m
i
s
1
0.95
0.9
0.85
0
5
10
15
20
25
# of transformations
Figure 6: Diversity of blocks and call graphs.
were used to generate Nk. This, in turn, could allow him to in-
telligently update blocks he has previously tampered with, without
having to re-analyze them from scratch.
Determining the ancestor Nk−1 of a newly updated block Nk
can be accomplished in one of two ways: by comparing the call
graphs before and after the update (3.4.1.1) or by comparing Nk
against all blocks in the block bag, looking for a similar block
(3.4.1.2). We claim that because our primitives generate high en-
tropy in both the call graph and the content of the blocks, recogniz-
ing ancestry should be very difﬁcult.
To demonstrate the call graph diversity generated by our system,
we repeatedly applied the merge and split obfuscations to func-
tions from the gzip SPEC benchmark. We compared call-graph
similarity between the obfuscated and unobfuscated program using
326
time (since our system is based on CIL which generates C code as
output), and dynamic load time on the client. Transformation of
blocks is usually on the order of a tenth of a second (even with our
most complex primitives). Compilation currently takes roughly a
second per function that needed to be changed and transfer across
the network takes roughly half of a second per function. The com-
pilation time dominates the cost in our current model, but this could
be remedied by compiling multiple ﬁles concurrently on separate
computation units or by using a system such as LLVM that does
not require a ﬁnal compilation stage.
6. FUTURE WORK
The latency of updates clients suffer in our system is acceptable
for some applications (say, medical records databases) but not for
those with real-time constraints, such as multiplayer online games.
We are investigating multiple ways to reduce the delays, both those
caused by network overhead and by code transformations.
Currently, we are working on supporting background generation
of working sets. The idea is simple and easily realized using our
diversity graph: (1) determine (through proﬁling or static analy-
sis) active sets that will be commonly occurring on the client and
generate block working sets compatible with these active sets in
the background; (2) during an update cycle, serve the client with
a pre-generated working set if one matches his current active set,
otherwise generate one on the ﬂy.
Previous work that has suggested diversiﬁcation-for-security has
typically ignored the correctness issues that follow with randomiz-
ing code, or have assumed the existence of extensive test suites that
could be run after diversiﬁcation. Our situation is more difﬁcult
than most since we generate code continuously at runtime, under
time constraints, meaning comprehensive testing after each trans-
formation is infeasible. However, we do keep track of the history
of each generated block through our diversity graph, and we hope
to use this information to integrate project unit tests in the system.
7. CONCLUSIONS
We have described a system for detecting tampering of clients
running on untrusted nodes in a distributed system. Our system
continuously updates client code, keeping it in a state of constant
ﬂux, giving the adversary a limited time-window for analyzing and
tampering with the code. We employ protocol-preserving code
transformations which provide diversity to slow down the adver-
sary’s analysis and tampering of the code, and transformations that
are not protocol-preserving to make it harder for him to tamper with
the code without modifying its expected behavior, thereby making
it easier for the trusted server to detect the tampering.
The security afforded by the system is a function of the frequency
of code updates and the complexity and variability generated by in-
dividual code transformations. This, in turn, is related to the com-
putational overhead a particular program can afford to suffer. To
manage overhead, our diversity scheduler is designed to allow de-
tailed control over which parts of the program to transform, which
transformations to apply, and uses a rollback technique to avoid
compounding too many transformations.
We have shown that our infrastructure itself causes a perfor-
mance overhead ranging from 4% to 23% and that, for several
easy-to-implement attacks, the server reliably and quickly detects
the malicious behavior.
the algorithm from Shang et al. [28]. The data series in Figure 6 (b)
shows the similarity scores after each iteration of the transforma-
tions. For comparison, the horizontal lines in the graph show simi-
larity scores of gzip to other unrelated SPEC programs; this result
demonstrates that the call graph of the modiﬁed program is often
less similar to its ancestor than to that of unrelated code.
Failing to use the call graph to identify Nk−1, the attacker may
attempt to scan all blocks in his bag and measure their similarity
to Nk. We defend against this attack by ensuring that we can gen-
erate signiﬁcantly diverse implementations of a block via obfusca-
tions. To demonstrate this block diversity, we measured n-gram
similarity [25] of obfuscated blocks to their ancestors (Nk to Nk−1
pairings) and compared these scores with the n-gram similarity of
unrelated code (Nk to Nj pairings). To simulate the process of a
successful decompilation, we ﬁrst converted block contents with
normalized source code, replacing literals and variable names with
place-holders and converting the program to high-level intermedi-
ate code. The gray bars in Figure 6 (a) show the distribution of
n-gram similarities of randomly-chosen unrelated blocks (Nk to
Nj pairings). The black bars show the distribution of n-gram sim-
ilarities of obfuscated blocks to their unobfuscated ancestors (Nk
to Nk−1 pairings). Obfuscations used were interpreter and ﬂatten.
The average n-gram similarity of unrelated blocks was .57; the av-
erage similarity of a block to its ancestor was .66. Though some
obfuscated functions show high similarity to their ancestors, many
other block-to-ancestor pairings show low similarity, and would be
clearly buried in the noise of all possible block-to-block pairings,
making them impractical to ﬁnd via n-gram analysis.
5. PERFORMANCE EVALUATION
With respect to performance, we evaluate the overhead of our
infrastructure (without any primitives applied), the overhead added
by individual primitives, and the delay the client experiences as the
result of an update. While it would seem interesting to measure
the “typical” or average overhead a program might suffer, this is
not possible, since it depends as much on the mix of primitives and
strategies the scheduler chooses as on the nature of the program
itself. For many applications it is more interesting, in fact, to mea-
sure worst case performance, for example the longest delay a user
might suffer as the result of an update. For all measurements we
therefore attempt to evaluate the worst case performance overhead
for a few of the SPEC benchmarks.
Client measurements were taken with our client running on a lap-
top with 4 GB of memory and an Intel 2.9 GHz Core i5 processor,
running Ubuntu Linux. Server measurements were taken with our
server running on an Amazon EC2 m1.small instance, providing
1.7 GB of memory and one EC2 compute unit.
Our infrastructure has multiple potential sources of overhead.
First of all, our client code client.c is a C program that origi-
nally might have been compiled as a single large ﬁle. Our system
instead breaks this into multiple ﬁles consisting of a single function,
each one compiled separately, and this will impede inter-procedural
optimization. Furthermore, an intelligent linker might place related
functions of client.c on the same physical page, but the client
in our system does not see the entire program at once, and cannot
make the same choices as easily. A further consequence of continu-
ous updates is that we add a level of indirection to all function calls.
To evaluate the overhead that our infrastructure adds, we measured
the change in runtime for bzip, gzip, mcf and crafty. We
found the cost as typically near a ﬁve to ten percent runtime in-
crease, though crafty was closer to a twenty percent increase.
The latency of an update is the sum of network delay, the time
to transform blocks using our primitives, compilation and linking
327
Intermediate language and tools for analysis and
transformation of C programs. In Compiler Construction,
pages 213–228, Grenoble, 2002.
[21] T. Proebsting. Optimizing ANSI C with superoperators. In
POPL’96. ACM Press, Jan. 1996.
[22] P. R.C. Sandia report: Advanced metering infrastructure
security considerations, Nov. 2007.
[23] R. Sailer, X. Zhang, T. Jaeger, and L. van Doorn. Design and
implementation of a tcg-based integrity measurement
architecture. In SSYM’04, pages 16–16, 2004.
[24] R. Scandariato, Y. Ofek, P. Falcarin, and M. Baldi.
Application-oriented trust in distributed computing.
Availability, Reliability and Security, International
Conference on, 0:434–439, 2008.
[25] S. Schleimer, D. Wilkerson, and A. Aiken. Winnowing:
Local algorithms for document ﬁngerprinting. In
Proceedings of the 2003 SIGMOD Conference, 2003.
[26] B. Schneier. Dr. Dobb’s Journal, Dec. 1999.
[27] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. van Doorn, and
P. Khosla. Pioneer: verifying code integrity and enforcing
untampered code execution on legacy systems. SIGOPS
Oper. Syst. Rev., 39(5):1–16, 2005.
[28] S. Shang, N. Zheng, J. Xu, M. Xu, and H. Zhang. Detecting
malware variants via function-call graph similarity. In
MALWARE, pages 113 –120, Oct. 2010.
[29] G. Stoyle, M. Hicks, G. Bierman, P. Sewell, and I. Neamtiu.
Mutatis mutandis: safe and predictable dynamic software
updating. In POPL ’05, pages 183–194. ACM, 2005.
[30] C. Wang, J. Hill, J. Knight, and J. Davidson. Protection of
software-based survivability mechanisms. Dsn, 00:0193,
2001.
[31] J. Xu, Z. Kalbarczyk, and R. K. Iyer. Transparent runtime
randomization for security. IEEE Symposium on Reliable
Distributed Systems, 0:260, 2003.
8. REFERENCES
[1] B. Anckaert, M. Jakubowski, R. Venkatesan, and K. D.
Bosschere. Run-time randomization to mitigate tampering.
In Proceedings of the second International Workshop on
Security, number 4752, pages 153–168, Berlin, Oct. 2007.
[2] B. Anckaert, B. D. Sutter, and K. D. Bosschere. Covert
communication through executables. In Program
Acceleration through Application and Architecture driven
Code Transformations, pages 83–85, Sept. 2004.
[3] R. Anderson and M. Kuhn. Low cost attacks on tamper
resistant devices. In IWSP: International Workshop on
Security Protocols, 1997.
[4] D. Aucsmith. Tamper resistant software: An implementation.
In R. J. Anderson, editor, Information Hiding, pages
317–333. Springer-Verlag, May 1996.
[5] E. G. Barrantes, D. H. Ackley, S. Forrest, and D. Stefanovi´c.
Randomized instruction set emulation. ACM Transactions on
Information and System Security (TISSEC), 8(1):3–40, Feb.
2005.
[6] M. Ceccato and P. Tonella. Codebender: Remote software
protection using orthogonal replacement. IEEE Software,
28(2):28–34, 2011.
[7] F. M. Cleveland. Cyber security issues for advanced
metering infrasttructure (AMI). In Power and Energy Society
General Meeting - Conversion and Delivery of Electrical
Energy in the 21st Century, pages 1–5. IEEE PES Power
Syst. Commun. Comm., July 2008.
[8] F. B. Cohen. Operating system protection through program
evolution. all.net/books/IP/evolve.html, 1992.
[9] C. Collberg and J. Nagra. Surreptitious Software:
Obfuscation, Watermarking, and Tamperprooﬁng for
Software Protection. Addison-Wesley, July 2009.
[10] C. Collberg, C. Thomborson, and D. Low. Manufacturing
cheap, resilient, and stealthy opaque constructs. In POPL’98,
Jan. 1998.
[11] L. D’Anna, B. Matt, A. Reisse, T. V. Vleck, S. Schwab, and
P. LeBlanc. Self-protecting mobile agents obfuscation report
— Final report. Technical Report 03-015, NAI Labs, June
2003.
[12] J.-F. Dhem, F. Koeune, P.-A. Leroux, P. Mestré, J.-J.
Quisquater, and J.-L. Willems. A practical implementation of
the timing attack. In CARDIS, pages 167–182, 1998.
[13] M. A. Ertl. Stack caching for interpreters. SIGPLAN Not.,
30(6):315–327, 1995.
[14] P. Falcarin, S. D. Carlo, A. Cabutto, N. Garazzino, and
D. Barberis. Exploting code mobility for dynamic binary
obfuscation. 2011.
[15] S. Forrest, A. Somayaji, and D. Ackley. Building diverse
computer systems. In HotOS-VI, pages 67–72, 1997.
[16] A. Haeberlen, P. Aditya, R. Rodrigues, and P. Druschel.
Accountable virtual machines, 2010.
[17] G. Hoglund and G. McGraw. Exploiting Online Games:
Cheating Massively Distributed Systems. Addison-Wesley,
2007.
[18] F. Hohl. Time limited blackbox security: Protecting mobile
agents from malicious hosts. In Mobile Agents and Security,
pages 92–113, 1998.
[19] M. Jakobsson and M. K. Reiter. Discouraging software
piracy using software aging. In DRM Workshop, pages 1–12,
2001.
[20] G. C. Necula, S. Mcpeak, S. P. Rahul, and W. Weimer. CIL:
328