ditional probability with the longest condition (i.e., the left-
most conditional probability in the chain) that is known in
T . Since T contains all unigrams, there is always an esti-
mation of P (Li+1|L1
i ). Pmax is then
deﬁned to be
i ), denoted by eP (Li+1|L1
Li+1∈I∪{&} eP (Li+1|L1
max
i ).
i ) and eP (Li+1|L1
If P (Li+1|L1
i ) are represented by nodes v
and v′ in T , respectively, then v′ is the Markov parent of
v in T , and any pair of corresponding nodes in the above
chain are Markov neighbors. For example, in Figure 1, v4 is
the Markov parent of v5; v5 and v1 are Markov neighbors.
Once Pmax is calculated, we can calculate the privacy pa-
rameter εv that is used for calculating the noisy counts of
v’s children as follow:
εv =
min(logPmax
¯ε
θ
c(v) , nmax − i)
,
where ¯ε is the remaining privacy budget (i.e., the total pri-
vacy budget ε minus the sum of privacy parameters con-
sumed by v and v’s ancestors). It can be observed that this
scheme ensures that the privacy budget used in a root-to-leaf
path is always ≤ ε.
10
Example 4.2. Continue from Example 4.1. For all nodes
in level 1, ε
5 is used to calculate their noisy counts. For
the expansion of the node labeled by v1 in Figure 1, we
4+10+9 = 0.43 and hv1 = 1. Therefore, the
have Pmax =
noisy counts of v1’s children are calculated with privacy pa-
rameter ε − ε
5 = 4ε
5 . For the expansion of node v2, we
4+10+9 = 0.43 and hv2 = 2. Hence its chil-
get Pmax =
dren’s noisy counts are calculated with privacy parameter
ε− ε
5 . For the expansion of node v3, we have Pmax = 4
2 = 2ε
5
9
5 is used to compute the noisy counts
10
and hv3 = 1. Thus, 2ε
of v3’s children.
The sensitivities of Q (Line 9) in diﬀerent levels are dif-
ferent. For Q in level i, a single record of length ≤ ℓmax
can change Q by at most ℓmax − i + 1. However, under the
adaptive privacy budget allocation scheme, we have to use
the largest sensitivity among all levels, that is ℓmax , in all
Laplace mechanisms; otherwise, ε-diﬀerential privacy may
be violated.
6432. If level(C(vc), T ) ≥ 2, ∀vi ∈ V −,
A(vi) = rvi · Xvj ∈V +
A(vj)
3. Otherwise,
(a) IfPvj ∈V + A(vj) ≤ c(v), ∀vi ∈ V −,
A(vi) =
c(v) −Pvj ∈V + A(vj)
|V −|
(b) Otherwise, ∀vi ∈ V −, A(vi) = 0
4. Renormalize: ∀vi ∈ V , c(vi) = c(v) ·
A(vi)
Pvj ∈V A(vj )
If vc can ﬁnd a high-quality Markov parent in T (i.e., one
representing an n-gram with n ≥ 2 5), we estimate its counts
from its high-quality siblings based on the ratio deﬁned in
Step 1. The idea behind this deﬁnition is that the ratio of
any node insigniﬁcantly changes between Markov neighbors,
and hence, it can be well approximated from the Markov
parents. Otherwise, we approximate the noisy counts by
assuming a uniform distribution, that is, equally distribute
the count left among the missing nodes (Step 3). In Step 4,
these estimated counts are normalized by the parent’s count
in order to obtain consistent approximations.
Example 4.3. Continue from Example 4.1. Suppose that
A(v9) = 2.1, A(v10) = 4, A(v11) does not pass θ, and
A(v12) = 1.9. Since rv11 = 0/(4 + 0 + 0) = 0, A(v11) :≈
(1.9 + 4 + 2.1) · 0 = 0. Finally, renormalizing the result,
we obtain c(v9) = 4 · 2.1/(2.1 + 4 + 1.9 + 0) ≈ 1, c(v10) =
4 · 4/(2.1 + 4 + 1.9 + 0) = 2, c(v11) = 0, c(v12) = 4 · 1.9/(2.1 +
4 + 1.9 + 0) ≈ 1.
4.3.5
Synthetic Sequential Database Construction
The released n-grams are useful for many data analysis
tasks. However, it is often necessary to generate a synthetic
database for diﬀerent types of queries and tasks.
In this
section, we propose an eﬃcient solution to construct a syn-
thetic sequential database from the exploration tree T (Line
17). The general idea is to iteratively generate longer grams
(up to size ℓmax ) based on the Markov assumption and make
use of the theorem below for synthetic sequential database
construction.
Theorem 4.1. Given the set of n-grams with size 1 ≤
n ≤ ℓmax , the (truncated) input database (with maximal se-
quence length ℓmax ) can be uniquely reconstructed.
Proof. (Sketch) Since ℓmax -grams can only be supported
by sequences of length ℓmax , all sequences of length ℓmax can
be uniquely reconstructed by ℓmax -grams. Once all ℓmax
sequences have been identiﬁed, we can update the n-gram
counts by decreasing the numbers of occurrences of the n-
grams in ℓmax sequences. The resulting set of n-grams (1 ≤
n ≤ ℓmax − 1) can then be considered as if they were gener-
ated from an input database with maximal sequence length
ℓmax − 1. Therefore, sequences of length ℓmax − 1 can be
uniquely reconstructed as well. Following this iterative pro-
cess, all sequences can be uniquely identiﬁed. This proof
explains the way we generate the synthetic database based
on noisy n-grams.
5A unigram conveys an unconditional probability and there-
fore cannot provide a very accurate estimation.
Intuitively, longer grams can be generated by “joining”
shorter grams. Formally, we deﬁne a join operation over
two n-grams. Let g1 = L11 → L12 → · · · → L1n and g2 =
L21 → L22 → · · · → L2n. Then g1 can join with g2 if
∀2 ≤ i ≤ n, L1i = L2(i−1), denoted by g1 ✶ g2, and g1 ✶
g2 = L11 → L12 → · · · → L1n → L2n. Note that the join
operation is not symmetric:
it is possible that g1 can join
with g2, but not vice versa.
Let the height of T be h. We iteratively extend T by
generating n-grams with h < n ≤ ℓmax , starting from level
h of T . We extend T level by level, where level n + 1,
representing all (n + 1)-grams, can be generated by joining
all possible n-grams in level n. Let g1 and g2 be two n-grams
that can be joined. Then we can estimate the count of the
joined (n + 1)-gram as follows:
|g1 ✶ g2| = c(g1) × P (L2n|g1)
= c(g1) × P (L2n|L11 → L12 → · · · → L1n)
≈ c(g1) × P (L2n|L12 → L13 → · · · → L1n)
= c(g1) × P (L2n|L21 → L22 · · · → L2(n−1))
≈ c(g1) ×
c(L21
2n)
c(L21
2(n−1))
Note that all counts in the above equation are noisy ones
for the reason of privacy (see more details in Section 5).
Since c(L21
2(n−1)) must have been known in the
extended T , |g1 ✶ g2| can be computed. We keep extending
T until: 1) ℓmax has been reached, or; 2) no grams in a level
can be joined.
2n) and c(L21
c(I3→I1) = 4 × 2
Example 4.4. Continue from Example 4.1. I2 → I3 →
I1 and I3 → I1 → I2 can be joined to generate I2 → I3 →
I1 → I2. Its count can be estimated by c(I2 → I3 → I1) ×
c(I3→I1→I2)
4 = 2. This 4-gram is represented as a
new node in T , as illustrated by v13 in Figure 1. Similarly,
I2 → I3 → I1 can join with I3 → I1 → I1, resulting in
I2 → I3 → I1 → I1. Since these two 4-grams cannot be
joined, the extension of T ends at level 4.
After extending T , we can generate the synthetic database
in the following way. Let the height of the extended T be
he. We start from level he. For each v ∈ levelSet(he, T ),
we publish c(v) copies of g(v), and update the counts of all
nodes supported by g(v) (i.e., all nodes representing a gram
that can be generated from g(v)). An n-gram supports at
i=1 i = n(n+1)
nodes and therefore requires at most
updates. With a hash map structure, each update
n(n+1)
2
most Pn
2
can be done in constant time.
Example 4.5. Continue from Example 4.4. For node v13
in level 4 of T , we publish 2 copies of I2 → I3 → I1 → I2
and update the counts of all nodes supported by g(v13), that
is, the nodes representing I1, I2, I3, I2 → I3, I3 → I1,
I1 → I2, I2 → I3 → I1 and I3 → I1 → I2.
5. PRIVACY ANALYSIS
We give the privacy guarantee of our approach below.
Theorem 5.1. Algorithm 1 satisﬁes ε-diﬀerential privacy.
Proof. (Sketch) Due to the correlation of the counts in
the same level of T (i.e., a single sequence can aﬀect mul-
tiple counts in a level), the sequential composition and par-
644allel composition properties [15] must be applied with cau-
tion. Hence, we prove the theorem by the deﬁnition of ε-
diﬀerential privacy. Consider two neighboring databases D
and D′. We ﬁrst consider Lines 1 − 15 of Algorithm 1, that
is, the construction of T . Let this part be denoted by A.
Then we need to prove P r[A(D)=T ]
In essence, T
is built on the noisy answers to a set of count queries (via
Laplace mechanism). Let each root-to-leaf path be indexed
by j. We denote a node in level i and path j by vij, its pri-
vacy parameter by εij, and its count in D and D′ by Q(D)ij
and Q(D′)ij, respectively. Then we have
P r[A(D′)=T ] ≤ eε.
P r[A(D) = T ]
P r[A(D′) = T ]
=
nmaxYi=1
|L|i
Yj=1
exp(−εij
exp(−εij
|c(vij )−Q(D)ij )|
)
|c(vij )−Q(D′)ij )|
ℓmax
ℓmax
(4)
)
We ﬁrst claim that a single record can only aﬀect at most
ℓmax root-to-leaf paths. This is due to two facts: (1) all an-
cestors of a node that is inﬂuenced by the additional record
must also be inﬂuenced; (2)Pj |Q(D)ij − Q(D′)ij| ≤ ℓmax .
Therefore, Equation 4 could be rewritten as
P r[A(D) = T ]
P r[A(D′) = T ]
|c(vij )−Q(D)ij )|
)
|c(vij )−Q(D′)ij )|
ℓmax
)
ℓmax
j=1 εij|Q(D)ij − Q(D′)ij|
=
exp(−εij
exp(−εij
ℓmaxYj=1
nmaxYi=1
≤ exp Pnmax
i=1 Pℓmax
≤ exp  1
ℓmaxXj=1
nmaxXi=1
≤ exp  1
ℓmax
ℓmax
≤ eε
ℓmax
εij!
ε!
ℓmaxXj=1
SincePi εij = ε, we have
P r[A(D) = T ]
P r[A(D′) = T ]
Note that εij is calculated based on noisy counts. Hence,
the construction of T satisﬁes ε-diﬀerential privacy. In ad-
dition to the construction of T , we enforce consistency con-
straints on T and generate the synthetic sequential database
in Lines 16 and 17. Since these two steps are conducted on
noisy data and do not require access to the original database,
they satisfy 0-diﬀerential privacy. Therefore, our solution as
a whole gives ε-diﬀerential privacy.
6. PERFORMANCE ANALYSIS
6.1 Error Analysis
The error of the sanitized data comes from three major
sources: ﬁrst, using n-grams with 1 ≤ n ≤ h to estimate
longer n-grams with h < n ≤ ℓmax (recall that h is the
height of the unextended tree, see Section 4.3.5); second, the
truncation conducted to limit the eﬀect of a single sequence;
third, the noise added to the n-grams with 1 ≤ n ≤ h to sat-
isfy diﬀerential privacy. We call the ﬁrst two types of error
approximation error and the last type of error Laplace er-
ror. Given a speciﬁc ε value, the total error of our approach
is determined by ℓmax and nmax . Intuitively, a smaller ℓmax
value incurs larger approximation error, but meanwhile it in-
troduces less Laplace error because of a smaller sensitivity.
Analogously, a smaller nmax value causes larger approxima-
tion error, but results in more accurate counts. Therefore
Table 4: Experimental dataset characteristics.
Datasets
MSNBC
STM
|D|
989,818
1,210,096
|I| max|S|
17
14,795
342
121
avg|S|
5.7
6.7
our goal is to identify good values for ℓmax and nmax that
minimize the sum of approximation error and Laplace error.
Due to the space limit, we experimentally study the eﬀect
of varying ℓmax and nmax values on the performance of our
solution and provide our insights in Appendix A. In gen-
eral, our solution is designed to perform stably well under
a relatively wide range of ℓmax and nmax values. In the rest
of this section, we only report the major results of our solu-
tion in terms of count query and frequent sequential pattern
mining.
6.2 Experimental Evaluation
!
We experimentally evaluate the performance of our solu-
tion (referred to as N-gram) in terms of two data analysis
tasks, namely count query and frequent sequential pattern
mining. As a reference point, for count query, we compare
the utility of our solution with the approach proposed in [4],
which relies on a preﬁx tree structure (referred to as Preﬁx );
for frequent sequential pattern mining, we compare our ap-
proach with both Preﬁx and the method designed in [16] for
ﬁnding frequent (sub)strings (referred to as FFS ). Two real-
life sequential datasets are used in our experiments. MSNBC
describes sequences of URL categories browsed by users in
time order on msnbc.com.
It is publicly available at the
UCI machine learning repository6. STM records sequences
of stations visited by passengers in time order in the Mon-
treal transportation system. It is provided by the Soci´et´e de
transport de Montr´eal 7. The detailed characteristics of the
datasets are summarized in Table 4, where |D| is the number
of records (sequences) in D, |I| is the universe size, max |S|
is the maximum length of sequences in D, and avg|S| is the
average length of sequences.
6.2.1 Count Query
To evaluate the performance of our approach for count
queries, we follow the evaluation scheme that has been widely
used in previous works [23], [22], [6]. The utility of a count
query Q is measured by the relative error of its answer on
the sanitized sequential database Q(eD) with respect to the
true answer on the original database Q(D), which is formal-
ized as follows:
error (Q(eD)) =
|Q(eD) − Q(D)|
max{Q(D), s}
,
where s is a sanity bound that mitigates the eﬀect of the
queries with extremely small selectivities [23], [22]. Follow-
ing the convention, s is set to 0.1% of |D|, the same setting
as [23], [22], [6]. In particular, the answer to Q is deﬁned
to be the number of occurrences of Q in a database. For
example, given Q = I2 → I3, its answer over the dataset
in Table 1 is 6. This type of count queries plays an im-
portant role in many applications, for example, calculating
riderships in a transportation system.
In the ﬁrst set of experiments, we examine the average
relative errors of count queries under diﬀerent query sizes
6http://archive.ics.uci.edu/ml/
7http://www.stm.info
645r
o
r
r
E
e
v
i
t
l
a
e
R
e
g
a
r
e
v
A
r
o
r
r
E
e
v
i
t
l
a
e
R
e
g
a
r
e