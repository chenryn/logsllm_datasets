title:Undermining User Privacy on Mobile Devices Using AI
author:Berk G&quot;ulmezoglu and
Andreas Zankl and
M. Caner Tol and
Saad Islam and
Thomas Eisenbarth and
Berk Sunar
Undermining User Privacy on Mobile Devices Using AI
Berk Gulmezoglu
M. Caner Tol
Andreas Zankl
Fraunhofer AISEC
PI:EMAIL
Worcester Polytechnic Institute
PI:EMAIL
Middle East Technical University
PI:EMAIL
Saad Islam
Worcester Polytechnic Institute
PI:EMAIL
Thomas Eisenbarth
University of Lübeck
PI:EMAIL
Berk Sunar
Worcester Polytechnic Institute
PI:EMAIL
ABSTRACT
Over the past years, literature has shown that attacks exploiting
the microarchitecture of modern processors pose a serious threat to
user privacy. This is because applications leave distinct footprints in
the processor, which malware can use to infer user activities. In this
work, we show that these inference attacks can greatly be enhanced
with advanced AI techniques. In particular, we focus on profiling
the activity in the last-level cache (LLC) of ARM processors. We
employ a simple Prime+Probe based monitoring technique to ob-
tain cache traces, which we classify with deep learning methods
including convolutional neural networks. We demonstrate our ap-
proach on an off-the-shelf Android phone by launching a successful
attack from an unprivileged, zero-permission app in well under
a minute. The app detects running applications, opened websites,
and streaming videos with up to 98% accuracy and a profiling phase
of at most 6 seconds. This is possible, as deep learning compensates
measurement disturbances stemming from the inherently noisy
LLC monitoring and unfavorable cache characteristics. In summary,
our results show that thanks to advanced AI techniques, inference
attacks are becoming alarmingly easy to execute in practice. This
once more calls for countermeasures that confine microarchitec-
tural leakage and protect mobile phone applications, especially
those valuing the privacy of their users.
CCS CONCEPTS
• Security and privacy → Mobile platform security; Software
and application security; Side-channel analysis and counter-
measures; • Computing methodologies → Machine learning.
KEYWORDS
Artificial Intelligence; Machine Learning; User Privacy; Activity
Inference; Cache Attack; Microarchitecture; ARM; Mobile Device
ACM Reference Format:
Berk Gulmezoglu, Andreas Zankl, M. Caner Tol, Saad Islam, Thomas Eisen-
barth, and Berk Sunar. 2019. Undermining User Privacy on Mobile Devices
Using AI. In ACM Asia Conference on Computer and Communications Secu-
rity (AsiaCCS ’19), July 9–12, 2019, Auckland, New Zealand. ACM, New York,
NY, USA, 14 pages. https://doi.org/10.1145/3321705.3329804
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
AsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6752-3/19/07...$15.00
https://doi.org/10.1145/3321705.3329804
1 INTRODUCTION
In 2017, more than 2 billion Android devices were used monthly [44].
The security and privacy of the applications deployed on these de-
vices are therefore of great relevance. The Android operating system
consequently employs a variety of protection mechanisms. Apps
run in sandboxes, inter-process communication is regulated, and
users have some degree of control via the permission system. The
majority of these features protects against software-based attacks
and logical side-channel attacks. The processor hardware, however,
also constitutes an attack surface. In particular, the shared processor
cache heavily speeds up the execution of applications. As a side
effect, each application leaves a footprint in the cache that can be
profiled by others. These footprints, in turn, contain sensitive infor-
mation about the application activity. Jana et al. [26] showed that
browsing activity yields unique memory footprints that allow the
inference of accessed websites. Oren et al. [36] demonstrated that
these footprints can be observed in the cache even from JavaScript
code distributed by a malicious website. While these attacks have
succeeded based on a solid amount of engineering, the increasing
complexity of applications, operating systems (OS), and proces-
sors make their implementation laborious and cumbersome. Yet,
studying side-channel attacks is important to protect security and
privacy critical applications in the long term. We believe that ma-
chine learning (ML), especially deep learning (DL) techniques such
as convolutional neural networks, assist in making side-channel
analysis significantly more scalable. Deep learning reduces the hu-
man effort by efficiently extracting relevant information from noisy
and complex observations. At the same time, it introduces a new
risk as attacks become more potent and easier to implement in
practice. We demonstrate this risk and compile a malicious Android
application, which, despite having no privileges or permissions, can
infer user activities across application and OS boundaries. With the
app, we are able to detect other running applications with up to
98% confidence. With this information, we focus on activities that
happen within an application. We detect visited websites in Google
Chrome and identify videos that are streamed in the Netflix and
YouTube applications. Those inferences are possible by analyzing
simple last-level cache (LLC) observations of at most 6 seconds
with machine learning algorithms. The entire attack succeeds in
well under a minute and reveals sensitive information about the
mobile phone user. None of the currently employed protection
mechanisms prevent our attack, as the LLC is shared between dif-
ferent processes and can be monitored from user space. Our cache
profiling technique is based on the Prime+Probe attack [45], which
relies on cache eviction to monitor the LLC. Cache eviction, in turn,
requires sets of memory addresses that map to a single cache set.
Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand214In contrast to previous work, we compile these address sets with a
novel algorithm that succeeds even for imprecise timing sources,
random line replacement policies, and without access to physical
memory addresses. This introduces a certain amount of noise in
the cache observations. We counter this effect by applying machine
learning, and in particular deep learning, to the observations. We
also compare the performance of the employed machine learning
techniques. While support vector machines (SVMs) and stacked
autoencoders (SAEs) struggle during classification, convolution
neural networks (CNNs) succeed in efficiently extracting distinct
features and, thus, surpass the other techniques. As CNNs have
recently gained attention in the field of side-channel analysis, we
explain our parameter selection and compare it to related work.
For the implementation of our attack, we neither require the target
phone to be rooted nor the malicious application to have certain
privileges or permissions. On our test device, a Nexus 5X, the An-
droid OS is up-to-date and all security patches are installed. The
malicious code runs in the background, requires no human contri-
bution during the attack, and draws little attention due to the short
profiling phase.
Our Contribution. In summary, we
• propose an inference attack on mobile devices that works
without privileges, permissions, or special OS interfaces.
• find eviction sets with a novel dynamic timing test that works
even with imprecise timing sources, random line replace-
ment policies, and virtual addresses only.
• classify cache observations using machine learning (SVM,
SAE, CNN) and thereby infer running applications, opened
websites, and streaming videos.
• achieve classification rates of up to 98% with a profiling
phase of at most 6 seconds and an overall attack time of well
under a minute.
The rest of the paper is organized as follows: Section 2 provides a
brief background on cache profiling and machine learning. Section 3
explains the proposed inference attack in detail. Section 4 presents
the results of our experiments followed by a discussion in Section 5.
Section 6 gives an overview of previous work and compares our
results with other techniques. Section 7 concludes our work.
2 BACKGROUND
This section provides a brief introduction to the employed cache
profiling and machine learning techniques.
2.1 Cache Profiling
The cache of modern processors consists of multiple levels. Higher
levels are small and often private to processor cores, while lower
levels are larger and shared among cores. The last-level cache (LLC)
is the last stage before external memory (e.g. RAM). Among gen-
eral purpose processors, set-associative caches are common. These
caches are split into a number of cache sets, each of which contains
a number of cache lines (equal to the associativity). While every
address is deterministically mapped to a cache set, the exact line
the corresponding data will be stored on is chosen by a replacement
policy. ARM-based application processors mostly employ random
2
selection policies, while Intel x86 processors often implement vari-
ants of least-recently used (LRU). Throughout the cache, data is
stored on fixed-size cache lines of typically 64 bytes.
Prime+Probe. The cache is a resource that is competitively shared
between executing threads. This means that the cache activity of
each thread influences the runtime of all other threads. A malicious
application can alter its cache footprint and time its execution
such that it learns what other applications are executing. This is
the basis for cache attacks and often referred to as cache profiling.
Tromer et al. [45] proposed a prominent profiling technique called
Prime+Probe. In the prime step, the adversary fills one or more cache
sets with own data. This is done by accessing a set of addresses that
all map to the same cache set. This set of addresses is called eviction
set. After a short waiting period, the adversary measures how long
it takes to re-accesses all addresses in the eviction set. If no other
thread placed data in the monitored cache set, this re-access cycle
will be fast. In contrast, if one or more cache lines got replaced in
the meantime, the re-access cycle will trigger line replacements
and, thus, will be slower. As a result, the timing measurements of
the adversary reflect the activity in the cache.
2.2 Machine Learning
The following paragraphs introduce support vector machines, stacked
autoencoders, and convolutional neural networks.
Support Vector Machines (SVMs). SVMs construct a classifier
by mapping training data into a higher dimensional space, where
distinct features can efficiently be separated. This separation is
achieved with a hyperplane that maximizes the margin between
the classes. A regularization parameter allows a tunable degree of
misclassification while finding the hyperplane.
Stacked Autoencoders (SAEs). An autoencoder (AE) is a type of
neural network that can be trained to reconstruct an input. The
network consists of an encoder function h = f (x ) that extracts dis-
tinct features from the input x and a decoder unit that reconstructs
the original data r = д(h). The network is trained such that the
error between r and x is minimized. Stacked AEs are constructed
by combining multiple AEs sequentially. The idea behind SAEs is
to learn only useful input features instead of learning an exact copy
of the input. With a softmax layer at the end, SAEs can be used as
classifiers for supervised learning.
Convolutional Neural Networks (CNNs). CNNs consist of neu-
rons that are interconnected and grouped into layers. Each neuron
computes a weighted sum of its inputs using a (non-) linear activa-
tion function. Those inputs either stem from the actual inputs to the
network or from previous layers. A typical CNN comprises multiple
layers of neurons. Convolution layers are the core of the CNN. They
consist of filters that are slid over the width and height of the input
to learn any two-dimensional patterns. The activation functions
in the neurons thereby extract distinct features from each input.
For efficiency, the neurons are connected only to a local region of
the input. The depth of the convolution layer defines the number
of filters and the stride controls how fast they are moved over the
input. Pooling layers apply a filter to the input and forward only
the maximum coefficient from every subregion. This reduces the
size of the input and avoids over-fitting, as only the most dominant
features from the convolution layer are passed to the rest of the
Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand2158192 B
allocated virtual memory
4096 B
page 0
page 1
63
...
16
15
12
11
56
0
cache set
cache line
64 B
line 0
line 1
...
line 63 line 0
line 1
...
line 63
page table index /
page frame number
page offset
set 0
1
...
512
513
last level-cache
...
Figure 1: Mapping of virtual memory to cache sets.
network. Dense or fully-connected layers learn additional relations
between the activations of the previous layers. The final layer of the
network is a loss layer with a size of 1 × L, where L is the number
of labels or classes. A back-propagation algorithm decreases the
loss value by updating the weights in the network. For each classi-
fication, the network provides a probability estimate that indicates
the confidence of the model for a predicted class. The sum of all
estimates equals to 1.
3 INFERENCE ATTACK
The threat model of our proposed inference attack assumes that
a mobile device user installs a malicious application from an app
store on Android. This happens regularly, as malicious apps offer
benign functionality to disguise malicious background activities
(e.g. hidden crypto currency mining). The malicious code needed
for our attack operates from user space and does not need any app
permissions. This means that we neither require a rooted phone, nor
ask the user for certain permissions, nor rely on any exploits, e.g.,
to escalate privileges or to break out of sandboxes. Furthermore, we
do not rely on features or programming interfaces that might not
be available on all Android versions. The sole task of our malicious
code is to profile the LLC and classify victim activities with pre-
trained ML/DL models. Once the LLC profiles have been gathered,
the models are queried to infer sensitive information.
3.1 Attack Outline
The proposed inference attack consists of two main phases. In the
training phase, the attacker creates ML/DL models on a training
device that is similar to the target device. Ideally, the processor
and operating system are identical on both devices. The models
are created by recording raw LLC profiles of target applications,
websites, and videos, followed by preparing the feature vectors, and
training the ML/DL algorithms with them. The trained models are
then directly integrated into the malicious application, which is
subsequently published in the app store. In the attack phase, the
malicious app prepares eviction sets for profiling the LLC on the tar-
get device. Subsequently, the LLC sets are profiled in a Prime+Probe
manner and the feature vectors are extracted. Finally, the feature
vectors are classified with the pre-trained models to infer opened
applications, visited websites, and streamed videos. All steps of the
attack phase are lightweight and can be executed in the background
without drawing notable attention.
3
Figure 2: Virtual/physical address and its interpretation.
3.2 Finding Eviction Sets
Once deployed, the first task of the malicious app is to find eviction
sets on the target device. An eviction set is a group of memory
addresses that map to the same cache set. These addresses are
called set-congruent. Figure 1 illustrates the problem of finding set-
congruent addresses and forming eviction sets. The graphic shows
a block of virtual memory that is backed by two fixed-size memory
pages. In the figure, we assume a common page size of 4 KiB. As
soon as an address within the block is accessed, the corresponding
memory content is brought into the processor cache. Since the
cache manages data on fixed-size cache lines, one access will cache
multiple bytes. We assume a common cache line size of 64 bytes.
The illustrated cache is set-associative and holds multiple lines per
cache set. Memory that is brought into the cache is deterministically
assigned to a cache set. For last-level caches, this assignment is
commonly derived from physical addresses that are unavailable to
most user space applications. Figure 2 illustrates the link between
virtual and physical addresses, and how they are interpreted by the
cache. The most significant bits of each virtual address are the page
table index, while the least significant bits are the page offset. A
page size of 4 KiB yields loд2(4096) = 12 offset bits. The page table
index is used to lookup an entry in the page tables that contains the