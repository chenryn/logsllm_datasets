 **Apache Airflow version** :1.10.12
**Kubernetes version (if you are using kubernetes)** (use `kubectl version`):
v.1.14.8
**Environment** :
  * **Cloud provider or hardware configuration** :AWS/EKS
  * **OS** (e.g. from /etc/os-release):
  * **Kernel** (e.g. `uname -a`):
  * **Install tools** :
  * **Others** :
**What happened** :
I upgraded my airflow cluster from 1.10.10 to 1.10.12 and the python version
from 3.7 to 3.8. The cluster is deployed on EKS using the K8s executor and
syncing logs using s3. I found a repeatable bug on the UI. When I try to get
the logs from a dag with just one attempt, it tries to get the logs from the
pod, and as we erased the pods by default when they finished using the
airflow.cfg options, so as the pod is erased, it cannot fetch logs from the
pod, and it raised an error instead of searching the logs from S3.
    *** Unable to fetch logs from worker pod hermesemaileventshermesemaileventstransationalloadredshift-7ffa ***
    (403)
    Reason: Forbidden
When the task has two attempts, I can sync the logs from the UI easily
    *** Reading remote log from s3://wt-prod-euwest1-zephyr/logs/adsales/adsales-shutterstock-image-report/2020-10-06T08:30:00+00:00/8.log.
I think this bug is related to this PR. #8626
**What you expected to happen** :
Be able to sync the logs from s3 after the pod is deleted. If the pod do not
exist it should be able to fetch logs from a remote storage
**How to reproduce it** :
**Anything else we need to know** :