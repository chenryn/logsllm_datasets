title:A methodology for the generation of efficient error detection mechanisms
author:Matthew Leeke and
Saima Arif and
Arshad Jhumka and
Sarabjot Singh Anand
A Methodology for the Generation of Efﬁcient Error Detection Mechanisms
Matthew Leeke, Saima Arif, Arshad Jhumka and Sarabjot Singh Anand
Department of Computer Science
University of Warwick
Coventry, UK, CV4 7AL
{matt, saima, arshad, ssanand}@dcs.warwick.ac.uk
Abstract—A dependable software system must contain error
detection mechanisms and error recovery mechanisms. Soft-
ware components for the detection of errors are typically
designed based on a system speciﬁcation or the experience
of software engineers, with their efﬁciency typically being
measured using fault injection and metrics such as coverage
and latency. In this paper, we introduce a methodology for
the design of highly efﬁcient error detection mechanisms. The
proposed methodology combines fault injection analysis and
data mining techniques in order to generate predicates for
efﬁcient error detection mechanisms. The results presented
demonstrate the viability of the methodology as an approach
for the development of efﬁcient error detection mechanisms,
as the predicates generated yield a true positive rate of almost
100% and a false positive rate very close to 0% for the
detection of failure-inducing states. The main advantage of the
proposed methodology over current state-of-the-art approaches
is that efﬁcient detectors are obtained by design, rather than
by using speciﬁcation-based detector design or the experience
of software engineers.
Keywords-Software Dependability; Fault Injection; Data Min-
ing; Error Detection Mechanisms; Decision Tree Induction
I. INTRODUCTION
The design of dependable software systems is known to be
an inherently difﬁcult problem [1]. A dependable software
system must contain two types of dependability component,
viz., error detection mechanisms and error recovery mech-
anisms [2], which are commonly known as detectors and
correctors respectively. A detector component is a program
component
that asserts the validity of a predicate in a
program at a given location [2] [3]. To evaluate the efﬁ-
ciency of a detector component in a software system, fault
injection is used to evaluate metrics [4], such as coverage
and latency [5], that capture efﬁciency properties [6] [7].
If the values of these efﬁciency metrics do not reach a
given threshold, the detector component under test must be
redesigned or relocated and the efﬁciency metrics evaluated
once more. This process is repeated until the efﬁciency prop-
erties of the detector component satisfy the dependability
requirements placed on the software system.
Detector components are currently designed based on
a system speciﬁcation [6] or the experience of software
engineers [8]. It has been shown that the efﬁciency properties
of detectors can be classiﬁed along two dimensions; (i)
completeness and (ii) accuracy [3]. The completeness of a
detector component relates to its ability to detect erroneous
states, i.e., to ﬂag true positives, whilst accuracy relates to
its ability to avoid making incorrect detections, i.e., to avoid
false positives. An erroneous state is one that will lead to
failure if the error is not handled. Failure is characterised
as a violation of the system speciﬁcation. A detector that is
both complete and accurate is known as a perfect detector.
However, due to implementation constraints, e.g., read/write
restrictions, it is, in general, not possible to develop perfect
detectors [9]. A perfect detector at a given location in a
program is therefore the most efﬁcient detector for that
location. In this paper, we use the term efﬁcient detector to
refer to a detector with high completeness and high accuracy.
Research that has addressed the systematic design of efﬁ-
cient detectors has generally focused on ﬁnite-state software
systems [2] [3] [10] [11]. However, little work has focused
on the systematic design of efﬁcient detectors for real-world
(inﬁnite-state) software systems. In this paper, we address
this problem through a novel methodology for the design
of efﬁcient detector components. Most signiﬁcantly,
the
proposed methodology is applicable to inﬁnite-state software
systems and generates detector components whose efﬁciency
is guaranteed by design. The premise of the methodology is
that, given a program location at which a detector component
will be located and for which the detector must be obtained,
optimised data mining techniques can be used to analyse
fault injection data in order to obtain efﬁcient predicates for
that detector component.
A. Contributions
In this paper, we make the following speciﬁc contributions:
• We propose a methodology for the generation of efﬁ-
cient predicates for detectors based on the application
of data mining techniques to fault injection data.
• We apply the proposed methodology to modules in
three complex software systems using the PROPANE
tool [12] and the Weka Data Mining suite [13].
• We evaluate the effectiveness and complexity of the
generated error detection mechanisms using ten-fold
cross validation, as well as documenting the variance
associated with the models generated by this process
and showing that our the methodology produces efﬁ-
cient detection predicates.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
978-1-4244-9233-6/11/$26.00 ©2011 IEEE25The overarching contribution of this paper is to propose
a data mining-based approach as an effective methodology
for the generation of efﬁcient error detection mechanisms.
The generality of the proposed methodology is based on
the premise that fault injection analysis performed under a
known fault model can capture some relationship between
the states of an executing program and the behaviours
exhibited by that program. Typically fault injection data
is interpreted with respect
to the goal of understanding
situations that lead to failure. The approach advocated in
this paper takes this notion further, in that data mining
algorithms are used to generate predicates which capture
aspects of program correctness to be used in error detection
mechanisms. The generated predicates are efﬁcient, in the
sense that they have a high accuracy and completeness.
B. Paper Structure
The remainder of this paper is structured as follows: In
Section II we provide a survey of existing work in the area of
detector design. We then detail the adopted system and fault
models in Section III. In Section IV we provide background
on data mining techniques that are relevant to the proposed
methodology. In Section V we provide an overview of the
methodology, as well as a description of each step required
for its application. In Section VI we explain the experimental
setup used to validate the proposed methodology. In Section
VII we demonstrate the type of results the methodology is
capable of generating. In Section VIII we discuss the main
characteristics and limitations of the methodology, before
concluding in Section IX with a paper summary and a
discussion of future work.
II. RELATED WORK
When designing a dependable software system, two impor-
tant challenges exist, namely (i) the design of the depend-
ability components, i.e, detectors and correctors, and (ii) the
subsequent location/placement of these components [14] to
contain the propagation of errors. Metrics, such as coverage
and latency [5], are often used to evaluate the efﬁciency
of dependability components. In general, coverage relates
to the design problem, while latency relates to the location
problem. In this paper, we focus on the design problem and
assume that the program locations are known, e.g., through
techniques such as [14].
A. Detector Design
Several previous approaches to the detector design problem
have focused on experimentally evaluating the coverage and
latency of executable assertions (EAs) using fault injection.
Through these approaches, it was established that EAs ex-
hibiting high coverage and low latency serve to reduce error
propagation. However, designing such EAs is difﬁcult and
error-prone, as demonstrated in [8], where it was remarked
that “...the process of writing self checks is obviously
difﬁcult”. To remedy this the authors in [8] suggested that
“...more training or experience might be helpful”.
One approach for designing EAs uses the speciﬁcation of
a software system, or the constraints placed on its signals,
i.e., parameters, to design corresponding EAs, e.g., [6] [7].
These EAs may not exhibit the high efﬁciency needed in
dependable systems [15]. Speciﬁcally, it has been shown
in [15] that such EAs may not ﬂag erroneous states, i.e.,
false negatives, or may incorrectly ﬂag correct states as
being erroneous, i.e., false positives. When EAs do not meet
the coverage threshold required of a system, they must be
redesigned. However, very little work exists that helps with
the reﬁnement of EAs in practical software. The reﬁnement
of detectors has been investigated in ﬁnite-state systems,
represented as state transition systems, e.g., [3] [11]. In
these approaches, polynomial-time algorithms have been
developed to automatically reﬁne detectors. As a matter of
contrast, we target the reﬁnement of predicates for EAs for
real-world (inﬁnite state) software systems, through the use
of data mining techniques. This problem has received very
little attention in existing literature.
B. Data Mining Techniques
Data mining techniques have been used in the analysis of
failure data for dependable software. For example, Pint´er
et.al [16] used data mining techniques on raw data obtained
during dependability benchmarking to identify key infras-
tructural factors for determining the behaviour of systems
in the presence of faults. These investigations can help to
identify weaknesses or vulnerabilities in a software system.
As a matter of contrast, we propose a new approach, which
complements existing ones, where by predicates for error
detection mechanisms are discovered in order to limit error
propagation, i.e, we develop detection mechanisms that ad-
dress vulnerabilities. Data mining techniques have also been
applied to address a number of other software dependability
issues. For example, in the context of computer security,
data mining has been shown to be an effective approach to
intrusion detection and anomaly identiﬁcation [17] [18].
C. Static Analysis
A static analysis of a program is performed without ex-
ecuting the program. Techniques that implement notions
of static analysis include model checking [19], data-ﬂow
analysis [20] and abstract interpretation [21]. Model check-
ing approaches generally consider systems that have ﬁnite
state or may be reduced to ﬁnite state by some degree
of abstraction, whilst data-ﬂow analysis is a lattice-based
technique for gathering information about a possible set of
permissible values. Abstract interpretation is a technique
where the aim is to model the effect that every statement
has on the state of an abstract machine, i.e., it executes
the software based on the mathematical properties of each
statement and declaration. Such an abstract machine is
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
26known to over-approximate the behaviours of a system.
The abstract system is therefore made simpler to analyse
at the expense of incompleteness, as not every property that
is true of the original system is also true of the abstract
system. However, if properly done, abstract interpretation
is sound, meaning that every property that is true of the
abstract system can be mapped to a property that is true of
the original system.
It is well-known that, barring some hypothesis that the
state space of programs is ﬁnite and small, ﬁnding all
possible run-time errors, or more generally any kind of
violation of a speciﬁcation on the ﬁnal result of a program, is
undecidable. Thus, static analyses performed on a program
are, in general, sound, in the sense that the properties they
report are true.
D. Likely Program Invariants
An invariant is a property that holds at a certain point or
points in a program. Determining all the sound invariants for
a program may be undecidable. Further, invariants reported
may not be sound, i.e., an invariant may hold true for most
executions, but not for some. Thus, determining likely invari-
ants [22] may be the best approximation, though steps must
be taken to handle false positives. The use of invariants is
valuable in many aspects of software development, including
program design, implementation, testing and maintenance.
Unfortunately, explicit invariants are usually absent from
programs, depriving programmers and automated tools of
their beneﬁts. The seminal work on discovering likely pro-
gram invariants [22] shows how invariants can be dynam-
ically detected from program traces that capture variable
values at program points of interest. The user runs the target
program over a test suite to create the traces, and an invariant
detector determines which properties and relationships hold
over both explicit variables and other expressions. A tool,
called Daikon, exists that supports the discovery of likely
program invariants. Subsequently, several applications of
the techniques have been proposed. For example, Demsky
et.al [23] applied these techniques to discover invariants of
abstract data types. More recently, these techniques have
been applied to detect permanent hardware failures [24].
Dynamic invariant detection is a machine learning technique
that can be applied to arbitrary data. However, invariants
generally do not hold in presence of transient failures. Our
approach differs from using Daikon as the tool has to be run
in parallel with the software under test, i.e., it is an online
approach, while our approach operates on data derived from
fault injection, i.e., it is an ofﬂine approach. Moreover, our
approach seeks to detect erroneous states that lead to failure
rather than all erroneous states.
III. MODELS
In this section, we present the system model and fault model
assumed by the analysis presented in this paper.
A. System Model
The methodology presented in this paper is concerned with
the analysis and enhancement of modular software, thus
we adopt a generic model of modular software systems. A
software system S is considered to be a set of intercon-
nected modules M1 . . . Mn. A module Mk contains a set
of non-composite variables Vk and a sequence of actions
Ak1 . . . Aki. The variables in Vk have a speciﬁc domain of
values. Each action in Ak1 . . . Aki may read or write to a
subset of variables in(cid:83)
k Vk.
In this paper we assume software to be grey box, meaning
that access to source code is permitted, but knowledge of
functionality and structure is not assumed, i.e., white box
access with black box knowledge.
B. Fault model
We assume a transient data value fault model [25], which
occurs when internal variables of a system hold erroneous
values. The transient fault model is generally used to model
hardware faults in which bit ﬂips occur in memory areas that
causes instantaneous changes to values held in memory.
IV. DATA MINING
Given a real-world process, great strides have been made
with respect to the modelling, collection, storage and query-
ing of data generated by the process. The process data is
usually modelled by a set of entities, their attributes and
their relationship to other entities. This is commonly known
as the relational model of data. Data generated, and hence
stored, within such a relational data model is a sample of all
the data that may be generated by the process. Often, rather
than being interested in the retrieval of stored data, we are
more interested in forecasting behaviours of the process not
previously encountered or learning some knowledge about
the process if the process itself is not well understood. For
example. we might be interested to learn how a software
system may behave when faced with an injected fault.
Data mining aims to learn useful and actionable knowl-
edge from large collections of data. In simple domains, it
is not unusual to assume that the data is a single relation
consisting of a set of n input attributes that deﬁne an n-
dimensional space called the Instance Space, I. Every point
in I is a potential state of the process being modelled. In
supervised learning the data mining algorithm is tasked with
learning a good approximation, ˆf, of an unknown function f
(referred to as the target function) given a training data set,
T ⊆ I, consisting of the N pairs (cid:104)xi, f (xi)(cid:105). If the function
is a discrete one the task is referred to as classiﬁcation. In
the case of learning a function from data generated through
fault injection, the function is binary as a program state
is either going to lead to a failure or not. The task of
learning a binary function is often referred to as concept
learning, a special case of classiﬁcation. Instances of the
class of interest, known as the concept, are referred to as
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 