title:Are We One Hop Away from a Better Internet?
author:Yi-Ching Chiu and
Brandon Schlinker and
Abhishek Balaji Radhakrishnan and
Ethan Katz-Bassett and
Ramesh Govindan
Are We One Hop Away from a Better Internet?
Yi-Ching Chiu∗, Brandon Schlinker∗, Abhishek Balaji Radhakrishnan,
Ethan Katz-Bassett, Ramesh Govindan
Department of Computer Science, University of Southern California
ABSTRACT
The Internet suﬀers from well-known performance, reliability, and
security problems. However, proposed improvements have seen lit-
tle adoption due to the diﬃculties of Internet-wide deployment. We
observe that, instead of trying to solve these problems in the general
case, it may be possible to make substantial progress by focusing
on solutions tailored to the paths between popular content providers
and their clients, which carry a large share of Internet traﬃc.
In this paper, we identify one property of these paths that may
provide a foothold for deployable solutions: they are often very short.
Our measurements show that Google connects directly to networks
hosting more than 60% of end-user preﬁxes, and that other large
content providers have similar connectivity. These direct paths open
the possibility of solutions that sidestep the headache of Internet-
wide deployability, and we sketch approaches one might take to
improve performance and security in this setting.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Network topology;
C.2.5 [Local and Wide-Area Networks]: Internet
Keywords
Measurements; Internet topology
1.
INTRODUCTION
Internet routing suﬀers from a range of problems, including
slow convergence [25, 43], long-lasting outages [23], circuitous
routes [41], and vulnerability to IP spooﬁng [6] and preﬁx hi-
jacking [44]. The research and operations communities have re-
sponded with a range of proposed ﬁxes [7,22,24,30,31]. However,
proposed solutions to these well-known problems have seen little
adoption [28,33,35].
One challenge is that some proposals require widespread adoption
to be eﬀective [6, 22, 28]. Such solutions are hard to deploy, since
they require updates to millions of devices across tens of thousands
∗These authors contributed equally to this work.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
© 2015 ACM. ISBN 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815719.
of networks. A second challenge is that the goal is often an approach
that works in the general case, applicable equally to any Internet
path, and it may be diﬃcult to design such general solutions.
We argue that, instead of solving problems for arbitrary paths, we
can think in terms of solving problems for an arbitrary byte, query,
or dollar, thereby putting more focus on paths that carry a higher
volume of traﬃc. Most traﬃc concentrates along a small number
of routes due to a number of trends: the rise of Internet video
had led to Netﬂix and YouTube alone accounting for nearly half
of North American traﬃc [2], more services are moving to shared
cloud infrastructure, and a small number of mobile and broadband
providers deliver Internet connectivity to end-users. This skewed
distribution means that an approach to improving routing can have
substantial impact even if it only works well over these important
paths. Further, it may be possible to take advantage of properties
of these paths, of the traﬃc along them, or of the providers using
them, in order to develop tailored approaches that provide increased
beneﬁt in these scenarios.
This paper focuses on one attribute of these high traﬃc routes:
they are very short. Our measurements show that, whereas the av-
erage path on the Internet traverses 1-2 intermediate transit ASes,
most paths from a large content provider, Google, go directly from
Google’s network into the client’s network.
While previous results suggested that the Internet has been “ﬂat-
tening” in this manner [20,26], our results are novel in a number of
ways. First, whereas previous work observed ﬂattening in measure-
ments sampling a small subset of the Internet, we quantify the full
degree of ﬂattening for a major content provider. Our measurements
cover paths to 3.8M /24 preﬁxes–all of the preﬁxes observed to re-
quest content from a major CDN–whereas earlier work measured
from only 50 [20] or 110 [26] networks. Peering links, especially of
content providers like Google, are notoriously hard to uncover, with
previous work projecting that traditional measurement techniques
miss 90% of these links [34]. Our results support a similar conclu-
sion to this projection: Whereas a previous study found up to 100
links per content provider across years of measurements [40] and
CAIDA’s analysis lists 184 Google peers [3], our analysis uncovers
links from Google to over 5700 peers.
Second, we show that, from the same content provider, popular
paths serving high volume client networks tend to be shorter than
paths to other networks. Some content providers even host servers
in other networks [9], which in eﬀect shortens paths further.
Third, in addition to quantifying Google’s connectivity, we pro-
vide context. We show that ASes that Google does not peer with
often have a local geographic footprint and low query volumes. In
addition, our measurements for other providers suggest that Mi-
crosoft has short peering paths similar to Google, whereas Amazon
relies on Tier 1 and other providers for much of its routing.
523We conclude by asking whether it might be possible to take
advantage of short paths–in particular those in which the content
provider peers directly with the client network–to make progress on
long-standing routing problems. Is it easier to make progress in this
setting that, while limited, holds for much of our web activity?
• The need to work over paths that span multiple administrative
boundaries caused, for example, our previous route reliability
work to require complex lockstep coordination among thousands
of networks [22]. Is coordination simpliﬁed when all concerned
parties already have a peering relationship?
• The source and destination of traﬃc have direct incentive to guar-
antee the quality of the route between them, but intermediate net-
works lack visibility into end-to-end issues. With direct paths that
bypass intermediate transit providers, can we design approaches
that use the natural incentives of the source and destination–
especially of large content providers–to deploy improvements?
• Some solutions designed to apply universally provide little beneﬁt
over simpler but less general techniques in likely scenarios [28].
Given the disproportionate role of a small number of providers,
can we achieve extra beneﬁt by tailoring our approaches to apply
to these few important players?
We have not answered these questions, but we sketch problems
where short paths might provide a foothold for a solution. We hope
this paper will encourage the community to take advantage of the
short paths of popular services to sidestep hurdles and improve
Internet routing.
2. DATASETS AND DATA PROCESSING
Our measurement goal is to assess the AS path lengths between
popular content providers and consumers of content. We use collec-
tions of traceroutes as well as a dataset of query volumes to estimate
the importance of diﬀerent paths.
Datasets. To assess paths from users to popular content, we use: (1)
traceroutes from PlanetLab to establish a baseline of path lengths
along arbitrary (not necessarily popular) routes; (2) a CDN log cap-
turing query volumes from end users around the world; (3) tracer-
outes from popular cloud service providers to preﬁxes around the
world; and (4) traceroutes from RIPE Atlas probes around the world
to popular cloud and content providers.
Traceroutes from PlanetLab. A day of iPlane traceroutes from April
2015 [29], which contains traceroutes from all PlanetLab sites to
154K BGP preﬁxes. These traceroutes represent the view of routing
available from an academic testbed.
End-User Query Volumes. Aggregated and anonymized queries to
a large CDN, giving (normalized) aggregate query count per /24
client preﬁx in one hour in 2013 across all of the CDN’s globally
distributed servers. The log includes queries from 3.8M client pre-
ﬁxes originated by 37496 ASes. The set has wide coverage, includ-
ing clients in every country in the world, according to MaxMind’s
geolocation database [1].
While the exact per preﬁx volumes would vary across provider
and time, we expect that the trends shown by our results would
remain similar. To demonstrate that our CDN log has reasonable
query distributions, we compare it with a similar Akamai log from
2014 (Fig. 21 in [16]). The total number of /24 preﬁxes requesting
content in the Akamai log is 3.76M, similar to our log’s 3.8M
preﬁxes. If V C
are the percentage of queries from the top n
and V A
n
n
preﬁxes in our CDN dataset and in Akamai’s dataset, respectively,
then |V C
n − V A
n | < 6% across all n. The datasets are particularly
similar when it comes to the contribution of the most active client
n − V A
preﬁxes: |V C
n | < 2% for n ≤ 100, 000, which accounts for
≈31% of the total query volume.
Traceroutes from the cloud. In March and August/September 2015,
we issued traceroutes from Google Compute Engine (GCE) [Central
US region], Amazon EC2 (EC2) [Northern Virginia region], and
IBM SoftLayer [Dallas DAL06 datacenter] to all 3.8M preﬁxes in
our CDN trace and all 154K iPlane destinations. For each preﬁx
in the CDN log, we chose a target IP address from a 2015 ISI
hitlist [15] to maximize the chance of a response. We issued the
traceroutes using Scamper [27], which implements best practices
like Paris traceroute [5].
Traceroutes from RIPE Atlas. The RIPE Atlas platform includes
small hardware probes hosted in thousands of networks around the
world. In April 2015, we issued traceroutes from Atlas probes in
approximately 1600 ASes around the world towards our cloud VMs
and a small number of popular websites.
Processing traceroutes to obtain AS paths. Our measurements
are IP-level traceroutes, but our analysis is over AS-level paths.
Challenges exist in converting IP-level paths to AS paths [32]; we
do not innovate on this front and simply adopt widely-used practices.
First, we remove any unresponsive hops, private IP addresses,
and IP addresses associated with Internet Exchange Points (IXPs).1
Next, we use a dataset from iPlane [29] to convert the remaining
IP addresses to the ASes that originate them, and we remove any
ASes that correspond to IXPs. If the iPlane data does not include an
AS mapping for an IP address, we insert an unknown AS indicator
into the AS path. We remove one or more unknown AS indicators
if the ASes on both sides of the unknown segment are the same,
or if a single unknown hop separates two known ASes. After we
apply these heuristics, we discard paths that still contains unknown
segments. We then merge adjacent ASes in a path if they are siblings
or belong to the same organization, using existing organization
lists [8], since these ASes are under shared administration.2
Finally, we exclude paths that do not reach the destination AS.
For our traceroutes from the cloud, we are left with paths to 3M of
the 3.8M /24 preﬁxes.
3.
INTERNET PATH LENGTHS
How long are Internet paths? In this section, we demonstrate that
the answer depends on the measurements used. We show that most
ﬂows from some popular web services to clients traverse at most
one inter-AS link (or one hop), whereas traditional measurement
datasets result in longer paths.
3.1 Measuring paths from the cloud
Paths from the cloud are short. As a baseline, we use our set
of traceroutes from PlanetLab to iPlane destinations, as traceroutes
from academic testbeds are commonly used in academic studies.
1We ﬁlter IXPs because they simply facilitate connectivity between
peers. We use two CAIDA supplementary lists [3, 21] to identify
ASes and IPs associated with IXPs.
2We compared the AS paths inferred by our approach with those
inferred by a state-of-the-art approach designed to exclude paths
with unclear AS translations [12], generating the results in our
paper using both approaches. The minor diﬀerences in the output
of the two approaches do not impact our results meaningfully, and
so we only present results from our approach.
524Paths from the cloud to end-users are even shorter. In order to
understand the paths between the cloud and end-users, we analyze
3M traceroutes from GCE to client preﬁxes in our CDN trace (§2).
We assume that, since these preﬁxes contain clients of one CDN,
most of them host end-users likely to use other large web services
like Google’s. As seen in Figure 1, 61% of the preﬁxes have one
hop paths from GCE, meaning their origin ASes peer directly with
Google, compared to 41% of the iPlane destinations.
Preﬁxes with more traﬃc have shorter paths. The preceding
analysis considers the distribution of AS hops across preﬁxes, but
the distribution across queries/requests/ﬂows/bytes may diﬀer, as
per preﬁx volumes vary. For example, in our CDN trace, the ratio
between the highest and lowest per preﬁx query volume is 8.7M:1.
To approximate the number of AS hops experienced by queries, the
GCE to end-users, weighted line in Figure 1 weights each of the 3M
preﬁxes by its query volume in our CDN trace (§2), with over 66%
of the queries coming from preﬁxes with a one hop path.
While our quantitative results would diﬀer with a trace from a
diﬀerent provider, we believe that qualitative diﬀerences between
high and low volume paths would hold. The dataset has limitations:
the trace is only one hour, so suﬀers time-of-day distortions, and
preﬁx weights are representative of the CDN’s client distribution
but not necessarily Google’s client distribution. However, the dataset
suﬃces for our purposes: precise ratios are not as important as the
trends of how paths with no/low traﬃc diﬀer from paths with high
traﬃc, and a preﬁx that originates many queries in this dataset is
more likely to host users generating many queries for other services.
Path lengths to a single AS can vary. We observed traceroutes
traversing paths of diﬀerent lengths to reach diﬀerent preﬁxes within
the same destination AS. Possible explanations for this include: (1)
traﬃc engineering by Google, the destination AS, or a party in
between; and (2) split ASes, which do not announce their entire
network at every peering, often due to lack of a backbone or a
capacity constraint. Of 17,905 ASes that had multiple traceroutes
in our dataset, 4876 ASes had paths with diﬀerent lengths. Those