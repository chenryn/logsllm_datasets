title:Who Are You? A Statistical Approach to Measuring User Authenticity
author:David Freeman and
Sakshi Jain and
Markus D&quot;urmuth and
Battista Biggio and
Giorgio Giacinto
Who Are You? A Statistical Approach to
Measuring User Authenticity
David Mandell Freeman
and Sakshi Jain
LinkedIn Corporation
{dfreeman,sjain2}@linkedin.com
Markus D¨urmuth
Ruhr-Universit¨at Bochum
PI:EMAIL
Battista Biggio
and Giorgio Giacinto
Universit`a di Cagliari
{battista.biggio,giacinto}@diee.unica.it
Abstract—Passwords are used for user authentication by
almost every Internet service today, despite a number of well-
known weaknesses. Numerous attempts to replace passwords
have failed, in part because changing users’ behavior has proven
to be difﬁcult. One approach to strengthening password-based
authentication without changing user experience is to classify
login attempts into normal and suspicious activity based on a
number of parameters such as source IP, geo-location, browser
conﬁguration, and time of day. For the suspicious attempts,
the service can then require additional veriﬁcation, e.g., by
an additional phone-based authentication step. Systems working
along these principles have been deployed by a number of Internet
services but have never been studied publicly. In this work, we
perform the ﬁrst public evaluation of a classiﬁcation system for
user authentication. In particular:
(i) We develop a statistical framework for identifying suspicious
login attempts.
(ii) We develop a fully functional prototype implementation that
can be evaluated efﬁciently on large datasets.
(iii) We validate our system on a sample of real-life login data
from LinkedIn as well as simulated attacks, and demonstrate
that a majority of attacks can be prevented by imposing
additional veriﬁcation steps on only a small fraction of users.
(iv) We provide a systematic study of possible attackers against
such a system, including attackers targeting the classiﬁer
itself.
I.
INTRODUCTION
Almost every Internet service today authenticates its users
using passwords: each user is associated with a short block
of text that is supposedly known only to that user. Advan-
tages to this system are that passwords are nearly universally
understood by users and that
they are well supported by
current infrastructures. However, passwords in practice have
numerous security ﬂaws that have been well documented in
both the research literature and the popular press: users choose
simple and/or easy-to-guess passwords; users reuse passwords
across services, meaning that a compromise of accounts on
one service leads to compromise of accounts on many other
Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’16, 21-24 February 2016, San Diego, CA, USA
Copyright 2016 Internet Society, ISBN 1-891562-41-X
http://dx.doi.org/10.14722/ndss.2016.23240
services; users are often tricked into revealing their passwords
to attackers (e.g., via “phishing”); and modern password-
cracking tools have become very powerful—the best has been
reported to guess up to 2.7 billion passwords per second on a
single GPU [52].
Many innovative techniques have been proposed to deal
with these problems, and several have been implemented in
practice. One common proposal is two-factor authentication,
by which the user must conﬁrm that she has possession of
another credential linked to the account. This second factor is
typically a hardware token, an authentication app (e.g. [30]),
or, with reduced security beneﬁts, a mobile phone number or
an email address. Most major websites (e.g. Google, Facebook,
LinkedIn, and Twitter) now offer a two-factor authentication
solution. However, two-factor authentication, being an opt-in
process, suffers from low adoption rates and does little to
thwart a large-scale attack on an Internet service.
Biometric authentication techniques, including ﬁngerprint
and face recognition [39], [34], and typing dynamics [28],
[41], [32], [19], have also been investigated as an alternative
to password-based authentication, but limited performance on
very large numbers of users and risks for privacy leaks have
actually slowed down its adoption in large online services.
Instead of replacing passwords, more recently there has
been signiﬁcant effort to increase the security of password-
based authentication. Examples include several methods for
increasing the entropy of users’ passwords [35], [33], [11] as
well as methods for discouraging reuse across websites [35],
[27]. Most of these proposals (discussed in more detail in
Sect. VII) require changing user behavior, and to date none
has achieved widespread adoption.
Given the difﬁculty of changing users’ behavior, in practice
one must assume that any password can easily fall into the
hands of an attacker. Many websites thus use a different
approach: to impose extra friction on authentication attempts
they determine to be suspicious. For example, between entering
a correct password and proceeding into the site, a service can
require a user to solve a CAPTCHA,1 verify an email address,
receive an SMS message, or answer security questions. For
maximum security a site could pose such “challenges” on
every login attempt; however, this level of friction would be
highly detrimental to the site’s level of engagement, as a large
1A CAPTCHA, i.e., a Completely Automated Public Turing test to tell
Computers and Humans Apart, is a challenge-response test used to verify
whether the user is a human being or a bot.
percentage of legitimate users will be unwilling or unable to
solve these challenges. There is thus a need to classify the
level of suspiciousness of any authentication attempt and only
block or challenge the most suspicious.
The classiﬁcation of login attempts (e.g., into three tiers
good/suspicious/bad) is derived from the data available at the
time of the login, including source IP address, geo-location,
operating system and browser conﬁguration,
the account’s
patterns of usage, and more. However, the details of these
derivations are generally not disclosed by the sites employing
such classiﬁers. Consequently, any public evaluation and com-
parison is missing. In addition, these defenses have not been
tested for their resistance to attacks targeting the classiﬁers
themselves, nor does there exist a systematic treatment of
attacks against such a login service. The lack of public
benchmarks, and even any baseline performance, makes it
difﬁcult to compare and evaluate systems.
Our contribution. In this work we provide the ﬁrst public
study of the classiﬁcation of login attempts, which we call
reinforced authentication. We advocate a “security by design”
approach, avoiding basing the security of the classiﬁer on the
secrecy of its design. This is important because recent work
on the behavior of classiﬁers in the presence of attacks (so-
called adversarial machine learning) has shown that the state
of a classiﬁer can be reconstructed by repeatedly querying the
classiﬁer, questioning the validity of an approach based on
“security by obscurity” [3], [5], [6], [31]. These results can
also be used to harden classiﬁers against these attacks, without
requiring secrecy of the design.
More concretely, our contributions are as follows:
• We develop a statistical framework for identifying sus-
picious authentication attempts. We begin with a likeli-
hood ratio test and show how the desired ratio can be
approximated in the presence of sparse data. Our main
formula (7) can be computed even in the absence of
labeled account takeover data. When such labeled data is
present, we can use the terms in the formula as features
in a logistic regression model to achieve greater accuracy.
• We develop a prototype implementation of the described
system. The main contribution here is smoothing tech-
niques to handle cases where there is no data, such
as when a user logs in from a previously unseen IP
address. Our solution supports the intuition that the level
of suspiciousness should increase as the user gets farther
away from a previously seen location. We demonstrate
that the proposed system is efﬁcient enough for a practical
large-scale deployment.
• We validate our proposal on a sample of real-life user
login data from LinkedIn. When using six months of
history and only two features (IP address and useragent),
we achieve AUC 0.96 and ﬁnd that the most suspicious
10% of login attempts include 89% of labeled account
takeover attempts.
• Inspired by recent work in the area of adversarial machine
learning [3], [5], [6], [31], we classify potential attacks
against such a classiﬁer, taking into account the speciﬁc
challenges of an authentication system. This classiﬁcation
allows us to assess our system’s security by hypothesizing
potential attack scenarios and simulating attacks. In a
number of experiments, we demonstrate the efﬁciency of
various feature combinations and evaluate the effective-
ness of attackers trying to evade the classiﬁer.
This work is not intended to replace alternative authen-
tication methods such as two-factor authentication, but
is
orthogonal and can be applied, in principle, to any kind of login
procedure. We expect it to be particularly helpful in securing
the large majority of accounts that are unable or unwilling to
use stronger authentication mechanisms.
II. REINFORCED AUTHENTICATION
The underlying idea behind reinforcing user authentication
is to exploit complementary information beyond the validation
of user credentials. This information can be extracted from
the HTTP session logs of authentication sessions established
between the user and the web server (including, e.g., the IP
address, useragent, timestamp, and cookies) and then compared
against data available from the user’s login history through a
carefully designed statistical machine-learning approach. The
principal result of this section is Eq. (7), which gives a scoring
function that can be computed using only per-user and global
login history and asset reputation systems; in particular, labeled
account compromise data is not required for the basic scoring
function.
Let us denote with u ∈ U a given user account, with
x = (x1, . . . , xd) ∈ X a d-dimensional set of feature values
characterizing a login attempt (e.g., timestamp, IP address,
browser, etc.), and with y ∈ Y = {L, A} the class label of
legitimate login (L) or attacks (A). In the sequel, uppercase
letters will be used to denote random variables (r.v.), and
lowercase letters to denote the corresponding realizations; for
example, if the r.v. X denotes the “IP address”, then x will
correspond to a speciﬁc IP address (e.g., 127.0.0.1). We assume
that for each account, login samples of either class are gener-
ated according to an underlying (though unknown) probability
distribution p(X, U, Y ), for which we are only given a set of
(i.i.d.) samples D = {xi, ui, yi}n
i=1 representing the available
login history for each user. We also assume that in all cases
the provided credentials are correct. If the provided credentials
are wrong, then the login attempt is rejected regardless of the
output of the reinforced authentication module.
Given this notation, reinforced authentication can be for-
mulated as the task of learning a classiﬁcation function f :
X × U (cid:55)→ Y that, for each set of features x and user u,
accurately predicts whether the corresponding login attempt is
a legitimate login, or an account takeover. For compactness,
we will denote this function as fu(x) ∈ {L, A}.
If the underlying distribution p were known, the decision
function yielding the minimum probability of wrong predic-
tions (i.e., minimizing generalization error or risk) would be
given by the so-called Maximum-A-Posteriori (MAP) crite-
rion:
p(Y = y|X = x, U = u) .
(1)
arg max
y∈Y
For two classes, the MAP criterion amounts to deciding for
2
Y = A if
p(Y = A|X = x, U = u)
p(Y = L|X = x, U = u)
> 1,
(2)
and for Y = L otherwise. Applying Bayes’ Theorem to the
numerator and denominator of (2) gives
p(Y = A|X = x, U = u)
p(Y = L|X = x, U = u)
,
(3)
and noting that the prior probabilities are not dependent on x
and u, the MAP criterion becomes
=
p(x, u|Y = A)
p(x, u|Y = L)
p(Y = A)
p(Y = L)
p(x, u|Y = A)
p(x, u|Y = L)
(cid:123)(cid:122)
(cid:125)
(cid:124)
gu(x)
≶ p(Y = L)
p(Y = A)
(cid:124)
(cid:123)(cid:122)
θ
(cid:125)
;
(4)
is, decide for class Y = A if gu(x) > θ, and for
that
Y = L otherwise. The threshold θ can be generally adjusted
on a validation set to balance the trade-off between the rate
of misclassiﬁed legitimate logins (false positives, FP) and the
attack detection rate (true positives, TP). This rule is is widely-
known in biometric identity veriﬁcation as likelihood ratio,
or Neyman-Pearson criterion [43]. According to the Neyman-
Pearson lemma, for a given FP rate, one can select θ such that
the likelihood ratio test maximizes the detection rate (TP).
If the probability distributions p(A|x, u) and p(L|x, u) are
exactly known, this rule is optimal, i.e., it yields the maximum
detection rate (and no other test outperforms this one in terms
of statistical power). However, in practice the aforementioned
probability distributions are not known, and they have to be
estimated from the available data D. The performance of
the likelihood ratio test will thus depend on how well such
distributions are estimated.
Finally, if the conﬁdence score gu(x) is too close to the
threshold, i.e., the prediction is not very conﬁdent, then one
may consider requesting additional information from the user
to validate the login. For example, we may request the user to
verify a phone number or email address or to provide some
personal information that can be validated by the system (e.g.,