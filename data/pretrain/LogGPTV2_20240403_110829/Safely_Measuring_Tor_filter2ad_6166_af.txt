2006.
[13] DWORK, C., MCSHERRY, F., NISSIM, K., AND SMITH,
A. Calibrating noise to sensitivity in private data
analysis. In Theory of Cryptography Conference (2006).
[14] ELAHI, T., DANEZIS, G., AND GOLDBERG, I. Privex:
Private collection of trafﬁc statistics for anonymous
communication networks. In ACM Conference on
Computer and Communications Security (2014).
[15] ERLINGSSON, U., PIHUR, V., AND KOROLOVA, A.
Rappor: Randomized aggregatable privacy-preserving
ordinal response. In ACM Conference on Computer and
Communications Security (2014).
[16] GOULET, D., JOHNSON, A., KADIANAKIS, G., AND
LOESING, K. Hidden-service statistics reported by
relays. Tech. rep., The Tor Project, Inc., April 2015.
[17] HARDT, M., AND ROTH, A. Beating randomized
response on incoherent matrices. In ACM Symposium on
Theory of Computing (2012).
[18] JANSEN, R., BAUER, K., HOPPER, N., AND
DINGLEDINE, R. Methodically modeling the Tor
network. In USENIX Conference on Cyber Security
Experimentation and Test (2012).
[19] JANSEN, R., AND HOPPER, N. Shadow: Running Tor in
a box for accurate and efﬁcient experimentation. In
Network and Distributed System Security Symposium
(2012).
[20] KADIANAKIS, G., AND LOESING, K. Extrapolating
network totals from hidden-service statistics. Tech.
rep., The Tor Project, January 2015.
[21] LINDELL, Y. Composition of Secure Multi-Party Protocols,
A Comprehensive Study. Lecture Notes in Computer
Science. Springer, 2003.
[22] LOESING, K., MURDOCH, S. J., AND DINGLEDINE, R.
A case study on measuring statistical data in the Tor
anonymity network. In Financial Cryptograpy and Data
Security (2010).
[23] MCCOY, D., BAUER, K., GRUNWALD, D., KOHNO, T.,
AND SICKER, D. Shining light in dark places:
Understanding the Tor network. In Privacy Enhancing
Technologies Symposium (2008).
[24] MCSHERRY, F., AND MIRONOV, I. Differentially private
recommender systems: building privacy into the net. In
ACM knowledge discovery and data mining (KDD) (2009).
[25] MELIS, L., DANEZIS, G., AND CRISTOFARO, E. D.
Efﬁcient private statistics with succinct sketches. In
Network and Distributed System Security Symposium
(2015).
[26] OWEN, G., AND SAVAGE, N. Empirical analysis of Tor
hidden services. IET Information Security (2016).
[27] SHIVA PRASAD KASIVISWANATHAN, A. S. On the
‘semantics’ of differential privacy: A bayesian
formulation. J. of Privacy and Conﬁdentiality 6, 1 (2014).
[28] SOGHOIAN, C. Enforced community standards for
research on users of the Tor anonymity network. In
Workshop on Ethics in Computer Security Research (2011).
APPENDIX
A. DETERMINING OPTIMAL NOISE
Given  and δ for the statistics, PrivCount allocates  and
δ across the statistics by choosing k and δk for each statis-
tic sk. Then, given k and δk, it computes a value σk such
that the Gaussian mechanism [12] with standard deviation
σk provides k-differential privacy with probability 1 − δk.
This guarantees (, δ)-differential privacy across all statistics.
We ﬁrst give an algorithm to compute σk, next give a coun-
terexample showing that the formula of Elahi et al. [14] (Sec-
tion 4.4.1) does not guarantee (, δ)-differential privacy, and
ﬁnally give an algorithm to allocate  to minimize across all
statistics the maximum ratio of σk to the estimated value of
the statistic vk.
A.1 Computing σ∗(, δ)
Simple formulas to determine σ given  and δ are pre-
sented for the single-dimensional case by Dwork et al. [12]
(Section 2.1) and for the multi-dimensional case by Hardt
and Roth [17] (Theorem 2.6). However, for the single-dimensional
Gaussian mechanism, we can slightly improve upon this while
still providing (, δ)-differential privacy across all statistics.
Let D be the set of possible inputs among which we have
deﬁned adjacency (see Section 2.3 for a discussion of adja-
cency). Let D ∼ D(cid:48) indicate that inputs D, D(cid:48) ∈ D are ad-
jacent. Let mechanism M : D → R be a randomized al-
gorithm with some output space R. The deﬁnition of (, δ)-
differential privacy is as follows:
DEFINITION 1. M satisﬁes (, δ)-differential privacy if, for
all adjacent inputs D and D(cid:48) and all S ⊆ R,
Pr[M(D) ∈ S] ≤ ePr[M(D(cid:48)) ∈ S] + δ.
Let f : D → R be some query. The Gaussian mechanism
depends on the sensitivity of f , which is deﬁned as follows:
DEFINITION 2. The sensitivity of f is
∆ f = max
D∼D(cid:48):
D,D(cid:48)∈D
| f (D) − f (D(cid:48))|.
To deﬁne the Gaussian mechanism formally, recall from
Section 2.2 that Normal(µ, σ) denotes the normal distribution
with mean µ and standard deviation σ. Given ∆ f , the Gaus-
sian mechanism using the bound for σ of Dwork et al. [12] is
as follows:
DEFINITION 3. Let σ(, δ) = ∆ f −1(cid:112)2 ln(2/δ). Let N ∼
Normal(0, σ(, δ)). The Gaussian mechanism G : R → R is
G(x) = f (x) + N.
G satisﬁes (, δ)-differential privacy for , δ ≤ 1 [12].
Recall (Section 2.2) that φ(µ, σ; x) denotes the probability
density function of the normal distribution with mean µ and
standard deviation σ. Similarly, let Φ(µ, σ; x) denote the cu-
mulative distribution function of the normal distribution. The
value σ(, δ) is chosen such that the set of values x such that
φ(0, σ; x) ≤ eφ(∆ f , σ; x) has probability at least 1 − δ. We
simply performs binary search in (0, ∆ f −1(cid:112)2 ln(2/δ)] for
can reduce the noise variance somewhat by ﬁnding the small-
est value σ∗ with this property. To compute σ∗, PrivCount
the smallest value σ such that Φ(0, σ; x∗) ≤ δ, where x∗ =
−(σ∗2/∆ f ) + ∆ f /2, which is the smallest value x satisfying
φ(0, σ; x)/φ(∆ f , σ; x) ≤ e.
A.2 Counterexample for PrivEx σ
We show that Elahi et al. provide an invalid formula relat-
ing σ, , and δ (see [14], Section 4.4.1). They state that, after
determining a σ that provides the desired limit on the adver-
sary’s “advantage”, (, δ)-differential privacy holds for any 
and δ such that the following is satisﬁed:
(cid:115)
(cid:18) 1.25
(cid:19)
.
δ
ln
∆ f

σ ≥
However, the following counterexample shows that this is
incorrect.
Suppose that  = 0.2, δ = 10−6, and ∆ f = 1. Then set
(cid:115)
(cid:18) 1.25
(cid:19)
10−6
ln
≈ 18.734.
σ =
1
0.2
Then let
x∗ = −
σ2
∆ f
+
∆ f
2 ≈ −69.693.
Then Φ(0, σ; x∗) = 9.956 · 10−5, and Φ(∆ f , σ; x∗) = 8.048 ·
10−5. Thus, Φ(0, σ; x∗) − eΦ(∆ f , σ; x∗) = 1.257 · 10−6 > δ,
which violates (, δ)-differential privacy.
A.3 Optimally allocating 
Given δk = δ/l for all l, PrivCount divides  among the
statistics in order to make their noise proportional to their
expected values vk. More precisely, it chooses k > 0 mini-
mizing maxk √nσk/vk, where σk = σ∗(k, δk) and ∑k k = .
This is equivalent to ﬁnding some global noise ratio ρ and
k such that √nσk/vk = ρ, for all k, because otherwise the
maximum ratio could be decreased be increasing the k of
the maximizing statistic and decreasing by the same amount
the k of some statistic with a smaller noise ratio.
Given some global ratio ρ, it must be that σk = ρvk/√n.
Then each k can be computed from σk and δk via binary
search in a process very similar to that used to calculate σk
from k and δk. Such ρ is feasible if and only if ∑k k ≤ .
Thus we can ﬁnd the optimal allocation for  by performing
a binary search for the smallest ratio ρ that is feasible. Let ρ∗
be this smallest ratio.
To determine a range in which to search for ρ∗, we begin
by computing an optimal allocation using the upper bound
of the values σk from Dwork et al. [12]:
(cid:113)
σ(cid:48)(k, δk) = ∆k−1
k
2 ln(2/δk),
where ∆k is the sensitivity of statistic sk. We can solve for the
optimal allocation (cid:48)k using σ(cid:48) for the σk by setting the noise
ratios all equal:
∀k
∆1
⇔ ∀k
√nσ(cid:48)((cid:48)1, δ1)
(cid:112)2 ln(2/δ1)
v1
(cid:48)1v1
⇔ ∀k
∆1
(cid:48)1v1
=
=
=
⇔ ∀k(cid:48)k =
⇒ (cid:48)1 =
√nσ(cid:48)((cid:48)k, δk)
(cid:112)2 ln(2/δk)
∆k
vk
(cid:48)kvk
∆k
(cid:48)kvk
∆kv1(cid:48)1
∆1vk

1 + ∑k>1(∆kv1)/(∆1vk)
,
(cid:34)
where the last line holds because ∑k (cid:48)k = .
Given this allocation, we can be certain that
ρ∗ ∈
min
k
√nσ∗((cid:48)k, δk)
vk
, max
k
√nσ∗((cid:48)k, δk)
vk
(cid:35)
,
which is true for any allocation of  because adjusting the
given allocation in order to equalize the noise ratios can only
decrease the maximum noise ratio and increase the mini-
mum noise ratio. Using the σ(cid:48) approximation as a basis for
determining this range is particularly good to the extent that
σ(cid:48) is generally close to the optimal σ∗. In fact, if σ(cid:48) ≤ cσ∗
for some c ≥ 1 (recall that we already have that σ∗ ≤ σ(cid:48)),
then computing ρ∗ to within some accuracy ι takes at most
log2((1 − 1/c)ρ∗/ι) rounds during the binary search. To see
this, let ρ(cid:48) = (√nσ(cid:48)((cid:48)k, δk))/vk, which is well-deﬁned be-
cause the (cid:48)k are set such that this ratio is equal for all k. Then
observe that
√nσ∗((cid:48)k, δk)
vk
max
k
≤ ρ(cid:48)
because σ∗ ≤ σ(cid:48) and that
√nσ∗((cid:48)k, δk)
vk
min
k
≥ ρ(cid:48)/c
because σ∗ ≥ σ(cid:48)/c. Finally, observe that ρ(cid:48) − ρ(cid:48)/c ≥ ρ∗ −
ρ∗/c because ρ(cid:48) ≥ ρ∗.
B. SECURITY AND PRIVACY PROOFS
model against any adversary that does not corrupt all SKs.
THEOREM 1. PrivCount UC-realizes FPC in the hybrid PKI
PROOF. The simulator runs the PrivCount protocol inter-
nally with the given adversary A. For honest parties, it re-
ceive the inputs up to the execution phase, and so it can sim-
ulate PrivCount in the ﬁrst two phases. The PKI ensures au-
thenticity of all messages in both the real and simulated pro-
tocols.
During broadcast, an abort message corresponds to a party
receiving an inconsistent “rebroadcast” from another party.
This event can only occur with a corrupt TS, and so the sim-
ulator can always observe it and can translate the event be-
tween the simulated protocol and the ideal world in either
direction.
During aggregation, for each honest DC or SK Pi the sim-
ulator can use as counter values the initialization values that
it sampled for the DCs. It can also use shared values created
at honest DCs and SKs.
If the TS is corrupt, then after the last honest party Pi leaves
execution, the simulator will receive the aggregated output
of honest DCs y. It can than send as the ﬁnal value to the
TS from Pi to be to be value left after subtracting from y the
sum of the initial counter values of all honest DCs. This is
indistinguishable from a real PrivCount execution from the
view of the adversary because (i) in both cases each value
from a DC is independent and uniformly random by virtue
of its shared value with any given one of the honest SKs, say,
Pi; (ii) the value from each honest SK except Pi is independent
and uniformly random by virtue of its shared value with any
honest DC, and (iii) Pi is the one value such that the ﬁnal sum
is y plus any values shared between honest DCs and corrupt
SKs and between honest SKs and corrupt DCs.
If the TS is not corrupt, then the simulator can extract the
change in counter value ∆k,b that adversary claims in its val-
ues to the TS by subtracting from the sum of ﬁnal values
from corrupt DCs and SKs the sum of the values shared be-
tween corrupt DCs and honest SKs and the initial values of
the counters of corrupt DCs. Then the output value from the
TS is the same in the real and ideal worlds.
Finally, suppose that a corrupt TS sends a subset Si that
doesn’t contain a minimal DC set. Then the simulator re-
ceives no output from aggregation (just ⊥), but it needs none
because some SK never provides a ﬁnal value to the TS. If
instead a corrupt TS sends as inputs to two honest SKs sub-
sets Si that contain different honest DCs, then the simulator
can send random ﬁnal values to the TS from all DCs and SKs,
because in the real PrivCount execution such subsets would
also result in uniformly random ﬁnal values as each SK con-
tains a shared value not included in any other sum.
THEOREM 2. If at least one SK Si is honest, and if, for each
minimal subset S of DCs in the deployment document that Si re-
Di∈H w2
ceives with honest subset H ⊆ S,
i ≥ 1, then the
output of PrivCount is (, δ)-differentially private.
∑
(cid:113)
∑
(cid:113)
Di∈H w2
PROOF. Because we have an honest SK, we can apply Thm. 1
and examine the output of FPC. That functionality outputs
to the environment or the adversary either something uncor-
related with the inputs or the sum of the initialized coun-
ters. The latter output has noise with standard deviation at
i σk ≥ σk in each counter. The values σk are
least
calculated such that for each k the set of values x such that
φ(0, σk; x) ≤ ek φ(∆k, σk; x) has probability at least 1 − δk,
where ∆k is the sensitivity of the kth statistic. Thus the set of
outputs x = (x1, . . . , xl) such that φ(0, σk; xk) > ek φ(∆k, σk; xk)
for some k has probability at most ∑k δk = δ. For all other
output vectors x, φ(0, σk; xk) ≤ ek φ(∆k, σk; xk) for all k, and
so the set of such vectors S has probability under any input
D at most e∑k k = e times the probability of S under an ad-
jacent input D(cid:48). Thus, the set of output statistics from FPC
in a given collection period is collectively (, δ)-differentially
private. Moreover, it is sufﬁcient just to show that the func-
tionality output from a single collection period is differen-
tially private because adjacent inputs databases only differ
in behavior during some period no longer than the reconﬁg-
uration time, and the functionality will not change its con-
ﬁguration and start new measurements without waiting for
that length of time.