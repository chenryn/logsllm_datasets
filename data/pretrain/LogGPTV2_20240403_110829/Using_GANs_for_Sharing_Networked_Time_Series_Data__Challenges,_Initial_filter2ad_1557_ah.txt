D1,D2
G
(cid:104)(cid:0)∥∇^x Di(Ti(ˆx))∥2 − 1(cid:1) 2(cid:105). Here T1(x) = x and T2(x) =
where Li, i ∈ {1, 2} is the Wasserstein loss of the original and sec-
ond discriminator, respectively: Li = Ex ∼px [Ti(Di(x))]−Ez∼pz [Di(Ti(G(z)))]−
λE^x∼p ^x
metadata part of x. ˆx := tx + (1 − t)G(z) where t ∼ Unif[0, 1].
Empirically, we find that α = 1 is enough for getting good fidelity
on metadata. As with all GANs, the generator and discriminators
are trained alternatively until convergence. Unlike naive GAN archi-
tectures, we did not observe problems with training instability, and
on our datasets, convergence required only up to 200,000 batches
(400 epochs when the number of training samples is 50,000).
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Metadata
Description
Possible Values
zh.wikipedia.org,
com-
mons.wikimedia.org,
etc.
The main
do-
main name of the
Wikipedia page
The access method mobile-web, desk-
top, all-access, etc.
spider,
all-agent,
etc.
Possible Values
The agent type
of
integers
Wikipedia
main
do-
access type
agent
Measurements
views
Description
number
The
views
Timestamp Discription
The date that the page view is counted
on
Possible Values
2015-07-01, etc.
Table 7: Schema of WWT dataset
Metadata
Description
Possible Values
technology
ISP
state
cable, fiber, etc.
connection
The
technology of the
unit
Internet
provider of the unit AT&T, Verizon, etc.
The state where the
unit is located
PA, CA, etc.
service
Measurements
Description
Possible Values
ping loss rate
traffic
counter
byte
UDP ping loss rate
to the server that
has lowest loss rate
within the hour
Total number of
bytes
and
the
received
(excluding
hour
due
the
to
activate
measurements)
traffic
the
sent
in
float numbers
integers
Timestamp Discription
The time of the measurement hour
Possible Values
2015-09-01 1:00, etc.
Table 8: Schema of MBA dataset
Generation flag for variable length: Time series may have different
lengths. For example, in GCUT dataset, different jobs have different
duration (Figure 9). We want to learn to generate sequences of
the right length organically (without requiring the user to specify
it). The simplest solution to pad all time series with 0 to the same
length. However, that would introduce a confusion on whether a
zero value means the measurements (say, daily page view) is zero,
or it is actually a padding token. Therefore, along with the origi-
nal measurements, we add generation flag to each time step: [1, 0]
481
Figure 17: Generation flag for enabling variable length se-
quence.
if the time series does not end at this time step, and [0, 1] if the
time series ends exactly at this time step (Figure 17). The generator
outputs generation flag [p1, p2] through a softmax output layer,
so that p1, p2 ∈ [0, 1] and p1 + p2 = 1. [p1, p2] is used to deter-
mine whether we should continue unrolling the RNN to the next
time step. One way to interpret this is that p1 gives the probability
that the time series should continue at this time step. Therefore, if
p1  p2, we continue unrolling the RNN to generate mea-
surements for the next time step(s). The generation flags are also
fed to the discriminator as part of the features, so the generator can
also learn sample length characteristics.
We also want to highlight that if the user wants to control the
length of the generated samples, our architecture can also support
this by iterating the RNN generator for the given desired number
of steps.
AR: We used p = 3, i.e., used the past three samples to predict
the next. The AR model was an MLP with 4 hidden layers and 200
units in each layer. The MLP was trained using Adam optimizer
[66] with learning rate of 0.001 and batch size of 100.
RNN: For this baseline, we used LSTM (Long short term memory)
[58] variant of RNN. It is 1 layers of LSTM with 100 units. The
network was trained using Adam optimizer with learning rate of
0.001 and batch size of 100.
Naive GAN: The generator and discriminator are MLPs with 4
hidden layers and 200 units in each layer. Gradient penalty weight
was 10.0 as suggested in [50]. The network was trained using Adam
optimizer with learning rate of 0.001 and batch size of 100 for both
generator and discriminator.
C ADDITIONAL FIDELITY RESULTS
Temporal length: Figure 18 shows the length distribution of
DG and baselines in GCUT dataset. It is clear that DG has the best
fidelity.
Metadata distribution: Figure 19 shows the histogram of Wikipedia
domain of Naive GAN and DG. DG learns the distribution fairly
well, whereas naive GAN cannot.
Measurement -metadata correlations: We compute the CDF of
total bandwidth for DSL and cable users in MBA dataset. Figures
20(a) and 20(b) plot the full CDFs. Most of the baselines capture
the fact that cable users consume more bandwidth than DSL users.
However, DG appears to excel in regions of the distribution with less
data, e.g., very small bandwidth levels. In both cases, DG captures
the bandwidth distribution better than the other baselines. This
0.80.21.000…0010000R1R2R3R4R5OriginalmeasurementsGenerationflagPaddings0.20.00.900110Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
(a)
(b)
F
D
C
Bandwidth (GB)
Figure 20: Total bandwidth usage in 2 weeks in MBA dataset
for (a) DSL and (b) cable users.
Generated sample 1st nearest
(a)
2nd nearest
3rd nearest
(b)
(c)
Figure 21: Three time series samples selected uniformly
at random from the synthetic dataset generated using
DG and the corresponding top-3 nearest neighbours (based
on square error) from the real WWT dataset. The time series
shown here is daily page views (normalized).
Generated sample 1st nearest
(a)
2nd nearest
3rd nearest
(b)
(c)
Figure 22: Three time series samples selected uniformly
at random from the synthetic dataset generated using
DG and the corresponding top-3 nearest neighbours (based
on square error) from the real GCUT dataset. The time series
shown here is CPU rate (normalized).
DG does not simply memorize training samples: Figure 21,
22, 23 show the some generated samples from DG and their nearest
(based on squared error) samples in training data from the three
datasets. The results show that DG is not memorizing training
samples. To achieve the good fidelity results we have shown before,
DG must indeed learn the underlying structure of the samples.
t
n
u
o
C
Task duration (seconds)
Figure 18: Histogram of task duration for the GCUT dataset.
DoppelGANger gives the best fidelity.
t
n
u
o
C
Wikipedia domain
Figure 19: Histograms of Wikipedia domain from
WWT dataset.
means that DG has a high fidelity on measurement distribution,
and also successfully capture how metadata (i.e., DSL and cable)
influence the measurements.
482
010203040500500010000RealDoppelGANger010203040500500010000RealHMM01020304050050001000015000RealAR01020304050050001000015000RealRNN010203040500500010000RealNaive GANcommons.wikimedia.orgde.wikipedia.orgen.wikipedia.orges.wikipedia.orgfr.wikipedia.orgja.wikipedia.orgru.wikipedia.orgwww.mediawiki.orgzh.wikipedia.org02000400060008000RealDoppelGANgercommons.wikimedia.orgde.wikipedia.orgen.wikipedia.orges.wikipedia.orgfr.wikipedia.orgja.wikipedia.orgru.wikipedia.orgwww.mediawiki.orgzh.wikipedia.org05000100001500020000RealNaive GAN01020304050600.00.20.40.60.81.0RealARRNNHMMDoppelGANgerNaive GAN01020304050600.00.20.40.60.81.0RealARRNNHMMDoppelGANgerNaive GAN0100200300400500−1.0−0.9−0.8−0.7−0.60100200300400500−1.0−0.9−0.8−0.7−0.60100200300400500−1.0−0.9−0.8−0.7−0.60100200300400500−1.0−0.9−0.8−0.7−0.60100200300400500−0.6−0.4−0.20.00100200300400500−0.6−0.4−0.20.00100200300400500−0.6−0.4−0.20.00100200300400500−0.6−0.4−0.20.001002003004005000.50.60.701002003004005000.50.60.701002003004005000.50.60.701002003004005000.50.60.7012345670.000.020.040.06012345670.000.020.040.06012345670.000.020.040.06012345670.000.020.040.060.000.250.500.751.001.251.501.752.000.000.020.040.060.080.000.250.500.751.001.251.501.752.000.000.020.040.060.080.000.250.500.751.001.251.501.752.000.000.020.040.060.080.000.250.500.751.001.251.501.752.000.000.020.040.060.08012345670.00.10.20.3012345670.00.10.20.3012345670.00.10.20.3012345670.00.10.20.3IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Generated sample 1st nearest
(a)
2nd nearest
3rd nearest
(b)
(c)
Figure 23: Three time series samples selected uniformly
at random from the synthetic dataset generated using
DG and the corresponding top-3 nearest neighbours (based
on square error) from the real MBA dataset. The time series
shown here is traffic byte counter (normalized).
Figure 24: Coefficient of determination for WWT time series
forecasting. Higher is better.
D ADDITIONAL CASE STUDY RESULTS
Predictive modeling: For the WWT dataset, the predictive mod-
eling task involves forecasting of the page views for next 50 days,
given those for the first 500 days. We want to learn a (relatively)
parsimonious model that can take an arbitrary length-500 time
series as input and predict the next 50 time steps. For this purpose,
we train various regression models: an MLP with five hidden layers
(200 nodes each), and MLP with just one hidden layer (100 nodes),
a linear regression model, and a Kernel regression model using an
RBF kernel. To evaluate each model, we compute the so-called coef-
ficient of determination, R2, which captures how well a regression
model describes a particular dataset.11
Figure 24 shows the R2 for each of these models for each of our
generative models and the real data. Here we train each regression
model on generated data (B) and test it on real data (A’), hence
it is to be expected that real data performs best. It is clear that
DG performs better than other baselines for all regression models.
Note that sometimes RNN, AR, and naive GANs baselines have
large negative R2 which are therefore not visualized in this plot.
Algorithm comparison: Figure 25, 26 show the ranking of predic-
tion algorithms on DG’s and baselines’ generated data. Combined
with 5, we see that DG and AR are the best for preserving ranking
of prediction algorithms.
f (x), R 2 is defined as R 2 = 1 − i (yi −f (xi ))2
i (yi − ¯y)2
11For a time series with points (xi, yi) for i = 1, . . . , n and a regression function
i yi is the mean
y-value. Notice that −∞ ≤ R 2 ≤ 1, and a higher score indicates a better fit.

where ¯y = 1
n
Figure 25: Ranking of end event type prediction algorithms
on GCUT dataset.
Figure 26: Ranking of traffic prediction algorithms on WWT
dataset.
483
010203040500.00.10.20.3010203040500.00.10.20.3010203040500.00.10.20.3010203040500.00.10.20.3010203040500.0000.0250.0500.0750.100010203040500.0000.0250.0500.0750.100010203040500.0000.0250.0500.0750.100010203040500.0000.0250.0500.0750.100010203040500.050.100.15010203040500.050.100.15010203040500.050.100.15010203040500.050.100.15KernelRidgeLinearRegressionMLP (1 layer)MLP (5 layers)0.000.250.500.751.00real dataDoppelGANgerRNNARHMMNaive GANreal dataDoppelGANgerRNNARHMMNaive GAN0.000.250.500.751.00MLPNaiveBayesLogisticRegr.DecisionTreeLinearSVMreal dataDoppelGANgerRNNARHMMNaive GAN0.000.250.500.751.00MLP (5 layers)LinearRegr.KernelRidgeMLP (1 layer)