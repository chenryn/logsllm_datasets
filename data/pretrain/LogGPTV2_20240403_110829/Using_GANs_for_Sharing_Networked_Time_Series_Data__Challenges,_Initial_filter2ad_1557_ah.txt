### D1, D2 and G

The objective function for the discriminators \(D_1\) and \(D_2\) is given by:
\[
\mathbb{E}_{\hat{x}} \left[ \left( \| \nabla_{\hat{x}} D_i(T_i(\hat{x})) \|^2 - 1 \right)^2 \right]
\]
where \(T_1(x) = x\) and \(T_2(x)\) are transformations. Here, \(L_i\) for \(i \in \{1, 2\}\) represents the Wasserstein loss for the original and second discriminator, respectively:
\[
L_i = \mathbb{E}_{x \sim p_x} [T_i(D_i(x))] - \mathbb{E}_{z \sim p_z} [D_i(T_i(G(z)))] - \lambda \mathbb{E}_{\hat{x} \sim p_{\hat{x}}} [\| \nabla_{\hat{x}} D_i(T_i(\hat{x})) \|^2 - 1]^2
\]
where \(\hat{x} := t x + (1 - t) G(z)\) with \(t \sim \text{Unif}[0, 1]\). Empirically, we find that setting \(\alpha = 1\) is sufficient to achieve good fidelity on metadata. As with all GANs, the generator and discriminators are trained alternately until convergence. Unlike naive GAN architectures, we did not observe issues with training instability, and on our datasets, convergence required only up to 200,000 batches (400 epochs when the number of training samples is 50,000).

### Metadata Schemas

#### WWT Dataset Schema
- **Metadata**
  - **Description**: The main domain name of the Wikipedia page.
  - **Possible Values**: `zh.wikipedia.org`, `commons.wikimedia.org`, etc.
- **Access Type**
  - **Description**: The access method.
  - **Possible Values**: `mobile-web`, `desktop`, `all-access`, etc.
- **Agent**
  - **Description**: The agent type.
  - **Possible Values**: `spider`, `all-agent`, etc.
- **Measurements**
  - **Views**
    - **Description**: The number of views.
  - **Timestamp**
    - **Description**: The date that the page view is counted on.
    - **Possible Values**: `2015-07-01`, etc.

#### MBA Dataset Schema
- **Metadata**
  - **Technology**
    - **Description**: The technology of the Internet connection.
    - **Possible Values**: `cable`, `fiber`, etc.
  - **ISP**
    - **Description**: The Internet service provider.
    - **Possible Values**: `AT&T`, `Verizon`, etc.
  - **State**
    - **Description**: The state where the unit is located.
    - **Possible Values**: `PA`, `CA`, etc.
- **Measurements**
  - **Ping Loss Rate**
    - **Description**: UDP ping loss rate to the server with the lowest loss rate within the hour.
  - **Traffic Counter**
    - **Description**: Total number of bytes received and sent in the hour (excluding activations).
  - **Timestamp**
    - **Description**: The time of the measurement hour.
    - **Possible Values**: `2015-09-01 1:00`, etc.

### Generation Flag for Variable Length Time Series

Time series may have different lengths. For example, in the GCUT dataset, different jobs have different durations (Figure 9). We aim to learn to generate sequences of the right length organically without requiring the user to specify it. A simple solution is to pad all time series with zeros to the same length. However, this introduces ambiguity about whether a zero value represents an actual measurement or a padding token. Therefore, along with the original measurements, we add a generation flag to each time step:
- `[1, 0]` if the time series does not end at this time step.
- `[0, 1]` if the time series ends exactly at this time step (Figure 17).

The generator outputs the generation flag \([p_1, p_2]\) through a softmax output layer, ensuring \(p_1, p_2 \in [0, 1]\) and \(p_1 + p_2 = 1\). This flag is used to determine whether to continue unrolling the RNN to the next time step. If \(p_1 > p_2\), the RNN continues to generate measurements for the next time step(s). The generation flags are also fed to the discriminator as part of the features, allowing the generator to learn sample length characteristics.

If the user wants to control the length of the generated samples, our architecture can support this by iterating the RNN generator for the desired number of steps.

### Baseline Models

- **AR (AutoRegressive Model)**
  - **Configuration**: Used \(p = 3\) (past three samples to predict the next).
  - **Model**: MLP with 4 hidden layers and 200 units in each layer.
  - **Training**: Adam optimizer with learning rate 0.001 and batch size 100.

- **RNN (Recurrent Neural Network)**
  - **Configuration**: LSTM variant with 1 layer and 100 units.
  - **Training**: Adam optimizer with learning rate 0.001 and batch size 100.

- **Naive GAN**
  - **Generator and Discriminator**: MLPs with 4 hidden layers and 200 units in each layer.
  - **Gradient Penalty Weight**: 10.0.
  - **Training**: Adam optimizer with learning rate 0.001 and batch size 100 for both generator and discriminator.

### Additional Fidelity Results

- **Temporal Length**: Figure 18 shows the length distribution of DG and baselines in the GCUT dataset, indicating that DG has the best fidelity.
- **Metadata Distribution**: Figure 19 shows the histogram of the Wikipedia domain for Naive GAN and DG. DG learns the distribution well, whereas Naive GAN cannot.
- **Measurement-Metadata Correlations**: Figures 20(a) and 20(b) plot the CDFs of total bandwidth for DSL and cable users in the MBA dataset. DG captures the bandwidth distribution better than other baselines, especially in regions with less data.

### Generated Samples

- **Figures 21, 22, 23**: Show some generated samples from DG and their nearest neighbors (based on square error) in the real datasets. The results indicate that DG does not simply memorize training samples but learns the underlying structure.

### Additional Case Study Results

- **Predictive Modeling**: For the WWT dataset, the task involves forecasting page views for the next 50 days given the first 500 days. Various regression models were trained, including MLPs, linear regression, and kernel regression. Figure 24 shows the coefficient of determination (\(R^2\)) for each model, indicating that DG performs better than other baselines.
- **Algorithm Comparison**: Figures 25 and 26 show the ranking of prediction algorithms on DG's and baselines' generated data, indicating that DG and AR are the best for preserving the ranking of prediction algorithms.