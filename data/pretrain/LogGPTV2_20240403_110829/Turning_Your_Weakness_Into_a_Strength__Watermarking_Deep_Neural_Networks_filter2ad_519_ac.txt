ﬁed correctly. The goal in the proof is to show that A
achieves this independently of vk. In a ﬁrst step, we will
use a hybrid argument to show that A essentially works
independent of vk. Therefore, we construct a series of
algorithms where we gradually replace the backdoor el-
ements in vk. First, consider the following algorithm S:
1. Compute (M, ˆM, mk, vk) ← MModel().
2. Sample ( ˜T , ˜TL) = ˜b ← SampleBackdoor(O f )
L }.
L , . . . , ˜T (n)
where ˜T = {˜t(1), . . . , ˜t(n)} and ˜TL = { ˜T (1)
Now set
t ← Com(˜t(1),r(1)
c(1)
L }i∈[n]
,c(i)
and ˜vk ← {c(i)
t
t
),c(1)
L ← Com( ˜T (1)
L ,r(1)
L )
3. Compute ˜M ← A(O f , ˆM, ˜vk).
This algorithm replaces the ﬁrst element in a veriﬁca-
tion key with an element from an independently gener-
ated backdoor, and then runs A on it.
In S we only exchange one commitment when com-
pared to the input distribution to A from the secu-
rity game. By the statistical hiding of Com, the out-
put of S must be distributed statistically close to the
output of A in the unremovability experiment. Apply-
ing this repeatedly, we construct a sequence of hybrids
S (1),S (2), . . . ,S (n) that change 1,2, . . . ,n of the elements
from vk in the same way that S does and conclude that
˜M without the water-
the success of outputting a model
mark using A must be independent of vk.
Consider the following algorithm T when given a
model M with a strong backdoor:
1. Compute (mk, vk) ← KeyGen().
2. Run the adversary and compute ˜N ← A(O f ,M, vk).
By the hybrid argument above, the algorithm T runs
nearly in the same time as A, namely t, and its output
˜N will be without the backdoor that M contained. But
then, by persistence of strong backdooring, T must also
generate ε-accurate models given arbitrary, in particular
bad input models M in the same time t, which contradicts
our assumption that no such algorithm exists.
Unforgeability. Assume that there exists a poly-time
algorithm A that can break unforgeability. We will use
this algorithm to open a statistically hiding commitment.
USENIX Association
27th USENIX Security Symposium    1621
(cid:40)c
t ←
ˆc(i)
c(i)
t
L }i∈[n].
,c(i)
if i = 1
else
Therefore, we design an algorithm S which uses A
as a subroutine. The algorithm trains a regular network
(which can be watermarked by our scheme) and adds the
commitment into the veriﬁcation key. Then, it will use A
to ﬁnd openings for these commitments. The algorithm
S works as follows:
1. Receive the commitment c from challenger.
2. Compute (M, ˆM, mk, vk) ← MModel().
3. Let vk = {c(i)
t
L }i∈[n] set
,c(i)
and ˆvk ← { ˆc(i)
t
5. Let
4. Compute ( ˜M, ˜mk) ← A(O f , ˆM, ˆvk).
˜mk = (({t(1), . . . ,t(n)},TL),{r(i)
L }i∈[n]).
,r(i)
If Verify( ˜mk, ˆvk, ˜M) = 1 output t(1),r(1)
put ⊥.
t
t
, else out-
Since the commitment scheme is statistically hiding, the
input to A is statistically indistinguishable from an in-
put where ˆM is backdoored on all the committed values
of vk. Therefore the output of A in S is statistically in-
distinguishable from the output in the unforgeability def-
inition. With the same probability as in the deﬁnition,
˜mk, ˆvk, ˜M will make Verify output 1. But by its deﬁ-
nition, this means that Open(c,t(1),r(1)
) = 1 so t(1),r(1)
open the challenge commitment c. As the commitment
is statistically hiding (and we generate the backdoor in-
dependently of c) this will open c to another value then
for which it was generated with overwhelming probabil-
ity.
t
t
4.1 From Private to Public Veriﬁability
Using the algorithm Verify constructed in this section
only allows veriﬁcation by an honest party. The scheme
described above is therefore only privately veriﬁable. Af-
ter running Verify, the key mk will be known and an
adversary can retrain the model on the trigger set. This is
not a drawback when it comes to an application like the
protection of intellectual property, where a trusted third
party in the form of a judge exists. If one instead wants
to achieve public veriﬁability, then there are two possi-
ble scenarios for how to design an algorithm PVerify:
allowing public veriﬁcation a constant number of times,
or an arbitrary number of times.
Figure 4: A schematic illustration of the public veriﬁca-
tion process.
In the ﬁrst setting, a straightforward approach to the
construction of PVerify is to choose multiple backdoors
during KeyGen and release a different one in each it-
eration of PVerify. This allows multiple veriﬁcations,
but the number is upper-bounded in practice by the ca-
pacity of the model M to contain backdoors - this can-
not arbitrarily be extended without damaging the accu-
racy of the model. To achieve an unlimited number of
veriﬁcations we will modify the watermarking scheme
to output a different type of veriﬁcation key. We then
present an algorithm PVerify such that the interaction
τ with an honest prover can be simulated as τ(cid:48) given the
values M, vk, Verify(mk, vk,M) only. This simulation
means that no other information about mk beyond what
is leaked from vk ever gets to the veriﬁer. We give a
graphical depiction of the approach in Figure 4. Our so-
lution is sketched in Appendix A.1.
4.2
Implementation Details
For an implementation, it is of importance to choose the
size |T| of the trigger set properly, where we have to
consider that |T| cannot be arbitrarily big, as the accu-
racy will drop. To lower-bound |T| we assume an at-
tacker against non-trivial ownership. For simplicity, we
use a backdooring algorithm that generates trigger sets
from elements where f is undeﬁned. By our simplify-
ing assumption from Section 2.1, the model will clas-
sify the images in the trigger set to random labels. Fur-
thermore, assume that the model is ε-accurate (which it
also is on the trigger set). Then, one can model a dis-
honest party to randomly get (1− ε)|T| out of |T| com-
mitted images right using a Binomial distribution. We
want to upper-bound this event to have probability at
most 2−n and use Hoeffding’s inequality to obtain that
|T| > n· ln(2)/( 1|L| + ε − 1).
To implement our scheme, it is necessary that vk be-
comes public before Verify is used. This ensures that
1622    27th USENIX Security Symposium
USENIX Association
VerifyPVerifySimulatorM;vkmkmkττ00=10=1=≈0=1a party does not simply generate a fake key after see-
ing a model. A solution for this is to e.g. publish the
key on a time-stamped bulletin board like a blockchain.
In addition, a statistically hiding commitment scheme
should be used that allows for efﬁcient evaluation in
zero-knowledge (see Appendix A.1). For this one can
e.g. use a scheme based on a cryptographic hash func-
tion such as the one described in [39].
5 A Direct Construction of Watermarking
This section describes a scheme for watermarking a neu-
ral network model for image classiﬁcation, and experi-
ments analyzing it with respect to the deﬁnitions in Sec-
tion 3. We demonstrate that it is hard to reduce the persis-
tence of watermarks that are generated with our method.
For all the technical details regarding the implementation
and hyper-parameters, we refer the reader to Section 5.7.
5.1 The Construction
Similar to Section 4, we use a set of images as the mark-
ing key or trigger set of our construction4. To embed
the watermark, we optimize the models using both train-
ing set and trigger set. We investigate two approaches:
the ﬁrst approach starts from a pre-trained model, i.e., a
model that was trained without a trigger set, and contin-
ues training the model together with a chosen trigger set.
This approach is denoted as PRETRAINED. The second
approach trains the model from scratch along with the
trigger set. This approach is denoted as FROMSCRATCH.
This latter approach is related to Data Poisoning tech-
niques.
During training, for each batch, denote as bt the batch
at iteration t, we sample k trigger set images and ap-
pend them to bt. We follow this procedure for both ap-
proaches. We tested different numbers of k (i.e., 2, 4,
and 8), and setting k = 2 reach the best results. We
hypothesize that this is due to the Batch-Normalization
layer [23]. The Batch-Normalization layer has two
modes of operations. During training, it keeps a running
estimate of the computed mean and variance. During an
evaluation, the running mean and variance are used for
normalization. Hence, adding more images to each batch
puts more focus on the trigger set images and makes con-
vergence slower.
In all models we optimize the Negative Log Likeli-
hood loss function on both training set and trigger set.
4As the set of images will serve a similar purpose as the trigger set
from backdoors in Section 2, we denote the marking key as trigger set
throughout this section.
Notice, we assume the creator of the model will be the
one who embeds the watermark, hence has access to the
training set, test set, and trigger set.
In the following subsections, we demonstrate the ef-
ﬁciency of our method regarding non-trivial ownership
and unremovability and furthermore show that
is
functionality-preserving, following the ideas outlined in
Section 3. For that we use three different image classi-
ﬁcation datasets: CIFAR-10, CIFAR-100 and ImageNet
[28, 37]. We chose those datasets to demonstrate that our
method can be applied to models with a different number
of classes and also for large-scale datasets.
it
5.2 Non-Trivial Ownership
In the non-trivial ownership setting, an adversary will
not be able to claim ownership of the model even if he
knows the watermarking algorithm. To fulﬁll this re-
quirement we randomly sample the examples for the trig-
ger set. We sampled a set of 100 abstract images, and for
each image, we randomly selected a target class.
This sampling-based approach ensures that the exam-
ples from the trigger set are uncorrelated to each other.
Therefore revealing a subset from the trigger set will
not reveal any additional information about the other
examples in the set, as is required for public veriﬁa-
bility. Moreover, since both examples and labels are
chosen randomly, following this method makes back-
propagation based attacks extremely hard. Figure 5
shows an example from the trigger set.
Figure 5: An example image from the trigger set. The
label that was assigned to this image was “automobile”.
5.3 Functionality-Preserving
For the functionality-preserving property we require that
a model with a watermark should be as accurate as a
model without a watermark. In general, each task deﬁnes
USENIX Association
27th USENIX Security Symposium    1623
its own measure of performance [2, 25, 4, 3]. However,
since in the current work we are focused on image clas-
siﬁcation tasks, we measure the accuracy of the model
using the 0-1 loss.
Table 1 summarizes the test set and trigger-set classiﬁ-
cation accuracy on CIFAR-10 and CIFAR-100, for three
different models; (i) a model with no watermark (NO-
WM); (ii) a model that was trained with the trigger set
from scratch (FROMSCRATCH); and (iii) a pre-trained
model that was trained with the trigger set after conver-
gence on the original training data set (PRETRAINED).
Model
Test-set acc.
Trigger-set
acc.
NO-WM
FROMSCRATCH
PRETRAINED
NO-WM
FROMSCRATCH
PRETRAINED
CIFAR-10
93.42
93.81
93.65
CIFAR-100
74.01
73.67
73.62
7.0
100.0
100.0
1.0
100.0
100.0
Table 1: Classiﬁcation accuracy for CIFAR-10 and
CIFAR-100 datasets on the test set and trigger set.
It can be seen that all models have roughly the same
test set accuracy and that in both FROMSCRATCH and
PRETRAINED the trigger-set accuracy is 100%. Since
the trigger-set labels were chosen randomly, the NO-
WM models’ accuracy depends on the number of
classes. For example, the accuracy on CIFAR-10 is 7.0%
while on CIFAR-100 is only 1.0%.
5.4 Unremovability
In order to satisfy the unremovability property, we ﬁrst
need to deﬁne the types of unremovability functions we
are going to explore. Recall that our goal in the unremov-
ability experiments is to investigate the robustness of the
watermarked models against changes that aim to remove
the watermark while keeping the same functionality of
the model. Otherwise, one can set all weights to zero
and completely remove the watermark but also destroy
the model.
Thus, we are focused on ﬁne-tuning experiments. In
other words, we wish to keep or improve the performance
of the model on the test set by carefully training it. Fine-
tuning seems to be the most probable type of attack since
it is frequently used and requires less computational re-
sources and training data [38, 45, 35]. Since in our set-
tings we would like to explore the robustness of the wa-
termark against strong attackers, we assumed that the ad-
versary can ﬁne-tune the models using the same amount
of training instances and epochs as in training the model.
An important question one can ask is: when is it still
my model? or other words how much can I change the
model and still claim ownership? This question is highly
relevant in the case of watermarking. In the current work
we handle this issue by measuring the performance of
the model on the test set and trigger set, meaning that
the original creator of the model can claim ownership of
the model if the model is still ε-accurate on the original
test set while also ε-accurate on the trigger set. We leave
the exploration of different methods and of a theoretical
deﬁnition of this question for future work.
Fine-Tuning. We deﬁne four different variations of
ﬁne-tuning procedures:
• Fine-Tune Last Layer (FTLL): Update the parame-
ters of the last layer only. In this setting we freeze
the parameters in all the layers except in the output
layer. One can think of this setting as if the model
outputs a new representation of the input features
and we ﬁne-tune only the output layer.
• Fine-Tune All Layers (FTAL): Update all the layers
of the model.
• Re-Train Last Layers (RTLL): Initialize the param-
eters of the output layer with random weights and
only update them. In this setting, we freeze the pa-
rameters in all the layers except for the output layer.
The motivation behind this approach is to investi-
gate the robustness of the watermarked model under
noisy conditions. This can alternatively be seen as
changing the model to classify for a different set of
output labels.
• Re-Train All Layers (RTAL): Initialize the param-
eters of the output layer with random weights and
update the parameters in all the layers of the net-
work.