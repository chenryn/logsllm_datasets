As shown in Figure 5(b), the value of iter4 is (cid:100)R/3UNROLL(cid:101)
in all iterations of Loop 2 except those that are part of
the last two iterations of Loop 1. For the latter, iter4 is
(cid:100)((R + n mod R)/2)/3UNROLL(cid:101), which is a lower value.
Consequently, by counting the number of DCG patterns that
have a low value of iter4, and dividing it by 2, we attain iter2.
We then follow the procedure of Step 2 to calculate k. Specif-
ically, all Loop 2 iterations but the last two execute a block of
size Q; the last two execute a block of size (Q + k mod Q)/2
each (Figure 5(a)). Hence, we time the execution of two itera-
tions of Loop 2 in the ﬁrst Loop 1 iteration: a “normal” one
(t(cid:48)
normal) and the last one (t(cid:48)
small). We then compute k like in
Step 2:
k = (iter2 − 2)× Q + 2× t(cid:48)
t(cid:48)
normal
small
× Q
Step 4: Extract iter1 and determine the value of n. If
we take the total number of DCG patterns in the execu-
tion from Step 1 and divide that by iter2, we obtain iter1.
We know that all Loop 1 iterations but the last two exe-
cute a block of size R; the last two execute a block of size
(R + n mod R)/2 each. To compute the size of the latter
block, we note that, in the last two iterations of Loop 1, iter4
is (cid:100)((R + n mod R)/2)/3UNROLL(cid:101). Since both iter4 and
3UNROLL are known, we can estimate (R + n mod R)/2. We
neglect the effect of the ceiling operator because 3UNROLL
is a very small number. Hence, we compute n as:
n = (iter1 − 2)× R + 2× iter4 × 3UNROLL
Our attack cannot handle the cases when m or k are less
than or equal to twice their corresponding block sizes. For
example, when m is less than or equal to 2× P, there is no
iteration of Loop 3 that operates on a smaller block size. Our
procedure cannot compute the exact value of m, and can only
say that m ≤ 2P.
6 Generalization of the Attack on GEMM
Our attack can be generalized to other BLAS libraries, since
all of them use blocked matrix-multiplication, and most of
them implement Goto’s algorithm [20]. We show that our
attack is still effective, using the Intel MKL library as an
example. MKL is a widely used library but is closed source.
We reverse engineer the scheduling of the three-level nested
loop in MKL and its block sizes. The information is enough
for us to apply the same attack procedure in Section 5.3 to
obtain matrix dimensions.
Constructing the DCG We apply binary analysis [41, 73]
techniques to construct the DCG of the GEMM function in
MKL, shown in Figure 6. The pattern is the same as the
DCG of OpenBLAS in Figure 4. Thus, the attack strategy in
Section 5 also works towards MKL.
Extracting Block Sizes Similar to OpenBLAS, in MKL,
there exists a linear relationship between matrix dimensions
USENIX Association
29th USENIX Security Symposium    2011
RRRQQQP①R + n mod R②Q + k mod Q③P + m mod PLoop 1Loop 2Loop 3iter4R3UNROLLR+n	mod	R23UNROLL(a)(b)TimeQQQQQQ②②②②①③P③P③QQQQQQP③P③②Figure 6: DCG of blocked GEMM in Intel MKL, with the
number of invocations per iteration of Loop 2.
and iteration count for each of the loops, as shown in Formu-
las 1 and 2. When the matrix size increases by a block size,
the corresponding iteration count increases by 1. We leverage
this relationship to reverse engineer the block sizes for MKL.
Speciﬁcally, we gradually increase the input dimension size
until the number of iterations increments. The stride on the
input dimension that triggers the change of iteration count is
the block size.
Special Cases According to our analysis, MKL follows
a different DCG when dealing with small matrices. First,
instead of executing three-level nested loops, it uses a single-
level loop, tiling on the dimension that has the largest value
among m, n, k. Second, the kernel computation is performed
directly on the input matrices, without packing and buffering
operations.
For these special cases, we slightly adjust the attack strategy
in Figure 5. We use side channels to monitor the number of
iterations on that single-level loop and the time spent for each
iteration. We then use the number of iterations to deduce
the size of the largest dimension. Finally, we use the timing
information for each iteration to deduce the product of the
other two dimensions.
7 Experimental Setup
Attack Platform We evaluate our attacks on a Dell work-
station Precision T1700, which has a 4-core Intel Xeon E3
processor and an 8GB DDR3-1600 memory. The processor
has two levels of private caches and a shared last level cache.
The ﬁrst level caches are a 32KB instruction cache and a
32KB data cache. The second level cache is 256KB. The
shared last level cache is 8MB. We test our attacks on a same-
OS scenario using Ubuntu 4.2.0-27, where the attacker and
the victim are different processes within the same bare-metal
server. Our attacks should be applicable to other platforms,
as the effectiveness of Flush+Reload and Prime+Probe has
been proved in multiple hardware platforms [38, 69].
Victim DNNs We use a VGG [52]
instance and a
ResNet [26] instance as victim DNNs. VGG is represen-
tative of early DNNs (e.g., AlexNet [33] and LeNet [34]).
ResNet is representative of state-of-the-art DNNs. Both are
standard and widely-used CNNs with a large number of layers
and hyper-parameters. ResNet additionally features shortcut
connections.
There are several versions of VGG, with 11 to 19 layers.
All VGGs have 5 types of layers, which are replicated a dif-
ferent number of times. We show our results on VGG-16.
There are several versions of ResNet, with 18 to 152 layers.
All of them consist of the same 4 types of modules, which are
replicated a different number of times. Each module contains
3 or 4 layers, which are all different. We show our results on
ResNet-50.
The victim programs are implemented using the Keras [10]
framework, with Theano [54] as the backend. We execute
each DNN instance with a single thread.
Attack Implementation We use Flush+Reload and
Prime+Probe attacks. In both attacks, the attacker and the
victim are different processes and are pinned to different
cores, only sharing the last level cache.
In Flush+Reload, the attacker and the victim share the
BLAS library via page de-duplication. The attacker probes
one address in itcopy and one in oncopy every 2,000 cycles.
There is no need to probe any address in kernel, as the access
pattern is clear enough. Our Prime+Probe attack targets the
last level cache. We construct two sets of conﬂict addresses
for the two probing addresses using the algorithm proposed
by Liu et al. [38]. The Prime+Probe uses the same monitoring
interval length of 2,000 cycles.
8 Evaluation
We ﬁrst evaluate our attacks on the GEMM function. We
then show the effectiveness of our attack on neural network
inference, followed by an analysis of the search space of DNN
architectures.
8.1 Attacking GEMM
8.1.1 Attack Examples
Figure 7 shows raw traces generated by Flush+Reload and
Prime+Probe when monitoring the execution of the GEMM
function in OpenBLAS. Due to space limitations, we only
show the traces for one iteration of Loop 2 (Algorithm 1).
Figure 7(a) is generated under Flush+Reload. It shows
the latency of the attacker’s reload accesses to the probing
addresses in the itcopy and oncopy functions for each mon-
itoring interval. In the ﬁgure, we only show the instances
where the access took less than 75 cycles. These instances
correspond to cache hits and, therefore, cases when the victim
executed the corresponding function. Figure 7(b) is generated
under Prime+Probe. It shows the latency of the attacker’s
probe accesses to the conﬂict addresses. We only show the in-
stances where the accesses took more than 500 cycles. These
instances correspond to cache misses of at least one conﬂict
address. They are the cases when the victim executed the
corresponding function.
2012    29th USENIX Security Symposium
USENIX Association
copybncopybncopyanker0ker0#pairs=iter4#pairs=iter3-1Figure 7: Flush+Reload (a) and Prime+Probe (b) traces of the GEMM execution. The monitoring interval is 2,000 cycles.
(a)
(b)
Since we select the probing addresses to be within
in-function loops (Section 5.2), a cluster of hits in the
Flush+Reload trace (or misses in the Prime+Probe trace) indi-
cates the time period when the victim is executing the probed
function.
In both traces, the victim calls itcopy before interval
2,000, then calls oncopy 11 times between intervals 2,000
and 7,000. It then calls itcopy another two times in inter-
vals 7,000 and 13,000. The trace matches the DCG shown in
Figure 4. We can easily derive that iter4 = 11 and iter3 = 3.
8.1.2 Handling Noise
Comparing the two traces in Figure 7, we can observe that
Prime+Probe suffers much more noise than Flush+Reload.
The noise in Flush+Reload is generally sparsely and randomly
distributed, and thus can be easily ﬁltered out. However,
Prime+Probe has noise in consecutive monitoring intervals,
as shown in Figure 7(b). It happens mainly due to the non-
determinism of the cache replacement policy [13]. When
one of the cache ways is used by the victim’s line, it takes
multiple “prime” operations to guarantee that the victim’s line
is selected to be evicted. It is more difﬁcult to distinguish the
victim’s accesses from such noise.
We leverage our knowledge of the execution patterns in
GEMM to handle the noise in Prime+Probe. First, recall
that we pick the probing addresses within tight loops inside
each of the probing functions (Section 5.2). Therefore, for
each invocation of the functions, the corresponding probing
address is accessed multiple times, which is observed as a
cluster of cache misses in Prime+Probe. We count the num-
ber of consecutive cache misses in each cluster to obtain its
size. The size of a cluster of cache misses that are due to
noise is smaller than size of a cluster of misses that are caused
by the victim’s accesses. Thus, we discard the clusters with
small sizes. Second, due to the three-level loop structure,
each probing function, such as oncopy, is called repetitively
with consistent interval lengths between each invocation (Fig-
ure 4). Thus, we compute the distances between neighboring
clusters and discard the clusters with abnormal distances to
their neighbors.
These two steps are effective enough to handle the noise in
Prime+Probe. However, when tracing MKL’s special cases
that use a single-level loop (Section 6), we ﬁnd that using
Prime+Probe is ineffective to obtain useful information. Such
environment affects the accuracy of the Cache Telepathy at-
tack, as we will see in Section 8.3.
8.2 Extracting Hyper-parameters of DNNs
We show the effectiveness of our attack by extracting the
hyper-parameters of VGG-16 [52] and ResNet-50 [26]. Fig-
ures 8(a), 8(b), and 8(c) show the extracted values of the n, k,
and m matrix parameters, respectively, using Flush+Reload.
In each ﬁgure, we show the values for each of the layers (L1,
L2, L3, and L4) in the 4 distinct modules in ResNet-50 (M1,
M2, M3, and M4), and for the 5 distinct layers in VGG-16
(B1, B2, B3, B4, and B5). We do not show the other layers
because they are duplicates of the layers shown.
Figure 8: Extracted values of the n, k, and m matrix param-
eters for VGG-16 and ResNet-50 using Flush+Reload on
OpenBLAS.
The ﬁgures show three data points for each parameter (e.g.,
m) and each layer (e.g., L1 in ResNet-M2): a hollowed circle,
a solid square or rectangle, and a solid circle. The hollowed
circle indicates the actual value of the parameter. The solid
square or rectangle indicates the value of the parameter de-
USENIX Association
29th USENIX Security Symposium    2013
020004000600080001000012000Time (Interval ID)5560657075Latency (Cycles)itcopyoncopy20004000600080001000012000Time (Interval ID)6009001200Latency (Cycles)itcopyoncopy(a) Extracting n0500100015002000actual valuedetected value or range from side channeldeduced value using DNN constraints(b) Extracting k01000200030004000L1L2L3L4L1L2L3L4L1L2L3L4L1L2L3L4B1B2B3B4B5ResNet-M1ResNet-M2ResNet-M3ResNet-M4VGG(c) Extracting m102103104tected with our side channel attack. When the side channel
attack can only narrow down the possible value to a range, the
ﬁgure shows a rectangle. Finally, the solid circle indicates the
value of the parameter that we deduce, based on the detected
value and some DNN constraints. For example, for parame-
ter m in layer L1 of ResNet-M2, the actual value is 784, the
detected value range is [524,1536], and the deduced value is
784.
We will discuss how we obtain the solid circles later. Here,
we compare the actual and the detected values (hollowed cir-
cles and solid squares/rectangles). Figure 8(a) shows that our
attack is always able to determine the n value with negligible
error. The reason is that, to compute n, we need to estimate
iter1 and iter4 (Section 5.3), and it can be shown that most of
the noise comes from estimating iter4. However, since iter4 is
multiplied by the small 3UNROLL parameter in the equation
for n, the impact of such noise is small.
Figures 8(b) and (c) show that the attack is able to ac-
curately determine the m and k values for all the layers in
ResNet-M1 and VGG, and for most layers in ResNet-M4.
However, it can only derive ranges of values for most of the
ResNet-M2 and ResNet-M3 layers. This is because the m and
k values in these layers are often smaller than twice of the
corresponding block sizes (Section 5.3).
In Figure 9, we show the same set of results by analyz-
ing the traces generated using Prime+Probe. Compared to
the results from Flush+Reload, there are some differences
of detected values or ranges, especially in ResNet-M3 and
ResNet-M4.
negligible error, or can provide a range where the actual
value falls in. We will next show that, in many cases, the
imprecision from the negligible error and the ranges can be
eliminated after applying DNN constraints (Section 8.3.2).
8.3 Size of Architecture Search Space
In this section, we compare the number of architectures in the
search space without Cache Telepathy (which we call original
space), and with Cache Telepathy (which we call reduced
space). In both cases, we only consider reasonable hyper-
parameters for the layers as follows. For fully-connected
layers, the number of neurons can be 2i, where 8 ≤ i ≤ 13. For
convolutional layers, the number of ﬁlters can be a multiple
of 64 (64× i, where 1 ≤ i ≤ 32), and the ﬁlter size can be an
integer value between 1 and 11.
8.3.1 Size of the Original Search Space
To be conservative, when computing the size of the original
search space, we assume that the attacker knows the number
of layers and type of each layer in the oracle DNN. There
exist 352 different conﬁgurations for each convolutional layer
without considering pooling or striding, and 6 conﬁgurations
for each fully-connected layer. Moreover, considering the
existence of non-sequential connections, given L layers, there
are L× 2L−1 possible ways to connect them.
A network like VGG-16 has ﬁve different layers (B1, B2,
B3, B4, and B5), and no shortcuts. If we consider only these
ﬁve different layers, the size of the search space is about
5.4×1012 candidate architectures. A network like ResNet-50
has 4 different modules (M1, M2, M3, and M4) and some
shortcuts inside these modules. If we consider only these
four different modules, the size of the search space is about
6× 1046 candidate architectures. Overall, the original search
space is intractable.
8.3.2 Determining the Reduced Search Space
Using the detected values of the matrix parameters in Sec-
tion 8.2, we ﬁrst determine the possible connections between
layers by locating shortcuts. Next, for each possible connec-
tion conﬁguration, we calculate the possible hyper-parameters
for each layer. The ﬁnal search space is computed as
search space =
C