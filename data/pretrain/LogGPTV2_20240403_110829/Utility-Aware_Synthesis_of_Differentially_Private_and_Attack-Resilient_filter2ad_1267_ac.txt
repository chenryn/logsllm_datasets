because although we have a mobility model Π for intra-trajectory
movement, we need a start state and an end state for specifying
the two endpoints of our walk. Without the trip distribution we
can assume a uniform distribution of start and end states, however,
as Figure 4 shows, the trip distributions of real datasets are often
heavily skewed. Thus, assuming a uniform distribution in place of
the actual distribution is far from ideal, and will result in bogus
synthetic trips that jeopardize utility and authenticity.
Assume that we discretize Ω(Dr eal) using grid A. We denote a
trip using its start cell Cstar t and destination cell Cend, as: Cstar t (cid:123)
Cend. Let h(Dr eal , Cstar t (cid:123) Cend) be a function that computes
the number of trips Cstar t (cid:123) Cend in database Dr eal , and ˆh de-
note its private noisy version. For brevity we drop Dr eal from the
notation and simply write h(Cstar t (cid:123) Cend). Then, treating X as
a random variable over the domain of the trip distribution A × A,
Figure 4: Cumulative Distribution Function (CDF) of the trip
distribution R of 3 datasets, compared with the CDF of uni-
form distribution. For consistency we enforced a 3x3 uni-
form grid on all datasets and ordered the cell pairs on x-axis
in non-decreasing trip frequency.
the entries in the trip distribution denoted R are calculated as:
(cid:16)
X = (Cstar t , Cend)(cid:17)
=
Pr


 ˆh(Cst ar t (cid:123)Cend)
ˆh(Ci (cid:123)Cj)
Cj
0
Ci
for Cstar t , Cend ∈ A
otherwise
Note that our definition ensures R is a probability mass function
(pmf) since its entries sum to 1.
In the case of a two-layer grid where one GPS location is indexed
by both a top-level and bottom-level cell, we can use constrained
inference for improved accuracy and consistency [15, 25, 43]. In
particular, we employ the following linear Ordinary Least Squares
(OLS) approach. We denote by Ci a cell in the top-level of the grid,
and by Ci, j a cell in the bottom-level of the grid where 1 ≤ j ≤ M
2
i
(see the notation in Figure 3). We use budget θε3 when obtaining top-
level trip counts ˆh(Ci (cid:123) Cj), and budget (1 − θ)ε3 when obtaining
bottom-level counts ˆh(Ci,k (cid:123) Cj,l). Note that if we had no privacy
or perturbation requirement, and we simply used the noise-free
counts h instead of ˆh, the following would hold: h(Ci (cid:123) Cj) =
l h(Ci,k (cid:123) Cj,l). However, this may not hold after each h
is perturbed with random noise. To re-establish consistency and
minimize noise impact, OLS asserts that given the noisy values of
ˆh, we can obtain optimized trip counts, denoted ˆh′(Ci (cid:123) Cj), as:


k
ˆh
′(Ci (cid:123) Cj) =
+
2
θ
2
2
i M
M
j
(1 − θ)2 + θ
2
M
(1 − θ)2
(1 − θ)2 + θ
2
M
2
i M
2
j
2
i M
2
j
· ˆh(Ci (cid:123) Cj)
·

ˆh(Ci,k (cid:123) Cj,l)
k
l
When optimizing the counts for the bottom-level, differences in
optimized top-level counts calculated above are distributed equally
among the bottom-level cells:
Finally, optimized trip counts ˆh′ are used in place of ˆh in the defi-
nition of R. For the derivation of OLS and a worked example, we
refer the reader to Appendix B.
4.4 Route Length Distribution
The final component in AdaTrace’s private synopsis consists of
fitted distributions of route lengths. We fit a different theoretical
ˆh
′(Ci,k (cid:123) Cj,l) = ˆh(Ci,k (cid:123) Cj,l)
ˆh′(Ci (cid:123) Cj) −

+
ˆh(Ci,s (cid:123) Cj,t)
s
2
M
i M
t
2
j
:Grid A, trip distribution R, mobility model Π, length
distributions L
Algorithm 1: Trajectory synthesis algorithm
Input
Output:A candidate synthetic trajectory Tsyn
1 Pick a random sample (Cstar t , Cend) from pmf of R
2 Retrieve from L the fitted probability distribution PD for trip
Cstar t (cid:123) Cend
3 Pick a random sample ℓ from PD
4 Initialize Tsyn with Tsyn[1] = Cstar t and Tsyn[ℓ] = Cend
5 for i = 2 to ℓ − 1 do
for Ccand ∈ A do
Retrieve from Π:
w1 = Pr
T [i] = Ccand | T [1] . . . T [i-1](cid:17) and
(cid:16)
(cid:16)
(cid:17)
T [ℓ] = Cend | T [1] . . . T [i-1]Ccand
w2 = Pr
Set the weight of Ccand equal to w1 · w2
end
Sample Cchosen from A with probability proportional to
its weight calculated above
Set Tsyn[i] = Cchosen
6
7
8
9
10
11
12 end
13 return T syn
The algorithm can be studied in four steps. First, it determines
the start and end points of the synthetic trajectory Tsyn by sam-
pling from the trip distribution. Second, it determines the length of
Tsyn by sampling from the appropriate route length distribution
in L. Third, it initializes Tsyn with first location as the starting
cell of the trip, and last location as the destination cell of the trip.
Fourth, given the two endpoints of Tsyn, intermediate locations
are found using a random walk on the mobility model Π. When
determining the i’th position of Tsyn considering the cells of grid
A as candidates, each candidate is given a weight that consists of
two sub-weights denoted w1 and w2. w1 performs a look-back and
finds the probability that the next location is Ccand given the previ-
ous locations, as in the straightforward application of the Markov
assumption. This is essentially a one-step transition probability. On
the other hand, w2 performs a look-ahead and finds the probability
that the final location is Cend given the previous locations and
assuming the current location is set to Ccand. This is an (ℓ − i)-step
transition probability, which is computed using a combination of 1-
step transition probabilities. To improve efficiency, we pre-compute
multiple-step transition probabilities after learning Π so that the
same computation is not repeated for different Tsyn.
The above generates a trajectory for a single trip between well-
defined start and end locations. This is sufficient when each user’s
GPS record in Dr eal corresponds to a short-term trip, such as an
Uber or taxi ride. However, if Dr eal is collected over long periods of
time (e.g., several days), then a user’s record may contain multiple
trips. In this case, the synthesis algorithm can be run multiple times
per user, such that in each iteration, the starting location of the
next trajectory is equal to the last known location of the previous
trajectory. Then, these trajectories can be stitched together to form
the final GPS record of the user with a desired number of trips.
Figure 5: Comparing frequencies of observed route lengths
(histogram) with candidate distributions.
distribution for each different trip Cstar t (cid:123) Cend. The distribution
is learned based on observed route lengths in Dr eal that make this
trip. Our rationale in enforcing trip-specific length distributions is
that trajectories travelling between certain regions may take more
indirect routes than others, for reasons such as unavailability of
roads, traffic avoidance, and so on. Hence, trip-specific length dis-
tributions will be more accurate than global distributions (learned
using whole Dr eal ) used in previous works [36, 55].
We treat observed route lengths as a histogram. We consider
multiple well-known distributions with different shapes, such as
uniform, exponential and Poisson distributions, as candidates to
capture the shape of our histogram. An example is shown in Figure
5. The candidate distributions have one characteristic in common:
Their parameters are directly related to aggregate summary statis-
tics that can be privately and accurately obtained from Dr eal . For ex-
ample, the Poisson distribution has a single parameter that is equal
to the mean length, whereas the λ parameter of the exponential
distribution is related to the median length med as: λ = ln2/med. We
then use the Laplace and Exponential mechanisms to privately fetch
statistics such as means and medians. A private mean can be fetched
by decomposing it into a noisy sum divided by a noisy count, where
the Laplace mechanism is used to inject noise. A private median
can be fetched using Cormode et al.’s adaptation of the Exponential
mechanism with scoring function q := −|rank(x) − rank(med)|
[15]. The mechanism returns noisy median x instead of the actual
median, and the intuition behind this particular q is that if x is close
to the actual median, then its rank will be similar to the median’s
rank. Thus, the score of each candidate is negatively impacted by
how much its rank deviates from rank(med).
After building multiple candidate distributions as above, we
select one distribution as the best fit, store it in AdaTrace’s memory,
and drop the remaining. We use a goodness of fit test to determine
which distribution is the best fit. While there are several tests to
2 test statistic
measure goodness of fit, we use the value of the χ
since its differentially private implementation is well known [22].
For example, in Figure 5 the goodness of fit test would select the
exponential distribution as the best fit, since its shape is closest to
the shape of the histogram.
4.5 Trajectory Synthesis Algorithm
AdaTrace’s synthesis algorithm combines the four features in its
private synopsis: the density-aware grid A, the mobility model
Π(Dr eal), the trip distribution R and the collection L of length
distributions for each trip. It outputs synthetic trajectories based
on the skeleton presented in Algorithm 1.
5 PRIVACY ANALYSIS
AdaTrace has differential privacy and attack resilience as its two
privacy goals. In this section we give concrete definitions and pri-
vacy parameters with which AdaTrace fulfills these goals. Our
definitions are parameterized to enable the data owner to explicitly
control the leakage potential of synthetic trajectories by specifying
the desired privacy parameters.
budgets (one for each feature in the synopsis) such that4
5.1 Differential Privacy Preservation
AdaTrace satisfies ε-differential privacy as a whole. Recall that we
treat ε as the total privacy budget and distribute it to four sub-
i =1 εi = ε.
Learning and perturbing a feature consumes the εi allocated to it,
thus depleting the total ε after the perturbation phase is complete,
according to the sequential composition property. Then, any addi-
tional data-independent perturbation performed to satisfy attack
resilience counts as post-processing. Note that sequential compo-
sition holds even when subsequent computations incorporate the
outcomes of preceding computations [35], hence the fact that Ada-
Trace uses the perturbed grid in subsequent steps (Markov chain,
trip distribution) does not violate ε-differential privacy. Also note
that AdaTrace’s synthesis algorithm performs sampling and calcu-
lation on perturbed features without accessing the actual database
Dr eal . As a result, AdaTrace remains ε-differentially private.
Distribution of ε into εi can be done either by AdaTrace auto-
matically or according to the specifications of the data owner. The
current implementation of AdaTrace comes with a default budget
distribution, which we empirically determined to yield high aver-
age utility: ε1 = ε/9 for the grid, ε2 = 4ε/9 for the Markov mobility
model, ε3 = 3ε/9 for the trip distribution and ε4 = ε/9 for the length
distribution. We can observe from this that not all features require
high budgets to be accurate, and allocating an unnecessarily high
budget to such features will negatively impact utility since it would
steal from those features that do require high budgets to remain
accurate. For example, the grid and length distribution components
need lower budgets in comparison to others. One way to further op-
timize budget distribution is to leverage regression-based learning
on Dr eal and ε, a plan for future work.
Although the above strategy is beneficial from a utility maximiza-
tion perspective, we should also discuss its privacy implications.
Since each individual feature will satisfy εi-differential privacy, a
higher εi will cause the feature to be accurately preserved, whereas
a lower εi will cause it to be more perturbed. A variable budget dis-
tribution allows the data owner (e.g., service provider) to distribute
ε in a way that reflects which artifact he perceives is more sensitive
and should thus be more perturbed to protect its privacy. For exam-
ple, if the data owner feels that spatial densities are more sensitive,
then a lower ε1 can be assigned to grid construction. If trip distri-
butions are sensitive, a lower ε3 can be assigned, causing a more
perturbed trip distribution. This provides flexibility with respect to
different perceptions regarding what needs to be protected.
5.2 Enforcement of Attack Resilience
We now discuss how AdaTrace ensures attack resilience against
the 3 privacy threats listed in Section 3.3.
Bayesian Inference. Recall that we denote by Z a sensitive zone
such as a hospital, health clinic or religious place, by B(Z) the
prior belief of an adversary regarding the users who visit Z, and by
B(Z|Dsyn) the posterior belief having observed Dsyn. The threat
stems from B(Z|Dsyn) being significantly different than B(Z). To
combat the threat, AdaTrace enforces the following defense.
Defense 1 (ϑ). Denoting by EMD the Earth Mover’s Distance, we
say that Dsyn is attack-resilient if the following holds, and susceptible
otherwise.
EMD(cid:0)B(Z), B(Z|Dsyn)(cid:1) ≤ ϑ
Intuitively, the privacy guarantee is that upon observing Dsyn,
an adversary’s belief regarding users visiting Z does not change
significantly, where the level of significance is controlled by param-
eter ϑ. This prohibits both positive and negative disclosures, i.e.,
the adversary is prohibited not only from learning that significantly
higher than 10% of Z’s visitors live in a certain neighborhood, but
also that significantly lower than 10% of Z’s visitors live in a cer-